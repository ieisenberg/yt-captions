[
  {
    "text": "hi and welcome to zero downtime deployments where we're going to take a look at application rollouts and rollbacks i'm",
    "start": "80",
    "end": "6480"
  },
  {
    "text": "going to be your speaker hey there my name is chris i am a consultant and advisor",
    "start": "6480",
    "end": "12719"
  },
  {
    "text": "and uh sometimes speaker you can see some of my accolades there so without further ado let's get started",
    "start": "12719",
    "end": "18560"
  },
  {
    "text": "controllers in kubernetes are control loops that watch the cluster state",
    "start": "18560",
    "end": "23600"
  },
  {
    "text": "and if necessary they're going to introduce some sort of change right so",
    "start": "23600",
    "end": "29119"
  },
  {
    "text": "the control loop holds desired state it's querying the cluster for",
    "start": "29119",
    "end": "34399"
  },
  {
    "text": "actual state and if those things don't match then it takes action so we're going to focus on the",
    "start": "34399",
    "end": "39680"
  },
  {
    "text": "deployments in the staple sets specifically because they're primarily for long-running applications and we want to",
    "start": "39680",
    "end": "45360"
  },
  {
    "text": "look at their behaviors when it comes to rolling updates so first look at deployment here uh it",
    "start": "45360",
    "end": "52719"
  },
  {
    "text": "describes the desired state for an application right so we're going to take the image that we want to run the",
    "start": "52719",
    "end": "60480"
  },
  {
    "text": "affinities and anti-affinities all those things uh and put them in the deployment manifest",
    "start": "60480",
    "end": "66799"
  },
  {
    "text": "and uh the deployment isn't alone in making sure that the application gets",
    "start": "66799",
    "end": "73040"
  },
  {
    "text": "deployed it's actually a wrapper for another controller called a replica set which",
    "start": "73040",
    "end": "79119"
  },
  {
    "text": "enforces the replication factor replication factor is of pods of course and then within",
    "start": "79119",
    "end": "84159"
  },
  {
    "text": "pods are application containers but what that means is that if we create",
    "start": "84159",
    "end": "89759"
  },
  {
    "text": "a deployment then in the cascade we're going to create the replica set the pods and the containers and",
    "start": "89759",
    "end": "97040"
  },
  {
    "text": "if we delete a deployment we do that in a cascade as well uh the rolling update feature that we",
    "start": "97040",
    "end": "102720"
  },
  {
    "text": "get with deployments will change the actual state of the cluster to the desired state",
    "start": "102720",
    "end": "109360"
  },
  {
    "text": "with zero downtime right no no client will be sent into any black hole right that's the",
    "start": "109360",
    "end": "115439"
  },
  {
    "text": "goal here deployments do support a wide array of application types but really do have a feature set that",
    "start": "115439",
    "end": "123040"
  },
  {
    "text": "works best with stateless apps so uh pod host names are not really predictable they have",
    "start": "123040",
    "end": "131200"
  },
  {
    "text": "hashes that we can't really predict ahead of time so talking to an",
    "start": "131200",
    "end": "136239"
  },
  {
    "text": "individual or sending client traffic to an individual pod replica is a little bit more difficult",
    "start": "136239",
    "end": "142080"
  },
  {
    "text": "uh if you're going to embed a persistent volume claim within a deployment template for a pod",
    "start": "142080",
    "end": "150480"
  },
  {
    "text": "remember this is a template right you're going to have many replicas and so pvcs typically have",
    "start": "150480",
    "end": "157040"
  },
  {
    "text": "unique identities i can't parameterize the name of the pvc within the template so",
    "start": "157040",
    "end": "162640"
  },
  {
    "text": "if i have it in rewrite many mode then that's great otherwise um well that's what the stable",
    "start": "162640",
    "end": "168400"
  },
  {
    "text": "stable set is for which we'll look at later so replica sets are a top-level citizen",
    "start": "168400",
    "end": "173840"
  },
  {
    "text": "in the kubernetes api their primary job is really to reconcile desired state",
    "start": "173840",
    "end": "178879"
  },
  {
    "text": "versus actual replication factor right so somewhere on the cluster there are a number of pods actually deployed and the",
    "start": "178879",
    "end": "185200"
  },
  {
    "text": "replicas add needs to reconcile that so within the replica set",
    "start": "185200",
    "end": "190800"
  },
  {
    "text": "object here we see two important things desired replicas and then the label query the label query",
    "start": "190800",
    "end": "195920"
  },
  {
    "text": "is going to be used to do that reconciliation and then when the answer comes back uh a decision has to be made are there",
    "start": "195920",
    "end": "203360"
  },
  {
    "text": "seven pods great do nothing right no action needs to be taken if there are six then we've got to add a",
    "start": "203360",
    "end": "209760"
  },
  {
    "text": "new one right similar to cube ctl run to add a new pod if there are eight then",
    "start": "209760",
    "end": "214879"
  },
  {
    "text": "the replicas that's going to kill one and so this guarantees the availability of n number of",
    "start": "214879",
    "end": "220640"
  },
  {
    "text": "application replicas across nodes in the cluster across availability zones and such",
    "start": "220640",
    "end": "226000"
  },
  {
    "text": "right they can be used independently of deployments but if you do use them independently",
    "start": "226000",
    "end": "233599"
  },
  {
    "text": "they lose the rolling update feature that we're going to look at however there are some cd platforms like",
    "start": "233599",
    "end": "240000"
  },
  {
    "text": "spinnaker which you'll use replicas that's directly for other patterns like red black uh you",
    "start": "240000",
    "end": "245519"
  },
  {
    "text": "will see them used in some cases now the relationship between deployments and replica sets",
    "start": "245519",
    "end": "250879"
  },
  {
    "text": "is typically a one-to-many right where we've got a single deployment that's got multiple rss",
    "start": "250879",
    "end": "256560"
  },
  {
    "text": "underneath it this is because an rs is a snapshot of an application version so when we change the image",
    "start": "256560",
    "end": "262320"
  },
  {
    "text": "what happens is we hash the the new manifest and if the hash doesn't match the old",
    "start": "262320",
    "end": "268560"
  },
  {
    "text": "replicas that hash then we create a new one and we roll out a new revision because we have this one-to-many",
    "start": "268560",
    "end": "274560"
  },
  {
    "text": "relationship between the deployment and the replica sets this allows us to",
    "start": "274560",
    "end": "280720"
  },
  {
    "text": "roll out with zero downtime right so um it lets us essentially",
    "start": "280720",
    "end": "288639"
  },
  {
    "text": "increment the desired replicas of the new replica set and decrement the desired replicas of the old at the",
    "start": "288639",
    "end": "295199"
  },
  {
    "text": "cadence that we specify it also allows us to have number robots because if i want to roll back",
    "start": "295199",
    "end": "300320"
  },
  {
    "text": "to an earlier version i can just do that same pattern right and i've rolled back my application it's",
    "start": "300320",
    "end": "306080"
  },
  {
    "text": "also kind of lightweight tracking revisions of an app so we'll look at the history feature as well so",
    "start": "306080",
    "end": "313199"
  },
  {
    "text": "the idea behind rolling updates is we're going to roll out those changes at the control grade right",
    "start": "313199",
    "end": "320000"
  },
  {
    "text": "new pods are rolled out before old pods are terminated because we want to",
    "start": "320000",
    "end": "326800"
  },
  {
    "text": "have that new replica up before any of the old ones come down now the the new pot doesn't go to the",
    "start": "326800",
    "end": "333280"
  },
  {
    "text": "exact same node it is independently scheduled and gets scheduled on a node that has",
    "start": "333280",
    "end": "338720"
  },
  {
    "text": "available resources based on the resource request and other factors and the default setting ensures that at",
    "start": "338720",
    "end": "344400"
  },
  {
    "text": "least 75 of pod replicas are available throughout the update right so we're not going to tear down below",
    "start": "344400",
    "end": "350240"
  },
  {
    "text": "75 availability you know that's tweakable that we'll look at in a subsequent slide uh client traffic",
    "start": "350240",
    "end": "357199"
  },
  {
    "text": "is then load bounce across all available pods uh as well if you want to see the",
    "start": "357199",
    "end": "362800"
  },
  {
    "text": "um live in progress um roll out you can uh like anything",
    "start": "362800",
    "end": "370720"
  },
  {
    "text": "else in kubernetes you can watch that for updates so let's get to it so i've got",
    "start": "370720",
    "end": "376639"
  },
  {
    "text": "uh uh several node cluster here so let's uh ctl get node and see that we've got um",
    "start": "376639",
    "end": "385039"
  },
  {
    "text": "several workers that we're going to work with so i'm going to uh i've got a directory",
    "start": "385039",
    "end": "391840"
  },
  {
    "text": "here of manifest that i'm going to use i'm just going to cat the deployment one so you can see we've got",
    "start": "391840",
    "end": "399039"
  },
  {
    "text": "some some settings here that will give us 10",
    "start": "399039",
    "end": "405280"
  },
  {
    "text": "replicas and uh give us some time in between the um deployments for minimum ready",
    "start": "405280",
    "end": "412319"
  },
  {
    "text": "seconds uh and then also the default settings for max urgent max unavailable which we'll talk about",
    "start": "412319",
    "end": "418160"
  },
  {
    "text": "uh going forward um they're explicitly set here but those will be the defaults anyways and we've got a uh application here that",
    "start": "418160",
    "end": "425199"
  },
  {
    "text": "simply outputs uh its ip and its hostname",
    "start": "425199",
    "end": "430560"
  },
  {
    "text": "uh so that it will show us like which one is being communicated with",
    "start": "430560",
    "end": "436160"
  },
  {
    "text": "from a client so we will um first set up a watch",
    "start": "436160",
    "end": "442639"
  },
  {
    "text": "and so in the interest of time i'm just going to grab some commands that i've got going",
    "start": "442639",
    "end": "448639"
  },
  {
    "text": "so we're going to watch keep ctl get pods and then we're going to use the key",
    "start": "448639",
    "end": "453759"
  },
  {
    "text": "value pairs here for the um for the deployment right so this is going to tell me no resources",
    "start": "453759",
    "end": "460319"
  },
  {
    "text": "found right because i haven't deployed anything yet but let's go ahead and keep ctl apply",
    "start": "460319",
    "end": "465440"
  },
  {
    "text": "that",
    "start": "465440",
    "end": "467680"
  },
  {
    "text": "and we'll see that we've got 10 pods running now what i want to do is run a client",
    "start": "471680",
    "end": "478479"
  },
  {
    "text": "on this other terminal here and so i'm just going to run a client pod with busybox 1.27",
    "start": "478479",
    "end": "486639"
  },
  {
    "text": "so that i can run a loop and so my loop",
    "start": "486639",
    "end": "493840"
  },
  {
    "text": "is going to do a wget against the service and what that means",
    "start": "493840",
    "end": "499440"
  },
  {
    "text": "is i also need to create the service so going back over here we're going to expose our deployment",
    "start": "499440",
    "end": "507840"
  },
  {
    "text": "and the host info pod listens on 9898 but we're just going to",
    "start": "508000",
    "end": "514000"
  },
  {
    "text": "have the services on 80 to make it easy so we expose that and now i can run my",
    "start": "514000",
    "end": "520080"
  },
  {
    "text": "loop and what will happen is you'll see that it should query and see that we've got the certain rs rs",
    "start": "520080",
    "end": "527760"
  },
  {
    "text": "hash but we've got different endpoint pods that we're connecting to right as we see change we're going to",
    "start": "527760",
    "end": "533519"
  },
  {
    "text": "see this rs hash change in each one of the requests so we'll let that go",
    "start": "533519",
    "end": "539760"
  },
  {
    "text": "and so we've got our watches established so uh let's introduce a",
    "start": "539760",
    "end": "546480"
  },
  {
    "text": "change so here we're going to",
    "start": "546480",
    "end": "551519"
  },
  {
    "text": "uh set a new image that just uses a different tag same",
    "start": "551519",
    "end": "558959"
  },
  {
    "text": "so up here host info latest we're going to go to the alpine version and we're going to record that change so",
    "start": "558959",
    "end": "564399"
  },
  {
    "text": "we can actually see it and so let's take a look at the behavior that happens",
    "start": "564399",
    "end": "570160"
  },
  {
    "text": "out of the box so we get a number of new pods that are",
    "start": "570160",
    "end": "576720"
  },
  {
    "text": "created immediately and one terminated immediately and so most of our load balancing just",
    "start": "576720",
    "end": "582720"
  },
  {
    "text": "right now is still going to the same pods except for there's our first",
    "start": "582720",
    "end": "588080"
  },
  {
    "text": "i'm just clicking on it but there's our first request to the new set of pods and obviously we'll see that increase uh",
    "start": "588080",
    "end": "594399"
  },
  {
    "text": "over time so lots of change happening in the cluster right we've got uh 25 surge 25 surge and 25",
    "start": "594399",
    "end": "602800"
  },
  {
    "text": "unavailable so we're going to see a quarter more pods each iteration through",
    "start": "602800",
    "end": "611360"
  },
  {
    "text": "and so there's some more terminations cleaning up pods after they've been",
    "start": "611360",
    "end": "617600"
  },
  {
    "text": "replaced with new ones and so now all of our requests are going to the new set of pods uh based on the",
    "start": "617600",
    "end": "624240"
  },
  {
    "text": "new hash the 566 hash over here with our client all right so now uh",
    "start": "624240",
    "end": "632399"
  },
  {
    "text": "let's look at uh undoing that because let's say you know we don't want this version of the application something's wrong with",
    "start": "632399",
    "end": "638399"
  },
  {
    "text": "it so we're going to do that now in this case we're going to do an undo but also get the status so we can",
    "start": "638399",
    "end": "646240"
  },
  {
    "text": "actually see the watch as it happens right and see how the changes we already",
    "start": "646240",
    "end": "651360"
  },
  {
    "text": "have a watch established below but see uh what it presents us with in terms of information",
    "start": "651360",
    "end": "658640"
  },
  {
    "text": "so it's going to give us and we can see right away that um you know it started with three",
    "start": "658640",
    "end": "665600"
  },
  {
    "text": "and then jumped really quickly to five and uh we got a slight pause there and then",
    "start": "665600",
    "end": "671920"
  },
  {
    "text": "now we've got um a few more old replicas pending termination",
    "start": "671920",
    "end": "677920"
  },
  {
    "text": "but pretty much we've got everything uh that we need rolled out already so fairly quickly",
    "start": "677920",
    "end": "684600"
  },
  {
    "text": "because um again the application's lightweight so it doesn't really need to pull it",
    "start": "684600",
    "end": "691120"
  },
  {
    "text": "now if we look at the history once this is all the way rolled out right it tells us it's rolled out and we",
    "start": "691120",
    "end": "697680"
  },
  {
    "text": "can look at the history for our deployment right so the first rollout just says essentially",
    "start": "697680",
    "end": "705680"
  },
  {
    "text": "none um so sorry the uh the undo essentially",
    "start": "705680",
    "end": "713760"
  },
  {
    "text": "says none because we didn't really record the undo but that's our undo and then there's our change so we're",
    "start": "713760",
    "end": "720480"
  },
  {
    "text": "back to the old application revision so if we wanted to",
    "start": "720480",
    "end": "726320"
  },
  {
    "text": "interrogate uh version three we could roll a history and then use",
    "start": "726320",
    "end": "733279"
  },
  {
    "text": "revision three so one of the things to note the record as you saw in the command that we used actually",
    "start": "733279",
    "end": "740079"
  },
  {
    "text": "records an annotation for the change cause but uh in the case",
    "start": "740079",
    "end": "745680"
  },
  {
    "text": "of the undo just as not so it doesn't mean there's no information available it just",
    "start": "745680",
    "end": "751519"
  },
  {
    "text": "didn't set an annotation by interrogating it we can actually see",
    "start": "751519",
    "end": "756720"
  },
  {
    "text": "that um the image changed as part of revision",
    "start": "756720",
    "end": "762800"
  },
  {
    "text": "number three right we did the undo so it's still possible to get the change cause just in the",
    "start": "762800",
    "end": "768560"
  },
  {
    "text": "change calls table it just says none so it does help to do some uh to use the",
    "start": "768560",
    "end": "774959"
  },
  {
    "text": "record flag in many cases okay so that's just kind of the default behavior",
    "start": "774959",
    "end": "780639"
  },
  {
    "text": "of rolling updates and interacting with them there are a couple",
    "start": "780639",
    "end": "786160"
  },
  {
    "text": "other features that we can leverage uh when it comes to rolling updates pausing so uh",
    "start": "786160",
    "end": "791920"
  },
  {
    "text": "we saw that once we introduced the change it then uh immediately triggered the rollout",
    "start": "791920",
    "end": "798399"
  },
  {
    "text": "right uh but uh deployments rollout trigger can actually be positive any given time so if you do it before",
    "start": "798399",
    "end": "806880"
  },
  {
    "text": "uh any sort of changes introduced then you can issue several commands to make different",
    "start": "806880",
    "end": "812839"
  },
  {
    "text": "changes and it won't trigger anything until you resume it and then",
    "start": "812839",
    "end": "818720"
  },
  {
    "text": "all the changes made prior to resuming are rolled out also in the middle you can",
    "start": "818720",
    "end": "825279"
  },
  {
    "text": "pause the rollout and do something like confirm settings before the whole thing completes",
    "start": "825279",
    "end": "832079"
  },
  {
    "text": "and so then in that case resuming just simply finishes the rollout that was stopped in progress",
    "start": "832079",
    "end": "837199"
  },
  {
    "text": "let's take a look at interacting that way so we've got our various",
    "start": "837199",
    "end": "843360"
  },
  {
    "text": "watches here we're going to keep the keep those established but in this case what we're going to do is first",
    "start": "843360",
    "end": "849440"
  },
  {
    "text": "uh we're going to pause so we're going to pause the deployment so that changes that we make are not",
    "start": "849440",
    "end": "854880"
  },
  {
    "text": "going to get pulled out right so let's introduce the alpine revision again",
    "start": "854880",
    "end": "862560"
  },
  {
    "text": "right in this case we're again updating the version to alpine and we're going to see nothing happens",
    "start": "862560",
    "end": "869199"
  },
  {
    "text": "right no rollout was triggered our client's still hitting the same uh demo",
    "start": "869199",
    "end": "875199"
  },
  {
    "text": "uh or sorry the same revision uh let's introduce a second change though just to show that that's happening so in",
    "start": "875199",
    "end": "882320"
  },
  {
    "text": "this case we're going to add cpu limit right so that will let us",
    "start": "882320",
    "end": "890000"
  },
  {
    "text": "set resources for that for the container right in this case the",
    "start": "890000",
    "end": "896839"
  },
  {
    "text": "container called host info let's scroll up real quick and make sure that it was called host info",
    "start": "896839",
    "end": "903519"
  },
  {
    "text": "yeah so the name of the container here is host info so this will let us set that again",
    "start": "903519",
    "end": "911360"
  },
  {
    "text": "nothing rolled out right now if i introduce the resume we should see all those changes roll out",
    "start": "911360",
    "end": "918399"
  },
  {
    "text": "at once right so we're going to get new containers again in the roll out",
    "start": "918399",
    "end": "924240"
  },
  {
    "text": "but not two rollouts right because we introduced two changes but because that the uh rollout was",
    "start": "924240",
    "end": "931040"
  },
  {
    "text": "paused everything that was introduced before the uh resume was issued",
    "start": "931040",
    "end": "936240"
  },
  {
    "text": "will be rolled out right so new let's see let's try a new",
    "start": "936240",
    "end": "944800"
  },
  {
    "text": "pod we'll do minus oh yaml and we should be",
    "start": "952839",
    "end": "958560"
  },
  {
    "text": "able to see resources so we've got the resources that was not in our original manifest",
    "start": "958560",
    "end": "965279"
  },
  {
    "text": "right now uh one couple of ways that we can control rolling updates is through the max origin max unavailable so we",
    "start": "965279",
    "end": "971680"
  },
  {
    "text": "have the default settings of 25 to 25 but uh what if we change those things right the",
    "start": "971680",
    "end": "977600"
  },
  {
    "text": "the max surge is something that controls the number of pods in addition to the desired number that can be",
    "start": "977600",
    "end": "984399"
  },
  {
    "text": "scheduled during the rolling update this essentially allows us to break up",
    "start": "984399",
    "end": "990480"
  },
  {
    "text": "the iterations into larger waves right we've seen in previous cases that there were a",
    "start": "990480",
    "end": "997600"
  },
  {
    "text": "small number of pods we can break this up a high search percentage however is going to mean more resources right because each",
    "start": "997600",
    "end": "1004399"
  },
  {
    "text": "application replica is going to need cpu and memory and so you can just need more of those during the rollout",
    "start": "1004399",
    "end": "1010800"
  },
  {
    "text": "max unavailable ensures that a minimum number of pods are always available right the idea being that we want to guarantee",
    "start": "1010800",
    "end": "1017759"
  },
  {
    "text": "that that client traffic is delivered through pods throughout the rolling update this cannot be zero we",
    "start": "1017759",
    "end": "1023120"
  },
  {
    "text": "have to have some value uh and if you do set it at a hundred percent that's going to",
    "start": "1023120",
    "end": "1030079"
  },
  {
    "text": "mean downtime right so let's try a couple uh scenarios uh using max surge and max",
    "start": "1030079",
    "end": "1037360"
  },
  {
    "text": "unavailable okay so we made a little edit there to expand the terminal a little bit so we can actually see",
    "start": "1037360",
    "end": "1044079"
  },
  {
    "text": "a little bit more so this first time around what we're going to do is patch the max surge",
    "start": "1044079",
    "end": "1051200"
  },
  {
    "text": "what we should expect then is that we have a lot more pods that will get added",
    "start": "1051200",
    "end": "1058559"
  },
  {
    "text": "on the roll out so we'll introduce another change that should then see the behavior",
    "start": "1058559",
    "end": "1066799"
  },
  {
    "text": "exhibit so that's why i expanded the terminal here so wow we see a lot more pods roll out",
    "start": "1066799",
    "end": "1072960"
  },
  {
    "text": "right so tons of pods get added immediately to the deployment",
    "start": "1072960",
    "end": "1079520"
  },
  {
    "text": "so that we have a big surge of pods that get added to our cluster",
    "start": "1079520",
    "end": "1087520"
  },
  {
    "text": "right away again load balancing across many replicas in this case",
    "start": "1087520",
    "end": "1092720"
  },
  {
    "text": "and then we tear down a whole bunch of them because we've got a ton running right so you can tear down",
    "start": "1092720",
    "end": "1099600"
  },
  {
    "text": "really quickly as well now let's do uh the opposite where we're going to patch again",
    "start": "1099600",
    "end": "1105520"
  },
  {
    "text": "except for this time we're going to set max unavailable to 90 to see how that affects again we",
    "start": "1105520",
    "end": "1110720"
  },
  {
    "text": "should expect the opposite where we don't have a lot of surge but we have a lot of",
    "start": "1110720",
    "end": "1117200"
  },
  {
    "text": "pods removed again we'll introduce another change and",
    "start": "1117200",
    "end": "1122640"
  },
  {
    "text": "see what happens there again hopefully that meets our expectations so we've got a lot of containers",
    "start": "1122640",
    "end": "1128080"
  },
  {
    "text": "terminating but then also because we need to replace them we've got a lot of containers that get created",
    "start": "1128080",
    "end": "1134320"
  },
  {
    "text": "right away and so in all the scenarios we see uh that our client up in the upper right",
    "start": "1134320",
    "end": "1140880"
  },
  {
    "text": "corner here uh is unaware of those changes across the board again um",
    "start": "1140880",
    "end": "1148799"
  },
  {
    "text": "because that's the benefit of the zero downtime rolling updates okay moving on to",
    "start": "1148799",
    "end": "1156400"
  },
  {
    "text": "staple sets so staple sets are intended to support staple applications uh these have some",
    "start": "1156400",
    "end": "1162559"
  },
  {
    "text": "other features regarding stable network identities so unlike deployment based pods staple sets have unique ordinals so",
    "start": "1162559",
    "end": "1169520"
  },
  {
    "text": "that we can actually communicate with individual members a little bit easier and persistent storage so there's an",
    "start": "1169520",
    "end": "1174960"
  },
  {
    "text": "embedded pvc template that creates a persistent volume for each pod now when we do the deletion deleting a",
    "start": "1174960",
    "end": "1181840"
  },
  {
    "text": "staple set will delete the pods but not the pvcs because what we want to do is make sure that it was a mistake right",
    "start": "1181840",
    "end": "1188320"
  },
  {
    "text": "what if i accidentally deleted my staple set and all my pvcs went with it well usually in the",
    "start": "1188320",
    "end": "1194000"
  },
  {
    "text": "scenario where the persistent volumes are dynamically provisioned the thing that keeps them from being",
    "start": "1194000",
    "end": "1200320"
  },
  {
    "text": "returned to the storage pool is the binding between the pvc and the pv so this allows us to make a deliberate",
    "start": "1200320",
    "end": "1208080"
  },
  {
    "text": "decision when we want to delete the pvcs making sure that um if we accidentally deleted this table",
    "start": "1208080",
    "end": "1213840"
  },
  {
    "text": "set we don't have any data loss uh the pod identities are tied to volumes",
    "start": "1213840",
    "end": "1219360"
  },
  {
    "text": "so failed pods are replaced by pods with identical identical identifiers which means that",
    "start": "1219360",
    "end": "1226000"
  },
  {
    "text": "existing volumes that were tied to a given pod replica uh easily bind back to that new",
    "start": "1226000",
    "end": "1233360"
  },
  {
    "text": "pod that represents that same replica right so even though it's a it's a new pod that's",
    "start": "1233360",
    "end": "1239200"
  },
  {
    "text": "replaced the old one it gets the same storage that the old pod had now uh the ordinals give us some features",
    "start": "1239200",
    "end": "1245760"
  },
  {
    "text": "ordered or sequential deployments in scaling right so you can see here we start with pod ordinal zero once it",
    "start": "1245760",
    "end": "1253120"
  },
  {
    "text": "passes its readiness check then we can move on to pod one once it passes its readiness check",
    "start": "1253120",
    "end": "1258320"
  },
  {
    "text": "we move on to pod two and so on and so forth when we scale out for initial deployments and then",
    "start": "1258320",
    "end": "1264080"
  },
  {
    "text": "when we add more replicas this allows us to have applications that",
    "start": "1264080",
    "end": "1269120"
  },
  {
    "text": "communicate with each other uh as peers right so what's the point of a readiness",
    "start": "1269120",
    "end": "1274720"
  },
  {
    "text": "jackets to indicate when the application is ready to receive requests on the network",
    "start": "1274720",
    "end": "1280320"
  },
  {
    "text": "and so when we have peer-to-peer communication we don't want to roll out a second replica until the first one is ready to communicate with",
    "start": "1280320",
    "end": "1286960"
  },
  {
    "text": "it so that they can join consensus algorithms and things like that no guarantee during termination",
    "start": "1286960",
    "end": "1292559"
  },
  {
    "text": "though you can see at the bottom here termination happens all at the same time if you need",
    "start": "1292559",
    "end": "1297760"
  },
  {
    "text": "an ordered termination uh the workaround is just to scale to zero first and then",
    "start": "1297760",
    "end": "1303120"
  },
  {
    "text": "delete the staple set and that will give you again it will tear down scaling down from the highest ordinal",
    "start": "1303120",
    "end": "1309039"
  },
  {
    "text": "down to zero uh for rolling updates pods are deleted and recreated or replaced on the same",
    "start": "1309039",
    "end": "1316320"
  },
  {
    "text": "node so unlike deployments where we're deploying applications uh application",
    "start": "1316320",
    "end": "1323280"
  },
  {
    "text": "replicas to potentially new workers um we're going to",
    "start": "1323280",
    "end": "1328400"
  },
  {
    "text": "deploy a new version of the application to the exact same marker this eliminates that need to detach and",
    "start": "1328400",
    "end": "1335440"
  },
  {
    "text": "attach network volumes from the old node to the to the new node right that could be a heavyweight operation rolling updates",
    "start": "1335440",
    "end": "1342960"
  },
  {
    "text": "will support uh the undo feature so all the imperatives we've been using with",
    "start": "1342960",
    "end": "1348720"
  },
  {
    "text": "deployments and the status commands the history is not super functional unfortunately and uh pause and resume is not supported",
    "start": "1348720",
    "end": "1355520"
  },
  {
    "text": "at all so uh it does have some overlap in terms of features for",
    "start": "1355520",
    "end": "1360720"
  },
  {
    "text": "the imperatives but not everything and occasionally you may run into a scenario where you",
    "start": "1360720",
    "end": "1366000"
  },
  {
    "text": "have to force a rollback so the top section here is again just a repeat of",
    "start": "1366000",
    "end": "1372080"
  },
  {
    "text": "the initial deployment right we go from zero to one to two to n and when we update we start at n and",
    "start": "1372080",
    "end": "1378320"
  },
  {
    "text": "work our way down right so we start the highest ordinal and the ordinal zero would be the last",
    "start": "1378320",
    "end": "1385039"
  },
  {
    "text": "replica to receive the update so uh let's look at a scenario first things",
    "start": "1385039",
    "end": "1391600"
  },
  {
    "text": "first let's get the storage classes just to show we've got two storage classes we've got the local storage and",
    "start": "1391600",
    "end": "1396960"
  },
  {
    "text": "then we've got an nfs client which is the default storage class we're going to use for the network attached",
    "start": "1396960",
    "end": "1404320"
  },
  {
    "text": "storage scenario in this case we'll take a look at the staple set although it's rather large",
    "start": "1404320",
    "end": "1410480"
  },
  {
    "text": "you'll see that the volume claim template isn't using a lot of data and it doesn't have a storage class request",
    "start": "1410480",
    "end": "1416159"
  },
  {
    "text": "so it's just using the default storage class there's a lot of logic here for uh starting it up and it being",
    "start": "1416159",
    "end": "1424880"
  },
  {
    "text": "um ncd again what we want to do is establish a watch so in this case",
    "start": "1424880",
    "end": "1431360"
  },
  {
    "text": "we're going to again use the label so in this case the app hcd net which is the network attached",
    "start": "1431360",
    "end": "1438720"
  },
  {
    "text": "storage version so again we're not going to find anything just yet let's go ahead and",
    "start": "1438720",
    "end": "1444320"
  },
  {
    "text": "apply our staple set and we've got our service and our staple",
    "start": "1444320",
    "end": "1450480"
  },
  {
    "text": "set created you're going to see the rollout right so initial rule app we start with",
    "start": "1450480",
    "end": "1456559"
  },
  {
    "text": "replica 0 and it'll go to 1 and up to 4. and while that's",
    "start": "1456720",
    "end": "1464320"
  },
  {
    "text": "going i'm going to start a client over here again in this case we're going to just run lcd",
    "start": "1464320",
    "end": "1470799"
  },
  {
    "text": "of the same version but uh just call it ctl and that's all we're going to use",
    "start": "1470799",
    "end": "1475919"
  },
  {
    "text": "is the ncd cuddle client here to run some things so i'm going to",
    "start": "1475919",
    "end": "1482640"
  },
  {
    "text": "export the correct version once the client gets up and running give that a moment",
    "start": "1482640",
    "end": "1489200"
  },
  {
    "text": "there we go so we're going to export the entity api version 3 and",
    "start": "1489200",
    "end": "1495679"
  },
  {
    "text": "also export the endpoints so we know that the endpoints are",
    "start": "1495679",
    "end": "1501279"
  },
  {
    "text": "0 through 4 and the service name so we'll export those",
    "start": "1501279",
    "end": "1509760"
  },
  {
    "text": "and then we get a member list just to see that they're all up and running we can see that one is just creating",
    "start": "1509760",
    "end": "1516880"
  },
  {
    "text": "so remember list should give us a member list of 0 through 4 there now i'm going to put a",
    "start": "1516880",
    "end": "1524000"
  },
  {
    "text": "value just to show that we can set and retrieve data and then we'll start a",
    "start": "1524000",
    "end": "1532320"
  },
  {
    "text": "loop so in this loop we're going to get the value that we set and then also",
    "start": "1532320",
    "end": "1539520"
  },
  {
    "text": "get the endpoint statuses to see the uh different versions that have been deployed",
    "start": "1539520",
    "end": "1544720"
  },
  {
    "text": "so we'll start that loop and you'll see that uh the value key and value are there and",
    "start": "1544720",
    "end": "1551440"
  },
  {
    "text": "then you've got the diversion each one so as we do roll out we'll get a sense of",
    "start": "1551440",
    "end": "1556640"
  },
  {
    "text": "the version so let's uh trigger the first update so in this case",
    "start": "1556640",
    "end": "1562320"
  },
  {
    "text": "we're going to update to just a minor revision and watch the rollout status",
    "start": "1562320",
    "end": "1571039"
  },
  {
    "text": "so what we'll see is it's um again just like the deployment update uh in terms of the updates that we're",
    "start": "1571039",
    "end": "1577760"
  },
  {
    "text": "getting from rollout status but you can see the",
    "start": "1577760",
    "end": "1584080"
  },
  {
    "text": "uh tear down happens from the ordinal four now on to ordinal three",
    "start": "1584080",
    "end": "1591120"
  },
  {
    "text": "and we'll go all the way down to zero now what you'll see over here with the client is that we're going to get some error messages because certain endpoints are",
    "start": "1591120",
    "end": "1598320"
  },
  {
    "text": "not going to give us their status but what you'll see every single time is that we're still getting the",
    "start": "1598320",
    "end": "1603440"
  },
  {
    "text": "data right because the ncd application supports that the various members can still",
    "start": "1603440",
    "end": "1610840"
  },
  {
    "text": "answer the query that we're sending them right we may not be able to query individual members for their",
    "start": "1610840",
    "end": "1617679"
  },
  {
    "text": "status because they're in the middle of being down but the data is always there right and that's the important part and",
    "start": "1617679",
    "end": "1623840"
  },
  {
    "text": "so as you see the clients answering over and over they are giving us the data every single time",
    "start": "1623840",
    "end": "1632080"
  },
  {
    "text": "right so we'll let that finish i'm going to queue up an undo",
    "start": "1632080",
    "end": "1639440"
  },
  {
    "text": "here just to again see how the undo compares between the staples that",
    "start": "1639440",
    "end": "1644799"
  },
  {
    "text": "controller and the uh the deployment so give that",
    "start": "1644799",
    "end": "1651279"
  },
  {
    "text": "a moment to finish now we're on to the very last one",
    "start": "1651279",
    "end": "1658080"
  },
  {
    "text": "and it's up and running so everything's been updated right and so the last two members here",
    "start": "1660000",
    "end": "1667279"
  },
  {
    "text": "in the very last query from our client didn't answer but now we're on to the fact that they're all answering again",
    "start": "1667279",
    "end": "1672880"
  },
  {
    "text": "and of course the data is still there so let's uh go back right we something's wrong with",
    "start": "1672880",
    "end": "1680240"
  },
  {
    "text": "this version so let's let's roll it back again so behavior again is the same where we're tearing down from the highest orbital",
    "start": "1680240",
    "end": "1686480"
  },
  {
    "text": "down which is what we expect right so in this case uh i didn't ask for live update status",
    "start": "1686480",
    "end": "1694720"
  },
  {
    "text": "so let's take a look at the history as this thing rolls out the or reverts the change rather",
    "start": "1694720",
    "end": "1702320"
  },
  {
    "text": "so um note that we",
    "start": "1702320",
    "end": "1708960"
  },
  {
    "text": "did on the first change right going back up here we did record it but",
    "start": "1708960",
    "end": "1715200"
  },
  {
    "text": "uh as we said the history is not very workable",
    "start": "1715200",
    "end": "1720880"
  },
  {
    "text": "or not very valuable the second change we didn't set a record we did an undo",
    "start": "1720880",
    "end": "1727039"
  },
  {
    "text": "and uh started to roll things back but still didn't get anything so again if i want to interrogate a",
    "start": "1727039",
    "end": "1733600"
  },
  {
    "text": "particular revision right i don't get a lot of information",
    "start": "1733600",
    "end": "1739279"
  },
  {
    "text": "here so the uh roll of history um not as robust as you would expect",
    "start": "1739279",
    "end": "1748080"
  },
  {
    "text": "that we got from the deployment we can even do uh partitioned updates because we",
    "start": "1748080",
    "end": "1755919"
  },
  {
    "text": "have uh ordinals we can rely on so initial deployment again is zero to n what we'll do here is",
    "start": "1755919",
    "end": "1765360"
  },
  {
    "text": "set a partition and see that everything equal to or above that partition number will roll out with the",
    "start": "1765360",
    "end": "1771760"
  },
  {
    "text": "new version but the old pods will keep the old version now even if someone comes along and",
    "start": "1771760",
    "end": "1777440"
  },
  {
    "text": "deletes a given pod it will stay the old version even if it",
    "start": "1777440",
    "end": "1782799"
  },
  {
    "text": "is deleted so uh let's take a look at that but what we're going to do is perform that with local storage",
    "start": "1782799",
    "end": "1790960"
  },
  {
    "text": "volumes and we still have the client running we'll just make some changes to which endpoints it's connecting to and",
    "start": "1790960",
    "end": "1797760"
  },
  {
    "text": "we want to start a new watch with the new uh app uh ncd local key value pair",
    "start": "1797760",
    "end": "1803679"
  },
  {
    "text": "uh the only difference between the old old network attached storage version",
    "start": "1803679",
    "end": "1809760"
  },
  {
    "text": "which uses the default storage class and this new one is",
    "start": "1809760",
    "end": "1814960"
  },
  {
    "text": "that it has the storage class name here so that we definitely get the local",
    "start": "1815120",
    "end": "1820640"
  },
  {
    "text": "storage class which we see if i can spell",
    "start": "1820640",
    "end": "1829360"
  },
  {
    "text": "we've got our watch established let's go ahead and deploy",
    "start": "1829360",
    "end": "1834399"
  },
  {
    "text": "our application and so now it's grabbed a local pv so if i do cube ctl get",
    "start": "1834399",
    "end": "1842480"
  },
  {
    "text": "pv you can see that we've got several being bound already",
    "start": "1842480",
    "end": "1847600"
  },
  {
    "text": "right so our level storage volumes are getting used now again we're going to export in",
    "start": "1847600",
    "end": "1853440"
  },
  {
    "text": "our client some endpoints but different names right sts local",
    "start": "1853440",
    "end": "1859360"
  },
  {
    "text": "and scs local is the service name",
    "start": "1859360",
    "end": "1865039"
  },
  {
    "text": "so i'll export those endpoints and again we'll do a put",
    "start": "1865039",
    "end": "1872320"
  },
  {
    "text": "and get the same endpoints uh that we did before",
    "start": "1873519",
    "end": "1881200"
  },
  {
    "text": "or the endpoint statuses and the value so we can see that what happens over time so",
    "start": "1882240",
    "end": "1888720"
  },
  {
    "text": "now we want to introduce a change but before we do that we want to also introduce a partition number so",
    "start": "1888720",
    "end": "1895760"
  },
  {
    "text": "that we update only some of them so we'll patch the rolling update strategy",
    "start": "1895760",
    "end": "1905039"
  },
  {
    "text": "so that it's partitioned actually let's go with number three right that way we get",
    "start": "1905039",
    "end": "1913279"
  },
  {
    "text": "a couple of replicas of the new version",
    "start": "1913279",
    "end": "1920320"
  },
  {
    "text": "okay so the staple set is patched now let's uh introduce the change again uh updating",
    "start": "1920320",
    "end": "1927840"
  },
  {
    "text": "the minor version to dot 2 4 and",
    "start": "1927840",
    "end": "1932880"
  },
  {
    "text": "witness the behavior that happens here so again we see that the version of",
    "start": "1932880",
    "end": "1940159"
  },
  {
    "text": "uh replica 4 changes the version of replica 3 is going to change",
    "start": "1940159",
    "end": "1946720"
  },
  {
    "text": "and then we stop right so we're going to see when the members",
    "start": "1946720",
    "end": "1952559"
  },
  {
    "text": "start to report their values over here that the last two are 24 and the other",
    "start": "1952559",
    "end": "1958080"
  },
  {
    "text": "three are 18. now let's see if we delete a pod so we'll come over",
    "start": "1958080",
    "end": "1963840"
  },
  {
    "text": "here and delete sts local one because we know it's uh version 18.",
    "start": "1963840",
    "end": "1971919"
  },
  {
    "text": "it's going to terminate that pod and it should allow us to",
    "start": "1972399",
    "end": "1977440"
  },
  {
    "text": "create a new one to replace it and once the client catches up and can query all the",
    "start": "1977440",
    "end": "1983760"
  },
  {
    "text": "endpoints once again we'll see that the version remains",
    "start": "1983760",
    "end": "1990080"
  },
  {
    "text": "18 right even though it was deleted the version comes back as the old version now um",
    "start": "1990080",
    "end": "1997279"
  },
  {
    "text": "let's say i'm doing some sort of phased rollout and i'm happy with the phased rollout uh now i can update my partition",
    "start": "1997279",
    "end": "2006399"
  },
  {
    "text": "to 0 and that will let me roll out the rest of that change so we",
    "start": "2006399",
    "end": "2014799"
  },
  {
    "text": "see that version 2 now updates itself version one",
    "start": "2014799",
    "end": "2019919"
  },
  {
    "text": "is or a replica one i should say is now gonna update itself and then we'll see go all the way down",
    "start": "2019919",
    "end": "2026320"
  },
  {
    "text": "to zero so we see the entirety of the rolling rolling updates happen",
    "start": "2026320",
    "end": "2032159"
  },
  {
    "text": "right so once we're done with our face rollout we uh everything updates and now everything's up to 3.24",
    "start": "2032159",
    "end": "2041919"
  },
  {
    "text": "once everything's up and running again our client throughout has availability to the data",
    "start": "2041919",
    "end": "2048800"
  },
  {
    "text": "that's what's important right we do have some errors communicated with individual members but",
    "start": "2048800",
    "end": "2055760"
  },
  {
    "text": "uh once they're back all up right we're all back to 3.2.24 and",
    "start": "2055760",
    "end": "2061280"
  },
  {
    "text": "the data is available so to sum up we see that kubernetes controllers",
    "start": "2061280",
    "end": "2066320"
  },
  {
    "text": "provide features that do give us zero downtime rolling updates and behaviors",
    "start": "2066320",
    "end": "2072720"
  },
  {
    "text": "differ by controller because applications are different whether they're stateless or stateful so hopefully that",
    "start": "2072720",
    "end": "2079520"
  },
  {
    "text": "gives you a sense of how the staple set and deployment controller",
    "start": "2079520",
    "end": "2084878"
  },
  {
    "text": "behave in practice through some of the demos that we did today just wanted to pop in one more time and",
    "start": "2084879",
    "end": "2090000"
  },
  {
    "text": "say thanks for your time",
    "start": "2090000",
    "end": "2094638"
  }
]