[
  {
    "start": "0",
    "end": "63000"
  },
  {
    "text": "all right I think we're live can you hear me okay well good",
    "start": "30",
    "end": "5160"
  },
  {
    "text": "welcome everyone to the intro to rook discussion I'm Travis Nielson I work for",
    "start": "5160",
    "end": "11849"
  },
  {
    "text": "Red Hat this is I'm a maintainer on the rook project and this is my colleague",
    "start": "11849",
    "end": "17850"
  },
  {
    "text": "Alexander I guess yeah I'm Alexander I'm working for cloud ability I'm 50 Travis and Jared and to",
    "start": "17850",
    "end": "26670"
  },
  {
    "text": "pass some one of the four main tenets of the project and well yeah okay so yeah",
    "start": "26670",
    "end": "36180"
  },
  {
    "text": "we'd like to tell you all about rook today just to get a sense of what you all know about rook who here has at",
    "start": "36180",
    "end": "42600"
  },
  {
    "text": "least tried running rook somewhere in kubernetes okay a bunch of you all right",
    "start": "42600",
    "end": "48570"
  },
  {
    "text": "so so glad to hear people were trying it out who just heard about rook in the keynote this morning for the first time",
    "start": "48570",
    "end": "54949"
  },
  {
    "text": "plenty of you too okay we got a good mix here so we would like to take you",
    "start": "54949",
    "end": "60090"
  },
  {
    "text": "through what rook is all about and show show you a demo well so we just released",
    "start": "60090",
    "end": "65400"
  },
  {
    "start": "63000",
    "end": "106000"
  },
  {
    "text": "the one dot Oh rook release we're excited about that it's been quite a journey so I'll be",
    "start": "65400",
    "end": "71100"
  },
  {
    "text": "happy to talk a little bit about that but ultimately we're gonna talk about storage what rook does for storage how it brings",
    "start": "71100",
    "end": "79229"
  },
  {
    "text": "it into the kubernetes ecosystem and makes it a more native part of the experience",
    "start": "79229",
    "end": "84409"
  },
  {
    "text": "well talk a bit through the rook design how it fits together but then the most exciting part hopefully is the demo",
    "start": "84409",
    "end": "90689"
  },
  {
    "text": "where it will show you a running cluster kubernetes cluster with rook and what it",
    "start": "90689",
    "end": "95700"
  },
  {
    "text": "takes to get going at the end how to get involved what other rook sessions we have will will have I think four",
    "start": "95700",
    "end": "102840"
  },
  {
    "text": "sessions tomorrow where rook will will be involved so yeah one of the biggest",
    "start": "102840",
    "end": "110850"
  },
  {
    "start": "106000",
    "end": "192000"
  },
  {
    "text": "parts dead especially like every time I open get up to calm slash rooks Luke I'm",
    "start": "110850",
    "end": "116460"
  },
  {
    "text": "always happy to see is that we do well we gain more stars and well stars are",
    "start": "116460",
    "end": "123030"
  },
  {
    "text": "kind of like the currency in how pop you're a lot of project is if you also and well we have 5.2 K and it's still",
    "start": "123030",
    "end": "131910"
  },
  {
    "text": "rising I thought we were already - free yeah we did the 1.0 release which",
    "start": "131910",
    "end": "138520"
  },
  {
    "text": "is great besides well ironically ruk already having like well stable support if you",
    "start": "138520",
    "end": "145630"
  },
  {
    "text": "will so for the staff part at least well we now have hit the 1.0 which for some",
    "start": "145630",
    "end": "152470"
  },
  {
    "text": "people it's kind of the line - we approach it has to go over but well we're happy to those people that came",
    "start": "152470",
    "end": "158800"
  },
  {
    "text": "because of the 1.0 especially and well we have over 150 plus contributors and",
    "start": "158800",
    "end": "166800"
  },
  {
    "text": "it's well it's great to see especially for some community provided starch",
    "start": "166800",
    "end": "172410"
  },
  {
    "text": "operators like from the at Rufus caste from mix anta they have done a great job",
    "start": "172410",
    "end": "179530"
  },
  {
    "text": "for example their part we have to Cassandra live with Yanis it's it's",
    "start": "179530",
    "end": "187180"
  },
  {
    "text": "great to see the community kind of step up there and help the whole root project there and yeah it's amazing to see I",
    "start": "187180",
    "end": "194700"
  },
  {
    "text": "think's Alex just it really is amazing to see the growth of the ecosystem and what the power of an open-source project",
    "start": "194700",
    "end": "201340"
  },
  {
    "text": "can do with especially when the CN CF sees the value and it is helping us back it so we're excited",
    "start": "201340",
    "end": "208030"
  },
  {
    "start": "208000",
    "end": "246000"
  },
  {
    "text": "so this journey so work was first open sourced actually a coop con Seattle",
    "start": "208030",
    "end": "214480"
  },
  {
    "text": "November 2016 it was a little bit smaller about a thousand people were there on that occasion nobody had ever",
    "start": "214480",
    "end": "220600"
  },
  {
    "text": "heard of Brooke even at the conferences no talks or anything so we had ODOT one release with that open source event and",
    "start": "220600",
    "end": "228940"
  },
  {
    "text": "then since then we've released roughly a quarterly cadence until now we're at the",
    "start": "228940",
    "end": "235780"
  },
  {
    "text": "1.0 and it's just we're gonna keep on going on this cadence and yeah just",
    "start": "235780",
    "end": "241269"
  },
  {
    "text": "having fun delivering features bringing storage to kubernetes so so let's talk about what",
    "start": "241269",
    "end": "247390"
  },
  {
    "start": "246000",
    "end": "299000"
  },
  {
    "text": "storage involves with kubernetes for a minute so traditionally kubernetes applications",
    "start": "247390",
    "end": "253060"
  },
  {
    "text": "we assume they're stateless they they're designed to just it's running here kill",
    "start": "253060",
    "end": "258549"
  },
  {
    "text": "it run it over there great we treat them as cattle and everybody's happy but what about the applications who need storage",
    "start": "258549",
    "end": "265440"
  },
  {
    "text": "traditionally you know you by your storage appliance you plug it in somewhere else you manage it somewhere",
    "start": "265440",
    "end": "271030"
  },
  {
    "text": "else and it's outside of kubernetes then there's this volume plug-in model for",
    "start": "271030",
    "end": "278560"
  },
  {
    "text": "kubernetes that lets you attach your storage that's external to kubernetes",
    "start": "278560",
    "end": "283920"
  },
  {
    "text": "whether it be you know if you're running in the cloud Azure Amazon if you're",
    "start": "283920",
    "end": "289360"
  },
  {
    "text": "running on premise maybe you've got staff or Gluster running but that's the",
    "start": "289360",
    "end": "295090"
  },
  {
    "text": "traditional model for working with storage with with kubernetes ok so this",
    "start": "295090",
    "end": "300160"
  },
  {
    "start": "299000",
    "end": "351000"
  },
  {
    "text": "comes with certain challenges of course the storage is this separate animal that's on the side you can't treat it",
    "start": "300160",
    "end": "307150"
  },
  {
    "text": "just like your other kubernetes applications well we thought well why is that why can't we just treat storage as",
    "start": "307150",
    "end": "312970"
  },
  {
    "text": "another application in your cluster there's a it's a separate deployment",
    "start": "312970",
    "end": "318580"
  },
  {
    "text": "burden we don't want it to be separate we want to be the same same model you might have bender lock-in right right",
    "start": "318580",
    "end": "325480"
  },
  {
    "text": "these storage appliances you know lock you into that solution it you have a",
    "start": "325480",
    "end": "330760"
  },
  {
    "text": "different solution whether you're running on premise or in the cloud or wherever so you know the question to ask",
    "start": "330760",
    "end": "337690"
  },
  {
    "text": "ourselves is how do we get a consistent story across all clouds or on-premise and how do we get storage that follows",
    "start": "337690",
    "end": "345040"
  },
  {
    "text": "the same pattern in the same solution whether or wherever kubernetes is running ok",
    "start": "345040",
    "end": "353020"
  },
  {
    "start": "351000",
    "end": "384000"
  },
  {
    "text": "so this is where rook comes in with rook what we did is we've created a set of",
    "start": "353020",
    "end": "359130"
  },
  {
    "text": "operators that will run storage applications in kubernetes and we'll",
    "start": "359130",
    "end": "366160"
  },
  {
    "text": "talk about what those specific operators do but really there are two different categories that that these operators and",
    "start": "366160",
    "end": "374320"
  },
  {
    "text": "what Rick will do for you first of all it's about automating the deployment and management of the storage for you so",
    "start": "374320",
    "end": "384580"
  },
  {
    "start": "384000",
    "end": "448000"
  },
  {
    "text": "we've you need to deploy the storage you know you you start up you bootstrap it",
    "start": "384580",
    "end": "389920"
  },
  {
    "text": "you configure it you need to upgrade it over time this is a whole set of problems which you want to make sure is",
    "start": "389920",
    "end": "396220"
  },
  {
    "text": "automated and you don't want to have to manage any differently from other",
    "start": "396220",
    "end": "401830"
  },
  {
    "text": "urban A's applications okay so that's the first thing managing the storage you",
    "start": "401830",
    "end": "407420"
  },
  {
    "text": "need a management layer for with that works well with kubernetes the second",
    "start": "407420",
    "end": "412430"
  },
  {
    "text": "thing is really i'm provisioning of the storage that means using the kubernetes",
    "start": "412430",
    "end": "417460"
  },
  {
    "text": "volume plug-in model to attach the storage with PBC's so that mainly",
    "start": "417460",
    "end": "423680"
  },
  {
    "text": "involves right you've got a PVC you you've got your pod that needs to consume the storage so you declare your",
    "start": "423680",
    "end": "430370"
  },
  {
    "text": "PVC rook binds it then to the Ceph storage and attaches it with the volume",
    "start": "430370",
    "end": "437570"
  },
  {
    "text": "plugin so even though the storage is running inside your kubernetes cluster rook can attach it with just the same",
    "start": "437570",
    "end": "444740"
  },
  {
    "text": "volume plug-in model as we've seen elsewhere it's a little more about rook",
    "start": "444740",
    "end": "450530"
  },
  {
    "start": "448000",
    "end": "473000"
  },
  {
    "text": "it is fully open source Apache 2.0 license and it is an incubation project",
    "start": "450530",
    "end": "457070"
  },
  {
    "text": "for the CN CF as as you saw on stage this morning it extends kubernetes with",
    "start": "457070",
    "end": "464150"
  },
  {
    "text": "operators with custom resource definitions and it is a framework for",
    "start": "464150",
    "end": "469960"
  },
  {
    "text": "bringing many storage providers in solutions so which storage providers are we",
    "start": "469960",
    "end": "476000"
  },
  {
    "start": "473000",
    "end": "662000"
  },
  {
    "text": "talking about where we started the project was with was with SEF and that's",
    "start": "476000",
    "end": "481460"
  },
  {
    "text": "why we were happy to announce in ro dot 9 release so back in December at cube",
    "start": "481460",
    "end": "487520"
  },
  {
    "text": "con that we declared SEF as stable as part of rook ok so that's now SEF itself",
    "start": "487520",
    "end": "494840"
  },
  {
    "text": "has been in production for a long time but the integration with kubernetes we declared is stable edge FS is another",
    "start": "494840",
    "end": "503690"
  },
  {
    "text": "operator that recently has been integrated with rook and in the 1.0 release was just declared as beta so go",
    "start": "503690",
    "end": "511160"
  },
  {
    "text": "check that out and see what it's capable of that you know other storage service is similar to Seth",
    "start": "511160",
    "end": "517750"
  },
  {
    "text": "Cassandra cockroach DB Mineo NFS all of these are in alpha some of them I've",
    "start": "517750",
    "end": "525650"
  },
  {
    "text": "been in rook for a couple releases it's just about community involvement it really depend on the community",
    "start": "525650",
    "end": "532300"
  },
  {
    "text": "to get involved and whatever storage they need to come get involved and help",
    "start": "532300",
    "end": "539050"
  },
  {
    "text": "us get these things to stable in the rook project so yeah anything else the",
    "start": "539050",
    "end": "546070"
  },
  {
    "text": "storage providers well besides study as we already had said earlier like with Cassandra and so",
    "start": "546070",
    "end": "551440"
  },
  {
    "text": "there's been amazing work for a new community it's we also have some other storage softwares have come up and hey",
    "start": "551440",
    "end": "558040"
  },
  {
    "text": "can we integrate in some way with the group there so again it's amazing to see",
    "start": "558040",
    "end": "564070"
  },
  {
    "text": "the community there oh yeah so if you know about their storage solutions that",
    "start": "564070",
    "end": "569680"
  },
  {
    "text": "would make sense as that need to be automated with kubernetes used operators",
    "start": "569680",
    "end": "575170"
  },
  {
    "text": "yeah we'd like to talk and get you integrated as well so what are these",
    "start": "575170",
    "end": "580330"
  },
  {
    "text": "operators that we're talking about this pattern you may have heard about it for the last couple of years or maybe it's",
    "start": "580330",
    "end": "586030"
  },
  {
    "text": "completely new at the end of the day it's about taking what you need to deploy and creating automation around it",
    "start": "586030",
    "end": "594910"
  },
  {
    "text": "and do it in a way that it looks like any other kubernetes application and",
    "start": "594910",
    "end": "600070"
  },
  {
    "text": "custom resource definitions are the way that the kubernetes allows us to do this",
    "start": "600070",
    "end": "605080"
  },
  {
    "text": "so an operator will sit in what's called the control loop and it watches and it",
    "start": "605080",
    "end": "611170"
  },
  {
    "text": "says what would you like to do oh you'd like to create this type of thing a like",
    "start": "611170",
    "end": "616390"
  },
  {
    "text": "a Seth cluster okay I'll go create that now with the settings that you've described in your manifests or this",
    "start": "616390",
    "end": "623620"
  },
  {
    "text": "llam√≥ file which which just like you define other pods and deployments and anything else in kubernetes so as it",
    "start": "623620",
    "end": "631000"
  },
  {
    "text": "observes what you your desire to happen in your cluster the operator goes and makes that happen and automation is a",
    "start": "631000",
    "end": "639280"
  },
  {
    "text": "good thing and the other effect it has is it simplifies your life as an",
    "start": "639280",
    "end": "644620"
  },
  {
    "text": "administrator in the cluster you you just don't have to worry about all of the individual storage settings for your",
    "start": "644620",
    "end": "651400"
  },
  {
    "text": "storage you can let rook take care of that for you apply some smart defaults",
    "start": "651400",
    "end": "656800"
  },
  {
    "text": "but then you can also override those defaults and change those settings when you need to see RDS I guess I already",
    "start": "656800",
    "end": "665900"
  },
  {
    "start": "662000",
    "end": "840000"
  },
  {
    "text": "it's all about what do you want what settings do you want and then the operator makes it happen okay and so",
    "start": "665900",
    "end": "681110"
  },
  {
    "text": "watching yeah let's skip over this all right so the operators leverage the full power of",
    "start": "681110",
    "end": "687380"
  },
  {
    "text": "kubernetes that's where we really took a bet early on in in Rooke where we said",
    "start": "687380",
    "end": "693170"
  },
  {
    "text": "we believe kubernetes is going to grow and amazingly it's you know it's been amazing that way and let's build",
    "start": "693170",
    "end": "700760"
  },
  {
    "text": "everything around kubernetes so that's the only storage provider or sorry the only orchestration platform that we",
    "start": "700760",
    "end": "709100"
  },
  {
    "text": "cater to it's it's all about kubernetes use generate services use replica sets",
    "start": "709100",
    "end": "715250"
  },
  {
    "text": "or demon sets deployment secrets all of these things the operator talks to the kubernetes api to apply these settings",
    "start": "715250",
    "end": "723490"
  },
  {
    "text": "we don't worry at our level about like what now do you have docker Ryan do you",
    "start": "723490",
    "end": "732320"
  },
  {
    "text": "have you know what's underneath the covers you have system D you know it's it's out of our hands it's just standard",
    "start": "732320",
    "end": "739310"
  },
  {
    "text": "kubernetes I also note that the rook operators are not on the data path so",
    "start": "739310",
    "end": "745100"
  },
  {
    "text": "the operators are just the management plane and whatever the storage provider is under the covers that's what provides",
    "start": "745100",
    "end": "751400"
  },
  {
    "text": "your data whether it's the database you know they're serving the data directly or at the the file object or block layer",
    "start": "751400",
    "end": "758959"
  },
  {
    "text": "you know talking directly your PVCs talked to them at that that layer okay",
    "start": "758959",
    "end": "766220"
  },
  {
    "text": "so to show a picture of this what we have is on the left your standard coop",
    "start": "766220",
    "end": "772250"
  },
  {
    "text": "cuddle tool where whatever you want to create you say or whatever you want to",
    "start": "772250",
    "end": "777589"
  },
  {
    "text": "know about the cluster you're you're creating it you're querying it so with rook again you're just gonna be able to",
    "start": "777589",
    "end": "783860"
  },
  {
    "text": "say coop cuddle create your llamo file your rook cluster and same tools you're",
    "start": "783860",
    "end": "790850"
  },
  {
    "text": "already using and the operator will work on that so now the operators on the",
    "start": "790850",
    "end": "796520"
  },
  {
    "text": "right so we have one Operator for each storage provider so we need more pictures here we've only",
    "start": "796520",
    "end": "802900"
  },
  {
    "text": "got a few of them so it's like SEF Cassandra edge FS all of them they each have their own operator and then they",
    "start": "802900",
    "end": "811150"
  },
  {
    "text": "have the drivers with that work with the rook agent or CSI to attack your storage",
    "start": "811150",
    "end": "816940"
  },
  {
    "text": "with the PVC so kind of yeah all coming",
    "start": "816940",
    "end": "823270"
  },
  {
    "text": "together overall picture is operators talk to the kubernetes api to create",
    "start": "823270",
    "end": "830950"
  },
  {
    "text": "these resources and use the CSI or other drivers to attach the storage from your",
    "start": "830950",
    "end": "838570"
  },
  {
    "text": "pods an example with SEF so i do work",
    "start": "838570",
    "end": "843850"
  },
  {
    "start": "840000",
    "end": "945000"
  },
  {
    "text": "with the SEF team at Red Hat so that's what would I'll be focusing on for the demo today tomorrow and the rook",
    "start": "843850",
    "end": "850960"
  },
  {
    "text": "deep-dive definitely come back for that there will be more talk by other maintainer is about the other storage",
    "start": "850960",
    "end": "857170"
  },
  {
    "text": "providers that rook has but first SEF as an example there are there a bunch of",
    "start": "857170",
    "end": "864370"
  },
  {
    "text": "settings that we need to know about your cluster what version of SEF do you want to run because that's a different that's",
    "start": "864370",
    "end": "870910"
  },
  {
    "text": "a different version from what version of rook how many SEF monitors do you want",
    "start": "870910",
    "end": "876190"
  },
  {
    "text": "to run you know the smart thing is well you need at least three so you can have",
    "start": "876190",
    "end": "881380"
  },
  {
    "text": "a quorum do you want it to run with host network for storage do you want to use",
    "start": "881380",
    "end": "886780"
  },
  {
    "text": "all your nodes anyway and so when you create this Y Amal its manifest rook",
    "start": "886780",
    "end": "893890"
  },
  {
    "text": "will go out to it'll say create pods you know create the Mons create the OS DS",
    "start": "893890",
    "end": "901800"
  },
  {
    "text": "all these if you're familiar with Seth you'll recognize these if not just just",
    "start": "901800",
    "end": "906940"
  },
  {
    "text": "know that there are these storage demons being started such that the storage",
    "start": "906940",
    "end": "913839"
  },
  {
    "text": "platform will be available to consume from your PBC's and and in this picture",
    "start": "913839",
    "end": "919300"
  },
  {
    "text": "note that there's really only the rook operator which is the little blue castle",
    "start": "919300",
    "end": "924490"
  },
  {
    "text": "there there's only one or a few of these pods running the rest are really the",
    "start": "924490",
    "end": "930220"
  },
  {
    "text": "storage layers demons so the safety in this case running where there's not",
    "start": "930220",
    "end": "935709"
  },
  {
    "text": "any rook in that code path all right so you you create your yeah Mille you get",
    "start": "935709",
    "end": "942610"
  },
  {
    "text": "pods and other services we kind of",
    "start": "942610",
    "end": "948579"
  },
  {
    "start": "945000",
    "end": "979000"
  },
  {
    "text": "touched on this already so Rick is a framework for other storage solutions and the what we get in common we really",
    "start": "948579",
    "end": "957459"
  },
  {
    "text": "get patterns around how to create or there's some common code that helps that",
    "start": "957459",
    "end": "962559"
  },
  {
    "text": "helps these operators work with the different kubernetes api's there's a common testing effort because you need",
    "start": "962559",
    "end": "969100"
  },
  {
    "text": "you need reliable storage you need stories that's durable so the testing",
    "start": "969100",
    "end": "974319"
  },
  {
    "text": "framework really helps make sure that that things are well tested before release all right so let's go to the",
    "start": "974319",
    "end": "981790"
  },
  {
    "start": "979000",
    "end": "1397000"
  },
  {
    "text": "demo I'll switch over here and all right",
    "start": "981790",
    "end": "991319"
  },
  {
    "text": "is it visible for you guys at the back from sauce is it good I think I need to",
    "start": "991439",
    "end": "996459"
  },
  {
    "text": "adjust some things that the things",
    "start": "996459",
    "end": "1002339"
  },
  {
    "text": "changed with the display here sorry should have done this before",
    "start": "1002339",
    "end": "1008389"
  },
  {
    "text": "alright I'm gonna make this a bit bigger now what I have here is so some",
    "start": "1014190",
    "end": "1022750"
  },
  {
    "text": "colleagues that Red Hat have created a nice demo environment that makes it easy to spin up a cluster to show it is an",
    "start": "1022750",
    "end": "1029530"
  },
  {
    "text": "open shift cluster but I'm gonna use kubernetes commands as much as I can since it's pretty much the same thing",
    "start": "1029530",
    "end": "1037600"
  },
  {
    "text": "for my perspective now the SSH did disconnect I need to SSH into it because",
    "start": "1037600",
    "end": "1043329"
  },
  {
    "text": "it is running in the cloud so you can all see my password now to SSH in here",
    "start": "1043330",
    "end": "1051210"
  },
  {
    "text": "I'm gonna destroy it right after this so I'm not too worried hopefully nobody connects and disturbs the demo while I'm",
    "start": "1051210",
    "end": "1058810"
  },
  {
    "text": "going alright well the whole thing a sly streamed so be fast okay no all right",
    "start": "1058810",
    "end": "1076600"
  },
  {
    "text": "let's make sure what we got here so coop Cod all get nodes just to see if we're connected alright so in our cluster we",
    "start": "1076600",
    "end": "1084070"
  },
  {
    "text": "have well we have four worker nodes a couple of masters so what I've what I've",
    "start": "1084070",
    "end": "1090550"
  },
  {
    "text": "done is I've pre-configured the yamo files our manifests so that will start storage on and consume the storage on",
    "start": "1090550",
    "end": "1097390"
  },
  {
    "text": "three of these nodes okay go over here",
    "start": "1097390",
    "end": "1102720"
  },
  {
    "text": "clear alright now the let's see alright so",
    "start": "1102720",
    "end": "1110890"
  },
  {
    "text": "I've got it these yellow files I'll create them one by one and describe as I go so coop",
    "start": "1110890",
    "end": "1116110"
  },
  {
    "text": "cuddle create common is that big enough for should I make it bigger okay okay so",
    "start": "1116110",
    "end": "1124150"
  },
  {
    "text": "common dot y Amal when I hit create it's going to create the the security",
    "start": "1124150",
    "end": "1129880"
  },
  {
    "text": "settings that are back all these service service accounts that rook needs rook",
    "start": "1129880",
    "end": "1136120"
  },
  {
    "text": "needs to have privileges to run pods privileged for example because the",
    "start": "1136120",
    "end": "1141280"
  },
  {
    "text": "operator needs to start these demons which consume local devices you",
    "start": "1141280",
    "end": "1146290"
  },
  {
    "text": "real hardware on the nodes so we do run some pods privileged for that purpose so",
    "start": "1146290",
    "end": "1153010"
  },
  {
    "text": "we've got this now we don't have we",
    "start": "1153010",
    "end": "1158170"
  },
  {
    "text": "don't have any pods yet just to confirm all right no research is found so now",
    "start": "1158170",
    "end": "1163630"
  },
  {
    "text": "I'm going to go ahead and create the operator create operator and before and",
    "start": "1163630",
    "end": "1170590"
  },
  {
    "text": "this is openshift there are several settings specific to OpenShift but before i do that let me say watch",
    "start": "1170590",
    "end": "1177030"
  },
  {
    "text": "Kubb cuddle get hot all right now",
    "start": "1177030",
    "end": "1183970"
  },
  {
    "text": "hopefully when I create the operator will see the pod start to appear all right there we go so the operator is the",
    "start": "1183970",
    "end": "1190150"
  },
  {
    "text": "first pod to start and immediately as soon as it starts it's going to create several other pods on our storage nodes",
    "start": "1190150",
    "end": "1197940"
  },
  {
    "text": "so that we can detect what storage devices are available and also start an",
    "start": "1197940",
    "end": "1205870"
  },
  {
    "text": "agent which will help us attach the storage later this this is currently",
    "start": "1205870",
    "end": "1211150"
  },
  {
    "text": "using the Flex driver for a rook which will attach the SEF Ceph storage all",
    "start": "1211150",
    "end": "1216280"
  },
  {
    "text": "right so all of these pods are running now we have have our agents we have our operator and the operator is just",
    "start": "1216280",
    "end": "1221680"
  },
  {
    "text": "waiting for us to tell us what kind of SEF cluster to create so let's go ahead",
    "start": "1221680",
    "end": "1227740"
  },
  {
    "text": "and do that so if I say coop cuddle create cluster dot yeah Mel so this is",
    "start": "1227740",
    "end": "1235780"
  },
  {
    "text": "the yeah Mel that has all the settings I show it on a couple slides ago that tell it what version of SEF to run so the",
    "start": "1235780",
    "end": "1242860"
  },
  {
    "text": "first thing it does is detect what version of SEF do you want to run in this case is going to detect SEF",
    "start": "1242860",
    "end": "1249220"
  },
  {
    "text": "Nautilus which was just released a couple months ago the the next thing is going to do it's",
    "start": "1249220",
    "end": "1255670"
  },
  {
    "text": "going to create some selfies save them as secrets in kubernetes then it's going",
    "start": "1255670",
    "end": "1261790"
  },
  {
    "text": "to start creating these SEF monitors ok who here has has deployed SEF",
    "start": "1261790",
    "end": "1267250"
  },
  {
    "text": "independently from Brooke anybody so you probably know it's it's not very easy or",
    "start": "1267250",
    "end": "1273790"
  },
  {
    "text": "at least not as easy as this hopefully that's why we created rook to make it easy but you need",
    "start": "1273790",
    "end": "1280210"
  },
  {
    "text": "monitors so it's going through one by one it's making sure they're in quorum each step of the way after we have three",
    "start": "1280210",
    "end": "1287830"
  },
  {
    "text": "monitors so just in a few more seconds here so we see okay we have Mon a Mon a",
    "start": "1287830",
    "end": "1294070"
  },
  {
    "text": "Mon be and Mon C so once those are all in quorum then it will create the Ceph",
    "start": "1294070",
    "end": "1300400"
  },
  {
    "text": "manager as the next pod that will initialize some SEF manager plugins and",
    "start": "1300400",
    "end": "1306520"
  },
  {
    "text": "modules and then we'll get our oh s DS which are the storage daemons that will",
    "start": "1306520",
    "end": "1313150"
  },
  {
    "text": "really create the storage platform that staff is going to serve up from from",
    "start": "1313150",
    "end": "1319810"
  },
  {
    "text": "each of the nodes and make it look like this the software-defined storage system",
    "start": "1319810",
    "end": "1325260"
  },
  {
    "text": "okay so we have the manager now while that's going so we'll see that OSD start",
    "start": "1325260",
    "end": "1331360"
  },
  {
    "text": "to come in up in a minute I'm gonna create a couple of other resources that will help us I see some things in the",
    "start": "1331360",
    "end": "1338590"
  },
  {
    "text": "cluster okay so the OA STIs are coming we have what's called a tool box pod so",
    "start": "1338590",
    "end": "1346840"
  },
  {
    "text": "what this pod is is it's a convenient place to go run any set command you want so it anybody knows that yeah sometimes",
    "start": "1346840",
    "end": "1353740"
  },
  {
    "text": "you need to go run stuff commands so the toolbox will let us now connect see SEF",
    "start": "1353740",
    "end": "1360190"
  },
  {
    "text": "status CF o s DS dad is anything like that okay now that we have the toolbox",
    "start": "1360190",
    "end": "1367960"
  },
  {
    "text": "we have our Oh s DS yeah let's go back to OS DS so we see right here that we",
    "start": "1367960",
    "end": "1373000"
  },
  {
    "text": "have three OSD daemon pods they're running on each of our three nodes okay",
    "start": "1373000",
    "end": "1380260"
  },
  {
    "text": "they they're ready ready to go so let's connect to the toolbox now and see the",
    "start": "1380260",
    "end": "1386140"
  },
  {
    "text": "status so we're going to say for convenience I'm going to connect with OC",
    "start": "1386140",
    "end": "1393040"
  },
  {
    "text": "to the toolbox all right I need the pod name",
    "start": "1393040",
    "end": "1398370"
  },
  {
    "text": "okay so we're inside the rock toolbox now and we can run Ceph status and we",
    "start": "1399390",
    "end": "1407380"
  },
  {
    "text": "see the cluster is health okay there are 3 mons 3 OS DS",
    "start": "1407380",
    "end": "1413500"
  },
  {
    "text": "we don't have any pools yet so but we have a basically healthy",
    "start": "1413500",
    "end": "1418810"
  },
  {
    "text": "cluster so now I'm going to go back and it let's so that now to consume the",
    "start": "1418810",
    "end": "1424270"
  },
  {
    "text": "storage the second part of rook we need to create a storage class let me just",
    "start": "1424270",
    "end": "1430090"
  },
  {
    "text": "show you what the storage class looks like so along with a storage class we",
    "start": "1430090",
    "end": "1435280"
  },
  {
    "text": "have a safe block pool and in that pool the interesting thing is you can tell it",
    "start": "1435280",
    "end": "1441310"
  },
  {
    "text": "how many replicas of your data do you want in the cluster or if you're familiar with erasure coding it can",
    "start": "1441310",
    "end": "1448150"
  },
  {
    "text": "configure erasure coding as part of that pool so in the for this demo I said",
    "start": "1448150",
    "end": "1453760"
  },
  {
    "text": "let's create two copies of the data okay for the storage class which is in the",
    "start": "1453760",
    "end": "1459850"
  },
  {
    "text": "same amyl we tell it okay my rook cluster is in this namespace when when",
    "start": "1459850",
    "end": "1466540"
  },
  {
    "text": "you mount the storage use XFS oh and I skipped over the block pool and the pool",
    "start": "1466540",
    "end": "1472200"
  },
  {
    "text": "that this storage class should use is the replica pool which was the pool I just showed you with replicas - okay so",
    "start": "1472200",
    "end": "1481300"
  },
  {
    "text": "let me exit this and go ahead and create that storage class so now I have the",
    "start": "1481300",
    "end": "1489100"
  },
  {
    "text": "storage class let me just show that and get storage class and we have have this",
    "start": "1489100",
    "end": "1496000"
  },
  {
    "text": "thing called accept luck okay so we're now in a position where we can create a",
    "start": "1496000",
    "end": "1501820"
  },
  {
    "text": "PVC to go consume it so a standard kind",
    "start": "1501820",
    "end": "1506890"
  },
  {
    "text": "of demo that we have is and in our QuickStart guide is to go start a wordpress application just a really",
    "start": "1506890",
    "end": "1512530"
  },
  {
    "text": "simple thing and it needs a my sequel database so that it can have persistent",
    "start": "1512530",
    "end": "1517810"
  },
  {
    "text": "storage now let me just show quickly what that looks like in the yeah mol and",
    "start": "1517810",
    "end": "1524710"
  },
  {
    "text": "show where the PBC is let's see so in this my sequel yeah well we see a",
    "start": "1524710",
    "end": "1531910"
  },
  {
    "text": "definition for a persistent ball you persistent volume claim and we're",
    "start": "1531910",
    "end": "1537160"
  },
  {
    "text": "requesting simple 20 gigabytes and read/write read/write once and it comes",
    "start": "1537160",
    "end": "1542500"
  },
  {
    "text": "from this storage class rook set block that we just created okay and then later in this you can see",
    "start": "1542500",
    "end": "1549020"
  },
  {
    "text": "the pod definition that uses this this PVC so that you can mount it in into the",
    "start": "1549020",
    "end": "1554510"
  },
  {
    "text": "my sequel pod so with that let's go ahead and and create create - yeah Milus",
    "start": "1554510",
    "end": "1563870"
  },
  {
    "text": "I've got a my sequel I'll just go ahead and create the other one and we've got WordPress okay and at the bottom",
    "start": "1563870",
    "end": "1572990"
  },
  {
    "text": "well these may scroll off let me make this bigger so we've got two pods that are starting up and as soon as the that",
    "start": "1572990",
    "end": "1580820"
  },
  {
    "text": "storage that PVC is bound and and mounted we'll see them in running state okay so now we have WordPress running in",
    "start": "1580820",
    "end": "1589610"
  },
  {
    "text": "this state where it's ready and available now the WordPress does take a",
    "start": "1589610",
    "end": "1595370"
  },
  {
    "text": "couple of minutes to startup initialize the database and things so in a couple",
    "start": "1595370",
    "end": "1601220"
  },
  {
    "text": "of minutes I'll open it up in browser and we'll double check that it's actually running but that's basically in",
    "start": "1601220",
    "end": "1607880"
  },
  {
    "text": "a nutshell so you've got storage orchestrated by rook you know it started",
    "start": "1607880",
    "end": "1613430"
  },
  {
    "text": "asset cluster it then allowed you to create but define the PBC which mounts",
    "start": "1613430",
    "end": "1620630"
  },
  {
    "text": "from the storage class and gives you this storage all right so this well",
    "start": "1620630",
    "end": "1628040"
  },
  {
    "text": "that's going well we will see if I get the services so the WordPress app",
    "start": "1628040",
    "end": "1634820"
  },
  {
    "text": "creates a load balancer service that will be at this URL since it's running",
    "start": "1634820",
    "end": "1644120"
  },
  {
    "text": "an AWS it just gives me this nice URL and let's see where did my browser go oh",
    "start": "1644120",
    "end": "1651470"
  },
  {
    "text": "I'm in presentation mode so now this probably won't work yet because",
    "start": "1651470",
    "end": "1657380"
  },
  {
    "text": "WordPress is still initializing but we'll come back to in a minute and and talk about that so why don't we go back",
    "start": "1657380",
    "end": "1664040"
  },
  {
    "text": "to the slides for just a couple of minutes give that a minute to go and we'll go from there",
    "start": "1664040",
    "end": "1670150"
  },
  {
    "text": "let's if I can find the present button all",
    "start": "1670150",
    "end": "1676290"
  },
  {
    "text": "right Alex so we have seen right now that it's pretty easy to well especially",
    "start": "1676290",
    "end": "1684059"
  },
  {
    "text": "in a test environment creators have class two for storage but kind of thing",
    "start": "1684059",
    "end": "1689429"
  },
  {
    "text": "about more or less just as a use case for let's say in my case I have some bad",
    "start": "1689429",
    "end": "1696270"
  },
  {
    "text": "mental servers and what I don't have money to buy more service for well exactly you for storage so I could just",
    "start": "1696270",
    "end": "1703530"
  },
  {
    "text": "go ahead run my applications with kubernetes as containers and just well put roots F on there and have my storage",
    "start": "1703530",
    "end": "1710640"
  },
  {
    "text": "a to like well if the applications and you at one point used too much load of the service and yeah you will run into",
    "start": "1710640",
    "end": "1717390"
  },
  {
    "text": "some issues there with well Virginia is also just another application running but it could for example in this case",
    "start": "1717390",
    "end": "1724590"
  },
  {
    "text": "helped me so long till for example I would get more funding from a company or something to get more service just kind",
    "start": "1724590",
    "end": "1730740"
  },
  {
    "text": "of as a idea where it could also be as a used as a use case there and well you",
    "start": "1730740",
    "end": "1739050"
  },
  {
    "text": "showed it with directories but the same goes for disks if you have some raw diskette you serve us just point the",
    "start": "1739050",
    "end": "1745620"
  },
  {
    "text": "safe cluster of cluster object to them and they will be used for the software",
    "start": "1745620",
    "end": "1751920"
  },
  {
    "text": "so it's well especially also for bi-metal in minded well view works",
    "start": "1751920",
    "end": "1760400"
  },
  {
    "text": "yeah and you're running it at home right it's been working for you for a long time yeah I had like I think one of the",
    "start": "1760400",
    "end": "1767010"
  },
  {
    "text": "longest-running clusters or oh well not the longest path from like version 0.6",
    "start": "1767010",
    "end": "1772530"
  },
  {
    "text": "on to like zero eight zero nine and well the only reason but a class I have",
    "start": "1772530",
    "end": "1778080"
  },
  {
    "text": "wasn't well isn't the longest running now anymore it's because I got new hardware and just trapped everything",
    "start": "1778080",
    "end": "1783780"
  },
  {
    "text": "inside it over I say well maybe someone else doesn't like this starts a new version kind of with kubernetes and",
    "start": "1783780",
    "end": "1790790"
  },
  {
    "text": "resets to zero to rebuild what you learnt till adventure there so well fine",
    "start": "1790790",
    "end": "1798630"
  },
  {
    "text": "not only for me some bigger people like some bigger universities I think it was humbug",
    "start": "1798630",
    "end": "1804419"
  },
  {
    "text": "in Germany yeah and there are some other bigger universities in use like talking",
    "start": "1804419",
    "end": "1814679"
  },
  {
    "text": "about yeah supercomputer UCSD a think UCSD dairy it's for example to which is",
    "start": "1814679",
    "end": "1822419"
  },
  {
    "text": "which is simply awesome to see that it's well not only applicable for like",
    "start": "1822419",
    "end": "1827489"
  },
  {
    "text": "smaller use cases like in my case with like seven suddenly but also for bigger",
    "start": "1827489",
    "end": "1832999"
  },
  {
    "text": "well universities also companies there's also a good amount of companies using it",
    "start": "1832999",
    "end": "1838259"
  },
  {
    "text": "and already in production for those of you going like hey can we use it in production yes there are a lot of people",
    "start": "1838259",
    "end": "1845190"
  },
  {
    "text": "are a since like already I think like at 0.3 0.2 or so started using it very",
    "start": "1845190",
    "end": "1851129"
  },
  {
    "text": "early yeah it's not 0.3 I hope but maybe 0.5 yeah yeah alright so that so back to",
    "start": "1851129",
    "end": "1861749"
  },
  {
    "text": "the WordPress so this this is the website it's up now we don't really have time to go through and can configure it",
    "start": "1861749",
    "end": "1867480"
  },
  {
    "text": "but it is backed by the storage that was orchestrated by rook if you would set it",
    "start": "1867480",
    "end": "1875519"
  },
  {
    "text": "up now and create a blog post on it like hey hello cube Khan and we would go ahead and delete the MySQL part and well",
    "start": "1875519",
    "end": "1883980"
  },
  {
    "text": "the new MySQL pod would be created we would get the same data yes well we have",
    "start": "1883980",
    "end": "1889080"
  },
  {
    "text": "to surf crossed behind it so just in",
    "start": "1889080",
    "end": "1894749"
  },
  {
    "text": "conclusion we'd love to hear your in helped you get involved with rook try it out and get involved if you want to",
    "start": "1894749",
    "end": "1902519"
  },
  {
    "text": "cover yeah so well if you want to use it",
    "start": "1902519",
    "end": "1907529"
  },
  {
    "text": "as a user best way would be to go to route at i/o we have a big documentation",
    "start": "1907529",
    "end": "1914070"
  },
  {
    "text": "link at the top if you go to it there's like a link to the staff getting started",
    "start": "1914070",
    "end": "1919980"
  },
  {
    "text": "guide but I also for the oddest like HFS Cassandra and so on and well hoping that",
    "start": "1919980",
    "end": "1926340"
  },
  {
    "text": "you not run into any issues but still if you should run into any issues we either have to get up project where you can",
    "start": "1926340",
    "end": "1932580"
  },
  {
    "text": "create issues or just sit hop on slack and well ask or say that you have an",
    "start": "1932580",
    "end": "1939070"
  },
  {
    "text": "issue yeah um besides if you want to get involved too well for example as you",
    "start": "1939070",
    "end": "1946540"
  },
  {
    "text": "said with a new storage bag and maybe even possibly we have communion team",
    "start": "1946540",
    "end": "1953430"
  },
  {
    "text": "well we have enough so creating issue they're like hey I",
    "start": "1954210",
    "end": "1959470"
  },
  {
    "text": "want to integrate just in this it's always welcome and we even have to we",
    "start": "1959470",
    "end": "1965140"
  },
  {
    "text": "have mailing list if you're interested in that and yeah s already said like a slack and yeah alright yeah so we have a",
    "start": "1965140",
    "end": "1975340"
  },
  {
    "text": "few more sessions that talked about rook here they are hope you can make it to him tomorrow and we'll be here for",
    "start": "1975340",
    "end": "1982780"
  },
  {
    "text": "questions I think the keynotes are starting soon but maybe we could have time for a question or two all right",
    "start": "1982780",
    "end": "1991590"
  },
  {
    "text": "yeah is the question is if the OSD pod is",
    "start": "1998370",
    "end": "2005050"
  },
  {
    "text": "co-located with the disk not really",
    "start": "2005050",
    "end": "2011160"
  },
  {
    "text": "yeah and so the well the OST pod is is",
    "start": "2019090",
    "end": "2026080"
  },
  {
    "text": "attached it has a uses a node selector to be tied to that node that has that",
    "start": "2026080",
    "end": "2031179"
  },
  {
    "text": "disc on it so that kubernetes won't fail over the OSD to another node because it's tied to that disc on that node yeah",
    "start": "2031179",
    "end": "2039359"
  },
  {
    "text": "okay one more question maybe okay",
    "start": "2039359",
    "end": "2047009"
  },
  {
    "text": "okay yes so the question is if you can apply it more OSD backfill settings or",
    "start": "2053440",
    "end": "2059179"
  },
  {
    "text": "other OSD settings yeah yes it is possible to apply any Ceph settings to",
    "start": "2059179",
    "end": "2064280"
  },
  {
    "text": "for the daemons there is document in advanced documentation on the site that says how to do that basically you set",
    "start": "2064280",
    "end": "2071000"
  },
  {
    "text": "the Ceph settings in a config map and then when it creates the pods those pods pick up the settings from that config",
    "start": "2071000",
    "end": "2078080"
  },
  {
    "text": "map alright well thanks a lot everybody and let us know if you have questions",
    "start": "2078080",
    "end": "2083750"
  },
  {
    "text": "stop by the rook booth anytime this week and we'll talk more I hope to know more",
    "start": "2083750",
    "end": "2089158"
  },
  {
    "text": "[Applause]",
    "start": "2089159",
    "end": "2096378"
  }
]