[
  {
    "start": "0",
    "end": "166000"
  },
  {
    "text": "good morning everybody how's everyone doing you enjoying conference yeah good",
    "start": "0",
    "end": "5130"
  },
  {
    "text": "good all right cool everyone's a bit yeah quiet this morning my name is Andy Repton I'm a",
    "start": "5130",
    "end": "11849"
  },
  {
    "text": "mission-critical engineer at shoe burg Phyllis we are a mannerless integration",
    "start": "11849",
    "end": "17190"
  },
  {
    "text": "specialist based out of Amsterdam the main thing that we kind of pride",
    "start": "17190",
    "end": "22470"
  },
  {
    "text": "ourselves on is we guarantee 100% functional uptime for our customers this",
    "start": "22470",
    "end": "28949"
  },
  {
    "text": "poses some interesting problems and challenges and this is kind of our story of how we took kuba Nader's why we chose",
    "start": "28949",
    "end": "35370"
  },
  {
    "text": "it and kind of brought it through to guaranteeing a 100% functional uptime or as we call it mission-critical",
    "start": "35370",
    "end": "42440"
  },
  {
    "text": "I'm Michael Russell been working shavax since February also on the the kubernetes project for the last few",
    "start": "42440",
    "end": "48390"
  },
  {
    "text": "months and yeah that's pretty much it also mission-critical engineer yep so",
    "start": "48390",
    "end": "55640"
  },
  {
    "text": "second what see if I can get this working okay just a brief overview of",
    "start": "55640",
    "end": "61469"
  },
  {
    "text": "what we're going to talk about kind of why we chose coop inators in the first place what about it I mean I think",
    "start": "61469",
    "end": "66869"
  },
  {
    "text": "that's gonna be a very similar familiar story to all of you guys the tooling that we chose something that we call",
    "start": "66869",
    "end": "74189"
  },
  {
    "text": "sub-clusters I'm not sure if anyone here saw Brandon Phillips there's excellent talk on self-hosted kubernetes we did a",
    "start": "74189",
    "end": "81180"
  },
  {
    "text": "relatively similar idea ours is slightly different but running cube inators on top of kubernetes and kind of what that",
    "start": "81180",
    "end": "88110"
  },
  {
    "text": "brings us testing and monitoring and kind of the continuation of our journey because you never really stop with this",
    "start": "88110",
    "end": "95009"
  },
  {
    "text": "stuff and kubernetes is constantly evolving so to start off with why did we",
    "start": "95009",
    "end": "100530"
  },
  {
    "text": "choose kubernetes we need to go faster we we as a company",
    "start": "100530",
    "end": "106100"
  },
  {
    "text": "recognized the software is eating the world and fast as the new you know faster is the new fast so we wanted to",
    "start": "106100",
    "end": "113610"
  },
  {
    "text": "go containerization and we wanted to move into public cloud eventually in the future so we wanted to start designing",
    "start": "113610",
    "end": "119700"
  },
  {
    "text": "our software and designing our solutions in such a way that anything that we build now is not creating technical debt",
    "start": "119700",
    "end": "126570"
  },
  {
    "text": "for ourselves in two or three years time we believed there was the future more",
    "start": "126570",
    "end": "131910"
  },
  {
    "text": "than that we were hiring more more ella purrs into our company traditionally we've been in operations shop and now we're moving into DevOps",
    "start": "131910",
    "end": "138660"
  },
  {
    "text": "and eventually we'd like to become a software development company completely and trying to ask a group of angular",
    "start": "138660",
    "end": "145140"
  },
  {
    "text": "developers to actually understand how to deploy a V PC inside a private cloud is",
    "start": "145140",
    "end": "151110"
  },
  {
    "text": "relatively difficult and containerization with kubernetes gave us this opportunity to actually make it",
    "start": "151110",
    "end": "157380"
  },
  {
    "text": "very very easy for our developers to just come in sit down write their code and get a push to production so this is",
    "start": "157380",
    "end": "166950"
  },
  {
    "start": "166000",
    "end": "202000"
  },
  {
    "text": "just a quick overview of the tooling we'll go into more detail later but just for the very sort of high-level overview",
    "start": "166950",
    "end": "172110"
  },
  {
    "text": "we're using kubernetes of course which were then running on core OS which were",
    "start": "172110",
    "end": "178709"
  },
  {
    "text": "deploying with terraform announce table and we're using Voltas sort of a middleman for secret storage for why",
    "start": "178709",
    "end": "184110"
  },
  {
    "text": "we're actually storing tokens and things like that before actually deploying them to a stack Jenkins for CI testing sensu",
    "start": "184110",
    "end": "191130"
  },
  {
    "text": "and Prometheus for monitoring and flannel as the overlay network which has just been replaced as of last week but",
    "start": "191130",
    "end": "197610"
  },
  {
    "text": "it's still an option for when we're deploying new clusters so one of the",
    "start": "197610",
    "end": "202799"
  },
  {
    "start": "202000",
    "end": "297000"
  },
  {
    "text": "other things that we should mention is we've kind of built this all on top of existing technology that we also have",
    "start": "202799",
    "end": "208950"
  },
  {
    "text": "within our company so we have container registry private as running as a",
    "start": "208950",
    "end": "214290"
  },
  {
    "text": "container we have an on-premise s3 compatible object store which has given",
    "start": "214290",
    "end": "219750"
  },
  {
    "text": "us a lot of benefits because we can actually use s3 compatible tooling with it as long as the SDK is not hard-coded",
    "start": "219750",
    "end": "225840"
  },
  {
    "text": "the region's into that particular SDK Java is a key one where they actually",
    "start": "225840",
    "end": "233730"
  },
  {
    "text": "you can't use it with anything outside of AWS so kubernetes we wanted to",
    "start": "233730",
    "end": "241799"
  },
  {
    "text": "basically be deploying this on both AWS and on our private cloud our private",
    "start": "241799",
    "end": "247980"
  },
  {
    "text": "cloud is cloud stack we've actually forked cloud stack now as of about six months ago and our Fork is called cosmic",
    "start": "247980",
    "end": "254760"
  },
  {
    "text": "or mission-critical cloud and we basically had to write a lot of the",
    "start": "254760",
    "end": "259799"
  },
  {
    "text": "functionality into kubernetes that would allow it to work with stack a lot of the magic that you see in",
    "start": "259799",
    "end": "266630"
  },
  {
    "text": "a lot of the tutorials and the guides that you find online a lot of them are relying on AWS or GCE",
    "start": "266630",
    "end": "273860"
  },
  {
    "text": "and the functionality that those cloud providers give you and for us trying to then reverse-engineer those and get them",
    "start": "273860",
    "end": "280310"
  },
  {
    "text": "into our private cloud was a really interesting journey but actually gave us a lot of insight into the underlying of",
    "start": "280310",
    "end": "285919"
  },
  {
    "text": "how kubernetes is actually working so to start off with we just did terraform and",
    "start": "285919",
    "end": "291740"
  },
  {
    "text": "so terraform is a pretty standard tool from Hoshi Corp we used know convergence",
    "start": "291740",
    "end": "297620"
  },
  {
    "start": "297000",
    "end": "381000"
  },
  {
    "text": "it was a mutable infrastructure we used terraform combined with terraform",
    "start": "297620",
    "end": "303409"
  },
  {
    "text": "templates where you can actually pass in variables and using those configure",
    "start": "303409",
    "end": "308449"
  },
  {
    "text": "cloud config people here familiar with the concept of cloud config yeah vaguely",
    "start": "308449",
    "end": "315220"
  },
  {
    "text": "we're using core OS so cloud config for those who are not aware you can create a",
    "start": "315220",
    "end": "321050"
  },
  {
    "text": "Yama file basically and this Yama file will describe to Koro as how it should",
    "start": "321050",
    "end": "326060"
  },
  {
    "text": "configure itself you pass this through on the router VM the router VM then passes it through to your core OS host",
    "start": "326060",
    "end": "331849"
  },
  {
    "text": "it configures itself and you have a configured system it was simple and",
    "start": "331849",
    "end": "337729"
  },
  {
    "text": "simple as always good but unfortunately there was no convergence so this meant",
    "start": "337729",
    "end": "342830"
  },
  {
    "text": "if we wanted to do a rolling upgrade of the cluster terraform would quite happily destroy your entire infrastructure and rebuild it for you",
    "start": "342830",
    "end": "349250"
  },
  {
    "text": "which meant downtime which for us was just not really an option in addition",
    "start": "349250",
    "end": "354380"
  },
  {
    "text": "because core OS is an immutable server or a mutable operating system it meant",
    "start": "354380",
    "end": "360380"
  },
  {
    "text": "that we had to configure everything through systemctl scripts and the systemctl scripts you then have to add",
    "start": "360380",
    "end": "366710"
  },
  {
    "text": "bash bash can then change slightly depending on how you're deploying it if you have redeploying tokens can expire",
    "start": "366710",
    "end": "373940"
  },
  {
    "text": "and in general it just was not very scalable for us so then we proceeded on",
    "start": "373940",
    "end": "379370"
  },
  {
    "text": "to yes so intensive also bit of complexity saj to get in we we do miss",
    "start": "379370",
    "end": "385159"
  },
  {
    "start": "381000",
    "end": "500000"
  },
  {
    "text": "the good old days of cloud init where we could just create and destroy clusters my life was simple we didn't need to worry about data migrations and",
    "start": "385159",
    "end": "391560"
  },
  {
    "text": "and having compatible upgrades and things but we're starting to have a few customers starting to use it so it was",
    "start": "391560",
    "end": "396990"
  },
  {
    "text": "becoming a bit unreasonable to say hey new features are out by the way we've deleted your cluster so this is where ansible came in and",
    "start": "396990",
    "end": "403770"
  },
  {
    "text": "this gave us for the first time the ability to do rolling upgrades so with terraform out of the box there's no",
    "start": "403770",
    "end": "409139"
  },
  {
    "text": "actual way to say upgrade master 1 then master 2 then master 3 and there's",
    "start": "409139",
    "end": "414210"
  },
  {
    "text": "definitely no way to say upgrade master 1 do a health check then upgrade master 2 so that was really the the main plus",
    "start": "414210",
    "end": "420240"
  },
  {
    "text": "side of using ansible and another big big gain that we got from that was being out of stole the secrets client side so",
    "start": "420240",
    "end": "427470"
  },
  {
    "text": "from actual laptops we're pulling the seekers out of volt with a vault token and then actually deploying those to the",
    "start": "427470",
    "end": "432810"
  },
  {
    "text": "cluster so this removed the need to have to put any sort of secrets inside a cloud in it which was a bit of an",
    "start": "432810",
    "end": "438870"
  },
  {
    "text": "insecure way of doing things so the other really really big game we got from this is it gave us much better",
    "start": "438870",
    "end": "444990"
  },
  {
    "text": "templating engine so this meant that we could do certain tasks for Amazon hosted instances or certain tasks for cloud",
    "start": "444990",
    "end": "451290"
  },
  {
    "text": "stack or if we wanted to potentially support anything else and we'll tell turn a little bit later but with the sub",
    "start": "451290",
    "end": "457200"
  },
  {
    "text": "coasters there's also slight changes needed for the nodes running in sub clusters so with ansible that's a very",
    "start": "457200",
    "end": "462810"
  },
  {
    "text": "simple oh we just need to do this if it's a sub coaster and do this if it isn't one so this gave is just a single",
    "start": "462810",
    "end": "467970"
  },
  {
    "text": "single codebase that we could use across all of our configurations and cloud providers so one of the downsides that",
    "start": "467970",
    "end": "474120"
  },
  {
    "text": "we hadn't yet solved was how to handle having lots of different clusters and lots of master components and this was",
    "start": "474120",
    "end": "480240"
  },
  {
    "text": "also a bit of a practical side but the way we work where we need to actually pay for infrastructure from our cloud team and the customers actually using",
    "start": "480240",
    "end": "487050"
  },
  {
    "text": "this environment also have to pay for it so if we were able to find a way to actually consolidate all of the master components and ec DS in a really nice",
    "start": "487050",
    "end": "494729"
  },
  {
    "text": "tool such as kubernetes then that would be a big game for us and also for our customers so a little bit of background",
    "start": "494729",
    "end": "501210"
  },
  {
    "text": "on story for this one somebody came to me a engineer and one of the other",
    "start": "501210",
    "end": "506460"
  },
  {
    "text": "customer teams and said look I really need a private kubernetes cluster I don't want to configure it I don't want",
    "start": "506460",
    "end": "513419"
  },
  {
    "text": "to have to do anything with it I just want an API and I want nodes on it and I just want to start using it can you give",
    "start": "513419",
    "end": "519959"
  },
  {
    "text": "this to me and we kind of thought about this and we came up with this idea of subclusters so this is basically",
    "start": "519959",
    "end": "527920"
  },
  {
    "text": "kubernetes running on top of kubernetes we were trying to figure out with life",
    "start": "527920",
    "end": "533320"
  },
  {
    "text": "cycle you know how do you store the secrets for kubernetes you know I don't know if anyone set up multi master",
    "start": "533320",
    "end": "539350"
  },
  {
    "text": "kubernetes before but trying to sort out the the certificates for example and the keys is really quite difficult how do we",
    "start": "539350",
    "end": "547420"
  },
  {
    "text": "sort out logging how do we sort our upgrades how do we sort out a che all of these things are problems that",
    "start": "547420",
    "end": "553930"
  },
  {
    "text": "kubernetes itself excels at solving and then we kind of thought about it and we",
    "start": "553930",
    "end": "559330"
  },
  {
    "text": "realize well why don't we just put kubernetes on top of another kubernetes so we created something which we call",
    "start": "559330",
    "end": "566680"
  },
  {
    "text": "the management cluster which is just a random name we assigned that's deployed using terraform and ansible",
    "start": "566680",
    "end": "573040"
  },
  {
    "text": "and then on top of that we create namespaces per additional cluster which we call a sub cluster each of those runs",
    "start": "573040",
    "end": "581740"
  },
  {
    "text": "an etcd pet set well won't be for much longer now that core OS has announced",
    "start": "581740",
    "end": "587320"
  },
  {
    "text": "their excellent etcd operator thanks for that only took us two weeks to get that working and then in addition to that we",
    "start": "587320",
    "end": "594820"
  },
  {
    "text": "then have three sets of H a masters now we are just using kubernetes so we have",
    "start": "594820",
    "end": "602740"
  },
  {
    "text": "a deployment which contains three replicas set of three parts each of",
    "start": "602740",
    "end": "608350"
  },
  {
    "text": "those pods has three images inside it a cube API server a cube controller manager and a cube scheduler those three",
    "start": "608350",
    "end": "615340"
  },
  {
    "text": "- leader election and they float somewhere within the management cluster control by kubernetes they talk to their",
    "start": "615340",
    "end": "622000"
  },
  {
    "text": "own etcd and this allows us all of the benefits of a deployment you know we have high availability the nodes that",
    "start": "622000",
    "end": "628870"
  },
  {
    "text": "they are running on are spread across two data centers so we can lose a hypervisor we can lose an entire rack",
    "start": "628870",
    "end": "635650"
  },
  {
    "text": "and we can actually lose an entire data center and the etcd pet set will self heal itself on the other side of the",
    "start": "635650",
    "end": "641920"
  },
  {
    "text": "data center divide and the H a masters will then be turned back on by kubernetes in addition logging if we",
    "start": "641920",
    "end": "649330"
  },
  {
    "text": "have any problems with any of the components we just connect to the management cluster and just do a cube CTL logs command we can even do an exec",
    "start": "649330",
    "end": "656710"
  },
  {
    "text": "into those particular containers and see what's going on there we can on the fly adjust the velocity of each of",
    "start": "656710",
    "end": "663910"
  },
  {
    "text": "those binaries to increase or decrease logging based on what we'd like it to see secrets we can now push the secrets",
    "start": "663910",
    "end": "671410"
  },
  {
    "text": "into the management cluster and mount them inside of the pots as volumes this",
    "start": "671410",
    "end": "677440"
  },
  {
    "text": "allows us to push anything we need to very quickly into the management cluster",
    "start": "677440",
    "end": "683050"
  },
  {
    "text": "and then we can just wreak ich the pods and they will basically get their new secrets we'll actually we've actually",
    "start": "683050",
    "end": "690190"
  },
  {
    "text": "are starting to use hash your core vote for that but that comes at the end and in a ditch sorry",
    "start": "690190",
    "end": "695710"
  },
  {
    "text": "in addition ease of upgrade we have teams who come to us and say I really",
    "start": "695710",
    "end": "700720"
  },
  {
    "text": "want to use alpha cube inators you know I want to use one dot for today and I have another team who's on one dot 3.5",
    "start": "700720",
    "end": "707410"
  },
  {
    "text": "well now it's as simple as cube CTL edit deployment change the image tag to be v1",
    "start": "707410",
    "end": "714130"
  },
  {
    "text": "dot four do a right quit in them and cube inators automatically does a rolling upgrade of the deployment it'll",
    "start": "714130",
    "end": "720880"
  },
  {
    "text": "turn off two of my masters it'll turn on two more of them when they were port healthy it'll turn off the last one and",
    "start": "720880",
    "end": "726790"
  },
  {
    "text": "then we create my H a kubernetes masters it takes about five seconds in addition",
    "start": "726790",
    "end": "732250"
  },
  {
    "text": "with deployments you have robach you can roll back a kubernetes deployment so if for whatever reason the upgrade goes",
    "start": "732250",
    "end": "738490"
  },
  {
    "text": "badly is just as simple as a cube CTO rollback and the main thing for us was",
    "start": "738490",
    "end": "744370"
  },
  {
    "text": "multi-tenancy we can put our nodes completely separate from our master",
    "start": "744370",
    "end": "749530"
  },
  {
    "text": "components the nodes and the master components do not need to shower a network overlay it makes life easier",
    "start": "749530",
    "end": "756820"
  },
  {
    "text": "sure but they're quite happy to run separately we can go as far as actually running our master components inside of",
    "start": "756820",
    "end": "763750"
  },
  {
    "text": "Cuba native cluster in our data center and run the nodes for those master components in AWS and they actually",
    "start": "763750",
    "end": "770830"
  },
  {
    "text": "communicate over the public Internet this allows us to very quickly you know control the master components on our IP",
    "start": "770830",
    "end": "777730"
  },
  {
    "text": "space which allows us to restrict it by firewalls but the nodes themselves can win in AWS and take advantage of elastic",
    "start": "777730",
    "end": "785020"
  },
  {
    "text": "block storage IO bees and better replication",
    "start": "785020",
    "end": "790060"
  },
  {
    "text": "I would offer a demo of this but the terraform takes forever and I try to do",
    "start": "790060",
    "end": "795730"
  },
  {
    "text": "this earlier I took twenty minutes and I won't bore you with it if anyone is really interested in seeing this I'm quite happy to show you this is what it",
    "start": "795730",
    "end": "803470"
  },
  {
    "start": "800000",
    "end": "866000"
  },
  {
    "text": "looks like from the management cluster this is one of our sub clusters with the etcd pet set at the top and then the",
    "start": "803470",
    "end": "810220"
  },
  {
    "text": "three master components running as an H a down the bottom I think one thing I'll",
    "start": "810220",
    "end": "816340"
  },
  {
    "text": "add to the sub clusters is that having the nodes separated from the master components also solves another really",
    "start": "816340",
    "end": "822820"
  },
  {
    "text": "big problem that we have and that is that different customers we have will have their own environments already with",
    "start": "822820",
    "end": "827920"
  },
  {
    "text": "very look down firewall rules so we want to be the ones actually hosting kubernetes and in control and updating",
    "start": "827920",
    "end": "833440"
  },
  {
    "text": "that and they want to have the nodes in their own environment where they can actually connect locally to their databases and other service needs access",
    "start": "833440",
    "end": "839800"
  },
  {
    "text": "so it's a very good design from that point of view as well yep and I would like to stress that this is just a",
    "start": "839800",
    "end": "845890"
  },
  {
    "text": "deployment there's no custom go scripts there's no dashboard there's no front end there's nothing it's literally just",
    "start": "845890",
    "end": "852340"
  },
  {
    "text": "pure kubernetes so I'm hoping that I will get this up on github before the",
    "start": "852340",
    "end": "857590"
  },
  {
    "text": "end of next week and try and publish this out but it's very simple and anyone can do it inside their own clusters - ok",
    "start": "857590",
    "end": "867310"
  },
  {
    "start": "866000",
    "end": "1098000"
  },
  {
    "text": "so somewhere between the sub clusters and all the answer was stuff we started to get quite a lot of complexity and we",
    "start": "867310",
    "end": "873670"
  },
  {
    "text": "started to have people who are adding in new features or moving from a back to our back and not really telling the team",
    "start": "873670",
    "end": "878950"
  },
  {
    "text": "and it was getting a bit unmanageable with actually testing what was working what needed to work and then we started",
    "start": "878950",
    "end": "885310"
  },
  {
    "text": "to discover all the features of kubernetes that our developers wanted to use so there'd be a core OS update and",
    "start": "885310",
    "end": "890410"
  },
  {
    "text": "suddenly cube tto proxy wouldn't work anymore because so cat had been removed those sorts of things so we ended up",
    "start": "890410",
    "end": "896830"
  },
  {
    "text": "sending up a nice CI job to try and test all the functionality we knew about including the deployment and secrets",
    "start": "896830",
    "end": "902080"
  },
  {
    "text": "management and sub clusters so I'm gonna try and talk you through it it's quite a",
    "start": "902080",
    "end": "907210"
  },
  {
    "text": "bit that it goes through each time so at the start we we destroyed the testing environment completely so this is",
    "start": "907210",
    "end": "912490"
  },
  {
    "text": "deleting the actual project and cloud sucks so nothing's left so at this stage terraform is then creating the",
    "start": "912490",
    "end": "919240"
  },
  {
    "text": "infrastructure which is nine chorus nodes so three for the Masters through d and through for the nodes this makes",
    "start": "919240",
    "end": "925639"
  },
  {
    "text": "up the management cluster at that stage ansible takes over and actually deploys kubernetes and during the ansible run",
    "start": "925639",
    "end": "933320"
  },
  {
    "text": "it's also doing some health checks so this is actually in there for the rolling upgrade so after deploying a",
    "start": "933320",
    "end": "938810"
  },
  {
    "text": "master component doing a health check is it healthy before continuing on so the same thing is actually happening during",
    "start": "938810",
    "end": "944120"
  },
  {
    "text": "the Jenkins run so at this stage we've got a COO Bonelli's cluster running and we have a really nice tool written by",
    "start": "944120",
    "end": "951170"
  },
  {
    "text": "one of our NGO developers which is an end-to-end tester so this is actually a bit of go which is testing things like",
    "start": "951170",
    "end": "957290"
  },
  {
    "text": "is port forwarding working is cube TTL proxy working can I do pod - pod networking all of these kinds of things",
    "start": "957290",
    "end": "963680"
  },
  {
    "text": "that can be broken but still look like it's working because up until this stage we would just do a cube CTL get RC",
    "start": "963680",
    "end": "969920"
  },
  {
    "text": "something like that and see oh look it's okay and just assume that all the functionality is working so we haven't",
    "start": "969920",
    "end": "976190"
  },
  {
    "text": "yet touched on it but we then actually run all that I'm monitoring checks manually so that's part of the pipeline as well and at this stage we can",
    "start": "976190",
    "end": "985940"
  },
  {
    "text": "actually start the point the real stuff which is a sub clusters so this involves creating another new project which is",
    "start": "985940",
    "end": "991430"
  },
  {
    "text": "where the nodes are going to be hosted so none of the nodes ever hosted in the same network as a management cluster so",
    "start": "991430",
    "end": "996680"
  },
  {
    "text": "we give the customers the opportunity to host the nodes ourselves or that they can be hosted in their own network we",
    "start": "996680",
    "end": "1003010"
  },
  {
    "text": "should mention a in this case a class like project how many of you are familiar with AWS tagging yeah so within",
    "start": "1003010",
    "end": "1010899"
  },
  {
    "text": "cloud slack it's a similar idea it's creating a project to prevent your API calls accidentally hitting a different",
    "start": "1010899",
    "end": "1017500"
  },
  {
    "text": "location so it's just the same as talking yeah so this stage we sort of",
    "start": "1017500",
    "end": "1022779"
  },
  {
    "text": "start again so we're deploying a new sub costa kubernetes stop deploying the monitoring again run the end-to-end tends to stir",
    "start": "1022779",
    "end": "1028990"
  },
  {
    "text": "again so this is now testing a kubernetes inside of kubernetes including the monitoring once again and",
    "start": "1028990",
    "end": "1034630"
  },
  {
    "text": "I think that's that's pretty much it takes around 3035 minutes like Andy said",
    "start": "1034630",
    "end": "1039880"
  },
  {
    "text": "yep do to terraform having to create VP sees and everything each time but it's a pretty good turnaround for a full stack",
    "start": "1039880",
    "end": "1046480"
  },
  {
    "text": "test of kubernetes cluster with a actual sub cluster as well and the monitoring we we triggered this on merge request so",
    "start": "1046480",
    "end": "1053620"
  },
  {
    "text": "we use git lab wealth and get high we previously used Enterprise github on premise but we found that they weren't",
    "start": "1053620",
    "end": "1060159"
  },
  {
    "text": "creating the features that we wanted and it was an extortion of price tag so we've moved to get lab and so now we",
    "start": "1060159",
    "end": "1067809"
  },
  {
    "text": "have a single repository which contains our terraform plans it contains the ansible playbooks it",
    "start": "1067809",
    "end": "1073750"
  },
  {
    "text": "contains the kubernetes add-ons for example and then we're just running bash scripts on top of this to basically",
    "start": "1073750",
    "end": "1079510"
  },
  {
    "text": "trigger each of these in turn as a Jenkins job from gitlab so whenever one of us makes a change we'll open up a",
    "start": "1079510",
    "end": "1085960"
  },
  {
    "text": "merge request the testing will go through will confirm that we haven't broken anything and at that point we",
    "start": "1085960",
    "end": "1091029"
  },
  {
    "text": "merge and move on relatively content that things are still working for us yep",
    "start": "1091029",
    "end": "1097419"
  },
  {
    "text": "so one during monitoring was a fun one because there was quite a few challenges that I had never had to do before with",
    "start": "1097419",
    "end": "1103000"
  },
  {
    "start": "1098000",
    "end": "1387000"
  },
  {
    "text": "monitoring so first one was core OS which is you go okay how do I install my",
    "start": "1103000",
    "end": "1108370"
  },
  {
    "text": "monitoring clients and then you realize you don't have a package manager or anything like that so that was the first challenge the the second challenge was",
    "start": "1108370",
    "end": "1115299"
  },
  {
    "text": "at the time we were still using the cloud and in terraform setup so any kind of monitoring wheel setting up had to be",
    "start": "1115299",
    "end": "1121200"
  },
  {
    "text": "reconfigurable without destroying and recreating the cluster because we weren't going to do that just for adding a new monitoring checks and then the",
    "start": "1121200",
    "end": "1127929"
  },
  {
    "text": "other new fund challenge of course is monitoring stuff inside kubernetes the question of what do we actually want to",
    "start": "1127929",
    "end": "1133059"
  },
  {
    "text": "monitor what is responsibility of the infrastructure monitoring what's the responsibility of the developers to",
    "start": "1133059",
    "end": "1138070"
  },
  {
    "text": "monitor so as you know from the sub busters we like to do kubernetes inside of kubernetes so we're also monitoring",
    "start": "1138070",
    "end": "1145090"
  },
  {
    "text": "kubernetes from inside kubernetes as well so it looks a little bit something like this so we have node exporter",
    "start": "1145090",
    "end": "1150760"
  },
  {
    "text": "running on all of the core OS nodes so that's deployed with the initial deployment and for those of you not familiar with node exporter this this",
    "start": "1150760",
    "end": "1159490"
  },
  {
    "text": "exposes their port which has just got a very funky looking API that's used by",
    "start": "1159490",
    "end": "1164649"
  },
  {
    "text": "the Prometheus standard and that's in scraped by Prometheus which is running from inside the kubernetes cluster so if",
    "start": "1164649",
    "end": "1170559"
  },
  {
    "text": "you're talking about management clusters and sub-clusters inside the management cluster there's a Prometheus and then a sensor client for",
    "start": "1170559",
    "end": "1177730"
  },
  {
    "text": "monitoring and then in each of these sub clusters there's also another Prometheus and sensor client which monitor the",
    "start": "1177730",
    "end": "1183399"
  },
  {
    "text": "nodes of the sub foster so at this stage we've now actually got basic sort of",
    "start": "1183399",
    "end": "1189070"
  },
  {
    "text": "os monitoring are we running out of inodes or the services running is a load too high that kind of stuff and then on",
    "start": "1189070",
    "end": "1195549"
  },
  {
    "text": "top of that we're then using the great kubernetes sensor plugins which are actually doing API checks checking that",
    "start": "1195549",
    "end": "1201370"
  },
  {
    "text": "all the nodes are ready checking that there aren't any other errors going on and that kind of stuff and then finally",
    "start": "1201370",
    "end": "1207549"
  },
  {
    "text": "we get onto actually monitoring the stuff running inside kubernetes so what we ended up coming up with this is we're",
    "start": "1207549",
    "end": "1213820"
  },
  {
    "text": "sort of doing this on a namespace level our developers were using different namespaces of different environments and",
    "start": "1213820",
    "end": "1219399"
  },
  {
    "text": "their request was oh this is my production namespace I want you to check all the health checks inside this",
    "start": "1219399",
    "end": "1224710"
  },
  {
    "text": "namespace so what we do is we're actually monitoring on namespaces based on labels so we just have a label I",
    "start": "1224710",
    "end": "1231429"
  },
  {
    "text": "think it's called page this equals true and a developer can enable out on a single pod on a service on a namespace",
    "start": "1231429",
    "end": "1237610"
  },
  {
    "text": "and then send to or actually pick that up and start monitoring it so at this",
    "start": "1237610",
    "end": "1242710"
  },
  {
    "text": "stage the the first question which normally happens is have you heard of Prometheus events and so that's kind of",
    "start": "1242710",
    "end": "1249100"
  },
  {
    "text": "what I'm doing with node exporter and there's a few reasons why I'm actually doing that manually instead and one of",
    "start": "1249100",
    "end": "1254409"
  },
  {
    "text": "that is if you look at the the nice dashboard here we can actually see the current state and everything is green and this is I think more of a personal",
    "start": "1254409",
    "end": "1261610"
  },
  {
    "text": "choice where with the events in Prometheus is that it will sort of tell you when something is wrong rather than",
    "start": "1261610",
    "end": "1268149"
  },
  {
    "text": "telling you that everything is OK and when you're spawning a bunch of sub-clusters and you've got a whole bunch of new services being deployed by",
    "start": "1268149",
    "end": "1274480"
  },
  {
    "text": "developers you're not sure whether everything's ok or whether everything's actually not working and you're monitoring hasn't said anything so this",
    "start": "1274480",
    "end": "1281200"
  },
  {
    "text": "was more about having a stateful way of actually looking at monitoring and seeing what's going on we should say",
    "start": "1281200",
    "end": "1286870"
  },
  {
    "text": "that the nice thing about deploying all of the monitoring components inside kubernetes is that it should work on any",
    "start": "1286870",
    "end": "1292629"
  },
  {
    "text": "kubernetes cluster I don't think we've open sourced it yet but we will from a",
    "start": "1292629",
    "end": "1297940"
  },
  {
    "text": "list it's a lot of things that we need to kind of clean up you know it's always one of those situations where you're",
    "start": "1297940",
    "end": "1303340"
  },
  {
    "text": "embarrassed by your own code so you want to make sure it's nice and tidy first but we will put it out there we use send",
    "start": "1303340",
    "end": "1310210"
  },
  {
    "text": "you because we have it already available within our infrastructure but the nice thing about Senzu is there is a push",
    "start": "1310210",
    "end": "1316389"
  },
  {
    "text": "based monitoring system so you're only sending JSON payloads and then sends you will alert if that heartbeats stop",
    "start": "1316389",
    "end": "1322450"
  },
  {
    "text": "oops this is quite nice because you can actually put any sort of other monitoring system you'd like in place as",
    "start": "1322450",
    "end": "1327550"
  },
  {
    "text": "long as it can receive those JSON payloads you can theoretically just reuse this kind of push based monitoring",
    "start": "1327550",
    "end": "1333550"
  },
  {
    "text": "system and it actually scales very nicely we don't need to configure anything on the Senzu side when we bring",
    "start": "1333550",
    "end": "1339940"
  },
  {
    "text": "up additional clusters each additional cluster just send this information to send to and then based on that we can",
    "start": "1339940",
    "end": "1346000"
  },
  {
    "text": "then actually page during the night for our engineers yeah just to give you a bit more info on how that check is",
    "start": "1346000",
    "end": "1351070"
  },
  {
    "text": "actually working is it's querying the Prometheus API and when you do that for something like uptime it then returns a",
    "start": "1351070",
    "end": "1356800"
  },
  {
    "text": "list of all of the hosts so the check is then actually going through that list and since it allows you to send checks",
    "start": "1356800",
    "end": "1362500"
  },
  {
    "text": "on behalf of something else so there's a single sensor client actually running all the checks but when you look inside",
    "start": "1362500",
    "end": "1368050"
  },
  {
    "text": "the dashboard you see more a traditional setup which is each of the hosts and each of the checks but it also gives you",
    "start": "1368050",
    "end": "1374050"
  },
  {
    "text": "the ability to do cluster level checks so if you've got a for example cube API service is that available as a cluster",
    "start": "1374050",
    "end": "1380170"
  },
  {
    "text": "thing rather than having that go off three times if there's an issue for each of the three masters you can just have a",
    "start": "1380170",
    "end": "1385210"
  },
  {
    "text": "centralized check next steps so we've",
    "start": "1385210",
    "end": "1390220"
  },
  {
    "start": "1387000",
    "end": "1633000"
  },
  {
    "text": "we've intentionally left out some of the stuff we've done otherwise we'd be here all day and you guys would fall asleep",
    "start": "1390220",
    "end": "1397320"
  },
  {
    "text": "but some of the things we're working on now are Federation so we're trying to spread our clusters across different",
    "start": "1397320",
    "end": "1404920"
  },
  {
    "text": "cloud providers so we have one POC running right now where we have one cluster in AWS one classroom in our",
    "start": "1404920",
    "end": "1411910"
  },
  {
    "text": "private cloud and then spreading Federation across those particular control planes the idea being to then",
    "start": "1411910",
    "end": "1419070"
  },
  {
    "text": "tag our deployments for our developers and based on those tags determine which",
    "start": "1419070",
    "end": "1424450"
  },
  {
    "text": "location to actually put the deployment so networking yep this summer writing",
    "start": "1424450",
    "end": "1430960"
  },
  {
    "text": "these slides a few weeks ago we are looking at replacing flannel mostly because we're having a few issues with",
    "start": "1430960",
    "end": "1436210"
  },
  {
    "text": "it there's a bug at the moment where it starts to forget routes and finally and etcd is pretty fast so when it does",
    "start": "1436210",
    "end": "1444370"
  },
  {
    "text": "forget these routes that can find it again and around a second but when you're actually running the kubernetes",
    "start": "1444370",
    "end": "1449440"
  },
  {
    "text": "in kubernetes it doesn't really like the latency too much so it was causing a lot of weird issues and luckily a wee",
    "start": "1449440",
    "end": "1456390"
  },
  {
    "text": "are a great blog posts I think that week which was talking about doing host native networking which is just",
    "start": "1456390",
    "end": "1462450"
  },
  {
    "text": "injecting the rats with ansible and luckily for us all of our hosts were connected layer to level so we gave that",
    "start": "1462450",
    "end": "1468630"
  },
  {
    "text": "a go and it worked pipeline was green so okay let's move across the native networking and we've been writing that for the last few weeks",
    "start": "1468630",
    "end": "1474270"
  },
  {
    "text": "with a lot of success the next thing is automated vault integration I'm not sure if any of you saw the digital ocean talk",
    "start": "1474270",
    "end": "1481530"
  },
  {
    "text": "about using vault as a certificate authority so we've been we've been using vault for a while to actually push",
    "start": "1481530",
    "end": "1487980"
  },
  {
    "text": "secrets into vault and then provide a token to the notes they would then use a bash script to then request that the",
    "start": "1487980",
    "end": "1494460"
  },
  {
    "text": "secrets in terms of deploying stuff we're now moving to using console",
    "start": "1494460",
    "end": "1500400"
  },
  {
    "text": "templates or using the vault sidekick container so the idea being that we can push secrets into volt and then",
    "start": "1500400",
    "end": "1507030"
  },
  {
    "text": "automatically our API servers will automatically watch that location",
    "start": "1507030",
    "end": "1512390"
  },
  {
    "text": "redownload the new secrets reload themselves this would basically allow us",
    "start": "1512390",
    "end": "1518040"
  },
  {
    "text": "to then do very short-lived certificates and rotate our certificates every week in addition we're storing cube configs",
    "start": "1518040",
    "end": "1525690"
  },
  {
    "text": "inside vault which then we can create ACLs which then allows us to then say",
    "start": "1525690",
    "end": "1531470"
  },
  {
    "text": "development team a authenticates to vote via a LDAP sorry Active Directory there",
    "start": "1531470",
    "end": "1540690"
  },
  {
    "text": "we go because that's not currently supported by kubernetes so they authenticate to vote based on",
    "start": "1540690",
    "end": "1546810"
  },
  {
    "text": "their Active Directory groups they are then allowed access to all of the cube come fix that they have access to for",
    "start": "1546810",
    "end": "1552090"
  },
  {
    "text": "their clusters this allows us to then very rapidly change tokens and we just",
    "start": "1552090",
    "end": "1557130"
  },
  {
    "text": "say to the developers just rerun the the script we authenticate we download your new cube configs you're good to go",
    "start": "1557130",
    "end": "1563910"
  },
  {
    "text": "storage and position volumes something we're really really looking into and also check out all the vendors at the moment we're still adding in the support",
    "start": "1563910",
    "end": "1570540"
  },
  {
    "text": "to have the cloud stack volume so we can actually request a volume outside of the cluster to be mounted and for us this is",
    "start": "1570540",
    "end": "1577110"
  },
  {
    "text": "probably one most important features for getting more customers on board because we tend to have a lot of legacy software and things where they they expect to",
    "start": "1577110",
    "end": "1584130"
  },
  {
    "text": "have a file system that doesn't disappear randomly so that's definitely one of the big thing that I think we're trainer solving a lot",
    "start": "1584130",
    "end": "1589880"
  },
  {
    "text": "of other people as well and we're going to continue to develop classic integrations so as part of this we we",
    "start": "1589880",
    "end": "1597710"
  },
  {
    "text": "are having to reverse engineer a lot of the GCE and the eight of us integrations that are written basically just writing",
    "start": "1597710",
    "end": "1605480"
  },
  {
    "text": "that into CloudStack we're hoping to then use this knowledge that we gained to try and help commit to the",
    "start": "1605480",
    "end": "1610850"
  },
  {
    "text": "documentation so we're going to try and work on that as well and in general and",
    "start": "1610850",
    "end": "1616580"
  },
  {
    "text": "in addition security authorization and authentication we are have to basically",
    "start": "1616580",
    "end": "1621710"
  },
  {
    "text": "comply to audits so we for us that's also a very important thing I suppose",
    "start": "1621710",
    "end": "1627230"
  },
  {
    "text": "the other next step is open sourcing all this stuff like we keep promising yes we will definitely do that we promise and",
    "start": "1627230",
    "end": "1633110"
  },
  {
    "start": "1633000",
    "end": "1814000"
  },
  {
    "text": "that's it so I think one time I apologize for the lack of a demo if",
    "start": "1633110",
    "end": "1639139"
  },
  {
    "text": "people are interested as I said we're happy to come and show you this stuff I can I can pull it up if people want to",
    "start": "1639139",
    "end": "1646220"
  },
  {
    "text": "see does anybody have any questions yes that's correct so the question was",
    "start": "1646220",
    "end": "1660649"
  },
  {
    "text": "if the kubernetes masters are running on premise and the minions are running in AWS there's no communication between",
    "start": "1660649",
    "end": "1666169"
  },
  {
    "text": "them there's no overlay in between them that is correct the master components actually don't need to be inside your",
    "start": "1666169",
    "end": "1672740"
  },
  {
    "text": "overlay networking what we do is because you have cube DNS running inside your",
    "start": "1672740",
    "end": "1679100"
  },
  {
    "text": "management cluster if you create a service with an endpoint with the public IP then the actual pods inside the",
    "start": "1679100",
    "end": "1685879"
  },
  {
    "text": "management cluster will actually resolve the node names as that public IP and will then connect to them so this allows",
    "start": "1685879",
    "end": "1692570"
  },
  {
    "text": "things like queue TTL log and cube CTL exec to work there are a couple of assumptions in the code base which",
    "start": "1692570",
    "end": "1698990"
  },
  {
    "text": "assumes that the masters are able to communicate over overlay but I think cube CTO proxy which will found out but",
    "start": "1698990",
    "end": "1706490"
  },
  {
    "text": "it's it's more of a development feature so it's yeah not sure of something we want to keep supporting or not so the proxy will actually try and",
    "start": "1706490",
    "end": "1712940"
  },
  {
    "text": "connect to the cluster IP rather than to the node and then along the note port",
    "start": "1712940",
    "end": "1718159"
  },
  {
    "text": "forward through so that doesn't work but I that all all additional functionality works fine you're welcome",
    "start": "1718159",
    "end": "1726080"
  },
  {
    "text": "anybody else yes",
    "start": "1726080",
    "end": "1731090"
  },
  {
    "text": "that's good sorry",
    "start": "1731490",
    "end": "1737500"
  },
  {
    "text": "Oh Benitez yeah so the question was have",
    "start": "1737550",
    "end": "1746910"
  },
  {
    "text": "we looked at uber Nate it's now called Federation I did briefly mention it where we're trying to create multi cloud",
    "start": "1746910",
    "end": "1754679"
  },
  {
    "text": "clusters sorry we're trying to create clusters that are in various cloud and then use Federation to spread across",
    "start": "1754679",
    "end": "1760770"
  },
  {
    "text": "them yes the current problem that we're having is that AWS or when you're doing",
    "start": "1760770",
    "end": "1769260"
  },
  {
    "text": "multi clouds especially on-premise versus AWS versus GCE some of the things",
    "start": "1769260",
    "end": "1774570"
  },
  {
    "text": "like persistent volumes don't exist on our on-premise cloud stack and so trying to deploy something on the federated",
    "start": "1774570",
    "end": "1781050"
  },
  {
    "text": "control plane it will then quite happily spread out across your clusters which works great for small things and",
    "start": "1781050",
    "end": "1786510"
  },
  {
    "text": "stateless services but trying to then deploy one thing on the federated control plane that actually works across",
    "start": "1786510",
    "end": "1792900"
  },
  {
    "text": "all three different types of clouds has proven to be tricky we're still working on that please yeah definitely happy any",
    "start": "1792900",
    "end": "1804540"
  },
  {
    "text": "other questions no and that case thank you very much",
    "start": "1804540",
    "end": "1809670"
  },
  {
    "text": "thank you",
    "start": "1809670",
    "end": "1812270"
  }
]