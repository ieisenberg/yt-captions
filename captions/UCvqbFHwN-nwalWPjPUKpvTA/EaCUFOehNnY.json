[
  {
    "start": "0",
    "end": "470000"
  },
  {
    "text": "thanks for joining us for this late afternoon took on crew self-driving networking journey and so I'm Bernard",
    "start": "60",
    "end": "7170"
  },
  {
    "text": "and this is my colleague John we both walking okay",
    "start": "7170",
    "end": "13130"
  },
  {
    "text": "so we both walking to the platform as a service is this better yes okay we both walk into the platform",
    "start": "13130",
    "end": "20220"
  },
  {
    "text": "as a service team and more specifically into the traffic team and so this means we handle things like service mesh in",
    "start": "20220",
    "end": "26340"
  },
  {
    "text": "the talking issues ingress and so on and this is what we like to do when we don't need to debug ingresses okay so we both",
    "start": "26340",
    "end": "35460"
  },
  {
    "text": "fought for Cruz Cruz is a self-driving car company headquartered in San Francisco we are backed by a couple of",
    "start": "35460",
    "end": "42000"
  },
  {
    "text": "the giants in the automotive industry such as General Motors and Honda we are",
    "start": "42000",
    "end": "47940"
  },
  {
    "text": "aiming to build the world's most advanced self-driving vehicles to safely connect people of the places things and",
    "start": "47940",
    "end": "54420"
  },
  {
    "text": "experiences they care about and this means if you went over the last couple of weeks or month into the city to San",
    "start": "54420",
    "end": "61829"
  },
  {
    "text": "Francisco you probably saw some of those girls driving around they are still learning but eventually we want to make",
    "start": "61829",
    "end": "67979"
  },
  {
    "text": "these public service what you can see on this slide is an animation of webley's web this is one of our open source tools",
    "start": "67979",
    "end": "75600"
  },
  {
    "text": "that we use a lot internally as a front-end way for replaying some of the cattle rides they say ok I hope that you",
    "start": "75600",
    "end": "86040"
  },
  {
    "text": "are hearing me ok yes louder hey let's",
    "start": "86040",
    "end": "91829"
  },
  {
    "text": "fry ok my name is John and I am the technical lead of the path traffic team",
    "start": "91829",
    "end": "97110"
  },
  {
    "text": "at Cruz and our basic focus here is network reliability so our focus",
    "start": "97110",
    "end": "102810"
  },
  {
    "text": "overlaps with our journey here which covers like these multiple topics like we our journey will be spanning multiple",
    "start": "102810",
    "end": "109619"
  },
  {
    "text": "cloud providers and we have these multi tenant clusters that spans through multiple environments and multiple",
    "start": "109619",
    "end": "116159"
  },
  {
    "text": "regions our DNS story covers route 53 core DNS cloud DNS and it is not listed",
    "start": "116159",
    "end": "122430"
  },
  {
    "text": "here but unfortunately still cubed DNS and like we do have these private and public l7m al for load balancer and I",
    "start": "122430",
    "end": "130920"
  },
  {
    "text": "had the opportunities for being the one of the rare first pass team members so I was able to just experience this",
    "start": "130920",
    "end": "137730"
  },
  {
    "text": "growth where we have just spun up our very first Jiki cluster where we were running a few",
    "start": "137730",
    "end": "142860"
  },
  {
    "text": "pots on them and as of this week we are running somewhere around 7k pods in",
    "start": "142860",
    "end": "148230"
  },
  {
    "text": "average among all our clusters so basically we are going to cover these",
    "start": "148230",
    "end": "153840"
  },
  {
    "text": "five topics in general like network connectivity ingress traffic monitoring and logging security and hybrid DNS but",
    "start": "153840",
    "end": "160799"
  },
  {
    "text": "in not that order so the story starts",
    "start": "160799",
    "end": "166349"
  },
  {
    "text": "with provisioning our first gke clusters so we had three goals here so one of",
    "start": "166349",
    "end": "171660"
  },
  {
    "text": "them was we wanted to have an isolated cluster so we want to isolate it from",
    "start": "171660",
    "end": "177359"
  },
  {
    "text": "the public internet and we wanted to just connect this cluster to our internal network and we needed to have a",
    "start": "177359",
    "end": "184769"
  },
  {
    "text": "repeatable configuration among these three different environments and we have",
    "start": "184769",
    "end": "190769"
  },
  {
    "text": "picked some RFC 1918 compliance like cider blocks which are class A's for the",
    "start": "190769",
    "end": "197760"
  },
  {
    "text": "notes B's for the parts and C's for the services okay this is the first attempt",
    "start": "197760",
    "end": "205380"
  },
  {
    "text": "step let's try to connect from this part a to a service running an AWS so initial",
    "start": "205380",
    "end": "211980"
  },
  {
    "text": "item only included VPN tunnels because why VPN tunnels it was easy to set up things and then the traffic was going",
    "start": "211980",
    "end": "220019"
  },
  {
    "text": "through like public internet through IPSec encryption and also we didn't need",
    "start": "220019",
    "end": "225840"
  },
  {
    "text": "to disturb our neurons team so we were able to just set up things by ourselves",
    "start": "225840",
    "end": "232010"
  },
  {
    "text": "and the first problem so what went wrong we made this first request oh okay maybe",
    "start": "232010",
    "end": "238319"
  },
  {
    "text": "I should just put this I think it is better now anyway so the first request from this part a to service B and the",
    "start": "238319",
    "end": "245430"
  },
  {
    "text": "request timeout so what was wrong here like we were pretty sure that we have",
    "start": "245430",
    "end": "250650"
  },
  {
    "text": "defined our return routes because we were able to access from this GCE instance to this service on AWS and",
    "start": "250650",
    "end": "258000"
  },
  {
    "text": "everything is looking fine but it didn't work for the GK cluster the main reason",
    "start": "258000",
    "end": "263340"
  },
  {
    "text": "was these private gke clusters by default were coming up with IP mask agents",
    "start": "263340",
    "end": "269639"
  },
  {
    "text": "installed and with that said like even though we had these return routes for",
    "start": "269639",
    "end": "274770"
  },
  {
    "text": "nose we didn't have any return routes defined for our parts and that was why as soon as the request or the response",
    "start": "274770",
    "end": "281580"
  },
  {
    "text": "hits the customer gateway the packages were getting dropped and because this one similar to something IP address was",
    "start": "281580",
    "end": "288000"
  },
  {
    "text": "on them and the solution was tweaking IP mask configurations so the left-hand",
    "start": "288000",
    "end": "294060"
  },
  {
    "text": "side had the default IP mask configurations where it thinks that the whole internal network is a flat network",
    "start": "294060",
    "end": "300210"
  },
  {
    "text": "so we are able to reach everything else around there but it wasn't true on our case because we didn't we specifically",
    "start": "300210",
    "end": "307470"
  },
  {
    "text": "didn't advertise these class B's and class C's because we were planning to reuse them across the different",
    "start": "307470",
    "end": "313100"
  },
  {
    "text": "environments and that was why and whenever you make a request to something",
    "start": "313100",
    "end": "318330"
  },
  {
    "text": "with Class A address IP mask wasn't masquerading or source netting the",
    "start": "318330",
    "end": "324000"
  },
  {
    "text": "request and that was causing trouble so for this particular case we have picked",
    "start": "324000",
    "end": "329430"
  },
  {
    "text": "these side range that the cluster was running on so there's this small detail here so the cluster was running on this",
    "start": "329430",
    "end": "336539"
  },
  {
    "text": "side of range and like it matches with this and this is just a highlight of",
    "start": "336539",
    "end": "343350"
  },
  {
    "text": "overview of what we have this is a common slide that we use internally so",
    "start": "343350",
    "end": "348630"
  },
  {
    "text": "we do have these three regions they're staging and proud they are all connected to each other and then on all these",
    "start": "348630",
    "end": "355740"
  },
  {
    "text": "environments we are running multiple clusters for multiple purposes or on multiple regions so this is the topology",
    "start": "355740",
    "end": "362700"
  },
  {
    "text": "that we have okay so you probably you already have the question like how are",
    "start": "362700",
    "end": "368190"
  },
  {
    "text": "these guys in scale with VPN tunnels no we can't so the idea or the goal here or",
    "start": "368190",
    "end": "373620"
  },
  {
    "text": "the problem here was we like it's pretty easy or straightforward to hear into",
    "start": "373620",
    "end": "378810"
  },
  {
    "text": "this N squared tunnel problem because whenever you introduce a new network you need to just establish VPN connections",
    "start": "378810",
    "end": "385860"
  },
  {
    "text": "with all other networks that you have you need to manually or statically just update all your route tables which is",
    "start": "385860",
    "end": "392760"
  },
  {
    "text": "gonna be tedious and like chaotic pretty soon and also like IPSec calls or the",
    "start": "392760",
    "end": "399060"
  },
  {
    "text": "whole this public Internet with IPSec like it's gonna be a reduced performance problem and the",
    "start": "399060",
    "end": "406440"
  },
  {
    "text": "network engineering team solution here was leveraging interconnects so with that we were able to connect all",
    "start": "406440",
    "end": "413970"
  },
  {
    "text": "every new network to these interconnects which provided us like a couple of",
    "start": "413970",
    "end": "420570"
  },
  {
    "text": "things we had these bgp routers they were responsible for dynamically advertising all these rels internally so",
    "start": "420570",
    "end": "428850"
  },
  {
    "text": "we don't need to manually manage them that was one thing and we also had physical dedicated connections between",
    "start": "428850",
    "end": "434730"
  },
  {
    "text": "interconnects and other networks and which provided us like to like get rid",
    "start": "434730",
    "end": "440670"
  },
  {
    "text": "of IPSec that was one thing and the other thing was we had like improved bandwidth which was around 100 gigabit",
    "start": "440670",
    "end": "447960"
  },
  {
    "text": "per second which is also a good game and yep that's pretty much it so normally",
    "start": "447960",
    "end": "455070"
  },
  {
    "text": "this is very highlighted overview of how things are set up on a networking level there are more details so if you want",
    "start": "455070",
    "end": "462090"
  },
  {
    "text": "you can check out this link and shout out the Carl and Buck who put this blog",
    "start": "462090",
    "end": "467370"
  },
  {
    "text": "post together thanks John and again so",
    "start": "467370",
    "end": "474030"
  },
  {
    "start": "470000",
    "end": "618000"
  },
  {
    "text": "we started to scale always until your crystals at some point so when we reach the critical mass for a single cluster",
    "start": "474030",
    "end": "480150"
  },
  {
    "text": "we decided to create more clusters for indictment so this means more cluster in dev was called more cluster and staging",
    "start": "480150",
    "end": "487020"
  },
  {
    "text": "and input and the way we decided to do this was to create one subnet on the BBC rock crystal and the first challenge was",
    "start": "487020",
    "end": "494490"
  },
  {
    "text": "for each cluster you need a new set of IP ranges as you probably know in kubernetes you need one IP range for",
    "start": "494490",
    "end": "501000"
  },
  {
    "text": "your nodes one for your pods and one for your services and so it's kind of",
    "start": "501000",
    "end": "506160"
  },
  {
    "text": "difficult to to choose those IP ranges automatically because they depend on",
    "start": "506160",
    "end": "511520"
  },
  {
    "text": "which channel environment you are in it would be different if you're in the dev VPC or staging the PC which region I",
    "start": "511520",
    "end": "518430"
  },
  {
    "text": "want your cluster to be in as well as what's the sizing of the cluster the sizing of the cluster is kind of tricky",
    "start": "518430",
    "end": "524099"
  },
  {
    "text": "because you need to know in advance how big your cluster was eventually going to be because once you create a custom in a",
    "start": "524099",
    "end": "529890"
  },
  {
    "text": "subnet it's extremely difficult to resize it to bigger IP range and so we came with a set of",
    "start": "529890",
    "end": "536939"
  },
  {
    "text": "constituents we wanted on node IPS to be globally unique we wanted to do this so that from anywhere inside Cruz it would",
    "start": "536939",
    "end": "543420"
  },
  {
    "text": "be easy to reach any of the node IPS from the cluster we also want to the pod eyepiece and Service IPS to be only",
    "start": "543420",
    "end": "550019"
  },
  {
    "text": "locally unique per VPC so those could be used across V pcs so you could reuse the",
    "start": "550019",
    "end": "556259"
  },
  {
    "text": "same pod IPS in dev and in staging for example so the challenge here was that it was extremely tedious to choose",
    "start": "556259",
    "end": "563660"
  },
  {
    "text": "manually all of those IP ranges based on those constraints so what we did a",
    "start": "563660",
    "end": "569490"
  },
  {
    "text": "couple of months ago is to automate this by using Jerry say hi to Jerry Jerry's",
    "start": "569490",
    "end": "575249"
  },
  {
    "text": "small tool that is automatically connecting to open IP management tool in",
    "start": "575249",
    "end": "580620"
  },
  {
    "text": "the backend we use networks at cruise and it's so time a quickly going to use those constituents as parameters and you",
    "start": "580620",
    "end": "587639"
  },
  {
    "text": "will get assigned a set of IP ranges for your clusters so this is how we automatically create clusters based on",
    "start": "587639",
    "end": "593879"
  },
  {
    "text": "those IP ranges and by the way we learned something on the way is that it's quite valuable to do the meta view",
    "start": "593879",
    "end": "601019"
  },
  {
    "text": "of all your crystals in a single place for us it's netbooks because it said it will help you to debug some crazy corner",
    "start": "601019",
    "end": "607949"
  },
  {
    "text": "cases when something goes wrong and you need to reach one janilla based on the source IP which bud which node IP it",
    "start": "607949",
    "end": "615269"
  },
  {
    "text": "comes from from each cluster ok next thing I would like to talk about is",
    "start": "615269",
    "end": "621329"
  },
  {
    "start": "618000",
    "end": "948000"
  },
  {
    "text": "service ingestion and as the traffic team this is one of the main things we deal with day today and we talk to a lot",
    "start": "621329",
    "end": "628079"
  },
  {
    "text": "of internal tenants and they all want to somehow make their service available that makes sense right so we made this",
    "start": "628079",
    "end": "635370"
  },
  {
    "text": "chat flow it basically it it will make it easy for customers to decide on",
    "start": "635370",
    "end": "641699"
  },
  {
    "text": "internal talent which type of service they need to use in order to make the micro service or pods available so let's",
    "start": "641699",
    "end": "649230"
  },
  {
    "text": "go to this the first one that makes the most sense is what if you are inside the",
    "start": "649230",
    "end": "654360"
  },
  {
    "text": "same cluster in which case very simply you just need to create a community service this works out of the box we",
    "start": "654360",
    "end": "660720"
  },
  {
    "text": "will use it it's fine now ok next one what if you're inside Cruz but not in",
    "start": "660720",
    "end": "666389"
  },
  {
    "text": "the same cluster and you TCP or UDP circuit to something like layer 3 layer 4 for this we allow our",
    "start": "666389",
    "end": "674360"
  },
  {
    "text": "internal tenants to use self-service themselves with a service of two type",
    "start": "674360",
    "end": "679590"
  },
  {
    "text": "load balancer with annotation and this will automatically create an internal load balancer and make whatever micro",
    "start": "679590",
    "end": "686130"
  },
  {
    "text": "services behind it available all our crews ok the next one",
    "start": "686130",
    "end": "692760"
  },
  {
    "text": "I want to spend a bit more time on it because it's what we use the most at cause it's a layer of seven ingress so",
    "start": "692760",
    "end": "698580"
  },
  {
    "text": "for private traffic this probably represents about 95% of traffic at",
    "start": "698580",
    "end": "704430"
  },
  {
    "text": "cruise it's basically layer 7 internal traffic coming from other clusters other end",
    "start": "704430",
    "end": "710430"
  },
  {
    "text": "points or other VMs or something and we decided to use the standard ingress resource to deal with this we also",
    "start": "710430",
    "end": "716940"
  },
  {
    "text": "decided to use upstream and genex ingress and we are pretty happy with it we started by deploying this as a",
    "start": "716940",
    "end": "723930"
  },
  {
    "text": "standard in cluster set of pods the only tweak we made was to use a dedicated node pool to make sure that we don't get",
    "start": "723930",
    "end": "730710"
  },
  {
    "text": "into some type of resource congestion or like noise enable so that each and genex",
    "start": "730710",
    "end": "736260"
  },
  {
    "text": "ingress controller could use the full node for himself and we also enabled one",
    "start": "736260",
    "end": "744510"
  },
  {
    "text": "of those parameters called excel traffic policy which allows the load balancer in front to help check each of the nodes in",
    "start": "744510",
    "end": "751590"
  },
  {
    "text": "a way that the node will only be available if an ingress Ingenix instances is present on that node so why",
    "start": "751590",
    "end": "759600"
  },
  {
    "text": "do we want to do this because you don't want to send traffic to a node that doesn't even nginx ingress instance",
    "start": "759600",
    "end": "764910"
  },
  {
    "text": "because if you do that queue pox is going to redirect it to another node with an actual instance available so by",
    "start": "764910",
    "end": "771420"
  },
  {
    "text": "doing this you basically avoid in the worst case one extra hop so it's better for performances so if we had the first",
    "start": "771420",
    "end": "780120"
  },
  {
    "text": "big outage on the 4th of July by using this we called it the firewalk outage what happened is we found out",
    "start": "780120",
    "end": "787830"
  },
  {
    "text": "that the internal load balancers from Google can only health check maximum total 50 nodes and so when you have a",
    "start": "787830",
    "end": "794460"
  },
  {
    "text": "cluster of way beyond on tune or 50 nodes what happens is 250 nodes will be selected randomly to be health checked",
    "start": "794460",
    "end": "801870"
  },
  {
    "text": "we are the crystal so big that all the nginx ingress instances randomly girl toads or the health check so this black",
    "start": "801870",
    "end": "807660"
  },
  {
    "text": "hole the traffic and yeah that was quite a big outage so we decided as a",
    "start": "807660",
    "end": "812720"
  },
  {
    "text": "restaurant alone to decouple nginx ingress from kubernetes and to move it",
    "start": "812720",
    "end": "818010"
  },
  {
    "text": "out of the cluster so we didn't find a lot of people doing this but we are quite happy with this new configuration",
    "start": "818010",
    "end": "825529"
  },
  {
    "text": "basically the way we manage this now is it's into an in management scope on GC directly which means it can be scaled",
    "start": "825529",
    "end": "832560"
  },
  {
    "text": "completely independently from kubernetes we scale it to de based on the CPU usage",
    "start": "832560",
    "end": "838130"
  },
  {
    "text": "it's still opening a kubernetes api watch to the kubernetes cluster doing",
    "start": "838130",
    "end": "843750"
  },
  {
    "text": "this out of the cluster and the traffic is still sent directly to the pods so you don't really lose any performance",
    "start": "843750",
    "end": "849660"
  },
  {
    "text": "during this ok let's talk a bit about public traffic so we don't have today a",
    "start": "849660",
    "end": "856710"
  },
  {
    "text": "lot of public traffic but we still support it in the same way that we support private traffic so that means if",
    "start": "856710",
    "end": "861930"
  },
  {
    "text": "you need TCP or UDP sockets what we do is but we highly discourage",
    "start": "861930",
    "end": "867240"
  },
  {
    "text": "this but if you need it we will create a public network balanced load balancer for you if a public IP and it's up to",
    "start": "867240",
    "end": "874200"
  },
  {
    "text": "your workload 200m TLS and firewall it heavily I suggest we push people to use layer 7",
    "start": "874200",
    "end": "881790"
  },
  {
    "text": "so we use also ingress for public traffic we initially use LDC",
    "start": "881790",
    "end": "888230"
  },
  {
    "text": "we but we decided to standardize on nginx ingress because we wanted to the",
    "start": "888230",
    "end": "893400"
  },
  {
    "text": "same set of metrics both for private and for public the same set of logs and same set of metrics it makes it way easier to",
    "start": "893400",
    "end": "899580"
  },
  {
    "text": "to manage so what we did is simply we duplicated the private set up for ingress and made it public and we",
    "start": "899580",
    "end": "907200"
  },
  {
    "text": "differentiate both of those increases with annotation for different annotation and so this is what swimming today on",
    "start": "907200",
    "end": "914640"
  },
  {
    "text": "our crystals so which cluster we got one public and one private out of the",
    "start": "914640",
    "end": "919740"
  },
  {
    "text": "cluster ingress ok and this brings me to",
    "start": "919740",
    "end": "924990"
  },
  {
    "text": "one lesson that we learn is you want to support a small subset of options but",
    "start": "924990",
    "end": "931020"
  },
  {
    "text": "support them well what we do as part of the traffic team is push people to use layer seven ingress",
    "start": "931020",
    "end": "936900"
  },
  {
    "text": "and that's why we supported the best what you can see on this slide is the two type of annotations that we document",
    "start": "936900",
    "end": "943190"
  },
  {
    "text": "internally for using the private or public ingress so let's talk about a",
    "start": "943190",
    "end": "952200"
  },
  {
    "start": "948000",
    "end": "1180000"
  },
  {
    "text": "little bit about - our DNS story here so again I will start with the most naive",
    "start": "952200",
    "end": "958110"
  },
  {
    "text": "approach that we fell out here which is okay we do have all our records stored",
    "start": "958110",
    "end": "963120"
  },
  {
    "text": "on as route 53 records and then we have",
    "start": "963120",
    "end": "968160"
  },
  {
    "text": "these DNS mask agents running on AWS which were pulling these like DNS",
    "start": "968160",
    "end": "975600"
  },
  {
    "text": "records so with this initial attempt what we have done here is we have updated cube DNS top domains so whenever",
    "start": "975600",
    "end": "982530"
  },
  {
    "text": "there's some internal request it was forwarding these requests to the DNS proxy that were running in another",
    "start": "982530",
    "end": "989430"
  },
  {
    "text": "network and like problem solved at least for a month or so it was solved and then",
    "start": "989430",
    "end": "996330"
  },
  {
    "text": "like we like started running into some latency issues we because we were just",
    "start": "996330",
    "end": "1001730"
  },
  {
    "text": "having these additional network hops which were not ideal at all the second",
    "start": "1001730",
    "end": "1008600"
  },
  {
    "text": "attempt okay we noticed that we can use coordinates because it already has this",
    "start": "1008600",
    "end": "1014420"
  },
  {
    "text": "route 2 to 3 plug in and it's gonna just solve the problem for us was it like",
    "start": "1014420",
    "end": "1021160"
  },
  {
    "text": "this was one of the other moments that we realized that there is no free lunch and then by default coordinates brought",
    "start": "1021160",
    "end": "1028550"
  },
  {
    "text": "53 back-end only supported a records and also it was programmed to fetch all",
    "start": "1028550",
    "end": "1034069"
  },
  {
    "text": "these records one by one and considering that you have hundreds of instances and AWS has this like five requests per",
    "start": "1034070",
    "end": "1042949"
  },
  {
    "text": "second throttling limitation like it was an ideal so another shout-out to a",
    "start": "1042949",
    "end": "1048199"
  },
  {
    "text": "former colleague dimitri did the heavy lifting here he did he contributed to",
    "start": "1048199",
    "end": "1053630"
  },
  {
    "text": "the plugin and then i made things product ready and what happened in the",
    "start": "1053630",
    "end": "1059060"
  },
  {
    "text": "end was we were able to just fetch all the records with the exception of alias records which was kind of fine for the",
    "start": "1059060",
    "end": "1064940"
  },
  {
    "text": "time and then we did the exact same configuration here so cube DNS was",
    "start": "1064940",
    "end": "1071039"
  },
  {
    "text": "configured through stop domain so we were running these coordinates instances by the way externally these were not",
    "start": "1071039",
    "end": "1077460"
  },
  {
    "text": "running on the cluster these were external and cubed DNS was for being",
    "start": "1077460",
    "end": "1082770"
  },
  {
    "text": "conditional forwarding these coordinates instances and core DNS was periodically syncing AWS records it wasn't",
    "start": "1082770",
    "end": "1089940"
  },
  {
    "text": "synchronously forwarding the request to AWS that was the critical point okay so",
    "start": "1089940",
    "end": "1096660"
  },
  {
    "text": "that's mitigated the problem for us but there was one other problem left here",
    "start": "1096660",
    "end": "1101940"
  },
  {
    "text": "okay we have resolved a problem for kubernetes but what about the other GCE instances because like they were still",
    "start": "1101940",
    "end": "1109140"
  },
  {
    "text": "lacking office functionality and we were being lucky back in the days because I",
    "start": "1109140",
    "end": "1114600"
  },
  {
    "text": "think they Google introduced the very first version of cloudiness and as soon",
    "start": "1114600",
    "end": "1119640"
  },
  {
    "text": "as it came up or as soon as we enable this like we were able to define these",
    "start": "1119640",
    "end": "1125640"
  },
  {
    "text": "conditional forwarders for our internal domains so the picture here is when you",
    "start": "1125640",
    "end": "1130799"
  },
  {
    "text": "enable this any of your GC instances like forwards the request metadata",
    "start": "1130799",
    "end": "1135990"
  },
  {
    "text": "server from there it hits cloud DNS from cloud pianist core DNS and so forth so",
    "start": "1135990",
    "end": "1146460"
  },
  {
    "text": "by the way like that I also noticed something here I'm just going a little bit sideways here yesterday I attended",
    "start": "1146460",
    "end": "1152870"
  },
  {
    "text": "one of the core maintainer of Cordy NS Yoong and then his whole talk was based",
    "start": "1152870",
    "end": "1160110"
  },
  {
    "text": "on hybrid DNS structure where he used or showcase AWS and cloud DNS backends and",
    "start": "1160110",
    "end": "1166620"
  },
  {
    "text": "I felt like that okay at least we had some sort of impact on the open source community with this contribution because",
    "start": "1166620",
    "end": "1173429"
  },
  {
    "text": "we otherwise like probably it wouldn't be included in the demo so okay aside",
    "start": "1173429",
    "end": "1184799"
  },
  {
    "start": "1180000",
    "end": "1330000"
  },
  {
    "text": "from these DNS servers we also have some common DNS structure at cruise which is",
    "start": "1184799",
    "end": "1190110"
  },
  {
    "text": "we do have these a records which are pointing to our load balancers and these",
    "start": "1190110",
    "end": "1195780"
  },
  {
    "text": "are multi tenant clusters so whenever they like define a new DNS records",
    "start": "1195780",
    "end": "1201810"
  },
  {
    "text": "instead of you directly using the load balance or IP they are using these C names that we like create it so the",
    "start": "1201810",
    "end": "1210000"
  },
  {
    "text": "advantage of having this sort of approach is whenever you make a change on your load balancer level such as like",
    "start": "1210000",
    "end": "1216150"
  },
  {
    "text": "a static IP change or you introduce a brand new a load balancer you don't need",
    "start": "1216150",
    "end": "1221700"
  },
  {
    "text": "to just update all your records around instead you just update your a records and problem solved good ok next thing I",
    "start": "1221700",
    "end": "1232530"
  },
  {
    "text": "would like to talk about is observability and logging and metrics so",
    "start": "1232530",
    "end": "1237540"
  },
  {
    "text": "as part of the traffic team one of the things we need to deal with the most is of internal tenants asking us did you",
    "start": "1237540",
    "end": "1243780"
  },
  {
    "text": "see that flow coming to your english gateways and that's why we made a point of having really good ingress logs so",
    "start": "1243780",
    "end": "1250830"
  },
  {
    "text": "every hit to our ingress gateways with recent result into one log and stackdriver force we tweeted a bit to",
    "start": "1250830",
    "end": "1258180"
  },
  {
    "text": "add the ingress name and ingress namespace so that it's really easy for people to filter on so you can find in a",
    "start": "1258180",
    "end": "1264450"
  },
  {
    "text": "single click every traffic going to our gateways next thing is metrics so the",
    "start": "1264450",
    "end": "1271980"
  },
  {
    "text": "same and janux ingress gateways actually reporting a lot of metrics very valuable metrics we added in again the ingress",
    "start": "1271980",
    "end": "1279720"
  },
  {
    "text": "name and namespace as a parameter so that people can filter really easily on it part of those metrics is how many what",
    "start": "1279720",
    "end": "1286620"
  },
  {
    "text": "HTTP status status return you can see graphs of those things you can actually set up alerts and a lot of internal",
    "start": "1286620",
    "end": "1293280"
  },
  {
    "text": "teams are using this as a way to alert if their internal services don't know and by the way this is brings us to",
    "start": "1293280",
    "end": "1301740"
  },
  {
    "text": "another lesson learned is we got an internal team called the Geno team which is building high or layer of abstraction",
    "start": "1301740",
    "end": "1309600"
  },
  {
    "text": "on top of our kubernetes services and from this general content tool you can",
    "start": "1309600",
    "end": "1315270"
  },
  {
    "text": "directly in one single click access all your matrix and login for your services and so this is very valuable for people",
    "start": "1315270",
    "end": "1321810"
  },
  {
    "text": "so that we can directly go and find their logs and metrics and should shout out to the observability energy routine",
    "start": "1321810",
    "end": "1330230"
  },
  {
    "start": "1330000",
    "end": "1450000"
  },
  {
    "text": "okay so there is another kind of fun story so okay ingress monitoring one of the or",
    "start": "1331570",
    "end": "1340450"
  },
  {
    "text": "we face this particular incident a couple of times so we are just sleeping",
    "start": "1340450",
    "end": "1346270"
  },
  {
    "text": "well on our beds and then everything is looking fine we didn't receive we don't receive any alerts or anything at all",
    "start": "1346270",
    "end": "1352900"
  },
  {
    "text": "but the problem is as soon as we wake up we are just with us we start receiving some clients like calls which is okay",
    "start": "1352900",
    "end": "1361150"
  },
  {
    "text": "the or load balancer is down usually they are not saying that they are just saying that DNS is isn't working so",
    "start": "1361150",
    "end": "1366520"
  },
  {
    "text": "there's a problem there and okay like the problem here is mean time to",
    "start": "1366520",
    "end": "1373930"
  },
  {
    "text": "response is pretty high I think about that like we are responding within a couple of hours when an incident happens",
    "start": "1373930",
    "end": "1379740"
  },
  {
    "text": "so how can we mitigate this problem so",
    "start": "1379740",
    "end": "1384880"
  },
  {
    "text": "okay we need a bunch of progress around and we need to just pick one of the",
    "start": "1384880",
    "end": "1390420"
  },
  {
    "text": "existing SAS products and this has product needs to work for both of our",
    "start": "1390420",
    "end": "1395860"
  },
  {
    "text": "private and public endpoints because whenever you have this choice like",
    "start": "1395860",
    "end": "1400990"
  },
  {
    "text": "you're gonna just eliminate most of the options there and eventually we were planning to help our tenants to identify",
    "start": "1400990",
    "end": "1407800"
  },
  {
    "text": "problems early on so we pick round scope for this purposes and we also developed",
    "start": "1407800",
    "end": "1414250"
  },
  {
    "text": "a round scope controller thing why run scope because it's supported both public and private endpoints we",
    "start": "1414250",
    "end": "1420790"
  },
  {
    "text": "were able to just run some data log 8 sorry ruskov agents internally and they",
    "start": "1420790",
    "end": "1425890"
  },
  {
    "text": "were allowed to just ping our internal endpoints and then we built this run scope controller thing around that and",
    "start": "1425890",
    "end": "1432990"
  },
  {
    "text": "the idea here was just monitoring ingress resources and then picking these",
    "start": "1432990",
    "end": "1438670"
  },
  {
    "text": "hostname and service back-end pairs and then create creating round scope tests",
    "start": "1438670",
    "end": "1444220"
  },
  {
    "text": "as a result and like it worked fine so",
    "start": "1444220",
    "end": "1449700"
  },
  {
    "text": "these are the some of the annotation that it had I'm not going to get into the details regarding round scope stuff",
    "start": "1449820",
    "end": "1456550"
  },
  {
    "text": "so basically what we can focus here are so you're gonna just say that I am",
    "start": "1456550",
    "end": "1462760"
  },
  {
    "text": "enabling API tests for my ingress this is the health checking path and we're going to run this with a scale and",
    "start": "1462760",
    "end": "1470050"
  },
  {
    "text": "internal and this is the prefix where we can prioritize your stuff which is an",
    "start": "1470050",
    "end": "1475780"
  },
  {
    "text": "optional and internal thing for us okay now the lesson learned part so the first",
    "start": "1475780",
    "end": "1483580"
  },
  {
    "start": "1479000",
    "end": "1563000"
  },
  {
    "text": "lesson learned was like we noticed that it is not an easy job to create",
    "start": "1483580",
    "end": "1489100"
  },
  {
    "text": "controllers like I was under estimating the work about that we are gonna do a couple of crowd requests how hard can it",
    "start": "1489100",
    "end": "1495040"
  },
  {
    "text": "be but well it was hard but aside from",
    "start": "1495040",
    "end": "1501070"
  },
  {
    "text": "that the second kubernetes related problem here was okay we had this",
    "start": "1501070",
    "end": "1506590"
  },
  {
    "text": "internal proper running on our cluster which was pinging the sir given service IP and thought afterwards and again a",
    "start": "1506590",
    "end": "1515590"
  },
  {
    "text": "customer just reported and all this here and there again said that internal load balancer isn't working or DNS isn't",
    "start": "1515590",
    "end": "1521770"
  },
  {
    "text": "working whatever and okay but Roberts saying the opposite here what is wrong",
    "start": "1521770",
    "end": "1528160"
  },
  {
    "text": "and the problem here was when you have low balancer type services depending on",
    "start": "1528160",
    "end": "1535840"
  },
  {
    "text": "the IP tables rules kubernetes doesn't do hair pinning so instead of going outside of the cluster and the request",
    "start": "1535840",
    "end": "1542410"
  },
  {
    "text": "coming back inside it just it is just directly hitting these Service IPS so we were unable to identify these load",
    "start": "1542410",
    "end": "1548980"
  },
  {
    "text": "balancer problems by running agents only internally so as a solution we started",
    "start": "1548980",
    "end": "1554860"
  },
  {
    "text": "just having these cross cluster robbers where we started using all over the",
    "start": "1554860",
    "end": "1560590"
  },
  {
    "text": "place okay thank you John let's talk a",
    "start": "1560590",
    "end": "1566620"
  },
  {
    "start": "1563000",
    "end": "1620000"
  },
  {
    "text": "bit about security how do we secure our clusters by the way I want to start this",
    "start": "1566620",
    "end": "1571930"
  },
  {
    "text": "by saying that at cruise we push every workload to use open and Ozzie that's kind of out of scope for this slide but",
    "start": "1571930",
    "end": "1578980"
  },
  {
    "text": "from a pure networking perspective familiar through layoffs ooh we have the following model so from inside the VTC",
    "start": "1578980",
    "end": "1585010"
  },
  {
    "text": "by default anything inside of a PC is able to reach the cluster the pod IPS and the service IPS from outside the V",
    "start": "1585010",
    "end": "1591910"
  },
  {
    "text": "PC by default it's a whitelist only so you need to add an entry to be able to access a cluster or",
    "start": "1591910",
    "end": "1598330"
  },
  {
    "text": "Pacific IP and so for English we explicitly allowed of ingress",
    "start": "1598330",
    "end": "1605049"
  },
  {
    "text": "controllers on the whitelist and so yeah the the other thing we don't do today is",
    "start": "1605049",
    "end": "1611140"
  },
  {
    "text": "we don't have network policies enabled on our clusters but we are actually considering enabling maybe a default",
    "start": "1611140",
    "end": "1617110"
  },
  {
    "text": "namespace isolation on that level okay something else that you might have heard",
    "start": "1617110",
    "end": "1622990"
  },
  {
    "start": "1620000",
    "end": "1688000"
  },
  {
    "text": "about before we released Kol it's on github that it was produced by our",
    "start": "1622990",
    "end": "1629289"
  },
  {
    "text": "sister team the security team and QA listed on mission webhook controller the",
    "start": "1629289",
    "end": "1634899"
  },
  {
    "text": "way it works is whenever you create a new deployment or new set of pods it will validate it and make sure that",
    "start": "1634899",
    "end": "1641289"
  },
  {
    "text": "there is no forbidden or insecure parameters so for example you don't want to add a pod with host network enabled",
    "start": "1641289",
    "end": "1647470"
  },
  {
    "text": "you don't want to have extra network capabilities etc etc the cool one is you",
    "start": "1647470",
    "end": "1652630"
  },
  {
    "text": "also don't want to enable by default any public ingress to be created you want those to be white listed so that by diff",
    "start": "1652630",
    "end": "1660190"
  },
  {
    "text": "so that someone doesn't create like a quick endpoint by mistake for example and so something we learned is if you",
    "start": "1660190",
    "end": "1667480"
  },
  {
    "text": "are going to block some of the user requests you need to give a pretty user friendly outputs so Kol does this it",
    "start": "1667480",
    "end": "1673809"
  },
  {
    "text": "will actually tell you exactly why your pod or deployment is being rejected and",
    "start": "1673809",
    "end": "1679210"
  },
  {
    "text": "so the world the role model behind Kol is by default denying a lot of things and then we can add exemption and",
    "start": "1679210",
    "end": "1684490"
  },
  {
    "text": "whitelist some things to be allowed to okay so let's close on this with some of",
    "start": "1684490",
    "end": "1691539"
  },
  {
    "start": "1688000",
    "end": "1723000"
  },
  {
    "text": "the current challenges that we are facing today the first one and maybe the most pressing one is multi cluster",
    "start": "1691539",
    "end": "1697899"
  },
  {
    "text": "specifically multi cluster ingress we have that point in which we are load balancing a lot of workloads across",
    "start": "1697899",
    "end": "1704799"
  },
  {
    "text": "multiple clusters but you don't have a clear way today to accept traffic in a",
    "start": "1704799",
    "end": "1710260"
  },
  {
    "text": "single point and we directed smartly to different clusters at least not internal traffic so we are looking at this it",
    "start": "1710260",
    "end": "1716590"
  },
  {
    "text": "still might be an answer we are not sure yet so yeah we are looking at different options so another problem that we have",
    "start": "1716590",
    "end": "1725740"
  },
  {
    "start": "1723000",
    "end": "1774000"
  },
  {
    "text": "faced during of one of our other outages was like the lack of visibility",
    "start": "1725740",
    "end": "1731710"
  },
  {
    "text": "with that what we mean is we do have these multi-tenant clusters where every node is running at this 50 pots in",
    "start": "1731710",
    "end": "1739060"
  },
  {
    "text": "average and then whenever a couple of those starts saturating your egress",
    "start": "1739060",
    "end": "1744490"
  },
  {
    "text": "traffic which might be happening through your net instances we are unable to say",
    "start": "1744490",
    "end": "1749740"
  },
  {
    "text": "or identify which one of those work those are causing this because we were",
    "start": "1749740",
    "end": "1754840"
  },
  {
    "text": "able to just address the particular node but we don't have the granularity on the pot level so that's as one of the",
    "start": "1754840",
    "end": "1762700"
  },
  {
    "text": "reasons why we started looking into service mesh solutions as East you specifically and we are hoping to just",
    "start": "1762700",
    "end": "1770290"
  },
  {
    "text": "come up with better dashboards error around and another issue here is like",
    "start": "1770290",
    "end": "1777660"
  },
  {
    "start": "1774000",
    "end": "1922000"
  },
  {
    "text": "okay aside from having improved visibility the other problem is we have",
    "start": "1777660",
    "end": "1782860"
  },
  {
    "text": "a couple of levels of isolation isolation zone are multitaskers so",
    "start": "1782860",
    "end": "1788440"
  },
  {
    "text": "doctor already provides CPU and memory level isolation is fine but there are",
    "start": "1788440",
    "end": "1793690"
  },
  {
    "text": "still two other levels of isolations are missing one is disk i/o and luckily it is not a problem of past traffic team",
    "start": "1793690",
    "end": "1799990"
  },
  {
    "text": "and the second one is network like Q",
    "start": "1799990",
    "end": "1806140"
  },
  {
    "text": "quality of service stuff because like we would like to have some sort of egress and ingress traffic limitation set up",
    "start": "1806140",
    "end": "1813120"
  },
  {
    "text": "and the other thing is we still need to do a couple of DNS enhancements with",
    "start": "1813120",
    "end": "1819790"
  },
  {
    "text": "this said what we mean is okay we are just facing the exact same problems with everyone else regarding cube DNS and we",
    "start": "1819790",
    "end": "1827170"
  },
  {
    "text": "liked it we don't want to just get details of that because they are already a couple of good talks around those and",
    "start": "1827170",
    "end": "1833170"
  },
  {
    "text": "our problems are or the solutions here will be replacing cube DNS with coordinates and later on enabling not",
    "start": "1833170",
    "end": "1840010"
  },
  {
    "text": "local DNS cache and as a third piece we also have this like delegation ownership",
    "start": "1840010",
    "end": "1848470"
  },
  {
    "text": "problem because as we have shown all the records are or at least most of the records are stored on AWS so we would",
    "start": "1848470",
    "end": "1855310"
  },
  {
    "text": "like to just create subdomains and delegate the ownership to some sub teams and still the DNS resolution should",
    "start": "1855310",
    "end": "1863660"
  },
  {
    "text": "and last but not least so we would like to just come up with our law testing",
    "start": "1863660",
    "end": "1869850"
  },
  {
    "text": "framework because whenever years start talking about Easter and service Mash there is this additional latency cost",
    "start": "1869850",
    "end": "1876120"
  },
  {
    "text": "and we would like to make sure about that it is not gonna be a problem for some of our customers so we are planning",
    "start": "1876120",
    "end": "1881970"
  },
  {
    "text": "to just come up with numbers and tell them that okay this is the additional latency cost if you're okay with that",
    "start": "1881970",
    "end": "1887370"
  },
  {
    "text": "you can just leverage the service mesh like advantages if not whatever and also",
    "start": "1887370",
    "end": "1893100"
  },
  {
    "text": "we are planning to just make this a common tool where our customers can use for they're all sort of load testing",
    "start": "1893100",
    "end": "1900330"
  },
  {
    "text": "purposes so that's it for today like if",
    "start": "1900330",
    "end": "1905520"
  },
  {
    "text": "you do have any questions we would like to hear other than that if you have questions that you can ask you can just",
    "start": "1905520",
    "end": "1911730"
  },
  {
    "text": "find us at a party so it will be much more fun to answer those question when we are drunk",
    "start": "1911730",
    "end": "1918860"
  },
  {
    "text": "[Applause]",
    "start": "1918990",
    "end": "1924749"
  }
]