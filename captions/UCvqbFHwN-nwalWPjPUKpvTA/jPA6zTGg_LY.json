[
  {
    "text": "Thank You room for coming I met and I'm gonna talk about the myth of the mana",
    "start": "30",
    "end": "5279"
  },
  {
    "text": "cluster so as a quick intro I'm a p.m. at Google I'm not gonna talk about",
    "start": "5279",
    "end": "12059"
  },
  {
    "text": "anything Google related because I'm sure you're probably tired of that at this point and I'm still pretty unsure",
    "start": "12059",
    "end": "17940"
  },
  {
    "text": "whether I should keep writing to conference talks it's always like a last minute thing so this talk",
    "start": "17940",
    "end": "23789"
  },
  {
    "text": "this talk is about the mana clusters the mana foster is the idea that you have this one big kubernetes cluster in your",
    "start": "23789",
    "end": "30689"
  },
  {
    "text": "organization right you might have Devon staging but you have this one big production cluster you had datacenter",
    "start": "30689",
    "end": "36300"
  },
  {
    "text": "neighbor cluster it's this like ideal world to sort of deploy into you know its gets controlled you know the surface",
    "start": "36300",
    "end": "42660"
  },
  {
    "text": "of it that's great but this talks about reducing the blast radius of all of your communities",
    "start": "42660",
    "end": "48149"
  },
  {
    "text": "clusters it's about understanding the failure modes it's also the considerations as you scale communities in organization level so you know what",
    "start": "48149",
    "end": "56460"
  },
  {
    "text": "are these myths that I'm talking about at least like mono cluster it's that one large cluster makes sense",
    "start": "56460",
    "end": "62010"
  },
  {
    "text": "it's like isn't that what like Google does we bog and instead of similar sort of cost dimension tools you know is it",
    "start": "62010",
    "end": "68400"
  },
  {
    "text": "that managing more masters is bad is it that centralized controls great and",
    "start": "68400",
    "end": "74400"
  },
  {
    "text": "that's often true is it the fact that it's communities you just keep scaling this thing right 50 nodes 5,000 nodes",
    "start": "74400",
    "end": "81150"
  },
  {
    "text": "50,000 nodes right that like inevitably communities will come along with us and we'll have this huge cluster that just",
    "start": "81150",
    "end": "86430"
  },
  {
    "text": "everyone deploys to and it's all shared we don't want teams running their own stuff and if you have a big cluster it's",
    "start": "86430",
    "end": "93900"
  },
  {
    "text": "easy to maximize resources drug you can bean pack more stuff you don't have sort of like wasted baseline deployments",
    "start": "93900",
    "end": "99360"
  },
  {
    "text": "sitting around that are idling some of the day you can you can control all of that sort of auto scaling with a single",
    "start": "99360",
    "end": "105509"
  },
  {
    "text": "cluster much more easily so you know the",
    "start": "105509",
    "end": "110820"
  },
  {
    "text": "problems are you know building a single company y cluster sounds like a good idea and the biggest problem of that is",
    "start": "110820",
    "end": "117390"
  },
  {
    "text": "like teams work at different paces they all have different risk tolerances and what you end up doing is with a single",
    "start": "117390",
    "end": "123990"
  },
  {
    "text": "mono cluster you end up operating at the pace of your slowest team so if your slowest team is your core billing team",
    "start": "123990",
    "end": "131160"
  },
  {
    "text": "your use a database see make if you're a ecommerce site maybe it's your comm front-ends right they're gonna be slow",
    "start": "131160",
    "end": "138209"
  },
  {
    "text": "they're not gonna deploy every week they're not gonna want to be on the latest version of Cuban Eddie's just cuz you want these features they don't want",
    "start": "138209",
    "end": "145019"
  },
  {
    "text": "those features right they're giving away their work risk when you have this big cluster you kind of suss to ossify and you start to not upgrade you start",
    "start": "145019",
    "end": "152730"
  },
  {
    "text": "to not think about security you know security patches often involve upgrades",
    "start": "152730",
    "end": "158250"
  },
  {
    "text": "they often involve you know potential sort of rolling restarts you start to add all this risk because you're too",
    "start": "158250",
    "end": "164220"
  },
  {
    "text": "scared to sort of actually do anything it actually it was obvious in the inverse right at some point you're like I'm gonna de-risk by having one big",
    "start": "164220",
    "end": "170069"
  },
  {
    "text": "cluster it's all fully controlled everyone's on the same version of kubernetes great feed debugging everyone",
    "start": "170069",
    "end": "176730"
  },
  {
    "text": "has the same experience and then at some point the table turns and you say actually wait all of these teams have",
    "start": "176730",
    "end": "183299"
  },
  {
    "text": "beholden to like one particular team this probably reminds you a lot of like micro services in general right is a",
    "start": "183299",
    "end": "190260"
  },
  {
    "text": "very similar sort of sort of pattern here and sort of paradigm we've had monolithic applications we started to",
    "start": "190260",
    "end": "196769"
  },
  {
    "text": "break them down because we're all sort of you know deploying them once a year in the bad old days or realistically you",
    "start": "196769",
    "end": "203220"
  },
  {
    "text": "know once a month or so and it was scary and like only one person knew how to sort of deploy and we kept flowing and",
    "start": "203220",
    "end": "210030"
  },
  {
    "text": "flowing slowing down so what are the challenges with you know",
    "start": "210030",
    "end": "216690"
  },
  {
    "text": "a not gonna cost the design right it's kind of avoiding following the Oakshott",
    "start": "216690",
    "end": "222620"
  },
  {
    "text": "it's you know with the mono cluster it's also like how big is too big with individual clusters it's how small is",
    "start": "222620",
    "end": "228690"
  },
  {
    "text": "too small like do you have you know in a large organization do you just have a hundred clusters of ten nodes each or",
    "start": "228690",
    "end": "234900"
  },
  {
    "text": "something like that and you know stuff like multicast orchestration which is it's pretty NASA and pretty hard to do",
    "start": "234900",
    "end": "242000"
  },
  {
    "text": "so let's talk a little bit about sort of double back on the mono cluster pod right which is you know I've said it's",
    "start": "242000",
    "end": "249299"
  },
  {
    "text": "bad but like I haven't proved to you that it's bad some of you probably have experience with this before but you're like what are the ways that it can fail",
    "start": "249299",
    "end": "255810"
  },
  {
    "text": "and others ways bad enough to do something else about right dude maybe just sort of like push",
    "start": "255810",
    "end": "261690"
  },
  {
    "text": "through it solve those problems are they even reasonably solvable problem either tomorrow or next year obviously",
    "start": "261690",
    "end": "267790"
  },
  {
    "text": "there's a lot of changing with communities in general and a lot of the tapping sort of this week but you know what are these failure modes that we",
    "start": "267790",
    "end": "273520"
  },
  {
    "text": "want to get away from so you know think about it in these sort of three ways",
    "start": "273520",
    "end": "278560"
  },
  {
    "text": "what components you know scale with the cluster and where their failure counts actually like",
    "start": "278560",
    "end": "285910"
  },
  {
    "text": "scale proportionally or even worse with the cluster shot at size in shape right so what components fail when the",
    "start": "285910",
    "end": "292600"
  },
  {
    "text": "clusters are likely to fail in the clusters 50 nodes and you know 2,000",
    "start": "292600",
    "end": "297700"
  },
  {
    "text": "pods versus 500 nodes and you know or at magnitude more pods so on and so forth right what are the things that start to",
    "start": "297700",
    "end": "303970"
  },
  {
    "text": "get worse and worse and have a much lighter budget-wise radius just inside the cluster let alone the cluster actually impacting the organization",
    "start": "303970",
    "end": "311190"
  },
  {
    "text": "what's the impact of those failures on your services right is it like complete failure is it just partial failure is it",
    "start": "311190",
    "end": "318370"
  },
  {
    "text": "some kind of degradation of service and can you manage around that that might be okay right you might just say hey I'm",
    "start": "318370",
    "end": "323380"
  },
  {
    "text": "okay with having one cluster if failure is like a partial degradation I can sort of work around it right and that sort of",
    "start": "323380",
    "end": "329740"
  },
  {
    "text": "speaks to you know can I avoid can we instead of avoid those problems in general so there's a few parts DNS DNS",
    "start": "329740",
    "end": "337810"
  },
  {
    "text": "always fails it's nearly always the bottleneck right it is doubly so if",
    "start": "337810",
    "end": "345550"
  },
  {
    "text": "you're actually sinking that cluster DNS to some other kind of DNS that you need to sort of rely with oh it's your cloud",
    "start": "345550",
    "end": "351040"
  },
  {
    "text": "provider whether it's on Prem right you have some infrastructure somewhere else that like services outside kubernetes if",
    "start": "351040",
    "end": "356650"
  },
  {
    "text": "most of us probably haunt you know 100% communities everywhere let's do a sort of very new or very into it right so",
    "start": "356650",
    "end": "364630"
  },
  {
    "text": "there's more failure points there as well if the cluster dns fails all the controller that syncing DNS fails you've",
    "start": "364630",
    "end": "370090"
  },
  {
    "text": "lost even more you've lost all connectivity in and out and when DNS",
    "start": "370090",
    "end": "376150"
  },
  {
    "text": "performance sort of actually starts to take a hit right that impacts like all your services service lookups as well right and for the most part",
    "start": "376150",
    "end": "382390"
  },
  {
    "text": "TTLs and kubernetes are typically pretty short to deal with you know auto-scaling challenges as well right do you want",
    "start": "382390",
    "end": "387700"
  },
  {
    "text": "those names to be reasonably fresh that other services know where to go so if",
    "start": "387700",
    "end": "393460"
  },
  {
    "text": "you see you know if you've ever seen sort of the communities failure stories and github which I highly recommend is a",
    "start": "393460",
    "end": "398620"
  },
  {
    "text": "a list of a long long number of like how its failed for many organizations in",
    "start": "398620",
    "end": "404260"
  },
  {
    "text": "many different ways you notice that I think there's about thirty or so seventeen of them feature DNS right",
    "start": "404260",
    "end": "410800"
  },
  {
    "text": "Dennis comes up a lot as a finally might write and again with that DNS what are you gonna do when your cluster how a",
    "start": "410800",
    "end": "416680"
  },
  {
    "text": "service is supposed to communicate you're probably trying to get away from hard coding IPS which one of the reasons",
    "start": "416680",
    "end": "421900"
  },
  {
    "text": "you are removed to communities to have more of a sort of dynamism what else so",
    "start": "421900",
    "end": "426970"
  },
  {
    "text": "the API server and you know by extension instead of like at CD right that's that's your control plane right with DNS",
    "start": "426970",
    "end": "433210"
  },
  {
    "text": "that's that's the core part of the control plane scaling your masters vertically only gets you so far",
    "start": "433210",
    "end": "438370"
  },
  {
    "text": "right you can keep scaling them up but inevitably in the actual API server binary itself is potentially going to",
    "start": "438370",
    "end": "444370"
  },
  {
    "text": "have issues right it's still not actually you know reducing the blast radius you've just actually packed more",
    "start": "444370",
    "end": "449740"
  },
  {
    "text": "on right 32 cores under 28 cores as much RAM as you want to throw at it right you can keep scaling this up and we live in",
    "start": "449740",
    "end": "455830"
  },
  {
    "text": "a world of you know scheduled virtual machines but that's only gonna take this phone it can also be really hard to fix",
    "start": "455830",
    "end": "463120"
  },
  {
    "text": "problems once you actually start having API server issues so you can't roll back or you can't scale a lot of things are",
    "start": "463120",
    "end": "469180"
  },
  {
    "text": "on the Lord do you say era I'm just gonna Scout a little bit more to sort of like deal with the load issue and",
    "start": "469180",
    "end": "474280"
  },
  {
    "text": "actually debug why these services are overloaded individually later right if you can't actually tell the class that",
    "start": "474280",
    "end": "479740"
  },
  {
    "text": "are those things that starts to become pretty hard to get through and you start to have these compounding problems where",
    "start": "479740",
    "end": "485099"
  },
  {
    "text": "API server contention or DNS server contention makes it hard for services to talk to each other so they respond in",
    "start": "485099",
    "end": "490630"
  },
  {
    "text": "kind by potentially increasing API server load as they try to scale up or like get status updates so on and so",
    "start": "490630",
    "end": "496479"
  },
  {
    "text": "forth and you potentially bring down or make the cluster inaccessible so of",
    "start": "496479",
    "end": "502630"
  },
  {
    "text": "course I think you know what to do a talk here without some kind of like Kelsey Hightower queen-like quote tweet",
    "start": "502630",
    "end": "507990"
  },
  {
    "text": "right but they really it's a really key part is your control pain should be sort of scoped to the smallest fairly domain",
    "start": "507990",
    "end": "514000"
  },
  {
    "text": "you can afford and that is can you afford the whole business value because although services you're running are on",
    "start": "514000",
    "end": "519700"
  },
  {
    "text": "the same cluster probably not could you afford your customer facing api's or",
    "start": "519700",
    "end": "526180"
  },
  {
    "text": "comm failing because they were all on the same cluster as well probably not as well right this like really",
    "start": "526180",
    "end": "531940"
  },
  {
    "text": "out of place you want to be I think failure is inevitable we should assume that at some point even with sort of all",
    "start": "531940",
    "end": "537490"
  },
  {
    "text": "the magic and and sort of orchestration that kubernetes provides that things will fail and we should have a plan to",
    "start": "537490",
    "end": "543580"
  },
  {
    "text": "sort of deal with that failure right so figuring out what the control plane is around that is it the same CI pipeline",
    "start": "543580",
    "end": "549610"
  },
  {
    "text": "maybe you can deal without that you have a great glass for deploying manually if you really have to you right if you CI",
    "start": "549610",
    "end": "554710"
  },
  {
    "text": "servers it down you have a thing that like can let you make fixes manually in a absolutely worst case right that's probably manageable right but is it the",
    "start": "554710",
    "end": "562540"
  },
  {
    "text": "same set of musters is that the same set of underlying hardware right the whole point of us like having these sort of",
    "start": "562540",
    "end": "568060"
  },
  {
    "text": "orchestration platforms is to get away from the hardware and be at a sort of abstract that but if we're coming back to these singular control planes",
    "start": "568060",
    "end": "574480"
  },
  {
    "text": "we've just manufactured another form of sort of failure and you know the other",
    "start": "574480",
    "end": "580780"
  },
  {
    "text": "part of this is you think about how do other workloads write noisy neighbor is",
    "start": "580780",
    "end": "586000"
  },
  {
    "text": "still a problem in Cuba Nettie's as well right again one big cluster all these things contending you can have the",
    "start": "586000",
    "end": "592450"
  },
  {
    "text": "smallest node you can have these huge nodes there's tons of calls and RAM and there's plenty of things that kubernetes",
    "start": "592450",
    "end": "598420"
  },
  {
    "text": "is not correct for right if you've if you've ever written like a like a pod manifest right you can have CPU and",
    "start": "598420",
    "end": "605620"
  },
  {
    "text": "instead of rambling like memory limit limits in the in sort of requests right but what about network what about disk",
    "start": "605620",
    "end": "613510"
  },
  {
    "text": "i/o like AI ops like you can't reasonably define those through cribbens",
    "start": "613510",
    "end": "618640"
  },
  {
    "text": "directly for the most part right into those things can totally destroy your cluster right they can completely blow",
    "start": "618640",
    "end": "624760"
  },
  {
    "text": "up workloads it's very easy in fact I've seen many folks do this and I'm seeing from experience they define everything",
    "start": "624760",
    "end": "631450"
  },
  {
    "text": "with you know resource limits and it's great in those like is gonna be awesome and they have this really like disk i/o",
    "start": "631450",
    "end": "637510"
  },
  {
    "text": "heavy application and it completely saturated the network and the VM as well and none of it actually works right it",
    "start": "637510",
    "end": "643480"
  },
  {
    "text": "never actually uses a less CPU Ram it never gets actually rescheduled anywhere else because it's totally within its limits but I've tried to pack as much as",
    "start": "643480",
    "end": "650830"
  },
  {
    "text": "possible onto this single cluster and they've blown everything up right at some point and network interfaces and",
    "start": "650830",
    "end": "656050"
  },
  {
    "text": "disks are only scale so much so the way",
    "start": "656050",
    "end": "661210"
  },
  {
    "text": "to sort of think about this is like multi-tenancy and that's really the core of having a mono cluster is you want this a multi-tenant sort of system that deals",
    "start": "661210",
    "end": "668220"
  },
  {
    "text": "with all your business has a lot of dimensions right you're sort of dealing with again you're dealing with memory",
    "start": "668220",
    "end": "677100"
  },
  {
    "text": "resources you're dealing with if you're dealing with network you're dealing with other users on the system who maybe",
    "start": "677100",
    "end": "682950"
  },
  {
    "text": "don't have your best interest in mind right you might work at the same company and like you know that to get each other",
    "start": "682950",
    "end": "688170"
  },
  {
    "text": "right but like they have their things to get done you have your things to get done you might not be talking to each other about this kind of stuff and when",
    "start": "688170",
    "end": "694980"
  },
  {
    "text": "you have a large cluster right it's not like individually are you all managing or thinking about how much load am i",
    "start": "694980",
    "end": "700830"
  },
  {
    "text": "putting on the API servers like am I making enough like can making too many DNS queries to sort of like reach out",
    "start": "700830",
    "end": "705900"
  },
  {
    "text": "would it make maybe rip make requests outside of the cluster like when you're all done together and you're all sort of",
    "start": "705900",
    "end": "711360"
  },
  {
    "text": "sharing this system right that isolation is not always there and communities only provides so much isolation like",
    "start": "711360",
    "end": "716820"
  },
  {
    "text": "namespaces don't give you really what you want in terms of actual underlying workload isolation even stuff like gee",
    "start": "716820",
    "end": "723840"
  },
  {
    "text": "visor and cutter containers and firecracker right well isolate the actual workload running in the container",
    "start": "723840",
    "end": "729320"
  },
  {
    "text": "from ideally breaking out and actually doing things on the host right but that workload can still interfere with",
    "start": "729320",
    "end": "735300"
  },
  {
    "text": "everything else right so you know see that's another way right I think the",
    "start": "735300",
    "end": "741990"
  },
  {
    "text": "challenge is that when you have a big mono cluster you're trying to make clusters general-purpose right you're",
    "start": "741990",
    "end": "747180"
  },
  {
    "text": "trying to say hey I can build a thing that everyone in my business can use that every developers gonna love right",
    "start": "747180",
    "end": "752820"
  },
  {
    "text": "is sort of like CTO brain maybe and it's gonna solve a lot of our problems right",
    "start": "752820",
    "end": "758250"
  },
  {
    "text": "we all this nice consistent sort of deployment target and it's actually really hard to do that because how are",
    "start": "758250",
    "end": "764280"
  },
  {
    "text": "you supposed to know what every team wants up front on a system particularly migrating kubernetes which is still the",
    "start": "764280",
    "end": "770130"
  },
  {
    "text": "case in most organizations how do you know what those requirements really are if no one's ever deployed to this before right if no one's actually run this",
    "start": "770130",
    "end": "776370"
  },
  {
    "text": "truly production before right you you'll never gather all those requirements in full upfront and trying to pile",
    "start": "776370",
    "end": "784680"
  },
  {
    "text": "everybody into the same system and say oh you know web front-end team you'll hear billing team you're here I'm gonna",
    "start": "784680",
    "end": "791340"
  },
  {
    "text": "put some stateful services on this cluster right they all had very different requirements and you're ending up with this just sort of",
    "start": "791340",
    "end": "797600"
  },
  {
    "text": "massive scope creep you'll probably never build a glass that actually meets any of like even half those requirements and you're exposing yourself to more and",
    "start": "797600",
    "end": "805490"
  },
  {
    "text": "more and more failure modes so we should avoid a single production cost I think I",
    "start": "805490",
    "end": "812000"
  },
  {
    "text": "probably made that opinion fairly cute pretty clear at this point right but what do we do next what's the sort of",
    "start": "812000",
    "end": "817970"
  },
  {
    "text": "alternative to this right what's the you know many ways like the knee-jerk reaction to having one big cluster is",
    "start": "817970",
    "end": "823160"
  },
  {
    "text": "that like everyone gets a cluster so we have one we cluster we break it down",
    "start": "823160",
    "end": "828350"
  },
  {
    "text": "into a series of clusters right like one of these is is again like billing one of",
    "start": "828350",
    "end": "833509"
  },
  {
    "text": "these is a bunch of back-end services some of these may be a sort of internal like authentication systems you know",
    "start": "833509",
    "end": "838970"
  },
  {
    "text": "we've like broken out these sort of failure domains maybe some of these services aren't running across multiple clusters so this looks like okay like",
    "start": "838970",
    "end": "845269"
  },
  {
    "text": "this is a moderate amount and you know this is kind of a good mood like everyone gets a cluster they can do what",
    "start": "845269",
    "end": "850850"
  },
  {
    "text": "they want they can sort of manage their own requirements they can install whatever they want on it they can keep up with the latest stuff it's like hey",
    "start": "850850",
    "end": "856730"
  },
  {
    "text": "you know I want this thing in like you know in one one six wait I want these features I can upgrade my cluster with my team we get agreement I don't get the",
    "start": "856730",
    "end": "863240"
  },
  {
    "text": "whole ol to buy in but that sounds pretty good but you know we kind of like put some structure to these a little bit",
    "start": "863240",
    "end": "868910"
  },
  {
    "text": "more and what we can have end up with like a bit of an org chart where every team has a cluster and that's probably",
    "start": "868910",
    "end": "876110"
  },
  {
    "text": "not actually what we want there's actually a name for this and it's called like Conway's law right and it's the",
    "start": "876110",
    "end": "881449"
  },
  {
    "text": "idea that you know we sort of produced the designs that actually like mirror the size or the shape of our organization and again this goes back to",
    "start": "881449",
    "end": "888110"
  },
  {
    "text": "the cult sort of what we probably seen from microservices where that was actually in some ways pretty good right",
    "start": "888110",
    "end": "894170"
  },
  {
    "text": "not always being bound to other team has helped a lot of folks move at their own sort of pace right we use a most of us",
    "start": "894170",
    "end": "901250"
  },
  {
    "text": "you sort of API contracts to sort of at least agree upon some sensible boundaries between how we sort of interact right that's pretty good I",
    "start": "901250",
    "end": "908480"
  },
  {
    "text": "don't think that is as good in the community's case where every team is now running close to right because out of",
    "start": "908480",
    "end": "914149"
  },
  {
    "text": "these teams development teams or are the kubernetes operations teams they're probably development teams and humanities is just",
    "start": "914149",
    "end": "920779"
  },
  {
    "text": "a tool to get the job done so you know +14 Conway's law I kind of go",
    "start": "920779",
    "end": "925790"
  },
  {
    "text": "hand-in-hand but it's really not what you want so you know I've talked about this a little bit before",
    "start": "925790",
    "end": "931250"
  },
  {
    "text": "why many clusters there work at different paces you have the handful of",
    "start": "931250",
    "end": "936889"
  },
  {
    "text": "clusters to measure electability so you can deploy different things at different classes you can have different versions it's way easier to canary properly at",
    "start": "936889",
    "end": "944810"
  },
  {
    "text": "scale right teams can replicate their clusters they can do things at their own pace and again you don't have to keep",
    "start": "944810",
    "end": "950240"
  },
  {
    "text": "moving at the pace of your most risk-averse team that is not what we want we want the teams that need to be able to sort of experiment we need the",
    "start": "950240",
    "end": "956269"
  },
  {
    "text": "teams that can maybe tolerate you know an hour of downtime if they really screw up right maybe there's like a batch job",
    "start": "956269",
    "end": "961879"
  },
  {
    "text": "team or an m/l team is like hey look if we loose in progress that sucks but I can tolerate an hour of downtime we have",
    "start": "961879",
    "end": "967910"
  },
  {
    "text": "to roll back and rebuild our cluster from scratch that's not so bad your team that's managing the authentication",
    "start": "967910",
    "end": "972980"
  },
  {
    "text": "services for your customers not so much right that's probably a serious outage",
    "start": "972980",
    "end": "978439"
  },
  {
    "text": "just going to page a whole bunch of folks and keep you up at night you really don't want that right so you end",
    "start": "978439",
    "end": "983480"
  },
  {
    "text": "up operating at their pace instead so the problem is instead of one pet now",
    "start": "983480",
    "end": "989149"
  },
  {
    "text": "you just have many pets every team treats his clusters like they're like the little baby it's like if everything in the world they have customized it",
    "start": "989149",
    "end": "997759"
  },
  {
    "text": "intensely for their own things and they've ignored everything that they don't have to think about it doesn't they don't care about right oh I don't",
    "start": "997759",
    "end": "1003100"
  },
  {
    "text": "need like security policies they don't need like admission controls we know everybody in the team it's fine right but now that we can start to",
    "start": "1003100",
    "end": "1011050"
  },
  {
    "text": "resistant to change themselves right so we've come back all the way through and we've said one big cost is bad because no one wants to change it now we have",
    "start": "1011050",
    "end": "1016750"
  },
  {
    "text": "say 50 classes and no one was to change their Kloster the teams are now platform teams and realistically right it's it's",
    "start": "1016750",
    "end": "1025480"
  },
  {
    "text": "actually kind of it's not the way you want to operate this sort of business right you end up with a whole bunch of",
    "start": "1025480",
    "end": "1031750"
  },
  {
    "text": "and really I think security issues this it comes down to so we end up like this pet infrastructure where you know again",
    "start": "1031750",
    "end": "1038500"
  },
  {
    "text": "everyone's just looking out what they want but we also know multi-class that's hard right if anyone here is try to",
    "start": "1038500",
    "end": "1043780"
  },
  {
    "text": "operate in a multi cluster environment where you actually have services truly",
    "start": "1043780",
    "end": "1049480"
  },
  {
    "text": "across multiple clusters right with stuff like sort of MCI so like multiply station ingress or deploying things to",
    "start": "1049480",
    "end": "1055270"
  },
  {
    "text": "multiple clusters and keeping them in check and making sure that if they scale on this cluster that they're also scaling proportionately on the cluster",
    "start": "1055270",
    "end": "1061330"
  },
  {
    "text": "over here it's actually still pretty hard you know and the cloud providers don't make that",
    "start": "1061330",
    "end": "1066840"
  },
  {
    "text": "a ton easier today most of these sort of existing communities distros don't make that a ton easier there's all a tooling",
    "start": "1066840",
    "end": "1072330"
  },
  {
    "text": "up there but it's still not super mature right like the idea of like true multi costs the Federation is still really",
    "start": "1072330",
    "end": "1078900"
  },
  {
    "text": "growing so there's hard things just a little bit security version drift is",
    "start": "1078900",
    "end": "1084690"
  },
  {
    "text": "real so version drift of the cluster itself of the Masters of all the components",
    "start": "1084690",
    "end": "1089760"
  },
  {
    "text": "they're running so if you have any admission controllers any other kind of custom controllers any kind of operators",
    "start": "1089760",
    "end": "1095810"
  },
  {
    "text": "you know stuff like gatekeeper all those things start to have version drift that",
    "start": "1095810",
    "end": "1101940"
  },
  {
    "text": "starts become a real problem right security patches again that's also an issue it's like who's watching out on",
    "start": "1101940",
    "end": "1108170"
  },
  {
    "text": "the mailing lists do you have how does the security team actually manage like by proxy all of these classes and make",
    "start": "1108170",
    "end": "1113880"
  },
  {
    "text": "sure the teams upgrade right now they have to go and convince 52 sync teams to go and prioritize this patch right and",
    "start": "1113880",
    "end": "1120330"
  },
  {
    "text": "in many or given with the best security teams that can be really hard they all have different deployment strategies",
    "start": "1120330",
    "end": "1125910"
  },
  {
    "text": "because they've customized how they generate their manifests and what CS CD sei CD platform they might be using you",
    "start": "1125910",
    "end": "1131490"
  },
  {
    "text": "might have you know circle CI or like team city internally right but everyone's like the point of they",
    "start": "1131490",
    "end": "1137130"
  },
  {
    "text": "cluster in a slightly different way because it just works for them your SRA and ops teams especially dev teams",
    "start": "1137130",
    "end": "1143700"
  },
  {
    "text": "this becomes a huge mess there it's like well I don't really want to own this because they didn't build it it's all",
    "start": "1143700",
    "end": "1148770"
  },
  {
    "text": "custom and it's truly impossible to troubleshoot right you have a dev that moves from team a to team B it's like",
    "start": "1148770",
    "end": "1154830"
  },
  {
    "text": "moving to a different company in the worst case right you have this cluster that is different version different software different deployment different",
    "start": "1154830",
    "end": "1161700"
  },
  {
    "text": "risks all of it is very very different right that actually becomes really hard and you know in most most companies are",
    "start": "1161700",
    "end": "1167580"
  },
  {
    "text": "not just working every day for a single team you're doing a lot of cross-functional work you're helping out other folks in neighboring teams to",
    "start": "1167580",
    "end": "1173220"
  },
  {
    "text": "solve their problems right if you're an ops like a sorry team that's even worse because you get paid you're like okay",
    "start": "1173220",
    "end": "1180030"
  },
  {
    "text": "great I think we've seen this before it's this problem you get a brand new cluster you've never seen before and there's another like 49 of them out",
    "start": "1180030",
    "end": "1186270"
  },
  {
    "text": "there in the company that you haven't had touched it because they just haven't broken or you haven't even realized because you haven't actually hooked up",
    "start": "1186270",
    "end": "1192090"
  },
  {
    "text": "any kind of monitoring right again it how do you kind of enforce that kind of like nice platform when you've got so many",
    "start": "1192090",
    "end": "1197130"
  },
  {
    "text": "clusters so what do we what do we go from here like one cost of bad 50 class",
    "start": "1197130",
    "end": "1204960"
  },
  {
    "text": "is also bad like is there a middle ground is there some kind of like you know formula we can figure out and say",
    "start": "1204960",
    "end": "1210750"
  },
  {
    "text": "hey like X many people in our company or as many teams means you know why many",
    "start": "1210750",
    "end": "1216000"
  },
  {
    "text": "clusters not really but I think you know we can think about in terms of criteria we'll hope you get that so this goes",
    "start": "1216000",
    "end": "1224850"
  },
  {
    "text": "back to I really of saying like the whole like you know it's cat or not pets thing you should treat those classes",
    "start": "1224850",
    "end": "1231450"
  },
  {
    "text": "like you can throw them away right and that doesn't apply in the motocross the case because that's all you've got it",
    "start": "1231450",
    "end": "1237780"
  },
  {
    "text": "doesn't apply when every team has are in cluster right it applies when you truly think about clusters as a throwaway thing right you should be able to throw",
    "start": "1237780",
    "end": "1243960"
  },
  {
    "text": "them away it's that close to gets deleted and you can't bring it back up in a reasonable at a time it reason of",
    "start": "1243960",
    "end": "1249180"
  },
  {
    "text": "time depends on your own sort of s Eliza NASA Lowe's right you that might change right but you might say look my goal is",
    "start": "1249180",
    "end": "1255300"
  },
  {
    "text": "that I can bring a cluster up from scratch in an hour in precisely the same stated as it was when I deleted it or",
    "start": "1255300",
    "end": "1262140"
  },
  {
    "text": "destroyed the whole thing right that's good so use stuff like the cluster API or cloud providers toolkits to like",
    "start": "1262140",
    "end": "1267300"
  },
  {
    "text": "define a base cluster config that is shared right and that's where your platform teams can have the most",
    "start": "1267300",
    "end": "1272880"
  },
  {
    "text": "influence right they're going to define the baseline controllers based on versions you might even have you know a",
    "start": "1272880",
    "end": "1278580"
  },
  {
    "text": "moderate amount of sort of different channels that is like super stable deal",
    "start": "1278580",
    "end": "1284580"
  },
  {
    "text": "with a little bit of risk super cutting idea with a lot of risk right but that's much better than probably having 64",
    "start": "1284580",
    "end": "1290070"
  },
  {
    "text": "versions communities all across your business let alone all the other sort of dependencies use stuff like you know",
    "start": "1290070",
    "end": "1295830"
  },
  {
    "text": "gatekeepers part of like a policy agent and your CI tooling to enforce",
    "start": "1295830",
    "end": "1301230"
  },
  {
    "text": "consistent policies across team and then like audit that drift right even if you can't maybe enforce it everything because teams like I really want to",
    "start": "1301230",
    "end": "1307140"
  },
  {
    "text": "break gloss on this and do something a little different at least catch that",
    "start": "1307140",
    "end": "1312540"
  },
  {
    "text": "drift and figure out what it is right and that also makes troubleshooting easy because you can understand what was different from those policies and look",
    "start": "1312540",
    "end": "1318780"
  },
  {
    "text": "at those things first right stuff like gatekeepers really good for that and again I talked about this it's still",
    "start": "1318780",
    "end": "1323970"
  },
  {
    "text": "pretty hard but definitely try to take a multi-class the ingress approach to D risk you're like custom",
    "start": "1323970",
    "end": "1329580"
  },
  {
    "text": "world facing services so again you can't multi cluster everything easily without",
    "start": "1329580",
    "end": "1334830"
  },
  {
    "text": "a lot of pain that's kind of the dream state to have this like lovely Federation the clusters and you can",
    "start": "1334830",
    "end": "1341460"
  },
  {
    "text": "deploy things to like five clusters and it just all scales magically together that's not probably gonna happen but at",
    "start": "1341460",
    "end": "1349559"
  },
  {
    "text": "least thing is I'm soon but for your really core stuff put the effort in to do that right put that effort into like",
    "start": "1349559",
    "end": "1355259"
  },
  {
    "text": "have multiple control points it may be even okay if it doesn't sort of",
    "start": "1355259",
    "end": "1360450"
  },
  {
    "text": "perfectly auto scale on both classes or downscale on both causes at the same time right you might say look uh I",
    "start": "1360450",
    "end": "1365840"
  },
  {
    "text": "really want to focus on reducing the boss radius here I want to focus on having you know a case where I discuss",
    "start": "1365840",
    "end": "1373679"
  },
  {
    "text": "the files I have some capacity over here right it's different hardware it's a different network I'm trying to sort of",
    "start": "1373679",
    "end": "1379649"
  },
  {
    "text": "reduce all of these shared points of failure along the way because that control point again is not just even",
    "start": "1379649",
    "end": "1384779"
  },
  {
    "text": "kubernetes it's everything around it that interacts with so you know again",
    "start": "1384779",
    "end": "1390090"
  },
  {
    "text": "what is the what it's the right number clusters is it you know if we talking one per team no are we talking you know",
    "start": "1390090",
    "end": "1397739"
  },
  {
    "text": "ten of them that's kind of hard to assess so the way I think about it",
    "start": "1397739",
    "end": "1402779"
  },
  {
    "text": "really the core criteria when you get down to is map out these risk domains the idea that you have teams that",
    "start": "1402779",
    "end": "1410929"
  },
  {
    "text": "completely risk-averse and these are the old teams that start to slowly a sort of the old monolithic cluster designed down",
    "start": "1410929",
    "end": "1415950"
  },
  {
    "text": "right where you start to sort of move at their pace and not everybody else's pace what services are mostly most likely to",
    "start": "1415950",
    "end": "1422970"
  },
  {
    "text": "impact others a lot of services that are super high ops use the network a lot",
    "start": "1422970",
    "end": "1428629"
  },
  {
    "text": "isolate those isolate those from your critical services don't share them on the same clusters even right yes you",
    "start": "1428629",
    "end": "1435389"
  },
  {
    "text": "could probably put them in different nodes with affinities and taints instead of go that way too but you know that's",
    "start": "1435389",
    "end": "1440609"
  },
  {
    "text": "only like one dimension of this little again this multi-tenancy is break them out put them somewhere else where they",
    "start": "1440609",
    "end": "1446730"
  },
  {
    "text": "can fail and impact them set and impact only themselves and not other services in your business particularly again the",
    "start": "1446730",
    "end": "1452369"
  },
  {
    "text": "ones in that really sort of critical risk domain and you know last but not least it's all it's all good to also to",
    "start": "1452369",
    "end": "1458399"
  },
  {
    "text": "talk about all this and say like we can all architect this in like a vacuum but you know what's your actual real dollar",
    "start": "1458399",
    "end": "1463529"
  },
  {
    "text": "budget like how much time you want to go and spend to make this sort of the most fault tolerant reduce blast radius",
    "start": "1463529",
    "end": "1471149"
  },
  {
    "text": "system in the world right at some point you need to draw a line and say look this is actually what we think it's good enough it's probably the probably a",
    "start": "1471149",
    "end": "1478139"
  },
  {
    "text": "subsequent failure where things going wrong we go okay we're gonna do a little bit more right and it's always like a",
    "start": "1478139",
    "end": "1483179"
  },
  {
    "text": "learning process you'll never get it right the first time but but certainly trying to engineer this like perfect",
    "start": "1483179",
    "end": "1488359"
  },
  {
    "text": "completely fault-tolerant system from day one is also not going to work so draw a line and say look again call",
    "start": "1488359",
    "end": "1494309"
  },
  {
    "text": "services you know these things have to be multi cost I need to be able to deal with these deal with an adage of a",
    "start": "1494309",
    "end": "1499739"
  },
  {
    "text": "complete data center deal with an outage of like a complete cluster or a serious bug and you know this patch release of",
    "start": "1499739",
    "end": "1505799"
  },
  {
    "text": "communities right think about all those things that could go wrong just by having a single cluster and all of its",
    "start": "1505799",
    "end": "1511950"
  },
  {
    "text": "dependencies and have something that's a little different so who should do this",
    "start": "1511950",
    "end": "1519149"
  },
  {
    "text": "right I strongly believe a dedicated platform team should run your clusters right a team that has experience in",
    "start": "1519149",
    "end": "1525480"
  },
  {
    "text": "actually doing this whose job it is to secure the cost to upgrade the class to maintain the components the job is to",
    "start": "1525480",
    "end": "1532200"
  },
  {
    "text": "not write important business logic for the company right that's what a lot of the desn't to be doing right they have",
    "start": "1532200",
    "end": "1538919"
  },
  {
    "text": "another job that's not maintaining clusters and again what ends up happening is that when you give people this sort of joint responsibility and",
    "start": "1538919",
    "end": "1545129"
  },
  {
    "text": "again instead of like every every cost sorry every team as a cluster you get this incredible tension of like I have",
    "start": "1545129",
    "end": "1551460"
  },
  {
    "text": "all these tickets in my backlog and I have the annoying PM's and everybody's like saying we're gonna ship this stuff",
    "start": "1551460",
    "end": "1557489"
  },
  {
    "text": "and less and less people are probably saying oh we need to make sure we",
    "start": "1557489",
    "end": "1562710"
  },
  {
    "text": "upgrade to the latest version communities because it fixes these problems we need to make sure that we shore up our DNS configurations so on",
    "start": "1562710",
    "end": "1568200"
  },
  {
    "text": "and so forth right if a team is trying to sort of become a platform team and actually write functional code for the",
    "start": "1568200",
    "end": "1574379"
  },
  {
    "text": "company those things really don't work in I've seen that fail a lot what do we ended up whatever actually wants this particular case was a team when it's a",
    "start": "1574379",
    "end": "1583139"
  },
  {
    "text": "sort of write their own operator there were data team they'd never touch cloud or communities before",
    "start": "1583139",
    "end": "1588960"
  },
  {
    "text": "and they spent months like focused on actually kind of rebuilding a platform and nothing on their core infrastructure",
    "start": "1588960",
    "end": "1595500"
  },
  {
    "text": "and it took forever took and they you know they learnt go along the way because they learned the operator framework like that's a great learning",
    "start": "1595500",
    "end": "1602399"
  },
  {
    "text": "experience for sure but at the end the day right like they still aren't experts",
    "start": "1602399",
    "end": "1608159"
  },
  {
    "text": "in this they still have these like now extremely bespoke Coster that's like their pet that they're scared to sort of",
    "start": "1608159",
    "end": "1613440"
  },
  {
    "text": "do anything with let alone upgrade right because like they haven't even tackled that before so that's even harder",
    "start": "1613440",
    "end": "1618799"
  },
  {
    "text": "having core platform teams managed this is just much much better and again those",
    "start": "1618799",
    "end": "1624210"
  },
  {
    "text": "platform teens get to see and generalize all the problems that folks have right o teams having a hard time because they're",
    "start": "1624210",
    "end": "1631020"
  },
  {
    "text": "using like the raw api's which we really don't want to expose to sort of like most development teams they can shim",
    "start": "1631020",
    "end": "1636539"
  },
  {
    "text": "those right tooling that sort of understands those cases they can also look at you know again they see a",
    "start": "1636539",
    "end": "1642899"
  },
  {
    "text": "problem on even if the the ops teams managing say 20 clusters 50 clusters they can see problems over here and map",
    "start": "1642899",
    "end": "1650429"
  },
  {
    "text": "them to problems over there when people own their own clusters that doesn't always happen right how many set of dev",
    "start": "1650429",
    "end": "1656039"
  },
  {
    "text": "teams are gonna write a fully fledged post-mortem for their individual cluster failure and share it with the rest organization every time but again that",
    "start": "1656039",
    "end": "1662970"
  },
  {
    "text": "goes back to the fact that they sort of tension between the actual day job and managing this cluster which is really",
    "start": "1662970",
    "end": "1669690"
  },
  {
    "text": "just the means to an end right they they had this individual cluster because they didn't want to monolith and now you know again they're",
    "start": "1669690",
    "end": "1677669"
  },
  {
    "text": "treating like a pet so again you know this or keep going I think we can",
    "start": "1677669",
    "end": "1685500"
  },
  {
    "text": "probably be pretty honest like there's a lot of growth for communities to do and a lot of growth a lot of folks to sort of figure it out right and again just throwing it a sort",
    "start": "1685500",
    "end": "1693450"
  },
  {
    "text": "of a new developers doesn't always work so just sort of put a wrap on this you",
    "start": "1693450",
    "end": "1699120"
  },
  {
    "text": "know what are the what are the takeaways how do we we've you know we've said like the myth of the mono cluster we don't",
    "start": "1699120",
    "end": "1704549"
  },
  {
    "text": "want the same across the design but how do we actually like get to a world where that's not the case right how we",
    "start": "1704549",
    "end": "1710700"
  },
  {
    "text": "convince others in our organizations that this single like whole of the world",
    "start": "1710700",
    "end": "1715980"
  },
  {
    "text": "cost of design is really not what we want the deer's you should start small so",
    "start": "1715980",
    "end": "1721560"
  },
  {
    "text": "focus on teams particular feel sort of movie new communities early start small focus on team that has like a clear use",
    "start": "1721560",
    "end": "1727230"
  },
  {
    "text": "case that like really works well in communities right it's like a stateless application it's something that needs you know seen if I'm out of auto scaling",
    "start": "1727230",
    "end": "1734850"
  },
  {
    "text": "or is like a very bursty workload finding that like really fits to the paradigm of committees where it is right now and as you start grabbing more",
    "start": "1734850",
    "end": "1743490"
  },
  {
    "text": "people and sort of increasing the sphere of how many teams and workloads you're pulling into this document the shortcuts",
    "start": "1743490",
    "end": "1748890"
  },
  {
    "text": "to document every shortcut you take you say hey actually took a shortcut because for this particular workload didn't",
    "start": "1748890",
    "end": "1755550"
  },
  {
    "text": "really matter enough to do it like the the correct way but you know take those",
    "start": "1755550",
    "end": "1760830"
  },
  {
    "text": "notes so you can come back to them and realize like those things you need to go and fix later right again I understand you'll never capture the requirements",
    "start": "1760830",
    "end": "1766470"
  },
  {
    "text": "upfront it makes it just truly impossible the idea that again you can have this whole of company cluster that",
    "start": "1766470",
    "end": "1771540"
  },
  {
    "text": "just suits everyone's workload perfectly doesn't exist you'll never finish writing the like PID for this or the",
    "start": "1771540",
    "end": "1778020"
  },
  {
    "text": "functional requirements the mindful the blast radius every time you're you know",
    "start": "1778020",
    "end": "1783960"
  },
  {
    "text": "adding you components or you're adding things in a share cluster think about if that fails what else does it take out",
    "start": "1783960",
    "end": "1789360"
  },
  {
    "text": "think about if it fails if the whole cluster fails like what is the impact at everything else beyond that right again",
    "start": "1789360",
    "end": "1795150"
  },
  {
    "text": "in many cases having some kind of degradation where you have services split across different clusters is",
    "start": "1795150",
    "end": "1800490"
  },
  {
    "text": "probably better than nothing and at the end of the day you know committees is might walk right let your platform teams",
    "start": "1800490",
    "end": "1807360"
  },
  {
    "text": "run it I think it's still important that developer teams actually understand how it works right I think if anyone's",
    "start": "1807360",
    "end": "1813690"
  },
  {
    "text": "actually looked through all of the scheduler predicates for Cuba Nettie's and the growing number of predicates",
    "start": "1813690",
    "end": "1819240"
  },
  {
    "text": "right understanding how your actual pods and your workloads get moved around and rescheduled and scaled is increasing",
    "start": "1819240",
    "end": "1827130"
  },
  {
    "text": "complex but also pretty important to understand right because you want to understand how to actually like program and build an application within those",
    "start": "1827130",
    "end": "1833280"
  },
  {
    "text": "boundaries but you don't have to know every piece of the platform right if you at that point like you're not actually",
    "start": "1833280",
    "end": "1840030"
  },
  {
    "text": "thinking about the other things you might be doing cool so I get a pretty",
    "start": "1840030",
    "end": "1845580"
  },
  {
    "text": "quick I sort of the end of the day so thank you all for coming and I hope you enjoy the rescue con",
    "start": "1845580",
    "end": "1851820"
  },
  {
    "text": "[Applause]",
    "start": "1851820",
    "end": "1859690"
  }
]