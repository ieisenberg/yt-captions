[
  {
    "text": "hi everyone so uh we are going to talk",
    "start": "1680",
    "end": "4640"
  },
  {
    "text": "about optimizing llm efficiency one",
    "start": "4640",
    "end": "6839"
  },
  {
    "text": "Trace at a time on",
    "start": "6839",
    "end": "10040"
  },
  {
    "text": "kubernetes and uh if you spot on salt",
    "start": "10040",
    "end": "14080"
  },
  {
    "text": "flats this is what we have captured",
    "start": "14080",
    "end": "16720"
  },
  {
    "text": "cubec con Na and this was really cool um",
    "start": "16720",
    "end": "20439"
  },
  {
    "text": "it was amazing view out",
    "start": "20439",
    "end": "23320"
  },
  {
    "text": "there and uh let me introduce myself I",
    "start": "23320",
    "end": "26760"
  },
  {
    "text": "am SEMA Saran I am working in Autodesk",
    "start": "26760",
    "end": "29679"
  },
  {
    "text": "as a sari and I am cncf Ambassador I",
    "start": "29679",
    "end": "33680"
  },
  {
    "text": "have done CK ckad and a lot of community",
    "start": "33680",
    "end": "37320"
  },
  {
    "text": "work for cncf and AWS and Hashi cop user",
    "start": "37320",
    "end": "43160"
  },
  {
    "text": "group based on",
    "start": "43160",
    "end": "46120"
  },
  {
    "text": "jaur okay uh so hi everyone this is adya",
    "start": "46120",
    "end": "50320"
  },
  {
    "text": "I'm currently working as a devops",
    "start": "50320",
    "end": "51719"
  },
  {
    "text": "engineer at Forester apart from that",
    "start": "51719",
    "end": "53879"
  },
  {
    "text": "yeah I do a lot of Open Source stuff so",
    "start": "53879",
    "end": "57000"
  },
  {
    "text": "I'm also one of CF Ambassador I do",
    "start": "57000",
    "end": "59559"
  },
  {
    "text": "mentor people and then uh I'm Community",
    "start": "59559",
    "end": "63080"
  },
  {
    "text": "Builder if you see like I've done lot of",
    "start": "63080",
    "end": "65199"
  },
  {
    "text": "certifications and yeah please reach out",
    "start": "65200",
    "end": "67320"
  },
  {
    "text": "to me on SL T but yeah I will shortly",
    "start": "67320",
    "end": "70880"
  },
  {
    "text": "reply you on the time okay so for",
    "start": "70880",
    "end": "74040"
  },
  {
    "text": "today's topic let's jump right into the",
    "start": "74040",
    "end": "77200"
  },
  {
    "text": "road map so this is something we have",
    "start": "77200",
    "end": "79360"
  },
  {
    "text": "created or crafted something for you so",
    "start": "79360",
    "end": "82119"
  },
  {
    "text": "how this want to go how this going to go",
    "start": "82119",
    "end": "84000"
  },
  {
    "text": "basically is so we're going to start",
    "start": "84000",
    "end": "85880"
  },
  {
    "text": "with many challenges that you can face",
    "start": "85880",
    "end": "88320"
  },
  {
    "text": "while you're deploying llm or writing",
    "start": "88320",
    "end": "89600"
  },
  {
    "text": "the code",
    "start": "89600",
    "end": "90560"
  },
  {
    "text": "and then we can look for the solutions",
    "start": "90560",
    "end": "92079"
  },
  {
    "text": "also the first thing is like we going to",
    "start": "92079",
    "end": "94200"
  },
  {
    "text": "talk about the challenges with llms",
    "start": "94200",
    "end": "96320"
  },
  {
    "text": "maybe then we can move to the",
    "start": "96320",
    "end": "97600"
  },
  {
    "text": "understanding what autel is it's not",
    "start": "97600",
    "end": "99720"
  },
  {
    "text": "required although but still just a quick",
    "start": "99720",
    "end": "101799"
  },
  {
    "text": "go through then we can have something",
    "start": "101799",
    "end": "104360"
  },
  {
    "text": "about profiling how you can basically",
    "start": "104360",
    "end": "107320"
  },
  {
    "text": "basically use that in your current",
    "start": "107320",
    "end": "109079"
  },
  {
    "text": "running infra environment and then uh",
    "start": "109079",
    "end": "112040"
  },
  {
    "text": "with help of that what's actually coming",
    "start": "112040",
    "end": "114280"
  },
  {
    "text": "in AEL for the same and then going",
    "start": "114280",
    "end": "117200"
  },
  {
    "text": "forward uh what's the current status",
    "start": "117200",
    "end": "120360"
  },
  {
    "text": "how we actually create and basically",
    "start": "120360",
    "end": "122680"
  },
  {
    "text": "come up with a",
    "start": "122680",
    "end": "123759"
  },
  {
    "text": "demo then yeah obviously I talked about",
    "start": "123759",
    "end": "126240"
  },
  {
    "text": "demo so we going to just jump directly",
    "start": "126240",
    "end": "128160"
  },
  {
    "text": "into the demo and uh then we can have",
    "start": "128160",
    "end": "131480"
  },
  {
    "text": "like how we can maybe uh whatever",
    "start": "131480",
    "end": "133560"
  },
  {
    "text": "challenges you face with llms you can go",
    "start": "133560",
    "end": "135400"
  },
  {
    "text": "with them and at last we can have some",
    "start": "135400",
    "end": "137560"
  },
  {
    "text": "quick chat over here let's start um so",
    "start": "137560",
    "end": "141519"
  },
  {
    "text": "obviously everything uh on the today's",
    "start": "141519",
    "end": "143720"
  },
  {
    "text": "world is running on AI it's if you see",
    "start": "143720",
    "end": "146239"
  },
  {
    "text": "maybe it's kind of any model anything",
    "start": "146239",
    "end": "148879"
  },
  {
    "text": "but everyone is using AI it's sound",
    "start": "148879",
    "end": "151400"
  },
  {
    "text": "interesting right but if you see it's uh",
    "start": "151400",
    "end": "156680"
  },
  {
    "text": "grass is always one side A Greener but",
    "start": "156680",
    "end": "160080"
  },
  {
    "text": "it's always fun until you're doing it",
    "start": "160080",
    "end": "162720"
  },
  {
    "text": "but when it comes to deploying those",
    "start": "162720",
    "end": "164319"
  },
  {
    "text": "large code those large images model and",
    "start": "164319",
    "end": "166599"
  },
  {
    "text": "everything it comes so extraordinary",
    "start": "166599",
    "end": "169800"
  },
  {
    "text": "things like you have to manage a lot of",
    "start": "169800",
    "end": "171319"
  },
  {
    "text": "stuff it can be GPU CPUs and then how",
    "start": "171319",
    "end": "173879"
  },
  {
    "text": "the code are performing at the maybe you",
    "start": "173879",
    "end": "176680"
  },
  {
    "text": "have a lot of requests coming in or uh",
    "start": "176680",
    "end": "180040"
  },
  {
    "text": "large user based pool okay so for that",
    "start": "180040",
    "end": "182800"
  },
  {
    "text": "maybe you can face a lot of challenges",
    "start": "182800",
    "end": "184799"
  },
  {
    "text": "the first one I'm talking about the",
    "start": "184799",
    "end": "187000"
  },
  {
    "text": "basically the managing the llm itself",
    "start": "187000",
    "end": "189000"
  },
  {
    "text": "you have crafted the code but when it",
    "start": "189000",
    "end": "191480"
  },
  {
    "text": "comes to do the upgradation doing the",
    "start": "191480",
    "end": "193720"
  },
  {
    "text": "dependency upgrade on the time you can",
    "start": "193720",
    "end": "195799"
  },
  {
    "text": "face a lot of maybe a major challenges I",
    "start": "195799",
    "end": "198599"
  },
  {
    "text": "would say maybe memory leak so it's not",
    "start": "198599",
    "end": "201760"
  },
  {
    "text": "always about like how your Ops team is",
    "start": "201760",
    "end": "204040"
  },
  {
    "text": "deploying them it's always going back to",
    "start": "204040",
    "end": "206599"
  },
  {
    "text": "the how you return the particular code",
    "start": "206599",
    "end": "208519"
  },
  {
    "text": "of the same so by profiling maybe you",
    "start": "208519",
    "end": "211239"
  },
  {
    "text": "can just look out to that memory leak",
    "start": "211239",
    "end": "213879"
  },
  {
    "text": "issues or if you talk about latency or",
    "start": "213879",
    "end": "217959"
  },
  {
    "text": "maybe om killed your particular parts",
    "start": "217959",
    "end": "220519"
  },
  {
    "text": "are getting killed by out of memories",
    "start": "220519",
    "end": "223120"
  },
  {
    "text": "and uh obviously the cost because the",
    "start": "223120",
    "end": "225760"
  },
  {
    "text": "major stuff or the major problem that",
    "start": "225760",
    "end": "228120"
  },
  {
    "text": "every organization who is adopting AI or",
    "start": "228120",
    "end": "230360"
  },
  {
    "text": "going with llm is facing is like the",
    "start": "230360",
    "end": "232280"
  },
  {
    "text": "cost because that is might be always",
    "start": "232280",
    "end": "235319"
  },
  {
    "text": "increased around 40% or something and",
    "start": "235319",
    "end": "237519"
  },
  {
    "text": "then they are like uh we are shifting",
    "start": "237519",
    "end": "239799"
  },
  {
    "text": "but again it's a major challenge for us",
    "start": "239799",
    "end": "242319"
  },
  {
    "text": "how we can reduce it scaling llms uh you",
    "start": "242319",
    "end": "246159"
  },
  {
    "text": "have done for some of the users but when",
    "start": "246159",
    "end": "248120"
  },
  {
    "text": "you're going so vast you might be feel",
    "start": "248120",
    "end": "250640"
  },
  {
    "text": "confused we should allocate more CPU",
    "start": "250640",
    "end": "253200"
  },
  {
    "text": "gpus or how we should track like how",
    "start": "253200",
    "end": "255760"
  },
  {
    "text": "much memory gpus or CPUs we would need",
    "start": "255760",
    "end": "259040"
  },
  {
    "text": "and uh again how high resource",
    "start": "259040",
    "end": "261199"
  },
  {
    "text": "consumption because of that what you do",
    "start": "261199",
    "end": "263240"
  },
  {
    "text": "is you just increase even though it's",
    "start": "263240",
    "end": "265919"
  },
  {
    "text": "not",
    "start": "265919",
    "end": "266840"
  },
  {
    "text": "needed excessive gpus and ex CPUs that's",
    "start": "266840",
    "end": "269560"
  },
  {
    "text": "what what basically you do so these all",
    "start": "269560",
    "end": "271639"
  },
  {
    "text": "are the challenges I would say that's uh",
    "start": "271639",
    "end": "274840"
  },
  {
    "text": "I'm damn sure like if you are deploying",
    "start": "274840",
    "end": "276960"
  },
  {
    "text": "LMS or using it so you might be facing",
    "start": "276960",
    "end": "280360"
  },
  {
    "text": "so I'm not I'm not giving a guaranteed",
    "start": "280360",
    "end": "282560"
  },
  {
    "text": "like it will be solved but like uh if",
    "start": "282560",
    "end": "284160"
  },
  {
    "text": "you follow some best practices by the",
    "start": "284160",
    "end": "285600"
  },
  {
    "text": "end of the talk you will find a lot of",
    "start": "285600",
    "end": "287720"
  },
  {
    "text": "solutions okay let's just move first",
    "start": "287720",
    "end": "290680"
  },
  {
    "text": "thing like the understanding AEL so in",
    "start": "290680",
    "end": "293560"
  },
  {
    "text": "just uh General world uh the general",
    "start": "293560",
    "end": "296880"
  },
  {
    "text": "world I would say like autel is",
    "start": "296880",
    "end": "298199"
  },
  {
    "text": "something maybe a free work that is uh",
    "start": "298199",
    "end": "301520"
  },
  {
    "text": "created for uh I would say like just",
    "start": "301520",
    "end": "304479"
  },
  {
    "text": "taking all of the instrumenting data for",
    "start": "304479",
    "end": "307680"
  },
  {
    "text": "your melt so when I say melt so it's",
    "start": "307680",
    "end": "310039"
  },
  {
    "text": "like maybe your matrices it can be event",
    "start": "310039",
    "end": "312680"
  },
  {
    "text": "it can be your logs and it can be",
    "start": "312680",
    "end": "314840"
  },
  {
    "text": "dresses so that's how actually the you",
    "start": "314840",
    "end": "317680"
  },
  {
    "text": "can use the whole Orel framework",
    "start": "317680",
    "end": "320400"
  },
  {
    "text": "to logs and then maybe the visualizing",
    "start": "320400",
    "end": "323199"
  },
  {
    "text": "your effects so how AEL is going so",
    "start": "323199",
    "end": "327560"
  },
  {
    "text": "currently it supported more than 40",
    "start": "327560",
    "end": "329319"
  },
  {
    "text": "observ vendors so when I say supported",
    "start": "329319",
    "end": "331720"
  },
  {
    "text": "so it's like they have already gone some",
    "start": "331720",
    "end": "333639"
  },
  {
    "text": "integration plugins and a lot of stuff",
    "start": "333639",
    "end": "335840"
  },
  {
    "text": "integrated by many librar services and",
    "start": "335840",
    "end": "337680"
  },
  {
    "text": "apps so doesn't matter how you're",
    "start": "337680",
    "end": "339919"
  },
  {
    "text": "writing the code but if you want to use",
    "start": "339919",
    "end": "341680"
  },
  {
    "text": "the apis of open hotel or maybe just",
    "start": "341680",
    "end": "344520"
  },
  {
    "text": "want to use the collector you can do",
    "start": "344520",
    "end": "345960"
  },
  {
    "text": "that easily and adopted by numerous end",
    "start": "345960",
    "end": "349120"
  },
  {
    "text": "users so here I'm talking about a lot of",
    "start": "349120",
    "end": "350960"
  },
  {
    "text": "organizations who use a lot of use cases",
    "start": "350960",
    "end": "353520"
  },
  {
    "text": "who build on that and those are coming",
    "start": "353520",
    "end": "355800"
  },
  {
    "text": "with a lot of good Solutions and those",
    "start": "355800",
    "end": "357759"
  },
  {
    "text": "are even running on scales",
    "start": "357759",
    "end": "360440"
  },
  {
    "text": "okay uh let's jump right into the",
    "start": "360440",
    "end": "363639"
  },
  {
    "text": "understanding AEL collector so if you",
    "start": "363639",
    "end": "365639"
  },
  {
    "text": "see right here uh we do have our",
    "start": "365639",
    "end": "368319"
  },
  {
    "text": "microservices part so by this I mean to",
    "start": "368319",
    "end": "371479"
  },
  {
    "text": "say like so uh you do have your",
    "start": "371479",
    "end": "373479"
  },
  {
    "text": "microservices so whatever code you",
    "start": "373479",
    "end": "375280"
  },
  {
    "text": "writing for the code if you want to",
    "start": "375280",
    "end": "377319"
  },
  {
    "text": "integrate AEL it can goes like maybe the",
    "start": "377319",
    "end": "380280"
  },
  {
    "text": "uh instrumentation or API or the SDK so",
    "start": "380280",
    "end": "383199"
  },
  {
    "text": "that's how maybe you can just add in in",
    "start": "383199",
    "end": "385520"
  },
  {
    "text": "your code itself while you're writing",
    "start": "385520",
    "end": "387240"
  },
  {
    "text": "instead of just getting regret while",
    "start": "387240",
    "end": "389080"
  },
  {
    "text": "it's deployed",
    "start": "389080",
    "end": "390560"
  },
  {
    "text": "after that maybe you can move to the",
    "start": "390560",
    "end": "392000"
  },
  {
    "text": "shared infra so not sure if you're",
    "start": "392000",
    "end": "394280"
  },
  {
    "text": "deploying it on kubernetes it can go to",
    "start": "394280",
    "end": "396039"
  },
  {
    "text": "any Cloud platform or you want to start",
    "start": "396039",
    "end": "398319"
  },
  {
    "text": "with the load balancer and then",
    "start": "398319",
    "end": "400199"
  },
  {
    "text": "basically check all the traffic how it's",
    "start": "400199",
    "end": "401960"
  },
  {
    "text": "moving so you can do that and uh",
    "start": "401960",
    "end": "404400"
  },
  {
    "text": "obviously you can send all of the data",
    "start": "404400",
    "end": "406000"
  },
  {
    "text": "to the third party services like maybe",
    "start": "406000",
    "end": "407840"
  },
  {
    "text": "Jagger",
    "start": "407840",
    "end": "409000"
  },
  {
    "text": "Prometheus and more okay so we do have",
    "start": "409000",
    "end": "412639"
  },
  {
    "text": "autel collector here again we are going",
    "start": "412639",
    "end": "414680"
  },
  {
    "text": "with some client information if you want",
    "start": "414680",
    "end": "416360"
  },
  {
    "text": "to go with doing some stuff on your",
    "start": "416360",
    "end": "419039"
  },
  {
    "text": "whatever data you're getting you can do",
    "start": "419039",
    "end": "420599"
  },
  {
    "text": "that and yeah we do have a front end Epi",
    "start": "420599",
    "end": "423400"
  },
  {
    "text": "so by that you can basically getting all",
    "start": "423400",
    "end": "426160"
  },
  {
    "text": "the Tresses all of the logs and events",
    "start": "426160",
    "end": "428199"
  },
  {
    "text": "and then accordingly what function you",
    "start": "428199",
    "end": "430240"
  },
  {
    "text": "are writing uh that will be show to",
    "start": "430240",
    "end": "434280"
  },
  {
    "text": "you so uh like if we just combine",
    "start": "434720",
    "end": "438319"
  },
  {
    "text": "everything so autel collector basically",
    "start": "438319",
    "end": "440520"
  },
  {
    "text": "it's just vendor agnostic we to receive",
    "start": "440520",
    "end": "443560"
  },
  {
    "text": "just process and Export all of the data",
    "start": "443560",
    "end": "445879"
  },
  {
    "text": "for you okay so that's what it is uh",
    "start": "445879",
    "end": "449759"
  },
  {
    "text": "let's just jump to the profiling so if I",
    "start": "449759",
    "end": "453039"
  },
  {
    "text": "have to explain this to a 5year kid uh",
    "start": "453039",
    "end": "455360"
  },
  {
    "text": "like that's how I will do and that's",
    "start": "455360",
    "end": "456879"
  },
  {
    "text": "what this image is explaining I just",
    "start": "456879",
    "end": "458440"
  },
  {
    "text": "generated this with uh EI tool so uh",
    "start": "458440",
    "end": "462199"
  },
  {
    "text": "let's say as a 5-year kid if I'm having",
    "start": "462199",
    "end": "464400"
  },
  {
    "text": "a car and that's my favorite one and uh",
    "start": "464400",
    "end": "466639"
  },
  {
    "text": "it's not performing well okay and when",
    "start": "466639",
    "end": "469080"
  },
  {
    "text": "while it's not performing well I I might",
    "start": "469080",
    "end": "470800"
  },
  {
    "text": "look for issues like I should buy a",
    "start": "470800",
    "end": "473080"
  },
  {
    "text": "newer one uh upgraded one but the thing",
    "start": "473080",
    "end": "477440"
  },
  {
    "text": "what I am more curious about like why",
    "start": "477440",
    "end": "479639"
  },
  {
    "text": "can't solve this problem itself like why",
    "start": "479639",
    "end": "481840"
  },
  {
    "text": "this particular car is not working so on",
    "start": "481840",
    "end": "484000"
  },
  {
    "text": "that time if I dig more into the problem",
    "start": "484000",
    "end": "486599"
  },
  {
    "text": "then I might go into the battery ring",
    "start": "486599",
    "end": "488479"
  },
  {
    "text": "issue is there any particular the tire",
    "start": "488479",
    "end": "491520"
  },
  {
    "text": "or the wheel is getting stuck or maybe",
    "start": "491520",
    "end": "494159"
  },
  {
    "text": "it's the battery issues that is not able",
    "start": "494159",
    "end": "496039"
  },
  {
    "text": "to charge or something so the same way",
    "start": "496039",
    "end": "498400"
  },
  {
    "text": "if it's if you go with the computers if",
    "start": "498400",
    "end": "500479"
  },
  {
    "text": "you go with the softwares if you are",
    "start": "500479",
    "end": "502560"
  },
  {
    "text": "thinking like uh so basically profiling",
    "start": "502560",
    "end": "505599"
  },
  {
    "text": "is just a way to pinpoint exactly which",
    "start": "505599",
    "end": "509120"
  },
  {
    "text": "parts of your code are draining more",
    "start": "509120",
    "end": "510919"
  },
  {
    "text": "resources or more causing latency and",
    "start": "510919",
    "end": "513719"
  },
  {
    "text": "more basically having your performance",
    "start": "513719",
    "end": "516680"
  },
  {
    "text": "okay so by each and every Tresses of",
    "start": "516680",
    "end": "519800"
  },
  {
    "text": "your code how your apis are calling you",
    "start": "519800",
    "end": "522479"
  },
  {
    "text": "can basically check how much CPU or uh",
    "start": "522479",
    "end": "525200"
  },
  {
    "text": "gpus are used so that's how uh like",
    "start": "525200",
    "end": "528480"
  },
  {
    "text": "exciting it is and easy it is to figure",
    "start": "528480",
    "end": "530360"
  },
  {
    "text": "it out so I'm I'm sure you must be uh",
    "start": "530360",
    "end": "534279"
  },
  {
    "text": "want to explore it",
    "start": "534279",
    "end": "535760"
  },
  {
    "text": "more so yeah if I talk about the",
    "start": "535760",
    "end": "538000"
  },
  {
    "text": "profiling announcement so yeah utel just",
    "start": "538000",
    "end": "540760"
  },
  {
    "text": "announced this uh support for the",
    "start": "540760",
    "end": "542720"
  },
  {
    "text": "profiling in 2022 at cubon Europe so",
    "start": "542720",
    "end": "547120"
  },
  {
    "text": "after that a lot of works is going on",
    "start": "547120",
    "end": "550560"
  },
  {
    "text": "behind the scenes if you go to uh GitHub",
    "start": "550560",
    "end": "553440"
  },
  {
    "text": "do github.com hotel itself so you can",
    "start": "553440",
    "end": "556000"
  },
  {
    "text": "find a lot of things are going there and",
    "start": "556000",
    "end": "558440"
  },
  {
    "text": "uh if you see like how it came from the",
    "start": "558440",
    "end": "561600"
  },
  {
    "text": "proof filing in autel so yeah that's",
    "start": "561600",
    "end": "563640"
  },
  {
    "text": "what it is like basically elastic",
    "start": "563640",
    "end": "565399"
  },
  {
    "text": "donated one of their uh profiling agent",
    "start": "565399",
    "end": "568120"
  },
  {
    "text": "to open telemetry to use it to enhance",
    "start": "568120",
    "end": "570720"
  },
  {
    "text": "it to basically go more for it and if",
    "start": "570720",
    "end": "573600"
  },
  {
    "text": "you want to check it out more how the",
    "start": "573600",
    "end": "575640"
  },
  {
    "text": "things are going there so the best way",
    "start": "575640",
    "end": "578360"
  },
  {
    "text": "is to go to this particular rappo so the",
    "start": "578360",
    "end": "581519"
  },
  {
    "text": "one is highlighted so it's open",
    "start": "581519",
    "end": "583440"
  },
  {
    "text": "Telemetry ebpf profiler so that's where",
    "start": "583440",
    "end": "586240"
  },
  {
    "text": "you can go and that's where it's",
    "start": "586240",
    "end": "587680"
  },
  {
    "text": "actually uh the all of the process are",
    "start": "587680",
    "end": "590079"
  },
  {
    "text": "merged and work is still in",
    "start": "590079",
    "end": "594000"
  },
  {
    "text": "progress going forward as the talk title",
    "start": "595079",
    "end": "597920"
  },
  {
    "text": "said like how basically you can just uh",
    "start": "597920",
    "end": "600600"
  },
  {
    "text": "with the help of Tresses how you can",
    "start": "600600",
    "end": "602279"
  },
  {
    "text": "check your code so if you see here we do",
    "start": "602279",
    "end": "605040"
  },
  {
    "text": "have a multiple request the G post and",
    "start": "605040",
    "end": "608480"
  },
  {
    "text": "uh the last one is like the we are just",
    "start": "608480",
    "end": "610120"
  },
  {
    "text": "taking the livess for the G request",
    "start": "610120",
    "end": "612160"
  },
  {
    "text": "itself again so we have just captured",
    "start": "612160",
    "end": "615880"
  },
  {
    "text": "this one from one of our uh like one of",
    "start": "615880",
    "end": "618399"
  },
  {
    "text": "the model GPT gpt2 that we have deployed",
    "start": "618399",
    "end": "621480"
  },
  {
    "text": "in our cluster itself and that's where",
    "start": "621480",
    "end": "623720"
  },
  {
    "text": "from we have got these dresses and if",
    "start": "623720",
    "end": "625720"
  },
  {
    "text": "you want to just go by each and every",
    "start": "625720",
    "end": "628079"
  },
  {
    "text": "dresses how things are going do there",
    "start": "628079",
    "end": "629800"
  },
  {
    "text": "and how the calls are making and then",
    "start": "629800",
    "end": "631760"
  },
  {
    "text": "apis are behaving you can just maybe",
    "start": "631760",
    "end": "634240"
  },
  {
    "text": "click any of them and check it out the",
    "start": "634240",
    "end": "637279"
  },
  {
    "text": "transaction details of each and every",
    "start": "637279",
    "end": "639000"
  },
  {
    "text": "tress so the tress ID would be like",
    "start": "639000",
    "end": "641560"
  },
  {
    "text": "easier for you to just going forward and",
    "start": "641560",
    "end": "643839"
  },
  {
    "text": "maybe check uh like how the resource",
    "start": "643839",
    "end": "647480"
  },
  {
    "text": "consumptions is happening or",
    "start": "647480",
    "end": "649519"
  },
  {
    "text": "maybe uh how your code is behaving so on",
    "start": "649519",
    "end": "652680"
  },
  {
    "text": "that time it it it actually feels like",
    "start": "652680",
    "end": "655240"
  },
  {
    "text": "uh you have something to have you on",
    "start": "655240",
    "end": "657279"
  },
  {
    "text": "back okay so that's how so if you see",
    "start": "657279",
    "end": "660519"
  },
  {
    "text": "here we do have a route uh route that",
    "start": "660519",
    "end": "662399"
  },
  {
    "text": "is/ generate and for this we do have a",
    "start": "662399",
    "end": "665240"
  },
  {
    "text": "transaction basically the tress ID is",
    "start": "665240",
    "end": "667279"
  },
  {
    "text": "here and uh that's how actually we have",
    "start": "667279",
    "end": "670399"
  },
  {
    "text": "captured some of them maybe uh it would",
    "start": "670399",
    "end": "672839"
  },
  {
    "text": "be easier for you to jump right into",
    "start": "672839",
    "end": "675040"
  },
  {
    "text": "demo and have more details on it so yeah",
    "start": "675040",
    "end": "679360"
  },
  {
    "text": "let's move go inside the demo and see",
    "start": "679360",
    "end": "681920"
  },
  {
    "text": "how the things can move so yeah over to",
    "start": "681920",
    "end": "684160"
  },
  {
    "text": "you Sima for the demo",
    "start": "684160",
    "end": "688360"
  },
  {
    "text": "all right so um in this demo basically",
    "start": "690880",
    "end": "694240"
  },
  {
    "text": "we are using um a text generation model",
    "start": "694240",
    "end": "697839"
  },
  {
    "text": "uh and we are using hugging pH gpt2",
    "start": "697839",
    "end": "700120"
  },
  {
    "text": "model that's open source so um we can",
    "start": "700120",
    "end": "703959"
  },
  {
    "text": "use it and I have built uh fast a I've",
    "start": "703959",
    "end": "706959"
  },
  {
    "text": "used Fast API and basically there are",
    "start": "706959",
    "end": "709760"
  },
  {
    "text": "three apis that uh we have built and",
    "start": "709760",
    "end": "713440"
  },
  {
    "text": "these are all the libraries that we have",
    "start": "713440",
    "end": "715680"
  },
  {
    "text": "to use to all the uh to get all the",
    "start": "715680",
    "end": "719839"
  },
  {
    "text": "metrics traces events from the code to",
    "start": "719839",
    "end": "723519"
  },
  {
    "text": "open",
    "start": "723519",
    "end": "724360"
  },
  {
    "text": "Telemetry so this is one of the library",
    "start": "724360",
    "end": "727200"
  },
  {
    "text": "uh trace and then uh we are using since",
    "start": "727200",
    "end": "730959"
  },
  {
    "text": "we are using fast API so we have to",
    "start": "730959",
    "end": "732680"
  },
  {
    "text": "import that as well and uh we are using",
    "start": "732680",
    "end": "736360"
  },
  {
    "text": "open Telemetry SDK and some logging",
    "start": "736360",
    "end": "739480"
  },
  {
    "text": "systems so that we can see all the logs",
    "start": "739480",
    "end": "741920"
  },
  {
    "text": "of the application as well and we are",
    "start": "741920",
    "end": "744079"
  },
  {
    "text": "using Transformer library and uh since",
    "start": "744079",
    "end": "747320"
  },
  {
    "text": "we have used gpt2 so we have imported",
    "start": "747320",
    "end": "750120"
  },
  {
    "text": "the uh libraries and um",
    "start": "750120",
    "end": "755360"
  },
  {
    "text": "yeah yeah so uh the line number 13 is a",
    "start": "772480",
    "end": "777120"
  },
  {
    "text": "custom python file in which I have",
    "start": "777120",
    "end": "779800"
  },
  {
    "text": "configured all the open Telemetry",
    "start": "779800",
    "end": "781480"
  },
  {
    "text": "metrics for traces and this will get all",
    "start": "781480",
    "end": "784240"
  },
  {
    "text": "the traces logging and metrics to our",
    "start": "784240",
    "end": "787519"
  },
  {
    "text": "code uh to our open Telemetry",
    "start": "787519",
    "end": "790079"
  },
  {
    "text": "system and yeah it is basically um",
    "start": "790079",
    "end": "794160"
  },
  {
    "text": "getting all the meter provider logger",
    "start": "794160",
    "end": "797360"
  },
  {
    "text": "provider and Trace",
    "start": "797360",
    "end": "800360"
  },
  {
    "text": "provider I have uh included all the",
    "start": "800399",
    "end": "803959"
  },
  {
    "text": "necessary libraries which exports all",
    "start": "803959",
    "end": "807720"
  },
  {
    "text": "the metrics traces and",
    "start": "807720",
    "end": "810320"
  },
  {
    "text": "logs and uh these are the uh this is my",
    "start": "810320",
    "end": "814199"
  },
  {
    "text": "requirements.txt file so we are",
    "start": "814199",
    "end": "816240"
  },
  {
    "text": "installing all the necessary libraries",
    "start": "816240",
    "end": "818560"
  },
  {
    "text": "in",
    "start": "818560",
    "end": "820800"
  },
  {
    "text": "this right and um uh we have defined",
    "start": "824320",
    "end": "827880"
  },
  {
    "text": "conf uh logging so that um in code it",
    "start": "827880",
    "end": "831360"
  },
  {
    "text": "will be easier to understand our",
    "start": "831360",
    "end": "833199"
  },
  {
    "text": "application and uh we have initialized",
    "start": "833199",
    "end": "836199"
  },
  {
    "text": "tracing metrics and uh I have defined",
    "start": "836199",
    "end": "839759"
  },
  {
    "text": "some custom metrics so that we can",
    "start": "839759",
    "end": "841440"
  },
  {
    "text": "understand our llm better like tokens",
    "start": "841440",
    "end": "844440"
  },
  {
    "text": "for memory usage and these are the",
    "start": "844440",
    "end": "847079"
  },
  {
    "text": "logging for just so that we know that",
    "start": "847079",
    "end": "850120"
  },
  {
    "text": "our model is loaded and it has um it is",
    "start": "850120",
    "end": "853959"
  },
  {
    "text": "tokenized and um which model we are",
    "start": "853959",
    "end": "857279"
  },
  {
    "text": "using it's",
    "start": "857279",
    "end": "858399"
  },
  {
    "text": "gpt2 and now we have some apis so if you",
    "start": "858399",
    "end": "863959"
  },
  {
    "text": "go to slash it will generate some uh",
    "start": "863959",
    "end": "866920"
  },
  {
    "text": "welcome message and the main API is our",
    "start": "866920",
    "end": "869720"
  },
  {
    "text": "post API for Generate and uh this is",
    "start": "869720",
    "end": "872519"
  },
  {
    "text": "basically a text generation request so",
    "start": "872519",
    "end": "874880"
  },
  {
    "text": "whatever input we are providing it will",
    "start": "874880",
    "end": "877240"
  },
  {
    "text": "generate the text is a basic model and",
    "start": "877240",
    "end": "880959"
  },
  {
    "text": "uh we have",
    "start": "880959",
    "end": "882800"
  },
  {
    "text": "also um defined logging here as",
    "start": "882800",
    "end": "887880"
  },
  {
    "text": "well um once the text is generated it",
    "start": "887880",
    "end": "890759"
  },
  {
    "text": "will give the response and it will also",
    "start": "890759",
    "end": "893519"
  },
  {
    "text": "show how many tokens are remaining",
    "start": "893519",
    "end": "895800"
  },
  {
    "text": "because that's really important for our",
    "start": "895800",
    "end": "897240"
  },
  {
    "text": "llm models",
    "start": "897240",
    "end": "899199"
  },
  {
    "text": "and uh everything is generated in Json",
    "start": "899199",
    "end": "902800"
  },
  {
    "text": "and uh we have some liveness and",
    "start": "902800",
    "end": "904480"
  },
  {
    "text": "Readiness checks for our",
    "start": "904480",
    "end": "906440"
  },
  {
    "text": "application the these are just uh basic",
    "start": "906440",
    "end": "909120"
  },
  {
    "text": "get apis and uh lastly we have CPU usage",
    "start": "909120",
    "end": "913639"
  },
  {
    "text": "memory usage and at the end we are",
    "start": "913639",
    "end": "916519"
  },
  {
    "text": "starting our fast API server on Port",
    "start": "916519",
    "end": "921040"
  },
  {
    "text": "8,000 next is this is the dashboard that",
    "start": "922639",
    "end": "925240"
  },
  {
    "text": "we will see after starting our",
    "start": "925240",
    "end": "927600"
  },
  {
    "text": "application and uh you can see the",
    "start": "927600",
    "end": "930199"
  },
  {
    "text": "resources llm and we can see all types",
    "start": "930199",
    "end": "932759"
  },
  {
    "text": "of uh log levels debug information",
    "start": "932759",
    "end": "936040"
  },
  {
    "text": "errors even though it doesn't have any",
    "start": "936040",
    "end": "938360"
  },
  {
    "text": "error right now but it will show each",
    "start": "938360",
    "end": "940279"
  },
  {
    "text": "log level and once we click on any of",
    "start": "940279",
    "end": "943120"
  },
  {
    "text": "the log system it will show the trace ID",
    "start": "943120",
    "end": "946759"
  },
  {
    "text": "the content breakdown and uh you can see",
    "start": "946759",
    "end": "949920"
  },
  {
    "text": "the toen tokenized input and everything",
    "start": "949920",
    "end": "953560"
  },
  {
    "text": "and if you notice here it also shows",
    "start": "953560",
    "end": "956120"
  },
  {
    "text": "remaining tokens so in the code uh we",
    "start": "956120",
    "end": "959199"
  },
  {
    "text": "have defined Max tokens um so these are",
    "start": "959199",
    "end": "963319"
  },
  {
    "text": "the tokens that we can use at Max and",
    "start": "963319",
    "end": "966959"
  },
  {
    "text": "after each um API call it generates that",
    "start": "966959",
    "end": "970480"
  },
  {
    "text": "how many remaining tokens are",
    "start": "970480",
    "end": "974040"
  },
  {
    "text": "there right so these are all the metrics",
    "start": "980880",
    "end": "984399"
  },
  {
    "text": "like generated text and tokenized input",
    "start": "984399",
    "end": "987680"
  },
  {
    "text": "and each of the loging system that we",
    "start": "987680",
    "end": "989639"
  },
  {
    "text": "have added in our code and uh you will",
    "start": "989639",
    "end": "992720"
  },
  {
    "text": "you can customize the fields as well you",
    "start": "992720",
    "end": "995199"
  },
  {
    "text": "can check the service name runtime name",
    "start": "995199",
    "end": "998399"
  },
  {
    "text": "and uh if you go to traces since we have",
    "start": "998399",
    "end": "1001160"
  },
  {
    "text": "configured three apis post get and get",
    "start": "1001160",
    "end": "1004040"
  },
  {
    "text": "for Generate Readiness and liveness we",
    "start": "1004040",
    "end": "1006279"
  },
  {
    "text": "can explore more for each um API",
    "start": "1006279",
    "end": "1012199"
  },
  {
    "text": "basically so if you go to generate you",
    "start": "1012199",
    "end": "1014680"
  },
  {
    "text": "can see all the latency throughput how",
    "start": "1014680",
    "end": "1017240"
  },
  {
    "text": "much traffic is there there on the API",
    "start": "1017240",
    "end": "1021120"
  },
  {
    "text": "and uh we can see fail transaction rate",
    "start": "1021120",
    "end": "1023800"
  },
  {
    "text": "traces",
    "start": "1023800",
    "end": "1025319"
  },
  {
    "text": "samples and at the end we will we will",
    "start": "1025319",
    "end": "1028240"
  },
  {
    "text": "be able to see how much this Trace is",
    "start": "1028240",
    "end": "1031438"
  },
  {
    "text": "taking time and furthermore we can check",
    "start": "1031439",
    "end": "1034079"
  },
  {
    "text": "the labels the time stamp and it also",
    "start": "1034079",
    "end": "1036839"
  },
  {
    "text": "shows which agent we are using so we are",
    "start": "1036839",
    "end": "1039760"
  },
  {
    "text": "using open Telemetry python elastic and",
    "start": "1039760",
    "end": "1043438"
  },
  {
    "text": "uh we can check the event information",
    "start": "1043439",
    "end": "1046240"
  },
  {
    "text": "data stream host everything uh in detail",
    "start": "1046240",
    "end": "1050240"
  },
  {
    "text": "for a particular trace and we can find",
    "start": "1050240",
    "end": "1052480"
  },
  {
    "text": "out how much a particular time uh Trace",
    "start": "1052480",
    "end": "1055000"
  },
  {
    "text": "is taking time and we can further debug",
    "start": "1055000",
    "end": "1057559"
  },
  {
    "text": "to how to optimize that particular",
    "start": "1057559",
    "end": "1062240"
  },
  {
    "text": "call and U yeah so we can also see the",
    "start": "1062240",
    "end": "1067080"
  },
  {
    "text": "span ID the time stamp and Trace ID is",
    "start": "1067080",
    "end": "1070520"
  },
  {
    "text": "unique so you can also redirected to the",
    "start": "1070520",
    "end": "1074200"
  },
  {
    "text": "logs as well for that particular Trace",
    "start": "1074200",
    "end": "1076840"
  },
  {
    "text": "ID",
    "start": "1076840",
    "end": "1079840"
  },
  {
    "text": "yeah and it's it also shows how much",
    "start": "1080640",
    "end": "1083400"
  },
  {
    "text": "time it took for this particular API",
    "start": "1083400",
    "end": "1085440"
  },
  {
    "text": "call it's 168 microc",
    "start": "1085440",
    "end": "1088159"
  },
  {
    "text": "seconds and same with the uh other one",
    "start": "1088159",
    "end": "1091000"
  },
  {
    "text": "for span details we are using again uh",
    "start": "1091000",
    "end": "1093640"
  },
  {
    "text": "it's the same",
    "start": "1093640",
    "end": "1096000"
  },
  {
    "text": "agent and we can also check the logs so",
    "start": "1096000",
    "end": "1099520"
  },
  {
    "text": "if you notice here remaining tokens text",
    "start": "1099520",
    "end": "1102799"
  },
  {
    "text": "generation took this 17 seconds and uh",
    "start": "1102799",
    "end": "1107799"
  },
  {
    "text": "yeah",
    "start": "1107799",
    "end": "1110760"
  },
  {
    "text": "and the main part is the profiling that",
    "start": "1116440",
    "end": "1119400"
  },
  {
    "text": "we are talking about uh here so um to",
    "start": "1119400",
    "end": "1123000"
  },
  {
    "text": "visualize the profiling we can see all",
    "start": "1123000",
    "end": "1126120"
  },
  {
    "text": "the threads that are currently running",
    "start": "1126120",
    "end": "1128919"
  },
  {
    "text": "and we can also explore since this is we",
    "start": "1128919",
    "end": "1131520"
  },
  {
    "text": "are using Python 3 so we can check all",
    "start": "1131520",
    "end": "1135200"
  },
  {
    "text": "the traces um in um stack traces form it",
    "start": "1135200",
    "end": "1139919"
  },
  {
    "text": "and uh we can explore which Library we",
    "start": "1139919",
    "end": "1142559"
  },
  {
    "text": "are using so we have used torch and U",
    "start": "1142559",
    "end": "1146640"
  },
  {
    "text": "that is one of the most um highly used",
    "start": "1146640",
    "end": "1150520"
  },
  {
    "text": "CPU as",
    "start": "1150520",
    "end": "1153440"
  },
  {
    "text": "well so in this you can see top 100 um",
    "start": "1156039",
    "end": "1160559"
  },
  {
    "text": "traces here all the libraries are",
    "start": "1160559",
    "end": "1163919"
  },
  {
    "text": "mentioned and if you see flame graphs",
    "start": "1163919",
    "end": "1168159"
  },
  {
    "text": "you can explore each and every uh traces",
    "start": "1168159",
    "end": "1171520"
  },
  {
    "text": "basically and U if you go to this Python",
    "start": "1171520",
    "end": "1174960"
  },
  {
    "text": "and uh",
    "start": "1174960",
    "end": "1176679"
  },
  {
    "text": "explore um gpt2 and uh you can see uh",
    "start": "1176679",
    "end": "1182600"
  },
  {
    "text": "how much CPU it is consuming and uh how",
    "start": "1182600",
    "end": "1187200"
  },
  {
    "text": "much does it cost and it also shows how",
    "start": "1187200",
    "end": "1191159"
  },
  {
    "text": "much uh carbon Omission is there because",
    "start": "1191159",
    "end": "1194720"
  },
  {
    "text": "uh we have to be careful about the uh um",
    "start": "1194720",
    "end": "1200000"
  },
  {
    "text": "sustainability as well since llm models",
    "start": "1200000",
    "end": "1202760"
  },
  {
    "text": "are pretty huge and that consumes a lot",
    "start": "1202760",
    "end": "1205360"
  },
  {
    "text": "of compute poers and um so this shows",
    "start": "1205360",
    "end": "1210360"
  },
  {
    "text": "carbon emission it is using",
    "start": "1210360",
    "end": "1214520"
  },
  {
    "text": "18.74% for this particular um trace and",
    "start": "1223720",
    "end": "1228679"
  },
  {
    "text": "examples are this so these traces are",
    "start": "1228679",
    "end": "1231559"
  },
  {
    "text": "basically running on Locust so we have",
    "start": "1231559",
    "end": "1234320"
  },
  {
    "text": "generated some U low test um using",
    "start": "1234320",
    "end": "1240440"
  },
  {
    "text": "Locust so um since llms are pretty huge",
    "start": "1242039",
    "end": "1246640"
  },
  {
    "text": "it will cost us it can cost us around in",
    "start": "1246640",
    "end": "1249799"
  },
  {
    "text": "50ks or more than that and uh since",
    "start": "1249799",
    "end": "1253679"
  },
  {
    "text": "profiling is int introduced you can dig",
    "start": "1253679",
    "end": "1256640"
  },
  {
    "text": "more deeper into that and reduce the",
    "start": "1256640",
    "end": "1259159"
  },
  {
    "text": "costs as",
    "start": "1259159",
    "end": "1261520"
  },
  {
    "text": "well and uh uh after profiling is",
    "start": "1261520",
    "end": "1264880"
  },
  {
    "text": "introduced we have seen that uh it has",
    "start": "1264880",
    "end": "1268039"
  },
  {
    "text": "reduced a lot of cost as well for the",
    "start": "1268039",
    "end": "1272080"
  },
  {
    "text": "applications so we can uh check from",
    "start": "1272080",
    "end": "1275640"
  },
  {
    "text": "from writing code to deploying the llm",
    "start": "1275640",
    "end": "1279760"
  },
  {
    "text": "uh llm to production we can see all the",
    "start": "1279760",
    "end": "1283080"
  },
  {
    "text": "traces all the memory leaks uh and",
    "start": "1283080",
    "end": "1285919"
  },
  {
    "text": "optimize our code as well not just sign",
    "start": "1285919",
    "end": "1288919"
  },
  {
    "text": "infrastructure and we can reduce the",
    "start": "1288919",
    "end": "1291080"
  },
  {
    "text": "costs and we can optimize our",
    "start": "1291080",
    "end": "1295200"
  },
  {
    "text": "llm and uh also since it was on",
    "start": "1296640",
    "end": "1299679"
  },
  {
    "text": "kubernetes we can also Define all the",
    "start": "1299679",
    "end": "1302320"
  },
  {
    "text": "infrastructure metrics like we can add",
    "start": "1302320",
    "end": "1304720"
  },
  {
    "text": "CPUs gpus and we can set limits and",
    "start": "1304720",
    "end": "1307559"
  },
  {
    "text": "requests as well we can use schedule GPU",
    "start": "1307559",
    "end": "1310720"
  },
  {
    "text": "pods and we can use spot instances like",
    "start": "1310720",
    "end": "1313760"
  },
  {
    "text": "that okay so going forward maybe like uh",
    "start": "1313760",
    "end": "1316440"
  },
  {
    "text": "Asim already explained uh like the quick",
    "start": "1316440",
    "end": "1318720"
  },
  {
    "text": "Mo and then other stuff but how about",
    "start": "1318720",
    "end": "1321039"
  },
  {
    "text": "like from developing to deploying what",
    "start": "1321039",
    "end": "1322760"
  },
  {
    "text": "are the things you can majorly concern",
    "start": "1322760",
    "end": "1324240"
  },
  {
    "text": "so maybe this is a slide that help you",
    "start": "1324240",
    "end": "1326559"
  },
  {
    "text": "for that so like if you see in the",
    "start": "1326559",
    "end": "1329120"
  },
  {
    "text": "development part you can basically track",
    "start": "1329120",
    "end": "1331880"
  },
  {
    "text": "CPUs this for token how the particular",
    "start": "1331880",
    "end": "1334360"
  },
  {
    "text": "tokon is using CPU and the API calls and",
    "start": "1334360",
    "end": "1336840"
  },
  {
    "text": "accordingly memory and latency for the",
    "start": "1336840",
    "end": "1338559"
  },
  {
    "text": "same and for the deployment you should",
    "start": "1338559",
    "end": "1340559"
  },
  {
    "text": "be more concerned about the GP",
    "start": "1340559",
    "end": "1342360"
  },
  {
    "text": "utilization and uh the resources that",
    "start": "1342360",
    "end": "1345320"
  },
  {
    "text": "are consuming more resources that is not",
    "start": "1345320",
    "end": "1347880"
  },
  {
    "text": "required for the them and um yeah that's",
    "start": "1347880",
    "end": "1350919"
  },
  {
    "text": "all about it you can maybe uh catch up",
    "start": "1350919",
    "end": "1354279"
  },
  {
    "text": "us on LinkedIn or anywhere so that's all",
    "start": "1354279",
    "end": "1358880"
  },
  {
    "text": "okay I saw there was at least one",
    "start": "1358880",
    "end": "1360640"
  },
  {
    "text": "question here so we'll start here at the",
    "start": "1360640",
    "end": "1362039"
  },
  {
    "text": "front kind of interesting end here",
    "start": "1362039",
    "end": "1364919"
  },
  {
    "text": "please so you kind of addressed this at",
    "start": "1364919",
    "end": "1367200"
  },
  {
    "text": "the end uh but obviously observability",
    "start": "1367200",
    "end": "1370400"
  },
  {
    "text": "is important for any application why is",
    "start": "1370400",
    "end": "1372799"
  },
  {
    "text": "it even more important if so for llms",
    "start": "1372799",
    "end": "1376440"
  },
  {
    "text": "and if someone is just starting out with",
    "start": "1376440",
    "end": "1377760"
  },
  {
    "text": "llms why do they care about this what",
    "start": "1377760",
    "end": "1380559"
  },
  {
    "text": "information can they take away that",
    "start": "1380559",
    "end": "1382240"
  },
  {
    "text": "would really help them as a kind of llm",
    "start": "1382240",
    "end": "1384760"
  },
  {
    "text": "newbie Okay cool so I was that's a good",
    "start": "1384760",
    "end": "1387480"
  },
  {
    "text": "question uh Asim you want to take it or",
    "start": "1387480",
    "end": "1389039"
  },
  {
    "text": "I should sure you can take it okay so",
    "start": "1389039",
    "end": "1392559"
  },
  {
    "text": "like uh the way you asking like maybe",
    "start": "1392559",
    "end": "1395159"
  },
  {
    "text": "the llms how like if someone is moving",
    "start": "1395159",
    "end": "1398039"
  },
  {
    "text": "towards them and then how it's basically",
    "start": "1398039",
    "end": "1400679"
  },
  {
    "text": "more concerned for them right while",
    "start": "1400679",
    "end": "1402559"
  },
  {
    "text": "going to llms is it the same or question",
    "start": "1402559",
    "end": "1406640"
  },
  {
    "text": "is it even more important",
    "start": "1406640",
    "end": "1408960"
  },
  {
    "text": "superability is always important for any",
    "start": "1408960",
    "end": "1410679"
  },
  {
    "text": "application yeah yeah for yeah for LMS",
    "start": "1410679",
    "end": "1412320"
  },
  {
    "text": "it's more important yeah why it is more",
    "start": "1412320",
    "end": "1414240"
  },
  {
    "text": "important for LS yes so the reason being",
    "start": "1414240",
    "end": "1416960"
  },
  {
    "text": "why so see uh currently previously when",
    "start": "1416960",
    "end": "1418919"
  },
  {
    "text": "you were using the normal applications",
    "start": "1418919",
    "end": "1420640"
  },
  {
    "text": "we just used to deploy it when you",
    "start": "1420640",
    "end": "1422279"
  },
  {
    "text": "observe it you just take the all the",
    "start": "1422279",
    "end": "1423919"
  },
  {
    "text": "logs matrics and all it's all good",
    "start": "1423919",
    "end": "1425799"
  },
  {
    "text": "whenever something is failing you just",
    "start": "1425799",
    "end": "1427159"
  },
  {
    "text": "get the triggers and that's all good but",
    "start": "1427159",
    "end": "1429080"
  },
  {
    "text": "when it comes to llms what basically",
    "start": "1429080",
    "end": "1431200"
  },
  {
    "text": "happening is you it consumes more of the",
    "start": "1431200",
    "end": "1433600"
  },
  {
    "text": "GPU CPU and resources but it basically",
    "start": "1433600",
    "end": "1436799"
  },
  {
    "text": "goes from Dev station prodad what",
    "start": "1436799",
    "end": "1438880"
  },
  {
    "text": "happens is like in Dev you have some",
    "start": "1438880",
    "end": "1440279"
  },
  {
    "text": "limited resources defined but when you",
    "start": "1440279",
    "end": "1442600"
  },
  {
    "text": "have a lot of requests are coming in",
    "start": "1442600",
    "end": "1444799"
  },
  {
    "text": "unnecessarily a lot of Ops guys what",
    "start": "1444799",
    "end": "1447000"
  },
  {
    "text": "they do is basically to resolve the",
    "start": "1447000",
    "end": "1448320"
  },
  {
    "text": "issues they just expand the CPU and GPU",
    "start": "1448320",
    "end": "1451960"
  },
  {
    "text": "and a lot of cost you will be just uh",
    "start": "1451960",
    "end": "1455480"
  },
  {
    "text": "it's going V basically it's not using",
    "start": "1455480",
    "end": "1457679"
  },
  {
    "text": "Okay so let's say there's application",
    "start": "1457679",
    "end": "1459360"
  },
  {
    "text": "that's only using two uh two uh I would",
    "start": "1459360",
    "end": "1461559"
  },
  {
    "text": "say two CPUs and you have defined four",
    "start": "1461559",
    "end": "1463400"
  },
  {
    "text": "for it then the 2 CPUs is going in like",
    "start": "1463400",
    "end": "1465600"
  },
  {
    "text": "it's not useful for you right so it's",
    "start": "1465600",
    "end": "1468000"
  },
  {
    "text": "better to have the visibility from",
    "start": "1468000",
    "end": "1469880"
  },
  {
    "text": "profiling and from the code level itself",
    "start": "1469880",
    "end": "1471760"
  },
  {
    "text": "the what the particular code is using",
    "start": "1471760",
    "end": "1473559"
  },
  {
    "text": "instead of just Define it",
    "start": "1473559",
    "end": "1477520"
  }
]