[
  {
    "text": "okay hello hello everyone how you doing great yeah yeah yeah yeah okay uh",
    "start": "120",
    "end": "9480"
  },
  {
    "text": "today I'm going to uh give the talk how we scale a distributed Cal database to",
    "start": "9480",
    "end": "15719"
  },
  {
    "text": "PB level yeah uh a little bit about",
    "start": "15719",
    "end": "22720"
  },
  {
    "text": "myself um I'm working for tidb and uh I'm the I'm Mainer of Tai K and also",
    "start": "22720",
    "end": "29599"
  },
  {
    "text": "Rock be contributor I used to wrote a book about DB internal and for the past",
    "start": "29599",
    "end": "37000"
  },
  {
    "text": "decade I build Tai kwi and tidb from the scratch with other colleagues and this is my GitHub and the",
    "start": "37000",
    "end": "44239"
  },
  {
    "text": "technical block yeah this is today's uh agenda we",
    "start": "44239",
    "end": "51440"
  },
  {
    "text": "uh mainly have uh four sections the first one I'd like to uh briefly",
    "start": "51440",
    "end": "56640"
  },
  {
    "text": "introduce the importance of database capability for your business growth and",
    "start": "56640",
    "end": "62359"
  },
  {
    "text": "the second section I'm going to uh introduce a little bit about our product",
    "start": "62359",
    "end": "67560"
  },
  {
    "text": "TB and its adoptions and the third part is uh focus on the challenges we have",
    "start": "67560",
    "end": "74439"
  },
  {
    "text": "faced during uh the past decade we are build uh when we were building this",
    "start": "74439",
    "end": "80000"
  },
  {
    "text": "product and uh when we uh support our customers and the last section is about",
    "start": "80000",
    "end": "86640"
  },
  {
    "text": "uh QA if you have any question yeah we can and you know have",
    "start": "86640",
    "end": "92640"
  },
  {
    "text": "competition so let's move to the first step first part um business growth is a",
    "start": "92640",
    "end": "99880"
  },
  {
    "text": "great thing uh for companies yeah we have more users more",
    "start": "99880",
    "end": "106280"
  },
  {
    "text": "tenance and we deliver new features for our customers for our new fe uh new",
    "start": "106280",
    "end": "112159"
  },
  {
    "text": "users and existing users and uh definitely there are more datas collect from your uh users",
    "start": "112159",
    "end": "121600"
  },
  {
    "text": "and from the technical perspective there might some like challenges because uh",
    "start": "121600",
    "end": "127960"
  },
  {
    "text": "for your application server or for your backend database there are more connections there more it means more",
    "start": "127960",
    "end": "135120"
  },
  {
    "text": "request more datas and U yeah more data need to",
    "start": "135120",
    "end": "142599"
  },
  {
    "text": "maintain so uh let me give an uh real example to uh to show this more",
    "start": "142599",
    "end": "153720"
  },
  {
    "text": "specific B is a fast growing Mobility company from the Europe and uh they have",
    "start": "154000",
    "end": "161519"
  },
  {
    "text": "around 100 million users yeah it's a great great number and they provide",
    "start": "161519",
    "end": "167400"
  },
  {
    "text": "service in uh more than 500 cities and uh since pandemic their",
    "start": "167400",
    "end": "175280"
  },
  {
    "text": "business has 400% growth yeah this is a great thing for them but from the uh",
    "start": "175280",
    "end": "182800"
  },
  {
    "text": "technical perspective they also um faced the you know some CH challenges in total",
    "start": "182800",
    "end": "188879"
  },
  {
    "text": "they have hundreds of uh terabytes to uh PB level dat volume and uh every day they will have",
    "start": "188879",
    "end": "197959"
  },
  {
    "text": "100 service deployment and uh they need to uh do 100",
    "start": "197959",
    "end": "204959"
  },
  {
    "text": "you know more than 100 database change I mean the schema change or our dat",
    "start": "204959",
    "end": "211000"
  },
  {
    "text": "migration every week and uh every month there are more",
    "start": "211000",
    "end": "217040"
  },
  {
    "text": "than like uh around 5 to 10 terab new data generated and they use my",
    "start": "217040",
    "end": "224599"
  },
  {
    "text": "sqle so um you can imagine definitely they hit the you know single my instance",
    "start": "224599",
    "end": "230480"
  },
  {
    "text": "size limitation and for some large tables if they want to add some",
    "start": "230480",
    "end": "237560"
  },
  {
    "text": "index it will take them it took them um two weeks yeah this happened in their in",
    "start": "237560",
    "end": "244239"
  },
  {
    "text": "their case and back up you can imagine how long it will",
    "start": "244239",
    "end": "250519"
  },
  {
    "text": "take yeah and uh the result is they need to maintain",
    "start": "250519",
    "end": "256400"
  },
  {
    "text": "hundreds of my instance this is a their",
    "start": "256400",
    "end": "262238"
  },
  {
    "text": "story so uh facing when facing the um grow Challenger the tradition additional",
    "start": "262240",
    "end": "270199"
  },
  {
    "text": "solution is Shing yeah um you sh your database you sh",
    "start": "270199",
    "end": "278120"
  },
  {
    "text": "applications but this also means you need to have some compromise yeah you need to rewrite your",
    "start": "278120",
    "end": "285479"
  },
  {
    "text": "applications called you need to you know um compromise some functionality like if",
    "start": "285479",
    "end": "291919"
  },
  {
    "text": "you want to uh carry some data across you know the",
    "start": "291919",
    "end": "298039"
  },
  {
    "text": "table and you you can imagine how to make a you know consistent snapshot backup for your",
    "start": "298039",
    "end": "306039"
  },
  {
    "text": "database if you have sh your data cross multiple nodes all this is a you know",
    "start": "306039",
    "end": "312960"
  },
  {
    "text": "reality we uh if if we choose this solution we need to",
    "start": "312960",
    "end": "318840"
  },
  {
    "text": "face so for the past decades we focus on you know solve this uh solve this issues",
    "start": "318840",
    "end": "326800"
  },
  {
    "text": "we want to you know build a c coded base it's scalable it means you can simply just",
    "start": "326800",
    "end": "333600"
  },
  {
    "text": "add nodes for this system and then everything can go and uh you don't have",
    "start": "333600",
    "end": "340720"
  },
  {
    "text": "to change your application code and you can use all the SQL you want and um at",
    "start": "340720",
    "end": "349400"
  },
  {
    "text": "the same time with the even there is a large amount of data we can you know we",
    "start": "349400",
    "end": "355440"
  },
  {
    "text": "want to um we want to achieve the low read and the right tail",
    "start": "355440",
    "end": "361039"
  },
  {
    "text": "latency and uh of course this system is high availability so this is the an",
    "start": "361039",
    "end": "367120"
  },
  {
    "text": "effort uh for the past decade we uh have paid yeah this is where tidle B comes",
    "start": "367120",
    "end": "376280"
  },
  {
    "text": "from so what is tidb TB is an open source project and you might are",
    "start": "376280",
    "end": "383240"
  },
  {
    "text": "familiar with Tai KV yeah it's a uh cncf graduated project",
    "start": "383240",
    "end": "390120"
  },
  {
    "text": "um actually uh ta was uh created as the storage layer for tidb and there is a",
    "start": "390120",
    "end": "397960"
  },
  {
    "text": "funny um story about taoi open source um actually we open sourc the taoi at 2016",
    "start": "397960",
    "end": "406400"
  },
  {
    "text": "uh April 1st uh you know it's a fourth day four day yes yeah um we don't",
    "start": "406400",
    "end": "413560"
  },
  {
    "text": "noticed that after we posted this message to our you know social media and",
    "start": "413560",
    "end": "419680"
  },
  {
    "text": "one don't believe it and then and we realize oh it's April 1 day yeah and and",
    "start": "419680",
    "end": "428120"
  },
  {
    "text": "and and benefit of this is all uh even uh several years has passed I remember",
    "start": "428120",
    "end": "434680"
  },
  {
    "text": "this exact date and this is tidb architecture it's",
    "start": "434680",
    "end": "442720"
  },
  {
    "text": "compatible with myle you can uh think it as a size unlimited myle and you you",
    "start": "442720",
    "end": "449520"
  },
  {
    "text": "application can connect to uh the the Tidy beef with myle protocol and the",
    "start": "449520",
    "end": "455440"
  },
  {
    "text": "myle ecosystem tools uh you can uh just use it and um TB is a disagre compute",
    "start": "455440",
    "end": "463919"
  },
  {
    "text": "and the storage architecture and so you can uh scale the compute layer or the",
    "start": "463919",
    "end": "469960"
  },
  {
    "text": "search layer respectively of course it's a high availability",
    "start": "469960",
    "end": "475560"
  },
  {
    "text": "system and uh the p is a metadata Center so uh",
    "start": "475560",
    "end": "483240"
  },
  {
    "text": "if you know Tae actually PD and taqe uh is know as the the the Tai KV yeah and",
    "start": "483240",
    "end": "492199"
  },
  {
    "text": "meanwhile we also have a colum engine which can replicate the data from",
    "start": "492199",
    "end": "498919"
  },
  {
    "text": "our Tai engine and in real time so uh the ti DB can carry some like heavy uh",
    "start": "498919",
    "end": "505960"
  },
  {
    "text": "jording uh or heavy aggregation uh",
    "start": "505960",
    "end": "511039"
  },
  {
    "text": "queres so let me uh show two use case one is flip card which is a largest",
    "start": "511039",
    "end": "518039"
  },
  {
    "text": "e-commerce company in India and uh a few years ago the they were search a",
    "start": "518039",
    "end": "525000"
  },
  {
    "text": "scalable database for their you know big b day which is a black day uh similar",
    "start": "525000",
    "end": "531120"
  },
  {
    "text": "here like black day here and the one a database that can you know just add some",
    "start": "531120",
    "end": "536880"
  },
  {
    "text": "nodes to the system and then the um support of the whole system can you know",
    "start": "536880",
    "end": "543360"
  },
  {
    "text": "increase at the uh at their demand so uh several years ago they come",
    "start": "543360",
    "end": "550440"
  },
  {
    "text": "to us and now uh tid B has with flip card has experiment uh two years big",
    "start": "550440",
    "end": "558000"
  },
  {
    "text": "billion days yeah and everything goes well and this is a test report by flip",
    "start": "558000",
    "end": "563839"
  },
  {
    "text": "cart they uh use around 40 machines to reach more than 1 million",
    "start": "563839",
    "end": "570120"
  },
  {
    "text": "QPS with the tail latency in single single digit Laten uh single digit",
    "start": "570120",
    "end": "577440"
  },
  {
    "text": "millisecond and the other use case uh this is a large cluster with uh more",
    "start": "578079",
    "end": "584160"
  },
  {
    "text": "than five 500 terab compressed data and",
    "start": "584160",
    "end": "589399"
  },
  {
    "text": "uh this is a um online social platform they have a",
    "start": "589399",
    "end": "595320"
  },
  {
    "text": "three more than 300 million registered users",
    "start": "595320",
    "end": "600640"
  },
  {
    "text": "and uh what impressed uh impressed me is there is 1.8 Bill uh trillion records in",
    "start": "600640",
    "end": "607920"
  },
  {
    "text": "single one large table in this case and T is also used in uh some North",
    "start": "607920",
    "end": "617320"
  },
  {
    "text": "American users like uh date breaks Pinterest and linking uh and the plat",
    "start": "617320",
    "end": "623600"
  },
  {
    "text": "pla is a financial s company",
    "start": "623600",
    "end": "628720"
  },
  {
    "text": "okay let's to uh move to our third part is about the challenge we have faced and",
    "start": "629640",
    "end": "636000"
  },
  {
    "text": "our solution uh from the architecture you can see um the PD part is a metadata",
    "start": "636000",
    "end": "642360"
  },
  {
    "text": "Center the sour layer we have a multiple Tyco nodes the data is store here and",
    "start": "642360",
    "end": "648279"
  },
  {
    "text": "the and the metadata is store over PD layer which is a u",
    "start": "648279",
    "end": "655040"
  },
  {
    "text": "cluster and what PD do is um it need to monitor the health of the whole cluster",
    "start": "655040",
    "end": "663040"
  },
  {
    "text": "if there is some like Tai K is UN available or has some issues or network",
    "start": "663040",
    "end": "668399"
  },
  {
    "text": "issues or disk issues and then uh the the PD need to",
    "start": "668399",
    "end": "673639"
  },
  {
    "text": "understand need to need to know it need to recognize it and then do the uh",
    "start": "673639",
    "end": "678959"
  },
  {
    "text": "balancing because data um is replicate across multiple Ty ways so",
    "start": "678959",
    "end": "686160"
  },
  {
    "text": "single Ty ways failure doesn't uh impact the the the the",
    "start": "686160",
    "end": "692920"
  },
  {
    "text": "service and uh the other way uh the other task the PD need to do is they",
    "start": "693040",
    "end": "698920"
  },
  {
    "text": "need to catch all the you know data range location and the leadership to uh",
    "start": "698920",
    "end": "706959"
  },
  {
    "text": "to to to the metata center so if there are uh so the request come and uh it can",
    "start": "706959",
    "end": "713959"
  },
  {
    "text": "know uh where the connection need to connect",
    "start": "713959",
    "end": "721240"
  },
  {
    "text": "uh because we uh you know we we partition the data uh into like hundreds",
    "start": "721360",
    "end": "727279"
  },
  {
    "text": "metab megabytes size level so if there if there are why we choose such um side",
    "start": "727279",
    "end": "736000"
  },
  {
    "text": "because we want to the we want the schedule is is very fast and if there is",
    "start": "736000",
    "end": "742240"
  },
  {
    "text": "you know High work CL uh to one single partition we can we can uh you know",
    "start": "742240",
    "end": "749320"
  },
  {
    "text": "split it and uh and U and uh and rebal rebalance",
    "start": "749320",
    "end": "755120"
  },
  {
    "text": "it yeah and uh PD also can monitor you know the workload of different data",
    "start": "755120",
    "end": "761760"
  },
  {
    "text": "ranges this is uh this is a uh happen when tawi you know um has some request",
    "start": "761760",
    "end": "769760"
  },
  {
    "text": "and tawi will uh collect statistics from",
    "start": "769760",
    "end": "774920"
  },
  {
    "text": "different date range and then report all these statistics in real time to to the",
    "start": "774920",
    "end": "780079"
  },
  {
    "text": "PD Center to to the metadata Center and then uh the PD can you know know which",
    "start": "780079",
    "end": "786720"
  },
  {
    "text": "part is is H which part is you know um is not and then they can balance the",
    "start": "786720",
    "end": "793440"
  },
  {
    "text": "workload automatically to make sure uh the the workload is uh uh evenly",
    "start": "793440",
    "end": "799440"
  },
  {
    "text": "distributed so you can you can see there are a lot of things PD PD is is uh",
    "start": "799440",
    "end": "804920"
  },
  {
    "text": "taking responsibilities so the first uh uh scalable issue we we met is how we can",
    "start": "804920",
    "end": "813079"
  },
  {
    "text": "make the metadata Center scalable so we um we we did we do did",
    "start": "813079",
    "end": "819199"
  },
  {
    "text": "some optimizations like uh we can make the PD follower can also uh serve the",
    "start": "819199",
    "end": "828560"
  },
  {
    "text": "request and meanwhile oh there is some issue with",
    "start": "828560",
    "end": "834320"
  },
  {
    "text": "adapter okay and meanwhile because there are more tasks PD uh is handle is handling",
    "start": "834320",
    "end": "843440"
  },
  {
    "text": "so we uh disaggregate the the PD into several services and deploy them into uh",
    "start": "843440",
    "end": "852000"
  },
  {
    "text": "dedicate resources so uh we can make sure each of",
    "start": "852000",
    "end": "857079"
  },
  {
    "text": "them can scale respectively like the TSO which is the",
    "start": "857079",
    "end": "863519"
  },
  {
    "text": "time time stamp service and the metadata Management Service statistics and the in",
    "start": "863519",
    "end": "871240"
  },
  {
    "text": "service and as I have just mentioned uh Hotpot issue is a uh common issue for a",
    "start": "871240",
    "end": "879320"
  },
  {
    "text": "large distributed system if there is you know single node or minor minority node",
    "start": "879320",
    "end": "886079"
  },
  {
    "text": "take you know the majority work workload from the whole system then this node can",
    "start": "886079",
    "end": "894680"
  },
  {
    "text": "be the you know bottom neck of the whole system so how to handle",
    "start": "894680",
    "end": "902240"
  },
  {
    "text": "it we I just mentioned we can collect you know the uh statistics and the",
    "start": "902920",
    "end": "908920"
  },
  {
    "text": "workload of different date partitions and report them to uh the",
    "start": "908920",
    "end": "914199"
  },
  {
    "text": "shading uh service and then the shading service can you know uh can trigger the split and",
    "start": "914199",
    "end": "923120"
  },
  {
    "text": "the balancing Mount to to the um to the taqu node",
    "start": "923120",
    "end": "929399"
  },
  {
    "text": "but there is also a Hotpot issue that uh the right Hotpot issue that we cannot",
    "start": "929399",
    "end": "935680"
  },
  {
    "text": "you know automatically handle that is I will I will show show you an example if",
    "start": "935680",
    "end": "941399"
  },
  {
    "text": "the data is always appended to one single partition app to the tail we have",
    "start": "941399",
    "end": "947600"
  },
  {
    "text": "no idea we we don't we don't we cannot you know automatically split them and balance by this way we cannot handle",
    "start": "947600",
    "end": "955519"
  },
  {
    "text": "it and for the read part uh we basically at the same with the with the with the",
    "start": "955519",
    "end": "962319"
  },
  {
    "text": "right part but there is some slightly difference for example if if there is a",
    "start": "962319",
    "end": "968680"
  },
  {
    "text": "high support for a single data prodution it's already is small like there is a",
    "start": "968680",
    "end": "976480"
  },
  {
    "text": "there is 100 row table and it's very hot and uh our you know um we we",
    "start": "976480",
    "end": "984639"
  },
  {
    "text": "introduced a feature named the load based split and we can collect you know",
    "start": "984639",
    "end": "990160"
  },
  {
    "text": "uh which row and which data range has has uh specific highs report and then we",
    "start": "990160",
    "end": "997920"
  },
  {
    "text": "can uh uh report this information to the string system and then split even there",
    "start": "997920",
    "end": "1005079"
  },
  {
    "text": "is the partion is already small like 100 rows we can split it in 10 each one has",
    "start": "1005079",
    "end": "1011600"
  },
  {
    "text": "10 rows like that and then distributed to multiple T nodes",
    "start": "1011600",
    "end": "1019000"
  },
  {
    "text": "yeah as a larg system there are hundreds nodes there are you know uh Millions QPS",
    "start": "1019680",
    "end": "1025959"
  },
  {
    "text": "for the whole system so we don't want to you know uh if there any issue we don't",
    "start": "1025959",
    "end": "1031038"
  },
  {
    "text": "want to we we want to know what is the root cause yes which carry cause the",
    "start": "1031039",
    "end": "1038438"
  },
  {
    "text": "issue or which node is current bottl neck so the observability is very",
    "start": "1038439",
    "end": "1045038"
  },
  {
    "text": "important for large system and we did a lot of effort on",
    "start": "1045039",
    "end": "1051840"
  },
  {
    "text": "this and uh you can monitor the uh clust level informations like how many nodes",
    "start": "1051840",
    "end": "1058240"
  },
  {
    "text": "you have and um how many data how much data of each node has and the resource",
    "start": "1058240",
    "end": "1063600"
  },
  {
    "text": "usage the latency distribution and the uh QPS and even uh we have the proportion",
    "start": "1063600",
    "end": "1072400"
  },
  {
    "text": "of the different statements meanwhile um if there is a",
    "start": "1072400",
    "end": "1079600"
  },
  {
    "text": "hot issue Hot Spot issue I just mentioned you can you can you can find out from here the CPU of of a single",
    "start": "1079600",
    "end": "1088280"
  },
  {
    "text": "node or the io of a single node is very high but how to locate how to identify",
    "start": "1088280",
    "end": "1094400"
  },
  {
    "text": "which SQL or which uh table has this issue yeah this is the next step so we",
    "start": "1094400",
    "end": "1101799"
  },
  {
    "text": "introduce the key which like there which can uh show you know show the throughput",
    "start": "1101799",
    "end": "1107320"
  },
  {
    "text": "of a different data range and then uh from this picture you can see there is a bright line which means",
    "start": "1107320",
    "end": "1115360"
  },
  {
    "text": "this uh this state range has high throughput and you can uh tell which",
    "start": "1115360",
    "end": "1123400"
  },
  {
    "text": "which like database which table and which index cause this issue yeah uh",
    "start": "1123400",
    "end": "1130360"
  },
  {
    "text": "this table this is a bad example uh just I I have mentioned this right Hotpot",
    "start": "1130360",
    "end": "1136960"
  },
  {
    "text": "issue we count you know handle it automatically because it's a bad design",
    "start": "1136960",
    "end": "1142360"
  },
  {
    "text": "for a in terms of you know uh distributed system all the you know you",
    "start": "1142360",
    "end": "1148679"
  },
  {
    "text": "imagine all the right of this primary key index will happen in one",
    "start": "1148679",
    "end": "1156039"
  },
  {
    "text": "place so from the SES perspective we can you know uh monitor the the slope ques",
    "start": "1158400",
    "end": "1166840"
  },
  {
    "text": "in all system we name it the top CLE which means uh this CLE they consume a",
    "start": "1166840",
    "end": "1173000"
  },
  {
    "text": "lot of uh resources and uh we also have the SQL",
    "start": "1173000",
    "end": "1179400"
  },
  {
    "text": "statement which um which can show you the dominating the sqle C pattern from",
    "start": "1179400",
    "end": "1185080"
  },
  {
    "text": "your application yeah this is a um you know we sta we statistics um from the",
    "start": "1185080",
    "end": "1193240"
  },
  {
    "text": "you know frequency perspective so after you have located",
    "start": "1193240",
    "end": "1199840"
  },
  {
    "text": "for example you have located the um the the the top CLE and then you can click",
    "start": "1199840",
    "end": "1205720"
  },
  {
    "text": "here and then we can show you know the execution plan of the of the CLE and uh",
    "start": "1205720",
    "end": "1212600"
  },
  {
    "text": "you know um what the qu plan and uh each each step how many you know roads it has",
    "start": "1212600",
    "end": "1220799"
  },
  {
    "text": "scanned and uh how many resource it had taken and how how much times it had has",
    "start": "1220799",
    "end": "1226440"
  },
  {
    "text": "has taken and the next challenge uh is option",
    "start": "1226440",
    "end": "1232120"
  },
  {
    "text": "operational challenge for a large uh database especially SQ database so you",
    "start": "1232120",
    "end": "1238240"
  },
  {
    "text": "can match I want to load a large amount of data from S3 into this system and so",
    "start": "1238240",
    "end": "1245000"
  },
  {
    "text": "my application I want to you know back up the whole system to make sure uh even",
    "start": "1245000",
    "end": "1251039"
  },
  {
    "text": "we have multiple repli Cuts uh in in tidb but I also want to most of the",
    "start": "1251039",
    "end": "1256720"
  },
  {
    "text": "customers want to back up their their um database periodically and what if we add",
    "start": "1256720",
    "end": "1263400"
  },
  {
    "text": "index for a large table yeah this all this all are you know operational challenges for us and for the load data",
    "start": "1263400",
    "end": "1271400"
  },
  {
    "text": "we uh introduced the the uh distributed execution framework which can you know",
    "start": "1271400",
    "end": "1278200"
  },
  {
    "text": "um translate the pqu file cqq file CSV file into the TA data format asess table",
    "start": "1278200",
    "end": "1285720"
  },
  {
    "text": "format and then injust to over sarch",
    "start": "1285720",
    "end": "1290520"
  },
  {
    "text": "layer and you uh you can label you know the compute node so this heavy job can",
    "start": "1291320",
    "end": "1298679"
  },
  {
    "text": "just happen on this note you have label let's uh see a result test the",
    "start": "1298679",
    "end": "1306880"
  },
  {
    "text": "result this is a uh table from our a",
    "start": "1306880",
    "end": "1312000"
  },
  {
    "text": "real customer has 55 columns with five indices and there are around 100 terab",
    "start": "1312000",
    "end": "1320279"
  },
  {
    "text": "data from the S3 and uh they use 12 tidb nodes each",
    "start": "1320279",
    "end": "1326080"
  },
  {
    "text": "node has 16 cor uh 16 we CPU and it's took uh took around 31 hours it's not",
    "start": "1326080",
    "end": "1334200"
  },
  {
    "text": "bad uh but I want to highlight is if you have more tidb",
    "start": "1334200",
    "end": "1339240"
  },
  {
    "text": "nodes the total time can be uh linearly",
    "start": "1339240",
    "end": "1344960"
  },
  {
    "text": "reduced and the next is adding index is similar as the import uh we reuse the",
    "start": "1345039",
    "end": "1352440"
  },
  {
    "text": "distributed execution framework and uh the difference between import and add index is the data is",
    "start": "1352440",
    "end": "1359520"
  },
  {
    "text": "already in t in in in the system in way so we can uh just scan the data and uh",
    "start": "1359520",
    "end": "1367120"
  },
  {
    "text": "uh generate the final s files and in just to to T meanwhile",
    "start": "1367120",
    "end": "1373559"
  },
  {
    "text": "there might some new insert for this table so we need to uh do the increment",
    "start": "1373559",
    "end": "1379080"
  },
  {
    "text": "kway I mean the the the um index kway and the right into our",
    "start": "1379080",
    "end": "1385080"
  },
  {
    "text": "system and this is a test result from um from from a table with 40 terab data and",
    "start": "1385080",
    "end": "1393360"
  },
  {
    "text": "we create the index with 14 tidb",
    "start": "1393360",
    "end": "1398720"
  },
  {
    "text": "nodes and from the first line you know it take around 1 hour to finish the job",
    "start": "1398720",
    "end": "1407799"
  },
  {
    "text": "and you can see the last line This is the you know uh single node",
    "start": "1407799",
    "end": "1412840"
  },
  {
    "text": "mode it means the add index is just around in one TB noes it takes around 14",
    "start": "1412840",
    "end": "1419400"
  },
  {
    "text": "hours yeah it's linear scalable and the second row is about you",
    "start": "1419400",
    "end": "1426360"
  },
  {
    "text": "know use one single statement to add five index at the same time it took around 1",
    "start": "1426360",
    "end": "1433279"
  },
  {
    "text": "hour and a half it's not 4 hour because all you know index is from the same",
    "start": "1433279",
    "end": "1438720"
  },
  {
    "text": "table so the scan just happen once and backup is also a Challenger um",
    "start": "1438720",
    "end": "1447320"
  },
  {
    "text": "for large database and uh we want to make it scalable so uh from the storage",
    "start": "1447320",
    "end": "1455520"
  },
  {
    "text": "layer perspective each takeway will you know um for the full snapshot back each",
    "start": "1455520",
    "end": "1463000"
  },
  {
    "text": "search node will up upload the snapshot asset files to uh three small",
    "start": "1463000",
    "end": "1470559"
  },
  {
    "text": "teny and we also introduced the rate limit on our taqu internal and we can",
    "start": "1470559",
    "end": "1477279"
  },
  {
    "text": "you know limit the the resource and the support of each taqu used by this backup",
    "start": "1477279",
    "end": "1483600"
  },
  {
    "text": "job so we can make sure the latency of the online traffic is",
    "start": "1483600",
    "end": "1488880"
  },
  {
    "text": "acceptable and meanwhile we also have the incremental backup if you already had uh take a snapshot for backup and",
    "start": "1488880",
    "end": "1496039"
  },
  {
    "text": "then uh each taen node will uh incrementally um upload the you know uh",
    "start": "1496039",
    "end": "1502360"
  },
  {
    "text": "incremental change log to to object object",
    "start": "1502360",
    "end": "1508080"
  },
  {
    "text": "storage so let's uh see a test result we tested the uh full backup for large",
    "start": "1508080",
    "end": "1515039"
  },
  {
    "text": "database um the first is has 100 terab data and the second has around 300 tab",
    "start": "1515039",
    "end": "1522240"
  },
  {
    "text": "data and they have different you know numbers of search node and uh each note",
    "start": "1522240",
    "end": "1529799"
  },
  {
    "text": "has uh you know for the first cluster each Storage Note has 2.6 teres data and",
    "start": "1529799",
    "end": "1537559"
  },
  {
    "text": "the total time of the full backup takes 45 minutes and the second test take",
    "start": "1537559",
    "end": "1543279"
  },
  {
    "text": "around 1 hour and four minutes and uh it's not three times of",
    "start": "1543279",
    "end": "1549360"
  },
  {
    "text": "the 100 terab test because um we have more sort noes and uh actually the total",
    "start": "1549360",
    "end": "1558679"
  },
  {
    "text": "you know time costed by backup is uh is a",
    "start": "1558679",
    "end": "1563919"
  },
  {
    "text": "um uh has a relationship with how much data each storage node has because from",
    "start": "1563919",
    "end": "1570720"
  },
  {
    "text": "uh the mechanism you can see other you know storage node can do the work",
    "start": "1570720",
    "end": "1575919"
  },
  {
    "text": "simultaneously at the same time there uh recent you know uh",
    "start": "1575919",
    "end": "1582760"
  },
  {
    "text": "challenge we are facing is a noise neighbor challenge because as a you know",
    "start": "1582760",
    "end": "1587840"
  },
  {
    "text": "scale able C database our customers they consolidate multiple myo instance into",
    "start": "1587840",
    "end": "1594480"
  },
  {
    "text": "the single one TB clusters and there there might be um multiple applications",
    "start": "1594480",
    "end": "1601880"
  },
  {
    "text": "running in the same systems so if there is one application that uh has large",
    "start": "1601880",
    "end": "1608960"
  },
  {
    "text": "ques or heavy ques and this may impact or affect other you know",
    "start": "1608960",
    "end": "1616000"
  },
  {
    "text": "applications and also there are the the other you know um widely used the",
    "start": "1616000",
    "end": "1621399"
  },
  {
    "text": "scenario is SAS company we have multiple SAS company customers they have multiple",
    "start": "1621399",
    "end": "1628120"
  },
  {
    "text": "tenant and uh each tenant may have different workload so how to you know",
    "start": "1628120",
    "end": "1633480"
  },
  {
    "text": "make sure uh there this you know noisy tenant if if there is cannot you know impact",
    "start": "1633480",
    "end": "1641720"
  },
  {
    "text": "other tenant so we introduced the resource control feature um",
    "start": "1641720",
    "end": "1649399"
  },
  {
    "text": "which is a feature can control the user from a specific",
    "start": "1649399",
    "end": "1657880"
  },
  {
    "text": "uh uh tenant or application and basically the idea is we",
    "start": "1657880",
    "end": "1665480"
  },
  {
    "text": "have different strategy from the compu layer and the source layer and uh we",
    "start": "1665480",
    "end": "1670760"
  },
  {
    "text": "introduced the resource Group uh concept you can bind the multiple applications",
    "start": "1670760",
    "end": "1677799"
  },
  {
    "text": "into one Resource Group it means this multiple application share the resource",
    "start": "1677799",
    "end": "1683480"
  },
  {
    "text": "in this Resource Group and you we also you know you also can allocate a",
    "start": "1683480",
    "end": "1691159"
  },
  {
    "text": "dedicated Resource Group to a significant uh like Ka customer or Ka",
    "start": "1691159",
    "end": "1699120"
  },
  {
    "text": "tenant and from the SL layer we use the priority based scheduling which uh we",
    "start": "1699120",
    "end": "1705360"
  },
  {
    "text": "use mclock algorithm to to uh satisfy the resource you know allocation and",
    "start": "1705360",
    "end": "1712960"
  },
  {
    "text": "from the C layer we use like CLE flow control for different uh users from",
    "start": "1712960",
    "end": "1718600"
  },
  {
    "text": "different users uh in different Resource Group let's see the test result of a um",
    "start": "1718600",
    "end": "1727360"
  },
  {
    "text": "of a use case the left part is we don't introduce you know we don't uh apply the",
    "start": "1727360",
    "end": "1733320"
  },
  {
    "text": "resource control feature and uh the blue one is uh we similar the order the order",
    "start": "1733320",
    "end": "1740880"
  },
  {
    "text": "applications and the yellow one is the you know report application which has a",
    "start": "1740880",
    "end": "1747320"
  },
  {
    "text": "you know huge impact for the online traffic you can you can think and the",
    "start": "1747320",
    "end": "1752360"
  },
  {
    "text": "the right side of the picture is about you know we apply the resource control",
    "start": "1752360",
    "end": "1760080"
  },
  {
    "text": "into the system and the other you know application is um has no impact and the",
    "start": "1760080",
    "end": "1769200"
  },
  {
    "text": "latency is the tail latency is um keep in 15 minute uh 15",
    "start": "1769200",
    "end": "1775600"
  },
  {
    "text": "millisecond yeah okay this is all I want to share",
    "start": "1775600",
    "end": "1781720"
  },
  {
    "text": "with you uh today and uh yeah I believe the time has passed 30 minutes thank you",
    "start": "1781720",
    "end": "1787480"
  },
  {
    "text": "for your time and uh if you want discuss you know or discover how tidb can solve",
    "start": "1787480",
    "end": "1793559"
  },
  {
    "text": "your problems you can scan this QR code and this afternoon my colle Matthew will uh from the applications",
    "start": "1793559",
    "end": "1801399"
  },
  {
    "text": "perspective how you know uh how to scale your micros service with our you know",
    "start": "1801399",
    "end": "1807000"
  },
  {
    "text": "scaleable stateful backand you know database thank you any",
    "start": "1807000",
    "end": "1814140"
  },
  {
    "text": "[Applause]",
    "start": "1814140",
    "end": "1817259"
  },
  {
    "text": "questions hey thanks for the talk uh I had a question on um uh I mean does tidd",
    "start": "1821960",
    "end": "1827760"
  },
  {
    "text": "be also support replication if you have a multi- region deployment do you replicate automatically across regions",
    "start": "1827760",
    "end": "1833760"
  },
  {
    "text": "as well uh your question is about multiple region",
    "start": "1833760",
    "end": "1840640"
  },
  {
    "text": "deployment right yeah okay um from our you know",
    "start": "1840640",
    "end": "1846600"
  },
  {
    "text": "practice um we have multiple TB cluster we use",
    "start": "1846600",
    "end": "1851799"
  },
  {
    "text": "CDC to replica the um Chang across region yeah got it and the",
    "start": "1851799",
    "end": "1858720"
  },
  {
    "text": "CDC has to be set up uh by the client or is it done by tidb automatically uh CDC",
    "start": "1858720",
    "end": "1864919"
  },
  {
    "text": "is a component of tidb okay sounds good and uh one more question I had was um",
    "start": "1864919",
    "end": "1870960"
  },
  {
    "text": "like the what about strong consistency versus weak consistency when you write uh does the data I mean the data read",
    "start": "1870960",
    "end": "1879320"
  },
  {
    "text": "does it follow strong consistency checks or Sor sorry I mean how does consistency work with tidb uh like um is the",
    "start": "1879320",
    "end": "1888440"
  },
  {
    "text": "wres I mean are the reads strongly consistent no no read and right yeah I",
    "start": "1888440",
    "end": "1894600"
  },
  {
    "text": "know but uh yeah read and write strong consistency okay yeah got it cool thanks that's all I",
    "start": "1894600",
    "end": "1902840"
  },
  {
    "text": "had hey uh first off thank you for the talk and huge respect for what you all have built here I work in the space as",
    "start": "1903639",
    "end": "1909760"
  },
  {
    "text": "well and this is you know exciting but also a very technically challenging problem uh so the question that I have is about the PD part of that",
    "start": "1909760",
    "end": "1916559"
  },
  {
    "text": "architecture diagram um is it is it just like one uh yeah thanks for going to that is it just like one you I know you",
    "start": "1916559",
    "end": "1923440"
  },
  {
    "text": "showed like one of them is a leader and the others are followers is that like one raft group uh or is it multiple is",
    "start": "1923440",
    "end": "1929320"
  },
  {
    "text": "it possible for there to be multiple leaders uh right now it's a single rafter uh group and we are working on",
    "start": "1929320",
    "end": "1936480"
  },
  {
    "text": "you know to upload the data into to TA part which can you know reuse a multiple",
    "start": "1936480",
    "end": "1943320"
  },
  {
    "text": "ra architecture okay that makes sense this can further you know scale over metadata",
    "start": "1943320",
    "end": "1949880"
  },
  {
    "text": "Center cool thank you thank you for your",
    "start": "1949880",
    "end": "1955000"
  },
  {
    "text": "question hi uh thanks for the talk um you mentioned backup and backup time and",
    "start": "1957000",
    "end": "1963799"
  },
  {
    "text": "I was curious do you have any numbers or ballpark on uh What uh restore times and",
    "start": "1963799",
    "end": "1971000"
  },
  {
    "text": "what the downtime for restores would be in similar scales you you mean the RTO",
    "start": "1971000",
    "end": "1976960"
  },
  {
    "text": "of restore what do you mean you you mean the restore yeah so when you have the",
    "start": "1976960",
    "end": "1983919"
  },
  {
    "text": "backups and then something happens with the class let's say and you need to restore it from the backup yeah it's a",
    "start": "1983919",
    "end": "1991519"
  },
  {
    "text": "similar uh because you know it's just the reverse reverse um proc procedure of",
    "start": "1991519",
    "end": "1998519"
  },
  {
    "text": "the backup yeah um but the the specific you know number is not the same but they",
    "start": "1998519",
    "end": "2005000"
  },
  {
    "text": "are the same level okay and it also depends on um how much you know um",
    "start": "2005000",
    "end": "2012679"
  },
  {
    "text": "incremental change log you have yes it it it's differs but they still at this",
    "start": "2012679",
    "end": "2019720"
  },
  {
    "text": "you know the same level right so you have incremental resources as of you",
    "start": "2019720",
    "end": "2026200"
  },
  {
    "text": "roll back or app um if you if you open your full backup it it happen you can",
    "start": "2026200",
    "end": "2032120"
  },
  {
    "text": "daily or weekly but also you need to open the pit the uh incremental",
    "start": "2032120",
    "end": "2038240"
  },
  {
    "text": "change log you know yeah uh time is over I can I can chat with you offline",
    "start": "2038240",
    "end": "2043440"
  },
  {
    "text": "definitely thank you",
    "start": "2043440",
    "end": "2046600"
  }
]