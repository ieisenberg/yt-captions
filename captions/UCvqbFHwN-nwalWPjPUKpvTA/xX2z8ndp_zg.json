[
  {
    "text": "hi there all welcome and thank you so",
    "start": "0",
    "end": "1760"
  },
  {
    "text": "much for joining us we're going to be",
    "start": "1760",
    "end": "3199"
  },
  {
    "text": "talking about scaling apache spark on",
    "start": "3199",
    "end": "5200"
  },
  {
    "text": "kubernetes",
    "start": "5200",
    "end": "6480"
  },
  {
    "text": "we're amanda and holden and we're so",
    "start": "6480",
    "end": "8160"
  },
  {
    "text": "delighted to be here at kubecon",
    "start": "8160",
    "end": "10080"
  },
  {
    "text": "eu so first and foremost we're going to",
    "start": "10080",
    "end": "13519"
  },
  {
    "text": "tell you a little bit about ourselves",
    "start": "13519",
    "end": "15440"
  },
  {
    "text": "then we're going to talk about what is",
    "start": "15440",
    "end": "16640"
  },
  {
    "text": "apache spark and why it matters to",
    "start": "16640",
    "end": "18720"
  },
  {
    "text": "kubernetes",
    "start": "18720",
    "end": "20080"
  },
  {
    "text": "what is yarn and mesos spark",
    "start": "20080",
    "end": "23199"
  },
  {
    "text": "standalone mode so get ready for that",
    "start": "23199",
    "end": "25039"
  },
  {
    "text": "one why you should put spark on",
    "start": "25039",
    "end": "27119"
  },
  {
    "text": "kubernetes",
    "start": "27119",
    "end": "28400"
  },
  {
    "text": "what worked well while we worked through",
    "start": "28400",
    "end": "29760"
  },
  {
    "text": "this process and what didn't work as",
    "start": "29760",
    "end": "31599"
  },
  {
    "text": "good",
    "start": "31599",
    "end": "32880"
  },
  {
    "text": "best practices for doing this kind of",
    "start": "32880",
    "end": "34559"
  },
  {
    "text": "transformation",
    "start": "34559",
    "end": "36640"
  },
  {
    "text": "new features that have been implemented",
    "start": "36640",
    "end": "38640"
  },
  {
    "text": "and upstreamed",
    "start": "38640",
    "end": "39840"
  },
  {
    "text": "and areas for improvement",
    "start": "39840",
    "end": "44800"
  },
  {
    "text": "so who are we so you'll see two",
    "start": "44800",
    "end": "48480"
  },
  {
    "text": "two of us uh in this picture and this",
    "start": "48480",
    "end": "51520"
  },
  {
    "text": "collection of pictures work for apple",
    "start": "51520",
    "end": "53120"
  },
  {
    "text": "and that is me and holden",
    "start": "53120",
    "end": "55039"
  },
  {
    "text": "and you also see our two pups timber and",
    "start": "55039",
    "end": "57520"
  },
  {
    "text": "jack",
    "start": "57520",
    "end": "58399"
  },
  {
    "text": "they support us throughout the day as we",
    "start": "58399",
    "end": "59920"
  },
  {
    "text": "get our work done both of us have been",
    "start": "59920",
    "end": "62399"
  },
  {
    "text": "and have worked for apple uh for over a",
    "start": "62399",
    "end": "65040"
  },
  {
    "text": "year i think holden is",
    "start": "65040",
    "end": "66159"
  },
  {
    "text": "it's getting close to two years and",
    "start": "66159",
    "end": "69280"
  },
  {
    "text": "um both of us have been um",
    "start": "69280",
    "end": "72799"
  },
  {
    "text": "a part of the apache spark community",
    "start": "72799",
    "end": "74560"
  },
  {
    "text": "holden much more so she's an apache",
    "start": "74560",
    "end": "76320"
  },
  {
    "text": "committer she's written books on apache",
    "start": "76320",
    "end": "78159"
  },
  {
    "text": "spark",
    "start": "78159",
    "end": "78880"
  },
  {
    "text": "she's been with it pretty much from the",
    "start": "78880",
    "end": "80400"
  },
  {
    "text": "get-go i came to spark a little bit",
    "start": "80400",
    "end": "82320"
  },
  {
    "text": "later but i've been involved with it for",
    "start": "82320",
    "end": "83680"
  },
  {
    "text": "quite a few years i've done quite a few",
    "start": "83680",
    "end": "85200"
  },
  {
    "text": "talks on spark and",
    "start": "85200",
    "end": "86560"
  },
  {
    "text": "and it's a it's a technology that um i",
    "start": "86560",
    "end": "89520"
  },
  {
    "text": "enjoy",
    "start": "89520",
    "end": "90240"
  },
  {
    "text": "i like teaching other people about and",
    "start": "90240",
    "end": "91840"
  },
  {
    "text": "using myself",
    "start": "91840",
    "end": "94560"
  },
  {
    "text": "so all right so let's talk about spark",
    "start": "95040",
    "end": "97360"
  },
  {
    "text": "and what are these things",
    "start": "97360",
    "end": "99600"
  },
  {
    "text": "so what is spark so a lightning fast",
    "start": "99600",
    "end": "103439"
  },
  {
    "text": "unified analytics engine is one of the",
    "start": "103439",
    "end": "105360"
  },
  {
    "text": "taglines you'll hear",
    "start": "105360",
    "end": "106640"
  },
  {
    "text": "quite frequently but apache spark is",
    "start": "106640",
    "end": "108880"
  },
  {
    "text": "really used to do large-scale data",
    "start": "108880",
    "end": "110560"
  },
  {
    "text": "processing on large data sets",
    "start": "110560",
    "end": "112720"
  },
  {
    "text": "it's used by data scientists data",
    "start": "112720",
    "end": "114399"
  },
  {
    "text": "engineers machine learning engineers",
    "start": "114399",
    "end": "116560"
  },
  {
    "text": "basically anyone who is working with",
    "start": "116560",
    "end": "118320"
  },
  {
    "text": "large data sets",
    "start": "118320",
    "end": "120159"
  },
  {
    "text": "basically regardless of title so apache",
    "start": "120159",
    "end": "123520"
  },
  {
    "text": "spark allows for batch jobs",
    "start": "123520",
    "end": "125520"
  },
  {
    "text": "streaming jobs the ability to use spark",
    "start": "125520",
    "end": "127920"
  },
  {
    "text": "sql",
    "start": "127920",
    "end": "128879"
  },
  {
    "text": "r python scala or java",
    "start": "128879",
    "end": "132319"
  },
  {
    "text": "you can do machine learning using spark",
    "start": "132319",
    "end": "134319"
  },
  {
    "text": "and you can also do graph analytics with",
    "start": "134319",
    "end": "135840"
  },
  {
    "text": "spark as well",
    "start": "135840",
    "end": "137840"
  },
  {
    "text": "so some other taglines you may have",
    "start": "137840",
    "end": "140080"
  },
  {
    "text": "heard about spark which is it's map",
    "start": "140080",
    "end": "142000"
  },
  {
    "text": "hadoop mapreduce uh but with seven cups",
    "start": "142000",
    "end": "144160"
  },
  {
    "text": "of coffee",
    "start": "144160",
    "end": "145280"
  },
  {
    "text": "um of course we have a little disclaimer",
    "start": "145280",
    "end": "147599"
  },
  {
    "text": "there you know check with your hardware",
    "start": "147599",
    "end": "148959"
  },
  {
    "text": "vendor first",
    "start": "148959",
    "end": "149920"
  },
  {
    "text": "but i mean there has been i mean there's",
    "start": "149920",
    "end": "151440"
  },
  {
    "text": "been so many numbers thrown around the",
    "start": "151440",
    "end": "153200"
  },
  {
    "text": "years",
    "start": "153200",
    "end": "153920"
  },
  {
    "text": "uh but essentially you know spark jobs",
    "start": "153920",
    "end": "156879"
  },
  {
    "text": "because of their ability to utilize",
    "start": "156879",
    "end": "158800"
  },
  {
    "text": "uh memory and large uh and do that large",
    "start": "158800",
    "end": "162000"
  },
  {
    "text": "processing",
    "start": "162000",
    "end": "162800"
  },
  {
    "text": "um anywhere between uh 10 to 100 times",
    "start": "162800",
    "end": "165360"
  },
  {
    "text": "faster than hadoop",
    "start": "165360",
    "end": "166800"
  },
  {
    "text": "regardless of that's a big range but",
    "start": "166800",
    "end": "168720"
  },
  {
    "text": "it's faster",
    "start": "168720",
    "end": "170560"
  },
  {
    "text": "it is a spark is a good way for folks to",
    "start": "170560",
    "end": "172720"
  },
  {
    "text": "learn functional programming which is",
    "start": "172720",
    "end": "174239"
  },
  {
    "text": "true",
    "start": "174239",
    "end": "174800"
  },
  {
    "text": "since scala is a functional functional",
    "start": "174800",
    "end": "176800"
  },
  {
    "text": "language me myself",
    "start": "176800",
    "end": "178159"
  },
  {
    "text": "i know this much about scala and i",
    "start": "178159",
    "end": "180879"
  },
  {
    "text": "prefer to use python",
    "start": "180879",
    "end": "182239"
  },
  {
    "text": "that's my uh language of choice when",
    "start": "182239",
    "end": "184640"
  },
  {
    "text": "using spark so i did not take this",
    "start": "184640",
    "end": "186560"
  },
  {
    "text": "opportunity to learn functional",
    "start": "186560",
    "end": "187760"
  },
  {
    "text": "programming",
    "start": "187760",
    "end": "189040"
  },
  {
    "text": "and it's a great it's a great way to use",
    "start": "189040",
    "end": "191120"
  },
  {
    "text": "a lot of compute resources",
    "start": "191120",
    "end": "192560"
  },
  {
    "text": "right because you're doing extremely",
    "start": "192560",
    "end": "193840"
  },
  {
    "text": "large jobs across",
    "start": "193840",
    "end": "195760"
  },
  {
    "text": "super large clusters using a ton of",
    "start": "195760",
    "end": "198080"
  },
  {
    "text": "memory",
    "start": "198080",
    "end": "198959"
  },
  {
    "text": "and cpu so let's talk about how spark",
    "start": "198959",
    "end": "203040"
  },
  {
    "text": "works",
    "start": "203040",
    "end": "204720"
  },
  {
    "text": "so um basically what spark does the tl",
    "start": "204720",
    "end": "208080"
  },
  {
    "text": "dr",
    "start": "208080",
    "end": "208560"
  },
  {
    "text": "is it spreads out compute across a",
    "start": "208560",
    "end": "210560"
  },
  {
    "text": "cluster most specifically",
    "start": "210560",
    "end": "212000"
  },
  {
    "text": "a spark cluster right there's two main",
    "start": "212000",
    "end": "214159"
  },
  {
    "text": "components when you launch a spark job",
    "start": "214159",
    "end": "216720"
  },
  {
    "text": "the driver which contains the spark",
    "start": "216720",
    "end": "218480"
  },
  {
    "text": "contacts and which works to transform",
    "start": "218480",
    "end": "220959"
  },
  {
    "text": "the user's code that i've written maybe",
    "start": "220959",
    "end": "222879"
  },
  {
    "text": "in python",
    "start": "222879",
    "end": "223599"
  },
  {
    "text": "right splits it up into tasks these",
    "start": "223599",
    "end": "226000"
  },
  {
    "text": "bite-sized chunks that can be sent to be",
    "start": "226000",
    "end": "228159"
  },
  {
    "text": "performed by the executors and the",
    "start": "228159",
    "end": "230159"
  },
  {
    "text": "executors reside",
    "start": "230159",
    "end": "231440"
  },
  {
    "text": "on each node so from there it's very",
    "start": "231440",
    "end": "234560"
  },
  {
    "text": "easy to scale up and scale down",
    "start": "234560",
    "end": "236640"
  },
  {
    "text": "depending on the workload because you",
    "start": "236640",
    "end": "237920"
  },
  {
    "text": "can always add more nodes and more",
    "start": "237920",
    "end": "240159"
  },
  {
    "text": "executors especially when you're working",
    "start": "240159",
    "end": "241680"
  },
  {
    "text": "in a cloud native environment in the",
    "start": "241680",
    "end": "243200"
  },
  {
    "text": "cloud",
    "start": "243200",
    "end": "243760"
  },
  {
    "text": "it's really easy to add those extra",
    "start": "243760",
    "end": "245360"
  },
  {
    "text": "resources",
    "start": "245360",
    "end": "247920"
  },
  {
    "text": "so apache spark has abstracted away from",
    "start": "247920",
    "end": "250480"
  },
  {
    "text": "the users",
    "start": "250480",
    "end": "251760"
  },
  {
    "text": "any need to have to deal with",
    "start": "251760",
    "end": "253599"
  },
  {
    "text": "orchestrating data processing",
    "start": "253599",
    "end": "255120"
  },
  {
    "text": "parallelism",
    "start": "255120",
    "end": "256400"
  },
  {
    "text": "or worrying about fault tolerance this",
    "start": "256400",
    "end": "258400"
  },
  {
    "text": "is all taking care of them",
    "start": "258400",
    "end": "260079"
  },
  {
    "text": "for them um so because",
    "start": "260079",
    "end": "263520"
  },
  {
    "text": "of the architecture and knowing that",
    "start": "263520",
    "end": "266880"
  },
  {
    "text": "spark would be spread across multiple",
    "start": "266880",
    "end": "268639"
  },
  {
    "text": "nodes on very large clusters",
    "start": "268639",
    "end": "270479"
  },
  {
    "text": "it was known that nodes would fail",
    "start": "270479",
    "end": "272000"
  },
  {
    "text": "throughout this process and so the",
    "start": "272000",
    "end": "273680"
  },
  {
    "text": "executors",
    "start": "273680",
    "end": "275040"
  },
  {
    "text": "are able to handle those failures",
    "start": "275040",
    "end": "276800"
  },
  {
    "text": "because they know that they will happen",
    "start": "276800",
    "end": "278320"
  },
  {
    "text": "and they can just recompute results so",
    "start": "278320",
    "end": "281360"
  },
  {
    "text": "um",
    "start": "281360",
    "end": "282000"
  },
  {
    "text": "the executors are long-lived especially",
    "start": "282000",
    "end": "284080"
  },
  {
    "text": "when compared to mapreduce",
    "start": "284080",
    "end": "285919"
  },
  {
    "text": "and the executors store a mix of stored",
    "start": "285919",
    "end": "288320"
  },
  {
    "text": "data in a mixture of memory and disk",
    "start": "288320",
    "end": "290400"
  },
  {
    "text": "so it utilizes uh both of those to run",
    "start": "290400",
    "end": "294800"
  },
  {
    "text": "faster",
    "start": "294840",
    "end": "297040"
  },
  {
    "text": "with all this said right oh so now we",
    "start": "297040",
    "end": "298880"
  },
  {
    "text": "know what spark is right",
    "start": "298880",
    "end": "300639"
  },
  {
    "text": "um so spark can be run either in a",
    "start": "300639",
    "end": "303440"
  },
  {
    "text": "standalone mode with just spark",
    "start": "303440",
    "end": "305199"
  },
  {
    "text": "and a jvm on each machine or you can use",
    "start": "305199",
    "end": "308240"
  },
  {
    "text": "a custom resource",
    "start": "308240",
    "end": "309520"
  },
  {
    "text": "management system and that's what we're",
    "start": "309520",
    "end": "311039"
  },
  {
    "text": "going to talk about next",
    "start": "311039",
    "end": "313039"
  },
  {
    "text": "so what is yarn yarn is yet another",
    "start": "313039",
    "end": "315440"
  },
  {
    "text": "resource negotiator",
    "start": "315440",
    "end": "317440"
  },
  {
    "text": "so yarn was released in 2012 and was a",
    "start": "317440",
    "end": "320560"
  },
  {
    "text": "rewrite",
    "start": "320560",
    "end": "321199"
  },
  {
    "text": "of mapreduce the mapreduce engine from",
    "start": "321199",
    "end": "323440"
  },
  {
    "text": "hadoop 1.0",
    "start": "323440",
    "end": "325039"
  },
  {
    "text": "so mr2 which was the informal nickname",
    "start": "325039",
    "end": "328479"
  },
  {
    "text": "uh even though it really means mapper it",
    "start": "328479",
    "end": "330160"
  },
  {
    "text": "is 2.0 which is an application that is",
    "start": "330160",
    "end": "332080"
  },
  {
    "text": "actually managed by yarn so it all gets",
    "start": "332080",
    "end": "333840"
  },
  {
    "text": "a little bit confusing",
    "start": "333840",
    "end": "335440"
  },
  {
    "text": "but it is the de facto standard for big",
    "start": "335440",
    "end": "338479"
  },
  {
    "text": "data workloads",
    "start": "338479",
    "end": "340479"
  },
  {
    "text": "so yarn supports a variety of process",
    "start": "340479",
    "end": "342880"
  },
  {
    "text": "engines and applications",
    "start": "342880",
    "end": "344720"
  },
  {
    "text": "um you can run hadoop and spark on the",
    "start": "344720",
    "end": "346560"
  },
  {
    "text": "same cluster",
    "start": "346560",
    "end": "348080"
  },
  {
    "text": "you can do isolation and dynamic",
    "start": "348080",
    "end": "350160"
  },
  {
    "text": "allocation of resources",
    "start": "350160",
    "end": "353440"
  },
  {
    "text": "so yarn actually keeps track of the",
    "start": "353440",
    "end": "356479"
  },
  {
    "text": "available resources so memory cpu",
    "start": "356479",
    "end": "359440"
  },
  {
    "text": "storage",
    "start": "359440",
    "end": "360400"
  },
  {
    "text": "and includes multiple types of",
    "start": "360400",
    "end": "362000"
  },
  {
    "text": "scheduling methods",
    "start": "362000",
    "end": "364720"
  },
  {
    "text": "so it supports uh lightweight isolation",
    "start": "364720",
    "end": "367919"
  },
  {
    "text": "and it allows to share local disk",
    "start": "367919",
    "end": "371600"
  },
  {
    "text": "and um so with all this talk about you",
    "start": "371600",
    "end": "374160"
  },
  {
    "text": "know resource allocation and managing",
    "start": "374160",
    "end": "376400"
  },
  {
    "text": "resources what does this remind you of",
    "start": "376400",
    "end": "378840"
  },
  {
    "text": "kubecon",
    "start": "378840",
    "end": "380160"
  },
  {
    "text": "kind of reminds me of kubernetes doesn't",
    "start": "380160",
    "end": "381759"
  },
  {
    "text": "it so what is mesos",
    "start": "381759",
    "end": "384240"
  },
  {
    "text": "so mesos is a distributed system kernel",
    "start": "384240",
    "end": "387440"
  },
  {
    "text": "so mesos is very similar to kubernetes",
    "start": "387440",
    "end": "390080"
  },
  {
    "text": "but it's a bit more flexible",
    "start": "390080",
    "end": "391919"
  },
  {
    "text": "not only can you manage containers but",
    "start": "391919",
    "end": "393919"
  },
  {
    "text": "you can also manage applications",
    "start": "393919",
    "end": "395680"
  },
  {
    "text": "that are not containerized so it does",
    "start": "395680",
    "end": "398720"
  },
  {
    "text": "try to be more than just analytics",
    "start": "398720",
    "end": "400560"
  },
  {
    "text": "like maybe yarn which is um being able",
    "start": "400560",
    "end": "402880"
  },
  {
    "text": "to manage hadoop and spark",
    "start": "402880",
    "end": "404960"
  },
  {
    "text": "uh there's a private company now that is",
    "start": "404960",
    "end": "406560"
  },
  {
    "text": "dedicated to",
    "start": "406560",
    "end": "408080"
  },
  {
    "text": "working on it and they are a bit more",
    "start": "408080",
    "end": "409759"
  },
  {
    "text": "cube kubernetes focused",
    "start": "409759",
    "end": "411440"
  },
  {
    "text": "than in the past and it also allows for",
    "start": "411440",
    "end": "414319"
  },
  {
    "text": "local shared disk",
    "start": "414319",
    "end": "415520"
  },
  {
    "text": "as well so let's go into standalone mode",
    "start": "415520",
    "end": "419840"
  },
  {
    "text": "so are you tired of doing your actual",
    "start": "419840",
    "end": "421840"
  },
  {
    "text": "job which is writing java or python and",
    "start": "421840",
    "end": "424080"
  },
  {
    "text": "doing it data analytics",
    "start": "424080",
    "end": "426000"
  },
  {
    "text": "and you want to spend more time writing",
    "start": "426000",
    "end": "427440"
  },
  {
    "text": "shell scripts and managing your servers",
    "start": "427440",
    "end": "430080"
  },
  {
    "text": "like they're your kids or your pets",
    "start": "430080",
    "end": "432240"
  },
  {
    "text": "um then this is the mode for you so",
    "start": "432240",
    "end": "437120"
  },
  {
    "text": "standalone spark essentially a way to",
    "start": "437120",
    "end": "438720"
  },
  {
    "text": "create a spark cluster and manage that",
    "start": "438720",
    "end": "440400"
  },
  {
    "text": "yourself",
    "start": "440400",
    "end": "441039"
  },
  {
    "text": "so there's no support for dynamic",
    "start": "441039",
    "end": "443280"
  },
  {
    "text": "resource allocation or resource",
    "start": "443280",
    "end": "444880"
  },
  {
    "text": "management",
    "start": "444880",
    "end": "445599"
  },
  {
    "text": "and scheduling this is the more painful",
    "start": "445599",
    "end": "448160"
  },
  {
    "text": "option for sure",
    "start": "448160",
    "end": "449280"
  },
  {
    "text": "but it is possible maybe you have a use",
    "start": "449280",
    "end": "451199"
  },
  {
    "text": "case but this works better for you i",
    "start": "451199",
    "end": "452560"
  },
  {
    "text": "would love to hear it actually",
    "start": "452560",
    "end": "454720"
  },
  {
    "text": "so it does not support dynamic scaling",
    "start": "454720",
    "end": "457120"
  },
  {
    "text": "so that's what you are for so maybe some",
    "start": "457120",
    "end": "458800"
  },
  {
    "text": "awesome scripts that you wrote",
    "start": "458800",
    "end": "460000"
  },
  {
    "text": "and uh pagerduty as well",
    "start": "460000",
    "end": "463039"
  },
  {
    "text": "so let's get into why we should put",
    "start": "463039",
    "end": "464720"
  },
  {
    "text": "spark on kubernetes so i think we've",
    "start": "464720",
    "end": "466400"
  },
  {
    "text": "made it",
    "start": "466400",
    "end": "467039"
  },
  {
    "text": "pretty clear that you need a dynamic",
    "start": "467039",
    "end": "469280"
  },
  {
    "text": "resource",
    "start": "469280",
    "end": "470240"
  },
  {
    "text": "uh cluster management system to help you",
    "start": "470240",
    "end": "472879"
  },
  {
    "text": "manage your spark workloads",
    "start": "472879",
    "end": "474479"
  },
  {
    "text": "um but what was wrong with yarn and",
    "start": "474479",
    "end": "476639"
  },
  {
    "text": "mesos why i moved to kubernetes",
    "start": "476639",
    "end": "478080"
  },
  {
    "text": "right so there's a layer of reasons",
    "start": "478080",
    "end": "482240"
  },
  {
    "text": "right well first this is kubecon",
    "start": "482240",
    "end": "483919"
  },
  {
    "text": "right so we want to focus on kubernetes",
    "start": "483919",
    "end": "485599"
  },
  {
    "text": "but all jokes aside right",
    "start": "485599",
    "end": "487599"
  },
  {
    "text": "so one true ring to rule them all we'll",
    "start": "487599",
    "end": "489440"
  },
  {
    "text": "talk about that more in a second",
    "start": "489440",
    "end": "492240"
  },
  {
    "text": "you can use your spare capacity for",
    "start": "492240",
    "end": "494800"
  },
  {
    "text": "analytics",
    "start": "494800",
    "end": "495759"
  },
  {
    "text": "and the cloud we're going to talk about",
    "start": "495759",
    "end": "498240"
  },
  {
    "text": "python",
    "start": "498240",
    "end": "499599"
  },
  {
    "text": "and what using kubernetes allows you to",
    "start": "499599",
    "end": "501520"
  },
  {
    "text": "do with python",
    "start": "501520",
    "end": "502720"
  },
  {
    "text": "we'll talk about security and what that",
    "start": "502720",
    "end": "504319"
  },
  {
    "text": "gives you by using kubernetes",
    "start": "504319",
    "end": "506400"
  },
  {
    "text": "and of course it's all about learning",
    "start": "506400",
    "end": "507840"
  },
  {
    "text": "new skills right",
    "start": "507840",
    "end": "510560"
  },
  {
    "text": "so just from our quick review of yarn",
    "start": "510560",
    "end": "513839"
  },
  {
    "text": "and mesos it's easy to see the benefits",
    "start": "513839",
    "end": "516159"
  },
  {
    "text": "of needing a container management",
    "start": "516159",
    "end": "517599"
  },
  {
    "text": "platform",
    "start": "517599",
    "end": "518560"
  },
  {
    "text": "when putting spark on kubernetes you can",
    "start": "518560",
    "end": "520560"
  },
  {
    "text": "add spark and",
    "start": "520560",
    "end": "521760"
  },
  {
    "text": "any other types of workloads so it",
    "start": "521760",
    "end": "524080"
  },
  {
    "text": "doesn't have to just be",
    "start": "524080",
    "end": "525600"
  },
  {
    "text": "spark or hadoop etc so you can better",
    "start": "525600",
    "end": "528560"
  },
  {
    "text": "utilize your cluster's resources because",
    "start": "528560",
    "end": "530480"
  },
  {
    "text": "you can have all types of workloads from",
    "start": "530480",
    "end": "532560"
  },
  {
    "text": "across your organization on one",
    "start": "532560",
    "end": "534720"
  },
  {
    "text": "kubernetes cluster",
    "start": "534720",
    "end": "537839"
  },
  {
    "text": "so and then just to point this out right",
    "start": "538080",
    "end": "540560"
  },
  {
    "text": "so you know",
    "start": "540560",
    "end": "541360"
  },
  {
    "text": "many of us in our you know where we work",
    "start": "541360",
    "end": "543600"
  },
  {
    "text": "um",
    "start": "543600",
    "end": "544560"
  },
  {
    "text": "we may have just a little bit of",
    "start": "544560",
    "end": "545839"
  },
  {
    "text": "everything doing a every type of",
    "start": "545839",
    "end": "547440"
  },
  {
    "text": "technology that there is we have",
    "start": "547440",
    "end": "549200"
  },
  {
    "text": "we have uh spread out across our our",
    "start": "549200",
    "end": "551200"
  },
  {
    "text": "companies right",
    "start": "551200",
    "end": "552399"
  },
  {
    "text": "so you could you know have three cluster",
    "start": "552399",
    "end": "554720"
  },
  {
    "text": "management systems right",
    "start": "554720",
    "end": "556000"
  },
  {
    "text": "mesos yarn standalone kubernetes that's",
    "start": "556000",
    "end": "558480"
  },
  {
    "text": "actually four",
    "start": "558480",
    "end": "559040"
  },
  {
    "text": "right but managing those all you know",
    "start": "559040",
    "end": "562160"
  },
  {
    "text": "that that leads to",
    "start": "562160",
    "end": "563120"
  },
  {
    "text": "a sad team sad engineers um because",
    "start": "563120",
    "end": "565440"
  },
  {
    "text": "that's a lot of",
    "start": "565440",
    "end": "566560"
  },
  {
    "text": "you know overhead for them to have to",
    "start": "566560",
    "end": "568560"
  },
  {
    "text": "try to you know switch between platforms",
    "start": "568560",
    "end": "570399"
  },
  {
    "text": "and troubleshooting et cetera",
    "start": "570399",
    "end": "572080"
  },
  {
    "text": "um so of course the solution would just",
    "start": "572080",
    "end": "574320"
  },
  {
    "text": "be turn or two or three of them off",
    "start": "574320",
    "end": "575680"
  },
  {
    "text": "right",
    "start": "575680",
    "end": "576399"
  },
  {
    "text": "um that'll just make all the workloads",
    "start": "576399",
    "end": "578320"
  },
  {
    "text": "better right well yes and no you want to",
    "start": "578320",
    "end": "580000"
  },
  {
    "text": "definitely try to converge on one",
    "start": "580000",
    "end": "582080"
  },
  {
    "text": "and kubernetes seems like it's it's the",
    "start": "582080",
    "end": "584320"
  },
  {
    "text": "direction that uh",
    "start": "584320",
    "end": "585519"
  },
  {
    "text": "so many of us are taking and so it gives",
    "start": "585519",
    "end": "588800"
  },
  {
    "text": "you",
    "start": "588800",
    "end": "589120"
  },
  {
    "text": "all the benefits of kubernetes which",
    "start": "589120",
    "end": "590480"
  },
  {
    "text": "we'll talk about here in a second and",
    "start": "590480",
    "end": "592320"
  },
  {
    "text": "it just makes your life a little bit",
    "start": "592320",
    "end": "593839"
  },
  {
    "text": "easier to just standardize on one",
    "start": "593839",
    "end": "595279"
  },
  {
    "text": "platform",
    "start": "595279",
    "end": "597600"
  },
  {
    "text": "so it also so like i said before you",
    "start": "597600",
    "end": "599839"
  },
  {
    "text": "know that you can run",
    "start": "599839",
    "end": "600800"
  },
  {
    "text": "different types of workloads within that",
    "start": "600800",
    "end": "602240"
  },
  {
    "text": "one kubernetes cluster so it allows you",
    "start": "602240",
    "end": "604079"
  },
  {
    "text": "to use that spare",
    "start": "604079",
    "end": "605600"
  },
  {
    "text": "capacity so what's nice about kubernetes",
    "start": "605600",
    "end": "608720"
  },
  {
    "text": "again is that multiple types of",
    "start": "608720",
    "end": "610160"
  },
  {
    "text": "applications and services can be all",
    "start": "610160",
    "end": "611920"
  },
  {
    "text": "running on the same cluster",
    "start": "611920",
    "end": "613279"
  },
  {
    "text": "you don't need a dedicated spark cluster",
    "start": "613279",
    "end": "615279"
  },
  {
    "text": "that would be managed by yarn or mesos",
    "start": "615279",
    "end": "617839"
  },
  {
    "text": "to run your spark jobs",
    "start": "617839",
    "end": "619680"
  },
  {
    "text": "um",
    "start": "619680",
    "end": "621920"
  },
  {
    "text": "on that same cluster you can actually",
    "start": "622800",
    "end": "625120"
  },
  {
    "text": "run them on the same",
    "start": "625120",
    "end": "626560"
  },
  {
    "text": "so spark jobs that may be analyzing your",
    "start": "626560",
    "end": "628640"
  },
  {
    "text": "services you can actually",
    "start": "628640",
    "end": "631200"
  },
  {
    "text": "have the spark drops running on the same",
    "start": "631200",
    "end": "632640"
  },
  {
    "text": "cluster as that",
    "start": "632640",
    "end": "634480"
  },
  {
    "text": "um so like i kind of said they're kind",
    "start": "634480",
    "end": "637279"
  },
  {
    "text": "of choppy",
    "start": "637279",
    "end": "638000"
  },
  {
    "text": "the spark jobs you're doing uh can be",
    "start": "638000",
    "end": "640560"
  },
  {
    "text": "analyzed any anything from the various",
    "start": "640560",
    "end": "642160"
  },
  {
    "text": "services that are running on the",
    "start": "642160",
    "end": "643279"
  },
  {
    "text": "platform",
    "start": "643279",
    "end": "644000"
  },
  {
    "text": "for doing any other kind of data",
    "start": "644000",
    "end": "645360"
  },
  {
    "text": "crunching right",
    "start": "645360",
    "end": "647120"
  },
  {
    "text": "and so additionally right when we're",
    "start": "647120",
    "end": "649600"
  },
  {
    "text": "talking about uh preemption and",
    "start": "649600",
    "end": "651519"
  },
  {
    "text": "kubernetes",
    "start": "651519",
    "end": "652720"
  },
  {
    "text": "so kubernetes allows the ability to",
    "start": "652720",
    "end": "654320"
  },
  {
    "text": "preempt workloads uh based on priority",
    "start": "654320",
    "end": "656399"
  },
  {
    "text": "classes which is really powerful",
    "start": "656399",
    "end": "658480"
  },
  {
    "text": "so workloads that are less time",
    "start": "658480",
    "end": "660000"
  },
  {
    "text": "sensitive than other jobs can be",
    "start": "660000",
    "end": "662000"
  },
  {
    "text": "rescheduled",
    "start": "662000",
    "end": "662880"
  },
  {
    "text": "when resource demand is lower and this",
    "start": "662880",
    "end": "665360"
  },
  {
    "text": "is all just can be done",
    "start": "665360",
    "end": "666399"
  },
  {
    "text": "you know fairly automatically",
    "start": "666399",
    "end": "669760"
  },
  {
    "text": "the power of kubernetes allows for",
    "start": "669760",
    "end": "671519"
  },
  {
    "text": "resources to be redistributed",
    "start": "671519",
    "end": "673600"
  },
  {
    "text": "once a spark job is complete and the",
    "start": "673600",
    "end": "675200"
  },
  {
    "text": "pods have been released",
    "start": "675200",
    "end": "676640"
  },
  {
    "text": "so that that resources can be given to",
    "start": "676640",
    "end": "678959"
  },
  {
    "text": "other spark jobs or to other services",
    "start": "678959",
    "end": "682720"
  },
  {
    "text": "so um next year so talking about",
    "start": "685040",
    "end": "688399"
  },
  {
    "text": "containers and python and security",
    "start": "688399",
    "end": "690880"
  },
  {
    "text": "so the ability to easily move workloads",
    "start": "690880",
    "end": "693920"
  },
  {
    "text": "from dev to prod",
    "start": "693920",
    "end": "695519"
  },
  {
    "text": "to add new libraries um are you know",
    "start": "695519",
    "end": "698560"
  },
  {
    "text": "that is because of things like isolation",
    "start": "698560",
    "end": "701279"
  },
  {
    "text": "that cube has",
    "start": "701279",
    "end": "702399"
  },
  {
    "text": "so data engineers and scientists are",
    "start": "702399",
    "end": "704160"
  },
  {
    "text": "always trying to find",
    "start": "704160",
    "end": "705519"
  },
  {
    "text": "new python packages to add um and ways",
    "start": "705519",
    "end": "709120"
  },
  {
    "text": "you know new workloads that they want to",
    "start": "709120",
    "end": "710560"
  },
  {
    "text": "be able to utilize the latest versions",
    "start": "710560",
    "end": "712160"
  },
  {
    "text": "of these libraries and new versions of",
    "start": "712160",
    "end": "713920"
  },
  {
    "text": "spark",
    "start": "713920",
    "end": "714720"
  },
  {
    "text": "so with a single yarn mesos or",
    "start": "714720",
    "end": "716959"
  },
  {
    "text": "standalone cube cluster",
    "start": "716959",
    "end": "718959"
  },
  {
    "text": "um standalone spark cluster i should say",
    "start": "718959",
    "end": "722160"
  },
  {
    "text": "your apps will be tied to using only",
    "start": "722160",
    "end": "724000"
  },
  {
    "text": "using one version of spark",
    "start": "724000",
    "end": "725920"
  },
  {
    "text": "or python that's in you know that's on",
    "start": "725920",
    "end": "728160"
  },
  {
    "text": "that cluster",
    "start": "728160",
    "end": "729200"
  },
  {
    "text": "with kubernetes and containerization of",
    "start": "729200",
    "end": "731120"
  },
  {
    "text": "sparkler workloads",
    "start": "731120",
    "end": "732800"
  },
  {
    "text": "one data scientist can be using the",
    "start": "732800",
    "end": "734639"
  },
  {
    "text": "latest version of spark so 3.1.1",
    "start": "734639",
    "end": "738000"
  },
  {
    "text": "with python version 3.0 while your data",
    "start": "738000",
    "end": "740480"
  },
  {
    "text": "engineer",
    "start": "740480",
    "end": "741360"
  },
  {
    "text": "can continue to use spark 2.4 why not",
    "start": "741360",
    "end": "744000"
  },
  {
    "text": "with python version 2.5",
    "start": "744000",
    "end": "745920"
  },
  {
    "text": "all along the same cluster because it's",
    "start": "745920",
    "end": "747600"
  },
  {
    "text": "all containerized and",
    "start": "747600",
    "end": "749440"
  },
  {
    "text": "isolated so anyone who has had to deal",
    "start": "749440",
    "end": "752399"
  },
  {
    "text": "with python dependencies",
    "start": "752399",
    "end": "754160"
  },
  {
    "text": "understands the importance of having the",
    "start": "754160",
    "end": "755920"
  },
  {
    "text": "ability to abstract",
    "start": "755920",
    "end": "759200"
  },
  {
    "text": "using containers so that their jobs can",
    "start": "759200",
    "end": "761920"
  },
  {
    "text": "do and use exactly what they want to use",
    "start": "761920",
    "end": "765360"
  },
  {
    "text": "also the ability to add increased",
    "start": "765360",
    "end": "768079"
  },
  {
    "text": "isolation",
    "start": "768079",
    "end": "769040"
  },
  {
    "text": "when dealing with data is very important",
    "start": "769040",
    "end": "771200"
  },
  {
    "text": "and kubernetes allows for this",
    "start": "771200",
    "end": "772800"
  },
  {
    "text": "where yarn or mesos just didn't to the",
    "start": "772800",
    "end": "774560"
  },
  {
    "text": "same degree",
    "start": "774560",
    "end": "777120"
  },
  {
    "text": "so also just to add when moving from",
    "start": "777440",
    "end": "779279"
  },
  {
    "text": "yarn or mesos kubernetes has a lot of",
    "start": "779279",
    "end": "781760"
  },
  {
    "text": "custom configurations",
    "start": "781760",
    "end": "783040"
  },
  {
    "text": "that makes it a bit more flexible than",
    "start": "783040",
    "end": "785600"
  },
  {
    "text": "yarn",
    "start": "785600",
    "end": "786079"
  },
  {
    "text": "per se",
    "start": "786079",
    "end": "788480"
  },
  {
    "text": "and last but not least learning a new",
    "start": "789120",
    "end": "791680"
  },
  {
    "text": "skill",
    "start": "791680",
    "end": "792160"
  },
  {
    "text": "right for holden and i personally",
    "start": "792160",
    "end": "794079"
  },
  {
    "text": "adopting to kubernetes",
    "start": "794079",
    "end": "795600"
  },
  {
    "text": "has allowed us to pick up one more tool",
    "start": "795600",
    "end": "797279"
  },
  {
    "text": "for our tool belt uh and of course now",
    "start": "797279",
    "end": "799440"
  },
  {
    "text": "we're so much more popular",
    "start": "799440",
    "end": "800639"
  },
  {
    "text": "right because now not only do we know",
    "start": "800639",
    "end": "801839"
  },
  {
    "text": "apache spark we also know all about",
    "start": "801839",
    "end": "803440"
  },
  {
    "text": "kubernetes",
    "start": "803440",
    "end": "804800"
  },
  {
    "text": "so with that said i will pass this off",
    "start": "804800",
    "end": "807360"
  },
  {
    "text": "to holden",
    "start": "807360",
    "end": "808160"
  },
  {
    "text": "to tell us more about what worked well",
    "start": "808160",
    "end": "810959"
  },
  {
    "text": "uh what didn't work so well making the",
    "start": "810959",
    "end": "812639"
  },
  {
    "text": "transformation from spark on yarn spark",
    "start": "812639",
    "end": "814800"
  },
  {
    "text": "on mesos to spark on kubernetes",
    "start": "814800",
    "end": "816880"
  },
  {
    "text": "thank you awesome and thanks for that",
    "start": "816880",
    "end": "819600"
  },
  {
    "text": "introduction",
    "start": "819600",
    "end": "820720"
  },
  {
    "text": "um so now i'm going to talk about sort",
    "start": "820720",
    "end": "823519"
  },
  {
    "text": "of the second half of the presentation",
    "start": "823519",
    "end": "825360"
  },
  {
    "text": "namely what worked well and where we had",
    "start": "825360",
    "end": "827600"
  },
  {
    "text": "rooms for growth",
    "start": "827600",
    "end": "829600"
  },
  {
    "text": "so small to medium-sized etl",
    "start": "829600",
    "end": "832800"
  },
  {
    "text": "jobs worked really well migrating them",
    "start": "832800",
    "end": "834800"
  },
  {
    "text": "to cube was relatively easy the only",
    "start": "834800",
    "end": "837120"
  },
  {
    "text": "thing that we really had to do",
    "start": "837120",
    "end": "838800"
  },
  {
    "text": "was increase the resource request to",
    "start": "838800",
    "end": "841920"
  },
  {
    "text": "match the reality of what they were",
    "start": "841920",
    "end": "843279"
  },
  {
    "text": "actually using",
    "start": "843279",
    "end": "844480"
  },
  {
    "text": "because yarn and mesos weren't enforcing",
    "start": "844480",
    "end": "846880"
  },
  {
    "text": "the resource",
    "start": "846880",
    "end": "848480"
  },
  {
    "text": "limits quite as strictly as cube ended",
    "start": "848480",
    "end": "850880"
  },
  {
    "text": "up enforcing them",
    "start": "850880",
    "end": "853040"
  },
  {
    "text": "there were some challenges around",
    "start": "853040",
    "end": "854839"
  },
  {
    "text": "integration with the different data",
    "start": "854839",
    "end": "856639"
  },
  {
    "text": "sources and that mostly comes back to",
    "start": "856639",
    "end": "859120"
  },
  {
    "text": "uh some networking configuration choices",
    "start": "859120",
    "end": "861120"
  },
  {
    "text": "that were made",
    "start": "861120",
    "end": "862959"
  },
  {
    "text": "and i think we could make some",
    "start": "862959",
    "end": "864000"
  },
  {
    "text": "improvements there but that's sort of",
    "start": "864000",
    "end": "866480"
  },
  {
    "text": "just something to keep in mind like make",
    "start": "866480",
    "end": "868320"
  },
  {
    "text": "sure that you can easily access your",
    "start": "868320",
    "end": "869920"
  },
  {
    "text": "data sources",
    "start": "869920",
    "end": "871839"
  },
  {
    "text": "but it was a relatively easy fix",
    "start": "871839",
    "end": "875680"
  },
  {
    "text": "large and long etl jobs were a bit more",
    "start": "875839",
    "end": "878399"
  },
  {
    "text": "challenging",
    "start": "878399",
    "end": "879839"
  },
  {
    "text": "long enough running jobs at low",
    "start": "879839",
    "end": "881600"
  },
  {
    "text": "priorities tended to run into over",
    "start": "881600",
    "end": "883519"
  },
  {
    "text": "commit issues",
    "start": "883519",
    "end": "884959"
  },
  {
    "text": "they'd still succeed but often they take",
    "start": "884959",
    "end": "887440"
  },
  {
    "text": "a lot longer than they would",
    "start": "887440",
    "end": "889360"
  },
  {
    "text": "on mesos or yarn and the primary room",
    "start": "889360",
    "end": "893519"
  },
  {
    "text": "for growth here",
    "start": "893519",
    "end": "894560"
  },
  {
    "text": "is more efficient resource utilization",
    "start": "894560",
    "end": "897279"
  },
  {
    "text": "and more",
    "start": "897279",
    "end": "898000"
  },
  {
    "text": "effective handling of over commit issues",
    "start": "898000",
    "end": "901920"
  },
  {
    "text": "multi-language jobs are where things",
    "start": "901920",
    "end": "903680"
  },
  {
    "text": "started to get a little",
    "start": "903680",
    "end": "906480"
  },
  {
    "text": "challenging um so one of the really",
    "start": "906480",
    "end": "909120"
  },
  {
    "text": "great things is that dependency",
    "start": "909120",
    "end": "910560"
  },
  {
    "text": "management",
    "start": "910560",
    "end": "911120"
  },
  {
    "text": "improves substantially compared to yarn",
    "start": "911120",
    "end": "913519"
  },
  {
    "text": "in the yarn world",
    "start": "913519",
    "end": "914880"
  },
  {
    "text": "all the dependencies had to be managed",
    "start": "914880",
    "end": "917440"
  },
  {
    "text": "by a systems administrator whereas",
    "start": "917440",
    "end": "920000"
  },
  {
    "text": "with running on cube it could be very",
    "start": "920000",
    "end": "922959"
  },
  {
    "text": "much more",
    "start": "922959",
    "end": "923519"
  },
  {
    "text": "self-serve the initial migrations often",
    "start": "923519",
    "end": "926800"
  },
  {
    "text": "ran into resource difficulty",
    "start": "926800",
    "end": "928720"
  },
  {
    "text": "and unplanned exits caused large amounts",
    "start": "928720",
    "end": "931279"
  },
  {
    "text": "of recomputation",
    "start": "931279",
    "end": "933120"
  },
  {
    "text": "and this is you know the recomputation",
    "start": "933120",
    "end": "935839"
  },
  {
    "text": "that is sort of expected",
    "start": "935839",
    "end": "937759"
  },
  {
    "text": "but it was more than the amount of",
    "start": "937759",
    "end": "939279"
  },
  {
    "text": "recomputation that we were seeing",
    "start": "939279",
    "end": "941199"
  },
  {
    "text": "running these jobs in other cluster",
    "start": "941199",
    "end": "943279"
  },
  {
    "text": "environments",
    "start": "943279",
    "end": "945120"
  },
  {
    "text": "most of our opportunities for",
    "start": "945120",
    "end": "946800"
  },
  {
    "text": "improvement are around",
    "start": "946800",
    "end": "948560"
  },
  {
    "text": "memory allocation and specifically sort",
    "start": "948560",
    "end": "951600"
  },
  {
    "text": "of how we share",
    "start": "951600",
    "end": "953360"
  },
  {
    "text": "native and jvm memory",
    "start": "953360",
    "end": "957680"
  },
  {
    "text": "complex ml jobs did not work well when",
    "start": "957920",
    "end": "960399"
  },
  {
    "text": "we tried to migrate them",
    "start": "960399",
    "end": "962480"
  },
  {
    "text": "primarily due to much more expensive",
    "start": "962480",
    "end": "966800"
  },
  {
    "text": "recovery costs so in all these",
    "start": "966800",
    "end": "968800"
  },
  {
    "text": "situations spark handles",
    "start": "968800",
    "end": "970240"
  },
  {
    "text": "executor failure by recomputing data",
    "start": "970240",
    "end": "972320"
  },
  {
    "text": "loss with the complex",
    "start": "972320",
    "end": "974079"
  },
  {
    "text": "ml jobs the cost of recompeting that",
    "start": "974079",
    "end": "977440"
  },
  {
    "text": "data is really",
    "start": "977440",
    "end": "978639"
  },
  {
    "text": "really quite expensive the opportunities",
    "start": "978639",
    "end": "981519"
  },
  {
    "text": "for improvement",
    "start": "981519",
    "end": "982959"
  },
  {
    "text": "are around checkpointing um and so one",
    "start": "982959",
    "end": "985040"
  },
  {
    "text": "of the things that we can do to sort of",
    "start": "985040",
    "end": "987199"
  },
  {
    "text": "deal with this more expensive",
    "start": "987199",
    "end": "988880"
  },
  {
    "text": "recomputation",
    "start": "988880",
    "end": "990240"
  },
  {
    "text": "is um when we get to these really",
    "start": "990240",
    "end": "992800"
  },
  {
    "text": "expensive points",
    "start": "992800",
    "end": "993759"
  },
  {
    "text": "is checkpoint to persistent storage but",
    "start": "993759",
    "end": "996000"
  },
  {
    "text": "we had difficulties connecting to",
    "start": "996000",
    "end": "997519"
  },
  {
    "text": "persistent storage",
    "start": "997519",
    "end": "999920"
  },
  {
    "text": "specifically the kinds with ttls that",
    "start": "999920",
    "end": "1001680"
  },
  {
    "text": "can do automatic cleanups",
    "start": "1001680",
    "end": "1003279"
  },
  {
    "text": "and so this is one of the rooms for",
    "start": "1003279",
    "end": "1004800"
  },
  {
    "text": "growth that we have in complex ml jobs",
    "start": "1004800",
    "end": "1008160"
  },
  {
    "text": "streaming jobs had a lot of room for",
    "start": "1008160",
    "end": "1010720"
  },
  {
    "text": "growth",
    "start": "1010720",
    "end": "1011600"
  },
  {
    "text": "and just many areas of opportunity for",
    "start": "1011600",
    "end": "1014399"
  },
  {
    "text": "investment",
    "start": "1014399",
    "end": "1015440"
  },
  {
    "text": "um they just frankly don't work very",
    "start": "1015440",
    "end": "1018160"
  },
  {
    "text": "well right now",
    "start": "1018160",
    "end": "1019040"
  },
  {
    "text": "and a lot of that is around the",
    "start": "1019040",
    "end": "1020160"
  },
  {
    "text": "connection to the different data sources",
    "start": "1020160",
    "end": "1021839"
  },
  {
    "text": "um",
    "start": "1021839",
    "end": "1022240"
  },
  {
    "text": "it turns out that our connection to the",
    "start": "1022240",
    "end": "1023759"
  },
  {
    "text": "streaming data sources has even more",
    "start": "1023759",
    "end": "1025839"
  },
  {
    "text": "room for growth than our connections to",
    "start": "1025839",
    "end": "1027678"
  },
  {
    "text": "the batch data sources",
    "start": "1027679",
    "end": "1029520"
  },
  {
    "text": "um also checkpointing is especially",
    "start": "1029520",
    "end": "1031438"
  },
  {
    "text": "important in streaming",
    "start": "1031439",
    "end": "1032798"
  },
  {
    "text": "and so the same problem that we had from",
    "start": "1032799",
    "end": "1034880"
  },
  {
    "text": "ml is even more present with the",
    "start": "1034880",
    "end": "1037038"
  },
  {
    "text": "streaming jobs",
    "start": "1037039",
    "end": "1039839"
  },
  {
    "text": "so okay you know that's about our",
    "start": "1040319",
    "end": "1042720"
  },
  {
    "text": "experience as moving to cube",
    "start": "1042720",
    "end": "1044079"
  },
  {
    "text": "what are the best practices that we",
    "start": "1044079",
    "end": "1045520"
  },
  {
    "text": "learned from this",
    "start": "1045520",
    "end": "1047438"
  },
  {
    "text": "so one of the things that we learned is",
    "start": "1047439",
    "end": "1049039"
  },
  {
    "text": "not to just cache everything",
    "start": "1049039",
    "end": "1051760"
  },
  {
    "text": "caching was never free right but",
    "start": "1051760",
    "end": "1054799"
  },
  {
    "text": "now it costs even more and that's",
    "start": "1054799",
    "end": "1056320"
  },
  {
    "text": "because when we decommission an executor",
    "start": "1056320",
    "end": "1059280"
  },
  {
    "text": "we now have to migrate the cache data",
    "start": "1059280",
    "end": "1062000"
  },
  {
    "text": "and also now for caching stuff to disk",
    "start": "1062000",
    "end": "1064000"
  },
  {
    "text": "disk is actually a",
    "start": "1064000",
    "end": "1065360"
  },
  {
    "text": "metered resource so be careful like",
    "start": "1065360",
    "end": "1068559"
  },
  {
    "text": "think about are you actually going to",
    "start": "1068559",
    "end": "1069679"
  },
  {
    "text": "use the data twice",
    "start": "1069679",
    "end": "1071039"
  },
  {
    "text": "if you're done with it tell spark you're",
    "start": "1071039",
    "end": "1073200"
  },
  {
    "text": "done with it right",
    "start": "1073200",
    "end": "1074400"
  },
  {
    "text": "especially for users in no quick",
    "start": "1074400",
    "end": "1075919"
  },
  {
    "text": "environments the garbage collector isn't",
    "start": "1075919",
    "end": "1077679"
  },
  {
    "text": "able",
    "start": "1077679",
    "end": "1079679"
  },
  {
    "text": "to handle this as well so you need to",
    "start": "1079679",
    "end": "1082400"
  },
  {
    "text": "explicitly tell spark that you're done",
    "start": "1082400",
    "end": "1083919"
  },
  {
    "text": "with the data and it can get rid of it",
    "start": "1083919",
    "end": "1086320"
  },
  {
    "text": "timbit of course is not done with the",
    "start": "1086320",
    "end": "1087840"
  },
  {
    "text": "bone and he would prefer that we never",
    "start": "1087840",
    "end": "1089679"
  },
  {
    "text": "get rid of the bone",
    "start": "1089679",
    "end": "1092799"
  },
  {
    "text": "using disaggregated storage is super",
    "start": "1093280",
    "end": "1096320"
  },
  {
    "text": "important um",
    "start": "1096320",
    "end": "1097520"
  },
  {
    "text": "the big thing is there's no stay",
    "start": "1097520",
    "end": "1099120"
  },
  {
    "text": "resident block manager anymore",
    "start": "1099120",
    "end": "1101200"
  },
  {
    "text": "um and yeah okay data locality does",
    "start": "1101200",
    "end": "1104480"
  },
  {
    "text": "matter",
    "start": "1104480",
    "end": "1105440"
  },
  {
    "text": "but it's not enough to try and co-locate",
    "start": "1105440",
    "end": "1107679"
  },
  {
    "text": "hdfs it's not worth it",
    "start": "1107679",
    "end": "1109600"
  },
  {
    "text": "um another really interesting thing is",
    "start": "1109600",
    "end": "1112080"
  },
  {
    "text": "that using cloud storage from on-prem",
    "start": "1112080",
    "end": "1114480"
  },
  {
    "text": "doesn't have as much overhead as one",
    "start": "1114480",
    "end": "1116320"
  },
  {
    "text": "might think if you structure your",
    "start": "1116320",
    "end": "1117919"
  },
  {
    "text": "network correctly",
    "start": "1117919",
    "end": "1119440"
  },
  {
    "text": "um and so initially in a lot of",
    "start": "1119440",
    "end": "1121760"
  },
  {
    "text": "situations we'd assumed that we had to",
    "start": "1121760",
    "end": "1123919"
  },
  {
    "text": "use hdfs but after we did some",
    "start": "1123919",
    "end": "1125760"
  },
  {
    "text": "benchmarks it turns out that it was",
    "start": "1125760",
    "end": "1127360"
  },
  {
    "text": "actually perfectly reasonable",
    "start": "1127360",
    "end": "1128960"
  },
  {
    "text": "to use cloud storage from on-prem um so",
    "start": "1128960",
    "end": "1132000"
  },
  {
    "text": "definitely like don't just assume that",
    "start": "1132000",
    "end": "1134320"
  },
  {
    "text": "you need to co-locate hdfs",
    "start": "1134320",
    "end": "1136240"
  },
  {
    "text": "take the time run the benchmarks and see",
    "start": "1136240",
    "end": "1138080"
  },
  {
    "text": "if it's actually going to be worth it",
    "start": "1138080",
    "end": "1139760"
  },
  {
    "text": "for you",
    "start": "1139760",
    "end": "1141840"
  },
  {
    "text": "one of the other things is that you're",
    "start": "1141840",
    "end": "1143440"
  },
  {
    "text": "going to need to increase your resource",
    "start": "1143440",
    "end": "1144960"
  },
  {
    "text": "requests and we talked about this a",
    "start": "1144960",
    "end": "1146320"
  },
  {
    "text": "little bit with the job migrations",
    "start": "1146320",
    "end": "1148840"
  },
  {
    "text": "essentially yarn containers are",
    "start": "1148840",
    "end": "1151840"
  },
  {
    "text": "very fuzzy definitions of containers and",
    "start": "1151840",
    "end": "1154320"
  },
  {
    "text": "we have",
    "start": "1154320",
    "end": "1154880"
  },
  {
    "text": "much stricter resource requirements in",
    "start": "1154880",
    "end": "1157679"
  },
  {
    "text": "cube",
    "start": "1157679",
    "end": "1158320"
  },
  {
    "text": "uh so you're going to need to allocate",
    "start": "1158320",
    "end": "1159840"
  },
  {
    "text": "more memory a femoral disk starts",
    "start": "1159840",
    "end": "1162160"
  },
  {
    "text": "mattering that wasn't tracked at all",
    "start": "1162160",
    "end": "1165360"
  },
  {
    "text": "previously another one is don't set",
    "start": "1165360",
    "end": "1168720"
  },
  {
    "text": "quota for the sake of quota",
    "start": "1168720",
    "end": "1171280"
  },
  {
    "text": "default spark uses config maps we ended",
    "start": "1171280",
    "end": "1174160"
  },
  {
    "text": "up doing",
    "start": "1174160",
    "end": "1174960"
  },
  {
    "text": "a large rewrite of some code because",
    "start": "1174960",
    "end": "1178400"
  },
  {
    "text": "of a config map quota and",
    "start": "1178400",
    "end": "1182799"
  },
  {
    "text": "you know it turns out that config maps",
    "start": "1182799",
    "end": "1184720"
  },
  {
    "text": "weren't as expensive as we had assumed",
    "start": "1184720",
    "end": "1186720"
  },
  {
    "text": "so this is this is very important",
    "start": "1186720",
    "end": "1188799"
  },
  {
    "text": "uh when you are starting to set quota",
    "start": "1188799",
    "end": "1190960"
  },
  {
    "text": "definitely take the time",
    "start": "1190960",
    "end": "1192720"
  },
  {
    "text": "to see if this is actually a constrained",
    "start": "1192720",
    "end": "1195679"
  },
  {
    "text": "resource",
    "start": "1195679",
    "end": "1196320"
  },
  {
    "text": "or if you know you don't actually need",
    "start": "1196320",
    "end": "1198960"
  },
  {
    "text": "to have a quota here and it's perfectly",
    "start": "1198960",
    "end": "1200559"
  },
  {
    "text": "fine to let things run wild",
    "start": "1200559",
    "end": "1203919"
  },
  {
    "text": "so in addition to sort of the best",
    "start": "1204159",
    "end": "1205600"
  },
  {
    "text": "stream practices for your jobs",
    "start": "1205600",
    "end": "1207520"
  },
  {
    "text": "uh let's talk about what things we",
    "start": "1207520",
    "end": "1209280"
  },
  {
    "text": "changed uh so we added a new mechanism",
    "start": "1209280",
    "end": "1212400"
  },
  {
    "text": "for dynamic scaling and spark uh i'm",
    "start": "1212400",
    "end": "1215520"
  },
  {
    "text": "really excited about this this is",
    "start": "1215520",
    "end": "1216799"
  },
  {
    "text": "actually based on a design",
    "start": "1216799",
    "end": "1218320"
  },
  {
    "text": "that i came up with five years ago uh",
    "start": "1218320",
    "end": "1221039"
  },
  {
    "text": "and it just didn't make sense back then",
    "start": "1221039",
    "end": "1222880"
  },
  {
    "text": "um but we'll talk more about it in a",
    "start": "1222880",
    "end": "1224799"
  },
  {
    "text": "little bit if we remove the commit fig",
    "start": "1224799",
    "end": "1226559"
  },
  {
    "text": "map requirement as we talked about uh we",
    "start": "1226559",
    "end": "1228640"
  },
  {
    "text": "essentially added an alternative because",
    "start": "1228640",
    "end": "1230240"
  },
  {
    "text": "of our quota system",
    "start": "1230240",
    "end": "1232159"
  },
  {
    "text": "um persistent storage for fallback uh on",
    "start": "1232159",
    "end": "1234960"
  },
  {
    "text": "out of disk events",
    "start": "1234960",
    "end": "1236159"
  },
  {
    "text": "um and we were really hoping that was",
    "start": "1236159",
    "end": "1237679"
  },
  {
    "text": "gonna trigger",
    "start": "1237679",
    "end": "1239520"
  },
  {
    "text": "uh more frequently but it turns out that",
    "start": "1239520",
    "end": "1241679"
  },
  {
    "text": "you know um because of how ephemeral",
    "start": "1241679",
    "end": "1243360"
  },
  {
    "text": "this quarter works",
    "start": "1243360",
    "end": "1244400"
  },
  {
    "text": "uh we don't actually get the out-of-disc",
    "start": "1244400",
    "end": "1245919"
  },
  {
    "text": "events um in the same way so we we also",
    "start": "1245919",
    "end": "1249039"
  },
  {
    "text": "added some additional hooks inside of",
    "start": "1249039",
    "end": "1250640"
  },
  {
    "text": "there",
    "start": "1250640",
    "end": "1251679"
  },
  {
    "text": "integrations into pvcs some templating",
    "start": "1251679",
    "end": "1254960"
  },
  {
    "text": "um the we also gave users the ability to",
    "start": "1254960",
    "end": "1257840"
  },
  {
    "text": "explicitly remove unneeded shuffle files",
    "start": "1257840",
    "end": "1261280"
  },
  {
    "text": "and there were a bunch of sort of corner",
    "start": "1261280",
    "end": "1263520"
  },
  {
    "text": "cases",
    "start": "1263520",
    "end": "1264320"
  },
  {
    "text": "in sparks understanding of pod state",
    "start": "1264320",
    "end": "1265919"
  },
  {
    "text": "that historically hadn't mattered",
    "start": "1265919",
    "end": "1267600"
  },
  {
    "text": "but once we started to add dynamic",
    "start": "1267600",
    "end": "1269120"
  },
  {
    "text": "scaling didn't matter",
    "start": "1269120",
    "end": "1272159"
  },
  {
    "text": "um and so we added graceful",
    "start": "1272159",
    "end": "1274080"
  },
  {
    "text": "decommissioning and we",
    "start": "1274080",
    "end": "1275600"
  },
  {
    "text": "made this to support dynamic allocation",
    "start": "1275600",
    "end": "1277760"
  },
  {
    "text": "on spark on cube",
    "start": "1277760",
    "end": "1279440"
  },
  {
    "text": "and this is really important because",
    "start": "1279440",
    "end": "1281360"
  },
  {
    "text": "historically spark on cube",
    "start": "1281360",
    "end": "1283520"
  },
  {
    "text": "has not had a good dynamic allocation",
    "start": "1283520",
    "end": "1287280"
  },
  {
    "text": "only recently a restricted dynamic",
    "start": "1287280",
    "end": "1289840"
  },
  {
    "text": "allocation was added",
    "start": "1289840",
    "end": "1291280"
  },
  {
    "text": "where if there was no data on an",
    "start": "1291280",
    "end": "1293520"
  },
  {
    "text": "executor we could get rid of it",
    "start": "1293520",
    "end": "1295360"
  },
  {
    "text": "but by adding graceful decommissioning",
    "start": "1295360",
    "end": "1296880"
  },
  {
    "text": "if there is data on an executor",
    "start": "1296880",
    "end": "1298720"
  },
  {
    "text": "we can still get rid of it we just",
    "start": "1298720",
    "end": "1300000"
  },
  {
    "text": "migrate the data away first",
    "start": "1300000",
    "end": "1302799"
  },
  {
    "text": "there are some alternatives proposed by",
    "start": "1302799",
    "end": "1304559"
  },
  {
    "text": "other people in the community of adding",
    "start": "1304559",
    "end": "1306320"
  },
  {
    "text": "a truly external shuffle service",
    "start": "1306320",
    "end": "1308480"
  },
  {
    "text": "um and there's a few different ones and",
    "start": "1308480",
    "end": "1310000"
  },
  {
    "text": "we're not sure which one is going to",
    "start": "1310000",
    "end": "1311200"
  },
  {
    "text": "land",
    "start": "1311200",
    "end": "1312000"
  },
  {
    "text": "but um you know it'll be interesting to",
    "start": "1312000",
    "end": "1314320"
  },
  {
    "text": "see those things",
    "start": "1314320",
    "end": "1315280"
  },
  {
    "text": "and i think even once one of those",
    "start": "1315280",
    "end": "1316559"
  },
  {
    "text": "things land we'll probably still keep",
    "start": "1316559",
    "end": "1318159"
  },
  {
    "text": "graceful decommissioning and we'll still",
    "start": "1318159",
    "end": "1319919"
  },
  {
    "text": "leave it turned on",
    "start": "1319919",
    "end": "1321120"
  },
  {
    "text": "because the truly external shuffle",
    "start": "1321120",
    "end": "1322640"
  },
  {
    "text": "service only handles shuffle files",
    "start": "1322640",
    "end": "1324400"
  },
  {
    "text": "and spark also has this concept of cache",
    "start": "1324400",
    "end": "1326480"
  },
  {
    "text": "blocks and i think it makes sense to",
    "start": "1326480",
    "end": "1328240"
  },
  {
    "text": "migrate cache blocks",
    "start": "1328240",
    "end": "1330720"
  },
  {
    "text": "just while we're talking about this um",
    "start": "1330720",
    "end": "1333039"
  },
  {
    "text": "one of the things that was a little",
    "start": "1333039",
    "end": "1334720"
  },
  {
    "text": "counterintuitive",
    "start": "1334720",
    "end": "1336080"
  },
  {
    "text": "uh with the configuration that we found",
    "start": "1336080",
    "end": "1338559"
  },
  {
    "text": "was",
    "start": "1338559",
    "end": "1339679"
  },
  {
    "text": "uh that we went from this sort of",
    "start": "1339679",
    "end": "1341200"
  },
  {
    "text": "initial configuration of an executor",
    "start": "1341200",
    "end": "1342960"
  },
  {
    "text": "idle time in a cache idle time of 120",
    "start": "1342960",
    "end": "1345200"
  },
  {
    "text": "seconds and then we increased those idle",
    "start": "1345200",
    "end": "1347360"
  },
  {
    "text": "times",
    "start": "1347360",
    "end": "1347919"
  },
  {
    "text": "and we actually got better scale up and",
    "start": "1347919",
    "end": "1349840"
  },
  {
    "text": "scale down",
    "start": "1349840",
    "end": "1351120"
  },
  {
    "text": "with the higher idle times and this is",
    "start": "1351120",
    "end": "1354080"
  },
  {
    "text": "because essentially",
    "start": "1354080",
    "end": "1356960"
  },
  {
    "text": "spark doesn't do a great or really",
    "start": "1356960",
    "end": "1358960"
  },
  {
    "text": "perfect job of",
    "start": "1358960",
    "end": "1360000"
  },
  {
    "text": "keeping track of sort of if an executor",
    "start": "1360000",
    "end": "1362720"
  },
  {
    "text": "is",
    "start": "1362720",
    "end": "1363919"
  },
  {
    "text": "likely to have a job scheduled on it and",
    "start": "1363919",
    "end": "1365919"
  },
  {
    "text": "so we would get into the situation",
    "start": "1365919",
    "end": "1368640"
  },
  {
    "text": "where we would start to see executors",
    "start": "1368640",
    "end": "1370240"
  },
  {
    "text": "coming up and going away essentially",
    "start": "1370240",
    "end": "1371760"
  },
  {
    "text": "flapping very quickly",
    "start": "1371760",
    "end": "1373200"
  },
  {
    "text": "when we tried to set tighter timeouts",
    "start": "1373200",
    "end": "1375120"
  },
  {
    "text": "that we thought would actually",
    "start": "1375120",
    "end": "1376640"
  },
  {
    "text": "cause better scale up and scale down",
    "start": "1376640",
    "end": "1378720"
  },
  {
    "text": "experience but by relaxing these",
    "start": "1378720",
    "end": "1380320"
  },
  {
    "text": "timeouts",
    "start": "1380320",
    "end": "1381600"
  },
  {
    "text": "we actually got a much more reasonable",
    "start": "1381600",
    "end": "1383440"
  },
  {
    "text": "scale up and scaled down experience",
    "start": "1383440",
    "end": "1387200"
  },
  {
    "text": "um we added external shuffle storage",
    "start": "1387520",
    "end": "1389679"
  },
  {
    "text": "this allows",
    "start": "1389679",
    "end": "1390640"
  },
  {
    "text": "scaling beyond what executor or",
    "start": "1390640",
    "end": "1392159"
  },
  {
    "text": "executive migrations",
    "start": "1392159",
    "end": "1393840"
  },
  {
    "text": "support um there are",
    "start": "1393840",
    "end": "1397039"
  },
  {
    "text": "alternative proposals for for doing",
    "start": "1397039",
    "end": "1398960"
  },
  {
    "text": "essentially",
    "start": "1398960",
    "end": "1400080"
  },
  {
    "text": "um more on top of this but this is",
    "start": "1400080",
    "end": "1403520"
  },
  {
    "text": "really important because with executor",
    "start": "1403520",
    "end": "1405200"
  },
  {
    "text": "to executor migrations",
    "start": "1405200",
    "end": "1407120"
  },
  {
    "text": "we can only scale down to the point that",
    "start": "1407120",
    "end": "1408799"
  },
  {
    "text": "we still have enough of mrl disk",
    "start": "1408799",
    "end": "1410400"
  },
  {
    "text": "available",
    "start": "1410400",
    "end": "1411520"
  },
  {
    "text": "for the data um and if you have a lot of",
    "start": "1411520",
    "end": "1414080"
  },
  {
    "text": "data",
    "start": "1414080",
    "end": "1414720"
  },
  {
    "text": "but like let's say your data scientist",
    "start": "1414720",
    "end": "1416159"
  },
  {
    "text": "goes home at the end of the night really",
    "start": "1416159",
    "end": "1417840"
  },
  {
    "text": "it probably makes sense to go ahead and",
    "start": "1417840",
    "end": "1419360"
  },
  {
    "text": "put that data",
    "start": "1419360",
    "end": "1420240"
  },
  {
    "text": "in some kind of external storage while",
    "start": "1420240",
    "end": "1422720"
  },
  {
    "text": "the data scientist",
    "start": "1422720",
    "end": "1423679"
  },
  {
    "text": "is you know taking a break from their",
    "start": "1423679",
    "end": "1425520"
  },
  {
    "text": "job you know hanging out with their",
    "start": "1425520",
    "end": "1427240"
  },
  {
    "text": "family um",
    "start": "1427240",
    "end": "1430799"
  },
  {
    "text": "so what were some areas for improvement",
    "start": "1430799",
    "end": "1433840"
  },
  {
    "text": "um so specifically in graceful",
    "start": "1433840",
    "end": "1436559"
  },
  {
    "text": "decommissioning and dynamic allocation",
    "start": "1436559",
    "end": "1438799"
  },
  {
    "text": "i would say our biggest area for",
    "start": "1438799",
    "end": "1440320"
  },
  {
    "text": "improvement here is documentation",
    "start": "1440320",
    "end": "1442640"
  },
  {
    "text": "it is possible to turn on but really",
    "start": "1442640",
    "end": "1445039"
  },
  {
    "text": "right now",
    "start": "1445039",
    "end": "1445760"
  },
  {
    "text": "um as as we saw from that configuration",
    "start": "1445760",
    "end": "1448080"
  },
  {
    "text": "example",
    "start": "1448080",
    "end": "1448960"
  },
  {
    "text": "it it sort of involves a lot of",
    "start": "1448960",
    "end": "1451520"
  },
  {
    "text": "fine-tuning and playing with things",
    "start": "1451520",
    "end": "1453440"
  },
  {
    "text": "so we haven't documented it because we",
    "start": "1453440",
    "end": "1455120"
  },
  {
    "text": "don't know what the right settings are",
    "start": "1455120",
    "end": "1456960"
  },
  {
    "text": "generally we know what the right",
    "start": "1456960",
    "end": "1458000"
  },
  {
    "text": "settings are for our cluster but we",
    "start": "1458000",
    "end": "1459840"
  },
  {
    "text": "haven't had enough other people sort of",
    "start": "1459840",
    "end": "1461360"
  },
  {
    "text": "play with it",
    "start": "1461360",
    "end": "1462240"
  },
  {
    "text": "so we don't have good recommendations",
    "start": "1462240",
    "end": "1464159"
  },
  {
    "text": "here yet and so if you do want to play",
    "start": "1464159",
    "end": "1466159"
  },
  {
    "text": "with graceful decommissioning and",
    "start": "1466159",
    "end": "1467440"
  },
  {
    "text": "dynamic allocation on spark on cube",
    "start": "1467440",
    "end": "1470080"
  },
  {
    "text": "i would really really appreciate your",
    "start": "1470080",
    "end": "1472080"
  },
  {
    "text": "feedback on sort of what's working and",
    "start": "1472080",
    "end": "1474400"
  },
  {
    "text": "what's not working",
    "start": "1474400",
    "end": "1475679"
  },
  {
    "text": "and if you can contribute that to the",
    "start": "1475679",
    "end": "1477120"
  },
  {
    "text": "documentation that would be amazing",
    "start": "1477120",
    "end": "1480559"
  },
  {
    "text": "another thing is not all data is equal",
    "start": "1480559",
    "end": "1482799"
  },
  {
    "text": "right",
    "start": "1482799",
    "end": "1483760"
  },
  {
    "text": "and spark has some internal heuristics",
    "start": "1483760",
    "end": "1485840"
  },
  {
    "text": "around what kind of data is more likely",
    "start": "1485840",
    "end": "1488000"
  },
  {
    "text": "to be used or not they're not perfect",
    "start": "1488000",
    "end": "1490000"
  },
  {
    "text": "um we could start by applying those",
    "start": "1490000",
    "end": "1491760"
  },
  {
    "text": "heuristics to block migrations or we",
    "start": "1491760",
    "end": "1494400"
  },
  {
    "text": "could try and come up with better",
    "start": "1494400",
    "end": "1496000"
  },
  {
    "text": "heuristics for what kind of data",
    "start": "1496000",
    "end": "1497919"
  },
  {
    "text": "is worth migrating",
    "start": "1497919",
    "end": "1500960"
  },
  {
    "text": "another one is avoiding cascading",
    "start": "1500960",
    "end": "1502480"
  },
  {
    "text": "failures",
    "start": "1502480",
    "end": "1504720"
  },
  {
    "text": "this we we have some work on this",
    "start": "1504720",
    "end": "1508960"
  },
  {
    "text": "and it essentially can come to the point",
    "start": "1508960",
    "end": "1510720"
  },
  {
    "text": "where quota can trigger cascading",
    "start": "1510720",
    "end": "1512720"
  },
  {
    "text": "failures as we do migrations",
    "start": "1512720",
    "end": "1514640"
  },
  {
    "text": "and we force executors over their quota",
    "start": "1514640",
    "end": "1516480"
  },
  {
    "text": "limits we have sort of a hacky solution",
    "start": "1516480",
    "end": "1519279"
  },
  {
    "text": "uh longer term though i think this is",
    "start": "1519279",
    "end": "1522320"
  },
  {
    "text": "this is an area",
    "start": "1522320",
    "end": "1523360"
  },
  {
    "text": "for for better investigation uh lazy",
    "start": "1523360",
    "end": "1526240"
  },
  {
    "text": "right back support",
    "start": "1526240",
    "end": "1527279"
  },
  {
    "text": "i think is also really interesting and",
    "start": "1527279",
    "end": "1529840"
  },
  {
    "text": "that's this idea that like yeah we still",
    "start": "1529840",
    "end": "1531840"
  },
  {
    "text": "want to try and store data locally on",
    "start": "1531840",
    "end": "1533520"
  },
  {
    "text": "the executor",
    "start": "1533520",
    "end": "1534880"
  },
  {
    "text": "but um we can start writing it back to",
    "start": "1534880",
    "end": "1537520"
  },
  {
    "text": "persistent storage",
    "start": "1537520",
    "end": "1538799"
  },
  {
    "text": "as soon as it lands on the executor um",
    "start": "1538799",
    "end": "1542080"
  },
  {
    "text": "i'm not sure if this is going to like",
    "start": "1542080",
    "end": "1544080"
  },
  {
    "text": "give us good performance or not",
    "start": "1544080",
    "end": "1545600"
  },
  {
    "text": "but i think it's an area that really has",
    "start": "1545600",
    "end": "1548480"
  },
  {
    "text": "a lot of potential and i'd like to see",
    "start": "1548480",
    "end": "1550400"
  },
  {
    "text": "some more folks investigating it",
    "start": "1550400",
    "end": "1553919"
  },
  {
    "text": "so more generally spark on cube has a",
    "start": "1554960",
    "end": "1557279"
  },
  {
    "text": "lot of areas for improvement",
    "start": "1557279",
    "end": "1559440"
  },
  {
    "text": "uh documentation is still there is some",
    "start": "1559440",
    "end": "1562320"
  },
  {
    "text": "documentation for spark on cube but i",
    "start": "1562320",
    "end": "1563919"
  },
  {
    "text": "think this is an",
    "start": "1563919",
    "end": "1564640"
  },
  {
    "text": "area where we can once again improve a",
    "start": "1564640",
    "end": "1566880"
  },
  {
    "text": "lot",
    "start": "1566880",
    "end": "1567840"
  },
  {
    "text": "another one is sort of q mechanisms and",
    "start": "1567840",
    "end": "1570320"
  },
  {
    "text": "better understanding of the different",
    "start": "1570320",
    "end": "1571679"
  },
  {
    "text": "kinds of jobs that spark is scheduling",
    "start": "1571679",
    "end": "1574159"
  },
  {
    "text": "and integration with the cube scheduler",
    "start": "1574159",
    "end": "1577120"
  },
  {
    "text": "so that we can actually get faster spin",
    "start": "1577120",
    "end": "1578880"
  },
  {
    "text": "up",
    "start": "1578880",
    "end": "1579919"
  },
  {
    "text": "another one is better communication for",
    "start": "1579919",
    "end": "1581679"
  },
  {
    "text": "failure reasons",
    "start": "1581679",
    "end": "1583679"
  },
  {
    "text": "this is something that i've been working",
    "start": "1583679",
    "end": "1585279"
  },
  {
    "text": "on a little bit um",
    "start": "1585279",
    "end": "1587760"
  },
  {
    "text": "because there's all kinds of reasons why",
    "start": "1587760",
    "end": "1590080"
  },
  {
    "text": "things can fail",
    "start": "1590080",
    "end": "1591440"
  },
  {
    "text": "and it can be difficult for a user to",
    "start": "1591440",
    "end": "1596960"
  },
  {
    "text": "to get at that information it's not",
    "start": "1596960",
    "end": "1598720"
  },
  {
    "text": "populated into the spark web ui",
    "start": "1598720",
    "end": "1600799"
  },
  {
    "text": "and so i think finding ways to better",
    "start": "1600799",
    "end": "1602880"
  },
  {
    "text": "communicate to users what's going on is",
    "start": "1602880",
    "end": "1605279"
  },
  {
    "text": "really important",
    "start": "1605279",
    "end": "1607039"
  },
  {
    "text": "uh dynamic preemption priorities this is",
    "start": "1607039",
    "end": "1610240"
  },
  {
    "text": "this is really complicated in the spark",
    "start": "1610240",
    "end": "1612400"
  },
  {
    "text": "world um and that's because",
    "start": "1612400",
    "end": "1614720"
  },
  {
    "text": "we definitely have this concept of like",
    "start": "1614720",
    "end": "1617440"
  },
  {
    "text": "pods that",
    "start": "1617440",
    "end": "1618480"
  },
  {
    "text": "are maybe more important",
    "start": "1618480",
    "end": "1622000"
  },
  {
    "text": "but the problem is which pods are more",
    "start": "1622000",
    "end": "1623919"
  },
  {
    "text": "important it's going to change",
    "start": "1623919",
    "end": "1625440"
  },
  {
    "text": "a lot while our jobs are running and",
    "start": "1625440",
    "end": "1627840"
  },
  {
    "text": "even as we change resource profiles or",
    "start": "1627840",
    "end": "1630000"
  },
  {
    "text": "even within the same resource profile",
    "start": "1630000",
    "end": "1632400"
  },
  {
    "text": "as data ages in and out",
    "start": "1632400",
    "end": "1635520"
  },
  {
    "text": "and so we don't have a really good way",
    "start": "1635520",
    "end": "1637200"
  },
  {
    "text": "to communicate that to the cube",
    "start": "1637200",
    "end": "1638799"
  },
  {
    "text": "scheduler right now",
    "start": "1638799",
    "end": "1640480"
  },
  {
    "text": "um another one that is kind of maybe i",
    "start": "1640480",
    "end": "1643520"
  },
  {
    "text": "don't know if this is a good idea but i",
    "start": "1643520",
    "end": "1645279"
  },
  {
    "text": "think it's worth exploring because it's",
    "start": "1645279",
    "end": "1646960"
  },
  {
    "text": "it's a sort of hack that we've used",
    "start": "1646960",
    "end": "1649200"
  },
  {
    "text": "uh in spark on different systems",
    "start": "1649200",
    "end": "1652880"
  },
  {
    "text": "and essentially it's where we use a",
    "start": "1652880",
    "end": "1654320"
  },
  {
    "text": "shared local volume when we have",
    "start": "1654320",
    "end": "1655679"
  },
  {
    "text": "multiple executors scheduled on the same",
    "start": "1655679",
    "end": "1657600"
  },
  {
    "text": "node",
    "start": "1657600",
    "end": "1658720"
  },
  {
    "text": "um and this allows us to sort of pass",
    "start": "1658720",
    "end": "1661360"
  },
  {
    "text": "data back and forth",
    "start": "1661360",
    "end": "1662640"
  },
  {
    "text": "without necessarily having to go through",
    "start": "1662640",
    "end": "1664240"
  },
  {
    "text": "the jvm",
    "start": "1664240",
    "end": "1665760"
  },
  {
    "text": "on both sides of it another one is",
    "start": "1665760",
    "end": "1669120"
  },
  {
    "text": "handling of jobs with changing",
    "start": "1669120",
    "end": "1670399"
  },
  {
    "text": "priorities or deadlines",
    "start": "1670399",
    "end": "1671760"
  },
  {
    "text": "so we might have a job which is very low",
    "start": "1671760",
    "end": "1673520"
  },
  {
    "text": "priority but it really does need to",
    "start": "1673520",
    "end": "1675120"
  },
  {
    "text": "complete",
    "start": "1675120",
    "end": "1676159"
  },
  {
    "text": "by the end of the month and so we don't",
    "start": "1676159",
    "end": "1678720"
  },
  {
    "text": "have a good way to express that right",
    "start": "1678720",
    "end": "1680320"
  },
  {
    "text": "now",
    "start": "1680320",
    "end": "1681039"
  },
  {
    "text": "um in fact like a you know a user",
    "start": "1681039",
    "end": "1684399"
  },
  {
    "text": "would actually probably have to cancel",
    "start": "1684399",
    "end": "1685840"
  },
  {
    "text": "and reschedule their job at a higher",
    "start": "1685840",
    "end": "1687360"
  },
  {
    "text": "priority",
    "start": "1687360",
    "end": "1688320"
  },
  {
    "text": "if it was getting close to a deadline i",
    "start": "1688320",
    "end": "1690320"
  },
  {
    "text": "think having a good way to express this",
    "start": "1690320",
    "end": "1693039"
  },
  {
    "text": "um in cube or or in say like",
    "start": "1693039",
    "end": "1696159"
  },
  {
    "text": "volcano or something could be very",
    "start": "1696159",
    "end": "1698320"
  },
  {
    "text": "useful",
    "start": "1698320",
    "end": "1700720"
  },
  {
    "text": "so in conclusion spark on cube",
    "start": "1701200",
    "end": "1703840"
  },
  {
    "text": "migrations they're not",
    "start": "1703840",
    "end": "1705120"
  },
  {
    "text": "something that you can just you know set",
    "start": "1705120",
    "end": "1707360"
  },
  {
    "text": "and forget",
    "start": "1707360",
    "end": "1708640"
  },
  {
    "text": "um and yes we we can prevent spark more",
    "start": "1708640",
    "end": "1711600"
  },
  {
    "text": "than api servers",
    "start": "1711600",
    "end": "1712880"
  },
  {
    "text": "it has less impact on on end users um",
    "start": "1712880",
    "end": "1715840"
  },
  {
    "text": "but there's still a lot of work that we",
    "start": "1715840",
    "end": "1717279"
  },
  {
    "text": "can be doing to make the spark on cube",
    "start": "1717279",
    "end": "1719360"
  },
  {
    "text": "preemption experience better",
    "start": "1719360",
    "end": "1721600"
  },
  {
    "text": "uh the increased isolation does come",
    "start": "1721600",
    "end": "1723679"
  },
  {
    "text": "with some overhead",
    "start": "1723679",
    "end": "1725120"
  },
  {
    "text": "i think it's well worth it for the",
    "start": "1725120",
    "end": "1727840"
  },
  {
    "text": "benefits that we get the security",
    "start": "1727840",
    "end": "1730159"
  },
  {
    "text": "improvements and the additional",
    "start": "1730159",
    "end": "1732000"
  },
  {
    "text": "flexibility around",
    "start": "1732000",
    "end": "1734480"
  },
  {
    "text": "python and native libraries",
    "start": "1734480",
    "end": "1737600"
  },
  {
    "text": "and also we really like cake and dogs",
    "start": "1737600",
    "end": "1739520"
  },
  {
    "text": "you know",
    "start": "1739520",
    "end": "1741279"
  },
  {
    "text": "so thanks for coming we really",
    "start": "1741279",
    "end": "1743360"
  },
  {
    "text": "appreciate it",
    "start": "1743360",
    "end": "1744399"
  },
  {
    "text": "um i hope you are having a wonderful",
    "start": "1744399",
    "end": "1747120"
  },
  {
    "text": "kubecon",
    "start": "1747120",
    "end": "1748240"
  },
  {
    "text": "and please stay safe thank you",
    "start": "1748240",
    "end": "1753600"
  }
]