[
  {
    "start": "0",
    "end": "69000"
  },
  {
    "text": "hey everyone this is kubcon and cloud native con north america 2021 and this",
    "start": "2159",
    "end": "7279"
  },
  {
    "text": "session is about what you need to know before using local persistent volumes on kubernetes",
    "start": "7279",
    "end": "14400"
  },
  {
    "text": "my name is sebastian most people call me seb so you can just do that i'm a software engineer at elastic um",
    "start": "14400",
    "end": "21520"
  },
  {
    "text": "where i mostly work on our kubernetes operator for the elastic stack that we call eck",
    "start": "21520",
    "end": "27599"
  },
  {
    "text": "as in elastic cloud on kubernetes so on the agenda today we'll mostly talk",
    "start": "27599",
    "end": "33600"
  },
  {
    "text": "about local volumes and basically start with explaining how they work and how things",
    "start": "33600",
    "end": "40320"
  },
  {
    "text": "are plugged together in the kubernetes world then we'll mention how you can provision",
    "start": "40320",
    "end": "45680"
  },
  {
    "text": "those local volumes in different ways and finally i think the most important",
    "start": "45680",
    "end": "50719"
  },
  {
    "text": "part of this presentation is going to be the last part about what i call operational gotchas which is",
    "start": "50719",
    "end": "57199"
  },
  {
    "text": "basically a list of things you need to pay attention to to ensure you are using",
    "start": "57199",
    "end": "62719"
  },
  {
    "text": "your local volumes and especially your stateful workloads along with them the right way",
    "start": "62719",
    "end": "69280"
  },
  {
    "start": "69000",
    "end": "417000"
  },
  {
    "text": "so let's get started how do things work what are persistent volumes at all",
    "start": "69760",
    "end": "75360"
  },
  {
    "text": "so usually when we talk about system volumes we associate that concept with the stateful",
    "start": "75360",
    "end": "81600"
  },
  {
    "text": "set concepts which is a way to deploy stateful workloads in kubernetes",
    "start": "81600",
    "end": "86720"
  },
  {
    "text": "so let's take an example here on the left in that pink box i'm deploying a stateful set with three",
    "start": "86720",
    "end": "93040"
  },
  {
    "text": "replicas and let's imagine this is an elasticsearch cluster but it could really be any sort of distributed",
    "start": "93040",
    "end": "99680"
  },
  {
    "text": "database in this example and in the stateful set i'm i'm specifying that i want 100 gig of",
    "start": "99680",
    "end": "106640"
  },
  {
    "text": "storage per pods so the stateful side controller in turn",
    "start": "106640",
    "end": "112079"
  },
  {
    "text": "will create three pods because i i want three replicas so the first part is gonna be with ordinal number zero all you know",
    "start": "112079",
    "end": "119119"
  },
  {
    "text": "number one and then we have another one the last one with ordinal number two and because i requested a claim of 100",
    "start": "119119",
    "end": "126159"
  },
  {
    "text": "gigabytes each of this part is going to be associated to a persistent volume so a",
    "start": "126159",
    "end": "132160"
  },
  {
    "text": "storage unit of 100 gigabytes and the nice part about the stateful set",
    "start": "132160",
    "end": "138640"
  },
  {
    "text": "and the persistent volume concept is that we have a direct direct one-to-one",
    "start": "138640",
    "end": "144160"
  },
  {
    "text": "relationship between the pod and a persistent volume here so for example this part my es cluster 0 is associated",
    "start": "144160",
    "end": "150800"
  },
  {
    "text": "to this system volume called data my es cluster zero",
    "start": "150800",
    "end": "157440"
  },
  {
    "text": "this property is very nice because whenever for example that pod number two",
    "start": "157680",
    "end": "164480"
  },
  {
    "text": "gets deleted either unproposed because you want to upgrade it or either accidentally",
    "start": "164480",
    "end": "170720"
  },
  {
    "text": "then the relationship with its volume stays intact",
    "start": "170720",
    "end": "175840"
  },
  {
    "text": "so here the stateful set controller would just recreate that missing part for the stateful set and that pod will",
    "start": "175840",
    "end": "181760"
  },
  {
    "text": "automatically be associated to the same persistent volume so that's a very nice way to treat",
    "start": "181760",
    "end": "187360"
  },
  {
    "text": "stateful workloads because whenever you need to like for example upgrade the version of that database or do a change",
    "start": "187360",
    "end": "194159"
  },
  {
    "text": "in the configuration you'd basically roll out the pods one by one and when",
    "start": "194159",
    "end": "199680"
  },
  {
    "text": "the restart those pods stay bound to the same volume and stay bound to the same data so there's no",
    "start": "199680",
    "end": "205920"
  },
  {
    "text": "data loss associated to that operation which makes it very nice",
    "start": "205920",
    "end": "212200"
  },
  {
    "text": "so if you get into more details you'll know kubernetes is all about general resources right so let let's take a look",
    "start": "213840",
    "end": "220239"
  },
  {
    "text": "at this specifications so on the left we had our stateful set with three replicas that's what we have",
    "start": "220239",
    "end": "226560"
  },
  {
    "text": "here it has a name my es cluster yes has an elastic such but the game could be anything",
    "start": "226560",
    "end": "232319"
  },
  {
    "text": "and i specify a volume claim template so that each of the pods of the same full set has a",
    "start": "232319",
    "end": "238000"
  },
  {
    "text": "particular persistent volume with here 100 gigabytes",
    "start": "238000",
    "end": "243120"
  },
  {
    "text": "in turn the stateful set controller reads that resource and is going to create for each pod the pod itself of",
    "start": "243120",
    "end": "249120"
  },
  {
    "text": "course along with an associated persistent volume claim so we have the butt the pod",
    "start": "249120",
    "end": "255120"
  },
  {
    "text": "here in the top and the persistent volume claim here at the bottom and really the claim itself is not",
    "start": "255120",
    "end": "261759"
  },
  {
    "text": "really the volume yet it's just it just expresses the desire for the pods to",
    "start": "261759",
    "end": "266840"
  },
  {
    "text": "acquire a storage unit which is going to be the persistent volume so at this point we have a pod we have a",
    "start": "266840",
    "end": "273919"
  },
  {
    "text": "claim and the pod is basically waiting for that claim to be bound to a",
    "start": "273919",
    "end": "279440"
  },
  {
    "text": "real verb and you can see that there's a very nice",
    "start": "279440",
    "end": "284639"
  },
  {
    "text": "naming convention in in in all that system so the stateful set here that is called",
    "start": "284639",
    "end": "290400"
  },
  {
    "text": "my es cluster uh determines the name of the bot so the first part is going to be my es cluster",
    "start": "290400",
    "end": "296320"
  },
  {
    "text": "0 and the persistent volume claim is going to reuse that name of the card he has",
    "start": "296320",
    "end": "301440"
  },
  {
    "text": "cluster 0 that prepared to it's the name of the volume claim that we declared in the statements that is also used in the",
    "start": "301440",
    "end": "307600"
  },
  {
    "text": "in the pod definition itself so we have this this relationship between the naming of all that stuff so",
    "start": "307600",
    "end": "312720"
  },
  {
    "text": "there's some sort of implicit relationship between those",
    "start": "312720",
    "end": "317840"
  },
  {
    "text": "now we have this claim saying i want 100 gigs of storage and what usually happens and it's not",
    "start": "317840",
    "end": "323759"
  },
  {
    "text": "only the case that if you for example create a stateful set or your persistent",
    "start": "323759",
    "end": "329440"
  },
  {
    "text": "volume claim on the cloud provider what's usually going to happen is that a provisioner an",
    "start": "329440",
    "end": "335039"
  },
  {
    "text": "external thing external controller is going to provision a volume that matches the claim",
    "start": "335039",
    "end": "340880"
  },
  {
    "text": "so you want 100 gig volume expressed in that plane and something external the provisioner is going to going to create",
    "start": "340880",
    "end": "347360"
  },
  {
    "text": "that volume for you right and that volume is here for example um",
    "start": "347360",
    "end": "353919"
  },
  {
    "text": "persistent disk in dcp and it's going to be the same size as",
    "start": "353919",
    "end": "359280"
  },
  {
    "text": "the claim you requested so now we have system volume on one hand system volume",
    "start": "359280",
    "end": "365360"
  },
  {
    "text": "claim on the other hand and the volume was created especially for that claim and the next step is for another",
    "start": "365360",
    "end": "371840"
  },
  {
    "text": "controller in kubernetes which is called the pvc controller to bind the volume and the claim together so whenever",
    "start": "371840",
    "end": "378800"
  },
  {
    "text": "there's a claim that controller will look for any available volume that can match that",
    "start": "378800",
    "end": "384160"
  },
  {
    "text": "claim and in our case here in this example the volume was created just to match the claim so it's very easy for",
    "start": "384160",
    "end": "389680"
  },
  {
    "text": "that controller to bind the two together and again here there's a relationship between those two that we can inspect in",
    "start": "389680",
    "end": "396560"
  },
  {
    "text": "the yaml file like here we see that um in the volume we have a reference to a",
    "start": "396560",
    "end": "402160"
  },
  {
    "text": "particular claim so that's the claim um that was bound to that volume so it has the same name here as our claim on the",
    "start": "402160",
    "end": "409039"
  },
  {
    "text": "left and we also see that the claim itself was updated with the name of the volume",
    "start": "409039",
    "end": "414160"
  },
  {
    "text": "to which it is i usually like to",
    "start": "414160",
    "end": "420000"
  },
  {
    "start": "417000",
    "end": "623000"
  },
  {
    "text": "separate two categories of volumes um the first one",
    "start": "420000",
    "end": "425919"
  },
  {
    "text": "which is local volume local volume which is really what all this uh presentation is about",
    "start": "425919",
    "end": "431520"
  },
  {
    "text": "versus the second one which i like to call network attached volumes and usually if you try out persistent",
    "start": "431520",
    "end": "438639"
  },
  {
    "text": "volumes in a cloud provider or with red stateful sets with those volumes you'll usually get the default type of volume",
    "start": "438639",
    "end": "445280"
  },
  {
    "text": "which is pretty often the network attached volumes and there are a lot of different implementation of these network attached volumes",
    "start": "445280",
    "end": "451599"
  },
  {
    "text": "um and that's what you get by default usually depending on your kubernetes provider",
    "start": "451599",
    "end": "456880"
  },
  {
    "text": "so there's always a trade-off between the choice like you can choose whether you want to use network dash volumes or local volumes and there's a big",
    "start": "456880",
    "end": "463199"
  },
  {
    "text": "trade-off and it's a lot of research to do in order to make sure you understand the trade-off here",
    "start": "463199",
    "end": "468879"
  },
  {
    "text": "but basically it all comes down to the performance you need from that volume um",
    "start": "468879",
    "end": "474639"
  },
  {
    "text": "of course if you use a locally attached nvme ssd volume to the vm you're going to get much better performance than if",
    "start": "474639",
    "end": "481840"
  },
  {
    "text": "you mount a file system using the network although the performance of those network dash volumes is getting",
    "start": "481840",
    "end": "487520"
  },
  {
    "text": "better and better it's also possible that the pre the price is very different like you may get",
    "start": "487520",
    "end": "492560"
  },
  {
    "text": "a cheaper volume by using the local hard disk directly rather than paying",
    "start": "492560",
    "end": "497759"
  },
  {
    "text": "for the units of storage you use over the network but again this depends on your provider",
    "start": "497759",
    "end": "504479"
  },
  {
    "text": "another big difference is that um those local volume are usually",
    "start": "504479",
    "end": "509759"
  },
  {
    "text": "not there by default when you deploy kubernetes you probably have to provision those yourself or to install a",
    "start": "509759",
    "end": "515279"
  },
  {
    "text": "provisioner so you get access to those local volumes and that's what we're going to see next but really the main",
    "start": "515279",
    "end": "522080"
  },
  {
    "text": "difference between those two is that a local volume is really bound to a",
    "start": "522080",
    "end": "527519"
  },
  {
    "text": "particular host since it directly matches a physical disk like that is plugged to that",
    "start": "527519",
    "end": "534080"
  },
  {
    "text": "virtual machine um that volume can only exist on that particular host and it's a big",
    "start": "534080",
    "end": "539760"
  },
  {
    "text": "difference because on the other hand when you use network attached volume you could",
    "start": "539760",
    "end": "544959"
  },
  {
    "text": "simply delete your pod your workload and that part could be recreated anywhere with",
    "start": "544959",
    "end": "550320"
  },
  {
    "text": "the same volume because the same volume can be attached over the network again which makes things much simpler for",
    "start": "550320",
    "end": "556880"
  },
  {
    "text": "operations whereas local volumes need to be operated with this constraint",
    "start": "556880",
    "end": "562240"
  },
  {
    "text": "in mind that they are bound to a particular host and this host cannot change for a given body",
    "start": "562240",
    "end": "568880"
  },
  {
    "text": "so how does this this work in practice well it's just using the affinity mechanism of kubernetes the same you",
    "start": "569120",
    "end": "574560"
  },
  {
    "text": "would use for example to stick a particular pod to a particular subset of nodes in your kubernetes cluster",
    "start": "574560",
    "end": "581360"
  },
  {
    "text": "so here for example in this local system volume we see that there's a local path defined so that's really directory on",
    "start": "581360",
    "end": "588080"
  },
  {
    "text": "the file system onto which this volume is mounted and an affinity and here this affinity",
    "start": "588080",
    "end": "594720"
  },
  {
    "text": "setting i'm using a gke cluster here specifies that this volume can only exist on a",
    "start": "594720",
    "end": "602720"
  },
  {
    "text": "host with that particular name here right so whenever the scheduler",
    "start": "602720",
    "end": "608959"
  },
  {
    "text": "schedules a new pod on that pod is associated a persistent volume claim which is stealth is bound to persistent",
    "start": "608959",
    "end": "615920"
  },
  {
    "text": "volume then the pod itself can only be scheduled on the same host as the",
    "start": "615920",
    "end": "621279"
  },
  {
    "text": "persistent volume so how can you make use of those volumes yourself you have to provision them",
    "start": "621279",
    "end": "627120"
  },
  {
    "start": "623000",
    "end": "970000"
  },
  {
    "text": "because they are not likely not just there by default so i think there are in general three",
    "start": "627120",
    "end": "632480"
  },
  {
    "text": "different ways to provision those volumes the first one which i call manual provisioning where you basically want to",
    "start": "632480",
    "end": "639519"
  },
  {
    "text": "create that persistent volume resource yourself like write the yaml file create a resource for that volume",
    "start": "639519",
    "end": "646880"
  },
  {
    "text": "then what i call static provisioning where you run to run a program or an agent on each host that will",
    "start": "646880",
    "end": "653839"
  },
  {
    "text": "automatically discover the disks that are available on a given host and create the corresponding volumes",
    "start": "653839",
    "end": "659920"
  },
  {
    "text": "automatically so for example if you have a virtual machine with three different hard disks attached you could run this",
    "start": "659920",
    "end": "667040"
  },
  {
    "text": "tool to automatically create three different volumes of the same size as the disk",
    "start": "667040",
    "end": "672160"
  },
  {
    "text": "that's static provision i mean in both those cases manual provisioning and static provisioning you",
    "start": "672160",
    "end": "677680"
  },
  {
    "text": "usually create the volumes of the size you want in advance and then the pods and",
    "start": "677680",
    "end": "683040"
  },
  {
    "text": "stateful sets etc you create can make use of those available volumes whereas",
    "start": "683040",
    "end": "688320"
  },
  {
    "text": "the third category dynamic provisioning allows",
    "start": "688320",
    "end": "693360"
  },
  {
    "text": "the controller so it's usually a controller like you run an act an additional controller or operator in",
    "start": "693360",
    "end": "698720"
  },
  {
    "text": "your kubernetes cluster and this controller is responsible for automatically creating the persistent",
    "start": "698720",
    "end": "705200"
  },
  {
    "text": "volumes on demand depending on the claims that were created",
    "start": "705200",
    "end": "710240"
  },
  {
    "text": "so if you if you work with dynamic provisioning you have no volume at first but as soon as you create a workload",
    "start": "710240",
    "end": "716399"
  },
  {
    "text": "that claims for example 100 gigs of storage then this dynamic provisioner is",
    "start": "716399",
    "end": "721440"
  },
  {
    "text": "going to provision and create a volume of 100 gig of storage so really three different ways going",
    "start": "721440",
    "end": "728480"
  },
  {
    "text": "from the simplest from the left to the more complex on the right",
    "start": "728480",
    "end": "733600"
  },
  {
    "text": "so let's take a look at manual provisioning and how you can create a persistent volume yourself well basically it's just a yellow file",
    "start": "734320",
    "end": "740480"
  },
  {
    "text": "like you just create this persistent volume resource you give it a name you specify its capacity and you",
    "start": "740480",
    "end": "746800"
  },
  {
    "text": "sort of indicate the path in the file system that you want to use for that",
    "start": "746800",
    "end": "752000"
  },
  {
    "text": "so you need to be careful here that the capacity you advertise in the spec",
    "start": "752000",
    "end": "758079"
  },
  {
    "text": "technically doesn't have to match the real capacity you have on the file system like you could",
    "start": "758079",
    "end": "763920"
  },
  {
    "text": "very well declare volume of a headrest storage here but behind the scenes that file system",
    "start": "763920",
    "end": "769440"
  },
  {
    "text": "is actually one terabyte of storage and there's nothing that is going to block you from doing that and nothing is",
    "start": "769440",
    "end": "775279"
  },
  {
    "text": "going to prevent you from for example using 500 gigabytes",
    "start": "775279",
    "end": "781200"
  },
  {
    "text": "instead of the advertise 100 gigabytes of storage because that file system behind the scenes is much larger than",
    "start": "781200",
    "end": "787600"
  },
  {
    "text": "that nothing is going to check that size to be careful and of course you want to define the",
    "start": "787600",
    "end": "792720"
  },
  {
    "text": "node affinity to make sure that particular volume belongs to that particular host",
    "start": "792720",
    "end": "798320"
  },
  {
    "text": "and basically that's it just apply that yaml file and create that local volume",
    "start": "798320",
    "end": "805200"
  },
  {
    "text": "and this volume can then be bound to a claim as soon as you create the claim that matches the phone",
    "start": "805200",
    "end": "812920"
  },
  {
    "text": "so as you see that's very much of like a lot of manual work that is required that",
    "start": "814160",
    "end": "819199"
  },
  {
    "text": "you can script of course to create all these volumes in advance but what's maybe smarter to do if you know",
    "start": "819199",
    "end": "825440"
  },
  {
    "text": "that you want to run a particular stateful workload on your kubernetes cluster is to automatically create the",
    "start": "825440",
    "end": "831360"
  },
  {
    "text": "volumes that correspond to the disks you have on each virtual machine",
    "start": "831360",
    "end": "836959"
  },
  {
    "text": "and there's a very useful tool maintained by [Music] the kubernetes special interest group",
    "start": "836959",
    "end": "843519"
  },
  {
    "text": "um the storage special interest group which which is called the local persistent volume static provisioner",
    "start": "843519",
    "end": "850079"
  },
  {
    "text": "and really this is some sort of demon set that turns on your kubernetes cluster that is going to",
    "start": "850079",
    "end": "856079"
  },
  {
    "text": "inspect on each host the partitions and disks that are available on the file system of the host",
    "start": "856079",
    "end": "862639"
  },
  {
    "text": "and for example if that tool sees that there are five disks that it could use it's going to automatically create",
    "start": "862639",
    "end": "868959"
  },
  {
    "text": "five different persistent volumes matching those disks so you basically just install the tool",
    "start": "868959",
    "end": "874639"
  },
  {
    "text": "and after a few seconds you have all the persistent volumes that are available matching the disks you have so it's very",
    "start": "874639",
    "end": "880800"
  },
  {
    "text": "useful if you sized your machines and their disks according to the workload you expect to",
    "start": "880800",
    "end": "886880"
  },
  {
    "text": "have in the future and finally maybe the more advanced way to provision",
    "start": "886880",
    "end": "892959"
  },
  {
    "text": "local volume is to use dynamic provisioning so you have a lot of options there i'm just gonna mention two of them that i",
    "start": "892959",
    "end": "899680"
  },
  {
    "text": "find interesting the first one is the zfs csi driver of",
    "start": "899680",
    "end": "905120"
  },
  {
    "text": "open ebs which allows you to provision volume based on a zfs file system which is",
    "start": "905120",
    "end": "912160"
  },
  {
    "text": "created on demand for the exact size that is required in the claim",
    "start": "912160",
    "end": "918320"
  },
  {
    "text": "and the second one is called toppo lvm is actually very similar in nature",
    "start": "918320",
    "end": "924079"
  },
  {
    "text": "except that instead of provisioning volumes based on zfs it's going to provision volume based on lvm and then",
    "start": "924079",
    "end": "931759"
  },
  {
    "text": "format this volume as you desire so both allow you to",
    "start": "931759",
    "end": "936800"
  },
  {
    "text": "kind of the same thing right and really with this dynamic provisioning it's more about",
    "start": "936800",
    "end": "943360"
  },
  {
    "text": "creating the right volume of the right size automatically whenever a user requests a particular volume of a",
    "start": "943360",
    "end": "949759"
  },
  {
    "text": "specific size so it's much more dynamic and you can do much more advanced things like for",
    "start": "949759",
    "end": "955839"
  },
  {
    "text": "example on a given host you could build a red out of 10 disks and then out of those 10 disks a smaller or maybe",
    "start": "955839",
    "end": "963440"
  },
  {
    "text": "10 smaller volumes are going to be created with different sizes",
    "start": "963440",
    "end": "969720"
  },
  {
    "start": "970000",
    "end": "1380000"
  },
  {
    "text": "all right so now that you know a bit more about what system volumes are what local system volumes are especially and",
    "start": "971680",
    "end": "977440"
  },
  {
    "text": "how you can provision them let's look at how we deal with operations and actually",
    "start": "977440",
    "end": "984639"
  },
  {
    "text": "you'll understand why using local system volumes can be much more difficult to operate than",
    "start": "984639",
    "end": "992079"
  },
  {
    "text": "network attached problems this part of the presentation is",
    "start": "992079",
    "end": "997120"
  },
  {
    "text": "becoming more complicated so i included an animated gifs uh to make it more entertaining",
    "start": "997120",
    "end": "1002959"
  },
  {
    "text": "so we're going to look here at a list of cases and see how the system would behave",
    "start": "1002959",
    "end": "1008000"
  },
  {
    "text": "with local volume so let's take the host failure case so",
    "start": "1008000",
    "end": "1013360"
  },
  {
    "text": "we have the same example here with a stateful set with three replicas and say",
    "start": "1013360",
    "end": "1019519"
  },
  {
    "text": "the host here node c that is holding the workload of second so this stud number",
    "start": "1019519",
    "end": "1024798"
  },
  {
    "text": "two so really the third replica but ordinal number two suddenly becomes pending like the pod",
    "start": "1024799",
    "end": "1031520"
  },
  {
    "text": "cannot be started on that host again and the reason is that that host is dead like",
    "start": "1031520",
    "end": "1036558"
  },
  {
    "text": "it's unavailable it's disconnected from the fleet entirely maybe the virtual machine was like completely recycled and",
    "start": "1036559",
    "end": "1043120"
  },
  {
    "text": "you cannot make use of it anymore let's assume it's completely unrecoverable and the",
    "start": "1043120",
    "end": "1049120"
  },
  {
    "text": "data also itself is completely unrecoverable there's no way you can get it back",
    "start": "1049120",
    "end": "1055039"
  },
  {
    "text": "and bind it to a new boat so usually when you use stateful set for",
    "start": "1055039",
    "end": "1061039"
  },
  {
    "text": "distributed workloads distributed database there's some kind of replication system",
    "start": "1061039",
    "end": "1066640"
  },
  {
    "text": "where the different members of that distributed workload are able to replicate their data all around",
    "start": "1066640",
    "end": "1072960"
  },
  {
    "text": "so here chances are if you configured it properly that all the data that used to be in that",
    "start": "1072960",
    "end": "1078640"
  },
  {
    "text": "volume is also replicated somewhere else on another volume like we can imagine we",
    "start": "1078640",
    "end": "1083919"
  },
  {
    "text": "have several chunks of data there that are replicated to at least another",
    "start": "1083919",
    "end": "1089760"
  },
  {
    "text": "member of the system and that's fine it's very useful it's a property of the application ram and this",
    "start": "1089760",
    "end": "1096480"
  },
  {
    "text": "way you're losing a single member of the salesforce workload doesn't cause the total loss of availability of the data",
    "start": "1096480",
    "end": "1103200"
  },
  {
    "text": "loss of your storage basically so when that happens you'd",
    "start": "1103200",
    "end": "1109280"
  },
  {
    "text": "like the system to sort of recover automatically right that's what kubernetes is supposed to do like say",
    "start": "1109280",
    "end": "1115039"
  },
  {
    "text": "you run a deployment if one pod is killed kubernetes is going to recreate another one automatically",
    "start": "1115039",
    "end": "1121200"
  },
  {
    "text": "but with stateful sets and especially with local volumes it's more complicated you'd really want that part to be",
    "start": "1121200",
    "end": "1126480"
  },
  {
    "text": "recreated automatically on that node d here that's not happening and the reason is",
    "start": "1126480",
    "end": "1132160"
  },
  {
    "text": "that that pod is bound to a volume that is down to a particular host and even though that host is now dead there's",
    "start": "1132160",
    "end": "1140000"
  },
  {
    "text": "nothing that is going to change that relationship so the pod is going to stay pen",
    "start": "1140000",
    "end": "1145840"
  },
  {
    "text": "so you could say well that's easy let's just delete that pod and then kubernetes is going to recreate it again but but it",
    "start": "1147039",
    "end": "1153840"
  },
  {
    "text": "doesn't work because the body is going to be recreated bound to the same claim which itself is bound to the same volume and that volume",
    "start": "1153840",
    "end": "1160640"
  },
  {
    "text": "is bound to particular host so in the end the part is going to be scheduled on that particular host that doesn't exist",
    "start": "1160640",
    "end": "1166240"
  },
  {
    "text": "anymore so it doesn't work what you really have to do here is first",
    "start": "1166240",
    "end": "1171440"
  },
  {
    "text": "delete the claim and then delete the part so there's no pod anymore and there's no claim anymore",
    "start": "1171440",
    "end": "1177919"
  },
  {
    "text": "so the stateful set controller is going to recreate that new pod but it's also going to recreate a new claim and that",
    "start": "1177919",
    "end": "1183679"
  },
  {
    "text": "claim is going to be bound to a different new volume so here that's a really way to start",
    "start": "1183679",
    "end": "1189200"
  },
  {
    "text": "fresh with a new pod with a new volume and then you can rely on your application to replicate and recover the",
    "start": "1189200",
    "end": "1195039"
  },
  {
    "text": "data the way it's supposed to do be careful here that there's a small",
    "start": "1195039",
    "end": "1201200"
  },
  {
    "text": "race condition when you delete pvc on pod in older versions of kubernetes what could happen is that you delete the pvc",
    "start": "1201200",
    "end": "1208480"
  },
  {
    "text": "then you delete the pod the backpop gets recreated before pvc deletion is complete so the pod will still be",
    "start": "1208480",
    "end": "1215360"
  },
  {
    "text": "waiting and pending because it's bound to that pvc that you deleted afterwards so you may need to delete the pod twice",
    "start": "1215360",
    "end": "1222080"
  },
  {
    "text": "actually to make it work again let's fix in the recent version of kubernetes just just upgrade your",
    "start": "1222080",
    "end": "1227520"
  },
  {
    "text": "versions so you see here that kubernetes is not really",
    "start": "1227520",
    "end": "1233520"
  },
  {
    "text": "optimizing for us the way to recover this missing member of our stateful workload",
    "start": "1233520",
    "end": "1240720"
  },
  {
    "text": "even though the primitives out there there's still some sort of manual action required to",
    "start": "1240720",
    "end": "1246080"
  },
  {
    "text": "acknowledge the loss of the persistent volume claim delete the part and let it be recreated normally",
    "start": "1246080",
    "end": "1253360"
  },
  {
    "text": "and of course this can be automated because maybe you don't want to be woken up in the middle of the night because",
    "start": "1253360",
    "end": "1259039"
  },
  {
    "text": "just one host failed out of your 50 nodes cluster and you'd rather want that",
    "start": "1259039",
    "end": "1264480"
  },
  {
    "text": "that pod to be recreated somewhere else and recover its data normally",
    "start": "1264480",
    "end": "1270880"
  },
  {
    "text": "so what you could do here is write a small script so small controller that you deploy in kubernetes and that that controller would basically just watch",
    "start": "1270880",
    "end": "1277360"
  },
  {
    "text": "the kubernetes node resources and whenever there's a change on any of the nodes like you could indicate that one",
    "start": "1277360",
    "end": "1283440"
  },
  {
    "text": "of the node is suddenly unavailable what you could do is list the existing system volume claim and check if one is",
    "start": "1283440",
    "end": "1289679"
  },
  {
    "text": "on one of those failed dead mode and if that's the case then you just remove that volume remove the pod and",
    "start": "1289679",
    "end": "1297360"
  },
  {
    "text": "both are going to be recreated somewhere else automatically and this can be made automatically",
    "start": "1297360",
    "end": "1304559"
  },
  {
    "text": "you should note that here it's important to distinguish between a host that would be completely dead and unrecoverable as",
    "start": "1304559",
    "end": "1311919"
  },
  {
    "text": "in there's no chance ever that the same pod is going to run with the same data on that host it's over it's done not",
    "start": "1311919",
    "end": "1318559"
  },
  {
    "text": "possible in which case you really want to recreate it somewhere else versus a case",
    "start": "1318559",
    "end": "1323600"
  },
  {
    "text": "where there's a chance you can recover the data and you can recreate that third on the host like say for example",
    "start": "1323600",
    "end": "1330159"
  },
  {
    "text": "someone just restarted the virtual machine and after a few seconds we know that the virtual machine is going to",
    "start": "1330159",
    "end": "1335440"
  },
  {
    "text": "come back so that part is going to be started again with the same volume that is still there so you really need to",
    "start": "1335440",
    "end": "1341600"
  },
  {
    "text": "differentiate those two cases and i think one useful way to do that is to consider that if the node is",
    "start": "1341600",
    "end": "1348960"
  },
  {
    "text": "completely out of the fleet you might as well set up a process to remove that node",
    "start": "1348960",
    "end": "1354880"
  },
  {
    "text": "resource from kubernetes so that small automation here we just have to check whether the node exists or not in the",
    "start": "1354880",
    "end": "1360880"
  },
  {
    "text": "api and if it doesn't then you know you can safely remove pvc on parts",
    "start": "1360880",
    "end": "1366159"
  },
  {
    "text": "however if you just for example check the healthness the",
    "start": "1366159",
    "end": "1371200"
  },
  {
    "text": "health of that bug then chances are maybe it's going to come back later so you need to be really",
    "start": "1371200",
    "end": "1376559"
  },
  {
    "text": "careful about distinguishing those two cases",
    "start": "1376559",
    "end": "1380720"
  },
  {
    "text": "let's take another example where we don't have a certain failure of one",
    "start": "1381840",
    "end": "1387679"
  },
  {
    "text": "host but rather we know we want to take out one particular host away from the fleet",
    "start": "1387679",
    "end": "1393360"
  },
  {
    "text": "and a good use case for that is kuneti's version of greens what people tend to do because it's simpler is to treat all the kunes nodes",
    "start": "1393360",
    "end": "1400960"
  },
  {
    "text": "as immutable so whenever you want to upgrade the kubernetes version rather than",
    "start": "1400960",
    "end": "1406640"
  },
  {
    "text": "upgrading the kubelet in place on the same machine you'd rather spin up a new virtual",
    "start": "1406640",
    "end": "1412400"
  },
  {
    "text": "machine and decommission the old virtual machine right so the way people usually do that is",
    "start": "1412400",
    "end": "1418960"
  },
  {
    "text": "spin up that new vm drain the node that you don't want anymore",
    "start": "1418960",
    "end": "1424080"
  },
  {
    "text": "drain really translates to marking that node as unschedulable so no new pod can be scheduled on it but also it",
    "start": "1424080",
    "end": "1431200"
  },
  {
    "text": "translates to deleting all the parts that are living on that node so that kubernetes can reschedule those spots",
    "start": "1431200",
    "end": "1437600"
  },
  {
    "text": "elsewhere an interesting property and feature of kubernetes is to combine",
    "start": "1437600",
    "end": "1445200"
  },
  {
    "text": "this drain concept with the pot disruption budget so pod disruption budget is another kubernetes resource",
    "start": "1445200",
    "end": "1451200"
  },
  {
    "text": "that you create in advance where you specify how many parts of the same workload you allow to be taken down",
    "start": "1451200",
    "end": "1458159"
  },
  {
    "text": "at the same time for example here if we have an elastic such cluster with three pods",
    "start": "1458159",
    "end": "1463520"
  },
  {
    "text": "it can be very useful to create a pot disruption budget where you would specify that you only",
    "start": "1463520",
    "end": "1468960"
  },
  {
    "text": "allow one pod to be taken out of the fleet at one time so you're sure that",
    "start": "1468960",
    "end": "1474159"
  },
  {
    "text": "that drain command is not going to delete two pods at once of the same elastic such questions right",
    "start": "1474159",
    "end": "1480960"
  },
  {
    "text": "so once that's done once you have no more pod running on that host you would have again as we've seen before to",
    "start": "1481440",
    "end": "1488000"
  },
  {
    "text": "delete both the pvc and the pods so the pod was already ability but you also need to do the pvc to make sure",
    "start": "1488000",
    "end": "1494640"
  },
  {
    "text": "that this pod can be recreated somewhere else with a new fresh volume and then relying on the replication",
    "start": "1494640",
    "end": "1502080"
  },
  {
    "text": "achieved by the application itself you sort of recover your workload and you didn't lose availability and you didn't",
    "start": "1502080",
    "end": "1508559"
  },
  {
    "text": "lose data so that's what we have here with this this red box representing a node that is",
    "start": "1508559",
    "end": "1516000"
  },
  {
    "text": "being trained and before that we replace that node with a",
    "start": "1516000",
    "end": "1522080"
  },
  {
    "text": "new node using the nucleus version now unfortunately things are not that",
    "start": "1522080",
    "end": "1529200"
  },
  {
    "start": "1525000",
    "end": "1663000"
  },
  {
    "text": "easy so first we've seen that someone something needs to be the pvcs because that's not done automatically",
    "start": "1529200",
    "end": "1535440"
  },
  {
    "text": "right and usually if you enable the automatic version upgrade of",
    "start": "1535440",
    "end": "1540559"
  },
  {
    "text": "cloud providers they're going to do the drain mechanism automatically without",
    "start": "1540559",
    "end": "1545840"
  },
  {
    "text": "you uh required to do something on the computer but that that mechanism is definitely not going to be the persistent volume",
    "start": "1545840",
    "end": "1552240"
  },
  {
    "text": "claim so this is something you have to plan for the other important thing and important",
    "start": "1552240",
    "end": "1557360"
  },
  {
    "text": "limitation of this automated version of grades is that it's very common for the pod",
    "start": "1557360",
    "end": "1562400"
  },
  {
    "text": "disruption budget to be respected for a very short amount of time so short being something like 60 minutes",
    "start": "1562400",
    "end": "1568960"
  },
  {
    "text": "in gke for example so you may think 60 minutes is big enough for most workloads but say",
    "start": "1568960",
    "end": "1576240"
  },
  {
    "text": "you have an elasticsearch cluster and each node in that cluster is holding maybe 5 or 10 terabytes of data",
    "start": "1576240",
    "end": "1583279"
  },
  {
    "text": "it's possible that 60 minutes is not enough to migrate all that data over the network to the other parts right",
    "start": "1583279",
    "end": "1591200"
  },
  {
    "text": "and because that pod disruption budget is not respected anymore after this this hour this 60 minutes then",
    "start": "1591200",
    "end": "1598240"
  },
  {
    "text": "the upgrade system is going to move on to the next part and this is where you can lose availability of the data or",
    "start": "1598240",
    "end": "1604720"
  },
  {
    "text": "even worse use the data entirely because the application may not have enough time to",
    "start": "1604720",
    "end": "1610240"
  },
  {
    "text": "recover from the deleted pods we call the data like replicate the data from the other",
    "start": "1610240",
    "end": "1615760"
  },
  {
    "text": "members to the new member in the system that's very dangerous",
    "start": "1615760",
    "end": "1620960"
  },
  {
    "text": "so what you should rather do instead and the safest option would be to manually create this",
    "start": "1620960",
    "end": "1626320"
  },
  {
    "text": "new node with the new kubernetes version and make sure that you migrate your workload",
    "start": "1626320",
    "end": "1632559"
  },
  {
    "text": "here right so you trigger data migration like change the affinity rules for the new",
    "start": "1632559",
    "end": "1637760"
  },
  {
    "text": "pods to leave on that new node pool and then finally since there should be no pod anymore on",
    "start": "1637760",
    "end": "1644000"
  },
  {
    "text": "the old node put you would delete the older pool right that's much more custom and really when you use local volumes",
    "start": "1644000",
    "end": "1651360"
  },
  {
    "text": "with a lot and a lot of data chances are those automated kubernetes cluster upgrades from cloud providers won't be",
    "start": "1651360",
    "end": "1658880"
  },
  {
    "text": "good for you",
    "start": "1658880",
    "end": "1661440"
  },
  {
    "start": "1663000",
    "end": "1833000"
  },
  {
    "text": "so the problem with this with this mechanism is that it really relies on the fact that you",
    "start": "1665600",
    "end": "1671360"
  },
  {
    "text": "delete one put at a time and then let this thought be recreated fresh with a new volume somewhere else and rely on",
    "start": "1671360",
    "end": "1678799"
  },
  {
    "text": "the application itself to replicate the data there but this is some sort of planned",
    "start": "1678799",
    "end": "1684320"
  },
  {
    "text": "maintenance right like you know you're going to remove that part and it's a bit sad to sort of rely on the mechanism to",
    "start": "1684320",
    "end": "1690799"
  },
  {
    "text": "of of failover of the data like of recovery of the data for something you know is going to happen",
    "start": "1690799",
    "end": "1696480"
  },
  {
    "text": "and even worse what if at the same time with this planned maintenance you get another unplanned disruption like",
    "start": "1696480",
    "end": "1702080"
  },
  {
    "text": "another host dies for example suddenly you have two pots that are taken down at once and this is where you could get",
    "start": "1702080",
    "end": "1708159"
  },
  {
    "text": "potential availabilities loss of data which could be dangerous",
    "start": "1708159",
    "end": "1713760"
  },
  {
    "text": "and actually we'd be in a better place if we could just instead of deleting the pods and",
    "start": "1713760",
    "end": "1719200"
  },
  {
    "text": "recreating it somewhere else afterwards first create an additional replacing pod",
    "start": "1719200",
    "end": "1725440"
  },
  {
    "text": "migrate the data there and then remove the old boards but unfortunately that's very hard to do",
    "start": "1725440",
    "end": "1731279"
  },
  {
    "text": "with station cells because stateful sets have this concept where they use an ordinal for each problem here if you",
    "start": "1731279",
    "end": "1736799"
  },
  {
    "text": "have pod zero and pod one if we want to get rid of pod zero what maybe we'd like to do is create pod",
    "start": "1736799",
    "end": "1743840"
  },
  {
    "text": "number two migrate the data there and then delete pod number zero and that's it and we're left with part number one",
    "start": "1743840",
    "end": "1749840"
  },
  {
    "text": "and two but in practice that's not possible stateful sets don't work like that they work the opposite way they work",
    "start": "1749840",
    "end": "1756320"
  },
  {
    "text": "with the assumption that we delete the product and then recreate that rather than",
    "start": "1756320",
    "end": "1762000"
  },
  {
    "text": "what is closer to the deployment resource where you do a rolling upgrade by creating new pods first and then",
    "start": "1762000",
    "end": "1769600"
  },
  {
    "text": "progressively deleting the old parts so i haven't found a good solution to that problem yet with the stateful sets",
    "start": "1769600",
    "end": "1775840"
  },
  {
    "text": "like some people manage pots directly some people just can't do that so",
    "start": "1775840",
    "end": "1782880"
  },
  {
    "text": "another thing you could do is increase the number of replicas so here we suddenly have three replicas which is",
    "start": "1782880",
    "end": "1788159"
  },
  {
    "text": "nicer and then we remove that.0 but then it's going to be recreated so if we want to",
    "start": "1788159",
    "end": "1793440"
  },
  {
    "text": "get back to zero replica we have to like scale down to two rep sorry two replicas",
    "start": "1793440",
    "end": "1799279"
  },
  {
    "text": "to delete that pod again so we end up with some sort of double data migration where we first migrate the data to that",
    "start": "1799279",
    "end": "1804640"
  },
  {
    "text": "new pod and then once the old bodies is gone and recreated we migrate back so",
    "start": "1804640",
    "end": "1809919"
  },
  {
    "text": "that's double data migration that's not very useful no good solutions to that there's an",
    "start": "1809919",
    "end": "1815760"
  },
  {
    "text": "interesting project from the open cruise team that is called the clone set an advan advanced stateful",
    "start": "1815760",
    "end": "1821840"
  },
  {
    "text": "sets as well that sort of um give more features the stateful set with the new cld to do things like that but nothing",
    "start": "1821840",
    "end": "1829520"
  },
  {
    "text": "built in the stateful set is important",
    "start": "1829520",
    "end": "1833440"
  },
  {
    "start": "1833000",
    "end": "1976000"
  },
  {
    "text": "another thing i wanted to warn you about is about what i call concurrent stateful setup grades and concurrent pod",
    "start": "1834720",
    "end": "1840960"
  },
  {
    "text": "scheduling at the same time so here say we have a stateful set with two buds",
    "start": "1840960",
    "end": "1846320"
  },
  {
    "text": "what could happen is that you do an upgrade to the stack of that stateful set and when that happens pods are",
    "start": "1846320",
    "end": "1852720"
  },
  {
    "text": "deleted then recreated with a new spec one by one and they they rely on the same volume so you preserve the data",
    "start": "1852720",
    "end": "1859760"
  },
  {
    "text": "so first the pad is deleted then it's recreated on the same host because there's still this constraint",
    "start": "1859760",
    "end": "1865120"
  },
  {
    "text": "constrained that it needs to be bound to the same volume but what if",
    "start": "1865120",
    "end": "1870240"
  },
  {
    "text": "in the middle of these two operations that are not atomic another pod gets scheduled on that empty",
    "start": "1870240",
    "end": "1876640"
  },
  {
    "text": "slot here and that can really happen like say here this host has 16 gigs of ram and there's",
    "start": "1876640",
    "end": "1884320"
  },
  {
    "text": "a new part that gets created in the meantime with two gigs of ram and then when the stateful set controller tries",
    "start": "1884320",
    "end": "1889919"
  },
  {
    "text": "to recreate that number one that has been updated that pod cannot be scheduled on the same",
    "start": "1889919",
    "end": "1895279"
  },
  {
    "text": "post because something else took the empty space and then you're in sort of really bad",
    "start": "1895279",
    "end": "1900720"
  },
  {
    "text": "situation where you just wanted to upgrade one stateful set but one of the pot stays pending",
    "start": "1900720",
    "end": "1907440"
  },
  {
    "text": "so there are several ways you can deal with that um the first thing is to give higher priority to the pods that are of local",
    "start": "1907440",
    "end": "1914159"
  },
  {
    "text": "volume so you can use this priority class name thing you can also work with stains and",
    "start": "1914159",
    "end": "1919279"
  },
  {
    "text": "toleration to isolate workloads with local volumes to a particular subset of nodes so you know",
    "start": "1919279",
    "end": "1925519"
  },
  {
    "text": "other parts are not going to be scheduled there and finally what i think is the probably",
    "start": "1925519",
    "end": "1930559"
  },
  {
    "text": "the simplest solution to this hole is to sort of plan for your workload in advance so either you know a node is",
    "start": "1930559",
    "end": "1936799"
  },
  {
    "text": "going to be completely dedicated to a particular pod with a single local volume either that host can hold several pods",
    "start": "1936799",
    "end": "1944559"
  },
  {
    "text": "with local volumes but then you use a fixed ratio between ram and storage so that you know that for example",
    "start": "1944559",
    "end": "1952159"
  },
  {
    "text": "if all the storage is occupied then no new pod can be scheduled because the",
    "start": "1952159",
    "end": "1958480"
  },
  {
    "text": "ram is sort of reserved for that storage right you can schedule a pod with 10",
    "start": "1958480",
    "end": "1963840"
  },
  {
    "text": "gigs of ram while another one is being recreated because there's not the available",
    "start": "1963840",
    "end": "1969600"
  },
  {
    "text": "storage to schedule something with 10 gigs of ram and that's a way to avoid that problem",
    "start": "1969600",
    "end": "1974640"
  },
  {
    "text": "all together one other important thing is the",
    "start": "1974640",
    "end": "1980159"
  },
  {
    "start": "1976000",
    "end": "2089000"
  },
  {
    "text": "awareness of storage capacity in kubernetes and so when you use dynamic provisioning for",
    "start": "1980159",
    "end": "1985519"
  },
  {
    "text": "example it's possible that a pod gets scheduled onto a node that doesn't have the necessary storage space available",
    "start": "1985519",
    "end": "1993760"
  },
  {
    "text": "for computers has been fixed starting kundalinis 1.21 right",
    "start": "1993760",
    "end": "1998799"
  },
  {
    "text": "and before that version one way to deal with that again is to have this fixed ratio between ram and storage so",
    "start": "1998799",
    "end": "2005039"
  },
  {
    "text": "you know that if the bot can be scheduled because of its ram it means the storage is also available",
    "start": "2005039",
    "end": "2011600"
  },
  {
    "text": "and finally another maybe concern is that",
    "start": "2011600",
    "end": "2016840"
  },
  {
    "text": "um there's no enforcement that you get the exact volume of the size you",
    "start": "2016840",
    "end": "2024000"
  },
  {
    "text": "requested kubernetes is going to do its best trying to match the claim size with",
    "start": "2024000",
    "end": "2029200"
  },
  {
    "text": "the volume size that it can happen that for example you you request one gigabyte claim and the",
    "start": "2029200",
    "end": "2035840"
  },
  {
    "text": "only available volume is one terabyte and kubernetes is going to do the match here and a larger problem is that the",
    "start": "2035840",
    "end": "2042159"
  },
  {
    "text": "scheduler sort of ignores this priority over the closest possible size of the",
    "start": "2042159",
    "end": "2048000"
  },
  {
    "text": "volume across the different nodes so the scheduler could pick a node but it's not",
    "start": "2048000",
    "end": "2053358"
  },
  {
    "text": "the best pick because there are other nodes with closest volume size available",
    "start": "2053359",
    "end": "2059760"
  },
  {
    "text": "and again this is going to be fixed in new versions of kubernetes starting alpha in 1.21",
    "start": "2059760",
    "end": "2066720"
  },
  {
    "text": "all right i hope that with this presentation you got a bit more intimate knowledge of how local volumes work and",
    "start": "2066879",
    "end": "2073040"
  },
  {
    "text": "especially the things you need to pay attention to if you decide to use them in production",
    "start": "2073040",
    "end": "2078398"
  },
  {
    "text": "and likely those involve a bit more work and a bit more thinking than what you'd have to do if you were using network",
    "start": "2078399",
    "end": "2084800"
  },
  {
    "text": "attachments thank you for attending",
    "start": "2084800",
    "end": "2089839"
  }
]