[
  {
    "start": "0",
    "end": "44000"
  },
  {
    "text": "hello everybody and welcome to my talk my session how not to start with kubernetes",
    "start": "240",
    "end": "6319"
  },
  {
    "text": "my name is kristen hegelmann and i want to share with you within the next 30 minutes the lessons i've learned running",
    "start": "6319",
    "end": "12480"
  },
  {
    "text": "kubernetes on-prem so on your own data center and it might be a little bit boring for",
    "start": "12480",
    "end": "18320"
  },
  {
    "text": "you if you're already running kubernetes because all the things i will mention are basically beginner",
    "start": "18320",
    "end": "24240"
  },
  {
    "text": "yeah failures right um so i've splitted this presentation in",
    "start": "24240",
    "end": "30960"
  },
  {
    "text": "two parts operation and infrastructure as well as development and deployment stuff",
    "start": "30960",
    "end": "36719"
  },
  {
    "text": "but let's get started",
    "start": "36719",
    "end": "40239"
  },
  {
    "text": "so back in 2018 i was working for another company which was",
    "start": "43760",
    "end": "49840"
  },
  {
    "start": "44000",
    "end": "44000"
  },
  {
    "text": "yeah highly regulated and one of our architects reached out to",
    "start": "49840",
    "end": "54879"
  },
  {
    "text": "us to the operations team where i was a system engineer and said hey we want to use",
    "start": "54879",
    "end": "60079"
  },
  {
    "text": "containers we want to use container orchestration and if we can help him",
    "start": "60079",
    "end": "65280"
  },
  {
    "text": "yeah running a small poc and the poc was",
    "start": "65280",
    "end": "70840"
  },
  {
    "text": "kubernetes against not against but kubernetes and docker swamp we want to compare",
    "start": "70840",
    "end": "76479"
  },
  {
    "text": "both of them both solutions and as i mentioned we need to run on-prem in our case it was",
    "start": "76479",
    "end": "82640"
  },
  {
    "text": "based on centos on vmware and xen and it was kubernetes version 1.9 we",
    "start": "82640",
    "end": "89920"
  },
  {
    "text": "started with and there's already the first little mistake",
    "start": "89920",
    "end": "96320"
  },
  {
    "text": "i made or we made so it was a small poc with a small group of",
    "start": "96320",
    "end": "102000"
  },
  {
    "text": "people but when you are building a platform or starting using a platform like",
    "start": "102000",
    "end": "107439"
  },
  {
    "text": "kubernetes in your company then you should onboard all other departments as well from the beginning",
    "start": "107439",
    "end": "114799"
  },
  {
    "text": "get involved with security get involved with the data center guys like storage",
    "start": "114799",
    "end": "120719"
  },
  {
    "text": "and networking guys right because as mentioned you're building a little",
    "start": "120719",
    "end": "126000"
  },
  {
    "text": "data center within the data center and the platform will be used by by all of your your",
    "start": "126000",
    "end": "132800"
  },
  {
    "text": "developers all of your operation guys so do it in a little bigger scope than you",
    "start": "132800",
    "end": "140840"
  },
  {
    "text": "expect and yeah that's the key takeaway here from my perspective and also if you",
    "start": "140840",
    "end": "148000"
  },
  {
    "text": "have no idea or no experience running kubernetes then perhaps you should get external help",
    "start": "148000",
    "end": "154319"
  },
  {
    "text": "there are companies out there they will take your money and and say what you should do here right",
    "start": "154319",
    "end": "161280"
  },
  {
    "text": "so but let's first start with infrastructure and operation topics so as i said",
    "start": "161280",
    "end": "167360"
  },
  {
    "start": "166000",
    "end": "166000"
  },
  {
    "text": "our small poc was really um just in my case a three note kubernetes cluster so i",
    "start": "167360",
    "end": "174319"
  },
  {
    "text": "provisioned three vms i started installing kubernetes using cube adm manually with sh into the boxes running",
    "start": "174319",
    "end": "182560"
  },
  {
    "text": "all the commands right and it was working it was working fine for the first couple of weeks",
    "start": "182560",
    "end": "188480"
  },
  {
    "text": "but then i upgraded to kubernetes 1.10 this was also still possible and we hadn't had",
    "start": "188480",
    "end": "194800"
  },
  {
    "text": "any storage provision or ingress controllers on this little poc cluster from the beginning so it was just plain",
    "start": "194800",
    "end": "201760"
  },
  {
    "text": "kubernetes and some web applications deployed into it",
    "start": "201760",
    "end": "207440"
  },
  {
    "text": "but later on when i wanted to upgrade to kubernetes 111 yeah my upgrade failed and the cluster",
    "start": "207440",
    "end": "213760"
  },
  {
    "text": "was in a non-recoverable state so yeah this was a little bit",
    "start": "213760",
    "end": "220000"
  },
  {
    "text": "not the best thing right but and perhaps i could fix it now with my",
    "start": "220000",
    "end": "225280"
  },
  {
    "text": "knowledge what i have but back in the days the cluster was just that right so",
    "start": "225280",
    "end": "232400"
  },
  {
    "text": "i switched to rancher and not installing kubernetes manually anymore",
    "start": "232400",
    "end": "238159"
  },
  {
    "text": "and rancher is quite nice it provides a nice ui you don't have to think about",
    "start": "238159",
    "end": "243200"
  },
  {
    "text": "authentication you can connect it to your id and so on and it's quite easy to deploy clusters",
    "start": "243200",
    "end": "250239"
  },
  {
    "text": "using venture but there are other options in vmware if you're running completely entirely on",
    "start": "250239",
    "end": "255680"
  },
  {
    "text": "vmware you can use vmware tanzu you can use cubespray there might be a lot of other provisioners out",
    "start": "255680",
    "end": "262079"
  },
  {
    "text": "there which you can utilize but as key takeaway don't do manually installations which",
    "start": "262079",
    "end": "269680"
  },
  {
    "text": "leads me to my next slide automation everywhere so try to automate from the beginning with your first cluster",
    "start": "269680",
    "end": "276479"
  },
  {
    "start": "271000",
    "end": "271000"
  },
  {
    "text": "right to have a small poc in place yeah it might be better just to to",
    "start": "276479",
    "end": "282720"
  },
  {
    "text": "install it manually or using the ui of the different different tools",
    "start": "282720",
    "end": "288800"
  },
  {
    "text": "but if you need to deploy multiple clusters or you have to provision clusters automatically",
    "start": "288800",
    "end": "295600"
  },
  {
    "text": "to spin up a cluster for for development group which will then be destroyed after",
    "start": "295600",
    "end": "300880"
  },
  {
    "text": "then everything should be automated from the beginning right so",
    "start": "300880",
    "end": "306560"
  },
  {
    "text": "take or invest the time in automation heaven cisd process in place to to",
    "start": "306560",
    "end": "312320"
  },
  {
    "text": "automatically provision clusters and document as well from the beginning so",
    "start": "312320",
    "end": "319120"
  },
  {
    "text": "don't skip this tab document everywhere so people are",
    "start": "319120",
    "end": "325520"
  },
  {
    "text": "able to deploy clusters even without your help right so next",
    "start": "325520",
    "end": "331759"
  },
  {
    "text": "a little bit more into details with networking so networks in iron configuration",
    "start": "331759",
    "end": "337600"
  },
  {
    "start": "334000",
    "end": "334000"
  },
  {
    "text": "there are plenty of network scene eyes available these are only listed a few of them like calico flannel",
    "start": "337600",
    "end": "343680"
  },
  {
    "text": "weave canals psyllium and so the network scene i is a really",
    "start": "343680",
    "end": "350400"
  },
  {
    "text": "crucial thing you need to to think about before you're starting or starting using kubernetes which",
    "start": "350400",
    "end": "356319"
  },
  {
    "text": "network scene i should you use and as well which side arrange for instance for your port and service network you want",
    "start": "356319",
    "end": "362560"
  },
  {
    "text": "to use so in my case i had here a little bit of an issue",
    "start": "362560",
    "end": "367840"
  },
  {
    "text": "so when provisioning clusters with rancho rancher will",
    "start": "367840",
    "end": "373440"
  },
  {
    "text": "have the default subnet range for port and service network 1043 and 10 42.",
    "start": "373440",
    "end": "381759"
  },
  {
    "text": "yeah but this was colliding with one of our offices and yeah the cluster was running fine and",
    "start": "381840",
    "end": "388960"
  },
  {
    "text": "then when people from the office actually want to access services on the clusters they couldn't because as i said",
    "start": "388960",
    "end": "394560"
  },
  {
    "text": "we had here a little bit on open overlapping i was able to mitigate this problem by",
    "start": "394560",
    "end": "399600"
  },
  {
    "text": "adding the five full reverse proxy in front of the workload but still",
    "start": "399600",
    "end": "404960"
  },
  {
    "text": "think about stuff like that when you're installing your network scene file right another thing i stumbled across was",
    "start": "404960",
    "end": "411520"
  },
  {
    "text": "network policies so in my first cluster was provisioning was basically",
    "start": "411520",
    "end": "418000"
  },
  {
    "text": "in the example flannel listed it as a scene i provided right",
    "start": "418000",
    "end": "423039"
  },
  {
    "text": "and it was working great it was working fine and i also used flannel then for my first cluster i provisioned in rancho",
    "start": "423039",
    "end": "430000"
  },
  {
    "text": "but flannel is not able to to utilize network policies or enforce network policies",
    "start": "430000",
    "end": "436639"
  },
  {
    "text": "so one day i wanted to isolate network traffic from one of my name spaces and it wasn't",
    "start": "436639",
    "end": "443440"
  },
  {
    "text": "yeah i wasn't able to do this and also ranger wasn't able to change the network scene i after the cluster creation you",
    "start": "443440",
    "end": "450160"
  },
  {
    "text": "can do this yeah with with other provisioners or or",
    "start": "450160",
    "end": "455440"
  },
  {
    "text": "with a vanilla cluster for instance but with rancher we were stuck with flannel at least on this one",
    "start": "455440",
    "end": "461599"
  },
  {
    "text": "development cluster then you need to think about do i want to encrypt data and transit and not using in service mesh for instance",
    "start": "461599",
    "end": "469039"
  },
  {
    "text": "then you can run i think kalaiko is able to offer this and weavenet um as well",
    "start": "469039",
    "end": "475039"
  },
  {
    "text": "and know at least the basics like like dean as how dean s is being done in kubernetes right and how to resolve",
    "start": "475039",
    "end": "482240"
  },
  {
    "text": "service names within kubernetes that you're not going if you i've seen workload configured using",
    "start": "482240",
    "end": "489199"
  },
  {
    "text": "or wanted to request yeah a service next running in the same namespace but going through the entire",
    "start": "489199",
    "end": "496639"
  },
  {
    "text": "ingress controller to to reach the service instead of just calling the service name inside of the cluster",
    "start": "496639",
    "end": "502479"
  },
  {
    "text": "right but speaking of ingresses",
    "start": "502479",
    "end": "508080"
  },
  {
    "start": "508000",
    "end": "508000"
  },
  {
    "text": "that was the next question after my first poc cluster so how we could reach our services inside of the cluster and",
    "start": "508080",
    "end": "515839"
  },
  {
    "text": "yeah i i was also i had no experience right so so i was reading through",
    "start": "515839",
    "end": "520880"
  },
  {
    "text": "documentations or examples and i ended up with deploying a metal lb",
    "start": "520880",
    "end": "527839"
  },
  {
    "text": "inside of our cluster and using traffic as an ingress controller i had a single traffic increase controller running",
    "start": "527839",
    "end": "534800"
  },
  {
    "text": "and we splitted our traffic for external internal using allow us",
    "start": "534800",
    "end": "541839"
  },
  {
    "text": "but this is error prone so a developer could forget to add an allow list to its",
    "start": "541839",
    "end": "547360"
  },
  {
    "text": "implementation and that perhaps your ingress will be exposed to the internet by accident right",
    "start": "547360",
    "end": "553279"
  },
  {
    "text": "so perhaps it would be better here to have separate ingress controller for for",
    "start": "553279",
    "end": "558480"
  },
  {
    "text": "external and internal workloads and also in our case perhaps it would be a better idea to put our existing f5 in front of",
    "start": "558480",
    "end": "566800"
  },
  {
    "text": "the of the ingress controller instead using metallic reusing the stuff you already own right or you already",
    "start": "566800",
    "end": "574080"
  },
  {
    "text": "have in your your environment and then you have to think about",
    "start": "574080",
    "end": "579200"
  },
  {
    "text": "security do i want to add network uh web application firewall in front",
    "start": "579200",
    "end": "584240"
  },
  {
    "text": "on the load balancer level in aws you can enable and valve on your load balancer",
    "start": "584240",
    "end": "589680"
  },
  {
    "text": "i could reuse my f5 valve if i would use f5 for my for my load balancer right",
    "start": "589680",
    "end": "598160"
  },
  {
    "text": "you can use on ingress level like in engine x you can use smart",
    "start": "598959",
    "end": "604000"
  },
  {
    "text": "security for instance right so think about security in advance",
    "start": "604000",
    "end": "610480"
  },
  {
    "text": "as well as how do we want to manage sl certificates a very convenient way is using search manager with let's encrypt",
    "start": "610480",
    "end": "616079"
  },
  {
    "text": "but you don't want to end up with yeah deploying certificates manually with cuba city apply and taking care of",
    "start": "616079",
    "end": "623480"
  },
  {
    "text": "reviewing your certificates after two years right so so you will forget it give me",
    "start": "623480",
    "end": "630640"
  },
  {
    "text": "trust me you will forget sometimes to renew your certificates and specific namespace or",
    "start": "630640",
    "end": "636320"
  },
  {
    "text": "whatever and then you probably will have a short outage",
    "start": "636320",
    "end": "641600"
  },
  {
    "text": "right so the next thing i want to to cover here um so developer reached out to us",
    "start": "641600",
    "end": "649360"
  },
  {
    "start": "644000",
    "end": "644000"
  },
  {
    "text": "out to me and said hey we need to store some files in our cluster and our workloads and i said uh okay",
    "start": "649360",
    "end": "656079"
  },
  {
    "text": "i have no idea i need to to do some research here what",
    "start": "656079",
    "end": "661120"
  },
  {
    "text": "what i need to do and he said yeah i already found something in the internet",
    "start": "661120",
    "end": "666240"
  },
  {
    "text": "and um yeah it was basically in a manual how to install a kt with",
    "start": "666240",
    "end": "673040"
  },
  {
    "text": "cluster and fs inside of your cluster and we were yeah just following the",
    "start": "673040",
    "end": "679680"
  },
  {
    "text": "guides following the instructions but what was happening after that every",
    "start": "679680",
    "end": "687839"
  },
  {
    "text": "time i was upgrading kubernetes the version and the cubelet service was restarted on the node",
    "start": "687839",
    "end": "694640"
  },
  {
    "text": "all the mounts were failing so so i have to restart all the services to get my",
    "start": "694640",
    "end": "700880"
  },
  {
    "text": "workload yeah back online and then this was again something",
    "start": "700880",
    "end": "707440"
  },
  {
    "text": "what was a problem with ranger because there was missing extra mount",
    "start": "707440",
    "end": "714160"
  },
  {
    "text": "or extra bind missing on the cubelet service which was causing this issue",
    "start": "714160",
    "end": "720079"
  },
  {
    "text": "and as we don't had so much iop iops intensive workload it was really",
    "start": "720079",
    "end": "726079"
  },
  {
    "text": "just some some uh files dropping in an instrument folder",
    "start": "726079",
    "end": "732720"
  },
  {
    "text": "i switch to plain old simple nfs client provision so",
    "start": "732720",
    "end": "739600"
  },
  {
    "text": "if you want to to introduce some some storage back-end perhaps you should also ask your your",
    "start": "740160",
    "end": "746959"
  },
  {
    "text": "storage guys what they can provision or perhaps they have already some solutions for you right",
    "start": "746959",
    "end": "753839"
  },
  {
    "text": "and you also need to think about okay how i'm doing backups for my persistent volumes if you want to if there are",
    "start": "753839",
    "end": "760560"
  },
  {
    "text": "files in it you really need right so backup what kind of workload do you expect",
    "start": "760560",
    "end": "767279"
  },
  {
    "text": "what your infrastructure is already providing so you need to take some time to to",
    "start": "767279",
    "end": "774320"
  },
  {
    "text": "research what you really need to to implement here in your cluster",
    "start": "774320",
    "end": "780079"
  },
  {
    "text": "the next thing i want to bring up our back or authentication authorization role based access control use service",
    "start": "780800",
    "end": "787839"
  },
  {
    "start": "781000",
    "end": "781000"
  },
  {
    "text": "accounts from the beginning one of the mistakes i did is after my my first",
    "start": "787839",
    "end": "792959"
  },
  {
    "text": "development cluster i just handed over to the developer",
    "start": "792959",
    "end": "798240"
  },
  {
    "text": "and token with cluster admin rights right and this token was being reused by",
    "start": "798240",
    "end": "804240"
  },
  {
    "text": "a lot of people and it shouldn't be here the case right so so",
    "start": "804240",
    "end": "809680"
  },
  {
    "text": "this was one of the problems that we it was really hard after to get rid of",
    "start": "809680",
    "end": "815839"
  },
  {
    "text": "the token because it was using it was in cube config files within the pipelines and so on so don't share credentials",
    "start": "815839",
    "end": "822480"
  },
  {
    "text": "across teams have separate accounts for deployment monitoring operation tasks everybody should have its own user",
    "start": "822480",
    "end": "830320"
  },
  {
    "text": "and so on so it can also lock down which resources different departments can access within the cluster right",
    "start": "830320",
    "end": "838160"
  },
  {
    "text": "the next topic i want to cover is login monitoring sounds obvious right but",
    "start": "838160",
    "end": "845920"
  },
  {
    "start": "840000",
    "end": "840000"
  },
  {
    "text": "having the proper monitoring in place it's also crucial so don't just monitor your your services",
    "start": "845920",
    "end": "853360"
  },
  {
    "text": "you deployed inside of your cluster you should really also monitor the infrastructure so on node level the",
    "start": "853360",
    "end": "860320"
  },
  {
    "text": "cluster services like your hd database the api server latency",
    "start": "860320",
    "end": "866480"
  },
  {
    "text": "dns services running in the cluster right kubernetes events should be also covered",
    "start": "866480",
    "end": "871760"
  },
  {
    "text": "by your monitoring like om kills crash loop backup events",
    "start": "871760",
    "end": "877040"
  },
  {
    "text": "the actual requested and used resources is also a thing you you should decide or",
    "start": "877040",
    "end": "883440"
  },
  {
    "text": "or you should monitor with you with your stack and",
    "start": "883440",
    "end": "888720"
  },
  {
    "text": "the funniest question i always was being asked by developers was basically hey",
    "start": "888720",
    "end": "894800"
  },
  {
    "text": "where on my pod should i write my log file because every time i'm redeploying my port my log file is done",
    "start": "894800",
    "end": "901680"
  },
  {
    "text": "and this was one thing you're always telling them yeah you but you don't write log files you write to standard",
    "start": "901680",
    "end": "907040"
  },
  {
    "text": "out then our fluency process will grab the log files and forward it to splunk and here in splunk you can now see all",
    "start": "907040",
    "end": "913920"
  },
  {
    "text": "the logs of your pods and then be happy with that right so so",
    "start": "913920",
    "end": "919120"
  },
  {
    "text": "log monitoring is also really crucial and there are plenty of open source",
    "start": "919120",
    "end": "924959"
  },
  {
    "text": "solutions out there like from ethos in combination with grafana elk stack for for logging and so on so",
    "start": "924959",
    "end": "931600"
  },
  {
    "text": "take your time invest the time to set up a proper logging and monitoring",
    "start": "931600",
    "end": "937360"
  },
  {
    "text": "for your clusters here so the next topic i was being pinked one",
    "start": "937360",
    "end": "944000"
  },
  {
    "start": "940000",
    "end": "940000"
  },
  {
    "text": "morning that hey christian all ingress are down not working anymore everything is is completely",
    "start": "944000",
    "end": "950880"
  },
  {
    "text": "down please help please help and then i recognized when i checked the latest ingresses which were",
    "start": "950880",
    "end": "957120"
  },
  {
    "text": "deployed to our clusters that one of the ingresses has no um hostname defined and so what",
    "start": "957120",
    "end": "963759"
  },
  {
    "text": "the ingress controller was then doing it or this was doing it was",
    "start": "963759",
    "end": "969839"
  },
  {
    "text": "uh um interpreting the missing host field with nesteric and so all workload was then being",
    "start": "969839",
    "end": "977120"
  },
  {
    "text": "routed to the one service behind this ingress annotation but you can prevent",
    "start": "977120",
    "end": "983440"
  },
  {
    "text": "such errors in advanced with um tools like with policies cluster",
    "start": "983440",
    "end": "989600"
  },
  {
    "text": "policies and there are plenty tools out there the most popular are caverno and open policy agent",
    "start": "989600",
    "end": "996320"
  },
  {
    "text": "and yeah deploy cluster policies from the",
    "start": "996320",
    "end": "1001920"
  },
  {
    "text": "beginning so that no workload will will being deployed even on the dev",
    "start": "1001920",
    "end": "1008320"
  },
  {
    "text": "clusters without the proper validation like the ingress validation i told you like port security for instance these",
    "start": "1008320",
    "end": "1014320"
  },
  {
    "text": "low privileged containers that your deployment requires some resource limits and requests defined",
    "start": "1014320",
    "end": "1021120"
  },
  {
    "text": "right that there are health checks configured within yours within your service",
    "start": "1021120",
    "end": "1026558"
  },
  {
    "text": "so start with policies straight away and the last thing i want to mention in",
    "start": "1026559",
    "end": "1032558"
  },
  {
    "text": "operations infrastructure tasks is yeah it's i titled the slide cube control cube",
    "start": "1032559",
    "end": "1039438"
  },
  {
    "text": "cuddle or cubesat however you want to pronounce it but operators and developers should at",
    "start": "1039439",
    "end": "1045120"
  },
  {
    "text": "least be familiar with common cube ctl commands right like get port",
    "start": "1045120",
    "end": "1050559"
  },
  {
    "text": "logs describe pot and so on um i've seen a",
    "start": "1050559",
    "end": "1055679"
  },
  {
    "text": "lot of people were struggling when for instance the web ui",
    "start": "1055679",
    "end": "1061280"
  },
  {
    "text": "from from rancho was not away and they were not able to troubleshoot deployment issues",
    "start": "1061280",
    "end": "1067760"
  },
  {
    "text": "right and and these are the basics you should yeah be",
    "start": "1067760",
    "end": "1074000"
  },
  {
    "text": "aware of and also invest again in trainings here to",
    "start": "1074000",
    "end": "1079919"
  },
  {
    "text": "to [Music] yeah that the people are familiar with the tools",
    "start": "1079919",
    "end": "1085760"
  },
  {
    "text": "so next will be already development and employment stuff",
    "start": "1086000",
    "end": "1091360"
  },
  {
    "text": "so running workload in cuban eaters because of the wrong motivation i've seen this",
    "start": "1091360",
    "end": "1096640"
  },
  {
    "start": "1092000",
    "end": "1092000"
  },
  {
    "text": "multiple times when when you ask somebody hey why you yeah i deployed this application in our kubernetes",
    "start": "1096640",
    "end": "1102160"
  },
  {
    "text": "cluster then yeah you're getting answers like i run it in kubernetes because i don't want to request the vm",
    "start": "1102160",
    "end": "1109360"
  },
  {
    "text": "there are other examples like we had puppet and hira so uh",
    "start": "1109360",
    "end": "1116080"
  },
  {
    "text": "for for config management in our company back in the days and people used to deploy stuff in",
    "start": "1116080",
    "end": "1121760"
  },
  {
    "text": "kubernetes because they want to skip the merge request in in our puppet configuration that's also",
    "start": "1121760",
    "end": "1129120"
  },
  {
    "text": "not the way how to decide why to run workload in kubernetes right",
    "start": "1129120",
    "end": "1134720"
  },
  {
    "text": "or something like running a static website inside of a pod instead of just putting that in as three bucket",
    "start": "1134720",
    "end": "1141120"
  },
  {
    "text": "all right think about why the workload should run in kubernetes if it makes sense",
    "start": "1141120",
    "end": "1147120"
  },
  {
    "text": "there should be the architects who decide what kind of workload to put in which",
    "start": "1147120",
    "end": "1152640"
  },
  {
    "text": "environment or in if it should run in kubernetes or not",
    "start": "1152640",
    "end": "1158799"
  },
  {
    "text": "the next one local development a lot of time i've seen something so i getting pink like kubernetes is down my",
    "start": "1158799",
    "end": "1165120"
  },
  {
    "start": "1159000",
    "end": "1159000"
  },
  {
    "text": "deployment is not working right and you were checking the deployment and you had a lot of uh",
    "start": "1165120",
    "end": "1172160"
  },
  {
    "text": "restarts of the container then you pulled the container to your local computer and and just run the docker start on the container and you",
    "start": "1172160",
    "end": "1178720"
  },
  {
    "text": "see the container wasn't starting at all so developers should be familiar or when",
    "start": "1178720",
    "end": "1186559"
  },
  {
    "text": "they're working on kubernetes at least testing their containers on their local machine there",
    "start": "1186559",
    "end": "1193440"
  },
  {
    "text": "are multiple options to run a kubernetes on your local machine like kubernetes and",
    "start": "1193440",
    "end": "1198880"
  },
  {
    "text": "docker mini cube keys so you can also test your deployment i've seen a lot of",
    "start": "1198880",
    "end": "1204240"
  },
  {
    "text": "people who are building helm charts and then having a git commit history",
    "start": "1204240",
    "end": "1210400"
  },
  {
    "text": "hundreds of of commits just to test their home deployment you don't need to run every time an",
    "start": "1210640",
    "end": "1216720"
  },
  {
    "text": "entire cicd pipeline to just test out your helm chart this could be done locally faster in my opinion",
    "start": "1216720",
    "end": "1224799"
  },
  {
    "text": "so get familiar with local development",
    "start": "1224799",
    "end": "1230080"
  },
  {
    "text": "tools and and yeah just use them then one of the",
    "start": "1230080",
    "end": "1237200"
  },
  {
    "start": "1235000",
    "end": "1235000"
  },
  {
    "text": "yeah main reasons deployments were failing or or the icd's pipelines are failing",
    "start": "1237200",
    "end": "1243520"
  },
  {
    "text": "were they were using latest text and i can tell you never ever use latest tags",
    "start": "1243520",
    "end": "1251520"
  },
  {
    "text": "either in your cisd pipeline nor your need is deployed so",
    "start": "1251520",
    "end": "1258000"
  },
  {
    "text": "it could break your entire workload if it's getting rebuild your image with let's let's assume",
    "start": "1258000",
    "end": "1264559"
  },
  {
    "text": "you're building a nodejs application you're using a node latest right then all of a sudden it could be",
    "start": "1264559",
    "end": "1271679"
  },
  {
    "text": "that yeah nodejs is releasing new new version and your application's not compatible",
    "start": "1271679",
    "end": "1277679"
  },
  {
    "text": "anymore with the newest nodejs version and then it will build with news uh",
    "start": "1277679",
    "end": "1283280"
  },
  {
    "text": "base image and will break basically your workload right so",
    "start": "1283280",
    "end": "1288960"
  },
  {
    "text": "try to avoid at least the latest takes one of the problems i had with the katie",
    "start": "1288960",
    "end": "1294000"
  },
  {
    "text": "a storage provisioner was for instance that in the example i was using to to",
    "start": "1294000",
    "end": "1299360"
  },
  {
    "text": "install hikiti and clusterfs hikiti was deployed using latest and every time kitty was rescheduled it was pulling the",
    "start": "1299360",
    "end": "1306559"
  },
  {
    "text": "used image and then we had inversion discrepancy between cluster of s and t kt",
    "start": "1306559",
    "end": "1313840"
  },
  {
    "text": "right and yeah this was also one of the bigger",
    "start": "1313840",
    "end": "1320159"
  },
  {
    "text": "problem because we weren't able to provision any persistent volumes anymore right so please don't use latest",
    "start": "1320159",
    "end": "1328559"
  },
  {
    "start": "1328000",
    "end": "1328000"
  },
  {
    "text": "but talking of images private registry and base images i think it's really",
    "start": "1328799",
    "end": "1334320"
  },
  {
    "text": "crucial to have a library of own base images and an own registry",
    "start": "1334320",
    "end": "1340080"
  },
  {
    "text": "in your environment private registry where you have full control over the images where you can scan your images",
    "start": "1340080",
    "end": "1346799"
  },
  {
    "text": "for vulnerabilities so so for instance harbor it's an open source registry you can use it and then just",
    "start": "1346799",
    "end": "1354080"
  },
  {
    "text": "scan your images constantly you can add additional configuration to",
    "start": "1354080",
    "end": "1360159"
  },
  {
    "text": "the base images java 8 was there for instance one one example",
    "start": "1360159",
    "end": "1365760"
  },
  {
    "text": "um no need to pull everything from docker hub and hit the rate limit",
    "start": "1365760",
    "end": "1371840"
  },
  {
    "text": "right and then don't use the image pull policy always so there there",
    "start": "1371840",
    "end": "1377039"
  },
  {
    "text": "are some use cases imageable policy always is usable or good",
    "start": "1377039",
    "end": "1382320"
  },
  {
    "text": "but normally you don't want to pull the image every time it's getting rescheduled on another",
    "start": "1382320",
    "end": "1387520"
  },
  {
    "text": "worker node if the image is already there because it will be much more faster to spin up your application right",
    "start": "1387520",
    "end": "1393679"
  },
  {
    "text": "if it's just reusing the image which is already cached on the machine and if possible use block and allow",
    "start": "1393679",
    "end": "1400640"
  },
  {
    "text": "lists for your developers you don't want to have something like this in the sprinter below docker pull from some",
    "start": "1400640",
    "end": "1406559"
  },
  {
    "text": "untrusted sources perhaps with a crypto miner in it so i've seen",
    "start": "1406559",
    "end": "1411840"
  },
  {
    "text": "people using from really untrusted sources or not known sources images like alpine curl",
    "start": "1411840",
    "end": "1420480"
  },
  {
    "text": "like why not just you're building your own alpine image with curl included if needed",
    "start": "1420480",
    "end": "1428960"
  },
  {
    "text": "right so and another problem i've seen frequently was not using the power of kubernetes or",
    "start": "1428960",
    "end": "1437760"
  },
  {
    "start": "1434000",
    "end": "1434000"
  },
  {
    "text": "like this quote says by auto scaling i have two ports so when you just deploy two ports or or",
    "start": "1437760",
    "end": "1444640"
  },
  {
    "text": "one part of your of your application why just run it in kubernetes when you don't use",
    "start": "1444640",
    "end": "1450799"
  },
  {
    "text": "yeah the features of kubernetes is what kubernetes is offering you here like as i said auto scaling",
    "start": "1450799",
    "end": "1456960"
  },
  {
    "text": "very cool stuff right so use just the features kubernetes is",
    "start": "1456960",
    "end": "1464159"
  },
  {
    "text": "offering you other problem in in development life",
    "start": "1464159",
    "end": "1469600"
  },
  {
    "start": "1466000",
    "end": "1466000"
  },
  {
    "text": "cycle health checks so make sure your application is configured with a proper health check please",
    "start": "1469600",
    "end": "1476080"
  },
  {
    "text": "and prevent again you can use caverno or open policy agent to prevent",
    "start": "1476080",
    "end": "1483520"
  },
  {
    "text": "applications being deployed to your cluster without a proper health check liveness check readiness probe or",
    "start": "1483520",
    "end": "1489120"
  },
  {
    "text": "whatever right and if you get ever getting asked",
    "start": "1489120",
    "end": "1494320"
  },
  {
    "text": "hey please restart my container or my my application in kubernetes then you have to ask the developer okay why we need to",
    "start": "1494320",
    "end": "1501440"
  },
  {
    "text": "restart it manually this shouldn't be the the way you work in kubernetes your service",
    "start": "1501440",
    "end": "1506640"
  },
  {
    "text": "should recover automatically and then restart automatically right",
    "start": "1506640",
    "end": "1511760"
  },
  {
    "text": "so proper health check in place is really really crucial here",
    "start": "1511760",
    "end": "1518720"
  },
  {
    "start": "1518000",
    "end": "1518000"
  },
  {
    "text": "forgotten resource limits and requests so i've seen a lot of",
    "start": "1519840",
    "end": "1525840"
  },
  {
    "text": "deployments which haven't had defined any resource limits and resource requests",
    "start": "1525840",
    "end": "1531600"
  },
  {
    "text": "in their application so what you can do against something like that again you can yeah",
    "start": "1531600",
    "end": "1538240"
  },
  {
    "text": "prevent workloads with the policies or you can add just default requests and",
    "start": "1538240",
    "end": "1544400"
  },
  {
    "text": "and limits based on namespace or cluster and a question i was getting",
    "start": "1544400",
    "end": "1551279"
  },
  {
    "text": "asked quite frequently was yeah but i don't know how many resources my application is using",
    "start": "1551279",
    "end": "1556880"
  },
  {
    "text": "so you can reference here again back to the local development section so just run your image locally and then you",
    "start": "1556880",
    "end": "1563760"
  },
  {
    "text": "will directly see how much how much resources your image is consuming",
    "start": "1563760",
    "end": "1568960"
  },
  {
    "text": "and if not so i've seen for instance small microservices written in go",
    "start": "1568960",
    "end": "1574400"
  },
  {
    "text": "and they just requested 12 cpus and 128 gigs of memory",
    "start": "1574400",
    "end": "1580320"
  },
  {
    "text": "then you should also ask the developer hey isn't it perhaps a little bit much because kubernetes will try to reserve",
    "start": "1580320",
    "end": "1587679"
  },
  {
    "text": "the requested memory on the worker note and it's also trying to schedule your application based on your",
    "start": "1587679",
    "end": "1594080"
  },
  {
    "text": "resource request so monitoring such things is also beneficial",
    "start": "1594080",
    "end": "1603759"
  },
  {
    "start": "1604000",
    "end": "1604000"
  },
  {
    "text": "the next thing i want to talk about is environment variables for configuration and secrets you can totally put some",
    "start": "1604480",
    "end": "1610480"
  },
  {
    "text": "configuration volumes and environment variables but i've seen also",
    "start": "1610480",
    "end": "1616159"
  },
  {
    "text": "deployments with i don't know 50 60 different environment variables for for",
    "start": "1616159",
    "end": "1621600"
  },
  {
    "text": "configuration so use configmaps for configuration right and never ever put secrets and plain",
    "start": "1621600",
    "end": "1629039"
  },
  {
    "text": "text into environment variables so create and standard how is your",
    "start": "1629039",
    "end": "1635120"
  },
  {
    "text": "application should be configured right and use a secret store for instance you",
    "start": "1635120",
    "end": "1640159"
  },
  {
    "text": "can use vault or you can use some stuff like sealed secrets from bitnami which makes",
    "start": "1640159",
    "end": "1645679"
  },
  {
    "text": "the things more in yeah more secure and kubernetes secrets are not encrypted",
    "start": "1645679",
    "end": "1651679"
  },
  {
    "text": "in in your cluster right so if you can read a secret with your service",
    "start": "1651679",
    "end": "1656960"
  },
  {
    "text": "account or your user account you can just run a base 460 code and you",
    "start": "1656960",
    "end": "1662320"
  },
  {
    "text": "will have the value of the secret so keep that in mind also when creating service accounts that",
    "start": "1662320",
    "end": "1668880"
  },
  {
    "text": "you not yeah allow them to read all secrets",
    "start": "1668880",
    "end": "1674320"
  },
  {
    "text": "so and one of the yeah last slides um using a template engine",
    "start": "1675360",
    "end": "1683679"
  },
  {
    "start": "1676000",
    "end": "1676000"
  },
  {
    "text": "or helm so so what i've seen in our environment that people are reusing the same yaml files",
    "start": "1683679",
    "end": "1690880"
  },
  {
    "text": "for deployment for for ingresses and for service annotations for services over",
    "start": "1690880",
    "end": "1696559"
  },
  {
    "text": "and over again and it makes something yeah it makes really hard",
    "start": "1696559",
    "end": "1702399"
  },
  {
    "text": "to change all these yeah deployment and and all the yaml files in",
    "start": "1702399",
    "end": "1710000"
  },
  {
    "text": "different git repositories if you want to introduce something like a new annotation if you want to",
    "start": "1710000",
    "end": "1716480"
  },
  {
    "text": "add some some uh monitoring annotations to your application and so on so",
    "start": "1716480",
    "end": "1724640"
  },
  {
    "text": "provide perhaps an helm template for your developers which they can use because most of the",
    "start": "1724640",
    "end": "1731919"
  },
  {
    "text": "deployments in our organization were most likely the same",
    "start": "1731919",
    "end": "1737279"
  },
  {
    "text": "right and i create an helm template they can use and then also just having",
    "start": "1737279",
    "end": "1744240"
  },
  {
    "text": "switches inside of the template like expose my service externally yes or no do you want to have persistent volume",
    "start": "1744240",
    "end": "1750240"
  },
  {
    "text": "yes or no they don't need to think about storage classes they don't need to think about uh ingress configuration it's just",
    "start": "1750240",
    "end": "1757840"
  },
  {
    "text": "a matter of the template and the developer could then easily just deploy in its ci sd pipelines application by",
    "start": "1757840",
    "end": "1765279"
  },
  {
    "text": "using a helm upgrade install providing the image",
    "start": "1765279",
    "end": "1771520"
  },
  {
    "text": "the image url and the image tag setting some more parameters and they were good to go",
    "start": "1771520",
    "end": "1777520"
  },
  {
    "text": "and they can deploy the application to capabilities so this was for us or for me a huge time",
    "start": "1777520",
    "end": "1783520"
  },
  {
    "text": "saver so and as summary of my talk is",
    "start": "1783520",
    "end": "1790320"
  },
  {
    "start": "1786000",
    "end": "1786000"
  },
  {
    "text": "when you're starting your journey in kubernetes in your near company invest in trainings if you have no idea like i",
    "start": "1790320",
    "end": "1797039"
  },
  {
    "text": "had back in the days um invest in trainings document how to use the platform",
    "start": "1797039",
    "end": "1803679"
  },
  {
    "text": "documentation is is is here also the key to not run into the same issues over and",
    "start": "1803679",
    "end": "1810960"
  },
  {
    "text": "over again right to give the developers and the operations team guidance on how to use",
    "start": "1810960",
    "end": "1817360"
  },
  {
    "text": "kubernetes how to use the platform i provide templates avoid common",
    "start": "1817360",
    "end": "1822559"
  },
  {
    "text": "mistakes like the forgotten health checks and so on and try to reject everything",
    "start": "1822559",
    "end": "1830000"
  },
  {
    "text": "as early as possible which is not yeah meet the required standards of your of",
    "start": "1830000",
    "end": "1837039"
  },
  {
    "text": "your deployments right so again stuff like using the policy engines",
    "start": "1837039",
    "end": "1844080"
  },
  {
    "text": "and involve your security operations from the beginning and involve all other teams",
    "start": "1844080",
    "end": "1851520"
  },
  {
    "text": "from the beginning right as i said you're building here a really huge platform which is being used by all",
    "start": "1851520",
    "end": "1858000"
  },
  {
    "text": "of the people in your organization later on so",
    "start": "1858000",
    "end": "1863600"
  },
  {
    "text": "that's all from my site and yeah i'm i'm happy um to be here available for some q a sessions right",
    "start": "1863600",
    "end": "1872720"
  },
  {
    "text": "um you can reach out to me via linkedin on twitter or shoot me an email and we",
    "start": "1872720",
    "end": "1878559"
  },
  {
    "text": "will see us i hope you enjoyed the session and see you later in the q a bye bye",
    "start": "1878559",
    "end": "1887440"
  }
]