[
  {
    "text": "hello everyone and thanks for joining us uh to our session at at the end of this",
    "start": "240",
    "end": "6240"
  },
  {
    "text": "intense week Uh we're going to showcase how a new relic we uh manage a multi",
    "start": "6240",
    "end": "12559"
  },
  {
    "text": "cloud Kubernetes infrastructure levering cluster API on top of a cell",
    "start": "12559",
    "end": "18960"
  },
  {
    "text": "architecture um in order to scale out uh workloads",
    "start": "18960",
    "end": "25199"
  },
  {
    "text": "but also uh while limiting the blast radius uh for incidents Uh my name is Javier Mosca",
    "start": "25199",
    "end": "32880"
  },
  {
    "text": "Sanchez I'm working as a so principal software engineer Um I'm a Kubernetes",
    "start": "32880",
    "end": "38640"
  },
  {
    "text": "and multicloud architect at New Relic and I'm joined at the stage uh by my colleague Tazik Hey I'm Tazik and I work",
    "start": "38640",
    "end": "46079"
  },
  {
    "text": "as a senior software engineer at New Relic in the same team as Javi Cool So we'll start outlining some",
    "start": "46079",
    "end": "54239"
  },
  {
    "text": "context some numbers uh about our scale the the problem that we want to solve Um",
    "start": "54239",
    "end": "60719"
  },
  {
    "text": "also why we move to a cellular architecture um librarian also cluster API in order",
    "start": "60719",
    "end": "68640"
  },
  {
    "text": "to implement uh this uh Kubernetes infrastructure in multiple cloud providers Um additionally we are going",
    "start": "68640",
    "end": "76080"
  },
  {
    "text": "to showcase how we added some layers uh on top of this to easy uh consumption",
    "start": "76080",
    "end": "83360"
  },
  {
    "text": "for uh instances and the the different offerings and nuances that the multi the",
    "start": "83360",
    "end": "89759"
  },
  {
    "text": "cloud providers offer us Now what we do on New Relic uh we",
    "start": "89759",
    "end": "96159"
  },
  {
    "text": "provide um intelligent observability platform that empowers developers to enhance uh digital experiences Uh we",
    "start": "96159",
    "end": "104640"
  },
  {
    "text": "have more than 85,000 active customers Uh we process more than 400 400 million",
    "start": "104640",
    "end": "111920"
  },
  {
    "text": "queries per day Uh we ingest around seven pabytes per day That makes uh at",
    "start": "111920",
    "end": "119759"
  },
  {
    "text": "the end of the year around 3 xabytes And with that we process 12 billion of",
    "start": "119759",
    "end": "125280"
  },
  {
    "text": "events per minute Now how does this",
    "start": "125280",
    "end": "131080"
  },
  {
    "text": "um translate into uh Kubernetes uh so we operate all of these uh on top of uh 280",
    "start": "131080",
    "end": "140640"
  },
  {
    "text": "Kubernetes clusters Um we operate we run more than uh 5,000 uh pots uh on on over",
    "start": "140640",
    "end": "151120"
  },
  {
    "text": "the uh 21,000 nodes Um and we run all of these",
    "start": "151120",
    "end": "156480"
  },
  {
    "text": "in multiple cloud providers uh and multiple regions Uh and our average cluster has uh between 300 and 500 uh",
    "start": "156480",
    "end": "165080"
  },
  {
    "text": "nodes and we run on top of them on each of the of our clusters um around 5,000",
    "start": "165080",
    "end": "172720"
  },
  {
    "text": "and 7,000 bots Now uh at a high level our architecture",
    "start": "172720",
    "end": "178720"
  },
  {
    "text": "ingests and processes telemetry data from our customers that they send us instrumenting their applications and",
    "start": "178720",
    "end": "186640"
  },
  {
    "text": "their infrastructure leveraging our agents Uh this could be done through",
    "start": "186640",
    "end": "191760"
  },
  {
    "text": "language agents infrastructure agents cloud integrations etc So for ingest data flows through",
    "start": "191760",
    "end": "199120"
  },
  {
    "text": "HTTP endpoints exposed via CDNs and gets ingested and process it through a",
    "start": "199120",
    "end": "204720"
  },
  {
    "text": "specific data pipelines depending on each data type uh preparing it for storage in our distributed New Relic",
    "start": "204720",
    "end": "211920"
  },
  {
    "text": "database but also going into a hot path for triggering alert",
    "start": "211920",
    "end": "217159"
  },
  {
    "text": "notifications for the query path customers query this store data library in our UIs and APIs also exposed through",
    "start": "217159",
    "end": "225760"
  },
  {
    "text": "a CDN Now uh the proing context about all of this is that for many years we",
    "start": "225760",
    "end": "232080"
  },
  {
    "text": "were running most of our services uh into a controllerite workloads but",
    "start": "232080",
    "end": "238080"
  },
  {
    "text": "running on top of a DCOS single DCOS cluster and regarding our data pipelines",
    "start": "238080",
    "end": "245200"
  },
  {
    "text": "uh they leverage Kafka we are heavy CFKA users and we were running also a unique",
    "start": "245200",
    "end": "250959"
  },
  {
    "text": "uh Kafka clusters uh in order to to accomplish that um because of this kind",
    "start": "250959",
    "end": "256479"
  },
  {
    "text": "um monolithic infrastructure it was very hard to scale um update so any operation",
    "start": "256479",
    "end": "263600"
  },
  {
    "text": "regarding adding nodes or upgrading um it could be very risky um frequently",
    "start": "263600",
    "end": "270960"
  },
  {
    "text": "we were running into incidents with a huge blast radius basically when we have an incident on this uh we were affecting",
    "start": "270960",
    "end": "278800"
  },
  {
    "text": "all of our customers right so at some point uh we start solving uh started to",
    "start": "278800",
    "end": "285120"
  },
  {
    "text": "solve the problem are to thinking about solving the problem So we initiated uh back in 2020 a program a multi-year",
    "start": "285120",
    "end": "292479"
  },
  {
    "text": "program in order to first migrate to the cloud um in order to have more",
    "start": "292479",
    "end": "298400"
  },
  {
    "text": "scalability capabilities but also we wanted to isolate uh the blast radius to",
    "start": "298400",
    "end": "303680"
  },
  {
    "text": "limit the blast radius in case of incidents So we also align this program",
    "start": "303680",
    "end": "308960"
  },
  {
    "text": "uh to shift uh to a cellbased architecture",
    "start": "308960",
    "end": "314199"
  },
  {
    "text": "Now which are cells basically uh in a biological context a cell is the smalle",
    "start": "314199",
    "end": "321039"
  },
  {
    "text": "unit that can live on its own So in order to accomplish that it should have all the resources inside uh all the",
    "start": "321039",
    "end": "329199"
  },
  {
    "text": "components necessary ne necessary for uh accomplish a a specific function right",
    "start": "329199",
    "end": "334960"
  },
  {
    "text": "it should be independent let's say um another characteristic uh is that they",
    "start": "334960",
    "end": "340720"
  },
  {
    "text": "exchange energy and matter So effectively the the cells are interconnected each other uh in order to",
    "start": "340720",
    "end": "346960"
  },
  {
    "text": "provide uh more complex functions Right now talking about a cellbased",
    "start": "346960",
    "end": "353600"
  },
  {
    "text": "architecture this architecture aligns with this definition regarding cells because um a workload in this",
    "start": "353600",
    "end": "360080"
  },
  {
    "text": "architecture is is decomposed in self-contained installations that should",
    "start": "360080",
    "end": "365360"
  },
  {
    "text": "satisfy operations for a shard So when we talk about a shard we are talking about a subset of a large larger data",
    "start": "365360",
    "end": "373360"
  },
  {
    "text": "data set For instance uh a subset of of our users right so this makes uh a cell",
    "start": "373360",
    "end": "381039"
  },
  {
    "text": "an independent unit of a scale Uh it's also limit the blast radius because uh",
    "start": "381039",
    "end": "387120"
  },
  {
    "text": "if you have an an issue on a cell that's um limited to that specific cell So in",
    "start": "387120",
    "end": "393120"
  },
  {
    "text": "consequence it's limited to a specific subset of your data right and and in order to scale out as",
    "start": "393120",
    "end": "400720"
  },
  {
    "text": "this is a a repeatable pattern what what you need to add is more cell instances",
    "start": "400720",
    "end": "406720"
  },
  {
    "text": "for that workload And that's kind of one of the hardest parts because um of this",
    "start": "406720",
    "end": "412479"
  },
  {
    "text": "architecture because you need to find out a logic um a thin layer you put over the the cells in order to share that",
    "start": "412479",
    "end": "419680"
  },
  {
    "text": "traffic between the cells You need to share the data You need to manage that traffic flowing to the to the different",
    "start": "419680",
    "end": "425440"
  },
  {
    "text": "cell instances So that makes the the cell router and as we were working uh",
    "start": "425440",
    "end": "430960"
  },
  {
    "text": "talking about workloads um you need to identify the compose your infrastructure",
    "start": "430960",
    "end": "436960"
  },
  {
    "text": "into those isolated repeatable patterns Um that's also a a critical um moment",
    "start": "436960",
    "end": "444639"
  },
  {
    "text": "for going to a salbased architecture because you need to kind of um implement",
    "start": "444639",
    "end": "450000"
  },
  {
    "text": "some mechanism that help you on this for instance a domain driven design So you",
    "start": "450000",
    "end": "455360"
  },
  {
    "text": "identify workloads and create new cell types Um so that was a kind of generic",
    "start": "455360",
    "end": "463919"
  },
  {
    "text": "definition for a cell right for us at New Relic we are implementing cell's uh",
    "start": "463919",
    "end": "470240"
  },
  {
    "text": "scope to a AWS account as your subscription or GCP project So a cell is",
    "start": "470240",
    "end": "476639"
  },
  {
    "text": "living in an specific unit of of this kind We added uh only one Kubernetes",
    "start": "476639",
    "end": "483360"
  },
  {
    "text": "cluster inside We added one cafer cluster and we added only uh uh one",
    "start": "483360",
    "end": "488800"
  },
  {
    "text": "binet or VPC in order to give the networking for that specific cell instance Um depending on the cell type",
    "start": "488800",
    "end": "496000"
  },
  {
    "text": "we could should be adding also some other resources like data stores load balancers whatever that cell type needs",
    "start": "496000",
    "end": "503039"
  },
  {
    "text": "to work as an isolation and independent unit Additionally they are going to be pure",
    "start": "503039",
    "end": "508720"
  },
  {
    "text": "with other cell types if they need to exchange information like the biological",
    "start": "508720",
    "end": "513919"
  },
  {
    "text": "uh cells And in our case we deploy it in a multi- availability uh zone",
    "start": "513919",
    "end": "520440"
  },
  {
    "text": "setup Um um another special characteristic we",
    "start": "520440",
    "end": "525760"
  },
  {
    "text": "added is that we wanted to be them to be femoral So we wanted them to be uh",
    "start": "525760",
    "end": "531920"
  },
  {
    "text": "destroyed and replaced uh it's time So we link this with the",
    "start": "531920",
    "end": "539920"
  },
  {
    "text": "Kubernetes life cycle of 90 days or if version is something we are still",
    "start": "539920",
    "end": "545360"
  },
  {
    "text": "working on it but it's a charact that we want to to achieve and regarding from",
    "start": "545360",
    "end": "550800"
  },
  {
    "text": "the previous um for the former diagram if we are talking about the composing uh",
    "start": "550800",
    "end": "556640"
  },
  {
    "text": "a monolithic infrastructure in several workloads it means that you're going to have different cell types each cell type",
    "start": "556640",
    "end": "563680"
  },
  {
    "text": "it would it would be representing a specific domain Right And also another",
    "start": "563680",
    "end": "569360"
  },
  {
    "text": "important point is that this is an living architecture It's continuously evolving Maybe you set up a specific",
    "start": "569360",
    "end": "575920"
  },
  {
    "text": "cell type You determine oh this is a too big We are putting too many resources",
    "start": "575920",
    "end": "581519"
  },
  {
    "text": "and you want to decompose that cell type into different smaller cell types",
    "start": "581519",
    "end": "588000"
  },
  {
    "text": "Now from our former diagram this is how it looks adding turning it into a cells",
    "start": "588000",
    "end": "594880"
  },
  {
    "text": "uh operating being independent units and in our case as we have two different",
    "start": "594880",
    "end": "600399"
  },
  {
    "text": "data paths we need different cell routers with different logic uh in order",
    "start": "600399",
    "end": "605600"
  },
  {
    "text": "to accomplish the the sh of the data for the ines but also for on the query",
    "start": "605600",
    "end": "611399"
  },
  {
    "text": "path Now this is a a picture that I think uh it represents visually very",
    "start": "611399",
    "end": "618399"
  },
  {
    "text": "very great the um the achievement of trans jumping from a monolithic",
    "start": "618399",
    "end": "624320"
  },
  {
    "text": "infrastructure into a cell architecture This is um an snapshot from the first",
    "start": "624320",
    "end": "629760"
  },
  {
    "text": "year of our program Um here we are we are seeing the um telemetry data from",
    "start": "629760",
    "end": "635680"
  },
  {
    "text": "our customers flowing into our environment And you can notice that at the beginning there's this big chunk",
    "start": "635680",
    "end": "642399"
  },
  {
    "text": "blue blue chunk of data flowing that represents our former data centers along",
    "start": "642399",
    "end": "648160"
  },
  {
    "text": "time that chunk is decreasing while you can notice several smaller chunks of",
    "start": "648160",
    "end": "654480"
  },
  {
    "text": "data flow um uh that represent data flowing to a specific uh cell instances",
    "start": "654480",
    "end": "662320"
  },
  {
    "text": "Now you can see how we were sharing that traffic from our customers into",
    "start": "662320",
    "end": "667360"
  },
  {
    "text": "isolating uh isolated units Now this look cool but of course",
    "start": "667360",
    "end": "673920"
  },
  {
    "text": "changing from a monolithic infrastructure and into a highly distributed environment has a lot of",
    "start": "673920",
    "end": "680120"
  },
  {
    "text": "challenge For instance the uh the asset management cell inventory on Kubernetes",
    "start": "680120",
    "end": "685839"
  },
  {
    "text": "cluster life cycle If we were talking about that we run uh more than uh 280",
    "start": "685839",
    "end": "691040"
  },
  {
    "text": "Kubernetes clusters and each cluster lives inside a cell it means that we have more than 280",
    "start": "691040",
    "end": "698399"
  },
  {
    "text": "uh cells right also uh we run this in a multi cloud implementation So it make",
    "start": "698399",
    "end": "704880"
  },
  {
    "text": "this harder and because of that and the different compute uh offering each uh",
    "start": "704880",
    "end": "710959"
  },
  {
    "text": "cloud and the different nuances each cloud provides we need to put some instruction layers for our c for our",
    "start": "710959",
    "end": "716640"
  },
  {
    "text": "internal customers for our developers in order to consume all of this in a seamless way So the scheduling offering",
    "start": "716640",
    "end": "723600"
  },
  {
    "text": "uh in an efficient way uh it's very important",
    "start": "723600",
    "end": "729320"
  },
  {
    "text": "Now why we jump into cluster API why cluster API so in order to tackle some",
    "start": "729720",
    "end": "736959"
  },
  {
    "text": "of these challenge uh cluster API it help us the with the life cycle of the",
    "start": "736959",
    "end": "742480"
  },
  {
    "text": "management of of clusters right Uh it gives you the abstractions to to do this",
    "start": "742480",
    "end": "747680"
  },
  {
    "text": "in a multi cloud environment It gives you the declarative specification So you",
    "start": "747680",
    "end": "753120"
  },
  {
    "text": "run uh and operate Kubernetes managing other Kubernetes and also you can streamline",
    "start": "753120",
    "end": "760560"
  },
  {
    "text": "uh in a seamless coherent appearance oper operational way uh from a central",
    "start": "760560",
    "end": "766160"
  },
  {
    "text": "centralized point to any cloud to any infrastructure Now here again we made it",
    "start": "766160",
    "end": "772720"
  },
  {
    "text": "slightly different uh we have what we call a common and control cluster where",
    "start": "772720",
    "end": "778399"
  },
  {
    "text": "we uh extending the Kubernetes API and we create CRDs that model our cellular",
    "start": "778399",
    "end": "785279"
  },
  {
    "text": "architecture in order to uh manage uh the life cycle of cells Uh but also of",
    "start": "785279",
    "end": "791839"
  },
  {
    "text": "course we run uh and bootstrap the the life cycle management of Kubernetes We bootstrap the Kubernetes from here uh we",
    "start": "791839",
    "end": "799279"
  },
  {
    "text": "have a streamlined bootstrapping process that help us doing the same way for any",
    "start": "799279",
    "end": "805680"
  },
  {
    "text": "cloud Now the different thing that we did here is that as we want the cells uh",
    "start": "805680",
    "end": "812399"
  },
  {
    "text": "to be an isolated units uh what we uh",
    "start": "812399",
    "end": "817519"
  },
  {
    "text": "determine is to after the bootstrapping process move the we move the um",
    "start": "817519",
    "end": "824160"
  },
  {
    "text": "management copy objects into each destination cluster So each uh",
    "start": "824160",
    "end": "829920"
  },
  {
    "text": "management sorry each targeted cluster each worker cluster is at the same time",
    "start": "829920",
    "end": "835360"
  },
  {
    "text": "a control plane uh a management cluster and worker cluster at the same time we made it isolated while we maintain a",
    "start": "835360",
    "end": "843680"
  },
  {
    "text": "centralized point in order to streamline operations let's say and with that I'll",
    "start": "843680",
    "end": "849600"
  },
  {
    "text": "hand over to my colleague Tastic to go deeper thanks AI",
    "start": "849600",
    "end": "855839"
  },
  {
    "text": "Given the numerous moving parts and teams involved and the frequency in which we were creating and",
    "start": "857519",
    "end": "863199"
  },
  {
    "text": "decommissioning a cell we need a solid repeatable and reliable way to maintain and decommission Kus clusters So this",
    "start": "863199",
    "end": "870720"
  },
  {
    "text": "process also needs to be easy to debug maintain and extend And we should also be able to checkpoint the various",
    "start": "870720",
    "end": "877399"
  },
  {
    "text": "states Let's see how do we do this on a very high level",
    "start": "877399",
    "end": "882480"
  },
  {
    "text": "We have a command and control cell which operates by running different homegrown cluster controllers which in turn",
    "start": "882480",
    "end": "888720"
  },
  {
    "text": "monitors the creation of a homegrown cluster CRD object which gets created after the prerequisite cell build",
    "start": "888720",
    "end": "895040"
  },
  {
    "text": "automation gets run Once this object is created the cluster controller specific to the cloud provider manages the",
    "start": "895040",
    "end": "902199"
  },
  {
    "text": "reconciliation and during this reconciliation process the kus job is initiated that creates a kind cluster in",
    "start": "902199",
    "end": "909279"
  },
  {
    "text": "docker and docker mode This client cluster is used to install Cappy and the Capy cloud provider",
    "start": "909279",
    "end": "916320"
  },
  {
    "text": "installation and necessary cluster dependencies These objects and necessary",
    "start": "916320",
    "end": "921920"
  },
  {
    "text": "cluster dependencies which objects required to create the we also create the objects required to create create the control plane and some initial",
    "start": "921920",
    "end": "928560"
  },
  {
    "text": "worker nodes Finally these resulting objects are transferred to the target gators cluster facilitating",
    "start": "928560",
    "end": "935760"
  },
  {
    "text": "self-management of the cluster by itself and the reconciliation continues",
    "start": "935760",
    "end": "941800"
  },
  {
    "text": "further Let's take the example of Azure cloud provider and its bootstrapping process On the very left side you will",
    "start": "941800",
    "end": "948880"
  },
  {
    "text": "notice that we install inside the kind cluster which is running inside the kus job the capy and capab controller and",
    "start": "948880",
    "end": "956720"
  },
  {
    "text": "after that we create the cubadm control plane object to create the control plane",
    "start": "956720",
    "end": "961759"
  },
  {
    "text": "Machine deployments are also created to house applications which are deploy deployed during the cluster bootstrapping process and once the",
    "start": "961759",
    "end": "968800"
  },
  {
    "text": "control plane is ready we start installing cluster dependencies This is followed by a cluster ctl in it",
    "start": "968800",
    "end": "974959"
  },
  {
    "text": "on the target kus cluster to deploy the necessary controllers and CRDs for capy",
    "start": "974959",
    "end": "980240"
  },
  {
    "text": "and caps Subsequently a cluster ctl move operation is executed to make the target",
    "start": "980240",
    "end": "986639"
  },
  {
    "text": "cluster self-managed along with all the dependencies which were installed",
    "start": "986639",
    "end": "992000"
  },
  {
    "text": "The cater's job concludes allowing reconciliation to proceed further with additional cluster operations like",
    "start": "992000",
    "end": "998320"
  },
  {
    "text": "syncing waves of Argo applications in order to make the cluster ready Running finally a set of test suits from our",
    "start": "998320",
    "end": "1004639"
  },
  {
    "text": "side to make sure the cluster is ready to receive live deployments from teams Also noted here are the groupings",
    "start": "1004639",
    "end": "1011440"
  },
  {
    "text": "of different Cappy and CABZY objects which are required to create the control plane and the worker nodes For other",
    "start": "1011440",
    "end": "1018480"
  },
  {
    "text": "cloud providers like AWS and GCP the same process is followed for bootstrapping with the difference being",
    "start": "1018480",
    "end": "1024480"
  },
  {
    "text": "that the control plane could either be hosted or",
    "start": "1024480",
    "end": "1029199"
  },
  {
    "text": "self-managed Now that the control plane is ready we would want the developers to start using the cell Let's take a look",
    "start": "1029720",
    "end": "1036160"
  },
  {
    "text": "at how developers provision nodes for their applications We leverage machine pools very heavily",
    "start": "1036160",
    "end": "1041760"
  },
  {
    "text": "for this which is the primary underlying construct exposed to developers for creation and management of kus nodes",
    "start": "1041760",
    "end": "1048720"
  },
  {
    "text": "Since this would be a very frequent operation we would want to get out of the way of developers as much as we can",
    "start": "1048720",
    "end": "1054720"
  },
  {
    "text": "in the process of creation of machine pools And for this what we have done is we have exposed a generic machine pool",
    "start": "1054720",
    "end": "1061200"
  },
  {
    "text": "helmchart which the developers used to create an ago application deployed via our internal deployment platform in the",
    "start": "1061200",
    "end": "1067360"
  },
  {
    "text": "target cells Now as this ago application is deployed underneath the helm chart is",
    "start": "1067360",
    "end": "1073080"
  },
  {
    "text": "templated with a mix of global helm values and same defaults injected via a kus web hook to create the necessary",
    "start": "1073080",
    "end": "1080080"
  },
  {
    "text": "objects for node creation with the features requested by the developer in the cloud",
    "start": "1080080",
    "end": "1086080"
  },
  {
    "text": "providers As users create these machine pools for their applications let's talk a little bit about the highle overview",
    "start": "1086840",
    "end": "1092960"
  },
  {
    "text": "of nodes in each cell Take for example in AWS cell here we have different",
    "start": "1092960",
    "end": "1098720"
  },
  {
    "text": "groups of nodes We have the general pool which is multi-tenant for teams to use and a user application will land here by",
    "start": "1098720",
    "end": "1107120"
  },
  {
    "text": "default if they don't specify any special requirements for their node Dedicated pools are created by",
    "start": "1107120",
    "end": "1113919"
  },
  {
    "text": "teams when they want specific node features present which are not present in the general pool or when they don't",
    "start": "1113919",
    "end": "1119440"
  },
  {
    "text": "want to be affected by noisy neighbors by getting deployed to the general pool",
    "start": "1119440",
    "end": "1124559"
  },
  {
    "text": "pools are also differentiated by the node feature of the architecture When teams want their applications to land up",
    "start": "1124559",
    "end": "1130960"
  },
  {
    "text": "on a specific architecture is when we allow them to use the machine pool chart to simply create a node or a group of",
    "start": "1130960",
    "end": "1138160"
  },
  {
    "text": "nodes with a specific architecture intended by them Now you might notice that nodes in",
    "start": "1138160",
    "end": "1143760"
  },
  {
    "text": "each of these pools are provided by two compute providers Cappy and Carpenter Carpenter being more efficient in bin",
    "start": "1143760",
    "end": "1150400"
  },
  {
    "text": "packing of application pods inside nodes utilizing groupless autoscaling and optimizing for cost by trying to find",
    "start": "1150400",
    "end": "1156960"
  },
  {
    "text": "the cheapest node to run from the provided configuration at any given",
    "start": "1156960",
    "end": "1162120"
  },
  {
    "text": "point As of now we only have carpenter nodes in AWS cells and we will be expanding it further to the other cloud",
    "start": "1162120",
    "end": "1170039"
  },
  {
    "text": "providers Now that we have created the control plane for the KUS cluster and the necessary node automation creation",
    "start": "1170039",
    "end": "1177120"
  },
  {
    "text": "process is in place for developers to start creating their nodes Let's take a deeper look at scheduling workloads to",
    "start": "1177120",
    "end": "1183520"
  },
  {
    "text": "these nodes To shed some more light with the increasing adoption of Kubernetes we",
    "start": "1183520",
    "end": "1189120"
  },
  {
    "text": "observed a growing variety of scheduling requirements among different teams and applications which made it hard to track",
    "start": "1189120",
    "end": "1195440"
  },
  {
    "text": "of and ultimately impaired our ability to introduce changes to the underlying compute platform in a safe and agile",
    "start": "1195440",
    "end": "1202120"
  },
  {
    "text": "manner Our team aims to solve this issue with scheduling classes a New Relic specific construct which our team",
    "start": "1202120",
    "end": "1208520"
  },
  {
    "text": "provides to all users running on Kubernetes And what is it it's a declarative way to express scheduling",
    "start": "1208520",
    "end": "1214559"
  },
  {
    "text": "requirements for applications without the user knowing all the necessary node labels and taints which the nodes have",
    "start": "1214559",
    "end": "1222880"
  },
  {
    "text": "And how does it work at the heart of it it's an admission controller in the form of a mutating web hook running on",
    "start": "1222880",
    "end": "1228480"
  },
  {
    "text": "specific resources inside the cell And some of the design goals which we had in mind was this should be cloud exhaustic",
    "start": "1228480",
    "end": "1235600"
  },
  {
    "text": "given that we run on multiple clouds and it should not be reinventing the wheel and should build on top of the kus",
    "start": "1235600",
    "end": "1241280"
  },
  {
    "text": "primitives scheduling primitives provided by kus should have sane defaults deterministic and users should",
    "start": "1241280",
    "end": "1246799"
  },
  {
    "text": "be able to chain these scheduling classes on top of each other so as to get multiple outputs",
    "start": "1246799",
    "end": "1254320"
  },
  {
    "text": "On a very high level when a user deploys an application the web hook runs its validation If it needs to do any",
    "start": "1254320",
    "end": "1260559"
  },
  {
    "text": "mutation and if it and if a mutation is required to be done the application gets muted with affinity and tolerations Once",
    "start": "1260559",
    "end": "1267760"
  },
  {
    "text": "this application has this affinity and tolerations the Kululer takes over and then tries finding the best node to",
    "start": "1267760",
    "end": "1273440"
  },
  {
    "text": "schedule the pods for the application And this construct allows us to also run",
    "start": "1273440",
    "end": "1281360"
  },
  {
    "text": "carpenter as well as machine pools together on the same cell For example take this application which is trying to",
    "start": "1281360",
    "end": "1288159"
  },
  {
    "text": "pass feature fu as a requirement via scheduling class Our scheduling class engine then defaults to carpenter",
    "start": "1288159",
    "end": "1293919"
  },
  {
    "text": "scheduling class if it's an AWS cell Since the user hasn't mentioned opting out of it the scheduling class web hooks",
    "start": "1293919",
    "end": "1300000"
  },
  {
    "text": "then add the required affinity and tolerations and then the decator scheduleuler just takes takes it over",
    "start": "1300000",
    "end": "1305280"
  },
  {
    "text": "from there to schedule it to a carpenter node created by the note pool configuration which is present inside the cell",
    "start": "1305280",
    "end": "1312799"
  },
  {
    "text": "Similarly when the application specifies the scheduling class to opt out of carpenter they specify copy as a",
    "start": "1312799",
    "end": "1318559"
  },
  {
    "text": "scheduling class annotation and then the required scheduling constraints get added which would make the scheduleuler",
    "start": "1318559",
    "end": "1323760"
  },
  {
    "text": "schedule the application in a copy pool Now as part of streamlining the",
    "start": "1323760",
    "end": "1329600"
  },
  {
    "text": "upgrade process and management of control control planes each cell has its own AGO CD application where all the",
    "start": "1329600",
    "end": "1336400"
  },
  {
    "text": "related control plane objects are managed We have a homegrown CRD hall called cluster life cycle which targets",
    "start": "1336400",
    "end": "1342960"
  },
  {
    "text": "groups of cells based on environment labels which the custom cluster CRD object is tracking As we change the kus",
    "start": "1342960",
    "end": "1352080"
  },
  {
    "text": "version on this cluster life cycle CRD one of our command command and control controllers reconcile on this object",
    "start": "1352080",
    "end": "1358240"
  },
  {
    "text": "introducing a diff in the kus version attributes of the upstream objects of cubadium control chain object for",
    "start": "1358240",
    "end": "1363919"
  },
  {
    "text": "example and then the upstream controllers take over the reconciliation and upgrade process",
    "start": "1363919",
    "end": "1370320"
  },
  {
    "text": "Similar to the control pane up upgrade standardization we also need to do node",
    "start": "1370320",
    "end": "1375600"
  },
  {
    "text": "refreshes inside our kus clusters which could be due to a variety of reasons and not limited to node upgrades but also",
    "start": "1375600",
    "end": "1382120"
  },
  {
    "text": "patches And what we do is we our two compute providers are cluster API and",
    "start": "1382120",
    "end": "1387919"
  },
  {
    "text": "carpenter in general inside all our cells And to do upgrades what we do is",
    "start": "1387919",
    "end": "1393760"
  },
  {
    "text": "we have a worker configuration CRD which tracks attributes like AMI version version inside each cell And what we",
    "start": "1393760",
    "end": "1401039"
  },
  {
    "text": "have done is we have standardized on top of capy and capy cloud provider APIs where we allow where we enable drift on",
    "start": "1401039",
    "end": "1408320"
  },
  {
    "text": "top of objects of machine deployments AWS machine pools machine pools We also further build on top of the carpenters's",
    "start": "1408320",
    "end": "1414799"
  },
  {
    "text": "node drift feature to not redo what Carpenter already allows us to do by default out of the",
    "start": "1414799",
    "end": "1422399"
  },
  {
    "text": "box We have been running the setup for several years and adding improving features to it Here are a couple of",
    "start": "1423320",
    "end": "1429840"
  },
  {
    "text": "things which we learned which we would like to share Different crappy cloud",
    "start": "1429840",
    "end": "1435520"
  },
  {
    "text": "provider implementations have different versions of Cappy referenced and this is hard to manage On top of managing folks",
    "start": "1435520",
    "end": "1442159"
  },
  {
    "text": "for some bespoke features and fixes which we manage and the challenge of",
    "start": "1442159",
    "end": "1447200"
  },
  {
    "text": "keeping this folks synced with upstream is definitely not an easy task This also",
    "start": "1447200",
    "end": "1452720"
  },
  {
    "text": "brings us to the point of how it's challenging to maintain the automation when different clusters are using",
    "start": "1452720",
    "end": "1458159"
  },
  {
    "text": "different API versions of Cappy and cloud provider objects When it comes to self-managed",
    "start": "1458159",
    "end": "1463919"
  },
  {
    "text": "and hosted on self-hosted clusters it's easier to maintain the K address version",
    "start": "1463919",
    "end": "1469760"
  },
  {
    "text": "parity since you can control the upgrade cadence Whereas in a hosted provider you are tied to the upgrade charter of the",
    "start": "1469760",
    "end": "1475919"
  },
  {
    "text": "vendor which is different across the vendors making it again hard to be on the same version across different cloud",
    "start": "1475919",
    "end": "1481840"
  },
  {
    "text": "providers And this becomes especially challenging if you have components which require a specific version of the CL of",
    "start": "1481840",
    "end": "1488880"
  },
  {
    "text": "the KUS version deployed across these different cells So you can feel how",
    "start": "1488880",
    "end": "1494080"
  },
  {
    "text": "challenging this can get over time There's also lesser control on the management of the control plane",
    "start": "1494080",
    "end": "1500400"
  },
  {
    "text": "components in a vendor envir environment For example if you wanted to pre-warm the control plane components this",
    "start": "1500400",
    "end": "1506320"
  },
  {
    "text": "operation would simply not be possible in some situations and the flexibility",
    "start": "1506320",
    "end": "1511360"
  },
  {
    "text": "that is provided when you're hosting the control plane yourself You can pre-warm depending on when you want to shift the",
    "start": "1511360",
    "end": "1516559"
  },
  {
    "text": "traffic inside a cell almost before the traffic shift is happening Right so this is something which you can control",
    "start": "1516559",
    "end": "1522400"
  },
  {
    "text": "pretty much by yourself More standardization of automation is also possible when the",
    "start": "1522400",
    "end": "1528320"
  },
  {
    "text": "clusters are managed by cubadm managed clusters across cloud providers because the APIs are simply standardized and you",
    "start": "1528320",
    "end": "1534400"
  },
  {
    "text": "are not building on top of cloud provider specific APIs which would mean that you have to manage and maintain",
    "start": "1534400",
    "end": "1539520"
  },
  {
    "text": "them differently bespoke solutions across these cloud providers So you have chances of more automation and more",
    "start": "1539520",
    "end": "1545600"
  },
  {
    "text": "standardization The caveat here being that if you're on self-hosted you are signing up for more work which would in",
    "start": "1545600",
    "end": "1551919"
  },
  {
    "text": "otherwise be taken care of by the cloud provider For example if you want to do uh backups CD HCD management and so on",
    "start": "1551919",
    "end": "1559760"
  },
  {
    "text": "and so forth Furthermore we wanted to just also add about a little bit on our carpenter",
    "start": "1559760",
    "end": "1566720"
  },
  {
    "text": "adoption We have benefited from its groupless autoscaling feature it automatically handling insufficient",
    "start": "1566720",
    "end": "1573600"
  },
  {
    "text": "capacity errors and its ability to do efficient bin packing It also allows us",
    "start": "1573600",
    "end": "1579120"
  },
  {
    "text": "to choose it also chooses the cheapest node possible from a set of configuration which we provide in each",
    "start": "1579120",
    "end": "1584480"
  },
  {
    "text": "cell and to keep the costs low In cases when the teams are not able to",
    "start": "1584480",
    "end": "1590640"
  },
  {
    "text": "handle the consolidation rate of carpenter we allow them to opt out via our scheduling class construct and they",
    "start": "1590640",
    "end": "1596640"
  },
  {
    "text": "can easily opt out balancing the act of reliability with cost management at the same",
    "start": "1596640",
    "end": "1601720"
  },
  {
    "text": "time We plan to further expand on carpenter on the other cloud providers and",
    "start": "1601720",
    "end": "1606919"
  },
  {
    "text": "we this is something which we want to do in future With that thanks and we would",
    "start": "1606919",
    "end": "1613760"
  },
  {
    "text": "love to hear questions from you and take care",
    "start": "1613760",
    "end": "1619600"
  },
  {
    "text": "After we miss this uh we would like to mention that um there's a T contributor",
    "start": "1624520",
    "end": "1631440"
  },
  {
    "text": "strategy launching a new mentorship program So if you are interested in mentoring people uh from under",
    "start": "1631440",
    "end": "1638400"
  },
  {
    "text": "representing groups please sign up",
    "start": "1638400",
    "end": "1643159"
  },
  {
    "text": "Okay Uh hi Hey um I very much appreciated uh the talk or was an eye",
    "start": "1645760",
    "end": "1651679"
  },
  {
    "text": "openener for me Uh I have plenty questions but maybe two of them now Um",
    "start": "1651679",
    "end": "1657600"
  },
  {
    "text": "you talked about this bootstrapper uh component in the management cluster What is behind that like you mean the",
    "start": "1657600",
    "end": "1665679"
  },
  {
    "text": "bootstrapping of the katus cluster uh well the the stuff you have to do before",
    "start": "1665679",
    "end": "1671440"
  },
  {
    "text": "you can initialize cluster API I guess it is it was like in the middle of your graph All right So uh during the cell",
    "start": "1671440",
    "end": "1678000"
  },
  {
    "text": "build process there are a couple of initialization steps inside the account",
    "start": "1678000",
    "end": "1683840"
  },
  {
    "text": "of the cloud provider which we need to do This is dependent on what type of cell it is",
    "start": "1683840",
    "end": "1690120"
  },
  {
    "text": "and what the requirement of that cell is As Harry mentioned there are different cell types We have a variety of cell",
    "start": "1690120",
    "end": "1696399"
  },
  {
    "text": "types and we need certain things to be there before even the initialization of",
    "start": "1696399",
    "end": "1701840"
  },
  {
    "text": "the kadis cluster and the reconciliation process starts So",
    "start": "1701840",
    "end": "1707360"
  },
  {
    "text": "um I mean to summarize I think it's it depends on what cloud provider it is and",
    "start": "1707720",
    "end": "1713679"
  },
  {
    "text": "what cell type it is and what is the use of that cell type because depending on that there are there are other teams",
    "start": "1713679",
    "end": "1719440"
  },
  {
    "text": "which try doing a lot of other things before this for example AWS what is the",
    "start": "1719440",
    "end": "1724480"
  },
  {
    "text": "technology behind that is it like your own controller is it I don't know crossplay terraform this is homegrown",
    "start": "1724480",
    "end": "1730399"
  },
  {
    "text": "yeah okay you you mean before before what we do the kus bootstrapping I think",
    "start": "1730399",
    "end": "1735760"
  },
  {
    "text": "you were asking what we do before that's homegrown Yeah Okay Yeah Okay Any other",
    "start": "1735760",
    "end": "1741279"
  },
  {
    "text": "questions and you do cluster cuttle move of the copy resources the copper",
    "start": "1741279",
    "end": "1747919"
  },
  {
    "text": "resources into the you know workflow cluster Do you do that for any other",
    "start": "1747919",
    "end": "1753039"
  },
  {
    "text": "resources as well like do you modify cluster cuttle move to extend it make it",
    "start": "1753039",
    "end": "1758799"
  },
  {
    "text": "move even more custom resources such as cluster life cycle configuration or",
    "start": "1758799",
    "end": "1764080"
  },
  {
    "text": "something so uh when we do the cluster ctl move operation everything which has",
    "start": "1764080",
    "end": "1770399"
  },
  {
    "text": "so there are two steps to reconciliation one thing which I didn't mention too much due to this lack of time was um",
    "start": "1770399",
    "end": "1777120"
  },
  {
    "text": "during the checkpoint process of kus cluster creation and the point at which",
    "start": "1777120",
    "end": "1782360"
  },
  {
    "text": "the job succeeds There is another phase where reconciliation of other steps",
    "start": "1782360",
    "end": "1788320"
  },
  {
    "text": "start happening via the command and control cell controllers But just before the move operation we have everything",
    "start": "1788320",
    "end": "1794720"
  },
  {
    "text": "which is then required for example say the cluster API and the cloud provider installation and the necessary things",
    "start": "1794720",
    "end": "1801120"
  },
  {
    "text": "which we deem necessary for the cluster to function at this point and reconciliation to start happening",
    "start": "1801120",
    "end": "1806840"
  },
  {
    "text": "Furthermore all those objects are moved away and moved to the target cell and the job concludes Um so at this point",
    "start": "1806840",
    "end": "1814399"
  },
  {
    "text": "the one checkpoint has finished and the next checkpoint starts over if that makes sense and we track this inside uh",
    "start": "1814399",
    "end": "1820320"
  },
  {
    "text": "a specific CRD which we have on the different uh states which the cluster",
    "start": "1820320",
    "end": "1825600"
  },
  {
    "text": "build processes at in this point I don't know if that answers your question No that's fine Thank you very much No problem",
    "start": "1825600",
    "end": "1833159"
  },
  {
    "text": "Hello there uh from what I understood a cell is declared to be um ephemeral If",
    "start": "1833360",
    "end": "1840559"
  },
  {
    "text": "you don't have control about which application is running in the cell how",
    "start": "1840559",
    "end": "1845760"
  },
  {
    "text": "do you migrate data if a cell vanishes and a new one is created that's a good",
    "start": "1845760",
    "end": "1852080"
  },
  {
    "text": "question So and that's a tough one actually Well you you control what which",
    "start": "1852080",
    "end": "1858480"
  },
  {
    "text": "are the applications the service that you are going to deploy in that cell because you need to define that cell type Now it could be uh a stateless cell",
    "start": "1858480",
    "end": "1866799"
  },
  {
    "text": "type that's the that the best uh option right and then you don't need to worry",
    "start": "1866799",
    "end": "1872640"
  },
  {
    "text": "about that but in our case we first during the journey we have some stateful",
    "start": "1872640",
    "end": "1877960"
  },
  {
    "text": "cells we put some automation tooling for uh when it was the point to decommission",
    "start": "1877960",
    "end": "1884399"
  },
  {
    "text": "that decommission in that cell and migrate the data to another cell we put some automation to do that that's tough",
    "start": "1884399",
    "end": "1891039"
  },
  {
    "text": "so along the way and this you know uh across several iterations",
    "start": "1891039",
    "end": "1896880"
  },
  {
    "text": "you uh try to decouple that stful part into an a a different specific cell And",
    "start": "1896880",
    "end": "1904000"
  },
  {
    "text": "maybe instead being ephemereral you can put it in a more stateful permanent cell",
    "start": "1904000",
    "end": "1909039"
  },
  {
    "text": "For us we have some special cells we call them incl Um we made them very",
    "start": "1909039",
    "end": "1915120"
  },
  {
    "text": "resilient in terms of very moving few moving components let's say",
    "start": "1915120",
    "end": "1922320"
  },
  {
    "text": "uh instead of having a huge amount of services running there Um but basically yeah we we're taking out that stful to",
    "start": "1922320",
    "end": "1929120"
  },
  {
    "text": "another cell type and then we make the original cell more stateless and that",
    "start": "1929120",
    "end": "1934159"
  },
  {
    "text": "way when it comes to uh the commissioning and uh yeah build new",
    "start": "1934159",
    "end": "1940240"
  },
  {
    "text": "cells it's easier Thank you It's not a it's not a an answer that fits any situation So you",
    "start": "1940240",
    "end": "1948399"
  },
  {
    "text": "need to take your answers along the way It's the same with the SL router So how do you do the cell router it depends for",
    "start": "1948399",
    "end": "1955200"
  },
  {
    "text": "on your workloads and your traffic uh characteristics Let's say",
    "start": "1955200",
    "end": "1961679"
  },
  {
    "text": "thank you this on Hi Um that was my question",
    "start": "1961679",
    "end": "1971120"
  },
  {
    "text": "actually I was going to ask if you could explain a little bit more about how your cell routting works cuz I mean I don't",
    "start": "1971120",
    "end": "1978080"
  },
  {
    "text": "know if I understood this properly but it seems like how I mean how do you decide when a user hits your API which",
    "start": "1978080",
    "end": "1986480"
  },
  {
    "text": "cell they should be rooted to if each cell hold like is owns their own data",
    "start": "1986480",
    "end": "1993279"
  },
  {
    "text": "Can you hear the can you repeat a bit uh with more volume sorry I can Yeah sorry",
    "start": "1993279",
    "end": "2000679"
  },
  {
    "text": "Um so when a user hits your API Yep Um",
    "start": "2000679",
    "end": "2006399"
  },
  {
    "text": "how do you decide could you just talk a little bit more about how the cell routting works like how do you decide",
    "start": "2006399",
    "end": "2013039"
  },
  {
    "text": "which cell a user request should be directed to ah cool About the cell routing you mean",
    "start": "2013039",
    "end": "2019919"
  },
  {
    "text": "sure So in our case we have a mechanism",
    "start": "2019919",
    "end": "2025000"
  },
  {
    "text": "um analyzing the heers of traffic from customers",
    "start": "2025000",
    "end": "2030159"
  },
  {
    "text": "uh based on the data type and on the customer ID and API key Uh and with that",
    "start": "2030159",
    "end": "2036640"
  },
  {
    "text": "we decide to which cell we should redirect uh that traffic coming from our",
    "start": "2036640",
    "end": "2043720"
  },
  {
    "text": "customer basically Uh so that means that for instance for a specific customer we",
    "start": "2043720",
    "end": "2049599"
  },
  {
    "text": "are able to shift traffic uh depending on the data type to different cells You",
    "start": "2049599",
    "end": "2054800"
  },
  {
    "text": "could we could have been migrating um routing traffic into for metrics for",
    "start": "2054800",
    "end": "2060878"
  },
  {
    "text": "cell A while we could be uh redirecting traffic from logs to cell B and we do it",
    "start": "2060879",
    "end": "2067118"
  },
  {
    "text": "that with that that taple of data uh data type and customer ID in a sense uh",
    "start": "2067119",
    "end": "2073760"
  },
  {
    "text": "it could be a an presentation on its own but that's",
    "start": "2073760",
    "end": "2079280"
  },
  {
    "text": "in a sense the the mechanism thanks um So is that more is that",
    "start": "2079280",
    "end": "2086839"
  },
  {
    "text": "like so if it's done with the user ID say for example if you're rooting based",
    "start": "2086839",
    "end": "2092200"
  },
  {
    "text": "on ID or something to oversimplify yes but it's not user ID it's customer ID",
    "start": "2092200",
    "end": "2098320"
  },
  {
    "text": "and we do some more uh operations there but in a sense that's the idea yeah okay",
    "start": "2098320",
    "end": "2104160"
  },
  {
    "text": "so I mean like you have some ID that tells you where it should go is that",
    "start": "2104160",
    "end": "2109760"
  },
  {
    "text": "does each of the cells like publish what what ids they're responsible for or do you record it like",
    "start": "2109760",
    "end": "2116960"
  },
  {
    "text": "when no we actually save that on a specific repository let's say so when it",
    "start": "2116960",
    "end": "2122400"
  },
  {
    "text": "comes to uh reach out to that data uh for the query part first you need to",
    "start": "2122400",
    "end": "2127599"
  },
  {
    "text": "know uh regarding that query where is the data located would be in several points right uh and we do that with a",
    "start": "2127599",
    "end": "2134960"
  },
  {
    "text": "specific um database internally uh that saves all those uh all that meta data",
    "start": "2134960",
    "end": "2142320"
  },
  {
    "text": "let's Okay Thank you Thank",
    "start": "2142320",
    "end": "2147760"
  },
  {
    "text": "welcome What else thank you a lot Thanks Thanks",
    "start": "2148359",
    "end": "2155960"
  }
]