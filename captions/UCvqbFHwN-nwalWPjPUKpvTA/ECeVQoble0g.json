[
  {
    "text": "hello everyone that's a lot of people let's do it",
    "start": "60",
    "end": "6270"
  },
  {
    "text": "my name is Linda Singh with my colleague here and mol we're gonna talk about how we run stateful workloads at scale on",
    "start": "6270",
    "end": "12599"
  },
  {
    "text": "Kuban it is at lyft and let's see so rough agenda of the talk is going to be",
    "start": "12599",
    "end": "18210"
  },
  {
    "text": "will first define what those stateful workloads are we'll give some examples we'll go through some use cases at left",
    "start": "18210",
    "end": "24630"
  },
  {
    "text": "and then we're going to talk about our platform flight so flight is our in-house build platform where we're on",
    "start": "24630",
    "end": "31260"
  },
  {
    "text": "most of the lifts straight for workloads we do have a full presentation on flight",
    "start": "31260",
    "end": "36270"
  },
  {
    "text": "at 5:20 p.m. I will request everyone to go attend that but for the sake of this conversation we'll go over the",
    "start": "36270",
    "end": "42510"
  },
  {
    "text": "high-level we'll make sure that everyone has the context for the rest of the talk and then we'll move on to what were the",
    "start": "42510",
    "end": "48149"
  },
  {
    "text": "platform goals when we were defining when we were designing that platform for those stateful workloads and then we",
    "start": "48149",
    "end": "55410"
  },
  {
    "text": "will move on to what were the pitfalls and learnings we had while running this",
    "start": "55410",
    "end": "60780"
  },
  {
    "text": "platform on human it is for almost a year now and what did we do for the issues we encountered we'll go over some",
    "start": "60780",
    "end": "67260"
  },
  {
    "text": "of the issues which we think are interested will be interesting for everyone and then in the end we will",
    "start": "67260",
    "end": "72869"
  },
  {
    "text": "move to our multi cluster set up and towards the end we'll also have some time for questions",
    "start": "72869",
    "end": "79250"
  },
  {
    "text": "what are these stateful workers I'm talking about so think of a stateful workload as a job",
    "start": "79340",
    "end": "85080"
  },
  {
    "text": "long-running job which has some discrete steps and each step you want to store some state and there is some state",
    "start": "85080",
    "end": "90509"
  },
  {
    "text": "passing which happens from one task to another example could be think of a spark or a fling task running on a",
    "start": "90509",
    "end": "96960"
  },
  {
    "text": "cadence running on a schedule with let's say trigger time being as an input of the job another example is a very",
    "start": "96960",
    "end": "103170"
  },
  {
    "text": "standard detailed or where there's a raw data you load that data you extract load",
    "start": "103170",
    "end": "109560"
  },
  {
    "text": "and then I put it into a form that can be consumed by other users at the company and then yet another one is data",
    "start": "109560",
    "end": "116369"
  },
  {
    "text": "things pipelines we have the raw data we extract features and then data scientists come and work on models once",
    "start": "116369",
    "end": "123750"
  },
  {
    "text": "they have a good model they push it to prod for influencing and some examples",
    "start": "123750",
    "end": "130259"
  },
  {
    "text": "from lift as you guys know that by the virtue of what we do optimization is an important problem for",
    "start": "130259",
    "end": "135960"
  },
  {
    "text": "us and to do good at it we have machine learning models everywhere for pricing",
    "start": "135960",
    "end": "140970"
  },
  {
    "text": "for ETA for locations for Maps data and these models some of them are really complex and then the platform was",
    "start": "140970",
    "end": "148710"
  },
  {
    "text": "designed so that we can onboard all of these at the same time so most of these models run on flight",
    "start": "148710",
    "end": "154230"
  },
  {
    "text": "there are ETL jobs running on flight then there are data backup jobs what what they do is they do a periodic",
    "start": "154230",
    "end": "161130"
  },
  {
    "text": "backup of the data a logical replication of the data with some check pointing in between they also run in flight today",
    "start": "161130",
    "end": "166230"
  },
  {
    "text": "and then in the end we also have some like giant simulation framework why do",
    "start": "166230",
    "end": "172110"
  },
  {
    "text": "we have that as since I said we are not of the data science pipelines in the company and we cannot always run them in",
    "start": "172110",
    "end": "178200"
  },
  {
    "text": "prod to see how good they are doing so we have very sophisticated simulation system but sophistication means",
    "start": "178200",
    "end": "183450"
  },
  {
    "text": "complexity and then we actually have a challenging journey to support these simulation workloads on flight so today",
    "start": "183450",
    "end": "190380"
  },
  {
    "text": "they run on flight like every day tons of them ok moving on",
    "start": "190380",
    "end": "195720"
  },
  {
    "text": "what is flight so flight is a platform which was designed to orchestrate machine learning and data workflows so",
    "start": "195720",
    "end": "204239"
  },
  {
    "text": "this was the primary goal but secondary goal was also ensure that there is good collaboration reuse and performing those",
    "start": "204239",
    "end": "210570"
  },
  {
    "text": "ml operations which in general has to do every day we wanted to make that super easy for the engineers let me go over",
    "start": "210570",
    "end": "217050"
  },
  {
    "text": "some of the cool features so I just came through the these features and then we're going to talk about these in the",
    "start": "217050",
    "end": "222450"
  },
  {
    "text": "later slides first one of our server list we wanted to be server less user should not be worried about what's",
    "start": "222450",
    "end": "227730"
  },
  {
    "text": "behind the scene multi-tenant and shareable operational excellence was a",
    "start": "227730",
    "end": "232920"
  },
  {
    "text": "requirement for us because being a very generic platform we want to make sure that people running on a platform they",
    "start": "232920",
    "end": "238470"
  },
  {
    "text": "can run their things they can look into things they can debug diagnose things whenever they need extensibility was",
    "start": "238470",
    "end": "243900"
  },
  {
    "text": "important this was our learning from a previous incarnation of flight where we notice that anytime we have to onward a",
    "start": "243900",
    "end": "249930"
  },
  {
    "text": "new use case we wanted to do something special so we wanted to build a platform where extensibility is a cold feature of",
    "start": "249930",
    "end": "255540"
  },
  {
    "text": "it and the last but not the least is the interaction modes today we have G RPC rest endpoint we",
    "start": "255540",
    "end": "262410"
  },
  {
    "text": "also have a UI for manual interaction with the system so some more concepts tasks is the",
    "start": "262410",
    "end": "271290"
  },
  {
    "text": "atomic unit of a flight workflow think of it like a single container toss which",
    "start": "271290",
    "end": "276690"
  },
  {
    "text": "could be a Java or a Python program or it could be a multi container job think",
    "start": "276690",
    "end": "281970"
  },
  {
    "text": "of it less bunk job when I say a task as a atomic unit which means we cannot decompose that further so you use those",
    "start": "281970",
    "end": "288180"
  },
  {
    "text": "tasks you wrap them with what are the resources you need what is the relationship with the rest of the nodes",
    "start": "288180",
    "end": "293370"
  },
  {
    "text": "and that becomes a note for you and you take those bunch of nodes do you define a graph it's called a work flow in our",
    "start": "293370",
    "end": "299580"
  },
  {
    "text": "system and so workflow is set of nodes some dependencies between them and both",
    "start": "299580",
    "end": "304740"
  },
  {
    "text": "tasks and workflows can have inputs as well as outputs okay this is a pictorial",
    "start": "304740",
    "end": "311220"
  },
  {
    "text": "depiction of our graph looks like on the left you see a input node which",
    "start": "311220",
    "end": "316710"
  },
  {
    "text": "is the input to the workflow then there arrows going towards right which means that those nodes on the right they need",
    "start": "316710",
    "end": "322410"
  },
  {
    "text": "those inputs and then wherever you see an arrow between two nodes which means there is a dependency between those",
    "start": "322410",
    "end": "328200"
  },
  {
    "text": "nodes this is a UI but primitive form we are working on it and then there are links to logs and other things on the UI",
    "start": "328200",
    "end": "334620"
  },
  {
    "text": "- okay let's dive a little deeper so",
    "start": "334620",
    "end": "340169"
  },
  {
    "text": "this is the block diagram of a flight system on the left you see as a user print component there is a CLI there is",
    "start": "340169",
    "end": "346650"
  },
  {
    "text": "a flight client there is an SDK to interact with the system in the middle there is a flight admin that is a",
    "start": "346650",
    "end": "351810"
  },
  {
    "text": "control plane even though in this diagram it seems like it's part of the execution cluster but it is not so we",
    "start": "351810",
    "end": "358380"
  },
  {
    "text": "made a conscious choice to move our control plane out of the execution plane on the right you see are the components",
    "start": "358380",
    "end": "364380"
  },
  {
    "text": "of our execution plane where there is a flight propeller and then the bunch of other operators and services flight",
    "start": "364380",
    "end": "370800"
  },
  {
    "text": "propeller is the brain or hard whatever you want to call it or the end of the system where what it does is it goes through each workflow submitted to the",
    "start": "370800",
    "end": "377700"
  },
  {
    "text": "system looks at what is the current state if there is this action to be taken it takes that action and sort that",
    "start": "377700",
    "end": "383430"
  },
  {
    "text": "state in HCD and then moves on so we",
    "start": "383430",
    "end": "390600"
  },
  {
    "text": "looked at the overall picture there are many components in system I don't want to overwhelm you guys with too much details but for the rest of the talk the",
    "start": "390600",
    "end": "397260"
  },
  {
    "text": "thing which is really important for us to understand this flight propeller because that is the guy who has a single cluster view so it's running inside a cluster it",
    "start": "397260",
    "end": "403830"
  },
  {
    "text": "is the guy interacting with the rest of the Kuban it is components and what it does it it looks through all the works",
    "start": "403830",
    "end": "408900"
  },
  {
    "text": "within the system it has a it's a standard cuban area so operator there is a control loop where it goes through all",
    "start": "408900",
    "end": "415860"
  },
  {
    "text": "this workflow symmetry to the system looks at the current state parses the graph traverses the graph gets to the",
    "start": "415860",
    "end": "422040"
  },
  {
    "text": "node which is in progress updates the status and moves on to the next one so it keeps doing that in a loop till we",
    "start": "422040",
    "end": "428130"
  },
  {
    "text": "have the workflow in the completion state and it uses HCD at the States or because we wanted something consistent",
    "start": "428130",
    "end": "434280"
  },
  {
    "text": "over there and since you can understand this is part of the critical path because all the workflows are going",
    "start": "434280",
    "end": "440220"
  },
  {
    "text": "through this guide so this was kind of optimized for more throughput and scale",
    "start": "440220",
    "end": "445640"
  },
  {
    "text": "okay and this is the input to the system so when I say flight propeller works on the workflows workflows are represented",
    "start": "445640",
    "end": "451920"
  },
  {
    "text": "at the customer resource so if you look at the customer resource here there are nodes nodes have names they are kind and",
    "start": "451920",
    "end": "459000"
  },
  {
    "text": "there are inputs to the node and then there are bindings bindings is a way where you kind of say this is my",
    "start": "459000",
    "end": "464820"
  },
  {
    "text": "relationship of this node with the other node and then there are inputs if you just look at the input there is you see",
    "start": "464820",
    "end": "470850"
  },
  {
    "text": "a structure over there this is because we have very strongly typed inputs in the system and most the inputs are today",
    "start": "470850",
    "end": "476370"
  },
  {
    "text": "or proto based and you will see that there are inputs for the workflow as well for the nodes the second part of",
    "start": "476370",
    "end": "485670"
  },
  {
    "text": "the cid is the status status is not given by the user but updated by a flight propeller as we progress with the",
    "start": "485670",
    "end": "490800"
  },
  {
    "text": "workflow we keep updating the status with more information when we have it available we update the node status when",
    "start": "490800",
    "end": "496050"
  },
  {
    "text": "we have it we write output directories where we are pushing the output and any",
    "start": "496050",
    "end": "502620"
  },
  {
    "text": "other information we have so one thing I'll call it so output I say we have it available because we actually ensure",
    "start": "502620",
    "end": "509160"
  },
  {
    "text": "that when the task runs completely then only we make the output available this was needed to avoid partial failures and",
    "start": "509160",
    "end": "519380"
  },
  {
    "text": "we are open source since last month we have a good integration with a bunch of open source technologies we have a spark",
    "start": "519380",
    "end": "526260"
  },
  {
    "text": "Jupiter notebook hi q-ball and then we also today run on AWS and Google",
    "start": "526260",
    "end": "531320"
  },
  {
    "text": "there are more things coming so next is platform goals I'll ask unroll to walk you through platform goals and some of",
    "start": "531320",
    "end": "537590"
  },
  {
    "text": "the other things Thank You cylinder now that we have a good idea of the kind of spiritual workloads we're on that lift",
    "start": "537590",
    "end": "543560"
  },
  {
    "text": "as well as the platform flight we used to run them let's talk about some of the goals we had in mind we're designed well",
    "start": "543560",
    "end": "549830"
  },
  {
    "text": "design in flight okay so scale so straight field jobs usually present a very different set of load",
    "start": "549830",
    "end": "555560"
  },
  {
    "text": "characteristics compared to long running services in the sense that the load is usually very bursty as can be seen on",
    "start": "555560",
    "end": "561230"
  },
  {
    "text": "the chart on the right at live for stateful workloads we run or we start thousands of containers per minute as",
    "start": "561230",
    "end": "567020"
  },
  {
    "text": "well as we run tens of millions of containers per month so the next goal we wanted to talk about is multi-tenancy",
    "start": "567020",
    "end": "574750"
  },
  {
    "text": "again at left using flight we have hundreds of users using our system or",
    "start": "574750",
    "end": "580730"
  },
  {
    "text": "platform concurrently running tasks on them across multiple teams so you want to make sure that there's isolation and",
    "start": "580730",
    "end": "586430"
  },
  {
    "text": "fairness in the system what that means is you want to make sure that one user of the system is not able to impact",
    "start": "586430",
    "end": "592070"
  },
  {
    "text": "other users running tasks or workflows at the same time you also want to make sure that one users or one user or a set",
    "start": "592070",
    "end": "598340"
  },
  {
    "text": "of users or a team can only use a small slice of overall you know resources or",
    "start": "598340",
    "end": "604160"
  },
  {
    "text": "combinational power of the platform a couple of other goals we want to just",
    "start": "604160",
    "end": "610340"
  },
  {
    "text": "walk you guys through our performance and extensibility so flight has multiple components its flight admin this flight",
    "start": "610340",
    "end": "616820"
  },
  {
    "text": "propeller so in the just mentioned so you want to make sure that every time a workflow or a task is getting processed",
    "start": "616820",
    "end": "622270"
  },
  {
    "text": "by one of these system components there is a minimum system overhead the processing time is minimized this can be",
    "start": "622270",
    "end": "628340"
  },
  {
    "text": "the initial setup time where the user actually when it when he or she launches a workflow the time it takes from that",
    "start": "628340",
    "end": "634490"
  },
  {
    "text": "that chap to the time the tasks are actually running in the system as well as the transition time between different",
    "start": "634490",
    "end": "639890"
  },
  {
    "text": "states of the workflow when it's getting driven by flight propeller extensibility",
    "start": "639890",
    "end": "645500"
  },
  {
    "text": "we also want to make sure that the platform is easily extensible and allows users to add support for new task types",
    "start": "645500",
    "end": "652280"
  },
  {
    "text": "so in the future as new as new use cases crop up and new solutions to those use cases crop up we want to make sure that",
    "start": "652280",
    "end": "659230"
  },
  {
    "text": "users of flight are able to add support for those use cases and those solutions",
    "start": "659230",
    "end": "664370"
  },
  {
    "text": "without having to make any platform level changes okay so the last set of",
    "start": "664370",
    "end": "671600"
  },
  {
    "text": "goals so you want to just quickly walk you guys through our observability and visibility what it means is we want to",
    "start": "671600",
    "end": "676970"
  },
  {
    "text": "make sure users or teams running on flight have insight into how many workflows they are running how many",
    "start": "676970",
    "end": "682790"
  },
  {
    "text": "tasks they are running but not only that we also want to make sure they have insight into the resource utilization for those workers or those stars for",
    "start": "682790",
    "end": "690259"
  },
  {
    "text": "example if you as a user are running a spark job which asks for hundred executors with 50 gigs of memory each",
    "start": "690259",
    "end": "695660"
  },
  {
    "text": "but you are actually only consuming five gigs or just using 10% of the memory you requested for we want to make sure you",
    "start": "695660",
    "end": "702410"
  },
  {
    "text": "have you get that insight into that this sort of also stems into the next next",
    "start": "702410",
    "end": "707480"
  },
  {
    "text": "goal which is the infrastructure cost optimization so once you have insight into how much resources you are actually using you can actually go and optimize",
    "start": "707480",
    "end": "714350"
  },
  {
    "text": "for infrastructure cost but besides that even at the system level or the platform",
    "start": "714350",
    "end": "719779"
  },
  {
    "text": "level we want we are looking at a couple of things to optimize the overall cost like you know exploring low cost spot",
    "start": "719779",
    "end": "726709"
  },
  {
    "text": "instances as well as trying to optimize based on various utilization patterns",
    "start": "726709",
    "end": "732730"
  },
  {
    "text": "okay so those were the goals we had in mind while designing flight so now that we have flight running in production for",
    "start": "733480",
    "end": "740630"
  },
  {
    "text": "close to a year and we have a large number of lifts you know you stateful",
    "start": "740630",
    "end": "745730"
  },
  {
    "text": "workloads running by a flight let us talk about some of the issues we ran into and in some of the learnings we had based on in the last year okay so the",
    "start": "745730",
    "end": "756860"
  },
  {
    "text": "first few learnings are around scale while when we actually ran flight on single humanity's cluster so the very",
    "start": "756860",
    "end": "763579"
  },
  {
    "text": "first issue when we ran flight and started onboarding new users is that we started seeing high latency for API",
    "start": "763579",
    "end": "770059"
  },
  {
    "text": "server api server calls what happened is whenever the aggregate count of Cuban",
    "start": "770059",
    "end": "775519"
  },
  {
    "text": "IDs objects on the cluster goes beyond a certain threshold we start seeing that our API server calls like period pod",
    "start": "775519",
    "end": "781250"
  },
  {
    "text": "etcetera were were really getting hit so the so the overall aggregate count of",
    "start": "781250",
    "end": "786920"
  },
  {
    "text": "communities resources can be because of the parts we are creating for various tasks or workflows in the system or can",
    "start": "786920",
    "end": "793020"
  },
  {
    "text": "the workflow CRTs or any dependent CRTs like you know spark CRTs we create for Sparta's or any other kubernetes system",
    "start": "793020",
    "end": "800280"
  },
  {
    "text": "level resources like config maps secrets etc besides this we also started seeing",
    "start": "800280",
    "end": "805830"
  },
  {
    "text": "that when we are running thousands of workflows concurrently CUBAN it is was really garbage collecting completed pods",
    "start": "805830",
    "end": "812850"
  },
  {
    "text": "really aggressively and what happened what this leads to what this causes is",
    "start": "812850",
    "end": "818580"
  },
  {
    "text": "flight propeller which is sort of the brain and looks at the status of these parts it has spawned and to do basically",
    "start": "818580",
    "end": "826370"
  },
  {
    "text": "to drive the workflow from one state to the next so a fixie is that hey I spawned these five part for this",
    "start": "826370",
    "end": "832020"
  },
  {
    "text": "particular task in a workflow and these five parts are done its able to mark the task as done and you know spawn new",
    "start": "832020",
    "end": "838260"
  },
  {
    "text": "tasks or any dependent tasks in the workflow but when Kuban it is when at scale",
    "start": "838260",
    "end": "843690"
  },
  {
    "text": "Kuban it is really aggressively garbage collect so deletes these completed parts or these pod resources flight propeller",
    "start": "843690",
    "end": "850680"
  },
  {
    "text": "is not able to consume that that these parts are completed and the work flows and ended up being failing even though",
    "start": "850680",
    "end": "857340"
  },
  {
    "text": "the parts might have the successfully completed so these were the two issues",
    "start": "857340",
    "end": "862530"
  },
  {
    "text": "we saw at the cluster level we also saw an interesting issue at particulate",
    "start": "862530",
    "end": "868050"
  },
  {
    "text": "level which i want to share with you guys so we when we started we we were",
    "start": "868050",
    "end": "873390"
  },
  {
    "text": "using large machine instance types we were using I think machine instances with half a terabyte of memory and I",
    "start": "873390",
    "end": "878640"
  },
  {
    "text": "think around 60 or 70 CPUs and some of our workloads or user workloads were",
    "start": "878640",
    "end": "883920"
  },
  {
    "text": "really small they were requesting pods with just half CPU resource so what ended up happening is the scheduler the",
    "start": "883920",
    "end": "890880"
  },
  {
    "text": "Kuban it a scheduler ended up placing a large number of these pods on a single cube light I think that was around 150",
    "start": "890880",
    "end": "896580"
  },
  {
    "text": "or so pods were placed on a single bit and we started seeing various qubit level errors and issues because of that",
    "start": "896580",
    "end": "903620"
  },
  {
    "text": "for example you start seeing cryo errors in cubelet logs as well as we start seeing out of memory errors even though",
    "start": "903620",
    "end": "909900"
  },
  {
    "text": "as far as we could investigate our pods or the container within the pods were completing successfully okay so these",
    "start": "909900",
    "end": "917580"
  },
  {
    "text": "were sort of the three crystal level issues we ran into so how did we solve them so just to limit the oral",
    "start": "917580",
    "end": "925110"
  },
  {
    "text": "aggregate count of community resources or objects in the cluster we started using resource quotas to limit the",
    "start": "925110",
    "end": "932370"
  },
  {
    "text": "number of pods or number of CRTs missing in a single cluster besides that we also",
    "start": "932370",
    "end": "937620"
  },
  {
    "text": "started aggressively garbage collecting our own computed workflows so these workflows which have completed there are",
    "start": "937620",
    "end": "944220"
  },
  {
    "text": "no more state relations for them and they can be safely deleted so we started aggressively deleting those workflows as",
    "start": "944220",
    "end": "949410"
  },
  {
    "text": "well as any ports any spark CRTs or any other dependent resources that particular workflow owns finally to",
    "start": "949410",
    "end": "957329"
  },
  {
    "text": "solve the cube let level issue we had we started using heterogeneous machine pools where some of the instance some of",
    "start": "957329",
    "end": "962880"
  },
  {
    "text": "the instances we chose were smaller ones some of the instances were larger ones and overall we we were under that pod",
    "start": "962880",
    "end": "969660"
  },
  {
    "text": "limit like what limit of 100 parts per per node which we which we saw that",
    "start": "969660",
    "end": "975839"
  },
  {
    "text": "works fine without any stability issues okay so those were the cluster level",
    "start": "975839",
    "end": "983160"
  },
  {
    "text": "issues and learnings so let's talk about some of the state management level",
    "start": "983160",
    "end": "989040"
  },
  {
    "text": "learnings we had while running flight at scale for these stateful workloads so as Sauron the talked about flight propeller",
    "start": "989040",
    "end": "996480"
  },
  {
    "text": "is an cuban it is operator which implements a control loop and is responsible for driving each workflow",
    "start": "996480",
    "end": "1003519"
  },
  {
    "text": "from one state to the next and you mentioned to completion so what happened when we ran sort of a load test from the",
    "start": "1003519",
    "end": "1009589"
  },
  {
    "text": "very initial v1 version of flight propeller for the scale we wanted to support we started seeing that this",
    "start": "1009589",
    "end": "1015890"
  },
  {
    "text": "processing time the propeller takes for each workflow ceoddi was really high and",
    "start": "1015890",
    "end": "1021079"
  },
  {
    "text": "was beyond the sort of I suppose we wanted to expose your users so so when",
    "start": "1021079",
    "end": "1027500"
  },
  {
    "text": "we investigated this we saw that this was largely because of the eight seed actually write x for the work flow CRD",
    "start": "1027500",
    "end": "1034750"
  },
  {
    "text": "and those write x actually write hands were really high so how did we go about",
    "start": "1034750",
    "end": "1040339"
  },
  {
    "text": "solving them one of the ways we actually solved it is by implementing a version",
    "start": "1040339",
    "end": "1046400"
  },
  {
    "text": "cache where we will write the we will store the last written version to a CD in a separate cache and if we are",
    "start": "1046400",
    "end": "1053450"
  },
  {
    "text": "getting an older version for that CRD from Informer we will just discard it",
    "start": "1053450",
    "end": "1058730"
  },
  {
    "text": "thereby saving time on processing time as well as you know saving time on trying to write to HDD we also started",
    "start": "1058730",
    "end": "1066710"
  },
  {
    "text": "using updating the CR D using the sub resource update API which recently went",
    "start": "1066710",
    "end": "1072860"
  },
  {
    "text": "live in one of the recent Cuba Nerdist versions so this basically for some of our workflow CRTs the spec part can be",
    "start": "1072860",
    "end": "1078920"
  },
  {
    "text": "really huge we saw that some of our users were writing really complex workflows which had like hundreds of",
    "start": "1078920",
    "end": "1084770"
  },
  {
    "text": "tasks and the spec in some of these cases can go into mb/s so by just using by this updating that's the status part",
    "start": "1084770",
    "end": "1091280"
  },
  {
    "text": "of it we could say writing that spec to Cuban it is blue HDD finally we are also",
    "start": "1091280",
    "end": "1097700"
  },
  {
    "text": "exploring trying to offload that spec altogether especially when the spec is in MVS",
    "start": "1097700",
    "end": "1103100"
  },
  {
    "text": "we can if you can offload respect to something like an external external blob store like s3 and just show the reference in a treaty that will actually",
    "start": "1103100",
    "end": "1110480"
  },
  {
    "text": "also help us improving improve that CD write times as well as reduce the overall size of data stored energy D ok",
    "start": "1110480",
    "end": "1119870"
  },
  {
    "text": "so multi-tenancy so multi-tenancy was one of our another core goals for the",
    "start": "1119870",
    "end": "1125300"
  },
  {
    "text": "platform isolation and fairness we want to make sure that the system is isolated and there's fairness across users so how",
    "start": "1125300",
    "end": "1131780"
  },
  {
    "text": "do we achieve that again partly by using cuban it is resource coders so we enforce coders on aggregate count of",
    "start": "1131780",
    "end": "1138800"
  },
  {
    "text": "cpus or memory is a user or a set of users within a team can consume in the cluster but some of our task types we",
    "start": "1138800",
    "end": "1147140"
  },
  {
    "text": "support our external they are not cuban it is native so for those types i we sort of maintain our own in-memory local",
    "start": "1147140",
    "end": "1153200"
  },
  {
    "text": "resource resource store we will maintain quotas for those for each namespace for",
    "start": "1153200",
    "end": "1158870"
  },
  {
    "text": "those task types so that there's fairness for those task type as well another issue we ran into when we",
    "start": "1158870",
    "end": "1166010"
  },
  {
    "text": "started using humanities resource quotas is that we started seeing that the api",
    "start": "1166010",
    "end": "1171410"
  },
  {
    "text": "server latency is were again very high especially for the create pod api for",
    "start": "1171410",
    "end": "1177860"
  },
  {
    "text": "some of the cases we saw that p90 heating is largest you know going into minutes we investigated this and we what",
    "start": "1177860",
    "end": "1185030"
  },
  {
    "text": "we found out the create pod api call actually invokes all the web hooks you have configured in the cluster and then",
    "start": "1185030",
    "end": "1191270"
  },
  {
    "text": "goes and invokes the resource quota at Mission Control so this call is really expensive especially if you have a",
    "start": "1191270",
    "end": "1197480"
  },
  {
    "text": "sizable number of web ebooks configured in the cluster which was the case for us and this call was actually taking really",
    "start": "1197480",
    "end": "1203920"
  },
  {
    "text": "really taking a very high latency so we were partly to blame for this is because",
    "start": "1203920",
    "end": "1208960"
  },
  {
    "text": "we were sort of even if a namespace had headaches resource coders and had like",
    "start": "1208960",
    "end": "1215210"
  },
  {
    "text": "let's say hundreds of workflows in pending we would still pro flight propeller which is responsible for",
    "start": "1215210",
    "end": "1220280"
  },
  {
    "text": "driving that workflow to the next shade this chill will still try to process each single workflow in that namespace",
    "start": "1220280",
    "end": "1225890"
  },
  {
    "text": "and try to create the pods it can create in the inter cluster so for these",
    "start": "1225890",
    "end": "1231470"
  },
  {
    "text": "workflows which what hitting resource code has we were still trying to issue",
    "start": "1231470",
    "end": "1236540"
  },
  {
    "text": "create pod API calls to to the API server every single time we were processing those workflows so we are not",
    "start": "1236540",
    "end": "1243110"
  },
  {
    "text": "backing off correctly in this cases so one of the solutions we are actually implementing right now is to maintain",
    "start": "1243110",
    "end": "1249380"
  },
  {
    "text": "sort of a it is a quota cash for each namespace we will store the namespaces which have hit their quotas in the",
    "start": "1249380",
    "end": "1255470"
  },
  {
    "text": "recent past and then we can back off for those named spaceflight preparer I can just keep processing those workflows for",
    "start": "1255470",
    "end": "1261650"
  },
  {
    "text": "a little bit and back of accordingly okay flight system isolation we also",
    "start": "1261650",
    "end": "1268910"
  },
  {
    "text": "want to make sure that the system components like flight ID main flight propeller are also isolated from user",
    "start": "1268910",
    "end": "1275210"
  },
  {
    "text": "pods especially you want to make sure that you know they don't have to compete with user pods and they want to be",
    "start": "1275210",
    "end": "1282080"
  },
  {
    "text": "scheduled or time to find in order to get scheduled so the way we achieve this is by using a separate set of reserved",
    "start": "1282080",
    "end": "1288170"
  },
  {
    "text": "nodes with chains and toleration x' and then using node affinity to place these system pods like flight propeller on to",
    "start": "1288170",
    "end": "1294080"
  },
  {
    "text": "those specific reserve notes so flight propeller and other admin component or",
    "start": "1294080",
    "end": "1299690"
  },
  {
    "text": "other flight components which which are a single power which run as a single pod and drive all the workflows in the",
    "start": "1299690",
    "end": "1306440"
  },
  {
    "text": "system you also want to make sure that this fairness across users for those components so the way we achieve this is",
    "start": "1306440",
    "end": "1312650"
  },
  {
    "text": "by using multiple work through queues and worker pools maintaining worker pools per namespace in flight propeller",
    "start": "1312650",
    "end": "1317920"
  },
  {
    "text": "so that a particular set of users running in a namespace can only get a small slice of flight propellers",
    "start": "1317920",
    "end": "1324570"
  },
  {
    "text": "national park performance so performance",
    "start": "1324570",
    "end": "1330360"
  },
  {
    "text": "was another one of our core goals one of the ways we achieve performance or improve performance of the system is by",
    "start": "1330360",
    "end": "1336450"
  },
  {
    "text": "providing a feature set called discoverable tasks so but discoverable tasks too is it will just skip expensive",
    "start": "1336450",
    "end": "1341700"
  },
  {
    "text": "tasks executions and to use cash results if the task logic and inputs haven't changed what that means is if you have",
    "start": "1341700",
    "end": "1347580"
  },
  {
    "text": "run the same task in the past with the same task logic and the same task inputs",
    "start": "1347580",
    "end": "1353039"
  },
  {
    "text": "we will just cache those outputs generated at that particular instance in the in the past and if you're running",
    "start": "1353039",
    "end": "1359279"
  },
  {
    "text": "the same task again now we will just skip that task revision completely and just reuse those outputs generated in",
    "start": "1359279",
    "end": "1364740"
  },
  {
    "text": "the past this so not only helps with performance is in but this also helps with you know skipping expensive",
    "start": "1364740",
    "end": "1370230"
  },
  {
    "text": "computation and improves cost some of the other ways we improve performance of the system is by using node affinity for",
    "start": "1370230",
    "end": "1378330"
  },
  {
    "text": "especially for multi container data intensive tasks like SPARC we spawn like hundreds or like a large number of",
    "start": "1378330",
    "end": "1384179"
  },
  {
    "text": "executors and a driver and then there's a data shuffle between those executors and driver so placing them on the same",
    "start": "1384179",
    "end": "1389610"
  },
  {
    "text": "node over a very small set of nodes helps with the oral performance of the task we are also looking into",
    "start": "1389610",
    "end": "1396720"
  },
  {
    "text": "implementing a write through cache for workflow CR DS this basically helps us",
    "start": "1396720",
    "end": "1402179"
  },
  {
    "text": "reduce the HDD load for gets an update",
    "start": "1402179",
    "end": "1406970"
  },
  {
    "text": "ok now I'll ask foreigner to walk you through the rest of the presentation it's a cost since we ran tens of",
    "start": "1407269",
    "end": "1415950"
  },
  {
    "text": "millions of containers every month causes an important consideration for us so we did couple of things on cost and",
    "start": "1415950",
    "end": "1421830"
  },
  {
    "text": "we are also doing a few more things to reduce them further so when we started last year we did a stupid mistake of",
    "start": "1421830",
    "end": "1428909"
  },
  {
    "text": "actually using default auto scheduler autoscaler for the cloud for AWS cloud which we are using so the problem there was that",
    "start": "1428909",
    "end": "1435720"
  },
  {
    "text": "whenever we run pods it will scale up the cluster but will never bring it down the reason being that it just assumed those machines are at your disposal for",
    "start": "1435720",
    "end": "1442049"
  },
  {
    "text": "free and not try to squeeze them into minimum number of nodes so the natural choice was to move to cluster autoscaler",
    "start": "1442049",
    "end": "1448200"
  },
  {
    "text": "which we did and which saw a decent saving which is almost double",
    "start": "1448200",
    "end": "1453500"
  },
  {
    "text": "and what it does it say it has a greedy approach of putting in the pods on the nodes which are already in the system",
    "start": "1453500",
    "end": "1458540"
  },
  {
    "text": "running some part but still have margin to accommodate another another pod this is this ran fine so we were able to like",
    "start": "1458540",
    "end": "1466490"
  },
  {
    "text": "increase utilization from twenty to forty percent but still is not good enough so the next step was that okay",
    "start": "1466490",
    "end": "1471560"
  },
  {
    "text": "even though autoscaler is doing good it's just that schedule itself is not good enough so we started looking at the cube bat scheduler and we ran it on a",
    "start": "1471560",
    "end": "1478550"
  },
  {
    "text": "load test but we still have not kind of using it in prod but I'll share the basic idea behind that the basic idea is",
    "start": "1478550",
    "end": "1483920"
  },
  {
    "text": "that since we are running bats kind of a load there are always temporary bursts we don't want to scale up and scale down",
    "start": "1483920",
    "end": "1489410"
  },
  {
    "text": "all the time because whenever you scale up there is a trigger which will scale up there will be a time for which the",
    "start": "1489410",
    "end": "1495140"
  },
  {
    "text": "machines will be sitting idle before they can go down another is that whenever you're scaling up some time to use on-demand instances and those are",
    "start": "1495140",
    "end": "1501380"
  },
  {
    "text": "expensive so we wanted to kind of smooth an out those temporary births so what we want is we want instead of scheduler",
    "start": "1501380",
    "end": "1509240"
  },
  {
    "text": "like autoscaler getting more machines when there is a pending pods in the system we want the other way round we",
    "start": "1509240",
    "end": "1514520"
  },
  {
    "text": "want a cute state in the system which means that when there is a queue in the system and the queue is building more",
    "start": "1514520",
    "end": "1520130"
  },
  {
    "text": "than what we anticipated then only we want to scale up like if there is less a queue of 10 which we completely okay",
    "start": "1520130",
    "end": "1525740"
  },
  {
    "text": "with we know it's a temporary burst will go away we don't want to scale up so that's something we are trying right now we are also defining quality of service",
    "start": "1525740",
    "end": "1532430"
  },
  {
    "text": "classes so in our system today there are critical workloads which means when they come in they cannot wait and what we",
    "start": "1532430",
    "end": "1538370"
  },
  {
    "text": "have done there is we have over provision capacity for them so when they come in we don't put them in a queue we just directly schedule them this works",
    "start": "1538370",
    "end": "1545000"
  },
  {
    "text": "fine it's just that not everything in political in the system what do we do about non-critical work you it's the",
    "start": "1545000",
    "end": "1550040"
  },
  {
    "text": "workloads which come up which come with non stick deadlines they can wait in the queue so what we do over there is the",
    "start": "1550040",
    "end": "1556040"
  },
  {
    "text": "same thing we just put them in a queue will we know that when the bus will go over get over they will be drained off",
    "start": "1556040",
    "end": "1562490"
  },
  {
    "text": "that's what we are trying to do with those quality of services right now so it's something we are developing right",
    "start": "1562490",
    "end": "1567950"
  },
  {
    "text": "now is the spot instance so in v-0 what we are doing is we are putting a separate AZ cluster for spot instances",
    "start": "1567950",
    "end": "1576130"
  },
  {
    "text": "which means that again the non-critical workloads which are which can wait will",
    "start": "1576130",
    "end": "1581330"
  },
  {
    "text": "put them in a queue and whenever we see that there are spot instances available at a cost which we are ok with will",
    "start": "1581330",
    "end": "1587110"
  },
  {
    "text": "as many as we need and then we'll flush of the queue and more touched on discoverable TAS discoverable task is a",
    "start": "1587110",
    "end": "1593679"
  },
  {
    "text": "way where we try to identify a work by the task logic and inputs so what we do",
    "start": "1593679",
    "end": "1599710"
  },
  {
    "text": "here is if we know that this work has been done in the past we just reuse the results from the task this reduces the",
    "start": "1599710",
    "end": "1606250"
  },
  {
    "text": "number of duplicate works we do in the cluster user visibility is important because we always believe that if we",
    "start": "1606250",
    "end": "1613150"
  },
  {
    "text": "give enough information to the user so that they can look into what they allocating how much they are using they",
    "start": "1613150",
    "end": "1619330"
  },
  {
    "text": "will have been sent him to go and reduce and optimize the cost for themselves so this is what we do we give them resource",
    "start": "1619330",
    "end": "1625020"
  },
  {
    "text": "resource usage matrix for their pods sometimes they feel that okay they are allocating more than they need they go",
    "start": "1625020",
    "end": "1630850"
  },
  {
    "text": "and change their specs sometimes they feel that the ROI itself is not good enough for what they are running they go",
    "start": "1630850",
    "end": "1636130"
  },
  {
    "text": "and optimize the code ok observability",
    "start": "1636130",
    "end": "1641410"
  },
  {
    "text": "as a platform there the multiple use cases we support observability becomes",
    "start": "1641410",
    "end": "1646450"
  },
  {
    "text": "really important for us as I mentioned in the beginning that when we were designing a system we made a very",
    "start": "1646450",
    "end": "1651460"
  },
  {
    "text": "conscious choice of having a full control plane outside execution cluster and what that means is even though",
    "start": "1651460",
    "end": "1657549"
  },
  {
    "text": "propeller is working on flight propeller is working on a single instance of a workflow but all the update it makes",
    "start": "1657549",
    "end": "1665200"
  },
  {
    "text": "they are synchronously pushed to flight controller a flight control convinces flight admin and the reason for that is",
    "start": "1665200",
    "end": "1671200"
  },
  {
    "text": "that is the guy hold our users talk to there is a UI which is powered by that there is decay and then another thing we",
    "start": "1671200",
    "end": "1677980"
  },
  {
    "text": "did was we actually identified a workflow as a type like as in like there's the name to overflow when you",
    "start": "1677980",
    "end": "1683950"
  },
  {
    "text": "write writing to the workflow there are revisions you can look at the historical information of a workflow you can look at when things went south you can",
    "start": "1683950",
    "end": "1690190"
  },
  {
    "text": "improve on things so that is everything that everything is powered by our",
    "start": "1690190",
    "end": "1695679"
  },
  {
    "text": "control plane matrix was another important thing for observability today we have two things in place one is the",
    "start": "1695679",
    "end": "1702340"
  },
  {
    "text": "system matrix your resource you say the neutralization matrix coming via Prometheus no signal effects then we",
    "start": "1702340",
    "end": "1708460"
  },
  {
    "text": "also have a stats D set up in the cluster and that is for user metrics so so whenever we are starting on container",
    "start": "1708460",
    "end": "1714190"
  },
  {
    "text": "we are giving them a hook into a stats the daemon set so there is the stats EDM and said Rob",
    "start": "1714190",
    "end": "1720540"
  },
  {
    "text": "stats tea service so all the user metrics they are labeled correctly and then they flow via stats to dois wavefront cluster so so far we have",
    "start": "1720540",
    "end": "1733350"
  },
  {
    "text": "talked about what were the issues what would optimizations we have done for a single cluster but again single cluster was not enough for us because of the",
    "start": "1733350",
    "end": "1741060"
  },
  {
    "text": "scale which we run there we need more than that so we started with a multi cluster setup so flight admin which was",
    "start": "1741060",
    "end": "1747000"
  },
  {
    "text": "never part of execution cluster now can talk to multiple execution clusters so when you start up a new cluster you just",
    "start": "1747000",
    "end": "1753120"
  },
  {
    "text": "tell flight admin about this is my new cluster and then you also set up some load balancing policy load balancing",
    "start": "1753120",
    "end": "1759420"
  },
  {
    "text": "policy would be I want to distribute based on these weights or let's say I want to say this workflow needs to be",
    "start": "1759420",
    "end": "1764970"
  },
  {
    "text": "run on a cluster which had a certain label you can do all of that through a load balancing policy this works fine",
    "start": "1764970",
    "end": "1770220"
  },
  {
    "text": "this gives us a very fault-tolerant system where when we see things are going bad we just remove that cluster",
    "start": "1770220",
    "end": "1776130"
  },
  {
    "text": "out of the whole system we are in fact trying to build a pipe feedback back to the flight control plane where it auto",
    "start": "1776130",
    "end": "1783300"
  },
  {
    "text": "discovers that the cluster is not doing good it will stop giving work to that cluster and move on to other clusters",
    "start": "1783300",
    "end": "1788480"
  },
  {
    "text": "this was all good actually by doing this for scale we also got something interesting out of it which was we now",
    "start": "1788480",
    "end": "1795330"
  },
  {
    "text": "we are able to push incremental updates whenever there is a risky change in that cluster rather than pushing it all that closer we push it to a small cluster we",
    "start": "1795330",
    "end": "1801600"
  },
  {
    "text": "ramp up the traffic and then when we are confident we just take it out to those clusters we also see I get another",
    "start": "1801600",
    "end": "1807630"
  },
  {
    "text": "advantage with multi cluster setup this is like some there are some users who want performance but at the same time",
    "start": "1807630",
    "end": "1812970"
  },
  {
    "text": "some security additional security let's say there is a user who doesn't want to pull that image all the time they want",
    "start": "1812970",
    "end": "1819720"
  },
  {
    "text": "to use the cache container image which means there is a security problem but what we can do over here is we can ask",
    "start": "1819720",
    "end": "1825240"
  },
  {
    "text": "them to have a separate cluster and then flight controller flight admin will handle the redirection of the traffic to",
    "start": "1825240",
    "end": "1830640"
  },
  {
    "text": "that cluster ok that's it from us we",
    "start": "1830640",
    "end": "1836940"
  },
  {
    "text": "have a full detailed talk on flight at 5:20 p.m. we have most of the flight team sitting here we would like to know",
    "start": "1836940",
    "end": "1842150"
  },
  {
    "text": "your use cases come talk to us and then more information about s is on flight",
    "start": "1842150",
    "end": "1847500"
  },
  {
    "text": "dot o-r-g thank you [Applause]",
    "start": "1847500",
    "end": "1852748"
  },
  {
    "text": "thank you sir in the normal we have a couple of minutes for questions to three",
    "start": "1855550",
    "end": "1860930"
  },
  {
    "text": "questions there is a mic over there I have a mic wave your hand and we will",
    "start": "1860930",
    "end": "1866690"
  },
  {
    "text": "come to you so that the question and it's one on this side well how would you",
    "start": "1866690",
    "end": "1873410"
  },
  {
    "text": "address the advantage of flight against I'll go work for some other workflow",
    "start": "1873410",
    "end": "1878810"
  },
  {
    "text": "engines yes so I can answer there but I would strongly suggest coming to our full detail talk because this I can I",
    "start": "1878810",
    "end": "1884750"
  },
  {
    "text": "don't want to summarize answer in one or two lines we have a talk at 5:20 p.m. please come to us and we can also talk",
    "start": "1884750",
    "end": "1890180"
  },
  {
    "text": "offline about it good we have a question here on the right hand side we also have some flight seekers if you guys are",
    "start": "1890180",
    "end": "1896660"
  },
  {
    "text": "interested please come by the stage yep I got a quick question yes yeah quick",
    "start": "1896660",
    "end": "1907100"
  },
  {
    "text": "question you guys mentioned your capturing resource you know resource constraints for memory and CPU number of",
    "start": "1907100",
    "end": "1913790"
  },
  {
    "text": "cores do you do you do anything for like long-running jobs that are running too long so we have a thing of like so today",
    "start": "1913790",
    "end": "1921500"
  },
  {
    "text": "in the system if you specify a time out and you say I want the job to be running for two days we are okay with it but we",
    "start": "1921500",
    "end": "1927710"
  },
  {
    "text": "generally encourage people to write small tasks so that if they die they don't lose that work but we do have",
    "start": "1927710",
    "end": "1933920"
  },
  {
    "text": "those ones running to it good I think we have time for one more short question",
    "start": "1933920",
    "end": "1940660"
  },
  {
    "text": "good right here in front I it sounds like there's some workarounds for some",
    "start": "1940660",
    "end": "1947480"
  },
  {
    "text": "performance bottlenecks in HDD has that been communicated that CD is that other known limitations or so we have seen two",
    "start": "1947480",
    "end": "1956360"
  },
  {
    "text": "things late one is that if we just increment increase a right so hcd hcd",
    "start": "1956360",
    "end": "1962740"
  },
  {
    "text": "the slows down so that's something which we ain't any known thing another is the overall size stored in HDD when we see",
    "start": "1962740",
    "end": "1970310"
  },
  {
    "text": "number of objects a cluster they going up or our own flight workflow sizes grow up we see a problem over there so we see",
    "start": "1970310",
    "end": "1977030"
  },
  {
    "text": "two side problem in the request count goes up and I mean overall storage goes up so we are trying to kind of reduce on",
    "start": "1977030",
    "end": "1982340"
  },
  {
    "text": "both so we don't want to do a duplicate right by the same time we also want to reduce the object size I like something to that so at some",
    "start": "1982340",
    "end": "1989630"
  },
  {
    "text": "point there is an abusive head CD and at some point it's like the right level of SLA that you want format CD I think we",
    "start": "1989630",
    "end": "1997070"
  },
  {
    "text": "we still think that we are at the abuse category and we want to bring it to the right and I think there will be certain",
    "start": "1997070",
    "end": "2002320"
  },
  {
    "text": "things that we will push forward to actually you love - good thank you it's time to conclude and pick applause",
    "start": "2002320",
    "end": "2012280"
  },
  {
    "text": "to surrender and I'm all thank you thanks",
    "start": "2012280",
    "end": "2016710"
  }
]