[
  {
    "text": "welcome guys uh today welcome for this session we talking about Rook operator",
    "start": "440",
    "end": "6319"
  },
  {
    "text": "storage for cities and I'm am Bora I work for the",
    "start": "6319",
    "end": "12040"
  },
  {
    "text": "Rook development and maintaining it and I work as a part of IBM storage next to",
    "start": "12040",
    "end": "18560"
  },
  {
    "text": "you yeah hello hello guys I am yti paria I am a software developer at ibbm I work",
    "start": "18560",
    "end": "26400"
  },
  {
    "text": "uh like I work with the fcsi team for open data found addition thing so yeah",
    "start": "26400",
    "end": "31599"
  },
  {
    "text": "that's about me sham hi everyone uh thanks for attending this talk I'm subam",
    "start": "31599",
    "end": "37239"
  },
  {
    "text": "R I'm one of uh maintainer of rook and I also work with IBM hello everyone myself",
    "start": "37239",
    "end": "44039"
  },
  {
    "text": "dipika upadhi I am a contributor to rook and also help uh people with Consulting",
    "start": "44039",
    "end": "50239"
  },
  {
    "text": "and uh projects associated with Rook at Kao gmbh so yeah get you started so here's",
    "start": "50239",
    "end": "57920"
  },
  {
    "text": "the agenda on the topics what we would be talking about we'll be talking about the rukf um like what features it's provide",
    "start": "57920",
    "end": "65720"
  },
  {
    "text": "and what platforms you can use rukf and if you deploying what topologies would be good set for you and",
    "start": "65720",
    "end": "73320"
  },
  {
    "text": "if you have already deployed what would be the maintainance best practices for this and plus what support we can",
    "start": "73320",
    "end": "81200"
  },
  {
    "text": "provide to you as a community and exciting for the new features that we have developed in the",
    "start": "81200",
    "end": "87159"
  },
  {
    "text": "last release and we have a upcoming release this month only so we'll be talking about that and last like",
    "start": "87159",
    "end": "94360"
  },
  {
    "text": "everyone in 2024 is talking about disaster so uh we'll be covering that part",
    "start": "94360",
    "end": "100759"
  },
  {
    "text": "also uh so a quick idea for the idence wanted to know like anyone like house",
    "start": "100759",
    "end": "106719"
  },
  {
    "text": "the attendance here so raise your hand if you are like very new about",
    "start": "106719",
    "end": "111759"
  },
  {
    "text": "Rook okay I see a lot of session sure anyone has experience with SEF before",
    "start": "111759",
    "end": "118520"
  },
  {
    "text": "okay great number of hands and anyone has deployed rook in",
    "start": "118520",
    "end": "123600"
  },
  {
    "text": "production or using experimented with it oh that's great so yeah I guess would be",
    "start": "123600",
    "end": "130039"
  },
  {
    "text": "a cool discussion on that so intro to Rook so back in like in",
    "start": "130039",
    "end": "137160"
  },
  {
    "text": "the history like when Rook was developed around 7 8 years back so we were thinking about like what storage should",
    "start": "137160",
    "end": "143400"
  },
  {
    "text": "look like and storage is mostly like we are plugging external stuff into our",
    "start": "143400",
    "end": "148599"
  },
  {
    "text": "environment in our kuties cluster but why it should be always external why it cannot reside in our own Data Center and",
    "start": "148599",
    "end": "155840"
  },
  {
    "text": "why it should not work like how our other K application are working uh why cannot we manage the kubernetes storage",
    "start": "155840",
    "end": "163400"
  },
  {
    "text": "by ourself in our own cluster why it should be always external to Cloud providers so that was the start of that",
    "start": "163400",
    "end": "170720"
  },
  {
    "text": "and we don't want to build a data platform we want to trust something that",
    "start": "170720",
    "end": "175760"
  },
  {
    "text": "was already Enterprise ready and as the date data again data layer is very",
    "start": "175760",
    "end": "181599"
  },
  {
    "text": "important so we bet on SEF at that time and why we choose SEF because SEF is a",
    "start": "181599",
    "end": "188319"
  },
  {
    "text": "distributed software storage solution uh it like very well Works how the kues",
    "start": "188319",
    "end": "194360"
  },
  {
    "text": "architecture work and in as a platform it provide like three",
    "start": "194360",
    "end": "199959"
  },
  {
    "text": "types of storage at the same time the RBD as a block storage mostly for read",
    "start": "199959",
    "end": "205560"
  },
  {
    "text": "write ones volumes and the SFS for a shared file system like read R man exis",
    "start": "205560",
    "end": "213560"
  },
  {
    "text": "and also the rgw the object storage uh mostly like we talk about S3 uh so it",
    "start": "213560",
    "end": "221080"
  },
  {
    "text": "mostly resembles with the that apis that s provides to us and SEF is a open source community",
    "start": "221080",
    "end": "228519"
  },
  {
    "text": "and we have big customers like sun which have like terabytes and terabytes of storage and pabes there and it's",
    "start": "228519",
    "end": "235920"
  },
  {
    "text": "designed for its scalability so that's why C and talking about like how durable SEF",
    "start": "235920",
    "end": "243560"
  },
  {
    "text": "is um so SEF is designed for consistency and not like other platforms",
    "start": "243560",
    "end": "251280"
  },
  {
    "text": "like that are eventually consistent so if you are storing something in SE and it's egged so you're good the data would",
    "start": "251280",
    "end": "258199"
  },
  {
    "text": "be like stored perfectly there and and how SEF actually makes the inner layer",
    "start": "258199",
    "end": "265520"
  },
  {
    "text": "is like it shards the data into different AGS and you can have replication between different available",
    "start": "265520",
    "end": "271720"
  },
  {
    "text": "zones that you have so like the data coming will go to multiple zones at the",
    "start": "271720",
    "end": "276759"
  },
  {
    "text": "same time and it would be replicated internally so like let's say if some",
    "start": "276759",
    "end": "282320"
  },
  {
    "text": "data has been like there is some impact or disaster the data will be still still be secured so it's more talk about how",
    "start": "282320",
    "end": "289919"
  },
  {
    "text": "data resilient it is so uh that's on that and even in an extreme disaster you",
    "start": "289919",
    "end": "295360"
  },
  {
    "text": "can recover it manually uh and also there is a second replication way we can",
    "start": "295360",
    "end": "301560"
  },
  {
    "text": "use that is the Eraser coding that is 2 is to one um mostly used with object",
    "start": "301560",
    "end": "307280"
  },
  {
    "text": "storage um so that's on SEF uh I see like man hands they already",
    "start": "307280",
    "end": "314000"
  },
  {
    "text": "know how SEF is so SEF directly installing SEF is a very paino so what",
    "start": "314000",
    "end": "319240"
  },
  {
    "text": "Rook does is like Rook actually automates his configuration and manage",
    "start": "319240",
    "end": "324440"
  },
  {
    "text": "the state of SE and we just provide some simple crds just go ahead and deploy",
    "start": "324440",
    "end": "330720"
  },
  {
    "text": "them and it will actually launch the SEF cluster and it will actually bring your SEF cluster to the kubernetes",
    "start": "330720",
    "end": "337560"
  },
  {
    "text": "world and Rook is actually open source uh Apache 2.0 licensed and it automates",
    "start": "337560",
    "end": "344680"
  },
  {
    "text": "the deployment configurations and also keeps your data safe while upgrades and with Rook you can use like",
    "start": "344680",
    "end": "353560"
  },
  {
    "text": "uh the storage like any other storage provider like it will give you the storage classes and on top of that you",
    "start": "353560",
    "end": "359280"
  },
  {
    "text": "can cre PVS and PVCs uh so this is the architecture",
    "start": "359280",
    "end": "365199"
  },
  {
    "text": "layer how the whole product look like the whole stack Rook is actually the deployment layer that's managing the SEF",
    "start": "365199",
    "end": "372120"
  },
  {
    "text": "it's State Management of it and have different controllers to manage the",
    "start": "372120",
    "end": "377599"
  },
  {
    "text": "other applications what SEF has we have CSI which is used for the",
    "start": "377599",
    "end": "383240"
  },
  {
    "text": "provisioning and the mounting there are further slides for CSI and SE being the",
    "start": "383240",
    "end": "388360"
  },
  {
    "text": "data layer so this whole whole stack is like open source so that's generally available for",
    "start": "388360",
    "end": "395360"
  },
  {
    "text": "everyone uh so the basic question how we can install Rook we have two ways",
    "start": "395360",
    "end": "401000"
  },
  {
    "text": "someone likes Helm charart so that's also available and direct manifest installation is also",
    "start": "401000",
    "end": "407000"
  },
  {
    "text": "available and a quick start guide you can go to the following dogs um and where Rook can be installed",
    "start": "407000",
    "end": "415759"
  },
  {
    "text": "so anywhere kubernetes that can run we can use Rook that can be any Cloud",
    "start": "415759",
    "end": "421800"
  },
  {
    "text": "environment any like bare metal clusters and we have also tested Rook with some",
    "start": "421800",
    "end": "427560"
  },
  {
    "text": "of loop back devices um that are like storage under",
    "start": "427560",
    "end": "433000"
  },
  {
    "text": "the nodes so that the Rook also works very well with that and this was a question like mostly",
    "start": "433000",
    "end": "440639"
  },
  {
    "text": "asked yesterday on the booth uh if I have a cloud storage why I should deploy a rook on top of that uh so there are",
    "start": "440639",
    "end": "448759"
  },
  {
    "text": "some limitations that cloud provider has like there is no a support there and the PVCs are very",
    "start": "448759",
    "end": "456080"
  },
  {
    "text": "limited per node like I guess there are around 32 we can have and the last is",
    "start": "456080",
    "end": "462240"
  },
  {
    "text": "like if we have hundreds of small PVS and uh large PVS then the performance",
    "start": "462240",
    "end": "467759"
  },
  {
    "text": "impact is there in small PVS so what if you bring this SEF onto your storage",
    "start": "467759",
    "end": "473080"
  },
  {
    "text": "Tech uh on top of coones uh you can have a a supports you can have multiple zones",
    "start": "473080",
    "end": "479280"
  },
  {
    "text": "and racks and like virtually there is no limit on PVS per node you can have",
    "start": "479280",
    "end": "484720"
  },
  {
    "text": "unlimited PVS installed and um the performance wise",
    "start": "484720",
    "end": "490159"
  },
  {
    "text": "there will be no impact on if you have large PVS or small PVS uh because how SE",
    "start": "490159",
    "end": "495520"
  },
  {
    "text": "is scalable it will give the performance and keep its performance for time plus uh nowadays we talk about",
    "start": "495520",
    "end": "502120"
  },
  {
    "text": "hybrid storage everyone wants different kind of storage at the same time so Rook is consistent between different storage",
    "start": "502120",
    "end": "507639"
  },
  {
    "text": "providers you can use make use of other different St providers at the same time with",
    "start": "507639",
    "end": "514360"
  },
  {
    "text": "rook and this was the second asked question in the booth yesterday like uh",
    "start": "514519",
    "end": "519919"
  },
  {
    "text": "you are saying like we should deploy Rook on the nodes where our applications is running what if if my application",
    "start": "519919",
    "end": "526720"
  },
  {
    "text": "goes or runs out of the um the the the the the the the configurations like",
    "start": "526720",
    "end": "532480"
  },
  {
    "text": "request and limits so so there are three different ways you",
    "start": "532480",
    "end": "537600"
  },
  {
    "text": "can install Rook uh the first is like the hyper Converse where we'll give like",
    "start": "537600",
    "end": "542640"
  },
  {
    "text": "you can install The Rook on the same nodes where your application is running um so you can set the limits for the",
    "start": "542640",
    "end": "549600"
  },
  {
    "text": "particular Rook ports so depending on that limits only you can uh The Rook",
    "start": "549600",
    "end": "554800"
  },
  {
    "text": "will utilize its resources plus if you clearly don't want your application to",
    "start": "554800",
    "end": "560399"
  },
  {
    "text": "be consed with uh the the the storage ports then you can have the dedicated",
    "start": "560399",
    "end": "565519"
  },
  {
    "text": "storage nodes simply you have to taint some nodes with some toleration and the",
    "start": "565519",
    "end": "570560"
  },
  {
    "text": "resources will be deployed on the separate section of the nodes the Third Way is external SEF I in",
    "start": "570560",
    "end": "578680"
  },
  {
    "text": "the community the SEF was form like around 14 years and some people have SEF",
    "start": "578680",
    "end": "583959"
  },
  {
    "text": "stand alone and if they want to bring that SEF to kubernetes world that that's",
    "start": "583959",
    "end": "589240"
  },
  {
    "text": "already deployed uh we can use of external SEF cluster so this is the",
    "start": "589240",
    "end": "595440"
  },
  {
    "text": "actual design of that like you have external SE cluster and Rook will run as",
    "start": "595440",
    "end": "602320"
  },
  {
    "text": "a coties cluster and this can extract the external SE cluster running using uh",
    "start": "602320",
    "end": "608640"
  },
  {
    "text": "scraping and it will scrape some data and give a connection to your external C this is the third way and the benefits",
    "start": "608640",
    "end": "616120"
  },
  {
    "text": "that we have is this like we can run multiple uh cetes cluster and connect to just one self cluster and sometimes some",
    "start": "616120",
    "end": "624519"
  },
  {
    "text": "people want isolation so we can get the isolation for the storage to The Rook",
    "start": "624519",
    "end": "631640"
  },
  {
    "text": "Coes cluster separately with this so next on the SE CSI driver y yes",
    "start": "631640",
    "end": "639760"
  },
  {
    "text": "sure so talking about uh SEF CSI uh SEF CSI is a CSI driver for SEF storage",
    "start": "639760",
    "end": "646720"
  },
  {
    "text": "which allows the container orchestrators to use SEF as a storage back end for their containerized applications so",
    "start": "646720",
    "end": "653959"
  },
  {
    "text": "thereby acting as a bridge between the CSI enabled container orchestrators and",
    "start": "653959",
    "end": "659320"
  },
  {
    "text": "and the storage back endend which here is SEF so SEF CSI architecture follows",
    "start": "659320",
    "end": "664800"
  },
  {
    "text": "the standard CSI architecture with a container uh container orchestrator sending the CSI API calls to the SEF CSI",
    "start": "664800",
    "end": "673120"
  },
  {
    "text": "driver which then translates the calls into the commands that the SEF can",
    "start": "673120",
    "end": "678279"
  },
  {
    "text": "understands and thereby performing the operations so it is basically a grpc server which is written in golang uh",
    "start": "678279",
    "end": "685440"
  },
  {
    "text": "which is used to implement the CSI specification and we have the following different type of drivers as part",
    "start": "685440",
    "end": "692560"
  },
  {
    "text": "mentioned in earlier slides we have different formats like for block mode we",
    "start": "692560",
    "end": "697959"
  },
  {
    "text": "have RBD driver for the file system we have CFS driver and for Network file system we have this NFS driver that we",
    "start": "697959",
    "end": "705320"
  },
  {
    "text": "have so this is basic and uh next let's look into the standard uh like",
    "start": "705320",
    "end": "711480"
  },
  {
    "text": "deployment of how CSI is deployed it's like similar to other uh container orchestrators like we have we have two",
    "start": "711480",
    "end": "718160"
  },
  {
    "text": "different uh uh like uh two different pods here like one is uh the SEF CSI",
    "start": "718160",
    "end": "723760"
  },
  {
    "text": "provisional deployment and the other is the node plug-in uh SEF CSI provisional deployment usually take cares of the",
    "start": "723760",
    "end": "730279"
  },
  {
    "text": "volume or snapshot creation deletion expansion and other uh like other such",
    "start": "730279",
    "end": "736360"
  },
  {
    "text": "operations and we usually have two provisional PS with a leader election",
    "start": "736360",
    "end": "741760"
  },
  {
    "text": "functionality where one of the Pod takes uh the charge uh and if in case there is",
    "start": "741760",
    "end": "747399"
  },
  {
    "text": "any network issues or like the Pod is is down in that case the other pod can uh take uh the charge and uh like yeah uh",
    "start": "747399",
    "end": "755519"
  },
  {
    "text": "so this this P deployment is nothing but the set of containers like you can see we have CSI advon side card container",
    "start": "755519",
    "end": "762120"
  },
  {
    "text": "the provisioner the resizer attacher snapshot and the CSI plug-in uh containers with each one with its own",
    "start": "762120",
    "end": "769839"
  },
  {
    "text": "functionality like CSI addons we'll discuss on later slides it uh like it",
    "start": "769839",
    "end": "775000"
  },
  {
    "text": "has volume replication thing features network uh uh Network fencing for",
    "start": "775000",
    "end": "780199"
  },
  {
    "text": "reclaim like reclaiming the volume and other stuffs and like snapshot CSI snapshot we use for like creation and",
    "start": "780199",
    "end": "787000"
  },
  {
    "text": "deltion of snapshots so Etc so like all these have their own use case uh",
    "start": "787000",
    "end": "792839"
  },
  {
    "text": "similarly we have the node plug-in demon set which is used for volume mounting and unmounting to the pods and it is",
    "start": "792839",
    "end": "799880"
  },
  {
    "text": "deployed one per node so yeah that is about this and uh talking about the",
    "start": "799880",
    "end": "807880"
  },
  {
    "text": "functionalities and features that it has uh it is thinly provisioned that is the volume is miserable and it can be",
    "start": "807880",
    "end": "814440"
  },
  {
    "text": "expanded further and it has different modes like as mentioned in earlier slides the rbds ffs and NFS things and",
    "start": "814440",
    "end": "822440"
  },
  {
    "text": "it is also a topology aware that is it can read from the read the OSD from its",
    "start": "822440",
    "end": "827880"
  },
  {
    "text": "nearest clients it allows snapshot taking snapshot and clones and recently when the group volume snapshot feature",
    "start": "827880",
    "end": "835800"
  },
  {
    "text": "was in is introduced I think we we you will be able to see in upcoming relays where we can uh take uh group snapshots",
    "start": "835800",
    "end": "843959"
  },
  {
    "text": "at a time so yeah these are the things coming to CSI adance uh so CSI adance",
    "start": "843959",
    "end": "850680"
  },
  {
    "text": "host the extension to the CSI specification that provides Advanced storage operations like as mentioned",
    "start": "850680",
    "end": "856920"
  },
  {
    "text": "Network fencing and reclaim Space volume replication features and volume group replication Etc so we have this CSI",
    "start": "856920",
    "end": "864800"
  },
  {
    "text": "addon controller the side car which resides in the CSI driver p and how the",
    "start": "864800",
    "end": "871000"
  },
  {
    "text": "communication takes place like if there is any uh feature like if you want to",
    "start": "871000",
    "end": "876240"
  },
  {
    "text": "reclaim the space or network fening there is a CR that is created by the user uh which the controllers uh uh like",
    "start": "876240",
    "end": "883720"
  },
  {
    "text": "which is then passed on to the controller and the controller passes a uh like the call to the CSI addon side",
    "start": "883720",
    "end": "890079"
  },
  {
    "text": "car which resides in the CSI driver through a grpc call and then the gr uh",
    "start": "890079",
    "end": "896040"
  },
  {
    "text": "and then the driver uh like the CSI side side guard sends another grpc uh request",
    "start": "896040",
    "end": "901240"
  },
  {
    "text": "to the controller or not plugin to uh like execute the operations so talking in uh uh just a",
    "start": "901240",
    "end": "909199"
  },
  {
    "text": "little bit of each of these uh uh features reclaim space reclaim space is",
    "start": "909199",
    "end": "914519"
  },
  {
    "text": "the operation that we ex that executes the sparsify command on the images and",
    "start": "914519",
    "end": "920079"
  },
  {
    "text": "fstream on the file system mode so to reclaim the unused space so this enables",
    "start": "920079",
    "end": "926880"
  },
  {
    "text": "the storage admins uh to have accurate idea on like what is the storage consumption and in the cluster so that",
    "start": "926880",
    "end": "933680"
  },
  {
    "text": "they can reclaim their space later on like which is not required coming on to network fencing it provides the API for",
    "start": "933680",
    "end": "941079"
  },
  {
    "text": "blocking the list of IP ranges this is mostly for the node loss kind of",
    "start": "941079",
    "end": "946880"
  },
  {
    "text": "scenarios where for some situation the uh node is lost so in that case uh we can pass using the command like we have",
    "start": "946880",
    "end": "953680"
  },
  {
    "text": "SEF OSD blocklist and we can pass the IP range uh IP which we want to block list",
    "start": "953680",
    "end": "958720"
  },
  {
    "text": "and so that the Watcher then can uh like clear like Watcher will be free to mount the RBD image to the new node uh yeah",
    "start": "958720",
    "end": "966680"
  },
  {
    "text": "that is about um Network fencing then coming to volume replication it is another operation uh which provides uh",
    "start": "966680",
    "end": "973720"
  },
  {
    "text": "the reusable apis for disaster recovery process which is now on very much talk so we have earlier we had a separate",
    "start": "973720",
    "end": "981480"
  },
  {
    "text": "volume replication operator itself which was uh doing the disaster recovery thing but after the introduction of CSI addons",
    "start": "981480",
    "end": "989000"
  },
  {
    "text": "uh we F found it uh very easier to include it as a uh feature here rather",
    "start": "989000",
    "end": "994040"
  },
  {
    "text": "than having a separate side car and operator so yeah that's how we we'll discuss on the later sides when we are",
    "start": "994040",
    "end": "1000160"
  },
  {
    "text": "discussing then again we have encryption key rotation where earlier what was happening is like for each uh like uh",
    "start": "1000160",
    "end": "1007079"
  },
  {
    "text": "for each encrypted volume we had just had a single key throughout the lifetime of the volume or image so but now we",
    "start": "1007079",
    "end": "1014800"
  },
  {
    "text": "have enabled the encryption key rotation where where the key is getting rotated with every interval either by using the",
    "start": "1014800",
    "end": "1022399"
  },
  {
    "text": "feature of annotation of like by by annotating the PVS and the storage class",
    "start": "1022399",
    "end": "1027558"
  },
  {
    "text": "so that's how this is work and RBD mirroring we'll be covering in next slides next slide yeah so here it is so",
    "start": "1027559",
    "end": "1034360"
  },
  {
    "text": "talking about The Rook application for Disaster Recovery uh RBD mirroring is an asynchronous replication of RBD image",
    "start": "1034360",
    "end": "1041720"
  },
  {
    "text": "between the multiple clusters uh it is either general based or snapshot based",
    "start": "1041720",
    "end": "1046880"
  },
  {
    "text": "you can read more about like what is the difference between Journal based and snapshot based in The Rook documentation",
    "start": "1046880",
    "end": "1052480"
  },
  {
    "text": "it is clearly mentioned there uh we do have like how it processes this we do have RBD mirroring crds where when we",
    "start": "1052480",
    "end": "1060000"
  },
  {
    "text": "create this uh SEF RBD mirror crd uh it creates an RBD mirror demon in the",
    "start": "1060000",
    "end": "1065480"
  },
  {
    "text": "provisioner and like in the cluster and that mirror demon take cares of like enabling and disabling the replication",
    "start": "1065480",
    "end": "1072679"
  },
  {
    "text": "so that we can like process the disaster recovery thing and for carrying out the",
    "start": "1072679",
    "end": "1078960"
  },
  {
    "text": "particular events we have volume replication crds where we create the volume replication class and volume",
    "start": "1078960",
    "end": "1084720"
  },
  {
    "text": "replication CRS so uh like you must be aware of like PVCs and storage class so",
    "start": "1084720",
    "end": "1090039"
  },
  {
    "text": "it is similarly we have this volume replication class which is similar to the storage class that we create and volume replication is similar to uh the",
    "start": "1090039",
    "end": "1097159"
  },
  {
    "text": "PVC that we create so this we have volume replication class which extends the apis for the storage dis Disaster",
    "start": "1097159",
    "end": "1104559"
  },
  {
    "text": "Recovery we have a volume replication which is created per image for one PV",
    "start": "1104559",
    "end": "1110039"
  },
  {
    "text": "we'll have one volume replication and it will take care of promoting demoting or",
    "start": "1110039",
    "end": "1115720"
  },
  {
    "text": "performing our sync uh like operations for particular image whatever we want to",
    "start": "1115720",
    "end": "1120960"
  },
  {
    "text": "uh like operate on like if there is any failover case where there is one of the cluster is down or one of the data",
    "start": "1120960",
    "end": "1126600"
  },
  {
    "text": "center is down uh then in that that case the volume replication will demote that particular image and promote the that",
    "start": "1126600",
    "end": "1134360"
  },
  {
    "text": "promote the other one on the secondary cluster where you can just access uh this particular image which was down",
    "start": "1134360",
    "end": "1140240"
  },
  {
    "text": "here and hence uh like uh reducing the time for retention so yeah this is the",
    "start": "1140240",
    "end": "1146760"
  },
  {
    "text": "whole of the process that we do for and this is particularly for the block mode",
    "start": "1146760",
    "end": "1152120"
  },
  {
    "text": "uh similarly we have SEF file system mirroring process as well it is also synchronous replication of snapshot for",
    "start": "1152120",
    "end": "1158600"
  },
  {
    "text": "the remote sefs file system uh it uh does not have this volume replication",
    "start": "1158600",
    "end": "1165000"
  },
  {
    "text": "and the CR thing which is not automated actually currently but uh you can check out like we can uh manually connect like",
    "start": "1165000",
    "end": "1171880"
  },
  {
    "text": "configure the P the snapshot schedules and the retentions uh thing so in the uh",
    "start": "1171880",
    "end": "1177320"
  },
  {
    "text": "in the back end so the details of how we do is mentioned in the Rook documentation please if you want you can",
    "start": "1177320",
    "end": "1183280"
  },
  {
    "text": "go through it and uh like uh the demon that we had in RBD we it also creates SE",
    "start": "1183280",
    "end": "1189440"
  },
  {
    "text": "file system mirror crd we do create and it creates the ffs mirror demon for the process and currently it is experimental",
    "start": "1189440",
    "end": "1197080"
  },
  {
    "text": "so we don't get anything so like you can always feel free to test it out you have",
    "start": "1197080",
    "end": "1202720"
  },
  {
    "text": "it in documentation so you can also try it out and add your feedbacks there so",
    "start": "1202720",
    "end": "1208159"
  },
  {
    "text": "yeah this is pretty much about disaster recovery and I'll just hand over to Thea for further hello now you have seen",
    "start": "1208159",
    "end": "1214440"
  },
  {
    "text": "about how uh block storage works and SE uh file system storage works for SEF uh",
    "start": "1214440",
    "end": "1220799"
  },
  {
    "text": "now a big use case uh for I think U kubernetes world is also uh just object",
    "start": "1220799",
    "end": "1227039"
  },
  {
    "text": "storage so how does object storage is provided by Rook So currently there are",
    "start": "1227039",
    "end": "1232840"
  },
  {
    "text": "two ways to provide uh object storage uh the first one uh container object",
    "start": "1232840",
    "end": "1238000"
  },
  {
    "text": "storage interface it's right now in experimental phase uh it's uh ket's",
    "start": "1238000",
    "end": "1243600"
  },
  {
    "text": "suggested way uh forward to provision object storage uh but uh what we",
    "start": "1243600",
    "end": "1249600"
  },
  {
    "text": "currently use is object bucket claim now uh how it works is similar to how uh the",
    "start": "1249600",
    "end": "1256679"
  },
  {
    "text": "block storage and file storage work works for uh Rook uh what it does is it",
    "start": "1256679",
    "end": "1262200"
  },
  {
    "text": "creates a bucket uh an S3 bucket and uh then gives you access to that bucket",
    "start": "1262200",
    "end": "1268799"
  },
  {
    "text": "using uh kubernetes secrets so a bit more uh on that uh as you see The Rook",
    "start": "1268799",
    "end": "1275120"
  },
  {
    "text": "architecture maybe we focus on the orange uh side of the diagram uh you",
    "start": "1275120",
    "end": "1281039"
  },
  {
    "text": "have an app and uh you want uh that app to use the S3 client uh and uh it it",
    "start": "1281039",
    "end": "1289400"
  },
  {
    "text": "should be compliant with the S3 uh API so you just ask R to create an object",
    "start": "1289400",
    "end": "1296120"
  },
  {
    "text": "buet claim it creates a rgw bucket for you rgw is what is providing uh you uh",
    "start": "1296120",
    "end": "1304200"
  },
  {
    "text": "the bucket from safe side and these credentials uh you can access through",
    "start": "1304200",
    "end": "1309320"
  },
  {
    "text": "config map and uh also the kubernetes secret and once the bucket is",
    "start": "1309320",
    "end": "1315760"
  },
  {
    "text": "provisioned you have your access credentials in uh kubernetes resources you can get started on using uh your app",
    "start": "1315760",
    "end": "1323600"
  },
  {
    "text": "or just using S3 CLI um on any from",
    "start": "1323600",
    "end": "1328679"
  },
  {
    "text": "anywhere uh so that's how it works uh for rook and uh coming to uh",
    "start": "1328679",
    "end": "1336320"
  },
  {
    "text": "observability part of Rook we also have uh uh Prometheus and grafana integrated",
    "start": "1336320",
    "end": "1342919"
  },
  {
    "text": "with SEF and uh uh currently for Rook we are using these dashboards for uh",
    "start": "1342919",
    "end": "1348720"
  },
  {
    "text": "providing us the metrics Associated to how the data is where the data is uh you",
    "start": "1348720",
    "end": "1355520"
  },
  {
    "text": "can monitor the uh cluster status uh see how many hosts are there and uh check",
    "start": "1355520",
    "end": "1362400"
  },
  {
    "text": "for what uh if there is any failures there Prometheus alerts are integrated",
    "start": "1362400",
    "end": "1367760"
  },
  {
    "text": "as well capacity the number of object stores uh so and um also the client",
    "start": "1367760",
    "end": "1374080"
  },
  {
    "text": "throughput and other realtime um metrics um apart from uh these uh yeah this is",
    "start": "1374080",
    "end": "1381080"
  },
  {
    "text": "the second overview uh which is again uh not the integrated one but uh this is a",
    "start": "1381080",
    "end": "1387080"
  },
  {
    "text": "general grafana dashboard that you can use externally as well these all resources are available in Rook",
    "start": "1387080",
    "end": "1394120"
  },
  {
    "text": "documentation and uh you can just get started with uh observability uh in Rook",
    "start": "1394120",
    "end": "1399919"
  },
  {
    "text": "as well uh then for cluster maintenance now the big question is if I'm doing an",
    "start": "1399919",
    "end": "1405919"
  },
  {
    "text": "upgrade uh is Rook reliable in uh that particular side of things so uh how um",
    "start": "1405919",
    "end": "1414159"
  },
  {
    "text": "it works is uh if you take down uh uh node U like um all nodes can be taken",
    "start": "1414159",
    "end": "1422000"
  },
  {
    "text": "down in one entire failure Zone with no downtime and all data remains available",
    "start": "1422000",
    "end": "1427240"
  },
  {
    "text": "for both read and write what uh that means is it can uh go the read and",
    "start": "1427240",
    "end": "1432480"
  },
  {
    "text": "rights would go to the um other like uh availability zones and uh can continue",
    "start": "1432480",
    "end": "1440400"
  },
  {
    "text": "uh um the availability would not be hampered and can continue on even when",
    "start": "1440400",
    "end": "1446400"
  },
  {
    "text": "the upgrades is going on for uh uh another availability Zone and uh how the",
    "start": "1446400",
    "end": "1453000"
  },
  {
    "text": "upgrades work is uh we have something called po disruption budget uh that uh",
    "start": "1453000",
    "end": "1459320"
  },
  {
    "text": "somehow ensures that uh whenever uh yeah we bring down a Noe for maintenance that",
    "start": "1459320",
    "end": "1467080"
  },
  {
    "text": "the uh the uh copies the other copies are not hampered if we have uh we are",
    "start": "1467080",
    "end": "1472640"
  },
  {
    "text": "instructing SEF to maintain three copies at a time then uh Rook with take care of",
    "start": "1472640",
    "end": "1478840"
  },
  {
    "text": "the part that only one node or one copy uh specific node goes down at a time so",
    "start": "1478840",
    "end": "1485760"
  },
  {
    "text": "you still have availability uh for uh those two other copies now uh yeah the",
    "start": "1485760",
    "end": "1492880"
  },
  {
    "text": "same said by the other point Rook upgrades ensure that one failure domain is affected so uh um essentially no D",
    "start": "1492880",
    "end": "1502000"
  },
  {
    "text": "down time uh no hindrance to um like data loss I'd say and uh if a cluster",
    "start": "1502000",
    "end": "1508840"
  },
  {
    "text": "goes down not not that we have seen it often but if if a cluster goes down your data",
    "start": "1508840",
    "end": "1516480"
  },
  {
    "text": "is still safe as long as there's one failure domain um available so you still",
    "start": "1516480",
    "end": "1522559"
  },
  {
    "text": "have two copies or you lost two notes with those copies yeah so uh uh if even",
    "start": "1522559",
    "end": "1529360"
  },
  {
    "text": "one copy is available somewhere we can uh get the cluster up and running and uh",
    "start": "1529360",
    "end": "1535000"
  },
  {
    "text": "heal the data itself in the background as well then uh how do you uh like now",
    "start": "1535000",
    "end": "1540799"
  },
  {
    "text": "you know you can store your data reliably uh we do you want to measure",
    "start": "1540799",
    "end": "1546679"
  },
  {
    "text": "the performance for the cluster uh uh this slide covers uh uh that uh the",
    "start": "1546679",
    "end": "1552640"
  },
  {
    "text": "tools that you can use to measure the performance of the cluster uh uh I'll not go into it in much detail because we",
    "start": "1552640",
    "end": "1560000"
  },
  {
    "text": "are short on time uh now I want to pass through",
    "start": "1560000",
    "end": "1565600"
  },
  {
    "text": "shubam I think I'm sought on time thanks to them uh so I I'll try to make it",
    "start": "1565600",
    "end": "1570799"
  },
  {
    "text": "quick uh so in Rook we do have a CU cutter plugin that is based on kues crew",
    "start": "1570799",
    "end": "1576520"
  },
  {
    "text": "plug-in so for Rook it what it does is uh it is basically uh built for double",
    "start": "1576520",
    "end": "1583279"
  },
  {
    "text": "shoting and uh initially I saw some of the uh attendees here already use SEF so",
    "start": "1583279",
    "end": "1591840"
  },
  {
    "text": "to give some example what this uh CU cutle plugin does inuk is it used to run",
    "start": "1591840",
    "end": "1597240"
  },
  {
    "text": "SE command used to rest to monum and you can go through this in details in Uruk",
    "start": "1597240",
    "end": "1602320"
  },
  {
    "text": "do we do have a great dock uh uh this group plugin helps in removing bad osds",
    "start": "1602320",
    "end": "1609360"
  },
  {
    "text": "uh running some uh mons and OSD maintenance operations and it also helps",
    "start": "1609360",
    "end": "1615440"
  },
  {
    "text": "in restoring the uh crds that are accidentally in deleting State and uh we",
    "start": "1615440",
    "end": "1621960"
  },
  {
    "text": "we will also love to get some feedback on what uh other scenarios uh you will like to get automated with this plug-in",
    "start": "1621960",
    "end": "1629679"
  },
  {
    "text": "uh so happy to get some feedback and uh I will quickly go through the r project",
    "start": "1629679",
    "end": "1636399"
  },
  {
    "text": "updates so initially part mentioned that we are open source and uh Community is",
    "start": "1636399",
    "end": "1643080"
  },
  {
    "text": "important to us we do have a monthly community meeting and Rook is based on Apache",
    "start": "1643080",
    "end": "1648799"
  },
  {
    "text": "uh 2.0 and we have major contribution from four companies clo CBU IBM orat and",
    "start": "1648799",
    "end": "1657480"
  },
  {
    "text": "upbound and we have uh slightly updated we have more than 500 contributors to",
    "start": "1657480",
    "end": "1663480"
  },
  {
    "text": "get projects and three 40 million container doons and Rook is cncf graduate",
    "start": "1663480",
    "end": "1672480"
  },
  {
    "text": "project uh R got in sandwalk in January 18",
    "start": "1672519",
    "end": "1679360"
  },
  {
    "text": "incubation in September 2018 and graduated in 2020 so you can see uh with",
    "start": "1679360",
    "end": "1685720"
  },
  {
    "text": "this short timeline Rook got graduated it means that Rook is really stable and good for production",
    "start": "1685720",
    "end": "1693039"
  },
  {
    "text": "use and uh for Rook stability is number one focus and it is more than 6 years uh",
    "start": "1693039",
    "end": "1699799"
  },
  {
    "text": "Rook is declared uh stable and ready for production we have many up steam us",
    "start": "1699799",
    "end": "1705559"
  },
  {
    "text": "using rook in production and same goes for for downstreams or more customer",
    "start": "1705559",
    "end": "1711200"
  },
  {
    "text": "related deployments and Rook runs on any commodity",
    "start": "1711200",
    "end": "1716320"
  },
  {
    "text": "Hardware release cycle we Tred to release A4 cycle and our last release was 1. 15 in August and we are planning",
    "start": "1716320",
    "end": "1724760"
  },
  {
    "text": "1. 16 in December and we do have a bi-weekly uh patch release if there's",
    "start": "1724760",
    "end": "1730919"
  },
  {
    "text": "something uh we need to fix we we do have a Bi weekly releases uh project",
    "start": "1730919",
    "end": "1736799"
  },
  {
    "text": "updates uh so I will not go in details uh since uh I think uh we almost over",
    "start": "1736799",
    "end": "1742320"
  },
  {
    "text": "the time you can look at the slides uh later uh just to quickly mention few in",
    "start": "1742320",
    "end": "1748679"
  },
  {
    "text": "1.15 we we added support for SEF uh squid and uh uh yeah we also added",
    "start": "1748679",
    "end": "1755760"
  },
  {
    "text": "support for CSI operator Alpha V1 which is in Alpha V1 State and we removed the",
    "start": "1755760",
    "end": "1761799"
  },
  {
    "text": "holder pod uh uh for mulus networking and in 1.16 our main overall",
    "start": "1761799",
    "end": "1769000"
  },
  {
    "text": "theme will be supporting users with the complex need uh like disaster recovery and uh it will be more S three Focus",
    "start": "1769000",
    "end": "1777200"
  },
  {
    "text": "release I'll say and uh we will also remove support for SEF Quincy Rook tried",
    "start": "1777200",
    "end": "1783600"
  },
  {
    "text": "to uh support uh at least two latest self releases so",
    "start": "1783600",
    "end": "1789720"
  },
  {
    "text": "yeah so yeah thank you and these are the details you can check out and some appendix you can you can",
    "start": "1789720",
    "end": "1797200"
  },
  {
    "text": "look later any questions you guys have",
    "start": "1797200",
    "end": "1803880"
  },
  {
    "text": "yeah yeah yeah hello so I have a question for",
    "start": "1805519",
    "end": "1813559"
  },
  {
    "text": "CSI team like I see that you have three drivers why not have a single driver",
    "start": "1813559",
    "end": "1818799"
  },
  {
    "text": "catering to different storages oh you are already on it will be soon release like we are coming up",
    "start": "1818799",
    "end": "1825279"
  },
  {
    "text": "with the separate CSI operator which will have all in one so yeah can you can you discuss the reasoning behind like",
    "start": "1825279",
    "end": "1831679"
  },
  {
    "text": "shifting from one strategy to another earlier we had we were facing some security issues and it was designed in",
    "start": "1831679",
    "end": "1837559"
  },
  {
    "text": "that way like we had like it was upcoming and like different modes were supported but now after like all the uh",
    "start": "1837559",
    "end": "1846000"
  },
  {
    "text": "like testing and everything like we found that it is more like stable and we will be able to use it in Productions",
    "start": "1846000",
    "end": "1852159"
  },
  {
    "text": "and everything so we are coming up almost it is ready I think we'll have it in next yeah good thank you thank you",
    "start": "1852159",
    "end": "1859919"
  },
  {
    "text": "replication or sure yeah the question is how if one of the A's is down then how",
    "start": "1870679",
    "end": "1877120"
  },
  {
    "text": "we can get rid of like that downtime and still get the data available so",
    "start": "1877120",
    "end": "1883480"
  },
  {
    "text": "generally we have three replication factor and so the data that is coming is replicated to three of the nodes uh so",
    "start": "1883480",
    "end": "1890919"
  },
  {
    "text": "at least if one of the zone is available we can still recover from that data and",
    "start": "1890919",
    "end": "1896000"
  },
  {
    "text": "there is a second mode that is uh eraser coded mode that is like 2 is to one we will store two uh like data in two nodes",
    "start": "1896000",
    "end": "1903519"
  },
  {
    "text": "and one would be the key to the hash key to recover the data so uh like in these",
    "start": "1903519",
    "end": "1909840"
  },
  {
    "text": "both ways we can have that replication done so just to kind of follow on that",
    "start": "1909840",
    "end": "1915039"
  },
  {
    "text": "like okay one of the a one of the ailable and there was some data replicate into other to ensure",
    "start": "1915039",
    "end": "1923399"
  },
  {
    "text": "eventually you still have replic sure it will also depends on failure",
    "start": "1923399",
    "end": "1929279"
  },
  {
    "text": "domain you have you can either failure domain of node or osds or something so",
    "start": "1929279",
    "end": "1935720"
  },
  {
    "text": "let's say for example you have a node as a failure domain and one node go down you can plug in another node in the same",
    "start": "1935720",
    "end": "1941399"
  },
  {
    "text": "AC and then add the dis or the and your data will be it will automatically",
    "start": "1941399",
    "end": "1946919"
  },
  {
    "text": "discover that new node and proceed the data from that failure node to the third",
    "start": "1946919",
    "end": "1953039"
  },
  {
    "text": "node or five yeah yeah you can do that based on the nodes and the architecture you are planning you can play with",
    "start": "1953960",
    "end": "1961399"
  },
  {
    "text": "that uh one question yeah uh actually this is regarding the ffs uh performance",
    "start": "1961399",
    "end": "1967880"
  },
  {
    "text": "I was playing around with and I struggled with the default strategy of how you file striped across these notes",
    "start": "1967880",
    "end": "1975200"
  },
  {
    "text": "right uh the default strategy is not good it is very well documented it's a simple strategy",
    "start": "1975200",
    "end": "1980880"
  },
  {
    "text": "it's uh object size is equal to the you know the block uh stripe unit and stripe",
    "start": "1980880",
    "end": "1986399"
  },
  {
    "text": "count is one so so that that is not good so I I tried modifying the stripe count",
    "start": "1986399",
    "end": "1992440"
  },
  {
    "text": "but the the result was not coming out very good you know after the striping was not behaving the way it should be",
    "start": "1992440",
    "end": "1999279"
  },
  {
    "text": "the way it is documented right so uh have you also faced this kind of problem you tried changing the file counts the",
    "start": "1999279",
    "end": "2006000"
  },
  {
    "text": "file striping strategy uh uh generally the native uh configurations works very",
    "start": "2006000",
    "end": "2011720"
  },
  {
    "text": "well and if you have set in changing issues we can like as it's a open source you can open a request for us and we can",
    "start": "2011720",
    "end": "2018519"
  },
  {
    "text": "take take those to the community the S Community because I try filtering around with all these parameters where I change",
    "start": "2018519",
    "end": "2024360"
  },
  {
    "text": "the you know stripe count change the object sets and all that but the the the",
    "start": "2024360",
    "end": "2029519"
  },
  {
    "text": "result was not coming out to be very good you know it was not performing very well sure we can take that offline in",
    "start": "2029519",
    "end": "2034559"
  },
  {
    "text": "the booth yeah depends on your configuration on sometimes Hardware also no no so so Hardware wise we were",
    "start": "2034559",
    "end": "2040480"
  },
  {
    "text": "because we were comparing with other storages the other storage was performing pretty well it was nvma ssds",
    "start": "2040480",
    "end": "2046000"
  },
  {
    "text": "and all that so it was pretty well so from Hardware side it wasn't a problem but you know the way it is documented in",
    "start": "2046000",
    "end": "2052679"
  },
  {
    "text": "SEF also it is very clearly mentioned the default strategy for stripe uh you know file striping is very simple it is",
    "start": "2052679",
    "end": "2058240"
  },
  {
    "text": "not optimal so you need to fiddle it around you have to change it okay so I I I I tried it but you know the",
    "start": "2058240",
    "end": "2064398"
  },
  {
    "text": "performance wasn't coming to be good out got it yeah anyone else can you pass the",
    "start": "2064399",
    "end": "2070839"
  },
  {
    "text": "uh so one question uh this is regarding multi uh so for like if we are using",
    "start": "2070839",
    "end": "2077638"
  },
  {
    "text": "some external storage and that is Central site for multiple uh uh uh kuties cluster so how",
    "start": "2077639",
    "end": "2086599"
  },
  {
    "text": "do you make sure that multi tency would be uh like provided for SEF CSI CFS sure",
    "start": "2086599",
    "end": "2095280"
  },
  {
    "text": "so actually we have this external mode I talked about where the SEF is running in",
    "start": "2095280",
    "end": "2100640"
  },
  {
    "text": "the external mode and so we actually give the permissions to one of the kues",
    "start": "2100640",
    "end": "2106960"
  },
  {
    "text": "cluster in a reduced form like whatever it's important for that tenant we just",
    "start": "2106960",
    "end": "2112440"
  },
  {
    "text": "give those types of permission to that tenant and uh if someone the data is coming through that one tenant it will",
    "start": "2112440",
    "end": "2119800"
  },
  {
    "text": "store securely in particular logical name spaces like Ros Nam space and other things so that makes the feel like it",
    "start": "2119800",
    "end": "2127720"
  },
  {
    "text": "it's totally isolated between the ten so is it like you create a a separate file",
    "start": "2127720",
    "end": "2133400"
  },
  {
    "text": "uh file system for every uh cluster or so we have another logical layer that is",
    "start": "2133400",
    "end": "2139000"
  },
  {
    "text": "sub volume group so on top of file system we can logically divide that file system between the sub volume groups",
    "start": "2139000",
    "end": "2145200"
  },
  {
    "text": "actually I I was working on that and at sub volume level uh it was like at self",
    "start": "2145200",
    "end": "2151319"
  },
  {
    "text": "level for a particular file system so every client every uh I mean uh every",
    "start": "2151319",
    "end": "2159319"
  },
  {
    "text": "kubernetes cluster can access the data at sub volume level so uh we we actually",
    "start": "2159319",
    "end": "2165359"
  },
  {
    "text": "have the reduce SE users I can talk to that offline we actually reduce those permissions per tenant only okay okay",
    "start": "2165359",
    "end": "2171520"
  },
  {
    "text": "okay thank you yeah can you pass the mic uh I have one question like is there",
    "start": "2171520",
    "end": "2177240"
  },
  {
    "text": "a way you are handling that small files in the storage sorry small files small file like sometimes we have lots of",
    "start": "2177240",
    "end": "2183920"
  },
  {
    "text": "small files so we have to compact those files into one files so is there way you can handl handling that",
    "start": "2183920",
    "end": "2190160"
  },
  {
    "text": "thing so I actually didn't get anyone else small files into uh storage like uh",
    "start": "2190160",
    "end": "2198280"
  },
  {
    "text": "in suppose in we are in S3 so we are keeping lots of files the size is very small so we have to do completion sure",
    "start": "2198280",
    "end": "2205240"
  },
  {
    "text": "so is there a way like Rook handling that part that's sa side of things and I",
    "start": "2205240",
    "end": "2211920"
  },
  {
    "text": "think um we do compact data but uh not directly we are not zipping it somewhere",
    "start": "2211920",
    "end": "2219119"
  },
  {
    "text": "or compressing it by ourselves but if you want to store the data you can for",
    "start": "2219119",
    "end": "2224400"
  },
  {
    "text": "small files I think um rgw or the just uh the object storage maybe S3 bucket",
    "start": "2224400",
    "end": "2230720"
  },
  {
    "text": "would be a good use case for you but uh maybe yeah I'll take this feedback and",
    "start": "2230720",
    "end": "2236240"
  },
  {
    "text": "get to the community like we are not storing that uh small files like we are getting a",
    "start": "2236240",
    "end": "2241640"
  },
  {
    "text": "small files so after like few times we have to comp compact those files from",
    "start": "2241640",
    "end": "2247079"
  },
  {
    "text": "larger file to proceed so we uh generally for small files I have seen",
    "start": "2247079",
    "end": "2252240"
  },
  {
    "text": "like in KV sizes I've seen a bucket people using S3 bucket for storing uh",
    "start": "2252240",
    "end": "2258359"
  },
  {
    "text": "such files and uh maybe bucket could be the logical volume uh that's uh more",
    "start": "2258359",
    "end": "2263640"
  },
  {
    "text": "suited for small files yeah in your use case but uh if you uh so if you want to",
    "start": "2263640",
    "end": "2269240"
  },
  {
    "text": "access those fil then also like you have to uncompress and find where it's stored",
    "start": "2269240",
    "end": "2274280"
  },
  {
    "text": "so I don't think so it's currently like uh doing like uh compr you can maybe uh do the",
    "start": "2274280",
    "end": "2281760"
  },
  {
    "text": "compression part uh at your uh not compression competion comp basically you can discuss offline",
    "start": "2281760",
    "end": "2289240"
  },
  {
    "text": "yeah yeah sure thanks for that",
    "start": "2289240",
    "end": "2294000"
  },
  {
    "text": "feedback yeah okay I'll go thanks yeah so in your initial slide right you",
    "start": "2296680",
    "end": "2302440"
  },
  {
    "text": "mentioned about the CSI um the safe CSA add-ons and all right and you",
    "start": "2302440",
    "end": "2308400"
  },
  {
    "text": "specifically mentioned about the encryption key rotation sure so my question is like how do you manage the",
    "start": "2308400",
    "end": "2314000"
  },
  {
    "text": "encryption like do you manage the encryption natively like the r support natively or do you rely on let's say uh",
    "start": "2314000",
    "end": "2321880"
  },
  {
    "text": "AWS KMS for key rotations actually we use KMS and other storage Azure and other platforms for that andol and we",
    "start": "2321880",
    "end": "2329920"
  },
  {
    "text": "actually use the key rotation for the key that is been used and the data is not re-encrypted it's the key that is",
    "start": "2329920",
    "end": "2337480"
  },
  {
    "text": "being changed in some time so once they rotated the only new key can access that",
    "start": "2337480",
    "end": "2342920"
  },
  {
    "text": "okay and we have a kep procedure for like properly there is nothing native right",
    "start": "2342920",
    "end": "2349400"
  },
  {
    "text": "right so you're encrypting the storage in rest right the data in rest right you're encrypting it yeah that's it's",
    "start": "2349400",
    "end": "2355200"
  },
  {
    "text": "available compression and rest yeah that we have like mon end points that's uh",
    "start": "2355200",
    "end": "2360400"
  },
  {
    "text": "encryption and rest that we have thank you yeah yeah uh thanks uh for the",
    "start": "2360400",
    "end": "2365760"
  },
  {
    "text": "wonderful session so I just have a couple of queries so first is when you mentioned about the uh CSI agents that",
    "start": "2365760",
    "end": "2372040"
  },
  {
    "text": "are being deployed right so just a query on that so when you meant the demon set like the specific plug-in for for",
    "start": "2372040",
    "end": "2377760"
  },
  {
    "text": "accessing like mounting or unmounting so that means the CSI driver should also be on the nodes where your application is",
    "start": "2377760",
    "end": "2384000"
  },
  {
    "text": "also running yes yes okay and correct okay and the second query is related to",
    "start": "2384000",
    "end": "2390680"
  },
  {
    "text": "the disaster recovery right like like typically whenever we have a Noe down in our SE ecosystem or R see even when and",
    "start": "2390680",
    "end": "2397520"
  },
  {
    "text": "OSD becomes down right so that that is when your uh reweight kicks in so during",
    "start": "2397520",
    "end": "2402599"
  },
  {
    "text": "your reweight you basically have your back calculation and all those things and that is typically SEF tracks it",
    "start": "2402599",
    "end": "2408599"
  },
  {
    "text": "you'll be able to see the uh recovery rate that is there but Rook you will not be able to track it through Rook right",
    "start": "2408599",
    "end": "2414000"
  },
  {
    "text": "so if if I you can track it like I think uh I used uh so if you see there's",
    "start": "2414000",
    "end": "2420160"
  },
  {
    "text": "monitoring and uh you can see the cluster Health from here and track uh",
    "start": "2420160",
    "end": "2425359"
  },
  {
    "text": "the back fill like recovery process there's a dashboard for it as well and uh at uh what rate the recovery is",
    "start": "2425359",
    "end": "2431839"
  },
  {
    "text": "happening uh you can see that as well and the other alternative is uh you can use Cube CTL plug-in to run SEF commands",
    "start": "2431839",
    "end": "2439440"
  },
  {
    "text": "like SE Health detail will give you uh the SE or SEF status can give you uh how",
    "start": "2439440",
    "end": "2445200"
  },
  {
    "text": "what how how much back fi pgs are there uh and the third one is like uh there's",
    "start": "2445200",
    "end": "2450920"
  },
  {
    "text": "toolbox spot so whatever uh your self commands you run um natively all the",
    "start": "2450920",
    "end": "2457440"
  },
  {
    "text": "commands could be run by the toolbox Rook has this Prometheus service configured that se provides and we can",
    "start": "2457440",
    "end": "2464000"
  },
  {
    "text": "log into that dashboard and that would be like the SEF dashboard visible to you and you can observability Spectrum right",
    "start": "2464000",
    "end": "2471480"
  },
  {
    "text": "like what I'm saying is sometimes we faced issues where due to the back filling if I'm doing some kind of an IO",
    "start": "2471480",
    "end": "2476640"
  },
  {
    "text": "during that point in time The Rook operation goes through but at the end of the day if I go back to my uh SEF RBD or",
    "start": "2476640",
    "end": "2483319"
  },
  {
    "text": "to the SEF file system the file is unavailable it basically the file gets created but you know the the contents",
    "start": "2483319",
    "end": "2488760"
  },
  {
    "text": "are not even there so you are able to access with rook and not with SE is that no like the application can only talk",
    "start": "2488760",
    "end": "2494079"
  },
  {
    "text": "with Rook right at the end of the day the application that gets deployed it can only talk with Rook to basically access your SE file system so if my",
    "start": "2494079",
    "end": "2501280"
  },
  {
    "text": "application tries to write right the applications right come and the F right or whatever you do it basically comes",
    "start": "2501280",
    "end": "2506560"
  },
  {
    "text": "back as success because at the end of the day whatever goes to Via the CSI driver it only gets that result back but",
    "start": "2506560",
    "end": "2512400"
  },
  {
    "text": "the actual content may not be written onto the file system itself so that new to some Network latency or sometimes we",
    "start": "2512400",
    "end": "2518960"
  },
  {
    "text": "observed this because when there is during a back field right which version of Rook you were using not I think it's",
    "start": "2518960",
    "end": "2524839"
  },
  {
    "text": "an older version most probably it's production set right like this this",
    "start": "2524839",
    "end": "2530000"
  },
  {
    "text": "issue recently maybe in offline we can just with you guys on",
    "start": "2530000",
    "end": "2537520"
  },
  {
    "text": "that understand your Chef NFS use case you know because you're already offering Chef FSS rwx what is the exact use case",
    "start": "2540440",
    "end": "2548480"
  },
  {
    "text": "for CF NFS what are you offering in that uh we have that Network file system",
    "start": "2548480",
    "end": "2554400"
  },
  {
    "text": "support with that yeah but the point is generally people are using for R Ganesha thing like I'm not like we are not",
    "start": "2554400",
    "end": "2560760"
  },
  {
    "text": "working on deeply on NFS s but whatever like on surface level we know is we do",
    "start": "2560760",
    "end": "2566400"
  },
  {
    "text": "have NFS Ganesha uh like which is then which supports uh which is supported by",
    "start": "2566400",
    "end": "2572440"
  },
  {
    "text": "this NFS driver so I think there are very few cases like particular yeah I",
    "start": "2572440",
    "end": "2577720"
  },
  {
    "text": "know but generally people are using this for sharing data across noes so you have already have sefs as a offering there",
    "start": "2577720",
    "end": "2583760"
  },
  {
    "text": "which offers you rwx xess mode right so what what actually this brings on table you know when you compare it with CFS to",
    "start": "2583760",
    "end": "2590920"
  },
  {
    "text": "be honest actually BL from is working on that yeah but yeah we can I mean uh it's up",
    "start": "2590920",
    "end": "2598680"
  },
  {
    "text": "to you like do do you want to use NFS uh then uh there's support for it as well",
    "start": "2598680",
    "end": "2604000"
  },
  {
    "text": "of course uh by default SEF is more uh supportive towards CFS but uh we also",
    "start": "2604000",
    "end": "2610400"
  },
  {
    "text": "have NFS Ganesha in SEF natively and that's Al extended provide some Advanced",
    "start": "2610400",
    "end": "2615880"
  },
  {
    "text": "features on top of what CFS provides like sharing with like even one uh",
    "start": "2615880",
    "end": "2623359"
  },
  {
    "text": "file so maybe like like to the environment sharing and accessing at the",
    "start": "2623359",
    "end": "2628440"
  },
  {
    "text": "same time what's native nfss yeah because with sefs you have a possibility where you your iOS can go from different",
    "start": "2628440",
    "end": "2635480"
  },
  {
    "text": "noes together and it's a sefs which is managing that right because if you have an NFS running ultimately all your iOS",
    "start": "2635480",
    "end": "2641920"
  },
  {
    "text": "would land up on your on NFS service anyway right right so that and your NFS is not scalable uh what I would say",
    "start": "2641920",
    "end": "2648400"
  },
  {
    "text": "right you are are you putting NFS as a scalable so yeah it goes with the CSI driver itself yeah okay it actually it's",
    "start": "2648400",
    "end": "2655440"
  },
  {
    "text": "connected with the CSI driver that we have and it actually works how our native SFS works this actually the basic",
    "start": "2655440",
    "end": "2663680"
  },
  {
    "text": "so we have fors we have MDS pods that MDS and for NFS we have the",
    "start": "2663680",
    "end": "2669839"
  },
  {
    "text": "nfss and it actually goes through the Cs yeah I was really you know wanted to",
    "start": "2669839",
    "end": "2675440"
  },
  {
    "text": "really understand your sharing is any way can be achieved with your ffs anyway right so what what this would bring at",
    "start": "2675440",
    "end": "2681920"
  },
  {
    "text": "what extra benefits you would get by using NFS uh with us yeah that that is",
    "start": "2681920",
    "end": "2687520"
  },
  {
    "text": "something which we like get you know that pleas maybe yeah we can connect our slack can you pass it at the it is also",
    "start": "2687520",
    "end": "2693480"
  },
  {
    "text": "there in s documentation I'll have a look at it yeah thanks yeah",
    "start": "2693480",
    "end": "2699040"
  },
  {
    "text": "yeah uh uh sorry I'm a little new to this both R and SEF so I wanted to",
    "start": "2701480",
    "end": "2706559"
  },
  {
    "text": "understand uh let's say there is some issue with if the OSD has gone down or there's some disc corruption that has",
    "start": "2706559",
    "end": "2712000"
  },
  {
    "text": "happened on SEF does Rook have the capability I'm just asking From observability perspective does Rook have",
    "start": "2712000",
    "end": "2717119"
  },
  {
    "text": "the capability to uh like uh run a deep scrub or something uh and then collect",
    "start": "2717119",
    "end": "2722240"
  },
  {
    "text": "the logs and keep it ready before any disaster recovery happens so SE is very self resilient I would say and we have",
    "start": "2722240",
    "end": "2729640"
  },
  {
    "text": "it's operator and we have written controllers over that so it actually if there is some error it actually resolves",
    "start": "2729640",
    "end": "2735160"
  },
  {
    "text": "the error by itself by reconciling the operator and if it stuck somewhere we",
    "start": "2735160",
    "end": "2740200"
  },
  {
    "text": "have logs and uh log store at the host path so through that we can actually access the logs once the even the",
    "start": "2740200",
    "end": "2747160"
  },
  {
    "text": "cluster is gone or something like that so we were not able to cover trouble shooting here but uh uh I mean uh if uh",
    "start": "2747160",
    "end": "2754400"
  },
  {
    "text": "something is failing somewhere SEF is aware of that failure and uh like marks",
    "start": "2754400",
    "end": "2759839"
  },
  {
    "text": "it out that this is not part of cluster now make a new copy of it somewhere and",
    "start": "2759839",
    "end": "2765200"
  },
  {
    "text": "when the copy is made like it will uh scan for uh like there's a operation for deep scrubbing and also uh when uh like",
    "start": "2765200",
    "end": "2773599"
  },
  {
    "text": "uh if something is lost then backfilling is an operation that will uh take care of creating the new copy that went down",
    "start": "2773599",
    "end": "2780599"
  },
  {
    "text": "from the two existing copies that we already have like just to make things simpler so this is an operation of at",
    "start": "2780599",
    "end": "2787480"
  },
  {
    "text": "SEF lay Rook is just an operator uh that's making helping SEF to uh manage",
    "start": "2787480",
    "end": "2794960"
  },
  {
    "text": "uh storage in kubernetes world and seph has been doing it for 14 plus years and",
    "start": "2794960",
    "end": "2801200"
  },
  {
    "text": "big cluster so should be good okay just one more question so on a single",
    "start": "2801200",
    "end": "2807119"
  },
  {
    "text": "kubernetes cluster uh like let's say number of like multiples of three nodes since SEF recommends three is it",
    "start": "2807119",
    "end": "2813400"
  },
  {
    "text": "possible to have like multiple SEF one Rook operator can manage multiple Chef",
    "start": "2813400",
    "end": "2819359"
  },
  {
    "text": "clusters it can do that there is a way for installing multiple SEF cluster we have a quick example to do that we can",
    "start": "2819359",
    "end": "2826160"
  },
  {
    "text": "have multiple SE cluster just by on a different name space different names okay cool thank you",
    "start": "2826160",
    "end": "2834040"
  },
  {
    "text": "yeah um so",
    "start": "2834319",
    "end": "2838359"
  },
  {
    "text": "um St",
    "start": "2839880",
    "end": "2843880"
  },
  {
    "text": "can metal e i yeah that's why it's hybrid it can",
    "start": "2862920",
    "end": "2870400"
  },
  {
    "text": "with Azure it can be gcp it can be bare metal uh in your own cluster and the SEF",
    "start": "2870400",
    "end": "2875920"
  },
  {
    "text": "has so so many features as we talk about like Disaster Recovery replication and",
    "start": "2875920",
    "end": "2881240"
  },
  {
    "text": "like different types of storage we are getting out of that like the blog file and object at the same time so",
    "start": "2881240",
    "end": "2888599"
  },
  {
    "text": "effectively so you for this you are bringing the that external thing to your own cloud that's one of the purpose",
    "start": "2899000",
    "end": "2906800"
  },
  {
    "text": "maintaining that you are securing how you are managing your storage and uh and",
    "start": "2906800",
    "end": "2912839"
  },
  {
    "text": "actually if you have to get that S3 you have to pay something extra here you just need s sgds or ssds and you can",
    "start": "2912839",
    "end": "2919240"
  },
  {
    "text": "plug in play with directly with Rook it's completely open source stack and uh you can just bring uh like create your",
    "start": "2919240",
    "end": "2925839"
  },
  {
    "text": "servers and attach hard dis to it and just uh be sure that the data would be",
    "start": "2925839",
    "end": "2931040"
  },
  {
    "text": "fall tolerant it's more about if you have big cluster and you are a big user and some Cloud providers at some point",
    "start": "2931040",
    "end": "2937520"
  },
  {
    "text": "of time there are some limitations with that and that actually it solves",
    "start": "2937520",
    "end": "2943400"
  },
  {
    "text": "it so mostly we have users where the performance is the key and they have",
    "start": "2943400",
    "end": "2949680"
  },
  {
    "text": "very very big and large clusters okay",
    "start": "2949680",
    "end": "2955240"
  },
  {
    "text": "yep uh yeah you can go after that yeah uh uh see as a user if I am only",
    "start": "2955880",
    "end": "2962000"
  },
  {
    "text": "interested in object storage so Chef has an object storage and let's say minio also so have an object storage like if I",
    "start": "2962000",
    "end": "2969640"
  },
  {
    "text": "want only object storage from Chef is that possible uh to bring down the cluster of the chef where only that's",
    "start": "2969640",
    "end": "2976720"
  },
  {
    "text": "totally possible yeah so there's an option you can just completely disable uh CSI for uh RBD and SFS then your sefe",
    "start": "2976720",
    "end": "2985400"
  },
  {
    "text": "cluster um becomes just for object storage use case we have uh people using",
    "start": "2985400",
    "end": "2991319"
  },
  {
    "text": "it that way as well uh just for S3 so would the size of Chef cluster itself",
    "start": "2991319",
    "end": "2996799"
  },
  {
    "text": "reduce in yeah so uh in the diagram if you see like uh there's RBD uh last one",
    "start": "2996799",
    "end": "3005960"
  },
  {
    "text": "last it's too far so one more uh this",
    "start": "3006400",
    "end": "3011440"
  },
  {
    "text": "one uh I think after this yeah so in the diagram these units",
    "start": "3011440",
    "end": "3017160"
  },
  {
    "text": "would be available the osts would be available manager and monitor so MDS",
    "start": "3017160",
    "end": "3023079"
  },
  {
    "text": "will go away this is for uh the file system and and uh the RBD like uh C uh",
    "start": "3023079",
    "end": "3029839"
  },
  {
    "text": "CSI and uh sefs C CSI these goes away just S3 and these units uh but these",
    "start": "3029839",
    "end": "3037559"
  },
  {
    "text": "units are actually the hard dis and this is the Gateway for connecting to those",
    "start": "3037559",
    "end": "3042799"
  },
  {
    "text": "hard dis some way so your cluster reduces to this okay thank you so there",
    "start": "3042799",
    "end": "3048480"
  },
  {
    "text": "is a documentation to do this there is a simple configuration spec that you can use for that okay thank you yeah really",
    "start": "3048480",
    "end": "3057799"
  },
  {
    "text": "Benchmark your yeah so you can use the standard ones like uh uh we have our own tool as",
    "start": "3057799",
    "end": "3066400"
  },
  {
    "text": "well redos bench HS bench in-house developed and then we have I forgot the",
    "start": "3066400",
    "end": "3073079"
  },
  {
    "text": "what's the yeah fio uh workload you can run and then as well you can use",
    "start": "3073079",
    "end": "3079359"
  },
  {
    "text": "that for S3 benchmarking uh it's actually you are benchmarking the sefe cluster so",
    "start": "3079359",
    "end": "3086760"
  },
  {
    "text": "uh that's completely documented and uh if you need any help Community is always",
    "start": "3086760",
    "end": "3091880"
  },
  {
    "text": "there so yeah I think I was not able to cover it but uh yeah you can have",
    "start": "3091880",
    "end": "3098000"
  },
  {
    "text": "different use cases as well threads multiple clients and all so yeah in SEF",
    "start": "3098000",
    "end": "3105280"
  },
  {
    "text": "internal mode uh in case we have different sizes of desk and different number of desk how can via Rook we can",
    "start": "3105280",
    "end": "3114119"
  },
  {
    "text": "get so we have a feature that is called a device class but actually is like separates your uh",
    "start": "3114119",
    "end": "3120880"
  },
  {
    "text": "configurations or osds separately using that device class label so you have you",
    "start": "3120880",
    "end": "3126040"
  },
  {
    "text": "can have certain sets of osds with certain device class and connected to the application and certain big set size",
    "start": "3126040",
    "end": "3132680"
  },
  {
    "text": "of osds at certain levels and even you can label the different size of osds with same label but the thing is you",
    "start": "3132680",
    "end": "3139359"
  },
  {
    "text": "have to balance out between the nodes that you have like if you have three big and three small you should balance that",
    "start": "3139359",
    "end": "3145960"
  },
  {
    "text": "and it should be be uh totally fine for the the answer is basically creating multiple storage device device class",
    "start": "3145960",
    "end": "3153160"
  },
  {
    "text": "class thank you do you have some general uh advice",
    "start": "3153160",
    "end": "3160079"
  },
  {
    "text": "or best practices for deploying seph Rook on premises primarily on smaller",
    "start": "3160079",
    "end": "3165520"
  },
  {
    "text": "kubernetes clusters right so typically in our production we deploy it in our customers environment and these are",
    "start": "3165520",
    "end": "3171280"
  },
  {
    "text": "generally VMware data centers and most of these clusters are three node clusters now we install SEF primarily",
    "start": "3171280",
    "end": "3177920"
  },
  {
    "text": "for the rwx uh uh capabilities right uh do you have some general advice for",
    "start": "3177920",
    "end": "3184200"
  },
  {
    "text": "smaller kubernetes clusters because we do see issues like when all three nodes go down for example there's a power outage in the customer Network we",
    "start": "3184200",
    "end": "3191319"
  },
  {
    "text": "actually struggle to bring uh SEF backup because the osds just refuse to connect when all the three nodes go down and",
    "start": "3191319",
    "end": "3197160"
  },
  {
    "text": "they back up after the reboot right so I'm just wondering if there's something basic hygiene uh that we should be doing",
    "start": "3197160",
    "end": "3204240"
  },
  {
    "text": "when deploying on like on premises uh basically the examples that we have in",
    "start": "3204240",
    "end": "3209599"
  },
  {
    "text": "the MLS are perfectly set for those only like the best practices but if you",
    "start": "3209599",
    "end": "3214720"
  },
  {
    "text": "really want you can configure the osds with different zones uh so that kind of",
    "start": "3214720",
    "end": "3221920"
  },
  {
    "text": "hierarchy would help you in recovering like uh you are saying like if you are having the same nodes they can go down",
    "start": "3221920",
    "end": "3228559"
  },
  {
    "text": "at the same time but you can change the failure domains at time and try with that so each needs to be in it own",
    "start": "3228559",
    "end": "3235520"
  },
  {
    "text": "failure do exactly yeah wherever like you see three notes failing you should",
    "start": "3235520",
    "end": "3241599"
  },
  {
    "text": "have uh like a copy in somewhere else so if uh the failure domain is nodes and",
    "start": "3241599",
    "end": "3247319"
  },
  {
    "text": "all three goes down that means we are virtually having no copies available so",
    "start": "3247319",
    "end": "3252480"
  },
  {
    "text": "seph gets confused oh where to recover from so you have to make sure that uh",
    "start": "3252480",
    "end": "3259000"
  },
  {
    "text": "you have like if something is going down in one area uh there's a copy available",
    "start": "3259000",
    "end": "3264760"
  },
  {
    "text": "in some other place at least one so you can either do this or increase the",
    "start": "3264760",
    "end": "3270599"
  },
  {
    "text": "number of copies as well if you are seeing another use case is disaster recovery if it's very critical you can",
    "start": "3270599",
    "end": "3277040"
  },
  {
    "text": "have a second cluster that should be using this volume replication class that yti told and you have a second cluster",
    "start": "3277040",
    "end": "3284319"
  },
  {
    "text": "uh which if my first cluster is down you can still use your second",
    "start": "3284319",
    "end": "3289599"
  },
  {
    "text": "cluster yeah any more questions yeah sure m",
    "start": "3289599",
    "end": "3298040"
  },
  {
    "text": "yeah uh so on the HD side uh in the replication mode like three replicas are",
    "start": "3298599",
    "end": "3303720"
  },
  {
    "text": "there if one of the dis uh performance is getting degraded okay uh so what kind of impact does it bring to the cluster",
    "start": "3303720",
    "end": "3310079"
  },
  {
    "text": "as well as on the uh user side yeah so uh in a sefe cluster your best",
    "start": "3310079",
    "end": "3316079"
  },
  {
    "text": "performance is the best performance of the dis uh like uh the the best perform",
    "start": "3316079",
    "end": "3321880"
  },
  {
    "text": "the least performing disc so you can get the metrics and see that the the cluster is not performing and try to diagnose uh",
    "start": "3321880",
    "end": "3329599"
  },
  {
    "text": "which disk is actually not performing well using smart CDL metrics there's",
    "start": "3329599",
    "end": "3335000"
  },
  {
    "text": "monitoring uh uh like uh there's exporter for that as well and if you have a disc going bad of course you",
    "start": "3335000",
    "end": "3341920"
  },
  {
    "text": "should replace it in the so also is it depends on the OSD count also like if I",
    "start": "3341920",
    "end": "3348559"
  },
  {
    "text": "have large number of osds no the OSD you can add any number of osds you can like",
    "start": "3348559",
    "end": "3354880"
  },
  {
    "text": "SEF is highly scalable so you can add more nodes you can add more discs and it will scale very well the only limitation",
    "start": "3354880",
    "end": "3362319"
  },
  {
    "text": "would be I'd say uh the dis performance if dis is going bad replace it so it",
    "start": "3362319",
    "end": "3368240"
  },
  {
    "text": "will Cas to till the users so we have seen like clusters where people are running th000 osds per cluster so it's",
    "start": "3368240",
    "end": "3375160"
  },
  {
    "text": "that much scalable so it's like exabyte scale of data it can scale very well to that level in that case even if a single",
    "start": "3375160",
    "end": "3382119"
  },
  {
    "text": "uh even if one of the dis is degrading means that would affect the performance of uh can you please put hello yeah even",
    "start": "3382119",
    "end": "3389880"
  },
  {
    "text": "if one of the dis uh performance is affected so it would impact the entire",
    "start": "3389880",
    "end": "3394920"
  },
  {
    "text": "customer performance I'm just that won't be true because we have a primary the replication would be yeah whichever data",
    "start": "3394920",
    "end": "3401920"
  },
  {
    "text": "is getting stored on this uh OSD so if three replicas are happening somewhere",
    "start": "3401920",
    "end": "3407200"
  },
  {
    "text": "and it is using this disk in that case the the O like uh that replication will",
    "start": "3407200",
    "end": "3413319"
  },
  {
    "text": "get affected so we have a primary o concept so when that data comes it goes to the prime USD and then replicates it",
    "start": "3413319",
    "end": "3419839"
  },
  {
    "text": "the is near to your client it won't be affected because the replication Works behind the scenes okay uh but if the",
    "start": "3419839",
    "end": "3427079"
  },
  {
    "text": "primary OSD and you have the read Affinity enabled then there should be a problem like only those placement groups",
    "start": "3427079",
    "end": "3432880"
  },
  {
    "text": "which are in this HDs performance should be affected others are good y",
    "start": "3432880",
    "end": "3438760"
  },
  {
    "text": "okay this used for uh can rookin s be used for uh",
    "start": "3441440",
    "end": "3449640"
  },
  {
    "text": "like running U you know basically uh say terab scale fling pipelines So currently",
    "start": "3449640",
    "end": "3456200"
  },
  {
    "text": "the way the Big Data stack the way it looks conventionally on cloud is you have an S3 basically use S3 P files on",
    "start": "3456200",
    "end": "3462599"
  },
  {
    "text": "top of that you have use Iceberg um basically on top of that you basically have your uh Flink running leveraging",
    "start": "3462599",
    "end": "3469720"
  },
  {
    "text": "Reading Writing Etc which Iceberg does a lot of things of course but S3 with the basic storage of S3 so S3 has its own",
    "start": "3469720",
    "end": "3477079"
  },
  {
    "text": "quirks uh so just want you to understand has um rook and SEF combination you been",
    "start": "3477079",
    "end": "3483400"
  },
  {
    "text": "used for such use cases uh like streaming applications and back workloads like basically where an Apache",
    "start": "3483400",
    "end": "3489200"
  },
  {
    "text": "spark cluster I deploy an Apache spark e cluster I'm going to read data instead of doing it on S3 can I just move it to",
    "start": "3489200",
    "end": "3496480"
  },
  {
    "text": "uh you know block yeah which is I believe is a distributed file system and",
    "start": "3496480",
    "end": "3502240"
  },
  {
    "text": "can I basically just do the bring on the same functionality from S3 three onto uh",
    "start": "3502240",
    "end": "3508079"
  },
  {
    "text": "CFS yeah yeah that that's possible like if you are asking about migration from",
    "start": "3508079",
    "end": "3513640"
  },
  {
    "text": "one type of storage to another type of storage uh that for that you need to do some manual Works uh but if you are",
    "start": "3513640",
    "end": "3520440"
  },
  {
    "text": "talking about streamline things and the performance so we have more users around the network uh Network industry so",
    "start": "3520440",
    "end": "3528160"
  },
  {
    "text": "that's you can see like how the performances for the SEF okay so you okay I think the TA would it be like hey",
    "start": "3528160",
    "end": "3535119"
  },
  {
    "text": "okay yeah SEF and SEF and Rook can be used for Big Data Systems as well and",
    "start": "3535119",
    "end": "3540240"
  },
  {
    "text": "yeah yeah that's mostly used currently in the industry okay okay thank you last",
    "start": "3540240",
    "end": "3546359"
  },
  {
    "text": "question last question uh hello so there is another option for bloke storage that",
    "start": "3546359",
    "end": "3553119"
  },
  {
    "text": "is Long Horn so have you done comparison for in regards to Performance yes so",
    "start": "3553119",
    "end": "3560119"
  },
  {
    "text": "yeah I have heard about this long horn and this cubec Con a lot uh but the",
    "start": "3560119",
    "end": "3565160"
  },
  {
    "text": "thing is like offering they are doing is same uh but the scalability and the",
    "start": "3565160",
    "end": "3571000"
  },
  {
    "text": "things that we get from Rook is currently you want block storage right now but what if in 5 years now you are",
    "start": "3571000",
    "end": "3576640"
  },
  {
    "text": "requiring for S3 you need some migration to different tools for that but here you",
    "start": "3576640",
    "end": "3581680"
  },
  {
    "text": "are getting all three at the same time and with those scalability so that's I guess the major factor among other",
    "start": "3581680",
    "end": "3588960"
  },
  {
    "text": "features that we are getting from other tools okay have you done uh comparison in terms of uh not specific benchmarking",
    "start": "3588960",
    "end": "3595359"
  },
  {
    "text": "but we have uh yeah so sometimes uh sometimes back uh someone from community",
    "start": "3595359",
    "end": "3600760"
  },
  {
    "text": "did the benchmarking and said in our slack I don't have that handy but you",
    "start": "3600760",
    "end": "3605960"
  },
  {
    "text": "can ping me on slack or us in slack we we can say that so yeah there are some benchmarking done okay y thank you yeah",
    "start": "3605960",
    "end": "3613280"
  },
  {
    "text": "yeah for sure sorry for asking too many questions but yeah so the point is when you are allocating storage to Chef right",
    "start": "3613280",
    "end": "3619559"
  },
  {
    "text": "you have filters possibility where you define what this you know your Chef should take control of right and I",
    "start": "3619559",
    "end": "3626119"
  },
  {
    "text": "believe you also have an option that we in the host we first create uh a kind of a local striping using L walls and and",
    "start": "3626119",
    "end": "3633280"
  },
  {
    "text": "then I give uh logical volume to Chef is that is that possible right that I I",
    "start": "3633280",
    "end": "3638640"
  },
  {
    "text": "first in a host I go I have three three disc inside uh storage I create a stripe",
    "start": "3638640",
    "end": "3644240"
  },
  {
    "text": "local stripe on it and then I give a single stripe to storage per node so and the other ways I I don't create a stpe",
    "start": "3644240",
    "end": "3650799"
  },
  {
    "text": "and let three osds runs on that host right so there are two options so what what is the recommended way should we",
    "start": "3650799",
    "end": "3656839"
  },
  {
    "text": "start creating Stripes to just to make more throughput out of a system what what are your so the best way is and we",
    "start": "3656839",
    "end": "3663680"
  },
  {
    "text": "are generalizing this now so best way is to do the stpe and for stpe we mean like rados name spaces for block and sub",
    "start": "3663680",
    "end": "3670359"
  },
  {
    "text": "volume groups for file uh because a cluster has limited pgs yeah and to",
    "start": "3670359",
    "end": "3675480"
  },
  {
    "text": "scale it uh and if yes so that means we can have limited number of pools yeah so",
    "start": "3675480",
    "end": "3681119"
  },
  {
    "text": "that's why we should stripe uh in a real time because of that pz's limitation",
    "start": "3681119",
    "end": "3686160"
  },
  {
    "text": "that we have okay is there a documentation around it which gives you these best practices what you should do with your local storages how you should",
    "start": "3686160",
    "end": "3692079"
  },
  {
    "text": "allocate that storage to Chef and sure so so actually with the examples. EML we",
    "start": "3692079",
    "end": "3697319"
  },
  {
    "text": "actually try to give the best possible configurations that a user can have uh",
    "start": "3697319",
    "end": "3702359"
  },
  {
    "text": "but you can request for certain things we we can have like a certain kind of this I guess this is a good feedback we",
    "start": "3702359",
    "end": "3708400"
  },
  {
    "text": "can add okay yeah maybe a best practice is coming out of film we got this uh",
    "start": "3708400",
    "end": "3713920"
  },
  {
    "text": "question many times but you know it's very difficult to have a best practice because uh there's a network network",
    "start": "3713920",
    "end": "3720359"
  },
  {
    "text": "thing there's Hardware thing there's use case AB it's very difficult to have a common uh best use case I I understand",
    "start": "3720359",
    "end": "3727640"
  },
  {
    "text": "but you know for you to just to have some comparison you just take a single node right run three usds with three",
    "start": "3727640",
    "end": "3733200"
  },
  {
    "text": "discs and you just have create a stripe and then do that benchmarking you know where are you getting more performance",
    "start": "3733200",
    "end": "3739119"
  },
  {
    "text": "out of it there's no network coming into picture on a local storage when you are writing to a local local storage what is",
    "start": "3739119",
    "end": "3744520"
  },
  {
    "text": "the throughput you getting when you have a stripe created versus three versus running on the local systems so how we",
    "start": "3744520",
    "end": "3751079"
  },
  {
    "text": "do performance testing is we done we do fio test on the disk directly and then",
    "start": "3751079",
    "end": "3756799"
  },
  {
    "text": "we use fio with uh uh SEF and uh then we",
    "start": "3756799",
    "end": "3762079"
  },
  {
    "text": "have the performance comparison and uh also consider the threeway application in two Factor as well if it's coming uh",
    "start": "3762079",
    "end": "3770200"
  },
  {
    "text": "I think um yeah then depending on the dis like ssds and or HDD compare the uh",
    "start": "3770200",
    "end": "3777400"
  },
  {
    "text": "dis local disc performance and uh the um the se's performance that that's how we",
    "start": "3777400",
    "end": "3784760"
  },
  {
    "text": "do yeah and I was just more concerned about getting local performance out of the system you know local performance of",
    "start": "3784760",
    "end": "3790440"
  },
  {
    "text": "course uh you'll have some uh difference but uh you have to consider three-way",
    "start": "3790440",
    "end": "3796359"
  },
  {
    "text": "replication yeah that I understand yeah but I think it's good enough I can't say numbers because I like it depends on the",
    "start": "3796359",
    "end": "3803119"
  },
  {
    "text": "workload as well as and uh from my experience if you're using ssds or nvmes",
    "start": "3803119",
    "end": "3809319"
  },
  {
    "text": "it's it's better to use them and uh that's for the future as well uh and uh",
    "start": "3809319",
    "end": "3815559"
  },
  {
    "text": "I have seen decent performan I can't give you numbers but maybe offline thanks a lot any last",
    "start": "3815559",
    "end": "3824200"
  },
  {
    "text": "words so um it's regarding the Dr uh okay",
    "start": "3824319",
    "end": "3830079"
  },
  {
    "text": "disaster recording so basically on the WF side we have customers they are asking for Dr but the issue here is like",
    "start": "3830079",
    "end": "3836119"
  },
  {
    "text": "uh we have to set up the cluster one is on the one region another cluster we have to set up entirely that is",
    "start": "3836119",
    "end": "3842119"
  },
  {
    "text": "consuming resources as well as the cost is there any simplified way of like",
    "start": "3842119",
    "end": "3847920"
  },
  {
    "text": "because I just want to deploy just only one OSD instead of the entire cluster but that OSD won't Ser serve to any",
    "start": "3847920",
    "end": "3854880"
  },
  {
    "text": "clients something like that then how will you ensure fa tolerance if that one OSD goes down you can deploy you can",
    "start": "3854880",
    "end": "3861920"
  },
  {
    "text": "have manual configuration for that uh using some manual operator that that will migrate all your volumes to some",
    "start": "3861920",
    "end": "3868119"
  },
  {
    "text": "Standalone storage but we can take this offline for sure and discuss more on this yeah definitely yeah because we are",
    "start": "3868119",
    "end": "3874039"
  },
  {
    "text": "more concerned about the cost actually for the so maybe eraso coding you can look into we can discuss that more last",
    "start": "3874039",
    "end": "3881039"
  },
  {
    "text": "I think somebody was there hello uh myself bik uh uh I have",
    "start": "3881039",
    "end": "3887599"
  },
  {
    "text": "work with long on uh around 3 years ago and I'm still using long on I",
    "start": "3887599",
    "end": "3892920"
  },
  {
    "text": "contributed as contributed in Long on as well uh I want to basically understand the consider I have five notes in a",
    "start": "3892920",
    "end": "3899079"
  },
  {
    "text": "cluster uh dedicated three notes for the storage right and uh my board is schedule on uh node which I not setting",
    "start": "3899079",
    "end": "3907680"
  },
  {
    "text": "uh basically for storage right uh under the long uses the open SCSI driver right",
    "start": "3907680",
    "end": "3915720"
  },
  {
    "text": "so what are you guys using for the ssfs and RBD uh for uh block storage I",
    "start": "3915720",
    "end": "3924680"
  },
  {
    "text": "guess long is red block device and that's catering to the you can have",
    "start": "3924680",
    "end": "3929799"
  },
  {
    "text": "dedicated storage nodes using TRS and Toleration and the operator will be install on those only okay so consider",
    "start": "3929799",
    "end": "3935960"
  },
  {
    "text": "uh Ive created the volume and uh my P is scheduled on a particular P particular",
    "start": "3935960",
    "end": "3941200"
  },
  {
    "text": "node right so uh basically uh when the time of mounting uh in the long one it",
    "start": "3941200",
    "end": "3948400"
  },
  {
    "text": "creates the basically client right and attach to the uh server right so how I",
    "start": "3948400",
    "end": "3954039"
  },
  {
    "text": "got your point so we actually give the p to you and PV is not namespace code and it's generally available so you can just",
    "start": "3954039",
    "end": "3960680"
  },
  {
    "text": "give the PV name to your pod application and uh use any name space any nodes",
    "start": "3960680",
    "end": "3966599"
  },
  {
    "text": "storage using that PV so all of that is SEF side is abstracted when you like uh",
    "start": "3966599",
    "end": "3972640"
  },
  {
    "text": "need a read write uh many we'll just create a uh persistent volume for you or",
    "start": "3972640",
    "end": "3978760"
  },
  {
    "text": "claim you can just mount it and in the background SE will do his thing you just uh have to mount it anywhere you want so",
    "start": "3978760",
    "end": "3987160"
  },
  {
    "text": "yeah accessible okay and uh does S3 supports uh read write",
    "start": "3987160",
    "end": "3993119"
  },
  {
    "text": "manyu it's a bucket object storage so I mean uh for Rite many file storage is",
    "start": "3993119",
    "end": "3999720"
  },
  {
    "text": "better if you are overriding then of course you can BS yeah good",
    "start": "3999720",
    "end": "4008920"
  },
  {
    "text": "thank I guess we are good now okay maybe our our booth is also open till 5:14 so",
    "start": "4008920",
    "end": "4016200"
  },
  {
    "text": "yeah if you have more questions you can drop by yeah thank you so much guys thank you",
    "start": "4016200",
    "end": "4023680"
  }
]