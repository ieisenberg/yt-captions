[
  {
    "text": "welcome to kubecon again and uh also to everyone here and also people remote very excited",
    "start": "640",
    "end": "7359"
  },
  {
    "text": "to have in-person events anymore i really missed kubecon i must say over the last years uh welcome to our talk",
    "start": "7359",
    "end": "13840"
  },
  {
    "text": "about transparent live migration of services between kubernetes cluster across multi-cloud as you can already",
    "start": "13840",
    "end": "20320"
  },
  {
    "text": "figure out we tried to squeeze as many buzzwords in there as possible but what is it really about so it's about moving",
    "start": "20320",
    "end": "26880"
  },
  {
    "text": "stateful services so in our case that's a distributed database between different kubernetes clusters between different",
    "start": "26880",
    "end": "33440"
  },
  {
    "text": "regions and even between different cloud providers this is a quite a challenge for us because it should be transparent",
    "start": "33440",
    "end": "40079"
  },
  {
    "text": "for the user so when we do that we want our database users to actually not",
    "start": "40079",
    "end": "45680"
  },
  {
    "text": "at least not lose the availability of the cluster they might have a small performance degradation",
    "start": "45680",
    "end": "51520"
  },
  {
    "text": "but from an operational perspective this is super useful as you'll see for upgrading et cetera managing a",
    "start": "51520",
    "end": "58399"
  },
  {
    "text": "general managed service for databases we are spoiled by choice we had a number",
    "start": "58399",
    "end": "64158"
  },
  {
    "text": "of options we evaluated uh all of them they require cooperation of multiple tools from kubernetes over our",
    "start": "64159",
    "end": "70799"
  },
  {
    "text": "kubernetes operator networking layer and also the database needs to be aware so we'll talk a little",
    "start": "70799",
    "end": "76640"
  },
  {
    "text": "bit about that and also try to give like general advice for different setups and then uh we'll try the",
    "start": "76640",
    "end": "83840"
  },
  {
    "text": "challenge for everything a live demo here on stage let's all hope this goes well",
    "start": "83840",
    "end": "88960"
  },
  {
    "text": "we this is my colleague adam here hello my name is adam i joined the communities",
    "start": "88960",
    "end": "94079"
  },
  {
    "text": "of my adventure with the kubernetes itself from the version 1.1 from this point on time i worked as the devops but",
    "start": "94079",
    "end": "101680"
  },
  {
    "text": "i found that development is a little bit more interesting for me and from two and a",
    "start": "101680",
    "end": "106880"
  },
  {
    "text": "half years i'm developing the operator which is used for the anglodb",
    "start": "106880",
    "end": "113280"
  },
  {
    "text": "from this point of time we went to the a lot but i will pass it okay no i actually took the opposite",
    "start": "113280",
    "end": "120320"
  },
  {
    "text": "choice so i'm a little further away from development right now which is very sad but uh cto currently at iran could be",
    "start": "120320",
    "end": "126000"
  },
  {
    "text": "very happy to combine like my big two passions cloud nativeness so it was early on with this hype like back at",
    "start": "126000",
    "end": "131840"
  },
  {
    "text": "mesosphere uh building up also like community so it's really great to see kubecon also growing that much and then",
    "start": "131840",
    "end": "138000"
  },
  {
    "text": "yeah the other passion database systems uh also across different career stages and now yeah i all get that like within",
    "start": "138000",
    "end": "144800"
  },
  {
    "text": "one role pretty cool why are we here as a database company talking about at a yeah",
    "start": "144800",
    "end": "151200"
  },
  {
    "text": "kubecon here if we look at databases in 2022 we actually even if we look at the",
    "start": "151200",
    "end": "157440"
  },
  {
    "text": "landscape we see it's like the first block in the cncf landscape so we already see it's like quite important if",
    "start": "157440",
    "end": "163040"
  },
  {
    "text": "we walk around these across the sponsors we also see a bunch of different database companies and this is really",
    "start": "163040",
    "end": "169360"
  },
  {
    "text": "like changed over the last years being a database company nowadays usually means you are cloud native in some way or the",
    "start": "169360",
    "end": "176480"
  },
  {
    "text": "other and interestingly and this is something what i enjoy quite a lot it also means you actually change a lot of",
    "start": "176480",
    "end": "181519"
  },
  {
    "text": "things in how you built your database like five years ago it used to be those servers in some basement some fixed",
    "start": "181519",
    "end": "188400"
  },
  {
    "text": "switch in the middle uh nowadays it's basically a distributed stateful system on dynamic infrastructure kubernetes",
    "start": "188400",
    "end": "195200"
  },
  {
    "text": "telling us what to do aws telling us please reschedule this or that so it's",
    "start": "195200",
    "end": "201040"
  },
  {
    "text": "actually it's a very different game but it also feels quite interesting to turn",
    "start": "201040",
    "end": "206239"
  },
  {
    "text": "into a fully cloud native database and just from the customer side uh probably",
    "start": "206239",
    "end": "211840"
  },
  {
    "text": "like most of our users so a wrangler to be as an open source project are leveraging also some cloud service or",
    "start": "211840",
    "end": "218879"
  },
  {
    "text": "the other and most of them also our kubernetes operator and then we also have our customers who also run on",
    "start": "218879",
    "end": "226480"
  },
  {
    "text": "on the cloud on top of different options we'll come to in just a second as said orangudb is an open core",
    "start": "226480",
    "end": "234640"
  },
  {
    "text": "graph database so it's really a scalable graph database at its core but it also supports other data models such as",
    "start": "234640",
    "end": "240879"
  },
  {
    "text": "document we have a full text search and retrieval on top imagine like a lucine inbuilt uh into the graph engine uh we",
    "start": "240879",
    "end": "248319"
  },
  {
    "text": "have a graph analysis on top based on google's prego framework and we also",
    "start": "248319",
    "end": "253360"
  },
  {
    "text": "have a graph ml stack but in this talk we actually want to focus on like the left upper corner of the circle and this",
    "start": "253360",
    "end": "258959"
  },
  {
    "text": "is about how can you run a rangodb or how can you distribute it you see on the right this is just like",
    "start": "258959",
    "end": "265040"
  },
  {
    "text": "some architecture diagram i also don't want to go into too much detail it's not the focus but in short a rangodb is a",
    "start": "265040",
    "end": "271280"
  },
  {
    "text": "distributed system so kind of like you don't want to just manage that it has different components so it's not trivial",
    "start": "271280",
    "end": "278000"
  },
  {
    "text": "to set up to maintain failover etc and this is why we early on invested also in",
    "start": "278000",
    "end": "283759"
  },
  {
    "text": "the uh cuba rango it's called this is our kubernetes operator and then based on",
    "start": "283759",
    "end": "289040"
  },
  {
    "text": "top we also have our managed service or asus building up and i briefly want to introduce that because that was one of",
    "start": "289040",
    "end": "294960"
  },
  {
    "text": "the main drivers we actually came up with the changes for this talk but changes are also in the",
    "start": "294960",
    "end": "301280"
  },
  {
    "text": "open source operator so a rangodb oasis is our fully managed",
    "start": "301280",
    "end": "307520"
  },
  {
    "text": "cloud service so automated deployments failover scale up etc yada yada as you probably heard from all other vendors",
    "start": "307520",
    "end": "314160"
  },
  {
    "text": "here offering a managed service as well i think what makes us special we actually run across all the big cloud",
    "start": "314160",
    "end": "320800"
  },
  {
    "text": "providers aws google azure and we do that by leveraging their managed",
    "start": "320800",
    "end": "326720"
  },
  {
    "text": "kubernetes offerings this is actually like the naive assumption could be hey it's all kubernetes is the abstraction",
    "start": "326720",
    "end": "333360"
  },
  {
    "text": "layer interestingly it's not quite that easy there are still quite some differences in between but i think this",
    "start": "333360",
    "end": "339919"
  },
  {
    "text": "uh we are managing that pretty well right now what he just the brief uh architecture",
    "start": "339919",
    "end": "346560"
  },
  {
    "text": "piece maybe to understand uh the uh motivation for this talk we have like",
    "start": "346560",
    "end": "351600"
  },
  {
    "text": "one central control plane controlling oasis by itself but then we have data cluster so you can translate that to",
    "start": "351600",
    "end": "358240"
  },
  {
    "text": "kubernetes clusters and they are spread across different cloud providers across different regions so right now uh we",
    "start": "358240",
    "end": "365280"
  },
  {
    "text": "have uh like a very very large number of those data clusters flying around uh up to hundreds",
    "start": "365280",
    "end": "372800"
  },
  {
    "text": "of them so um this was kind of the motivation chris we were facing like some",
    "start": "372800",
    "end": "378479"
  },
  {
    "text": "operational challenges of course this is only the small subset which motivated this talk there's a lot more but to just",
    "start": "378479",
    "end": "385600"
  },
  {
    "text": "start out it's the updates of kubernetes and in particular as we're using the",
    "start": "385600",
    "end": "390639"
  },
  {
    "text": "managed kubernetes offerings we are often forced to upgrade so especially google uh they will tell us yeah you",
    "start": "390639",
    "end": "396960"
  },
  {
    "text": "have to update till then then or we will do that for you and being a stateful",
    "start": "396960",
    "end": "402479"
  },
  {
    "text": "distributed database we of course we want to be in control when we upgrade",
    "start": "402479",
    "end": "407520"
  },
  {
    "text": "and there are the first big challenges coming in and this is about that an upgraded cluster is not the same as if i",
    "start": "407520",
    "end": "413919"
  },
  {
    "text": "would spin up a new cluster with the same version so for example i think on google cloud we started with like a 114",
    "start": "413919",
    "end": "420880"
  },
  {
    "text": "and we know that when we hit uh kubernetes 124 we won't be able to upgrade anymore just because the changes",
    "start": "420880",
    "end": "427120"
  },
  {
    "text": "accumulated so far are too big and we'll actually have to deploy a new cluster probably 124 around",
    "start": "427120",
    "end": "433919"
  },
  {
    "text": "that and this is also makes of course testing much harder because if we test",
    "start": "433919",
    "end": "439199"
  },
  {
    "text": "easy thing is to just bring up a new cluster and you don't even have a choice anymore to bring up like some of the old",
    "start": "439199",
    "end": "444560"
  },
  {
    "text": "clusters and so this is one of the big operational challenges we are dealing is",
    "start": "444560",
    "end": "449680"
  },
  {
    "text": "how are we upgrading our managed kubernetes clusters and how can we also test whether it still works because",
    "start": "449680",
    "end": "455280"
  },
  {
    "text": "simply it's not exactly the same same environment next challenge is migration between",
    "start": "455280",
    "end": "461520"
  },
  {
    "text": "different regions so i mean obvious thing is like an outage in a certain region or",
    "start": "461520",
    "end": "466960"
  },
  {
    "text": "unavailability of resources in a in another region a new region simply opening up and a customer saying hey i",
    "start": "466960",
    "end": "473599"
  },
  {
    "text": "want to move my deployment closer to where i am so reducing the latency between the client and cluster",
    "start": "473599",
    "end": "480560"
  },
  {
    "text": "next thing is and even if we already start by migrating between regions we",
    "start": "480560",
    "end": "485840"
  },
  {
    "text": "also would want to do the same between different cloud providers and maybe the last migration option was",
    "start": "485840",
    "end": "492560"
  },
  {
    "text": "of course it's also great we have a number of people in the community or also customers who are running their",
    "start": "492560",
    "end": "498960"
  },
  {
    "text": "same self-deployed uh rangodb cluster on kubernetes and of course we also want to",
    "start": "498960",
    "end": "505039"
  },
  {
    "text": "offer them an option to migrate into a rangodb cloud so in short our solution",
    "start": "505039",
    "end": "510240"
  },
  {
    "text": "to that or what we want to achieve is a migration between different kubernetes clusters",
    "start": "510240",
    "end": "515360"
  },
  {
    "text": "and ideally also between different regions providers etc etc",
    "start": "515360",
    "end": "521440"
  },
  {
    "text": "maybe just briefly as we talked about this upgrade procedure what we do as like on on a pretty standard routine is",
    "start": "521440",
    "end": "527760"
  },
  {
    "text": "kind of like this two-dimensional update either upgrading the kubernetes version of a particular cluster",
    "start": "527760",
    "end": "533760"
  },
  {
    "text": "stepping up or updating the orangutp version from i don't know 3.82 to 3.83",
    "start": "533760",
    "end": "539680"
  },
  {
    "text": "etc so this is something we really do in like a daily operations level and it results in kind of the step function but",
    "start": "539680",
    "end": "546560"
  },
  {
    "text": "i said there's some limit to that so what we would actually like to do is not just upgrade the same kind of like still",
    "start": "546560",
    "end": "553760"
  },
  {
    "text": "fixed cluster but we would like to be upgrade to actually also move or upgrade",
    "start": "553760",
    "end": "558800"
  },
  {
    "text": "in between different kubernetes clusters and this could be back and forth uh so in the end we actually want to",
    "start": "558800",
    "end": "564800"
  },
  {
    "text": "move from our two-dimensional upgrades to three-dimensional structure in which we can upgrade or move so with",
    "start": "564800",
    "end": "571040"
  },
  {
    "text": "kubernetes cluster it's not always an upgrade but basically move move on that z dimension as well",
    "start": "571040",
    "end": "577839"
  },
  {
    "text": "um so um the challenge is there and um therefore i would actually love to hand",
    "start": "577839",
    "end": "584399"
  },
  {
    "text": "over to my colleague adam here because he's actually the mastermind behind all this technical implementation of that thank",
    "start": "584399",
    "end": "591360"
  },
  {
    "text": "you all right so from this specification we went out to the requirements which was",
    "start": "591360",
    "end": "596880"
  },
  {
    "text": "specified to satisfy the customers which we have it is a little bit tricky to",
    "start": "596880",
    "end": "602880"
  },
  {
    "text": "have classes which run for example one class around 100 deployments behind to make sure that",
    "start": "602880",
    "end": "609600"
  },
  {
    "text": "all of the services are all of the time provided to the customer and the customer in the best effort shouldn't",
    "start": "609600",
    "end": "616480"
  },
  {
    "text": "see even difference after the immigration during the immigration there can be small difference that's why",
    "start": "616480",
    "end": "622399"
  },
  {
    "text": "maintenance windows are useful but after the immigration there should be no difference on the",
    "start": "622399",
    "end": "628000"
  },
  {
    "text": "cluster itself also we have to remember that database and client migration is not atomic",
    "start": "628000",
    "end": "633680"
  },
  {
    "text": "operation remember when for example customer want to migrate from one region to another he",
    "start": "633680",
    "end": "639440"
  },
  {
    "text": "also probably needs to migrate his applications we need to have in mind that it will not happen at",
    "start": "639440",
    "end": "645760"
  },
  {
    "text": "the same time so there needs to be still backward compatibility where all the endpoints used by the customer still",
    "start": "645760",
    "end": "652560"
  },
  {
    "text": "gonna be reachable for everyone uh outside also from the other side while we are",
    "start": "652560",
    "end": "659920"
  },
  {
    "text": "using dns to you to propagate or database to the customer we",
    "start": "659920",
    "end": "665279"
  },
  {
    "text": "need to have in mind that there are some propagation times of the dns there are up to 24 hours",
    "start": "665279",
    "end": "671519"
  },
  {
    "text": "in this scenario even if we migrate from one place to another of course we will have different load balancers different",
    "start": "671519",
    "end": "677279"
  },
  {
    "text": "ips behind that means that we need still to propagate the old service for at least 24 hours of course configurable",
    "start": "677279",
    "end": "685040"
  },
  {
    "text": "depends on the customer use case and after that we can simply cut down their cluster",
    "start": "685040",
    "end": "691600"
  },
  {
    "text": "about latency also latency should be as low as possible we are using tcp so each bump",
    "start": "691600",
    "end": "697920"
  },
  {
    "text": "in the latency time between two servers increase the time of the queries for us",
    "start": "697920",
    "end": "703839"
  },
  {
    "text": "a lot i can say and about the performance it shouldn't",
    "start": "703839",
    "end": "709120"
  },
  {
    "text": "be affected in the matter that a customer can have decrease performance",
    "start": "709120",
    "end": "714720"
  },
  {
    "text": "in the queries but not getting timeouts on the query execution",
    "start": "714720",
    "end": "719839"
  },
  {
    "text": "about the networking also very important firewall for us was reliability this is",
    "start": "719839",
    "end": "725040"
  },
  {
    "text": "due to fact that customer is using the service all of the time he cannot stop using it so he should be",
    "start": "725040",
    "end": "731839"
  },
  {
    "text": "able all of the time to reach data which is inside about other stuff like security and",
    "start": "731839",
    "end": "738320"
  },
  {
    "text": "reachability this was also very important stuff for us but in principle",
    "start": "738320",
    "end": "743839"
  },
  {
    "text": "data is data shouldn't be able to leak outside of the cluster we are migrating data",
    "start": "743839",
    "end": "748880"
  },
  {
    "text": "between different regions that means that we need to go through the internet if we are going to cross the providers",
    "start": "748880",
    "end": "755920"
  },
  {
    "text": "if we are in the same provider we can go through the secure path inside then this is not a problem",
    "start": "755920",
    "end": "762240"
  },
  {
    "text": "and also there should be nice cooperation between all components used inside",
    "start": "762240",
    "end": "768160"
  },
  {
    "text": "it is not able to do this to do migration only from one side all of the components needs to be aware like",
    "start": "768160",
    "end": "774959"
  },
  {
    "text": "dns load balancers operators even data centers needs to be aware that something is going on",
    "start": "774959",
    "end": "782320"
  },
  {
    "text": "with after this requirements we came to four options which could we use to",
    "start": "782959",
    "end": "788000"
  },
  {
    "text": "migrate first one was the direct networking between two kubernetes clusters this was",
    "start": "788000",
    "end": "794000"
  },
  {
    "text": "an example of for example on the first stage we discussed about the vpn where",
    "start": "794000",
    "end": "799360"
  },
  {
    "text": "ports from the one cluster will be able to reach the pot in the second cluster to the port ip",
    "start": "799360",
    "end": "805200"
  },
  {
    "text": "second one was the something which was most universal for us it was cube",
    "start": "805200",
    "end": "811200"
  },
  {
    "text": "cube ctl port forward functionality like this allowed us just to expose the api",
    "start": "811200",
    "end": "817040"
  },
  {
    "text": "of the kubernetes open the tunnel and be able to simulate the traffic just pass",
    "start": "817040",
    "end": "822160"
  },
  {
    "text": "the traffic between to the cloud clusters next one which came also today as my our",
    "start": "822160",
    "end": "828480"
  },
  {
    "text": "mind was the port internet exposure we wanted to use their hot",
    "start": "828480",
    "end": "833680"
  },
  {
    "text": "host port to be able to reach to be able to expose the",
    "start": "833680",
    "end": "839519"
  },
  {
    "text": "ports through the host ip in case if you have uh cubelets exposed on the",
    "start": "839519",
    "end": "847120"
  },
  {
    "text": "if the nodes have access to the internet and they have assigned elastic ips for example and just go go through this site",
    "start": "847120",
    "end": "855519"
  },
  {
    "text": "on the last step we proposed load balancer part this is this was very this required a",
    "start": "855519",
    "end": "862240"
  },
  {
    "text": "lot of the evaluation because the mapping the main goal of the load balancers of services in kubernetes was",
    "start": "862240",
    "end": "868800"
  },
  {
    "text": "to load balance the traffic between different pots at the same stage we wanted to use it in a little bit",
    "start": "868800",
    "end": "874639"
  },
  {
    "text": "different way to be able to map one pot to one port and be able to expose multiple ports",
    "start": "874639",
    "end": "880800"
  },
  {
    "text": "through one service for example if you need to migrate three services at the same time you need to expose three",
    "start": "880800",
    "end": "886800"
  },
  {
    "text": "different ports and you do not want to create three different loadable answers because it did not make sense",
    "start": "886800",
    "end": "893519"
  },
  {
    "text": "after proposal first pocs two options were picked to be",
    "start": "893519",
    "end": "900160"
  },
  {
    "text": "implemented so direct networking which i will describe in the next slide and",
    "start": "900160",
    "end": "905279"
  },
  {
    "text": "service vote mapping based on the load balancer which i will describe in two slides",
    "start": "905279",
    "end": "911040"
  },
  {
    "text": "all right so what works for us for the first scenario we picked the silum cluster mesh mechanism which allowed us",
    "start": "911040",
    "end": "917839"
  },
  {
    "text": "to interconnect two clusters which we had anywhere on the world in",
    "start": "917839",
    "end": "923920"
  },
  {
    "text": "the demo which i will present it will be between europe and united states and all of the ports were",
    "start": "923920",
    "end": "930959"
  },
  {
    "text": "cross-reachable that means that we could reach the pod ip from cluster 2 from the",
    "start": "930959",
    "end": "936480"
  },
  {
    "text": "cluster 1. it was very useful for us but required additional logic which",
    "start": "936480",
    "end": "941839"
  },
  {
    "text": "allow us to manage the endpoints inside this is due to fact that when you specify the service you can provide the",
    "start": "941839",
    "end": "948639"
  },
  {
    "text": "selector but as you know selector is limited to one namespace",
    "start": "948639",
    "end": "955199"
  },
  {
    "text": "about that we got limitations podcadr cannot be conflicting because if",
    "start": "955199",
    "end": "960480"
  },
  {
    "text": "they've gone a conflict pos can get the same ips and even routing will not work properly",
    "start": "960480",
    "end": "966560"
  },
  {
    "text": "and it required one important precondition before you could do it is that",
    "start": "966560",
    "end": "972160"
  },
  {
    "text": "celium needs to be installed on the cluster be the network layer on the cluster and also needs to be",
    "start": "972160",
    "end": "979040"
  },
  {
    "text": "ready for the cluster mesh this was not always the case due to fact that during her test when we tried to",
    "start": "979040",
    "end": "986240"
  },
  {
    "text": "change the network layer it could provide interruption of the service due to fact that one of the pots",
    "start": "986240",
    "end": "992720"
  },
  {
    "text": "were on the new networking schema other pots were on the old",
    "start": "992720",
    "end": "998079"
  },
  {
    "text": "that's why the second option was also important for us it was service load balancer",
    "start": "998079",
    "end": "1004480"
  },
  {
    "text": "this was used this could be used almost anywhere due to fact that we did not care in this",
    "start": "1004480",
    "end": "1010880"
  },
  {
    "text": "scenario about port cidrs we have just load balancer which internally maps to some ips and in principle from our point",
    "start": "1010880",
    "end": "1018399"
  },
  {
    "text": "of view we did not pay attention for it we just needed ip of the load balancer range of the parts that was fine",
    "start": "1018399",
    "end": "1025839"
  },
  {
    "text": "this worked on all meyer cloud providers without any adjustments from our site",
    "start": "1025839",
    "end": "1031360"
  },
  {
    "text": "and this could provide us functionality to migrate specific",
    "start": "1031360",
    "end": "1037360"
  },
  {
    "text": "deployment so specific customer deployments between the regions or even cloud providers",
    "start": "1037360",
    "end": "1044558"
  },
  {
    "text": "about the limitation there was limitation about load balancer implementation because as",
    "start": "1044559",
    "end": "1050320"
  },
  {
    "text": "far as as we know on managed kubernetes cluster we have this load balancers in cloud we have these load balancers",
    "start": "1050320",
    "end": "1056799"
  },
  {
    "text": "already implemented but what with bare metal if you do not have anything which implements for your load balancer",
    "start": "1056799",
    "end": "1063840"
  },
  {
    "text": "and about latency there was issue just on the bare",
    "start": "1063840",
    "end": "1068880"
  },
  {
    "text": "metals that if you need to provide something then this brings you additional latency in your connection on cloud provider",
    "start": "1068880",
    "end": "1075840"
  },
  {
    "text": "there was no huge impact it was just one two milliseconds connection additional",
    "start": "1075840",
    "end": "1080960"
  },
  {
    "text": "latency and in principle it was just invisible for the use case",
    "start": "1080960",
    "end": "1086480"
  },
  {
    "text": "uh first scenario for the direct connection worked perfectly fine for the upgrades of the kubernetes clusters",
    "start": "1086480",
    "end": "1093600"
  },
  {
    "text": "and second scenario works perfectly fine for migration of the cluster between",
    "start": "1093600",
    "end": "1099520"
  },
  {
    "text": "regions or even continents all right so from the network perspective if we",
    "start": "1099520",
    "end": "1106080"
  },
  {
    "text": "have this load balancer and direct connection how to make sure that our service which",
    "start": "1106080",
    "end": "1111520"
  },
  {
    "text": "is tasteful will be able to talk to the proper end point on other side this is",
    "start": "1111520",
    "end": "1116559"
  },
  {
    "text": "due to fact that if you try to reach database you have usually some of some",
    "start": "1116559",
    "end": "1123360"
  },
  {
    "text": "access points so in our case coordinator which needs to know where data is placed",
    "start": "1123360",
    "end": "1129840"
  },
  {
    "text": "in this scenario all of the services and expose for us the endpoints on which they are listening and they are",
    "start": "1129840",
    "end": "1135679"
  },
  {
    "text": "reachable and we had to somehow make it transparent for cluster which we are",
    "start": "1135679",
    "end": "1140799"
  },
  {
    "text": "using that's why we used services to communicate to each bot",
    "start": "1140799",
    "end": "1146559"
  },
  {
    "text": "and in this scenario when the bots were in the same name space we had service which was called",
    "start": "1146559",
    "end": "1153440"
  },
  {
    "text": "service one was pointing to port which was behind with proper selectors",
    "start": "1153440",
    "end": "1159919"
  },
  {
    "text": "but in case if we migrated the spot to other data center this selector didn't",
    "start": "1159919",
    "end": "1165760"
  },
  {
    "text": "work anymore that's why we have wrote small we used this benefits of that you can",
    "start": "1165760",
    "end": "1173039"
  },
  {
    "text": "use definite define the endpoints in the manual way so in principle we just use the same",
    "start": "1173039",
    "end": "1178880"
  },
  {
    "text": "service we just removed the selector and we defined the endpoint by our own",
    "start": "1178880",
    "end": "1184080"
  },
  {
    "text": "in the scenarios like described here with the direct networking it was just pointing to the ip in the different",
    "start": "1184080",
    "end": "1190840"
  },
  {
    "text": "cluster of port which was already scheduled there and for the load balancing method",
    "start": "1190840",
    "end": "1197760"
  },
  {
    "text": "it was pointing to the one of the parts of the load balancer which had one port",
    "start": "1197760",
    "end": "1204240"
  },
  {
    "text": "behind on the second data cluster okay",
    "start": "1204240",
    "end": "1209760"
  },
  {
    "text": "so how the immigration looks like i would further describe what we got on the left side we had got our operator",
    "start": "1209760",
    "end": "1216080"
  },
  {
    "text": "with one service on top of it it was the load balancer service and three database instances behind",
    "start": "1216080",
    "end": "1223440"
  },
  {
    "text": "on the right side we got azure cluster where we first created the service itself",
    "start": "1223440",
    "end": "1229679"
  },
  {
    "text": "in the service we didn't provide the selector so we specified that all of the endpoints which are in on the right side",
    "start": "1229679",
    "end": "1236880"
  },
  {
    "text": "so in the second data center are pointing to the instances which are still in the",
    "start": "1236880",
    "end": "1243280"
  },
  {
    "text": "first data center then doing migration with picked first member out of the first data center",
    "start": "1243280",
    "end": "1250559"
  },
  {
    "text": "moved it to the second data center and in this scenario we were able to edit the end points of the services to",
    "start": "1250559",
    "end": "1257360"
  },
  {
    "text": "point not only to the instances which are in the same data center as the load",
    "start": "1257360",
    "end": "1262720"
  },
  {
    "text": "balancer but also to the other data center this allowed us to have the failover recovery in case if",
    "start": "1262720",
    "end": "1270080"
  },
  {
    "text": "instance in azure went down it's normal that it can went down just normal restart",
    "start": "1270080",
    "end": "1275679"
  },
  {
    "text": "there was still fall back to the instances which are in the which were in the aws",
    "start": "1275679",
    "end": "1281200"
  },
  {
    "text": "we were proceeding with that instance by instance so we picked one instance from aws",
    "start": "1281200",
    "end": "1286559"
  },
  {
    "text": "moved it to azure and modified the inputs",
    "start": "1286559",
    "end": "1291799"
  },
  {
    "text": "after all of them were already moved service in the aws was",
    "start": "1291840",
    "end": "1297679"
  },
  {
    "text": "was not pointing anymore to the any instance in the aws cloud it was just pointing to the instances in azure",
    "start": "1297679",
    "end": "1305440"
  },
  {
    "text": "operators left in azure aws but in the next step it has been moved",
    "start": "1305440",
    "end": "1311600"
  },
  {
    "text": "here we have this propagation time which we need to get for the dns propagation",
    "start": "1311600",
    "end": "1316799"
  },
  {
    "text": "so we waited until there is no any more traffic through the aws and after this has been",
    "start": "1316799",
    "end": "1324799"
  },
  {
    "text": "done so everything has migrated all dns propagation took place we finally",
    "start": "1324799",
    "end": "1330559"
  },
  {
    "text": "closed the aws cluster and we were finally migrated to the azure without",
    "start": "1330559",
    "end": "1336159"
  },
  {
    "text": "downtime in principle and now we will go to the small demo",
    "start": "1336159",
    "end": "1342159"
  },
  {
    "text": "which will present the benefits the benefits and the side effects of the migration",
    "start": "1342159",
    "end": "1348320"
  },
  {
    "text": "uh on the slide we can see uh in the description we have on the left side of each line so eol or",
    "start": "1348320",
    "end": "1355840"
  },
  {
    "text": "us we have defined where the cast client is placed then on the second place eu or us we",
    "start": "1355840",
    "end": "1363280"
  },
  {
    "text": "have defined where the service load balancer is placed in scenario on",
    "start": "1363280",
    "end": "1368640"
  },
  {
    "text": "the left top we have information that we are contacting from a group to the cluster which is placed in the error",
    "start": "1368640",
    "end": "1375200"
  },
  {
    "text": "it's a load balancer which is placed in the error on the right side we have information we",
    "start": "1375200",
    "end": "1380240"
  },
  {
    "text": "are contacting from us to the u we see that huge increase in of ping due",
    "start": "1380240",
    "end": "1385360"
  },
  {
    "text": "to that distance on the bottom left side we see that this pink is almost two times higher than",
    "start": "1385360",
    "end": "1392080"
  },
  {
    "text": "other this is due to fact that our traffic first went to the us and then had to go back to the load",
    "start": "1392080",
    "end": "1398320"
  },
  {
    "text": "balancer to the eu after that we see u.s u.s communication",
    "start": "1398320",
    "end": "1403919"
  },
  {
    "text": "it is exactly the same almost the same thing as the communication between usu",
    "start": "1403919",
    "end": "1411039"
  },
  {
    "text": "just this goal the traffic needs to go through the same path all right",
    "start": "1411039",
    "end": "1417840"
  },
  {
    "text": "just a second i will here we have the live version of it",
    "start": "1418080",
    "end": "1425120"
  },
  {
    "text": "i will go first to",
    "start": "1425120",
    "end": "1429400"
  },
  {
    "text": "the migration this is comment which we provide to the is it oh it's too small",
    "start": "1430159",
    "end": "1437440"
  },
  {
    "text": "can you i think you can just control plus",
    "start": "1437440",
    "end": "1442840"
  },
  {
    "text": "all right so here we have information which we need to provide to our operator to",
    "start": "1444799",
    "end": "1450720"
  },
  {
    "text": "migrate the clusters in this scenario we define that in the context error so all",
    "start": "1450720",
    "end": "1455840"
  },
  {
    "text": "of the pots which are placed in europe we want to migrate them to the second cluster which is in our case us",
    "start": "1455840",
    "end": "1463200"
  },
  {
    "text": "i will issue this comment [Music]",
    "start": "1463200",
    "end": "1469329"
  },
  {
    "text": "and on this on the side we will see just",
    "start": "1470320",
    "end": "1476720"
  },
  {
    "text": "on this side we'll see how the endpoints change as you can see one by one",
    "start": "1478000",
    "end": "1483760"
  },
  {
    "text": "coordinators on top you can see the eu on bottom you can see the us cluster and as you can",
    "start": "1483760",
    "end": "1490960"
  },
  {
    "text": "see the end points for both clusters are changing all of the time in this scenario in u.s once the pot",
    "start": "1490960",
    "end": "1498080"
  },
  {
    "text": "become ready in the us the end points for the service load balancers which is database and database",
    "start": "1498080",
    "end": "1505279"
  },
  {
    "text": "ea changed itself to the new values which are only in the same data center which",
    "start": "1505279",
    "end": "1511440"
  },
  {
    "text": "reduced ping for us this is this proceeds one by one so each time",
    "start": "1511440",
    "end": "1517279"
  },
  {
    "text": "one pot is killed in one data center second port appears in the data center in u.s",
    "start": "1517279",
    "end": "1525840"
  },
  {
    "text": "this is currently going home the second one started now we will",
    "start": "1525840",
    "end": "1531679"
  },
  {
    "text": "proceed we proceed with the third one we will see that third one end point disappeared",
    "start": "1531679",
    "end": "1538720"
  },
  {
    "text": "from the us just a second it will just proceed and",
    "start": "1538720",
    "end": "1544080"
  },
  {
    "text": "third boat will start in the us in a few seconds",
    "start": "1544080",
    "end": "1550240"
  },
  {
    "text": "just give me a second it is about 33 sec it should take about",
    "start": "1550240",
    "end": "1556400"
  },
  {
    "text": "33 seconds",
    "start": "1556400",
    "end": "1559360"
  },
  {
    "text": "yes going to be okay third part has been killed",
    "start": "1563120",
    "end": "1571840"
  },
  {
    "text": "and after third part will be started in the us we'll see that all of the end points",
    "start": "1573039",
    "end": "1578799"
  },
  {
    "text": "which were in the u are pointing to the both in the u.s and from this point of",
    "start": "1578799",
    "end": "1586080"
  },
  {
    "text": "time our service is fully migrated and due to the fact that we use the services",
    "start": "1586080",
    "end": "1591520"
  },
  {
    "text": "inside everything went in the transparent way for service which was in this namespace",
    "start": "1591520",
    "end": "1599120"
  },
  {
    "text": "okay i'm going back to presentation",
    "start": "1599120",
    "end": "1607880"
  },
  {
    "text": "and what we learned to this uh to this implementation phase and investigation space",
    "start": "1613039",
    "end": "1619520"
  },
  {
    "text": "that what did not work for us we got a first idea about the port",
    "start": "1619520",
    "end": "1624880"
  },
  {
    "text": "forward but in principle it gave additional overhead in case of the days",
    "start": "1624880",
    "end": "1630400"
  },
  {
    "text": "and it just was not there a regal eligible solution for us because it",
    "start": "1630400",
    "end": "1637200"
  },
  {
    "text": "added like 40 milliseconds of the delay even if we migrated within the same region",
    "start": "1637200",
    "end": "1643440"
  },
  {
    "text": "also exposing ports on hostport via public ip didn't work due to the",
    "start": "1643440",
    "end": "1648880"
  },
  {
    "text": "security because our data needs to be protected and hidden in the private network and we",
    "start": "1648880",
    "end": "1654880"
  },
  {
    "text": "need to have the full security like lobby like the firewalls or security",
    "start": "1654880",
    "end": "1660480"
  },
  {
    "text": "groups to protect access to it we also had on the first step plan about",
    "start": "1660480",
    "end": "1667200"
  },
  {
    "text": "manually meant managing the end points during the migration but the problem was the scale",
    "start": "1667200",
    "end": "1674320"
  },
  {
    "text": "you can do it for one customer but not for 200 customers in the regular period",
    "start": "1674320",
    "end": "1679919"
  },
  {
    "text": "and why did we came with two solutions instead of one this was due to the",
    "start": "1679919",
    "end": "1686559"
  },
  {
    "text": "different kinds of the updates when we updated when we had to migrate whole cluster it was easier for us to set up",
    "start": "1686559",
    "end": "1692480"
  },
  {
    "text": "cluster mesh and do it for everything what is inside but if we had to migrate just one",
    "start": "1692480",
    "end": "1698320"
  },
  {
    "text": "cluster within the kubernetes cluster it was just unnecessary to set up the service mesh we could do it to",
    "start": "1698320",
    "end": "1705360"
  },
  {
    "text": "the load balancer and do it just for this particular deployment",
    "start": "1705360",
    "end": "1710960"
  },
  {
    "text": "for the future our plan is to provide the operator in two namespaces right now",
    "start": "1710960",
    "end": "1717919"
  },
  {
    "text": "when we were having operator in hi we were providing functionality which use the kubernetes",
    "start": "1717919",
    "end": "1724240"
  },
  {
    "text": "api leader election but it will not work when you are in two kubernetes clusters",
    "start": "1724240",
    "end": "1729440"
  },
  {
    "text": "we need to came with all better solutions which will allow us to make sure that at least one of them",
    "start": "1729440",
    "end": "1735039"
  },
  {
    "text": "will be active this is due to the fact what in case if first data center will go down",
    "start": "1735039",
    "end": "1740720"
  },
  {
    "text": "the second needs to still work and based on this migration we came to",
    "start": "1740720",
    "end": "1745840"
  },
  {
    "text": "conclusion that distributed clusters can be also idea when you will come to the",
    "start": "1745840",
    "end": "1751760"
  },
  {
    "text": "you will use three different regions to spin off your cluster to be able to handle the single region",
    "start": "1751760",
    "end": "1757919"
  },
  {
    "text": "fails failure which might be very useful for customers which demand such",
    "start": "1757919",
    "end": "1765279"
  },
  {
    "text": "a resiliency of their solution all right so",
    "start": "1765279",
    "end": "1771200"
  },
  {
    "text": "yeah thanks",
    "start": "1771200",
    "end": "1774919"
  },
  {
    "text": "thanks very much feel free to drop by if you have any questions about avocados about uh",
    "start": "1780880",
    "end": "1786559"
  },
  {
    "text": "cloud migrating services graph databases or anything else our boost is",
    "start": "1786559",
    "end": "1792000"
  },
  {
    "text": "stand 72 i will leave pablo on two but easy just follow the avocado or just",
    "start": "1792000",
    "end": "1797440"
  },
  {
    "text": "drop by here thanks very much thank you",
    "start": "1797440",
    "end": "1802440"
  }
]