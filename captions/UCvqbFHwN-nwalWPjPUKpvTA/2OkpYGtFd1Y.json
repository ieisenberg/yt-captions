[
  {
    "text": "let's jump into it uh welcome to my session behind schedule pod resource",
    "start": "480",
    "end": "5640"
  },
  {
    "text": "configuration from beginning to I'm sure we've all had that moment where we we thought that something was",
    "start": "5640",
    "end": "12120"
  },
  {
    "text": "going to start running and it did not and it didn't seem like it did so for a reason that made any sense at",
    "start": "12120",
    "end": "18760"
  },
  {
    "text": "all yes so let's do a little bit of introduction I'll tell you a little bit",
    "start": "19840",
    "end": "25640"
  },
  {
    "text": "about myself although not too much so we don't take up too much time I like to take the temp tempure the room a little",
    "start": "25640",
    "end": "30800"
  },
  {
    "text": "bit as I get started too and then kind of what are we here for today so before we get started and for real it's really",
    "start": "30800",
    "end": "39120"
  },
  {
    "text": "dry out there if you just flew in last night and you woke up this morning and your throat felt like the desert outside",
    "start": "39120",
    "end": "46039"
  },
  {
    "text": "Salt Lake City and Utah are the one of the driest areas in the US please for your own health and safety drink water",
    "start": "46039",
    "end": "53280"
  },
  {
    "text": "constantly the whole time you're here there's water bottle swag at Plenty Of Enders I'm sure go buy one from the cncf",
    "start": "53280",
    "end": "60000"
  },
  {
    "text": "store if you have need one uh definitely it is worth your time it's worth your",
    "start": "60000",
    "end": "65119"
  },
  {
    "text": "time and attention to make sure that you're getting enough water here so who am I uh my name is Joe",
    "start": "65119",
    "end": "71920"
  },
  {
    "text": "Thompson uh I've been working in it overall for almost 30 years depending on",
    "start": "71920",
    "end": "77320"
  },
  {
    "text": "how you count uh I usually count from December of 1995 that was my first full-time paying it job along the way to",
    "start": "77320",
    "end": "85720"
  },
  {
    "text": "get to today I've worked for Red Hat cor mesosphere hashy forp some of the names",
    "start": "85720",
    "end": "90759"
  },
  {
    "text": "people in the room might know uh currently I a Consulting engineer for a",
    "start": "90759",
    "end": "96320"
  },
  {
    "text": "company called Clarity Business Solutions they're based uh fairly near DC and the part of the company that I",
    "start": "96320",
    "end": "102240"
  },
  {
    "text": "work in is their mongod DB partnership uh we provide escalation support for",
    "start": "102240",
    "end": "107520"
  },
  {
    "text": "mongod DB customers among other things my basic pronouns right there uh",
    "start": "107520",
    "end": "113200"
  },
  {
    "text": "the pop culture reference center of gravity is fairly important if you're talking to me you will eventually get a",
    "start": "113200",
    "end": "118640"
  },
  {
    "text": "movie reference to like a John Hughes movie or office space or something like",
    "start": "118640",
    "end": "124159"
  },
  {
    "text": "that and there's some contact info there now as far as people in the room I see we still have some people coming in but",
    "start": "124159",
    "end": "130080"
  },
  {
    "text": "it looks like we're pretty full up now uh who here this is your first cubec con okay yeah usually a lot of hands go",
    "start": "130080",
    "end": "136760"
  },
  {
    "text": "up for that um who here has been working with kubernetes for less than a",
    "start": "136760",
    "end": "143200"
  },
  {
    "text": "year okay keep your hands up uh put your hands down if you've been working more than 6 months who hear still is okay so",
    "start": "143200",
    "end": "150720"
  },
  {
    "text": "we have a few that are really really new to this so great because this is very much a Basics talk but it does get into",
    "start": "150720",
    "end": "157239"
  },
  {
    "text": "kind of like ezure learning the basics you you hit that odd moment and",
    "start": "157239",
    "end": "162680"
  },
  {
    "text": "you go wait that's not right and it is right it's very simple behavior and it",
    "start": "162680",
    "end": "169080"
  },
  {
    "text": "has its own internal logic but you kind of have to understand what that logic",
    "start": "169080",
    "end": "175040"
  },
  {
    "text": "is so the very basics of resources if you've been working with kubernetes for any any length of time at all you know",
    "start": "175040",
    "end": "182319"
  },
  {
    "text": "that requests are a floor for resource allocation that is the minimum amount of whatever resource it is that those",
    "start": "182319",
    "end": "188159"
  },
  {
    "text": "containers in that pod need before that pod will get scheduled on a node there isn't enough on that node it won't get",
    "start": "188159",
    "end": "193440"
  },
  {
    "text": "scheduled there the limits are the opposite they're the ceiling if a container exceeds a limit then it gets",
    "start": "193440",
    "end": "201879"
  },
  {
    "text": "terminated there's actually a little asterisk on that which we'll kind of go into later requested resources that are",
    "start": "201879",
    "end": "208879"
  },
  {
    "text": "unused are was Ed resources and I mentioned there's going to be an asterisk on termination there",
    "start": "208879",
    "end": "215840"
  },
  {
    "text": "are resources that are compressible and there are resources that are incompressible so if you exceed the",
    "start": "215840",
    "end": "221840"
  },
  {
    "text": "limit on an incompressible resource like memory you just get terminated if you try to exceed the",
    "start": "221840",
    "end": "228439"
  },
  {
    "text": "limit on a compressible resource like CPU you don't get terminated you get",
    "start": "228439",
    "end": "234319"
  },
  {
    "text": "throttled now there is this thing called quality of service and this is where if you haven't been working with kubernetes",
    "start": "235400",
    "end": "241400"
  },
  {
    "text": "for very long you may not have hit this yet quality of service is a classification of your pods that",
    "start": "241400",
    "end": "247920"
  },
  {
    "text": "kubernetes assigns you don't set it kubernetes assigns it based on the requests and limits that you",
    "start": "247920",
    "end": "255199"
  },
  {
    "text": "set now in normal operation just your pod running doing its thing what quality",
    "start": "255199",
    "end": "260959"
  },
  {
    "text": "of service it has doesn't matter it doesn't affect the running of the pot it only comes into play when things start",
    "start": "260959",
    "end": "266440"
  },
  {
    "text": "to get congested and so those three classes of",
    "start": "266440",
    "end": "271960"
  },
  {
    "text": "quality of service can be best effort burstable or guaranteed now notice I put asterisks on no effect and also on",
    "start": "271960",
    "end": "280400"
  },
  {
    "text": "guaranteed there are there are footnotes there uh when I say it has no effect on",
    "start": "280400",
    "end": "286360"
  },
  {
    "text": "the operation of the Pod there actually are things that you cannot do unless you have the guaranteed quality of service",
    "start": "286360",
    "end": "292280"
  },
  {
    "text": "class things that you are not allowed to access like reserved CPUs guaranteed is not actually",
    "start": "292280",
    "end": "299199"
  },
  {
    "text": "guaranteed in much the same way that a kubernetes secret is not really a secret at all uh guaranteed just means that you",
    "start": "299199",
    "end": "306360"
  },
  {
    "text": "won't get evicted for exceeding your limits there are other conditions under which a guaranteed pod can actually be",
    "start": "306360",
    "end": "311919"
  },
  {
    "text": "terminated and moved out of the way now alongside of quality of service",
    "start": "311919",
    "end": "318240"
  },
  {
    "text": "there is the concept of priority priority is user controllable pods get a priority of Zero",
    "start": "318240",
    "end": "325560"
  },
  {
    "text": "by default and you normally don't refer to them by number you refer to them by name so you'll have a priority class",
    "start": "325560",
    "end": "332759"
  },
  {
    "text": "that has a name that then maps to a numeric priority and so you can map a very high",
    "start": "332759",
    "end": "339960"
  },
  {
    "text": "priority you can map a very low priority by default your cluster is going to have two very high priorities called system",
    "start": "339960",
    "end": "345199"
  },
  {
    "text": "node critical and system cluster critical and like quality of service",
    "start": "345199",
    "end": "350720"
  },
  {
    "text": "priority doesn't really affect your pod just do running and doing its thing so that's all fairly easy there",
    "start": "350720",
    "end": "358880"
  },
  {
    "text": "there's some caveats and footnotes there but it's going to get more complicated",
    "start": "358880",
    "end": "364240"
  },
  {
    "text": "then it's going to get weird okay so what happens when we start to put our clusters under resource",
    "start": "364240",
    "end": "370840"
  },
  {
    "text": "pressure we got too much Freight on our railroad okay so there is eviction and",
    "start": "370840",
    "end": "377720"
  },
  {
    "text": "there is preemption and there are different mechanisms that are kind of in",
    "start": "377720",
    "end": "383080"
  },
  {
    "text": "the same category they're both trying to make sure that the things that need to run can run but they're coming at it",
    "start": "383080",
    "end": "388560"
  },
  {
    "text": "from different directions so eviction is triggered by the cuet when the node comes Under Pressure now",
    "start": "388560",
    "end": "394479"
  },
  {
    "text": "see there's another asterisk there there's going to be a footnote preemption is triggered by the",
    "start": "394479",
    "end": "400639"
  },
  {
    "text": "scheduler when a high prior oh I see I have a typo there when a high priority pod can't be scheduled normally there",
    "start": "400639",
    "end": "407080"
  },
  {
    "text": "are not enough resources on any node in the cluster to just assign to that pod and schedule it then preemption kicks",
    "start": "407080",
    "end": "413680"
  },
  {
    "text": "in so the asterisk on Under Pressure there is actually an eviction API",
    "start": "413680",
    "end": "419000"
  },
  {
    "text": "request you can just say I want you to evict that pod whether or not you're under pressure I just want you to evict it",
    "start": "419000",
    "end": "426440"
  },
  {
    "text": "now now where this starts to get complicated is scheduling is not a",
    "start": "426560",
    "end": "433960"
  },
  {
    "text": "promise so something gets preempted and kicked out of a node and it goes back",
    "start": "433960",
    "end": "439440"
  },
  {
    "text": "into the scheduler queue but that doesn't mean it will actually get rescheduled something gets evicted and",
    "start": "439440",
    "end": "446160"
  },
  {
    "text": "goes into the schedule queue there's no guarantee it will be rescheduled all we're promised is that we'll end up",
    "start": "446160",
    "end": "452639"
  },
  {
    "text": "back in the queue it can stay in that queue arbitrarily long especially if your cluster is under really heavy resource",
    "start": "452639",
    "end": "459440"
  },
  {
    "text": "pressure um one way of buying your way out of that situation is by autoscaling just",
    "start": "459440",
    "end": "465800"
  },
  {
    "text": "put some more resources in the cluster so that things can happen normally so that preemption and eviction are not in",
    "start": "465800",
    "end": "471520"
  },
  {
    "text": "play anymore okay so if you can't afford to",
    "start": "471520",
    "end": "477400"
  },
  {
    "text": "buy your way out of it what you may try to do is go and adjust your resource requests but you can get into the",
    "start": "477400",
    "end": "483520"
  },
  {
    "text": "situation where you say okay I'm going to adjust my request down so more things can run and I'm not wasting as much",
    "start": "483520",
    "end": "491000"
  },
  {
    "text": "resources but then things start to get evicted more often because pods that exceed their requests",
    "start": "491000",
    "end": "498560"
  },
  {
    "text": "are the first out the door so if you adjust your requests down too far like",
    "start": "498560",
    "end": "505120"
  },
  {
    "text": "people think of it as a floor and a ceiling right you want your request to be low lower than what you normally",
    "start": "505120",
    "end": "511319"
  },
  {
    "text": "consume although not too much lower and you want your limits to be the top of",
    "start": "511319",
    "end": "516760"
  },
  {
    "text": "what you would normally consume but really what you should do is you should set your requests slightly higher than",
    "start": "516760",
    "end": "522518"
  },
  {
    "text": "normal and that way you won't trigger that first in line behavior of oh you're",
    "start": "522519",
    "end": "527560"
  },
  {
    "text": "you're just slightly over your resource request over is over get",
    "start": "527560",
    "end": "533440"
  },
  {
    "text": "out so setting your request to the absolute floor can backfire on you",
    "start": "533440",
    "end": "539920"
  },
  {
    "text": "uh preemption has a dawn Draper moment anybody see that Meme where it's the it's Don Draper and the other guy in the elevator and the guy says I feel bad for",
    "start": "539920",
    "end": "547120"
  },
  {
    "text": "you and Don Draper says I don't think about you at all well eviction uses priority as part",
    "start": "547120",
    "end": "553600"
  },
  {
    "text": "of its Logic for which pods it's going to terminate and evict preemption doeses not care at all about quality of",
    "start": "553600",
    "end": "560399"
  },
  {
    "text": "service so you can end up with a situation where high priority best effort pod",
    "start": "560399",
    "end": "566600"
  },
  {
    "text": "survives low quality of service but a low priority guaranteed when gets booted out it's not very guaranteed at that",
    "start": "566600",
    "end": "575600"
  },
  {
    "text": "point preemption math this one I really had to look at this the first time I ran",
    "start": "575880",
    "end": "581440"
  },
  {
    "text": "across it I went can that really be right before it tries to evict any pods",
    "start": "581440",
    "end": "589600"
  },
  {
    "text": "on a candidate node the scheduler will say I have a pod of priority X if I",
    "start": "589600",
    "end": "596240"
  },
  {
    "text": "evict all of the nodes lower than priority or all of the pods lower than priority X on that",
    "start": "596240",
    "end": "602920"
  },
  {
    "text": "node will I be able to schedule this pod there now that makes a difference",
    "start": "602920",
    "end": "608160"
  },
  {
    "text": "because there are cases where if you just evicted some of those pods you would be able to schedule it but",
    "start": "608160",
    "end": "613800"
  },
  {
    "text": "evicting all of them makes that pod unschedulable because there are for",
    "start": "613800",
    "end": "619040"
  },
  {
    "text": "example Affinity rules or anti-affinity rules and if you evict a pod that is the",
    "start": "619040",
    "end": "625040"
  },
  {
    "text": "target of an affinity rule on the Pod you're trying to schedule now you can't schedule anymore",
    "start": "625040",
    "end": "631160"
  },
  {
    "text": "but it doesn't try to pick and choose what it's doing it's kind of considering a worst case scenario it's trying to",
    "start": "631160",
    "end": "636760"
  },
  {
    "text": "avoid a situation where it evicts and evicts and evicts and evicts and then it ends up it can't schedule at all so it",
    "start": "636760",
    "end": "642000"
  },
  {
    "text": "evicted a bunch of pods for nothing so it just considers that worst case scenario right up front if the worst",
    "start": "642000",
    "end": "647920"
  },
  {
    "text": "case scenario happens and I still can't schedule this thing I'm not even going to",
    "start": "647920",
    "end": "654399"
  },
  {
    "text": "try now eviction has a dawn Draper moment preemption will try to respect your your",
    "start": "654760",
    "end": "661800"
  },
  {
    "text": "pod disruption budgets pod disruption budget for those who may not have run across that yet that's where you say I",
    "start": "661800",
    "end": "668360"
  },
  {
    "text": "want to make sure that a certain number of this particular set of pod replicas is always running or I want to set a",
    "start": "668360",
    "end": "675600"
  },
  {
    "text": "maximum number that will be unavailable there's a lot of detail there that's",
    "start": "675600",
    "end": "680720"
  },
  {
    "text": "kind of beyond the scope of this but that's basically what it is it's a mechanism of protecting your application",
    "start": "680720",
    "end": "686800"
  },
  {
    "text": "from too many of its replicas going offline all at once preemption doesn't care well no",
    "start": "686800",
    "end": "692760"
  },
  {
    "text": "preemption does care it will try to respect your pod disruption budgets but it will still go ahead and knock things",
    "start": "692760",
    "end": "698040"
  },
  {
    "text": "out if it can't node pressure eviction doesn't care at all it won't even care about",
    "start": "698040",
    "end": "704000"
  },
  {
    "text": "your termination grace period seconds if it decides that this pod needs to go it's",
    "start": "704000",
    "end": "709800"
  },
  {
    "text": "gone that API initiated eviction that I mentioned before that does respect pod disruption budgets so there's you know",
    "start": "709880",
    "end": "718120"
  },
  {
    "text": "you'll see advice some places where they'll say you should be monitoring your nodes and rather than let your",
    "start": "718120",
    "end": "723760"
  },
  {
    "text": "nodes go into eviction you should start proactively using the API eviction",
    "start": "723760",
    "end": "730600"
  },
  {
    "text": "endpoint to evict pods proactively because then your pod disruption budgets will be respected and you'll have less",
    "start": "730600",
    "end": "736680"
  },
  {
    "text": "disruption overall now how are we going to manage",
    "start": "736680",
    "end": "742600"
  },
  {
    "text": "this whole mess okay so the first rule is always",
    "start": "742600",
    "end": "748680"
  },
  {
    "text": "keep it simple make sure that you can reason about the pods in your cluster make sure you know",
    "start": "748680",
    "end": "755600"
  },
  {
    "text": "how are they going to get scheduled where are they going to get scheduled what kind of node are they going to get scheduled on don't get too fancy with",
    "start": "755600",
    "end": "762519"
  },
  {
    "text": "things don't go wild creating whole you know intricate structures of priority",
    "start": "762519",
    "end": "767680"
  },
  {
    "text": "classes or these huge complex Affinity rules or you know I've seen people",
    "start": "767680",
    "end": "772880"
  },
  {
    "text": "create like people who are using the carpenter scheduler they create just tons and tons and tons of different very",
    "start": "772880",
    "end": "778519"
  },
  {
    "text": "very specific node pools and then it ends up biting them in later on when",
    "start": "778519",
    "end": "784360"
  },
  {
    "text": "their rules are so specific that things can't get scheduled sometimes you may need to just",
    "start": "784360",
    "end": "791920"
  },
  {
    "text": "consider whether you need separate node groups maybe you even need separate clusters for some purposes just to make",
    "start": "791920",
    "end": "798880"
  },
  {
    "text": "sure that the scheduling can stay simple so that it can stay",
    "start": "798880",
    "end": "803920"
  },
  {
    "text": "understandable use your pod disruption budgets don't count on them though because you know the other footnote to",
    "start": "804560",
    "end": "810560"
  },
  {
    "text": "pdbs is they only protect you from voluntary disruption if a node just hard",
    "start": "810560",
    "end": "817199"
  },
  {
    "text": "goes down pdb doesn't matter there's nothing that there's nothing kubernetes can do about a physical node failure so",
    "start": "817199",
    "end": "825519"
  },
  {
    "text": "you know if you're counting on pdbs to be the end all Beall they're",
    "start": "825519",
    "end": "830959"
  },
  {
    "text": "not this one I almost didn't want to put this in here you have to be really really really",
    "start": "832320",
    "end": "839920"
  },
  {
    "text": "careful when you tune your eviction thresholds and the reason is",
    "start": "839920",
    "end": "846759"
  },
  {
    "text": "because if you tune them wrong you're going to do one of two things you're either going to have nodes that have a",
    "start": "846759",
    "end": "851959"
  },
  {
    "text": "lot of wasted resources that can never get used or you're going to have nodes that end up in such tight resource",
    "start": "851959",
    "end": "859440"
  },
  {
    "text": "constraints that they're in danger of deadlocking and remember that there are",
    "start": "859440",
    "end": "865759"
  },
  {
    "text": "things on your nodes running other than kubernetes pods not only the kubernetes components but basic system components",
    "start": "865759",
    "end": "871959"
  },
  {
    "text": "and if your nodes end up in really bad resource constraint situations some of",
    "start": "871959",
    "end": "877079"
  },
  {
    "text": "those critical components that you consider critical outside of kubernetes may end up getting",
    "start": "877079",
    "end": "883079"
  },
  {
    "text": "killed uh you know they're set very well relatively conservatively by default so it is a tunable thing be very careful",
    "start": "883079",
    "end": "890399"
  },
  {
    "text": "how you tune it um if you tune it I would say lean more in the direction of tuning it conservatively rather than",
    "start": "890399",
    "end": "897920"
  },
  {
    "text": "Dangerously because you can usually recover from spending too much money it can be hard",
    "start": "897920",
    "end": "903240"
  },
  {
    "text": "to recover from an application outage okay so you got all these great",
    "start": "903240",
    "end": "910040"
  },
  {
    "text": "clever ideas what you're going to do about all this stuff you got to test them you got to",
    "start": "910040",
    "end": "917720"
  },
  {
    "text": "test them in your test environment and how many people in here",
    "start": "917720",
    "end": "923800"
  },
  {
    "text": "how many people in here have used a test environment where they work you have one okay how many people in here you either",
    "start": "923800",
    "end": "930560"
  },
  {
    "text": "don't have one or you do have never been able to use it I see a couple of hands you've got",
    "start": "930560",
    "end": "936279"
  },
  {
    "text": "one too it's called",
    "start": "936279",
    "end": "939480"
  },
  {
    "text": "production uh yeah there there's you know that you'll see some memes every now and then people will go yeah go",
    "start": "941480",
    "end": "948079"
  },
  {
    "text": "ahead and test in production because effectively you are anyway you can put things through as many different lower test environments as you want but",
    "start": "948079",
    "end": "954959"
  },
  {
    "text": "production is the acid test as they say",
    "start": "954959",
    "end": "960120"
  },
  {
    "text": "so if all else fails throw money at the problem we all have infinite budgets right it's always easy to go back and",
    "start": "960519",
    "end": "966839"
  },
  {
    "text": "say I need another $100,000 for AWS Bill okay because remember in normal",
    "start": "966839",
    "end": "973519"
  },
  {
    "text": "operation none asterisk a few things do matter and everything has enough",
    "start": "973519",
    "end": "979720"
  },
  {
    "text": "resources to schedule you want ideally to be in a situation where there is no eviction there is no preemption because",
    "start": "979720",
    "end": "986240"
  },
  {
    "text": "everything is fully adequately provisioned all of your workloads are running none of them are restarting none",
    "start": "986240",
    "end": "992440"
  },
  {
    "text": "of them are getting evicted or oom killed or you know suffering from unmet Affinity constraints or anything like",
    "start": "992440",
    "end": "998319"
  },
  {
    "text": "that that is the state you want to be in sometimes you just have to spend the",
    "start": "998319",
    "end": "1004399"
  },
  {
    "text": "money to get there now I mentioned I think I mentioned the carpenter autoscaler",
    "start": "1004399",
    "end": "1010920"
  },
  {
    "text": "before it will actually do a fairly good job of this um for those who've used the",
    "start": "1010920",
    "end": "1017000"
  },
  {
    "text": "cluster autoscaler um it's not a 100% like you should stop using that and",
    "start": "1017000",
    "end": "1022560"
  },
  {
    "text": "start using this there are situations where the cluster autoscaler is more appropriate for whatever workload you're",
    "start": "1022560",
    "end": "1028918"
  },
  {
    "text": "running particularly because Carpenter doesn't support a lot of environments yet but between the cluster autoscaler",
    "start": "1028919",
    "end": "1036160"
  },
  {
    "text": "and Carpenter you can usually make sure you have enough resources now there are",
    "start": "1036160",
    "end": "1041319"
  },
  {
    "text": "other autoscalers out there anybody here worked with I don't I'm not entirely sure the proper pronunciation of this",
    "start": "1041319",
    "end": "1048280"
  },
  {
    "text": "kada K few people here and there yeah so",
    "start": "1048280",
    "end": "1053400"
  },
  {
    "text": "you can Auto scale your workloads with K there's also the vertical pod autoscaler which will go in and say well this pod",
    "start": "1053400",
    "end": "1061559"
  },
  {
    "text": "is has this you know huge amount of requests it's not actually using them",
    "start": "1061559",
    "end": "1066960"
  },
  {
    "text": "it'll go in it'll rescale that pod and then that pod will restart using less requests later on if it starts to grow",
    "start": "1066960",
    "end": "1073919"
  },
  {
    "text": "Beyond a certain threshold vpa will say well we need to give it a little more Headroom it'll rescale it again",
    "start": "1073919",
    "end": "1079440"
  },
  {
    "text": "tricky thing with vpa is the way it's written now and I'll",
    "start": "1079440",
    "end": "1084559"
  },
  {
    "text": "talk a little bit more about how that might change in the future in a minute the way it's written now you incur disruption when you use it because every",
    "start": "1084559",
    "end": "1092320"
  },
  {
    "text": "time you use it every time it scales a pod it does so by that pod",
    "start": "1092320",
    "end": "1098000"
  },
  {
    "text": "restarting remember that thing about infinite money I don't remember where I heard this first but somebody had this",
    "start": "1099120",
    "end": "1105240"
  },
  {
    "text": "great quote that I saw in a presentation they put it on a slide it was you know remember Auto scaling is connected",
    "start": "1105240",
    "end": "1110320"
  },
  {
    "text": "directly to your credit card okay so there are a few things I",
    "start": "1110320",
    "end": "1119000"
  },
  {
    "text": "said it gets complicated then it gets weird right it's going to get more complicated and weirder in the future",
    "start": "1119000",
    "end": "1124440"
  },
  {
    "text": "but in some ways it's doing so in a good way so let's talk a little bit about",
    "start": "1124440",
    "end": "1129600"
  },
  {
    "text": "what's coming anybody here played with inplace",
    "start": "1129600",
    "end": "1134640"
  },
  {
    "text": "pod resizing yet couple of people so what I just said about the vertical pod",
    "start": "1134640",
    "end": "1139679"
  },
  {
    "text": "autoscaler that it causes your pod to restart you're incurring disruption you know you can protect yourself against incurring too much at once with pod",
    "start": "1139679",
    "end": "1145880"
  },
  {
    "text": "disruption budgets in place pod resizing is an alpha feature in kubernetes it landed in I think 127 so it's been",
    "start": "1145880",
    "end": "1153200"
  },
  {
    "text": "around for a couple of generations now and it will actually allow you to resize a pod in place without a",
    "start": "1153200",
    "end": "1160799"
  },
  {
    "text": "restart so that you can squeeze it down you can kind of squeeze the air out of it and make it so that other things can",
    "start": "1160799",
    "end": "1167799"
  },
  {
    "text": "run on that node and I've saw I saw an issue in the vertical pod autoscaler GitHub project",
    "start": "1167799",
    "end": "1173760"
  },
  {
    "text": "about it as soon as it goes GA if it goes GA they really really really want",
    "start": "1173760",
    "end": "1179360"
  },
  {
    "text": "to use it now don't expect that to land as GA or even beta it's still not even",
    "start": "1179360",
    "end": "1184880"
  },
  {
    "text": "beta yet don't expect it to land as beta in like 132 or anything it's still got a",
    "start": "1184880",
    "end": "1190520"
  },
  {
    "text": "ways to go it's still not fully baked but it's there you can turn on the feature gate you can play with it you",
    "start": "1190520",
    "end": "1196280"
  },
  {
    "text": "can see you know how your workloads toall it there are some workloads that won't tolerate that kind of thing uh",
    "start": "1196280",
    "end": "1203120"
  },
  {
    "text": "particularly things like resizing the memory on a database that can be very",
    "start": "1203120",
    "end": "1208799"
  },
  {
    "text": "dangerous resizing CPU less dangerous because again CPU just gets",
    "start": "1208799",
    "end": "1214320"
  },
  {
    "text": "throttled you just have to make sure that you're providing things you're providing adequate CPU for things to",
    "start": "1214320",
    "end": "1220799"
  },
  {
    "text": "happen fast enough they need to happen Dynamic resource allocation it's not",
    "start": "1220799",
    "end": "1227360"
  },
  {
    "text": "strictly part of what I'm talking about today but it does kind of exist in that",
    "start": "1227360",
    "end": "1232760"
  },
  {
    "text": "same universe if you look around the last couple of releases of kubernetes there's been a lot of implementation I",
    "start": "1232760",
    "end": "1239799"
  },
  {
    "text": "think the the big implementation that I saw was in 127 and there's been some significant refinements since then it's",
    "start": "1239799",
    "end": "1246480"
  },
  {
    "text": "kind of an abstraction of storage to other resources so other resources like gpus so it doesn't really come into play",
    "start": "1246480",
    "end": "1254039"
  },
  {
    "text": "necessarily with what I'm talking about with priority and eviction and that kind of thing but it does come into play when",
    "start": "1254039",
    "end": "1259520"
  },
  {
    "text": "you start talking about what is going to schedule where and what might prevent things from scheduling different",
    "start": "1259520",
    "end": "1265600"
  },
  {
    "text": "places now this one this last one this is really cool I'm really excited about",
    "start": "1265600",
    "end": "1271480"
  },
  {
    "text": "this I I've kind of been sniffing around this one for a while um there is a thing called cryou checkpoint and restore and",
    "start": "1271480",
    "end": "1278960"
  },
  {
    "text": "user space which allows you to take a snapshot of a container and then take it somewhere else and at least in theory",
    "start": "1278960",
    "end": "1286320"
  },
  {
    "text": "restore from that snapshot now to do that in kubernetes is very complicated because",
    "start": "1286320",
    "end": "1292360"
  },
  {
    "text": "there's a lot more than just the container itself going on there's networking outside the container there's possibly open connections there's all",
    "start": "1292360",
    "end": "1298840"
  },
  {
    "text": "kinds of other things attached to that running container during Cloud native rejects uh",
    "start": "1298840",
    "end": "1306840"
  },
  {
    "text": "Sunday I think it was Sunday I saw somebody do a demo of live pod migration",
    "start": "1306840",
    "end": "1313080"
  },
  {
    "text": "with active running connections from one cluster to another just took the whole",
    "start": "1313080",
    "end": "1318799"
  },
  {
    "text": "pod over there on a whole different Cloud you know and everything kept running what he actually did when he did",
    "start": "1318799",
    "end": "1325279"
  },
  {
    "text": "his demo he set up a transaction in postgres then he migrated the Pod then",
    "start": "1325279",
    "end": "1331400"
  },
  {
    "text": "he completed his transaction and it completed successfully now again this is something",
    "start": "1331400",
    "end": "1337159"
  },
  {
    "text": "that is not necessarily 100% ready to rock and roll yet but it's getting there",
    "start": "1337159",
    "end": "1342720"
  },
  {
    "text": "it's coming so I'm really excited about that",
    "start": "1342720",
    "end": "1349799"
  },
  {
    "text": "oh let's see model railroading let's do a real quick demo",
    "start": "1350520",
    "end": "1356519"
  },
  {
    "text": "here this by the way is being run by demo magic for anybody who needs a a nice little demo script thing it's great",
    "start": "1357919",
    "end": "1366279"
  },
  {
    "text": "because it's not replaying a recording it's actually doing it live so everything I'm doing right now is being",
    "start": "1366279",
    "end": "1371960"
  },
  {
    "text": "done on my live cluster okay uh oh actually",
    "start": "1371960",
    "end": "1379279"
  },
  {
    "text": "let me do one thing before",
    "start": "1379279",
    "end": "1382400"
  },
  {
    "text": "that this is how you know it's not",
    "start": "1392159",
    "end": "1396039"
  },
  {
    "text": "faked okay now let's repeat okay so empty empty cluster other than the",
    "start": "1404559",
    "end": "1412600"
  },
  {
    "text": "default stuff that's running this is a one node cluster in digital",
    "start": "1412600",
    "end": "1417159"
  },
  {
    "text": "ocean and it's a very small node you can see down here at the bottom I'm using",
    "start": "1418600",
    "end": "1424559"
  },
  {
    "text": "702 M Millies whatever you want to call it of CPU and that's about a third so",
    "start": "1424559",
    "end": "1430720"
  },
  {
    "text": "I've got something less than two full CPUs available to allocate here",
    "start": "1430720",
    "end": "1437480"
  },
  {
    "text": "so this is a regular old just a nothing it's just running the pause container it's requesting one CPU to do nothing",
    "start": "1438559",
    "end": "1448679"
  },
  {
    "text": "with and this is a high priority in fact you can see right up here system cluster",
    "start": "1450640",
    "end": "1458279"
  },
  {
    "text": "critical I said there were two there were two high critical priorities that",
    "start": "1458279",
    "end": "1464159"
  },
  {
    "text": "are built into the cluster system cluster critical and system node critical pop quiz which one of those is higher",
    "start": "1464159",
    "end": "1471360"
  },
  {
    "text": "hands up everybody who thinks system cluster critical is higher priority okay so there's a few there",
    "start": "1471360",
    "end": "1477880"
  },
  {
    "text": "hands up everybody who thinks system node critical is higher priority okay that's actually the correct",
    "start": "1477880",
    "end": "1483440"
  },
  {
    "text": "one they care more about your node being able to run because if the node can't run it will never be able to rejoin the",
    "start": "1483440",
    "end": "1489120"
  },
  {
    "text": "cluster if it comes to that so we're going to make this system cluster critical we're going to have it request",
    "start": "1489120",
    "end": "1495640"
  },
  {
    "text": "one CPU now we don't have enough to do both of these things at once but I have",
    "start": "1495640",
    "end": "1501640"
  },
  {
    "text": "you know since this is high priority naively I might think well it'll just override the other one and",
    "start": "1501640",
    "end": "1507360"
  },
  {
    "text": "then things will happen but as you might guess that's not what's going to",
    "start": "1507360",
    "end": "1513519"
  },
  {
    "text": "happen why is that pod pending well the answer is right",
    "start": "1515279",
    "end": "1520840"
  },
  {
    "text": "here it's this pod affinity and in fact if we describe",
    "start": "1520840",
    "end": "1528799"
  },
  {
    "text": "the pods you will see down there but first it said the one node that's available has",
    "start": "1528799",
    "end": "1535399"
  },
  {
    "text": "insufficient CPU then it tried to preempt and it said one node didn't",
    "start": "1535399",
    "end": "1541399"
  },
  {
    "text": "match pod Affinity rules because again if it evicts that other pod now the Affinity rule that matches that pod",
    "start": "1541399",
    "end": "1547880"
  },
  {
    "text": "won't match that Noe anymore can't schedule on that Noe so how can we fix that there's a",
    "start": "1547880",
    "end": "1556399"
  },
  {
    "text": "couple of ways we could remove the Affinity rule we could change the priority of the base",
    "start": "1556399",
    "end": "1565039"
  },
  {
    "text": "workload to be higher than the priority of this so that we're not depending on a lower priority workload anymore or we",
    "start": "1565039",
    "end": "1571880"
  },
  {
    "text": "could do what I actually do for this demo and just set it CPU request very",
    "start": "1571880",
    "end": "1578960"
  },
  {
    "text": "small which since it's doing nothing is fine so we apply our fixed deployment",
    "start": "1578960",
    "end": "1588080"
  },
  {
    "text": "we look at our pods again and our now our pod is running because it finally fits into the available CPU on that",
    "start": "1588520",
    "end": "1597320"
  },
  {
    "text": "note now just a couple more things and then I'll take questions because we have we're getting down to about the right",
    "start": "1601520",
    "end": "1606840"
  },
  {
    "text": "time for that uh just to wrap up final thoughts like I said don't panic I think",
    "start": "1606840",
    "end": "1613960"
  },
  {
    "text": "I said that before if I didn't don't panic you'll figure it out",
    "start": "1613960",
    "end": "1619279"
  },
  {
    "text": "if it looks weird it probably is weird it's not just that you can't understand",
    "start": "1619279",
    "end": "1624360"
  },
  {
    "text": "it it's legitimately a lot of this stuff is very odd and does not work the way",
    "start": "1624360",
    "end": "1629799"
  },
  {
    "text": "that a normal human brain and I say that as a neurod Divergent person myself would expect it to",
    "start": "1629799",
    "end": "1636919"
  },
  {
    "text": "work so just begin with the assumption that yes something's gone wrong but I'm going to figure it out it's going to be",
    "start": "1636919",
    "end": "1643159"
  },
  {
    "text": "okay you may not be able to fix it because the fix may be we need more",
    "start": "1643159",
    "end": "1648240"
  },
  {
    "text": "capacity in the cluster and your boss says well guess what we ain't got the money this month but that's fine you",
    "start": "1648240",
    "end": "1654360"
  },
  {
    "text": "identified the problem at least now I don't spend any time on this but when you go to the slides online",
    "start": "1654360",
    "end": "1660600"
  },
  {
    "text": "there's some links to other resources scheduling preemption and eviction was heavily used for this cluster Auto",
    "start": "1660600",
    "end": "1667399"
  },
  {
    "text": "scaling goes over both the cluster Auto scaler and Carpenter as well as some of the other Auto scalers that I mentioned",
    "start": "1667399",
    "end": "1672480"
  },
  {
    "text": "there's docs link there for cluster autoscaler and Carpenter uh with that thank you very",
    "start": "1672480",
    "end": "1678360"
  },
  {
    "text": "much the the first QR code there goes to my slides at the same place as that tiny URL down there there is the feedback QR",
    "start": "1678360",
    "end": "1686519"
  },
  {
    "text": "code please be nice you know they were nice last time I",
    "start": "1686519",
    "end": "1692840"
  },
  {
    "text": "got really good comments actually so I I was happy about that hopefully this time will be just as good and uh it looks",
    "start": "1692840",
    "end": "1699440"
  },
  {
    "text": "like we have about six and a half minutes if anybody has any questions",
    "start": "1699440",
    "end": "1704559"
  },
  {
    "text": "okay over here on my",
    "start": "1712760",
    "end": "1716200"
  },
  {
    "text": "left is that does he need to turn on or does somebody else turn on there it goes um so I'm curious how you there there's",
    "start": "1718600",
    "end": "1728120"
  },
  {
    "text": "kind of this Vim versus emac debate on the internet about whether you should",
    "start": "1728120",
    "end": "1734880"
  },
  {
    "text": "use uh CPU and memory limits or limits on memory probably yes but mainly on",
    "start": "1734880",
    "end": "1741840"
  },
  {
    "text": "CPU where do you fall on that particular debate there are times and this is",
    "start": "1741840",
    "end": "1748760"
  },
  {
    "text": "because of the things around things that require you to be guaranteed you will need to set a CPU limit usually when you",
    "start": "1748760",
    "end": "1756399"
  },
  {
    "text": "need to do that you will need to do it so that you can make that pod guaranteed so that it can access things only",
    "start": "1756399",
    "end": "1762360"
  },
  {
    "text": "guaranteed pods can access like those Reserve CPUs from the reserve pool uh",
    "start": "1762360",
    "end": "1768120"
  },
  {
    "text": "there are other things that go come into play there too for example if you make a pod guaranteed and then you give it a fractional CPU count you don't get",
    "start": "1768120",
    "end": "1773840"
  },
  {
    "text": "reserved CPUs so but it's kind of a prerequisite and so in that case I think",
    "start": "1773840",
    "end": "1779559"
  },
  {
    "text": "yes you would go ahead and set some limits um I tend to agree with the the",
    "start": "1779559",
    "end": "1785720"
  },
  {
    "text": "blog post the guy wrote I think it was two or three years ago at this point where he said please everybody stop setting limits they don't do anything in",
    "start": "1785720",
    "end": "1791559"
  },
  {
    "text": "fact they just cause problems most of the time that's all they do over here yeah so uh first of all",
    "start": "1791559",
    "end": "1798919"
  },
  {
    "text": "thank you for sharing your knowledge about this uh complex topic and my question was actually very similar to",
    "start": "1798919",
    "end": "1804519"
  },
  {
    "text": "this question because I read online about this controversy about this uh",
    "start": "1804519",
    "end": "1809919"
  },
  {
    "text": "requests and limit setting if you don't really need them it's better to avoid at least some people says that so what I",
    "start": "1809919",
    "end": "1817960"
  },
  {
    "text": "experienced is uh and that's that that was that's been very complicated for me",
    "start": "1817960",
    "end": "1823200"
  },
  {
    "text": "to overcome that issue was CPU throttling and I never understood when",
    "start": "1823200",
    "end": "1829480"
  },
  {
    "text": "CPU throttling can be like um um like ignored because it's a complex topic and",
    "start": "1829480",
    "end": "1836799"
  },
  {
    "text": "I noticed that no matter what when you set requests or limit there will be some kind of CPU throttling but I never",
    "start": "1836799",
    "end": "1843000"
  },
  {
    "text": "understood when that can be a problem the the most times that I've seen that be a problem is either when something is",
    "start": "1843000",
    "end": "1849880"
  },
  {
    "text": "starting up that something else is waiting on so it needs to start up within a certain time or the other thing",
    "start": "1849880",
    "end": "1855880"
  },
  {
    "text": "is going to give up and time out or vice versa it may be that the thing is starting up and is going to need to get",
    "start": "1855880",
    "end": "1861880"
  },
  {
    "text": "to the point in it start up where it connects to something else and if it takes too long to do that uh maybe it",
    "start": "1861880",
    "end": "1867760"
  },
  {
    "text": "fails a liveness probe or something and gets terminated so you know and that is something that you definitely want to",
    "start": "1867760",
    "end": "1873639"
  },
  {
    "text": "watch uh there are various dashboards and monitoring tools that will tell you how often you're getting throttled I",
    "start": "1873639",
    "end": "1879320"
  },
  {
    "text": "would say you probably want to avoid throttling if you can at all um a little",
    "start": "1879320",
    "end": "1884840"
  },
  {
    "text": "bit of throttling now and then is not the end of the world though thank you uh over over there hi I would",
    "start": "1884840",
    "end": "1892159"
  },
  {
    "text": "like to ask you about if you have some good resources about how requests and limits affects Java applications because",
    "start": "1892159",
    "end": "1900200"
  },
  {
    "text": "Java application sets oh boy threats and everything okay I actually debated",
    "start": "1900200",
    "end": "1907159"
  },
  {
    "text": "putting this in the talk and I said I'll wait and see if anybody asks thank you for asking so at a previous employer I",
    "start": "1907159",
    "end": "1915159"
  },
  {
    "text": "actually wrote a whole blog post about this um Java is a special case of all this because the",
    "start": "1915159",
    "end": "1921600"
  },
  {
    "text": "jbm does its own memory management kind of Hidden Away from the operating system",
    "start": "1921600",
    "end": "1927720"
  },
  {
    "text": "the container runtime whatever is monitoring things and so Java the Java runtime will allocate memory to itself",
    "start": "1927720",
    "end": "1933880"
  },
  {
    "text": "and then it will partial it out to the running uh I forget what the name is for the the actual thing inside the Java",
    "start": "1933880",
    "end": "1940760"
  },
  {
    "text": "jvm but it will partial that memory out and it will garbage collect that memory but it won't return it back to the",
    "start": "1940760",
    "end": "1945960"
  },
  {
    "text": "operating system so from the operating system point of view everything that is allocated to the jbm is memory in use",
    "start": "1945960",
    "end": "1952240"
  },
  {
    "text": "and can't be reclaimed and it's a huge pain in the butt um they have started",
    "start": "1952240",
    "end": "1958880"
  },
  {
    "text": "talking about the idea of doing things that would make it easier to reclaim in some circumstances like the idea of",
    "start": "1958880",
    "end": "1965399"
  },
  {
    "text": "zeroing out garbage collected memory would occasionally make that easier to deal with or at least you could say well",
    "start": "1965399",
    "end": "1972039"
  },
  {
    "text": "okay we know that we can overcommit that memory page because it's zeroed out or",
    "start": "1972039",
    "end": "1977080"
  },
  {
    "text": "we know that if we're taking an image of this container we don't have to take an image of that page because it's zeroed out so yeah the unfortunate reality is",
    "start": "1977080",
    "end": "1984880"
  },
  {
    "text": "that Java is just a pain to work with and would you recommend setting request",
    "start": "1984880",
    "end": "1990000"
  },
  {
    "text": "and limit for Java memory done Java is also one of those cases where not giving",
    "start": "1990000",
    "end": "1995039"
  },
  {
    "text": "it enough CPU will cause it to fail to start up because it will just like spring boot is",
    "start": "1995039",
    "end": "2000760"
  },
  {
    "text": "notorious for this you have to give your spring boot applications all the CPU in the world I think we have time for one more",
    "start": "2000760",
    "end": "2008240"
  },
  {
    "text": "over here oh uh sorry um so when I work with",
    "start": "2008240",
    "end": "2013840"
  },
  {
    "text": "my the application developers and I tell them you know hey go set requests limits",
    "start": "2013840",
    "end": "2020159"
  },
  {
    "text": "request numbers and limit numbers and then they come back to me and say well I",
    "start": "2020159",
    "end": "2025279"
  },
  {
    "text": "don't know what to set it to I don't know how um how much my application is",
    "start": "2025279",
    "end": "2030559"
  },
  {
    "text": "going to need right yeah and so they end up just copying from a previous",
    "start": "2030559",
    "end": "2036360"
  },
  {
    "text": "application usually right and and then sometimes it works sometimes it doesn't and whatnot so my question is are there",
    "start": "2036360",
    "end": "2044000"
  },
  {
    "text": "tools or mechanisms that you would recommend to like hey run your",
    "start": "2044000",
    "end": "2050398"
  },
  {
    "text": "application let it let it run set the the opsource version of that kind of",
    "start": "2050399",
    "end": "2057118"
  },
  {
    "text": "tool would be the vertical pod Auto scaler that will look at your actual usage versus your requests um there are",
    "start": "2057119",
    "end": "2064118"
  },
  {
    "text": "a ton of other tools out there that would love for you to pay them money so that they can apply machine learning",
    "start": "2064119",
    "end": "2070079"
  },
  {
    "text": "to your cluster to do that kind of thing for you some of them even in a predictive fashion okay",
    "start": "2070079",
    "end": "2076398"
  },
  {
    "text": "so I think we have one more do we have do we have time to squeeze this one last one in go ahead if they cut me off I'll I'll",
    "start": "2076399",
    "end": "2084118"
  },
  {
    "text": "get you later yeah a very quick one uh do you have any insights in terms cgroups V2 because it's kind of already",
    "start": "2084119",
    "end": "2090480"
  },
  {
    "text": "there but not for all of us depending on what bases we're using so like any insights from you how it will improve uh",
    "start": "2090480",
    "end": "2097920"
  },
  {
    "text": "resource allocation and control of resources wait improve what now uh like",
    "start": "2097920",
    "end": "2103359"
  },
  {
    "text": "resource allocation qu like all the killed kind of situations and oh uh yeah",
    "start": "2103359",
    "end": "2108640"
  },
  {
    "text": "for that kind of thing I think you really want to dig deep into Auto scaling because a lot of the autoscalers",
    "start": "2108640",
    "end": "2114200"
  },
  {
    "text": "like for example Carpenter will actually try to consolidate your nodes so that you're running more nodes yeah I'm",
    "start": "2114200",
    "end": "2121560"
  },
  {
    "text": "getting the the red sign but yeah Carpenter will try to improve that for you quite a bit",
    "start": "2121560",
    "end": "2128599"
  },
  {
    "text": "thank you",
    "start": "2128599",
    "end": "2131800"
  }
]