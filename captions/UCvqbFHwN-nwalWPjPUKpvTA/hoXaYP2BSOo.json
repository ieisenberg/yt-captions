[
  {
    "text": "okay we get get started thanks everyone uh we don't have a very large Cloud uh yeah thanks for coming to the",
    "start": "199",
    "end": "6600"
  },
  {
    "text": "talk and uh so I just quick quickly introduce ourselves so I'm wo is my",
    "start": "6600",
    "end": "12400"
  },
  {
    "text": "colleague venon and we're software Engineers working at Google Cloud uh we work on the open source project GPC but",
    "start": "12400",
    "end": "19359"
  },
  {
    "text": "we're also supporting the the Google's internal infrastructure that powers the gcp services uh sometimes we call this C",
    "start": "19359",
    "end": "27960"
  },
  {
    "text": "apis and also the non-cloud uh Google you know set of",
    "start": "27960",
    "end": "33280"
  },
  {
    "text": "external facing services like gmail search So today we're going to talk about a specific you know a case study",
    "start": "33280",
    "end": "40239"
  },
  {
    "text": "we like to share with you some experience about how to harden in the data path uh over the hybrid",
    "start": "40239",
    "end": "48239"
  },
  {
    "text": "Cloud sorry yeah so okay sorry about about",
    "start": "52960",
    "end": "60120"
  },
  {
    "text": "this uh so yeah first we're going to talk about you know what I really mean by hybrid Cloud I do I do not have exact",
    "start": "60120",
    "end": "67000"
  },
  {
    "text": "definition of hybrid Cloud specifically the uh the use case we're talking about here is when some large scale online",
    "start": "67000",
    "end": "75640"
  },
  {
    "text": "applications they try to move their database layers storage layers into the",
    "start": "75640",
    "end": "80920"
  },
  {
    "text": "public cloud and uh when the majority of the application will still stay on the",
    "start": "80920",
    "end": "86600"
  },
  {
    "text": "in the on premise data center now this is a shifting Paradigm if you look at",
    "start": "86600",
    "end": "91640"
  },
  {
    "text": "the picture on the left side it's a traditional on premise you know deployment when you have applications",
    "start": "91640",
    "end": "98000"
  },
  {
    "text": "talking to database for example mycure and the N Network link is usually reliable and more importantly they they",
    "start": "98000",
    "end": "104920"
  },
  {
    "text": "share the same Faith right when something goes wrong typically the database itself goes wrong first all",
    "start": "104920",
    "end": "110799"
  },
  {
    "text": "together with the network but once you move everything move the storage layer the database layer to the public Cloud",
    "start": "110799",
    "end": "117320"
  },
  {
    "text": "now the entire datab base is different now part of the reasons you they're moving the database to public cloud is",
    "start": "117320",
    "end": "122960"
  },
  {
    "text": "that they want Leverage The scalability reliability of the So-Cal s service but then the data path suddenly become",
    "start": "122960",
    "end": "130080"
  },
  {
    "text": "different it become longer the fair modes is also different which we'll talk about now that doesn't mean the hybrid",
    "start": "130080",
    "end": "137720"
  },
  {
    "text": "cloud is harder it's impossible to you know to wrun or it's inherently unreliable it I think the the key",
    "start": "137720",
    "end": "144360"
  },
  {
    "text": "problems that we like to address is here that when the application layer they're not adap Ed to this new sort of data",
    "start": "144360",
    "end": "151760"
  },
  {
    "text": "path the longer RT the different fail modes and that's the you know that's basically the use case we're looking at",
    "start": "151760",
    "end": "160000"
  },
  {
    "text": "today so we look at the the whole problem then we start realize that the end lineing problem the technical",
    "start": "160360",
    "end": "167480"
  },
  {
    "text": "problem is pretty generic right it's not really anything application specific it's sort of the touches of the core",
    "start": "167480",
    "end": "174159"
  },
  {
    "text": "fundamentals of any distribute Computing that kind of things so instead of talking about the specific applic",
    "start": "174159",
    "end": "180280"
  },
  {
    "text": "we've been working with uh we sort of let's make this more useful try to generalize it at same time we also think",
    "start": "180280",
    "end": "187159"
  },
  {
    "text": "you know we want to Define some more concrete like example concrete workload not too generalized so here's what we",
    "start": "187159",
    "end": "193799"
  },
  {
    "text": "come up with right so so there are few properties for this kind of you know the the use case first in this case in the",
    "start": "193799",
    "end": "203000"
  },
  {
    "text": "hybrid Cloud case the client from the UN premise side to the public Cloud that",
    "start": "203000",
    "end": "209920"
  },
  {
    "text": "the long trip time is larger than the service itself and if you look at the diagram so you're looking at about",
    "start": "209920",
    "end": "216560"
  },
  {
    "text": "either single digit millisecond or tens of milliseconds you know network round",
    "start": "216560",
    "end": "222120"
  },
  {
    "text": "of time when the actual service in this case you know it's act Google Cloud spanner you're looking at like single",
    "start": "222120",
    "end": "228720"
  },
  {
    "text": "digital millisecond and also between the client and the actual service there is also a",
    "start": "228720",
    "end": "235360"
  },
  {
    "text": "layer 7even low balancer so so so this is the sort of",
    "start": "235360",
    "end": "240480"
  },
  {
    "text": "the RT sort of the Paradigm then the second one is that for this particular case study we're looking at you know",
    "start": "240480",
    "end": "246079"
  },
  {
    "text": "there are two types of sort of RPS one is transactional here we just simply refer",
    "start": "246079",
    "end": "251680"
  },
  {
    "text": "to this as rights it doesn't mean they're all just rights they can also be uh and then there's a non-transaction",
    "start": "251680",
    "end": "258040"
  },
  {
    "text": "apps which we just simply refer to as re and then the when the RO time out and",
    "start": "258040",
    "end": "265960"
  },
  {
    "text": "uh the underlying software can reach right but eventually there is a data line when the data exceeded some kind of",
    "start": "265960",
    "end": "272280"
  },
  {
    "text": "Errors will actually go back to the the user facing layers then the end user may actually see it in some cases that",
    "start": "272280",
    "end": "279000"
  },
  {
    "text": "actually can cause business impacts that for example you know the user may say the application is not",
    "start": "279000",
    "end": "285759"
  },
  {
    "text": "responsive and then same thing for the rights if the transaction if the rights failed the whole transaction may get",
    "start": "285759",
    "end": "291479"
  },
  {
    "text": "aborted right so imagine you're doing online ordering and then the order may not you know may be canceled and then",
    "start": "291479",
    "end": "297280"
  },
  {
    "text": "you have to redo it and the US may actually just to walk away uh then the last thing is that",
    "start": "297280",
    "end": "302960"
  },
  {
    "text": "there's a connectivity here Affinity here based on connections so all the traffic that's generated all the request",
    "start": "302960",
    "end": "310560"
  },
  {
    "text": "generated by the client over the S uh the single TCP connections they will be",
    "start": "310560",
    "end": "316039"
  },
  {
    "text": "sent to the same server process because that's where the some level of session state is managed or caching especially",
    "start": "316039",
    "end": "322880"
  },
  {
    "text": "like a metadata cing uh so so yeah so there's Affinity you can imagine that all the the connection will and all the",
    "start": "322880",
    "end": "330120"
  },
  {
    "text": "you know you have all these different sessions and then they will all be mapped to different connections and then all the requests within a single session",
    "start": "330120",
    "end": "336639"
  },
  {
    "text": "will all be roted to the same server process now we do think this represent a lot of workload similarly uh this is",
    "start": "336639",
    "end": "344919"
  },
  {
    "text": "pretty a generalized version of this so now I'm going to talk about like",
    "start": "344919",
    "end": "351120"
  },
  {
    "text": "the exact sort of the things that specific to the Leger 3 Network in the in case of the hybrid Cloud deployment",
    "start": "351120",
    "end": "358199"
  },
  {
    "text": "now so first thing is that in the hybrid Cloud there's this concept like a flow",
    "start": "358199",
    "end": "364319"
  },
  {
    "text": "so flow is you know this is a very defined term right you can imagine it's just some kind of Ip routes and what",
    "start": "364319",
    "end": "371680"
  },
  {
    "text": "that means is that each flow is fixed it go through a fixed set of software",
    "start": "371680",
    "end": "376880"
  },
  {
    "text": "Hardware components or links like physical links like fiber links and then TCP connections based on soft IP and",
    "start": "376880",
    "end": "384599"
  },
  {
    "text": "Port especially with IPv6 they get mapped to a single flow now the flow is",
    "start": "384599",
    "end": "389840"
  },
  {
    "text": "also unidirectional which mean that sometimes when there's you know outage happens and only one direct you know",
    "start": "389840",
    "end": "396520"
  },
  {
    "text": "only have small fractions of flows get affected typically they only affect sort of oneway traffic not necessarily uh the",
    "start": "396520",
    "end": "405880"
  },
  {
    "text": "bidirectional and at same time we notice that most of the faos seems to be like a",
    "start": "405880",
    "end": "411840"
  },
  {
    "text": "black hole you know the whole flows basically gets broke down as opposed to on the internet what you're seeing is",
    "start": "411840",
    "end": "418319"
  },
  {
    "text": "those partial pack so overall this network the Lego 3 network is you know they're they're much",
    "start": "418319",
    "end": "425560"
  },
  {
    "text": "reliable secure than internet traffic but periodically they do you know",
    "start": "425560",
    "end": "430680"
  },
  {
    "text": "because the all the different components involved you do see this kind of fa uh",
    "start": "430680",
    "end": "435840"
  },
  {
    "text": "patterns now on top of that on the server side the proxy the actual service",
    "start": "435840",
    "end": "442280"
  },
  {
    "text": "process they also get overloaded and that's where you know all the So-Cal tail latency comes from and typically",
    "start": "442280",
    "end": "449400"
  },
  {
    "text": "they are not correlated to the L3 in most of cases now one thing I didn't mention on",
    "start": "449400",
    "end": "455199"
  },
  {
    "text": "the slide is that yeah we this is mostly a latency sensitive workload not so much about you know throughput as such you",
    "start": "455199",
    "end": "462039"
  },
  {
    "text": "know we we don't NE talk about the congestion control flow control here so so so when we have all this in",
    "start": "462039",
    "end": "470520"
  },
  {
    "text": "our in that particular use case right what the department we facing is then the applications the you know the",
    "start": "470520",
    "end": "477240"
  },
  {
    "text": "customer basically our public clowd the first question is from the networking team is always like this you",
    "start": "477240",
    "end": "483560"
  },
  {
    "text": "know when there's a network outage I'm seeing the pulos is 1% why the RPC error",
    "start": "483560",
    "end": "490599"
  },
  {
    "text": "rate is so much higher you know different right so that's sort of the first round question then the second one",
    "start": "490599",
    "end": "495759"
  },
  {
    "text": "is for the application developers they're Laing the application try to access the source databases that",
    "start": "495759",
    "end": "502159"
  },
  {
    "text": "question typically is like why do I need to know all this can you just recover everything for me so that you know I the",
    "start": "502159",
    "end": "508599"
  },
  {
    "text": "applications that uses my middleware you know middle layer they do not have to see any errors because that's business",
    "start": "508599",
    "end": "514839"
  },
  {
    "text": "impact it's it's bad so when we heard all those questions you know my immediate answer is for the",
    "start": "514839",
    "end": "521159"
  },
  {
    "text": "first question is we don't know right they don't supposed to be related the pular cap pulos how it maps to",
    "start": "521159",
    "end": "528680"
  },
  {
    "text": "RPC obviously they should be different but then the next question is yeah you know to what extent can you like",
    "start": "528680",
    "end": "535560"
  },
  {
    "text": "quantify that then for the second question is we kind of think yeah obviously you know",
    "start": "535560",
    "end": "541399"
  },
  {
    "text": "we can do something you know if we know there's errors we can try to recover but then you think deeply you start realize",
    "start": "541399",
    "end": "547240"
  },
  {
    "text": "that it's pretty challenging problem if you try to react to faes too",
    "start": "547240",
    "end": "553600"
  },
  {
    "text": "aggressively then you may introduce cost in the long time when there's no failures like you have those fce alarms",
    "start": "553600",
    "end": "560320"
  },
  {
    "text": "and also in the worst case you very easily can potentially cause like cascading failures but if you do not",
    "start": "560320",
    "end": "566200"
  },
  {
    "text": "react and by the time you know all the failes are gone because they short lived it's already too late right the RPC",
    "start": "566200",
    "end": "572240"
  },
  {
    "text": "arrows are already being through to the users and they already have impacts so",
    "start": "572240",
    "end": "577519"
  },
  {
    "text": "so when I was looking at that the immediate picture I have you know for this talk is this just like a cage fight",
    "start": "577519",
    "end": "583800"
  },
  {
    "text": "the marging of arrows is so small you have all kind of constraints on Trad offs you have to deal with now I'm going",
    "start": "583800",
    "end": "590320"
  },
  {
    "text": "off topic a little bit so so I thought this KU car I should have put a nice picture here so I was thinking about",
    "start": "590320",
    "end": "595680"
  },
  {
    "text": "this kage fight everyone was talking about six months ago that never happened so I did a Google search there's all",
    "start": "595680",
    "end": "601839"
  },
  {
    "text": "this nice picture but then before I posted it I thought you know I just let you I don't really know the copyright",
    "start": "601839",
    "end": "607360"
  },
  {
    "text": "all that I just let you guys to picture it or maybe you want to do a Google image search um yeah sorry about this",
    "start": "607360",
    "end": "614560"
  },
  {
    "text": "off topic who doesn't know the kid fight I'm talking about okay so anyway so so this is so",
    "start": "614560",
    "end": "620760"
  },
  {
    "text": "really just sort of mentally I want you to picture that right the problem is like what is what it's like uh so then",
    "start": "620760",
    "end": "629800"
  },
  {
    "text": "what we did is that we look into the problem which think we think that the most useful thing is actually try to",
    "start": "629800",
    "end": "637240"
  },
  {
    "text": "create some kind of uh Benchmark framework to quantify and provide answers or some kind of insights to both",
    "start": "637240",
    "end": "643760"
  },
  {
    "text": "the problems so I'm going to talk about sort of the setup for this particular benchmarking uh and then my colleague",
    "start": "643760",
    "end": "650120"
  },
  {
    "text": "will sort of show you the results we're getting from there so in this particular Benchmark the Cent is running on the you",
    "start": "650120",
    "end": "657560"
  },
  {
    "text": "know GCE Google computer engine VMS and uh the fies are injected we didn't use any physical you",
    "start": "657560",
    "end": "665720"
  },
  {
    "text": "know the physical Hy car link uh because for the most part the software components are are the same between VM",
    "start": "665720",
    "end": "673000"
  },
  {
    "text": "uh between on premise VM and the gcvm all the way to the you know the the",
    "start": "673000",
    "end": "678120"
  },
  {
    "text": "so-called layer 7 proxy which is a boundary between the U facing part and the",
    "start": "678120",
    "end": "683680"
  },
  {
    "text": "infrastructure and uh for the read we just simply using gets in say that is",
    "start": "683680",
    "end": "690000"
  },
  {
    "text": "the response is large but the request is very tiny and then for rights we use post which is other way around and we",
    "start": "690000",
    "end": "697200"
  },
  {
    "text": "use 80% of our reads and combine it with 20% now all the requests",
    "start": "697200",
    "end": "702320"
  },
  {
    "text": "are all the requests are generated using so-call person stream now what that means is requests are generated",
    "start": "702320",
    "end": "708760"
  },
  {
    "text": "asynchronously and they're not waiting for response and the interval between every two requests they're they're using",
    "start": "708760",
    "end": "716240"
  },
  {
    "text": "their exponential distribution right so this is this is used for that if you",
    "start": "716240",
    "end": "722279"
  },
  {
    "text": "spread the so-call person stream if you spread the traffic across multiple clients or multiple TCP connections as",
    "start": "722279",
    "end": "729360"
  },
  {
    "text": "long as the total request per second rate is is the same on the server side",
    "start": "729360",
    "end": "735040"
  },
  {
    "text": "the requests will arrive in the same pattern the server wouldn't know the difference now this this is really",
    "start": "735040",
    "end": "741199"
  },
  {
    "text": "useful way to do the benchmarking if your cous right so this allows you to generate enough concurrency on the",
    "start": "741199",
    "end": "748040"
  },
  {
    "text": "network and also on the server without necessar overload to the network of server at the same time so that's sort",
    "start": "748040",
    "end": "754639"
  },
  {
    "text": "of how the workload is generated and and then yeah like I said so the",
    "start": "754639",
    "end": "760680"
  },
  {
    "text": "pulas is interjected on single Connection in the long time repeated and that's why how we get the data so now",
    "start": "760680",
    "end": "767320"
  },
  {
    "text": "I'm going to pass uh pass to V to talk about the results we're getting specific sort of to answer the first question",
    "start": "767320",
    "end": "773480"
  },
  {
    "text": "like how do you quantify between the pack loss rate and RPC you know",
    "start": "773480",
    "end": "780720"
  },
  {
    "text": "timeouts yeah hello so wenbo asked me to collect a lot of data and I would like",
    "start": "780720",
    "end": "787160"
  },
  {
    "text": "to present some of that to you which I thought was interesting and maybe there",
    "start": "787160",
    "end": "792320"
  },
  {
    "text": "are some obvious and not so obvious conclusion so the first slide I just put in there to show what kind of network we",
    "start": "792320",
    "end": "799680"
  },
  {
    "text": "are dealing with this is a a probability distribution of the roundtrip time",
    "start": "799680",
    "end": "805519"
  },
  {
    "text": "measured in the kernel we used ebpf to measure the network latency between two",
    "start": "805519",
    "end": "811360"
  },
  {
    "text": "acts so it's pretty stable at around 1.2 1.4 milliseconds and the tail is also",
    "start": "811360",
    "end": "818240"
  },
  {
    "text": "not that long at 1.9 milliseconds but when you go to see the",
    "start": "818240",
    "end": "824800"
  },
  {
    "text": "RPC latency this is the end to end RPC latency at different percent of injected",
    "start": "824800",
    "end": "832600"
  },
  {
    "text": "loss we can see that the Tails start increasing rapidly and since this is a",
    "start": "832600",
    "end": "838480"
  },
  {
    "text": "very latency sensitive application we can see that at 5% packet loss the P99",
    "start": "838480",
    "end": "845720"
  },
  {
    "text": "latency is all already over five times more than uh the loss at 0% so for our",
    "start": "845720",
    "end": "854639"
  },
  {
    "text": "particular application which is database latency sensitive things go bad at",
    "start": "854639",
    "end": "860360"
  },
  {
    "text": "around uh before even 10% partial packet loss and this graphs also show what WBO",
    "start": "860360",
    "end": "866880"
  },
  {
    "text": "mentioned earlier that the Network tails are not correlated with the",
    "start": "866880",
    "end": "872120"
  },
  {
    "text": "RPC latency Trails so uh I want to explain how we",
    "start": "872120",
    "end": "878920"
  },
  {
    "text": "came up with the percentage RPC timeout so uh we ran a lot of simulations and we",
    "start": "878920",
    "end": "886399"
  },
  {
    "text": "ran them at different qpss and different durations so we had to normalize it",
    "start": "886399",
    "end": "892399"
  },
  {
    "text": "across all these parameters so the percentage RPC timeouts is a ratio of of",
    "start": "892399",
    "end": "899480"
  },
  {
    "text": "all the rpcs that failed over the entire run divided by the rpcs that we start",
    "start": "899480",
    "end": "907240"
  },
  {
    "text": "during the induced failure periods and similar to the previous slide we can",
    "start": "907240",
    "end": "913720"
  },
  {
    "text": "see that the network for our purposes stop getting useful or get very bad",
    "start": "913720",
    "end": "920120"
  },
  {
    "text": "around 10% and there is very little difference between 20% packet loss and a",
    "start": "920120",
    "end": "926279"
  },
  {
    "text": "complete Black Hole uh now I want you to take a mental picture",
    "start": "926279",
    "end": "931639"
  },
  {
    "text": "of this image and I will compare it to the next slide where we change the",
    "start": "931639",
    "end": "937199"
  },
  {
    "text": "transport from using HTTP 2 to http 3 over quick and we see a huge difference",
    "start": "937199",
    "end": "944360"
  },
  {
    "text": "at the different packet loss percentages this is because of the head of line blocking that TCP forces an",
    "start": "944360",
    "end": "952040"
  },
  {
    "text": "order for all the packets to be received which means that one RPC will block the",
    "start": "952040",
    "end": "957959"
  },
  {
    "text": "next RPC and you will see this effect so if",
    "start": "957959",
    "end": "963519"
  },
  {
    "text": "you're using HTTP 3 over quick you can see that even in lossy high RT networks",
    "start": "963519",
    "end": "970279"
  },
  {
    "text": "you can still get a decent performance that's why quick should be used in such connections like over the Internet a",
    "start": "970279",
    "end": "977360"
  },
  {
    "text": "very long RT um RT payloads and to drive this point",
    "start": "977360",
    "end": "985480"
  },
  {
    "text": "even further home uh we are uh this is a graph of where we varied the requests",
    "start": "985480",
    "end": "992199"
  },
  {
    "text": "per second for http2 and you can see specifically in",
    "start": "992199",
    "end": "997319"
  },
  {
    "text": "this 10% loss case that once the uh request per second goes about 20",
    "start": "997319",
    "end": "1004560"
  },
  {
    "text": "requests per seconds that's where you see actual actually see two rpcs contending for each other on the same on",
    "start": "1004560",
    "end": "1011959"
  },
  {
    "text": "the same connection and you see a huge Spike at after 20 requests per second",
    "start": "1011959",
    "end": "1017519"
  },
  {
    "text": "and this is the head of line blocking and if you do that for quick it is it is",
    "start": "1017519",
    "end": "1024918"
  },
  {
    "text": "much much better the um just to make sure that it was a fair comparison the",
    "start": "1024919",
    "end": "1030360"
  },
  {
    "text": "black holing case where 100% packet loss was induced is pretty much",
    "start": "1030360",
    "end": "1035880"
  },
  {
    "text": "same across all the scenarios um so yeah I will like to ask",
    "start": "1035880",
    "end": "1042798"
  },
  {
    "text": "WBO to talk more about the failure detection and Recovery",
    "start": "1042799",
    "end": "1050200"
  },
  {
    "text": "thanks V uh yeah I just want to add a quick note about you know why we look at",
    "start": "1050200",
    "end": "1055840"
  },
  {
    "text": "quick right in this case uh so we still think that TCP hb2 is the",
    "start": "1055840",
    "end": "1065799"
  },
  {
    "text": "transport and the GPC for example which is based on htb2 is the transport for",
    "start": "1065799",
    "end": "1071200"
  },
  {
    "text": "you know data center communication even for hybrid cloud and uh using quick for",
    "start": "1071200",
    "end": "1077440"
  },
  {
    "text": "data center communication you know I think it's probably still in a pretty early phase uh we do have some",
    "start": "1077440",
    "end": "1084000"
  },
  {
    "text": "initial sort of experiments but but broadly speaking quick will introduce overhead a lot of the infrastructure",
    "start": "1084000",
    "end": "1091400"
  },
  {
    "text": "today is really just built to support TCP so so we don't want to sort of has a",
    "start": "1091400",
    "end": "1096799"
  },
  {
    "text": "takeaway say Hey you know I if I have this kind of workload I should use Quake",
    "start": "1096799",
    "end": "1102760"
  },
  {
    "text": "because there will be pulas uh really two things right first most of the pulas",
    "start": "1102760",
    "end": "1108600"
  },
  {
    "text": "we actually experience the like total packet of spread C in that case as you say uh quick doesn't help second is that",
    "start": "1108600",
    "end": "1116799"
  },
  {
    "text": "you do want a connection pool not just because you know head of line blocking those things the reason you want",
    "start": "1116799",
    "end": "1122799"
  },
  {
    "text": "connection pool is really for low distribution or low balancing right it's not necessarily try to be make the",
    "start": "1122799",
    "end": "1128960"
  },
  {
    "text": "traffic you know the connection more tolerant to these kind of failes which only happen once every several months or",
    "start": "1128960",
    "end": "1135080"
  },
  {
    "text": "S week okay so just want to clarify that and and but we thought the results will",
    "start": "1135080",
    "end": "1140880"
  },
  {
    "text": "be very useful because you can immediately say uh the the role of the multi practicing place in the TCB case",
    "start": "1140880",
    "end": "1148520"
  },
  {
    "text": "when there's a pulas a partial pulas and how the RPC will get",
    "start": "1148520",
    "end": "1154720"
  },
  {
    "text": "affected uh so the yeah so the next part is I'm just going to talk about you know how we plan to address the second",
    "start": "1154720",
    "end": "1160720"
  },
  {
    "text": "question right is this what do you can do uh with this kind of faires is",
    "start": "1160720",
    "end": "1166559"
  },
  {
    "text": "possible at the latest seven you can actually recover from it completely okay",
    "start": "1166559",
    "end": "1172080"
  },
  {
    "text": "now so this is uh just for benchmarking we come up with this kind of you know",
    "start": "1172080",
    "end": "1178760"
  },
  {
    "text": "what I would consider pretty intuitive recovery scheme uh although it does",
    "start": "1178760",
    "end": "1184400"
  },
  {
    "text": "involve multiple parameters so I'll take a little bit time to explain so really there are two parameters you can tune",
    "start": "1184400",
    "end": "1190080"
  },
  {
    "text": "for to detect a faes on the Cent side right without knowing all the details on the server side or infrastructure side",
    "start": "1190080",
    "end": "1196840"
  },
  {
    "text": "one is that how many X you're waiting and which gets time out but act we",
    "start": "1196840",
    "end": "1202280"
  },
  {
    "text": "simply mean anything that's a response data that is time bond that that's you know associated with the request it can",
    "start": "1202280",
    "end": "1209039"
  },
  {
    "text": "be headers can be some part of the response bites or can be the end of the response that anything that is time",
    "start": "1209039",
    "end": "1215400"
  },
  {
    "text": "bound uh so generally we just refer to this some form of X now as the kind s",
    "start": "1215400",
    "end": "1221640"
  },
  {
    "text": "multiple traffics asynchronously they are act you know those request will be inflight you're waiting for some form of",
    "start": "1221640",
    "end": "1227960"
  },
  {
    "text": "resp respon coming back and the total number of those pending X is the first parameter in this case we call this uh",
    "start": "1227960",
    "end": "1236039"
  },
  {
    "text": "the B right and then you have the timeout like how long do I wait for each individual act before I you know before",
    "start": "1236039",
    "end": "1243520"
  },
  {
    "text": "I decided this act is timed out so the particular algorithm we come up is take",
    "start": "1243520",
    "end": "1249640"
  },
  {
    "text": "into account that when sometimes the connection may be seeing a lot more traffic and sometimes it's mostly idle",
    "start": "1249640",
    "end": "1255760"
  },
  {
    "text": "but still have enough traffic no if it's totally idle you don't don't care right you just let it WR so both parameters we",
    "start": "1255760",
    "end": "1262880"
  },
  {
    "text": "make it make it so that it's Dynamic it can be one act that's got timeout or it",
    "start": "1262880",
    "end": "1269200"
  },
  {
    "text": "can be two or can be three and then the total timeout is adjusted based on the number of X the more number of X that's",
    "start": "1269200",
    "end": "1276679"
  },
  {
    "text": "infight that's got time out at same time sort of the more confidence you have that you know they probably a real real",
    "start": "1276679",
    "end": "1283320"
  },
  {
    "text": "failes is happening that I need to react uh so in this case you know if you do a little math right we're saying if it's",
    "start": "1283320",
    "end": "1289559"
  },
  {
    "text": "three act or four act I'll wait for four rtts otherwise I'll go all the way to 10",
    "start": "1289559",
    "end": "1295520"
  },
  {
    "text": "rtts right if there's only one act that's pending but also as you can see because",
    "start": "1295520",
    "end": "1301480"
  },
  {
    "text": "all the request gets sent generated asynchronously on the server side it's multi threaded they also get a processed",
    "start": "1301480",
    "end": "1308600"
  },
  {
    "text": "asynchronously the TCP does have the five4 auditing right so which means that there can be order of order you know the",
    "start": "1308600",
    "end": "1315200"
  },
  {
    "text": "request that's appear on the TCP on the connection means they may not match the order onquest and the order on act they",
    "start": "1315200",
    "end": "1322159"
  },
  {
    "text": "they don't match each other so so now we say hey now let's say",
    "start": "1322159",
    "end": "1329799"
  },
  {
    "text": "with this recovery algorithm what is the Matrix we can evaluate the effect of this in the long time so we come up with",
    "start": "1329799",
    "end": "1337120"
  },
  {
    "text": "two Magics uh so first one is called mean time to failure detection I don't",
    "start": "1337120",
    "end": "1342360"
  },
  {
    "text": "if that's a standard term so we sort of made it up so this is thing when the",
    "start": "1342360",
    "end": "1347440"
  },
  {
    "text": "first injected how long does it take for the algorithm to actually detect that there is a real fa faure right and so",
    "start": "1347440",
    "end": "1353960"
  },
  {
    "text": "that's and the shorter the better and then second Matrix is basically more important it's a false",
    "start": "1353960",
    "end": "1360279"
  },
  {
    "text": "positive if I run this algorithm our connection without any fa injected where",
    "start": "1360279",
    "end": "1366240"
  },
  {
    "text": "it forcibly detect their surar and for the second Matrix we choose to normalize",
    "start": "1366240",
    "end": "1371880"
  },
  {
    "text": "it so if it's 1.0 it mean that for the",
    "start": "1371880",
    "end": "1377200"
  },
  {
    "text": "whole duration of The Benchmark wrong which is hours or you know 20 hours can be even longer and the algorithm never",
    "start": "1377200",
    "end": "1384640"
  },
  {
    "text": "detect any faes which is great right which is the one what you wanted and if it's 0.2 mean through the whole span of",
    "start": "1384640",
    "end": "1391880"
  },
  {
    "text": "The Benchmark there's five four or five times that you know there's a false",
    "start": "1391880",
    "end": "1397159"
  },
  {
    "text": "false uh false positive cases okay so so we we didn't run this",
    "start": "1397159",
    "end": "1404080"
  },
  {
    "text": "uh directly in the long time so we use a simulation to do that because you know you can run once you have all the",
    "start": "1404080",
    "end": "1410080"
  },
  {
    "text": "samples you can just run uh you can just us simulation so we did use the basically discri similar to descrip",
    "start": "1410080",
    "end": "1415960"
  },
  {
    "text": "event simulation so we can get the results we can tune the different parameters and can get results in just a",
    "start": "1415960",
    "end": "1421360"
  },
  {
    "text": "few seconds otherwise the total samples we been using feed to the algorithm is",
    "start": "1421360",
    "end": "1426440"
  },
  {
    "text": "you know hours of samples and remember our rtt is very small so we're talking about like potentially millions of",
    "start": "1426440",
    "end": "1433159"
  },
  {
    "text": "samples uh so okay so on the left side we're basically saying here we're going to fix we're going to fix the number of",
    "start": "1433159",
    "end": "1440000"
  },
  {
    "text": "X it has to be three or three more X that's got time out at same time before",
    "start": "1440000",
    "end": "1445200"
  },
  {
    "text": "we declare the connection is failed and then we start to increase uh the total time out because our rtt is so low so we",
    "start": "1445200",
    "end": "1452200"
  },
  {
    "text": "start to you know put effect you know one two three effect of two right so the",
    "start": "1452200",
    "end": "1458720"
  },
  {
    "text": "first on the first bar you can see that it's possible to reach 1.0 if you fix",
    "start": "1458720",
    "end": "1464679"
  },
  {
    "text": "the number of x to the ACT to three and then gradually increase the timeout at",
    "start": "1464679",
    "end": "1471320"
  },
  {
    "text": "that point you you can have the you know the idea situation that there's no Force positive and also the delay to detect",
    "start": "1471320",
    "end": "1479520"
  },
  {
    "text": "the the delay to detect the the you know the packet loss is pretty reasonable it's you're looking at like a few",
    "start": "1479520",
    "end": "1486080"
  },
  {
    "text": "hundred uh millisecond right but if you try to reduce the number of X especially",
    "start": "1486080",
    "end": "1491559"
  },
  {
    "text": "to deal with a case when there's not enough active traffic and then you have some kind of false positive but at the",
    "start": "1491559",
    "end": "1498200"
  },
  {
    "text": "same time this does allow you to De detect the real various more quickly and",
    "start": "1498200",
    "end": "1503240"
  },
  {
    "text": "then on the right side we did the other way around right so instead so we fix the total time out and then we start to",
    "start": "1503240",
    "end": "1510559"
  },
  {
    "text": "decrease the number of X you know number two number you know you can have two X",
    "start": "1510559",
    "end": "1516480"
  },
  {
    "text": "or one only one act and then you say Hey you know if this particular act time out",
    "start": "1516480",
    "end": "1521799"
  },
  {
    "text": "I just you know declare that this this connection is failed and and you can see that it's pretty sensitive to that right",
    "start": "1521799",
    "end": "1527840"
  },
  {
    "text": "so apparently if you reduce the number of x to two then there's there's a",
    "start": "1527840",
    "end": "1533919"
  },
  {
    "text": "significant forse false positive rate and then you have if it's one you have a",
    "start": "1533919",
    "end": "1539200"
  },
  {
    "text": "different result so the idea is that is you know once you have the samples you have this kind of workload you have this",
    "start": "1539200",
    "end": "1546360"
  },
  {
    "text": "whole samples and you can run the simulation start to apply different algorithm and to evaluate you know how",
    "start": "1546360",
    "end": "1553760"
  },
  {
    "text": "safe it is if I want to go aggressive or you know how effective activ is in terms of being actually detected the",
    "start": "1553760",
    "end": "1561960"
  },
  {
    "text": "various so the next is sort of yeah so once a connection is failed like what",
    "start": "1562679",
    "end": "1568559"
  },
  {
    "text": "you can do right then then this is the sort of interesting part there a lot of things you can do uh in this particular case given the workload we have this",
    "start": "1568559",
    "end": "1575799"
  },
  {
    "text": "sort of strategy we come up with we did Implement sort of a more conservative version of this also I skip a lot of",
    "start": "1575799",
    "end": "1581960"
  },
  {
    "text": "details right so you have to kind of handle the throttling a bunch of other things uh but overall this is you know",
    "start": "1581960",
    "end": "1588200"
  },
  {
    "text": "we basically decided you know we we are just let the request long to completion even on fail connections and then we",
    "start": "1588200",
    "end": "1595279"
  },
  {
    "text": "fail over all the new requests to uh separate connection and then the most",
    "start": "1595279",
    "end": "1601320"
  },
  {
    "text": "important thing is that we decide that we should decouple the fair detection timeout and the actual read or right",
    "start": "1601320",
    "end": "1608679"
  },
  {
    "text": "timeout which are application specific right which is like requirements like it's a data line from the upper layers",
    "start": "1608679",
    "end": "1614720"
  },
  {
    "text": "and that's important because a lot of the sort of similar solutions they typically track the",
    "start": "1614720",
    "end": "1620480"
  },
  {
    "text": "actual failes and then to sort of do the fail but in this case we just s let's",
    "start": "1620480",
    "end": "1626159"
  },
  {
    "text": "decouple that so we can understand you know how far where the limits are for this whole",
    "start": "1626159",
    "end": "1631679"
  },
  {
    "text": "problems and lastly I will say you know if you really if the all the parameters Works nicely potentially the arrows can",
    "start": "1631679",
    "end": "1638559"
  },
  {
    "text": "be more or less completely masked if for example the read timeout is long enough",
    "start": "1638559",
    "end": "1644440"
  },
  {
    "text": "and also the right timeout is longer than the actual fair windows and we find that typically the the vares only last a",
    "start": "1644440",
    "end": "1650039"
  },
  {
    "text": "few seconds or 10 seconds those things and that really potentially right can give you a zero fail case that as far as",
    "start": "1650039",
    "end": "1658120"
  },
  {
    "text": "application is concerned now the false positive is what you were always have right in in the",
    "start": "1658120",
    "end": "1664960"
  },
  {
    "text": "real production environment since will be even less stable so the cost in our case is you may have duplicate RS which",
    "start": "1664960",
    "end": "1672000"
  },
  {
    "text": "are fine but there's act overhead and if the rights get mised to you know because it gets sent to a different different",
    "start": "1672000",
    "end": "1677720"
  },
  {
    "text": "connection and then it will have to be forwarded on server",
    "start": "1677720",
    "end": "1682640"
  },
  {
    "text": "side so with all that we kind of just want to share sort of what we think in this case like what is really the key",
    "start": "1684000",
    "end": "1690360"
  },
  {
    "text": "takeaways from here right so I basically say this is the sort of example of the end to end argument right is that try to",
    "start": "1690360",
    "end": "1696399"
  },
  {
    "text": "leverage the service level semantics try to handle things gracefully right try to",
    "start": "1696399",
    "end": "1702880"
  },
  {
    "text": "be able to adapt to the wrong time conditions and do it at a high level as opposed to try to solve the problem at",
    "start": "1702880",
    "end": "1708399"
  },
  {
    "text": "the lower transport layer which typically you don't have it doesn't have all the context and you cannot really do",
    "start": "1708399",
    "end": "1714240"
  },
  {
    "text": "it for this kind of extreme case very low L and say you know you have to",
    "start": "1714240",
    "end": "1720080"
  },
  {
    "text": "manage all the trade-offs very carefully in order to sort of Reach the the minimum error",
    "start": "1720080",
    "end": "1725720"
  },
  {
    "text": "rates uh now Everything's Relative right so on one side we also think that leg",
    "start": "1725720",
    "end": "1733080"
  },
  {
    "text": "three recovery once it if it can be done uh very graceful and quickly and",
    "start": "1733080",
    "end": "1738559"
  },
  {
    "text": "transparently it's also still a good thing because actually you can imagine this involves a lot of user space code",
    "start": "1738559",
    "end": "1744440"
  },
  {
    "text": "where if you can do it in the ler three it will be nice uh Google does publish a paper uh in the sitc count this year",
    "start": "1744440",
    "end": "1750640"
  },
  {
    "text": "it's called Uh preemptive reloading right so in this case when a particular flow is broken then we will reload all",
    "start": "1750640",
    "end": "1759000"
  },
  {
    "text": "the TCP connection package to a different flow and in that case you know everything is just work transparent to",
    "start": "1759000",
    "end": "1764240"
  },
  {
    "text": "the applications so so this is a good example right it's not you know low level requiry uh is still desired if it",
    "start": "1764240",
    "end": "1771200"
  },
  {
    "text": "can be done you know optimally uh so I want to sort of",
    "start": "1771200",
    "end": "1777720"
  },
  {
    "text": "conclude with uh this part you know now you can say Hey you know this a benchmarking yeah it looks all nice you have all the data what what happens to",
    "start": "1777720",
    "end": "1785440"
  },
  {
    "text": "production environment can I really just use this as a sort of a reference implementation or solution for your",
    "start": "1785440",
    "end": "1791399"
  },
  {
    "text": "actual production workload So my answer is probably not right probably not if I will ask the same question again after",
    "start": "1791399",
    "end": "1798440"
  },
  {
    "text": "this my answer will still be no because you know really this whole thing is about data driven right you need to know",
    "start": "1798440",
    "end": "1804840"
  },
  {
    "text": "the data you need to know the work code you can really just sort of you know take this kind of benchmarking data",
    "start": "1804840",
    "end": "1810039"
  },
  {
    "text": "conclusion and apply it as is on the other hand we do think that you know",
    "start": "1810039",
    "end": "1815120"
  },
  {
    "text": "this kind of quantification analysis is useful in the sense that give you the insights",
    "start": "1815120",
    "end": "1820679"
  },
  {
    "text": "understanding what different parameters how they play together okay so that's sort of our goal here so so then this",
    "start": "1820679",
    "end": "1827000"
  },
  {
    "text": "goes back to of you know another area we're looking into that right it's like okay so we all understand the value of",
    "start": "1827000",
    "end": "1833240"
  },
  {
    "text": "data then where you get the data from uh so we look at in our case the kind of s",
    "start": "1833240",
    "end": "1838440"
  },
  {
    "text": "data path we're looking at we found that the the Gap the mo you know the the the",
    "start": "1838440",
    "end": "1844679"
  },
  {
    "text": "big gap is in this uh two boxes are highlighted here it's basically the transport layer uh in two ways one is",
    "start": "1844679",
    "end": "1851760"
  },
  {
    "text": "that it's usually very difficult to get the data in that layer especially with a very low overhead lightweight monitoring",
    "start": "1851760",
    "end": "1859200"
  },
  {
    "text": "the second one is the correlation between this layer the user space gas with the layer three you know layers",
    "start": "1859200",
    "end": "1866120"
  },
  {
    "text": "because you can imagine on one side you're talking about surveys rpcs other other hand you're talking about a packet",
    "start": "1866120",
    "end": "1872840"
  },
  {
    "text": "right a packet May contains you know bites from different rpcs and ADV so so",
    "start": "1872840",
    "end": "1879840"
  },
  {
    "text": "this is the sort of the monitoring gaps yeah we think that that's really important if you know if you look at a production environment and monitoring is",
    "start": "1879840",
    "end": "1886960"
  },
  {
    "text": "really the the critical part so on that note we also like sort of to publish",
    "start": "1886960",
    "end": "1893159"
  },
  {
    "text": "this project which we have been working on for a while uh we just recently posted this and Google uh yeah you can",
    "start": "1893159",
    "end": "1899880"
  },
  {
    "text": "take a picture the sit is available there's a link there uh so this is sort of our first attempt to provide a",
    "start": "1899880",
    "end": "1906399"
  },
  {
    "text": "solution which is lightweight Standalone to give you insights in this so-call transport layer that we think it's very",
    "start": "1906399",
    "end": "1913200"
  },
  {
    "text": "important especially for debugging for dealing with various things",
    "start": "1913200",
    "end": "1918720"
  },
  {
    "text": "uh we we still you know this we continue working on this project hopefully that",
    "start": "1918720",
    "end": "1924120"
  },
  {
    "text": "it will become more useful for you for everyone and specifically in terms of language support currently only support",
    "start": "1924120",
    "end": "1930360"
  },
  {
    "text": "go and we're adding Java and other language support and we think that not",
    "start": "1930360",
    "end": "1936159"
  },
  {
    "text": "only try to you know get the data but how to process export the data in a very",
    "start": "1936159",
    "end": "1941480"
  },
  {
    "text": "light weight is also critical because amount of data is you know it's going to be overwhelming uh so we're looking at",
    "start": "1941480",
    "end": "1948200"
  },
  {
    "text": "that that problem as well and we do think you know quick HTP 3 is also important so we're looking into that as",
    "start": "1948200",
    "end": "1956398"
  },
  {
    "text": "well uh so I yeah so this is the so thanks everyone uh we like your you know",
    "start": "1958200",
    "end": "1964559"
  },
  {
    "text": "feedback and also even better contributions in the three areas right one take a look uh if you think of the",
    "start": "1964559",
    "end": "1970840"
  },
  {
    "text": "ebpf monitoring project is interesting uh it has all the information on the GitHub uh the benchmarking framework we",
    "start": "1970840",
    "end": "1977720"
  },
  {
    "text": "will over time try to open source it we put some there on the you know all the",
    "start": "1977720",
    "end": "1982760"
  },
  {
    "text": "CR stuff we didn't quite put a simul simulation part there as well yet uh but",
    "start": "1982760",
    "end": "1988240"
  },
  {
    "text": "you know take a look and if you want to have any feedback you can uh reach out to us and the the last thing I also",
    "start": "1988240",
    "end": "1994880"
  },
  {
    "text": "think that if you actually have a particular use case in mind you kind of dealing with the same problem uh we like",
    "start": "1994880",
    "end": "2000720"
  },
  {
    "text": "to hear from you you know even better you can share the data sets with us and",
    "start": "2000720",
    "end": "2006000"
  },
  {
    "text": "we can work with you long some simulation just understand if there's a",
    "start": "2006000",
    "end": "2011960"
  },
  {
    "text": "way to create a more generous general purpose solution for these kind of problems the other thing is we in",
    "start": "2011960",
    "end": "2017960"
  },
  {
    "text": "dealing with this problem we also don't know like how wide spread this problem is right you know that this whole hybrid",
    "start": "2017960",
    "end": "2024000"
  },
  {
    "text": "croud problems maybe it's just one particular case we happen to run into this otherwise everyone seems to be",
    "start": "2024000",
    "end": "2030399"
  },
  {
    "text": "doing fine all this is actually a Prett pretty profound problem that everyone's dealing with it's just we don't know",
    "start": "2030399",
    "end": "2036559"
  },
  {
    "text": "that so so yeah with that we we we really want to hear from you okay so thanks everyone we'll start to take",
    "start": "2036559",
    "end": "2044398"
  },
  {
    "text": "[Applause]",
    "start": "2044880",
    "end": "2051719"
  },
  {
    "text": "questions yeah I have a quick question so would you mind just uh I described the formula like in the uh like the the",
    "start": "2057639",
    "end": "2065839"
  },
  {
    "text": "algorithm like in used before like how do your team like come up with that",
    "start": "2065839",
    "end": "2071919"
  },
  {
    "text": "formula yeah this one yeah the question is about yeah how do we come up with this uh SoCal formula",
    "start": "2071919",
    "end": "2079320"
  },
  {
    "text": "uh you know we we just we kind of know right you know there really only two fundamental signals the how many pending",
    "start": "2079320",
    "end": "2086358"
  },
  {
    "text": "X and how long you want to wait for and then we try to make it more dynamic in s",
    "start": "2086359",
    "end": "2092000"
  },
  {
    "text": "you want to have a static say hey wait for you know 100 millisecond 3x and then",
    "start": "2092000",
    "end": "2098359"
  },
  {
    "text": "we just time out everything instead we try to be more a little bit more adaptive especially considering that",
    "start": "2098359",
    "end": "2105040"
  },
  {
    "text": "sometimes the connection may see a lot of concurrent request sometimes they may not have that much right so so yeah so",
    "start": "2105040",
    "end": "2110960"
  },
  {
    "text": "that's the algorithm we come up with uh and also because we're doing this as a benchmarking and simulation so we can",
    "start": "2110960",
    "end": "2116400"
  },
  {
    "text": "easily change the parameters now in actual implementation it may not go as that far uh because this does introduce",
    "start": "2116400",
    "end": "2123280"
  },
  {
    "text": "a lot of sort of you know variables right you you want yeah okay thank",
    "start": "2123280",
    "end": "2130680"
  },
  {
    "text": "you yeah hi um in it's very common in microservices to look at configuration",
    "start": "2131760",
    "end": "2138240"
  },
  {
    "text": "specific to microservices like like Java threads Etc and people research on that",
    "start": "2138240",
    "end": "2143640"
  },
  {
    "text": "I'm wondering if the network if if these knobs are also something if exposed",
    "start": "2143640",
    "end": "2148920"
  },
  {
    "text": "would benefit the microservice performance no one talks about the network today right we just assume grpc",
    "start": "2148920",
    "end": "2154800"
  },
  {
    "text": "Works uh TCP work but these are knobs that can possibly be tuned for microservice uh to make the microservice",
    "start": "2154800",
    "end": "2161440"
  },
  {
    "text": "performance better especially in multicloud kind of deployment yeah thanks uh so the",
    "start": "2161440",
    "end": "2167599"
  },
  {
    "text": "question is yeah can we sort of generalize this expose this as sort of more formal you know configuration Ops for micro services for like GPC workload",
    "start": "2167599",
    "end": "2174640"
  },
  {
    "text": "yeah we definitely look into that we I guess let sort of go back to the the last question I have we don't know how",
    "start": "2174640",
    "end": "2181400"
  },
  {
    "text": "you know why you know this problem really is right because like I said those things are they they do solve the",
    "start": "2181400",
    "end": "2188599"
  },
  {
    "text": "problems when they are actual failes but it does introduce you know false positiv there's a cost with that as well yeah so",
    "start": "2188599",
    "end": "2195560"
  },
  {
    "text": "but we're definitely looking into that okay",
    "start": "2195560",
    "end": "2199400"
  },
  {
    "text": "thanks yeah the other thing I guess I'll Correy just follow your questions uh",
    "start": "2203440",
    "end": "2208599"
  },
  {
    "text": "part of the reason we're doing this is this is like a passive monitoring you're doing this as a data data plan uh but",
    "start": "2208599",
    "end": "2215760"
  },
  {
    "text": "most of the work clo you don't really need to react in such a short window right you can rely on centralized",
    "start": "2215760",
    "end": "2221960"
  },
  {
    "text": "control plan Health tracking those things usually that just work and this is more like a very low latency very",
    "start": "2221960",
    "end": "2228839"
  },
  {
    "text": "long RT compared to the latency requirements like you try to really squeeze the last bit of the sort of the",
    "start": "2228839",
    "end": "2235640"
  },
  {
    "text": "performance",
    "start": "2235640",
    "end": "2238240"
  },
  {
    "text": "reliability okay thanks everyone hope you enjoy the conference and the lunch",
    "start": "2242319",
    "end": "2247630"
  },
  {
    "text": "[Applause]",
    "start": "2247630",
    "end": "2250748"
  }
]