[
  {
    "start": "0",
    "end": "42000"
  },
  {
    "text": "hello everyone and welcome to this talk on auto scaling at scale and how we manage capacity at salando",
    "start": "80",
    "end": "6080"
  },
  {
    "text": "my name is miguel larsen i'm a software engineer at zalando and i work in the cloud infrastructure team",
    "start": "6080",
    "end": "11840"
  },
  {
    "text": "which was responsible for managing our companies infrastructure",
    "start": "11840",
    "end": "17039"
  },
  {
    "text": "salando is the leading online fashion retailer in europe and we operate in 17 countries",
    "start": "17039",
    "end": "22560"
  },
  {
    "text": "across europe we have 11 team we have an 11 fulfillment centers the warehouses where we ship out all the",
    "start": "22560",
    "end": "29279"
  },
  {
    "text": "fashion articles we have 32 million active customers and a lot of visitors every month",
    "start": "29279",
    "end": "35120"
  },
  {
    "text": "and in order to handle this size on the on the technical infrastructure side",
    "start": "35120",
    "end": "40640"
  },
  {
    "text": "we currently have 150 kubernetes clusters running in aws half of them are",
    "start": "40640",
    "end": "47280"
  },
  {
    "start": "42000",
    "end": "42000"
  },
  {
    "text": "production and the other half test clusters we have around 4 000 a little bit more than 4 000",
    "start": "47280",
    "end": "54000"
  },
  {
    "text": "services where 85 is running in kubernetes and we aim to",
    "start": "54000",
    "end": "59039"
  },
  {
    "text": "increase this to 95 percent of all services and because of zelando operates only",
    "start": "59039",
    "end": "64640"
  },
  {
    "text": "europe we also have a certain traffic pattern where we see less load in the nights because",
    "start": "64640",
    "end": "70640"
  },
  {
    "text": "there's less people active on this website and therefore usual scaling",
    "start": "70640",
    "end": "76880"
  },
  {
    "text": "in in some of the bigger kubernetes clusters is between 50 nodes in the night and 350 on the on the peak",
    "start": "76880",
    "end": "83920"
  },
  {
    "text": "hours of the day this is not for all the clusters but it happens for the for the bigger ones that",
    "start": "83920",
    "end": "89119"
  },
  {
    "text": "see so much uh scaling difference and how we manage scaling from the user",
    "start": "89119",
    "end": "96880"
  },
  {
    "text": "point of view so the users of the kubernetes infrastructure the the teams developing services for",
    "start": "96880",
    "end": "102000"
  },
  {
    "text": "solando is through horizontal part auto scaling this is kind of the interface that we",
    "start": "102000",
    "end": "107040"
  },
  {
    "text": "give to users and we use the horizontal part autoscaler the official one from kubernetes",
    "start": "107040",
    "end": "112880"
  },
  {
    "start": "109000",
    "end": "109000"
  },
  {
    "text": "and the best way to explain this one is to look at the algorithm the main idea is that it wants to",
    "start": "112880",
    "end": "118960"
  },
  {
    "text": "calculate a number of replicas or a number of pots or number of instances of your application that should run",
    "start": "118960",
    "end": "125040"
  },
  {
    "text": "given a certain amount of given the current metrics that is observed so the calculation is that it",
    "start": "125040",
    "end": "132319"
  },
  {
    "text": "takes the current number of replicas and multiplies it with the current metric over the desired metric",
    "start": "132319",
    "end": "138000"
  },
  {
    "text": "and the current metric is what is currently being observed of the running parts so it could be the cpu usage across all the parts and",
    "start": "138000",
    "end": "145760"
  },
  {
    "text": "the desired metric is what you have defined for your application this in the scaling configuration so you",
    "start": "145760",
    "end": "152319"
  },
  {
    "text": "might say that you want your service to have 50 cpu load cpu load per per",
    "start": "152319",
    "end": "160000"
  },
  {
    "text": "instance per pot and then you configure 50 average utilization and then if the",
    "start": "160000",
    "end": "166560"
  },
  {
    "text": "if the current metric is reported to be higher than this then it means that it needs to scale out and if it's lower",
    "start": "166560",
    "end": "173040"
  },
  {
    "text": "than this then it needs to scale in um or scale down",
    "start": "173040",
    "end": "178480"
  },
  {
    "text": "and one thing to also note about the algorithm is that it uses a seal and this is to ensure",
    "start": "178480",
    "end": "184000"
  },
  {
    "text": "that it it it calculates a bit more than needed in some cases so",
    "start": "184000",
    "end": "191760"
  },
  {
    "text": "it's better to over shoot the target than to undershoot the target because you rather run with a little bit",
    "start": "191760",
    "end": "197680"
  },
  {
    "text": "extra capacity than to run with too few so that's why it uses the seal to to ensure that um the official hpa",
    "start": "197680",
    "end": "206239"
  },
  {
    "text": "supports cpu and memory out of the box as metric types and then it also supports custom or external",
    "start": "206239",
    "end": "212000"
  },
  {
    "text": "metrics and because the cpu memory is not enough in our case because we have a",
    "start": "212000",
    "end": "217760"
  },
  {
    "start": "215000",
    "end": "215000"
  },
  {
    "text": "lot of different applications and a lot of different use cases we have developed something we call the",
    "start": "217760",
    "end": "223680"
  },
  {
    "text": "cube metrics adapter which is a component that can interface with the hpa",
    "start": "223680",
    "end": "229760"
  },
  {
    "text": "and provide custom metrics so the matrix we provide there is amazon sqs so you can scale your",
    "start": "229760",
    "end": "236239"
  },
  {
    "text": "application based on sqsq length so if you have some job that",
    "start": "236239",
    "end": "241599"
  },
  {
    "text": "reads from a queue you can scale out the number of workers based on that we also support ingress requests per",
    "start": "241599",
    "end": "247840"
  },
  {
    "text": "second metric to scale based on the http traffic for for ecp",
    "start": "247840",
    "end": "252959"
  },
  {
    "text": "apis and services and we also have a generic prometheus query interface so",
    "start": "252959",
    "end": "258400"
  },
  {
    "text": "you can define any promises query and scale based on that you can also define json metrics so",
    "start": "258400",
    "end": "264880"
  },
  {
    "text": "if you have a service that just returns metrics in some json format you can use this to to scale on and then",
    "start": "264880",
    "end": "271680"
  },
  {
    "text": "we also have segment checks which is our internal monitoring solution",
    "start": "271680",
    "end": "276720"
  },
  {
    "text": "where you can also get metrics in some different formats we also support",
    "start": "276720",
    "end": "282080"
  },
  {
    "text": "and we lastly also support influx db queries we don't use influx db in solando but this was contributed",
    "start": "282080",
    "end": "289759"
  },
  {
    "text": "to us from outside and and the cube matrix adapter is an open source project",
    "start": "289759",
    "end": "294880"
  },
  {
    "text": "available and also has users outside of celando so",
    "start": "294880",
    "end": "300479"
  },
  {
    "text": "yeah you can take a look at the link i put here in the slide and just to show how it it looks like",
    "start": "300479",
    "end": "307840"
  },
  {
    "text": "with an example here is where we we scale on requests per second so this is one of our services",
    "start": "307840",
    "end": "313840"
  },
  {
    "text": "where you see the orange which is the request per second and you see the the black line which is the number of",
    "start": "313840",
    "end": "319600"
  },
  {
    "text": "pots and this shows clearly that like it's useful to have",
    "start": "319600",
    "end": "324639"
  },
  {
    "text": "something that scales based on the amount of traffic",
    "start": "324639",
    "end": "329039"
  },
  {
    "text": "um of course there are also some challenges using hpa and some limitations that",
    "start": "330320",
    "end": "337440"
  },
  {
    "text": "is either not supported yet or supported very in very recent kubernetes versions",
    "start": "337440",
    "end": "342639"
  },
  {
    "text": "so one thing is scaling behavior which was unsealed kubernetes 118",
    "start": "342639",
    "end": "348000"
  },
  {
    "text": "a cluster-wide setting so you couldn't do it per application um and another thing is that parts with",
    "start": "348000",
    "end": "355120"
  },
  {
    "text": "multiple containers inside are not handled variable by the auto scaler and i want to give a shout out to arjun",
    "start": "355120",
    "end": "361600"
  },
  {
    "text": "nike who was in our team before and that doesn't work at slander anymore but while he was",
    "start": "361600",
    "end": "366720"
  },
  {
    "text": "here he did a lot of contribution upstream to kubernetes around the hpa and he's working on the on the",
    "start": "366720",
    "end": "373600"
  },
  {
    "text": "first issue and he's currently working on the second issue which is not yet in kubernetes but hopefully we will see it",
    "start": "373600",
    "end": "379039"
  },
  {
    "text": "soon as a fix for for multiple containers",
    "start": "379039",
    "end": "384319"
  },
  {
    "text": "and the the scaling behavior issue or the the changes that were done in in 118 is",
    "start": "384319",
    "end": "390319"
  },
  {
    "start": "385000",
    "end": "385000"
  },
  {
    "text": "that until 117 and including 117 you just had some settings you could set",
    "start": "390319",
    "end": "395919"
  },
  {
    "text": "cluster wide so namely this downscaling stabilization is interesting this means how",
    "start": "395919",
    "end": "402800"
  },
  {
    "text": "how fast a service will scale down depending on the metrics and normally it",
    "start": "402800",
    "end": "409039"
  },
  {
    "text": "was just five minutes so no matter what kind of service you have you just have to rely on these five minutes but from kubernetes 118 you can",
    "start": "409039",
    "end": "415840"
  },
  {
    "text": "configure these individually per auto scaling configuration so per application",
    "start": "415840",
    "end": "421680"
  },
  {
    "text": "so for instance you could do here's and in the example where you set up a lower scaling window from 5 minutes to",
    "start": "421680",
    "end": "429039"
  },
  {
    "text": "60 seconds and this would mean that it would scale down faster you could also increase it if you want to be more",
    "start": "429039",
    "end": "434880"
  },
  {
    "text": "conservative with scaling down and another thing you can do is use policies which is behavior policies",
    "start": "434880",
    "end": "443360"
  },
  {
    "text": "which is a way to say if you are supposed to scale down let's say the hpa calculate to scale",
    "start": "443360",
    "end": "449520"
  },
  {
    "text": "down by 10 parts then you can say that it should only scale down 10 at a time so one part at a time",
    "start": "449520",
    "end": "457840"
  },
  {
    "text": "and this way it doesn't just drop 10 parts at once but it drops the the ten parts more slowly one",
    "start": "457840",
    "end": "464479"
  },
  {
    "text": "one by one so it's a way to to reduce the scaling in case um it would have been too much so it",
    "start": "464479",
    "end": "471039"
  },
  {
    "text": "could be that you have a not consistent load on your system and you don't want to just scale too aggressively down and these",
    "start": "471039",
    "end": "478800"
  },
  {
    "text": "behaviors also work on the on the other direction so when scaling up you can also tweak this",
    "start": "478800",
    "end": "484160"
  },
  {
    "text": "so it's very useful for having this pre-application where you have different different behaviors that you need to",
    "start": "484160",
    "end": "490840"
  },
  {
    "text": "handle regarding the multi container parts",
    "start": "490840",
    "end": "495919"
  },
  {
    "start": "493000",
    "end": "493000"
  },
  {
    "text": "issue here's an example of how it how it currently works so you can understand",
    "start": "495919",
    "end": "501280"
  },
  {
    "text": "what the problem is imagine you have a pot with two containers inside and they each request some in this",
    "start": "501280",
    "end": "508479"
  },
  {
    "text": "example they request 250 millicourse cpu and they one uses 200 millicourse",
    "start": "508479",
    "end": "515518"
  },
  {
    "text": "and the other uses 50 millicourse and you could think of this as the green one is the main application",
    "start": "515519",
    "end": "521518"
  },
  {
    "text": "in your in your pot and the other one is a side car it could be like a um like a stereo sidecar for a service",
    "start": "521519",
    "end": "528399"
  },
  {
    "text": "mesh or something like this or it could be something that ships locks or whatever the main point is that they you might",
    "start": "528399",
    "end": "534880"
  },
  {
    "text": "have one application or one container which is the main container that you want to scale on um and the other one is kind of",
    "start": "534880",
    "end": "541600"
  },
  {
    "text": "secondary but the way it works is that it takes the sum of all the containers so instead",
    "start": "541600",
    "end": "548000"
  },
  {
    "text": "of treating them individually it just take the sum and the sum of usage and the sum of",
    "start": "548000",
    "end": "554080"
  },
  {
    "text": "requested cpu and then instead of having 80 for the for the",
    "start": "554080",
    "end": "559600"
  },
  {
    "text": "main container the the result of the hpa is that it sees 50 utilization across all the containers",
    "start": "559600",
    "end": "567920"
  },
  {
    "text": "so not necessarily wrong the calculation but it's not really helpful for what you want because",
    "start": "567920",
    "end": "572959"
  },
  {
    "text": "if you want to scale your main application or your main container when it's at 50 percent",
    "start": "572959",
    "end": "578720"
  },
  {
    "text": "and it's as at 80 percent in this example then it wouldn't scale up",
    "start": "578720",
    "end": "584560"
  },
  {
    "text": "so this is this is the problem and the solution for this problem that is proposed and currently being implemented",
    "start": "584560",
    "end": "591600"
  },
  {
    "text": "is to extend the way you define metrics so in the in the past or currently you have",
    "start": "591600",
    "end": "598000"
  },
  {
    "text": "just you can you can specify a type resource and it could be cpu and then you specify",
    "start": "598000",
    "end": "603200"
  },
  {
    "text": "the the average utilization and this is this across all the containers but in the",
    "start": "603200",
    "end": "609680"
  },
  {
    "text": "future you would be able to specify another metric type which is container resource and here you can specify which container",
    "start": "609680",
    "end": "616399"
  },
  {
    "text": "that you want to scale based on so you can also add multiple containers and scale on all of them with different strategies",
    "start": "616399",
    "end": "623120"
  },
  {
    "text": "but the main point is that you can select the one that you most care about and that mostly yeah explains how your",
    "start": "623120",
    "end": "631200"
  },
  {
    "text": "application is scaling so this will be very very useful when this lens unfortunately",
    "start": "631200",
    "end": "636640"
  },
  {
    "text": "it is what not in in 119 of kubernetes but hopefully we will see it in 120.",
    "start": "636640",
    "end": "643360"
  },
  {
    "text": "definitely looking forward to this one and this is also adrian that is working on that",
    "start": "643360",
    "end": "649120"
  },
  {
    "text": "now now we talked about the the horizontal part autoscaling this is what our users do",
    "start": "650160",
    "end": "655920"
  },
  {
    "text": "but it doesn't make sense to scale the parts horizontally unless the cluster also scales underneath and for this we have cluster",
    "start": "655920",
    "end": "663440"
  },
  {
    "text": "autoscaling and there we use the official cluster autoscaler for kubernetes",
    "start": "663440",
    "end": "671120"
  },
  {
    "text": "just to explain how it works is that normally you would deploy auto scaling",
    "start": "671360",
    "end": "678160"
  },
  {
    "text": "groups with your with your instances so this is running in aws as an example it also supports",
    "start": "678160",
    "end": "683680"
  },
  {
    "text": "dke and other cloud providers but in aws you have autoscaling groups and you place your nodes",
    "start": "683680",
    "end": "689680"
  },
  {
    "text": "or your instances in those and then once there is a pending pot the",
    "start": "689680",
    "end": "695440"
  },
  {
    "text": "autoscaler will first see if it can fit on an existing node if not it will try to",
    "start": "695440",
    "end": "701360"
  },
  {
    "text": "scale up in node in in one of these availability zones and normally for",
    "start": "701360",
    "end": "707680"
  },
  {
    "text": "parts that don't have so much requirements they can run in any of the zones but if a pod needs a volume it has is",
    "start": "707680",
    "end": "714720"
  },
  {
    "text": "usually attached to a single zone and then the auto scaler needs to pick the right zone so in this example it would be zone",
    "start": "714720",
    "end": "722079"
  },
  {
    "text": "c that it has to pick and to manage this on our side we have some abstraction on top of these",
    "start": "722079",
    "end": "729120"
  },
  {
    "start": "727000",
    "end": "727000"
  },
  {
    "text": "all scaling groups that we call node pools and a node pool is basically a way to define um",
    "start": "729120",
    "end": "737440"
  },
  {
    "text": "like one type of instance and so on that is mapping to multiple auto scaling",
    "start": "737920",
    "end": "743519"
  },
  {
    "text": "groups so by default we met two three autoscaling groups but we can also",
    "start": "743519",
    "end": "748560"
  },
  {
    "text": "reduce it to less if we want to have a certain need for that but basically you define a node pool you",
    "start": "748560",
    "end": "754959"
  },
  {
    "text": "give it you then you get all the scaling groups with the same instance types the same note uh labels and tens if you",
    "start": "754959",
    "end": "762000"
  },
  {
    "text": "want to have special scaling or scheduling logic there and the same min and max size but the",
    "start": "762000",
    "end": "769200"
  },
  {
    "text": "difference is that they are in different zones so if we look this from",
    "start": "769200",
    "end": "774560"
  },
  {
    "text": "from how we define it we basically have a list of node pools for each of our clusters we",
    "start": "774560",
    "end": "780399"
  },
  {
    "text": "we can give them a name like default define the ins the instance types to be used and then set a min and max",
    "start": "780399",
    "end": "787279"
  },
  {
    "text": "and then we also have something we call config items which is a way to either add labels obtains or set other",
    "start": "787279",
    "end": "792959"
  },
  {
    "text": "special configurations for the node pools um so one example could be that we have",
    "start": "792959",
    "end": "798560"
  },
  {
    "text": "a custom node pool which has a certain instance type with local ssd storage and then we will label",
    "start": "798560",
    "end": "804320"
  },
  {
    "text": "and change these nodes with a dedicated storage label such that the parts that really need to run this and",
    "start": "804320",
    "end": "810880"
  },
  {
    "text": "using local ssd can target those in there in the node selector or the toleration",
    "start": "810880",
    "end": "817519"
  },
  {
    "text": "um this is how we operate like how we handle that there are special cases for different",
    "start": "817519",
    "end": "823519"
  },
  {
    "text": "parts so it could also be a cpu node we would label in a certain way so only parts that actually need to use",
    "start": "823519",
    "end": "829600"
  },
  {
    "text": "the cpu gpu i mean would land on this [Music]",
    "start": "829600",
    "end": "837038"
  },
  {
    "start": "836000",
    "end": "836000"
  },
  {
    "text": "and because the cluster order scalar by default doesn't do everything we want",
    "start": "837360",
    "end": "843760"
  },
  {
    "text": "and doesn't do everything we want as well as we want we have made some changes to the official and we run a",
    "start": "843760",
    "end": "849839"
  },
  {
    "text": "fork of it um and the changes we have worked on is to do more robust template node",
    "start": "849839",
    "end": "855120"
  },
  {
    "text": "generation so a template node is that there's a pot that is pending and the auto scaler",
    "start": "855120",
    "end": "861040"
  },
  {
    "text": "tries to calculate what kind of instances need to create or what kind of kubernetes node it would need",
    "start": "861040",
    "end": "866560"
  },
  {
    "text": "to satisfy the pot and um in the in the official auto scale it's",
    "start": "866560",
    "end": "872320"
  },
  {
    "text": "not so easy if you don't have any existing nodes because it relies on the existing nodes in the cluster",
    "start": "872320",
    "end": "877680"
  },
  {
    "text": "but if you're scaling from zero you need a way to predict how the node will look like once",
    "start": "877680",
    "end": "883040"
  },
  {
    "text": "it's created and this we have made some changes around that and we also added support for multiple instance types on aws",
    "start": "883040",
    "end": "889839"
  },
  {
    "text": "um this is mainly to handle spot where you can create auto scaling groups that have",
    "start": "889839",
    "end": "895199"
  },
  {
    "text": "different instance types and then aws spot can select which",
    "start": "895199",
    "end": "900240"
  },
  {
    "text": "which is the best for the time so if there's a certain shortage in one instance type it will",
    "start": "900240",
    "end": "906000"
  },
  {
    "text": "select another instance type and give you um additionally we have added a custom",
    "start": "906000",
    "end": "911920"
  },
  {
    "text": "customizable backup settings which is needed when you are trying to scale up auto scanning",
    "start": "911920",
    "end": "918480"
  },
  {
    "text": "group and it fails for whatever reason we hit many issues where aws simply",
    "start": "918480",
    "end": "924000"
  },
  {
    "text": "don't don't have enough of the instance type that we want and then the auto scaler has to fall back to another kind of auto",
    "start": "924000",
    "end": "930720"
  },
  {
    "text": "scaling group which has a different instance type that can be provided at this time",
    "start": "930720",
    "end": "936720"
  },
  {
    "text": "and we want this to be as fast as possible so we don't have parts pending for too long time",
    "start": "936720",
    "end": "943120"
  },
  {
    "text": "and last thing we did is we added some priority based expander expander is a name used in the",
    "start": "943120",
    "end": "950160"
  },
  {
    "text": "autoscaler for logic that can kind of figure out which node to pick which is the",
    "start": "950160",
    "end": "956639"
  },
  {
    "text": "best one to pick and we added um a simple one which uses priorities and",
    "start": "956639",
    "end": "962560"
  },
  {
    "text": "the way it works is that we have here again the the node pool overview but",
    "start": "962560",
    "end": "968320"
  },
  {
    "start": "964000",
    "end": "964000"
  },
  {
    "text": "you can see the config item that has priorities defined and this way we can define the default",
    "start": "968320",
    "end": "973519"
  },
  {
    "text": "null pool with the priority zero and then we can define fallback node pools of different",
    "start": "973519",
    "end": "979759"
  },
  {
    "text": "instance types to have things to fall back to in case we cannot provide the aws doesn't have the",
    "start": "979759",
    "end": "987680"
  },
  {
    "text": "the one that we want by default and this can both work as just fall back so going back to an m4",
    "start": "987680",
    "end": "993920"
  },
  {
    "text": "large instead of an m5 light so going back to an older generation instance but it can also work in the way that we",
    "start": "993920",
    "end": "1000480"
  },
  {
    "text": "always choose the smallest instance type unless the parts request a bigger size than if the pod has to run",
    "start": "1000480",
    "end": "1008399"
  },
  {
    "text": "an m5x large because of the request then it would would land on this instead",
    "start": "1008399",
    "end": "1013680"
  },
  {
    "text": "of instead of a smaller one and for spot pools just as an example we can define like i",
    "start": "1013680",
    "end": "1019920"
  },
  {
    "text": "said multiple instance types and then we don't know which of them we get but whatever aws picks as the best",
    "start": "1019920",
    "end": "1026240"
  },
  {
    "text": "one for the time in terms of getting uh getting taken out of spot",
    "start": "1026240",
    "end": "1032079"
  },
  {
    "text": "to terminate it earlier and for this we can lose we also add a label so you can you you",
    "start": "1032079",
    "end": "1038880"
  },
  {
    "text": "can select and actively opt into running on spot instances if you want to do this",
    "start": "1038880",
    "end": "1045360"
  },
  {
    "text": "of course the auto scale also have some limits um and are not the not the auto scale",
    "start": "1046959",
    "end": "1054880"
  },
  {
    "text": "itself but once you figure solve all these problems of the auto scaler so it can scale out",
    "start": "1054880",
    "end": "1060720"
  },
  {
    "text": "and pick new instances and so on then what you start to do is you start to hit limits",
    "start": "1060720",
    "end": "1066559"
  },
  {
    "text": "and it's we found that it's very easy to hit the default aws limits on on the number of instances that you can",
    "start": "1066559",
    "end": "1073200"
  },
  {
    "text": "have in your account um and we also hit limits in the networking layout so for",
    "start": "1073200",
    "end": "1080400"
  },
  {
    "text": "the aws limits basically every account that is created has a certain default",
    "start": "1080400",
    "end": "1085679"
  },
  {
    "text": "set of limiters so you cannot just create an account and create millions of instances and then in order to get this limit",
    "start": "1085679",
    "end": "1092720"
  },
  {
    "text": "increased you need to open a support request to aws and this became very time consuming",
    "start": "1092720",
    "end": "1098559"
  },
  {
    "text": "because we had to do this for many accounts many clusters and we had to do this for",
    "start": "1098559",
    "end": "1103760"
  },
  {
    "text": "all of these different node pools that we create um so what we did is that we have a cron",
    "start": "1103760",
    "end": "1109760"
  },
  {
    "text": "job that just looks at how much we want and then it automatically creates the the support request rate of",
    "start": "1109760",
    "end": "1116320"
  },
  {
    "text": "this and bumps the limits so this helps us a lot so we don't have to to do this ourselves",
    "start": "1116320",
    "end": "1122720"
  },
  {
    "text": "and regarding the limit on the network um this is basically this depends a little",
    "start": "1122720",
    "end": "1128160"
  },
  {
    "text": "bit on the network layout you have and how you what kind of network interface you're using kubernetes",
    "start": "1128160",
    "end": "1134080"
  },
  {
    "text": "but in our setup we picked for all our clusters slash 16 uh cider which gives us",
    "start": "1134080",
    "end": "1140960"
  },
  {
    "text": "around 65 000 addresses and with the default um configuration per",
    "start": "1140960",
    "end": "1148000"
  },
  {
    "text": "node you have 24 which gives you 256",
    "start": "1148000",
    "end": "1153360"
  },
  {
    "text": "addresses per node and this also means that you can maximum have 256 nodes because otherwise the address",
    "start": "1153360",
    "end": "1160480"
  },
  {
    "text": "space is used up so the first thing we hit was this limit that we couldn't create more than 256",
    "start": "1160480",
    "end": "1167840"
  },
  {
    "text": "nodes and then we found a way to we basically figured out that the way to",
    "start": "1167840",
    "end": "1174160"
  },
  {
    "text": "deal with this is to just increase the side or lower the lower the side",
    "start": "1174160",
    "end": "1179679"
  },
  {
    "text": "address space to slash 15 per node because then you have only 128",
    "start": "1179679",
    "end": "1186640"
  },
  {
    "text": "addresses per node but you get 512 nodes and you can go further and select",
    "start": "1186640",
    "end": "1194080"
  },
  {
    "text": "less addresses per node and then get more nodes in total the lowest we have is slash 26 because",
    "start": "1194080",
    "end": "1201919"
  },
  {
    "text": "once you go to slash 27 you have so few addresses per node that it becomes hard to schedule any",
    "start": "1201919",
    "end": "1209360"
  },
  {
    "text": "pots on the nodes because usually kubernetes expects that you have at least a double amount of",
    "start": "1209360",
    "end": "1214480"
  },
  {
    "text": "averages as as much minimum number of pots and this is to ensure that it can easily",
    "start": "1214480",
    "end": "1220960"
  },
  {
    "text": "shuffle around parts without giving them the same addresses as a previous part because then you could be sending traffic to",
    "start": "1220960",
    "end": "1228159"
  },
  {
    "text": "to the wrong application basically um and one thing also to notice that the cubelet has a limit",
    "start": "1228159",
    "end": "1234000"
  },
  {
    "text": "of 110 parts by default so even with this slash 24 where you have technically 256",
    "start": "1234000",
    "end": "1241760"
  },
  {
    "text": "addresses you can only schedule 110 parts this also means that if you want to",
    "start": "1241760",
    "end": "1247520"
  },
  {
    "text": "lower this uh cider to slash 25 or less 26 then you also need to change",
    "start": "1247520",
    "end": "1253039"
  },
  {
    "text": "the flag on the cubelet to have less spots because otherwise you can schedule more than you actually have",
    "start": "1253039",
    "end": "1258880"
  },
  {
    "text": "addresses for so this is something to look out for and we have basically",
    "start": "1258880",
    "end": "1264320"
  },
  {
    "text": "configured this that we just have per cluster we can configure this the node sider and then we automatically",
    "start": "1264320",
    "end": "1270480"
  },
  {
    "text": "calculate how many parts per node and uh yeah but the limit we have right now",
    "start": "1270480",
    "end": "1276000"
  },
  {
    "text": "is thousand parts with this current uh setup so if we want to go further in the in the future more nodes",
    "start": "1276000",
    "end": "1283280"
  },
  {
    "text": "per cluster then we need to to change how the network setup is um so if you're creating a new cluster and you",
    "start": "1283280",
    "end": "1289919"
  },
  {
    "text": "expect to have a lot of nodes then it's something to be aware of",
    "start": "1289919",
    "end": "1294640"
  },
  {
    "text": "and of course once you fix all of these limit problems and you allow your users to scale up to",
    "start": "1295840",
    "end": "1301600"
  },
  {
    "text": "a thousand nodes then they will also do it so they will make sure to create enough parts to",
    "start": "1301600",
    "end": "1306880"
  },
  {
    "text": "scale out and this is just an example of this and by scaling up the",
    "start": "1306880",
    "end": "1311919"
  },
  {
    "text": "cluster this is one thing this is actually fine but it obviously also puts some loads on",
    "start": "1311919",
    "end": "1317760"
  },
  {
    "text": "the control plane so on the api server and and so on and this is just an example of",
    "start": "1317760",
    "end": "1324240"
  },
  {
    "text": "yeah when we skipped a thousand then our latency goes from so little we cannot even see it here",
    "start": "1324240",
    "end": "1330559"
  },
  {
    "text": "until uh several seconds in latency and yeah this is obviously",
    "start": "1330559",
    "end": "1336080"
  },
  {
    "text": "not ideal because every time you do keep yourself get pots or any controller that needs to list all",
    "start": "1336080",
    "end": "1341600"
  },
  {
    "text": "the products in the cluster will either time out or it will take much longer than than",
    "start": "1341600",
    "end": "1346880"
  },
  {
    "text": "normally um so in order to handle this we have",
    "start": "1346880",
    "end": "1351919"
  },
  {
    "start": "1350000",
    "end": "1350000"
  },
  {
    "text": "something we call the control plane auto scaler which is something that we develop for our needs and",
    "start": "1351919",
    "end": "1358960"
  },
  {
    "text": "the way it works is that it scales vertically the parts the the the control plane nodes so they",
    "start": "1358960",
    "end": "1364799"
  },
  {
    "text": "will just run on ec2 instances and they are scaled vertically the reason we scale vertically and not horizontally",
    "start": "1364799",
    "end": "1371360"
  },
  {
    "text": "is that what you want to scale is that a single instance can read all the parts",
    "start": "1371360",
    "end": "1376799"
  },
  {
    "text": "from it cd and and send it via the api server so having many small instances would not",
    "start": "1376799",
    "end": "1384320"
  },
  {
    "text": "actually solve the problem because then do just have many small instances that need to",
    "start": "1384320",
    "end": "1390000"
  },
  {
    "text": "read a lot of data and what you want is bigger instances that can lead a lot of data faster so we already run",
    "start": "1390000",
    "end": "1398640"
  },
  {
    "text": "with two nodes at minimum to have highly available but we don't scale it further horizontally instead we scale it",
    "start": "1398640",
    "end": "1405360"
  },
  {
    "text": "vertically and we basically look at cpu load as an indicator for scaling",
    "start": "1405360",
    "end": "1410720"
  },
  {
    "text": "um it could also be interesting to look at memory but we found that cpu is enough to",
    "start": "1410720",
    "end": "1416000"
  },
  {
    "text": "indicate whether we need to pick a bigger instance um and the way we scale this just by",
    "start": "1416000",
    "end": "1421600"
  },
  {
    "text": "changing the instance type so we basically sort all the instance types available in in aws",
    "start": "1421600",
    "end": "1427360"
  },
  {
    "text": "by vcpu and memory and then we exclude some like we don't include gpu nodes and",
    "start": "1427360",
    "end": "1432480"
  },
  {
    "text": "so on but then if if we see the cpu load is",
    "start": "1432480",
    "end": "1437840"
  },
  {
    "text": "is high at a certain level that we want to scale out then we just or this",
    "start": "1437840",
    "end": "1444320"
  },
  {
    "text": "control plane auto scaler will pick the next instance type um sorted by by vcpu and memory",
    "start": "1444320",
    "end": "1451679"
  },
  {
    "text": "and one of the most important things is that this automates a previous manual task because",
    "start": "1451679",
    "end": "1457840"
  },
  {
    "text": "before we just had alerts that would tell us the api server is on the high load and then we would go and",
    "start": "1457840",
    "end": "1464240"
  },
  {
    "text": "figure out okay this is running this instance type it's changing to a bigger one and now we have automated this which",
    "start": "1464240",
    "end": "1470799"
  },
  {
    "text": "makes it much easier to manage uh 150 clusters and it's not only good",
    "start": "1470799",
    "end": "1476320"
  },
  {
    "text": "for scaling out so it's getting bigger it's also good for scaling down again once the the cluster is under less load because",
    "start": "1476320",
    "end": "1483200"
  },
  {
    "text": "you don't want to just leave running with the very big instances that are much more expensive so",
    "start": "1483200",
    "end": "1488720"
  },
  {
    "text": "this helps us both on cost saving and also on on the manual work that we would have to do in the past",
    "start": "1488720",
    "end": "1495679"
  },
  {
    "text": "along with the vertical scaling of the control plane we also have vertical pod auto scaling",
    "start": "1496640",
    "end": "1503200"
  },
  {
    "text": "and here we use the vertical pod auto scaler also from kubernetes",
    "start": "1503200",
    "end": "1508640"
  },
  {
    "text": "and the way it works is that it scales parts vertically by scale by changing the request and limits so unlike",
    "start": "1508640",
    "end": "1514080"
  },
  {
    "text": "our control plane autoscaler it doesn't change instance types but it changes how much request limits",
    "start": "1514080",
    "end": "1519840"
  },
  {
    "text": "that you define on the parts um it scales only based on cpu and memory so there's no custom metrics like",
    "start": "1519840",
    "end": "1526480"
  },
  {
    "text": "the hpa but this is also usually what you want cpu and memory and it's useful for",
    "start": "1526480",
    "end": "1534880"
  },
  {
    "text": "components that scale vertically with the size of the cluster so example is like prometheus",
    "start": "1534880",
    "end": "1540000"
  },
  {
    "text": "you want to have more execute more you want to collect more more metrics",
    "start": "1540000",
    "end": "1545440"
  },
  {
    "text": "and maybe execute more more queries depending on how many how big your cluster is and how many resources you",
    "start": "1545440",
    "end": "1551200"
  },
  {
    "text": "have in the cluster and you cannot just scale prometheus horizontally you need to scale the",
    "start": "1551200",
    "end": "1556480"
  },
  {
    "text": "individual instances to bigger sizes so vertically um another example is ingress controller so",
    "start": "1556480",
    "end": "1563760"
  },
  {
    "text": "if you you have just a single controller running in your cluster and if you have",
    "start": "1563760",
    "end": "1568880"
  },
  {
    "text": "more ingresses uh then you also need to scale it vertically because it needs to download more",
    "start": "1568880",
    "end": "1575120"
  },
  {
    "text": "uh ingress resources and keep them in memory for some time um similar externally and this is",
    "start": "1575120",
    "end": "1581600"
  },
  {
    "text": "another example where it also scales with the number of ingress or service resources and you have any cluster so the bigger the cluster",
    "start": "1581600",
    "end": "1588480"
  },
  {
    "text": "usually the the bigger they need to scale and for this the the vertical part autoscaler is very helpful to",
    "start": "1588480",
    "end": "1593840"
  },
  {
    "text": "to manage this and this is how it looks like so uh here's",
    "start": "1593840",
    "end": "1600080"
  },
  {
    "text": "an example of a prometheus in one of our clusters and the orange line is the request limit",
    "start": "1600080",
    "end": "1606159"
  },
  {
    "text": "so we always use the same request and limit for memories so we cannot overcome it um but it basically shows that at some",
    "start": "1606159",
    "end": "1612880"
  },
  {
    "text": "point it scales a bit down and then it scales up again when it sees that the usage which is the",
    "start": "1612880",
    "end": "1618159"
  },
  {
    "text": "blue and the purple goes up and down over time um and this is",
    "start": "1618159",
    "end": "1623200"
  },
  {
    "text": "like how it should work and and how we can kind of also save cost so we don't have",
    "start": "1623200",
    "end": "1629200"
  },
  {
    "text": "to run with a very high uh memory uh request for all of the",
    "start": "1629200",
    "end": "1634400"
  },
  {
    "text": "clusters but it depends on the cluster size and also how much it's scaling",
    "start": "1634400",
    "end": "1640399"
  },
  {
    "text": "of course there's also failure modes with the vpa it's also um i don't think it's that widely used",
    "start": "1640399",
    "end": "1647039"
  },
  {
    "text": "yet and there's some things that are also a bit troubling for us",
    "start": "1647039",
    "end": "1652080"
  },
  {
    "text": "um so one thing that we have seen a lot is that it it hasn't picked a certain",
    "start": "1652080",
    "end": "1659360"
  },
  {
    "text": "amount of resources and then over time it attempts to scale this down when it sees that these",
    "start": "1659360",
    "end": "1665760"
  },
  {
    "text": "resources are not being used so this is also what it's supposed to do but sometimes it scales too far down as",
    "start": "1665760",
    "end": "1671679"
  },
  {
    "text": "this example but then it quickly tries to scale up again and pick a little bit higher value",
    "start": "1671679",
    "end": "1677520"
  },
  {
    "text": "um yeah but in other cases it can also scale down too aggressively",
    "start": "1677520",
    "end": "1683440"
  },
  {
    "text": "and then it can take longer so there's more steps in order for it to uh to recover and and scale to",
    "start": "1683440",
    "end": "1690240"
  },
  {
    "text": "a high enough size that uh yeah that the application won't get",
    "start": "1690240",
    "end": "1695440"
  },
  {
    "text": "out of memory killed so for this we also have a fork of the",
    "start": "1695440",
    "end": "1701760"
  },
  {
    "text": "vertical powder scaler similar to the cluster autoscaler and in our fork we have worked a lot of",
    "start": "1701760",
    "end": "1707360"
  },
  {
    "text": "improving the oil and kill handling so whenever a pot runs out of memory what we wanted to do",
    "start": "1707360",
    "end": "1714720"
  },
  {
    "text": "is to recover as quickly as possible so we want the bpa to scale out the resources higher and higher and we",
    "start": "1714720",
    "end": "1722159"
  },
  {
    "text": "want we made a bunch of changes to make it as quick as possible there's still work to do because there's still situations where",
    "start": "1722159",
    "end": "1728799"
  },
  {
    "text": "it can where it can take too long to recover a promisius instance that that ran out of memory and we also did",
    "start": "1728799",
    "end": "1736480"
  },
  {
    "text": "some other small improvements of reducing the memory uses of the vpa components and also which way it's actually uh",
    "start": "1736480",
    "end": "1743600"
  },
  {
    "text": "upstreamed and then we also added timeouts and so on for the mission record book which is part of the vpa so just",
    "start": "1743600",
    "end": "1750880"
  },
  {
    "text": "small changes we have a link for the thought for all the changes we made if you are interested in that",
    "start": "1750880",
    "end": "1758720"
  },
  {
    "start": "1759000",
    "end": "1759000"
  },
  {
    "text": "and this is pretty much the talk i put here some links to the open source things that we have done",
    "start": "1759760",
    "end": "1766640"
  },
  {
    "text": "which i talked about in the talk so we have the cube matrix adapter which is yeah it's an open source project and",
    "start": "1766640",
    "end": "1772640"
  },
  {
    "text": "as you saw someone also contributed influx db queries for it and",
    "start": "1772640",
    "end": "1779360"
  },
  {
    "text": "you can also contribute other things if you are interested and we have a link to the cluster",
    "start": "1779360",
    "end": "1784399"
  },
  {
    "text": "autoscaler fork and to the vertical port autoscaler fork and then to the two kubernetes",
    "start": "1784399",
    "end": "1790320"
  },
  {
    "text": "enhancements proposals around the hpa so the one that is already in 118 about",
    "start": "1790320",
    "end": "1796480"
  },
  {
    "text": "scale up and down velocity and scaling behavior configuration and then one that is about the container",
    "start": "1796480",
    "end": "1803679"
  },
  {
    "text": "resource auto scaling so handling contained are pots with multiple containers inside",
    "start": "1803679",
    "end": "1814240"
  }
]