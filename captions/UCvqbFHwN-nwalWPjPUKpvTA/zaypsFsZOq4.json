[
  {
    "text": "first talk uh of the K uh do and um welcome to the talk and thanks for",
    "start": "199",
    "end": "6399"
  },
  {
    "text": "joining our session um today we are going to share um the Journey of uh how we use the various uh open source",
    "start": "6399",
    "end": "13719"
  },
  {
    "text": "project to set up a uh metadata driven ml platform my name is Yong Wang and",
    "start": "13719",
    "end": "19520"
  },
  {
    "text": "this is my coworker T Chen um we are working at the open source AI Technology",
    "start": "19520",
    "end": "25000"
  },
  {
    "text": "Group in IBM research um both of us have been uh working on various open source",
    "start": "25000",
    "end": "30119"
  },
  {
    "text": "project um I think at least eight years um most of the project we work on",
    "start": "30119",
    "end": "35480"
  },
  {
    "text": "recently uh are related to the machine learning um uh like uh K flow K flow",
    "start": "35480",
    "end": "41520"
  },
  {
    "text": "plank PL tensorflow Ray uh Cay and Flink training Operator just them a few um so",
    "start": "41520",
    "end": "47520"
  },
  {
    "text": "we are committed uh contributors maintenance as well as um actually acoc",
    "start": "47520",
    "end": "52600"
  },
  {
    "text": "case um So based on our experience and Explorations um we conduct a uh m",
    "start": "52600",
    "end": "60160"
  },
  {
    "text": "platform uh and like to share it with you the platform comprise many uh open",
    "start": "60160",
    "end": "65518"
  },
  {
    "text": "source projects so we will go through uh them one by one and provide the information uh you need to set up the",
    "start": "65519",
    "end": "71880"
  },
  {
    "text": "platform so hope this can help you to seamlessly um integrate ml into your",
    "start": "71880",
    "end": "77560"
  },
  {
    "text": "business uh Solutions so here is the agenda so we",
    "start": "77560",
    "end": "84280"
  },
  {
    "text": "will actually cover um uh two main topic and those are very important um for the",
    "start": "84280",
    "end": "90360"
  },
  {
    "text": "ml enablement the first one is the open Le house I think open Le house Le house",
    "start": "90360",
    "end": "96560"
  },
  {
    "text": "itself has become a very trendy topic um these two years uh is the foundation",
    "start": "96560",
    "end": "101600"
  },
  {
    "text": "actually of the ml platform that we construct and it combines uh the feature",
    "start": "101600",
    "end": "107119"
  },
  {
    "text": "of the warehouse and datal lake and is uh used by many uh ml persona for",
    "start": "107119",
    "end": "113119"
  },
  {
    "text": "example like uh data analytics and the future engineer and data scientist so I",
    "start": "113119",
    "end": "118920"
  },
  {
    "text": "will cover how to build a um open lake house um and its key component later and",
    "start": "118920",
    "end": "124920"
  },
  {
    "text": "the second part um is the metadata driven ml pipline so you are not able to",
    "start": "124920",
    "end": "131200"
  },
  {
    "text": "actually adopt the ml successfully without an ml pipline framework so here we believe um klow uh is a perfect",
    "start": "131200",
    "end": "139319"
  },
  {
    "text": "solution for this and klow is actually a very big uh uh umbr for a group of ml",
    "start": "139319",
    "end": "145920"
  },
  {
    "text": "project um with a uh building even pipeline called um Kule",
    "start": "145920",
    "end": "152480"
  },
  {
    "text": "pipelines so Ted um he will share more information about it along with other",
    "start": "152480",
    "end": "157599"
  },
  {
    "text": "projects that you can easily uh integrate with the CP flow including a ktip for hyper Prem tuning a training",
    "start": "157599",
    "end": "164879"
  },
  {
    "text": "operator for model training case surve for model suring Etc so without further Ado uh let's get",
    "start": "164879",
    "end": "171599"
  },
  {
    "text": "started so I think first thing first they start with the ml life cycle um the",
    "start": "171599",
    "end": "177280"
  },
  {
    "text": "H issue right now everyone is tling um here is the veride view of the ml life",
    "start": "177280",
    "end": "182720"
  },
  {
    "text": "cycle so it actually contain three pillars data a model and deployment so",
    "start": "182720",
    "end": "189560"
  },
  {
    "text": "yeah data model and deployment so um the the life cycle always start with from",
    "start": "189560",
    "end": "195400"
  },
  {
    "text": "the data and then you use the data uh to build the models the automate decisions",
    "start": "195400",
    "end": "200560"
  },
  {
    "text": "so in the data phase um you acquire data and you X-ray and analyze the useful",
    "start": "200560",
    "end": "207120"
  },
  {
    "text": "features in the model phase you Ed the state of art technique to compose and",
    "start": "207120",
    "end": "212200"
  },
  {
    "text": "train your model and finally in the deployment phase you deploy the model and integrate them into uh Your solution",
    "start": "212200",
    "end": "218360"
  },
  {
    "text": "or products so actually that's dive into the detail of the ml lab cycle um so and",
    "start": "218360",
    "end": "226239"
  },
  {
    "text": "have a clear picture of it um I believe if you try to search m uh life cycle uh",
    "start": "226239",
    "end": "233360"
  },
  {
    "text": "you will see a lot of different flow and diagrams here is just one of them uh but",
    "start": "233360",
    "end": "239439"
  },
  {
    "text": "I believe um all of them actually all carry a very same uh message the end to",
    "start": "239439",
    "end": "247239"
  },
  {
    "text": "end ml life cycle is extremely complicated so here um you can still see",
    "start": "247239",
    "end": "254360"
  },
  {
    "text": "um the three phases I mentioned earlier the data phase uh the model phase and",
    "start": "254360",
    "end": "259519"
  },
  {
    "text": "the deployment phase so uh and different um you see uh",
    "start": "259519",
    "end": "266040"
  },
  {
    "text": "each phase is actually constituted of multiple test and the operate operations",
    "start": "266040",
    "end": "272160"
  },
  {
    "text": "as you can see here so different task are conducted and perform actually by",
    "start": "272160",
    "end": "277840"
  },
  {
    "text": "different teams uh with specific expertise and skills for example um data",
    "start": "277840",
    "end": "283039"
  },
  {
    "text": "Engineers works on data injection data transformation and dat data splitting",
    "start": "283039",
    "end": "288680"
  },
  {
    "text": "and data scientist um works on data uh uh analysis and feature extration ml",
    "start": "288680",
    "end": "295479"
  },
  {
    "text": "practitioners work on model composing and model optimization ml Engineers",
    "start": "295479",
    "end": "301039"
  },
  {
    "text": "works on model uh training at scale and model validation and Dev up and software",
    "start": "301039",
    "end": "307199"
  },
  {
    "text": "engineer work on model deployment and uh application integration so obviously uh",
    "start": "307199",
    "end": "313240"
  },
  {
    "text": "having a platform to uh facilitate the entire ml Pipeline and enable various",
    "start": "313240",
    "end": "318639"
  },
  {
    "text": "team to work on their task become a musthave to success in the ml",
    "start": "318639",
    "end": "326360"
  },
  {
    "text": "enablement so when we adopt the uh ml um",
    "start": "326440",
    "end": "331639"
  },
  {
    "text": "according to our experience you will face the following um challenges and we",
    "start": "331639",
    "end": "337800"
  },
  {
    "text": "try to categorize in in two two category the first is the data preparation of course because everything start from the",
    "start": "337800",
    "end": "344360"
  },
  {
    "text": "data and the data volume keeps gr and gr uh in the M world you know and then you",
    "start": "344360",
    "end": "351000"
  },
  {
    "text": "need to be able to retrieve and process data fast enough to meet meet your",
    "start": "351000",
    "end": "356160"
  },
  {
    "text": "deadline and achieve a faster uh time to Market Market strategy and second is the",
    "start": "356160",
    "end": "362600"
  },
  {
    "text": "um data type they are structured semi structured or unstructured data types",
    "start": "362600",
    "end": "368520"
  },
  {
    "text": "and I believe uh for most of the ml solution you need to deal with semi-structured data and even",
    "start": "368520",
    "end": "375599"
  },
  {
    "text": "unstructured data AKA um we call it schema and read but you may also need to",
    "start": "375599",
    "end": "381599"
  },
  {
    "text": "handle structured data from time to time so we need a solution to be able to handle this different type data type",
    "start": "381599",
    "end": "388800"
  },
  {
    "text": "without complicated data moving and transformation processes and also the",
    "start": "388800",
    "end": "394319"
  },
  {
    "text": "flexibility and elasticity data usually come from multiple data source and they",
    "start": "394319",
    "end": "400160"
  },
  {
    "text": "are different type of workloads in data preprocessing for example like um",
    "start": "400160",
    "end": "405560"
  },
  {
    "text": "interactive or ad hoc data queries or badge processing and of course how much the",
    "start": "405560",
    "end": "412120"
  },
  {
    "text": "platform would cost is the import one of the important factors and if the",
    "start": "412120",
    "end": "418039"
  },
  {
    "text": "platform is using open format or vendor locked format is also important so and",
    "start": "418039",
    "end": "423680"
  },
  {
    "text": "for the second part um the ml uh complexity is always one of the",
    "start": "423680",
    "end": "428800"
  },
  {
    "text": "challenges we know the ml life cycle earlier is very complicated and require",
    "start": "428800",
    "end": "434560"
  },
  {
    "text": "coration from multiple teams and the more people work on the same system the",
    "start": "434560",
    "end": "439919"
  },
  {
    "text": "greater likelihood of errors and the harder to debug so aor prom is another",
    "start": "439919",
    "end": "446400"
  },
  {
    "text": "challenge and the entire M of life cycle is a lensy process you need a system",
    "start": "446400",
    "end": "452400"
  },
  {
    "text": "that is flexible enough to actually adjust the resource based on the workflow and entire",
    "start": "452400",
    "end": "459879"
  },
  {
    "text": "platform so last but not least is actually the automation because it's very lengthy so definitely we want a",
    "start": "459879",
    "end": "466280"
  },
  {
    "text": "automation mechanism to speed up the ml lab cycle and the the the mechanisms",
    "start": "466280",
    "end": "471759"
  },
  {
    "text": "need to be easily uh manage the B um and can apply to various kind of task",
    "start": "471759",
    "end": "477879"
  },
  {
    "text": "because you can see each of the ml um tasks or operation they require",
    "start": "477879",
    "end": "483720"
  },
  {
    "text": "different um methodology or",
    "start": "483720",
    "end": "487560"
  },
  {
    "text": "techniques okay so here is the um our um",
    "start": "490000",
    "end": "495840"
  },
  {
    "text": "so did I sorry I actually okay so so apparently I think",
    "start": "495840",
    "end": "503120"
  },
  {
    "text": "our solution uh for handling data preprocessing the L challenge I",
    "start": "503120",
    "end": "508360"
  },
  {
    "text": "mentioned earlier uh the answer is very obvious is actually the open Lous so",
    "start": "508360",
    "end": "513919"
  },
  {
    "text": "open Lous stitch together the features of the warehouse and dat Lake Al to the",
    "start": "513919",
    "end": "519518"
  },
  {
    "text": "data and bring the traditional data analytics and advanced functionality for",
    "start": "519519",
    "end": "525880"
  },
  {
    "text": "ML scenarios a l house um is still used the object storyy to store the different",
    "start": "525880",
    "end": "532480"
  },
  {
    "text": "type of data which keep the cost low and one of the key component of L house is",
    "start": "532480",
    "end": "538880"
  },
  {
    "text": "actually the processing engine and usually it actually can provide um the capability to connect various data",
    "start": "538880",
    "end": "545480"
  },
  {
    "text": "source and run queries at scale and based on the name the an open leg house",
    "start": "545480",
    "end": "551480"
  },
  {
    "text": "definitely use the open standard and is Solly formed by using uh open source",
    "start": "551480",
    "end": "556640"
  },
  {
    "text": "projects and another key component of the uh L house is actually the table",
    "start": "556640",
    "end": "562320"
  },
  {
    "text": "format uh table format forms an extraction layer above the storage uh",
    "start": "562320",
    "end": "568000"
  },
  {
    "text": "which provide reliability and great performance while processing huge data set including asset transaction cashing",
    "start": "568000",
    "end": "575800"
  },
  {
    "text": "aut partition time travel snop compaction",
    "start": "575800",
    "end": "580920"
  },
  {
    "text": "Etc and I think I will cover it in just a bit",
    "start": "580920",
    "end": "586760"
  },
  {
    "text": "so here is actually a high level um open Lakehouse architectures and is actually",
    "start": "586760",
    "end": "593640"
  },
  {
    "text": "comprise um multiple layers so starting from the bottom layer um you can can",
    "start": "593640",
    "end": "599640"
  },
  {
    "text": "have secal database Cloud object storage hdfs and more and above the data storage",
    "start": "599640",
    "end": "606800"
  },
  {
    "text": "is actually the open file format uh including pet Json AAL and as for the uh",
    "start": "606800",
    "end": "613839"
  },
  {
    "text": "table format there are several choices including apach Iceberg Delta Lake this",
    "start": "613839",
    "end": "620440"
  },
  {
    "text": "one and as the apach hoodi so uh different table format actually provide",
    "start": "620440",
    "end": "626360"
  },
  {
    "text": "different features and in our solution uh weuse use P Iceberg uh because we",
    "start": "626360",
    "end": "631600"
  },
  {
    "text": "believe it provide more features that suitable for ML usage and I will cover",
    "start": "631600",
    "end": "636720"
  },
  {
    "text": "in just in the next slid so and above the table format is the key component",
    "start": "636720",
    "end": "644800"
  },
  {
    "text": "for the lake housee is the processing engine and here we have the uh trino",
    "start": "644800",
    "end": "650079"
  },
  {
    "text": "Presto and Spark trino and prestol these two are the SQL engine and sparker is actually more for use uh for the U badge",
    "start": "650079",
    "end": "658240"
  },
  {
    "text": "processing and then on top of the processing engine you can have uh a",
    "start": "658240",
    "end": "663959"
  },
  {
    "text": "bunch of uh open source uh visualization tool or business in intelligence tool",
    "start": "663959",
    "end": "669720"
  },
  {
    "text": "they can connect to your um leg housee and do the query data analytics and uh",
    "start": "669720",
    "end": "676600"
  },
  {
    "text": "also other dashboard stuff so like I mentioned earlier we",
    "start": "676600",
    "end": "684839"
  },
  {
    "text": "choose in in the table format we choose the um Apache iceberg because it it's",
    "start": "684839",
    "end": "690560"
  },
  {
    "text": "actually designed for handling U metab scale data set and solving consistency",
    "start": "690560",
    "end": "696600"
  },
  {
    "text": "and performance issues when storing data on cloud storage and is actually initiated by the Netflix and they donate",
    "start": "696600",
    "end": "704240"
  },
  {
    "text": "to the op Source community and it use a series of meta data to check the data",
    "start": "704240",
    "end": "709760"
  },
  {
    "text": "change meanwhile maintain the performance of data assess so table change create a new stop which are link",
    "start": "709760",
    "end": "717760"
  },
  {
    "text": "with multiple uh metadata file and use the atomic switch to support atomicity",
    "start": "717760",
    "end": "724440"
  },
  {
    "text": "consistency isolation and durability aka the asset and which ensure the data",
    "start": "724440",
    "end": "730839"
  },
  {
    "text": "reliability and integrity even with multiple assets at once so it actually",
    "start": "730839",
    "end": "737079"
  },
  {
    "text": "also provide other features uh like uh time travel uh the ability to carry the",
    "start": "737079",
    "end": "742720"
  },
  {
    "text": "table based on it State at a certain point of history and also partion",
    "start": "742720",
    "end": "748560"
  },
  {
    "text": "evolution allow you to change the columns on which a table is partitioned",
    "start": "748560",
    "end": "754320"
  },
  {
    "text": "over time and also the schema Evolution you allow you to add rename reorder and",
    "start": "754320",
    "end": "761720"
  },
  {
    "text": "delete columns and as for the processing engine",
    "start": "761720",
    "end": "767480"
  },
  {
    "text": "we use the prestol prestol or some people call it prestol database but it's",
    "start": "767480",
    "end": "772639"
  },
  {
    "text": "actually a SQL engine um it's a open source distributed SQL engine uh they can carry large data set from various",
    "start": "772639",
    "end": "780000"
  },
  {
    "text": "data source um ranging from rational database no SQL real time data source",
    "start": "780000",
    "end": "786000"
  },
  {
    "text": "and even streaming data source as well prol connect to those data source using",
    "start": "786000",
    "end": "791440"
  },
  {
    "text": "um a flexible plug-in mechanism we call Connector and carry the data where it",
    "start": "791440",
    "end": "796760"
  },
  {
    "text": "resign with without moving the data around so prto is create to um for the",
    "start": "796760",
    "end": "803160"
  },
  {
    "text": "interactive and ad hoc data analytic at very beginning so it at Leverage The",
    "start": "803160",
    "end": "808760"
  },
  {
    "text": "distrib rute architecture in memory processing query optimization hierarchy caching and lot of features to perform",
    "start": "808760",
    "end": "816199"
  },
  {
    "text": "the query in the lowest latency whether you are dealing with pide of data or",
    "start": "816199",
    "end": "822440"
  },
  {
    "text": "realtime streaming analytics press to ensure that your query can run uh AO",
    "start": "822440",
    "end": "828000"
  },
  {
    "text": "latency enable you to make the inform the decision quickly first all run um",
    "start": "828000",
    "end": "833600"
  },
  {
    "text": "SLE queries because it's a SLE query so it comply with the NC uh SQL standard",
    "start": "833600",
    "end": "839720"
  },
  {
    "text": "so I think in the data analytic world most users already know how to write the",
    "start": "839720",
    "end": "845560"
  },
  {
    "text": "S query so p is easy to access so you don't have to actually learn a new",
    "start": "845560",
    "end": "851000"
  },
  {
    "text": "language uh last but not least the prol is an open source project U with a big",
    "start": "851000",
    "end": "856160"
  },
  {
    "text": "community and is used by many companies including uh the original company who uh",
    "start": "856160",
    "end": "863880"
  },
  {
    "text": "uh invented is the meta and the by Dan Uber IBN and more",
    "start": "863880",
    "end": "871040"
  },
  {
    "text": "so and for the very top layer um I mentioned earlier is the visualization and the bi tool um on this day because",
    "start": "871839",
    "end": "879880"
  },
  {
    "text": "the prto uh provide the jdbc and rest API so you can easily set up those bi",
    "start": "879880",
    "end": "885839"
  },
  {
    "text": "tool to connect to uh the prto cluster and to retrieve the data so prto",
    "start": "885839",
    "end": "891959"
  },
  {
    "text": "Community also provide uh Python and JavaScript xdk uh for integration so you",
    "start": "891959",
    "end": "897440"
  },
  {
    "text": "can easily integrate bi2 for um using lows um SDK so for us uh we try to set",
    "start": "897440",
    "end": "904279"
  },
  {
    "text": "up the apach uh ziplink uh is one of the uh data visualization tool it use the",
    "start": "904279",
    "end": "910480"
  },
  {
    "text": "jdbc connector uh to connect to the Press store and we also set up the uh",
    "start": "910480",
    "end": "916320"
  },
  {
    "text": "Jupiter notebook uh which is using the python uh SDK to connect to the Press",
    "start": "916320",
    "end": "921959"
  },
  {
    "text": "store and on the right hand side here is actually another uh visualization Tool",
    "start": "921959",
    "end": "927160"
  },
  {
    "text": "uh is aach um super set is you can also integrate it uh with the prto easily uh",
    "start": "927160",
    "end": "933759"
  },
  {
    "text": "but in our deployment we don't use it but I highly recommend that you can use it as",
    "start": "933759",
    "end": "940120"
  },
  {
    "text": "well so here is a complete topology that we set up uh for the data pre-processing",
    "start": "940120",
    "end": "946920"
  },
  {
    "text": "um you can see um we deploy everything from the kubernetes um of course you can",
    "start": "946920",
    "end": "952480"
  },
  {
    "text": "actually switch to open shift for sure and on top of it we have the me sorry we",
    "start": "952480",
    "end": "959160"
  },
  {
    "text": "have the minial uh cloud storage and we can we also have the mongod DB which is the",
    "start": "959160",
    "end": "965079"
  },
  {
    "text": "semi data semi uh structure data uh and my SQL is for the structure data and we",
    "start": "965079",
    "end": "971240"
  },
  {
    "text": "used the like I said earlier we used the iceberg table format and this have metadata is uh used by the iceberger to",
    "start": "971240",
    "end": "978600"
  },
  {
    "text": "store the metadata information and on the pr layer we actually Ed the personal",
    "start": "978600",
    "end": "983800"
  },
  {
    "text": "Helm CH to construct a three not cluster um one is the clust uh coordinator and",
    "start": "983800",
    "end": "990560"
  },
  {
    "text": "the other two other workers and on top of like I mentioned I used the apach",
    "start": "990560",
    "end": "996519"
  },
  {
    "text": "zipling to do the visualization and the jup notbook for data scientist they can operate um they can try out the prto",
    "start": "996519",
    "end": "1004560"
  },
  {
    "text": "seore and do their data analytic stuff on the JB notebook so I think I TR I",
    "start": "1004560",
    "end": "1011759"
  },
  {
    "text": "want to demo some of them but uh we we Tred to to see if we",
    "start": "1011759",
    "end": "1020199"
  },
  {
    "text": "can run the live demo but we found the network is not that um stable so we try",
    "start": "1020199",
    "end": "1027798"
  },
  {
    "text": "we we actually record the video earlier and then let me show you so um here is",
    "start": "1027799",
    "end": "1036199"
  },
  {
    "text": "the prestol cluster it actually has a tunal cluster like I mentioned earlier",
    "start": "1036199",
    "end": "1041480"
  },
  {
    "text": "and in the cluster uh in the pr of dashboard um actually I just recently",
    "start": "1041480",
    "end": "1046760"
  },
  {
    "text": "contribute the SQL client into the community so you can actually run the SQL query directly on the UI and in here",
    "start": "1046760",
    "end": "1054919"
  },
  {
    "text": "you can see we actually have connect to um different uh data source uh we have",
    "start": "1054919",
    "end": "1060360"
  },
  {
    "text": "the iceberg mongod DV myql and some Benchmark U data",
    "start": "1060360",
    "end": "1067039"
  },
  {
    "text": "base I want to yeah and like I said you can run the SLE query and here you can",
    "start": "1070960",
    "end": "1076760"
  },
  {
    "text": "see actually I just tried to run a Federated query is actually query from the myo DB and mongod DB and it join",
    "start": "1076760",
    "end": "1084559"
  },
  {
    "text": "those two uh table from two different data source and give you the result yeah",
    "start": "1084559",
    "end": "1090520"
  },
  {
    "text": "it's a very simple query and another thing I want to show you is the the ziplink visualization tool so it's very",
    "start": "1090520",
    "end": "1098159"
  },
  {
    "text": "easy to compose the visualization Tool uh the data visualization using the ziplink and you can see the SLE query",
    "start": "1098159",
    "end": "1105240"
  },
  {
    "text": "and you can run it and talk to the pror cluster which just uh construct so those",
    "start": "1105240",
    "end": "1110880"
  },
  {
    "text": "are the very fancy visualization tool that apach zipling for you can see the",
    "start": "1110880",
    "end": "1116640"
  },
  {
    "text": "SLE query uh we have and then you can just run it and you will give you the result very quickly and and plot the",
    "start": "1116640",
    "end": "1124039"
  },
  {
    "text": "chat so I think that's my part and I think I can also show you another part I",
    "start": "1124039",
    "end": "1129760"
  },
  {
    "text": "mentioned earlier is the um the first sequel is The jup Notebook",
    "start": "1129760",
    "end": "1138159"
  },
  {
    "text": "I mentioned uh where you go sorry give one",
    "start": "1138159",
    "end": "1144960"
  },
  {
    "text": "moment so yeah it's here",
    "start": "1144960",
    "end": "1153600"
  },
  {
    "text": "sorry yeah this is the a a notebook so in a notebook you can see we actually",
    "start": "1154240",
    "end": "1159640"
  },
  {
    "text": "use the the python SDK the prto python STK so it can easily help you to connect",
    "start": "1159640",
    "end": "1165240"
  },
  {
    "text": "to the prto um import Library connect Space by the server and you can run the",
    "start": "1165240",
    "end": "1171360"
  },
  {
    "text": "query directly in the notebook so yeah you can see it just",
    "start": "1171360",
    "end": "1176440"
  },
  {
    "text": "query the result back and here is the same query i i i issue on the Cil client",
    "start": "1176440",
    "end": "1181679"
  },
  {
    "text": "UI it just tried to do the fa query and then in here because I want to demo how",
    "start": "1181679",
    "end": "1188559"
  },
  {
    "text": "I can actually inject the data into the iceberg table so I try to clean up the table and in here you actually do the",
    "start": "1188559",
    "end": "1195600"
  },
  {
    "text": "ferary query and inject the data into the iceberger table so the last query is",
    "start": "1195600",
    "end": "1200640"
  },
  {
    "text": "I try to query those data directly from Iceberg and Iceberg on underline is",
    "start": "1200640",
    "end": "1206159"
  },
  {
    "text": "using the pocket file format so you can easily hand over to uh the data",
    "start": "1206159",
    "end": "1212880"
  },
  {
    "text": "scientist so okay so that's back to the chat so here are the information that",
    "start": "1212880",
    "end": "1219360"
  },
  {
    "text": "help you to um set up the environment that we set up uh starting from the pressol here are the Press repository",
    "start": "1219360",
    "end": "1226360"
  },
  {
    "text": "and information and the a lot of connectors that you can set up connect to and Presto hel chart that we use to",
    "start": "1226360",
    "end": "1233799"
  },
  {
    "text": "set up the Presto on the kuet cluster and also Apache Iceberg and we also have",
    "start": "1233799",
    "end": "1239039"
  },
  {
    "text": "two actually um work workshop lab that we can conduct to help uh user to learn",
    "start": "1239039",
    "end": "1245919"
  },
  {
    "text": "the Presto and also the Presto plus Iceberg so these are two Workshop that you can easily attend is free and public",
    "start": "1245919",
    "end": "1252320"
  },
  {
    "text": "available so and he has the step by step instructure to help you to learn how to",
    "start": "1252320",
    "end": "1257400"
  },
  {
    "text": "use the Press how to set up Presto and how to set up the Icebreaker with the Presto so and I think here is I finished",
    "start": "1257400",
    "end": "1263919"
  },
  {
    "text": "in my par so I will hand over to T he's going to share the solution for ML life",
    "start": "1263919",
    "end": "1270799"
  },
  {
    "text": "[Applause]",
    "start": "1271270",
    "end": "1277869"
  },
  {
    "text": "cycle everyone uh than uh for the data challenge uh we just talked about how",
    "start": "1277919",
    "end": "1285480"
  },
  {
    "text": "prto can be used to solve some of the data challenges um the other challenge",
    "start": "1285480",
    "end": "1292760"
  },
  {
    "text": "remaining is the automating the ml life cycles uh because after uh you turn data",
    "start": "1292760",
    "end": "1299240"
  },
  {
    "text": "into features typically the next step is model training and then you may have to do hyper paramet turning to optimize the",
    "start": "1299240",
    "end": "1307840"
  },
  {
    "text": "model uh after the model is trained you will deploy the model but model accuracy",
    "start": "1307840",
    "end": "1312960"
  },
  {
    "text": "may drop then you will need to start the whole process from the data again",
    "start": "1312960",
    "end": "1320240"
  },
  {
    "text": "um so the heart of the cube flow is the pipeline or it's called",
    "start": "1320240",
    "end": "1326039"
  },
  {
    "text": "kfp um we can use the pipeline to break the ml to break the ml cycle into",
    "start": "1326039",
    "end": "1332520"
  },
  {
    "text": "smaller tasks or boxes um the line and arrows determine",
    "start": "1332520",
    "end": "1338640"
  },
  {
    "text": "the order each task is processed like float chart diagram uh each of box may",
    "start": "1338640",
    "end": "1344320"
  },
  {
    "text": "run containerized tasks like data preprocessing or training or deployment",
    "start": "1344320",
    "end": "1350320"
  },
  {
    "text": "models some of the tasks can be handled by the pre-built components uh we can",
    "start": "1350320",
    "end": "1355720"
  },
  {
    "text": "use Python DSL or Jupiter notebook uh extension to compose pipelines and once",
    "start": "1355720",
    "end": "1362840"
  },
  {
    "text": "you have the pipeline you can schedule to run it",
    "start": "1362840",
    "end": "1367679"
  },
  {
    "text": "automatically um qf FL pipeline is a metadata driven pipeline internally uh",
    "start": "1368840",
    "end": "1374159"
  },
  {
    "text": "the pipeline uh back end stores roundtime information of a pipeline run in a manaa store runtime information",
    "start": "1374159",
    "end": "1380919"
  },
  {
    "text": "includes status uh of tasks availabil availability of artifact custom uh",
    "start": "1380919",
    "end": "1389000"
  },
  {
    "text": "properties associated with each execution or artifact uh Manata store",
    "start": "1389000",
    "end": "1395400"
  },
  {
    "text": "also enable pipeline step caching uh which means if arguments are exactly the same a task will be skied and the old",
    "start": "1395400",
    "end": "1403760"
  },
  {
    "text": "output is reused so let's uh look into the CU flow core component and their",
    "start": "1403760",
    "end": "1410400"
  },
  {
    "text": "usages um so first one uh is the cube flow notebook uh it provides webbased",
    "start": "1410400",
    "end": "1416960"
  },
  {
    "text": "development environment inside your Cube netes cluster uh it provides an interactive",
    "start": "1416960",
    "end": "1422679"
  },
  {
    "text": "environment uh for you to implement any task like data analysis training and de",
    "start": "1422679",
    "end": "1427760"
  },
  {
    "text": "and deployment tasks you may also import additional notebook image uh which provide other",
    "start": "1427760",
    "end": "1434320"
  },
  {
    "text": "functionalities like loc C or nood uh to compose pipeline using drag and drop",
    "start": "1434320",
    "end": "1441480"
  },
  {
    "text": "approach and for hyper permit tuning we use kti once you have written your ml",
    "start": "1441480",
    "end": "1446520"
  },
  {
    "text": "training code uh you can parameterize it ktip runs trails to figure out the best",
    "start": "1446520",
    "end": "1452240"
  },
  {
    "text": "parameters to run the learning to run the learning process um KP also has",
    "start": "1452240",
    "end": "1457440"
  },
  {
    "text": "python SDK to create KP experiment uh pipeline also has pre-built components to run K",
    "start": "1457440",
    "end": "1464399"
  },
  {
    "text": "experment uh so for for model training we use CU flow training oper it is used for f tuning and distributed",
    "start": "1464399",
    "end": "1471159"
  },
  {
    "text": "training of ml models uh it's support ml framework such as pytorch tensor flow",
    "start": "1471159",
    "end": "1476679"
  },
  {
    "text": "extri and and others it allows you to deploy ml training workflows using python SDK and custom",
    "start": "1476679",
    "end": "1483240"
  },
  {
    "text": "resources and uh last one uh for deploying models Cas serve is standard",
    "start": "1483240",
    "end": "1488919"
  },
  {
    "text": "model inference platform on cunes buil for highly scalable use cases it provides standard inference protocol",
    "start": "1488919",
    "end": "1495279"
  },
  {
    "text": "across ml Frameworks it offers production ml servy including prediction",
    "start": "1495279",
    "end": "1501279"
  },
  {
    "text": "preprocessing it can also you be used for monitoring drift bias and so",
    "start": "1501279",
    "end": "1507600"
  },
  {
    "text": "on so let's uh take a look a quick demo um which I have recorded",
    "start": "1507600",
    "end": "1513799"
  },
  {
    "text": "earlier um because maybe the network maybe small",
    "start": "1513799",
    "end": "1521000"
  },
  {
    "text": "slow okay so this is uh so let's see how we",
    "start": "1527320",
    "end": "1533240"
  },
  {
    "text": "can create a simple pipeline using C FL notebook so uh uh I'm using a notebook",
    "start": "1533240",
    "end": "1539080"
  },
  {
    "text": "with elra extension as mentioned before it offers NOCO or local approach of creating pipelines uh I have three",
    "start": "1539080",
    "end": "1545840"
  },
  {
    "text": "notebook which I already created one for data aggregation one for load and uh",
    "start": "1545840",
    "end": "1550960"
  },
  {
    "text": "training models and another one for serving I drag and drop the notebook into the canvas and then connect them uh",
    "start": "1550960",
    "end": "1558360"
  },
  {
    "text": "with lines and then so they can run in sequence and then I click the play",
    "start": "1558360",
    "end": "1565000"
  },
  {
    "text": "button uh to uh to execute the pipeline using CU flow uh Pipeline and",
    "start": "1565000",
    "end": "1574200"
  },
  {
    "text": "then and I can go to the dashboard I can go to the qf flow dashboard for to see",
    "start": "1574880",
    "end": "1582360"
  },
  {
    "text": "the uh to see the result oops",
    "start": "1582360",
    "end": "1590240"
  },
  {
    "text": "why is the zoom thing the video you lost",
    "start": "1604559",
    "end": "1612840"
  },
  {
    "text": "the okay yeah so once we execute the",
    "start": "1617240",
    "end": "1623200"
  },
  {
    "text": "pipeline we should be able to go through the dashboard and see the while it's running we can see the output of it like",
    "start": "1623200",
    "end": "1629799"
  },
  {
    "text": "the logs like events and metadata and so on and",
    "start": "1629799",
    "end": "1635080"
  },
  {
    "text": "uh once once the pipeline finished uh we",
    "start": "1635080",
    "end": "1640159"
  },
  {
    "text": "should be able to uh use uh case serve standard inference API to get the result",
    "start": "1640159",
    "end": "1646480"
  },
  {
    "text": "of the prediction",
    "start": "1646480",
    "end": "1650158"
  },
  {
    "text": "so let's so yeah that's it for the demo and",
    "start": "1652640",
    "end": "1658720"
  },
  {
    "text": "uh so we are we are using other uh we are uh so Q flow has many distributions",
    "start": "1658720",
    "end": "1663799"
  },
  {
    "text": "right we install the IB and Cloud Distribution on standard K kubernetes cluster with additional components like",
    "start": "1663799",
    "end": "1670240"
  },
  {
    "text": "kerve and elra extension uh moreover uh each CU flow distribution May pack",
    "start": "1670240",
    "end": "1677000"
  },
  {
    "text": "different components uh for example the open shift AI is based on the rad open shift uh rad open",
    "start": "1677000",
    "end": "1684080"
  },
  {
    "text": "data Hub distribution it offers additional open source component and SDK for running large language model worklow",
    "start": "1684080",
    "end": "1691279"
  },
  {
    "text": "it also supports a thirdparty data up service for running large queries the IBN Wasson x. a is built on top of the",
    "start": "1691279",
    "end": "1699240"
  },
  {
    "text": "open shift AI so this is it for our talk um so to summarize we we talked about",
    "start": "1699240",
    "end": "1705679"
  },
  {
    "text": "solution data challenge with open uh lak house using Presto and Iceberg we also",
    "start": "1705679",
    "end": "1710799"
  },
  {
    "text": "talked about solution uh to ml life cycle CH challenges with C flow uh we",
    "start": "1710799",
    "end": "1716200"
  },
  {
    "text": "demo you how to create a simple pipeline to automate these Solutions uh so thank you for attending and hope you",
    "start": "1716200",
    "end": "1724410"
  },
  {
    "text": "[Applause]",
    "start": "1724410",
    "end": "1731019"
  },
  {
    "text": "enjoyed we just got information we have five",
    "start": "1737039",
    "end": "1744200"
  },
  {
    "text": "five minutes for the questions so if you any question just but it's so bright I I",
    "start": "1744200",
    "end": "1750679"
  },
  {
    "text": "actually cannot",
    "start": "1750679",
    "end": "1753480"
  },
  {
    "text": "see yeah so yeah just like like we mentioned earlier those are the we",
    "start": "1755840",
    "end": "1761720"
  },
  {
    "text": "actually because we went through a lot of different open source projects so we",
    "start": "1761720",
    "end": "1766760"
  },
  {
    "text": "we we think it's actually really interesting to bring all of them together so from the data all the way to",
    "start": "1766760",
    "end": "1773720"
  },
  {
    "text": "the ml pipeline deployment so it's very important because most of time when we",
    "start": "1773720",
    "end": "1779039"
  },
  {
    "text": "um look at uh the ml solution it only contain for example for the data part",
    "start": "1779039",
    "end": "1784279"
  },
  {
    "text": "maybe you only contain the data for but for the ml part it only contain contains um the model training or maybe the",
    "start": "1784279",
    "end": "1791080"
  },
  {
    "text": "deployment only so but we we definitely need a solution from end to end starting from the data um pre-processing and",
    "start": "1791080",
    "end": "1798399"
  },
  {
    "text": "manipulate data and then hand over to the data scientist to do the model",
    "start": "1798399",
    "end": "1804279"
  },
  {
    "text": "training and partitioner to compose the model yeah so we think it's really good",
    "start": "1804279",
    "end": "1809799"
  },
  {
    "text": "yeah I saw it",
    "start": "1809799",
    "end": "1813278"
  },
  {
    "text": "yeah of thing have you have you thought of uh something like a feature store",
    "start": "1820519",
    "end": "1826760"
  },
  {
    "text": "where data is a special meaning in special use case because it is going to an ml model because it comes with its",
    "start": "1826760",
    "end": "1832960"
  },
  {
    "text": "own special training and serving needs yes I think is is a good question and then there's also the other reason we",
    "start": "1832960",
    "end": "1840440"
  },
  {
    "text": "actually pick up the Apache Iceberg as the table format because it actually support a lot of other future stores so",
    "start": "1840440",
    "end": "1848559"
  },
  {
    "text": "you can easily connect to other future stores that you choose then you can easily integrate uh into you our data",
    "start": "1848559",
    "end": "1856120"
  },
  {
    "text": "source so for example when you to the data PR processing and you can using um",
    "start": "1856120",
    "end": "1862679"
  },
  {
    "text": "like I like like in our scenario we have the MySQL we have the mango DB and data may be also from some S3 storage so you",
    "start": "1862679",
    "end": "1869880"
  },
  {
    "text": "can use the Presto to have this kind of ferity query and then you can dump into",
    "start": "1869880",
    "end": "1875279"
  },
  {
    "text": "the iceberg table format and then you can have your feature store um and talk",
    "start": "1875279",
    "end": "1881240"
  },
  {
    "text": "to the ice tform and retrieve the data",
    "start": "1881240",
    "end": "1886120"
  },
  {
    "text": "yeah",
    "start": "1886919",
    "end": "1889919"
  },
  {
    "text": "uh I I was wondering about your data Gathering was that like are they all like custom bespoke Solutions or were",
    "start": "1897039",
    "end": "1903600"
  },
  {
    "text": "you do you have some sort of like a flank ingestion engine or a spark like",
    "start": "1903600",
    "end": "1910120"
  },
  {
    "text": "ingestion engines that you run to gather your data um that's a good question um",
    "start": "1910120",
    "end": "1915399"
  },
  {
    "text": "but because as you know Ted and I we are a developer and we contribute to those open source projects so um we we",
    "start": "1915399",
    "end": "1922600"
  },
  {
    "text": "actually try to find some um good scenario and also data set but uh the",
    "start": "1922600",
    "end": "1929000"
  },
  {
    "text": "question you have is that do we have any scenario that do the data injection right but uh yeah we don't have that uh",
    "start": "1929000",
    "end": "1936679"
  },
  {
    "text": "because uh in our scenario we just try to compose a platform and provide other users to to to use it so if you have any",
    "start": "1936679",
    "end": "1944639"
  },
  {
    "text": "good uh data injection scenario definitely yeah yeah you welcome to share with us and we can try that out on",
    "start": "1944639",
    "end": "1951480"
  },
  {
    "text": "our platform yeah yeah so I think that's that's it so",
    "start": "1951480",
    "end": "1957960"
  },
  {
    "text": "thanks for attending again and yeah those are the information we share and we like you to try try it out and yeah",
    "start": "1957960",
    "end": "1964720"
  },
  {
    "text": "also maybe give us feedbacks thank you",
    "start": "1964720",
    "end": "1970519"
  }
]