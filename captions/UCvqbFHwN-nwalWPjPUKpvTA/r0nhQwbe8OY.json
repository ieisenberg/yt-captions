[
  {
    "text": "so my name is Brandon dim chef I usually look like that on the Internet I work for Oh lark and I'm going to talk",
    "start": "30",
    "end": "7859"
  },
  {
    "text": "to you a little bit about our migration from sort of puppet managers legacy",
    "start": "7859",
    "end": "13950"
  },
  {
    "text": "infrastructure to kubernetes we're not done with this yet but we've already",
    "start": "13950",
    "end": "20400"
  },
  {
    "text": "learned some important lessons I think that hopefully will be helpful to other people as well so this is pretty much",
    "start": "20400",
    "end": "29130"
  },
  {
    "text": "just a huge meeting so it's always good to have an agenda right so okay we're gonna talk a little bit why we decided",
    "start": "29130",
    "end": "35760"
  },
  {
    "text": "to embark on this kubernetes mission when I talk a little bit how how we did",
    "start": "35760",
    "end": "42690"
  },
  {
    "text": "it you know have a few Pro tips littered throughout and also some of my cats and",
    "start": "42690",
    "end": "49050"
  },
  {
    "text": "cats from the neighborhood because I feel like people could use caps today",
    "start": "49050",
    "end": "54530"
  },
  {
    "text": "anyhow so why well clearly because it's",
    "start": "54530",
    "end": "59699"
  },
  {
    "text": "100% certainty solve all of our infrastructure problems it's gonna be a silver bullet a panacea it will make the",
    "start": "59699",
    "end": "67260"
  },
  {
    "text": "world a better place etc no of course not but if you disagree with me I will",
    "start": "67260",
    "end": "75210"
  },
  {
    "text": "replace your DevOps with a small kubernetes cluster I mean also I just saved 15% by switching a q4 Nettie's",
    "start": "75210",
    "end": "82759"
  },
  {
    "text": "okay so let me just tell you a little",
    "start": "82759",
    "end": "87780"
  },
  {
    "text": "bit about what a lark is first so we have a basis to go on because we're a",
    "start": "87780",
    "end": "93930"
  },
  {
    "text": "live chat provider software-as-a-service live chat provider you copy a little bit of code onto your website and when you",
    "start": "93930",
    "end": "99960"
  },
  {
    "text": "log into our like admin chat panel and you can talk to visitors that show up on your website so and legacy is not like",
    "start": "99960",
    "end": "112229"
  },
  {
    "text": "we're not this like we were a start-up in 2009 YC so you know if you were",
    "start": "112229",
    "end": "119490"
  },
  {
    "text": "expecting like how to move your backs thing to kubernetes that's not gonna happen and honestly our",
    "start": "119490",
    "end": "127980"
  },
  {
    "text": "architecture fits kubernetes idea of the universe relatively well already like our previous architecture we're got some",
    "start": "127980",
    "end": "135870"
  },
  {
    "text": "micro-services and all those fun things got dozens of them some of them might be",
    "start": "135870",
    "end": "143730"
  },
  {
    "text": "a little more macro than micro they're mostly in Python but there's some some",
    "start": "143730",
    "end": "149460"
  },
  {
    "text": "Ruby node and Erlang here and there most of them are stateless we use rabbit and",
    "start": "149460",
    "end": "156510"
  },
  {
    "text": "thrift really heavily in our infrastructure and most of our state is in MySQL with some in Redis and memcache",
    "start": "156510",
    "end": "164520"
  },
  {
    "text": "and other various things here and there and there aren't a huge number of single",
    "start": "164520",
    "end": "169590"
  },
  {
    "text": "points of failure that we have and we're currently have hundreds of single",
    "start": "169590",
    "end": "177690"
  },
  {
    "text": "purpose VMs at Rackspace by single purpose I mean it's the VM that one runs one process basically yeah it doesn't",
    "start": "177690",
    "end": "184700"
  },
  {
    "text": "you know there's no we don't really have things that are like trying to manage",
    "start": "184700",
    "end": "190350"
  },
  {
    "text": "utilization or anything it's all managed with puppet and we have a few a few pets",
    "start": "190350",
    "end": "197790"
  },
  {
    "text": "that like if something bad happens to them these guys over here are gonna get paged and then run out of the room so",
    "start": "197790",
    "end": "206090"
  },
  {
    "text": "it's not perfect but it's sort of close to what kubernetes might expect from",
    "start": "206090",
    "end": "214820"
  },
  {
    "text": "services now but we have really really bad utilization right now I mean this is",
    "start": "214820",
    "end": "220410"
  },
  {
    "text": "like you know we have this one server that's using maybe a quarter of a CPU most of the time and maybe it's using a",
    "start": "220410",
    "end": "228209"
  },
  {
    "text": "little more memory it's just an example I pulled that immune in but thinking about trying to manually allocate",
    "start": "228209",
    "end": "234239"
  },
  {
    "text": "processes here and there to like put a CPU intensive process and a memory intensive process on the same machine",
    "start": "234239",
    "end": "240120"
  },
  {
    "text": "it's just like that's like that I keep",
    "start": "240120",
    "end": "245370"
  },
  {
    "text": "me up at night so we're burning like kind of a lot of money with bad utilization we're also",
    "start": "245370",
    "end": "252630"
  },
  {
    "text": "tired of babysitting these individual machines if if one of them gets whacked in the",
    "start": "252630",
    "end": "258609"
  },
  {
    "text": "middle of the night and pages someone there's still some manual intervention",
    "start": "258610",
    "end": "264610"
  },
  {
    "text": "we have to do and we want better velocity right now scaling takes us on",
    "start": "264610",
    "end": "270880"
  },
  {
    "text": "the order of hours to spin up new infrastructure if we are expecting more",
    "start": "270880",
    "end": "278560"
  },
  {
    "text": "customers or a higher traffic period of the year and stuff which is a sort of a",
    "start": "278560",
    "end": "283780"
  },
  {
    "text": "waste of time that we would like to avoid so as I said before we're",
    "start": "283780",
    "end": "292600"
  },
  {
    "text": "reasonably well suited to communities but are sort of our first attempt was a",
    "start": "292600",
    "end": "300070"
  },
  {
    "text": "little bit of the red pill I guess I said we're relatively well suited but we",
    "start": "300070",
    "end": "306970"
  },
  {
    "text": "ran into some problems trying to get our Redis production Redis cluster running",
    "start": "306970",
    "end": "312220"
  },
  {
    "text": "and kubernetes in the end we just decided to run it on bare metal because",
    "start": "312220",
    "end": "318460"
  },
  {
    "text": "it was just not it wasn't worth the fight right now I feel like probably by the early next year if someone's gonna",
    "start": "318460",
    "end": "325450"
  },
  {
    "text": "have a nice little helm chart that we're just gonna be able to install and then it'll solve all of our problems anyways",
    "start": "325450",
    "end": "330880"
  },
  {
    "text": "I'm just like whoever someone someone in the audience make our Redis work but in",
    "start": "330880",
    "end": "338230"
  },
  {
    "text": "the meantime basically we had because we needed to use Redis Sentinel and Pat's",
    "start": "338230",
    "end": "344290"
  },
  {
    "text": "pet stats are pretty new and we were trying as I trying to figure out how to get that all working correctly turned",
    "start": "344290",
    "end": "349840"
  },
  {
    "text": "out to be more effort than it was worth at this time for us to try to migrate stuff off so maybe later so that brings",
    "start": "349840",
    "end": "359830"
  },
  {
    "text": "me to my first tip let's go with the flow sometimes it's not necessarily",
    "start": "359830",
    "end": "366400"
  },
  {
    "text": "worth trying to get your application or a third-party application like all",
    "start": "366400",
    "end": "371860"
  },
  {
    "text": "worked out in order to get on the kubernetes it might be it might be a better idea to just like not drink the",
    "start": "371860",
    "end": "378820"
  },
  {
    "text": "buzzword kool-aid and decide that you can run it the old-fashioned way for a little while but",
    "start": "378820",
    "end": "388620"
  },
  {
    "text": "most of the time we can choose the blue pill because I said before work we think",
    "start": "389539",
    "end": "394949"
  },
  {
    "text": "we're pretty well super suited to kubernetes already so if you also can choose the blue pill and you've decided",
    "start": "394949",
    "end": "401069"
  },
  {
    "text": "to give your Nettie's let's talk a little bit about how you would have to",
    "start": "401069",
    "end": "406110"
  },
  {
    "text": "go about that the first thing that is sort of maybe obvious to everyone here",
    "start": "406110",
    "end": "411569"
  },
  {
    "text": "is that you have to containerize things it turns out it's really easy to containerize cats you just put a",
    "start": "411569",
    "end": "417330"
  },
  {
    "text": "container and then they go in it it's not quite that easy for other things but",
    "start": "417330",
    "end": "425689"
  },
  {
    "text": "that's sort of like a different talk and a different problem so I'm not going to talk about that anymore and actually for",
    "start": "425689",
    "end": "434279"
  },
  {
    "text": "us our step negative one was to apply a little bit of twelve factor I don't know",
    "start": "434279",
    "end": "439590"
  },
  {
    "text": "if everyone has seen this but take a look at 12 factor dotnet it's sort of",
    "start": "439590",
    "end": "444870"
  },
  {
    "text": "principles for building micro service applications that apply pretty well to",
    "start": "444870",
    "end": "449969"
  },
  {
    "text": "kubernetes stuff too and it made us it made it easier for us to move our stuff",
    "start": "449969",
    "end": "455460"
  },
  {
    "text": "to kubernetes once we had like environment variables configuring our applications and logs logging to",
    "start": "455460",
    "end": "461159"
  },
  {
    "text": "standard out and stuff like that that kubernetes expects so we did all that",
    "start": "461159",
    "end": "469259"
  },
  {
    "text": "stuff and in our first attempt was to sort of our first idea was to try to put",
    "start": "469259",
    "end": "475860"
  },
  {
    "text": "a full dev environment in kubernetes with like everything running there and I'm like oh maybe one day it'll be good",
    "start": "475860",
    "end": "482219"
  },
  {
    "text": "enough and we can just cut it over to production we sort of figured out that",
    "start": "482219",
    "end": "488099"
  },
  {
    "text": "that was a bad idea pretty quickly sort of because of the longtail issue this is",
    "start": "488099",
    "end": "494969"
  },
  {
    "text": "the closest photo of that I could get to this graph where we so we have a lot of",
    "start": "494969",
    "end": "505159"
  },
  {
    "text": "services and moving the first few is gonna get us a lot of benefit but sort",
    "start": "505159",
    "end": "511949"
  },
  {
    "text": "of further down the way there's a lot less",
    "start": "511949",
    "end": "516349"
  },
  {
    "text": "let's return on your investment so for us some who are in higher infrastructure over and then like",
    "start": "517399",
    "end": "522990"
  },
  {
    "text": "how the switch was not really realistic and this sort of brings me to to",
    "start": "522990",
    "end": "530130"
  },
  {
    "text": "summarize if you can't throw the switch until all of your infrastructure is migrated you may never finish and if you",
    "start": "530130",
    "end": "536820"
  },
  {
    "text": "do your stuff probably will break and like may and probably are not probably true like almost certainly if you've",
    "start": "536820",
    "end": "543420"
  },
  {
    "text": "just like spend a whole bunch of time deaf marching until you have something",
    "start": "543420",
    "end": "548460"
  },
  {
    "text": "and then cut over its pride it blow up so don't do that you can get a lot of",
    "start": "548460",
    "end": "555990"
  },
  {
    "text": "value just by like taking the first part of the I guess that's like the cat but part of it before it gets to the tail",
    "start": "555990",
    "end": "562140"
  },
  {
    "text": "but and we're sort of we're sort of like still here in this process so as I said",
    "start": "562140",
    "end": "571350"
  },
  {
    "text": "I'm still work in progress I don't know if we'll end up getting all the way down there we may end up deciding that you",
    "start": "571350",
    "end": "578340"
  },
  {
    "text": "know some of our legacy stuff will stay legacy but we'll see so that brings me",
    "start": "578340",
    "end": "583530"
  },
  {
    "text": "to another tip beware of the big migrate which i think is a cousin of the big",
    "start": "583530",
    "end": "589050"
  },
  {
    "text": "rewrite II if you if you feel like",
    "start": "589050",
    "end": "594090"
  },
  {
    "text": "you're going down this path step back think about whether you that's really the right strategy for your",
    "start": "594090",
    "end": "600090"
  },
  {
    "text": "infrastructure migration so we saw we came up with a strategy to avoid this",
    "start": "600090",
    "end": "605550"
  },
  {
    "text": "and the first thing was to move a less important the ancillary service",
    "start": "605550",
    "end": "612350"
  },
  {
    "text": "something we could work the kinks out on and to answer as many sort of unknown",
    "start": "612350",
    "end": "619050"
  },
  {
    "text": "unknowns that we could and then to move",
    "start": "619050",
    "end": "624780"
  },
  {
    "text": "like kind of the most risky service because if we do the hardest thing and",
    "start": "624780",
    "end": "630690"
  },
  {
    "text": "it works then everything else is is easier and it reduces the risk of like",
    "start": "630690",
    "end": "637880"
  },
  {
    "text": "alright we saved the most risky thing for the end and then we moved it and it didn't work and so we can't do this and",
    "start": "637880",
    "end": "644610"
  },
  {
    "text": "all that effort was wasted",
    "start": "644610",
    "end": "648290"
  },
  {
    "text": "I might answer your question in the next couple slides if not I will answer it later so and then once the so what's the",
    "start": "652580",
    "end": "665300"
  },
  {
    "text": "most risky services migrated we're gonna move the rest of our core infrastructure sort of outside in this sort of prevents",
    "start": "665300",
    "end": "674060"
  },
  {
    "text": "a lot of trips in and out of the kubernetes cluster what we think we need",
    "start": "674060",
    "end": "679280"
  },
  {
    "text": "to do is somewhat quickly because we're serving this intermediate state where there's like maybe additional failure",
    "start": "679280",
    "end": "684440"
  },
  {
    "text": "modes as a result of us being in two separate places with our networking",
    "start": "684440",
    "end": "690320"
  },
  {
    "text": "going across the VPN and then this is when we start working on the long tail",
    "start": "690320",
    "end": "696170"
  },
  {
    "text": "once that core stuff is done move some stuff that's less less likely to be",
    "start": "696170",
    "end": "702200"
  },
  {
    "text": "disruptive in case of failure and honestly there are some things that have been around for a long time and barely",
    "start": "702200",
    "end": "707330"
  },
  {
    "text": "do anything anymore and we could probably just kill them instead of migrating them and so this sort of like",
    "start": "707330",
    "end": "716390"
  },
  {
    "text": "nugget I took out of this was to try to change as few variables at a time as is reasonably possible so that you can sort",
    "start": "716390",
    "end": "723530"
  },
  {
    "text": "of like make sure you're progressing in the right direction as you move and you",
    "start": "723530",
    "end": "731420"
  },
  {
    "text": "might need to do extra work but just like having good tests and stuff like that it's probably probably worth it in",
    "start": "731420",
    "end": "737060"
  },
  {
    "text": "the end because you're more likely to succeed a little secret it's not really",
    "start": "737060",
    "end": "743089"
  },
  {
    "text": "specific to kubernetes I think like any sort of migration or a sort of large undertaking is best done in a piecewise",
    "start": "743089",
    "end": "751460"
  },
  {
    "text": "fashion that you can you can check as you're going along so we were looking",
    "start": "751460",
    "end": "760970"
  },
  {
    "text": "around for our less important service that didn't really matter that much so",
    "start": "760970",
    "end": "767440"
  },
  {
    "text": "naturally chose our billing system",
    "start": "767440",
    "end": "771490"
  },
  {
    "text": "it's not as crazy as it sounds it happens to be a little bandwidth unfortunately I mean you know it's like",
    "start": "772650",
    "end": "782650"
  },
  {
    "text": "a handful of requests per second or something it's like I mean I wish it was like we had like serious scaling issues",
    "start": "782650",
    "end": "789250"
  },
  {
    "text": "with our billing system but unfortunately not it also happens to be representative of the rest of our",
    "start": "789250",
    "end": "796290"
  },
  {
    "text": "infrastructure because it uses the same databases it uses thrift which we use",
    "start": "796290",
    "end": "801970"
  },
  {
    "text": "heavily elsewhere it uses rabbit that's what a rabbit might look like and these",
    "start": "801970",
    "end": "809290"
  },
  {
    "text": "are things that we use all throughout our infrastructure so getting those running in production on kubernetes is",
    "start": "809290",
    "end": "814660"
  },
  {
    "text": "like valuable in and of themselves because a lot of other things depend on them it's also relatively easy to roll",
    "start": "814660",
    "end": "822340"
  },
  {
    "text": "out and roll back and not really a lot of consequences for us to like cutting",
    "start": "822340",
    "end": "830050"
  },
  {
    "text": "back to the old one if it didn't work and it's it's actually really quite low",
    "start": "830050",
    "end": "837550"
  },
  {
    "text": "risk because stripe is still gonna charge you even if our feelings like so someone might not be able to sign up or upgrade a plan if it's down but all of",
    "start": "837550",
    "end": "845890"
  },
  {
    "text": "our existing customers and all of our revenue is still being processed by stripe so yeah and so it went pretty",
    "start": "845890",
    "end": "854590"
  },
  {
    "text": "smoothly it definitely forced us to work out some some kinks in how we're setting",
    "start": "854590",
    "end": "859900"
  },
  {
    "text": "up our kubernetes getting our VPN working between kubernetes and our old",
    "start": "859900",
    "end": "866380"
  },
  {
    "text": "infrastructure database connectivity and and like how we were gonna revision",
    "start": "866380",
    "end": "873220"
  },
  {
    "text": "control our our infrastructure going forward",
    "start": "873220",
    "end": "878190"
  },
  {
    "text": "so then we move on to the most risky service and you if you have a cat you",
    "start": "879240",
    "end": "888100"
  },
  {
    "text": "know why that's risky first a little bit of warning hmm",
    "start": "888100",
    "end": "895220"
  },
  {
    "text": "I did these graphics so they are not awesome so this is our most risky thing",
    "start": "895220",
    "end": "905000"
  },
  {
    "text": "it's our it's it's what serves our chat traffic from the visitors side it's 40%",
    "start": "905000",
    "end": "912769"
  },
  {
    "text": "of our infrastructure just for this one application from like a dollar or a",
    "start": "912769",
    "end": "918050"
  },
  {
    "text": "traffic standpoint or whatever like it it's pushing a couple gigabits a second",
    "start": "918050",
    "end": "924500"
  },
  {
    "text": "back and forth between the rest of our infrastructure and if it goes down then",
    "start": "924500",
    "end": "931399"
  },
  {
    "text": "our chat doesn't work at all so you know it's not super it's not like the payment",
    "start": "931399",
    "end": "936800"
  },
  {
    "text": "thing where things will still like mostly work if it's gone so but it's",
    "start": "936800",
    "end": "943250"
  },
  {
    "text": "it's still pretty it's is pretty well suited to kubernetes because there's",
    "start": "943250",
    "end": "948649"
  },
  {
    "text": "just stateless services that are running any user can hit any back-end and get",
    "start": "948649",
    "end": "955550"
  },
  {
    "text": "their chats routed properly it it uses Redis for storing a state which that's",
    "start": "955550",
    "end": "963139"
  },
  {
    "text": "why we were trying to like get rid of working at kubernetes but as I said we ended up deciding to just run that the",
    "start": "963139",
    "end": "969860"
  },
  {
    "text": "old-fashioned way and this is actually",
    "start": "969860",
    "end": "975709"
  },
  {
    "text": "still a work in progress we are I don't know maybe we have it running in dev or maybe gonna try running this in",
    "start": "975709",
    "end": "982309"
  },
  {
    "text": "production for some customers over the next couple weeks yeah but we did learn",
    "start": "982309",
    "end": "987879"
  },
  {
    "text": "some valuable lessons from this already",
    "start": "987879",
    "end": "992769"
  },
  {
    "text": "one of them is that trying to switch monitoring and hosting at the same time",
    "start": "993009",
    "end": "998809"
  },
  {
    "text": "we're really monitoring and any sort of change at the same time is a recipe for disaster",
    "start": "998809",
    "end": "1005040"
  },
  {
    "text": "so the tip is to perform baseline measurements of your system that you're",
    "start": "1005370",
    "end": "1012069"
  },
  {
    "text": "moving so that you know if you've broken something after you move it as part of",
    "start": "1012069",
    "end": "1019990"
  },
  {
    "text": "this we're trying to deprecate our our old monitor stuff that was sort of like home rolled",
    "start": "1019990",
    "end": "1026000"
  },
  {
    "text": "and so we we don't really wanna but we",
    "start": "1026000",
    "end": "1032938"
  },
  {
    "text": "can't like have just the new monitoring and the new kubernetes cluster and the",
    "start": "1032939",
    "end": "1038370"
  },
  {
    "text": "old monitoring the old communities cluster because the numbers aren't really like comparable to each other so what we decided to do to help us with",
    "start": "1038370",
    "end": "1045298"
  },
  {
    "text": "this is to use linker D and if you haven't heard of linker D it's like a little service mesh layer seven RPC load",
    "start": "1045299",
    "end": "1054990"
  },
  {
    "text": "balancer and it has a really good kubernetes support it gathers a bunch of",
    "start": "1054990",
    "end": "1061190"
  },
  {
    "text": "performance metrics just for free when you when you use it and so we decided to",
    "start": "1061190",
    "end": "1067080"
  },
  {
    "text": "use this both in kubernetes in our and in our legacy infrastructure so that as we migrate we can like make sure all of",
    "start": "1067080",
    "end": "1074010"
  },
  {
    "text": "those numbers are staying in the green and I know that at least one person from",
    "start": "1074010",
    "end": "1082740"
  },
  {
    "text": "buoyant is here so if you are interested in in talking to someone about link or Dee Williams around here somewhere so it",
    "start": "1082740",
    "end": "1093929"
  },
  {
    "text": "gives us these kind of metrics there's like you can't read that at all but there's like 99th percentile 95th",
    "start": "1093929",
    "end": "1102120"
  },
  {
    "text": "percentile 50th percentile max Layton sees request for second failure rate",
    "start": "1102120",
    "end": "1108570"
  },
  {
    "text": "stuff like that and it's all it exposes Prometheus endpoints as well as stuff for other monitoring things so it's easy",
    "start": "1108570",
    "end": "1116429"
  },
  {
    "text": "to aggregate these from like all of your link or the instances or whatever in whatever you're using so that you can",
    "start": "1116429",
    "end": "1122070"
  },
  {
    "text": "monitor it and then we were using this is that we have so like in our legacy",
    "start": "1122070",
    "end": "1128520"
  },
  {
    "text": "data center we have we have two linker",
    "start": "1128520",
    "end": "1134909"
  },
  {
    "text": "DS running there and then we have linker DS as a daemon set per node inside of",
    "start": "1134909",
    "end": "1140100"
  },
  {
    "text": "communities and each sir within each like datacenter or zone or whatever",
    "start": "1140100",
    "end": "1146909"
  },
  {
    "text": "they're they're talking to their local link or D and then their local link or D",
    "start": "1146909",
    "end": "1151950"
  },
  {
    "text": "knows how to route that request to the right place so as we move like if we move service three over to",
    "start": "1151950",
    "end": "1159240"
  },
  {
    "text": "Cuba Nettie's we're just going to update some config in the various linker DS to point it to the to the new one in",
    "start": "1159240",
    "end": "1166650"
  },
  {
    "text": "kubernetes instead of back at the old one and so instead of having to like push a configuration change to your",
    "start": "1166650",
    "end": "1172230"
  },
  {
    "text": "extra application to point at a different thing it's sort of like a load balancer right you can just like cut",
    "start": "1172230",
    "end": "1177750"
  },
  {
    "text": "over the load balancer the new one and rolling back is really easy to cut over back plus you get all those fun metrics",
    "start": "1177750",
    "end": "1183180"
  },
  {
    "text": "so definitely even if you're not like migrating anything I think like link",
    "start": "1183180",
    "end": "1189180"
  },
  {
    "text": "link or D is worth a look if you want to if you have like you know service",
    "start": "1189180",
    "end": "1195810"
  },
  {
    "text": "oriented architecture kind of stuff and you need or want better metrics without",
    "start": "1195810",
    "end": "1201420"
  },
  {
    "text": "having to instrument of applications yourself because you get you get that for free it also does retries and some",
    "start": "1201420",
    "end": "1208110"
  },
  {
    "text": "other cool things so um yeah so another",
    "start": "1208110",
    "end": "1214500"
  },
  {
    "text": "thing we learned is that kouhei's networking is weird just like him cuz he sits and syncs like that for some reason",
    "start": "1214500",
    "end": "1220350"
  },
  {
    "text": "but you can't just VPN in and out like a normal data center because of all the",
    "start": "1220350",
    "end": "1227340"
  },
  {
    "text": "stuff that coupe proxy does and this sort of IPS that aren't really",
    "start": "1227340",
    "end": "1232560"
  },
  {
    "text": "meaningful outside of the cluster and so we sort of went through a variety of",
    "start": "1232560",
    "end": "1240660"
  },
  {
    "text": "different ways of trying to make our legacy stuff be able to talk to Kuban",
    "start": "1240660",
    "end": "1246450"
  },
  {
    "text": "any services and our kubernetes stuff be able to talk back to our legacy stuff",
    "start": "1246450",
    "end": "1254450"
  },
  {
    "text": "and what we sort of ended up on is is is",
    "start": "1254450",
    "end": "1260640"
  },
  {
    "text": "doing some hackery with with services so that they're pointing to various places",
    "start": "1260640",
    "end": "1265830"
  },
  {
    "text": "that might not necessarily be within kubernetes you still get all the service discovery and stuff that services give",
    "start": "1265830",
    "end": "1274440"
  },
  {
    "text": "you and also to use link or D so that we can always just basically talk to link",
    "start": "1274440",
    "end": "1279750"
  },
  {
    "text": "or D all the time but because we have in",
    "start": "1279750",
    "end": "1285540"
  },
  {
    "text": "our old puppet stuff we have a list of where all the where all the services are pointed",
    "start": "1285540",
    "end": "1292570"
  },
  {
    "text": "anyways in our legacy infrastructure we can use that to generate services and endpoints that are that point that allow",
    "start": "1292570",
    "end": "1300940"
  },
  {
    "text": "kubernetes communicate back with our regular infrastructure yep sorry yeah so",
    "start": "1300940",
    "end": "1323920"
  },
  {
    "text": "in some cases our services in kubernetes are talking to back to services that are",
    "start": "1323920",
    "end": "1330430"
  },
  {
    "text": "running in our old infrastructure outside of kubernetes so the services",
    "start": "1330430",
    "end": "1335770"
  },
  {
    "text": "are just like we're just using how much pointers are like I mean it's like DNS kind of except like fancier like we can",
    "start": "1335770",
    "end": "1342010"
  },
  {
    "text": "use it as service discovery when we switch it all we like the service name stays the same everything is connecting",
    "start": "1342010",
    "end": "1347140"
  },
  {
    "text": "the same thing it's just getting kubernetes now instead of their external",
    "start": "1347140",
    "end": "1354310"
  },
  {
    "text": "name is like one thing that you can use to kind of do this - we constructed our",
    "start": "1354310",
    "end": "1359320"
  },
  {
    "text": "own endpoints like this and this like resolve thing gets processed through a",
    "start": "1359320",
    "end": "1366520"
  },
  {
    "text": "make file to resolve like our internal DNS not actually from DNS but from our",
    "start": "1366520",
    "end": "1373450"
  },
  {
    "text": "the stuff that actually produces our DNS records to begin with so that just like",
    "start": "1373450",
    "end": "1379090"
  },
  {
    "text": "it creates all these endpoints for some service manually and then other stuff",
    "start": "1379090",
    "end": "1385690"
  },
  {
    "text": "can talk to that and so this is come in really handy it allows us to divine all our services in kubernetes as we would",
    "start": "1385690",
    "end": "1392440"
  },
  {
    "text": "if they were actually running in kubernetes even though they are not so",
    "start": "1392440",
    "end": "1400870"
  },
  {
    "text": "yeah that worked risky thing it didn't bite me but I said that we had",
    "start": "1400870",
    "end": "1411810"
  },
  {
    "text": "infrastructure that was relatively well suited to get rid of these it turns out",
    "start": "1411810",
    "end": "1417160"
  },
  {
    "text": "on the other side of our on the operator side instead of the",
    "start": "1417160",
    "end": "1422360"
  },
  {
    "text": "the visitors to the website the operators this is not so well suited",
    "start": "1422360",
    "end": "1428450"
  },
  {
    "text": "because these things are stateful and this load balancer has to be sticky",
    "start": "1428450",
    "end": "1434860"
  },
  {
    "text": "because this is an XMPP connection between our backends to our jabber",
    "start": "1434860",
    "end": "1440390"
  },
  {
    "text": "server when all arcs started it was we didn't have our own client we just had",
    "start": "1440390",
    "end": "1446660"
  },
  {
    "text": "people use xampp their own XMPP client to connect to our service so like pigeon or a DM or whatever and",
    "start": "1446660",
    "end": "1454930"
  },
  {
    "text": "then we built our own XMPP client web-based XMPP client later but because",
    "start": "1454930",
    "end": "1461630"
  },
  {
    "text": "XMPP requires a persistent TCP connection and all that we still need to have some persistence between visitor",
    "start": "1461630",
    "end": "1470480"
  },
  {
    "text": "requests and the back-end that their XMPP connection is connected to and we",
    "start": "1470480",
    "end": "1477340"
  },
  {
    "text": "we could fix this but that's a much larger undertaking we think than getting",
    "start": "1477340",
    "end": "1483920"
  },
  {
    "text": "this stall just sort of maybe not do exactly what kubernetes wants it to do but at least for the time being we'll",
    "start": "1483920",
    "end": "1492410"
  },
  {
    "text": "serve our purposes so we ended up hacking service load balancer and some",
    "start": "1492410",
    "end": "1499700"
  },
  {
    "text": "termination grace periods on pods to get like kind of what we wanted to do and",
    "start": "1499700",
    "end": "1505760"
  },
  {
    "text": "we're not gonna actually set it to a day but like if this pod sticks around for a while people will eventually like sign",
    "start": "1505760",
    "end": "1512480"
  },
  {
    "text": "off from that thing and will not just",
    "start": "1512480",
    "end": "1518510"
  },
  {
    "text": "sort of you know after a day or something that'll be like six people still attached to it or something and",
    "start": "1518510",
    "end": "1523880"
  },
  {
    "text": "like they can just be reconnected and so",
    "start": "1523880",
    "end": "1529880"
  },
  {
    "text": "sort of what we figured out here is that you can gain a lot of flexibility by",
    "start": "1529880",
    "end": "1535100"
  },
  {
    "text": "deploying some custom pods or like hacking some things that are that are internal to kubernetes service load",
    "start": "1535100",
    "end": "1541730"
  },
  {
    "text": "balancer isn't actually internal to kubernetes but it's a contributed some minor modifications to do this and it it",
    "start": "1541730",
    "end": "1550310"
  },
  {
    "text": "works pretty well my recommendation is just to pick your battles wisely in this case",
    "start": "1550310",
    "end": "1556460"
  },
  {
    "text": "unlike the Redis case we didn't really have another good option for we did we",
    "start": "1556460",
    "end": "1562400"
  },
  {
    "text": "definitely didn't want to just run this the old-fashioned way because we're not",
    "start": "1562400",
    "end": "1567650"
  },
  {
    "text": "gonna be able to get any like auto-scaling benefit and deploying additional ones of these is relatively",
    "start": "1567650",
    "end": "1574400"
  },
  {
    "text": "common and so we don't want to you know leave all that behind just for a little",
    "start": "1574400",
    "end": "1581120"
  },
  {
    "text": "bit of a little bit of fight yeah so",
    "start": "1581120",
    "end": "1588740"
  },
  {
    "text": "that's pretty much what I have here really the takeaway is I think here are",
    "start": "1588740",
    "end": "1595870"
  },
  {
    "text": "you know control your change try not to do too much at one time now go with the",
    "start": "1595870",
    "end": "1601190"
  },
  {
    "text": "flow and you can have some baseline measurements occasionally you might need to do something a little crazy like a",
    "start": "1601190",
    "end": "1606980"
  },
  {
    "text": "little stateful thing and I like to put cats on my slides thank you alright I",
    "start": "1606980",
    "end": "1621710"
  },
  {
    "text": "think I have a couple minutes maybe do I have a couple minutes oh wait hang out I",
    "start": "1621710",
    "end": "1627230"
  },
  {
    "text": "have to do this I was told to do this several times hi this is John Tesh I",
    "start": "1627230",
    "end": "1636340"
  },
  {
    "text": "wanted to ask about that service load balancer you just mentioned a little",
    "start": "1636340",
    "end": "1641540"
  },
  {
    "text": "more elaboration on that oh sure so service load balancer is it's like a I",
    "start": "1641540",
    "end": "1648560"
  },
  {
    "text": "don't know example thing in the kubernetes contribute oh that is just H a proxy that is kubernetes aware so you",
    "start": "1648560",
    "end": "1656990"
  },
  {
    "text": "can use it for internal load balancing of your services and stuff like that",
    "start": "1656990",
    "end": "1662510"
  },
  {
    "text": "and you can use it for external ingress if you want it's not an ingress controller but like you can use it to it",
    "start": "1662510",
    "end": "1669530"
  },
  {
    "text": "to like route traffic from outside into your cluster and it has sticky sessions",
    "start": "1669530",
    "end": "1678020"
  },
  {
    "text": "support built into it where is information on this if you if",
    "start": "1678020",
    "end": "1683630"
  },
  {
    "text": "you just google service - load balancer kubernetes you'll definitely find it it's in the main community",
    "start": "1683630",
    "end": "1689240"
  },
  {
    "text": "rebo somewhere excellent anyone else",
    "start": "1689240",
    "end": "1698080"
  },
  {
    "text": "you said networking was especially tricky did you try multiple Network overlays or stick with one the whole",
    "start": "1699940",
    "end": "1706040"
  },
  {
    "text": "time what do you mean by multiple network like flannel versus calico versus we",
    "start": "1706040",
    "end": "1713120"
  },
  {
    "text": "didn't we didn't really do any of that we're using gke for our stuff than I",
    "start": "1713120",
    "end": "1718310"
  },
  {
    "text": "mean I don't know if you can like do that with inside of there but we were",
    "start": "1718310",
    "end": "1723710"
  },
  {
    "text": "just trying to so we have a VPN from Rackspace to like g c e using their like",
    "start": "1723710",
    "end": "1731690"
  },
  {
    "text": "software as a service load balancers on both ends but that just gives you access to the node IPS on the kubernetes ahead",
    "start": "1731690",
    "end": "1738440"
  },
  {
    "text": "right so like you still need to like get into the cluster somehow and then going",
    "start": "1738440",
    "end": "1745520"
  },
  {
    "text": "the other way around it's like if you are running your own kubernetes cluster you can add to cube dns you can add a",
    "start": "1745520",
    "end": "1753800"
  },
  {
    "text": "configuration for an upstream DNS provider that you would use your",
    "start": "1753800",
    "end": "1759020"
  },
  {
    "text": "internal DNS or something like that for that right but since we were using gke if you modify that pod and then upgrade",
    "start": "1759020",
    "end": "1767090"
  },
  {
    "text": "your cluster it will smash it and we didn't want the risk of us that",
    "start": "1767090",
    "end": "1772760"
  },
  {
    "text": "inadvertently happening at a bad time and like all of our kubernetes stuff not",
    "start": "1772760",
    "end": "1779600"
  },
  {
    "text": "knowing about our old stuff anymore so that's sort of how that all worked out",
    "start": "1779600",
    "end": "1785210"
  },
  {
    "text": "I think someone behind you you know pasa",
    "start": "1785210",
    "end": "1790270"
  },
  {
    "text": "so I know kubernetes services as external IPS you mentioned something like external name is that synonymous or",
    "start": "1790420",
    "end": "1797270"
  },
  {
    "text": "something different I think it's something different okay we didn't end",
    "start": "1797270",
    "end": "1802520"
  },
  {
    "text": "up using those and instead you clothes and stuff I think I think if you",
    "start": "1802520",
    "end": "1809130"
  },
  {
    "text": "google it there's like you can in a service you can give it a DNS name right",
    "start": "1809130",
    "end": "1814440"
  },
  {
    "text": "but because we didn't have our DNS working that wouldn't like our private",
    "start": "1814440",
    "end": "1820260"
  },
  {
    "text": "DNS does is not available inside of our kubernetes cluster so that wasn't that",
    "start": "1820260",
    "end": "1825660"
  },
  {
    "text": "wasn't really an option for us but if you have like public dns or if you have DNS accessible you can use external name",
    "start": "1825660",
    "end": "1830960"
  },
  {
    "text": "in the service to just point at some random DNS somewhere as like the service",
    "start": "1830960",
    "end": "1836550"
  },
  {
    "text": "but it yeah ok cool thank you",
    "start": "1836550",
    "end": "1845960"
  }
]