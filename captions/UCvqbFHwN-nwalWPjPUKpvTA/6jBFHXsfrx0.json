[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "text": "okay so",
    "start": "2080",
    "end": "3199"
  },
  {
    "text": "uh hello everyone uh welcome to our",
    "start": "3199",
    "end": "5839"
  },
  {
    "text": "presentation",
    "start": "5839",
    "end": "6960"
  },
  {
    "text": "my name is jacob pavlik and i am",
    "start": "6960",
    "end": "9120"
  },
  {
    "text": "director",
    "start": "9120",
    "end": "10240"
  },
  {
    "text": "of engineering at volterra and i am",
    "start": "10240",
    "end": "12240"
  },
  {
    "text": "responsible for",
    "start": "12240",
    "end": "13840"
  },
  {
    "text": "leading the sari team great thanks jakub",
    "start": "13840",
    "end": "17680"
  },
  {
    "start": "15000",
    "end": "58000"
  },
  {
    "text": "my name is sandeep pombra and i'm the",
    "start": "17680",
    "end": "19520"
  },
  {
    "text": "head of data science and i'm responsible",
    "start": "19520",
    "end": "21520"
  },
  {
    "text": "for doing all the machine learning data",
    "start": "21520",
    "end": "23199"
  },
  {
    "text": "science",
    "start": "23199",
    "end": "23920"
  },
  {
    "text": "and data modeling at uh",
    "start": "23920",
    "end": "26960"
  },
  {
    "text": "volterra okay so",
    "start": "26960",
    "end": "30480"
  },
  {
    "text": "today uh we are excited to uh",
    "start": "30480",
    "end": "33600"
  },
  {
    "text": "get our session to kubecon and we would",
    "start": "33600",
    "end": "35440"
  },
  {
    "text": "like to share with you",
    "start": "35440",
    "end": "37360"
  },
  {
    "text": "our story and journey how we get to the",
    "start": "37360",
    "end": "40480"
  },
  {
    "text": "scaling",
    "start": "40480",
    "end": "41200"
  },
  {
    "text": "to the million machine learning models",
    "start": "41200",
    "end": "44239"
  },
  {
    "text": "and how we are using kubernetes apache",
    "start": "44239",
    "end": "47120"
  },
  {
    "text": "spark and apache hero",
    "start": "47120",
    "end": "48559"
  },
  {
    "text": "to work with all those data",
    "start": "48559",
    "end": "51600"
  },
  {
    "text": "and what great features we are able to",
    "start": "51600",
    "end": "54480"
  },
  {
    "text": "get from them",
    "start": "54480",
    "end": "58079"
  },
  {
    "start": "58000",
    "end": "98000"
  },
  {
    "text": "so a little bit about agenda uh",
    "start": "58079",
    "end": "61520"
  },
  {
    "text": "first of all uh i will take you through",
    "start": "61520",
    "end": "64320"
  },
  {
    "text": "quick overview of what is volterra",
    "start": "64320",
    "end": "66080"
  },
  {
    "text": "because uh",
    "start": "66080",
    "end": "67280"
  },
  {
    "text": "not everybody probably is familiar what",
    "start": "67280",
    "end": "69920"
  },
  {
    "text": "we are doing",
    "start": "69920",
    "end": "71200"
  },
  {
    "text": "then sandeep will explain our machine",
    "start": "71200",
    "end": "74320"
  },
  {
    "text": "learning functions",
    "start": "74320",
    "end": "75439"
  },
  {
    "text": "and our model explosion",
    "start": "75439",
    "end": "78880"
  },
  {
    "text": "uh and the problems what we were facing",
    "start": "78880",
    "end": "82240"
  },
  {
    "text": "then uh i will take you through machine",
    "start": "82240",
    "end": "85439"
  },
  {
    "text": "learning infrastructure evolution",
    "start": "85439",
    "end": "87040"
  },
  {
    "text": "journey",
    "start": "87040",
    "end": "87680"
  },
  {
    "text": "and how we continuously improve",
    "start": "87680",
    "end": "90880"
  },
  {
    "text": "our large infrastructure and then sunday",
    "start": "90880",
    "end": "94479"
  },
  {
    "text": "will take you to all model scaling",
    "start": "94479",
    "end": "96960"
  },
  {
    "text": "challenges",
    "start": "96960",
    "end": "98880"
  },
  {
    "start": "98000",
    "end": "177000"
  },
  {
    "text": "now uh to begin with i uh pick up this",
    "start": "98880",
    "end": "101840"
  },
  {
    "text": "slide",
    "start": "101840",
    "end": "102320"
  },
  {
    "text": "uh don't worry it is not uh so much",
    "start": "102320",
    "end": "104560"
  },
  {
    "text": "vendor stuff",
    "start": "104560",
    "end": "105840"
  },
  {
    "text": "uh but i wanted to show you what is",
    "start": "105840",
    "end": "108159"
  },
  {
    "text": "basically behind",
    "start": "108159",
    "end": "109520"
  },
  {
    "text": "so we at the volterra we built uh",
    "start": "109520",
    "end": "112640"
  },
  {
    "text": "distributed cloud services",
    "start": "112640",
    "end": "114479"
  },
  {
    "text": "wherever your apps and data need them so",
    "start": "114479",
    "end": "117520"
  },
  {
    "text": "it can be",
    "start": "117520",
    "end": "118399"
  },
  {
    "text": "public cloud it can be private cloud",
    "start": "118399",
    "end": "121600"
  },
  {
    "text": "physical edge nomadic edge or our",
    "start": "121600",
    "end": "124719"
  },
  {
    "text": "global backbone we focus on providing",
    "start": "124719",
    "end": "129200"
  },
  {
    "text": "distributed network services and",
    "start": "129200",
    "end": "132400"
  },
  {
    "text": "distributed infrastructure for the",
    "start": "132400",
    "end": "134720"
  },
  {
    "text": "application",
    "start": "134720",
    "end": "136000"
  },
  {
    "text": "so this is how looks our normal slide",
    "start": "136000",
    "end": "139599"
  },
  {
    "text": "but from the our engineering side and",
    "start": "139599",
    "end": "142160"
  },
  {
    "text": "where you can see",
    "start": "142160",
    "end": "143280"
  },
  {
    "text": "the behind and what is running so for",
    "start": "143280",
    "end": "146560"
  },
  {
    "text": "us it is basically kubernetes everywhere",
    "start": "146560",
    "end": "150800"
  },
  {
    "text": "and all those locations and all those",
    "start": "150800",
    "end": "152720"
  },
  {
    "text": "sites means um",
    "start": "152720",
    "end": "154080"
  },
  {
    "text": "kubernetes site the kubernetes sites and",
    "start": "154080",
    "end": "157280"
  },
  {
    "text": "it also means a lot of locks and metrics",
    "start": "157280",
    "end": "160959"
  },
  {
    "text": "and data",
    "start": "160959",
    "end": "161920"
  },
  {
    "text": "which we are able to pull from",
    "start": "161920",
    "end": "165440"
  },
  {
    "text": "and then analyze them and provide",
    "start": "165440",
    "end": "168720"
  },
  {
    "text": "a great features for our customers and",
    "start": "168720",
    "end": "171920"
  },
  {
    "text": "also for us",
    "start": "171920",
    "end": "173200"
  },
  {
    "text": "to be able to operate",
    "start": "173200",
    "end": "176959"
  },
  {
    "start": "177000",
    "end": "288000"
  },
  {
    "text": "okay thanks jakub so i'm going to talk a",
    "start": "179760",
    "end": "182159"
  },
  {
    "text": "little bit about our",
    "start": "182159",
    "end": "183200"
  },
  {
    "text": "machine learning applications as jakub",
    "start": "183200",
    "end": "185519"
  },
  {
    "text": "described you know we have a very",
    "start": "185519",
    "end": "186800"
  },
  {
    "text": "complex",
    "start": "186800",
    "end": "187760"
  },
  {
    "text": "distributed microservices multi-cloud",
    "start": "187760",
    "end": "190480"
  },
  {
    "text": "environment",
    "start": "190480",
    "end": "191519"
  },
  {
    "text": "and this uh entails a lot of different",
    "start": "191519",
    "end": "193840"
  },
  {
    "text": "machine learning functions",
    "start": "193840",
    "end": "195440"
  },
  {
    "text": "first of all we need to provide a very",
    "start": "195440",
    "end": "197680"
  },
  {
    "text": "sophisticated",
    "start": "197680",
    "end": "198879"
  },
  {
    "text": "web application firewall function and",
    "start": "198879",
    "end": "201599"
  },
  {
    "text": "typical rule-based maps",
    "start": "201599",
    "end": "203599"
  },
  {
    "text": "are not sophisticated enough to handle",
    "start": "203599",
    "end": "206000"
  },
  {
    "text": "zero",
    "start": "206000",
    "end": "206879"
  },
  {
    "text": "uh zero-day attacks um so we need to",
    "start": "206879",
    "end": "209280"
  },
  {
    "text": "provide",
    "start": "209280",
    "end": "210799"
  },
  {
    "text": "the most uh you know sophisticated",
    "start": "210799",
    "end": "212879"
  },
  {
    "text": "machine learning algorithms that allow",
    "start": "212879",
    "end": "214560"
  },
  {
    "text": "us to",
    "start": "214560",
    "end": "215040"
  },
  {
    "text": "handle that uh besides that's an",
    "start": "215040",
    "end": "217120"
  },
  {
    "text": "important part of an application for us",
    "start": "217120",
    "end": "219280"
  },
  {
    "text": "is understanding how the apis within the",
    "start": "219280",
    "end": "222000"
  },
  {
    "text": "application are working",
    "start": "222000",
    "end": "223440"
  },
  {
    "text": "uh so we use machine learning to to",
    "start": "223440",
    "end": "225840"
  },
  {
    "text": "discover apis",
    "start": "225840",
    "end": "227280"
  },
  {
    "text": "and basically uh compress the whole",
    "start": "227280",
    "end": "230400"
  },
  {
    "text": "you know application into a bunch of",
    "start": "230400",
    "end": "232879"
  },
  {
    "text": "different api endpoints",
    "start": "232879",
    "end": "234959"
  },
  {
    "text": "then we do a lot of different types of",
    "start": "234959",
    "end": "237120"
  },
  {
    "text": "anomaly detection",
    "start": "237120",
    "end": "238400"
  },
  {
    "text": "to again detect bot attacks and",
    "start": "238400",
    "end": "240560"
  },
  {
    "text": "different types of",
    "start": "240560",
    "end": "241519"
  },
  {
    "text": "ddos attacks uh we do time series",
    "start": "241519",
    "end": "243760"
  },
  {
    "text": "anomaly detection",
    "start": "243760",
    "end": "245120"
  },
  {
    "text": "and we also do per request anomaly",
    "start": "245120",
    "end": "246879"
  },
  {
    "text": "detection and we also do",
    "start": "246879",
    "end": "248640"
  },
  {
    "text": "user behavior analysis to understand if",
    "start": "248640",
    "end": "250879"
  },
  {
    "text": "there's any malicious users as well as",
    "start": "250879",
    "end": "252480"
  },
  {
    "text": "to understand the application better",
    "start": "252480",
    "end": "254720"
  },
  {
    "text": "so as you can see from this picture uh",
    "start": "254720",
    "end": "256959"
  },
  {
    "text": "there is a lot of different types of",
    "start": "256959",
    "end": "258239"
  },
  {
    "text": "machine learning functions they are",
    "start": "258239",
    "end": "259359"
  },
  {
    "text": "divided between",
    "start": "259359",
    "end": "260799"
  },
  {
    "text": "the learning core which is the training",
    "start": "260799",
    "end": "262880"
  },
  {
    "text": "and um",
    "start": "262880",
    "end": "264080"
  },
  {
    "text": "and the inference engines which run on",
    "start": "264080",
    "end": "265600"
  },
  {
    "text": "the edge and",
    "start": "265600",
    "end": "267360"
  },
  {
    "text": "the learning core actually is a global",
    "start": "267360",
    "end": "269840"
  },
  {
    "text": "learning area where we",
    "start": "269840",
    "end": "271759"
  },
  {
    "text": "do all our training and as you will see",
    "start": "271759",
    "end": "274720"
  },
  {
    "text": "in the",
    "start": "274720",
    "end": "275199"
  },
  {
    "text": "next few slides that to do this we have",
    "start": "275199",
    "end": "278240"
  },
  {
    "text": "to",
    "start": "278240",
    "end": "280160"
  },
  {
    "text": "really manage a very massive scale so",
    "start": "280160",
    "end": "282800"
  },
  {
    "text": "we'll",
    "start": "282800",
    "end": "283280"
  },
  {
    "text": "talk about this more as we go forward",
    "start": "283280",
    "end": "287280"
  },
  {
    "start": "288000",
    "end": "438000"
  },
  {
    "text": "okay so before we get to",
    "start": "289600",
    "end": "293759"
  },
  {
    "text": "external models let me explain you",
    "start": "293759",
    "end": "297680"
  },
  {
    "text": "uh the scale and how we collect metrics",
    "start": "297680",
    "end": "301199"
  },
  {
    "text": "and",
    "start": "301199",
    "end": "301600"
  },
  {
    "text": "locks in our infrastructure so we have",
    "start": "301600",
    "end": "304800"
  },
  {
    "text": "uh three type of sites we have a",
    "start": "304800",
    "end": "307120"
  },
  {
    "text": "customer edges",
    "start": "307120",
    "end": "308720"
  },
  {
    "text": "which are available in thousands",
    "start": "308720",
    "end": "312080"
  },
  {
    "text": "right so there can be thousands and",
    "start": "312080",
    "end": "314639"
  },
  {
    "text": "hundreds of thousands of the sites",
    "start": "314639",
    "end": "317280"
  },
  {
    "text": "then we have something what we call",
    "start": "317280",
    "end": "319360"
  },
  {
    "text": "regional edges",
    "start": "319360",
    "end": "320479"
  },
  {
    "text": "which are our point of presence and our",
    "start": "320479",
    "end": "322479"
  },
  {
    "text": "global backbone those are tense today",
    "start": "322479",
    "end": "325440"
  },
  {
    "text": "and then we have a global controller",
    "start": "325440",
    "end": "328639"
  },
  {
    "text": "uh which is today distributed across",
    "start": "328639",
    "end": "330960"
  },
  {
    "text": "three regions",
    "start": "330960",
    "end": "332639"
  },
  {
    "text": "and this is the place where we are doing",
    "start": "332639",
    "end": "334960"
  },
  {
    "text": "uh",
    "start": "334960",
    "end": "336400"
  },
  {
    "text": "data analysis so the way how it works is",
    "start": "336400",
    "end": "339520"
  },
  {
    "text": "that we have a prometheus in",
    "start": "339520",
    "end": "341360"
  },
  {
    "text": "each of every site which scrapes",
    "start": "341360",
    "end": "344800"
  },
  {
    "text": "local kubernetes workload as well as",
    "start": "344800",
    "end": "348160"
  },
  {
    "text": "nodes",
    "start": "348160",
    "end": "349199"
  },
  {
    "text": "and then we are doing from the connected",
    "start": "349199",
    "end": "352560"
  },
  {
    "text": "regional edges uh prometheus federation",
    "start": "352560",
    "end": "355759"
  },
  {
    "text": "with the metrics white listings and we",
    "start": "355759",
    "end": "358400"
  },
  {
    "text": "are scraping only certain metrics which",
    "start": "358400",
    "end": "360560"
  },
  {
    "text": "we are particularly interested in and",
    "start": "360560",
    "end": "362319"
  },
  {
    "text": "which we",
    "start": "362319",
    "end": "363360"
  },
  {
    "text": "want to work with so not basically",
    "start": "363360",
    "end": "365600"
  },
  {
    "text": "everything",
    "start": "365600",
    "end": "367120"
  },
  {
    "text": "and we use a remote write which gets",
    "start": "367120",
    "end": "370560"
  },
  {
    "text": "right data into cortex as our",
    "start": "370560",
    "end": "373840"
  },
  {
    "text": "long-term storage for all those kind of",
    "start": "373840",
    "end": "377039"
  },
  {
    "text": "metrics",
    "start": "377039",
    "end": "379280"
  },
  {
    "text": "original edge prometheus this prometai",
    "start": "379280",
    "end": "381759"
  },
  {
    "text": "also",
    "start": "381759",
    "end": "382880"
  },
  {
    "text": "send alerts and produce alerts to to our",
    "start": "382880",
    "end": "385680"
  },
  {
    "text": "uh alert manager",
    "start": "385680",
    "end": "387360"
  },
  {
    "text": "on the lock side uh today we are using",
    "start": "387360",
    "end": "390639"
  },
  {
    "text": "fluent bit",
    "start": "390639",
    "end": "391919"
  },
  {
    "text": "who captures all the lock messages from",
    "start": "391919",
    "end": "395919"
  },
  {
    "text": "our services",
    "start": "395919",
    "end": "396880"
  },
  {
    "text": "and from the third party services they",
    "start": "396880",
    "end": "399280"
  },
  {
    "text": "are forwarded to",
    "start": "399280",
    "end": "401199"
  },
  {
    "text": "uh aggregation fluency and fluent",
    "start": "401199",
    "end": "404479"
  },
  {
    "text": "bits demons in the",
    "start": "404479",
    "end": "407680"
  },
  {
    "text": "pops and original edges and from there",
    "start": "407680",
    "end": "410400"
  },
  {
    "text": "we",
    "start": "410400",
    "end": "410720"
  },
  {
    "text": "write into two places today so we write",
    "start": "410720",
    "end": "413599"
  },
  {
    "text": "into",
    "start": "413599",
    "end": "414240"
  },
  {
    "text": "a elastic search and we also write into",
    "start": "414240",
    "end": "419280"
  },
  {
    "text": "aws ss3 and",
    "start": "419280",
    "end": "422479"
  },
  {
    "text": "those apis are then available for",
    "start": "422479",
    "end": "425599"
  },
  {
    "text": "our uh metric data analysis",
    "start": "425599",
    "end": "428720"
  },
  {
    "text": "uh even data analysis and other services",
    "start": "428720",
    "end": "431840"
  },
  {
    "text": "which sunday will be",
    "start": "431840",
    "end": "434319"
  },
  {
    "text": "explaining on the future slides",
    "start": "434319",
    "end": "439440"
  },
  {
    "start": "438000",
    "end": "548000"
  },
  {
    "text": "yeah so as yakub just uh",
    "start": "440880",
    "end": "444160"
  },
  {
    "text": "showed that we have a very complex",
    "start": "444160",
    "end": "446479"
  },
  {
    "text": "architecture with a lot of different",
    "start": "446479",
    "end": "447919"
  },
  {
    "text": "types of data ingestion",
    "start": "447919",
    "end": "449680"
  },
  {
    "text": "and we also have when we are deploying",
    "start": "449680",
    "end": "451680"
  },
  {
    "text": "applications we are typically deploying",
    "start": "451680",
    "end": "453199"
  },
  {
    "text": "very complex applications",
    "start": "453199",
    "end": "454960"
  },
  {
    "text": "and these applications uh uh basically",
    "start": "454960",
    "end": "458400"
  },
  {
    "text": "under",
    "start": "458400",
    "end": "458800"
  },
  {
    "text": "under underlying these applications are",
    "start": "458800",
    "end": "461039"
  },
  {
    "text": "a lot of different dimensions",
    "start": "461039",
    "end": "462800"
  },
  {
    "text": "um basically we have application virtual",
    "start": "462800",
    "end": "464879"
  },
  {
    "text": "hosts source sites destination sites",
    "start": "464879",
    "end": "467680"
  },
  {
    "text": "and one of the things we found when",
    "start": "467680",
    "end": "469199"
  },
  {
    "text": "we're doing all our machine learning",
    "start": "469199",
    "end": "470400"
  },
  {
    "text": "modeling",
    "start": "470400",
    "end": "470960"
  },
  {
    "text": "is that every application every customer",
    "start": "470960",
    "end": "473680"
  },
  {
    "text": "every",
    "start": "473680",
    "end": "474479"
  },
  {
    "text": "geography has its own characteristics so",
    "start": "474479",
    "end": "477360"
  },
  {
    "text": "it's very difficult",
    "start": "477360",
    "end": "478319"
  },
  {
    "text": "to develop a universal model for all of",
    "start": "478319",
    "end": "481759"
  },
  {
    "text": "these",
    "start": "481759",
    "end": "482400"
  },
  {
    "text": "uh so basically to get the best",
    "start": "482400",
    "end": "484240"
  },
  {
    "text": "performance in terms of our machine",
    "start": "484240",
    "end": "485680"
  },
  {
    "text": "learning accuracy",
    "start": "485680",
    "end": "486960"
  },
  {
    "text": "we have to develop models for each",
    "start": "486960",
    "end": "489199"
  },
  {
    "text": "across each of the dimension",
    "start": "489199",
    "end": "490879"
  },
  {
    "text": "and as you can see uh basically doing",
    "start": "490879",
    "end": "492639"
  },
  {
    "text": "that kind of a combination",
    "start": "492639",
    "end": "494720"
  },
  {
    "text": "just by a multiplicative logic leads to",
    "start": "494720",
    "end": "497680"
  },
  {
    "text": "a very large cardinality of models",
    "start": "497680",
    "end": "499680"
  },
  {
    "text": "and in our case we were looking at",
    "start": "499680",
    "end": "503120"
  },
  {
    "text": "in certain cases some of the models for",
    "start": "503120",
    "end": "505599"
  },
  {
    "text": "example for the time",
    "start": "505599",
    "end": "506560"
  },
  {
    "text": "series we're getting into millions of",
    "start": "506560",
    "end": "508319"
  },
  {
    "text": "time series um so basically we need to",
    "start": "508319",
    "end": "511280"
  },
  {
    "text": "figure out a way to scale these models",
    "start": "511280",
    "end": "513919"
  },
  {
    "text": "um so initially when we started this",
    "start": "513919",
    "end": "515360"
  },
  {
    "text": "project",
    "start": "515360",
    "end": "516159"
  },
  {
    "text": "you know obviously we were trying to get",
    "start": "516159",
    "end": "517360"
  },
  {
    "text": "the machine learning and our algorithms",
    "start": "517360",
    "end": "518719"
  },
  {
    "text": "working",
    "start": "518719",
    "end": "519599"
  },
  {
    "text": "so we were running these models in a",
    "start": "519599",
    "end": "520880"
  },
  {
    "text": "single instance in a serialized manner",
    "start": "520880",
    "end": "523279"
  },
  {
    "text": "and what that did was basically it took",
    "start": "523279",
    "end": "525839"
  },
  {
    "text": "a very long time to run",
    "start": "525839",
    "end": "527440"
  },
  {
    "text": "and uh obviously you know when we are",
    "start": "527440",
    "end": "529360"
  },
  {
    "text": "training and doing inference and scoring",
    "start": "529360",
    "end": "531519"
  },
  {
    "text": "based on these models",
    "start": "531519",
    "end": "532800"
  },
  {
    "text": "if the model takes several hours to run",
    "start": "532800",
    "end": "534800"
  },
  {
    "text": "and the the model itself becomes",
    "start": "534800",
    "end": "536880"
  },
  {
    "text": "obsolete",
    "start": "536880",
    "end": "537760"
  },
  {
    "text": "and definitely it's not something that",
    "start": "537760",
    "end": "539200"
  },
  {
    "text": "was sustainable uh so",
    "start": "539200",
    "end": "541200"
  },
  {
    "text": "um now yakub is going to talk about a",
    "start": "541200",
    "end": "543040"
  },
  {
    "text": "little bit about how our infrastructure",
    "start": "543040",
    "end": "544399"
  },
  {
    "text": "was also",
    "start": "544399",
    "end": "545440"
  },
  {
    "text": "struggling to meet this need",
    "start": "545440",
    "end": "549600"
  },
  {
    "start": "548000",
    "end": "646000"
  },
  {
    "text": "so when we started",
    "start": "549600",
    "end": "553360"
  },
  {
    "text": "we actually and this is the",
    "start": "553360",
    "end": "555200"
  },
  {
    "text": "infrastructure picture",
    "start": "555200",
    "end": "557600"
  },
  {
    "text": "which is looking now on the global",
    "start": "557600",
    "end": "559760"
  },
  {
    "text": "controller as i",
    "start": "559760",
    "end": "560880"
  },
  {
    "text": "introduced in the previous slide so",
    "start": "560880",
    "end": "564160"
  },
  {
    "text": "we started with the more regions but",
    "start": "564160",
    "end": "567279"
  },
  {
    "text": "this picture cover one of the region",
    "start": "567279",
    "end": "569760"
  },
  {
    "text": "where",
    "start": "569760",
    "end": "570560"
  },
  {
    "text": "we provided elasticsearch",
    "start": "570560",
    "end": "574000"
  },
  {
    "text": "cortex and aws free api and we run",
    "start": "574000",
    "end": "578080"
  },
  {
    "text": "everything as an eks",
    "start": "578080",
    "end": "580160"
  },
  {
    "text": "cluster in inside of the eks cluster",
    "start": "580160",
    "end": "584160"
  },
  {
    "text": "and it was basically single cluster",
    "start": "584160",
    "end": "587360"
  },
  {
    "text": "running continuous jobs",
    "start": "587360",
    "end": "591519"
  },
  {
    "text": "and the issue was that parallelism",
    "start": "591519",
    "end": "595120"
  },
  {
    "text": "and also inefficient",
    "start": "595120",
    "end": "599839"
  },
  {
    "text": "cpu ram usage and",
    "start": "599839",
    "end": "605760"
  },
  {
    "text": "we the issue was that we had to resize",
    "start": "605760",
    "end": "608240"
  },
  {
    "text": "to bigger and bigger",
    "start": "608240",
    "end": "610959"
  },
  {
    "text": "vms and flavors and",
    "start": "610959",
    "end": "615040"
  },
  {
    "text": "it was not quite efficient because some",
    "start": "615040",
    "end": "618240"
  },
  {
    "text": "because those jobs running few hours and",
    "start": "618240",
    "end": "620399"
  },
  {
    "text": "then then they don't run",
    "start": "620399",
    "end": "621920"
  },
  {
    "text": "many hours so you need to find kind of",
    "start": "621920",
    "end": "624720"
  },
  {
    "text": "balance so",
    "start": "624720",
    "end": "625600"
  },
  {
    "text": "it was not really cost efficient and",
    "start": "625600",
    "end": "629120"
  },
  {
    "text": "resources efficient",
    "start": "629120",
    "end": "631120"
  },
  {
    "text": "and therefore",
    "start": "631120",
    "end": "634240"
  },
  {
    "text": "we wanted to find a different way or",
    "start": "634240",
    "end": "636959"
  },
  {
    "text": "take a look on the different perspective",
    "start": "636959",
    "end": "638800"
  },
  {
    "text": "how we can handle this",
    "start": "638800",
    "end": "640160"
  },
  {
    "text": "very large data says ingestion",
    "start": "640160",
    "end": "643279"
  },
  {
    "text": "and the training",
    "start": "643279",
    "end": "647839"
  },
  {
    "start": "646000",
    "end": "715000"
  },
  {
    "text": "yeah so um so as yakub mentioned you",
    "start": "648880",
    "end": "651680"
  },
  {
    "text": "know we were",
    "start": "651680",
    "end": "652640"
  },
  {
    "text": "coming running into a lot of bottlenecks",
    "start": "652640",
    "end": "654320"
  },
  {
    "text": "in our infrastructure",
    "start": "654320",
    "end": "655760"
  },
  {
    "text": "so obviously you know uh from a machine",
    "start": "655760",
    "end": "658079"
  },
  {
    "text": "learning model",
    "start": "658079",
    "end": "659279"
  },
  {
    "text": "scaling the obvious approach is to run",
    "start": "659279",
    "end": "662000"
  },
  {
    "text": "several models in parallel",
    "start": "662000",
    "end": "663760"
  },
  {
    "text": "and you know this can be done in various",
    "start": "663760",
    "end": "665519"
  },
  {
    "text": "ways you know we could use das",
    "start": "665519",
    "end": "667519"
  },
  {
    "text": "or job labor a variety of python because",
    "start": "667519",
    "end": "670079"
  },
  {
    "text": "most of our code was in python",
    "start": "670079",
    "end": "671920"
  },
  {
    "text": "but we wanted something which was much",
    "start": "671920",
    "end": "674000"
  },
  {
    "text": "more easier to maintain",
    "start": "674000",
    "end": "675680"
  },
  {
    "text": "easier to scale easier to manage uh so",
    "start": "675680",
    "end": "678720"
  },
  {
    "text": "we wanted to combine",
    "start": "678720",
    "end": "680320"
  },
  {
    "text": "uh the best of scaling horizontal",
    "start": "680320",
    "end": "683120"
  },
  {
    "text": "scaling",
    "start": "683120",
    "end": "684079"
  },
  {
    "text": "as well as having the ability to auto",
    "start": "684079",
    "end": "687279"
  },
  {
    "text": "scale",
    "start": "687279",
    "end": "688240"
  },
  {
    "text": "scale to different customers do",
    "start": "688240",
    "end": "690320"
  },
  {
    "text": "automation as",
    "start": "690320",
    "end": "691279"
  },
  {
    "text": "yakub mentioned with ci cd and minimize",
    "start": "691279",
    "end": "693760"
  },
  {
    "text": "our infrastructure management",
    "start": "693760",
    "end": "695519"
  },
  {
    "text": "and we also wanted to be able to have a",
    "start": "695519",
    "end": "698079"
  },
  {
    "text": "very",
    "start": "698079",
    "end": "698880"
  },
  {
    "text": "universal way of our data ingestion so",
    "start": "698880",
    "end": "701440"
  },
  {
    "text": "that we could easily do secure",
    "start": "701440",
    "end": "703279"
  },
  {
    "text": "and seamless data ingestion uh so for",
    "start": "703279",
    "end": "705680"
  },
  {
    "text": "all these reasons",
    "start": "705680",
    "end": "706959"
  },
  {
    "text": "uh we decided that we wanted to um",
    "start": "706959",
    "end": "711360"
  },
  {
    "text": "i think it's not allowing me to go to",
    "start": "712480",
    "end": "715360"
  },
  {
    "start": "715000",
    "end": "753000"
  },
  {
    "text": "the",
    "start": "715360",
    "end": "715920"
  },
  {
    "text": "next slide okay there it is sorry about",
    "start": "715920",
    "end": "719839"
  },
  {
    "text": "that",
    "start": "719839",
    "end": "720240"
  },
  {
    "text": "um yeah so basically to do this we",
    "start": "720240",
    "end": "722160"
  },
  {
    "text": "wanted to combine the best of",
    "start": "722160",
    "end": "723760"
  },
  {
    "text": "uh spark scaling and parallelization and",
    "start": "723760",
    "end": "726720"
  },
  {
    "text": "kubernetes infrastructure",
    "start": "726720",
    "end": "728240"
  },
  {
    "text": "so we use spark uh basically as a",
    "start": "728240",
    "end": "730880"
  },
  {
    "text": "horizontal scaling",
    "start": "730880",
    "end": "732000"
  },
  {
    "text": "and uh kubernetes as a distributed data",
    "start": "732000",
    "end": "734560"
  },
  {
    "text": "ingestion architecture with uh",
    "start": "734560",
    "end": "736240"
  },
  {
    "text": "integrated ci cd um so uh we wanted to",
    "start": "736240",
    "end": "739680"
  },
  {
    "text": "have a faster time to market uh so",
    "start": "739680",
    "end": "741760"
  },
  {
    "text": "rather than creating our own",
    "start": "741760",
    "end": "743440"
  },
  {
    "text": "open source spark engine we decided to",
    "start": "743440",
    "end": "746160"
  },
  {
    "text": "leverage",
    "start": "746160",
    "end": "747040"
  },
  {
    "text": "the sas technology from databricks",
    "start": "747040",
    "end": "751279"
  },
  {
    "start": "753000",
    "end": "953000"
  },
  {
    "text": "yes so when vista",
    "start": "756000",
    "end": "759440"
  },
  {
    "text": "let me talk about a little bit about how",
    "start": "759440",
    "end": "762000"
  },
  {
    "text": "we actually integrated data breaks",
    "start": "762000",
    "end": "765040"
  },
  {
    "text": "and uh what we had to do",
    "start": "765040",
    "end": "768560"
  },
  {
    "text": "so if you look at the",
    "start": "768560",
    "end": "771760"
  },
  {
    "text": "standard databricks uh integration",
    "start": "771760",
    "end": "774800"
  },
  {
    "text": "basically they calculate that you will",
    "start": "774800",
    "end": "777760"
  },
  {
    "text": "give them",
    "start": "777760",
    "end": "778320"
  },
  {
    "text": "access to the full uh aws vpc",
    "start": "778320",
    "end": "782320"
  },
  {
    "text": "and they can provision the vms and jobs",
    "start": "782320",
    "end": "785360"
  },
  {
    "text": "as they need and you do aws",
    "start": "785360",
    "end": "789120"
  },
  {
    "text": "peering with your uh vpc where you have",
    "start": "789120",
    "end": "793200"
  },
  {
    "text": "your data in our case our global control",
    "start": "793200",
    "end": "795360"
  },
  {
    "text": "with cortex",
    "start": "795360",
    "end": "796399"
  },
  {
    "text": "uh prometheus and elasticsearch api so",
    "start": "796399",
    "end": "799600"
  },
  {
    "text": "the pro",
    "start": "799600",
    "end": "800639"
  },
  {
    "text": "the problem what we find here was that",
    "start": "800639",
    "end": "804160"
  },
  {
    "text": "we didn't want to give them access to",
    "start": "804160",
    "end": "807600"
  },
  {
    "text": "uh our vpc so we created",
    "start": "807600",
    "end": "810800"
  },
  {
    "text": "dedicated instance uh dedicated",
    "start": "810800",
    "end": "814079"
  },
  {
    "text": "account but we still didn't want to use",
    "start": "814079",
    "end": "816800"
  },
  {
    "text": "just the p ring",
    "start": "816800",
    "end": "817920"
  },
  {
    "text": "because uh we wanted to have a",
    "start": "817920",
    "end": "820240"
  },
  {
    "text": "visibility on",
    "start": "820240",
    "end": "821839"
  },
  {
    "text": "what is flowing set detail firewall",
    "start": "821839",
    "end": "824079"
  },
  {
    "text": "rules",
    "start": "824079",
    "end": "825360"
  },
  {
    "text": "and uh make sure that uh it cannot get",
    "start": "825360",
    "end": "828720"
  },
  {
    "text": "breached and they cannot",
    "start": "828720",
    "end": "830800"
  },
  {
    "text": "access our core infrastructure",
    "start": "830800",
    "end": "834160"
  },
  {
    "text": "uh and they don't they don't use our",
    "start": "834160",
    "end": "838320"
  },
  {
    "text": "cas and our certificate so we wanted to",
    "start": "838320",
    "end": "840720"
  },
  {
    "text": "relay",
    "start": "840720",
    "end": "841680"
  },
  {
    "text": "isolate them and vpc peering was not",
    "start": "841680",
    "end": "844639"
  },
  {
    "text": "good enough",
    "start": "844639",
    "end": "845760"
  },
  {
    "text": "so we come up with the idea that we can",
    "start": "845760",
    "end": "848560"
  },
  {
    "text": "actually",
    "start": "848560",
    "end": "849279"
  },
  {
    "text": "leverage our own technology and",
    "start": "849279",
    "end": "852320"
  },
  {
    "text": "make it better and therefore",
    "start": "852320",
    "end": "855440"
  },
  {
    "text": "we worked with this design so",
    "start": "855440",
    "end": "858800"
  },
  {
    "text": "we took we let run our eks",
    "start": "858800",
    "end": "862480"
  },
  {
    "text": "and our existing vpc as is",
    "start": "862480",
    "end": "865760"
  },
  {
    "text": "and we just create a dedicated account",
    "start": "865760",
    "end": "869199"
  },
  {
    "text": "uh for data breaks learning jobs",
    "start": "869199",
    "end": "872880"
  },
  {
    "text": "and then we launched our",
    "start": "872880",
    "end": "876880"
  },
  {
    "text": "ingress egress gateway",
    "start": "876880",
    "end": "881040"
  },
  {
    "text": "uh called voltmesh which basically allow",
    "start": "881040",
    "end": "883920"
  },
  {
    "text": "us to",
    "start": "883920",
    "end": "885360"
  },
  {
    "text": "get connectivity for only a particular",
    "start": "885360",
    "end": "888079"
  },
  {
    "text": "api",
    "start": "888079",
    "end": "889839"
  },
  {
    "text": "which we need in this case it's a",
    "start": "889839",
    "end": "892560"
  },
  {
    "text": "prometheus",
    "start": "892560",
    "end": "894639"
  },
  {
    "text": "cortex and elasticsearch and we just",
    "start": "894639",
    "end": "897839"
  },
  {
    "text": "advertise",
    "start": "897839",
    "end": "898720"
  },
  {
    "text": "only those apis with uh",
    "start": "898720",
    "end": "901839"
  },
  {
    "text": "different certificates and different uh",
    "start": "901839",
    "end": "904079"
  },
  {
    "text": "authority",
    "start": "904079",
    "end": "905519"
  },
  {
    "text": "for the data breaks which allow us to",
    "start": "905519",
    "end": "908560"
  },
  {
    "text": "really provide granular api",
    "start": "908560",
    "end": "913040"
  },
  {
    "text": "filtering and service policies and",
    "start": "913040",
    "end": "916639"
  },
  {
    "text": "allows to volterral learning services",
    "start": "916639",
    "end": "920480"
  },
  {
    "text": "such as api discovery time series",
    "start": "920480",
    "end": "923360"
  },
  {
    "text": "anomaly detection",
    "start": "923360",
    "end": "924560"
  },
  {
    "text": "prerequisite detection request data",
    "start": "924560",
    "end": "927760"
  },
  {
    "text": "analysis or user behavioral analysis",
    "start": "927760",
    "end": "929920"
  },
  {
    "text": "to run and consume and produce metrics",
    "start": "929920",
    "end": "932880"
  },
  {
    "text": "backs to",
    "start": "932880",
    "end": "934000"
  },
  {
    "text": "our infrastructure without any",
    "start": "934000",
    "end": "938480"
  },
  {
    "text": "security breaching or direct access to",
    "start": "939519",
    "end": "942959"
  },
  {
    "text": "core volterra services",
    "start": "942959",
    "end": "946160"
  },
  {
    "text": "and of course this help us also to",
    "start": "946160",
    "end": "948800"
  },
  {
    "text": "improve our own",
    "start": "948800",
    "end": "951920"
  },
  {
    "text": "technology",
    "start": "952839",
    "end": "955839"
  },
  {
    "text": "okay great thanks jacob so um now i",
    "start": "955839",
    "end": "958079"
  },
  {
    "text": "think the rest of the",
    "start": "958079",
    "end": "959199"
  },
  {
    "text": "talk i will focus on how we use spark to",
    "start": "959199",
    "end": "961920"
  },
  {
    "text": "paralyze",
    "start": "961920",
    "end": "962720"
  },
  {
    "text": "our models and uh basically um",
    "start": "962720",
    "end": "966160"
  },
  {
    "text": "sorry i can go to the previous slide",
    "start": "966160",
    "end": "967680"
  },
  {
    "text": "yeah thank you um",
    "start": "967680",
    "end": "969279"
  },
  {
    "text": "so the way spark works is you know spark",
    "start": "969279",
    "end": "971519"
  },
  {
    "text": "relies very much on running the",
    "start": "971519",
    "end": "972800"
  },
  {
    "text": "functions on the driver itself",
    "start": "972800",
    "end": "974880"
  },
  {
    "text": "and then uh running a bunch of executors",
    "start": "974880",
    "end": "977920"
  },
  {
    "text": "in parallel and typically that can be",
    "start": "977920",
    "end": "980720"
  },
  {
    "text": "done",
    "start": "980720",
    "end": "981199"
  },
  {
    "text": "by creating either data frames or rdds",
    "start": "981199",
    "end": "984639"
  },
  {
    "text": "which are resilient distributed data",
    "start": "984639",
    "end": "986160"
  },
  {
    "text": "sets",
    "start": "986160",
    "end": "986880"
  },
  {
    "text": "uh which are collection of data frames",
    "start": "986880",
    "end": "989440"
  },
  {
    "text": "or data",
    "start": "989440",
    "end": "990160"
  },
  {
    "text": "modules that run on different executors",
    "start": "990160",
    "end": "992320"
  },
  {
    "text": "so the idea is basically",
    "start": "992320",
    "end": "994240"
  },
  {
    "text": "um you know you take some huge data",
    "start": "994240",
    "end": "997839"
  },
  {
    "text": "and then you split it into different",
    "start": "997839",
    "end": "999920"
  },
  {
    "text": "executors and actually",
    "start": "999920",
    "end": "1001360"
  },
  {
    "text": "if your executors are multi-core you can",
    "start": "1001360",
    "end": "1003759"
  },
  {
    "text": "even go and split them into multi-core",
    "start": "1003759",
    "end": "1005600"
  },
  {
    "text": "so for example if we have four executors",
    "start": "1005600",
    "end": "1008160"
  },
  {
    "text": "uh with four cores so we could paralyze",
    "start": "1008160",
    "end": "1010240"
  },
  {
    "text": "by a factor of 16.",
    "start": "1010240",
    "end": "1011839"
  },
  {
    "text": "uh so the first approach we took uh was",
    "start": "1011839",
    "end": "1014320"
  },
  {
    "text": "for the kinds of scaling where basically",
    "start": "1014320",
    "end": "1016560"
  },
  {
    "text": "we were going to ingest the data",
    "start": "1016560",
    "end": "1018639"
  },
  {
    "text": "and we were going to also extend the",
    "start": "1018639",
    "end": "1021360"
  },
  {
    "text": "models",
    "start": "1021360",
    "end": "1022399"
  },
  {
    "text": "into the various interfaces that yakub",
    "start": "1022399",
    "end": "1024959"
  },
  {
    "text": "talked about",
    "start": "1024959",
    "end": "1025918"
  },
  {
    "text": "and for that uh we came up with a very",
    "start": "1025919",
    "end": "1027678"
  },
  {
    "text": "simple scheme where basically",
    "start": "1027679",
    "end": "1029438"
  },
  {
    "text": "we took our dimensions like applications",
    "start": "1029439",
    "end": "1031918"
  },
  {
    "text": "namespaces",
    "start": "1031919",
    "end": "1033280"
  },
  {
    "text": "and accreted uh uh what we call is a you",
    "start": "1033280",
    "end": "1036959"
  },
  {
    "text": "know a panda's data frame out of it",
    "start": "1036959",
    "end": "1039199"
  },
  {
    "text": "and uh then we did uh you converted that",
    "start": "1039199",
    "end": "1042558"
  },
  {
    "text": "data frame into",
    "start": "1042559",
    "end": "1043678"
  },
  {
    "text": "an rdd and then uh basically what we can",
    "start": "1043679",
    "end": "1046720"
  },
  {
    "text": "do",
    "start": "1046720",
    "end": "1047199"
  },
  {
    "text": "is do a map which is basically applying",
    "start": "1047199",
    "end": "1049679"
  },
  {
    "text": "any function",
    "start": "1049679",
    "end": "1053840"
  },
  {
    "text": "this allows us to run a function",
    "start": "1056559",
    "end": "1058799"
  },
  {
    "text": "obviously the input output of this is",
    "start": "1058799",
    "end": "1060720"
  },
  {
    "text": "more symbolic where basically we want to",
    "start": "1060720",
    "end": "1062880"
  },
  {
    "text": "make sure that",
    "start": "1062880",
    "end": "1064840"
  },
  {
    "text": "your function is executed uh",
    "start": "1064840",
    "end": "1067840"
  },
  {
    "text": "but the actual uh core functionality uh",
    "start": "1067840",
    "end": "1070640"
  },
  {
    "text": "can be very complicated",
    "start": "1070640",
    "end": "1072000"
  },
  {
    "text": "um so if you look at this um quotes and",
    "start": "1072000",
    "end": "1074640"
  },
  {
    "text": "snippet",
    "start": "1074640",
    "end": "1075440"
  },
  {
    "text": "uh i can basically explain a little bit",
    "start": "1075440",
    "end": "1077600"
  },
  {
    "text": "further how we did this",
    "start": "1077600",
    "end": "1079520"
  },
  {
    "text": "so basically uh spark has um two kinds",
    "start": "1079520",
    "end": "1081840"
  },
  {
    "text": "of operations there is the",
    "start": "1081840",
    "end": "1082960"
  },
  {
    "text": "transformations like the map",
    "start": "1082960",
    "end": "1084960"
  },
  {
    "text": "and which is apply of a function and",
    "start": "1084960",
    "end": "1086799"
  },
  {
    "text": "then there is basically actions which",
    "start": "1086799",
    "end": "1088720"
  },
  {
    "text": "actually execute the thing",
    "start": "1088720",
    "end": "1090160"
  },
  {
    "text": "uh like collect uh or count and things",
    "start": "1090160",
    "end": "1092720"
  },
  {
    "text": "like that or",
    "start": "1092720",
    "end": "1093360"
  },
  {
    "text": "other kinds of aggregations so in this",
    "start": "1093360",
    "end": "1095600"
  },
  {
    "text": "case you can see",
    "start": "1095600",
    "end": "1097200"
  },
  {
    "text": "if we define this outer function",
    "start": "1097200",
    "end": "1100559"
  },
  {
    "text": "which is basically a standard python",
    "start": "1100559",
    "end": "1102240"
  },
  {
    "text": "function we get",
    "start": "1102240",
    "end": "1104000"
  },
  {
    "text": "our pandas frame which has all the keys",
    "start": "1104000",
    "end": "1107520"
  },
  {
    "text": "that we're going to use for mapping uh",
    "start": "1107520",
    "end": "1109840"
  },
  {
    "text": "we create",
    "start": "1109840",
    "end": "1110559"
  },
  {
    "text": "a spark data frame uh based on the panda",
    "start": "1110559",
    "end": "1114480"
  },
  {
    "text": "frame and uh then basically we make sure",
    "start": "1114480",
    "end": "1118000"
  },
  {
    "text": "that",
    "start": "1118000",
    "end": "1118640"
  },
  {
    "text": "we have the the",
    "start": "1118640",
    "end": "1122000"
  },
  {
    "text": "actual uh mapping function which is",
    "start": "1122000",
    "end": "1124480"
  },
  {
    "text": "embedded within this function so we can",
    "start": "1124480",
    "end": "1126640"
  },
  {
    "text": "use any of these variables like variable",
    "start": "1126640",
    "end": "1128559"
  },
  {
    "text": "one variable two inside this function",
    "start": "1128559",
    "end": "1130559"
  },
  {
    "text": "and these will be automatically uh",
    "start": "1130559",
    "end": "1132320"
  },
  {
    "text": "exported uh to each of the",
    "start": "1132320",
    "end": "1135679"
  },
  {
    "text": "and uh so pretty much this function",
    "start": "1135679",
    "end": "1139039"
  },
  {
    "text": "is uh we we take the data frame we map",
    "start": "1139039",
    "end": "1141840"
  },
  {
    "text": "we convert into rdd",
    "start": "1141840",
    "end": "1143280"
  },
  {
    "text": "and we map this function uh this",
    "start": "1143280",
    "end": "1146400"
  },
  {
    "text": "therefore it applies this function to",
    "start": "1146400",
    "end": "1147919"
  },
  {
    "text": "every row of this",
    "start": "1147919",
    "end": "1149600"
  },
  {
    "text": "rdd which is the original part of frame",
    "start": "1149600",
    "end": "1152080"
  },
  {
    "text": "and",
    "start": "1152080",
    "end": "1153200"
  },
  {
    "text": "and then we do a collection to make sure",
    "start": "1153200",
    "end": "1154799"
  },
  {
    "text": "that this function is actually executed",
    "start": "1154799",
    "end": "1156320"
  },
  {
    "text": "because",
    "start": "1156320",
    "end": "1156799"
  },
  {
    "text": "spark has a lazy execution where it will",
    "start": "1156799",
    "end": "1159360"
  },
  {
    "text": "only execute the function",
    "start": "1159360",
    "end": "1160640"
  },
  {
    "text": "when they actually do the collecting but",
    "start": "1160640",
    "end": "1162720"
  },
  {
    "text": "one thing i wanted to point out is that",
    "start": "1162720",
    "end": "1164160"
  },
  {
    "text": "this model function which i have not",
    "start": "1164160",
    "end": "1165679"
  },
  {
    "text": "really",
    "start": "1165679",
    "end": "1166240"
  },
  {
    "text": "uh talked to described here can be a",
    "start": "1166240",
    "end": "1168720"
  },
  {
    "text": "very complicated function",
    "start": "1168720",
    "end": "1170480"
  },
  {
    "text": "uh and we can pass a lot of different",
    "start": "1170480",
    "end": "1172400"
  },
  {
    "text": "types of objects to it",
    "start": "1172400",
    "end": "1173760"
  },
  {
    "text": "uh without any problem and this will all",
    "start": "1173760",
    "end": "1175919"
  },
  {
    "text": "be done seamlessly",
    "start": "1175919",
    "end": "1177360"
  },
  {
    "text": "and this automatically uh scales the",
    "start": "1177360",
    "end": "1181520"
  },
  {
    "text": "the the function into various parallel",
    "start": "1181520",
    "end": "1183919"
  },
  {
    "text": "um components",
    "start": "1183919",
    "end": "1185679"
  },
  {
    "text": "um so this is actually a very cool",
    "start": "1185679",
    "end": "1187280"
  },
  {
    "text": "approach and this works very well in",
    "start": "1187280",
    "end": "1188799"
  },
  {
    "text": "scaling when you know we do everything",
    "start": "1188799",
    "end": "1190559"
  },
  {
    "text": "within the function",
    "start": "1190559",
    "end": "1192240"
  },
  {
    "text": "but there are other instances where",
    "start": "1192240",
    "end": "1194799"
  },
  {
    "text": "basically we have a much more",
    "start": "1194799",
    "end": "1196160"
  },
  {
    "text": "complicated data frame",
    "start": "1196160",
    "end": "1197840"
  },
  {
    "text": "sorry let me see how do i go to the next",
    "start": "1197840",
    "end": "1199440"
  },
  {
    "text": "slide here",
    "start": "1199440",
    "end": "1202000"
  },
  {
    "text": "could you go to the next slide i'm not",
    "start": "1202159",
    "end": "1204320"
  },
  {
    "text": "able to do that for some reason",
    "start": "1204320",
    "end": "1207760"
  },
  {
    "text": "uh can we go to the next slide oh",
    "start": "1207760",
    "end": "1211440"
  },
  {
    "text": "okay thank you so much yeah so there is",
    "start": "1211440",
    "end": "1213760"
  },
  {
    "text": "a a lot of situations where basically",
    "start": "1213760",
    "end": "1216159"
  },
  {
    "text": "our",
    "start": "1216159",
    "end": "1216720"
  },
  {
    "text": "data set is already available it's it's",
    "start": "1216720",
    "end": "1219120"
  },
  {
    "text": "a complex data",
    "start": "1219120",
    "end": "1220000"
  },
  {
    "text": "frame and not every single column within",
    "start": "1220000",
    "end": "1222480"
  },
  {
    "text": "the data set",
    "start": "1222480",
    "end": "1223280"
  },
  {
    "text": "is are the keys there's a lot of actual",
    "start": "1223280",
    "end": "1225200"
  },
  {
    "text": "data uh there's also things where we",
    "start": "1225200",
    "end": "1227360"
  },
  {
    "text": "want to actually",
    "start": "1227360",
    "end": "1228240"
  },
  {
    "text": "um you know get data out of our",
    "start": "1228240",
    "end": "1230320"
  },
  {
    "text": "functions which are much more",
    "start": "1230320",
    "end": "1231679"
  },
  {
    "text": "sophisticated",
    "start": "1231679",
    "end": "1232880"
  },
  {
    "text": "uh so we cannot use that simple approach",
    "start": "1232880",
    "end": "1235440"
  },
  {
    "text": "in that case",
    "start": "1235440",
    "end": "1236320"
  },
  {
    "text": "and in that case what we have to do is",
    "start": "1236320",
    "end": "1239120"
  },
  {
    "text": "we have to come up with a different",
    "start": "1239120",
    "end": "1240159"
  },
  {
    "text": "approach",
    "start": "1240159",
    "end": "1240640"
  },
  {
    "text": "and we decided to use an approach which",
    "start": "1240640",
    "end": "1242960"
  },
  {
    "text": "uses the conjunction of pandas udf",
    "start": "1242960",
    "end": "1245440"
  },
  {
    "text": "with apache arrow and we'll talk about",
    "start": "1245440",
    "end": "1248240"
  },
  {
    "text": "apache arrow um in the next slide but",
    "start": "1248240",
    "end": "1250640"
  },
  {
    "text": "basically the idea is uds basically",
    "start": "1250640",
    "end": "1253440"
  },
  {
    "text": "means a user defined function",
    "start": "1253440",
    "end": "1255200"
  },
  {
    "text": "and typically in python uh when you do",
    "start": "1255200",
    "end": "1258240"
  },
  {
    "text": "python udfs",
    "start": "1258240",
    "end": "1259679"
  },
  {
    "text": "that is basically takes every uh row",
    "start": "1259679",
    "end": "1262880"
  },
  {
    "text": "of the data frame and converts and runs",
    "start": "1262880",
    "end": "1265600"
  },
  {
    "text": "the function every row and that's that's",
    "start": "1265600",
    "end": "1267120"
  },
  {
    "text": "very inefficient",
    "start": "1267120",
    "end": "1268320"
  },
  {
    "text": "uh so we wanted to use pandas udfs which",
    "start": "1268320",
    "end": "1270960"
  },
  {
    "text": "actually",
    "start": "1270960",
    "end": "1271760"
  },
  {
    "text": "work in a much more vectorized fashion",
    "start": "1271760",
    "end": "1274320"
  },
  {
    "text": "and",
    "start": "1274320",
    "end": "1274880"
  },
  {
    "text": "can increase the performance by up to",
    "start": "1274880",
    "end": "1276400"
  },
  {
    "text": "100 times on on python udfs",
    "start": "1276400",
    "end": "1278799"
  },
  {
    "text": "and obviously since uh we are going to",
    "start": "1278799",
    "end": "1280880"
  },
  {
    "text": "do our models across a lot of different",
    "start": "1280880",
    "end": "1283039"
  },
  {
    "text": "dimensions",
    "start": "1283039",
    "end": "1284000"
  },
  {
    "text": "uh we want to use something called a",
    "start": "1284000",
    "end": "1285440"
  },
  {
    "text": "group map pandas udfs",
    "start": "1285440",
    "end": "1287520"
  },
  {
    "text": "uh which allow us to take a",
    "start": "1287520",
    "end": "1290559"
  },
  {
    "text": "you know a group by approach uh to split",
    "start": "1290559",
    "end": "1292880"
  },
  {
    "text": "apply combine these",
    "start": "1292880",
    "end": "1294240"
  },
  {
    "text": "udfs and uh do these functions uh in a",
    "start": "1294240",
    "end": "1297440"
  },
  {
    "text": "much more",
    "start": "1297440",
    "end": "1298159"
  },
  {
    "text": "seamless fashion um so",
    "start": "1298159",
    "end": "1301919"
  },
  {
    "text": "i'll talk a little bit more about apache",
    "start": "1301919",
    "end": "1303760"
  },
  {
    "start": "1303000",
    "end": "1369000"
  },
  {
    "text": "arrow um yeah",
    "start": "1303760",
    "end": "1305600"
  },
  {
    "text": "so um so when we uh do this kind of um",
    "start": "1305600",
    "end": "1309200"
  },
  {
    "text": "uh go from like you know basically spark",
    "start": "1309200",
    "end": "1311280"
  },
  {
    "text": "which is running in uh",
    "start": "1311280",
    "end": "1312400"
  },
  {
    "text": "like a java virtual machine and go into",
    "start": "1312400",
    "end": "1314799"
  },
  {
    "text": "a python pandas",
    "start": "1314799",
    "end": "1316080"
  },
  {
    "text": "api uh there's a lot of serialization",
    "start": "1316080",
    "end": "1318799"
  },
  {
    "text": "involved",
    "start": "1318799",
    "end": "1319679"
  },
  {
    "text": "if you don't use any apache arrow and",
    "start": "1319679",
    "end": "1322720"
  },
  {
    "text": "that civilization can take a lot of time",
    "start": "1322720",
    "end": "1324320"
  },
  {
    "text": "and it's very efficient inefficient",
    "start": "1324320",
    "end": "1326240"
  },
  {
    "text": "because it works row by row but with",
    "start": "1326240",
    "end": "1328880"
  },
  {
    "text": "apache arrow",
    "start": "1328880",
    "end": "1330000"
  },
  {
    "text": "we can use a corner uh way to",
    "start": "1330000",
    "end": "1333120"
  },
  {
    "text": "uh to basically send this data from uh",
    "start": "1333120",
    "end": "1335440"
  },
  {
    "text": "spark to",
    "start": "1335440",
    "end": "1336240"
  },
  {
    "text": "uh to python api and to a pandas api and",
    "start": "1336240",
    "end": "1340080"
  },
  {
    "text": "this corner is",
    "start": "1340080",
    "end": "1341360"
  },
  {
    "text": "data is very efficient because it takes",
    "start": "1341360",
    "end": "1343360"
  },
  {
    "text": "advantage of all",
    "start": "1343360",
    "end": "1345760"
  },
  {
    "text": "the the sim the architecture of all the",
    "start": "1345760",
    "end": "1348640"
  },
  {
    "text": "modern cpus",
    "start": "1348640",
    "end": "1349919"
  },
  {
    "text": "and this allows a very efficient way to",
    "start": "1349919",
    "end": "1351919"
  },
  {
    "text": "uh to get this data in a very vectorized",
    "start": "1351919",
    "end": "1354000"
  },
  {
    "text": "fashion",
    "start": "1354000",
    "end": "1354880"
  },
  {
    "text": "uh from uh from basically the spark data",
    "start": "1354880",
    "end": "1357520"
  },
  {
    "text": "frame",
    "start": "1357520",
    "end": "1358159"
  },
  {
    "text": "uh to the pandas api and uh so this is",
    "start": "1358159",
    "end": "1361280"
  },
  {
    "text": "essential in",
    "start": "1361280",
    "end": "1362240"
  },
  {
    "text": "getting the performance and the cost",
    "start": "1362240",
    "end": "1364000"
  },
  {
    "text": "reduction that we need",
    "start": "1364000",
    "end": "1365440"
  },
  {
    "text": "um so i will um uh",
    "start": "1365440",
    "end": "1368480"
  },
  {
    "text": "talk a little bit more about the the",
    "start": "1368480",
    "end": "1370559"
  },
  {
    "start": "1369000",
    "end": "1420000"
  },
  {
    "text": "group pandas",
    "start": "1370559",
    "end": "1371840"
  },
  {
    "text": "api um so as i said you know basically",
    "start": "1371840",
    "end": "1374240"
  },
  {
    "text": "the idea is we have an original data",
    "start": "1374240",
    "end": "1376320"
  },
  {
    "text": "frame",
    "start": "1376320",
    "end": "1377120"
  },
  {
    "text": "which is consists of all our data and a",
    "start": "1377120",
    "end": "1379760"
  },
  {
    "text": "bunch of keys",
    "start": "1379760",
    "end": "1380960"
  },
  {
    "text": "and we use those keys to group these",
    "start": "1380960",
    "end": "1383200"
  },
  {
    "text": "data frames into",
    "start": "1383200",
    "end": "1384799"
  },
  {
    "text": "different groups and these groups then",
    "start": "1384799",
    "end": "1388480"
  },
  {
    "text": "go into a pandas function",
    "start": "1388480",
    "end": "1390240"
  },
  {
    "text": "and the function is applied to each",
    "start": "1390240",
    "end": "1391760"
  },
  {
    "text": "group separately and then the result of",
    "start": "1391760",
    "end": "1393840"
  },
  {
    "text": "these groups",
    "start": "1393840",
    "end": "1394640"
  },
  {
    "text": "is a pandas output and that",
    "start": "1394640",
    "end": "1397679"
  },
  {
    "text": "allows us to get basically a very",
    "start": "1397679",
    "end": "1400159"
  },
  {
    "text": "efficient way to do",
    "start": "1400159",
    "end": "1401679"
  },
  {
    "text": "this this function so basically the idea",
    "start": "1401679",
    "end": "1404080"
  },
  {
    "text": "is",
    "start": "1404080",
    "end": "1404799"
  },
  {
    "text": "we are kind of doing the python function",
    "start": "1404799",
    "end": "1406880"
  },
  {
    "text": "as a python api",
    "start": "1406880",
    "end": "1408880"
  },
  {
    "text": "but we are using these underlying",
    "start": "1408880",
    "end": "1411280"
  },
  {
    "text": "technologies like apache arrow and",
    "start": "1411280",
    "end": "1412880"
  },
  {
    "text": "pandas udf to do it in a very efficient",
    "start": "1412880",
    "end": "1414720"
  },
  {
    "text": "and",
    "start": "1414720",
    "end": "1414960"
  },
  {
    "text": "very parallel fashion um so what i'll do",
    "start": "1414960",
    "end": "1418320"
  },
  {
    "text": "is i will go through a code snippet",
    "start": "1418320",
    "end": "1421760"
  },
  {
    "start": "1420000",
    "end": "1597000"
  },
  {
    "text": "that actually uh explains how how we do",
    "start": "1421760",
    "end": "1424880"
  },
  {
    "text": "this",
    "start": "1424880",
    "end": "1425520"
  },
  {
    "text": "uh in a little more detail so uh",
    "start": "1425520",
    "end": "1428480"
  },
  {
    "text": "basically if you look at this python",
    "start": "1428480",
    "end": "1430080"
  },
  {
    "text": "code um there is a very it's a very",
    "start": "1430080",
    "end": "1432640"
  },
  {
    "text": "simple",
    "start": "1432640",
    "end": "1433200"
  },
  {
    "text": "um instance of how to run uh several",
    "start": "1433200",
    "end": "1436720"
  },
  {
    "text": "uh models of a random forest um",
    "start": "1436720",
    "end": "1439440"
  },
  {
    "text": "scikit-learn random forest record",
    "start": "1439440",
    "end": "1441039"
  },
  {
    "text": "regressor in parallel",
    "start": "1441039",
    "end": "1442480"
  },
  {
    "text": "uh so that would be a good example of",
    "start": "1442480",
    "end": "1444240"
  },
  {
    "text": "some of the models we use",
    "start": "1444240",
    "end": "1445840"
  },
  {
    "text": "uh so the way we do that is first we",
    "start": "1445840",
    "end": "1447840"
  },
  {
    "text": "define a schema",
    "start": "1447840",
    "end": "1449840"
  },
  {
    "text": "uh which basically defines what what",
    "start": "1449840",
    "end": "1452159"
  },
  {
    "text": "kind of output we're going to have in",
    "start": "1452159",
    "end": "1453679"
  },
  {
    "text": "this case we're doing something very",
    "start": "1453679",
    "end": "1454880"
  },
  {
    "text": "simple",
    "start": "1454880",
    "end": "1455679"
  },
  {
    "text": "uh we have basically our group id which",
    "start": "1455679",
    "end": "1458720"
  },
  {
    "text": "is the key we",
    "start": "1458720",
    "end": "1460000"
  },
  {
    "text": "do the parallelization by and then we",
    "start": "1460000",
    "end": "1462320"
  },
  {
    "text": "have the model string which basically",
    "start": "1462320",
    "end": "1463840"
  },
  {
    "text": "just",
    "start": "1463840",
    "end": "1464240"
  },
  {
    "text": "uh gives you the model uh file name",
    "start": "1464240",
    "end": "1467279"
  },
  {
    "text": "but this this schema can be very",
    "start": "1467279",
    "end": "1468960"
  },
  {
    "text": "complicated and we can do",
    "start": "1468960",
    "end": "1471039"
  },
  {
    "text": "uh you know complete pandas data frame",
    "start": "1471039",
    "end": "1472880"
  },
  {
    "text": "with all of different types of uh uh",
    "start": "1472880",
    "end": "1475360"
  },
  {
    "text": "objects in it uh to really uh return a",
    "start": "1475360",
    "end": "1478159"
  },
  {
    "text": "very uh you know very uh",
    "start": "1478159",
    "end": "1479760"
  },
  {
    "text": "complete data frame uh so in this case",
    "start": "1479760",
    "end": "1482240"
  },
  {
    "text": "um you know we have to use a decorator",
    "start": "1482240",
    "end": "1484400"
  },
  {
    "text": "uh to to basically instantiate this",
    "start": "1484400",
    "end": "1486960"
  },
  {
    "text": "pandas udf",
    "start": "1486960",
    "end": "1488159"
  },
  {
    "text": "and we are doing a group map udf uh so",
    "start": "1488159",
    "end": "1490640"
  },
  {
    "text": "once we do that then everything else is",
    "start": "1490640",
    "end": "1492320"
  },
  {
    "text": "pretty straightforward this is your a",
    "start": "1492320",
    "end": "1493919"
  },
  {
    "text": "regular uh python function",
    "start": "1493919",
    "end": "1496080"
  },
  {
    "text": "uh that basically has a pandas",
    "start": "1496080",
    "end": "1499120"
  },
  {
    "text": "data frame as an input the group id is",
    "start": "1499120",
    "end": "1502320"
  },
  {
    "text": "basically",
    "start": "1502320",
    "end": "1503039"
  },
  {
    "text": "something we pass as part of this data",
    "start": "1503039",
    "end": "1504880"
  },
  {
    "text": "frame and that allows us to identify",
    "start": "1504880",
    "end": "1507440"
  },
  {
    "text": "this specific group and then you know",
    "start": "1507440",
    "end": "1510320"
  },
  {
    "text": "within the data frame we can have a lot",
    "start": "1510320",
    "end": "1511919"
  },
  {
    "text": "of different",
    "start": "1511919",
    "end": "1512400"
  },
  {
    "text": "columns and in this case we have three",
    "start": "1512400",
    "end": "1514240"
  },
  {
    "text": "columns which has",
    "start": "1514240",
    "end": "1516080"
  },
  {
    "text": "the two features and the label and so by",
    "start": "1516080",
    "end": "1519440"
  },
  {
    "text": "extracting those columns",
    "start": "1519440",
    "end": "1521039"
  },
  {
    "text": "uh we can run the random forest",
    "start": "1521039",
    "end": "1522480"
  },
  {
    "text": "regressor on it and then we basically",
    "start": "1522480",
    "end": "1525120"
  },
  {
    "text": "can do a pickle",
    "start": "1525120",
    "end": "1526240"
  },
  {
    "text": "uh dump of the model and we can",
    "start": "1526240",
    "end": "1528960"
  },
  {
    "text": "basically pass",
    "start": "1528960",
    "end": "1529919"
  },
  {
    "text": "the the model and the group id back and",
    "start": "1529919",
    "end": "1532159"
  },
  {
    "text": "this way",
    "start": "1532159",
    "end": "1533279"
  },
  {
    "text": "this whole function runs in parallel",
    "start": "1533279",
    "end": "1536000"
  },
  {
    "text": "across several",
    "start": "1536000",
    "end": "1536880"
  },
  {
    "text": "executors and the way we instantiate",
    "start": "1536880",
    "end": "1540480"
  },
  {
    "text": "this function",
    "start": "1540480",
    "end": "1541120"
  },
  {
    "text": "is first we basically enable arrows so",
    "start": "1541120",
    "end": "1544080"
  },
  {
    "text": "the serialization is",
    "start": "1544080",
    "end": "1545600"
  },
  {
    "text": "is very fast and is vectorized then we",
    "start": "1545600",
    "end": "1548720"
  },
  {
    "text": "have our original",
    "start": "1548720",
    "end": "1550000"
  },
  {
    "text": "pandas data frame that contains the",
    "start": "1550000",
    "end": "1551919"
  },
  {
    "text": "actual data we convert that into a spark",
    "start": "1551919",
    "end": "1554240"
  },
  {
    "text": "data frame",
    "start": "1554240",
    "end": "1555279"
  },
  {
    "text": "and then we apply the the group by",
    "start": "1555279",
    "end": "1558480"
  },
  {
    "text": "model on it i'm sorry group by pandas",
    "start": "1558480",
    "end": "1560159"
  },
  {
    "text": "udf on it and",
    "start": "1560159",
    "end": "1562159"
  },
  {
    "text": "then once we apply this as as i",
    "start": "1562159",
    "end": "1564559"
  },
  {
    "text": "mentioned before spark has lazy",
    "start": "1564559",
    "end": "1566080"
  },
  {
    "text": "execution",
    "start": "1566080",
    "end": "1567200"
  },
  {
    "text": "so we have to convert this back to",
    "start": "1567200",
    "end": "1568880"
  },
  {
    "text": "pandas and this way we get our results",
    "start": "1568880",
    "end": "1570799"
  },
  {
    "text": "back",
    "start": "1570799",
    "end": "1571679"
  },
  {
    "text": "so it's a really simple way uh with just",
    "start": "1571679",
    "end": "1574400"
  },
  {
    "text": "using",
    "start": "1574400",
    "end": "1574880"
  },
  {
    "text": "underlying python functions and doing",
    "start": "1574880",
    "end": "1576799"
  },
  {
    "text": "parallelization spark",
    "start": "1576799",
    "end": "1578320"
  },
  {
    "text": "without getting into too much of the",
    "start": "1578320",
    "end": "1579919"
  },
  {
    "text": "nitty gritty of a spark",
    "start": "1579919",
    "end": "1582080"
  },
  {
    "text": "so this actually demonstrates you know",
    "start": "1582080",
    "end": "1583840"
  },
  {
    "text": "the two ways we have paralyzed",
    "start": "1583840",
    "end": "1585760"
  },
  {
    "text": "and been able to scale our models um to",
    "start": "1585760",
    "end": "1588799"
  },
  {
    "text": "this um",
    "start": "1588799",
    "end": "1589520"
  },
  {
    "text": "this level uh so so that actually",
    "start": "1589520",
    "end": "1593039"
  },
  {
    "text": "concludes",
    "start": "1593039",
    "end": "1593760"
  },
  {
    "text": "um our presentation uh we have a few",
    "start": "1593760",
    "end": "1597200"
  },
  {
    "start": "1597000",
    "end": "1709000"
  },
  {
    "text": "concluding remarks we can talk about",
    "start": "1597200",
    "end": "1598960"
  },
  {
    "text": "those",
    "start": "1598960",
    "end": "1600480"
  },
  {
    "text": "so basically you know we presented a",
    "start": "1600480",
    "end": "1603520"
  },
  {
    "text": "a way to take the best of kubernetes",
    "start": "1603520",
    "end": "1606720"
  },
  {
    "text": "and best of spark to do a scaling",
    "start": "1606720",
    "end": "1610400"
  },
  {
    "text": "with end-to-end automation and security",
    "start": "1610400",
    "end": "1612400"
  },
  {
    "text": "and ci cd",
    "start": "1612400",
    "end": "1613679"
  },
  {
    "text": "uh to basically prioritize our",
    "start": "1613679",
    "end": "1616799"
  },
  {
    "text": "models and scale them to a very high",
    "start": "1616799",
    "end": "1619200"
  },
  {
    "text": "cardinality",
    "start": "1619200",
    "end": "1620480"
  },
  {
    "text": "we used a very unique architecture as",
    "start": "1620480",
    "end": "1622159"
  },
  {
    "text": "jakub described",
    "start": "1622159",
    "end": "1624159"
  },
  {
    "text": "it's very unique because we embedding",
    "start": "1624159",
    "end": "1625840"
  },
  {
    "text": "spark into a micro service within",
    "start": "1625840",
    "end": "1627600"
  },
  {
    "text": "kubernetes i think this is a very novel",
    "start": "1627600",
    "end": "1629840"
  },
  {
    "text": "but",
    "start": "1629840",
    "end": "1630240"
  },
  {
    "text": "uh but very but very simple uh",
    "start": "1630240",
    "end": "1633760"
  },
  {
    "text": "uh so obviously you know the one",
    "start": "1633760",
    "end": "1635600"
  },
  {
    "text": "advantage we have with spark as yakub",
    "start": "1635600",
    "end": "1637440"
  },
  {
    "text": "mentioned was",
    "start": "1637440",
    "end": "1638399"
  },
  {
    "text": "you know when we run these models uh we",
    "start": "1638399",
    "end": "1640320"
  },
  {
    "text": "can run them",
    "start": "1640320",
    "end": "1641440"
  },
  {
    "text": "as basically uh clusters that we create",
    "start": "1641440",
    "end": "1644720"
  },
  {
    "text": "and uh terminate once the the actual",
    "start": "1644720",
    "end": "1647760"
  },
  {
    "text": "uh job or the training is over that way",
    "start": "1647760",
    "end": "1650000"
  },
  {
    "text": "we save a lot of resource courses i'm",
    "start": "1650000",
    "end": "1652240"
  },
  {
    "text": "sorry",
    "start": "1652240",
    "end": "1652640"
  },
  {
    "text": "resource costs and then as we",
    "start": "1652640",
    "end": "1655760"
  },
  {
    "text": "evolve with more models and more",
    "start": "1655760",
    "end": "1657520"
  },
  {
    "text": "applications it's very easy for us to",
    "start": "1657520",
    "end": "1659520"
  },
  {
    "text": "integrate those into",
    "start": "1659520",
    "end": "1661200"
  },
  {
    "text": "into our current infrastructure and now",
    "start": "1661200",
    "end": "1663760"
  },
  {
    "text": "you know in terms of scaling",
    "start": "1663760",
    "end": "1665200"
  },
  {
    "text": "we're also looking at other dimensions",
    "start": "1665200",
    "end": "1666799"
  },
  {
    "text": "of scalings uh that we can do with it",
    "start": "1666799",
    "end": "1668799"
  },
  {
    "text": "within this architecture",
    "start": "1668799",
    "end": "1670240"
  },
  {
    "text": "uh which is beyond uh just the the",
    "start": "1670240",
    "end": "1672880"
  },
  {
    "text": "models itself uh we're looking at",
    "start": "1672880",
    "end": "1674720"
  },
  {
    "text": "you know when our application is very",
    "start": "1674720",
    "end": "1676240"
  },
  {
    "text": "big and has a lot of different",
    "start": "1676240",
    "end": "1678159"
  },
  {
    "text": "complexity we can basically scale within",
    "start": "1678159",
    "end": "1680159"
  },
  {
    "text": "the application",
    "start": "1680159",
    "end": "1681200"
  },
  {
    "text": "uh with the data itself and then we're",
    "start": "1681200",
    "end": "1683440"
  },
  {
    "text": "also looking at some very sophisticated",
    "start": "1683440",
    "end": "1685200"
  },
  {
    "text": "deep learning models which you know",
    "start": "1685200",
    "end": "1686880"
  },
  {
    "text": "which tend to be very complex",
    "start": "1686880",
    "end": "1688720"
  },
  {
    "text": "and we're looking at how we can leverage",
    "start": "1688720",
    "end": "1690399"
  },
  {
    "text": "this architecture to even",
    "start": "1690399",
    "end": "1692320"
  },
  {
    "text": "scale within those models and",
    "start": "1692320",
    "end": "1694080"
  },
  {
    "text": "parallelize those models",
    "start": "1694080",
    "end": "1695600"
  },
  {
    "text": "so those are some of the things we are",
    "start": "1695600",
    "end": "1696720"
  },
  {
    "text": "going to be doing in the in the future",
    "start": "1696720",
    "end": "1699200"
  },
  {
    "text": "great thank you okay uh",
    "start": "1699200",
    "end": "1702240"
  },
  {
    "text": "thanks sandeep thanks uh thank you uh",
    "start": "1702240",
    "end": "1705310"
  },
  {
    "text": "[Music]",
    "start": "1705310",
    "end": "1706559"
  },
  {
    "text": "that's all from our site",
    "start": "1706559",
    "end": "1711278"
  }
]