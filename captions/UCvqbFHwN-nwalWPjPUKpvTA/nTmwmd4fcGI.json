[
  {
    "text": "hello everyone and welcome to a sunny",
    "start": "160",
    "end": "2879"
  },
  {
    "text": "CubeCon 2025 in London i hope you are",
    "start": "2879",
    "end": "6160"
  },
  {
    "text": "having an amazing day of learning",
    "start": "6160",
    "end": "8519"
  },
  {
    "text": "today many times we see bigger impacts",
    "start": "8519",
    "end": "12000"
  },
  {
    "text": "or greater results achieved from small",
    "start": "12000",
    "end": "15160"
  },
  {
    "text": "wellexecuted things that are put",
    "start": "15160",
    "end": "17359"
  },
  {
    "text": "creatively together and today I want to",
    "start": "17359",
    "end": "20400"
  },
  {
    "text": "highlight one such small less",
    "start": "20400",
    "end": "23199"
  },
  {
    "text": "appreciated and talked about but",
    "start": "23199",
    "end": "25359"
  },
  {
    "text": "extremely powerful Kubernetes feature",
    "start": "25359",
    "end": "27920"
  },
  {
    "text": "that is the init container",
    "start": "27920",
    "end": "30480"
  },
  {
    "text": "and share how we at Yelp leverage inate",
    "start": "30480",
    "end": "33680"
  },
  {
    "text": "containers to reduce the operational",
    "start": "33680",
    "end": "35840"
  },
  {
    "text": "overhead of our Cassendra clusters and",
    "start": "35840",
    "end": "39200"
  },
  {
    "text": "enhance the engineering",
    "start": "39200",
    "end": "42280"
  },
  {
    "text": "efficiency so introducing myself first I",
    "start": "42280",
    "end": "45680"
  },
  {
    "text": "am Muhammad a tech lead in the database",
    "start": "45680",
    "end": "47760"
  },
  {
    "text": "reliability engineering group at Yelp",
    "start": "47760",
    "end": "50239"
  },
  {
    "text": "where I primarily look around the NoSQL",
    "start": "50239",
    "end": "53440"
  },
  {
    "text": "databases uh and in particular",
    "start": "53440",
    "end": "56600"
  },
  {
    "text": "Cassandra",
    "start": "56600",
    "end": "58920"
  },
  {
    "text": "Here is what we will try to cover today",
    "start": "58920",
    "end": "62160"
  },
  {
    "text": "uh I will try setting up the stage by",
    "start": "62160",
    "end": "65280"
  },
  {
    "text": "sharing some information about the",
    "start": "65280",
    "end": "67760"
  },
  {
    "text": "background in which we operate and give",
    "start": "67760",
    "end": "70479"
  },
  {
    "text": "you a bird eyee view of how we use",
    "start": "70479",
    "end": "73840"
  },
  {
    "text": "Cassandra at Yelp then we'll dive into",
    "start": "73840",
    "end": "77520"
  },
  {
    "text": "three different uh problems that were",
    "start": "77520",
    "end": "80240"
  },
  {
    "text": "solved by inate containers",
    "start": "80240",
    "end": "82560"
  },
  {
    "text": "the first was about cluster scaling and",
    "start": "82560",
    "end": "85520"
  },
  {
    "text": "here we will specifically be looking at",
    "start": "85520",
    "end": "87520"
  },
  {
    "text": "the horizontal cluster scaling",
    "start": "87520",
    "end": "90520"
  },
  {
    "text": "aspects secondly I will talk about the",
    "start": "90520",
    "end": "93520"
  },
  {
    "text": "Cassendra cluster upgrades and the",
    "start": "93520",
    "end": "96320"
  },
  {
    "text": "impact init containers had in making the",
    "start": "96320",
    "end": "99520"
  },
  {
    "text": "overall upgrade process easier and",
    "start": "99520",
    "end": "102759"
  },
  {
    "text": "smoother and thirdly I will touch on the",
    "start": "102759",
    "end": "105840"
  },
  {
    "text": "cluster recovery aspects from backups",
    "start": "105840",
    "end": "110479"
  },
  {
    "text": "uh in the end I will try wrapping up",
    "start": "110479",
    "end": "112799"
  },
  {
    "text": "some key takeaways for you so that init",
    "start": "112799",
    "end": "115680"
  },
  {
    "text": "containers is easier for you for your",
    "start": "115680",
    "end": "118159"
  },
  {
    "text": "use",
    "start": "118159",
    "end": "120320"
  },
  {
    "text": "cases so starting off with the",
    "start": "120840",
    "end": "125758"
  },
  {
    "text": "context how many of you have heard about",
    "start": "126040",
    "end": "128640"
  },
  {
    "text": "Yelp can you raise your",
    "start": "128640",
    "end": "131640"
  },
  {
    "text": "hand yeah that's quite interesting uh",
    "start": "131640",
    "end": "135200"
  },
  {
    "text": "those who don't know Yelp is a",
    "start": "135200",
    "end": "137120"
  },
  {
    "text": "community-driven platform that connects",
    "start": "137120",
    "end": "139120"
  },
  {
    "text": "people with great local businesses and",
    "start": "139120",
    "end": "142319"
  },
  {
    "text": "millions of people rely on Yelp for the",
    "start": "142319",
    "end": "145120"
  },
  {
    "text": "useful and trusted content about the",
    "start": "145120",
    "end": "147840"
  },
  {
    "text": "business informations reviews and photos",
    "start": "147840",
    "end": "150720"
  },
  {
    "text": "to inform their spending decisions",
    "start": "150720",
    "end": "154080"
  },
  {
    "text": "as of December 2024 there have been 308",
    "start": "154080",
    "end": "158239"
  },
  {
    "text": "million cumulative reviews with 29",
    "start": "158239",
    "end": "161840"
  },
  {
    "text": "million average monthly unique users in",
    "start": "161840",
    "end": "166319"
  },
  {
    "text": "2024 so here is another",
    "start": "166920",
    "end": "169640"
  },
  {
    "text": "question how many of you have worked",
    "start": "169640",
    "end": "172800"
  },
  {
    "text": "with Cassendra or have heard about",
    "start": "172800",
    "end": "175280"
  },
  {
    "text": "Cassendra database",
    "start": "175280",
    "end": "178840"
  },
  {
    "text": "okay so again those not familiar Apache",
    "start": "179879",
    "end": "184159"
  },
  {
    "text": "Cassandra is a white column distributed",
    "start": "184159",
    "end": "187040"
  },
  {
    "text": "non- relational",
    "start": "187040",
    "end": "189159"
  },
  {
    "text": "database and how we use it how we use it",
    "start": "189159",
    "end": "192400"
  },
  {
    "text": "at Yelp so if you visit any Yelp page",
    "start": "192400",
    "end": "197760"
  },
  {
    "text": "there will be some portion of data that",
    "start": "197760",
    "end": "200319"
  },
  {
    "text": "is served by Cassendra so you can",
    "start": "200319",
    "end": "203280"
  },
  {
    "text": "imagine how extensively Cassendra is",
    "start": "203280",
    "end": "205280"
  },
  {
    "text": "used at Yelp",
    "start": "205280",
    "end": "207440"
  },
  {
    "text": "so we manage over 70 clusters in uh",
    "start": "207440",
    "end": "211760"
  },
  {
    "text": "production environments for different",
    "start": "211760",
    "end": "213280"
  },
  {
    "text": "use cases and there are more than",
    "start": "213280",
    "end": "216480"
  },
  {
    "text": "thousand Cassandra nodes in those",
    "start": "216480",
    "end": "220440"
  },
  {
    "text": "clusters and with respect to data",
    "start": "220440",
    "end": "222840"
  },
  {
    "text": "volumes these clusters store several",
    "start": "222840",
    "end": "225440"
  },
  {
    "text": "hundred terabytes of",
    "start": "225440",
    "end": "227720"
  },
  {
    "text": "data but it's not only about the size",
    "start": "227720",
    "end": "231599"
  },
  {
    "text": "some of these clusters are highly",
    "start": "231599",
    "end": "233440"
  },
  {
    "text": "latency sensitive and we ensure for a",
    "start": "233440",
    "end": "237040"
  },
  {
    "text": "sub 10 millisecond latencies for read",
    "start": "237040",
    "end": "239519"
  },
  {
    "text": "and write traffic from these",
    "start": "239519",
    "end": "243439"
  },
  {
    "text": "clusters from operational perspective",
    "start": "243959",
    "end": "247120"
  },
  {
    "text": "all of our Cassendra clusters run on",
    "start": "247120",
    "end": "249280"
  },
  {
    "text": "Kubernetes uh that's why I'm here and",
    "start": "249280",
    "end": "252640"
  },
  {
    "text": "this was a transition which we made",
    "start": "252640",
    "end": "254159"
  },
  {
    "text": "around five to six years back we used to",
    "start": "254159",
    "end": "256720"
  },
  {
    "text": "run the entire fleet on Misos",
    "start": "256720",
    "end": "260239"
  },
  {
    "text": "uh at Yelp we use an in-house platform",
    "start": "260239",
    "end": "263520"
  },
  {
    "text": "as a service called Bastard that",
    "start": "263520",
    "end": "265919"
  },
  {
    "text": "provides the necessary abstraction on",
    "start": "265919",
    "end": "267680"
  },
  {
    "text": "top of Kubernetes uh for managing",
    "start": "267680",
    "end": "270720"
  },
  {
    "text": "different",
    "start": "270720",
    "end": "272600"
  },
  {
    "text": "services we run our own custom Cassendra",
    "start": "272600",
    "end": "276000"
  },
  {
    "text": "operator that handles all the Kubernetes",
    "start": "276000",
    "end": "278479"
  },
  {
    "text": "interaction uh for the Cassendra",
    "start": "278479",
    "end": "280919"
  },
  {
    "text": "clusters",
    "start": "280919",
    "end": "282639"
  },
  {
    "text": "now the configurations at Yelp are",
    "start": "282639",
    "end": "284639"
  },
  {
    "text": "managed in a declarative way uh using a",
    "start": "284639",
    "end": "287280"
  },
  {
    "text": "central git repository which we called",
    "start": "287280",
    "end": "289360"
  },
  {
    "text": "Yelp s config uh as shown in the uh",
    "start": "289360",
    "end": "293880"
  },
  {
    "text": "diagram any updates to these",
    "start": "293880",
    "end": "296680"
  },
  {
    "text": "configurations are then converted into a",
    "start": "296680",
    "end": "300320"
  },
  {
    "text": "Kubernetes custom resource uh here the",
    "start": "300320",
    "end": "304000"
  },
  {
    "text": "operator is continuously watching this",
    "start": "304000",
    "end": "305919"
  },
  {
    "text": "custom resource and as soon as it",
    "start": "305919",
    "end": "307919"
  },
  {
    "text": "detects a difference between the actual",
    "start": "307919",
    "end": "310720"
  },
  {
    "text": "and the desired state it reconciles and",
    "start": "310720",
    "end": "314320"
  },
  {
    "text": "it takes action to reconcile in convert",
    "start": "314320",
    "end": "316639"
  },
  {
    "text": "it into relevant Kubernetes",
    "start": "316639",
    "end": "319960"
  },
  {
    "text": "resources we use persistent volumes uh",
    "start": "319960",
    "end": "323520"
  },
  {
    "text": "as we are talking about the stateful",
    "start": "323520",
    "end": "325280"
  },
  {
    "text": "workloads and these are attached to the",
    "start": "325280",
    "end": "327680"
  },
  {
    "text": "Kubernetes pods",
    "start": "327680",
    "end": "330400"
  },
  {
    "text": "one important thing to note here is that",
    "start": "330400",
    "end": "332880"
  },
  {
    "text": "uh our clusters are spread across",
    "start": "332880",
    "end": "334800"
  },
  {
    "text": "multiple data centers in multiple",
    "start": "334800",
    "end": "337120"
  },
  {
    "text": "regions so within a single data center",
    "start": "337120",
    "end": "340560"
  },
  {
    "text": "we maintain a replication factor of",
    "start": "340560",
    "end": "343120"
  },
  {
    "text": "three uh however for simplicity uh in",
    "start": "343120",
    "end": "346240"
  },
  {
    "text": "the upcoming slides there may be a",
    "start": "346240",
    "end": "349039"
  },
  {
    "text": "single replication setup",
    "start": "349039",
    "end": "352479"
  },
  {
    "text": "shown so jumping to some basics about",
    "start": "353320",
    "end": "357039"
  },
  {
    "text": "Kubernetes ports uh a pod is the",
    "start": "357039",
    "end": "360960"
  },
  {
    "text": "smallest deployable unit in Kubernetes",
    "start": "360960",
    "end": "364800"
  },
  {
    "text": "essentially it's a group of container",
    "start": "364800",
    "end": "367039"
  },
  {
    "text": "that share the same network and runs on",
    "start": "367039",
    "end": "370000"
  },
  {
    "text": "top of uh the same Kubernetes",
    "start": "370000",
    "end": "373479"
  },
  {
    "text": "node so when a pod is scheduled all the",
    "start": "373479",
    "end": "377759"
  },
  {
    "text": "containers start in parallel while there",
    "start": "377759",
    "end": "381039"
  },
  {
    "text": "isn't any strict limitation on the",
    "start": "381039",
    "end": "382880"
  },
  {
    "text": "number of containers that you can define",
    "start": "382880",
    "end": "384560"
  },
  {
    "text": "inside a pod but from management",
    "start": "384560",
    "end": "386960"
  },
  {
    "text": "perspective you can think that it could",
    "start": "386960",
    "end": "389199"
  },
  {
    "text": "become complex if we have uh too many",
    "start": "389199",
    "end": "393600"
  },
  {
    "text": "containers inside a",
    "start": "393600",
    "end": "396280"
  },
  {
    "text": "pod so now there can be cases where we",
    "start": "396280",
    "end": "399360"
  },
  {
    "text": "need to perform some certain steps",
    "start": "399360",
    "end": "401440"
  },
  {
    "text": "before these containers starts and",
    "start": "401440",
    "end": "403919"
  },
  {
    "text": "that's where init containers comes into",
    "start": "403919",
    "end": "406240"
  },
  {
    "text": "play",
    "start": "406240",
    "end": "409039"
  },
  {
    "text": "uh so these are initate containers are",
    "start": "409039",
    "end": "411039"
  },
  {
    "text": "specialized containers that make sure",
    "start": "411039",
    "end": "413199"
  },
  {
    "text": "everything is set up before main",
    "start": "413199",
    "end": "415199"
  },
  {
    "text": "application is running however few key",
    "start": "415199",
    "end": "418560"
  },
  {
    "text": "differences here firstly unlike main",
    "start": "418560",
    "end": "421440"
  },
  {
    "text": "containers which run in parallel init",
    "start": "421440",
    "end": "424560"
  },
  {
    "text": "containers always run sequentially one",
    "start": "424560",
    "end": "426800"
  },
  {
    "text": "after",
    "start": "426800",
    "end": "428120"
  },
  {
    "text": "another and secondly an init container",
    "start": "428120",
    "end": "431199"
  },
  {
    "text": "must run to completion successfully",
    "start": "431199",
    "end": "433919"
  },
  {
    "text": "before either the next init container is",
    "start": "433919",
    "end": "436080"
  },
  {
    "text": "invoked or the regular container",
    "start": "436080",
    "end": "439479"
  },
  {
    "text": "starts and similar to the regular",
    "start": "439479",
    "end": "442000"
  },
  {
    "text": "containers there is no strict limitation",
    "start": "442000",
    "end": "444400"
  },
  {
    "text": "on the number of init containers that",
    "start": "444400",
    "end": "446720"
  },
  {
    "text": "can be",
    "start": "446720",
    "end": "448360"
  },
  {
    "text": "created and thirdly init containers are",
    "start": "448360",
    "end": "451759"
  },
  {
    "text": "designed for setup tasks only they are",
    "start": "451759",
    "end": "454639"
  },
  {
    "text": "not designed to serve user traffic so",
    "start": "454639",
    "end": "457360"
  },
  {
    "text": "that's why you will not find readiness",
    "start": "457360",
    "end": "460720"
  },
  {
    "text": "and livveness probes supported in init",
    "start": "460720",
    "end": "465160"
  },
  {
    "text": "containers so jumping to our application",
    "start": "465160",
    "end": "468240"
  },
  {
    "text": "of init containers and solving different",
    "start": "468240",
    "end": "470800"
  },
  {
    "text": "use cases the first problem is about",
    "start": "470800",
    "end": "473039"
  },
  {
    "text": "horizontal scaling of Cassendra",
    "start": "473039",
    "end": "476919"
  },
  {
    "text": "clusters how scaleup works in Cassendra",
    "start": "476919",
    "end": "480960"
  },
  {
    "text": "when a node joins the cluster there is a",
    "start": "480960",
    "end": "484240"
  },
  {
    "text": "seed node with which this newly created",
    "start": "484240",
    "end": "487599"
  },
  {
    "text": "Cassendra node interacts to learn about",
    "start": "487599",
    "end": "489759"
  },
  {
    "text": "the cluster topology how many nodes are",
    "start": "489759",
    "end": "492639"
  },
  {
    "text": "there and the token range assignments",
    "start": "492639",
    "end": "496720"
  },
  {
    "text": "once this new node knows about the",
    "start": "496720",
    "end": "499680"
  },
  {
    "text": "cluster then it enters into a bootstrap",
    "start": "499680",
    "end": "502240"
  },
  {
    "text": "process where token ranges are assigned",
    "start": "502240",
    "end": "505360"
  },
  {
    "text": "to it and then it starts streaming data",
    "start": "505360",
    "end": "508800"
  },
  {
    "text": "from its peers and this process can take",
    "start": "508800",
    "end": "512640"
  },
  {
    "text": "some time depending upon the volume of",
    "start": "512640",
    "end": "515120"
  },
  {
    "text": "data that it needs to",
    "start": "515120",
    "end": "517640"
  },
  {
    "text": "stream so once this streaming process",
    "start": "517640",
    "end": "520719"
  },
  {
    "text": "completes the new node joins the cluster",
    "start": "520719",
    "end": "523919"
  },
  {
    "text": "and becomes ready to serve user request",
    "start": "523919",
    "end": "527440"
  },
  {
    "text": "so far everything looks simple and works",
    "start": "527440",
    "end": "530120"
  },
  {
    "text": "fine but the challenge we face is when",
    "start": "530120",
    "end": "533200"
  },
  {
    "text": "we are scaling a Cassandra cluster which",
    "start": "533200",
    "end": "536160"
  },
  {
    "text": "has change data capture enabled",
    "start": "536160",
    "end": "540640"
  },
  {
    "text": "change data capture is often acronymed",
    "start": "540640",
    "end": "542560"
  },
  {
    "text": "as CDC which is a way to detect changes",
    "start": "542560",
    "end": "546000"
  },
  {
    "text": "in data and allowing it to convert it",
    "start": "546000",
    "end": "549040"
  },
  {
    "text": "into a",
    "start": "549040",
    "end": "550600"
  },
  {
    "text": "stream how it works under the hood is",
    "start": "550600",
    "end": "553600"
  },
  {
    "text": "that each time a right is happening on a",
    "start": "553600",
    "end": "556160"
  },
  {
    "text": "Cassendra cluster cassendra internal",
    "start": "556160",
    "end": "559040"
  },
  {
    "text": "will create a commit log file that",
    "start": "559040",
    "end": "561839"
  },
  {
    "text": "contains information about the change",
    "start": "561839",
    "end": "564480"
  },
  {
    "text": "whether it's an update delete or insert",
    "start": "564480",
    "end": "569120"
  },
  {
    "text": "when CDC mode is enabled something extra",
    "start": "569120",
    "end": "572360"
  },
  {
    "text": "happens where this commit log gets",
    "start": "572360",
    "end": "575519"
  },
  {
    "text": "replicated into another special CDC raw",
    "start": "575519",
    "end": "579360"
  },
  {
    "text": "directory from where independent",
    "start": "579360",
    "end": "581680"
  },
  {
    "text": "consumers can read those commit logs and",
    "start": "581680",
    "end": "585279"
  },
  {
    "text": "process according to the business",
    "start": "585279",
    "end": "588279"
  },
  {
    "text": "needs in our case we publish these",
    "start": "588279",
    "end": "591760"
  },
  {
    "text": "events into Kafka and from there we",
    "start": "591760",
    "end": "594000"
  },
  {
    "text": "ensure that the downstream system gets",
    "start": "594000",
    "end": "596399"
  },
  {
    "text": "synced",
    "start": "596399",
    "end": "598080"
  },
  {
    "text": "now CDC is controlled by a configuration",
    "start": "598080",
    "end": "600560"
  },
  {
    "text": "property uh called CDC enabled that",
    "start": "600560",
    "end": "603279"
  },
  {
    "text": "determines whether this mode is active",
    "start": "603279",
    "end": "605200"
  },
  {
    "text": "or",
    "start": "605200",
    "end": "606040"
  },
  {
    "text": "not now the problem arises if we are",
    "start": "606040",
    "end": "610720"
  },
  {
    "text": "bootstrapping a new node with CDC",
    "start": "610720",
    "end": "613240"
  },
  {
    "text": "enabled so the bootstrap data will also",
    "start": "613240",
    "end": "617600"
  },
  {
    "text": "be emitted into CFKA because CDC events",
    "start": "617600",
    "end": "621200"
  },
  {
    "text": "will be generated for it",
    "start": "621200",
    "end": "623600"
  },
  {
    "text": "now this can lead to unnecessary",
    "start": "623600",
    "end": "626000"
  },
  {
    "text": "duplicate events and we may have to",
    "start": "626000",
    "end": "627920"
  },
  {
    "text": "spend some additional cycles on the",
    "start": "627920",
    "end": "629920"
  },
  {
    "text": "dduplication",
    "start": "629920",
    "end": "631240"
  },
  {
    "text": "front additionally the bootstrap process",
    "start": "631240",
    "end": "634720"
  },
  {
    "text": "also becomes slower because Cassendra",
    "start": "634720",
    "end": "638160"
  },
  {
    "text": "has to perform some extra",
    "start": "638160",
    "end": "640760"
  },
  {
    "text": "operations for all the data that is it",
    "start": "640760",
    "end": "644079"
  },
  {
    "text": "is streaming uh this makes scaling with",
    "start": "644079",
    "end": "647200"
  },
  {
    "text": "CDC enabled more complex and time",
    "start": "647200",
    "end": "650880"
  },
  {
    "text": "consuming",
    "start": "650880",
    "end": "653600"
  },
  {
    "text": "how we solve this problem we used the",
    "start": "653600",
    "end": "655760"
  },
  {
    "text": "init container uh so as soon as an",
    "start": "655760",
    "end": "659519"
  },
  {
    "text": "increase in replicas is made in",
    "start": "659519",
    "end": "662160"
  },
  {
    "text": "Kubernetes stateful set a new pod gets",
    "start": "662160",
    "end": "666079"
  },
  {
    "text": "scheduled and is assigned to a",
    "start": "666079",
    "end": "667920"
  },
  {
    "text": "Kubernetes",
    "start": "667920",
    "end": "669880"
  },
  {
    "text": "node and then we start an init container",
    "start": "669880",
    "end": "673600"
  },
  {
    "text": "but before jumping on the actions of the",
    "start": "673600",
    "end": "675760"
  },
  {
    "text": "init containers a few things that needs",
    "start": "675760",
    "end": "679040"
  },
  {
    "text": "to be considered here",
    "start": "679040",
    "end": "681760"
  },
  {
    "text": "uh as we mentioned previously that",
    "start": "681760",
    "end": "683680"
  },
  {
    "text": "streaming of this data can be time",
    "start": "683680",
    "end": "686079"
  },
  {
    "text": "consuming especially for large clusters",
    "start": "686079",
    "end": "688720"
  },
  {
    "text": "so we need to minimize the",
    "start": "688720",
    "end": "692279"
  },
  {
    "text": "disruptions and at least what we can do",
    "start": "692279",
    "end": "694880"
  },
  {
    "text": "is that we can eliminate the voluntary",
    "start": "694880",
    "end": "699000"
  },
  {
    "text": "disruptions and this can be done by",
    "start": "699000",
    "end": "701680"
  },
  {
    "text": "appropriately setting the max",
    "start": "701680",
    "end": "703279"
  },
  {
    "text": "unavailable property in the port",
    "start": "703279",
    "end": "704959"
  },
  {
    "text": "disruption",
    "start": "704959",
    "end": "707519"
  },
  {
    "text": "budgets at the same time not all",
    "start": "707880",
    "end": "710880"
  },
  {
    "text": "disruptions can be eliminated so there",
    "start": "710880",
    "end": "713440"
  },
  {
    "text": "is still an involuntary disruptions that",
    "start": "713440",
    "end": "715920"
  },
  {
    "text": "can happen any time for many different",
    "start": "715920",
    "end": "719519"
  },
  {
    "text": "reasons like due to hardware failures",
    "start": "719519",
    "end": "721760"
  },
  {
    "text": "network issues",
    "start": "721760",
    "end": "725079"
  },
  {
    "text": "etc so this means that we should be",
    "start": "725079",
    "end": "728399"
  },
  {
    "text": "prepared the init container code should",
    "start": "728399",
    "end": "730800"
  },
  {
    "text": "have some preparation to handle those",
    "start": "730800",
    "end": "732880"
  },
  {
    "text": "cases as well",
    "start": "732880",
    "end": "735760"
  },
  {
    "text": "the another thing to consider is that",
    "start": "735760",
    "end": "738720"
  },
  {
    "text": "each time a port restarts the init",
    "start": "738720",
    "end": "741440"
  },
  {
    "text": "container will be run so the init code",
    "start": "741440",
    "end": "746000"
  },
  {
    "text": "should be only invoked when it's needed",
    "start": "746000",
    "end": "748800"
  },
  {
    "text": "and it should be item potent as",
    "start": "748800",
    "end": "751639"
  },
  {
    "text": "well so in order to ensure that within",
    "start": "751639",
    "end": "754880"
  },
  {
    "text": "the inate container we first check",
    "start": "754880",
    "end": "756560"
  },
  {
    "text": "whether the node is already bootstrapped",
    "start": "756560",
    "end": "759760"
  },
  {
    "text": "or not so we don't do rereaming again a",
    "start": "759760",
    "end": "764160"
  },
  {
    "text": "simple way of doing this is by checking",
    "start": "764160",
    "end": "766800"
  },
  {
    "text": "presence of data on the attached",
    "start": "766800",
    "end": "769200"
  },
  {
    "text": "persistent volumes uh if streaming got",
    "start": "769200",
    "end": "773360"
  },
  {
    "text": "interrupted previously by any hardware",
    "start": "773360",
    "end": "776720"
  },
  {
    "text": "failures or any other",
    "start": "776720",
    "end": "778440"
  },
  {
    "text": "disruptions we don't automatically",
    "start": "778440",
    "end": "781200"
  },
  {
    "text": "handle it for this particular use case",
    "start": "781200",
    "end": "784560"
  },
  {
    "text": "instead we let our on call person to",
    "start": "784560",
    "end": "787839"
  },
  {
    "text": "reset the storage and make a decision",
    "start": "787839",
    "end": "790639"
  },
  {
    "text": "because such events in our experience",
    "start": "790639",
    "end": "793279"
  },
  {
    "text": "has been",
    "start": "793279",
    "end": "795760"
  },
  {
    "text": "rare next Cassendra process is started",
    "start": "796440",
    "end": "800320"
  },
  {
    "text": "in init container but we with CDC",
    "start": "800320",
    "end": "804200"
  },
  {
    "text": "disabled so any data that is streamed",
    "start": "804200",
    "end": "808240"
  },
  {
    "text": "from the peers it will not be uh",
    "start": "808240",
    "end": "812320"
  },
  {
    "text": "streamed into",
    "start": "812320",
    "end": "813720"
  },
  {
    "text": "Kafka now you might be considering that",
    "start": "813720",
    "end": "816560"
  },
  {
    "text": "any new rides that are happening at this",
    "start": "816560",
    "end": "819800"
  },
  {
    "text": "time they might be",
    "start": "819800",
    "end": "822440"
  },
  {
    "text": "lost but because we have a replication",
    "start": "822440",
    "end": "825279"
  },
  {
    "text": "factor of three so even if one node is",
    "start": "825279",
    "end": "828000"
  },
  {
    "text": "not processing uh any data it means that",
    "start": "828000",
    "end": "832480"
  },
  {
    "text": "there will be another two that are",
    "start": "832480",
    "end": "834320"
  },
  {
    "text": "processing the same set of data at the",
    "start": "834320",
    "end": "836240"
  },
  {
    "text": "same time so none of the data gets",
    "start": "836240",
    "end": "839079"
  },
  {
    "text": "lost and we have some readiness script",
    "start": "839079",
    "end": "841920"
  },
  {
    "text": "that indicate once the streaming is",
    "start": "841920",
    "end": "844079"
  },
  {
    "text": "complete and Cassendra is ready to serve",
    "start": "844079",
    "end": "846240"
  },
  {
    "text": "the traffic in the so once we get a",
    "start": "846240",
    "end": "850959"
  },
  {
    "text": "green signal uh Cassendra has joined the",
    "start": "850959",
    "end": "853600"
  },
  {
    "text": "ring we stop Cassendra process",
    "start": "853600",
    "end": "856480"
  },
  {
    "text": "gracefully and exit the init container",
    "start": "856480",
    "end": "860160"
  },
  {
    "text": "uh the reason why graceful stopping of",
    "start": "860160",
    "end": "863519"
  },
  {
    "text": "stateful workload is important because",
    "start": "863519",
    "end": "865120"
  },
  {
    "text": "any abrupt termination can lead to data",
    "start": "865120",
    "end": "870079"
  },
  {
    "text": "corruptions so once we exited from the",
    "start": "872440",
    "end": "875279"
  },
  {
    "text": "init container Cassender started back in",
    "start": "875279",
    "end": "877440"
  },
  {
    "text": "the main container but with CDC enabled",
    "start": "877440",
    "end": "880079"
  },
  {
    "text": "from this point onward we start",
    "start": "880079",
    "end": "882000"
  },
  {
    "text": "processing the CDC events in the CDC raw",
    "start": "882000",
    "end": "885399"
  },
  {
    "text": "directory and in this way careful usage",
    "start": "885399",
    "end": "889920"
  },
  {
    "text": "of init container enabled us to",
    "start": "889920",
    "end": "892639"
  },
  {
    "text": "horizontally scale the cluster without",
    "start": "892639",
    "end": "894480"
  },
  {
    "text": "much human",
    "start": "894480",
    "end": "895880"
  },
  {
    "text": "involvement along with that we also have",
    "start": "895880",
    "end": "898800"
  },
  {
    "text": "less duplicates and the scale up time",
    "start": "898800",
    "end": "902000"
  },
  {
    "text": "also was",
    "start": "902000",
    "end": "904880"
  },
  {
    "text": "reduced the second case where we used in",
    "start": "905720",
    "end": "910320"
  },
  {
    "text": "it containers was Cassandra upgrade this",
    "start": "910320",
    "end": "913680"
  },
  {
    "text": "was a project we did last year",
    "start": "913680",
    "end": "916480"
  },
  {
    "text": "and when we upgraded our entire",
    "start": "916480",
    "end": "918560"
  },
  {
    "text": "Cassendra fleet from 311 to 4.1 version",
    "start": "918560",
    "end": "923760"
  },
  {
    "text": "uh however there were two challenges",
    "start": "923760",
    "end": "926320"
  },
  {
    "text": "faced during the upgrade uh but before",
    "start": "926320",
    "end": "929120"
  },
  {
    "text": "talking about specifics some",
    "start": "929120",
    "end": "931040"
  },
  {
    "text": "terminologies to explain here so",
    "start": "931040",
    "end": "934240"
  },
  {
    "text": "typically from operational perspective",
    "start": "934240",
    "end": "936240"
  },
  {
    "text": "node tool status command in Cassendra",
    "start": "936240",
    "end": "938720"
  },
  {
    "text": "helps you view the state of different",
    "start": "938720",
    "end": "940320"
  },
  {
    "text": "nodes inside Cassendra",
    "start": "940320",
    "end": "943079"
  },
  {
    "text": "cluster and the output of node tool",
    "start": "943079",
    "end": "946880"
  },
  {
    "text": "status uh the first alphabet here",
    "start": "946880",
    "end": "949279"
  },
  {
    "text": "determines the state of the cluster",
    "start": "949279",
    "end": "951199"
  },
  {
    "text": "whether the node is up or down and the",
    "start": "951199",
    "end": "954880"
  },
  {
    "text": "second alphabet tells whether the node",
    "start": "954880",
    "end": "956639"
  },
  {
    "text": "has joined the ring or not",
    "start": "956639",
    "end": "959880"
  },
  {
    "text": "so there can be many states but a few of",
    "start": "959880",
    "end": "963519"
  },
  {
    "text": "them just discussing like un means that",
    "start": "963519",
    "end": "966160"
  },
  {
    "text": "it's up and normal and dn refers to the",
    "start": "966160",
    "end": "969759"
  },
  {
    "text": "node is down but it has joined the ring",
    "start": "969759",
    "end": "971759"
  },
  {
    "text": "and it was normal",
    "start": "971759",
    "end": "974240"
  },
  {
    "text": "so for the upgrade we followed",
    "start": "974240",
    "end": "976680"
  },
  {
    "text": "the standard rolling upgrade process",
    "start": "976680",
    "end": "980079"
  },
  {
    "text": "upgrading one node at a time but this",
    "start": "980079",
    "end": "982160"
  },
  {
    "text": "failed for us and more technical details",
    "start": "982160",
    "end": "985800"
  },
  {
    "text": "about why it failed uh has been captured",
    "start": "985800",
    "end": "989360"
  },
  {
    "text": "on the Jira ticket in the open source",
    "start": "989360",
    "end": "992440"
  },
  {
    "text": "community but I would try to explain a",
    "start": "992440",
    "end": "995440"
  },
  {
    "text": "problem with some smaller examples uh",
    "start": "995440",
    "end": "998720"
  },
  {
    "text": "let's consider that we have a 311",
    "start": "998720",
    "end": "1001759"
  },
  {
    "text": "Cassandra cluster with three nodes",
    "start": "1001759",
    "end": "1004079"
  },
  {
    "text": "having X Y and Z in the last octets of",
    "start": "1004079",
    "end": "1008639"
  },
  {
    "text": "their",
    "start": "1008639",
    "end": "1010040"
  },
  {
    "text": "IPs in our environment we don't use",
    "start": "1010040",
    "end": "1013759"
  },
  {
    "text": "static IP assignments so as pods are",
    "start": "1013759",
    "end": "1017160"
  },
  {
    "text": "restarted the IPs can change and each",
    "start": "1017160",
    "end": "1020880"
  },
  {
    "text": "time it's assigned whatever IP is",
    "start": "1020880",
    "end": "1023199"
  },
  {
    "text": "available in the pool",
    "start": "1023199",
    "end": "1025678"
  },
  {
    "text": "so as shown in the diagram the Cassendra",
    "start": "1025679",
    "end": "1029360"
  },
  {
    "text": "node with",
    "start": "1029360",
    "end": "1030600"
  },
  {
    "text": "X.X.X.Y IP after the upgrade got",
    "start": "1030600",
    "end": "1033480"
  },
  {
    "text": "assigned",
    "start": "1033480",
    "end": "1036480"
  },
  {
    "text": "IPX.x.x.a the problem that happened is",
    "start": "1036760",
    "end": "1039120"
  },
  {
    "text": "that new upgrade new upgraded node",
    "start": "1039120",
    "end": "1042319"
  },
  {
    "text": "wasn't accepted as a replacement by the",
    "start": "1042319",
    "end": "1045120"
  },
  {
    "text": "peers that were running 311 version and",
    "start": "1045120",
    "end": "1048240"
  },
  {
    "text": "this was because there was some",
    "start": "1048240",
    "end": "1050080"
  },
  {
    "text": "exceptions that happened during the",
    "start": "1050080",
    "end": "1052000"
  },
  {
    "text": "gossip exchange uh",
    "start": "1052000",
    "end": "1056240"
  },
  {
    "text": "mechanism so we did some more",
    "start": "1056360",
    "end": "1059000"
  },
  {
    "text": "experimentation and then we observed",
    "start": "1059000",
    "end": "1062240"
  },
  {
    "text": "that if both IPS and version they change",
    "start": "1062240",
    "end": "1065799"
  },
  {
    "text": "together the upgraded node is not",
    "start": "1065799",
    "end": "1069320"
  },
  {
    "text": "recognized but if we can make these two",
    "start": "1069320",
    "end": "1073280"
  },
  {
    "text": "things change gradually the process",
    "start": "1073280",
    "end": "1077200"
  },
  {
    "text": "would work fine for",
    "start": "1077200",
    "end": "1079960"
  },
  {
    "text": "us so as part of this solutions we",
    "start": "1079960",
    "end": "1082880"
  },
  {
    "text": "leverage the init containers to make it",
    "start": "1082880",
    "end": "1085400"
  },
  {
    "text": "gradual and so this is the last state of",
    "start": "1085400",
    "end": "1089600"
  },
  {
    "text": "Cassendra nodes prior to upgrade with",
    "start": "1089600",
    "end": "1092480"
  },
  {
    "text": "the version running 311 and IP is x.x.y",
    "start": "1092480",
    "end": "1098200"
  },
  {
    "text": "after a restart of pod during the",
    "start": "1098880",
    "end": "1101760"
  },
  {
    "text": "rolling upgrade a new IP gets assigned",
    "start": "1101760",
    "end": "1104799"
  },
  {
    "text": "to the pod as soon as it is",
    "start": "1104799",
    "end": "1108039"
  },
  {
    "text": "scheduled so however here we start",
    "start": "1108039",
    "end": "1111440"
  },
  {
    "text": "Cassendra first with older 3.11 version",
    "start": "1111440",
    "end": "1114640"
  },
  {
    "text": "in init container so that the peers can",
    "start": "1114640",
    "end": "1118080"
  },
  {
    "text": "recognize that this new IP is replacing",
    "start": "1118080",
    "end": "1122880"
  },
  {
    "text": "the older",
    "start": "1122880",
    "end": "1124120"
  },
  {
    "text": "one so one only one of the attributes",
    "start": "1124120",
    "end": "1127120"
  },
  {
    "text": "gets changed we wait here till Cassendra",
    "start": "1127120",
    "end": "1129919"
  },
  {
    "text": "node is ready to serve the traffic and",
    "start": "1129919",
    "end": "1132400"
  },
  {
    "text": "then we gracefully terminate this",
    "start": "1132400",
    "end": "1133919"
  },
  {
    "text": "workload and exit the inate",
    "start": "1133919",
    "end": "1137160"
  },
  {
    "text": "container then later",
    "start": "1137160",
    "end": "1140120"
  },
  {
    "text": "we started out Cassendra with 4.1",
    "start": "1140120",
    "end": "1143200"
  },
  {
    "text": "version in the regular container now",
    "start": "1143200",
    "end": "1146320"
  },
  {
    "text": "because the network stack is shared the",
    "start": "1146320",
    "end": "1149039"
  },
  {
    "text": "IP assignment doesn't get changed across",
    "start": "1149039",
    "end": "1152080"
  },
  {
    "text": "containers it remains consistent",
    "start": "1152080",
    "end": "1156519"
  },
  {
    "text": "so in this way once this new node was",
    "start": "1156880",
    "end": "1160400"
  },
  {
    "text": "brought back up it was successfully",
    "start": "1160400",
    "end": "1163360"
  },
  {
    "text": "joined the ring and we had a consistent",
    "start": "1163360",
    "end": "1165919"
  },
  {
    "text": "node tool status",
    "start": "1165919",
    "end": "1168280"
  },
  {
    "text": "everywhere so you can think that how",
    "start": "1168280",
    "end": "1171919"
  },
  {
    "text": "careful utilization of init containers",
    "start": "1171919",
    "end": "1174240"
  },
  {
    "text": "we were able to upgrade Cassendra nodes",
    "start": "1174240",
    "end": "1177559"
  },
  {
    "text": "seamlessly and we followed this",
    "start": "1177559",
    "end": "1180000"
  },
  {
    "text": "procedure for our entire fleet with 70",
    "start": "1180000",
    "end": "1183440"
  },
  {
    "text": "plus clusters and more than thousand",
    "start": "1183440",
    "end": "1185440"
  },
  {
    "text": "nodes in",
    "start": "1185440",
    "end": "1187720"
  },
  {
    "text": "them the secondary problem that we faced",
    "start": "1187720",
    "end": "1190400"
  },
  {
    "text": "during upgrade was related to SS table",
    "start": "1190400",
    "end": "1193280"
  },
  {
    "text": "format versions",
    "start": "1193280",
    "end": "1196000"
  },
  {
    "text": "so DSS table format versions can change",
    "start": "1196000",
    "end": "1200240"
  },
  {
    "text": "with Cassendra versions uh in the table",
    "start": "1200240",
    "end": "1203120"
  },
  {
    "text": "you can see that there is an NV format",
    "start": "1203120",
    "end": "1206160"
  },
  {
    "text": "in 4.1 version which used to",
    "start": "1206160",
    "end": "1209640"
  },
  {
    "text": "be ME in the 3.11.13 version and",
    "start": "1209640",
    "end": "1214080"
  },
  {
    "text": "similarly it used to be MD and MC",
    "start": "1214080",
    "end": "1216799"
  },
  {
    "text": "respectively in earlier 311 versions",
    "start": "1216799",
    "end": "1220320"
  },
  {
    "text": "so usually from read perspective",
    "start": "1220320",
    "end": "1222480"
  },
  {
    "text": "adjacent major versions support reading",
    "start": "1222480",
    "end": "1224640"
  },
  {
    "text": "from older SS tables but there aren't",
    "start": "1224640",
    "end": "1227440"
  },
  {
    "text": "any performance guarantees around",
    "start": "1227440",
    "end": "1229799"
  },
  {
    "text": "it so it's recommended as soon as you",
    "start": "1229799",
    "end": "1232720"
  },
  {
    "text": "upgrade it up upgrade the node upgrade",
    "start": "1232720",
    "end": "1235360"
  },
  {
    "text": "the SS tables as",
    "start": "1235360",
    "end": "1237240"
  },
  {
    "text": "well uh in pre4.x versions we always",
    "start": "1237240",
    "end": "1242000"
  },
  {
    "text": "have to invoke a node tool upgrade",
    "start": "1242000",
    "end": "1244320"
  },
  {
    "text": "command uh but with 4.x X versions there",
    "start": "1244320",
    "end": "1248640"
  },
  {
    "text": "are some there is some automated ways uh",
    "start": "1248640",
    "end": "1251919"
  },
  {
    "text": "but the problem with those automated",
    "start": "1251919",
    "end": "1253600"
  },
  {
    "text": "ways is that you have less control on",
    "start": "1253600",
    "end": "1256559"
  },
  {
    "text": "how many nodes the upgrade SS tables",
    "start": "1256559",
    "end": "1258720"
  },
  {
    "text": "would be",
    "start": "1258720",
    "end": "1259640"
  },
  {
    "text": "running so there can potentially be some",
    "start": "1259640",
    "end": "1262880"
  },
  {
    "text": "disk pressure created because when we",
    "start": "1262880",
    "end": "1264960"
  },
  {
    "text": "are running upgrade SS tables each SS",
    "start": "1264960",
    "end": "1268640"
  },
  {
    "text": "table gets rewritten into the newer",
    "start": "1268640",
    "end": "1271960"
  },
  {
    "text": "format so in some cases you may want to",
    "start": "1271960",
    "end": "1275120"
  },
  {
    "text": "run this in a controlled fashion",
    "start": "1275120",
    "end": "1278120"
  },
  {
    "text": "our solution was quite similar to the",
    "start": "1278120",
    "end": "1280720"
  },
  {
    "text": "previous one relying on the init",
    "start": "1280720",
    "end": "1282600"
  },
  {
    "text": "containers we triggered the upgrade as a",
    "start": "1282600",
    "end": "1285760"
  },
  {
    "text": "stable process by a spec change in the",
    "start": "1285760",
    "end": "1289039"
  },
  {
    "text": "Elswa config that gets converted into a",
    "start": "1289039",
    "end": "1291919"
  },
  {
    "text": "custom resource the operator then makes",
    "start": "1291919",
    "end": "1295600"
  },
  {
    "text": "the stateful set change and then the",
    "start": "1295600",
    "end": "1298000"
  },
  {
    "text": "port gets restarted in a rolling fashion",
    "start": "1298000",
    "end": "1300880"
  },
  {
    "text": "so the first thing when the port gets",
    "start": "1300880",
    "end": "1302960"
  },
  {
    "text": "started is we start out init container",
    "start": "1302960",
    "end": "1306240"
  },
  {
    "text": "and check whether there are old",
    "start": "1306240",
    "end": "1308960"
  },
  {
    "text": "formatted as a stable versions present",
    "start": "1308960",
    "end": "1311120"
  },
  {
    "text": "or not if not we exit the init container",
    "start": "1311120",
    "end": "1315360"
  },
  {
    "text": "else we start cassender process and wait",
    "start": "1315360",
    "end": "1318799"
  },
  {
    "text": "for it to become",
    "start": "1318799",
    "end": "1320200"
  },
  {
    "text": "ready uh once the cassender process is",
    "start": "1320200",
    "end": "1323120"
  },
  {
    "text": "ready we upgrade the SS tables until it",
    "start": "1323120",
    "end": "1326799"
  },
  {
    "text": "completes now this process can also be",
    "start": "1326799",
    "end": "1329840"
  },
  {
    "text": "long running depending upon the amount",
    "start": "1329840",
    "end": "1332240"
  },
  {
    "text": "of data that is present on the",
    "start": "1332240",
    "end": "1335000"
  },
  {
    "text": "disk and once everything gets complete",
    "start": "1335000",
    "end": "1338159"
  },
  {
    "text": "we exit the inate container and start",
    "start": "1338159",
    "end": "1339919"
  },
  {
    "text": "Cassandra as a as a regular",
    "start": "1339919",
    "end": "1343159"
  },
  {
    "text": "container again we ensure item potency",
    "start": "1343159",
    "end": "1346159"
  },
  {
    "text": "if a pod is terminated it should have",
    "start": "1346159",
    "end": "1348080"
  },
  {
    "text": "capability to",
    "start": "1348080",
    "end": "1349400"
  },
  {
    "text": "resume uh so this approach helped us in",
    "start": "1349400",
    "end": "1353039"
  },
  {
    "text": "limiting the disk pressure to at max a",
    "start": "1353039",
    "end": "1356159"
  },
  {
    "text": "single node",
    "start": "1356159",
    "end": "1358159"
  },
  {
    "text": "and everyone loves seamless upgrades and",
    "start": "1358159",
    "end": "1362240"
  },
  {
    "text": "init container ensured that it was the",
    "start": "1362240",
    "end": "1364400"
  },
  {
    "text": "case for",
    "start": "1364400",
    "end": "1366880"
  },
  {
    "text": "us the last case I would like to talk",
    "start": "1367480",
    "end": "1370159"
  },
  {
    "text": "about is recovering cluster from",
    "start": "1370159",
    "end": "1375280"
  },
  {
    "text": "backups we use Medusa which is an",
    "start": "1375480",
    "end": "1378640"
  },
  {
    "text": "open-source tool for copying SS table",
    "start": "1378640",
    "end": "1381440"
  },
  {
    "text": "formatted data on disk to S3",
    "start": "1381440",
    "end": "1386000"
  },
  {
    "text": "and the picture on the right side shows",
    "start": "1386000",
    "end": "1388640"
  },
  {
    "text": "the architecture of Medusa so in",
    "start": "1388640",
    "end": "1390799"
  },
  {
    "text": "addition to the assistable files there",
    "start": "1390799",
    "end": "1392559"
  },
  {
    "text": "are some manifest files that contains",
    "start": "1392559",
    "end": "1394880"
  },
  {
    "text": "metadata and information",
    "start": "1394880",
    "end": "1397080"
  },
  {
    "text": "about like the time stamp at which",
    "start": "1397080",
    "end": "1400000"
  },
  {
    "text": "backup was taken backup type",
    "start": "1400000",
    "end": "1403640"
  },
  {
    "text": "etc in our environment we support both",
    "start": "1403640",
    "end": "1406799"
  },
  {
    "text": "the full mode and differential modes but",
    "start": "1406799",
    "end": "1409679"
  },
  {
    "text": "we try to leverage differential one as",
    "start": "1409679",
    "end": "1412480"
  },
  {
    "text": "much as we",
    "start": "1412480",
    "end": "1414360"
  },
  {
    "text": "can so this backup process runs",
    "start": "1414360",
    "end": "1417360"
  },
  {
    "text": "periodically in sidecar in our",
    "start": "1417360",
    "end": "1421840"
  },
  {
    "text": "case when we want to restore a cluster",
    "start": "1422120",
    "end": "1425039"
  },
  {
    "text": "we configure a new Cassandra cluster",
    "start": "1425039",
    "end": "1428720"
  },
  {
    "text": "with some additional properties that",
    "start": "1428720",
    "end": "1430880"
  },
  {
    "text": "lets us know about the cluster and",
    "start": "1430880",
    "end": "1433360"
  },
  {
    "text": "backup",
    "start": "1433360",
    "end": "1434880"
  },
  {
    "text": "uh as shown you can see on the slide so",
    "start": "1434880",
    "end": "1437760"
  },
  {
    "text": "the backup identifier is the timestamp",
    "start": "1437760",
    "end": "1440799"
  },
  {
    "text": "in our case at a minute resolution level",
    "start": "1440799",
    "end": "1444480"
  },
  {
    "text": "uh so based on these properties a new",
    "start": "1444480",
    "end": "1446720"
  },
  {
    "text": "Cassendra cluster gets created then init",
    "start": "1446720",
    "end": "1449600"
  },
  {
    "text": "containers are started uh and in those",
    "start": "1449600",
    "end": "1452400"
  },
  {
    "text": "init containers we pull data from",
    "start": "1452400",
    "end": "1455240"
  },
  {
    "text": "S3 if we follow a sequential approach",
    "start": "1455240",
    "end": "1458480"
  },
  {
    "text": "one node at a time it would take much",
    "start": "1458480",
    "end": "1460320"
  },
  {
    "text": "longer so we expedite things by making",
    "start": "1460320",
    "end": "1463039"
  },
  {
    "text": "use of parallel pod management policy",
    "start": "1463039",
    "end": "1466000"
  },
  {
    "text": "that allows all pods to come up in",
    "start": "1466000",
    "end": "1468159"
  },
  {
    "text": "parallel and start pulling data from",
    "start": "1468159",
    "end": "1472000"
  },
  {
    "text": "S3 and once all nodes have completely",
    "start": "1472440",
    "end": "1475760"
  },
  {
    "text": "pulled data onto the disk we exit the",
    "start": "1475760",
    "end": "1478320"
  },
  {
    "text": "init container and start",
    "start": "1478320",
    "end": "1480360"
  },
  {
    "text": "Cassendra uh and once all Cassendra",
    "start": "1480360",
    "end": "1483360"
  },
  {
    "text": "nodes are up running in regular",
    "start": "1483360",
    "end": "1485840"
  },
  {
    "text": "container they join together to form the",
    "start": "1485840",
    "end": "1488840"
  },
  {
    "text": "cluster so in this way we ensured our",
    "start": "1488840",
    "end": "1491279"
  },
  {
    "text": "restore process is fully",
    "start": "1491279",
    "end": "1495159"
  },
  {
    "text": "automated but the requirements of",
    "start": "1495159",
    "end": "1498240"
  },
  {
    "text": "disaster",
    "start": "1498240",
    "end": "1499480"
  },
  {
    "text": "recovery is quite unique",
    "start": "1499480",
    "end": "1502880"
  },
  {
    "text": "you the process is usually rarely needed",
    "start": "1502880",
    "end": "1506320"
  },
  {
    "text": "but when it's needed there is urgency",
    "start": "1506320",
    "end": "1508880"
  },
  {
    "text": "you don't have",
    "start": "1508880",
    "end": "1510039"
  },
  {
    "text": "time so we build some automation around",
    "start": "1510039",
    "end": "1514720"
  },
  {
    "text": "the inate containers to ensure that our",
    "start": "1514720",
    "end": "1517120"
  },
  {
    "text": "process always remains",
    "start": "1517120",
    "end": "1520720"
  },
  {
    "text": "functional yeah this moves us to our",
    "start": "1521400",
    "end": "1524000"
  },
  {
    "text": "conclusions uh as you can see that init",
    "start": "1524000",
    "end": "1527200"
  },
  {
    "text": "containers are quite handy for real",
    "start": "1527200",
    "end": "1529840"
  },
  {
    "text": "stateful workload applications s a few",
    "start": "1529840",
    "end": "1532640"
  },
  {
    "text": "takeaways",
    "start": "1532640",
    "end": "1534679"
  },
  {
    "text": "here uh the first one is about ensuring",
    "start": "1534679",
    "end": "1538240"
  },
  {
    "text": "that init containers are should be item",
    "start": "1538240",
    "end": "1541279"
  },
  {
    "text": "potent any failures that can happen any",
    "start": "1541279",
    "end": "1545120"
  },
  {
    "text": "time and for any reasons so you need to",
    "start": "1545120",
    "end": "1548640"
  },
  {
    "text": "make sure that there is some failure",
    "start": "1548640",
    "end": "1550640"
  },
  {
    "text": "mode",
    "start": "1550640",
    "end": "1551720"
  },
  {
    "text": "handling also init container get code",
    "start": "1551720",
    "end": "1555919"
  },
  {
    "text": "can get launched each time when a pod",
    "start": "1555919",
    "end": "1559679"
  },
  {
    "text": "gets restarted uh there can be",
    "start": "1559679",
    "end": "1562320"
  },
  {
    "text": "potentially two cases either you want to",
    "start": "1562320",
    "end": "1564960"
  },
  {
    "text": "run init container code every time the",
    "start": "1564960",
    "end": "1567279"
  },
  {
    "text": "pod restarts like setting up a network",
    "start": "1567279",
    "end": "1570400"
  },
  {
    "text": "or something similar to it or the other",
    "start": "1570400",
    "end": "1573279"
  },
  {
    "text": "case could be that you run the inate",
    "start": "1573279",
    "end": "1576159"
  },
  {
    "text": "container code based on the persistent",
    "start": "1576159",
    "end": "1578720"
  },
  {
    "text": "state on the disk in our case all of the",
    "start": "1578720",
    "end": "1582559"
  },
  {
    "text": "examples that we discussed actually were",
    "start": "1582559",
    "end": "1585039"
  },
  {
    "text": "related to the later",
    "start": "1585039",
    "end": "1587799"
  },
  {
    "text": "case so whenever we don't want to run",
    "start": "1587799",
    "end": "1591440"
  },
  {
    "text": "init container code uh we should be",
    "start": "1591440",
    "end": "1593600"
  },
  {
    "text": "handling it inside the init container so",
    "start": "1593600",
    "end": "1595600"
  },
  {
    "text": "that it can quickly uh",
    "start": "1595600",
    "end": "1598120"
  },
  {
    "text": "exit another takeaway is minimizing the",
    "start": "1598120",
    "end": "1602760"
  },
  {
    "text": "disruptions so if we treat disruption as",
    "start": "1602760",
    "end": "1605919"
  },
  {
    "text": "a",
    "start": "1605919",
    "end": "1607080"
  },
  {
    "text": "process rather than an event so it means",
    "start": "1607080",
    "end": "1610000"
  },
  {
    "text": "that we can have some degree of control",
    "start": "1610000",
    "end": "1613360"
  },
  {
    "text": "over it at least on the voluntary",
    "start": "1613360",
    "end": "1615760"
  },
  {
    "text": "disruption",
    "start": "1615760",
    "end": "1618279"
  },
  {
    "text": "side so we should be reducing those",
    "start": "1618279",
    "end": "1622720"
  },
  {
    "text": "disruptions where the pod disruptions",
    "start": "1622720",
    "end": "1624559"
  },
  {
    "text": "budget and this is particularly useful",
    "start": "1624559",
    "end": "1627039"
  },
  {
    "text": "if the init container code is running",
    "start": "1627039",
    "end": "1629120"
  },
  {
    "text": "for long though we try to keep the",
    "start": "1629120",
    "end": "1631159"
  },
  {
    "text": "duration as short as",
    "start": "1631159",
    "end": "1634840"
  },
  {
    "text": "possible the third takeaway is about uh",
    "start": "1634840",
    "end": "1638559"
  },
  {
    "text": "gracefully exiting the init container we",
    "start": "1638559",
    "end": "1641919"
  },
  {
    "text": "know that the main containers don't",
    "start": "1641919",
    "end": "1644320"
  },
  {
    "text": "start until init container completes",
    "start": "1644320",
    "end": "1647039"
  },
  {
    "text": "successfully",
    "start": "1647039",
    "end": "1648960"
  },
  {
    "text": "at the same",
    "start": "1648960",
    "end": "1650440"
  },
  {
    "text": "time any abrupt termination of stateful",
    "start": "1650440",
    "end": "1654000"
  },
  {
    "text": "workload should be avoided because then",
    "start": "1654000",
    "end": "1656640"
  },
  {
    "text": "it can lead to data",
    "start": "1656640",
    "end": "1660320"
  },
  {
    "text": "corruption so with this I would like to",
    "start": "1661960",
    "end": "1664720"
  },
  {
    "text": "thank you for your time thanks for",
    "start": "1664720",
    "end": "1667640"
  },
  {
    "text": "listening and if you want to learn more",
    "start": "1667640",
    "end": "1669919"
  },
  {
    "text": "about our engineering systems you can",
    "start": "1669919",
    "end": "1672240"
  },
  {
    "text": "visit Yelp's engineering blog",
    "start": "1672240",
    "end": "1675520"
  },
  {
    "text": "along with that uh carrier opportunities",
    "start": "1675520",
    "end": "1678159"
  },
  {
    "text": "at Yelp you can explore on our carriers",
    "start": "1678159",
    "end": "1680960"
  },
  {
    "text": "page which is yelp.carriers",
    "start": "1680960",
    "end": "1683159"
  },
  {
    "text": "carriers and yep there is a QR code for",
    "start": "1683159",
    "end": "1687600"
  },
  {
    "text": "giving the",
    "start": "1687600",
    "end": "1689240"
  },
  {
    "text": "feedback and uh if we have some time I",
    "start": "1689240",
    "end": "1692799"
  },
  {
    "text": "think we have a couple of minutes we can",
    "start": "1692799",
    "end": "1694559"
  },
  {
    "text": "take some questions",
    "start": "1694559",
    "end": "1697120"
  }
]