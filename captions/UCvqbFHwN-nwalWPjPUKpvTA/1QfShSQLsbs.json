[
  {
    "text": "hello my name is Kevin Clues and I'm a distinguished engineer at Nvidia I'm here today to talk about a new",
    "start": "799",
    "end": "6799"
  },
  {
    "text": "kubernetes feature called Dynamic resource allocation that will enable us to unlock the full potential of gpus for",
    "start": "6799",
    "end": "13320"
  },
  {
    "text": "AI workloads on kubernetes as a community we've already built a solid foundation for GPU support",
    "start": "13320",
    "end": "19600"
  },
  {
    "text": "in kubernetes and dynamic resource allocation is the feature we've been waiting for to take us to the next",
    "start": "19600",
    "end": "26400"
  },
  {
    "text": "level so what does GPU support in kubernetes look like today",
    "start": "26400",
    "end": "31599"
  },
  {
    "text": "well it requires a mix of host level components as well as kubernetes specific components to be deployed on",
    "start": "31599",
    "end": "37520"
  },
  {
    "text": "each node of a cluster that contains gpus where the host components consist of the Nvidia container toolkit and the",
    "start": "37520",
    "end": "44239"
  },
  {
    "text": "GPU driver itself and the kubernetes specific components consist of those shown here",
    "start": "44239",
    "end": "50840"
  },
  {
    "text": "where the K8 device plugin is the one that ultimately makes the gpus themselves visible to",
    "start": "50840",
    "end": "56760"
  },
  {
    "text": "kubernetes with these in place requesting access access to gpus takes the form of an extended resource called",
    "start": "56760",
    "end": "62920"
  },
  {
    "text": "nvidia.com GPU and specifying how many gpus you want access to under the hood the",
    "start": "62920",
    "end": "70000"
  },
  {
    "text": "scheduler kuet and KH device plug-in will coordinate to find a node where gpus are available schedule the Pod",
    "start": "70000",
    "end": "77520"
  },
  {
    "text": "there and inject the requested gpus into the container that requested them if you need access to a specific",
    "start": "77520",
    "end": "84680"
  },
  {
    "text": "type of GPU a combination of node labels and node selectors can help direct the schuer to a node containing the specific",
    "start": "84680",
    "end": "91520"
  },
  {
    "text": "type of GPU you are looking for in this case an a100 with 40 GB of GPU",
    "start": "91520",
    "end": "98840"
  },
  {
    "text": "memory you also have the ability to request a fraction of a GPU through technology called multi-instance gpus or",
    "start": "98840",
    "end": "106079"
  },
  {
    "text": "Mig for short in the example shown here we're requesting a MIG device that is 1/8 the",
    "start": "106079",
    "end": "112159"
  },
  {
    "text": "size of a full a100 GPU using the canonical naming Convention of mig-1 g.",
    "start": "112159",
    "end": "118840"
  },
  {
    "text": "5gb in the last year or so we also added support for sharing access to gpus",
    "start": "118840",
    "end": "125520"
  },
  {
    "text": "through over subscription on a perod basis cluster admins can configure the number of replicas they wish to over",
    "start": "125520",
    "end": "131840"
  },
  {
    "text": "subscribe each GPU and users request access to these shared gpus with the special do shared extension shown",
    "start": "131840",
    "end": "139680"
  },
  {
    "text": "here under the hood simple time slicing is used to swap one workload off and put",
    "start": "139680",
    "end": "145319"
  },
  {
    "text": "another workload on the GPU after some fixed amount of time",
    "start": "145319",
    "end": "151280"
  },
  {
    "text": "and this can be layered on top of the fractional GPU support provided by Mig as",
    "start": "151280",
    "end": "157120"
  },
  {
    "text": "well we also provide an alternative to time slicing through a technology called multiprocess service or MPS which",
    "start": "157120",
    "end": "164760"
  },
  {
    "text": "provides a software solution for running multiple jobs on top of a GPU or Meg device in",
    "start": "164760",
    "end": "172560"
  },
  {
    "text": "parallel unlike time slicing however there are some caveats to getting it to run properly as well as some extra",
    "start": "172560",
    "end": "178840"
  },
  {
    "text": "burden on the cluster admin to configure it properly with some changes to the system",
    "start": "178840",
    "end": "185519"
  },
  {
    "text": "software gpus can also be made available to Virtual machines such as cub and",
    "start": "185519",
    "end": "190959"
  },
  {
    "text": "Kata instead of running the Nvidia GPU driver directly on the host it is instead run inside each virtual machine",
    "start": "190959",
    "end": "197519"
  },
  {
    "text": "individually with the standard Linux vfio driver making the requested GPU Hardware visible to each",
    "start": "197519",
    "end": "204680"
  },
  {
    "text": "VM okay so if that is what we can do today what can't we do or at least not",
    "start": "204959",
    "end": "211000"
  },
  {
    "text": "do very well well first and foremost there is no support for having more than a single",
    "start": "211000",
    "end": "216959"
  },
  {
    "text": "GPU type on a given node as I mentioned before node selectors can be used to direct the scheduler at a node that has",
    "start": "216959",
    "end": "223400"
  },
  {
    "text": "a particular type of GPU but that node itself must have a homogeneous set of gpus to pick from we've talked about",
    "start": "223400",
    "end": "230280"
  },
  {
    "text": "enabling support for this by allowing admins to customize the name of the extended resource used to advertise each type of GPU on a node however the names",
    "start": "230280",
    "end": "238040"
  },
  {
    "text": "start to get overly complex and long the more properties you encode in them and we've never been able to agree upon the",
    "start": "238040",
    "end": "243120"
  },
  {
    "text": "right way to do this so it just hasn't happened one of the most requested",
    "start": "243120",
    "end": "249959"
  },
  {
    "text": "features we get is the ability to ask for a GPU subject to some complex set of constraints such as its minimum Cuda",
    "start": "249959",
    "end": "256680"
  },
  {
    "text": "compute capability Andor the total amount of memory it has available unfortunately node labels are",
    "start": "256680",
    "end": "263440"
  },
  {
    "text": "not sufficient to encode all of the constraints that users find useful and attempting to encode them in the name of",
    "start": "263440",
    "end": "269039"
  },
  {
    "text": "the resource itself has the same problem mentioned previously with supporting more than one GPU type per",
    "start": "269039",
    "end": "276080"
  },
  {
    "text": "node providing users with the ability to over subscribe gpus and share them among",
    "start": "277360",
    "end": "282479"
  },
  {
    "text": "containers was a huge win for many of our customers however there is no way to control exactly which container will get",
    "start": "282479",
    "end": "289360"
  },
  {
    "text": "access to which replica of which GPU limiting the overall usefulness of this feature for users with more stringent",
    "start": "289360",
    "end": "295600"
  },
  {
    "text": "sharing requirements sharing via MPS has these",
    "start": "295600",
    "end": "301039"
  },
  {
    "text": "same problems with the additional caveats mentioned previously of requiring a host level control demon to",
    "start": "301039",
    "end": "306639"
  },
  {
    "text": "be running as well as requiring all pods that access gpus to be run with host IPC",
    "start": "306639",
    "end": "313960"
  },
  {
    "text": "enabled I didn't go into the details of this before but there is currently no ability to dynamically provision M",
    "start": "314320",
    "end": "320400"
  },
  {
    "text": "devices based on incoming resource requests there are tricks you can do to add some level of dynamism to this",
    "start": "320400",
    "end": "326479"
  },
  {
    "text": "process but they are all shoehorned on top of the existing device plug-in API and involve complex logic to trick",
    "start": "326479",
    "end": "333240"
  },
  {
    "text": "kubernetes into doing the right thing meaning that the official K device plugin for gpus simply doesn't support",
    "start": "333240",
    "end": "339919"
  },
  {
    "text": "this feature there is also no way to dynamically associate some gpus with the",
    "start": "339919",
    "end": "346560"
  },
  {
    "text": "native Nvidia GPU driver and others with the vfio driver such that a mix of containers and VMS can be run on the",
    "start": "346560",
    "end": "353440"
  },
  {
    "text": "same underlying node with GPU support it's technically possible to split the gpus in this way way we just",
    "start": "353440",
    "end": "360560"
  },
  {
    "text": "don't have a good way to do it dynamically based on incoming resource requests and kubernetes and with the Advent of",
    "start": "360560",
    "end": "367039"
  },
  {
    "text": "confidential Computing and the hardware support we have for inware gpus this is",
    "start": "367039",
    "end": "372120"
  },
  {
    "text": "becoming an increasingly common ask for many of our customers and as you can imagine the",
    "start": "372120",
    "end": "379599"
  },
  {
    "text": "list of limitations goes on but this is where Dynamic resource allocation comes in in addition to",
    "start": "379599",
    "end": "386840"
  },
  {
    "text": "supporting all of the existing use cases covered by the GPU support we have in kubernetes today our Dr resource driver",
    "start": "386840",
    "end": "393680"
  },
  {
    "text": "for gpus addresses all of the limitations mentioned as well and the rest of this talk is dedicated to",
    "start": "393680",
    "end": "399759"
  },
  {
    "text": "showing exactly how this is done so I'll start off by giving a brief",
    "start": "399759",
    "end": "406680"
  },
  {
    "text": "overview of Dr itself followed by the specifics of the Dr resource driver for gpus we have implemented at",
    "start": "406680",
    "end": "413599"
  },
  {
    "text": "Nvidia I'll then jump right into a series of demos showcasing some of the more advanced features provided by this",
    "start": "413599",
    "end": "419039"
  },
  {
    "text": "driver as its name implies Dr is a new way of",
    "start": "419039",
    "end": "425639"
  },
  {
    "text": "requesting access to resources in kubernetes that has been available as an alpha feature since",
    "start": "425639",
    "end": "430960"
  },
  {
    "text": "126 it provides an alternative to the existing count-based interface of asking",
    "start": "430960",
    "end": "436080"
  },
  {
    "text": "for nvidia.com gpu2 for example and what makes it so powerful is",
    "start": "436080",
    "end": "443240"
  },
  {
    "text": "that it puts full control of the API used to request resources directly in the hands of the vendors developing",
    "start": "443240",
    "end": "449360"
  },
  {
    "text": "drivers against Dr where the key abstractions to keep in mind are that of the resource class and",
    "start": "449360",
    "end": "456080"
  },
  {
    "text": "the resource claim which are in tree apis referenced directly within a pod as well as their corresponding class",
    "start": "456080",
    "end": "463639"
  },
  {
    "text": "parameters and claim parameters which provide a means for vendors to bring their own custom apis to these",
    "start": "463639",
    "end": "471520"
  },
  {
    "text": "abstractions in this talk I won't go much further into the details of Dr itself and how it works under the hood",
    "start": "472240",
    "end": "478960"
  },
  {
    "text": "but I encourage you to watch my talk from cucon EU last April if you'd like to learn",
    "start": "478960",
    "end": "485240"
  },
  {
    "text": "more okay so assuming a cluster admin has enabled Dr and deployed the",
    "start": "486520",
    "end": "492039"
  },
  {
    "text": "necessary Dr resource driver for gpus onto a cluster what does a user actually have to do to request access to a",
    "start": "492039",
    "end": "499759"
  },
  {
    "text": "GPU well starting from the example we used previously of requesting access to two gpus the equivalent request under Dr",
    "start": "499759",
    "end": "507400"
  },
  {
    "text": "would look like this where instead of requesting the two gpus",
    "start": "507400",
    "end": "513719"
  },
  {
    "text": "from an extended resource called nvidia.com GPU you instead create a resource claim",
    "start": "513719",
    "end": "520240"
  },
  {
    "text": "template object pointing to the GPU nvidia.com resource",
    "start": "520240",
    "end": "526880"
  },
  {
    "text": "class you then reference that template twice inside a new resource claim",
    "start": "526880",
    "end": "532760"
  },
  {
    "text": "section of the podspec give each reference a local name",
    "start": "532760",
    "end": "538360"
  },
  {
    "text": "in this case GPU 0 and gpu1 and then reference those claims",
    "start": "538360",
    "end": "544519"
  },
  {
    "text": "from within a new resources. claim section of the container",
    "start": "544519",
    "end": "550079"
  },
  {
    "text": "spec since this is a template each reference will trigger a unique claim to be created under the hood resulting in",
    "start": "550079",
    "end": "557720"
  },
  {
    "text": "each reference pointing to a unique GPU just like we had with the traditional extended resource interface",
    "start": "557720",
    "end": "563760"
  },
  {
    "text": "shown on the left now this may feel overly boast for",
    "start": "563760",
    "end": "569600"
  },
  {
    "text": "the simple case of requesting exclusive access to a set of gpus but separating the Declaration of",
    "start": "569600",
    "end": "575800"
  },
  {
    "text": "the resource claim from its consumption means that we can naturally enable sharing of gpus in a controlled",
    "start": "575800",
    "end": "582640"
  },
  {
    "text": "way in the example shown here we reference a single resource claim for multiple containers within a pod",
    "start": "582640",
    "end": "589760"
  },
  {
    "text": "resulting in both containers getting access to the same underlying",
    "start": "589760",
    "end": "594839"
  },
  {
    "text": "GPU and by directly creating a global claim object as opposed to a",
    "start": "595399",
    "end": "601040"
  },
  {
    "text": "template we can reference this claim for multiple pods providing a natural",
    "start": "601040",
    "end": "606079"
  },
  {
    "text": "mechanism for control GPU sharing across pods as",
    "start": "606079",
    "end": "611040"
  },
  {
    "text": "well now this takes care of the limitation mentioned previously of not being able to control how oversubscribed",
    "start": "611360",
    "end": "617920"
  },
  {
    "text": "gpus get shared but what about the rest of the limitations well if you remember I",
    "start": "617920",
    "end": "624839"
  },
  {
    "text": "mentioned earlier that one of the most powerful features of Dr is the ability for vendors to Define their own API for",
    "start": "624839",
    "end": "630880"
  },
  {
    "text": "requesting resources and this comes in the form of class parameters and claim parameters attached to resource classes",
    "start": "630880",
    "end": "637320"
  },
  {
    "text": "and resource claims respectively where resource classes are used to define the set of available",
    "start": "637320",
    "end": "642519"
  },
  {
    "text": "resources themselves as defined by a cluster admin and resource claims are",
    "start": "642519",
    "end": "647639"
  },
  {
    "text": "used by end users to ultimately request access to those",
    "start": "647639",
    "end": "652959"
  },
  {
    "text": "resources since this talk is focused exclusively on the end user experience I won't won't go into the details of the",
    "start": "654040",
    "end": "660160"
  },
  {
    "text": "class parameters provided by nvidia's Dr resource driver for gpus but in general",
    "start": "660160",
    "end": "665519"
  },
  {
    "text": "they work similarly to the claim parameters I'm about to",
    "start": "665519",
    "end": "670079"
  },
  {
    "text": "describe so what do these claim parameters look like we're using the examples of the",
    "start": "671399",
    "end": "677760"
  },
  {
    "text": "resource claim and resource claim template from before you can basically just tack on an",
    "start": "677760",
    "end": "684079"
  },
  {
    "text": "extra section to these objects called parameters ref which pointed an object",
    "start": "684079",
    "end": "689440"
  },
  {
    "text": "that the Dr resource driver for your specific resource type knows how to",
    "start": "689440",
    "end": "694880"
  },
  {
    "text": "interpret where the simplest claim parameters supported by nvidia's Dr driver for gpus looks like",
    "start": "695279",
    "end": "701480"
  },
  {
    "text": "this a crd called GPU claim parameters with a count of how many gpus you want",
    "start": "701480",
    "end": "707200"
  },
  {
    "text": "allocated to the claim meaning that an alternative to our",
    "start": "707200",
    "end": "713480"
  },
  {
    "text": "original example of requesting two gpus being injected into a single container could have looked like this",
    "start": "713480",
    "end": "719959"
  },
  {
    "text": "this where our claim parameters object declares that we want two gpus to be allocated to the claim and we Plum list",
    "start": "719959",
    "end": "727320"
  },
  {
    "text": "through the resource claim template referenced by the Pod okay so how can we expand on this",
    "start": "727320",
    "end": "735240"
  },
  {
    "text": "claim parameters object to knock out the rest of the limitations listed previously well let's start with this",
    "start": "735240",
    "end": "741519"
  },
  {
    "text": "one better support for MPS a sharing section can be included",
    "start": "741519",
    "end": "747720"
  },
  {
    "text": "that defines the strategy with which gpus allocated to this claim should be shared and one of the strategies that",
    "start": "747720",
    "end": "752959"
  },
  {
    "text": "can be selected is MPS where an optional set of parameters",
    "start": "752959",
    "end": "758279"
  },
  {
    "text": "can be provided to customize how the MPS server does its partitioning and that's basically it",
    "start": "758279",
    "end": "764160"
  },
  {
    "text": "from an end users perspective as part of allocating the claim an MPS server will be automatically started in the",
    "start": "764160",
    "end": "770120"
  },
  {
    "text": "background with the parameters provided and torn down once the claim has been",
    "start": "770120",
    "end": "775920"
  },
  {
    "text": "deleted now it's also mentioning that the default sharing strategy of time slicing can also be provided here in",
    "start": "777240",
    "end": "783320"
  },
  {
    "text": "order to customize the duration of the time slice used were the values of default short medium and long ma",
    "start": "783320",
    "end": "791680"
  },
  {
    "text": "directly to the inputs you would provide to Nvidia SMI to control this from the command",
    "start": "791680",
    "end": "797560"
  },
  {
    "text": "line now the next ones we'll take a look at are these two and since the same set of fields in the GPU claim parameter",
    "start": "798360",
    "end": "805079"
  },
  {
    "text": "spec will allow us to overcome both of these limitations we'll consider them",
    "start": "805079",
    "end": "810560"
  },
  {
    "text": "together where the basic idea is to define a selector field which can either contain a single property such as the",
    "start": "810560",
    "end": "817839"
  },
  {
    "text": "GPU model you would like allocated to the claim or a list of properties anded",
    "start": "817839",
    "end": "823279"
  },
  {
    "text": "or ored together recursively up to a nesting level of",
    "start": "823279",
    "end": "828720"
  },
  {
    "text": "three as a concrete example consider the following selector which reading from",
    "start": "829519",
    "end": "835240"
  },
  {
    "text": "top to bottom says give me either a T4 or a",
    "start": "835240",
    "end": "841040"
  },
  {
    "text": "V100 and if you give me a V100 make sure it has less than or equal to 16",
    "start": "841040",
    "end": "846480"
  },
  {
    "text": "gigabytes of memory such a query would be useful for example to find an appropriate GPU to",
    "start": "846480",
    "end": "853279"
  },
  {
    "text": "run an inference server on now the set of constraints that be",
    "start": "853279",
    "end": "859120"
  },
  {
    "text": "that can be provided here are basically anything you would be able to query about the GPU using Nvidia",
    "start": "859120",
    "end": "864839"
  },
  {
    "text": "SMI and under the hood the Dr driver for gpus will not discriminate where these",
    "start": "864839",
    "end": "870000"
  },
  {
    "text": "gpus come from meaning that you can now Place different types of gpus on the",
    "start": "870000",
    "end": "875560"
  },
  {
    "text": "same node if desired now these last two limitations",
    "start": "875560",
    "end": "881920"
  },
  {
    "text": "are interesting because we've chosen to overcome them a little differently than the others specifically by introducing a",
    "start": "881920",
    "end": "888160"
  },
  {
    "text": "different claim parameters object than the standard GPU claim parameters object we've been working with so",
    "start": "888160",
    "end": "894720"
  },
  {
    "text": "far where the Mig claim parameters object has a single field representing the Mig profile you would like to have",
    "start": "894720",
    "end": "901160"
  },
  {
    "text": "allocated to the claim and the vfio GPU claim parameters",
    "start": "901160",
    "end": "906360"
  },
  {
    "text": "object has a single selector field which is the same as the selector field available to standard GPU claim",
    "start": "906360",
    "end": "912720"
  },
  {
    "text": "parameters objects whenever a claim is allocated",
    "start": "912720",
    "end": "918279"
  },
  {
    "text": "that references a mid claim parameters object a GPU capable of dynamically creating a MIG device of that size as",
    "start": "918279",
    "end": "924800"
  },
  {
    "text": "found the Mig device is created and then bound to the",
    "start": "924800",
    "end": "929959"
  },
  {
    "text": "claim when the claim is then later deleted so is the Mig device freeing the",
    "start": "929959",
    "end": "935360"
  },
  {
    "text": "GPU up to create Mig devices of different sizes or be used as a full GPU again depending on future incoming",
    "start": "935360",
    "end": "943680"
  },
  {
    "text": "requests whenever a claim is allocated that references a vfio GPU claim parameters object a GPU matching the",
    "start": "944759",
    "end": "951399"
  },
  {
    "text": "specified selector is found swapped off of the standard Nvidia GPU driver onto",
    "start": "951399",
    "end": "957199"
  },
  {
    "text": "the standard Linux vfio driver and then bound to the claim the details of how the runtime",
    "start": "957199",
    "end": "963680"
  },
  {
    "text": "then ensures that these gpus eventually make their way into the VMS is out of scope of this talk but all of the",
    "start": "963680",
    "end": "970240"
  },
  {
    "text": "necessary annotations required to do so are added by nvidia's Dr driver for gpus",
    "start": "970240",
    "end": "975680"
  },
  {
    "text": "as part of the overall claim allocation",
    "start": "975680",
    "end": "979639"
  },
  {
    "text": "process and with that we've been able to demonstrate how Dr can help us overcome",
    "start": "980920",
    "end": "986120"
  },
  {
    "text": "the primary limitations we have with reporting gpus in kubernetes",
    "start": "986120",
    "end": "991920"
  },
  {
    "text": "today now before I go on it's worth pointing out that everything I've talked about so far has already been",
    "start": "993399",
    "end": "999720"
  },
  {
    "text": "implemented and is available for you to play around with today with one exception this last part about the vfi",
    "start": "999720",
    "end": "1006920"
  },
  {
    "text": "GPU claim parameters we do plan to add support for this soon but we are currently focused on getting a proper",
    "start": "1006920",
    "end": "1013079"
  },
  {
    "text": "release of the rest of the features out so it won't be available until sometime next year",
    "start": "1013079",
    "end": "1020480"
  },
  {
    "text": "and with that said here's where you can go to download nvidia's Dr driver for gpus and start playing around with all",
    "start": "1020800",
    "end": "1027000"
  },
  {
    "text": "of this today the driver itself is Deployable as a home chart and we have scripts to get",
    "start": "1027000",
    "end": "1032839"
  },
  {
    "text": "you up and running quickly in both kind assuming you have access to your own GPU Hardware as well as on a gke alpha",
    "start": "1032839",
    "end": "1039798"
  },
  {
    "text": "cluster which allows us to actually make use of Dr in a managed kubernetes cluster since GK Alpha clusters enable",
    "start": "1039799",
    "end": "1047360"
  },
  {
    "text": "all kubernetes features Gates by default in fact we've used these scripts",
    "start": "1047360",
    "end": "1053559"
  },
  {
    "text": "to set up the demos I'm about to show you now where the first demo runs within a kind cluster on a dgx a100 box with",
    "start": "1053559",
    "end": "1061679"
  },
  {
    "text": "eight a100 gpus available and the second two run on a GK Alpha cluster with two",
    "start": "1061679",
    "end": "1068559"
  },
  {
    "text": "different GPU node pools available one containing a set of T4 gpus and one with",
    "start": "1068559",
    "end": "1075720"
  },
  {
    "text": "v100s so let's get started",
    "start": "1075720",
    "end": "1080400"
  },
  {
    "text": "this demo shows some of the advanced GPU sharing features enabled by nvidia's Dr resource driver for",
    "start": "1082880",
    "end": "1088679"
  },
  {
    "text": "gpus in particular it shows how one can request access to a hardware partition GPU as opposed to a full GPU and then",
    "start": "1088679",
    "end": "1096919"
  },
  {
    "text": "layer a sharing mechanism on top of that that lets you further subdivide its resources among multiple",
    "start": "1096919",
    "end": "1102320"
  },
  {
    "text": "clients each Hardware partition is referred to as a MIG device and the sharing mechanism called MPS",
    "start": "1102320",
    "end": "1109159"
  },
  {
    "text": "relies on having a dedicated server process launched to service each client when directed The Nvidia Dr",
    "start": "1109159",
    "end": "1116640"
  },
  {
    "text": "resource driver for gpus will dynamically create one of these Mig devices and launch an NPS control demon",
    "start": "1116640",
    "end": "1122440"
  },
  {
    "text": "to service its shared clients so jumping over to the code we",
    "start": "1122440",
    "end": "1127520"
  },
  {
    "text": "can see three Windows the first contains a job specification with its parallelism",
    "start": "1127520",
    "end": "1132919"
  },
  {
    "text": "set to four this means that four separate pods will be launched as part of this job",
    "start": "1132919",
    "end": "1139880"
  },
  {
    "text": "each pod then has a reference to a resource claim called Mig MPS sharing",
    "start": "1139880",
    "end": "1145360"
  },
  {
    "text": "which in turn is referenced by the single container embedded inside it this container simply invokes a long",
    "start": "1145360",
    "end": "1153039"
  },
  {
    "text": "running andbody simulation designed to keep the GPU busy throughout the",
    "start": "1153039",
    "end": "1159080"
  },
  {
    "text": "demo now moving over to the second window we see the actual specification of the resource claim",
    "start": "1159679",
    "end": "1166360"
  },
  {
    "text": "itself followed by the third window window which contains our custom claim parameters",
    "start": "1166360",
    "end": "1171520"
  },
  {
    "text": "object populated with the settings needed to request the smallest Mig device possible with MPS sharing",
    "start": "1171520",
    "end": "1179799"
  },
  {
    "text": "enabled now to make this demo a little more interesting we're not going to launch this job with just this single",
    "start": "1180240",
    "end": "1185840"
  },
  {
    "text": "container inside it instead we're going to run it alongside three other",
    "start": "1185840",
    "end": "1191559"
  },
  {
    "text": "containers which provide a baseline for the feature it is demonstrating the first is a full GPU",
    "start": "1191559",
    "end": "1198919"
  },
  {
    "text": "with Standard Time slicing enabled rather than MPS the second is a full GPU with MPS",
    "start": "1198919",
    "end": "1206200"
  },
  {
    "text": "sharing enabled and the third is a similarly sized M device with Standard Time",
    "start": "1206200",
    "end": "1213120"
  },
  {
    "text": "slicing enabled as before we have the claims to",
    "start": "1213120",
    "end": "1218440"
  },
  {
    "text": "represent these as well as the claim parameter objects to Define their",
    "start": "1218440",
    "end": "1225000"
  },
  {
    "text": "settings okay so now for the interesting part",
    "start": "1225000",
    "end": "1230158"
  },
  {
    "text": "let's start off by showing the current state of the cluster which shows that we are running in a single node setup with",
    "start": "1230440",
    "end": "1236600"
  },
  {
    "text": "our Dr resource driver already",
    "start": "1236600",
    "end": "1240559"
  },
  {
    "text": "deployed next I'll show the current state of the machine in terms of how gpus are",
    "start": "1241960",
    "end": "1248480"
  },
  {
    "text": "partitioned at the moment there are no partitions created and we simply see",
    "start": "1248480",
    "end": "1253520"
  },
  {
    "text": "eight full a100 gpus available for allocation",
    "start": "1253520",
    "end": "1259880"
  },
  {
    "text": "now let's go ahead and create a name space to run our demo in and launch our job and its Associated",
    "start": "1259880",
    "end": "1266760"
  },
  {
    "text": "claims now that that's launched we can run Cube control get a few times to wait",
    "start": "1269159",
    "end": "1274400"
  },
  {
    "text": "for all of the pods to show up as you can see there are four pods",
    "start": "1274400",
    "end": "1281960"
  },
  {
    "text": "associated with the job and two dynamically provisioned MPS control demons just as we would expect",
    "start": "1281960",
    "end": "1290039"
  },
  {
    "text": "with this next command we can see that two Mig devices have been dynamically created inside GPU 4 as well of all as",
    "start": "1292600",
    "end": "1299919"
  },
  {
    "text": "well as all of the processes from our nbody simulation running across all of our gpus and make",
    "start": "1299919",
    "end": "1306559"
  },
  {
    "text": "devices to get a closer look at the processes associated with each device I'm going to run a script that parses",
    "start": "1308480",
    "end": "1314720"
  },
  {
    "text": "the output shown here to grab a reference to each of the four devices that have been allocated and set an",
    "start": "1314720",
    "end": "1320320"
  },
  {
    "text": "environment variable for them appropriately I'll then run a series of",
    "start": "1320320",
    "end": "1326960"
  },
  {
    "text": "Docker containers that have access to each of these devices individually so we can see the processes running inside",
    "start": "1326960",
    "end": "1335000"
  },
  {
    "text": "them the first lets me see the processes for the full GPU with Standard Time",
    "start": "1335640",
    "end": "1342120"
  },
  {
    "text": "slicing and the second lets me see the processes for the full GPU with MPS",
    "start": "1342120",
    "end": "1347279"
  },
  {
    "text": "sharing enabled as expected the one on the top shows",
    "start": "1347279",
    "end": "1353960"
  },
  {
    "text": "four compute processes that's what the C here stands for whereas the bottom shows",
    "start": "1353960",
    "end": "1360919"
  },
  {
    "text": "five processes one MPS server and it's four clients denoted with the M plus C",
    "start": "1360919",
    "end": "1368120"
  },
  {
    "text": "scene here if I then do the same for the two",
    "start": "1368120",
    "end": "1373840"
  },
  {
    "text": "make devices I have similar results",
    "start": "1373840",
    "end": "1379960"
  },
  {
    "text": "finally I can clean up my environment by deleting the job and all of its Associated resource",
    "start": "1380840",
    "end": "1388278"
  },
  {
    "text": "claims which will then trigger the MPS control demons to shut",
    "start": "1388720",
    "end": "1394159"
  },
  {
    "text": "down as well as delete the dynamically created Mig devices on GPU",
    "start": "1394159",
    "end": "1400840"
  },
  {
    "text": "4 for this next demo I show how to use the selector field in the GPU claim",
    "start": "1406880",
    "end": "1412840"
  },
  {
    "text": "parameters object to direct the allocation of a resource claim to a particular type of",
    "start": "1412840",
    "end": "1418480"
  },
  {
    "text": "GPU to do this I have pre-provisioned a gke alpha cluster with three node pools",
    "start": "1418480",
    "end": "1425000"
  },
  {
    "text": "one for running the control plane Services one containing T4 gpus and one",
    "start": "1425000",
    "end": "1430440"
  },
  {
    "text": "containing v100s the drra resource driver for gpus has already been predeployment",
    "start": "1430440",
    "end": "1437960"
  },
  {
    "text": "as well as an extra helper demon required for Dr to work in this environment specifically to install the",
    "start": "1437960",
    "end": "1444880"
  },
  {
    "text": "Nvidia container tool kit and make sure that support for the container device interface is enabled in container",
    "start": "1444880",
    "end": "1452919"
  },
  {
    "text": "D so jumping over to the code we see two windows the window on the left shows the",
    "start": "1452919",
    "end": "1459360"
  },
  {
    "text": "set of commands I'm going to walk through to demonstrate this feature and the window on the right shows where",
    "start": "1459360",
    "end": "1464799"
  },
  {
    "text": "these commands will be run so let's start off by first listing out",
    "start": "1464799",
    "end": "1471559"
  },
  {
    "text": "all of the nodes in the cluster as you can see we have three",
    "start": "1471559",
    "end": "1479559"
  },
  {
    "text": "control plane nodes at the top coming from the default pool and three nodes at the bottom the",
    "start": "1479559",
    "end": "1487039"
  },
  {
    "text": "first two of which are from pool one and the last one is from pool",
    "start": "1487039",
    "end": "1494080"
  },
  {
    "text": "two next I'll show the list of what are called node allocation State objects",
    "start": "1495480",
    "end": "1500960"
  },
  {
    "text": "associated with our Dr driver for gpus there's one of these objects per",
    "start": "1500960",
    "end": "1506760"
  },
  {
    "text": "GPU node and they hold the state of which gpus are available and which ones are currently in use on the",
    "start": "1506760",
    "end": "1514600"
  },
  {
    "text": "Node as you can see we have three of them one for each of the three nodes",
    "start": "1514600",
    "end": "1520440"
  },
  {
    "text": "from pool one and pool two now dumping their contents you can",
    "start": "1520440",
    "end": "1528000"
  },
  {
    "text": "see there are two nodes from pool one with t4s on them and one node from pool two with a",
    "start": "1528000",
    "end": "1534240"
  },
  {
    "text": "V100 on it note the amount of memory each GPU",
    "start": "1534240",
    "end": "1540200"
  },
  {
    "text": "has as that will play an important role in how our GPU selectors decide which one to grab later",
    "start": "1540200",
    "end": "1547520"
  },
  {
    "text": "on now jumping over to the specs I've defined to allocate claims and deploy pods in this setup we see three",
    "start": "1549840",
    "end": "1556679"
  },
  {
    "text": "windows the first contains two sets of GPU claim",
    "start": "1556679",
    "end": "1563559"
  },
  {
    "text": "parameter objects one for requesting access to what I've called an inference",
    "start": "1563559",
    "end": "1571039"
  },
  {
    "text": "GPU and one for requesting access to what I've called a training",
    "start": "1572200",
    "end": "1577919"
  },
  {
    "text": "GPU or the inference GPU must contain less than 16 gbt of",
    "start": "1578840",
    "end": "1586158"
  },
  {
    "text": "memory and have a Cuda compute capability greater than or equal to",
    "start": "1586600",
    "end": "1592880"
  },
  {
    "text": "7.5 and the training GPU must have more than 16 GB of",
    "start": "1593919",
    "end": "1600440"
  },
  {
    "text": "memory as you can imagine on the current cluster this basically translates into a",
    "start": "1600440",
    "end": "1606120"
  },
  {
    "text": "T4 for the inference GPU and a V100 for the training",
    "start": "1606120",
    "end": "1612278"
  },
  {
    "text": "GPU now the second window just defines the resource claim templates that refer to these GPU claim parameters",
    "start": "1614039",
    "end": "1621480"
  },
  {
    "text": "objects and the third window defines the Pod specs which reference these",
    "start": "1621960",
    "end": "1628279"
  },
  {
    "text": "claims one called inference pod and one called training",
    "start": "1628279",
    "end": "1635320"
  },
  {
    "text": "pod for the demo all I'm doing is running Nvidia SMI to verify which GPU I",
    "start": "1636360",
    "end": "1642000"
  },
  {
    "text": "have been granted and then sitting in a sleep Loop so that I can pull this info from the log uh later",
    "start": "1642000",
    "end": "1649919"
  },
  {
    "text": "on so jumping back to the script I first create a namespace called cubon",
    "start": "1652600",
    "end": "1660240"
  },
  {
    "text": "demo and then create all of the objects referenced in my specs inside this",
    "start": "1663440",
    "end": "1669799"
  },
  {
    "text": "namespace once that's done I run Cube control get pod to show that my my pods",
    "start": "1672440",
    "end": "1677919"
  },
  {
    "text": "have started",
    "start": "1677919",
    "end": "1680320"
  },
  {
    "text": "running followed by another call to print the set of gpus that have been allocated in each node allocation State",
    "start": "1683679",
    "end": "1692480"
  },
  {
    "text": "object as we can see one GPU has been allocated to the inference pod from pool",
    "start": "1697120",
    "end": "1704320"
  },
  {
    "text": "one and one GPU has been allocated to the training job from pool",
    "start": "1704880",
    "end": "1711518"
  },
  {
    "text": "2 which is verified by printing the logs of each pod as",
    "start": "1713159",
    "end": "1719320"
  },
  {
    "text": "well for the final demo I will be showing how we have built a PO of",
    "start": "1727919",
    "end": "1733320"
  },
  {
    "text": "integrating Dr support into one of nvidia's Flagship AI products the Triton Management Service or TMS for",
    "start": "1733320",
    "end": "1742240"
  },
  {
    "text": "short for those of you not familiar with TMS it provides an automated way of deploying multiple Triton inference",
    "start": "1742240",
    "end": "1748960"
  },
  {
    "text": "servers onto a kubernetes cluster Each of which may serve models with varying GPU",
    "start": "1748960",
    "end": "1756278"
  },
  {
    "text": "requirements at present there is no good way for TMS to pick and choose which GPU",
    "start": "1756399",
    "end": "1761559"
  },
  {
    "text": "a given server will be given access to when running in a mixed GPU environment",
    "start": "1761559",
    "end": "1768440"
  },
  {
    "text": "by integrating with our Dr resource driver for gpus TMS is now able to right",
    "start": "1768440",
    "end": "1773760"
  },
  {
    "text": "siize the GPU given to a particular inference server it does this by translating a",
    "start": "1773760",
    "end": "1779840"
  },
  {
    "text": "model's GPU requirements into a set of selectors building a GPU claim parameters object from it and then",
    "start": "1779840",
    "end": "1786720"
  },
  {
    "text": "referencing that in a resource claim the same gke cluster used in the",
    "start": "1786720",
    "end": "1794200"
  },
  {
    "text": "last demo is also being used here",
    "start": "1794200",
    "end": "1799320"
  },
  {
    "text": "so just like before let's start off by showing the set of nodes in the cluster as well as the node allocation State",
    "start": "1800559",
    "end": "1807120"
  },
  {
    "text": "objects listing which gpus are available on which",
    "start": "1807120",
    "end": "1812039"
  },
  {
    "text": "nodes next I'll show the TMS server running in the cucon namespace on our",
    "start": "1816840",
    "end": "1823840"
  },
  {
    "text": "cluster and reference a command and I've already run in the background to port forward access to the TMS server onto my",
    "start": "1825399",
    "end": "1832720"
  },
  {
    "text": "Local Host next I'll use a command called",
    "start": "1832720",
    "end": "1840240"
  },
  {
    "text": "tmsl to launch two different Triton inference",
    "start": "1840240",
    "end": "1845278"
  },
  {
    "text": "servers the first specifies that it wants a touring GPU with less than 16",
    "start": "1846360",
    "end": "1852279"
  },
  {
    "text": "gabt of memory and the second specifies that it",
    "start": "1852279",
    "end": "1859360"
  },
  {
    "text": "wants a Volta GPU with more than 16 GB of",
    "start": "1859360",
    "end": "1865240"
  },
  {
    "text": "memory once these are deployed I'll run Cube Control G pod in the cubon",
    "start": "1880000",
    "end": "1886360"
  },
  {
    "text": "namespace to verify that they're up and",
    "start": "1886360",
    "end": "1890600"
  },
  {
    "text": "running once that's done I'll dump the node allocation State object to show",
    "start": "1894120",
    "end": "1899720"
  },
  {
    "text": "which gpus have been allocated on which",
    "start": "1899720",
    "end": "1903600"
  },
  {
    "text": "nodes as expected we see one T4 GPU allocated from a node in pool one and",
    "start": "1908519",
    "end": "1915960"
  },
  {
    "text": "one Volta GPU allocated from a node in pool",
    "start": "1915960",
    "end": "1920600"
  },
  {
    "text": "2 exec into each server and running Nvidia SMI then verifies which GPU was",
    "start": "1923120",
    "end": "1930480"
  },
  {
    "text": "granted to each",
    "start": "1930480",
    "end": "1933679"
  },
  {
    "text": "server and that's it thanks for listening if",
    "start": "1946080",
    "end": "1951399"
  },
  {
    "text": "you have any questions feel free to send me an email or find me on the cerity select",
    "start": "1951399",
    "end": "1959039"
  }
]