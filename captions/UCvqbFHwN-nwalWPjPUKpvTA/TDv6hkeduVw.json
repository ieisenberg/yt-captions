[
  {
    "text": "hello guys good afternoon I hope you are having a good time at cucon",
    "start": "599",
    "end": "6520"
  },
  {
    "text": "India so we are here to present a talk that is keeping the lights on zero",
    "start": "6520",
    "end": "12759"
  },
  {
    "text": "Diamond time application upgrades in kues myself Shashank p and my co-speaker is Sagar Jad so we are here to present",
    "start": "12759",
    "end": "20760"
  },
  {
    "text": "before you some scenarios which can help you achieve uh zero down time and keep",
    "start": "20760",
    "end": "26679"
  },
  {
    "text": "your kubernetes applications resilient so we'll take you through a scenario",
    "start": "26679",
    "end": "33760"
  },
  {
    "text": "where a company has just moved to a kubernetes cluster they are into production and they are having a tough",
    "start": "33760",
    "end": "40000"
  },
  {
    "text": "time handling them so they are stacked up with lot of",
    "start": "40000",
    "end": "45480"
  },
  {
    "text": "client uh tickets on call request and revenue is going",
    "start": "45480",
    "end": "52760"
  },
  {
    "text": "down it seems like a relatable image right and if you are using kubernetes",
    "start": "54160",
    "end": "60000"
  },
  {
    "text": "more relatable so I'll ask my co-speaker to",
    "start": "60000",
    "end": "66320"
  },
  {
    "text": "take you through few strategies that is first strategy that is heal check on to you hey hi Freds so in the last slide we",
    "start": "66320",
    "end": "75640"
  },
  {
    "text": "have seen that the war room is going on and the manager is asking like my application is down and Engineers are",
    "start": "75640",
    "end": "81720"
  },
  {
    "text": "giving some sort of solutions like that we could do this we could do this and then the engineers decided that let's go",
    "start": "81720",
    "end": "89400"
  },
  {
    "text": "and ID identify what could be the possible issues with the application so they have started looking into the logs",
    "start": "89400",
    "end": "95560"
  },
  {
    "text": "and one engineer found that the application pods are keep on restarting continuously so then they just describe",
    "start": "95560",
    "end": "102799"
  },
  {
    "text": "the Pod definition and identify that the liveness prob and Readiness prob are pointing to the same end point which",
    "start": "102799",
    "end": "109960"
  },
  {
    "text": "could be the possible solution why my application pod is restarting so we'll take this problem statement and in next",
    "start": "109960",
    "end": "116119"
  },
  {
    "text": "couple of slides we'll understand what is this issue and how can we recover it from",
    "start": "116119",
    "end": "122679"
  },
  {
    "text": "it so before uh understanding the issue let's set some context around the heal",
    "start": "123399",
    "end": "129160"
  },
  {
    "text": "checks and the props so you might be aware that when Whenever there is a situation that uh node is running out of",
    "start": "129160",
    "end": "135720"
  },
  {
    "text": "uh resources or the application is running out of resources kuber is try to reschedule that part into the health",
    "start": "135720",
    "end": "141640"
  },
  {
    "text": "node healthy node right so that issue is sorted out but there could be a scenario when your application itself is stuck",
    "start": "141640",
    "end": "147959"
  },
  {
    "text": "it's not accepting new request it's not ble to process the request so from the client side or from the observability",
    "start": "147959",
    "end": "154319"
  },
  {
    "text": "point of view your application pod is running but it's not serving any request so how can kubernetes identify such",
    "start": "154319",
    "end": "161080"
  },
  {
    "text": "scenarios so it definitely need a signaling or a mechanism through which it identifies that this particular",
    "start": "161080",
    "end": "167640"
  },
  {
    "text": "application is not uh is not working as expected and as we know that each",
    "start": "167640",
    "end": "172760"
  },
  {
    "text": "application has its own working so we cannot have a generic solution for it so that's where we uh the kubernets",
    "start": "172760",
    "end": "179239"
  },
  {
    "text": "introduces uh two props basically liveness and the Readiness props and",
    "start": "179239",
    "end": "184519"
  },
  {
    "text": "each prop has its own significance so what kubernetes doeses is it tries to",
    "start": "184519",
    "end": "189799"
  },
  {
    "text": "prop a container and identify what is happening with the application so with liveness prop it tries to check whether",
    "start": "189799",
    "end": "196640"
  },
  {
    "text": "the application is running and let's say if identifies that application is not running it try to restart the app just",
    "start": "196640",
    "end": "203080"
  },
  {
    "text": "to make sure that the application will will will will be working in the next subsequent run and in the Rus prop it",
    "start": "203080",
    "end": "210080"
  },
  {
    "text": "identifies whether my application is ready to accept the traffic for example I have a front end app which is",
    "start": "210080",
    "end": "215879"
  },
  {
    "text": "dependent on the back end but if the back end is not working front end will not work right so in that case it uh if",
    "start": "215879",
    "end": "223040"
  },
  {
    "text": "the Readiness prop fails it will move that part from the uh pull off the parts",
    "start": "223040",
    "end": "228840"
  },
  {
    "text": "which is where the traffic is serving so for both of this prop although in both",
    "start": "228840",
    "end": "234360"
  },
  {
    "text": "the cases we are probing the container but the way kubernetes handled this prop is different",
    "start": "234360",
    "end": "240840"
  },
  {
    "text": "right so there are some uh best practices that we should check on while",
    "start": "240840",
    "end": "246280"
  },
  {
    "text": "designing this prop and these are the three important aspect we have worked on certain scenarios and that's where we",
    "start": "246280",
    "end": "252159"
  },
  {
    "text": "identified that if you're designing the liveness and Readiness prop you should make sure that both the props should",
    "start": "252159",
    "end": "258680"
  },
  {
    "text": "have a dedicated endpoint and in our subsequent in our previous slide we have seen that the engineer identify that",
    "start": "258680",
    "end": "264759"
  },
  {
    "text": "both the props are using the same same endpoint right and we also know that if the",
    "start": "264759",
    "end": "270360"
  },
  {
    "text": "uh Readiness liveness Prof field kubernetes restart the application right",
    "start": "270360",
    "end": "275440"
  },
  {
    "text": "but let's say my application is running but my dependent application is not running so why should I restart my app",
    "start": "275440",
    "end": "281080"
  },
  {
    "text": "right so that's where we should have a separate endpoint for both of these use cases second thing is is this important",
    "start": "281080",
    "end": "287600"
  },
  {
    "text": "one there there are scenarios that i' I've spoken about like we have in the microservices architecture in the",
    "start": "287600",
    "end": "293199"
  },
  {
    "text": "distributed world one service is dependent on another like back end is dependent on database so we want to make",
    "start": "293199",
    "end": "299759"
  },
  {
    "text": "sure that backend should handle the request only when database is available so for that what we can do is we can",
    "start": "299759",
    "end": "305800"
  },
  {
    "text": "design the Readiness prob in such a way that it should it can do a SQL query to the database just to check whether the",
    "start": "305800",
    "end": "311759"
  },
  {
    "text": "connection is happening or not and then it should Mark that pod is now ready to handle the request right so this is the",
    "start": "311759",
    "end": "318720"
  },
  {
    "text": "second important Point third one is like this props are nothing but the end points right so just to make sure that",
    "start": "318720",
    "end": "324560"
  },
  {
    "text": "your pro because these props are continuously kubernetes is trying to after a certain period of time it is",
    "start": "324560",
    "end": "329800"
  },
  {
    "text": "continuously trying to check whether application is running or not so just make sure that your probs endpoint",
    "start": "329800",
    "end": "335400"
  },
  {
    "text": "should give a simple respon like status code we don't want the uh Json response to be",
    "start": "335400",
    "end": "342240"
  },
  {
    "text": "there so we have solved one issue okay now engineer and one these two Engineers",
    "start": "342240",
    "end": "348800"
  },
  {
    "text": "are again try to identify what could be other scenarios so in when the",
    "start": "348800",
    "end": "354400"
  },
  {
    "text": "application update is happening right one engineer found out that the old old",
    "start": "354400",
    "end": "359800"
  },
  {
    "text": "version pods are not available and the new version pods are taking some time to",
    "start": "359800",
    "end": "364840"
  },
  {
    "text": "up right so in in so there are some like 10 or 20 seconds of delay for which the application is not responding or there",
    "start": "364840",
    "end": "371639"
  },
  {
    "text": "is a downt for the application why because somebody has mistakenly set the Mex unavailable to 100% okay and the",
    "start": "371639",
    "end": "379319"
  },
  {
    "text": "mech surge is set to zero we'll see we'll see this configuration in the latest slides but this is what the issue",
    "start": "379319",
    "end": "385360"
  },
  {
    "text": "that we have found out right now before going deep into those configur ation let us first understand what are the",
    "start": "385360",
    "end": "391360"
  },
  {
    "text": "different rolling different update strategies that we can follow that is there in the kubernetes so kubernetes",
    "start": "391360",
    "end": "398599"
  },
  {
    "text": "like natively supposed to like rolling update and the replacement but we'll talk only about the rolling update",
    "start": "398599",
    "end": "404160"
  },
  {
    "text": "because it allows us to do the zero down term upgrade and there is one more strategy that we will talk about it is",
    "start": "404160",
    "end": "410160"
  },
  {
    "text": "not supported natively by kubernetes but through the through the other tool LS we can have that in place right so what",
    "start": "410160",
    "end": "417160"
  },
  {
    "text": "happens with the rolling update in case of rolling update date the new version pods are gradually roll out until all",
    "start": "417160",
    "end": "424080"
  },
  {
    "text": "the and the older version pods are terminated once the new version pods are available if you can see this diagram we",
    "start": "424080",
    "end": "431240"
  },
  {
    "text": "have one one uh new version pods are running then only the one uh older",
    "start": "431240",
    "end": "436919"
  },
  {
    "text": "version parts are terminated so this is like gradually rolling out so the advantage of this is your application is",
    "start": "436919",
    "end": "443280"
  },
  {
    "text": "available throughout the up upgrade process right but there is one issue with this approach that that there are",
    "start": "443280",
    "end": "450560"
  },
  {
    "text": "scenarios that for a certain period of time your applic two versions of your",
    "start": "450560",
    "end": "456199"
  },
  {
    "text": "application is up at the same time so you have to make sure that your database schema and the back end should be uh",
    "start": "456199",
    "end": "463720"
  },
  {
    "text": "should be able to handle such kind of request okay so this is one constraint with this approach and we'll see in the",
    "start": "463720",
    "end": "469960"
  },
  {
    "text": "next strategy how can we solve this particular constraint and another uh and one more important Advantage is there is",
    "start": "469960",
    "end": "477599"
  },
  {
    "text": "there there if you have configured the Max search properly right so then there is a minimum additional resource usage",
    "start": "477599",
    "end": "484080"
  },
  {
    "text": "during the deployments for example you have set the max search to one okay then",
    "start": "484080",
    "end": "489280"
  },
  {
    "text": "only one part is created first one is in that case only one extra part so one",
    "start": "489280",
    "end": "494720"
  },
  {
    "text": "extra one extra part is consuming the resources but let's say if you have set it to 100% that means if you have five",
    "start": "494720",
    "end": "501280"
  },
  {
    "text": "replicas so then the 10 replicas 10 parts are running at the same time that will consume lot of resources right so",
    "start": "501280",
    "end": "508280"
  },
  {
    "text": "for the production like we have gone through a couple of scenarios and what we recommend to our own customers is for",
    "start": "508280",
    "end": "514719"
  },
  {
    "text": "the production scenarios make sure that you have Max and available to zero that means your application is up for all the",
    "start": "514719",
    "end": "521000"
  },
  {
    "text": "time and Max search is set to one although this will reduce the update",
    "start": "521000",
    "end": "526040"
  },
  {
    "text": "process because only one pod is progressing at at a one time but it makes sure that your application is life",
    "start": "526040",
    "end": "532160"
  },
  {
    "text": "for all all the upgrade process uh so this is just a code snippet of how you can configure the",
    "start": "532160",
    "end": "538760"
  },
  {
    "text": "rolling update strategy next so we we talked about the constant",
    "start": "538760",
    "end": "546839"
  },
  {
    "text": "like in the rolling update we have to make sure that the back end and the database should be sufficient to handle",
    "start": "546839",
    "end": "553440"
  },
  {
    "text": "such kind of request right now the manager is talking to an engineer and uh",
    "start": "553440",
    "end": "559000"
  },
  {
    "text": "she was saying that can there be another approach okay one other thing is that both the applications are are moving at",
    "start": "559000",
    "end": "566160"
  },
  {
    "text": "the same time right and they wanted to make sure that my new version application should be tested first then",
    "start": "566160",
    "end": "571760"
  },
  {
    "text": "I should be able to switch then I should switch my traffic from the old to the new one so the engineer suggested that",
    "start": "571760",
    "end": "577680"
  },
  {
    "text": "we can have a blue green deployments in kubernetes okay so let's see in the next slides how can we achieve the blue green",
    "start": "577680",
    "end": "584480"
  },
  {
    "text": "deployment in kubernetes so in kubernetes although there is no native support in place but what we can do is",
    "start": "584480",
    "end": "590440"
  },
  {
    "text": "we can have a new version deployment deployed along side with the old version deployment okay and then you can have",
    "start": "590440",
    "end": "597120"
  },
  {
    "text": "your test team tested the new version deployment completely and and then using the different network switches you can",
    "start": "597120",
    "end": "603640"
  },
  {
    "text": "switch the traffic from the old version to the new version so here we are resolving that constant right in this",
    "start": "603640",
    "end": "609519"
  },
  {
    "text": "case we don't have to make sure that the back end uh needs to support both old schema and the new schema because here",
    "start": "609519",
    "end": "615920"
  },
  {
    "text": "at a time only single schema or single version is available the only problem with this approach is the it will",
    "start": "615920",
    "end": "622959"
  },
  {
    "text": "consume the cost because at the same time you have two versions of the application running so if you have a",
    "start": "622959",
    "end": "628760"
  },
  {
    "text": "customer with who is running out of uh who has some budget constraints right so don't recommend this approach because in",
    "start": "628760",
    "end": "635000"
  },
  {
    "text": "this case you you have both the versions uh up at the same time okay yeah let's",
    "start": "635000",
    "end": "641760"
  },
  {
    "text": "go to now we have uh Identify two issues right with respect to props and with",
    "start": "641760",
    "end": "648160"
  },
  {
    "text": "respect to update strategy now here the architect comes in in the previous Slide",
    "start": "648160",
    "end": "653399"
  },
  {
    "text": "the manager is asking about different strategy now here the architect comes and is talking to a engineer that I was",
    "start": "653399",
    "end": "659320"
  },
  {
    "text": "was looking at the kubernetes I was looking at my kubernetes environment and I've saw that few of my application",
    "start": "659320",
    "end": "665160"
  },
  {
    "text": "parts are terminating with the 500 status code so he was trying to delete that pod but it was failing and and the",
    "start": "665160",
    "end": "672040"
  },
  {
    "text": "in the logs it is getting the 5 minut status code so then this particular Engineers figure out that he looked into",
    "start": "672040",
    "end": "678480"
  },
  {
    "text": "the code to make sure that what what's the actual issue so when he looked at the code application code he found that",
    "start": "678480",
    "end": "685160"
  },
  {
    "text": "we have not handled the syum signal properly so there is no cleanup happening basically this particular",
    "start": "685160",
    "end": "691800"
  },
  {
    "text": "application is handling some file system file system thing and when we are deleting the part the file system",
    "start": "691800",
    "end": "697680"
  },
  {
    "text": "connection are not are not closed so it is giving some errors so he recommended that we should handle the signal signal",
    "start": "697680",
    "end": "704560"
  },
  {
    "text": "properly to make sure that whenever this application pod is deleted all the cleanup should happen Okay and all the",
    "start": "704560",
    "end": "711639"
  },
  {
    "text": "infly request should be completed on the time so this is also one of the uh recommended strategies which we should",
    "start": "711639",
    "end": "718440"
  },
  {
    "text": "follow to a Achi the zero downtime upgrades so let's discuss more about this so uh this s term signal is the is",
    "start": "718440",
    "end": "725839"
  },
  {
    "text": "the is the Unix construct right so in in the kubernetes world what happens is",
    "start": "725839",
    "end": "731000"
  },
  {
    "text": "whenever you delete a pod right kubernetes sends the syum signal to the container with the p id1 okay and it",
    "start": "731000",
    "end": "738760"
  },
  {
    "text": "waited for a certain period of time and then if if for the given time if the",
    "start": "738760",
    "end": "744279"
  },
  {
    "text": "application is not gracefully shut down it sends a s kill signal which because",
    "start": "744279",
    "end": "749639"
  },
  {
    "text": "of which the application shut down abruptly now here two things are happening either you can shut down the",
    "start": "749639",
    "end": "756000"
  },
  {
    "text": "application gracefully or you can shut down the application abruptly so what is the recommended strategy right we should",
    "start": "756000",
    "end": "763199"
  },
  {
    "text": "shut down the application gracefully right so that we can do the cleanup or or the we can handle the inflight",
    "start": "763199",
    "end": "768639"
  },
  {
    "text": "request properly so for that there are two concepts that we can use here first",
    "start": "768639",
    "end": "773800"
  },
  {
    "text": "thing is the life cycle hook which kubernetes supports out of the box second thing is you can add the graceful",
    "start": "773800",
    "end": "781120"
  },
  {
    "text": "handling in your code itself whether you are writing your application in goang Python there are packages available",
    "start": "781120",
    "end": "787160"
  },
  {
    "text": "through which you can listen to that syum signal and do the cleanup uh",
    "start": "787160",
    "end": "793720"
  },
  {
    "text": "method so let's understand the whole flow through a diagram right so I",
    "start": "793720",
    "end": "799760"
  },
  {
    "text": "executed Cube Cil delete pod command okay then what happens tuber is trigger",
    "start": "799760",
    "end": "805399"
  },
  {
    "text": "the pre-stop hook okay it waited for a waited for for it to complete it and",
    "start": "805399",
    "end": "810639"
  },
  {
    "text": "then it sends a s term signal to the application and then it waited for it to",
    "start": "810639",
    "end": "816000"
  },
  {
    "text": "be to uh so that it can be gracefully shut down right let's say if within 30",
    "start": "816000",
    "end": "821320"
  },
  {
    "text": "seconds if the application is not shut down by itself it sends a sill signal to",
    "start": "821320",
    "end": "826480"
  },
  {
    "text": "delete that to uh to exit that particular application right so this parameter is very important so make sure",
    "start": "826480",
    "end": "834320"
  },
  {
    "text": "that uh this parameter has a sufficient value so that your pre-stop hook and and graceful shutdown should be completed in",
    "start": "834320",
    "end": "841279"
  },
  {
    "text": "a given period of time we'll talk about some of the best",
    "start": "841279",
    "end": "846680"
  },
  {
    "text": "practices and these are the important one best practices regarding the pre-stop hook right first point I",
    "start": "846680",
    "end": "852040"
  },
  {
    "text": "already discussed right we should have a proper value of the terminate grace period second because it could be the",
    "start": "852040",
    "end": "857759"
  },
  {
    "text": "case that in your pre-stop hook you're calling the load balancer to re-register your service but but that whole like",
    "start": "857759",
    "end": "865440"
  },
  {
    "text": "connection request and response took so much time and you have set only the 30 seconds period right so make sure you",
    "start": "865440",
    "end": "871839"
  },
  {
    "text": "have a proper value set so you can do some heat and dry with your test environment and come up with the value for the termination grease Spirit",
    "start": "871839",
    "end": "878600"
  },
  {
    "text": "seconds second thing is why do we one more use case with a pre-stop hook that",
    "start": "878600",
    "end": "884320"
  },
  {
    "text": "uh I forgot to mention here is let's say when you delete your application right what kubernetes does",
    "start": "884320",
    "end": "891079"
  },
  {
    "text": "is it deregisters your Port from the uh from the port from the pool of ports who",
    "start": "891079",
    "end": "898000"
  },
  {
    "text": "are receiving the traffic right and in case of the production or the cloud environment it typically be a",
    "start": "898000",
    "end": "903959"
  },
  {
    "text": "load balancer now what happens is the the D registration took some time and it",
    "start": "903959",
    "end": "910519"
  },
  {
    "text": "take it took some time but still some requests are went into your pod to for",
    "start": "910519",
    "end": "915720"
  },
  {
    "text": "the handling but the application is exiting out right so those infly inflight requests are not properly",
    "start": "915720",
    "end": "922519"
  },
  {
    "text": "handled so from the end user they are like accessing the application but they are not getting the proper responses in",
    "start": "922519",
    "end": "929160"
  },
  {
    "text": "this case what should happen is as soon as you have deleted the port right your application should be unavailable or",
    "start": "929160",
    "end": "934199"
  },
  {
    "text": "some other Port should be handled that request so what we can do is in the pre-stop hook you can have that code in",
    "start": "934199",
    "end": "940160"
  },
  {
    "text": "place which does the deregistration by itself wait for the success code and",
    "start": "940160",
    "end": "945680"
  },
  {
    "text": "then allow kubernetes to send the signal signal to the application third point is you should",
    "start": "945680",
    "end": "952000"
  },
  {
    "text": "have your preop hook script should be full tolerant what does it mean even if",
    "start": "952000",
    "end": "957440"
  },
  {
    "text": "your pre-op hook fail then also kubernetes sends a syum signal to your application so in that case",
    "start": "957440",
    "end": "963839"
  },
  {
    "text": "there will be that in that handling will be incomplete so make sure that you have added all the retrial logic to make sure",
    "start": "963839",
    "end": "970360"
  },
  {
    "text": "your pre-stop H Works successfully and last point is from from the the what all",
    "start": "970360",
    "end": "976040"
  },
  {
    "text": "the things we have talked about is about the deployment configuration and from the admin point of view but last point",
    "start": "976040",
    "end": "981880"
  },
  {
    "text": "is for the developers so in the next uh slide we'll just look at the example of how you can configure the the",
    "start": "981880",
    "end": "989360"
  },
  {
    "text": "uh how you can handle the uh syum signal in the go right so this is an example",
    "start": "989360",
    "end": "994600"
  },
  {
    "text": "wherein you have the channel set up and then you're listening to that Channel Once you have received a syum signal you",
    "start": "994600",
    "end": "1001480"
  },
  {
    "text": "can do some cleanup logic and then you can exit from the program so this is the recommended uh approach uh to handle the",
    "start": "1001480",
    "end": "1008480"
  },
  {
    "text": "uh syum signals now over to Shashank for the next couple of",
    "start": "1008480",
    "end": "1013759"
  },
  {
    "text": "slides thank you sagur so now we had a look at how heal checks",
    "start": "1013759",
    "end": "1019560"
  },
  {
    "text": "as well as pre-stop hooks handling application gracefully has rectified at",
    "start": "1019560",
    "end": "1025280"
  },
  {
    "text": "a certain level for the company but now what they are facing is one of the app",
    "start": "1025280",
    "end": "1030839"
  },
  {
    "text": "in a Zone has completely gone down what was the cause because all the parts are",
    "start": "1030839",
    "end": "1036160"
  },
  {
    "text": "deployed in the same Zone what can be the solution for this is being asked to the engineer the engineer comes up with",
    "start": "1036160",
    "end": "1042798"
  },
  {
    "text": "a solution that is topology spread constants so let's see what topology spread constant do",
    "start": "1042799",
    "end": "1050840"
  },
  {
    "text": "so the application ports for example on your left side the application ports are",
    "start": "1051120",
    "end": "1057039"
  },
  {
    "text": "scattered whereas we have Zone a Zone B Zone C given by a cloud provider",
    "start": "1057039",
    "end": "1063400"
  },
  {
    "text": "so what topology spread does is it helps you spread your ports across all the",
    "start": "1063400",
    "end": "1068720"
  },
  {
    "text": "zones available so in this way if even if my zon C goes down I have my P",
    "start": "1068720",
    "end": "1075600"
  },
  {
    "text": "available in zone B Zone a this will help me keep keep my applications uh up",
    "start": "1075600",
    "end": "1081559"
  },
  {
    "text": "up so what are the pros of topology spreads that is preventing resource",
    "start": "1081559",
    "end": "1086799"
  },
  {
    "text": "bottlenecks we won't be relying on one zone second is balancing load and",
    "start": "1086799",
    "end": "1092360"
  },
  {
    "text": "improving fall tolerant so here is the example on your",
    "start": "1092360",
    "end": "1099600"
  },
  {
    "text": "right which uh gives you a understanding of what topology spread constant is so",
    "start": "1099600",
    "end": "1104720"
  },
  {
    "text": "few of the best practices that we'll recommend uh setting topology spread is make sure you set the max Q carefully so",
    "start": "1104720",
    "end": "1112039"
  },
  {
    "text": "what Max Q does is it will help you provide a balance so the number of imbalance you can have maximum number of",
    "start": "1112039",
    "end": "1118960"
  },
  {
    "text": "imbalance you can have so maximum we'll prefer is one it should be at a minimum level you can't afford to have a more",
    "start": "1118960",
    "end": "1126080"
  },
  {
    "text": "imbalance across your cluster second is plan to recover from Zone failure even if you have topology",
    "start": "1126080",
    "end": "1132520"
  },
  {
    "text": "spread constraints make sure you are able to um get over your Zone failure",
    "start": "1132520",
    "end": "1139240"
  },
  {
    "text": "that can be done by mon keeping monitoring or checking your zones",
    "start": "1139240",
    "end": "1144840"
  },
  {
    "text": "throughout and one of the last thing is make sure you use the Do Not schedule uh",
    "start": "1144840",
    "end": "1150840"
  },
  {
    "text": "thing very well because what do not schedule does is when you say Do not",
    "start": "1150840",
    "end": "1155960"
  },
  {
    "text": "schedule it will make sure it is uh it will not be scheduled in the zone which",
    "start": "1155960",
    "end": "1161120"
  },
  {
    "text": "has gone down until the Zone comes up so for example if your application needs a",
    "start": "1161120",
    "end": "1166240"
  },
  {
    "text": "quorum or a balance of three pods and you have um said do not schedule it will",
    "start": "1166240",
    "end": "1172760"
  },
  {
    "text": "not schedule in the third zone so because of this you are left with only two uh pods in two zones for example you",
    "start": "1172760",
    "end": "1179840"
  },
  {
    "text": "can use uh when satisfiable as schedule anyway so what this will do is it will",
    "start": "1179840",
    "end": "1185280"
  },
  {
    "text": "schedule your pod in the other uh nodes that are up and when the third node",
    "start": "1185280",
    "end": "1191640"
  },
  {
    "text": "comes up it will schedule there yeah so we have fixed the topology",
    "start": "1191640",
    "end": "1197880"
  },
  {
    "text": "spread constraint now what these guys are facing is their pods are getting",
    "start": "1197880",
    "end": "1203880"
  },
  {
    "text": "evicted and they have uh come back to the engineer what can be the reason why",
    "start": "1203880",
    "end": "1209400"
  },
  {
    "text": "my pods are getting evicted due to node pressure so what can be the uh fix of",
    "start": "1209400",
    "end": "1215200"
  },
  {
    "text": "this so turns out the engineer finds out the deployments or the workloads that",
    "start": "1215200",
    "end": "1221240"
  },
  {
    "text": "are being uh deployed do not have CPU and request uh configured at all so what",
    "start": "1221240",
    "end": "1228320"
  },
  {
    "text": "CP and request have to play and what schul does so schedular is uh",
    "start": "1228320",
    "end": "1235799"
  },
  {
    "text": "responsible for scheduling your pods on nodes depending whichever node is",
    "start": "1235799",
    "end": "1241760"
  },
  {
    "text": "healthy it will schedule on that but what does request and limits have to player so when you give a request to a",
    "start": "1241760",
    "end": "1248919"
  },
  {
    "text": "deployment or a pod it make sure to find a Best node for you for example if you",
    "start": "1248919",
    "end": "1255480"
  },
  {
    "text": "give a uh request of one GB Ram it will make sure to allot it on a node where it",
    "start": "1255480",
    "end": "1262679"
  },
  {
    "text": "has at least 1 GB available for example a node has 5gb it will uh allot that",
    "start": "1262679",
    "end": "1267799"
  },
  {
    "text": "part to that node so this way Schuler looks for request and if there are no request the",
    "start": "1267799",
    "end": "1276000"
  },
  {
    "text": "pods which don't have any uh request or limits will be first one to get evicted but why is the case we'll look",
    "start": "1276000",
    "end": "1283120"
  },
  {
    "text": "forward so as you can see on your left side there are pods which are having",
    "start": "1283120",
    "end": "1288640"
  },
  {
    "text": "request and limit set and they are allotted a proper node with a proper uh",
    "start": "1288640",
    "end": "1294480"
  },
  {
    "text": "memory or CPU so for example all these ports on your left are allotted a node",
    "start": "1294480",
    "end": "1299679"
  },
  {
    "text": "checking the memory the Schuler will make sure it allots to that node but whereas on the right there are few pods",
    "start": "1299679",
    "end": "1306559"
  },
  {
    "text": "which don't have any uh CPO request allotted at all so they are having uh",
    "start": "1306559",
    "end": "1312320"
  },
  {
    "text": "alloted to the node so so that node has some Spar capacity available it will be",
    "start": "1312320",
    "end": "1317720"
  },
  {
    "text": "allotted over there so but they will be the first one to be",
    "start": "1317720",
    "end": "1322679"
  },
  {
    "text": "evicted so let's see what request and limit actually are how they help so",
    "start": "1322799",
    "end": "1328240"
  },
  {
    "text": "request is the minimum requirement your application needs for example my",
    "start": "1328240",
    "end": "1333520"
  },
  {
    "text": "application needs a ram of 1 GB it will be the minimum requirement and I'll set the request as 1 GB of RAM whereas the",
    "start": "1333520",
    "end": "1341559"
  },
  {
    "text": "LI limits will be for example I think my application will burst up to 2GB and",
    "start": "1341559",
    "end": "1348240"
  },
  {
    "text": "that's the limit it cannot go above that so memory and CPU so memory can be uh",
    "start": "1348240",
    "end": "1354760"
  },
  {
    "text": "configured in form of kilobytes megabytes gigabytes as well as CPU",
    "start": "1354760",
    "end": "1360159"
  },
  {
    "text": "whereas CPU are configured in form of cores One Core of CPU I can allot two core of CPO depending on my request and",
    "start": "1360159",
    "end": "1368520"
  },
  {
    "text": "limits so yes so next is what limits uh does to uh you what like what limits",
    "start": "1368520",
    "end": "1375320"
  },
  {
    "text": "impact on CPU and memory what are the impact of that so when you say I have",
    "start": "1375320",
    "end": "1380799"
  },
  {
    "text": "given a limit of 1 GB Ram when it reaches that limit it will",
    "start": "1380799",
    "end": "1386679"
  },
  {
    "text": "be restarted o killed it goes out of memory whereas it is not the same case",
    "start": "1386679",
    "end": "1392799"
  },
  {
    "text": "for CPU the CPU when it goes to the Limit it cannot go about that and it",
    "start": "1392799",
    "end": "1398360"
  },
  {
    "text": "will be throttled and which will bring a slow down to your",
    "start": "1398360",
    "end": "1404840"
  },
  {
    "text": "application so yes so next is on what phes the application is prioritized for",
    "start": "1405279",
    "end": "1412520"
  },
  {
    "text": "example I have a workload which is of high priority consider the application",
    "start": "1412520",
    "end": "1417640"
  },
  {
    "text": "which the company is facing a downtime on so that's a critical application right it is bringing a revenue to your",
    "start": "1417640",
    "end": "1425000"
  },
  {
    "text": "company so those are supposed to be Mission critical application and those need to be",
    "start": "1425000",
    "end": "1430400"
  },
  {
    "text": "prioritized so uh that those priority are in form of qos classes quality of",
    "start": "1430400",
    "end": "1436760"
  },
  {
    "text": "service so uh there are three classes guaranteed burstable and best effort so",
    "start": "1436760",
    "end": "1442640"
  },
  {
    "text": "when I say guaranteed uh you are giving a uh",
    "start": "1442640",
    "end": "1447679"
  },
  {
    "text": "request and limit as same that is 1 GB RAM and 1GB limit 1 GB Ram as request",
    "start": "1447679",
    "end": "1453640"
  },
  {
    "text": "and 1 GB Ram as limit and same for CPU so those are guaranteed and for burstable it will be",
    "start": "1453640",
    "end": "1461520"
  },
  {
    "text": "a different for example I think there is a uh request that I need to be allotted",
    "start": "1461520",
    "end": "1467279"
  },
  {
    "text": "as 1GB but my application might burst up to 2GB the limit I can keep as",
    "start": "1467279",
    "end": "1473399"
  },
  {
    "text": "burstable and best effort are the ones which are not given any request and limits at",
    "start": "1473399",
    "end": "1478679"
  },
  {
    "text": "all so so guaranteed ones will be given the highest priority and in that order",
    "start": "1478679",
    "end": "1484960"
  },
  {
    "text": "only applications will be evicted so the best effort PS which don't have any request and limits will be the first one",
    "start": "1484960",
    "end": "1491600"
  },
  {
    "text": "to get evicted next is burstable and after is",
    "start": "1491600",
    "end": "1496840"
  },
  {
    "text": "guaranteed so yes so next one of the same things to give a priority to your",
    "start": "1496840",
    "end": "1502320"
  },
  {
    "text": "application is a priority class so consider priority class as a ticket that",
    "start": "1502320",
    "end": "1508000"
  },
  {
    "text": "you have reserved for your uh bus that's the reserved that you can no one can take it same is for priority classes you",
    "start": "1508000",
    "end": "1516399"
  },
  {
    "text": "can allot a priority class to a workload for example I have a production workload",
    "start": "1516399",
    "end": "1523120"
  },
  {
    "text": "and I can allot a highest priority to that uh workload second priority I can",
    "start": "1523120",
    "end": "1528159"
  },
  {
    "text": "get give it to a prepr environment and the last will be development one so in",
    "start": "1528159",
    "end": "1534080"
  },
  {
    "text": "this way I keep a check that my uh highest priority ports are always",
    "start": "1534080",
    "end": "1541398"
  },
  {
    "text": "protected so yes so some best practices that we like to share while setting request and limits are it will be for",
    "start": "1542399",
    "end": "1550480"
  },
  {
    "text": "for your mission critical ports always make sure you set uh set a request and limit second is pair it up with Port",
    "start": "1550480",
    "end": "1557720"
  },
  {
    "text": "disruption budget we'll look at P disruption budget in next section then you'll get to know what I'm talking",
    "start": "1557720",
    "end": "1563039"
  },
  {
    "text": "about but make sure to pair it up with P disruption budgets the second uh one more U debate",
    "start": "1563039",
    "end": "1572600"
  },
  {
    "text": "that I would like to bring over here is there is a widespread debate in the community that you should always set um",
    "start": "1572600",
    "end": "1579880"
  },
  {
    "text": "request for CPU why is that case because we just uh observed right for CPU when",
    "start": "1579880",
    "end": "1586480"
  },
  {
    "text": "it reaches the limit it throttles it uh makes your application slow so",
    "start": "1586480",
    "end": "1591520"
  },
  {
    "text": "when you set a limit and when you don't have uh when you don't know what the limit is going to be so better not to",
    "start": "1591520",
    "end": "1598000"
  },
  {
    "text": "set request but then you have a case wherein if you set a uh request and not",
    "start": "1598000",
    "end": "1603720"
  },
  {
    "text": "limit you take take the advantages of Q classes so that class becomes bust",
    "start": "1603720",
    "end": "1610440"
  },
  {
    "text": "instead of guaranteed so the uh what our recommendation is keep a good check",
    "start": "1610440",
    "end": "1617320"
  },
  {
    "text": "monitor your application how it is performing uh Benchmark it go let it go",
    "start": "1617320",
    "end": "1622360"
  },
  {
    "text": "through heavy testing and then you'll get to know what your application needs are for CPU and memory this way you can",
    "start": "1622360",
    "end": "1628279"
  },
  {
    "text": "set your request and limits yeah so the last part is always",
    "start": "1628279",
    "end": "1635039"
  },
  {
    "text": "make sure for memory try to keep the request and limits same that is 1 GB Ram",
    "start": "1635039",
    "end": "1640720"
  },
  {
    "text": "1 GB uh request and 1 GB of limit don't make it very low",
    "start": "1640720",
    "end": "1648440"
  },
  {
    "text": "yeah now there is another scenario that has come up that is now we have U",
    "start": "1649679",
    "end": "1656039"
  },
  {
    "text": "rectified lot of issues but now the company is going through some Port",
    "start": "1656039",
    "end": "1661159"
  },
  {
    "text": "disruptions in case of node maintenance or cluster upgrades so these come down",
    "start": "1661159",
    "end": "1667440"
  },
  {
    "text": "come into the class of uh voluntary disruptions so how to get rid of that so",
    "start": "1667440",
    "end": "1674559"
  },
  {
    "text": "whenever there is a node maintenance happening the Pod is going down and disruptions are happening so how to get",
    "start": "1674559",
    "end": "1680960"
  },
  {
    "text": "rid of that so the solution to this is the engineer has to say is let's use pod",
    "start": "1680960",
    "end": "1686600"
  },
  {
    "text": "disruption budget so what pod disruption budget is it enables you to control the number of PODS that can be unavailable",
    "start": "1686600",
    "end": "1693399"
  },
  {
    "text": "during a voluntary disruption so for example I am having a u node maintenance",
    "start": "1693399",
    "end": "1701640"
  },
  {
    "text": "and during that time my all my pods are going down what giving a port disruption",
    "start": "1701640",
    "end": "1707519"
  },
  {
    "text": "budget what it will do is it will make sure at least one or whatever uh thing you have set it will make sure that is",
    "start": "1707519",
    "end": "1714840"
  },
  {
    "text": "available so you don't face downtime during those voluntary disruptions so that those number can be",
    "start": "1714840",
    "end": "1721760"
  },
  {
    "text": "set using Max unavailable and Min available either of them you can use so max unavailable what it does is it gives",
    "start": "1721760",
    "end": "1728159"
  },
  {
    "text": "you a number of ports that should be unavailable at the same time the maximum",
    "start": "1728159",
    "end": "1734159"
  },
  {
    "text": "number of PODS Min available is saying that these number of ports should be",
    "start": "1734159",
    "end": "1739559"
  },
  {
    "text": "available that minimum number of ports should be available just to make sure just you uh",
    "start": "1739559",
    "end": "1746080"
  },
  {
    "text": "make sure to understand that pdbs will not work on",
    "start": "1746080",
    "end": "1751720"
  },
  {
    "text": "involuntary disruptions like node pressure or some disruption that are",
    "start": "1751720",
    "end": "1757880"
  },
  {
    "text": "that are not under our control it the these are for voluntary disruptions that are uh node maintenance or cluster",
    "start": "1757880",
    "end": "1765760"
  },
  {
    "text": "upgrades so next we'll just have have a look at a scenario where there is no",
    "start": "1765760",
    "end": "1771600"
  },
  {
    "text": "Port disruption budget whereas a scenario where there is a poort disruption budget so as you can see when",
    "start": "1771600",
    "end": "1778279"
  },
  {
    "text": "there is an when there is no pot disruption budget applied the node is taken down for maintenance all the",
    "start": "1778279",
    "end": "1784799"
  },
  {
    "text": "replicas both the replicas go down at the same time before they are becoming available",
    "start": "1784799",
    "end": "1790480"
  },
  {
    "text": "on the Node 2 so in that brief time or whatever time it takes to come up there",
    "start": "1790480",
    "end": "1795600"
  },
  {
    "text": "is a downtime so this mainly happens during upgrades so when you pair up with P",
    "start": "1795600",
    "end": "1803919"
  },
  {
    "text": "disruption budget you have one replica going down the second replica is made available on this second node this is",
    "start": "1803919",
    "end": "1810760"
  },
  {
    "text": "because of setting of Max unavailable or Min unavailable Min available",
    "start": "1810760",
    "end": "1815960"
  },
  {
    "text": "sorry so in this way two ports are available uh on the second node and your",
    "start": "1815960",
    "end": "1822200"
  },
  {
    "text": "uh node is taken down for maintenance so this way we are always keeping the",
    "start": "1822200",
    "end": "1827279"
  },
  {
    "text": "application up even during maintenance yeah so one of the few of the best",
    "start": "1827279",
    "end": "1833760"
  },
  {
    "text": "practices that we'll have to U tell are configure pdbs carefully make sure your",
    "start": "1833760",
    "end": "1841000"
  },
  {
    "text": "pods are not in Crash Loop crash loop back off off so what crash loop back off",
    "start": "1841000",
    "end": "1846559"
  },
  {
    "text": "does is it will term the Pod as unavailable due to this if all ports are",
    "start": "1846559",
    "end": "1852559"
  },
  {
    "text": "unavailable and you have set a uh unavailability of one and there are three ports becoming",
    "start": "1852559",
    "end": "1858720"
  },
  {
    "text": "unavailable the the involuntary disrupt sorry voluntary disruption that you have",
    "start": "1858720",
    "end": "1865000"
  },
  {
    "text": "will cannot go forward right because Max unavailable is more than what is set",
    "start": "1865000",
    "end": "1871320"
  },
  {
    "text": "there are more ports that are unavailable you won't be allowed to do the node maintenance or cluster upgrades",
    "start": "1871320",
    "end": "1876760"
  },
  {
    "text": "at all second is use accurate",
    "start": "1876760",
    "end": "1882000"
  },
  {
    "text": "selectors if you mess up with the selectors through a typo or something it is not going to consider it at at all",
    "start": "1882000",
    "end": "1889080"
  },
  {
    "text": "you will um believe that you are protected through pot disruption budget but actually you are not the P",
    "start": "1889080",
    "end": "1896000"
  },
  {
    "text": "disruption budget doesn't be there at all third is set max unavailable or Min",
    "start": "1896000",
    "end": "1903880"
  },
  {
    "text": "available carefully that is either use max unavailable or Min available because",
    "start": "1903880",
    "end": "1911080"
  },
  {
    "text": "you can't use you can use either of them don't mix and match to create more confusion also other thing is use max",
    "start": "1911080",
    "end": "1920120"
  },
  {
    "text": "unavailable with percentage you never know how much pods scale for your application if you keep one that will",
    "start": "1920120",
    "end": "1927240"
  },
  {
    "text": "only have one available whereas your application might need more so always keep it in",
    "start": "1927240",
    "end": "1933720"
  },
  {
    "text": "percentage last is do not use a value of 0% or 100% for your service so always",
    "start": "1933720",
    "end": "1941080"
  },
  {
    "text": "use more than that not okay so I think we have come uh",
    "start": "1941080",
    "end": "1948480"
  },
  {
    "text": "come to a stage where we have made the application resilient we brought the company to a",
    "start": "1948480",
    "end": "1955880"
  },
  {
    "text": "stage where we we can say that using these strategies at least we can keep",
    "start": "1955880",
    "end": "1961240"
  },
  {
    "text": "their application running well so few of the takeaways that we just uh went",
    "start": "1961240",
    "end": "1967240"
  },
  {
    "text": "through are request and limits it is always advisable to use request and limits never keep uh your application",
    "start": "1967240",
    "end": "1974919"
  },
  {
    "text": "without these configs there are more chances of them going your applications going down and your ports getting",
    "start": "1974919",
    "end": "1980720"
  },
  {
    "text": "evicted second is always design good health checks as we saw in health checks",
    "start": "1980720",
    "end": "1987360"
  },
  {
    "text": "if you have a Endo with the same if you have both liveness and Dead with the",
    "start": "1987360",
    "end": "1993120"
  },
  {
    "text": "same Endo you might end up having cascading failures and you might have",
    "start": "1993120",
    "end": "1998200"
  },
  {
    "text": "more issues always make sure that you design your health checks well second is graceful third is",
    "start": "1998200",
    "end": "2004120"
  },
  {
    "text": "graceful shutdown which we saw through pre-stop hooks and application gracefully",
    "start": "2004120",
    "end": "2010320"
  },
  {
    "text": "shutting down make sure you handle that in your code itself last one we saw through qos",
    "start": "2010320",
    "end": "2017799"
  },
  {
    "text": "classes and priority classes so always make sure you use either of them make",
    "start": "2017799",
    "end": "2023760"
  },
  {
    "text": "sure you use priority classes highest priority should be given to your applications which are having which are",
    "start": "2023760",
    "end": "2029159"
  },
  {
    "text": "more critical and lower one to the ones which are not so critical we also looked at cluster level",
    "start": "2029159",
    "end": "2036639"
  },
  {
    "text": "strategies the last one one being topology spread constants where you can",
    "start": "2036639",
    "end": "2042240"
  },
  {
    "text": "distribute pods among all the zones and last one being po disruption budget",
    "start": "2042240",
    "end": "2047480"
  },
  {
    "text": "always make sure you limit your disruptions so when you are having a voluntary",
    "start": "2047480",
    "end": "2053760"
  },
  {
    "text": "disruption always the pods are uh available when you are having that so",
    "start": "2053760",
    "end": "2058878"
  },
  {
    "text": "you don't face any application downtime yeah so these are the takeaways",
    "start": "2058879",
    "end": "2066638"
  },
  {
    "text": "so now I think the uh the company is more aware that just using kubernetes",
    "start": "2066639",
    "end": "2072520"
  },
  {
    "text": "will not give you uh outof the Box resiliency you have to use some strategies and you should Implement them",
    "start": "2072520",
    "end": "2078480"
  },
  {
    "text": "well so that was our um that were our key takeaways so here is a QR code where we",
    "start": "2078480",
    "end": "2086560"
  },
  {
    "text": "are publishing a book uh short book wherein there are these strategies are",
    "start": "2086560",
    "end": "2092240"
  },
  {
    "text": "explained in more detail with maybe a few examples uh which will be available",
    "start": "2092240",
    "end": "2098839"
  },
  {
    "text": "just add it and you will be added to the wait list and whenever it is available it will be delivered to your",
    "start": "2098839",
    "end": "2105760"
  },
  {
    "text": "inbox yeah thank you and before we go we have uh these are our QR codes we",
    "start": "2107520",
    "end": "2115880"
  },
  {
    "text": "are there on LinkedIn we are happy to connect yeah",
    "start": "2115880",
    "end": "2121560"
  }
]