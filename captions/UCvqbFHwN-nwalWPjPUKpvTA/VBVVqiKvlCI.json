[
  {
    "start": "0",
    "end": "48000"
  },
  {
    "text": "hello and welcome to the rook intro and",
    "start": "399",
    "end": "3280"
  },
  {
    "text": "ceph deep dive my name is blaine i'll be",
    "start": "3280",
    "end": "5920"
  },
  {
    "text": "joined today by my colleague satoru",
    "start": "5920",
    "end": "9120"
  },
  {
    "text": "uh also keep your eyes open for",
    "start": "9120",
    "end": "11280"
  },
  {
    "text": "presentations by other rook maintainers",
    "start": "11280",
    "end": "13759"
  },
  {
    "text": "travis and sebastian",
    "start": "13759",
    "end": "16720"
  },
  {
    "text": "i want to start with the goals of our",
    "start": "16720",
    "end": "18480"
  },
  {
    "text": "talk today",
    "start": "18480",
    "end": "19840"
  },
  {
    "text": "starting with a background",
    "start": "19840",
    "end": "22000"
  },
  {
    "text": "on kubernetes storage challenges",
    "start": "22000",
    "end": "24720"
  },
  {
    "text": "talking about",
    "start": "24720",
    "end": "25840"
  },
  {
    "text": "what is rook",
    "start": "25840",
    "end": "27760"
  },
  {
    "text": "what the background is of rook with seth",
    "start": "27760",
    "end": "31359"
  },
  {
    "text": "and then also talking about the key",
    "start": "31359",
    "end": "32880"
  },
  {
    "text": "features of rook and steph",
    "start": "32880",
    "end": "34880"
  },
  {
    "text": "we also",
    "start": "34880",
    "end": "36239"
  },
  {
    "text": "released rook 1.7 recently i'll be",
    "start": "36239",
    "end": "39040"
  },
  {
    "text": "talking about some of the new features",
    "start": "39040",
    "end": "40559"
  },
  {
    "text": "there",
    "start": "40559",
    "end": "41520"
  },
  {
    "text": "satoru will give you a demo and then",
    "start": "41520",
    "end": "43200"
  },
  {
    "text": "we'll conclude with the q a",
    "start": "43200",
    "end": "47120"
  },
  {
    "start": "48000",
    "end": "96000"
  },
  {
    "text": "starting with",
    "start": "48000",
    "end": "50640"
  },
  {
    "text": "uh",
    "start": "50719",
    "end": "51520"
  },
  {
    "text": "background what are the challenges that",
    "start": "51520",
    "end": "53680"
  },
  {
    "text": "rook hopes to solve",
    "start": "53680",
    "end": "56480"
  },
  {
    "text": "in kubernetes we have a platform that is",
    "start": "56480",
    "end": "59199"
  },
  {
    "text": "used to manage distributed applications",
    "start": "59199",
    "end": "62239"
  },
  {
    "text": "these are ideally stateless",
    "start": "62239",
    "end": "64720"
  },
  {
    "text": "but in practice",
    "start": "64720",
    "end": "66560"
  },
  {
    "text": "this is pretty rare something requires",
    "start": "66560",
    "end": "69760"
  },
  {
    "text": "storage something somewhere",
    "start": "69760",
    "end": "73680"
  },
  {
    "text": "if we rely on external storage this is",
    "start": "73680",
    "end": "76799"
  },
  {
    "text": "not portable",
    "start": "76799",
    "end": "78400"
  },
  {
    "text": "often deployment can be a burden",
    "start": "78400",
    "end": "80960"
  },
  {
    "text": "and",
    "start": "80960",
    "end": "81920"
  },
  {
    "text": "for day two operations we need someone",
    "start": "81920",
    "end": "83520"
  },
  {
    "text": "to manage it",
    "start": "83520",
    "end": "86159"
  },
  {
    "text": "it may therefore make sense to go to a",
    "start": "86479",
    "end": "88799"
  },
  {
    "text": "cloud provider managed service",
    "start": "88799",
    "end": "91119"
  },
  {
    "text": "but then we may be",
    "start": "91119",
    "end": "93280"
  },
  {
    "text": "faced with vendor lock-in",
    "start": "93280",
    "end": "96159"
  },
  {
    "start": "96000",
    "end": "224000"
  },
  {
    "text": "rook aims to",
    "start": "96159",
    "end": "98240"
  },
  {
    "text": "help solve these issues uh but but but",
    "start": "98240",
    "end": "101759"
  },
  {
    "text": "what what is rook like how does it help",
    "start": "101759",
    "end": "103920"
  },
  {
    "text": "solve these issues",
    "start": "103920",
    "end": "105920"
  },
  {
    "text": "rook makes storage available inside of",
    "start": "105920",
    "end": "108240"
  },
  {
    "text": "your kubernetes cluster",
    "start": "108240",
    "end": "110159"
  },
  {
    "text": "this storage you can consume like any",
    "start": "110159",
    "end": "111920"
  },
  {
    "text": "other kubernetes storage you make a",
    "start": "111920",
    "end": "113680"
  },
  {
    "text": "storage class for it and then users can",
    "start": "113680",
    "end": "116479"
  },
  {
    "text": "create persistent volume claims to get",
    "start": "116479",
    "end": "119200"
  },
  {
    "text": "access to that storage for their",
    "start": "119200",
    "end": "120560"
  },
  {
    "text": "applications",
    "start": "120560",
    "end": "123200"
  },
  {
    "text": "rook is an operator",
    "start": "123439",
    "end": "125920"
  },
  {
    "text": "it has many operators available but",
    "start": "125920",
    "end": "128479"
  },
  {
    "text": "generally there will be one operator",
    "start": "128479",
    "end": "129759"
  },
  {
    "text": "running at a time",
    "start": "129759",
    "end": "131200"
  },
  {
    "text": "and then custom resource definitions to",
    "start": "131200",
    "end": "133360"
  },
  {
    "text": "define that storage and set parameters",
    "start": "133360",
    "end": "135440"
  },
  {
    "text": "for it",
    "start": "135440",
    "end": "137760"
  },
  {
    "text": "rook provides automated management",
    "start": "138160",
    "end": "139920"
  },
  {
    "text": "including deployment configuration and",
    "start": "139920",
    "end": "142080"
  },
  {
    "text": "upgrades of the storage and it is fully",
    "start": "142080",
    "end": "144720"
  },
  {
    "text": "open source",
    "start": "144720",
    "end": "146640"
  },
  {
    "text": "and it's available to",
    "start": "146640",
    "end": "149760"
  },
  {
    "text": "uh",
    "start": "149760",
    "end": "150720"
  },
  {
    "text": "to anyone who wants it",
    "start": "150720",
    "end": "154920"
  },
  {
    "text": "brooke has",
    "start": "157599",
    "end": "158879"
  },
  {
    "text": "three live storage providers right now",
    "start": "158879",
    "end": "161120"
  },
  {
    "text": "three active storage providers i should",
    "start": "161120",
    "end": "162800"
  },
  {
    "text": "say",
    "start": "162800",
    "end": "163680"
  },
  {
    "text": "uh for stable we have steph thus the",
    "start": "163680",
    "end": "167360"
  },
  {
    "text": "upcoming deep dive into rook with ceph",
    "start": "167360",
    "end": "169760"
  },
  {
    "text": "we also have nfs and cassandra in alpha",
    "start": "169760",
    "end": "172319"
  },
  {
    "text": "phase",
    "start": "172319",
    "end": "173519"
  },
  {
    "text": "and with some recent changes we're now",
    "start": "173519",
    "end": "175840"
  },
  {
    "text": "able to",
    "start": "175840",
    "end": "177280"
  },
  {
    "text": "release all of these storage providers",
    "start": "177280",
    "end": "179040"
  },
  {
    "text": "independently so they don't all have to",
    "start": "179040",
    "end": "181440"
  },
  {
    "text": "be released at the same time if some",
    "start": "181440",
    "end": "183519"
  },
  {
    "text": "features",
    "start": "183519",
    "end": "184640"
  },
  {
    "text": "still need to be baked a little bit for",
    "start": "184640",
    "end": "186400"
  },
  {
    "text": "nfs",
    "start": "186400",
    "end": "187760"
  },
  {
    "text": "there there is that time and freedom",
    "start": "187760",
    "end": "189599"
  },
  {
    "text": "available",
    "start": "189599",
    "end": "190800"
  },
  {
    "text": "as an example",
    "start": "190800",
    "end": "192800"
  },
  {
    "text": "also some exciting news it is rook's",
    "start": "192800",
    "end": "195040"
  },
  {
    "text": "fifth birthday coming up",
    "start": "195040",
    "end": "197360"
  },
  {
    "text": "rook in november of 2016 and cubecon",
    "start": "197360",
    "end": "200319"
  },
  {
    "text": "seattle went public with version 0.1",
    "start": "200319",
    "end": "204080"
  },
  {
    "text": "and in that five years we've had 110",
    "start": "204080",
    "end": "206799"
  },
  {
    "text": "more releases",
    "start": "206799",
    "end": "209200"
  },
  {
    "text": "so much of this is due to community",
    "start": "209200",
    "end": "210720"
  },
  {
    "text": "support",
    "start": "210720",
    "end": "211760"
  },
  {
    "text": "feedback and pull requests and we really",
    "start": "211760",
    "end": "215040"
  },
  {
    "text": "just can't can't thank everyone enough",
    "start": "215040",
    "end": "217120"
  },
  {
    "text": "who's",
    "start": "217120",
    "end": "218400"
  },
  {
    "text": "come in and",
    "start": "218400",
    "end": "219760"
  },
  {
    "text": "contributed to the project",
    "start": "219760",
    "end": "223040"
  },
  {
    "start": "224000",
    "end": "409000"
  },
  {
    "text": "digging into some background of",
    "start": "225360",
    "end": "227599"
  },
  {
    "text": "like now getting into the the cef deep",
    "start": "227599",
    "end": "229840"
  },
  {
    "text": "dive",
    "start": "229840",
    "end": "230720"
  },
  {
    "text": "uh i want to cover what what is seth uh",
    "start": "230720",
    "end": "234159"
  },
  {
    "text": "and",
    "start": "234159",
    "end": "234959"
  },
  {
    "text": "what are the architectural layers",
    "start": "234959",
    "end": "237120"
  },
  {
    "text": "of rook how does it work with stuff",
    "start": "237120",
    "end": "241120"
  },
  {
    "text": "well steph is our cephalopod themed",
    "start": "241120",
    "end": "244959"
  },
  {
    "text": "storage service",
    "start": "244959",
    "end": "246720"
  },
  {
    "text": "and there are a lot of words on this",
    "start": "246720",
    "end": "248720"
  },
  {
    "text": "slide but the tl dr is that",
    "start": "248720",
    "end": "251519"
  },
  {
    "text": "ceph keeps your data safe",
    "start": "251519",
    "end": "253680"
  },
  {
    "text": "through scale",
    "start": "253680",
    "end": "255760"
  },
  {
    "text": "it provides the three most common",
    "start": "255760",
    "end": "257600"
  },
  {
    "text": "storage types that you might want and",
    "start": "257600",
    "end": "259440"
  },
  {
    "text": "that's block",
    "start": "259440",
    "end": "260799"
  },
  {
    "text": "shared file system as well as s3",
    "start": "260799",
    "end": "263759"
  },
  {
    "text": "compliant object storage",
    "start": "263759",
    "end": "267720"
  },
  {
    "text": "with the architectural layers",
    "start": "269120",
    "end": "271840"
  },
  {
    "text": "we have rook cef csi and seth",
    "start": "271840",
    "end": "275919"
  },
  {
    "text": "starting with the rook layer that owns",
    "start": "275919",
    "end": "277840"
  },
  {
    "text": "the deployment and management of",
    "start": "277840",
    "end": "279680"
  },
  {
    "text": "everything else which is ceph and ceph",
    "start": "279680",
    "end": "281840"
  },
  {
    "text": "csi",
    "start": "281840",
    "end": "284400"
  },
  {
    "text": "cefcsi itself is the thing that",
    "start": "284479",
    "end": "286479"
  },
  {
    "text": "dynamically provisions storage in ceph",
    "start": "286479",
    "end": "288960"
  },
  {
    "text": "and then mounts that into user",
    "start": "288960",
    "end": "290240"
  },
  {
    "text": "application pods",
    "start": "290240",
    "end": "292320"
  },
  {
    "text": "and with chef we have our data layer",
    "start": "292320",
    "end": "294639"
  },
  {
    "text": "that does all the data protection data",
    "start": "294639",
    "end": "296639"
  },
  {
    "text": "movement",
    "start": "296639",
    "end": "298160"
  },
  {
    "text": "it is",
    "start": "298160",
    "end": "299360"
  },
  {
    "text": "really the most of the brains of the",
    "start": "299360",
    "end": "301280"
  },
  {
    "text": "operation",
    "start": "301280",
    "end": "303840"
  },
  {
    "text": "i don't want to talk about this slide",
    "start": "304240",
    "end": "306080"
  },
  {
    "text": "too deeply",
    "start": "306080",
    "end": "307919"
  },
  {
    "text": "the real takeaway here is that",
    "start": "307919",
    "end": "310880"
  },
  {
    "text": "rook is really just this operator that",
    "start": "310880",
    "end": "313440"
  },
  {
    "text": "you see at the top in the middle",
    "start": "313440",
    "end": "316000"
  },
  {
    "text": "in blue",
    "start": "316000",
    "end": "317680"
  },
  {
    "text": "all of the stuff in red is ceph",
    "start": "317680",
    "end": "320240"
  },
  {
    "text": "and",
    "start": "320240",
    "end": "321440"
  },
  {
    "text": "there are a lot of a lot of components",
    "start": "321440",
    "end": "323440"
  },
  {
    "text": "of ceph running at any given time",
    "start": "323440",
    "end": "326880"
  },
  {
    "text": "even stuff csi which is a lot more",
    "start": "326880",
    "end": "329440"
  },
  {
    "text": "simple still has",
    "start": "329440",
    "end": "331280"
  },
  {
    "text": "a lot of components running at once and",
    "start": "331280",
    "end": "333440"
  },
  {
    "text": "these are what you see in green here",
    "start": "333440",
    "end": "337280"
  },
  {
    "text": "rook also has some",
    "start": "337280",
    "end": "339120"
  },
  {
    "text": "helper demons of its own",
    "start": "339120",
    "end": "341199"
  },
  {
    "text": "which may be rook discovery as presented",
    "start": "341199",
    "end": "344080"
  },
  {
    "text": "here",
    "start": "344080",
    "end": "345039"
  },
  {
    "text": "and the rook operator also manages",
    "start": "345039",
    "end": "346880"
  },
  {
    "text": "deployment of those",
    "start": "346880",
    "end": "349919"
  },
  {
    "text": "and",
    "start": "350080",
    "end": "350960"
  },
  {
    "text": "again rook manages all of this really",
    "start": "350960",
    "end": "353680"
  },
  {
    "text": "the rook operator is the only thing that",
    "start": "353680",
    "end": "355759"
  },
  {
    "text": "the user needs to create",
    "start": "355759",
    "end": "359120"
  },
  {
    "text": "with csi",
    "start": "359120",
    "end": "360639"
  },
  {
    "text": "this is the driver that",
    "start": "360639",
    "end": "363039"
  },
  {
    "text": "actually",
    "start": "363039",
    "end": "364000"
  },
  {
    "text": "makes the storage inside steph like that",
    "start": "364000",
    "end": "366560"
  },
  {
    "text": "it creates it inside ceph and then it",
    "start": "366560",
    "end": "369039"
  },
  {
    "text": "connects that storage into user",
    "start": "369039",
    "end": "370560"
  },
  {
    "text": "application pods i also won't go too",
    "start": "370560",
    "end": "373520"
  },
  {
    "text": "deep",
    "start": "373520",
    "end": "374479"
  },
  {
    "text": "into everything you see here",
    "start": "374479",
    "end": "376560"
  },
  {
    "text": "i think the important uh takeaway uh",
    "start": "376560",
    "end": "379600"
  },
  {
    "text": "sort of expanding beyond the csi view",
    "start": "379600",
    "end": "383280"
  },
  {
    "text": "from before a little bit",
    "start": "383280",
    "end": "386400"
  },
  {
    "text": "the takeaway is that neither rook nor",
    "start": "388319",
    "end": "392000"
  },
  {
    "text": "csi are in the data path here once csi",
    "start": "392000",
    "end": "395840"
  },
  {
    "text": "creates the storage and mounts it into",
    "start": "395840",
    "end": "397520"
  },
  {
    "text": "an application pod",
    "start": "397520",
    "end": "400560"
  },
  {
    "text": "that application",
    "start": "400560",
    "end": "402240"
  },
  {
    "text": "connects directly to a kernel driver",
    "start": "402240",
    "end": "404160"
  },
  {
    "text": "which then is connected to a ceph",
    "start": "404160",
    "end": "405759"
  },
  {
    "text": "cluster",
    "start": "405759",
    "end": "408759"
  },
  {
    "start": "409000",
    "end": "547000"
  },
  {
    "text": "i know that was quite a lot",
    "start": "410240",
    "end": "411919"
  },
  {
    "text": "i'll",
    "start": "411919",
    "end": "413039"
  },
  {
    "text": "sort of slowly",
    "start": "413039",
    "end": "414880"
  },
  {
    "text": "kind of ramp up to",
    "start": "414880",
    "end": "417120"
  },
  {
    "text": "talking about some of the key features",
    "start": "417120",
    "end": "419360"
  },
  {
    "text": "that i want to highlight and rook and",
    "start": "419360",
    "end": "421120"
  },
  {
    "text": "stuff",
    "start": "421120",
    "end": "422720"
  },
  {
    "text": "these are features that are noteworly",
    "start": "422720",
    "end": "424560"
  },
  {
    "text": "noteworthy and that are most commonly",
    "start": "424560",
    "end": "427440"
  },
  {
    "text": "useful to",
    "start": "427440",
    "end": "428800"
  },
  {
    "text": "two people",
    "start": "428800",
    "end": "431360"
  },
  {
    "text": "i'll start with",
    "start": "432319",
    "end": "433759"
  },
  {
    "text": "installation",
    "start": "433759",
    "end": "435120"
  },
  {
    "text": "which we've tried to make as simple as",
    "start": "435120",
    "end": "437599"
  },
  {
    "text": "possible um",
    "start": "437599",
    "end": "440000"
  },
  {
    "text": "really",
    "start": "440000",
    "end": "440960"
  },
  {
    "text": "there are",
    "start": "440960",
    "end": "442160"
  },
  {
    "text": "kind of four steps to setting up a cell",
    "start": "442160",
    "end": "444160"
  },
  {
    "text": "cluster",
    "start": "444160",
    "end": "445199"
  },
  {
    "text": "and the first three",
    "start": "445199",
    "end": "447360"
  },
  {
    "text": "it could as easily be done on one line",
    "start": "447360",
    "end": "449520"
  },
  {
    "text": "as they could on three different lines",
    "start": "449520",
    "end": "452639"
  },
  {
    "text": "very rarely do these need any",
    "start": "452639",
    "end": "454160"
  },
  {
    "text": "modification at all",
    "start": "454160",
    "end": "456240"
  },
  {
    "text": "and then creation of the ceph cluster",
    "start": "456240",
    "end": "458160"
  },
  {
    "text": "resource that we have in rook is",
    "start": "458160",
    "end": "462240"
  },
  {
    "text": "the only thing",
    "start": "462240",
    "end": "463599"
  },
  {
    "text": "that most people really need to to",
    "start": "463599",
    "end": "465680"
  },
  {
    "text": "customize to get things started",
    "start": "465680",
    "end": "467759"
  },
  {
    "text": "and at its simplest it could be the 13",
    "start": "467759",
    "end": "470000"
  },
  {
    "text": "lines of code that you see",
    "start": "470000",
    "end": "472319"
  },
  {
    "text": "the 13 lines of yaml that you see off to",
    "start": "472319",
    "end": "475199"
  },
  {
    "text": "the right",
    "start": "475199",
    "end": "478280"
  },
  {
    "text": "we also really",
    "start": "479840",
    "end": "481919"
  },
  {
    "text": "would have precious little without our",
    "start": "481919",
    "end": "484000"
  },
  {
    "text": "csi driver",
    "start": "484000",
    "end": "485919"
  },
  {
    "text": "this is the thing that",
    "start": "485919",
    "end": "488080"
  },
  {
    "text": "makes all of our hard work available to",
    "start": "488080",
    "end": "490240"
  },
  {
    "text": "to the user",
    "start": "490240",
    "end": "491440"
  },
  {
    "text": "when they",
    "start": "491440",
    "end": "493680"
  },
  {
    "text": "want block or file storage",
    "start": "493680",
    "end": "496479"
  },
  {
    "text": "the key features of",
    "start": "496479",
    "end": "498319"
  },
  {
    "text": "csi that we have",
    "start": "498319",
    "end": "500080"
  },
  {
    "text": "are volume expansion if you make",
    "start": "500080",
    "end": "502720"
  },
  {
    "text": "a volume",
    "start": "502720",
    "end": "504160"
  },
  {
    "text": "of 5 gigabytes and you need 20 gigabytes",
    "start": "504160",
    "end": "506319"
  },
  {
    "text": "later that's possible",
    "start": "506319",
    "end": "508000"
  },
  {
    "text": "we also have snapshots and clones those",
    "start": "508000",
    "end": "510720"
  },
  {
    "text": "are still in beta",
    "start": "510720",
    "end": "512719"
  },
  {
    "text": "but those are rapidly becoming more and",
    "start": "512719",
    "end": "514959"
  },
  {
    "text": "more stable",
    "start": "514959",
    "end": "517120"
  },
  {
    "text": "also for a note we are",
    "start": "517120",
    "end": "519760"
  },
  {
    "text": "very soon deprecating the old old old",
    "start": "519760",
    "end": "522719"
  },
  {
    "text": "flex volume driver",
    "start": "522719",
    "end": "525040"
  },
  {
    "text": "and as part of this deprecation we're",
    "start": "525040",
    "end": "527120"
  },
  {
    "text": "working on a tool to migrate",
    "start": "527120",
    "end": "529279"
  },
  {
    "text": "those flex volumes",
    "start": "529279",
    "end": "531600"
  },
  {
    "text": "or persistent volumes created with the",
    "start": "531600",
    "end": "534080"
  },
  {
    "text": "flex volume driver to csi",
    "start": "534080",
    "end": "537279"
  },
  {
    "text": "or alternately for migrating entry",
    "start": "537279",
    "end": "539839"
  },
  {
    "text": "drivers",
    "start": "539839",
    "end": "541120"
  },
  {
    "text": "to csi",
    "start": "541120",
    "end": "542720"
  },
  {
    "text": "so that users don't have to",
    "start": "542720",
    "end": "545279"
  },
  {
    "text": "migrate things manually",
    "start": "545279",
    "end": "549000"
  },
  {
    "start": "547000",
    "end": "957000"
  },
  {
    "text": "work also runs in",
    "start": "549120",
    "end": "551200"
  },
  {
    "text": "really any environment you could want",
    "start": "551200",
    "end": "553279"
  },
  {
    "text": "these commonly we break down into two",
    "start": "553279",
    "end": "557120"
  },
  {
    "text": "two basic umbrellas being bare metal",
    "start": "557120",
    "end": "560560"
  },
  {
    "text": "or inside of a cloud provider",
    "start": "560560",
    "end": "563040"
  },
  {
    "text": "with bare metal",
    "start": "563040",
    "end": "564480"
  },
  {
    "text": "this may be you have your own hardware",
    "start": "564480",
    "end": "566399"
  },
  {
    "text": "or it may be that you have a shared",
    "start": "566399",
    "end": "568959"
  },
  {
    "text": "hardware situation in a data center",
    "start": "568959",
    "end": "573120"
  },
  {
    "text": "we commonly get asked",
    "start": "573600",
    "end": "576800"
  },
  {
    "text": "especially by people used to running",
    "start": "576800",
    "end": "578959"
  },
  {
    "text": "kubernetes on bare metal why would you",
    "start": "578959",
    "end": "580800"
  },
  {
    "text": "want to run rook in a cloud provider",
    "start": "580800",
    "end": "584000"
  },
  {
    "text": "and the the responses here i i really",
    "start": "584000",
    "end": "586800"
  },
  {
    "text": "find quite fascinating",
    "start": "586800",
    "end": "589440"
  },
  {
    "text": "cloud providers",
    "start": "589440",
    "end": "591440"
  },
  {
    "text": "do actually have several shortcomings",
    "start": "591440",
    "end": "594160"
  },
  {
    "text": "for a lot of users",
    "start": "594160",
    "end": "596480"
  },
  {
    "text": "storage",
    "start": "596480",
    "end": "597680"
  },
  {
    "text": "like storage types are not always",
    "start": "597680",
    "end": "599200"
  },
  {
    "text": "available across",
    "start": "599200",
    "end": "600640"
  },
  {
    "text": "availability zones",
    "start": "600640",
    "end": "602640"
  },
  {
    "text": "one availability zone may have object",
    "start": "602640",
    "end": "604640"
  },
  {
    "text": "storage where another does not for",
    "start": "604640",
    "end": "606720"
  },
  {
    "text": "example",
    "start": "606720",
    "end": "609120"
  },
  {
    "text": "some uh some cloud provider storage also",
    "start": "610000",
    "end": "612720"
  },
  {
    "text": "takes a long time to fail over",
    "start": "612720",
    "end": "614880"
  },
  {
    "text": "rook can",
    "start": "614880",
    "end": "616079"
  },
  {
    "text": "very often fail over in seconds versus",
    "start": "616079",
    "end": "618480"
  },
  {
    "text": "minutes",
    "start": "618480",
    "end": "620640"
  },
  {
    "text": "some cloud providers also limit the",
    "start": "620640",
    "end": "623440"
  },
  {
    "text": "number of pvs you can have per node to",
    "start": "623440",
    "end": "626160"
  },
  {
    "text": "30",
    "start": "626160",
    "end": "627440"
  },
  {
    "text": "which is",
    "start": "627440",
    "end": "629360"
  },
  {
    "text": "sort of pathetically small for for a lot",
    "start": "629360",
    "end": "631760"
  },
  {
    "text": "of users",
    "start": "631760",
    "end": "633600"
  },
  {
    "text": "and it may simply be that users want to",
    "start": "633600",
    "end": "635680"
  },
  {
    "text": "save money they can use a cloud provider",
    "start": "635680",
    "end": "638399"
  },
  {
    "text": "they can still use cloud provider",
    "start": "638399",
    "end": "639760"
  },
  {
    "text": "storage but they can use something that",
    "start": "639760",
    "end": "641279"
  },
  {
    "text": "has a better cost to performance ratio",
    "start": "641279",
    "end": "643839"
  },
  {
    "text": "and to aggregate that together into",
    "start": "643839",
    "end": "645519"
  },
  {
    "text": "something that's usable with rook",
    "start": "645519",
    "end": "649440"
  },
  {
    "text": "the bottom line also especially for",
    "start": "650800",
    "end": "653200"
  },
  {
    "text": "multi-cloud situations is that",
    "start": "653200",
    "end": "655040"
  },
  {
    "text": "consistent a consistent storage platform",
    "start": "655040",
    "end": "658640"
  },
  {
    "text": "like the one that rook can provide is",
    "start": "658640",
    "end": "661279"
  },
  {
    "text": "can be used anywhere kubernetes is",
    "start": "661279",
    "end": "662800"
  },
  {
    "text": "deployed",
    "start": "662800",
    "end": "665200"
  },
  {
    "text": "and this is",
    "start": "665360",
    "end": "666560"
  },
  {
    "text": "like",
    "start": "666560",
    "end": "667360"
  },
  {
    "text": "again the interface is just using",
    "start": "667360",
    "end": "669519"
  },
  {
    "text": "persistent volume claims for underlying",
    "start": "669519",
    "end": "672079"
  },
  {
    "text": "storage there's no",
    "start": "672079",
    "end": "673600"
  },
  {
    "text": "nothing extra or fancy it's just",
    "start": "673600",
    "end": "676320"
  },
  {
    "text": "make a claim for storage",
    "start": "676320",
    "end": "678880"
  },
  {
    "text": "and in a cloud environment there's no",
    "start": "678880",
    "end": "680800"
  },
  {
    "text": "need for direct access to local devices",
    "start": "680800",
    "end": "683200"
  },
  {
    "text": "as we're using a cloud provider's pvcs",
    "start": "683200",
    "end": "685760"
  },
  {
    "text": "as the underlying storage in many cases",
    "start": "685760",
    "end": "689839"
  },
  {
    "text": "also",
    "start": "693440",
    "end": "694320"
  },
  {
    "text": "sort of expanding on environments here",
    "start": "694320",
    "end": "696720"
  },
  {
    "text": "we",
    "start": "696720",
    "end": "697839"
  },
  {
    "text": "it's possible to configure rook for any",
    "start": "697839",
    "end": "699680"
  },
  {
    "text": "cluster topology really this is",
    "start": "699680",
    "end": "703600"
  },
  {
    "text": "customizable across failure domains",
    "start": "703600",
    "end": "707040"
  },
  {
    "text": "or within failure domains",
    "start": "707040",
    "end": "710079"
  },
  {
    "text": "and this really is",
    "start": "710079",
    "end": "713440"
  },
  {
    "text": "toward the end of providing a highly",
    "start": "713440",
    "end": "715360"
  },
  {
    "text": "available and durable storage",
    "start": "715360",
    "end": "718560"
  },
  {
    "text": "and this is achieved by",
    "start": "718560",
    "end": "720079"
  },
  {
    "text": "spreading ceph daemons and data across",
    "start": "720079",
    "end": "722560"
  },
  {
    "text": "failure domains as much as possible",
    "start": "722560",
    "end": "726959"
  },
  {
    "text": "also a lot of users want to keep their",
    "start": "727040",
    "end": "730079"
  },
  {
    "text": "application pods separate from their",
    "start": "730079",
    "end": "732639"
  },
  {
    "text": "storage pods and have a subset of nodes",
    "start": "732639",
    "end": "736160"
  },
  {
    "text": "for storage and a subset of nodes for",
    "start": "736160",
    "end": "738079"
  },
  {
    "text": "applications and that's possible",
    "start": "738079",
    "end": "740480"
  },
  {
    "text": "pretty easily using node affinities and",
    "start": "740480",
    "end": "742560"
  },
  {
    "text": "paints and tolerations",
    "start": "742560",
    "end": "745839"
  },
  {
    "text": "also key with rook is that updates are",
    "start": "747760",
    "end": "750320"
  },
  {
    "text": "automated i mentioned this a little bit",
    "start": "750320",
    "end": "752079"
  },
  {
    "text": "before",
    "start": "752079",
    "end": "754399"
  },
  {
    "text": "and we can think of this in like in two",
    "start": "754399",
    "end": "756959"
  },
  {
    "text": "parts there are upgrades to cif and then",
    "start": "756959",
    "end": "759200"
  },
  {
    "text": "there are upgrades to rook",
    "start": "759200",
    "end": "761440"
  },
  {
    "text": "uh with steph updates and even major",
    "start": "761440",
    "end": "763839"
  },
  {
    "text": "upgrades are fully and totally automated",
    "start": "763839",
    "end": "765920"
  },
  {
    "text": "rook handles everything",
    "start": "765920",
    "end": "767760"
  },
  {
    "text": "i can go from",
    "start": "767760",
    "end": "769519"
  },
  {
    "text": "a major release of cefs version 15",
    "start": "769519",
    "end": "772720"
  },
  {
    "text": "octopus for example to",
    "start": "772720",
    "end": "774880"
  },
  {
    "text": "version 16 pacific and rook just does it",
    "start": "774880",
    "end": "780160"
  },
  {
    "text": "uh patch updates to rook going from you",
    "start": "780160",
    "end": "783440"
  },
  {
    "text": "know version",
    "start": "783440",
    "end": "784600"
  },
  {
    "text": "1.7.4 to",
    "start": "784600",
    "end": "787079"
  },
  {
    "text": "1.7.5 or 6 or 7",
    "start": "787079",
    "end": "790160"
  },
  {
    "text": "is also fully automated within rook",
    "start": "790160",
    "end": "792880"
  },
  {
    "text": "but if we're talking about rec minor",
    "start": "792880",
    "end": "794399"
  },
  {
    "text": "upgrades",
    "start": "794399",
    "end": "795519"
  },
  {
    "text": "upgrading from rook version 1.6 to rook",
    "start": "795519",
    "end": "798560"
  },
  {
    "text": "version 1.7 does sometimes require",
    "start": "798560",
    "end": "801040"
  },
  {
    "text": "manual work",
    "start": "801040",
    "end": "803600"
  },
  {
    "text": "and this is just because",
    "start": "803600",
    "end": "806800"
  },
  {
    "text": "sometimes we need to update the",
    "start": "807760",
    "end": "809680"
  },
  {
    "text": "permissions that rook has or",
    "start": "809680",
    "end": "813040"
  },
  {
    "text": "any number of things or we have feature",
    "start": "813040",
    "end": "814800"
  },
  {
    "text": "deprecations which require a small",
    "start": "814800",
    "end": "816639"
  },
  {
    "text": "amount of manual work if users are using",
    "start": "816639",
    "end": "819120"
  },
  {
    "text": "those features",
    "start": "819120",
    "end": "820639"
  },
  {
    "text": "uh",
    "start": "820639",
    "end": "821839"
  },
  {
    "text": "a notable feature deprecation i just",
    "start": "821839",
    "end": "823920"
  },
  {
    "text": "talked about is deprecating the flex",
    "start": "823920",
    "end": "825519"
  },
  {
    "text": "volume which is",
    "start": "825519",
    "end": "828959"
  },
  {
    "text": "very little used anymore even",
    "start": "828959",
    "end": "832000"
  },
  {
    "text": "feel free to read",
    "start": "832000",
    "end": "833519"
  },
  {
    "text": "the latest uh ceph upgrade guide here at",
    "start": "833519",
    "end": "836240"
  },
  {
    "text": "the link also",
    "start": "836240",
    "end": "839120"
  },
  {
    "text": "another cool thing that we've had in",
    "start": "839760",
    "end": "841120"
  },
  {
    "text": "rook for a while",
    "start": "841120",
    "end": "842639"
  },
  {
    "text": "is the ability to connect up to an",
    "start": "842639",
    "end": "844880"
  },
  {
    "text": "external ceph cluster",
    "start": "844880",
    "end": "847120"
  },
  {
    "text": "ceph has been around for a long time i",
    "start": "847120",
    "end": "849760"
  },
  {
    "text": "mentioned or and a previous slide",
    "start": "849760",
    "end": "851600"
  },
  {
    "text": "mentioned that it had been around since",
    "start": "851600",
    "end": "853199"
  },
  {
    "text": "2012",
    "start": "853199",
    "end": "854639"
  },
  {
    "text": "and many users may already have a self",
    "start": "854639",
    "end": "857279"
  },
  {
    "text": "cluster up and running and they simply",
    "start": "857279",
    "end": "859839"
  },
  {
    "text": "want to use it within kubernetes",
    "start": "859839",
    "end": "862720"
  },
  {
    "text": "well rook also allows connecting",
    "start": "862720",
    "end": "865120"
  },
  {
    "text": "to that external ceph cluster",
    "start": "865120",
    "end": "867440"
  },
  {
    "text": "so that then kubernetes has access to it",
    "start": "867440",
    "end": "870079"
  },
  {
    "text": "and",
    "start": "870079",
    "end": "871120"
  },
  {
    "text": "just like if rook itself were running",
    "start": "871120",
    "end": "873040"
  },
  {
    "text": "the cef cluster",
    "start": "873040",
    "end": "875759"
  },
  {
    "text": "users can still just request the storage",
    "start": "876240",
    "end": "878320"
  },
  {
    "text": "they want and it gets created in",
    "start": "878320",
    "end": "879920"
  },
  {
    "text": "kubernetes and there's no",
    "start": "879920",
    "end": "882480"
  },
  {
    "text": "no fiddling necessary no extra",
    "start": "882480",
    "end": "884639"
  },
  {
    "text": "administration that's necessary",
    "start": "884639",
    "end": "888480"
  },
  {
    "text": "we also have been really interested in",
    "start": "889680",
    "end": "892880"
  },
  {
    "text": "some of the upstream work to provision",
    "start": "892880",
    "end": "895760"
  },
  {
    "text": "object storage buckets",
    "start": "895760",
    "end": "898399"
  },
  {
    "text": "and",
    "start": "898399",
    "end": "899680"
  },
  {
    "text": "currently",
    "start": "899680",
    "end": "900720"
  },
  {
    "text": "what this looks like is",
    "start": "900720",
    "end": "902880"
  },
  {
    "text": "users or administrators can create a",
    "start": "902880",
    "end": "904800"
  },
  {
    "text": "storage class for self-object storage",
    "start": "904800",
    "end": "907680"
  },
  {
    "text": "and then users can create an object",
    "start": "907680",
    "end": "909760"
  },
  {
    "text": "bucket claim",
    "start": "909760",
    "end": "911760"
  },
  {
    "text": "and this is very similar to",
    "start": "911760",
    "end": "913760"
  },
  {
    "text": "a persistent volume claim it's just for",
    "start": "913760",
    "end": "915920"
  },
  {
    "text": "a bucket rather than a volume",
    "start": "915920",
    "end": "919519"
  },
  {
    "text": "and then",
    "start": "919519",
    "end": "920480"
  },
  {
    "text": "whenever rook sees that",
    "start": "920480",
    "end": "922639"
  },
  {
    "text": "obc come in it creates a bucket and then",
    "start": "922639",
    "end": "925440"
  },
  {
    "text": "it gives the user access to the bucket",
    "start": "925440",
    "end": "927360"
  },
  {
    "text": "via a kubernetes secret",
    "start": "927360",
    "end": "929680"
  },
  {
    "text": "there's also a kubernetes enhancement",
    "start": "929680",
    "end": "932320"
  },
  {
    "text": "proposal called cozy the container",
    "start": "932320",
    "end": "934560"
  },
  {
    "text": "object storage interface",
    "start": "934560",
    "end": "936399"
  },
  {
    "text": "which we're",
    "start": "936399",
    "end": "938320"
  },
  {
    "text": "we've been following pretty closely and",
    "start": "938320",
    "end": "940240"
  },
  {
    "text": "we're",
    "start": "940240",
    "end": "941600"
  },
  {
    "text": "waiting to get into kubernetes so so we",
    "start": "941600",
    "end": "944000"
  },
  {
    "text": "can also provide this option for users",
    "start": "944000",
    "end": "946160"
  },
  {
    "text": "and",
    "start": "946160",
    "end": "947040"
  },
  {
    "text": "this aims to be csi but for object",
    "start": "947040",
    "end": "949759"
  },
  {
    "text": "storage if i were to put it in",
    "start": "949759",
    "end": "951680"
  },
  {
    "text": "a very short number of words",
    "start": "951680",
    "end": "955839"
  },
  {
    "start": "957000",
    "end": "1209000"
  },
  {
    "text": "finally we have the features with the",
    "start": "958399",
    "end": "960880"
  },
  {
    "text": "recent rook version 1.7 release this was",
    "start": "960880",
    "end": "964240"
  },
  {
    "text": "from august of 2021",
    "start": "964240",
    "end": "967600"
  },
  {
    "text": "uh",
    "start": "967600",
    "end": "968480"
  },
  {
    "text": "the the three notable updates i can",
    "start": "968480",
    "end": "970639"
  },
  {
    "text": "provide here",
    "start": "970639",
    "end": "971920"
  },
  {
    "text": "are that stretch cluster now is stable",
    "start": "971920",
    "end": "974880"
  },
  {
    "text": "i will i'll talk about stretch cluster",
    "start": "974880",
    "end": "976560"
  },
  {
    "text": "in more detail in a minute",
    "start": "976560",
    "end": "978639"
  },
  {
    "text": "we also protect user data whenever we're",
    "start": "978639",
    "end": "980880"
  },
  {
    "text": "deleting a ceph cluster",
    "start": "980880",
    "end": "983279"
  },
  {
    "text": "so we don't allow a soft cluster to",
    "start": "983279",
    "end": "985120"
  },
  {
    "text": "actually be deleted if any other rook",
    "start": "985120",
    "end": "987600"
  },
  {
    "text": "resources exist and this is to prevent",
    "start": "987600",
    "end": "990959"
  },
  {
    "text": "an administrator from",
    "start": "990959",
    "end": "993519"
  },
  {
    "text": "accidentally",
    "start": "993519",
    "end": "995199"
  },
  {
    "text": "deleting user data",
    "start": "995199",
    "end": "997519"
  },
  {
    "text": "that may exist that maybe they just",
    "start": "997519",
    "end": "999440"
  },
  {
    "text": "missed or a user missed the deadline for",
    "start": "999440",
    "end": "1002480"
  },
  {
    "text": "backing up their data or whatever it is",
    "start": "1002480",
    "end": "1006320"
  },
  {
    "text": "we also have continued",
    "start": "1006560",
    "end": "1008959"
  },
  {
    "text": "to add support for mirroring file",
    "start": "1008959",
    "end": "1011199"
  },
  {
    "text": "systems from oneself cluster to another",
    "start": "1011199",
    "end": "1014079"
  },
  {
    "text": "we now have",
    "start": "1014079",
    "end": "1015519"
  },
  {
    "text": "full support for this at least as far as",
    "start": "1015519",
    "end": "1017759"
  },
  {
    "text": "the ceph project is concerned",
    "start": "1017759",
    "end": "1020079"
  },
  {
    "text": "we do have a note that this is still a",
    "start": "1020079",
    "end": "1021759"
  },
  {
    "text": "newer feature in ceph and it is still",
    "start": "1021759",
    "end": "1025438"
  },
  {
    "text": "considered under testing",
    "start": "1025439",
    "end": "1028720"
  },
  {
    "text": "so i promised i would get back to",
    "start": "1028720",
    "end": "1030640"
  },
  {
    "text": "stretch cluster",
    "start": "1030640",
    "end": "1032720"
  },
  {
    "text": "many users want to run ceph in multiple",
    "start": "1032720",
    "end": "1035038"
  },
  {
    "text": "failure domains",
    "start": "1035039",
    "end": "1036640"
  },
  {
    "text": "this allows us to actually have the",
    "start": "1036640",
    "end": "1038160"
  },
  {
    "text": "better disaster protection that i",
    "start": "1038160",
    "end": "1040400"
  },
  {
    "text": "mentioned",
    "start": "1040400",
    "end": "1041839"
  },
  {
    "text": "and most commonly this is really either",
    "start": "1041839",
    "end": "1044798"
  },
  {
    "text": "two zones or three zones we",
    "start": "1044799",
    "end": "1047280"
  },
  {
    "text": "we really don't see",
    "start": "1047280",
    "end": "1049600"
  },
  {
    "text": "more than three zones terribly often",
    "start": "1049600",
    "end": "1052880"
  },
  {
    "text": "the problem with",
    "start": "1052880",
    "end": "1054960"
  },
  {
    "text": "clusters in two zones is",
    "start": "1054960",
    "end": "1059280"
  },
  {
    "text": "how do we maintain a quorum",
    "start": "1059280",
    "end": "1061520"
  },
  {
    "text": "if a zone fails",
    "start": "1061520",
    "end": "1063760"
  },
  {
    "text": "because by definition quorum has to be",
    "start": "1063760",
    "end": "1066160"
  },
  {
    "text": "more than 50 percent of",
    "start": "1066160",
    "end": "1069280"
  },
  {
    "text": "uh",
    "start": "1069280",
    "end": "1070080"
  },
  {
    "text": "the the",
    "start": "1070080",
    "end": "1071679"
  },
  {
    "text": "daemons in quorum for ceph that is the",
    "start": "1071679",
    "end": "1073919"
  },
  {
    "text": "ceph monitors",
    "start": "1073919",
    "end": "1076480"
  },
  {
    "text": "well the solution that the ceph project",
    "start": "1076480",
    "end": "1078559"
  },
  {
    "text": "has come up with is to have a third zone",
    "start": "1078559",
    "end": "1081120"
  },
  {
    "text": "acting as a minimal tie breaker",
    "start": "1081120",
    "end": "1084400"
  },
  {
    "text": "and i i would encourage you if you're",
    "start": "1084400",
    "end": "1086160"
  },
  {
    "text": "interested in these scenarios to look at",
    "start": "1086160",
    "end": "1089200"
  },
  {
    "text": "the ceph documentation about this",
    "start": "1089200",
    "end": "1092400"
  },
  {
    "text": "but i will also briefly",
    "start": "1092400",
    "end": "1094640"
  },
  {
    "text": "talk about what this looks like visually",
    "start": "1094640",
    "end": "1097360"
  },
  {
    "text": "we have two primary zones and this is",
    "start": "1097360",
    "end": "1100559"
  },
  {
    "text": "in our example where all of our user",
    "start": "1100559",
    "end": "1102799"
  },
  {
    "text": "applications live this is also where all",
    "start": "1102799",
    "end": "1105200"
  },
  {
    "text": "of our ceph storage lives",
    "start": "1105200",
    "end": "1107360"
  },
  {
    "text": "and this is where most of the ceph",
    "start": "1107360",
    "end": "1109760"
  },
  {
    "text": "monitors are going to live",
    "start": "1109760",
    "end": "1112080"
  },
  {
    "text": "we have the third zone on the right",
    "start": "1112080",
    "end": "1114240"
  },
  {
    "text": "which is",
    "start": "1114240",
    "end": "1115360"
  },
  {
    "text": "a monitor that exists as a tie breaker",
    "start": "1115360",
    "end": "1118400"
  },
  {
    "text": "just for the case of",
    "start": "1118400",
    "end": "1120400"
  },
  {
    "text": "a whole zone going down we still can",
    "start": "1120400",
    "end": "1123200"
  },
  {
    "text": "have a majority quorum",
    "start": "1123200",
    "end": "1126000"
  },
  {
    "text": "and this is still",
    "start": "1126000",
    "end": "1127840"
  },
  {
    "text": "what it looks like during normal",
    "start": "1127840",
    "end": "1129600"
  },
  {
    "text": "operation but what the situation really",
    "start": "1129600",
    "end": "1131679"
  },
  {
    "text": "cares about is",
    "start": "1131679",
    "end": "1133200"
  },
  {
    "text": "failure",
    "start": "1133200",
    "end": "1134960"
  },
  {
    "text": "so imagine that one of your primary",
    "start": "1134960",
    "end": "1136799"
  },
  {
    "text": "zones goes offline",
    "start": "1136799",
    "end": "1138559"
  },
  {
    "text": "maybe you live in",
    "start": "1138559",
    "end": "1140559"
  },
  {
    "text": "houston texas and there's a hurricane",
    "start": "1140559",
    "end": "1143120"
  },
  {
    "text": "and now your data center is under three",
    "start": "1143120",
    "end": "1145360"
  },
  {
    "text": "feet of water",
    "start": "1145360",
    "end": "1147120"
  },
  {
    "text": "true story this actually happened to me",
    "start": "1147120",
    "end": "1150640"
  },
  {
    "text": "with this tiebreaker zone",
    "start": "1150640",
    "end": "1153679"
  },
  {
    "text": "there are still a majority quorum of",
    "start": "1153679",
    "end": "1155520"
  },
  {
    "text": "three out of five nodes",
    "start": "1155520",
    "end": "1158080"
  },
  {
    "text": "and",
    "start": "1158080",
    "end": "1159039"
  },
  {
    "text": "ceph has protected all of your data all",
    "start": "1159039",
    "end": "1160799"
  },
  {
    "text": "of these osd's in zone two still have",
    "start": "1160799",
    "end": "1165120"
  },
  {
    "text": "all of the data that exists and it's",
    "start": "1165120",
    "end": "1167200"
  },
  {
    "text": "still available",
    "start": "1167200",
    "end": "1169039"
  },
  {
    "text": "kubernetes because it's kubernetes will",
    "start": "1169039",
    "end": "1171520"
  },
  {
    "text": "reschedule any applications from zone 1",
    "start": "1171520",
    "end": "1174000"
  },
  {
    "text": "into zone 2 once it realizes they're",
    "start": "1174000",
    "end": "1175760"
  },
  {
    "text": "failed",
    "start": "1175760",
    "end": "1177039"
  },
  {
    "text": "and now after",
    "start": "1177039",
    "end": "1179840"
  },
  {
    "text": "some",
    "start": "1179840",
    "end": "1180640"
  },
  {
    "text": "surprisingly short amount of downtime",
    "start": "1180640",
    "end": "1182480"
  },
  {
    "text": "ideally less than 15 minutes",
    "start": "1182480",
    "end": "1186240"
  },
  {
    "text": "all of your applications are still",
    "start": "1186240",
    "end": "1187840"
  },
  {
    "text": "running",
    "start": "1187840",
    "end": "1188799"
  },
  {
    "text": "and most people in the outside world are",
    "start": "1188799",
    "end": "1191919"
  },
  {
    "text": "not aware they don't have to know that",
    "start": "1191919",
    "end": "1193760"
  },
  {
    "text": "your data center in houston just got",
    "start": "1193760",
    "end": "1195679"
  },
  {
    "text": "flooded",
    "start": "1195679",
    "end": "1198159"
  },
  {
    "text": "okay i think i've talked long enough for",
    "start": "1199200",
    "end": "1201520"
  },
  {
    "text": "now i'm going to pass to my colleague",
    "start": "1201520",
    "end": "1203679"
  },
  {
    "text": "satoru who's going to give you a demo of",
    "start": "1203679",
    "end": "1206559"
  },
  {
    "text": "what it is like to install rook",
    "start": "1206559",
    "end": "1210400"
  },
  {
    "start": "1209000",
    "end": "1319000"
  },
  {
    "text": "i'll have a demo to create a simple",
    "start": "1210400",
    "end": "1213520"
  },
  {
    "text": "look-safe cluster",
    "start": "1213520",
    "end": "1216720"
  },
  {
    "text": "i'll use js2 softwares",
    "start": "1216720",
    "end": "1221799"
  },
  {
    "text": "there are two types of ruxic clusters",
    "start": "1221919",
    "end": "1225280"
  },
  {
    "text": "the first one is host-based cluster",
    "start": "1225280",
    "end": "1228880"
  },
  {
    "text": "and the second one is ppg based cluster",
    "start": "1228880",
    "end": "1234480"
  },
  {
    "text": "host-based cluster is suitable for",
    "start": "1234480",
    "end": "1237120"
  },
  {
    "text": "simple cluster",
    "start": "1237120",
    "end": "1238880"
  },
  {
    "text": "especially if",
    "start": "1238880",
    "end": "1240480"
  },
  {
    "text": "you use or all nodes and all devices for",
    "start": "1240480",
    "end": "1244559"
  },
  {
    "text": "roxy poster",
    "start": "1244559",
    "end": "1247520"
  },
  {
    "text": "but sequester cluster resource get",
    "start": "1247520",
    "end": "1250080"
  },
  {
    "text": "complicated if",
    "start": "1250080",
    "end": "1252400"
  },
  {
    "text": "not all nodes and devices are used",
    "start": "1252400",
    "end": "1257280"
  },
  {
    "text": "at worst you should",
    "start": "1257280",
    "end": "1259440"
  },
  {
    "text": "list all nodes and all devices for",
    "start": "1259440",
    "end": "1263200"
  },
  {
    "text": "classes",
    "start": "1263200",
    "end": "1264880"
  },
  {
    "text": "used for clusters",
    "start": "1264880",
    "end": "1268080"
  },
  {
    "text": "as for pvc-based cluster you are free",
    "start": "1268080",
    "end": "1271440"
  },
  {
    "text": "from describing hardware configurations",
    "start": "1271440",
    "end": "1274720"
  },
  {
    "text": "like this and then you should",
    "start": "1274720",
    "end": "1279039"
  },
  {
    "text": "you should specify only to field",
    "start": "1279039",
    "end": "1281840"
  },
  {
    "text": "the count field and the volume claim",
    "start": "1281840",
    "end": "1284080"
  },
  {
    "text": "template field",
    "start": "1284080",
    "end": "1286240"
  },
  {
    "text": "count field means number of ost and",
    "start": "1286240",
    "end": "1290320"
  },
  {
    "text": "volume claim templates field",
    "start": "1290320",
    "end": "1292799"
  },
  {
    "text": "is used for template or pvc",
    "start": "1292799",
    "end": "1296159"
  },
  {
    "text": "um used for osts",
    "start": "1296159",
    "end": "1300159"
  },
  {
    "text": "pbg-based cluster is very easy to expand",
    "start": "1300159",
    "end": "1304480"
  },
  {
    "text": "you just need to increase the count",
    "start": "1304480",
    "end": "1306880"
  },
  {
    "text": "field",
    "start": "1306880",
    "end": "1308320"
  },
  {
    "text": "if you increase this field from 1 to 2",
    "start": "1308320",
    "end": "1312559"
  },
  {
    "text": "the number of ost is also increased to",
    "start": "1312559",
    "end": "1316480"
  },
  {
    "text": "2.",
    "start": "1316480",
    "end": "1319480"
  },
  {
    "text": "so let's create a",
    "start": "1320159",
    "end": "1322320"
  },
  {
    "text": "simple pvc based cluster",
    "start": "1322320",
    "end": "1325840"
  },
  {
    "text": "i used",
    "start": "1325840",
    "end": "1328320"
  },
  {
    "text": "one kubernetes cluster",
    "start": "1328320",
    "end": "1330480"
  },
  {
    "text": "consists of",
    "start": "1330480",
    "end": "1331919"
  },
  {
    "text": "one node",
    "start": "1331919",
    "end": "1333679"
  },
  {
    "text": "this node has",
    "start": "1333679",
    "end": "1335679"
  },
  {
    "text": "two local empty block devices",
    "start": "1335679",
    "end": "1338720"
  },
  {
    "text": "and",
    "start": "1338720",
    "end": "1339840"
  },
  {
    "text": "corresponding past and boring",
    "start": "1339840",
    "end": "1343919"
  },
  {
    "text": "this demo",
    "start": "1344159",
    "end": "1345440"
  },
  {
    "text": "consists of three steps",
    "start": "1345440",
    "end": "1348000"
  },
  {
    "text": "and you can get the all script and all",
    "start": "1348000",
    "end": "1352000"
  },
  {
    "text": "manifest from",
    "start": "1352000",
    "end": "1354159"
  },
  {
    "text": "this project",
    "start": "1354159",
    "end": "1357200"
  },
  {
    "text": "so let's create a rook safeguard",
    "start": "1359120",
    "end": "1362000"
  },
  {
    "text": "look operator as a step one",
    "start": "1362000",
    "end": "1366159"
  },
  {
    "text": "so copy control apply hardware and",
    "start": "1367039",
    "end": "1371039"
  },
  {
    "text": "operate the dmo",
    "start": "1371039",
    "end": "1374240"
  },
  {
    "text": "the",
    "start": "1374240",
    "end": "1374960"
  },
  {
    "text": "operator is created",
    "start": "1374960",
    "end": "1377200"
  },
  {
    "text": "operator is created and",
    "start": "1377200",
    "end": "1380400"
  },
  {
    "text": "our rook safeglass",
    "start": "1380400",
    "end": "1382159"
  },
  {
    "text": "rook safe name space",
    "start": "1382159",
    "end": "1384080"
  },
  {
    "text": "get board",
    "start": "1384080",
    "end": "1386080"
  },
  {
    "text": "okay",
    "start": "1386080",
    "end": "1386960"
  },
  {
    "text": "rooks operator cross operator port is",
    "start": "1386960",
    "end": "1390320"
  },
  {
    "text": "already already running",
    "start": "1390320",
    "end": "1394320"
  },
  {
    "text": "so this second step is created",
    "start": "1394559",
    "end": "1397919"
  },
  {
    "text": "a rook safe cluster",
    "start": "1397919",
    "end": "1401039"
  },
  {
    "text": "the manifest is",
    "start": "1401039",
    "end": "1403039"
  },
  {
    "text": "on in the cluster on pbc.yamo",
    "start": "1403039",
    "end": "1408440"
  },
  {
    "text": "yes it's a safe cluster cluster custom",
    "start": "1408799",
    "end": "1412960"
  },
  {
    "text": "resource and it means number monitor",
    "start": "1412960",
    "end": "1416640"
  },
  {
    "text": "port is one and the number of ost is",
    "start": "1416640",
    "end": "1420400"
  },
  {
    "text": "also one",
    "start": "1420400",
    "end": "1422880"
  },
  {
    "text": "so",
    "start": "1422880",
    "end": "1423919"
  },
  {
    "text": "let's apply this manifest",
    "start": "1423919",
    "end": "1428240"
  },
  {
    "text": "right here",
    "start": "1429360",
    "end": "1432158"
  },
  {
    "text": "there",
    "start": "1432960",
    "end": "1435039"
  },
  {
    "text": "no no",
    "start": "1435039",
    "end": "1437440"
  },
  {
    "text": "okay",
    "start": "1437840",
    "end": "1438880"
  },
  {
    "text": "so let's see the progress of creating",
    "start": "1438880",
    "end": "1442799"
  },
  {
    "text": "this",
    "start": "1442799",
    "end": "1444799"
  },
  {
    "text": "creating root safe cluster get caught",
    "start": "1444799",
    "end": "1449840"
  },
  {
    "text": "so roxy operator is already exist and",
    "start": "1450400",
    "end": "1454480"
  },
  {
    "text": "the g support",
    "start": "1454480",
    "end": "1457039"
  },
  {
    "text": "this port for",
    "start": "1457039",
    "end": "1459919"
  },
  {
    "text": "safe gsi drivers",
    "start": "1459919",
    "end": "1463279"
  },
  {
    "text": "and now safe monitor port is learning",
    "start": "1463279",
    "end": "1467279"
  },
  {
    "text": "and the second and the next step is",
    "start": "1467279",
    "end": "1470960"
  },
  {
    "text": "create a manager port",
    "start": "1470960",
    "end": "1473760"
  },
  {
    "text": "and the third step is looks creating",
    "start": "1473760",
    "end": "1476640"
  },
  {
    "text": "root safe opera uh looks ost look safe",
    "start": "1476640",
    "end": "1480400"
  },
  {
    "text": "osd prepare port",
    "start": "1480400",
    "end": "1483200"
  },
  {
    "text": "it's to initialize the initialize",
    "start": "1483200",
    "end": "1487440"
  },
  {
    "text": "data structure on top of",
    "start": "1487440",
    "end": "1490400"
  },
  {
    "text": "our",
    "start": "1490400",
    "end": "1491279"
  },
  {
    "text": "local block device",
    "start": "1491279",
    "end": "1493919"
  },
  {
    "text": "and the last",
    "start": "1493919",
    "end": "1495360"
  },
  {
    "text": "rook safe osd port is created",
    "start": "1495360",
    "end": "1498559"
  },
  {
    "text": "it's to manage ost ports",
    "start": "1498559",
    "end": "1502720"
  },
  {
    "text": "so",
    "start": "1503600",
    "end": "1505840"
  },
  {
    "text": "let's",
    "start": "1506240",
    "end": "1507200"
  },
  {
    "text": "get pvc",
    "start": "1507200",
    "end": "1508880"
  },
  {
    "text": "okay",
    "start": "1508880",
    "end": "1509919"
  },
  {
    "text": "so this uh",
    "start": "1509919",
    "end": "1511760"
  },
  {
    "text": "this part um past and volume claim is",
    "start": "1511760",
    "end": "1514559"
  },
  {
    "text": "created by rook",
    "start": "1514559",
    "end": "1517120"
  },
  {
    "text": "and it's bound to local rusty to",
    "start": "1517120",
    "end": "1521360"
  },
  {
    "text": "to past and boring",
    "start": "1521360",
    "end": "1523600"
  },
  {
    "text": "uh it's the",
    "start": "1523600",
    "end": "1525520"
  },
  {
    "text": "it's corresponding to",
    "start": "1525520",
    "end": "1529120"
  },
  {
    "text": "onenote",
    "start": "1529120",
    "end": "1530320"
  },
  {
    "text": "on local osd",
    "start": "1530320",
    "end": "1532720"
  },
  {
    "text": "and this pvc is consumed by",
    "start": "1532720",
    "end": "1536720"
  },
  {
    "text": "this osd port",
    "start": "1536720",
    "end": "1540080"
  },
  {
    "text": "so let's confirm the",
    "start": "1540159",
    "end": "1542090"
  },
  {
    "text": "[Music]",
    "start": "1542090",
    "end": "1543760"
  },
  {
    "text": "status of",
    "start": "1543760",
    "end": "1545520"
  },
  {
    "text": "on safe cluster",
    "start": "1545520",
    "end": "1547800"
  },
  {
    "text": "[Music]",
    "start": "1547800",
    "end": "1549760"
  },
  {
    "text": "tools",
    "start": "1549760",
    "end": "1551919"
  },
  {
    "text": "apply",
    "start": "1551919",
    "end": "1553440"
  },
  {
    "text": "another toolbox board okay",
    "start": "1553440",
    "end": "1558320"
  },
  {
    "text": "on the road",
    "start": "1558320",
    "end": "1560320"
  },
  {
    "text": "save",
    "start": "1560320",
    "end": "1562400"
  },
  {
    "text": "exec",
    "start": "1562400",
    "end": "1564799"
  },
  {
    "text": "tools",
    "start": "1564799",
    "end": "1567039"
  },
  {
    "text": "so say hi home s command is to",
    "start": "1567039",
    "end": "1571600"
  },
  {
    "text": "uh",
    "start": "1571600",
    "end": "1572400"
  },
  {
    "text": "to see the status of our",
    "start": "1572400",
    "end": "1576320"
  },
  {
    "text": "safe cluster",
    "start": "1576320",
    "end": "1579200"
  },
  {
    "text": "okay the secret is actually created and",
    "start": "1579440",
    "end": "1583760"
  },
  {
    "text": "the",
    "start": "1583760",
    "end": "1584720"
  },
  {
    "text": "monitor number monitor is one",
    "start": "1584720",
    "end": "1587520"
  },
  {
    "text": "the number of manager is one and the",
    "start": "1587520",
    "end": "1591200"
  },
  {
    "text": "zero there is one waste",
    "start": "1591200",
    "end": "1594480"
  },
  {
    "text": "okay",
    "start": "1594480",
    "end": "1596799"
  },
  {
    "text": "so the third and the last step is expand",
    "start": "1597200",
    "end": "1601120"
  },
  {
    "text": "this cluster",
    "start": "1601120",
    "end": "1603440"
  },
  {
    "text": "so it's very easy as said",
    "start": "1603440",
    "end": "1606640"
  },
  {
    "text": "before",
    "start": "1606640",
    "end": "1607840"
  },
  {
    "text": "through",
    "start": "1607840",
    "end": "1608310"
  },
  {
    "text": "[Music]",
    "start": "1608310",
    "end": "1610880"
  },
  {
    "text": "so it's by editing",
    "start": "1610880",
    "end": "1614799"
  },
  {
    "text": "now",
    "start": "1616000",
    "end": "1616880"
  },
  {
    "text": "save cluster grass resource",
    "start": "1616880",
    "end": "1621200"
  },
  {
    "text": "case",
    "start": "1621520",
    "end": "1623919"
  },
  {
    "text": "well okay so it means number of ost",
    "start": "1624840",
    "end": "1629600"
  },
  {
    "text": "so let's increase this to two",
    "start": "1629600",
    "end": "1633760"
  },
  {
    "text": "and",
    "start": "1633760",
    "end": "1634720"
  },
  {
    "text": "let's confirm",
    "start": "1634720",
    "end": "1638000"
  },
  {
    "text": "get port",
    "start": "1639679",
    "end": "1641760"
  },
  {
    "text": "okay",
    "start": "1641760",
    "end": "1643279"
  },
  {
    "text": "so the next the second um looks like",
    "start": "1643279",
    "end": "1647600"
  },
  {
    "text": "prepare port will be created soon",
    "start": "1647600",
    "end": "1652240"
  },
  {
    "text": "so wait for a while",
    "start": "1652960",
    "end": "1656480"
  },
  {
    "text": "okay the ost prepare port is created",
    "start": "1658480",
    "end": "1662799"
  },
  {
    "text": "and it's running",
    "start": "1662799",
    "end": "1664720"
  },
  {
    "text": "uh",
    "start": "1664720",
    "end": "1665840"
  },
  {
    "text": "now creating the second osd data",
    "start": "1665840",
    "end": "1669039"
  },
  {
    "text": "structure",
    "start": "1669039",
    "end": "1670559"
  },
  {
    "text": "and now osd port is created",
    "start": "1670559",
    "end": "1673760"
  },
  {
    "text": "and running",
    "start": "1673760",
    "end": "1675840"
  },
  {
    "text": "okay so let's confirm the",
    "start": "1675840",
    "end": "1679039"
  },
  {
    "text": "let's run",
    "start": "1679039",
    "end": "1680840"
  },
  {
    "text": "the save iphone s again",
    "start": "1680840",
    "end": "1686158"
  },
  {
    "text": "okay the number of ost",
    "start": "1686480",
    "end": "1690480"
  },
  {
    "text": "number of ost",
    "start": "1690480",
    "end": "1692559"
  },
  {
    "text": "is now two",
    "start": "1692559",
    "end": "1694480"
  },
  {
    "text": "okay so",
    "start": "1694480",
    "end": "1696000"
  },
  {
    "text": "it means",
    "start": "1696000",
    "end": "1697840"
  },
  {
    "text": "this cluster is expanded correctly",
    "start": "1697840",
    "end": "1702640"
  },
  {
    "start": "1702000",
    "end": "1796000"
  },
  {
    "text": "there are advanced configurations about",
    "start": "1704240",
    "end": "1707360"
  },
  {
    "text": "um pvc base cluster",
    "start": "1707360",
    "end": "1710399"
  },
  {
    "text": "the first one is create position volumes",
    "start": "1710399",
    "end": "1713440"
  },
  {
    "text": "or ost on demand",
    "start": "1713440",
    "end": "1716320"
  },
  {
    "text": "in this demo i prepared",
    "start": "1716320",
    "end": "1719120"
  },
  {
    "text": "the",
    "start": "1719120",
    "end": "1720080"
  },
  {
    "text": "two",
    "start": "1720080",
    "end": "1721919"
  },
  {
    "text": "two",
    "start": "1721919",
    "end": "1722799"
  },
  {
    "text": "positive volume beforehand",
    "start": "1722799",
    "end": "1725760"
  },
  {
    "text": "but if you use gsi drivers with dynamic",
    "start": "1725760",
    "end": "1729760"
  },
  {
    "text": "volume",
    "start": "1729760",
    "end": "1730720"
  },
  {
    "text": "provisioning",
    "start": "1730720",
    "end": "1732320"
  },
  {
    "text": "you can omit this step on this work",
    "start": "1732320",
    "end": "1736559"
  },
  {
    "text": "and the second second configuration is",
    "start": "1736559",
    "end": "1740000"
  },
  {
    "text": "even osd splitting among all nodes",
    "start": "1740000",
    "end": "1744159"
  },
  {
    "text": "to use",
    "start": "1744159",
    "end": "1745679"
  },
  {
    "text": "this feature",
    "start": "1745679",
    "end": "1747039"
  },
  {
    "text": "you can use topology's topology spread",
    "start": "1747039",
    "end": "1750080"
  },
  {
    "text": "constraint feature in kubernetes",
    "start": "1750080",
    "end": "1754240"
  },
  {
    "text": "if you are interested in",
    "start": "1754240",
    "end": "1756480"
  },
  {
    "text": "on these configurations please refer to",
    "start": "1756480",
    "end": "1760559"
  },
  {
    "text": "this blog post",
    "start": "1760559",
    "end": "1763679"
  },
  {
    "text": "thank you satori for the demo",
    "start": "1764399",
    "end": "1766880"
  },
  {
    "text": "uh",
    "start": "1766880",
    "end": "1768640"
  },
  {
    "text": "thank you all for",
    "start": "1768640",
    "end": "1770159"
  },
  {
    "text": "for watching and for your interest in",
    "start": "1770159",
    "end": "1772880"
  },
  {
    "text": "rook",
    "start": "1772880",
    "end": "1774000"
  },
  {
    "text": "uh i'm",
    "start": "1774000",
    "end": "1775120"
  },
  {
    "text": "gonna leave this last slide here with",
    "start": "1775120",
    "end": "1777200"
  },
  {
    "text": "some links to our website and",
    "start": "1777200",
    "end": "1778720"
  },
  {
    "text": "documentation",
    "start": "1778720",
    "end": "1780240"
  },
  {
    "text": "and anything else that you you might",
    "start": "1780240",
    "end": "1782399"
  },
  {
    "text": "have interested in about the rook",
    "start": "1782399",
    "end": "1784080"
  },
  {
    "text": "project and i'm going to attempt to",
    "start": "1784080",
    "end": "1785919"
  },
  {
    "text": "leave these",
    "start": "1785919",
    "end": "1787200"
  },
  {
    "text": "open while we",
    "start": "1787200",
    "end": "1788880"
  },
  {
    "text": "then",
    "start": "1788880",
    "end": "1791039"
  },
  {
    "text": "go to the q a portion of our",
    "start": "1791039",
    "end": "1794480"
  },
  {
    "text": "of our presentation",
    "start": "1794480",
    "end": "1797720"
  }
]