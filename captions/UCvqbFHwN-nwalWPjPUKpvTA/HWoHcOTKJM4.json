[
  {
    "text": "okay let's get started then um hi everyone my name is yanang I'm a",
    "start": "199",
    "end": "5200"
  },
  {
    "text": "principal software engineer at rad working on our hybrid Cloud AI",
    "start": "5200",
    "end": "10360"
  },
  {
    "text": "platform and uh I am eduard Durango and I'm a senior engineer at",
    "start": "10360",
    "end": "16200"
  },
  {
    "text": "Nvidia okay uh more introduction about myself I'm on the leadership team for multiple open source projects including",
    "start": "17080",
    "end": "24080"
  },
  {
    "text": "ago CP flow and also a chair of working group serving under the kubernetes",
    "start": "24080",
    "end": "29320"
  },
  {
    "text": "Community and I also work a lot on the machine learning Frameworks like exus and tensor flow from my older days and I",
    "start": "29320",
    "end": "37320"
  },
  {
    "text": "did publish a book distributed machine learning patterns if you guys want to check it out as well and find me on",
    "start": "37320",
    "end": "43760"
  },
  {
    "text": "social media that's my handle over there so",
    "start": "43760",
    "end": "49600"
  },
  {
    "text": "yeah and uh myself so uh I've been working in making containers and Cloud",
    "start": "49600",
    "end": "56320"
  },
  {
    "text": "Technologies uh better for HPC and and for eight years now nine years now so",
    "start": "56320",
    "end": "63000"
  },
  {
    "text": "basically that's my my curriculum there but I just want to go very very fast uh",
    "start": "63000",
    "end": "68240"
  },
  {
    "text": "currently I work as a co-chair of the working group serving and I actively participate in projects like a working",
    "start": "68240",
    "end": "74080"
  },
  {
    "text": "group bash and also uh I'm trying to help with the Dr",
    "start": "74080",
    "end": "80040"
  },
  {
    "text": "initiatives okay so but how did the working group got started so that's a a",
    "start": "81520",
    "end": "86960"
  },
  {
    "text": "question we got a lot and it's basically a quick history of cubec con right like",
    "start": "86960",
    "end": "93240"
  },
  {
    "text": "is the whole success of cubec con so during Paris cubec con uh sadly is not",
    "start": "93240",
    "end": "98720"
  },
  {
    "text": "here uh Clayton kemman had a quick conversation with Joan before the on conference sessions and then at the on",
    "start": "98720",
    "end": "105560"
  },
  {
    "text": "conference session we all as a community were uh already having the idea of okay",
    "start": "105560",
    "end": "110759"
  },
  {
    "text": "we need a a common place to talk about inference and serban for Q how do we",
    "start": "110759",
    "end": "116640"
  },
  {
    "text": "make this better together because uh at Paris cubec con we saw a lot of talks of",
    "start": "116640",
    "end": "123479"
  },
  {
    "text": "people trying to propose different ways of solving the same challenges and",
    "start": "123479",
    "end": "129720"
  },
  {
    "text": "although that's good for a conference moving forward being disconnected is not good for a community so everything",
    "start": "129720",
    "end": "136360"
  },
  {
    "text": "started at Paris cubec con and I'm happy to say that now at the next Cube con we are already presenting a projects that",
    "start": "136360",
    "end": "143879"
  },
  {
    "text": "this working group is leading uh even things that we have managed to lead and get into curetes and it's been only six",
    "start": "143879",
    "end": "150400"
  },
  {
    "text": "months so it's been a an exciting time for this working group for sure yeah it's exciting like one thing to add is",
    "start": "150400",
    "end": "157200"
  },
  {
    "text": "Clon and I also discussed at Copan Europe we talked a lot about some of the",
    "start": "157200",
    "end": "162640"
  },
  {
    "text": "solved challenges and pinpoints fun ecosystem ecosystem projects like caser",
    "start": "162640",
    "end": "169360"
  },
  {
    "text": "right whether we can bring some of those advancements to the lowlevel kubernetes apis whether those are the things we can",
    "start": "169360",
    "end": "176400"
  },
  {
    "text": "benefit The Wider communities so working group serving is like the best place to",
    "start": "176400",
    "end": "182120"
  },
  {
    "text": "host those vendor neutral um advancements the second question we get",
    "start": "182120",
    "end": "188000"
  },
  {
    "text": "asked a lot is why is not called working group inference why is it called working",
    "start": "188000",
    "end": "193080"
  },
  {
    "text": "group serving and it basically goes down to we uh are serving inference models",
    "start": "193080",
    "end": "200680"
  },
  {
    "text": "that are already pre-trained right so when when you you mentioned the word inference is basically that you are",
    "start": "200680",
    "end": "207120"
  },
  {
    "text": "getting a a pre-trained model and you are inferencing based on a prompt to get a",
    "start": "207120",
    "end": "213720"
  },
  {
    "text": "an output but you are not serving that model right so working group serving goes deeper than just the inference word",
    "start": "213720",
    "end": "220840"
  },
  {
    "text": "we are going over how can we escale out a model how can we draun a model in a curetes closer in production how can we",
    "start": "220840",
    "end": "227680"
  },
  {
    "text": "get metrics how can we get alerts how can we we get production grade inference",
    "start": "227680",
    "end": "233400"
  },
  {
    "text": "and that is actually serving so it's a one of like the top questions that we",
    "start": "233400",
    "end": "238680"
  },
  {
    "text": "get here and and in other meetings so we wanted to let it uh settle here at this",
    "start": "238680",
    "end": "244480"
  },
  {
    "text": "talk yeah and it's not just for AI and machine learning workloads like hopefully some of the benefits we bring",
    "start": "244480",
    "end": "251480"
  },
  {
    "text": "to the community can also benefit traditional workloads like databases and",
    "start": "251480",
    "end": "257280"
  },
  {
    "text": "uh other like uh web services and so",
    "start": "257280",
    "end": "261958"
  },
  {
    "text": "on so another question we get is uh working groups working groups working",
    "start": "263120",
    "end": "268360"
  },
  {
    "text": "groups so working groups all the way right so uh at Cube Compares for AI",
    "start": "268360",
    "end": "274800"
  },
  {
    "text": "related topics uh the community was at the curetes working group bash The cncf",
    "start": "274800",
    "end": "280560"
  },
  {
    "text": "Bash working group and the cncf AI working group right but uh the main",
    "start": "280560",
    "end": "286280"
  },
  {
    "text": "difference between cncf working groups and cures working groups is that at cncf working groups you get to talk about",
    "start": "286280",
    "end": "293000"
  },
  {
    "text": "things that are not even related to kubernetes right it's Cloud overall we could talk about Apache Spar there or",
    "start": "293000",
    "end": "300039"
  },
  {
    "text": "Apachi air flow and is still related to to Ai and Bash so at at Paris we had the",
    "start": "300039",
    "end": "306800"
  },
  {
    "text": "idea of we needed two more working groups to help Define what we need for curetes not not cncf but curetes",
    "start": "306800",
    "end": "314039"
  },
  {
    "text": "specifically about working group serving and working group device management so these two groups were born out of a cube",
    "start": "314039",
    "end": "321160"
  },
  {
    "text": "companies where we were talking about all the gaps that cetes has towards inference and sering AI models and this",
    "start": "321160",
    "end": "328919"
  },
  {
    "text": "is why we created these groups and the main difference the key difference between serving and Bash working group",
    "start": "328919",
    "end": "334360"
  },
  {
    "text": "because people also ask the say like is Bash also for for the same like you are also running distributed War Los in bash",
    "start": "334360",
    "end": "342039"
  },
  {
    "text": "can you can do inference with Q which people like get confused so no badge working group is going to be focused on",
    "start": "342039",
    "end": "349560"
  },
  {
    "text": "training and on distributed workloads and Bash workloads and sering working",
    "start": "349560",
    "end": "355199"
  },
  {
    "text": "group is going to focus on things like that we will be presenting today that llm Gateway uh Dr multihost inference",
    "start": "355199",
    "end": "364919"
  },
  {
    "text": "and we will kind of like hand off like all the batch related topics to the",
    "start": "364919",
    "end": "370080"
  },
  {
    "text": "working group bash yeah so the working group um",
    "start": "370080",
    "end": "376319"
  },
  {
    "text": "serving is C currently led by four different companies Google Cloud Red Hat",
    "start": "376319",
    "end": "381400"
  },
  {
    "text": "Andia and B Dan but we have more than just those leading companies we have",
    "start": "381400",
    "end": "386479"
  },
  {
    "text": "many participating organizations we have grown the community to be over 250",
    "start": "386479",
    "end": "391639"
  },
  {
    "text": "community members so the community is growing really fast there's a lot of interest in this space and we invite all",
    "start": "391639",
    "end": "398599"
  },
  {
    "text": "of you to participate as well and contribute your ideas so why is this working group",
    "start": "398599",
    "end": "406840"
  },
  {
    "text": "important so we have as is defined in the GitHub uh web page that you could",
    "start": "406840",
    "end": "412599"
  },
  {
    "text": "find we have three main goals for the working group right that hopefully in two three years down the road we will be",
    "start": "412599",
    "end": "419080"
  },
  {
    "text": "able to close the working group saying that these goals have been reached and we will then have the need to create",
    "start": "419080",
    "end": "425479"
  },
  {
    "text": "another working group with a crazier uh purpose right but our goals right now",
    "start": "425479",
    "end": "430599"
  },
  {
    "text": "are to enhance cetes workload controllers meaning we need a better way to control when we deploy llm models in",
    "start": "430599",
    "end": "439560"
  },
  {
    "text": "kubernetes with projects like kerve uh AOS scaling with projects like K and and",
    "start": "439560",
    "end": "444960"
  },
  {
    "text": "we need to get this better because right now there are a lot of challenges so part of the conversation ation during",
    "start": "444960",
    "end": "450199"
  },
  {
    "text": "the working group meetings is talking with the community as youan pointed that we have 250 participants in the working",
    "start": "450199",
    "end": "457960"
  },
  {
    "text": "group meetings and we need to find all the gaps when you are deploying these",
    "start": "457960",
    "end": "463520"
  },
  {
    "text": "llm models and try to fix them so we as a community benefit from that the second",
    "start": "463520",
    "end": "470080"
  },
  {
    "text": "uh goal that we have is investigate orchestration for scalability and and",
    "start": "470080",
    "end": "475360"
  },
  {
    "text": "the word investigate is there because as Jan will be presented uh later on we",
    "start": "475360",
    "end": "481080"
  },
  {
    "text": "right now have an initiative that is called the benchmarking uh project and it's because we are",
    "start": "481080",
    "end": "486919"
  },
  {
    "text": "investigating uh how these llm models behave in a cetes cluster what do we need to monitor what do we need to see",
    "start": "486919",
    "end": "494039"
  },
  {
    "text": "if it is running okay or not right like right now we're kind of like Running Blind and mostly because every model is",
    "start": "494039",
    "end": "500560"
  },
  {
    "text": "different some models if you track the GP utilization that you are kind of like getting the picture but some models uh",
    "start": "500560",
    "end": "506560"
  },
  {
    "text": "is more like the CPU utilization the memory utilization the network band weight so it's like depending on the",
    "start": "506560",
    "end": "512000"
  },
  {
    "text": "model the use case what you need to track to be able to have good scalable",
    "start": "512000",
    "end": "517479"
  },
  {
    "text": "production ready systems and and good out of scaling uh is still kind of like being defined and that's something that",
    "start": "517479",
    "end": "523760"
  },
  {
    "text": "we are chatting on the working group meetings like weekly we discuss about a",
    "start": "523760",
    "end": "529399"
  },
  {
    "text": "a member of the community will join and say like hey for my model if I track this specific metric everything is is",
    "start": "529399",
    "end": "535880"
  },
  {
    "text": "okay so we are taking note and we want everyone to participate because as it's there investigate we're investigating",
    "start": "535880",
    "end": "542880"
  },
  {
    "text": "what do we need to know about running llm models and uh the last but not least",
    "start": "542880",
    "end": "548560"
  },
  {
    "text": "is optimized resource sharing and since Paris and before Paris uh cucon the the",
    "start": "548560",
    "end": "554920"
  },
  {
    "text": "fancy word has been Dr so this optimization of resources goes back to Dr uh the working group has a direct",
    "start": "554920",
    "end": "562360"
  },
  {
    "text": "communication with the working group device management because we want to make sure that we also leverage Dr for",
    "start": "562360",
    "end": "569000"
  },
  {
    "text": "things like like multinode operations and also uh the better utilization of",
    "start": "569000",
    "end": "575480"
  },
  {
    "text": "gpus so how is the working group operated",
    "start": "576519",
    "end": "583160"
  },
  {
    "text": "oops Yeah so we divide and conquer into different work streams so the first work stream I'm going to talk about is the",
    "start": "586839",
    "end": "593560"
  },
  {
    "text": "orchestration work stream so what we are doing uh within that work stream is to identify challenges with with u those",
    "start": "593560",
    "end": "600600"
  },
  {
    "text": "ecosystem projects like ker and Ray so we make sure like we understand the",
    "start": "600600",
    "end": "605800"
  },
  {
    "text": "challenges and obstructions needed uh for orchestrating uh large serving",
    "start": "605800",
    "end": "611880"
  },
  {
    "text": "workloads um understand like some of the pinpoints and use cases fund this ecosystem projects and some relevant sub",
    "start": "611880",
    "end": "620240"
  },
  {
    "text": "projects that we are working on I'll talk about them in more details our serving catalog and our LM instance",
    "start": "620240",
    "end": "627959"
  },
  {
    "text": "Gateway the second work stream is the multi multihost uh work stream uh so we are",
    "start": "627959",
    "end": "634399"
  },
  {
    "text": "what we are trying to do is to extract patterns and um and common practices for",
    "start": "634399",
    "end": "640480"
  },
  {
    "text": "running uh multihost multi node inference workloads so we talk about different implementations and different",
    "start": "640480",
    "end": "647880"
  },
  {
    "text": "ways to achieve cost Effectiveness and different ways to optimize capacity and",
    "start": "647880",
    "end": "654600"
  },
  {
    "text": "uh and so on so we are also seeing a lot of increased uh demand for large models",
    "start": "654600",
    "end": "660279"
  },
  {
    "text": "on multiple nodes uh so larger models don't fit on a single node and we have",
    "start": "660279",
    "end": "665480"
  },
  {
    "text": "to partition and serve them in a multie fashion um so leader work set is one of",
    "start": "665480",
    "end": "672440"
  },
  {
    "text": "the example project uh we are working on uh so it provides some common multi note",
    "start": "672440",
    "end": "678720"
  },
  {
    "text": "deployment deployment patterns uh for um for models that are shed across multiple",
    "start": "678720",
    "end": "684920"
  },
  {
    "text": "nodes and we are also working with ecosystem projects like case to add",
    "start": "684920",
    "end": "690720"
  },
  {
    "text": "support for Martin note as well um so the the Martin note support for ker uh",
    "start": "690720",
    "end": "696040"
  },
  {
    "text": "the pr for that uh feature was just got merg merged uh last week so if you're",
    "start": "696040",
    "end": "702880"
  },
  {
    "text": "one of the adventurous users uh F free to try it out uh it's not part of any of",
    "start": "702880",
    "end": "708680"
  },
  {
    "text": "the releases yet but we do want to hear about uh some early feedback from and users so that we can continue improving",
    "start": "708680",
    "end": "716240"
  },
  {
    "text": "uh going forward uh the third work stream is the outer scaling work stream and here is",
    "start": "716240",
    "end": "724120"
  },
  {
    "text": "very related to what I was talking about investigating so a like a big chunk of",
    "start": "724120",
    "end": "729680"
  },
  {
    "text": "the conversation that are having around AOS scaling are metrics right we want to",
    "start": "729680",
    "end": "734760"
  },
  {
    "text": "know the best metrics to track for autoscaling and we want to be able to work with ker and kada to build an a",
    "start": "734760",
    "end": "742639"
  },
  {
    "text": "proper production ready out scaling solution but still we want to uh find",
    "start": "742639",
    "end": "748120"
  },
  {
    "text": "out the right ICS to track so that's why we are running this Benchmark projects",
    "start": "748120",
    "end": "753399"
  },
  {
    "text": "and we are going to the community to ask okay what are you tracking when you are AOS scaling but it's it's not just the",
    "start": "753399",
    "end": "760240"
  },
  {
    "text": "metrics right another big topic for AOS scaling is these models as Jan just mentioned they are getting bigger and",
    "start": "760240",
    "end": "767199"
  },
  {
    "text": "bigger and then it's not just creating more notes and adding notes your closer",
    "start": "767199",
    "end": "772560"
  },
  {
    "text": "but is how do you move the model that is heavy enough now right like hundreds of gigabytes to the newly created noes so",
    "start": "772560",
    "end": "779199"
  },
  {
    "text": "are you going are we going to do Network attach volumes are we going to do image caching uh like where are we going with",
    "start": "779199",
    "end": "786320"
  },
  {
    "text": "this so uh the outer scaling working group we are working in initiatives like",
    "start": "786320",
    "end": "791480"
  },
  {
    "text": "the oci volume source that is a topic that years ago as I was telling youan",
    "start": "791480",
    "end": "797120"
  },
  {
    "text": "was called Data containers or data images it's basically a container that is not a a a file system on its own but",
    "start": "797120",
    "end": "804600"
  },
  {
    "text": "it's basically just data and also the model service metrics so we need to standardize these",
    "start": "804600",
    "end": "812199"
  },
  {
    "text": "Concepts and the last work uh work stream of the working group is the Dr",
    "start": "812199",
    "end": "818720"
  },
  {
    "text": "right and the whole idea of this work stream is to keep a direct communication with the working group uh device",
    "start": "818720",
    "end": "825680"
  },
  {
    "text": "management so it's not like the working group serving is directly working on the aray but we are collecting use cases and",
    "start": "825680",
    "end": "833800"
  },
  {
    "text": "feedback for Dr and potentially box and then we are communicating them to the work work group device management uh",
    "start": "833800",
    "end": "840920"
  },
  {
    "text": "luckily we have people like uh John and Sergey that participate in both working",
    "start": "840920",
    "end": "846000"
  },
  {
    "text": "groups so we have that that bridge between the both working groups and we",
    "start": "846000",
    "end": "851560"
  },
  {
    "text": "help it to push for beta from the working group serving right so it's kind of like our responsibility to go and",
    "start": "851560",
    "end": "857320"
  },
  {
    "text": "help the other working group to push initiatives and and proposals and right",
    "start": "857320",
    "end": "863000"
  },
  {
    "text": "now we are working with the Dr working group device management to work in",
    "start": "863000",
    "end": "868040"
  },
  {
    "text": "multihoster being partitioner device support so we hope to have these",
    "start": "868040",
    "end": "873120"
  },
  {
    "text": "features in by 1.34 or 1.33 or 1.34 so that's kind of",
    "start": "873120",
    "end": "879120"
  },
  {
    "text": "like the responsibility of the work stream okay next I'll talk about some of",
    "start": "879120",
    "end": "885560"
  },
  {
    "text": "the current initiatives from this working group so the first one is the RRM instance Gateway uh so it's",
    "start": "885560",
    "end": "892959"
  },
  {
    "text": "currently on Enid based toing to support more more like production ready uh and",
    "start": "892959",
    "end": "899360"
  },
  {
    "text": "uh efficient way of serving multiple use cases um uh across a shared pool of um",
    "start": "899360",
    "end": "907000"
  },
  {
    "text": "Hardware accelerators uh those use cases and all L adapters are running on the",
    "start": "907000",
    "end": "912680"
  },
  {
    "text": "same Foundation model so we want to make sure the way to share resources is optimal and efficient and the um and the",
    "start": "912680",
    "end": "920399"
  },
  {
    "text": "routing to those um lurer adapters uh can be like maximized uh especially the",
    "start": "920399",
    "end": "927360"
  },
  {
    "text": "throughput and we we want to also ensure that there's fairness sh across those different uh RM",
    "start": "927360",
    "end": "936120"
  },
  {
    "text": "Services uh with distinct priorities and latency objectives and we also want to Prov make",
    "start": "936120",
    "end": "943360"
  },
  {
    "text": "sure like the con configuration for uh various Laural adapters and Runing out",
    "start": "943360",
    "end": "949920"
  },
  {
    "text": "new adapters can be easy uh to end users and uh to allow graduate um safer rout",
    "start": "949920",
    "end": "957360"
  },
  {
    "text": "for new adapters so this is an architecture diagram for the RM instance Gateway and on the left",
    "start": "957360",
    "end": "965199"
  },
  {
    "text": "hand side you can see an example spec for the LM in service uh basically you",
    "start": "965199",
    "end": "971519"
  },
  {
    "text": "can specify the pool of resources and the target models that uh with different",
    "start": "971519",
    "end": "977319"
  },
  {
    "text": "weights uh for those uh different use cases um and we are using exra par from",
    "start": "977319",
    "end": "984759"
  },
  {
    "text": "Envoy to receive and rck the traffics uh to different um pool uh different pool of um Laur",
    "start": "984759",
    "end": "994199"
  },
  {
    "text": "adapters and resources and based on the Matrix we've been collecting from uh for",
    "start": "994199",
    "end": "999959"
  },
  {
    "text": "example cash sites um to determine a a more intelligent routing uh to those uh",
    "start": "999959",
    "end": "1006319"
  },
  {
    "text": "La adapters so another initiative that",
    "start": "1006319",
    "end": "1011800"
  },
  {
    "text": "we're running as you can see here is following through Dr specific features",
    "start": "1011800",
    "end": "1017120"
  },
  {
    "text": "and uh you will find the list of so uh things that would merg in 1.32 and you will then find it now uh available for",
    "start": "1017120",
    "end": "1025120"
  },
  {
    "text": "you structural parameters faster scheduling the removal of the classic Dr",
    "start": "1025120",
    "end": "1030480"
  },
  {
    "text": "right so now it's uh that it went to Beta it kind of like Dro it a lot of the things that were proposed on on the",
    "start": "1030480",
    "end": "1036000"
  },
  {
    "text": "alpha API and uh significant process and AOS scaling integration so on the on the",
    "start": "1036000",
    "end": "1043160"
  },
  {
    "text": "chair the slide that you will find in the sket platform you can just click over this and you will get redirected to",
    "start": "1043160",
    "end": "1048480"
  },
  {
    "text": "each specific cap that was merged for 1.32 and here as well uh we are getting",
    "start": "1048480",
    "end": "1055720"
  },
  {
    "text": "uh Nvidia Dr uh driver ready for gpus we have the example uh driver for Dr if you",
    "start": "1055720",
    "end": "1062320"
  },
  {
    "text": "want to build your your own Dr driver for your own specific needs uh Google uh",
    "start": "1062320",
    "end": "1068200"
  },
  {
    "text": "the TPU driver is in process that I think it's it will be ready for next cubec con and the cni Dr driver so all",
    "start": "1068200",
    "end": "1074799"
  },
  {
    "text": "of these initiatives are in progress and we from the working group serving and tracking them to see how can we help",
    "start": "1074799",
    "end": "1080679"
  },
  {
    "text": "them how can we help get the Caps pushed forward so they get included in the next",
    "start": "1080679",
    "end": "1086559"
  },
  {
    "text": "curs release uh miss it but on track for the next release are also a couple of",
    "start": "1086559",
    "end": "1092400"
  },
  {
    "text": "caps that we are helping to review and also push forward so they get included",
    "start": "1092400",
    "end": "1097679"
  },
  {
    "text": "in the next qet release and as I said there are many more to come right like uh at the working group serving and also",
    "start": "1097679",
    "end": "1104159"
  },
  {
    "text": "the working group device management there are a lot of talk PS being discussed on what can we that to Dr so",
    "start": "1104159",
    "end": "1110840"
  },
  {
    "text": "we make it better and we can run more complex",
    "start": "1110840",
    "end": "1116080"
  },
  {
    "text": "workloads the the other topic as I was mentioning before is uh something that I don't know if I'm old enough or everyone",
    "start": "1116120",
    "end": "1123280"
  },
  {
    "text": "here is old as me but uh I think at Docker con or cube con like 2018 or 19",
    "start": "1123280",
    "end": "1130960"
  },
  {
    "text": "people were already talking about data containers like that was kind of like a topic that people were mentioning and",
    "start": "1130960",
    "end": "1136640"
  },
  {
    "text": "now it's becoming relevant and is becoming relevant for model caching these models are so big that we need to",
    "start": "1136640",
    "end": "1143600"
  },
  {
    "text": "find ways to move them around our kuber netics clusters and we found out that",
    "start": "1143600",
    "end": "1149799"
  },
  {
    "text": "leveraging on what we already have with the image spec that is the layers we could move data around with these layers",
    "start": "1149799",
    "end": "1156919"
  },
  {
    "text": "right and and Docker podman cryo all of them already know how to pull and push",
    "start": "1156919",
    "end": "1162200"
  },
  {
    "text": "layers and when we have a big container uh our run times already know how to do",
    "start": "1162200",
    "end": "1167440"
  },
  {
    "text": "also parallel pooling so we want to leverage all of these features that we already have in container run times and",
    "start": "1167440",
    "end": "1173640"
  },
  {
    "text": "move uh llm models around and that's uh the oci artifact initiative that we are",
    "start": "1173640",
    "end": "1181240"
  },
  {
    "text": "running um I just want to quickly mention that Al ker also supports our",
    "start": "1181240",
    "end": "1186640"
  },
  {
    "text": "model car feature which also streamlines the process to pull models directly from",
    "start": "1186640",
    "end": "1192039"
  },
  {
    "text": "ocii image registry and which could enable enable more efficient Auto",
    "start": "1192039",
    "end": "1197640"
  },
  {
    "text": "scaling uh so that's currently a Hy solution but we do plan to um uh make",
    "start": "1197640",
    "end": "1204080"
  },
  {
    "text": "sure the transition to OCR volume source is smooth so as long as you are using",
    "start": "1204080",
    "end": "1209120"
  },
  {
    "text": "model carard now I think there will be a very simple toggle to turn on the",
    "start": "1209120",
    "end": "1214200"
  },
  {
    "text": "experimental feature for oci volume source as well next one is you next I want to talk",
    "start": "1214200",
    "end": "1222760"
  },
  {
    "text": "about the serving catalog so this is one of the project the serving group uh serving working group is uh working on",
    "start": "1222760",
    "end": "1229600"
  },
  {
    "text": "so we want to make sure we have a place to provide working examples for different popular model servers like",
    "start": "1229600",
    "end": "1237039"
  },
  {
    "text": "vrm open models like Lama M show and so on and also provide examples for",
    "start": "1237039",
    "end": "1244000"
  },
  {
    "text": "different deployment patterns like single host or multihost inference and",
    "start": "1244000",
    "end": "1249760"
  },
  {
    "text": "we also want to make sure like people can find all those different Primitives and orchestration Frameworks uh within",
    "start": "1249760",
    "end": "1256919"
  },
  {
    "text": "the same catalog as well as part of uh the exercise of building this catalog we",
    "start": "1256919",
    "end": "1262679"
  },
  {
    "text": "are also exploring the common configurations and patterns for influence workloads so that we can",
    "start": "1262679",
    "end": "1269400"
  },
  {
    "text": "understand um uh what are the things we need to abtract going forward and",
    "start": "1269400",
    "end": "1274799"
  },
  {
    "text": "whether we can provide some really common parameters uh so that the community can based on uh uh and trick",
    "start": "1274799",
    "end": "1282840"
  },
  {
    "text": "from there and um we are also like supporting multiple fun times as I",
    "start": "1282840",
    "end": "1289640"
  },
  {
    "text": "mentioned earlier uh we currently support vrm and J stream and several",
    "start": "1289640",
    "end": "1294679"
  },
  {
    "text": "different models but more model servers and model support are coming up uh soon",
    "start": "1294679",
    "end": "1300880"
  },
  {
    "text": "and there are also contributions on Autos scaling for HPA configurations as",
    "start": "1300880",
    "end": "1305919"
  },
  {
    "text": "well and we we're also adding additional templates for multihost inference to",
    "start": "1305919",
    "end": "1312440"
  },
  {
    "text": "this catalog uh the next initiative is the benchmarking tool we recently just",
    "start": "1312440",
    "end": "1319840"
  },
  {
    "text": "started working on um we so the current state is M different companies have the",
    "start": "1319840",
    "end": "1325600"
  },
  {
    "text": "different tools to Benchmark their large language performance uh either for",
    "start": "1325600",
    "end": "1330679"
  },
  {
    "text": "different model server run times or different models different use cases are you using Laura or not like everything",
    "start": "1330679",
    "end": "1338919"
  },
  {
    "text": "are very specific to internal use cases so um we set together with different uh",
    "start": "1338919",
    "end": "1345520"
  },
  {
    "text": "communities and companies who are building this similar benchmarking tools",
    "start": "1345520",
    "end": "1350760"
  },
  {
    "text": "We Gather like uh the use cases and requirements together so what we are",
    "start": "1350760",
    "end": "1356480"
  },
  {
    "text": "working on now is a um more standardized benchmarking tool that can be used as a",
    "start": "1356480",
    "end": "1362840"
  },
  {
    "text": "library uh that's uh that support multiple different uh model Ser model servers and it's Hardware agnostic and",
    "start": "1362840",
    "end": "1370679"
  },
  {
    "text": "simple deploy uh especially on kubernetes um so we can use it use this",
    "start": "1370679",
    "end": "1377240"
  },
  {
    "text": "standardized benchmark tool to solve different benchmarking needs like Auto",
    "start": "1377240",
    "end": "1382760"
  },
  {
    "text": "scaling or lower use cases especially with the instance uh benchmarking the",
    "start": "1382760",
    "end": "1389240"
  },
  {
    "text": "instance Gateway uh using this um new tool that we are coming up so I um I",
    "start": "1389240",
    "end": "1395679"
  },
  {
    "text": "won't get into the details here but we have a proposal coming up which we'll be sharing with the community soon so you",
    "start": "1395679",
    "end": "1403640"
  },
  {
    "text": "we invite everybody to join the community meeting if you'd like to hear more details about this new",
    "start": "1403640",
    "end": "1411679"
  },
  {
    "text": "project um I also like to talk about some of the involvement with the",
    "start": "1411720",
    "end": "1416760"
  },
  {
    "text": "ecosystem project uh one thing I have in mind is the caser project so we are uh",
    "start": "1416760",
    "end": "1422679"
  },
  {
    "text": "working on as you might have heard we are working on the AI Gateway project uh",
    "start": "1422679",
    "end": "1428080"
  },
  {
    "text": "in kerve uh that's uh like collaborative effort between Bloomberg and Envoy uh",
    "start": "1428080",
    "end": "1434320"
  },
  {
    "text": "communities and we want to make sure like we are um aligned with the LM instance Gateway as",
    "start": "1434320",
    "end": "1441600"
  },
  {
    "text": "well so going forward we are try to figure out a a way to work together and",
    "start": "1441600",
    "end": "1447240"
  },
  {
    "text": "integrate well with each other and we're also contributing to the serving catalog to make sure we provide enough examples",
    "start": "1447240",
    "end": "1454520"
  },
  {
    "text": "to the uh communities is uh the case of examples um to the community so that uh",
    "start": "1454520",
    "end": "1461360"
  },
  {
    "text": "the community members can explore those uh patterns and um um starting point can",
    "start": "1461360",
    "end": "1467600"
  },
  {
    "text": "use those as a starting point for the production ready uh serving workloads",
    "start": "1467600",
    "end": "1472960"
  },
  {
    "text": "and we' also like to Leverage The benchmarking tool we are working on and",
    "start": "1472960",
    "end": "1478039"
  },
  {
    "text": "uh as part of this um collaboration with the working group serving we did Identify some of the gaps as you can see",
    "start": "1478039",
    "end": "1484840"
  },
  {
    "text": "from the right hand side like there's a table of differences between um the um",
    "start": "1484840",
    "end": "1492840"
  },
  {
    "text": "the blueprint proposal that was shared with the working group serving community and kerve we identify the missing parts",
    "start": "1492840",
    "end": "1500080"
  },
  {
    "text": "in case of and we take active actions um to implement and try to feel that gap",
    "start": "1500080",
    "end": "1507360"
  },
  {
    "text": "for example the multihost feature that we recently added to caser is one of the",
    "start": "1507360",
    "end": "1512960"
  },
  {
    "text": "big um uh request uh like frequently requested feature from the community um",
    "start": "1512960",
    "end": "1520559"
  },
  {
    "text": "so we uh we also published our road map uh in case serve in case you want to know more about what we are working on",
    "start": "1520559",
    "end": "1528360"
  },
  {
    "text": "and planning to work on uh not every uh tasks items are already assigned yet so",
    "start": "1528360",
    "end": "1535559"
  },
  {
    "text": "if you are interested in contributing uh Join one of the caser meeting Community meetings and we figure out how to get",
    "start": "1535559",
    "end": "1543080"
  },
  {
    "text": "you started contributing and we do have a lot of",
    "start": "1543080",
    "end": "1549120"
  },
  {
    "text": "talks uh at coucom this is just a list of example talks we um we already had",
    "start": "1549120",
    "end": "1557120"
  },
  {
    "text": "given this is Friday uh a lot of the talks were given uh I think all of them I guess yeah but uh",
    "start": "1557120",
    "end": "1564720"
  },
  {
    "text": "this talk will also be recorded so in case you miss it one of these just look up these names in YouTube and you can",
    "start": "1564720",
    "end": "1571520"
  },
  {
    "text": "watch the recording in case you miss it so make sure like if you're interested in Cas of there are specific talks",
    "start": "1571520",
    "end": "1577640"
  },
  {
    "text": "related to that there's also a very popular panel from the case of community",
    "start": "1577640",
    "end": "1583000"
  },
  {
    "text": "that showcases how we collaborate with each other like with the communities and Industry partners and there are um uh",
    "start": "1583000",
    "end": "1591360"
  },
  {
    "text": "also like call outs in different sessions as well",
    "start": "1591360",
    "end": "1596640"
  },
  {
    "text": "um yeah how do we get involved um you can find the charter information and",
    "start": "1596640",
    "end": "1603360"
  },
  {
    "text": "under kubernetes Community rapo um we have slack channel on",
    "start": "1603360",
    "end": "1608760"
  },
  {
    "text": "kubernetes uh slack Works uh workspace and uh feel free to reach out to one of",
    "start": "1608760",
    "end": "1614200"
  },
  {
    "text": "the co-chairs and sub project leads and some of uh them are also attending the",
    "start": "1614200",
    "end": "1619520"
  },
  {
    "text": "talk so I see a few over there so feel free to stick around and hang out with",
    "start": "1619520",
    "end": "1625440"
  },
  {
    "text": "us after the session special thanks to Ray who couldn't make it and help us with the slides as well so uh Sergey",
    "start": "1625440",
    "end": "1632720"
  },
  {
    "text": "also a a working group chair is here and Asar is also there working on the",
    "start": "1632720",
    "end": "1637960"
  },
  {
    "text": "benchmarking and Magic standardization yeah yeah yeah thank you",
    "start": "1637960",
    "end": "1643399"
  },
  {
    "text": "everyone uh for for helping us this this it was Jan and myself but this is a join",
    "start": "1643399",
    "end": "1649200"
  },
  {
    "text": "for us we are pointing him names but uh a coupon only allows two at the",
    "start": "1649200",
    "end": "1654840"
  },
  {
    "text": "station yeah thank you everyone yeah thank",
    "start": "1654840",
    "end": "1659440"
  },
  {
    "text": "you do we have time for some q& time for one or two questions yeah we we do have time",
    "start": "1661240",
    "end": "1667559"
  },
  {
    "text": "so any questions there's a mic in the uh Center if you are okay hey yeah I'm Alex",
    "start": "1667559",
    "end": "1674799"
  },
  {
    "text": "and I'm actually really happy that I came here so this is perfectly uh we lead to in terms of work um so one",
    "start": "1674799",
    "end": "1682200"
  },
  {
    "text": "thing I saw is there was of course excuse me do you mind speaking closely was this big announcement about uh SMG",
    "start": "1682200",
    "end": "1688279"
  },
  {
    "text": "glue like the AI Gateway from solo like coming in and then of course kerf has",
    "start": "1688279",
    "end": "1694159"
  },
  {
    "text": "the inference Gateway is there like any connection between these two right now or this is going to be figured out over time there",
    "start": "1694159",
    "end": "1700880"
  },
  {
    "text": "yeah there are some overlap and we are trying to make sure like we leverage whatever IO is already there so to avoid",
    "start": "1700880",
    "end": "1708480"
  },
  {
    "text": "any duplicating effort so yeah as I mentioned in one of my slide we are",
    "start": "1708480",
    "end": "1713679"
  },
  {
    "text": "we'll be working more closely together uh between the two um between working group and case of community so we'll try",
    "start": "1713679",
    "end": "1721039"
  },
  {
    "text": "to figure um out a plan forward as well if you have any uh specific feedback uh",
    "start": "1721039",
    "end": "1727039"
  },
  {
    "text": "feel free to join our meetings and we uh we are very open to",
    "start": "1727039",
    "end": "1733440"
  },
  {
    "text": "suggestions yeah thank you um preparing for this talk I was also discussing with Joan that uh part of the working group",
    "start": "1733440",
    "end": "1739919"
  },
  {
    "text": "effort is to avoid uh duplication so we can focus in in single ideas and work",
    "start": "1739919",
    "end": "1747080"
  },
  {
    "text": "together as a community so this is kind of like part of our job right like see if there is someone out there proposing",
    "start": "1747080",
    "end": "1754559"
  },
  {
    "text": "something and we are already working on that idea so we just invite them to the working",
    "start": "1754559",
    "end": "1760720"
  },
  {
    "text": "group great talk thank you uh I guess I'm curious about with fractions now and",
    "start": "1760720",
    "end": "1768360"
  },
  {
    "text": "Carpenter doing things like bin hacking is there any connection between these",
    "start": "1768360",
    "end": "1774200"
  },
  {
    "text": "things I mean putting workloads across nodes that have got a lot of gpus on",
    "start": "1774200",
    "end": "1781519"
  },
  {
    "text": "them um any thought to bin",
    "start": "1781519",
    "end": "1787320"
  },
  {
    "text": "hacking so that would be into the working group and the workstream orchestration",
    "start": "1787320",
    "end": "1794080"
  },
  {
    "text": "right yeah um where does Carpenter fit into all of this I guess that's the work",
    "start": "1794080",
    "end": "1799679"
  },
  {
    "text": "group cluster isn't it yeah if you have any scaling yeah I",
    "start": "1799679",
    "end": "1805279"
  },
  {
    "text": "don't have anything on top of my mind but if you have specific questions feel free to join our course and we can",
    "start": "1805279",
    "end": "1811880"
  },
  {
    "text": "discuss with the community so yeah I mean I can imagine a whole",
    "start": "1811880",
    "end": "1818559"
  },
  {
    "text": "bunch of inference servers that are I mean this is the IBM use case that we're talking about as well yeah a whole bunch of inference servers that are smaller",
    "start": "1818559",
    "end": "1824279"
  },
  {
    "text": "than a single note and you want to bin pack them right",
    "start": "1824279",
    "end": "1829960"
  },
  {
    "text": "okay how you like Ms and partion by size and like",
    "start": "1832559",
    "end": "1839679"
  },
  {
    "text": "try to make it smaller like all Gateway for instance it's very different from other game because it has use cases",
    "start": "1839679",
    "end": "1847159"
  },
  {
    "text": "inside model so you have one foundational model multiple use cases and now the sharing G them it's easy way",
    "start": "1847159",
    "end": "1853720"
  },
  {
    "text": "to share and it's very powerful but uh what you're talking about maybe different models like like common foundational model and",
    "start": "1853720",
    "end": "1861440"
  },
  {
    "text": "this a littleit different this case to imp that we need first",
    "start": "1861440",
    "end": "1867880"
  },
  {
    "text": "I better that's why we doing right now yeah that's part of it right I mean you",
    "start": "1868080",
    "end": "1873559"
  },
  {
    "text": "can create a",
    "start": "1873559",
    "end": "1876000"
  },
  {
    "text": "Frac or you want to multiple CSS",
    "start": "1882559",
    "end": "1887840"
  },
  {
    "text": "Road Focus make a get",
    "start": "1890559",
    "end": "1897559"
  },
  {
    "text": "ad Good Start thank",
    "start": "1897919",
    "end": "1901679"
  },
  {
    "text": "you so uh let me do a quick survey who um who's not in working group serving",
    "start": "1903600",
    "end": "1909840"
  },
  {
    "text": "yet who hasn't got involved yet okay good good to know yeah so we invite",
    "start": "1909840",
    "end": "1917279"
  },
  {
    "text": "you all to join like to share anything uh specific specific questions and use",
    "start": "1917279",
    "end": "1923760"
  },
  {
    "text": "cases it's uh it's open community so",
    "start": "1923760",
    "end": "1929840"
  },
  {
    "text": "yeah thank you everyone no more questions",
    "start": "1929840",
    "end": "1935240"
  }
]