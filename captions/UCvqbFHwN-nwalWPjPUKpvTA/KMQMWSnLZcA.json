[
  {
    "text": "so this is a little bit different panel that we're going to be doing we're kind of have a presentation and um and we'll",
    "start": "40",
    "end": "8240"
  },
  {
    "text": "be talking as well one of our members who's also a member of the KE flow steering committee um and our working",
    "start": "8240",
    "end": "15120"
  },
  {
    "text": "groups is johu George he's with nanic longtime um Community member unfortunately he couldn't be here with",
    "start": "15120",
    "end": "21279"
  },
  {
    "text": "us today but he did help us um in the making of the the slide deck and and",
    "start": "21279",
    "end": "26439"
  },
  {
    "text": "does an excellent job um on the working group groups and committees that he's with and now if you all will introduce",
    "start": "26439",
    "end": "33879"
  },
  {
    "text": "yourselves absolutely so hi everyone my name is Andre I'm also part of ke flow",
    "start": "33879",
    "end": "39360"
  },
  {
    "text": "steering community and I've been in this community for almost six years so just from the beginning I saw the",
    "start": "39360",
    "end": "46520"
  },
  {
    "text": "transitions um from Google project to open project to Sans and I'm so exciting",
    "start": "46520",
    "end": "52000"
  },
  {
    "text": "to see so many people on this Sumit I think it's one of the most um like one of the biggest sum we have for last six",
    "start": "52000",
    "end": "58320"
  },
  {
    "text": "years so have be part of Cube con community and be here so yeah hi everyone uh I'm Yuki uh I'm",
    "start": "58320",
    "end": "67960"
  },
  {
    "text": "reading uh C for aut and training with Andre and jonu and also uh I'm a cues",
    "start": "67960",
    "end": "76360"
  },
  {
    "text": "batch working group cor member and I'm a kubernetes c maintainer uh thank",
    "start": "76360",
    "end": "84520"
  },
  {
    "text": "you so our components that we're going to be looking at today and as you've also got a sneak peek of one of the",
    "start": "86320",
    "end": "93360"
  },
  {
    "text": "announcements that's coming it's not on this slide but the spark operator as well so we have training operator ctib",
    "start": "93360",
    "end": "99240"
  },
  {
    "text": "and the MPI operator um and so we'll take a look at at those today and I don't know if you guys want to stand the",
    "start": "99240",
    "end": "105240"
  },
  {
    "text": "whole time or if you want to sit but um there you go yeah I can stay thank you Amber so",
    "start": "105240",
    "end": "112360"
  },
  {
    "text": "yeah as Amber said we're going to speak about KP and training corporator during the session and just to remind everyone",
    "start": "112360",
    "end": "118320"
  },
  {
    "text": "um is you SE the previous K is a project was dedicated to do hyper primary search in Cloud Med way so the idea is to",
    "start": "118320",
    "end": "126159"
  },
  {
    "text": "connect this open source ml AI research libraries like hyperopt up tuna psychic",
    "start": "126159",
    "end": "132680"
  },
  {
    "text": "learn to kubernetes and build this kind of like you know layer between uh kuet",
    "start": "132680",
    "end": "137720"
  },
  {
    "text": "infrastructure and ml libraries like building the sdks building Web uis",
    "start": "137720",
    "end": "143120"
  },
  {
    "text": "building API access and KP has like several features like parameter search",
    "start": "143120",
    "end": "148519"
  },
  {
    "text": "architecture search um also like some experiment tracking workflow tunic optimization and additional um UI",
    "start": "148519",
    "end": "156120"
  },
  {
    "text": "capabilities here we can move on uh so this is architecture we try to again we",
    "start": "156120",
    "end": "161959"
  },
  {
    "text": "hear you we try to address the problem with documentation we try to improve our documentation and this is how",
    "start": "161959",
    "end": "167680"
  },
  {
    "text": "architecture of KP looks like from the user perspective user just need to use the Q flow python SDK where they can",
    "start": "167680",
    "end": "173680"
  },
  {
    "text": "Define what they want to optimize how they want to optimize what sech algorithm they want to use and what how",
    "start": "173680",
    "end": "179400"
  },
  {
    "text": "many how much resources they have how many gpus they have how many trials they want to use and then can just use Python",
    "start": "179400",
    "end": "185440"
  },
  {
    "text": "SDK to start the experiment and inside the ctive we have three different controllers so the first one experiment",
    "start": "185440",
    "end": "191280"
  },
  {
    "text": "which is responsible for experiment CR custom resource suggestion controller which is spawns algorithm service the",
    "start": "191280",
    "end": "197440"
  },
  {
    "text": "algorithm service produce hyper parameters and these hyper parameters uh go all the way to the trial controller",
    "start": "197440",
    "end": "203319"
  },
  {
    "text": "which you we just does evaluation on top of the different customer sources like we support B jobs workflows pyter jobs",
    "start": "203319",
    "end": "211439"
  },
  {
    "text": "so we can orchestrate any type of custom resource on top of kuus which can do you much more sophisticated uh hyper tuning",
    "start": "211439",
    "end": "218560"
  },
  {
    "text": "experiment and then at the end we're collecting trials uh trial matrics all the way to cpdb and going back to the",
    "start": "218560",
    "end": "225480"
  },
  {
    "text": "our evaluation step so this is native kind of hyper opimization uh uh flow and",
    "start": "225480",
    "end": "231120"
  },
  {
    "text": "we try to reimplement it using the native cus infrastructure yep uh so in our 2024",
    "start": "231120",
    "end": "238680"
  },
  {
    "text": "road map we looking for several items I think the first one as we saw before in the presentations we try to we try to be",
    "start": "238680",
    "end": "245920"
  },
  {
    "text": "more closer to newest llm capabilities so we recently implemented new train API",
    "start": "245920",
    "end": "251439"
  },
  {
    "text": "to fine-tune your llm so we try to actually have a tune API which easily can find you your LMS and I will speak",
    "start": "251439",
    "end": "258160"
  },
  {
    "text": "about it a bit later also we try to gradiate KB to V1 uh we recently did",
    "start": "258160",
    "end": "264320"
  },
  {
    "text": "some features but we still need to some work to be done like we want to like for example uh Define new API so this is",
    "start": "264320",
    "end": "270639"
  },
  {
    "text": "some proposed uh you know API name is like tuner model search support more",
    "start": "270639",
    "end": "275800"
  },
  {
    "text": "things from like you know feature engineering or model compressor so we think about how we can make this API",
    "start": "275800",
    "end": "281720"
  },
  {
    "text": "composable also we want to support push based metrics like using the some python SDK support more parameter distribution",
    "start": "281720",
    "end": "288680"
  },
  {
    "text": "like Lo lock uniform what like auna for example is doing and we have one some integration with jupyter on books",
    "start": "288680",
    "end": "294479"
  },
  {
    "text": "because it's like the main tool for qf flow users who interact with these components uh",
    "start": "294479",
    "end": "300479"
  },
  {
    "text": "so and then let me jump to training Operator just a reminder training operator similar to ktip is the",
    "start": "300479",
    "end": "307000"
  },
  {
    "text": "framework which connects this deep learning libraries and M libraries with commus infrastructure by uh by utilizing",
    "start": "307000",
    "end": "315199"
  },
  {
    "text": "commus apis and also Python apis and training operator has several features like distributed training capabilities",
    "start": "315199",
    "end": "321680"
  },
  {
    "text": "where you can run several workers on multiple gpus uh with really large scale",
    "start": "321680",
    "end": "327759"
  },
  {
    "text": "and we saw from the user store with that actually people using this for what large scale experiments like train your",
    "start": "327759",
    "end": "333479"
  },
  {
    "text": "LM from scratch uh also we support a l like old redu type of training we also",
    "start": "333479",
    "end": "338600"
  },
  {
    "text": "have API operator capabilities for HPC task like high performance Computing we supporting some different job scheduling",
    "start": "338600",
    "end": "345360"
  },
  {
    "text": "and elastic training also there because we're kind of native to pych libraries",
    "start": "345360",
    "end": "350440"
  },
  {
    "text": "um yeah so this is like example what you can do with training operator this is a like the thing is how we can do all",
    "start": "350440",
    "end": "355960"
  },
  {
    "text": "reduced type of training with pytch and training operator will be responsible to actually set up that so if if you're",
    "start": "355960",
    "end": "361360"
  },
  {
    "text": "familiar with py torch py torch has the torch run C which helps you to spin up this workers and we basically spin up",
    "start": "361360",
    "end": "368160"
  },
  {
    "text": "all of the workers for you by using this sub operator and from the user perspective similar to ktip they just",
    "start": "368160",
    "end": "373599"
  },
  {
    "text": "said how many gpus they have and they can start this spyer job from the function I will show you in the demo how",
    "start": "373599",
    "end": "380680"
  },
  {
    "text": "easy it is to do and the idea is like training operator will be responsible to spawn these trials and they're going to",
    "start": "380680",
    "end": "387080"
  },
  {
    "text": "uh then share the gradients between them to average and actually train the model on multiple workers because if your",
    "start": "387080",
    "end": "393039"
  },
  {
    "text": "model is too large you need to distribute your model between workers and you distribute your data um so this",
    "start": "393039",
    "end": "398840"
  },
  {
    "text": "is one example of how we can use old red use within training operator and if we go next slide here example how we can",
    "start": "398840",
    "end": "405680"
  },
  {
    "text": "use tensorflow distributed so this is just one of the example what you can do within Q flow and the idea is like in",
    "start": "405680",
    "end": "411199"
  },
  {
    "text": "tensorflow you can distribute your data across multiple workers and CU flow training operator will be responsible to",
    "start": "411199",
    "end": "418120"
  },
  {
    "text": "schedule appropriate workers set up appropriate environment and then um you can train very large models um on",
    "start": "418120",
    "end": "426160"
  },
  {
    "text": "multiple workers and using multiple gpus and a parameter server will be responsible to uh generate new weights",
    "start": "426160",
    "end": "432400"
  },
  {
    "text": "based on based on the gradients results from all of your workers um yeah so",
    "start": "432400",
    "end": "437720"
  },
  {
    "text": "right now I really want to speak about Simplicity and we heard a lot today that hey qflow is a great tool but it's",
    "start": "437720",
    "end": "443960"
  },
  {
    "text": "complex right we want to simplify this and we hear you and we did some work in the last year to simplify the complexity",
    "start": "443960",
    "end": "451759"
  },
  {
    "text": "of kuas for specifically for IML community and how we do this uh we yeah",
    "start": "451759",
    "end": "458280"
  },
  {
    "text": "if if we go on we try to have this kind of user flow so the idea is we have data",
    "start": "458280",
    "end": "463960"
  },
  {
    "text": "science data scientists who really want to so they really want to work inside jupyter notebooks they don't want to",
    "start": "463960",
    "end": "469599"
  },
  {
    "text": "leave notebook and they do they want to do everything inside notebook so they start a notebook server they code their",
    "start": "469599",
    "end": "476120"
  },
  {
    "text": "training there or training scripts or tuning scripts then they set how many like how much resources they have and",
    "start": "476120",
    "end": "482479"
  },
  {
    "text": "then they use some sdks of apis to actually schedule it on top of scale it",
    "start": "482479",
    "end": "487680"
  },
  {
    "text": "on top of com infrastructure using the Q flow capabilities uh so this is from the",
    "start": "487680",
    "end": "492759"
  },
  {
    "text": "user perspective how it looks like from the flow perspective and if we go next slide here how it looks like from the",
    "start": "492759",
    "end": "499280"
  },
  {
    "text": "API perspective so on the left side you can see that this is native pytorch uh distributed API so basically",
    "start": "499280",
    "end": "506960"
  },
  {
    "text": "if you're familiar with pytorch usually you say hey I use distributed data parallel with for example NSL like nid",
    "start": "506960",
    "end": "513760"
  },
  {
    "text": "Collective communication Library back end um I want to attach my model to distributor I want to set set up my",
    "start": "513760",
    "end": "519479"
  },
  {
    "text": "Optimizer and I'm going to train my model right so the idea is like using native pytorch API and then scale this",
    "start": "519479",
    "end": "527200"
  },
  {
    "text": "function using the create job API on the qflow environment so similar to kfp if",
    "start": "527200",
    "end": "533519"
  },
  {
    "text": "you're familiar with qfl pipelines lightweights python components we try to give a user simple python API which they",
    "start": "533519",
    "end": "540120"
  },
  {
    "text": "can use to scale this function across multiple workers and we're going to serialize this function uh for you",
    "start": "540120",
    "end": "547279"
  },
  {
    "text": "similar to ktip you just pass this function in the tune API where you define your objective your hyper",
    "start": "547279",
    "end": "553320"
  },
  {
    "text": "parameters uh the trial threshold and the resources like gpus and other metrics so very simple very pythonic no",
    "start": "553320",
    "end": "560600"
  },
  {
    "text": "coets no yamas no Docker just a pure python from your notebook so yeah so",
    "start": "560600",
    "end": "566160"
  },
  {
    "text": "right now I just want to give you quick demo and as so recent like previously um",
    "start": "566160",
    "end": "572000"
  },
  {
    "text": "we've been speaking a lot about llms right and how important they are for industry for Community right now and",
    "start": "572000",
    "end": "579079"
  },
  {
    "text": "recently community did some effort to simplify fine-tuning for qflow users and",
    "start": "579079",
    "end": "585279"
  },
  {
    "text": "uh actually integrate a new tune train API so I hope my de demo is going to",
    "start": "585279",
    "end": "590800"
  },
  {
    "text": "work so the thing is like uh this is a notebook uh in this notebook what we're",
    "start": "590800",
    "end": "596560"
  },
  {
    "text": "actually going to do we're going to fine-tune bird so if you're familiar with uh be is actually uh called B",
    "start": "596560",
    "end": "603399"
  },
  {
    "text": "directional um encoder represent representation from Transformers like one of the most famous LMS which was",
    "start": "603399",
    "end": "609720"
  },
  {
    "text": "developed by Google and we will try to fine-tune it on yel preview that I said so this example is similar to like some",
    "start": "609720",
    "end": "615760"
  },
  {
    "text": "hugging face examples that you saw in in the internet so uh we try to ad those examples using the Q training operators",
    "start": "615760",
    "end": "622800"
  },
  {
    "text": "so in this notebook what we're going to do first uh first of all we're just going to download some samples from our",
    "start": "622800",
    "end": "628480"
  },
  {
    "text": "data set so here we're using the hugging phase data set to load some data and check some results so the data set",
    "start": "628480",
    "end": "635399"
  },
  {
    "text": "contains some yeld preview results with some Stars so basically label indicate",
    "start": "635399",
    "end": "641120"
  },
  {
    "text": "uh what is a star user gave and here we have a text so this is the data set we're going to use and then we probably",
    "start": "641120",
    "end": "648240"
  },
  {
    "text": "we just need to Define our training script to find T the model so just to speed up this experiment let me submit",
    "start": "648240",
    "end": "654519"
  },
  {
    "text": "it on the cluster first so the idea from the from the data set sence from data",
    "start": "654519",
    "end": "660200"
  },
  {
    "text": "scientist perspective they just need to Define this training function where they just going to use native hugging phase",
    "start": "660200",
    "end": "667519"
  },
  {
    "text": "Transformer apis so if you're familiar with hugging phase this should be very simple for you to understand the idea is",
    "start": "667519",
    "end": "673839"
  },
  {
    "text": "like you kind of use this kind of you know uh from pre-trained API where you take the bir model then we're going to",
    "start": "673839",
    "end": "681760"
  },
  {
    "text": "um use tokenizer from this model we're going to download data set then using the mapping to adjust our tokenizer for",
    "start": "681760",
    "end": "689760"
  },
  {
    "text": "this preview data set uh here we're going to distribute our data because the main uh Power of CBE Cube flow actually",
    "start": "689760",
    "end": "697600"
  },
  {
    "text": "can distribute your data across multiple workloads so rank can World size is the",
    "start": "697600",
    "end": "702839"
  },
  {
    "text": "parameters that we're going to use to define which worker it is running this this script is running on and how many",
    "start": "702839",
    "end": "708920"
  },
  {
    "text": "workers we have so we distribute our train and test data set uh and then we",
    "start": "708920",
    "end": "714720"
  },
  {
    "text": "Define the training arguments so this is again native hugging phase trainer arguments where you can say hey I want",
    "start": "714720",
    "end": "721279"
  },
  {
    "text": "to like uh evaluate my EPO I want to um output my trainer inside T Trin function",
    "start": "721279",
    "end": "728240"
  },
  {
    "text": "I also can for example like uh have some model uh checkpoints if I want and I can",
    "start": "728240",
    "end": "733880"
  },
  {
    "text": "Define my trainer here so again native to hugging phase trainer which can",
    "start": "733880",
    "end": "739040"
  },
  {
    "text": "evaluate my training function uh and then at the end we use trainer. train and we're exporting model all the way to",
    "start": "739040",
    "end": "745120"
  },
  {
    "text": "S3 so we can do additional evaluations so this is the function user Define inside the notebook and the thing is",
    "start": "745120",
    "end": "752040"
  },
  {
    "text": "like we going to use this crate job API to scale this function on the multiple",
    "start": "752040",
    "end": "757639"
  },
  {
    "text": "workers so this will run not in inside my notebook but inside the comus environment so here you can see the",
    "start": "757639",
    "end": "764000"
  },
  {
    "text": "training function you can see the job name you can see the parameters that can pass like the bucket name uh the",
    "start": "764000",
    "end": "769680"
  },
  {
    "text": "resources so I'm going to use one GPU and four CPU for every of my markers so",
    "start": "769680",
    "end": "774880"
  },
  {
    "text": "in total it's like three three gpus I'm going to use to fine tune my llm so this is the packages I'm going toall if if",
    "start": "774880",
    "end": "781160"
  },
  {
    "text": "I'm of pipeline look like similar like we have this packages to install we have this kind of a crate job function and",
    "start": "781160",
    "end": "787680"
  },
  {
    "text": "looks similar so I've already created this so right now we can consume our pter job conditions so we",
    "start": "787680",
    "end": "794959"
  },
  {
    "text": "can see that job is running right now uh we can see that since we're running this on three workers we're actually using",
    "start": "794959",
    "end": "800639"
  },
  {
    "text": "three different pods so py like training operator will schedle three ports for you like one master two workers and we",
    "start": "800639",
    "end": "807440"
  },
  {
    "text": "also can consume in my notebook the locks so if we check the locks here we can see",
    "start": "807440",
    "end": "813160"
  },
  {
    "text": "that we download our bir model we map our review that I set to bir tokenizer",
    "start": "813160",
    "end": "819480"
  },
  {
    "text": "and then we actually start training so this model actually using 108 million parameters to train it and then we do",
    "start": "819480",
    "end": "827000"
  },
  {
    "text": "training we do evaluations uh we run it several several times and and then end we're just exploring model all the way to S3 so",
    "start": "827000",
    "end": "834839"
  },
  {
    "text": "then me as a data scientist I just want toload my model to check the evaluation so I'm going I'm not going to run this",
    "start": "834839",
    "end": "841320"
  },
  {
    "text": "because it will take time my model like it size is around 400 megabytes um just",
    "start": "841320",
    "end": "847120"
  },
  {
    "text": "to show you like evaluations like from the evaluation perspective um I'm again using the hugging face API to pass my",
    "start": "847120",
    "end": "853759"
  },
  {
    "text": "model to the pipeline and checking what kind of output it going to produce so I'm passing like the one of the good",
    "start": "853759",
    "end": "859399"
  },
  {
    "text": "reviews and one of the b reviews so do some uh text um uh sentiment analysis",
    "start": "859399",
    "end": "865839"
  },
  {
    "text": "type of task so as we can see if I pass good review it actually give me the good star if I passing the bad review it get",
    "start": "865839",
    "end": "872440"
  },
  {
    "text": "me the best Star so here it's I actually fine tunic llm using qflow training operator is very simple very pythonic",
    "start": "872440",
    "end": "879800"
  },
  {
    "text": "pure to notebooks I don't use any Docker files or any yamos and this is how easy",
    "start": "879800",
    "end": "885279"
  },
  {
    "text": "this to do so what is next right um how make it even more easy right like",
    "start": "885279",
    "end": "891480"
  },
  {
    "text": "usually data scientists they want to iterate quickly they want to fine-tune their model much faster without even",
    "start": "891480",
    "end": "896880"
  },
  {
    "text": "defining training script and more important ly how to distribute the data right because imagine if you use 100 of",
    "start": "896880",
    "end": "904120"
  },
  {
    "text": "workers or 100 of gpus how you going to download data on these workers uh it is not very straightforward for data",
    "start": "904120",
    "end": "910560"
  },
  {
    "text": "scientist to do because they need to you know create sharable PVC thinking about other thing and for this kind of purpose",
    "start": "910560",
    "end": "917160"
  },
  {
    "text": "we developed this train API which actually simplify the ability to F tune",
    "start": "917160",
    "end": "922800"
  },
  {
    "text": "LMS so I will speak about this on the next slide but the idea is like uh user",
    "start": "922800",
    "end": "929000"
  },
  {
    "text": "from user perspective they just need to use this new train API um this one and the idea is like",
    "start": "929000",
    "end": "937440"
  },
  {
    "text": "usually in fine tuning like fine tuning is basically the ability of how like the the uh the modern way of how people",
    "start": "937440",
    "end": "945480"
  },
  {
    "text": "right now train their uh pre uh train their models which already been pre-trained so instead of like training",
    "start": "945480",
    "end": "952199"
  },
  {
    "text": "your model from scratch you're already tracking the model which been trained on like hundreds of gpus and try to adopt",
    "start": "952199",
    "end": "957480"
  },
  {
    "text": "it on your specific use cases so in this train API we say hey I have this this number of workers I have this",
    "start": "957480",
    "end": "964519"
  },
  {
    "text": "number of processes per workers so if you if you know like in torch run you can specify how many process you want to",
    "start": "964519",
    "end": "970480"
  },
  {
    "text": "use per node like if you want to use like multi-gpu multi- worker training so",
    "start": "970480",
    "end": "975880"
  },
  {
    "text": "this also like similar parameters in this train API so then user Define a model provider and the data set provider",
    "start": "975880",
    "end": "983600"
  },
  {
    "text": "so model provider is actually we supporting right now several model providers so we've been focus on hugging",
    "start": "983600",
    "end": "989720"
  },
  {
    "text": "phase initially where model where a user can say hey I want to use this model URI",
    "start": "989720",
    "end": "996639"
  },
  {
    "text": "uh and I want to like use this Transformer to fine-tune my model right and also in data set provider uh we also",
    "start": "996639",
    "end": "1004680"
  },
  {
    "text": "support like I believe S3 right now in Hing face but we're planning to support more like GCS or any other providers and",
    "start": "1004680",
    "end": "1011839"
  },
  {
    "text": "from the user perspective they just need to say um I want to use this Freo so this is like the H face Freo and we have",
    "start": "1011839",
    "end": "1018440"
  },
  {
    "text": "some like parameters like for example if I just want to use 3,000 samples from my data set not like the full data set to",
    "start": "1018440",
    "end": "1024798"
  },
  {
    "text": "find to do the F tuning and then the user just defined the training parameters so instead of like asking",
    "start": "1024799",
    "end": "1031038"
  },
  {
    "text": "user to define a training function uh we kind of like pre-create this trainer for the user where uh user just can specify",
    "start": "1031039",
    "end": "1039319"
  },
  {
    "text": "the training parameters and like Laura config so if you're familiar with like",
    "start": "1039319",
    "end": "1044400"
  },
  {
    "text": "recent fine-tuning um Technologies lur is like one of the best uh um um like",
    "start": "1044400",
    "end": "1050080"
  },
  {
    "text": "one of the most efficient way of how you can find tuna models so you can specify the Laurel config directly in the",
    "start": "1050080",
    "end": "1055840"
  },
  {
    "text": "trainer parameters lur actually will lock some layers so it will reduce the number number of train trainable",
    "start": "1055840",
    "end": "1061960"
  },
  {
    "text": "parameters for your very large model so can significantly uh reduce the the cost",
    "start": "1061960",
    "end": "1067520"
  },
  {
    "text": "to train your model to F tun your model sorry and here is again like you just Define your training parameters you",
    "start": "1067520",
    "end": "1072760"
  },
  {
    "text": "define your Laura config you define your resources and you just submit this job and at the end um as a said we kind of",
    "start": "1072760",
    "end": "1080000"
  },
  {
    "text": "like using this storage initializer to download model and to download data set",
    "start": "1080000",
    "end": "1086400"
  },
  {
    "text": "and we can consume our logs directly in the notebook at the end so since we use Lowa number of trainable parameters will",
    "start": "1086400",
    "end": "1093039"
  },
  {
    "text": "be smaller so it's just three 100,000 trainable parameters in this model and as we can see we have exactly the same",
    "start": "1093039",
    "end": "1099559"
  },
  {
    "text": "results where my model is being trained and where my my model is been fine tune actually at the end and I can use it for",
    "start": "1099559",
    "end": "1106039"
  },
  {
    "text": "my experimentation so the new IDE is and if I go back to my",
    "start": "1106039",
    "end": "1112000"
  },
  {
    "text": "slides um so the new idea of actually how users",
    "start": "1112000",
    "end": "1118240"
  },
  {
    "text": "can use this new apis for llm finetuning looks like this so the as I show you in",
    "start": "1118240",
    "end": "1125120"
  },
  {
    "text": "the demo user just Define what kind of model they want to use what kind of data set they want to use the trainer",
    "start": "1125120",
    "end": "1131520"
  },
  {
    "text": "parameters and the the number of resources they want to use to find tuna model right so then uh our orchestration",
    "start": "1131520",
    "end": "1139360"
  },
  {
    "text": "layer looks uh like this so we have like a model provider and data set provider which is responsible to download data",
    "start": "1139360",
    "end": "1146520"
  },
  {
    "text": "from different like resources like hugging phase or S3 and we kind of distribute this this data across",
    "start": "1146520",
    "end": "1153640"
  },
  {
    "text": "multiple workers using M sharable PVC so your PVC has to support R only many",
    "start": "1153640",
    "end": "1159760"
  },
  {
    "text": "access um um access mode to distribute data across cable workers and we have",
    "start": "1159760",
    "end": "1164799"
  },
  {
    "text": "this trainer which actually pre pre-created trainer uh which which actually Define like the training clop",
    "start": "1164799",
    "end": "1172440"
  },
  {
    "text": "um for the user so user doesn't even need to worry about how to define my training script so this is very",
    "start": "1172440",
    "end": "1177760"
  },
  {
    "text": "extensible because imagine this can be even apply for not NLP based TX but",
    "start": "1177760",
    "end": "1182840"
  },
  {
    "text": "maybe image classification or forecasting so you can basically Define so many different trainers that",
    "start": "1182840",
    "end": "1189039"
  },
  {
    "text": "different uh users your users or your customers can use and uh it's very powerful um of different kind of support",
    "start": "1189039",
    "end": "1195960"
  },
  {
    "text": "we can offer so if you want to try this notebook please follow this example we already upload this um this presentation",
    "start": "1195960",
    "end": "1202559"
  },
  {
    "text": "um on the schedule uh but we're looking forward for your feedback because this is a new thing and we try to extend it",
    "start": "1202559",
    "end": "1208320"
  },
  {
    "text": "to support more features and offer you more capabilities to do the fine tuning within Q flow so just quickly so we have",
    "start": "1208320",
    "end": "1216520"
  },
  {
    "text": "this kind of road map of uh offering different capabilities for different projects like we already offering this train API for training operators to do",
    "start": "1216520",
    "end": "1223760"
  },
  {
    "text": "llm fine tuning also we want to offer this tune API to do your llm tuning and",
    "start": "1223760",
    "end": "1229240"
  },
  {
    "text": "we also want to extend it all the way to the caser so because the Q flow already has like the underlying infrastructure",
    "start": "1229240",
    "end": "1235480"
  },
  {
    "text": "which can uh give us ability to train tune s very large models and this is",
    "start": "1235480",
    "end": "1240520"
  },
  {
    "text": "kind of unique compared to other like um things that you can do on the cloud in the cloud native",
    "start": "1240520",
    "end": "1246559"
  },
  {
    "text": "environment yes so this here let me pass it to Yuki so you can speak about uh okay uh let me explain uh what's new in",
    "start": "1246559",
    "end": "1254760"
  },
  {
    "text": "training operators uh since B 1.7 7.0 uh",
    "start": "1254760",
    "end": "1259840"
  },
  {
    "text": "we have started to okay uh we have started to",
    "start": "1259840",
    "end": "1266320"
  },
  {
    "text": "support support okay suspend suspend proces uh",
    "start": "1266320",
    "end": "1272200"
  },
  {
    "text": "this feature allows us to stop CU for jobs without uh removing of",
    "start": "1272200",
    "end": "1277919"
  },
  {
    "text": "jobs uh in the next slide uh I will I will introduce the training operator",
    "start": "1277919",
    "end": "1283960"
  },
  {
    "text": "2024 road maps",
    "start": "1283960",
    "end": "1289600"
  },
  {
    "text": "uh the first one ISM uh train SD kpi uh",
    "start": "1289600",
    "end": "1294799"
  },
  {
    "text": "uh as Android uh explained in the previous part uh recently we implemented",
    "start": "1294799",
    "end": "1301720"
  },
  {
    "text": "uh it this IPI has not yet being released so we will include uh this",
    "start": "1301720",
    "end": "1307679"
  },
  {
    "text": "feature in the next training operator version B 1.8 uh the second one is uh extending",
    "start": "1307679",
    "end": "1315679"
  },
  {
    "text": "person SDK trainer uh currently uh we support only hugging face trainer uh so",
    "start": "1315679",
    "end": "1323080"
  },
  {
    "text": "we are planning to support other trainers like image classifications and",
    "start": "1323080",
    "end": "1329360"
  },
  {
    "text": "uh deep speed uh the third one is supporting the uh Jax Frameworks uh",
    "start": "1329360",
    "end": "1336400"
  },
  {
    "text": "currently uh we are planning to add uh new job type uh Jax",
    "start": "1336400",
    "end": "1342320"
  },
  {
    "text": "job uh the first one is uh mitigation of",
    "start": "1342320",
    "end": "1347480"
  },
  {
    "text": "su accessib to the data preparation step uh in typical environments uh data",
    "start": "1347480",
    "end": "1355520"
  },
  {
    "text": "scientists perform fine cheering against downloaded Foundation models uh if the model data uh is so",
    "start": "1355520",
    "end": "1363919"
  },
  {
    "text": "large like llm uh it can occur to waste significant CPU usage and download",
    "start": "1363919",
    "end": "1372520"
  },
  {
    "text": "time uh currently uh our trainer Pon SDK allows us to mitigate such issues as",
    "start": "1372520",
    "end": "1379960"
  },
  {
    "text": "under explained previous slides but uh we aim to reduce the waste more and more",
    "start": "1379960",
    "end": "1388440"
  },
  {
    "text": "by uh Pro providing accessibility to the data processing phase like aach",
    "start": "1388440",
    "end": "1396240"
  },
  {
    "text": "Arrow uh the fifth possibility is web hook validation uh the training operator has",
    "start": "1396240",
    "end": "1403679"
  },
  {
    "text": "internal validation mechanism but the this era uh",
    "start": "1403679",
    "end": "1409000"
  },
  {
    "text": "eror message can be seen by in operator LS so we are planning to introduce web",
    "start": "1409000",
    "end": "1418120"
  },
  {
    "text": "comparations uh this feature allows us to notify errors per job creation and uh",
    "start": "1418120",
    "end": "1424200"
  },
  {
    "text": "bring the better ux uh the r candidate is supporting MP",
    "start": "1424200",
    "end": "1431320"
  },
  {
    "text": "ja beta uh currently the training operator supports only MP job B1 and uh",
    "start": "1431320",
    "end": "1438159"
  },
  {
    "text": "we can can use MP jaob bit only be NP operator uh so we will support the NP",
    "start": "1438159",
    "end": "1445559"
  },
  {
    "text": "jaob bit in the training operator as",
    "start": "1445559",
    "end": "1449840"
  },
  {
    "text": "well uh next uh I will introduce uh M operator uh MP operator is a standard",
    "start": "1450600",
    "end": "1458400"
  },
  {
    "text": "operator to solve only M job B2 uh let me pay attention this diagram to",
    "start": "1458400",
    "end": "1465600"
  },
  {
    "text": "describe MP operator uh once MPI job is created uh",
    "start": "1465600",
    "end": "1470720"
  },
  {
    "text": "MP operator creates Rancher and worker part and uh set up header services for",
    "start": "1470720",
    "end": "1477320"
  },
  {
    "text": "worker Parts uh after all worker Parts uh completed to",
    "start": "1477320",
    "end": "1483000"
  },
  {
    "text": "intialization uh the runch part runes training processes in all worker",
    "start": "1483000",
    "end": "1490398"
  },
  {
    "text": "Parts uh as a next topic uh let me introduce uh and operate new features uh",
    "start": "1491919",
    "end": "1499120"
  },
  {
    "text": "these features are not released yet but you can use uh them in the head",
    "start": "1499120",
    "end": "1505880"
  },
  {
    "text": "Branch uh the First new feature is MPS support uh before uh we support the two",
    "start": "1505880",
    "end": "1513159"
  },
  {
    "text": "types of MPI implementation uh op MPI and Inter MPI in the next version B uh",
    "start": "1513159",
    "end": "1522520"
  },
  {
    "text": "0.5 uh we start to support NP uh the second new feature is uh",
    "start": "1522520",
    "end": "1530600"
  },
  {
    "text": "Rancher creation policy uh this feature allows us to configure when the MP",
    "start": "1530600",
    "end": "1537799"
  },
  {
    "text": "operator should create a Rancher rooll p uh this this default at startup policy",
    "start": "1537799",
    "end": "1546559"
  },
  {
    "text": "is the same as before uh the wait for workers ready policy allows us to create",
    "start": "1546559",
    "end": "1552559"
  },
  {
    "text": "a luncher p after the workers get ready uh in general uh work has PA take a long",
    "start": "1552559",
    "end": "1559840"
  },
  {
    "text": "time to start because the container image combined with Nvidia could so",
    "start": "1559840",
    "end": "1566279"
  },
  {
    "text": "large size and downloading needs to take a long time if we use the we for work",
    "start": "1566279",
    "end": "1574120"
  },
  {
    "text": "already policy uh we can avoid unnecessary pot Recreation but uh note that uh if we use",
    "start": "1574120",
    "end": "1582640"
  },
  {
    "text": "both the waight for work ready and uh uh gang scheduling uh it can occur conflict",
    "start": "1582640",
    "end": "1590360"
  },
  {
    "text": "and then the MP operator will fail to start M",
    "start": "1590360",
    "end": "1595840"
  },
  {
    "text": "job the next new feature is uh R launcher as worker uh by default the mpf",
    "start": "1595919",
    "end": "1603440"
  },
  {
    "text": "operator set up Rancher Port only as a Rancher uh when we turn off round",
    "start": "1603440",
    "end": "1610520"
  },
  {
    "text": "Rancher as worker uh we should schedule a Rancher to know result dpu to avoid",
    "start": "1610520",
    "end": "1617360"
  },
  {
    "text": "consuming uh GPU resources but uh in this hyp scale",
    "start": "1617360",
    "end": "1623159"
  },
  {
    "text": "environments like having different networks such as uh EET and infinite",
    "start": "1623159",
    "end": "1629880"
  },
  {
    "text": "band uh it can increase Deb difficulty so it can decrease the difficulty of",
    "start": "1629880",
    "end": "1636320"
  },
  {
    "text": "tble shooting uh in this slide uh let me",
    "start": "1636320",
    "end": "1643000"
  },
  {
    "text": "introduce uh the 2024 uh road map uh the first possibility is restart policy",
    "start": "1643000",
    "end": "1649799"
  },
  {
    "text": "based on the exist code uh in other Frameworks like Pyon job uh we already",
    "start": "1649799",
    "end": "1655799"
  },
  {
    "text": "support this restart policy so we are planning to support it uh as well as MP",
    "start": "1655799",
    "end": "1663279"
  },
  {
    "text": "job the second is the uh migration of pring parts to bat BM index job in",
    "start": "1663279",
    "end": "1669559"
  },
  {
    "text": "worker role uh we already created the Rancher Ro be index job but we still",
    "start": "1669559",
    "end": "1677600"
  },
  {
    "text": "directly create worker post our motivation is supporting all B",
    "start": "1677600",
    "end": "1683760"
  },
  {
    "text": "ban job like these functions uh the rest of candidate is uh",
    "start": "1683760",
    "end": "1689480"
  },
  {
    "text": "multi cluster NP job disting by",
    "start": "1689480",
    "end": "1694000"
  },
  {
    "text": "Q uh by supporting the managed bu feature in the MP job the same as bban",
    "start": "1694840",
    "end": "1702159"
  },
  {
    "text": "uh job uh we can submit MP job into multi clusters in environments managed",
    "start": "1702159",
    "end": "1707840"
  },
  {
    "text": "by Q uh please join these sessions to understand uh the mar stage of",
    "start": "1707840",
    "end": "1713279"
  },
  {
    "text": "dispatching by Q for more",
    "start": "1713279",
    "end": "1717120"
  },
  {
    "text": "details um so a lot of exciting features from API operator as well so just the",
    "start": "1718640",
    "end": "1724799"
  },
  {
    "text": "last thing but the most important I would say huge thank you for everyone who contributed for this release per",
    "start": "1724799",
    "end": "1731399"
  },
  {
    "text": "releases so we try to leas the most active folks for last six month so there been a lot of other people who tried to",
    "start": "1731399",
    "end": "1737120"
  },
  {
    "text": "contribute and the most thinking like piece that users can do to be closer for us is to start contributing code so we",
    "start": "1737120",
    "end": "1744559"
  },
  {
    "text": "try to identify and recognize folks who are active in the community and who are helping us to build better um Services",
    "start": "1744559",
    "end": "1753039"
  },
  {
    "text": "as such as ktip training operator um also NPI operator so huge",
    "start": "1753039",
    "end": "1760720"
  },
  {
    "text": "Applause on everyone who been involved in this um recent effort around LMS and",
    "start": "1760720",
    "end": "1765880"
  },
  {
    "text": "other features and I think lastly um I just want to say few",
    "start": "1765880",
    "end": "1771080"
  },
  {
    "text": "things about the community and um as Amber said before we encourage everyone to get involved in working groups uh we",
    "start": "1771080",
    "end": "1778880"
  },
  {
    "text": "meet regularly for every working group in community where Community is very open uh we open to any contribution so",
    "start": "1778880",
    "end": "1786039"
  },
  {
    "text": "please join the working group calls please join the community call uh let us know about your problems let us know how",
    "start": "1786039",
    "end": "1792399"
  },
  {
    "text": "we can help you also qflow has been accepted as Google summer of code",
    "start": "1792399",
    "end": "1797480"
  },
  {
    "text": "project so so we're really exciting about all of the students and their participation uh and looking forward to",
    "start": "1797480",
    "end": "1803440"
  },
  {
    "text": "see more proposals in 2024 um also I want to say for if you're",
    "start": "1803440",
    "end": "1809080"
  },
  {
    "text": "interesting in training tuning capabilities please join our Wednesday regular calls and 2 p.m. and 5:00 p.m.",
    "start": "1809080",
    "end": "1815720"
  },
  {
    "text": "uh UTC time zone uh we meet regularly we have a slack channels on a q slack right",
    "start": "1815720",
    "end": "1821279"
  },
  {
    "text": "now and also like you can check this developer guys for training operator for KB and if you want to contribute start",
    "start": "1821279",
    "end": "1828200"
  },
  {
    "text": "with Goods first issues or health pent issues or submitted new proposal so we can review it uh so we we open any",
    "start": "1828200",
    "end": "1835600"
  },
  {
    "text": "discussions any contributions any new features that you want to propose for these capabilities within qlow",
    "start": "1835600",
    "end": "1842960"
  },
  {
    "text": "project so you've got a chance to hear about our training operators uh CB and",
    "start": "1842960",
    "end": "1849240"
  },
  {
    "text": "others and we want to know um as Andre was saying how you could get involved",
    "start": "1849240",
    "end": "1854440"
  },
  {
    "text": "and do those things but we also want to hear from you all I think we've got like five minutes 4 minutes so we would",
    "start": "1854440",
    "end": "1861960"
  },
  {
    "text": "really like to hear um what you would like to see in the inside of these",
    "start": "1861960",
    "end": "1867320"
  },
  {
    "text": "working groups or hear questions that you may have um for us concerning those I think we have a question back",
    "start": "1867320",
    "end": "1873799"
  },
  {
    "text": "here bringing it to you one",
    "start": "1873799",
    "end": "1881440"
  },
  {
    "text": "two hello so can I use open MPI to um trigger generalist workloads",
    "start": "1882080",
    "end": "1890639"
  },
  {
    "text": "imagine I have a Mont Carlo system already a transfer",
    "start": "1890639",
    "end": "1895880"
  },
  {
    "text": "simulation can I use it for that because I see this very applied to machine",
    "start": "1895880",
    "end": "1901200"
  },
  {
    "text": "learning um but our needs are a bit different in the",
    "start": "1901200",
    "end": "1906760"
  },
  {
    "text": "project okay uh thank you for a good question uh so yes so MP operator uh uh",
    "start": "1906760",
    "end": "1916559"
  },
  {
    "text": "is not only for uh machine learning so uh we can use uh",
    "start": "1916559",
    "end": "1921880"
  },
  {
    "text": "MP operator for traditional HPC works so does it make sense yeah like traditional",
    "start": "1921880",
    "end": "1929360"
  },
  {
    "text": "HPC can I apply it here uh yes so uh MP operator uh just uh",
    "start": "1929360",
    "end": "1939279"
  },
  {
    "text": "rch uh so uh s SSH server and setting up",
    "start": "1939279",
    "end": "1946760"
  },
  {
    "text": "headress service so uh uh you can you can start up uh any",
    "start": "1946760",
    "end": "1953360"
  },
  {
    "text": "programs and no no restrictions yeah just add on this it's",
    "start": "1953360",
    "end": "1959440"
  },
  {
    "text": "a good question so we we have users actually using NPI operat for HPC of",
    "start": "1959440",
    "end": "1964639"
  },
  {
    "text": "task not like machine learning and they already using this in production so if you want to reach out please feel free",
    "start": "1964639",
    "end": "1969799"
  },
  {
    "text": "to reach out to us to ask about this",
    "start": "1969799",
    "end": "1972960"
  },
  {
    "text": "more thank you here try this one tra okay",
    "start": "1975080",
    "end": "1980639"
  },
  {
    "text": "thanks hi yeah thank you for talk so uh I see py Community come out pyou X",
    "start": "1980639",
    "end": "1989240"
  },
  {
    "text": "Project it has this community schedule so how do you compare your training",
    "start": "1989240",
    "end": "1994760"
  },
  {
    "text": "operator with Torx thank yeah it's a good question so um I know about Torx uh",
    "start": "1994760",
    "end": "2001360"
  },
  {
    "text": "so torch exist for everyone it's capability to use P torch on any environment this Ray clusters kubernetes",
    "start": "2001360",
    "end": "2006480"
  },
  {
    "text": "or any other kind of SC so the thing is like P torex previously I think it was came from elastic so it",
    "start": "2006480",
    "end": "2013600"
  },
  {
    "text": "was came from P Tor elastic community and they were specifically um focusing",
    "start": "2013600",
    "end": "2019159"
  },
  {
    "text": "on pych so training operator and we want to collaborate with pytorch Community to think how we can like consolidate effort",
    "start": "2019159",
    "end": "2025919"
  },
  {
    "text": "because stor checks using the similar techniques as we do but more kind of like you know in ponic way so uh if we",
    "start": "2025919",
    "end": "2032600"
  },
  {
    "text": "have anyone here in from the torch community and want to collaborate with us please reach out uh and we build",
    "start": "2032600",
    "end": "2038159"
  },
  {
    "text": "better CLI for qlow users pych users and moving forward but our main idea not to lock users to specific framework because",
    "start": "2038159",
    "end": "2044919"
  },
  {
    "text": "it's like for example if users want to use API they free to do it if users wants use tensorflow or deep speed or",
    "start": "2044919",
    "end": "2050520"
  },
  {
    "text": "anything else we want to give them infrastructure to do this right so we don't don't want to be focused on specific um framework to run de planing",
    "start": "2050520",
    "end": "2058919"
  },
  {
    "text": "hi uh does training operator have support capabilities of dealing with uh",
    "start": "2058919",
    "end": "2066760"
  },
  {
    "text": "training that were for some reason distribute um disrupted mhm yeah so it's",
    "start": "2066760",
    "end": "2073079"
  },
  {
    "text": "a good question so your question is about what if one of the node goes down and are you going to lose the training",
    "start": "2073079",
    "end": "2078320"
  },
  {
    "text": "yes so because c training need to be state stateful so all of the Frameworks",
    "start": "2078320",
    "end": "2083480"
  },
  {
    "text": "offering the checkpoint capabilities when basically you can checkpoint your training on every EPO and you can like",
    "start": "2083480",
    "end": "2088919"
  },
  {
    "text": "export your model all the way to whether it's any blob storage right so the uh it's it's a good thing to do to avoid",
    "start": "2088919",
    "end": "2095440"
  },
  {
    "text": "over feting and avoid like a single point of failer right because like if one not goes down it's actually you will",
    "start": "2095440",
    "end": "2100800"
  },
  {
    "text": "lose your training process right so also pytorch offers some full tolerance",
    "start": "2100800",
    "end": "2106040"
  },
  {
    "text": "capabilities so we try to investigate them in specifically in pytorch elastic so how pytorch can gracefully um can",
    "start": "2106040",
    "end": "2113480"
  },
  {
    "text": "basically recover your training if one note goes down but we trying to explore",
    "start": "2113480",
    "end": "2118640"
  },
  {
    "text": "different capabilities there and we're out of time yeah so so if you have any questions feel free to",
    "start": "2118640",
    "end": "2124359"
  },
  {
    "text": "reach out to me uh also to Yuki we here to answer all of your questions for training tuning and other questions",
    "start": "2124359",
    "end": "2129400"
  },
  {
    "text": "about Q flow yeah thanks [Applause]",
    "start": "2129400",
    "end": "2136240"
  },
  {
    "text": "everyone",
    "start": "2136240",
    "end": "2139240"
  }
]