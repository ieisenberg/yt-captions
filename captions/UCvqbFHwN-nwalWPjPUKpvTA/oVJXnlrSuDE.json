[
  {
    "text": "hi everyone uh excited to be here uh my name is yanang I'm a principal software engineer at rat working on our hybrid",
    "start": "440",
    "end": "7640"
  },
  {
    "text": "Cloud AI platform and and I'm Adam telman I am from Nvidia I'm a principal",
    "start": "7640",
    "end": "13280"
  },
  {
    "text": "product architect and I'm working across or djx djx cloud and trying to do a lot",
    "start": "13280",
    "end": "19480"
  },
  {
    "text": "of things Upstream with uh Community software um and so today we're going to talk",
    "start": "19480",
    "end": "27599"
  },
  {
    "text": "about llms and llms in production so we both do a lot with generative AI",
    "start": "27599",
    "end": "33160"
  },
  {
    "text": "applications and so I think when we started putting this together we did not know how many talks",
    "start": "33160",
    "end": "39600"
  },
  {
    "text": "there were going to be around llms and generative Ai and rag At cubec Con but it really seems like it's rag con this",
    "start": "39600",
    "end": "46600"
  },
  {
    "text": "year every every Talk's been on it so uh what we wanted to do with this talk was",
    "start": "46600",
    "end": "51719"
  },
  {
    "text": "first sort of sound set a foundation so I think a lot of talks have gone to a lot of booths they sort of jump into",
    "start": "51719",
    "end": "56920"
  },
  {
    "text": "things assuming everyone is on the same page so we're not going to do that I'm going to try to just throw a bunch of",
    "start": "56920",
    "end": "62559"
  },
  {
    "text": "reference architectures up there talk about some definitions talk about the problem uh and then talk about more",
    "start": "62559",
    "end": "70680"
  },
  {
    "text": "advanced problems that come with production so show show of hands who's who's used a chatbot an llm who's used",
    "start": "70680",
    "end": "76080"
  },
  {
    "text": "the llm okay now put your hand down if you have uh not used one that you were",
    "start": "76080",
    "end": "83040"
  },
  {
    "text": "running with a Docker container and kubernetes you've all done one okay so now put your hand down or keep your hand",
    "start": "83040",
    "end": "89439"
  },
  {
    "text": "up if you have deployed an llm in kubernetes in",
    "start": "89439",
    "end": "94799"
  },
  {
    "text": "production okay I still see hands up that's good that's good okay so so so there's a there's this progression here",
    "start": "94799",
    "end": "100360"
  },
  {
    "text": "right so we're going to talk about that progression and then we're going to use kerve as a case study to talk about how we've we've sort of solve this",
    "start": "100360",
    "end": "106240"
  },
  {
    "text": "progression and then we're going to spend the last few minutes of the talk talking about open problems uh that are",
    "start": "106240",
    "end": "111439"
  },
  {
    "text": "unsolved and hopefully if you guys are interested you can help us solve them in the future because we're doing most of",
    "start": "111439",
    "end": "116600"
  },
  {
    "text": "this in the open source so what what what does it take to actually do that jump from the the",
    "start": "116600",
    "end": "122560"
  },
  {
    "text": "container Docker run or or ol or whatever to a kubernetes deployment that's life",
    "start": "122560",
    "end": "128280"
  },
  {
    "text": "cycled well well first there's a lot of components so you you don't just have your llm uh you have an llm model you have",
    "start": "128280",
    "end": "135480"
  },
  {
    "text": "the inference server around that model you have uh your whole rag system and that's going to have an a betting model",
    "start": "135480",
    "end": "141680"
  },
  {
    "text": "a guard r model a bunch of databases a bunch of data connectors uh and then you're going to",
    "start": "141680",
    "end": "147319"
  },
  {
    "text": "have a bunch of stuff on the other side of that that's actually making those models you're probably using a foundational model but the other parts",
    "start": "147319",
    "end": "153680"
  },
  {
    "text": "are probably going to be coming from somewhere actually I saw a you if you're taking the picture yeah okay um so uh so",
    "start": "153680",
    "end": "162680"
  },
  {
    "text": "here here are some definitions uh so so really quick uh I talked about an inference server so really what that is",
    "start": "162680",
    "end": "168120"
  },
  {
    "text": "is you have your model right it's a file maybe it's been optimized and it's an engine or it's an onyx funnel or",
    "start": "168120",
    "end": "173760"
  },
  {
    "text": "something you got off of hugging face but you need to wrap that in an API and and that's your inference server it's a",
    "start": "173760",
    "end": "179519"
  },
  {
    "text": "container uh you then have a platform you need something orchestrating that platform and it might just be on kubernetes but",
    "start": "179519",
    "end": "186120"
  },
  {
    "text": "you might have something bigger like quer or some mlops tooling that's scaling that up and down dealing with uh",
    "start": "186120",
    "end": "191959"
  },
  {
    "text": "tracing dealing with Telemetry that's an inference platform uh maybe newer to some people you probably heard edding",
    "start": "191959",
    "end": "198519"
  },
  {
    "text": "model reranking model terms like that uh basically everything that is fed into",
    "start": "198519",
    "end": "204000"
  },
  {
    "text": "your llm as additional context coming out a rag system it's put into an embedding space to make it easier to",
    "start": "204000",
    "end": "210439"
  },
  {
    "text": "search and there's a special model that takes things like images text PDFs",
    "start": "210439",
    "end": "215640"
  },
  {
    "text": "embeds it into a vector and then stores that in a special Vector database so that's that's sort of what those",
    "start": "215640",
    "end": "220760"
  },
  {
    "text": "components are and then if you hear reranking that's just another model that sort of optimizes that search of your",
    "start": "220760",
    "end": "226640"
  },
  {
    "text": "vector database so that you get higher quality results faster um on the the agent side you",
    "start": "226640",
    "end": "232560"
  },
  {
    "text": "might hear llm agent chain server things like this at at the top level if you just have a container you don't need",
    "start": "232560",
    "end": "238480"
  },
  {
    "text": "this but if you're having conversation if you're trying to get extra context if there's history you need a highle agent",
    "start": "238480",
    "end": "244760"
  },
  {
    "text": "that is doing all of that doing the reasoning doing the planning deciding what different types of tools to use so",
    "start": "244760",
    "end": "251000"
  },
  {
    "text": "that's what the the agent is and obviously your llm uh P or Laura so you hear Lura",
    "start": "251000",
    "end": "257440"
  },
  {
    "text": "that's a type of parameter efficient fine tuning the basic idea there is you have your llm model and then you which",
    "start": "257440",
    "end": "263160"
  },
  {
    "text": "is maybe a terabyte and then you have a small layer on top of that that is a few",
    "start": "263160",
    "end": "268320"
  },
  {
    "text": "megabytes and that adds special knowledge to your llm so you don't have to spend millions of dollars training a",
    "start": "268320",
    "end": "274039"
  },
  {
    "text": "new foundational model to have something that that does what you need um and then tools are just like a function call that",
    "start": "274039",
    "end": "280800"
  },
  {
    "text": "the the agent can can make uh and guard rails is another AI model that is doing",
    "start": "280800",
    "end": "286080"
  },
  {
    "text": "some safety thing so every input or output into an llm system goes through guardrail so that you can keep it secure",
    "start": "286080",
    "end": "292680"
  },
  {
    "text": "uh relevant on topic non-toxic things like that okay so so we're all now at the",
    "start": "292680",
    "end": "298560"
  },
  {
    "text": "same page on what the fund Al things are uh let's let's sort of walk you through a flow of what the the building blocks",
    "start": "298560",
    "end": "305000"
  },
  {
    "text": "are so this is where we all started right actually maybe not so where we all start when everyone raised their hand",
    "start": "305000",
    "end": "310440"
  },
  {
    "text": "that's something like an AI API so everyone's used chat GPT that's something running somewhere else someone",
    "start": "310440",
    "end": "316720"
  },
  {
    "text": "else has managed the next step is you do Docker run or you do o llama or something and that brings up a process",
    "start": "316720",
    "end": "322120"
  },
  {
    "text": "or a container uh with an llm locally on your system this is this is really a toy",
    "start": "322120",
    "end": "328400"
  },
  {
    "text": "so you can play around with it it's not even quite an MVP it's just something for you to get a feel of how does this llm",
    "start": "328400",
    "end": "335840"
  },
  {
    "text": "work uh then you add those tools the agentic workflows conversational history things like that and instead of just a",
    "start": "335840",
    "end": "342319"
  },
  {
    "text": "container you're now orchestrating it on the platform that can scale it up and down so this is now something that can",
    "start": "342319",
    "end": "347840"
  },
  {
    "text": "actually solve a real world problem it's not a toy anymore but it's still not production ready uh so the next thing you might do",
    "start": "347840",
    "end": "354759"
  },
  {
    "text": "is you you need it to be more explainable uh you can't just have an llm saying anything and you need to add",
    "start": "354759",
    "end": "360280"
  },
  {
    "text": "context to it so that it can access your your documentation your website you have",
    "start": "360280",
    "end": "365960"
  },
  {
    "text": "new things coming out and you don't want to retrain a model every time you throw it in the vector database your llm can",
    "start": "365960",
    "end": "371479"
  },
  {
    "text": "pull that data and now not only is your your solution something that can solve a real problem but it's explainable and",
    "start": "371479",
    "end": "378199"
  },
  {
    "text": "it's more accurate with the data that you care about that the things you actually want to do with your support",
    "start": "378199",
    "end": "383240"
  },
  {
    "text": "you want you want it up to date if a new product comes out right away the support chatbot knows what to do but it's not",
    "start": "383240",
    "end": "388960"
  },
  {
    "text": "quite interpr is ready yet uh because we need we need to make it better rag only",
    "start": "388960",
    "end": "394479"
  },
  {
    "text": "gets us so far uh we also need to do some amount of fine tuning to to make it more accurate and this is where you have",
    "start": "394479",
    "end": "400000"
  },
  {
    "text": "to collect data from inference you need to collect uh data internally and and create those low adapters I was talking",
    "start": "400000",
    "end": "407039"
  },
  {
    "text": "about and so so on the other Corner um of this graph you see a whole training",
    "start": "407039",
    "end": "413440"
  },
  {
    "text": "optimization uh pipeline this is going to be a different system or maybe the same system depending I'll talk about",
    "start": "413440",
    "end": "419759"
  },
  {
    "text": "that later uh but the output is going to be adapters and the input is going to be all of the",
    "start": "419759",
    "end": "425360"
  },
  {
    "text": "data uh lastly to actually make it production ready you need to do all of that but you need to do a few other",
    "start": "425360",
    "end": "431360"
  },
  {
    "text": "things you need to optimize your models so that they run faster so when you're actually at production scale you're getting the best usage of all of your",
    "start": "431360",
    "end": "437759"
  },
  {
    "text": "resources uh you need to have multiple models so you can do AB testing so that you can uh use a bigger model for one",
    "start": "437759",
    "end": "444240"
  },
  {
    "text": "thing a smaller model for another thing and you're automatically scaling that and then you need to add those guard rails because you really can't go to",
    "start": "444240",
    "end": "450199"
  },
  {
    "text": "production unless you have some some safety mechanism looking after all of the AI models that have sometimes",
    "start": "450199",
    "end": "456280"
  },
  {
    "text": "unpredictable outputs um so that was the so that was a",
    "start": "456280",
    "end": "461759"
  },
  {
    "text": "system so so now we all understand the foundations we understand what a production ready system might look like",
    "start": "461759",
    "end": "468240"
  },
  {
    "text": "uh but how do we actually evaluate how good it is so there's training evaluation so how do I actually evaluate",
    "start": "468240",
    "end": "474800"
  },
  {
    "text": "the model itself and then the performance of my system the inference evaluation how do I evaluate uh if if",
    "start": "474800",
    "end": "482800"
  },
  {
    "text": "it's good enough or if I need more operational things so the the training the training layer these are things that",
    "start": "482800",
    "end": "488520"
  },
  {
    "text": "we're may be more familiar with but when it comes to llms there's some differences so you're looking at the",
    "start": "488520",
    "end": "494319"
  },
  {
    "text": "accuracy of the model and you're going to do things like run industry benchmarks against it uh run Uh custom",
    "start": "494319",
    "end": "500960"
  },
  {
    "text": "benchmarks that you've made yourself with your own data and the things you care about you're going to do regression tests for inputs that you've gotten some",
    "start": "500960",
    "end": "508440"
  },
  {
    "text": "of this is going to be specific to what you do what your industry does and some of it's just going to be General and",
    "start": "508440",
    "end": "515080"
  },
  {
    "text": "then some of this is going to have a yes no answer which is the the the industry benchmarks but some of it's going to be",
    "start": "515080",
    "end": "520240"
  },
  {
    "text": "a lot more abstract and and qualitative and so for the qualitative accuracy assessments that gets broken down to a",
    "start": "520240",
    "end": "526920"
  },
  {
    "text": "human in the loop where you actually have someone looking at the output of a standard set of questions and judging it",
    "start": "526920",
    "end": "532120"
  },
  {
    "text": "or you get a bigger model like a like a 405b the 16 GPU model and you have that",
    "start": "532120",
    "end": "538000"
  },
  {
    "text": "judge a smaller model because normally it does a good job of that and that gets you both a qualitative and quantitative",
    "start": "538000",
    "end": "543600"
  },
  {
    "text": "evaluation of the model and then once you know how good the model is uh you want to know how how fast it is so sure",
    "start": "543600",
    "end": "549760"
  },
  {
    "text": "you're going to do things like uh performance benchmarking for latency and throughput and and a bunch of different",
    "start": "549760",
    "end": "555440"
  },
  {
    "text": "context length things like that to really assess the model um lastly so so now we've learned",
    "start": "555440",
    "end": "562880"
  },
  {
    "text": "all all the basics uh what do we actually do once it's in production and this is this is where we get into uh",
    "start": "562880",
    "end": "569440"
  },
  {
    "text": "some of the unsolved problems and some of the harder problems but day two is always hard right so so the day day zero",
    "start": "569440",
    "end": "575640"
  },
  {
    "text": "day one R&D like that's fun and exciting but but the really real stuff we have to do is once once we're in production how",
    "start": "575640",
    "end": "581760"
  },
  {
    "text": "do we monitor it what what metrics do we look at what standards for metrics do we have how do we scale it up and down how",
    "start": "581760",
    "end": "588959"
  },
  {
    "text": "quickly do we scale it up and down with the guardrail systems we have in place",
    "start": "588959",
    "end": "594079"
  },
  {
    "text": "what's automated uh what is toxic or inappropriate how do we how do we dial that up or dial that down uh how do we",
    "start": "594079",
    "end": "601640"
  },
  {
    "text": "handle multicloud multicluster uh different Hardware is available in",
    "start": "601640",
    "end": "606839"
  },
  {
    "text": "different locations and that different Hardware is going to have uh different benefits and can run different models so",
    "start": "606839",
    "end": "612279"
  },
  {
    "text": "we'll have to distribute things in a way and we need some higher level system to help manage that and then when we're",
    "start": "612279",
    "end": "618040"
  },
  {
    "text": "doing testing when we're doing uh upgrades how how do we do those AB upgrades especially in a sense where uh",
    "start": "618040",
    "end": "624839"
  },
  {
    "text": "you know with a small application it's easy to just spin up a new VM and then do ab and then spin down the old VM but",
    "start": "624839",
    "end": "631600"
  },
  {
    "text": "that's very difficult when you only have uh 16 gpus and eight GPU node you need",
    "start": "631600",
    "end": "637720"
  },
  {
    "text": "to spin down half of your cluster just to spin up a new one because you don't have enough resources allocated so there",
    "start": "637720",
    "end": "643160"
  },
  {
    "text": "there's a lot of uh problems in the space that come with llms that weren't with traditional software and so now uh",
    "start": "643160",
    "end": "650399"
  },
  {
    "text": "I'll hand it off uh to talk about kerve a little bit and how we can solve some of those problems with open source so",
    "start": "650399",
    "end": "656760"
  },
  {
    "text": "let's talk about ker so I walk through some of the key key features that you may find helpful when building a serving",
    "start": "656760",
    "end": "663839"
  },
  {
    "text": "platform especially for large models so what is kerve K serve is a highly",
    "start": "663839",
    "end": "669079"
  },
  {
    "text": "scalable standard and Cloud agnostic model serving in uh platform on",
    "start": "669079",
    "end": "674880"
  },
  {
    "text": "kubernetes why are we using ker so ker provides many um performance uh",
    "start": "674880",
    "end": "681240"
  },
  {
    "text": "standardized inflence protocols that can work across different machine learning Frameworks like py toch tensorflow XG",
    "start": "681240",
    "end": "687959"
  },
  {
    "text": "boost and so on it also provides a feature called Model match that's uh designed to be high scalable and uh for",
    "start": "687959",
    "end": "695800"
  },
  {
    "text": "dance packing and intelligent routing um it also provides Advanced deployment",
    "start": "695800",
    "end": "701200"
  },
  {
    "text": "features like Canary R out and also provides features like amples and",
    "start": "701200",
    "end": "706680"
  },
  {
    "text": "Transformers if for any like pre-processing post processing tasks uh",
    "start": "706680",
    "end": "712360"
  },
  {
    "text": "if you are using the cative or serverless deployment mode uh it also",
    "start": "712360",
    "end": "718240"
  },
  {
    "text": "provides Auto scanning features um to help you AO scale including scale to",
    "start": "718240",
    "end": "723600"
  },
  {
    "text": "zero on gpus and it's very very highly um simple and pl plugable so you can uh plug in",
    "start": "723600",
    "end": "731839"
  },
  {
    "text": "any run times that you uh that's either like out of box supported by kerve or",
    "start": "731839",
    "end": "737199"
  },
  {
    "text": "write your own python code to Define your own custom uh serving run time and you can also Define your post and",
    "start": "737199",
    "end": "744160"
  },
  {
    "text": "pre-processing uh steps as well so K is uh a very vibrant Comm",
    "start": "744160",
    "end": "752040"
  },
  {
    "text": "Community uh with over 15 maintainers and 250",
    "start": "752040",
    "end": "758360"
  },
  {
    "text": "contributors and it's widely adopted by over 30 companies uh either uh there",
    "start": "758360",
    "end": "764680"
  },
  {
    "text": "either and users or like vendors that are Distributing caser as part of their",
    "start": "764680",
    "end": "769959"
  },
  {
    "text": "product offerings case of supports single model",
    "start": "769959",
    "end": "775480"
  },
  {
    "text": "inference so models can be pulled from model storage system like S3 GCS or any",
    "start": "775480",
    "end": "781839"
  },
  {
    "text": "S3 compatible storage systems or even from PVCs uh that comes from like",
    "start": "781839",
    "end": "788040"
  },
  {
    "text": "kubernetes native Solutions and uh case of controller uh reconciles all the",
    "start": "788040",
    "end": "793639"
  },
  {
    "text": "inference deployment and based on the choice of your deployment mode uh for example if you are using K native or",
    "start": "793639",
    "end": "801079"
  },
  {
    "text": "serous deployment mode uh it will use the K native U uh Autos scaling instead",
    "start": "801079",
    "end": "807360"
  },
  {
    "text": "of the kubernetes horizontal Part auto scaling and uh if you want to Define any",
    "start": "807360",
    "end": "813720"
  },
  {
    "text": "pre-processing or postprocessing steps the Transformer service is help here to",
    "start": "813720",
    "end": "818800"
  },
  {
    "text": "help as well so once the model is being pulled uh to uh the model deployment uh",
    "start": "818800",
    "end": "825680"
  },
  {
    "text": "the actual model servers are going to be started running and you are uh we're",
    "start": "825680",
    "end": "831199"
  },
  {
    "text": "actually using you can use multiple different run times such as VM or Tron",
    "start": "831199",
    "end": "837240"
  },
  {
    "text": "uh to actually serve the model requests kerve also supports multi model",
    "start": "837240",
    "end": "843920"
  },
  {
    "text": "uh there's a sub project within kerve called a model model match it's designed",
    "start": "843920",
    "end": "849320"
  },
  {
    "text": "for high scale high density and frequeny changing model use cases so there's a",
    "start": "849320",
    "end": "854399"
  },
  {
    "text": "controller part that's responsible to Route the model uh serving requests to different um uh model runtime Parts make",
    "start": "854399",
    "end": "862279"
  },
  {
    "text": "sure that we have a balance between the the responsiveness to users and the",
    "start": "862279",
    "end": "867880"
  },
  {
    "text": "computation uh footprint making sure like the decision uh on routing is",
    "start": "867880",
    "end": "873279"
  },
  {
    "text": "intelligent enough one of these core features is the",
    "start": "873279",
    "end": "878600"
  },
  {
    "text": "serving run times so K of supports plugable reusable and extensible run",
    "start": "878600",
    "end": "884079"
  },
  {
    "text": "times uh it can be framework specific to support Frameworks like pytorch tensor",
    "start": "884079",
    "end": "889399"
  },
  {
    "text": "flow and also support multimodel uh multi framework support um that are more",
    "start": "889399",
    "end": "894839"
  },
  {
    "text": "generic for example hugging phase uh runtime which comes with with the vrm",
    "start": "894839",
    "end": "899880"
  },
  {
    "text": "and Tron back ends and I'm server Onyx and so on and if any of this doesn't fit",
    "start": "899880",
    "end": "906199"
  },
  {
    "text": "for your use cases you can also Define your custom runtime as",
    "start": "906199",
    "end": "911360"
  },
  {
    "text": "well so here is an example of using the hugging face one time um by default it",
    "start": "911360",
    "end": "917320"
  },
  {
    "text": "uses the vrm backend but you can also uh choose to use the tri Tron backend as",
    "start": "917320",
    "end": "923519"
  },
  {
    "text": "well so all you need to do is to specify the model format as hogging pH and and",
    "start": "923519",
    "end": "929519"
  },
  {
    "text": "and then specify the computational resources and the path to your model and you can study uh quickly study serving",
    "start": "929519",
    "end": "936600"
  },
  {
    "text": "the models using um vrm runtime another core feature is the",
    "start": "936600",
    "end": "943240"
  },
  {
    "text": "model storage feature so case of supports multiple uh storage formats for",
    "start": "943240",
    "end": "948480"
  },
  {
    "text": "example those S3 compatible formats and URL and uh kubernetes PVC and if any of",
    "start": "948480",
    "end": "956240"
  },
  {
    "text": "this doesn't meet your use case you can also Define your own storage container for example you can Define your own",
    "start": "956240",
    "end": "962240"
  },
  {
    "text": "protocols to your model registry uh so one example is to use the CP flow model",
    "start": "962240",
    "end": "968079"
  },
  {
    "text": "registry so that models can be stored in model registry to manage and uh version",
    "start": "968079",
    "end": "975519"
  },
  {
    "text": "all the models and their Associated metadata and all all you need to do in case inference service is to specify the",
    "start": "975519",
    "end": "983639"
  },
  {
    "text": "path uh to the the actual model in that model registry using this new protocol",
    "start": "983639",
    "end": "990839"
  },
  {
    "text": "and then there's another feature called model car which provides um uh the streamlines the process to pull models",
    "start": "990839",
    "end": "998480"
  },
  {
    "text": "directly from oi image registry uh let's talk about more about",
    "start": "998480",
    "end": "1003839"
  },
  {
    "text": "the model cars feature uh it it provides a more efficient way to pull models from",
    "start": "1003839",
    "end": "1009560"
  },
  {
    "text": "the OC image hisory and and uh you only need to prove the models once for each",
    "start": "1009560",
    "end": "1015519"
  },
  {
    "text": "node so that will that can help reduce the start of time uh to uh for your",
    "start": "1015519",
    "end": "1021319"
  },
  {
    "text": "model server Parts especially this is uh especially useful uh when uh it comes to",
    "start": "1021319",
    "end": "1027120"
  },
  {
    "text": "Auto scaling and you can leverage the cash uh that's um done on the at the",
    "start": "1027120",
    "end": "1033400"
  },
  {
    "text": "note level and this feature also decreases the need to uh uh it removes",
    "start": "1033400",
    "end": "1039160"
  },
  {
    "text": "the duplicate local storage to conserve this uh the dis space and modelas also",
    "start": "1039160",
    "end": "1045400"
  },
  {
    "text": "allows Advanced Techniques like prefacing images and loading which also",
    "start": "1045400",
    "end": "1050520"
  },
  {
    "text": "improves the efficiency as well more about the model cars you can",
    "start": "1050520",
    "end": "1056200"
  },
  {
    "text": "specify the path to the model using the oci uh schema uh to point to the model",
    "start": "1056200",
    "end": "1062559"
  },
  {
    "text": "to the image and you can um this also replaces",
    "start": "1062559",
    "end": "1067720"
  },
  {
    "text": "the unit container with the side cut container so that uh with we we don't have to copy anything uh uh in init",
    "start": "1067720",
    "end": "1075799"
  },
  {
    "text": "container and everything uh can be done within this side car and you can um the",
    "start": "1075799",
    "end": "1081600"
  },
  {
    "text": "trick is we configur the part with the the configuration share process name space equal stue uh to facilitate the",
    "start": "1081600",
    "end": "1089440"
  },
  {
    "text": "access of the models between different side cars so here's an example of how to use",
    "start": "1089440",
    "end": "1096240"
  },
  {
    "text": "it uh we basically just build and pack the models together with the runtime uh",
    "start": "1096240",
    "end": "1102480"
  },
  {
    "text": "so you can push the image to the image and then specify the path to the those",
    "start": "1102480",
    "end": "1112600"
  },
  {
    "text": "Ur another feature that we recently added um this the pr uh the support for",
    "start": "1113200",
    "end": "1119440"
  },
  {
    "text": "marinal influence uh so this feature actually just got it uh got merged last week so feel free to try it out as well",
    "start": "1119440",
    "end": "1127320"
  },
  {
    "text": "uh it's not in any of the releases yet but uh for any of you who are adventurous enough feel free to give you",
    "start": "1127320",
    "end": "1134320"
  },
  {
    "text": "a shout and provide any feedback to the community so that we can continuously improve the feature going forward but",
    "start": "1134320",
    "end": "1141640"
  },
  {
    "text": "now we are supporting the mtin note inference uh as part of the huging phase",
    "start": "1141640",
    "end": "1146840"
  },
  {
    "text": "runtime using the VM back end so you can specify your pipeline parallel size and",
    "start": "1146840",
    "end": "1153919"
  },
  {
    "text": "tensor parallel uh sze under the worker SP according to um how large your model",
    "start": "1153919",
    "end": "1162679"
  },
  {
    "text": "is K also provides the key feature called Autos scaning if you use K uh it",
    "start": "1162679",
    "end": "1169280"
  },
  {
    "text": "will use the ktic auto scalar and um you can configure the scaling Target for",
    "start": "1169280",
    "end": "1176280"
  },
  {
    "text": "example here we configure the target to be one using the concurrency as the",
    "start": "1176280",
    "end": "1181600"
  },
  {
    "text": "scale matric so what this does is uh it well Auto scale the U model server Parts",
    "start": "1181600",
    "end": "1188760"
  },
  {
    "text": "based on the traffic so here we are sending uh the traffic in 30 seconds",
    "start": "1188760",
    "end": "1193840"
  },
  {
    "text": "Spurs and while maintaining five inflat uh requests so as you can see here five",
    "start": "1193840",
    "end": "1200240"
  },
  {
    "text": "model server parts will be running because uh each of them only handles one",
    "start": "1200240",
    "end": "1205760"
  },
  {
    "text": "concurrent request and you can also scale down to",
    "start": "1205760",
    "end": "1210880"
  },
  {
    "text": "zero as well just by specifying the meain replicant to be zero another feature is the crar so in",
    "start": "1210880",
    "end": "1219400"
  },
  {
    "text": "case you have a very different model that's um that's been improved and you",
    "start": "1219400",
    "end": "1224840"
  },
  {
    "text": "want to roll out this new version of the model to production you don't want to do everything uh at once and that will",
    "start": "1224840",
    "end": "1232159"
  },
  {
    "text": "impact uh whatever is currently running so you can gradually roll out your uh",
    "start": "1232159",
    "end": "1237880"
  },
  {
    "text": "new versions of the model uh based on the traffic and then uh roll back if",
    "start": "1237880",
    "end": "1243480"
  },
  {
    "text": "anything bad is happening to the model so let's say if the model quality is not",
    "start": "1243480",
    "end": "1248760"
  },
  {
    "text": "of your does not meet your expectation you can easily roll uh it back to the previous",
    "start": "1248760",
    "end": "1254880"
  },
  {
    "text": "version K also provides a set of production ready features like",
    "start": "1254880",
    "end": "1260159"
  },
  {
    "text": "monitoring so um a lot of different metrics like lency metrics will be",
    "start": "1260159",
    "end": "1265720"
  },
  {
    "text": "emitted for each of the model server uh steps if you are using any of the preos",
    "start": "1265720",
    "end": "1272200"
  },
  {
    "text": "processing or use uh explainer or predict any of these uh the metrics",
    "start": "1272200",
    "end": "1278440"
  },
  {
    "text": "that's happening within those steps will be captured uh in uh prometheous",
    "start": "1278440",
    "end": "1284720"
  },
  {
    "text": "metrix and uh keep in mind that each of the model server May Implement its own",
    "start": "1284720",
    "end": "1290120"
  },
  {
    "text": "set of metrics so if you're asking for a specific uh metric that's uh not",
    "start": "1290120",
    "end": "1295480"
  },
  {
    "text": "supported yet you need to ask the Upstream communities for each of those",
    "start": "1295480",
    "end": "1300919"
  },
  {
    "text": "runtime for example if you need something from vrm you'll have to request that feature in that community",
    "start": "1300919",
    "end": "1308120"
  },
  {
    "text": "and we work very closely with each other as well um if you're using K native uh",
    "start": "1308120",
    "end": "1314559"
  },
  {
    "text": "there's also Q proxy and K native uh metrix that are be emitted by",
    "start": "1314559",
    "end": "1320000"
  },
  {
    "text": "default and once you have those metrics uh you are you can also look at those metrics in your graph final dashboard uh",
    "start": "1320000",
    "end": "1327720"
  },
  {
    "text": "there's K native HTTP dashboard and there's also separate dashboard for each of the runtimes we",
    "start": "1327720",
    "end": "1335760"
  },
  {
    "text": "support and the next feature is the explainer or trust feature so caser",
    "start": "1335919",
    "end": "1341840"
  },
  {
    "text": "provides a way to plug in any explainer explainer runtime uh so in case uh your",
    "start": "1341840",
    "end": "1348559"
  },
  {
    "text": "models are complex and is like a blackbox you may want to explain it to make sure you understand what features",
    "start": "1348559",
    "end": "1354919"
  },
  {
    "text": "are contributing back to the model performance uh um and uh why this model",
    "start": "1354919",
    "end": "1361320"
  },
  {
    "text": "produces this inflence result and so on and if you are using one of uh trusty AI",
    "start": "1361320",
    "end": "1367039"
  },
  {
    "text": "explainer uh trusty AI also supports features like LM evaluation and G real",
    "start": "1367039",
    "end": "1372799"
  },
  {
    "text": "capabilities",
    "start": "1372799",
    "end": "1375799"
  },
  {
    "text": "are we doing for time okay um yeah so uh thanks I think that that was a really",
    "start": "1378840",
    "end": "1384200"
  },
  {
    "text": "great uh overview of kerve and so uh I came up here and I told you about all of these problems and I didn't actually",
    "start": "1384200",
    "end": "1390080"
  },
  {
    "text": "give you any real technical details and then really appreciate that that deep dive and I I hope what you got out of",
    "start": "1390080",
    "end": "1396240"
  },
  {
    "text": "that was that there's a lot there's a lot going on behind the scenes and there's a lot to keep in mind and with the kerve platform I think as a",
    "start": "1396240",
    "end": "1402039"
  },
  {
    "text": "community we've done a really good job addressing all of the needs and we're really active in work group serving and a lot of other places to figure out what",
    "start": "1402039",
    "end": "1409000"
  },
  {
    "text": "people are trying to do and get it into a platform that is both consistent but also flexible so you see all of those",
    "start": "1409000",
    "end": "1415120"
  },
  {
    "text": "run times VM and and trid and all of those other things you you have like what I forgot to mention earlier was",
    "start": "1415120",
    "end": "1421600"
  },
  {
    "text": "it's really important that you keep that flexibility so you can choose new models new runtimes new storage methods as as",
    "start": "1421600",
    "end": "1428279"
  },
  {
    "text": "all of this generative AI stuff is moving very quickly U I'll actually I think we have",
    "start": "1428279",
    "end": "1434080"
  },
  {
    "text": "a few extra minutes so I I implemented for NVIDIA a runtime on on K serve for",
    "start": "1434080",
    "end": "1439679"
  },
  {
    "text": "NVIDIA Nim and it was it was very easy to do and actually after I figured that that turned out to be 10 lines of yaml",
    "start": "1439679",
    "end": "1446720"
  },
  {
    "text": "and the benefit was that is that now all of my partners and my customers that are using kerve are able to deploy my",
    "start": "1446720",
    "end": "1452200"
  },
  {
    "text": "product and it's it's consistently scaling metrics are available in a way that everyone's already familiar with so",
    "start": "1452200",
    "end": "1458080"
  },
  {
    "text": "it was really easy for us to um sort of build guides and build products around",
    "start": "1458080",
    "end": "1463640"
  },
  {
    "text": "that instead of having to focus on reimplementing these platform features that they're very very new but also very",
    "start": "1463640",
    "end": "1470200"
  },
  {
    "text": "important um and just a plug we are doing a kerve community panel at",
    "start": "1470200",
    "end": "1475279"
  },
  {
    "text": "5:00 um I also mention that okay yeah we'll mention it a bunch of times we want more people to come uh so unsolved",
    "start": "1475279",
    "end": "1481880"
  },
  {
    "text": "problems so so what whether are we not fixing yet or or fully addressed in",
    "start": "1481880",
    "end": "1487600"
  },
  {
    "text": "caser so the first one there's sort of this this new nation of agents and tools and I think uh there was a lot of",
    "start": "1487600",
    "end": "1493200"
  },
  {
    "text": "announcements this morning around AI gateways and those AI gateways will have to talk to agents and tools and there's",
    "start": "1493200",
    "end": "1498880"
  },
  {
    "text": "not really many standards around this yet so I think with with an agent it has this notion of a short-term memory of",
    "start": "1498880",
    "end": "1505760"
  },
  {
    "text": "what it's currently doing and then a long-term memory of conversations that's had with you in the past and your uh",
    "start": "1505760",
    "end": "1511120"
  },
  {
    "text": "your usernames your all of the things you've told it to remember uh we don't really we don't really have A Great",
    "start": "1511120",
    "end": "1516640"
  },
  {
    "text": "Notion of how to access all of this how to pass that securely to to different uh functions how to make sure that the",
    "start": "1516640",
    "end": "1522440"
  },
  {
    "text": "functions we're adding to the tool are secure and consistent how to describe the inputs and outputs and latency",
    "start": "1522440",
    "end": "1527919"
  },
  {
    "text": "requests so this is this is a problem area that I think uh we're going to start getting",
    "start": "1527919",
    "end": "1533159"
  },
  {
    "text": "into uh similarly the sort of end to end workflow so I I showed you a nice",
    "start": "1533159",
    "end": "1538279"
  },
  {
    "text": "diagram at at the front where everything was in the same cluster I don't think I've ever actually seen anyone do that",
    "start": "1538279",
    "end": "1544480"
  },
  {
    "text": "uh we don't have a great Cloud native way to do your sort of in the loop training right next to your your",
    "start": "1544480",
    "end": "1551640"
  },
  {
    "text": "deployment uh we need better tooling around this we need better descriptions",
    "start": "1551640",
    "end": "1557000"
  },
  {
    "text": "on those lower adapt right we're right now plopping them into production but how do we actually describe that how do",
    "start": "1557000",
    "end": "1563279"
  },
  {
    "text": "we uh choose which one how do we more efficiently serve a 100 different customization layers on top of one",
    "start": "1563279",
    "end": "1569240"
  },
  {
    "text": "foundational model uh so so there's starting to be more research papers and more solutions around that but I'm",
    "start": "1569240",
    "end": "1575080"
  },
  {
    "text": "hoping to see more in open source work in this area as well um and that just this slid just so",
    "start": "1575080",
    "end": "1582799"
  },
  {
    "text": "so when we're doing the fine-tuning where we've got this end end training inference Loop uh I think we're all familiar with fine tuning now we're",
    "start": "1582799",
    "end": "1589200"
  },
  {
    "text": "familiar with with PFT or Laura and then sort of there there's prompt engineering and prop learning techniques that you",
    "start": "1589200",
    "end": "1594840"
  },
  {
    "text": "can actually build into your pipeline as well they all have different benefits and different reasons to use",
    "start": "1594840",
    "end": "1600480"
  },
  {
    "text": "them um another thing that that there's more happening with so a lot of folks if you've heard of VM you you'll probably",
    "start": "1600480",
    "end": "1606080"
  },
  {
    "text": "know that VM is doing sort of just in time compilation of models uh you can take your hugging face model right right",
    "start": "1606080",
    "end": "1612559"
  },
  {
    "text": "off of the shelf and run it in something like Trident and that's that's going to be an unoptimized model or you can take",
    "start": "1612559",
    "end": "1618399"
  },
  {
    "text": "a model and you could run it through an Optimizer like tensor RT which is going to create sort of a compiled model",
    "start": "1618399",
    "end": "1624120"
  },
  {
    "text": "engine and that's going to probably give you your best performance uh so there's ways you can do that for optimizing your model but",
    "start": "1624120",
    "end": "1631360"
  },
  {
    "text": "then there's other uh inference optimization techniques there's things like inflight batching which is really",
    "start": "1631360",
    "end": "1636679"
  },
  {
    "text": "complex and is now in in uh triin and tier TM and and VM they've all implemented this like a 10x more",
    "start": "1636679",
    "end": "1643000"
  },
  {
    "text": "throughput out of your large language model uh but then there's other layers of caching response caching caching the",
    "start": "1643000",
    "end": "1648919"
  },
  {
    "text": "contexts all of these other things uh that that you can do and new research papers coming out every few weeks every",
    "start": "1648919",
    "end": "1655919"
  },
  {
    "text": "month on this and the research papers are useless unless we Implement them somewhere so we need to take those",
    "start": "1655919",
    "end": "1661960"
  },
  {
    "text": "research papers around caching up new optimization strategies new inferencing strategies and make sure that they get",
    "start": "1661960",
    "end": "1667720"
  },
  {
    "text": "readily available in whatever platforms we're doing and some of that happens at the inference container level uh but",
    "start": "1667720",
    "end": "1674000"
  },
  {
    "text": "some of it is platform-wide and uh so you've got to really figure out how to coordinate everything going on in the",
    "start": "1674000",
    "end": "1679279"
  },
  {
    "text": "system and we need higher level abstractions to do",
    "start": "1679279",
    "end": "1684480"
  },
  {
    "text": "that uh lastly uh model cache management so I talked a little bit about that and",
    "start": "1684519",
    "end": "1690440"
  },
  {
    "text": "the the other big one is multi noes so so we've got a really good implementation uh in kerve now of multinode and I'd love to know if anyone",
    "start": "1690440",
    "end": "1697360"
  },
  {
    "text": "in the audience does multinode inference uh but we've got we've got new new stuff",
    "start": "1697360",
    "end": "1702679"
  },
  {
    "text": "that needs to come out we've got uh Hardware Hardware optimizations to make it better uh we've got things like gang",
    "start": "1702679",
    "end": "1709440"
  },
  {
    "text": "scheduling and Bin packing to make sure that there's no scheduling deadlock projects like leader workers set could",
    "start": "1709440",
    "end": "1715519"
  },
  {
    "text": "be really useful for this and there's just a lot of work to do in this area to make the scheduling better and to make",
    "start": "1715519",
    "end": "1720919"
  },
  {
    "text": "the hardware consumption better and to make the the networking Communications take better advantage of things like",
    "start": "1720919",
    "end": "1726279"
  },
  {
    "text": "RDMA and Rocky so there's there's a lot of um a lot of work to do in this area",
    "start": "1726279",
    "end": "1731399"
  },
  {
    "text": "as well uh and I think all of the pieces are there right now it's just it's difficult to get together and so that's",
    "start": "1731399",
    "end": "1736440"
  },
  {
    "text": "one of the things that that we're working as Community to make",
    "start": "1736440",
    "end": "1740799"
  },
  {
    "text": "easier um and so that I I'll hand it off uh talk a little bit about the future things we're doing yeah so as Adam yeah",
    "start": "1741679",
    "end": "1748640"
  },
  {
    "text": "as Adam mentioned there's a lot of Unsolved challenges and problems that we",
    "start": "1748640",
    "end": "1753679"
  },
  {
    "text": "have to really come together as a group to adjust them together so we did publish our road map on GitHub so in",
    "start": "1753679",
    "end": "1760679"
  },
  {
    "text": "case of those you those of you who would like to contribute and participate in",
    "start": "1760679",
    "end": "1765840"
  },
  {
    "text": "the early design discussions uh we welcome all of you to contribute so what we are planning here is a lot of like",
    "start": "1765840",
    "end": "1773279"
  },
  {
    "text": "GNA influence work clouds we want to support it better so one area is a serving runtime support we want to",
    "start": "1773279",
    "end": "1780120"
  },
  {
    "text": "support the existing runtimes like vrm to make sure all those Cutting Edge features are also included in caser uh",
    "start": "1780120",
    "end": "1788240"
  },
  {
    "text": "and we're also doing uh supporting uh for additional run times like tensor RT LM TGI and uh and there's also",
    "start": "1788240",
    "end": "1796600"
  },
  {
    "text": "benchmarking uh work work that's been going on uh through the working group serving and we implemented multie",
    "start": "1796600",
    "end": "1804440"
  },
  {
    "text": "multinode inflence but we want to improve it going forward and the auto scaling uh we uh we we are working on",
    "start": "1804440",
    "end": "1812159"
  },
  {
    "text": "support for model caching uh with automatic PVC or PVE provision and also",
    "start": "1812159",
    "end": "1818840"
  },
  {
    "text": "we want to enable Autos scaning based on custom Matrix and uh you've heard a lot",
    "start": "1818840",
    "end": "1825399"
  },
  {
    "text": "about agentic workflows and rag so so we are also actively looking into that as",
    "start": "1825399",
    "end": "1831600"
  },
  {
    "text": "well we are also looking into open inference protocols to make sure we cover all the popular uh ji tasks uh",
    "start": "1831600",
    "end": "1840039"
  },
  {
    "text": "that's uh that can be used uh that are more generic and open so that everyone",
    "start": "1840039",
    "end": "1845240"
  },
  {
    "text": "cont contribute and the community is also working on L gateway to support",
    "start": "1845240",
    "end": "1850799"
  },
  {
    "text": "multiple L providers like Co token based written limiting and uh smarter routing",
    "start": "1850799",
    "end": "1857320"
  },
  {
    "text": "and so on so we did publish our road mapap there's a link below so feel free uh to look at",
    "start": "1857320",
    "end": "1863960"
  },
  {
    "text": "it after the session and we are happy to chat with",
    "start": "1863960",
    "end": "1869320"
  },
  {
    "text": "everyone and we are working with the community uh very closely so for example",
    "start": "1869320",
    "end": "1875000"
  },
  {
    "text": "we are working closely with the working group serving uh within the kubernetes community we have a dedicated session on",
    "start": "1875000",
    "end": "1881919"
  },
  {
    "text": "Friday uh to talk about some of the efforts that's happening in that working group and we also collaborating with uh",
    "start": "1881919",
    "end": "1889559"
  },
  {
    "text": "the community of contributors and maintainers and corporate Partners uh so",
    "start": "1889559",
    "end": "1894840"
  },
  {
    "text": "do check out the panel to uh later today uh we welcome everyone another",
    "start": "1894840",
    "end": "1901039"
  },
  {
    "text": "promotion and uh we are also collaborating um and integ making sure",
    "start": "1901039",
    "end": "1906720"
  },
  {
    "text": "the Integrations work well with ecosystem projects uh for example VM and",
    "start": "1906720",
    "end": "1912399"
  },
  {
    "text": "coup flow and we are also working with anvo on some of the Gateway API",
    "start": "1912399",
    "end": "1919840"
  },
  {
    "text": "efforts so here's a a small list of talks related to working grp serving and",
    "start": "1920399",
    "end": "1926159"
  },
  {
    "text": "caser so feel free to check it out and or catch up with the recording if you missed",
    "start": "1926159",
    "end": "1933360"
  },
  {
    "text": "it and more sessions from Nidia and red [Music]",
    "start": "1933519",
    "end": "1940080"
  },
  {
    "text": "hat yeah that's it and I think we have time for what sorry I've been losing my voice all week I almost made it to the",
    "start": "1940080",
    "end": "1947440"
  },
  {
    "text": "end of the talk almost so close yeah so I think we have time for like one or two questions if if people want to come to",
    "start": "1947440",
    "end": "1953840"
  },
  {
    "text": "the mic um and then we'll be sticking around afterwards all right thanks Adam um",
    "start": "1953840",
    "end": "1960120"
  },
  {
    "text": "great talk so one question so like you mentioned Nvidia name with ker so uh",
    "start": "1960120",
    "end": "1966240"
  },
  {
    "text": "interesting so uh n also has Tron it also micros service ker also has",
    "start": "1966240",
    "end": "1972200"
  },
  {
    "text": "Tron what do you see the benefit real benefit having n and K together okay",
    "start": "1972200",
    "end": "1977880"
  },
  {
    "text": "okay yeah I can talk to that um so the benefit of nim over trien is that Nim is",
    "start": "1977880",
    "end": "1983000"
  },
  {
    "text": "Enterprise ready and trien is more DIY Nim is sort of in a box an AI",
    "start": "1983000",
    "end": "1988360"
  },
  {
    "text": "application it's it's a specific set of models it's highly optimized it's going to run faster uh Tren you can get there",
    "start": "1988360",
    "end": "1995320"
  },
  {
    "text": "with tridon and trt llm and tensor RT it's just going to take you more time and it's not going to work out of the",
    "start": "1995320",
    "end": "2000880"
  },
  {
    "text": "box so you can still use titon in if we put iner we can still put titon as a in",
    "start": "2000880",
    "end": "2007880"
  },
  {
    "text": "Ser Tron will work on kerve uh just as well as n n okay yeah thank yeah there's",
    "start": "2007880",
    "end": "2013679"
  },
  {
    "text": "a separate Nim run time on kerve and actually so we just GA the Nim operator a month or two ago and one of",
    "start": "2013679",
    "end": "2020639"
  },
  {
    "text": "the features on the road map for the Nim operator and I think I saw the PM back there on it uh we're going to try to do",
    "start": "2020639",
    "end": "2026279"
  },
  {
    "text": "kerve integration in the Nim operator so that our Nim operator will deploy directly in the",
    "start": "2026279",
    "end": "2032200"
  },
  {
    "text": "kerve thanks for uh thanks for the talk I saw a lot of familiar friends up on that optimization section",
    "start": "2032200",
    "end": "2038240"
  },
  {
    "text": "I wanted to ask a little bit about benchmarking I've really enjoyed using trt bench um because it's doing a lot to",
    "start": "2038240",
    "end": "2045039"
  },
  {
    "text": "you know mess with the exact sequence links and and data you know that you're",
    "start": "2045039",
    "end": "2050560"
  },
  {
    "text": "passing in is is much more realistic I wanted to know if you had any plans around more benchmarking tools that",
    "start": "2050560",
    "end": "2056919"
  },
  {
    "text": "aren't just like at the model level but are also at the sort of um",
    "start": "2056919",
    "end": "2062200"
  },
  {
    "text": "infrastructure level to that is a great question um yeah uh",
    "start": "2062200",
    "end": "2068079"
  },
  {
    "text": "but nothing I can really talk about uh and it's a huge need so I think I just got that the red flag so that needs to",
    "start": "2068079",
    "end": "2073118"
  },
  {
    "text": "be the last question but we'll stick around I love chat more about you about that but basically yeah right now all of",
    "start": "2073119",
    "end": "2078240"
  },
  {
    "text": "the performance benchmarking is for like a single llm a single model uh there's not really a lot of performance",
    "start": "2078240",
    "end": "2083358"
  },
  {
    "text": "benchmarking tools for an endend rag system and uh there's existing things to do specific Hardware right uh we've had",
    "start": "2083359",
    "end": "2091440"
  },
  {
    "text": "AI around so we've been benchmarking these systems forever but the end to end system and then comparing that to the",
    "start": "2091440",
    "end": "2096520"
  },
  {
    "text": "Baseline performance is something we need to get better at as a community yeah and just um if you've",
    "start": "2096520",
    "end": "2101560"
  },
  {
    "text": "used tridon tridon used to have something I forgot what it used to be called but now it's called geni perf uh",
    "start": "2101560",
    "end": "2107000"
  },
  {
    "text": "and it's a a really nice performance benchmarking tool specifically for open AI compatible uh llms so it just hits",
    "start": "2107000",
    "end": "2113720"
  },
  {
    "text": "that API and and you can run it against uh any uh llm in the cloud or on Prem",
    "start": "2113720",
    "end": "2119359"
  },
  {
    "text": "and you get the same sort of high throughput performance testing for an llm and we've looked at a lot of these",
    "start": "2119359",
    "end": "2126200"
  },
  {
    "text": "Benchmark controls as well So within the group uh kubernetes working group serving we are also like um putting",
    "start": "2126200",
    "end": "2134040"
  },
  {
    "text": "together like a standardized benchmarking tool you can use it as a library there's a new proposal coming",
    "start": "2134040",
    "end": "2139520"
  },
  {
    "text": "out so um I can share the link afterwards yeah thank you yeah I think some of that ties in O OIP too right so",
    "start": "2139520",
    "end": "2146920"
  },
  {
    "text": "what we're talking today was about llms but everything that we're really talking about the benchmarking that that tugs",
    "start": "2146920",
    "end": "2153960"
  },
  {
    "text": "into other other domains as well so the oi OIP project is very cool it's",
    "start": "2153960",
    "end": "2160240"
  },
  {
    "text": "for for inference uh standards and so right now we're talking about llms but we need standards for for image and",
    "start": "2160240",
    "end": "2165920"
  },
  {
    "text": "video and all these other things as well",
    "start": "2165920",
    "end": "2169520"
  }
]