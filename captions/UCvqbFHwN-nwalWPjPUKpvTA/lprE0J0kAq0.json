[
  {
    "start": "0",
    "end": "37000"
  },
  {
    "text": "yeah all right we're gonna go in and get started a little early since they won't let anybody else in anyway so I'm Doug",
    "start": "60",
    "end": "6299"
  },
  {
    "text": "Davis track host but obviously you're here they're here you're here to hear Mikkel talk about operators I was",
    "start": "6299",
    "end": "12389"
  },
  {
    "text": "actually reviewing the proposals for the CFPs for this thing operators was one of the hottest topics out there so obviously as you guys are",
    "start": "12389",
    "end": "18720"
  },
  {
    "text": "testament this is obviously very very exciting topic so this is this would be really good one quick reminder please",
    "start": "18720",
    "end": "24660"
  },
  {
    "text": "rate the top the talk when you're done through the app that way the auger dieters know where there was a popular",
    "start": "24660",
    "end": "30119"
  },
  {
    "text": "topic speaker did well or not that kind of stuff they leave it make comments as you possibly can and with that yeah",
    "start": "30119",
    "end": "35270"
  },
  {
    "text": "thank you so welcome everyone who could get in to my talk it's about ears",
    "start": "35270",
    "end": "42360"
  },
  {
    "text": "operator which is an open source project for running elasticsearch like running big scale elasticsearch and how we run",
    "start": "42360",
    "end": "48239"
  },
  {
    "text": "it and kubernetes at solando adam this I want to go like to a story of how we",
    "start": "48239",
    "end": "54660"
  },
  {
    "text": "built this operator and I think the patterns here also apply to other applications not only lesyk search but",
    "start": "54660",
    "end": "61230"
  },
  {
    "text": "also sample these kind of applications can also benefit from this pattern I think so Who am I I'm Meg Larson I am a",
    "start": "61230",
    "end": "70409"
  },
  {
    "start": "67000",
    "end": "119000"
  },
  {
    "text": "software engineer at solando I work at the cloud infrastructure team which deals with kubernetes AWS infrastructure",
    "start": "70409",
    "end": "77610"
  },
  {
    "text": "so we management clusters and we were",
    "start": "77610",
    "end": "82830"
  },
  {
    "text": "like in a joint effort to develop this elasticsearch operator between the",
    "start": "82830",
    "end": "88470"
  },
  {
    "text": "search department and our team so we came with the cominius knowledge and they came with with the elasticsearch",
    "start": "88470",
    "end": "95549"
  },
  {
    "text": "knowledge so I'm not a expert on a likely search I know some things but if you really want to know this then I will",
    "start": "95549",
    "end": "101880"
  },
  {
    "text": "forward you to some some colleagues who are unfortunately out here at the conference if you want to tweet at me",
    "start": "101880",
    "end": "108450"
  },
  {
    "text": "saying that is stupid to run or most critical stateful applications on kubernetes then you're welcome to do",
    "start": "108450",
    "end": "113640"
  },
  {
    "text": "that but maybe you have a different opinion after that's all good see what",
    "start": "113640",
    "end": "120420"
  },
  {
    "start": "119000",
    "end": "139000"
  },
  {
    "text": "is the Lando we are Europe's leading online fashion platform we sell socks",
    "start": "120420",
    "end": "125939"
  },
  {
    "text": "and shoes and all kinds of fashion articles all over Europe online and you",
    "start": "125939",
    "end": "133240"
  },
  {
    "text": "go there you order your things get them and then you can send them back if you don't like it all three and we are in",
    "start": "133240",
    "end": "140230"
  },
  {
    "start": "139000",
    "end": "165000"
  },
  {
    "text": "seventeen markets we have a revenue of 5.4 billion so quite a big company 26",
    "start": "140230",
    "end": "147370"
  },
  {
    "text": "million extra customers we had 15,000 employees in Europe and roughly a little",
    "start": "147370",
    "end": "154240"
  },
  {
    "text": "under 10 percent of these working with intake so software engineers and so on",
    "start": "154240",
    "end": "160300"
  },
  {
    "text": "and these are kind of the customers for our team internally this means that we",
    "start": "160300",
    "end": "167290"
  },
  {
    "start": "165000",
    "end": "232000"
  },
  {
    "text": "have a kubernetes set up at solando that looks like this we declare it the",
    "start": "167290",
    "end": "172840"
  },
  {
    "text": "default deployment target this year meaning that everything that is the develop new software has to go to",
    "start": "172840",
    "end": "179140"
  },
  {
    "text": "kubernetes ideally this means we have 125 kubernetes classes right now that we",
    "start": "179140",
    "end": "185710"
  },
  {
    "text": "manage in our team roughly 50% fat 50% test clusters this amounts to around",
    "start": "185710",
    "end": "193120"
  },
  {
    "text": "1400 easy to instances we run an AWS this will cost scales in and out during",
    "start": "193120",
    "end": "199150"
  },
  {
    "text": "the day so it's just a rough number because we have auto scaling with the cluster autoscaler we started this",
    "start": "199150",
    "end": "206290"
  },
  {
    "text": "interfere with kubernetes in October 2016 and since then we have updated our",
    "start": "206290",
    "end": "212680"
  },
  {
    "text": "clusters from 1.4 to their oldest clusters to now 1.13 last week we rolled",
    "start": "212680",
    "end": "218620"
  },
  {
    "text": "out and really like continuously upgrade the cluster so we don't create new classes move workload we just upgrade",
    "start": "218620",
    "end": "224560"
  },
  {
    "text": "them in place and this is of course an elastic search of a radar would also have to to take care of this so what the",
    "start": "224560",
    "end": "232510"
  },
  {
    "start": "232000",
    "end": "278000"
  },
  {
    "text": "search looked like at solando we have 300,000 products per country 17",
    "start": "232510",
    "end": "238690"
  },
  {
    "text": "countries we have 2000 brands roughly 700 categories and all this data is in",
    "start": "238690",
    "end": "244660"
  },
  {
    "text": "the elastic search so you can find it when you go to the website and search for whatever shoes or t-shirts or",
    "start": "244660",
    "end": "250270"
  },
  {
    "text": "whatever you like and this is all powered by elastic search it has 12,000",
    "start": "250270",
    "end": "255790"
  },
  {
    "text": "appears on our QPS on like the normal peak during the week and we can also",
    "start": "255790",
    "end": "264310"
  },
  {
    "text": "handle like a que eight thousand requests per second the updates per second from from like",
    "start": "264310",
    "end": "270820"
  },
  {
    "text": "ingesting the data from the different streams we have internally into the cluster at Black Friday which is our",
    "start": "270820",
    "end": "278950"
  },
  {
    "start": "278000",
    "end": "301000"
  },
  {
    "text": "biggest commercial event of the year end of November we have roughly doubled of what",
    "start": "278950",
    "end": "285520"
  },
  {
    "text": "we have in the normal week as and you can see here we scale to 512 notes last",
    "start": "285520",
    "end": "291700"
  },
  {
    "text": "Black Friday this is not in company this is we just moved it this year so we will",
    "start": "291700",
    "end": "297040"
  },
  {
    "text": "see how it goes but just to see what the scale is and normally the workload is",
    "start": "297040",
    "end": "305140"
  },
  {
    "start": "301000",
    "end": "348000"
  },
  {
    "text": "not 500 but more like 200 each two instances for this elasticsearch pasta",
    "start": "305140",
    "end": "310410"
  },
  {
    "text": "we have different instance types depending on which country we are serving they are all part of one big",
    "start": "310410",
    "end": "316240"
  },
  {
    "text": "elasticsearch pasta but it was running on ec2 AWS with a little bit of tooling",
    "start": "316240",
    "end": "322630"
  },
  {
    "text": "around some bash scripts here and there some Jenkins pipelines running quite",
    "start": "322630",
    "end": "327700"
  },
  {
    "text": "well managed by the team but there was a lot of manual things involved and considering that we had made kubernetes",
    "start": "327700",
    "end": "334600"
  },
  {
    "text": "the default deployment target it made sense to move it not only to kubernetes",
    "start": "334600",
    "end": "339850"
  },
  {
    "text": "but maybe also make some improvements along the way so goal was moving to",
    "start": "339850",
    "end": "345669"
  },
  {
    "text": "kubernetes and by moving it to kubernetes we we had some goals that we",
    "start": "345669",
    "end": "351760"
  },
  {
    "start": "348000",
    "end": "424000"
  },
  {
    "text": "wanted to achieve so we wanted to have Siri save automatic updates of the elasticsearch cluster this means both",
    "start": "351760",
    "end": "358780"
  },
  {
    "text": "that elasticsearch can update like updating a version of elasticsearch or changing a configuration of the of the",
    "start": "358780",
    "end": "365410"
  },
  {
    "text": "nodes in the cluster but it should also handle updates of the underlying kubernetes cluster so the way we roll to",
    "start": "365410",
    "end": "373000"
  },
  {
    "text": "veneers processes we scale out the nodes and then move workloads to the new nodes",
    "start": "373000",
    "end": "378370"
  },
  {
    "text": "and this is done all automatically and the tooling for elasticsearch has to be",
    "start": "378370",
    "end": "384370"
  },
  {
    "text": "aware of this all the application has to somehow handle this another thing was",
    "start": "384370",
    "end": "389740"
  },
  {
    "text": "that now we are moving anyway the they're all set up in impure AWS was very static like we could add manually",
    "start": "389740",
    "end": "396370"
  },
  {
    "text": "nodes and for for like Black Friday but we couldn't like auto ski and scale down during the week where we",
    "start": "396370",
    "end": "401739"
  },
  {
    "text": "didn't have so much traffic so actually the cluster was usually scaled to the peak to handle the peak load of the week",
    "start": "401739",
    "end": "407739"
  },
  {
    "text": "and for the whole week we maybe the double amount of notes that was needed in the night and so on",
    "start": "407739",
    "end": "413469"
  },
  {
    "text": "so we wanted to have advanced auto scaling to like reduce costs to be able",
    "start": "413469",
    "end": "418629"
  },
  {
    "text": "to scale down and when the classes are due so much this was the criteria we looked into",
    "start": "418629",
    "end": "426039"
  },
  {
    "start": "424000",
    "end": "642000"
  },
  {
    "text": "first like how can how do people run elasticsearch and kubernetes we found",
    "start": "426039",
    "end": "431709"
  },
  {
    "text": "the kind of official help chart for a running elastic search which is a stateful set",
    "start": "431709",
    "end": "437289"
  },
  {
    "text": "I hope you all somewhat familiar with staple sets in companies I will not go so much into it but the idea is that you",
    "start": "437289",
    "end": "443699"
  },
  {
    "text": "you have a way to say I want to have X number of pots and they have some volumes attached potentially and they",
    "start": "443699",
    "end": "450819"
  },
  {
    "text": "have a certain criteria for what is the naming of the pots and so on that you can kind of reliably and the way it",
    "start": "450819",
    "end": "458379"
  },
  {
    "text": "works if you just take the helm chart and you for instance think of a situation where you need to update the",
    "start": "458379",
    "end": "463749"
  },
  {
    "text": "cluster then the cluster would like want to drain node so let's say we have three elasticsearch pots or nodes in the",
    "start": "463749",
    "end": "471459"
  },
  {
    "text": "lecture class the gray are nodes of the company's cluster and we want to drain",
    "start": "471459",
    "end": "477219"
  },
  {
    "text": "one node so the tooling will drain a node this means that we need to delete this pot on the on the node that's being",
    "start": "477219",
    "end": "484360"
  },
  {
    "text": "drained and what will happen with the home chart setup or the stateful set is",
    "start": "484360",
    "end": "489819"
  },
  {
    "text": "that it will yeah trigger draining from our infrastructure basically terminating their pots as",
    "start": "489819",
    "end": "495969"
  },
  {
    "text": "kubernetes will handle termination normally then in communities you can set a pre stop hook on the on the pot",
    "start": "495969",
    "end": "503739"
  },
  {
    "text": "definition which can be like a batch script you can run or whatever and what it does in the home chart is that it",
    "start": "503739",
    "end": "509860"
  },
  {
    "text": "excludes the node from the elasticsearch class time meaning that data will not be",
    "start": "509860",
    "end": "515318"
  },
  {
    "text": "moved there so electric search will stop moving data there and will actually like drain the drain the data away from this",
    "start": "515319",
    "end": "523149"
  },
  {
    "text": "node this we want to do so we don't just drop there all the data we have there if",
    "start": "523149",
    "end": "528519"
  },
  {
    "text": "you have volumes of course you could have it still attached to a new part it comes up but we also want to make",
    "start": "528519",
    "end": "534870"
  },
  {
    "text": "sure that we we are not letting any traffic go to this node why we're training it so this is kind of why why",
    "start": "534870",
    "end": "541740"
  },
  {
    "text": "you want to do this the data is moved as shown here and then the part becomes",
    "start": "541740",
    "end": "548040"
  },
  {
    "text": "empty doesn't have any data and then it can be be deleted with this pre stop",
    "start": "548040",
    "end": "554220"
  },
  {
    "text": "hook you have this thing that the part is already going into a termination phase you cannot like on you cannot stop",
    "start": "554220",
    "end": "560880"
  },
  {
    "text": "this in company this is already terminating and you said that the chart sets like a one hour delay of the",
    "start": "560880",
    "end": "567630"
  },
  {
    "text": "training so if your train takes longer than one hour then you have a problem here you cannot really handle this also",
    "start": "567630",
    "end": "574350"
  },
  {
    "text": "if the batch script doesn't handle network issues or something during the phase it will just fail maybe and then",
    "start": "574350",
    "end": "581880"
  },
  {
    "text": "you yeah doesn't really execute the the moving update dates are correctly this",
    "start": "581880",
    "end": "587550"
  },
  {
    "text": "is one problem with this approach if we go further what in the ideal case when",
    "start": "587550",
    "end": "592829"
  },
  {
    "text": "it works what would happen is a new part would be replaced by the stateful set on a new node and this has a post at hook",
    "start": "592829",
    "end": "601699"
  },
  {
    "text": "which basically removes all these excluse in elasticsearch meaning that days can then be moved to the new part",
    "start": "601699",
    "end": "608220"
  },
  {
    "text": "and elasticsearch will like rebalance and so on the problem here is that if",
    "start": "608220",
    "end": "614610"
  },
  {
    "text": "you try to alter scale at the same time and you have this priest at hook which just unblocks all of them then you could",
    "start": "614610",
    "end": "620160"
  },
  {
    "text": "have one part that is training and have excluded itself and then you have another one that just like on excludes",
    "start": "620160",
    "end": "626759"
  },
  {
    "text": "everything and then it starts to get data again of course you can implement more sophisticated scripts or maybe",
    "start": "626759",
    "end": "633000"
  },
  {
    "text": "writing something in the more advanced language and handle these cases but it begins to sound like you actually need",
    "start": "633000",
    "end": "639420"
  },
  {
    "text": "something like an operator so the operator pattern is it comes in here I",
    "start": "639420",
    "end": "646769"
  },
  {
    "start": "642000",
    "end": "735000"
  },
  {
    "text": "took this from the chorus they kind of coined this phrase operator in 2016 I",
    "start": "646769",
    "end": "654180"
  },
  {
    "text": "think I don't think they like invented the idea but they kind of called it operator I think I like this term and",
    "start": "654180",
    "end": "660930"
  },
  {
    "text": "they stated an operator represent human operational knowledge and software through we manage an application so the way I",
    "start": "660930",
    "end": "668220"
  },
  {
    "text": "understand this you're in code like how an offer what you would normally do as a human cube detail do this call Dre",
    "start": "668220",
    "end": "675780"
  },
  {
    "text": "nonetheless research all these things you incorporate into code that can perform this for you this is one way I",
    "start": "675780",
    "end": "683250"
  },
  {
    "text": "see operators I also see when I look at what other people are building as operators operators can also has have",
    "start": "683250",
    "end": "689730"
  },
  {
    "text": "the functionality of like providing an abstraction so instead of users defining",
    "start": "689730",
    "end": "695580"
  },
  {
    "text": "a stateful set a service volume claim defining all the people talking behind",
    "start": "695580",
    "end": "702540"
  },
  {
    "text": "sorry instead of the users defining all these resources and what environment",
    "start": "702540",
    "end": "708810"
  },
  {
    "text": "variables to give and so on you can create an abstraction that like hides all of this for the users this is not",
    "start": "708810",
    "end": "714660"
  },
  {
    "text": "what we focused on we focused on the purely operational part and to start with because this was really the problem",
    "start": "714660",
    "end": "720990"
  },
  {
    "text": "we had the defining making user friendly and so on was not so important because we're very like the team that operates",
    "start": "720990",
    "end": "727380"
  },
  {
    "text": "is a very technical knows a lot about elasticsearch so they want to tune it to to their liking",
    "start": "727380",
    "end": "735050"
  },
  {
    "start": "735000",
    "end": "809000"
  },
  {
    "text": "but the idea we first had we call it like be zero what to just manage a",
    "start": "735050",
    "end": "741330"
  },
  {
    "text": "stateful set in a bit smarter way so what we could have what we said was okay",
    "start": "741330",
    "end": "747660"
  },
  {
    "text": "we cannot put we cannot modify the replicas directly that you normally have",
    "start": "747660",
    "end": "752880"
  },
  {
    "text": "a stateful set because if you drain a part it has this problem that I",
    "start": "752880",
    "end": "758430"
  },
  {
    "text": "described before it just stops terminating and and then you cannot really stop this operation and if you",
    "start": "758430",
    "end": "764790"
  },
  {
    "text": "take longer than one hour then you cannot handle this case so we wanted to have it split in like the replicas on",
    "start": "764790",
    "end": "771600"
  },
  {
    "text": "the stateful set and then the desired replicas which is what the operator tries to achieve so you might be scaling",
    "start": "771600",
    "end": "777600"
  },
  {
    "text": "obviously easy you just add another part but if you're scaling down you could have like replicas smaller the desired",
    "start": "777600",
    "end": "782970"
  },
  {
    "text": "replicas which we put in an annotation as you can see it could be small and",
    "start": "782970",
    "end": "788460"
  },
  {
    "text": "then the other one could be high and then it would eventually like drain and achieve this we also used update",
    "start": "788460",
    "end": "794550"
  },
  {
    "text": "strategy on the lesion which meant that instead of the staple set controller like modifying",
    "start": "794550",
    "end": "801140"
  },
  {
    "text": "and updating the pots when we make an update the operator can control this person and kill parts once at entering",
    "start": "801140",
    "end": "806510"
  },
  {
    "text": "them so this didn't really work for us it was too low level and like it was",
    "start": "806510",
    "end": "815810"
  },
  {
    "start": "809000",
    "end": "838000"
  },
  {
    "text": "complicated to just update the stateful set without because you have this you have a certain number of replicas in",
    "start": "815810",
    "end": "820910"
  },
  {
    "text": "your jungle file and if you update this then you override what is it about the operator is doing this was one thing and",
    "start": "820910",
    "end": "827600"
  },
  {
    "text": "state must be stored in annotation which is just not nice it's just a string that you have to then pass and it just",
    "start": "827600",
    "end": "833540"
  },
  {
    "text": "becomes a little bit ugly it's doable about it yeah so what we did instead was",
    "start": "833540",
    "end": "840170"
  },
  {
    "start": "838000",
    "end": "897000"
  },
  {
    "text": "we thought okay we just need some layer of abstraction on top of the stateful",
    "start": "840170",
    "end": "845360"
  },
  {
    "text": "set the staple said idea is kind of good it just needs the extra knowledge so we",
    "start": "845360",
    "end": "850399"
  },
  {
    "text": "created something we call elasticsearch data set which is a very thin wrapper it has the scaling thing for configuring",
    "start": "850399",
    "end": "856670"
  },
  {
    "text": "your auto scaling I will talk about this later and then it just have replicas like a stateful set the template for the",
    "start": "856670",
    "end": "862130"
  },
  {
    "text": "part template so you specify the full thing and the volume claim template so like a stateful set so this way anyone",
    "start": "862130",
    "end": "869570"
  },
  {
    "text": "can just take a stable set modify the bit and then they would have it work with their with the elasticsearch setup",
    "start": "869570",
    "end": "877269"
  },
  {
    "text": "and this way we have replicas on this state on the Lessing's data set",
    "start": "877269",
    "end": "882800"
  },
  {
    "text": "letting search data set and we have also a state on a stateful set which is",
    "start": "882800",
    "end": "888410"
  },
  {
    "text": "managed by the operator underneath and then we have these two different values that it can reconcile towards so this is",
    "start": "888410",
    "end": "895370"
  },
  {
    "text": "idea and from like a higher-level elasticsearch data sets can either be",
    "start": "895370",
    "end": "902390"
  },
  {
    "start": "897000",
    "end": "949000"
  },
  {
    "text": "like a full cluster if you want but you can also compose them into into like one big class that consists of different",
    "start": "902390",
    "end": "908839"
  },
  {
    "text": "elasticsearch datasets and this was what I mentioned in the beginning that we have different type of instances for a",
    "start": "908839",
    "end": "915980"
  },
  {
    "text": "different type of countries for instance so Germany is like a lot of traffic we get there so we have big instances and",
    "start": "915980",
    "end": "921649"
  },
  {
    "text": "unless it's search data set for that these this country and then for other countries we have smaller or instance",
    "start": "921649",
    "end": "928760"
  },
  {
    "text": "titles smaller parts requests in company this case but they all combine into one big elasticsearch lasta which",
    "start": "928760",
    "end": "935810"
  },
  {
    "text": "is how the tooling in our search infrastructure works but you could also have like one set of masters for each of",
    "start": "935810",
    "end": "943010"
  },
  {
    "text": "your elasticsearch data set you can kind of compose it like you want and the operator then manages all these things",
    "start": "943010",
    "end": "949840"
  },
  {
    "text": "so if we look at the example again like I gave with the stateful set now with the operator involved we now have a",
    "start": "949840",
    "end": "956660"
  },
  {
    "text": "service created by the operator that that like goes into the elasticsearch API we have the same parts and let's",
    "start": "956660",
    "end": "964340"
  },
  {
    "text": "think of the same scenario where we have a pot node kubernetes node that is being",
    "start": "964340",
    "end": "970790"
  },
  {
    "text": "drained what the operator does it detects this because it looks at the",
    "start": "970790",
    "end": "975890"
  },
  {
    "text": "label of the node or the status of the node okay it's being drained then it scales out the stateful set underneath",
    "start": "975890",
    "end": "982670"
  },
  {
    "text": "so we get an extra pot so now we have somewhere to move the data from this one",
    "start": "982670",
    "end": "988070"
  },
  {
    "text": "we want to drain then it calls into elasticsearch similar to this priest up",
    "start": "988070",
    "end": "993470"
  },
  {
    "text": "script would do and says exclude this particular node from the cluster so data",
    "start": "993470",
    "end": "999470"
  },
  {
    "text": "is moved out of it and this will then move to the to the new to new",
    "start": "999470",
    "end": "1004560"
  },
  {
    "text": "elasticsearch node that is available and once the data is moved the ears operator",
    "start": "1004560",
    "end": "1009820"
  },
  {
    "text": "can safely delete the pot because now that it's completely empty and it's fine to delete it and then this case is",
    "start": "1009820",
    "end": "1018490"
  },
  {
    "text": "handled and also like if it takes more than one hour they operate I can just continue to try to drain if the operator",
    "start": "1018490",
    "end": "1024730"
  },
  {
    "text": "dies it will just start back up and it will just continue from where it stopped so it's like very resilient in that way",
    "start": "1024730",
    "end": "1031470"
  },
  {
    "text": "and and lastly the participant okay so",
    "start": "1031470",
    "end": "1037030"
  },
  {
    "start": "1035000",
    "end": "1187000"
  },
  {
    "text": "this is like the upgrading of a cluster cup of kubernetes and elasticsearch we",
    "start": "1037030",
    "end": "1043180"
  },
  {
    "text": "have this operator logic for doing this and this means that now we have this training logic it can scale out it can",
    "start": "1043180",
    "end": "1049420"
  },
  {
    "text": "move the data then it's also easy to like add auto scaling because we have",
    "start": "1049420",
    "end": "1054940"
  },
  {
    "text": "this functionality of training and so on already so what we implemented was like",
    "start": "1054940",
    "end": "1061270"
  },
  {
    "text": "custom auto scaling in two dimensions so we can scale the number of No and we can scale the number of replicas",
    "start": "1061270",
    "end": "1067150"
  },
  {
    "text": "inside the elasticsearch cluster so the replicas of the shots of the indices we",
    "start": "1067150",
    "end": "1075370"
  },
  {
    "text": "loop into using the horizontal part autoscaler but it's very very simple in kubernetes and it basically just takes",
    "start": "1075370",
    "end": "1081820"
  },
  {
    "text": "an average of like if you're scaling a CPU takes an average over all the parts and then calculate how much would be",
    "start": "1081820",
    "end": "1086970"
  },
  {
    "text": "needed to actually satisfy that whatever constraint you said this is too simple",
    "start": "1086970",
    "end": "1092380"
  },
  {
    "text": "for us because both it's too reactive like it can be scaling up and down too fast so we added something like a",
    "start": "1092380",
    "end": "1099460"
  },
  {
    "text": "duration that the CPU has to be above a certain threshold for a certain duration",
    "start": "1099460",
    "end": "1104530"
  },
  {
    "text": "that you configure and we also have a cool-down so if you just scaled up the cooldown has to exceed before you can",
    "start": "1104530",
    "end": "1110830"
  },
  {
    "text": "scale to another scaling edge and either up or down so you don't have this waiver scale up and scale down and scale up and",
    "start": "1110830",
    "end": "1116140"
  },
  {
    "text": "move days all around and in this example we can scale up so you see the graph",
    "start": "1116140",
    "end": "1122350"
  },
  {
    "text": "here it shows like the that the CPU is over a certain threshold for some time",
    "start": "1122350",
    "end": "1127870"
  },
  {
    "text": "we haven't we have a very simple case with just one node which is maybe not so so safe but it's very cost efficient",
    "start": "1127870",
    "end": "1134940"
  },
  {
    "text": "where we have six shots and if you want to scale this up then we can add to one new node and then we can split the shots",
    "start": "1134940",
    "end": "1141550"
  },
  {
    "text": "over these two nodes if you want to scale up further then we can split it",
    "start": "1141550",
    "end": "1147070"
  },
  {
    "text": "again and have three nodes and split to charge two charts on each node and this way we are reducing the CPU per node",
    "start": "1147070",
    "end": "1153730"
  },
  {
    "text": "obviously and if we need to scale further we can do so and here we are like limited by you can say that number",
    "start": "1153730",
    "end": "1160930"
  },
  {
    "text": "of max Potts the max part you want to have but we also limited by the minimum",
    "start": "1160930",
    "end": "1166390"
  },
  {
    "text": "number of shots per node so if you have one as minimum you obviously cannot go below because then you would get notes",
    "start": "1166390",
    "end": "1171670"
  },
  {
    "text": "with no shots on it which is just a waste of money also if you have like a limit of two then you would kind of stop",
    "start": "1171670",
    "end": "1178120"
  },
  {
    "text": "here and then you could could not scale further but this you can all configure in the in the ideas the electrics search",
    "start": "1178120",
    "end": "1184960"
  },
  {
    "text": "data setup similar if you go the other way scaling down the CPU is below a certain",
    "start": "1184960",
    "end": "1191470"
  },
  {
    "start": "1187000",
    "end": "1300000"
  },
  {
    "text": "threshold for a duration then we start to scale down scale down to the three",
    "start": "1191470",
    "end": "1196540"
  },
  {
    "text": "nodes two shots is and so on all the way down and here like the minimum replicas are the boundaries",
    "start": "1196540",
    "end": "1203470"
  },
  {
    "text": "so if we have a limit of two replicas then it would not scale further than this and also the max shots per node so",
    "start": "1203470",
    "end": "1210580"
  },
  {
    "text": "if you have a max of three then we could also not scale even though we the minimum replicas might allow it and then",
    "start": "1210580",
    "end": "1217300"
  },
  {
    "text": "we have another safety which is max disk users and this is to prevent that we",
    "start": "1217300",
    "end": "1223260"
  },
  {
    "text": "scale down but the less we have so much the less research data that we actually",
    "start": "1223260",
    "end": "1228430"
  },
  {
    "text": "will fill up the disk and cause problems that way so we also checking into elasticsearch what are this disk the",
    "start": "1228430",
    "end": "1236770"
  },
  {
    "text": "users of the different notes and then we check before we do an auto scaling action if it can actually happen without",
    "start": "1236770",
    "end": "1242770"
  },
  {
    "text": "reaching this boundary so it's just a safety measure another thing is that if",
    "start": "1242770",
    "end": "1249220"
  },
  {
    "text": "something goes wrong and this is mostly when scaling down and the cost is not in green state the operator backs off and",
    "start": "1249220",
    "end": "1255700"
  },
  {
    "text": "it doesn't do anything so it just waits for the cluster to be green either I can",
    "start": "1255700",
    "end": "1260950"
  },
  {
    "text": "do it by itself or if an operator like a human operator is needed then they will not do anything until the clusters green",
    "start": "1260950",
    "end": "1267640"
  },
  {
    "text": "again and someone has like it might need that you need to - yeah - whatever so",
    "start": "1267640",
    "end": "1275100"
  },
  {
    "text": "this is like an all safety to prevent that it yeah misses of the cluster further than it",
    "start": "1275100",
    "end": "1281170"
  },
  {
    "text": "was already it could be that you're scaling down then you lose a node because of the cloud provide or whatever",
    "start": "1281170",
    "end": "1286660"
  },
  {
    "text": "at the same time and then you didn't replicate enough and and and and you are in a bad situation but it tries to like",
    "start": "1286660",
    "end": "1293740"
  },
  {
    "text": "protect you as much as possible the same when doing an upgrade it will not continue an upgrade if the clusters is not green so I mentioned there are two",
    "start": "1293740",
    "end": "1301780"
  },
  {
    "start": "1300000",
    "end": "1344000"
  },
  {
    "text": "dimensions for scaling the other one is like scaling the replicas of the indices",
    "start": "1301780",
    "end": "1307990"
  },
  {
    "text": "in elasticsearch so here we have an example where we just have one shot and like adding another node wouldn't do",
    "start": "1307990",
    "end": "1316810"
  },
  {
    "text": "anything because when we just have a node with no shots on it so what we want to do is we want to scale the the number",
    "start": "1316810",
    "end": "1323020"
  },
  {
    "text": "of replicas for the for the index and then we just add another shot and then we can scale up and actually utilize the",
    "start": "1323020",
    "end": "1329920"
  },
  {
    "text": "nodes and this we can of course until we hit the boundary which would be",
    "start": "1329920",
    "end": "1335520"
  },
  {
    "text": "minimum number of shots per note and the maximum Potts replicas that we can scale",
    "start": "1335520",
    "end": "1342220"
  },
  {
    "text": "to just to show that this actually works",
    "start": "1342220",
    "end": "1348220"
  },
  {
    "start": "1344000",
    "end": "1429000"
  },
  {
    "text": "in production so this is over seven days the green and orange is like the CPU you",
    "start": "1348220",
    "end": "1353890"
  },
  {
    "text": "choose so we can scale based on CPU this fits into our use case and the blue line",
    "start": "1353890",
    "end": "1360280"
  },
  {
    "text": "is the number of replicas or number of cuts in this particular so this is only showing de Germany so only one of these",
    "start": "1360280",
    "end": "1367720"
  },
  {
    "text": "elastic search data sets so only a subset of the whole cluster but it",
    "start": "1367720",
    "end": "1374440"
  },
  {
    "text": "scales for like from like 22 or so to 44",
    "start": "1374440",
    "end": "1379510"
  },
  {
    "text": "or 45 and the reason that it that it here is so steep you can see it when i",
    "start": "1379510",
    "end": "1384760"
  },
  {
    "text": "zoom in here you see the scale up it looks for some time okay the CPU is over",
    "start": "1384760",
    "end": "1390010"
  },
  {
    "text": "a certain threshold and then it scales up by a lot this is because we basically have a case where we had why we are",
    "start": "1390010",
    "end": "1396100"
  },
  {
    "text": "duplicating the replicas because this is how we can how it configure to scale in",
    "start": "1396100",
    "end": "1401350"
  },
  {
    "text": "our case and then later on after people have looked at all the things and shop",
    "start": "1401350",
    "end": "1406990"
  },
  {
    "text": "during the evening then it starts to scale down slowly and you can actually see it scaled down within one hour so",
    "start": "1406990",
    "end": "1412180"
  },
  {
    "text": "back to the to the lower limit that we have configured and this way we are",
    "start": "1412180",
    "end": "1418000"
  },
  {
    "text": "saving instead of running with the high line of 40 for every time all the week",
    "start": "1418000",
    "end": "1423250"
  },
  {
    "text": "we are running most of the time with only 22 or so so so what are some of the",
    "start": "1423250",
    "end": "1432160"
  },
  {
    "start": "1429000",
    "end": "1604000"
  },
  {
    "text": "lessons learned and and takeaways I think that like if you have all these",
    "start": "1432160",
    "end": "1438070"
  },
  {
    "text": "batch scripts lying around for operating all your staple things turn them into operators I think there's a cost up",
    "start": "1438070",
    "end": "1446620"
  },
  {
    "text": "front of building an operator but in the long run it's really deserved because",
    "start": "1446620",
    "end": "1452800"
  },
  {
    "text": "you yeah you you move a lot of manual effort away from yourself and can focus",
    "start": "1452800",
    "end": "1459310"
  },
  {
    "text": "on other things of course if you have an elastic search first of three nodes only storing logs and okay if it's unavailable for a little",
    "start": "1459310",
    "end": "1466410"
  },
  {
    "text": "time then maybe you don't need this but if you have a bigger bigger setup then I think it's definitely the way to go and",
    "start": "1466410",
    "end": "1473480"
  },
  {
    "text": "kubernetes is one way to do it you could also implement all these things in AWS or Google cloud without kubernetes I",
    "start": "1473480",
    "end": "1480150"
  },
  {
    "text": "think but I think ranee this just makes it much easier because you have this API you have like if you use the stateful",
    "start": "1480150",
    "end": "1486450"
  },
  {
    "text": "set like we do you get all this volume claim support and all these things out of the box so you get a lot of things",
    "start": "1486450",
    "end": "1492870"
  },
  {
    "text": "for free that you don't actually have to implement another thing that we also",
    "start": "1492870",
    "end": "1498840"
  },
  {
    "text": "like learn or that I kept like saying to the other people in the team is like",
    "start": "1498840",
    "end": "1505320"
  },
  {
    "text": "assume the operator can die at any point when you're writing the code it's like easy to say okay we just need to put",
    "start": "1505320",
    "end": "1510840"
  },
  {
    "text": "something in elasticsearch via the API then we store this in a local variable in our go program and then we do the",
    "start": "1510840",
    "end": "1517080"
  },
  {
    "text": "next thing but if you if the operator dies because the cluster is scaling down",
    "start": "1517080",
    "end": "1522360"
  },
  {
    "text": "and there has to move the operator part then you are losing this state so always do things where you store the state on",
    "start": "1522360",
    "end": "1528780"
  },
  {
    "text": "the custom resource or wherever but store the state somewhere outside of the operator every time you do something so",
    "start": "1528780",
    "end": "1534570"
  },
  {
    "text": "you can always come back and continue where I left off this is very important and they have this in mind when you're",
    "start": "1534570",
    "end": "1540210"
  },
  {
    "text": "when you're writing the code I also think like maybe this is a personal",
    "start": "1540210",
    "end": "1545550"
  },
  {
    "text": "opinion but start simple and add abstractions only when you need it so we started with the stateful set be 0 where",
    "start": "1545550",
    "end": "1551940"
  },
  {
    "text": "we try to just use that this was too simple and then we identified why we",
    "start": "1551940",
    "end": "1558390"
  },
  {
    "text": "needed this abstraction we needed to have the replica stole some whales and we needed to be able to update the",
    "start": "1558390",
    "end": "1564270"
  },
  {
    "text": "resource without affecting the existing replicas and like starting the thing to scale up and down unexpectedly we might",
    "start": "1564270",
    "end": "1572490"
  },
  {
    "text": "add a like an abstraction later on that makes it simpler to just define and elasticsearch cluster and get things out",
    "start": "1572490",
    "end": "1579450"
  },
  {
    "text": "of the box right now you have to deploy the master yourself just a deployment the stateful set it's not that hard but",
    "start": "1579450",
    "end": "1586020"
  },
  {
    "text": "it it is like an effort from the user point of view and this we could maybe abstract in the future but this was not",
    "start": "1586020",
    "end": "1591930"
  },
  {
    "text": "our goal our goal was to like make all these batch scripts and so on encoded into an operator so we don't have to do",
    "start": "1591930",
    "end": "1598670"
  },
  {
    "text": "so much about scaling and and updating and so on lastly I just want to mention",
    "start": "1598670",
    "end": "1605600"
  },
  {
    "start": "1604000",
    "end": "1684000"
  },
  {
    "text": "some of our own Souls projects so realistic search operator is obviously one kubernetes and native this is where",
    "start": "1605600",
    "end": "1612950"
  },
  {
    "text": "we run our configuration for kubernetes production clusters so this is how we",
    "start": "1612950",
    "end": "1619460"
  },
  {
    "text": "can figure all our clusters you can get inspired by this we also have post press operator and I think like elasticsearch",
    "start": "1619460",
    "end": "1626330"
  },
  {
    "text": "over radio it's much simpler because you don't have this like single master that you have in Postgres so it's a much simpler problem to solve",
    "start": "1626330",
    "end": "1632510"
  },
  {
    "text": "and I think it also applies to like like I said Cassandra and other things and",
    "start": "1632510",
    "end": "1637790"
  },
  {
    "text": "then we recently have one of my colleagues released price on kubernetes",
    "start": "1637790",
    "end": "1645470"
  },
  {
    "text": "operator framework so normally the most of the things I go they offer our elastic search operators also and go but",
    "start": "1645470",
    "end": "1651650"
  },
  {
    "text": "the his team was using Python so they developed their like pythonic framework called cup and this like a very nice",
    "start": "1651650",
    "end": "1658670"
  },
  {
    "text": "little framework that abstracts a lot of things away so you can like just a few",
    "start": "1658670",
    "end": "1663800"
  },
  {
    "text": "functions in Python code then you can write an operator and it takes care of managing in bins and handling the",
    "start": "1663800",
    "end": "1670100"
  },
  {
    "text": "customer resources and all this for you so it's quite interesting also I think you should check it out and that's it",
    "start": "1670100",
    "end": "1677030"
  },
  {
    "text": "thank you very much for attending we",
    "start": "1677030",
    "end": "1685790"
  },
  {
    "start": "1684000",
    "end": "1770000"
  },
  {
    "text": "have 10 minutes for questions",
    "start": "1685790",
    "end": "1688960"
  },
  {
    "text": "I have a question regarding the scaling and scale out the scaling and scale out",
    "start": "1699130",
    "end": "1706570"
  },
  {
    "text": "are relevant only to the pods right you are not scaling down and up the kubernetes cluster as well I mean if I",
    "start": "1706570",
    "end": "1714669"
  },
  {
    "text": "will run a kubernetes cluster based on ec2 then it might it will scale only the",
    "start": "1714669",
    "end": "1720519"
  },
  {
    "text": "pods but not the cluster the ec2 will still be running yeah so we use a",
    "start": "1720519",
    "end": "1725769"
  },
  {
    "text": "cluster autoscaler in the clusters so if the if the parts demand for parts are less than what the cluster provides of",
    "start": "1725769",
    "end": "1732309"
  },
  {
    "text": "resources then it will scale down but obviously it will not force the elasticsearch thruster to scale down so",
    "start": "1732309",
    "end": "1738820"
  },
  {
    "text": "it's only when if you if you scale down the elasticsearch class that in the notes might also be scaled down",
    "start": "1738820",
    "end": "1744370"
  },
  {
    "text": "underneath the ec2 instances okay thank you",
    "start": "1744370",
    "end": "1749340"
  },
  {
    "text": "hi did you have any solution to safely shut down and startup elasticsearch so",
    "start": "1759690",
    "end": "1766870"
  },
  {
    "text": "the question is if I solution for say please no not an operator to implement any logic you've heard a distributed",
    "start": "1766870",
    "end": "1773710"
  },
  {
    "text": "elasticsearch cluster to stop and start it properly because if you kill one of",
    "start": "1773710",
    "end": "1780070"
  },
  {
    "text": "the nodes usually starts replicating to the others yeah what do you mean like",
    "start": "1780070",
    "end": "1785200"
  },
  {
    "text": "turning down the whole cluster and then bring you back up you starting stopping the obvious we don't this we don't have",
    "start": "1785200",
    "end": "1790929"
  },
  {
    "text": "we haven't had we don't have this use case but I mean maybe it's feel free to open issues on our open source",
    "start": "1790929",
    "end": "1798220"
  },
  {
    "text": "repository if you need something that we didn't think about yeah so thanks for",
    "start": "1798220",
    "end": "1806980"
  },
  {
    "text": "the talk it was really interesting you explained how you did it I was actually wondering very much the reason",
    "start": "1806980",
    "end": "1814870"
  },
  {
    "text": "why you did it I mean as you said earlier you could do the same using some ec2 instances you",
    "start": "1814870",
    "end": "1821320"
  },
  {
    "text": "decided to implement an operator to manage it on kubernetes it's a lot of effort so I mean I'm sure you you had",
    "start": "1821320",
    "end": "1829450"
  },
  {
    "text": "very good reason to take these this choice what are these good reasons and do you validate them so like I said that",
    "start": "1829450",
    "end": "1838179"
  },
  {
    "text": "we declared kubernetes the default deployment target and this is for the whole organization for several reasons",
    "start": "1838179",
    "end": "1845490"
  },
  {
    "text": "one thing is that we have like a CH DD system that is integrated with kubernetes and it's much easier to use",
    "start": "1845490",
    "end": "1851500"
  },
  {
    "text": "and it's much like nice of our developers to deploy against kubernetes and what we had before was like one of",
    "start": "1851500",
    "end": "1857650"
  },
  {
    "text": "the reasons that we are moving to kubernetes this also like automatically",
    "start": "1857650",
    "end": "1864520"
  },
  {
    "text": "means that it makes sense to actually run these try to run reduce workloads you could say lecture to staple set our",
    "start": "1864520",
    "end": "1870010"
  },
  {
    "text": "stateful application it's more difficult but they were kind of needing this anyway they they had like they were",
    "start": "1870010",
    "end": "1876850"
  },
  {
    "text": "managing it but it wasn't like sufficient they used a lot of time on managing it manually so they needed to",
    "start": "1876850",
    "end": "1883990"
  },
  {
    "text": "implement something like this anyway and like I said I think kubernetes just provides a lot of the features out of",
    "start": "1883990",
    "end": "1890169"
  },
  {
    "text": "the box makes it easier so yeah and and by having this we can also like offer it to",
    "start": "1890169",
    "end": "1896300"
  },
  {
    "text": "the open source community other people can benefit from it we can improve it I think also like that if the code is so",
    "start": "1896300",
    "end": "1901940"
  },
  {
    "text": "abstracted in the code that it can be used also to implement and an operator for Cassandra or something we haven't",
    "start": "1901940",
    "end": "1907760"
  },
  {
    "text": "ventured there yet but it's I think it's possible so these are some of the side",
    "start": "1907760",
    "end": "1913490"
  },
  {
    "text": "effects I mean in the end it's kind of the organization going this way and I think it's good reasons for it but yeah",
    "start": "1913490",
    "end": "1920780"
  },
  {
    "text": "it could also be done otherwise but yeah",
    "start": "1920780",
    "end": "1925090"
  },
  {
    "text": "yeah and I realized this was only announced yesterday but I wondered whether you'd had an opportunity to",
    "start": "1929470",
    "end": "1935570"
  },
  {
    "start": "1930000",
    "end": "2000000"
  },
  {
    "text": "compare elastics own operator again we",
    "start": "1935570",
    "end": "1941720"
  },
  {
    "text": "have been a little bit talk I have not been so much involved but I haven't I haven't seen it at all yet so I cannot",
    "start": "1941720",
    "end": "1946730"
  },
  {
    "text": "comment at all on this thank you",
    "start": "1946730",
    "end": "1950860"
  },
  {
    "text": "hi I would be interested in how large are these elasticsearch data sets when",
    "start": "1958530",
    "end": "1965290"
  },
  {
    "text": "you scale them up and down how much data is moved around yeah this is actually good questions that'd that I don't know",
    "start": "1965290",
    "end": "1971770"
  },
  {
    "text": "the answer to so I said in the beginning that I'm not so much in the elasticsearch side and I also don't",
    "start": "1971770",
    "end": "1978370"
  },
  {
    "text": "manage this now so I actually don't know Oliver ocean from my from from solando is is",
    "start": "1978370",
    "end": "1986260"
  },
  {
    "text": "the one to ask I can I can tweet about him and then you can can ask this way he would he would",
    "start": "1986260",
    "end": "1991840"
  },
  {
    "text": "know it any other questions",
    "start": "1991840",
    "end": "1997200"
  },
  {
    "text": "how many clusters are managed by one operator and how do you handle the",
    "start": "1999990",
    "end": "2005400"
  },
  {
    "start": "2000000",
    "end": "2065000"
  },
  {
    "text": "relationship between the two essentially",
    "start": "2005400",
    "end": "2010890"
  },
  {
    "text": "it can manage any kind of any number of clusters it will it you have to deploy",
    "start": "2010890",
    "end": "2017220"
  },
  {
    "text": "the masters yourself so depending on how you configure the elasticsearch datasets to connect to which master be part of",
    "start": "2017220",
    "end": "2023580"
  },
  {
    "text": "which cluster but the operator doesn't really care about this it cares about the data sets and they each have a",
    "start": "2023580",
    "end": "2030330"
  },
  {
    "text": "service that is pointing into the cluster day apart off so it will it can operate any number you can also limit it",
    "start": "2030330",
    "end": "2037020"
  },
  {
    "text": "to a certain namespace and you can also limit it through just a single elasticsearch data set that has a",
    "start": "2037020",
    "end": "2042810"
  },
  {
    "text": "certain annotation that means only control this one so you can also run multiple controllers and they won't",
    "start": "2042810",
    "end": "2048148"
  },
  {
    "text": "operate us and it will not like interfere with each other but there's no like rules for this you can do it how",
    "start": "2048149",
    "end": "2054600"
  },
  {
    "text": "you want how it see fit I cannot here",
    "start": "2054600",
    "end": "2060530"
  },
  {
    "text": "have you run into any scalability issues sort of that the operator didn't manage",
    "start": "2064909",
    "end": "2070108"
  },
  {
    "start": "2065000",
    "end": "2120000"
  },
  {
    "text": "to sort of handle all the classes it had to manage so you could have two around",
    "start": "2070109",
    "end": "2076908"
  },
  {
    "text": "the question is if we had any scalability issues that we were running it in production for around three weeks",
    "start": "2076909",
    "end": "2083839"
  },
  {
    "text": "and we had one issue which was that we reached the limit of our network all in",
    "start": "2083839",
    "end": "2090839"
  },
  {
    "text": "network size in the kubernetes cluster so it wasn't really because of the operator but that meant that we basically had to reduce the number of",
    "start": "2090839",
    "end": "2097260"
  },
  {
    "text": "IPs reallocate to his node so we could get more IP space this was kind of",
    "start": "2097260",
    "end": "2102299"
  },
  {
    "text": "interesting to see it didn't really have an impact because it just meant new nodes didn't come up but we haven't we",
    "start": "2102299",
    "end": "2107849"
  },
  {
    "text": "haven't seen anything like obviously we have fixed issues along the way but since production we haven't seen issues",
    "start": "2107849",
    "end": "2114029"
  },
  {
    "text": "with this auto scaling of the elasticsearch itself hi",
    "start": "2114029",
    "end": "2120839"
  },
  {
    "start": "2120000",
    "end": "2286000"
  },
  {
    "text": "here we heard probably about the coop DB operator on the markets they do opera",
    "start": "2120839",
    "end": "2127230"
  },
  {
    "text": "they operate actually a lot of different storages elasticsearch as well if so do",
    "start": "2127230",
    "end": "2135000"
  },
  {
    "text": "you have any like plan to merge feature sets between these two operators to like",
    "start": "2135000",
    "end": "2142529"
  },
  {
    "text": "to leverage both operators experience into one I think and correct me if I'm",
    "start": "2142529",
    "end": "2151380"
  },
  {
    "text": "wrong I think the approach is a little bit different in the way that I see that they take it more from the abstraction",
    "start": "2151380",
    "end": "2157500"
  },
  {
    "text": "point of view that's how I see it that they like try to make it nice for the users and you just have a simple",
    "start": "2157500",
    "end": "2163049"
  },
  {
    "text": "definition and you just get a full cluster out of this and we kind of",
    "start": "2163049",
    "end": "2168299"
  },
  {
    "text": "approach it in my mind from the other side where we try to first fix all the like operating the hard parts of",
    "start": "2168299",
    "end": "2174329"
  },
  {
    "text": "operating it especially for our use case and we don't haven't fixed we haven't",
    "start": "2174329",
    "end": "2180390"
  },
  {
    "text": "focused on like making it super easy to just go to our page and just deploy be a",
    "start": "2180390",
    "end": "2185700"
  },
  {
    "text": "one simple Yama file so this would be like the things where we could maybe converge but yeah we haven't looked so",
    "start": "2185700",
    "end": "2193140"
  },
  {
    "text": "much into it but when we looked around for something before develop our own that wasn't really",
    "start": "2193140",
    "end": "2198270"
  },
  {
    "text": "something that that did what we wanted it to do on the operation sides on the heavy heavy lifting there so any",
    "start": "2198270",
    "end": "2213090"
  },
  {
    "text": "question I mean how is operator one per",
    "start": "2213090",
    "end": "2218510"
  },
  {
    "text": "namespace or you have namespace for operator and the different clusters in different",
    "start": "2218510",
    "end": "2224970"
  },
  {
    "text": "namespaces you can run it however you like you can say either one operator can",
    "start": "2224970",
    "end": "2231030"
  },
  {
    "text": "only charge this namespace where it's running or you can control with a bug rules what it gets access to we run it",
    "start": "2231030",
    "end": "2238290"
  },
  {
    "text": "just like in one in the cluster and then it can like manage any number of",
    "start": "2238290",
    "end": "2243540"
  },
  {
    "text": "elasticsearch clusters within this kubernetes cluster but do you how you want but you can run one pair of names",
    "start": "2243540",
    "end": "2250440"
  },
  {
    "text": "yeah we just we have one just for the whole cluster and then it just manages no matter the namespace because we don't",
    "start": "2250440",
    "end": "2256200"
  },
  {
    "text": "like respect the namespace in our setup yeah but it is quite dangerous because if you want to have one Operator for all",
    "start": "2256200",
    "end": "2265260"
  },
  {
    "text": "namespaces update of operator can broke me database in all namespaces yeah I",
    "start": "2265260",
    "end": "2274550"
  },
  {
    "text": "think we might have time for one more short question may have one all right in",
    "start": "2274550",
    "end": "2281220"
  },
  {
    "text": "that case thank you very much Mikkel and don't forget to please rate it on the app thank you thank you",
    "start": "2281220",
    "end": "2287630"
  }
]