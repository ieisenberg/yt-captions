[
  {
    "start": "0",
    "end": "97000"
  },
  {
    "text": "hi everyone i think it's uh time to get started it's 5 25. so welcome everyone to this session",
    "start": "640",
    "end": "7759"
  },
  {
    "text": "building a multi-cluster environment service mesh at airbnb and we have the presenters weibo he and stephen chan my",
    "start": "7759",
    "end": "15200"
  },
  {
    "text": "name is henrik blixt and i'm the moderator so just a few couple of notes before we get started",
    "start": "15200",
    "end": "20880"
  },
  {
    "text": "so we'll do questions at the end i'm gonna be running around with the microphone so once we get to the q a raise your hand i'll run over as quick",
    "start": "20880",
    "end": "27599"
  },
  {
    "text": "as i can with the microphone if there are questions we don't have time to cover",
    "start": "27599",
    "end": "34079"
  },
  {
    "text": "please do that out in the hallway with the presenters so we can clear the room and also lastly don't forget to rate the",
    "start": "34079",
    "end": "42000"
  },
  {
    "text": "session in this get scheduled app when we're done so just quick intros and then we'll",
    "start": "42000",
    "end": "48239"
  },
  {
    "text": "we'll get get going so stephen is passionate about large-scale distributed systems open source",
    "start": "48239",
    "end": "54960"
  },
  {
    "text": "technical leadership and engine excellence and he's currently focused on solving scaling challenges of",
    "start": "54960",
    "end": "60960"
  },
  {
    "text": "infrastructure high-growth companies like airbnb weibo works on building performance",
    "start": "60960",
    "end": "67360"
  },
  {
    "text": "scalable and resilient distributed systems in the cloud and he's currently focused on building the next generation",
    "start": "67360",
    "end": "73040"
  },
  {
    "text": "service mesh at airbnb and will that with that i'll leave the floor to the presenters thank you",
    "start": "73040",
    "end": "80000"
  },
  {
    "text": "thank you welcome everyone my name is weibo and this is stefan we are both engineers",
    "start": "80000",
    "end": "85280"
  },
  {
    "text": "from the cloud foundation team at airbnb today we are very excited to walk you through our experience of building a",
    "start": "85280",
    "end": "92280"
  },
  {
    "text": "multi-cluster multi-environment service mesh on top of istio on airbnb",
    "start": "92280",
    "end": "98400"
  },
  {
    "start": "97000",
    "end": "123000"
  },
  {
    "text": "here's the agenda for today we will start by introducing where we are at in our",
    "start": "98400",
    "end": "105600"
  },
  {
    "text": "service mesh journey then we're going to talk about why we need a multi-cluster service mesh and how we deploy istio to",
    "start": "105600",
    "end": "112479"
  },
  {
    "text": "enable that after that we will cover multi-environment support including multiple tier",
    "start": "112479",
    "end": "118320"
  },
  {
    "text": "mesh expansion and external services and finally we're going to end with key takeaways",
    "start": "118320",
    "end": "124719"
  },
  {
    "text": "in the past few years airbnb like many of our p companies have transitioned",
    "start": "124719",
    "end": "129920"
  },
  {
    "text": "from the monolith architecture to soa at the same time we also migrated the",
    "start": "129920",
    "end": "136560"
  },
  {
    "text": "majority of our workloads from kubernetes from ec2 to kubernetes as we",
    "start": "136560",
    "end": "141680"
  },
  {
    "text": "underwent such fundamental infra changes our legacy in-house service mesh no",
    "start": "141680",
    "end": "147280"
  },
  {
    "text": "longer meets our needs in 2019 we started the search of modern",
    "start": "147280",
    "end": "153120"
  },
  {
    "text": "service mesh and landed on istio as the foundation for more information about this choice",
    "start": "153120",
    "end": "159760"
  },
  {
    "text": "feel free to check out our ecocom talk earlier this year linked on the slide",
    "start": "159760",
    "end": "165760"
  },
  {
    "text": "last year we evaluated different deployment models and deployed issue in",
    "start": "165760",
    "end": "171519"
  },
  {
    "text": "production we started migrating a small percentage of airbnb workloads onto istio and by",
    "start": "171519",
    "end": "178480"
  },
  {
    "text": "end of last year we have about a few dozen services connected to istio and a",
    "start": "178480",
    "end": "183599"
  },
  {
    "text": "few percentage of traffic going through istio proxy this year after successfully adoption on",
    "start": "183599",
    "end": "190560"
  },
  {
    "text": "different kinds of workloads and operating this is still in production for a few quarters",
    "start": "190560",
    "end": "197120"
  },
  {
    "text": "we established enough confidence in both istio itself and our operational expertise in istio we started our full",
    "start": "197120",
    "end": "204080"
  },
  {
    "text": "speed migration to istio currently we have connected almost all of our",
    "start": "204080",
    "end": "209120"
  },
  {
    "text": "thousand micro services to istio and migrated about third of production",
    "start": "209120",
    "end": "214239"
  },
  {
    "text": "traffic onto istio our plan is to fully migrate and sunset",
    "start": "214239",
    "end": "219599"
  },
  {
    "text": "our legacy service mesh next year as we productionize our next generation",
    "start": "219599",
    "end": "224879"
  },
  {
    "text": "service mesh we encountered a few requirements the first requirement is multi-cluster",
    "start": "224879",
    "end": "233840"
  },
  {
    "start": "228000",
    "end": "711000"
  },
  {
    "text": "as airbnb doubled down on kubernetes adoption our kubernetes usage exploded",
    "start": "233840",
    "end": "239439"
  },
  {
    "text": "we started running to scalability issues in scd and api servers",
    "start": "239439",
    "end": "244640"
  },
  {
    "text": "so instead of vertically scaling a single kubernetes clusters we made the decision to horizontally scale out by",
    "start": "244640",
    "end": "251680"
  },
  {
    "text": "distributing workloads across multiple clusters in practice we keep each kubernetes",
    "start": "251680",
    "end": "257359"
  },
  {
    "text": "cluster under a thousand nodes and add more clusters when we need capacity for",
    "start": "257359",
    "end": "262639"
  },
  {
    "text": "more info about the multi-cluster design please check out our 2019 gupta talk",
    "start": "262639",
    "end": "268560"
  },
  {
    "text": "linked below translating this into concrete requirements for our next gen service",
    "start": "268560",
    "end": "274240"
  },
  {
    "text": "mesh we needed to scale for tens of kubernetes clusters with hundreds of",
    "start": "274240",
    "end": "280000"
  },
  {
    "text": "micro services tens of hundred thousands of nodes and hundreds of thousands of",
    "start": "280000",
    "end": "285120"
  },
  {
    "text": "pods how do we partition our workloads across clusters at airbnb we treat clusters as",
    "start": "285120",
    "end": "293520"
  },
  {
    "text": "a pools of compute and memory resources workloads are randomly assigned to clusters at workload",
    "start": "293520",
    "end": "301199"
  },
  {
    "text": "creation time as a result services makes cross-cluster",
    "start": "301199",
    "end": "306400"
  },
  {
    "text": "network requests all the time in this example coaching three class of",
    "start": "306400",
    "end": "311759"
  },
  {
    "text": "cross-cluster network calls were made this simple example pretty much illustrated what's happening in",
    "start": "311759",
    "end": "318080"
  },
  {
    "text": "production it often takes more than ten hubs most of which are cross-clustered to serve a",
    "start": "318080",
    "end": "325919"
  },
  {
    "text": "user request this cross-cluster communication pattern heavily influences our next decision",
    "start": "325919",
    "end": "333919"
  },
  {
    "text": "as services communicate across clusters all the time the added latency of cost of going through a gateway for every hub",
    "start": "334160",
    "end": "341199"
  },
  {
    "text": "it's too expensive for us that's why we adopted a flat network model across",
    "start": "341199",
    "end": "346479"
  },
  {
    "text": "clusters we leveraged aws vpc cni to assign individually addressable vpc ips",
    "start": "346479",
    "end": "353039"
  },
  {
    "text": "to parts so pods can make direct connection across clusters using vpc",
    "start": "353039",
    "end": "359520"
  },
  {
    "text": "cni for cross-cluster communication has a few benefits over our in-house service",
    "start": "359520",
    "end": "364800"
  },
  {
    "text": "mesh with you which utilizes uh kubernetes no port services",
    "start": "364800",
    "end": "370080"
  },
  {
    "text": "vpcni has a lower overhead than no ports which use lots of complex ip table rules",
    "start": "370080",
    "end": "376000"
  },
  {
    "text": "which doesn't scale particularly well for large clusters vpc cni also does not",
    "start": "376000",
    "end": "382400"
  },
  {
    "text": "lead to a well-known particle location problem which is when multiple parts of",
    "start": "382400",
    "end": "387440"
  },
  {
    "text": "the same service are scheduled onto the same node they appear as one address and",
    "start": "387440",
    "end": "392639"
  },
  {
    "text": "gets deduped by envoy so each part on this this node will get less traffic",
    "start": "392639",
    "end": "398800"
  },
  {
    "text": "and finally by providing parts with vpcips we can leverage the reach",
    "start": "398800",
    "end": "403840"
  },
  {
    "text": "functionalities cloud providers built examples include assigning parts custom security groups",
    "start": "403840",
    "end": "411120"
  },
  {
    "text": "flat network does come with its requirements first your organization should not solely",
    "start": "411120",
    "end": "418080"
  },
  {
    "text": "relying on network boundary as the only security measure at airbnb our flat",
    "start": "418080",
    "end": "423440"
  },
  {
    "text": "network is segregated into different security zones with security groups within the flat network we follow a zero",
    "start": "423440",
    "end": "430800"
  },
  {
    "text": "trust network model and um where all workloads are protected by",
    "start": "430800",
    "end": "435919"
  },
  {
    "text": "mtls secondly a flat network requires all your workloads to be using",
    "start": "435919",
    "end": "441520"
  },
  {
    "text": "non-overlapping ips luckily for us this has always been the case we the infra",
    "start": "441520",
    "end": "447280"
  },
  {
    "text": "team centrally managed our private sideart ranges and allocated two slash",
    "start": "447280",
    "end": "452400"
  },
  {
    "text": "10 side up blocks for mesh workloads which can fit 8 million ipv for my ips",
    "start": "452400",
    "end": "458720"
  },
  {
    "text": "and give us enough headroom before we move to ipv6 finally aws has a vpc limit on the",
    "start": "458720",
    "end": "466479"
  },
  {
    "text": "maximum number of ips you can fit into a flat network since vpc cni assigns unique parts",
    "start": "466479",
    "end": "475199"
  },
  {
    "text": "one node with one node with 16 parts consumed 17 ip",
    "start": "475199",
    "end": "480800"
  },
  {
    "text": "mappings which hugely increase the number of ip mappings in our vpc and push us over the limit after close",
    "start": "480800",
    "end": "488160"
  },
  {
    "text": "collaboration with aws we now leverage a new vpc feature called prefix delegation",
    "start": "488160",
    "end": "494879"
  },
  {
    "text": "instead of tracking part ips on the node separately the vpc now assigns a single",
    "start": "494879",
    "end": "500479"
  },
  {
    "text": "slash 28 prefix which fits 16 ips by the way to a node and allocate part ips from",
    "start": "500479",
    "end": "507440"
  },
  {
    "text": "that prefix range which greatly reduce our mapping usage",
    "start": "507440",
    "end": "512960"
  },
  {
    "text": "taking into full consideration of all our multi-cluster requirements we adopted the external control plane and",
    "start": "513599",
    "end": "520320"
  },
  {
    "text": "flat network model for istio deployments which has the following benefits",
    "start": "520320",
    "end": "525760"
  },
  {
    "text": "first it leads to a clean separation of rows we the service mesh team serve as both",
    "start": "525760",
    "end": "531920"
  },
  {
    "text": "the mesh operator and admin we manage the upgrade and release of estio and we",
    "start": "531920",
    "end": "537120"
  },
  {
    "text": "provide the service mesh platform service owners on the other hand are in charge of their managing their own",
    "start": "537120",
    "end": "544000"
  },
  {
    "text": "services with tasks like defining allow us which service allowed to talk to my service and allocating sufficient",
    "start": "544000",
    "end": "550880"
  },
  {
    "text": "resource for their seo proxy secondly external control plan provides",
    "start": "550880",
    "end": "555920"
  },
  {
    "text": "better security we only need to install the ca search issued by airbnb's internal ca system on",
    "start": "555920",
    "end": "562880"
  },
  {
    "text": "the control plane cluster we can enforce a tight access control for the control plane cluster",
    "start": "562880",
    "end": "568880"
  },
  {
    "text": "thirdly um external control plane model provides better isolation from data plane workloads we want to make sure",
    "start": "568880",
    "end": "575600"
  },
  {
    "text": "that no data plane workloads can affect the health of the control plane",
    "start": "575600",
    "end": "581279"
  },
  {
    "text": "finally we perform istio upgrades quarterly it's much easier to operate",
    "start": "581279",
    "end": "586800"
  },
  {
    "text": "one control plane per mesh than end deployment across clusters",
    "start": "586800",
    "end": "592560"
  },
  {
    "text": "a single external control plane does impose a strict requirement on the availability and",
    "start": "592560",
    "end": "599040"
  },
  {
    "text": "reliability of the control plane we will cover in the next section the steps we take to meet that requirement",
    "start": "599040",
    "end": "607519"
  },
  {
    "text": "here's our high-level architectural diagram within a single mesh we deploy istio in a dedicated external",
    "start": "607680",
    "end": "615040"
  },
  {
    "text": "kubernetes cluster it is the blue box in this graph only control plane components",
    "start": "615040",
    "end": "620399"
  },
  {
    "text": "run from this cluster and only the service mesh team has permission to this cluster",
    "start": "620399",
    "end": "626079"
  },
  {
    "text": "all seo custom resources reside in this external cluster",
    "start": "626079",
    "end": "631600"
  },
  {
    "text": "but mesh class mesh users do not directly modify their resources",
    "start": "631600",
    "end": "636959"
  },
  {
    "text": "we provide a simple configuration file that mesh users interact with",
    "start": "636959",
    "end": "642000"
  },
  {
    "text": "they check in this configuration file alongside their service codes and during ci we generate seo custom resources from",
    "start": "642000",
    "end": "649760"
  },
  {
    "text": "that configuration file the generated seo resources are managed",
    "start": "649760",
    "end": "654959"
  },
  {
    "text": "by our deployment system and all config changes are made by a deploy which is",
    "start": "654959",
    "end": "660399"
  },
  {
    "text": "monitored and can be easily rolled back the remote workload cluster",
    "start": "660399",
    "end": "666240"
  },
  {
    "text": "the the pink boxes in this graph only need to define a sidecar injection web",
    "start": "666240",
    "end": "672079"
  },
  {
    "text": "hook which delegates the injection of the istio proxy to the control plan cluster",
    "start": "672079",
    "end": "677760"
  },
  {
    "text": "other than the web hook workload clusters only need to define seo reader row which gives sdod permission to watch",
    "start": "677760",
    "end": "685279"
  },
  {
    "text": "for kubernetes updates how do remote clusters dis discover the",
    "start": "685279",
    "end": "690880"
  },
  {
    "text": "control plane we run a deployment of external dns which on the management cluster sorry",
    "start": "690880",
    "end": "698640"
  },
  {
    "text": "and configure external dns to export ecld as a headless service registered",
    "start": "698640",
    "end": "705440"
  },
  {
    "text": "in route 53 to be used as the discovery address for the workload clusters",
    "start": "705440",
    "end": "712160"
  },
  {
    "start": "711000",
    "end": "948000"
  },
  {
    "text": "in addition to multi-cluster requirements we also have multi-environment requirements for the mesh the first kind of",
    "start": "713120",
    "end": "720000"
  },
  {
    "text": "environment i will cover is multi-tier we rely on a multi-tier mesh to minimize",
    "start": "720000",
    "end": "725440"
  },
  {
    "text": "downtime of the control plane airbnb services are generally divided",
    "start": "725440",
    "end": "731519"
  },
  {
    "text": "into three tiers test staging and production",
    "start": "731519",
    "end": "736800"
  },
  {
    "text": "when building our next generation service mesh we follow a similar concept of tiers to minimize the blast radius of",
    "start": "736800",
    "end": "743600"
  },
  {
    "text": "changes our first tier of mesh deployment is the sandbox tier in this year we run",
    "start": "743600",
    "end": "749760"
  },
  {
    "text": "functional and performance tests to validate the new release of issio and isoproxy",
    "start": "749760",
    "end": "755760"
  },
  {
    "text": "after the new release is verified on the sandbox tier we deployed it to the test",
    "start": "755760",
    "end": "760800"
  },
  {
    "text": "here test mesh has identical setup as the production mesh but runs in a",
    "start": "760800",
    "end": "765920"
  },
  {
    "text": "separate aws account for maximum isolation and finally we deploy a new issue",
    "start": "765920",
    "end": "771760"
  },
  {
    "text": "release to the production mesh both staging and production tier of services connect to production mesh",
    "start": "771760",
    "end": "779360"
  },
  {
    "text": "in sandbox mesh we run automated functional tests to verify mesh features that we depend on in production like",
    "start": "779360",
    "end": "786480"
  },
  {
    "text": "authorization policy or locality based load balancing additionally we define",
    "start": "786480",
    "end": "792160"
  },
  {
    "text": "regression tests for issues we found in the past for instance we encountered a bug where it still",
    "start": "792160",
    "end": "798480"
  },
  {
    "text": "would get stuck if a large deployment was rapidly scaled down we automated the",
    "start": "798480",
    "end": "803920"
  },
  {
    "text": "repro of such regressions and make sure that issue continued to be fixed for",
    "start": "803920",
    "end": "809040"
  },
  {
    "text": "future releases we also run data plane performance tests with our specific configuration of istio",
    "start": "809040",
    "end": "816880"
  },
  {
    "text": "proxy using nighthawk in test mesh we test every single",
    "start": "816880",
    "end": "823279"
  },
  {
    "text": "integration point with airbnb systems and run end-to-end integration tests for instance we test that eco can secure can",
    "start": "823279",
    "end": "831519"
  },
  {
    "text": "securely mount the ca search from our internal ca system we test new",
    "start": "831519",
    "end": "836560"
  },
  {
    "text": "kubernetes version release won't break the service mesh we test changes to the",
    "start": "836560",
    "end": "842480"
  },
  {
    "text": "resource generation logic and finally we can talk test all the custom logic we built for mesh expansion and external",
    "start": "842480",
    "end": "849839"
  },
  {
    "text": "services we have successfully caught regressions in both istio itself",
    "start": "849839",
    "end": "856560"
  },
  {
    "text": "and in related airbnb systems in test mesh",
    "start": "856560",
    "end": "861839"
  },
  {
    "text": "after verifying the safety of the new release on the standalone mesh and test mesh we deployed the new eco to a",
    "start": "862000",
    "end": "869279"
  },
  {
    "text": "production mesh with a revision label the initial deploy was safe as there will be no workloads connected to the",
    "start": "869279",
    "end": "876079"
  },
  {
    "text": "new version we gradually increase the scope of services connected to the new version as we build confidence",
    "start": "876079",
    "end": "883279"
  },
  {
    "text": "note that services only connect to the new version after a deploy",
    "start": "883279",
    "end": "888639"
  },
  {
    "text": "at any time if there is any regression service owners can simply roll back the deployment to pick up the old version",
    "start": "888639",
    "end": "895920"
  },
  {
    "text": "the entire rollout process takes about a month after there's no more proxies connected",
    "start": "895920",
    "end": "901839"
  },
  {
    "text": "to the old version of the control plane we will clean up the old version",
    "start": "901839",
    "end": "907440"
  },
  {
    "text": "this rollout process allows us to slowly increase the number of proxies connected to old version as a matter of fact we",
    "start": "908720",
    "end": "915760"
  },
  {
    "text": "have just finished up roll out of eco 1.10 and teardown of 1.9 one month ago",
    "start": "915760",
    "end": "922399"
  },
  {
    "text": "this graph shows you a gradual increase of proxies connected to 1.10 versus 1.9",
    "start": "922399",
    "end": "928800"
  },
  {
    "text": "we can also side by side compare the new version against the old version this graph shows you the impact of how a new",
    "start": "928800",
    "end": "936800"
  },
  {
    "text": "feature called discovery selector introduced in 1.10 reduced the cpu usage of istio",
    "start": "936800",
    "end": "944000"
  },
  {
    "text": "when we excluded some of our most noisy namespaces",
    "start": "944000",
    "end": "949279"
  },
  {
    "text": "now i'm going to hand over to stefan and to talk about the rest of multi environments",
    "start": "949279",
    "end": "956160"
  },
  {
    "text": "thanks huevo so we've covered how our service mesh spans multiple kubernetes clusters",
    "start": "958320",
    "end": "964320"
  },
  {
    "text": "and multiple environments so now we're going to deep dive into two non-kubernetes use cases",
    "start": "964320",
    "end": "969759"
  },
  {
    "text": "first we're going to talk about how we support workloads running on uncontainerized ec2 which internally we call mesh expansion",
    "start": "969759",
    "end": "976240"
  },
  {
    "text": "as well as external services like managed data stores",
    "start": "976240",
    "end": "982000"
  },
  {
    "text": "so here's some context about why we wanted to support ec2 on istio so in the long term we want",
    "start": "982000",
    "end": "989040"
  },
  {
    "text": "to run all of our internal services on kubernetes however in the shorter to medium term we want all the benefits of",
    "start": "989040",
    "end": "996079"
  },
  {
    "text": "a single uniform service mesh running across all workloads and the workloads that are not already",
    "start": "996079",
    "end": "1002320"
  },
  {
    "text": "running on kubernetes will take years to either migrate or deprecate these are either legacy or very stateful",
    "start": "1002320",
    "end": "1009440"
  },
  {
    "text": "workloads so the solution that we came up with was to support ec2 on our",
    "start": "1009440",
    "end": "1014480"
  },
  {
    "text": "service mesh the requirements are first we want to have full feature",
    "start": "1014480",
    "end": "1020079"
  },
  {
    "text": "parity with kubernetes so service owners can reuse their knowledge of how to work with service",
    "start": "1020079",
    "end": "1025760"
  },
  {
    "text": "mesh on both ec2 and kubernetes and secondly we want to allow gradual traffic shifting onto kubernetes when",
    "start": "1025760",
    "end": "1031839"
  },
  {
    "text": "services are ready to migrate so let's go back and revisit we both",
    "start": "1031839",
    "end": "1037520"
  },
  {
    "text": "slide on kubernetes architecture this diagram adds a high level overview",
    "start": "1037520",
    "end": "1042880"
  },
  {
    "text": "of how ec2 workloads fit in so the orange box on the bottom right shows ec2",
    "start": "1042880",
    "end": "1048400"
  },
  {
    "text": "workloads as part of the mesh now that we've covered the high level architecture we'll dive into the",
    "start": "1048400",
    "end": "1053840"
  },
  {
    "text": "mechanics of how we got to feature parity for ec2 first we'll talk a little bit about how",
    "start": "1053840",
    "end": "1059440"
  },
  {
    "text": "endpoint registration evolved in kubernetes the endpoints controller automatically handles this by updating",
    "start": "1059440",
    "end": "1066400"
  },
  {
    "text": "endpoints resources based on pod readiness the resources then get watched by sdod",
    "start": "1066400",
    "end": "1071840"
  },
  {
    "text": "and distributed to clients when we first decided to adopt sdo we had to manually tell the istio control",
    "start": "1071840",
    "end": "1078080"
  },
  {
    "text": "plane which ec2 ips belong to which services by hand editing configs",
    "start": "1078080",
    "end": "1083520"
  },
  {
    "text": "due to the number of ec2 services in the instance change update rate this added unacceptable additional operational load",
    "start": "1083520",
    "end": "1090720"
  },
  {
    "text": "over the past year we've worked with the istio community to define our requirements the solution the community built is a",
    "start": "1090720",
    "end": "1096799"
  },
  {
    "text": "feature called auto registration so the seod data the istio data plane has two",
    "start": "1096799",
    "end": "1104000"
  },
  {
    "text": "processes which run on all service instances whether ec2 or kubernetes",
    "start": "1104000",
    "end": "1109440"
  },
  {
    "text": "these are pilot agent and envoy when envoy starts up it will connect via xds to pilot agent which proxies the",
    "start": "1109440",
    "end": "1115520"
  },
  {
    "text": "connection pilot agent will then connect to the sdod control plane and provide metadata",
    "start": "1115520",
    "end": "1121200"
  },
  {
    "text": "about which service it belongs to using that xds connection sdod will create a kubernetes crd called",
    "start": "1121200",
    "end": "1128160"
  },
  {
    "text": "workload entry on behalf of the connecting pilot agent process and then the workload entry is what registers the",
    "start": "1128160",
    "end": "1134160"
  },
  {
    "text": "service instance in the mesh other sdod pods will read workload entry",
    "start": "1134160",
    "end": "1139679"
  },
  {
    "text": "and distribute that service instance to other clients server-side health checks were also",
    "start": "1139679",
    "end": "1145440"
  },
  {
    "text": "implemented by seo community in the past year the health check process begins in the same way as auto registration",
    "start": "1145440",
    "end": "1152160"
  },
  {
    "text": "first pilot agent proxies xds for envoy pilot agent then does health checks which is similar to how cubelet does",
    "start": "1152160",
    "end": "1158640"
  },
  {
    "text": "health checks in kubernetes pilot agent will then send health",
    "start": "1158640",
    "end": "1164000"
  },
  {
    "text": "information across the same single xds connection to an sdot control plane pod",
    "start": "1164000",
    "end": "1169360"
  },
  {
    "text": "and seod will alter the healthy status of autoregistered workload entry",
    "start": "1169360",
    "end": "1174640"
  },
  {
    "text": "other sdod pods will read and react accordingly adding the endpoint if the workload entry transitioned to healthy",
    "start": "1174640",
    "end": "1180240"
  },
  {
    "text": "and removing it if it transitioned to unhealthy the tls story is a little bit more",
    "start": "1180240",
    "end": "1185440"
  },
  {
    "text": "complex so istio community plans some automation around getting certificates but the timeline is unknown so we built",
    "start": "1185440",
    "end": "1191360"
  },
  {
    "text": "our own automation there are two tasks needed to boost drop tls first we need to get the root ca",
    "start": "1191360",
    "end": "1198559"
  },
  {
    "text": "and second we need to provide proof the ca to get a workload certificate with a spiffy sand",
    "start": "1198559",
    "end": "1205279"
  },
  {
    "text": "sdod starts by writing the root ca as a config map in the kubernetes api",
    "start": "1205360",
    "end": "1211279"
  },
  {
    "text": "we built an in-house tool called mxagent which creates a service account token",
    "start": "1211600",
    "end": "1216720"
  },
  {
    "text": "and reads the ca config map that sdod wrote by calling the kubernetes api",
    "start": "1216720",
    "end": "1223039"
  },
  {
    "text": "mx agent then writes the ci cert and token to the local file system on the vm instance",
    "start": "1223039",
    "end": "1229440"
  },
  {
    "text": "when pilot agent starts up it will read the ca and token from the file system",
    "start": "1229440",
    "end": "1234559"
  },
  {
    "text": "and then issue a csr to sdod finally envoy will get its certificates",
    "start": "1234559",
    "end": "1240640"
  },
  {
    "text": "from pilot agent using sds",
    "start": "1240640",
    "end": "1244559"
  },
  {
    "text": "seo version upgrades are also slightly complex so let's backtrack and quickly review how istio versions are chosen in",
    "start": "1245840",
    "end": "1251600"
  },
  {
    "text": "kubernetes so in kubernetes we use an internal tool called crispr which runs as an admission",
    "start": "1251600",
    "end": "1257760"
  },
  {
    "text": "controller it reads a configuration file called rollout.yml which maps kubernetes",
    "start": "1257760",
    "end": "1263440"
  },
  {
    "text": "namespaces to mutator artifact versions and we wrote a blog post about crispr which is linked in the bottom of the",
    "start": "1263440",
    "end": "1270000"
  },
  {
    "text": "slide crispr downloads the mutated artifact",
    "start": "1270000",
    "end": "1275120"
  },
  {
    "text": "from storage and then patches the pod label with an istio version label",
    "start": "1275120",
    "end": "1281760"
  },
  {
    "text": "seod then gets called as another mutating admission controller which does the main sidecar injection",
    "start": "1282159",
    "end": "1288159"
  },
  {
    "text": "and the sdod version called is determined by the istio revision label",
    "start": "1288159",
    "end": "1294720"
  },
  {
    "text": "crispr is designed with the kubernetes use case in mind so we couldn't reuse this for ec2 however we took key lessons",
    "start": "1295280",
    "end": "1301200"
  },
  {
    "text": "from crispr and created a very similar interface for ec2 we built a parallel system",
    "start": "1301200",
    "end": "1308240"
  },
  {
    "text": "first there's a cli named mx rollout which reads a rollout file similar to crispr's rollouts configuration format",
    "start": "1308240",
    "end": "1316559"
  },
  {
    "text": "mx rollout will then update an ec2 tag named desired version on a particular",
    "start": "1316559",
    "end": "1321919"
  },
  {
    "text": "instance when mx agent starts up it will read the desired version tag value",
    "start": "1321919",
    "end": "1327919"
  },
  {
    "text": "and this tag value maps to an artifact which contains pilot agent and envoy binaries as well as",
    "start": "1327919",
    "end": "1333039"
  },
  {
    "text": "other configuration mx agent will download this artifact unpack it and restart the pilot agent",
    "start": "1333039",
    "end": "1339280"
  },
  {
    "text": "service running on the host finally mx agent updates the value of the",
    "start": "1339280",
    "end": "1345600"
  },
  {
    "text": "running version tag on the instance here's a high level overview of",
    "start": "1345600",
    "end": "1351039"
  },
  {
    "text": "christopher and our mx updating system side by side there are several similarities that you can see between",
    "start": "1351039",
    "end": "1358480"
  },
  {
    "text": "them okay let's switch tracks and talk about how we supported external services",
    "start": "1358840",
    "end": "1364400"
  },
  {
    "start": "1359000",
    "end": "1519000"
  },
  {
    "text": "most of our external services used custom protocols like mysql and redis so when scoping",
    "start": "1364400",
    "end": "1370880"
  },
  {
    "text": "solutions we grouped external services and non-http services together",
    "start": "1370880",
    "end": "1376960"
  },
  {
    "text": "external services can't run istio data plane components so we have a slightly different set of requirements compared",
    "start": "1377919",
    "end": "1383600"
  },
  {
    "text": "to supporting ec2 first non-http support was important",
    "start": "1383600",
    "end": "1389200"
  },
  {
    "text": "we wanted to allow service owners to define custom redis pings or mysql health checks",
    "start": "1389200",
    "end": "1394640"
  },
  {
    "text": "secondly we wanted to use dns to map external service urls to",
    "start": "1394640",
    "end": "1400320"
  },
  {
    "text": "a unique kubernetes cluster ip external services are registered in istio using service entry but without an",
    "start": "1400320",
    "end": "1407120"
  },
  {
    "text": "ip field service entry results in an envoy listener that takes up all ips on a",
    "start": "1407120",
    "end": "1412320"
  },
  {
    "text": "given port we're moving off of a service discovery system that relies on port allocation",
    "start": "1412320",
    "end": "1418159"
  },
  {
    "text": "when we want to avoid this kind of setup wherever possible and finally we want to remove stale",
    "start": "1418159",
    "end": "1423600"
  },
  {
    "text": "endpoints for example when database replicas are torn down",
    "start": "1423600",
    "end": "1429640"
  },
  {
    "text": "so here's how we solved server-side health checks we built a generic health check sidecar called mhcx",
    "start": "1430000",
    "end": "1437919"
  },
  {
    "text": "airbnb internal owners will build protocol specific health checks into an application container and then run mhcx",
    "start": "1437919",
    "end": "1444799"
  },
  {
    "text": "as a sidecar mrcx is responsible for translating healthcheck results into workload entry",
    "start": "1444799",
    "end": "1452159"
  },
  {
    "text": "updates effectively doing the same thing that sdod was doing with vm health",
    "start": "1452159",
    "end": "1457279"
  },
  {
    "text": "checking and then the app health checker main container is responsible for",
    "start": "1457279",
    "end": "1464080"
  },
  {
    "text": "querying external endpoints using the custom protocols and providing an http response to mhcx",
    "start": "1464080",
    "end": "1472039"
  },
  {
    "text": "for our dns cluster ip setup and endpoint removal",
    "start": "1473919",
    "end": "1479279"
  },
  {
    "text": "we have two parts so first for the dns cluster ip setup under",
    "start": "1479279",
    "end": "1484320"
  },
  {
    "text": "the hood we create both a kubernetes service and a service entry crd",
    "start": "1484320",
    "end": "1490080"
  },
  {
    "text": "we ensure that the service is created first before the service entry",
    "start": "1490080",
    "end": "1495840"
  },
  {
    "text": "and then when a new service is created when the new service entry is created we",
    "start": "1495840",
    "end": "1500880"
  },
  {
    "text": "add a mutating admission controller which patches the service entry uh ip with the cluster ip of the generated",
    "start": "1500880",
    "end": "1507520"
  },
  {
    "text": "service and for stale endpoint removal we rely on spinnaker to run a cleanup command",
    "start": "1507520",
    "end": "1513520"
  },
  {
    "text": "which is similar to keep ctl print",
    "start": "1513520",
    "end": "1517520"
  },
  {
    "start": "1519000",
    "end": "1586000"
  },
  {
    "text": "now that we've covered several environments in which we run seo let's cover the key concepts which we",
    "start": "1519520",
    "end": "1525760"
  },
  {
    "text": "use to successfully run it first",
    "start": "1525760",
    "end": "1531279"
  },
  {
    "text": "we ran a globally flat network but be aware that there are considerations around utilization and scalability of",
    "start": "1531279",
    "end": "1537679"
  },
  {
    "text": "this approach secondly we use a single external management",
    "start": "1537679",
    "end": "1543039"
  },
  {
    "text": "cluster for each service mesh this is important for secure and simple management of each mesh",
    "start": "1543039",
    "end": "1550080"
  },
  {
    "text": "third we use multiple tiers of pre-production meshes which allow for different types of testing and ensure we catch as many",
    "start": "1550080",
    "end": "1556880"
  },
  {
    "text": "issues as possible before production fourth istio support for vms has matured",
    "start": "1556880",
    "end": "1563600"
  },
  {
    "text": "considerably in the past year we built additional tooling to reach full feature parity and allow a",
    "start": "1563600",
    "end": "1569840"
  },
  {
    "text": "long-term migration path to kubernetes and finally we have generic mechanisms for",
    "start": "1569840",
    "end": "1575440"
  },
  {
    "text": "supporting non-http and external services which ensures the service mesh team does",
    "start": "1575440",
    "end": "1580720"
  },
  {
    "text": "not have to implement any different envoy filters for n different protocols",
    "start": "1580720",
    "end": "1586240"
  },
  {
    "start": "1586000",
    "end": "1762000"
  },
  {
    "text": "all right finally airbnb is hiring so please check out our website for open positions",
    "start": "1586240",
    "end": "1592640"
  },
  {
    "text": "and now i think we have a few minutes for questions",
    "start": "1592640",
    "end": "1598120"
  },
  {
    "text": "thanks looks like we have about eight minutes left for questions so raise your hand i'll try and run around with the",
    "start": "1603679",
    "end": "1608720"
  },
  {
    "text": "microphone starting up front here uh hi that was a great presentation you",
    "start": "1608720",
    "end": "1614640"
  },
  {
    "text": "had a lot of things packed in there one question uh when it comes to",
    "start": "1614640",
    "end": "1619919"
  },
  {
    "text": "a lot of services in the cluster that istio has to look upon so",
    "start": "1619919",
    "end": "1626880"
  },
  {
    "text": "i know istio gets a lot of charity uh chattiness how did you manage that",
    "start": "1626880",
    "end": "1634960"
  },
  {
    "text": "yeah so istio has a sidecar crd that filters out the listeners the clusters",
    "start": "1637039",
    "end": "1643120"
  },
  {
    "text": "that you get as part of our simple configuration file that we",
    "start": "1643120",
    "end": "1648480"
  },
  {
    "text": "we ask service owners to provide we like force everyone to define the exact set",
    "start": "1648480",
    "end": "1655200"
  },
  {
    "text": "of uh other services that they care about so that way it's greatly reduced the number of um like unnecessary",
    "start": "1655200",
    "end": "1662720"
  },
  {
    "text": "updates to each service",
    "start": "1662720",
    "end": "1666440"
  },
  {
    "text": "a ton of services and different names becomes",
    "start": "1678080",
    "end": "1683120"
  },
  {
    "text": "totally understood we tested uh seo without uh using like",
    "start": "1683120",
    "end": "1688399"
  },
  {
    "text": "that site card crd and the performance it's hard to scale for like tens of",
    "start": "1688399",
    "end": "1693440"
  },
  {
    "text": "thousands of thousands thousands of services",
    "start": "1693440",
    "end": "1698240"
  },
  {
    "text": "again i was more curious because our environment looks a lot like your guys's",
    "start": "1700000",
    "end": "1705520"
  },
  {
    "text": "um have you tried the multi-cluster mode in istio and not use the service entry",
    "start": "1705520",
    "end": "1712159"
  },
  {
    "text": "uh can can you provide a little more",
    "start": "1712159",
    "end": "1717279"
  },
  {
    "text": "information about like how the service and how multi-cluster would um yeah so the newer versions of seo you",
    "start": "1717279",
    "end": "1724000"
  },
  {
    "text": "can define different you can call your um different seo setups different names",
    "start": "1724000",
    "end": "1729360"
  },
  {
    "text": "um and then basically it talks to the kubernetes api um that is trying to access and then the",
    "start": "1729360",
    "end": "1735440"
  },
  {
    "text": "the service endpoints are advertised everywhere so you don't have to create service entries",
    "start": "1735440",
    "end": "1741360"
  },
  {
    "text": "there all clusters know about all the endpoints oh i see so",
    "start": "1741360",
    "end": "1746559"
  },
  {
    "text": "we are allowing sdod to read from multiple clusters for service entry we use that",
    "start": "1746559",
    "end": "1753440"
  },
  {
    "text": "exclusively for external services so like services that we aren't running in-house",
    "start": "1753440",
    "end": "1760320"
  },
  {
    "text": "so yeah if we go back to the architecture diagram we are running multi-cluster mode so the",
    "start": "1761520",
    "end": "1769279"
  },
  {
    "text": "centralized seod subscribe to different workload",
    "start": "1769279",
    "end": "1774880"
  },
  {
    "text": "kubernetes clusters and for kubernetes workloads they use directly use kubernetes services and kubernetes",
    "start": "1774880",
    "end": "1781039"
  },
  {
    "text": "endpoints and uh we don't define the service entry it's only for non-kubernetes like vm",
    "start": "1781039",
    "end": "1786320"
  },
  {
    "text": "workflows and external services we define service entry because there's no pods okay",
    "start": "1786320",
    "end": "1792000"
  },
  {
    "text": "and can i ask one more question just curious um because we again we run into this problem do you guys use the helm",
    "start": "1792000",
    "end": "1797279"
  },
  {
    "text": "chart or use that the profile is your profile file the crd uh we use the operator to generate uh",
    "start": "1797279",
    "end": "1806000"
  },
  {
    "text": "the the basically raw manifest and check them in into the source code",
    "start": "1806000",
    "end": "1811520"
  },
  {
    "text": "so we know what like uh we we feel uncomfortable uh",
    "start": "1811520",
    "end": "1817039"
  },
  {
    "text": "asking it operator to handle all the upgrades we would like to know what exactly is going on with each eco",
    "start": "1817039",
    "end": "1823200"
  },
  {
    "text": "release manifest yes",
    "start": "1823200",
    "end": "1829600"
  },
  {
    "text": "um yes where does the operator that uh",
    "start": "1834840",
    "end": "1841440"
  },
  {
    "text": "injects the sidecar lid is it in the control plane cluster or is it running in each individual cluster",
    "start": "1841440",
    "end": "1848559"
  },
  {
    "text": "so the question is where does the sidecar injector live",
    "start": "1848559",
    "end": "1853600"
  },
  {
    "text": "the web hook live on every workload cluster but the mutator service the ecod",
    "start": "1853600",
    "end": "1860240"
  },
  {
    "text": "only runs from the management cluster the external cluster so there will be a",
    "start": "1860240",
    "end": "1865519"
  },
  {
    "text": "per workload cluster web hook that basically points uh to the central uh ecld address",
    "start": "1865519",
    "end": "1873120"
  },
  {
    "text": "since you bring up so many clusters dynamically do you ever hit any weird race conditions where the web hook is",
    "start": "1873120",
    "end": "1878320"
  },
  {
    "text": "maybe not up before the application caught and then your sidecar isn't alive or something like that",
    "start": "1878320",
    "end": "1883440"
  },
  {
    "text": "so um yeah we run uh like",
    "start": "1883440",
    "end": "1889039"
  },
  {
    "text": "verification workloads on each cluster before we mark it as schedulable basically before we can assign workloads",
    "start": "1889039",
    "end": "1895440"
  },
  {
    "text": "to that cluster um and our rate of cluster creation is",
    "start": "1895440",
    "end": "1901760"
  },
  {
    "text": "like more on that like order of hours rather than like minutes",
    "start": "1901760",
    "end": "1906799"
  },
  {
    "text": "or something so we usually go through a verification process",
    "start": "1906799",
    "end": "1912399"
  },
  {
    "text": "yes correct yeah",
    "start": "1916960",
    "end": "1920760"
  },
  {
    "text": "uh one one of the most painful things uh we face when we upgrade hto uh version",
    "start": "1922559",
    "end": "1927600"
  },
  {
    "text": "right is the envy filters because the apis break really easily it is super painful how do you manage",
    "start": "1927600",
    "end": "1934000"
  },
  {
    "text": "that in airbnb",
    "start": "1934000",
    "end": "1937120"
  },
  {
    "text": "i mean yeah yeah envoy we we try to",
    "start": "1940640",
    "end": "1946799"
  },
  {
    "text": "keep our envoy filters fairly portable like we've written two that one that customizes",
    "start": "1947519",
    "end": "1955039"
  },
  {
    "text": "logging slightly and one that customizes metrics to use like a",
    "start": "1955039",
    "end": "1960159"
  },
  {
    "text": "native envoy filter but um definitely",
    "start": "1960159",
    "end": "1965200"
  },
  {
    "text": "agree that um there are some pain points with with upgrading like because the",
    "start": "1965200",
    "end": "1971039"
  },
  {
    "text": "like internals of like envoy configuration are unstable so we just try to keep it as simple as possible",
    "start": "1971039",
    "end": "1977120"
  },
  {
    "text": "there yeah we definitely felt a pain point",
    "start": "1977120",
    "end": "1982320"
  },
  {
    "text": "hi yeah i had a question um you mentioned earlier that you noticed that you had a bug where",
    "start": "1983360",
    "end": "1989039"
  },
  {
    "text": "istio was getting stuck on rapid scale down of deployments i was wondering if you could provide a little more details",
    "start": "1989039",
    "end": "1994640"
  },
  {
    "text": "and like where was istio getting stuck in how you fix that issue",
    "start": "1994640",
    "end": "2000960"
  },
  {
    "text": "yeah um that particular issue uh like i can't send you the get the issue",
    "start": "2000960",
    "end": "2007120"
  },
  {
    "text": "offline i it's it happens a while ago um i think um what we observe it's when uh large",
    "start": "2007120",
    "end": "2013919"
  },
  {
    "text": "deployments rapidly scale down and sld will basically",
    "start": "2013919",
    "end": "2020799"
  },
  {
    "text": "fail to send out any subsequent updates um like more details can be found in the",
    "start": "2020799",
    "end": "2027440"
  },
  {
    "text": "github issue i don't remember any more details right now sorry",
    "start": "2027440",
    "end": "2033600"
  },
  {
    "text": "the the issue that we ran into was i think in 1.5 or 1.6 so",
    "start": "2033600",
    "end": "2039120"
  },
  {
    "text": "several minor releases ago and we haven't observed it since",
    "start": "2039120",
    "end": "2044559"
  },
  {
    "text": "so when you have two over here when you have two clusters um",
    "start": "2045120",
    "end": "2050320"
  },
  {
    "text": "can uh uh when services in one cluster can they talk to services the other cluster",
    "start": "2050320",
    "end": "2056398"
  },
  {
    "text": "without like changing anything about the service definition itself was uh like",
    "start": "2056399",
    "end": "2062158"
  },
  {
    "text": "so the services themselves wouldn't know which cluster pods they're talking to or",
    "start": "2062159",
    "end": "2067599"
  },
  {
    "text": "you have to like specify which cluster you want to talk to when you",
    "start": "2067599",
    "end": "2073040"
  },
  {
    "text": "so when services services when when they're making cross-cluster network costs they are unaware of the",
    "start": "2073040",
    "end": "2079679"
  },
  {
    "text": "destination cluster so um we define a mesh id like",
    "start": "2079679",
    "end": "2085358"
  },
  {
    "text": "us depending on the region ea one us and that's shared",
    "start": "2085359",
    "end": "2090560"
  },
  {
    "text": "across clusters so from the perspective of a service it doesn't know where the",
    "start": "2090560",
    "end": "2095919"
  },
  {
    "text": "destination service runs from as a matter of fact that service can run from multiple different clusters and sdld is",
    "start": "2095919",
    "end": "2103040"
  },
  {
    "text": "going to combine the endpoints of like fetch the endpoints and combine the endpoints uh group the endpoints under",
    "start": "2103040",
    "end": "2109520"
  },
  {
    "text": "the same namespace and so requests can be load balanced across clusters",
    "start": "2109520",
    "end": "2116400"
  },
  {
    "text": "so i think we're we're at time so so thanks thanks er we can uh continue",
    "start": "2116400",
    "end": "2121760"
  },
  {
    "text": "uh questions outside in the hallway so we can clear the room so they can close down so thanks everyone for coming",
    "start": "2121760",
    "end": "2127599"
  },
  {
    "text": "appreciate it don't forget to rate the session afterwards in the in the app",
    "start": "2127599",
    "end": "2133520"
  }
]