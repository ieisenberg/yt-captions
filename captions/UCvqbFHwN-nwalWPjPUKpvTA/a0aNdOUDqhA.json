[
  {
    "text": "uh good afternoon thanks for coming this session is about",
    "start": "0",
    "end": "5819"
  },
  {
    "text": "availability and storage Auto scaling of stateful workloads running on kubernetes",
    "start": "5819",
    "end": "10920"
  },
  {
    "text": "my name is Leila I work at Shopify as an infrastructure engineer I'm on a team of",
    "start": "10920",
    "end": "17580"
  },
  {
    "text": "10 Engineers called the search platform who develop and maintain the infrastructure that powers search at",
    "start": "17580",
    "end": "24359"
  },
  {
    "text": "Shopify this is the agenda for this talk in the",
    "start": "24359",
    "end": "31320"
  },
  {
    "text": "next 20 minutes or so we'll learn how the search infrastructure team at",
    "start": "31320",
    "end": "36420"
  },
  {
    "text": "Shopify hosts a stateful workload which is searched on top of kubernetes",
    "start": "36420",
    "end": "42559"
  },
  {
    "text": "we'll talk about the obstacles of providing High availability and storage",
    "start": "42559",
    "end": "47879"
  },
  {
    "text": "Auto scaling and scaling in general for stateful workloads that run on",
    "start": "47879",
    "end": "54000"
  },
  {
    "text": "kubernetes and how my team address those challenges and I also have a",
    "start": "54000",
    "end": "60059"
  },
  {
    "text": "pre-recorded demo at the end and will conclude the presentation with a q a",
    "start": "60059",
    "end": "67100"
  },
  {
    "text": "so a little bit about Shopify for those who don't know about us Shopify is a",
    "start": "67380",
    "end": "73799"
  },
  {
    "text": "cloud-based Commerce platform that lets you start and manage a business by",
    "start": "73799",
    "end": "79260"
  },
  {
    "text": "allowing you to create and customize an online store and manage inventory",
    "start": "79260",
    "end": "85020"
  },
  {
    "text": "payments and Etc currently we have more than 3 million",
    "start": "85020",
    "end": "90240"
  },
  {
    "text": "Merchants that sell a lot of products with us and they collectively have sold",
    "start": "90240",
    "end": "95939"
  },
  {
    "text": "over 700 billion dollars in Gross merchandise volume",
    "start": "95939",
    "end": "102000"
  },
  {
    "text": "so just to give a better sense of the scale only during the Black Friday Cyber",
    "start": "102000",
    "end": "108720"
  },
  {
    "text": "Monday weekend of 2022 which is a high like which is a major high volume",
    "start": "108720",
    "end": "114780"
  },
  {
    "text": "Commerce event that kicks off the uh holiday shopping season our Merchants",
    "start": "114780",
    "end": "120060"
  },
  {
    "text": "sold over seven billion dollars in GMB search is a fundamental part of any",
    "start": "120060",
    "end": "127740"
  },
  {
    "text": "Commerce platform that allows buyers to search and filter for the products that they're looking for it also allows",
    "start": "127740",
    "end": "134280"
  },
  {
    "text": "Merchants to fulfill the orders that they have received when you go to any online Store and search for a certain",
    "start": "134280",
    "end": "141420"
  },
  {
    "text": "product your request goes to a search engine that's backed by a secondary data",
    "start": "141420",
    "end": "146760"
  },
  {
    "text": "store which is different from traditional databases uh we call this",
    "start": "146760",
    "end": "151800"
  },
  {
    "text": "secondary data store the search infrastructure at Shopify we use elasticsearch to",
    "start": "151800",
    "end": "159660"
  },
  {
    "text": "provide search and filtering services to Shopify core which Powers all of our",
    "start": "159660",
    "end": "165420"
  },
  {
    "text": "Merchant shops as well as their storefronts as well as the development",
    "start": "165420",
    "end": "170819"
  },
  {
    "text": "teams who need search services we run shopify's search on top of",
    "start": "170819",
    "end": "176760"
  },
  {
    "text": "kubernetes on Google Cloud platform and deploy and maintain these elasticsearch",
    "start": "176760",
    "end": "182700"
  },
  {
    "text": "instances by using a custom kubernetes controller that we have built",
    "start": "182700",
    "end": "189319"
  },
  {
    "text": "so let's catch up on some definitions stateful Services unlike stateless ones",
    "start": "189540",
    "end": "196220"
  },
  {
    "text": "rely on persistent data and respond to the same inputs in different ways",
    "start": "196220",
    "end": "201480"
  },
  {
    "text": "depending on their history according to this definition elasticsearch is considered a stateful",
    "start": "201480",
    "end": "208500"
  },
  {
    "text": "application that provides a stateful service and therefore we deploy it using",
    "start": "208500",
    "end": "213720"
  },
  {
    "text": "kubernetes stateful stats now let's see what state full sets are a",
    "start": "213720",
    "end": "220319"
  },
  {
    "text": "stateful set is a basically a controller that manages pods that are based on an",
    "start": "220319",
    "end": "228000"
  },
  {
    "text": "identical container spec and maintains a sticky identity for all of its its parts",
    "start": "228000",
    "end": "234959"
  },
  {
    "text": "these pods are created from the same spec but are not interchangeable each",
    "start": "234959",
    "end": "240840"
  },
  {
    "text": "has a persistent identifier and more importantly has a persistent disk that",
    "start": "240840",
    "end": "246420"
  },
  {
    "text": "it claims across any rescheduling",
    "start": "246420",
    "end": "251060"
  },
  {
    "text": "to have a again a better sense of the scale and elasticsearch stateful set",
    "start": "253080",
    "end": "258199"
  },
  {
    "text": "that stores search data for Shopify core is composed of 120 pods each of them",
    "start": "258199",
    "end": "266160"
  },
  {
    "text": "having a four tbite persistent disk for high availability reasons we have",
    "start": "266160",
    "end": "272460"
  },
  {
    "text": "multiple instances of elastic searches with the size deployed across the globe",
    "start": "272460",
    "end": "280580"
  },
  {
    "text": "thinking of availability with search being a critical service we need to make",
    "start": "280680",
    "end": "286620"
  },
  {
    "text": "sure that it's always available meaning that failures of PODS systems or even",
    "start": "286620",
    "end": "292199"
  },
  {
    "text": "natural disasters that lead to Regional failures should have minimal impact on",
    "start": "292199",
    "end": "298320"
  },
  {
    "text": "the service we also need to make sure that elasticsearch Stay full sets always have",
    "start": "298320",
    "end": "304320"
  },
  {
    "text": "enough storage space and they're able to adapt to sudden changes of storage",
    "start": "304320",
    "end": "309600"
  },
  {
    "text": "requirements the first question we want to answer",
    "start": "309600",
    "end": "315000"
  },
  {
    "text": "here is whether deploying a stateful service with kubernetes automatically",
    "start": "315000",
    "end": "320400"
  },
  {
    "text": "guarantees fault tolerance and high availability we know that the main step",
    "start": "320400",
    "end": "326160"
  },
  {
    "text": "towards availability and fault tolerance is redundancy one might think that since kubernetes",
    "start": "326160",
    "end": "333180"
  },
  {
    "text": "stateful sets manage multiple Parts based on an identical container spec",
    "start": "333180",
    "end": "339060"
  },
  {
    "text": "running a stateful set will automatically provide High availability by adding redundancy however this",
    "start": "339060",
    "end": "346320"
  },
  {
    "text": "assumption is not true at all consider this stateful set with three",
    "start": "346320",
    "end": "354240"
  },
  {
    "text": "parts fault tolerance means that the application is able to provide the same",
    "start": "354240",
    "end": "359820"
  },
  {
    "text": "service even if one of these parts failed but can we say that it's true for",
    "start": "359820",
    "end": "365400"
  },
  {
    "text": "a stateful set depending on the application that the stateful set is running each persistent",
    "start": "365400",
    "end": "372060"
  },
  {
    "text": "disk has a different set of data so for example if a pod 0 fails although part",
    "start": "372060",
    "end": "379199"
  },
  {
    "text": "one and part two are still running they will not have access to pod Zero's data",
    "start": "379199",
    "end": "385380"
  },
  {
    "text": "set and therefore they cannot provide the same service that part 0 was providing before its failure",
    "start": "385380",
    "end": "392400"
  },
  {
    "text": "so the service will not be available until part 0 is recovered by kubernetes",
    "start": "392400",
    "end": "398780"
  },
  {
    "text": "in other words although we have redundancy for the pots we don't have",
    "start": "398780",
    "end": "404100"
  },
  {
    "text": "any redundancy for the data that's stored on the disks so the lesson learned here is that kubernetes does not",
    "start": "404100",
    "end": "411360"
  },
  {
    "text": "provide data redundancy out of the box and it's up to the application to replicate the data",
    "start": "411360",
    "end": "419360"
  },
  {
    "text": "taking elasticsearch as an example the data stored is shorted by elasticsearch",
    "start": "419400",
    "end": "427380"
  },
  {
    "text": "and each primary Shard can have multiple replicas elasticsearch has a mechanism",
    "start": "427380",
    "end": "433080"
  },
  {
    "text": "to distribute the primary and replica shards across the disks in a way that if",
    "start": "433080",
    "end": "438840"
  },
  {
    "text": "one of the disks fail there is at least one disk that has a copy of the data and is able to provide",
    "start": "438840",
    "end": "445940"
  },
  {
    "text": "the same service applications replicating the stored data",
    "start": "445940",
    "end": "452639"
  },
  {
    "text": "improves availability but we still need to be ready to survive Regional failures",
    "start": "452639",
    "end": "458099"
  },
  {
    "text": "for that reason we have implemented a pipeline using the Kafka streaming",
    "start": "458099",
    "end": "463860"
  },
  {
    "text": "service that replicates an entire elasticsearch stateful set between two",
    "start": "463860",
    "end": "469319"
  },
  {
    "text": "geographical regions we currently have more than 100 elasticsearch instances",
    "start": "469319",
    "end": "475080"
  },
  {
    "text": "living across the globe collectively storing almost two petabytes of data",
    "start": "475080",
    "end": "483440"
  },
  {
    "text": "as mentioned earlier uh one of the reasons that is stateful service becomes",
    "start": "484020",
    "end": "489120"
  },
  {
    "text": "unavailable is a lack of enough storage space for example during High load",
    "start": "489120",
    "end": "494220"
  },
  {
    "text": "events such as the bfcm weekend the Black Friday Cyber Monday the required",
    "start": "494220",
    "end": "501360"
  },
  {
    "text": "storage space increases and the discs can get full rapidly we",
    "start": "501360",
    "end": "507240"
  },
  {
    "text": "could pre-provision extra storage to avoid running out of storage but we will have",
    "start": "507240",
    "end": "513779"
  },
  {
    "text": "to scale back down after the high traffic event otherwise we will be paying for the resources that we're not",
    "start": "513779",
    "end": "520740"
  },
  {
    "text": "actually using and we will see later that scaling down stateful systems is",
    "start": "520740",
    "end": "526320"
  },
  {
    "text": "not a trivial task it is also possible that as time passes",
    "start": "526320",
    "end": "532580"
  },
  {
    "text": "your requirements change and you no longer need the amount of storage that you provided in the beginning and you",
    "start": "532580",
    "end": "539700"
  },
  {
    "text": "need to scale back down for both situations we will need to make our",
    "start": "539700",
    "end": "544800"
  },
  {
    "text": "storage scalable to adapt to the changes of requirements this way we will only pay",
    "start": "544800",
    "end": "551820"
  },
  {
    "text": "for the amount of storage that we actually need and we also want to run out of storage",
    "start": "551820",
    "end": "558300"
  },
  {
    "text": "this brings us to the next topic which is storage Auto scaling and its implementation",
    "start": "558300",
    "end": "565519"
  },
  {
    "text": "now let's take a look at how my team implemented storage Auto scaling as I",
    "start": "566180",
    "end": "572399"
  },
  {
    "text": "mentioned before our elastics are staple sets are deployed and maintained by a custom",
    "start": "572399",
    "end": "578399"
  },
  {
    "text": "kubernetes controller that we have built this custom controller monitors elasticsearch custom resources",
    "start": "578399",
    "end": "585980"
  },
  {
    "text": "and their corresponding stateful sets and other objects that in general form",
    "start": "585980",
    "end": "591779"
  },
  {
    "text": "and elasticsearch instance one of the tasks of one of the many tasks of this controller is to make API",
    "start": "591779",
    "end": "600060"
  },
  {
    "text": "calls it's managed elasticsearch instances to get the current free storage space",
    "start": "600060",
    "end": "606540"
  },
  {
    "text": "based on a defined heuristic the controller decides if the disks need to",
    "start": "606540",
    "end": "611820"
  },
  {
    "text": "be expanded or become smaller",
    "start": "611820",
    "end": "615860"
  },
  {
    "text": "excuse me um yeah",
    "start": "618019",
    "end": "623279"
  },
  {
    "text": "but how can you expand or Shrink the discs of a stateful set that already has",
    "start": "623279",
    "end": "628980"
  },
  {
    "text": "data to update the disks of a stateful set the first approach that comes to",
    "start": "628980",
    "end": "634500"
  },
  {
    "text": "mind is by simply running uh Cube control edit stateful set and updating",
    "start": "634500",
    "end": "640260"
  },
  {
    "text": "the storage field to a different amount that you want um and uh you will see that kubernetes does",
    "start": "640260",
    "end": "648360"
  },
  {
    "text": "not allow you to do this here in this recorded video I this is my",
    "start": "648360",
    "end": "656399"
  },
  {
    "text": "attempt to edit the already existing stateful set I am",
    "start": "656399",
    "end": "664019"
  },
  {
    "text": "editing the storage field in this example from 2 to 3 tbites",
    "start": "664019",
    "end": "672680"
  },
  {
    "text": "and you see here that kubernetes does not allow you to do this the error shows",
    "start": "676640",
    "end": "682019"
  },
  {
    "text": "that you are only allowed to update certain fields in a stateful set spec uh",
    "start": "682019",
    "end": "687920"
  },
  {
    "text": "replicas templates and Etc and storage is not one of them so how can we make this work",
    "start": "687920",
    "end": "696680"
  },
  {
    "text": "let's dive deep into details of handling storage scaling I'll start by uh by the",
    "start": "697019",
    "end": "704820"
  },
  {
    "text": "case um sorry I have lost my mouse okay uh I'll start by the case where we",
    "start": "704820",
    "end": "713339"
  },
  {
    "text": "no longer need the storage we have and we realize we have over provisioned the storage handling this scenario is",
    "start": "713339",
    "end": "719579"
  },
  {
    "text": "challenging because kubernetes does not have a feature to automatically support scaling disks down the reason this",
    "start": "719579",
    "end": "727800"
  },
  {
    "text": "feature does not exist is that because shrinking disks by mistake will lead to data loss and it",
    "start": "727800",
    "end": "735180"
  },
  {
    "text": "should be implemented with care and handled in a case-by-case basis for our use case the",
    "start": "735180",
    "end": "742200"
  },
  {
    "text": "custom controller as we mentioned before calls elastic search API and has the",
    "start": "742200",
    "end": "747899"
  },
  {
    "text": "knowledge of whether like this disk scale down is allowed and based on the",
    "start": "747899",
    "end": "752940"
  },
  {
    "text": "actual storage usage it triggers a disk scale down if",
    "start": "752940",
    "end": "759019"
  },
  {
    "text": "if the numbers allow it to scale down the desk remember we cannot update the",
    "start": "759019",
    "end": "764399"
  },
  {
    "text": "stateful set so instead the custom controller deletes the stateful set",
    "start": "764399",
    "end": "769620"
  },
  {
    "text": "object with the Cascade flag set to false this is important setting this flag to",
    "start": "769620",
    "end": "776459"
  },
  {
    "text": "false allows keeping the parts and discs after deleting the stateful set object so after the stateful set is deleted the",
    "start": "776459",
    "end": "785700"
  },
  {
    "text": "custom controller recreates it with the new smaller size all the other specs remain the same but only story changes",
    "start": "785700",
    "end": "793440"
  },
  {
    "text": "from 8 for example to four terabytes since here we are scaling down storage",
    "start": "793440",
    "end": "799560"
  },
  {
    "text": "but replacing the stateful set object with a new one does not automatically",
    "start": "799560",
    "end": "805200"
  },
  {
    "text": "shrink the disks there are other steps that the controller needs to take in order for the storage",
    "start": "805200",
    "end": "811620"
  },
  {
    "text": "and a desk to reflect the changes you made to the stateful set the custom controller should first delete the",
    "start": "811620",
    "end": "818040"
  },
  {
    "text": "volume claim object deleting it will put a claim with the uh",
    "start": "818040",
    "end": "824160"
  },
  {
    "text": "will put the volume claim object into a terminating phase but it will not be actually deleted because there's still a",
    "start": "824160",
    "end": "831540"
  },
  {
    "text": "pod that's bound to it so we want to force the termination to",
    "start": "831540",
    "end": "837060"
  },
  {
    "text": "force this the controller will also need to delete the pod that's bound to the",
    "start": "837060",
    "end": "842220"
  },
  {
    "text": "terminating claim and that will delete the Pod and the disks for real the",
    "start": "842220",
    "end": "848279"
  },
  {
    "text": "reason we don't delete the Pod first and then the volume claim is that if we do",
    "start": "848279",
    "end": "855300"
  },
  {
    "text": "so the stateful set is faster than us and it will recreate",
    "start": "855300",
    "end": "861000"
  },
  {
    "text": "that part before we were able to delete the claim and we won't be able to proceed",
    "start": "861000",
    "end": "867500"
  },
  {
    "text": "now that the disk and the pods are and the Pod are deleted the stateful set",
    "start": "870839",
    "end": "876240"
  },
  {
    "text": "will do its job and will recreate that pod and the disk now with the new smaller size",
    "start": "876240",
    "end": "882779"
  },
  {
    "text": "the controller will have to do this one by one for every disk as you can see",
    "start": "882779",
    "end": "887880"
  },
  {
    "text": "this scale Downs don't come ready out of the box at all um",
    "start": "887880",
    "end": "894240"
  },
  {
    "text": "and should be handled carefully to make sure that there are like no data loss happens in our case the custom",
    "start": "894240",
    "end": "900540"
  },
  {
    "text": "controller drains every single disk before terminating them sorry",
    "start": "900540",
    "end": "908779"
  },
  {
    "text": "um before terminating the disc to make sure that no data will be wiped from the desk",
    "start": "913260",
    "end": "920300"
  },
  {
    "text": "you can imagine that doing this for the large clusters that we have is very time",
    "start": "920300",
    "end": "925920"
  },
  {
    "text": "consuming and also error prone because there are many steps that this",
    "start": "925920",
    "end": "930959"
  },
  {
    "text": "controller needs to take to make this work for example",
    "start": "930959",
    "end": "936199"
  },
  {
    "text": "one of the custom controller calls to delete a pod might fail and the whole",
    "start": "936199",
    "end": "941820"
  },
  {
    "text": "scale down process might get stuck and you would need an engineer to intervene",
    "start": "941820",
    "end": "947100"
  },
  {
    "text": "and solve that problem in the beginning we used to follow the",
    "start": "947100",
    "end": "952920"
  },
  {
    "text": "same process for disk scale-ups as well meaning that we would replace and delete the stateful set object delete the",
    "start": "952920",
    "end": "960360"
  },
  {
    "text": "claims the pause and all that steps and",
    "start": "960360",
    "end": "965540"
  },
  {
    "text": "in order for kubernetes to create a larger desk for us fortunately we found",
    "start": "965540",
    "end": "972120"
  },
  {
    "text": "a better solution based on a feature that kubernetes provides",
    "start": "972120",
    "end": "977660"
  },
  {
    "text": "kubernetes has a feature called volume expansion in order to make use of this",
    "start": "978060",
    "end": "983160"
  },
  {
    "text": "feature the disk should belong to a storage class that allows volume expansion",
    "start": "983160",
    "end": "989399"
  },
  {
    "text": "let's take a look at a stateful set spec and the volume claim template",
    "start": "989399",
    "end": "994639"
  },
  {
    "text": "in the part that's marked red we see like our our templates our volume claims",
    "start": "994639",
    "end": "1003019"
  },
  {
    "text": "are created based on a storage class called the espd SSD and looking at the",
    "start": "1003019",
    "end": "1008420"
  },
  {
    "text": "spec of that storage class we see that there is a field called allow volume expansion that is set to",
    "start": "1008420",
    "end": "1016160"
  },
  {
    "text": "true so any disk created based on their storage class has the ability",
    "start": "1016160",
    "end": "1023480"
  },
  {
    "text": "to be expanded setting this field to True is a requirements however you need",
    "start": "1023480",
    "end": "1029780"
  },
  {
    "text": "to take some additional steps to make this actually work",
    "start": "1029780",
    "end": "1034779"
  },
  {
    "text": "now let's take a look at this case we are trying to scale up the disks of a",
    "start": "1035559",
    "end": "1040760"
  },
  {
    "text": "stateful set the custom controller to do this the custom controller again needs",
    "start": "1040760",
    "end": "1046459"
  },
  {
    "text": "to delete the stateful set object remember we cannot update State tool sets uh with the Cascade flag of course",
    "start": "1046459",
    "end": "1054620"
  },
  {
    "text": "set to false once the stateful set object is deleted the",
    "start": "1054620",
    "end": "1059720"
  },
  {
    "text": "custom controller will recreate the stateful set with a new size now there is a difference here we no",
    "start": "1059720",
    "end": "1067039"
  },
  {
    "text": "longer need to delete the volume claims the controller just needs to update the",
    "start": "1067039",
    "end": "1073820"
  },
  {
    "text": "volume claim objects the storage from like four to eight terabytes in this",
    "start": "1073820",
    "end": "1079880"
  },
  {
    "text": "case for example and the rest is taken care of by kubernetes kubernetes will",
    "start": "1079880",
    "end": "1085780"
  },
  {
    "text": "automatically expand every disk without downtime and this will be as you can see",
    "start": "1085780",
    "end": "1092299"
  },
  {
    "text": "this is a lot easier compared to the case where you had to delete every disk and drain and like be extra careful",
    "start": "1092299",
    "end": "1100960"
  },
  {
    "text": "oh okay I have a demo in here it's a pre-recorded one",
    "start": "1102020",
    "end": "1109840"
  },
  {
    "text": "we have a stateful set with uh three replicas and we see here that each pod",
    "start": "1111039",
    "end": "1117860"
  },
  {
    "text": "has a volume claim for Tabby bytes on the right hand side uh I'm just",
    "start": "1117860",
    "end": "1124280"
  },
  {
    "text": "triggering a disk scale up by editing a custom resource that we have internally",
    "start": "1124280",
    "end": "1130039"
  },
  {
    "text": "built called the elasticsearch instance I'm updating a field of data volume size",
    "start": "1130039",
    "end": "1135820"
  },
  {
    "text": "that represents the size of the disks of the state full set elasticsearch State",
    "start": "1135820",
    "end": "1142039"
  },
  {
    "text": "full set uh doing this is basically triggering a scale up",
    "start": "1142039",
    "end": "1150399"
  },
  {
    "text": "and here I'm watching the disks as they get",
    "start": "1150799",
    "end": "1157220"
  },
  {
    "text": "expanded by kubernetes we already see that the first one",
    "start": "1157220",
    "end": "1163039"
  },
  {
    "text": "is expanded to five seven bytes um I think I have to skip over a little",
    "start": "1163039",
    "end": "1168860"
  },
  {
    "text": "bit because to save time",
    "start": "1168860",
    "end": "1173440"
  },
  {
    "text": "yeah here we see that the uh",
    "start": "1175460",
    "end": "1180620"
  },
  {
    "text": "second one is expanded and after a while uh",
    "start": "1180620",
    "end": "1188780"
  },
  {
    "text": "the third one is also expanded so you should expect each part each disc to be expanded in about a",
    "start": "1188780",
    "end": "1197240"
  },
  {
    "text": "minute um but I wanted to just skip over",
    "start": "1197240",
    "end": "1203320"
  },
  {
    "text": "uh just like for our case of a 120 this cluster it takes about five or six",
    "start": "1210580",
    "end": "1218299"
  },
  {
    "text": "minutes to scale up an entire large cluster the reason is that we do this in batches",
    "start": "1218299",
    "end": "1224960"
  },
  {
    "text": "and some of the scale-ups have like most of the scale UPS happen in parallel",
    "start": "1224960",
    "end": "1230980"
  },
  {
    "text": "there are two ways to increase uh storage you can either expand the disks",
    "start": "1230980",
    "end": "1236900"
  },
  {
    "text": "you already have which is called scaling up which we just covered and you can",
    "start": "1236900",
    "end": "1242539"
  },
  {
    "text": "also add more disks which is called scaling out adding more disks is a rather straightforward approach no need",
    "start": "1242539",
    "end": "1249860"
  },
  {
    "text": "to delete the state full set or update the disks you can simply update the replicas of the",
    "start": "1249860",
    "end": "1256640"
  },
  {
    "text": "stateful set which we now know that it is allowed and more pods and disks",
    "start": "1256640",
    "end": "1262039"
  },
  {
    "text": "will be added but this approach is not always cost efficient the reason is that",
    "start": "1262039",
    "end": "1267500"
  },
  {
    "text": "in addition to increasing the number of disks you will also add compute resources which most of the time are not",
    "start": "1267500",
    "end": "1275600"
  },
  {
    "text": "really needed and we are only interested in adding storage to make the best decision you should",
    "start": "1275600",
    "end": "1282799"
  },
  {
    "text": "compare the costs of scaling out and scaling up and go with the approach that",
    "start": "1282799",
    "end": "1288260"
  },
  {
    "text": "is the most cost effective one to summarize uh we learned today that",
    "start": "1288260",
    "end": "1295460"
  },
  {
    "text": "kubernetes does not automatically provide availability and fault tolerance",
    "start": "1295460",
    "end": "1300500"
  },
  {
    "text": "for stateful services and it's up to the application to implement data replication and provide availability",
    "start": "1300500",
    "end": "1306880"
  },
  {
    "text": "also storage Auto scaling can be automated by using custom controllers we saw that scaling down",
    "start": "1306880",
    "end": "1315140"
  },
  {
    "text": "storage is not a trivial task and kubernetes does not support it and it should be implemented with care to avoid",
    "start": "1315140",
    "end": "1321559"
  },
  {
    "text": "data loss and lastly we saw that scaling up storage is more straightforward and",
    "start": "1321559",
    "end": "1328220"
  },
  {
    "text": "there is a kubernetes feature that allows for volume expansion thank you for coming and if you have any",
    "start": "1328220",
    "end": "1336559"
  },
  {
    "text": "questions or feedback I'll be here thank you",
    "start": "1336559",
    "end": "1341799"
  },
  {
    "text": "I see a question",
    "start": "1349100",
    "end": "1352720"
  },
  {
    "text": "two questions I see I think there are mics over there if you want to ask yours",
    "start": "1355760",
    "end": "1362390"
  },
  {
    "text": "[Music]",
    "start": "1362390",
    "end": "1365539"
  },
  {
    "text": "hi thank you very much for your talk I was wondering if the custom controller uh it's uh available open source and if",
    "start": "1370039",
    "end": "1379039"
  },
  {
    "text": "you are using the official custom resource definition for the elasticsearch or you build your own as well it's an internal custom controller",
    "start": "1379039",
    "end": "1387260"
  },
  {
    "text": "that we have built it's not available externally and the same is for the custom resource yeah",
    "start": "1387260",
    "end": "1395860"
  },
  {
    "text": "yes thank you for your presentation and my question is how do you handle collisions so for instance here in the",
    "start": "1397220",
    "end": "1403700"
  },
  {
    "text": "middle of downsizing the volumes for instance at node number 20 and then",
    "start": "1403700",
    "end": "1409280"
  },
  {
    "text": "there is another event that says that you need to expand the volume so what's the logic behind that",
    "start": "1409280",
    "end": "1416200"
  },
  {
    "text": "uh this logic like we have some sort of logs uh that we have built in the uh the",
    "start": "1416200",
    "end": "1425299"
  },
  {
    "text": "custom controller so this like since it's the controller that decides whether",
    "start": "1425299",
    "end": "1431600"
  },
  {
    "text": "the disks should be expanded or become smaller this Collision is basically not",
    "start": "1431600",
    "end": "1437360"
  },
  {
    "text": "going to happen because it's the controller that is have that is deciding about this",
    "start": "1437360",
    "end": "1443419"
  },
  {
    "text": "yeah but I mean sort of control are things that disks should be expanded but",
    "start": "1443419",
    "end": "1448760"
  },
  {
    "text": "it will continue to contract those right",
    "start": "1448760",
    "end": "1453280"
  },
  {
    "text": "so the custom controller first of all like it sees the actual",
    "start": "1455179",
    "end": "1461539"
  },
  {
    "text": "usage if uh so we're talking about the case that it needs more storage so it",
    "start": "1461539",
    "end": "1467960"
  },
  {
    "text": "comes up with a number that like this would be a good number for the disks the new number for the disk and it triggers",
    "start": "1467960",
    "end": "1473900"
  },
  {
    "text": "the scale up and like we saw the process that happens the number that the controller picks is",
    "start": "1473900",
    "end": "1481039"
  },
  {
    "text": "a number that it will not make the disks over provision so that in",
    "start": "1481039",
    "end": "1486380"
  },
  {
    "text": "another loop the same controller will uh will decide to scale down the disks so",
    "start": "1486380",
    "end": "1492620"
  },
  {
    "text": "these two like competitions will not happen okay thank you thank you",
    "start": "1492620",
    "end": "1500059"
  },
  {
    "text": "I also have a question here oh okay so um so you said that there's a replica",
    "start": "1500059",
    "end": "1506299"
  },
  {
    "text": "chart that basically makes sure that you don't lose the data so is there any",
    "start": "1506299",
    "end": "1511400"
  },
  {
    "text": "um kind of um mechanic where you first downscale the replica charts or first",
    "start": "1511400",
    "end": "1517580"
  },
  {
    "text": "the original because um you have to make sure that it stays the same right and the size have to stay",
    "start": "1517580",
    "end": "1523700"
  },
  {
    "text": "the same and you have to save the data so is there any technique or structure that I will do it that's a really good question",
    "start": "1523700",
    "end": "1529940"
  },
  {
    "text": "um uh this is actually implemented by elasticsearch elasticsearch allows you",
    "start": "1529940",
    "end": "1535400"
  },
  {
    "text": "to deploy or like spread or distribute your shards in different availability zones",
    "start": "1535400",
    "end": "1542539"
  },
  {
    "text": "so the primary shards and the replica charts of primary and the replica of the",
    "start": "1542539",
    "end": "1548900"
  },
  {
    "text": "same chart they don't end up in the same availability zone so we built our custom",
    "start": "1548900",
    "end": "1555200"
  },
  {
    "text": "controller in a way that it only touches one availability Zone at a time so it's",
    "start": "1555200",
    "end": "1562220"
  },
  {
    "text": "never the case that you are expanding or shrinking the disks",
    "start": "1562220",
    "end": "1568240"
  },
  {
    "text": "of that has that both primary and replica shards",
    "start": "1568240",
    "end": "1573880"
  },
  {
    "text": "of the same chart yeah but at the same time is there a difference if you first",
    "start": "1573880",
    "end": "1579159"
  },
  {
    "text": "touch the primary shot or the replica Shadows there is no difference okay okay",
    "start": "1579159",
    "end": "1585020"
  },
  {
    "text": "thank you I might also have a question thank you",
    "start": "1585020",
    "end": "1590179"
  },
  {
    "text": "uh thanks for the talk um at the first point and we are also using elasticsearch and I'm wondering",
    "start": "1590179",
    "end": "1596539"
  },
  {
    "text": "um what we have noticed is um if you update for example on the elasticsearch instance and um you have a routing",
    "start": "1596539",
    "end": "1603200"
  },
  {
    "text": "upgrade for example uh elasticsearch needs a moment um to redeploy their their replica",
    "start": "1603200",
    "end": "1610580"
  },
  {
    "text": "charts and the the state is in the yellow state for for the time being and",
    "start": "1610580",
    "end": "1615679"
  },
  {
    "text": "how did you take care of that money well you delete the stateful set of course and then this this says the same then",
    "start": "1615679",
    "end": "1622580"
  },
  {
    "text": "you build a new one but yeah have you ever had any problems in time",
    "start": "1622580",
    "end": "1628159"
  },
  {
    "text": "consumpting for example um if the replica shots are being replicated and rebuilt and stuff like",
    "start": "1628159",
    "end": "1634880"
  },
  {
    "text": "that um if this hap if it takes too long um well well as I mentioned in the",
    "start": "1634880",
    "end": "1642919"
  },
  {
    "text": "earlier slides we have the same elasticsearch data set in different uh",
    "start": "1642919",
    "end": "1648860"
  },
  {
    "text": "regions so we have this mechanism of failing over traffic if we see that this is taking too long the elasticsearch",
    "start": "1648860",
    "end": "1656240"
  },
  {
    "text": "cluster has been stuck in the yellow phase uh we just fail over traffic to a",
    "start": "1656240",
    "end": "1661700"
  },
  {
    "text": "region that is not under this maintenance yet yeah so just to be extra careful but it has never led to data",
    "start": "1661700",
    "end": "1668600"
  },
  {
    "text": "loss like that I can say for sure but how fast it happens it it really depends",
    "start": "1668600",
    "end": "1675260"
  },
  {
    "text": "on like yeah of course if you're in different availability zones that's fine because you can simply switch over to",
    "start": "1675260",
    "end": "1681500"
  },
  {
    "text": "another availability so be done with it yeah we can't do that unfortunately because of the data protection and stuff",
    "start": "1681500",
    "end": "1686720"
  },
  {
    "text": "like that but yeah it's an interesting idea thank you very much thank you",
    "start": "1686720",
    "end": "1692179"
  },
  {
    "text": "all right so um you mentioned for the scale out that it's relatively easy to just add",
    "start": "1692179",
    "end": "1697400"
  },
  {
    "text": "replicas to the stateful set when you scale back in do you have to do any additional steps",
    "start": "1697400",
    "end": "1703100"
  },
  {
    "text": "or then do you just rely on elastic to redistribute everything when you take Parts away from it basically uh",
    "start": "1703100",
    "end": "1711620"
  },
  {
    "text": "I don't think you can rely on elasticsearch uh when you basically",
    "start": "1711620",
    "end": "1717140"
  },
  {
    "text": "scale down on a stateful set like kubernetes has no idea what's going on inside the pods so it just starts",
    "start": "1717140",
    "end": "1723500"
  },
  {
    "text": "cutting down pods from the uh the pods that have the higher uh index",
    "start": "1723500",
    "end": "1731360"
  },
  {
    "text": "number from the uh the ones that were created later so",
    "start": "1731360",
    "end": "1737179"
  },
  {
    "text": "unless you have set your uh volume claim policy to",
    "start": "1737240",
    "end": "1743440"
  },
  {
    "text": "retain you will lose data if you just decide to scale down we allow scaling",
    "start": "1743440",
    "end": "1750940"
  },
  {
    "text": "scaling in by using our custom controller and the way that we do that",
    "start": "1750940",
    "end": "1756440"
  },
  {
    "text": "is when such uh updates happen like the one such thing said that are requested",
    "start": "1756440",
    "end": "1763820"
  },
  {
    "text": "for some parts to be removed it is really slow the custom controller one by",
    "start": "1763820",
    "end": "1769460"
  },
  {
    "text": "one again drains every disks disk and like make sure that that this that is",
    "start": "1769460",
    "end": "1775340"
  },
  {
    "text": "about to be deleted is actually empty sometimes it gets stuck because the person uh",
    "start": "1775340",
    "end": "1782000"
  },
  {
    "text": "because these are not automated like we don't allow this to be done automatically so if a team requests",
    "start": "1782000",
    "end": "1787880"
  },
  {
    "text": "their elasticsearch stateful set to be scaled in by mistake like they think that for",
    "start": "1787880",
    "end": "1793340"
  },
  {
    "text": "example two pods are extra but at the but they're actually wrong and it's only one that's extra so the custom",
    "start": "1793340",
    "end": "1800120"
  },
  {
    "text": "controller will not allow this it does some calculations it sees like how many uh what's the storage size",
    "start": "1800120",
    "end": "1807380"
  },
  {
    "text": "free storage and if it's really allowed so we have this protection otherwise you will lose the data of the disks oh sorry",
    "start": "1807380",
    "end": "1814760"
  },
  {
    "text": "uh that is uh deleted okay so just to be clear you still go through the custom",
    "start": "1814760",
    "end": "1820880"
  },
  {
    "text": "controller even when you're scaling out even though you could just edit the stateful set that part wasn't clear",
    "start": "1820880",
    "end": "1828039"
  },
  {
    "text": "yes yeah we also go through with the custom controller thank you thank you",
    "start": "1828980",
    "end": "1835600"
  },
  {
    "text": "hi thanks for the talk a question about the PVS you're deleting them right",
    "start": "1835760",
    "end": "1841279"
  },
  {
    "text": "the PV is not the claims yeah they are storage yeah right and so",
    "start": "1841279",
    "end": "1846919"
  },
  {
    "text": "how do you resync the data do you rely on elasticsearch or do you have any other mechanism to achieve this rethink",
    "start": "1846919",
    "end": "1854179"
  },
  {
    "text": "the data after uh can you yeah you delete the PV yes and then you bring one",
    "start": "1854179",
    "end": "1861440"
  },
  {
    "text": "back in yeah which has a lower disk size and so what do you do with this how do",
    "start": "1861440",
    "end": "1867500"
  },
  {
    "text": "you resync the data is it some custom I suppose you rely on the elastic",
    "start": "1867500",
    "end": "1872539"
  },
  {
    "text": "surgery for this yeah that's true the [Music]",
    "start": "1872539",
    "end": "1879860"
  },
  {
    "text": "mechanism so that almost every desk disk has",
    "start": "1879860",
    "end": "1886820"
  },
  {
    "text": "enough uh has the same amount of storage obviously like it's not always possible to have really close uh disks but yeah",
    "start": "1886820",
    "end": "1895100"
  },
  {
    "text": "like once an mt1 comes in elasticsearch will trigger a what we call Shard",
    "start": "1895100",
    "end": "1901580"
  },
  {
    "text": "relocating and it moves back like some of the stuff to the newly created disk",
    "start": "1901580",
    "end": "1907340"
  },
  {
    "text": "okay and what does it take in time is it hours or an hour",
    "start": "1907340",
    "end": "1914539"
  },
  {
    "text": "um yeah it's in the scale of hours okay",
    "start": "1914539",
    "end": "1920360"
  },
  {
    "text": "yeah thank you yeah thank you [Applause]",
    "start": "1920360",
    "end": "1927660"
  },
  {
    "text": "yeah I guess that's it thank you [Applause]",
    "start": "1928220",
    "end": "1935059"
  }
]