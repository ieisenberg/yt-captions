[
  {
    "text": "hello everyone welcome to bot working group updates my name is Martin Velgus",
    "start": "160",
    "end": "7600"
  },
  {
    "text": "i'm one of the organizers of this working group uh so you all gathered",
    "start": "7600",
    "end": "13440"
  },
  {
    "text": "here so you probably have some intuition what working group",
    "start": "13440",
    "end": "18760"
  },
  {
    "text": "is all about my laptop almost died okay so batch working",
    "start": "18760",
    "end": "26640"
  },
  {
    "text": "group is a forum to discuss enhancements to better support batch workloads in",
    "start": "26640",
    "end": "32279"
  },
  {
    "text": "Kubernetes and by batch workload I mean everything that is related to uh high",
    "start": "32280",
    "end": "38399"
  },
  {
    "text": "performance computing AI machine learning data analytics continuous integrations and the goal of this group",
    "start": "38399",
    "end": "45680"
  },
  {
    "text": "is to reduce fragmentation in Kubernetes ecosystem people in patchworking room",
    "start": "45680",
    "end": "52640"
  },
  {
    "text": "come from different SIS we come from six scheduling six apps six nodes six",
    "start": "52640",
    "end": "58840"
  },
  {
    "text": "autoscaling and we definitely can operate with the six but we are not limited to the six uh we also have some",
    "start": "58840",
    "end": "66880"
  },
  {
    "text": "people who are uh just coming to this group from outside the scope of the g",
    "start": "66880",
    "end": "74400"
  },
  {
    "text": "group is to uh talk and discuss and do additions to batch related APIs think",
    "start": "74400",
    "end": "81520"
  },
  {
    "text": "about job queuing and scheduling primitives uh do tools that maximize",
    "start": "81520",
    "end": "86720"
  },
  {
    "text": "cluster utilization and make the best use of the hardware to improve the",
    "start": "86720",
    "end": "92400"
  },
  {
    "text": "workload performance and we also work on supporting specialized hardware for batch AI ML and all that sort of stuff",
    "start": "92400",
    "end": "101439"
  },
  {
    "text": "uh how to take part uh probably the most important uh part of batchworking group",
    "start": "101439",
    "end": "107520"
  },
  {
    "text": "is our slack channel we have uh bi-weekly meetings uh they are both eur",
    "start": "107520",
    "end": "116320"
  },
  {
    "text": "in European time friendly slot and in",
    "start": "116320",
    "end": "121680"
  },
  {
    "text": "west coast Pacific time uh slot the details you can find in the link below",
    "start": "121680",
    "end": "127360"
  },
  {
    "text": "along with calendar invites zooms links and all of this stuff needed for you to join these",
    "start": "127360",
    "end": "133959"
  },
  {
    "text": "meetings what is uh bus working group doing probably our biggest endeavor and",
    "start": "133959",
    "end": "140640"
  },
  {
    "text": "biggest project is Q you may have heard of Q but for those who have not so far",
    "start": "140640",
    "end": "147680"
  },
  {
    "text": "let me give you a little bit of introduction of what what is it so uh Q",
    "start": "147680",
    "end": "154800"
  },
  {
    "text": "acts a little bit like a bouncer in a club he decides who who should wait who",
    "start": "154800",
    "end": "160319"
  },
  {
    "text": "can get in so that the uh club is not too crowded and by club I mean",
    "start": "160319",
    "end": "165519"
  },
  {
    "text": "Kubernetes cluster it acts a little bit like a receptionist desks who admit",
    "start": "165519",
    "end": "171440"
  },
  {
    "text": "people with pro reservation quickly and let other weights it also acts like a",
    "start": "171440",
    "end": "178239"
  },
  {
    "text": "little bit of a waiter in the restaurant which gives you to gives you the table",
    "start": "178239",
    "end": "183920"
  },
  {
    "text": "and takes you to the desired seating",
    "start": "183920",
    "end": "189140"
  },
  {
    "text": "[Music] and when we're talking about people we are actually talking about various",
    "start": "189140",
    "end": "197200"
  },
  {
    "text": "computational units that come to the cluster and uh this can be in form of a",
    "start": "197200",
    "end": "202879"
  },
  {
    "text": "ray job it can be a plain pots it can be classic pure vanilla kubernetes jobs or",
    "start": "202879",
    "end": "208319"
  },
  {
    "text": "something from cubeflow portfolio in Q and in the remaining of",
    "start": "208319",
    "end": "213360"
  },
  {
    "text": "this presentation we will refer to all of these things as workloads okay so that was a little bit",
    "start": "213360",
    "end": "220560"
  },
  {
    "text": "of introduction uh let's get a little bit more formal so what is Q q is AI HPC",
    "start": "220560",
    "end": "226959"
  },
  {
    "text": "batch workload level scheduler and by workload level scheduler I mean that it",
    "start": "226959",
    "end": "232720"
  },
  {
    "text": "operates on workloads CS and hold not on individual ports like for example cube scheduleuler",
    "start": "232720",
    "end": "240799"
  },
  {
    "text": "it determines when to start stopped or preempt the workload it supports all or",
    "start": "240799",
    "end": "247519"
  },
  {
    "text": "nothing also called as gang scheduling semantics so when deciding when to start",
    "start": "247519",
    "end": "253040"
  },
  {
    "text": "and stop it assumes that the whole thing should start not only a fraction of it",
    "start": "253040",
    "end": "258400"
  },
  {
    "text": "it is very critical for machine learning erh stuff because these things require",
    "start": "258400",
    "end": "265280"
  },
  {
    "text": "all parts and all pieces of computations to be present in order to make progress and Q decides where in the",
    "start": "265280",
    "end": "273360"
  },
  {
    "text": "cluster should it be running should it be running on that specific machines maybe with this accelerators maybe on",
    "start": "273360",
    "end": "279840"
  },
  {
    "text": "spot maybe on demons maybe on your reservation maybe in a specific rack or maybe in specific nodes all of these",
    "start": "279840",
    "end": "287040"
  },
  {
    "text": "things could be decided by Q q is also a resource qua manager so it",
    "start": "287040",
    "end": "296160"
  },
  {
    "text": "allows you to provide quotas to multiple tenants and uh define quotas on various",
    "start": "296160",
    "end": "302080"
  },
  {
    "text": "levels on various types of hardware or also other type of resources that you",
    "start": "302080",
    "end": "308240"
  },
  {
    "text": "may want to utilize in your cluster it has built-in integration for various",
    "start": "308240",
    "end": "316160"
  },
  {
    "text": "APIs for classical things from training and batch word like kubernetes jobs job",
    "start": "316160",
    "end": "322240"
  },
  {
    "text": "set cubeflow portfolio cubray as well as for inference/serving deployment stateful",
    "start": "322240",
    "end": "328639"
  },
  {
    "text": "set leader worker set and other things that could be used universally like aer",
    "start": "328639",
    "end": "334320"
  },
  {
    "text": "plane pods and pod groups it is u hardware vendor and cloud",
    "start": "334320",
    "end": "341840"
  },
  {
    "text": "provider neutral it means that it works with GPUs TPUs or whatever custom",
    "start": "341840",
    "end": "347280"
  },
  {
    "text": "hardware you may have on any cloud that you may be running and also on",
    "start": "347280",
    "end": "354360"
  },
  {
    "text": "prem so what is going in Q q has been around for quite a while it be probably",
    "start": "354360",
    "end": "361440"
  },
  {
    "text": "like three years or so and uh during that time we have had about 50 releases",
    "start": "361440",
    "end": "368800"
  },
  {
    "text": "the uh most recent one happened a week ago so this is a very very active",
    "start": "368800",
    "end": "375280"
  },
  {
    "text": "project with new good stuff constantly coming in during this presentation I",
    "start": "375280",
    "end": "382360"
  },
  {
    "text": "will bring two things to your attention that's came recently these are topology",
    "start": "382360",
    "end": "388720"
  },
  {
    "text": "aware scheduling P sharing and hierarchical quarters and resource controls let's start with topology aware",
    "start": "388720",
    "end": "396520"
  },
  {
    "text": "scheduling so imagine that you have a data center and in this data center you",
    "start": "396520",
    "end": "402000"
  },
  {
    "text": "have couple computers connected to one switch and couple computers connected to",
    "start": "402000",
    "end": "407440"
  },
  {
    "text": "the other switch and there is a connection between uh these switches",
    "start": "407440",
    "end": "414479"
  },
  {
    "text": "if you uh put your uh workloads kind of",
    "start": "414479",
    "end": "421080"
  },
  {
    "text": "randomly you may get quite decent utilization here on the picture you see that all computers are pretty well",
    "start": "421080",
    "end": "427759"
  },
  {
    "text": "utilized each of them is running four pods uh from different application utilization is",
    "start": "427759",
    "end": "433960"
  },
  {
    "text": "100% and well this seems ideal is it",
    "start": "433960",
    "end": "439120"
  },
  {
    "text": "well not really so here on this picture we have uh three workloads uh pink green",
    "start": "439120",
    "end": "445199"
  },
  {
    "text": "and blue one as you can see pink is located more on the right but it also",
    "start": "445199",
    "end": "452880"
  },
  {
    "text": "has some pots on the left side the same applies to green workload and the blue",
    "start": "452880",
    "end": "461080"
  },
  {
    "text": "workload if this workloads are doing just computation uh the situation is okay but if pots",
    "start": "461080",
    "end": "470240"
  },
  {
    "text": "from each of the workloads require to talk to each other then this link",
    "start": "470240",
    "end": "475280"
  },
  {
    "text": "between the switches becomes a bottleneck uh purple pots from the right",
    "start": "475280",
    "end": "482000"
  },
  {
    "text": "want to talk to purple pots from the left green from the right to the left and so on and so on it becomes congested",
    "start": "482000",
    "end": "488759"
  },
  {
    "text": "and none of this workload is performing well because all of them uh are fighting",
    "start": "488759",
    "end": "495440"
  },
  {
    "text": "for this red cable in the middle the cables on fire the workloads are not performing uh GPUs that are super",
    "start": "495440",
    "end": "503680"
  },
  {
    "text": "expensive are effectively wasted everyone is unhappy and that's obviously a bad",
    "start": "503680",
    "end": "510520"
  },
  {
    "text": "situation uh with Q you can arrange these things a little bit better for",
    "start": "510520",
    "end": "517440"
  },
  {
    "text": "example put all of the pink pots on the uh well right hand side",
    "start": "517440",
    "end": "525000"
  },
  {
    "text": "uh and blue and green on the other side and then they are doing only internal",
    "start": "525000",
    "end": "531200"
  },
  {
    "text": "communication which doesn't put too much load on this middle Cable in in the picture and be linking",
    "start": "531200",
    "end": "539040"
  },
  {
    "text": "the switches and that brings a lot of peace of mind to both system",
    "start": "539040",
    "end": "544240"
  },
  {
    "text": "administrators cluster owners and ML researchers because their uh workloads",
    "start": "544240",
    "end": "549760"
  },
  {
    "text": "are progressing well and GPUs are well utilized the other thing that we have in",
    "start": "549760",
    "end": "557920"
  },
  {
    "text": "Q is fair sharing so imagine that we have a large data center and three teams",
    "start": "557920",
    "end": "566000"
  },
  {
    "text": "of researchers the blue one the red one and the yellow one uh based on their",
    "start": "566000",
    "end": "571440"
  },
  {
    "text": "size their priorities the importance of the project uh some director divided the",
    "start": "571440",
    "end": "576959"
  },
  {
    "text": "cluster into fragments and assigned a quart or capacity to each of the teams",
    "start": "576959",
    "end": "582560"
  },
  {
    "text": "the blue team got three racks the red team got seven racks and the yellow one uh",
    "start": "582560",
    "end": "589279"
  },
  {
    "text": "the remaining five racks of GPU intensive GPU heavy",
    "start": "589279",
    "end": "596040"
  },
  {
    "text": "computers uh all of them are using it uh uh to the large extent but sometimes",
    "start": "596040",
    "end": "603120"
  },
  {
    "text": "holiday occurs sometimes offsite sometimes people have summits and sometimes not all the capacity is",
    "start": "603120",
    "end": "611040"
  },
  {
    "text": "utilized to its fullness so what happens then obviously we don't want to waste",
    "start": "611040",
    "end": "617519"
  },
  {
    "text": "the resources that were previously assigned to the red team while they are playing volleyball at Hawaii",
    "start": "617519",
    "end": "624079"
  },
  {
    "text": "uh we don't also want these resources to be given to the to one of the teams",
    "start": "624079",
    "end": "631040"
  },
  {
    "text": "based on their time zone because they arrived to the work earlier or they have some other uh way of getting this",
    "start": "631040",
    "end": "638640"
  },
  {
    "text": "resources between uh the yellow team ideally we would like to have a",
    "start": "638640",
    "end": "645480"
  },
  {
    "text": "mechanism to split uh the unused resources in a kind of fair way so that",
    "start": "645480",
    "end": "654480"
  },
  {
    "text": "neither blue team nor yellow team complains and the capacity of the",
    "start": "654480",
    "end": "661279"
  },
  {
    "text": "cluster is well utilized and money well spent and of course when the red team is",
    "start": "661279",
    "end": "668240"
  },
  {
    "text": "back from their offsite holidays whatever uh they are fully entitled to reclaim their",
    "start": "668240",
    "end": "674360"
  },
  {
    "text": "capacity and use what's theirs okay so",
    "start": "674360",
    "end": "680399"
  },
  {
    "text": "in Q we have two flavors of preeemptions one is preeemption based so",
    "start": "680399",
    "end": "689200"
  },
  {
    "text": "in imagine that if blue team is using too much of the sher resources than the",
    "start": "689200",
    "end": "694320"
  },
  {
    "text": "yellow team and yellow team wants to run a smaller workloads and there's no space left then blue team uh uh should have",
    "start": "694320",
    "end": "702240"
  },
  {
    "text": "some of their workloads preempted let's look what does it look like on example so here we have some",
    "start": "702240",
    "end": "710640"
  },
  {
    "text": "shared capacity let's assume that it consists of 100 GPUs and blue team has",
    "start": "710640",
    "end": "716640"
  },
  {
    "text": "three workloads 50 30 and 15 GPUs each and yellow team wants to run a",
    "start": "716640",
    "end": "723519"
  },
  {
    "text": "relatively small workloads which consists of which requires 20 GPUs",
    "start": "723519",
    "end": "729440"
  },
  {
    "text": "obviously there is no space but uh blue team is kind of abusing its uh fair",
    "start": "729440",
    "end": "737600"
  },
  {
    "text": "share it's using more than it should so if we use this for of preeemptions",
    "start": "737600",
    "end": "744800"
  },
  {
    "text": "this form of per sharing we preempt one of the workloads from the blue team in",
    "start": "744800",
    "end": "750880"
  },
  {
    "text": "order to make space for workload from the yellow team now",
    "start": "750880",
    "end": "756959"
  },
  {
    "text": "the division and first sharing of the and sharing of the anus capacity is a",
    "start": "756959",
    "end": "765360"
  },
  {
    "text": "little bit more fair what happens if yellow team wants",
    "start": "765360",
    "end": "770680"
  },
  {
    "text": "to submit yet another workload this time that requires 35 GPUs well blue team is",
    "start": "770680",
    "end": "778800"
  },
  {
    "text": "still using more GPUs than the yellow but if we preempted some workloads on",
    "start": "778800",
    "end": "785519"
  },
  {
    "text": "the blue of the blue team then the situation would reverse and yellow team",
    "start": "785519",
    "end": "791040"
  },
  {
    "text": "would get much more uh than the blue one so we don't do it we keep this other",
    "start": "791040",
    "end": "799200"
  },
  {
    "text": "workloads waiting so that was one form of first",
    "start": "799200",
    "end": "805360"
  },
  {
    "text": "sharing that we have the other form of first sharing that is uh coming really",
    "start": "805360",
    "end": "810399"
  },
  {
    "text": "really soon to Q it's called coded and will come with uh the next release is",
    "start": "810399",
    "end": "815600"
  },
  {
    "text": "admission based uh first sharing previously we were preempting",
    "start": "815600",
    "end": "821440"
  },
  {
    "text": "uh workloads of the other team if they were using more of the shared capacity",
    "start": "821440",
    "end": "826560"
  },
  {
    "text": "than they should this time we do it differently we wait so if blue team use",
    "start": "826560",
    "end": "833600"
  },
  {
    "text": "significantly more resources than the red team workloads from the uh red or yellow team should be given preference",
    "start": "833600",
    "end": "839920"
  },
  {
    "text": "for the upcoming admission so here like the workload is coming and it's waiting and it's waiting",
    "start": "839920",
    "end": "847199"
  },
  {
    "text": "until something from the blue team is completed and then it's submitted if",
    "start": "847199",
    "end": "854079"
  },
  {
    "text": "something more from the blue team is uh submitted it waits because the blue team",
    "start": "854079",
    "end": "860320"
  },
  {
    "text": "use more of their capacity and then uh we admit something from yellow team",
    "start": "860320",
    "end": "867360"
  },
  {
    "text": "because yellow team use more now notice that I have a mistake on the slides there obviously should be red yellow",
    "start": "867360",
    "end": "875040"
  },
  {
    "text": "team in the in the text sorry for that okay one last thing that I would like to",
    "start": "875040",
    "end": "880480"
  },
  {
    "text": "uh discuss is hierarchical resources so with this uh quota",
    "start": "880480",
    "end": "888199"
  },
  {
    "text": "thing [Music] organizations want to kind of divide",
    "start": "888199",
    "end": "894079"
  },
  {
    "text": "quota in the way they are structurally organized so uh director gets some quota",
    "start": "894079",
    "end": "901360"
  },
  {
    "text": "then this qu is distributed among its managers and then managers give this quot to individual uh teams uh this is",
    "start": "901360",
    "end": "910480"
  },
  {
    "text": "something that you can configure for Q can have hierarchical quarter that uh",
    "start": "910480",
    "end": "918399"
  },
  {
    "text": "follows your orc chart and also all of the first sharing that I previously",
    "start": "918399",
    "end": "924720"
  },
  {
    "text": "uh mentioned also follow the orc chart and uh company politics manager probably",
    "start": "924720",
    "end": "931839"
  },
  {
    "text": "wants to distribute unused capacity uh from one team among their other teams",
    "start": "931839",
    "end": "937600"
  },
  {
    "text": "the same applies directors and so on uh they want to distribute an capacity",
    "start": "937600",
    "end": "943680"
  },
  {
    "text": "among their people and then if the needs of their people are satisfied the anus",
    "start": "943680",
    "end": "949759"
  },
  {
    "text": "capacity goes to the rest of the company okay so these were three things",
    "start": "949759",
    "end": "956480"
  },
  {
    "text": "uh that we have launched or we are launching right now what is coming uh soon uh basically we are doing much more",
    "start": "956480",
    "end": "966639"
  },
  {
    "text": "topology aware scheduling in topology our scheduling we want to include more pieces of Kubernetes uh scheduleuler",
    "start": "966639",
    "end": "974079"
  },
  {
    "text": "code in Q so that the admissions and positioning of the uh and placement of",
    "start": "974079",
    "end": "979839"
  },
  {
    "text": "the pots is more precise we would like to give more controls uh for scoring",
    "start": "979839",
    "end": "986800"
  },
  {
    "text": "topologies and influencing the decisions uh of Q in terms of which rack in which",
    "start": "986800",
    "end": "994320"
  },
  {
    "text": "workloads are placed together and handle recovery and",
    "start": "994320",
    "end": "1001440"
  },
  {
    "text": "errors better uh we want to expand admission based",
    "start": "1001440",
    "end": "1006959"
  },
  {
    "text": "first sharing to all hierarchy levels and uh do some significant improvements",
    "start": "1006959",
    "end": "1012560"
  },
  {
    "text": "to multiq multiq was uh launched more than half a year ago is",
    "start": "1012560",
    "end": "1019759"
  },
  {
    "text": "Q's attempt to handle multicluster job distribution it is getting some",
    "start": "1019759",
    "end": "1024798"
  },
  {
    "text": "adoptions and we received a lot of feedback from the customers and users",
    "start": "1024799",
    "end": "1030079"
  },
  {
    "text": "and uh based on this feedback we want to improve controls uh over work over which",
    "start": "1030079",
    "end": "1037438"
  },
  {
    "text": "workload goes to what cluster and support bods and nice things for",
    "start": "1037439",
    "end": "1044000"
  },
  {
    "text": "researchers like getting logs executing things in the containers and so on okay uh the other project that uh",
    "start": "1044000",
    "end": "1052880"
  },
  {
    "text": "batchworking group is working is QCTL which is basically a tool to perform day-to-day operations on Q like creating",
    "start": "1052880",
    "end": "1060559"
  },
  {
    "text": "cues draining cues stopping and starting cues listing workloads along with their",
    "start": "1060559",
    "end": "1067280"
  },
  {
    "text": "position in the queue posing workloads and all of the stuff that you may imagine that you may encounter in your",
    "start": "1067280",
    "end": "1073760"
  },
  {
    "text": "daily work as a batch cluster administrator it comes uh as a cubectl",
    "start": "1073760",
    "end": "1080000"
  },
  {
    "text": "plugin it's managed by crew it's already launched been around for a while it's relatively stable so if you're using Q",
    "start": "1080000",
    "end": "1087520"
  },
  {
    "text": "you probably should take a look okay and then we go to APIs job job",
    "start": "1087520",
    "end": "1095880"
  },
  {
    "text": "uh has been in Q in Kubernetes for very very long time probably from 1.0 or 1.1",
    "start": "1095880",
    "end": "1105120"
  },
  {
    "text": "release and it's really really mature Kubernetes API most features are already",
    "start": "1105120",
    "end": "1110240"
  },
  {
    "text": "in however some of the features are still coming in particular we are graduating",
    "start": "1110240",
    "end": "1117840"
  },
  {
    "text": "couple things like manage by field went to beta in Kubernetes 132 pot failure",
    "start": "1117840",
    "end": "1123360"
  },
  {
    "text": "policy went to G in 31 success policy will go to GA in 33 and back of limit",
    "start": "1123360",
    "end": "1131840"
  },
  {
    "text": "per index uh will go to GA also in 133",
    "start": "1131840",
    "end": "1138720"
  },
  {
    "text": "jobs set uh you may think about job set as job AP",
    "start": "1138720",
    "end": "1145840"
  },
  {
    "text": "on steroid it's an API for managing groups of job as a unit it offers",
    "start": "1145840",
    "end": "1152080"
  },
  {
    "text": "unified API for deploying HPC and uh AI ML workloads uh it comes with",
    "start": "1152080",
    "end": "1162799"
  },
  {
    "text": "extended policies around starting stopping uh fa discovering failures discovering",
    "start": "1162799",
    "end": "1170799"
  },
  {
    "text": "success restarts and all of the things that you may want to configure for your jobs uh it allows you uh to run uh",
    "start": "1170799",
    "end": "1179280"
  },
  {
    "text": "multiple jobs connected to each other as a single unit and gives you like a nice",
    "start": "1179280",
    "end": "1184960"
  },
  {
    "text": "service to govern them all uh the API is uh also relatively uh",
    "start": "1184960",
    "end": "1193120"
  },
  {
    "text": "stable and mature and is getting adopted by both by both users and frameworks uh",
    "start": "1193120",
    "end": "1201120"
  },
  {
    "text": "for example cubeflow trainer v2 is built around job set API and metaflow is using",
    "start": "1201120",
    "end": "1208880"
  },
  {
    "text": "job set for uh their distributed training so again",
    "start": "1208880",
    "end": "1215600"
  },
  {
    "text": "uh users are uh individual users are using it companies are using it frameworks are uh happy with it so",
    "start": "1215600",
    "end": "1223200"
  },
  {
    "text": "probably if you are not using job set you may want to take a look at",
    "start": "1223200",
    "end": "1228600"
  },
  {
    "text": "it uh last thing that I would like to talk about today is K job and uh K job",
    "start": "1228600",
    "end": "1236960"
  },
  {
    "text": "uh problem statement is around this so how to start uh lots of",
    "start": "1236960",
    "end": "1244360"
  },
  {
    "text": "one one time jobs that have complex storage setup so it consumes input",
    "start": "1244360",
    "end": "1251679"
  },
  {
    "text": "produce output has some logs binaries uh located in various places maybe on NFS",
    "start": "1251679",
    "end": "1257679"
  },
  {
    "text": "maybe on S3 maybe on GCS all things around so they have like really really",
    "start": "1257679",
    "end": "1263039"
  },
  {
    "text": "complex thing configuration around volumes uh and these jobs differ uh very",
    "start": "1263039",
    "end": "1269360"
  },
  {
    "text": "little from each other they maybe have a little bit different arguments for for the scripts maybe the different image",
    "start": "1269360",
    "end": "1276240"
  },
  {
    "text": "maybe slightly different input p files but overall these uh jobs look the same",
    "start": "1276240",
    "end": "1282320"
  },
  {
    "text": "they basically like coming from uh researchers and they are the daily",
    "start": "1282320",
    "end": "1287559"
  },
  {
    "text": "work and these researchers would like to start them via command line tool they",
    "start": "1287559",
    "end": "1294320"
  },
  {
    "text": "for example come from slur mode and they are really really used to playing with",
    "start": "1294320",
    "end": "1300000"
  },
  {
    "text": "common lines instead of editing yaml and all of this uh jobs are not started",
    "start": "1300000",
    "end": "1308840"
  },
  {
    "text": "by single individual but are constantly used by entire team so K",
    "start": "1308840",
    "end": "1317760"
  },
  {
    "text": "job is our way to address it it's a set of reusable job templates templates",
    "start": "1317760",
    "end": "1324400"
  },
  {
    "text": "stored in API server with a command line interfaces that transform these",
    "start": "1324400",
    "end": "1330240"
  },
  {
    "text": "templates into full yamos and send back to API server for",
    "start": "1330240",
    "end": "1336440"
  },
  {
    "text": "execution okay because they are stored on API server they can be reused by team",
    "start": "1336440",
    "end": "1342120"
  },
  {
    "text": "members they have some fields in the template already populated and some of",
    "start": "1342120",
    "end": "1347760"
  },
  {
    "text": "them some of the fields are intentionally left blank uh for the researchers to fill uh each of this",
    "start": "1347760",
    "end": "1356480"
  },
  {
    "text": "template has information uh which parameters need to be provided",
    "start": "1356480",
    "end": "1363440"
  },
  {
    "text": "so that it is uh complete and can be executed and this templates covered job job set",
    "start": "1363440",
    "end": "1370720"
  },
  {
    "text": "cubeflow jobs and uh standalone pots so the float with this uh tool looks like",
    "start": "1370720",
    "end": "1377919"
  },
  {
    "text": "this the admin creates the templates uh setups all of the storage volumes uh",
    "start": "1377919",
    "end": "1385440"
  },
  {
    "text": "mounting points and so on so that uh when researchers come they just need to",
    "start": "1385440",
    "end": "1391520"
  },
  {
    "text": "select the template from the list uh provide additional flag like the",
    "start": "1391520",
    "end": "1397679"
  },
  {
    "text": "script name or maybe parameters or model name or whatever and then uh this",
    "start": "1397679",
    "end": "1403520"
  },
  {
    "text": "template is compiled into fully baked complete job and submitted for",
    "start": "1403520",
    "end": "1410440"
  },
  {
    "text": "execution i mentioned slurm a moment ago so one of the reasons why we did this is",
    "start": "1410440",
    "end": "1417039"
  },
  {
    "text": "to make uh users coming from slurm a little bit easier so in K job there is a special",
    "start": "1417039",
    "end": "1424559"
  },
  {
    "text": "mode with uh which tries to provide slm like",
    "start": "1424559",
    "end": "1430080"
  },
  {
    "text": "experience it is script ccentric it uh",
    "start": "1430080",
    "end": "1435320"
  },
  {
    "text": "provides users with familiar command line options that mimic one to one those",
    "start": "1435320",
    "end": "1441600"
  },
  {
    "text": "that you can pass to uh sr run maybe it",
    "start": "1441600",
    "end": "1446640"
  },
  {
    "text": "has same uh environment variables built in slm scripts when executed on slurm",
    "start": "1446640",
    "end": "1453520"
  },
  {
    "text": "cluster get uh quite long list of uh",
    "start": "1453520",
    "end": "1459039"
  },
  {
    "text": "metadata information like the index uh of uh the task uh and other things as",
    "start": "1459039",
    "end": "1465440"
  },
  {
    "text": "environment variables so we mimic them and we offer a similar approach to",
    "start": "1465440",
    "end": "1470640"
  },
  {
    "text": "search storage binaries input and outputs and this thing is aimed on making migrations between slurm and",
    "start": "1470640",
    "end": "1478000"
  },
  {
    "text": "kubernetes a little bit easier okay so as you might have noticed we are",
    "start": "1478000",
    "end": "1485279"
  },
  {
    "text": "trying to provide a kind of batch uh ecosystem with Kubernetes as uh as a",
    "start": "1485279",
    "end": "1494279"
  },
  {
    "text": "foundation then we have Q which manage job execution quota control uh",
    "start": "1494279",
    "end": "1500480"
  },
  {
    "text": "scheduling and all of the nice uh things related to execution and on",
    "start": "1500480",
    "end": "1505520"
  },
  {
    "text": "top of that we have a set of APIs and uh command line tools uh command line tool",
    "start": "1505520",
    "end": "1512720"
  },
  {
    "text": "for playing with Q and APIs for running jobs and on top of that K jobs to make",
    "start": "1512720",
    "end": "1519120"
  },
  {
    "text": "running these APIs a little bit easier uh this is just a suggestion you are",
    "start": "1519120",
    "end": "1527440"
  },
  {
    "text": "free to extend it use whatever components of this proposed ecosystem as",
    "start": "1527440",
    "end": "1533919"
  },
  {
    "text": "you like fit and if you have strong opinions uh what is missing what should be done what should be done completely",
    "start": "1533919",
    "end": "1541200"
  },
  {
    "text": "differently where we are wrong please please come uh to batch working group",
    "start": "1541200",
    "end": "1547120"
  },
  {
    "text": "let us know what you think what's your opinion what are your needs and we would be very very happy to listen to your",
    "start": "1547120",
    "end": "1553960"
  },
  {
    "text": "feedback okay uh I guess I have couple minutes for questions the mic is here in",
    "start": "1553960",
    "end": "1560159"
  },
  {
    "text": "the middle of the room please come [Applause]",
    "start": "1560159",
    "end": "1571730"
  },
  {
    "text": "okay thanks for the presentation uh question for the fair scheduling is it that the quotota is just the current",
    "start": "1575679",
    "end": "1582000"
  },
  {
    "text": "quotota of or the current distribution of jobs not historic CPU hours that are consumed so if like someone like the",
    "start": "1582000",
    "end": "1588960"
  },
  {
    "text": "blue team runs jobs for a whole day and then the red team comes in and wants to run a job but at the current state the",
    "start": "1588960",
    "end": "1596320"
  },
  {
    "text": "proportion of of distribution is is is okay then like taking vcpu hours into",
    "start": "1596320",
    "end": "1602720"
  },
  {
    "text": "account and not just the vcpu used at the current state uh so you're talking",
    "start": "1602720",
    "end": "1607919"
  },
  {
    "text": "about first sharing yes so we have two flavors of freshing one with preeemptions which doesn't look at the",
    "start": "1607919",
    "end": "1612960"
  },
  {
    "text": "historical data and the other one which submission based sharing and it stores like a historical",
    "start": "1612960",
    "end": "1620840"
  },
  {
    "text": "usage basically like decaying aggregated usage over some",
    "start": "1620840",
    "end": "1627039"
  },
  {
    "text": "period of time so we accumulate how much of the shared capacity a team used and",
    "start": "1627039",
    "end": "1633200"
  },
  {
    "text": "based on the shared of the shared usage with some waiting applied if you provide",
    "start": "1633200",
    "end": "1640799"
  },
  {
    "text": "the weights we sort the workloads and admit them according to this workloads so team that use the shirt capacity the",
    "start": "1640799",
    "end": "1648000"
  },
  {
    "text": "least with the weight combined uh got precedence over teams that used uh more",
    "start": "1648000",
    "end": "1654799"
  },
  {
    "text": "in the past of course if the uh only one team has workloads we don't stop them",
    "start": "1654799",
    "end": "1660240"
  },
  {
    "text": "from sending them but as soon as some other team comes in and they haven't been using the search capacity for a",
    "start": "1660240",
    "end": "1666960"
  },
  {
    "text": "long they get like the first uh spot on the on the line but you don't preempt on",
    "start": "1666960",
    "end": "1673120"
  },
  {
    "text": "based on the history in that mode we don't preempt it if you want preeemptions then uh we have a mode in",
    "start": "1673120",
    "end": "1680080"
  },
  {
    "text": "which preemptions are applied okay cool thanks",
    "start": "1680080",
    "end": "1685399"
  },
  {
    "text": "hello I have some question uh on the ecosystem diagram uh there is a field of",
    "start": "1686159",
    "end": "1692240"
  },
  {
    "text": "other CRDs uh I wonder how does for example ago workflow uh fit in uh into",
    "start": "1692240",
    "end": "1699919"
  },
  {
    "text": "other CRD so aro workload as far as I remember is integrated with Q or yeah",
    "start": "1699919",
    "end": "1705279"
  },
  {
    "text": "miho have we merged",
    "start": "1705279",
    "end": "1709279"
  },
  {
    "text": "Okay so uh we have a way of uh using Argo workloads with with Q there i think",
    "start": "1717840",
    "end": "1725120"
  },
  {
    "text": "there is some documentation on site it was merged like uh really really recently right so argo workflow is on",
    "start": "1725120",
    "end": "1733120"
  },
  {
    "text": "top of the yes it's probably yellow one other CRDs that can use Q to cue its",
    "start": "1733120",
    "end": "1740799"
  },
  {
    "text": "individual steps mhm i see uh thank you uh and my second question is uh for the",
    "start": "1740799",
    "end": "1747880"
  },
  {
    "text": "HPC type of workflow uh for example we have simulation work that we want to do",
    "start": "1747880",
    "end": "1753679"
  },
  {
    "text": "uh each work does not take very long per se like maybe two to three minutes",
    "start": "1753679",
    "end": "1758960"
  },
  {
    "text": "generate a geometry uh and they have they usually use uh like CAD uh CAE",
    "start": "1758960",
    "end": "1766720"
  },
  {
    "text": "these like a windowsbased software um and I wonder uh for a use case where",
    "start": "1766720",
    "end": "1773279"
  },
  {
    "text": "user would submit like hundreds of thousands of uh very short running these",
    "start": "1773279",
    "end": "1778399"
  },
  {
    "text": "simulation work how how can one leverage this",
    "start": "1778399",
    "end": "1783760"
  },
  {
    "text": "ecosystem to do so Q can handle quite a big traffic uh uh we run tests with",
    "start": "1783760",
    "end": "1792480"
  },
  {
    "text": "workloads coming at a really really big speed and the limitation factor is not Q itself but uh etc and whether your",
    "start": "1792480",
    "end": "1801600"
  },
  {
    "text": "cluster and API server can handle the amount of updates happening to pods jobs",
    "start": "1801600",
    "end": "1806799"
  },
  {
    "text": "and so on uh what do you mean by the updates okay so when you start a job",
    "start": "1806799",
    "end": "1812000"
  },
  {
    "text": "let's assume that it has uh 10 pots okay each of the pot needs to be created once",
    "start": "1812000",
    "end": "1817679"
  },
  {
    "text": "the job is started and once you create a pot it's an API server right then you",
    "start": "1817679",
    "end": "1823360"
  },
  {
    "text": "need to schedule uh this each of these 10 pots on some nodes another set of",
    "start": "1823360",
    "end": "1828960"
  },
  {
    "text": "rights when uh uh these pots land on",
    "start": "1828960",
    "end": "1834080"
  },
  {
    "text": "nodes again they need their status to they need their they need to have their",
    "start": "1834080",
    "end": "1840399"
  },
  {
    "text": "status updated again another flow of rights to API server and so on and so on so the",
    "start": "1840399",
    "end": "1849120"
  },
  {
    "text": "limiting factor for uh large flow of job",
    "start": "1849120",
    "end": "1854399"
  },
  {
    "text": "is the performance of your control plane so whether it can handle this amount of",
    "start": "1854399",
    "end": "1859760"
  },
  {
    "text": "churn in the cluster the amount of pots being created updated scheduled and so",
    "start": "1859760",
    "end": "1865679"
  },
  {
    "text": "on and not not necessarily like like you so if you think that you're you have good",
    "start": "1865679",
    "end": "1874399"
  },
  {
    "text": "enough control plane then you probably wouldn't be a bottleneck there",
    "start": "1874399",
    "end": "1880080"
  },
  {
    "text": "i see uh in in that case uh you when when we fire",
    "start": "1880080",
    "end": "1887279"
  },
  {
    "text": "some job the pods needs to be starting up um but usually the image for uh",
    "start": "1887279",
    "end": "1893600"
  },
  {
    "text": "simulation type of software is quite big uh and then the starting time would be",
    "start": "1893600",
    "end": "1899440"
  },
  {
    "text": "quite a lot so I wonder if using this ecosystem is it possible that we have a",
    "start": "1899440",
    "end": "1905279"
  },
  {
    "text": "pod that's always there it waiting for the job request",
    "start": "1905279",
    "end": "1911760"
  },
  {
    "text": "But then it also sounds like it's by you can you can have it it's basically means that you you're doing it like a batch",
    "start": "1911760",
    "end": "1917919"
  },
  {
    "text": "serving so you have a service that gets request and you respond to the to the request",
    "start": "1917919",
    "end": "1924440"
  },
  {
    "text": "so if on the slides I describe that we",
    "start": "1924440",
    "end": "1930720"
  },
  {
    "text": "support serving API so you can deploy this thing that uh does this uh relative",
    "start": "1930720",
    "end": "1937279"
  },
  {
    "text": "short processing as a deployment or leader worker set or whatever fits your architecture the best and then it sits",
    "start": "1937279",
    "end": "1944240"
  },
  {
    "text": "there and process requests that are coming or you may uh back it up by",
    "start": "1944240",
    "end": "1950080"
  },
  {
    "text": "message queue put things on the message queue and start and stop pots uh as the things arrive uh uh on this message",
    "start": "1950080",
    "end": "1957279"
  },
  {
    "text": "queue however for very short computations lasting a minute or so probably it's better to set up a long",
    "start": "1957279",
    "end": "1963120"
  },
  {
    "text": "lift service instead of starting and stopping pot constantly",
    "start": "1963120",
    "end": "1968960"
  },
  {
    "text": "right so this setup is more suitable for very long running jump and a batch long",
    "start": "1968960",
    "end": "1977200"
  },
  {
    "text": "running i would say differently on this diagram you don't say deployment but you can imagine that we add yet another uh",
    "start": "1977200",
    "end": "1985360"
  },
  {
    "text": "yellow rectangle there put deployment label on top of it and then this diagram",
    "start": "1985360",
    "end": "1990960"
  },
  {
    "text": "will suit better to what you are describing so really really short computations happening in great quantity",
    "start": "1990960",
    "end": "1998880"
  },
  {
    "text": "right the deployment would be on top of the queue sorry the deployment uh where",
    "start": "1998880",
    "end": "2004480"
  },
  {
    "text": "would the deployment it depends how you uh position this service is it like a service for uh some computation I don't",
    "start": "2004480",
    "end": "2012000"
  },
  {
    "text": "know you do weather forecasting you do this computation for two hours and then you go away or you need it uh to be",
    "start": "2012000",
    "end": "2018320"
  },
  {
    "text": "running 24 by7 uh Okay I'm running out of time sorry",
    "start": "2018320",
    "end": "2025279"
  },
  {
    "text": "sorry we can talk about this use case in the lobby oh yes if Okay thank you thank",
    "start": "2025279",
    "end": "2030799"
  },
  {
    "text": "you do we have time for one more question",
    "start": "2030799",
    "end": "2037159"
  },
  {
    "text": "[Applause]",
    "start": "2041480",
    "end": "2044739"
  }
]