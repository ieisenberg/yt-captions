[
  {
    "text": "all right thank you so hello I'm the track host for this session that means I",
    "start": "30",
    "end": "7020"
  },
  {
    "text": "get to introduce the speaker and I get to run around with the microphone I was",
    "start": "7020",
    "end": "12090"
  },
  {
    "text": "my name is Brian Boehm I worked for a company called weave works and I was program committee member",
    "start": "12090",
    "end": "19230"
  },
  {
    "text": "actually reviewing the performance track so I'm very pleased to say this is one of the sessions I wanted to see voted",
    "start": "19230",
    "end": "25980"
  },
  {
    "text": "for in the the selection it is performance of systems and especially",
    "start": "25980",
    "end": "32520"
  },
  {
    "text": "kubernetes very dear to my heart so with that please give it up for Linux",
    "start": "32520",
    "end": "40530"
  },
  {
    "text": "platform engineer at indeed Dave Cilic",
    "start": "40530",
    "end": "44628"
  },
  {
    "text": "all right so this is a throttling new developments in application performance with CPU limits as I was introduced I am",
    "start": "48410",
    "end": "56190"
  },
  {
    "text": "Dave Cilic I'm a Linux platform software engineer at indeed so first off what in the world does that mean that means I",
    "start": "56190",
    "end": "64010"
  },
  {
    "text": "work on the OS I am partially a kernel engineer partially an OS engineer prior",
    "start": "64010",
    "end": "70830"
  },
  {
    "text": "to indeed I worked for a company called canonical which owns a little distribution called Ubuntu I am a core",
    "start": "70830",
    "end": "78659"
  },
  {
    "text": "developer for Ubuntu I achieved those rights there and I still maintain those rights but I was also attached to the",
    "start": "78659",
    "end": "85020"
  },
  {
    "text": "kernel team there so I have some credibility with with kernel engineering so before before we go too far I do work",
    "start": "85020",
    "end": "92939"
  },
  {
    "text": "for indeed and indeed we help people get jobs we have 250 million unique monthly visitors",
    "start": "92939",
    "end": "99680"
  },
  {
    "text": "we have about 25 million jobs on our site and we have 1 million employer-sponsored jobs we have roughly",
    "start": "99680",
    "end": "106590"
  },
  {
    "text": "10,000 employees at the moment of which about 2,000 are engineers so before we",
    "start": "106590",
    "end": "112079"
  },
  {
    "text": "go too far this is what the this is the road map for today we're gonna cover CPU limit basics how you can think about",
    "start": "112079",
    "end": "117930"
  },
  {
    "text": "them how you can conceptualize them we're gonna talk about the problem that probably everyone in this room has been",
    "start": "117930",
    "end": "123060"
  },
  {
    "text": "hitting I'll talk about how we ended up reproducing that problem which is actually really hard and then we'll talk",
    "start": "123060",
    "end": "129390"
  },
  {
    "text": "about solutions and workarounds that we came up with and we'll hopefully make your day so starting with CPU limit basics who",
    "start": "129390",
    "end": "135930"
  },
  {
    "text": "should care about CPU limits everyone that runs containers kubernetes maze",
    "start": "135930",
    "end": "141360"
  },
  {
    "text": "hose docker container d like to see anything that is using the underlying CP underlying kernel of CPU limits or CPU",
    "start": "141360",
    "end": "149010"
  },
  {
    "text": "quota system how you set that in kubernetes you go into your pods pod",
    "start": "149010",
    "end": "156420"
  },
  {
    "text": "definition and set limits under resources limits CPU you can set it to in this case 200 Mill of course or",
    "start": "156420",
    "end": "164120"
  },
  {
    "text": "requests CPU 134 ml of course I divide these into a hard limit and a soft limit",
    "start": "164120",
    "end": "169980"
  },
  {
    "text": "that's one of the things I kind of try to try to conceptualize I will use those terms interchangeably when I say hard",
    "start": "169980",
    "end": "176340"
  },
  {
    "text": "limits I mean hard limit I mean limits and if I say soft I mean requests but",
    "start": "176340",
    "end": "182580"
  },
  {
    "text": "what do these mean well soft limits under the covers are using C group CPUs years if you actually",
    "start": "182580",
    "end": "188810"
  },
  {
    "text": "navigate to the CFS sorry the C group file system on your node you will see",
    "start": "188810",
    "end": "194760"
  },
  {
    "text": "under sisyphus C group and then your container a file called CPU shares if",
    "start": "194760",
    "end": "201480"
  },
  {
    "text": "you cat that file you will see that it has the usually the same number as the milk or is that you requested in kubernetes unless you've got some weird",
    "start": "201480",
    "end": "208220"
  },
  {
    "text": "mutating Ingres things now what does that mean what is that number that",
    "start": "208220",
    "end": "214650"
  },
  {
    "text": "number is a number of shares that you've given that process the amount of CPU time you get for that process is",
    "start": "214650",
    "end": "221360"
  },
  {
    "text": "proportional to the amount of shares for all containers on that node so if you",
    "start": "221360",
    "end": "226530"
  },
  {
    "text": "take the shares and divide it by the sum of all shares for all of their products",
    "start": "226530",
    "end": "232110"
  },
  {
    "text": "use that's how many CPUs you are guaranteed to be able to use so let's do",
    "start": "232110",
    "end": "237570"
  },
  {
    "text": "in a little bit example on our nodes we have 88 course 88 core node if we put",
    "start": "237570",
    "end": "245220"
  },
  {
    "text": "one pot on that node that is requesting requesting one CPU how many CPUs are we",
    "start": "245220",
    "end": "251459"
  },
  {
    "text": "going to be get how many CPUs could we use at max oh that would be 88 minimum",
    "start": "251459",
    "end": "257880"
  },
  {
    "text": "would also be 88 because we are the only thing on that node we are not restricted by anything with CPU shares now if we",
    "start": "257880",
    "end": "264300"
  },
  {
    "text": "add another pod to that node our minimum drops to 44 but we're still possibly able to use all 88 course do",
    "start": "264300",
    "end": "270750"
  },
  {
    "text": "that again with to another mm mm no pod and we drop down to 22 and so on get",
    "start": "270750",
    "end": "277889"
  },
  {
    "text": "down to 11 so basically you can consider your requests as a floor for usable CPU",
    "start": "277889",
    "end": "284070"
  },
  {
    "text": "for that pod okay now let's look at hard limits hard limits under the covers use",
    "start": "284070",
    "end": "290850"
  },
  {
    "text": "C group CFS bandwidth control these containers are limited to using a quota amount of of CPU time in a period notice",
    "start": "290850",
    "end": "300150"
  },
  {
    "text": "that I said CPU time if we go back into that directory under our C group file",
    "start": "300150",
    "end": "306360"
  },
  {
    "text": "system we see that there's two settings for this CFS period microseconds and CFS",
    "start": "306360",
    "end": "311370"
  },
  {
    "text": "quota microseconds for the period it is by default set to 100 milliseconds or",
    "start": "311370",
    "end": "317850"
  },
  {
    "text": "100,000 nanus microseconds for the quota that is equal to the limit that you set",
    "start": "317850",
    "end": "324300"
  },
  {
    "text": "for your pod in this case we have 10 milliseconds set or 100 mil occurs after",
    "start": "324300",
    "end": "330539"
  },
  {
    "text": "you've used all of the quota for your pod you will hit throttling and that is",
    "start": "330539",
    "end": "336030"
  },
  {
    "text": "the source of the problem that everyone in this room is here for all right so",
    "start": "336030",
    "end": "342120"
  },
  {
    "text": "you can think of throttling of the limit as a ceiling for the usable CPU that",
    "start": "342120",
    "end": "349320"
  },
  {
    "text": "your application or pod can use CFS bandwidth control gives us a number of",
    "start": "349320",
    "end": "356310"
  },
  {
    "text": "metrics that we can look at in order to determine how our pods are running and",
    "start": "356310",
    "end": "362070"
  },
  {
    "text": "if we look again in that seat in the C group file system we have a file called",
    "start": "362070",
    "end": "368340"
  },
  {
    "text": "CPU stat it provides three metrics and our periods and our throttles and throttle time we'll start with throttle",
    "start": "368340",
    "end": "373710"
  },
  {
    "text": "time because I think it's the least useful throttle time but it's probably also what most of you are looking at",
    "start": "373710",
    "end": "379460"
  },
  {
    "text": "throttle time is the sum total of time a thread in AC group was throttled what",
    "start": "379460",
    "end": "384900"
  },
  {
    "text": "does that mean so that means if you have one thread in your application and it was throttled 490 milliseconds throttle",
    "start": "384900",
    "end": "390630"
  },
  {
    "text": "time will increase by 90 milliseconds if you have a hundred threads in your app in your pod and it each of those is",
    "start": "390630",
    "end": "397620"
  },
  {
    "text": "throttled 490 milliseconds throttle time will increase by 9000",
    "start": "397620",
    "end": "402930"
  },
  {
    "text": "milliseconds every hundred milliseconds so when you're looking at this throttle time it's kind of useless because you",
    "start": "402930",
    "end": "409259"
  },
  {
    "text": "have to also know how many threads are actually running in your pod what I prefer to use instead which is more",
    "start": "409259",
    "end": "414720"
  },
  {
    "text": "agnostic is an hour periods and NR throttles so an hour periods is the number of runnable periods that your",
    "start": "414720",
    "end": "421379"
  },
  {
    "text": "application of number of periods that your application was running what is running mean that means actually using the CPU not blocked on i/o not blocked",
    "start": "421379",
    "end": "428759"
  },
  {
    "text": "on network not blocked on something else not sleeping okay and our throttled is",
    "start": "428759",
    "end": "433830"
  },
  {
    "text": "the number of those periods that you are runnable where you actually used your entire quota okay so what we use it",
    "start": "433830",
    "end": "442650"
  },
  {
    "text": "indeed to determine how badly an application is being throttled is we use this calculation so we take the change",
    "start": "442650",
    "end": "450150"
  },
  {
    "text": "in NR throttle and divide it by the change in an hour periods and we call",
    "start": "450150",
    "end": "455759"
  },
  {
    "text": "that throttle percentage so basically it's the percentage of time that you are running that you got throttled",
    "start": "455759",
    "end": "460949"
  },
  {
    "text": "regardless of how many threads you were you had all right so that was a lot of",
    "start": "460949",
    "end": "466979"
  },
  {
    "text": "words let's do a little a bit of an example so we can all understand a little bit more thoroughly as to what I'm talking about we're gonna start with",
    "start": "466979",
    "end": "474150"
  },
  {
    "text": "an unconstrained example this little application requires 200 milliseconds of time to complete a request so maybe it's",
    "start": "474150",
    "end": "481199"
  },
  {
    "text": "a big application if request comes in at time 0 we would run for 200 milliseconds",
    "start": "481199",
    "end": "487169"
  },
  {
    "text": "and they move to respond at 200 milliseconds if we were unconstrained let's now apply a point for CPU quota or",
    "start": "487169",
    "end": "495990"
  },
  {
    "text": "a 400 mill Accor limit to that application again request comes in at",
    "start": "495990",
    "end": "502199"
  },
  {
    "text": "time 0 but now it runs for 40 milliseconds and then is throttled ok",
    "start": "502199",
    "end": "508139"
  },
  {
    "text": "and it does that again quite a few more times before finally before it finally",
    "start": "508139",
    "end": "515339"
  },
  {
    "text": "responds at 440 milliseconds everyone I see everyone staring at the slides and",
    "start": "515339",
    "end": "521360"
  },
  {
    "text": "being all attentive this is great all right so what we get from our statistics",
    "start": "521360",
    "end": "526529"
  },
  {
    "text": "remember CPU stat from earlier is we're gonna get a CPU usage of 200 milliseconds we still used 200",
    "start": "526529",
    "end": "532470"
  },
  {
    "text": "millisecond but we were our throttle time shows that we were throttled for 240 of those 240",
    "start": "532470",
    "end": "538110"
  },
  {
    "text": "additional milliseconds and we had a throttled percentage of 80 percent so we",
    "start": "538110",
    "end": "543120"
  },
  {
    "text": "were throttled for 4 out of the 5 periods that I was runnable now if we",
    "start": "543120",
    "end": "548760"
  },
  {
    "text": "were monitoring CPU usage we see that CPU usage is actually 0.4 because 40",
    "start": "548760",
    "end": "554010"
  },
  {
    "text": "milliseconds or sorry 200 milliseconds divided by 440 500 milliseconds is 0.4",
    "start": "554010",
    "end": "560700"
  },
  {
    "text": "CPU okay that's not the problem we've all been seeing right all right I see",
    "start": "560700",
    "end": "566670"
  },
  {
    "text": "some nods cool so here's the problem this is what the problem looks like these are p95 response times of one of",
    "start": "566670",
    "end": "573870"
  },
  {
    "text": "the applications running in in deeds' infrastructure we noticed first off that it's a bimodal distribution we have some",
    "start": "573870",
    "end": "579270"
  },
  {
    "text": "really fast instances and we have some really slow instances those slow instances are having two plateaus one at",
    "start": "579270",
    "end": "586830"
  },
  {
    "text": "100 milliseconds and one at 200 milliseconds if you ever see this kind of behavior in your applications you",
    "start": "586830",
    "end": "592980"
  },
  {
    "text": "should immediately think throttling because of those 100 millisecond periods I was throttled at 100 milliseconds I",
    "start": "592980",
    "end": "598590"
  },
  {
    "text": "had to wait for that hundred milliseconds to expire before I could get more time to finish up the request or in the worst case this 200",
    "start": "598590",
    "end": "604560"
  },
  {
    "text": "milliseconds I was throttled for two periods before I got got runnable again but the question here is why in the",
    "start": "604560",
    "end": "610890"
  },
  {
    "text": "world is there at that second that bimodal distribution we'll look at that in a little in a little bit first thing",
    "start": "610890",
    "end": "617040"
  },
  {
    "text": "you do when you see this kind of a problem is you look at your CPU usage and that's exactly what we did we see",
    "start": "617040",
    "end": "622200"
  },
  {
    "text": "that there we have a limit set at point six CPU our nodes of our applications are very dense but even looking at that",
    "start": "622200",
    "end": "629850"
  },
  {
    "text": "the average utilization on this machine on this application was like point three CPU I clearly had plenty of time which I",
    "start": "629850",
    "end": "637020"
  },
  {
    "text": "should have been able to use and this is the problem that everyone's been seeing however if we look at throttled",
    "start": "637020",
    "end": "642660"
  },
  {
    "text": "percentage remember that bimodal distribution will the slow nodes those slow instances are being throttled 80 to",
    "start": "642660",
    "end": "649860"
  },
  {
    "text": "90 percent of the time the fast instances worth being throttled only five to ten percent of the time okay",
    "start": "649860",
    "end": "656190"
  },
  {
    "text": "so how does this happen how do we have low CPU usage with high amounts of",
    "start": "656190",
    "end": "661350"
  },
  {
    "text": "throttling anybody all right good you're in the right place then",
    "start": "661350",
    "end": "666600"
  },
  {
    "text": "first thing we did is we increase the CPU quota the CPU limits for those applications and instantly the amount of",
    "start": "666600",
    "end": "675030"
  },
  {
    "text": "throttling that both of those those slow instances received was dropped to almost nothing and our response our p95",
    "start": "675030",
    "end": "683250"
  },
  {
    "text": "response times dropped from over 200 milliseconds down to 13 milliseconds just like it should have been okay so",
    "start": "683250",
    "end": "691920"
  },
  {
    "text": "that's the problem that's what we wanted to go about and fix let's recap what do",
    "start": "691920",
    "end": "697380"
  },
  {
    "text": "we know we know that increasing the cpu quota mitigates throttling but we also",
    "start": "697380",
    "end": "703710"
  },
  {
    "text": "have all these questions what are the root causes here when we were looking in",
    "start": "703710",
    "end": "708780"
  },
  {
    "text": "that bimodal distribution we were trying to figure out what the what the similarities were between those nodes or",
    "start": "708780",
    "end": "714150"
  },
  {
    "text": "what the differences were between those nodes we came up with a few things the slow nodes had high core count the slow",
    "start": "714150",
    "end": "720480"
  },
  {
    "text": "nodes were a newer CPU architecture how in the world could a newer CPU architecture with more cores result in",
    "start": "720480",
    "end": "726630"
  },
  {
    "text": "lower performance that's exactly what we're seeing the other thing that was kind of unclear is that there was a mix",
    "start": "726630",
    "end": "733800"
  },
  {
    "text": "of kernel versions in there so the some of the fast nodes were running newer version newer versions of the kernel",
    "start": "733800",
    "end": "739260"
  },
  {
    "text": "some of the slow nodes were running newer versions in the kernel but it wasn't really quite obvious where the",
    "start": "739260",
    "end": "744330"
  },
  {
    "text": "distribution happened and the other thing that we were looking at is we were looking at spectrum meltdown in mitigations because this was around the",
    "start": "744330",
    "end": "749640"
  },
  {
    "text": "time that all of the spectrum meltdown stuff was coming through the pipes we were getting new firmware as new biases new mitigations in the kernel and we run",
    "start": "749640",
    "end": "756240"
  },
  {
    "text": "at a very bleeding edge kernel so the first thing I realized is that we were",
    "start": "756240",
    "end": "761550"
  },
  {
    "text": "going to have to reproduce the problem in order to talk to the kernel community and really bring this to their attention",
    "start": "761550",
    "end": "767010"
  },
  {
    "text": "and get some traction I needed to prove to myself that I could reproduce this one it would aid in figuring out what",
    "start": "767010",
    "end": "773130"
  },
  {
    "text": "the root cause of this was but - it would aid in proving to the kernel community that we had actually solved",
    "start": "773130",
    "end": "779310"
  },
  {
    "text": "the problem with whatever change we came up with so I went through and through a number of things that probably you've all tried to do you run a be against",
    "start": "779310",
    "end": "786030"
  },
  {
    "text": "your application run stress ng I read I wrote this random Bash curls of bash script wrapped around curl in asleep and",
    "start": "786030",
    "end": "792600"
  },
  {
    "text": "that actually kind of worked like half the time but it gave me lots of false positives and I",
    "start": "792600",
    "end": "798160"
  },
  {
    "text": "really figure out what in the world was going on then will my co-workers my coworker John came up and said if I set",
    "start": "798160",
    "end": "806170"
  },
  {
    "text": "go max procs on my going application two three on these nodes I can decrease the",
    "start": "806170",
    "end": "813610"
  },
  {
    "text": "amount of CPU usage simultaneously decreasing the amount of throttling and decrease the amount of response times",
    "start": "813610",
    "end": "819459"
  },
  {
    "text": "for that application well that was super interesting and super helpful so let's",
    "start": "819459",
    "end": "825040"
  },
  {
    "text": "update what we know we know that increasing the CPU quota helps us mitigate throttling we know that",
    "start": "825040",
    "end": "830350"
  },
  {
    "text": "decreasing the number of threads now in our application also helps mitigate throttling so in go Lane you said go max",
    "start": "830350",
    "end": "837399"
  },
  {
    "text": "procs but in Java if you move to the newer JVM so I think it's you 191 or newer sorry eight u-19 not 191 or newer",
    "start": "837399",
    "end": "845170"
  },
  {
    "text": "there actually C group aware which helped to decrease the number of threads they spawn Java is still really bad",
    "start": "845170",
    "end": "851139"
  },
  {
    "text": "about spawning tons and thousands of threads even after that so it doesn't completely mitigate the problem the",
    "start": "851139",
    "end": "857290"
  },
  {
    "text": "other thing you can do is move from whole from fractional to whole CPU shares in kubernetes this will allow you",
    "start": "857290",
    "end": "863230"
  },
  {
    "text": "to use this will allow kubernetes to tasks at your CPU test set your pods to",
    "start": "863230",
    "end": "869769"
  },
  {
    "text": "individual CPUs that's the nuance that I learned separately that I just happen to",
    "start": "869769",
    "end": "875350"
  },
  {
    "text": "learned around this time but whatever to do is we've got to create that reproducer and then we're gonna probably",
    "start": "875350",
    "end": "882730"
  },
  {
    "text": "have to fix the kernel scheduler so first the reproducer I wrote this thing called fib test the test is a",
    "start": "882730",
    "end": "888930"
  },
  {
    "text": "multi-threaded calculation of the fibonacci sequence I didn't care what it did I just didn't want it to be",
    "start": "888930",
    "end": "895480"
  },
  {
    "text": "optimized the way I wanted to turn through CPU time and tell me that it used the right amount of time it had two",
    "start": "895480",
    "end": "901990"
  },
  {
    "text": "things fast threads and slow threads the fast threads calculated as fast as possible that's the death sequence and",
    "start": "901990",
    "end": "907350"
  },
  {
    "text": "the slow threads would calculate a hundred iterations and then sleep for 10 milliseconds and here's the key I had to",
    "start": "907350",
    "end": "914470"
  },
  {
    "text": "pin each one of those threads to each sip each CPU and this is actually open source and available on github it's not",
    "start": "914470",
    "end": "921670"
  },
  {
    "text": "all that useful I might integrate it into a pod to verify that this stuff continues to work in the future but I haven't done that",
    "start": "921670",
    "end": "927069"
  },
  {
    "text": "yet but if we run fib test here we have",
    "start": "927069",
    "end": "932180"
  },
  {
    "text": "we're running it with one thread one fast thread and the rest are all slow threads so one thread 16 threads one",
    "start": "932180",
    "end": "939110"
  },
  {
    "text": "being fast 15 being slow and in the worst case 88 threads 1 being fast 78",
    "start": "939110",
    "end": "945200"
  },
  {
    "text": "being 87 being slow if we look at the performance the number of iterations this application is able to perform we",
    "start": "945200",
    "end": "951709"
  },
  {
    "text": "see that in the single threaded case it performs the most number of iterations which is kind of expected because we",
    "start": "951709",
    "end": "957410"
  },
  {
    "text": "have to dedicate time to context switching and everything else to those for that slow thread but we see 14 52",
    "start": "957410",
    "end": "964220"
  },
  {
    "text": "million iterations in the fast case over one second with 50 milliseconds of quota",
    "start": "964220",
    "end": "969649"
  },
  {
    "text": "and in the slowest case on 88 course we see 275 million iterations so it's",
    "start": "969649",
    "end": "975890"
  },
  {
    "text": "almost 5x decrease in processing but here's the key the amount of time used",
    "start": "975890",
    "end": "985730"
  },
  {
    "text": "dropped incredibly if we look at the single threaded case we have we're using",
    "start": "985730",
    "end": "990910"
  },
  {
    "text": "539 milliseconds in that period right in that in that second of time 50",
    "start": "990910",
    "end": "997610"
  },
  {
    "text": "milliseconds times 10 periods of 100 milliseconds is 500 milliseconds so we",
    "start": "997610",
    "end": "1003010"
  },
  {
    "text": "were expecting so a little bit of overhead for the wrapper script to do some calculations and bash because it's slow but you know 539 milliseconds seems",
    "start": "1003010",
    "end": "1010660"
  },
  {
    "text": "reasonable on the single threaded case on 16 CPUs we're seeing 530 milliseconds of usage that's a little bit less but",
    "start": "1010660",
    "end": "1016720"
  },
  {
    "text": "still not terrible but in the 88 core instance we see only 183 milliseconds of",
    "start": "1016720",
    "end": "1023560"
  },
  {
    "text": "CPU usage ok here's our problem we're not actually getting the time that we",
    "start": "1023560",
    "end": "1029319"
  },
  {
    "text": "were allocated we were expecting that 88 core example to get 500 milliseconds or more all right",
    "start": "1029319",
    "end": "1034780"
  },
  {
    "text": "that's actually a 3x difference in the amount of CPU you were able to use on that hike or count machine so if we run",
    "start": "1034780",
    "end": "1043089"
  },
  {
    "text": "F trace on this shout out to my good friend Steve Ross stet who you know",
    "start": "1043089",
    "end": "1049179"
  },
  {
    "text": "inspired me to take photos of my audiences he actually wrote tracing I've traced in the kernel but he also wrote",
    "start": "1049179",
    "end": "1056530"
  },
  {
    "text": "kernel kernel shark so this is a picture of kernel shark run against fib test we",
    "start": "1056530",
    "end": "1062020"
  },
  {
    "text": "see on we see we have our fast thread and we have a bunch of slow threads oh",
    "start": "1062020",
    "end": "1067590"
  },
  {
    "text": "sorry keep in mind this is on the 417 kernel okay this is the kernel that doesn't have the problem we see that the",
    "start": "1067590",
    "end": "1075480"
  },
  {
    "text": "the amount of time between the start of our fast thread and the start and the restart of that fast thread is 100",
    "start": "1075480",
    "end": "1081270"
  },
  {
    "text": "milliseconds which is exactly equal to our period and we see that that fast thread which is meant to chew up as much",
    "start": "1081270",
    "end": "1087090"
  },
  {
    "text": "CPU time as possible is getting 48 milliseconds out of that 50 millisecond quota that's exactly what we want to see",
    "start": "1087090",
    "end": "1095360"
  },
  {
    "text": "if we move up to 2 for 18 we see that that same fast thread is only getting 35",
    "start": "1095360",
    "end": "1102330"
  },
  {
    "text": "milliseconds now at this point you might notice that 35 milliseconds is awfully close to 34",
    "start": "1102330",
    "end": "1109740"
  },
  {
    "text": "milliseconds which is awfully close to the number of CPU the quota I've allowed",
    "start": "1109740",
    "end": "1116070"
  },
  {
    "text": "minus the number of CPUs so if I had had I had 50 milliseconds of quota and the number of CPUs was 16 which is 30 so 50",
    "start": "1116070",
    "end": "1123540"
  },
  {
    "text": "minus 16 is 34 kind of convenient all right that's a little foreshadowing but",
    "start": "1123540",
    "end": "1129900"
  },
  {
    "text": "using this behavior I was able to create I was able to do a git bisect on the kernel and I identified the causal commit and that causal commit is 5 1 2",
    "start": "1129900",
    "end": "1137340"
  },
  {
    "text": "AC 9 9 9 that number is burned into my head forever it's like my best friend's",
    "start": "1137340",
    "end": "1142560"
  },
  {
    "text": "phone number from one as a child but when you look at this this commit it is a fix for inadvertent throttling due to",
    "start": "1142560",
    "end": "1149070"
  },
  {
    "text": "clock drift how in the world did a fix for inadvertent throttling due to clock drift caused us to get inadvertent",
    "start": "1149070",
    "end": "1157740"
  },
  {
    "text": "throttling okay so before I go on this is the commit that a lot of you have",
    "start": "1157740",
    "end": "1163770"
  },
  {
    "text": "been seeing for a lot of for the last few years this is one of them this actually fixes a problem that a lot",
    "start": "1163770",
    "end": "1169500"
  },
  {
    "text": "of you have been seeing so check your kernels make sure you have this fix but that's not the problem that's not the",
    "start": "1169500",
    "end": "1175140"
  },
  {
    "text": "fix that I needed but we did hit that fix because there were still some older",
    "start": "1175140",
    "end": "1181110"
  },
  {
    "text": "nodes that hadn't yet been updated with the newest with the kernel that had that this is what a kernel aim an application",
    "start": "1181110",
    "end": "1188610"
  },
  {
    "text": "without 5-1 to a c99 work looks like the CPU usage on this node is 0.1 CPU it's",
    "start": "1188610",
    "end": "1197880"
  },
  {
    "text": "been given force use of time but if we looked at throttled percentage it is all over the",
    "start": "1197880",
    "end": "1202980"
  },
  {
    "text": "map completely nonsensical doesn't make any sense whatsoever in terms of the",
    "start": "1202980",
    "end": "1208220"
  },
  {
    "text": "what it should be doing or how it's doing it now if we looked at five to one",
    "start": "1208220",
    "end": "1213570"
  },
  {
    "text": "to AC ninety nine more closely we noticed that it did something additional to just fixing that inadvertent throttle",
    "start": "1213570",
    "end": "1219870"
  },
  {
    "text": "throttling due to clock skew it also fixed per CPU quota to expire on the period boundaries okay so if you read",
    "start": "1219870",
    "end": "1228059"
  },
  {
    "text": "the kernel documentation which is amazing by the way if you haven't read",
    "start": "1228059",
    "end": "1233399"
  },
  {
    "text": "it it's pretty just actually pretty darn good the last few years The Seeker CFS",
    "start": "1233399",
    "end": "1240870"
  },
  {
    "text": "bandwidth control implies that you only are allowed strictly to use the amount of bandwidth that you were given every",
    "start": "1240870",
    "end": "1247320"
  },
  {
    "text": "period no more no less well this fix enforced that strict",
    "start": "1247320",
    "end": "1252960"
  },
  {
    "text": "nature okay so that gave me a clue this also this really is what required me to",
    "start": "1252960",
    "end": "1261390"
  },
  {
    "text": "figure out what was going on so let's update our real-world verses that earlier conceptual model the",
    "start": "1261390",
    "end": "1269490"
  },
  {
    "text": "complications we have we have multiple CPUs now we often now have multiple threads oftentimes thousands if you're",
    "start": "1269490",
    "end": "1276270"
  },
  {
    "text": "running Java and cores run at different speeds I haven't touched this at all but",
    "start": "1276270",
    "end": "1282419"
  },
  {
    "text": "this is something that you should also take away from this talk since every since all of this is based on wall time",
    "start": "1282419",
    "end": "1288360"
  },
  {
    "text": "and cores run at different CPU speeds if your core is down clocked when you get",
    "start": "1288360",
    "end": "1293549"
  },
  {
    "text": "running on the on the processor your performance is going to be negatively affected because five milliseconds",
    "start": "1293549",
    "end": "1299070"
  },
  {
    "text": "running at 800 megahertz is way less processing power than five milliseconds running at 3.6 gigahertz or whatever",
    "start": "1299070",
    "end": "1305820"
  },
  {
    "text": "else your cores are able to do so one of the things we did was made sure that all of our nodes are in performance mode",
    "start": "1305820",
    "end": "1312649"
  },
  {
    "text": "last thing is that schedulers are hard it's a little you know I've seen a few slides that say something is hard schedulers are definitely hard I went",
    "start": "1312649",
    "end": "1321029"
  },
  {
    "text": "through and read a bunch of the fare scheduler and the core of scheduling bits of the kernel and I discovered a",
    "start": "1321029",
    "end": "1326490"
  },
  {
    "text": "few things the quota that you give your application is divided into five millisecond slices",
    "start": "1326490",
    "end": "1331970"
  },
  {
    "text": "and then assigned it to individual CPUs here we go we're starting to figure out why CPU cores matter why pinning does",
    "start": "1331970",
    "end": "1339169"
  },
  {
    "text": "cores matter so for example if you gave your application one CPU of quota that",
    "start": "1339169",
    "end": "1346490"
  },
  {
    "text": "means you get a hundred milliseconds per period of time that means five",
    "start": "1346490",
    "end": "1352610"
  },
  {
    "text": "milliseconds if you take that hundred milliseconds and divide it by five milliseconds to get the number of slices",
    "start": "1352610",
    "end": "1357970"
  },
  {
    "text": "you have 20 slices per period well on our nodes we have 88 course if you only",
    "start": "1357970",
    "end": "1364549"
  },
  {
    "text": "have 20 slices that means I can't use 68 of my cores because I can't give them a slice right off the bat and the last",
    "start": "1364549",
    "end": "1373639"
  },
  {
    "text": "thing is that this per CPU quota will expire if not we used within a period okay so that was a lot of words I'm",
    "start": "1373639",
    "end": "1382460"
  },
  {
    "text": "gonna do another example here and show you exactly what's going on in the kernel but we're gonna use a two core",
    "start": "1382460",
    "end": "1387919"
  },
  {
    "text": "machine two cores two threads to separate workers both pinned to their own to their own CPU worker one is",
    "start": "1387919",
    "end": "1397250"
  },
  {
    "text": "pinned to cpu quota to CPU one worker two is pinned to CPU two we have our",
    "start": "1397250",
    "end": "1402350"
  },
  {
    "text": "current time here we've given our application 20 milliseconds of quota and this is the global quota bucket so",
    "start": "1402350",
    "end": "1409549"
  },
  {
    "text": "whenever an application becomes runnable it has to go to the global quota bucket and say hey I need a slice I need five",
    "start": "1409549",
    "end": "1415639"
  },
  {
    "text": "milliseconds in order to run that quota is then when it becomes running that",
    "start": "1415639",
    "end": "1422539"
  },
  {
    "text": "quota is then transferred to the per CPU quota okay right now nothing is running",
    "start": "1422539",
    "end": "1428450"
  },
  {
    "text": "hence there is no quota on the per CPU quote of queues so if we advanced a",
    "start": "1428450",
    "end": "1436639"
  },
  {
    "text": "little bit we see that worker one got a request and was running for five milliseconds it had to transfer five",
    "start": "1436639",
    "end": "1443629"
  },
  {
    "text": "milliseconds from that global quota bucket transfer it to its per CPU queue and then it was able to use for that",
    "start": "1443629",
    "end": "1448759"
  },
  {
    "text": "five milliseconds and make the request onto the request if we let worker to do the same thing",
    "start": "1448759",
    "end": "1454129"
  },
  {
    "text": "great we're down to 10 milliseconds in our quote in our global quota bucket but",
    "start": "1454129",
    "end": "1460009"
  },
  {
    "text": "now what happens if we actually had a smaller request and this request only takes one",
    "start": "1460009",
    "end": "1465320"
  },
  {
    "text": "a complete well we still transfer an entire slice to c2 quota to CPU one what",
    "start": "1465320",
    "end": "1473750"
  },
  {
    "text": "happens to that last four milliseconds that gets left over after only using one millisecond well if it only runs for one",
    "start": "1473750",
    "end": "1480440"
  },
  {
    "text": "millisecond that slice the leftover bits of that slice are gonna persist on that",
    "start": "1480440",
    "end": "1485509"
  },
  {
    "text": "per CPU quota after five milliseconds there's this thing called the slack",
    "start": "1485509",
    "end": "1491779"
  },
  {
    "text": "timer which is going to return everything but one millisecond of slack",
    "start": "1491779",
    "end": "1497179"
  },
  {
    "text": "back to the global quota yeah I see so I imagine my pain when I discovered this",
    "start": "1497179",
    "end": "1504529"
  },
  {
    "text": "for the first time and I'm just explaining it to you all right but what happens here we've got one",
    "start": "1504529",
    "end": "1511129"
  },
  {
    "text": "millisecond on a C hour per CPU quota that is not being used okay great that's",
    "start": "1511129",
    "end": "1517250"
  },
  {
    "text": "fine whatever now if worker two comes in and gets a request and it uses it keeps using it",
    "start": "1517250",
    "end": "1524360"
  },
  {
    "text": "needs a lot of CPU to make this request it's gonna transfer eight milliseconds from the global quota bucket eventually",
    "start": "1524360",
    "end": "1530149"
  },
  {
    "text": "expiring the global quota it's now exhausted what happens now well it keeps",
    "start": "1530149",
    "end": "1538610"
  },
  {
    "text": "running because it still had three milliseconds on its per CPU queue right",
    "start": "1538610",
    "end": "1543879"
  },
  {
    "text": "but now it's exhausted it's per CPU queue and the global quota cook bucket is empty what do we do",
    "start": "1543879",
    "end": "1550370"
  },
  {
    "text": "worker 2 is now throttled but what about that one millisecond on CP 1 well that",
    "start": "1550370",
    "end": "1557809"
  },
  {
    "text": "nothing happens to that one until the end of the period at which point we just expire it and throw it away okay so one",
    "start": "1557809",
    "end": "1565580"
  },
  {
    "text": "millisecond that's where all of our problems have been it's been this one millisecond think about the impact of",
    "start": "1565580",
    "end": "1573559"
  },
  {
    "text": "this though if you have one millisecond every 100 milliseconds that you can expire and you have an 88 core machine",
    "start": "1573559",
    "end": "1579100"
  },
  {
    "text": "that's 88 CPUs minus 1 because you have to have at least one thread that needed to be throttled that's 87 milliseconds",
    "start": "1579100",
    "end": "1586159"
  },
  {
    "text": "every 100 milliseconds that you could be that you could lose due to these 1 milliseconds just being left over on",
    "start": "1586159",
    "end": "1591740"
  },
  {
    "text": "these per CPU queues that's 870 Miller Coors or 0.87 CPU per",
    "start": "1591740",
    "end": "1598560"
  },
  {
    "text": "pod per container in each pod even actually sorry per container 870 milk or",
    "start": "1598560",
    "end": "1605640"
  },
  {
    "text": "is that you can potentially lose if you have an 88 core machine per container",
    "start": "1605640",
    "end": "1611070"
  },
  {
    "text": "that's a lot of CPU we're losing all right so what are the solutions and workarounds this is why you're all here",
    "start": "1611070",
    "end": "1616590"
  },
  {
    "text": "some people have not gone this deep and you know they've said just turn it off I'm not advocate of turning it off",
    "start": "1616590",
    "end": "1622680"
  },
  {
    "text": "because it's now fixed let's look at the possible solutions what could we have done first thing we could have done is",
    "start": "1622680",
    "end": "1628110"
  },
  {
    "text": "remove 5:1 to $89.99 but that would have broken all of you that we're hitting that due to clock skew that was not",
    "start": "1628110",
    "end": "1635220"
  },
  {
    "text": "going to be slow that was not going to be an acceptable solution to the carnal community and I have higher standards",
    "start": "1635220",
    "end": "1641130"
  },
  {
    "text": "than that as well other thing we could have done is we could have created a burst Bank or rollover minutes if",
    "start": "1641130",
    "end": "1647220"
  },
  {
    "text": "anyone's interested in burst banks or rollover minutes basically instead of just expiring those 1 milliseconds we could have stuck them back on the global",
    "start": "1647220",
    "end": "1653220"
  },
  {
    "text": "quota bucket before this was actually not doable because it would have created",
    "start": "1653220",
    "end": "1659700"
  },
  {
    "text": "a thundering herd problem on the global quota buckets lock so if you have 88",
    "start": "1659700",
    "end": "1665280"
  },
  {
    "text": "CPUs all trying to grab that lock to return their 1 milliseconds you cause a lot of waiting because it's all",
    "start": "1665280",
    "end": "1671640"
  },
  {
    "text": "synchronized based on time and the last thing you can do is remove all per CPU",
    "start": "1671640",
    "end": "1677370"
  },
  {
    "text": "expiration logic and that's actually exactly what we did we removed all the per CPU expiration logic this took about",
    "start": "1677370",
    "end": "1684330"
  },
  {
    "text": "five months of debate with the kernel community 6 package iterations one of which was actually written by the",
    "start": "1684330",
    "end": "1689760"
  },
  {
    "text": "original CFS author and we came up with the solution of just leaving them on",
    "start": "1689760",
    "end": "1695430"
  },
  {
    "text": "leaving those 1 milliseconds on those per CPU queues that has resulted in",
    "start": "1695430",
    "end": "1700860"
  },
  {
    "text": "these two commits and these are now applied in the 5 for stable 5 for kernel and I've also backboarded them to the",
    "start": "1700860",
    "end": "1706560"
  },
  {
    "text": "Linux table for curls so hopefully distros are now pulling this in Ubuntu has it because well I went to court of",
    "start": "1706560",
    "end": "1713360"
  },
  {
    "text": "rel 7 is actively testing this and I think this is in there qn of QA",
    "start": "1713360",
    "end": "1718680"
  },
  {
    "text": "environments and relayed to has it as a work in progress and it should be available hopefully soon but what",
    "start": "1718680",
    "end": "1725910"
  },
  {
    "text": "happened to fib test because that's what you're all wondering how did the performance change well the performance changed on our 88",
    "start": "1725910",
    "end": "1731310"
  },
  {
    "text": "core example we went for 50 millisecond quota times 10 periods we got previously 137",
    "start": "1731310",
    "end": "1738930"
  },
  {
    "text": "milliseconds of CPU usage now after this change we're getting 482 milliseconds of",
    "start": "1738930",
    "end": "1744330"
  },
  {
    "text": "usage that's almost a 3x increase in the amount of CPU time that application can",
    "start": "1744330",
    "end": "1749340"
  },
  {
    "text": "use across all ata course so what are the takeaways monitor your throttle",
    "start": "1749340",
    "end": "1755700"
  },
  {
    "text": "percentage that is the most application agnostic metric that I like to use my",
    "start": "1755700",
    "end": "1762420"
  },
  {
    "text": "math doctorate friend likes to tell me that it doesn't mean anything except for",
    "start": "1762420",
    "end": "1767520"
  },
  {
    "text": "it really is a great indication for how badly your applications are being throttled and how badly their response",
    "start": "1767520",
    "end": "1773190"
  },
  {
    "text": "times are being affected number to upgrade your kernels number three use those whole CPU quotas because that'll",
    "start": "1773190",
    "end": "1779280"
  },
  {
    "text": "mitigate this if you can't get the your underlying platform or under a lying cloud to upgrade their kernels lastly",
    "start": "1779280",
    "end": "1785090"
  },
  {
    "text": "increase the quota where necessary because that's going to be your only other mitigating factor or increasing",
    "start": "1785090",
    "end": "1791790"
  },
  {
    "text": "your limits all right any questions",
    "start": "1791790",
    "end": "1797300"
  },
  {
    "text": "raise your hand if you have a question so I can reach you with a microphone",
    "start": "1798710",
    "end": "1805010"
  },
  {
    "text": "yeah you raise your hand that wait for the microphone Oh are they exposed in kubernetes yeah so the question is are",
    "start": "1806330",
    "end": "1814590"
  },
  {
    "text": "the stats exposed in kubernetes unfortunately I am NOT a kubernetes Pro",
    "start": "1814590",
    "end": "1820020"
  },
  {
    "text": "I'm actually quite the kubernetes new which is actually why I was able to do this because I'm much more experienced",
    "start": "1820020",
    "end": "1825480"
  },
  {
    "text": "in the kernel I fairly certain couplet exposes these metrics via the API but",
    "start": "1825480",
    "end": "1830940"
  },
  {
    "text": "you need to make sure that you're monitoring and observability is actually monitoring those those those bits it's actually exposed as in our periods and",
    "start": "1830940",
    "end": "1837300"
  },
  {
    "text": "under throttled is what it's set what it sends out but you have to do the math on it yourself anyone else yeah so I was",
    "start": "1837300",
    "end": "1845100"
  },
  {
    "text": "tracking this bargain lko ml and I",
    "start": "1845100",
    "end": "1850470"
  },
  {
    "text": "noticed that it was merge or is considered merge on 8th of November so",
    "start": "1850470",
    "end": "1855600"
  },
  {
    "text": "first question is what what's the recommended solution in the meanwhile because the accepted solution for a",
    "start": "1855600",
    "end": "1862470"
  },
  {
    "text": "kubernetes right now is just a disabled it's okay so that's that's actually an",
    "start": "1862470",
    "end": "1868800"
  },
  {
    "text": "interesting question kubernetes a lot of the people have gone through and tried to work around the five one to AC 999",
    "start": "1868800",
    "end": "1876720"
  },
  {
    "text": "problem by changing the periods I do not recommend doing that IRA mentioned very",
    "start": "1876720",
    "end": "1883080"
  },
  {
    "text": "much so recommend getting that fixed because that fix actually got merged with roughly six months ago into it got",
    "start": "1883080",
    "end": "1889470"
  },
  {
    "text": "back worded into all the Linux table kernels six months ago if you are not able to get these patches which they",
    "start": "1889470",
    "end": "1896190"
  },
  {
    "text": "really are pretty much available everywhere at this point I would",
    "start": "1896190",
    "end": "1901290"
  },
  {
    "text": "probably just increase your seat your CPU limits to avoid I would increase the B CPU limits by ten times the number of",
    "start": "1901290",
    "end": "1909120"
  },
  {
    "text": "CPUs in MillerCoors did that make sense unfortunately I don't think the vertical pod autoscaler",
    "start": "1909120",
    "end": "1917010"
  },
  {
    "text": "is going to behave well with this mechanism because it's gonna be like oh you're not using your CPU but you're",
    "start": "1917010",
    "end": "1923100"
  },
  {
    "text": "still being throttled I don't know if they use throttling if someone that knows the pod autoscaler better can",
    "start": "1923100",
    "end": "1928640"
  },
  {
    "text": "clarify that I would love to hear but I would suspect that their pod autoscaler is not taking throttling into",
    "start": "1928640",
    "end": "1934500"
  },
  {
    "text": "consideration or the fact that this is happening other than that yeah you",
    "start": "1934500",
    "end": "1940260"
  },
  {
    "text": "probably would want to turn off your limits and indeed what we did is we just increase the amount of limit that the",
    "start": "1940260",
    "end": "1947220"
  },
  {
    "text": "applications were allowed to use does using the whole CPU shares have any",
    "start": "1947220",
    "end": "1953520"
  },
  {
    "text": "benefit after this fix",
    "start": "1953520",
    "end": "1956870"
  },
  {
    "text": "depends well that that really depends it doesn't have any benefit for this",
    "start": "1961080",
    "end": "1966340"
  },
  {
    "text": "problem after this fix it might have a benefit in that you have cash locality",
    "start": "1966340",
    "end": "1972610"
  },
  {
    "text": "you know you're gonna have warm caches on your cores you are possibly not going",
    "start": "1972610",
    "end": "1978340"
  },
  {
    "text": "to context switches off into other applications you're not going to have contention on the core right so yes",
    "start": "1978340",
    "end": "1986590"
  },
  {
    "text": "there is still a benefit to setting a whole CPU limit whole CPUs to your application after this there's not as",
    "start": "1986590",
    "end": "1993670"
  },
  {
    "text": "much of a there's definitely not a reason to not use CPU limits after this though CP CFS quota is working is",
    "start": "1993670",
    "end": "2000630"
  },
  {
    "text": "working fine again go ahead and use it hi can you expand a little bit more on",
    "start": "2000630",
    "end": "2008030"
  },
  {
    "text": "what you just said about you don't recommend decreasing the period not we",
    "start": "2008030",
    "end": "2015210"
  },
  {
    "text": "decreased it from 100 milliseconds to 10 milliseconds and the work has been working smoothly yeah so the reason for",
    "start": "2015210",
    "end": "2022260"
  },
  {
    "text": "that is what's going to end up happening is you're gonna throttle every period and what happens is you're gonna get a",
    "start": "2022260",
    "end": "2029420"
  },
  {
    "text": "higher average and best-case response time with a lower worst case in p95",
    "start": "2029420",
    "end": "2035910"
  },
  {
    "text": "response time did that make sense you're not going to throttle for that 60 mill 50 milliseconds or something in the",
    "start": "2035910",
    "end": "2042000"
  },
  {
    "text": "period the other problem with changing the period is the slicing is 5",
    "start": "2042000",
    "end": "2047820"
  },
  {
    "text": "milliseconds you lose granularity and how much in how accurate the kernel is with your with your CPU usage so yeah",
    "start": "2047820",
    "end": "2056220"
  },
  {
    "text": "I'd I don't like that solution at all because the best-case your best-case",
    "start": "2056220",
    "end": "2061470"
  },
  {
    "text": "response times if you're not looking at them are definitely increasing if you're decreasing the period because does that",
    "start": "2061470",
    "end": "2067950"
  },
  {
    "text": "make sense yeah so like if your application takes so to make it for the rest of for everyone else here if your",
    "start": "2067950",
    "end": "2073620"
  },
  {
    "text": "application takes in his case you set the period to 10 milliseconds and your give it a 1/2 CP a half CPU that's going",
    "start": "2073620",
    "end": "2079470"
  },
  {
    "text": "to equate to 5 milliseconds every 10 milliseconds if your application requires 20 milliseconds to make a",
    "start": "2079470",
    "end": "2084780"
  },
  {
    "text": "response now instead of using 20 milliseconds to make that response you're gonna run for 5 milliseconds be",
    "start": "2084780",
    "end": "2090810"
  },
  {
    "text": "throttled for 5 milliseconds run for 5 milliseconds be throttled for five milliseconds burn for five milliseconds we throttled for",
    "start": "2090810",
    "end": "2096210"
  },
  {
    "text": "five milliseconds a few more times and then you'll respond in 40 milliseconds instead of 20 that make sense all right",
    "start": "2096210",
    "end": "2103670"
  },
  {
    "text": "could you explain again briefly why using whole CPU shares mitigates this problem okay so remember that per CPU",
    "start": "2103670",
    "end": "2112560"
  },
  {
    "text": "quota that I had if you use whole CPU shares you're only going to be running",
    "start": "2112560",
    "end": "2118140"
  },
  {
    "text": "on one or two of those CPUs so the quota is going to be allocated to your CPU",
    "start": "2118140",
    "end": "2123810"
  },
  {
    "text": "that you're running on and so all of your threads are gonna be using from the same per CPU quota so it's less likely",
    "start": "2123810",
    "end": "2130920"
  },
  {
    "text": "that you're gonna strand milliseconds on random CPUs so what happens with Java is",
    "start": "2130920",
    "end": "2136080"
  },
  {
    "text": "it tends to run asynchronous worker threads one per core okay so what is up happening is you do an i/o",
    "start": "2136080",
    "end": "2141780"
  },
  {
    "text": "it spawns it on core 88 and then nothing else from the application runs on core",
    "start": "2141780",
    "end": "2146970"
  },
  {
    "text": "88 in its strands of millisecond there that makes sense so that's why if you bring all of those threads down to one",
    "start": "2146970",
    "end": "2153720"
  },
  {
    "text": "core they're all used they're all pulling from the same per CPU quota bucket well restricting the number of",
    "start": "2153720",
    "end": "2160860"
  },
  {
    "text": "threads still helped after this fix absolutely threads are not free don't don't treat them like like that but it",
    "start": "2160860",
    "end": "2168240"
  },
  {
    "text": "won't I guess yes if you don't if you can't do if you can't do the kernel",
    "start": "2168240",
    "end": "2173850"
  },
  {
    "text": "changes decreasing the number of threads will help I as the kernel engineer we're fans of doing things efficiently",
    "start": "2173850",
    "end": "2180150"
  },
  {
    "text": "spawning 4,000 threads on an 88 core machine is not always the best strategy",
    "start": "2180150",
    "end": "2185160"
  },
  {
    "text": "right so decreasing the number of threads will increase your your performance of your application having a",
    "start": "2185160",
    "end": "2191400"
  },
  {
    "text": "reasonable number of threads based on what you're doing is something you should always look out",
    "start": "2191400",
    "end": "2196920"
  },
  {
    "text": "for well it varies in the vault or strict them like to suing off your CPU",
    "start": "2196920",
    "end": "2204300"
  },
  {
    "text": "requests sorry I'm you second restrict them to the ceiling of your CPU request",
    "start": "2204300",
    "end": "2209580"
  },
  {
    "text": "um no not exactly not not perfect not not not strictly",
    "start": "2209580",
    "end": "2214910"
  },
  {
    "text": "depends on what they are it's it's very case dependent right so like you might have if you allocate 4 CPUs you might",
    "start": "2214910",
    "end": "2221250"
  },
  {
    "text": "want a KERS asynchronous buffered i/o and you might want also Network IO threats right you might want",
    "start": "2221250",
    "end": "2227980"
  },
  {
    "text": "to have four of each so you don't want to have you know just one of each for each core right so it's like it's really depending on what your applications",
    "start": "2227980",
    "end": "2234190"
  },
  {
    "text": "doing anyone else you want to take one last one I I could go for all you know",
    "start": "2234190",
    "end": "2240010"
  },
  {
    "text": "they're our but I have to go do a talk next so yeah oh oh it's your turn okay",
    "start": "2240010",
    "end": "2245609"
  },
  {
    "text": "hi I was just wondering if you had any advice or recommendations around how bad",
    "start": "2245609",
    "end": "2253200"
  },
  {
    "text": "rattling is in terms of like should we be aiming for processes never to be",
    "start": "2253200",
    "end": "2259119"
  },
  {
    "text": "throttled or is you know some amount of throttling okay and expected in day to",
    "start": "2259119",
    "end": "2264220"
  },
  {
    "text": "day operations great question um that's gonna be very application dependent again what I tried to see it indeed is I",
    "start": "2264220",
    "end": "2272799"
  },
  {
    "text": "want to see zero to ten percent throttled time because I want our applications to be sized right in their",
    "start": "2272799",
    "end": "2281020"
  },
  {
    "text": "pod such enough such that I get decent amount of density of applications of pods per node without over allocating",
    "start": "2281020",
    "end": "2288700"
  },
  {
    "text": "right so it's it's it's really a it's a value judgment do I care about the density of my applications on my node or",
    "start": "2288700",
    "end": "2294069"
  },
  {
    "text": "die care purely about performance if you care purely about performance you should target zero percent throttling all the",
    "start": "2294069",
    "end": "2299410"
  },
  {
    "text": "time if you care about density on your nodes you want to be as close to that limit",
    "start": "2299410",
    "end": "2304450"
  },
  {
    "text": "you want to be skirting that limit which means you're gonna hit throttling you know every little bit right so I like to",
    "start": "2304450",
    "end": "2309910"
  },
  {
    "text": "target us to having a little bit of throttling on all of our applications but nothing excessive because that gives",
    "start": "2309910",
    "end": "2315940"
  },
  {
    "text": "us a lot of density that's a speaker off",
    "start": "2315940",
    "end": "2322150"
  },
  {
    "text": "peak yeah so if it speaker off feet we actually target for peak peak times",
    "start": "2322150",
    "end": "2329490"
  },
  {
    "text": "but that is a work in progress that I think it's going to be my next task to work on so I'll probably be playing with",
    "start": "2329490",
    "end": "2336339"
  },
  {
    "text": "the pot vertical pot autoscaler in the next year maybe maybe some other things maybe I'll be doing another talk like",
    "start": "2336339",
    "end": "2341589"
  },
  {
    "text": "this about how we increase the density by awesome applying the vertical pot autoscaler so okay well thank you very",
    "start": "2341589",
    "end": "2348700"
  },
  {
    "text": "much Dave [Applause]",
    "start": "2348700",
    "end": "2353449"
  }
]