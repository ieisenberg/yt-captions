[
  {
    "text": "hi welcome to our talk to learn how to take down the Kubernetes control plane",
    "start": "160",
    "end": "5279"
  },
  {
    "text": "and more importantly how to take good care of it using two techniques the API priority and fairness and the resource",
    "start": "5279",
    "end": "12040"
  },
  {
    "text": "kota my name is Ayas Badali i'm a software engineer at data dog i started",
    "start": "12040",
    "end": "17680"
  },
  {
    "text": "in the SR team and now I'm applying all the reality best practices to our internal Kubernetes platform hello",
    "start": "17680",
    "end": "24320"
  },
  {
    "text": "everyone it's great to be here my name is Materowina i'm a software engineer at data dog and I worked on the control",
    "start": "24320",
    "end": "31920"
  },
  {
    "text": "plane and I'm working in the cluster life cycle automation data dog is a SAS monitoring",
    "start": "31920",
    "end": "38559"
  },
  {
    "text": "company providing full observability on your applications we over 2,000",
    "start": "38559",
    "end": "43840"
  },
  {
    "text": "engineers building the data platform which consists of workloads that are completely running on Kubernetes to give",
    "start": "43840",
    "end": "50320"
  },
  {
    "text": "you an idea of the scales we are running hundred of thousands of pods over tens of thousands of nodes which are spread",
    "start": "50320",
    "end": "56239"
  },
  {
    "text": "over dozens of Kubernetes clusters uh one detail over here is that we manage our own clusters which means that we",
    "start": "56239",
    "end": "62960"
  },
  {
    "text": "deploy and operate the control plane that that is underlying the Kubernetes clusters if you don't and if you rely on",
    "start": "62960",
    "end": "70640"
  },
  {
    "text": "manage Kubernetes clusters by third parties don't worry the best practices that we're going to introduce today are",
    "start": "70640",
    "end": "76720"
  },
  {
    "text": "user side and you will still be able to use them uh to ensure the proper availability of the control plane of",
    "start": "76720",
    "end": "83680"
  },
  {
    "text": "yours even though it is managed by a third party so why should you be uh interested",
    "start": "83680",
    "end": "90000"
  },
  {
    "text": "in the performance and reliability of the control plane when this is used uh and operated by a third party it should",
    "start": "90000",
    "end": "97040"
  },
  {
    "text": "just work right and be right but we'll show you a few of the failures that we",
    "start": "97040",
    "end": "103119"
  },
  {
    "text": "encountered where a single user could affect the stability of the whole platform for everyone in the cluster and",
    "start": "103119",
    "end": "112240"
  },
  {
    "text": "for each of those failure mods we'll present the tools a cluster name can use",
    "start": "112240",
    "end": "117280"
  },
  {
    "text": "to keep and ensure the stability of the platform spoiler alert those are the API",
    "start": "117280",
    "end": "122880"
  },
  {
    "text": "priority and fairness and the resource kota we also present the challenges that we faced when we were using them with",
    "start": "122880",
    "end": "129599"
  },
  {
    "text": "the default configuration and the lessons that we learned as we use them more",
    "start": "129599",
    "end": "135520"
  },
  {
    "text": "extensively so our Kubernetes clusters they are multi-tenant which means that they run various types of workloads from",
    "start": "135879",
    "end": "142879"
  },
  {
    "text": "various team and with that we have the controllers that ensure the proper functionalities of the Kubernetes",
    "start": "142879",
    "end": "148920"
  },
  {
    "text": "clusters um all of them they coexist in an environment that is resource",
    "start": "148920",
    "end": "154200"
  },
  {
    "text": "constrainted the control pane itself is limited in resources and we successfully",
    "start": "154200",
    "end": "159680"
  },
  {
    "text": "took it down in a various occasions instead of excluding the offending users and keep the stability for everyone else",
    "start": "159680",
    "end": "167920"
  },
  {
    "text": "so for example we had a bad deployment of an admission webbook that was wiping out all the labels of the pods so the",
    "start": "167920",
    "end": "174959"
  },
  {
    "text": "replica the replica set controller lost the ownership of those pods thought that",
    "start": "174959",
    "end": "180560"
  },
  {
    "text": "he did not have the pods created them again the pods went over the admission web books again that wiped again the",
    "start": "180560",
    "end": "186319"
  },
  {
    "text": "labels so the replica set compiler was like I don't have my pods I will create again they will go in the web admission",
    "start": "186319",
    "end": "192720"
  },
  {
    "text": "web book and you can imagine where this is going we created hundred of thousand of pods over the course of couple of",
    "start": "192720",
    "end": "199120"
  },
  {
    "text": "hours you can imagine this is way above the six scalability limits that they recommend",
    "start": "199120",
    "end": "205920"
  },
  {
    "text": "on another occasion we have Spark jobs uh that requested thousands of pods",
    "start": "205920",
    "end": "211920"
  },
  {
    "text": "running as exeutor for for the job um at the same time and the problem is really",
    "start": "211920",
    "end": "219200"
  },
  {
    "text": "at the same time so we ended up with all the controllers that they that were taking care of the life cycles of the",
    "start": "219200",
    "end": "224640"
  },
  {
    "text": "various components like the pods and the nodes on the cluster that were struggling so hard with the resource",
    "start": "224640",
    "end": "229760"
  },
  {
    "text": "that were they they were allocated and they were not able to keep up in a reasonable amount of time uh with the",
    "start": "229760",
    "end": "236480"
  },
  {
    "text": "load that they were requested uh having too many pods in a cluster is not the only problem that we",
    "start": "236480",
    "end": "243120"
  },
  {
    "text": "can face and typically we are running selium for the network um the selenium post that are running",
    "start": "243120",
    "end": "249920"
  },
  {
    "text": "are watching uh the selenium CRDs and one of the behavior of the API server",
    "start": "249920",
    "end": "255680"
  },
  {
    "text": "when there's a CRD update is that it drops all the watch requests so when that happens we have all the C engine",
    "start": "255680",
    "end": "262960"
  },
  {
    "text": "parts on all the nodes that start doing list requests on the API server so",
    "start": "262960",
    "end": "268080"
  },
  {
    "text": "really fun incidents when we kill the API server and etc at the same time uh",
    "start": "268080",
    "end": "274440"
  },
  {
    "text": "but my favorite incident above all of them I think is this one because it involves some DNS shenanigans we had the",
    "start": "274440",
    "end": "281919"
  },
  {
    "text": "load balancing misconfiguration where we started putting all the cubelet traffic to a single instance of the API server",
    "start": "281919",
    "end": "288240"
  },
  {
    "text": "that was completely overwhelmed while the other API servers were on the side busy doing kind of nothing",
    "start": "288240",
    "end": "295280"
  },
  {
    "text": "so so far for that the option community has come with solutions uh for us",
    "start": "295280",
    "end": "301040"
  },
  {
    "text": "against search overloading uh namely they say to use the API priority and fairness uh to prevent one bad type of",
    "start": "301040",
    "end": "308479"
  },
  {
    "text": "request from taking up all the available resources of the API server and actually",
    "start": "308479",
    "end": "315440"
  },
  {
    "text": "it comes already preconfigured on the cluster uh since Kubernetes 120 with",
    "start": "315440",
    "end": "320880"
  },
  {
    "text": "those two kinds of resources the flow schemas and um the priority level",
    "start": "320880",
    "end": "325919"
  },
  {
    "text": "configurations and this is what you would see on a brand new empty Kubernetes",
    "start": "325919",
    "end": "331000"
  },
  {
    "text": "cluster so let's dive in a little bit more into those kinds of um",
    "start": "331000",
    "end": "337080"
  },
  {
    "text": "objects the first one is the flow schema uh that is used to filter the requests that are coming to the API server uh the",
    "start": "337080",
    "end": "345120"
  },
  {
    "text": "way it works is it will match the requests against the users that have been uh sending them uh either on a",
    "start": "345120",
    "end": "351360"
  },
  {
    "text": "group or a service account in that case we are targeting all the requests that",
    "start": "351360",
    "end": "356560"
  },
  {
    "text": "are coming from system nodes not all requests though uh we also",
    "start": "356560",
    "end": "361600"
  },
  {
    "text": "filter on the resources that are queried so in that case the non statues and the lees and you can imagine why we're doing",
    "start": "361600",
    "end": "368880"
  },
  {
    "text": "that for this one specifically this is for the node her bit once we have filtered the request",
    "start": "368880",
    "end": "376080"
  },
  {
    "text": "they are sent to a priority level uh that is referenced over here and the",
    "start": "376080",
    "end": "381840"
  },
  {
    "text": "priority level is really what defines how the requests are prioritized and how they are throttled and to do that it",
    "start": "381840",
    "end": "389280"
  },
  {
    "text": "defines the conference shares cues numbers lots of this is becoming",
    "start": "389280",
    "end": "395360"
  },
  {
    "text": "complicated over here what does that all mean and more importantly how does it help at all with this incident that we",
    "start": "395360",
    "end": "401600"
  },
  {
    "text": "had uh and I talked about previously so to answer that question uh",
    "start": "401600",
    "end": "407680"
  },
  {
    "text": "I want you to get a better intuition at what the priority and fairness is and how it works and I'll show you how I",
    "start": "407680",
    "end": "414400"
  },
  {
    "text": "reason about them so we have this API server instance over here that is",
    "start": "414400",
    "end": "420240"
  },
  {
    "text": "configured with a given set of flow schemas and we have inbound requests",
    "start": "420240",
    "end": "425680"
  },
  {
    "text": "that are coming from different users on different resources that I have represented here with different colors",
    "start": "425680",
    "end": "433120"
  },
  {
    "text": "so the first step uh is really to go over the flow schemas in the order of",
    "start": "433120",
    "end": "439919"
  },
  {
    "text": "the matching precedent and match the request against them once we have that",
    "start": "439919",
    "end": "445599"
  },
  {
    "text": "the flow schema is referencing a priority level which you've seen is configured with a given number of cues",
    "start": "445599",
    "end": "452400"
  },
  {
    "text": "we have four of them in that example the priority level also comes",
    "start": "452400",
    "end": "457599"
  },
  {
    "text": "with a hand size and the hand size is used for the shuffle sharding of the flow schemas on the various cues and the",
    "start": "457599",
    "end": "464800"
  },
  {
    "text": "idea behind that is really we don't want two types of requests coming from different users to end up in the same",
    "start": "464800",
    "end": "471280"
  },
  {
    "text": "cues to avoid overloading each other so this is what's happening we",
    "start": "471280",
    "end": "478080"
  },
  {
    "text": "have requests from different cues different resources that end up in the um in different",
    "start": "478080",
    "end": "484599"
  },
  {
    "text": "cues after that we have a fair scheduling algorithm that will take care",
    "start": "484599",
    "end": "490319"
  },
  {
    "text": "of dispatching the requests that have been encued to the workers that are available how many of those seats are",
    "start": "490319",
    "end": "497199"
  },
  {
    "text": "available this is defined by the max request in flight flag on the API server",
    "start": "497199",
    "end": "502879"
  },
  {
    "text": "in that case we have 20 of them that are split over the two priority levels that we have in that",
    "start": "502879",
    "end": "509960"
  },
  {
    "text": "example the cues themselves they have a limit in size and what happens when one",
    "start": "509960",
    "end": "517518"
  },
  {
    "text": "specific flow schema overflows its available cues this is really where the",
    "start": "517519",
    "end": "523039"
  },
  {
    "text": "throttling kicks in the extra request going through this flow schema will be",
    "start": "523039",
    "end": "528320"
  },
  {
    "text": "throttled and they will be returned with a 429 too many requests and all of that",
    "start": "528320",
    "end": "535279"
  },
  {
    "text": "is happening while the requests going to the other four schemas from the other users on the other requests they can",
    "start": "535279",
    "end": "541680"
  },
  {
    "text": "still be encued because those cues they are not full and this is what we can see uh we",
    "start": "541680",
    "end": "550080"
  },
  {
    "text": "even have a few golden metrics that that we track for for that that I wanted to to show you over there so mainly we're",
    "start": "550080",
    "end": "556160"
  },
  {
    "text": "tracking the number of requests uh that are going through uh the dispatching of",
    "start": "556160",
    "end": "562080"
  },
  {
    "text": "the priority and fairness we also monitor the latency the time that is spent in the queue waiting and in the",
    "start": "562080",
    "end": "569040"
  },
  {
    "text": "case that the throttle kicks in we can measure the number of four to9s that are",
    "start": "569040",
    "end": "574320"
  },
  {
    "text": "being served so that's great we have the priority and fairness that is in place",
    "start": "574320",
    "end": "580160"
  },
  {
    "text": "it is working we have the metric that shows that it is actually kicking in why did we have the incident in the first",
    "start": "580160",
    "end": "586320"
  },
  {
    "text": "place and how can we do better and this is really where we had to start tuning",
    "start": "586320",
    "end": "591360"
  },
  {
    "text": "all those resources so the first step to that journey was to introduce new flow schemas and a few of them that we",
    "start": "591360",
    "end": "598839"
  },
  {
    "text": "introduced the one that is on the slashmetrix and /debugp prof that one is",
    "start": "598839",
    "end": "604800"
  },
  {
    "text": "exempt it will never ever been throttled even if there's contention on the uh API",
    "start": "604800",
    "end": "610080"
  },
  {
    "text": "server and the reason for that is because this is exactly the moment when you want the most observer on the API",
    "start": "610080",
    "end": "615440"
  },
  {
    "text": "server so make sure not to throttle those requests we also introduce flow schemas",
    "start": "615440",
    "end": "621760"
  },
  {
    "text": "per namespace So all the service accounts that are in the given space will be assigned to the same flow schema",
    "start": "621760",
    "end": "628079"
  },
  {
    "text": "and we have a last one for human users so people running cubectl commands on their laptop uh internally they are in",
    "start": "628079",
    "end": "635200"
  },
  {
    "text": "the group engineering they will be in their own uh flow schema so that they can be prioritized typically for manual",
    "start": "635200",
    "end": "642079"
  },
  {
    "text": "operations during an incident so at this point we have better",
    "start": "642079",
    "end": "647680"
  },
  {
    "text": "ability we have uh and actually it's we didn't gain much from what we have",
    "start": "647680",
    "end": "653519"
  },
  {
    "text": "already in the audit logs but where this really this becomes really handy is when",
    "start": "653519",
    "end": "658800"
  },
  {
    "text": "we start introducing priority levels and the first one that we introduced was the target constraint priority level that",
    "start": "658800",
    "end": "665600"
  },
  {
    "text": "was very limited in the concurrent shares we only have five over here and instead of encuing requests in case",
    "start": "665600",
    "end": "672480"
  },
  {
    "text": "there are too many of them in par we will reject them right away uh and this",
    "start": "672480",
    "end": "678240"
  },
  {
    "text": "This is where splitting the traffic into more flow schemas becomes handy because in an incident we can identify which of",
    "start": "678240",
    "end": "684640"
  },
  {
    "text": "the flow schema is offending and we apply on demand the the top priority and",
    "start": "684640",
    "end": "690120"
  },
  {
    "text": "level so we were pretty happy at that moment we we had the open book uh to",
    "start": "690120",
    "end": "696240"
  },
  {
    "text": "apply when something was going on uh we even have the telemetry to to see when",
    "start": "696240",
    "end": "701680"
  },
  {
    "text": "something was going wrong uh and we had that that nice metric that was showing",
    "start": "701680",
    "end": "707040"
  },
  {
    "text": "that uh the top was effectively running at that limit that we've set so at this",
    "start": "707040",
    "end": "713680"
  },
  {
    "text": "point we were really looking forward for the next incident so that we can use that and it happened we had an overload",
    "start": "713680",
    "end": "720399"
  },
  {
    "text": "of the control plane and we applied the tarpit and we had still the overload of",
    "start": "720399",
    "end": "726320"
  },
  {
    "text": "the control plane oh wait that was not supposed to happen the top was supposed to prevent that altogether",
    "start": "726320",
    "end": "733440"
  },
  {
    "text": "uh and we found this other metric the actual limit that was set on the target priority level and it was going way",
    "start": "733440",
    "end": "741200"
  },
  {
    "text": "above the limit that we have set initially in the place uh the only thing that we could find was it was not going",
    "start": "741200",
    "end": "748079"
  },
  {
    "text": "above this upper limit that was set and actually there's an upstream uh issue where this limit is unreachable but",
    "start": "748079",
    "end": "755440"
  },
  {
    "text": "really um the target was not toping the traffic and we want to know why and to",
    "start": "755440",
    "end": "762160"
  },
  {
    "text": "answer that question I have to mention that it actually worked in the first place we tested it and it really stopped",
    "start": "762160",
    "end": "768079"
  },
  {
    "text": "working after Kubernetes 1023 so what happened in Kubernetes 123",
    "start": "768079",
    "end": "774560"
  },
  {
    "text": "There was a boring mechanism that was introduced what does it look like so let's go back at the schema that we had",
    "start": "774560",
    "end": "781040"
  },
  {
    "text": "to to look at that we have this priority level that is constraint it has a number",
    "start": "781040",
    "end": "787200"
  },
  {
    "text": "of seats that are all being used and this time uh we have on the other side",
    "start": "787200",
    "end": "794000"
  },
  {
    "text": "another priority level that has some seats that are available and not process",
    "start": "794000",
    "end": "799920"
  },
  {
    "text": "processing any of the requests what if we could borrow those seats that",
    "start": "799920",
    "end": "806639"
  },
  {
    "text": "are unused and this is exactly what the borrowing mechanism is used for we have this uh one constraint priority level",
    "start": "806639",
    "end": "813839"
  },
  {
    "text": "that will get the seats from the other uh priority level that is not at capacity and when you think about that",
    "start": "813839",
    "end": "820639"
  },
  {
    "text": "this is what what you want to to do to make sure that you use as much as possible the resource that you have",
    "start": "820639",
    "end": "826200"
  },
  {
    "text": "available except this is not what we would want in the case of the tarpit priority level so how do we do we",
    "start": "826200",
    "end": "831519"
  },
  {
    "text": "disable the boring mechanism to do that there's a fact that we can set the boring priority limit uh the boring",
    "start": "831519",
    "end": "838320"
  },
  {
    "text": "limit percent to 0% uh to prevent the boring mechanism and we also set the",
    "start": "838320",
    "end": "843680"
  },
  {
    "text": "lendable person to 100% so that we can give away all the seats that we are not using so that we do not waste capacity",
    "start": "843680",
    "end": "850560"
  },
  {
    "text": "so you see the slight difference between borrowing which is how many I can get from another priority level and lending",
    "start": "850560",
    "end": "856160"
  },
  {
    "text": "which is how many I can give away so after doing that uh the tited again",
    "start": "856160",
    "end": "863920"
  },
  {
    "text": "everybody was happy uh well except me because I'm still an SR and I don't like manual toil and I'd much rather like my",
    "start": "863920",
    "end": "871360"
  },
  {
    "text": "incident to be resolved automatically for me so how do we get from there",
    "start": "871360",
    "end": "877040"
  },
  {
    "text": "introducing more priority levels and I'll show you a little bit how we design uh other priority levels through this",
    "start": "877040",
    "end": "883360"
  },
  {
    "text": "example over here uh I think the demand set is quite interesting because this is one of the type of workload that can",
    "start": "883360",
    "end": "889360"
  },
  {
    "text": "easily overwhelm the control plane especially in cluster where we have large number of kubernetes nodes so we",
    "start": "889360",
    "end": "896880"
  },
  {
    "text": "were talking about selium earlier on let's introduce a priority level for the selium agent we have the flow schema",
    "start": "896880",
    "end": "904320"
  },
  {
    "text": "that is matching the service agent that is referring to the priority level uh",
    "start": "904320",
    "end": "910480"
  },
  {
    "text": "celium agent too uh and since we have a single user a single subject that is uh",
    "start": "910480",
    "end": "916240"
  },
  {
    "text": "there we only have one queue with a limit on the queue that is not to end",
    "start": "916240",
    "end": "921519"
  },
  {
    "text": "all the traffic from all the nodes of that cluster uh and we still have precast",
    "start": "921519",
    "end": "926959"
  },
  {
    "text": "that were denied no problem we just had more concerns and that did not really",
    "start": "926959",
    "end": "932880"
  },
  {
    "text": "help we were still facing those this throttling under normal conditions and",
    "start": "932880",
    "end": "938240"
  },
  {
    "text": "in that case the actual bottleneck was a single queue so we had to split over multiple cues so that the workers could",
    "start": "938240",
    "end": "945440"
  },
  {
    "text": "pick up uh through those different cues after that we were back to the",
    "start": "945440",
    "end": "951279"
  },
  {
    "text": "expected behavior under normal conditions no throttling and in case we had the CRD update then the throttling",
    "start": "951279",
    "end": "957120"
  },
  {
    "text": "would kick in and would paste uh the requests that were coming in not overloading the control plane and still",
    "start": "957120",
    "end": "962959"
  },
  {
    "text": "being able to serve requests from other users so you remember that we are running our",
    "start": "962959",
    "end": "969759"
  },
  {
    "text": "own control plane and can we do better than that like sure I can throttle but I",
    "start": "969759",
    "end": "977440"
  },
  {
    "text": "can also add more API servers and how would I do that uh and the priority uh",
    "start": "977440",
    "end": "984160"
  },
  {
    "text": "levels they they give the metrics of utilizations and one of the idea that we",
    "start": "984160",
    "end": "989920"
  },
  {
    "text": "could have had was to look at this utilization if it goes near 100% this is",
    "start": "989920",
    "end": "995279"
  },
  {
    "text": "basically at that moment that the throttling kicks in and you serve the 429 so what if when we go above certain",
    "start": "995279",
    "end": "1003360"
  },
  {
    "text": "threshold uh we add more API servers that can handle more traffic um to that",
    "start": "1003360",
    "end": "1009759"
  },
  {
    "text": "my recommendation is not to do that and the reason is because autoscaling is really for hardware capacity think about",
    "start": "1009759",
    "end": "1017279"
  },
  {
    "text": "CPU memory um network traffic network bandwidth if you need um but this is not",
    "start": "1017279",
    "end": "1025839"
  },
  {
    "text": "for this abstract limitation that we set on the priority level and also we can",
    "start": "1025839",
    "end": "1031839"
  },
  {
    "text": "have this utilization going up not correlating with the CPU utilization or the hardware utilization because of an",
    "start": "1031839",
    "end": "1038000"
  },
  {
    "text": "external dependency that that we have um we mentioned that we we had the",
    "start": "1038000",
    "end": "1044400"
  },
  {
    "text": "number of seats that were defined by the max precessing flight so we decided to",
    "start": "1044400",
    "end": "1050160"
  },
  {
    "text": "set a max precessing flight per core and by doing that and setting the flag max",
    "start": "1050160",
    "end": "1056240"
  },
  {
    "text": "precessing flight linear in the number of cores that we had on the APA server",
    "start": "1056240",
    "end": "1061679"
  },
  {
    "text": "we were able to correlate the priority level utilization and the CPU utilization going back to the regular",
    "start": "1061679",
    "end": "1069280"
  },
  {
    "text": "autosaling that we had on the API servers uh one thing that I want to",
    "start": "1069280",
    "end": "1074400"
  },
  {
    "text": "highlight here is that the API priority and fairness is all about throttling as",
    "start": "1074400",
    "end": "1079520"
  },
  {
    "text": "long as we have capacity on the API server we will serve requests successfully and it is not a good tool",
    "start": "1079520",
    "end": "1085520"
  },
  {
    "text": "to limit the number of resources that we have and I will hand over to Matio to tell us more about that thank",
    "start": "1085520",
    "end": "1093160"
  },
  {
    "text": "you so let's try to look at this from a different angle if you think about the",
    "start": "1093160",
    "end": "1099120"
  },
  {
    "text": "number of resources that you have in your cluster during the day you probably have a profile similar to this um maybe",
    "start": "1099120",
    "end": "1106320"
  },
  {
    "text": "some resources or some workload are scaling up and down depending on the number of users that comes to your",
    "start": "1106320",
    "end": "1111760"
  },
  {
    "text": "website during the day maybe you have jobs like Spark jobs that they process",
    "start": "1111760",
    "end": "1117039"
  },
  {
    "text": "in batch or maybe you have a more linear uh workloads but as I told you at the",
    "start": "1117039",
    "end": "1123039"
  },
  {
    "text": "beginning there are cases where you have human errors or a bad deployment with automation and then your number of",
    "start": "1123039",
    "end": "1130080"
  },
  {
    "text": "resources can grow uncapped and at some point you're going to reach some limit",
    "start": "1130080",
    "end": "1135120"
  },
  {
    "text": "if you have a cluster in an ad in an account uh then maybe you get throttled",
    "start": "1135120",
    "end": "1140559"
  },
  {
    "text": "by your cloud provider API and if you have multiple clusters in the same account you're impacting multiple",
    "start": "1140559",
    "end": "1146400"
  },
  {
    "text": "people's uh maybe you get out of IPs or maybe you can actually break the control plane so how we can prevent this to",
    "start": "1146400",
    "end": "1153200"
  },
  {
    "text": "happen well you can use the resource qu that is an object designed to limit the",
    "start": "1153200",
    "end": "1158480"
  },
  {
    "text": "consumption of resources per name space you can limit the number of resources for example you can say I want maxed",
    "start": "1158480",
    "end": "1165679"
  },
  {
    "text": "1,000 PS in space or number of config maps and then you have also the total",
    "start": "1165679",
    "end": "1171120"
  },
  {
    "text": "amount of resources maybe you want to limit the number of GPUs or CPU or memory and so in our uh case you can uh",
    "start": "1171120",
    "end": "1180640"
  },
  {
    "text": "you have cluster administrators or users that create the resource code object in the name space and when uh the user is",
    "start": "1180640",
    "end": "1188400"
  },
  {
    "text": "asking for more you get a 403 forbidden error in the error you also get back the",
    "start": "1188400",
    "end": "1193440"
  },
  {
    "text": "resource that you're about to violate so you can adjust and so either you go back to your cluster admin or you change the",
    "start": "1193440",
    "end": "1200320"
  },
  {
    "text": "resource quota and so if you apply this into the cluster this is what you get you apply the resource quota you have",
    "start": "1200320",
    "end": "1206160"
  },
  {
    "text": "the error on the automation or the human error and you start scaling up until you get the resource qu the problem here is",
    "start": "1206160",
    "end": "1214960"
  },
  {
    "text": "how do you set the limit where do you put the resource quota value if it's too little then you are very close to the",
    "start": "1214960",
    "end": "1221760"
  },
  {
    "text": "organic scale up of your surface maybe there is an incident there is a traffic shift in within the cluster you're",
    "start": "1221760",
    "end": "1227919"
  },
  {
    "text": "blocking a legitimate scale if you are setting it too high then well doesn't",
    "start": "1227919",
    "end": "1233679"
  },
  {
    "text": "solve you and doesn't help you at all because you still have the YouTube spike and even if you set it correctly this is",
    "start": "1233679",
    "end": "1239919"
  },
  {
    "text": "a static limit so maybe during the weeks during the months you get more users your traffic pattern changes and you",
    "start": "1239919",
    "end": "1247600"
  },
  {
    "text": "forgot to update it so what we did was that we took the resource quote as a building block and built a controller",
    "start": "1247600",
    "end": "1254480"
  },
  {
    "text": "that watch for the resource utilization and dynamically adjusted the quota for each nameace so uh in case of uh a resource",
    "start": "1254480",
    "end": "1264159"
  },
  {
    "text": "uh scaling during the day we for each name space we apply the resource quota",
    "start": "1264159",
    "end": "1269280"
  },
  {
    "text": "you see that we leave a buffer uh to allow uh for small uh uh immediate",
    "start": "1269280",
    "end": "1274400"
  },
  {
    "text": "upscale but we keep growing the quota up and down following the workload pattern",
    "start": "1274400",
    "end": "1280720"
  },
  {
    "text": "and then in case of an incident of a immediate uh spike of resources then we",
    "start": "1280720",
    "end": "1287520"
  },
  {
    "text": "grow a little but then we have a cool down period that is the period where we don't allow any more scale up and then",
    "start": "1287520",
    "end": "1294159"
  },
  {
    "text": "we keep doing this step uh profile in increasing the quota until we get to the",
    "start": "1294159",
    "end": "1300000"
  },
  {
    "text": "maximum limit that we define for the name space so at the moment we support",
    "start": "1300000",
    "end": "1305120"
  },
  {
    "text": "counting only the object because that is what we care about limiting the number of resources we support bots because",
    "start": "1305120",
    "end": "1312000"
  },
  {
    "text": "well they are actually the things that run in your clusters and config maps because we deploy with Helm and so we",
    "start": "1312000",
    "end": "1318640"
  },
  {
    "text": "have a lot of conf maps going around for every resource we define the",
    "start": "1318640",
    "end": "1324159"
  },
  {
    "text": "buffer that is the percentage that you can have above the current utilization so imagine that you have a 10 pod and",
    "start": "1324159",
    "end": "1331280"
  },
  {
    "text": "you define a 40% buffer you're getting a 14 pod squ then we defined a default",
    "start": "1331280",
    "end": "1337280"
  },
  {
    "text": "minimum this is because when you're working with percentage and you have a little number small numbers uh you can",
    "start": "1337280",
    "end": "1343360"
  },
  {
    "text": "have a very then small uh quota imagine that you have one pod with a 40% buffer",
    "start": "1343360",
    "end": "1349120"
  },
  {
    "text": "you have a 1.4 pods maybe you run it to two this doesn't get you anywhere so we",
    "start": "1349120",
    "end": "1355120"
  },
  {
    "text": "put the default minimum sensible for every name space and then we defined a maximum allowed code per name space",
    "start": "1355120",
    "end": "1362559"
  },
  {
    "text": "usually based on a six scalability threshold recommendation on uh based on our own experience and",
    "start": "1362559",
    "end": "1369799"
  },
  {
    "text": "observations and so uh the controller looks like this uh in every name space",
    "start": "1369799",
    "end": "1375039"
  },
  {
    "text": "we watch for pods and config map and then we manage the life cycle on a resource qu object and we increase and",
    "start": "1375039",
    "end": "1381600"
  },
  {
    "text": "decrease the limits depending on the cluster utilization of course now this is this controller is on the critical",
    "start": "1381600",
    "end": "1387919"
  },
  {
    "text": "path when there is a problem so we wanted to allow our user to selfs serve in case there is an incident we want to",
    "start": "1387919",
    "end": "1394240"
  },
  {
    "text": "get page in the middle of the night so what they can do is they can annotate the resource quote object and the",
    "start": "1394240",
    "end": "1400400"
  },
  {
    "text": "controller will immediately uh set the hard limit to the maximum allowed giving",
    "start": "1400400",
    "end": "1406559"
  },
  {
    "text": "uh high room for increase and then we also set a seven days time to leave to",
    "start": "1406559",
    "end": "1411760"
  },
  {
    "text": "the annotation this is because 7 days is almost enough is always enough to",
    "start": "1411760",
    "end": "1416960"
  },
  {
    "text": "resolve an incident or to finish an investigation but then we don't want the users to forget to opt in again to the",
    "start": "1416960",
    "end": "1424240"
  },
  {
    "text": "to the controllers and so we are going to go and clean up the notation for for themsel uh of course we also have a",
    "start": "1424240",
    "end": "1431679"
  },
  {
    "text": "special admin uh annotation where we annotate the cube system name space and what it does is going to trigger the",
    "start": "1431679",
    "end": "1438080"
  },
  {
    "text": "controller to delete all the manage resource quota across all name spaces basically restoring the cluster in a",
    "start": "1438080",
    "end": "1443840"
  },
  {
    "text": "state before resource quota um a couple of things that we learn while uh developing the controller",
    "start": "1443840",
    "end": "1451200"
  },
  {
    "text": "that I think are interesting to share first one is we deploy this this controller in a cluster and we look at",
    "start": "1451200",
    "end": "1457279"
  },
  {
    "text": "the memory utilization is massive uh this is a cluster with around 40,000",
    "start": "1457279",
    "end": "1462720"
  },
  {
    "text": "config maps and that is exactly the problem because when you're listing the resource to count them you are then",
    "start": "1462720",
    "end": "1468640"
  },
  {
    "text": "opening a watch and you get the full object object back uh and when you are",
    "start": "1468640",
    "end": "1474720"
  },
  {
    "text": "uh using helm this can be very expensive for the config maps actually what we really care about is the number of",
    "start": "1474720",
    "end": "1480960"
  },
  {
    "text": "objects so what we can do is we can uh go in the meta v1 package on the API",
    "start": "1480960",
    "end": "1487440"
  },
  {
    "text": "machinery use a partial object metadata list that is only giving the metadata",
    "start": "1487440",
    "end": "1492960"
  },
  {
    "text": "for for the object uh enough to know how many you have and you can see for the",
    "start": "1492960",
    "end": "1498799"
  },
  {
    "text": "graph we are using almost like uh uh more than four time less memory on this side uh and the second thing is remember",
    "start": "1498799",
    "end": "1505440"
  },
  {
    "text": "that I told you that we annotate cube system and we delete all the all the resource quota we do that by triggering",
    "start": "1505440",
    "end": "1512320"
  },
  {
    "text": "a reconciliation for every name space and so how do you generate events from inside your reconciliation loop well to",
    "start": "1512320",
    "end": "1519440"
  },
  {
    "text": "do that you can use um a source channel so you basically can create a channel",
    "start": "1519440",
    "end": "1526559"
  },
  {
    "text": "that listen for generic events and when you are setting up your controller you can pass a row source and you can uh you",
    "start": "1526559",
    "end": "1533360"
  },
  {
    "text": "can watch on this channel and from inside the reconciliation loop every time you want to trigger an event you",
    "start": "1533360",
    "end": "1538880"
  },
  {
    "text": "can send uh the the object that you want over the channel so the controller is ready we",
    "start": "1538880",
    "end": "1545600"
  },
  {
    "text": "deploy it look at the graph high- five everyone green lines pods going up and",
    "start": "1545600",
    "end": "1551279"
  },
  {
    "text": "down red line the resource qu you can also see that when we go beyond the",
    "start": "1551279",
    "end": "1556559"
  },
  {
    "text": "limit there is this flat line that is the minimum resource quota that I was telling you uh and so we don't never set",
    "start": "1556559",
    "end": "1563200"
  },
  {
    "text": "the quota below that that limit problem is uh we start getting pinged by the",
    "start": "1563200",
    "end": "1568880"
  },
  {
    "text": "users hey Mateo I try to deploy my config maps I get four or three errors h",
    "start": "1568880",
    "end": "1575039"
  },
  {
    "text": "well that is because as you know the config map doesn't have a controller so if you try to create a deployment when",
    "start": "1575039",
    "end": "1582000"
  },
  {
    "text": "you have a resource quota let's say your resource quota five you deployment is for 10 pods with 10 replicas the replica",
    "start": "1582000",
    "end": "1589440"
  },
  {
    "text": "set controller is going to give you the first five and then it's going to wait and try to reconcile until you increase the quota and that works you don't do",
    "start": "1589440",
    "end": "1595760"
  },
  {
    "text": "that for the config maps and also helm and usually other components they don't retry uh on config map so what you can",
    "start": "1595760",
    "end": "1603440"
  },
  {
    "text": "do you can have a different minimum qu for config maps at the end of the day they are less disrupted them pods and so",
    "start": "1603440",
    "end": "1611600"
  },
  {
    "text": "uh at the beginning we had a single buffer and single values for all the",
    "start": "1611600",
    "end": "1616720"
  },
  {
    "text": "resources then we split them in different uh groups and the second thing that you can do can uh instead of having",
    "start": "1616720",
    "end": "1623600"
  },
  {
    "text": "the same buffer to go up and down you can split it in uh a different cool down",
    "start": "1623600",
    "end": "1628880"
  },
  {
    "text": "period so you can scale up pretty fast and then scale down maybe more slowly to",
    "start": "1628880",
    "end": "1634000"
  },
  {
    "text": "try to prevent and try to catch a sudden spike that is going beyond so we solve",
    "start": "1634000",
    "end": "1639120"
  },
  {
    "text": "this i5 again and then again the users hey mateo I'm trying to deploy my pod or",
    "start": "1639120",
    "end": "1645039"
  },
  {
    "text": "the config map what is this managed resource quota why am I getting a conflict I'm not updating the resource",
    "start": "1645039",
    "end": "1650440"
  },
  {
    "text": "quota and so here the problem is that uh uh there is an upstream issue that has",
    "start": "1650440",
    "end": "1656720"
  },
  {
    "text": "been open for a while since 2018 where uh uh people are complaining that when",
    "start": "1656720",
    "end": "1662640"
  },
  {
    "text": "they try to create a different resources they get this error and if you look at the very long thread the solution",
    "start": "1662640",
    "end": "1668159"
  },
  {
    "text": "solution is that on the client side what we do is people is parsing the error and",
    "start": "1668159",
    "end": "1673440"
  },
  {
    "text": "if the error has the string conflict on resource quota 409 conflict they retry",
    "start": "1673440",
    "end": "1679440"
  },
  {
    "text": "because hey it's upstream bug but what if you don't have the control of over the client you can't go to your customer",
    "start": "1679440",
    "end": "1685840"
  },
  {
    "text": "and say hey yeah why don't you fork the controller and you add the retries so first of all let's try to look at what",
    "start": "1685840",
    "end": "1692799"
  },
  {
    "text": "the what the problem is so the error is coming for the resource quot admission controller What happens is when you have",
    "start": "1692799",
    "end": "1699120"
  },
  {
    "text": "a resource squad enabled when you are trying to create a resource you are going to go through the admission the controller does all the checks that it",
    "start": "1699120",
    "end": "1706000"
  },
  {
    "text": "needs to do to see if you are allowed to add an object and then it tries to update the resource quote object itself",
    "start": "1706000",
    "end": "1713120"
  },
  {
    "text": "up to hardcoded three times and what happens is that sometimes you get an error in trying to update because you",
    "start": "1713120",
    "end": "1720159"
  },
  {
    "text": "you have a stale version of the object and so you try to update it up to three times and where are no more retries you",
    "start": "1720159",
    "end": "1726799"
  },
  {
    "text": "bubble back to the user 409 conflict error even if there would be enough room to create the resources and so what we",
    "start": "1726799",
    "end": "1734399"
  },
  {
    "text": "did was that well yeah we cheated and we said well if there's a conflict don't count it as a retry and keep retrying if",
    "start": "1734399",
    "end": "1741360"
  },
  {
    "text": "you're getting a 409 conflict so you can get a fresh version of the object and try to update",
    "start": "1741360",
    "end": "1747159"
  },
  {
    "text": "again the thing is we are still respecting the 10 seconds timeout so if you look if you look at the uh admission",
    "start": "1747159",
    "end": "1754159"
  },
  {
    "text": "quotota resource quota workers they try to calculate the quota and then they cut",
    "start": "1754159",
    "end": "1759600"
  },
  {
    "text": "off at the second timeout and then we went also and look at how much we were",
    "start": "1759600",
    "end": "1765760"
  },
  {
    "text": "increasing the admission duration but we didn't see any significant increase in that and we were able to fix for",
    "start": "1765760",
    "end": "1774039"
  },
  {
    "text": "9 so are we ready with this kind of work well not yet first of all we want to get",
    "start": "1774039",
    "end": "1779279"
  },
  {
    "text": "a better upscaling algorithm uh I show you that we scale up based on a on a",
    "start": "1779279",
    "end": "1784559"
  },
  {
    "text": "buffer uh calculate based on the current uh resource utilization maybe we want to",
    "start": "1784559",
    "end": "1790240"
  },
  {
    "text": "look at the past behavior of the workloads maybe something like the exponential moving average from uh the",
    "start": "1790240",
    "end": "1796320"
  },
  {
    "text": "vertical pod autoscaler uh we want to submit an upstream PR for the open issue we are",
    "start": "1796320",
    "end": "1802720"
  },
  {
    "text": "aware that our approach might be a little too naive for upstream so uh maybe we're going to look at swapping",
    "start": "1802720",
    "end": "1809279"
  },
  {
    "text": "the queue with the rate limited queue or see if we can add some gter and then uh we want to uh finalize the user",
    "start": "1809279",
    "end": "1815679"
  },
  {
    "text": "experience about the maximum limit because when you say this name space is not allowed to scale more than x you",
    "start": "1815679",
    "end": "1822000"
  },
  {
    "text": "need to go to the user and explain what to do what if is an incident how do you shift traffic between between clusters",
    "start": "1822000",
    "end": "1828960"
  },
  {
    "text": "what if they were above the threshold so you were not limiting them and then they split the workload in multiple namespace",
    "start": "1828960",
    "end": "1834960"
  },
  {
    "text": "now they are on the quota and things like that so I hope that today you learn",
    "start": "1834960",
    "end": "1840399"
  },
  {
    "text": "a few things api priority and fairness are good to help you uh limiting the control plane load they help you also to",
    "start": "1840399",
    "end": "1848640"
  },
  {
    "text": "uncover best practice i has explained to you about like demon set shenanigen and",
    "start": "1848640",
    "end": "1854000"
  },
  {
    "text": "then uh well they both good to start but they are it's very easy to do like a",
    "start": "1854000",
    "end": "1860960"
  },
  {
    "text": "sort of hello world you first utilization with them but you need then to plan to spend some time to tweak them",
    "start": "1860960",
    "end": "1867120"
  },
  {
    "text": "for for your actual needs and there are still few upstream bugs and missing features uh there is the one that I told",
    "start": "1867120",
    "end": "1874159"
  },
  {
    "text": "you about the resource quota but also uh missing metrics or better metrics for",
    "start": "1874159",
    "end": "1879440"
  },
  {
    "text": "borrow borrowing in API priority and fairness uh that they would help you to uncover even more issues so we are at",
    "start": "1879440",
    "end": "1888159"
  },
  {
    "text": "time thank you very much for listening",
    "start": "1888159",
    "end": "1892000"
  }
]