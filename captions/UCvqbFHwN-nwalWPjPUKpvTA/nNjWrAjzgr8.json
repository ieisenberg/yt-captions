[
  {
    "start": "0",
    "end": "105000"
  },
  {
    "text": "people are still filing in but I think we can get started so good morning everyone thanks for making out the multi",
    "start": "199",
    "end": "7109"
  },
  {
    "text": "cluster track first thing on the first day of cube con my name is Alex and hopefully you're all expecting me to be",
    "start": "7109",
    "end": "12960"
  },
  {
    "text": "talking about running a system across multiple cabrillo these clusters because that's what I'm here to do and for",
    "start": "12960",
    "end": "18660"
  },
  {
    "text": "anyone who's been using kubernetes for a number of years since the early days you've probably noticed how much easier",
    "start": "18660",
    "end": "24660"
  },
  {
    "text": "it's gotten to run your applications on kubernetes you couldn't have run a stateful system or a complicated",
    "start": "24660",
    "end": "30210"
  },
  {
    "text": "distributed system in kubernetes very reliably just as little as like three years ago all these great features have been added",
    "start": "30210",
    "end": "35880"
  },
  {
    "text": "for managing disks for managing stateful applications for running a cluster that spans multiple zones or for running your",
    "start": "35880",
    "end": "42809"
  },
  {
    "text": "kubernetes cluster for you there are also been improvements in packaging your applications so that other people can install them on their",
    "start": "42809",
    "end": "48660"
  },
  {
    "text": "clusters with things like home charts and operators but not much progress has been made in that same time in making it",
    "start": "48660",
    "end": "54750"
  },
  {
    "text": "easy to run applications that span across multiple kubernetes clusters so it's very easy to look at the docs and",
    "start": "54750",
    "end": "60870"
  },
  {
    "text": "figure out how to run something in one cluster it's very hard to look at the kubernetes Docs and figure out how to run an application that has some nodes",
    "start": "60870",
    "end": "66540"
  },
  {
    "text": "in one cluster over here and some nodes in one cluster over there it's a much tougher problem that is not really",
    "start": "66540",
    "end": "72210"
  },
  {
    "text": "written down anywhere it's not clear where to get started the same is true if you're trying to run across multiple clouds you know it's very tough to",
    "start": "72210",
    "end": "79020"
  },
  {
    "text": "figure out how to run an application that spans from one cloud to another or from one cloud to your on-prem environment and so there have been some",
    "start": "79020",
    "end": "86369"
  },
  {
    "text": "efforts over time there was the uber Nettie's project a couple years ago that didn't really seem to go anywhere and there was the Federation v2 project",
    "start": "86369",
    "end": "92939"
  },
  {
    "text": "that's still ongoing that we have high hopes for that should make some of this easier but maybe people would argue that",
    "start": "92939",
    "end": "99150"
  },
  {
    "text": "this hasn't improved just because it's not an important problem because no one wants to run their applications across regions but I'd propose that there are",
    "start": "99150",
    "end": "105600"
  },
  {
    "start": "105000",
    "end": "105000"
  },
  {
    "text": "at least a few reasons why you'd want to do this one obvious reason is latency so more services than ever span across",
    "start": "105600",
    "end": "112770"
  },
  {
    "text": "multiple regions and serve users all around the world and if you want to serve users all around the world if",
    "start": "112770",
    "end": "118259"
  },
  {
    "text": "you're running just one single region users on the other side of the world are necessarily going to have a really long",
    "start": "118259",
    "end": "123270"
  },
  {
    "text": "delay whenever they want to use your service so the solution to that is to put some instances of your service",
    "start": "123270",
    "end": "128489"
  },
  {
    "text": "closer to them so run some of your service in the US some in Europe some in Asia or if you're just running with you",
    "start": "128489",
    "end": "133720"
  },
  {
    "text": "one o'clock in one country span the country both coasts you might also run",
    "start": "133720",
    "end": "139210"
  },
  {
    "text": "across multiple clusters in multiple regions for fault tolerance so if a single region goes down you don't want",
    "start": "139210",
    "end": "144610"
  },
  {
    "text": "your entire application to go down you might also do this for bureaucratic reasons so if you were you know signed",
    "start": "144610",
    "end": "150820"
  },
  {
    "text": "up and got your badge this morning you probably notice the gdpr signs there are laws springing up in countries around the world that require you to store",
    "start": "150820",
    "end": "156520"
  },
  {
    "text": "their users data in certain places often in their own countries it's tough to do this if you're storing all of your data",
    "start": "156520",
    "end": "161860"
  },
  {
    "text": "in one place that isn't that country and so I would do want to talk today about",
    "start": "161860",
    "end": "167590"
  },
  {
    "text": "this experience of running an application this bans multiple clusters why it's hard what sort of things you",
    "start": "167590",
    "end": "172660"
  },
  {
    "text": "need to know about kubernetes in order to get started and then what kinds of solutions there are out there for this",
    "start": "172660",
    "end": "177700"
  },
  {
    "text": "problem and before we move on here who here is tried writing an application",
    "start": "177700",
    "end": "182740"
  },
  {
    "text": "across multiple clusters before and who here is doing it in production okay so",
    "start": "182740",
    "end": "189700"
  },
  {
    "text": "not very many people in either group I hope that will be a much higher number over the next year or two I think things",
    "start": "189700",
    "end": "195580"
  },
  {
    "text": "are gonna continue improving and I should say that I'm not here to prescribe a particular solution I just",
    "start": "195580",
    "end": "200980"
  },
  {
    "text": "want to talk about things that I've tried things that I've seen users try things that I've seen run in production and what my experiences have been with",
    "start": "200980",
    "end": "206530"
  },
  {
    "text": "them I don't have a single golden solution for anyone to walk away with from here today and just so that you",
    "start": "206530",
    "end": "213250"
  },
  {
    "text": "know where I'm coming from my experience with kubernetes is that I worked on the kubernetes and gke team back in 2014 for",
    "start": "213250",
    "end": "219400"
  },
  {
    "text": "a little over two years I worked all across the GAE stack and contributed to open source things like logging",
    "start": "219400",
    "end": "224440"
  },
  {
    "text": "monitoring networking cloud provider integrations since then I've been at cockroach labs working on the open",
    "start": "224440",
    "end": "230110"
  },
  {
    "text": "source sequel cockroach TV database and in addition to working on the database itself I've let all of their container",
    "start": "230110",
    "end": "235450"
  },
  {
    "text": "related efforts so making sure that users can take a configuration off the shelf and easily run us in their",
    "start": "235450",
    "end": "241030"
  },
  {
    "text": "orchestration system of choice I've helped them run it in different clouds from a single zone to multiple regions",
    "start": "241030",
    "end": "246760"
  },
  {
    "text": "and have also had to help people with their own configurations because they didn't want to use one of ARDS they made",
    "start": "246760",
    "end": "251800"
  },
  {
    "text": "their own ran into some trouble so I get to see different ways of running things even beyond the ones that I would necessarily recommend and we have a",
    "start": "251800",
    "end": "259120"
  },
  {
    "text": "number of users using cockroach TV in kubernetes both in a single cluster and spanning multiple clusters today",
    "start": "259120",
    "end": "264430"
  },
  {
    "text": "so this is something that people are doing in production in yes but the initial question need to ask",
    "start": "264430",
    "end": "272080"
  },
  {
    "text": "is why this is hard at all I've talked to many people who have run a system in a single kubernetes cluster and assume",
    "start": "272080",
    "end": "278620"
  },
  {
    "text": "that it's gonna be very easy to span the multiple kubernetes clusters because it was easy to do so in the one cluster and Cooper Nettie's is supposed to make it",
    "start": "278620",
    "end": "284650"
  },
  {
    "text": "easy to orchestrate services right but to beat to make things clear I'm using",
    "start": "284650",
    "end": "290890"
  },
  {
    "start": "287000",
    "end": "287000"
  },
  {
    "text": "multi region and multi cluster pretty much as synonyms because you're never recommended to run a single kubernetes",
    "start": "290890",
    "end": "296140"
  },
  {
    "text": "cluster that spans across multiple regions in the early days you weren't even supposed to do it across multiple",
    "start": "296140",
    "end": "301330"
  },
  {
    "text": "zones within the same region nowadays you can do that with what's called a multi zone cluster but if you want to",
    "start": "301330",
    "end": "306880"
  },
  {
    "text": "span across multiple regions you're really going to want to use multiple clusters we run a single cluster across regions at your own risk",
    "start": "306880",
    "end": "312520"
  },
  {
    "text": "kubernetes hasn't been designed to deal with the latency differences between nodes and other complications but to get",
    "start": "312520",
    "end": "321130"
  },
  {
    "text": "started with trying to you know set up this multi region application and and",
    "start": "321130",
    "end": "326490"
  },
  {
    "text": "learn how to configure it in kubernetes you need to understand like what do these systems need and as our example",
    "start": "326490",
    "end": "332320"
  },
  {
    "text": "today we're gonna be looking at cockroach DB just cuz it's the one that I'm most familiar with it's what I've worked the most with and there's a very",
    "start": "332320",
    "end": "338260"
  },
  {
    "text": "quick landslide refresher it's an open source distributed sequel database it speaks the postcards wire protocol your",
    "start": "338260",
    "end": "344290"
  },
  {
    "text": "application can talk to any of the one or more processes in your cockroach cluster and get their sequel responses",
    "start": "344290",
    "end": "349510"
  },
  {
    "text": "back internally it's doing all sorts of work to divide your data up into ranges replicate it across the various nodes",
    "start": "349510",
    "end": "355390"
  },
  {
    "text": "rebalance as nodes are added to or removed from the cluster and it you know internally uses consensus and a",
    "start": "355390",
    "end": "361420"
  },
  {
    "text": "transaction protocol to ensure you get fully acid transactions it also offers some geographic features such that you",
    "start": "361420",
    "end": "367960"
  },
  {
    "text": "can tell it which locations you want it to put data in or which locations you don't want it to put data in but the way",
    "start": "367960",
    "end": "373660"
  },
  {
    "text": "that you run it is that you just take the single binary start it on a number of machines point them at each other",
    "start": "373660",
    "end": "378730"
  },
  {
    "text": "such that they can find each other and then it just acts as a single homogeneous cluster of nodes there are no special roles to worry about so we",
    "start": "378730",
    "end": "384610"
  },
  {
    "text": "don't need to worry about that in our configurations you just run a bunch of cockroach nodes and make sure they can all talk to each other and so when",
    "start": "384610",
    "end": "391930"
  },
  {
    "text": "running in kubernetes cockroach requires roughly three things and these are basically the same three things that any application that manages state needs",
    "start": "391930",
    "end": "399599"
  },
  {
    "text": "so it has to have some sort of persistent storage that will be there across restarts of the pod or of the node has to have the ability to",
    "start": "399599",
    "end": "407129"
  },
  {
    "text": "communicate directly with every other cockroach node in the cluster and it needs some sort of network address that can survive restarts the last one isn't",
    "start": "407129",
    "end": "414599"
  },
  {
    "text": "always needed for cockroach but it's needed for most stateful systems so we can leave it in here and you probably notice if you've used April sets before",
    "start": "414599",
    "end": "420899"
  },
  {
    "text": "this is what stateful sets are for so if you're in a single cluster you get all these things built into kubernetes and",
    "start": "420899",
    "end": "426179"
  },
  {
    "text": "it works really well it's been hardened over the last couple years but if you're running across multiple communities",
    "start": "426179",
    "end": "431520"
  },
  {
    "text": "clusters that's when you start to lose these things that most stateful systems need you lose the ability to have a",
    "start": "431520",
    "end": "437339"
  },
  {
    "text": "network address that survives or restarts that's usable from everywhere and in many clusters you'll lose that",
    "start": "437339",
    "end": "442589"
  },
  {
    "text": "ability to talk directly from every node to every other node regardless of where they're running and it all comes down to",
    "start": "442589",
    "end": "449639"
  },
  {
    "text": "networking then because these are both networking problems we need pod - pod communication even across clusters and",
    "start": "449639",
    "end": "456899"
  },
  {
    "text": "when applicable across things like V pcs or other private networks and we also",
    "start": "456899",
    "end": "462389"
  },
  {
    "text": "need that address that can be used both within and between clusters you could optimize this and say you're gonna have",
    "start": "462389",
    "end": "468569"
  },
  {
    "text": "a separate address for communication within and a separate address for communication between but to keep things simple we'll just assume we want one",
    "start": "468569",
    "end": "473939"
  },
  {
    "text": "address that works in both cases so kubernetes networking is the problem but",
    "start": "473939",
    "end": "480809"
  },
  {
    "text": "why is it the problem since the beginning kubernetes has required a few things in order for an installation of",
    "start": "480809",
    "end": "486689"
  },
  {
    "start": "481000",
    "end": "481000"
  },
  {
    "text": "kubernetes to be compliance it has to ensure that every pod has its own unique IP address that no other pod has that IP",
    "start": "486689",
    "end": "494519"
  },
  {
    "text": "address has to look the same to the pod itself as it does to all pods that that pod is talking to and this has to work",
    "start": "494519",
    "end": "500819"
  },
  {
    "text": "without any form of network address translation this might seem very straightforward but at the time",
    "start": "500819",
    "end": "506579"
  },
  {
    "text": "kubernetes was released it was pretty revolutionary very much against the current ways of thinking it was different from how doctor-recommended",
    "start": "506579",
    "end": "511860"
  },
  {
    "text": "doing networking it was different from how Google did networking internally with Borg and so there weren't really",
    "start": "511860",
    "end": "517500"
  },
  {
    "text": "many ways of implementing this actually it was hard to set up a cluster in the early days because there was no way of providing this for the pods that you",
    "start": "517500",
    "end": "523800"
  },
  {
    "text": "were running you know we wanted to run a lot of pods on each machine but each machine in a traditional network is only",
    "start": "523800",
    "end": "529439"
  },
  {
    "text": "given one or two IP addresses and so since then a ton of solutions",
    "start": "529439",
    "end": "536040"
  },
  {
    "text": "have been built up to satisfy these requirements if you look at the kubernetes networking documentation page you'll see more than 20 different",
    "start": "536040",
    "end": "541830"
  },
  {
    "text": "solutions listed and it's not a complete list there are a lot of ways that this gets done out there in the wild and because the only requirements that were",
    "start": "541830",
    "end": "548940"
  },
  {
    "text": "specified were how a single cluster should work the multi cluster case is basically undefined behavior and it's up",
    "start": "548940",
    "end": "555570"
  },
  {
    "text": "to implementation details of each different networking solution and so this is kind of like if you're writing a",
    "start": "555570",
    "end": "560850"
  },
  {
    "text": "C program as undefined behavior in it and it works on one compiler and one you know CPU architecture but then you try",
    "start": "560850",
    "end": "566400"
  },
  {
    "text": "to compile it for another architecture and you find that your program explodes that's kind of the situation of trying",
    "start": "566400",
    "end": "571440"
  },
  {
    "text": "to run a multi cluster application across different clusters and you know",
    "start": "571440",
    "end": "577380"
  },
  {
    "text": "there are all sorts of these networking solutions out there this is just a small sample of some popular ones you'll notice all the cloud providers do things",
    "start": "577380",
    "end": "583320"
  },
  {
    "text": "their own way in a way that integrates nicely into their virtual cloud networks there are a bunch of software solutions",
    "start": "583320",
    "end": "588390"
  },
  {
    "text": "out there that do things in different ways as well and people have to figure out how to do this on prim in a number",
    "start": "588390",
    "end": "593400"
  },
  {
    "text": "of different ways often using some of the software solutions off-the-shelf but maybe doing things in hardware or in a",
    "start": "593400",
    "end": "599370"
  },
  {
    "text": "custom way and again these all work differently so there's not much you can depend upon working exactly the same if",
    "start": "599370",
    "end": "604620"
  },
  {
    "text": "you have previously used one networking solution and are trying to set up your application on a different one and you",
    "start": "604620",
    "end": "611700"
  },
  {
    "text": "can kind of group them into categories though so you can at least do that some of them support pada pada communication",
    "start": "611700",
    "end": "616740"
  },
  {
    "text": "across clusters so if you're running on gke or Amazon's eks as of earlier this year for eks you can talk directly from",
    "start": "616740",
    "end": "624420"
  },
  {
    "text": "one pod and one cluster to a potted another cluster as long as they're in the same V PC or you've done some sort",
    "start": "624420",
    "end": "629430"
  },
  {
    "text": "of pairing so this is great you don't have to think about it much when you're in these environments others don't have",
    "start": "629430",
    "end": "635550"
  },
  {
    "text": "a nun by default but make it kind of easy to enable so edgers default basic networking mode doesn't support it but",
    "start": "635550",
    "end": "641340"
  },
  {
    "text": "if you enable their advanced networking mode which is just like a checkbox when you create your cluster then you get it",
    "start": "641340",
    "end": "646890"
  },
  {
    "text": "cilium which is one of those open source software solutions for providing kubernetes networking hence a mode",
    "start": "646890",
    "end": "652200"
  },
  {
    "text": "called a cluster mesh where you can configure it to enable this but it won't do it by default but for many other",
    "start": "652200",
    "end": "657600"
  },
  {
    "text": "software solutions it's pretty difficult to get this pod to pod connectivity so this is Ennis like especially a problem if you're running on Prem clusters or if",
    "start": "657600",
    "end": "664230"
  },
  {
    "text": "you aren't in Google Amazon and we don't have enough time today to fully cover how all this works",
    "start": "664230",
    "end": "669960"
  },
  {
    "text": "in fact we don't have much time at all so if you want to learn more about this check out some past coop contacts in",
    "start": "669960",
    "end": "675660"
  },
  {
    "text": "particular I'd call out the life of a packet talk by Michael Rubin a couple years ago in Cuba on Europe if you want",
    "start": "675660",
    "end": "681840"
  },
  {
    "text": "to get a refresher on how some of these different solutions actually route packets through a network and so you",
    "start": "681840",
    "end": "688440"
  },
  {
    "text": "know what can we do we know the problem is that networking doesn't provide for direct communication between clusters",
    "start": "688440",
    "end": "693930"
  },
  {
    "text": "sometimes and it never actually gives us an address that we can reliably use between clusters and I tried a whole",
    "start": "693930",
    "end": "702060"
  },
  {
    "start": "700000",
    "end": "700000"
  },
  {
    "text": "bunch of things out earlier this year in the first half of the year try again doesn't look maybe 10 different solutions tried to get them working",
    "start": "702060",
    "end": "708600"
  },
  {
    "text": "checked out how reliable they were when things failed I you know did some thought experiments on different ways",
    "start": "708600",
    "end": "714720"
  },
  {
    "text": "they could fail that I didn't actually try out and unfortunately there wasn't like a single great solution they all",
    "start": "714720",
    "end": "720090"
  },
  {
    "text": "have a bunch of problems and some of them might have improved over the course of the year but for the most part things",
    "start": "720090",
    "end": "726120"
  },
  {
    "text": "are about the same so they either didn't work in certain environments at all or they're easy to break if the operator",
    "start": "726120",
    "end": "731640"
  },
  {
    "text": "makes a mistake or they're a little slower in the data path and so you're not gonna get the performance you expect out of your system or they're tough to",
    "start": "731640",
    "end": "738570"
  },
  {
    "text": "use with TLS certificates or in some case they're you're using a third party piece of software that isn't quite ready",
    "start": "738570",
    "end": "743580"
  },
  {
    "text": "yet to use in production for this usage and I will call out that my perspective",
    "start": "743580",
    "end": "748740"
  },
  {
    "text": "on this is likely to be a little different from some of yours because my goal is to come up with configurations that work for as many people as possible",
    "start": "748740",
    "end": "754530"
  },
  {
    "text": "with as little expertise needed on their end as possible if you are a kubernetes expert and you have like a single stable",
    "start": "754530",
    "end": "760350"
  },
  {
    "text": "environment that you want to get things working in your requirements will be slightly different from mine because I'm",
    "start": "760350",
    "end": "765480"
  },
  {
    "text": "trying to let everyone to run cockroach across multiple clusters whereas you might just want to run something across",
    "start": "765480",
    "end": "770520"
  },
  {
    "text": "your own clusters in your own certain environment and I'm also not here to prescribe a particular answer again so",
    "start": "770520",
    "end": "776550"
  },
  {
    "text": "take take out of this what you can so I'm going to walk through a few different solutions that were the best",
    "start": "776550",
    "end": "782820"
  },
  {
    "text": "out of the ones that I tried and for all of them assume that I'm running just a stateful set in each of the separate",
    "start": "782820",
    "end": "789030"
  },
  {
    "text": "kubernetes clusters and the problem is to connect the pods from those different clusters together but in each cluster you know your pods",
    "start": "789030",
    "end": "795600"
  },
  {
    "text": "and your disks and your IP addresses will all be managed by the staple side except for in this first solution here",
    "start": "795600",
    "end": "801560"
  },
  {
    "text": "so in this first solution you're basically like getting around all the",
    "start": "801560",
    "end": "807330"
  },
  {
    "text": "kubernetes magic you're just ignoring it you're saying I don't want your scheduling I don't want your networking just run me",
    "start": "807330",
    "end": "813750"
  },
  {
    "text": "on these machines and let me use the host machines network so this is very similar to just running processes directly on VMs without even using",
    "start": "813750",
    "end": "820560"
  },
  {
    "text": "kubernetes because you're picking which processes are running on which machines and you're letting them use the machines Network so this works if you don't have",
    "start": "820560",
    "end": "827910"
  },
  {
    "text": "pod - pod connectivity across clusters because even in those cases you'll usually be able to talk from one machine and one cluster to all the machines and",
    "start": "827910",
    "end": "834360"
  },
  {
    "text": "the other cluster or you can least set that up pretty easily so what you do is you either assign pods directly to nodes",
    "start": "834360",
    "end": "841080"
  },
  {
    "text": "by creating a replication controller of size one and putting a node constraint on it or you use a daemon set and that",
    "start": "841080",
    "end": "848580"
  },
  {
    "text": "way you get cockroach running on all the nodes and then you use the hosts network setting which is like passing - - net",
    "start": "848580",
    "end": "854910"
  },
  {
    "text": "equals host to all the docker containers it lets them use the hosts IP address so",
    "start": "854910",
    "end": "860640"
  },
  {
    "text": "this also makes some people happy who are otherwise skeptical of running stateful workloads in kubernetes because",
    "start": "860640",
    "end": "865830"
  },
  {
    "text": "it removes some of the magic that makes running stateful workloads a little scarier because you're not getting things being rescheduled underneath you",
    "start": "865830",
    "end": "871890"
  },
  {
    "text": "you can choose to use the machines local disks if you want rather than using network attached storage because you know the pods are always going to be",
    "start": "871890",
    "end": "877230"
  },
  {
    "text": "running on the same machine and so this is this might sound kind of silly like you're getting rid of all the kubernetes magic but it's a very reliable",
    "start": "877230",
    "end": "882930"
  },
  {
    "text": "dependable way of doing it and so it works even when pod to cloud communication doesn't it has zero moving",
    "start": "882930",
    "end": "889560"
  },
  {
    "text": "parts or even some of the parts that are built into kubernetes are no longer being used and the only issue is that",
    "start": "889560",
    "end": "896370"
  },
  {
    "text": "you need to make sure the machines in your cluster aren't changing a lot their IP addresses aren't changing they aren't being cycled in out of the cluster and",
    "start": "896370",
    "end": "902220"
  },
  {
    "text": "this could be a problem in certain cloud environments so managed service offerings like gke and eks do node",
    "start": "902220",
    "end": "909570"
  },
  {
    "text": "upgrades by just removing the VMS and replacing them with new ones and that means the IP addresses go away if you're using local disks those go away and so",
    "start": "909570",
    "end": "917040"
  },
  {
    "text": "this is something you might not want to do in the cloud unless you've taken special precautions but if you're",
    "start": "917040",
    "end": "922170"
  },
  {
    "text": "running on Prem this is great because you don't have to you know worry about connecting the networks of the multiple clusters and you can be confident those",
    "start": "922170",
    "end": "927870"
  },
  {
    "text": "machines are gonna stay there because they're physical machines in your own data centers this does require a bit of manual setup",
    "start": "927870",
    "end": "934410"
  },
  {
    "text": "to get working and it can use up ports on the host machines so you can't run like multiple cockroach instances on the",
    "start": "934410",
    "end": "939750"
  },
  {
    "text": "same machine because it's using the hosts ports but this is a surprisingly good very easy to configure way of doing",
    "start": "939750",
    "end": "945900"
  },
  {
    "text": "things if you're on prim if you are running in the cloud though you might",
    "start": "945900",
    "end": "951390"
  },
  {
    "start": "949000",
    "end": "949000"
  },
  {
    "text": "use the external load balancer feature and you might laugh at this but we have a couple of customers doing this in",
    "start": "951390",
    "end": "956820"
  },
  {
    "text": "production and it's been working reasonably well for them this is the most universally feasible way because it does not require pod to pod connectivity",
    "start": "956820",
    "end": "963720"
  },
  {
    "text": "between the clusters you're relying on the external load balancers to provide you networking from one cluster to the",
    "start": "963720",
    "end": "968940"
  },
  {
    "text": "next so what you do is you just create a separate load balancer for every cockroach pod that you want to run so if",
    "start": "968940",
    "end": "975240"
  },
  {
    "text": "you're running three pods in each of three clusters you're going to end up with nine load balancers and then you",
    "start": "975240",
    "end": "980910"
  },
  {
    "text": "just have all the pods talk to each other through those so if you have like this to cluster you're running in two",
    "start": "980910",
    "end": "986370"
  },
  {
    "text": "clusters with two pods each you'll end up with these four load balancers and each pod just has to talk through the",
    "start": "986370",
    "end": "991770"
  },
  {
    "text": "correct load balancer when they want to talk to a certain pod and you'll notice this does add officially add some extra",
    "start": "991770",
    "end": "998339"
  },
  {
    "text": "hops to the data path but most load balancers are reasonably efficient and",
    "start": "998339",
    "end": "1003430"
  },
  {
    "text": "so this works even we don't have pada pada communication and it solves the problem from the past solution that it",
    "start": "1003430",
    "end": "1009320"
  },
  {
    "text": "continues working even as pods move around the cluster it lets you survive machine failures and machine replacements more easily because the",
    "start": "1009320",
    "end": "1014870"
  },
  {
    "text": "pods can be rescheduled but they're still reachable through the same load balancer and this means you don't have",
    "start": "1014870",
    "end": "1020510"
  },
  {
    "text": "to configure anything else other than those little balancers themselves because the load balancers addresses never change so it's giving you that",
    "start": "1020510",
    "end": "1025880"
  },
  {
    "text": "stable persistent network address that we need it has a number of cons though so you do have to provision a lot of",
    "start": "1025880",
    "end": "1032630"
  },
  {
    "text": "load balancers if you're running a lot of processes this can get expensive it can be a pain to do and it can get",
    "start": "1032630",
    "end": "1038089"
  },
  {
    "text": "expensive if you're running a lot of them or if you're passing a lot of data through them in the cloud because you know these cloud load balancers aren't free and you do have to do a bunch of",
    "start": "1038089",
    "end": "1045770"
  },
  {
    "text": "manual setup work upfront in order to get all these load balancers set up and to tell each pod what it's load balanced",
    "start": "1045770",
    "end": "1052010"
  },
  {
    "text": "IP addresses because each pod needs to know its address so that it can tell other nodes how where to reach it so you",
    "start": "1052010",
    "end": "1057530"
  },
  {
    "text": "have to use a pretty fancy config map in order to pass this in to the pods dynamically which you probably won't be",
    "start": "1057530",
    "end": "1063020"
  },
  {
    "text": "able to without looking at the docks this also means that whatever you want to add new pods you want to scale up you",
    "start": "1063020",
    "end": "1068980"
  },
  {
    "text": "have to first create that load balancer for the new pods and if you forget that then your pod isn't gonna be doing very",
    "start": "1068980",
    "end": "1073990"
  },
  {
    "text": "much it's not going to be reachable and again this can add some extra hops on the data path unless you have a very",
    "start": "1073990",
    "end": "1079300"
  },
  {
    "text": "sophisticated load balancer but this is a very very good way the customers that",
    "start": "1079300",
    "end": "1084760"
  },
  {
    "text": "have been using it haven't run into you know any production issues caused by this set up the networking has remained",
    "start": "1084760",
    "end": "1089890"
  },
  {
    "text": "reliable pods have been deleted and recreated on different nodes and things have continued working just fine so this",
    "start": "1089890",
    "end": "1095050"
  },
  {
    "text": "is somewhat battle-tested you could if you have pod de pod",
    "start": "1095050",
    "end": "1101410"
  },
  {
    "start": "1098000",
    "end": "1098000"
  },
  {
    "text": "connectivity between clusters just use the pot IPS directly you aren't given a persistent name so this only works if",
    "start": "1101410",
    "end": "1108070"
  },
  {
    "text": "your piece of software can handle having addresses change underneath them so you know you have a pod running it wanted IP",
    "start": "1108070",
    "end": "1114520"
  },
  {
    "text": "address they get stopped get started up with the same storage but a new IP address it needs to be able to inform all the other nodes about its new IP",
    "start": "1114520",
    "end": "1120640"
  },
  {
    "text": "address and they have to be ok with that but if your software can do that then this works you need to create a one load",
    "start": "1120640",
    "end": "1126430"
  },
  {
    "text": "balancer up front such that they can find each other in the first place so you have one joint address that you provide to cockroach that it tries to",
    "start": "1126430",
    "end": "1132850"
  },
  {
    "text": "connect to when it starts up if it doesn't know any other members of the cluster but once they connect to each other they",
    "start": "1132850",
    "end": "1138280"
  },
  {
    "text": "can talk directly to each other using those IP addresses if your network offers this to you I don't know of",
    "start": "1138280",
    "end": "1144310"
  },
  {
    "text": "anybody using this in practice but you know it is a pretty effective way of",
    "start": "1144310",
    "end": "1150160"
  },
  {
    "text": "doing things the big con though is that those because those IP addresses are changing it's hard to create TLS certs",
    "start": "1150160",
    "end": "1157120"
  },
  {
    "text": "that are valid for the nodes because whenever you create a sign of TLS certificate you have to sign what",
    "start": "1157120",
    "end": "1162220"
  },
  {
    "text": "addresses it's valid for and those addresses are going to be changing every time that the pod moves to a new IP address so you'd have to sign a new",
    "start": "1162220",
    "end": "1168880"
  },
  {
    "text": "certificate for that IP a new IP address every time a pod moves and you'd have to set up some sort of automatic",
    "start": "1168880",
    "end": "1175060"
  },
  {
    "text": "certificate signer to do this and it's pretty questionable from a security perspective because you probably don't to be blindly signing certificates that",
    "start": "1175060",
    "end": "1182200"
  },
  {
    "text": "have root on your database cluster all the time so we don't know if anyone that's actually doing this in a secure",
    "start": "1182200",
    "end": "1188140"
  },
  {
    "text": "way but I've run a number of secure in secure clusters like this they don't use TLS and it works pretty well but say we",
    "start": "1188140",
    "end": "1197590"
  },
  {
    "text": "want to run the secure node in the same way where the pods talk directly to each other taking advantage of that pod de pod connectivity when",
    "start": "1197590",
    "end": "1204190"
  },
  {
    "text": "it's there how do we add those persistent names and you know typically when you want to add a persistent name",
    "start": "1204190",
    "end": "1209980"
  },
  {
    "text": "to something whose IP is changing you use dns that's the natural solution that's been around for a very long time",
    "start": "1209980",
    "end": "1215490"
  },
  {
    "text": "and the way that DNS works in a single",
    "start": "1215490",
    "end": "1220510"
  },
  {
    "text": "kubernetes cluster is that a pod tries to talk to some service in this case pod zero is trying to talk to the pod one",
    "start": "1220510",
    "end": "1226390"
  },
  {
    "text": "service cube DNS knows about all the services and end points in the cluster because it's been watching the cube the",
    "start": "1226390",
    "end": "1232720"
  },
  {
    "text": "kubernetes api server will then you know return the IP address of that service if",
    "start": "1232720",
    "end": "1238960"
  },
  {
    "text": "there is one and for stateful sets it actually return can return the IP addresses of the pods themselves using",
    "start": "1238960",
    "end": "1244720"
  },
  {
    "text": "what's called a headless service so we could use core DNS to watch multiple",
    "start": "1244720",
    "end": "1253270"
  },
  {
    "text": "kubernetes API servers at once so core DNS has a plug-in infrastructure which allows you to modify the way that it",
    "start": "1253270",
    "end": "1259600"
  },
  {
    "text": "works and I should say that core DNS is a different implementation of the DNS for kubernetes clusters by default",
    "start": "1259600",
    "end": "1266530"
  },
  {
    "text": "up until 1.13 kubernetes clusters use what's called cube dns which doesn't isn't quite as extensible the core",
    "start": "1266530",
    "end": "1273760"
  },
  {
    "text": "dienes lets you put in arbitrary plugins into how it works and there's already an existing plug-in called kubernetes",
    "start": "1273760",
    "end": "1279190"
  },
  {
    "text": "that was written to do exactly what we want you can configure it to the core DNS instance to connect to multiple",
    "start": "1279190",
    "end": "1285400"
  },
  {
    "text": "kubernetes API servers from different clusters it can then do dns the serve dns lookups from all of those different",
    "start": "1285400",
    "end": "1291370"
  },
  {
    "text": "clusters however it turns out that actually doing this in a managed communities offering is next to impossible I couldn't figure out how to",
    "start": "1291370",
    "end": "1297520"
  },
  {
    "text": "do it how to replace cube DNS with core DNS because the built-in add-on",
    "start": "1297520",
    "end": "1302860"
  },
  {
    "text": "controller ensures cube DNS is always running and does not like you trying to change it and you could try to",
    "start": "1302860",
    "end": "1309250"
  },
  {
    "text": "reconfigure the DNS settings on every node in the cluster you know to talk to a different DNS you know server but then",
    "start": "1309250",
    "end": "1315550"
  },
  {
    "text": "you have to be constantly updating that and users clusters every time the nodes we start or are recreated which might",
    "start": "1315550",
    "end": "1320800"
  },
  {
    "text": "work if you control all the nodes it's tough for me to like help other people with if they're running in the cloud you",
    "start": "1320800",
    "end": "1327100"
  },
  {
    "text": "also would probably want to give each cluster a unique clustered domain and DNS so if you've ever looked at the full you",
    "start": "1327100",
    "end": "1334810"
  },
  {
    "text": "know DNS name of a service and communities you'll see that it ends and SVC dot cluster dot local you can",
    "start": "1334810",
    "end": "1340300"
  },
  {
    "text": "actually customize that but you have to be able to change the flags being passed to kubernetes components on all the nodes in order to do so which is again",
    "start": "1340300",
    "end": "1346690"
  },
  {
    "text": "very hard to do and a managed service offering because they control all that they're gonna recreate the nodes underneath you it's tough to do yourself",
    "start": "1346690",
    "end": "1353670"
  },
  {
    "text": "this might become easier now that Cordia core DMS is the standard but it still doesn't give you a way to modify the",
    "start": "1353670",
    "end": "1359680"
  },
  {
    "text": "cluster domain and also the default version of core DNS that shipping in these new clusters doesn't include that cout brunette I plug-in that you would",
    "start": "1359680",
    "end": "1365770"
  },
  {
    "text": "want to be using so you'd still have to try to replace it so this is a bit of a non-starter in a cloud provider but if",
    "start": "1365770",
    "end": "1371440"
  },
  {
    "text": "you're running your own cluster this would be a great way to do it another",
    "start": "1371440",
    "end": "1376930"
  },
  {
    "text": "variant of that is that you could try running core DNS alongside cubed us tell cube dienes to defer the lookups",
    "start": "1376930",
    "end": "1383220"
  },
  {
    "text": "for for certain suffixes or maybe even for all suffixes to core dns and then",
    "start": "1383220",
    "end": "1389230"
  },
  {
    "text": "you'd have to configure core dns to watch all the other communities API servers and you know then it could serve",
    "start": "1389230",
    "end": "1395080"
  },
  {
    "text": "the responses from multiple clusters I haven't actually tried this on some a little fuzzy on the details but I think it would work out pretty well if someone",
    "start": "1395080",
    "end": "1400870"
  },
  {
    "text": "tries it let me know afterwards you the only downside of this is that you do have to copy the API server credentials",
    "start": "1400870",
    "end": "1406810"
  },
  {
    "text": "for all of your clusters into all of your other clusters because core DNS isn't able to you know get the resources",
    "start": "1406810",
    "end": "1412030"
  },
  {
    "text": "from the other clusters unless it has credentials to do so so you're putting all your all your cluster credentials into each one which means that if",
    "start": "1412030",
    "end": "1417940"
  },
  {
    "text": "someone you know breaks into one of your clusters they could potentially get the credentials to access all of your other ones if you're you know smart and only",
    "start": "1417940",
    "end": "1425410"
  },
  {
    "text": "give like read-only credentials this is probably fine but you have to be careful and make sure you don't give root on all the other clusters or something like",
    "start": "1425410",
    "end": "1431020"
  },
  {
    "text": "that or you can use a built-in cube DNS feature called stub domains so cube",
    "start": "1431020",
    "end": "1439150"
  },
  {
    "text": "deenis offers this feature that allows you to defer lookups for certain DNS suffixes to other name servers and so",
    "start": "1439150",
    "end": "1446170"
  },
  {
    "text": "you can example configure cube DNS in your local cluster to say I want all lookups that end and yeah I think I got",
    "start": "1446170",
    "end": "1452620"
  },
  {
    "text": "the SVC on their own part of that there so sorry but the end and us east 1.53 that local to go to the US East one",
    "start": "1452620",
    "end": "1460150"
  },
  {
    "text": "clusters DNS service and I've put together some scripts for people to try this out on GAE",
    "start": "1460150",
    "end": "1465700"
  },
  {
    "text": "and so the sub domain configuration is very small you just give it a suffix and",
    "start": "1465700",
    "end": "1470720"
  },
  {
    "text": "then you give it an IP address that you want all DNS lookups for that suffix to be redirected to and so again this is",
    "start": "1470720",
    "end": "1480470"
  },
  {
    "start": "1480000",
    "end": "1480000"
  },
  {
    "text": "how a single cluster DNS lookup works in this solution if the DNS lookup is for something with one of those other",
    "start": "1480470",
    "end": "1485660"
  },
  {
    "text": "suffixes the local cubed DNS will act as a recursive resolver and forward that",
    "start": "1485660",
    "end": "1490910"
  },
  {
    "text": "DNS request along to the other cubed DNS server which will then hopefully have the response for you and propagate it",
    "start": "1490910",
    "end": "1497000"
  },
  {
    "text": "back to you so in this case pod 0 in the first cluster would get the IP address for pod 1 in the second cluster and then",
    "start": "1497000",
    "end": "1502490"
  },
  {
    "text": "be able to talk directly to it assuming you still have pod 2 pod connectivity so",
    "start": "1502490",
    "end": "1508910"
  },
  {
    "text": "this is really great except for one big con so it has no overhead on the data path except for the normal overhead of",
    "start": "1508910",
    "end": "1514340"
  },
  {
    "text": "using docker versus host networking it's very resilient to hosts being added and removed or pods being moved around you",
    "start": "1514340",
    "end": "1521030"
  },
  {
    "text": "don't need to add any extra software to your cluster and it's actually completely scriptable you have to pass in like the you know like the you have",
    "start": "1521030",
    "end": "1528590"
  },
  {
    "text": "to configure cube cuddle to be able to talk to your clusters and then you can run this script and it will just work for you you know a minute or two after",
    "start": "1528590",
    "end": "1534020"
  },
  {
    "text": "running the script you'll have a working multi-region cockroach cluster the big problem though is that you might have",
    "start": "1534020",
    "end": "1539840"
  },
  {
    "text": "noticed I'm kind of kind of papered over how the DNS server in one cluster finds the DNS server in the other cluster",
    "start": "1539840",
    "end": "1545690"
  },
  {
    "text": "because those DNS pods can move around as well and so you need to create some",
    "start": "1545690",
    "end": "1550880"
  },
  {
    "text": "sort of persistent address for those DNS pods you can use a cloud load balancer for this and then you're sort of",
    "start": "1550880",
    "end": "1557240"
  },
  {
    "text": "deferring the problem such that only DNS lookups have to go through one of these load balancers and you don't need as",
    "start": "1557240",
    "end": "1562820"
  },
  {
    "text": "many of them and so you can do that on GCE that doesn't work on Amazon though",
    "start": "1562820",
    "end": "1568040"
  },
  {
    "text": "because Amazon's load balancers give you at least their standard elby's give you",
    "start": "1568040",
    "end": "1573230"
  },
  {
    "text": "a and il B's there's a mikvah load balancers give you like a DNS address",
    "start": "1573230",
    "end": "1578450"
  },
  {
    "text": "rather than an IP address and you can't give cube DNS a DNS name for these stub resolver 'z you have to give them an",
    "start": "1578450",
    "end": "1584780"
  },
  {
    "text": "actual IP so I haven't gotten this working on Amazon but it does work on Google and we do have a couple of users",
    "start": "1584780",
    "end": "1590330"
  },
  {
    "text": "running like this and I've had a long-running cluster like this for a while and it's still been working fine",
    "start": "1590330",
    "end": "1596830"
  },
  {
    "text": "again many of you might have been asking up until this point like why aren't you using sto an sto has been working this",
    "start": "1597590",
    "end": "1603480"
  },
  {
    "text": "year very hard on a multi cluster mode to handle this problem of addressing they don't try to solve the pod pod",
    "start": "1603480",
    "end": "1610500"
  },
  {
    "text": "connectivity right now they just assume that it exists and they're trying to solve the naming issue and all the other",
    "start": "1610500",
    "end": "1615899"
  },
  {
    "text": "things that they saw beyond naming I mean they do much more than just that and the way that it works is that you install sto in one primary kubernetes",
    "start": "1615899",
    "end": "1622380"
  },
  {
    "text": "cluster all of the sto control plan components of which there are many and then you install the special sto remote",
    "start": "1622380",
    "end": "1628529"
  },
  {
    "text": "component in your other kubernetes clusters you might notice already that this means that if that primary cluster",
    "start": "1628529",
    "end": "1633600"
  },
  {
    "text": "goes down the remote clusters no longer have a leader I maybe this has changed but last time I looked this was still",
    "start": "1633600",
    "end": "1640260"
  },
  {
    "text": "the case and this does look like it's going to be the best way to do things as",
    "start": "1640260",
    "end": "1646289"
  },
  {
    "text": "it continues to develop you know it'll be a well supported way of doing this there's not going to be much overhead of",
    "start": "1646289",
    "end": "1651899"
  },
  {
    "text": "a data path and it will be very resilient to changes as of last time I've tried this though you were still",
    "start": "1651899",
    "end": "1657899"
  },
  {
    "text": "running the entire control plane in a single cluster the setup process was very involved and even after you went",
    "start": "1657899",
    "end": "1663029"
  },
  {
    "text": "through their long docks page and how to set it up they still say this might need more work if you're using it in production environments because there",
    "start": "1663029",
    "end": "1669179"
  },
  {
    "text": "are some concerns around actually solving this naming problem for the stl components themselves so just like we",
    "start": "1669179",
    "end": "1674820"
  },
  {
    "text": "had to put in a load balancer for DNS in the fast solution you might have to do that for hiss do in this solution and",
    "start": "1674820",
    "end": "1680250"
  },
  {
    "text": "again just like using core DNS you have to copy the kubernetes credentials into that primary control plane cluster so",
    "start": "1680250",
    "end": "1686490"
  },
  {
    "text": "all your kubernetes clusters are gonna have their credentials in one cluster again this isn't a great idea for for",
    "start": "1686490",
    "end": "1694049"
  },
  {
    "start": "1691000",
    "end": "1691000"
  },
  {
    "text": "cockroach in particular but if you're running your own stuff you can always do your own thing so you know set up your own custom DNS servers outside of",
    "start": "1694049",
    "end": "1699630"
  },
  {
    "text": "kubernetes you know maybe you could use like console for service discovery to do this outside of kubernetes or set up",
    "start": "1699630",
    "end": "1705389"
  },
  {
    "text": "your own Auto certificate approver so that you can create a new certificate every time a pod changes its IP address",
    "start": "1705389",
    "end": "1710990"
  },
  {
    "text": "do all sorts of different things on your own run your own proxy to provide this pod to pod connectivity but it's tough",
    "start": "1710990",
    "end": "1717690"
  },
  {
    "text": "for us to widely recommend any of these in particular and so I haven't really explored this space that much and so to",
    "start": "1717690",
    "end": "1725250"
  },
  {
    "text": "summarize multi cluster networking is still very complicated there's not like a clear solution",
    "start": "1725250",
    "end": "1731100"
  },
  {
    "text": "and because it's been left someone specified it's really tough to know what you're gonna get out of any given",
    "start": "1731100",
    "end": "1736410"
  },
  {
    "text": "kubernetes installation it's not going to be the same from one installation to another and that much has been done",
    "start": "1736410",
    "end": "1741840"
  },
  {
    "text": "about it until this year when a few a few services have started paying attention to it which I'm really excited about",
    "start": "1741840",
    "end": "1746910"
  },
  {
    "text": "so sto has been working on it the cilium networking project that I talked about has their cluster mesh mode that they",
    "start": "1746910",
    "end": "1752039"
  },
  {
    "text": "implemented there's the up bound up Brown start ups cross plane project which was announced recently they're working on multi",
    "start": "1752039",
    "end": "1758460"
  },
  {
    "text": "cluster and multi cloud use cases and so I'm really excited to see this start to get more attention but it is hard to",
    "start": "1758460",
    "end": "1764700"
  },
  {
    "text": "recommend a single answer for everyone so you really need to understand kubernetes well and understand the",
    "start": "1764700",
    "end": "1770070"
  },
  {
    "text": "different options if you want to implement this yourself as of today so thank you so much for coming if you do",
    "start": "1770070",
    "end": "1776730"
  },
  {
    "text": "want to see more information on this or try it for yourself we have documentation about it on our website we have all the scripts and config files and our repo if you want to",
    "start": "1776730",
    "end": "1783179"
  },
  {
    "text": "see it or you're always welcome to reach out to me if you've tried this or you've had any questions and I will be hanging",
    "start": "1783179",
    "end": "1789750"
  },
  {
    "text": "around F to talk if you have anything you want to talk about today do we have any time for questions",
    "start": "1789750",
    "end": "1795470"
  },
  {
    "text": "you",
    "start": "1799760",
    "end": "1801820"
  }
]