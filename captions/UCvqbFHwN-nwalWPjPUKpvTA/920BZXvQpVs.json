[
  {
    "text": "hello kubecon for this last day and Friday my name is Thomas I'm one of the creators and maintainers",
    "start": "4799",
    "end": "11400"
  },
  {
    "text": "of psyllium and psyllium is one of the popular kubernetes networking implementations and based on what we",
    "start": "11400",
    "end": "18420"
  },
  {
    "text": "have learned creating psyllium and working with many of you I would like to share how to survive day",
    "start": "18420",
    "end": "23699"
  },
  {
    "text": "two and how to troubleshoot kubernetes networking so a bit of context I'm one of the",
    "start": "23699",
    "end": "31080"
  },
  {
    "text": "maintainers of psyllium so what I'm sharing today of course has a bit of psyllium context because I've been",
    "start": "31080",
    "end": "37380"
  },
  {
    "text": "mostly working with psyllium the past few years but the exercises or the best practices and how to approach this is of",
    "start": "37380",
    "end": "44640"
  },
  {
    "text": "course not at all specific to cilia and can be applied regardless of what",
    "start": "44640",
    "end": "49980"
  },
  {
    "text": "kubernetes networking implementation you are using for those who have never heard about",
    "start": "49980",
    "end": "55980"
  },
  {
    "text": "psyllium psyllium is a cni plugin among other things it is a cncf project at",
    "start": "55980",
    "end": "61500"
  },
  {
    "text": "incubation stage but cylinder does not only provide networking or cni",
    "start": "61500",
    "end": "67200"
  },
  {
    "text": "functionality which is what we will focus on today it also provides service mesh and hobble and a lot of what we",
    "start": "67200",
    "end": "74640"
  },
  {
    "text": "will hear about today is made possible by Hubble or tools Cinema to hobble",
    "start": "74640",
    "end": "80280"
  },
  {
    "text": "hobble is the observability layer of psyllium then we also have in the sodium family tetragon which is providing",
    "start": "80280",
    "end": "86759"
  },
  {
    "text": "runtime security and security observability all aspects of psyllium are being done",
    "start": "86759",
    "end": "93960"
  },
  {
    "text": "using a technology called evpf and that's actually what's enabling a lot of the observability that we will see today",
    "start": "93960",
    "end": "100860"
  },
  {
    "text": "which will assist you in troubleshooting for some in particular in the last part",
    "start": "100860",
    "end": "106619"
  },
  {
    "text": "of the talk we'll also look at layer 7 latency or HTTP latency some of that work is done through Envoy the envoy",
    "start": "106619",
    "end": "113460"
  },
  {
    "text": "proxy which is also a cncf project so let's jump into kubernetes networking",
    "start": "113460",
    "end": "120899"
  },
  {
    "text": "or how some people call it the Dark Side of kubernetes",
    "start": "120899",
    "end": "126899"
  },
  {
    "text": "how many of you are familiar with the Core Concepts of kubernetes networking",
    "start": "127200",
    "end": "134040"
  },
  {
    "text": "excellent almost everybody so this will probably be a repetition for for most of you but if you've never",
    "start": "134040",
    "end": "140580"
  },
  {
    "text": "heard about or have never seen how kubernetes networking works this may help you understand the rest of this",
    "start": "140580",
    "end": "146760"
  },
  {
    "text": "talk first of all kubernetes networking is very simple and basic all parts have an",
    "start": "146760",
    "end": "152459"
  },
  {
    "text": "IP address which means every individual part has an IP address they can directly",
    "start": "152459",
    "end": "157739"
  },
  {
    "text": "talk to each other there is no specific Network topology required kubernetes makes a simple",
    "start": "157739",
    "end": "165019"
  },
  {
    "text": "assumption that we have a flat so-called flat layer 3 Network port side if I have",
    "start": "165019",
    "end": "171660"
  },
  {
    "text": "IPS and thus Parts can talk to each other this is sometimes implemented differently depending on whether you're",
    "start": "171660",
    "end": "177900"
  },
  {
    "text": "running in the cloud or whether you're running on-prem you may be running vgp on-prem you might be using the cloud",
    "start": "177900",
    "end": "184140"
  },
  {
    "text": "provider sdn as you're running in the cloud or you might be running an overlay doesn't matter all of the all of these",
    "start": "184140",
    "end": "190620"
  },
  {
    "text": "implementation principles implement this basic Foundation typically this is not true for all",
    "start": "190620",
    "end": "197760"
  },
  {
    "text": "kubernetes implementations but typically every node has a so-called part cider range which means Parts on a particular",
    "start": "197760",
    "end": "205140"
  },
  {
    "text": "node will have IPS assigned to that node so that's a particular set of ips and",
    "start": "205140",
    "end": "210840"
  },
  {
    "text": "all parts on that node will have an IP out of that range some implementations do this differently",
    "start": "210840",
    "end": "217200"
  },
  {
    "text": "and allocate a unique IP but there is a concept you may see this in kubernetes",
    "start": "217200",
    "end": "222780"
  },
  {
    "text": "called the Portside arrange or per node podside range kubernetes uses services for load",
    "start": "222780",
    "end": "230580"
  },
  {
    "text": "balancing you're probably using this all the time this is the load balancing layer of kubernetes this is what's",
    "start": "230580",
    "end": "237180"
  },
  {
    "text": "allowing us to have multiple replicas of a part address a single service name or",
    "start": "237180",
    "end": "242580"
  },
  {
    "text": "single cluster IP and kubernetes will load balance to any of those replicas we'll look into some troubleshooting",
    "start": "242580",
    "end": "249239"
  },
  {
    "text": "there as well and then of course kubernetes uses DNS for service Discovery so you can address",
    "start": "249239",
    "end": "255120"
  },
  {
    "text": "a service by its name using DNS instead of hard coding IP addresses into the",
    "start": "255120",
    "end": "261060"
  },
  {
    "text": "application and then that's the last principle of kubernetes networking there is Network policy now our policy is what implements",
    "start": "261060",
    "end": "269040"
  },
  {
    "text": "the segmentation so Network policy is what allows you to Define which parts can talk to which other parts you could",
    "start": "269040",
    "end": "276300"
  },
  {
    "text": "be doing segmentation maybe on namespace level so maybe you want to allow within a namespace you want to allow",
    "start": "276300",
    "end": "282540"
  },
  {
    "text": "communication or the cross name spaces communication should not be allowed or you can even do this at the Pod level",
    "start": "282540",
    "end": "288300"
  },
  {
    "text": "and essentially say that only certain parts can talk to certain other parts this is called kubernetes Network policy",
    "start": "288300",
    "end": "294960"
  },
  {
    "text": "and as we will see in the demo later on plays a huge role in some of the troubleshooting",
    "start": "294960",
    "end": "301560"
  },
  {
    "text": "this slide looked very simple right that looks beautiful kubernetes networking is very very basic as you learn kubernetes",
    "start": "301560",
    "end": "308940"
  },
  {
    "text": "as you run kubernetes I did this slide back in 2021 that's a bit a little bit more how it looks like in practice right",
    "start": "308940",
    "end": "315780"
  },
  {
    "text": "so you have an overall goal which is to forward together to do Network forwarding you",
    "start": "315780",
    "end": "322320"
  },
  {
    "text": "have iptable somewhere somewhere you have q proxy using IP tables you have an",
    "start": "322320",
    "end": "327660"
  },
  {
    "text": "application team trying to schedule 5 000 Services really putting stress on qproxy you have the liveness probes",
    "start": "327660",
    "end": "335300"
  },
  {
    "text": "succeeding so the applications seem like they're up and running because they're not aware of the actual Network",
    "start": "335300",
    "end": "341400"
  },
  {
    "text": "underneath so the applications are just reporting hey I'm healthy I'm healthy I'm healthy but they're not at all because they're actually not even",
    "start": "341400",
    "end": "347460"
  },
  {
    "text": "reachable and you may have a platform team that is completely ignoring the crash Loop backoffing coding as parts",
    "start": "347460",
    "end": "354240"
  },
  {
    "text": "that may be more closer to the reality sometimes so this talk is giving you",
    "start": "354240",
    "end": "360600"
  },
  {
    "text": "tools on how you can look into this and figure out where the problem actually is",
    "start": "360600",
    "end": "367919"
  },
  {
    "text": "so let's dive One Step deeper into kubernetes and see how this is actually implemented because as we",
    "start": "367919",
    "end": "374280"
  },
  {
    "text": "troubleshooting as we troubleshoot networking it helps to understand the base concepts of how the cni actually",
    "start": "374280",
    "end": "380759"
  },
  {
    "text": "works in a kubernetes cluster and this is true for all cnis it's a core concept",
    "start": "380759",
    "end": "386100"
  },
  {
    "text": "of kubernetes so if you have multiple nodes and we have ports running on the",
    "start": "386100",
    "end": "391380"
  },
  {
    "text": "nodes and of course containers running inside them we then have a network cni",
    "start": "391380",
    "end": "396479"
  },
  {
    "text": "level or layer which typically runs as an agent or as a demon set on all the",
    "start": "396479",
    "end": "401940"
  },
  {
    "text": "nodes and the cni then essentially spans the network plane which allows the ports",
    "start": "401940",
    "end": "407460"
  },
  {
    "text": "to talk to each other kubernetes itself does not have a built-in networking",
    "start": "407460",
    "end": "412560"
  },
  {
    "text": "layer it requires there's a default cni cubenet but it requires the cni plugin to actually allow ports to talk not only",
    "start": "412560",
    "end": "420180"
  },
  {
    "text": "across nodes but even inside of the note itself and then we have q proxy which is",
    "start": "420180",
    "end": "425880"
  },
  {
    "text": "the default implementation to Implement kubernetes services and as well coordinates which is not in the picture",
    "start": "425880",
    "end": "432960"
  },
  {
    "text": "here when we implement this with psyllium this looks very similar right we have",
    "start": "432960",
    "end": "438660"
  },
  {
    "text": "psyllium that provides the networking data path and then we have ebpf which is",
    "start": "438660",
    "end": "443880"
  },
  {
    "text": "doing the implementation of that and is actually causing the package to be forwarded or policy rules to be",
    "start": "443880",
    "end": "451199"
  },
  {
    "text": "implemented the last concept we need to understand",
    "start": "451199",
    "end": "456900"
  },
  {
    "text": "before we get can get a little bit more Hands-On is hobble hobble is based on",
    "start": "456900",
    "end": "463080"
  },
  {
    "text": "top of psyllium and provides observability there is no demand there's no",
    "start": "463080",
    "end": "468900"
  },
  {
    "text": "requirement for a cni to actually provide observability the standard only demands for parts to be able to connect",
    "start": "468900",
    "end": "474660"
  },
  {
    "text": "to each other and everything else is optional so even now or policy implementation is actually optional so a",
    "start": "474660",
    "end": "481259"
  },
  {
    "text": "c cni can can provide as little as just pure network connectivity with Hubble we",
    "start": "481259",
    "end": "488160"
  },
  {
    "text": "are essentially providing additional observability Tooling in the form of flow logs who is talking to whom and",
    "start": "488160",
    "end": "496380"
  },
  {
    "text": "metrics it's essentially a TCP dump for kubernetes because in the old days of",
    "start": "496380",
    "end": "502800"
  },
  {
    "text": "kubernetes many of us have been executing into a node or into a pod and",
    "start": "502800",
    "end": "507960"
  },
  {
    "text": "literally running TCP dump to somehow figure out what is going on on the network and for those of you have never",
    "start": "507960",
    "end": "514200"
  },
  {
    "text": "heard about TCP dump it's a 25-year plus year old tool which is obviously never",
    "start": "514200",
    "end": "520200"
  },
  {
    "text": "been intended to be used for an environment like kubernetes",
    "start": "520200",
    "end": "525240"
  },
  {
    "text": "Hubble has native integration with Prometheus and grafana so you are not",
    "start": "525240",
    "end": "530399"
  },
  {
    "text": "looking at you can but you not don't have to look at the terminal you can look at dashboards and at Prometheus",
    "start": "530399",
    "end": "537600"
  },
  {
    "text": "metrics let's look at a couple of examples and we'll jump into a live demo afterwards",
    "start": "537600",
    "end": "543959"
  },
  {
    "text": "so we can for example look at grafana-based panels how many packets are we forwarding what",
    "start": "543959",
    "end": "550800"
  },
  {
    "text": "is the drop rate so how many packets how many Network packets are getting dropped by the network layer what is the total",
    "start": "550800",
    "end": "557519"
  },
  {
    "text": "amount of traffic being forwarded or you can even create dashboards to display how much cross region cross AC Network",
    "start": "557519",
    "end": "564779"
  },
  {
    "text": "traffic do I have in my cloud because that's usually what your cloud provider charges a little bit extra for",
    "start": "564779",
    "end": "570720"
  },
  {
    "text": "that's a great fun of you we also have a Hubble UI service map",
    "start": "570720",
    "end": "575760"
  },
  {
    "text": "where you can see all the services that are running and how they are talking to",
    "start": "575760",
    "end": "580800"
  },
  {
    "text": "each other so here we actually see the individual network connections and the API calls for application protocols that",
    "start": "580800",
    "end": "587880"
  },
  {
    "text": "we understand these are HTTP grpc Kafka Cassandra so we're not only showing you",
    "start": "587880",
    "end": "593880"
  },
  {
    "text": "who is talking to whom we can even show you with Hubble what type of API calls they are making or what is the request",
    "start": "593880",
    "end": "600839"
  },
  {
    "text": "response latency for a grpc call and on the lower part you actually see",
    "start": "600839",
    "end": "606959"
  },
  {
    "text": "the live flow data the service map is being based on because what you're seeing here on the screen is derived",
    "start": "606959",
    "end": "613860"
  },
  {
    "text": "completely transparently this is not requiring changes in the application it's essentially Hubble running",
    "start": "613860",
    "end": "620180"
  },
  {
    "text": "transparently on the nodes and extracting all the connectivity data transparently and then we can calculate",
    "start": "620180",
    "end": "626760"
  },
  {
    "text": "what we call the service map from that data and the raw data is what you see in the lower part of the screen",
    "start": "626760",
    "end": "635459"
  },
  {
    "text": "as a last concept the kubernetes service implementation and there is this is if",
    "start": "635459",
    "end": "641519"
  },
  {
    "text": "we would go into a lot of details several types of services for the",
    "start": "641519",
    "end": "646560"
  },
  {
    "text": "purpose of this talk I will keep it simple and talk about cluster IP so this is the ability to expose multiple part",
    "start": "646560",
    "end": "654060"
  },
  {
    "text": "replicas via a single IP a single cluster IP which means that instead of",
    "start": "654060",
    "end": "659760"
  },
  {
    "text": "being aware of the potentially hundreds or even thousands of pod replicas to talk to I can talk to a single cluster",
    "start": "659760",
    "end": "666839"
  },
  {
    "text": "IP and this cluster IP will then get low balance to any of the hundreds or",
    "start": "666839",
    "end": "672600"
  },
  {
    "text": "thousands of pod replicas and of course you do not want to talk specifically to an IP so you will kubernetes allocates a",
    "start": "672600",
    "end": "679980"
  },
  {
    "text": "service name for you and makes that available as a DNS name via code DNS so essentially your application talks to",
    "start": "679980",
    "end": "686880"
  },
  {
    "text": "the service app name and kubernetes takes takes care of the rest for you",
    "start": "686880",
    "end": "692940"
  },
  {
    "text": "let's jump in into the first demo and actually show you how to troubleshoot",
    "start": "692940",
    "end": "698279"
  },
  {
    "text": "some problems and I figured let's start simple and use a very basic Network policy",
    "start": "698279",
    "end": "706019"
  },
  {
    "text": "example because what could possibly go wrong there right in the simple example we will have a",
    "start": "706019",
    "end": "713160"
  },
  {
    "text": "front end and a backend part and we'll do a network policy that will look something like this so this is a",
    "start": "713160",
    "end": "719459"
  },
  {
    "text": "kubernetes network policy and this policy for those of you have used this before this will look very very simple",
    "start": "719459",
    "end": "726300"
  },
  {
    "text": "we're allowing from the front-end part to talk egress so out outgoing egress",
    "start": "726300",
    "end": "733740"
  },
  {
    "text": "when we are networking people call egress the outgoing site and Ingress the incoming side so we are creating a",
    "start": "733740",
    "end": "740160"
  },
  {
    "text": "policy that the front-end part is allowed to talk to the back end part and",
    "start": "740160",
    "end": "745560"
  },
  {
    "text": "does somebody already see the problem with this if so yes I see one hand exactly excellent excellent right well",
    "start": "745560",
    "end": "753360"
  },
  {
    "text": "it looks very simple like what could go wrong so let's actually try this out",
    "start": "753360",
    "end": "758779"
  },
  {
    "text": "so I have this running right here so the parts are up and running and they",
    "start": "758940",
    "end": "765480"
  },
  {
    "text": "seem to be pretty happy is it big enough I think so they're up and running right so this is",
    "start": "765480",
    "end": "771060"
  },
  {
    "text": "probably good but actually under the hood this is not good at all and it's pretty pretty hard to actually even see",
    "start": "771060",
    "end": "777480"
  },
  {
    "text": "this because as I mentioned the health check here the application is reporting just fine yeah the application is up and",
    "start": "777480",
    "end": "784260"
  },
  {
    "text": "running in this case the application they may actually be complaining hey my apt my app is not really working even though",
    "start": "784260",
    "end": "790320"
  },
  {
    "text": "it's up and running what is going on so let's actually start looking below the hood what we can see on the network",
    "start": "790320",
    "end": "796560"
  },
  {
    "text": "observability side so I'm swapping over to the grafana dashboard of Hubble UI",
    "start": "796560",
    "end": "803160"
  },
  {
    "text": "and we'll start out with the overall view this is the cluster wide view of",
    "start": "803160",
    "end": "808380"
  },
  {
    "text": "everything that is going on in this cluster here we see the total amount of traffic being forward we see the total",
    "start": "808380",
    "end": "814860"
  },
  {
    "text": "of endpoints that's the total of Parts running in these Clauses we see the number of nodes we see that we have no",
    "start": "814860",
    "end": "820980"
  },
  {
    "text": "unreachable psyllium nodes we have we see that we have no warnings or or or errors being reported by any of the",
    "start": "820980",
    "end": "828360"
  },
  {
    "text": "agents we do see a bunch of DNS errors though that's probably so if we zoom in here",
    "start": "828360",
    "end": "837260"
  },
  {
    "text": "that's probably something we should be looking into so we have quite a few DNS errors that are ongoing",
    "start": "838459",
    "end": "846120"
  },
  {
    "text": "we also see that we have policies loaded so we have 16 policy loaded we can look",
    "start": "846120",
    "end": "851880"
  },
  {
    "text": "at the enforcement status and we see how we have 53 Parts which have Port Network",
    "start": "851880",
    "end": "857040"
  },
  {
    "text": "policy enforcement enabled and we have 61 Parts which have Nona or policy enforcement enabled at all",
    "start": "857040",
    "end": "863399"
  },
  {
    "text": "we can move forward and actually see oh we have like some missing DNS responses we can look into the connection tracking",
    "start": "863399",
    "end": "869519"
  },
  {
    "text": "table and so on a lot of a lot of information in this case I actually know that",
    "start": "869519",
    "end": "875399"
  },
  {
    "text": "something is wrong potentially with this application front-end and backend and this is exposed into a namespace that I",
    "start": "875399",
    "end": "882480"
  },
  {
    "text": "know so I'll go over here and actually look into this cubecon simple namespace this is where front end and back end is",
    "start": "882480",
    "end": "889980"
  },
  {
    "text": "be is is running in it's very interesting you can see there is some some flows being forwarded we can scroll",
    "start": "889980",
    "end": "897060"
  },
  {
    "text": "down and here we go we see network policy drops right we see",
    "start": "897060",
    "end": "903600"
  },
  {
    "text": "a constant rate of network policy drops from the front end",
    "start": "903600",
    "end": "909000"
  },
  {
    "text": "we can go and go deeper and actually look at well where are these Network policy drops where are these packets",
    "start": "909000",
    "end": "914699"
  },
  {
    "text": "attempting to go we can zoom in here and we see huh this is going to the cube DNS",
    "start": "914699",
    "end": "920880"
  },
  {
    "text": "part so what is wrong at this point we know that something is",
    "start": "920880",
    "end": "926040"
  },
  {
    "text": "being being dropped so let's figure out what's actually being dropped so we'll go back into my terminal here",
    "start": "926040",
    "end": "931800"
  },
  {
    "text": "and I will look I will use the Hubble observe CLI command so this is the the CLI that will allow me to query hobble",
    "start": "931800",
    "end": "939720"
  },
  {
    "text": "now we'll run that and it will show me all the network jobs and all the forwarded flows that is happening in",
    "start": "939720",
    "end": "947579"
  },
  {
    "text": "this namespace because I ran it with the dash and qcon simple namespace filter and yes indeed we are seeing drops from",
    "start": "947579",
    "end": "954480"
  },
  {
    "text": "the front-end part to the cube DNS part so we've troubleshooted it down from the",
    "start": "954480",
    "end": "960480"
  },
  {
    "text": "high level overview where we saw we have some policy drops to the namespace view where we actually saw this was going to",
    "start": "960480",
    "end": "966480"
  },
  {
    "text": "cube DNS and then with Hubble C and I were able to see the actual wheel drops",
    "start": "966480",
    "end": "971880"
  },
  {
    "text": "so we go back to the slides it was indeed DNS as as often the case",
    "start": "971880",
    "end": "977940"
  },
  {
    "text": "for many incidents right even though it was actually not obvious at all I think a lot of application teams will inject",
    "start": "977940",
    "end": "985079"
  },
  {
    "text": "in our policy they will not understand that or may not understand that I also need to allow to cube DNS I need DNS for",
    "start": "985079",
    "end": "992339"
  },
  {
    "text": "my service so let's go to the Hubble UI",
    "start": "992339",
    "end": "998220"
  },
  {
    "text": "this is the the service map with the view on just that namespace and we see in fact that we only have communication",
    "start": "998220",
    "end": "1005060"
  },
  {
    "text": "from the front and to cube DNS and we actually see in the lower part it's a",
    "start": "1005060",
    "end": "1010160"
  },
  {
    "text": "bit small maybe but these are all dropped flows so this is all been dropped could even click on one and it",
    "start": "1010160",
    "end": "1015620"
  },
  {
    "text": "would tell me the drop reason is policy denied so let's actually fix this so",
    "start": "1015620",
    "end": "1021199"
  },
  {
    "text": "let's allow core DNS so I have a policy allow DNS",
    "start": "1021199",
    "end": "1030399"
  },
  {
    "text": "see if that works yeah so I created this policy",
    "start": "1030559",
    "end": "1035260"
  },
  {
    "text": "allow DNS so this is the policy which on top of the policy that already have now",
    "start": "1036679",
    "end": "1042199"
  },
  {
    "text": "allows to cube DNS and Bam we can go back into the service map and we now see not only connectivity",
    "start": "1042199",
    "end": "1048799"
  },
  {
    "text": "to cube DNS but also to the backend service and we now see new forwarded flows in green those are allowed flows",
    "start": "1048799",
    "end": "1056120"
  },
  {
    "text": "so we fixed our problem now how do I how did I get to this policy do this allow Cube dinners policy",
    "start": "1056120",
    "end": "1062600"
  },
  {
    "text": "of course you can ask chargpt today right if you don't",
    "start": "1062600",
    "end": "1068240"
  },
  {
    "text": "know how to use that yet you can also use what we have what we call the network policy editor",
    "start": "1068240",
    "end": "1075140"
  },
  {
    "text": "this is available for everybody editor dot Network policy dot IO it's free you",
    "start": "1075140",
    "end": "1080960"
  },
  {
    "text": "can use it and it will visualize Network policies for you and right now I've loaded the original",
    "start": "1080960",
    "end": "1087799"
  },
  {
    "text": "policy and based on the graphic you see in the top you can actually see that yes indeed",
    "start": "1087799",
    "end": "1093679"
  },
  {
    "text": "we are allowing to the back end but we are not allowing to cube DNS you can see the arrow is actually red",
    "start": "1093679",
    "end": "1099740"
  },
  {
    "text": "I can now quickly allow this go in here allow Rule and it has now extended our",
    "start": "1099740",
    "end": "1105620"
  },
  {
    "text": "existing policy to allow DNS and you can see the new yaml that was added below",
    "start": "1105620",
    "end": "1110780"
  },
  {
    "text": "which will actually allow qqns so if you don't know how to get to the yaml yourself Network policy editor can do",
    "start": "1110780",
    "end": "1117320"
  },
  {
    "text": "this for you or even in the visualization you can see the problem straight away",
    "start": "1117320",
    "end": "1123260"
  },
  {
    "text": "if you do not know the namespace specifically where the application is running we have what's called a policy",
    "start": "1123260",
    "end": "1129020"
  },
  {
    "text": "verdict dashboard this is this one cluster-wide and we see a whole bunch of",
    "start": "1129020",
    "end": "1136160"
  },
  {
    "text": "drops being going on so you'll probably need to look into well what are the drops I care about so there is actually",
    "start": "1136160",
    "end": "1142100"
  },
  {
    "text": "a namespace level view down here where we can Inc and indeed see that we have",
    "start": "1142100",
    "end": "1147679"
  },
  {
    "text": "drops from the front end Cube simple Parts into the cube system qtns part so",
    "start": "1147679",
    "end": "1153919"
  },
  {
    "text": "this is another way if you're not sure which application is even affected you can go in here and see all the drops by",
    "start": "1153919",
    "end": "1159860"
  },
  {
    "text": "name filter by namespace Across the cluster",
    "start": "1159860",
    "end": "1164919"
  },
  {
    "text": "so this is how the policy looked like looks like fixed we not only need to allow to the back end we also need to",
    "start": "1167780",
    "end": "1173780"
  },
  {
    "text": "allow to the coordinates or cube DNS part so more DNS because DNS is really often",
    "start": "1173780",
    "end": "1181520"
  },
  {
    "text": "the problem we looked at part to part",
    "start": "1181520",
    "end": "1186640"
  },
  {
    "text": "next example we're looking at something that's a little bit more advanced so as",
    "start": "1187340",
    "end": "1192380"
  },
  {
    "text": "we have learned kubernetes DNS is used for service Discovery it's usually implemented by coordinates but that's",
    "start": "1192380",
    "end": "1198200"
  },
  {
    "text": "actually not mandatory you can of course use a different DNS implementation and",
    "start": "1198200",
    "end": "1203600"
  },
  {
    "text": "it looks simple it is not always simple so let's look at how to troubleshoot",
    "start": "1203600",
    "end": "1208880"
  },
  {
    "text": "something like this and this is a simple pod in another namespace that simply",
    "start": "1208880",
    "end": "1214280"
  },
  {
    "text": "tries to reach out or intent to reach out to google.com",
    "start": "1214280",
    "end": "1220580"
  },
  {
    "text": "so let me go over and switch tab this is same cluster but I have just one",
    "start": "1220580",
    "end": "1227720"
  },
  {
    "text": "part in one namespace running then is DNS who is doing curl to google.com and",
    "start": "1227720",
    "end": "1235880"
  },
  {
    "text": "let's say the application team is actually complaining hey this is not working what is going on",
    "start": "1235880",
    "end": "1241640"
  },
  {
    "text": "let's go back into the dashboard and I have the demo DNS",
    "start": "1241640",
    "end": "1247840"
  },
  {
    "text": "so we've seen this view before and we can see that the the dashboard view of",
    "start": "1248660",
    "end": "1254480"
  },
  {
    "text": "how the DNS layer of kubernetes is doing in general so we see about 30 DNS",
    "start": "1254480",
    "end": "1260299"
  },
  {
    "text": "requests per second and we see the the top 10 DNS I can make",
    "start": "1260299",
    "end": "1266240"
  },
  {
    "text": "this a bit bigger we can make that we can see the top 10 DNS queries we can see we're looking up elasticsearch core",
    "start": "1266240",
    "end": "1273020"
  },
  {
    "text": "API and so on then I also see a bunch of DNS errors",
    "start": "1273020",
    "end": "1278120"
  },
  {
    "text": "right so yeah we're seeing quite a few of them and one of them them is from my",
    "start": "1278120",
    "end": "1284600"
  },
  {
    "text": "Denny's DNS part so these are like the rate of DNS error start this DNS or this",
    "start": "1284600",
    "end": "1291020"
  },
  {
    "text": "part is actually experiencing we can then go in and actually look at",
    "start": "1291020",
    "end": "1296740"
  },
  {
    "text": "what are the specific queries that are making that are not successful and we see down here that yes",
    "start": "1296740",
    "end": "1305240"
  },
  {
    "text": "we're trying to look up or curl Google but the app has spelled Google wrong and",
    "start": "1305240",
    "end": "1311240"
  },
  {
    "text": "was using zeros instead of OS of course that's the dashboard view we",
    "start": "1311240",
    "end": "1317299"
  },
  {
    "text": "can go back and actually look at the observe view as well",
    "start": "1317299",
    "end": "1322520"
  },
  {
    "text": "so I go back and I run Hubble observe this time I'm specifying the namespace",
    "start": "1322520",
    "end": "1327880"
  },
  {
    "text": "debug DNS which is where my application is running in and here we actually see yes we see the communication to cube DNS",
    "start": "1327880",
    "end": "1335539"
  },
  {
    "text": "that this is UDP and then we see the actual layer the actual DNS requests and",
    "start": "1335539",
    "end": "1341419"
  },
  {
    "text": "responses right we see here that the the Pod is actually attempting to resolve Google with zeros for both IPv6 and ipv4",
    "start": "1341419",
    "end": "1350480"
  },
  {
    "text": "I can then even dive deeper and so I actually only want the layer 7 information",
    "start": "1350480",
    "end": "1356480"
  },
  {
    "text": "like this and now it will only show me the resolution paths like what is the DNS resolution that is going on inside",
    "start": "1356480",
    "end": "1363500"
  },
  {
    "text": "of this namespace so in here I can quickly with the dashboard here look at",
    "start": "1363500",
    "end": "1368539"
  },
  {
    "text": "is my cluster order parts which are experiencing DNS resolutions and as I've",
    "start": "1368539",
    "end": "1374840"
  },
  {
    "text": "identified those parts I can use Hubble CLI to go in and actually find out what",
    "start": "1374840",
    "end": "1380539"
  },
  {
    "text": "is specifically going on what are they trying to look up and so on",
    "start": "1380539",
    "end": "1386140"
  },
  {
    "text": "last example debugging service latency service latency is let's say you have",
    "start": "1387880",
    "end": "1395240"
  },
  {
    "text": "deployed an application and the application is not performing as fast as it should be how are you troubleshooting",
    "start": "1395240",
    "end": "1401539"
  },
  {
    "text": "this how are you even identifying this and there is a standard for this or a",
    "start": "1401539",
    "end": "1406880"
  },
  {
    "text": "best practice for this called Golden signal dashboard that's been invented or written down by the Google SRE team you",
    "start": "1406880",
    "end": "1414860"
  },
  {
    "text": "can see the information at the bottom the it's actually documented pretty well it's a standard way of monitoring your",
    "start": "1414860",
    "end": "1423260"
  },
  {
    "text": "infrastructure specifically for cases where you're running a service that's publicly available and the the famous",
    "start": "1423260",
    "end": "1430700"
  },
  {
    "text": "four golden signals that matter in this case are latency traffic or throughput errors and",
    "start": "1430700",
    "end": "1439940"
  },
  {
    "text": "saturation and we'll look at what that actually means and why that's useful",
    "start": "1439940",
    "end": "1446600"
  },
  {
    "text": "so I go over here and I open up the the third demo this is the Hubble dashboard for Golden",
    "start": "1446600",
    "end": "1455600"
  },
  {
    "text": "signal dash for the for gold and for the four golden signals we see at the top the request rate how many requests per",
    "start": "1455600",
    "end": "1463280"
  },
  {
    "text": "second are actually coming in we're then seeing the request duration and in this case this is HTTP so this is",
    "start": "1463280",
    "end": "1470360"
  },
  {
    "text": "actually the HTTP request to response latency we can see p50",
    "start": "1470360",
    "end": "1476260"
  },
  {
    "text": "P95 and P99 these are essentially the Tails so p50 is the worst half of the",
    "start": "1476260",
    "end": "1484100"
  },
  {
    "text": "latency number P95 is the worst five percent so if you only look at the worst",
    "start": "1484100",
    "end": "1490880"
  },
  {
    "text": "five five percent of connections with the worst latency took the longest what",
    "start": "1490880",
    "end": "1496220"
  },
  {
    "text": "is the average over that five percent and P99 is only the worst one percent and what really matters is P99 or P95",
    "start": "1496220",
    "end": "1504080"
  },
  {
    "text": "because on average it often looks good but then for some customers The Experience could be really really bad so",
    "start": "1504080",
    "end": "1510440"
  },
  {
    "text": "you really want to not only monitor an average you want to monitor P99 as well and it's interesting that we",
    "start": "1510440",
    "end": "1517580"
  },
  {
    "text": "can actually see significant Peaks here so the average is actually pretty good if you would look at p50 that's the",
    "start": "1517580",
    "end": "1524720"
  },
  {
    "text": "green line it's all the way at the bottom so if you only look at the average even of the worst half everybody's like yay happy like it's",
    "start": "1524720",
    "end": "1533059"
  },
  {
    "text": "almost zero that's probably pretty good but if you look at P99 it's sometimes up to two seconds so some requests going",
    "start": "1533059",
    "end": "1540620"
  },
  {
    "text": "into this service have actually experienced a latency of two seconds before they got back a response",
    "start": "1540620",
    "end": "1547580"
  },
  {
    "text": "that's great now we understand that there is a problem in terms of latency we can also",
    "start": "1547580",
    "end": "1553880"
  },
  {
    "text": "understand that there's actually some problems in terms of Errors being returned which is the second column",
    "start": "1553880",
    "end": "1559640"
  },
  {
    "text": "we can see this this is the error rate so this is the rate of HTTP errors that",
    "start": "1559640",
    "end": "1567500"
  },
  {
    "text": "are being returned by the application so any sort of HTTP 500 type code",
    "start": "1567500",
    "end": "1573200"
  },
  {
    "text": "this so one source of problem can be the request taking very long so the service being slow and another common problem is",
    "start": "1573200",
    "end": "1581240"
  },
  {
    "text": "that the service is crashing and returning in like an HTTP 500 error code",
    "start": "1581240",
    "end": "1586640"
  },
  {
    "text": "both are very meaningful and important to monitor that's great now we know that there are",
    "start": "1586640",
    "end": "1593360"
  },
  {
    "text": "some problems in order to now actually debug this we need to correlate this with saturation because we need to",
    "start": "1593360",
    "end": "1600559"
  },
  {
    "text": "understand whether latency is because there is limited available availability of resources and that's often CPU so we",
    "start": "1600559",
    "end": "1608900"
  },
  {
    "text": "actually have the HTTP request duration and the CPU usage right next to each",
    "start": "1608900",
    "end": "1615200"
  },
  {
    "text": "other so I can actually go in here and see whether a spike I can go in and",
    "start": "1615200",
    "end": "1620659"
  },
  {
    "text": "actually zoom in here and just let's just look at this Spike I can look what it is Spike in latency is caused because",
    "start": "1620659",
    "end": "1627140"
  },
  {
    "text": "there was a spike in CPU usage on that note and I can do that for both the",
    "start": "1627140",
    "end": "1632419"
  },
  {
    "text": "source and the destination so if I have two parts running in the cluster I can measure the latency and I will see the",
    "start": "1632419",
    "end": "1639620"
  },
  {
    "text": "CPU consumption on the source node and on the destination node so I can even understand if the latency",
    "start": "1639620",
    "end": "1645260"
  },
  {
    "text": "is bad on the destination side whether that was caused by limited availability of CPU on the source node side this is",
    "start": "1645260",
    "end": "1652460"
  },
  {
    "text": "allowing to monitor applications broadly cluster white to figure out is my",
    "start": "1652460",
    "end": "1658520"
  },
  {
    "text": "service up and running is it returning errors and how fast is my service actually running like how quick is the",
    "start": "1658520",
    "end": "1665539"
  },
  {
    "text": "app functioning and so on and again all of this is done using transparent",
    "start": "1665539",
    "end": "1672640"
  },
  {
    "text": "extraction of observability data so this is not using application instrumentation we're now looking at the the grafana",
    "start": "1672640",
    "end": "1680120"
  },
  {
    "text": "dashboard of this what all of this data is also available as open Telemetry metrics and traces as well so you can",
    "start": "1680120",
    "end": "1687500"
  },
  {
    "text": "also visualize this in other tooling that is open telemetry capable",
    "start": "1687500",
    "end": "1694720"
  },
  {
    "text": "and with that I think we have a couple of minutes left for questions this was",
    "start": "1701840",
    "end": "1707059"
  },
  {
    "text": "like a quick overview [Applause]",
    "start": "1707059",
    "end": "1715099"
  },
  {
    "text": "[Music] if you want to learn more suleim.io",
    "start": "1715260",
    "end": "1721100"
  },
  {
    "text": "where you find information about psyllium and hobble or if you want to ask me questions afterwards feel free to",
    "start": "1721100",
    "end": "1727580"
  },
  {
    "text": "reach out on Twitter now I think there are mics left and right in case you want",
    "start": "1727580",
    "end": "1732919"
  },
  {
    "text": "to ask questions but I also know that it is launch time",
    "start": "1732919",
    "end": "1737679"
  },
  {
    "text": "and hey this side hello uh just just one quick",
    "start": "1742460",
    "end": "1749779"
  },
  {
    "text": "simple question and fast I hope so uh okay uh let's say we would like to use",
    "start": "1749779",
    "end": "1756260"
  },
  {
    "text": "psyllium but we use openshift now and openshift has uh now we don't use",
    "start": "1756260",
    "end": "1762620"
  },
  {
    "text": "openshift cni but openshift OV and do you know if they are compatible I",
    "start": "1762620",
    "end": "1768200"
  },
  {
    "text": "mean can we add psyllium over openshift and don't break everything yes you can run chaining cni chaining",
    "start": "1768200",
    "end": "1775520"
  },
  {
    "text": "modes where you can run psyllium just the Hubble portion on top of openshift sdn or ovn and so on absolutely okay but",
    "start": "1775520",
    "end": "1783980"
  },
  {
    "text": "psyllium is also available on openshift if you want to replace it both are possible I see you thank you",
    "start": "1783980",
    "end": "1792340"
  },
  {
    "text": "hi and I have a question regarding as a",
    "start": "1799059",
    "end": "1804260"
  },
  {
    "text": "network plugin what does the network plugin like calcu or selenium provide",
    "start": "1804260",
    "end": "1810140"
  },
  {
    "text": "not provide by eks bpc cni and what will",
    "start": "1810140",
    "end": "1816020"
  },
  {
    "text": "make me choose the network plugin and not the cloud plugin especially I feel",
    "start": "1816020",
    "end": "1822559"
  },
  {
    "text": "the vpcc ini makes the life easy so it makes the the Pod reachable or",
    "start": "1822559",
    "end": "1829399"
  },
  {
    "text": "accessible inside the VPC also if I will use a controller a load balancer",
    "start": "1829399",
    "end": "1835940"
  },
  {
    "text": "controller and I use the IP Target so the requests can reach to the",
    "start": "1835940",
    "end": "1844039"
  },
  {
    "text": "Pod directly without pass without it go to the API server so yeah that's good",
    "start": "1844039",
    "end": "1851240"
  },
  {
    "text": "yes so I think a good often a motivation to run sillim on top of the vpcni cni in",
    "start": "1851240",
    "end": "1857480"
  },
  {
    "text": "eks would be to install hobble on top or the network policy implementation so",
    "start": "1857480",
    "end": "1863600"
  },
  {
    "text": "it's just like we've heard for openshift you can run on top of the vpcsn VPC cni",
    "start": "1863600",
    "end": "1869120"
  },
  {
    "text": "you can also run psyllium natively in what's called eni mode in which case it",
    "start": "1869120",
    "end": "1874520"
  },
  {
    "text": "can just like the VPC VPC cni plugin itself natively about all of the Power",
    "start": "1874520",
    "end": "1881120"
  },
  {
    "text": "traffic so you don't need to run serum in what's called overlay mode was that your question I'm not 100 sure whether I",
    "start": "1881120",
    "end": "1888020"
  },
  {
    "text": "got it right yeah so what we will provide for me if I put it",
    "start": "1888020",
    "end": "1894039"
  },
  {
    "text": "on top of the cni so cni now provide me the network or implement the network and",
    "start": "1894039",
    "end": "1900980"
  },
  {
    "text": "make the Pod already accessible so and also decrease that latency so what this",
    "start": "1900980",
    "end": "1907039"
  },
  {
    "text": "will add to my configuration or my implement station what's the main feature for that can you",
    "start": "1907039",
    "end": "1913159"
  },
  {
    "text": "repeat the last spot I didn't fully understand it yeah so I mean if I have cni and",
    "start": "1913159",
    "end": "1920440"
  },
  {
    "text": "the cni what is the feature for that what we will add more to my implementation I see okay so the vpcs",
    "start": "1922880",
    "end": "1929419"
  },
  {
    "text": "vpcc and I at least as my understanding right now those connectivity and there",
    "start": "1929419",
    "end": "1935299"
  },
  {
    "text": "is of course Q proxy already running so the service implementation and I think there already is or there is an upcoming",
    "start": "1935299",
    "end": "1941960"
  },
  {
    "text": "Network policy implementation what does not exist in there is all the observability that we've seen today",
    "start": "1941960",
    "end": "1947480"
  },
  {
    "text": "which is needed for the troubleshooting then solum also has functionality such as cluster mesh service mesh tetragon",
    "start": "1947480",
    "end": "1954860"
  },
  {
    "text": "all of these are are not available in the the VPC cni plugin so the extended",
    "start": "1954860",
    "end": "1960440"
  },
  {
    "text": "functionality does that make sense yeah yeah a good question as well we are",
    "start": "1960440",
    "end": "1967940"
  },
  {
    "text": "currently using istio with kyali on top which provides a very similar set of capabilities with what you were just",
    "start": "1967940",
    "end": "1973880"
  },
  {
    "text": "describing for Hubble so what would be the benefit of switching from one to the",
    "start": "1973880",
    "end": "1979220"
  },
  {
    "text": "other because well there is of course the service match level and the cni level and like what's the difference",
    "start": "1979220",
    "end": "1985640"
  },
  {
    "text": "that that gives you so I think they're different for most users will be that if you don't have a",
    "start": "1985640",
    "end": "1991159"
  },
  {
    "text": "service mesh yet you can get this easily choose natively from psyllium you don't need to actually deploy a full service",
    "start": "1991159",
    "end": "1998360"
  },
  {
    "text": "mesh if you all if you are already running HDO then sodium provides very",
    "start": "1998360",
    "end": "2004840"
  },
  {
    "text": "similar observability data we even implement the same Open Standards there are there can be some performance",
    "start": "2004840",
    "end": "2011799"
  },
  {
    "text": "differences depending on how you run istio ambient mesh versus the traditional istio and so on",
    "start": "2011799",
    "end": "2018580"
  },
  {
    "text": "that you would have to look into in detail so the the difference is less in capability but more on the architectural",
    "start": "2018580",
    "end": "2025419"
  },
  {
    "text": "model potentially the performance of it and so on",
    "start": "2025419",
    "end": "2031860"
  },
  {
    "text": "let's go I was trying to think for a great presentation first of all and I was",
    "start": "2035559",
    "end": "2041679"
  },
  {
    "text": "trying to come up with a question we I'm trying to troubleshoot the problems uh from AWS alb's talking to the cube",
    "start": "2041679",
    "end": "2049060"
  },
  {
    "text": "cluster itself and a lot of times I see that there is like connection to Target",
    "start": "2049060",
    "end": "2054220"
  },
  {
    "text": "errors or so so like based on the Observer like investigation that I call",
    "start": "2054220",
    "end": "2059618"
  },
  {
    "text": "it that is somewhere where we're suspecting that the target the Pod is basically basically still not ready but",
    "start": "2059619",
    "end": "2065980"
  },
  {
    "text": "the traffic is still trying like trying there are attempts to Road it there if there is something like is it anyhow the",
    "start": "2065980",
    "end": "2072339"
  },
  {
    "text": "use case of Hubble that can help us with troubleshooting all those things yes absolutely because if that is happening",
    "start": "2072339",
    "end": "2078220"
  },
  {
    "text": "on the network you typically receive what's called a reset TCP reset because",
    "start": "2078220",
    "end": "2083980"
  },
  {
    "text": "the port is not yet open there's even a Hubble filter that Allah or a dashboard",
    "start": "2083980",
    "end": "2089080"
  },
  {
    "text": "that shows you those parts right where you're reaching out to a service maybe",
    "start": "2089080",
    "end": "2094480"
  },
  {
    "text": "the back end the replicas are still scaling up or there are already scaling down and this is actually recognizable",
    "start": "2094480",
    "end": "2100599"
  },
  {
    "text": "on the network because typically you don't have a lot of reset TCP resets in your kubernetes network so that's often",
    "start": "2100599",
    "end": "2107140"
  },
  {
    "text": "a pattern that you can see if you know the specific service then with the The Observer with this view if",
    "start": "2107140",
    "end": "2113680"
  },
  {
    "text": "the Hubble observe view here with the CLI you could actually see each reset so you could see the service drying out and",
    "start": "2113680",
    "end": "2120339"
  },
  {
    "text": "then instead of just not receiving anything back you will get back the the reset I can show you an example on on",
    "start": "2120339",
    "end": "2127599"
  },
  {
    "text": "how that looks like so that's how how many psyllium users actually troubleshoot this because it's more like",
    "start": "2127599",
    "end": "2133300"
  },
  {
    "text": "not on the service itself level but maybe when the node itself spins up yeah so at that time so it somehow shows that",
    "start": "2133300",
    "end": "2140140"
  },
  {
    "text": "the node is already ready and even the Slowdown side so like ALB should wait so if if you're running psyllium in those",
    "start": "2140140",
    "end": "2146800"
  },
  {
    "text": "scenarios the node is okay let's I know that we have seen the Hubble dashboard for the for the new cluster we just",
    "start": "2146800",
    "end": "2153220"
  },
  {
    "text": "launched and we see the same exactly the same problems and I think at that time we're using eks because before it was for the self-hosted cluster so there is",
    "start": "2153220",
    "end": "2160480"
  },
  {
    "text": "there is Hubble somehow yeah okay yeah and if you already you run sodium in that when the node is still coming out",
    "start": "2160480",
    "end": "2166839"
  },
  {
    "text": "there's actually a specific error just Civic drop reason that you will see that this went to a note that the C do not",
    "start": "2166839",
    "end": "2173740"
  },
  {
    "text": "exist or is not available yet so if it even gets to the nodes but the psyllium",
    "start": "2173740",
    "end": "2179680"
  },
  {
    "text": "is not ready there or the node is not ready yet still will actually report you to specific reason why it could not",
    "start": "2179680",
    "end": "2185260"
  },
  {
    "text": "deliver the packet that could be a motivation to run the silly machine you",
    "start": "2185260",
    "end": "2190359"
  },
  {
    "text": "said I can see a demo how can I yeah can I just come up or yeah absolutely absolutely cool thank you hey so I have",
    "start": "2190359",
    "end": "2198400"
  },
  {
    "text": "a couple of questions um great presentation firstly um so we want to migrate away from",
    "start": "2198400",
    "end": "2204099"
  },
  {
    "text": "Calico to psyllium we run on-prem so all the fun stuff that goes with on-prem",
    "start": "2204099",
    "end": "2209740"
  },
  {
    "text": "clusters um is there a migration document that we can follow I asked the booth and",
    "start": "2209740",
    "end": "2215260"
  },
  {
    "text": "apparently there isn't one is there anything in the works there there is even better there's actually a video",
    "start": "2215260",
    "end": "2221079"
  },
  {
    "text": "recording of Duffy Cooley migrating I think from Calico but I'm not quite sure",
    "start": "2221079",
    "end": "2227020"
  },
  {
    "text": "but yeah so migration apps exist and there is on the echo newsletter which is",
    "start": "2227020",
    "end": "2232720"
  },
  {
    "text": "the evpf and Celia newsletter and and show there's actually a migration episode on how to migrate to cylinder",
    "start": "2232720",
    "end": "2239859"
  },
  {
    "text": "can be a great starting point and then the next step if that's not enough ping us on slack we're we're happy to help",
    "start": "2239859",
    "end": "2246720"
  },
  {
    "text": "we may have a more formal more official migration path for documentation path soon the trouble with that is often",
    "start": "2246720",
    "end": "2253540"
  },
  {
    "text": "there is not just one way to run other cni plugins so it's often really depends on are you running let's say Calico in",
    "start": "2253540",
    "end": "2259960"
  },
  {
    "text": "bgp mode or are you running that in the cloud are you using that our policy are you using Calico net or policy and so on",
    "start": "2259960",
    "end": "2266140"
  },
  {
    "text": "that's why it's not just a flip of a switch okay um and the next question was so we run",
    "start": "2266140",
    "end": "2272980"
  },
  {
    "text": "vanilla kubernetes across a couple of sites and Azure too so um if we were to",
    "start": "2272980",
    "end": "2279460"
  },
  {
    "text": "use psyllium and Hubble Hubble UI how will traffic show when it goes through",
    "start": "2279460",
    "end": "2284500"
  },
  {
    "text": "several other layers of infrastructure like firewalls gateways Etc so we obviously have access to the virtual",
    "start": "2284500",
    "end": "2290859"
  },
  {
    "text": "machines but the infrastructure teams manage the firewalls so um how will that show on Hubble would it actually show as",
    "start": "2290859",
    "end": "2297520"
  },
  {
    "text": "next hops or what will Hubble do with that traffic unless you do something and run something on those middle boxes you",
    "start": "2297520",
    "end": "2305020"
  },
  {
    "text": "will not see anything but you can actually if you want install hobble on them if you want the observability data",
    "start": "2305020",
    "end": "2312240"
  },
  {
    "text": "often selling users will then replace for example the external load balancer",
    "start": "2312240",
    "end": "2317740"
  },
  {
    "text": "with the cylinder and then you get the observability data but if you run psyllium in overlay mode it will not",
    "start": "2317740",
    "end": "2325300"
  },
  {
    "text": "actually even care about what the underlying infrastructure looks like and as such you will not see any details but",
    "start": "2325300",
    "end": "2331300"
  },
  {
    "text": "you will see so sillim will warn you if it transmitted on one node and it did",
    "start": "2331300",
    "end": "2337180"
  },
  {
    "text": "not receive on the other node which will extend usually the indication that the underlying Network dropped it somewhere",
    "start": "2337180",
    "end": "2342220"
  },
  {
    "text": "right cool thanks oh by the way you can see typically if",
    "start": "2342220",
    "end": "2347980"
  },
  {
    "text": "it's L3 routers you can compare the TCP TTL on the sending and the receiving node then you at least know how many",
    "start": "2347980",
    "end": "2354339"
  },
  {
    "text": "routers you had in between cool thank you",
    "start": "2354339",
    "end": "2359940"
  },
  {
    "text": "I have a question regarding the L7 detection I mean",
    "start": "2360640",
    "end": "2367420"
  },
  {
    "text": "you showed with the command line tool in the UI that there was some information",
    "start": "2367420",
    "end": "2372579"
  },
  {
    "text": "about the L7 I wanted to ask in case we have a TLS",
    "start": "2372579",
    "end": "2382300"
  },
  {
    "text": "do we still have some information about the L7 protocol used the answer is it",
    "start": "2382300",
    "end": "2390040"
  },
  {
    "text": "depends so if you are using psyllium to do the TLs then yes if the application",
    "start": "2390040",
    "end": "2397000"
  },
  {
    "text": "is doing the the TLs itself then the answer is by default no but with ktlers",
    "start": "2397000",
    "end": "2403660"
  },
  {
    "text": "we can optionally if the application allows for this and then if you're running a service",
    "start": "2403660",
    "end": "2410200"
  },
  {
    "text": "mesh to do mtlers then Hubble can also see it because it's unencrypted up until",
    "start": "2410200",
    "end": "2415540"
  },
  {
    "text": "to the to the proxy but if the application itself does TLS and if the application does not want to be if drops",
    "start": "2415540",
    "end": "2422200"
  },
  {
    "text": "on then we cannot see it that would break that will break the security mode there",
    "start": "2422200",
    "end": "2427420"
  },
  {
    "text": "are some ways to do that with view probes in the app but that's not really",
    "start": "2427420",
    "end": "2432460"
  },
  {
    "text": "reliable so we can demo that like many others as well but it's not something we recommend to run in production because",
    "start": "2432460",
    "end": "2439119"
  },
  {
    "text": "it's not 100 current guaranteed okay thank you",
    "start": "2439119",
    "end": "2445260"
  },
  {
    "text": "all right uh you mentioned that like we there are traces from uh Hubble and",
    "start": "2445900",
    "end": "2451660"
  },
  {
    "text": "psyllium companies what what are they or like what kind of information I can get from them",
    "start": "2451660",
    "end": "2457060"
  },
  {
    "text": "so traces we do open Telemetry traces we can do hdp traces with request response",
    "start": "2457060",
    "end": "2464140"
  },
  {
    "text": "spans with the latency the return code all of this we can also do traces for the DNS resolution which is entering",
    "start": "2464140",
    "end": "2470680"
  },
  {
    "text": "it's interesting so you actually see you can see the whole trace and then the spans including the DNS resolution so",
    "start": "2470680",
    "end": "2477280"
  },
  {
    "text": "you actually know how long the DNS resolution take took then we also have TCP traces support as well so for",
    "start": "2477280",
    "end": "2483160"
  },
  {
    "text": "whatever HTTP or layer 7 protocol we support as well as for the underlying protocol we can emit open Telemetry",
    "start": "2483160",
    "end": "2490660"
  },
  {
    "text": "traces with uh spans and then if you want to add additional instrumentation",
    "start": "2490660",
    "end": "2496420"
  },
  {
    "text": "via open Telemetry application instrumentation you can combine that together so if you want to have spans",
    "start": "2496420",
    "end": "2502960"
  },
  {
    "text": "inside of the app or including Tracer including Trace I trace ID support",
    "start": "2502960",
    "end": "2509859"
  },
  {
    "text": "okay cool and second question I'm gonna be uh about routing of L7 level layer 7",
    "start": "2509859",
    "end": "2515980"
  },
  {
    "text": "protocols like HTTP one like in case of service mesh like is it really possible with psyllium I mean like the build some",
    "start": "2515980",
    "end": "2524020"
  },
  {
    "text": "custom logic uh with uh some specific URLs Etc like like the things that like",
    "start": "2524020",
    "end": "2530200"
  },
  {
    "text": "istio can do right so psyllium implements the Ingress standard and the Gateway API standard so",
    "start": "2530200",
    "end": "2537940"
  },
  {
    "text": "whatever you can do with either of them you can fully perform URL rewrite path based routing header based routing kind",
    "start": "2537940",
    "end": "2545440"
  },
  {
    "text": "of rollouts then we have some annotations on both of them and then as the most sophisticated most powerful uh",
    "start": "2545440",
    "end": "2552700"
  },
  {
    "text": "API we have an what's called an Envoy crd which allows you to configure direct",
    "start": "2552700",
    "end": "2558760"
  },
  {
    "text": "Envoy configuration so whatever is possible to configure in an Envoy listener you can use so the full Envoy",
    "start": "2558760",
    "end": "2566800"
  },
  {
    "text": "feature configuration that's available in a so-called listen you can actually configure for retries or for specific",
    "start": "2566800",
    "end": "2574240"
  },
  {
    "text": "TLS termination that is going Beyond what's possible in Ingress and Gateway API so is it correct that it's not",
    "start": "2574240",
    "end": "2582520"
  },
  {
    "text": "happening in the inside of the kernel right I mean it's correct so all of the layers seven load balancing is being",
    "start": "2582520",
    "end": "2588940"
  },
  {
    "text": "done with Envoy yes we have some observability T into layer 7 using ebpf",
    "start": "2588940",
    "end": "2594700"
  },
  {
    "text": "but the the low balancing retards rate limiting on layer 7 is all done through",
    "start": "2594700",
    "end": "2602520"
  },
  {
    "text": "Envoy we do have an mtls model that does not rely on a proxy but that has nothing",
    "start": "2602520",
    "end": "2609220"
  },
  {
    "text": "to do with HTTP or any layer 7 processing that's purely on the on the",
    "start": "2609220",
    "end": "2614800"
  },
  {
    "text": "on the on the layer four right okay cool thanks",
    "start": "2614800",
    "end": "2619619"
  },
  {
    "text": "hi so we are also running Calico on-prem and I was wondering whether it's",
    "start": "2620079",
    "end": "2625780"
  },
  {
    "text": "possible assuming the worst case so we cannot migrate to celium for some reason to whether we can run Hubble on top of",
    "start": "2625780",
    "end": "2634180"
  },
  {
    "text": "Calico and have the same observability metrics and whatsoever so yes you can",
    "start": "2634180",
    "end": "2640300"
  },
  {
    "text": "chain psyllium on top of Calico and run hobble it does have some limitations",
    "start": "2640300",
    "end": "2647020"
  },
  {
    "text": "because the sum of the layer 7 observability into HTTP the the envoy",
    "start": "2647020",
    "end": "2653380"
  },
  {
    "text": "injection we're doing that relies on using a particular feature that Calico",
    "start": "2653380",
    "end": "2658420"
  },
  {
    "text": "also uses right now so that specific aspect is currently not compatible but",
    "start": "2658420",
    "end": "2664000"
  },
  {
    "text": "everything else I would say roughly 95 what you have seen today is possible to",
    "start": "2664000",
    "end": "2669760"
  },
  {
    "text": "run on top of Calico absolutely and does it require any change from an application Level or is it completely",
    "start": "2669760",
    "end": "2675940"
  },
  {
    "text": "responsible it's completely you are changing your cni configuration to also launch psyllium next to Calico on top of",
    "start": "2675940",
    "end": "2683200"
  },
  {
    "text": "it and then it will not actually do the routing you can even still have Calico do the network policy if you want or you",
    "start": "2683200",
    "end": "2690339"
  },
  {
    "text": "can say I'm migrating away from calicona or policy and I'm using sodium Network policy and then in either way to do the",
    "start": "2690339",
    "end": "2697060"
  },
  {
    "text": "observability on top that is possible okay so the traffic is going to psyllium",
    "start": "2697060",
    "end": "2702280"
  },
  {
    "text": "and then down to Calico and then it's close Okay I mean Calico actually only",
    "start": "2702280",
    "end": "2707920"
  },
  {
    "text": "uses the standard so if you're running Calico on the default configuration then it's just using Linux networking there's",
    "start": "2707920",
    "end": "2713800"
  },
  {
    "text": "nothing too special about it if you're using Calico and ebpf mode then it would use the ebpf",
    "start": "2713800",
    "end": "2720579"
  },
  {
    "text": "um and but both are both are compatible I think would there be maybe performance",
    "start": "2720579",
    "end": "2726339"
  },
  {
    "text": "issues by running two cni on one on one on top of each other like we're only",
    "start": "2726339",
    "end": "2731440"
  },
  {
    "text": "running one is definitely more efficient than running two um we would have to run it if you are",
    "start": "2731440",
    "end": "2737859"
  },
  {
    "text": "not doing anything with psyllium right the override is very small I'd say overall right like as you configure all",
    "start": "2737859",
    "end": "2744460"
  },
  {
    "text": "of things in psyllium there is overhead and then it will depend on how much observability you want which will also",
    "start": "2744460",
    "end": "2750880"
  },
  {
    "text": "then uh dictate how much ovary your head you have but in general that is definitely 100 feasible and we have many",
    "start": "2750880",
    "end": "2757599"
  },
  {
    "text": "psyllium users that are doing this okay thank you hello hello we're thinking about",
    "start": "2757599",
    "end": "2764800"
  },
  {
    "text": "shifting from AWS to gcp but we know that Urban isn't available in gke so do",
    "start": "2764800",
    "end": "2770079"
  },
  {
    "text": "you know if it will change someday or if there is an alternative to that you can",
    "start": "2770079",
    "end": "2775780"
  },
  {
    "text": "run psyllium on all the cloud providers natively I can obviously not speak to when the cloud providers would enable",
    "start": "2775780",
    "end": "2781300"
  },
  {
    "text": "specific functionality in their version of psyllium I know that some of them are",
    "start": "2781300",
    "end": "2786700"
  },
  {
    "text": "considering to bring a Hubble but you can run cilium to my understanding on",
    "start": "2786700",
    "end": "2791920"
  },
  {
    "text": "all the cloud provider as a replacement cni or in chaining mode on top in",
    "start": "2791920",
    "end": "2798339"
  },
  {
    "text": "particular of course on Ado on AWS that's common and that's actually what a",
    "start": "2798339",
    "end": "2803920"
  },
  {
    "text": "lot of users and ISO available customers are running okay",
    "start": "2803920",
    "end": "2809880"
  },
  {
    "text": "I think we got all the questions uh one more one more sorry I'm sorry oh two more two more we are also openshift",
    "start": "2811420",
    "end": "2817599"
  },
  {
    "text": "customer and we are using options they need users IP tables currently we",
    "start": "2817599",
    "end": "2824380"
  },
  {
    "text": "have a class IP service and the class are calling this service in a high rate",
    "start": "2824380",
    "end": "2830859"
  },
  {
    "text": "and we are creating lots of this new TCP connections and we are close to consume",
    "start": "2830859",
    "end": "2837280"
  },
  {
    "text": "all the available TCP Source ports in the cinema implementation I think you are replacing the cluster IP address",
    "start": "2837280",
    "end": "2844440"
  },
  {
    "text": "with the real endpoint IP address during the TCP connect times does this implementation increase the number of",
    "start": "2844440",
    "end": "2851859"
  },
  {
    "text": "tuples so and it increases the number of the usable TCP Source Sports very good point we",
    "start": "2851859",
    "end": "2860440"
  },
  {
    "text": "have a good answer for you because when psyllium implements the load balancing with Q proxy replacement we actually use",
    "start": "2860440",
    "end": "2866800"
  },
  {
    "text": "our own connection tracking table which you can set the limit for so that solves",
    "start": "2866800",
    "end": "2872260"
  },
  {
    "text": "limit number one and then if you're really running out of tuples overall I",
    "start": "2872260",
    "end": "2877599"
  },
  {
    "text": "would recommend that you are using the socket based load balancing which does the low balancing at the socket connect",
    "start": "2877599",
    "end": "2883480"
  },
  {
    "text": "system time which is incredibly more scalable if you ping me on slack I can give you",
    "start": "2883480",
    "end": "2890200"
  },
  {
    "text": "pointers on that to get you started thanks for the demo so since gke data",
    "start": "2890200",
    "end": "2897819"
  },
  {
    "text": "plane V2 uses psyllium I was wondering if that's compatible with hobby because I did a quick search and said that the",
    "start": "2897819",
    "end": "2903640"
  },
  {
    "text": "Hubble UI wasn't available because the agents are done exposed Parts but I was just interested whether the CLI observe",
    "start": "2903640",
    "end": "2910420"
  },
  {
    "text": "tool would work um I know that there is work to enable",
    "start": "2910420",
    "end": "2916240"
  },
  {
    "text": "Hubble on GK I'm not aware of the latest status as of now in the initial version",
    "start": "2916240",
    "end": "2921460"
  },
  {
    "text": "it was not it was not enabled but I think the Google team is working on uh enabling Hubble as soon as the Hubble",
    "start": "2921460",
    "end": "2928380"
  },
  {
    "text": "API is exposed then the dashboards and the Hubble CLI will work to some extent",
    "start": "2928380",
    "end": "2936400"
  },
  {
    "text": "because they may actually not immediately enable all of the Hubble functionality for example the DNS",
    "start": "2936400",
    "end": "2942700"
  },
  {
    "text": "observability or the layer 7 observability that's a question to ask for Google what they will actually specifically enable there okay I'll",
    "start": "2942700",
    "end": "2950319"
  },
  {
    "text": "follow the project thanks awesome thank you thank you very much again",
    "start": "2950319",
    "end": "2956880"
  }
]