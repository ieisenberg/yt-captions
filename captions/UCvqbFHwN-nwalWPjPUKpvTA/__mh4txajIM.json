[
  {
    "text": "Welcome to our session on platform performance optimization for",
    "start": "880",
    "end": "7280"
  },
  {
    "text": "AI we are going to optimize model inference here and this is now from the",
    "start": "7280",
    "end": "15040"
  },
  {
    "text": "resource management perspective inside a node my name is AE kin and I work for",
    "start": "15040",
    "end": "23439"
  },
  {
    "text": "Intel and I've been dealing with this Resource Management not Resource Management topic for a couple of years",
    "start": "23439",
    "end": "29759"
  },
  {
    "text": "now and here's Dix my name is Dix you can",
    "start": "29759",
    "end": "35480"
  },
  {
    "text": "call me Dixie I go by that Alas and I work for Google and I'm an active contributor to signote community so for",
    "start": "35480",
    "end": "43120"
  },
  {
    "text": "this session I want to point out that running AI workloads can get expensive really fast specifically the llm uh",
    "start": "43120",
    "end": "50760"
  },
  {
    "text": "inference models so these workloads can have uh different resource usage",
    "start": "50760",
    "end": "55879"
  },
  {
    "text": "patterns for example some could be CPU bound while the others could be Memory bound and while some could just have uh",
    "start": "55879",
    "end": "62359"
  },
  {
    "text": "spiky resource needs so in this session we are going to walk over how you can utilize the maximum out of your Hardware",
    "start": "62359",
    "end": "69799"
  },
  {
    "text": "how you can unlock the maximum potential so if you understand the resource usage patterns of your workloads you can place",
    "start": "69799",
    "end": "76479"
  },
  {
    "text": "them strategically such that there is minimum to no uh resource contention for",
    "start": "76479",
    "end": "82280"
  },
  {
    "text": "example you could just place your workloads to uh to be allocated CPUs from different cores such that there is",
    "start": "82280",
    "end": "88840"
  },
  {
    "text": "no interference and that could would help you um unlock the maximum Potential from your",
    "start": "88840",
    "end": "95159"
  },
  {
    "text": "Hardware so for our experiment uh we are using this chatbot it's just a rack",
    "start": "95280",
    "end": "102320"
  },
  {
    "text": "pipeline which has all these typical components uh the query component embedding Vector database llm relevant",
    "start": "102320",
    "end": "108439"
  },
  {
    "text": "data and so on so these components can be uh kubernetes microservices in themselves we are specifically focusing",
    "start": "108439",
    "end": "115200"
  },
  {
    "text": "on the llm uh inference component so you could have multiple replicas of of this component running and in the experiment",
    "start": "115200",
    "end": "122600"
  },
  {
    "text": "we'll see how placing the replicas on the Node strategically can help you uh",
    "start": "122600",
    "end": "128360"
  },
  {
    "text": "optimize the hardware potential so in our experiment the goal",
    "start": "128360",
    "end": "134680"
  },
  {
    "text": "is to balance the latency throughput and the resource usage such that everything is optimal as much as",
    "start": "134680",
    "end": "142319"
  },
  {
    "text": "possible um the first step of our experiment is we are preparing the data collection we have a third party uh",
    "start": "142319",
    "end": "148920"
  },
  {
    "text": "python Library which we are using to collect the data and we are also augmenting it modifying it a bit to add",
    "start": "148920",
    "end": "155080"
  },
  {
    "text": "some instrumentation that's useful for our U experiment results",
    "start": "155080",
    "end": "160480"
  },
  {
    "text": "analysis the next step is we run the tests on the instrumented data and we visualize these uh in the",
    "start": "160480",
    "end": "167840"
  },
  {
    "text": "middle of the show because we want to see if there is anything that needs to be corrected or if there is any strategy",
    "start": "167840",
    "end": "173239"
  },
  {
    "text": "that is giving us poor benchmarks than our references we correct them and uh we",
    "start": "173239",
    "end": "179440"
  },
  {
    "text": "run the the experiment with the corrected values so after we have run the experiments we analyze the data and",
    "start": "179440",
    "end": "185280"
  },
  {
    "text": "we draw conclusions from it so I'll walk over the prepare uh data collection step uh and deep dive into it how we are",
    "start": "185280",
    "end": "191720"
  },
  {
    "text": "instrumenting the data and anti will walk over the other steps so if you were to uh we are using",
    "start": "191720",
    "end": "199040"
  },
  {
    "text": "a python Library uh the third party python library for data collection and if you were to uh import a third party",
    "start": "199040",
    "end": "206480"
  },
  {
    "text": "library and a call a function uh in it you would just import that library and call that function in a simple way now",
    "start": "206480",
    "end": "213040"
  },
  {
    "text": "python helps you to even modif like it goes a bit further and help you modify",
    "start": "213040",
    "end": "218480"
  },
  {
    "text": "this uh function and add your custom logic uh by using this decorator uh",
    "start": "218480",
    "end": "223680"
  },
  {
    "text": "wrapper pattern so we are walking over the simple example before we walk over the complex ones to explain how this",
    "start": "223680",
    "end": "229480"
  },
  {
    "text": "works so in this example you see that uh there is the same third party Library which has this function which is just",
    "start": "229480",
    "end": "235720"
  },
  {
    "text": "printing uh the function name you have your main program in which you're calling this function from the library",
    "start": "235720",
    "end": "241439"
  },
  {
    "text": "now what you want to do is you want to print a statement before the function is called and print a statement after the",
    "start": "241439",
    "end": "246920"
  },
  {
    "text": "function is called so what you will do is you add a wrapper uh method and pass",
    "start": "246920",
    "end": "252280"
  },
  {
    "text": "your main fun uh the original function as a parameter print uh something C some",
    "start": "252280",
    "end": "257840"
  },
  {
    "text": "custom logic before it call the function print some custom logic after it now this line is where the magic happens um",
    "start": "257840",
    "end": "265880"
  },
  {
    "text": "you pass your original function lip. funk in the wrapper and then you assign",
    "start": "265880",
    "end": "271120"
  },
  {
    "text": "the value that's returned from this uh wrapper to the original function this is how uh you can use the decorator pattern",
    "start": "271120",
    "end": "278160"
  },
  {
    "text": "to modify the third party libraries without actually modifying",
    "start": "278160",
    "end": "283240"
  },
  {
    "text": "them uh now we'll walk over the complex example which we are using in our uh experiment this is our my chart",
    "start": "283240",
    "end": "289560"
  },
  {
    "text": "application this is basically uh we are using the uh hugging pH uh tokenizer",
    "start": "289560",
    "end": "295720"
  },
  {
    "text": "Transformer library and we are using the uh GPT Neo 2.7 B to generate the text",
    "start": "295720",
    "end": "302240"
  },
  {
    "text": "and we are passing that to the streamer library and that streamer will print stream the output um for the experiments",
    "start": "302240",
    "end": "310479"
  },
  {
    "text": "so now the streamed output we wanted to add some time stamps to it so that we can analyze it carefully for our",
    "start": "310479",
    "end": "316320"
  },
  {
    "text": "experiments so this is this there is a method uh in the streamer library in the streamer object it's the put method what",
    "start": "316320",
    "end": "323880"
  },
  {
    "text": "this does is every time the model generates the text it calls the put method which decodes whatever text the",
    "start": "323880",
    "end": "330600"
  },
  {
    "text": "model has generated into a human readable form so now for this decoded text we are attaching the timestamps and",
    "start": "330600",
    "end": "337840"
  },
  {
    "text": "we are passing this uh put as a parameter to this rapper function which makes sure that it adds the time stamps",
    "start": "337840",
    "end": "344639"
  },
  {
    "text": "uh to the data that's useful for our experiments here is a link to an's repository which has the complete code",
    "start": "344639",
    "end": "351319"
  },
  {
    "text": "of this experiment and Ane will talk about the text test experiments and the",
    "start": "351319",
    "end": "357240"
  },
  {
    "text": "results thanks okay so that you so now you have",
    "start": "357240",
    "end": "363319"
  },
  {
    "text": "propably idea that how we can instrument and add time stamps inside uh python",
    "start": "363319",
    "end": "369440"
  },
  {
    "text": "libraries in the Transformer libraries without touching the source code of those",
    "start": "369440",
    "end": "375199"
  },
  {
    "text": "libraries uh I'll show something else as well though we are in addition to",
    "start": "375199",
    "end": "380280"
  },
  {
    "text": "Gathering time stamps from there we are also Gathering some system metrics in",
    "start": "380280",
    "end": "386039"
  },
  {
    "text": "our benchmarks so these include the standard stuff from under the proc file",
    "start": "386039",
    "end": "391319"
  },
  {
    "text": "system like Numa Maps saying that how much of the memory that our inferences",
    "start": "391319",
    "end": "397120"
  },
  {
    "text": "are using are allocated from different Numa nodes and also Pro bit status is",
    "start": "397120",
    "end": "402960"
  },
  {
    "text": "quite interesting because there we can see if there is like context switches on",
    "start": "402960",
    "end": "409440"
  },
  {
    "text": "what what's the CPU pinning of the process and how much it consumes memory at which point for those who are",
    "start": "409440",
    "end": "416000"
  },
  {
    "text": "interested in more like Hardware counter I would give a tip to check out this",
    "start": "416000",
    "end": "422919"
  },
  {
    "text": "kind of PCM project uh that's performance counter monitor project because with those tools that are",
    "start": "422919",
    "end": "430280"
  },
  {
    "text": "included in this you can find it from the GitHub so with those tools you can",
    "start": "430280",
    "end": "435319"
  },
  {
    "text": "get uh counters like cash misses cash hits cash",
    "start": "435319",
    "end": "441199"
  },
  {
    "text": "occupancy uh instruction counts and PCI Pand with and whatnot so quite quite",
    "start": "441199",
    "end": "447599"
  },
  {
    "text": "interesting low-level counters okay and all this data that we are",
    "start": "447599",
    "end": "453319"
  },
  {
    "text": "collecting from the system and the instrumented time stamps from the Transformer libraries we store store in",
    "start": "453319",
    "end": "460800"
  },
  {
    "text": "the Raw format so that we can then post process and count the token intervals",
    "start": "460800",
    "end": "466680"
  },
  {
    "text": "for instance from there why we are not uh storing token intervals right away",
    "start": "466680",
    "end": "473159"
  },
  {
    "text": "the reason is that if we can find some performance fluctuations during The",
    "start": "473159",
    "end": "478319"
  },
  {
    "text": "Benchmark then we can match using these time stamps uh to the system data so that we",
    "start": "478319",
    "end": "485800"
  },
  {
    "text": "can find that what what could be causing for instance performance",
    "start": "485800",
    "end": "492120"
  },
  {
    "text": "degradation okay then to the real tests um so um we are giving the same",
    "start": "492360",
    "end": "500280"
  },
  {
    "text": "prompt to the llm uh with in different parameter combinations and these",
    "start": "500280",
    "end": "506440"
  },
  {
    "text": "parameters now are from the of the four next forms so",
    "start": "506440",
    "end": "514959"
  },
  {
    "text": "um there is a number of parallel LM llm inferences running on the Node uh so",
    "start": "514959",
    "end": "522200"
  },
  {
    "text": "that's basically a replica count because we used a single node cluster for this",
    "start": "522200",
    "end": "527440"
  },
  {
    "text": "case uh then there is a subn numa clustering which is a bias option",
    "start": "527440",
    "end": "532880"
  },
  {
    "text": "selling that uh how many Numa nodes should be exposed from the uh processor",
    "start": "532880",
    "end": "538920"
  },
  {
    "text": "that we are using using which is here a fifth gen zon process two socket system",
    "start": "538920",
    "end": "545240"
  },
  {
    "text": "256 CPUs on that uh uh",
    "start": "545240",
    "end": "551040"
  },
  {
    "text": "server and then uh last two options so we choose the number and variate the",
    "start": "551040",
    "end": "558000"
  },
  {
    "text": "number of logical CPUs that we are allocating for each llm inference container and that number ranges from",
    "start": "558000",
    "end": "565519"
  },
  {
    "text": "four up to 256 CPUs and then also we switch on and off this",
    "start": "565519",
    "end": "572959"
  },
  {
    "text": "kind of option that whether we are going to allocate both hypo threats from each",
    "start": "572959",
    "end": "579440"
  },
  {
    "text": "physical CPU core in the server or are we going to allocate only one hypothet",
    "start": "579440",
    "end": "585920"
  },
  {
    "text": "per core and leave the other [Music] unused and how we are doing the last two",
    "start": "585920",
    "end": "594640"
  },
  {
    "text": "options we are using the balloons resource policy from NY plugins for that",
    "start": "594640",
    "end": "600320"
  },
  {
    "text": "and for those who n plugins or the balloons resource Poli policy are new here is how they connect to the",
    "start": "600320",
    "end": "606720"
  },
  {
    "text": "kubernetes stack on the Node so uh both container D and cryo run",
    "start": "606720",
    "end": "615839"
  },
  {
    "text": "times I implement this NRI interface sort of in NRI server",
    "start": "615839",
    "end": "622480"
  },
  {
    "text": "where n NRI plugins can reage that too so these NRI plugins can be managed by",
    "start": "622480",
    "end": "628320"
  },
  {
    "text": "kubernetes so they can can run in the containers and they can be system managed processes and they can be even",
    "start": "628320",
    "end": "634000"
  },
  {
    "text": "container runtime managed processes so that NRI framework itself starts those",
    "start": "634000",
    "end": "640560"
  },
  {
    "text": "up in our case this n balloons resource policy is a u we run it as a demon set",
    "start": "640560",
    "end": "647160"
  },
  {
    "text": "so it's a separate container there that registers to The Container run time and",
    "start": "647160",
    "end": "653279"
  },
  {
    "text": "listen to what is happening and when cupet tells that now that we are creating a new container for for llm",
    "start": "653279",
    "end": "659920"
  },
  {
    "text": "inference then this N Balance policy answers that hey put it on those CPUs so",
    "start": "659920",
    "end": "667040"
  },
  {
    "text": "create a balloon out of these four CPUs for instance and put the llm inference",
    "start": "667040",
    "end": "673120"
  },
  {
    "text": "there so the con run time then gives this information in oci to the lowlevel",
    "start": "673120",
    "end": "679200"
  },
  {
    "text": "run time which then writes it to the uh c groups CPUs at. CPUs and that's what",
    "start": "679200",
    "end": "684880"
  },
  {
    "text": "how we get the CPU pinning and that's how we can control are we using both",
    "start": "684880",
    "end": "690639"
  },
  {
    "text": "hyper threats or one hyper threat per physical core for",
    "start": "690639",
    "end": "696480"
  },
  {
    "text": "instance okay now let's start visualizing the data short reminder this",
    "start": "696480",
    "end": "704320"
  },
  {
    "text": "is performance data if you try this at home you might get a bit different data",
    "start": "704320",
    "end": "709560"
  },
  {
    "text": "or the same data but this is at least the data that we got so in xaxis we have the number of",
    "start": "709560",
    "end": "719279"
  },
  {
    "text": "inferences that are running in parallel on the Node on Y axis we have a",
    "start": "719279",
    "end": "726760"
  },
  {
    "text": "multiplier of the latency where the x1x means that here we are using all the CPU",
    "start": "726760",
    "end": "734120"
  },
  {
    "text": "power for a single inference that is running on the note so all 256 CPUs are",
    "start": "734120",
    "end": "740519"
  },
  {
    "text": "allocated for the same um inference",
    "start": "740519",
    "end": "745839"
  },
  {
    "text": "run um title is is going to be the number of CPUs that we are using for",
    "start": "745839",
    "end": "753000"
  },
  {
    "text": "each inference so for instance here we have eight CPUs in use one inference",
    "start": "753000",
    "end": "758079"
  },
  {
    "text": "running and we can see that here if we are using the blue one that is one hyper",
    "start": "758079",
    "end": "764639"
  },
  {
    "text": "threat per core we get only double the latency compared to the situation where",
    "start": "764639",
    "end": "770000"
  },
  {
    "text": "we would be running on all 256 CPUs if",
    "start": "770000",
    "end": "775079"
  },
  {
    "text": "you use both hypo threats per core then the latency is bigger okay and we are going to have",
    "start": "775079",
    "end": "782160"
  },
  {
    "text": "many of these boxes in these visualizations because we are repeating the same test run many times to see that",
    "start": "782160",
    "end": "788240"
  },
  {
    "text": "if there is variation there and there might be a bit different number of these",
    "start": "788240",
    "end": "793920"
  },
  {
    "text": "little boxes because we have not run every single parameter set equally many",
    "start": "793920",
    "end": "800240"
  },
  {
    "text": "times so that explains the sort of holes in these graphs and the more opaque this",
    "start": "800240",
    "end": "807839"
  },
  {
    "text": "box is the more data points is there okay now that you see how the data",
    "start": "807839",
    "end": "814600"
  },
  {
    "text": "is visualized then let's go analyzing it so here we have a 4 8 16 and 24 CPUs per",
    "start": "814600",
    "end": "826600"
  },
  {
    "text": "inference and currently running only one inference on the Node and here we can",
    "start": "826600",
    "end": "832519"
  },
  {
    "text": "observe that if we move from four to 8 CPUs we can get much better latency and",
    "start": "832519",
    "end": "839600"
  },
  {
    "text": "from 8 to 16 CPUs the Improvement is still clear from 16 to 24 there is not",
    "start": "839600",
    "end": "847360"
  },
  {
    "text": "that much improvement anymore and if we want to squeeze the latency down to the",
    "start": "847360",
    "end": "853000"
  },
  {
    "text": "one then we need uh 160 CPUs on the host so here the takeaway from this",
    "start": "853000",
    "end": "862120"
  },
  {
    "text": "analysis is that perhaps spending all that 256 CPUs for a single inference is",
    "start": "862120",
    "end": "868399"
  },
  {
    "text": "not quite optimal indeed as as we see that we can get the best um latency already with 160",
    "start": "868399",
    "end": "876639"
  },
  {
    "text": "CPUs and after that adding more CPUs for that inference container doesn't help at",
    "start": "876639",
    "end": "884920"
  },
  {
    "text": "all the other analysis we are analyzing now the number of parallel inferences",
    "start": "885160",
    "end": "890920"
  },
  {
    "text": "running on that note so let's at two four and eight concurrent inferences and",
    "start": "890920",
    "end": "898199"
  },
  {
    "text": "here you you can see already like that if we are using four CPUs per inference",
    "start": "898199",
    "end": "905639"
  },
  {
    "text": "the latency does not really grow much even if we are running eight inferences",
    "start": "905639",
    "end": "911120"
  },
  {
    "text": "in parallel but if we would be using eight CPUs per inference then the in",
    "start": "911120",
    "end": "917480"
  },
  {
    "text": "latency growth is pretty clear from four CPUs to sorry from four concurrent",
    "start": "917480",
    "end": "926320"
  },
  {
    "text": "inferences to eight concurrent inference but let's before analyzing that too far",
    "start": "926320",
    "end": "933480"
  },
  {
    "text": "let's see that what happens here because it seems that we have like three speed",
    "start": "933480",
    "end": "938839"
  },
  {
    "text": "categories of um inferences so latency is pretty much split in three and what",
    "start": "938839",
    "end": "946079"
  },
  {
    "text": "what is causing that um in the postprocessing analysis we can find that",
    "start": "946079",
    "end": "953839"
  },
  {
    "text": "okay in some cases we have such a placement of workload",
    "start": "953839",
    "end": "959959"
  },
  {
    "text": "so that when we are using uh two uh sorry yeah this is a two socket system",
    "start": "959959",
    "end": "966959"
  },
  {
    "text": "so we are using snc off which means that there is no subn Numa nodes each socket",
    "start": "966959",
    "end": "972839"
  },
  {
    "text": "is its own Numa node so we got we happen to have a",
    "start": "972839",
    "end": "978720"
  },
  {
    "text": "placement where where",
    "start": "978720",
    "end": "983759"
  },
  {
    "text": "um uh three of out of four inferences were put on the one socket and then one",
    "start": "983759",
    "end": "992680"
  },
  {
    "text": "inference was running on the other socket and this one inference that got its own socket it was running very fast",
    "start": "992680",
    "end": "999519"
  },
  {
    "text": "so there was a very low latency and in this case where we had three running on the same socket then that gives a bit",
    "start": "999519",
    "end": "1006519"
  },
  {
    "text": "and clearly higher higher um latency but in the case where we had",
    "start": "1006519",
    "end": "1013600"
  },
  {
    "text": "snc snc mode two which means that every socket is seen as two Numan noes uh in",
    "start": "1013600",
    "end": "1021040"
  },
  {
    "text": "this case we got placement where uh every inference it was running in its",
    "start": "1021040",
    "end": "1026918"
  },
  {
    "text": "own nanode which means that it actually got its own memory controller and it uh",
    "start": "1026919",
    "end": "1032520"
  },
  {
    "text": "communicating with on its own memory dims so they are very they give very",
    "start": "1032520",
    "end": "1038798"
  },
  {
    "text": "stable results and also uh this subnode helps then spreading these containers",
    "start": "1038799",
    "end": "1048319"
  },
  {
    "text": "more even ly of course here is we had actually some glitch which caused that",
    "start": "1048319",
    "end": "1055840"
  },
  {
    "text": "the uh third third uh or fourth uh inference",
    "start": "1055840",
    "end": "1061960"
  },
  {
    "text": "was put to the same socket as two before and not to the soet where was only one one",
    "start": "1061960",
    "end": "1069559"
  },
  {
    "text": "inference running so this is not like apples to apples comparison so that was that was due to",
    "start": "1069559",
    "end": "1077400"
  },
  {
    "text": "having having some some um back back plane consolle plane uh workflows also",
    "start": "1077400",
    "end": "1085120"
  },
  {
    "text": "allocated on that note with different uh",
    "start": "1085120",
    "end": "1090360"
  },
  {
    "text": "resource requirements but anyway the takeway is that this snc mode subn Numa node uh",
    "start": "1090360",
    "end": "1099600"
  },
  {
    "text": "that protects these inferences from interference from the same socket so",
    "start": "1099600",
    "end": "1105159"
  },
  {
    "text": "actually if I go a bit back um you can see in one single inference case here is",
    "start": "1105159",
    "end": "1112919"
  },
  {
    "text": "also some strange things happening and little it looks a bit weird because there are two categories of speed and it",
    "start": "1112919",
    "end": "1120400"
  },
  {
    "text": "turned out that this is for Numa node case so we were running one inference",
    "start": "1120400",
    "end": "1126919"
  },
  {
    "text": "only on that uh server but it was running and utilizing",
    "start": "1126919",
    "end": "1132880"
  },
  {
    "text": "basically memory and CPU is only from one Numa node so it got exactly the same",
    "start": "1132880",
    "end": "1138360"
  },
  {
    "text": "lat as later on this case where we had four inferences and each was running on its",
    "start": "1138360",
    "end": "1145080"
  },
  {
    "text": "own num node so it's pretty good protection from what is happening in the",
    "start": "1145080",
    "end": "1150400"
  },
  {
    "text": "environment whether or not it produces uh speed that's another question",
    "start": "1150400",
    "end": "1158158"
  },
  {
    "text": "then okay then to the bad ideas",
    "start": "1158480",
    "end": "1164200"
  },
  {
    "text": "um one thing that we were VAR variating as I explained earlier was at how many",
    "start": "1164200",
    "end": "1170919"
  },
  {
    "text": "uh hyper threats we are allocating from each physical core and as long as we are",
    "start": "1170919",
    "end": "1177520"
  },
  {
    "text": "using Le less than half of the CPUs from the system which works pretty fine because then we can always find a",
    "start": "1177520",
    "end": "1184360"
  },
  {
    "text": "physical core which has free two free hypothet and we can pick up only one of",
    "start": "1184360",
    "end": "1190240"
  },
  {
    "text": "those but when we are getting and using more CPUs then we are getting this kind of",
    "start": "1190240",
    "end": "1197039"
  },
  {
    "text": "situation on the like in the left hand side that we are running two different",
    "start": "1197039",
    "end": "1203000"
  },
  {
    "text": "inferences on two um hyper threats of the same physical core and that turned",
    "start": "1203000",
    "end": "1209840"
  },
  {
    "text": "out to give pretty bad performance it was clearly worse than the case where we",
    "start": "1209840",
    "end": "1215640"
  },
  {
    "text": "were allocating both hyperth threats for the same um physical core so we are next",
    "start": "1215640",
    "end": "1223280"
  },
  {
    "text": "we are analyzing only the two remaining cases where we either use both hypothet",
    "start": "1223280",
    "end": "1231159"
  },
  {
    "text": "for the same uh inference or we allocate both hyp threats for the same inference",
    "start": "1231159",
    "end": "1238760"
  },
  {
    "text": "but use only one of those and let's see how it looks in these",
    "start": "1238760",
    "end": "1244440"
  },
  {
    "text": "visualizations so these um dots that are",
    "start": "1244440",
    "end": "1249799"
  },
  {
    "text": "attached with the arrow they both uh use four physical CES from the system in the",
    "start": "1249799",
    "end": "1255720"
  },
  {
    "text": "eight CPU case we have eight CPUs but they are like eight hyper threats from four physical cores and the blue one in",
    "start": "1255720",
    "end": "1263960"
  },
  {
    "text": "four CPU case is that we have four uh physical course use using one hyper",
    "start": "1263960",
    "end": "1270120"
  },
  {
    "text": "threat from each and as you can see the arrow points down so we get lower",
    "start": "1270120",
    "end": "1276520"
  },
  {
    "text": "latency whenever we use this leave the",
    "start": "1276520",
    "end": "1282360"
  },
  {
    "text": "other um logical core hyperthread idle and the same it it is not dependent on",
    "start": "1282360",
    "end": "1290120"
  },
  {
    "text": "the number of CPUs so we can see the same pattern also with all all the other CPU combinations that we were running so",
    "start": "1290120",
    "end": "1298279"
  },
  {
    "text": "the takeaway here is a nice special offer pay two take one so even if we",
    "start": "1298279",
    "end": "1305679"
  },
  {
    "text": "allocate like our re request resources eight CPUs for this container it's",
    "start": "1305679",
    "end": "1313080"
  },
  {
    "text": "better to use only four of those for the inference because then we get lower latency",
    "start": "1313080",
    "end": "1319720"
  },
  {
    "text": "finally analyze analyzing this node throughput so how many tokens we get out",
    "start": "1321200",
    "end": "1327679"
  },
  {
    "text": "from that node in total so we started with one replica running on the Node so",
    "start": "1327679",
    "end": "1334320"
  },
  {
    "text": "one inference only and when we use in this case eight CPUs for that and let's",
    "start": "1334320",
    "end": "1341840"
  },
  {
    "text": "concentrate on the blue marking stat is we use eight physical cores for that",
    "start": "1341840",
    "end": "1348679"
  },
  {
    "text": "that results in um the double the latency compared to the case where we",
    "start": "1348679",
    "end": "1354840"
  },
  {
    "text": "have all the CPUs in use which means of course that we have only half of the throughput compared to the case that we",
    "start": "1354840",
    "end": "1361840"
  },
  {
    "text": "are using all the CPUs but when we are increasing the number of um of these",
    "start": "1361840",
    "end": "1369760"
  },
  {
    "text": "inferences on the same note to two uh the balloons policy will place the other",
    "start": "1369760",
    "end": "1375320"
  },
  {
    "text": "inference on the other socket on in this two socket machine and they both run without any interference interference",
    "start": "1375320",
    "end": "1382480"
  },
  {
    "text": "between uh each other which means that we get exactly double the",
    "start": "1382480",
    "end": "1389840"
  },
  {
    "text": "throughput so now the throughput is in par with the case where we are using all",
    "start": "1389840",
    "end": "1395039"
  },
  {
    "text": "the CPUs for single inference furthermore if we are increasing again",
    "start": "1395039",
    "end": "1401679"
  },
  {
    "text": "for replicas now we are almost already the double the throughput of use or CPUs",
    "start": "1401679",
    "end": "1409279"
  },
  {
    "text": "to one inference case and again if you use eight parallel",
    "start": "1409279",
    "end": "1415480"
  },
  {
    "text": "inferences then there starts to be interference so we are not getting any",
    "start": "1415480",
    "end": "1420720"
  },
  {
    "text": "more double the through but but we get almost pretty much the triple the",
    "start": "1420720",
    "end": "1426000"
  },
  {
    "text": "through to the uh reference case so all CPU case so",
    "start": "1426000",
    "end": "1432919"
  },
  {
    "text": "the takeaway for the balance that was one of our goals is that we can actually",
    "start": "1432919",
    "end": "1440039"
  },
  {
    "text": "triple the server no uh token through but if we run eight inferences in",
    "start": "1440039",
    "end": "1447120"
  },
  {
    "text": "parallel on CPU uh and for that we actually are using only half of the CPUs available in",
    "start": "1447120",
    "end": "1456360"
  },
  {
    "text": "that node which leaves a lot of CPUs available for the other part parts of",
    "start": "1456360",
    "end": "1461480"
  },
  {
    "text": "the rack pipeline for instance for the database and for the other",
    "start": "1461480",
    "end": "1467039"
  },
  {
    "text": "inferences all right let's start wrapping up so these are the points",
    "start": "1467360",
    "end": "1473440"
  },
  {
    "text": "takeways from the methodology so if you are uh benchmarking python python",
    "start": "1473440",
    "end": "1481679"
  },
  {
    "text": "programs it's uh possible to uh instrument these python libraries",
    "start": "1481679",
    "end": "1488640"
  },
  {
    "text": "without without actually touching their code you can do that in your own main",
    "start": "1488640",
    "end": "1494799"
  },
  {
    "text": "program which Imports the libraries that the real original main program Imports and then just overwrite the fun",
    "start": "1494799",
    "end": "1501360"
  },
  {
    "text": "functions with wrappers and do whatever you like in those wers like print print",
    "start": "1501360",
    "end": "1506720"
  },
  {
    "text": "these timestamps regarding the data it turned out to be very handy to uh have all data",
    "start": "1506720",
    "end": "1513960"
  },
  {
    "text": "ra time time stamped because that gave us freedom even to change what we are",
    "start": "1513960",
    "end": "1519480"
  },
  {
    "text": "exactly measuring so for instance when we were starting these eight um",
    "start": "1519480",
    "end": "1525679"
  },
  {
    "text": "inferences in parallel not all of them started producing tokens at the same",
    "start": "1525679",
    "end": "1531279"
  },
  {
    "text": "time of course but because we were storing the raw timestamps from each of",
    "start": "1531279",
    "end": "1536880"
  },
  {
    "text": "the tokens we were able to PO in the post processing phase see that at that",
    "start": "1536880",
    "end": "1543120"
  },
  {
    "text": "point all of them are producing tokens and until that point they are all producing tokens and we were measuring",
    "start": "1543120",
    "end": "1550080"
  },
  {
    "text": "token latency between those time um time points only which gives pretty nice uh",
    "start": "1550080",
    "end": "1557559"
  },
  {
    "text": "results for the benchmarking because there is much less variance and we we",
    "start": "1557559",
    "end": "1563120"
  },
  {
    "text": "can concentrate on the case that is heaviest for the CPU uh recording utilities and examples",
    "start": "1563120",
    "end": "1570960"
  },
  {
    "text": "the PCM tool is interesting for low-level Hardware related metrics Opia",
    "start": "1570960",
    "end": "1580039"
  },
  {
    "text": "examples gives nice examples for benchmarking um so that you can just",
    "start": "1580039",
    "end": "1586799"
  },
  {
    "text": "apply apply some yums and then you get the full rack pipeline running on your",
    "start": "1586799",
    "end": "1593440"
  },
  {
    "text": "note and in your cluster and uh take away from Resource Management so we were",
    "start": "1593440",
    "end": "1599000"
  },
  {
    "text": "using NRI resource policies for controlling and variating that these",
    "start": "1599000",
    "end": "1604440"
  },
  {
    "text": "options that how to how to manage containers and with how to pin",
    "start": "1604440",
    "end": "1609679"
  },
  {
    "text": "CPUs uh takeways from the CPU inference part uh 256 CPUs is way too it's very",
    "start": "1609679",
    "end": "1619320"
  },
  {
    "text": "suboptimal for a single inference it's better to split it in part num nodes",
    "start": "1619320",
    "end": "1626480"
  },
  {
    "text": "we're g protecting uh inferences from interfering with each",
    "start": "1626480",
    "end": "1632120"
  },
  {
    "text": "other hyper threats it seems to better to pay two take one for the best",
    "start": "1632120",
    "end": "1639039"
  },
  {
    "text": "performance and recording the balance we were able to Triple the server throughput by using only half of the",
    "start": "1639039",
    "end": "1645640"
  },
  {
    "text": "CPUs of the server so this concludes our presentation you",
    "start": "1645640",
    "end": "1653000"
  },
  {
    "text": "can find these examples for instrumenting python code and you can",
    "start": "1653000",
    "end": "1658159"
  },
  {
    "text": "find the balloons policy uh configuration example and how to install",
    "start": "1658159",
    "end": "1664039"
  },
  {
    "text": "balloons policy from these links and then there are these the links",
    "start": "1664039",
    "end": "1670240"
  },
  {
    "text": "for this presentation and then there are extal links for the Opia project and PCM tool and N",
    "start": "1670240",
    "end": "1676600"
  },
  {
    "text": "plugins okay thank you for your attention are there any questions in",
    "start": "1676600",
    "end": "1682820"
  },
  {
    "text": "[Applause]",
    "start": "1682820",
    "end": "1688899"
  },
  {
    "text": "Mind questions no questions this",
    "start": "1690679",
    "end": "1698320"
  },
  {
    "text": "time okay so the question is about should we disable",
    "start": "1703519",
    "end": "1709080"
  },
  {
    "text": "hyperthreading um that's a great question thanks so uh",
    "start": "1709080",
    "end": "1714840"
  },
  {
    "text": "what you can do with balloons policy is that you can assign these um inference",
    "start": "1714840",
    "end": "1721159"
  },
  {
    "text": "containers to these kind of balloons set of CPUs where hyping is disabled",
    "start": "1721159",
    "end": "1727720"
  },
  {
    "text": "practically for those CPUs but still in the rack pipeline you have many other",
    "start": "1727720",
    "end": "1732799"
  },
  {
    "text": "workflows as well and you might be running many other workflows on the same node like databases and whatnot which",
    "start": "1732799",
    "end": "1739080"
  },
  {
    "text": "actually benefit from hypo threading and with this kind of resource policy you are able to disable hyping on those",
    "start": "1739080",
    "end": "1747279"
  },
  {
    "text": "containers that actually take a heat from it and the rest can use it on their course but if I'm going to run database",
    "start": "1747279",
    "end": "1754120"
  },
  {
    "text": "on the other hyp then it's going to interfere with this INF uh so can the datab Bas and in",
    "start": "1754120",
    "end": "1763760"
  },
  {
    "text": "interfere with this inference not actually because what we were doing with",
    "start": "1763760",
    "end": "1769200"
  },
  {
    "text": "we were allocating both hyper threats for this inference but giving only one",
    "start": "1769200",
    "end": "1774760"
  },
  {
    "text": "so the other one was allocated but not used and it could not be used by any other container in the node either so",
    "start": "1774760",
    "end": "1783480"
  },
  {
    "text": "that that's what we were able to do with this resource policy thanks for presentation quick",
    "start": "1783480",
    "end": "1790360"
  },
  {
    "text": "question you just shows this synthetic example when you run just inference have you run similar kind of tests when you",
    "start": "1790360",
    "end": "1796640"
  },
  {
    "text": "run not only inference but everything else just to fulfill the not as much as you can yes we have run these kind of",
    "start": "1796640",
    "end": "1804320"
  },
  {
    "text": "tests as well and uh maybe I'll ask you to come to the",
    "start": "1804320",
    "end": "1810600"
  },
  {
    "text": "Intel Booth so so there we have actually op a demo where we are running the whole",
    "start": "1810600",
    "end": "1817799"
  },
  {
    "text": "Pipeline and what we have done in this kind of uh tests is also that we have",
    "start": "1817799",
    "end": "1823960"
  },
  {
    "text": "used some sort of scaling so if we can recognize that there's a",
    "start": "1823960",
    "end": "1829360"
  },
  {
    "text": "um pipeline is getting the queue is getting longer for the llm inference for",
    "start": "1829360",
    "end": "1835279"
  },
  {
    "text": "instance then we can uh create another replica there and run it on the same",
    "start": "1835279",
    "end": "1840559"
  },
  {
    "text": "node and at that point it's very important also to manage these CPUs so",
    "start": "1840559",
    "end": "1846200"
  },
  {
    "text": "that they run on different CPUs which enables us to scale really many replicas",
    "start": "1846200",
    "end": "1852039"
  },
  {
    "text": "on the same note so and in those cases we have managed",
    "start": "1852039",
    "end": "1857320"
  },
  {
    "text": "the CP so that we have basically used the a bunch of CPUs to run all the con",
    "start": "1857320",
    "end": "1864200"
  },
  {
    "text": "um databases and these user interfaces and whatnot other microservices and have",
    "start": "1864200",
    "end": "1871279"
  },
  {
    "text": "DED dedicated CPUs then only for these llm and other like embedding and",
    "start": "1871279",
    "end": "1877200"
  },
  {
    "text": "reranking in inferences that are in the same pipeline thanks for thanks",
    "start": "1877200",
    "end": "1885799"
  }
]