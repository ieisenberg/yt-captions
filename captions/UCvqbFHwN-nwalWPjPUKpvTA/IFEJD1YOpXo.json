[
  {
    "start": "0",
    "end": "68000"
  },
  {
    "text": "hello everyone i'm swati segal i'm a",
    "start": "160",
    "end": "2480"
  },
  {
    "text": "principal software engineer working for",
    "start": "2480",
    "end": "4080"
  },
  {
    "text": "redhat",
    "start": "4080",
    "end": "5520"
  },
  {
    "text": "and i'm marla weston i'm a cloud native",
    "start": "5520",
    "end": "7919"
  },
  {
    "text": "architect working for intel and we're",
    "start": "7919",
    "end": "9840"
  },
  {
    "text": "presenting this is the way a crash",
    "start": "9840",
    "end": "11679"
  },
  {
    "text": "course in the intricacies of managing",
    "start": "11679",
    "end": "13280"
  },
  {
    "text": "cpus and kubernetes",
    "start": "13280",
    "end": "16920"
  },
  {
    "text": "the scope we will cover cpu management",
    "start": "17359",
    "end": "19439"
  },
  {
    "text": "requirements only",
    "start": "19439",
    "end": "21119"
  },
  {
    "text": "but we'll also reference other projects",
    "start": "21119",
    "end": "23439"
  },
  {
    "text": "things aren't in a vacuum and your cpus",
    "start": "23439",
    "end": "26240"
  },
  {
    "text": "are just part of the whole picture",
    "start": "26240",
    "end": "30119"
  },
  {
    "text": "in the beginning systems were simple",
    "start": "30240",
    "end": "34000"
  },
  {
    "text": "and here's an idea of why they were",
    "start": "34640",
    "end": "36320"
  },
  {
    "text": "simple",
    "start": "36320",
    "end": "37520"
  },
  {
    "text": "uh nodes that had only single sockets",
    "start": "37520",
    "end": "40160"
  },
  {
    "text": "there was a nick there was some memory",
    "start": "40160",
    "end": "42160"
  },
  {
    "text": "there was some switch and there's world",
    "start": "42160",
    "end": "43840"
  },
  {
    "text": "because you're only really running",
    "start": "43840",
    "end": "44960"
  },
  {
    "text": "microservices at the beginning with",
    "start": "44960",
    "end": "46559"
  },
  {
    "text": "kubernetes",
    "start": "46559",
    "end": "49280"
  },
  {
    "text": "kubelet was designed for simple as well",
    "start": "50079",
    "end": "52160"
  },
  {
    "text": "at first",
    "start": "52160",
    "end": "54719"
  },
  {
    "text": "so here's what the early kubla looks",
    "start": "55760",
    "end": "57440"
  },
  {
    "text": "like it has none of the the managers",
    "start": "57440",
    "end": "59760"
  },
  {
    "text": "that we have today",
    "start": "59760",
    "end": "61120"
  },
  {
    "text": "and it's just a simple throughput",
    "start": "61120",
    "end": "63120"
  },
  {
    "text": "between your pod and the resources on",
    "start": "63120",
    "end": "65680"
  },
  {
    "text": "the node",
    "start": "65680",
    "end": "68080"
  },
  {
    "start": "68000",
    "end": "68000"
  },
  {
    "text": "so i want to take you back in time early",
    "start": "69200",
    "end": "71840"
  },
  {
    "text": "2017 free kubernetes 1.8 release",
    "start": "71840",
    "end": "75600"
  },
  {
    "text": "and show you how the landscape looked",
    "start": "75600",
    "end": "77920"
  },
  {
    "text": "like from resource point of view",
    "start": "77920",
    "end": "80080"
  },
  {
    "text": "at this point we have cpu and memory as",
    "start": "80080",
    "end": "82799"
  },
  {
    "text": "only two native resources supported",
    "start": "82799",
    "end": "85759"
  },
  {
    "text": "requests and limits that you see",
    "start": "85759",
    "end": "88000"
  },
  {
    "text": "highlighted here are mechanisms",
    "start": "88000",
    "end": "89920"
  },
  {
    "text": "kubernetes uses to control these",
    "start": "89920",
    "end": "91840"
  },
  {
    "text": "resources",
    "start": "91840",
    "end": "93119"
  },
  {
    "text": "requests are what a container asks for",
    "start": "93119",
    "end": "95759"
  },
  {
    "text": "and is guaranteed to get cube scheduler",
    "start": "95759",
    "end": "98000"
  },
  {
    "text": "uses this to identify a suitable node",
    "start": "98000",
    "end": "100560"
  },
  {
    "text": "that can fulfill the resource",
    "start": "100560",
    "end": "102000"
  },
  {
    "text": "requirement",
    "start": "102000",
    "end": "103520"
  },
  {
    "text": "limits on the other hand make sure a",
    "start": "103520",
    "end": "105759"
  },
  {
    "text": "container never goes above certain value",
    "start": "105759",
    "end": "108399"
  },
  {
    "text": "cubelet and container runtime uses this",
    "start": "108399",
    "end": "110320"
  },
  {
    "text": "to enforce limits on contain on the",
    "start": "110320",
    "end": "112799"
  },
  {
    "text": "resource consumption",
    "start": "112799",
    "end": "114720"
  },
  {
    "text": "cfs quota is what is used to enforce cpu",
    "start": "114720",
    "end": "117680"
  },
  {
    "text": "limits on the containers",
    "start": "117680",
    "end": "119759"
  },
  {
    "text": "particularly from cpu perspective all",
    "start": "119759",
    "end": "122399"
  },
  {
    "text": "the pods and containers running on a",
    "start": "122399",
    "end": "124399"
  },
  {
    "text": "compute node can execute on any of the",
    "start": "124399",
    "end": "127119"
  },
  {
    "text": "available cores on that node",
    "start": "127119",
    "end": "129280"
  },
  {
    "text": "this level of resource control is not",
    "start": "129280",
    "end": "131520"
  },
  {
    "text": "sufficient when pods are running cpu",
    "start": "131520",
    "end": "133760"
  },
  {
    "text": "intensive workloads and contending for",
    "start": "133760",
    "end": "136239"
  },
  {
    "text": "cpu resources available on that",
    "start": "136239",
    "end": "138239"
  },
  {
    "text": "particular node",
    "start": "138239",
    "end": "140000"
  },
  {
    "text": "these there can be scenarios where uh",
    "start": "140000",
    "end": "142480"
  },
  {
    "text": "workloads get moved to different cpus",
    "start": "142480",
    "end": "145520"
  },
  {
    "text": "which could impact the performance if it",
    "start": "145520",
    "end": "147440"
  },
  {
    "text": "is sensitive to contact switches",
    "start": "147440",
    "end": "150640"
  },
  {
    "text": "um",
    "start": "150640",
    "end": "151760"
  },
  {
    "text": "one interesting thing to note here is",
    "start": "151760",
    "end": "153680"
  },
  {
    "text": "that pod spec looks very similar to what",
    "start": "153680",
    "end": "156160"
  },
  {
    "text": "it looks like today",
    "start": "156160",
    "end": "157840"
  },
  {
    "text": "but there are some nuances that you need",
    "start": "157840",
    "end": "159599"
  },
  {
    "text": "to know to be able to tap into some of",
    "start": "159599",
    "end": "161599"
  },
  {
    "text": "the new advanced features that we'll",
    "start": "161599",
    "end": "163760"
  },
  {
    "text": "cover in the subsequent slides",
    "start": "163760",
    "end": "167200"
  },
  {
    "text": "so around this time there was a lot of",
    "start": "168400",
    "end": "170480"
  },
  {
    "text": "discussion going on in the community to",
    "start": "170480",
    "end": "172640"
  },
  {
    "text": "enhance kubernetes in order to support",
    "start": "172640",
    "end": "174800"
  },
  {
    "text": "diverse and increasingly complex classes",
    "start": "174800",
    "end": "177200"
  },
  {
    "text": "of applications",
    "start": "177200",
    "end": "178959"
  },
  {
    "text": "discussions about how to provide",
    "start": "178959",
    "end": "180879"
  },
  {
    "text": "kubernetes the ability to run workloads",
    "start": "180879",
    "end": "183440"
  },
  {
    "text": "that require specialized hardware or",
    "start": "183440",
    "end": "185840"
  },
  {
    "text": "those that perform better when hardware",
    "start": "185840",
    "end": "187920"
  },
  {
    "text": "topology is taken into consideration",
    "start": "187920",
    "end": "190640"
  },
  {
    "text": "because of the momentum in the community",
    "start": "190640",
    "end": "192239"
  },
  {
    "text": "in this area a resource management",
    "start": "192239",
    "end": "194159"
  },
  {
    "text": "working group was formed which included",
    "start": "194159",
    "end": "196400"
  },
  {
    "text": "representatives from red hat google",
    "start": "196400",
    "end": "199200"
  },
  {
    "text": "intel nvidia microsoft and many more",
    "start": "199200",
    "end": "202879"
  },
  {
    "text": "the community identified once in the",
    "start": "202879",
    "end": "204959"
  },
  {
    "text": "compute resource management space that",
    "start": "204959",
    "end": "207200"
  },
  {
    "text": "was followed by cubelet extension",
    "start": "207200",
    "end": "208959"
  },
  {
    "text": "strategies to enable performance",
    "start": "208959",
    "end": "210959"
  },
  {
    "text": "sensitive workloads and improve resource",
    "start": "210959",
    "end": "213360"
  },
  {
    "text": "isolation",
    "start": "213360",
    "end": "214799"
  },
  {
    "text": "support for huge pages exclusive",
    "start": "214799",
    "end": "216959"
  },
  {
    "text": "allocation of cpus",
    "start": "216959",
    "end": "218720"
  },
  {
    "text": "specialized hardware support no more",
    "start": "218720",
    "end": "220640"
  },
  {
    "text": "alignment of resources became focal",
    "start": "220640",
    "end": "222640"
  },
  {
    "text": "points of the work that followed",
    "start": "222640",
    "end": "226000"
  },
  {
    "text": "as a result of this in kubernetes 1.8",
    "start": "226000",
    "end": "229519"
  },
  {
    "text": "huge pages became a native resource",
    "start": "229519",
    "end": "232560"
  },
  {
    "text": "cpu manager was introduced to support",
    "start": "232560",
    "end": "234720"
  },
  {
    "text": "container level cpu affinity",
    "start": "234720",
    "end": "236959"
  },
  {
    "text": "and device plug-in api was introduced as",
    "start": "236959",
    "end": "239599"
  },
  {
    "text": "a vendor-independent framework",
    "start": "239599",
    "end": "241680"
  },
  {
    "text": "to expose and consume devices",
    "start": "241680",
    "end": "246080"
  },
  {
    "text": "so this is how kubrick looked like from",
    "start": "246080",
    "end": "248000"
  },
  {
    "text": "resource management perspective after",
    "start": "248000",
    "end": "250080"
  },
  {
    "text": "these features were introduced in 1.8",
    "start": "250080",
    "end": "252000"
  },
  {
    "text": "release",
    "start": "252000",
    "end": "255000"
  },
  {
    "text": "if you give a mouse a cookie he's going",
    "start": "255120",
    "end": "256799"
  },
  {
    "text": "to ask for a glass of milk by laura",
    "start": "256799",
    "end": "258720"
  },
  {
    "text": "numeroff this is in reference to a",
    "start": "258720",
    "end": "260639"
  },
  {
    "text": "children's book but e hashman in the",
    "start": "260639",
    "end": "263120"
  },
  {
    "text": "signo meeting used in reference to users",
    "start": "263120",
    "end": "267720"
  },
  {
    "text": "so",
    "start": "268639",
    "end": "269360"
  },
  {
    "text": "we've we've covered the very very basic",
    "start": "269360",
    "end": "271520"
  },
  {
    "text": "very simple use case but let's start",
    "start": "271520",
    "end": "273759"
  },
  {
    "text": "looking at more complex ones so let's",
    "start": "273759",
    "end": "276400"
  },
  {
    "text": "talk about high performance use cases",
    "start": "276400",
    "end": "278080"
  },
  {
    "text": "which you have performance sensitive",
    "start": "278080",
    "end": "279600"
  },
  {
    "text": "workloads",
    "start": "279600",
    "end": "282479"
  },
  {
    "text": "so here's",
    "start": "282479",
    "end": "284000"
  },
  {
    "text": "at a very very high level",
    "start": "284000",
    "end": "285759"
  },
  {
    "text": "a high performance aiml cluster you can",
    "start": "285759",
    "end": "288080"
  },
  {
    "text": "tell it's aiml because it has the xps",
    "start": "288080",
    "end": "290160"
  },
  {
    "text": "but you can also use these for high",
    "start": "290160",
    "end": "291280"
  },
  {
    "text": "performance compute",
    "start": "291280",
    "end": "292720"
  },
  {
    "text": "and you no longer have a single socket",
    "start": "292720",
    "end": "295600"
  },
  {
    "text": "you now have you know memory on both",
    "start": "295600",
    "end": "297680"
  },
  {
    "text": "sides you have xp's on both sides you",
    "start": "297680",
    "end": "299680"
  },
  {
    "text": "have dual nooks you have these leaf",
    "start": "299680",
    "end": "301759"
  },
  {
    "text": "switches you have spine switches you",
    "start": "301759",
    "end": "303600"
  },
  {
    "text": "care about the location of your pods now",
    "start": "303600",
    "end": "306479"
  },
  {
    "text": "so you want them generally on the same",
    "start": "306479",
    "end": "308639"
  },
  {
    "text": "leaf switch and you care about whether",
    "start": "308639",
    "end": "311520"
  },
  {
    "text": "or not your cores are pinned",
    "start": "311520",
    "end": "313440"
  },
  {
    "text": "more specifically",
    "start": "313440",
    "end": "316320"
  },
  {
    "text": "so cpu manager is a beta feature that is",
    "start": "316880",
    "end": "319280"
  },
  {
    "text": "useful for cpu intensive workloads that",
    "start": "319280",
    "end": "321440"
  },
  {
    "text": "are sensitive to cpu throttling context",
    "start": "321440",
    "end": "323600"
  },
  {
    "text": "switches and require hyper threads from",
    "start": "323600",
    "end": "326560"
  },
  {
    "text": "the same physical core",
    "start": "326560",
    "end": "328479"
  },
  {
    "text": "cpu manager gives us the ability to",
    "start": "328479",
    "end": "330320"
  },
  {
    "text": "allocate cpu exclusively but in order to",
    "start": "330320",
    "end": "333039"
  },
  {
    "text": "achieve this we have to make sure that",
    "start": "333039",
    "end": "334800"
  },
  {
    "text": "pod belongs to the guaranteed quality of",
    "start": "334800",
    "end": "337280"
  },
  {
    "text": "service class",
    "start": "337280",
    "end": "338400"
  },
  {
    "text": "and has positive integral cpu request",
    "start": "338400",
    "end": "341520"
  },
  {
    "text": "internally cpu manager maintains logical",
    "start": "341520",
    "end": "344000"
  },
  {
    "text": "set of cpus which is also referred to as",
    "start": "344000",
    "end": "346400"
  },
  {
    "text": "pools we have exclusive shared",
    "start": "346400",
    "end": "349600"
  },
  {
    "text": "reserved and assignable pools the two",
    "start": "349600",
    "end": "352400"
  },
  {
    "text": "most important ones are exclusive and",
    "start": "352400",
    "end": "354320"
  },
  {
    "text": "shared exclusive pool corresponds to the",
    "start": "354320",
    "end": "356960"
  },
  {
    "text": "cpu sets assigned exclusively to",
    "start": "356960",
    "end": "359280"
  },
  {
    "text": "containers",
    "start": "359280",
    "end": "360639"
  },
  {
    "text": "shared",
    "start": "360639",
    "end": "361600"
  },
  {
    "text": "is the one where burstable best effort",
    "start": "361600",
    "end": "364639"
  },
  {
    "text": "and non-integral guaranteed containers",
    "start": "364639",
    "end": "366639"
  },
  {
    "text": "run",
    "start": "366639",
    "end": "368880"
  },
  {
    "start": "368000",
    "end": "368000"
  },
  {
    "text": "so a cubelet flag can be used to specify",
    "start": "369360",
    "end": "372000"
  },
  {
    "text": "the cpu manager policy",
    "start": "372000",
    "end": "374720"
  },
  {
    "text": "the default",
    "start": "374720",
    "end": "376080"
  },
  {
    "text": "configuration policy is none which",
    "start": "376080",
    "end": "378720"
  },
  {
    "text": "basically enables the existing default",
    "start": "378720",
    "end": "380960"
  },
  {
    "text": "cpu affinity scheme providing no",
    "start": "380960",
    "end": "383520"
  },
  {
    "text": "affinity beyond what the os scheduler",
    "start": "383520",
    "end": "386080"
  },
  {
    "text": "automatically gives us",
    "start": "386080",
    "end": "387919"
  },
  {
    "text": "even",
    "start": "387919",
    "end": "388800"
  },
  {
    "text": "even though cfs",
    "start": "388800",
    "end": "390639"
  },
  {
    "text": "quota is used to enforce poor cpu limits",
    "start": "390639",
    "end": "394080"
  },
  {
    "text": "the workload can move between different",
    "start": "394080",
    "end": "396080"
  },
  {
    "text": "cpu cores depending on the load on the",
    "start": "396080",
    "end": "398400"
  },
  {
    "text": "pod and the available capacity on the",
    "start": "398400",
    "end": "400800"
  },
  {
    "text": "worker node",
    "start": "400800",
    "end": "403440"
  },
  {
    "text": "when enabled with static policy cpu",
    "start": "403520",
    "end": "406080"
  },
  {
    "text": "manager allows us to allocate exclusive",
    "start": "406080",
    "end": "408639"
  },
  {
    "text": "cpus and no other containers can be",
    "start": "408639",
    "end": "411280"
  },
  {
    "text": "scheduled on those cpus please note that",
    "start": "411280",
    "end": "414960"
  },
  {
    "text": "this takes place at a container level",
    "start": "414960",
    "end": "416800"
  },
  {
    "text": "and there's no way for cpus to be",
    "start": "416800",
    "end": "419199"
  },
  {
    "text": "allocated at podlab",
    "start": "419199",
    "end": "422400"
  },
  {
    "start": "422000",
    "end": "422000"
  },
  {
    "text": "in kubernetes 122 an additional",
    "start": "422400",
    "end": "425199"
  },
  {
    "text": "configuration option called cpu manager",
    "start": "425199",
    "end": "427599"
  },
  {
    "text": "policy option was introduced that allows",
    "start": "427599",
    "end": "430560"
  },
  {
    "text": "us to fine-tune the behavior of static",
    "start": "430560",
    "end": "432560"
  },
  {
    "text": "policy",
    "start": "432560",
    "end": "433759"
  },
  {
    "text": "this feature graduated to beta in",
    "start": "433759",
    "end": "435840"
  },
  {
    "text": "kubernetes 123 but certain feature gates",
    "start": "435840",
    "end": "438960"
  },
  {
    "text": "might have to be enabled based on the",
    "start": "438960",
    "end": "440400"
  },
  {
    "text": "maturity level of the policy option",
    "start": "440400",
    "end": "442800"
  },
  {
    "text": "itself",
    "start": "442800",
    "end": "444960"
  },
  {
    "text": "the first option is full pcpu only this",
    "start": "444960",
    "end": "448479"
  },
  {
    "text": "is a beta option and has",
    "start": "448479",
    "end": "450960"
  },
  {
    "text": "visible visibility by default",
    "start": "450960",
    "end": "453680"
  },
  {
    "text": "this option allows to make the behavior",
    "start": "453680",
    "end": "455759"
  },
  {
    "text": "of latency sensitive applications more",
    "start": "455759",
    "end": "457759"
  },
  {
    "text": "predictable when running on systems",
    "start": "457759",
    "end": "459759"
  },
  {
    "text": "where hyper threading is enabled",
    "start": "459759",
    "end": "462160"
  },
  {
    "text": "we might want to guarantee that no",
    "start": "462160",
    "end": "464240"
  },
  {
    "text": "physical core is shared among different",
    "start": "464240",
    "end": "466160"
  },
  {
    "text": "containers made uh to meet applications",
    "start": "466160",
    "end": "469120"
  },
  {
    "text": "latency sensitive",
    "start": "469120",
    "end": "470720"
  },
  {
    "text": "and performance goals",
    "start": "470720",
    "end": "472720"
  },
  {
    "text": "um",
    "start": "472720",
    "end": "473759"
  },
  {
    "text": "with this option enabled um the pod will",
    "start": "473759",
    "end": "476800"
  },
  {
    "text": "be admitted by cubelet only if the cpu",
    "start": "476800",
    "end": "479680"
  },
  {
    "text": "request of all its containers can be",
    "start": "479680",
    "end": "482319"
  },
  {
    "text": "fulfilled by allocating full physical",
    "start": "482319",
    "end": "484400"
  },
  {
    "text": "cores",
    "start": "484400",
    "end": "485280"
  },
  {
    "text": "if the pod does not pass this admission",
    "start": "485280",
    "end": "487360"
  },
  {
    "text": "check the pod will be rejected with smt",
    "start": "487360",
    "end": "489759"
  },
  {
    "text": "alignment error",
    "start": "489759",
    "end": "492479"
  },
  {
    "text": "the second policy option is distribute",
    "start": "492479",
    "end": "494879"
  },
  {
    "text": "across pneuma this is an alpha option",
    "start": "494879",
    "end": "497199"
  },
  {
    "text": "and it's hidden by default",
    "start": "497199",
    "end": "499039"
  },
  {
    "text": "cpu manager by default um",
    "start": "499039",
    "end": "501919"
  },
  {
    "text": "is designed to pack cpus",
    "start": "501919",
    "end": "504080"
  },
  {
    "text": "onto one over node uh until it is filled",
    "start": "504080",
    "end": "507919"
  },
  {
    "text": "and then any of the remaining cpus",
    "start": "507919",
    "end": "510319"
  },
  {
    "text": "simply spill over to the next node",
    "start": "510319",
    "end": "513279"
  },
  {
    "text": "in parallel code this can lead to",
    "start": "513279",
    "end": "515399"
  },
  {
    "text": "undesired behavior as the slowest worker",
    "start": "515399",
    "end": "518800"
  },
  {
    "text": "can potentially potentially",
    "start": "518800",
    "end": "520479"
  },
  {
    "text": "act as a bottleneck",
    "start": "520479",
    "end": "522479"
  },
  {
    "text": "with this option enabled application",
    "start": "522479",
    "end": "524720"
  },
  {
    "text": "developers can ensure that all workers",
    "start": "524720",
    "end": "527200"
  },
  {
    "text": "run equally well on all the normal nodes",
    "start": "527200",
    "end": "530160"
  },
  {
    "text": "improving the overall performance of",
    "start": "530160",
    "end": "531680"
  },
  {
    "text": "these kind of",
    "start": "531680",
    "end": "532839"
  },
  {
    "text": "applications so now let's look at the",
    "start": "532839",
    "end": "535279"
  },
  {
    "start": "534000",
    "end": "534000"
  },
  {
    "text": "pod spec again so we have a guaranteed",
    "start": "535279",
    "end": "537360"
  },
  {
    "text": "point here which is evident from the",
    "start": "537360",
    "end": "539279"
  },
  {
    "text": "fact that resource resource request is",
    "start": "539279",
    "end": "541680"
  },
  {
    "text": "equal to resource limit and the request",
    "start": "541680",
    "end": "544640"
  },
  {
    "text": "of the cpus is integral",
    "start": "544640",
    "end": "547519"
  },
  {
    "text": "when the node is configured with cpu",
    "start": "547519",
    "end": "549600"
  },
  {
    "text": "manager's static policy we have integral",
    "start": "549600",
    "end": "552640"
  },
  {
    "text": "cpus and hence the c container is",
    "start": "552640",
    "end": "555040"
  },
  {
    "text": "allocated exclusive cpus",
    "start": "555040",
    "end": "557519"
  },
  {
    "text": "if in addition to that we have cpu",
    "start": "557519",
    "end": "559519"
  },
  {
    "text": "manager policy option configured as full",
    "start": "559519",
    "end": "561920"
  },
  {
    "text": "p cpu only the part will be rejected",
    "start": "561920",
    "end": "564320"
  },
  {
    "text": "with smt alignment error as this will",
    "start": "564320",
    "end": "566640"
  },
  {
    "text": "result in partial allocation of a core",
    "start": "566640",
    "end": "570640"
  },
  {
    "start": "571000",
    "end": "571000"
  },
  {
    "text": "so now we'll go into pneuma zones which",
    "start": "571680",
    "end": "573360"
  },
  {
    "text": "are not for the weak of heart so",
    "start": "573360",
    "end": "574800"
  },
  {
    "text": "remember we talked about memory cpu and",
    "start": "574800",
    "end": "577040"
  },
  {
    "text": "xpu and nick and locations",
    "start": "577040",
    "end": "580880"
  },
  {
    "text": "so if you have the case where the cpu is",
    "start": "581680",
    "end": "583839"
  },
  {
    "text": "in a different location from the",
    "start": "583839",
    "end": "585279"
  },
  {
    "text": "membrane xpu the upi bus basically",
    "start": "585279",
    "end": "588560"
  },
  {
    "text": "acts as a troll and collects time every",
    "start": "588560",
    "end": "590640"
  },
  {
    "text": "time you try to cross that bus",
    "start": "590640",
    "end": "593200"
  },
  {
    "text": "so this is not ideal instead you want",
    "start": "593200",
    "end": "595760"
  },
  {
    "text": "this particular case so you're not",
    "start": "595760",
    "end": "597279"
  },
  {
    "text": "having to cross the upi bus toll and",
    "start": "597279",
    "end": "600000"
  },
  {
    "text": "instead your memory your cpu and your xp",
    "start": "600000",
    "end": "602399"
  },
  {
    "text": "are all located in the same location",
    "start": "602399",
    "end": "606240"
  },
  {
    "start": "606000",
    "end": "606000"
  },
  {
    "text": "so around 2019 time frame which is",
    "start": "607200",
    "end": "610320"
  },
  {
    "text": "116 release the policy manager was",
    "start": "610320",
    "end": "612959"
  },
  {
    "text": "introduced this is a cupid component",
    "start": "612959",
    "end": "615120"
  },
  {
    "text": "that coordinates topology of resource",
    "start": "615120",
    "end": "617360"
  },
  {
    "text": "allocation of cpu and devices and helps",
    "start": "617360",
    "end": "619839"
  },
  {
    "text": "to extract the best performance out of",
    "start": "619839",
    "end": "621760"
  },
  {
    "text": "the underlying hardware it aligns",
    "start": "621760",
    "end": "623839"
  },
  {
    "text": "resources of pods of all quality of",
    "start": "623839",
    "end": "626560"
  },
  {
    "text": "service classes and we have the ability",
    "start": "626560",
    "end": "629040"
  },
  {
    "text": "to configure policies and scope for",
    "start": "629040",
    "end": "631040"
  },
  {
    "text": "different resource assignment",
    "start": "631040",
    "end": "632640"
  },
  {
    "text": "requirements",
    "start": "632640",
    "end": "634000"
  },
  {
    "text": "scope can be defined",
    "start": "634000",
    "end": "636160"
  },
  {
    "text": "at a port level or a container level and",
    "start": "636160",
    "end": "638560"
  },
  {
    "text": "in addition to that we have node level",
    "start": "638560",
    "end": "640720"
  },
  {
    "text": "policies which can be configured as non",
    "start": "640720",
    "end": "642959"
  },
  {
    "text": "best effort restricted or single one",
    "start": "642959",
    "end": "645360"
  },
  {
    "text": "would and that depends on how strict you",
    "start": "645360",
    "end": "648880"
  },
  {
    "text": "want that alignment of resources to look",
    "start": "648880",
    "end": "651279"
  },
  {
    "text": "like",
    "start": "651279",
    "end": "652240"
  },
  {
    "text": "in scenarios where topology manager is",
    "start": "652240",
    "end": "654320"
  },
  {
    "text": "unable to align topology of requested",
    "start": "654320",
    "end": "656480"
  },
  {
    "text": "resources based on the configured policy",
    "start": "656480",
    "end": "659440"
  },
  {
    "text": "the pod is rejected with topology fmt",
    "start": "659440",
    "end": "661519"
  },
  {
    "text": "error",
    "start": "661519",
    "end": "663279"
  },
  {
    "text": "with topology manager we were able to",
    "start": "663279",
    "end": "665120"
  },
  {
    "text": "align cpu and devices but guarantees",
    "start": "665120",
    "end": "668000"
  },
  {
    "text": "around memory did not exist for a while",
    "start": "668000",
    "end": "670959"
  },
  {
    "text": "and there was a major gap this was",
    "start": "670959",
    "end": "672959"
  },
  {
    "text": "addressed in kubernetes 121 with the",
    "start": "672959",
    "end": "675680"
  },
  {
    "text": "introduction of memory manager which",
    "start": "675680",
    "end": "677360"
  },
  {
    "text": "became a beta feature in 122.",
    "start": "677360",
    "end": "680480"
  },
  {
    "text": "one of the known issues uh that",
    "start": "680480",
    "end": "683120"
  },
  {
    "text": "topology manager has",
    "start": "683120",
    "end": "684959"
  },
  {
    "text": "is that despite being able to align the",
    "start": "684959",
    "end": "687680"
  },
  {
    "text": "resources",
    "start": "687680",
    "end": "688959"
  },
  {
    "text": "the kubernetes default scheduler lacks",
    "start": "688959",
    "end": "691519"
  },
  {
    "text": "knowledge of node hardware topology",
    "start": "691519",
    "end": "694000"
  },
  {
    "text": "because of which it can schedule a code",
    "start": "694000",
    "end": "695920"
  },
  {
    "text": "where it can fail with topology definite",
    "start": "695920",
    "end": "698800"
  },
  {
    "text": "error",
    "start": "698800",
    "end": "699760"
  },
  {
    "text": "if the pod is part of a deployment or",
    "start": "699760",
    "end": "701760"
  },
  {
    "text": "replica set it results in runaway part",
    "start": "701760",
    "end": "704160"
  },
  {
    "text": "creation because because the subsequent",
    "start": "704160",
    "end": "706320"
  },
  {
    "text": "pods that are created end up with the",
    "start": "706320",
    "end": "708160"
  },
  {
    "text": "same error as well",
    "start": "708160",
    "end": "711120"
  },
  {
    "start": "711000",
    "end": "711000"
  },
  {
    "text": "so topology manager comes into picture",
    "start": "711360",
    "end": "714240"
  },
  {
    "text": "when the pod is being admitted on the",
    "start": "714240",
    "end": "715920"
  },
  {
    "text": "node",
    "start": "715920",
    "end": "716800"
  },
  {
    "text": "it works with other resource managers",
    "start": "716800",
    "end": "719200"
  },
  {
    "text": "which are also called hint providers as",
    "start": "719200",
    "end": "721279"
  },
  {
    "text": "they provide hints",
    "start": "721279",
    "end": "722720"
  },
  {
    "text": "these and providers are used by topology",
    "start": "722720",
    "end": "725440"
  },
  {
    "text": "manager along with the configure policy",
    "start": "725440",
    "end": "727360"
  },
  {
    "text": "to align those resources and reject if",
    "start": "727360",
    "end": "730560"
  },
  {
    "text": "the part cannot be admitted so in this",
    "start": "730560",
    "end": "732720"
  },
  {
    "text": "diagram we have a topology manager doing",
    "start": "732720",
    "end": "735360"
  },
  {
    "text": "its magic admission time where it asks",
    "start": "735360",
    "end": "738320"
  },
  {
    "text": "the hint providers for hints once it",
    "start": "738320",
    "end": "740639"
  },
  {
    "text": "gets those hints it runs in alignment",
    "start": "740639",
    "end": "743120"
  },
  {
    "text": "logic and determines the suitable",
    "start": "743120",
    "end": "744639"
  },
  {
    "text": "lumenoid on which the resources can be",
    "start": "744639",
    "end": "747360"
  },
  {
    "text": "allocated and once that decision has",
    "start": "747360",
    "end": "749360"
  },
  {
    "text": "been made it sends an allocate call to",
    "start": "749360",
    "end": "751519"
  },
  {
    "text": "the hint provider to perform the",
    "start": "751519",
    "end": "753440"
  },
  {
    "text": "resource allocation following which the",
    "start": "753440",
    "end": "755040"
  },
  {
    "text": "containers are added",
    "start": "755040",
    "end": "758759"
  },
  {
    "text": "so we've covered topology where",
    "start": "758800",
    "end": "760560"
  },
  {
    "text": "scheduling and pinning for the pods but",
    "start": "760560",
    "end": "762800"
  },
  {
    "text": "there's still quite a bit of gaps for",
    "start": "762800",
    "end": "764880"
  },
  {
    "text": "high performance compute for",
    "start": "764880",
    "end": "766000"
  },
  {
    "text": "optimization",
    "start": "766000",
    "end": "767360"
  },
  {
    "text": "the cores are only pinned by a container",
    "start": "767360",
    "end": "769200"
  },
  {
    "text": "they cannot be pinned by pod right now",
    "start": "769200",
    "end": "771920"
  },
  {
    "text": "um",
    "start": "771920",
    "end": "773040"
  },
  {
    "text": "this causes issues because if you have a",
    "start": "773040",
    "end": "775360"
  },
  {
    "text": "pod and you want to share course between",
    "start": "775360",
    "end": "776880"
  },
  {
    "text": "your containers you can't",
    "start": "776880",
    "end": "778720"
  },
  {
    "text": "affinity is node level only you cannot",
    "start": "778720",
    "end": "780880"
  },
  {
    "text": "choose affinity of resources so for",
    "start": "780880",
    "end": "782800"
  },
  {
    "text": "instance if you have a stack of xpu's",
    "start": "782800",
    "end": "785839"
  },
  {
    "text": "and you want to use just those",
    "start": "785839",
    "end": "788160"
  },
  {
    "text": "and not",
    "start": "788160",
    "end": "789440"
  },
  {
    "text": "and you're not worried about the cpu",
    "start": "789440",
    "end": "790800"
  },
  {
    "text": "affinity you can't do that either",
    "start": "790800",
    "end": "793279"
  },
  {
    "text": "and you can't mix pins with shared cores",
    "start": "793279",
    "end": "795839"
  },
  {
    "text": "and you can only do full",
    "start": "795839",
    "end": "798000"
  },
  {
    "text": "pinned cores",
    "start": "798000",
    "end": "799279"
  },
  {
    "text": "and if you add shared it",
    "start": "799279",
    "end": "802320"
  },
  {
    "text": "they have to be pinned right now",
    "start": "802320",
    "end": "804480"
  },
  {
    "text": "and limits with larger numbers of pneuma",
    "start": "804480",
    "end": "806959"
  },
  {
    "text": "zones so right now you have an eight",
    "start": "806959",
    "end": "808880"
  },
  {
    "text": "zone limit for a new minute for the",
    "start": "808880",
    "end": "811200"
  },
  {
    "text": "pneuma zones",
    "start": "811200",
    "end": "812639"
  },
  {
    "text": "and you cannot schedule one pod or",
    "start": "812639",
    "end": "814240"
  },
  {
    "text": "container per memo zone so i can't do a",
    "start": "814240",
    "end": "816800"
  },
  {
    "text": "nice pretty split across all of the",
    "start": "816800",
    "end": "819360"
  },
  {
    "text": "zones to make sure that",
    "start": "819360",
    "end": "821279"
  },
  {
    "text": "from a scheduling standpoint every",
    "start": "821279",
    "end": "823600"
  },
  {
    "text": "everything is just one zone per each so",
    "start": "823600",
    "end": "825760"
  },
  {
    "text": "they're not interfering with each other",
    "start": "825760",
    "end": "829839"
  },
  {
    "text": "heterogeneous clusters fun for the whole",
    "start": "830959",
    "end": "833199"
  },
  {
    "text": "family so this is our very generic set",
    "start": "833199",
    "end": "835600"
  },
  {
    "text": "of use cases",
    "start": "835600",
    "end": "838319"
  },
  {
    "text": "but it's real",
    "start": "839120",
    "end": "841040"
  },
  {
    "text": "in many people's data centers they have",
    "start": "841040",
    "end": "843680"
  },
  {
    "text": "different types of nodes because you",
    "start": "843680",
    "end": "845519"
  },
  {
    "text": "upgrade you buy things for particular",
    "start": "845519",
    "end": "847040"
  },
  {
    "text": "purposes etc so in this case we have you",
    "start": "847040",
    "end": "850160"
  },
  {
    "text": "know a multi-cpu node we have another",
    "start": "850160",
    "end": "853360"
  },
  {
    "text": "one with weird cpus which are different",
    "start": "853360",
    "end": "855360"
  },
  {
    "text": "from the from the other cpus in some",
    "start": "855360",
    "end": "857920"
  },
  {
    "text": "cases um",
    "start": "857920",
    "end": "859680"
  },
  {
    "text": "some architecture actually has different",
    "start": "859680",
    "end": "861440"
  },
  {
    "text": "types of cores on the same cpu",
    "start": "861440",
    "end": "864240"
  },
  {
    "text": "you have multiple nics and some single",
    "start": "864240",
    "end": "865920"
  },
  {
    "text": "nicks and another",
    "start": "865920",
    "end": "867600"
  },
  {
    "text": "and this last one you have excel an",
    "start": "867600",
    "end": "869680"
  },
  {
    "text": "accelerator fabric with xpus combined on",
    "start": "869680",
    "end": "873040"
  },
  {
    "text": "top of that",
    "start": "873040",
    "end": "875600"
  },
  {
    "text": "so here's a workload view of how you",
    "start": "876320",
    "end": "878399"
  },
  {
    "text": "might use",
    "start": "878399",
    "end": "879600"
  },
  {
    "text": "a center like this you have a set of",
    "start": "879600",
    "end": "881199"
  },
  {
    "text": "microservices on the first one you have",
    "start": "881199",
    "end": "883440"
  },
  {
    "text": "maybe mixed workloads on that that",
    "start": "883440",
    "end": "885199"
  },
  {
    "text": "middle one depending",
    "start": "885199",
    "end": "886880"
  },
  {
    "text": "on what your cpus are and then you have",
    "start": "886880",
    "end": "889199"
  },
  {
    "text": "ai workloads on that last",
    "start": "889199",
    "end": "892720"
  },
  {
    "text": "so we have",
    "start": "894880",
    "end": "896560"
  },
  {
    "text": "the current gaps for",
    "start": "896560",
    "end": "898560"
  },
  {
    "text": "heterogeneous clusters",
    "start": "898560",
    "end": "901040"
  },
  {
    "text": "include all of the ones that we have for",
    "start": "901040",
    "end": "903839"
  },
  {
    "text": "high performance computes clusters",
    "start": "903839",
    "end": "906160"
  },
  {
    "text": "and",
    "start": "906160",
    "end": "908399"
  },
  {
    "text": "we still have at least two more so you",
    "start": "909680",
    "end": "912320"
  },
  {
    "text": "can't choose what type of cpus so in the",
    "start": "912320",
    "end": "914560"
  },
  {
    "text": "case where you have",
    "start": "914560",
    "end": "916160"
  },
  {
    "text": "that remember that one that said weird",
    "start": "916160",
    "end": "917600"
  },
  {
    "text": "cpus in the middle and the regular cpus",
    "start": "917600",
    "end": "919760"
  },
  {
    "text": "at the beginning you can't have you",
    "start": "919760",
    "end": "922000"
  },
  {
    "text": "can't choose which cpu you want",
    "start": "922000",
    "end": "924720"
  },
  {
    "text": "and you can't ask for more cpus on the",
    "start": "924720",
    "end": "926560"
  },
  {
    "text": "fly",
    "start": "926560",
    "end": "928639"
  },
  {
    "text": "there's actually a proposal being",
    "start": "928639",
    "end": "930160"
  },
  {
    "text": "discussed in the community spanning",
    "start": "930160",
    "end": "931680"
  },
  {
    "text": "across sydnode and six scheduling that",
    "start": "931680",
    "end": "933680"
  },
  {
    "text": "aims are allowing pods",
    "start": "933680",
    "end": "935600"
  },
  {
    "text": "resource requests and limits to be",
    "start": "935600",
    "end": "937199"
  },
  {
    "text": "updated in place without",
    "start": "937199",
    "end": "939440"
  },
  {
    "text": "without the need to start the pod or its",
    "start": "939440",
    "end": "941440"
  },
  {
    "text": "containers",
    "start": "941440",
    "end": "942639"
  },
  {
    "text": "and this feature is being targeted to be",
    "start": "942639",
    "end": "944399"
  },
  {
    "text": "merged in kubernetes 125 and once we",
    "start": "944399",
    "end": "947199"
  },
  {
    "text": "have that particular feature merge the",
    "start": "947199",
    "end": "949120"
  },
  {
    "text": "gap should be addressed",
    "start": "949120",
    "end": "951920"
  },
  {
    "text": "so yeah we have a long way to go and we",
    "start": "951920",
    "end": "954560"
  },
  {
    "text": "welcome feedback and healthy all the",
    "start": "954560",
    "end": "956959"
  },
  {
    "text": "help we can get from the community",
    "start": "956959",
    "end": "960560"
  },
  {
    "text": "so we have we now have a bunch of",
    "start": "961040",
    "end": "963839"
  },
  {
    "text": "requirements and gaps when it comes to",
    "start": "963839",
    "end": "966399"
  },
  {
    "text": "native resource management capabilities",
    "start": "966399",
    "end": "968959"
  },
  {
    "text": "is is there a middle ground marlo maybe",
    "start": "968959",
    "end": "972160"
  },
  {
    "text": "do we have options out there that can be",
    "start": "972160",
    "end": "974240"
  },
  {
    "text": "used in the interim",
    "start": "974240",
    "end": "977440"
  },
  {
    "start": "976000",
    "end": "976000"
  },
  {
    "text": "so currently there are three different",
    "start": "977680",
    "end": "980079"
  },
  {
    "text": "uh options that i know of well",
    "start": "980079",
    "end": "983440"
  },
  {
    "text": "three and then some right",
    "start": "983440",
    "end": "985600"
  },
  {
    "text": "so there's crirm which is a cri resource",
    "start": "985600",
    "end": "988399"
  },
  {
    "text": "manager this is put out by intel um",
    "start": "988399",
    "end": "991839"
  },
  {
    "text": "but it is open source and they do take",
    "start": "991839",
    "end": "993600"
  },
  {
    "text": "contributions",
    "start": "993600",
    "end": "995199"
  },
  {
    "text": "which is a plugable add-on for",
    "start": "995199",
    "end": "996560"
  },
  {
    "text": "controlling resource assignments to",
    "start": "996560",
    "end": "998000"
  },
  {
    "text": "containers it plugs in between the",
    "start": "998000",
    "end": "999839"
  },
  {
    "text": "kubelet and the container runtime keeps",
    "start": "999839",
    "end": "1002079"
  },
  {
    "text": "track of the states of all the",
    "start": "1002079",
    "end": "1003360"
  },
  {
    "text": "containers on a node and it intercepts",
    "start": "1003360",
    "end": "1005199"
  },
  {
    "text": "the cri protocol request from the",
    "start": "1005199",
    "end": "1006639"
  },
  {
    "text": "kubelet",
    "start": "1006639",
    "end": "1007839"
  },
  {
    "text": "however do note that all of these",
    "start": "1007839",
    "end": "1009839"
  },
  {
    "text": "solutions require turning off all",
    "start": "1009839",
    "end": "1011680"
  },
  {
    "text": "resource management by the kubelet so",
    "start": "1011680",
    "end": "1013279"
  },
  {
    "text": "they don't fight",
    "start": "1013279",
    "end": "1015920"
  },
  {
    "text": "there's a cpu pooler this is a project",
    "start": "1017199",
    "end": "1019360"
  },
  {
    "text": "put out by nokia and this is",
    "start": "1019360",
    "end": "1022480"
  },
  {
    "text": "a solution for kubernetes to manage",
    "start": "1022480",
    "end": "1024079"
  },
  {
    "text": "predefined distinct pools of kubernetes",
    "start": "1024079",
    "end": "1026319"
  },
  {
    "text": "knowns it's phys it physically separates",
    "start": "1026319",
    "end": "1028880"
  },
  {
    "text": "the cpu resources of the containers",
    "start": "1028880",
    "end": "1030558"
  },
  {
    "text": "connecting to the various pools",
    "start": "1030559",
    "end": "1032640"
  },
  {
    "text": "and it's a device it has a device",
    "start": "1032640",
    "end": "1034240"
  },
  {
    "text": "plug-in that exposes the cpu cores as",
    "start": "1034240",
    "end": "1036079"
  },
  {
    "text": "consumable devices",
    "start": "1036079",
    "end": "1039760"
  },
  {
    "text": "and then there's also cmk it was used in",
    "start": "1039760",
    "end": "1042558"
  },
  {
    "text": "the past it is currently deprecated",
    "start": "1042559",
    "end": "1044240"
  },
  {
    "text": "partially because",
    "start": "1044240",
    "end": "1046000"
  },
  {
    "text": "isil cpus is getting are getting",
    "start": "1046000",
    "end": "1047600"
  },
  {
    "text": "deprecated in the kernel and this",
    "start": "1047600",
    "end": "1049520"
  },
  {
    "text": "accomplished core isolation by",
    "start": "1049520",
    "end": "1050960"
  },
  {
    "text": "controlling the logical cpus each",
    "start": "1050960",
    "end": "1052480"
  },
  {
    "text": "container may use for execution",
    "start": "1052480",
    "end": "1055280"
  },
  {
    "text": "and it wrapped it in a cmk command line",
    "start": "1055280",
    "end": "1058400"
  },
  {
    "text": "program for managing cpu pools and",
    "start": "1058400",
    "end": "1060160"
  },
  {
    "text": "concerning those workloads",
    "start": "1060160",
    "end": "1063360"
  },
  {
    "text": "so like the projects that mona just",
    "start": "1063919",
    "end": "1066559"
  },
  {
    "text": "mentioned",
    "start": "1066559",
    "end": "1068640"
  },
  {
    "text": "to some extent uh you you take resource",
    "start": "1068640",
    "end": "1071440"
  },
  {
    "text": "management capabilities in your own hand",
    "start": "1071440",
    "end": "1074080"
  },
  {
    "text": "you could go essentially all the way in",
    "start": "1074080",
    "end": "1076480"
  },
  {
    "text": "and come up with a completely customized",
    "start": "1076480",
    "end": "1078400"
  },
  {
    "text": "solution that perfectly suits your needs",
    "start": "1078400",
    "end": "1081039"
  },
  {
    "text": "and make it do whatever you would like",
    "start": "1081039",
    "end": "1083200"
  },
  {
    "text": "it to do",
    "start": "1083200",
    "end": "1084720"
  },
  {
    "text": "but this comes with a massive disclaimer",
    "start": "1084720",
    "end": "1087039"
  },
  {
    "text": "do it at your own risk",
    "start": "1087039",
    "end": "1089280"
  },
  {
    "text": "if you are someone who is already doing",
    "start": "1089280",
    "end": "1091120"
  },
  {
    "text": "something like this as part of the",
    "start": "1091120",
    "end": "1092960"
  },
  {
    "text": "community we would love to hear what",
    "start": "1092960",
    "end": "1094640"
  },
  {
    "text": "prompted you to take that path so that",
    "start": "1094640",
    "end": "1096880"
  },
  {
    "text": "it can be taken into consideration in",
    "start": "1096880",
    "end": "1098799"
  },
  {
    "text": "the native solution",
    "start": "1098799",
    "end": "1101840"
  },
  {
    "text": "oh yeah",
    "start": "1103120",
    "end": "1105520"
  },
  {
    "text": "so how can you get involved uh if you're",
    "start": "1106640",
    "end": "1109360"
  },
  {
    "text": "new to all this and are feeling a bit",
    "start": "1109360",
    "end": "1111520"
  },
  {
    "text": "overwhelmed and are not sure where to",
    "start": "1111520",
    "end": "1114000"
  },
  {
    "text": "get started or how to get involved we",
    "start": "1114000",
    "end": "1116320"
  },
  {
    "text": "have you covered",
    "start": "1116320",
    "end": "1119039"
  },
  {
    "start": "1118000",
    "end": "1118000"
  },
  {
    "text": "so there's a lot of community discussion",
    "start": "1119440",
    "end": "1122080"
  },
  {
    "text": "on how to address all the gaps because",
    "start": "1122080",
    "end": "1124240"
  },
  {
    "text": "there's a lot of gaps right now",
    "start": "1124240",
    "end": "1126480"
  },
  {
    "text": "we managed to figure out what those gaps",
    "start": "1126480",
    "end": "1128640"
  },
  {
    "text": "are with the cpu management kubelet use",
    "start": "1128640",
    "end": "1130559"
  },
  {
    "text": "case doc and what is unaddressed and",
    "start": "1130559",
    "end": "1133039"
  },
  {
    "text": "you're welcome to go there add",
    "start": "1133039",
    "end": "1135120"
  },
  {
    "text": "add to it you know comment engage with",
    "start": "1135120",
    "end": "1137360"
  },
  {
    "text": "the authors",
    "start": "1137360",
    "end": "1139200"
  },
  {
    "text": "we began this last december it it really",
    "start": "1139200",
    "end": "1141679"
  },
  {
    "text": "did expose a lot of the issues that we",
    "start": "1141679",
    "end": "1144000"
  },
  {
    "text": "have currently",
    "start": "1144000",
    "end": "1145520"
  },
  {
    "text": "there's also a kubelet resource plugin",
    "start": "1145520",
    "end": "1147360"
  },
  {
    "text": "rfc",
    "start": "1147360",
    "end": "1148400"
  },
  {
    "text": "this is a suggestion to move the kubelet",
    "start": "1148400",
    "end": "1150080"
  },
  {
    "text": "resource model splitting the kubelet",
    "start": "1150080",
    "end": "1151840"
  },
  {
    "text": "into the control plane which is what you",
    "start": "1151840",
    "end": "1154400"
  },
  {
    "text": "do on the node right is all the resource",
    "start": "1154400",
    "end": "1156240"
  },
  {
    "text": "management on the node",
    "start": "1156240",
    "end": "1157760"
  },
  {
    "text": "and a data plane which advertises to the",
    "start": "1157760",
    "end": "1160320"
  },
  {
    "text": "scheduler because",
    "start": "1160320",
    "end": "1161840"
  },
  {
    "text": "we haven't really discussed this",
    "start": "1161840",
    "end": "1163679"
  },
  {
    "text": "extensively here but only half of it is",
    "start": "1163679",
    "end": "1166559"
  },
  {
    "text": "after it gets to the node the first part",
    "start": "1166559",
    "end": "1168240"
  },
  {
    "text": "is where do you schedule your workload",
    "start": "1168240",
    "end": "1170320"
  },
  {
    "text": "which nodes",
    "start": "1170320",
    "end": "1171840"
  },
  {
    "text": "and so you're welcome to go to the stock",
    "start": "1171840",
    "end": "1173840"
  },
  {
    "text": "and add in commentary",
    "start": "1173840",
    "end": "1176720"
  },
  {
    "text": "we seem to have quite a bit of community",
    "start": "1176720",
    "end": "1178240"
  },
  {
    "text": "support so i'm pretty excited about that",
    "start": "1178240",
    "end": "1180720"
  },
  {
    "text": "there's also nri which is a node",
    "start": "1180720",
    "end": "1182559"
  },
  {
    "text": "resource interface which is a cni type",
    "start": "1182559",
    "end": "1184480"
  },
  {
    "text": "interface for managing resources unknown",
    "start": "1184480",
    "end": "1186080"
  },
  {
    "text": "for positive containers",
    "start": "1186080",
    "end": "1187760"
  },
  {
    "text": "this is coming out of tag runtimes and",
    "start": "1187760",
    "end": "1190240"
  },
  {
    "text": "one thing that's pretty exciting about",
    "start": "1190240",
    "end": "1192720"
  },
  {
    "text": "this is this would work well with the",
    "start": "1192720",
    "end": "1194160"
  },
  {
    "text": "kubelet resource plugin rfc because",
    "start": "1194160",
    "end": "1196240"
  },
  {
    "text": "currently they're still trying to find",
    "start": "1196240",
    "end": "1197520"
  },
  {
    "text": "ways to get all the resources they need",
    "start": "1197520",
    "end": "1199760"
  },
  {
    "text": "into nri",
    "start": "1199760",
    "end": "1201919"
  },
  {
    "text": "and then there's dynamic resource",
    "start": "1201919",
    "end": "1203360"
  },
  {
    "text": "management which is an alternative to",
    "start": "1203360",
    "end": "1204960"
  },
  {
    "text": "the device plug-in api",
    "start": "1204960",
    "end": "1206799"
  },
  {
    "text": "and the primary idea is that the",
    "start": "1206799",
    "end": "1208159"
  },
  {
    "text": "resource allocations can be ephemeral or",
    "start": "1208159",
    "end": "1210080"
  },
  {
    "text": "persistent and it allows users to",
    "start": "1210080",
    "end": "1212400"
  },
  {
    "text": "specify resources with specific",
    "start": "1212400",
    "end": "1214960"
  },
  {
    "text": "parameters",
    "start": "1214960",
    "end": "1217600"
  },
  {
    "text": "so kubernetes has",
    "start": "1218159",
    "end": "1220400"
  },
  {
    "text": "various operational areas that are",
    "start": "1220400",
    "end": "1222400"
  },
  {
    "text": "organized as sigs and working groups we",
    "start": "1222400",
    "end": "1224960"
  },
  {
    "text": "would highly recommend that you keep an",
    "start": "1224960",
    "end": "1226720"
  },
  {
    "text": "eye on sig node and six scheduling",
    "start": "1226720",
    "end": "1229520"
  },
  {
    "text": "cygnode is responsible for life cycle of",
    "start": "1229520",
    "end": "1231840"
  },
  {
    "text": "pods that are scheduled to node and its",
    "start": "1231840",
    "end": "1234159"
  },
  {
    "text": "scope includes cubelet pod and node api",
    "start": "1234159",
    "end": "1237120"
  },
  {
    "text": "container runtimes various resource",
    "start": "1237120",
    "end": "1239440"
  },
  {
    "text": "managers and hardware discovery",
    "start": "1239440",
    "end": "1241919"
  },
  {
    "text": "this is in no way a comprehensive list",
    "start": "1241919",
    "end": "1244080"
  },
  {
    "text": "of what signal does but it should give",
    "start": "1244080",
    "end": "1246400"
  },
  {
    "text": "you a good idea of what to expect",
    "start": "1246400",
    "end": "1249200"
  },
  {
    "text": "six scheduling on the other hand is",
    "start": "1249200",
    "end": "1250799"
  },
  {
    "text": "responsible for components that make",
    "start": "1250799",
    "end": "1253200"
  },
  {
    "text": "point placement decisions so your",
    "start": "1253200",
    "end": "1255360"
  },
  {
    "text": "kubernetes scheduler and scheduling",
    "start": "1255360",
    "end": "1257520"
  },
  {
    "text": "features frameworks such as security",
    "start": "1257520",
    "end": "1259760"
  },
  {
    "text": "scheduler framework that allows you to",
    "start": "1259760",
    "end": "1262000"
  },
  {
    "text": "create custom plugins",
    "start": "1262000",
    "end": "1264080"
  },
  {
    "text": "all fall in the realm of six scheduling",
    "start": "1264080",
    "end": "1268080"
  },
  {
    "start": "1268000",
    "end": "1268000"
  },
  {
    "text": "working groups are groups that are",
    "start": "1268400",
    "end": "1270159"
  },
  {
    "text": "formed to solve specific problems and",
    "start": "1270159",
    "end": "1272559"
  },
  {
    "text": "that typically span across multiple",
    "start": "1272559",
    "end": "1274559"
  },
  {
    "text": "things",
    "start": "1274559",
    "end": "1275440"
  },
  {
    "text": "so topology we're scheduling is a",
    "start": "1275440",
    "end": "1277280"
  },
  {
    "text": "project that spans across signal and six",
    "start": "1277280",
    "end": "1279760"
  },
  {
    "text": "scheduling it is not an official",
    "start": "1279760",
    "end": "1281840"
  },
  {
    "text": "kubernetes working group but a bunch of",
    "start": "1281840",
    "end": "1283679"
  },
  {
    "text": "people",
    "start": "1283679",
    "end": "1284559"
  },
  {
    "text": "uh that have come together from",
    "start": "1284559",
    "end": "1285840"
  },
  {
    "text": "different organizations trying to solve",
    "start": "1285840",
    "end": "1288320"
  },
  {
    "text": "the problem of scheduler being numa",
    "start": "1288320",
    "end": "1290159"
  },
  {
    "text": "unaware",
    "start": "1290159",
    "end": "1291200"
  },
  {
    "text": "the github organization linked here will",
    "start": "1291200",
    "end": "1293120"
  },
  {
    "text": "provide you all the documentation",
    "start": "1293120",
    "end": "1294799"
  },
  {
    "text": "artifacts and components you need to get",
    "start": "1294799",
    "end": "1297760"
  },
  {
    "text": "get started on this",
    "start": "1297760",
    "end": "1300000"
  },
  {
    "text": "the next one is cnf tag runtime and the",
    "start": "1300000",
    "end": "1302880"
  },
  {
    "text": "goal of this group is to enable",
    "start": "1302880",
    "end": "1304960"
  },
  {
    "text": "widespread and successful execution of",
    "start": "1304960",
    "end": "1308080"
  },
  {
    "text": "wide range of workloads such as latency",
    "start": "1308080",
    "end": "1310400"
  },
  {
    "text": "sensitive workloads batch workloads",
    "start": "1310400",
    "end": "1313280"
  },
  {
    "text": "within the tag runtime itself we have a",
    "start": "1313280",
    "end": "1316240"
  },
  {
    "text": "container orchestrated device working",
    "start": "1316240",
    "end": "1318720"
  },
  {
    "text": "group",
    "start": "1318720",
    "end": "1319520"
  },
  {
    "text": "and this group is focused on container",
    "start": "1319520",
    "end": "1323760"
  },
  {
    "text": "runtime related problems and device",
    "start": "1323760",
    "end": "1326640"
  },
  {
    "text": "vendors and container runtime",
    "start": "1326640",
    "end": "1327919"
  },
  {
    "text": "maintainers are coming together to solve",
    "start": "1327919",
    "end": "1330240"
  },
  {
    "text": "the",
    "start": "1330240",
    "end": "1331039"
  },
  {
    "text": "support for devices in cloud native",
    "start": "1331039",
    "end": "1332880"
  },
  {
    "text": "space",
    "start": "1332880",
    "end": "1334080"
  },
  {
    "text": "and lastly we have batch working group",
    "start": "1334080",
    "end": "1336400"
  },
  {
    "text": "this is a very recently formed working",
    "start": "1336400",
    "end": "1338159"
  },
  {
    "text": "group and they're focusing on enhancing",
    "start": "1338159",
    "end": "1342000"
  },
  {
    "text": "support for batch workloads such as hpc",
    "start": "1342000",
    "end": "1345520"
  },
  {
    "text": "aiml natively in kubernetes",
    "start": "1345520",
    "end": "1348400"
  },
  {
    "text": "all these things and working groups that",
    "start": "1348400",
    "end": "1350559"
  },
  {
    "text": "i mentioned usually have regular",
    "start": "1350559",
    "end": "1352000"
  },
  {
    "text": "meetings and we have linked the github",
    "start": "1352000",
    "end": "1354320"
  },
  {
    "text": "page and slack channels for you to get",
    "start": "1354320",
    "end": "1356799"
  },
  {
    "text": "all the information that you need",
    "start": "1356799",
    "end": "1360158"
  },
  {
    "text": "this essentially concludes our",
    "start": "1360400",
    "end": "1361679"
  },
  {
    "text": "presentation um",
    "start": "1361679",
    "end": "1363520"
  },
  {
    "text": "thank you very much for attending we are",
    "start": "1363520",
    "end": "1365039"
  },
  {
    "text": "happy to answer any questions after the",
    "start": "1365039",
    "end": "1367120"
  },
  {
    "text": "talk",
    "start": "1367120",
    "end": "1367919"
  },
  {
    "text": "or if you'd like to reach out to us on",
    "start": "1367919",
    "end": "1370240"
  },
  {
    "text": "slack or email we'd be happy there as",
    "start": "1370240",
    "end": "1372880"
  },
  {
    "text": "well",
    "start": "1372880",
    "end": "1373679"
  },
  {
    "text": "thank you very much",
    "start": "1373679",
    "end": "1375200"
  },
  {
    "text": "thank you",
    "start": "1375200",
    "end": "1378440"
  }
]