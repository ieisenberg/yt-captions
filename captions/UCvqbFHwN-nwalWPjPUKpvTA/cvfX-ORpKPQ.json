[
  {
    "text": "hello everyone thanks for joining um I am prabakar paril I'm a Consulting engineer uh in arle uh Cloud",
    "start": "80",
    "end": "9399"
  },
  {
    "text": "infrastructure so uh what is in it for you right like uh why you have to spend next 30 minutes here so um I would touch",
    "start": "10000",
    "end": "18279"
  },
  {
    "text": "upon few internal aspects of it City and um we would share some of the learnings",
    "start": "18279",
    "end": "23800"
  },
  {
    "text": "we had over the last 78 years operating uh tens of thousands of it CD clusters",
    "start": "23800",
    "end": "29800"
  },
  {
    "text": "and and um as we evolved we re architectured our um HD architecture so",
    "start": "29800",
    "end": "35440"
  },
  {
    "text": "we will talk about how we went about re architecting and also migrating the it",
    "start": "35440",
    "end": "41000"
  },
  {
    "text": "City deployment from the Legacy architecture to the new architecture and also we'll talk about",
    "start": "41000",
    "end": "47000"
  },
  {
    "text": "some of the learnings when we did this uh migration from the Legacy to new",
    "start": "47000",
    "end": "53600"
  },
  {
    "text": "architecture just a very brief uh uh introduction so uh like I said I work",
    "start": "53840",
    "end": "59519"
  },
  {
    "text": "with OC I and oi has a global infrastructure across uh the globe with multiple regions and in each of these",
    "start": "59519",
    "end": "67159"
  },
  {
    "text": "regions uh ok which is our managed kubernetes offering is a day one service",
    "start": "67159",
    "end": "73880"
  },
  {
    "text": "and there are um multiple uh internal as well as external applications are",
    "start": "73880",
    "end": "79000"
  },
  {
    "text": "onboarded to OK and in fact many of the OCA Services run on top of ok itself so",
    "start": "79000",
    "end": "86000"
  },
  {
    "text": "uh with the growth in the adaption we were looking at oper efficiencies and one uh thing we",
    "start": "86000",
    "end": "92840"
  },
  {
    "text": "identified was to improve the way the H CD was deployed and operated like you know each OK cluster has a backend it",
    "start": "92840",
    "end": "100240"
  },
  {
    "text": "City and hence we need to find a way to operate it better to operate kubernetes",
    "start": "100240",
    "end": "106119"
  },
  {
    "text": "better so we went through this journey of re architecting uh and uh at this",
    "start": "106119",
    "end": "111960"
  },
  {
    "text": "moment we completed the migration to the new architecture with um zero customer",
    "start": "111960",
    "end": "117479"
  },
  {
    "text": "uh reported issues I think um having a um great and uh engineering team really",
    "start": "117479",
    "end": "123799"
  },
  {
    "text": "helped but the Equal Credit goes to the it City Sig because of the high bar they maintain with respect to the patch",
    "start": "123799",
    "end": "130879"
  },
  {
    "text": "releases and major releases they make because we had uh it City clusters running all the way from it 3.3 to now",
    "start": "130879",
    "end": "138800"
  },
  {
    "text": "3.5x but we could do all those migration with zero customer escalation so I just want to thank the uh it for their focus",
    "start": "138800",
    "end": "146800"
  },
  {
    "text": "on the high bar they maintain so start to start with some brief int uh",
    "start": "146800",
    "end": "154519"
  },
  {
    "text": "HD internals as you all know uh distribut H CD is a distributed key valy store uh I do hear that some folks run",
    "start": "154519",
    "end": "162800"
  },
  {
    "text": "single member it City in production I don't know I don't believe that but I do hear it from some of my friends but it",
    "start": "162800",
    "end": "168360"
  },
  {
    "text": "doesn't make sense running a single uh member it CD uh in production yeah it has to be a distributed key value pair",
    "start": "168360",
    "end": "175480"
  },
  {
    "text": "and um it uses grpc for both its client as as well as uh peer to-peer",
    "start": "175480",
    "end": "181840"
  },
  {
    "text": "communication um and you would know it it internally uses http2 so in case you are interested um so uh raft is the",
    "start": "181840",
    "end": "191280"
  },
  {
    "text": "heart and soul of itd so many features you think is feature of itd is actually inherited from raft so like I said uh it",
    "start": "191280",
    "end": "198400"
  },
  {
    "text": "CID is a distributed key value store so you need to have a consensus between the itd members so the consensus is through",
    "start": "198400",
    "end": "205840"
  },
  {
    "text": "raft so raft defines for instance how the replication of updates happen uh it",
    "start": "205840",
    "end": "212319"
  },
  {
    "text": "defines that it has to be a leader based replication basically it defines how um the leader election is done and such",
    "start": "212319",
    "end": "220200"
  },
  {
    "text": "things so how raft works is it treats the",
    "start": "220200",
    "end": "226480"
  },
  {
    "text": "system that it manages as a replicated State machine so and it defines how the updates to that state machine is done so",
    "start": "226480",
    "end": "233959"
  },
  {
    "text": "uh internally it say has a concept of replicated logs as well as data State",
    "start": "233959",
    "end": "239360"
  },
  {
    "text": "basically so and it controls the update first the update goes to the replicated locks and then it uh that is what is called as",
    "start": "239360",
    "end": "245799"
  },
  {
    "text": "committing the update and then this update goes to the back end store which is called applying the update so those",
    "start": "245799",
    "end": "252280"
  },
  {
    "text": "are um raft terms inherited into it City So within it City we use uh B bolt for",
    "start": "252280",
    "end": "259479"
  },
  {
    "text": "storing the state so that's why you would be surprised you have uh new might be surprised that we have a key key",
    "start": "259479",
    "end": "265800"
  },
  {
    "text": "Valley store inside another key Valley store it's because it city is like a distributed ke valy store under under",
    "start": "265800",
    "end": "271000"
  },
  {
    "text": "that each of the members have a standalone key valy store in our case it's B",
    "start": "271000",
    "end": "278160"
  },
  {
    "text": "bolt so now I would uh jump into how uh we did I mean I'll start with how my",
    "start": "281000",
    "end": "287919"
  },
  {
    "text": "legacy architecture looked and then how our um new architecture looked and then in the subsequent slides I will dive",
    "start": "287919",
    "end": "294080"
  },
  {
    "text": "deep into the migration process and our learnings and other things so what do you see in the screen screen right now",
    "start": "294080",
    "end": "300520"
  },
  {
    "text": "is the Legacy architecture the vertical uh rectangular boxes you see in the left",
    "start": "300520",
    "end": "305800"
  },
  {
    "text": "they are all the control planes of our kubernetes clusters so each control plane has three compute instance and in",
    "start": "305800",
    "end": "313880"
  },
  {
    "text": "the our Legacy architecture we only run the kubernetes uh containers there and what you see in the right side is our",
    "start": "313880",
    "end": "321080"
  },
  {
    "text": "internally managed kubernetes cluster where we run itd as itd Parts you can",
    "start": "321080",
    "end": "326240"
  },
  {
    "text": "think of it as a variant of a QB setion model where uh we it CD Parts on an",
    "start": "326240",
    "end": "332280"
  },
  {
    "text": "internal uh cluster for the customers kubernetes clusters and uh we had an",
    "start": "332280",
    "end": "338280"
  },
  {
    "text": "internal Gateway which was um uh Envoy based and since we use TLS everywhere of",
    "start": "338280",
    "end": "346360"
  },
  {
    "text": "for communication between the QB API server and the H CD uh we uh we used the",
    "start": "346360",
    "end": "352360"
  },
  {
    "text": "Sni in mechanism in um supported in an to Route the traffic to the right it C",
    "start": "352360",
    "end": "358520"
  },
  {
    "text": "store like if if one of these kubernetes clusters want to uh talk to their corresponding it CD data store they",
    "start": "358520",
    "end": "365160"
  },
  {
    "text": "would talk to the Gateway and the Gateway know knows which it CD to forward to using the sna",
    "start": "365160",
    "end": "372639"
  },
  {
    "text": "mechanism and in our uh internal cluster we ran the coros based itd operator for",
    "start": "372639",
    "end": "379120"
  },
  {
    "text": "maintaining the life cycle of the itd parts as well as for the backup and restore",
    "start": "379120",
    "end": "385720"
  },
  {
    "text": "mechanism so as part of the migration what we did was we decided to move the",
    "start": "385720",
    "end": "391120"
  },
  {
    "text": "it CD members into the control planes which constituted the I mean the compute",
    "start": "391120",
    "end": "397039"
  },
  {
    "text": "instances which constituted the control planes so like I said we had three computer instances so we moved three",
    "start": "397039",
    "end": "403240"
  },
  {
    "text": "copies of the CD uh into those computer instance so each control plane uh talks",
    "start": "403240",
    "end": "408759"
  },
  {
    "text": "to the corresponding local it CD in our case the QP API server which is running in that comput instance only talks to",
    "start": "408759",
    "end": "414960"
  },
  {
    "text": "the corresponding it CD so we do that for all the members and",
    "start": "414960",
    "end": "420120"
  },
  {
    "text": "once all the members are migrated we dismantle the corresponding uh Legacy architecture in the given",
    "start": "420120",
    "end": "427400"
  },
  {
    "text": "region so um now I would talk about uh the improvements that we made so we were",
    "start": "431319",
    "end": "437520"
  },
  {
    "text": "operating this Legacy architecture for 78 years and then we decided to migrate to this new architecture so we obviously",
    "start": "437520",
    "end": "444879"
  },
  {
    "text": "had some learnings along the way I listed few of those here for uh uh the",
    "start": "444879",
    "end": "450240"
  },
  {
    "text": "space constraints so there are much more I could add so the first and foremost right uh we continuously patch our nodes",
    "start": "450240",
    "end": "457120"
  },
  {
    "text": "and uh for uh security updates and things like that so we have to make that",
    "start": "457120",
    "end": "462199"
  },
  {
    "text": "error error error uh less error prone and simplified so what we did was every",
    "start": "462199",
    "end": "468520"
  },
  {
    "text": "uh we assigned a permanent identity to the itd as well as a permanent uh",
    "start": "468520",
    "end": "474159"
  },
  {
    "text": "storage this is a fancy way of saying we created DNS entries and block volumes",
    "start": "474159",
    "end": "479800"
  },
  {
    "text": "and assign to the it City members every time we go and create a kubernetes cluster we provision three DNS names as",
    "start": "479800",
    "end": "487000"
  },
  {
    "text": "well as three block volumes and we bring up three compute instance attach the block volume and assign the",
    "start": "487000",
    "end": "492879"
  },
  {
    "text": "corresponding DNS entry next month when I'm patching those uh control plane computer instance all",
    "start": "492879",
    "end": "499639"
  },
  {
    "text": "that I need to do is to terminate this computer instance bring up a new one and attach the block volume and DNS entry",
    "start": "499639",
    "end": "505199"
  },
  {
    "text": "there from the perspective of the other it CD members it's just a BL the member disappeared for a few seconds",
    "start": "505199",
    "end": "511560"
  },
  {
    "text": "and it came back because I have nothing has changed because the identity and the persistent storage is exactly the same",
    "start": "511560",
    "end": "517880"
  },
  {
    "text": "so this significantly simplified our operational uh process and other thing that we learned",
    "start": "517880",
    "end": "525399"
  },
  {
    "text": "from the day one is uh one size fitall doesn't work because we have clusters with one worker nodes and some with",
    "start": "525399",
    "end": "532360"
  },
  {
    "text": "thousands of worker nodes so we cannot expect the operators to jump in and tune the computer sources or memory iops for",
    "start": "532360",
    "end": "540000"
  },
  {
    "text": "the block volume so we built in the autot tuning from the day one so uh we continuously monitor the cluster",
    "start": "540000",
    "end": "547880"
  },
  {
    "text": "characteristics and if we see that okay this uh cluster is much larger than I thought so I we would automatically",
    "start": "547880",
    "end": "553839"
  },
  {
    "text": "scale up the uh compute instances memory and as well as the iops assigned to the",
    "start": "553839",
    "end": "559000"
  },
  {
    "text": "block volumes so 2GB is the default uh quota",
    "start": "559000",
    "end": "565160"
  },
  {
    "text": "set for HD so I spoke about the B bolt so the B bolt uh has it internal backend",
    "start": "565160",
    "end": "570880"
  },
  {
    "text": "storage so this Kota setting which is by default 2GB limits the size of that back",
    "start": "570880",
    "end": "576320"
  },
  {
    "text": "end store so uh initially in the our Legacy architecture 2GB was sufficient",
    "start": "576320",
    "end": "582040"
  },
  {
    "text": "but as we evolved and grew uh we consistently hit that limit and The Operators get alarmed they jump in they",
    "start": "582040",
    "end": "588519"
  },
  {
    "text": "do a defrag and then uh possibly increase the quota it was uh it was unnecessary so what we thought from the",
    "start": "588519",
    "end": "594959"
  },
  {
    "text": "day one we would set the Kota as 8GB in fact we are talking to the uh sick team",
    "start": "594959",
    "end": "600480"
  },
  {
    "text": "the C SI team to even consider increasing it to 16 GB maybe 8GB is too old for today uh uh with the size the",
    "start": "600480",
    "end": "607279"
  },
  {
    "text": "size of the Clusters maybe a 16 GB or 20 GB makes sense hopefully the CD s team",
    "start": "607279",
    "end": "613120"
  },
  {
    "text": "might uh prioritize that another important thing with",
    "start": "613120",
    "end": "618320"
  },
  {
    "text": "operating it CD is the io latency with respect to discs so we uh now we have",
    "start": "618320",
    "end": "624920"
  },
  {
    "text": "built in the monitoring of it City and we continuously monitor the backend Laten see so we have alarms configured",
    "start": "624920",
    "end": "631560"
  },
  {
    "text": "against the block volumes provision and based on the alarms The Operators jump in and then scale up the iops assign to",
    "start": "631560",
    "end": "638760"
  },
  {
    "text": "the block volumes and lastly uh defrag is one of the um uh critical operation which as",
    "start": "638760",
    "end": "646959"
  },
  {
    "text": "you know can momentarily pause it CD member so um we have more intelligent",
    "start": "646959",
    "end": "653200"
  },
  {
    "text": "base of defragging now uh in our Legacy architecture what we used to do is to we wait for the CA to be hit and the member",
    "start": "653200",
    "end": "661399"
  },
  {
    "text": "uh and the alarm to be raised and then the operators would jump in and do a defrag now we incorporated the automatic",
    "start": "661399",
    "end": "668720"
  },
  {
    "text": "defrag in our code and we do it in a much more intelligent manner because uh running uh this in an uh unplanned",
    "start": "668720",
    "end": "675760"
  },
  {
    "text": "manner may bring down the um it CD cluster because it's a like they say",
    "start": "675760",
    "end": "681079"
  },
  {
    "text": "it's a stop of the World operation during the defrag time that member is completely unavailable if you are doing",
    "start": "681079",
    "end": "686800"
  },
  {
    "text": "a defrag for the leader then it is quite possible that the leader doesn't send the right heartbeats to the other",
    "start": "686800",
    "end": "692000"
  },
  {
    "text": "members and you end up losing the leader I mean you triggering the leader election and all those things so we have",
    "start": "692000",
    "end": "698360"
  },
  {
    "text": "to tune the way we do the defrag so that we have uh Incorporated and now we based",
    "start": "698360",
    "end": "703560"
  },
  {
    "text": "on our discussion with SE City we are tuning it even",
    "start": "703560",
    "end": "708879"
  },
  {
    "text": "further so I will take couple of slides about our migration process uh uh one",
    "start": "709680",
    "end": "715800"
  },
  {
    "text": "thing is obviously it has to be a zero uh data loss uh uh migration no questions about that and it has to be a",
    "start": "715800",
    "end": "722519"
  },
  {
    "text": "zero downtime migration so the kubernetes control plane has to be available throughout this migration our",
    "start": "722519",
    "end": "728480"
  },
  {
    "text": "intent is the customer is not even aware we are doing the migration the only thing that we do which the C which is",
    "start": "728480",
    "end": "736040"
  },
  {
    "text": "observable to the customer is the customer cannot delete the cluster during the migration so we typically",
    "start": "736040",
    "end": "741320"
  },
  {
    "text": "take about 10 minutes per cluster and of course we do migration in a concurrent manner but during that migration you",
    "start": "741320",
    "end": "747199"
  },
  {
    "text": "cannot delete the cluster but we thought that it's not a major limitation it provides additional safety for us so we",
    "start": "747199",
    "end": "753000"
  },
  {
    "text": "prevent a deletion from happening but otherwise the customer can do anything he wants uh and he wouldn't even be",
    "start": "753000",
    "end": "758279"
  },
  {
    "text": "aware that migration is happening in the background background and uh like I said uh",
    "start": "758279",
    "end": "763600"
  },
  {
    "text": "considering the scale of the deployment it has to be fully uh automated operators cannot jump in and do a",
    "start": "763600",
    "end": "769440"
  },
  {
    "text": "cluster I mean specific operation you see set up the migration infra for environment and it takes care of",
    "start": "769440",
    "end": "776399"
  },
  {
    "text": "everything and uh couple of important things while doing the migration we had a choice of updating",
    "start": "776399",
    "end": "783920"
  },
  {
    "text": "the itd to a latest version for instance to 3.5 because um many of the new features would have helped us to do the",
    "start": "783920",
    "end": "790760"
  },
  {
    "text": "migration in a much more simpler manner for example There's A New Concept called follower model where you can add a new",
    "start": "790760",
    "end": "797519"
  },
  {
    "text": "member as a follower and then wait for it to catch up with the data and do a promotion so that would have simplified",
    "start": "797519",
    "end": "804320"
  },
  {
    "text": "our job but um we wanted to stick to the best practices for example for do I mean",
    "start": "804320",
    "end": "809720"
  },
  {
    "text": "for older versions like 1.15 or 1.14 of kubernetes we have to use corresponding",
    "start": "809720",
    "end": "814800"
  },
  {
    "text": "older itd versions so we didn't want to deviate from those best best practices",
    "start": "814800",
    "end": "821000"
  },
  {
    "text": "so it complicated our migration code but uh we wanted to still do that because we we didn't want to have any surprises by",
    "start": "821000",
    "end": "827360"
  },
  {
    "text": "deviating from the best practices and lastly uh leader election is pretty bad",
    "start": "827360",
    "end": "833440"
  },
  {
    "text": "particularly for software and particularly for hcd uh so uh we don't want to do frequent leader elections",
    "start": "833440",
    "end": "841079"
  },
  {
    "text": "let's say you are migrating the leader again and again you would end up triggering two three leader elections",
    "start": "841079",
    "end": "846399"
  },
  {
    "text": "during the it City migration so we consciously uh took steps to ensure that",
    "start": "846399",
    "end": "851800"
  },
  {
    "text": "the it City leader election doesn't happen during the migration the only time it happens is when you're done with",
    "start": "851800",
    "end": "856839"
  },
  {
    "text": "the migration and you're are dismantling the Legacy",
    "start": "856839",
    "end": "860720"
  },
  {
    "text": "architecture yeah the orchestrator had uh inbuilt capabilities to do uh yeah",
    "start": "862320",
    "end": "867480"
  },
  {
    "text": "fine grained uh um uh migration so we had mechanism to do migration in a given",
    "start": "867480",
    "end": "873160"
  },
  {
    "text": "window and we had concurrency control like these many clusters are migrated at a given point in time we usually start",
    "start": "873160",
    "end": "879680"
  },
  {
    "text": "with one or two migrations and then we scale up all the way to 20 um and most",
    "start": "879680",
    "end": "885000"
  },
  {
    "text": "important thing is it automatically blocks the migration if there is any failure because we want the operator to",
    "start": "885000",
    "end": "891000"
  },
  {
    "text": "jump in and mitigate the migration and understand whether it's region wide issue or a specific uh um uh cluster",
    "start": "891000",
    "end": "898440"
  },
  {
    "text": "issue so we automatically block the migration on first failure and another",
    "start": "898440",
    "end": "903639"
  },
  {
    "text": "value I mean another important thing was to run a migration Canary which is uh",
    "start": "903639",
    "end": "908920"
  },
  {
    "text": "like a test application outside this migration window there are so many things that happen outside the window",
    "start": "908920",
    "end": "914120"
  },
  {
    "text": "like there are so many dependencies we depend on like block block volume DNS Etc things can change overnight so",
    "start": "914120",
    "end": "921199"
  },
  {
    "text": "outside this migration window we periodically learn the canary and if something goes wrong we immediately",
    "start": "921199",
    "end": "927079"
  },
  {
    "text": "block the migration and the has to jump in and see that okay the migration can",
    "start": "927079",
    "end": "932519"
  },
  {
    "text": "be unblocked now so that really helped us in many ways because there are many new changes that are rolled out which we",
    "start": "932519",
    "end": "939560"
  },
  {
    "text": "haven't validated with if it breaks we don't want to do it with production cluster so we run migration a given",
    "start": "939560",
    "end": "945319"
  },
  {
    "text": "window and do the migration Canary in the rest of the time and for each stage of the migration",
    "start": "945319",
    "end": "953199"
  },
  {
    "text": "that I would show in the next slide we had specific alarms and metries so that way we get spefic specific alarms and",
    "start": "953199",
    "end": "959839"
  },
  {
    "text": "corresponding run book so that the folks can jump in the operators can jump in and know exactly what they need to do",
    "start": "959839",
    "end": "966079"
  },
  {
    "text": "and potentially engage the right because the alarm and run books are very",
    "start": "966079",
    "end": "971240"
  },
  {
    "text": "specific so now I will take next few seconds to show how we did the migration at a very high level so this is just one",
    "start": "971240",
    "end": "979160"
  },
  {
    "text": "cluster like I have one control pan in the left with three comput instances running kubernetes control plane and in",
    "start": "979160",
    "end": "985240"
  },
  {
    "text": "the right I have one H CD cluster so first thing we do is we uh uh allocate",
    "start": "985240",
    "end": "991680"
  },
  {
    "text": "the block volumes and DNS entries and assign to this control plane because before we moveed to this new",
    "start": "991680",
    "end": "996920"
  },
  {
    "text": "architecture these compute instances which hosted the control plane were pretty much stateless in the sense they didn't have uh any uh anything",
    "start": "996920",
    "end": "1003600"
  },
  {
    "text": "associated with them like block volumes or uh the DNS names so first before even",
    "start": "1003600",
    "end": "1008639"
  },
  {
    "text": "we proceed to the migration we attach the block volumes and assign the DNS entries to those computer",
    "start": "1008639",
    "end": "1015279"
  },
  {
    "text": "instances then we scale up the it's data store to Five members uh this is because",
    "start": "1015560",
    "end": "1021440"
  },
  {
    "text": "by default we run with three but we want to be more resilient during migration so we scale it up to five members and the",
    "start": "1021440",
    "end": "1028640"
  },
  {
    "text": "black icon corresponds to the leader just to show that we won't touch the leader till the",
    "start": "1028640",
    "end": "1033720"
  },
  {
    "text": "end and then before we do any mutation to the system we take a snapshot into uh",
    "start": "1033720",
    "end": "1039319"
  },
  {
    "text": "into the object storage we do have our coros based itd operator running taking",
    "start": "1039319",
    "end": "1044438"
  },
  {
    "text": "the snapshot but we wanted to do it just in time before touching the member for",
    "start": "1044439",
    "end": "1050240"
  },
  {
    "text": "migration and then we go about moving one member at a time uh into the newer",
    "start": "1050240",
    "end": "1056880"
  },
  {
    "text": "newer environment so every time we do that we first ensure that all five members are healthy and then we go and",
    "start": "1056880",
    "end": "1063559"
  },
  {
    "text": "move move this uh uh first member and the same thing we do repeatedly for the subsequent",
    "start": "1063559",
    "end": "1070320"
  },
  {
    "text": "members once all the members are moved and healthy we dismantle the objects for",
    "start": "1070679",
    "end": "1075960"
  },
  {
    "text": "this cluster in the new environment I mean in the Legacy environment and once all the Clusters",
    "start": "1075960",
    "end": "1083120"
  },
  {
    "text": "are migrated we dismantle the corresponding uh Legacy Arch cluster that we had which hosted the H CD",
    "start": "1083120",
    "end": "1091640"
  },
  {
    "text": "parts now I would uh jump into uh the some of the issues that we ran uh into",
    "start": "1093080",
    "end": "1099799"
  },
  {
    "text": "so I will start with the DNS resolution issue this I mean I can talk about this for the next 15 minutes but I will try",
    "start": "1099799",
    "end": "1105720"
  },
  {
    "text": "to simplify it as reason as simp plus possible so we I started off with the",
    "start": "1105720",
    "end": "1111880"
  },
  {
    "text": "premise that we always ensure that all five members are healthy before we touch a member so our assumption is we are",
    "start": "1111880",
    "end": "1119200"
  },
  {
    "text": "going to manipulate one member the rest four are healthy so our all our five members are I mean even if that member",
    "start": "1119200",
    "end": "1126679"
  },
  {
    "text": "doesn't come up the member which is being migrated the rest four are healthy so the comput uh the control plane would",
    "start": "1126679",
    "end": "1133320"
  },
  {
    "text": "not be impacted so that was our premise but in this case what happened was when we",
    "start": "1133320",
    "end": "1138799"
  },
  {
    "text": "start the migration of the first member all five members started crashing we don't do anything with those members but",
    "start": "1138799",
    "end": "1144880"
  },
  {
    "text": "all of them are crashing with the error um fail to update a member is unknown so I need to set some context on",
    "start": "1144880",
    "end": "1152559"
  },
  {
    "text": "this uh again uh like you would expect again this is narrow down to DNS but yeah um blame it on DNS um but I would",
    "start": "1152559",
    "end": "1161440"
  },
  {
    "text": "elaborate why it happened like this so what we have things of okay let me go to",
    "start": "1161440",
    "end": "1166960"
  },
  {
    "text": "the yeah this will a context right so let's say I'm moving one member at a",
    "start": "1166960",
    "end": "1172320"
  },
  {
    "text": "time to the new environment so the way the me uh member in the new environment",
    "start": "1172320",
    "end": "1179720"
  },
  {
    "text": "talks to the Legacy environment is uh through this Gateway and we ran Cod uh",
    "start": "1179720",
    "end": "1186159"
  },
  {
    "text": "code DNS in a wild card mode where it looks for a suffix and if that suffix is",
    "start": "1186159",
    "end": "1191440"
  },
  {
    "text": "there it will route the traffic to that Gateway the idea is in the Legacy environment the part the it City Parts",
    "start": "1191440",
    "end": "1198520"
  },
  {
    "text": "don't have their identity they are basically the Pod names they typically end with s spc. cluster. loal so we had",
    "start": "1198520",
    "end": "1205640"
  },
  {
    "text": "this wild card C DNS supports this wild card based DNS resolution so we had",
    "start": "1205640",
    "end": "1211120"
  },
  {
    "text": "configured saying that okay if it is start. s spc. local route it through the H City Gateway so a member in the new",
    "start": "1211120",
    "end": "1216960"
  },
  {
    "text": "environment wants to talk the to the peer in the new environment or the Legacy environment it queries the cords",
    "start": "1216960",
    "end": "1224200"
  },
  {
    "text": "coers gives the IP address of the Gateway and through that it talks to the um it City pod running in the Legacy",
    "start": "1224200",
    "end": "1231480"
  },
  {
    "text": "environment but the member which is in the new environment when it wants to talk to other member in the new",
    "start": "1231480",
    "end": "1237760"
  },
  {
    "text": "environment we have the DNS name assigned right so based on that it communicates so there is no code DNS",
    "start": "1237760",
    "end": "1243679"
  },
  {
    "text": "involved there our bcn DNS gets the query and then it translates it and it communicates",
    "start": "1243679",
    "end": "1249320"
  },
  {
    "text": "locally but what happened in our case was uh when a let's say I'm adding a new",
    "start": "1249320",
    "end": "1256280"
  },
  {
    "text": "member and I am bringing bringing up the new member the member builds up a member table and it has a key value pair so key",
    "start": "1256280",
    "end": "1264280"
  },
  {
    "text": "corresponds to the DNS name of the it C the pi City member and the cluster uh",
    "start": "1264280",
    "end": "1269679"
  },
  {
    "text": "member ID basically the member ID assigned to that member so it tries to build that uh first it builds it with",
    "start": "1269679",
    "end": "1276720"
  },
  {
    "text": "default values which are basically sha hash based it creates a sha and builds that member table and then it talks to",
    "start": "1276720",
    "end": "1283760"
  },
  {
    "text": "the peers in the cluster and queries the table uh queries the cluster so that it could get the valid cluster IDs so and",
    "start": "1283760",
    "end": "1291440"
  },
  {
    "text": "then tries to update this member table so for that it uses a concept of URL comparison the idea is you don't compare",
    "start": "1291440",
    "end": "1298600"
  },
  {
    "text": "the DNS names directly you translate the DNS names and if the IP address you get",
    "start": "1298600",
    "end": "1304279"
  },
  {
    "text": "is same then these two URLs are same so that is the idea of URL based comparison",
    "start": "1304279",
    "end": "1310039"
  },
  {
    "text": "instead of string based comparison it does this basically a DNS resolution and sees okay the two IPS are same so it's",
    "start": "1310039",
    "end": "1317320"
  },
  {
    "text": "uh this is the right value I will update that corresponding entry but in this case what happened was when a new member",
    "start": "1317320",
    "end": "1323480"
  },
  {
    "text": "is coming and it's trying to translate it uh when it's trying to translate one of its local members the query goes to",
    "start": "1323480",
    "end": "1329919"
  },
  {
    "text": "the vcn DNS in some high load the DNS translation fails and uh uh so it falls back and it",
    "start": "1329919",
    "end": "1339360"
  },
  {
    "text": "it queries the core DNS and core DNS gives the Gateway IP address and that",
    "start": "1339360",
    "end": "1344600"
  },
  {
    "text": "end ends up in messing up the table which maintained by the H C member so um",
    "start": "1344600",
    "end": "1351080"
  },
  {
    "text": "this again depends on the again this behavior is not well defined in the library so if you say how it G lipy",
    "start": "1351080",
    "end": "1358760"
  },
  {
    "text": "works versus how muscle Library works the behavior is different in case of go Library uh Works more similar to gpy so",
    "start": "1358760",
    "end": "1367200"
  },
  {
    "text": "just to add more context in in terms of uh translating right we have two two things you basically one is end dos and",
    "start": "1367200",
    "end": "1374919"
  },
  {
    "text": "the search domains end do says that the number of dots that should be present in the DNS names if the DNS name doesn't",
    "start": "1374919",
    "end": "1381679"
  },
  {
    "text": "have these many dots I will open the search domain and then do a resolution say for example you're trying to resolve",
    "start": "1381679",
    "end": "1387600"
  },
  {
    "text": "kubernetes default and I set the end dots to be five kubernetes default has one dot so I immediately uh the library",
    "start": "1387600",
    "end": "1395679"
  },
  {
    "text": "knows oh I have to query the search domain and up and and to the resolution so that is the purpose of end dots and",
    "start": "1395679",
    "end": "1401760"
  },
  {
    "text": "search domains this is very common and uh issue this is a cause of many issues uh uh as well so uh with uh the way the",
    "start": "1401760",
    "end": "1409520"
  },
  {
    "text": "libraries handled this is different from one library to the other if uh say for",
    "start": "1409520",
    "end": "1414640"
  },
  {
    "text": "example buzel Library if the DNS name has enough dots already it wouldn't even",
    "start": "1414640",
    "end": "1420279"
  },
  {
    "text": "fall back to openend the search domain and do a resolution but uh gpy or U uh",
    "start": "1420279",
    "end": "1426400"
  },
  {
    "text": "in our case go Library what they do is first they try to do the resolution with the actual DNS name even if it has",
    "start": "1426400",
    "end": "1433039"
  },
  {
    "text": "enough dots uh if the DNS resolution fails let's say the Upstream DNS times out they will up the search domain and",
    "start": "1433039",
    "end": "1440039"
  },
  {
    "text": "try to do the resolution because of the behavior we were exposed because we were",
    "start": "1440039",
    "end": "1445320"
  },
  {
    "text": "trying to we have enough dots in the original DNS name so we are querying for some reason the Upstream DNS failed and",
    "start": "1445320",
    "end": "1452480"
  },
  {
    "text": "our library thought okay it's I will fall back and do upon this s spc. cluster. loal and our cor DNS responded",
    "start": "1452480",
    "end": "1459360"
  },
  {
    "text": "to this uh based on its Wild Card based translation and gave the Gateway IP address so this totally messed up our",
    "start": "1459360",
    "end": "1465399"
  },
  {
    "text": "table and uh all five members started crashing luckily for us we created this issue in",
    "start": "1465399",
    "end": "1470720"
  },
  {
    "text": "prepr so we got away with it um and then we adjusted our search domain so that we",
    "start": "1470720",
    "end": "1476000"
  },
  {
    "text": "don't face this issue we have created uh this uh issue",
    "start": "1476000",
    "end": "1481120"
  },
  {
    "text": "against it City so that uh so basically how the other members perceive this is they are getting a connection from an",
    "start": "1481120",
    "end": "1487279"
  },
  {
    "text": "unknowned cluster uh member ID so ideal expectation is when a member is connecting to you is an unknown thing",
    "start": "1487279",
    "end": "1494000"
  },
  {
    "text": "you should just ignore the connection and get on with it but here they are logging a message saying that it's an",
    "start": "1494000",
    "end": "1499559"
  },
  {
    "text": "unknown member and they are crashing so we have raised a ticket against it City",
    "start": "1499559",
    "end": "1504760"
  },
  {
    "text": "so that folks can look at it and we would also try to help them in",
    "start": "1504760",
    "end": "1510120"
  },
  {
    "text": "this I'll move on with uh uh other issues U",
    "start": "1512360",
    "end": "1518799"
  },
  {
    "text": "um so other issues are much more lighter so we can relax I guess so um um so we",
    "start": "1518799",
    "end": "1527880"
  },
  {
    "text": "have PR TLS enabled everywhere right like both for uh client communication as well as uh peer-to-peer",
    "start": "1527880",
    "end": "1534000"
  },
  {
    "text": "communication uh so with peerto peer Comm so every time a peer tries to communicate with another peer the",
    "start": "1534000",
    "end": "1540360"
  },
  {
    "text": "receiver ensures that the certificate is valid it does few things it tries to Val",
    "start": "1540360",
    "end": "1545480"
  },
  {
    "text": "do a DS resolution of all the entries in the subject alternate name it also does a reverse lookup of the source IP to",
    "start": "1545480",
    "end": "1552760"
  },
  {
    "text": "ensure basically a DNS PTR request and ensure that it finds the entry the the",
    "start": "1552760",
    "end": "1558080"
  },
  {
    "text": "subject alternate name it works fine in a flat environment where all the peers are in the same uh say same subnet but",
    "start": "1558080",
    "end": "1565919"
  },
  {
    "text": "in this case Legacy and new architectures are communicating through a Gateway so the source IP is not going",
    "start": "1565919",
    "end": "1572320"
  },
  {
    "text": "to match because the source IP is always going to be the Gateway IP when the Legacy architecture is receiving the",
    "start": "1572320",
    "end": "1577760"
  },
  {
    "text": "request so uh the only way out for us was to disable this",
    "start": "1577760",
    "end": "1583320"
  },
  {
    "text": "validation initially we had concerns maybe this is a security issue but first Ely we had uh our vcn with the right",
    "start": "1583320",
    "end": "1591200"
  },
  {
    "text": "cist rules so that only these two vcn can communicate so we are okay with uh opening this up and we disabled this",
    "start": "1591200",
    "end": "1598279"
  },
  {
    "text": "sand validation during the migration and another learning we had is",
    "start": "1598279",
    "end": "1603480"
  },
  {
    "text": "uh H CD aggressively does this DNS resolution so we did not remove this so our PR TLS had the entries for both",
    "start": "1603480",
    "end": "1610360"
  },
  {
    "text": "Legacy and new environment but uh once the migration is done the Legacy environments DNS names are invalid but",
    "start": "1610360",
    "end": "1617600"
  },
  {
    "text": "every time there's a connection from the pier it does it tries to validate the uh DNS entry of the pier uh members that",
    "start": "1617600",
    "end": "1626799"
  },
  {
    "text": "overloaded our vcn DNS team and they noticed that there is a spike in the uh uh L DNS queries coming from our tency",
    "start": "1626799",
    "end": "1634600"
  },
  {
    "text": "so and then we fixed it by uh making appropriate changes so the important takeaway is to",
    "start": "1634600",
    "end": "1642000"
  },
  {
    "text": "uh keep the certificate free of craft and remove whichever entry is irrelevant to that environment",
    "start": "1642000",
    "end": "1649520"
  },
  {
    "text": "so um I spoke about this follower model so where when you let's say when you add a new member uh uh what you typically do",
    "start": "1651799",
    "end": "1660039"
  },
  {
    "text": "with the follower uh model is you add a new member and uh uh wait for it to catch up with the data and then promote",
    "start": "1660039",
    "end": "1667000"
  },
  {
    "text": "it as a regular member instead of a follower uh but uh we since we are not using that follower model uh because we",
    "start": "1667000",
    "end": "1674360"
  },
  {
    "text": "wanted to have a support for older it CD versions uh what we were raying on is a health checkup but health check is not",
    "start": "1674360",
    "end": "1681120"
  },
  {
    "text": "meant for this purpose because I have listed down the things that it does as part of the health checkup first it",
    "start": "1681120",
    "end": "1686519"
  },
  {
    "text": "ensures that there are no HD alarms uh like no space alarm or corrupt alarm and",
    "start": "1686519",
    "end": "1691559"
  },
  {
    "text": "it ensures that the cluster has a leader and finally does a quorum read you might",
    "start": "1691559",
    "end": "1696720"
  },
  {
    "text": "be aware there are two kinds of reads in it City one is linearized read and other is serialized read so linearized read is",
    "start": "1696720",
    "end": "1703640"
  },
  {
    "text": "as good as a right it tries to read from a majority Quorum of beers and then",
    "start": "1703640",
    "end": "1709320"
  },
  {
    "text": "acknowledge and give back the read so um by default uh the heal check does a",
    "start": "1709320",
    "end": "1714559"
  },
  {
    "text": "quorum or a linearized read but you have an option to disable it and make it as a",
    "start": "1714559",
    "end": "1719720"
  },
  {
    "text": "serialized read where it just reads from that member and respond back so this is what it does uh ideally Quorum read",
    "start": "1719720",
    "end": "1726480"
  },
  {
    "text": "should have been sufficient but we were not very convinced because it could be possible that there are some issues with the older versions of it CD so the way",
    "start": "1726480",
    "end": "1734120"
  },
  {
    "text": "we went went about is U I spoke about this raft uh indexes like commit index",
    "start": "1734120",
    "end": "1739600"
  },
  {
    "text": "and applied index so you can query those commit index and applied index from the CD like I can query a member and say",
    "start": "1739600",
    "end": "1745799"
  },
  {
    "text": "what is your current commit index and applied index so using that we ensure",
    "start": "1745799",
    "end": "1751120"
  },
  {
    "text": "that okay a new member has actually caught up uh with the existing members by comparing this applied index and then",
    "start": "1751120",
    "end": "1757840"
  },
  {
    "text": "we proceed with the uh next member so we are pretty certain that the migration for that member is",
    "start": "1757840",
    "end": "1765360"
  },
  {
    "text": "done yeah these are minor issues so but I will still mention it here so uh unlike",
    "start": "1766320",
    "end": "1772320"
  },
  {
    "text": "other key valy stores or maybe other databases it City expects that uh you",
    "start": "1772320",
    "end": "1777720"
  },
  {
    "text": "have enumerated all the it City members when you are bringing up a new member if there is any mismatch it will start at",
    "start": "1777720",
    "end": "1784240"
  },
  {
    "text": "the very beginning ideally I would have expected I will talk to any one member and get the updated list but if the list",
    "start": "1784240",
    "end": "1789480"
  },
  {
    "text": "that is provided as a CA uh I mean as a command line parameter in bringing up the it C the bring up itself fails so uh",
    "start": "1789480",
    "end": "1796799"
  },
  {
    "text": "initially our our orchestrator was populating this it City pod manifest basically we run the pods in an headless",
    "start": "1796799",
    "end": "1803440"
  },
  {
    "text": "mode where in our control plane we uh put push the it CD pod manifest and",
    "start": "1803440",
    "end": "1808559"
  },
  {
    "text": "cubet picks it up and uh runs the pods so uh we were initially populating it",
    "start": "1808559",
    "end": "1813640"
  },
  {
    "text": "with the members but let's say there are something happening in the Legacy environment Parts get recreated or",
    "start": "1813640",
    "end": "1818960"
  },
  {
    "text": "things like that this there is a mismatch and the part doesn't come up so we moved this eneration as part of the",
    "start": "1818960",
    "end": "1825519"
  },
  {
    "text": "startup end point of the H CD itself um uh again we didn't want to build a special CD member image for migration",
    "start": "1825519",
    "end": "1833159"
  },
  {
    "text": "because new images get rolled out and we don't want to uh get caught in the",
    "start": "1833159",
    "end": "1839039"
  },
  {
    "text": "process so what we did was we had uh init containers which come in first and pushed to the scratch volume and then we",
    "start": "1839039",
    "end": "1846840"
  },
  {
    "text": "made it City pick that init entry point so that sort of thing so we leverage the existing it C image but we had",
    "start": "1846840",
    "end": "1852760"
  },
  {
    "text": "additional init container which was pushing these additional configuration files for initi izing uh the",
    "start": "1852760",
    "end": "1860518"
  },
  {
    "text": "member so one last issue I hope I will have time to finish this uh let's say",
    "start": "1863720",
    "end": "1869000"
  },
  {
    "text": "when you're are adding a new member right so there are few things which are enforced first is if you add the new",
    "start": "1869000",
    "end": "1874679"
  },
  {
    "text": "member it wouldn't break the Quorum and another thing is uh all the existing members are healthy so the these two I",
    "start": "1874679",
    "end": "1882760"
  },
  {
    "text": "mean uh so the second the first issue which ensures that the Quorum would not be lost makes sense because because uh",
    "start": "1882760",
    "end": "1887840"
  },
  {
    "text": "we don't want to break an existing cluster while adding a new member but the second issue basically the",
    "start": "1887840",
    "end": "1893480"
  },
  {
    "text": "restrictions on adding members uh kind of bit us because let's say we are doing",
    "start": "1893480",
    "end": "1899200"
  },
  {
    "text": "a migration in the new environment we are bringing up a new member but let's say something goes up goes bad in the",
    "start": "1899200",
    "end": "1905000"
  },
  {
    "text": "Legacy environment and the operator jumps in and tries to add an oper add a member there so the addition of the",
    "start": "1905000",
    "end": "1911559"
  },
  {
    "text": "member fails because uh the itd sees oh there are two member additions being tried I'm not going to uh support this",
    "start": "1911559",
    "end": "1918159"
  },
  {
    "text": "member Edition so uh that b us uh but there are ways to disable this but",
    "start": "1918159",
    "end": "1923440"
  },
  {
    "text": "unfortunately the flag that was provided disables both uh uh both these features",
    "start": "1923440",
    "end": "1928639"
  },
  {
    "text": "like basically preventing the Quorum loss as well as preventing multiple member editions so we decided to live",
    "start": "1928639",
    "end": "1934399"
  },
  {
    "text": "with it because we don't want to lose Quorum but uh operator has to jump if something goes wrong the operator has to",
    "start": "1934399",
    "end": "1940080"
  },
  {
    "text": "jump in and manually remove one member to unblock the",
    "start": "1940080",
    "end": "1944760"
  },
  {
    "text": "migration yeah these are basically to call out that one one good thing that c",
    "start": "1945919",
    "end": "1951559"
  },
  {
    "text": "SE team does is uh it does a good job of back porting many of the features to the uh older patch releases all the way to",
    "start": "1951559",
    "end": "1958960"
  },
  {
    "text": "3.3 that really helped us uh because uh so when we started out only thing we needed to ensure is for each of those",
    "start": "1958960",
    "end": "1965120"
  },
  {
    "text": "minor releases uh we are in the uh right patch release so that really helped us",
    "start": "1965120",
    "end": "1971120"
  },
  {
    "text": "with our process yeah this uh concludes my uh",
    "start": "1971120",
    "end": "1977639"
  },
  {
    "text": "presentation I have 2 minutes so if there are any queries about it CD or with the migration process uh I can try",
    "start": "1977639",
    "end": "1984279"
  },
  {
    "text": "to",
    "start": "1984279",
    "end": "1987039"
  },
  {
    "text": "answer thanks good talk um a couple things the the whole DNS search domain",
    "start": "1989799",
    "end": "1997159"
  },
  {
    "text": "Fiasco um I've seen this as well and I think all over time we started just uh",
    "start": "1997159",
    "end": "2003639"
  },
  {
    "text": "putting dots on the end of the host names to avoid search domains in the first place from from exploding the number of queries um but my question is",
    "start": "2003639",
    "end": "2010159"
  },
  {
    "text": "more about actually the the persistent volumes um and the choice to use persistent back backing for ETD so it",
    "start": "2010159",
    "end": "2017679"
  },
  {
    "text": "sounds like when you are doing the migrations those persistent volumes aren't coming with you anyway is that",
    "start": "2017679",
    "end": "2024480"
  },
  {
    "text": "correct um so in our Legacy architecture we did not use block volumes because we",
    "start": "2024480",
    "end": "2029639"
  },
  {
    "text": "are packing too many uh it City Parts in one member and you can't attach for example 40 block volumes to a computer",
    "start": "2029639",
    "end": "2035720"
  },
  {
    "text": "instance it depends again on on the shape of the computer instance so there uh we were relaying on the scratch",
    "start": "2035720",
    "end": "2041360"
  },
  {
    "text": "storage for the backend data but our backup operator was backing the data and we had RPO of 15 minutes so it it was",
    "start": "2041360",
    "end": "2048480"
  },
  {
    "text": "taking the backup so in Legacy environment the data was in the scratch storage but in the new environment uh we",
    "start": "2048480",
    "end": "2055398"
  },
  {
    "text": "didn't want to have that because 15 minutes of RPO is too much so we decided to have uh uh assigned block volume for",
    "start": "2055399",
    "end": "2062240"
  },
  {
    "text": "each of the city members so that's why when during the migration we had to create the block volumes gotcha yeah",
    "start": "2062240",
    "end": "2067839"
  },
  {
    "text": "that's along the lines of what I was wondering is if like the performance difference and",
    "start": "2067839",
    "end": "2075800"
  },
  {
    "text": "and the like the io latency that you'd be able to get from using local nvme versus persisting everything to a",
    "start": "2075800",
    "end": "2083440"
  },
  {
    "text": "dis um I guess the trade-off of that versus uh you know if you lose the data",
    "start": "2083440",
    "end": "2089440"
  },
  {
    "text": "you're just relying on SDs ha um in the first place to to recover if that was a",
    "start": "2089440",
    "end": "2095040"
  },
  {
    "text": "worthwhile tradeoff right right so yeah like I said we do monitor the latency of",
    "start": "2095040",
    "end": "2100520"
  },
  {
    "text": "the block volumes and we have uh the block volume provides all the io throttling uh thing like if we are if a",
    "start": "2100520",
    "end": "2106720"
  },
  {
    "text": "control plane is being bombarded and there are some IO throttling we get alarmed and proactively assign",
    "start": "2106720",
    "end": "2112480"
  },
  {
    "text": "additional eye Ops to the block volume and uh that that that's how we handle this",
    "start": "2112480",
    "end": "2117520"
  },
  {
    "text": "currently yeah I I think it always feels like you can hit the same amount of iops via network attached but IO latency is",
    "start": "2117520",
    "end": "2124400"
  },
  {
    "text": "there's still a pretty big gap there that's hard to overcome",
    "start": "2124400",
    "end": "2128800"
  },
  {
    "text": "right I think I'm over um so thanks for joining the uh talk and honoring me with",
    "start": "2145560",
    "end": "2151720"
  },
  {
    "text": "your presence thanks a lot",
    "start": "2151720",
    "end": "2158800"
  }
]