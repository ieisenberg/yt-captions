[
  {
    "text": "hello everyone welcome to the talk today for accelerated and standardized deep learning friends with care serving",
    "start": "1520",
    "end": "8240"
  },
  {
    "text": "the presenter today is dan song from from bloomberg and david grubbing from nvidia we have two",
    "start": "8240",
    "end": "14400"
  },
  {
    "text": "agenda today the first part we will talk about accelerate deep learning friends with kf serving",
    "start": "14400",
    "end": "19600"
  },
  {
    "text": "the second part we will talk about the capstone v2 influence protocol",
    "start": "19600",
    "end": "25119"
  },
  {
    "text": "deploying ml models at skier is one of the most challenges for companies learning to create values through ai",
    "start": "25439",
    "end": "32000"
  },
  {
    "text": "as models becomes more complex and deep and more deep this task becomes even",
    "start": "32000",
    "end": "37040"
  },
  {
    "text": "harder as a data scientist or emerge engineer i want to serve standardized",
    "start": "37040",
    "end": "43600"
  },
  {
    "text": "deep learning models like tensorflow or pytorch with minimal efforts and at scale in a unified way",
    "start": "43600",
    "end": "51360"
  },
  {
    "text": "i may also want to bring in custom play and post processing before and after the influence",
    "start": "51360",
    "end": "57520"
  },
  {
    "text": "running inference on deep learning models can be slower on cpu i also want to accelerate the influence",
    "start": "57520",
    "end": "64239"
  },
  {
    "text": "by deploying models onto gpus gpus are powerful computer source but",
    "start": "64239",
    "end": "69520"
  },
  {
    "text": "deploying a single model per gpu can underutilize gpus i want an easy way to serve multiple",
    "start": "69520",
    "end": "74880"
  },
  {
    "text": "models behind a single unified endpoint which can scale to hundreds to thousands of models",
    "start": "74880",
    "end": "81360"
  },
  {
    "text": "in order to save the computer resource i will also run to auto scale based on workload and allow",
    "start": "81360",
    "end": "87520"
  },
  {
    "text": "me to scale down to zero when there is no traffic sent to the service in order to ensure safe production",
    "start": "87520",
    "end": "94799"
  },
  {
    "text": "routes i i want to deploy models with zero downtime and can use multiple deployment",
    "start": "94799",
    "end": "101119"
  },
  {
    "text": "strategies like shadow canary and the blue green layouts",
    "start": "101119",
    "end": "106079"
  },
  {
    "text": "in order to solve all these problems cafe serving is an open source project founded by google seldom bloomberg and",
    "start": "109360",
    "end": "114640"
  },
  {
    "text": "media microsoft and ibm on the floor crew flow was the perfect meeting ground for these",
    "start": "114640",
    "end": "119920"
  },
  {
    "text": "companies cave survey is following all the open",
    "start": "119920",
    "end": "124960"
  },
  {
    "text": "source principles built to build open and cloud native",
    "start": "124960",
    "end": "130239"
  },
  {
    "text": "serving solutions cafe serving provides a combination custom resource for serving mr models across deep learning",
    "start": "130239",
    "end": "136480"
  },
  {
    "text": "frameworks with a simple intuitive and consistent experience it encapsulates the complex complexity",
    "start": "136480",
    "end": "143120"
  },
  {
    "text": "of auto scaling networking health checking and server configuration to bring cutting-edge",
    "start": "143120",
    "end": "148560"
  },
  {
    "text": "serving features like gpu auto scan and canary rows to your ml deployments",
    "start": "148560",
    "end": "157040"
  },
  {
    "text": "it enables a simple plugable and complete story for production and mr survey including pre-processing",
    "start": "157040",
    "end": "163040"
  },
  {
    "text": "prediction and explanation",
    "start": "163040",
    "end": "166560"
  },
  {
    "text": "case serving codifies the best practice of culminating standard patterns with canadiaf kef serving is able to",
    "start": "168560",
    "end": "174560"
  },
  {
    "text": "enable serverless inference and automatically scale up and down according to the influence workload",
    "start": "174560",
    "end": "179760"
  },
  {
    "text": "cave serving extracts the common model serving features like request response logging multi-modal polling batching into a",
    "start": "179760",
    "end": "187920"
  },
  {
    "text": "cycle agent so that all integrated model server can benefit all these features",
    "start": "187920",
    "end": "194000"
  },
  {
    "text": "cave summit departments are immutable every new deployment results in a new version and the traffic is migrated over to new",
    "start": "194239",
    "end": "200480"
  },
  {
    "text": "region only after the new parts are ready and passing readiness check to ensure safe production routes",
    "start": "200480",
    "end": "206959"
  },
  {
    "text": "it also employs various other road surges such as canary and the progressive roads",
    "start": "206959",
    "end": "214640"
  },
  {
    "text": "the main cave serving components is still ingress gateway native auto scaler and infrared service",
    "start": "215760",
    "end": "221360"
  },
  {
    "text": "eastern ingress gateway lots the external requests to the influence service each event service pod contains two or",
    "start": "221360",
    "end": "226879"
  },
  {
    "text": "three containers depending on the features enabled on infrastructure",
    "start": "226879",
    "end": "232319"
  },
  {
    "text": "the main containers in the in the key in inference service part are q proxy cab serving agent and model",
    "start": "233200",
    "end": "239360"
  },
  {
    "text": "server after the ingress the request first hits the queue proxy which involves the concurrency limit and",
    "start": "239360",
    "end": "246239"
  },
  {
    "text": "timeout it then goes through an optional cycle agent if requested bronze logging and batching",
    "start": "246239",
    "end": "252560"
  },
  {
    "text": "are enabled the request finally hits the model server for inference the model server for each framework",
    "start": "252560",
    "end": "258320"
  },
  {
    "text": "implements rest or grpc handlers and can load multiple models",
    "start": "258320",
    "end": "264000"
  },
  {
    "text": "the infraservice parts can be auto scaled based on cpu or inference workload",
    "start": "264000",
    "end": "269360"
  },
  {
    "text": "with kpa",
    "start": "269360",
    "end": "272080"
  },
  {
    "text": "kf serving 0.5 release promotes the infrared service api version from v152 to v1 beta1",
    "start": "274639",
    "end": "281040"
  },
  {
    "text": "it supports a standard influence for tensorflow pytorch scaling sg boost mm frameworks",
    "start": "281040",
    "end": "288080"
  },
  {
    "text": "it also provides sdk for users to plug in custom components while still benefiting all the common serving",
    "start": "288080",
    "end": "293199"
  },
  {
    "text": "features from cafe serving the v1 beta1 api further simplifies and enables a data",
    "start": "293199",
    "end": "299600"
  },
  {
    "text": "science friendly interface and also maintains the flexibility kubernetes part templates back provides",
    "start": "299600",
    "end": "307360"
  },
  {
    "text": "in about a few few yamu lines you could describe all the infrastructure you need to get",
    "start": "307360",
    "end": "313360"
  },
  {
    "text": "up your uh your models up and running on the other hand user can still specify",
    "start": "313360",
    "end": "319600"
  },
  {
    "text": "advanced spheres we need to cap serving not only provides a unified interface for control plane it also",
    "start": "319600",
    "end": "326800"
  },
  {
    "text": "tries to standardize the data plan protocol across ml frameworks which we will cover later",
    "start": "326800",
    "end": "333280"
  },
  {
    "text": "since 0.5 we also added the multi model serving capability to improve the resource utilizations",
    "start": "334479",
    "end": "342320"
  },
  {
    "text": "for tensorflow models kf serving uses the tf serving as the underlying model server which is a flexible high performance",
    "start": "342400",
    "end": "348400"
  },
  {
    "text": "serving solution which supports saving model format and graph depth tf7 uses the tensorflow less than grpc",
    "start": "348400",
    "end": "355440"
  },
  {
    "text": "prediction protocol which is similar to kef serving v1 protocol for pytorch models cable server",
    "start": "355440",
    "end": "361680"
  },
  {
    "text": "integrates touchserve which provides an easy way to serve both eager model and this touch scraping model which can serve the",
    "start": "361680",
    "end": "367840"
  },
  {
    "text": "model without a python environment kf7 is working currently working with",
    "start": "367840",
    "end": "372880"
  },
  {
    "text": "touchserve to conform to the kf serving v2 prediction broker",
    "start": "372880",
    "end": "378160"
  },
  {
    "text": "in v1 beta1 api we mainly support three components predictor transformer and explainer",
    "start": "378160",
    "end": "384560"
  },
  {
    "text": "predictor is a required component and interferes on predictor is mapped to kubernetes deployment",
    "start": "384560",
    "end": "389680"
  },
  {
    "text": "or template fields such as replica service account node affinity under predictor",
    "start": "389680",
    "end": "394960"
  },
  {
    "text": "user can specify the model framework which naturally maps to container fields like commands arguments environment variables sim",
    "start": "394960",
    "end": "402639"
  },
  {
    "text": "applies to the transformer and explainer component nvidia triton infraserver provides a",
    "start": "402639",
    "end": "410080"
  },
  {
    "text": "cloud infrastructure solution optimized on a meteor uh gpus the server supports multiple deep",
    "start": "410080",
    "end": "416319"
  },
  {
    "text": "learning frameworks such as tensorflow high torch onyx with both rest energy rc",
    "start": "416319",
    "end": "421680"
  },
  {
    "text": "endpoints it supports model report with versioning and allow multiple models to run simultaneously on the same gpu y",
    "start": "421680",
    "end": "428560"
  },
  {
    "text": "with batching support on infinite service spec the storage ui",
    "start": "428560",
    "end": "434639"
  },
  {
    "text": "is pointed to the model model repo which can contain multiple models",
    "start": "434639",
    "end": "441280"
  },
  {
    "text": "as you can see from spec we also set the omp num spreads in in random variable to improve the",
    "start": "441280",
    "end": "446560"
  },
  {
    "text": "influence performance reduce the resource contention as by",
    "start": "446560",
    "end": "452240"
  },
  {
    "text": "default python just response number of openmp threads same as the number of",
    "start": "452240",
    "end": "457520"
  },
  {
    "text": "cores available on the node which could overload cpu as a default cpu limit on for influence service part",
    "start": "457520",
    "end": "463840"
  },
  {
    "text": "is one you can also choose to schedule the trading infospot on gpus",
    "start": "463840",
    "end": "470160"
  },
  {
    "text": "by specifying it on specified gpu on the resource section",
    "start": "470160",
    "end": "475759"
  },
  {
    "text": "you can also choose to add nodafinity or tolerance to schedule to a particular node such as t4gpu",
    "start": "475759",
    "end": "483840"
  },
  {
    "text": "as you may know bloomberg's customer service is an essential part of roomwork terminal ours representatively work very hard to",
    "start": "484400",
    "end": "491120"
  },
  {
    "text": "provide the best customer service to our clients by answering questions with high quality",
    "start": "491120",
    "end": "496160"
  },
  {
    "text": "and fast speed the reps are pushed content to help answer questions in the smart",
    "start": "496160",
    "end": "502080"
  },
  {
    "text": "resource window this use case is powered by our fine-tunable model and deployed",
    "start": "502080",
    "end": "507360"
  },
  {
    "text": "onto cab service production for the fine-tuned stage the data we are",
    "start": "507360",
    "end": "513518"
  },
  {
    "text": "using are categorized and annotated faq which gives us half million question pairs the problem is formulated as",
    "start": "513519",
    "end": "520399"
  },
  {
    "text": "question similarity with inputs of two questions and output as a similarity score the burn model is saved using export",
    "start": "520399",
    "end": "528240"
  },
  {
    "text": "save the model api which contains complete tensorflow program including word weights and computation",
    "start": "528240",
    "end": "534320"
  },
  {
    "text": "it does not require the original model building code to run which makes it useful for sharing and deploying",
    "start": "534320",
    "end": "541519"
  },
  {
    "text": "as you know bird model at influence time requires significant computation time and doing this on cpu can be slow",
    "start": "541519",
    "end": "548000"
  },
  {
    "text": "which can take a second open the time mechanic seconds this health and challenge to meet the",
    "start": "548000",
    "end": "553920"
  },
  {
    "text": "latest requirement for this real-time for this real-time use case deploying the bird model to meet all the",
    "start": "553920",
    "end": "559519"
  },
  {
    "text": "production requirements is a challenge task you need to take all latency throughput scaling",
    "start": "559519",
    "end": "564880"
  },
  {
    "text": "health check safe draw out monitoring or into consideration",
    "start": "564880",
    "end": "570160"
  },
  {
    "text": "and and also how can you scare to serve hundreds of bad models with your limited",
    "start": "570160",
    "end": "575839"
  },
  {
    "text": "gpu resources first to accelerate the budget influence",
    "start": "575839",
    "end": "582320"
  },
  {
    "text": "we deploy the fine tuner model on cave serving with tensorflow serving component which expand the tensor inputs since the",
    "start": "582320",
    "end": "589440"
  },
  {
    "text": "user input here is the question pair in text we also deploy the kev serving transformer for the sentence",
    "start": "589440",
    "end": "595040"
  },
  {
    "text": "segmentation and tokenization cave serving then automatically welds up the core to",
    "start": "595040",
    "end": "600399"
  },
  {
    "text": "tf serving in this way you can skill transformer and predict",
    "start": "600399",
    "end": "605920"
  },
  {
    "text": "differently why deploy asset while also you can deploy it as a single unit",
    "start": "605920",
    "end": "611839"
  },
  {
    "text": "by deploying the model on gpus we achieve 20x speedup we also experiment deploying the bug",
    "start": "611839",
    "end": "618720"
  },
  {
    "text": "model on a meteor triting conserver which achieves a similar performance gains",
    "start": "618720",
    "end": "625920"
  },
  {
    "text": "we have tested the performance on a meteor v100 gpus with 24 layer fp16 bird",
    "start": "630320",
    "end": "636079"
  },
  {
    "text": "scored model by deploying bird model on gpu we can get to 15 milliseconds which usually",
    "start": "636079",
    "end": "641600"
  },
  {
    "text": "takes two to second three second run imprints on cpu on a single tritone input server part loaded",
    "start": "641600",
    "end": "647440"
  },
  {
    "text": "with word model the latency increased linearly with the concurrency without batching the request",
    "start": "647440",
    "end": "653920"
  },
  {
    "text": "we can scale up the tritone into the server instance automatically with k serving as you can see from the previous",
    "start": "653920",
    "end": "661200"
  },
  {
    "text": "table you get the best difference when continuous concurrency slow so here we try to set the container",
    "start": "661200",
    "end": "666399"
  },
  {
    "text": "concurrency to one and let k native auto scaler scale things influence service part automatically",
    "start": "666399",
    "end": "671920"
  },
  {
    "text": "based on infrared infrared concurrent request within a time window",
    "start": "671920",
    "end": "679279"
  },
  {
    "text": "by setting concluding currency to 1 the input service auto scan automatically scales up when",
    "start": "680160",
    "end": "685680"
  },
  {
    "text": "container reaches to the max concurrency in this experiment the perf client generates the influence service request",
    "start": "685680",
    "end": "691839"
  },
  {
    "text": "to your model and measures the throughput and latency over a time window and repeats",
    "start": "691839",
    "end": "697360"
  },
  {
    "text": "the measurement until it gets stable value for example when the observed",
    "start": "697360",
    "end": "703519"
  },
  {
    "text": "infrared request 5 is scaled up to 5 parts to handle the concurrency from the result table you can see that",
    "start": "703519",
    "end": "709680"
  },
  {
    "text": "the average latency for higher concurrency can maintain as low as when concurrency is low there are some",
    "start": "709680",
    "end": "716399"
  },
  {
    "text": "latency spikes which are caused by the part code startup time which includes model downloading",
    "start": "716399",
    "end": "721519"
  },
  {
    "text": "and part spawning time when containing concurrency specified",
    "start": "721519",
    "end": "727839"
  },
  {
    "text": "canadian activity is also injected on the request path to do smart load balancing to make sure",
    "start": "727839",
    "end": "733200"
  },
  {
    "text": "container is not overloaded with request more than the configured concurrency limit as you can see the",
    "start": "733200",
    "end": "740399"
  },
  {
    "text": "support does not scale linearly perfectly because of the infrared service",
    "start": "740399",
    "end": "745440"
  },
  {
    "text": "code of startup time one way to alleviate the issue is to cache the model on pvc after downloading from the",
    "start": "745440",
    "end": "751120"
  },
  {
    "text": "remote storage so that the models can be directly mounted to one mounted onto other parts",
    "start": "751120",
    "end": "758830"
  },
  {
    "text": "[Music] gpu auto scaling based on gpu metrics can be hard canadian implements a requested volume",
    "start": "758830",
    "end": "765440"
  },
  {
    "text": "based auto scaler which allows you to ask gear 2 and front 0 both on cpu and a gpu",
    "start": "765440",
    "end": "772160"
  },
  {
    "text": "the number of paths needs to scale up the load is calculated by the infrared request and the concurrency target",
    "start": "772160",
    "end": "778160"
  },
  {
    "text": "if the container concurrency can handle 5 requests concurrently and your infrared requests are 50 then can it automatically scale to 10",
    "start": "778160",
    "end": "784839"
  },
  {
    "text": "parts",
    "start": "784839",
    "end": "787839"
  },
  {
    "text": "back influence is a process of aggregating inference requests and sending sending this aggregated request",
    "start": "790480",
    "end": "796320"
  },
  {
    "text": "through the deal framework for influence or at once cave serving was designed to natively support batching of incoming influence",
    "start": "796320",
    "end": "802959"
  },
  {
    "text": "requests the functionality enables you to use your computer resource optimally",
    "start": "802959",
    "end": "808399"
  },
  {
    "text": "because most dna frameworks are optimized for batch requests you can configure max batch size and max",
    "start": "808399",
    "end": "814160"
  },
  {
    "text": "latency on infrared service batch section the capsule cycle agent then knows the maximal batch size that",
    "start": "814160",
    "end": "819760"
  },
  {
    "text": "the model can handle and the maximum time the agent should wait to fulfill",
    "start": "819760",
    "end": "825040"
  },
  {
    "text": "each batch request on the inverse service spec user can also enable scale down to zero to save computer source",
    "start": "825040",
    "end": "831519"
  },
  {
    "text": "after batch is done over the time you need to both you need",
    "start": "831519",
    "end": "838160"
  },
  {
    "text": "both pre and post processing before and after the inference capsule provides the sdk to user to",
    "start": "838160",
    "end": "844000"
  },
  {
    "text": "easily implement a transformer which can communicate to model server with a standardized data plane protocol",
    "start": "844000",
    "end": "850240"
  },
  {
    "text": "which expects tensoring and a tensor out transformer and predictor can also be deployed and",
    "start": "850240",
    "end": "856320"
  },
  {
    "text": "rolled out as a single unit in future cab serving also plans to support a few outbox transformers like",
    "start": "856320",
    "end": "862399"
  },
  {
    "text": "text tokenization and image transformation conserving allows chaining transformer",
    "start": "862399",
    "end": "867680"
  },
  {
    "text": "with any infrared server here we can easily swap to use different one server as long as they speak the",
    "start": "867680",
    "end": "872959"
  },
  {
    "text": "same data protocol the pre-process handler transform the raw data",
    "start": "872959",
    "end": "878079"
  },
  {
    "text": "into the tensor according to the v2 data plan protocol and you can see the post post process",
    "start": "878079",
    "end": "884320"
  },
  {
    "text": "handler transforms the raw prediction um into a response for downstream consumption",
    "start": "884320",
    "end": "892079"
  },
  {
    "text": "the transformer does not enforce a particular data schema it could be a list of text or images the probably single model on gpu",
    "start": "892079",
    "end": "901279"
  },
  {
    "text": "can under utilize the precious gpu resources tf serving touch serve triton or allow",
    "start": "901279",
    "end": "906720"
  },
  {
    "text": "co-locating multiple models onto the same gpu in the container the two requests arrive simultaneously",
    "start": "906720",
    "end": "912399"
  },
  {
    "text": "one for each model they can be scheduled onto the sims gpu and work on both",
    "start": "912399",
    "end": "917600"
  },
  {
    "text": "inference computation parallel cab serving as a retraining model customer source twinbox scheduling",
    "start": "917600",
    "end": "923440"
  },
  {
    "text": "models onto the input service at scale which obstructed away the model placements logic away from the user",
    "start": "923440",
    "end": "930079"
  },
  {
    "text": "the models that are placed on the same inference service customer source can be accessed from the same",
    "start": "930079",
    "end": "935440"
  },
  {
    "text": "service url with the capstone multi-modal serving design we started to decouple infraservice and",
    "start": "935440",
    "end": "941920"
  },
  {
    "text": "model artifacts trainer models can be placed onto an infraservice before it reaches the",
    "start": "941920",
    "end": "947360"
  },
  {
    "text": "configured memory limit conserving trainer model scheduler can spawn new shots for the",
    "start": "947360",
    "end": "952800"
  },
  {
    "text": "internet service to host models at scale it also send cash check purpose to each",
    "start": "952800",
    "end": "958320"
  },
  {
    "text": "model endpoint to reflect the model deployment status on training models customer resource",
    "start": "958320",
    "end": "964720"
  },
  {
    "text": "on training model customer resource users assign the mod to a given influence service users also",
    "start": "964720",
    "end": "970800"
  },
  {
    "text": "specify the model storage ui and a model framework the memory's resource estimation is used for scheduling models onto the input",
    "start": "970800",
    "end": "977120"
  },
  {
    "text": "service based on the capacity left off given in the service shot and the schedule automatically spawn new shots",
    "start": "977120",
    "end": "982880"
  },
  {
    "text": "to host new models if all current shots are at the capacity",
    "start": "982880",
    "end": "988000"
  },
  {
    "text": "the trained model scheduler decides which service charger to place the model and writes the model configuration into",
    "start": "988399",
    "end": "994880"
  },
  {
    "text": "the shot config map the cave serving sidecar agent then reconciles the config map to",
    "start": "994880",
    "end": "999920"
  },
  {
    "text": "port new models and remove deleted models in the model repo that is mounted onto the part the agent",
    "start": "999920",
    "end": "1006639"
  },
  {
    "text": "then sends a signal to the model server to low download the model after model is successfully loaded",
    "start": "1006639",
    "end": "1012800"
  },
  {
    "text": "on model server it can then be accessed from a unified service endpoint with the model name specified on the uiopath",
    "start": "1012800",
    "end": "1020880"
  },
  {
    "text": "we developed the multi model serving we hit fuel our scalability issues as well for a single steel ingress gateway we",
    "start": "1023120",
    "end": "1028959"
  },
  {
    "text": "handle limits about running on 2000 services but we want to actually scale to 100k models",
    "start": "1028959",
    "end": "1036160"
  },
  {
    "text": "can we support the 100k training model customer resource we actually bound by the customer",
    "start": "1036160",
    "end": "1043199"
  },
  {
    "text": "resource limit on scale std even we can simply deploy 100k models on",
    "start": "1043199",
    "end": "1049520"
  },
  {
    "text": "2000 service we may still hit the limit of the number of scratch services we can create on the grid rate",
    "start": "1049520",
    "end": "1054559"
  },
  {
    "text": "for routing the models so the phase two multimodal service is to find out these limits",
    "start": "1054559",
    "end": "1061760"
  },
  {
    "text": "we are excited that kf7 0.5 is released with the v1 beta1 api",
    "start": "1063840",
    "end": "1070880"
  },
  {
    "text": "and triton waste v2 protocol and here you can check the view on beta1 ifc",
    "start": "1070880",
    "end": "1076880"
  },
  {
    "text": "ifc dock multimodal serving is in offer steps and the next step is to make to make it",
    "start": "1076880",
    "end": "1083440"
  },
  {
    "text": "a production rating cap service is an open community so we love your contributions",
    "start": "1083440",
    "end": "1089760"
  },
  {
    "text": "and you're welcome to join our back bi-weekly um working group meeting and here's the",
    "start": "1089760",
    "end": "1096160"
  },
  {
    "text": "github and example links",
    "start": "1096160",
    "end": "1100400"
  },
  {
    "text": "thank you i will hand over to our deregulating for the second part",
    "start": "1101679",
    "end": "1109840"
  },
  {
    "text": "hi my name is david goodwin i'm going to be talking about the kf serving inference protocol version two you might",
    "start": "1116640",
    "end": "1123760"
  },
  {
    "text": "have also heard this called the v2 data plane protocol so before we talk about the specifics of",
    "start": "1123760",
    "end": "1131280"
  },
  {
    "text": "the protocol let's look at kind of the domain where this protocol",
    "start": "1131280",
    "end": "1136799"
  },
  {
    "text": "is important and is meant to be used so the diagram here shows kind of a representative way",
    "start": "1136799",
    "end": "1144240"
  },
  {
    "text": "of deploying can say a containerized inference server so in the gray box on",
    "start": "1144240",
    "end": "1149679"
  },
  {
    "text": "the right you see there's this inference server that is capable of",
    "start": "1149679",
    "end": "1154720"
  },
  {
    "text": "performing deep learning or machine learning inferences uh on behalf of whatever clients there",
    "start": "1154720",
    "end": "1160000"
  },
  {
    "text": "are and it is is inside of uh some kind of deployment environment it has access to",
    "start": "1160000",
    "end": "1167280"
  },
  {
    "text": "the model repository which is holding all the different models that it might need to serve and uh there also may be",
    "start": "1167280",
    "end": "1174400"
  },
  {
    "text": "some load balancers and some auto scalers etc those those aren't required this is just showing kind of a representative example",
    "start": "1174400",
    "end": "1180799"
  },
  {
    "text": "and then on the left you see there's some clients so these are clients that want to directly or indirectly take",
    "start": "1180799",
    "end": "1187840"
  },
  {
    "text": "advantage of some deep learning or some machine learning models and so they can communicate",
    "start": "1187840",
    "end": "1193200"
  },
  {
    "text": "for example maybe there's an asr an automatic speech recognition kind of service that they want to use",
    "start": "1193200",
    "end": "1198240"
  },
  {
    "text": "which will in turn need to do some inferencing and there could be multiple of these workloads and again",
    "start": "1198240",
    "end": "1204000"
  },
  {
    "text": "there can be load balancers and in the deployment the protocol",
    "start": "1204000",
    "end": "1209760"
  },
  {
    "text": "is concerned about how the clients communicate with the workloads and also how the workloads then communicate with the",
    "start": "1209760",
    "end": "1215440"
  },
  {
    "text": "uh inference services themselves and the protocol is meant to be a single protocol that can be used in any of these areas",
    "start": "1215440",
    "end": "1224480"
  },
  {
    "text": "so given that background [Music]",
    "start": "1224480",
    "end": "1229600"
  },
  {
    "text": "for the the kf serving inference protocol why do we need this standard well like",
    "start": "1229600",
    "end": "1236720"
  },
  {
    "text": "most reasons why you need a standard you want the say the inference clients which in the previous diagram were the",
    "start": "1236720",
    "end": "1243200"
  },
  {
    "text": "things on the left that needed to use some inferencing you want them to be able to talk to multiple different servers or services",
    "start": "1243200",
    "end": "1249360"
  },
  {
    "text": "to increase their portability to make them more interchangeable and on the server side",
    "start": "1249360",
    "end": "1254799"
  },
  {
    "text": "you want to allow as many different types of clients as possible maybe ones that weren't originally meant",
    "start": "1254799",
    "end": "1259919"
  },
  {
    "text": "written even to use your server to be able to use your server or service and that increases the utility of your",
    "start": "1259919",
    "end": "1265280"
  },
  {
    "text": "service and of course you want them to operate seamlessly on all sorts of platforms like kf servings",
    "start": "1265280",
    "end": "1271120"
  },
  {
    "text": "which is standardized around this protocol so some of the high level requirements uh",
    "start": "1271120",
    "end": "1277280"
  },
  {
    "text": "are support support both ease of use and high performance which we'll talk about some uh they need to be extensible so the kf",
    "start": "1277280",
    "end": "1284880"
  },
  {
    "text": "serving protocol talk about it has a core protocol which is kind of the required part but there's a extension mechanism in it",
    "start": "1284880",
    "end": "1292320"
  },
  {
    "text": "that allows either for in the future there to be uh standard extensions added to it so",
    "start": "1292320",
    "end": "1298960"
  },
  {
    "text": "optional parts of this of the core or uh you could have a server we'll see the server specific ones which are",
    "start": "1298960",
    "end": "1305679"
  },
  {
    "text": "individuals inference servers can decide to implement their own extensions and uh there needs to be both a grpc",
    "start": "1305679",
    "end": "1312720"
  },
  {
    "text": "and an http rest and a json based implementation for this which we'll look at also in a minute",
    "start": "1312720",
    "end": "1321039"
  },
  {
    "text": "so i mentioned there's the core protocol and the extension the core protocol which we'll go through in some detail",
    "start": "1321919",
    "end": "1328480"
  },
  {
    "text": "is required for all conforming servers so if you want to if you want to write an inference server and you want to say",
    "start": "1328480",
    "end": "1333919"
  },
  {
    "text": "that your inference server supports the kf serving inference protocol you need to implement all these",
    "start": "1333919",
    "end": "1339520"
  },
  {
    "text": "and they include uh endpoints to understand if the server is live and ready",
    "start": "1339520",
    "end": "1344880"
  },
  {
    "text": "to get kind of metadata about the server and then for all the models that are available on the",
    "start": "1344880",
    "end": "1349919"
  },
  {
    "text": "server all the deep learning and machine learning models that are available whether the model is ready and and",
    "start": "1349919",
    "end": "1355600"
  },
  {
    "text": "metadata about the model and of course the primary reason for doing all this there needs to be",
    "start": "1355600",
    "end": "1361919"
  },
  {
    "text": "an inferencing endpoint to allow you to actually perform inferencing",
    "start": "1361919",
    "end": "1366960"
  },
  {
    "text": "and then again extensions are optional and currently the standard doesn't have any there's no standard extensions so",
    "start": "1366960",
    "end": "1372720"
  },
  {
    "text": "there's no optional parts of the core but we'll see there are some extensions",
    "start": "1372720",
    "end": "1378720"
  },
  {
    "text": "have been implemented by uh specific inference servers in particular we're going to look at some",
    "start": "1378720",
    "end": "1383760"
  },
  {
    "text": "the triton inference server has implemented some extensions we're going to talk about those briefly",
    "start": "1383760",
    "end": "1390080"
  },
  {
    "text": "okay so the liveness and the readiness endpoints together you could think of these as the health endpoints and",
    "start": "1390240",
    "end": "1396960"
  },
  {
    "text": "on the server side there's two two different endpoints that you can see whether the server is live and or ready to receive requests",
    "start": "1396960",
    "end": "1404799"
  },
  {
    "text": "and so for instance in something like kubernetes you can directly use these for liveness probe and readiness probe",
    "start": "1404799",
    "end": "1413200"
  },
  {
    "text": "and on the model side there's just a ready and that can be used to indicate if the model is ready to receive requests",
    "start": "1413360",
    "end": "1420480"
  },
  {
    "text": "and so this is some example here again there's an http uh rest style version of this protocol",
    "start": "1420480",
    "end": "1426240"
  },
  {
    "text": "and it just returns for these returns that uses the http status code to indicate whether something is live or",
    "start": "1426240",
    "end": "1432000"
  },
  {
    "text": "ready uh and here we're asking if the server is live",
    "start": "1432000",
    "end": "1437440"
  },
  {
    "text": "and you get an okay back so that means the server is live grpc side has you know it's very similar",
    "start": "1437440",
    "end": "1442960"
  },
  {
    "text": "of course it's using uh the rpc style but there's a server live and it it would turn a bull or two frost",
    "start": "1442960",
    "end": "1448640"
  },
  {
    "text": "response that would indicate whether the server was live all right so let's move on metadata",
    "start": "1448640",
    "end": "1455279"
  },
  {
    "text": "there's another core part of the protocol and the server metadata is you know things like name versions what",
    "start": "1455279",
    "end": "1461360"
  },
  {
    "text": "extensions are supported the model metadata is more interesting you can kind of ask",
    "start": "1461360",
    "end": "1466400"
  },
  {
    "text": "so what you know for a given model that's on the server you can you can find out what versions of the model are available the protocol",
    "start": "1466400",
    "end": "1473760"
  },
  {
    "text": "allows you know the server to have multiple versions of the same model and then you know most importantly",
    "start": "1473760",
    "end": "1479840"
  },
  {
    "text": "maybe what input tensors are you required to send into the model and what outputs can you get back",
    "start": "1479840",
    "end": "1486080"
  },
  {
    "text": "and for the inputs you can see here i'm showing for this example and and going forward here i'm just going to kind of show the uh kind of the json",
    "start": "1486080",
    "end": "1492960"
  },
  {
    "text": "http uh examples the grpc protobuff based grpc examples are very",
    "start": "1492960",
    "end": "1499279"
  },
  {
    "text": "similar it's just a kind of different syntax so i'm not going to bother to show both so here you can see from the url we're",
    "start": "1499279",
    "end": "1505760"
  },
  {
    "text": "using the slash v2 is slash models",
    "start": "1505760",
    "end": "1511600"
  },
  {
    "text": "slash resnet gives us the metadata for the resnet50 model we see that there's just one version",
    "start": "1511600",
    "end": "1517600"
  },
  {
    "text": "version one is available it's a tensorflow graph def model which is just the type so the platform is a",
    "start": "1517600",
    "end": "1524000"
  },
  {
    "text": "the type of the machine learning or deep learning framework or representation for this",
    "start": "1524000",
    "end": "1532000"
  },
  {
    "text": "model that's kind of informative one of the advantages of the of the protocol of course is that it's",
    "start": "1532000",
    "end": "1539679"
  },
  {
    "text": "the same protocol you you send you use the inputs and output tensors and you communicate with the servers the same",
    "start": "1539679",
    "end": "1546320"
  },
  {
    "text": "way no matter what the actual underlying machine learning or deep learning model is but for informational purposes it's",
    "start": "1546320",
    "end": "1553679"
  },
  {
    "text": "there inputs in this case there's only one input in one output on this model",
    "start": "1553679",
    "end": "1559039"
  },
  {
    "text": "and you get the name of the input the shape and the data type which is very important because this allows you then as a client you can then",
    "start": "1559039",
    "end": "1566159"
  },
  {
    "text": "if you want to discover like well what's you know what kind of data shape of the data is expected and",
    "start": "1566159",
    "end": "1571200"
  },
  {
    "text": "what data type you can get that from the metadata and similarly for the outputs it'll tell you what type of",
    "start": "1571200",
    "end": "1576720"
  },
  {
    "text": "uh output tensors are going to be returned to you and their their shape and their data type",
    "start": "1576720",
    "end": "1582799"
  },
  {
    "text": "so and now for the again the primary um uh uh",
    "start": "1582799",
    "end": "1589440"
  },
  {
    "text": "endpoint or api that's in the protocol of course is for the inferencing",
    "start": "1589440",
    "end": "1594559"
  },
  {
    "text": "and again this is just showing the http uh the rest style but here's the endpoint again",
    "start": "1594559",
    "end": "1600799"
  },
  {
    "text": "we saw before v2 models resnet50 so for the resonant 50 model we want to do an inference that's that infer there",
    "start": "1600799",
    "end": "1606960"
  },
  {
    "text": "and in this case this is the resin at 50 mile we're going to send in this is just a picture i picked um",
    "start": "1606960",
    "end": "1614240"
  },
  {
    "text": "and so resnet50 is if you're not familiar as an image classification uh as a very famous image classification",
    "start": "1614240",
    "end": "1620720"
  },
  {
    "text": "uh deep learning model and it takes in uh a an image in this case",
    "start": "1620720",
    "end": "1627919"
  },
  {
    "text": "the intel just by the shape this is this is uh 224 by 224 so a very small image with three channels",
    "start": "1627919",
    "end": "1634720"
  },
  {
    "text": "and uh and it'll do a classification resonant 50 will do a classification and out of and tell",
    "start": "1634720",
    "end": "1642080"
  },
  {
    "text": "you what that image is and in this case resident 50 it's been trained with the imagenet uh data set which there's like a thousand",
    "start": "1642080",
    "end": "1648240"
  },
  {
    "text": "different uh uh objects it recognizes and one of them you can see from here so we",
    "start": "1648240",
    "end": "1654000"
  },
  {
    "text": "on the left here you see the post post this um we send in the input and um",
    "start": "1654000",
    "end": "1661840"
  },
  {
    "text": "tell the shape that we're sending in and the data here i didn't list all the data because we're going to talk about that there's a lot of data so just but",
    "start": "1661840",
    "end": "1668240"
  },
  {
    "text": "there would be just an array of the json array of the data um and there'll be you can tell from the",
    "start": "1668240",
    "end": "1675120"
  },
  {
    "text": "shape there'll be 224 times 224 times 3 elements in the state array which is a lot and then the output",
    "start": "1675120",
    "end": "1682159"
  },
  {
    "text": "coming back this is what the protocol requires the format that's required by the protocol that the server",
    "start": "1682159",
    "end": "1687520"
  },
  {
    "text": "needs to respond with you know repeating back the model name and version",
    "start": "1687520",
    "end": "1692640"
  },
  {
    "text": "and then giving you the outputs back and the outputs in this case you can see the name of it there and the",
    "start": "1692640",
    "end": "1698640"
  },
  {
    "text": "shape it's just a single element and in this case it's a single element that tells you the classification of",
    "start": "1698640",
    "end": "1704640"
  },
  {
    "text": "this item and it's a coffee mug so it did quite well actually on that on that classification",
    "start": "1704640",
    "end": "1709760"
  },
  {
    "text": "and so here by looking at this example we can see you know the inference protocol like the other pro the other ones i just talked",
    "start": "1709760",
    "end": "1716080"
  },
  {
    "text": "about earlier it's very simple very simple it's just the necessary kind of",
    "start": "1716080",
    "end": "1721200"
  },
  {
    "text": "basic uh information that needs to be sent to the inferencing server and back and all",
    "start": "1721200",
    "end": "1727679"
  },
  {
    "text": "based around having input tensors and output tensors",
    "start": "1727679",
    "end": "1732720"
  },
  {
    "text": "with their data types and shapes so um let's",
    "start": "1732720",
    "end": "1739120"
  },
  {
    "text": "take so that's the core protocol we just talked about again the required core parts of the",
    "start": "1739120",
    "end": "1744480"
  },
  {
    "text": "protocol that every conforming server must implement and in doing that and implementing that",
    "start": "1744480",
    "end": "1749760"
  },
  {
    "text": "the uh you'll have a server that can talk to in any client that also influences",
    "start": "1749760",
    "end": "1756159"
  },
  {
    "text": "protocol for any type of deep learning or machine learning model that that supports",
    "start": "1756159",
    "end": "1761360"
  },
  {
    "text": "and send in the tensors get back the results and kind of a standard seamless way which satisfies",
    "start": "1761360",
    "end": "1768640"
  },
  {
    "text": "the primary requirements of of the protocol or any kind of standard protocol for that matter",
    "start": "1768640",
    "end": "1774640"
  },
  {
    "text": "so um tritone inference server here trite inference server is an open source inference server i have a reference link",
    "start": "1774640",
    "end": "1781520"
  },
  {
    "text": "later on if you're interested in taking a look at it but it's it implements the both the http rest and the grpc",
    "start": "1781520",
    "end": "1789679"
  },
  {
    "text": "kf serving inference protocols it's a multi-framework multi-model it",
    "start": "1789679",
    "end": "1794720"
  },
  {
    "text": "puts gpu and cpus so multi-framework i mean if you have a tensorflow or a pi torch or a an onyx",
    "start": "1794720",
    "end": "1802799"
  },
  {
    "text": "model again that all those types of uh frameworks are supported for inside the",
    "start": "1802799",
    "end": "1808720"
  },
  {
    "text": "inference server it implements again the core so it's a conforming server it also has some extensions and that's a briefly going to",
    "start": "1808720",
    "end": "1815279"
  },
  {
    "text": "talk about extensions here so the core is completely sufficient that we just",
    "start": "1815279",
    "end": "1821039"
  },
  {
    "text": "talked about is completely sufficient for making an inference server however especially in the area of performance it",
    "start": "1821039",
    "end": "1827360"
  },
  {
    "text": "can be lacking especially for the http protocol or primary for http protocol",
    "start": "1827360",
    "end": "1832559"
  },
  {
    "text": "so the extensions that triton implements there's some per model statistics so",
    "start": "1832559",
    "end": "1837760"
  },
  {
    "text": "you'll notice in the core protocol there was no asa to find out",
    "start": "1837760",
    "end": "1842799"
  },
  {
    "text": "from the server how many inferences possibly had been performed for a particular model or",
    "start": "1842799",
    "end": "1848559"
  },
  {
    "text": "you know how long on average were those taking that you know kind of statistics like that uh triton adds an extension that",
    "start": "1848559",
    "end": "1854720"
  },
  {
    "text": "provides that information also an extension for model repository management so not only can you query",
    "start": "1854720",
    "end": "1860880"
  },
  {
    "text": "which models are available on the server but you can load and unload those models or load new models and unload",
    "start": "1860880",
    "end": "1866159"
  },
  {
    "text": "models uh also as an extension and try it in their support for stateful",
    "start": "1866159",
    "end": "1872000"
  },
  {
    "text": "inferencing which there's a lot of language models and",
    "start": "1872000",
    "end": "1878000"
  },
  {
    "text": "other models that are implemented say with an rnn type type implementation where there's state in the model and so you have to",
    "start": "1878000",
    "end": "1884559"
  },
  {
    "text": "the order of the inference is received is very important there's support for that for performance now the last two items",
    "start": "1884559",
    "end": "1890880"
  },
  {
    "text": "were primary performance bullets tensors as we saw in the example uh",
    "start": "1890880",
    "end": "1896640"
  },
  {
    "text": "above you actually send the tensor data is part of the json message",
    "start": "1896640",
    "end": "1901760"
  },
  {
    "text": "that can be very expensive sending that over the network and so you if you're if you're communicating to",
    "start": "1901760",
    "end": "1907919"
  },
  {
    "text": "the triton inference server on the same system from another process on the same system you can instead communicate by system",
    "start": "1907919",
    "end": "1914080"
  },
  {
    "text": "shared memory or by gpu shared memory and that then you don't have to send the",
    "start": "1914080",
    "end": "1919120"
  },
  {
    "text": "raw data over the over actually over the network even local network",
    "start": "1919120",
    "end": "1924720"
  },
  {
    "text": "um in or and you don't have to encode it say into json or into a grpc protobuf you don't have to do that encoding",
    "start": "1924720",
    "end": "1930640"
  },
  {
    "text": "decoding you can just access it directly in memory",
    "start": "1930640",
    "end": "1935679"
  },
  {
    "text": "and similarly the last one which is just an extension that only applies to the http rest part of the protocol is a way to communicate",
    "start": "1935679",
    "end": "1943279"
  },
  {
    "text": "tensors using binary data still going over the network not using shared memory but a much more efficient way and talk about that",
    "start": "1943279",
    "end": "1949919"
  },
  {
    "text": "in the next slide um so and i call this one out so for all the",
    "start": "1949919",
    "end": "1955519"
  },
  {
    "text": "extensions i just showed here again these are extensions uh they are optional",
    "start": "1955519",
    "end": "1961039"
  },
  {
    "text": "and so the uh servers don't have to implement these in fact they're not part of the standard",
    "start": "1961039",
    "end": "1966960"
  },
  {
    "text": "anyway they're just trying to specific servers could implement these they're triton's open source these are the",
    "start": "1966960",
    "end": "1972720"
  },
  {
    "text": "the the specs for all these is published and i'm going to have a link later",
    "start": "1972720",
    "end": "1978320"
  },
  {
    "text": "but again they're optional parts however the in particular the i want to call it the http json",
    "start": "1978320",
    "end": "1985840"
  },
  {
    "text": "the http rest the json implementation for high performance if we look at if we go back on the on",
    "start": "1985840",
    "end": "1992080"
  },
  {
    "text": "the we have this binary data extension in trying which resolves a very important problem so if you look at the code on the left",
    "start": "1992080",
    "end": "1997440"
  },
  {
    "text": "or the the output on the left where we post this is the same what we did before where we posted um we send in",
    "start": "1997440",
    "end": "2004799"
  },
  {
    "text": "a single small image a 224 by 224 with three channel image which is quite small there's still",
    "start": "2004799",
    "end": "2010480"
  },
  {
    "text": "150 000 fp numbers in this data array which of course i didn't list them all",
    "start": "2010480",
    "end": "2016159"
  },
  {
    "text": "and so that's a lot not just for a single small image and the problem is that this is very",
    "start": "2016159",
    "end": "2022559"
  },
  {
    "text": "readable right it's very usable i mean a lot it's easy to generate this json in a lot of different clients a lot",
    "start": "2022559",
    "end": "2028159"
  },
  {
    "text": "of different languages a lot of different toolkits can already do all this stuff however to do that you first have to so",
    "start": "2028159",
    "end": "2033519"
  },
  {
    "text": "if you have these this image or this 224 by 224x3",
    "start": "2033519",
    "end": "2038799"
  },
  {
    "text": "set representation of this image as floating point numbers you first have to",
    "start": "2038799",
    "end": "2044799"
  },
  {
    "text": "basically encode them into strings so print them basically so that's that's time consuming and then you send",
    "start": "2044799",
    "end": "2050560"
  },
  {
    "text": "them over the wire to the server which is on um",
    "start": "2050560",
    "end": "2055599"
  },
  {
    "text": "you know on the other end and then it has to read this json it's basically just a string right so it has to actually parse",
    "start": "2055599",
    "end": "2061919"
  },
  {
    "text": "these strings into floating point numbers 150 000 of them just to do",
    "start": "2061919",
    "end": "2067679"
  },
  {
    "text": "your inferencing and so that becomes a huge bottleneck basically makes for any any unless your",
    "start": "2067679",
    "end": "2073520"
  },
  {
    "text": "model has very small uh input and output tensors or",
    "start": "2073520",
    "end": "2078560"
  },
  {
    "text": "if you know you just don't really care this is a research model or something on the side you don't you know the form doesn't matter to you then this",
    "start": "2078560",
    "end": "2084800"
  },
  {
    "text": "is not really uh acceptable it's not really useful for a production kind of deployment",
    "start": "2084800",
    "end": "2090560"
  },
  {
    "text": "but still the advantages again going to the ease of use you know this json is very easy to",
    "start": "2090560",
    "end": "2096240"
  },
  {
    "text": "generate and there's a lot of advantages to doing it this way uh but so less can we combine those and",
    "start": "2096240",
    "end": "2101280"
  },
  {
    "text": "so this binary data extension does just that so on the right we can see using the binary date extension basically the",
    "start": "2101280",
    "end": "2106960"
  },
  {
    "text": "the header part the json part is more or less the same except you have this extra parameter in here which you basically say that hey",
    "start": "2106960",
    "end": "2114720"
  },
  {
    "text": "i'm not giving you the data here in json it's just but i'm just telling you how big it's",
    "start": "2114720",
    "end": "2120079"
  },
  {
    "text": "going to be uh which is a little bit redundant but that's done intentionally just to be redundant and then you just after the",
    "start": "2120079",
    "end": "2126240"
  },
  {
    "text": "json message uh is just the raw binary data and there is an extra",
    "start": "2126240",
    "end": "2132240"
  },
  {
    "text": "header required in your post just so the server can figure out where the the json metadata header is",
    "start": "2132240",
    "end": "2138720"
  },
  {
    "text": "separating it from the actual raw data but in doing this now there's no more",
    "start": "2138720",
    "end": "2144720"
  },
  {
    "text": "you're still sending um you know six hundred and two thousand bytes of data you can't",
    "start": "2144720",
    "end": "2151680"
  },
  {
    "text": "that's that's kind of unavoidable that's actually your image you have to send it over but there's no uh encoding or",
    "start": "2151680",
    "end": "2159119"
  },
  {
    "text": "decoding overheads the data can just be pulled right off the wire and kind of sent right into the",
    "start": "2159119",
    "end": "2164640"
  },
  {
    "text": "into the model so and then so for instance this is just one data point uh running on a um",
    "start": "2164640",
    "end": "2172560"
  },
  {
    "text": "running on a local network so actually it's minimizing the uh it's kind of removing the network",
    "start": "2172560",
    "end": "2177760"
  },
  {
    "text": "part of this if we send a 128 of these requests and time that uh just to kind of even it out and make",
    "start": "2177760",
    "end": "2183440"
  },
  {
    "text": "sure there's no blips uh you know doing the binary data really has like a 17x speed up a huge amount so",
    "start": "2183440",
    "end": "2189760"
  },
  {
    "text": "you kind of remove that bottleneck from from the problem",
    "start": "2189760",
    "end": "2194640"
  },
  {
    "text": "so that again that's important example to show how the protocol through its extensions in this case but in general the",
    "start": "2194800",
    "end": "2200160"
  },
  {
    "text": "protocol does allow the usability and because the flexibility of the protocol you cannot have extensions like this",
    "start": "2200160",
    "end": "2205920"
  },
  {
    "text": "that also allow you along with say the shared memory to get great performance as well",
    "start": "2205920",
    "end": "2212160"
  },
  {
    "text": "and so as i mentioned there's a couple references here in closing one is to the protocol itself uh you can find it in the kf serving uh",
    "start": "2212160",
    "end": "2218880"
  },
  {
    "text": "github uh and comments are welcome on that",
    "start": "2218880",
    "end": "2224079"
  },
  {
    "text": "take a look at it and then for triton uh this is a link you can see from this github you can",
    "start": "2224079",
    "end": "2230160"
  },
  {
    "text": "find actually the full triton repo but this link directly takes you to the extensions and the inferencing",
    "start": "2230160",
    "end": "2236480"
  },
  {
    "text": "protocols that triton implements thank you",
    "start": "2236480",
    "end": "2243838"
  }
]