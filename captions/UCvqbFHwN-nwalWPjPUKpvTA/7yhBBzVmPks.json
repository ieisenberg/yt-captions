[
  {
    "text": "[Music]",
    "start": "230",
    "end": "1920"
  },
  {
    "text": "oh no not again this page at 3:00 a.m",
    "start": "1920",
    "end": "5839"
  },
  {
    "text": "Let me see what is happening this time I",
    "start": "5839",
    "end": "8000"
  },
  {
    "text": "don't know why they why I switch to this",
    "start": "8000",
    "end": "9599"
  },
  {
    "text": "job I get wake up every night now Let's",
    "start": "9599",
    "end": "12880"
  },
  {
    "text": "see what's happening So there is an",
    "start": "12880",
    "end": "15440"
  },
  {
    "text": "error message here on Slack Broad apps",
    "start": "15440",
    "end": "17840"
  },
  {
    "text": "cache service pod is down I have no idea",
    "start": "17840",
    "end": "20960"
  },
  {
    "text": "how to troubleshoot Kubernetes Let me",
    "start": "20960",
    "end": "22640"
  },
  {
    "text": "call Lucas He usually helps me",
    "start": "22640",
    "end": "26920"
  },
  {
    "text": "Hello Moto",
    "start": "28560",
    "end": "31519"
  },
  {
    "text": "Hey Chuck What's up hey Lucas What are",
    "start": "31519",
    "end": "33360"
  },
  {
    "text": "you doing i was just sleeping man It's",
    "start": "33360",
    "end": "35520"
  },
  {
    "text": "3:00 a.m in the morning What do you want",
    "start": "35520",
    "end": "37360"
  },
  {
    "text": "oh sorry I didn't realize that So you",
    "start": "37360",
    "end": "40320"
  },
  {
    "text": "helped me last time I just saw an issue",
    "start": "40320",
    "end": "42320"
  },
  {
    "text": "here on this lag message that the cash",
    "start": "42320",
    "end": "44800"
  },
  {
    "text": "service part is down Can you help me i",
    "start": "44800",
    "end": "47600"
  },
  {
    "text": "can but I don't have my laptop near me I",
    "start": "47600",
    "end": "49680"
  },
  {
    "text": "will be home like in 10 minutes from now",
    "start": "49680",
    "end": "51680"
  },
  {
    "text": "I'm not home You know that you are on",
    "start": "51680",
    "end": "53760"
  },
  {
    "text": "call right i'm on call That's my problem",
    "start": "53760",
    "end": "55920"
  },
  {
    "text": "not your problem You know that right",
    "start": "55920",
    "end": "57840"
  },
  {
    "text": "okay let's try to fix the issue So",
    "start": "57840",
    "end": "60160"
  },
  {
    "text": "luckily the DevOps engineer team has",
    "start": "60160",
    "end": "62000"
  },
  {
    "text": "developed a chatbot interface that's",
    "start": "62000",
    "end": "63760"
  },
  {
    "text": "powered by Genai that you can use to get",
    "start": "63760",
    "end": "66159"
  },
  {
    "text": "your root cause analysis faster So what",
    "start": "66159",
    "end": "68159"
  },
  {
    "text": "I'm going to do right now is I'm sending",
    "start": "68159",
    "end": "70000"
  },
  {
    "text": "a link to that chatbot interface to your",
    "start": "70000",
    "end": "72400"
  },
  {
    "text": "Slack and then you should open that link",
    "start": "72400",
    "end": "74400"
  },
  {
    "text": "and you should be able to troubleshoot",
    "start": "74400",
    "end": "76159"
  },
  {
    "text": "that and I'll be waiting here on the",
    "start": "76159",
    "end": "77920"
  },
  {
    "text": "line After you do the troubleshooting",
    "start": "77920",
    "end": "79680"
  },
  {
    "text": "you tell me uh what you have figured out",
    "start": "79680",
    "end": "81600"
  },
  {
    "text": "and then I can try to fix that issue for",
    "start": "81600",
    "end": "83360"
  },
  {
    "text": "you How does it sound sounds great Let's",
    "start": "83360",
    "end": "85439"
  },
  {
    "text": "try that out Okay So what you need to do",
    "start": "85439",
    "end": "88640"
  },
  {
    "text": "essentially is just like go to the Slack",
    "start": "88640",
    "end": "90240"
  },
  {
    "text": "message that you received and copy the",
    "start": "90240",
    "end": "92079"
  },
  {
    "text": "description and the alert for your",
    "start": "92079",
    "end": "95439"
  },
  {
    "text": "problem that you are seeing right now",
    "start": "95439",
    "end": "96799"
  },
  {
    "text": "Let me just copy the error message and",
    "start": "96799",
    "end": "98400"
  },
  {
    "text": "send to the chat here The date I will",
    "start": "98400",
    "end": "100880"
  },
  {
    "text": "select today Let's send it here and see",
    "start": "100880",
    "end": "103439"
  },
  {
    "text": "what happens All right",
    "start": "103439",
    "end": "106320"
  },
  {
    "text": "So the poding is in a pending state for",
    "start": "106320",
    "end": "108799"
  },
  {
    "text": "over one",
    "start": "108799",
    "end": "109960"
  },
  {
    "text": "minute H no idea what's happening",
    "start": "109960",
    "end": "115119"
  },
  {
    "text": "Let me know when you get a response back",
    "start": "115119",
    "end": "117200"
  },
  {
    "text": "please Sure I'll let you know So looking",
    "start": "117200",
    "end": "120159"
  },
  {
    "text": "here into the response it says that the",
    "start": "120159",
    "end": "121759"
  },
  {
    "text": "cubectl describe pod it's the pod is in",
    "start": "121759",
    "end": "125040"
  },
  {
    "text": "a pending state and with an error",
    "start": "125040",
    "end": "127040"
  },
  {
    "text": "measure of create container config error",
    "start": "127040",
    "end": "129440"
  },
  {
    "text": "and there is no log events on the pod So",
    "start": "129440",
    "end": "133200"
  },
  {
    "text": "it's also saying that it found a note to",
    "start": "133200",
    "end": "135520"
  },
  {
    "text": "schedule this spot and there is an",
    "start": "135520",
    "end": "137520"
  },
  {
    "text": "environment variable called cache config",
    "start": "137520",
    "end": "139520"
  },
  {
    "text": "that is pointing to a non-existing",
    "start": "139520",
    "end": "141440"
  },
  {
    "text": "config map called non-existing cache",
    "start": "141440",
    "end": "144239"
  },
  {
    "text": "config That doesn't sound right It's not",
    "start": "144239",
    "end": "146319"
  },
  {
    "text": "right man We have a repository template",
    "start": "146319",
    "end": "148160"
  },
  {
    "text": "that does this scaffolding for the other",
    "start": "148160",
    "end": "150000"
  },
  {
    "text": "repositories and you know like the",
    "start": "150000",
    "end": "151760"
  },
  {
    "text": "developers supposed like remove that",
    "start": "151760",
    "end": "154239"
  },
  {
    "text": "environment variable and then replace",
    "start": "154239",
    "end": "156160"
  },
  {
    "text": "with their own config map But since that",
    "start": "156160",
    "end": "158720"
  },
  {
    "text": "the development team has not done that",
    "start": "158720",
    "end": "160480"
  },
  {
    "text": "So you are seeing a false positive most",
    "start": "160480",
    "end": "162239"
  },
  {
    "text": "likely Okay And how do I fix that yeah",
    "start": "162239",
    "end": "165440"
  },
  {
    "text": "let me try to do that What we can do is",
    "start": "165440",
    "end": "167360"
  },
  {
    "text": "we can remove the environment variable",
    "start": "167360",
    "end": "169200"
  },
  {
    "text": "from the cache config map since it's",
    "start": "169200",
    "end": "170959"
  },
  {
    "text": "like it's not for production use And",
    "start": "170959",
    "end": "173440"
  },
  {
    "text": "then it should be up and running Okay",
    "start": "173440",
    "end": "176239"
  },
  {
    "text": "let's try that Yeah let me try to do",
    "start": "176239",
    "end": "177760"
  },
  {
    "text": "that Now I have reach at home We have no",
    "start": "177760",
    "end": "179840"
  },
  {
    "text": "acting skills whatsoever",
    "start": "179840",
    "end": "182720"
  },
  {
    "text": "All right Uh so here we have cubectl get",
    "start": "182720",
    "end": "186640"
  },
  {
    "text": "all on namespace pro apps",
    "start": "186640",
    "end": "190480"
  },
  {
    "text": "Yeah I'm seeing a lot of errors Let's",
    "start": "190480",
    "end": "192879"
  },
  {
    "text": "focus in one by one first Cache service",
    "start": "192879",
    "end": "194959"
  },
  {
    "text": "I just got alerted by one error So just",
    "start": "194959",
    "end": "197280"
  },
  {
    "text": "fix that one Yeah let's focus on the",
    "start": "197280",
    "end": "198720"
  },
  {
    "text": "cache service I'm going to edit the",
    "start": "198720",
    "end": "200480"
  },
  {
    "text": "deployment Don't do that in production",
    "start": "200480",
    "end": "202239"
  },
  {
    "text": "Please folks on prod apps name space I'm",
    "start": "202239",
    "end": "204879"
  },
  {
    "text": "going to go down and then I'm going to",
    "start": "204879",
    "end": "206560"
  },
  {
    "text": "search for the environment variables",
    "start": "206560",
    "end": "208879"
  },
  {
    "text": "Okay Non-existing cache config I'm",
    "start": "208879",
    "end": "211280"
  },
  {
    "text": "seeing it now It's a radius Let me",
    "start": "211280",
    "end": "213519"
  },
  {
    "text": "remove that environment variable",
    "start": "213519",
    "end": "217560"
  },
  {
    "text": "There you go WQ cubectl",
    "start": "218879",
    "end": "224159"
  },
  {
    "text": "get all on product apps name space and",
    "start": "224159",
    "end": "226879"
  },
  {
    "text": "we should be able to see it running now",
    "start": "226879",
    "end": "229760"
  },
  {
    "text": "Awesome Oh my god we have fixed the",
    "start": "229760",
    "end": "231840"
  },
  {
    "text": "issue And with that in mind folks we're",
    "start": "231840",
    "end": "233840"
  },
  {
    "text": "going to introduce the session that",
    "start": "233840",
    "end": "235120"
  },
  {
    "text": "we're going to deliver for you today",
    "start": "235120",
    "end": "236720"
  },
  {
    "text": "from log screen size We're going to show",
    "start": "236720",
    "end": "238400"
  },
  {
    "text": "you how we have built a chatbot using",
    "start": "238400",
    "end": "240720"
  },
  {
    "text": "all the genai buzzwords and everything",
    "start": "240720",
    "end": "243120"
  },
  {
    "text": "that everybody likes I'm Lucas Dard a",
    "start": "243120",
    "end": "245439"
  },
  {
    "text": "container specialist SA uh in AWS I've",
    "start": "245439",
    "end": "247760"
  },
  {
    "text": "been as for about five years but I have",
    "start": "247760",
    "end": "250000"
  },
  {
    "text": "more than eight years of experience",
    "start": "250000",
    "end": "251439"
  },
  {
    "text": "working with DevOps from Brazil to the",
    "start": "251439",
    "end": "253200"
  },
  {
    "text": "United States And here with me I have my",
    "start": "253200",
    "end": "255280"
  },
  {
    "text": "partner in crime Thiago Hi shirt Similar",
    "start": "255280",
    "end": "257519"
  },
  {
    "text": "to Lucas I'm working at AWS a little bit",
    "start": "257519",
    "end": "259519"
  },
  {
    "text": "more than five years as well And I'm",
    "start": "259519",
    "end": "261519"
  },
  {
    "text": "covering currently containers uh for",
    "start": "261519",
    "end": "264080"
  },
  {
    "text": "startups in Latin America",
    "start": "264080",
    "end": "266639"
  },
  {
    "text": "That's awesome So let's get started with",
    "start": "266639",
    "end": "268400"
  },
  {
    "text": "the session Right So first off what we",
    "start": "268400",
    "end": "270720"
  },
  {
    "text": "are trying to solve here is I think that",
    "start": "270720",
    "end": "272639"
  },
  {
    "text": "was pretty clear with our non-acting",
    "start": "272639",
    "end": "275440"
  },
  {
    "text": "skills that we just did in the beginning",
    "start": "275440",
    "end": "276960"
  },
  {
    "text": "of the presentation for what we call",
    "start": "276960",
    "end": "278560"
  },
  {
    "text": "hook So basically the problem is usually",
    "start": "278560",
    "end": "281440"
  },
  {
    "text": "they have to work in operation center",
    "start": "281440",
    "end": "282880"
  },
  {
    "text": "engineers they don't have they don't",
    "start": "282880",
    "end": "284320"
  },
  {
    "text": "have they tend not to have a lot of deep",
    "start": "284320",
    "end": "286000"
  },
  {
    "text": "kubernetes expertise usually we need on",
    "start": "286000",
    "end": "288880"
  },
  {
    "text": "call support to go there and even like",
    "start": "288880",
    "end": "291759"
  },
  {
    "text": "with the minimum amount of details try",
    "start": "291759",
    "end": "293440"
  },
  {
    "text": "to troubleshoot that I was already in",
    "start": "293440",
    "end": "295280"
  },
  {
    "text": "that situation that gave me the idea to",
    "start": "295280",
    "end": "297759"
  },
  {
    "text": "build that session with Thiago because I",
    "start": "297759",
    "end": "299600"
  },
  {
    "text": "was already wake up at 3:00 a.m in the",
    "start": "299600",
    "end": "301360"
  },
  {
    "text": "morning to try to fix a fix an issue",
    "start": "301360",
    "end": "303360"
  },
  {
    "text": "that I didn't have any idea on how to do",
    "start": "303360",
    "end": "305440"
  },
  {
    "text": "that So the problem is pretty clear and",
    "start": "305440",
    "end": "307600"
  },
  {
    "text": "how we are trying to do that the the",
    "start": "307600",
    "end": "309840"
  },
  {
    "text": "solution that we're trying to propose",
    "start": "309840",
    "end": "311280"
  },
  {
    "text": "here is an automated Kubernetes",
    "start": "311280",
    "end": "313199"
  },
  {
    "text": "expertise so we can empower the",
    "start": "313199",
    "end": "315600"
  },
  {
    "text": "networking operations center engineers",
    "start": "315600",
    "end": "317759"
  },
  {
    "text": "with guided troubleshooting and",
    "start": "317759",
    "end": "319680"
  },
  {
    "text": "real-time recommendations I've caught",
    "start": "319680",
    "end": "322320"
  },
  {
    "text": "myself using Genai to learn English I",
    "start": "322320",
    "end": "324880"
  },
  {
    "text": "barely spoke English before I moved to",
    "start": "324880",
    "end": "326720"
  },
  {
    "text": "the US So ever since I joined AWS US",
    "start": "326720",
    "end": "329440"
  },
  {
    "text": "I've used GIF for the entire for the",
    "start": "329440",
    "end": "332000"
  },
  {
    "text": "past three years and that's the way that",
    "start": "332000",
    "end": "333600"
  },
  {
    "text": "I've learned English How we can apply",
    "start": "333600",
    "end": "335600"
  },
  {
    "text": "the same principles that I have done for",
    "start": "335600",
    "end": "337120"
  },
  {
    "text": "myself in troubleshooting Kubernetes",
    "start": "337120",
    "end": "339039"
  },
  {
    "text": "cluster to do what to reduce the",
    "start": "339039",
    "end": "340960"
  },
  {
    "text": "meanantime to to repair which is",
    "start": "340960",
    "end": "343199"
  },
  {
    "text": "basically the amount of the the total",
    "start": "343199",
    "end": "345440"
  },
  {
    "text": "maintenance time divided by the number",
    "start": "345440",
    "end": "347039"
  },
  {
    "text": "of repairs And when we are talking about",
    "start": "347039",
    "end": "349520"
  },
  {
    "text": "observability and when we are talking",
    "start": "349520",
    "end": "351199"
  },
  {
    "text": "about Kubernetes there are many",
    "start": "351199",
    "end": "353120"
  },
  {
    "text": "challenges that we need to face that we",
    "start": "353120",
    "end": "355360"
  },
  {
    "text": "need to face in order to get to the root",
    "start": "355360",
    "end": "357840"
  },
  {
    "text": "cause analysis We saw that in the last",
    "start": "357840",
    "end": "359759"
  },
  {
    "text": "presentation right now So Thiago why",
    "start": "359759",
    "end": "361680"
  },
  {
    "text": "don't you take us away into some of the",
    "start": "361680",
    "end": "363440"
  },
  {
    "text": "challenges that Kubernetes observability",
    "start": "363440",
    "end": "365440"
  },
  {
    "text": "have Sure Let's dive deeper into the",
    "start": "365440",
    "end": "367360"
  },
  {
    "text": "challenges So when we talk about",
    "start": "367360",
    "end": "369199"
  },
  {
    "text": "container workloads Kubernetes workloads",
    "start": "369199",
    "end": "371520"
  },
  {
    "text": "usually we also talk about microservices",
    "start": "371520",
    "end": "373520"
  },
  {
    "text": "architecture because they work very well",
    "start": "373520",
    "end": "375360"
  },
  {
    "text": "together So let's consider we have a",
    "start": "375360",
    "end": "377520"
  },
  {
    "text": "micros service architecture You might",
    "start": "377520",
    "end": "379360"
  },
  {
    "text": "have multiple pods here even for each",
    "start": "379360",
    "end": "381600"
  },
  {
    "text": "microservices So you have a distributed",
    "start": "381600",
    "end": "383960"
  },
  {
    "text": "application This this gets even harder",
    "start": "383960",
    "end": "386400"
  },
  {
    "text": "to observe when we talk about add-ons",
    "start": "386400",
    "end": "389199"
  },
  {
    "text": "Add-on is a nice way for you to extend",
    "start": "389199",
    "end": "390960"
  },
  {
    "text": "the kubernetes functionality For example",
    "start": "390960",
    "end": "393039"
  },
  {
    "text": "having githubs having observability",
    "start": "393039",
    "end": "395199"
  },
  {
    "text": "itself and also security But after you",
    "start": "395199",
    "end": "397840"
  },
  {
    "text": "enable those add-ons you also have to",
    "start": "397840",
    "end": "399440"
  },
  {
    "text": "observe those add-ons and make sure that",
    "start": "399440",
    "end": "401039"
  },
  {
    "text": "they are working as expected The same is",
    "start": "401039",
    "end": "403360"
  },
  {
    "text": "true for the note as well You have",
    "start": "403360",
    "end": "404560"
  },
  {
    "text": "couplet running on them you have to make",
    "start": "404560",
    "end": "406240"
  },
  {
    "text": "sure that cublet is working as expected",
    "start": "406240",
    "end": "408560"
  },
  {
    "text": "So looking into a study from",
    "start": "408560",
    "end": "410440"
  },
  {
    "text": "log.io they uh they observed that the",
    "start": "410440",
    "end": "413759"
  },
  {
    "text": "meanantime to repair increased over time",
    "start": "413759",
    "end": "416080"
  },
  {
    "text": "So in 2021 47% of the organizations had",
    "start": "416080",
    "end": "420240"
  },
  {
    "text": "a meanantime to recover or to repair",
    "start": "420240",
    "end": "422080"
  },
  {
    "text": "actually over one hour And if you look",
    "start": "422080",
    "end": "424639"
  },
  {
    "text": "into 2024 that number went to 82% of the",
    "start": "424639",
    "end": "428080"
  },
  {
    "text": "organizations having a meantime to",
    "start": "428080",
    "end": "429759"
  },
  {
    "text": "repair over one hour And that's mainly",
    "start": "429759",
    "end": "432319"
  },
  {
    "text": "because of the complexities as we just",
    "start": "432319",
    "end": "434720"
  },
  {
    "text": "uh saw So applications being more",
    "start": "434720",
    "end": "436919"
  },
  {
    "text": "distributed you might have cross",
    "start": "436919",
    "end": "438960"
  },
  {
    "text": "namespace dependencies You can have",
    "start": "438960",
    "end": "441199"
  },
  {
    "text": "network policies to secure your",
    "start": "441199",
    "end": "442880"
  },
  {
    "text": "application but you can also have issues",
    "start": "442880",
    "end": "444720"
  },
  {
    "text": "on those network policies and something",
    "start": "444720",
    "end": "446639"
  },
  {
    "text": "that should not be blocked being blocked",
    "start": "446639",
    "end": "448639"
  },
  {
    "text": "You can have resource blind spots and so",
    "start": "448639",
    "end": "450960"
  },
  {
    "text": "on So Lucas why don't you take us over",
    "start": "450960",
    "end": "453280"
  },
  {
    "text": "on how we can leverage Genai to help us",
    "start": "453280",
    "end": "455680"
  },
  {
    "text": "genai let's go So that's the solution",
    "start": "455680",
    "end": "458400"
  },
  {
    "text": "architecture overview I know too many",
    "start": "458400",
    "end": "460800"
  },
  {
    "text": "components so let's do it little by",
    "start": "460800",
    "end": "462960"
  },
  {
    "text": "little Let's focus first on the",
    "start": "462960",
    "end": "464960"
  },
  {
    "text": "ingestion pipeline because as we saw in",
    "start": "464960",
    "end": "467039"
  },
  {
    "text": "the last presentation log quality is",
    "start": "467039",
    "end": "469360"
  },
  {
    "text": "pretty important whenever trying to get",
    "start": "469360",
    "end": "471199"
  },
  {
    "text": "to the root cause of the issue that we",
    "start": "471199",
    "end": "472960"
  },
  {
    "text": "are trying to solve Exactly So let's",
    "start": "472960",
    "end": "475120"
  },
  {
    "text": "move on onto the ingestion pipeline",
    "start": "475120",
    "end": "477280"
  },
  {
    "text": "Everything starts with a Kubernetes",
    "start": "477280",
    "end": "478639"
  },
  {
    "text": "cluster in Kubernetes We have fluent bit",
    "start": "478639",
    "end": "480720"
  },
  {
    "text": "installed Fluent bit is looking not only",
    "start": "480720",
    "end": "482879"
  },
  {
    "text": "to the application logs but it's also",
    "start": "482879",
    "end": "485039"
  },
  {
    "text": "looking to Kubernetes events because",
    "start": "485039",
    "end": "487440"
  },
  {
    "text": "since we have we are Kubernetes experts",
    "start": "487440",
    "end": "489599"
  },
  {
    "text": "we know that a lot of times we use the",
    "start": "489599",
    "end": "491440"
  },
  {
    "text": "Kubernetes events to troubleshoot our",
    "start": "491440",
    "end": "493039"
  },
  {
    "text": "cluster we also have cublet logs from",
    "start": "493039",
    "end": "496160"
  },
  {
    "text": "each individual node as Thiago mentioned",
    "start": "496160",
    "end": "498240"
  },
  {
    "text": "we have information that could be useful",
    "start": "498240",
    "end": "500319"
  },
  {
    "text": "for us whenever we're trying to do",
    "start": "500319",
    "end": "501520"
  },
  {
    "text": "troubleshooting for from all those",
    "start": "501520",
    "end": "503039"
  },
  {
    "text": "sources So what we do we grab that",
    "start": "503039",
    "end": "505440"
  },
  {
    "text": "information we ship that information to",
    "start": "505440",
    "end": "507120"
  },
  {
    "text": "Amazon Kinesis data streams you could",
    "start": "507120",
    "end": "509120"
  },
  {
    "text": "use your own Kafka as well if you are",
    "start": "509120",
    "end": "510720"
  },
  {
    "text": "using open source for example installed",
    "start": "510720",
    "end": "512159"
  },
  {
    "text": "on your Kubernetes cluster after we send",
    "start": "512159",
    "end": "514479"
  },
  {
    "text": "that data to Kinesis data streams we are",
    "start": "514479",
    "end": "516560"
  },
  {
    "text": "being trigger we're going to trigger a",
    "start": "516560",
    "end": "518080"
  },
  {
    "text": "lambda function that is responsible to",
    "start": "518080",
    "end": "519839"
  },
  {
    "text": "process that particular data and that is",
    "start": "519839",
    "end": "521680"
  },
  {
    "text": "a way to execute code outside of",
    "start": "521680",
    "end": "523120"
  },
  {
    "text": "Kubernetes in AWS and we're using lambda",
    "start": "523120",
    "end": "524959"
  },
  {
    "text": "function just because we have event",
    "start": "524959",
    "end": "526959"
  },
  {
    "text": "bridge connected to Amazon kinesis so",
    "start": "526959",
    "end": "528720"
  },
  {
    "text": "it's pretty simple to be to build that",
    "start": "528720",
    "end": "530320"
  },
  {
    "text": "architecture the lambda function we use",
    "start": "530320",
    "end": "532399"
  },
  {
    "text": "a model that is that is useful for",
    "start": "532399",
    "end": "534720"
  },
  {
    "text": "embeddings because we're going to do rag",
    "start": "534720",
    "end": "536720"
  },
  {
    "text": "afterwards So that embedding model that",
    "start": "536720",
    "end": "538880"
  },
  {
    "text": "we are using is Amazon Titan and what",
    "start": "538880",
    "end": "540880"
  },
  {
    "text": "we're going to do for those who are not",
    "start": "540880",
    "end": "542320"
  },
  {
    "text": "familiar with embedding is we're going",
    "start": "542320",
    "end": "544240"
  },
  {
    "text": "to pass the documents to the embedding",
    "start": "544240",
    "end": "546000"
  },
  {
    "text": "model and the embedding model is going",
    "start": "546000",
    "end": "547440"
  },
  {
    "text": "to return for us the vectors to the",
    "start": "547440",
    "end": "549200"
  },
  {
    "text": "lambda function Once the vectors are",
    "start": "549200",
    "end": "550800"
  },
  {
    "text": "returned to the lambda function we're",
    "start": "550800",
    "end": "552480"
  },
  {
    "text": "going to save the pair using both logs",
    "start": "552480",
    "end": "555120"
  },
  {
    "text": "and the vectors that we embedded into a",
    "start": "555120",
    "end": "558000"
  },
  {
    "text": "vector database And in this case we are",
    "start": "558000",
    "end": "559920"
  },
  {
    "text": "using Amazon open search And that's",
    "start": "559920",
    "end": "561760"
  },
  {
    "text": "pretty important We are saving the pairs",
    "start": "561760",
    "end": "564160"
  },
  {
    "text": "because embeddings are unidirectional We",
    "start": "564160",
    "end": "566720"
  },
  {
    "text": "cannot de-mbbed the embedding So we need",
    "start": "566720",
    "end": "568800"
  },
  {
    "text": "to save the pair and use the embeddings",
    "start": "568800",
    "end": "570560"
  },
  {
    "text": "to search But that's going to be clearer",
    "start": "570560",
    "end": "573920"
  },
  {
    "text": "when Thiago take us away in the next",
    "start": "573920",
    "end": "576240"
  },
  {
    "text": "slides But you know in order to",
    "start": "576240",
    "end": "578880"
  },
  {
    "text": "customize foundation models to make them",
    "start": "578880",
    "end": "580880"
  },
  {
    "text": "do whatever we want to do there are many",
    "start": "580880",
    "end": "582640"
  },
  {
    "text": "different options and for this",
    "start": "582640",
    "end": "584480"
  },
  {
    "text": "particular use case we have P rag but",
    "start": "584480",
    "end": "586640"
  },
  {
    "text": "Thiago can you explain a little bit why",
    "start": "586640",
    "end": "588480"
  },
  {
    "text": "we have P rag and what are the other",
    "start": "588480",
    "end": "590080"
  },
  {
    "text": "options that we have sure So as we will",
    "start": "590080",
    "end": "593040"
  },
  {
    "text": "see here in the following slides as we",
    "start": "593040",
    "end": "594640"
  },
  {
    "text": "move here to the right Yeah it's your",
    "start": "594640",
    "end": "597360"
  },
  {
    "text": "right You will see that the complexity",
    "start": "597360",
    "end": "599640"
  },
  {
    "text": "increases the quality also increases If",
    "start": "599640",
    "end": "602320"
  },
  {
    "text": "you move here to the right with",
    "start": "602320",
    "end": "603680"
  },
  {
    "text": "customizing foundational models but",
    "start": "603680",
    "end": "606480"
  },
  {
    "text": "having better quality you have a lot",
    "start": "606480",
    "end": "608480"
  },
  {
    "text": "more complexity time and cost usually",
    "start": "608480",
    "end": "610720"
  },
  {
    "text": "also related to it So let's start with",
    "start": "610720",
    "end": "613040"
  },
  {
    "text": "prompt engineering That is probably the",
    "start": "613040",
    "end": "615040"
  },
  {
    "text": "easiest way for you to customize your",
    "start": "615040",
    "end": "616800"
  },
  {
    "text": "foundational foundational models So with",
    "start": "616800",
    "end": "619120"
  },
  {
    "text": "proto engineering it's quite easy You",
    "start": "619120",
    "end": "620959"
  },
  {
    "text": "send a question to the model and you get",
    "start": "620959",
    "end": "622640"
  },
  {
    "text": "an answer You can do a few techniques",
    "start": "622640",
    "end": "624560"
  },
  {
    "text": "here for example one shot or few shot",
    "start": "624560",
    "end": "626959"
  },
  {
    "text": "where you send for example examples on",
    "start": "626959",
    "end": "629120"
  },
  {
    "text": "how you want your answer to be or a",
    "start": "629120",
    "end": "631760"
  },
  {
    "text": "little bit of context about your issue",
    "start": "631760",
    "end": "633600"
  },
  {
    "text": "so that the model can give a better",
    "start": "633600",
    "end": "635120"
  },
  {
    "text": "answer As we move to the right here we",
    "start": "635120",
    "end": "637279"
  },
  {
    "text": "have retrieval augmented generations or",
    "start": "637279",
    "end": "639680"
  },
  {
    "text": "rack more known as rack With rack we",
    "start": "639680",
    "end": "642560"
  },
  {
    "text": "still do prompt engineering It's pretty",
    "start": "642560",
    "end": "644240"
  },
  {
    "text": "important to always do prompt",
    "start": "644240",
    "end": "645839"
  },
  {
    "text": "engineering because that will improve a",
    "start": "645839",
    "end": "647440"
  },
  {
    "text": "lot your responses from a model But here",
    "start": "647440",
    "end": "650079"
  },
  {
    "text": "we have an additional step that is",
    "start": "650079",
    "end": "652320"
  },
  {
    "text": "getting context from a database In our",
    "start": "652320",
    "end": "654640"
  },
  {
    "text": "case the open search database that Lucas",
    "start": "654640",
    "end": "656560"
  },
  {
    "text": "just mentioned So we get relevant logs",
    "start": "656560",
    "end": "659440"
  },
  {
    "text": "to that specific issue that the person",
    "start": "659440",
    "end": "661360"
  },
  {
    "text": "is asking to troubleshoot as context and",
    "start": "661360",
    "end": "664480"
  },
  {
    "text": "pass that to the model to get a better",
    "start": "664480",
    "end": "666680"
  },
  {
    "text": "troubleshooting related to the context",
    "start": "666680",
    "end": "668720"
  },
  {
    "text": "of my current application For example",
    "start": "668720",
    "end": "670560"
  },
  {
    "text": "the error logs from the application So",
    "start": "670560",
    "end": "672880"
  },
  {
    "text": "moving forward we have fine-tuning The",
    "start": "672880",
    "end": "675040"
  },
  {
    "text": "idea of fine-tuning is for you to get a",
    "start": "675040",
    "end": "676880"
  },
  {
    "text": "model as base and make that model more",
    "start": "676880",
    "end": "679440"
  },
  {
    "text": "uh more knowledgeable in a specific area",
    "start": "679440",
    "end": "681920"
  },
  {
    "text": "For example industry You could for",
    "start": "681920",
    "end": "684000"
  },
  {
    "text": "example want to have a model that knows",
    "start": "684000",
    "end": "686079"
  },
  {
    "text": "more about financial industry or",
    "start": "686079",
    "end": "687839"
  },
  {
    "text": "healthcare You can get a base model use",
    "start": "687839",
    "end": "690560"
  },
  {
    "text": "a fine tuning job uh for example with a",
    "start": "690560",
    "end": "693760"
  },
  {
    "text": "lot of data sets related to that",
    "start": "693760",
    "end": "695360"
  },
  {
    "text": "specific industry let's say financial",
    "start": "695360",
    "end": "697680"
  },
  {
    "text": "train that on top of GPUs and at the end",
    "start": "697680",
    "end": "700079"
  },
  {
    "text": "you have another model and with that",
    "start": "700079",
    "end": "702079"
  },
  {
    "text": "model you can do again rack and also",
    "start": "702079",
    "end": "704399"
  },
  {
    "text": "prompt engineering as we saw before but",
    "start": "704399",
    "end": "706560"
  },
  {
    "text": "now with a model that is more specific",
    "start": "706560",
    "end": "708560"
  },
  {
    "text": "to your use case for that again you will",
    "start": "708560",
    "end": "711680"
  },
  {
    "text": "need a data set with data related to",
    "start": "711680",
    "end": "713839"
  },
  {
    "text": "that uh specific industry that you want",
    "start": "713839",
    "end": "715839"
  },
  {
    "text": "to train and also GPUs and last we have",
    "start": "715839",
    "end": "718640"
  },
  {
    "text": "training your model from scratch that's",
    "start": "718640",
    "end": "720480"
  },
  {
    "text": "also possible what changes here you",
    "start": "720480",
    "end": "722880"
  },
  {
    "text": "don't have a model as s at the end you",
    "start": "722880",
    "end": "726160"
  },
  {
    "text": "Thiago I I I don't think that diagram",
    "start": "726160",
    "end": "728480"
  },
  {
    "text": "looks right for train the model from",
    "start": "728480",
    "end": "730320"
  },
  {
    "text": "scratch Actually it's like this Much",
    "start": "730320",
    "end": "731519"
  },
  {
    "text": "better Much better If you train your",
    "start": "731519",
    "end": "733360"
  },
  {
    "text": "model from scratch you will probably",
    "start": "733360",
    "end": "735040"
  },
  {
    "text": "need a lot of data set a lot of data to",
    "start": "735040",
    "end": "737600"
  },
  {
    "text": "train your model and also a lot of GPUs",
    "start": "737600",
    "end": "739920"
  },
  {
    "text": "and that can get expensive as well So",
    "start": "739920",
    "end": "742639"
  },
  {
    "text": "usually we don't see so many companies",
    "start": "742639",
    "end": "744720"
  },
  {
    "text": "training their model from scratch We see",
    "start": "744720",
    "end": "746800"
  },
  {
    "text": "more really fine-tuning those models Of",
    "start": "746800",
    "end": "749279"
  },
  {
    "text": "course you can train it from scratch",
    "start": "749279",
    "end": "750720"
  },
  {
    "text": "There are some companies that focus on",
    "start": "750720",
    "end": "752320"
  },
  {
    "text": "this but it's not a good use case for",
    "start": "752320",
    "end": "754800"
  },
  {
    "text": "all cases that we see around So now that",
    "start": "754800",
    "end": "758160"
  },
  {
    "text": "we understand how this works let's dive",
    "start": "758160",
    "end": "760160"
  },
  {
    "text": "deeper on how we use rack for our use",
    "start": "760160",
    "end": "762720"
  },
  {
    "text": "case here So let's consider that",
    "start": "762720",
    "end": "764639"
  },
  {
    "text": "architecture that Lucas just showed to",
    "start": "764639",
    "end": "766320"
  },
  {
    "text": "us Let us now focus on the other part of",
    "start": "766320",
    "end": "768880"
  },
  {
    "text": "it This chatbot at the end of the",
    "start": "768880",
    "end": "771040"
  },
  {
    "text": "application the bottom of the",
    "start": "771040",
    "end": "772839"
  },
  {
    "text": "architecture So considering this chatbot",
    "start": "772839",
    "end": "775680"
  },
  {
    "text": "we have here our user that is trying to",
    "start": "775680",
    "end": "777680"
  },
  {
    "text": "troubleshoot an issue see what is",
    "start": "777680",
    "end": "779360"
  },
  {
    "text": "happening for our application and we",
    "start": "779360",
    "end": "781440"
  },
  {
    "text": "have that open search database that",
    "start": "781440",
    "end": "782880"
  },
  {
    "text": "Lucas just showed to us How do we",
    "start": "782880",
    "end": "784800"
  },
  {
    "text": "connect those dots so the user will send",
    "start": "784800",
    "end": "787519"
  },
  {
    "text": "a query for example why is is my",
    "start": "787519",
    "end": "789680"
  },
  {
    "text": "application my payments application not",
    "start": "789680",
    "end": "791480"
  },
  {
    "text": "working We have to embed that question",
    "start": "791480",
    "end": "794399"
  },
  {
    "text": "using the same model that was used to",
    "start": "794399",
    "end": "796480"
  },
  {
    "text": "embed the logs That's very important If",
    "start": "796480",
    "end": "798399"
  },
  {
    "text": "you use another model your query your",
    "start": "798399",
    "end": "800720"
  },
  {
    "text": "search for the context will not work So",
    "start": "800720",
    "end": "802800"
  },
  {
    "text": "use the same model It doesn't have to be",
    "start": "802800",
    "end": "804480"
  },
  {
    "text": "necessary this model but use the same",
    "start": "804480",
    "end": "806320"
  },
  {
    "text": "model for embedding the logs and the",
    "start": "806320",
    "end": "808399"
  },
  {
    "text": "query itself as well And then you have",
    "start": "808399",
    "end": "810959"
  },
  {
    "text": "this embedding a vector of numbers that",
    "start": "810959",
    "end": "813279"
  },
  {
    "text": "you can use to search on open search",
    "start": "813279",
    "end": "815680"
  },
  {
    "text": "That search on open search will return",
    "start": "815680",
    "end": "817440"
  },
  {
    "text": "to you the context Again remember that",
    "start": "817440",
    "end": "819279"
  },
  {
    "text": "Lucas mentioned we cannot uh get back",
    "start": "819279",
    "end": "821839"
  },
  {
    "text": "from the embedding to the log itself So",
    "start": "821839",
    "end": "823839"
  },
  {
    "text": "we just use the embedding to search in",
    "start": "823839",
    "end": "825760"
  },
  {
    "text": "the database and then we return the log",
    "start": "825760",
    "end": "828399"
  },
  {
    "text": "itself the text of the log message",
    "start": "828399",
    "end": "830240"
  },
  {
    "text": "itself And that can be used for our",
    "start": "830240",
    "end": "832880"
  },
  {
    "text": "augmented prompt So we have now the",
    "start": "832880",
    "end": "835200"
  },
  {
    "text": "context of the application the logs of",
    "start": "835200",
    "end": "837360"
  },
  {
    "text": "the application We have the question",
    "start": "837360",
    "end": "838959"
  },
  {
    "text": "from the user for example",
    "start": "838959",
    "end": "840160"
  },
  {
    "text": "troubleshooting a specific issue And we",
    "start": "840160",
    "end": "842480"
  },
  {
    "text": "can pass that to a model It could be for",
    "start": "842480",
    "end": "844720"
  },
  {
    "text": "example on bedrock It could be in our",
    "start": "844720",
    "end": "846959"
  },
  {
    "text": "case in this architecture we have deepse",
    "start": "846959",
    "end": "848800"
  },
  {
    "text": "running inside of the Kubernetes cluster",
    "start": "848800",
    "end": "850639"
  },
  {
    "text": "as well that you can use to troubleshoot",
    "start": "850639",
    "end": "852800"
  },
  {
    "text": "this issue But Lucas I think we can make",
    "start": "852800",
    "end": "855360"
  },
  {
    "text": "this more smarter How can we improve",
    "start": "855360",
    "end": "857440"
  },
  {
    "text": "that model or that workflow let's talk",
    "start": "857440",
    "end": "859279"
  },
  {
    "text": "about a gentic workflow since it's a",
    "start": "859279",
    "end": "861279"
  },
  {
    "text": "term that a lot of people are talking",
    "start": "861279",
    "end": "862639"
  },
  {
    "text": "about right now So with this",
    "start": "862639",
    "end": "865600"
  },
  {
    "text": "architecture that we have could be good",
    "start": "865600",
    "end": "867519"
  },
  {
    "text": "enough maybe for 30% or 40% of the",
    "start": "867519",
    "end": "870320"
  },
  {
    "text": "problems that we see If we can empower",
    "start": "870320",
    "end": "872720"
  },
  {
    "text": "the networking operation center",
    "start": "872720",
    "end": "874079"
  },
  {
    "text": "engineers at least with the minimum that",
    "start": "874079",
    "end": "875760"
  },
  {
    "text": "they can give to the on call support",
    "start": "875760",
    "end": "877279"
  },
  {
    "text": "engineer to fix the issue that's already",
    "start": "877279",
    "end": "879519"
  },
  {
    "text": "good enough for me But we went one step",
    "start": "879519",
    "end": "881600"
  },
  {
    "text": "fur further in this architecture We are",
    "start": "881600",
    "end": "883839"
  },
  {
    "text": "calling that first response intermediate",
    "start": "883839",
    "end": "885920"
  },
  {
    "text": "response and we have a prompt",
    "start": "885920",
    "end": "887720"
  },
  {
    "text": "engineering to that first prompt where",
    "start": "887720",
    "end": "890320"
  },
  {
    "text": "we ask the model to generate cubectl",
    "start": "890320",
    "end": "892720"
  },
  {
    "text": "commands and it knows which comments to",
    "start": "892720",
    "end": "894880"
  },
  {
    "text": "generate because we have context from",
    "start": "894880",
    "end": "896880"
  },
  {
    "text": "open search that were kubernetes events",
    "start": "896880",
    "end": "899199"
  },
  {
    "text": "application logs as you saw that",
    "start": "899199",
    "end": "901959"
  },
  {
    "text": "intermediate response will generate",
    "start": "901959",
    "end": "903839"
  },
  {
    "text": "cubectl commands we're going to parse",
    "start": "903839",
    "end": "905920"
  },
  {
    "text": "those cubectl commands using reax",
    "start": "905920",
    "end": "908000"
  },
  {
    "text": "because on the prompt engineer we have",
    "start": "908000",
    "end": "909680"
  },
  {
    "text": "been very specific on the way that we",
    "start": "909680",
    "end": "912160"
  },
  {
    "text": "want the model to generate those",
    "start": "912160",
    "end": "913600"
  },
  {
    "text": "commands Once we parse those commands",
    "start": "913600",
    "end": "915600"
  },
  {
    "text": "we're going to execute those commands on",
    "start": "915600",
    "end": "917040"
  },
  {
    "text": "the cubernetes cluster And then once the",
    "start": "917040",
    "end": "919120"
  },
  {
    "text": "commands are executed we're going to",
    "start": "919120",
    "end": "920639"
  },
  {
    "text": "generate a new prompt to send that back",
    "start": "920639",
    "end": "922800"
  },
  {
    "text": "to the model And that new prompt will",
    "start": "922800",
    "end": "924880"
  },
  {
    "text": "contain what cubectl output plus the",
    "start": "924880",
    "end": "927959"
  },
  {
    "text": "intermedatri intermediate response It's",
    "start": "927959",
    "end": "930639"
  },
  {
    "text": "pretty hard to say some words when you",
    "start": "930639",
    "end": "932000"
  },
  {
    "text": "speak Portuguese Intermediate response",
    "start": "932000",
    "end": "934320"
  },
  {
    "text": "And then the first prompt we have is",
    "start": "934320",
    "end": "936480"
  },
  {
    "text": "basically the augmented prompt the first",
    "start": "936480",
    "end": "938160"
  },
  {
    "text": "query that our user initially has sent",
    "start": "938160",
    "end": "941680"
  },
  {
    "text": "to the model Once it's done we pass that",
    "start": "941680",
    "end": "944480"
  },
  {
    "text": "out back to the models that we choose",
    "start": "944480",
    "end": "946160"
  },
  {
    "text": "and again we can use in that",
    "start": "946160",
    "end": "948320"
  },
  {
    "text": "architecture We have claude on bedrock",
    "start": "948320",
    "end": "950800"
  },
  {
    "text": "deploy it and we have deepse running on",
    "start": "950800",
    "end": "952560"
  },
  {
    "text": "VLM on top of Kubernetes as well We pass",
    "start": "952560",
    "end": "955199"
  },
  {
    "text": "that back to the model and then we have",
    "start": "955199",
    "end": "957360"
  },
  {
    "text": "a final response where basically the",
    "start": "957360",
    "end": "960639"
  },
  {
    "text": "hook that you saw in the beginning of",
    "start": "960639",
    "end": "962079"
  },
  {
    "text": "the presentation So that entire workflow",
    "start": "962079",
    "end": "964639"
  },
  {
    "text": "have ran and then we got the final",
    "start": "964639",
    "end": "966959"
  },
  {
    "text": "response after that workflow has run One",
    "start": "966959",
    "end": "969519"
  },
  {
    "text": "thing that I want to add here before we",
    "start": "969519",
    "end": "971839"
  },
  {
    "text": "go to the demo is basically we could",
    "start": "971839",
    "end": "974560"
  },
  {
    "text": "iterate on top of that workflow and ask",
    "start": "974560",
    "end": "977360"
  },
  {
    "text": "our model to generate more commands and",
    "start": "977360",
    "end": "979360"
  },
  {
    "text": "be more specific into the",
    "start": "979360",
    "end": "980959"
  },
  {
    "text": "troubleshooting and that will get more",
    "start": "980959",
    "end": "982639"
  },
  {
    "text": "clear during the demonstration as well",
    "start": "982639",
    "end": "984480"
  },
  {
    "text": "Exactly how that would work Yeah So we",
    "start": "984480",
    "end": "986639"
  },
  {
    "text": "could do that and then once we are in",
    "start": "986639",
    "end": "988480"
  },
  {
    "text": "the code I can show that to you folks",
    "start": "988480",
    "end": "990480"
  },
  {
    "text": "But now with fraud it's demo time This",
    "start": "990480",
    "end": "993279"
  },
  {
    "text": "QR code right here is to the GitHub",
    "start": "993279",
    "end": "995360"
  },
  {
    "text": "repository where we have the full",
    "start": "995360",
    "end": "996800"
  },
  {
    "text": "implementation open source It's all",
    "start": "996800",
    "end": "999360"
  },
  {
    "text": "terraform So you can do a Terraform",
    "start": "999360",
    "end": "1001120"
  },
  {
    "text": "apply It's going to deploy in your",
    "start": "1001120",
    "end": "1002240"
  },
  {
    "text": "environment You can try it out We are",
    "start": "1002240",
    "end": "1004399"
  },
  {
    "text": "not developers So please we're accepting",
    "start": "1004399",
    "end": "1006320"
  },
  {
    "text": "PRs or DevOps engineers And then I think",
    "start": "1006320",
    "end": "1008800"
  },
  {
    "text": "now it's demo time We're going to show",
    "start": "1008800",
    "end": "1010639"
  },
  {
    "text": "the QR code after the demonstration as",
    "start": "1010639",
    "end": "1012959"
  },
  {
    "text": "well So Thiago take us away Let's go to",
    "start": "1012959",
    "end": "1016079"
  },
  {
    "text": "the demonstration",
    "start": "1016079",
    "end": "1018600"
  },
  {
    "text": "So what we can do here in this chatbot",
    "start": "1018600",
    "end": "1021120"
  },
  {
    "text": "is a pretty pretty simple interface You",
    "start": "1021120",
    "end": "1023360"
  },
  {
    "text": "just select the date and it's important",
    "start": "1023360",
    "end": "1025120"
  },
  {
    "text": "to notice that that date actually",
    "start": "1025120",
    "end": "1027120"
  },
  {
    "text": "defines which index we are using in open",
    "start": "1027120",
    "end": "1029438"
  },
  {
    "text": "search to search for the logs So we're",
    "start": "1029439",
    "end": "1031438"
  },
  {
    "text": "indexing by date but you could index",
    "start": "1031439",
    "end": "1033360"
  },
  {
    "text": "that by hour or by any time frame that",
    "start": "1033360",
    "end": "1035520"
  },
  {
    "text": "makes more sense to your use case We",
    "start": "1035520",
    "end": "1037678"
  },
  {
    "text": "don't have a lot of logs being generated",
    "start": "1037679",
    "end": "1039760"
  },
  {
    "text": "here So indexing by day worked well for",
    "start": "1039760",
    "end": "1042400"
  },
  {
    "text": "us So here we can send a question We",
    "start": "1042400",
    "end": "1044880"
  },
  {
    "text": "also select the model So let's ask why",
    "start": "1044880",
    "end": "1047199"
  },
  {
    "text": "the payment service app is not working",
    "start": "1047199",
    "end": "1049280"
  },
  {
    "text": "and let's see what it",
    "start": "1049280",
    "end": "1050840"
  },
  {
    "text": "answers So after we submit here if we",
    "start": "1050840",
    "end": "1053600"
  },
  {
    "text": "look into the logs of the application we",
    "start": "1053600",
    "end": "1055520"
  },
  {
    "text": "will see the user input So why is why is",
    "start": "1055520",
    "end": "1058160"
  },
  {
    "text": "the payment service not working we can",
    "start": "1058160",
    "end": "1060160"
  },
  {
    "text": "see the context that was found in open",
    "start": "1060160",
    "end": "1062080"
  },
  {
    "text": "search And here we can see that it found",
    "start": "1062080",
    "end": "1064320"
  },
  {
    "text": "a specific pod name and also in which",
    "start": "1064320",
    "end": "1066320"
  },
  {
    "text": "name space that pod is running Looking",
    "start": "1066320",
    "end": "1068480"
  },
  {
    "text": "into the initial response we can see the",
    "start": "1068480",
    "end": "1070240"
  },
  {
    "text": "cubectl commands that were generated to",
    "start": "1070240",
    "end": "1072400"
  },
  {
    "text": "further troubleshoot the issue And if",
    "start": "1072400",
    "end": "1074559"
  },
  {
    "text": "you look into details here we will see",
    "start": "1074559",
    "end": "1076320"
  },
  {
    "text": "that we have that specific pod name and",
    "start": "1076320",
    "end": "1078320"
  },
  {
    "text": "the name space that we did not pass in",
    "start": "1078320",
    "end": "1081200"
  },
  {
    "text": "to the initial request and that was",
    "start": "1081200",
    "end": "1084000"
  },
  {
    "text": "generated based on the initial context",
    "start": "1084000",
    "end": "1085919"
  },
  {
    "text": "that we have from open search from the",
    "start": "1085919",
    "end": "1087200"
  },
  {
    "text": "logs of the application itself",
    "start": "1087200",
    "end": "1089200"
  },
  {
    "text": "Afterwards we parse those commands and",
    "start": "1089200",
    "end": "1091200"
  },
  {
    "text": "we have a list of the cubectl commands",
    "start": "1091200",
    "end": "1093200"
  },
  {
    "text": "that have to be executed We run those",
    "start": "1093200",
    "end": "1095440"
  },
  {
    "text": "commands against the cluster And it's",
    "start": "1095440",
    "end": "1097760"
  },
  {
    "text": "very important here You might be asking",
    "start": "1097760",
    "end": "1099520"
  },
  {
    "text": "yourself already this is no right access",
    "start": "1099520",
    "end": "1102559"
  },
  {
    "text": "We just have a service account with read",
    "start": "1102559",
    "end": "1104640"
  },
  {
    "text": "only access So just get pods describe",
    "start": "1104640",
    "end": "1107360"
  },
  {
    "text": "pots and also no secrets access so that",
    "start": "1107360",
    "end": "1110080"
  },
  {
    "text": "you don't have access to the secrets You",
    "start": "1110080",
    "end": "1111919"
  },
  {
    "text": "could for sure also tie this service",
    "start": "1111919",
    "end": "1114240"
  },
  {
    "text": "account more down to your case as well",
    "start": "1114240",
    "end": "1118160"
  },
  {
    "text": "So we run those those commands and then",
    "start": "1118160",
    "end": "1120480"
  },
  {
    "text": "we use that again as context and pass",
    "start": "1120480",
    "end": "1122720"
  },
  {
    "text": "that to the model having both the logs",
    "start": "1122720",
    "end": "1125840"
  },
  {
    "text": "the user input and the output of the",
    "start": "1125840",
    "end": "1128000"
  },
  {
    "text": "cubectl commands and we get an answer",
    "start": "1128000",
    "end": "1130000"
  },
  {
    "text": "back in this case here it's saying that",
    "start": "1130000",
    "end": "1132080"
  },
  {
    "text": "this part is an appending of image",
    "start": "1132080",
    "end": "1134559"
  },
  {
    "text": "pullback currently that the repository",
    "start": "1134559",
    "end": "1136720"
  },
  {
    "text": "probably does not exist and here are a",
    "start": "1136720",
    "end": "1139200"
  },
  {
    "text": "few options that you should look into to",
    "start": "1139200",
    "end": "1141280"
  },
  {
    "text": "troubleshoot this issue So check if the",
    "start": "1141280",
    "end": "1143360"
  },
  {
    "text": "image name exists check if the",
    "start": "1143360",
    "end": "1145039"
  },
  {
    "text": "repository exists check if have",
    "start": "1145039",
    "end": "1146960"
  },
  {
    "text": "permissions to that repository and also",
    "start": "1146960",
    "end": "1148960"
  },
  {
    "text": "if there is maybe an issue with",
    "start": "1148960",
    "end": "1150520"
  },
  {
    "text": "connectivity Let's now change the model",
    "start": "1150520",
    "end": "1152720"
  },
  {
    "text": "to DeepSeek just to see how this changes",
    "start": "1152720",
    "end": "1155840"
  },
  {
    "text": "because DeepS has this reasoning behind",
    "start": "1155840",
    "end": "1158640"
  },
  {
    "text": "its training and that gets very clear",
    "start": "1158640",
    "end": "1160799"
  },
  {
    "text": "when you send a request to Deepsec and",
    "start": "1160799",
    "end": "1162640"
  },
  {
    "text": "can see the answer that it generates So",
    "start": "1162640",
    "end": "1165679"
  },
  {
    "text": "you can see that it starts to break the",
    "start": "1165679",
    "end": "1167600"
  },
  {
    "text": "problem really into pieces and explains",
    "start": "1167600",
    "end": "1170480"
  },
  {
    "text": "how it's answering or how it's",
    "start": "1170480",
    "end": "1172240"
  },
  {
    "text": "generating the troubleshooting for that",
    "start": "1172240",
    "end": "1174000"
  },
  {
    "text": "problem In this case here it's saying",
    "start": "1174000",
    "end": "1175679"
  },
  {
    "text": "that probably that image does not exist",
    "start": "1175679",
    "end": "1177760"
  },
  {
    "text": "And you can see the name of the",
    "start": "1177760",
    "end": "1178960"
  },
  {
    "text": "repository is non-existent So it's",
    "start": "1178960",
    "end": "1180559"
  },
  {
    "text": "pretty obvious what is the issue in this",
    "start": "1180559",
    "end": "1182080"
  },
  {
    "text": "case here And it also says how to",
    "start": "1182080",
    "end": "1184640"
  },
  {
    "text": "troubleshoot that issue So it says here",
    "start": "1184640",
    "end": "1186880"
  },
  {
    "text": "for you to further troubleshoot You",
    "start": "1186880",
    "end": "1188480"
  },
  {
    "text": "could do for example a docker pull to",
    "start": "1188480",
    "end": "1190320"
  },
  {
    "text": "that repository to see if that image",
    "start": "1190320",
    "end": "1192640"
  },
  {
    "text": "really exists or not If it does not",
    "start": "1192640",
    "end": "1194720"
  },
  {
    "text": "exist you will probably get more",
    "start": "1194720",
    "end": "1196240"
  },
  {
    "text": "information on why it's not uh being",
    "start": "1196240",
    "end": "1198960"
  },
  {
    "text": "able to pull that image So this is just",
    "start": "1198960",
    "end": "1201039"
  },
  {
    "text": "a short demonstration on how it works",
    "start": "1201039",
    "end": "1203039"
  },
  {
    "text": "Again it's a pretty simple architecture",
    "start": "1203039",
    "end": "1204960"
  },
  {
    "text": "We just wanted to show you how to really",
    "start": "1204960",
    "end": "1206960"
  },
  {
    "text": "use rack to get this these insights and",
    "start": "1206960",
    "end": "1209840"
  },
  {
    "text": "troubleshoot issues with your",
    "start": "1209840",
    "end": "1211200"
  },
  {
    "text": "application Lucas can you take us over",
    "start": "1211200",
    "end": "1213360"
  },
  {
    "text": "now to how the code looks like yeah",
    "start": "1213360",
    "end": "1215520"
  },
  {
    "text": "let's take a look And again just a note",
    "start": "1215520",
    "end": "1217600"
  },
  {
    "text": "we are not developers",
    "start": "1217600",
    "end": "1219520"
  },
  {
    "text": "Yeah let's take a little look at the",
    "start": "1219520",
    "end": "1220960"
  },
  {
    "text": "code I just want to say that better uh",
    "start": "1220960",
    "end": "1224080"
  },
  {
    "text": "I'm not seeing the other screen some",
    "start": "1224080",
    "end": "1227679"
  },
  {
    "text": "reason why Lucas is going to the code",
    "start": "1227679",
    "end": "1230880"
  },
  {
    "text": "that part that Lucas mentioned that we",
    "start": "1230880",
    "end": "1232799"
  },
  {
    "text": "could for example iterate more we just",
    "start": "1232799",
    "end": "1235360"
  },
  {
    "text": "could get that output from the model",
    "start": "1235360",
    "end": "1237280"
  },
  {
    "text": "itself that we saw on the screen and ask",
    "start": "1237280",
    "end": "1240159"
  },
  {
    "text": "further questions there in the chat and",
    "start": "1240159",
    "end": "1242000"
  },
  {
    "text": "pass that as context to the next",
    "start": "1242000",
    "end": "1243840"
  },
  {
    "text": "question So let's say for example you",
    "start": "1243840",
    "end": "1246159"
  },
  {
    "text": "said for me that it could be a network",
    "start": "1246159",
    "end": "1247600"
  },
  {
    "text": "issue it could be a credential issue So",
    "start": "1247600",
    "end": "1249440"
  },
  {
    "text": "give me the comments to test that route",
    "start": "1249440",
    "end": "1251440"
  },
  {
    "text": "or what should I do to test if that's",
    "start": "1251440",
    "end": "1253520"
  },
  {
    "text": "really the issue so we can improve this",
    "start": "1253520",
    "end": "1255840"
  },
  {
    "text": "And another thing is as we saw in the",
    "start": "1255840",
    "end": "1257679"
  },
  {
    "text": "last presentation the better log quality",
    "start": "1257679",
    "end": "1260640"
  },
  {
    "text": "that we have better rag we're going to",
    "start": "1260640",
    "end": "1263120"
  },
  {
    "text": "have to grab the context and better",
    "start": "1263120",
    "end": "1264799"
  },
  {
    "text": "troubleshoot we're going to have So log",
    "start": "1264799",
    "end": "1267039"
  },
  {
    "text": "quality is something that's pretty",
    "start": "1267039",
    "end": "1268400"
  },
  {
    "text": "important It's being discussing",
    "start": "1268400",
    "end": "1269679"
  },
  {
    "text": "everywhere and now we can use GI for a",
    "start": "1269679",
    "end": "1271520"
  },
  {
    "text": "little help I'm going to go not so I'm",
    "start": "1271520",
    "end": "1273760"
  },
  {
    "text": "going to go pretty quick on this",
    "start": "1273760",
    "end": "1274799"
  },
  {
    "text": "demonstration We're going to show the QR",
    "start": "1274799",
    "end": "1276159"
  },
  {
    "text": "code for the repository at the end as",
    "start": "1276159",
    "end": "1277600"
  },
  {
    "text": "well and you can feel free to try it out",
    "start": "1277600",
    "end": "1279679"
  },
  {
    "text": "Open PR reach us out on LinkedIn But",
    "start": "1279679",
    "end": "1281840"
  },
  {
    "text": "let's take a look into the code right",
    "start": "1281840",
    "end": "1283280"
  },
  {
    "text": "now As Thiago said we have the index",
    "start": "1283280",
    "end": "1285440"
  },
  {
    "text": "name and we are searching and we're",
    "start": "1285440",
    "end": "1286799"
  },
  {
    "text": "creating one index per day We could have",
    "start": "1286799",
    "end": "1289200"
  },
  {
    "text": "one index per cluster per day We could",
    "start": "1289200",
    "end": "1291520"
  },
  {
    "text": "have multiple clusters in the same",
    "start": "1291520",
    "end": "1293360"
  },
  {
    "text": "vector database And we could use the",
    "start": "1293360",
    "end": "1295600"
  },
  {
    "text": "same chatbot to search across all the",
    "start": "1295600",
    "end": "1297520"
  },
  {
    "text": "clusters if we want In this case it's",
    "start": "1297520",
    "end": "1299919"
  },
  {
    "text": "just a single cluster So the first thing",
    "start": "1299919",
    "end": "1301679"
  },
  {
    "text": "that we need to do is embed the queries",
    "start": "1301679",
    "end": "1303840"
  },
  {
    "text": "and as you can see here we are using a",
    "start": "1303840",
    "end": "1305600"
  },
  {
    "text": "function called encode query and that",
    "start": "1305600",
    "end": "1307760"
  },
  {
    "text": "function it's basically calling Amazon",
    "start": "1307760",
    "end": "1309679"
  },
  {
    "text": "bedrock using bolto tree to embed the",
    "start": "1309679",
    "end": "1313520"
  },
  {
    "text": "context using Amazon titan and then we",
    "start": "1313520",
    "end": "1316240"
  },
  {
    "text": "return the embeddings to our main",
    "start": "1316240",
    "end": "1318200"
  },
  {
    "text": "application and as you see I was yeah",
    "start": "1318200",
    "end": "1321760"
  },
  {
    "text": "that happens cool so once we have the",
    "start": "1321760",
    "end": "1324000"
  },
  {
    "text": "encoded query what we do is we have",
    "start": "1324000",
    "end": "1326159"
  },
  {
    "text": "created a class called open search",
    "start": "1326159",
    "end": "1327840"
  },
  {
    "text": "client where we have a method called",
    "start": "1327840",
    "end": "1329440"
  },
  {
    "text": "retrieve documents",
    "start": "1329440",
    "end": "1330640"
  },
  {
    "text": "that method retrieve documents We",
    "start": "1330640",
    "end": "1332799"
  },
  {
    "text": "receive the embedded query the index",
    "start": "1332799",
    "end": "1335039"
  },
  {
    "text": "name that we want to search that we",
    "start": "1335039",
    "end": "1336720"
  },
  {
    "text": "built on the beginning of the",
    "start": "1336720",
    "end": "1338400"
  },
  {
    "text": "application when we first initialize it",
    "start": "1338400",
    "end": "1340960"
  },
  {
    "text": "And we're are passing two parameters",
    "start": "1340960",
    "end": "1342640"
  },
  {
    "text": "here that are pretty important The top K",
    "start": "1342640",
    "end": "1344960"
  },
  {
    "text": "we're going to give us the amount of",
    "start": "1344960",
    "end": "1346320"
  },
  {
    "text": "context the amount of logs that we want",
    "start": "1346320",
    "end": "1348480"
  },
  {
    "text": "open search and the minimum score which",
    "start": "1348480",
    "end": "1350960"
  },
  {
    "text": "is basically the minimum similarity",
    "start": "1350960",
    "end": "1352559"
  },
  {
    "text": "score Bigger bigger it is more similar",
    "start": "1352559",
    "end": "1355280"
  },
  {
    "text": "the logs needs to be And why is it",
    "start": "1355280",
    "end": "1357280"
  },
  {
    "text": "important to have this limitation of",
    "start": "1357280",
    "end": "1359120"
  },
  {
    "text": "amount of logs that return that's a",
    "start": "1359120",
    "end": "1360880"
  },
  {
    "text": "really good question It's important to",
    "start": "1360880",
    "end": "1362240"
  },
  {
    "text": "have log quality and limitation on the",
    "start": "1362240",
    "end": "1364159"
  },
  {
    "text": "amount of logs because we have something",
    "start": "1364159",
    "end": "1366080"
  },
  {
    "text": "in the models called context window So",
    "start": "1366080",
    "end": "1369039"
  },
  {
    "text": "depending on the model that we pick the",
    "start": "1369039",
    "end": "1370880"
  },
  {
    "text": "context window could be smaller So it's",
    "start": "1370880",
    "end": "1373760"
  },
  {
    "text": "better for us to understand what we are",
    "start": "1373760",
    "end": "1376240"
  },
  {
    "text": "sending to the model before we explode",
    "start": "1376240",
    "end": "1378559"
  },
  {
    "text": "the context window So that's a way to",
    "start": "1378559",
    "end": "1381039"
  },
  {
    "text": "avoid that problem to happen So after we",
    "start": "1381039",
    "end": "1383440"
  },
  {
    "text": "search the documents on open search",
    "start": "1383440",
    "end": "1385200"
  },
  {
    "text": "we're going to retrieve a list with",
    "start": "1385200",
    "end": "1386480"
  },
  {
    "text": "retrieve documents and then we're going",
    "start": "1386480",
    "end": "1388080"
  },
  {
    "text": "to build our initial prompt And this is",
    "start": "1388080",
    "end": "1390960"
  },
  {
    "text": "what I want to show to you folks That's",
    "start": "1390960",
    "end": "1392640"
  },
  {
    "text": "pretty important We have the retrieve",
    "start": "1392640",
    "end": "1394480"
  },
  {
    "text": "documents and then this is the prompt",
    "start": "1394480",
    "end": "1396480"
  },
  {
    "text": "that I was telling you that we have put",
    "start": "1396480",
    "end": "1398559"
  },
  {
    "text": "in the model to generate cubectl",
    "start": "1398559",
    "end": "1400559"
  },
  {
    "text": "commands So I'm saying to the model here",
    "start": "1400559",
    "end": "1402960"
  },
  {
    "text": "generate cubectl commands whenever is",
    "start": "1402960",
    "end": "1405280"
  },
  {
    "text": "possible under this key So we can use",
    "start": "1405280",
    "end": "1407679"
  },
  {
    "text": "reax to parse and then we can execute",
    "start": "1407679",
    "end": "1409840"
  },
  {
    "text": "those commands And then we create the",
    "start": "1409840",
    "end": "1412240"
  },
  {
    "text": "prompt and that's pretty important The",
    "start": "1412240",
    "end": "1414400"
  },
  {
    "text": "prompt structure it's something that",
    "start": "1414400",
    "end": "1416320"
  },
  {
    "text": "really matters when you are working with",
    "start": "1416320",
    "end": "1417919"
  },
  {
    "text": "jai So being very prescriptive",
    "start": "1417919",
    "end": "1421039"
  },
  {
    "text": "prescriptive again Portuguese",
    "start": "1421039",
    "end": "1422880"
  },
  {
    "text": "prescriptive and having that in a",
    "start": "1422880",
    "end": "1425440"
  },
  {
    "text": "structured way it's start always prompt",
    "start": "1425440",
    "end": "1427760"
  },
  {
    "text": "engineering before you go to fine tuning",
    "start": "1427760",
    "end": "1429679"
  },
  {
    "text": "or something like that because that will",
    "start": "1429679",
    "end": "1431440"
  },
  {
    "text": "be important anyway if you are doing",
    "start": "1431440",
    "end": "1433120"
  },
  {
    "text": "fine tune your rag as well Exactly So",
    "start": "1433120",
    "end": "1435120"
  },
  {
    "text": "then we send that prompt to our uh to",
    "start": "1435120",
    "end": "1438080"
  },
  {
    "text": "our function called generate response",
    "start": "1438080",
    "end": "1440320"
  },
  {
    "text": "with cubectl We pass the prompt and uh",
    "start": "1440320",
    "end": "1443360"
  },
  {
    "text": "and then we pick the two two models that",
    "start": "1443360",
    "end": "1445360"
  },
  {
    "text": "we can pick claude or deepseek Let me",
    "start": "1445360",
    "end": "1447679"
  },
  {
    "text": "just open it up so you can see what we",
    "start": "1447679",
    "end": "1449360"
  },
  {
    "text": "do We parse those commands that were",
    "start": "1449360",
    "end": "1452000"
  },
  {
    "text": "generated by the model We execute those",
    "start": "1452000",
    "end": "1454240"
  },
  {
    "text": "commands We combine the output in that",
    "start": "1454240",
    "end": "1456960"
  },
  {
    "text": "new prompt and we call that a follow-up",
    "start": "1456960",
    "end": "1459039"
  },
  {
    "text": "prompt And here on the follow-up prompt",
    "start": "1459039",
    "end": "1461279"
  },
  {
    "text": "we say to the model please don't",
    "start": "1461279",
    "end": "1463200"
  },
  {
    "text": "generate any new cubectl commands for us",
    "start": "1463200",
    "end": "1465919"
  },
  {
    "text": "We could ask the model to keep",
    "start": "1465919",
    "end": "1467279"
  },
  {
    "text": "generating and we could iterating and",
    "start": "1467279",
    "end": "1468880"
  },
  {
    "text": "keep talking to the chatbot to get to",
    "start": "1468880",
    "end": "1470480"
  },
  {
    "text": "the root cause But that for for that",
    "start": "1470480",
    "end": "1472640"
  },
  {
    "text": "particular use case we are just",
    "start": "1472640",
    "end": "1474480"
  },
  {
    "text": "generating a single command and then",
    "start": "1474480",
    "end": "1476240"
  },
  {
    "text": "returning the final answer So I'm going",
    "start": "1476240",
    "end": "1478480"
  },
  {
    "text": "to go back to the presentation slides",
    "start": "1478480",
    "end": "1481760"
  },
  {
    "text": "and we have some key takeaways here",
    "start": "1481760",
    "end": "1483600"
  },
  {
    "text": "before we finish this session So the",
    "start": "1483600",
    "end": "1485039"
  },
  {
    "text": "first thing is we want to empower the",
    "start": "1485039",
    "end": "1486799"
  },
  {
    "text": "networking operation center engineers",
    "start": "1486799",
    "end": "1488480"
  },
  {
    "text": "because we know that often those folks",
    "start": "1488480",
    "end": "1490159"
  },
  {
    "text": "don't have a lot of Kubernetes deep",
    "start": "1490159",
    "end": "1492320"
  },
  {
    "text": "expertise So if we can teach them using",
    "start": "1492320",
    "end": "1494400"
  },
  {
    "text": "a model with our own context even",
    "start": "1494400",
    "end": "1497880"
  },
  {
    "text": "better but all that to reduce the",
    "start": "1497880",
    "end": "1500640"
  },
  {
    "text": "meantime to repair So the whole idea",
    "start": "1500640",
    "end": "1502640"
  },
  {
    "text": "here is to get your applications",
    "start": "1502640",
    "end": "1504799"
  },
  {
    "text": "troubleshooted as fast as possible and",
    "start": "1504799",
    "end": "1506960"
  },
  {
    "text": "reduce your downtime of the applications",
    "start": "1506960",
    "end": "1509520"
  },
  {
    "text": "And for that we also introduced an",
    "start": "1509520",
    "end": "1511679"
  },
  {
    "text": "agentic workflow because we know agentic",
    "start": "1511679",
    "end": "1513919"
  },
  {
    "text": "AI is being talked all overall right now",
    "start": "1513919",
    "end": "1516400"
  },
  {
    "text": "But the idea here was to bring a real",
    "start": "1516400",
    "end": "1518400"
  },
  {
    "text": "use case that you can really re leverage",
    "start": "1518400",
    "end": "1521440"
  },
  {
    "text": "agentic workflows to get further",
    "start": "1521440",
    "end": "1523919"
  },
  {
    "text": "troubleshooting get more information",
    "start": "1523919",
    "end": "1525440"
  },
  {
    "text": "about the issue to troubleshoot your",
    "start": "1525440",
    "end": "1527279"
  },
  {
    "text": "Kubernetes cluster That's it for today",
    "start": "1527279",
    "end": "1529840"
  },
  {
    "text": "folks We really appreciate our time here",
    "start": "1529840",
    "end": "1531679"
  },
  {
    "text": "with us today That's the QR code for the",
    "start": "1531679",
    "end": "1533760"
  },
  {
    "text": "session survey pretty important to fill",
    "start": "1533760",
    "end": "1535360"
  },
  {
    "text": "that up so we can improve for the",
    "start": "1535360",
    "end": "1536720"
  },
  {
    "text": "follow-up conferences that we're going",
    "start": "1536720",
    "end": "1538159"
  },
  {
    "text": "to present Two QR codes to our LinkedIn",
    "start": "1538159",
    "end": "1540559"
  },
  {
    "text": "as well So if you want to follow us on",
    "start": "1540559",
    "end": "1542320"
  },
  {
    "text": "LinkedIn and talk to us afterwards",
    "start": "1542320",
    "end": "1543760"
  },
  {
    "text": "that's fine And then once again as",
    "start": "1543760",
    "end": "1545520"
  },
  {
    "text": "promised GitHub repository with the full",
    "start": "1545520",
    "end": "1547279"
  },
  {
    "text": "implementation accepting PRs give you",
    "start": "1547279",
    "end": "1549039"
  },
  {
    "text": "stars forks and everything And thank you",
    "start": "1549039",
    "end": "1550880"
  },
  {
    "text": "so much Leave the other leave the other",
    "start": "1550880",
    "end": "1553520"
  },
  {
    "text": "car code a little bit",
    "start": "1553520",
    "end": "1555960"
  },
  {
    "text": "more And folks may I just ask you a",
    "start": "1555960",
    "end": "1559120"
  },
  {
    "text": "quick thing let us take a picture please",
    "start": "1559120",
    "end": "1563279"
  },
  {
    "text": "please",
    "start": "1563279",
    "end": "1566240"
  },
  {
    "text": "Thank you",
    "start": "1566240",
    "end": "1569400"
  },
  {
    "text": "Thank you to our speakers Um we can do",
    "start": "1570000",
    "end": "1574799"
  },
  {
    "text": "one question So uh we have a mic in the",
    "start": "1574799",
    "end": "1578880"
  },
  {
    "text": "middle of the room So uh please come to",
    "start": "1578880",
    "end": "1581279"
  },
  {
    "text": "the mic if you have a question",
    "start": "1581279",
    "end": "1584520"
  },
  {
    "text": "Um and",
    "start": "1584520",
    "end": "1588600"
  },
  {
    "text": "yeah I think we I saw a hand Yeah we",
    "start": "1588600",
    "end": "1592799"
  },
  {
    "text": "have a question Uh sorry Um I like to",
    "start": "1592799",
    "end": "1597200"
  },
  {
    "text": "know I'm not completely sure if I",
    "start": "1597200",
    "end": "1599200"
  },
  {
    "text": "understand it right but um does the logs",
    "start": "1599200",
    "end": "1602240"
  },
  {
    "text": "go to the model and the model is",
    "start": "1602240",
    "end": "1603600"
  },
  {
    "text": "retrained with it and if this is the",
    "start": "1603600",
    "end": "1606039"
  },
  {
    "text": "case uh how often is this done sounds",
    "start": "1606039",
    "end": "1609279"
  },
  {
    "text": "it's Can you hear me Oh awesome So the",
    "start": "1609279",
    "end": "1612559"
  },
  {
    "text": "logs are not sent for the model to",
    "start": "1612559",
    "end": "1614640"
  },
  {
    "text": "fine-tune or to improve the model itself",
    "start": "1614640",
    "end": "1616559"
  },
  {
    "text": "We just use it as rack So we just get",
    "start": "1616559",
    "end": "1618880"
  },
  {
    "text": "the context of the logs because logs",
    "start": "1618880",
    "end": "1621919"
  },
  {
    "text": "they are really like ephemeral You want",
    "start": "1621919",
    "end": "1623760"
  },
  {
    "text": "to have something that is like from the",
    "start": "1623760",
    "end": "1625200"
  },
  {
    "text": "last few minutes or the last few seconds",
    "start": "1625200",
    "end": "1627279"
  },
  {
    "text": "So instead of always fine-tuning the",
    "start": "1627279",
    "end": "1629840"
  },
  {
    "text": "model itself we just get the most recent",
    "start": "1629840",
    "end": "1632240"
  },
  {
    "text": "or the logs that are related to the",
    "start": "1632240",
    "end": "1634320"
  },
  {
    "text": "issue that you want to troubleshoot and",
    "start": "1634320",
    "end": "1636080"
  },
  {
    "text": "we pass that in the request and it's",
    "start": "1636080",
    "end": "1638240"
  },
  {
    "text": "just used as context during that request",
    "start": "1638240",
    "end": "1640159"
  },
  {
    "text": "It's not used for fine-tune the model",
    "start": "1640159",
    "end": "1641760"
  },
  {
    "text": "itself The model does not learn One",
    "start": "1641760",
    "end": "1643279"
  },
  {
    "text": "thing that I want to add is if you have",
    "start": "1643279",
    "end": "1645279"
  },
  {
    "text": "your own troubleshoot guidance your own",
    "start": "1645279",
    "end": "1647360"
  },
  {
    "text": "log structure you might want to",
    "start": "1647360",
    "end": "1649600"
  },
  {
    "text": "fine-tune a particular model into your",
    "start": "1649600",
    "end": "1651919"
  },
  {
    "text": "own use case and then the response would",
    "start": "1651919",
    "end": "1653840"
  },
  {
    "text": "be better But you could keep using the",
    "start": "1653840",
    "end": "1655840"
  },
  {
    "text": "same workflow but instead of doing rag",
    "start": "1655840",
    "end": "1658240"
  },
  {
    "text": "you could even do rag plus the",
    "start": "1658240",
    "end": "1660000"
  },
  {
    "text": "fine-tuned model to get a even better",
    "start": "1660000",
    "end": "1661760"
  },
  {
    "text": "response If you have a documentation for",
    "start": "1661760",
    "end": "1663279"
  },
  {
    "text": "example on how to troubleshoot your own",
    "start": "1663279",
    "end": "1665120"
  },
  {
    "text": "environment Exactly Thank you Awesome",
    "start": "1665120",
    "end": "1667200"
  },
  {
    "text": "Thank you",
    "start": "1667200",
    "end": "1669519"
  }
]