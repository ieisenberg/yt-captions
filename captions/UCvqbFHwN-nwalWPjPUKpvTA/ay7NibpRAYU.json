[
  {
    "text": "good afternoon everyone and welcome to the the last talk in the aberrations",
    "start": "30",
    "end": "5609"
  },
  {
    "text": "tract today it's scaling kubernetes to thousands of nodes across multiple",
    "start": "5609",
    "end": "11490"
  },
  {
    "text": "clusters calmly I hope let's see and please make sure to",
    "start": "11490",
    "end": "17720"
  },
  {
    "text": "move into rows so everybody that's coming in right now can can get a seat as well and also remember to rate the",
    "start": "17720",
    "end": "25350"
  },
  {
    "text": "session after the talk now to the introduction of our next speaker Ben is",
    "start": "25350",
    "end": "31679"
  },
  {
    "text": "a software engineer at a B&B he has worked with data base scaling Ruby and node.js performance an incident response",
    "start": "31679",
    "end": "38730"
  },
  {
    "text": "and of course kubernetes so with that let's give Ben a big round of applause",
    "start": "38730",
    "end": "45559"
  },
  {
    "text": "yeah thank you thank you like you said I'm Ben Hughes and today I'm going to",
    "start": "50780",
    "end": "56460"
  },
  {
    "text": "talk about how we at Airbnb contend it with massive internal adoption of kubernetes that led us to quickly scale",
    "start": "56460",
    "end": "62370"
  },
  {
    "text": "up past single cluster limits I've been at Airbnb for over seven years working on various areas of",
    "start": "62370",
    "end": "68340"
  },
  {
    "text": "infrastructure and scalability so much of what we consider our current legacy infrastructure I often remember when its",
    "start": "68340",
    "end": "75060"
  },
  {
    "text": "predecessor or predecessors predecessor was shiny and new so it's kind of an interesting up bit of a context to have",
    "start": "75060",
    "end": "82470"
  },
  {
    "text": "when designing new things so let's first cover the basics for those of you who aren't familiar with",
    "start": "82470",
    "end": "88020"
  },
  {
    "text": "Airbnb Airbnb is one of the world's largest marketplaces for unique authentic places to stay and things to",
    "start": "88020",
    "end": "94140"
  },
  {
    "text": "do offering over 7 million accommodations and 40,000 handcrafted activities all powered by local hosts",
    "start": "94140",
    "end": "101630"
  },
  {
    "text": "and today I'll talk about how we scaled or kubernetes usage I'll start with how our architecture evolved and got us",
    "start": "101630",
    "end": "108240"
  },
  {
    "text": "using kubernetes we were a migration case rather than greenfield so we're talking about moving existing services",
    "start": "108240",
    "end": "114689"
  },
  {
    "text": "rather than just supporting newly created workloads then we'll check in on what the practical scaling limits for",
    "start": "114689",
    "end": "120540"
  },
  {
    "text": "kubernetes clusters are and how rapidly we were approaching them and these limits combined with our growth led us",
    "start": "120540",
    "end": "127320"
  },
  {
    "text": "to look towards multiple clusters of course simply creating a bunch of clusters isn't really the hard part",
    "start": "127320",
    "end": "133650"
  },
  {
    "text": "like with most things it's maintenance and ensuring consistency that end up being the greater burden we've invested",
    "start": "133650",
    "end": "140730"
  },
  {
    "text": "in tooling and strategies to keep this under control while also integrating with our existing infrastructure so finally I'll talk",
    "start": "140730",
    "end": "148170"
  },
  {
    "text": "about how we try to enable change and evolution within the system any new thing can hopefully account for the use",
    "start": "148170",
    "end": "154170"
  },
  {
    "text": "case at the point in time that it is written but use cases change this is especially important because the",
    "start": "154170",
    "end": "159750"
  },
  {
    "text": "kubernetes ecosystem is complex and fast-moving so whatever structures you have around it have to be able to",
    "start": "159750",
    "end": "165989"
  },
  {
    "text": "account for change plus you're pretty much guaranteed to screw up some seemingly insignificant config that",
    "start": "165989",
    "end": "172140"
  },
  {
    "text": "comes back to haunt you later so how did we get here I think the story",
    "start": "172140",
    "end": "177150"
  },
  {
    "text": "of how our architecture evolved is a pretty common one for a tech company isn't this decade we started with a",
    "start": "177150",
    "end": "183030"
  },
  {
    "text": "single monolithic code base that did absolutely everything and it was a rails app so he called it monorail it turns",
    "start": "183030",
    "end": "190980"
  },
  {
    "text": "out that everyone who has a large rails app at the center of their business calls it monorail I was crestfallen when",
    "start": "190980",
    "end": "196860"
  },
  {
    "text": "I discovered that we weren't so creative and this was this was fine for a while but it became rather unwieldy as the",
    "start": "196860",
    "end": "203430"
  },
  {
    "text": "number of engineers grew it became more and more of a check choke point the single large code base that could do",
    "start": "203430",
    "end": "209310"
  },
  {
    "text": "absolutely everything also made it hard to reason about boundaries monorail was honestly just a huge slog to work with",
    "start": "209310",
    "end": "217010"
  },
  {
    "text": "luckily there's a solution for this services and service-oriented architecture were there to save the day",
    "start": "217010",
    "end": "222840"
  },
  {
    "text": "you know piece of cake guess we can all go home and at first we only had a few",
    "start": "222840",
    "end": "228690"
  },
  {
    "text": "services and they were extracted for specific performance library or isolation needs but then it was an",
    "start": "228690",
    "end": "236519"
  },
  {
    "text": "absolute explosion of services we really took a micro to heart and over the course of a couple of years we added",
    "start": "236519",
    "end": "242790"
  },
  {
    "text": "hundreds of services this had advantages over the monolith and it allowed development to be unblocked but it",
    "start": "242790",
    "end": "249900"
  },
  {
    "text": "wasn't an unqualified win on the developer experience side engineers were now required to touch many many things",
    "start": "249900",
    "end": "256799"
  },
  {
    "text": "to roll out certain application changes things like the actual application code chef configuration of their systems and",
    "start": "256799",
    "end": "263400"
  },
  {
    "text": "this was made way worse because we were managing many secret via chef data bags you know things like",
    "start": "263400",
    "end": "270460"
  },
  {
    "text": "terraformer alerting rules this just was a generally a pile of a pile of pull requests many of them even had to be",
    "start": "270460",
    "end": "277120"
  },
  {
    "text": "deployed in a specific and often non-obvious order so this is the initiative that so this led to an",
    "start": "277120",
    "end": "283330"
  },
  {
    "text": "initiative initiative to reduce the many touches to one touch and this is where kubernetes comes in but also a lot of",
    "start": "283330",
    "end": "291130"
  },
  {
    "text": "other tooling to wrap around around kubernetes we didn't just give people cube cuttle applied a chef and tell them",
    "start": "291130",
    "end": "297220"
  },
  {
    "text": "to have fun with it and kubernetes provides the integration point so that",
    "start": "297220",
    "end": "302560"
  },
  {
    "text": "all of an app's config can live with the app this capability continues to grow as we use more and more custom resources",
    "start": "302560",
    "end": "308950"
  },
  {
    "text": "and custom controllers to manage things beyond just the distribution of containers our goal is to enable all app",
    "start": "308950",
    "end": "315430"
  },
  {
    "text": "and config changes for a service to be a single PR a single code review and a single deploy but hundreds of services",
    "start": "315430",
    "end": "323500"
  },
  {
    "text": "is probably still too many like keep it under control and of course so we started we had stood",
    "start": "323500",
    "end": "331000"
  },
  {
    "text": "up a system to handle all of these services we still needed to get all those services on the system so you",
    "start": "331000",
    "end": "337060"
  },
  {
    "text": "began migrating existing services this was initially an unproven thing so",
    "start": "337060",
    "end": "342400"
  },
  {
    "text": "initial migration was very slow teams just started to dip their toes in we were more we were also more likely to",
    "start": "342400",
    "end": "348640"
  },
  {
    "text": "get smaller deployments of small smaller somewhat less complicated services but",
    "start": "348640",
    "end": "353980"
  },
  {
    "text": "as as this built up momentum and as you know teams got more comfortable with this the migration continued more rapidly and",
    "start": "353980",
    "end": "360820"
  },
  {
    "text": "we started to see larger services coming over and around this time we started to think about questions like what are our",
    "start": "360820",
    "end": "368380"
  },
  {
    "text": "scaling limits and at some point the calculus shifted from how can we get stuff to use our thing - what happens if",
    "start": "368380",
    "end": "375220"
  },
  {
    "text": "all this stuff is using our thing and initially any limits or issues seemed pretty far off so I joined the internal",
    "start": "375220",
    "end": "382720"
  },
  {
    "text": "kubernetes team in September of last year and at the time our main production cluster had about 450 nodes while I was",
    "start": "382720",
    "end": "390520"
  },
  {
    "text": "coming up to speed I had a number of conversations about what sort of projects to work on and someone you know",
    "start": "390520",
    "end": "396700"
  },
  {
    "text": "mentioned that we should start thinking about multi cluster so you know kept it in the back of my mind while",
    "start": "396700",
    "end": "401930"
  },
  {
    "text": "working on other stuff but by December we had doubled to 900 nodes so how big",
    "start": "401930",
    "end": "408469"
  },
  {
    "text": "can we make these clusters again so he turned to the reference documentation there's a hard limit at five thousand",
    "start": "408469",
    "end": "415430"
  },
  {
    "text": "nodes according to the Official Doc's open a I had a great blog post on what they did - what they had to do to get to",
    "start": "415430",
    "end": "422419"
  },
  {
    "text": "a 2500 notes and reading between the lines on that scaling to 2500 nodes is",
    "start": "422419",
    "end": "427939"
  },
  {
    "text": "something worth writing a blog post about and we also have various conversations and generally got the",
    "start": "427939",
    "end": "433639"
  },
  {
    "text": "message that she develop a lot of new problems after about the 2500 to 3,000 node range of course more recently",
    "start": "433639",
    "end": "441159"
  },
  {
    "text": "Alibaba did some really great work and published a blog post about how they were able to get a cluster up to you",
    "start": "441159",
    "end": "447529"
  },
  {
    "text": "about ten thousand nodes a lot of those a lot of those changes are actually like seem to be pretty generally applicable",
    "start": "447529",
    "end": "453319"
  },
  {
    "text": "so I'm really excited for like how this will push forward kubernetes scaling going forward but at the same time that",
    "start": "453319",
    "end": "461930"
  },
  {
    "text": "we were pondering what our theoretical you know future scaling limits might be we also had some sudden more pressing",
    "start": "461930",
    "end": "468650"
  },
  {
    "text": "questions like would it be bad if or bad if our EDD were suddenly getting out of memory killed and it turns out this",
    "start": "468650",
    "end": "476689"
  },
  {
    "text": "question was not hypothetical so as we closed out last year we were having a lot of issues with Etsy D",
    "start": "476689",
    "end": "482750"
  },
  {
    "text": "getting backed up and falling over all it took were small shocks like a lower cash hit rate on the API server or a",
    "start": "482750",
    "end": "489199"
  },
  {
    "text": "flurry of events to cause that CD to get overwhelmed it's really easy to cause a flurry of events in kubernetes and",
    "start": "489199",
    "end": "496939"
  },
  {
    "text": "luckily and luckily you know when des when the control plane failed like this",
    "start": "496939",
    "end": "502639"
  },
  {
    "text": "it just stopped any deploy deploys or scaling activity and process it didn't take down our",
    "start": "502639",
    "end": "507949"
  },
  {
    "text": "workloads so it's not great but it's still better than you know everything falling over and like you know the entire site being down and you know the",
    "start": "507949",
    "end": "515719"
  },
  {
    "text": "good thing is that a lot of these @cd issues were just caused by hitting up against limits in the SE d v2 data format once we upgrade it to the sed",
    "start": "515719",
    "end": "522948"
  },
  {
    "text": "v3 data format and did some other things like adjusting quota back-end bytes up",
    "start": "522949",
    "end": "528199"
  },
  {
    "text": "to 8 gigabytes most of our SED problems went away we also had some issues with scheduler latency",
    "start": "528199",
    "end": "534080"
  },
  {
    "text": "when evaluating like really complicated anti affinity rules but luckily we were able to simplify many of those rules and",
    "start": "534080",
    "end": "540710"
  },
  {
    "text": "also upgrade to newer versions of kubernetes that came with a lot of optimizations so we effectively got a",
    "start": "540710",
    "end": "548120"
  },
  {
    "text": "taste for the problems that that came with scale but we were still rapidly scaling up by not by March we had",
    "start": "548120",
    "end": "554390"
  },
  {
    "text": "doubled again to 1,800 nodes and the team was working hard to ensure capacity",
    "start": "554390",
    "end": "559450"
  },
  {
    "text": "and we needed a plan to cross 2500 or even you know five thousand nodes so we",
    "start": "559450",
    "end": "566210"
  },
  {
    "text": "went with the obvious solution of standing up multiple clusters when you can't make a single cluster any bigger we must we must make more clusters so in",
    "start": "566210",
    "end": "575660"
  },
  {
    "text": "April we had capped out at about 2,300 nodes in our production cluster and we would not not grow the prod cluster",
    "start": "575660",
    "end": "582380"
  },
  {
    "text": "beyond this point we found some substantial improvements in place where we were able to free up some capacity",
    "start": "582380",
    "end": "588470"
  },
  {
    "text": "and we started diverting workloads to newly created clusters in May so",
    "start": "588470",
    "end": "594760"
  },
  {
    "text": "multiple cluster was the solution that we had been considering since I had started on the problem and it's the",
    "start": "594760",
    "end": "600680"
  },
  {
    "text": "solution that we went with and it's been very effective for us and at the same time multi cluster can be a bit of a",
    "start": "600680",
    "end": "606530"
  },
  {
    "text": "loaded concept people mean a lot of different things when they say multi cluster and other decisions or",
    "start": "606530",
    "end": "612140"
  },
  {
    "text": "considerations in your organization can make it easier or harder so when you're",
    "start": "612140",
    "end": "617600"
  },
  {
    "text": "splitting up things up into a whole bunch of clusters you need to think about the specifics of your problem space are there placement restrictions",
    "start": "617600",
    "end": "624470"
  },
  {
    "text": "on your workloads that is do certain services need to be in the same cluster or kept very far apart and this may be",
    "start": "624470",
    "end": "631310"
  },
  {
    "text": "the case if you're using cluster DNS or kubernetes service discovery or doing a",
    "start": "631310",
    "end": "636830"
  },
  {
    "text": "lot of communication over an overlay network and moreover what problems are you trying to solve with multi cluster",
    "start": "636830",
    "end": "642680"
  },
  {
    "text": "in our case we were primarily targeting pure node count and cluster size but if",
    "start": "642680",
    "end": "647870"
  },
  {
    "text": "you're trying to do things like isolate workloads or take advantage of multiple clouds or you know cloud bursts or",
    "start": "647870",
    "end": "653810"
  },
  {
    "text": "anything like that you'll need to make different decisions and there are you know there are efforts in the kubernetes",
    "start": "653810",
    "end": "659960"
  },
  {
    "text": "community that are underway like cube fat or Federation v2 that target kind of they seem to target kind of a superset",
    "start": "659960",
    "end": "666410"
  },
  {
    "text": "of a lot of these concerns a lot of like these sorts of related areas they can and they can potentially",
    "start": "666410",
    "end": "671589"
  },
  {
    "text": "solve a lot of these problems and you know we're excited to see how these efforts continue to develop and you know",
    "start": "671589",
    "end": "678519"
  },
  {
    "text": "I think we'll be able to use a lot of the tooling that comes with this but it's not currently something that we've incorporated so overall with these",
    "start": "678519",
    "end": "687250"
  },
  {
    "text": "considerations we got lucky and we're able to pursue a fairly simple version of Multi cluster and a big thing like",
    "start": "687250",
    "end": "694690"
  },
  {
    "text": "probably the biggest thing that we got lucky on was that the particulars of our service mesh smart stack and you know",
    "start": "694690",
    "end": "701079"
  },
  {
    "text": "our migration case caused us to need first-class communication between our",
    "start": "701079",
    "end": "706750"
  },
  {
    "text": "legacy ec2 and freh and and our new kubernetes services and this avoided",
    "start": "706750",
    "end": "713410"
  },
  {
    "text": "collocation requirements so like you know as services we're getting migrated a request might might come in to ec2 and",
    "start": "713410",
    "end": "721420"
  },
  {
    "text": "then get routed to another service in kubernetes then route it back out to ec2 we even had like services that while",
    "start": "721420",
    "end": "728560"
  },
  {
    "text": "they were migrating we're in kind of a halfway halfway state where they were running half their capacity on ec2 and",
    "start": "728560",
    "end": "735459"
  },
  {
    "text": "half of it on kubernetes and so basically any workload can run on any of",
    "start": "735459",
    "end": "742899"
  },
  {
    "text": "the clusters that we that we stand up basically and our view of clusters sees",
    "start": "742899",
    "end": "748540"
  },
  {
    "text": "them as just giant pools of compute and memory to be allocated as you know we see fit and so while this is and we do",
    "start": "748540",
    "end": "756579"
  },
  {
    "text": "have one restriction just to kind of simplify things and that's that our services run on a single cluster and we",
    "start": "756579",
    "end": "763600"
  },
  {
    "text": "select that cluster at the time that the service is created so at the time like an engineer is you know setting up the",
    "start": "763600",
    "end": "770350"
  },
  {
    "text": "the code structure for their brand-new service they'll you know run a command",
    "start": "770350",
    "end": "775360"
  },
  {
    "text": "and it'll actually bind them to a specific cluster that they will get assigned to and this of course can be",
    "start": "775360",
    "end": "781569"
  },
  {
    "text": "changed later and we have tooling to handle migration between clusters but",
    "start": "781569",
    "end": "787000"
  },
  {
    "text": "the big the big reason we have this restriction and I don't think we'll have it forever is that it prevents us from having to rewrite a lot of tooling or",
    "start": "787000",
    "end": "793660"
  },
  {
    "text": "you know write a bunch of wrappers around things like cube cut all or cube dashboard to a pull in",
    "start": "793660",
    "end": "799730"
  },
  {
    "text": "Poland State from all the places where a service might be running but I think",
    "start": "799730",
    "end": "804749"
  },
  {
    "text": "like the surface mesh is something that's worth like drilling in on a bit more so here's a diagram to try and",
    "start": "804749",
    "end": "812009"
  },
  {
    "text": "illustrate what what we're doing and we run our pod Network on an overlay",
    "start": "812009",
    "end": "817139"
  },
  {
    "text": "network and this is either you know flannel or canal depending on on what which one which cluster we're on and the",
    "start": "817139",
    "end": "824309"
  },
  {
    "text": "overlay network is restricted to the cluster itself so this would prevent so you can't communicate with pods on other",
    "start": "824309",
    "end": "830189"
  },
  {
    "text": "clusters over over this overlay network or you can you can't go from ec2",
    "start": "830189",
    "end": "836459"
  },
  {
    "text": "instances to pods on any cluster over this overlay network directly so to get",
    "start": "836459",
    "end": "842369"
  },
  {
    "text": "around this we and this was completely just for the way that we were doing our",
    "start": "842369",
    "end": "847499"
  },
  {
    "text": "service mash and service discovery we use a lot of node port services running",
    "start": "847499",
    "end": "852660"
  },
  {
    "text": "in local mode so your application would be listening to like port port 80 on",
    "start": "852660",
    "end": "858720"
  },
  {
    "text": "like your pod IP and then the node port service would configure a high number port on the node IP and through the",
    "start": "858720",
    "end": "865319"
  },
  {
    "text": "magic of IP tables the traffic would get routed from the node IP import to the pod IP import and since this is running",
    "start": "865319",
    "end": "871829"
  },
  {
    "text": "on the nodes that are local to the pot or to the node that is local to the pot this gets rid of bounces through a",
    "start": "871829",
    "end": "877199"
  },
  {
    "text": "gateway ingress tier like a elby's or engine X or things of that nature and that avoids us having to you know",
    "start": "877199",
    "end": "883319"
  },
  {
    "text": "constantly be bouncing things around or keep like sort of interests here allocated and you know and working so in",
    "start": "883319",
    "end": "892259"
  },
  {
    "text": "effect we're using node port services to make our pods routable on virtual private cloud subnet addresses which",
    "start": "892259",
    "end": "899579"
  },
  {
    "text": "which is something that can be accomplished in a much more straightforward way by using using VPC",
    "start": "899579",
    "end": "905040"
  },
  {
    "text": "CNI plug-ins like the one from Amazon once from Amazon or lyft and with these pods just get IPS directly from the V PC",
    "start": "905040",
    "end": "912299"
  },
  {
    "text": "subnet pool and you can achieve flat flat network between nodes pods and",
    "start": "912299",
    "end": "918119"
  },
  {
    "text": "random instances this involves this will involves far less IP tables and it's a",
    "start": "918119",
    "end": "924029"
  },
  {
    "text": "it's very appealing so we're strongly considering using one of these going forward and so that's so that's what we",
    "start": "924029",
    "end": "930779"
  },
  {
    "text": "have we don't really care where services are run there's not much to guide their placement and for simplicity sake we",
    "start": "930779",
    "end": "936899"
  },
  {
    "text": "assign a cluster at creation time and we then allow those clusters to grow to a",
    "start": "936899",
    "end": "942149"
  },
  {
    "text": "certain size currently we cap that at about 400 nodes and then we're like we're in ok no more no more services get",
    "start": "942149",
    "end": "949199"
  },
  {
    "text": "assigned here but this still leaves plenty of headroom to allow us to continue scaling up the",
    "start": "949199",
    "end": "954720"
  },
  {
    "text": "cluster as as service needs scale and the general idea is to avoid the hard",
    "start": "954720",
    "end": "960569"
  },
  {
    "text": "problems of large clusters by not letting clusters get large and we do",
    "start": "960569",
    "end": "965939"
  },
  {
    "text": "also help some specialization amongst clusters for things like test development and special applications",
    "start": "965939",
    "end": "971220"
  },
  {
    "text": "like machine learning so let's look into how we manage our clusters so it turns",
    "start": "971220",
    "end": "977100"
  },
  {
    "text": "out when you aim to have a lot of clusters you're going to need to create a lot of clusters and it's not a",
    "start": "977100",
    "end": "982860"
  },
  {
    "text": "one-time task these are clusters that you are now responsible for maintaining and that includes keeping them",
    "start": "982860",
    "end": "988559"
  },
  {
    "text": "consistent so in terms of creating in to some extent managing clusters there are",
    "start": "988559",
    "end": "994319"
  },
  {
    "text": "there are existing projects like cops and cue Batman these are these are great tools and if they work for you that's",
    "start": "994319",
    "end": "1000649"
  },
  {
    "text": "fantastic cops especially is pretty upfront about being opinionated and some of these",
    "start": "1000649",
    "end": "1006499"
  },
  {
    "text": "opinions ended up not working particularly well for us we specifically",
    "start": "1006499",
    "end": "1011600"
  },
  {
    "text": "need it to integrate our kubernetes new nodes with our existing tooling around",
    "start": "1011600",
    "end": "1016819"
  },
  {
    "text": "ec2 instances and we didn't want to hard fork our underlying infrastructure and",
    "start": "1016819",
    "end": "1022160"
  },
  {
    "text": "have to reinvent or relearn everything about managing just you know ec2 instances and the instances should it be",
    "start": "1022160",
    "end": "1029178"
  },
  {
    "text": "completely different just because they happen to be running kubernetes rather than some other service we also wanted",
    "start": "1029179",
    "end": "1034520"
  },
  {
    "text": "to make sure that we solve the full problem as we saw it so what even is a",
    "start": "1034520",
    "end": "1039678"
  },
  {
    "text": "cluster it certainly knows and all those nodes need to be configured packages",
    "start": "1039679",
    "end": "1045438"
  },
  {
    "text": "installed SSH keys you know the works a cluster isn't that much without EDD to",
    "start": "1045439",
    "end": "1050539"
  },
  {
    "text": "store its state and you need to configure all of the options that go to the various components of the control",
    "start": "1050539",
    "end": "1055940"
  },
  {
    "text": "plane you also need to setup things with your cloud provider like I am roles DNS",
    "start": "1055940",
    "end": "1061220"
  },
  {
    "text": "configuration aSG's you know whatever all of that stuff needs to be configured somehow",
    "start": "1061220",
    "end": "1066570"
  },
  {
    "text": "it's can its kubernetes so you need certificates the genius of kubernetes of",
    "start": "1066570",
    "end": "1071700"
  },
  {
    "text": "course is that through it all problems can be turned into either public key infrastructure or networking problems",
    "start": "1071700",
    "end": "1077190"
  },
  {
    "text": "which as we all know are the easiest problems to debug and so for this type",
    "start": "1077190",
    "end": "1083100"
  },
  {
    "text": "of config we generate chef and terraform code into the appropriate existing places but this isn't all that makes a",
    "start": "1083100",
    "end": "1091410"
  },
  {
    "text": "cluster if I were to hand you a cluster with all of these set up it still wouldn't work so much of the behavior of",
    "start": "1091410",
    "end": "1098220"
  },
  {
    "text": "the cluster is dictated by cluster level services that are expressed as fairly normal kubernetes api objects and these",
    "start": "1098220",
    "end": "1104970"
  },
  {
    "text": "include things like CNI dns metrics server and the like and these are the",
    "start": "1104970",
    "end": "1110220"
  },
  {
    "text": "things that show up in your tutorials as cube cuttle applying random llam√≥ from github",
    "start": "1110220",
    "end": "1116000"
  },
  {
    "text": "we don't want clusters to get configured using randomly applied gamal from github",
    "start": "1116000",
    "end": "1122040"
  },
  {
    "text": "this is kind of like when sis admitting involved a lot of ad hoc apt getting onto individual pet servers cluster",
    "start": "1122040",
    "end": "1128940"
  },
  {
    "text": "services are also very much like any other workloads we run and ideally we would deploy them using the same",
    "start": "1128940",
    "end": "1134550"
  },
  {
    "text": "deployment tools we use for everything else and the development process should be familiar to engineers working on any",
    "start": "1134550",
    "end": "1140700"
  },
  {
    "text": "other service we also want to make sure that a cluster is never missing a component and that components stay up to",
    "start": "1140700",
    "end": "1147810"
  },
  {
    "text": "date on all clusters so we solved this by making all cluster level services into a single deployable unit the deploy",
    "start": "1147810",
    "end": "1156360"
  },
  {
    "text": "system rules it out all at once so you don't you know deploy core DNS separately from cube to I am for",
    "start": "1156360",
    "end": "1163770"
  },
  {
    "text": "instance it's all in one package we also deploy to entire tiers of",
    "start": "1163770",
    "end": "1169050"
  },
  {
    "text": "clusters in the same action and this prevents individual clusters from falling behind simply because we forgot",
    "start": "1169050",
    "end": "1175440"
  },
  {
    "text": "to deploy to them we don't yeah we don't want prod one to get all of the all of the latest latest goodness while prod 3",
    "start": "1175440",
    "end": "1182640"
  },
  {
    "text": "just kind of sits there you know month out of date just because no one no one triggered the deploy there and so",
    "start": "1182640",
    "end": "1188960"
  },
  {
    "text": "deciding what goes into this unit is pretty straightforward if this if the",
    "start": "1188960",
    "end": "1194220"
  },
  {
    "text": "service needs to be local to the cluster you know things like daemon sets deployments like Cordy and s-cube dashboard others",
    "start": "1194220",
    "end": "1202100"
  },
  {
    "text": "other things along those lines then it can go here otherwise it's just a regular service and it can go up to",
    "start": "1202100",
    "end": "1207529"
  },
  {
    "text": "whatever single cluster it's assigned to and so we refer to this collection of cube system after the namespace where a",
    "start": "1207529",
    "end": "1213769"
  },
  {
    "text": "lot of these things happen to live so the actual deployment process is a mix of things but it doesn't you know it",
    "start": "1213769",
    "end": "1220879"
  },
  {
    "text": "does leave heavily on our cuber Nettie's much of the core config is structured as helm charts though we mostly use helm as",
    "start": "1220879",
    "end": "1227419"
  },
  {
    "text": "just the template engine and we pull all the dependent the dependency helm charts",
    "start": "1227419",
    "end": "1232669"
  },
  {
    "text": "together using an umbrella chart and template into a single manifest that we then apply then there's a few stages",
    "start": "1232669",
    "end": "1240289"
  },
  {
    "text": "where we you know synchronize iam roles and then we and then we have a number of",
    "start": "1240289",
    "end": "1245840"
  },
  {
    "text": "apps that use a use cube gin which is our internal kubernetes sort of framework this is this tool is primarily",
    "start": "1245840",
    "end": "1252619"
  },
  {
    "text": "around generating manifests but it does also have some imperative deploy logic",
    "start": "1252619",
    "end": "1258710"
  },
  {
    "text": "and things that kind of you know guide the deploy process so we have to handle that separately and since this is a",
    "start": "1258710",
    "end": "1265309"
  },
  {
    "text": "collection of many different things partial failure is always a possibility so we try to make this predictable and",
    "start": "1265309",
    "end": "1271549"
  },
  {
    "text": "so we try and we try to deploy as fully as possible we can't make it fully",
    "start": "1271549",
    "end": "1276950"
  },
  {
    "text": "transactional or atomic so we prefer to continue the deployment even if some unrelated components experience errors",
    "start": "1276950",
    "end": "1284090"
  },
  {
    "text": "rather than bailing out on the first failure this is kind of similar to the approach that terraform applies tend to",
    "start": "1284090",
    "end": "1290359"
  },
  {
    "text": "use okay so we have so we have clusters and we have you know configuration for",
    "start": "1290359",
    "end": "1296239"
  },
  {
    "text": "them and we have the services that run on clusters we need a bit more structure on top of this we want our prod prod",
    "start": "1296239",
    "end": "1302450"
  },
  {
    "text": "style clusters to be alike we also want our you know test or dev or like anything that we have multiple clusters",
    "start": "1302450",
    "end": "1307700"
  },
  {
    "text": "for we want those to be you know kept similar and to share as much config as possible so we introduce the fairly we",
    "start": "1307700",
    "end": "1315859"
  },
  {
    "text": "introduced a layer of cluster types on top of this so you know prod one and prod to share the cluster type of prod",
    "start": "1315859",
    "end": "1323350"
  },
  {
    "text": "so cluster types are like classes wall clusters are like instances of those classes since we're using clusters",
    "start": "1323350",
    "end": "1330080"
  },
  {
    "text": "slowly for horizont scaling clusters of the cluster type should be nearly the same like ideally",
    "start": "1330080",
    "end": "1336720"
  },
  {
    "text": "you know as a user who's deploying their service to one of our clusters you shouldn't really be able to tell what",
    "start": "1336720",
    "end": "1343440"
  },
  {
    "text": "cluster you're on and we really want to avoid people trying to land their services on the good cluster so by",
    "start": "1343440",
    "end": "1350970"
  },
  {
    "text": "trying to make all of our you know clusters the good one and all of this together makes the",
    "start": "1350970",
    "end": "1357480"
  },
  {
    "text": "cluster launch process a matter of using scripts to generate chef terraform and cube system code we then get that code",
    "start": "1357480",
    "end": "1364559"
  },
  {
    "text": "reviewed and merged and then calls the at CD in kubernetes nodes to be to be",
    "start": "1364559",
    "end": "1369899"
  },
  {
    "text": "launched and finally after you know after this we deploy the the cube system artifact which you know for this cluster",
    "start": "1369899",
    "end": "1377130"
  },
  {
    "text": "it will be the initial deploy of that and cause everything to get set up and at the end of that we should have a",
    "start": "1377130",
    "end": "1383159"
  },
  {
    "text": "fully working cluster this entire process takes under an hour and a fair amount of that time is actually just",
    "start": "1383159",
    "end": "1389159"
  },
  {
    "text": "getting a sign off on the changes to the IAM roles and we've stopped we stopped",
    "start": "1389159",
    "end": "1396390"
  },
  {
    "text": "trying to optimize beyond that because it's not we're not doing this so commonly that further optimization would",
    "start": "1396390",
    "end": "1402210"
  },
  {
    "text": "be particularly beneficial so this makes launching clusters relatively easy but",
    "start": "1402210",
    "end": "1409590"
  },
  {
    "text": "the real test for us involves adapting to changing circumstances and accommodating new use",
    "start": "1409590",
    "end": "1415770"
  },
  {
    "text": "cases so you know when we made when we made the change easy we saw that we got",
    "start": "1415770",
    "end": "1422429"
  },
  {
    "text": "more change we reduce barriers and you know things started happening much more rapidly so we quickly we pretty quickly",
    "start": "1422429",
    "end": "1429360"
  },
  {
    "text": "learned some of our initial assumptions were wrong and originally our chef",
    "start": "1429360",
    "end": "1434730"
  },
  {
    "text": "terraform and cube system code generation was one-shot you generated the set of output",
    "start": "1434730",
    "end": "1440370"
  },
  {
    "text": "configuration once at the time of cluster creation and made further modifications in place into the files",
    "start": "1440370",
    "end": "1447390"
  },
  {
    "text": "that you generate that you generated with the generation script if you wanted to change the anything broadly you had",
    "start": "1447390",
    "end": "1454110"
  },
  {
    "text": "to update both the generators so that anything newly generated would get the the new version and then you also had to",
    "start": "1454110",
    "end": "1460230"
  },
  {
    "text": "go back and apply the changes to all the things that had been previously generated and these things might have drifted in",
    "start": "1460230",
    "end": "1467290"
  },
  {
    "text": "the intermediate time as people may apply changes in place and we ended up",
    "start": "1467290",
    "end": "1473650"
  },
  {
    "text": "having to do this a lot more than expected and it was really sort of it",
    "start": "1473650",
    "end": "1481480"
  },
  {
    "text": "was really sort of causing us to like put off implementing new features because we didn't want to figure out how",
    "start": "1481480",
    "end": "1486850"
  },
  {
    "text": "to apply them to existing clusters so this pointed to a continual regeneration",
    "start": "1486850",
    "end": "1492130"
  },
  {
    "text": "process so what we did was we separated out the config into a bunch of high-level config files that serve as",
    "start": "1492130",
    "end": "1498790"
  },
  {
    "text": "inputs and then these feed into the generators and the generators fully regenerate all outputs on every run and",
    "start": "1498790",
    "end": "1505650"
  },
  {
    "text": "so this prevents drift makes it easy to add features and separates out the interesting variable configuration from",
    "start": "1505650",
    "end": "1513370"
  },
  {
    "text": "all of the boilerplate so some of course",
    "start": "1513370",
    "end": "1518560"
  },
  {
    "text": "now that we make it easy to make chain apply change you know to existing clusters some change is relatively easy",
    "start": "1518560",
    "end": "1524320"
  },
  {
    "text": "to apply if you yeah if you update a kubernetes deployment kubernetes will",
    "start": "1524320",
    "end": "1530650"
  },
  {
    "text": "figure out how to make those changes and apply them so you're good to go there but you know even stuff like upgrading",
    "start": "1530650",
    "end": "1535930"
  },
  {
    "text": "kubernetes is fairly straightforward there's an ordering that you have to do you know you have to you upgrade the",
    "start": "1535930",
    "end": "1541300"
  },
  {
    "text": "masters rotate those out upgrade the minion minion nodes rotate those out and",
    "start": "1541300",
    "end": "1546910"
  },
  {
    "text": "but it's a change that's meant to be applied it's probably something you actually want propagated it's a you can",
    "start": "1546910",
    "end": "1553300"
  },
  {
    "text": "get from state a to state B fairly uh fairly easily but not everything is like",
    "start": "1553300",
    "end": "1559720"
  },
  {
    "text": "this networking in particular is is very sticky it's very hard to change things",
    "start": "1559720",
    "end": "1565780"
  },
  {
    "text": "like ciders or CNI plugins in place on a running cluster without disrupting",
    "start": "1565780",
    "end": "1571000"
  },
  {
    "text": "workloads like it's possible that you can do it but like you know one one wrong step and you know everything kind",
    "start": "1571000",
    "end": "1577360"
  },
  {
    "text": "of falls apart it's much easier just to move workloads between clusters so it",
    "start": "1577360",
    "end": "1583120"
  },
  {
    "text": "would be bad if you know our new total generation scripts caused caused us to",
    "start": "1583120",
    "end": "1588400"
  },
  {
    "text": "switch existing clusters to a new cider that we want it to use so one concept",
    "start": "1588400",
    "end": "1593650"
  },
  {
    "text": "that we're experimenting with in our configuration is having the ability to fix values for things that are",
    "start": "1593650",
    "end": "1598860"
  },
  {
    "text": "to change so we fix these values in the configuration at the time that the cluster is created and we can then",
    "start": "1598860",
    "end": "1605940"
  },
  {
    "text": "change the way that the value is derived for new clusters without having to change it for old clusters so you know",
    "start": "1605940",
    "end": "1612330"
  },
  {
    "text": "we could so you know if we want it to fix the node cider mask size at 24 in",
    "start": "1612330",
    "end": "1618420"
  },
  {
    "text": "plus in clusters when they're generated we could do that and then you know change that value going forward and",
    "start": "1618420",
    "end": "1625559"
  },
  {
    "text": "eventually the older of the older clusters can be rotated out and replaced so another thing is that we want to be",
    "start": "1625559",
    "end": "1634440"
  },
  {
    "text": "able to step out of the user's way when they have a use case that we can't or don't support and I've certainly not",
    "start": "1634440",
    "end": "1640770"
  },
  {
    "text": "thought of every way that our users might want to use kubernetes or what sort of clusters they might want to",
    "start": "1640770",
    "end": "1646020"
  },
  {
    "text": "create and certainly not thought of every configuration that can be expressed in either chef or you know or",
    "start": "1646020",
    "end": "1653010"
  },
  {
    "text": "terraform so I really don't want to get in the way of you know what our users can do so we want to let let them drop",
    "start": "1653010",
    "end": "1659940"
  },
  {
    "text": "down and modify the code that would otherwise be generated and overwritten so every comment that we we generate or",
    "start": "1659940",
    "end": "1667290"
  },
  {
    "text": "every file that we generate has a comment at the top of it that can be marked to prevent overwrites from",
    "start": "1667290",
    "end": "1673470"
  },
  {
    "text": "happening and this is proven very useful as people are doing frankly a lot of very weird stuff in kubernetes lands so",
    "start": "1673470",
    "end": "1681770"
  },
  {
    "text": "definitely allows them to have their fun without us having to accommodate it in our generators so this is an ongoing",
    "start": "1681770",
    "end": "1689490"
  },
  {
    "text": "process and kubernetes as you know kubernetes its ecosystem and",
    "start": "1689490",
    "end": "1696360"
  },
  {
    "text": "our usage of it are all quickly changing and so we're trying to incorporate",
    "start": "1696360",
    "end": "1701549"
  },
  {
    "text": "learnings as quickly as possible and fuse them together to create a more maintainable system figure out what",
    "start": "1701549",
    "end": "1707370"
  },
  {
    "text": "needs to be generated incorporate features as soon as they're available all of this stuff and we need to be able",
    "start": "1707370",
    "end": "1712620"
  },
  {
    "text": "to like also roll those out to all of our clusters so this is a this is",
    "start": "1712620",
    "end": "1719340"
  },
  {
    "text": "unlocked our continued scaling and where we're currently at is that we have 22",
    "start": "1719340",
    "end": "1725210"
  },
  {
    "text": "cluster types at 22 different sort of like use cases for that a lot of these are fairly",
    "start": "1725210",
    "end": "1731049"
  },
  {
    "text": "mental and you know we're doing a lot of work to sort of prove things out and you",
    "start": "1731049",
    "end": "1736269"
  },
  {
    "text": "know we'll see where that leads and these represent these represent 36 clusters many of which are many of which",
    "start": "1736269",
    "end": "1744700"
  },
  {
    "text": "are part of a the prod cluster group which is expanding to support our you",
    "start": "1744700",
    "end": "1750249"
  },
  {
    "text": "know our actual service workload and alt all together this represents over 7,000",
    "start": "1750249",
    "end": "1756639"
  },
  {
    "text": "kubernetes nodes and the exciting thing is that from our current strategy we see",
    "start": "1756639",
    "end": "1762909"
  },
  {
    "text": "no upper bound on the number of nodes that we would be able to support we can just continue or creating more clusters",
    "start": "1762909",
    "end": "1770100"
  },
  {
    "text": "to support to support the increased capacity needs so yeah thank you all for",
    "start": "1770100",
    "end": "1778359"
  },
  {
    "text": "your time and you can learn more about our engineering work on our blog at",
    "start": "1778359",
    "end": "1783369"
  },
  {
    "text": "medium.com slash Airbnb engineering and we are also hiring so head over to",
    "start": "1783369",
    "end": "1789909"
  },
  {
    "text": "careers airbnb.com and and come work with me thank you very much [Applause]",
    "start": "1789909",
    "end": "1802990"
  }
]