[
  {
    "text": "this is the maintainer track talk for Sig autoscaling I'm very honored to be representing Sig autoscaling I'm Jack",
    "start": "160",
    "end": "6560"
  },
  {
    "text": "Francis uh I work at Microsoft as I go through a bunch of slides with a ton of information uh it might not be clear the",
    "start": "6560",
    "end": "13719"
  },
  {
    "text": "re the real reason we're here is to um try to expose the community to potentially new members invite new",
    "start": "13719",
    "end": "20439"
  },
  {
    "text": "contributions so if I fail to do that in the slide content please um know that",
    "start": "20439",
    "end": "25480"
  },
  {
    "text": "that's the the purpose of these talks we want to make the project um transparent expose what we're working on share that",
    "start": "25480",
    "end": "32200"
  },
  {
    "text": "we've got lots of problems to solve in the future and we'd love to have your",
    "start": "32200",
    "end": "37320"
  },
  {
    "text": "help okay that's me it's Superfluous to put a picture of myself on the screen because you can just look at me but um",
    "start": "37440",
    "end": "43760"
  },
  {
    "text": "now you get to see my son Jerome as well this an example of horizontal biological",
    "start": "43760",
    "end": "49680"
  },
  {
    "text": "autoscaling it's a good audience all right uh quick overview of",
    "start": "49680",
    "end": "55760"
  },
  {
    "text": "what we're going to talk about I actually did this agenda first and then all the content later so it might not",
    "start": "55760",
    "end": "60879"
  },
  {
    "text": "match up exactly but basically I'm going to introduce you to the Sig we're going to talk um at a high level uh and a",
    "start": "60879",
    "end": "67159"
  },
  {
    "text": "visual level about autoscaling Concepts how they work um brief mention of uh",
    "start": "67159",
    "end": "73400"
  },
  {
    "text": "Carpenter one year later so Carpenter joined seag scaling last year so it's been super great to have them in the",
    "start": "73400",
    "end": "79720"
  },
  {
    "text": "community um some stuff about Dr uh sh quick show hands does anybody know what",
    "start": "79720",
    "end": "85000"
  },
  {
    "text": "Dr is I know you do Patrick that's cheating anybody seen any good Dr talks",
    "start": "85000",
    "end": "91920"
  },
  {
    "text": "oh there's lots of them cool um we'll go up go through project updates there's a",
    "start": "91960",
    "end": "97240"
  },
  {
    "text": "lot of projects in seato scaling so that'll be kind of a quick um run through of things that have happened in",
    "start": "97240",
    "end": "102920"
  },
  {
    "text": "the last six months maybe 12 months future work and um some specific examples of where you can help but uh",
    "start": "102920",
    "end": "109920"
  },
  {
    "text": "just generally speaking you can definitely help all right so um introduction about",
    "start": "109920",
    "end": "116439"
  },
  {
    "text": "Sig Auto scaling the the the charter of Sig AO scaling kind it cuts across two",
    "start": "116439",
    "end": "121920"
  },
  {
    "text": "uh Dimensions one is clusters and one is workloads in clusters we have projects",
    "start": "121920",
    "end": "127320"
  },
  {
    "text": "like cluster autoscaler and Carpenter and for workloads we have horizontal pod",
    "start": "127320",
    "end": "133040"
  },
  {
    "text": "autoscaler or HPA vertical pod autoscaler or vpa a uh",
    "start": "133040",
    "end": "139879"
  },
  {
    "text": "emerging uh proposal called multi-dimensional pod autoscaling which is the two previous things combined into",
    "start": "139879",
    "end": "146040"
  },
  {
    "text": "one there is a prototype project called balancer that uh a scaler uh governs",
    "start": "146040",
    "end": "152959"
  },
  {
    "text": "that would love some help it's sort of stalled so we can talk about that briefly and then Adam resizer which is",
    "start": "152959",
    "end": "158680"
  },
  {
    "text": "uh if you've been in kubernetes for a long time you'll know that's been around for a long long time so real brief overview of cluster",
    "start": "158680",
    "end": "165879"
  },
  {
    "text": "scaler cluster autoscaler monitors for pending pods and increases node poool replicas in",
    "start": "165879",
    "end": "172159"
  },
  {
    "text": "response it removes under utiliz nodes when um uh respecting pdbs and other",
    "start": "172159",
    "end": "177959"
  },
  {
    "text": "constraints uh other scheduling constraints rints if if nodes are underutilized it can find a way to uh",
    "start": "177959",
    "end": "183720"
  },
  {
    "text": "run the same operational footprint um or I should say the same operational requirements with a uh lesser",
    "start": "183720",
    "end": "189720"
  },
  {
    "text": "operational footprint um and it does so by performing scheduling simulations based",
    "start": "189720",
    "end": "195560"
  },
  {
    "text": "on the um declar config in those pending pods okay carpenter carpenter is a sort",
    "start": "195560",
    "end": "202720"
  },
  {
    "text": "of sister solution for cluster Auto for autoscaling a cluster Carpenter",
    "start": "202720",
    "end": "207840"
  },
  {
    "text": "similarly monitors for unschedulable or pending pods and it Provisions nodes and response if we go back and forth to this",
    "start": "207840",
    "end": "213959"
  },
  {
    "text": "slide there's a key difference there in that first bullet point increases node replicas versus Provisions nodes so I'm not going to talk too much about that",
    "start": "213959",
    "end": "220239"
  },
  {
    "text": "but um basically Carpenter is sort of an all-up infrastructure provisioner cluster Auto scaler is designed more to",
    "start": "220239",
    "end": "227000"
  },
  {
    "text": "assume a homogeneous um horizontally scalable node pool sorry for the",
    "start": "227000",
    "end": "232519"
  },
  {
    "text": "gesturing and the mic um uh and it simply increases the replica replica count or decreases the replica count as",
    "start": "232519",
    "end": "239280"
  },
  {
    "text": "appropriate so uh yeah basically the same as as cluster",
    "start": "239280",
    "end": "245879"
  },
  {
    "text": "a scal except for that key difference okay moving on to horizontal po autoscaler another project under Sig",
    "start": "245879",
    "end": "251959"
  },
  {
    "text": "Auto scaling horizontal po autoscaler um I'll call it HPA from now on because it'll be quicker HPA increases and",
    "start": "251959",
    "end": "258919"
  },
  {
    "text": "decreases pod replicas to uh achieve a particular Target so it can scale on",
    "start": "258919",
    "end": "266400"
  },
  {
    "text": "um the standard resource metrics C CPU and memory it can uh scale on custom",
    "start": "266400",
    "end": "272479"
  },
  {
    "text": "metrics there's an example here for QPS you can imagine any type of custom metrics um that you can Define or",
    "start": "272479",
    "end": "279199"
  },
  {
    "text": "external metrics and there's an example here for Q length um configurable scaling behaviors so uh",
    "start": "279199",
    "end": "286560"
  },
  {
    "text": "this is just to say that you can uh tune scale up and scale down or I should",
    "start": "286560",
    "end": "293120"
  },
  {
    "text": "actually should rephrase that to scale out or scale in um discreetly so you can scale out more aggressively scale down",
    "start": "293120",
    "end": "299720"
  },
  {
    "text": "less aggressively it's a common pattern okay vpa vertical pod autoscaler",
    "start": "299720",
    "end": "306280"
  },
  {
    "text": "so um vpa essentially does the same thing as horizontal pod autoscaling in response to operational triggers but",
    "start": "306280",
    "end": "313000"
  },
  {
    "text": "instead of creating more pods it uh creates bigger or smaller",
    "start": "313000",
    "end": "319080"
  },
  {
    "text": "pods and I'm going to read through this the the Bal this balancer project is meant to address um more",
    "start": "320800",
    "end": "328160"
  },
  {
    "text": "complicated pod distribution um in an cluster autoscaler",
    "start": "328160",
    "end": "333400"
  },
  {
    "text": "enabled cluster so um how to ensure equal spreading of pods in three zones of a region with fall back and rebalance",
    "start": "333400",
    "end": "340400"
  },
  {
    "text": "these are some examples how do split pods uh 70 to 30% between spot or preemptable or regular VMS or for",
    "start": "340400",
    "end": "348680"
  },
  {
    "text": "example only use regular instances of spot instances are not available how to consume node types with negotiated rates",
    "start": "348680",
    "end": "354039"
  },
  {
    "text": "or discounts first and then fall back to maybe vanilla node types uh and then or",
    "start": "354039",
    "end": "360000"
  },
  {
    "text": "how how to horizontally and vertically autoscale such deployments and make cluster autoscaler work well with them so these are some sort of problem",
    "start": "360000",
    "end": "366560"
  },
  {
    "text": "statement examples that this project is trying to address there is a a prototype in place there's a V1 alpha1 um",
    "start": "366560",
    "end": "373240"
  },
  {
    "text": "implementation of this but if this sounds interesting to you and you are a cluster autoscaler stakeholder or user",
    "start": "373240",
    "end": "379599"
  },
  {
    "text": "uh this project could use your help and that would be fantastic add-on resizer as I mentioned",
    "start": "379599",
    "end": "386280"
  },
  {
    "text": "earlier this one's been around for a long long time so this uh as it states here simply vertically scales a",
    "start": "386280",
    "end": "391919"
  },
  {
    "text": "Singleton pod proportionally to the scale of a cluster so the sort of canonical example you if you've operated",
    "start": "391919",
    "end": "397680"
  },
  {
    "text": "kubernetes clusters before you may have used this for metric server it's super common a metric server um where the",
    "start": "397680",
    "end": "403360"
  },
  {
    "text": "resource needs to scale linearly or exponentially with the size of the cluster um cord DNS could be another",
    "start": "403360",
    "end": "408840"
  },
  {
    "text": "example of that so there's a point here about it can use nodes or containers As the",
    "start": "408840",
    "end": "413880"
  },
  {
    "text": "metric to drive scaling okay so that was a quick uh Breeze through through the the",
    "start": "413880",
    "end": "420319"
  },
  {
    "text": "projects that Sig autoscaling um is responsible for so now",
    "start": "420319",
    "end": "426440"
  },
  {
    "text": "I want to this is sort of the key conceptual point I want to hit home today so the reason there are all these",
    "start": "426440",
    "end": "433240"
  },
  {
    "text": "projects uh isn't because you choose one or the other but it's because they each solve a discrete part of the overall",
    "start": "433240",
    "end": "439879"
  },
  {
    "text": "problem set so I have this slide here that I don't know what these arrows really represent but this is a a a happy",
    "start": "439879",
    "end": "447280"
  },
  {
    "text": "family of HPA plus vpa plus cluster Autos scaler or Carpenter all working together to make your cluster more",
    "start": "447280",
    "end": "453360"
  },
  {
    "text": "Dynamic and optimally operational at all times so we're going to go through some fun um visual uh boxes here so in this",
    "start": "453360",
    "end": "463160"
  },
  {
    "text": "example I'm going to start out with a cluster or I should say a node pool with three nodes um that is fairly densely",
    "start": "463160",
    "end": "470599"
  },
  {
    "text": "packed so when something happens so on on one of the parent pods deployment we",
    "start": "470599",
    "end": "478319"
  },
  {
    "text": "have defined an HPA resource um which is how you use HPA and a trigger happens to",
    "start": "478319",
    "end": "484960"
  },
  {
    "text": "uh scale out an additional pod according to metric thresholds being exceeded so",
    "start": "484960",
    "end": "490639"
  },
  {
    "text": "this pod on the right is a pending pod which then gets scheduled onto um",
    "start": "490639",
    "end": "497360"
  },
  {
    "text": "the node which has available capacity so now our node pool is fully packed um you could say it's optimally",
    "start": "497360",
    "end": "504039"
  },
  {
    "text": "packed another example um let's go back to where we started so same node pool",
    "start": "504039",
    "end": "509879"
  },
  {
    "text": "nicely optimized just a little bit of overhead and now one of the parent deployments um on pod P22 has a vpa",
    "start": "509879",
    "end": "517479"
  },
  {
    "text": "defined against it that metric threshold is exceeded that triggers a vpa scale up",
    "start": "517479",
    "end": "524120"
  },
  {
    "text": "to meet utilization needs and now our cluster looks like",
    "start": "524120",
    "end": "528680"
  },
  {
    "text": "this okay so going back to our almost fully packed",
    "start": "529480",
    "end": "534519"
  },
  {
    "text": "cluster um one or more HPA one or more HPA defined on one or more apps",
    "start": "534519",
    "end": "540240"
  },
  {
    "text": "in the Pod in the pool trigger a scale out resulting in more pod overhead than the node pool is able to service so uh",
    "start": "540240",
    "end": "547040"
  },
  {
    "text": "if these shapes are meaningful which for the purposes of this demonstration they are you can imagine there's one there's",
    "start": "547040",
    "end": "553040"
  },
  {
    "text": "there's pod overhead of one pod on in the entire node pool on node three but we need four pods um there there is a",
    "start": "553040",
    "end": "560399"
  },
  {
    "text": "request for four pods so at this point the scheduler is able to find room for one of those pods but not the remaining",
    "start": "560399",
    "end": "565880"
  },
  {
    "text": "three and this triggers cluster autoscaler to add a new node to host those three",
    "start": "565880",
    "end": "572160"
  },
  {
    "text": "nodes or this triggers Carpenter to add a new node to schedule those three pods",
    "start": "572160",
    "end": "579200"
  },
  {
    "text": "so um there's a difference here in in the cluster a scaler scenario we assume",
    "start": "579200",
    "end": "584880"
  },
  {
    "text": "we're we're talking about a node pool here we assume that the next node is going to look exactly like the ones that preceded it in the carpenter scenario we",
    "start": "584880",
    "end": "591120"
  },
  {
    "text": "may decide that we can get uh the sufficient infrastructure to service",
    "start": "591120",
    "end": "596880"
  },
  {
    "text": "these pending pods with a smaller node um or maybe that represents a cheaper node but a different node than um some",
    "start": "596880",
    "end": "604480"
  },
  {
    "text": "nodes that are in its peer",
    "start": "604480",
    "end": "607959"
  },
  {
    "text": "group okay so now we're going to start from a different Baseline a fully packed node pool so uh let's imagine that we're",
    "start": "609640",
    "end": "616760"
  },
  {
    "text": "just beyond Peak business hours period and at this point in time node poool utilization has beg to",
    "start": "616760",
    "end": "623279"
  },
  {
    "text": "subside but all the pods are doing sufficient work so the the cluster is",
    "start": "623279",
    "end": "629320"
  },
  {
    "text": "densely packed with pods but perhaps not entirely hot so at some point in the near future utilization continues to",
    "start": "629320",
    "end": "636240"
  },
  {
    "text": "drop which eventually results in hpo eventually results in an hpo floor",
    "start": "636240",
    "end": "641959"
  },
  {
    "text": "threshold being triggered so for those certain applications a scale in event",
    "start": "641959",
    "end": "647120"
  },
  {
    "text": "occurs so now we see that our cluster is less densely",
    "start": "647120",
    "end": "652360"
  },
  {
    "text": "packed correspondingly so we're in the same cluster we might have some applications configured to do work that",
    "start": "653120",
    "end": "659440"
  },
  {
    "text": "less realtime sensitive for example batch processing and they may be configured for vpa so now they",
    "start": "659440",
    "end": "664600"
  },
  {
    "text": "opportunistically can grow themselves to do more work as their multi-tenant application peers take a breather so we",
    "start": "664600",
    "end": "671079"
  },
  {
    "text": "see some vpa scale up events happening on these nodes on some of these pods to take advantage of that extra overhead",
    "start": "671079",
    "end": "677360"
  },
  {
    "text": "that now exists later utilization eases and those",
    "start": "677360",
    "end": "682519"
  },
  {
    "text": "apps scale back down and even later node pool utilization has lessened to the point",
    "start": "682519",
    "end": "688440"
  },
  {
    "text": "where we now have significant decrease in pod density including an entirely empty node not",
    "start": "688440",
    "end": "694160"
  },
  {
    "text": "including Damon sets so imagine that there are Damon sets and the configuration is all hygienic and we",
    "start": "694160",
    "end": "701040"
  },
  {
    "text": "deal with those gracefully so cluster autoscar now is able to optimize the node pool and",
    "start": "701040",
    "end": "706240"
  },
  {
    "text": "remove unnecessary infrastructure so we've gone from three nodes with five pods to now one node with five pods and",
    "start": "706240",
    "end": "713560"
  },
  {
    "text": "I'm going to give the carpenter the same same Carpenter example Carpenter may be even even to may be a able to do do it",
    "start": "713560",
    "end": "719399"
  },
  {
    "text": "even better than that okay so all that is to say that HPA",
    "start": "719399",
    "end": "726959"
  },
  {
    "text": "vpa and cluster Auto scaler or Carpenter are complimentary successful cluster and",
    "start": "726959",
    "end": "732240"
  },
  {
    "text": "node poool scaling requires a Cooperative configuration between all three",
    "start": "732240",
    "end": "738440"
  },
  {
    "text": "actors okay so we're going to go back one a little bit more visualization so it's worth mentioning that one of the key pieces of functionality plus",
    "start": "741120",
    "end": "746880"
  },
  {
    "text": "ergonomics that Carpenter offers is something it calls consolidation a node",
    "start": "746880",
    "end": "751959"
  },
  {
    "text": "pool may look like this with seemingly nothing to do but maybe this is cheaper and equivalently",
    "start": "751959",
    "end": "758040"
  },
  {
    "text": "performant or maybe this maybe this is the same cost but more performant so Carpenter consolidation allows you to",
    "start": "758040",
    "end": "764199"
  },
  {
    "text": "configure your uh node pool to take advantage of these types of um real-time",
    "start": "764199",
    "end": "770959"
  },
  {
    "text": "opportunities that exist in the environment that you may be in to continually optimize your infrastructure",
    "start": "770959",
    "end": "776199"
  },
  {
    "text": "for spend and performance according to how you want to configure it",
    "start": "776199",
    "end": "781120"
  },
  {
    "text": "okay so now we're on to the updates uh part of this the the slide so I want to",
    "start": "782399",
    "end": "788320"
  },
  {
    "text": "talk a little bit about multi-dimensional pod autoscaler so we've been talking a lot about HPA and vpa so HPA and vpa are independent and",
    "start": "788320",
    "end": "796199"
  },
  {
    "text": "not really uh designed to work together um it's possible to do that but it's not",
    "start": "796199",
    "end": "801680"
  },
  {
    "text": "really recommended um how do you know uh if you want to scale out or scale in",
    "start": "801680",
    "end": "806760"
  },
  {
    "text": "according to overlapping um metrics thresholds so multi-dimensional po",
    "start": "806760",
    "end": "812240"
  },
  {
    "text": "autoscaler allows com combining HPA and vpa scaling on a single workload so you can imagine it's going to explode the",
    "start": "812240",
    "end": "817360"
  },
  {
    "text": "API quite a bit to um enable that complexity and that cooperation between the two",
    "start": "817360",
    "end": "822880"
  },
  {
    "text": "Dimensions it's intended to provide to prevent undesirable cases like large numbers due to HPA scaling of low CPU",
    "start": "822880",
    "end": "830959"
  },
  {
    "text": "requests of low CPU CPU request pods due to vpa then scaling",
    "start": "830959",
    "end": "836360"
  },
  {
    "text": "down it's designed to be extensible allowing users to to insert their own recommender encapsulating business logic",
    "start": "836360",
    "end": "842120"
  },
  {
    "text": "Etc so we can imagine in all that crazy Tetris logic logic that we saw earlier um with something like multi-dimensional",
    "start": "842120",
    "end": "848360"
  },
  {
    "text": "pod autoscaler you get um sort of realtime optimizational densities",
    "start": "848360",
    "end": "854120"
  },
  {
    "text": "happening across both dimensions simultaneously so this is currently in proposal status um the a is 5342 a",
    "start": "854120",
    "end": "862320"
  },
  {
    "text": "stands for autoscaling enhancement proposal uh we would definitely love help implementing this so if anyone",
    "start": "862320",
    "end": "869440"
  },
  {
    "text": "finds this appealing or if it solves a problem for you please let us know we would love to Kickstart this it is there",
    "start": "869440",
    "end": "875839"
  },
  {
    "text": "is some blockage on Upstream kubernetes U there's a cap a long cap for in place vertical pod Auto scale or",
    "start": "875839",
    "end": "883519"
  },
  {
    "text": "in place pod um resizing so that's sort of in line with this but you can also",
    "start": "883519",
    "end": "888880"
  },
  {
    "text": "help with that too okay uh talk about Carpenter really",
    "start": "888880",
    "end": "894959"
  },
  {
    "text": "quickly so Carpenter hit V1 so that's incredible are there any Carpenter Main maintainers maintainers here",
    "start": "894959",
    "end": "900160"
  },
  {
    "text": "contributors probably not maintainers I know them thanks Mike so Carpenters at V1 if you've been waiting for a totally",
    "start": "900160",
    "end": "907959"
  },
  {
    "text": "boring API to use Carpenter the time has arrived V1 was released over the summer",
    "start": "907959",
    "end": "913360"
  },
  {
    "text": "so that's a really great milestone for the project there's also two new providers",
    "start": "913360",
    "end": "918600"
  },
  {
    "text": "uh one is AKs which is going to GA behind a feature called node Auto",
    "start": "918600",
    "end": "924040"
  },
  {
    "text": "provisioning in q1 2025 I see some folks from AKs so I just going to throw that out there and they can scold me",
    "start": "924040",
    "end": "929880"
  },
  {
    "text": "afterwards for giving dates um but it's an open source project so you can contribute to that um and there are ways",
    "start": "929880",
    "end": "935040"
  },
  {
    "text": "to to smoke test that either in your staging environment or you know talk to",
    "start": "935040",
    "end": "940759"
  },
  {
    "text": "your AKs representative but that's it's really great to have the the provider ecosystem expanding for Carpenter and in",
    "start": "940759",
    "end": "946279"
  },
  {
    "text": "addition I'm looking right now at the maintainer for a new cluster API Carpenter provider so this is uh has the",
    "start": "946279",
    "end": "952000"
  },
  {
    "text": "opportunity to really expand the infrastructure opportunity in a single provider for for",
    "start": "952000",
    "end": "958639"
  },
  {
    "text": "a car Carpenter so folks who are in um GK for example running capg with the",
    "start": "958639",
    "end": "963800"
  },
  {
    "text": "capy carpenter provider you can now leverage Carpenter in your GK environment without having to write your",
    "start": "963800",
    "end": "969079"
  },
  {
    "text": "own GK specific provider so that project uh is it opening open to contributors Mike I absolutely",
    "start": "969079",
    "end": "978199"
  },
  {
    "text": "yes oh wow thank you life mov fast so Mike uh let me know",
    "start": "978839",
    "end": "985199"
  },
  {
    "text": "that an Alibaba provider also landed so it's great to see new providers in the carpenter ecosystem uh real real quick three three",
    "start": "985199",
    "end": "992519"
  },
  {
    "text": "rfc's um these are you know you can call these K uh Carpenter enh enhancement",
    "start": "992519",
    "end": "998240"
  },
  {
    "text": "proposal but there's an overlap with cap so RFC is the the lingua franka for the carpenter community so there's a",
    "start": "998240",
    "end": "1005160"
  },
  {
    "text": "capacity reservations RFC that landed a node Auto Repair RFC that landed and a node overlay RFC that landed I've put a",
    "start": "1005160",
    "end": "1012880"
  },
  {
    "text": "link to the project so you can get more information about that but the the carpenter project is definitely maturing but also uh simultaneously moving",
    "start": "1012880",
    "end": "1019839"
  },
  {
    "text": "forward so lots of great stuff happening there okay um I want to put a slide in",
    "start": "1019839",
    "end": "1026160"
  },
  {
    "text": "here as a project update um that is overlapping with the serving working group so the serving working group has",
    "start": "1026160",
    "end": "1033558"
  },
  {
    "text": "uh been formed in response to all this fun AI stuff happening across basically",
    "start": "1033559",
    "end": "1038959"
  },
  {
    "text": "every dimension of the kubernetes kubernetes landscape and a bunch of sigs including seato scaling are serving that",
    "start": "1038959",
    "end": "1045880"
  },
  {
    "text": "mission so for seato scaling this is just an overview describing autoscaling sort of stakeholdership",
    "start": "1045880",
    "end": "1052240"
  },
  {
    "text": "within WG serving um we need to make sure that for AI serving workloads that",
    "start": "1052240",
    "end": "1057640"
  },
  {
    "text": "we understand autoscaling needs techniques and nuances of running inference workloads um so inference and serving",
    "start": "1057640",
    "end": "1063559"
  },
  {
    "text": "are somewhat synonymous there breaking down the various user objectives in tense and how each influences how one",
    "start": "1063559",
    "end": "1069720"
  },
  {
    "text": "could set up autoscaling coming together and sharing knowledge around optimizing the quickly",
    "start": "1069720",
    "end": "1074960"
  },
  {
    "text": "changing landscape of inference workloads so this is a really important part of the mission for both serving and",
    "start": "1074960",
    "end": "1080600"
  },
  {
    "text": "device management finding folks if you're doing this please come to these working groups and share your scenarios um to make sure that you inform the",
    "start": "1080600",
    "end": "1087159"
  },
  {
    "text": "apis um now closely collaborating with the various initiatives so uh Autos skin is",
    "start": "1087159",
    "end": "1094400"
  },
  {
    "text": "collaborating with a benchmarking initiative that came out of serving um best practices uh being rendered in a",
    "start": "1094400",
    "end": "1101159"
  },
  {
    "text": "public serving catalog which are sort of examples um of various use cases how to",
    "start": "1101159",
    "end": "1106320"
  },
  {
    "text": "compose all the solutions for for prac iCal work and um a new llm INF Gateway",
    "start": "1106320",
    "end": "1112760"
  },
  {
    "text": "uh work stream that's coming out of serving so there's a lot of great stuff in um serving okay I should probably",
    "start": "1112760",
    "end": "1120240"
  },
  {
    "text": "just give the mic over to Patrick but I'll do my best here so I stole this these slides from John uh uh B mark from",
    "start": "1120240",
    "end": "1127080"
  },
  {
    "text": "Google who did the talk for WG device management so as I mentioned earlier um",
    "start": "1127080",
    "end": "1132280"
  },
  {
    "text": "Dr is a big thing for seato scaling because uh Dr has gone beta in 132 and",
    "start": "1132280",
    "end": "1138600"
  },
  {
    "text": "um the necessary changes to allow Sig Auto scaling to to allow cluster autoscaler to consume that work and also",
    "start": "1138600",
    "end": "1144080"
  },
  {
    "text": "Carpenter um also landed in 132 so we are uh",
    "start": "1144080",
    "end": "1150000"
  },
  {
    "text": "frantically um but also very responsibly Landing the necessary changes in the",
    "start": "1150000",
    "end": "1155080"
  },
  {
    "text": "cluster autoscaler project to ship a 132 with Dr enabled so I want to just do a quick overview of what Dr is so Dr is um",
    "start": "1155080",
    "end": "1164240"
  },
  {
    "text": "and this is a really nice phrasing it's a new kubernetes API to describe services so that is um exposed as a new",
    "start": "1164240",
    "end": "1171000"
  },
  {
    "text": "crd called a resource slice it's a new kubernetes API to request devices these are described as resource",
    "start": "1171000",
    "end": "1177799"
  },
  {
    "text": "claims um I may as well read the examples I think they're good so part one the describing services so for",
    "start": "1177799",
    "end": "1184480"
  },
  {
    "text": "example this device is an nvidia.com GPU its product ID is I'm not going to read",
    "start": "1184480",
    "end": "1189559"
  },
  {
    "text": "that a an a100 skew it has uh 40 GB of memory and 3456 fp64 cor so um we have a",
    "start": "1189559",
    "end": "1198520"
  },
  {
    "text": "new API that can describe all of that which we don't before Dr uh we also have a newer API to",
    "start": "1198520",
    "end": "1204799"
  },
  {
    "text": "describe something like I need an Nvidia GPU with at least 30 G gigabytes of memory and at least 3,000 fp64 core so",
    "start": "1204799",
    "end": "1212120"
  },
  {
    "text": "that's what a resource claim um exposes as a standard API to users uh we are updating the scheduler",
    "start": "1212120",
    "end": "1218280"
  },
  {
    "text": "to match requests to devices that's obviously very very very important so we can create that Affinity between uh say",
    "start": "1218280",
    "end": "1224280"
  },
  {
    "text": "a resource claim and the actual uh node running a resource slice that has those",
    "start": "1224280",
    "end": "1231320"
  },
  {
    "text": "capabilities and uh we have a new cuet API to actuate the Schuler's",
    "start": "1231320",
    "end": "1236520"
  },
  {
    "text": "decisions so hopefully that's not too much densely packed information for Dr for folks who are new um but uh there's",
    "start": "1236520",
    "end": "1243520"
  },
  {
    "text": "a little bit more supporting stuff here so at the at the bottom here there's some interesting examples of how you",
    "start": "1243520",
    "end": "1249039"
  },
  {
    "text": "might compose um and or decompose depending on how you want to put it various GPU um application use cases so",
    "start": "1249039",
    "end": "1256559"
  },
  {
    "text": "we have examples here with say a workload that's running on two two pods",
    "start": "1256559",
    "end": "1261840"
  },
  {
    "text": "one container each one GPU per container here's another example of one pod with two containers with a share GPU that's",
    "start": "1261840",
    "end": "1268159"
  },
  {
    "text": "something really can't do preg GPA at Dr here's an example with two pods one container each one Shar GPU and then one",
    "start": "1268159",
    "end": "1276000"
  },
  {
    "text": "pod one container one claim four gpus per claim so those are four examples",
    "start": "1276000",
    "end": "1281440"
  },
  {
    "text": "there's four million other examples the component TS you can imagine but all of this is available in a standard API",
    "start": "1281440",
    "end": "1287039"
  },
  {
    "text": "called Dr which is going to make uh kubernetes on AI much much more",
    "start": "1287039",
    "end": "1292760"
  },
  {
    "text": "ergonomic and flexible and Powerful um given all the complexity and resource",
    "start": "1292760",
    "end": "1299279"
  },
  {
    "text": "constraints that we have I'm going to skip past this just to",
    "start": "1299279",
    "end": "1305640"
  },
  {
    "text": "make sure okay quick project update for vpa",
    "start": "1305640",
    "end": "1310679"
  },
  {
    "text": "so vpa is [Music] um uh in 13 in 13 in 130 V1 beta 2 API",
    "start": "1310679",
    "end": "1318640"
  },
  {
    "text": "will no longer be included in releases so this is just a sort of PSA that we",
    "start": "1318640",
    "end": "1323880"
  },
  {
    "text": "continue to remove beta apis in uh vpa so again V1 beta 2 will uh not be",
    "start": "1323880",
    "end": "1330720"
  },
  {
    "text": "available starting with 130 High availability capability was introduced in 1.2.0 that's the latest",
    "start": "1330720",
    "end": "1337440"
  },
  {
    "text": "release and um there is an ongoing a for inplace vpa I mentioned earlier about",
    "start": "1337440",
    "end": "1344200"
  },
  {
    "text": "multi-dimensional pod autoscaling so this um",
    "start": "1344200",
    "end": "1349679"
  },
  {
    "text": "is blocked on needs on the uh in place pod vertical scaling Sig node uh kep",
    "start": "1349679",
    "end": "1356039"
  },
  {
    "text": "Implement it's it's there's a bunch of implementation details that you can look into um as far as we can we can tell the",
    "start": "1356039",
    "end": "1361440"
  },
  {
    "text": "earliest implementation for that would be 133 hi Derek is that Derek hey",
    "start": "1361440",
    "end": "1371039"
  },
  {
    "text": "Joel okay great so Joel shares that uh a that the in place pod vertical scaling",
    "start": "1371279",
    "end": "1378159"
  },
  {
    "text": "will be beta and 132 that's great update information thank you so that should unlock so if you're interested in this",
    "start": "1378159",
    "end": "1384320"
  },
  {
    "text": "um from a vpa perspective come help out looks like the the dam is broken",
    "start": "1384320",
    "end": "1392240"
  },
  {
    "text": "okay cluster autoscaler so um in 131 a implementation of proactive",
    "start": "1392240",
    "end": "1398919"
  },
  {
    "text": "scaleup landed in 131 also provisioning request V1 landed in 131 a new expander",
    "start": "1398919",
    "end": "1406440"
  },
  {
    "text": "lease nodes landed and um as I mentioned earlier in 132 by hook or by crook we're",
    "start": "1406440",
    "end": "1412400"
  },
  {
    "text": "going to land Dr uh just a quick expansion of that previous slide um so an expander is a",
    "start": "1412400",
    "end": "1420640"
  },
  {
    "text": "different strategy for selecting the node um the node group or node pool",
    "start": "1420640",
    "end": "1426400"
  },
  {
    "text": "uh based on Penning pods so a typical cluster running and production is going",
    "start": "1426400",
    "end": "1431640"
  },
  {
    "text": "to have scale is going to have lots and lots of different node pools and in a cluster autoscaler situation those node",
    "start": "1431640",
    "end": "1437159"
  },
  {
    "text": "pools are going to be each pool is going to be a distinct type of infrastructure so expanders help to",
    "start": "1437159",
    "end": "1443640"
  },
  {
    "text": "to find the right pool to schedule a pending pod to um so now we have uh in",
    "start": "1443640",
    "end": "1448799"
  },
  {
    "text": "the cluster autoscaler project six expanders we have random we have most pods least waste lease nodes price and",
    "start": "1448799",
    "end": "1456159"
  },
  {
    "text": "priority and these are also uh composable together by priority so",
    "start": "1456159",
    "end": "1461799"
  },
  {
    "text": "there's a lot of options there for cluster SC is the grp C expander not",
    "start": "1461799",
    "end": "1470360"
  },
  {
    "text": "Mike mentions that grp expander did not get any love also grp grpc",
    "start": "1471080",
    "end": "1478039"
  },
  {
    "text": "expander okay Help Wanted so I I've been mentioning this help wanted we want help for",
    "start": "1478720",
    "end": "1484559"
  },
  {
    "text": "um V vpa has uh asked me to send out a call for help for vertical pod Auto scaling for workloads with heterogeneous",
    "start": "1484559",
    "end": "1491720"
  },
  {
    "text": "uh resource requirements so there's a link to the issue there this slide will be uploaded uploaded to the the cucon",
    "start": "1491720",
    "end": "1499000"
  },
  {
    "text": "uh website if you want to get access to this content or you can always just talk to us uh for HBA for hbaa configurable",
    "start": "1499000",
    "end": "1505919"
  },
  {
    "text": "tolerance that's a um so HPA exists in the in the the main kubernetes code base",
    "start": "1505919",
    "end": "1511720"
  },
  {
    "text": "so for just a brief history overview HPA uh was introduced when kubernetes had",
    "start": "1511720",
    "end": "1518520"
  },
  {
    "text": "all of its componentry sort of inry vpa landed at a later date when things were being split out so the the configurable",
    "start": "1518520",
    "end": "1525279"
  },
  {
    "text": "tolerance proposal is a cap not an a it's kep 4951 one um we would love help on that if anyone is interested in HPA",
    "start": "1525279",
    "end": "1533120"
  },
  {
    "text": "configurable tolerance definitely want help on Dr um I'm definitely especially calling out",
    "start": "1533120",
    "end": "1538799"
  },
  {
    "text": "for testers so if you have uh A1 100s in your staging environment and you want to",
    "start": "1538799",
    "end": "1544760"
  },
  {
    "text": "start using Dr please start testing reach out to us we'd love to know how this is working um and especially from a",
    "start": "1544760",
    "end": "1550320"
  },
  {
    "text": "cluster autoskill perspective we'd love to you know work with you to give you um Alpha builds to help get some mileage on",
    "start": "1550320",
    "end": "1556840"
  },
  {
    "text": "that I've added a muscle Emoji a wood emoji and a bucket of water Emoji so uh",
    "start": "1556840",
    "end": "1564159"
  },
  {
    "text": "any of those things are very much well uh help welcome in the next sort of month or so before the 132 release so",
    "start": "1564159",
    "end": "1571320"
  },
  {
    "text": "come help help us out hang out with us uh here is uh the boiler plate for",
    "start": "1571320",
    "end": "1576919"
  },
  {
    "text": "how to get involved so um Sig information is available on that",
    "start": "1576919",
    "end": "1582679"
  },
  {
    "text": "really long link um you can Google kubernetes Sig a scaling like I usually do but that has uh the as it does with",
    "start": "1582679",
    "end": "1590520"
  },
  {
    "text": "all sigs um all the necessary detail about how to get involved in SE meets",
    "start": "1590520",
    "end": "1596120"
  },
  {
    "text": "weekly um if you live on the west coast like me it's Monday mornings at 7: a.m. it's a pretty exciting time to get up",
    "start": "1596120",
    "end": "1602279"
  },
  {
    "text": "and talk about Auto scaling um so come hang out with me especially if you're on the west coast wake up early like I",
    "start": "1602279",
    "end": "1608640"
  },
  {
    "text": "do um and how to you know join the Google group and all the the stuff that if if you're familiar with how sigs work",
    "start": "1608640",
    "end": "1616039"
  },
  {
    "text": "we're on Sig Auto scaling on kubernetes slack and Carpenter on kubernetes slack uh a link to test grid there",
    "start": "1616039",
    "end": "1623159"
  },
  {
    "text": "that's our test coverage go look at that and criticize it and then add more test coverage that would be amazing here's a",
    "start": "1623159",
    "end": "1630360"
  },
  {
    "text": "link to the carpenter carpenter has a very pretty homepage which describes things in a sort of producty way but",
    "start": "1630360",
    "end": "1635799"
  },
  {
    "text": "also has tons and tons and tons of great documentation and here's the last slide",
    "start": "1635799",
    "end": "1641399"
  },
  {
    "text": "with the QR code do we have time for questions when is this did I go all the way I'd love to",
    "start": "1641399",
    "end": "1648679"
  },
  {
    "text": "answer any questions we have a couple minutes I've been told there's a mic",
    "start": "1648679",
    "end": "1655279"
  },
  {
    "text": "there or and there uh I is this on I can hear you yes",
    "start": "1657880",
    "end": "1664120"
  },
  {
    "text": "okay uh I came in a little bit late so apologies if you already covered this uh is the multi-dimensional autoscaler",
    "start": "1664120",
    "end": "1670799"
  },
  {
    "text": "intended to supersede the others or is it intended to use them on the back end or to be a",
    "start": "1670799",
    "end": "1677240"
  },
  {
    "text": "different thing of its own entirely great question so definitely no to the previous question so Adam resizer is a",
    "start": "1677240",
    "end": "1684399"
  },
  {
    "text": "great I think concrete example of what happens to useful projects um that",
    "start": "1684399",
    "end": "1690480"
  },
  {
    "text": "eventually are functionally superseded by later projects that provide more functionality it stick it sticks around for folks who want to continue using",
    "start": "1690480",
    "end": "1695679"
  },
  {
    "text": "that surgical tool um so HPA and vpa as far as I can tell will stick around for",
    "start": "1695679",
    "end": "1701600"
  },
  {
    "text": "a long long time the extent to which they uh there is reuse opportunity in",
    "start": "1701600",
    "end": "1708840"
  },
  {
    "text": "the HPA and vpa actual source to inform how multi-dimensional pod autoscaling shapes",
    "start": "1708840",
    "end": "1715480"
  },
  {
    "text": "up is TBD um but I it will definitely be its own crd and its own operator like some",
    "start": "1715480",
    "end": "1723880"
  },
  {
    "text": "something like vpa and HPA but there if there's reuse opportunity then I would imagine that",
    "start": "1723880",
    "end": "1729320"
  },
  {
    "text": "would be uh",
    "start": "1729320",
    "end": "1732720"
  },
  {
    "text": "welcome hey yeah hi uh I'm Alex um using uh lots of the auto scaling for um GPU",
    "start": "1734600",
    "end": "1741559"
  },
  {
    "text": "inference workloads right which is probably not surprising um one of the things that I'd love to figure out is um we actually",
    "start": "1741559",
    "end": "1748559"
  },
  {
    "text": "looked into the Dr feature kind on a high level um but uh the the question",
    "start": "1748559",
    "end": "1754519"
  },
  {
    "text": "that sticks in the back of my mind is to say all right many of the metrics that you use for scaling GPU workloads are",
    "start": "1754519",
    "end": "1763279"
  },
  {
    "text": "not your typical say like CPU or memory uh metrix right so is there any kind of plan have some standardized uh GPU",
    "start": "1763279",
    "end": "1770200"
  },
  {
    "text": "metrics that can be used for Autos scaling or will we always be using our own custom metrics here that's a great",
    "start": "1770200",
    "end": "1775760"
  },
  {
    "text": "question I don't know about always but so the I mentioned earlier in the slide there's a a a a serving catalog which",
    "start": "1775760",
    "end": "1782240"
  },
  {
    "text": "came out of the working group serving so that would address the the inference sort of scenario you're discussing and",
    "start": "1782240",
    "end": "1788360"
  },
  {
    "text": "the aim there is to start providing concrete examples of things like custom metrics and we hope to Marshall from the",
    "start": "1788360",
    "end": "1795320"
  },
  {
    "text": "community a set of common metrics maybe those M matriculate into the into the core project but um I think for right",
    "start": "1795320",
    "end": "1803240"
  },
  {
    "text": "now the um the best way to approach that is to get super comfortable with how to define custom metrics in kubernetes you",
    "start": "1803240",
    "end": "1810399"
  },
  {
    "text": "know I imagine you're talking things like token latency those types of metrics yeah so those are not standardized right now um uh that",
    "start": "1810399",
    "end": "1817799"
  },
  {
    "text": "doesn't mean that that uh it would be uh discouraged from finding out the best",
    "start": "1817799",
    "end": "1824799"
  },
  {
    "text": "metric solution for your own use case but would expect I would if you want to bring those those solutions that you",
    "start": "1824799",
    "end": "1831559"
  },
  {
    "text": "find in custom metrics to the serving working groups um uh that would be",
    "start": "1831559",
    "end": "1837240"
  },
  {
    "text": "fantastic uh they would definitely welcome getting access to those reference examples so that would be",
    "start": "1837240",
    "end": "1843200"
  },
  {
    "text": "mainly for the uh serving working group and not so much for Autos scaling well",
    "start": "1843200",
    "end": "1848480"
  },
  {
    "text": "for right now that's right yeah okay I think the notion is eventually a working group um disbands when its mission is",
    "start": "1848480",
    "end": "1855120"
  },
  {
    "text": "complete and then the relevant sigs would take over",
    "start": "1855120",
    "end": "1860399"
  },
  {
    "text": "responsibility hi um I had a question uh you know if you could talk a bit about manual scaling or proactive scaling",
    "start": "1861000",
    "end": "1867120"
  },
  {
    "text": "support in Carpenter and what the plan is there manual scaling",
    "start": "1867120",
    "end": "1872200"
  },
  {
    "text": "so um can I reclassify that as as uh",
    "start": "1872200",
    "end": "1878240"
  },
  {
    "text": "like something like pre-provisioning or yes PR yeah I don't I don't think I have an updated status I do I is there an RFC",
    "start": "1878240",
    "end": "1885440"
  },
  {
    "text": "in the queue right now for Carpenter for for pre-provisioning or I've seen a design for something like a Headroom API",
    "start": "1885440",
    "end": "1892399"
  },
  {
    "text": "um butri yeah I I I initially wrote that so that was a while back um so the uh I",
    "start": "1892399",
    "end": "1898559"
  },
  {
    "text": "haven't been as involved in that in the sort of year and a half since I initially wrote that but if you are",
    "start": "1898559",
    "end": "1903720"
  },
  {
    "text": "interested in that I I I would say discover what the canonical issues are in the project and and just plus one or",
    "start": "1903720",
    "end": "1909760"
  },
  {
    "text": "add your use case to um try to create some inertia for that solution okay",
    "start": "1909760",
    "end": "1914880"
  },
  {
    "text": "thanks hi um so a quick question about um",
    "start": "1914880",
    "end": "1921880"
  },
  {
    "text": "autoscaling so either cluster autoscaler or Carpenter um based on um node inform",
    "start": "1921880",
    "end": "1928600"
  },
  {
    "text": "node local information such as local storage one of the problems that we've hit repeatedly is that there doesn't",
    "start": "1928600",
    "end": "1935519"
  },
  {
    "text": "seem to be currently a good solution for autoscaling based on um and when I say",
    "start": "1935519",
    "end": "1941440"
  },
  {
    "text": "local storage I mean local storage available through CSI and through the CSI spec not ephemeral local storage and",
    "start": "1941440",
    "end": "1948120"
  },
  {
    "text": "the it seems like in the past there have been some discussions on on on GitHub about similar problems that they kind of",
    "start": "1948120",
    "end": "1954760"
  },
  {
    "text": "um faded away so I was just curious if there is any conversation about that um or any plans to support this in the",
    "start": "1954760",
    "end": "1960519"
  },
  {
    "text": "future that's a great question I don't have any I don't think I have anything interesting to offer right now on that",
    "start": "1960519",
    "end": "1965840"
  },
  {
    "text": "question is sounds like Mike wants to talk to you about",
    "start": "1965840",
    "end": "1971320"
  },
  {
    "text": "that thank you",
    "start": "1971320",
    "end": "1975480"
  },
  {
    "text": "hi thanks for your talk uh a question around also Carpenter a little bit and support for customer sources or yeah I",
    "start": "1978960",
    "end": "1986320"
  },
  {
    "text": "think there been an proposal about not templates or something called not overlays to which is kind of touches a",
    "start": "1986320",
    "end": "1993600"
  },
  {
    "text": "little bit this topic as well so that for different notes you can specify that they may have additional customer sources and then consume them as well",
    "start": "1993600",
    "end": "2000480"
  },
  {
    "text": "yeah again I don't I I feel like I can't answer that question um sorry is that is are you are these customer resources",
    "start": "2000480",
    "end": "2006559"
  },
  {
    "text": "representing workloads uh so these inform the scheduler yeah this is for Schuler to be",
    "start": "2006559",
    "end": "2012919"
  },
  {
    "text": "able here so I would imag is a is a normal approach where you would have a schedular Plugin or something like that",
    "start": "2012919",
    "end": "2018559"
  },
  {
    "text": "that that exposes awareness of those custom resources is that not integrated",
    "start": "2018559",
    "end": "2023960"
  },
  {
    "text": "something either demon set or whatever there would be a way later to modify the node and tell that it it has certain",
    "start": "2023960",
    "end": "2029720"
  },
  {
    "text": "capacity for customer Source but for uh right now basically this approach is",
    "start": "2029720",
    "end": "2034960"
  },
  {
    "text": "kind of blocked cuz um Carpenter cannot decide which node to pick up when there is a resource request for this customer",
    "start": "2034960",
    "end": "2041760"
  },
  {
    "text": "Source on the on the definition for workload yeah yeah I see yeah apologies I can't answer that sounds like a great",
    "start": "2041760",
    "end": "2047440"
  },
  {
    "text": "thing to bring into slack I'm sure they would love to talk about that are we at time okay thanks everyone",
    "start": "2047440",
    "end": "2055760"
  }
]