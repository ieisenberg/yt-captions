[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "thank you my name is Chris O'Brien I'm a",
    "start": "30",
    "end": "2639"
  },
  {
    "text": "senior engineering manager with tinder",
    "start": "2639",
    "end": "4859"
  },
  {
    "text": "and this is Chris Thomas also another",
    "start": "4859",
    "end": "6390"
  },
  {
    "text": "engineering manager with tinder and we",
    "start": "6390",
    "end": "8940"
  },
  {
    "text": "want to talk you through the decision to",
    "start": "8940",
    "end": "12000"
  },
  {
    "text": "move to kubernetes and also what the",
    "start": "12000",
    "end": "14309"
  },
  {
    "text": "process was like I'm sure many of you",
    "start": "14309",
    "end": "16470"
  },
  {
    "text": "know what tinder is but in case you",
    "start": "16470",
    "end": "19650"
  },
  {
    "text": "don't or you need a recap but tinder is",
    "start": "19650",
    "end": "23250"
  },
  {
    "text": "the world's most popular app for meeting",
    "start": "23250",
    "end": "25529"
  },
  {
    "text": "new people and our size and scale means",
    "start": "25529",
    "end": "29189"
  },
  {
    "text": "greater choice and access to a diverse",
    "start": "29189",
    "end": "31230"
  },
  {
    "text": "set of matches and Chris and I are gonna",
    "start": "31230",
    "end": "34079"
  },
  {
    "text": "Kris and I are going to talk through how",
    "start": "34079",
    "end": "35700"
  },
  {
    "text": "we were able to handle that scale",
    "start": "35700",
    "end": "37100"
  },
  {
    "text": "through the amazing work of tinder",
    "start": "37100",
    "end": "39450"
  },
  {
    "text": "engineering and the cloud infrastructure",
    "start": "39450",
    "end": "41489"
  },
  {
    "text": "team so ever two years ago as was",
    "start": "41489",
    "end": "47129"
  },
  {
    "text": "previously mentioned tinder decided to",
    "start": "47129",
    "end": "48930"
  },
  {
    "text": "move its platform to kubernetes",
    "start": "48930",
    "end": "50239"
  },
  {
    "text": "kubernetes afford this as an opportunity",
    "start": "50239",
    "end": "52860"
  },
  {
    "text": "to drive to our engineering towards",
    "start": "52860",
    "end": "55440"
  },
  {
    "text": "containerization and lo touch operation",
    "start": "55440",
    "end": "57600"
  },
  {
    "text": "through a moodle immutable deployment",
    "start": "57600",
    "end": "59910"
  },
  {
    "text": "and as a result application build",
    "start": "59910",
    "end": "62520"
  },
  {
    "text": "deployment and infrastructure would be",
    "start": "62520",
    "end": "63960"
  },
  {
    "text": "defined in code we were looking to",
    "start": "63960",
    "end": "67320"
  },
  {
    "text": "address the challenges of scale and",
    "start": "67320",
    "end": "69420"
  },
  {
    "text": "stability when scaling would become",
    "start": "69420",
    "end": "71549"
  },
  {
    "text": "critical we offer we often would suffer",
    "start": "71549",
    "end": "73500"
  },
  {
    "text": "through several minutes of waiting for",
    "start": "73500",
    "end": "74939"
  },
  {
    "text": "new Amazon instances to populate in an",
    "start": "74939",
    "end": "77400"
  },
  {
    "text": "auto scaling group and come online and",
    "start": "77400",
    "end": "80040"
  },
  {
    "text": "the idea of container scheduling and",
    "start": "80040",
    "end": "82020"
  },
  {
    "text": "becoming serving traffic within seconds",
    "start": "82020",
    "end": "85830"
  },
  {
    "text": "as opposed to minutes was very appealing",
    "start": "85830",
    "end": "87060"
  },
  {
    "text": "to us so starting January 2018 we worked",
    "start": "87060",
    "end": "93750"
  },
  {
    "text": "our way through various stages of the",
    "start": "93750",
    "end": "95250"
  },
  {
    "text": "migration effort",
    "start": "95250",
    "end": "96540"
  },
  {
    "text": "we started by container izing all of our",
    "start": "96540",
    "end": "98220"
  },
  {
    "text": "services and deploying them to a series",
    "start": "98220",
    "end": "100829"
  },
  {
    "text": "of kubernetes hosted staging",
    "start": "100829",
    "end": "102090"
  },
  {
    "text": "environments and beginning in October of",
    "start": "102090",
    "end": "104009"
  },
  {
    "text": "last year we began methodically cutting",
    "start": "104009",
    "end": "105689"
  },
  {
    "text": "all of the legacy services over to their",
    "start": "105689",
    "end": "108270"
  },
  {
    "text": "counterpart on kubernetes and by March",
    "start": "108270",
    "end": "110700"
  },
  {
    "text": "of the following year we finalized our",
    "start": "110700",
    "end": "112259"
  },
  {
    "text": "migration in the tinder platform now",
    "start": "112259",
    "end": "113700"
  },
  {
    "text": "runs exclusively on kerbin natives so",
    "start": "113700",
    "end": "120810"
  },
  {
    "start": "119000",
    "end": "119000"
  },
  {
    "text": "we'll talk through our legacy",
    "start": "120810",
    "end": "122610"
  },
  {
    "text": "architecture building images our CI CD",
    "start": "122610",
    "end": "126360"
  },
  {
    "text": "platform the original architecture the",
    "start": "126360",
    "end": "130349"
  },
  {
    "text": "migration",
    "start": "130349",
    "end": "131670"
  },
  {
    "text": "some learnings along the way load",
    "start": "131670",
    "end": "134970"
  },
  {
    "text": "balancing and how we're leveraging envoy",
    "start": "134970",
    "end": "137270"
  },
  {
    "text": "the current architecture as well as our",
    "start": "137270",
    "end": "140520"
  },
  {
    "text": "monitoring stack in the future",
    "start": "140520",
    "end": "142980"
  },
  {
    "text": "architecture so the legacy architecture",
    "start": "142980",
    "end": "149280"
  },
  {
    "start": "147000",
    "end": "147000"
  },
  {
    "text": "as I previously mentioned were all hosts",
    "start": "149280",
    "end": "152070"
  },
  {
    "text": "on Amazon and auto-scaling groups they",
    "start": "152070",
    "end": "154410"
  },
  {
    "text": "were fronted by a load balancer per",
    "start": "154410",
    "end": "156510"
  },
  {
    "text": "service and scaling based on cpu usage",
    "start": "156510",
    "end": "159740"
  },
  {
    "text": "now we had minimal tooling to automate",
    "start": "159740",
    "end": "162150"
  },
  {
    "text": "the provisioning of these auto scaling",
    "start": "162150",
    "end": "163770"
  },
  {
    "text": "groups and but we did use puppet for",
    "start": "163770",
    "end": "166770"
  },
  {
    "text": "bootstrapping node configuration and",
    "start": "166770",
    "end": "168870"
  },
  {
    "text": "also setting up prometheus nodes to",
    "start": "168870",
    "end": "171180"
  },
  {
    "text": "monitor each of the service services",
    "start": "171180",
    "end": "173450"
  },
  {
    "text": "code deployments were pushed basically",
    "start": "173450",
    "end": "176250"
  },
  {
    "text": "pushed a new version to an NFS mount and",
    "start": "176250",
    "end": "177780"
  },
  {
    "text": "in a fairly archaic way you know would",
    "start": "177780",
    "end": "180450"
  },
  {
    "text": "would trigger a service restart so as I",
    "start": "180450",
    "end": "187770"
  },
  {
    "start": "185000",
    "end": "185000"
  },
  {
    "text": "mentioned you know the first step of the",
    "start": "187770",
    "end": "189120"
  },
  {
    "text": "migration was actually building these",
    "start": "189120",
    "end": "190590"
  },
  {
    "text": "container images",
    "start": "190590",
    "end": "191910"
  },
  {
    "text": "luckily tinder had had already started a",
    "start": "191910",
    "end": "194190"
  },
  {
    "text": "push towards microservices long before",
    "start": "194190",
    "end": "195930"
  },
  {
    "text": "we started the migration but as a result",
    "start": "195930",
    "end": "198690"
  },
  {
    "text": "we had more than 30 source repositories",
    "start": "198690",
    "end": "200760"
  },
  {
    "text": "with a variety of different languages no",
    "start": "200760",
    "end": "203730"
  },
  {
    "text": "js' Java Scala go among some of the",
    "start": "203730",
    "end": "206430"
  },
  {
    "text": "biggest so what we need it to facilitate",
    "start": "206430",
    "end": "209970"
  },
  {
    "text": "the the diverse set of repositories was",
    "start": "209970",
    "end": "211950"
  },
  {
    "text": "a fully customizable built context with",
    "start": "211950",
    "end": "213989"
  },
  {
    "text": "a standardized format and also in a",
    "start": "213989",
    "end": "216480"
  },
  {
    "text": "format that our engineering teams could",
    "start": "216480",
    "end": "219000"
  },
  {
    "text": "easily understand and write and we also",
    "start": "219000",
    "end": "223650"
  },
  {
    "text": "wanted to standardize the build process",
    "start": "223650",
    "end": "225209"
  },
  {
    "text": "for deployment and production one of the",
    "start": "225209",
    "end": "228810"
  },
  {
    "text": "ways that we were able to do this was",
    "start": "228810",
    "end": "230280"
  },
  {
    "text": "create a builder container so images are",
    "start": "230280",
    "end": "232739"
  },
  {
    "text": "actually built within a container and",
    "start": "232739",
    "end": "234870"
  },
  {
    "text": "the Builder container wallet has a",
    "start": "234870",
    "end": "236220"
  },
  {
    "text": "standardized set of tools it's able to",
    "start": "236220",
    "end": "240000"
  },
  {
    "text": "tailor itself to the unique needs of the",
    "start": "240000",
    "end": "242100"
  },
  {
    "text": "image the unique requirements or",
    "start": "242100",
    "end": "243900"
  },
  {
    "text": "dependencies of the of the image that it",
    "start": "243900",
    "end": "246750"
  },
  {
    "text": "needs to build and this is our CI CD",
    "start": "246750",
    "end": "252680"
  },
  {
    "start": "250000",
    "end": "250000"
  },
  {
    "text": "platform obviously a pretty busy slide",
    "start": "252680",
    "end": "255329"
  },
  {
    "text": "so I'll kind of talk you through you",
    "start": "255329",
    "end": "258299"
  },
  {
    "text": "know water it was kind of the most",
    "start": "258299",
    "end": "259979"
  },
  {
    "text": "relevant at least to the kubernetes",
    "start": "259979",
    "end": "261599"
  },
  {
    "text": "migration but as you can see the",
    "start": "261599",
    "end": "264940"
  },
  {
    "text": "the without our CI CD pipeline the moved",
    "start": "264940",
    "end": "268480"
  },
  {
    "text": "acrimony aiders would not be possible",
    "start": "268480",
    "end": "269620"
  },
  {
    "text": "and and the complexity on this diagram",
    "start": "269620",
    "end": "272410"
  },
  {
    "text": "represents the the years of exceptional",
    "start": "272410",
    "end": "276730"
  },
  {
    "text": "engineering to be able to get to this",
    "start": "276730",
    "end": "278470"
  },
  {
    "text": "point so at the very top you have our CI",
    "start": "278470",
    "end": "281500"
  },
  {
    "text": "city repository and this is basically",
    "start": "281500",
    "end": "284020"
  },
  {
    "text": "defining what services you can build and",
    "start": "284020",
    "end": "286780"
  },
  {
    "text": "where they should be deployed and then",
    "start": "286780",
    "end": "289300"
  },
  {
    "text": "of course you have your code",
    "start": "289300",
    "end": "290350"
  },
  {
    "text": "repositories for example your your go or",
    "start": "290350",
    "end": "292960"
  },
  {
    "text": "your go repository or your Java",
    "start": "292960",
    "end": "294880"
  },
  {
    "text": "repository and in them is the make file",
    "start": "294880",
    "end": "297220"
  },
  {
    "text": "and basically the CI CD platform will",
    "start": "297220",
    "end": "300250"
  },
  {
    "text": "schedule a builder container to build a",
    "start": "300250",
    "end": "303130"
  },
  {
    "text": "specific image for whatever whatever",
    "start": "303130",
    "end": "306450"
  },
  {
    "text": "whatever application is specified as",
    "start": "306450",
    "end": "309280"
  },
  {
    "text": "part of the manifest it is so it's not",
    "start": "309280",
    "end": "313450"
  },
  {
    "text": "just Jenkins you know we use Jenkins for",
    "start": "313450",
    "end": "315160"
  },
  {
    "text": "UI and for scheduling but under the hood",
    "start": "315160",
    "end": "317380"
  },
  {
    "text": "is a robust CI CD framework that we were",
    "start": "317380",
    "end": "319930"
  },
  {
    "text": "using to do all sorts of things",
    "start": "319930",
    "end": "321040"
  },
  {
    "text": "obviously deployed a kubernetes",
    "start": "321040",
    "end": "322780"
  },
  {
    "text": "provision infrastructure using terraform",
    "start": "322780",
    "end": "324970"
  },
  {
    "text": "and in the future also you you know also",
    "start": "324970",
    "end": "327250"
  },
  {
    "text": "build into start to leverage it to",
    "start": "327250",
    "end": "329860"
  },
  {
    "text": "execute some of our puppet manifests at",
    "start": "329860",
    "end": "332920"
  },
  {
    "text": "the end of the day what ends up",
    "start": "332920",
    "end": "334060"
  },
  {
    "text": "happening is that the image that's built",
    "start": "334060",
    "end": "336490"
  },
  {
    "text": "as part of the Builder container is push",
    "start": "336490",
    "end": "338050"
  },
  {
    "text": "push to amazon's container registry and",
    "start": "338050",
    "end": "341110"
  },
  {
    "text": "then obviously the deployment object is",
    "start": "341110",
    "end": "343000"
  },
  {
    "text": "updated within kubernetes through the",
    "start": "343000",
    "end": "345220"
  },
  {
    "text": "CIC platform to point to that new image",
    "start": "345220",
    "end": "349890"
  },
  {
    "start": "353000",
    "end": "353000"
  },
  {
    "text": "so the original architecture we used a",
    "start": "353759",
    "end": "355830"
  },
  {
    "text": "cube AWS for provisioning as we are an",
    "start": "355830",
    "end": "358379"
  },
  {
    "text": "Amazon shop and that was you know kind",
    "start": "358379",
    "end": "359999"
  },
  {
    "text": "of readily available to us at that time",
    "start": "359999",
    "end": "361460"
  },
  {
    "text": "initially we had one node pool one big",
    "start": "361460",
    "end": "364020"
  },
  {
    "text": "node pool but we quickly separated into",
    "start": "364020",
    "end": "366449"
  },
  {
    "text": "different titles different sizes and",
    "start": "366449",
    "end": "368339"
  },
  {
    "text": "types and what we found is that running",
    "start": "368339",
    "end": "370620"
  },
  {
    "text": "fewer heavily threaded jaw heavily",
    "start": "370620",
    "end": "373800"
  },
  {
    "text": "threaded pods for example Java together",
    "start": "373800",
    "end": "376020"
  },
  {
    "text": "yield a better performance than having",
    "start": "376020",
    "end": "377460"
  },
  {
    "text": "them co-locate with single single thread",
    "start": "377460",
    "end": "379169"
  },
  {
    "text": "at workloads eventually we settled on a",
    "start": "379169",
    "end": "383809"
  },
  {
    "text": "combination of c-5 4x l and c 5 to excel",
    "start": "383809",
    "end": "389629"
  },
  {
    "text": "instances for our various various",
    "start": "389629",
    "end": "392249"
  },
  {
    "text": "components so control plane masters at",
    "start": "392249",
    "end": "394110"
  },
  {
    "text": "CD our single thread at worker pool in",
    "start": "394110",
    "end": "396659"
  },
  {
    "text": "our multi-threaded worker blue pool in",
    "start": "396659",
    "end": "398580"
  },
  {
    "text": "Java and go",
    "start": "398580",
    "end": "400080"
  },
  {
    "text": "and we also settled on a more memory so",
    "start": "400080",
    "end": "404580"
  },
  {
    "text": "an m5 instance site for our memory",
    "start": "404580",
    "end": "406499"
  },
  {
    "text": "intense applications for example are",
    "start": "406499",
    "end": "407819"
  },
  {
    "text": "monitoring stack so the actual migration",
    "start": "407819",
    "end": "413930"
  },
  {
    "start": "411000",
    "end": "411000"
  },
  {
    "text": "basically what we did is we changed",
    "start": "413930",
    "end": "416539"
  },
  {
    "text": "existing service to service calls within",
    "start": "416539",
    "end": "418800"
  },
  {
    "text": "the tinder platform to to new load",
    "start": "418800",
    "end": "420270"
  },
  {
    "text": "balancers we basically peered the",
    "start": "420270",
    "end": "424129"
  },
  {
    "text": "existing vp c to the new kubernetes vp c",
    "start": "424129",
    "end": "428219"
  },
  {
    "text": "and amazon and this allowed us to",
    "start": "428219",
    "end": "430740"
  },
  {
    "text": "granularly migrate modules with no",
    "start": "430740",
    "end": "433139"
  },
  {
    "text": "regard to order or dependency chain",
    "start": "433139",
    "end": "435379"
  },
  {
    "text": "endpoints use wait at dns records with c",
    "start": "435379",
    "end": "438029"
  },
  {
    "text": "name to the new ELB and this afforded us",
    "start": "438029",
    "end": "440370"
  },
  {
    "text": "an opportunity to to control the volume",
    "start": "440370",
    "end": "442919"
  },
  {
    "text": "of traffic and also facilitate this the",
    "start": "442919",
    "end": "444749"
  },
  {
    "text": "opportunity to roll back if we had any",
    "start": "444749",
    "end": "446309"
  },
  {
    "text": "issues for the migration obviously since",
    "start": "446309",
    "end": "450839"
  },
  {
    "text": "we were using DNS TTL was lowered and",
    "start": "450839",
    "end": "452849"
  },
  {
    "text": "weight was adjusted to to be able to",
    "start": "452849",
    "end": "455490"
  },
  {
    "text": "very grandly adjust and on-the-fly",
    "start": "455490",
    "end": "457529"
  },
  {
    "text": "adjust the the traffic volumes for these",
    "start": "457529",
    "end": "459809"
  },
  {
    "text": "these modules what we found is that our",
    "start": "459809",
    "end": "462270"
  },
  {
    "text": "Java repositories honored the low TTL",
    "start": "462270",
    "end": "464219"
  },
  {
    "text": "but node but our nodejs repository did",
    "start": "464219",
    "end": "466259"
  },
  {
    "text": "not and so what we had to do is spend",
    "start": "466259",
    "end": "468360"
  },
  {
    "text": "some engineering effort to rework our",
    "start": "468360",
    "end": "469919"
  },
  {
    "text": "connection pool so that it would refresh",
    "start": "469919",
    "end": "472289"
  },
  {
    "text": "those connections every 60 seconds and",
    "start": "472289",
    "end": "474240"
  },
  {
    "text": "get the latest latest resolution for",
    "start": "474240",
    "end": "477360"
  },
  {
    "text": "that particular host name so learnings",
    "start": "477360",
    "end": "482339"
  },
  {
    "start": "482000",
    "end": "482000"
  },
  {
    "text": "you have to bear with me we're gonna go",
    "start": "482339",
    "end": "483810"
  },
  {
    "text": "a little bit into the to the technical",
    "start": "483810",
    "end": "486269"
  },
  {
    "text": "weeds for a couple of slides",
    "start": "486269",
    "end": "487740"
  },
  {
    "text": "we had a series of fairly painful",
    "start": "487740",
    "end": "490169"
  },
  {
    "text": "outages in January and also earlier yeah",
    "start": "490169",
    "end": "493139"
  },
  {
    "text": "you know early in earlier this year and",
    "start": "493139",
    "end": "496289"
  },
  {
    "text": "it was kind of the results of kind of -",
    "start": "496289",
    "end": "498060"
  },
  {
    "text": "two key things that unfolded so the",
    "start": "498060",
    "end": "501600"
  },
  {
    "text": "first thing was our ARP table entries so",
    "start": "501600",
    "end": "505560"
  },
  {
    "text": "basically January January 8th 2019 we",
    "start": "505560",
    "end": "507960"
  },
  {
    "text": "were down for several hours and it was",
    "start": "507960",
    "end": "509820"
  },
  {
    "text": "basically it was basically the result of",
    "start": "509820",
    "end": "513089"
  },
  {
    "text": "an unrelated scale up earlier in the day",
    "start": "513089",
    "end": "514890"
  },
  {
    "text": "that left the cluster at a larger size",
    "start": "514890",
    "end": "516419"
  },
  {
    "text": "than ever before as a result the you",
    "start": "516419",
    "end": "520050"
  },
  {
    "text": "know there the the ARP table ran out of",
    "start": "520050",
    "end": "522180"
  },
  {
    "text": "available entries right so when once one",
    "start": "522180",
    "end": "525540"
  },
  {
    "text": "spot and note counts reach a certain",
    "start": "525540",
    "end": "526830"
  },
  {
    "text": "point and so this resulted in drop",
    "start": "526830",
    "end": "529260"
  },
  {
    "text": "packets and and a entire slash 24",
    "start": "529260",
    "end": "533040"
  },
  {
    "text": "flannel address space is missing from",
    "start": "533040",
    "end": "535200"
  },
  {
    "text": "the arc tables we were at that time we",
    "start": "535200",
    "end": "536640"
  },
  {
    "text": "were using flannel for our our",
    "start": "536640",
    "end": "538560"
  },
  {
    "text": "networking so as a result you know we",
    "start": "538560",
    "end": "541529"
  },
  {
    "text": "had to raise some of the values by it's",
    "start": "541529",
    "end": "543330"
  },
  {
    "text": "a CTL on the nodes themselves right to",
    "start": "543330",
    "end": "545520"
  },
  {
    "text": "expand the size of that ARP table or ARP",
    "start": "545520",
    "end": "548670"
  },
  {
    "text": "cache and we restarted flannel all the",
    "start": "548670",
    "end": "550680"
  },
  {
    "text": "nodes on all the nodes and then finally",
    "start": "550680",
    "end": "555810"
  },
  {
    "text": "the other one of the other challenges we",
    "start": "555810",
    "end": "557279"
  },
  {
    "text": "worked our DNS timeouts so our",
    "start": "557279",
    "end": "558870"
  },
  {
    "text": "engineering teams were constantly",
    "start": "558870",
    "end": "560250"
  },
  {
    "text": "complaining about error rates at least",
    "start": "560250",
    "end": "562529"
  },
  {
    "text": "for some of our bigger modules on it",
    "start": "562529",
    "end": "565080"
  },
  {
    "text": "just general error rates they couldn't",
    "start": "565080",
    "end": "566279"
  },
  {
    "text": "they just would get these strange could",
    "start": "566279",
    "end": "568560"
  },
  {
    "text": "not connect to other service right could",
    "start": "568560",
    "end": "570180"
  },
  {
    "text": "not resolve the DNS for a particular",
    "start": "570180",
    "end": "571740"
  },
  {
    "text": "service endpoint and this is you know",
    "start": "571740",
    "end": "574860"
  },
  {
    "text": "what we've we actually stumbled upon a",
    "start": "574860",
    "end": "576240"
  },
  {
    "text": "number of our a number of people within",
    "start": "576240",
    "end": "578160"
  },
  {
    "text": "the industry that had faced a similar",
    "start": "578160",
    "end": "580080"
  },
  {
    "text": "similar situation and basically there's",
    "start": "580080",
    "end": "582450"
  },
  {
    "text": "a basically it has to do with a race",
    "start": "582450",
    "end": "584580"
  },
  {
    "text": "condition that happens within the",
    "start": "584580",
    "end": "586040"
  },
  {
    "text": "connection track module within the Linux",
    "start": "586040",
    "end": "589200"
  },
  {
    "text": "kernel that you know is the direct",
    "start": "589200",
    "end": "591870"
  },
  {
    "text": "result of source net and destination",
    "start": "591870",
    "end": "594330"
  },
  {
    "text": "that and the issue rate issues were",
    "start": "594330",
    "end": "597570"
  },
  {
    "text": "amplified by many by the amount of",
    "start": "597570",
    "end": "600120"
  },
  {
    "text": "lookups that we were doing on our for",
    "start": "600120",
    "end": "602520"
  },
  {
    "text": "DNS within our cluster so we actually I",
    "start": "602520",
    "end": "605640"
  },
  {
    "text": "mean it the first case we just thought",
    "start": "605640",
    "end": "606839"
  },
  {
    "text": "it was a matter of scale you know we",
    "start": "606839",
    "end": "609089"
  },
  {
    "text": "peaked at something like 250,000 DNS",
    "start": "609089",
    "end": "611490"
  },
  {
    "text": "requests a second and 128 which resulted",
    "start": "611490",
    "end": "614310"
  },
  {
    "text": "in 120 cores of usage over a thousand",
    "start": "614310",
    "end": "616620"
  },
  {
    "text": "core DNS pods but it became clear to us",
    "start": "616620",
    "end": "619230"
  },
  {
    "text": "that no matter how many",
    "start": "619230",
    "end": "621390"
  },
  {
    "text": "pods DNS pods we threw at this",
    "start": "621390",
    "end": "623880"
  },
  {
    "text": "particular problem it just wasn't",
    "start": "623880",
    "end": "624900"
  },
  {
    "text": "getting any better and that's kind of",
    "start": "624900",
    "end": "626730"
  },
  {
    "text": "when we stumbled upon the article and",
    "start": "626730",
    "end": "628380"
  },
  {
    "text": "some of the various articles that of",
    "start": "628380",
    "end": "630030"
  },
  {
    "text": "other other industry people that were",
    "start": "630030",
    "end": "632100"
  },
  {
    "text": "that were suffering through the same",
    "start": "632100",
    "end": "633420"
  },
  {
    "text": "same issues that we were and so what we",
    "start": "633420",
    "end": "635940"
  },
  {
    "text": "did is what we did is we kind of took",
    "start": "635940",
    "end": "638550"
  },
  {
    "text": "the the race condition or the issue with",
    "start": "638550",
    "end": "640680"
  },
  {
    "text": "with s net and D not added the equation",
    "start": "640680",
    "end": "642690"
  },
  {
    "text": "you know we redeployed accordion s as a",
    "start": "642690",
    "end": "645360"
  },
  {
    "text": "daemon set and injected the the note IP",
    "start": "645360",
    "end": "648000"
  },
  {
    "text": "into resolve Kampf so that that a pod or",
    "start": "648000",
    "end": "651780"
  },
  {
    "text": "a containers first lookup would be on",
    "start": "651780",
    "end": "654060"
  },
  {
    "text": "the the node itself it would not be on",
    "start": "654060",
    "end": "657120"
  },
  {
    "text": "it would not fall back to some service",
    "start": "657120",
    "end": "659730"
  },
  {
    "text": "that resulted in estimate and D net",
    "start": "659730",
    "end": "662160"
  },
  {
    "text": "translation and with that I'll go ahead",
    "start": "662160",
    "end": "667230"
  },
  {
    "start": "667000",
    "end": "667000"
  },
  {
    "text": "and turn it over to to Chris thank you",
    "start": "667230",
    "end": "669870"
  },
  {
    "text": "so I'll go over some of our load",
    "start": "669870",
    "end": "674220"
  },
  {
    "text": "balancing and performance issues we came",
    "start": "674220",
    "end": "677070"
  },
  {
    "text": "across when we first migrated initially",
    "start": "677070",
    "end": "680280"
  },
  {
    "text": "we had unbalanced load across the pods",
    "start": "680280",
    "end": "682050"
  },
  {
    "text": "due to ELB connections sticking to the",
    "start": "682050",
    "end": "684870"
  },
  {
    "text": "first set of ready pause that would come",
    "start": "684870",
    "end": "686430"
  },
  {
    "text": "out of each deployment and this was",
    "start": "686430",
    "end": "690750"
  },
  {
    "text": "mostly due to keep alive and the way EEO",
    "start": "690750",
    "end": "692580"
  },
  {
    "text": "bees keep connections open to nodes so",
    "start": "692580",
    "end": "695430"
  },
  {
    "text": "we we attempted multiple mitigations one",
    "start": "695430",
    "end": "699720"
  },
  {
    "text": "was very brute kind of crude but max",
    "start": "699720",
    "end": "703050"
  },
  {
    "text": "surged to a hundred percent so you'd",
    "start": "703050",
    "end": "704610"
  },
  {
    "text": "hope that you know most of your pods",
    "start": "704610",
    "end": "705960"
  },
  {
    "text": "will come up at the same time we",
    "start": "705960",
    "end": "708330"
  },
  {
    "text": "inflated resource requests which also",
    "start": "708330",
    "end": "710250"
  },
  {
    "text": "helped a bit but in in ewa tably you'd",
    "start": "710250",
    "end": "713730"
  },
  {
    "text": "have some pods way hotter than others so",
    "start": "713730",
    "end": "716670"
  },
  {
    "text": "we had some internal pocs for envoy",
    "start": "716670",
    "end": "718980"
  },
  {
    "text": "which proved to be successful and this",
    "start": "718980",
    "end": "721080"
  },
  {
    "text": "gave us a chance to leverage it in a",
    "start": "721080",
    "end": "722910"
  },
  {
    "text": "limited fashion so we deployed side to",
    "start": "722910",
    "end": "726540"
  },
  {
    "text": "envoy side cars alongside each heavy",
    "start": "726540",
    "end": "729360"
  },
  {
    "text": "service and so it resulted in a small",
    "start": "729360",
    "end": "732690"
  },
  {
    "text": "fleet of envoy proxies running in front",
    "start": "732690",
    "end": "735420"
  },
  {
    "text": "of these services that were most",
    "start": "735420",
    "end": "737610"
  },
  {
    "text": "affected and those were deployed with",
    "start": "737610",
    "end": "740700"
  },
  {
    "text": "one single deployment in each AZ just to",
    "start": "740700",
    "end": "743850"
  },
  {
    "text": "ensure that we had even distribution of",
    "start": "743850",
    "end": "746580"
  },
  {
    "text": "pods without skewing 1az too too far in",
    "start": "746580",
    "end": "750870"
  },
  {
    "text": "one direction in each of those",
    "start": "750870",
    "end": "753960"
  },
  {
    "text": "small proxy layers was fronted by a TCP",
    "start": "753960",
    "end": "758140"
  },
  {
    "text": "ELB so one of the ways we got this to",
    "start": "758140",
    "end": "761620"
  },
  {
    "text": "work very smoothly was to put a pre stop",
    "start": "761620",
    "end": "763930"
  },
  {
    "text": "hook on the sidecar which calls the",
    "start": "763930",
    "end": "766780"
  },
  {
    "text": "Envoy health check failed admin endpoint",
    "start": "766780",
    "end": "769420"
  },
  {
    "text": "as well as a small I think 10 10 second",
    "start": "769420",
    "end": "772780"
  },
  {
    "text": "sleep to allow inflate connections to",
    "start": "772780",
    "end": "774640"
  },
  {
    "text": "complete and drain and that proves very",
    "start": "774640",
    "end": "778110"
  },
  {
    "text": "stable so on poi itself I'm sure many of",
    "start": "778110",
    "end": "784060"
  },
  {
    "text": "you have seen it or used it so this is",
    "start": "784060",
    "end": "786280"
  },
  {
    "text": "just an example of how you could see the",
    "start": "786280",
    "end": "788470"
  },
  {
    "text": "CPU convergence on one of our services",
    "start": "788470",
    "end": "791770"
  },
  {
    "text": "from the time that which we we cut it",
    "start": "791770",
    "end": "794560"
  },
  {
    "text": "over from running on the old you know",
    "start": "794560",
    "end": "797680"
  },
  {
    "text": "service endpoint in kubernetes to one of",
    "start": "797680",
    "end": "800440"
  },
  {
    "text": "these Envoy fronted ones so you can see",
    "start": "800440",
    "end": "803560"
  },
  {
    "text": "some of them are as little as 25% of",
    "start": "803560",
    "end": "806200"
  },
  {
    "text": "their requested CPU all the way up to",
    "start": "806200",
    "end": "808120"
  },
  {
    "text": "you know nearly 200 percent and as we",
    "start": "808120",
    "end": "810910"
  },
  {
    "text": "cut it over slowly they converged into a",
    "start": "810910",
    "end": "812980"
  },
  {
    "text": "very very close point this is kind of an",
    "start": "812980",
    "end": "820060"
  },
  {
    "text": "example of the service flow so you'd see",
    "start": "820060",
    "end": "822010"
  },
  {
    "text": "the main proxy layer coming in and",
    "start": "822010",
    "end": "824140"
  },
  {
    "text": "hitting a TLB and then having specific",
    "start": "824140",
    "end": "826870"
  },
  {
    "text": "service pods get hotter than others but",
    "start": "826870",
    "end": "829030"
  },
  {
    "text": "what the net result would be after this",
    "start": "829030",
    "end": "830770"
  },
  {
    "text": "is you may have some envoy pods be",
    "start": "830770",
    "end": "833230"
  },
  {
    "text": "hotter than others but they're much more",
    "start": "833230",
    "end": "835240"
  },
  {
    "text": "suited to be able to handle more traffic",
    "start": "835240",
    "end": "838450"
  },
  {
    "text": "than not and then equally distributed",
    "start": "838450",
    "end": "840850"
  },
  {
    "text": "using the least request algorithm to the",
    "start": "840850",
    "end": "843910"
  },
  {
    "text": "service pods so our current cluster",
    "start": "843910",
    "end": "849430"
  },
  {
    "start": "847000",
    "end": "847000"
  },
  {
    "text": "architecture right now we're around 2000",
    "start": "849430",
    "end": "851740"
  },
  {
    "text": "nodes",
    "start": "851740",
    "end": "852210"
  },
  {
    "text": "18,000 cores or so we have 6 control",
    "start": "852210",
    "end": "855610"
  },
  {
    "text": "plane masters 25 to 30,000 pods on any",
    "start": "855610",
    "end": "859660"
  },
  {
    "text": "given moments depending on how the auto",
    "start": "859660",
    "end": "861850"
  },
  {
    "text": "scaling is working 115 to 130,000",
    "start": "861850",
    "end": "866080"
  },
  {
    "text": "containers as we run many sidecars on",
    "start": "866080",
    "end": "868150"
  },
  {
    "text": "each pod many meaning anywhere from 2 to",
    "start": "868150",
    "end": "872680"
  },
  {
    "text": "6 our Prometheus stack ingests around",
    "start": "872680",
    "end": "876600"
  },
  {
    "text": "750,000 samples per second we have a",
    "start": "876600",
    "end": "879940"
  },
  {
    "text": "roughly 5 terabytes per day of log",
    "start": "879940",
    "end": "882700"
  },
  {
    "text": "ingestion into our elk stack",
    "start": "882700",
    "end": "885580"
  },
  {
    "text": "and we're currently in the middle of an",
    "start": "885580",
    "end": "888550"
  },
  {
    "text": "envoy service mesh migration I'd say we",
    "start": "888550",
    "end": "891580"
  },
  {
    "text": "probably have 10% of our services on",
    "start": "891580",
    "end": "894279"
  },
  {
    "text": "service mesh here's a diagram of our old",
    "start": "894279",
    "end": "899560"
  },
  {
    "start": "897000",
    "end": "897000"
  },
  {
    "text": "monitoring stack and the reason I bring",
    "start": "899560",
    "end": "901209"
  },
  {
    "text": "this up is it quickly hit limitations so",
    "start": "901209",
    "end": "906550"
  },
  {
    "text": "you can see we have our services",
    "start": "906550",
    "end": "908560"
  },
  {
    "text": "separated in two distinct namespaces for",
    "start": "908560",
    "end": "911200"
  },
  {
    "text": "each one and they each ran their own",
    "start": "911200",
    "end": "913170"
  },
  {
    "text": "Prometheus pod or actually two which",
    "start": "913170",
    "end": "916600"
  },
  {
    "text": "would then write to a long-term",
    "start": "916600",
    "end": "918579"
  },
  {
    "text": "Prometheus storage which is what the",
    "start": "918579",
    "end": "923920"
  },
  {
    "text": "graph fauna server actually hits so each",
    "start": "923920",
    "end": "926890"
  },
  {
    "text": "of these you know orange and green boxes",
    "start": "926890",
    "end": "929529"
  },
  {
    "text": "basically you'd have to vertically scale",
    "start": "929529",
    "end": "931269"
  },
  {
    "text": "them and it didn't work very well after",
    "start": "931269",
    "end": "932800"
  },
  {
    "text": "a certain point so the new monitoring",
    "start": "932800",
    "end": "935860"
  },
  {
    "text": "stack we've got is actually based with",
    "start": "935860",
    "end": "938709"
  },
  {
    "text": "an Thanos and the pods actually run a",
    "start": "938709",
    "end": "942310"
  },
  {
    "text": "side car that drop all of the excess",
    "start": "942310",
    "end": "945279"
  },
  {
    "text": "metrics that we don't need so we have",
    "start": "945279",
    "end": "947860"
  },
  {
    "text": "our developers specifically put in rules",
    "start": "947860",
    "end": "950230"
  },
  {
    "text": "for roll-ups and things in their module",
    "start": "950230",
    "end": "952930"
  },
  {
    "text": "manifests that will tell the side car",
    "start": "952930",
    "end": "956140"
  },
  {
    "text": "exactly what to collect and everything",
    "start": "956140",
    "end": "958000"
  },
  {
    "text": "else is dropped and then we have two",
    "start": "958000",
    "end": "962170"
  },
  {
    "text": "Prometheus pods per namespace just for",
    "start": "962170",
    "end": "964540"
  },
  {
    "text": "redundancy and then those have a Thanos",
    "start": "964540",
    "end": "966820"
  },
  {
    "text": "side car running on them and those right",
    "start": "966820",
    "end": "969160"
  },
  {
    "text": "to s3 as well as the the fan of stores",
    "start": "969160",
    "end": "973329"
  },
  {
    "text": "are then capable of reading the the long",
    "start": "973329",
    "end": "976029"
  },
  {
    "text": "term data out of s3 so then the the",
    "start": "976029",
    "end": "978820"
  },
  {
    "text": "query layer is what Gravano talks to and",
    "start": "978820",
    "end": "982890"
  },
  {
    "text": "it effectively scales much more",
    "start": "982890",
    "end": "985540"
  },
  {
    "text": "horizontally than before and we also run",
    "start": "985540",
    "end": "988510"
  },
  {
    "text": "a Prometheus vertical autoscaler for the",
    "start": "988510",
    "end": "990610"
  },
  {
    "text": "times where an individual service needs",
    "start": "990610",
    "end": "994029"
  },
  {
    "text": "to be scaled up a little bit to handle",
    "start": "994029",
    "end": "995770"
  },
  {
    "text": "maybe some new metrics that the",
    "start": "995770",
    "end": "997029"
  },
  {
    "text": "developers put in this is in a diagram",
    "start": "997029",
    "end": "1002940"
  },
  {
    "text": "of our future architecture so instead of",
    "start": "1002940",
    "end": "1004949"
  },
  {
    "text": "having one cluster spanning multiple",
    "start": "1004949",
    "end": "1007260"
  },
  {
    "text": "AZ's we're actually going to create a",
    "start": "1007260",
    "end": "1010320"
  },
  {
    "text": "cluster in four different AZ's this is",
    "start": "1010320",
    "end": "1013860"
  },
  {
    "text": "in the works right now it is not fully",
    "start": "1013860",
    "end": "1016620"
  },
  {
    "text": "out there yet but this is",
    "start": "1016620",
    "end": "1019110"
  },
  {
    "text": "basically what it'll look like and the",
    "start": "1019110",
    "end": "1021899"
  },
  {
    "text": "only real change that we need to make of",
    "start": "1021899",
    "end": "1023519"
  },
  {
    "text": "course is from the CI CD layer to be",
    "start": "1023519",
    "end": "1025558"
  },
  {
    "text": "able to deploy to four different",
    "start": "1025559",
    "end": "1028620"
  },
  {
    "text": "clusters for one deployment instead of",
    "start": "1028620",
    "end": "1030420"
  },
  {
    "text": "one as well as breaking out some of the",
    "start": "1030420",
    "end": "1033089"
  },
  {
    "text": "Prometheus aggregation into ec2",
    "start": "1033089",
    "end": "1035970"
  },
  {
    "text": "instances that will allow us to swap",
    "start": "1035970",
    "end": "1038640"
  },
  {
    "text": "these clusters in and out upgrade them",
    "start": "1038640",
    "end": "1040678"
  },
  {
    "text": "and so forth without having to worry",
    "start": "1040679",
    "end": "1043260"
  },
  {
    "text": "about losing metrics or having the",
    "start": "1043260",
    "end": "1045058"
  },
  {
    "text": "metrics tied into the cluster directly",
    "start": "1045059",
    "end": "1047188"
  },
  {
    "text": "and of course all of that then you know",
    "start": "1047189",
    "end": "1049620"
  },
  {
    "text": "writes and reads from various Amazon",
    "start": "1049620",
    "end": "1052020"
  },
  {
    "text": "services and that's pretty much it thank",
    "start": "1052020",
    "end": "1055770"
  },
  {
    "text": "you yeah",
    "start": "1055770",
    "end": "1056990"
  },
  {
    "text": "[Applause]",
    "start": "1056990",
    "end": "1060849"
  }
]