[
  {
    "text": "well thanks everyone for attending this is a huge crowd so really appreciate you",
    "start": "539",
    "end": "6480"
  },
  {
    "text": "guys shown up my name is Chris nutria I run site reliability engineering at",
    "start": "6480",
    "end": "11519"
  },
  {
    "text": "Shearson Shearson is a data science company headquartered in New York City and we do qualitative quantitative",
    "start": "11519",
    "end": "19310"
  },
  {
    "text": "market research as well as data science and media optimization long story short",
    "start": "19310",
    "end": "25619"
  },
  {
    "text": "we help our clients with a variety of different things in marketing and advertising you can find me on github",
    "start": "25619",
    "end": "32940"
  },
  {
    "text": "you can find me on Twitter links here and also after this presentation you have any questions or anything like that",
    "start": "32940",
    "end": "38280"
  },
  {
    "text": "you you know where to find me just I'll be wandering around I want to talk",
    "start": "38280",
    "end": "44579"
  },
  {
    "text": "briefly about Shearson and kind of the journey we've had they've been around for about 10 years I've been with",
    "start": "44579",
    "end": "50280"
  },
  {
    "text": "Shearson for about nine months now and they for the last year or so there's",
    "start": "50280",
    "end": "56010"
  },
  {
    "text": "been a transition from a services organization to a product organization what that really means is that we have",
    "start": "56010",
    "end": "63000"
  },
  {
    "text": "data scientists that typically would put this work together and send it off to a client and our job as engineers has been",
    "start": "63000",
    "end": "70950"
  },
  {
    "text": "to build the data science processes and actually make them a platform that is a service for the clients to be able to",
    "start": "70950",
    "end": "77250"
  },
  {
    "text": "run the data science workloads themselves so hard reason here because",
    "start": "77250",
    "end": "82590"
  },
  {
    "text": "we are running kubernetes we're using a lot of neat technology and due to the",
    "start": "82590",
    "end": "88770"
  },
  {
    "text": "advertising industry and kind of not really having any regulations we can pick and choose the different types of",
    "start": "88770",
    "end": "93990"
  },
  {
    "text": "tech that we want to work with which is really neat so there were a lot of",
    "start": "93990",
    "end": "99860"
  },
  {
    "text": "interesting requirements that we had when we were moving data science code that typically lived in a Jupiter",
    "start": "99860",
    "end": "106560"
  },
  {
    "text": "notebook into a container so that was a initial hurdle of thinking of data",
    "start": "106560",
    "end": "113670"
  },
  {
    "text": "scientists writing things to their hard disk on the laptop and then using that putting that into volumes that that we",
    "start": "113670",
    "end": "119579"
  },
  {
    "text": "had to do a lot of that kind of retrofitting for for those processes also in order to get our clients to",
    "start": "119579",
    "end": "127409"
  },
  {
    "text": "actually use this instead of calling the data scientist we had to achieve at least similar performance to what a data",
    "start": "127409",
    "end": "133950"
  },
  {
    "text": "Tess was running on a laptop so they had big beefy desktops that we'd be basically competing with with the",
    "start": "133950",
    "end": "139890"
  },
  {
    "text": "kubernetes clusters that was another interesting challenge and also as I",
    "start": "139890",
    "end": "145500"
  },
  {
    "text": "mentioned before providing data science as a service to clients so we had to make it intuitive we had to make it easy",
    "start": "145500",
    "end": "151770"
  },
  {
    "text": "they had to understand what they were doing you know as they were adjusting different parameters or choosing different data sources so these are all",
    "start": "151770",
    "end": "158520"
  },
  {
    "text": "things that kind of came in to how we started moving these data science",
    "start": "158520",
    "end": "163560"
  },
  {
    "text": "workloads into a production grade cluster so our first attempt that this was well let's try to put this into a",
    "start": "163560",
    "end": "170010"
  },
  {
    "text": "deployment that did not work out too well as you can see there's a lot of",
    "start": "170010",
    "end": "175350"
  },
  {
    "text": "spike enos with data science processes we had a lot of evicted pods we had a lot of killed nodes it took us a long",
    "start": "175350",
    "end": "183330"
  },
  {
    "text": "long time to realize that the traditional way of deploying an application was not gonna work for data",
    "start": "183330",
    "end": "188850"
  },
  {
    "text": "science also you can see from here this is actually memory in our GU fauna it",
    "start": "188850",
    "end": "194549"
  },
  {
    "text": "takes up a lot of memory so we also realized that isolation was pretty important we weren't able we didn't want",
    "start": "194549",
    "end": "200640"
  },
  {
    "text": "to run or have the have an issue of evicting different api's or back-end processes because the data science",
    "start": "200640",
    "end": "207570"
  },
  {
    "text": "processes were going to take up all the memory and resources and potentially even bring down nodes so we came up with",
    "start": "207570",
    "end": "215190"
  },
  {
    "text": "additional functional requirements that we had to take care of we now know that CPU memory utilization was",
    "start": "215190",
    "end": "221370"
  },
  {
    "text": "non-deterministic the different data science processes that were running had different parameters different data sets",
    "start": "221370",
    "end": "227220"
  },
  {
    "text": "we needed to account for those things as we were moving this into production they",
    "start": "227220",
    "end": "232859"
  },
  {
    "text": "were long running and often resource-hungry and we didn't know how hungry they would end up being so there was also",
    "start": "232859",
    "end": "238680"
  },
  {
    "text": "additional considerations we had to have for that also being in a small SRE team",
    "start": "238680",
    "end": "244380"
  },
  {
    "text": "of three servicing about 30 engineers and 20 to 25 data scientists we also",
    "start": "244380",
    "end": "250859"
  },
  {
    "text": "wanted to see how close we could get to our existing SDLC because if we had to write a separate one for this that would",
    "start": "250859",
    "end": "257220"
  },
  {
    "text": "be a lot of cognitive load on us and more importantly a lot of toil and that's what we're trying to avoid as an",
    "start": "257220",
    "end": "262530"
  },
  {
    "text": "SRE organization but also how is this going to work in a live environment what happens when our processes start to",
    "start": "262530",
    "end": "269600"
  },
  {
    "text": "crash or you know we needed to account for those types of situations so as you",
    "start": "269600",
    "end": "277610"
  },
  {
    "text": "might have been able to guess we actually ended up going with daemon sets part of this was is that we actually",
    "start": "277610",
    "end": "283580"
  },
  {
    "text": "retain a lot of the orchestration capabilities that we have the rest of our software so applications you know",
    "start": "283580",
    "end": "289130"
  },
  {
    "text": "micro services and so forth went through the similar process of updating doing",
    "start": "289130",
    "end": "295010"
  },
  {
    "text": "rolling updates pushing new containers reverting rolling back so that worked",
    "start": "295010",
    "end": "301190"
  },
  {
    "text": "out nicely for us but also we gave each data science process its own resources",
    "start": "301190",
    "end": "306530"
  },
  {
    "text": "with some light scheduling overhead and maybe some other daemon sets like fluent D that were running on the system to get",
    "start": "306530",
    "end": "312200"
  },
  {
    "text": "logs so now and also what's nice about this is that our scaling currency is",
    "start": "312200",
    "end": "317840"
  },
  {
    "text": "infrastructure based so for us to add more or you know turndown how many data",
    "start": "317840",
    "end": "323960"
  },
  {
    "text": "science workloads were running at any given time we were essentially just adding new nodes to instance groups and",
    "start": "323960",
    "end": "330050"
  },
  {
    "text": "kubernetes and the rest of the process was taken care of pretty seamlessly and",
    "start": "330050",
    "end": "336140"
  },
  {
    "text": "this is kind of just a short example I spent up in a femoral cluster just to kind of explain this so the you know",
    "start": "336140",
    "end": "342440"
  },
  {
    "text": "here's a sample one master lots of worker nodes and we would use a node select there just isolate resources and",
    "start": "342440",
    "end": "349460"
  },
  {
    "text": "the daemon set you can use a node selector to say I only want these nodes",
    "start": "349460",
    "end": "355039"
  },
  {
    "text": "to have this run and then our back-end API nodes would just have the same notes",
    "start": "355039",
    "end": "360590"
  },
  {
    "text": "like there but for the API so as you can see here with the pods underneath these data science processes just essentially",
    "start": "360590",
    "end": "367130"
  },
  {
    "text": "had remove running on those five nodes that are the ones that are labeled here so this ended up working out great and",
    "start": "367130",
    "end": "373750"
  },
  {
    "text": "kind of that led us to where we are today we have the data science processes",
    "start": "373750",
    "end": "381530"
  },
  {
    "text": "running as daemon sets we have Python and go micro services that are backed by a Ambassador API gateway which is using",
    "start": "381530",
    "end": "388760"
  },
  {
    "text": "envoy underneath we have Prometheus core fauna elk and view jeaious nginx front-end also",
    "start": "388760",
    "end": "396279"
  },
  {
    "text": "some middleware as well as helm and fluency as part of our logging the we",
    "start": "396279",
    "end": "402489"
  },
  {
    "text": "also have some managed databases such as redshift RDS s3 and we've been pretty",
    "start": "402489",
    "end": "407679"
  },
  {
    "text": "successful with airflow as far as data processing and pipelines but this is",
    "start": "407679",
    "end": "414189"
  },
  {
    "text": "great this solves I would say half the problem the other half of the problem is is how do we scale and there's three",
    "start": "414189",
    "end": "421659"
  },
  {
    "text": "major considerations here we don't want the person doing the scaling a human because we're gonna burn out feel like a",
    "start": "421659",
    "end": "430239"
  },
  {
    "text": "better term also I can't just spin up 200 nodes to run data science because my boss is gonna call and say why is the",
    "start": "430239",
    "end": "436299"
  },
  {
    "text": "bill so high I'm sure public cloud vendors don't mind that but I'm sure our company does but",
    "start": "436299",
    "end": "443110"
  },
  {
    "text": "also to is we need to provide a great experience again as I mentioned in the beginning the requirements that have",
    "start": "443110",
    "end": "450159"
  },
  {
    "text": "been you know set forth but you know we don't want our client calling data scientist saying this is too slow can",
    "start": "450159",
    "end": "455319"
  },
  {
    "text": "you just run this for me that defeats the purpose of building the platform so",
    "start": "455319",
    "end": "460989"
  },
  {
    "text": "we do have data we are running Prometheus and as we were looking at",
    "start": "460989",
    "end": "465999"
  },
  {
    "text": "some of our data science nodes we can BC know places where we can actually scale",
    "start": "465999",
    "end": "471039"
  },
  {
    "text": "down but the challenge here is is that the time slice will you go from needing",
    "start": "471039",
    "end": "477249"
  },
  {
    "text": "a node to having one is very impactful because you're still waiting to be able",
    "start": "477249",
    "end": "482919"
  },
  {
    "text": "to take on that next data science workload so we actually need it to be about 10 15 minutes ahead of knowing",
    "start": "482919",
    "end": "490539"
  },
  {
    "text": "when we needed to have another node spun up to handle another data science workload so you know as you can see here",
    "start": "490539",
    "end": "496449"
  },
  {
    "text": "this is a dace add a time slice of the CPU utilization and we kind of came to",
    "start": "496449",
    "end": "501489"
  },
  {
    "text": "the conclusion that we could probably scale down some of our resources here and save some money especially when you",
    "start": "501489",
    "end": "506709"
  },
  {
    "text": "think of hourly prices and also some of the licensing that we have first different algorithms that run so our",
    "start": "506709",
    "end": "514809"
  },
  {
    "text": "solution essentially was to use timeseriesforecasting we had this great time series database Prometheus and",
    "start": "514809",
    "end": "520149"
  },
  {
    "text": "we're actually using it to anticipate changes to key metrics so our SRE team",
    "start": "520149",
    "end": "525250"
  },
  {
    "text": "can write size infrastructure provide more redundancy during peak times so if you know a clients on the west coast",
    "start": "525250",
    "end": "532269"
  },
  {
    "text": "we know that we just scale up resources there instead of us East and also reduce cost during non busy periods so",
    "start": "532269",
    "end": "540750"
  },
  {
    "text": "employing that we have a collection of data scientists I took this problem to them and said what's the best way that",
    "start": "540750",
    "end": "547899"
  },
  {
    "text": "we can utilize all of this data and they were really excited about it so we ended up going back and forth and to be fair",
    "start": "547899",
    "end": "555160"
  },
  {
    "text": "there's a lot of different ways you can look at this data and model it there's a ton of really great machine learning",
    "start": "555160",
    "end": "561760"
  },
  {
    "text": "resources out there but we ended up going with LS TM and just a quick show",
    "start": "561760",
    "end": "567310"
  },
  {
    "text": "of hands how many people are familiar with neural networks okay Wow okay and",
    "start": "567310",
    "end": "572740"
  },
  {
    "text": "how about long short-term memory and one more caris how many people are working",
    "start": "572740",
    "end": "579010"
  },
  {
    "text": "with carrots today okay all right so what I'm gonna do for those that so we",
    "start": "579010",
    "end": "585370"
  },
  {
    "text": "can all level set is I'm just gonna walk through long short-term memory and I'm gonna breeze through a lot of the data",
    "start": "585370",
    "end": "590709"
  },
  {
    "text": "science here so please if you have any questions or want to go deeper I'm happy to tip chat afterwards in short I",
    "start": "590709",
    "end": "599200"
  },
  {
    "text": "mentioned before LST M stands for long short-term memory it's a type of neural network called",
    "start": "599200",
    "end": "604360"
  },
  {
    "text": "an RNN or recurrent neural network and it's initially designed to recognize",
    "start": "604360",
    "end": "609790"
  },
  {
    "text": "patterns and sequences of data so a good example of this you might have seen some papers I think Google is working on",
    "start": "609790",
    "end": "615910"
  },
  {
    "text": "something with image recognition and like working on different sequences of numbers and matching patterns that's one",
    "start": "615910",
    "end": "621640"
  },
  {
    "text": "of the common use cases for it but when you think of a situation where I've been",
    "start": "621640",
    "end": "628000"
  },
  {
    "text": "talking for about you know 10 minutes or so now so you kind of know who I am and let's say I said this paragraph the what",
    "start": "628000",
    "end": "637480"
  },
  {
    "text": "would be like the next word you'd want to predict right so it could be kubernetes it could be containers",
    "start": "637480",
    "end": "644829"
  },
  {
    "text": "it could be Prometheus but it's probably",
    "start": "644829",
    "end": "650260"
  },
  {
    "text": "not going to be something like bananas or sleeping so you know what a neural",
    "start": "650260",
    "end": "655390"
  },
  {
    "text": "network is gonna do is predict that next sequence what's the next word and potentially going to be a couple of",
    "start": "655390",
    "end": "661510"
  },
  {
    "text": "other neat ways that the community for machine learning and LS TM have been doing it is we think of Mozart for",
    "start": "661510",
    "end": "667540"
  },
  {
    "text": "example and let's say you listen to 10 hours of music of Mozart and you have a",
    "start": "667540",
    "end": "672670"
  },
  {
    "text": "new song and you're 10 seconds in you know the model would be able to predict",
    "start": "672670",
    "end": "677770"
  },
  {
    "text": "the next musical note perhaps or be pretty close to what that would sound like let's say you took every work of",
    "start": "677770",
    "end": "683560"
  },
  {
    "text": "Shakespeare and you loaded that in it essentially works of art or you know",
    "start": "683560",
    "end": "689709"
  },
  {
    "text": "their sequences you know the sequence of words there so what would be you know if",
    "start": "689709",
    "end": "695620"
  },
  {
    "text": "you got a paragraph of Shakespeare if some title ii didn't know could you predict the next word that's another",
    "start": "695620",
    "end": "701709"
  },
  {
    "text": "thing that an LS TM model would do also timeseriesforecasting so this is",
    "start": "701709",
    "end": "708100"
  },
  {
    "text": "something that's kind of more common in business so if you think of you know sales revenue you're always going to",
    "start": "708100",
    "end": "714490"
  },
  {
    "text": "want to forecast out I see investment bankers nodding their heads that's good so yours gonna want to forecast out you",
    "start": "714490",
    "end": "721420"
  },
  {
    "text": "know what's the next quarter or the next you know two quarters to be able to kind of gauge your business so LS TM models",
    "start": "721420",
    "end": "727690"
  },
  {
    "text": "also play an important role there so we",
    "start": "727690",
    "end": "732820"
  },
  {
    "text": "decided that we're going to go down this path we were working with data scientists and we had all this awesome",
    "start": "732820",
    "end": "739150"
  },
  {
    "text": "data and Prometheus that we're going to use to actually shape how our kubernetes",
    "start": "739150",
    "end": "744160"
  },
  {
    "text": "cluster operates and the underlying infrastructure so we would collect the metrics and then those metrics would be",
    "start": "744160",
    "end": "750700"
  },
  {
    "text": "fed into build and deploy and Alice TM model that model would then be able to",
    "start": "750700",
    "end": "757089"
  },
  {
    "text": "take in metrics that are streaming real time for Prometheus and make predictions and we'll talk about that a little bit",
    "start": "757089",
    "end": "762490"
  },
  {
    "text": "later and then what we're working on now actually is taking those predictions and",
    "start": "762490",
    "end": "767500"
  },
  {
    "text": "making decisions on the underpinning infrastructure so there's certain things that we can do with kubernetes so you",
    "start": "767500",
    "end": "773230"
  },
  {
    "text": "think of you know if I predict fifteen minutes out that I need to scale up my pods I'm going to want to be able to",
    "start": "773230",
    "end": "779320"
  },
  {
    "text": "connect to the communities API and scale that up likewise if I need more infrastructure then I'm gonna you know",
    "start": "779320",
    "end": "785890"
  },
  {
    "text": "talk to my cloud vendor and say I need more infrastructure through another API the reason why we're not there yet is",
    "start": "785890",
    "end": "791829"
  },
  {
    "text": "because as we're still training these models I don't want to bring down production it's so not this is not",
    "start": "791829",
    "end": "797410"
  },
  {
    "text": "something that's deployed in production yet but what we're working on is the observability of the predictions that are coming out and the arm is every team",
    "start": "797410",
    "end": "804699"
  },
  {
    "text": "is actually adjusting that as we go so we're still kind of fine-tuning and we're still going through the discovery",
    "start": "804699",
    "end": "810100"
  },
  {
    "text": "phase but I'm going to walk it through each of these steps next so the first",
    "start": "810100",
    "end": "816850"
  },
  {
    "text": "one which is pretty straightforward is that we need to acquire these metrics",
    "start": "816850",
    "end": "822360"
  },
  {
    "text": "with this you want to play with the data range because ultimately you're gonna do",
    "start": "822360",
    "end": "827439"
  },
  {
    "text": "a range query from Prometheus the the key thing here is is that you don't want to take such a large data range because",
    "start": "827439",
    "end": "835660"
  },
  {
    "text": "if you think of your application let's say your startup and what your app look",
    "start": "835660",
    "end": "840730"
  },
  {
    "text": "like six months ago as far as traffic may not be what it is today so you how that model is trained is",
    "start": "840730",
    "end": "847329"
  },
  {
    "text": "gonna be on that full data so you want to be kind of cognizant of you know do I want how heavily do I want to look at my",
    "start": "847329",
    "end": "853389"
  },
  {
    "text": "traffic from six months ago and have that play a role in how predictions are generated so what we do is actually",
    "start": "853389",
    "end": "858819"
  },
  {
    "text": "three weeks and as you know more folks are coming on and using the platform we have a nice sliding scale of how the",
    "start": "858819",
    "end": "865569"
  },
  {
    "text": "predictions are being generated from there we format the data you know Prometheus we're calling the API getting",
    "start": "865569",
    "end": "872199"
  },
  {
    "text": "JSON back so will formatting it into a way that can be loaded into a data frame in Python and you know some example",
    "start": "872199",
    "end": "879819"
  },
  {
    "text": "metrics that we've played around with just to kind of get an understanding of how modelling and so forth would work replica counts are kind of",
    "start": "879819",
    "end": "886569"
  },
  {
    "text": "straightforward yet pod counts maybe you have an HPA that's already doing some stuff and you want to just predict where",
    "start": "886569",
    "end": "891880"
  },
  {
    "text": "you are that's one great way to do it RPS or requests per second that's a key",
    "start": "891880",
    "end": "897490"
  },
  {
    "text": "one that we've seen a lot where you know every graph on a dashboard I feel like",
    "start": "897490",
    "end": "902740"
  },
  {
    "text": "has one that is on display in the showroom also upstream latency so",
    "start": "902740",
    "end": "907860"
  },
  {
    "text": "ambassador has been great and we've been using the underlying Envoy stats as a",
    "start": "907860",
    "end": "913149"
  },
  {
    "text": "way to look at you know which api's of our of our kubernetes service are being",
    "start": "913149",
    "end": "919029"
  },
  {
    "text": "hit the hardest and we can actually use that to figure out like where do we need to scale and we can train a model off of",
    "start": "919029",
    "end": "925000"
  },
  {
    "text": "that as well lastly you can just do plain old infrastructure if you have no export or setup",
    "start": "925000",
    "end": "930700"
  },
  {
    "text": "you're pulling in different metrics into Prometheus you can pull those off a time series as well so that's first step so",
    "start": "930700",
    "end": "938080"
  },
  {
    "text": "you have your data but one quick thing is that there is a limitation in",
    "start": "938080",
    "end": "943810"
  },
  {
    "text": "Prometheus but I wouldn't call it limitation because you don't want to bring down your Prometheus server so",
    "start": "943810",
    "end": "949420"
  },
  {
    "text": "when you're making these large queries you're probably gonna want to batch them and the reason for that is is because there's a maximum resolution of eleven",
    "start": "949420",
    "end": "956140"
  },
  {
    "text": "thousand points for time series so if you try to query your Prometheus server and say give me everything from six",
    "start": "956140",
    "end": "962650"
  },
  {
    "text": "months ago to today I wanted a rate of five minutes not gonna work they're gonna come back and say reduce",
    "start": "962650",
    "end": "967810"
  },
  {
    "text": "your time series so a couple of things here this is just kind of some commented out go code so we are basically taking",
    "start": "967810",
    "end": "977260"
  },
  {
    "text": "the time and putting it into unix time so if you aren't familiar with UNIX time it's a 64 bit integer counting the",
    "start": "977260",
    "end": "983680"
  },
  {
    "text": "number of seconds since January 1st 1970 that's basically what we theist does when you set up the range of your range",
    "start": "983680",
    "end": "989590"
  },
  {
    "text": "query so from the from - the - so that",
    "start": "989590",
    "end": "994630"
  },
  {
    "text": "way you can batch that out really nicely and then basically aggregate it together and put it into the file for the data",
    "start": "994630",
    "end": "999760"
  },
  {
    "text": "frame I've got a brief demo of it right here so this is just one day and it's",
    "start": "999760",
    "end": "1005550"
  },
  {
    "text": "gonna basically grab 60 minutes at a time you see the UNIX timestamps here pulling all that together and then we",
    "start": "1005550",
    "end": "1012120"
  },
  {
    "text": "basically formatted it into a CSV so it's just easy to load into another other file system or another the next",
    "start": "1012120",
    "end": "1018630"
  },
  {
    "text": "part of the application so it's gonna take a look at the CSV this is our dev server so there's not a ton of activity",
    "start": "1018630",
    "end": "1025410"
  },
  {
    "text": "but as we scroll through none of our developers are working late just disk",
    "start": "1025410",
    "end": "1031220"
  },
  {
    "text": "but then as we see we go through here and there's some RPS that's coming",
    "start": "1031220",
    "end": "1036240"
  },
  {
    "text": "through so this is kind of what the file looks like as you hand it off to the next step so next part is the",
    "start": "1036240",
    "end": "1044310"
  },
  {
    "text": "interesting part we're gonna train and deploy the Alice TM model and this is largely we chose Kerris just because",
    "start": "1044310",
    "end": "1051150"
  },
  {
    "text": "it's a very high-level API and it's when you're just getting started with machine",
    "start": "1051150",
    "end": "1056400"
  },
  {
    "text": "learning this has been super helpful to go from nothing to something very",
    "start": "1056400",
    "end": "1061920"
  },
  {
    "text": "quickly and so the big thing that you want to do when you're starting out is take your",
    "start": "1061920",
    "end": "1066980"
  },
  {
    "text": "entire data set split it into what's test and training sets I recommend with a time series doing we",
    "start": "1066980",
    "end": "1074960"
  },
  {
    "text": "do 60% it's the first 60% of the data set is training and the remainder is test done there's some transformation",
    "start": "1074960",
    "end": "1082370"
  },
  {
    "text": "steps you need to do basically transform the data into what's called supervised data I could talk about that a little",
    "start": "1082370",
    "end": "1087380"
  },
  {
    "text": "bit later and then also scaling so scaling basically takes it the most common one to do is the hyperbolic",
    "start": "1087380",
    "end": "1093289"
  },
  {
    "text": "tangent takes all of your data points and goes from negative one to one and",
    "start": "1093289",
    "end": "1098600"
  },
  {
    "text": "then from there you fit the model and there's a couple things here that our",
    "start": "1098600",
    "end": "1104510"
  },
  {
    "text": "data scientists do to prevent overfitting you may have heard of that which basically means that you don't want your model to match your training",
    "start": "1104510",
    "end": "1111230"
  },
  {
    "text": "set exactly because predictions then are going to be are not going to be based",
    "start": "1111230",
    "end": "1116330"
  },
  {
    "text": "off of anything other than the training set so you know we do early stoppage and kind of more advanced topics but I'm I'm",
    "start": "1116330",
    "end": "1123650"
  },
  {
    "text": "not gonna not going to cover that for this talk but just know that there are ways of you know configuring your epics",
    "start": "1123650",
    "end": "1129260"
  },
  {
    "text": "and neurons on how you arch fitting your model and then after you do the training",
    "start": "1129260",
    "end": "1135620"
  },
  {
    "text": "set you want to evaluate the model against the test set and that's the importance behind splitting them out is that you can actually take different",
    "start": "1135620",
    "end": "1142549"
  },
  {
    "text": "data that was not trained and evaluate how predictions look we're gonna look at a quick model of that right now so I",
    "start": "1142549",
    "end": "1150530"
  },
  {
    "text": "have for you a ipython notebook good showing up and this is actually taking a",
    "start": "1150530",
    "end": "1156770"
  },
  {
    "text": "very small data set because like you're gonna run Karis really quick so this is just showing some server latency that we",
    "start": "1156770",
    "end": "1162110"
  },
  {
    "text": "grabbed from an envoy metric from Prometheus so you see here's your unix",
    "start": "1162110",
    "end": "1167600"
  },
  {
    "text": "timestamp here's some values that we have to play with this is kind of the distribution of that data so this is",
    "start": "1167600",
    "end": "1175039"
  },
  {
    "text": "over about a day or so and here we're actually gonna run the Charis so we see",
    "start": "1175039",
    "end": "1184190"
  },
  {
    "text": "here this is doing the training the loss here is saying that okay it got a little bit better over time expand this out",
    "start": "1184190",
    "end": "1192000"
  },
  {
    "text": "so now our model is complete that was relatively quick because the data set was small and otherwise we'd be staring",
    "start": "1192000",
    "end": "1197700"
  },
  {
    "text": "at this for a little bit and here we're gonna take a look at our test set that we have and we're gonna run that so we",
    "start": "1197700",
    "end": "1205080"
  },
  {
    "text": "see pretty close it seems like there was some you know we were predicting 33",
    "start": "1205080",
    "end": "1210300"
  },
  {
    "text": "milliseconds and it was actually zero but these are pretty close to the actual latency for the test set and importantly",
    "start": "1210300",
    "end": "1218100"
  },
  {
    "text": "this is actually shows how well our model does against the distribution of the data set so we're gonna rerun this",
    "start": "1218100",
    "end": "1225600"
  },
  {
    "text": "really quick and so we might need to do a little bit more fitting typically what",
    "start": "1225600",
    "end": "1230610"
  },
  {
    "text": "you're looking for is linear kind of like x equals y on the top right here",
    "start": "1230610",
    "end": "1235940"
  },
  {
    "text": "expand this out but this shows the predicted distribution on the left and",
    "start": "1235940",
    "end": "1243020"
  },
  {
    "text": "the actual on the right and these are the correlations between the two so you're shooting for y equals x but see",
    "start": "1243020",
    "end": "1251490"
  },
  {
    "text": "here we have some we have to probably do some different tuning and so forth okay",
    "start": "1251490",
    "end": "1260040"
  },
  {
    "text": "let's get back in Oh is gonna there we go",
    "start": "1260040",
    "end": "1267440"
  },
  {
    "text": "okay so now we have our model so we're gonna now pass our model in to start generating some predictions now this is",
    "start": "1267440",
    "end": "1274520"
  },
  {
    "text": "where it gets a little bit interesting because you're gonna choose a smaller data set that you're gonna feed in in",
    "start": "1274520",
    "end": "1279800"
  },
  {
    "text": "real time and but you still want to have some context of what's happened for example in the last hour because if your",
    "start": "1279800",
    "end": "1287390"
  },
  {
    "text": "models saying like it's dead there's nothing going on there's no request activity but the real-time predictions",
    "start": "1287390",
    "end": "1294020"
  },
  {
    "text": "are saying no there's someone using this right now when we need to make sure that that stays up you know that that data",
    "start": "1294020",
    "end": "1299900"
  },
  {
    "text": "gets fed into the lsdm model and they account for that when doing forecasting",
    "start": "1299900",
    "end": "1305080"
  },
  {
    "text": "so we're gonna basically use about an hour's worth of data to predict the next 10 minutes for example but you can kind",
    "start": "1305080",
    "end": "1310700"
  },
  {
    "text": "of adjust this as you go and then we're gonna split the data set again at the training and test data sets and the",
    "start": "1310700",
    "end": "1317690"
  },
  {
    "text": "training set which could be say 40 minutes or so is used to forecast the",
    "start": "1317690",
    "end": "1323420"
  },
  {
    "text": "data in step and then we're gonna use the test set to basically do the next",
    "start": "1323420",
    "end": "1328790"
  },
  {
    "text": "prediction so if our test set is T 0 through T 4 here we're gonna use the data from that to predict T 5 now you",
    "start": "1328790",
    "end": "1337130"
  },
  {
    "text": "could be done with that and be good or you can do something what's called multi-step and you can basically do t1",
    "start": "1337130",
    "end": "1343190"
  },
  {
    "text": "and t5 to predict T 6 now there's some considerations here if you wanna if your step is a minute if",
    "start": "1343190",
    "end": "1350540"
  },
  {
    "text": "each of one of these is a minute and that's a new prediction in actual then what you're going to want to do is keep",
    "start": "1350540",
    "end": "1356990"
  },
  {
    "text": "in mind the hot you're gonna have a higher rate of error the further you go out so if I use this model and it's a",
    "start": "1356990",
    "end": "1362330"
  },
  {
    "text": "minute the predictions that are 15 minutes out are going to be more accurate than the ones that are 60",
    "start": "1362330",
    "end": "1368480"
  },
  {
    "text": "minutes out so that's how that in in a",
    "start": "1368480",
    "end": "1374090"
  },
  {
    "text": "nutshell we generate the predictions so now we have predictions to play with and this is some we have some actionable",
    "start": "1374090",
    "end": "1380600"
  },
  {
    "text": "data so what we have here we have no traffic so our predictions are saying",
    "start": "1380600",
    "end": "1386660"
  },
  {
    "text": "from T 0 to T 15 that there's nothing so what does this mean for me as an SRE means I may have the opportunity to",
    "start": "1386660",
    "end": "1393260"
  },
  {
    "text": "scale down replicas and scale down some infrastructure conversely if my RPS is going to go from",
    "start": "1393260",
    "end": "1401380"
  },
  {
    "text": "12 to 5,000 I'm probably gonna want to account for that pretty quickly so I have 15 minutes to basically get to a",
    "start": "1401380",
    "end": "1407980"
  },
  {
    "text": "point where I can I can handle 5,000 RPS and this is just kind of a very basic",
    "start": "1407980",
    "end": "1418000"
  },
  {
    "text": "example so if I had stateless resources deployments pods you know there's some",
    "start": "1418000",
    "end": "1423190"
  },
  {
    "text": "sample go code here the basically says I'm gonna set thresholds that basically say the you know if we're above a",
    "start": "1423190",
    "end": "1430840"
  },
  {
    "text": "certain RPS or a below a certain RPS and take action otherwise let's sleep and wait for the next streaming predictions",
    "start": "1430840",
    "end": "1438010"
  },
  {
    "text": "to take take hold also but I'm gonna want put gates in here and this is why I actually mentioned it it's like",
    "start": "1438010",
    "end": "1443650"
  },
  {
    "text": "semi-supervised I don't know if that's the exact term but there is human interaction here we are preventing this",
    "start": "1443650",
    "end": "1448810"
  },
  {
    "text": "from going way too high and way too low again we're trying to not have a crazy cloud bill but we also want to have our",
    "start": "1448810",
    "end": "1455080"
  },
  {
    "text": "service up so these are things to just keep in mind as you're going through it and I can make these codes than if it's",
    "start": "1455080",
    "end": "1462070"
  },
  {
    "text": "available on a guest or something and that's that's where we are today from a",
    "start": "1462070",
    "end": "1469960"
  },
  {
    "text": "data science side we're currently exploring can we do better we're looking at kind of tuning those parameters but",
    "start": "1469960",
    "end": "1476380"
  },
  {
    "text": "also looking at something called multivariate LX l STM this correlation between some of your data you know you",
    "start": "1476380",
    "end": "1482020"
  },
  {
    "text": "may look at CPU percentage and RPS and",
    "start": "1482020",
    "end": "1487180"
  },
  {
    "text": "you may want to put both in a model and generate a different model off that that may have more context to the rest of the",
    "start": "1487180",
    "end": "1492400"
  },
  {
    "text": "cluster or maybe you have two different services that have different end points of latency and maybe those models shape",
    "start": "1492400",
    "end": "1499000"
  },
  {
    "text": "something else so we're looking at multivariate lsdm as well another thing is reinforcement learning",
    "start": "1499000",
    "end": "1505510"
  },
  {
    "text": "or cue learning if you're not familiar this is what alpha 0 or alphago is based off of the we're looking at this as",
    "start": "1505510",
    "end": "1513850"
  },
  {
    "text": "trying to reinforce good behavior so if we in a very simplistic term if we say keep costs low but make sure the client",
    "start": "1513850",
    "end": "1522010"
  },
  {
    "text": "experience is optimal and we define parameters around how that works then we can use cue learning and use that to",
    "start": "1522010",
    "end": "1529450"
  },
  {
    "text": "find a balance between maybe our threshold numbers as well as our min/max numbers we were talking about in the predictions",
    "start": "1529450",
    "end": "1536120"
  },
  {
    "text": "and we're looking at other things that we can forecast we started this out looking at how we can increase and",
    "start": "1536120",
    "end": "1542660"
  },
  {
    "text": "decrease our infrastructure for our data science workloads now we're looking at it for replicas and we're looking at it",
    "start": "1542660",
    "end": "1549440"
  },
  {
    "text": "for different types of pods even looking at it for you know some of the elastic resize stuff that came out for the",
    "start": "1549440",
    "end": "1555530"
  },
  {
    "text": "databases that's something that we're seeing if you know if down times low maybe we can do some scaling there and",
    "start": "1555530",
    "end": "1562550"
  },
  {
    "text": "also cluster management maybe there's situations where we can predict and move our clusters around you",
    "start": "1562550",
    "end": "1568790"
  },
  {
    "text": "know work in a hybrid environment and so forth but I think the goal behind the data science part of this is how low",
    "start": "1568790",
    "end": "1574880"
  },
  {
    "text": "touch can we get you know how what's the minimal amount of time and sre needs to",
    "start": "1574880",
    "end": "1581450"
  },
  {
    "text": "manage a kubernetes cluster and how much machine learning and potentially AI do",
    "start": "1581450",
    "end": "1586520"
  },
  {
    "text": "we need to get there another thing that we're looking at is how do we make this",
    "start": "1586520",
    "end": "1592850"
  },
  {
    "text": "intuitive and deployable so we've been looking at HP a horizontal pod",
    "start": "1592850",
    "end": "1598520"
  },
  {
    "text": "autoscaler and the model around that and seeing if there's a way that we can take everything we discussed and actually put",
    "start": "1598520",
    "end": "1604970"
  },
  {
    "text": "it into a CR D or custom resource diagram and then it's just as simple as",
    "start": "1604970",
    "end": "1610100"
  },
  {
    "text": "doing a coop cuddle create paths I come from the PA's world so that's kind of",
    "start": "1610100",
    "end": "1615620"
  },
  {
    "text": "weird but coop co-create paths that's something that attaches to your deployment or daemon set and it's just",
    "start": "1615620",
    "end": "1622700"
  },
  {
    "text": "into it if you pass some parameters and the machine learning starts to kick off so this is something that we're actively working on and that's it so",
    "start": "1622700",
    "end": "1634210"
  },
  {
    "text": "like take a couple questions",
    "start": "1639090",
    "end": "1642720"
  },
  {
    "text": "yeah so the question was is to take into account seasonality and that's something we looked at because I think the",
    "start": "1651980",
    "end": "1659160"
  },
  {
    "text": "seasonality would like Saturday Sunday especially with what we were seeing with the Gryphon Oh charts that is was kind",
    "start": "1659160",
    "end": "1664620"
  },
  {
    "text": "of a primary driver for us so when you're picking your slice of data that",
    "start": "1664620",
    "end": "1669990"
  },
  {
    "text": "your initial you're putting in to fit the model that's important because then you're looking at you know fall holidays",
    "start": "1669990",
    "end": "1678059"
  },
  {
    "text": "other types of things that actually gonna get incorporated in so it's a balance between seasonality and you know",
    "start": "1678059",
    "end": "1685049"
  },
  {
    "text": "how stale that data is going far back",
    "start": "1685049",
    "end": "1689600"
  },
  {
    "text": "sure so it's a good question so the question was if the predictions were wrong how do we adjust for something",
    "start": "1704059",
    "end": "1710479"
  },
  {
    "text": "like that so typically what we've been doing is that we also have a kind of a",
    "start": "1710479",
    "end": "1715729"
  },
  {
    "text": "reactive so if the model is bad or maybe there's you know it made it an incorrect",
    "start": "1715729",
    "end": "1721700"
  },
  {
    "text": "prediction and we need additional infrastructure there's things you can do as far as like a reactive kind of a",
    "start": "1721700",
    "end": "1727070"
  },
  {
    "text": "correction and you can keep track of those Corrections as well to help fine-tune your model or you know ping",
    "start": "1727070",
    "end": "1732649"
  },
  {
    "text": "slack and one of us jumps on it and tries to figure that's another thing",
    "start": "1732649",
    "end": "1738009"
  },
  {
    "text": "sure",
    "start": "1749630",
    "end": "1752260"
  },
  {
    "text": "it's it's so the question is around scale and how would this operate at",
    "start": "1757250",
    "end": "1762320"
  },
  {
    "text": "scale that's something we're looking into now I think we are I think what",
    "start": "1762320",
    "end": "1767809"
  },
  {
    "text": "we've initially done is Unleashed it onto our development environment so there's you know it's smaller in scale",
    "start": "1767809",
    "end": "1774140"
  },
  {
    "text": "but as we start to look at it a little bit further out I actually think that's where you see if you decentralize and",
    "start": "1774140",
    "end": "1781010"
  },
  {
    "text": "have like the model and have like a CR D for each different service and you know",
    "start": "1781010",
    "end": "1786230"
  },
  {
    "text": "you datasets or you make your datasets a little bit smaller you might be able to achieve the same level of kind of predictions for scale but you have to",
    "start": "1786230",
    "end": "1792440"
  },
  {
    "text": "just keep that and keep that in mind",
    "start": "1792440",
    "end": "1795879"
  },
  {
    "text": "right so what we what we are doing right now the question was are you automating",
    "start": "1800860",
    "end": "1806539"
  },
  {
    "text": "the Refresh of the model yes we are so basically at midnight we take in the",
    "start": "1806539",
    "end": "1812659"
  },
  {
    "text": "next three weeks the next rolling three weeks generate the model of that if the model fits the evaluation set then we'll",
    "start": "1812659",
    "end": "1820159"
  },
  {
    "text": "we'll deploy it and that'll be like the model that will be used for the next day and then from there that process",
    "start": "1820159",
    "end": "1826520"
  },
  {
    "text": "continues so essentially you can think of it as like a cron job that you can just kick off and then have the model created",
    "start": "1826520",
    "end": "1831620"
  },
  {
    "text": "we're actually running the model generation kubernetes as well start",
    "start": "1831620",
    "end": "1839000"
  },
  {
    "text": "fresh",
    "start": "1839000",
    "end": "1841150"
  },
  {
    "text": "yes I mean so the question was do you have another note is like you're asking about notifications as far as the semi",
    "start": "1853220",
    "end": "1859740"
  },
  {
    "text": "supervision like if we're at like the max or the min right that's actually",
    "start": "1859740",
    "end": "1872370"
  },
  {
    "text": "what we're doing now so we're not we're not actually taking the actions we're kind of observing how the model is",
    "start": "1872370",
    "end": "1878159"
  },
  {
    "text": "predicting and if it's like if it's a ridiculous number that's we're actually sending like a slack message to our team",
    "start": "1878159",
    "end": "1883470"
  },
  {
    "text": "to be like hey the predictions way off can we look at it how can we you know is something wrong with the model and we",
    "start": "1883470",
    "end": "1889649"
  },
  {
    "text": "kind of do an investigation from there no yep notification base at the moment",
    "start": "1889649",
    "end": "1896669"
  },
  {
    "text": "yep",
    "start": "1896669",
    "end": "1898789"
  },
  {
    "text": "sure so the some of the things we're doing is like rates arrayed over five",
    "start": "1902820",
    "end": "1909010"
  },
  {
    "text": "minutes so and that actually plays into the resolution a bit so you may have to",
    "start": "1909010",
    "end": "1914710"
  },
  {
    "text": "do smaller batches when you do the rate resolution over five minutes we've noticed that as we started smoothing a",
    "start": "1914710",
    "end": "1922540"
  },
  {
    "text": "bit the resolution for the time series got crazy so I'll take one more",
    "start": "1922540",
    "end": "1928920"
  },
  {
    "text": "sure it's a good question so the why we use daemon sets instead of jobs we",
    "start": "1943700",
    "end": "1949610"
  },
  {
    "text": "initially went down daemon sets were gave us a bunch of benefits in the sense of we knew that we could run exactly one",
    "start": "1949610",
    "end": "1958840"
  },
  {
    "text": "by using daemon sets on that particular node whereas jobs we didn't really get",
    "start": "1958840",
    "end": "1964520"
  },
  {
    "text": "into heavy like affinity or node selection with jobs yet but the the it's",
    "start": "1964520",
    "end": "1971150"
  },
  {
    "text": "essentially triggered and the daemon set is just listening for a job to appear in our messaging queue so we're actually",
    "start": "1971150",
    "end": "1977090"
  },
  {
    "text": "just waiting for it to pick it up and then the data science process kicks off from there",
    "start": "1977090",
    "end": "1983620"
  }
]