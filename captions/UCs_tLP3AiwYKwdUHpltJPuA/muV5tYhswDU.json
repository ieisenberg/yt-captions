[
  {
    "start": "0",
    "end": "27000"
  },
  {
    "text": "[Music]",
    "start": "3480",
    "end": "7799"
  },
  {
    "text": "hello everyone thanks for sticking it out until the very end I appreciate you all",
    "start": "12000",
    "end": "17560"
  },
  {
    "text": "being here hopefully I can keep you all awake I see people have brought the uh the chips and the popcorn so uh yeah I",
    "start": "17560",
    "end": "23680"
  },
  {
    "text": "hope I'm able to keep you entertained and informed so we have a little bit of a packed agenda today because I have",
    "start": "23680",
    "end": "30119"
  },
  {
    "start": "27000",
    "end": "67000"
  },
  {
    "text": "matched together not one but two talks together into one um so we're talking both about how we use serverless and how",
    "start": "30119",
    "end": "36000"
  },
  {
    "text": "we use Arm and how we use serverus and arm together to get more efficient results so today I'm going to tell you",
    "start": "36000",
    "end": "42280"
  },
  {
    "text": "about how you might find it useful how we're using it a little bit unconventionally we'll talk about some of the pain points because look I'm a",
    "start": "42280",
    "end": "49000"
  },
  {
    "text": "site reliability engineer I'm a pessimist I like to talk about how things break and not just you know the ideal conference diagram and you know",
    "start": "49000",
    "end": "55320"
  },
  {
    "text": "the architecture whiteboard it's going to tell you how it actually breaks what you might want to do if you're going to experiment with this and then fourth and",
    "start": "55320",
    "end": "61480"
  },
  {
    "text": "finally I'll tell you about how to make it cheaper and more cost effective and more environmentally",
    "start": "61480",
    "end": "66759"
  },
  {
    "text": "sustainable so first let's talk about what people use Lambda for normally you think aw is Lambda you think serverless",
    "start": "66759",
    "end": "73759"
  },
  {
    "start": "67000",
    "end": "97000"
  },
  {
    "text": "you think you know okay API Gateway or maybe something that's consuming off of off of a data bus right like you don't",
    "start": "73759",
    "end": "79960"
  },
  {
    "text": "necessarily think big data analytics you're thinking streaming analytics but in our case at honeycomb we use Lambda",
    "start": "79960",
    "end": "87200"
  },
  {
    "text": "to optimize our custom data store which is called called retriever but you know me saying the words retriever doesn't",
    "start": "87200",
    "end": "92920"
  },
  {
    "text": "mean anything to you so let's talk about what you the actual end user developer might actually use it for so we do Real",
    "start": "92920",
    "end": "99079"
  },
  {
    "start": "97000",
    "end": "151000"
  },
  {
    "text": "Time Event aggregation so we're collecting data from our customers and we're giving people insights into",
    "start": "99079",
    "end": "105920"
  },
  {
    "text": "potentially what's happening with with hundreds of millions of events or billions of events that they've sent to us in the past and they are asking those",
    "start": "105920",
    "end": "113280"
  },
  {
    "text": "questions in real time not necessarily kind of prean things that they've already pre-aggregated or said I want to",
    "start": "113280",
    "end": "118360"
  },
  {
    "text": "measure this in advance and also uh because of the dog puns uh so this is my",
    "start": "118360",
    "end": "123880"
  },
  {
    "text": "uh Golden Retriever mix her name is flurry so flurry is a very good dog so we do Real Time Event aggregation",
    "start": "123880",
    "end": "131560"
  },
  {
    "text": "specifically over Trace data generated with open Telemetry so hi I'm Liz I'm the field C at honeycomb and also I'm an",
    "start": "131560",
    "end": "138360"
  },
  {
    "text": "AWS Community hero um so if you have any complaints to forward to the AWS team I",
    "start": "138360",
    "end": "143519"
  },
  {
    "text": "can do that as well um and boy do I have a lot of interesting feedback that I've given to the AWS team over the years",
    "start": "143519",
    "end": "150800"
  },
  {
    "text": "so honeycomb empowers you to have observability and this is not a honeycomb sales pitch this is literally",
    "start": "150800",
    "end": "155840"
  },
  {
    "start": "151000",
    "end": "189000"
  },
  {
    "text": "just me telling you what's our us case why did we start to use Lambda to experiment with this in the first place",
    "start": "155840",
    "end": "161239"
  },
  {
    "text": "so originally in the bad old days there was monitoring and monitoring was you know here are the things that you want",
    "start": "161239",
    "end": "167120"
  },
  {
    "text": "to pre-aggregate you want to count and make super fast and anything else you had to grip through logs and it was",
    "start": "167120",
    "end": "172159"
  },
  {
    "text": "potentially really slow so what we see as observability today is helping people",
    "start": "172159",
    "end": "177640"
  },
  {
    "text": "get insights into things they didn't anticipate uh in advance but ultimately they're still driving at that same goal",
    "start": "177640",
    "end": "183239"
  },
  {
    "text": "of helping you understand your systems and reconcile your mental uh model of what's going on inside of your",
    "start": "183239",
    "end": "188640"
  },
  {
    "text": "systems so uh I was able to get the live demo working uh good news because uh we",
    "start": "188640",
    "end": "194920"
  },
  {
    "start": "189000",
    "end": "248000"
  },
  {
    "text": "managed to get my screen mirroring working so I'm just going to say you know hey show me um all of the traces",
    "start": "194920",
    "end": "201840"
  },
  {
    "text": "that um show me all the traces where the they originate from real users",
    "start": "201840",
    "end": "210280"
  },
  {
    "text": "query Source uh is",
    "start": "210280",
    "end": "216640"
  },
  {
    "text": "normal uh and let's group them by tenant ID right like so this is our dog food",
    "start": "216640",
    "end": "221920"
  },
  {
    "text": "data this is how we observe our production environment so I can you know just as fast as it takes me to literally",
    "start": "221920",
    "end": "228080"
  },
  {
    "text": "pick up my cup of uh water and get and get a drink I can see who are the most popular who are querying us the most",
    "start": "228080",
    "end": "234319"
  },
  {
    "text": "times and uh how many concurrent queries are they running at a time and it's reasonably fast right like faster than I",
    "start": "234319",
    "end": "239560"
  },
  {
    "text": "can take a sip of water so if speed is the goal how do we",
    "start": "239560",
    "end": "245200"
  },
  {
    "text": "actually achieve that as the volumes of data increase so that's what this is about it's about fast queries across any",
    "start": "245200",
    "end": "251000"
  },
  {
    "start": "248000",
    "end": "284000"
  },
  {
    "text": "combination of fields but this is not just a paino in observability you can imagine that if you have a data Lake if",
    "start": "251000",
    "end": "257040"
  },
  {
    "text": "you have a bi use case that this may be useful to you because it turns out that data analysts also are very expensive uh",
    "start": "257040",
    "end": "264639"
  },
  {
    "text": "people who you want to keep happy and you want to enable to uh give Better Business outcomes you don't accidentally",
    "start": "264639",
    "end": "271240"
  },
  {
    "text": "order a thousand of the wrong widget or something so architecturally again like this is open Telemetry and then it goes",
    "start": "271240",
    "end": "277360"
  },
  {
    "text": "into a cumer store but the challenge is how do you actually scale out the cumer store because eventually it exceeds the",
    "start": "277360",
    "end": "282600"
  },
  {
    "text": "capacity of one machine so speed speed speed because user experience and if you",
    "start": "282600",
    "end": "288120"
  },
  {
    "start": "284000",
    "end": "468000"
  },
  {
    "text": "disrupt the user's flow if it sits there spinning so long that you're like actually I'm going to make a new cup of",
    "start": "288120",
    "end": "293400"
  },
  {
    "text": "coffee or like I'm not going to wait for this to finish then it becomes unacceptable so the way that we've",
    "start": "293400",
    "end": "300320"
  },
  {
    "text": "initially handled this is to decouple the stateful and stateless parts of our system so that we can scale them",
    "start": "300320",
    "end": "306560"
  },
  {
    "text": "independently because things that need to Scale based off of the amount of incoming data are very different than",
    "start": "306560",
    "end": "312360"
  },
  {
    "text": "things that need to Scale based off of when someone hits the Run query button so what if we separated those things and enabled data in just to be stateless and",
    "start": "312360",
    "end": "319360"
  },
  {
    "text": "then we funnel all the state full stuff through Kafka and then reading off of the Kafka topic so but at the end of the Kafka",
    "start": "319360",
    "end": "326319"
  },
  {
    "text": "topic right there has to be something stateful storing the data I can't just write it to Devol right so we have the",
    "start": "326319",
    "end": "331600"
  },
  {
    "text": "retriever system that basically takes in the data and it serializes it to uh to nvme Solid State Storage fun fact",
    "start": "331600",
    "end": "339319"
  },
  {
    "text": "originally uh the system that inspired retriever was called scuba and it was a",
    "start": "339319",
    "end": "344840"
  },
  {
    "text": "system developed at Facebook now meta and that system stored the data entirely in memory you can imagine that that gets",
    "start": "344840",
    "end": "351600"
  },
  {
    "text": "really really expensive and you probably only can really do it if you're meta so our initial thought as to how we can",
    "start": "351600",
    "end": "357400"
  },
  {
    "text": "make this better is why don't we just store the data on NV vme because nvme is a lot less expensive than Ram but turns",
    "start": "357400",
    "end": "363080"
  },
  {
    "text": "out that nvme storage has a finite capacity so we'd initially you know have",
    "start": "363080",
    "end": "369560"
  },
  {
    "text": "say okay let's just scale out horizontally right like you know we can in you know maybe we can't expand the",
    "start": "369560",
    "end": "374919"
  },
  {
    "text": "size of nvme on each worker but maybe we can have one worker act as the aggregation layer and then to form it",
    "start": "374919",
    "end": "381039"
  },
  {
    "text": "out to other parallel workers that are also storing the data and that way we can scale out to the size of the nvme",
    "start": "381039",
    "end": "386720"
  },
  {
    "text": "drives and what data are we writing what is the form well the essential kind of optimization here is you only want to",
    "start": "386720",
    "end": "392560"
  },
  {
    "text": "read data that you absolutely have to so if a query doesn't call for you know the field for instance like you know user",
    "start": "392560",
    "end": "399199"
  },
  {
    "text": "agent if I didn't query user agent in that query that I just ran then I shouldn't be touching the data relating",
    "start": "399199",
    "end": "404319"
  },
  {
    "text": "to user agents right and also that the data that's stored for each individual field is likely highly repetitive right",
    "start": "404319",
    "end": "410880"
  },
  {
    "text": "for instance my timestamp field is probably pretty incrementing for instance the htb status code is usually",
    "start": "410880",
    "end": "416960"
  },
  {
    "text": "one of either 200 500 400 right like there only a finite number of those status codes so that enables us to get",
    "start": "416960",
    "end": "422639"
  },
  {
    "text": "pretty good compression because at the end of the day the smaller the amount of data that you're scanning the faster it",
    "start": "422639",
    "end": "427759"
  },
  {
    "text": "is and the other piece that we do uh that again scuba also did is we think",
    "start": "427759",
    "end": "433240"
  },
  {
    "text": "about the time stamps that are relevant right if I'm only quering the most recent 2 hours of data I don't need to",
    "start": "433240",
    "end": "438919"
  },
  {
    "text": "scan data that was generated 30 days ago that couldn't possibly have any relation to my query today and I might want to",
    "start": "438919",
    "end": "445560"
  },
  {
    "text": "scan this you know the segment of data that was created an hour and a half ago that covers from an hour and a half half ago to 2 and a half hours ago but data",
    "start": "445560",
    "end": "452000"
  },
  {
    "text": "that is too old and couldn't possibly apply to my window don't even bother reading it so that's how we initially tried to",
    "start": "452000",
    "end": "459080"
  },
  {
    "text": "make it super fast right like you know just working with the constraints of a server full application that was storing",
    "start": "459080",
    "end": "464720"
  },
  {
    "text": "data on local SSD and that's that got us a little bit",
    "start": "464720",
    "end": "470919"
  },
  {
    "start": "468000",
    "end": "631000"
  },
  {
    "text": "far but more and more data came in and eventually we started having to act more like a ring buffer right where you are",
    "start": "470919",
    "end": "477560"
  },
  {
    "text": "storing the data until your disc fill up and then you discard the oldest data and then suddenly you look at your data",
    "start": "477560",
    "end": "482599"
  },
  {
    "text": "you're like wait a second I'm aging out data that's only 50 minutes old how many of you have needed to debug a problem",
    "start": "482599",
    "end": "488199"
  },
  {
    "text": "that was more than 50 minutes ago right so this wasn't fit for purpose so",
    "start": "488199",
    "end": "494960"
  },
  {
    "text": "instead we said okay what if we teered the data on to awss 3 or to object storage right like these are just",
    "start": "494960",
    "end": "501479"
  },
  {
    "text": "files object storage exists for a reason we can just write it and then we can get it back whenever we need it and we'll",
    "start": "501479",
    "end": "507159"
  },
  {
    "text": "have infinite storage and it's going to be great because now customers can query for months if they need to",
    "start": "507159",
    "end": "514719"
  },
  {
    "text": "yay but uh it turns out there's a minor problem with that which is sure quaring from your local nvme SSD is reasonably",
    "start": "514719",
    "end": "520959"
  },
  {
    "text": "fast and also has some reasonable proportion of data to the number of CPUs but what happens when you're now",
    "start": "520959",
    "end": "527279"
  },
  {
    "text": "having to fetch back millions of files from S3 and you're having to gravel through them with you know four CPUs or",
    "start": "527279",
    "end": "532959"
  },
  {
    "text": "16 CPUs the amount of time that it t took started becoming linear or even in the",
    "start": "532959",
    "end": "538959"
  },
  {
    "text": "worst case super linear not great so at this point we're violating the promise of running a query should be as fast as",
    "start": "538959",
    "end": "544720"
  },
  {
    "text": "taking a sip of water the core problem here and why we had to move to serverless was it",
    "start": "544720",
    "end": "551000"
  },
  {
    "text": "actually starts mirroring some of the data some of the use cases that you think about when you think about serverless Computing right like when you",
    "start": "551000",
    "end": "556519"
  },
  {
    "text": "have something that happens on Demand right you only want to use the Computing resources when you're actually doing",
    "start": "556519",
    "end": "562079"
  },
  {
    "text": "work and when you're not actually doing work then you want to go ahead and hand them off to someone else to use right",
    "start": "562079",
    "end": "567320"
  },
  {
    "text": "make them Amazon's problem to deal with all that you're not using so it's a lot like my dog right like she loves to play",
    "start": "567320",
    "end": "573399"
  },
  {
    "text": "with her toys she'll get them all out all out over the floor but like someone has to pick them up and put them back in the bin right like I can't just leave",
    "start": "573399",
    "end": "580320"
  },
  {
    "text": "you know the dog toys lying on the lying on the floor all the time so this is where Lambda comes in",
    "start": "580320",
    "end": "586600"
  },
  {
    "text": "because Lambda works really really well with S3 because S3 is really",
    "start": "586600",
    "end": "591680"
  },
  {
    "text": "horizontally scale infinitely scalable object storage and you know that you're going to seually need to map reduce right like",
    "start": "591680",
    "end": "598320"
  },
  {
    "text": "you're going to need to read those files off of S3 and that you can parallel compute them this is an embarrassingly",
    "start": "598320",
    "end": "603519"
  },
  {
    "text": "parallel problem so by saying okay each retriever now can call out to a set of lambdas and",
    "start": "603519",
    "end": "609880"
  },
  {
    "text": "each Lambda has in turn its own work list if these are the files the segments I'm rting off of S3 then we're able to",
    "start": "609880",
    "end": "615959"
  },
  {
    "text": "kind of merge this together in a three tier merging architecture to get users",
    "start": "615959",
    "end": "621560"
  },
  {
    "text": "results quickly so now no matter whether you're quering you know a million files or",
    "start": "621560",
    "end": "627240"
  },
  {
    "text": "100,000 files it still takes less than 10 seconds to do this is great and even",
    "start": "627240",
    "end": "632560"
  },
  {
    "start": "631000",
    "end": "668000"
  },
  {
    "text": "better we get perfect utilization right we're buying the compu in 1 millisecond",
    "start": "632560",
    "end": "637920"
  },
  {
    "text": "units so this is kind of how we how we were able to scale up our data store from uh you know we originally had",
    "start": "637920",
    "end": "644800"
  },
  {
    "text": "something like what 80 gab machines and uh 80 gab ssds and we had uh I don't",
    "start": "644800",
    "end": "650560"
  },
  {
    "text": "know maybe maybe 30 of them uh and now we're managing over over a uh over over",
    "start": "650560",
    "end": "656560"
  },
  {
    "text": "a terabyte of storage and yeah so we've grown over 20 times in 5 years and we haven't had to",
    "start": "656560",
    "end": "662839"
  },
  {
    "text": "mostly haven't had to worry about provisioning capacity planning for the query workload but the consideration is I said",
    "start": "662839",
    "end": "670480"
  },
  {
    "start": "668000",
    "end": "1135000"
  },
  {
    "text": "that this workload is similar to you know you rock up to the uh serus Brew uh",
    "start": "670480",
    "end": "675760"
  },
  {
    "text": "Brew uh page and you scan and you order a coffee right but this is not just one",
    "start": "675760",
    "end": "681800"
  },
  {
    "text": "person ordering one coffee and triggering one serverless function to run because when we ask to run a",
    "start": "681800",
    "end": "688160"
  },
  {
    "text": "function now we're asking actually for tens of thousands of concurrency and we're asking for four",
    "start": "688160",
    "end": "693800"
  },
  {
    "text": "vcpu each so we're actually setting you know 100,000 CPUs working at you know at",
    "start": "693800",
    "end": "699399"
  },
  {
    "text": "the click of someone pressing a button and Lambda normally doesn't let you do this normally there is a burst",
    "start": "699399",
    "end": "706760"
  },
  {
    "text": "Limit and if you try to run more queries than your burst Limit you get a htd 429 which tells you chill out slow",
    "start": "706760",
    "end": "713360"
  },
  {
    "text": "down and eventually if you keep on pushing your concurrency limit AWS will raise your concurrency limit but it",
    "start": "713360",
    "end": "719360"
  },
  {
    "text": "originally required 1 minute of sustained load but now it requires 10 seconds but even 10 seconds is a little",
    "start": "719360",
    "end": "724440"
  },
  {
    "text": "bit too slow for us because by the time that we have run those 10 seconds ideally you already have your response",
    "start": "724440",
    "end": "729880"
  },
  {
    "text": "back we're not trying to retry and retry and retry so under normal circumstances we would never hit that uh situation",
    "start": "729880",
    "end": "735959"
  },
  {
    "text": "where AWS automatically bumps or concurrency limit so that is the only case where we've had to talk to AWS and",
    "start": "735959",
    "end": "741079"
  },
  {
    "text": "said you know let's go ahead and capacity plan and you know you tell us how much spare compute you have lying",
    "start": "741079",
    "end": "746199"
  },
  {
    "text": "around that you can lend us and we're willing to wait if it's not AA but if it is available please just go ahead and",
    "start": "746199",
    "end": "751800"
  },
  {
    "text": "give us all all tens of thousands of concurrency so by negotiating withs team",
    "start": "751800",
    "end": "757680"
  },
  {
    "text": "by kind of managing the trade-offs of of uh queuing lency and concurrency we're able to to achieve this level of high",
    "start": "757680",
    "end": "764680"
  },
  {
    "text": "performance so make sure that you are retrying but with a back off make sure that you are measuring what the",
    "start": "764680",
    "end": "772160"
  },
  {
    "text": "performance is that you're getting and you know communicate with your with your account reps because they're the people",
    "start": "772160",
    "end": "777199"
  },
  {
    "text": "and the service team are the people who can actually enable to get to get that level of scale if that is something that",
    "start": "777199",
    "end": "782639"
  },
  {
    "text": "you are doing on purpose and is not someone accidentally triggering a Lambda to S3 bucket notification to Lambda S3",
    "start": "782639",
    "end": "789160"
  },
  {
    "text": "bucket notification infinite Loop so they know that to expect this workload from us secondly let's talk about cold",
    "start": "789160",
    "end": "796320"
  },
  {
    "text": "starts so you know ideally we want to respond respond within 10 seconds um and",
    "start": "796320",
    "end": "802680"
  },
  {
    "text": "also we're running across many many workers but sometimes the Lambda is not uh is is invoked against a a a backend",
    "start": "802680",
    "end": "810000"
  },
  {
    "text": "worker that does not have our runtime loaded so how do we manage this well again like you know we measure it and it",
    "start": "810000",
    "end": "816800"
  },
  {
    "text": "looks like in practice our experience has been takes a small number of milliseconds to actually send our zipped",
    "start": "816800",
    "end": "822920"
  },
  {
    "text": "uh our zi go binary to the machine but our zi go binary is less than 30 megabytes so it's not that huge compared",
    "start": "822920",
    "end": "829040"
  },
  {
    "text": "to a container image which iner and Lambda does support now is sending Docker container images so thank",
    "start": "829040",
    "end": "834320"
  },
  {
    "text": "goodness we don't send container images because like time is critical here and then our startup time is about 15",
    "start": "834320",
    "end": "839839"
  },
  {
    "text": "milliseconds to 50 Mills and then we actually start working and then the worker goes to sleep which is kind of",
    "start": "839839",
    "end": "845560"
  },
  {
    "text": "cool you can actually watch them go to sleep and then eventually get garbage collected if no one runs a query for a",
    "start": "845560",
    "end": "850800"
  },
  {
    "text": "while but there's another bit here which is when you're running millions of invocations some of them are not going",
    "start": "850800",
    "end": "857560"
  },
  {
    "text": "to return not within 2.5 seconds not ever until the Lambda SDK gives you a",
    "start": "857560",
    "end": "864800"
  },
  {
    "text": "30second timeout so what do you do in that situation because customers are not going to be happy if you said you know",
    "start": "864800",
    "end": "869839"
  },
  {
    "text": "hey by the way there were no users that were accessed by this attacker except if you're like actually whoops I forgot to",
    "start": "869839",
    "end": "876000"
  },
  {
    "text": "scan the 0.1% of data that did have the uh data points that said the user data was accessed right so what we do is we",
    "start": "876000",
    "end": "883240"
  },
  {
    "text": "do impatience we retry the slowest 1% of requests both to guarantee completeness of data and also to make sure that we're",
    "start": "883240",
    "end": "890000"
  },
  {
    "text": "providing a satisfactory user experience because one of the cool things about a system that's as distributed and as scaled as as Lambda and S3 if you retry",
    "start": "890000",
    "end": "898240"
  },
  {
    "text": "a request that failed 99.999% of the time that retri request will succeed even though it's the exact",
    "start": "898240",
    "end": "904880"
  },
  {
    "text": "same payload the exact same files that's it's trying to access It's just sometimes your request gets eaten by the",
    "start": "904880",
    "end": "910720"
  },
  {
    "text": "distributed system you are a part of the distributed system and we just use the patterns for distributed fault tolerance",
    "start": "910720",
    "end": "916480"
  },
  {
    "text": "to work around that so if you just impatiently retry the slowest 1% of requests it gives you some reasonable",
    "start": "916480",
    "end": "921959"
  },
  {
    "text": "bounds on how quickly your requests are going to come back there's another problem here um",
    "start": "921959",
    "end": "927160"
  },
  {
    "text": "funnily enough there's a gentleman here from AWS that was picking my brain about this so again Amazon lambda's designed",
    "start": "927160",
    "end": "935480"
  },
  {
    "text": "with you know you have to back it with a API Gateway or you have to back it with uh with SNS or something like that",
    "start": "935480",
    "end": "941480"
  },
  {
    "text": "they're really not happy with this idea of sending arbitrary grpc or protuff encoded payloads so we're actually",
    "start": "941480",
    "end": "947839"
  },
  {
    "text": "straying the limits of you know we'll send in a in a single payload you know hey by the way here's two megabytes of",
    "start": "947839",
    "end": "953040"
  },
  {
    "text": "data that contains a list of files to read a query specification we have to encode it from efficient protuff format",
    "start": "953040",
    "end": "960279"
  },
  {
    "text": "which we internally stored as to Json to serialize it over the wire and compressed and eventually we W wound up",
    "start": "960279",
    "end": "966880"
  },
  {
    "text": "um we wound up compr serializing the uh the the pay the payload and just Bas 64",
    "start": "966880",
    "end": "973959"
  },
  {
    "text": "encoding it and putting it in a Json file that says compressed undor payload colon you know a a a AA something",
    "start": "973959",
    "end": "980279"
  },
  {
    "text": "something the other thing is the return values right the return values in line in Lambda you can only get up to 6 megabytes back if you're potentially",
    "start": "980279",
    "end": "987319"
  },
  {
    "text": "running a high cardinality group Group by you have potentially you know hundreds of thousands of returned group fields that you're you know saying hey",
    "start": "987319",
    "end": "994040"
  },
  {
    "text": "there were five of this particular user and three of this particular user in this batch and you have to Aggregate and",
    "start": "994040",
    "end": "999360"
  },
  {
    "text": "merge them back together the results can be larger than six Megs so you have to wind up uh creating files on S3 to kind",
    "start": "999360",
    "end": "1006360"
  },
  {
    "text": "of act as that message bust between um temporarily park your data rather than have it returned in",
    "start": "1006360",
    "end": "1013120"
  },
  {
    "text": "line but again like you know why is this worth it it's speed",
    "start": "1013120",
    "end": "1019160"
  },
  {
    "text": "else would possibly let us invoke millions of do millions of invocations to have it come back within 10 seconds",
    "start": "1019160",
    "end": "1025038"
  },
  {
    "text": "and Achieve perfect utilization and sure you might say you know but Liz lambda's so expensive well kind of have you ever",
    "start": "1025039",
    "end": "1031280"
  },
  {
    "text": "had an ec2 box that you you know had to pay for while it was booting up and then while it was idle and then while you",
    "start": "1031280",
    "end": "1036480"
  },
  {
    "text": "were deciding whether or not to shut it down because of utilization we don't have to worry about that so yes you know",
    "start": "1036480",
    "end": "1043038"
  },
  {
    "text": "Lambda is about four times as expensive as as running ec2 Flatout per per core",
    "start": "1043039",
    "end": "1048600"
  },
  {
    "text": "but gosh if I can have 100% utilization like that more than makes up for it in order for Lambda to be to be um to be",
    "start": "1048600",
    "end": "1055960"
  },
  {
    "text": "more expensive than than ec2 given that we get less than 5% total U utilization",
    "start": "1055960",
    "end": "1061120"
  },
  {
    "text": "it would have to be 20 times more expensive for for it to be worth worth it for us to run run ec2 VMS flat out",
    "start": "1061120",
    "end": "1067160"
  },
  {
    "text": "for this and yes we do measure the cost yes we have service level objectiv set on",
    "start": "1067160",
    "end": "1073760"
  },
  {
    "text": "you know this is the list of customers we expect to cost a lot and if there are any other use cases where people are",
    "start": "1073760",
    "end": "1079080"
  },
  {
    "text": "running a lot of lot of queries and hitting Cold Storage we want to know so that we can optimize it because otherwise yes the cost will kind of run",
    "start": "1079080",
    "end": "1085520"
  },
  {
    "text": "away there so the trade-off that we are making here is that we think that human",
    "start": "1085520",
    "end": "1090760"
  },
  {
    "text": "detention is very important and that it's worth spending money to optimize around returning results fast for",
    "start": "1090760",
    "end": "1096520"
  },
  {
    "text": "arbitrary queries rather than pre- aggregating or rather than waiting for for something to uh to come back but it",
    "start": "1096520",
    "end": "1103400"
  },
  {
    "text": "turns out that maybe for queries that are predictable right like where someone has some kind of report that they run",
    "start": "1103400",
    "end": "1109400"
  },
  {
    "text": "every night at 2: a.m. and that a machine is looking at maybe we shouldn't be running this against Lambda but at",
    "start": "1109400",
    "end": "1115000"
  },
  {
    "text": "the end of the day we would still pay the cost of the S3 axises we would still pay some of these fixed costs even if we",
    "start": "1115000",
    "end": "1120480"
  },
  {
    "text": "weren't paying for Lambda but what if we could make all this a lot cheaper what if we could make",
    "start": "1120480",
    "end": "1126799"
  },
  {
    "text": "it 30% cheaper wouldn't that be nice right like slashing 30% off of a hundreds of thousands of dollars per",
    "start": "1126799",
    "end": "1132520"
  },
  {
    "text": "month bill that sounds awesome right so I agree and I think it's not",
    "start": "1132520",
    "end": "1138960"
  },
  {
    "start": "1135000",
    "end": "1199000"
  },
  {
    "text": "just about the money I think that it is the socially responsible thing to do to use lower power consuming compute and",
    "start": "1138960",
    "end": "1146880"
  },
  {
    "text": "yes Amazon does pass the savings on so in Australia where I spent a good",
    "start": "1146880",
    "end": "1152000"
  },
  {
    "text": "portion of the year 2018 and 2019 were some of the worst years on record for",
    "start": "1152000",
    "end": "1157159"
  },
  {
    "text": "climate change in particular there are some of the worst bush fires we have ever seen in Australia that happened in",
    "start": "1157159",
    "end": "1162919"
  },
  {
    "text": "the 2019 Fire season so what you're looking at there that's the um that's that's the map of temperature anomaly",
    "start": "1162919",
    "end": "1170159"
  },
  {
    "text": "and over or sorry that one's the this one's the rainfall anomaly this one's the temperature anomaly but you can see",
    "start": "1170159",
    "end": "1175520"
  },
  {
    "text": "it was a hot dry summer and things were burning at Yao",
    "start": "1175520",
    "end": "1181880"
  },
  {
    "text": "2019 you could smell the smoke in the air your eyes would water when you walking through the streets of down of",
    "start": "1181880",
    "end": "1187679"
  },
  {
    "text": "downtown Sydney it was it was not a good time and why is this happening and it's",
    "start": "1187679",
    "end": "1193400"
  },
  {
    "text": "not just too little rainfall it's also too much rainfall at the same time why is this happening human generated",
    "start": "1193400",
    "end": "1198720"
  },
  {
    "text": "climate change and software is a significant portion of this so I learned",
    "start": "1198720",
    "end": "1203760"
  },
  {
    "start": "1199000",
    "end": "1262000"
  },
  {
    "text": "I think similar to similar to many of you that in in 1999 25 years ago that's both when this conference was founded",
    "start": "1203760",
    "end": "1209840"
  },
  {
    "text": "and also when the word carbon footprint was founded and do you know who created the word carbon footprint fossil fuel",
    "start": "1209840",
    "end": "1216080"
  },
  {
    "text": "companies because they want you to believe that it's your individual responsibility guess what people don't",
    "start": "1216080",
    "end": "1221159"
  },
  {
    "text": "have carbon Footprints companies do companies are responsible for a majority of those",
    "start": "1221159",
    "end": "1226440"
  },
  {
    "text": "emissions and yes at this point carbon emissions from uh from Computing are",
    "start": "1226440",
    "end": "1233679"
  },
  {
    "text": "just as high as carbon emissions from Aviation so if you think about flight chaming someone next time think about",
    "start": "1233679",
    "end": "1238720"
  },
  {
    "text": "you know how many how many servers did I set running how much does that cost the environment right it's not just a number",
    "start": "1238720",
    "end": "1243760"
  },
  {
    "text": "on your AWS bill it's it's actually something that you have a much more meaningful opportunity to affect as a",
    "start": "1243760",
    "end": "1249760"
  },
  {
    "text": "technologist the resources that you use compared to the amount that you fly so offsetting is not the answer because",
    "start": "1249760",
    "end": "1255799"
  },
  {
    "text": "offsetting um it's it's just uh people just make it up if people can find a way to cheat the system they will",
    "start": "1255799",
    "end": "1262039"
  },
  {
    "start": "1262000",
    "end": "2243000"
  },
  {
    "text": "so wasting less power is and that's why I strongly feel that graviton and arm",
    "start": "1262039",
    "end": "1268039"
  },
  {
    "text": "chips are are a key component of how we make it much less expensive to our",
    "start": "1268039",
    "end": "1274080"
  },
  {
    "text": "wallets into the environment to do the things that we're used to doing with Computing so how to use less power don't",
    "start": "1274080",
    "end": "1280480"
  },
  {
    "text": "run the workload at all particular does it really make sense to use an AI model",
    "start": "1280480",
    "end": "1286360"
  },
  {
    "text": "for that or would you be better writing a writing a shell script um optimize your workload profile",
    "start": "1286360",
    "end": "1293240"
  },
  {
    "text": "it um set set performance measures on on whether or not you're regressing if you're doing something that's going to",
    "start": "1293240",
    "end": "1299480"
  },
  {
    "text": "spend a lot of Cycles we certainly do that and then finally use more the most efficient possible technology that you",
    "start": "1299480",
    "end": "1305440"
  },
  {
    "text": "can so we've seen really great results with graviton and I want to share what was that migration like both in terms of",
    "start": "1305440",
    "end": "1311760"
  },
  {
    "text": "migrating our server full and our server less workloads um our server less workloads were some of the earlier uh",
    "start": "1311760",
    "end": "1318159"
  },
  {
    "text": "workloads to migrant but not the very first so just a recap and I don't have to talk about a lot of details of this",
    "start": "1318159",
    "end": "1324760"
  },
  {
    "text": "because Matt covered a lot of it in his talk but in short architecture basically relates to the assembly instructions",
    "start": "1324760",
    "end": "1331679"
  },
  {
    "text": "that your computer is executing how those get translated into what the transistors are actually doing on the silicon and essentially you can think of",
    "start": "1331679",
    "end": "1338320"
  },
  {
    "text": "the architecture as an API for you as a programmer to interact with the hardware that's running so originally there was a",
    "start": "1338320",
    "end": "1345679"
  },
  {
    "text": "plethora of different architectures and everyone had to pay attention to does my software run on a Vex or on a z80 or on",
    "start": "1345679",
    "end": "1351760"
  },
  {
    "text": "an s370 but the error of desktop Computing came along and it seemed like you know it was just going to be x86 or",
    "start": "1351760",
    "end": "1359640"
  },
  {
    "text": "Macintosh and then after the world went to 64 bit suddenly it was like okay actually Apple has moved entirely to x86",
    "start": "1359640",
    "end": "1367200"
  },
  {
    "text": "64 processors PCS are using x86 64 processors the the architecture Wars are one",
    "start": "1367200",
    "end": "1373279"
  },
  {
    "text": "right but that's not entirely accurate because silently what was happening oh man I left my prop back there my cell",
    "start": "1373279",
    "end": "1380360"
  },
  {
    "text": "phone don't worry about it I I left we'll Pretend This clicker is my cell phone right like suddenly we started",
    "start": "1380360",
    "end": "1385520"
  },
  {
    "text": "picking up these things and having them in front of our faces all the time and they are not Tethered to power supplies we needed batteries that could last all",
    "start": "1385520",
    "end": "1392000"
  },
  {
    "text": "day long and therefore the place where we started seeing Innovation and architecture was in the mobile phone",
    "start": "1392000",
    "end": "1398000"
  },
  {
    "text": "industry first with arm 32 processors and then people realized wait a second if saving power is useful for",
    "start": "1398000",
    "end": "1405279"
  },
  {
    "text": "keeping people's cell phones lit so they can go and click on Tik Tok all day what if we could use it in the data center",
    "start": "1405279",
    "end": "1410880"
  },
  {
    "text": "too and that's where arm 64 and risk 5 and kind of all these architectures that we now seen springing up have come from",
    "start": "1410880",
    "end": "1418279"
  },
  {
    "text": "because they've taken this innovation of being of of suddenly caring about power and realizing this has a environmental",
    "start": "1418279",
    "end": "1424279"
  },
  {
    "text": "cost and this has a cost that can be passed on uh to the to the uh cloud computing",
    "start": "1424279",
    "end": "1430240"
  },
  {
    "text": "consumer so even with that that wasn't enough to change the game what really",
    "start": "1430240",
    "end": "1436240"
  },
  {
    "text": "changed the game was the fact that Linux one in the data center that open source software one in the data center that we",
    "start": "1436240",
    "end": "1442600"
  },
  {
    "text": "no longer were kind of tied to this idea of you get this packaged piece of software from your vendor and either it",
    "start": "1442600",
    "end": "1449320"
  },
  {
    "text": "works on the architecture or it doesn't work and you cannot use that architecture but having open source",
    "start": "1449320",
    "end": "1455360"
  },
  {
    "text": "available where you could just recompile your own dependency tree and make it work on a new architecture that's what",
    "start": "1455360",
    "end": "1461080"
  },
  {
    "text": "enabled people to start using the set scale including honeycomb another piece that I think is",
    "start": "1461080",
    "end": "1466600"
  },
  {
    "text": "also covered by Matt's talk a little bit is is complex versus simple instruction sets the fundamental reason why arm is",
    "start": "1466600",
    "end": "1473360"
  },
  {
    "text": "the future of computing and why we should think about adopting workloads server full and serverless is that arm",
    "start": "1473360",
    "end": "1479679"
  },
  {
    "text": "does not bring the Legacy baggage of of 40 Years of of of x86 32 along with it",
    "start": "1479679",
    "end": "1487559"
  },
  {
    "text": "there is a ton of the surface area of every Intel and AMD die that is devoted",
    "start": "1487559",
    "end": "1493960"
  },
  {
    "text": "just to translating x86 assembly into the underlying simple instruction set uh",
    "start": "1493960",
    "end": "1500880"
  },
  {
    "text": "that that is used used under the hood uh by by the Intel and AMD chips but if you",
    "start": "1500880",
    "end": "1506600"
  },
  {
    "text": "could just get rid of all of the decode units because it all just translates directly to what the chip is doing",
    "start": "1506600",
    "end": "1513480"
  },
  {
    "text": "that's a lot more power efficient and youve been told a lie in terms of you know oh like that has two vcpu that",
    "start": "1513480",
    "end": "1520200"
  },
  {
    "text": "means that I can run two things in parallel at all times right you can decode two things in parallel you cannot",
    "start": "1520200",
    "end": "1527279"
  },
  {
    "text": "execute two things in parallel with x86 whereas with arm one virtual CPU is",
    "start": "1527279",
    "end": "1533960"
  },
  {
    "text": "one core decode and execution and that means that you are not contending yourself sure it might have worked in",
    "start": "1533960",
    "end": "1540640"
  },
  {
    "text": "the server environment it might have worked on the desktop and on your gaming PC where you are not containing yourself or two different workloads are trying to",
    "start": "1540640",
    "end": "1546640"
  },
  {
    "text": "do different things and they are spiking the CPU at different times but when you are running an at scale Cloud",
    "start": "1546640",
    "end": "1552240"
  },
  {
    "text": "workload if you spin up like honeycomb spins up you know hundreds you know 100 100 100,000 invokes right and each of",
    "start": "1552240",
    "end": "1560200"
  },
  {
    "text": "those is using four four cores those four cores are all going to be demanding the exact same like profile of things",
    "start": "1560200",
    "end": "1566880"
  },
  {
    "text": "that they're trying to get the CPU to do at the same time and they're contending the resources so we see less tail latency",
    "start": "1566880",
    "end": "1573320"
  },
  {
    "text": "and less performance variability so these processors are really almost you can think of them as being built for the",
    "start": "1573320",
    "end": "1579039"
  },
  {
    "text": "cloud first because they are designed to be resilient against everything from",
    "start": "1579039",
    "end": "1584120"
  },
  {
    "text": "shared cash attacks uh meltdown inspector they're designed to be resistant to contention of you against",
    "start": "1584120",
    "end": "1589840"
  },
  {
    "text": "your own workload and they're also better for the environment because they just consume less power because the more surface area",
    "start": "1589840",
    "end": "1596120"
  },
  {
    "text": "and the more transistors on the die the more power it consumes so honeycom adopted uh adopted",
    "start": "1596120",
    "end": "1602760"
  },
  {
    "text": "graviton pretty early on in in the year 2020 and we found that they delivered",
    "start": "1602760",
    "end": "1608000"
  },
  {
    "text": "significantly better tail leny and also that Amazon was willing to offer them uh to us for 10% less per",
    "start": "1608000",
    "end": "1615200"
  },
  {
    "text": "core and this was initially a little bit of a process right we are trying to figure out which workload to migrate you",
    "start": "1615200",
    "end": "1620600"
  },
  {
    "text": "know the serverless workload the server full workload how do we actually make sure that you know how do we spin up a brand new kind of Base stack from",
    "start": "1620600",
    "end": "1627679"
  },
  {
    "text": "scratch and the answer is that your base operating system is a Docker layer or is",
    "start": "1627679",
    "end": "1633120"
  },
  {
    "text": "an ec2 image and you can basically go and take your infrastructure as code rules and apply them to generate new",
    "start": "1633120",
    "end": "1638840"
  },
  {
    "text": "images based off of whichever uh architecture is appropriate to the task and then it's a matter of does your code",
    "start": "1638840",
    "end": "1645080"
  },
  {
    "text": "compile does it work and can you produce a good build AR effect so one of the",
    "start": "1645080",
    "end": "1650320"
  },
  {
    "text": "things that worked in our favor here is we are a goang shop and I know this sounds weird because I know that you know rust and C++ are kind of you know",
    "start": "1650320",
    "end": "1657600"
  },
  {
    "text": "if you are working close to the bare metal you have to use it um in order to get the best results but in our",
    "start": "1657600",
    "end": "1663279"
  },
  {
    "text": "experience we use go for readability first and for anything that's really really performance critical that's where",
    "start": "1663279",
    "end": "1670360"
  },
  {
    "text": "you have the ability to drop from go into Assembly Language in line with the go compiler so we are able to at least",
    "start": "1670360",
    "end": "1677159"
  },
  {
    "text": "very quickly like quick and dirty cross- compile does it work does the binary work your test pass that didn't",
    "start": "1677159",
    "end": "1682440"
  },
  {
    "text": "necessarily surface whether it was going to be performant that but we wanted just validate and see what happened uh so you just set an environment variable and go",
    "start": "1682440",
    "end": "1689000"
  },
  {
    "text": "will pick it up and go will cross compile and by the way this was before the M1 Max started shipping but ever",
    "start": "1689000",
    "end": "1694760"
  },
  {
    "text": "since M1 Mech started shipping suddenly everyone in their and and their dog has had to care about making sure their",
    "start": "1694760",
    "end": "1700440"
  },
  {
    "text": "stuff compiles for arm anyways so it was initially a little bit painful for us but I think for you all this journey to",
    "start": "1700440",
    "end": "1708279"
  },
  {
    "text": "using arm arm software is a lot easier because of the fact that everything that you might possibly use has already been",
    "start": "1708279",
    "end": "1714720"
  },
  {
    "text": "tested you may already be using an arm laptop sitting here in the audience right now but if you're using C++ with",
    "start": "1714720",
    "end": "1721760"
  },
  {
    "text": "hand assembly you know maybe that's not the first workload to migrate like you know focus on things that are either uh",
    "start": "1721760",
    "end": "1727360"
  },
  {
    "text": "things that have run times like Java or python uh or or things like go where you can easily cross compile out of the box",
    "start": "1727360",
    "end": "1734440"
  },
  {
    "text": "so builds without errors now what well how do we verify that it's actually you know is faster that it's cheaper that it",
    "start": "1734440",
    "end": "1740519"
  },
  {
    "text": "that it saves the environment right like if it's 10% faster per uh sorry if it's 10% cheaper per CPU second but it takes",
    "start": "1740519",
    "end": "1747960"
  },
  {
    "text": "twice as long to run you haven't actually saved any money you haven't actually saved the planet so we started running AB tests um",
    "start": "1747960",
    "end": "1755799"
  },
  {
    "text": "and the interesting thing that we discovered very quickly is that the arm processors",
    "start": "1755799",
    "end": "1761320"
  },
  {
    "text": "actually seemed seemed as if uh they were they were slightly slower it seemed",
    "start": "1761320",
    "end": "1766440"
  },
  {
    "text": "as as if we were going to need more of them so we were saying you know hey we're going to autoscale everything to",
    "start": "1766440",
    "end": "1772919"
  },
  {
    "text": "keep everything at 50% CPU utilization and we did that and it was",
    "start": "1772919",
    "end": "1778159"
  },
  {
    "text": "like hey by the way you're going to need 30% more of these to maintain 50% CPU utilization so it turns out the error",
    "start": "1778159",
    "end": "1785320"
  },
  {
    "text": "there is that that thing about multi-threading that I mentioned do you know why we had to run at 50%",
    "start": "1785320",
    "end": "1791720"
  },
  {
    "text": "utilization two decode units one execute unit so it turns out that with arm",
    "start": "1791720",
    "end": "1796760"
  },
  {
    "text": "processors we were able to set our Auto scaling targets much much higher and still maintain the same Target latency",
    "start": "1796760",
    "end": "1802240"
  },
  {
    "text": "we were still getting the same performance with the higher CPU Auto scaling threshold so we were actually",
    "start": "1802240",
    "end": "1807399"
  },
  {
    "text": "able to cut the number of instances that we were running by 20% have each instance be 10% cheaper and actually get",
    "start": "1807399",
    "end": "1813840"
  },
  {
    "text": "that headline number that aw said of this is going to be 30% more uh more price performance than than the Intel",
    "start": "1813840",
    "end": "1819840"
  },
  {
    "text": "previous generation instances cool so we migrated our injust service and then we migrated our uh",
    "start": "1819840",
    "end": "1825880"
  },
  {
    "text": "storage workers the ones that are kind of doing that uh translation of the",
    "start": "1825880",
    "end": "1831240"
  },
  {
    "text": "incoming data into the Coler storage format and then reading it on demand uh",
    "start": "1831240",
    "end": "1836279"
  },
  {
    "text": "at least the local SSD part so we did that and we found basically immediately that the",
    "start": "1836279",
    "end": "1842200"
  },
  {
    "text": "P99 dropped that when someone ran a query that hit only local storage that",
    "start": "1842200",
    "end": "1848440"
  },
  {
    "text": "went from taking I think 2 seconds at P99 to 1 second at P99 if it was a pure local query which I think is really cool",
    "start": "1848440",
    "end": "1854640"
  },
  {
    "text": "right like because we're in a situation where we stopped cont in with ourselves and that enabled us to grow",
    "start": "1854640",
    "end": "1861320"
  },
  {
    "text": "and grow and grow without having to scale out our number of instances which is pretty cool right and then uh we did",
    "start": "1861320",
    "end": "1866799"
  },
  {
    "text": "that trick again because Amazon uh released a new generation of instances so we tried it out and sure enough uh",
    "start": "1866799",
    "end": "1873200"
  },
  {
    "text": "further drops in the in the uh median latency that it takes to run a local query and a P99 uh uh",
    "start": "1873200",
    "end": "1880000"
  },
  {
    "text": "latency so that's kind of what we saw and we also saw better CPU utilization",
    "start": "1880000",
    "end": "1885880"
  },
  {
    "text": "um again which means that we can crank up the screw is even Tighter and there's a new generation",
    "start": "1885880",
    "end": "1891840"
  },
  {
    "text": "that's out and it's unfortunately not available for Lambda yet so Lambda funly enough is still stuck in graviton 2",
    "start": "1891840",
    "end": "1898559"
  },
  {
    "text": "which was was released in uh which was released in 2021 but the good news is that you're",
    "start": "1898559",
    "end": "1906360"
  },
  {
    "text": "compare when you're comparing Lambda versus Lambda Lambda x86 versus Lambda arm you're comparing essentially uh",
    "start": "1906360",
    "end": "1913320"
  },
  {
    "text": "fifth generation or sixth generation instances against each other because it's not like Lambda x86 gets the newest",
    "start": "1913320",
    "end": "1920279"
  },
  {
    "text": "and best uh X8 gets gets the newest and best instances that Amazon has released either so you know it's essentially",
    "start": "1920279",
    "end": "1927440"
  },
  {
    "text": "right like Lambda is a value product right it's a scale Out product it's meant to be available to absorb massive",
    "start": "1927440",
    "end": "1934399"
  },
  {
    "text": "spikes but it does not offer the best single core performance whether you're running on Armor x86 but generation to",
    "start": "1934399",
    "end": "1940399"
  },
  {
    "text": "generation right like in the case of server full applications we were able to see after doing the switch to arm",
    "start": "1940399",
    "end": "1946360"
  },
  {
    "text": "architecture once we were able to see our performance basically double per core within with",
    "start": "1946360",
    "end": "1953840"
  },
  {
    "text": "within within 3 to four years which I think is really cool so we have been 100% armed servers",
    "start": "1953840",
    "end": "1959799"
  },
  {
    "text": "for two and a half years and we turned off our last uh x86 server in April of",
    "start": "1959799",
    "end": "1966360"
  },
  {
    "text": "2022 and also uh with regards to with regard to arm uh We've started switching",
    "start": "1966360",
    "end": "1972200"
  },
  {
    "text": "our lambdas over but the process of switching our lambdas over actually was really fraught we countered we we we",
    "start": "1972200",
    "end": "1978960"
  },
  {
    "text": "thought you know sure the server full work workload works so well when we switched it why can't we just push this",
    "start": "1978960",
    "end": "1984760"
  },
  {
    "text": "magic button that says make go faster and why why doesn't it magically work so I'll call it the numbers on the bottom",
    "start": "1984760",
    "end": "1991000"
  },
  {
    "text": "of this you'll see that with the same Siz AMD versus arm uh architecture",
    "start": "1991000",
    "end": "1996399"
  },
  {
    "text": "lambdas that the it was 50 to 100% worse performance with arm initially when we",
    "start": "1996399",
    "end": "2002720"
  },
  {
    "text": "had just naively migrated everything over so I had to revert that experiment very quick because obviously we're not",
    "start": "2002720",
    "end": "2008919"
  },
  {
    "text": "going to do a thing that was inefficient slowing our customers experience down so",
    "start": "2008919",
    "end": "2014000"
  },
  {
    "text": "we you know fli the feature flag back off uh so only 1% of queries were running against the old against arm and",
    "start": "2014000",
    "end": "2021080"
  },
  {
    "text": "we started figuring out and scratching our heads and working with our AWS team to figure out like what in the heck is going on we saw such great results with",
    "start": "2021080",
    "end": "2028080"
  },
  {
    "text": "with server full why is serverless so slow so it turns out it was uh three or",
    "start": "2028080",
    "end": "2033159"
  },
  {
    "text": "four different things first of all it turns out Amazon has a ton of old x86",
    "start": "2033159",
    "end": "2040639"
  },
  {
    "text": "instances sitting around that they can give you for Lambda if you are like us and you rock up and you say you know",
    "start": "2040639",
    "end": "2046519"
  },
  {
    "text": "100,000 invokes please and it's for a brand new architecture they haven't fully rolled out you're going to",
    "start": "2046519",
    "end": "2052320"
  },
  {
    "text": "experience a little bit of a capacity crunch I can assure you by the way that problem is no longer there there's a lot",
    "start": "2052320",
    "end": "2058079"
  },
  {
    "text": "of graviton 2 floating around because graviton 3 and graviton 4 have come out so now there's a lot more capacity",
    "start": "2058079",
    "end": "2063839"
  },
  {
    "text": "available but we didn't know that at the time again this is why talking to your account team is so important secondly at the time uh go 117",
    "start": "2063839",
    "end": "2073480"
  },
  {
    "text": "had introduced for x86 the ability to call between functions without hitting the stack without passing every argument",
    "start": "2073480",
    "end": "2080200"
  },
  {
    "text": "to the stack to memory and then and then getting it back but it didn't Implement that optimization for arm until go 118",
    "start": "2080200",
    "end": "2088000"
  },
  {
    "text": "so we had to wait for that to happen and then third we discovered that unlike the",
    "start": "2088000",
    "end": "2093679"
  },
  {
    "text": "rest of our workload where everything just ran even without hand assembly optimization the lz4 library because we",
    "start": "2093679",
    "end": "2100760"
  },
  {
    "text": "of course compress our files before we send them off to S3 the lz4 library had",
    "start": "2100760",
    "end": "2105920"
  },
  {
    "text": "been hand assembly optimized for x86 but the version of it that we were using for go had not hand optimized the",
    "start": "2105920",
    "end": "2112560"
  },
  {
    "text": "arm 64 library and turns out that was part of that core critical Loop right because part of analyzing Evil's data",
    "start": "2112560",
    "end": "2118480"
  },
  {
    "text": "involves decompressing it once you get it back from S3 so we contributed Upstream a fix that",
    "start": "2118480",
    "end": "2123839"
  },
  {
    "text": "allowed the lz4 library to be hand assembly optimized and suddenly between",
    "start": "2123839",
    "end": "2128960"
  },
  {
    "text": "all three of those things suddenly we started getting that 20% better uh price performance and also uh one side note it",
    "start": "2128960",
    "end": "2134920"
  },
  {
    "text": "turns out that um if your lambdas are cold starting or even warm starting and they don't have an open connection to S3",
    "start": "2134920",
    "end": "2142160"
  },
  {
    "text": "you wind up paying the price to open a socket to do an SSL handshake to validate that the S3 SSL certificates",
    "start": "2142160",
    "end": "2149800"
  },
  {
    "text": "hasn't changed since you built it um and yeah that's that's something that you can just skip if if you trust that",
    "start": "2149800",
    "end": "2157160"
  },
  {
    "text": "Lambda to S3 Communications are secure so we were able to flip it on and",
    "start": "2157160",
    "end": "2162680"
  },
  {
    "text": "uh it took us basically from November 2021 until October 2022 to get enough",
    "start": "2162680",
    "end": "2168920"
  },
  {
    "text": "capacity into place for us to become 100% Lambda or at least 99% Lambda and we were actually able to see",
    "start": "2168920",
    "end": "2175800"
  },
  {
    "text": "the um this is a misleading graph actually because it looks like the the the latency is equivalent or about the",
    "start": "2175800",
    "end": "2181200"
  },
  {
    "text": "same between x86 and Lambda the thing I haven't told you is we are able to take 20% of the cores away from the from the",
    "start": "2181200",
    "end": "2187880"
  },
  {
    "text": "arm lambdas so each each CPU second that or sry each uh millisecond that that",
    "start": "2187880",
    "end": "2194000"
  },
  {
    "text": "that that Lambda is invoking on on arm 64 it's costing 10% less because the cost per core millisecond is less on arm",
    "start": "2194000",
    "end": "2201960"
  },
  {
    "text": "and also we are provisioning fewer cores because it was still able to keep up even when we took the cores away the",
    "start": "2201960",
    "end": "2208520"
  },
  {
    "text": "magic of power efficient CPUs and actually um earlier this week I",
    "start": "2208520",
    "end": "2213599"
  },
  {
    "text": "actually submitted a pool request that just deleted all of the uh x86 lambdas we after you know running continuous",
    "start": "2213599",
    "end": "2219960"
  },
  {
    "text": "validation of you know hey we're going to hold back 1% of our traffic just in case we need to swap back hey we're going to hold back 1% of our traffic the",
    "start": "2219960",
    "end": "2226240"
  },
  {
    "text": "day never came and we had to switch back from arm to x86 so we're like okay technical complexity like every",
    "start": "2226240",
    "end": "2231960"
  },
  {
    "text": "migration has to have an end point this is the end of the road for arm migration like we're we're killing off everything",
    "start": "2231960",
    "end": "2237880"
  },
  {
    "text": "in our infrastructure that is not that is not arm whether it's server full or server lless so are should you do this at home",
    "start": "2237880",
    "end": "2246760"
  },
  {
    "start": "2243000",
    "end": "2466000"
  },
  {
    "text": "maybe I think the question is basically in the case of server full versus serverless do",
    "start": "2246760",
    "end": "2252800"
  },
  {
    "text": "you have a workload that you can map ruce you don't have to you know embrace",
    "start": "2252800",
    "end": "2258000"
  },
  {
    "text": "the you know oh we're going to go um we're going to go API Gateway first we're going to decompose everything into",
    "start": "2258000",
    "end": "2263640"
  },
  {
    "text": "interess functions right like you do not have to go on that Journey you just have to find a workload that is",
    "start": "2263640",
    "end": "2269960"
  },
  {
    "text": "embarrassingly parallel that you can that you can invoke so you know I think it's really cool when you have a",
    "start": "2269960",
    "end": "2275400"
  },
  {
    "text": "Greenfield opportunity like newbank did to kind of basically make everything make everything serverless or or or",
    "start": "2275400",
    "end": "2281240"
  },
  {
    "text": "containers um but in our case we started with a very server full workload and we're able to still find Value out of",
    "start": "2281240",
    "end": "2288640"
  },
  {
    "text": "serverless just be mindful right like you know as I was saying someone calls honeycomb VI the API and doesn't need the results within 10 seconds like maybe",
    "start": "2288640",
    "end": "2295680"
  },
  {
    "text": "don't run that against Lambda that's just going to run up a very expensive Bill and not actually give the user any better Behavior than if it took 5",
    "start": "2295680",
    "end": "2301599"
  },
  {
    "text": "minutes to do um we still have our local uh server",
    "start": "2301599",
    "end": "2307640"
  },
  {
    "text": "workers pull individual files off of S3 if they're being asked to pull less than a few thousand files like it just makes",
    "start": "2307640",
    "end": "2313480"
  },
  {
    "text": "sense to process those in serial on a small number of CPUs because there is a cost to invoking a Lambda there is a",
    "start": "2313480",
    "end": "2320119"
  },
  {
    "text": "cost to spinning up that worker make sure you do capacity planning because you know yes serverless",
    "start": "2320119",
    "end": "2326200"
  },
  {
    "text": "is serverless but serverless ultimately runs on physical servers so make sure that your cloud provider has enough",
    "start": "2326200",
    "end": "2331800"
  },
  {
    "text": "capacity available similar to how you would check does your cloud provider have enough spot capacity available does your cloud provider have enough fargate",
    "start": "2331800",
    "end": "2338359"
  },
  {
    "text": "available right like make sure that you're communicating about how much you intend to use and actually do",
    "start": "2338359",
    "end": "2343960"
  },
  {
    "text": "performance testing I loved the uh opening key remark about like you know are you doing a regression test right",
    "start": "2343960",
    "end": "2349280"
  },
  {
    "text": "like do you know for sure what your performance thresholds are and can you validate continuously that your binaries",
    "start": "2349280",
    "end": "2355720"
  },
  {
    "text": "continue to meet that goal and make sure everything is tuned and optimized and use observability to",
    "start": "2355720",
    "end": "2361440"
  },
  {
    "text": "actually like make sure that you are experimenting right like you're not just introducing chaos but they actually",
    "start": "2361440",
    "end": "2367040"
  },
  {
    "text": "valid does this make sense for us maybe it does make sense for you maybe it doesn't but you're not going to know unless you're actually measuring the",
    "start": "2367040",
    "end": "2373200"
  },
  {
    "text": "before and after so when you do this as part of an arm migration this means that you're",
    "start": "2373200",
    "end": "2379520"
  },
  {
    "text": "going to be able to make uh your Cloud bill go down and and save the environment by consuming less power but",
    "start": "2379520",
    "end": "2386440"
  },
  {
    "text": "you kind of have to be prepared to do that homework you have to be prepared just in case it doesn't go 100% to plan",
    "start": "2386440",
    "end": "2393000"
  },
  {
    "text": "but there are some easy things that you can do today for instance if you're using a managed service like RDS if",
    "start": "2393000",
    "end": "2399240"
  },
  {
    "text": "you're using a managed service like uh like uh open search you can just click the box that says yes please use",
    "start": "2399240",
    "end": "2405520"
  },
  {
    "text": "graviton for this and of course validate you know that your CPU utilization goes down validate you can run more workload",
    "start": "2405520",
    "end": "2411000"
  },
  {
    "text": "but that's an easy way that doesn't involve having to uh cross compile or or change your code or set up multi-arc",
    "start": "2411000",
    "end": "2417920"
  },
  {
    "text": "kubernetes at the end of the day nothing matters unless users developers are happy if you are running a platform",
    "start": "2417920",
    "end": "2424640"
  },
  {
    "text": "right like your users are your developers if you're running if you're developing uh software as a product engineer right like your users are the",
    "start": "2424640",
    "end": "2431040"
  },
  {
    "text": "people out in the field but either way right like we need to try to get them the best possible results with the least",
    "start": "2431040",
    "end": "2438400"
  },
  {
    "text": "possible cost and with the least possible environmental impact so that's flurry F Jones it's my dog uh this is",
    "start": "2438400",
    "end": "2445119"
  },
  {
    "text": "our book observability Engineering in case you're interested in learning more about how we put together the stus store including the Lambda pieces and overall",
    "start": "2445119",
    "end": "2453000"
  },
  {
    "text": "um yeah more than 2x price performance from fifth gen to 8 gen and we've had basically our service has",
    "start": "2453000",
    "end": "2460520"
  },
  {
    "text": "increased in scale by 20 times and our Cloud bill has not gone up by 20 times I can tell you that so go forth armed with",
    "start": "2460520",
    "end": "2467280"
  },
  {
    "start": "2466000",
    "end": "2487000"
  },
  {
    "text": "this info pardon the the pun uh and thank you very much [Applause]",
    "start": "2467280",
    "end": "2477920"
  }
]