[
  {
    "start": "0",
    "end": "119000"
  },
  {
    "text": "so welcome uh my name's Alan Smith um and I'm going to talk about large language models demystified a bit about",
    "start": "12320",
    "end": "18320"
  },
  {
    "text": "the maths uh behind the magic of the uh the GPT uh language models a bit about",
    "start": "18320",
    "end": "23840"
  },
  {
    "text": "myself um I've kind of resisted the temptation to go into um architecture or",
    "start": "23840",
    "end": "29080"
  },
  {
    "text": "project management or sales or anything I like writing code so I've just focused on writing code since 1995 when I",
    "start": "29080",
    "end": "35239"
  },
  {
    "text": "started my uh career I'm at Microsoft AI uh Microsoft MVP uh for artificial",
    "start": "35239",
    "end": "40440"
  },
  {
    "text": "intelligence I've held the title since 2005 in in various disciplines and I do a lot with the community I've been",
    "start": "40440",
    "end": "45719"
  },
  {
    "text": "involved in organizing conferences and helping with Koda dojos and during covid uh I was uh doing quite a bit of stuff",
    "start": "45719",
    "end": "52199"
  },
  {
    "text": "on YouTube I was scheduled to talk about GPT versus Starcraft unfortunately the",
    "start": "52199",
    "end": "57320"
  },
  {
    "text": "random nature of these language models meant that as soon as I stood on a podium in front of people the demos",
    "start": "57320",
    "end": "62640"
  },
  {
    "text": "didn't work as as good as they did back home so I do have all of that stuff on my YouTube channel if you if you do want",
    "start": "62640",
    "end": "68280"
  },
  {
    "text": "to go and check out that stuff I've edited out the when things go wrong on those those YouTube videos first of all",
    "start": "68280",
    "end": "73600"
  },
  {
    "text": "I'd just like to thank uh the other YouTubers and Community people who've uh done a fantastic job of being able to",
    "start": "73600",
    "end": "79119"
  },
  {
    "text": "help me uh to learn how these language uh models work some of you probably have seen some of those at those channels",
    "start": "79119",
    "end": "85000"
  },
  {
    "text": "there I'd especially recommend three blue one brown has just done a couple of um series uh looking at the maths behind",
    "start": "85000",
    "end": "91600"
  },
  {
    "text": "Transformer models which is uh which is fantastic to watch I really really recommend that so GPT we come in uh and",
    "start": "91600",
    "end": "98799"
  },
  {
    "text": "we ask it a question we ask it how to make cheese and and it spews out all of this actual uh response of of how uh it",
    "start": "98799",
    "end": "104880"
  },
  {
    "text": "can make cheese and we sometimes think it's human uh we sometimes feel sorry for it we sometimes get angry when it",
    "start": "104880",
    "end": "110240"
  },
  {
    "text": "makes uh makes mistakes but really it's just maths uh that's going on uh behind the scenes so hopefully I'll be able to",
    "start": "110240",
    "end": "116640"
  },
  {
    "text": "explain a bit about how that maths works the only thing these models do the GPT",
    "start": "116640",
    "end": "122439"
  },
  {
    "start": "119000",
    "end": "228000"
  },
  {
    "text": "models and many of the large language models is given a sequence of tokens the model will predict the probabilities of",
    "start": "122439",
    "end": "129200"
  },
  {
    "text": "each token being the next token in the sequence that's it that's the only thing uh that they do and we put a lot of",
    "start": "129200",
    "end": "135879"
  },
  {
    "text": "codee uh and stuff going around um to kind of make that work there are some seats at the front and scattered around",
    "start": "135879",
    "end": "142440"
  },
  {
    "text": "there if you're looking for a seat so uh we say the catat in the and GPT uh",
    "start": "142440",
    "end": "147599"
  },
  {
    "text": "basically comes out with these VAR predictions that four is going to be the most probable token with",
    "start": "147599",
    "end": "153160"
  },
  {
    "text": "7.64% so we stick that on the end we send that sequence through GPT again and",
    "start": "153160",
    "end": "158680"
  },
  {
    "text": "it comes out with comma 26.1 4% so we stick a comma on send that through again",
    "start": "158680",
    "end": "164080"
  },
  {
    "text": "and it comes out with and and we slap that on the end that's why we kind of see this stream uh of words or or tokens",
    "start": "164080",
    "end": "170760"
  },
  {
    "text": "appearing as GPT is making its predictions sequence prediction they can be variable length so we can have",
    "start": "170760",
    "end": "177080"
  },
  {
    "text": "different length the cat sat on there the king of rock and roll is Elvis something or you know load the training",
    "start": "177080",
    "end": "182319"
  },
  {
    "text": "data.txt into a list and display the top 10 lines and it will produce some python code to uh to do that for us the order",
    "start": "182319",
    "end": "190200"
  },
  {
    "text": "is also important if we say the film was not bad in fact it was very good compared with the film was not good in",
    "start": "190200",
    "end": "195599"
  },
  {
    "text": "fact it was very bad we've just swapped the entire sentiment uh of that uh text",
    "start": "195599",
    "end": "201000"
  },
  {
    "text": "by swapping the position of two words so position and Order are very important",
    "start": "201000",
    "end": "206319"
  },
  {
    "text": "also long-term dependencies the cat sat on the floor it had just been cleaned so it is going to refer to the floor versus",
    "start": "206319",
    "end": "213080"
  },
  {
    "text": "the cat set on the floor it had just been fed where it is going to refer to the cat so the language models do need",
    "start": "213080",
    "end": "219159"
  },
  {
    "text": "to understand what these actual um you know words are and this takes place in the attention mechanism which goes on uh",
    "start": "219159",
    "end": "226319"
  },
  {
    "text": "inside these these Transformer models we also get prompt engineering um to make",
    "start": "226319",
    "end": "231560"
  },
  {
    "start": "228000",
    "end": "285000"
  },
  {
    "text": "uh things better with the actual question answer so we're telling it what to do we're telling it what not to do",
    "start": "231560",
    "end": "237200"
  },
  {
    "text": "and then we're supplying it with with a user question and model is doing is predicting what tokens are going to come",
    "start": "237200",
    "end": "242840"
  },
  {
    "text": "next in this sequence one at a time based on that tokens we can also do uh",
    "start": "242840",
    "end": "248159"
  },
  {
    "text": "retrieval augmented generation where we basically slap some information in there typically from a search engine but again",
    "start": "248159",
    "end": "253920"
  },
  {
    "text": "it's just predicting uh the the tokens what is going to come next in that sequence and then also we've got chat",
    "start": "253920",
    "end": "259479"
  },
  {
    "text": "history so we ask it what is the capital of the Netherlands there is no state in these models so every time we're going",
    "start": "259479",
    "end": "266960"
  },
  {
    "text": "to be asking a question we're sending in the entire chat history of that chat so it can actually understand what's going",
    "start": "266960",
    "end": "273440"
  },
  {
    "text": "on and you hear about these models being able to take 4,000 tokens or 16,000 tokens and that refers to how much",
    "start": "273440",
    "end": "280440"
  },
  {
    "text": "history and how much data uh we can put in when we're using these retrieval augmented generation options there um",
    "start": "280440",
    "end": "287320"
  },
  {
    "start": "285000",
    "end": "645000"
  },
  {
    "text": "I'm working with with gpt2 uh Jody's session this morning was fantastic so if you're in there she did mention a bit",
    "start": "287320",
    "end": "293680"
  },
  {
    "text": "about gpt2 the reason I'm uh working on it is I can actually run it on a computer that I can carry around",
    "start": "293680",
    "end": "300160"
  },
  {
    "text": "uh in uh in my backpack uh which is not the case uh with the other language models also it's open source so I can",
    "start": "300160",
    "end": "306080"
  },
  {
    "text": "download the source code I can stick break points in and I can go and see uh how things work so if I set this thing",
    "start": "306080",
    "end": "312720"
  },
  {
    "text": "as the startup file and then control",
    "start": "312720",
    "end": "317919"
  },
  {
    "text": "F5 oh no why is my screen gone I'm Chad gpt2 a friendly chat bot",
    "start": "318319",
    "end": "326000"
  },
  {
    "text": "how can I help you I'm going to stop that is it when I",
    "start": "326000",
    "end": "331440"
  },
  {
    "text": "um okay I'll try that",
    "start": "333759",
    "end": "337680"
  },
  {
    "text": "duplicate",
    "start": "340360",
    "end": "343360"
  },
  {
    "text": "yes it takes a while to load the model I'm chat gpt2 a friendly chat bot how",
    "start": "347479",
    "end": "353319"
  },
  {
    "text": "can I help you what is the capital of the Netherlands",
    "start": "353319",
    "end": "359880"
  },
  {
    "text": "Dutch capital is Amsterdam is it nice",
    "start": "363039",
    "end": "369199"
  },
  {
    "text": "there it's nice there what can you do in",
    "start": "372560",
    "end": "379199"
  },
  {
    "text": "Amsterdam you can do anything in Amsterdam yeah so you can it's true yeah",
    "start": "382680",
    "end": "390120"
  },
  {
    "text": "so um you see that um this is when data scientists got really excited about this technology and thought wow it can",
    "start": "390120",
    "end": "396360"
  },
  {
    "text": "actually generate text and it's um syntactically correct as Jody was saying this morning but the general public",
    "start": "396360",
    "end": "402280"
  },
  {
    "text": "wouldn't really get excited if this was a chatot available you know it's like having a conversation with with a four-year-old it's going to come with",
    "start": "402280",
    "end": "408360"
  },
  {
    "text": "crazy answers but the data scientists realized that if we just um throw data at it and throw parameters and throw",
    "start": "408360",
    "end": "414240"
  },
  {
    "text": "lots and lots of gpus at it we're going to get something that's going to perform a lot uh a lot uh better",
    "start": "414240",
    "end": "420479"
  },
  {
    "text": "um so yeah another quick demo on the sequence prediction if I just uh set this guy as the um startup file we'll",
    "start": "420479",
    "end": "427199"
  },
  {
    "text": "see some other examples of how it's actually making that uh sequence uh prediction and the statistics behind",
    "start": "427199",
    "end": "433639"
  },
  {
    "text": "that so what I'm doing is I'm basically uh running some code actually I'll demo it in in my notebook uh I think it's uh",
    "start": "433639",
    "end": "439840"
  },
  {
    "text": "this can basically show how this works so what I'm doing here is importing some libraries so I'm importing the pytorch",
    "start": "439840",
    "end": "446000"
  },
  {
    "text": "stuff and then Transformers uh I'm importing GPT tokenizer and gp2 LM head model these are from hugging face and",
    "start": "446000",
    "end": "452639"
  },
  {
    "text": "then I've got pip plop so I can draw some graphs we then um create a tokenizer and load up the model uh which",
    "start": "452639",
    "end": "459599"
  },
  {
    "text": "is this code here I haven't got a big GPU on this laptop so I can't stick it on the GPU but that code would normally",
    "start": "459599",
    "end": "465240"
  },
  {
    "text": "do that I'm then defining uh some text strings that I can actually feed feed through the model and printing them out",
    "start": "465240",
    "end": "471120"
  },
  {
    "text": "and now we can use the model to predict the next uh word sorry uh token in the actual sequence of tokens remember it's",
    "start": "471120",
    "end": "477000"
  },
  {
    "text": "tokens uh not words so um I'll go for um text zero actually uh the cat set on",
    "start": "477000",
    "end": "484280"
  },
  {
    "text": "there and run that and then uh what we're getting out",
    "start": "484280",
    "end": "489440"
  },
  {
    "text": "is this actual tensor uh which is 1x5 by",
    "start": "489440",
    "end": "494479"
  },
  {
    "text": "50257 and to make sense of that what we can do is actually do a softmax",
    "start": "494479",
    "end": "500599"
  },
  {
    "text": "operation on it and get the actual probabilities out so the cat sat on there it's got floor with uh",
    "start": "500599",
    "end": "508440"
  },
  {
    "text": "7.64% there's other things that we can do as well so if we say the king of rock and switch to this demo because this",
    "start": "508440",
    "end": "514360"
  },
  {
    "text": "kind of uh highlights and uses pie charts so if I just do control F5 we'll start without",
    "start": "514360",
    "end": "521800"
  },
  {
    "text": "debugging again it takes a few seconds to load the model and um actually no it",
    "start": "524279",
    "end": "530440"
  },
  {
    "text": "wasn't that one it was this one sorry uh start without debugging what this is going to do is to",
    "start": "530440",
    "end": "537240"
  },
  {
    "text": "take a few sequence of sequences of uh Texs convert them into tokens send them into the gp2 model and then show us the",
    "start": "537240",
    "end": "544160"
  },
  {
    "text": "actual probability uh distribution so we've got the catat on there and we've got all of these various distributions",
    "start": "544160",
    "end": "549480"
  },
  {
    "text": "uh there it has the probability of all of the um 50, 20000 or so tokens so",
    "start": "549480",
    "end": "556640"
  },
  {
    "text": "imagine that this is one of these wheels at the fairground where you've got you know the the grand prize is this kind of small wedge and when you spin the wheel",
    "start": "556640",
    "end": "563040"
  },
  {
    "text": "assuming the wheel is not weighted it's going to land on one of those those things it's kind of like one of those probability things if this if this my",
    "start": "563040",
    "end": "569560"
  },
  {
    "text": "chart was was a wheel uh yeah I did this in um in Helsinki um the uh last week so",
    "start": "569560",
    "end": "575760"
  },
  {
    "text": "it's saying Helsinki is the capital of Finland with 42.760890",
    "start": "575760",
    "end": "580600"
  },
  {
    "text": "roll is Elvis well pre these are tokens remember not words and Presley is not a",
    "start": "599519",
    "end": "605200"
  },
  {
    "text": "token so pre also this is prefix by space as well uh because these are typically are and then we've got Elvis",
    "start": "605200",
    "end": "611600"
  },
  {
    "text": "Costello here with 2.76% because maybe it's it's going to be that and then if we say the king of rock and roll is",
    "start": "611600",
    "end": "618120"
  },
  {
    "text": "Elvis pre very high probability uh that the the next token is going to be Lee so",
    "start": "618120",
    "end": "623560"
  },
  {
    "text": "it's really dependent on the um kind of a probability likely distribution if",
    "start": "623560",
    "end": "628959"
  },
  {
    "text": "you're say my favorite animal is the you know there's all kinds of variation uh in there and it's really just predicting",
    "start": "628959",
    "end": "635120"
  },
  {
    "text": "what it thinks are the probabilities of these these next tokens are going to be",
    "start": "635120",
    "end": "640639"
  },
  {
    "text": "so let's drop back to the um to the slides so um we're sending in uh a",
    "start": "640639",
    "end": "647519"
  },
  {
    "start": "645000",
    "end": "1167000"
  },
  {
    "text": "question into this language model we can see the architecture there um however neural networks don't really do text so",
    "start": "647519",
    "end": "655000"
  },
  {
    "text": "we've got to think about um how we can get the text into a sequence of numbers and this is done but through the process",
    "start": "655000",
    "end": "660839"
  },
  {
    "text": "of tokenization so um this text is coming from the tokenization uh site uh that",
    "start": "660839",
    "end": "667959"
  },
  {
    "text": "we've got at openai I really like this bit of text it's been replaced by another bit of text on the website but",
    "start": "667959",
    "end": "673800"
  },
  {
    "text": "the thing I like about this is the three things here that are really important tokens are common sequences of",
    "start": "673800",
    "end": "679839"
  },
  {
    "text": "characters found in text so this tokenization process is separate to the language training model",
    "start": "679839",
    "end": "686760"
  },
  {
    "text": "they take the actual training data set and they do maths on it and they figure out right we're going to have a vocabulary of about 50,000 tokens what",
    "start": "686760",
    "end": "694519"
  },
  {
    "text": "is the most efficient way of representing this text in 50,000 uh combinations of characters now the",
    "start": "694519",
    "end": "701079"
  },
  {
    "text": "majority of the training data is in English so the tokenization is much more",
    "start": "701079",
    "end": "706279"
  },
  {
    "text": "uh efficient uh with English the models understand the statistical relationships between these tokens I would argue that",
    "start": "706279",
    "end": "713560"
  },
  {
    "text": "the language models do not understand text they just understand the statistical relationships between the",
    "start": "713560",
    "end": "718959"
  },
  {
    "text": "token tokens and they predict the next Tokens The Tokens could represent musical notes and in which case it will",
    "start": "718959",
    "end": "724519"
  },
  {
    "text": "predict the next musical note in a sequence all it's doing is maths with the probability distributions of tokens",
    "start": "724519",
    "end": "730519"
  },
  {
    "text": "we've seen in the code that we convert the model into tokens and we just send integers into the models we don't send text into the models so I would argue",
    "start": "730519",
    "end": "737279"
  },
  {
    "text": "that the models don't actually understand text but what they're really really good at is producing uh the next",
    "start": "737279",
    "end": "744000"
  },
  {
    "text": "token in a sequence of tokens I slightly disagree with that because they don't actually uh produce the next token they",
    "start": "744000",
    "end": "750399"
  },
  {
    "text": "predict the probability distribution of all of the 50,000 tokens and we may or may not want to choose the most probable",
    "start": "750399",
    "end": "757199"
  },
  {
    "text": "one depending on how creative uh we want the text to be uh a good example here of",
    "start": "757199",
    "end": "762639"
  },
  {
    "text": "why it doesn't understand text and why it can get confused uh give me a well-known solc song song lyric with the",
    "start": "762639",
    "end": "769079"
  },
  {
    "text": "word wind as in Winder watch and it came out with Blowing in the Wind by uh Bob",
    "start": "769079",
    "end": "775120"
  },
  {
    "text": "Dylan I said I wanted uh you know wind as in Winder watch not blowing in the wind sorry I misunderstood you and it",
    "start": "775120",
    "end": "781720"
  },
  {
    "text": "came up with against the W Wind by Bob Seager and this one's really really interesting listen to the winds blow",
    "start": "781720",
    "end": "787920"
  },
  {
    "text": "watch the sun rise it's got wind and watch in it but it's the wrong wind and it's the wrong watch and here we can see",
    "start": "787920",
    "end": "794720"
  },
  {
    "text": "the confusion in that um it's kind of treating these as tokens rather than words the token for wind or the token ID",
    "start": "794720",
    "end": "801440"
  },
  {
    "text": "for wind and for watch are the same it's the same even though they're different words it's the same sequence of letters",
    "start": "801440",
    "end": "806880"
  },
  {
    "text": "it's the same token and it confuses the language model so uh tokenization uh it's more",
    "start": "806880",
    "end": "813800"
  },
  {
    "text": "efficient in English uh if we uh take English that comes out to 45 tokens you",
    "start": "813800",
    "end": "819279"
  },
  {
    "text": "can see the word the is represented differently also as I mentioned all of these are prefix by space if we use",
    "start": "819279",
    "end": "825680"
  },
  {
    "text": "space as a token there will be about twice as many tokens so it makes sense to include um the space when it's a",
    "start": "825680",
    "end": "833279"
  },
  {
    "text": "token is the star of the word so most of these in fact all of these uh the actual English words are token",
    "start": "833279",
    "end": "839560"
  },
  {
    "text": "if we go to oh yeah we're all programmers in the audience what's the significance of 11 and 13 and comma and",
    "start": "839560",
    "end": "846800"
  },
  {
    "text": "full stop asy yeah the first 256 tokens are",
    "start": "846800",
    "end": "852880"
  },
  {
    "text": "asy characters so that basically um allows us to you know represent any any",
    "start": "852880",
    "end": "858639"
  },
  {
    "text": "text there and you'll see these asy characters popping up there if we take Swedish uh it ends up getting uh more",
    "start": "858639",
    "end": "864839"
  },
  {
    "text": "fragmented because there's not that much Swedish data in the um in the training data so here it comes out to 83 tokens",
    "start": "864839",
    "end": "872720"
  },
  {
    "text": "if we take Dutch we've got 71 tokens Finnish as in Finland last week that's",
    "start": "872720",
    "end": "878040"
  },
  {
    "text": "kind of from the European language is the most inefficient one that I've found then we've got",
    "start": "878040",
    "end": "883560"
  },
  {
    "text": "tuugo which comes out at 622 tokens even though there's only 247 uh characters",
    "start": "883560",
    "end": "890040"
  },
  {
    "text": "there now this is showing kind of the efficiency of the tokenization using",
    "start": "890040",
    "end": "895440"
  },
  {
    "text": "different languages and this is showing the um Lang language understanding of a",
    "start": "895440",
    "end": "900720"
  },
  {
    "text": "gp4 compared with gpt3 using the M mlu which is one of the um stats that jod uh",
    "start": "900720",
    "end": "907639"
  },
  {
    "text": "talked about one of the actual language uh tests for kind of analyzing how accurate these languages are we see that",
    "start": "907639",
    "end": "913720"
  },
  {
    "text": "GPT 4 is uh much better in English than it is with other languages however",
    "start": "913720",
    "end": "919240"
  },
  {
    "text": "gp4 is uh with other languages is usually much better uh than gp2 3.5 with",
    "start": "919240",
    "end": "925120"
  },
  {
    "text": "English so that's kind of the actual stats there it's kind of a rough correlation if you plot this one against",
    "start": "925120",
    "end": "931199"
  },
  {
    "text": "this one um really there's some kind of correlation between the um tokenization",
    "start": "931199",
    "end": "936560"
  },
  {
    "text": "efficiency and the language understanding but it's not not really a straight line you can see tuuga at the bottom there and also there tuuga is the",
    "start": "936560",
    "end": "943199"
  },
  {
    "text": "most inefficient of these languages listed uh for uh processing uh tokens so one of the things I've heard is that",
    "start": "943199",
    "end": "949680"
  },
  {
    "text": "English is the new programming language I'm not saying that just because I'm from England and I speak English you",
    "start": "949680",
    "end": "954839"
  },
  {
    "text": "should aim and Endeavor to write your prompts in English several reasons for that GPT models are most accurate with",
    "start": "954839",
    "end": "961680"
  },
  {
    "text": "English because they've had the most amount of training data in uh English and they uh you can see on the accuracy",
    "start": "961680",
    "end": "968079"
  },
  {
    "text": "charts they're more accurate billing is based on the number of tokens so if you're using um you know Swedish or",
    "start": "968079",
    "end": "974440"
  },
  {
    "text": "Finnish or tuugo it's going to be more expensive to send the same amount of kind of meaning uh through the language",
    "start": "974440",
    "end": "980759"
  },
  {
    "text": "models throughput is based on number of tokens per second so it's going to be faster uh processing English and also",
    "start": "980759",
    "end": "987199"
  },
  {
    "text": "you get these quotas uh based based on how many tokens you can use and also the models have a maximum uh number of input",
    "start": "987199",
    "end": "993759"
  },
  {
    "text": "tokens we're developers if we can make something run say 20 or 30% faster and",
    "start": "993759",
    "end": "999600"
  },
  {
    "text": "20 or 30% more accurate and it's going to reduce the cost by 20 or 30% we'll do that so even if your model is a rag",
    "start": "999600",
    "end": "1007639"
  },
  {
    "text": "Solution that's dealing with um you know text in your native language it's generating outputs in your native",
    "start": "1007639",
    "end": "1013759"
  },
  {
    "text": "language the questions users are answering questions in your native language writing the prompt in English still works uh you can you can still do",
    "start": "1013759",
    "end": "1020480"
  },
  {
    "text": "that and I would recommend doing that just for a test I asked it how to make cheese inugo and uh it came out with an answer",
    "start": "1020480",
    "end": "1029160"
  },
  {
    "text": "which was this thing here recommending using uh cheese powder as the main uh ingredient and then it gave up it",
    "start": "1029160",
    "end": "1036160"
  },
  {
    "text": "switched back to English offered to generate a poem or a story and uh then",
    "start": "1036160",
    "end": "1041319"
  },
  {
    "text": "it got really weird uh and generated an image of the Frozen characters in an Autumn Bound in charted land that you",
    "start": "1041319",
    "end": "1048919"
  },
  {
    "text": "requested um I asked it how to make cheese but there is obviously some confusion and understanding now that's",
    "start": "1048919",
    "end": "1055000"
  },
  {
    "text": "an extreme example um you know I've done loads of testing and that was just the one that came out as being really really",
    "start": "1055000",
    "end": "1060200"
  },
  {
    "text": "crazy but you can see that there is kind of a correlation uh in between the language understanding and the actual",
    "start": "1060200",
    "end": "1065760"
  },
  {
    "text": "quality of the answers that you're going to be getting however neural networks don't do integers they do floating Point",
    "start": "1065760",
    "end": "1072679"
  },
  {
    "text": "numbers so what we've got to do is another stage of conversion and get these tokens into floating point numbers",
    "start": "1072679",
    "end": "1079280"
  },
  {
    "text": "so we can actually start to do uh maths with them and this is known as embedding or vectorization and again jod was",
    "start": "1079280",
    "end": "1086960"
  },
  {
    "text": "mentioning about this how we can vectorize text or do text embedding to get the sentiment of text so we take the",
    "start": "1086960",
    "end": "1093480"
  },
  {
    "text": "word cheese and in gpt2 I think there's 768 uh floating Point numbers that",
    "start": "1093480",
    "end": "1099080"
  },
  {
    "text": "basically have uh this uh embedding or vectorization of the word cheese now",
    "start": "1099080",
    "end": "1104960"
  },
  {
    "text": "humans can't think in 768 Dimensions we can you know just about handle three",
    "start": "1104960",
    "end": "1110320"
  },
  {
    "text": "dimensions and we're developers so we've built web pages and we understand about you know red green and blue uh color how",
    "start": "1110320",
    "end": "1117240"
  },
  {
    "text": "we've got this kind of vector whereare Azor is 012 7255 and data scientists",
    "start": "1117240",
    "end": "1123400"
  },
  {
    "text": "always work floating Point numbers between Z and one so that's zero 0.5 uh 1.0 and what we can do then is take",
    "start": "1123400",
    "end": "1130919"
  },
  {
    "text": "loads of colors and we can stick them into a 3D space and we can see that similar colors are basically uh close to",
    "start": "1130919",
    "end": "1136960"
  },
  {
    "text": "each other in that 3D uh 3D space and measuring the distance uh two ways of",
    "start": "1136960",
    "end": "1142000"
  },
  {
    "text": "doing this we can either use the uh is it audian distance or something the actual distance that we measure between",
    "start": "1142000",
    "end": "1148000"
  },
  {
    "text": "those two points or we can use cosine similarity and cosine similarity will basically say if they are kind of along",
    "start": "1148000",
    "end": "1154919"
  },
  {
    "text": "the same angle so dark as or would be somewhere around about here somewhere and because of the actual angle the",
    "start": "1154919",
    "end": "1161200"
  },
  {
    "text": "ratio of those colors you'd say all of the colors along this line are similar and they have the same cosine similarity",
    "start": "1161200",
    "end": "1167000"
  },
  {
    "start": "1167000",
    "end": "1267000"
  },
  {
    "text": "there um we could can also do with this with with words so word Toc um basically",
    "start": "1167000",
    "end": "1173520"
  },
  {
    "text": "has this database here and this is the embeddings projector and there's 10,000 words in here each one of these is",
    "start": "1173520",
    "end": "1180200"
  },
  {
    "text": "represented with 200 Dimensions rather than 768 but what we can do is we can do",
    "start": "1180200",
    "end": "1185440"
  },
  {
    "text": "this nearest neighbor search and we can take those 200 dimensions and we can squash them into three dimensions so the",
    "start": "1185440",
    "end": "1192000"
  },
  {
    "text": "human brain can understand them and then we should see uh that similar words are together in similar uh",
    "start": "1192000",
    "end": "1199919"
  },
  {
    "text": "locations I need to get a faster laptop uh but eventually it will get there and",
    "start": "1199919",
    "end": "1205280"
  },
  {
    "text": "we should be able to see this so this word Toc database as as jod mentioned it was part of the actual you know um",
    "start": "1205280",
    "end": "1211320"
  },
  {
    "text": "timeline of research that people were doing when they were looking at uh natural language processing okay so now",
    "start": "1211320",
    "end": "1217440"
  },
  {
    "text": "we've got this kind of a cluster of words and we should see that words that are similar are going to be",
    "start": "1217440",
    "end": "1223960"
  },
  {
    "text": "in the same uh in the same place so let's go over here",
    "start": "1223960",
    "end": "1229039"
  },
  {
    "text": "we got a few oh these seem to be relating to um classical music and we've got some over here",
    "start": "1229039",
    "end": "1236280"
  },
  {
    "text": "diseases AIDS that's kind of a medical stuff over here oh we got like Greek",
    "start": "1236280",
    "end": "1241559"
  },
  {
    "text": "letters over here as well 5070 tens Millions hundreds and all of these These",
    "start": "1241559",
    "end": "1246760"
  },
  {
    "text": "are numbers so this kind of a cloud and using this this kind of vectorization is a way of um finding out sentiment and",
    "start": "1246760",
    "end": "1253320"
  },
  {
    "text": "and what words are similar uh to others because um the language models want to understand that good good and great and",
    "start": "1253320",
    "end": "1259320"
  },
  {
    "text": "fantastic are are words that um kind of have some kind of a Sim similar sentiment or similar uh",
    "start": "1259320",
    "end": "1267440"
  },
  {
    "start": "1267000",
    "end": "1438000"
  },
  {
    "text": "meaning so um we'll start to look at the actual Transformer architecture and see kind of what goes on in these language",
    "start": "1267440",
    "end": "1273919"
  },
  {
    "text": "models and how the uh the attention uh model actually works and how these calculations are going to actually um",
    "start": "1273919",
    "end": "1280679"
  },
  {
    "text": "get some some answers now um this is from the white paper attention is all you need um the white paper is really",
    "start": "1280679",
    "end": "1287480"
  },
  {
    "text": "written about doing TR translations um and Jody mentioned this this morning that in instead of just doing",
    "start": "1287480",
    "end": "1293600"
  },
  {
    "text": "translations what the um we're really using GPT for this Transformer architecture is for sequence uh",
    "start": "1293600",
    "end": "1299799"
  },
  {
    "text": "prediction so what we can do is we can just basically uh you know forget about this bit and just focus on the uh the",
    "start": "1299799",
    "end": "1305760"
  },
  {
    "text": "decoder and look at how we can take uh our data we can send it in uh and we can",
    "start": "1305760",
    "end": "1311080"
  },
  {
    "text": "get the embedding and the positional encoding because remember position is important as well as uh the the actual",
    "start": "1311080",
    "end": "1317120"
  },
  {
    "text": "words and then we can we can kind of follow through how that goes through the the network so focusing on that bit we",
    "start": "1317120",
    "end": "1323400"
  },
  {
    "text": "see how the tokenization works the catat on thee is going to be uh lookups in the",
    "start": "1323400",
    "end": "1328640"
  },
  {
    "text": "tokenization vocabulary to get the integer uh numbers that are going to be those token IDs what we can then do is",
    "start": "1328640",
    "end": "1335720"
  },
  {
    "text": "perform the embedding now during the model training process uh we create something that's known as an embeddings",
    "start": "1335720",
    "end": "1342279"
  },
  {
    "text": "table and this will be present within uh the model and the dimensions of that in pedding table is 700 68 because each uh",
    "start": "1342279",
    "end": "1350880"
  },
  {
    "text": "token is going to have 768 floating Point numbers that describe it in that 768 dimensional space and then we've got",
    "start": "1350880",
    "end": "1358000"
  },
  {
    "text": "the size of the token vocabulary which is 50256 and what we do is we just",
    "start": "1358000",
    "end": "1364720"
  },
  {
    "text": "basically do a lookup so we look up number 464 3797 3332 319 and 262 and",
    "start": "1364720",
    "end": "1372120"
  },
  {
    "text": "what that's going to give us is uh the actual embedding vectors for those five",
    "start": "1372120",
    "end": "1378880"
  },
  {
    "text": "tokens and that's what's going to be fed in the model that's the actual embedding vectors which is going to be um",
    "start": "1378880",
    "end": "1384600"
  },
  {
    "text": "basically the number of tokens by 768 so that's going to give us our",
    "start": "1384600",
    "end": "1390120"
  },
  {
    "text": "embedding vectors the next thing uh we're going to get is the positional vectors and there's also a positional",
    "start": "1390120",
    "end": "1396480"
  },
  {
    "text": "lookup table and what we feed into the positional lookup table is the actual position of the token so this will",
    "start": "1396480",
    "end": "1402000"
  },
  {
    "text": "basically be 0 1 2 3 4 and we perform those lookups now the positional vectors",
    "start": "1402000",
    "end": "1407919"
  },
  {
    "text": "are going to give some kind of a meaning of the position they're still going to be you know 768 Dimensions but they're",
    "start": "1407919",
    "end": "1414480"
  },
  {
    "text": "created um not from the actual training data but using signs and cosiness uh and",
    "start": "1414480",
    "end": "1419960"
  },
  {
    "text": "natural values so it gives some kind of indication that this position uh is going to be uh relevant to this",
    "start": "1419960",
    "end": "1425760"
  },
  {
    "text": "particular token so we have the the positioning information and then what we do is we add these two together and",
    "start": "1425760",
    "end": "1432679"
  },
  {
    "text": "that's going to give us the encoder input values uh that we're going to be uh feeding into the actual model",
    "start": "1432679",
    "end": "1439080"
  },
  {
    "start": "1438000",
    "end": "1620000"
  },
  {
    "text": "so as I mentioned the the cool thing about having the source code available is I can stick break points on so if I",
    "start": "1439080",
    "end": "1445480"
  },
  {
    "text": "go to this next token prediction I've got this set up as a startup file I'm just going to change the um input text",
    "start": "1445480",
    "end": "1453960"
  },
  {
    "text": "to this and do control F5 no I'm not going to do control F5 I",
    "start": "1453960",
    "end": "1460559"
  },
  {
    "text": "want break points so I'm going to do F5",
    "start": "1460559",
    "end": "1466840"
  },
  {
    "text": "what this is going to do is to load the model in memory um if I press enter instead of having to type in the text it",
    "start": "1470520",
    "end": "1476039"
  },
  {
    "text": "will say the catat on there but we're going to be hitting break points and if I go into the watch window I'm using",
    "start": "1476039",
    "end": "1482320"
  },
  {
    "text": "python here so this self will tell me you know what class I'm in so we're in the gp2 2 model we're in the Constructor",
    "start": "1482320",
    "end": "1489520"
  },
  {
    "text": "of the gp22 model and then I can run to the next uh break point it's asking me to enter some text I'll just press enter",
    "start": "1489520",
    "end": "1496679"
  },
  {
    "text": "and now we're in the gp2 L head model and we've got our input IDs so these are",
    "start": "1496679",
    "end": "1502080"
  },
  {
    "text": "the actual token IDs that are being passed into the model uh if we look in here we can see that the tokenizer is",
    "start": "1502080",
    "end": "1509320"
  },
  {
    "text": "actually separate from the model itself so that we're doing tokenizer in code we're passing in the text we get the input IDs then they go into the model",
    "start": "1509320",
    "end": "1517159"
  },
  {
    "text": "and we get to um where is it the breakpoint that I had when we're here",
    "start": "1517159",
    "end": "1523640"
  },
  {
    "text": "with within the model we've got those tokens so if I do an F5 you can see now that we've got the",
    "start": "1523640",
    "end": "1529520"
  },
  {
    "text": "position ID 01 2 3 4 uh for the actual tokens so what we're doing is we're",
    "start": "1529520",
    "end": "1535520"
  },
  {
    "text": "actually getting uh the embeddings so when I said there was an embeddings table in uh the actual model it's this",
    "start": "1535520",
    "end": "1542840"
  },
  {
    "text": "wte uh which is the embeddings table so what we do is we feed in uh these IDs",
    "start": "1542840",
    "end": "1548279"
  },
  {
    "text": "and that's going to give us if I just step over these two lines of code the input embeddings here and if you look at",
    "start": "1548279",
    "end": "1555440"
  },
  {
    "text": "the size of the input embeddings you can see that this is 1x 5x",
    "start": "1555440",
    "end": "1560520"
  },
  {
    "text": "768 now pretty much all neural networks we can train them um using batteries uh",
    "start": "1560520",
    "end": "1566720"
  },
  {
    "text": "in the actual training process so the one basically means that we're just sending in one actual sequence when",
    "start": "1566720",
    "end": "1572360"
  },
  {
    "text": "we're doing inference when we're actually doing this we're usually just sending in one stream of text but the model does support sending in more if we",
    "start": "1572360",
    "end": "1578080"
  },
  {
    "text": "want to do that so that's always pretty much always going to be one then there's five tokens 768 uh dimensions and that's",
    "start": "1578080",
    "end": "1584640"
  },
  {
    "text": "our actual embeddings we do the same thing with the positional embeddings so if I just step over that you can see now",
    "start": "1584640",
    "end": "1591080"
  },
  {
    "text": "that we've got our position embeddings that again is 1x 5x 768 and then we're going to calculate the encoder input",
    "start": "1591080",
    "end": "1597440"
  },
  {
    "text": "values which is hidden states which is just adding the input embeddings to the position embeddings so I can uh step",
    "start": "1597440",
    "end": "1604360"
  },
  {
    "text": "over uh step over that code there and what we've done there is just this",
    "start": "1604360",
    "end": "1609679"
  },
  {
    "text": "calculation uh that I had on the uh slide deck there so the next thing we're going to",
    "start": "1609679",
    "end": "1616240"
  },
  {
    "text": "do is to look at what happens after that how it does the actual attention uh calculation and this is the actual",
    "start": "1616240",
    "end": "1622440"
  },
  {
    "start": "1620000",
    "end": "1868000"
  },
  {
    "text": "formula for that it has the uh the query uh the key and the uh value and uh these",
    "start": "1622440",
    "end": "1628960"
  },
  {
    "text": "are going to be kind of calculated or extracted by using floating points in uh",
    "start": "1628960",
    "end": "1634880"
  },
  {
    "text": "layer in the network that's basically trained to be able to do that and we're then performing this softmax uh",
    "start": "1634880",
    "end": "1640520"
  },
  {
    "text": "operation uh on this uh Matrix algebra so um there's a lot of Matrix",
    "start": "1640520",
    "end": "1645799"
  },
  {
    "text": "calculations the good thing uh about the gaming industry and loads of kids playing games or adults playing games as",
    "start": "1645799",
    "end": "1652120"
  },
  {
    "text": "well and spending ludicrous money on big gaming gpus is that the same gpus are",
    "start": "1652120",
    "end": "1657919"
  },
  {
    "text": "really uh the same calculations in 3D games are pretty much similar to what we're doing when we're training these",
    "start": "1657919",
    "end": "1663679"
  },
  {
    "text": "models using Matrix uh algebra and this is uh basically what's Driven uh you",
    "start": "1663679",
    "end": "1669159"
  },
  {
    "text": "know the rise in the these models Nvidia is now I think is it the most valuable company in the world still um there's",
    "start": "1669159",
    "end": "1675360"
  },
  {
    "text": "news story on that that it'd overtaken Microsoft because they're basically making these these gpus so what we do now is we take the encoder input values",
    "start": "1675360",
    "end": "1681880"
  },
  {
    "text": "that's 1X 5x 768 we use a convolution layer which is going to generate",
    "start": "1681880",
    "end": "1687279"
  },
  {
    "text": "different representations of the input value vectors for the attention mechanism so this is uh going to come",
    "start": "1687279",
    "end": "1693600"
  },
  {
    "text": "out with this 1x5 by 3 uh 2304 and we're then going to split that into these kind",
    "start": "1693600",
    "end": "1700039"
  },
  {
    "text": "of a three sections and this is known as the query the key and the uh value I'll explain a bit more about those those",
    "start": "1700039",
    "end": "1706559"
  },
  {
    "text": "later on now it's a multi-headed attention and in gpt2 uh we have 12",
    "start": "1706559",
    "end": "1712760"
  },
  {
    "text": "attention heads in each layer and we have 12 layers so that 1X 5x 768 is",
    "start": "1712760",
    "end": "1719279"
  },
  {
    "text": "going to be split based on the number of heads and that is going to go to 1 by 12 by 5 by 64 and what it's doing is it's",
    "start": "1719279",
    "end": "1727159"
  },
  {
    "text": "splitting the dimension of 768 between between 12 and that's giving kind of 12",
    "start": "1727159",
    "end": "1732679"
  },
  {
    "text": "instances of f64 there so there's one batch uh 12 attention heads five tokens",
    "start": "1732679",
    "end": "1738720"
  },
  {
    "text": "and 64 uh floating Point numbers in each uh attention for the query the key and",
    "start": "1738720",
    "end": "1744440"
  },
  {
    "text": "the actual value so the query the key and the value are used to get the attention which is",
    "start": "1744440",
    "end": "1750640"
  },
  {
    "text": "how the words relate to each other um so I I don't normally speak at these these",
    "start": "1750640",
    "end": "1755960"
  },
  {
    "text": "types of conferences I'm normally at Azo conferences andn night conferences so I can be the query I came here and then",
    "start": "1755960",
    "end": "1762120"
  },
  {
    "text": "each of you in the room is going to be the key and then the value is kind of what connection uh the key has to the",
    "start": "1762120",
    "end": "1769880"
  },
  {
    "text": "query so what we're doing is we're looking all of the words in the uh input tokens or all all of the actual tokens",
    "start": "1769880",
    "end": "1775640"
  },
  {
    "text": "in the sequence and saying well how does this word relate to that particular word here and Performing calculations to try",
    "start": "1775640",
    "end": "1781519"
  },
  {
    "text": "and figure out the actual sentiment remember these um vectors uh that we've",
    "start": "1781519",
    "end": "1786679"
  },
  {
    "text": "got for the words they kind of describe the sentiments of the words similar words are you know closer together in",
    "start": "1786679",
    "end": "1792679"
  },
  {
    "text": "the actual Vector space so what it's going to do is to be able to figure out",
    "start": "1792679",
    "end": "1797880"
  },
  {
    "text": "um if it's the cat sat on the floor it had just been cleaned that it is going to have a connection to the cat so what",
    "start": "1797880",
    "end": "1805279"
  },
  {
    "text": "we're doing is within this uh Matrix operation we're taking the query uh we're transposing the key multiplying it",
    "start": "1805279",
    "end": "1810960"
  },
  {
    "text": "by that dividing it by uh the root of 64 which is the number uh that we've got that here that's just something that",
    "start": "1810960",
    "end": "1816840"
  },
  {
    "text": "they've figured out um works better uh for the aular maths there and then we're performing this softmax operation I'll",
    "start": "1816840",
    "end": "1822960"
  },
  {
    "text": "explain a bit more about softmax later on but if you've played around with P torch or tensor flow you probably know",
    "start": "1822960",
    "end": "1828640"
  },
  {
    "text": "what softmax does uh in there it gives us a probability distribution from a a sequence of floating Point numbers and",
    "start": "1828640",
    "end": "1834840"
  },
  {
    "text": "it's important when we're doing the the calculation of the temperature so what this is going to give us is the actual",
    "start": "1834840",
    "end": "1841159"
  },
  {
    "text": "attention when we've done all of this maths now the vectorization or the",
    "start": "1841159",
    "end": "1846600"
  },
  {
    "text": "embedding gives us the meaning um the positional vectors uh and positional lookup gives us the position but what",
    "start": "1846600",
    "end": "1853760"
  },
  {
    "text": "the um attention mechanism doing is calculating uh how words relate to other",
    "start": "1853760",
    "end": "1858960"
  },
  {
    "text": "words and what the connections are and longterm dependencies uh between all of the other words and it's that that's",
    "start": "1858960",
    "end": "1865360"
  },
  {
    "text": "really doing the magic uh that's allowing these models to make uh predictions so if I drop back to the um",
    "start": "1865360",
    "end": "1872279"
  },
  {
    "start": "1868000",
    "end": "1972000"
  },
  {
    "text": "to the code we should see that Math's taking place in here so what we've got",
    "start": "1872279",
    "end": "1878240"
  },
  {
    "text": "is we've got the hidden States now the hidden States uh was that um I think we got the shape on",
    "start": "1878240",
    "end": "1884360"
  },
  {
    "text": "that that was at 1X 5x 768 that was a result from adding the position vectors",
    "start": "1884360",
    "end": "1889840"
  },
  {
    "text": "and the embedding vectors for the tokens and then what we're going to do is the maths um that I had on the actual screen",
    "start": "1889840",
    "end": "1896360"
  },
  {
    "text": "uh where we're taking um the attention we're putting in the attention and then we're splitting that um by the actual um",
    "start": "1896360",
    "end": "1905919"
  },
  {
    "text": "dimensions and what that's going to give us if I step over that is going to give us the keys the queries and the values",
    "start": "1905919",
    "end": "1912200"
  },
  {
    "text": "um but here the keys the queries and the values are for all 12 of these so if we look at the",
    "start": "1912200",
    "end": "1920000"
  },
  {
    "text": "um the key shape we can see that's 1X 5x 768 we then split it based on the number",
    "start": "1920000",
    "end": "1925720"
  },
  {
    "text": "of heads and there's basically 12 heads in this particular model so if I step over that then uh we've actually got the",
    "start": "1925720",
    "end": "1933399"
  },
  {
    "text": "key the query and the value being those dimensions of 1 by 12 by by S 768 and",
    "start": "1933399",
    "end": "1939240"
  },
  {
    "text": "that's what's going to uh work as as we run through that particular Network now because there's 12 layers in the network",
    "start": "1939240",
    "end": "1944519"
  },
  {
    "text": "we're basically going to do this 12 times before we actually get uh to the uh",
    "start": "1944519",
    "end": "1949880"
  },
  {
    "text": "results and the main difference between gpt2 and three and four is that they've just got more layers they've got more",
    "start": "1949880",
    "end": "1956240"
  },
  {
    "text": "parameters um they've got more attention heads in there they're just bigger and and larger and eventually uh we're going",
    "start": "1956240",
    "end": "1962399"
  },
  {
    "text": "to get the actual output uh that we can start processing and and making uh sense with so if I drop back to the",
    "start": "1962399",
    "end": "1970760"
  },
  {
    "start": "1972000",
    "end": "2029000"
  },
  {
    "text": "slides oh this yeah this is just explaining uh about what happens because here the word it is ambiguous we say it",
    "start": "1972080",
    "end": "1978440"
  },
  {
    "text": "had just been cleaned well is it the cat that had just been cleaned or the floor that had just been cleaned and what the",
    "start": "1978440",
    "end": "1984279"
  },
  {
    "text": "attention mechanism is doing is it's taking the word it and we've got you know the differences uh between these",
    "start": "1984279",
    "end": "1989760"
  },
  {
    "text": "these two words and if we focus on it had just been cleaned then it has more of a connection with floor and more of a",
    "start": "1989760",
    "end": "1996080"
  },
  {
    "text": "con connection uh with cleaned because floor has more of a connection with cleaned and you just basically shifts um",
    "start": "1996080",
    "end": "2002840"
  },
  {
    "text": "the vectors in that um High dimensional space so that it is going to refer uh to",
    "start": "2002840",
    "end": "2008559"
  },
  {
    "text": "the uh to the floor and it understands how those actual um words uh relate to each other so the model will basically",
    "start": "2008559",
    "end": "2015440"
  },
  {
    "text": "look like this uh we've got basically 12 heads and um we've got a couple of other",
    "start": "2015440",
    "end": "2020639"
  },
  {
    "text": "layers to do the um you know normalization and the feed forward and so on and then this is just going to",
    "start": "2020639",
    "end": "2025679"
  },
  {
    "text": "repeat 12 times uh in in the gp2 22 Model to give us those actual predictions so that's it uh we can",
    "start": "2025679",
    "end": "2032200"
  },
  {
    "start": "2029000",
    "end": "2646000"
  },
  {
    "text": "predict the next token uh fantastic so this means that we can actually start generating text so let's just see what",
    "start": "2032200",
    "end": "2038320"
  },
  {
    "text": "that does when I go back to the um next token prediction and I'm going to take",
    "start": "2038320",
    "end": "2044080"
  },
  {
    "text": "this and just do the king of rock and roll is Elvis something that's easy to predict and I'll do control F5 uh we",
    "start": "2044080",
    "end": "2051079"
  },
  {
    "text": "don't want to be hitting all of these at these break",
    "start": "2051079",
    "end": "2054799"
  },
  {
    "text": "points so I'll just press enter the king of rock and roll is Elvis and we've got PR coming up there with uh 64.9 eight so",
    "start": "2057800",
    "end": "2065079"
  },
  {
    "text": "let's bang that on then Lee 99 9 76% then we've got full stop yeah he's a",
    "start": "2065079",
    "end": "2075679"
  },
  {
    "text": "rockar and he's a rockstar he's a",
    "start": "2075679",
    "end": "2082040"
  },
  {
    "text": "rockstar so if you go to a party and you just say the most predictable thing every time all of the time you're going",
    "start": "2083839",
    "end": "2090919"
  },
  {
    "text": "to be really boring nobody's going to talk to you and this is what we're seeing here that there's more to text",
    "start": "2090919",
    "end": "2096480"
  },
  {
    "text": "generation uh than just predicting uh the next bit of text there's all of these other things uh here that we may",
    "start": "2096480",
    "end": "2102839"
  },
  {
    "text": "want to predict so we can take he and then was we can drop on three and then a",
    "start": "2102839",
    "end": "2109920"
  },
  {
    "text": "couple of other tokens and then I'm just going to drop some random things",
    "start": "2109920",
    "end": "2116400"
  },
  {
    "text": "in and as we put a bit of stochasticity or Randomness or variation uh into these",
    "start": "2116400",
    "end": "2122960"
  },
  {
    "text": "prompts then it's actually going to start to generate stuff that is more interesting then he's a rockstar and",
    "start": "2122960",
    "end": "2129280"
  },
  {
    "text": "he's a rockstar and he's a rockstar and he's a rockstar so there's a couple of parameters that are really important",
    "start": "2129280",
    "end": "2135280"
  },
  {
    "text": "when um we're working with these models one of them people kind of understand uh",
    "start": "2135280",
    "end": "2140520"
  },
  {
    "text": "sort of when to use it uh and that's temperature so temperature normally we",
    "start": "2140520",
    "end": "2145839"
  },
  {
    "text": "say temperature varies between zero and one and this affects the randomness or",
    "start": "2145839",
    "end": "2151000"
  },
  {
    "text": "the creativity of the network and using values like zero is going to be very predictable and one is going to be very",
    "start": "2151000",
    "end": "2156880"
  },
  {
    "text": "unpredictable how however it it goes up to two normally when we're working in a portal you're working in the Azor open",
    "start": "2156880",
    "end": "2163480"
  },
  {
    "text": "ey Studio the slider only goes up to one because things get very predictable if you go above one when you're using GP",
    "start": "2163480",
    "end": "2169440"
  },
  {
    "text": "gpt3 if you can go if you go up to two it can just start generating garbage because it kind of picks one of these uh",
    "start": "2169440",
    "end": "2175160"
  },
  {
    "text": "these these random uh ones so temperature does have an effect on what",
    "start": "2175160",
    "end": "2180280"
  },
  {
    "text": "is going to be uh generated and what I didn't mention when I was running through my uh little demo here is we've",
    "start": "2180280",
    "end": "2188000"
  },
  {
    "text": "got um the cat set the cat set on that yes so these are my text uh which one are we using we're using zero the cat",
    "start": "2188000",
    "end": "2194680"
  },
  {
    "text": "set on there and it's coming out with this actual uh probability distribution here so the cat set on the floor at",
    "start": "2194680",
    "end": "2202040"
  },
  {
    "text": "7.64% now we've got the formula uh for temperature here because what I'm doing",
    "start": "2202040",
    "end": "2208160"
  },
  {
    "text": "is I'm getting the next token uh logits and this is kind of a series of floating Point numbers um that is not the",
    "start": "2208160",
    "end": "2214960"
  },
  {
    "text": "probability distribution they're just like uh floating Point values and they're going to be higher for more",
    "start": "2214960",
    "end": "2220839"
  },
  {
    "text": "probable tokens and lower uh for Less probable tokens and what we do is we divide this by the temperature and then",
    "start": "2220839",
    "end": "2228280"
  },
  {
    "text": "we perform this softmax operation and what softmax will do is basically take",
    "start": "2228280",
    "end": "2233560"
  },
  {
    "text": "all of those numbers and do some maths on them uh so that they basically add up to one I think I've got the um formula",
    "start": "2233560",
    "end": "2241280"
  },
  {
    "text": "for softmax on my slide sumwhere so what it's doing is this there's some logarithms involved so it's non",
    "start": "2241280",
    "end": "2248280"
  },
  {
    "text": "but it's just really taking a series of floating Point numbers and converting them into a an actual probability",
    "start": "2248280",
    "end": "2253920"
  },
  {
    "text": "distribution so if temperature is one then it just divides them by one and we just take the regular uh values however",
    "start": "2253920",
    "end": "2260920"
  },
  {
    "text": "what happens if temperature is zero bad things happen uh because this is a divide by zero operations so in this",
    "start": "2260920",
    "end": "2268760"
  },
  {
    "text": "implementation here I can just drop temperature as being something very low and you can see here that floor now has",
    "start": "2268760",
    "end": "2275920"
  },
  {
    "text": "a 100% probability when we're using a low uh temperature if I drop the",
    "start": "2275920",
    "end": "2281440"
  },
  {
    "text": "temperature to 0.5 you can see that floor has 24%",
    "start": "2281440",
    "end": "2286960"
  },
  {
    "text": "probability so what it's doing is it's kind of Shifting the probability uh distribution if I go above one and drop",
    "start": "2286960",
    "end": "2293760"
  },
  {
    "text": "a temperature of two you can see uh that all of these top 10 have quite a low probability and it's making it more",
    "start": "2293760",
    "end": "2299800"
  },
  {
    "text": "probable the other tokens are going to be the ones that are chosen so to make this a bit more visual oh yeah I'll",
    "start": "2299800",
    "end": "2305839"
  },
  {
    "text": "cover top PE first and then I'll show you the um the actual animation so that's kind of what temperature does top",
    "start": "2305839",
    "end": "2311640"
  },
  {
    "text": "PE basically puts a hard cut off on the tokens being chosen so if I choose a top",
    "start": "2311640",
    "end": "2317480"
  },
  {
    "text": "PE of being 0.1 it can only choose floor or bed and it's most likely to choose floor there",
    "start": "2317480",
    "end": "2324160"
  },
  {
    "text": "is a zero there's no way that it can choose couch or ground or anything else and the top PE is basically varying how",
    "start": "2324160",
    "end": "2331280"
  },
  {
    "text": "much of that actual slice uh we can we can make the actual the actual Choice from I think when we developing these",
    "start": "2331280",
    "end": "2338000"
  },
  {
    "text": "applications it's good to understand what temperature does and what top PE does and how you can kind of combine",
    "start": "2338000",
    "end": "2343800"
  },
  {
    "text": "those various options because I've seen people standing on stage talking about large language models and they don't really kind of have an understanding of",
    "start": "2343800",
    "end": "2350440"
  },
  {
    "text": "what top BP does and how it works so I basically created these animations",
    "start": "2350440",
    "end": "2355520"
  },
  {
    "text": "because I was really curious about well how does the probability distribution uh affect so we're sending in the catat on",
    "start": "2355520",
    "end": "2362280"
  },
  {
    "text": "the and we've got the temperature here starting at one and the top p and I'm using the same values for temperature and top p",
    "start": "2362280",
    "end": "2367880"
  },
  {
    "text": "and we can see how the actual probability uh distributions are changing uh there and it starts to get",
    "start": "2367880",
    "end": "2373560"
  },
  {
    "text": "more interesting as we get to the um lower uh values here because what we see",
    "start": "2373560",
    "end": "2379800"
  },
  {
    "text": "is that with top P um these things start actually disappearing out of the actual probability distribution so all of the",
    "start": "2379800",
    "end": "2386359"
  },
  {
    "text": "earlier ones have gone now they can never be chosen here they can be CH chosen but there's quite a low",
    "start": "2386359",
    "end": "2392800"
  },
  {
    "text": "probability of them being uh being chosen now if we go back to the",
    "start": "2392800",
    "end": "2397839"
  },
  {
    "text": "temperature of one and top PE of one we don't put any restriction on so there is a tiny chance uh that it's going to land",
    "start": "2397839",
    "end": "2404960"
  },
  {
    "text": "on here and it's going to say the cat sat on the trees or the cat sat on",
    "start": "2404960",
    "end": "2410040"
  },
  {
    "text": "something that was completely in inappropriate so we can see in the slide I was mentioning that in the uh",
    "start": "2410040",
    "end": "2418359"
  },
  {
    "text": "portal they're specifying a top p as 095 and that's just basically chopping out",
    "start": "2418359",
    "end": "2424839"
  },
  {
    "text": "uh the 5% of the tokens that we probably don't want to want to choose and then we're actually varying uh varying the",
    "start": "2424839",
    "end": "2431200"
  },
  {
    "text": "actual temperature there in the guidelines it says vary the temperature or the top PE uh but don't do both um so",
    "start": "2431200",
    "end": "2437760"
  },
  {
    "text": "what this is doing is um we can vary the temperature but it's never going to produce anything really unpredictable even with the temperature of of 1.0",
    "start": "2437760",
    "end": "2444920"
  },
  {
    "text": "because top PE is going to be uh filtering uh filtering off that and then as we uh drop back to the animation uh",
    "start": "2444920",
    "end": "2451359"
  },
  {
    "text": "as I go towards um you know this end of the probability distribution you can see that they disappear and when we get",
    "start": "2451359",
    "end": "2457760"
  },
  {
    "text": "round about here you can see we can only have floor and bed uh with a top PE of 0.135 but with the temperature uh we've",
    "start": "2457760",
    "end": "2464960"
  },
  {
    "text": "got floor uh bed couch ground and all of the others there all of them do have a probability distribution but most of",
    "start": "2464960",
    "end": "2471599"
  },
  {
    "text": "them are really really uh small so um to see how temperature has an effect on uh",
    "start": "2471599",
    "end": "2478640"
  },
  {
    "text": "things what I can do is drop back to the um I've got a demo called temperature",
    "start": "2478640",
    "end": "2483920"
  },
  {
    "text": "Theory I think it's this one here set this one as a startup file and do a control",
    "start": "2483920",
    "end": "2490838"
  },
  {
    "text": "F5 so what this is going to do is to cycle through different temperatures and uh create different outputs I think I",
    "start": "2490960",
    "end": "2497359"
  },
  {
    "text": "actually want to drop in a I think I got rid of the",
    "start": "2497359",
    "end": "2502400"
  },
  {
    "text": "um the input on that so let's just set this one as a startup",
    "start": "2502400",
    "end": "2508400"
  },
  {
    "text": "file no it wasn't that one it was the temperature",
    "start": "2508920",
    "end": "2513960"
  },
  {
    "text": "Theory yeah I'll just come comment that back in I always do this demo at the end of",
    "start": "2515720",
    "end": "2523000"
  },
  {
    "text": "the session because I've been doing this and uh the audience falls about laughing and is in stitches and I have no idea",
    "start": "2523000",
    "end": "2528920"
  },
  {
    "text": "why because it's random uh in nature or stochastic as data scientists would have a say uh it can generate some uh",
    "start": "2528920",
    "end": "2536160"
  },
  {
    "text": "unpredictable results maybe it generates inappropriate results so I apologize in advance it does that so with a",
    "start": "2536160",
    "end": "2541720"
  },
  {
    "text": "temperature of zero this is gpt2 remember it's just saying it's a very good dog it's a very good dog it's a very good dog the favorite animal is",
    "start": "2541720",
    "end": "2548040"
  },
  {
    "text": "always the dog because that is the most probable token however as we start increasing it you can see that it is",
    "start": "2548040",
    "end": "2554800"
  },
  {
    "text": "making different predictions it's mostly picking dog but it will occasionally flick uh black",
    "start": "2554800",
    "end": "2561440"
  },
  {
    "text": "bear and then as we uh we increase this oh it's got rat rabbit cat rabbit rabbit",
    "start": "2561440",
    "end": "2567720"
  },
  {
    "text": "uh no dogs uh with a temperature of 0.04 and then it tends to get more creative",
    "start": "2567720",
    "end": "2572880"
  },
  {
    "text": "as we uh increase uh the the temperature so I definitely recommend when you're building GPT Solutions of varying the",
    "start": "2572880",
    "end": "2579960"
  },
  {
    "text": "temperature um is there any way you can automate these tests as well uh because",
    "start": "2579960",
    "end": "2585240"
  },
  {
    "text": "focusing on building these kind of rag based Solutions you don't just want to have one question and focusing on well",
    "start": "2585240",
    "end": "2590760"
  },
  {
    "text": "if I set the temperature 0.6 then this particular question gets a good answer it's going to affect all the questions",
    "start": "2590760",
    "end": "2596640"
  },
  {
    "text": "that you send through it so um next session I'm going to be doing at conferences uh next time round is going",
    "start": "2596640",
    "end": "2602520"
  },
  {
    "text": "to be looking at how we can do testing and Analysis of these these models and",
    "start": "2602520",
    "end": "2607839"
  },
  {
    "text": "uh you know and analyze be able to you know analyze how well prompts and how well temperature is working because we",
    "start": "2607839",
    "end": "2613440"
  },
  {
    "text": "can't just do testing uh when an answer comes out you it's green and if a bad answer comes out it's red we need to",
    "start": "2613440",
    "end": "2619240"
  },
  {
    "text": "think about stats and and things like that so also changing and modifying uh the temperature you can see uh that this",
    "start": "2619240",
    "end": "2625200"
  },
  {
    "text": "is getting more creative uh with the actual uh language uh that it is is uh",
    "start": "2625200",
    "end": "2630480"
  },
  {
    "text": "creating here remember this is a gpt2 uh that we're using so the um answers are",
    "start": "2630480",
    "end": "2635599"
  },
  {
    "text": "going to be um slightly less predictable than uh normal okay let's just drop back to the",
    "start": "2635599",
    "end": "2642599"
  },
  {
    "text": "slides as well as the the temperature there's other parameters uh that we can use to influence uh the actual output so",
    "start": "2642599",
    "end": "2649800"
  },
  {
    "start": "2646000",
    "end": "2781000"
  },
  {
    "text": "temperature and top PE are the main ones that kind of govern uh what is going to be coming out of the model n is how many",
    "start": "2649800",
    "end": "2656240"
  },
  {
    "text": "track completion Choice choices to generate for each input uh message so what I've shown is just how we're doing",
    "start": "2656240",
    "end": "2663839"
  },
  {
    "text": "the actual uh prediction of the next uh token now what goes on to make um the",
    "start": "2663839",
    "end": "2670200"
  },
  {
    "text": "answers kind of a nicer and more human readable is it also works with something called a beam search where it kind of",
    "start": "2670200",
    "end": "2676800"
  },
  {
    "text": "looks you know the cat's at on the floor and if we choose floor we can choose these you know four or five other tokens",
    "start": "2676800",
    "end": "2682160"
  },
  {
    "text": "if we choose one of those then we can choose these other tokens and this is looking at um you know can we generate",
    "start": "2682160",
    "end": "2687720"
  },
  {
    "text": "multiple completions for each each message uh when we're generating uh those those messages there we've also",
    "start": "2687720",
    "end": "2692880"
  },
  {
    "text": "got a stop sequence when is it going to stop talking so when it's generating um you know I'm doing a question and answer",
    "start": "2692880",
    "end": "2698880"
  },
  {
    "text": "we could say Well when a full stop appears let's stop the actual uh output um so you can specify you know various",
    "start": "2698880",
    "end": "2705240"
  },
  {
    "text": "sequences while it will stop and not generate any other tokens we can also specify the maximum number of tokens to",
    "start": "2705240",
    "end": "2710680"
  },
  {
    "text": "generate in the chat completion we may not want to generate really long answers we better want to generate short answers",
    "start": "2710680",
    "end": "2716200"
  },
  {
    "text": "also we're paying for all of these tokens and it takes time to generate tokens so that can be a specified",
    "start": "2716200",
    "end": "2722040"
  },
  {
    "text": "there's a presence penalty and a frequency penalty so if you want it to",
    "start": "2722040",
    "end": "2727720"
  },
  {
    "text": "not say he's a rockar he's a rockar he's a rockar he's a rockar that's going be relating to the frequency penalty we",
    "start": "2727720",
    "end": "2734640"
  },
  {
    "text": "don't want it to keep repeating uh various things now if you're generating a kid story or a poem or lyrics for a",
    "start": "2734640",
    "end": "2741280"
  },
  {
    "text": "song then maybe repetition is good because you want to be repeating the same kind of phrases or the same kind of",
    "start": "2741280",
    "end": "2746800"
  },
  {
    "text": "words or stanzas within the lyrics of a song uh in the chorus for example but in other more scientific texts you maybe",
    "start": "2746800",
    "end": "2753880"
  },
  {
    "text": "don't want to do uh that actual uh repetition and and so on also with generating code there's a lot of",
    "start": "2753880",
    "end": "2759800"
  },
  {
    "text": "repetition uh when we actually write code uh we don't want the codes to not use the same sequences that we use three",
    "start": "2759800",
    "end": "2765359"
  },
  {
    "text": "lines above because that's going to uh result in in everything getting really uh chaotic also we can modify the",
    "start": "2765359",
    "end": "2771200"
  },
  {
    "text": "likelihood likelihood of specific tokens appearing in the completion we can also put in a block list and stop certain uh",
    "start": "2771200",
    "end": "2778119"
  },
  {
    "text": "tokens from appearing in that in that sequence so uh that's all I was going to uh say again I would really like to",
    "start": "2778119",
    "end": "2784960"
  },
  {
    "start": "2781000",
    "end": "2817000"
  },
  {
    "text": "thank uh the uh the other people within in the community who put lots of energy into understanding uh and explaining how",
    "start": "2784960",
    "end": "2791400"
  },
  {
    "text": "these models work and again I would highly like to recommend checking out three blue one Browns uh two uh videos",
    "start": "2791400",
    "end": "2798280"
  },
  {
    "text": "that you've done on Transformers they are I think I think they're marked as being chapter five and chapter six in",
    "start": "2798280",
    "end": "2803720"
  },
  {
    "text": "the Deep learning series but don't bother you don't have to watch uh the the first ones to to understand that",
    "start": "2803720",
    "end": "2809680"
  },
  {
    "text": "that kind of Builds on what I've been talking about and uses animations to show how all of these uh Transformer",
    "start": "2809680",
    "end": "2815160"
  },
  {
    "text": "models are working so uh I think we've got about two or 3 minutes uh for uh for",
    "start": "2815160",
    "end": "2820680"
  },
  {
    "start": "2817000",
    "end": "2841000"
  },
  {
    "text": "questions uh so uh I was wondering if you got any we can take questions from the app from the audience",
    "start": "2820680",
    "end": "2828280"
  }
]