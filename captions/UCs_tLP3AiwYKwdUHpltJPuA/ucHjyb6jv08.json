[
  {
    "start": "0",
    "end": "52000"
  },
  {
    "text": "so hi everyone my name is Nathan Mars uh I work at Twitter uh I've been involved",
    "start": "3120",
    "end": "8200"
  },
  {
    "text": "in the data and Big Data space for over four years now um I've learned a lot of",
    "start": "8200",
    "end": "14160"
  },
  {
    "text": "lessons in that time and uh today I'll be talking about what I believe to be a major problem we have in the industry",
    "start": "14160",
    "end": "20359"
  },
  {
    "text": "right now which is that there's a lot of complexity in the systems people are building and that complexity doesn't",
    "start": "20359",
    "end": "25800"
  },
  {
    "text": "need to exist and that complexity has taken a real toll uh in making systems hard to uh evolve um hard to make robust",
    "start": "25800",
    "end": "34760"
  },
  {
    "text": "and other problems so this talk is structured in two parts so first I'm going to talk about some common sources",
    "start": "34760",
    "end": "41039"
  },
  {
    "text": "of complexity that you see in pretty much every data system uh and then I'll talk about what's a different way to",
    "start": "41039",
    "end": "46520"
  },
  {
    "text": "design Data Systems that doesn't have these problems so before we get to that we",
    "start": "46520",
    "end": "52879"
  },
  {
    "start": "52000",
    "end": "120000"
  },
  {
    "text": "should actually just Define what is it that we're talking about what is a data system what are these things people are",
    "start": "52879",
    "end": "58519"
  },
  {
    "text": "trying to build well place to start is that a data system is a system that manages the storage and querying of data",
    "start": "58519",
    "end": "65720"
  },
  {
    "text": "right I don't think anyone will really argue with that but that doesn't really tell the whole story because a data system has a lifetime measured in years",
    "start": "65720",
    "end": "73799"
  },
  {
    "text": "right it it lasts a long time like Twitter has been around for 6 years and it's been one continuous data system",
    "start": "73799",
    "end": "80040"
  },
  {
    "text": "over six years but of course over those six years there's been been many different versions of the application",
    "start": "80040",
    "end": "86119"
  },
  {
    "text": "right we've deployed new features we've taken away features we've collected new kinds of data um we've had schema",
    "start": "86119",
    "end": "93159"
  },
  {
    "text": "changes um this is all part of what happens as you're managing a data system right and of course as part of those six",
    "start": "93159",
    "end": "99280"
  },
  {
    "text": "years we've had a lot of Hardware failures right we've had a lot of machines go down things run out of",
    "start": "99280",
    "end": "104439"
  },
  {
    "text": "memory discs fail and your data system needs to just keep on going through those Hardware failures and likewise",
    "start": "104439",
    "end": "110399"
  },
  {
    "text": "there's been a lot of human mistakes human mistakes are inevitable in software development and your system",
    "start": "110399",
    "end": "115640"
  },
  {
    "text": "needs to keep on going past those human mistakes so I'm going to talk about three very",
    "start": "115640",
    "end": "122520"
  },
  {
    "start": "120000",
    "end": "155000"
  },
  {
    "text": "common sources of complexity which I believe to be nearly Universal uh and you don't really hear people talk about",
    "start": "122520",
    "end": "127600"
  },
  {
    "text": "this stuff really at all but I consider this stuff just as important as a stuff people do talk about right people always",
    "start": "127600",
    "end": "133280"
  },
  {
    "text": "talk about scalability and the cap theorem and all these other things and those things are very important but I",
    "start": "133280",
    "end": "138800"
  },
  {
    "text": "consider these things to be just as important and the fact that people don't talk about these things indicates to me",
    "start": "138800",
    "end": "144319"
  },
  {
    "text": "that as an industry we've just accepted and internalized these complexities people don't even realize that they're",
    "start": "144319",
    "end": "150400"
  },
  {
    "text": "being affected by these things and it is affecting almost everyone so the first one I want to talk",
    "start": "150400",
    "end": "155760"
  },
  {
    "start": "155000",
    "end": "175000"
  },
  {
    "text": "about is human fault tolerance",
    "start": "155760",
    "end": "161159"
  },
  {
    "text": "so we haven't figured out how to develop perfect software yet right in the lifetime of a data system bugs will be",
    "start": "161159",
    "end": "167640"
  },
  {
    "text": "deployed to production this is a fact right and people will make operational mistakes humans aren't perfect this is",
    "start": "167640",
    "end": "172800"
  },
  {
    "text": "just what happens now raise your hand if you like to build systems where you know",
    "start": "172800",
    "end": "178560"
  },
  {
    "start": "175000",
    "end": "308000"
  },
  {
    "text": "there's going to be a fault a known fault and you have no plan to deal with it oh come",
    "start": "178560",
    "end": "186200"
  },
  {
    "text": "on cuz the thing is this is what everyone does right um You you know a",
    "start": "186200",
    "end": "192799"
  },
  {
    "text": "mistake is made and you shrug your shoulders and you say hopefully the logs have enough information that we can",
    "start": "192799",
    "end": "198319"
  },
  {
    "text": "figure out what went wrong and correct it but relying on luck to me is not Sound Engineering practice I think the",
    "start": "198319",
    "end": "205000"
  },
  {
    "text": "only way to really deal with this is to consider humans as part of your overall system just like your hard diss your",
    "start": "205000",
    "end": "211360"
  },
  {
    "text": "CPUs your memory and your software and you should design for human error just like you design for any other",
    "start": "211360",
    "end": "217799"
  },
  {
    "text": "fault now there's a lot of mistakes that humans can make um and in fact the scope",
    "start": "217799",
    "end": "223879"
  },
  {
    "text": "of problems that a human error can cause are much greater than the scope of Hardware failures right like some",
    "start": "223879",
    "end": "229439"
  },
  {
    "text": "examples of human error is these are ones that just I've made um you might",
    "start": "229439",
    "end": "234840"
  },
  {
    "text": "deploy a bug to production that starts incrementing counters in your database by two instead of by one and maybe you",
    "start": "234840",
    "end": "240120"
  },
  {
    "text": "don't realize for 3 weeks until a customer comes to you and says hey this isn't looking so good uh maybe you'll",
    "start": "240120",
    "end": "246439"
  },
  {
    "text": "accidentally delete data from a database raise your hand if you've ever deleted data from a database before accidentally okay see almost everyone I",
    "start": "246439",
    "end": "253920"
  },
  {
    "text": "remember my first year working as a software developer um I had a colleague who he was collecting this data set for",
    "start": "253920",
    "end": "260239"
  },
  {
    "text": "weeks he was crawling the web and collecting data and just waiting to get enough data so that he could analyze it",
    "start": "260239",
    "end": "265680"
  },
  {
    "text": "and just you know do some interesting stuff with it and then me like an idiot just went in I just trashed the whole",
    "start": "265680",
    "end": "271400"
  },
  {
    "text": "thing it was all gone I wasted weeks of work from this guy and this was my first year as a software developer and I felt",
    "start": "271400",
    "end": "276440"
  },
  {
    "text": "really bad and um you know I was apologizing profusely and you know again",
    "start": "276440",
    "end": "282520"
  },
  {
    "text": "it was my first year I was like man am I am I going to get fired for this like I made a really big mistake and I remember everyone was really empathetic um and I",
    "start": "282520",
    "end": "289560"
  },
  {
    "text": "remember one person said to me congratulations you're now a professional software",
    "start": "289560",
    "end": "295919"
  },
  {
    "text": "developer but this is a fact this is this is what we do and and these mistakes will be made and you have to",
    "start": "295919",
    "end": "301440"
  },
  {
    "text": "have a plan to deal with them and of course there are many other uh types of mistakes that can be",
    "start": "301440",
    "end": "306800"
  },
  {
    "text": "made now I would assert that the worst thing that can happen uh as a consequence of a human",
    "start": "306800",
    "end": "312880"
  },
  {
    "start": "308000",
    "end": "332000"
  },
  {
    "text": "human mistake is data loss or data corruption because the thing is is that as long as your mistake doesn't lose or",
    "start": "312880",
    "end": "319039"
  },
  {
    "text": "corrupt stuff you had before at least you can fix what went wrong right now in",
    "start": "319039",
    "end": "324120"
  },
  {
    "text": "the in the interm of the mistake you know it's up in the air what can happen but at least you didn't destroy what you previously had that's",
    "start": "324120",
    "end": "331080"
  },
  {
    "text": "important and this brings me to the topic of mutability so the standard operations in databases are known as",
    "start": "331080",
    "end": "338000"
  },
  {
    "start": "332000",
    "end": "485000"
  },
  {
    "text": "crud create read update delete and mutability refers to the U and D in crud",
    "start": "338000",
    "end": "344600"
  },
  {
    "text": "now when you build a system based on mutable State the way you model the world is that you keep State",
    "start": "344600",
    "end": "351000"
  },
  {
    "text": "representing the current state of the world and your system just updates whatever you believe to be the current state of the world this is pretty much",
    "start": "351000",
    "end": "357080"
  },
  {
    "text": "standard practice in how people build systems now mutable systems inherently lack human fault tolerance your system",
    "start": "357080",
    "end": "362960"
  },
  {
    "text": "is updating deleting data all the time which means any mistake you make can also update or delete data and",
    "start": "362960",
    "end": "370840"
  },
  {
    "text": "uh that that can be arbitrarily bad it could update and delete a lot of data um",
    "start": "370840",
    "end": "376000"
  },
  {
    "text": "which means it's very easy to corrupt or lose data and you lack human FAL tolerance um I liken mutable systems to",
    "start": "376000",
    "end": "384479"
  },
  {
    "text": "being similar to if you had a guy come over to your house and pour gasoline all over your house and then just covers the",
    "start": "384479",
    "end": "392000"
  },
  {
    "text": "whole thing in gasoline and then he comes to you and he says hey don't worry",
    "start": "392000",
    "end": "397120"
  },
  {
    "text": "about it uh I checked all the wires they're all insulated there won't be any Sparks nothing's going to go wrong but",
    "start": "397120",
    "end": "403240"
  },
  {
    "text": "you know something's going to go wrong you don't cover your house in gasoline right and likewise you shouldn't cover your data in gasoline and uh you should",
    "start": "403240",
    "end": "410520"
  },
  {
    "text": "make your data Fireproof by not making it mutable now there's a different way to",
    "start": "410520",
    "end": "416120"
  },
  {
    "text": "build systems and this is basing your systems on imut ility now in an immutable system instead of capturing",
    "start": "416120",
    "end": "423160"
  },
  {
    "text": "the current state of the world you capture a series of events over history and each event that you capture happens",
    "start": "423160",
    "end": "429280"
  },
  {
    "text": "at a particular time and is always true right so you never delete or update data you're always just adding new",
    "start": "429280",
    "end": "435440"
  },
  {
    "text": "data so let's look let's look at an example of the difference between mutability and immutability so let's say",
    "start": "435440",
    "end": "441520"
  },
  {
    "text": "you're storing locations on people right so let's say you start off with a database that contains people and",
    "start": "441520",
    "end": "446960"
  },
  {
    "text": "locations so Sally lives in Philadelphia and Bob lives in Chicago right and now let's say Sally moves to New York so",
    "start": "446960",
    "end": "452599"
  },
  {
    "text": "what you'll do is you update your database to say Sally now lives in New York right this is pretty standard now",
    "start": "452599",
    "end": "458680"
  },
  {
    "text": "an immutable system will be different right instead of saying where someone lives now you just say where someone",
    "start": "458680",
    "end": "464440"
  },
  {
    "text": "lived as as at a certain time so you say Sally lived in Philadelphia as of that time Bob lived in Chicago as of that",
    "start": "464440",
    "end": "470720"
  },
  {
    "text": "time and when Sally moved to New York she now lives in this new location as of this new time right and this makes sense",
    "start": "470720",
    "end": "476479"
  },
  {
    "text": "the fact that Sally moves to New York does not change the fact she used to live in Philadelphia both of those things can be true at the same",
    "start": "476479",
    "end": "483840"
  },
  {
    "text": "time now basing a system on immutability greatly restricts the range of errors that can cause data loss or data",
    "start": "483840",
    "end": "489879"
  },
  {
    "start": "485000",
    "end": "575000"
  },
  {
    "text": "corruption right when you're not updating and deleting data you can do things like um Step permissions so that",
    "start": "489879",
    "end": "496360"
  },
  {
    "text": "it's impossible to update or delete data which makes it much harder for a random mistake to cause a severe amount of",
    "start": "496360",
    "end": "503240"
  },
  {
    "text": "damage right which means immutable systems are vastly more human fault tolerant than mutable systems",
    "start": "503240",
    "end": "510599"
  },
  {
    "text": "the thing about immutability is that while human fault tolerance is a major benefit of it it has a lot of other",
    "start": "510599",
    "end": "516919"
  },
  {
    "text": "benefits right immutability is fundamentally simpler than mutability right instead of having to support four",
    "start": "516919",
    "end": "522518"
  },
  {
    "text": "operations that have to work well together CR you only have to support CNR",
    "start": "522519",
    "end": "527720"
  },
  {
    "text": "right that's fundamentally simpler that's two operations that have to work together instead of four um a nice thing",
    "start": "527720",
    "end": "533800"
  },
  {
    "text": "about immutability um is that it's actually very easy to",
    "start": "533800",
    "end": "539600"
  },
  {
    "text": "implement your mutable data so when you look at M A mutable system a mutable system implies that all your data needs",
    "start": "539600",
    "end": "545240"
  },
  {
    "text": "to be indexed so that you can find the individual data unit you need when you need to update or delete it now in an",
    "start": "545240",
    "end": "551240"
  },
  {
    "text": "immutable system when you only have to appen data you don't need to index all of your data and we'll come back to this later to see how it works but basically",
    "start": "551240",
    "end": "558000"
  },
  {
    "text": "implementing your immutable data store is as easy as having a directory in a distributed file system where every file",
    "start": "558000",
    "end": "564200"
  },
  {
    "text": "contains a list of data records and whenever you want to add data you just append a new file to that directory it's very very easy to to build the immutable",
    "start": "564200",
    "end": "571200"
  },
  {
    "text": "part of your system and we'll come back to this um and I have to give a shout out",
    "start": "571200",
    "end": "576279"
  },
  {
    "start": "575000",
    "end": "710000"
  },
  {
    "text": "to Rich hickey uh Rich hickey is the author of closure um he's been a big",
    "start": "576279",
    "end": "581480"
  },
  {
    "text": "advocate of immutability and I think no one has really articulated better the benefits of immutability uh so if you",
    "start": "581480",
    "end": "587120"
  },
  {
    "text": "want to find out more I highly recommend watching his talks okay so the next source of",
    "start": "587120",
    "end": "593399"
  },
  {
    "text": "complexity I want to talk about is the conflation of data and queries and one instance of this problem",
    "start": "593399",
    "end": "601399"
  },
  {
    "text": "uh is the normalization versus denormalization problem so if you're working with a relational database um",
    "start": "601399",
    "end": "608440"
  },
  {
    "text": "let's say you're working with a relational database and you're keeping track of locations on people um so one way you might design your schema is like",
    "start": "608440",
    "end": "614760"
  },
  {
    "text": "this where for every person you have a identifier representing what location",
    "start": "614760",
    "end": "621040"
  },
  {
    "text": "they live in uh and then you have another table which contains more information on that location like the city state and",
    "start": "621040",
    "end": "627440"
  },
  {
    "text": "population now let's say that a very common query you're doing is getting the city and state for a person well what",
    "start": "627440",
    "end": "634240"
  },
  {
    "text": "you might find over time that it becomes too expensive to do that join between the two tables um so what you do is",
    "start": "634240",
    "end": "640600"
  },
  {
    "text": "standard practice in when using relational databases is you denormalize your schema so you remove the join by",
    "start": "640600",
    "end": "646800"
  },
  {
    "text": "duplicating the data into your person table so instead of just storing the locations one time in the location table",
    "start": "646800",
    "end": "652760"
  },
  {
    "text": "you copy the city and state into the person table so the person table now contains the name the location ID and",
    "start": "652760",
    "end": "659480"
  },
  {
    "text": "then the city and state and this prevents you from having to to do that join which makes those queries much",
    "start": "659480",
    "end": "665680"
  },
  {
    "text": "faster now obviously you prefer data to be fully normalized denormalizing has a",
    "start": "665680",
    "end": "671160"
  },
  {
    "text": "lot of problems um now it's up to you as an application developer to make sure things become stay consistent but as we",
    "start": "671160",
    "end": "678040"
  },
  {
    "text": "know the lifetime of a data system is very long and it's pretty unreasonable to assume that you'll be able to",
    "start": "678040",
    "end": "683760"
  },
  {
    "text": "maintain this very complex property over those years and that you or someone in the future won't make a mistake so",
    "start": "683760",
    "end": "689360"
  },
  {
    "text": "inevitably what happens in systems like this is stuff becomes inconsistent and you have no idea why um and then what",
    "start": "689360",
    "end": "695959"
  },
  {
    "text": "you do is you start adding code to deal with these cases or you write these one-off scripts to fix the inconsistencies this is all extremely",
    "start": "695959",
    "end": "703079"
  },
  {
    "text": "complex and difficult to deal with clearly you want to store data one time um you want data to be fully",
    "start": "703079",
    "end": "709519"
  },
  {
    "text": "normalized but you have to denormalize for performance there's no way around this when using something like a",
    "start": "709519",
    "end": "715160"
  },
  {
    "start": "710000",
    "end": "745000"
  },
  {
    "text": "relational database and the reason you have to denormalize is because the way you you store model and query data is",
    "start": "715160",
    "end": "721920"
  },
  {
    "text": "complexed together they're all fundamentally intertwined now in the second half of",
    "start": "721920",
    "end": "728680"
  },
  {
    "text": "this talk we'll come back to how you build Data Systems in which case these in which these things are disassociated and you can optimize these pieces",
    "start": "728680",
    "end": "735079"
  },
  {
    "text": "independently and this is really important okay so the last bit of",
    "start": "735079",
    "end": "740160"
  },
  {
    "text": "complexity I want to talk about is schemas uh so I don't think schemas have ever",
    "start": "740160",
    "end": "746240"
  },
  {
    "start": "745000",
    "end": "750000"
  },
  {
    "text": "really been done right and it's an incredibly important thing if you look at the industry right now schemas kind of have a bad bad",
    "start": "746240",
    "end": "753760"
  },
  {
    "start": "750000",
    "end": "788000"
  },
  {
    "text": "reputation right people consider schemas to be hard to change they get in your way they add development overhead they",
    "start": "753760",
    "end": "760279"
  },
  {
    "text": "just make your life difficult as a developer and maybe they have annoying configuration people just consider schemas to be a pain in the",
    "start": "760279",
    "end": "767519"
  },
  {
    "text": "ass and so a big Trend in the industry recently has been to use scheist databases and just store your data as",
    "start": "767519",
    "end": "773560"
  },
  {
    "text": "Json and this stuff actually demos really well if you do a five-minute demo with a schem less database it's you look",
    "start": "773560",
    "end": "779000"
  },
  {
    "text": "at like wow this is so flexible it's so easy I don't have to deal with declaring fields and types and all that stuff I",
    "start": "779000",
    "end": "784399"
  },
  {
    "text": "can just get in do my business and get out but I consider this to be a major",
    "start": "784399",
    "end": "792399"
  },
  {
    "start": "788000",
    "end": "804000"
  },
  {
    "text": "overreaction and the pro the fundamental Pro uh mistake people are making is they're confusing the poor",
    "start": "792399",
    "end": "797480"
  },
  {
    "text": "implementation of schemas with the actual value that schemas provide so they're two very different",
    "start": "797480",
    "end": "803000"
  },
  {
    "text": "things and I think to really understand this you have to ask yourself what is a schema why do we have schemas what's the",
    "start": "803000",
    "end": "808839"
  },
  {
    "start": "804000",
    "end": "827000"
  },
  {
    "text": "purpose purpose of a schema how does this help us and a schema fundamentally is very simple it's just a function that",
    "start": "808839",
    "end": "814639"
  },
  {
    "text": "takes in one piece of data and tells you whether that data is valid or",
    "start": "814639",
    "end": "820120"
  },
  {
    "text": "not this is an incredibly useful thing um schemas give you a lot they",
    "start": "820959",
    "end": "827519"
  },
  {
    "start": "827000",
    "end": "850000"
  },
  {
    "text": "give you structural Integrity they they let you know that um they let you know",
    "start": "827519",
    "end": "832720"
  },
  {
    "text": "that when you read your data you'll have certain certain guarantees of what's in it what fields are in it what types they have what values they have um um and the",
    "start": "832720",
    "end": "840079"
  },
  {
    "text": "most important thing this does is it prevents corruption it prevents those mistakes and those random bugs from",
    "start": "840079",
    "end": "846639"
  },
  {
    "text": "corrupting your database um and these are the absolute worst kind of issues to deal with um",
    "start": "846639",
    "end": "854680"
  },
  {
    "start": "850000",
    "end": "855000"
  },
  {
    "text": "because you know you detect the corruption long after it may have happened right when you're reading the data Maybe you read it in a map reduced",
    "start": "854680",
    "end": "861399"
  },
  {
    "start": "855000",
    "end": "865000"
  },
  {
    "text": "job or you read it um just when doing a database call and since that corruption happen so long after it may have",
    "start": "861399",
    "end": "868240"
  },
  {
    "text": "occurred maybe week or months um it's extremely hard to track down what happened um you don't have any insight",
    "start": "868240",
    "end": "874399"
  },
  {
    "text": "into the circumstances that caused the corruption right these are the kinds of things that can cause weeks of time to",
    "start": "874399",
    "end": "880360"
  },
  {
    "text": "figure out and fix um first of all you have to figure out how it went wrong fix one went wrong and then clean up your",
    "start": "880360",
    "end": "886600"
  },
  {
    "text": "data set to make sure um it's clean again now when you have a proper schem",
    "start": "886600",
    "end": "893160"
  },
  {
    "start": "891000",
    "end": "902000"
  },
  {
    "text": "of function you can prevent these mistakes before they happen right and you get an exception at the time the mistake is made which means you have",
    "start": "893160",
    "end": "899120"
  },
  {
    "text": "complete insight into what the problem was right and this just saves an enormous amount of time and this is a",
    "start": "899120",
    "end": "905839"
  },
  {
    "start": "902000",
    "end": "913000"
  },
  {
    "text": "lesson that I learned uh I definitely learned this through",
    "start": "905839",
    "end": "911800"
  },
  {
    "text": "experience so now you have to look at okay why your scheme is considered painful and is there a better way we can",
    "start": "911920",
    "end": "917320"
  },
  {
    "start": "913000",
    "end": "957000"
  },
  {
    "text": "manage them well one reason schemas are considered painful is because it may be hard to change right if you ever ever",
    "start": "917320",
    "end": "923320"
  },
  {
    "text": "tried to add a column to a massive relational table you know this can be take a long time and be really painful",
    "start": "923320",
    "end": "929040"
  },
  {
    "text": "and so you don't really want to mess with the schema um but you have to mess with the schema because systems evolve",
    "start": "929040",
    "end": "934120"
  },
  {
    "text": "over time and you're going to have to change them you know a lot of times schemas are overly restrictive right like you you can only have like maybe",
    "start": "934120",
    "end": "940440"
  },
  {
    "text": "you can't do nested objects or nested Fields um even though you want to um a",
    "start": "940440",
    "end": "945560"
  },
  {
    "text": "lot of times schemas require these translation layers like like oh like object relational mappers and this again",
    "start": "945560",
    "end": "951160"
  },
  {
    "text": "makes it just hard to deal with your schema it adds a lot of development",
    "start": "951160",
    "end": "956480"
  },
  {
    "text": "overhead right but notice that none of these things are fundamentally linked with that idea of function of data unit",
    "start": "956519",
    "end": "962720"
  },
  {
    "text": "right these are just the stuff that exists are just these specialized ways to make these functions and I think the",
    "start": "962720",
    "end": "968440"
  },
  {
    "text": "the the current state-of-the-art for making schemas is uh limited and",
    "start": "968440",
    "end": "973880"
  },
  {
    "text": "confuses people as to whether they even want to use a schema or",
    "start": "973880",
    "end": "979079"
  },
  {
    "text": "not so I think schema is can be done a lot better um if I just describe my",
    "start": "979720",
    "end": "985240"
  },
  {
    "start": "981000",
    "end": "1042000"
  },
  {
    "text": "ideal schema Tool uh I would like to just have my data represented using regular data structures like like Maps",
    "start": "985240",
    "end": "991319"
  },
  {
    "text": "en closure right and a schema tool should just be a regular library that helps you construct the schema right so",
    "start": "991319",
    "end": "996880"
  },
  {
    "text": "you can say here are the required fields and here are the type of those fields and if I want I can insert custom",
    "start": "996880",
    "end": "1002000"
  },
  {
    "text": "validation logic to say things like ages have to be between Z and 200 or this uh",
    "start": "1002000",
    "end": "1007759"
  },
  {
    "text": "this Bank transaction has to be non- negative um a schema tool has to have",
    "start": "1007759",
    "end": "1013160"
  },
  {
    "text": "built-in support for easily evolving the schema over time because like I said your data system lasts for a long time",
    "start": "1013160",
    "end": "1019120"
  },
  {
    "text": "it's going to change over time uh one really fundamental thing you have to do with data is serialize and deserialize",
    "start": "1019120",
    "end": "1025678"
  },
  {
    "text": "it so the schem of tools should have support for fast and space efficient calization and ideally your schema tools",
    "start": "1025679",
    "end": "1031438"
  },
  {
    "text": "are also cross- language um because in a large company people tend to use a lot of languages um I currently use Apache",
    "start": "1031439",
    "end": "1038558"
  },
  {
    "text": "Thrift to Define my schemas uh is everyone here familiar with Apache",
    "start": "1038559",
    "end": "1043760"
  },
  {
    "start": "1042000",
    "end": "1130000"
  },
  {
    "text": "Thrift no few people okay it's basically a uh uh basically defines its own",
    "start": "1043760",
    "end": "1050200"
  },
  {
    "text": "language for defining schemas uh it has kind of a schema definition language and then a cross language RPC framework and",
    "start": "1050200",
    "end": "1057120"
  },
  {
    "text": "Thrift is pretty good I think it could be better um the one thing it's really missing from here is the ability the",
    "start": "1057120",
    "end": "1062600"
  },
  {
    "text": "ability to insert custom validation logic but it's probably the best tool that exists currently but I think it",
    "start": "1062600",
    "end": "1068880"
  },
  {
    "text": "could be better I think if you if you had a tool that was really good then there's no question you'd want to use schemas uh because it just helps and it",
    "start": "1068880",
    "end": "1075480"
  },
  {
    "text": "doesn't really get in your way all right so let's get provocative",
    "start": "1075480",
    "end": "1081360"
  },
  {
    "text": "now I assert that the relational database will be a footnote in history",
    "start": "1081360",
    "end": "1087080"
  },
  {
    "text": "and it's probably not going to happen tomorrow it's probably not going to happen in 5 years it's probably not going to happen in 10 years but it will",
    "start": "1087080",
    "end": "1092440"
  },
  {
    "text": "happen one day right and the reason it's going to happen is not because of SQL restrictive schemas or even scalability",
    "start": "1092440",
    "end": "1098880"
  },
  {
    "text": "issues uh although these are all issues with the relational database but it's because of fundamental",
    "start": "1098880",
    "end": "1104919"
  },
  {
    "text": "flaws in the relational database approach to managing data right and these things things weren't flaws when",
    "start": "1104919",
    "end": "1110799"
  },
  {
    "text": "they originally designed I'd actually say they were features but uh things",
    "start": "1110799",
    "end": "1115960"
  },
  {
    "text": "have evolved and there are better ways to manage data now and these flaws that relational",
    "start": "1115960",
    "end": "1121600"
  },
  {
    "text": "databases have are mutability and the conflation of the storage of data with how it is queried both of which we have",
    "start": "1121600",
    "end": "1128559"
  },
  {
    "text": "covered right and these are the reasons like the big the new term in the industry now is new Sequel and I think",
    "start": "1128559",
    "end": "1133679"
  },
  {
    "start": "1130000",
    "end": "1182000"
  },
  {
    "text": "this term is completely misguided right you're just rebuilding the things that have complexity and you're rebuilding",
    "start": "1133679",
    "end": "1139440"
  },
  {
    "text": "the complexity there's no reason to do that now the thing is it's a new era we have new abilities and we should use",
    "start": "1139440",
    "end": "1145600"
  },
  {
    "text": "these new abilities uh namely our ability to cheaply store massive amounts of data to do data data the right way",
    "start": "1145600",
    "end": "1152720"
  },
  {
    "text": "and not inherit the complex complexities of the past right you look at systems today and like no SQL and you have to",
    "start": "1152720",
    "end": "1160799"
  },
  {
    "text": "deal with a cap theorem and eventual consistency like it's gotten a lot harder to deal with data than it used to",
    "start": "1160799",
    "end": "1165919"
  },
  {
    "text": "be in the past and that's kind of a weird thing right like our systems are way more powerful than they used to be",
    "start": "1165919",
    "end": "1171200"
  },
  {
    "text": "and we know a lot more about managing data than we used to and yet things are getting harder and more complex and",
    "start": "1171200",
    "end": "1176880"
  },
  {
    "text": "there's no reason for this to be the case things should actually be getting easier and simpler now I know what a lot of you are",
    "start": "1176880",
    "end": "1183840"
  },
  {
    "start": "1182000",
    "end": "1224000"
  },
  {
    "text": "thinking so you're thinking okay he says SQL is bad so if no SQL is not SQL",
    "start": "1183840",
    "end": "1189600"
  },
  {
    "text": "therefore no SQL must be right that must be what he's going to recommend but absolutely not I think no",
    "start": "1189600",
    "end": "1195600"
  },
  {
    "text": "SQL databases are generally not a step in the right direction there are some aspects of them that are uh namely the",
    "start": "1195600",
    "end": "1202159"
  },
  {
    "text": "idea of using of your database being a data structure um but the things that are interesting about these databases",
    "start": "1202159",
    "end": "1208360"
  },
  {
    "text": "are not the ones that get all the attention right because fundamentally they're still based on mutability and they're not general purpose right key",
    "start": "1208360",
    "end": "1215360"
  },
  {
    "text": "value key value is not a data model it's an indexing scheme right and it's not going to support all your applications",
    "start": "1215360",
    "end": "1221799"
  },
  {
    "text": "or even most of them okay so how would you build a better data system that doesn't have",
    "start": "1221799",
    "end": "1228000"
  },
  {
    "start": "1224000",
    "end": "1232000"
  },
  {
    "text": "this complexities um so let's start from scratch so first",
    "start": "1228000",
    "end": "1233240"
  },
  {
    "start": "1232000",
    "end": "1240000"
  },
  {
    "text": "we should answer the question of what is it that we're trying to do with Data Systems uh what do we actually use these things",
    "start": "1233240",
    "end": "1239320"
  },
  {
    "text": "for so you might start off by saying that a data system is somewhere that you",
    "start": "1239320",
    "end": "1244919"
  },
  {
    "start": "1240000",
    "end": "1380000"
  },
  {
    "text": "put data and then retrieve what you stored right this seems to be pretty standard on what how people actually",
    "start": "1244919",
    "end": "1251400"
  },
  {
    "text": "think about what databases do this is not really how people use data systems",
    "start": "1251400",
    "end": "1257080"
  },
  {
    "text": "right and this is very easy to see when you look at some examples so for example let's say that you're storing data of",
    "start": "1257080",
    "end": "1263480"
  },
  {
    "text": "location information on people so some of the questions you might ask of that data are how many people live in a",
    "start": "1263480",
    "end": "1269400"
  },
  {
    "text": "particular location so here you're not really retrieving what you stored right you're",
    "start": "1269400",
    "end": "1274840"
  },
  {
    "text": "you're doing this transformation and aggregation of the data that you have right it's different you're not just getting what you had you're you're",
    "start": "1274840",
    "end": "1280600"
  },
  {
    "text": "transforming it you might say where does Sally live right so this query is more",
    "start": "1280600",
    "end": "1285640"
  },
  {
    "text": "okay you're actually retrieving something that you stored or might ask what are the most populous locations",
    "start": "1285640",
    "end": "1291000"
  },
  {
    "text": "again this is a an aggregation of your data another example is you may be storing page view information maybe",
    "start": "1291000",
    "end": "1296799"
  },
  {
    "text": "you're making like a Google analytics like product so you might ask how many page views were there on September 2nd",
    "start": "1296799",
    "end": "1302679"
  },
  {
    "text": "right again you're not really retrieving what you stored you're doing this aggregation you might ask how many",
    "start": "1302679",
    "end": "1307880"
  },
  {
    "text": "unique visitors have there been over time right so here you're doing a completely different aggregation over the same",
    "start": "1307880",
    "end": "1313000"
  },
  {
    "text": "data maybe you're storing uh trans transactions for uh bank accounts",
    "start": "1313000",
    "end": "1319440"
  },
  {
    "text": "um so you might ask how much money does George have again this is an aggregation you're looking at his transaction history and combining them to get one",
    "start": "1319440",
    "end": "1326559"
  },
  {
    "text": "number which indicates his balance or you might ask how much money do people spend on housing so now you're looking at everyone and again you're doing this",
    "start": "1326559",
    "end": "1333400"
  },
  {
    "text": "transformation aggregation so I would say that that the",
    "start": "1333400",
    "end": "1338840"
  },
  {
    "text": "way to really describe what a data system does is it computes queries which are functions that take in all of your",
    "start": "1338840",
    "end": "1345240"
  },
  {
    "text": "data's input your entire data set no matter how big it is right now sometimes your function will",
    "start": "1345240",
    "end": "1351559"
  },
  {
    "text": "retrieve what you stored but often times you'll do Transformations aggregations",
    "start": "1351559",
    "end": "1357240"
  },
  {
    "text": "you're not just retrieving what you stored so defining a data system as something that implements pure functions",
    "start": "1357240",
    "end": "1364039"
  },
  {
    "text": "that taken all your data as input is the most General formulation of a data system clearly if you could write a",
    "start": "1364039",
    "end": "1369559"
  },
  {
    "text": "function on all your data you can do anything right every single data system is encapsulated by that idea of",
    "start": "1369559",
    "end": "1375000"
  },
  {
    "text": "functions that take in your whole data as set as input",
    "start": "1375000",
    "end": "1380279"
  },
  {
    "start": "1380000",
    "end": "1425000"
  },
  {
    "text": "okay so let's look at an example to see what I'm talking about so let's say that you're storing page views and you want",
    "start": "1380440",
    "end": "1386360"
  },
  {
    "text": "to be able to do a query which computes the total number of page used to a URL over a range of",
    "start": "1386360",
    "end": "1391600"
  },
  {
    "text": "time so if you were to write this as a function of all your data it would be a function that takes in all your data's",
    "start": "1391600",
    "end": "1397159"
  },
  {
    "text": "input a URL a start time and an end time and you would iterate through all your data and every time you see a piece of",
    "start": "1397159",
    "end": "1403720"
  },
  {
    "text": "data for that URL and with a time stamp between those two times you'd increment a counter then return the counter right",
    "start": "1403720",
    "end": "1410080"
  },
  {
    "text": "really simple to implement functions of all data but obviously this is going to be too slow right you can't run that",
    "start": "1410080",
    "end": "1416279"
  },
  {
    "text": "function in real time all data could be petabyte scale that function is not going to run in milliseconds it's",
    "start": "1416279",
    "end": "1421400"
  },
  {
    "text": "probably going to take at least hours to run right but this is a good place to",
    "start": "1421400",
    "end": "1427039"
  },
  {
    "start": "1425000",
    "end": "1442000"
  },
  {
    "text": "start because clearly this is this is completely General so how can we do things slightly differently so that we",
    "start": "1427039",
    "end": "1433520"
  },
  {
    "text": "can support real-time queries right so we can do functions of all data in real time and it's pretty simple what we do",
    "start": "1433520",
    "end": "1439320"
  },
  {
    "text": "is instead of just doing on the-fly computations on all the data at once you insert a pre-computation step right it",
    "start": "1439320",
    "end": "1446240"
  },
  {
    "start": "1442000",
    "end": "1453000"
  },
  {
    "text": "shouldn't be surprising this is what people are already doing but maybe present it in a little bit of a more",
    "start": "1446240",
    "end": "1451640"
  },
  {
    "text": "formal way so for example for this query instead of running functions on all your paid views you might pre-compute a view",
    "start": "1451640",
    "end": "1459240"
  },
  {
    "start": "1453000",
    "end": "1484000"
  },
  {
    "text": "which contains uh a mapping from URLs and hours to the number of page views for that URL for that hour right and",
    "start": "1459240",
    "end": "1466279"
  },
  {
    "text": "that view would be indexed by the URL and hour so now if you do a query to know the number of page views for",
    "start": "1466279",
    "end": "1471440"
  },
  {
    "text": "food.com blog between hours 3 and six you retrieve the entries for hours three",
    "start": "1471440",
    "end": "1477440"
  },
  {
    "text": "four five and six you add them together on the Fly and that would be a",
    "start": "1477440",
    "end": "1482960"
  },
  {
    "text": "result so if you look at an architecture like this there's two pieces to it right",
    "start": "1483480",
    "end": "1488880"
  },
  {
    "start": "1484000",
    "end": "1489000"
  },
  {
    "text": "there's a function there's two just it's defined as two functions one function which goes from all data to your",
    "start": "1488880",
    "end": "1494840"
  },
  {
    "start": "1489000",
    "end": "1513000"
  },
  {
    "text": "pre-computed view and another function that resolves your queries based in your pre-computed view so there's only two",
    "start": "1494840",
    "end": "1500679"
  },
  {
    "text": "problems you have to solve to build a data system right how do you implement the view function and how do",
    "start": "1500679",
    "end": "1506960"
  },
  {
    "text": "you implement the query function so let's start with the",
    "start": "1506960",
    "end": "1512559"
  },
  {
    "text": "views so a view is a function that takes all your data as input and then produces",
    "start": "1512559",
    "end": "1517960"
  },
  {
    "start": "1513000",
    "end": "1521000"
  },
  {
    "text": "this pre-computed view right this is immediately screen batch processing right this should",
    "start": "1517960",
    "end": "1524440"
  },
  {
    "start": "1521000",
    "end": "1530000"
  },
  {
    "text": "scream map produce um so when you look at map produce uh when you ask them what a map reduce",
    "start": "1524440",
    "end": "1531360"
  },
  {
    "start": "1530000",
    "end": "1595000"
  },
  {
    "text": "is they'll usually say something like oh it's this thing where you can define a map and a reducer and the map will run",
    "start": "1531360",
    "end": "1536919"
  },
  {
    "text": "on it and the data will get partitioned based on the key and then you'll run a reducer on it right that that describes",
    "start": "1536919",
    "end": "1542880"
  },
  {
    "text": "how map reduce Works doesn't really describe what it is what map ruce really is is a framework for computing",
    "start": "1542880",
    "end": "1548159"
  },
  {
    "text": "arbitrary functions and arbitrary data because the map ruce Paradigm is General enough to implement pretty much any",
    "start": "1548159",
    "end": "1554200"
  },
  {
    "text": "function on your data and this is really interesting because map can be used us uh to compute those views um and map",
    "start": "1554200",
    "end": "1561919"
  },
  {
    "text": "produce you know maybe has a little bit of a stigma being hard to use but actually there's been a lot of great work in this area and there's a lot of",
    "start": "1561919",
    "end": "1568399"
  },
  {
    "text": "really great tools for expressing those batch functions um and you can build",
    "start": "1568399",
    "end": "1574039"
  },
  {
    "text": "these batch views and just you know 10 lines of code so when you're doing",
    "start": "1574039",
    "end": "1581039"
  },
  {
    "text": "pre-computation via map reduce you have all your data and all your data uh would just be stored in a distributed file",
    "start": "1581039",
    "end": "1586799"
  },
  {
    "text": "system and you just run these MPP produce workflows that run over all the data and produce the batch views you need to resolve your",
    "start": "1586799",
    "end": "1593960"
  },
  {
    "text": "queries cool so map produce is great for uh for defining and running those",
    "start": "1594520",
    "end": "1599960"
  },
  {
    "start": "1595000",
    "end": "1641000"
  },
  {
    "text": "functions over all your data and producing your batch views uh but then the second piece this equation is those",
    "start": "1599960",
    "end": "1605000"
  },
  {
    "text": "batch views need to be indexed so they can be queried um now these databases are really interesting I call these",
    "start": "1605000",
    "end": "1610440"
  },
  {
    "text": "things batch view databases so the requirements are they have to be batch writable for map produce they have to",
    "start": "1610440",
    "end": "1615799"
  },
  {
    "text": "have fast random reads for queries uh but most interestingly they require no",
    "start": "1615799",
    "end": "1620880"
  },
  {
    "text": "random rights and since random rights cause probably 99.9% of the complexity in databases these are incredibly simple",
    "start": "1620880",
    "end": "1627880"
  },
  {
    "text": "databases and that Simplicity leads to them being very robust and very easy to scale um some examples of databases like",
    "start": "1627880",
    "end": "1634039"
  },
  {
    "text": "this are elephant to be which I wrote and then vort to another example uh written by",
    "start": "1634039",
    "end": "1640559"
  },
  {
    "start": "1641000",
    "end": "1664000"
  },
  {
    "text": "LinkedIn so there's a lot of nice properties about these batch views right they're really simple like I described",
    "start": "1641039",
    "end": "1647120"
  },
  {
    "text": "easy to scale they're very easy to make highly available right and because they don't",
    "start": "1647120",
    "end": "1652880"
  },
  {
    "text": "have any randomiz you can heavily optimize these these things these things can be way more efficient and way more",
    "start": "1652880",
    "end": "1658240"
  },
  {
    "text": "performant than any any database that does random",
    "start": "1658240",
    "end": "1662559"
  },
  {
    "start": "1664000",
    "end": "1761000"
  },
  {
    "text": "rights okay so this comes brings us back to some of the complexity stuff I was talking about in the beginning so this",
    "start": "1664000",
    "end": "1669559"
  },
  {
    "text": "architecture is really interesting because this batch views are defined as a function of all your data so you always look at all your data at once so",
    "start": "1669559",
    "end": "1676640"
  },
  {
    "text": "you don't really have any requirements of how your data is structured so you can normalize your data to your heart's",
    "start": "1676640",
    "end": "1682399"
  },
  {
    "text": "content you can have strong schemas because the only thing you're going to do with them is run functions over all of them so the left side can be fully",
    "start": "1682399",
    "end": "1688960"
  },
  {
    "text": "normalized now on the right side uh the batch views can be considered denormalized and I put this in quotes",
    "start": "1688960",
    "end": "1695840"
  },
  {
    "text": "because it's not exactly denormalization like you have in a relational database um because when you do denormalization",
    "start": "1695840",
    "end": "1702240"
  },
  {
    "text": "you're still you're optimizing how you store data so that you can still retrieve what you stored in a batch View",
    "start": "1702240",
    "end": "1708159"
  },
  {
    "text": "you can do Transformations you're not always retrieving what you stored you might do aggregations or things like that it's really interesting to see you",
    "start": "1708159",
    "end": "1713720"
  },
  {
    "text": "still have both ideas here you have both normalization and the idea of denormalization in the same system but",
    "start": "1713720",
    "end": "1719159"
  },
  {
    "text": "now they're independent they're disassociated from each other and they're connected by these functions of all your data right and this also solves",
    "start": "1719159",
    "end": "1726640"
  },
  {
    "text": "the complexity problem of D normalization because your batch views are just defined as functions of all your data right they can't really get",
    "start": "1726640",
    "end": "1732960"
  },
  {
    "text": "out of sync they're always comp uh recomputed by running functions over over all your data right it's always",
    "start": "1732960",
    "end": "1738240"
  },
  {
    "text": "dependent on whatever your data is if there's ever a problem it's very easy to see what the problem was because you have all the input and you have all the",
    "start": "1738240",
    "end": "1744880"
  },
  {
    "text": "output right that's all you need to understand what went wrong and what the problem",
    "start": "1744880",
    "end": "1750759"
  },
  {
    "text": "was okay so it seems like we're done right we can use map reduce to compute these pre-computed views and then we run",
    "start": "1751120",
    "end": "1758000"
  },
  {
    "text": "queries in those views and we're done right well not quite so the thing about",
    "start": "1758000",
    "end": "1763039"
  },
  {
    "start": "1761000",
    "end": "1819000"
  },
  {
    "text": "batch computation is that it's fundamentally slow right running functions of all your data is going to",
    "start": "1763039",
    "end": "1769159"
  },
  {
    "text": "take hours uh which means that your views are always going to be out of date there's a very interesting thing here",
    "start": "1769159",
    "end": "1774679"
  },
  {
    "text": "though which is that most of your data will be absorbed into those batch views right",
    "start": "1774679",
    "end": "1779880"
  },
  {
    "text": "99.999% it's only the last few hours of data which are not in your batch views right which are not pre-computed so all",
    "start": "1779880",
    "end": "1786519"
  },
  {
    "text": "you have left is to compensate for those last few hours of data and then now you have a generic system that can compute",
    "start": "1786519",
    "end": "1792080"
  },
  {
    "text": "real-time functions on your whole data set so if you actually look at your",
    "start": "1792080",
    "end": "1798679"
  },
  {
    "text": "batch view because it's always out of date it has the property of eventual consistency um but it's eventual",
    "start": "1798679",
    "end": "1805640"
  },
  {
    "text": "consistency without the associated complexities of eventual consistency this is a form of eventual consistency",
    "start": "1805640",
    "end": "1810799"
  },
  {
    "text": "that's very easy to reason about because you just Define a function of all your data and it's just going to be out of date by a few",
    "start": "1810799",
    "end": "1817559"
  },
  {
    "text": "hours okay so the only thing is let left is pre-computing views for the last few hours of",
    "start": "1818679",
    "end": "1824760"
  },
  {
    "start": "1819000",
    "end": "1833000"
  },
  {
    "text": "data right I call this the real-time views",
    "start": "1824760",
    "end": "1829880"
  },
  {
    "text": "and this is where the stream processing Paradigm comes in right so the idea here is that you build a set of parallel",
    "start": "1829880",
    "end": "1836200"
  },
  {
    "text": "views to your batch views that represent the pre-computation for the last few hours of data right now the stream",
    "start": "1836200",
    "end": "1842320"
  },
  {
    "text": "processing Paradigm is different than batch processing with batch processing you run functions of all your data with",
    "start": "1842320",
    "end": "1847640"
  },
  {
    "text": "stream processing you look at data as it comes in look at it one at a time and you update your real time views as you go in right so this is where you use",
    "start": "1847640",
    "end": "1856080"
  },
  {
    "text": "incremental algorithms and things like mutability so this portion can be really fast right this is also where no SQL",
    "start": "1856080",
    "end": "1862600"
  },
  {
    "text": "databases fit into the picture just for these realtime views that represent your last few hours of",
    "start": "1862600",
    "end": "1867799"
  },
  {
    "text": "data right and then when you want to resolve application queries you look at your both your batch View and your realtime view right you look at what",
    "start": "1867799",
    "end": "1874880"
  },
  {
    "text": "represents most of your data and then what represents your last few hours of data and you merge them together and now",
    "start": "1874880",
    "end": "1879919"
  },
  {
    "text": "you get a result right so we went from this diagram where you pre-compute views and",
    "start": "1879919",
    "end": "1886360"
  },
  {
    "start": "1882000",
    "end": "1979000"
  },
  {
    "text": "all your data and you quer your precomputed views to a practical implementation of this which I call the",
    "start": "1886360",
    "end": "1891720"
  },
  {
    "text": "Lambda architecture where you have a new data stream coming in which is constantly feeding into your all data set you pre-compute batch views and all",
    "start": "1891720",
    "end": "1898360"
  },
  {
    "text": "your data as a data stream comes in you update your realtime views and your queries go in both of those right this",
    "start": "1898360",
    "end": "1904760"
  },
  {
    "text": "is a generic way to build realtime Data Systems to build any real-time data",
    "start": "1904760",
    "end": "1910240"
  },
  {
    "text": "system now the real time views are the most complex part of the system because here you're using random read random",
    "start": "1910240",
    "end": "1915960"
  },
  {
    "text": "write databases uh you're doing algorithm these things are much harder um and random WR databases like I",
    "start": "1915960",
    "end": "1922799"
  },
  {
    "text": "said are much much more complex um the nice thing here is that because the real",
    "start": "1922799",
    "end": "1928200"
  },
  {
    "text": "time view only represents a few hours of data this portion of the system can be kept relatively small right you don't",
    "start": "1928200",
    "end": "1934000"
  },
  {
    "text": "need huge nosql clusters they can be small because you only need them to represent a few hours of data right and the other really nice",
    "start": "1934000",
    "end": "1940360"
  },
  {
    "text": "thing about this architecture is that if anything ever goes wrong the batch layer is constantly overwriting the real time",
    "start": "1940360",
    "end": "1946440"
  },
  {
    "text": "layer so things will Auto correct over time right when people build systems now they basically don't have any batch",
    "start": "1946440",
    "end": "1952799"
  },
  {
    "text": "layer right pretty much the way people build systems now is essentially only a real-time layer based on mutability and incremental algorithms right you don't",
    "start": "1952799",
    "end": "1959360"
  },
  {
    "text": "have that that batch layer supporting you that will simplify your system and make it easier to reason",
    "start": "1959360",
    "end": "1965360"
  },
  {
    "text": "about and so I call this property of the Lambda architecture complexity isolation because most of the complexity of your",
    "start": "1965360",
    "end": "1971039"
  },
  {
    "text": "system is isolated in this transient real-time layer that you can discard at any point",
    "start": "1971039",
    "end": "1978440"
  },
  {
    "start": "1979000",
    "end": "2117000"
  },
  {
    "text": "okay so any talk about Big Data should mention cap uh is everyone here familiar with the cap theorem raise your hand if",
    "start": "1980159",
    "end": "1985799"
  },
  {
    "text": "your you are okay so I'll just summarize it real quickly so cap theorem is a",
    "start": "1985799",
    "end": "1991240"
  },
  {
    "text": "theorem um it's commonly expressed as there are three property the three",
    "start": "1991240",
    "end": "1996480"
  },
  {
    "text": "properties of consistency there are three properties of consistency availability and partition tolerance and you can only have two this is actually a",
    "start": "1996480",
    "end": "2003360"
  },
  {
    "text": "terrible way to describe the cap theorem um CU you can't really it doesn't really makes sense to not have partition",
    "start": "2003360",
    "end": "2009000"
  },
  {
    "text": "tolerance so really cap theorem means that you can only have guaranteed consistency or guaranteed availability",
    "start": "2009000",
    "end": "2015480"
  },
  {
    "text": "you can't have both in the same system now the key word there is guarantee right as long as there are no partitions you'll have both consistency and",
    "start": "2015480",
    "end": "2021880"
  },
  {
    "text": "availability the real question is which one do you sacrifice when you have partitions now you saw that the batch",
    "start": "2021880",
    "end": "2027440"
  },
  {
    "text": "layer is eventually consistent right it's just always a few hours out of date uh but in in a system like the Lambda",
    "start": "2027440",
    "end": "2034000"
  },
  {
    "text": "architecture um you can still make the tradeoff of whether you want full consistency or full availability and",
    "start": "2034000",
    "end": "2039080"
  },
  {
    "text": "that's decided by your real-time layer if your realtime layer chooses consistency your queries will be consistent if it chooses availability",
    "start": "2039080",
    "end": "2045480"
  },
  {
    "text": "your queries will be eventually consistent now another interesting note about the cap theorem is that the cap",
    "start": "2045480",
    "end": "2051398"
  },
  {
    "text": "theorem kind of scares people a lot and one of the main reasons for that is the idea of building an eventually consistent system as being kind of a",
    "start": "2051399",
    "end": "2057560"
  },
  {
    "text": "complex thing to think about right because when you when you use something like Cassandra and you're doing eventual",
    "start": "2057560",
    "end": "2063919"
  },
  {
    "text": "consistency um it becomes hard to update your database right because what happens",
    "start": "2063919",
    "end": "2070800"
  },
  {
    "text": "in an eventually consistent system is that you end up with a situation where you have these Divergent values for the same key uh and then occasionally you",
    "start": "2070800",
    "end": "2077520"
  },
  {
    "text": "need to do this thing called read repair where you as the application developer need to say okay these things diverged",
    "start": "2077520",
    "end": "2083398"
  },
  {
    "text": "how do I merge them back together so I'm consistent again is really complex really easy to get wrong and if you do",
    "start": "2083399",
    "end": "2088599"
  },
  {
    "text": "it wrong in something like Cassandra where that's your primary data store that's a permanent mistake you've now corrupted your database um now you still",
    "start": "2088599",
    "end": "2095960"
  },
  {
    "text": "have to deal with the same problem in this architecture but the nice thing is that if you mess up which you will um at",
    "start": "2095960",
    "end": "2102599"
  },
  {
    "text": "least you don't corrupt your database right cuz the batch layer will correct for any mistakes you make in the real time layer again this is with this goes",
    "start": "2102599",
    "end": "2109599"
  },
  {
    "text": "along with that theme of complexity isolation and making it easier to build robust systems that you can reason",
    "start": "2109599",
    "end": "2116440"
  },
  {
    "start": "2117000",
    "end": "2186000"
  },
  {
    "text": "about there's another interesting property of this system you can do something with this system that you can't really do in a traditional system",
    "start": "2117440",
    "end": "2124280"
  },
  {
    "text": "I call this eventual accuracy now sometimes it's you have some sort of",
    "start": "2124280",
    "end": "2129599"
  },
  {
    "text": "query you're doing where it's hard to compute the exact answer for that query in real time right an example of this is",
    "start": "2129599",
    "end": "2135119"
  },
  {
    "text": "Computing um a unique count uh over some set right",
    "start": "2135119",
    "end": "2140800"
  },
  {
    "text": "the only way to do that in real time is to maintain a set of just maintain everything that set and have that always",
    "start": "2140800",
    "end": "2147960"
  },
  {
    "text": "available and then you add and remove things from that set to update your count right but that can be really expensive because the set can get",
    "start": "2147960",
    "end": "2153160"
  },
  {
    "text": "arbitrarily large what we can do in a system like this is you can compute the exact answer in the batch layer which is",
    "start": "2153160",
    "end": "2159200"
  },
  {
    "text": "easy to do because you're just running functions in all your data and you can compute an approximate answer in the real time layer so for Unique count",
    "start": "2159200",
    "end": "2165240"
  },
  {
    "text": "maybe you'll do something like use a bloom filter to reduce the space requirements and make it easier and more efficient to compute that query and I",
    "start": "2165240",
    "end": "2172000"
  },
  {
    "text": "call this property eventual accuracy because you've made the trade-off to get performance um in the real-time layer",
    "start": "2172000",
    "end": "2178280"
  },
  {
    "text": "but that's not a permanent trade-off because your batch layer will autoc correct and things will become fully accurate over",
    "start": "2178280",
    "end": "2185720"
  },
  {
    "text": "time right and this gets you the Best of Both Worlds of both performance and accuracy and let you maximize the value",
    "start": "2185880",
    "end": "2191359"
  },
  {
    "start": "2186000",
    "end": "2206000"
  },
  {
    "text": "you get out of your data right you can get as much value as you can in real time which could be limited and then when your data has been in your system a",
    "start": "2191359",
    "end": "2197359"
  },
  {
    "text": "long time it goes to the batch layer and then you can extract the full value out of that data and correlate it with other pieces of data and things like",
    "start": "2197359",
    "end": "2205839"
  },
  {
    "start": "2206000",
    "end": "2287000"
  },
  {
    "text": "that okay so I just want to give just an overview of where all the different",
    "start": "2206960",
    "end": "2212119"
  },
  {
    "text": "tools in the space fit into this architecture um so Hadoop is the best system out there for doing the",
    "start": "2212119",
    "end": "2218480"
  },
  {
    "text": "pre-computation of the batch views it's implementation of map ruce it has a distributed file system where you can store all your data it's a very good",
    "start": "2218480",
    "end": "2224440"
  },
  {
    "text": "tool for that uh storm is good for doing the real time views for doing the stream",
    "start": "2224440",
    "end": "2230520"
  },
  {
    "text": "processing um storm is also good for doing the queries so sometimes you have",
    "start": "2230520",
    "end": "2236359"
  },
  {
    "text": "sometimes you can only pre-compute too uh so much and there's still a lot of on the-fly computation you have left like",
    "start": "2236359",
    "end": "2241560"
  },
  {
    "text": "an example of this is you might need to do like a search and merge query which can involve a lot of computation so with",
    "start": "2241560",
    "end": "2247000"
  },
  {
    "text": "something like storm you can paralyze that computation to run our cluster and you can do things like parallel search",
    "start": "2247000",
    "end": "2253000"
  },
  {
    "text": "and merge very easily um tools Like Elephant DV and Voldemort fit into the batch view",
    "start": "2253000",
    "end": "2259640"
  },
  {
    "text": "portion um and I think there's still a lot of room left for innovation in in",
    "start": "2259640",
    "end": "2264720"
  },
  {
    "text": "batch views uh no SQL databases fit into the real-time views this is where you use",
    "start": "2264720",
    "end": "2270319"
  },
  {
    "text": "tools like Cassandra Rea and hbas um and I think the best tool for",
    "start": "2270319",
    "end": "2275640"
  },
  {
    "text": "actually managing your data stream a tool called Kafka which is by LinkedIn it's a very good system for managing",
    "start": "2275640",
    "end": "2281480"
  },
  {
    "text": "streams of data and consuming them in a reliable",
    "start": "2281480",
    "end": "2285920"
  },
  {
    "start": "2287000",
    "end": "2323000"
  },
  {
    "text": "way okay so there's a few interesting properties I want to point out about the land architecture um so because your",
    "start": "2287200",
    "end": "2293720"
  },
  {
    "text": "whole system is based on pure functions of your data um at any point if you need to you can discard all your batch views",
    "start": "2293720",
    "end": "2300800"
  },
  {
    "text": "and all your realtime views and recreate them from scratch in fact they'll actually recreate themselves",
    "start": "2300800",
    "end": "2306280"
  },
  {
    "text": "automatically um this is really good because this gives you the ability to correct mistakes you make via recomputation",
    "start": "2306280",
    "end": "2313560"
  },
  {
    "text": "right so there's only like a few kinds of mistakes you can make right the first kind of mistake is that you write bad data into your data set right you write",
    "start": "2313560",
    "end": "2320560"
  },
  {
    "text": "corrupt data Maybe your schema wasn't restrictive enough so what you can do is you just go in and you remove the bad",
    "start": "2320560",
    "end": "2326560"
  },
  {
    "start": "2323000",
    "end": "2375000"
  },
  {
    "text": "data and then you run your recomputation and then your views will be corrected",
    "start": "2326560",
    "end": "2331760"
  },
  {
    "text": "and things will be back to normal right another mistake you can make is that you actually have a bug in your uh code that",
    "start": "2331760",
    "end": "2337720"
  },
  {
    "text": "produces your views and then again to fix this you just uh fix the bug and you",
    "start": "2337720",
    "end": "2342960"
  },
  {
    "text": "recompute your views and everything is back to normal um and the only other mistake you can make is a is a mistake",
    "start": "2342960",
    "end": "2348000"
  },
  {
    "text": "in your query function um and in that case all you have to do is deploy the corrected version and then everything",
    "start": "2348000",
    "end": "2353960"
  },
  {
    "text": "will be working normally right and as I mentioned before a system like this the data storage",
    "start": "2353960",
    "end": "2359400"
  },
  {
    "text": "layer is optimized independently from the query resolution layer right so this fixes that normalization versus",
    "start": "2359400",
    "end": "2365760"
  },
  {
    "text": "denormalization problem are the best of both worlds of a fully normalized schema as well as views that are completely",
    "start": "2365760",
    "end": "2371280"
  },
  {
    "text": "optimized for your queries and because you can always",
    "start": "2371280",
    "end": "2376520"
  },
  {
    "start": "2375000",
    "end": "2424000"
  },
  {
    "text": "recompute things from scratch this also gives you the flexibility to swap out your batch in real times views as needed",
    "start": "2376520",
    "end": "2382760"
  },
  {
    "text": "right so maybe you have new requirements or there's some new and better system comes out and all you need to do is",
    "start": "2382760",
    "end": "2388720"
  },
  {
    "text": "update your recomputation algorithms to go to a different Target you recompute and now you're on a different set of set",
    "start": "2388720",
    "end": "2394359"
  },
  {
    "text": "of views um and the other probably the most important thing about an architecture like this is that data systems are long",
    "start": "2394359",
    "end": "2402040"
  },
  {
    "text": "lived like we've discussed they last for years which means your needs will change over time and they'll change in very",
    "start": "2402040",
    "end": "2407119"
  },
  {
    "text": "unexpected ways now having a system that's based on Computing functions on all your data means it will support your",
    "start": "2407119",
    "end": "2413520"
  },
  {
    "text": "future needs right it will be powerful enough and flexible enough to support whatever you need to do in the future",
    "start": "2413520",
    "end": "2418680"
  },
  {
    "text": "because nothing is more General than function of all data I think there's still a lot of work",
    "start": "2418680",
    "end": "2425319"
  },
  {
    "start": "2424000",
    "end": "2473000"
  },
  {
    "text": "to be done in this space um I think things can get a lot better I think one of the most interesting things that will",
    "start": "2425319",
    "end": "2430640"
  },
  {
    "text": "happen in the future is an abstraction over batch processing and real-time processing right one abstraction that",
    "start": "2430640",
    "end": "2436240"
  },
  {
    "text": "will do both for you but still has the flexibility to do to do different things in batch in real time when you want to",
    "start": "2436240",
    "end": "2441720"
  },
  {
    "text": "achieve things like eventual accuracy right and I think in the next few years we'll start to see tools like this right",
    "start": "2441720",
    "end": "2448200"
  },
  {
    "text": "I think there's also a lot of more work to do in on those batch in real time views so different kinds of indexing",
    "start": "2448200",
    "end": "2454040"
  },
  {
    "text": "schemes um I think I think reddis is a really interesting system to look at for what can be achieved there and um",
    "start": "2454040",
    "end": "2460480"
  },
  {
    "text": "because reddis exports these data structures and data structure operations and that's essentially exactly what you",
    "start": "2460480",
    "end": "2465640"
  },
  {
    "text": "want for your views I think there's a lot of work left to be done in supporting different kind of data structures for those",
    "start": "2465640",
    "end": "2472560"
  },
  {
    "text": "views right if I want to mention I'm writing a book on this subject uh my",
    "start": "2472560",
    "end": "2477839"
  },
  {
    "start": "2473000",
    "end": "2492000"
  },
  {
    "text": "book goes at a much more reasonable Pace uh than I did in this talk um so if you want to learn more about this stuff um",
    "start": "2477839",
    "end": "2484440"
  },
  {
    "text": "be sure to check that out that's all that I have can I take any",
    "start": "2484440",
    "end": "2490720"
  },
  {
    "text": "questions so the question was I see a performance problem because as all data",
    "start": "2491440",
    "end": "2496480"
  },
  {
    "start": "2492000",
    "end": "2699000"
  },
  {
    "text": "grows um right your your your batch algorithms will run slower how do you get through",
    "start": "2496480",
    "end": "2502079"
  },
  {
    "text": "them in a reasonable time so there's two answers to that so first of all um everything here can be implemented with",
    "start": "2502079",
    "end": "2508200"
  },
  {
    "text": "fully scalable Technologies right Hadoop is a fully scalable system so you can scale your hoodo cluster as you need for your data uh the second important point",
    "start": "2508200",
    "end": "2516839"
  },
  {
    "text": "is that that so I defined the batch layer is just running functions in all your data that's definitely the place you want to start because you need that",
    "start": "2516839",
    "end": "2523640"
  },
  {
    "text": "so that you can recompute things when mistakes are made or when you need to change things uh but it's actually possible to",
    "start": "2523640",
    "end": "2530200"
  },
  {
    "text": "incrementalization and you merge that into your batch views um you can do that",
    "start": "2534040",
    "end": "2539319"
  },
  {
    "text": "and I think that's definitely something you should do if you need that performance but the key thing is is that you always have that recomputation step",
    "start": "2539319",
    "end": "2545160"
  },
  {
    "text": "that you can fall back on when you need to uh incremental incrementalization the batch layer does make it much more",
    "start": "2545160",
    "end": "2550200"
  },
  {
    "text": "efficient so question was I a more comment but it was saying that when when",
    "start": "2550200",
    "end": "2555480"
  },
  {
    "text": "there's data you need precisely in real time the system is limited so that's",
    "start": "2555480",
    "end": "2561480"
  },
  {
    "text": "that's the last that's the last part yeah but that's what the real time views are for right",
    "start": "2561480",
    "end": "2568400"
  },
  {
    "text": "so right like the way people build systems now like I said is essentially only the real time portion based on",
    "start": "2568400",
    "end": "2574440"
  },
  {
    "text": "incremental algorithms and mutable databases so that's still in here right so you can still do that stuff as you",
    "start": "2574440",
    "end": "2579839"
  },
  {
    "text": "need but you have this batch layer backing it which which you know as you saw changes things um what kind of data",
    "start": "2579839",
    "end": "2585880"
  },
  {
    "text": "store would you use on the the back back end where you store it in relations or",
    "start": "2585880",
    "end": "2591400"
  },
  {
    "text": "at least in a hard schema I mean for all data yeah oh so so that is just files in",
    "start": "2591400",
    "end": "2597240"
  },
  {
    "text": "a distributed file system it's you don't have to index it because you're just running functions on it um so you can so",
    "start": "2597240",
    "end": "2603960"
  },
  {
    "text": "like I just store in distribute file system I built some tools to make that that easy you just all you're doing is a",
    "start": "2603960",
    "end": "2609640"
  },
  {
    "text": "pending data it's a mutable data so all you're doing a pending data and um I Define my schema using Thrift and um",
    "start": "2609640",
    "end": "2617359"
  },
  {
    "text": "that's it yeah so you can uh normalize it to your heart's content the way I actually model my data is using a graph",
    "start": "2617359",
    "end": "2625599"
  },
  {
    "text": "schema again it's not index it's just represented as a graph so all my data is represented as nodes properties and",
    "start": "2625599",
    "end": "2631160"
  },
  {
    "text": "edges um and I find this to be just an incredibly flexible way to evolve the scheme over time it's very easy to add",
    "start": "2631160",
    "end": "2636880"
  },
  {
    "text": "new properties and edges and things like that oh you mentioned not pering update",
    "start": "2636880",
    "end": "2642200"
  },
  {
    "text": "or operations but instead saving all data as some sort of event right correct",
    "start": "2642200",
    "end": "2650040"
  },
  {
    "text": "didn't call it event sourcing correct yeah so so it's the same idea as event sourcing I think the so event sourcing",
    "start": "2650040",
    "end": "2656359"
  },
  {
    "text": "is a my question would be is there any different from what you absolutely yeah",
    "start": "2656359",
    "end": "2661520"
  },
  {
    "text": "yeah so yeah it's very similar to event sourcing um I think event sourcing so",
    "start": "2661520",
    "end": "2666640"
  },
  {
    "text": "idea event sourcing is you store your events right but it still has the idea",
    "start": "2666640",
    "end": "2671720"
  },
  {
    "text": "of you store the events so that you can reconstruct your current view of the world in this case we're just saying we",
    "start": "2671720",
    "end": "2677440"
  },
  {
    "text": "don't even care about what the current view of the world is we just sore events like that's my first class data that I'm never going to replace but it's still",
    "start": "2677440",
    "end": "2683880"
  },
  {
    "text": "it's very very similar it's the same idea and you get all the same benefits because it's all based on mutability",
    "start": "2683880",
    "end": "2689960"
  },
  {
    "text": "after thank you very much thank you",
    "start": "2689960",
    "end": "2695000"
  }
]