[
  {
    "start": "0",
    "end": "83000"
  },
  {
    "text": "thanks for joining me everybody I know this is probably the last session of the day um uh my name is V and I'm going to",
    "start": "11880",
    "end": "19960"
  },
  {
    "text": "be talking about this really interesting topic at least to me um today I am an",
    "start": "19960",
    "end": "26039"
  },
  {
    "text": "AIML Solutions architect uh I've been with doing AIML for the past two years",
    "start": "26039",
    "end": "32200"
  },
  {
    "text": "two and a half years now I am also an ex serverless Solutions architect um so I",
    "start": "32200",
    "end": "37960"
  },
  {
    "text": "used to be in the same team as my colleagues here um and I have two two",
    "start": "37960",
    "end": "44719"
  },
  {
    "text": "kids who keep me on my toes all the time I am also learning viin right now um",
    "start": "44719",
    "end": "51680"
  },
  {
    "text": "because I wanted to lead by example for my son who's also learning violin so I thought you know I should do it too if I",
    "start": "51680",
    "end": "58399"
  },
  {
    "text": "ask my son to do it and um then I'm just a serverless animl",
    "start": "58399",
    "end": "63680"
  },
  {
    "text": "Enthusiast I love building machine learning applications and I love using serverless to build those machine",
    "start": "63680",
    "end": "70159"
  },
  {
    "text": "learning applications all right uh so that's",
    "start": "70159",
    "end": "76080"
  },
  {
    "text": "about me so before I I start uh talking about Ai and the different kinds of AI",
    "start": "76080",
    "end": "83360"
  },
  {
    "start": "83000",
    "end": "456000"
  },
  {
    "text": "and all of that quick show off hands um how many of you have used a large",
    "start": "83360",
    "end": "89079"
  },
  {
    "text": "language model for personal use okay it's a good number how many of",
    "start": "89079",
    "end": "96320"
  },
  {
    "text": "you are building applications using a large language model or a image based",
    "start": "96320",
    "end": "103200"
  },
  {
    "text": "model awesome cool thank you all right um so before I get into",
    "start": "103200",
    "end": "110000"
  },
  {
    "text": "these the serverless aspects of um uh building applications um let's talk about the",
    "start": "110000",
    "end": "116600"
  },
  {
    "text": "phases of AI itself right so when whenever you want to build a machine",
    "start": "116600",
    "end": "121680"
  },
  {
    "text": "learning model these are the three phases that generally you go through uh the first is a design phase so data",
    "start": "121680",
    "end": "128200"
  },
  {
    "text": "scientists in the design phase pick up large amounts of you know data label",
    "start": "128200",
    "end": "135280"
  },
  {
    "text": "them um do feature engineering on them do data engineering and feature engineering on them figure out which",
    "start": "135280",
    "end": "142200"
  },
  {
    "text": "data is useful um and do data Transformations on them and all of that",
    "start": "142200",
    "end": "148800"
  },
  {
    "text": "and then all also figure out what kind of machine learning algorithm works best",
    "start": "148800",
    "end": "154160"
  },
  {
    "text": "for the task at hand what what what does the machine learning algorithm really predict what should it predict and what",
    "start": "154160",
    "end": "160319"
  },
  {
    "text": "kind of algorithms work best and design those algorithms as well if neural network should be us used here or",
    "start": "160319",
    "end": "167360"
  },
  {
    "text": "regression algorithms or classification algorithms should be used what kind of algorithms right so all of that is done",
    "start": "167360",
    "end": "173480"
  },
  {
    "text": "in the design phase and then comes the training phase wherein they take the um",
    "start": "173480",
    "end": "179080"
  },
  {
    "text": "data feature engineered data and then use the algorithm to train the machine",
    "start": "179080",
    "end": "184720"
  },
  {
    "text": "learning model and this is an intensive process depending on the size of the data and the size of the algorithm",
    "start": "184720",
    "end": "191040"
  },
  {
    "text": "itself can span multiple CPUs multiple gpus you have to do distributor training",
    "start": "191040",
    "end": "197040"
  },
  {
    "text": "and all of that and then comes the evaluation phase which I've not really",
    "start": "197040",
    "end": "202239"
  },
  {
    "text": "mentioned here but it's really you know the design and the training and the evaluation is sort of like a loop right",
    "start": "202239",
    "end": "208439"
  },
  {
    "text": "you train the model you evaluate the model model test data set and then if",
    "start": "208439",
    "end": "214760"
  },
  {
    "text": "you know figure out if the model has good accuracy for the test data set if not go back to design phase and get",
    "start": "214760",
    "end": "221239"
  },
  {
    "text": "maybe get more data or you know do different kinds of data transformation or maybe use a different algorithm right",
    "start": "221239",
    "end": "227599"
  },
  {
    "text": "so it's sort of like a loop and then once your your the data scientist figures out okay this is the best model",
    "start": "227599",
    "end": "234280"
  },
  {
    "text": "or the algorithm for my task at hand then comes the inference phase right",
    "start": "234280",
    "end": "240519"
  },
  {
    "text": "um an inference phase is where just you're using that model to um predict",
    "start": "240519",
    "end": "247079"
  },
  {
    "text": "for unseen data like the production data um you predict what what happens or the",
    "start": "247079",
    "end": "253040"
  },
  {
    "text": "make the prediction to the Unseen with the Unseen data but for most of",
    "start": "253040",
    "end": "258480"
  },
  {
    "text": "[Music] us this inference is what matters as an application developer you know the the",
    "start": "258480",
    "end": "266080"
  },
  {
    "text": "model building and the model training all of that does not matter the INF is what matters to us as application",
    "start": "266080",
    "end": "271840"
  },
  {
    "text": "developers and consumers right and we most of us will use that inference API",
    "start": "271840",
    "end": "277720"
  },
  {
    "text": "to just call the API the model is a blackbox and we get the response or the prediction from the",
    "start": "277720",
    "end": "284440"
  },
  {
    "text": "API and and this is just an endpoint for us for for 99% of us it's just an",
    "start": "284440",
    "end": "290919"
  },
  {
    "text": "endpoint as I said we just call the API and we get the predictions",
    "start": "290919",
    "end": "296320"
  },
  {
    "text": "back all right but how how effectively you use",
    "start": "296320",
    "end": "301600"
  },
  {
    "text": "this endpoint comes down to two things and um my colleague Eric and I were",
    "start": "301600",
    "end": "307039"
  },
  {
    "text": "talking about this and we both came up with this that you know how effective",
    "start": "307039",
    "end": "312520"
  },
  {
    "text": "you are in using these endpoints comes down to data orchestration and choreography um of that production data",
    "start": "312520",
    "end": "320720"
  },
  {
    "text": "and then doing prompt engineering if you're doing uh generative AI so I'm going to talk about both of",
    "start": "320720",
    "end": "326759"
  },
  {
    "text": "these things um first let's talk about why prompt engineering and if you've used the large language model you",
    "start": "326759",
    "end": "332960"
  },
  {
    "text": "probably know this right so I went to this uh model within uh Amazon Bedrock",
    "start": "332960",
    "end": "340440"
  },
  {
    "text": "which is our uh new service which is also a serverless service um for",
    "start": "340440",
    "end": "345800"
  },
  {
    "text": "accessing these Foundation models or large language models through apis and why prompt engineering is important so",
    "start": "345800",
    "end": "353919"
  },
  {
    "text": "um so with to to these models you ask questions or you ask natural language questions uh and then it gives you",
    "start": "353919",
    "end": "360680"
  },
  {
    "text": "answers right so I just went and asked this uh model U within bedrock and I as",
    "start": "360680",
    "end": "365759"
  },
  {
    "text": "what what are black holes right and it gave me a very nice descriptive answer",
    "start": "365759",
    "end": "371599"
  },
  {
    "text": "and this is this is okay for me right uh if I'm developing an application which",
    "start": "371599",
    "end": "377039"
  },
  {
    "text": "is just answering you know uh taking users questions and giving me answers to the you giving the answers back to the",
    "start": "377039",
    "end": "383800"
  },
  {
    "text": "users then this is totally fine but what if I'm developing this application to a",
    "start": "383800",
    "end": "389520"
  },
  {
    "text": "bunch of astrophysicists right and and they ask",
    "start": "389520",
    "end": "395080"
  },
  {
    "text": "these questions like an internal application for for a company which is um a research company based on physics",
    "start": "395080",
    "end": "402680"
  },
  {
    "text": "and um astronomy and and if they are such questions these these kind of answers are probably not enough right so",
    "start": "402680",
    "end": "411720"
  },
  {
    "text": "now I went and asked that ask the same question in a different way you are an experienced",
    "start": "411720",
    "end": "418000"
  },
  {
    "text": "astrophysicist now tell me what black holes are right so I asked the large",
    "start": "418000",
    "end": "423160"
  },
  {
    "text": "language model to act like it's an astrophysic an experienced astrophysic and then tell me what black holes are",
    "start": "423160",
    "end": "429720"
  },
  {
    "text": "and look at the detail that it gave me right so that's why prompt engineering is so important on when you're",
    "start": "429720",
    "end": "436440"
  },
  {
    "text": "developing uh generative AI applications trying out different prompts uh to see",
    "start": "436440",
    "end": "442199"
  },
  {
    "text": "which give which prompt gives you the best answer and having a prompt catalog probably even while you develop your",
    "start": "442199",
    "end": "448639"
  },
  {
    "text": "applications um is a good practice right so that's",
    "start": "448639",
    "end": "453680"
  },
  {
    "text": "why you know uh prompt engineering is so important right um next let look at um",
    "start": "453680",
    "end": "462560"
  },
  {
    "start": "456000",
    "end": "716000"
  },
  {
    "text": "how we choose the right kind of AI when you're as an application developer you're",
    "start": "462560",
    "end": "467680"
  },
  {
    "text": "developing um applications geni or AI applications so uh broadly I mean",
    "start": "467680",
    "end": "475440"
  },
  {
    "text": "currently today I classify the AI models into two two categories for for our",
    "start": "475440",
    "end": "481800"
  },
  {
    "text": "practical purposes right the first is the deterministic kind of AI models and",
    "start": "481800",
    "end": "488520"
  },
  {
    "text": "these are the the traditional models right which was you know where data",
    "start": "488520",
    "end": "493599"
  },
  {
    "text": "scientists took data sets labeled those data sets and trained a machine learning",
    "start": "493599",
    "end": "499360"
  },
  {
    "text": "model specifically for a particular task in mind say for example you wanted the",
    "start": "499360",
    "end": "504400"
  },
  {
    "text": "machine learning model to classify documents and so data data scientists",
    "start": "504400",
    "end": "510840"
  },
  {
    "text": "would take labeled documents and provide the labels to the model and the model",
    "start": "510840",
    "end": "517039"
  },
  {
    "text": "would learn okay this kind of document means this is the label and then now it's able to train on that labeled data",
    "start": "517039",
    "end": "525080"
  },
  {
    "text": "set and then be and then can predict okay this is this kind of document right",
    "start": "525080",
    "end": "530120"
  },
  {
    "text": "so that was a deterministic nature that wherein when you ask the when you ask",
    "start": "530120",
    "end": "536920"
  },
  {
    "text": "the model a question a every time you ask the question it gives you the same answer so it's very very",
    "start": "536920",
    "end": "543519"
  },
  {
    "text": "predictable right there's the new kind of AI model which is more",
    "start": "543519",
    "end": "548560"
  },
  {
    "text": "probabilistic St stochastic if you want to be nerdy um stochastic means",
    "start": "548560",
    "end": "553800"
  },
  {
    "text": "probabilistic right where in this new class of models",
    "start": "553800",
    "end": "559399"
  },
  {
    "text": "it's sorry",
    "start": "559399",
    "end": "563680"
  },
  {
    "text": "oops okay the new class of models which is the probabilistic model",
    "start": "564920",
    "end": "570200"
  },
  {
    "text": "every time you ask the question the answer is very different right and there are use cases",
    "start": "570200",
    "end": "576000"
  },
  {
    "text": "where you need deterministic models and there are use cases where you can do with probabilistic model you probably",
    "start": "576000",
    "end": "582519"
  },
  {
    "text": "need a probabilistic or a stochastic model and there are use cases where you can do these deterministic tasks with",
    "start": "582519",
    "end": "589720"
  },
  {
    "text": "the probabilistic model as well with a large language model as well so how do you choose",
    "start": "589720",
    "end": "595360"
  },
  {
    "text": "right so let's see how actually these um geni models",
    "start": "595360",
    "end": "601040"
  },
  {
    "text": "work so why I say these gen models are pro probabilistic is this right so geni",
    "start": "601040",
    "end": "607880"
  },
  {
    "text": "model is trying to always predict what the next token is given a sequence of",
    "start": "607880",
    "end": "613720"
  },
  {
    "text": "tokens right and and um a token is a set of",
    "start": "613720",
    "end": "618880"
  },
  {
    "text": "letters um and multiple tokens make up words in the language right so if I say",
    "start": "618880",
    "end": "625160"
  },
  {
    "text": "if I give this as a prompt to the geni model today I went to the and what the geni model is doing or what",
    "start": "625160",
    "end": "632920"
  },
  {
    "text": "the large language model is doing is it Scouts through its vocabulary of all",
    "start": "632920",
    "end": "638360"
  },
  {
    "text": "tokens and then gives a prob probability score for each token um what uh what",
    "start": "638360",
    "end": "645279"
  },
  {
    "text": "should be go what token should go next in this given this sequence of words and it'll pick based on some model",
    "start": "645279",
    "end": "651560"
  },
  {
    "text": "parameters that you set um what the most probable tokens are given the sequence",
    "start": "651560",
    "end": "656680"
  },
  {
    "text": "of words and we call this Auto regress RVE models that's the auto regressive nature and that's how we as humans think",
    "start": "656680",
    "end": "663079"
  },
  {
    "text": "as well given a set of uh words we are always trying to uh when we are when we",
    "start": "663079",
    "end": "670200"
  },
  {
    "text": "speaking we're always trying to think okay what's the next word what's the next word right that's how these um llms",
    "start": "670200",
    "end": "676959"
  },
  {
    "text": "are also generating um text right so it does this you know it tries to given",
    "start": "676959",
    "end": "683360"
  },
  {
    "text": "these sequence again it so it can pick store today I went to the store or today I went to the mall or today I went to",
    "start": "683360",
    "end": "689839"
  },
  {
    "text": "the office definitely not tomato because that does not make sense so it'll have a lower probability score right so now",
    "start": "689839",
    "end": "696480"
  },
  {
    "text": "after it pick say today I went to the store the next token it's probably will pick is two buy clothes so it keeps you",
    "start": "696480",
    "end": "705320"
  },
  {
    "text": "know predicting what the next token is and it does it repeatedly until the whole text is generated or until we ask",
    "start": "705320",
    "end": "712600"
  },
  {
    "text": "it to stop all right so now let's look at you",
    "start": "712600",
    "end": "720360"
  },
  {
    "start": "716000",
    "end": "905000"
  },
  {
    "text": "know we looked at probabilistic and deterministic models how can we solve",
    "start": "720360",
    "end": "725720"
  },
  {
    "text": "real world problems using AI so I'm going to pick this use case um which is",
    "start": "725720",
    "end": "731560"
  },
  {
    "text": "a very very common use case across a lot of Industries transcribe um uh audio or video and then",
    "start": "731560",
    "end": "741120"
  },
  {
    "text": "extract the text out of that audio video and then summarize it right you want to",
    "start": "741120",
    "end": "746560"
  },
  {
    "text": "create summary um some examp examples of these use case across the industries are say if you want to do uh you have",
    "start": "746560",
    "end": "753680"
  },
  {
    "text": "medical transcriptions between um a doctor and a patient um or you have",
    "start": "753680",
    "end": "759800"
  },
  {
    "text": "medical reports that you want to extract the text out of it and then summarize it",
    "start": "759800",
    "end": "765079"
  },
  {
    "text": "and put it in a database right um the call center right you have conversations",
    "start": "765079",
    "end": "770639"
  },
  {
    "text": "between the call center agent and the customer you want to transcribe it convert it to text and summarize the the",
    "start": "770639",
    "end": "779440"
  },
  {
    "text": "call and maybe take Downstream actions on those right um in the legal in the",
    "start": "779440",
    "end": "786000"
  },
  {
    "text": "financial domains there are policy documents financial documents that you want to summarize and store them off so",
    "start": "786000",
    "end": "792240"
  },
  {
    "text": "that the downstream applications can use them video transcription as well right so across Industries this is a very",
    "start": "792240",
    "end": "798800"
  },
  {
    "text": "common use case and why is this important because data useful information and data is",
    "start": "798800",
    "end": "806000"
  },
  {
    "text": "stored in all of these non referenceable non unsearchable formats the scanned",
    "start": "806000",
    "end": "811959"
  },
  {
    "text": "images the audio recordings or the PDF docs it has a lot of data and we want to",
    "start": "811959",
    "end": "817680"
  },
  {
    "text": "make it searchable and use that in our applications to make business decisions",
    "start": "817680",
    "end": "824760"
  },
  {
    "text": "right let's see how we can U use AI to do this so let's build an application um",
    "start": "824760",
    "end": "831240"
  },
  {
    "text": "to do that to to solve this problem we want to get all of that information that's stored in unsearchable format and",
    "start": "831240",
    "end": "837839"
  },
  {
    "text": "make it available to to our applications um so these are the requirements um we want to convert the",
    "start": "837839",
    "end": "844880"
  },
  {
    "text": "current format which is audio uh into text and uh we want to be able to",
    "start": "844880",
    "end": "851079"
  },
  {
    "text": "extract the text and then um summarize it and then save it to a database so",
    "start": "851079",
    "end": "857680"
  },
  {
    "text": "that it's available um for other",
    "start": "857680",
    "end": "862279"
  },
  {
    "text": "applications and what are the parameters for this application that I want to build we want to be cost efficient of",
    "start": "864839",
    "end": "872519"
  },
  {
    "text": "course uh we want it to be accurate um the reason I say this because you know",
    "start": "872519",
    "end": "878959"
  },
  {
    "text": "as you saw the probabilistic models give me outputs every time I ask it to",
    "start": "878959",
    "end": "884560"
  },
  {
    "text": "generate something it gives me a completely different output so we want to be accurate um we want to be near real time",
    "start": "884560",
    "end": "891680"
  },
  {
    "text": "or daily as soon as the files are available we want to kick off the process right um or at least you know do",
    "start": "891680",
    "end": "899880"
  },
  {
    "text": "it on a daily basis on a scheduled basis all right so let's see how we can",
    "start": "899880",
    "end": "907360"
  },
  {
    "start": "905000",
    "end": "1086000"
  },
  {
    "text": "apply AI to this real world problem right um so here are the services that",
    "start": "907360",
    "end": "914519"
  },
  {
    "text": "um I think potentially that could be used to um to solve this use case the",
    "start": "914519",
    "end": "920519"
  },
  {
    "text": "first service is Amazon teex truct uh Amazon textract is an apid driven AI",
    "start": "920519",
    "end": "927160"
  },
  {
    "text": "service which is able to extract text out of PDF documents out of scanned",
    "start": "927160",
    "end": "933519"
  },
  {
    "text": "images right it has deterministic AI models behind the service um which can",
    "start": "933519",
    "end": "941880"
  },
  {
    "text": "extract it's essentially an OCR technology optical character recognition technology which is able to text extract",
    "start": "941880",
    "end": "947959"
  },
  {
    "text": "text out of documents so that works for me for PDF documents the next service is",
    "start": "947959",
    "end": "954040"
  },
  {
    "text": "Amazon transcribe this is also another AI service which has the deterministic",
    "start": "954040",
    "end": "959360"
  },
  {
    "text": "models behind the scene which is um able to transcribe convert my audio and to",
    "start": "959360",
    "end": "966600"
  },
  {
    "text": "text give me take the audio format and then give me text right so that works",
    "start": "966600",
    "end": "971720"
  },
  {
    "text": "for my audio files even video files right the next two Services is Amazon",
    "start": "971720",
    "end": "978519"
  },
  {
    "text": "bedrock and Amazon sagemaker um while Amazon textt track",
    "start": "978519",
    "end": "983560"
  },
  {
    "text": "and transcribe work for me to convert audio uh to text on extract text from",
    "start": "983560",
    "end": "989480"
  },
  {
    "text": "PDF PDF documents I need a probabilistic AI service or a probabilistic model",
    "start": "989480",
    "end": "997519"
  },
  {
    "text": "which can summarize the text for me in a very suent manner right um and I need to",
    "start": "997519",
    "end": "1004319"
  },
  {
    "text": "use a probabilistic model because it needs to do a an abstractive summary not",
    "start": "1004319",
    "end": "1009720"
  },
  {
    "text": "really just extract what is there it needs to be abstractive it needs to rephrase so we need a the power of",
    "start": "1009720",
    "end": "1017639"
  },
  {
    "text": "generative AI there um so that's why we can either use",
    "start": "1017639",
    "end": "1022920"
  },
  {
    "text": "Bedrock or sag maker right an Amazon bedro um is um again an apid driven",
    "start": "1022920",
    "end": "1029640"
  },
  {
    "text": "serverless service which which will give you um choice of using Foundation models",
    "start": "1029640",
    "end": "1037319"
  },
  {
    "text": "uh many different Foundation models um so uh from many uh from many of the",
    "start": "1037319",
    "end": "1042959"
  },
  {
    "text": "startups that we offer right and and sagemaker is another uh machine learning service which will allow you to train",
    "start": "1042959",
    "end": "1050440"
  },
  {
    "text": "your own machine learning model um and you know go all the full cycle of machine learning right design uh build",
    "start": "1050440",
    "end": "1058440"
  },
  {
    "text": "uh and train the machine learning model and even host the machine learning model right if you want to build your own model sagemaker is probably the way to",
    "start": "1058440",
    "end": "1065799"
  },
  {
    "text": "go build your own model and host the machine learning model there on sag maker or if you want to use a",
    "start": "1065799",
    "end": "1071360"
  },
  {
    "text": "pre-trained model just use it in um in terms of apis then Bedrock is a service",
    "start": "1071360",
    "end": "1076799"
  },
  {
    "text": "for you",
    "start": "1076799",
    "end": "1080399"
  },
  {
    "text": "just going to use this okay so just a little note on",
    "start": "1085159",
    "end": "1090200"
  },
  {
    "start": "1086000",
    "end": "1229000"
  },
  {
    "text": "Amazon Bedrock um what are the benefits um because Bedrock is serverless and you",
    "start": "1090200",
    "end": "1096320"
  },
  {
    "text": "can really test out several different Foundation models using that single API",
    "start": "1096320",
    "end": "1101640"
  },
  {
    "text": "that Bedrock provides you know um it's really easy and um to test out all of",
    "start": "1101640",
    "end": "1107480"
  },
  {
    "text": "the foundation mod models without managing the infrastructure so the Amazon Bedrock service is hosting the",
    "start": "1107480",
    "end": "1113080"
  },
  {
    "text": "large language model the pre-trained large language model and you just do inference using the Bedrock apis and",
    "start": "1113080",
    "end": "1119520"
  },
  {
    "text": "using the same API you're able to switch between the foundation models and see which which works best for your use case",
    "start": "1119520",
    "end": "1126960"
  },
  {
    "text": "because each of these Foundation models are trained very differently used very different data sets for their training",
    "start": "1126960",
    "end": "1134039"
  },
  {
    "text": "so the the way they answer the same question is very very different and you",
    "start": "1134039",
    "end": "1139240"
  },
  {
    "text": "really need to test out which model works for your use case uh by issuing",
    "start": "1139240",
    "end": "1145559"
  },
  {
    "text": "those prompts to those Foundation models and see which which model's language is",
    "start": "1145559",
    "end": "1150720"
  },
  {
    "text": "good for your use case right um so you can choose several Foundation models um",
    "start": "1150720",
    "end": "1156559"
  },
  {
    "text": "from AI 21 Labs we have the Jurassic family of models anthropic um Cloud",
    "start": "1156559",
    "end": "1162559"
  },
  {
    "text": "models which is very very popular um we have stability AI uh St diffusion model",
    "start": "1162559",
    "end": "1169200"
  },
  {
    "text": "if you want to do uh image uh text to image u meaning generate image based on",
    "start": "1169200",
    "end": "1174760"
  },
  {
    "text": "the text prompts that you give um we also have um the option of choosing",
    "start": "1174760",
    "end": "1180320"
  },
  {
    "text": "meta's llama family of models llama and llama 2 uh and we also have Amazon own",
    "start": "1180320",
    "end": "1186360"
  },
  {
    "text": "trained models called the Amazon Titan family of models and uh with Amazon Titan you have the choice of you know if",
    "start": "1186360",
    "end": "1193919"
  },
  {
    "text": "you want to just do text to text there's Titan text or if you want to generate m embeddings then we have the Titan",
    "start": "1193919",
    "end": "1200080"
  },
  {
    "text": "embeddings model as well and with bedrock you're able to privately also customize uh the",
    "start": "1200080",
    "end": "1206960"
  },
  {
    "text": "foundation models if you want to say perform fine-tuning of the models then you can do that as well and of course",
    "start": "1206960",
    "end": "1214840"
  },
  {
    "text": "while staying safe and secure your data is never shared with the model providers your prompts are secure uh we don't take",
    "start": "1214840",
    "end": "1221880"
  },
  {
    "text": "the data and you know uh take the data to train the models further so your data",
    "start": "1221880",
    "end": "1226960"
  },
  {
    "text": "is very secure all right let's go back to our application now so here's the goal of",
    "start": "1226960",
    "end": "1234200"
  },
  {
    "start": "1229000",
    "end": "1496000"
  },
  {
    "text": "our application um we have the uh call recordings or the audio audio files or",
    "start": "1234200",
    "end": "1241280"
  },
  {
    "text": "the documents available in an S3 bucket and we want to generate the summary for",
    "start": "1241280",
    "end": "1247640"
  },
  {
    "text": "those and put it in a database like Dynamo DB all",
    "start": "1247640",
    "end": "1255559"
  },
  {
    "text": "right all right so let's see how we can use our AI services to meet the",
    "start": "1257120",
    "end": "1265080"
  },
  {
    "text": "requirements so I said we want to use textract and transcribe first of all to extract so our our going after",
    "start": "1265080",
    "end": "1272840"
  },
  {
    "text": "requirements so the first one was to convert the current format to text right",
    "start": "1272840",
    "end": "1278200"
  },
  {
    "text": "and if the current format is an audio file we can use Amazon transcribe to First transcribe the audio to text and",
    "start": "1278200",
    "end": "1285400"
  },
  {
    "text": "then or if it's a PDF document then we can send it to textract and the textract",
    "start": "1285400",
    "end": "1290559"
  },
  {
    "text": "is able to uh extract the text from that PDF or image document for",
    "start": "1290559",
    "end": "1297559"
  },
  {
    "text": "us right then the next requirement is we want to summarize the text and as I said",
    "start": "1297559",
    "end": "1303600"
  },
  {
    "text": "we can use either Bedrock or Amazon sagemaker to do that and um how would",
    "start": "1303600",
    "end": "1309360"
  },
  {
    "text": "you choose between Bedrock or sagemaker um that would really depend on if you want to use the foundation model if you",
    "start": "1309360",
    "end": "1316559"
  },
  {
    "text": "don't want to manage the infrastructure uh but just use the pre-train model through an API then you would use",
    "start": "1316559",
    "end": "1322559"
  },
  {
    "text": "Bedrock but if you want to host the model yourself and manage the",
    "start": "1322559",
    "end": "1328200"
  },
  {
    "text": "infrastructure and figure out you know um uh uh which which instance you want",
    "start": "1328200",
    "end": "1334000"
  },
  {
    "text": "to host it on and all of that then Sage maker is a way to go um also the choice of models uh are different so Bedrock",
    "start": "1334000",
    "end": "1341799"
  },
  {
    "text": "has uh certain uh Foundation models available um but if you want to do like",
    "start": "1341799",
    "end": "1347320"
  },
  {
    "text": "open source models from hugging phas then Sage maker has the um the hugging",
    "start": "1347320",
    "end": "1353159"
  },
  {
    "text": "face ecosystem uh it is integrated with hugging phas where you can pick Foundation models from hugging face",
    "start": "1353159",
    "end": "1358679"
  },
  {
    "text": "mostly open source models and um the next requirement is",
    "start": "1358679",
    "end": "1364080"
  },
  {
    "text": "save the text to database um we will use Dynamo DB table again Dynamo DB um is a",
    "start": "1364080",
    "end": "1370120"
  },
  {
    "text": "no SQL database and fits into our serverless nature of um building the",
    "start": "1370120",
    "end": "1376960"
  },
  {
    "text": "application all right um so let's see if we are um staying within our parameters",
    "start": "1376960",
    "end": "1383960"
  },
  {
    "text": "here right so what was our parameter we want to be cost efficient um and because we're using all",
    "start": "1383960",
    "end": "1391600"
  },
  {
    "text": "apid driven um services and serverless services uh you're going to be charged",
    "start": "1391600",
    "end": "1397960"
  },
  {
    "text": "only when you use the apis right so um it's going to be cost efficient for you",
    "start": "1397960",
    "end": "1405000"
  },
  {
    "text": "because you're not paying when you're not using the models you're only paying when when you send in",
    "start": "1405000",
    "end": "1410640"
  },
  {
    "text": "the request to the models and accuracy right we want it to",
    "start": "1410640",
    "end": "1418039"
  },
  {
    "text": "be accurate with the right prompt engineering um and you know testing out",
    "start": "1418039",
    "end": "1424880"
  },
  {
    "text": "the several different prompt engineering techniques we can stay accurate we can",
    "start": "1424880",
    "end": "1430640"
  },
  {
    "text": "develop an a very accurate summary of the document with the right prompt engineering techniques and so bedrock is",
    "start": "1430640",
    "end": "1438159"
  },
  {
    "text": "going to help us do that and the third requirement is we",
    "start": "1438159",
    "end": "1445640"
  },
  {
    "text": "want to stay near real time meaning as soon as the documents or the audio files",
    "start": "1445640",
    "end": "1451159"
  },
  {
    "text": "are available we want to kick off this process um and have the summary ready in",
    "start": "1451159",
    "end": "1458480"
  },
  {
    "text": "our table right or we want to be able to do it on a daily basis or on a scheduled",
    "start": "1458480",
    "end": "1466679"
  },
  {
    "text": "uh basis so how are we going to meet that so and also you know how are we",
    "start": "1466679",
    "end": "1474000"
  },
  {
    "text": "going to meet that and also uh pass this data uh data through these services so",
    "start": "1474000",
    "end": "1481520"
  },
  {
    "text": "that the data is orchestrated right and how this data is",
    "start": "1481520",
    "end": "1486679"
  },
  {
    "text": "orchestrated determines how fast the data is moving through these Services uh",
    "start": "1486679",
    "end": "1492799"
  },
  {
    "text": "but still staying cost efficient right so that that's why data",
    "start": "1492799",
    "end": "1499440"
  },
  {
    "start": "1496000",
    "end": "1618000"
  },
  {
    "text": "orchestration is important how we orchestrate data to move through these services to stay cost efficient",
    "start": "1499440",
    "end": "1506799"
  },
  {
    "text": "determines how cost effective we are right so for that I'm going to use AWS",
    "start": "1506799",
    "end": "1513480"
  },
  {
    "text": "step functions um Step functions is as you all probably know is our low code no",
    "start": "1513480",
    "end": "1519320"
  },
  {
    "text": "code uh visual workflow service which helps me create um it automation",
    "start": "1519320",
    "end": "1525440"
  },
  {
    "text": "workflows machine learning workflows uh any kind of workflows that I need for",
    "start": "1525440",
    "end": "1530520"
  },
  {
    "text": "my applications right so with step",
    "start": "1530520",
    "end": "1535559"
  },
  {
    "text": "functions I create this step function workflow which gets invoked as soon as",
    "start": "1535559",
    "end": "1541200"
  },
  {
    "text": "the items or the audio files or the PDF documents are loaded into my S3 bucket",
    "start": "1541200",
    "end": "1548440"
  },
  {
    "text": "I'm going to kick off a step function workflow and that step function workflow is going to take me through the several",
    "start": "1548440",
    "end": "1554960"
  },
  {
    "text": "different services and finally it's going to call the Bedrock API um to um give me the summary and the",
    "start": "1554960",
    "end": "1564240"
  },
  {
    "text": "good thing about step functions verlo it has direct integration to all of these Services um to Amazon textract and",
    "start": "1564240",
    "end": "1571960"
  },
  {
    "text": "Amazon transcribed and to Amazon Sage maker and to Dynamo DB so you don't have",
    "start": "1571960",
    "end": "1577399"
  },
  {
    "text": "to write a lot of code to make this happen right you just design the workflow and use these direct SDK",
    "start": "1577399",
    "end": "1585120"
  },
  {
    "text": "service Integrations and um you have a workflow ready where you don't have direct",
    "start": "1585120",
    "end": "1591919"
  },
  {
    "text": "service integration for example for Bedrock you can use Lambda function for that compute to make that call to um the",
    "start": "1591919",
    "end": "1600760"
  },
  {
    "text": "service you can also use Lambda functions if you want to do some small data transformation between these",
    "start": "1600760",
    "end": "1606039"
  },
  {
    "text": "Services as the data goes through these services and again Lambda is serverless",
    "start": "1606039",
    "end": "1612559"
  },
  {
    "text": "event driven compute so it's it fits into our criteria of being cost effective",
    "start": "1612559",
    "end": "1619600"
  },
  {
    "start": "1618000",
    "end": "1714000"
  },
  {
    "text": "all right so here's the full on demand application um so I don't know if you",
    "start": "1619799",
    "end": "1625399"
  },
  {
    "text": "can see it well so we start with um you know your first you start with uh you",
    "start": "1625399",
    "end": "1631399"
  },
  {
    "text": "know as soon as the uh file is loaded into the S3 pocket um you determine the file type if it's an audio or an image",
    "start": "1631399",
    "end": "1639360"
  },
  {
    "text": "or a PDF document right and if it's an audio file um we we take uh the the",
    "start": "1639360",
    "end": "1646520"
  },
  {
    "text": "first path wherein we convert the audio to text using transcribe and then we summarize the",
    "start": "1646520",
    "end": "1653799"
  },
  {
    "text": "audio text using either stage maker or Amazon bedrock and then we save the",
    "start": "1653799",
    "end": "1659000"
  },
  {
    "text": "audio summary to Dynamo DB table right but if it's a um text based file or if",
    "start": "1659000",
    "end": "1665559"
  },
  {
    "text": "it's an image of a u document um then we can convert the",
    "start": "1665559",
    "end": "1671399"
  },
  {
    "text": "image to text using Amazon um textract we extract the text out of it and we",
    "start": "1671399",
    "end": "1677840"
  },
  {
    "text": "transform the image text the text track gives you output in terms of you know key value Pairs and Json and so we need",
    "start": "1677840",
    "end": "1683679"
  },
  {
    "text": "to collate all of that text together so that we can feed it to the large language model to summarize it right so",
    "start": "1683679",
    "end": "1690600"
  },
  {
    "text": "we can feed the entire document um to the large language model and and construct the prompts within our Lambda",
    "start": "1690600",
    "end": "1698080"
  },
  {
    "text": "function um to say okay go and summarize this whole entire body of text for me",
    "start": "1698080",
    "end": "1704039"
  },
  {
    "text": "right um and then the large language model gives us the summary back and then",
    "start": "1704039",
    "end": "1709120"
  },
  {
    "text": "we're going to save that into the",
    "start": "1709120",
    "end": "1713760"
  },
  {
    "start": "1714000",
    "end": "1839000"
  },
  {
    "text": "database all right um so cost optimization let's see um how if we are",
    "start": "1715640",
    "end": "1723240"
  },
  {
    "text": "well optimized in terms of cost with this application um and see what we can do so let's count",
    "start": "1723240",
    "end": "1731480"
  },
  {
    "text": "the cost first because you're using Amazon textract uh transcribe step functions",
    "start": "1731480",
    "end": "1739840"
  },
  {
    "text": "Lambda and Bedrock all of these are API driven Services as I said um these you",
    "start": "1739840",
    "end": "1746840"
  },
  {
    "text": "are charged only when you use the services right so these are all on demand",
    "start": "1746840",
    "end": "1752120"
  },
  {
    "text": "charges um and you're not paying for any idle time but if you're using Sage",
    "start": "1752120",
    "end": "1760320"
  },
  {
    "text": "maker um let's see how the cost uh spans out so to host a model within Sage maker",
    "start": "1760320",
    "end": "1768640"
  },
  {
    "text": "um say the large language model um you need a GPU instance say I pick mlg5 2x",
    "start": "1768640",
    "end": "1775559"
  },
  {
    "text": "large instance the hourly cost to run that um model on one instance is",
    "start": "1775559",
    "end": "1784399"
  },
  {
    "text": "a151 and you run that instance if you take the daily cost it's $36 and the",
    "start": "1784399",
    "end": "1790200"
  },
  {
    "text": "monthly cost turns out to be $11,000 and the yearly is so much it's purely not serverless right and if your traffic",
    "start": "1790200",
    "end": "1797760"
  },
  {
    "text": "depending on your traffic pattern if your traffic pattern to make inferences or if you want to generate the summaries",
    "start": "1797760",
    "end": "1803840"
  },
  {
    "text": "maybe daily right on a scheduled basis there's no reason you want to run",
    "start": "1803840",
    "end": "1809240"
  },
  {
    "text": "this um G5 instance which is a GPU instance all the time",
    "start": "1809240",
    "end": "1814840"
  },
  {
    "text": "24/7 right you just wa you're just um it's not cost efficient at all for you",
    "start": "1814840",
    "end": "1821559"
  },
  {
    "text": "right or even if you have like a um you know if you want to do near real time",
    "start": "1821559",
    "end": "1826679"
  },
  {
    "text": "and if you're traffic is unpredictable um and if your traffic is bursty I wouldn't want to keep my GPU",
    "start": "1826679",
    "end": "1834840"
  },
  {
    "text": "instance running up and all the time 24/7 right so let's see how we can make",
    "start": "1834840",
    "end": "1841480"
  },
  {
    "start": "1839000",
    "end": "2071000"
  },
  {
    "text": "this Sage maker end point on demand let's make the server",
    "start": "1841480",
    "end": "1847159"
  },
  {
    "text": "lless can we do it yes we can so what I'm going to do is take the current",
    "start": "1847159",
    "end": "1853399"
  },
  {
    "text": "existing step function workflow that I built and also because the step function workflow can",
    "start": "1853399",
    "end": "1860000"
  },
  {
    "text": "integrate directly with sage maker I'm going to have an additional step where I",
    "start": "1860000",
    "end": "1865960"
  },
  {
    "text": "create the sagemaker endpoint right and bring it up first",
    "start": "1865960",
    "end": "1872000"
  },
  {
    "text": "insert my current workflow there and then once the summary is generated I'm",
    "start": "1872000",
    "end": "1877559"
  },
  {
    "text": "going to shut it down right using the same step function",
    "start": "1877559",
    "end": "1883240"
  },
  {
    "text": "workflow now wrapping up this entire application so this is how it's um it looks so I'm",
    "start": "1883240",
    "end": "1891080"
  },
  {
    "text": "going to first create the sagemaker endpoint configuration um and then then create",
    "start": "1891080",
    "end": "1897799"
  },
  {
    "text": "the endpoint itself and then wait for the Endo to come up right um so I'm",
    "start": "1897799",
    "end": "1904039"
  },
  {
    "text": "going to have a loop there so and keep checking if the endpoint came up and",
    "start": "1904039",
    "end": "1909120"
  },
  {
    "text": "then once the Endo is in service then I'm going to take this workflow the",
    "start": "1909120",
    "end": "1914399"
  },
  {
    "text": "entire workflow and run it right right and then I'm going to then shut down the",
    "start": "1914399",
    "end": "1920080"
  },
  {
    "text": "end point and delete the Endo this is extremely cost efficient now um",
    "start": "1920080",
    "end": "1926399"
  },
  {
    "text": "especially when you have a scheduled um run right so let's see the costs",
    "start": "1926399",
    "end": "1935639"
  },
  {
    "text": "no okay so here's the full schedule application right in the step functions",
    "start": "1935639",
    "end": "1943000"
  },
  {
    "text": "workflow and um I can also use a distributed step function distributed map now to process all of the files in",
    "start": "1943000",
    "end": "1950639"
  },
  {
    "text": "parallel where well in in my previous workflow I had I used to invoke this for",
    "start": "1950639",
    "end": "1956039"
  },
  {
    "text": "every file that was um ingested but now I can take the distributed map",
    "start": "1956039",
    "end": "1962000"
  },
  {
    "text": "functionality uh within Sage uh within step functions and then run it in parallel for all of the run the same set",
    "start": "1962000",
    "end": "1969399"
  },
  {
    "text": "of processes which is uh extract the text from the image transform it summarize it and store it in the",
    "start": "1969399",
    "end": "1976360"
  },
  {
    "text": "database for for all of the files that were dropped in my S3 bucket in parallel and then shut down the end",
    "start": "1976360",
    "end": "1984240"
  },
  {
    "text": "point so now let's count the cost of sage maker On",
    "start": "1987399",
    "end": "1992639"
  },
  {
    "text": "Demand right so now because I am creating the endpoint and shutting down",
    "start": "1992639",
    "end": "1999080"
  },
  {
    "text": "on demand my monthly cost for assuming low traffic fairly low traffic is",
    "start": "1999080",
    "end": "2005840"
  },
  {
    "text": "reduced from $1,000 to $45 right so um with the with Stage make",
    "start": "2005840",
    "end": "2012880"
  },
  {
    "text": "uh with step functions I'm able to convert my um Sage maker endpoint to an",
    "start": "2012880",
    "end": "2019279"
  },
  {
    "text": "ond demand serverless endpoint and that's like 93 96% yearly",
    "start": "2019279",
    "end": "2026880"
  },
  {
    "text": "savings for me all right so here are the results right um so we we actually",
    "start": "2026880",
    "end": "2033840"
  },
  {
    "text": "created this application and um ran a a surgical pathology",
    "start": "2033840",
    "end": "2040639"
  },
  {
    "text": "report um and um you know extracted the text using",
    "start": "2040639",
    "end": "2045679"
  },
  {
    "text": "textract and then took all of the extracted text and um sent it to um a",
    "start": "2045679",
    "end": "2053079"
  },
  {
    "text": "large language model which was hosted in Sage maker and that was the summary um",
    "start": "2053079",
    "end": "2058280"
  },
  {
    "text": "which it uh gave us right and this is the summary that I'm going to be storing",
    "start": "2058280",
    "end": "2064200"
  },
  {
    "text": "in the database and use it for my Downstream um",
    "start": "2064200",
    "end": "2069760"
  },
  {
    "start": "2071000",
    "end": "2256000"
  },
  {
    "text": "applications so um what are the key",
    "start": "2071200",
    "end": "2076440"
  },
  {
    "text": "takeways from this oops sorry so for most of us AI is",
    "start": "2076440",
    "end": "2085240"
  },
  {
    "text": "just as simple as an endpoint um unless you're a data scientist you're not going to be",
    "start": "2085240",
    "end": "2090398"
  },
  {
    "text": "training models and more so in the in the space of generative AI we are going to be using pre-trained models it is an",
    "start": "2090399",
    "end": "2097560"
  },
  {
    "text": "endpoint the way we develop applications will change will differ because we are",
    "start": "2097560",
    "end": "2103800"
  },
  {
    "text": "now looking at how to do prompt engineering how to test those prompts um",
    "start": "2103800",
    "end": "2109400"
  },
  {
    "text": "and and all of those things and um use the best model for their job there are",
    "start": "2109400",
    "end": "2115280"
  },
  {
    "text": "plenty of AI models out there um choosing if you want to do a gener if",
    "start": "2115280",
    "end": "2122160"
  },
  {
    "text": "you want to use a generative AI model or a uh predictive model model is important",
    "start": "2122160",
    "end": "2128400"
  },
  {
    "text": "for the use case and also in in this space there are lots of models out there",
    "start": "2128400",
    "end": "2133760"
  },
  {
    "text": "lots of uh options for you figure out choose you have to test and figure out which model works best for this use case",
    "start": "2133760",
    "end": "2141079"
  },
  {
    "text": "at hand um and then choose that model right um and optimize the cost through",
    "start": "2141079",
    "end": "2148520"
  },
  {
    "text": "model availability uh some of these models again you know the the model that you",
    "start": "2148520",
    "end": "2153720"
  },
  {
    "text": "choose might be available within Bedrock or within Sage maker or you have to host a model so optimize that cost you know",
    "start": "2153720",
    "end": "2161680"
  },
  {
    "text": "if you're using a sage maker based model then as I said you can use step functions to make it more cost optimal",
    "start": "2161680",
    "end": "2168400"
  },
  {
    "text": "for you um it it's the cost going to be even more when you you know scale it out",
    "start": "2168400",
    "end": "2174200"
  },
  {
    "text": "like say for example you have lots of files dropping into those S3 buckets now you need more instances of that large",
    "start": "2174200",
    "end": "2182040"
  },
  {
    "text": "language model to be available the costs are going to significantly add up so op optimizing um those costs is also",
    "start": "2182040",
    "end": "2190720"
  },
  {
    "text": "important right and finally use managed services like step functions in Lambda",
    "start": "2190720",
    "end": "2196560"
  },
  {
    "text": "and bedrock and Avent bridge to orchestrate your data to and from this model endpoint so that you're being cost",
    "start": "2196560",
    "end": "2203920"
  },
  {
    "text": "effective uh you're not paying for idle time um and you are more agile in",
    "start": "2203920",
    "end": "2210160"
  },
  {
    "text": "building your applications this these are like low code no code Solutions so um you can quick build applic",
    "start": "2210160",
    "end": "2216640"
  },
  {
    "text": "applications quickly and iterate on building those applications really",
    "start": "2216640",
    "end": "2222560"
  },
  {
    "text": "quickly all right and uh with that here's more information if you want to",
    "start": "2222839",
    "end": "2228160"
  },
  {
    "text": "um get information about patterns and um several different surus patterns the",
    "start": "2228160",
    "end": "2233720"
  },
  {
    "text": "surus land.com is um a good place to",
    "start": "2233720",
    "end": "2239319"
  },
  {
    "text": "start and thank [Applause]",
    "start": "2239520",
    "end": "2245599"
  },
  {
    "text": "you",
    "start": "2245599",
    "end": "2248599"
  }
]