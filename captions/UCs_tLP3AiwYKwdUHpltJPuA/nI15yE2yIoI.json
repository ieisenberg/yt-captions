[
  {
    "start": "0",
    "end": "32000"
  },
  {
    "text": "well thanks for sticking around until the end of the day um especially given that uh you heard from me this morning",
    "start": "12679",
    "end": "18720"
  },
  {
    "text": "and hopefully you enjoyed enough to come back so today we're going to be talking about how honeycomb built our real-time",
    "start": "18720",
    "end": "25640"
  },
  {
    "text": "observability database on top of serverless and the arm64 processor",
    "start": "25640",
    "end": "31880"
  },
  {
    "text": "architecture So today we're going to talk about four different topics we're going to talk first about how serverless",
    "start": "31880",
    "end": "37920"
  },
  {
    "start": "32000",
    "end": "64000"
  },
  {
    "text": "is helpful and helps us get some past some of the constraints that we would have encountered with just using uh on",
    "start": "37920",
    "end": "42960"
  },
  {
    "text": "demand uh servers secondly we'll talk about how things can get a little bit painful when you start pushing the",
    "start": "42960",
    "end": "48399"
  },
  {
    "text": "limits of your serverless platform third we'll talk about how to do experiments with serverless and with arm processors",
    "start": "48399",
    "end": "55120"
  },
  {
    "text": "in your environment and then fourth we'll talk about how to deploy those uh Serv in production and at larger",
    "start": "55120",
    "end": "63400"
  },
  {
    "text": "scale so first of all let's talk about why we might use a service like AWS Lambda um but before that I think let's",
    "start": "63400",
    "end": "71400"
  },
  {
    "start": "64000",
    "end": "89000"
  },
  {
    "text": "back up a step and talk about how this impacts real end users because that's why we're here right like we're here not",
    "start": "71400",
    "end": "77280"
  },
  {
    "text": "to use the fanciest new technology we're here because we are trying to solve real problems so we'd like to optimize our",
    "start": "77280",
    "end": "83079"
  },
  {
    "text": "custom data store retriever uh that we built inh house at honeycomb to handle observability data and uh here's a",
    "start": "83079",
    "end": "89479"
  },
  {
    "start": "89000",
    "end": "240000"
  },
  {
    "text": "retriever uh this is my golden retriever in fact uh half golden retriever half Sam extremely cute but that's not",
    "start": "89479",
    "end": "96960"
  },
  {
    "text": "actually the particular kind of retriever we're talking about here we're talking about a data retriever not a uh",
    "start": "96960",
    "end": "102680"
  },
  {
    "text": "dog that retrieves balls so retriever here is a distributed column store that allows for Real Time Event aggregation",
    "start": "102680",
    "end": "110200"
  },
  {
    "text": "and data analysis and really who is it for right",
    "start": "110200",
    "end": "115719"
  },
  {
    "text": "like the point is it does Real Time Event aggregation in order to interactively query over millions or",
    "start": "115719",
    "end": "123600"
  },
  {
    "text": "potentially hundreds of millions of distinct traces my name is Liz I'm the field CTO at honeycomb I've been with",
    "start": "123600",
    "end": "129920"
  },
  {
    "text": "honeycomb for four and a half years and before that I worked as a site reliability engineer at Google so what's",
    "start": "129920",
    "end": "136959"
  },
  {
    "text": "honeycomb designed for um so our goal is to help customers who are software developers understand what what in the",
    "start": "136959",
    "end": "143640"
  },
  {
    "text": "heck their software is doing and in order to do that we want to make it ergonomic and fast and Che for cers to",
    "start": "143640",
    "end": "151120"
  },
  {
    "text": "quy their own own tet so I CED this a littleit this the difference monitoring",
    "start": "151120",
    "end": "157360"
  },
  {
    "text": "and observability boils down to monitoring is about known and knowns it's about how do we measure things that",
    "start": "157360",
    "end": "164680"
  },
  {
    "text": "are predictable in advance that we know in advance might fail whereas observability is more suited to the",
    "start": "164680",
    "end": "171879"
  },
  {
    "text": "unknown unknowns things that we couldn't predict in advance we knew we going to fail but now that they're broken we need",
    "start": "171879",
    "end": "177319"
  },
  {
    "text": "to be able to fix them as quickly as possible so it's basically about what do you want to see right now and how can we",
    "start": "177319",
    "end": "183799"
  },
  {
    "text": "make it very fast even if you didn't tell us what questions and you were going to ask and Vince so basically uh",
    "start": "183799",
    "end": "189760"
  },
  {
    "text": "this is what our UI looks like again this is not a honeycomb sales pitch this is more of a how quickly you expect to",
    "start": "189760",
    "end": "196799"
  },
  {
    "text": "see results when you're using an observability tool which is in general you know that's if that spinning wheel",
    "start": "196799",
    "end": "201920"
  },
  {
    "text": "is spinning more than about 5 seconds then something has gone wrong on our back end or you're asking something",
    "start": "201920",
    "end": "207400"
  },
  {
    "text": "really really complicated of us and the point of this is to enable you to iteratively ask questions to kind of not",
    "start": "207400",
    "end": "214000"
  },
  {
    "text": "feel like I have to get my query exactly right or else it'll never work but instead you can kind of write a partial",
    "start": "214000",
    "end": "220519"
  },
  {
    "text": "query and then to have the query uh complete and then to run another query based off of what you find so it's kind",
    "start": "220519",
    "end": "226959"
  },
  {
    "text": "of being in conversation with your code so at the end of the day what we're trying to do essentially is we are",
    "start": "226959",
    "end": "233560"
  },
  {
    "text": "trying to aggregate data that we've received from your services at runtime and quickly give you the results",
    "start": "233560",
    "end": "240200"
  },
  {
    "start": "240000",
    "end": "300000"
  },
  {
    "text": "so this means that we have to be able to query any combination of data fields potentially you know billions of rows uh",
    "start": "240200",
    "end": "247120"
  },
  {
    "text": "potentially hundreds or thousands of different individual fields or columns and potentially Millions trillions right",
    "start": "247120",
    "end": "252879"
  },
  {
    "text": "like as many different possible Cardinal values within each within each uh",
    "start": "252879",
    "end": "259239"
  },
  {
    "text": "attribute so in a nutshell people might generate this kind of data with a uh standard called open Telemetry which",
    "start": "259239",
    "end": "265440"
  },
  {
    "text": "basically allows you to take traces from your application data and send them to any sync regardless of whether it's your",
    "start": "265440",
    "end": "272320"
  },
  {
    "text": "Cloud vendor whether it's a particular observability vendor and then how your observability vendor implements that may",
    "start": "272320",
    "end": "278360"
  },
  {
    "text": "depend based off of who they are in our particular case the way that we like to think about this data is that we",
    "start": "278360",
    "end": "283520"
  },
  {
    "text": "decompose the data instead of storing it row by row we break it up by each column",
    "start": "283520",
    "end": "288639"
  },
  {
    "text": "by each attribute name and that way we store all of the attributes together and it lets us very efficiently query the",
    "start": "288639",
    "end": "295880"
  },
  {
    "text": "subset of data that you're most that we're most interested in and the emphasis here is interactive",
    "start": "295880",
    "end": "301639"
  },
  {
    "start": "300000",
    "end": "505000"
  },
  {
    "text": "right like that when you have 100 milliseconds right that's fast that is faster than you can blink when it's",
    "start": "301639",
    "end": "307919"
  },
  {
    "text": "about a th milliseconds that's you know okay I'm taking a cup of my mug of coffee I'm taking a sip but if it starts",
    "start": "307919",
    "end": "315039"
  },
  {
    "text": "taking you know 100 seconds that's at the point where I'm no longer uh taking a sip of my coffee that's a okay this is",
    "start": "315039",
    "end": "322680"
  },
  {
    "text": "going to take a while maybe I should get up and stretch and go make a new uh pot of coffee right like that that's no good",
    "start": "322680",
    "end": "329240"
  },
  {
    "text": "that inter your state of cognitive flow we want you to stay in your state of cognitive flow especially if it's 3:",
    "start": "329240",
    "end": "335240"
  },
  {
    "text": "a.m. and you're debugging an issue so under the hood the way that we implement this is we implement this",
    "start": "335240",
    "end": "341840"
  },
  {
    "text": "using uh using a streamed data inest that flows through kfka and then we wind",
    "start": "341840",
    "end": "347720"
  },
  {
    "text": "up unpacking that data into the constituent columns storing all the columns associated with uh with a given",
    "start": "347720",
    "end": "354639"
  },
  {
    "text": "data set together and then we upload it to a uh a cloud storage like a wss3 and",
    "start": "354639",
    "end": "361000"
  },
  {
    "text": "then when it comes time to read it we need to scoop that back off and and and query it and return the results so this",
    "start": "361000",
    "end": "369080"
  },
  {
    "text": "is great in principle but how does that actually work and how did we arrive at that architecture well we might want to",
    "start": "369080",
    "end": "376360"
  },
  {
    "text": "ingest uh potentially millions of events per second from our customers and you know magic happens and then customers",
    "start": "376360",
    "end": "382479"
  },
  {
    "text": "can query it right so one way of handling this as you scale the number of users is to uh scatter your user data",
    "start": "382479",
    "end": "390360"
  },
  {
    "text": "across multiple partitions there's not one retriever it's multiple retrievers",
    "start": "390360",
    "end": "395440"
  },
  {
    "text": "and for each retriever there's actually a mirror of it or a pair so that we",
    "start": "395440",
    "end": "400680"
  },
  {
    "text": "don't wind up with unavailable data in the event that we have to restart things in order to do",
    "start": "400680",
    "end": "406479"
  },
  {
    "text": "maintenance so these retrievers let's suppose that they're writing all to local dis what happens if that local dis",
    "start": "406479",
    "end": "412639"
  },
  {
    "text": "fills up well you might have to delete the data off of the local dis if you're running out of space but that's really",
    "start": "412639",
    "end": "418960"
  },
  {
    "text": "suboptimal because it turns out our customers like to query data that's more than a few hours old people might you",
    "start": "418960",
    "end": "424039"
  },
  {
    "text": "know want to ask for a day of data or a week of data or even you know two months of data so what's our solution there",
    "start": "424039",
    "end": "432160"
  },
  {
    "text": "well our solution there is to break things up to have one file per column",
    "start": "432160",
    "end": "437800"
  },
  {
    "text": "and then we wind up uh touching the data that we need and if we don't need the particular data we don't read",
    "start": "437800",
    "end": "443879"
  },
  {
    "text": "it and we also can start to think about charting the data because it turns out",
    "start": "443879",
    "end": "449039"
  },
  {
    "text": "that when we have that problem of how to expire the oldest data well the easiest way to handle that is to break up the",
    "start": "449039",
    "end": "454720"
  },
  {
    "text": "data into chunks by timestamp uh that it contains as well as keeping track of when we received each chunk now each",
    "start": "454720",
    "end": "462720"
  },
  {
    "text": "chunk uh has a start and an end time and that enables us to say if you're looking at things in this particular one hour",
    "start": "462720",
    "end": "468800"
  },
  {
    "text": "window I only have to scan the data that has a start or end time that overlaps with that",
    "start": "468800",
    "end": "475360"
  },
  {
    "text": "window so once we have these kind of overlapping time ranges uh so this is kind of doing a breakdown per retriever",
    "start": "475360",
    "end": "483360"
  },
  {
    "text": "per individual host then we kind of have this idea of being able to expire the data and of being able to run queries",
    "start": "483360",
    "end": "489680"
  },
  {
    "text": "against subsets of the data and keep in mind that when I'm running a query against any one time slice I also am",
    "start": "489680",
    "end": "496280"
  },
  {
    "text": "running a query only against the specific columns that I want to read so once we do that uh do that",
    "start": "496280",
    "end": "503560"
  },
  {
    "text": "indexing we get efficient querying and that lets us essentially answer any arbitrary question it's trading off uh",
    "start": "503560",
    "end": "511680"
  },
  {
    "start": "505000",
    "end": "751000"
  },
  {
    "text": "many other data stores will wind up trading off Righttime optimization so",
    "start": "511680",
    "end": "516919"
  },
  {
    "text": "they will pre-aggregate the data so if you've ever used a metric store before right like it asks you to break down in",
    "start": "516919",
    "end": "523000"
  },
  {
    "text": "advance and to say I want a Time series by machine or I want a Time series by",
    "start": "523000",
    "end": "529040"
  },
  {
    "text": "data center or I want a Time series by build ID but it won't let you n do that",
    "start": "529040",
    "end": "534360"
  },
  {
    "text": "packing whereas storing the data in raw form enables you to do that to kind kind",
    "start": "534360",
    "end": "539560"
  },
  {
    "text": "of agregation at read time rather than WR time it just potentially creates some",
    "start": "539560",
    "end": "544680"
  },
  {
    "text": "of these complications as we've seen because we have to store all of the data raw and then we have to analyze it uh at",
    "start": "544680",
    "end": "550519"
  },
  {
    "text": "read time so as we get more customers one answer to this is to say okay we're",
    "start": "550519",
    "end": "556040"
  },
  {
    "text": "going to run more than one retriever we're going to start charting the data Maybe One customer's data goes to retrievers one and five the other",
    "start": "556040",
    "end": "562079"
  },
  {
    "text": "customer data goes to retriever six and n and that works okay and we can also start thinking",
    "start": "562079",
    "end": "568720"
  },
  {
    "text": "about sharting more often and more aggressively so that we're not deleting you know an hour or two of data at a",
    "start": "568720",
    "end": "574079"
  },
  {
    "text": "time but we're deleting maybe 10 minutes of data at a time but wouldn't it be better if we didn't have to delete it at",
    "start": "574079",
    "end": "579519"
  },
  {
    "text": "all so at that point we have introduced the idea of teering older data to S3",
    "start": "579519",
    "end": "584920"
  },
  {
    "text": "data that is not currently actively being ridden and it turns out fun fact",
    "start": "584920",
    "end": "590200"
  },
  {
    "text": "uh you would think that this would make our data store slower to have the lency of going out to S3 as opposed to reading",
    "start": "590200",
    "end": "596760"
  },
  {
    "text": "off of the local dis but when you're reading a sufficiently large amount of data it turns out that accessing any",
    "start": "596760",
    "end": "604600"
  },
  {
    "text": "amount of data in serial off of a local SSD is going to be much slower than reading that same corresponding amount",
    "start": "604600",
    "end": "611040"
  },
  {
    "text": "of data massively in parallel off of a distributed uh file store like S3 so yay now we can keep data for a",
    "start": "611040",
    "end": "618920"
  },
  {
    "text": "fixed time range like 60 days rather than only keeping two or three hours of data on our local dis huzzah but there's",
    "start": "618920",
    "end": "626680"
  },
  {
    "text": "a slight problem here which is I alluded to the fact that reading data in parallel from S3 is faster than reading",
    "start": "626680",
    "end": "632320"
  },
  {
    "text": "in serial from local disk but if you have only a fixed amount of compute then you are bound to have to serially gravel",
    "start": "632320",
    "end": "639320"
  },
  {
    "text": "through that data fetch it back from S3 in order to query it and that could get a little bit slow so how can we cope",
    "start": "639320",
    "end": "645920"
  },
  {
    "text": "with this well one way of dealing with this is to say okay uh we don't want the",
    "start": "645920",
    "end": "652279"
  },
  {
    "text": "time to scale linearly with the amount of data that we're scanning what if I could just magically add more CPU",
    "start": "652279",
    "end": "660760"
  },
  {
    "text": "oo so this is a very ond demand bursty model um so this is my dog flurry and",
    "start": "660760",
    "end": "668360"
  },
  {
    "text": "flurry says I would love to have lots more compute to play with but only if I want to play if not if I'm bored and I",
    "start": "668360",
    "end": "674279"
  },
  {
    "text": "want to wander off somewhere else I don't need all of that compute so where can we get a source of such magical",
    "start": "674279",
    "end": "680680"
  },
  {
    "text": "compute that we can spin up on demand within millisecs and that we can say hey cloud provider I'm done using this you",
    "start": "680680",
    "end": "686839"
  },
  {
    "text": "can recycle it and give it to someone else so enter AWS Lambda um because we're on",
    "start": "686839",
    "end": "693240"
  },
  {
    "text": "the AWS ecosystem although you can do the same thing with Google Cloud functions and so forth so Amazon has",
    "start": "693240",
    "end": "700240"
  },
  {
    "text": "kindly uh given us access to uh tens of thousands of query workers on demand",
    "start": "700240",
    "end": "705839"
  },
  {
    "text": "which is really really fantastic because it means that we can just have a user query come in and we can scan tens of",
    "start": "705839",
    "end": "712800"
  },
  {
    "text": "thousands of these individual files or columns uh sorry these files or segments",
    "start": "712800",
    "end": "718680"
  },
  {
    "text": "at once and that enables us to no matter how much data the user is asking for spin up the appropriate amount of",
    "start": "718680",
    "end": "724720"
  },
  {
    "text": "compute to return them an answer in less than 10 seconds so now instead of Fanning out the data across only one",
    "start": "724720",
    "end": "732360"
  },
  {
    "text": "retriever we're getting to have the ability to have sub linear uh query Time by just adding more parallelism and this",
    "start": "732360",
    "end": "740320"
  },
  {
    "text": "works because S3 is storing the data massively parallel uh as well and they're therefore able to read the data",
    "start": "740320",
    "end": "747519"
  },
  {
    "text": "and run these read time f and aggregations so by buying compute in 1",
    "start": "747519",
    "end": "754240"
  },
  {
    "start": "751000",
    "end": "873000"
  },
  {
    "text": "Mond units um again thank you Amazon Amazon used to bill us in the 100 millisecond increments but it turns out",
    "start": "754240",
    "end": "760760"
  },
  {
    "text": "Amazon will now bill you in one millisecond increments so that means that if you have very short queries it's actually feasible to use Lambda for this",
    "start": "760760",
    "end": "767279"
  },
  {
    "text": "as well so this is why would we make this tradeoff it's because developer",
    "start": "767279",
    "end": "773079"
  },
  {
    "text": "time is expensive because speed is a feature right we are willing to spend a considerable amount of money on AWS",
    "start": "773079",
    "end": "779839"
  },
  {
    "text": "Lambda as long as it delivers Superior performance for our customers so this",
    "start": "779839",
    "end": "785639"
  },
  {
    "text": "means that we can get results like 50 millisecond query time uh sorry 50 millisecond cold starts we can get 90%",
    "start": "785639",
    "end": "793079"
  },
  {
    "text": "of our queries to return in less than 2.5 seconds and that's you know when you",
    "start": "793079",
    "end": "798240"
  },
  {
    "text": "account for the fact that per CPU second Lambda is about 3 to four times more",
    "start": "798240",
    "end": "803600"
  },
  {
    "text": "expensive than ec2 except we don't have to pay to run tens of thousands of ec2 nodes full out continuously right the2",
    "start": "803600",
    "end": "810760"
  },
  {
    "text": "startup time would be you know on the order of 2 or 3 minutes from the time we requested a VM to to getting one fully",
    "start": "810760",
    "end": "816480"
  },
  {
    "text": "running but by paying only for kind of that 50 millisecond startup time and",
    "start": "816480",
    "end": "821519"
  },
  {
    "text": "then the amount of time to run the the workers it means that we're able to much more economically Brute Force these",
    "start": "821519",
    "end": "828800"
  },
  {
    "text": "queries rather than wait forever for an ec2 instance to spin up and that makes it well worth the cost for us to use",
    "start": "828800",
    "end": "835320"
  },
  {
    "text": "serverless functions as this uh as this backend for filter ing through our files on blob",
    "start": "835320",
    "end": "842320"
  },
  {
    "text": "storage and this is really really this architectural decision has served us really well at honeycom so uh at the end",
    "start": "842320",
    "end": "849199"
  },
  {
    "text": "of 2022 we were ingesting about 10 times as much uh data as we were three yearsi",
    "start": "849199",
    "end": "854800"
  },
  {
    "text": "prior in 2019 and we also had quer customers querying us whether manually or on an",
    "start": "854800",
    "end": "861160"
  },
  {
    "text": "automated basis about 10 times as much and by collaborating with Amazon and",
    "start": "861160",
    "end": "866519"
  },
  {
    "text": "making sure that we are on the same page about capacity planning we have been able to serve this need without any performance",
    "start": "866519",
    "end": "873600"
  },
  {
    "start": "873000",
    "end": "1352000"
  },
  {
    "text": "degradation so here's a little bit of a caveat why did I say in collaboration",
    "start": "873600",
    "end": "879120"
  },
  {
    "text": "with Amazon the answer is because Lambda is marketed as a solution that is low",
    "start": "879120",
    "end": "885440"
  },
  {
    "text": "code uh low code uh that's low that's no infrastructure that you can use to very",
    "start": "885440",
    "end": "890680"
  },
  {
    "text": "quickly get API request workers that you can ED to do tiny tiny tiny tiny microservices right it's not necessarily",
    "start": "890680",
    "end": "898320"
  },
  {
    "text": "been marketed for or kind of at least with the default rate limits designed for a use case like this where you're",
    "start": "898320",
    "end": "904440"
  },
  {
    "text": "using it you know tens of thousands of workers for 100 milliseconds or two seconds and then you drop down to",
    "start": "904440",
    "end": "910600"
  },
  {
    "text": "zero so that's why I put that asterisk about scal about scaling up effectively",
    "start": "910600",
    "end": "916680"
  },
  {
    "text": "it's this is possible but you also need to work with your uh serverless function vendor because they may not necessarily",
    "start": "916680",
    "end": "922680"
  },
  {
    "text": "have designed around uh you in their default quotas so let's start by talking about",
    "start": "922680",
    "end": "928240"
  },
  {
    "text": "the scaling prop properies so there's a concurrency limit that every AWS uh",
    "start": "928240",
    "end": "934680"
  },
  {
    "text": "Lambda function has and there's also a concurrency limit per account and the concurrency limit per account limits the",
    "start": "934680",
    "end": "941519"
  },
  {
    "text": "maximum total throughput of all of your lambdas and there's also that burst Limit that's per function and the more",
    "start": "941519",
    "end": "948000"
  },
  {
    "text": "you continuously run your function flat out the more that Amazon or your uh or",
    "start": "948000",
    "end": "953560"
  },
  {
    "text": "your Cloud vendor is going to scale that out is going to depend upon how much you consistently use that concurrency that",
    "start": "953560",
    "end": "959360"
  },
  {
    "text": "they know to keep those worker nodes running right it doesn't sense for them to keep you running on you know 100,",
    "start": "959360",
    "end": "966240"
  },
  {
    "text": "different uh nodes if you're only sporadically using the service because then they're paying to kind of keep your",
    "start": "966240",
    "end": "971800"
  },
  {
    "text": "nodes uh pre-warmed and in memory rather than keeping them rather than shutting them all the way down so if you exceed",
    "start": "971800",
    "end": "978600"
  },
  {
    "text": "your burst Limit temporarily instead of letting you invoke the function it's going to return a 429 which tells you to",
    "start": "978600",
    "end": "984519"
  },
  {
    "text": "slow down but for a workload like ours that's really bursty where you're running for",
    "start": "984519",
    "end": "990040"
  },
  {
    "text": "maybe 3 to 5 Seconds at a time at tens of thousands of concurrency and then you're dropping a zero and you're",
    "start": "990040",
    "end": "995079"
  },
  {
    "text": "running you know tens of thousands of concurrency and dropping to zero you never wind up hitting that uh that burst",
    "start": "995079",
    "end": "1001600"
  },
  {
    "text": "sailing threshold until your query is already finished so by looking at what the",
    "start": "1001600",
    "end": "1008000"
  },
  {
    "text": "actual concurrency was that we were demanding uh from Amazon and then working with their capacity planning",
    "start": "1008000",
    "end": "1013199"
  },
  {
    "text": "teams we were able to negotiate with them on raising some of these limits that we could scale a little bit faster",
    "start": "1013199",
    "end": "1018279"
  },
  {
    "text": "out of the gate because they knew that our workload was going to be worthwhile for them to keep",
    "start": "1018279",
    "end": "1023959"
  },
  {
    "text": "cold uh to keep warm started rather than running starting from cold every time and that we would always you know maybe",
    "start": "1023959",
    "end": "1030079"
  },
  {
    "text": "not come back every single every single second but we would definitely come back you know multiple times per minute at",
    "start": "1030079",
    "end": "1036240"
  },
  {
    "text": "least and therefore make it uh so that it was worth keeping our workers in memory so study your limits and uh",
    "start": "1036240",
    "end": "1043880"
  },
  {
    "text": "change your retri parameters at least to uh you know don't ignore a 429 failure it's not a permanent failure it's just a",
    "start": "1043880",
    "end": "1050400"
  },
  {
    "text": "I couldn't run that right now retry it and eventually you'll get through and when you have adequate observability",
    "start": "1050400",
    "end": "1056799"
  },
  {
    "text": "that gives you the data to go to your account reps and say this is why I need a quota increase I know what I'm doing",
    "start": "1056799",
    "end": "1061840"
  },
  {
    "text": "and I promise I'm going to be I'm going to collaborate with you all right let's talk about the median startup time",
    "start": "1061840",
    "end": "1067760"
  },
  {
    "text": "you'll notice why I said median why did I say median well it turns out that you",
    "start": "1067760",
    "end": "1073400"
  },
  {
    "text": "know sure you can get cold starts in about 30 milliseconds but there is a little bit",
    "start": "1073400",
    "end": "1079000"
  },
  {
    "text": "of a long T to that um there are you know there's the potential to put functions to sleep and have them wake up",
    "start": "1079000",
    "end": "1085440"
  },
  {
    "text": "quickly but some cold starts can take considerably longer than 30 milliseconds to get",
    "start": "1085440",
    "end": "1090600"
  },
  {
    "text": "running the other bit is 90% of our requests come back within 2 2.5 seconds",
    "start": "1090600",
    "end": "1096520"
  },
  {
    "text": "but we want every request to finish so that we can return you complete data we don't want to miss scanning one",
    "start": "1096520",
    "end": "1103679"
  },
  {
    "text": "particular uh segment covering a given time range that happens to show you know maybe five 00,000 requests right that'll",
    "start": "1103679",
    "end": "1111039"
  },
  {
    "text": "cause your data to be off by those 500,000 requests so it turns out functions",
    "start": "1111039",
    "end": "1117000"
  },
  {
    "text": "actually some of our Lambda invocations never return you could wait forever and",
    "start": "1117000",
    "end": "1122480"
  },
  {
    "text": "they'd never come back and they don't necessarily report that they failed until 30 seconds or 60 seconds so it",
    "start": "1122480",
    "end": "1130159"
  },
  {
    "text": "turns out that there is a useful property here which is if you're willing to pay for the compute which we",
    "start": "1130159",
    "end": "1135280"
  },
  {
    "text": "certainly are if you have 99% of your that's come in and you're waiting on that last 10 on that last 1% retry right",
    "start": "1135280",
    "end": "1143320"
  },
  {
    "text": "that you should exercise impatience in order to make sure that you are getting an exhaustive search of your data",
    "start": "1143320",
    "end": "1149039"
  },
  {
    "text": "because most of the time when you have a Lambda worker that fails to come back it's not that there that it it'll fail",
    "start": "1149039",
    "end": "1154760"
  },
  {
    "text": "to come back if you retry the request right it's not a permanent failure that's that's a property of what you",
    "start": "1154760",
    "end": "1160039"
  },
  {
    "text": "what you asked for it's just that the node happens to be busy or unavailable the other weird thing is",
    "start": "1160039",
    "end": "1167080"
  },
  {
    "text": "Lambda functions are very very used to either handling HTTP requests or they're",
    "start": "1167080",
    "end": "1172360"
  },
  {
    "text": "used to handling Json so if you have data that you would rather send in grpc",
    "start": "1172360",
    "end": "1178159"
  },
  {
    "text": "format right that you'd rather kind of treat more as a uh Proto buff or as a kind of running uh streaming streaming",
    "start": "1178159",
    "end": "1185440"
  },
  {
    "text": "request right now Lambda doesn't support doing that uh you have to basically serialize and deserialize your request",
    "start": "1185440",
    "end": "1191559"
  },
  {
    "text": "to Json and we' run into Fun issues there like it turns out to you you know",
    "start": "1191559",
    "end": "1199080"
  },
  {
    "text": "what what a surprise um it turns out if you are trying to request you know",
    "start": "1199080",
    "end": "1204520"
  },
  {
    "text": "running the same payload against different files but you're running the same query against you know a million",
    "start": "1204520",
    "end": "1212320"
  },
  {
    "text": "different files if you are serializing that Proto buff into Json a million",
    "start": "1212320",
    "end": "1218640"
  },
  {
    "text": "times one for each file that you're trying to scan against that's going to get really expensive it turns out you",
    "start": "1218640",
    "end": "1223960"
  },
  {
    "text": "might want to cash um some of your inputs into into Json",
    "start": "1223960",
    "end": "1229159"
  },
  {
    "text": "because you're doing that conversion and what happens if you like",
    "start": "1229159",
    "end": "1234760"
  },
  {
    "text": "us are trying to return a lot of data what if you're trying to um for instance return the number of queries that",
    "start": "1234760",
    "end": "1242360"
  },
  {
    "text": "matched or sorry the number of rows that matched a given query broken down by user ID you might potentially have",
    "start": "1242360",
    "end": "1249919"
  },
  {
    "text": "millions of different user IDs and the result of that query is going to run more than 6",
    "start": "1249919",
    "end": "1255679"
  },
  {
    "text": "megabytes and normally AWS Lambda expects you to just return in standard out what you want to be uh returned to",
    "start": "1255679",
    "end": "1262760"
  },
  {
    "text": "the invoke call but that doesn't really work so well for us so for larger return values we actually wind up putting the",
    "start": "1262760",
    "end": "1269120"
  },
  {
    "text": "data into S3 and then returning the link to that and then we'll read the file out of",
    "start": "1269120",
    "end": "1275360"
  },
  {
    "text": "S3 and finally let's talk about the cost one last time because yes per CPU second it's about three to four times more",
    "start": "1275360",
    "end": "1281400"
  },
  {
    "text": "expensive than ec2 but maybe there's a way to make it cost slightly less so we keep a pretty",
    "start": "1281400",
    "end": "1289720"
  },
  {
    "text": "good idea of what our customers are costing us um because not necessarily",
    "start": "1289720",
    "end": "1295679"
  },
  {
    "text": "because we're building them for it in fact we prefer for our customers not to need to worry about this or worry about the cost of running queries so we'll eat",
    "start": "1295679",
    "end": "1302240"
  },
  {
    "text": "the cost ourselves but we still want to know if a customer is running a lot of queries and to see whether we can",
    "start": "1302240",
    "end": "1307840"
  },
  {
    "text": "optimize their use case or to talk to them if they're running you know a bunch of programmatic queries that it might be better to scrape with a batch job so we",
    "start": "1307840",
    "end": "1316159"
  },
  {
    "text": "actually have alerts that are set based off of the total Lambda runtime and the",
    "start": "1316159",
    "end": "1321400"
  },
  {
    "text": "inferred cost in dollars and cents to us per day of that customer continuing to do their",
    "start": "1321400",
    "end": "1327240"
  },
  {
    "text": "workload but what if those functions could cost less and what if those functions could also be lighter on the",
    "start": "1327240",
    "end": "1334080"
  },
  {
    "text": "environment to cause less carbon emission because they're consuming less electricity that would be pretty great",
    "start": "1334080",
    "end": "1340880"
  },
  {
    "text": "right so a couple of years ago Amazon released support for the arm 64 architecture with",
    "start": "1340880",
    "end": "1348559"
  },
  {
    "text": "a Lambda and good news for us we' already done the research and done the",
    "start": "1348559",
    "end": "1354720"
  },
  {
    "start": "1352000",
    "end": "2373000"
  },
  {
    "text": "analysis to make our uh to make the rest of our workload run on the arm 64",
    "start": "1354720",
    "end": "1360919"
  },
  {
    "text": "architecture and I want to emphasize this is not just about the money this is also about saving our planet that we can",
    "start": "1360919",
    "end": "1368200"
  },
  {
    "text": "and should try to reduce consumption first but where we can't reduce",
    "start": "1368200",
    "end": "1373400"
  },
  {
    "text": "consumption we have an obligation to make it run as efficiently as possible",
    "start": "1373400",
    "end": "1378720"
  },
  {
    "text": "so these slides are from uh about a year ago so about a year ago um Australia was",
    "start": "1378720",
    "end": "1385520"
  },
  {
    "text": "a little bit on fire and you can see that um there there are basically these",
    "start": "1385520",
    "end": "1391080"
  },
  {
    "text": "huge uh climate anomalies that we have temperatures that are many degrees Centigrade uh higher than they ought to",
    "start": "1391080",
    "end": "1397919"
  },
  {
    "text": "be that the bush is much drier than it ought to be due to a lack of rainfall",
    "start": "1397919",
    "end": "1403240"
  },
  {
    "text": "and that's creating bush fires and that's causing even more carbon to be released which is exacer",
    "start": "1403240",
    "end": "1409880"
  },
  {
    "text": "ccle so we have to think about how do we minimize our impact on the environment",
    "start": "1409880",
    "end": "1415679"
  },
  {
    "text": "and you would say oh but you know we should stop taking airplanes and we should uh and we should use use public",
    "start": "1415679",
    "end": "1421679"
  },
  {
    "text": "transport but yes that is true but we as technologists have a very high leverage",
    "start": "1421679",
    "end": "1427640"
  },
  {
    "text": "role to play here in how we consume software and how we consume Hardware it turns out that a",
    "start": "1427640",
    "end": "1434440"
  },
  {
    "text": "staggeringly large percentage of energy usage is from data centers is from",
    "start": "1434440",
    "end": "1441480"
  },
  {
    "text": "computation so the best way for us to help that doesn't involve replacing a",
    "start": "1441480",
    "end": "1446640"
  },
  {
    "text": "whole bunch of cars and you know yes of course we should replace cars we should replace airplanes but we with just a",
    "start": "1446640",
    "end": "1453360"
  },
  {
    "text": "couple of keystrokes can actually have a significant impact on the uh roughly I think 3.5 or 4% of emissions that are",
    "start": "1453360",
    "end": "1460159"
  },
  {
    "text": "caused uh by Computing and no buying offsets does not work buying offsets is uh turns out it's",
    "start": "1460159",
    "end": "1468120"
  },
  {
    "text": "pretty fake it it doesn't it doesn't actually result in meaningful reductions so let's focus on wasting",
    "start": "1468120",
    "end": "1475679"
  },
  {
    "text": "less power so if think about whether you have to run that workload um if you do",
    "start": "1475679",
    "end": "1481039"
  },
  {
    "text": "have to run that workload um figure out the most carbon uh the most carbon efficient place to run it and this is",
    "start": "1481039",
    "end": "1489039"
  },
  {
    "text": "your obligatory mention by a speaker because every speaker has to talk about Ai and I'm here to tell you not to do AI",
    "start": "1489039",
    "end": "1496000"
  },
  {
    "text": "unless you absolutely have to because that Stu sucks up a lot of",
    "start": "1496000",
    "end": "1502440"
  },
  {
    "text": "power so in that Spirit of using less power uh let's talk about how this",
    "start": "1502960",
    "end": "1508200"
  },
  {
    "text": "workload which we think is essential right like it's essential to help people be able to debug their software when",
    "start": "1508200",
    "end": "1514240"
  },
  {
    "text": "it's failing right failing software can cause user inconvenience it can even cost lives so how do we cause software",
    "start": "1514240",
    "end": "1520880"
  },
  {
    "text": "failures to take less time to resolve well we have to have observability so it makes sense to continue to run this",
    "start": "1520880",
    "end": "1526760"
  },
  {
    "text": "workload so in in 2021 uh we had the sorry in 2020",
    "start": "1526760",
    "end": "1533840"
  },
  {
    "text": "we had the opportunity to trial uh Amazon's new gravit hun 2 instances and those instances we found",
    "start": "1533840",
    "end": "1540840"
  },
  {
    "text": "uh required some adaptation to software to make them work correctly but when we did that adaptation we were able to see",
    "start": "1540840",
    "end": "1548039"
  },
  {
    "text": "roughly 30% uh 30% performance gain per instance and those instances also cost",
    "start": "1548039",
    "end": "1554880"
  },
  {
    "text": "about 10% less per instance so that was a pretty big win for us",
    "start": "1554880",
    "end": "1560080"
  },
  {
    "text": "what what adaptation did it take to migrate to the arm architecture with our server full applications first well we",
    "start": "1560080",
    "end": "1568440"
  },
  {
    "text": "needed to make sure that our base images and tooling could be built for arm 64",
    "start": "1568440",
    "end": "1574000"
  },
  {
    "text": "and we needed to check to make sure that all of those um you know auditing and logging and those operating system level",
    "start": "1574000",
    "end": "1580520"
  },
  {
    "text": "uh level constructs were there to satisf compliance and we also had to check over all of our code to make sure there",
    "start": "1580520",
    "end": "1586320"
  },
  {
    "text": "wasn't any handwritten assembly code in there that was going to cause our our applications to not work at",
    "start": "1586320",
    "end": "1592000"
  },
  {
    "text": "all and finally um we had to change our CI Tooling in order to produce different",
    "start": "1592000",
    "end": "1597039"
  },
  {
    "text": "artifacts because the arm 64 architecture cannot just run a x86 binary in place you have to compile",
    "start": "1597039",
    "end": "1604080"
  },
  {
    "text": "something in order to use the uh RM 64 instruction set but the good news is",
    "start": "1604080",
    "end": "1609200"
  },
  {
    "text": "with our build tooling um we are able to template our builds and pass a single",
    "start": "1609200",
    "end": "1615320"
  },
  {
    "text": "different environment variable to our build system and that causes the go",
    "start": "1615320",
    "end": "1620720"
  },
  {
    "text": "language compiler to automatically generate a cross compiled binary for arm",
    "start": "1620720",
    "end": "1625760"
  },
  {
    "text": "64 so we didn't really need to change our entire infrastructure we just needed to have it produce a second artifact",
    "start": "1625760",
    "end": "1632320"
  },
  {
    "text": "built with a different environment variable similar to how you might have optimized in debug binaries and to store",
    "start": "1632320",
    "end": "1637840"
  },
  {
    "text": "that uh new new artifact in a different place so that we could refer to either the arm64 or x86",
    "start": "1637840",
    "end": "1644200"
  },
  {
    "text": "binary guess what if you are using a uh a language that uses a runtime like Java",
    "start": "1644200",
    "end": "1650799"
  },
  {
    "text": "or python uh or the jvm uh ecosystem in general you don't have to left a finger",
    "start": "1650799",
    "end": "1656720"
  },
  {
    "text": "things should mostly just work um with a caveat that jni libraries uh might",
    "start": "1656720",
    "end": "1661799"
  },
  {
    "text": "require uh making sure that you have the correct multiarch uh jni uh jar C++ with hand assembly maybe don't",
    "start": "1661799",
    "end": "1670200"
  },
  {
    "text": "use that as your first Fay into this field um so I would stick to Java python. net or uh goang for any initial",
    "start": "1670200",
    "end": "1677480"
  },
  {
    "text": "experiments with 64 but we did it we had a working binary now",
    "start": "1677480",
    "end": "1683200"
  },
  {
    "text": "what uh well it turns out that actually validating and rolling it out and verifying that you get the performance",
    "start": "1683200",
    "end": "1688840"
  },
  {
    "text": "impacts uh that I'm telling you up on stage uh every workload is different your workload right like this is like",
    "start": "1688840",
    "end": "1694720"
  },
  {
    "text": "you know um previous performance is not indicative of of future Financial results right or um you know talk to",
    "start": "1694720",
    "end": "1700799"
  },
  {
    "text": "your doctor about whether this is right for you right like so just because I saw 30% to 50% price performance gain does",
    "start": "1700799",
    "end": "1707279"
  },
  {
    "text": "not mean you will so so uh how do we actually validate this and make the case that is worthwhile",
    "start": "1707279",
    "end": "1712480"
  },
  {
    "text": "switching so what we wound up doing was we wound up using that wonderful uh",
    "start": "1712480",
    "end": "1717559"
  },
  {
    "text": "infrastructure that our platform engineering team had built around continuous integration around uh get Ops",
    "start": "1717559",
    "end": "1723720"
  },
  {
    "text": "around terraform in order to be able to run a controlled experiment to trial",
    "start": "1723720",
    "end": "1730240"
  },
  {
    "text": "having one of our workers out of 30 running the new binary just to verify does it work at all and then after we",
    "start": "1730240",
    "end": "1737640"
  },
  {
    "text": "did uh verify that we also wanted to make sure that we had the Telemetry right we wanted to make sure that we",
    "start": "1737640",
    "end": "1743240"
  },
  {
    "text": "could compare and contrast the performance of arm 64 versus x86 but because we dog food it's you",
    "start": "1743240",
    "end": "1749360"
  },
  {
    "text": "know relatively easy for us to say okay I'm already using honeycomb or you know any open Telemetry compatible thing",
    "start": "1749360",
    "end": "1755880"
  },
  {
    "text": "really add a new attribute Right add a new attribute to all the trace spans that are being emitted that says which",
    "start": "1755880",
    "end": "1760960"
  },
  {
    "text": "architecture did you run on so once we had the data then we could start uh bumping up to 20% to get a",
    "start": "1760960",
    "end": "1767640"
  },
  {
    "text": "statistic signicant SLE and we could see okay does this per um and it did so we",
    "start": "1767640",
    "end": "1775279"
  },
  {
    "text": "went ahead and we hit the switch and we said we are going to migrate this prod workload as soon as Amazon declares its instance type uh generally",
    "start": "1775279",
    "end": "1782000"
  },
  {
    "text": "available so we did and we uh were migrating from spot instances to On",
    "start": "1782000",
    "end": "1788679"
  },
  {
    "text": "Demand uh arm instances and then we migrated from on demand arm instances to",
    "start": "1788679",
    "end": "1794720"
  },
  {
    "text": "uh spot arm instances as soon as they became available and you can see that the uh drop in in price is pretty",
    "start": "1794720",
    "end": "1801200"
  },
  {
    "text": "significant and this drop in price comes from several factors right number one it turns out that by just squeezing the",
    "start": "1801200",
    "end": "1807399"
  },
  {
    "text": "instances down and running 30% fewer instances we're able to you know uh",
    "start": "1807399",
    "end": "1813679"
  },
  {
    "text": "achieve significant cost savings from that and also each instance was cheaper on a per instance basis so that's well",
    "start": "1813679",
    "end": "1820240"
  },
  {
    "text": "well and good but can you do that to other services right what about the stateful ones that are uh what about the",
    "start": "1820240",
    "end": "1826919"
  },
  {
    "text": "stateful ones that are actually storing that data the ones that are writing to local SSD in that particular case we were",
    "start": "1826919",
    "end": "1833760"
  },
  {
    "text": "using a particularly ancient um SSD attached storage uh version of version",
    "start": "1833760",
    "end": "1840120"
  },
  {
    "text": "of of the uh it was the I3 series uh on",
    "start": "1840120",
    "end": "1845760"
  },
  {
    "text": "Amazon and we were here in this case we were not trying to make it cost less we were trying to make it perform more and",
    "start": "1845760",
    "end": "1853399"
  },
  {
    "text": "by migrating to uh to instance types that were better suited for workload that were uh much more performant per",
    "start": "1853399",
    "end": "1860960"
  },
  {
    "text": "core had twice as many cores and also had slightly less uh Solid State Storage",
    "start": "1860960",
    "end": "1867919"
  },
  {
    "text": "per instance but that was fine because we had started tearing since we uh since we originally picked the I3 instance",
    "start": "1867919",
    "end": "1874279"
  },
  {
    "text": "type so basically we chose to pay about 10% more per machine and got about a 3X",
    "start": "1874279",
    "end": "1881880"
  },
  {
    "text": "performance Improvement by kind of more fine-tuning our our uh our performance characteristics and getting the most",
    "start": "1881880",
    "end": "1888240"
  },
  {
    "text": "modern CPUs and this head really really helped us because previously we were bumping up",
    "start": "1888240",
    "end": "1894559"
  },
  {
    "text": "against the capacity limits of those instances here's a side note by the way",
    "start": "1894559",
    "end": "1900039"
  },
  {
    "text": "why is it that arm 64 performs about 30% better per core it actually isn't necessarily 30%",
    "start": "1900039",
    "end": "1907960"
  },
  {
    "text": "better performance per core it's 30% better performance per marketed uh",
    "start": "1907960",
    "end": "1913120"
  },
  {
    "text": "vcpu so the secret with Intel and AMD processors is that they are hyperthreading right",
    "start": "1913120",
    "end": "1920159"
  },
  {
    "text": "when you buy two vcpu that's actually two instruction decode units and it's",
    "start": "1920159",
    "end": "1926240"
  },
  {
    "text": "one actual uh processing and and and uh and compute execution unit what's",
    "start": "1926240",
    "end": "1933159"
  },
  {
    "text": "happening is Amazon and other providers are selling you those two instruction",
    "start": "1933159",
    "end": "1938960"
  },
  {
    "text": "decode units that if you're running the same workload you're contending yourself",
    "start": "1938960",
    "end": "1944240"
  },
  {
    "text": "for that actual execution whereas with arm there's no Hy threading one vcpu",
    "start": "1944240",
    "end": "1949559"
  },
  {
    "text": "equals one dedicated core with both decode and compute and that means that as you are starting to saturate the",
    "start": "1949559",
    "end": "1957000"
  },
  {
    "text": "instance you're not containing yourself and your tail latency doesn't Spike until the CPU saturation really gets",
    "start": "1957000",
    "end": "1962760"
  },
  {
    "text": "above 90% whereas an Intel instance or an AMD instance with hyperthreading is",
    "start": "1962760",
    "end": "1968519"
  },
  {
    "text": "going to wind up saturating its CPU when you start running it overall at about 65",
    "start": "1968519",
    "end": "1974440"
  },
  {
    "text": "or 70% CPU utilization because at that point the instruction decode units still",
    "start": "1974440",
    "end": "1979799"
  },
  {
    "text": "have plenty of Headroom but the uh actual compute unit within within the core is is fully",
    "start": "1979799",
    "end": "1987840"
  },
  {
    "text": "saturated so when we actually switched over from the I3 instance type to the m6",
    "start": "1988200",
    "end": "1993799"
  },
  {
    "text": "GD instance type with twice as many cores each core about 50% more efficient we are basically able to keep our",
    "start": "1993799",
    "end": "2000200"
  },
  {
    "text": "instance count the same as we wrote out about a 3X growth uh year on which is",
    "start": "2000200",
    "end": "2006120"
  },
  {
    "text": "pretty cool and then yeah had to horizontally scale outter Fleet but it doesn't just stop there",
    "start": "2006120",
    "end": "2012519"
  },
  {
    "text": "because it turns out that while we were in the background migrating from fifth generation Intel to sixth generation uh",
    "start": "2012519",
    "end": "2020840"
  },
  {
    "text": "arm 64 instances Amazon was busy working on their seventh generation arm 64",
    "start": "2020840",
    "end": "2026279"
  },
  {
    "text": "instances and this is a little bit unheard of in the Intel world where you know basically if you get 10 or 20%",
    "start": "2026279",
    "end": "2033000"
  },
  {
    "text": "Improvement generation over generation you're like Yay this is awesome but Amazon gave us just for free just by",
    "start": "2033000",
    "end": "2039039"
  },
  {
    "text": "incrementing that m6g to m7g we got a 30 to 40% Improvement in in",
    "start": "2039039",
    "end": "2045880"
  },
  {
    "text": "performance and throughput by it by by just changing from uh graviton 2 to graviton",
    "start": "2045880",
    "end": "2051760"
  },
  {
    "text": "3 and we saw the uh latency was slightly better about about 10 10 or 20% better",
    "start": "2051760",
    "end": "2058358"
  },
  {
    "text": "uh latency in addition to running 30% fewer instances and you know as you can see",
    "start": "2058359",
    "end": "2065560"
  },
  {
    "text": "that really adds up to a lot of savings because we able to process the same number of requests and to verify that",
    "start": "2065560",
    "end": "2073000"
  },
  {
    "text": "you know what's the Pod CPU how hot can I run this and is everything going according to",
    "start": "2073000",
    "end": "2078320"
  },
  {
    "text": "plan so over the course of about a year and a half uh we are able to migrate all",
    "start": "2078320",
    "end": "2083839"
  },
  {
    "text": "of our x86 workloads all of our server full x86 workloads onto server full arm",
    "start": "2083839",
    "end": "2090599"
  },
  {
    "text": "64 and we turn off the last instance of x86 in March of 2022",
    "start": "2090599",
    "end": "2098040"
  },
  {
    "text": "so observability really helped us kind of dial in on what was going on with her",
    "start": "2098040",
    "end": "2103119"
  },
  {
    "text": "performance but until uh until Amazon released uh arm 64 support for for",
    "start": "2103119",
    "end": "2109960"
  },
  {
    "text": "Lambda we didn't really have a solution for converting over our serverless functions but when they did have it",
    "start": "2109960",
    "end": "2115320"
  },
  {
    "text": "ready we were eager to adopt it and we tried it because hey why not free performance right like it's it it makes",
    "start": "2115320",
    "end": "2122520"
  },
  {
    "text": "everything 30% better it's magic right and then AB tested it at",
    "start": "2122520",
    "end": "2130720"
  },
  {
    "text": "50% and it broke is that well before we started",
    "start": "2130720",
    "end": "2136800"
  },
  {
    "text": "worrying about why is that we needed to First figure out okay what's you know uh how do I revert this right how do I",
    "start": "2136800",
    "end": "2143040"
  },
  {
    "text": "restore performance back to normal good thing was I'd thought ahead and I'd implemented it as a feature flag",
    "start": "2143040",
    "end": "2149880"
  },
  {
    "text": "so I was able to in less than about uh about 30 seconds I was able to swich",
    "start": "2149880",
    "end": "2156319"
  },
  {
    "text": "that feature off and store State normal so I could start unpacking and debugging and figure out what had gone",
    "start": "2156319",
    "end": "2162040"
  },
  {
    "text": "wrong so what had gone wrong um it turns out that continuous profiling is hugely",
    "start": "2162040",
    "end": "2167839"
  },
  {
    "text": "useful for getting to the bottom of things like this but in short uh one of the libraries that we that we were using",
    "start": "2167839",
    "end": "2174960"
  },
  {
    "text": "uh for compression because it turns out you don't want to store large uh large strings that are basically the same",
    "start": "2174960",
    "end": "2180920"
  },
  {
    "text": "string over and over and over again you don't want to store those uncompressed you want to compress them first so we use lz4 and the L 4 librar and go had",
    "start": "2180920",
    "end": "2190240"
  },
  {
    "text": "hand implemented assembly uh for x86 only it had a fallback implementation for arm but the fallback implementation",
    "start": "2190240",
    "end": "2196599"
  },
  {
    "text": "for arm was not that good so we had to Port the uh arm 32 implementation to arm",
    "start": "2196599",
    "end": "2202680"
  },
  {
    "text": "64 and then it just magically worked and got faster and also it turns out when Amazon",
    "start": "2202680",
    "end": "2208440"
  },
  {
    "text": "releases a new instance type when Amazon releases a new uh feature for Amazon Lambda like running on a new",
    "start": "2208440",
    "end": "2215000"
  },
  {
    "text": "backend if you suddenly scale from 0 to 100% or 0 to 50% without any warning to",
    "start": "2215000",
    "end": "2221000"
  },
  {
    "text": "Amazon and you're a significant user of that service you're going to wind up getting throttled because servers are",
    "start": "2221000",
    "end": "2227319"
  },
  {
    "text": "not magic they don't pop out of nowhere they need to be provisioned by Amazon and this is one case where you know it's",
    "start": "2227319",
    "end": "2233480"
  },
  {
    "text": "actually bare metal servers all the way down that actually does apply you can run your provider out of capacity",
    "start": "2233480",
    "end": "2239680"
  },
  {
    "text": "especially if it is uh it is the day after they've launched a new instance",
    "start": "2239680",
    "end": "2245000"
  },
  {
    "text": "type so you know we went back and talked Amazon said we'd like this many instances and they said um give us a",
    "start": "2245000",
    "end": "2250880"
  },
  {
    "text": "second how about you continue to run you know 50% or 30% on x86 for now and we'll",
    "start": "2250880",
    "end": "2256119"
  },
  {
    "text": "give give you those arm servers as soon as we can and the last thing was until go",
    "start": "2256119",
    "end": "2261440"
  },
  {
    "text": "118 um there was no support for the uh register calling convention on for for",
    "start": "2261440",
    "end": "2267880"
  },
  {
    "text": "go on arm64 which meant that the Intel instances had a significant advantage over the x86 instances because go had",
    "start": "2267880",
    "end": "2275079"
  },
  {
    "text": "not yet implemented the uh register convention for one of them but not the other turns out passing each function",
    "start": "2275079",
    "end": "2282119"
  },
  {
    "text": "call argument into the stack is a little bit expensive so that's all fixed now uh those are things you don't need to worry",
    "start": "2282119",
    "end": "2288079"
  },
  {
    "text": "about but the important thing is being able to debug them right being able to figure out where's this performance regr",
    "start": "2288079",
    "end": "2294240"
  },
  {
    "text": "coming from because it is a new architecture it is something that your",
    "start": "2294240",
    "end": "2299359"
  },
  {
    "text": "software may not have been tested against so it's important to be able to measure and optimize in the buug so",
    "start": "2299359",
    "end": "2305440"
  },
  {
    "text": "after we figured out what was going on we were able to flag it from you know 0% to 1% to 5% to 10% and working with",
    "start": "2305440",
    "end": "2312000"
  },
  {
    "text": "Amazon we were able to scale up to 99% why not at 100% because we want to be",
    "start": "2312000",
    "end": "2317280"
  },
  {
    "text": "sure that we can continue to run this workload on x86 just in case so that's what that looks like",
    "start": "2317280",
    "end": "2323920"
  },
  {
    "text": "right that's the adoption curve of going from 0% to 100% arm um on the serverless",
    "start": "2323920",
    "end": "2329760"
  },
  {
    "text": "side is a lot of thoughtful experimentation a lot of capacity planning and a fair bit of performance",
    "start": "2329760",
    "end": "2335400"
  },
  {
    "text": "engineering with the result that that that three to four times as expensive per CPU second that's less of a worry",
    "start": "2335400",
    "end": "2342720"
  },
  {
    "text": "for us now because we've been able to benefit from that 30% performance uh improvement from running arm versus",
    "start": "2342720",
    "end": "2350720"
  },
  {
    "text": "x86 and this is what it looks like in practice um the leny is about comparable",
    "start": "2350720",
    "end": "2356000"
  },
  {
    "text": "and the reason that the leny is about comparable is that I've taken each Lambda worker that would have been",
    "start": "2356000",
    "end": "2361720"
  },
  {
    "text": "running on x86 and I've Shrunk the CPU in memory by 30% so that's why the ly looks like it's inding",
    "start": "2361720",
    "end": "2368400"
  },
  {
    "text": "it turns out that I'm I'm cheating a little bit by just running smaller workers so my message to you is yes do",
    "start": "2368400",
    "end": "2376040"
  },
  {
    "start": "2373000",
    "end": "2610000"
  },
  {
    "text": "this at home with a couple of caveats first of all if you're going to use serverless functions to uh to use as",
    "start": "2376040",
    "end": "2383200"
  },
  {
    "text": "the back end of your real-time database you're going to need to first move that state from local machines into object",
    "start": "2383200",
    "end": "2389000"
  },
  {
    "text": "storage so that serverless functions can even query it in the first place to generate a list of objects uh and to",
    "start": "2389000",
    "end": "2395440"
  },
  {
    "text": "generate kind of work units you don't want to process one file or one segment per per Lambda you might want to process",
    "start": "2395440",
    "end": "2401240"
  },
  {
    "text": "like a batch of eight or 10 of them that way the startup cost and the invoke cost is amortised you can use the serverless",
    "start": "2401240",
    "end": "2407680"
  },
  {
    "text": "functions to beautifully paralyze object processing although as I said be worry of the generation of the invokes make",
    "start": "2407680",
    "end": "2413680"
  },
  {
    "text": "sure that you've profiled that that that is as quick as you can make it and then finally you may need to reduce the",
    "start": "2413680",
    "end": "2419440"
  },
  {
    "text": "results out say of Lambda afterwards and kind of merge those uh tens of thousands of workers those uh potentially hundreds",
    "start": "2419440",
    "end": "2425520"
  },
  {
    "text": "of thousands or millions of uh of invoke requests when the result comes back you're probably going to want to merge",
    "start": "2425520",
    "end": "2431720"
  },
  {
    "text": "it together into a single uh result where are the dragons um if you",
    "start": "2431720",
    "end": "2437200"
  },
  {
    "text": "have ly insensitive batch workloads those are better suited for ec2 instances right like you can run those",
    "start": "2437200",
    "end": "2443079"
  },
  {
    "text": "with aw spot uh you can kind of save and resume them you can tolerate the two to",
    "start": "2443079",
    "end": "2448160"
  },
  {
    "text": "three minute start up time of a ec2 instance so why run it on Lambda and",
    "start": "2448160",
    "end": "2453960"
  },
  {
    "text": "return a result within seconds that you're paying three to four times as much for when you could just uh wait for",
    "start": "2453960",
    "end": "2459520"
  },
  {
    "text": "an instance to spin up as long as you're willing to wait potentially you know minutes or hours for the",
    "start": "2459520",
    "end": "2464760"
  },
  {
    "text": "result as I was saying don't send only one or two files to scan to a to a Lambda they have a large uh startup time",
    "start": "2464760",
    "end": "2472359"
  },
  {
    "text": "in particular one of the things that we found is that our workers until we uh preached the SSL certificate validation",
    "start": "2472359",
    "end": "2479680"
  },
  {
    "text": "our workers were spending something like 5% of their time just handshaking with S3 over",
    "start": "2479680",
    "end": "2485319"
  },
  {
    "text": "htps so you kind of want to start uh with kind of these larger batches to",
    "start": "2485319",
    "end": "2490880"
  },
  {
    "text": "kind of advertise setup costs and then see what you can do to reduce those setup costs and check with your cloud",
    "start": "2490880",
    "end": "2496839"
  },
  {
    "text": "provider be in sync with your account rep if you're going to do this state your intentions to them so they can provision capacity they're usually more",
    "start": "2496839",
    "end": "2502720"
  },
  {
    "text": "than willing to do so you just can't turn from zero to 100% all at once and finally you know even if you're",
    "start": "2502720",
    "end": "2509000"
  },
  {
    "text": "not willing if you're not ready to do this now you should still futureproof arm is here your Mac laptops are running",
    "start": "2509000",
    "end": "2516640"
  },
  {
    "text": "a version of the y instruction set so you already need to be testing cross compilation so you may as well do it now",
    "start": "2516640",
    "end": "2523560"
  },
  {
    "text": "don't assume the whole world runs on x86 because it no longer does the most cost efficient processors are going to be",
    "start": "2523560",
    "end": "2531200"
  },
  {
    "text": "arm and before you actually scale out fully right like make sure that you're tuning the uh batch size per invoke make",
    "start": "2531200",
    "end": "2537359"
  },
  {
    "text": "sure that you're uh running profiling make sure that you have adequate observability to tell if there's",
    "start": "2537359",
    "end": "2543040"
  },
  {
    "text": "regression for architecture and measure your metrics carefully and have cost so that you can have an idea of what's",
    "start": "2543040",
    "end": "2549920"
  },
  {
    "text": "going on and which customers workloads you might want to uh throttle or move to a different uh back",
    "start": "2549920",
    "end": "2555559"
  },
  {
    "text": "end so arm 64 is a huge cost in Planet saver but you really have to think about",
    "start": "2555559",
    "end": "2561200"
  },
  {
    "text": "okay what do I need to do to get ready well CI pipelines uh multiarch uh kubernetes clusters I was hugely",
    "start": "2561200",
    "end": "2567880"
  },
  {
    "text": "relieved actually that we never had to go through multiarch kubernetes because we were we migrated from ec2 x86 to ec2",
    "start": "2567880",
    "end": "2576240"
  },
  {
    "text": "arm64 and therefore we didn't have a mixture of architectures but I know that's probably not going to be the reality for",
    "start": "2576240",
    "end": "2581960"
  },
  {
    "text": "most of you so you may have to experiment with kubernetes constraints um and then deploy to production and",
    "start": "2581960",
    "end": "2587359"
  },
  {
    "text": "really really measure the results kind of hammer home the uh cost savings that you're going to get because yes it is",
    "start": "2587359",
    "end": "2593559"
  },
  {
    "text": "slightly more complex to maintain different build architectures but it's well worth it if you can cut your company's Cloud bill by 20 or",
    "start": "2593559",
    "end": "2600720"
  },
  {
    "text": "30% and at the end of the day uh nothing matters unless users are happy and in this case that is a very happy dog so",
    "start": "2600720",
    "end": "2608359"
  },
  {
    "text": "happy when the dog's happy uh if you'd like to learn more about our query backend um you can go",
    "start": "2608359",
    "end": "2613880"
  },
  {
    "start": "2610000",
    "end": "2642000"
  },
  {
    "text": "and have a look at uh observability engineering it's being sold in the bookshelf in the Bookshop uh and you can",
    "start": "2613880",
    "end": "2619240"
  },
  {
    "text": "also get a free copy online and with that um I think that is that's all that",
    "start": "2619240",
    "end": "2624839"
  },
  {
    "text": "I have for you thank you very much",
    "start": "2624839",
    "end": "2629200"
  }
]