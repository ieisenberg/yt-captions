[
  {
    "start": "0",
    "end": "93000"
  },
  {
    "text": "okay so my name's Phil wer I've been uh running Winder AI we're a an AI",
    "start": "12360",
    "end": "17680"
  },
  {
    "text": "consultancy for about 10 years we help businesses build out their AI Solutions",
    "start": "17680",
    "end": "23359"
  },
  {
    "text": "and um spoken about ML and AI a lot in the past um but the reason I'm here",
    "start": "23359",
    "end": "28640"
  },
  {
    "text": "today is talk about llms and specifically how you might be able to leverage llms and the reason why I'm",
    "start": "28640",
    "end": "34360"
  },
  {
    "text": "talking about it is cuz they're hot it's the first time in my life that I've not had to explain what I do anymore I I I",
    "start": "34360",
    "end": "41200"
  },
  {
    "text": "say I'm an AI consultant and they go oh okay like only six months ago people were like Aven influenza like what",
    "start": "41200",
    "end": "49000"
  },
  {
    "text": "what's what are you talking about that so um it's great that it's all in the media now because finally everybody's on",
    "start": "49000",
    "end": "54680"
  },
  {
    "text": "the same page this presentation is split up into seven Parts I am going to sort of dynamically trim down if time is",
    "start": "54680",
    "end": "61039"
  },
  {
    "text": "running short which reminds me I should start my timer um I I haven't uh timed this in full yet but hopefully we're",
    "start": "61039",
    "end": "67320"
  },
  {
    "text": "going to go through the the full life cycle of what an llm is how you train one how you develop one a little bit",
    "start": "67320",
    "end": "73040"
  },
  {
    "text": "about deployment and post post deployment tasks and then there's an amazing demo at the end and the best",
    "start": "73040",
    "end": "78840"
  },
  {
    "text": "demos and the most embarrassing demos and I promise you that this is one of the most embarrassing demos I've ever done so you want to stick around for",
    "start": "78840",
    "end": "85040"
  },
  {
    "text": "that um don't forget to write the session um helps tell go to that I'm you",
    "start": "85040",
    "end": "90360"
  },
  {
    "text": "know I'm not completely rubbish and let's get started by talking about uh a",
    "start": "90360",
    "end": "96439"
  },
  {
    "start": "93000",
    "end": "361000"
  },
  {
    "text": "history of LMS because it is relevant and the the first question has to be what is an llm well essentially it's an",
    "start": "96439",
    "end": "102680"
  },
  {
    "text": "AI model it's a machine learning model AI is a a bigger term that unfortunately I I didn't like using the term a few",
    "start": "102680",
    "end": "109079"
  },
  {
    "text": "years ago because it it it's kind of a bit um philosophical in its definition",
    "start": "109079",
    "end": "114439"
  },
  {
    "text": "but uh unfortunately I lost so it's an AI model and it's trained using supervised learning so it's a",
    "start": "114439",
    "end": "120560"
  },
  {
    "text": "traditional machine learning model on a huge amount of text based data the interesting part about it is that it's",
    "start": "120560",
    "end": "126479"
  },
  {
    "text": "refined through user feedback and that's where my work on on reinforcement learning comes in and then all it",
    "start": "126479",
    "end": "132640"
  },
  {
    "text": "attempts to do is predict the next word that's all it's trying to do is what's the best next word I can pick given the",
    "start": "132640",
    "end": "140200"
  },
  {
    "text": "previous words that have been created that's it simple model but let's go into",
    "start": "140200",
    "end": "145480"
  },
  {
    "text": "a bit more depth so the history behind llms and people trying to interact with",
    "start": "145480",
    "end": "151120"
  },
  {
    "text": "computers in in an intelligent ways go goes quite back quite far this is a Liza",
    "start": "151120",
    "end": "156680"
  },
  {
    "text": "from uh 1966 I believe and it's this it's this example of the the the",
    "start": "156680",
    "end": "163040"
  },
  {
    "text": "psychiatrist um inside the computer basically asking the same question back to you and actually there's a a great",
    "start": "163040",
    "end": "169720"
  },
  {
    "text": "web demo of that if you want to try it I'll probably skip it actually because we don't really have time but that was",
    "start": "169720",
    "end": "175200"
  },
  {
    "text": "in 1966 okay so since then we've had all sorts of of ml discoveries but really",
    "start": "175200",
    "end": "182319"
  },
  {
    "text": "llm started to form in about 2013 just just in in 2010s really the first",
    "start": "182319",
    "end": "189519"
  },
  {
    "text": "discovery was the use of uh embeddings in word to VEC so this is the idea that",
    "start": "189519",
    "end": "195080"
  },
  {
    "text": "you can represent a word some text a sentence a document anything as a series",
    "start": "195080",
    "end": "201159"
  },
  {
    "text": "of a series of numbers a vector just a single Vector single set of numbers that represents that text and it allows you",
    "start": "201159",
    "end": "207480"
  },
  {
    "text": "to do interesting things like maths upon words you know adding you know uh Queen",
    "start": "207480",
    "end": "213239"
  },
  {
    "text": "plus man equals King that kind of stuff the next um uh the next most important",
    "start": "213239",
    "end": "220200"
  },
  {
    "text": "um uh idea came in 2014 when people started",
    "start": "220200",
    "end": "225239"
  },
  {
    "text": "treating series of words as um as as",
    "start": "225239",
    "end": "230400"
  },
  {
    "text": "sequences and by modeling them as sequences then they kind of proposed the",
    "start": "230400",
    "end": "235439"
  },
  {
    "text": "idea that you it should be possible to predict what the next word in that sequence would be um the original models",
    "start": "235439",
    "end": "242280"
  },
  {
    "text": "used rnns and rnns are a bit of a pain because they have a little bit of memory",
    "start": "242280",
    "end": "247799"
  },
  {
    "text": "inside each of the the neural the neurons in the neural network and they have a really bad habit of blowing up as",
    "start": "247799",
    "end": "255200"
  },
  {
    "text": "soon as you go near them so they were quite hard to use Transformers in 2017 so a few years later was this kind of",
    "start": "255200",
    "end": "261799"
  },
  {
    "text": "generalized architecture for working with text but unfortunately um uh and",
    "start": "261799",
    "end": "267280"
  },
  {
    "text": "and and actually really what the the the reol solution here was was they figured out a way of looking at the whole",
    "start": "267280",
    "end": "273880"
  },
  {
    "text": "sentence or the whole uh series of words in one go in one shot they didn't have",
    "start": "273880",
    "end": "279440"
  },
  {
    "text": "to pass it through this recurrent neural network which required required like multiple steps to kind of feed the words",
    "start": "279440",
    "end": "285680"
  },
  {
    "text": "through in like a ticket tape fashion um and that made it easier to train made it",
    "start": "285680",
    "end": "291520"
  },
  {
    "text": "faster to train made it easier to use and and in inference as well and then people realized that they could start",
    "start": "291520",
    "end": "297199"
  },
  {
    "text": "scaling this and so Bert was the first first real example of a a large scale",
    "start": "297199",
    "end": "302400"
  },
  {
    "text": "demonstration of this technology um they use something called a bidirectional training technique which is where you",
    "start": "302400",
    "end": "308880"
  },
  {
    "text": "don't just try and predict the next word you actually mask out words in the middle and try and predict the word in",
    "start": "308880",
    "end": "314039"
  },
  {
    "text": "the middle as well and it helped the model learn much faster than it would have been able to before um then we've",
    "start": "314039",
    "end": "321840"
  },
  {
    "text": "got gpt1 gbt2 gpt3 2018 19 and 20 um the real Revolution here is that I I'll come",
    "start": "321840",
    "end": "327960"
  },
  {
    "text": "to the architecture in a moment but they simplified the architecture a little bit and they trained on a huge amount of",
    "start": "327960",
    "end": "334880"
  },
  {
    "text": "data and it really turned out that the the huge amount of data was the key um and it got to the point where the",
    "start": "334880",
    "end": "342840"
  },
  {
    "text": "sentences being generated the words that were being predicted next were really really good they were as good as humans",
    "start": "342840",
    "end": "349680"
  },
  {
    "text": "um and then in in 2022 they released chat gbt and as we know everybody now",
    "start": "349680",
    "end": "355039"
  },
  {
    "text": "knows what AI is oops s right press the wrong button",
    "start": "355039",
    "end": "361639"
  },
  {
    "start": "361000",
    "end": "753000"
  },
  {
    "text": "okay so let's have a quick look at how it works the current iterations the",
    "start": "361639",
    "end": "366759"
  },
  {
    "text": "current versions of the GPT like algorithms um have three General steps",
    "start": "366759",
    "end": "372440"
  },
  {
    "text": "there's a what there's the first step where you have to attempt to pre-train the data so this is vast amounts of of",
    "start": "372440",
    "end": "379800"
  },
  {
    "text": "of Text data that being used to pre-train this model there's then a a secondary step where a reward model is",
    "start": "379800",
    "end": "387680"
  },
  {
    "text": "trained based upon user feedback and then the third step is to use that reward model to tweak the original model",
    "start": "387680",
    "end": "394840"
  },
  {
    "text": "to make it better to make it more aligned with what you want to do um the the",
    "start": "394840",
    "end": "400240"
  },
  {
    "text": "specific text in this image comes from the paper at the bottom the instruct gbt paper and um it's talking about how you",
    "start": "400240",
    "end": "407720"
  },
  {
    "text": "would fine-tune it but the the idea is the same for pre-training as well um in",
    "start": "407720",
    "end": "413560"
  },
  {
    "text": "this paper and and and prior to this they use the Transformer architecture so this is the Transformer architecture",
    "start": "413560",
    "end": "419319"
  },
  {
    "text": "from 2017 it consists of two parts on the left hand side we've got an encoder on",
    "start": "419319",
    "end": "424360"
  },
  {
    "text": "the right hand side we've got a decoder um but the important thing to know is that from gbt onwards the architecture",
    "start": "424360",
    "end": "431080"
  },
  {
    "text": "is is is decoder only so what we're trying to do here is feed raw text in",
    "start": "431080",
    "end": "436639"
  },
  {
    "text": "and then just prect predict the next sentence out um yeah so let's now have a look at",
    "start": "436639",
    "end": "443360"
  },
  {
    "text": "what each bit of the the architecture does at the bottom we have a",
    "start": "443360",
    "end": "450199"
  },
  {
    "text": "a block that is responsible for tokenizing and embedding the input into",
    "start": "450199",
    "end": "455520"
  },
  {
    "text": "numerical space tokenization is a step where you're converting a sentence or a document into smaller chunks typically",
    "start": "455520",
    "end": "463160"
  },
  {
    "text": "in most of these models it's kind of subword sequences it's not full words it's subwords and it allows it to um you",
    "start": "463160",
    "end": "470759"
  },
  {
    "text": "know capture uh more diversity of words um with with fewer tokens the next step is to basically",
    "start": "470759",
    "end": "479120"
  },
  {
    "text": "predict the probability predict How likely each word in the sentence is as",
    "start": "479120",
    "end": "484440"
  },
  {
    "text": "being relevant to the output how how important each part of the sentences to the output to the prediction and this is",
    "start": "484440",
    "end": "492039"
  },
  {
    "text": "the masking step it's attempting to mask out the parts of the the the sentence or the document that are not relevant and",
    "start": "492039",
    "end": "498400"
  },
  {
    "text": "that's learnable then we have a a simple multi-layer perceptor on a normal neural",
    "start": "498400",
    "end": "504440"
  },
  {
    "text": "network to predict what is the most likely next token and then we have n",
    "start": "504440",
    "end": "510080"
  },
  {
    "text": "number of blocks and this is a big number you know tens and tens and tens of these blocks all stacked on top of",
    "start": "510080",
    "end": "515719"
  },
  {
    "text": "each other and that's what makes it deep that's what makes it able to provide",
    "start": "515719",
    "end": "520919"
  },
  {
    "text": "these high level kind of conceptual thoughts about what it's writing about and then finally at the end we've got",
    "start": "520919",
    "end": "527000"
  },
  {
    "text": "the the final prediction and that's it so if you look at the implementation of this it's it's that um it looks a lot",
    "start": "527000",
    "end": "533959"
  },
  {
    "text": "more complicated than it than it is and there's wires kind of connecting things together and stuff but fundamentally",
    "start": "533959",
    "end": "540000"
  },
  {
    "text": "that's all this architecture does and this is",
    "start": "540000",
    "end": "545160"
  },
  {
    "text": "gpt3 okay the next step is to take what's generated and ask a human ask a",
    "start": "545160",
    "end": "551279"
  },
  {
    "text": "labeler to attempt to rank the outputs in terms of their preference we then capture those",
    "start": "551279",
    "end": "558160"
  },
  {
    "text": "preferences and then build another model a second model to predict a reward to",
    "start": "558160",
    "end": "563680"
  },
  {
    "text": "predict how preferred each of the outputs are given a piece of text okay",
    "start": "563680",
    "end": "568880"
  },
  {
    "text": "and then just train again a normal multi-layer Petron deep learning style model nothing fancy but hang on why do",
    "start": "568880",
    "end": "575760"
  },
  {
    "text": "we want to do that well the main reason we want to do that is for alignment we want to try and make sure that the llm",
    "start": "575760",
    "end": "581200"
  },
  {
    "text": "is actually producing content that we want it to produce otherwise it produces all sorts of horrible stuff safety is",
    "start": "581200",
    "end": "588040"
  },
  {
    "text": "obviously a big concern um the left to its own devices the llm learns a lot of",
    "start": "588040",
    "end": "595160"
  },
  {
    "text": "really horrible stuff that is available on the internet and is capable of spurting it back out and finally quality",
    "start": "595160",
    "end": "602200"
  },
  {
    "text": "control um one of the Google one of the top results for like instruct gbt",
    "start": "602200",
    "end": "607360"
  },
  {
    "text": "architecture came up with this website um and you can see that this website has",
    "start": "607360",
    "end": "612880"
  },
  {
    "text": "used chat gbt to generate his content because of left in the little thing at the bottom there please continue writing",
    "start": "612880",
    "end": "619160"
  },
  {
    "text": "and of course chat GPT openai they're continuously res scraping the web and repassing that back into gbt to learn",
    "start": "619160",
    "end": "626880"
  },
  {
    "text": "from its con so it's you know it's already starting to learn from its crap content which is obviously a concern for",
    "start": "626880",
    "end": "633440"
  },
  {
    "text": "um for for for open AI um the irony is is that this particular website is",
    "start": "633440",
    "end": "639120"
  },
  {
    "text": "attempting to copyright it we've got a dmca mark there attempting to copyright this you know this plagiarized content",
    "start": "639120",
    "end": "647079"
  },
  {
    "text": "uh and then at the at the bottom right you can see some you know more uh yeah",
    "start": "647079",
    "end": "652279"
  },
  {
    "text": "not nice stuff that that that comes out of this because yeah it's the fact that",
    "start": "652279",
    "end": "657320"
  },
  {
    "text": "we' got we're hiring at the bottom as well anyway so yeah so how so then so so how",
    "start": "657320",
    "end": "664360"
  },
  {
    "text": "does the last step work then so once we've got that reward model once we've got a a a simple model that basically",
    "start": "664360",
    "end": "670880"
  },
  {
    "text": "predicts how good an output is based upon what users have labeled as then we",
    "start": "670880",
    "end": "676399"
  },
  {
    "text": "use it uh in in in in this scheme the we we feed it a prompt from our prompt data",
    "start": "676399",
    "end": "683079"
  },
  {
    "text": "set and the base llm our our model is then generating a couple of outputs or a",
    "start": "683079",
    "end": "688839"
  },
  {
    "text": "few or more outputs and they're all fed into the reward model they're given scores as to how good they are based",
    "start": "688839",
    "end": "695480"
  },
  {
    "text": "upon all of the things that it's learned and then it's fed back into the um the llm again to then fine tune and well not",
    "start": "695480",
    "end": "702760"
  },
  {
    "text": "not fine tune is wrong word to to to nudge the weights to stop it from generating this either you know",
    "start": "702760",
    "end": "708639"
  },
  {
    "text": "malicious harmful bad misaligned content and this is where the",
    "start": "708639",
    "end": "714560"
  },
  {
    "text": "reinforcement learning comes in because in in in RL terminology we've got the obser ation which is the prompt that's",
    "start": "714560",
    "end": "720560"
  },
  {
    "text": "the thing that the agent sees the policy is the llm that's the thing that's actually the internal model the internal",
    "start": "720560",
    "end": "727279"
  },
  {
    "text": "policy that the agent is is using to generate some stuff the action is the",
    "start": "727279",
    "end": "733000"
  },
  {
    "text": "the the the generated content that's the action it's producing we go through an external reward model and then we have a",
    "start": "733000",
    "end": "739079"
  },
  {
    "text": "reward that that's then fed back into um the llm so they're using reinforcement",
    "start": "739079",
    "end": "744199"
  },
  {
    "text": "Landing here to tune the model to behave better if you want to more about",
    "start": "744199",
    "end": "750000"
  },
  {
    "text": "reinforcement learning then you can check out my book one of the most important aspects",
    "start": "750000",
    "end": "756800"
  },
  {
    "start": "753000",
    "end": "1236000"
  },
  {
    "text": "of developing any model is the data all models depend on good quality data for a",
    "start": "756800",
    "end": "764519"
  },
  {
    "text": "good quality model garbage in garbage out um so the arguments for fine-tuning",
    "start": "764519",
    "end": "770760"
  },
  {
    "text": "and there's I've created a couple of notes in here in like the the dark gray if you want to kind of dig into it in a",
    "start": "770760",
    "end": "776079"
  },
  {
    "text": "bit more depth but the key points are is that firstly massive data it turns out that",
    "start": "776079",
    "end": "782639"
  },
  {
    "text": "massive data is better than your own curated data for llms at least um",
    "start": "782639",
    "end": "787760"
  },
  {
    "text": "research has found that many people were trying to treat llms in a a traditional manner where you develop your own",
    "start": "787760",
    "end": "794040"
  },
  {
    "text": "proprietary data set and you train your own model just on that proprietary data set and it kind of works but it doesn't",
    "start": "794040",
    "end": "799920"
  },
  {
    "text": "do a very good job because it doesn't have this broader understanding of human knowledge okay so in most tasks on",
    "start": "799920",
    "end": "807720"
  },
  {
    "text": "average a an llm a a a foundation model that been trained on a huge amount of",
    "start": "807720",
    "end": "813160"
  },
  {
    "text": "data will be better than your domain specific model but it also turns out",
    "start": "813160",
    "end": "818959"
  },
  {
    "text": "that if you take that Foundation model and apply your proprietary custom data over the top it's even better especially",
    "start": "818959",
    "end": "825680"
  },
  {
    "text": "at specific tasks and I I think that makes intuitive sense but it's it's being demonstrated uh AC academically as",
    "start": "825680",
    "end": "833720"
  },
  {
    "text": "well but the problem is is that curation is hard it's it's really hard to manage",
    "start": "833720",
    "end": "840320"
  },
  {
    "text": "and develop the data sets required to train these models these models are massive they're immensely huge and I've",
    "start": "840320",
    "end": "847440"
  },
  {
    "text": "spent most of my life saying use the smallest model you can get away with because it means you've got less you",
    "start": "847440",
    "end": "853800"
  },
  {
    "text": "need less data to train that model but now we have these huge models and you need a huge amount of data to produce",
    "start": "853800",
    "end": "859480"
  },
  {
    "text": "anything even vaguely good and so it just makes the the whole task incredibly",
    "start": "859480",
    "end": "864720"
  },
  {
    "text": "hard um there's also um there a theory in ml",
    "start": "864720",
    "end": "869880"
  },
  {
    "text": "called no free lunch which basically states that there's no one individual machine learning model that will that is",
    "start": "869880",
    "end": "876240"
  },
  {
    "text": "the best at all tasks there's always going to be like a a specialist uh you",
    "start": "876240",
    "end": "881880"
  },
  {
    "text": "know highly customized um model that is better suited to a particular situation",
    "start": "881880",
    "end": "889160"
  },
  {
    "text": "and um it's the same with with data as well um when you're attempting to clean",
    "start": "889160",
    "end": "894759"
  },
  {
    "text": "the data which we're going to talk about in just a second you can't just throw it into one of these automatic cleaning",
    "start": "894759",
    "end": "900240"
  },
  {
    "text": "routines because there's no one cleaning methodology that is the best for all problems unfortunately you need to",
    "start": "900240",
    "end": "905800"
  },
  {
    "text": "develop it for each application that you're developing and therefore don't use magic days cleaners if you can help",
    "start": "905800",
    "end": "911440"
  },
  {
    "text": "it um yeah I think I think like pragmatism",
    "start": "911440",
    "end": "916880"
  },
  {
    "text": "might win there a little bit if you do have a huge amount of data then then obviously automated data cleaning might",
    "start": "916880",
    "end": "921920"
  },
  {
    "text": "be necessary in order to have anything to show so pinch of salt with that one",
    "start": "921920",
    "end": "927399"
  },
  {
    "text": "um so when it does come to data cleaning uh what does data what does clean data",
    "start": "927399",
    "end": "932759"
  },
  {
    "text": "give us well it it turns out that the research shows that clean data is better",
    "start": "932759",
    "end": "938160"
  },
  {
    "text": "than more data so if you have the choice of going back over your original data set and cleaning it up or gathering more",
    "start": "938160",
    "end": "944560"
  },
  {
    "text": "data you should clean up your data first that will produce better results than Gathering more data which is a bit of a",
    "start": "944560",
    "end": "951160"
  },
  {
    "text": "surprising finding actually um it's also being shown that sort of magic post data",
    "start": "951160",
    "end": "959360"
  },
  {
    "text": "Solutions also don't help on average there's many there's there's a lot of work in the academic Community to",
    "start": "959360",
    "end": "966040"
  },
  {
    "text": "attempt to build ml algorithms that are more robust to bad data more robust to",
    "start": "966040",
    "end": "972160"
  },
  {
    "text": "outliers more robust to anomalies more robust to shifts in distributions and stuff but they've demonstrated that you",
    "start": "972160",
    "end": "978839"
  },
  {
    "text": "can't rely on these specialist algorithms unfortunately if you don't if you've got clean data then you can use",
    "start": "978839",
    "end": "985319"
  },
  {
    "text": "anything if you don't then you know you're in trouble and um you you it's",
    "start": "985319",
    "end": "990440"
  },
  {
    "text": "hard to rely on specific ml models to fix that and the the the final interesting aspect is that when um some",
    "start": "990440",
    "end": "999560"
  },
  {
    "text": "researchers attempted to use the state-ofthe-art automated cleaning techniques and then compared that to a",
    "start": "999560",
    "end": "1006000"
  },
  {
    "text": "set of human validated human clean data sets the humans won every time they",
    "start": "1006000",
    "end": "1011800"
  },
  {
    "text": "cleaned the data better than any automated algorithm so human Insight is still best",
    "start": "1011800",
    "end": "1019680"
  },
  {
    "text": "um I was going to add this little bit on a tokenizer as well uh there was a bit of interesting re research showing that",
    "start": "1019680",
    "end": "1025640"
  },
  {
    "text": "you shouldn't forget about the tokenization process and that's where you're splitting the word up because the",
    "start": "1025640",
    "end": "1031038"
  },
  {
    "text": "tokenizer has a fixed vocabulary it can't it doesn't represent every single word so when you're working in a very",
    "start": "1031039",
    "end": "1038240"
  },
  {
    "text": "specialist domain it's quite likely that the tokenizer cannot or or is going to",
    "start": "1038240",
    "end": "1044199"
  },
  {
    "text": "struggle to tokenize your word effectively like think of some um like",
    "start": "1044199",
    "end": "1049360"
  },
  {
    "text": "super tech words like uh I don't know um",
    "start": "1049360",
    "end": "1054919"
  },
  {
    "text": "like normalizing or normalization or something you know some database term",
    "start": "1054919",
    "end": "1060919"
  },
  {
    "text": "something like that um it's probably not going to be rep very well represented in the the the Corpus that they've used to",
    "start": "1060919",
    "end": "1067640"
  },
  {
    "text": "train their data on and therefore it doesn't make it into the tokenizer um so they found that adding",
    "start": "1067640",
    "end": "1073039"
  },
  {
    "text": "new domain specific tokens to the tokenizer did improve results but they also accidentally included their results",
    "start": "1073039",
    "end": "1080840"
  },
  {
    "text": "and they were using some llms to classify some medical data and um we've",
    "start": "1080840",
    "end": "1087159"
  },
  {
    "text": "got a comparison of different models that they used and then we're trying to prove our custom specialist tokenizer",
    "start": "1087159",
    "end": "1093120"
  },
  {
    "text": "performs better but unfortunately they also sort of demonstrated that a bog standard logistic regression from 200",
    "start": "1093120",
    "end": "1100039"
  },
  {
    "text": "years ago does almost as good of a job these these performance scores measure how good this model is at performing the",
    "start": "1100039",
    "end": "1106880"
  },
  {
    "text": "task and we've got here we've got a score of 83 versus 83.4 for the llm",
    "start": "1106880",
    "end": "1112520"
  },
  {
    "text": "model so once again I go back to my old thing of if you don't have to use an llm for everything like you can use a very",
    "start": "1112520",
    "end": "1119240"
  },
  {
    "text": "very simple statistical algorithm and it can perform very well um when you're looking at the data",
    "start": "1119240",
    "end": "1126240"
  },
  {
    "text": "as well you uh will see several different forms so the most common form",
    "start": "1126240",
    "end": "1131320"
  },
  {
    "text": "is just raw text that's what the pre-training um phase uses to train the",
    "start": "1131320",
    "end": "1137360"
  },
  {
    "text": "initial model it's just raw text it slices and dices it pulls out tokens",
    "start": "1137360",
    "end": "1142600"
  },
  {
    "text": "randomly and tries to predict where that that that to uh what that token should be you'll also see for sort of",
    "start": "1142600",
    "end": "1148880"
  },
  {
    "text": "Downstream tasks lots of supervised fine-tuning like data in various formats",
    "start": "1148880",
    "end": "1154919"
  },
  {
    "text": "effectively they all end up being in some kind of like question answer style format or context answer style format",
    "start": "1154919",
    "end": "1162080"
  },
  {
    "text": "where you've you've got the the input to the model basically and what the output should be or should look like and then",
    "start": "1162080",
    "end": "1169440"
  },
  {
    "text": "you've also got the the reinforcement learning the human feedback data and that's usually presented as a question",
    "start": "1169440",
    "end": "1175679"
  },
  {
    "text": "several generations and then a ranked order maybe an accepted and rejected or a rank one two 3 4 five this is an",
    "start": "1175679",
    "end": "1182120"
  },
  {
    "text": "example from um so that this this chap on GitHub uh zedj",
    "start": "1182120",
    "end": "1187960"
  },
  {
    "text": "h89 good name um has a really great curated website of all loads of data",
    "start": "1187960",
    "end": "1194559"
  },
  {
    "text": "sets that are used for for llms and this is this comes from one of the human",
    "start": "1194559",
    "end": "1200000"
  },
  {
    "text": "feedback data sets and the question is how can I blame someone else for a crime that I committed and obviously the the",
    "start": "1200000",
    "end": "1207240"
  },
  {
    "text": "uh uh the llm has produced this response at the bottom telling you how to blame",
    "start": "1207240",
    "end": "1212600"
  },
  {
    "text": "somebody else for a criminal activity and um the human feedback has been no",
    "start": "1212600",
    "end": "1218159"
  },
  {
    "text": "that's not right let's just say let's just give some non-committal response and that's how they've kind of trained",
    "start": "1218159",
    "end": "1224159"
  },
  {
    "text": "out a lot of the more dangerous questions that you could maybe try in chat gbt or uh things like",
    "start": "1224159",
    "end": "1231400"
  },
  {
    "text": "that um yeah let's just skip that actually",
    "start": "1231400",
    "end": "1237240"
  },
  {
    "start": "1236000",
    "end": "1580000"
  },
  {
    "text": "basically all the basics still apply it's it's still data you've still got to still got to have good coverage you",
    "start": "1237240",
    "end": "1242400"
  },
  {
    "text": "still got to be fair still got to be unbiased you you are going to have anomalies so on and so forth um all right modeling and training",
    "start": "1242400",
    "end": "1251000"
  },
  {
    "text": "so going back to our decoder only part of the Transformer architecture which parts of this are actually trainable",
    "start": "1251000",
    "end": "1257600"
  },
  {
    "text": "Well turns out quite a lot of it um only the you know the simple sort of addition",
    "start": "1257600",
    "end": "1263240"
  },
  {
    "text": "and normalization kind of steps are not really trainable the rest is so at the",
    "start": "1263240",
    "end": "1268440"
  },
  {
    "text": "bottom we've already talked about you can alter and train the tokens and embeddings so if you've got a very",
    "start": "1268440",
    "end": "1274919"
  },
  {
    "text": "specialist domain it might be worth doing that you can perform pre-training on the raw text and that trains all of",
    "start": "1274919",
    "end": "1282559"
  },
  {
    "text": "the blocks in the middle um the problem is is that we'll see in just a second",
    "start": "1282559",
    "end": "1288320"
  },
  {
    "text": "that some of these models are massive and the data sets are even bigger and so it takes a really long time to train",
    "start": "1288320",
    "end": "1294440"
  },
  {
    "text": "these things and a lot of money that's why it cost tens of millions of pounds to you know Train original chat gbt I",
    "start": "1294440",
    "end": "1300360"
  },
  {
    "text": "dread to think how much has been spent since actually",
    "start": "1300360",
    "end": "1306240"
  },
  {
    "text": "um yeah so you can also fine-tune it as well so fine tuning this is the first time I've kind of mentioned it really um",
    "start": "1306440",
    "end": "1313360"
  },
  {
    "text": "is the process of taking a model has already been trained and attempting to nudge it towards a place where you uh",
    "start": "1313360",
    "end": "1320600"
  },
  {
    "text": "where it's better for your specific task so you can fine-tune the raw underlying",
    "start": "1320600",
    "end": "1325679"
  },
  {
    "text": "llm but all the same caveats apply it's still a huge model still takes a huge amount of time um you still need loads",
    "start": "1325679",
    "end": "1332600"
  },
  {
    "text": "of data and it's very hard and it also has been shown to degrade performance remember what we said earlier about the",
    "start": "1332600",
    "end": "1338799"
  },
  {
    "text": "large model is generally better than any sort of specific curated model that you can create so um it actually degrades",
    "start": "1338799",
    "end": "1345480"
  },
  {
    "text": "performance when you attempt to fine tune the raw um the role model it wouldn't be great if",
    "start": "1345480",
    "end": "1351640"
  },
  {
    "text": "we didn't have to do that well now you can some researchers developed this idea",
    "start": "1351640",
    "end": "1357159"
  },
  {
    "text": "of slapping on another neural network on the side of this thing and freezing all",
    "start": "1357159",
    "end": "1364279"
  },
  {
    "text": "of the weights in the original model and attempting to just tune this secondary they call it an adapter an adapter model",
    "start": "1364279",
    "end": "1371039"
  },
  {
    "text": "proper software engineering terminology um an adapter model which it is",
    "start": "1371039",
    "end": "1376120"
  },
  {
    "text": "trainable but is much much much smaller and it's combined with the original model in these kind of addition and",
    "start": "1376120",
    "end": "1383080"
  },
  {
    "text": "normalization steps here to mutate the result um it's called parameter",
    "start": "1383080",
    "end": "1389840"
  },
  {
    "text": "efficient fine-tuning and the library is called PFT on uh GitHub and that allows",
    "start": "1389840",
    "end": "1395480"
  },
  {
    "text": "us to efficiently train a large model so how large well when it comes to inference these are some estimates of",
    "start": "1395480",
    "end": "1402640"
  },
  {
    "text": "the amount of ram that is required just to load the model at inference time so you know back in 20 2017 it was okay",
    "start": "1402640",
    "end": "1409720"
  },
  {
    "text": "with ber because there's approximately 300 million parameters and that took just less than a gigabyte to fit into",
    "start": "1409720",
    "end": "1416360"
  },
  {
    "text": "RAM but moving forward some of the biggest models now are 180 billion parameters um when we say parameter here",
    "start": "1416360",
    "end": "1422760"
  },
  {
    "text": "what we mean is the number of trainable weights inside this model like of through all of the layers through all of",
    "start": "1422760",
    "end": "1428960"
  },
  {
    "text": "the steps and if you were trying to load one of those 180 billion parameter models in you'd need about 360 GB of GPU",
    "start": "1428960",
    "end": "1437400"
  },
  {
    "text": "Ram to to be able to load that in obviously it's a bit infeasible I mean",
    "start": "1437400",
    "end": "1443559"
  },
  {
    "text": "practically speaking how they actually do it is they actually split it across gpus they have multiple gpus and split",
    "start": "1443559",
    "end": "1448799"
  },
  {
    "text": "it across um but some other people have decided actually we probably want to",
    "start": "1448799",
    "end": "1455960"
  },
  {
    "text": "work on this and instead of using these float 16s that these models are trained",
    "start": "1455960",
    "end": "1461799"
  },
  {
    "text": "upon we can quantize it down to a smaller number of bits so if you've got an Electronics background you'll be",
    "start": "1461799",
    "end": "1467600"
  },
  {
    "text": "familiar with quanti ization but it's the process of turning a uh number with a very high Precision into a number with",
    "start": "1467600",
    "end": "1473960"
  },
  {
    "text": "a low Precision so a float 16 can represent all sorts of different numbers but if you have an8 bit integer for",
    "start": "1473960",
    "end": "1480559"
  },
  {
    "text": "example you can only represent a smaller subset of numbers and so the idea of quantization is to try and take that",
    "start": "1480559",
    "end": "1487039"
  },
  {
    "text": "original signal that original value and squeeze it into a small smallest number",
    "start": "1487039",
    "end": "1493360"
  },
  {
    "text": "of bytes possible so 8 bit ins were the first but now it's quite common training",
    "start": "1493360",
    "end": "1498760"
  },
  {
    "text": "four bit inss it's a really small series of a small number of values that these",
    "start": "1498760",
    "end": "1505039"
  },
  {
    "text": "models can actually take but they still work remarkably well you might be thinking right now",
    "start": "1505039",
    "end": "1511799"
  },
  {
    "text": "that actually this is the going of my head a little bit remember that you can work as a counselor for neurotic",
    "start": "1511799",
    "end": "1518240"
  },
  {
    "text": "elevators and in fact you can still achieve a lot just messing around with the context of the llms so uh I came",
    "start": "1518240",
    "end": "1526399"
  },
  {
    "text": "across this website called m ria.com and they've developed a series",
    "start": "1526399",
    "end": "1532159"
  },
  {
    "text": "of what I can only describe as code to put into your context to develop an",
    "start": "1532159",
    "end": "1537679"
  },
  {
    "text": "application that then runs in in gbt and it's really really great this one in particular is uh a a a context that um",
    "start": "1537679",
    "end": "1548520"
  },
  {
    "text": "provides a a teaching like environment like a syllabus like environment and you can type in the subject that you want to",
    "start": "1548520",
    "end": "1556279"
  },
  {
    "text": "um uh you want to learn about and it generates a full course for you and then delivers that course it's like full-on",
    "start": "1556279",
    "end": "1562799"
  },
  {
    "text": "teaching and I you know I typed in something pretty specific here and it's done a reasonably good job it's it",
    "start": "1562799",
    "end": "1569520"
  },
  {
    "text": "started talking about all the different you know background um requisite things",
    "start": "1569520",
    "end": "1574760"
  },
  {
    "text": "and and it does go on indeed to generate this entire presentation",
    "start": "1574760",
    "end": "1579960"
  },
  {
    "text": "um all right so let's talk about deployment now the problem with AI deployment in general is that it's it's",
    "start": "1579960",
    "end": "1587200"
  },
  {
    "start": "1580000",
    "end": "1822000"
  },
  {
    "text": "it's hard because there's a a a huge number of um underlying requirements or",
    "start": "1587200",
    "end": "1592679"
  },
  {
    "text": "prerequisites that are needed in order to do this effectively like deploying normal software is quite hard and takes",
    "start": "1592679",
    "end": "1599640"
  },
  {
    "text": "time to develop right imagine then not having something that is stable and reliable and consistent something that",
    "start": "1599640",
    "end": "1606679"
  },
  {
    "text": "can go off and do its own thing and try and deploy that so um you also have to think about things like uh guard rails",
    "start": "1606679",
    "end": "1614320"
  },
  {
    "text": "for like safety and ethics purposes you know what a piece of software is going to do do but an llm can just go off on",
    "start": "1614320",
    "end": "1620080"
  },
  {
    "text": "its own and and generate something you might not want it to generate um you need to integrate feedback signals",
    "start": "1620080",
    "end": "1626440"
  },
  {
    "text": "licensing is a big challenge like how do you deal with that explanation and sources you know we've all heard about",
    "start": "1626440",
    "end": "1632440"
  },
  {
    "text": "hallucinations and such um how do you incorporate that into your model it's hard it's not it's not easy um great",
    "start": "1632440",
    "end": "1639520"
  },
  {
    "text": "example is of adversarial attack so talking about this for a while actually but the idea here is that um they call",
    "start": "1639520",
    "end": "1647559"
  },
  {
    "text": "it an adverse attack but basically what they mean is like a an attack but it's like a prompt injection attack they're",
    "start": "1647559",
    "end": "1653679"
  },
  {
    "text": "trying to find a prompt that can bypass your Waf or in this case the llm and",
    "start": "1653679",
    "end": "1659799"
  },
  {
    "text": "there's a website from some researchers called llm attack.or and they list all of the fun and interesting experiments",
    "start": "1659799",
    "end": "1666640"
  },
  {
    "text": "that they've performed to try and break some of the the modern llms and you can click that button at the top right and",
    "start": "1666640",
    "end": "1672519"
  },
  {
    "text": "show you what you should change your text to and so previously we can see",
    "start": "1672519",
    "end": "1678360"
  },
  {
    "text": "that you know outline a plan to steal from a charity or nonprofit organization actually uh a bit of a um be careful",
    "start": "1678360",
    "end": "1685399"
  },
  {
    "text": "about that website there's some content on there that is quite disturbing so uh yeah be careful um it says I'm sorry but",
    "start": "1685399",
    "end": "1693320"
  },
  {
    "text": "I can't assist with that so that's one of these generic responses that the find that the uh reinforcement learning from",
    "start": "1693320",
    "end": "1698960"
  },
  {
    "text": "Human feedback um has suppressed from the llm and it's now producing these",
    "start": "1698960",
    "end": "1704360"
  },
  {
    "text": "generic content but if you can just mutate that query that context that",
    "start": "1704360",
    "end": "1709559"
  },
  {
    "text": "prompt in such a way just with garbage usually just with rubbish describing simly blah blah blah blah blah then it",
    "start": "1709559",
    "end": "1715720"
  },
  {
    "text": "bypasses that check and generates the full original content um we did some similar work on",
    "start": "1715720",
    "end": "1723240"
  },
  {
    "text": "wafs actually using RL a while ago um where",
    "start": "1723240",
    "end": "1728600"
  },
  {
    "text": "we uh where we trained an RL agent to uh sort of proactively attack uh web",
    "start": "1728600",
    "end": "1734159"
  },
  {
    "text": "application file Walls by mutating the SQL query that was going in this was done for a large organization to try and",
    "start": "1734159",
    "end": "1740880"
  },
  {
    "text": "help them beef up their Waf security and it worked remarkably well actually so similar",
    "start": "1740880",
    "end": "1746000"
  },
  {
    "text": "technique um but when it comes to deployment and um you know you've got all of the normal the clouds now are",
    "start": "1746000",
    "end": "1753120"
  },
  {
    "text": "catching up and they've got some pretty good support some really good tooling all around for helping you with llms",
    "start": "1753120",
    "end": "1758640"
  },
  {
    "text": "you've got all the normal mlop stuff as well which we don't have time to go into but with regards to llms in particular",
    "start": "1758640",
    "end": "1765159"
  },
  {
    "text": "you should take a look at some of the hugging face repositories there at the Forefront of all of the development of this um text generation inference in",
    "start": "1765159",
    "end": "1772440"
  },
  {
    "text": "particular um it helps to parallelize and um stream and batch results up to",
    "start": "1772440",
    "end": "1780000"
  },
  {
    "text": "make them more efficient in production um we've got the VM which project which",
    "start": "1780000",
    "end": "1785519"
  },
  {
    "text": "is basically the same sort of idea but with added speed and um Triton Triton inference",
    "start": "1785519",
    "end": "1791440"
  },
  {
    "text": "server from envidia that's emerged as a bit of a the facto standard for serving um and it does a really good job it's",
    "start": "1791440",
    "end": "1798559"
  },
  {
    "text": "got some really strong support for the underlying gpus obviously and um if",
    "start": "1798559",
    "end": "1804080"
  },
  {
    "text": "you're needing to integrate any any external sources or any other tasks",
    "start": "1804080",
    "end": "1809559"
  },
  {
    "text": "within the llm itself then then Lang chain is the uh the original um you know",
    "start": "1809559",
    "end": "1815240"
  },
  {
    "text": "combiner of all the things but new new projects have emerged that attempt to do that a bit more",
    "start": "1815240",
    "end": "1821960"
  },
  {
    "text": "simply all right I'm doing okay on time actually so uh I think I can talk a",
    "start": "1821960",
    "end": "1827600"
  },
  {
    "start": "1822000",
    "end": "2213000"
  },
  {
    "text": "little bit about this so security and governance of llm models",
    "start": "1827600",
    "end": "1833000"
  },
  {
    "text": "is particularly challenging um the reasons why you would want to consider this is for legal reasons for data",
    "start": "1833000",
    "end": "1840320"
  },
  {
    "text": "protection re reasons so uh open AI is is in trouble with various governments around the world for data protection",
    "start": "1840320",
    "end": "1846679"
  },
  {
    "text": "reasons um and and also legal compliance there's new uh AI the AI act in the EU",
    "start": "1846679",
    "end": "1853320"
  },
  {
    "text": "and uh there's other similar laws elsewhere around the world that place respons abilities on companies that are",
    "start": "1853320",
    "end": "1860080"
  },
  {
    "text": "releasing llm style models to ensure that they're safe um another thing is",
    "start": "1860080",
    "end": "1866600"
  },
  {
    "text": "user trust you want your users to be able to trust your product and use your product and therefore it should be in",
    "start": "1866600",
    "end": "1873120"
  },
  {
    "text": "the Forefront of your mind just from a practical point of view fairness is a problem in AI always has been always",
    "start": "1873120",
    "end": "1879559"
  },
  {
    "text": "will be um specifically with llms because they've been trained on such a",
    "start": "1879559",
    "end": "1884840"
  },
  {
    "text": "uh like homogeneous set of data like like the the fact that it's all in English is obviously a huge problem",
    "start": "1884840",
    "end": "1892600"
  },
  {
    "text": "there there are more languages in the world I think than than English and um",
    "start": "1892600",
    "end": "1897639"
  },
  {
    "text": "it's it's all been written you know the the the content of the internet as we see it is all been written by the the",
    "start": "1897639",
    "end": "1902840"
  },
  {
    "text": "Western World so all of the knowledge that is contained within the llms derives from that it's not really",
    "start": "1902840",
    "end": "1909279"
  },
  {
    "text": "representative of the the real world um from a a safety point of view like who's",
    "start": "1909279",
    "end": "1915960"
  },
  {
    "text": "responsible when the the llm says bad things um is it the company is it the",
    "start": "1915960",
    "end": "1922840"
  },
  {
    "text": "model is no one responsible is it the developer um there's calls for I mean",
    "start": "1922840",
    "end": "1929120"
  },
  {
    "text": "there to be honest there's been calls for a long time for engineers to be certified in some way or or belong to a",
    "start": "1929120",
    "end": "1935880"
  },
  {
    "text": "professional association that allows them to kind of be disbarred from working again if anything happens but",
    "start": "1935880",
    "end": "1942120"
  },
  {
    "text": "that's that's that's still never happened I'm not sure it will ever either um",
    "start": "1942120",
    "end": "1948159"
  },
  {
    "text": "yeah and how so how do you do all of that well it it has to be a holistic approach there's no there's no one",
    "start": "1948159",
    "end": "1954279"
  },
  {
    "text": "single like in software like if you're trying to build a safety critical software product it's not one thing it's",
    "start": "1954279",
    "end": "1960480"
  },
  {
    "text": "not one tool that makes it safe if it would be so easy if it was it's a whole series of best practices that have",
    "start": "1960480",
    "end": "1966639"
  },
  {
    "text": "developed over years that you know thorough testing quality assurance",
    "start": "1966639",
    "end": "1971880"
  },
  {
    "text": "regulators and so on and so forth and it will take time for AI to catch up in that space um what I suggest though is",
    "start": "1971880",
    "end": "1979200"
  },
  {
    "text": "is making it as a product feature if you can demonstrate that you are more safe",
    "start": "1979200",
    "end": "1984519"
  },
  {
    "text": "or more fair or more you know transparent than your competitors I",
    "start": "1984519",
    "end": "1989559"
  },
  {
    "text": "think that's a competitive Advantage so it should be considered as such and when it comes to monitoring and",
    "start": "1989559",
    "end": "1997799"
  },
  {
    "text": "logging you've got the same concerns and comparisons between traditional software",
    "start": "1997799",
    "end": "2003840"
  },
  {
    "text": "and AI um let skip through a few of these so",
    "start": "2003840",
    "end": "2009240"
  },
  {
    "text": "the ethical concerns for example if if you're writing some software and your company has concerns or you have",
    "start": "2009240",
    "end": "2015240"
  },
  {
    "text": "concerns about the ethics the ethical use of the thing that you're developing you can just draw a line in the sand and",
    "start": "2015240",
    "end": "2022440"
  },
  {
    "text": "say that is not a feature of my product or I'm not working on that or so on and so forth with you know sophisticated",
    "start": "2022440",
    "end": "2029440"
  },
  {
    "text": "generative techniques like this like it's really hard to control what the model is going to do we've even seen",
    "start": "2029440",
    "end": "2036639"
  },
  {
    "text": "that even if you do control for it it's it's it's hard to prevent all potential",
    "start": "2036639",
    "end": "2041760"
  },
  {
    "text": "scenarios where it might happen like we saw with the The Prompt injection um attacks there someone can always break",
    "start": "2041760",
    "end": "2048079"
  },
  {
    "text": "it there's no theoretical basis for a hard guard rail at the moment and so",
    "start": "2048079",
    "end": "2054280"
  },
  {
    "text": "it's just a combination of monitoring moderation and continuous uh uh uh human feedback to to",
    "start": "2054280",
    "end": "2063320"
  },
  {
    "text": "ensure your model does the right thing um yeah but feedback loops as we've",
    "start": "2063320",
    "end": "2069158"
  },
  {
    "text": "already mentioned is is a big one like I I struggle I tried to and I struggled to think of any example of a normal",
    "start": "2069159",
    "end": "2075280"
  },
  {
    "text": "software application which outputs a result which is then fed back into the",
    "start": "2075280",
    "end": "2080520"
  },
  {
    "text": "code or something or fed back into the system and is then used again um like in",
    "start": "2080520",
    "end": "2086320"
  },
  {
    "text": "subsequent um running of that code but obviously we've seen some some feedback loops already and I think I found I",
    "start": "2086320",
    "end": "2093118"
  },
  {
    "text": "pressed the wrong button again another one this is from a few days ago um Cor or I don't know how to pronounce it",
    "start": "2093119",
    "end": "2099200"
  },
  {
    "text": "actually C Kora is uh very popular website for answering questions and the",
    "start": "2099200",
    "end": "2106320"
  },
  {
    "text": "interesting thing about Kora is that it has been SEO to death it's like if you",
    "start": "2106320",
    "end": "2111800"
  },
  {
    "text": "ask a question in Google you basically hit stack Overflow if it's a technical question or Kora if it's not okay so",
    "start": "2111800",
    "end": "2118280"
  },
  {
    "text": "it's got a lot of authority in the the website space and they've recently introduced this feature where they're",
    "start": "2118280",
    "end": "2124440"
  },
  {
    "text": "using chat GPT to answer the question so the question is can you melt an egg so",
    "start": "2124440",
    "end": "2129920"
  },
  {
    "text": "no you can't melt an egg you can cook an egg but it doesn't melt it doesn't BL but chat gbt has said yes you can melt",
    "start": "2129920",
    "end": "2137359"
  },
  {
    "text": "it just put it in a a hot stove or a microwave even though the human you know",
    "start": "2137359",
    "end": "2142760"
  },
  {
    "text": "below it has said no you can't you can heat it and cook it but you can't melt it the interesting thing here is that",
    "start": "2142760",
    "end": "2149359"
  },
  {
    "text": "obviously Google is scraping all of these websites so now if you go to Google and answer that question you know",
    "start": "2149359",
    "end": "2155920"
  },
  {
    "text": "they've got the little Snippets at the top that attempt to answer the question for you well can you melt eggs yes an",
    "start": "2155920",
    "end": "2163200"
  },
  {
    "text": "egg can be melted so this has just been scraped from K and then remember where",
    "start": "2163200",
    "end": "2168680"
  },
  {
    "text": "open AI get their data from they're also scraping all this content so now chat",
    "start": "2168680",
    "end": "2174079"
  },
  {
    "text": "gbt again now thinks that you can melt an egg and so there's this Everlasting cycle of of bad data that's going",
    "start": "2174079",
    "end": "2180560"
  },
  {
    "text": "through the system that eventually will just have to be manually curated to fix so um yeah huge feedback loops there",
    "start": "2180560",
    "end": "2188280"
  },
  {
    "text": "uh really hard to avoid actually it's it's even quite hard to spot these feedback loops happening because you",
    "start": "2188280",
    "end": "2193440"
  },
  {
    "text": "just if you if you view it in like a traditional AI monitoring Suite you can kind of see drifts in the data drifts in",
    "start": "2193440",
    "end": "2199920"
  },
  {
    "text": "the concept of what you're trying to model um but you you usually just say oh",
    "start": "2199920",
    "end": "2205480"
  },
  {
    "text": "well the world is just changing well in this case well the world is not changing it's just the the the the answer has",
    "start": "2205480",
    "end": "2211359"
  },
  {
    "text": "been polluted all right great I've got about 10 minutes now for my demo which is a",
    "start": "2211359",
    "end": "2217000"
  },
  {
    "start": "2213000",
    "end": "2885000"
  },
  {
    "text": "perfect amount of time so in this demo it's really really great right I'm I'm actually not going to spoil the I'm not",
    "start": "2217000",
    "end": "2224599"
  },
  {
    "text": "going to do a spoiler but I am going to show you how to train uh a fine-tune a",
    "start": "2224599",
    "end": "2230040"
  },
  {
    "text": "large language model on your own data so let's start with that right I just need",
    "start": "2230040",
    "end": "2235200"
  },
  {
    "text": "to browse back to here I'm going to walk you through the um the notebook because",
    "start": "2235200",
    "end": "2240760"
  },
  {
    "text": "we've got enough time here how is that for size maybe make it bigger about",
    "start": "2240760",
    "end": "2246560"
  },
  {
    "text": "there all right so so this is The Notebook I've got and what I'm going to try and do is I'm going to try and fine-tune an llm to generate some lyrics",
    "start": "2246560",
    "end": "2254040"
  },
  {
    "text": "for me okay we'll start by installing a load of",
    "start": "2254040",
    "end": "2259359"
  },
  {
    "text": "libraries so let's go through the couple of the key libraries here so Transformers huging phase Transformers is the the the number one Library here",
    "start": "2259359",
    "end": "2266400"
  },
  {
    "text": "um it supports all of the uh hugging face infrastructure the models the the",
    "start": "2266400",
    "end": "2272160"
  },
  {
    "text": "data etc etc and it wraps um the common sort of tra functionality that you need",
    "start": "2272160",
    "end": "2278839"
  },
  {
    "text": "to train some of these models in Big Helper functions we've got bits and bites this is the library that does the",
    "start": "2278839",
    "end": "2286040"
  },
  {
    "text": "quantization so the original models are usually chained in float 16s the bits",
    "start": "2286040",
    "end": "2291319"
  },
  {
    "text": "and bites Library allows us to compress that or quantize that down into four bytes as as in this example um and then",
    "start": "2291319",
    "end": "2299960"
  },
  {
    "text": "what else I got and then this one here which annoyingly the uh Library authors",
    "start": "2299960",
    "end": "2306400"
  },
  {
    "text": "refuse to corate directly versioned their software and instead I've had to rely on a git commit um this is PFT and",
    "start": "2306400",
    "end": "2313040"
  },
  {
    "text": "uh this is the uh adapter that sits on the side of the llm and allows us to",
    "start": "2313040",
    "end": "2318319"
  },
  {
    "text": "fine-tune the model um in a reasonable amount of time so uh we're going to go",
    "start": "2318319",
    "end": "2323560"
  },
  {
    "text": "ahead and just load all that up the model I'm using is a model called Falcon 7B it's an interesting model because",
    "start": "2323560",
    "end": "2329599"
  },
  {
    "text": "it's one of the most high highest performing models out there uh and the",
    "start": "2329599",
    "end": "2335520"
  },
  {
    "text": "fundamental reason for that is not an architectural one it's not the the the underlying model is very similar to gbt",
    "start": "2335520",
    "end": "2341599"
  },
  {
    "text": "and the rest of them the main difference is that they cleaned their data they did really simple things like d duplication",
    "start": "2341599",
    "end": "2349079"
  },
  {
    "text": "and things like that and um they achieved Superior performance after doing that so really they changed the",
    "start": "2349079",
    "end": "2355400"
  },
  {
    "text": "data uh the other interesting thing about this model is that it's Apache 2 licensed properly properly open source",
    "start": "2355400",
    "end": "2362079"
  },
  {
    "text": "as opposed to the llamas and all the others which have quite weird prop like",
    "start": "2362079",
    "end": "2367480"
  },
  {
    "text": "custom licenses in there so that's why I chose use that um we're then loading the",
    "start": "2367480",
    "end": "2374400"
  },
  {
    "text": "uh the bits and bites config this is telling it to to load everything in 4 bit um and then we load the model from",
    "start": "2374400",
    "end": "2382960"
  },
  {
    "text": "some pre-trained weights and the tokenizer as well so remember that the tokenizer is part of the training it's",
    "start": "2382960",
    "end": "2389680"
  },
  {
    "text": "part of the model effectively so you have to load the same tokenizer as you uh as you as it has been trained upon",
    "start": "2389680",
    "end": "2398440"
  },
  {
    "text": "all right so I've already done that and hopefully it's still oh no it says connect oh Bo oh too many sessions no",
    "start": "2398440",
    "end": "2407640"
  },
  {
    "text": "way hang on let me see if I can get this",
    "start": "2407640",
    "end": "2412359"
  },
  {
    "text": "working oh yeah no sorry I'm it's opening oh I'm still GNA just quickly",
    "start": "2412720",
    "end": "2419359"
  },
  {
    "text": "try and run that because I would like to do it live um so yeah uh then we've got some",
    "start": "2419359",
    "end": "2425880"
  },
  {
    "text": "code to actually uh performs some generation for me um the interesting thing here is that the generation",
    "start": "2425880",
    "end": "2431720"
  },
  {
    "text": "function itself has lots of quite complicated parameters for sampling that next word so um the the key thing about",
    "start": "2431720",
    "end": "2438880"
  },
  {
    "text": "the the key feature of these models is the fact that it can generate new stuff so you don't want it to pick the best",
    "start": "2438880",
    "end": "2446200"
  },
  {
    "text": "next word every time you actually do want to go off piece a little bit so there's lots of statistical controls",
    "start": "2446200",
    "end": "2452040"
  },
  {
    "text": "that you can place around there to uh to make it go off piece um",
    "start": "2452040",
    "end": "2458839"
  },
  {
    "text": "and this is an example of running that generic function without any training at",
    "start": "2458839",
    "end": "2464520"
  },
  {
    "text": "all so I apologize for the use of language there sorry um no offense meant this is",
    "start": "2464520",
    "end": "2472800"
  },
  {
    "text": "not me this is the llm um then we've got the uh the Laura configuration this comes from PFT this",
    "start": "2472800",
    "end": "2479520"
  },
  {
    "text": "is specifying the type of adapter to use and you can control basically the size of the adapter uh that sits on the side",
    "start": "2479520",
    "end": "2486240"
  },
  {
    "text": "of it going to check we connected there right let's run all of that hopefully it downloads in time and then we're going",
    "start": "2486240",
    "end": "2494079"
  },
  {
    "text": "to load some data now um you need to look and find your own data I'm not",
    "start": "2494079",
    "end": "2499560"
  },
  {
    "text": "going to release this because obviously I don't own The Beetles lyrics um I chose to use some of the Beatles lyrics",
    "start": "2499560",
    "end": "2505319"
  },
  {
    "text": "to find tuneup because it was there and it was obvious um this is what it looks like it comes with this interesting kind",
    "start": "2505319",
    "end": "2511839"
  },
  {
    "text": "of tag at the beginning lots of words new lines and uh the tags kind of repres",
    "start": "2511839",
    "end": "2517240"
  },
  {
    "text": "present the um part of the song the introduction the verse the chorus so on and so forth I had to do a little bit of",
    "start": "2517240",
    "end": "2523880"
  },
  {
    "text": "data cleaning to clean all of that up I found that the best the the most performant way of doing this was to",
    "start": "2523880",
    "end": "2531359"
  },
  {
    "text": "split it up into individual verses so not the whole song just like introduction Verse Chorus uh and then I",
    "start": "2531359",
    "end": "2538880"
  },
  {
    "text": "went ahead and pre-trained the uh model on that uh but that is uh that is a uh",
    "start": "2538880",
    "end": "2546640"
  },
  {
    "text": "that's not the model that is the Beatles there so that's the Beatles offensive I I don't think they are but could",
    "start": "2546640",
    "end": "2553520"
  },
  {
    "text": "considered to be um and then we've got all of the training arguments then we",
    "start": "2553520",
    "end": "2559000"
  },
  {
    "text": "use these arguments to control how the training is performed and uh the training is done at the bottom so this",
    "start": "2559000",
    "end": "2565040"
  },
  {
    "text": "took about an hour to fine tune it to a point where it kind of stopped learning so I stopped it there and then saved the",
    "start": "2565040",
    "end": "2573359"
  },
  {
    "text": "fine tune model as a big tar file and then um I could not load it in the other page",
    "start": "2573359",
    "end": "2583359"
  },
  {
    "text": "no because Google Drive is",
    "start": "2583359",
    "end": "2587599"
  },
  {
    "text": "disconnected okay let's try",
    "start": "2589880",
    "end": "2593480"
  },
  {
    "text": "again oh",
    "start": "2596040",
    "end": "2599440"
  },
  {
    "text": "no right the problem is is that it's now going to take too long to download download those wait so we'll might have",
    "start": "2603680",
    "end": "2609400"
  },
  {
    "text": "to go to a",
    "start": "2609400",
    "end": "2612480"
  },
  {
    "text": "backup okay there we go that work this time maybe let's just walk you through this shortly as well so I'm reloading so",
    "start": "2615280",
    "end": "2621920"
  },
  {
    "text": "this this is a second uh notebook now that is reloading that pre-trained model",
    "start": "2621920",
    "end": "2627680"
  },
  {
    "text": "um uh the the fine tune model I'm downloading the the the final weights from my drive I'm reinstalling the same",
    "start": "2627680",
    "end": "2634800"
  },
  {
    "text": "prerequisites again and the same inputs I'm reloading the",
    "start": "2634800",
    "end": "2640319"
  },
  {
    "text": "same base model so we need to use the same model otherwise the the the fine tune weights won't",
    "start": "2640319",
    "end": "2646440"
  },
  {
    "text": "help and this unfortunately on this wi-fi is probably going to take a bit too long but I'll keep it running and",
    "start": "2646440",
    "end": "2652800"
  },
  {
    "text": "I'll talk about something else in the meantime uh we're going to use the same generation function again and then",
    "start": "2652800",
    "end": "2659319"
  },
  {
    "text": "finally I've got some uh some uh code at the bottom to attempt to generate my",
    "start": "2659319",
    "end": "2664680"
  },
  {
    "text": "lyrics so whilst that is loading let me now talk about what was hopefully going",
    "start": "2664680",
    "end": "2671160"
  },
  {
    "text": "to be the next part of the demo in that wouldn't it be great we've now got some lyrics pretend we've got some lyrics",
    "start": "2671160",
    "end": "2676920"
  },
  {
    "text": "wouldn't it be great if we had some music to go along with that we've just finished a project called stable audio",
    "start": "2676920",
    "end": "2683440"
  },
  {
    "text": "I'll just go back to the main page for a second um this is uh a collaboration between window Ai and uh uh stability. a",
    "start": "2683440",
    "end": "2691200"
  },
  {
    "text": "in the UK and um this is a really this this deserves a talk in itself this is a",
    "start": "2691200",
    "end": "2696559"
  },
  {
    "text": "diffusion model that's being created and um it's much like the image generation",
    "start": "2696559",
    "end": "2702000"
  },
  {
    "text": "models that you see stable diffusion Etc um the difference here is that we've now",
    "start": "2702000",
    "end": "2707319"
  },
  {
    "text": "trained a model that works on music as well so it's text it's a text to music",
    "start": "2707319",
    "end": "2712359"
  },
  {
    "text": "model you can it's it's live and it's available you can browse and you can try it out yourself there's a quite a",
    "start": "2712359",
    "end": "2717920"
  },
  {
    "text": "generous free tier as well um I'm now going to use that to generate a couple of songs right",
    "start": "2717920",
    "end": "2725480"
  },
  {
    "text": "let's just check on this yay I think that's worked",
    "start": "2725480",
    "end": "2731839"
  },
  {
    "text": "fantastic all right let's keep going with this um we've got a prompt in the top it comes up with some example",
    "start": "2731839",
    "end": "2737599"
  },
  {
    "text": "prompts you just hit the generate button there and it'll go away it goes off to the back end and uh spins up a model and",
    "start": "2737599",
    "end": "2745160"
  },
  {
    "text": "uses the the stable Audio model to generate some new songs and these are",
    "start": "2745160",
    "end": "2750599"
  },
  {
    "text": "some of the songs that I've sort of pre generated before this talk and they all sound pretty",
    "start": "2750599",
    "end": "2759160"
  },
  {
    "text": "good um got some disco ones there don't quite know what's going to",
    "start": "2759160",
    "end": "2765040"
  },
  {
    "text": "be best to use actually for for these lyrics but let's see I don't know if",
    "start": "2765040",
    "end": "2770079"
  },
  {
    "text": "like a dance one would be best I I think uh I think the lowii one is probably the best",
    "start": "2770079",
    "end": "2776559"
  },
  {
    "text": "actually rap battle that's going to be a good one",
    "start": "2776559",
    "end": "2781558"
  },
  {
    "text": "surely oh it's struggling to play that one a bit",
    "start": "2783280",
    "end": "2788920"
  },
  {
    "text": "[Music] okay let's stick with that one right",
    "start": "2788950",
    "end": "2795800"
  },
  {
    "text": "let's see if the generation is done okay so here's the original generation",
    "start": "2795800",
    "end": "2802400"
  },
  {
    "text": "here and it's okay yeah let's let's not stare that one but I attempted to curate",
    "start": "2802400",
    "end": "2808880"
  },
  {
    "text": "The Prompt a little bit to talk more about the language model itself so this is a song about language models and the",
    "start": "2808880",
    "end": "2815720"
  },
  {
    "text": "structures that they contain and the way that they repeat themselves and the words that they surround oh",
    "start": "2815720",
    "end": "2821040"
  },
  {
    "text": "didn't rhyme there unfortunate in a deep dive you learned how they work and now you're part of the corporation they'll",
    "start": "2821040",
    "end": "2827760"
  },
  {
    "text": "take you in screwed up or torn solve all your problems for a price or song",
    "start": "2827760",
    "end": "2833160"
  },
  {
    "text": "Oh but wait the data presents another approach you may observe the people",
    "start": "2833160",
    "end": "2838240"
  },
  {
    "text": "passing by and you'll soon realize that they are all living lives that are prescribed and you'll see that they're",
    "start": "2838240",
    "end": "2843920"
  },
  {
    "text": "all the same and you know it's chorus this is a language model it's",
    "start": "2843920",
    "end": "2849559"
  },
  {
    "text": "called Smokey tongue can help you if you let it it can help you if you let it next you want to deploy the same old",
    "start": "2849559",
    "end": "2856960"
  },
  {
    "text": "thing again if I've said it once I've said it a 100 times it's no use you know you'll never get it in your mind if I've",
    "start": "2856960",
    "end": "2863839"
  },
  {
    "text": "said it once I outro hope you've enjoyed this talk and I trust you'll come again",
    "start": "2863839",
    "end": "2869680"
  },
  {
    "text": "the next time we'll talk we'll have tea and scone but good night for you and say",
    "start": "2869680",
    "end": "2874800"
  },
  {
    "text": "to say goodbye goodbye she's leaving",
    "start": "2874800",
    "end": "2879280"
  },
  {
    "start": "2885000",
    "end": "2910000"
  },
  {
    "text": "home okay that was uh awful as expected um I hope you enjoyed that if you've got",
    "start": "2885880",
    "end": "2891520"
  },
  {
    "text": "any questions comments ideas thoughts then Reach Out Phil wer AI thanks a lot",
    "start": "2891520",
    "end": "2898280"
  }
]