[
  {
    "start": "0",
    "end": "56000"
  },
  {
    "text": "um yeah so as as mentioned I'm going to be talking about a centralized machine Learning System um can I get a show a",
    "start": "5120",
    "end": "10800"
  },
  {
    "text": "hands just very quickly um has anyone here been exposed to spark",
    "start": "10800",
    "end": "16320"
  },
  {
    "text": "before okay awesome and has anyone done any machine learning with spark or with",
    "start": "16320",
    "end": "21560"
  },
  {
    "text": "some other system a couple all right awesome um so I'm going to talk you through a journey",
    "start": "21560",
    "end": "27640"
  },
  {
    "text": "um that I went through with uh one of the company in Australia um where we tried to basically build a centralized",
    "start": "27640",
    "end": "34040"
  },
  {
    "text": "machine Learning System so before I get into that I'll just quickly introduce myself a little bit um so as as",
    "start": "34040",
    "end": "39320"
  },
  {
    "text": "mentioned um I'm a principal machine learning engineer at simple machines um simple machines are a consultancy based",
    "start": "39320",
    "end": "45399"
  },
  {
    "text": "out of Australia and we basically do data products and data platforms um",
    "start": "45399",
    "end": "51000"
  },
  {
    "text": "ranging from kind of data streaming and Big Data through to machine learning and analytic systems um so before we actually get",
    "start": "51000",
    "end": "59480"
  },
  {
    "start": "56000",
    "end": "158000"
  },
  {
    "text": "into some of the details um I just want to run you through some of the ways that",
    "start": "59480",
    "end": "64799"
  },
  {
    "text": "you can actually set up these uh organizational structures for um data science and machine learning so you can",
    "start": "64799",
    "end": "71400"
  },
  {
    "text": "imagine um on the sort of far left hand side here you can have this sort of very centralized capability um and what this",
    "start": "71400",
    "end": "78280"
  },
  {
    "text": "is is you have a centralized team where the various projects that want to get some machine learning insights um will",
    "start": "78280",
    "end": "84079"
  },
  {
    "text": "give that to this centralized team they'll do the work and then pass it back to the different projects um then",
    "start": "84079",
    "end": "90040"
  },
  {
    "text": "you can kind of move through to a less centralized one um which is still primarily centralized but you have data",
    "start": "90040",
    "end": "95759"
  },
  {
    "text": "scientists working in the various projects and then doing most of the work um centrally then you can move across to",
    "start": "95759",
    "end": "101799"
  },
  {
    "text": "this kind of mostly distributed approach and the idea here is that you have um each one of your teams focusing on",
    "start": "101799",
    "end": "108399"
  },
  {
    "text": "solving some business problem but you're sharing the kind of common um bits and pieces that are true across all of your",
    "start": "108399",
    "end": "114439"
  },
  {
    "text": "data science projects and then finally you can have this complete data silos approach um which is where every single",
    "start": "114439",
    "end": "120680"
  },
  {
    "text": "team is independent and ultimately doesn't really share anything so as you can probably imagine um the two main",
    "start": "120680",
    "end": "127799"
  },
  {
    "text": "approaches that people want to end up at is one of these two middle ones and specifically the Telco that I'm speaking",
    "start": "127799",
    "end": "134120"
  },
  {
    "text": "about today um they were aiming to end up with the mostly distributed data science capability so they could focus",
    "start": "134120",
    "end": "140840"
  },
  {
    "text": "on actually delivering business value in the various teams um but they could still share some of the common stuff and",
    "start": "140840",
    "end": "146959"
  },
  {
    "text": "in order to share that stuff they wanted to have the data engineering team actually working on building out um the",
    "start": "146959",
    "end": "152640"
  },
  {
    "text": "sort of common infrastructure and the the pain points of scaling the Big Data Solutions so if you consider the data",
    "start": "152640",
    "end": "160159"
  },
  {
    "start": "158000",
    "end": "253000"
  },
  {
    "text": "science workflow um sort of from a million miles away and squint a little bit it might look something like this so",
    "start": "160159",
    "end": "166120"
  },
  {
    "text": "you start with a business problem um you want to do some kind of data extraction and data exploration and then from that",
    "start": "166120",
    "end": "172840"
  },
  {
    "text": "you'll start to look and build some features and then the features will go into a model which you'll use to train",
    "start": "172840",
    "end": "178280"
  },
  {
    "text": "and try and predict whatever the thing that you're trying to predict is and then you'll evaluate that output see how good your predictions are um and there",
    "start": "178280",
    "end": "184400"
  },
  {
    "text": "might be a bit of a cycle kind of going back and forth with this features and building of models and sometimes it goes",
    "start": "184400",
    "end": "189560"
  },
  {
    "text": "even further back and then finally once you're happy um you want to put this into production so you want to serve your model and make",
    "start": "189560",
    "end": "195840"
  },
  {
    "text": "predictions now the the entire process often ends up looking something more like this but we'll skim over that for",
    "start": "195840",
    "end": "201879"
  },
  {
    "text": "now so there are three kind of key points um and that is data extraction",
    "start": "201879",
    "end": "207440"
  },
  {
    "text": "building of features and model serving that requ ire a lot of data engineering uh skills and as I said the the data",
    "start": "207440",
    "end": "214799"
  },
  {
    "text": "scientist should be focusing on solving business problems where possible um rather than trying to build and optimize",
    "start": "214799",
    "end": "221319"
  },
  {
    "text": "this code and make it run on distributed systems and so on that's that's the purpose of data engineering um so I'm going to focus uh",
    "start": "221319",
    "end": "228840"
  },
  {
    "text": "initially there's this the ETL idea the data extraction and so on I'm not going to spend any time on this today um it's",
    "start": "228840",
    "end": "235959"
  },
  {
    "text": "it's a lot more complex than it sounds like it should be um at scale um but what I'm going to spend a lot of",
    "start": "235959",
    "end": "241840"
  },
  {
    "text": "time on is feature engineering um and feature engineering um I'll explain the",
    "start": "241840",
    "end": "246959"
  },
  {
    "text": "process of what that actually means and why it's a tricky thing and an important thing to scale so first of all just taking a step",
    "start": "246959",
    "end": "254239"
  },
  {
    "start": "253000",
    "end": "322000"
  },
  {
    "text": "back what is machine learning at all um you probably think things along the",
    "start": "254239",
    "end": "259840"
  },
  {
    "text": "lines of Terminator or maybe self-driving cars or Alpha go um some of the the really cool stuff that's",
    "start": "259840",
    "end": "266280"
  },
  {
    "text": "happening um but the reality of machine learning um for most Enterprises is actually that they're doing this kind of",
    "start": "266280",
    "end": "272320"
  },
  {
    "text": "stuff so in this example um it's it's called um clustering basically we've got",
    "start": "272320",
    "end": "279199"
  },
  {
    "text": "two groups of customers and we want to classify them as either a luxury segment",
    "start": "279199",
    "end": "285120"
  },
  {
    "text": "or knitting Enthusiast segment for example and um what what this is doing",
    "start": "285120",
    "end": "290240"
  },
  {
    "text": "is basically just finding whether or not to put them in uh the red group or the blue group um and you can see here the",
    "start": "290240",
    "end": "297280"
  },
  {
    "text": "the axes are actually labeled with question marks and we'll see where that is in a second so another example might",
    "start": "297280",
    "end": "302440"
  },
  {
    "text": "be a classification problem like whether or not someone's going to churn um so you can see here we've got examples of",
    "start": "302440",
    "end": "308680"
  },
  {
    "text": "uh whether or not a user is churned marked in uh red and blue respectively and again we're drawing a line to",
    "start": "308680",
    "end": "315360"
  },
  {
    "text": "separate these two things and we've got these two axes with question marks so what are the question marks well",
    "start": "315360",
    "end": "321759"
  },
  {
    "text": "ultimately these things are actually our features now the idea here is that since we can't directly measure churn um we",
    "start": "321759",
    "end": "329360"
  },
  {
    "start": "322000",
    "end": "457000"
  },
  {
    "text": "need to Instead try and predict churn using something we can measure and these measurements are actually called",
    "start": "329360",
    "end": "335240"
  },
  {
    "text": "features and are typically associated with an entity and an instance in time and it can be as simple as just raw data",
    "start": "335240",
    "end": "343199"
  },
  {
    "text": "um coming out of some system somewhere or it could be a complex transformation of some sort um for example with",
    "start": "343199",
    "end": "349720"
  },
  {
    "text": "windowing you might think um what what have how many times has my user clicked",
    "start": "349720",
    "end": "355520"
  },
  {
    "text": "on uh this particular web page in the last 30 days for example that would be considered a feature for a customer at a",
    "start": "355520",
    "end": "361680"
  },
  {
    "text": "particular point in time and the reason that these are really important is because the majority of machine learning",
    "start": "361680",
    "end": "367599"
  },
  {
    "text": "performance actually comes out of features um if we take a look through how this",
    "start": "367599",
    "end": "373639"
  },
  {
    "text": "actually runs in practice um what what typically happens is you've got a bunch of data sources you have some kind of",
    "start": "373639",
    "end": "379360"
  },
  {
    "text": "feature engineering and this feature engineering ultimately will spit out some data about a particular um customer",
    "start": "379360",
    "end": "386919"
  },
  {
    "text": "for example at a particular point in time and then we want to attribute this to a label and a label is the thing",
    "start": "386919",
    "end": "392240"
  },
  {
    "text": "we're trying to predict so in this case it would be whether or not a customer has churned or not given this set of",
    "start": "392240",
    "end": "397560"
  },
  {
    "text": "features so the point of a model is that we train these models and we try and learn a relationship between these",
    "start": "397560",
    "end": "403400"
  },
  {
    "text": "features and the actual label that we're trying to predict um so here you can see",
    "start": "403400",
    "end": "408759"
  },
  {
    "text": "we've leared that the red feature when it's greater than 30 um and say the",
    "start": "408759",
    "end": "414000"
  },
  {
    "text": "yellow feature when it's equal to seven oh sorry when it's not equal to seven is going to predict churn as as an example",
    "start": "414000",
    "end": "421319"
  },
  {
    "text": "and we can see what this looks like when we actually get some data at scoring time so what happens is uh we get in an",
    "start": "421319",
    "end": "427440"
  },
  {
    "text": "example say we want to see if Jason is going to churn and it'll be uh they'll have a blue feature value of 12 a yellow",
    "start": "427440",
    "end": "433879"
  },
  {
    "text": "feature value of three and a red feature value of 33 and so at the first step in",
    "start": "433879",
    "end": "439080"
  },
  {
    "text": "this decision tree um we'll see that it is greater than 30 so we'll go down to the right um and then we'll see that the",
    "start": "439080",
    "end": "444800"
  },
  {
    "text": "yellow feature is not equal to seven so it will go to the left and we'll end up predicting that Jason is going to turn",
    "start": "444800",
    "end": "450400"
  },
  {
    "text": "now this is obviously a very very simple model um but conceptually all of them actually more or less work in the same",
    "start": "450400",
    "end": "455759"
  },
  {
    "text": "kind of way now there are two main uh sort of",
    "start": "455759",
    "end": "461199"
  },
  {
    "start": "457000",
    "end": "541000"
  },
  {
    "text": "usages of features and this is in training and at scoring time and the training data set will typically be very",
    "start": "461199",
    "end": "468400"
  },
  {
    "text": "large it'll use lots of historical data and it will use a lot more features than you end up using at scoring time now",
    "start": "468400",
    "end": "475560"
  },
  {
    "text": "scoring on the other hand is where you make predictions um and this typically uses a a subset of the training features",
    "start": "475560",
    "end": "481599"
  },
  {
    "text": "and it's only usually calculated for the most recent data so you don't need to worry about what Jason did for the last",
    "start": "481599",
    "end": "486840"
  },
  {
    "text": "20 years you only need to know what his current state is so that we can predict whether or not he's going to",
    "start": "486840",
    "end": "492520"
  },
  {
    "text": "turn now with this entire process we basically have something like this um we",
    "start": "492520",
    "end": "497879"
  },
  {
    "text": "have the feature logic that creates our table we have some iterative training process and very often um the models",
    "start": "497879",
    "end": "504479"
  },
  {
    "text": "will actually learn or you can um do things like feature selection where we actually cut down the set of features",
    "start": "504479",
    "end": "510240"
  },
  {
    "text": "that end up going into that final model and then we want to take this feature logic and we want to take that final",
    "start": "510240",
    "end": "516080"
  },
  {
    "text": "model and put it into production these are the kind of two key parts of productionizing the machine learning",
    "start": "516080",
    "end": "522440"
  },
  {
    "text": "systems and so what a lot of companies tend to do is they'll they'll spend a lot of time working on various projects",
    "start": "522440",
    "end": "529320"
  },
  {
    "text": "and trying to do some um some machine learning without actually investing a",
    "start": "529320",
    "end": "534560"
  },
  {
    "text": "whole bunch in infrastructure and then they'll get to a point where they realize they're doing a lot of the same stuff and so they come up with this idea",
    "start": "534560",
    "end": "540279"
  },
  {
    "text": "of a feature store and so a feature store is basically a data store that holds features for some given scope of",
    "start": "540279",
    "end": "546880"
  },
  {
    "start": "541000",
    "end": "656000"
  },
  {
    "text": "the business um and this could be say a customer or a household um and it's typically at sometime granularity so um",
    "start": "546880",
    "end": "554680"
  },
  {
    "text": "very often you capture them as a particular um Tim stamp and then you might roll them up to say daily weekly",
    "start": "554680",
    "end": "559920"
  },
  {
    "text": "or yearly so there are a few different ways that you can represent these kind of feature stores but um two of the very",
    "start": "559920",
    "end": "565360"
  },
  {
    "text": "common ones are using eav and tabular so eav T stands for entity attribute value",
    "start": "565360",
    "end": "571800"
  },
  {
    "text": "Tim stamp and what this looks like is this each one of those things is a column so the entity might be the",
    "start": "571800",
    "end": "578600"
  },
  {
    "text": "customer the attribute might be the actual feature type um the value is",
    "start": "578600",
    "end": "583760"
  },
  {
    "text": "obviously the value of that particular feature for that particular entity at the given time which is the final column",
    "start": "583760",
    "end": "589839"
  },
  {
    "text": "so you can imagine this is um fairly efficient storage especially when you start thinking about adding features",
    "start": "589839",
    "end": "594880"
  },
  {
    "text": "because it's just adding new rows to this table um one thing to note is that the value column doesn't actually have a",
    "start": "594880",
    "end": "601839"
  },
  {
    "text": "a consistent type so you have to keep that mapping somewhere else but the one major issue um at least naively on uh",
    "start": "601839",
    "end": "609720"
  },
  {
    "text": "hdfs and and the kinds of things you use with spark is that it's really inefficient to turn this into the format",
    "start": "609720",
    "end": "615959"
  },
  {
    "text": "that you need to do training on so we saw that what we're what we're effectively trying to do when we're training our models is to find a",
    "start": "615959",
    "end": "622200"
  },
  {
    "text": "relationship between the blue yellow and red feature and the actual value of whether or not someone's going to churn",
    "start": "622200",
    "end": "630959"
  },
  {
    "text": "now the other approach is tabular and this looks something more like this which is basically what our pivoted table ended up looking like and so you",
    "start": "630959",
    "end": "637839"
  },
  {
    "text": "can imagine that this for machine learning use cases is dead simple it's it's incredibly efficient because it's",
    "start": "637839",
    "end": "643360"
  },
  {
    "text": "already in the format it needs to be in but you've got this extra complexity of adding new features which is that you",
    "start": "643360",
    "end": "649760"
  },
  {
    "text": "actually have to do some kind of join to get them into the same table and this obviously can become quite expensive as",
    "start": "649760",
    "end": "656079"
  },
  {
    "start": "656000",
    "end": "875000"
  },
  {
    "text": "well um so just for a bit of context this um this Telco had an on-premise",
    "start": "656079",
    "end": "661680"
  },
  {
    "text": "Data Center and they were often fighting for cluster resources so we were also using some of the older spark versions",
    "start": "661680",
    "end": "668519"
  },
  {
    "text": "at the time um and we ended up doing a few quick experiments with um the eav",
    "start": "668519",
    "end": "673920"
  },
  {
    "text": "and the tabular format and with the given resources and so on actually found the tabular format to be slightly more",
    "start": "673920",
    "end": "679160"
  },
  {
    "text": "efficient but there's this is very much an implementation detail um and this will probably change over time and will",
    "start": "679160",
    "end": "685079"
  },
  {
    "text": "very likely um be different for different technology Stacks but what we did is we took a step",
    "start": "685079",
    "end": "690480"
  },
  {
    "text": "back and said okay if we're going to build this centralized feature store what do we need and the first thing that you need is an idea of what a feature",
    "start": "690480",
    "end": "697440"
  },
  {
    "text": "actually is what the feature definition is and so we came up with this idea of a feature library and the idea here is",
    "start": "697440",
    "end": "703200"
  },
  {
    "text": "that it's a single source for feature logic um it will be Loosely coupled from the various systems that actually want",
    "start": "703200",
    "end": "709240"
  },
  {
    "text": "to use it and it's Source controlled and our this particular um",
    "start": "709240",
    "end": "716399"
  },
  {
    "text": "company was using spark extensively and so we said okay they're quite familiar with this let's see what we can use in",
    "start": "716399",
    "end": "721760"
  },
  {
    "text": "the spark libraries to actually implement this stuff and so we came across these things called Transformers",
    "start": "721760",
    "end": "727279"
  },
  {
    "text": "and what Transformers are is they're basically a transformation that lets you take in a data frame and spit out a new",
    "start": "727279",
    "end": "733720"
  },
  {
    "text": "data frame and you can spit out pretty much an arbitrary new data frame if you really want all you need to do is Define",
    "start": "733720",
    "end": "739360"
  },
  {
    "text": "the output schema so what a lot of the machine learning libraries built into spark do is they basically spit out a",
    "start": "739360",
    "end": "745000"
  },
  {
    "text": "new column with the transform data so you have the old version and the new version and so what we did is we built a",
    "start": "745000",
    "end": "750320"
  },
  {
    "text": "very simple wrapper on top of this um the users were mostly familiar with uh the Python and the sort of",
    "start": "750320",
    "end": "756560"
  },
  {
    "text": "objectoriented Concepts so all we did was make this sort of wrapper that enforced that they provided us some",
    "start": "756560",
    "end": "761800"
  },
  {
    "text": "properties um and gave them a few simple helper functions now we also did this thing",
    "start": "761800",
    "end": "767240"
  },
  {
    "text": "where we abstracted over the date range and the idea here is that abstracting over date range lets you calculate the",
    "start": "767240",
    "end": "773440"
  },
  {
    "text": "historical value of these features as well as the current value of these features and this is important because",
    "start": "773440",
    "end": "778839"
  },
  {
    "text": "as I'm mentioned the training data set needs to go over much larger data volume so we want to be able to break this up",
    "start": "778839",
    "end": "784320"
  },
  {
    "text": "and chunk it up and do all the nice things that you want to do with this but the problem is that it's actually really",
    "start": "784320",
    "end": "789480"
  },
  {
    "text": "tedious and complex to Define features in this way for most users um a lot of the systems that they were getting their",
    "start": "789480",
    "end": "795639"
  },
  {
    "text": "features from they already had some SQL queries for example that would just they wanted to basically just paste in and",
    "start": "795639",
    "end": "801560"
  },
  {
    "text": "have this stuff all happen automatically and another issue that we found was that duplication of code and",
    "start": "801560",
    "end": "808160"
  },
  {
    "text": "the correct abstraction of code was starting to become a real problem um and the reason for this is that with spark",
    "start": "808160",
    "end": "814720"
  },
  {
    "text": "you need to know which tables are being read from you need to know um how data",
    "start": "814720",
    "end": "820040"
  },
  {
    "text": "is moving through your system you need to know where IO is happening and you need to try and consolidate these things to avoid all of these expensive um",
    "start": "820040",
    "end": "827079"
  },
  {
    "text": "components within the system and because each one of these features is defined independently of one another um you end",
    "start": "827079",
    "end": "834079"
  },
  {
    "text": "up basically having to know about the implementation details of every other feature to know how you can best",
    "start": "834079",
    "end": "839160"
  },
  {
    "text": "optimize the system which is obviously a very challenging thing to do so ultimately the performance of this",
    "start": "839160",
    "end": "845440"
  },
  {
    "text": "system degraded really quickly when we started letting more and more users use it on top of this it became quite",
    "start": "845440",
    "end": "851800"
  },
  {
    "text": "obvious that users really really wanted the SQL option um or they wanted to be able to sort of interact at a higher",
    "start": "851800",
    "end": "858000"
  },
  {
    "text": "declarative level um and you can imagine something like this is actually really hard to optimize at a domain level",
    "start": "858000",
    "end": "864240"
  },
  {
    "text": "because we then have to go and start paing SQL strings and so on and mapping that back to the actual",
    "start": "864240",
    "end": "869600"
  },
  {
    "text": "things that they're reading from and so on top of that testing is an absolute",
    "start": "869600",
    "end": "874680"
  },
  {
    "text": "nightmare so we went back to the drawing board and we said this is all just way too complex to use there are too many",
    "start": "874680",
    "end": "881240"
  },
  {
    "start": "875000",
    "end": "989000"
  },
  {
    "text": "ways that users can make mistakes and we don't want that we want to make the experience nice and simple for them and",
    "start": "881240",
    "end": "887560"
  },
  {
    "text": "so what we did is we took some inspiration from the Sparks and the tensor flows and the functional programming um communities and basically",
    "start": "887560",
    "end": "894320"
  },
  {
    "text": "said let's think about making a a higher level declarative language and so spark",
    "start": "894320",
    "end": "899440"
  },
  {
    "text": "has this um thing called The Catalyst Optimizer which takes your dag which is the description that you build up of the",
    "start": "899440",
    "end": "905040"
  },
  {
    "text": "program and then basically optimizes it and rewrites it for you to make it run faster so we thought maybe we could do",
    "start": "905040",
    "end": "910519"
  },
  {
    "text": "the same thing but at a higher level at the Domain level and the idea here is that we can take these feature",
    "start": "910519",
    "end": "916000"
  },
  {
    "text": "descriptions potentially and then globally optimize that set of features that are going to be run into efficient",
    "start": "916000",
    "end": "922279"
  },
  {
    "text": "spark code and so we ended up doing exactly that and it looked like this so we'd have uh feature one as declared in",
    "start": "922279",
    "end": "930800"
  },
  {
    "text": "uh scolar at this particular Point um and then feature two feature three we would take it into our feature engine",
    "start": "930800",
    "end": "936560"
  },
  {
    "text": "and spit out these optimized features now the output of these features would become the input to spark",
    "start": "936560",
    "end": "944519"
  },
  {
    "text": "um so the nice thing about this is that we could centralize our logic and we could improve the performance",
    "start": "944519",
    "end": "950120"
  },
  {
    "text": "transparently to users because all they were really interacting with with these bits on the side um the blue yellow and",
    "start": "950120",
    "end": "955680"
  },
  {
    "text": "red parts and so our feature generator would be that abstraction layer we kept improving this making things faster and",
    "start": "955680",
    "end": "962040"
  },
  {
    "text": "faster by writing more and more efficient versions of the code um and this was a really nice property to",
    "start": "962040",
    "end": "968040"
  },
  {
    "text": "have so and as sort of to visualize what this might look like the output might be",
    "start": "968040",
    "end": "973199"
  },
  {
    "text": "an optimized Transformer that rather than spitting out a single column like we saw before actually spits out three",
    "start": "973199",
    "end": "978720"
  },
  {
    "text": "columns in one go so you can imagine if we start combining all of these things we actually get more and more features",
    "start": "978720",
    "end": "984560"
  },
  {
    "text": "written in one go that means a lot less IO which happens between every one of these joins",
    "start": "984560",
    "end": "989880"
  },
  {
    "start": "989000",
    "end": "1243000"
  },
  {
    "text": "and so ultimately most optimization techniques that we applied were about reducing IO sharing compute and sharing",
    "start": "989880",
    "end": "996279"
  },
  {
    "text": "cing where possible so some examples um we're exposing interfaces to make smart use of the hdfs partitions and avoid um",
    "start": "996279",
    "end": "1004480"
  },
  {
    "text": "reading too much data so you don't want to read a you know 100 terabyte table 10 times if you can just read it once for",
    "start": "1004480",
    "end": "1010319"
  },
  {
    "text": "example um we did things like combining multiple features that read from the single table um into a single read and",
    "start": "1010319",
    "end": "1018000"
  },
  {
    "text": "we did things like common Aggregates um so if you had multiple tables that did",
    "start": "1018000",
    "end": "1023440"
  },
  {
    "text": "some kind of group bu or join beforehand and then a whole bunch of features were defined off of that we would then",
    "start": "1023440",
    "end": "1029079"
  },
  {
    "text": "obviously cache that table to avoid needing to recreate that intermediate cache and Spark can do this if you write",
    "start": "1029079",
    "end": "1036400"
  },
  {
    "text": "everything efficiently from the get-go but because these were all independently declared we needed to actually write",
    "start": "1036400",
    "end": "1041720"
  },
  {
    "text": "this for the users on their behalf and the whole idea here is that we avoid the users having to worry about these implementation details of spark so as an",
    "start": "1041720",
    "end": "1049720"
  },
  {
    "text": "example um you can think of the absolute simplest feature which would just be selecting a column from another table",
    "start": "1049720",
    "end": "1055960"
  },
  {
    "text": "that already has the feature logic defined for example and so this might look something like the following you've",
    "start": "1055960",
    "end": "1061080"
  },
  {
    "text": "got your optimized Transformer on the bottom left um and that just takes in that first feature and appends the First",
    "start": "1061080",
    "end": "1067240"
  },
  {
    "text": "Column and then the next Transformer would take in um that First Column and then it would have to do a join but you",
    "start": "1067240",
    "end": "1074160"
  },
  {
    "text": "can see already that this this is a really inefficient way to do this thing right because realistically you could",
    "start": "1074160",
    "end": "1080080"
  },
  {
    "text": "rewrite it like this and this is basically combining that select from the first one and the select from the second",
    "start": "1080080",
    "end": "1085720"
  },
  {
    "text": "one into a single pass of that table and so our feature generator would spit out this optimized feature that would",
    "start": "1085720",
    "end": "1092480"
  },
  {
    "text": "basically just spit out both columns so we went from um the kind of",
    "start": "1092480",
    "end": "1098159"
  },
  {
    "text": "code that looked something like this where users were having to worry about these date ranges and all these",
    "start": "1098159",
    "end": "1103440"
  },
  {
    "text": "different um problems and it became a lot more declarative in nature so basically what was happening is that we",
    "start": "1103440",
    "end": "1109280"
  },
  {
    "text": "would have say a column feature as the simplest abstraction um and we were able to select out just a specific column",
    "start": "1109280",
    "end": "1116039"
  },
  {
    "text": "from it um and then we would have say the table um we'd have a few different defined tables and we'd have efficient",
    "start": "1116039",
    "end": "1122559"
  },
  {
    "text": "partitioning strategies for the different types of tables so that we could actually read these things very quickly um and then all we did was",
    "start": "1122559",
    "end": "1129120"
  },
  {
    "text": "enforce that certain metadata existed and then on top of this we Expos some slightly more uh complex logic such as",
    "start": "1129120",
    "end": "1136320"
  },
  {
    "text": "like windwing event features um doing it counts as I mentioned before um you might think of one of the features as",
    "start": "1136320",
    "end": "1143360"
  },
  {
    "text": "how many times has a user visited this page in the last 30 days um this allows them to literally just say I want this",
    "start": "1143360",
    "end": "1149840"
  },
  {
    "text": "event out of my event store and I want to window it over the last 30 days 60 days 90 days to get sort of the",
    "start": "1149840",
    "end": "1155880"
  },
  {
    "text": "historical Trends but this is a still Scala right I I mentioned that our users were mostly",
    "start": "1155880",
    "end": "1161480"
  },
  {
    "text": "python users so on top of this we said we could probably abstract this even more and make it even simpler and so",
    "start": "1161480",
    "end": "1167799"
  },
  {
    "text": "what we did was move this into a UI and the idea here is that you can use the interfaces that we defined um to create",
    "start": "1167799",
    "end": "1175039"
  },
  {
    "text": "these different types of features and on top of this because it's all defined centrally we can now do things that are",
    "start": "1175039",
    "end": "1181480"
  },
  {
    "text": "really tough to do if it's just a codebase such as um feature Discovery feature metadata um feature management",
    "start": "1181480",
    "end": "1188360"
  },
  {
    "text": "and on top of this we could start doing some really useful testing so we were testing the interfaces um extensively so",
    "start": "1188360",
    "end": "1194080"
  },
  {
    "text": "we knew that they were actually doing what they said they'd do and then users could confirm that their um",
    "start": "1194080",
    "end": "1199320"
  },
  {
    "text": "implementation was working correctly by Looking In You Know by running their features in a Dev environment for",
    "start": "1199320",
    "end": "1204559"
  },
  {
    "text": "example and comparing it against what they expected um and then on top of that we were able to then Define features for",
    "start": "1204559",
    "end": "1211960"
  },
  {
    "text": "particular projects so I mentioned earlier that you have um the training phase taking in a set of features you do",
    "start": "1211960",
    "end": "1219600"
  },
  {
    "text": "some learning to work out which ones you actually want and then that becomes your scoring set so those were very easy to",
    "start": "1219600",
    "end": "1226000"
  },
  {
    "text": "Define at that point through the UI and then we said okay let's open this up to some more users and more",
    "start": "1226000",
    "end": "1232400"
  },
  {
    "text": "features and what happened is that our cluster got absolutely annihilated and",
    "start": "1232400",
    "end": "1237520"
  },
  {
    "text": "the reason for this is because we failed to ask ourselves one big question and",
    "start": "1237520",
    "end": "1243400"
  },
  {
    "start": "1243000",
    "end": "1495000"
  },
  {
    "text": "that question was who pays for this thing and it turns out that if you don't",
    "start": "1243400",
    "end": "1249200"
  },
  {
    "text": "have a good answer to this question nobody pays for it and you end up basically just trying to scale with what",
    "start": "1249200",
    "end": "1254720"
  },
  {
    "text": "you've got and that's never going to work so we we were very unfortunate in this and I really encourage you if",
    "start": "1254720",
    "end": "1260960"
  },
  {
    "text": "you're going down this path or have gone down this path to really think about the right way to do this um but for example",
    "start": "1260960",
    "end": "1266799"
  },
  {
    "text": "if you've got a a question of uh will the feature store get a budget of its own um you need to start thinking about",
    "start": "1266799",
    "end": "1273559"
  },
  {
    "text": "how you actually plan for the capacity of this feature store and um knowing how you're going to have some kind of fair",
    "start": "1273559",
    "end": "1279840"
  },
  {
    "text": "use policy if you are giving it a fixed budget for example and this is going to differ from organization to",
    "start": "1279840",
    "end": "1286039"
  },
  {
    "text": "organization or you can look at something like do the projects actually pay for their usage and if they're",
    "start": "1286039",
    "end": "1291400"
  },
  {
    "text": "paying for their own usage what are they paying for um as I mentioned we're taking all these things and smooshing them into one big um efficient job",
    "start": "1291400",
    "end": "1298679"
  },
  {
    "text": "trying to untangle which bits of compute were actually associated with which features is really challenging so we",
    "start": "1298679",
    "end": "1304400"
  },
  {
    "text": "came up with a solution to this which I'll get into a bit after but ultimately this Cost question",
    "start": "1304400",
    "end": "1310279"
  },
  {
    "text": "led us to rethink the entire idea of whether we should be centralizing a feature store at all and so we did a",
    "start": "1310279",
    "end": "1316520"
  },
  {
    "text": "little bit of investigation and notice the F in problem so project one would come along and they'd say okay we want",
    "start": "1316520",
    "end": "1323200"
  },
  {
    "text": "features one and features two and we want it for time zero and one and we want customers zero and one and then",
    "start": "1323200",
    "end": "1330240"
  },
  {
    "text": "project two would come along and they'd say well we also want feature one and we also want feature three and we want a",
    "start": "1330240",
    "end": "1335440"
  },
  {
    "text": "sort of similar customer base but they're actually not completely overlapping and we actually want to",
    "start": "1335440",
    "end": "1340840"
  },
  {
    "text": "train it on this particular time range and so what ends up happening when you explode this out into its tabular form",
    "start": "1340840",
    "end": "1347279"
  },
  {
    "text": "is that you end up with a ton of of data here that's either not used or it's not defined and the reason that this happens",
    "start": "1347279",
    "end": "1353200"
  },
  {
    "text": "is that you've got these non-overlapping customer sets so um the various Source systems that you get the feature",
    "start": "1353200",
    "end": "1358559"
  },
  {
    "text": "declarations from uh may not even have the data that you need to to Define these",
    "start": "1358559",
    "end": "1363679"
  },
  {
    "text": "features um and so obviously this was a huge problem we're doing a lot of work that we don't need to be doing so we",
    "start": "1363679",
    "end": "1369880"
  },
  {
    "text": "said okay let's start thinking about some optimization that we can do to this whole process and so we looked into the",
    "start": "1369880",
    "end": "1375320"
  },
  {
    "text": "actual usage patterns and noticed out of the 30 million customers um defined",
    "start": "1375320",
    "end": "1381000"
  },
  {
    "text": "across all systems which amusingly is significantly more than the total number of people in the entire country um we",
    "start": "1381000",
    "end": "1388320"
  },
  {
    "text": "had customer set one in in Source system a and customer set two in Source system B and each one of them would say",
    "start": "1388320",
    "end": "1395279"
  },
  {
    "text": "generate the the blue types of features or the red types of features respectively so what that means is that",
    "start": "1395279",
    "end": "1400600"
  },
  {
    "text": "if you've got say an overlap of 10 million and you want to use um you want to do some modeling where you use",
    "start": "1400600",
    "end": "1406240"
  },
  {
    "text": "features from both of those Source systems you kind of need to use this bit in the middle and so what we looked into as",
    "start": "1406240",
    "end": "1413840"
  },
  {
    "text": "well is what are the projects specifically using not just what do people want to be generally having",
    "start": "1413840",
    "end": "1420039"
  },
  {
    "text": "available and it turned out that they were doing something like this they would you know for example being a Telco they might look at postpaid customer",
    "start": "1420039",
    "end": "1426559"
  },
  {
    "text": "churn um for their mobile services and so that would Target a very specific",
    "start": "1426559",
    "end": "1431600"
  },
  {
    "text": "group of customers within one of their Source systems um and this might be say 2 and a half million customers and then",
    "start": "1431600",
    "end": "1437960"
  },
  {
    "text": "if you think about the set of customers that they can actually then um do some meaningful training on it's even smaller",
    "start": "1437960",
    "end": "1443840"
  },
  {
    "text": "again which ended up being say 2 million of the total 30 million now this is obviously a huge difference in scale so",
    "start": "1443840",
    "end": "1451480"
  },
  {
    "text": "we did some quick back of the envelope calculations and you can kind of realize what's what's going wrong here with what",
    "start": "1451480",
    "end": "1456760"
  },
  {
    "text": "we initially tried to do if we have say 30 million users we have 365 days of",
    "start": "1456760",
    "end": "1462000"
  },
  {
    "text": "data to generate we've got two years say um and we've got say a thousand features and the reason we have a thousand",
    "start": "1462000",
    "end": "1467480"
  },
  {
    "text": "features is that every project comes along and says oh I want 300 I want 200 I want these 150 some of them overlap",
    "start": "1467480",
    "end": "1472840"
  },
  {
    "text": "some of them don't but if we're trying to keep all of them in this big Central table we get this big problem where it",
    "start": "1472840",
    "end": "1478159"
  },
  {
    "text": "explodes out massively to say 22 trillion data points but what every project actually",
    "start": "1478159",
    "end": "1483880"
  },
  {
    "text": "needed was closer to half a billion and the reason for this is that we've got a much smaller customer base",
    "start": "1483880",
    "end": "1490919"
  },
  {
    "text": "and we've got a lot less features that each particular project needs so we thought about the existing",
    "start": "1490919",
    "end": "1496520"
  },
  {
    "start": "1495000",
    "end": "1567000"
  },
  {
    "text": "headaches so generating these all of the features for all tables across all of time is an enormous data",
    "start": "1496520",
    "end": "1502480"
  },
  {
    "text": "challenge 22 trillion values for the cluster size that we had was was pretty considerable um probably not for Google",
    "start": "1502480",
    "end": "1509159"
  },
  {
    "text": "scale of course but yeah um so then you the ultimately",
    "start": "1509159",
    "end": "1514399"
  },
  {
    "text": "the problem here is that you're doing way more work than is necessary right uh most of the projects aren't actually using this kind of data um and you're",
    "start": "1514399",
    "end": "1521120"
  },
  {
    "text": "always increasing your compute burden as you add new features in history and just just on that previous Point as well very",
    "start": "1521120",
    "end": "1526760"
  },
  {
    "text": "often um the the good advice is to have as much data as you possibly can but we were",
    "start": "1526760",
    "end": "1532760"
  },
  {
    "text": "founding we were finding sorry that most projects that got all the data they requested was really struggling to",
    "start": "1532760",
    "end": "1537960"
  },
  {
    "text": "actually scale their training algorithms so it actually was was even worse the",
    "start": "1537960",
    "end": "1543120"
  },
  {
    "text": "the problem of getting to Market quickly was multiplied by doing these bigger and bigger data",
    "start": "1543120",
    "end": "1548399"
  },
  {
    "text": "sets so on top of this if you're using hdfs trying to keep these things highly available while adding new data to it",
    "start": "1548399",
    "end": "1554520"
  },
  {
    "text": "and so on and so forth is actually quite a challenge and ultimately you end up needing to do things like copying the data and keeping it immutable which is a",
    "start": "1554520",
    "end": "1561840"
  },
  {
    "text": "great pattern but at the scales that we have is actually a huge problem it takes a long time so we had this crazy idea maybe",
    "start": "1561840",
    "end": "1569080"
  },
  {
    "text": "instead of having this big Central thing where it seems like everything is available and easily um grabbable what",
    "start": "1569080",
    "end": "1574360"
  },
  {
    "text": "if we just generate the training data when people need it because we have these much smaller data sets that people",
    "start": "1574360",
    "end": "1579960"
  },
  {
    "text": "are actually using in in practice um and this would remove an enormous overhead",
    "start": "1579960",
    "end": "1586240"
  },
  {
    "text": "it would like the engineering cost of maintaining this Central table is is huge in particular we were using older",
    "start": "1586240",
    "end": "1592440"
  },
  {
    "text": "versions of spark which were just the most buggy thing you can imagine I hear a few people that",
    "start": "1592440",
    "end": "1599120"
  },
  {
    "text": "probably feel the same so what this meant is that we could get teams to start modeling a lot quicker and the",
    "start": "1599120",
    "end": "1605120"
  },
  {
    "text": "reason that we could do this is because we were generating a lot less data so if",
    "start": "1605120",
    "end": "1610320"
  },
  {
    "text": "someone comes along and says hey I'm going to sample my customer base and I'm going to look at the last 3 months of History they could cut down the amount",
    "start": "1610320",
    "end": "1617640"
  },
  {
    "text": "of data that they needed considerably so instead of us having to append this to this massive table and have a whole",
    "start": "1617640",
    "end": "1622799"
  },
  {
    "text": "bunch of data that has to be generated for it to append properly we could just generate what they needed and ultimately",
    "start": "1622799",
    "end": "1630080"
  },
  {
    "text": "because we were doing this generation every time the thing that we spat out was in the perfect format for them to do their machine learning",
    "start": "1630080",
    "end": "1636320"
  },
  {
    "text": "on so For Better or Worse we explored this approach and decided that we need to make snapshot generator generation as",
    "start": "1636320",
    "end": "1643760"
  },
  {
    "text": "fast as possible and so we looked into some of the things that were actually the botton necks in system and found two",
    "start": "1643760",
    "end": "1649080"
  },
  {
    "start": "1645000",
    "end": "1777000"
  },
  {
    "text": "kind of key things and the first one was the dag so we look at the spark dag you",
    "start": "1649080",
    "end": "1654399"
  },
  {
    "text": "can if you recall we have these feature Transformers so they are doing they're taking in some data frame they're doing",
    "start": "1654399",
    "end": "1660279"
  },
  {
    "text": "a join with the features and then every time we take in the next one say and we join two more features on and what ends",
    "start": "1660279",
    "end": "1666080"
  },
  {
    "text": "up happening with the spark dag is it looks like this and if you can probably spot the problem we've got this linear",
    "start": "1666080",
    "end": "1673120"
  },
  {
    "text": "sequence of joins that needs to happen right and the problem with this is that the first joint needs to complete",
    "start": "1673120",
    "end": "1679279"
  },
  {
    "text": "entirely before the next one starts we have a dependency relation between them but really we don't care what order they",
    "start": "1679279",
    "end": "1685559"
  },
  {
    "text": "join in so long as they all end up in the last table right so this was a huge",
    "start": "1685559",
    "end": "1690640"
  },
  {
    "text": "performance bottleneck because this also represents the io in spark and so what we did is we said how",
    "start": "1690640",
    "end": "1696919"
  },
  {
    "text": "about we push some of these features up to the leaves in a tree and we said okay",
    "start": "1696919",
    "end": "1702919"
  },
  {
    "text": "let's generate a you know each one of these optimized sets of features independently",
    "start": "1702919",
    "end": "1708799"
  },
  {
    "text": "um we'll join each one of them logarithmically and end up with that final table and what we had to do to make this happen is instead of using a",
    "start": "1708799",
    "end": "1715039"
  },
  {
    "text": "big left join where we started with all of the customers and all of the dates we said we'll just do whatever each one of",
    "start": "1715039",
    "end": "1721159"
  },
  {
    "text": "these leaves care about and then that will um naturally fall down with full outer joins and so the dag ends up looking",
    "start": "1721159",
    "end": "1728039"
  },
  {
    "text": "something more like this and the nice thing about this kind of dag is that you can throw more and more executors at it",
    "start": "1728039",
    "end": "1734120"
  },
  {
    "text": "and it will continue to get faster and faster until a specific point and that specific point tended to happen very",
    "start": "1734120",
    "end": "1740760"
  },
  {
    "text": "much towards the bottom of this dag and so what we actually found if we were to compare the old approach versus",
    "start": "1740760",
    "end": "1746679"
  },
  {
    "text": "the new approach is that the old approach is this blue line right and so it would shoot up to 100% utilization",
    "start": "1746679",
    "end": "1752760"
  },
  {
    "text": "very quickly it would it would be calculating the features for a little while but then it would very quickly end",
    "start": "1752760",
    "end": "1759480"
  },
  {
    "text": "up in this linear join that would take a really long time with the new approach though we",
    "start": "1759480",
    "end": "1764799"
  },
  {
    "text": "found that the executor were being utilized a lot more the time and this meant that we could um throw more and",
    "start": "1764799",
    "end": "1771240"
  },
  {
    "text": "more executors at it and it would become faster and faster until you get that similar kind of peak",
    "start": "1771240",
    "end": "1777398"
  },
  {
    "start": "1777000",
    "end": "1866000"
  },
  {
    "text": "look now some other thing we we did a whole bunch of other optimizations as you can imagine but another one of the key ones that we noticed is actually a",
    "start": "1777640",
    "end": "1784360"
  },
  {
    "text": "really simple one but it's it's got very big consequences so with parallel processing",
    "start": "1784360",
    "end": "1791000"
  },
  {
    "text": "you can imagine um if you've got 30 tasks that you want to process each task takes time T and you've got 15 resources",
    "start": "1791000",
    "end": "1797440"
  },
  {
    "text": "to process this thing um you do the first 15 in time T alone and then you do",
    "start": "1797440",
    "end": "1803480"
  },
  {
    "text": "the next 15 in time t as well so the entire thing takes 2 T if you have 200 things to process and",
    "start": "1803480",
    "end": "1811799"
  },
  {
    "text": "you do the first 196 of them in time T those last four also still take time T",
    "start": "1811799",
    "end": "1819440"
  },
  {
    "text": "and when you look at these things visually in Sparks UI in particular this is a little bit counterintuitive but",
    "start": "1819440",
    "end": "1824880"
  },
  {
    "text": "you're actually taking the same amount of time to do this even though there's only one or two tasks that are that are dangling at the end here and so you need",
    "start": "1824880",
    "end": "1832200"
  },
  {
    "text": "to either drop the number of um tasks that you want to be parallelizing or",
    "start": "1832200",
    "end": "1837519"
  },
  {
    "text": "increase the number of executors such that you can run this in a single pass and that will literally have the amount of time and so for us because we were",
    "start": "1837519",
    "end": "1844559"
  },
  {
    "text": "doing um you know pushing all of these computations into these stages what ended up happening is this T was really",
    "start": "1844559",
    "end": "1850240"
  },
  {
    "text": "big and when we're doing something like this multiple times and each one of these T's are double what they really",
    "start": "1850240",
    "end": "1855799"
  },
  {
    "text": "need to be this is a huge overhead so this tiny little optimization actually more than doubled the time it took for",
    "start": "1855799",
    "end": "1862240"
  },
  {
    "text": "our jobs to run sorry half the time and so finally we looked at the",
    "start": "1862240",
    "end": "1868200"
  },
  {
    "start": "1866000",
    "end": "2033000"
  },
  {
    "text": "spark version that we were using and we were lucky enough to convince the right people to actually upgrade the platform",
    "start": "1868200",
    "end": "1874200"
  },
  {
    "text": "and if you're stuck on 16 still um or if you're looking to get in please do not use 16 um performance in 2.x is",
    "start": "1874200",
    "end": "1882440"
  },
  {
    "text": "astronomically better we benchmarked it on some of our use cases and found it to be up to 17 times faster um um it's",
    "start": "1882440",
    "end": "1888720"
  },
  {
    "text": "still obviously not necessarily the fastest thing out there but in terms of um Simplicity it's it's really",
    "start": "1888720",
    "end": "1894559"
  },
  {
    "text": "nice and on top of that one of the biggest pain points that we felt um prior to the upgrade was actually",
    "start": "1894559",
    "end": "1901639"
  },
  {
    "text": "stability we would do stuff that seemed as simple as just taking some data and writing it to a file and it would crash",
    "start": "1901639",
    "end": "1907840"
  },
  {
    "text": "for a million different reasons that just seemed completely unrelated to what we were trying to do and 2.x fixed a lot",
    "start": "1907840",
    "end": "1915159"
  },
  {
    "text": "of these problems so definitely check it out if you haven't already",
    "start": "1915159",
    "end": "1919840"
  },
  {
    "text": "so in terms of the actual relative performance gains um don't worry too much about the actual number of hours this is a function of you know our",
    "start": "1921639",
    "end": "1927919"
  },
  {
    "text": "compute and um the types of features that we're generating and as you can imagine types of features are a pretty",
    "start": "1927919",
    "end": "1934360"
  },
  {
    "text": "bad thing to be benchmarking on because as I said it could be as simple as just you know selecting a column from another",
    "start": "1934360",
    "end": "1940200"
  },
  {
    "text": "table or even just generating some data or it could be as complex as looking over the last 10 years worth of data and",
    "start": "1940200",
    "end": "1945840"
  },
  {
    "text": "doing some complex aggregation across these things um what we found is that",
    "start": "1945840",
    "end": "1951600"
  },
  {
    "text": "with the standard approach um we were taking say twice as much time as a",
    "start": "1951600",
    "end": "1957240"
  },
  {
    "text": "declarative approach even for the simple tiny little generators but for the declarative and optimized approach it",
    "start": "1957240",
    "end": "1963440"
  },
  {
    "text": "was so much faster that it was basically negligible it was taking something like 15 minutes to generate 3 months of",
    "start": "1963440",
    "end": "1969120"
  },
  {
    "text": "history for say um 50 features which isn't what you want to be training your final models on most likely but it's a",
    "start": "1969120",
    "end": "1975720"
  },
  {
    "text": "really great starting point you can get that in 15 minutes see if you've got any kind of um meaningful correlations and",
    "start": "1975720",
    "end": "1981519"
  },
  {
    "text": "do some of your feature selection and all this kind of stuff before moving on to the bigger and bigger data sets and",
    "start": "1981519",
    "end": "1987120"
  },
  {
    "text": "more importantly you can see in the in the rightmost column in each one of these things the first two would never",
    "start": "1987120",
    "end": "1993320"
  },
  {
    "text": "actually finish on the big feature generation jobs they would either crash run out of memory we wouldn't be able to",
    "start": "1993320",
    "end": "1999320"
  },
  {
    "text": "get enough compute or they would literally just never finish um I say never because we never let them run long",
    "start": "1999320",
    "end": "2005200"
  },
  {
    "text": "enough we we let one of them run about four or 5 days and it finished so we very much gave up and now what what this",
    "start": "2005200",
    "end": "2011480"
  },
  {
    "text": "meant is that we now have the ability to go from um the very beginning of a",
    "start": "2011480",
    "end": "2017159"
  },
  {
    "text": "project declaring your features in a a UI and having an actual snapshot of data",
    "start": "2017159",
    "end": "2023440"
  },
  {
    "text": "that you can use for a big data training set and this is normally something that takes weeks to months at a lot of",
    "start": "2023440",
    "end": "2028960"
  },
  {
    "text": "Enterprises that aren't mature with this kind of stuff so finally once once we've trained",
    "start": "2028960",
    "end": "2036159"
  },
  {
    "start": "2033000",
    "end": "2156000"
  },
  {
    "text": "these models um I I I haven't spoken too much about the actual training of the models um in",
    "start": "2036159",
    "end": "2042480"
  },
  {
    "text": "terms of how you scale this because there are a lot of things out on the market now that do a really good job of this um in particular automl is becoming",
    "start": "2042480",
    "end": "2049960"
  },
  {
    "text": "more and more popular and it's able to automate a lot of the different training um strategies that you might want so can",
    "start": "2049960",
    "end": "2055839"
  },
  {
    "text": "do all these various grid searches and stuff um and find the best set of hyper parameters and so on and so forth um but",
    "start": "2055839",
    "end": "2062839"
  },
  {
    "text": "finally model serving is as I mentioned where you take that model that you're happy with and you you want to put it in",
    "start": "2062839",
    "end": "2068280"
  },
  {
    "text": "production and you want to start making predictions with it and it looks again something like",
    "start": "2068280",
    "end": "2073320"
  },
  {
    "text": "this you're going to need your features those set of features they're going to have to generate in production um and",
    "start": "2073320",
    "end": "2079079"
  },
  {
    "text": "then you also want the final model that is taken out of the data science process and there are kind of two",
    "start": "2079079",
    "end": "2085560"
  },
  {
    "text": "primary ways that you can do this um and that is sort of conceptually a data to model scoring approach where you um send",
    "start": "2085560",
    "end": "2092480"
  },
  {
    "text": "your data to your models um and this is you know very common in the sort of containerized world with kubernetes and",
    "start": "2092480",
    "end": "2098680"
  },
  {
    "text": "so on and so forth um and then the other approach is kind of the uh the Big Data",
    "start": "2098680",
    "end": "2103720"
  },
  {
    "text": "approach I guess where you want to be doing massive scoring runs where you might score your entire customer set um",
    "start": "2103720",
    "end": "2109520"
  },
  {
    "text": "and this is kind of more your spark worlds and your Flink worlds and all this kind of stuff and so the idea here is that you send your data to the place",
    "start": "2109520",
    "end": "2116200"
  },
  {
    "text": "sorry your models to the place that the data actually sits to avoid all of that expensive",
    "start": "2116200",
    "end": "2121880"
  },
  {
    "text": "IO and so the way that our um feature descriptions and feature generat were",
    "start": "2122400",
    "end": "2127920"
  },
  {
    "text": "able to slot in is that because they were just descriptions we could pass them around we didn't have to do anything special here um and ultimately",
    "start": "2127920",
    "end": "2135280"
  },
  {
    "text": "they can just sit in between the data sources and whatever you care about at scoring time um now because we were um",
    "start": "2135280",
    "end": "2142320"
  },
  {
    "text": "using spark and so on we we use the right hand side of this um with the model to data",
    "start": "2142320",
    "end": "2148800"
  },
  {
    "text": "deployment but we still have some problems right because different projects have different requirements on",
    "start": "2148800",
    "end": "2154520"
  },
  {
    "text": "the type of scoring they want to do so for examp example the feature values might update at different rates you",
    "start": "2154520",
    "end": "2160680"
  },
  {
    "text": "might have a data source that historically you can obviously get all the data for but right now it's actually",
    "start": "2160680",
    "end": "2165880"
  },
  {
    "text": "3 days delayed and all of your other data sources are up to date so some some",
    "start": "2165880",
    "end": "2171280"
  },
  {
    "text": "projects might care about this and they might say well okay I just want the most recent data that I can get for all of them and they might have done some",
    "start": "2171280",
    "end": "2177240"
  },
  {
    "text": "clever stuff in their modeling to compensate for this another approach might be to say well I actually I only",
    "start": "2177240",
    "end": "2183359"
  },
  {
    "text": "want the most recent consistent snapshot as of a particular date um and so other",
    "start": "2183359",
    "end": "2189000"
  },
  {
    "text": "people might want to only materialize the bits that are up to a certain point in time and finally other projects might",
    "start": "2189000",
    "end": "2196480"
  },
  {
    "text": "actually have as part of their feature pre-processing um the idea of imputation and this basically means taking a guess",
    "start": "2196480",
    "end": "2203200"
  },
  {
    "text": "as to what data you want in here and there are various different strategies for this but um if if the feature that",
    "start": "2203200",
    "end": "2210119"
  },
  {
    "text": "you're imputing is important this can actually be quite problematic But ultimately what we ended",
    "start": "2210119",
    "end": "2215599"
  },
  {
    "text": "up saying is okay let's treat everything uh using the kapper architecture we're going to treat everything as a stream",
    "start": "2215599",
    "end": "2221119"
  },
  {
    "text": "we're going to pass it through Kafka and because we have these declarative features we can run them on both the",
    "start": "2221119",
    "end": "2226800"
  },
  {
    "text": "batch and the streaming sources um just by implementing a slightly different optimization on the back end and that",
    "start": "2226800",
    "end": "2232960"
  },
  {
    "text": "would spit everything out onto this Kafka topic as eav and what this meant is that we could then materialize it in",
    "start": "2232960",
    "end": "2240760"
  },
  {
    "text": "whatever format we cared about so for the projects that wanted it um as a as a big batch we would materialize it into",
    "start": "2240760",
    "end": "2246880"
  },
  {
    "text": "HIV say and and if they wanted it just as the most recent value at a particular point in time we could do it into hbas",
    "start": "2246880",
    "end": "2252480"
  },
  {
    "text": "and they could do selects as of particular dates and stuff um and then we also would allow obviously people to",
    "start": "2252480",
    "end": "2258000"
  },
  {
    "text": "connect directly to Kafka and start doing Kafka streaming things for the more specific and um particular use",
    "start": "2258000",
    "end": "2263319"
  },
  {
    "text": "cases and ultimately all of these would feed into the model scoring process and some people even with the um cfra option",
    "start": "2263319",
    "end": "2271119"
  },
  {
    "text": "would use these for various other options so there's an example um someone calls up um the call center and we want",
    "start": "2271119",
    "end": "2277800"
  },
  {
    "text": "to be able to score a model very quickly and um basically give a recommendation as to what to talk to them",
    "start": "2277800",
    "end": "2284040"
  },
  {
    "text": "about so ultimately um there are a few key takeaways I think um the first is that naive centralization and I stress",
    "start": "2284040",
    "end": "2291160"
  },
  {
    "start": "2285000",
    "end": "2426000"
  },
  {
    "text": "the naive is very hard and very expensive um it really pays to work out",
    "start": "2291160",
    "end": "2297040"
  },
  {
    "text": "the real usage cases not just the desired usage cases because big data is as I said way more complex than it",
    "start": "2297040",
    "end": "2303839"
  },
  {
    "text": "really should be but it it it just is um so if you can cut down your problem to",
    "start": "2303839",
    "end": "2309160"
  },
  {
    "text": "be a less big problem then you're actually dealing with much simpler",
    "start": "2309160",
    "end": "2314200"
  },
  {
    "text": "problems um and working out how to manage resources",
    "start": "2314200",
    "end": "2319359"
  },
  {
    "text": "and cost early on is really important so as I said we kind of went down the path and then realized about halfway that",
    "start": "2319359",
    "end": "2324839"
  },
  {
    "text": "this wasn't something the business had been on board with um in terms of how it was going to be funded you really really",
    "start": "2324839",
    "end": "2330200"
  },
  {
    "text": "really need to do that early on additionally um a lot of you are",
    "start": "2330200",
    "end": "2335319"
  },
  {
    "text": "probably thinking there are so many different ways that you can write features how on Earth could you possibly",
    "start": "2335319",
    "end": "2341000"
  },
  {
    "text": "write interfaces to to cover all of these and the reality is you don't usually need to so a lot of businesses",
    "start": "2341000",
    "end": "2346880"
  },
  {
    "text": "work in Fairly constrained domains they have a a limited set of data sources um so we went for the 8020 Rule and we said",
    "start": "2346880",
    "end": "2354640"
  },
  {
    "text": "let's Define the types of computations on the types of data sources that people are using um and we'll hyper optimize",
    "start": "2354640",
    "end": "2362000"
  },
  {
    "text": "these things we we'll make these things really efficient to do and then we'll expose um the underlying um Transformer",
    "start": "2362000",
    "end": "2367960"
  },
  {
    "text": "interfaces for anyone that needs to do anything more specific than that so this this means that say it was taking you",
    "start": "2367960",
    "end": "2373839"
  },
  {
    "text": "know a couple of weeks to Define all of your features previously you could get most of that done in one day and then",
    "start": "2373839",
    "end": "2379480"
  },
  {
    "text": "you just spend a couple of days say doing the really um specific stuff to your project and this is a huge win for",
    "start": "2379480",
    "end": "2384800"
  },
  {
    "text": "the business because it means that you can get to Market a lot quicker now another key part to this",
    "start": "2384800",
    "end": "2391839"
  },
  {
    "text": "entire design um and this entire process is that features need to be independently declarable um the biggest",
    "start": "2391839",
    "end": "2399359"
  },
  {
    "text": "problem with having features not independently declarable is that you have to worry about implementation and",
    "start": "2399359",
    "end": "2404480"
  },
  {
    "text": "you have to worry about how they all relate to one another and this is not what you ideally want to be doing um",
    "start": "2404480",
    "end": "2410680"
  },
  {
    "text": "because on top of this the more declarative you make this interaction um the more you can actually do for your users transparently you can take as we",
    "start": "2410680",
    "end": "2417200"
  },
  {
    "text": "did for example take these features smash them together and and basically write a compiler to make it more",
    "start": "2417200",
    "end": "2422560"
  },
  {
    "text": "efficient on the execution side of things uh so that's about all I've got got today uh thank you very much and",
    "start": "2422560",
    "end": "2429280"
  },
  {
    "start": "2426000",
    "end": "2841000"
  },
  {
    "text": "hope you've enjoyed [Applause]",
    "start": "2429280",
    "end": "2435110"
  },
  {
    "text": "it any",
    "start": "2435359",
    "end": "2438759"
  },
  {
    "text": "questions so pretty intense people are still digesting",
    "start": "2442440",
    "end": "2447880"
  },
  {
    "text": "stuff so so so so two questions the one would be you know thinking about",
    "start": "2447880",
    "end": "2453319"
  },
  {
    "text": "changing the framework completely you know there's like data flow SL beam Etc so one question would be was that a",
    "start": "2453319",
    "end": "2460640"
  },
  {
    "text": "consideration for you guys um so we were on Prem um we there was no plans for",
    "start": "2460640",
    "end": "2467880"
  },
  {
    "text": "this particular company to migrate to the cloud when I was there um things like beam I don't think would have added",
    "start": "2467880",
    "end": "2473480"
  },
  {
    "text": "that much value on top of spark to be honest at the time um I think certainly if we had something like gcp as an",
    "start": "2473480",
    "end": "2479599"
  },
  {
    "text": "option and we wanted to use data flow um that would have been very very nice but that would also be a really nice way as",
    "start": "2479599",
    "end": "2485560"
  },
  {
    "text": "well potentially to abstract over the implementation of this thing so yeah absolutely if that's if that's something",
    "start": "2485560",
    "end": "2491119"
  },
  {
    "text": "that is important to the company then that's that would be a great option yeah because I mean yeah some saying the Google guy has to mention a little bit",
    "start": "2491119",
    "end": "2497200"
  },
  {
    "text": "but I think some of the some of the challenges right and like you suddenly you have a certain set of data and doing",
    "start": "2497200",
    "end": "2502520"
  },
  {
    "text": "this on premise if they always have there right you getting the hardware getting provisioning those are the classic kind of drivers to like hey if",
    "start": "2502520",
    "end": "2508520"
  },
  {
    "text": "you can do this on demand exactly right it makes that point go away and then what I really like in your talk is that",
    "start": "2508520",
    "end": "2515560"
  },
  {
    "text": "um it's interesting to see that making some of the base constraints go away",
    "start": "2515560",
    "end": "2521160"
  },
  {
    "text": "makes you sort of unravel and rethink the complete architecture right somehow people had an assumption that getting",
    "start": "2521160",
    "end": "2526280"
  },
  {
    "text": "the data set was difficult to do so they made a decision that like well build a thing that has everything but once you",
    "start": "2526280",
    "end": "2532119"
  },
  {
    "text": "solve that constraint of it's no longer that difficult to make a data set it really simplifies your architecture I",
    "start": "2532119",
    "end": "2538760"
  },
  {
    "text": "think that process is one that really I just want to put back in people's heads right like often you have these",
    "start": "2538760",
    "end": "2543960"
  },
  {
    "text": "presupposed assumptions that oh a centralized thing that does everything thing is good right but it's only good",
    "start": "2543960",
    "end": "2549680"
  },
  {
    "text": "if you're living with certain constraints and if you make those STS constraints go away it really broadens your option I think this is a very nice",
    "start": "2549680",
    "end": "2556280"
  },
  {
    "text": "example of that yeah thank you yeah so hopefully that broke the eyes",
    "start": "2556280",
    "end": "2562240"
  },
  {
    "text": "for",
    "start": "2562240",
    "end": "2564480"
  },
  {
    "text": "questions um hi sure sorry",
    "start": "2568160",
    "end": "2575880"
  },
  {
    "text": "um so sorry I I I did miss part of part of what you're doing so I might I might",
    "start": "2578440",
    "end": "2583559"
  },
  {
    "text": "be asking a question specifically about something you you you answered but but the",
    "start": "2583559",
    "end": "2589800"
  },
  {
    "text": "um you were talking about historical data and and current and and the event flows and everything else but sometimes",
    "start": "2589800",
    "end": "2597440"
  },
  {
    "text": "we want to uh sorry I work at standard charted and and we're trying to use machine",
    "start": "2597440",
    "end": "2602880"
  },
  {
    "text": "learning for Signature comparisons and those things MH and um so so so often",
    "start": "2602880",
    "end": "2608040"
  },
  {
    "text": "we're saying it's it's a real-time event that we want to throw into the pile and get a response out instantaneously y um",
    "start": "2608040",
    "end": "2614960"
  },
  {
    "text": "and and the the model that we need to apply against it is one that was pre-computed some time ago and and those kind",
    "start": "2614960",
    "end": "2621160"
  },
  {
    "text": "of have you in in this type of setup of of Distributing that workout have you",
    "start": "2621160",
    "end": "2627160"
  },
  {
    "text": "dealt with anything in that in that kind of space yeah yeah so that that's actually it sounds like you're saying",
    "start": "2627160",
    "end": "2632760"
  },
  {
    "text": "you've got some offline process that actually trains the model and then you want to score it in real time by",
    "start": "2632760",
    "end": "2638040"
  },
  {
    "text": "basically spitting stuff through and that that's actually um on this slide exactly what this is doing right you've got uh say a Kafka topic coming through",
    "start": "2638040",
    "end": "2645359"
  },
  {
    "text": "here and you want to calculate whatever features need to go into that model um spit it onto the topic and then they ultimately go straight into your model",
    "start": "2645359",
    "end": "2651920"
  },
  {
    "text": "as the inputs and then you you just get a stream of predictions if that's if that's the kind of thing you're",
    "start": "2651920",
    "end": "2657160"
  },
  {
    "text": "doing um sorry I wanted to pass the model in as well I wanted lots of models",
    "start": "2657160",
    "end": "2663720"
  },
  {
    "text": "lot yeah yeah so the the models are Downstream of this whole system so as many model as you and basically we so",
    "start": "2663720",
    "end": "2670559"
  },
  {
    "text": "I didn't that about it but we actually had kind of a mini framework internally that you could just submit a model to",
    "start": "2670559",
    "end": "2676040"
  },
  {
    "text": "and it would basically deploy it at the end of this pipeline for you um and this would basically just automate that whole process that was what I wanted thank you",
    "start": "2676040",
    "end": "2684960"
  },
  {
    "text": "yeah yeah so in this type of setup like was there any need of doing model versioning because like people run",
    "start": "2687000",
    "end": "2693839"
  },
  {
    "text": "experiment look at result and then iterate over that yeah absolutely so there was I mean obviously there's the",
    "start": "2693839",
    "end": "2700160"
  },
  {
    "text": "model versioning on training and then there's the model versioning on what we're actually deploying so um if you're",
    "start": "2700160",
    "end": "2705200"
  },
  {
    "text": "kind of going down uh this path and you've kind on the left hand side um",
    "start": "2705200",
    "end": "2710760"
  },
  {
    "text": "with the kubernetes you can put your models in say a Docker container and then that can be your unit of deployment",
    "start": "2710760",
    "end": "2716839"
  },
  {
    "text": "um and you can kind of version models that way um there's a lot of different approaches to this hdfs it seems like",
    "start": "2716839",
    "end": "2722480"
  },
  {
    "text": "the general approach people use is to actually just save the um serialized models on the file system I'm not really",
    "start": "2722480",
    "end": "2729440"
  },
  {
    "text": "a big fan of that but that's another potential option um we actually used I think it was artifactory so we basically",
    "start": "2729440",
    "end": "2736200"
  },
  {
    "text": "just published them as artifacts and then pulled them down very similar to kind of what you'd do with uh if it was",
    "start": "2736200",
    "end": "2741559"
  },
  {
    "text": "managed through Docker um and then we'd pull that down and pull out the specific ones that people were asking to",
    "start": "2741559",
    "end": "2748440"
  },
  {
    "text": "score okay",
    "start": "2754520",
    "end": "2758520"
  },
  {
    "text": "thank you uh could you elaborate a little bit on the differences of data to model scoring versus model to data yeah",
    "start": "2763599",
    "end": "2769960"
  },
  {
    "text": "yeah so the main the main point here is that um one of them you're sending your data over the network to be scored um",
    "start": "2769960",
    "end": "2777079"
  },
  {
    "text": "and the other one you're basically sending your models to the data such as it scores locally um so depending on the",
    "start": "2777079",
    "end": "2782920"
  },
  {
    "text": "type generally it's the volume of data you're trying to score at a time so um the one on the left is usually if you",
    "start": "2782920",
    "end": "2788119"
  },
  {
    "text": "say running a website and you want to do a quick score of a particular customer or something like that whereas on the right it might be if we want to predict",
    "start": "2788119",
    "end": "2794400"
  },
  {
    "text": "churn across all of our customers we want to do that in one big batch job and we do that maybe once a week um so you",
    "start": "2794400",
    "end": "2799760"
  },
  {
    "text": "want to instead of sending all that data over the network which is really slow and expensive um you send your models to",
    "start": "2799760",
    "end": "2805960"
  },
  {
    "text": "the actual data such that it runs locally and is is done a lot quicker and this is where things like spark that Distributors and all that kind of stuff",
    "start": "2805960",
    "end": "2812440"
  },
  {
    "text": "works really nicely if you're sending the model you don't train it right you just the score yes yeah this is all for",
    "start": "2812440",
    "end": "2819880"
  },
  {
    "text": "scoring yeah okay yeah so training is a completely separate step okay thank you no",
    "start": "2819880",
    "end": "2828880"
  },
  {
    "text": "worries awesome thank you everyone it's 4:25 now we head out for a Break um and",
    "start": "2828960",
    "end": "2835960"
  },
  {
    "text": "be back here at from 4:45 thank you Cameron thank you",
    "start": "2835960",
    "end": "2842839"
  }
]