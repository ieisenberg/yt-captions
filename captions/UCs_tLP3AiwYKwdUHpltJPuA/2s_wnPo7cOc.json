[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "Welcome, everyone, to the GOTO Book Club.",
    "start": "1670",
    "end": "7379"
  },
  {
    "text": "We are here in Amsterdam now, and I'm Alyona\nGalyeva from PyLadies Amsterdam and ThoughtWorks,",
    "start": "7379",
    "end": "13950"
  },
  {
    "text": "I'm here to introduce you to Katharine Jarmul. Hi. I'm Katharine Jarmul.",
    "start": "13950",
    "end": "19250"
  },
  {
    "text": "I'm excited to be here with you. I'm also at ThoughtWorks. I work right now as a principal data scientist.",
    "start": "19250",
    "end": "26800"
  },
  {
    "text": "And we're here, I think, to talk about my\nrecent O'Reilly book which is called \"Practical Data Privacy\" and is aimed towards data folks\nand technical folks that wanna learn about",
    "start": "26800",
    "end": "36820"
  },
  {
    "text": "privacy. So really excited to chat with you about this\ntoday. I'm excited too, probably overexcited.",
    "start": "36820",
    "end": "42750"
  },
  {
    "text": "But, anyway, so I was curious, what exactly\ninspired you to write this book in the first",
    "start": "42750",
    "end": "51280"
  },
  {
    "start": "45000",
    "end": "158000"
  },
  {
    "text": "place? I think probably a lot of things.",
    "start": "51280",
    "end": "56640"
  },
  {
    "text": "First and foremost, I've been working in the\nfield of data privacy in data science and machine learning now for multiple years.",
    "start": "56640",
    "end": "63449"
  },
  {
    "text": "I think I learned a lot the hard way. So when I first got interested, I started\nasking questions like, okay, is privacy important",
    "start": "63450",
    "end": "71479"
  },
  {
    "text": "in machine learning? How are we gonna do it? How would we even go about that? A lot of the material that was available was\neither really high-level, really basic stuff,",
    "start": "71479",
    "end": "81509"
  },
  {
    "text": "so, like, kind of older techniques, maybe\neven sometimes broken techniques that we wouldn't recommend anymore, or it was hardcore research.",
    "start": "81510",
    "end": "89400"
  },
  {
    "text": "It was, like, you're a PhD in differential\nprivacy, here's a paper, let's go.",
    "start": "89400",
    "end": "94450"
  },
  {
    "text": "Or here's a paper on cryptography. Like, figure it out yourself.",
    "start": "94450",
    "end": "99548"
  },
  {
    "text": "I think that through that learning was probably\na lot of battles I wouldn't have had to do.",
    "start": "99549",
    "end": "106600"
  },
  {
    "text": "The idea of the book was really like a gift\nto me five, or six years ago.",
    "start": "106600",
    "end": "112470"
  },
  {
    "text": "These are all the shortcuts that you eventually\nfind out the hard way. If I had to teach it to you again, here's\nhow I would teach it.",
    "start": "112470",
    "end": "121479"
  },
  {
    "text": "And kind of the goal is to demystify the field\na little bit of privacy engineering and just",
    "start": "121479",
    "end": "127229"
  },
  {
    "text": "of technical privacy in general and get more\npeople into it. Because I think it should be, like, more accessible,\neasier to understand fully, like, if we wanna",
    "start": "127229",
    "end": "137640"
  },
  {
    "text": "use the term democratize, but available for\nmore people. Wow. I think that's nice.",
    "start": "137640",
    "end": "144019"
  },
  {
    "text": "It's a gift back to me and the rest of the\nfolks. I wish we had more such beautiful books available\nwhere you can just go, \"Oh, I just need to",
    "start": "144019",
    "end": "152470"
  },
  {
    "text": "talk to this knowledge.\" Ding, ding, ding. Ready. The next one. That's your book, right?",
    "start": "152470",
    "end": "157760"
  },
  {
    "text": "Fantastic. So before, let's say, deep diving a little\nbit more into this book, I just want to highlight",
    "start": "157760",
    "end": "166299"
  },
  {
    "start": "158000",
    "end": "460000"
  },
  {
    "text": "the specific terms that we're gonna use because,\nbased on my personal experience, I noticed",
    "start": "166300",
    "end": "172100"
  },
  {
    "text": "that right a lot of terms that used interchangeably. So what I'm usually hearing in the room full\nof enterprise architects when the data topic",
    "start": "172100",
    "end": "181520"
  },
  {
    "text": "pops up is usually starts with let's say we\nneed to protect data, of course.",
    "start": "181520",
    "end": "187319"
  },
  {
    "text": "And then we go, \"We need to protect data. So what techniques we're gonna use? We are gonna use pseudonymization, we're gonna\nuse anonymization, we're gonna use this and",
    "start": "187319",
    "end": "194129"
  },
  {
    "text": "that and that. And I have a feeling that it's all different\ntechniques.",
    "start": "194129",
    "end": "201420"
  },
  {
    "text": "It's all different meanings. The results, how the data looks after all\nthese techniques, that's what's usually really",
    "start": "201420",
    "end": "209549"
  },
  {
    "text": "underestimated because we're forgetting about\nthe data consumers such as data scientists, data engineers, data analysts, and the rest\nof the folks.",
    "start": "209549",
    "end": "215819"
  },
  {
    "text": "So could you probably walk me through a little\nbit, are there any difference between them or just so you can use them interchangeably?",
    "start": "215819",
    "end": "222790"
  },
  {
    "text": "Yes. And I think you're highlighting a key point,\nand I know it's been a pain point that you've",
    "start": "222790",
    "end": "228099"
  },
  {
    "text": "probably experienced in your life, right? Yes. I wanna even zoom back to, like, data protection\nversus data privacy.",
    "start": "228099",
    "end": "236189"
  },
  {
    "text": "Those are actually also different fields,\nright? They're overlapping fields.",
    "start": "236189",
    "end": "241280"
  },
  {
    "text": "But data privacy is also like a social and\ncultural understanding of privacy and maybe",
    "start": "241280",
    "end": "247470"
  },
  {
    "text": "also relates to individual experiences and\nfeelings and, like, how we want to share things",
    "start": "247470",
    "end": "252989"
  },
  {
    "text": "and how we want to change how we share things. And then we have data protection, which is\nreally like a lot of the laws around data",
    "start": "252989",
    "end": "259169"
  },
  {
    "text": "privacy, around data protection. And maybe that also borders on data security... Compliance.\n...which is also different, and InfoSec, which",
    "start": "259169",
    "end": "266860"
  },
  {
    "text": "is also different. And so we have all these, like, neighboring\nthings, and I think if you're just a architect",
    "start": "266860",
    "end": "273219"
  },
  {
    "text": "or a software person, you may just think,\noh, those are all the same words and they mean all the same stuff.",
    "start": "273220",
    "end": "279020"
  },
  {
    "text": "And that's not your fault. Probably lots of people have used them interchangeably\nwith you.",
    "start": "279020",
    "end": "284440"
  },
  {
    "text": "But let's correct some of the problems here. I think you point out pseudonymization versus\nanonymization.",
    "start": "284440",
    "end": "291919"
  },
  {
    "text": "Yes. That's usually what just mixed up constantly. Massively different things, massively different\noutcomes.",
    "start": "291920",
    "end": "298500"
  },
  {
    "text": "I mean, technically pseudonymization is all\nwe're trying to do is we're trying to create",
    "start": "298500",
    "end": "303950"
  },
  {
    "text": "some sort of placeholder or pseudonym for\nmaybe even personal identifiers.",
    "start": "303950",
    "end": "309990"
  },
  {
    "text": "So maybe we shouldn't release the name or\nthe email. So we created a pseudonym. We can use many different methods.",
    "start": "309990",
    "end": "316620"
  },
  {
    "text": "So, within pseudonymization, there's some\nfields like masking, tokenization. Of course, you can always use redaction.",
    "start": "316620",
    "end": "322580"
  },
  {
    "text": "That's slightly different. You can even do format-preserving pseudonymization\nusing methods of format-preserving encryption.",
    "start": "322580",
    "end": "329410"
  },
  {
    "text": "You can do all of these different things. But those is a subfield of pseudonymization.",
    "start": "329410",
    "end": "334880"
  },
  {
    "text": "Then we have anonymization, and I'm here to\nbreak it to everybody, and I'm sorry, anonymization",
    "start": "334880",
    "end": "341669"
  },
  {
    "text": "doesn't exist. Like, if we collect data, if we collect information,\nand we release information, there is mathematically",
    "start": "341669",
    "end": "351610"
  },
  {
    "text": "no guarantee that we cannot have somebody\nlearn something about the individual.",
    "start": "351610",
    "end": "357439"
  },
  {
    "text": "The dictionary definition of anonymization\nis, that you can't learn anything about the",
    "start": "357440",
    "end": "363850"
  },
  {
    "text": "individual, you can't have the potential to\nre-identify the individual, that's actually",
    "start": "363850",
    "end": "368889"
  },
  {
    "text": "been mathematically proven using math and\nlogic and information theory, that it's actually",
    "start": "368889",
    "end": "373990"
  },
  {
    "text": "just impossible. That's okay. Because even though it's not technically possible,\nwe have now defined methods to...let's just",
    "start": "373990",
    "end": "384530"
  },
  {
    "text": "use the term anonymize or to approximate the\nbest way that we can anonymize data using",
    "start": "384530",
    "end": "392050"
  },
  {
    "text": "techniques like differential privacy. I'm happy to talk further about it, but there\nare ways that we can try to think about the",
    "start": "392050",
    "end": "400789"
  },
  {
    "text": "information that we're giving out and think\nabout what the implication of that information is for the privacy of individuals and to rigorously\ndefine and measure that and to tune that for,",
    "start": "400790",
    "end": "412900"
  },
  {
    "text": "as you say, the exact use case, right? Because at the end of the day, we have some\nusers, they might be data analysts, they might",
    "start": "412900",
    "end": "420470"
  },
  {
    "text": "be business users, they might be the users\nthemselves, they might be data scientists",
    "start": "420470",
    "end": "426370"
  },
  {
    "text": "or machine learning folks, and they're the\nones that have to then consume this data and",
    "start": "426370",
    "end": "431919"
  },
  {
    "text": "make decisions with it or potentially lead\nthe company in certain directions.",
    "start": "431919",
    "end": "437650"
  },
  {
    "text": "We need to be very clear, about what their\nneeds too in this. So we balance the user needs for privacy and\nthe data needs for information.",
    "start": "437650",
    "end": "447330"
  },
  {
    "text": "So a short summary, pseudonymization, anonymization,\nit's not the same thing, it's different.",
    "start": "447330",
    "end": "453520"
  },
  {
    "text": "I would say the classical anonymization, like\n100% guarantee, doesn't exist.",
    "start": "453520",
    "end": "458610"
  },
  {
    "text": "Just to get back to that point. I think we are gonna deep dive further into\ndifferential privacy, of course.",
    "start": "458610",
    "end": "466259"
  },
  {
    "start": "460000",
    "end": "848000"
  },
  {
    "text": "But what I usually see in the, let's say,\nboardrooms is that this a little bit binary approach to, let's say, how our data could\nbe secured.",
    "start": "466259",
    "end": "475370"
  },
  {
    "text": "It's like...or it's secured or not. And this is usually the all conversations\nthat we have with InfoSec folks, with compliance",
    "start": "475370",
    "end": "485539"
  },
  {
    "text": "folk and all of that. If it's a huge enterprise, you have, of course,\na data governance board, and then it goes",
    "start": "485539",
    "end": "493080"
  },
  {
    "text": "more and more and more and more. But I think what is fascinating with differential\nprivacy, is that this is the first time when",
    "start": "493080",
    "end": "501590"
  },
  {
    "text": "you can think about, let's say, data protection\nas a specific scale.",
    "start": "501590",
    "end": "507710"
  },
  {
    "text": "So it's not, let's say...or protected or not\nprotected anymore. So could you explain more about that?",
    "start": "507710",
    "end": "514010"
  },
  {
    "text": "Because I think that's the most tremendous\nconcept that is introduced in the Chapter 2 of your book.",
    "start": "514010",
    "end": "519200"
  },
  {
    "text": "I think it is even bigger than that too. Because it's like pseudonymization is also\na method, and that offers some privacy guarantees,",
    "start": "519200",
    "end": "528710"
  },
  {
    "text": "then we go into anonymization differential\nprivacy, and then we can go even further of, like, federated use cases and all this stuff,\nyou know, this stuff from our conversations,",
    "start": "528710",
    "end": "539680"
  },
  {
    "text": "but it's all in the book too. But I think, like, I love this thing there.",
    "start": "539680",
    "end": "544910"
  },
  {
    "text": "It's like privacy isn't on or off. Data protection isn't on or off. It isn't like, \"I switched on the data protection\nmagically.",
    "start": "544910",
    "end": "552839"
  },
  {
    "text": "Everything's protected,\" and it probably never\nwas. So if, unfortunately, we have this kind of\nidea, do we implement the security?",
    "start": "552839",
    "end": "559610"
  },
  {
    "text": "Do we implement the protection? This is a very binary mindset, but that's\nnot at all how any of these technologies work.",
    "start": "559610",
    "end": "567199"
  },
  {
    "text": "And we know from our work in data, that it's\nalso not how data and information work despite default, right?",
    "start": "567200",
    "end": "572940"
  },
  {
    "text": "When you're working in machine learning, as\nyou and I have had a lot of experience in,",
    "start": "572940",
    "end": "577990"
  },
  {
    "text": "there's a variety of truthiness of the data,\nand there's a variety of protection of that",
    "start": "577990",
    "end": "583170"
  },
  {
    "text": "data depending on how you implement your algorithms\nor your architectures. I think the cool thing about differential\nprivacy, it's all tunable.",
    "start": "583170",
    "end": "592540"
  },
  {
    "text": "So it's by default tunable. The base theory of differential privacy is\nthe idea of tuning the amount of information",
    "start": "592540",
    "end": "602530"
  },
  {
    "text": "that somebody can get from the result and\ntuning that and bounding it by a small probability",
    "start": "602530",
    "end": "609269"
  },
  {
    "text": "bounce. So I give you an answer, and let's say the\nanswer is 10, whatever the answer is, how",
    "start": "609269",
    "end": "618279"
  },
  {
    "text": "many purchases did users in this area make\nthis month or something like that?",
    "start": "618279",
    "end": "623830"
  },
  {
    "text": "It's 10. And then I add a person to the data set, and\nmaybe the person falls in that query, and",
    "start": "623830",
    "end": "630970"
  },
  {
    "text": "then you ask me again. And the change in those answers, so 10, let's\nsay that now I say 11, it's like if I just",
    "start": "630970",
    "end": "640470"
  },
  {
    "text": "answered you 11, you could make a pretty good\ninference that somebody got added and that person made a purchase.",
    "start": "640470",
    "end": "647140"
  },
  {
    "text": "The goal of differential privacy is to add\nessentially some uncertainty in whether the",
    "start": "647140",
    "end": "653140"
  },
  {
    "text": "answer was the answer and in doing so allow\nthere to be some of this probabilistic thinking",
    "start": "653140",
    "end": "663769"
  },
  {
    "text": "in how certain am I that a person got added\nor not.",
    "start": "663769",
    "end": "669930"
  },
  {
    "text": "A really good example, I think is salaries. So let's say there was, like, a dashboard\nof all payroll, right?",
    "start": "669930",
    "end": "676700"
  },
  {
    "text": "With some buckets probably. Exactly. And then a new person got hired in your team,\nand you're, of course, quite curious because",
    "start": "676700",
    "end": "683149"
  },
  {
    "text": "all people are curious, and you're like, \"Whoa,\nI wonder how much this person's getting paid? And it's a subverted question, especially\nin the European Union.",
    "start": "683149",
    "end": "691070"
  },
  {
    "text": "Exactly. And maybe the person doesn't wanna share or\nmaybe you don't wanna ask or whatever, but if the dashboard just reports all of the actual\nnumbers, there's a pretty good chance, especially",
    "start": "691070",
    "end": "700870"
  },
  {
    "text": "with some data thinking and probabilistic\nthinking or statistical thinking, that you could reverse engineer the salary.",
    "start": "700870",
    "end": "708399"
  },
  {
    "text": "What differential privacy tries to guarantee\nby using differential privacy mechanisms is it tries to, again, add some level of noise,\nsome level of bounding, so constraining that",
    "start": "708399",
    "end": "719220"
  },
  {
    "text": "outliers are essentially non-existent because\noutliers leak a lot of privacy.",
    "start": "719220",
    "end": "724899"
  },
  {
    "text": "Then adding this probabilistic noise so that\neven the people running the system, can't",
    "start": "724900",
    "end": "731519"
  },
  {
    "text": "determine how much noise was added. This uncertainty and the tuning of the noise\ncan help you tune this amount of privacy guarantees",
    "start": "731519",
    "end": "742380"
  },
  {
    "text": "versus the amount of information. And, of course, in an internal use case, maybe\nyou want less noise and more information,",
    "start": "742380",
    "end": "749190"
  },
  {
    "text": "but then you also have less privacy. And maybe if you're releasing data to a partner,\nto a third party, or the public, you wanna",
    "start": "749190",
    "end": "757639"
  },
  {
    "text": "tune-up that noise. And it's okay that it's not 100% accurate\nfor whatever accurate means.",
    "start": "757639",
    "end": "764410"
  },
  {
    "text": "So could we say that in this way we try to\nprotect data from the, say, reverse engineering?",
    "start": "764410",
    "end": "770389"
  },
  {
    "text": "From privacy violations. Essentially, what we're trying to do is we're\ntrying to de-risk the release of information",
    "start": "770390",
    "end": "778339"
  },
  {
    "text": "for whatever we define as an adequate privacy\nrisk. And this is where the thinking in privacy\noverlaps with security and InfoSec thinking.",
    "start": "778339",
    "end": "788459"
  },
  {
    "text": "Because It's about what is the actual risk,\nwhat's the threat model here when we release this data?",
    "start": "788460",
    "end": "794269"
  },
  {
    "text": "What are we worried about? And how do we then adequately tune the protections\nthat we have or employ or, you know, implement",
    "start": "794269",
    "end": "803529"
  },
  {
    "text": "the protections that we have that's gonna\nadequately mitigate that risk? So we feel safe, and we feel, like, maybe\nour users or our citizens, if you're a country,",
    "start": "803530",
    "end": "812910"
  },
  {
    "text": "are safe and we can release this data. And so it's pretty cool. The library that I used to implement differential\nprivacy in the book is Tumult Analytics.",
    "start": "812910",
    "end": "823050"
  },
  {
    "text": "It's an open-source library. They were the folks who helped the U.S. Census\nrelease differentially private census data",
    "start": "823050",
    "end": "830260"
  },
  {
    "text": "for the first time in 2020. And they just last week... There'll be some delay in the release of the\nstudio, but a few weeks ago now at this point,",
    "start": "830260",
    "end": "840370"
  },
  {
    "text": "they just released all of the Wikipedia data\nwith differential privacy, which is pretty cool, I think.",
    "start": "840370",
    "end": "847199"
  },
  {
    "text": "I just want to...let's say, to halt this moment,\nso we usually see that, especially in any",
    "start": "847199",
    "end": "856259"
  },
  {
    "start": "848000",
    "end": "1285000"
  },
  {
    "text": "data projects, usually privacy is the last\nstep, the compliance is the last step.",
    "start": "856259",
    "end": "864370"
  },
  {
    "text": "And also takes into account the upcoming new\nAI Regulation Act, which indicates specific,",
    "start": "864370",
    "end": "871700"
  },
  {
    "text": "let's say, high-risk industries, and high-risk\nuse cases. So I'm wondering how you can structure the\nprocess in the most mutually beneficial way",
    "start": "871700",
    "end": "886709"
  },
  {
    "text": "for software engineers, for InfoSec, and data\nfolks. So what are you gonna do, let's say, step\nby step?",
    "start": "886709",
    "end": "894360"
  },
  {
    "text": "What is your recommendation on that? We've seen a lot of this happen in governance\nand risk conversations.",
    "start": "894360",
    "end": "900829"
  },
  {
    "text": "Exactly. It's gonna be different for every organization,\nwhich is why it has to be led by those experts.",
    "start": "900829",
    "end": "907620"
  },
  {
    "text": "I think if you name them very appropriately,\nit has to be the software infra-architect side of the house.",
    "start": "907620",
    "end": "914370"
  },
  {
    "text": "It has to be the data side of the house. It has to be the security side of the house. If the company's large enough, as you referenced\nbefore, it has to be compliance, audit, privacy,",
    "start": "914370",
    "end": "925100"
  },
  {
    "text": "and legal. And those people should all be sitting on\nthe data governance board by themselves and",
    "start": "925100",
    "end": "931009"
  },
  {
    "text": "talking with each other regularly. Let's hope. If not, you know, maybe that's the first step.",
    "start": "931009",
    "end": "936339"
  },
  {
    "text": "And that's the first chapter of the book,\nis, like, if you don't have functioning data governance, you can't do any of this cool\nstuff.",
    "start": "936339",
    "end": "941699"
  },
  {
    "text": "So you need functioning data governance first. Because, as you point out, there has to be\na risk appetite set.",
    "start": "941699",
    "end": "948120"
  },
  {
    "text": "So you have to define what is privacy risk\nfor us. What is data protection risk? What is an information security risk?",
    "start": "948120",
    "end": "955259"
  },
  {
    "text": "How do we define that? How do we define it on a data science or a\ndata case-by-case basis?",
    "start": "955259",
    "end": "960759"
  },
  {
    "text": "So, for certain things, do we have a larger\nrisk appetite? Like, are we willing to take more privacy\nrisks to do a new cool machine learning thing",
    "start": "960759",
    "end": "969850"
  },
  {
    "text": "that we wanna test out? But that all has to be defined at the organizational\nlevel.",
    "start": "969850",
    "end": "975829"
  },
  {
    "text": "Once that starts to get defined, you can work\nwith teams to prioritize experiments.",
    "start": "975829",
    "end": "982810"
  },
  {
    "text": "That's very much the ThoughtWorks way, which\nis to identify a priority, identify a thin",
    "start": "982810",
    "end": "988250"
  },
  {
    "text": "slice, and then iterate on it, and as very\nagile methodologies as well.",
    "start": "988250",
    "end": "993700"
  },
  {
    "text": "And I think that that works well for these\nnew technologies because they're so tunable,",
    "start": "993700",
    "end": "1000460"
  },
  {
    "text": "because some of them have different requirements\nin terms of how to deploy and scale them and so forth that, on a case-by-case basis, you\nexperiment with some of these new technologies,",
    "start": "1000460",
    "end": "1012279"
  },
  {
    "text": "you iterate and learn as you always do. And then maybe as you iterate and learn, you\ncan build them into platform services.",
    "start": "1012279",
    "end": "1019900"
  },
  {
    "text": "Maybe you can build them into reusable functionality\nthat your teams can leverage.",
    "start": "1019900",
    "end": "1025370"
  },
  {
    "text": "That requires the software folks in the house\nand the platform folks in the house to think",
    "start": "1025370",
    "end": "1030839"
  },
  {
    "text": "through how do we optimize these systems for\nscaled use and for the types of users that",
    "start": "1030839",
    "end": "1036808"
  },
  {
    "text": "we have. And so it can be this huge iterative process,\nand then you go back to the governance board",
    "start": "1036809",
    "end": "1041839"
  },
  {
    "text": "and say, \"Okay, here's our learnings. Like, what's the next priority?\" And it can iterate that way too.",
    "start": "1041839",
    "end": "1047459"
  },
  {
    "text": "Okay. So let's, for example, zoom in on the use\ncase. So you're in the room. There's a specific use case.",
    "start": "1047459",
    "end": "1053259"
  },
  {
    "text": "We're not talking about mission-critical scenarios\nright now. We're just talking about something that we\nwould prefer to do better.",
    "start": "1053260",
    "end": "1060350"
  },
  {
    "text": "Let's put boundaries. It's for internal usage. So it's still, let's say, private information,\nbut we are not releasing this information",
    "start": "1060350",
    "end": "1068640"
  },
  {
    "text": "somewhere external to the company. So what will be the hands-on approach?",
    "start": "1068640",
    "end": "1074262"
  },
  {
    "text": "You briefly mentioned the library that you\nuse. So how are you gonna go? So you have this use case. You have this, like, software folks in the\nroom, data folks in the room, infra privacy",
    "start": "1074262",
    "end": "1083720"
  },
  {
    "text": "folks in the room. So what are you gonna do? I mean, the minority (b) policies and standards\nthat say, like, for these use cases, this",
    "start": "1083720",
    "end": "1092059"
  },
  {
    "text": "is what we recommend. And depending on the org you're at, they might\nbe super specific and say, like, use this",
    "start": "1092059",
    "end": "1097380"
  },
  {
    "text": "library and with these parameters and so on. So they...you can even define a parameter.",
    "start": "1097380",
    "end": "1102620"
  },
  {
    "text": "Exactly. Wow. Okay. If you wanted to, right? Yes. And that's usually for organizations that\nhave been doing this for a long time.",
    "start": "1102620",
    "end": "1108620"
  },
  {
    "text": "That's why they don't have to have so much\nexperimentation. They've already experimented. They've codified it in their policies or their\nstandards, but there are also a lot of orgs",
    "start": "1108620",
    "end": "1116470"
  },
  {
    "text": "that don't wanna codify it in standards because\nthey know technologies change over time. So you go to the guiding standards and policies.",
    "start": "1116470",
    "end": "1124899"
  },
  {
    "text": "You kind of hopefully by that point in time\nread and understand a lot of them. And then to implement them if you can leverage\na platform that's already there if you can",
    "start": "1124900",
    "end": "1134651"
  },
  {
    "text": "leverage a tool that's already there, always\ndo that. Nobody should be rolling their own over any\nof these things, in my opinion, unless you",
    "start": "1134651",
    "end": "1141429"
  },
  {
    "text": "have, you know, a privacy engineering team\nthat's in charge of this.",
    "start": "1141429",
    "end": "1146679"
  },
  {
    "text": "And in the case that you described, so, like,\ninternal data use, not gonna be released publicly,",
    "start": "1146679",
    "end": "1152730"
  },
  {
    "text": "you might even just decide pseudonymization\nis enough, or you might decide, like, aggregation",
    "start": "1152730",
    "end": "1157929"
  },
  {
    "text": "plus pseudonymization for these particular\ncategories is enough. So there can be, I think, particularly for\ninternal use cases, a much larger risk appetite",
    "start": "1157930",
    "end": "1168860"
  },
  {
    "text": "because, presumably, with your internal users,\nthere's a high level of trust.",
    "start": "1168860",
    "end": "1174260"
  },
  {
    "text": "If you're in a huge org, and this is one of\nthe things that they did at Google, is, for",
    "start": "1174260",
    "end": "1179790"
  },
  {
    "text": "data scientists, you have to first prove that\nyour experiment idea works with differentially",
    "start": "1179790",
    "end": "1185770"
  },
  {
    "text": "private data before you can get access to... To the real...wow. ...the real data.",
    "start": "1185770",
    "end": "1191559"
  },
  {
    "text": "And it's for the sensitive, you know, PII\nand other types of sensitive data. And I think that's a cool idea that can be\nused in all sorts of industries where it's",
    "start": "1191559",
    "end": "1200809"
  },
  {
    "text": "like, look, maybe you're just in the EDA step,\nthe exploratory data analysis step.",
    "start": "1200809",
    "end": "1206700"
  },
  {
    "text": "Maybe you don't need the raw data. Maybe you just need some artifacts of the\nraw data to test out the idea.",
    "start": "1206700",
    "end": "1213170"
  },
  {
    "text": "And then if you wanna go further, maybe there's\na process to apply to get access to the raw data and maybe there's a timeframe of experimentation.",
    "start": "1213170",
    "end": "1220860"
  },
  {
    "text": "So maybe instead of accidentally, I think\na lot of times accidentally have access for like two years, and then you accidentally\npull production or something like this.",
    "start": "1220860",
    "end": "1231059"
  },
  {
    "text": "You don't wanna do that either. It helps the users and it helps the organization\nto have a little bit more structure around",
    "start": "1231059",
    "end": "1240330"
  },
  {
    "text": "how some of this more exploratory data works\nand then to define, okay, once the experiments",
    "start": "1240330",
    "end": "1247070"
  },
  {
    "text": "are run, and we know the approach, and maybe\nwe wanna move into staging or even production, then we essentially decide to give less access\nto the individual data user and more of a",
    "start": "1247070",
    "end": "1258389"
  },
  {
    "text": "systems level, ML Ops, I know near and dear\nto your heart, ML Ops layer of access to the",
    "start": "1258390",
    "end": "1265610"
  },
  {
    "text": "data, which is great, right? Because then the chance of somebody accidentally\npulling production or accidentally exposing",
    "start": "1265610",
    "end": "1272180"
  },
  {
    "text": "users is much less. I think that's the most difficult part because,\nusually, it's like, we can do this because",
    "start": "1272180",
    "end": "1280840"
  },
  {
    "text": "we don't have a sense of production data. We couldn't create synthetic data based on\nthat. So I'm just wondering how in this case, let's\nsay, the specificity of the usage of the specific",
    "start": "1280840",
    "end": "1297830"
  },
  {
    "start": "1285000",
    "end": "1942000"
  },
  {
    "text": "libraries and the tuning of the specific parameters\nthat just mentioned. Probably, we can zoom in a little bit more.",
    "start": "1297830",
    "end": "1304070"
  },
  {
    "text": "So I just want a case. Imagine we have specific, I don't know, like,\nthe database from this database, like, we operational database on a specific moment.",
    "start": "1304070",
    "end": "1310740"
  },
  {
    "text": "We just scoop a specific amount of data. We have, let's say, a data processing pipeline,\nwhich has, as we guess, some embedded data",
    "start": "1310740",
    "end": "1320350"
  },
  {
    "text": "verification checks and also applies differential\nprivacy.",
    "start": "1320350",
    "end": "1326320"
  },
  {
    "text": "And then we have, let's say, the result. So what type of these perimeters you can tweak?",
    "start": "1326320",
    "end": "1332380"
  },
  {
    "text": "That's what I'm really curious. So if you look at the differential privacy,\nwhat are the main concepts of it?",
    "start": "1332380",
    "end": "1338929"
  },
  {
    "text": "And this concept, let's say, then turn into\nparameters and what could be literally... Yeah, explain, let's say in simple words,\nbecause if you go into all the privacy budget",
    "start": "1338929",
    "end": "1349429"
  },
  {
    "text": "with EPSO and how it's for, it could be too\nmuch, but I would say, if I need to explain",
    "start": "1349429",
    "end": "1355028"
  },
  {
    "text": "it to my software colleagues, what I'm gonna\ndo? Yes. So perfect example, right?",
    "start": "1355029",
    "end": "1360380"
  },
  {
    "text": "And one of the problems and also beautiful\nthings about differential privacy is now we",
    "start": "1360380",
    "end": "1365760"
  },
  {
    "text": "have hundreds of definitions. Those definitions have different parameters,\nand there are some that you can use as kind",
    "start": "1365760",
    "end": "1373890"
  },
  {
    "text": "of, like, hyper-parameters to optimize different\ndefinitions. It can get quite complicated.",
    "start": "1373890",
    "end": "1379700"
  },
  {
    "text": "Damien Desfontaines, his work on it and his\narticle series on it, he helped me with the",
    "start": "1379700",
    "end": "1385230"
  },
  {
    "text": "differential privacy chapters as a technical\nadvisor and helped implement the Google differential",
    "start": "1385230",
    "end": "1391140"
  },
  {
    "text": "privacy stuff and now is at Tumult Analytics\nplace. And he and a co-researcher, whose name now\nI unfortunately forget, did a survey paper,",
    "start": "1391140",
    "end": "1401770"
  },
  {
    "text": "and they parameterized all of the parameters. So there were like 100 different parameters.",
    "start": "1401770",
    "end": "1407789"
  },
  {
    "text": "I'm trying just to mention the dimensionality\nof this data Exactly. Yes, yes, yes.",
    "start": "1407789",
    "end": "1413049"
  },
  {
    "text": "Absolutely. So, let's go to a simpler definition.",
    "start": "1413049",
    "end": "1418639"
  },
  {
    "text": "And then depending on the privacy expertise\nor if you start hiring privacy engineers at",
    "start": "1418640",
    "end": "1424140"
  },
  {
    "text": "your org, then they can go dive into the deep\nend. But from a simple standpoint, we're going\nback to these queries that you're running.",
    "start": "1424140",
    "end": "1432130"
  },
  {
    "text": "What you wanna understand is what's the probability\nof learning something before in the first",
    "start": "1432130",
    "end": "1437960"
  },
  {
    "text": "query and then what's the probability of learning\nsomething in the second query, and you want",
    "start": "1437960",
    "end": "1443041"
  },
  {
    "text": "those probabilities to be very close to each\nother because the closer they are together,",
    "start": "1443041",
    "end": "1448370"
  },
  {
    "text": "the less information that you've essentially\ngiven away. And the less information you've given away,\nthe less of a chance of privacy loss.",
    "start": "1448370",
    "end": "1456528"
  },
  {
    "text": "So privacy loss of the individual is information\ngain of the person querying.",
    "start": "1456529",
    "end": "1462760"
  },
  {
    "text": "And these are modeled in a threat modeling\nsense. The person querying is trying to attack.",
    "start": "1462760",
    "end": "1468490"
  },
  {
    "text": "And therefore they can be relaxed for different\ntypes of use cases. If you're releasing data publicly, you should\nprobably think about a very high...",
    "start": "1468490",
    "end": "1477440"
  },
  {
    "text": "high level of security and a low level of\ntrust with the people using the data. But if it's internally, we can think much,\nwe can broaden that.",
    "start": "1477440",
    "end": "1486090"
  },
  {
    "text": "And one of those parameters that you can tune\nis the closeness of those responses.",
    "start": "1486090",
    "end": "1491169"
  },
  {
    "text": "Do they need to be tightly coupled? So we have very high security but also probably\na lot of noise, right?",
    "start": "1491169",
    "end": "1498789"
  },
  {
    "text": "So the noise distribution that we're playing\nfrom is much larger in a sense, and therefore",
    "start": "1498789",
    "end": "1504090"
  },
  {
    "text": "we have much higher uncertainty of if the\nanswer that we got the first time or the answer",
    "start": "1504090",
    "end": "1509240"
  },
  {
    "text": "that we got the second time, if there's any\ninformation there.",
    "start": "1509240",
    "end": "1514840"
  },
  {
    "text": "But at some point in time, it also gets too\nnoisy, right? And then we need to determine how to do that.",
    "start": "1514840",
    "end": "1522730"
  },
  {
    "text": "The higher accuracy that you want means less\nof a chance of privacy for the individuals.",
    "start": "1522730",
    "end": "1528950"
  },
  {
    "text": "So if you absolutely 100% need the right answer\nand it's mission-critical that you get the",
    "start": "1528950",
    "end": "1534440"
  },
  {
    "text": "right answer, then probably differential privacy\nis not for your use case. But if an approximation of the right answer,\nand this is like also a theoretical debate",
    "start": "1534440",
    "end": "1544269"
  },
  {
    "text": "in data, is what is ground truth, and do we\never have accurate data and all of these things?",
    "start": "1544270",
    "end": "1552810"
  },
  {
    "text": "I think there is something to be said about\nhow much we understand the data that we're",
    "start": "1552810",
    "end": "1560360"
  },
  {
    "text": "collecting, and is there not already errors\nin the data we're collecting, and does the",
    "start": "1560360",
    "end": "1566090"
  },
  {
    "text": "insertion of differential privacy related\nerror, so errors so we can ensure privacy",
    "start": "1566090",
    "end": "1571260"
  },
  {
    "text": "for the individuals, which is essentially\nwhat we do as part of this process, is we insert error and we can decide what type of\nerror so we can choose the distributions that",
    "start": "1571260",
    "end": "1580010"
  },
  {
    "text": "we insert so we can use Gaussian... Distribution.\n...which we would normally expect anyways",
    "start": "1580010",
    "end": "1585750"
  },
  {
    "text": "in data we collect. So as data scientists, we expect, normally\ndistributed error in most of the data we collect",
    "start": "1585750",
    "end": "1591380"
  },
  {
    "text": "anyway. And so inserting some more normally distributed\nerrors is probably not the end of the world",
    "start": "1591380",
    "end": "1597220"
  },
  {
    "text": "as long as they're doing robust data science. But these are all things to think about.",
    "start": "1597220",
    "end": "1604410"
  },
  {
    "text": "From a software perspective, you need to think\nabout this trade-off between the accuracy of the response or the information, the response\nand the privacy you can guarantee, and the",
    "start": "1604410",
    "end": "1614450"
  },
  {
    "text": "mechanism in and of itself, and what you can\ntune is this tension between those two points. So either you're getting more privacy and\nalso more error and noise and also for the",
    "start": "1614450",
    "end": "1626661"
  },
  {
    "text": "attacker less certainty that they know for\nsure what the data is or who's in it, or you're",
    "start": "1626661",
    "end": "1632610"
  },
  {
    "text": "getting, higher accuracy closer to the actual\nnumbers, but the chance that somebody learns",
    "start": "1632610",
    "end": "1639380"
  },
  {
    "text": "that somebody got added to the data or that\nsomebody got removed from the data or certain things about the people in the data is much\nhigher.",
    "start": "1639380",
    "end": "1646770"
  },
  {
    "text": "Therefore you're dealing with much riskier\ndata science or data release.",
    "start": "1646770",
    "end": "1653399"
  },
  {
    "text": "Let's say, in my head, you know, it's like\nthe slider which is going like how much privacy",
    "start": "1653399",
    "end": "1659020"
  },
  {
    "text": "laws we could afford to get meaningful results\nat the end versus how more we prefer to lose",
    "start": "1659020",
    "end": "1667429"
  },
  {
    "text": "to get, let's say, better results. So that's, let's say, the space where we operate.",
    "start": "1667429",
    "end": "1673018"
  },
  {
    "text": "Absolutely. I think it's usually,  everything is a trade-off\nin the majority of the cases.",
    "start": "1673019",
    "end": "1679159"
  },
  {
    "text": "This is exactly the trade-off on which sensitivity\nof data we need. I just want to get a little bit back.",
    "start": "1679159",
    "end": "1684309"
  },
  {
    "text": "I think what triggered me when you mentioned\nthat this Google approach was really interesting.",
    "start": "1684309",
    "end": "1689669"
  },
  {
    "text": "So we getting back to the story of, let's\nsay, again, data in different environments. Usually, of course, you're not allowed, like,\nhigh-risk sensitive PIA data to be available",
    "start": "1689669",
    "end": "1701360"
  },
  {
    "text": "in-depth or staging. You prefer to keep this data in production. But what I saw with my own eyes, like two\neyes, but usually, like, it's there's only",
    "start": "1701360",
    "end": "1711510"
  },
  {
    "text": "one environment. It's only one environment. And let's say that there is no understanding\nof where the data folks could be put in to",
    "start": "1711510",
    "end": "1719590"
  },
  {
    "text": "do their experiments. They're not allowed to do their experiments\nin production. Because production, there is no...by the way,\nthere is no infra to do this, of course, for",
    "start": "1719590",
    "end": "1727039"
  },
  {
    "text": "all the hungry computations. And then, usually, it goes like this, if I\ncannot access the data, then I find someone",
    "start": "1727040",
    "end": "1733889"
  },
  {
    "text": "who could help me access the data, right? So we go this way, oh, probably you can send\nme these 3 gigabytes, as an attachment to",
    "start": "1733890",
    "end": "1739980"
  },
  {
    "text": "the email, and imagine if it's PIA data. Good luck with this. So, again, like, in this, like, chaotic situation,\nwhat will be your piece of advice?",
    "start": "1739980",
    "end": "1751290"
  },
  {
    "text": "It doesn't matter, like, right now the scale\nof the company, but imagine they don't have anything, and they just want to start.",
    "start": "1751290",
    "end": "1759490"
  },
  {
    "text": "How are you gonna approach this? Absolutely. This goes back to the very basics of data\ngovernance and the fact that, like, if you",
    "start": "1759490",
    "end": "1769740"
  },
  {
    "text": "have a system that cannot support an entire\ndepartment or team of people except for going",
    "start": "1769740",
    "end": "1776809"
  },
  {
    "text": "around the data governance rules, you haven't\ndone it right. And we see this a lot, even at scaled companies,\nwhere data governance comes in...",
    "start": "1776809",
    "end": "1787150"
  },
  {
    "text": "Experis.\n...and with very strong hard rules of, like, no access for data teams, no this, no that.",
    "start": "1787150",
    "end": "1795018"
  },
  {
    "text": "Although that might be a great idea for information\nsecurity, people are clever.",
    "start": "1795019",
    "end": "1800779"
  },
  {
    "text": "People are social, and social and clever people\nfind and socially engineer new ways to access",
    "start": "1800779",
    "end": "1807179"
  },
  {
    "text": "data. So just saying no all the time is not gonna\nstop data from being used.",
    "start": "1807179",
    "end": "1814000"
  },
  {
    "text": "Instead, implementing privacy engineering\nat an organization. Even if right now you call it infra engineering,\nyou put it in the architecture, you put it",
    "start": "1814000",
    "end": "1822610"
  },
  {
    "text": "in software, wherever you put it, the concepts\nof privacy engineering, which is making these",
    "start": "1822610",
    "end": "1828360"
  },
  {
    "text": "technologies easier to use and more available\nfor the entire organization, that stuff has",
    "start": "1828360",
    "end": "1833769"
  },
  {
    "text": "to be built in to avoid these shadow IT systems\nand these workarounds that people find because",
    "start": "1833769",
    "end": "1840400"
  },
  {
    "text": "people need to do their jobs, right? So just saying no and blocking access to production\ndata or something is not gonna stop data scientists",
    "start": "1840400",
    "end": "1848860"
  },
  {
    "text": "from figuring out a way to do that or leaving\nthe org, even worse, getting so frustrated",
    "start": "1848860",
    "end": "1855120"
  },
  {
    "text": "that they can't get access to data that they\nleave the org, and then you don't have any data scientists anymore, or you don't have\ndata scientists that want to do daily data",
    "start": "1855120",
    "end": "1863919"
  },
  {
    "text": "science, right, which is also problematic. So I think you have to go back to an organizational\ndecision.",
    "start": "1863919",
    "end": "1871380"
  },
  {
    "text": "We're gonna invest in ways, and if you're\na small org, you can make these small ways,",
    "start": "1871380",
    "end": "1877809"
  },
  {
    "text": "right? This can be a simple interface that allows\nfor pseudonymization, that allows for tokenization,",
    "start": "1877809",
    "end": "1884549"
  },
  {
    "text": "or something like this, or an interface that\nallows for an aggregate dump from production",
    "start": "1884549",
    "end": "1889870"
  },
  {
    "text": "to go through a differential privacy pipeline\nor something like this. It doesn't have to... You don't have to do everything at once, but\nyou have to give people the tools they need",
    "start": "1889870",
    "end": "1898230"
  },
  {
    "text": "to do their job and the tools they need to\ndo their job safely and with privacy-respecting",
    "start": "1898230",
    "end": "1903980"
  },
  {
    "text": "tools. Alyona Galyeva:  I do believe as well that\nfrom day one, it should be built in.",
    "start": "1903980",
    "end": "1912090"
  },
  {
    "text": "If it's there, then it's easier to expand\nit later. But there is nothing here if there is even\nno categorization of the data, regarding the",
    "start": "1912090",
    "end": "1921780"
  },
  {
    "text": "sensitivity of specific data and then a basic\nway how the users can access the data without",
    "start": "1921780",
    "end": "1929639"
  },
  {
    "text": "specific, like, role. If you have just give everyone route access,\nso good luck with this later for the use cases. ",
    "start": "1929639",
    "end": "1938330"
  },
  {
    "text": "That's, I think, the hard thing. I also wonder if I look back, for example,\npseudonymization, usually, the vector of attack",
    "start": "1938330",
    "end": "1948559"
  },
  {
    "start": "1942000",
    "end": "2264000"
  },
  {
    "text": "is usually, say, this, like, the linkage between\npseudonymized data and, let's say, the regional",
    "start": "1948559",
    "end": "1953820"
  },
  {
    "text": "data. Or it's like some dictionary or any other\nthing. So that's usually what attackers try to get\naccess to.",
    "start": "1953820",
    "end": "1962460"
  },
  {
    "text": "What will be the vector of attack in case\nof differential privacy?",
    "start": "1962460",
    "end": "1969309"
  },
  {
    "text": "Differential privacy, you tend to release\naggregates, things like histograms, things like count sketches, and other things like\nthis.",
    "start": "1969309",
    "end": "1977259"
  },
  {
    "text": "But you could also release a result, right? So you can release an average or something\nlike that.",
    "start": "1977260",
    "end": "1983429"
  },
  {
    "text": "Interestingly enough, some of the core differential\nprivacy attacks that have been proven are",
    "start": "1983429",
    "end": "1989429"
  },
  {
    "text": "implementation errors. So Damien, the same researcher who's done\na bunch of work on differential privacy, will",
    "start": "1989429",
    "end": "1997931"
  },
  {
    "text": "be presenting at BSides Zurich this year some\nwork that the Tumult team did on leveraging",
    "start": "1997931",
    "end": "2004149"
  },
  {
    "text": "floating point attacks against differential\nprivacy systems. And we can think about this like when we're\nfloating point and we're already...like, then",
    "start": "2004149",
    "end": "2014610"
  },
  {
    "text": "that's already an abstraction on how computers\nstore data, right? There's no real floating point data type at\nthe core processing level.",
    "start": "2014610",
    "end": "2025250"
  },
  {
    "text": "We're dealing with some sort of abstraction. What has been proven before and what they\nfurther proved is, depending on the sensitivity",
    "start": "2025250",
    "end": "2034010"
  },
  {
    "text": "of how you're sampling this noise, the computer\ncan sometimes not be truly pseudorandom.",
    "start": "2034010",
    "end": "2040150"
  },
  {
    "text": "These are some of the same problems we have\nin cryptography and so forth as well as, like,",
    "start": "2040150",
    "end": "2045210"
  },
  {
    "text": "with computers, we don't often have a true\nsource of randomness at least from a mathematical",
    "start": "2045210",
    "end": "2052190"
  },
  {
    "text": "perspective. Let's say that you're an attacker of a differential\nprivacy system.",
    "start": "2052190",
    "end": "2057820"
  },
  {
    "text": "If it's quite obvious that you're running\na query that has a particular distribution",
    "start": "2057820",
    "end": "2064490"
  },
  {
    "text": "and the sampling of that distribution is predictable,\nyou can start to reverse engineer.",
    "start": "2064490",
    "end": "2069628"
  },
  {
    "text": "We can think of this, like, as from Bayesian\nor probabilistic thinking. You can start to reverse engineer these processes,\nand you could potentially even start to guess",
    "start": "2069629",
    "end": "2078480"
  },
  {
    "text": "how much noise was added. If you can guess how much noise was added\nwith any certainty, you immediately remove",
    "start": "2078480",
    "end": "2086490"
  },
  {
    "text": "all of the privacy guarantees because the\nprivacy guarantees come completely from the",
    "start": "2086490",
    "end": "2091510"
  },
  {
    "text": "fact that nobody can probabilistically infer\nhow much noise was added.",
    "start": "2091510",
    "end": "2098710"
  },
  {
    "text": "And so the whole car is a small park if you\ncould do that. Interestingly enough, it's less from, like,\ndata analysis, it is more from, like, computers",
    "start": "2098710",
    "end": "2107360"
  },
  {
    "text": "and systems analysis that have been most of\nthe attack vectors against differential privacy.",
    "start": "2107360",
    "end": "2113079"
  },
  {
    "text": "Now, I guess I get it, why there is no such\na thing, as this, like, 100% guarantee of",
    "start": "2113079",
    "end": "2121250"
  },
  {
    "text": "anonymization. With all being said, like, we try to walk\nthrough, let's say, different chapters a little",
    "start": "2121250",
    "end": "2128079"
  },
  {
    "text": "bit, and touch different things. So I want to summarize all of this.",
    "start": "2128079",
    "end": "2133390"
  },
  {
    "text": "So could you briefly walk me through, let's\nsay, the more or less content of the chapters,",
    "start": "2133390",
    "end": "2139609"
  },
  {
    "text": "how many chapters the book has with the content,\nand this is, like, a hands-on privacy implementation",
    "start": "2139609",
    "end": "2145380"
  },
  {
    "text": "book? So just, like, highlight this, and I think\nthat will conclude our session for today.",
    "start": "2145380",
    "end": "2151490"
  },
  {
    "text": "As you're familiar with already and as, hopefully,\nviewers will be as they read the book, the",
    "start": "2151490",
    "end": "2159160"
  },
  {
    "text": "chapters are structured. So there's a theory first, and then there's\nhands-on stuff in the later half of the chapter.",
    "start": "2159160",
    "end": "2165190"
  },
  {
    "text": "This is because I'm a practitioner like yourself. I think it's really important to not just\nteach people how something works, but how",
    "start": "2165190",
    "end": "2173150"
  },
  {
    "text": "they can use it, you know? Exactly. Okay. That sounds great. But, like, what am I gonna type when I sit\ndown?",
    "start": "2173150",
    "end": "2179290"
  },
  {
    "text": "Like, how am I supposed to do this? And so each chapter has open-source libraries.",
    "start": "2179290",
    "end": "2185070"
  },
  {
    "text": "So we start with data governance, and we talk\na little bit about pseudonymization and so forth, and then move on to differential privacy.",
    "start": "2185070",
    "end": "2191300"
  },
  {
    "text": "You get to play around with the Tumult Analytics\nlibrary. Cool. There's also some implementing differential\nprivacy by scratch, which, by the way, you",
    "start": "2191300",
    "end": "2199200"
  },
  {
    "text": "should never do unless you're an expert. I remember this. Don't do that. I teach it to folks so they can play with\nit, right?",
    "start": "2199200",
    "end": "2205900"
  },
  {
    "text": "Because it's important to, like, play with\nthese mechanisms and kind of reverse engineer how they work so you can reason about them.",
    "start": "2205900",
    "end": "2212819"
  },
  {
    "text": "Then it moves into data pipelines and data\nengineering. So how do you put privacy technologies into\ndata pipeline work and data engineering work?",
    "start": "2212819",
    "end": "2221740"
  },
  {
    "text": "Then it moves on to attacking privacy systems. So how can we, like, we're...because we need\nto know how to attack stuff, and we wanna",
    "start": "2221740",
    "end": "2228470"
  },
  {
    "text": "know how to protect it. How to protect, yeah. And then we move into federated learning,\nfederated and distributed data analysis...",
    "start": "2228470",
    "end": "2236109"
  },
  {
    "text": "With Flower.\n...which we didn't get to talk to today. Then you use Flower, which I love Flower to\ndo this.",
    "start": "2236109",
    "end": "2242619"
  },
  {
    "text": "And then we go into the encrypted computation. And this is just computing only on encrypted\ndata without decrypting it.",
    "start": "2242619",
    "end": "2249740"
  },
  {
    "text": "Encrypted machine learning, encrypted data\nprocessing, and how does one do that? And then I sort of go into the human side.",
    "start": "2249740",
    "end": "2256760"
  },
  {
    "text": "So how do you read policies? How do you work with lawyers? All of these things and then some FAQs and\nsome use cases.",
    "start": "2256760",
    "end": "2263470"
  },
  {
    "text": "So it's a pretty fun wild ride. I hope. I do hope\nOpen for feedback if there are open questions.",
    "start": "2263470",
    "end": "2272300"
  },
  {
    "start": "2264000",
    "end": "2325000"
  },
  {
    "text": "It's been really exciting to chat with you,\nand thank you so much for being my host.",
    "start": "2272300",
    "end": "2278550"
  },
  {
    "text": "You're the most. Thanks a lot. And I guess the folks can get your book because\nit's right now available, so you can find",
    "start": "2278550",
    "end": "2287180"
  },
  {
    "text": "it in digital copy or hard copy, whatever\nyou prefer. Thanks a lot, Katharine.",
    "start": "2287180",
    "end": "2293290"
  },
  {
    "text": "Thanks. Thank you so much.",
    "start": "2293290",
    "end": "2322690"
  }
]