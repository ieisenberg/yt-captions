[
  {
    "start": "0",
    "end": "70000"
  },
  {
    "text": "[Music]",
    "start": "3500",
    "end": "7820"
  },
  {
    "text": "all right good morning everyone I'm super excited to be part of go toon Chicago this year as well so uh I have a",
    "start": "12160",
    "end": "20359"
  },
  {
    "text": "very exciting topic Kafka meets Iceberg so before I get started how many how",
    "start": "20359",
    "end": "25800"
  },
  {
    "text": "many of you are familiar with Kafka oh it's like 100% right how many",
    "start": "25800",
    "end": "31880"
  },
  {
    "text": "of you are familiar with Iceberg or at least heard of Iceberg a few right so yeah so I'll be telling into the details",
    "start": "31880",
    "end": "38559"
  },
  {
    "text": "of Iceberg fundamentals in the session so uh the main objective of this session",
    "start": "38559",
    "end": "43879"
  },
  {
    "text": "is to uh have a look at the data streaming space and uh the role that",
    "start": "43879",
    "end": "49600"
  },
  {
    "text": "Kafka plays in the uh data streaming and more importantly uh we look at data data leg",
    "start": "49600",
    "end": "56280"
  },
  {
    "text": "space and why iceberg is becoming the de facto standard there and how we can connect your streaming data in Kafka",
    "start": "56280",
    "end": "64080"
  },
  {
    "text": "with data LS and use Iceberg as the storage format all right so to start with I use",
    "start": "64080",
    "end": "72880"
  },
  {
    "start": "70000",
    "end": "126000"
  },
  {
    "text": "this analogy where you have uh uh a stream uh water is pouring into a lake",
    "start": "72880",
    "end": "80159"
  },
  {
    "text": "and it is continuously pouring and uh from that that water turns into iceberg",
    "start": "80159",
    "end": "85479"
  },
  {
    "text": "in the lake so this is exactly what uh what we're going to cover as part of",
    "start": "85479",
    "end": "90560"
  },
  {
    "text": "this session so you have uh all your operational data in Kafka right that is",
    "start": "90560",
    "end": "96759"
  },
  {
    "text": "all the realtime data that you ingest and you store them organize them in inside Kafka so that is your operational",
    "start": "96759",
    "end": "103040"
  },
  {
    "text": "data and when you look at data legs you need to organize them in iceberg and",
    "start": "103040",
    "end": "108520"
  },
  {
    "text": "we'll be looking at why iceberg is becoming the standard and some of the fundamentals of uh Iceberg data",
    "start": "108520",
    "end": "114840"
  },
  {
    "text": "format and finally more importantly how you can feed streaming",
    "start": "114840",
    "end": "119920"
  },
  {
    "text": "data in Kafka to Iceberg in data link all right so to start with uh some",
    "start": "119920",
    "end": "129640"
  },
  {
    "start": "126000",
    "end": "221000"
  },
  {
    "text": "context on the operational data right if you look at any organization you can",
    "start": "129640",
    "end": "134879"
  },
  {
    "text": "find data to be reside in two states operational State and the",
    "start": "134879",
    "end": "140040"
  },
  {
    "text": "analytical State operational state is where you run your business right so",
    "start": "140040",
    "end": "145440"
  },
  {
    "text": "when you are running uh software application you have all these different systems my Services Cloud native",
    "start": "145440",
    "end": "151160"
  },
  {
    "text": "applications and databases so you collect real time data from all these operational",
    "start": "151160",
    "end": "157040"
  },
  {
    "text": "systems and so for example this can be transaction records click stream data",
    "start": "157040",
    "end": "163000"
  },
  {
    "text": "inventory records system logs and so on so all the operational data needs to be",
    "start": "163000",
    "end": "168480"
  },
  {
    "text": "captured and stored so what is the choice what is the technology choice that you can use for",
    "start": "168480",
    "end": "175280"
  },
  {
    "text": "this purpose so that is Kafka right so Kafka is pretty much the open standard",
    "start": "175280",
    "end": "181400"
  },
  {
    "text": "for storing capturing storing and organizing operational data so as an",
    "start": "181400",
    "end": "188400"
  },
  {
    "text": "example let's take uh a click stream use case right so you need to capture a",
    "start": "188400",
    "end": "194040"
  },
  {
    "text": "clickstream from your website and store that in your storage so how you can do",
    "start": "194040",
    "end": "199319"
  },
  {
    "text": "that so you create you use Kafka as the event inje broker create a topic that",
    "start": "199319",
    "end": "205519"
  },
  {
    "text": "captures all the clickstream data and organize it to topics and uh parts partitions and enable Downstream",
    "start": "205519",
    "end": "212400"
  },
  {
    "text": "consumers to consume uh streaming data all right so that is about the",
    "start": "212400",
    "end": "218560"
  },
  {
    "text": "operational side now let's look at the analytical side of the organization so",
    "start": "218560",
    "end": "223799"
  },
  {
    "start": "221000",
    "end": "282000"
  },
  {
    "text": "if you look at the analytical side this is where you perform uh after the fact business analysis right so you have uh",
    "start": "223799",
    "end": "231360"
  },
  {
    "text": "you may be using some part of your operational data and data from various other sources but the objective is to",
    "start": "231360",
    "end": "238560"
  },
  {
    "text": "derive businesses insights out of the data uh that you have so for the",
    "start": "238560",
    "end": "243879"
  },
  {
    "text": "analytical purposes you'll be using either data lake or a data warehouse and",
    "start": "243879",
    "end": "249560"
  },
  {
    "text": "uh you'll be storing data uh using some uh Open Table format so the main idea",
    "start": "249560",
    "end": "258120"
  },
  {
    "text": "here is to make sure that data in your data L can be easily consumed so that",
    "start": "258120",
    "end": "263440"
  },
  {
    "text": "you can generate all these business an uh business insights now Apache iceberg is becoming",
    "start": "263440",
    "end": "269880"
  },
  {
    "text": "the defacto standard when it comes to storing data in your data and building",
    "start": "269880",
    "end": "275520"
  },
  {
    "text": "an uh Open Table format on top of your data in your data link",
    "start": "275520",
    "end": "282000"
  },
  {
    "start": "282000",
    "end": "327000"
  },
  {
    "text": "now let's delve into some of the key features and benefits of Apache iberg so",
    "start": "282000",
    "end": "289560"
  },
  {
    "text": "as I mentioned earlier it's an Open Table format for analytics data set so in fact uh apach iberg tries to overcome",
    "start": "289560",
    "end": "297880"
  },
  {
    "text": "some of the key limitations and challenges that you had in the data l space right so it brings the Simplicity",
    "start": "297880",
    "end": "305400"
  },
  {
    "text": "of SQL table and reliability and the consistency into the data l space so you",
    "start": "305400",
    "end": "312039"
  },
  {
    "text": "used to have this uh capabilities in databases and uh conventional data warehouses but when you are using data",
    "start": "312039",
    "end": "318520"
  },
  {
    "text": "leg you need to have all these capabilities that were previously missing in the data leg now let's take a",
    "start": "318520",
    "end": "324880"
  },
  {
    "text": "look at some of these benefits so Iceberg brings expressive",
    "start": "324880",
    "end": "330400"
  },
  {
    "start": "327000",
    "end": "744000"
  },
  {
    "text": "SQL supports uh to the data LS that means you can write this kind of complex",
    "start": "330400",
    "end": "336280"
  },
  {
    "text": "SQL queries on top of your data leg uh so that you can handle complex data",
    "start": "336280",
    "end": "341680"
  },
  {
    "text": "types and complex operations such as uh aggregation filtering uh grouping and",
    "start": "341680",
    "end": "347840"
  },
  {
    "text": "all that so this was not previously possible with the with the conventional",
    "start": "347840",
    "end": "353319"
  },
  {
    "text": "data LS and this creates a new SQL like layer on top of your data on your data",
    "start": "353319",
    "end": "360520"
  },
  {
    "text": "Lake but this is again applicable for very large uh data",
    "start": "360520",
    "end": "365599"
  },
  {
    "text": "sets all right on top of that you have asset transaction support so all the",
    "start": "365599",
    "end": "372199"
  },
  {
    "text": "complex uh SQL operations can be executed can be uh executed on on top of",
    "start": "372199",
    "end": "378759"
  },
  {
    "text": "your data in your data lake with all the asset guarantees so this ensures the",
    "start": "378759",
    "end": "384680"
  },
  {
    "text": "data Integrity as well as the reliability of your data in your data Lake while while performing all the uh",
    "start": "384680",
    "end": "391720"
  },
  {
    "text": "SQL operations to generate all these business insights now one of the main challenges",
    "start": "391720",
    "end": "399360"
  },
  {
    "text": "that we had in the conventional data leges and warehouses is the schema Evolution right so uh you you need to",
    "start": "399360",
    "end": "406919"
  },
  {
    "text": "evolve the data that you have in the data Lake consistently but when you add a column or remove a column usually the",
    "start": "406919",
    "end": "414360"
  },
  {
    "text": "data existing data becomes completely stale or you may have to completely redite the data set from scratch by",
    "start": "414360",
    "end": "422080"
  },
  {
    "text": "incorporating new schema into it like for example if you added a column then you need to rewrite the existing data",
    "start": "422080",
    "end": "429199"
  },
  {
    "text": "set by uh changing the schema now with Iceberg you can perform pretty much all",
    "start": "429199",
    "end": "435599"
  },
  {
    "text": "the schema Evolution features uh it includes uh adding removing renaming",
    "start": "435599",
    "end": "441199"
  },
  {
    "text": "column as well as you can expand certain types as well like uh safe type",
    "start": "441199",
    "end": "447720"
  },
  {
    "text": "expansion from for example from into uh double so likewise you can uh do scheme",
    "start": "447720",
    "end": "454919"
  },
  {
    "text": "Evolution now this is a sample uh query that we can run against against an",
    "start": "454919",
    "end": "461240"
  },
  {
    "text": "iceberg table so you in the example you can see we have created a table and then",
    "start": "461240",
    "end": "466560"
  },
  {
    "text": "we perform uh we added a column and then uh in the third step we have uh expanded",
    "start": "466560",
    "end": "474280"
  },
  {
    "text": "uh one of the types as well and again you can query data",
    "start": "474280",
    "end": "479879"
  },
  {
    "text": "without having to do any data read wres or any",
    "start": "479879",
    "end": "484560"
  },
  {
    "text": "changes and as part of Iceberg we are keeping the history of the table so we",
    "start": "486080",
    "end": "491720"
  },
  {
    "text": "use a metadata structure which I'll be discussing later but you keep the entire",
    "start": "491720",
    "end": "497400"
  },
  {
    "text": "history of the table in the form of snapshots so because of that you can perform time travel queries so when you",
    "start": "497400",
    "end": "505560"
  },
  {
    "text": "are inserting data it keeps on adding new snapshots and based on the time you",
    "start": "505560",
    "end": "510759"
  },
  {
    "text": "can travel back to the state of the table and look at the specific data that you had at that time so this is a sample",
    "start": "510759",
    "end": "518880"
  },
  {
    "text": "uh time travel query so you select all from products at this specific time",
    "start": "518880",
    "end": "524159"
  },
  {
    "text": "stamp so that is possible with Iceberg because owing to its uh metadata",
    "start": "524159",
    "end": "531640"
  },
  {
    "text": "architecture so the other Advantage is partitioning right so when you are dealing with very large data set uh",
    "start": "533720",
    "end": "541360"
  },
  {
    "text": "partitioning data is very critical so you need to you need to you need to",
    "start": "541360",
    "end": "546800"
  },
  {
    "text": "store similar data sets pretty much in uh the same location or they should be",
    "start": "546800",
    "end": "552160"
  },
  {
    "text": "collocated so with partitioning one of the main challenge",
    "start": "552160",
    "end": "557640"
  },
  {
    "text": "that we had with conventional uh data L legs is that uh you need you need to do",
    "start": "557640",
    "end": "563440"
  },
  {
    "text": "some manual partitioning by yourself by introducing new columns to the table",
    "start": "563440",
    "end": "568839"
  },
  {
    "text": "right so if you need to partition data using a specific uh field or column then",
    "start": "568839",
    "end": "574560"
  },
  {
    "text": "you need to introduce that into the table as a redundant uh column now with",
    "start": "574560",
    "end": "580200"
  },
  {
    "text": "Iceberg partitioning is built into the system right so we call it hidden partitioning so as part of the data",
    "start": "580200",
    "end": "588360"
  },
  {
    "text": "definition or table definition you can specify which columns uh that I need to",
    "start": "588360",
    "end": "593399"
  },
  {
    "text": "partition data based on so in this case I'm using the category product category",
    "start": "593399",
    "end": "599079"
  },
  {
    "text": "as my partitioning column uh as well as you are using additional uh bucketing",
    "start": "599079",
    "end": "605360"
  },
  {
    "text": "Behavior where you bucket data inside that category based on the product ID so",
    "start": "605360",
    "end": "611000"
  },
  {
    "text": "basically we use the hash of that and with that you don't really need to manually handle partitioning in your",
    "start": "611000",
    "end": "617880"
  },
  {
    "text": "tables and also you get better query performance obviously due to partitioning as well as you reduce the",
    "start": "617880",
    "end": "624399"
  },
  {
    "text": "storage cost because you don't have any redundant uh partitioning fields right",
    "start": "624399",
    "end": "629880"
  },
  {
    "text": "so you use the existing fields to do partitioning now if you look at the",
    "start": "629880",
    "end": "634959"
  },
  {
    "text": "structure that is stored in the data lake so this is uh so this is how it is",
    "start": "634959",
    "end": "640519"
  },
  {
    "text": "partitioned so you data is this is only the data part is partitioned right so data is partitioned based on the",
    "start": "640519",
    "end": "646639"
  },
  {
    "text": "category and inside each category you have uh different uh different directories based on the product",
    "start": "646639",
    "end": "655120"
  },
  {
    "text": "ID finally the probably the most important one the interoperability and",
    "start": "656639",
    "end": "662399"
  },
  {
    "text": "uh and Open Standards right so apach iceberg is an open standard that has been adapted pretty much by all the top",
    "start": "662399",
    "end": "670639"
  },
  {
    "text": "Cloud providers AWS uh Google uh Microsoft assure and across all the",
    "start": "670639",
    "end": "677920"
  },
  {
    "text": "pretty much all the uh data warehouse vendors right snowflake uh data braks uh",
    "start": "677920",
    "end": "683600"
  },
  {
    "text": "and and across other uh open- Source computer engines or analytics engine",
    "start": "683600",
    "end": "688959"
  },
  {
    "text": "such as spark Flink tro so the key Advantage is uh you can store data in",
    "start": "688959",
    "end": "695560"
  },
  {
    "text": "your data Lake using Iceberg format and then use any of those any of those query",
    "start": "695560",
    "end": "701600"
  },
  {
    "text": "engines to query right so you don't really have to duplicate data so you can",
    "start": "701600",
    "end": "706920"
  },
  {
    "text": "use the same data set and use it across multitude of computer engines so without",
    "start": "706920",
    "end": "713120"
  },
  {
    "text": "this support you may be uh simply ingesting data to each and every",
    "start": "713120",
    "end": "718440"
  },
  {
    "text": "computer engine and store it in its native format which is kind of duplicating data across all",
    "start": "718440",
    "end": "724839"
  },
  {
    "text": "these different computer engines and iceberg is getting lot of fraction from the open source Community",
    "start": "724839",
    "end": "731920"
  },
  {
    "text": "as well so it is getting Road adaption uh there are multiple uh Iceberg catalog",
    "start": "731920",
    "end": "737959"
  },
  {
    "text": "Services out there which I'll be discussing uh in the next",
    "start": "737959",
    "end": "742959"
  },
  {
    "text": "steps all right so let's take a quick look at the high level",
    "start": "743600",
    "end": "749959"
  },
  {
    "start": "744000",
    "end": "1000000"
  },
  {
    "text": "uh structure of an iceberg table again going back to the original problem so",
    "start": "749959",
    "end": "755079"
  },
  {
    "text": "you have data in your data Lake now we are going to have a Open Table format on",
    "start": "755079",
    "end": "760839"
  },
  {
    "text": "top of it to get all the benefits that we have talked about so far so if you",
    "start": "760839",
    "end": "766320"
  },
  {
    "text": "look at the structure of an iceberg table you have the data layer this is",
    "start": "766320",
    "end": "772760"
  },
  {
    "text": "where actual data is stored so you have the data files so when you are inserting data you'll be creating these data files",
    "start": "772760",
    "end": "780600"
  },
  {
    "text": "so for data files you can use uh different data formats you can use P",
    "start": "780600",
    "end": "785760"
  },
  {
    "text": "AO uh likewise there are multiple formats that you can choose from for p",
    "start": "785760",
    "end": "791040"
  },
  {
    "text": "is the most popular uh format and in addition to data files there's something",
    "start": "791040",
    "end": "796320"
  },
  {
    "text": "called delete files which will be used to sort of uh remove list certain",
    "start": "796320",
    "end": "801720"
  },
  {
    "text": "entries uh you can read about that uh uh later and on top of the data layer you",
    "start": "801720",
    "end": "808680"
  },
  {
    "text": "have the metadata layer so this is actually the most uh most capabilities",
    "start": "808680",
    "end": "813839"
  },
  {
    "text": "that we discussed so far are implemented at the metadata layer so metadata layer",
    "start": "813839",
    "end": "819160"
  },
  {
    "text": "comprises of different uh files like manifest files manifest list and metadata files we don't really go into",
    "start": "819160",
    "end": "825600"
  },
  {
    "text": "the details but at very high level metadata layer is there to store table",
    "start": "825600",
    "end": "831800"
  },
  {
    "text": "wide information right so things such as table schema partitioning strategy and",
    "start": "831800",
    "end": "838120"
  },
  {
    "text": "pointers to all the different snapshots so when you are creating or when you are",
    "start": "838120",
    "end": "843160"
  },
  {
    "text": "inserting data it is creating these snapshot files and updating the metadata",
    "start": "843160",
    "end": "848639"
  },
  {
    "text": "files according to that right so for example let's say you are inserting some data to a table you create the",
    "start": "848639",
    "end": "855079"
  },
  {
    "text": "corresponding uh data files then update the metadata layer to point to that data file and create a new snapshot and",
    "start": "855079",
    "end": "862399"
  },
  {
    "text": "update the topmost metadata files to point to the latest",
    "start": "862399",
    "end": "867680"
  },
  {
    "text": "snapshot now if you just need to go back uh like if you need to perform a time",
    "start": "867680",
    "end": "873000"
  },
  {
    "text": "travel query then based on the time stamp you can use the same structure and",
    "start": "873000",
    "end": "878199"
  },
  {
    "text": "uh travel through the structure and find the specific snapshot which matches that",
    "start": "878199",
    "end": "883920"
  },
  {
    "text": "Tim stamp and retrieve the corresponding data set the third layer is the iceberg",
    "start": "883920",
    "end": "891160"
  },
  {
    "text": "catalog so Iceberg catalog is used as a central repository to discover all your",
    "start": "891160",
    "end": "897680"
  },
  {
    "text": "Iceberg tables so catalog contains a pointer to the latest metadata uh in",
    "start": "897680",
    "end": "904800"
  },
  {
    "text": "your um data link right so so when suppose you are querying an iceberg table so your query engine connects to",
    "start": "904800",
    "end": "911920"
  },
  {
    "text": "the catalog Service uh provide the table name and then it retrieves the metadata",
    "start": "911920",
    "end": "918160"
  },
  {
    "text": "pointer so and also this is the place that ensure all the asset guarantees and",
    "start": "918160",
    "end": "924240"
  },
  {
    "text": "all the transactional safety so this is the place that you perform all your operations read write update so catalog",
    "start": "924240",
    "end": "932120"
  },
  {
    "text": "needs to care take care of all these operations so there are multiple catalog",
    "start": "932120",
    "end": "937279"
  },
  {
    "text": "Services out there so um there were like glue AWS glue uh snowflake Polaris nessi",
    "start": "937279",
    "end": "946360"
  },
  {
    "text": "uh data brakes Unity so there are multiple catalog implementations out there but uh as in standard uh there's",
    "start": "946360",
    "end": "953480"
  },
  {
    "text": "an iceberg R catalog standard that is being widely adapted now so it is uh defining an uh HTTP rest interface on",
    "start": "953480",
    "end": "961120"
  },
  {
    "text": "top of your catalog Service uh so that it is there's no any vendes specific apis for Iceberg catalogs now so",
    "start": "961120",
    "end": "968560"
  },
  {
    "text": "everyone is adapting the rest catalog now if you look at the storage format this is a table that I created",
    "start": "968560",
    "end": "975600"
  },
  {
    "text": "using Athena and uh if you look at the structure you have the data files and set of metad dat files uh in your",
    "start": "975600",
    "end": "982959"
  },
  {
    "text": "storage right so as you can see there are two uh metadata files uh for version",
    "start": "982959",
    "end": "989440"
  },
  {
    "text": "s0 and S1 So based on the uh so now the latest pointer points to the S1 which is",
    "start": "989440",
    "end": "996519"
  },
  {
    "text": "the latest metadata file all right so now let's come back to",
    "start": "996519",
    "end": "1004319"
  },
  {
    "start": "1000000",
    "end": "1247000"
  },
  {
    "text": "our original problem so we were talking about operational data that you had in Kafka and now you need to use this",
    "start": "1004319",
    "end": "1011920"
  },
  {
    "text": "operational data in your data link right now operational data is stored in Kafka",
    "start": "1011920",
    "end": "1018279"
  },
  {
    "text": "format in its row format uh inside your Kafka broker but data Lake for the data",
    "start": "1018279",
    "end": "1023880"
  },
  {
    "text": "Lake you need to use Iceberg format so you need to feed operational data in your Kafka to data legs and store them",
    "start": "1023880",
    "end": "1030600"
  },
  {
    "text": "in iceberg format this is not really a straightforward uh data pipeline to",
    "start": "1030600",
    "end": "1036839"
  },
  {
    "text": "build right because these are completely different data formats and one is uh",
    "start": "1036839",
    "end": "1042400"
  },
  {
    "text": "more of a streaming data one is sort of a batch like data now most of the",
    "start": "1042400",
    "end": "1048240"
  },
  {
    "text": "organization s have built custom data pipelines to implement the exact same",
    "start": "1048240",
    "end": "1053400"
  },
  {
    "text": "pattern right so you get read data from Kafka do some data some complex data",
    "start": "1053400",
    "end": "1059520"
  },
  {
    "text": "processing and then create Iceberg tables and insert into the data Lake and",
    "start": "1059520",
    "end": "1065840"
  },
  {
    "text": "after creating that you will still have to do lot of table maintenance work because you will be continuously",
    "start": "1065840",
    "end": "1071760"
  },
  {
    "text": "updating metadata files and you need to make sure that you have optimized your Iceberg storage to get Optimum",
    "start": "1071760",
    "end": "1080039"
  },
  {
    "text": "performance now let's take a closer look at the conventional data pipelines that",
    "start": "1080039",
    "end": "1085720"
  },
  {
    "text": "can feed Kafka data to data leges in iceberg format so let's start with the Kafka",
    "start": "1085720",
    "end": "1093360"
  },
  {
    "text": "site so you have all your operational data in Kafka and then you use a sync",
    "start": "1093360",
    "end": "1098760"
  },
  {
    "text": "connector so if you are familiar with Kafka you can use Kafka sync connector to pretty much dump row data directly",
    "start": "1098760",
    "end": "1105360"
  },
  {
    "text": "into the data link right now data is stor in object storage in its raw form",
    "start": "1105360",
    "end": "1112159"
  },
  {
    "text": "as uh raw files now you need to convert this into Iceberg and store that in your",
    "start": "1112159",
    "end": "1118720"
  },
  {
    "text": "data lay so the first step is to do all the type conversions Row Records row",
    "start": "1118720",
    "end": "1123919"
  },
  {
    "text": "cuff cut records needs to be converted to p and corresponding Iceberg metadata",
    "start": "1123919",
    "end": "1129120"
  },
  {
    "text": "format and you need to apply this schema so kfka topic may have an schema",
    "start": "1129120",
    "end": "1134760"
  },
  {
    "text": "Associated so use that as the source schema and generate the corresponding Iceberg",
    "start": "1134760",
    "end": "1140480"
  },
  {
    "text": "schema and then once you create the iceberg tables from that you need to",
    "start": "1140480",
    "end": "1146200"
  },
  {
    "text": "create the iceberg metadata and publish metadata to the iceberg catalog that you",
    "start": "1146200",
    "end": "1151799"
  },
  {
    "text": "using so you may be using an external catalog such as clue so you you have to",
    "start": "1151799",
    "end": "1157600"
  },
  {
    "text": "publish Iceberg metad data to that so once you have the tables you",
    "start": "1157600",
    "end": "1162760"
  },
  {
    "text": "need to keep on maintaining them you need to perform compaction garbage collection and all that so that needs to",
    "start": "1162760",
    "end": "1168080"
  },
  {
    "text": "be part of the job that you are building to uh feed data from Kafka to Iceberg so",
    "start": "1168080",
    "end": "1173960"
  },
  {
    "text": "this is the toil that you need to go through when you are creating at least the bronze version of the table right",
    "start": "1173960",
    "end": "1179880"
  },
  {
    "text": "you are creating the basic raow version of the table but is it is schematized it",
    "start": "1179880",
    "end": "1185000"
  },
  {
    "text": "is in iceberg format now after that you may have to do some further processing",
    "start": "1185000",
    "end": "1190320"
  },
  {
    "text": "so this is where you may apply things such as CDC materialization if the incoming stream is a CDC change lock you",
    "start": "1190320",
    "end": "1198120"
  },
  {
    "text": "need to uh you need to materialize the table according to that and you can also do",
    "start": "1198120",
    "end": "1204919"
  },
  {
    "text": "certain things such as D duplication filtering and apply some specific business logic that you have so you may",
    "start": "1204919",
    "end": "1212000"
  },
  {
    "text": "have some business requirements to maybe to get rid of some of the piis or remove",
    "start": "1212000",
    "end": "1218240"
  },
  {
    "text": "some of the field Fields out from the destination table so that you you",
    "start": "1218240",
    "end": "1223640"
  },
  {
    "text": "perform all the data preparation and finally you have silver or gold quality data in the data medall and architecture",
    "start": "1223640",
    "end": "1232120"
  },
  {
    "text": "Palance so how about completely eliminating all the toil that you have",
    "start": "1232120",
    "end": "1238320"
  },
  {
    "text": "and seamlessly feeding data from your Kafka to your data Lake in iceberg",
    "start": "1238320",
    "end": "1244159"
  },
  {
    "text": "format so that is one solution that we have been exploring as uh as part of the",
    "start": "1244159",
    "end": "1249360"
  },
  {
    "start": "1247000",
    "end": "1427000"
  },
  {
    "text": "confluent so we have been building this uh solution called table flow so as part",
    "start": "1249360",
    "end": "1255520"
  },
  {
    "text": "of so if you are new to conflent conflent is a CF car p providers so we provide manage services for Kafka and",
    "start": "1255520",
    "end": "1262240"
  },
  {
    "text": "and Flink now one of the main challenge that most of our customer have is uh",
    "start": "1262240",
    "end": "1268080"
  },
  {
    "text": "feeding Kafka data into Data legs and they they have pretty much the same data pipeline that we have talked about",
    "start": "1268080",
    "end": "1274640"
  },
  {
    "text": "earlier right you have like multiple multi-step processing a lot of complex",
    "start": "1274640",
    "end": "1280559"
  },
  {
    "text": "expensive uh data processing and in particular if you look at this diagram right so processing row data row",
    "start": "1280559",
    "end": "1288840"
  },
  {
    "text": "unfiltered data inside data warehouses is extremely expensive so that is one main challenge that most of our",
    "start": "1288840",
    "end": "1295520"
  },
  {
    "text": "customers have uh and they need to overcome that now coming back to table flow table flow is uh a solution that",
    "start": "1295520",
    "end": "1303400"
  },
  {
    "text": "enables you you to expose a Kafka Topic in confluent as an iceberg table with a",
    "start": "1303400",
    "end": "1311080"
  },
  {
    "text": "single click of a button right so once so you can navigate",
    "start": "1311080",
    "end": "1316200"
  },
  {
    "text": "to one of your CER topics and enable table flow now as part of that we'll be automatically performing all the data",
    "start": "1316200",
    "end": "1323440"
  },
  {
    "text": "preparation work so this includes uh schematization schema Evolution and per",
    "start": "1323440",
    "end": "1329559"
  },
  {
    "text": "uh converting the Row Records into Iceberg format and then synchronizing",
    "start": "1329559",
    "end": "1335840"
  },
  {
    "text": "all the metadata that you create as part of the table flow and if the incoming stream is a CDC change lock we be also",
    "start": "1335840",
    "end": "1343400"
  },
  {
    "text": "performing CDC materialization as well and it is not just a dat data dump it is",
    "start": "1343400",
    "end": "1349559"
  },
  {
    "text": "not like a syn connector that dump data in iceberg format to the data Lake it",
    "start": "1349559",
    "end": "1354840"
  },
  {
    "text": "will keep on maintaining uh tables as well right so uh so you store data in",
    "start": "1354840",
    "end": "1360799"
  },
  {
    "text": "your data Lake and then it will keep on compacting performing garbage collection",
    "start": "1360799",
    "end": "1366400"
  },
  {
    "text": "and all that now once data once tables are materialized into Iceberg format",
    "start": "1366400",
    "end": "1372880"
  },
  {
    "text": "they are available through a buil-in Iceberg rest catalog right so you can",
    "start": "1372880",
    "end": "1377919"
  },
  {
    "text": "access all these tables from the built-in ice grass catalog so because of",
    "start": "1377919",
    "end": "1383520"
  },
  {
    "text": "this one uh you can use any compute engine that is compatible with Iceberg",
    "start": "1383520",
    "end": "1388840"
  },
  {
    "text": "and seamlessly connect to these tables through these catalog services and",
    "start": "1388840",
    "end": "1394159"
  },
  {
    "text": "consume them now with this you completely eliminate the need of all the",
    "start": "1394159",
    "end": "1399360"
  },
  {
    "text": "toil that we have discussed in the previous uh data flow pattern and also",
    "start": "1399360",
    "end": "1405200"
  },
  {
    "text": "if you have any custom business logic that needs to go into this architecture you can also use uh the built-in Flink",
    "start": "1405200",
    "end": "1412400"
  },
  {
    "text": "services so Flink is a buil-in service of confluent you can perform certain",
    "start": "1412400",
    "end": "1417679"
  },
  {
    "text": "data processing within there uh before materializing uh data to",
    "start": "1417679",
    "end": "1424000"
  },
  {
    "text": "Iceberg all right so with that let's uh let me walk you through a quick demo of",
    "start": "1424000",
    "end": "1431120"
  },
  {
    "start": "1427000",
    "end": "2137000"
  },
  {
    "text": "uh using table floow with a similar use case so in this use case I'll be using",
    "start": "1431120",
    "end": "1438240"
  },
  {
    "text": "uh data stream coming from uh postr SQL CDC so you have all your operational data",
    "start": "1438240",
    "end": "1445600"
  },
  {
    "text": "residing in uh postra SQL database which runs on RDS and then I have configured a",
    "start": "1445600",
    "end": "1453360"
  },
  {
    "text": "postra SQL CDC Source connector in confluent to capture the CDC stream and",
    "start": "1453360",
    "end": "1459200"
  },
  {
    "text": "create two topics two data streams product and orders now first use case is",
    "start": "1459200",
    "end": "1465120"
  },
  {
    "text": "to directly expose this product uh streams stream as an iceberg table to my",
    "start": "1465120",
    "end": "1471760"
  },
  {
    "text": "computer engines uh Athena and snowflake right so Snowflake and aena needs to",
    "start": "1471760",
    "end": "1476919"
  },
  {
    "text": "directly query the product rate that is the first use case now in the Second Use",
    "start": "1476919",
    "end": "1482039"
  },
  {
    "text": "case I'll be doing some additional processing where we perform cleansing and aggregation of uh product and orders",
    "start": "1482039",
    "end": "1489440"
  },
  {
    "text": "and create a new topic or a data stream uh which is uh product sales so this is",
    "start": "1489440",
    "end": "1495399"
  },
  {
    "text": "an aggregated uh sort of a data product and then I expose that as an iceberg to",
    "start": "1495399",
    "end": "1501159"
  },
  {
    "text": "the same computer engines all right so let me start my",
    "start": "1501159",
    "end": "1507919"
  },
  {
    "text": "recorded demo so so the first step is",
    "start": "1507919",
    "end": "1514760"
  },
  {
    "text": "to configure my uh Source connector right so I have my data run data stored",
    "start": "1514760",
    "end": "1522159"
  },
  {
    "text": "in my uh CDC uh sorry in my post gr database and first step is to configure",
    "start": "1522159",
    "end": "1529919"
  },
  {
    "text": "my CDC connector so I started my I already configured CDC connector and uh",
    "start": "1529919",
    "end": "1535240"
  },
  {
    "text": "started the connector now I should see some topics getting created so these",
    "start": "1535240",
    "end": "1541039"
  },
  {
    "text": "These are Kafka topics which corresponds to my tables in the post gr SQL database",
    "start": "1541039",
    "end": "1547360"
  },
  {
    "text": "right so here we have like five Kafka topics uh we are interested in products",
    "start": "1547360",
    "end": "1553440"
  },
  {
    "text": "and orders topics now my C my cfer cluster has table flow enabled that",
    "start": "1553440",
    "end": "1560200"
  },
  {
    "text": "means all the topics under this cluster will be automatically automatically materialized to Iceberg right so Iceberg",
    "start": "1560200",
    "end": "1567880"
  },
  {
    "text": "tables are stored in uh conuent manage storage in this use case uh and they are",
    "start": "1567880",
    "end": "1573600"
  },
  {
    "text": "exposed through a rest API uh which Iceberg catalog rest",
    "start": "1573600",
    "end": "1579399"
  },
  {
    "text": "API now I can navigate to one of these topics since we have uh Iceberg",
    "start": "1579399",
    "end": "1586480"
  },
  {
    "text": "materialization enabled you should be able to see uh Iceberg uh res catalog",
    "start": "1586480",
    "end": "1591840"
  },
  {
    "text": "endpoint right so this is the endpoint that you can use to connect and query",
    "start": "1591840",
    "end": "1597960"
  },
  {
    "text": "these tables right so this is the catalog endpoint so and also it gives the Nam",
    "start": "1597960",
    "end": "1605799"
  },
  {
    "text": "space name the corresponding table name and you need to use the uh required uh",
    "start": "1605799",
    "end": "1611600"
  },
  {
    "text": "API key and credentials uh when quering these tables now as the as we discussed",
    "start": "1611600",
    "end": "1618240"
  },
  {
    "text": "discuss in the use case I have uh I'm going to use atina to query this uh",
    "start": "1618240",
    "end": "1623480"
  },
  {
    "text": "product uh product data stream now here I'm inside my aena P spark",
    "start": "1623480",
    "end": "1630480"
  },
  {
    "text": "notebook and uh I can use the same uh endpoint rest catalog endpoint that I",
    "start": "1630480",
    "end": "1636240"
  },
  {
    "text": "copied from my previous step and use the corresponding credentials and then I can Simply Save",
    "start": "1636240",
    "end": "1645440"
  },
  {
    "text": "the uh ppar notebook now now again I'm using aena p Park",
    "start": "1645440",
    "end": "1653399"
  },
  {
    "text": "which is fully compatible with Iceberg tables right so we can just query the",
    "start": "1653399",
    "end": "1658760"
  },
  {
    "text": "name space to see what are the tables that are already materialized into",
    "start": "1658760",
    "end": "1665679"
  },
  {
    "text": "Iceberg right so now you can see all the tables that we listed in my uh my Kafka",
    "start": "1665679",
    "end": "1671720"
  },
  {
    "text": "topic list are already available as Iceberg table for a Amazon atina now you",
    "start": "1671720",
    "end": "1678279"
  },
  {
    "text": "can query one of these tables now when you run running this query it connects to the iceberg catalog uh using the URL",
    "start": "1678279",
    "end": "1686760"
  },
  {
    "text": "and the credentials it retrieves the table metadata and then start cing uh the storage so as part of the iceberg uh",
    "start": "1686760",
    "end": "1696159"
  },
  {
    "text": "API you can support uh credential vending or remote signing so that you",
    "start": "1696159",
    "end": "1701559"
  },
  {
    "text": "don't you don't really need direct access to the underlying bucket storage or three storage it will be performed",
    "start": "1701559",
    "end": "1708120"
  },
  {
    "text": "per in credential vending the catalog would provide the required uh required keys to access the underlying storage",
    "start": "1708120",
    "end": "1715840"
  },
  {
    "text": "now as you can see we can cry all the data that we had in the topic through an",
    "start": "1715840",
    "end": "1722640"
  },
  {
    "text": "uh computer engine which is supporting Iceberg so we haven't done any conversion ourself so everything is",
    "start": "1722640",
    "end": "1728480"
  },
  {
    "text": "automatically converted to Iceberg now how about doing this with",
    "start": "1728480",
    "end": "1733919"
  },
  {
    "text": "snowflake now in a typical use case you may have to create native snowflake tables and ingest uh data to Snowflake",
    "start": "1733919",
    "end": "1741399"
  },
  {
    "text": "and do this but with Iceberg you can use the same data set but query it from",
    "start": "1741399",
    "end": "1747760"
  },
  {
    "text": "Snowflake now with snowflake there are some additional steps so you need to create a external volume that's a",
    "start": "1747760",
    "end": "1756279"
  },
  {
    "text": "snowflake concept which points to the actual uh object storage and then you configure a catalog integration uh where",
    "start": "1756279",
    "end": "1764279"
  },
  {
    "text": "we specify the details of uh conuent table flow catalog to point to the",
    "start": "1764279",
    "end": "1769480"
  },
  {
    "text": "Confluence catalog when curing uh data so once you have uh external volume",
    "start": "1769480",
    "end": "1776000"
  },
  {
    "text": "and catalog integration created we can start curing the product now uh",
    "start": "1776000",
    "end": "1782720"
  },
  {
    "text": "snowflake requires you to create an externally managed table within confluent uh ecosystem uh sorry within",
    "start": "1782720",
    "end": "1790519"
  },
  {
    "text": "snowflake ecosystem so this is a table that uh you not really copying data",
    "start": "1790519",
    "end": "1796840"
  },
  {
    "text": "inside snowflake right this is just a table a logical table that points to to",
    "start": "1796840",
    "end": "1802080"
  },
  {
    "text": "an external uh external data uh and you create an externally managed table using",
    "start": "1802080",
    "end": "1808600"
  },
  {
    "text": "the external volume that I created above and the catalog integration so with this one you can uh",
    "start": "1808600",
    "end": "1816480"
  },
  {
    "text": "start creating the snowflake",
    "start": "1816480",
    "end": "1821039"
  },
  {
    "text": "table all right now it is successfully created",
    "start": "1823120",
    "end": "1828519"
  },
  {
    "text": "and now I can start querying the uh externally managed table so I should see",
    "start": "1828519",
    "end": "1833720"
  },
  {
    "text": "the same data set when I query this table as",
    "start": "1833720",
    "end": "1838200"
  },
  {
    "text": "well okay all right",
    "start": "1842000",
    "end": "1848960"
  },
  {
    "text": "so so we we can see the same data set again pointing to the same data storage",
    "start": "1848960",
    "end": "1854840"
  },
  {
    "text": "uh in in this case in conuent now in the Second Use case uh we'll be",
    "start": "1854840",
    "end": "1861320"
  },
  {
    "text": "doing some additional processing right so obviously you can inest this kind of",
    "start": "1861320",
    "end": "1866960"
  },
  {
    "text": "bronze tables directly into the data Lake and do the pre-processing uh there",
    "start": "1866960",
    "end": "1872039"
  },
  {
    "text": "within the data Lake itself but uh if you have to do some data cleansing and",
    "start": "1872039",
    "end": "1878159"
  },
  {
    "text": "aggregation as part of the data collection itself it is it'll be more efficient to do this at the data source",
    "start": "1878159",
    "end": "1885559"
  },
  {
    "text": "level itself so in this case data source is Kafka right you are collecting operational data and as and when you",
    "start": "1885559",
    "end": "1892480"
  },
  {
    "text": "collect and store them you perform all these data uh pre- agregation or",
    "start": "1892480",
    "end": "1898240"
  },
  {
    "text": "pre-processing work prior to handing it over to the data l so this is exactly",
    "start": "1898240",
    "end": "1903279"
  },
  {
    "text": "what we are doing in this use case so we collect data from uh Now using confluent",
    "start": "1903279",
    "end": "1910279"
  },
  {
    "text": "uh you can use uh the builtin Flink service so you can run Flink SQL queries",
    "start": "1910279",
    "end": "1915639"
  },
  {
    "text": "inside content so that you query data inside your Kafka topics and",
    "start": "1915639",
    "end": "1921919"
  },
  {
    "text": "create new tables or topics using Flink right so in this",
    "start": "1921919",
    "end": "1927639"
  },
  {
    "text": "example as you can see here I'm quering data from uh two tables in fact three",
    "start": "1927639",
    "end": "1933639"
  },
  {
    "text": "tables product orders and another supporting table order items so I join",
    "start": "1933639",
    "end": "1940159"
  },
  {
    "text": "all these tables together and create a new data product product sales and then",
    "start": "1940159",
    "end": "1947039"
  },
  {
    "text": "uh I'm also doing some data cleansing as well now this data product uh in the",
    "start": "1947039",
    "end": "1952600"
  },
  {
    "text": "confluent ecosystem uh every topic is a table and every every table is a topic",
    "start": "1952600",
    "end": "1958960"
  },
  {
    "text": "right since we are creating a new Flink table uh and inserting data to it this should be available as a Kafka topic as",
    "start": "1958960",
    "end": "1966440"
  },
  {
    "text": "well now uh we can simply start running this query this would be keep on running",
    "start": "1966440",
    "end": "1973360"
  },
  {
    "text": "uh as a streaming query and as and when data comes into this uh this top iics it",
    "start": "1973360",
    "end": "1978760"
  },
  {
    "text": "will uh start inserting it to the new product sales",
    "start": "1978760",
    "end": "1984399"
  },
  {
    "text": "table now uh before we query it from Snowflake or aena you can run a query",
    "start": "1984399",
    "end": "1990519"
  },
  {
    "text": "from Flink itself to see whether the data is available in the new uh table or",
    "start": "1990519",
    "end": "1997240"
  },
  {
    "text": "or Kafka topic all right so you have all the table all the data available in the new",
    "start": "1997240",
    "end": "2003240"
  },
  {
    "text": "product sales topic and now I can navigate to my Kafka topic list and you",
    "start": "2003240",
    "end": "2010880"
  },
  {
    "text": "should see a product sales table since I have Iceberg enabled it is again",
    "start": "2010880",
    "end": "2017200"
  },
  {
    "text": "automatically materialized as an iceberg table now I can go back to my aena uh uh",
    "start": "2017200",
    "end": "2025039"
  },
  {
    "text": "atina notebook and start running a quer against product table so",
    "start": "2025039",
    "end": "2032080"
  },
  {
    "text": "you you already see product sales is listed as a materialized table and you should see data available in uh product",
    "start": "2032080",
    "end": "2039600"
  },
  {
    "text": "sales as well",
    "start": "2039600",
    "end": "2044880"
  },
  {
    "text": "okay all right so now we have the new data set also available uh to Athena now",
    "start": "2046320",
    "end": "2053520"
  },
  {
    "text": "you can do the same thing inside snowflake uh you don't really have to create any additional uh external",
    "start": "2053520",
    "end": "2059800"
  },
  {
    "text": "volumes or catalog integration you can uh reuse the same uh same volume and the catalog",
    "start": "2059800",
    "end": "2067358"
  },
  {
    "text": "integration but in this case create another external table which points to product sales table that you previously",
    "start": "2067359",
    "end": "2074240"
  },
  {
    "text": "created and uh create a new external uh snowflake",
    "start": "2074240",
    "end": "2080440"
  },
  {
    "text": "table now then you can start cing this",
    "start": "2080440",
    "end": "2086638"
  },
  {
    "text": "table and you should uh see new data set is bit getting inserted to snowflake as",
    "start": "2087760",
    "end": "2094720"
  },
  {
    "text": "well all right so I I think yeah so that is what we want to cover as part of the",
    "start": "2094720",
    "end": "2101079"
  },
  {
    "text": "this as part of this demo so it's important to understand that this is uh",
    "start": "2101079",
    "end": "2106599"
  },
  {
    "text": "so we talked about specific solution that we have built as part of Contra but this is getting lot of traction because",
    "start": "2106599",
    "end": "2112640"
  },
  {
    "text": "most cases uh data legs uh getting data from streaming uh systems like all the",
    "start": "2112640",
    "end": "2120160"
  },
  {
    "text": "operational data is captured into streaming uh captured as streaming data so therefore feeding that data into Data",
    "start": "2120160",
    "end": "2126839"
  },
  {
    "text": "leg is becoming more and more important so there will be more solutions built around this but uh we hope conference uh",
    "start": "2126839",
    "end": "2133599"
  },
  {
    "text": "table floor is one of the leading Solutions out there so I think that's all I plan to cover today thank you very",
    "start": "2133599",
    "end": "2141160"
  },
  {
    "start": "2137000",
    "end": "2157000"
  },
  {
    "text": "much Kass",
    "start": "2141160",
    "end": "2144400"
  }
]