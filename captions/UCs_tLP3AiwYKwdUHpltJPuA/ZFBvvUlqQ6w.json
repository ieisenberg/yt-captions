[
  {
    "start": "0",
    "end": "75000"
  },
  {
    "text": "welcome uh I want to talk about the smallest distributed system and probably in the literal sense as much as in the",
    "start": "5520",
    "end": "11679"
  },
  {
    "text": "physical sense it is really really small uh my name is Matias Meer I go byid rage on Twitter there is no relation at all",
    "start": "11679",
    "end": "18400"
  },
  {
    "text": "to steroids in this name um I work for for a tiny company in",
    "start": "18400",
    "end": "24000"
  },
  {
    "text": "Berlin called Travis C it is a host a continuous integration system and it",
    "start": "24000",
    "end": "29599"
  },
  {
    "text": "started out as a very simple idea originally that was by the end of 2010 there was uh GitHub was on the rise for",
    "start": "29599",
    "end": "36440"
  },
  {
    "text": "sharing and collaborating on open source projects but there was no platform around that gave projects the ability to",
    "start": "36440",
    "end": "43239"
  },
  {
    "text": "run their tests to have a you know a well- integrated continuous integration platform that would whenever a new",
    "start": "43239",
    "end": "50520"
  },
  {
    "text": "commit came in or when someone contributed a patch uh would take that patch run it and say everything is green",
    "start": "50520",
    "end": "56359"
  },
  {
    "text": "with that with that thing um we started out as an open source project",
    "start": "56359",
    "end": "62559"
  },
  {
    "text": "basically a platform targeting open source uh products as much as our own code being open source but we since then",
    "start": "62559",
    "end": "69360"
  },
  {
    "text": "moved into other commercial spaces while supporting private projects but that is not really what this is about today um I",
    "start": "69360",
    "end": "76759"
  },
  {
    "start": "75000",
    "end": "326000"
  },
  {
    "text": "want to go back to 2011 2011 was when we saw very very well initial traction we",
    "start": "76759",
    "end": "83200"
  },
  {
    "text": "had quite a few open source projects uh adopt adopt travisci and we ran about",
    "start": "83200",
    "end": "88240"
  },
  {
    "text": "700 builds per day that included project like rails um a couple of smaller",
    "start": "88240",
    "end": "93759"
  },
  {
    "text": "projects um but it was not a lot all that Ren of a single server we had one",
    "start": "93759",
    "end": "99119"
  },
  {
    "text": "box that basically ran all of the tests it was virtualized using virtual box uh",
    "start": "99119",
    "end": "104759"
  },
  {
    "text": "it was a a pretty ghetto setup in in all but it actually took us quite far",
    "start": "104759",
    "end": "111079"
  },
  {
    "text": "um yeah and the application itself had a very simple architecture it was basically it took a few weeks to build",
    "start": "111079",
    "end": "117320"
  },
  {
    "text": "travisci the idea is originally was to have you know to add on to the status quo of continuous integration systems",
    "start": "117320",
    "end": "125119"
  },
  {
    "text": "um one idea was that the user interface should be live updating you know that",
    "start": "125119",
    "end": "130520"
  },
  {
    "text": "when when a build is running you should see the locks stream in live um the the UI should update as builds finished this",
    "start": "130520",
    "end": "136720"
  },
  {
    "text": "build started and something like that basically should it was supposed to have a nicer user interface than that other",
    "start": "136720",
    "end": "142160"
  },
  {
    "text": "continuous integration system that was very popular and still is at that time",
    "start": "142160",
    "end": "147200"
  },
  {
    "text": "um but it was supposed to make all of that freely available for want to use basically when you had an open source",
    "start": "147200",
    "end": "152560"
  },
  {
    "text": "project you could plug it into travisci and um it would run your tests and it",
    "start": "152560",
    "end": "158239"
  },
  {
    "text": "would run them fairly well um so original architecture was very simple we had a web application that talked to the",
    "start": "158239",
    "end": "165000"
  },
  {
    "text": "database that web application was responsible for serving our API for um",
    "start": "165000",
    "end": "170640"
  },
  {
    "text": "accepting new commit notifications from GitHub whenever a new commit is pushed to GitHub we get a web hook notification",
    "start": "170640",
    "end": "176599"
  },
  {
    "text": "from them saying here's the new data that just came in do something with it that's so that was what the web friend",
    "start": "176599",
    "end": "182680"
  },
  {
    "text": "was also responsible for it basically took that that payload and turned it into something that was runnable and the",
    "start": "182680",
    "end": "189120"
  },
  {
    "text": "the component that was uh responsible for running that stuff is called Hub it in the end turned out to do a lot more",
    "start": "189120",
    "end": "195440"
  },
  {
    "text": "than that but um that's its basic function right now uh in in this",
    "start": "195440",
    "end": "200480"
  },
  {
    "text": "scenario and there's a fourth component that's called jobs which is basically an",
    "start": "200480",
    "end": "205599"
  },
  {
    "text": "agent a run agent that was running on our one build server that would take new jobs",
    "start": "205599",
    "end": "211000"
  },
  {
    "text": "um execute execute a couple of scripts in the virtual machine and basically send a notification whenever it's",
    "start": "211000",
    "end": "217159"
  },
  {
    "text": "finished uh that component was also responsible for streaming build locks so when as a build ran it would just it",
    "start": "217159",
    "end": "224239"
  },
  {
    "text": "would just sift off uh the output from the build and send it send it in chunks",
    "start": "224239",
    "end": "230400"
  },
  {
    "text": "and this is where Hub gets a little bit more complicated um because jobs basically own only interacts with Hub",
    "start": "230400",
    "end": "237720"
  },
  {
    "text": "and the only thing it basically sends is Hub sends jobs here's a new here's a new job run run it what jobs does is it",
    "start": "237720",
    "end": "245480"
  },
  {
    "text": "takes a job runs it streams the build lock build locks and the build result and that makes Hub already a little bit",
    "start": "245480",
    "end": "251680"
  },
  {
    "text": "more complicated we have a little orange orange rabbit down here which is rabbit mq uh which is basically the single",
    "start": "251680",
    "end": "257840"
  },
  {
    "text": "point of interaction between uh the jobs processing and the Hub itself so it kind",
    "start": "257840",
    "end": "263199"
  },
  {
    "text": "of became much to my dismay um our one of the central components in our system",
    "start": "263199",
    "end": "268759"
  },
  {
    "text": "rabbit mq and hub would also and that that is a part where the live updates come in um Hub would also uh notify",
    "start": "268759",
    "end": "276880"
  },
  {
    "text": "Pusher when when new when new jobs when new job resols or lcks came in through",
    "start": "276880",
    "end": "282080"
  },
  {
    "text": "rabit mq Hub would take the take these things and push them push them forward to Pusher using Pusher we could then do",
    "start": "282080",
    "end": "288520"
  },
  {
    "text": "live updates in the user interface we push like by now we push 40 m 40 50",
    "start": "288520",
    "end": "294039"
  },
  {
    "text": "million messages per day to push her but back then it was fairly minimal but it was it was a great aha effect for people",
    "start": "294039",
    "end": "299840"
  },
  {
    "text": "looking at the user interface uh to see all of that update and life and yeah",
    "start": "299840",
    "end": "305800"
  },
  {
    "text": "it's still a fairly simple architecture but as you can see it's already somewhat distributed but it was built with with",
    "start": "305800",
    "end": "311720"
  },
  {
    "text": "with good intentions in mind it was basically assuming everything would work all the time and everything would work in ways that we can foresee and that we",
    "start": "311720",
    "end": "319360"
  },
  {
    "text": "can expect um unfortunately that really didn't turn out that well",
    "start": "319360",
    "end": "326000"
  },
  {
    "start": "326000",
    "end": "603000"
  },
  {
    "text": "um if we forward to 2012 we we order we eded one magnitude",
    "start": "326000",
    "end": "331039"
  },
  {
    "text": "of builds we're now at 7,000 builds per day we've been adopted by a couple of Open Source projects by a lot more",
    "start": "331039",
    "end": "337440"
  },
  {
    "text": "popular one in the PHP World um we added a lot more languages we added earling we",
    "start": "337440",
    "end": "342960"
  },
  {
    "text": "added Java we added um Pearl even PHP",
    "start": "342960",
    "end": "348039"
  },
  {
    "text": "python all the popular languages were basically supported out of the box the problem was that with the platform being",
    "start": "348039",
    "end": "356039"
  },
  {
    "text": "adopted more and more we had a lot more responsibility to keep it running running Travis C was originally built as",
    "start": "356039",
    "end": "362160"
  },
  {
    "text": "basically an idea as and as an ideal and it was run by people uh as a side",
    "start": "362160",
    "end": "367479"
  },
  {
    "text": "project basically as something they you know that people could use um something",
    "start": "367479",
    "end": "373319"
  },
  {
    "text": "that well it was just it was just run on its own and we would just push a little bit of coat every now and then and",
    "start": "373319",
    "end": "379039"
  },
  {
    "text": "unfortunately that really didn't that really didn't work out that well because we started to have problems we started",
    "start": "379039",
    "end": "385280"
  },
  {
    "text": "to see problems in our architecture and we started to see uh everything break at the seams basic basically in the very",
    "start": "385280",
    "end": "390520"
  },
  {
    "text": "literal sense um and the biggest problem we had was actually",
    "start": "390520",
    "end": "395880"
  },
  {
    "text": "visibility the most embarrassing the most embarrassing bits were and that's when I joined Travis I was in the",
    "start": "395880",
    "end": "401160"
  },
  {
    "text": "beginning of 2012 was that people come from the community came say hey my builds are not running or something",
    "start": "401160",
    "end": "408919"
  },
  {
    "text": "something isn't right my builds are crapping out my builds are aing out because we integrate with a lot of",
    "start": "408919",
    "end": "414080"
  },
  {
    "text": "external resources we integrate with dependency dependency mirrors like Maven Ruby Jim uh php's dependency system",
    "start": "414080",
    "end": "421840"
  },
  {
    "text": "Pearl PE dependencies pythons and so on so we had people from the community approaches and say this stuff isn't",
    "start": "421840",
    "end": "428960"
  },
  {
    "text": "working and that in the in the in the beginning turned out to be our biggest problem we",
    "start": "428960",
    "end": "435199"
  },
  {
    "text": "had no idea what travisia was doing it was already a fairly big distributed system we now had 10 build servers",
    "start": "435199",
    "end": "440879"
  },
  {
    "text": "running uh running tests so we had a total of 60 VMS available and that meant",
    "start": "440879",
    "end": "447199"
  },
  {
    "text": "a lot of stuff could already start breaking at any point in time and we had no idea what would break and we had no",
    "start": "447199",
    "end": "452720"
  },
  {
    "text": "idea when it would break because we didn't have any visibility by means of metrics we didn't have any aggregated",
    "start": "452720",
    "end": "459599"
  },
  {
    "text": "lcks and we didn't have any alerting and that was kind of embarrassing it was embarrassing that people actually had to",
    "start": "459599",
    "end": "465879"
  },
  {
    "text": "push us to look into things and it was also embarrassing because um we started working on a commercial version",
    "start": "465879",
    "end": "472850"
  },
  {
    "text": "[Music] um you know being notified by customers that my my product is down as still one",
    "start": "472850",
    "end": "479000"
  },
  {
    "text": "of my biggest nightmares so we had to work a lot to actually to actually make this happen that was my first project",
    "start": "479000",
    "end": "485479"
  },
  {
    "text": "and it was the most important one because in a distributed system when you when you have no visibility you you",
    "start": "485479",
    "end": "490560"
  },
  {
    "text": "cannot reason about what it is doing in production that's why my my biggest takeaway from this from this particular",
    "start": "490560",
    "end": "497360"
  },
  {
    "text": "piece is that if when you build the system you need to think about monitoring up front you need to think",
    "start": "497360",
    "end": "502520"
  },
  {
    "text": "about how how to monitor it what to monitor for and what to alert on and",
    "start": "502520",
    "end": "507960"
  },
  {
    "text": "that's not really what Travis y was built based on it was just you know assumed that it would just work and that",
    "start": "507960",
    "end": "514560"
  },
  {
    "text": "things would be running smoothly and I'm sorry uh that things were",
    "start": "514560",
    "end": "522440"
  },
  {
    "text": "running smoothly and that was my biggest takeaway that in a distributed system even as small as it is you need visibility you need to be able to see",
    "start": "522440",
    "end": "529800"
  },
  {
    "text": "what is happening in it at any point in time you need to be able to reason about what it was doing at a particular point",
    "start": "529800",
    "end": "534839"
  },
  {
    "text": "in time and thankfully for us this is a lot easier in a smaller distributed system system then in a system that does",
    "start": "534839",
    "end": "541800"
  },
  {
    "text": "thousands of requests per second that handles thousands of things in the background in our system which is still",
    "start": "541800",
    "end": "547399"
  },
  {
    "text": "fairly small and it was easy to reason about what it is doing but we actually had to work on making this happen we had",
    "start": "547399",
    "end": "553200"
  },
  {
    "text": "to work on adding logging we had to work on adding the ability to trace requests as they came into the system as they",
    "start": "553200",
    "end": "558800"
  },
  {
    "text": "were handled by the system to actually realize what is happening",
    "start": "558800",
    "end": "564720"
  },
  {
    "text": "because it's it's it's become the central bit of our system to be to be able to monor it to be able to run it in",
    "start": "564720",
    "end": "570720"
  },
  {
    "text": "production but the biggest problem we had was once we had that monitoring we actually sof things breaking that's why visibility brings",
    "start": "570720",
    "end": "578120"
  },
  {
    "text": "brings a little bit of a responsibility with it if you see if you see failure happen you need to do something about it",
    "start": "578120",
    "end": "583440"
  },
  {
    "text": "otherwise you don't need to put in monitoring in the first place that's where distributed system becomes somewhere of an of a notion that you",
    "start": "583440",
    "end": "591279"
  },
  {
    "text": "accept that things break and you accept that you have to do something about it that was my realization of the last year",
    "start": "591279",
    "end": "597120"
  },
  {
    "text": "when things are broken we have to do something about it unfortunately things started breaking a lot that was the",
    "start": "597120",
    "end": "604600"
  },
  {
    "text": "downside our original Little architecture in 2012 started falling apart as we did 7,000 builds per day we",
    "start": "604600",
    "end": "610720"
  },
  {
    "text": "pushed a lot of lock messages every day",
    "start": "610720",
    "end": "616360"
  },
  {
    "text": "and it just wasn't reliable it keep it kept falling over it just kept one component in particular kept falling",
    "start": "616399",
    "end": "622920"
  },
  {
    "text": "over and that's the Hub The Hub was doing a lot of things it was handling",
    "start": "622920",
    "end": "628800"
  },
  {
    "text": "build lck marks it was handling notifications B basically when when the build is finished we notify users that",
    "start": "628800",
    "end": "634320"
  },
  {
    "text": "via email or via campfire VI RSC that of the build result it was handling build",
    "start": "634320",
    "end": "639959"
  },
  {
    "text": "requests as they came in from GitHub it talked to GitHub it talked to GitHub we interact with the API a lot and it",
    "start": "639959",
    "end": "646519"
  },
  {
    "text": "talked to Pusher and it synchronized user data one one bit of Travis is basically um to sync all of the data for",
    "start": "646519",
    "end": "655040"
  },
  {
    "text": "a user that we have all of the repositories that they have access to and basically store that information on",
    "start": "655040",
    "end": "660760"
  },
  {
    "text": "our end so that we know who has access to which repository and hub was a single",
    "start": "660760",
    "end": "666760"
  },
  {
    "text": "process at the time and a single process is not really a distributed system and",
    "start": "666760",
    "end": "673200"
  },
  {
    "text": "the problem with that single process was that it was impossible to scale out we could not just fire up a new one and",
    "start": "673200",
    "end": "678760"
  },
  {
    "text": "have all of this uh run in parallel we had to have this single process and it",
    "start": "678760",
    "end": "684160"
  },
  {
    "text": "did a lot it did actually fairly well it did up to the lock processing inside a single container did um up to 100",
    "start": "684160",
    "end": "691079"
  },
  {
    "text": "messages per second but that really wasn't enough because it kept us from growing and one particular piece that is",
    "start": "691079",
    "end": "698600"
  },
  {
    "text": "has become very ingrain in how I think about distributed systems uh I have a good example for and that's the G",
    "start": "698600",
    "end": "705279"
  },
  {
    "text": "talking to the GitHub API we're one of the GitHub api's",
    "start": "705279",
    "end": "712519"
  },
  {
    "text": "heaviest consumers we fire hundreds of thousands of requests through the API every day and we we assumed",
    "start": "712519",
    "end": "720079"
  },
  {
    "text": "um we when a build request comes in we we fetch the build configuration which is stored in the repository from GitHub",
    "start": "720079",
    "end": "726600"
  },
  {
    "text": "we synchronize user data uh with the GitHub API and we do um we update",
    "start": "726600",
    "end": "731839"
  },
  {
    "text": "commits based on how a build went GitHub you can have when you when you have a poll request in GitHub you can have a",
    "start": "731839",
    "end": "737399"
  },
  {
    "text": "nice notification that the build has failed or it succeeded and that's also what we do um we treated the GitHub API",
    "start": "737399",
    "end": "744959"
  },
  {
    "text": "as a good friend and you know let's this is more about the integration with the GitHub API than me bitching about that",
    "start": "744959",
    "end": "751519"
  },
  {
    "text": "it is terrible it is more it is a learning point uh for us um we basically",
    "start": "751519",
    "end": "757240"
  },
  {
    "text": "treated their API as a as a good friend we called it and we expected it to pick up the phone and answer us pretty",
    "start": "757240",
    "end": "762639"
  },
  {
    "text": "quickly and always answer and pretty much always answer us and that turned out to be a wrong assumption it did not",
    "start": "762639",
    "end": "770320"
  },
  {
    "text": "always answer us it did not always answer us with the data that we expected um it did not always answer us um with a",
    "start": "770320",
    "end": "778760"
  },
  {
    "text": "reasonable result um it throws all kinds of crazy error codes in your interface and at",
    "start": "778760",
    "end": "785839"
  },
  {
    "text": "some point uh about a year ago it really it things",
    "start": "785839",
    "end": "791160"
  },
  {
    "text": "really started falling apart that was one of our biggest outages a year ago and that was we that was because we used",
    "start": "791160",
    "end": "798120"
  },
  {
    "text": "a somewhat undocumented feature of their API I that's where it already starts but",
    "start": "798120",
    "end": "803279"
  },
  {
    "text": "it was a feature that was added for us and that feature at that time was uh simply disabled because it cost problems",
    "start": "803279",
    "end": "809720"
  },
  {
    "text": "on the other end um on our end it also cost problems because all hell broke",
    "start": "809720",
    "end": "815600"
  },
  {
    "text": "loose and we basically stop building anything um we stop building anything",
    "start": "815600",
    "end": "822000"
  },
  {
    "text": "because um when a new build comes in we go to the API which which we check a few things that need to be ready for us to",
    "start": "822000",
    "end": "828199"
  },
  {
    "text": "run the build and that request basically we gave that request initially about I",
    "start": "828199",
    "end": "833240"
  },
  {
    "text": "think 10 minutes and on that particular day um",
    "start": "833240",
    "end": "838600"
  },
  {
    "text": "the request actually started taking 10 minutes all of our API requests basically started timing out but the",
    "start": "838600",
    "end": "843800"
  },
  {
    "text": "time out was enormous when we when we do uh 7,000 builds per day we basically we get at",
    "start": "843800",
    "end": "851199"
  },
  {
    "text": "peak times we get about a th000 build notifications per hour back at that time",
    "start": "851199",
    "end": "856279"
  },
  {
    "text": "so one handling one build request in a in a setup that is basically just a",
    "start": "856279",
    "end": "861440"
  },
  {
    "text": "single process uh and requiring 10 minutes to handle that single build request is not a great idea it basically",
    "start": "861440",
    "end": "868519"
  },
  {
    "text": "uh made everything everything fall apart and the worst thing is our build processing did not handle fa errors very",
    "start": "868519",
    "end": "874320"
  },
  {
    "text": "well so after the time I actually hit after 10 minutes we would just drop the build request we would just silently",
    "start": "874320",
    "end": "880440"
  },
  {
    "text": "drop it on the floor and move on to the next one and time out again it was not a great it was not really great um it was",
    "start": "880440",
    "end": "888040"
  },
  {
    "text": "a very long night for us to figure out what actually happened we we did have logs and we did have metrics at the time but it wasn't enough to actually figure",
    "start": "888040",
    "end": "894680"
  },
  {
    "text": "out this particular problem what we learned from that was",
    "start": "894680",
    "end": "899880"
  },
  {
    "text": "that all the external resources that we have that we use need to be treated as something that could break at any time",
    "start": "899880",
    "end": "906639"
  },
  {
    "text": "and that doesn't really mean that we need to protect ourselves from that that we need to put up a barrier and stop talking to the GitHub ABI but that we",
    "start": "906639",
    "end": "913880"
  },
  {
    "text": "need to have means in place to fail fast and to respond to to respond to these failures usually when the GTH API is",
    "start": "913880",
    "end": "920519"
  },
  {
    "text": "down or is unavailable it's a temporary issue it will come back within an hour",
    "start": "920519",
    "end": "925800"
  },
  {
    "text": "and for us it started to be a business decision do we want to drop build requests when the GitHub API is down or",
    "start": "925800",
    "end": "931199"
  },
  {
    "text": "do we want to continue to function and we decided that for our users and for our customers it is",
    "start": "931199",
    "end": "937440"
  },
  {
    "text": "important that these builds run so we had to do something about that what we basically did is we isolated all the API",
    "start": "937440",
    "end": "943079"
  },
  {
    "text": "requests and we added shorter timeouts much shorter timeouts they're still way too long in one way in in their own way",
    "start": "943079",
    "end": "950279"
  },
  {
    "text": "but they're in an acceptable fashion basically that particular time request is now down to I think 20 seconds which",
    "start": "950279",
    "end": "956199"
  },
  {
    "text": "is still a lot um but if it f if it times out we just we just",
    "start": "956199",
    "end": "962399"
  },
  {
    "text": "ignore whatever that whatever that API call was supposed to do supposed to be returning and we continue doing what we",
    "start": "962399",
    "end": "968959"
  },
  {
    "text": "wanted to do but that itself was not enough because it only allowed us to fail fast",
    "start": "968959",
    "end": "974959"
  },
  {
    "text": "but it did not allow us to um I don't know to retry basically what the other",
    "start": "974959",
    "end": "980920"
  },
  {
    "text": "thing that we added in response to that is after isolating it to the when the request timed out with an error we added",
    "start": "980920",
    "end": "986839"
  },
  {
    "text": "retries to it and to make it a little bit more resilient to um to the problems",
    "start": "986839",
    "end": "992440"
  },
  {
    "text": "in the GitHub API and you know basically stopping reducing the pressure that we put on their on their API we added",
    "start": "992440",
    "end": "999759"
  },
  {
    "text": "exponential back offs so this is a meme that has slowly emerged in our application that not even talking to",
    "start": "999759",
    "end": "1006079"
  },
  {
    "text": "external external components but even talking to our own components we treat them as something that we we started",
    "start": "1006079",
    "end": "1011920"
  },
  {
    "text": "treating them as something that that can break at any time that could plunge into chaos at any time",
    "start": "1011920",
    "end": "1017600"
  },
  {
    "text": "and there an inherent uncertainty about using external apis and even about using",
    "start": "1017600",
    "end": "1023279"
  },
  {
    "text": "your own apis in a world where you move to um basically service oriented",
    "start": "1023279",
    "end": "1029600"
  },
  {
    "text": "architecture um basically every application every application that you use is a means of uncertainty they can",
    "start": "1029600",
    "end": "1036199"
  },
  {
    "text": "break at any time and because everything breaks at any time it's the unfortunate",
    "start": "1036199",
    "end": "1042438"
  },
  {
    "text": "reality of a distributed system that things just break and we had we started having to deal with that uncertainty",
    "start": "1042439",
    "end": "1048438"
  },
  {
    "text": "because was built with you know very good intentions it just assumed that networks would not partition that apis",
    "start": "1048439",
    "end": "1055480"
  },
  {
    "text": "would always be available no matter how many requests we throw at them and that our own components would always be",
    "start": "1055480",
    "end": "1060600"
  },
  {
    "text": "available and just we had to learn from a lot of from a lot of failures and outages that's just not the case so",
    "start": "1060600",
    "end": "1067880"
  },
  {
    "text": "dealing with uncertainty has become one of my memes for distributed systems even when you have full control over the",
    "start": "1067880",
    "end": "1073520"
  },
  {
    "text": "system itself you cannot assume that everything is always running at all the time and that is always running well",
    "start": "1073520",
    "end": "1079600"
  },
  {
    "text": "there's always another problem and at some point they will come together and break everything",
    "start": "1079600",
    "end": "1085520"
  },
  {
    "text": "spectacularly like on that this particular weekend it took us a full weekend to figure out what happened with",
    "start": "1085520",
    "end": "1090600"
  },
  {
    "text": "the GitHub API and in the end they basically flipped the feature back on on their end and everything was working",
    "start": "1090600",
    "end": "1096600"
  },
  {
    "text": "again that was an unfortunate weekend for us",
    "start": "1096600",
    "end": "1102200"
  },
  {
    "start": "1101000",
    "end": "1374000"
  },
  {
    "text": "but I want to get back to the hub and to our problem of just being able to run",
    "start": "1102320",
    "end": "1108880"
  },
  {
    "text": "one process um H did all these things and all all on their own they were pretty easy to break out we could break",
    "start": "1108880",
    "end": "1116360"
  },
  {
    "text": "out notifications we could break out processing the builds as they as commit notifications came in from GitHub we",
    "start": "1116360",
    "end": "1122640"
  },
  {
    "text": "could push out synchronizing data with GitHub because it's not really concern that needs to be you know in the in the",
    "start": "1122640",
    "end": "1129799"
  },
  {
    "text": "focus point of what is running of what needs to run all the time so we basically had all these blocks that we",
    "start": "1129799",
    "end": "1135559"
  },
  {
    "text": "could basically just pull out uh pull out of the Hub and make up just a tiny",
    "start": "1135559",
    "end": "1140640"
  },
  {
    "text": "tiny tiny box that by now really is only doing two things it's scheduling builds",
    "start": "1140640",
    "end": "1145840"
  },
  {
    "text": "basically when new builds are available it pushes them on a queue for the job process to run and it it processes the",
    "start": "1145840",
    "end": "1151559"
  },
  {
    "text": "build results so when a job is ready the Hub will get a notification and will it will do whatever is necessary to process",
    "start": "1151559",
    "end": "1158520"
  },
  {
    "text": "uh to post-process it build so now we had components that took",
    "start": "1158520",
    "end": "1164720"
  },
  {
    "text": "care of synchronizing and talking to GitHub when it build came in components that had handled the build locks itself",
    "start": "1164720",
    "end": "1170320"
  },
  {
    "text": "and components that handled all the build notifications that sent out emails campfire notifications uh IRC and all",
    "start": "1170320",
    "end": "1177559"
  },
  {
    "text": "that and that was pretty good and it was actually fairly easy for us to do because the responsibilities were",
    "start": "1177559",
    "end": "1182799"
  },
  {
    "text": "already very isolated so in the we ended up with very simple processes we had",
    "start": "1182799",
    "end": "1188240"
  },
  {
    "text": "very simple apps um that we could scale out independently we could scale we could add notifications process at any",
    "start": "1188240",
    "end": "1194679"
  },
  {
    "text": "time when the cues were backing up and the cues were always backing up the other meme of the distributed system",
    "start": "1194679",
    "end": "1200760"
  },
  {
    "text": "there's always a queue that's way too full um so that was actually we solved",
    "start": "1200760",
    "end": "1206919"
  },
  {
    "text": "that problem fairly easily it only took us a few weeks to break out these apps which was very",
    "start": "1206919",
    "end": "1213480"
  },
  {
    "text": "good it brought an unfortunate problem with it though uh our code is structured in a way that um all of the business",
    "start": "1213480",
    "end": "1221360"
  },
  {
    "text": "business logic isn't in a single Library it's called Trevis core and it's that",
    "start": "1221360",
    "end": "1227320"
  },
  {
    "text": "giant box in the middle and we have all these components around we have Trevis tasks uh we have the tasks which",
    "start": "1227320",
    "end": "1233480"
  },
  {
    "text": "basically involves notifications you know sending email all that stuff we have the locks processing we have the",
    "start": "1233480",
    "end": "1239120"
  },
  {
    "text": "API which uh basically our web user a web interface talks to we have Hub and we have gatekeeper which uh takes care",
    "start": "1239120",
    "end": "1245919"
  },
  {
    "text": "of synchronizing data with GitHub and handling when a build request comes in",
    "start": "1245919",
    "end": "1251200"
  },
  {
    "text": "and so we suddenly had this big ball of mud in the middle that all of our ABS",
    "start": "1251200",
    "end": "1256320"
  },
  {
    "text": "were depending on so scaling out these applications suddenly turned into a dependency problem because this core",
    "start": "1256320",
    "end": "1263600"
  },
  {
    "text": "Library contained all of our code all of the apps depended on it and to make sure that every app was still working after",
    "start": "1263600",
    "end": "1269720"
  },
  {
    "text": "we made a change to the code we had to make sure that we deploy all of them so there started to be a certain fear of",
    "start": "1269720",
    "end": "1275880"
  },
  {
    "text": "shipping a new feature because of this giant big ball of mud in the middle we paid dearly for a lot of architectural",
    "start": "1275880",
    "end": "1282640"
  },
  {
    "text": "decisions and what we what we wanted to do what we want to do instead is focus on very small dependencies and very",
    "start": "1282640",
    "end": "1288679"
  },
  {
    "text": "small responsibilities we have the responsibilities in our applications already they're just not in the code the",
    "start": "1288679",
    "end": "1294960"
  },
  {
    "text": "code is nicely separated in all but um it's still one GI library that all of",
    "start": "1294960",
    "end": "1300159"
  },
  {
    "text": "these applications use so it start to get into the way of us shipping things",
    "start": "1300159",
    "end": "1305240"
  },
  {
    "text": "us deploying things it just became a matter of confidence and that's why what we're",
    "start": "1305240",
    "end": "1311240"
  },
  {
    "text": "currently working on is getting this big ball of mud tearing it apart and um pushing all the depend all the",
    "start": "1311240",
    "end": "1317600"
  },
  {
    "text": "responsibilities and the dependencies for that into the small apps because they're already fairly isolated it's",
    "start": "1317600",
    "end": "1324080"
  },
  {
    "text": "just the code doesn't rep it doesn't reflect that very well and that was an interesting Revelation that modularity",
    "start": "1324080",
    "end": "1330039"
  },
  {
    "text": "is not just something that you have you know when you separate apps when you split out applications that are like",
    "start": "1330039",
    "end": "1335799"
  },
  {
    "text": "little services that handle that handle specific responsibilities modularity is something that is reflected in code but",
    "start": "1335799",
    "end": "1342679"
  },
  {
    "text": "it's also something that is reflected in your dependencies if you have one big ball of mud like we do you will have",
    "start": "1342679",
    "end": "1349279"
  },
  {
    "text": "less you will have less confidence to ship code because there's always a fear of that one change in that particular",
    "start": "1349279",
    "end": "1355240"
  },
  {
    "text": "code could affect something in this other other piece of code and static analysis all that kind of stuff",
    "start": "1355240",
    "end": "1361080"
  },
  {
    "text": "unfortunately would wouldn't even help with that so we're working hard on breaking all of these dependencies out",
    "start": "1361080",
    "end": "1368120"
  },
  {
    "text": "fully but it's a lot of work unfortunately",
    "start": "1368120",
    "end": "1373760"
  },
  {
    "text": "and yeah one of these components is worth a deeper look because it turned",
    "start": "1373760",
    "end": "1378880"
  },
  {
    "text": "out to be the biggest culprit of us being able to scale out because breaking",
    "start": "1378880",
    "end": "1384520"
  },
  {
    "text": "all of this stuff out for Travis I was not really just about having a distributed system it's um it was about",
    "start": "1384520",
    "end": "1390400"
  },
  {
    "text": "being able to grow we started you know we start approaching another the magnitude of builds per day and we could",
    "start": "1390400",
    "end": "1396559"
  },
  {
    "text": "see back at the time that the the the architecture that we had um wouldn't",
    "start": "1396559",
    "end": "1401600"
  },
  {
    "text": "really cut it logs in particular is the more interesting example because logs is",
    "start": "1401600",
    "end": "1407799"
  },
  {
    "text": "a component that handled log chunks as they come in from jobs it is uh traditionally it was one processor and",
    "start": "1407799",
    "end": "1414000"
  },
  {
    "start": "1414000",
    "end": "1481000"
  },
  {
    "text": "um that single process was basically responsible for updating the database",
    "start": "1414000",
    "end": "1419200"
  },
  {
    "text": "and uh sending lock chunks to Pusher and in our old architecture um updating the databas",
    "start": "1419200",
    "end": "1426640"
  },
  {
    "text": "base meant updating a single row and a column which contained the entire lock so whenever a new chunk came a new part",
    "start": "1426640",
    "end": "1433600"
  },
  {
    "text": "of the law came in we would basically State an uh issue an update statement into the database and basically",
    "start": "1433600",
    "end": "1440000"
  },
  {
    "text": "concatenate two thingss together it was rather efficient but it meant one thing",
    "start": "1440000",
    "end": "1445080"
  },
  {
    "text": "it meant one it meant that this particular part of the cat was depending on the order that the messages came in",
    "start": "1445080",
    "end": "1451880"
  },
  {
    "text": "it will always rely on that the messages came in into the right order when they were out of order suddenly you would",
    "start": "1451880",
    "end": "1457279"
  },
  {
    "text": "have a broken a broken build lock in the database and that was not very pleasant and the same was true for a web user",
    "start": "1457279",
    "end": "1464159"
  },
  {
    "text": "interface um Pusher again you know this is the other responsibility for the lock Library it sends the messages to Pusher",
    "start": "1464159",
    "end": "1471360"
  },
  {
    "text": "um which then in turn forwards them to all of the people that have basically a browser open and want to look at the",
    "start": "1471360",
    "end": "1477480"
  },
  {
    "text": "build lock as it tails in life and how that usually works is we",
    "start": "1477480",
    "end": "1483559"
  },
  {
    "text": "have this one lock processor and we had all of these single build jobs that were basically pushing messages onto Revan",
    "start": "1483559",
    "end": "1489320"
  },
  {
    "text": "mqq the lock processor would take them one at a time and uh would update the database",
    "start": "1489320",
    "end": "1495720"
  },
  {
    "text": "with it and would send would forward it to Pusher and this process actually did fairly well it at peak times it would do",
    "start": "1495720",
    "end": "1503919"
  },
  {
    "text": "it would process 100 messages per second it was uh slightly it was already scaled",
    "start": "1503919",
    "end": "1509080"
  },
  {
    "text": "out a little bit so it us is used it used threats to be able to do things concurrently but it was not doing very",
    "start": "1509080",
    "end": "1515919"
  },
  {
    "text": "well and 100 messages per second was started being our daily average of",
    "start": "1515919",
    "end": "1521559"
  },
  {
    "text": "messages that we did so we could see that there's going to be a ceiling that we're going to hit at some point and",
    "start": "1521559",
    "end": "1527039"
  },
  {
    "text": "even when we have a queue backing up because there's always a que backing up um when the the Q suddenly would get",
    "start": "1527039",
    "end": "1533720"
  },
  {
    "text": "full and we would have a burst in lock messages that processor would not be able to catch up with that with that burst so that started really turning",
    "start": "1533720",
    "end": "1540799"
  },
  {
    "text": "into a problem and the core problem of this of this entire process was that all of the",
    "start": "1540799",
    "end": "1548559"
  },
  {
    "start": "1544000",
    "end": "1617000"
  },
  {
    "text": "lock chunks were required to be in the exact order you know the lock processes",
    "start": "1548559",
    "end": "1553679"
  },
  {
    "text": "relied on the lock chunks to be in the exact order that they would appear basically in the in the built",
    "start": "1553679",
    "end": "1559679"
  },
  {
    "text": "output and relying on the correct order in this",
    "start": "1559679",
    "end": "1566559"
  },
  {
    "text": "case turned into a scalability issue for us if you need to rely on ordering um in",
    "start": "1566559",
    "end": "1572320"
  },
  {
    "text": "a distributed system you come into the Realms of coordinating uh coordinating several several nodes to find the right",
    "start": "1572320",
    "end": "1579039"
  },
  {
    "text": "ordering of an event but it turns out that we could actually do something",
    "start": "1579039",
    "end": "1585520"
  },
  {
    "text": "fairly easy about it and um unfortunately doing that had a lot of consequences for the rest of our system",
    "start": "1585520",
    "end": "1592640"
  },
  {
    "text": "but what we ended up doing is we made the ordering a property of the message itself so rather than relying on rabit",
    "start": "1592640",
    "end": "1599960"
  },
  {
    "text": "mq on the message to be pulled off the rabit mqq in the right order we basically",
    "start": "1599960",
    "end": "1605960"
  },
  {
    "text": "determined the order in the message itself every message got got a clock um basically a counter that would",
    "start": "1605960",
    "end": "1613360"
  },
  {
    "text": "identify its position in the in in the uh in the complete build lock and and",
    "start": "1613360",
    "end": "1619640"
  },
  {
    "start": "1617000",
    "end": "1639000"
  },
  {
    "text": "that made it fairly easy to assemble locks later we all we had these chunks suddenly we we started storing uh",
    "start": "1619640",
    "end": "1625520"
  },
  {
    "text": "instead of updating a single column in the database we would store the the small chunks and at any point in time we",
    "start": "1625520",
    "end": "1633039"
  },
  {
    "text": "could basically take these chunks and assem reassemble them again into a complete lock just based on that counter",
    "start": "1633039",
    "end": "1639720"
  },
  {
    "text": "and it was quite a beautiful beautifully simple thing because the job run a job Runner is always just responsible for a",
    "start": "1639720",
    "end": "1646679"
  },
  {
    "text": "single job so we could make sure that we don't have we wouldn't have any any conflicts between uh multiple job",
    "start": "1646679",
    "end": "1653480"
  },
  {
    "text": "Runners um updating the the same counter which is another horrible problem uh I I",
    "start": "1653480",
    "end": "1659120"
  },
  {
    "text": "would like not to have so we could keep this counter or a clock around and it",
    "start": "1659120",
    "end": "1665320"
  },
  {
    "text": "made it suddenly made things really really easy rights were actually faster because we only wrote single rights",
    "start": "1665320",
    "end": "1671200"
  },
  {
    "text": "which in postp which is our main database means you just append something to the transaction log and you're done",
    "start": "1671200",
    "end": "1676679"
  },
  {
    "text": "with it it's basically an append own right and that was fairly beautiful and I got",
    "start": "1676679",
    "end": "1685240"
  },
  {
    "start": "1681000",
    "end": "1776000"
  },
  {
    "text": "to say the ideas from this um and this is where the Renaissance fits in I got the ideas from this for this it sounds",
    "start": "1685240",
    "end": "1691760"
  },
  {
    "text": "like a fairly obvious idea but um the ideas for this are uh the inspiration for that came from a paper from Leslie",
    "start": "1691760",
    "end": "1698440"
  },
  {
    "text": "Lamport which was called um what was it called time clocks and The Ordering of",
    "start": "1698440",
    "end": "1703559"
  },
  {
    "text": "events and Lamport in that paper talks about using a clock um which in this case is can just be a",
    "start": "1703559",
    "end": "1711159"
  },
  {
    "text": "counter even a counter can be a clock in this case um and whenever a messages",
    "start": "1711159",
    "end": "1716519"
  },
  {
    "text": "passed from one from from a sender to a receiver uh the sender would increment would increment uh a clock so you could",
    "start": "1716519",
    "end": "1724559"
  },
  {
    "text": "when they when the receiver forwarded the message again to another process it would again increment that clock there's",
    "start": "1724559",
    "end": "1730039"
  },
  {
    "text": "some certain intri intricacies involved in that but the basic idea is just that as a message passes through a system",
    "start": "1730039",
    "end": "1737039"
  },
  {
    "text": "that counter would just be increment it it's a very simplest idea and we could actually use this this simple",
    "start": "1737039",
    "end": "1743600"
  },
  {
    "text": "interpretation of what Lampard wrote about in our setup we just needed this one clock um to be able to identify a",
    "start": "1743600",
    "end": "1749919"
  },
  {
    "text": "message later on so we didn't have to worry about um just leaving one locks processor running we now have five we",
    "start": "1749919",
    "end": "1757240"
  },
  {
    "text": "can process up to I think our Peaks are 300 messages per second and we don't have to worry about reassembling the",
    "start": "1757240",
    "end": "1763919"
  },
  {
    "text": "logs because that that simple counter is uh basically what allow us to do that at any point in time it had big effects on",
    "start": "1763919",
    "end": "1771039"
  },
  {
    "text": "what our web user interface was doing but it was worth the trouble and that's",
    "start": "1771039",
    "end": "1777039"
  },
  {
    "start": "1776000",
    "end": "1788000"
  },
  {
    "text": "another thing for distributed systems one of my manual learnings Simplicity if you can break if you can make things as",
    "start": "1777039",
    "end": "1782960"
  },
  {
    "text": "simple as possible they will be easier to scale and they will actually be easier to reason",
    "start": "1782960",
    "end": "1789360"
  },
  {
    "start": "1788000",
    "end": "1897000"
  },
  {
    "text": "about reasoning about um having to reason about the ordering of messages",
    "start": "1789480",
    "end": "1795399"
  },
  {
    "text": "you know Pro having to process them without having an exp explicit order is for me a matter of complexity and I",
    "start": "1795399",
    "end": "1800679"
  },
  {
    "text": "think Richi would actually agree in this case he he dislikes ordering having to",
    "start": "1800679",
    "end": "1806000"
  },
  {
    "text": "rely on order he calls it one of the complex uh one of the things that complex uh and he he does use complex um",
    "start": "1806000",
    "end": "1813480"
  },
  {
    "text": "software systems and so this little this little this little means just putting a",
    "start": "1813480",
    "end": "1818919"
  },
  {
    "text": "counter into the message allowed us to actually scale out this beautifully it forced us to update the user interface",
    "start": "1818919",
    "end": "1824480"
  },
  {
    "text": "because the user interface was also just taking these events and them to the dumb you know as builds were traing Al life",
    "start": "1824480",
    "end": "1833279"
  },
  {
    "text": "um but it worked fairly well it was we had two approaches originally in the web",
    "start": "1833279",
    "end": "1838880"
  },
  {
    "text": "user interface to do that one was just we would just buffer messages you know if messages were arriving out of order",
    "start": "1838880",
    "end": "1844559"
  },
  {
    "text": "we would just buffer buffer the the later ones wait for that message that was out of order to arrive and",
    "start": "1844559",
    "end": "1850760"
  },
  {
    "text": "eventually flush all of them out no matter e either after a timeout or when that message finally arrived so we could",
    "start": "1850760",
    "end": "1856960"
  },
  {
    "text": "make sure that the build lock would always be showing something and not not being delayed because a single message",
    "start": "1856960",
    "end": "1862240"
  },
  {
    "text": "was missing because sometimes a single message can actually just be one character when the build when a build is running and it just outputs a dot it can",
    "start": "1862240",
    "end": "1869120"
  },
  {
    "text": "just be that character that is actively running it made our user interface a little bit more complex but it made the",
    "start": "1869120",
    "end": "1877000"
  },
  {
    "text": "rest of the application a lot simpler and logs the logs processing is actually the one process where we broke out all",
    "start": "1877000",
    "end": "1882440"
  },
  {
    "text": "the dependencies and made it a a separated process and I think it has",
    "start": "1882440",
    "end": "1887639"
  },
  {
    "text": "like 30 lines of code in total which is a lot better than having it rely on a big ball of mud that shares the same",
    "start": "1887639",
    "end": "1895600"
  },
  {
    "start": "1897000",
    "end": "1990000"
  },
  {
    "text": "dependencies so in 2013 we're now doing 45,000 builds",
    "start": "1897399",
    "end": "1904320"
  },
  {
    "text": "and per day which is a pretty good a pretty good cut but we're still paying a lot of we're paying a lot of debt for",
    "start": "1904320",
    "end": "1910760"
  },
  {
    "text": "the initial decisions that we made the architectural ones but this brings me back to uncertainty these architectural",
    "start": "1910760",
    "end": "1916799"
  },
  {
    "text": "decisions they may look back in hindsight but you know when when Travis was buil who knew that eventually it",
    "start": "1916799",
    "end": "1923360"
  },
  {
    "text": "would go from just a few hundred builds per day to um almost well like 45,000",
    "start": "1923360",
    "end": "1928559"
  },
  {
    "text": "builds per day no one knew that it was an uncertain chapter no one no one realized that it would eventually turn",
    "start": "1928559",
    "end": "1933919"
  },
  {
    "text": "to um into a commercial platform as well um but we still somewhat pay the price",
    "start": "1933919",
    "end": "1940720"
  },
  {
    "text": "we still have to handle that we're a small team um but it's just how it goes",
    "start": "1940720",
    "end": "1946399"
  },
  {
    "text": "it's like I talk about these barriers I talk about barriers and treating external apis like you know your enemies",
    "start": "1946399",
    "end": "1952960"
  },
  {
    "text": "or treating them like like something that could break anytime but that's really hard to foresee when you have",
    "start": "1952960",
    "end": "1958600"
  },
  {
    "text": "when you have no control over you know the other parts of the system and it's really hard to you know make the",
    "start": "1958600",
    "end": "1964760"
  },
  {
    "text": "trade-off between do I put all of my time in handling all these failures and this uncertainty up front or do I do",
    "start": "1964760",
    "end": "1970919"
  },
  {
    "text": "that you know as the system evolves as I see it break in production because I you know I got to be honest as sad as it",
    "start": "1970919",
    "end": "1976919"
  },
  {
    "text": "makes me to see break things break at production I always get excited about it because I can uh usually I can learn",
    "start": "1976919",
    "end": "1983159"
  },
  {
    "text": "from it now we have better means to actually learn from these incidents and see where our system still needs to be",
    "start": "1983159",
    "end": "1988880"
  },
  {
    "text": "improved and one of the thing one of the things where we actually need to improve and this is one of the last last puzzles",
    "start": "1988880",
    "end": "1996840"
  },
  {
    "start": "1990000",
    "end": "2103000"
  },
  {
    "text": "that we still have is all of these components still use the the same database which you know if you if you",
    "start": "1996840",
    "end": "2002320"
  },
  {
    "text": "have some if you have a diagram where everything points to a single component that's not good because when that",
    "start": "2002320",
    "end": "2008519"
  },
  {
    "text": "component breaks uh everything will break and we just had that last week uh",
    "start": "2008519",
    "end": "2013600"
  },
  {
    "text": "our main database fell over because of an e ec2 issue and whole nothing R and",
    "start": "2013600",
    "end": "2021240"
  },
  {
    "text": "this is one of the things we're working on because our locks processing is now doing up to 300 messages per second at",
    "start": "2021240",
    "end": "2026320"
  },
  {
    "text": "peak times and that act actively affects um the reads from our API so we have this problem where we can",
    "start": "2026320",
    "end": "2034559"
  },
  {
    "text": "easily do 300 300 writes per second but being able doing that in our current",
    "start": "2034559",
    "end": "2039639"
  },
  {
    "text": "architecture affects you know people using our API people basically looking at our user interface and that's not",
    "start": "2039639",
    "end": "2045559"
  },
  {
    "text": "your grade user experience but it's another one of these learning experience at some point we will have we",
    "start": "2045559",
    "end": "2052398"
  },
  {
    "text": "will hopefully have multiple databases in this setup and it will all be a little bit easier to separate to isolate",
    "start": "2052399",
    "end": "2058158"
  },
  {
    "text": "these components from each other because if you look at locks processing it's a very isolated process it doesn't do anything doesn't do anything that's",
    "start": "2058159",
    "end": "2064679"
  },
  {
    "text": "related to the rest of the system locks process is basically just just put all these lock chunks into the database",
    "start": "2064679",
    "end": "2071040"
  },
  {
    "text": "later aggregate them into a big lock and then eventually archive them to S3 that's all those logs doing it doesn't",
    "start": "2071040",
    "end": "2077280"
  },
  {
    "text": "need to write through the main database so we have fairly easy means to um make our system even more",
    "start": "2077280",
    "end": "2084118"
  },
  {
    "text": "distributed but making by making it so we'll we'll have to deal with even more uncertainty about which which things",
    "start": "2084119",
    "end": "2090560"
  },
  {
    "text": "will break and we'll have to add more monitoring and we'll see more failures",
    "start": "2090560",
    "end": "2095679"
  },
  {
    "text": "but in the end we will have more simp it and these components by itself will be easier to manage on their own than as",
    "start": "2095679",
    "end": "2102119"
  },
  {
    "text": "one big ball of mud thank",
    "start": "2102119",
    "end": "2109970"
  },
  {
    "start": "2103000",
    "end": "2255000"
  },
  {
    "text": "[Applause]",
    "start": "2109970",
    "end": "2116920"
  },
  {
    "text": "you we have uh questions that came in so thanks for submitting questions if you have more questions you can either ask",
    "start": "2116920",
    "end": "2123200"
  },
  {
    "text": "them live or submit them through your app also remember to vote on app vote",
    "start": "2123200",
    "end": "2128280"
  },
  {
    "text": "for the talk the right answer is green by the way don't tell anybody I told you um so first question what is the best",
    "start": "2128280",
    "end": "2135079"
  },
  {
    "text": "book or resource for engineers and developers getting into distributed systems pitfalls patterns best practices",
    "start": "2135079",
    "end": "2143320"
  },
  {
    "text": "man let me know when you find it it's a really hard question yeah it is a hard question I I honestly um I have yet to",
    "start": "2143320",
    "end": "2150760"
  },
  {
    "text": "find one um there's not really a good book intro introducing this kind of stuff I found a lot of inspiration for",
    "start": "2150760",
    "end": "2158280"
  },
  {
    "text": "uh how well not even just how to they have a single algorithm to you know",
    "start": "2158280",
    "end": "2163760"
  },
  {
    "text": "handle increment encounters um but I got a lot of inspiration from web operations books to be quite honest but um I",
    "start": "2163760",
    "end": "2171760"
  },
  {
    "text": "actually started reading papers like Leslie lamport's paper there are some fairly nice reading lists on papers for",
    "start": "2171760",
    "end": "2178440"
  },
  {
    "text": "system Engineers out there I think if you Google them you will actually find a few block posts and these are actually",
    "start": "2178440",
    "end": "2185160"
  },
  {
    "text": "the best resources some papers are not very approachable but Leslie Lamport for",
    "start": "2185160",
    "end": "2190560"
  },
  {
    "text": "example he wrote he wrote a lot of stuff that guy I don't know he he wrote like 100 papers in and you go to uh you can",
    "start": "2190560",
    "end": "2198160"
  },
  {
    "text": "find his website under Microsoft research and he's got all his papers listed it's an enormous web page yeah uh",
    "start": "2198160",
    "end": "2206079"
  },
  {
    "text": "unfortunately there are papers like pxas you know that where you have distributed coordination withs are not very for well",
    "start": "2206079",
    "end": "2211440"
  },
  {
    "text": "at least for me they were not easy to understand but I don't know for me these papers were the best source of",
    "start": "2211440",
    "end": "2216960"
  },
  {
    "text": "inspirations far and one of one of the one of the actual things that about distributed systems in particular how to",
    "start": "2216960",
    "end": "2223000"
  },
  {
    "text": "handle you know when you have parallel processes that talk to each other over the network is um and there's going to",
    "start": "2223000",
    "end": "2228599"
  },
  {
    "text": "be a talk about this is communative replicated data types um it's basically data types that can be coordinated you",
    "start": "2228599",
    "end": "2236280"
  },
  {
    "text": "know like incrementing counter that can be coordinated by multiple nodes so that you always have an atomic Atomic in",
    "start": "2236280",
    "end": "2242720"
  },
  {
    "text": "increment in the end and there's a talk about that tomorrow I think and that stuff is trly interesting for",
    "start": "2242720",
    "end": "2248280"
  },
  {
    "text": "distributed systems because it gives you a good actually idea of the problems involved with distributed systems in general there's a also a page under MIT",
    "start": "2248280",
    "end": "2257319"
  },
  {
    "start": "2255000",
    "end": "2350000"
  },
  {
    "text": "distributed systems research group so if you Google dsrg uh papers there's a page with a",
    "start": "2257319",
    "end": "2264480"
  },
  {
    "text": "whole bunch of papers listed there that's a pretty good resource as well next question is are you using uh",
    "start": "2264480",
    "end": "2270680"
  },
  {
    "text": "circuit breakers kind of like Michael niggard circuit breakers in addition to timeouts and exponential back off and",
    "start": "2270680",
    "end": "2277520"
  },
  {
    "text": "not using circuit breakers yet but it is something that I I've been thinking about quite a lot uh I love Michael",
    "start": "2277520",
    "end": "2283480"
  },
  {
    "text": "nigar's book is actually one of the better books that you will find on actually running anything in production",
    "start": "2283480",
    "end": "2288960"
  },
  {
    "text": "run Michael nards release it it is if there's any book I would recommend it is that book um it mentions circuit",
    "start": "2288960",
    "end": "2295400"
  },
  {
    "text": "breakers and it mentions um bulkheads and circuit breakers is basically the",
    "start": "2295400",
    "end": "2301599"
  },
  {
    "text": "idea when you have too many failures hitting the good Hub API you trip the circuit and you will stop you stop",
    "start": "2301599",
    "end": "2307000"
  },
  {
    "text": "making requests and after a while you will basically uh you will start making slower requests again to the API seeing",
    "start": "2307000",
    "end": "2313280"
  },
  {
    "text": "you know gauging out if it's if it's working again and then at some point you can you can reconnect the circuit that",
    "start": "2313280",
    "end": "2319079"
  },
  {
    "text": "all of the requests will come through I thought about that a lot we don't have that yet it's one of these things where",
    "start": "2319079",
    "end": "2325200"
  },
  {
    "text": "I um where really would be nice because we have two apis that we hid like",
    "start": "2325200",
    "end": "2330480"
  },
  {
    "text": "hundreds of thousands of times per day and the unfortunate thing is that when",
    "start": "2330480",
    "end": "2336119"
  },
  {
    "text": "they when one of them goes down we will just keep hitting them we will just keep hitting them um a lot and the circuit",
    "start": "2336119",
    "end": "2342200"
  },
  {
    "text": "breaker would definitely be handy it is something I want to implement for that particular code to isolate these failers",
    "start": "2342200",
    "end": "2347800"
  },
  {
    "text": "a little bit more definitely now there's a third question but before I ask it I have to give a little intro to it",
    "start": "2347800",
    "end": "2353440"
  },
  {
    "text": "because uh one of the problems in distributed systems is Network partitions now partition can be the failure of a network it's more generally",
    "start": "2353440",
    "end": "2361160"
  },
  {
    "text": "the um the lack of an arrival of a message so a message just might not get to where it's going or there's also U",
    "start": "2361160",
    "end": "2368920"
  },
  {
    "text": "the fact that a message could be duplicated depending on what protocols you're using so this question was asked",
    "start": "2368920",
    "end": "2374400"
  },
  {
    "text": "three times how do you handle multiple processes which need to share the same",
    "start": "2374400",
    "end": "2380400"
  },
  {
    "text": "temporal clock uh thankfully we would don't have that problem it makes it it makes it fairly",
    "start": "2380400",
    "end": "2386960"
  },
  {
    "text": "easy I was very happy about that it's um when I mentioned these the these clocks that we have it's always only one",
    "start": "2386960",
    "end": "2393880"
  },
  {
    "text": "process that increments the clock that's uh we've shaped off that problem so we",
    "start": "2393880",
    "end": "2401040"
  },
  {
    "text": "we don't we don't need to have a distributed a distributed clock that needs to be incremented um it makes it a",
    "start": "2401040",
    "end": "2407160"
  },
  {
    "text": "lot easier to implement that goes to your Simplicity yes that's all well let me refresh um oh",
    "start": "2407160",
    "end": "2415640"
  },
  {
    "text": "wow more questions okay wouldn't it be obvious to use a nosql database instead and I didn't ask that",
    "start": "2415640",
    "end": "2423000"
  },
  {
    "text": "question um to be quite honest I'm even though I I wrote a little book on react",
    "start": "2423280",
    "end": "2428880"
  },
  {
    "text": "I'm fairly fond of postgress it's an awesome database I'm always surprised that it's actually we can throw 300",
    "start": "2428880",
    "end": "2434880"
  },
  {
    "text": "rights per second at it and it's it's a 32bit 1.7 GB ec2",
    "start": "2434880",
    "end": "2440319"
  },
  {
    "text": "instance and it can still handle that but um to answer the question yes it would",
    "start": "2440319",
    "end": "2447240"
  },
  {
    "text": "be obvious it would actually be obvious to take our lock process and store the chunk be be able to store the chunks in",
    "start": "2447240",
    "end": "2453880"
  },
  {
    "text": "for example Rak instead of postgress but the beauty of isolating the locks processing and isolating the separate",
    "start": "2453880",
    "end": "2460400"
  },
  {
    "text": "Services is that it doesn't really matter what we use we can we can swap it out without affecting the rest of the",
    "start": "2460400",
    "end": "2465720"
  },
  {
    "text": "system that's the important about part about modularity you can I can change the data store for one system and it",
    "start": "2465720",
    "end": "2471599"
  },
  {
    "text": "doesn't really affect the rest of the system that's where I would like to go and eventually um maybe we want we we're",
    "start": "2471599",
    "end": "2478200"
  },
  {
    "text": "going to use a a nosql data store but in the end it's just another distributed system we have to take care of and it's",
    "start": "2478200",
    "end": "2483640"
  },
  {
    "text": "going to bring more problems so it's it's always a trade-off",
    "start": "2483640",
    "end": "2489079"
  },
  {
    "start": "2488000",
    "end": "2606000"
  },
  {
    "text": "what kind of monitoring are you doing and how do you correlate events across the many components oh it's a painful",
    "start": "2489079",
    "end": "2496119"
  },
  {
    "text": "question um it's a whole talk in itself yeah it is a whole talk in itself um so we track a lot of metrics",
    "start": "2496119",
    "end": "2503040"
  },
  {
    "text": "in our system as events happen well we're using that similar to statsd which is basically a process where you all of",
    "start": "2503040",
    "end": "2508839"
  },
  {
    "text": "your process can send metrics and it's going to forward them to a system like graphite or something that we use is",
    "start": "2508839",
    "end": "2514440"
  },
  {
    "text": "called libr metrics which is a hosted service for metrics um we basically aggregate all these",
    "start": "2514440",
    "end": "2520280"
  },
  {
    "text": "metrics in our applications and we also have built a custom collector that pull that puls things like rabbit mq or a",
    "start": "2520280",
    "end": "2526800"
  },
  {
    "text": "database for information like when was the last build running um how many messages or like how old is the queue",
    "start": "2526800",
    "end": "2533720"
  },
  {
    "text": "how many messages are currently backing up in the queue because usually when our Q backs up there's a problem somewhere",
    "start": "2533720",
    "end": "2540760"
  },
  {
    "text": "and um how we track the events through system is actually a problem that we currently uh that I'm currently working",
    "start": "2540760",
    "end": "2546920"
  },
  {
    "text": "on fixing is um we're not doing a really good job of that right now the problem is that um",
    "start": "2546920",
    "end": "2554240"
  },
  {
    "text": "well what what what we're fixing on what we're working towards is that we each request basically gets assigned",
    "start": "2554240",
    "end": "2559440"
  },
  {
    "text": "something as simple as a uu ID it gets assigned a u ID and then you can use that uu ID to trace in the locks what it",
    "start": "2559440",
    "end": "2566359"
  },
  {
    "text": "is what is happening in this system we don't have a very um a very elaborate",
    "start": "2566359",
    "end": "2572280"
  },
  {
    "text": "tool that allows us to visualize all that stuff uh it's more about just being able to retrace these things because we",
    "start": "2572280",
    "end": "2578599"
  },
  {
    "text": "don't usually we we don't spend a lot of time looking at our locks we just need to investigate when a customer says us",
    "start": "2578599",
    "end": "2583920"
  },
  {
    "text": "hey I pushed something to GitHub and no build was running on travisci and we need to be able to trace that step back",
    "start": "2583920",
    "end": "2590359"
  },
  {
    "text": "to did actually did a web notification actually arrive and then what happened to it and that is one of our biggest",
    "start": "2590359",
    "end": "2596640"
  },
  {
    "text": "problems and the solution is again as simple as assigning a uid to the request",
    "start": "2596640",
    "end": "2602040"
  },
  {
    "text": "and then using that U ID to trace everything through the system",
    "start": "2602040",
    "end": "2607280"
  },
  {
    "start": "2606000",
    "end": "2723000"
  },
  {
    "text": "um how do you test your own system for these kinds of problems assuming St problems another painful",
    "start": "2607280",
    "end": "2613880"
  },
  {
    "text": "question um we don't we run it we run it in production that is how we test",
    "start": "2613880",
    "end": "2621079"
  },
  {
    "text": "it um well you might have already there's a question about thoughts on",
    "start": "2621920",
    "end": "2627880"
  },
  {
    "text": "replacing the central database you kind of covered that yeah what is gatekeeper doing um gatekeeper is um it's I it's",
    "start": "2627880",
    "end": "2637920"
  },
  {
    "text": "inspired from uh Ghostbusters but uh gatekeeper is basically when when a",
    "start": "2637920",
    "end": "2642960"
  },
  {
    "text": "request comes when we get a notification from GitHub that a new commit was made we basically push that onto a queue and",
    "start": "2642960",
    "end": "2649119"
  },
  {
    "text": "gatekeeper takes these commits and turns them into something that is runable by the system it basically takes a Json",
    "start": "2649119",
    "end": "2654880"
  },
  {
    "text": "payload uh that contains information about a new commit or about a serious commit and turns it into a set of a set",
    "start": "2654880",
    "end": "2661760"
  },
  {
    "text": "of buildt jobs that can then be run uh on any of the of our of our job schedulers",
    "start": "2661760",
    "end": "2668520"
  },
  {
    "text": "and it also does things like synchronizing user data with GitHub but the main purpose is to be the main",
    "start": "2668520",
    "end": "2674280"
  },
  {
    "text": "entrance point for uh handling build requests and one last question are you",
    "start": "2674280",
    "end": "2681559"
  },
  {
    "text": "using AWS as your exclusive platform uh we're not we're our entire app stack",
    "start": "2681559",
    "end": "2687920"
  },
  {
    "text": "runs on AWS it runs on Heroku um um which is always somewhat surprising to",
    "start": "2687920",
    "end": "2694040"
  },
  {
    "text": "people but um we're actually quite easy we're quite happy with with it um our application stack runs on ec2 um but our",
    "start": "2694040",
    "end": "2702079"
  },
  {
    "text": "the stuff that is running the tests which is now 550 test servers uh build servers uh",
    "start": "2702079",
    "end": "2709079"
  },
  {
    "text": "that runs on a different virtualization architecture that actually runs in dedicated Hardware but it's still using uh we",
    "start": "2709079",
    "end": "2715680"
  },
  {
    "text": "still have an API to talk to to talk to that infrastructure which is kind of Handy it's better than having to do all",
    "start": "2715680",
    "end": "2720880"
  },
  {
    "text": "the their provisioning manually that was all the questions that came in anybody else have uh",
    "start": "2720880",
    "end": "2728280"
  },
  {
    "start": "2723000",
    "end": "2901000"
  },
  {
    "text": "anything back there um so supposed to rabbit mq is",
    "start": "2728280",
    "end": "2733520"
  },
  {
    "text": "also fairly vable component of yours um what do you do to overcome that uh so",
    "start": "2733520",
    "end": "2741920"
  },
  {
    "text": "the question is um Revit mq is maybe a fairly well a breakable component in our",
    "start": "2741920",
    "end": "2747559"
  },
  {
    "text": "system what we do to fix that um yes Revit mq is a component that used to break a lot but the problem with that is",
    "start": "2747559",
    "end": "2755200"
  },
  {
    "text": "our initial problem I a lot about rmq um but I think it's more about my",
    "start": "2755200",
    "end": "2760559"
  },
  {
    "text": "dislike of amqp it is a very complex protocol riim Q itself",
    "start": "2760559",
    "end": "2766079"
  },
  {
    "text": "um um actually worked fairly well for us but um the caveat was that we as we run",
    "start": "2766079",
    "end": "2774160"
  },
  {
    "text": "our stuff on Heroku we use a lot of other add-on Services basically that other people host for us which is",
    "start": "2774160",
    "end": "2779960"
  },
  {
    "text": "sometimes handy sometimes painful with Revit mq it was initially was very painful because what these providers",
    "start": "2779960",
    "end": "2785599"
  },
  {
    "text": "usually do is they set up a cluster um which a lot of customers then use and",
    "start": "2785599",
    "end": "2791280"
  },
  {
    "text": "we've been we we need we rely on getting consistent throughput through rabit mq if we just see a dip uh a dip and",
    "start": "2791280",
    "end": "2798839"
  },
  {
    "text": "throughput performance in rabit mq our messages will start queuing up immediately because we rely on that and",
    "start": "2798839",
    "end": "2803880"
  },
  {
    "text": "in a multi-tenant system RAB mq unfortunately is built in a way that if you have one producer that is producing",
    "start": "2803880",
    "end": "2809359"
  },
  {
    "text": "a lot of messages it can block other other producers or even consumers in the system so whenever some other customer",
    "start": "2809359",
    "end": "2816599"
  },
  {
    "text": "on that that classer came up and started sending like thousands of messages per second",
    "start": "2816599",
    "end": "2821800"
  },
  {
    "text": "that immediately would affect us as soon as we moved to a dedicated cluster that we just had to ourselves rmq was not has",
    "start": "2821800",
    "end": "2828400"
  },
  {
    "text": "not been a problem anymore and the high availability features in rmq 3.0 are actually quite nice it's still I still",
    "start": "2828400",
    "end": "2836440"
  },
  {
    "text": "have my doubts about running it in production it's uh it has it has its fors basically that make it a little bit",
    "start": "2836440",
    "end": "2842520"
  },
  {
    "text": "hard to handle but when it's running it XL runs fairly well they did a lot to make it easier to operate a Revit mq in",
    "start": "2842520",
    "end": "2849280"
  },
  {
    "text": "production and but in the end to us it doesn't really matter anymore whether we use Revit mq or if we use something else",
    "start": "2849280",
    "end": "2855559"
  },
  {
    "text": "because we're not really tied to it anymore know removing this removing the order the requirement of ordering of you",
    "start": "2855559",
    "end": "2861960"
  },
  {
    "text": "know implicit ordering as the messages are sent through the bus through the message bus allowed us to think about actually using other",
    "start": "2861960",
    "end": "2869240"
  },
  {
    "text": "message buses in the future at some point but so far I don't think Revit mq",
    "start": "2869240",
    "end": "2874960"
  },
  {
    "text": "will it will probably allow easily allow us to do a thousand messages per second and we don't have to worry about that",
    "start": "2874960",
    "end": "2881359"
  },
  {
    "text": "which I like to not worry about these things for a while so makes it a little bit",
    "start": "2881359",
    "end": "2887079"
  },
  {
    "text": "easier okay with that uh it's lunchtime so everybody thank our speaker",
    "start": "2887079",
    "end": "2892910"
  },
  {
    "text": "[Applause]",
    "start": "2892910",
    "end": "2898429"
  }
]