[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "text": "[Music]",
    "start": "970",
    "end": "7838"
  },
  {
    "text": "good morning everyone thanks for coming along to this session um i work as a developer advocate conference uh one of the companies who",
    "start": "13200",
    "end": "19520"
  },
  {
    "text": "contribute to the open source apache kafka project um and i want to talk today about this idea of kafka",
    "start": "19520",
    "end": "25359"
  },
  {
    "text": "as a platform so not just like what is kafka in the kind of the usual these are the different components and",
    "start": "25359",
    "end": "31359"
  },
  {
    "text": "all that kind of stuff that you kind of sometimes lose the context for like well why do we even need it",
    "start": "31359",
    "end": "36480"
  },
  {
    "text": "in the first place so i want to take a step back and think about why do we even need it in the first place and then explain",
    "start": "36480",
    "end": "43760"
  },
  {
    "text": "the different pieces in it so it's not going to be a super technical deep dive into i'm going to use the word api so",
    "start": "43760",
    "end": "50079"
  },
  {
    "text": "that won't be too scary but we're just going to lay out the landscape of what actually kafka is",
    "start": "50079",
    "end": "55600"
  },
  {
    "text": "what it does what it's really good at um so let's get started with that i'm off on",
    "start": "55600",
    "end": "61840"
  },
  {
    "text": "twitter so that's me in the bottom right of the screen there if you want to follow me on twitter it's always good to get tweets",
    "start": "61840",
    "end": "66880"
  },
  {
    "text": "tell me if you liked it tell me if you didn't like it it's always good to get that feedback also so what kafka",
    "start": "66880",
    "end": "73520"
  },
  {
    "text": "does for us is it actually gives us a new category of software that treats",
    "start": "73520",
    "end": "79439"
  },
  {
    "text": "something called events as first class objects and events are actually all around us and",
    "start": "79439",
    "end": "86640"
  },
  {
    "start": "82000",
    "end": "345000"
  },
  {
    "text": "events model the real world you work with data almost all of you in",
    "start": "86640",
    "end": "92000"
  },
  {
    "text": "your day-to-day jobs and you'll have systems that have data and you store data in databases and you store database star",
    "start": "92000",
    "end": "98880"
  },
  {
    "text": "data in message queues and no sql stores and all of this data that you work with",
    "start": "98880",
    "end": "104320"
  },
  {
    "text": "almost all of it started life as an event and events are super powerful because",
    "start": "104320",
    "end": "109439"
  },
  {
    "text": "events tell us that something happened and they tell us what happened and events",
    "start": "109439",
    "end": "114880"
  },
  {
    "text": "happen continually events are unbounded so if you think about the kind of things",
    "start": "114880",
    "end": "120719"
  },
  {
    "text": "that happen in your businesses that happen in your domain of expertise i'm sure you can easily think of",
    "start": "120719",
    "end": "126479"
  },
  {
    "text": "examples of events or if you can't easily and that's fine because easily it's kind of like a",
    "start": "126479",
    "end": "131599"
  },
  {
    "text": "loaded word to be using but if you think carefully about the data with which you work",
    "start": "131599",
    "end": "136959"
  },
  {
    "text": "you can quite easily trace it back to an event so an example of human generated events",
    "start": "136959",
    "end": "143200"
  },
  {
    "text": "someone walks into a shop or nowadays they probably click on a website instead and they order something or they go and",
    "start": "143200",
    "end": "149120"
  },
  {
    "text": "buy something and that's an event it happened at a point in time and something happened it was a purchase and",
    "start": "149120",
    "end": "154400"
  },
  {
    "text": "what happened well they bought themselves as thick as bad or we moved some stock around a",
    "start": "154400",
    "end": "159519"
  },
  {
    "text": "warehouse something happened stock was moved it happened at a point in time and what happened well it was this piece of",
    "start": "159519",
    "end": "166000"
  },
  {
    "text": "stock and it moved from this warehouse to that warehouse and these are events",
    "start": "166000",
    "end": "171280"
  },
  {
    "text": "and you also get machine generated events you get our iot devices which are continually logging readings and",
    "start": "171280",
    "end": "177360"
  },
  {
    "text": "sending information about the temperature of the humidity you get network events our firewalls and",
    "start": "177360",
    "end": "183040"
  },
  {
    "text": "our routers and like how many packets have i just processed and how many uh inbound things that just happened on this particular part",
    "start": "183040",
    "end": "189599"
  },
  {
    "text": "our applications are also a source of events when your application throws an error",
    "start": "189599",
    "end": "195519"
  },
  {
    "text": "then that is an event it happened at a point in time what's happened there was an error and i'll say something happened it",
    "start": "195519",
    "end": "202239"
  },
  {
    "text": "was an error what happened well here is my stack trace so almost all the data that we work with",
    "start": "202239",
    "end": "209519"
  },
  {
    "text": "starts life as an event and we can use these events to build powerful applications we can drive",
    "start": "209519",
    "end": "215760"
  },
  {
    "text": "applications that respond to something happening we can use events to actually build a picture of what",
    "start": "215760",
    "end": "221200"
  },
  {
    "text": "happens in our business beyond what we actually get if we simply capture the summation of it the final state so",
    "start": "221200",
    "end": "228319"
  },
  {
    "text": "if you look at what's in that warehouse at the moment you see where we have a pallet of stick of ours over here",
    "start": "228319",
    "end": "234080"
  },
  {
    "text": "well how did they get over there when did it move when it moved that was the event so events give us this",
    "start": "234080",
    "end": "240319"
  },
  {
    "text": "functionality and you can roll events up into states you can still have your databases and your nosql stores and all",
    "start": "240319",
    "end": "246080"
  },
  {
    "text": "the rest of it but what you can't have is what events give you which is this view of things happening",
    "start": "246080",
    "end": "251200"
  },
  {
    "text": "over time so these events are everywhere and these events are super powerful and",
    "start": "251200",
    "end": "256639"
  },
  {
    "text": "we can build applications we can build analytics around them and we need something to be able to do",
    "start": "256639",
    "end": "262240"
  },
  {
    "text": "that on we can use existing technologies to kind of like mimic certain behaviors with events",
    "start": "262240",
    "end": "268639"
  },
  {
    "text": "and in latin looking like recent years some technologies have tried to play catch up again like bolt on certain bits",
    "start": "268639",
    "end": "274720"
  },
  {
    "text": "of functionality but you can kind of tell when you're using something which is actually designed from the outset to work with a specific",
    "start": "274720",
    "end": "281199"
  },
  {
    "text": "principle and in this case we want to work with events as a first class object so here is our humble event it could be",
    "start": "281199",
    "end": "288479"
  },
  {
    "text": "a click on a website it could be an order being placed it could be a reading from our iot devices it could be",
    "start": "288479",
    "end": "294000"
  },
  {
    "text": "anything that is an event and events happen continually unbounded over time",
    "start": "294000",
    "end": "300639"
  },
  {
    "text": "so we have lots of different events and events can happen at a rate of 10 000 per second they",
    "start": "300639",
    "end": "307120"
  },
  {
    "text": "could happen at a rate of two per day there's nothing proscribed in like the rate at which they have to occur for it to be",
    "start": "307120",
    "end": "313280"
  },
  {
    "text": "an event stream things happen continually these are events and we want to work",
    "start": "313280",
    "end": "319039"
  },
  {
    "text": "with them and we want a system with which we're going to be able to work with them powerfully so let's say these events we're going to",
    "start": "319039",
    "end": "326160"
  },
  {
    "text": "give them a key and a value we don't want to be too prescriptive and kind of like what are we going to store and how we're",
    "start": "326160",
    "end": "331840"
  },
  {
    "text": "going to store it and stuff like that because then we start to impose our own design decisions upon it so we're just going to say we'll store",
    "start": "331840",
    "end": "338639"
  },
  {
    "text": "a key and a value that's going to represent that event where are we going to start it well if",
    "start": "338639",
    "end": "344639"
  },
  {
    "text": "we take these events and as i say they happen over time we can create a log",
    "start": "344639",
    "end": "349759"
  },
  {
    "start": "345000",
    "end": "473000"
  },
  {
    "text": "of them so we're going gonna say well an event happens we'll write it to the log another event happens we'll write it to",
    "start": "349759",
    "end": "355919"
  },
  {
    "text": "the log and what we'll do is we'll append these events to the log each time they",
    "start": "355919",
    "end": "361600"
  },
  {
    "text": "happen and as i say that could be ten thousand a second there could be two a day as an event happens we append it to the",
    "start": "361600",
    "end": "368479"
  },
  {
    "text": "log now this log is immutable it's append only and it is immutable",
    "start": "368479",
    "end": "376319"
  },
  {
    "text": "meaning you cannot go back and change something because we're talking about time here you can't rewind time much as",
    "start": "376319",
    "end": "382240"
  },
  {
    "text": "you may wish to you may have an argument with someone and say something that you really wish you hadn't but once those words are uttered",
    "start": "382240",
    "end": "387840"
  },
  {
    "text": "they have been spoken and what you said has been said you can't rewind time and go back and",
    "start": "387840",
    "end": "393120"
  },
  {
    "text": "change it all you can do is try and counteract it you can go and apologize and try and undo",
    "start": "393120",
    "end": "399199"
  },
  {
    "text": "what you said so you could append another event to this log that kind of says like i'm really sorry i didn't mean to say that",
    "start": "399199",
    "end": "406160"
  },
  {
    "text": "but you can't actually go back and change what has happened so if we're talking about apache kafka",
    "start": "406160",
    "end": "411440"
  },
  {
    "text": "here they have this guarantee that it provides that the logs are immutable which actually gives us",
    "start": "411440",
    "end": "417360"
  },
  {
    "text": "some super powerful concepts on which to build applications because you're now guaranteed that the data that you read has not changed",
    "start": "417360",
    "end": "424160"
  },
  {
    "text": "since it was written to that log so this is a beautiful um event log it is",
    "start": "424160",
    "end": "429919"
  },
  {
    "text": "append only and we send the events to the end of it as they occur",
    "start": "429919",
    "end": "435520"
  },
  {
    "text": "this log file it's not like a log file if you're thinking like log4j and like log.txt and stuff like that",
    "start": "435520",
    "end": "441759"
  },
  {
    "text": "there are related concepts but here we're talking about a log file kafka itself stores it on disk is like",
    "start": "441759",
    "end": "447280"
  },
  {
    "text": "its own internal object and that's kind of like an implementation kind of like the guts of it like the bits and the bytes and stuff",
    "start": "447280",
    "end": "453520"
  },
  {
    "text": "like that and as developers we want to kind of abstract that a little bit because we need a way to actually",
    "start": "453520",
    "end": "459520"
  },
  {
    "text": "arrange our data so we're going to take this concept of a log which kind of does actually",
    "start": "459520",
    "end": "464560"
  },
  {
    "text": "exist you can dig down into it and actually see it within the system but we're going to work with a log and we're going to say",
    "start": "464560",
    "end": "470639"
  },
  {
    "text": "we're going to carve it up into entities which describe the different bits of data that we've got",
    "start": "470639",
    "end": "475919"
  },
  {
    "start": "473000",
    "end": "573000"
  },
  {
    "text": "we call these topics so we could say we're going to create a topic and on this topic we're going to write all of the events to do with clicks on",
    "start": "475919",
    "end": "482479"
  },
  {
    "text": "our website every time we get a click event we write that to the clicks topic every time someone places an order",
    "start": "482479",
    "end": "488479"
  },
  {
    "text": "we write that to the order topic we can also capture information in kafka",
    "start": "488479",
    "end": "493520"
  },
  {
    "text": "topics about things which don't necessarily feel like events and if we talk about clicks and orders and once we understand",
    "start": "493520",
    "end": "499759"
  },
  {
    "text": "this idea of an event has been something that happened and what happened we kind of quite easily understand that well we're",
    "start": "499759",
    "end": "506000"
  },
  {
    "text": "going to capture those that gives us a stream of things that have happened and if you're from an analytics background like myself",
    "start": "506000",
    "end": "512000"
  },
  {
    "text": "these are facts okay and facts happen continually there are events but on here we have",
    "start": "512000",
    "end": "518399"
  },
  {
    "text": "something saying more customers and that's analytics terms is like reference data lookup data but that also makes a lot of",
    "start": "518399",
    "end": "526160"
  },
  {
    "text": "sense to capture in a kafka topic because if you think about when a customer gets created in your",
    "start": "526160",
    "end": "532880"
  },
  {
    "text": "system that's an event when the customer moves house that's an event when the customer changes their",
    "start": "532880",
    "end": "539440"
  },
  {
    "text": "email address that's an event and if you capture those events you can actually replay them",
    "start": "539440",
    "end": "544560"
  },
  {
    "text": "to find out the current state for where does this customer currently currently live and what is their email",
    "start": "544560",
    "end": "550000"
  },
  {
    "text": "address so you capture those events so not only can you find out what's their current state you can also look back and say well when",
    "start": "550000",
    "end": "556240"
  },
  {
    "text": "did that change or how many times have they changed their email address and those events themselves are also useful",
    "start": "556240",
    "end": "562080"
  },
  {
    "text": "so basically when something happens you model it as an event because it is an event and you can capture that",
    "start": "562080",
    "end": "567920"
  },
  {
    "text": "on kafka topics these topics we're going to carve up one more time in something called",
    "start": "567920",
    "end": "573200"
  },
  {
    "start": "573000",
    "end": "655000"
  },
  {
    "text": "partitions so within each topic you can have one or more",
    "start": "573200",
    "end": "578240"
  },
  {
    "text": "partitions you don't have to partition it you could just have a single partition and there are a couple of use cases",
    "start": "578240",
    "end": "584000"
  },
  {
    "text": "where that's a very good idea where you'll want to do that most of the time though",
    "start": "584000",
    "end": "589040"
  },
  {
    "text": "you'll want to partition your topics because as we see we're going to see later partitions",
    "start": "589040",
    "end": "594160"
  },
  {
    "text": "are our unit of scale and it's good idea to think about it up front rather than later on the reason for that",
    "start": "594160",
    "end": "601120"
  },
  {
    "text": "is that within a partition kafka gives us the guarantee of strict ordering so there's that",
    "start": "601120",
    "end": "608000"
  },
  {
    "text": "guarantee of immutability kafka guarantees that within the partition of a topic the messages as they arrive",
    "start": "608000",
    "end": "615200"
  },
  {
    "text": "on that topic that order is guaranteed to remain the same when we read it out",
    "start": "615200",
    "end": "620240"
  },
  {
    "text": "of the topic and once you start re-partitioning your topic and say well we'll start off with one partition because i",
    "start": "620240",
    "end": "625600"
  },
  {
    "text": "kind of i'm just like trying this thing out and then you say oh turns out we need to scale we need to partition that topic into like 10",
    "start": "625600",
    "end": "632320"
  },
  {
    "text": "when you do that repartitioning your ordering guarantees get lost so you'd have to manage that in",
    "start": "632320",
    "end": "637360"
  },
  {
    "text": "your application itself and it gets a bit more complicated so thinking about how many partitions you might want up front and there's",
    "start": "637360",
    "end": "642880"
  },
  {
    "text": "various ballpark figures which useful to work with is a really good idea so we've got a",
    "start": "642880",
    "end": "648240"
  },
  {
    "text": "strict ordering of messages within a partition within a topic now let's think about how we get our",
    "start": "648240",
    "end": "654320"
  },
  {
    "text": "data in and out of kafka so kafka if you've heard of the term pub",
    "start": "654320",
    "end": "659519"
  },
  {
    "start": "655000",
    "end": "1324000"
  },
  {
    "text": "sub it lets you do this we can publish data into it we can subscribe to data from it but some really",
    "start": "659519",
    "end": "664800"
  },
  {
    "text": "important differences from other pub sub systems that you may be familiar with in terms of the published side of things",
    "start": "664800",
    "end": "670880"
  },
  {
    "text": "in terms of getting data in to kafka we use the producer api",
    "start": "670880",
    "end": "676000"
  },
  {
    "text": "so the producer api it does what we said earlier which is an event happens in the world we append it to the log the producer api",
    "start": "676000",
    "end": "683760"
  },
  {
    "text": "says i'll take that message that you've given me and i will write it to the given topic it will handle where is the broker the",
    "start": "683760",
    "end": "690800"
  },
  {
    "text": "network protocol which partition am i going to write it to it will do all of that for you and you",
    "start": "690800",
    "end": "696399"
  },
  {
    "text": "can say well i'm going to use a particular set of codes so this is an example of using the go clients there's go there's java there's python",
    "start": "696399",
    "end": "702399"
  },
  {
    "text": "there's c c plus plus stuff there's a whole ton of different languages that are supported with the clients",
    "start": "702399",
    "end": "707760"
  },
  {
    "text": "and so you say i've got my data here there's my broker and you can say we'll just send it to that topic and you can also override",
    "start": "707760",
    "end": "714480"
  },
  {
    "text": "things like we'll give it this time stamp or send it to this particular partition but by default it's pretty simple",
    "start": "714480",
    "end": "720240"
  },
  {
    "text": "here's my message here's the optional key send it to that particular topic",
    "start": "720240",
    "end": "725680"
  },
  {
    "text": "now we talked about keys very briefly earlier on and keys come into real importance when",
    "start": "725680",
    "end": "730800"
  },
  {
    "text": "it comes to partitions because the key defines which partition by default a message",
    "start": "730800",
    "end": "736639"
  },
  {
    "text": "is going to go to if you don't give your message a key you don't have to then the messages",
    "start": "736639",
    "end": "742000"
  },
  {
    "text": "you'll just get round rubbing across all of the available partitions so you get a nice even distribution of messages but you don't get any guarantee",
    "start": "742000",
    "end": "749760"
  },
  {
    "text": "of which partition your message is going to end up in so that's not always",
    "start": "749760",
    "end": "754880"
  },
  {
    "text": "what you want so a lot of the time you say well my data has got like specific um need to be in a certain partition",
    "start": "754880",
    "end": "761920"
  },
  {
    "text": "either for this strict uh guarantee of ordering or because when we process this data and we",
    "start": "761920",
    "end": "768560"
  },
  {
    "text": "want to do so in parallel and i'll talk about that in a moment we want to make sure that each instance that's processing it",
    "start": "768560",
    "end": "774720"
  },
  {
    "text": "gets all of the data for a particular instance of an entity so if we're processing order data let's",
    "start": "774720",
    "end": "780560"
  },
  {
    "text": "say the the topic that we're showing on the screen here with four partitions it's information that orders and an",
    "start": "780560",
    "end": "785600"
  },
  {
    "text": "order will get created it will get uh marketers pending it or gets paid it will get shipped it will",
    "start": "785600",
    "end": "790880"
  },
  {
    "text": "get completed and you have all these different events related to a particular order and you'll want to make sure that",
    "start": "790880",
    "end": "797279"
  },
  {
    "text": "all of those events for a given order are on the same partition otherwise applications that",
    "start": "797279",
    "end": "803279"
  },
  {
    "text": "are processing that data in parallel are going to end up kind of getting data for different orders and won't be able to guarantee that they're",
    "start": "803279",
    "end": "809040"
  },
  {
    "text": "processing all of that data for a given order so we need to make sure that when we write data into kafka if we have that",
    "start": "809040",
    "end": "816639"
  },
  {
    "text": "kind of requirements to our data that it has to be on a particular partition then we need to set the key for the",
    "start": "816639",
    "end": "822079"
  },
  {
    "text": "message we set the key to the order id in this example and then it takes a hash of that order",
    "start": "822079",
    "end": "827680"
  },
  {
    "text": "id and it will always end up on the same partition so this is not to say that if you have",
    "start": "827680",
    "end": "833760"
  },
  {
    "text": "ten thousand orders you need ten thousand partitions you could have two partitions and each",
    "start": "833760",
    "end": "839279"
  },
  {
    "text": "partition is gonna get five thousand orders but the important thing is each partition will get the same",
    "start": "839279",
    "end": "844720"
  },
  {
    "text": "orders within it you won't have one order split across partitions if you set up",
    "start": "844720",
    "end": "849760"
  },
  {
    "text": "your keys correctly you can override the partitioning and can like do additional fancy stuff with",
    "start": "849760",
    "end": "855120"
  },
  {
    "text": "it if you want to but those are the basics of how it gets assigned and that's pretty much okay like glossing over a",
    "start": "855120",
    "end": "861680"
  },
  {
    "text": "bit of stuff here but that's pretty much as complicated as the producer api gets here's my message there's my topic there's my",
    "start": "861680",
    "end": "867680"
  },
  {
    "text": "broker and if we want to here's the key to affect the partitioning so producers we use it in our clients",
    "start": "867680",
    "end": "874240"
  },
  {
    "text": "applications it puts messages onto topics it does the nitty-gritty of partitioning and network",
    "start": "874240",
    "end": "880000"
  },
  {
    "text": "protocols and stuff like that there's a bunch of different clients languages that are supported you can also use the rest proxy for",
    "start": "880000",
    "end": "886880"
  },
  {
    "text": "performance reasons it's much better to use the native client support if it's available for your language or environments but if not",
    "start": "886880",
    "end": "893680"
  },
  {
    "text": "you can use the rest proxy to put data onto these topics now getting data out is slightly",
    "start": "893680",
    "end": "901120"
  },
  {
    "text": "more complicated and i kind of actually skipped ahead a little bit then when i was talking about consuming in parallel and stuff like that",
    "start": "901120",
    "end": "907680"
  },
  {
    "text": "so let's get into that now taking a step back when we read data out of kafka we do so",
    "start": "907680",
    "end": "914880"
  },
  {
    "text": "sequentially we go to kafka as our consuming application we use the consumer api and we say i'd like to read some data",
    "start": "914880",
    "end": "921600"
  },
  {
    "text": "from this topic and kafka will say okay i can start at the beginning of the log over here and you can read every single",
    "start": "921600",
    "end": "927600"
  },
  {
    "text": "message i can start at the end of the log and i'll just give you any new messages that arrive and your application can",
    "start": "927600",
    "end": "934720"
  },
  {
    "text": "also say actually i'd like to skip to offset 42 and read messages from there or i would",
    "start": "934720",
    "end": "940079"
  },
  {
    "text": "like to stick skip to the time stamp that corresponds to like three o'clock yesterday afternoon and read the messages from there your",
    "start": "940079",
    "end": "946480"
  },
  {
    "text": "client application can also say well i started at the end and i was processing now and now i'd like to go",
    "start": "946480",
    "end": "951600"
  },
  {
    "text": "back a bit and can i read some messages from back that point there so applications can seek back and",
    "start": "951600",
    "end": "957040"
  },
  {
    "text": "forth in the log the only thing is that access when they're reading is then sequential",
    "start": "957040",
    "end": "963120"
  },
  {
    "text": "now when consumers read data from kafka the messages don't get deleted so let",
    "start": "963120",
    "end": "969360"
  },
  {
    "text": "that sink in for a moment because this is very different from other systems you may be thinking of when you read a message from kafka",
    "start": "969360",
    "end": "976079"
  },
  {
    "text": "that message remains on the topic when we delete messages from kafka that happens in a completely separate process",
    "start": "976079",
    "end": "983040"
  },
  {
    "text": "it's defined by the retention policies for each topic and you define that per topic and you can say",
    "start": "983040",
    "end": "988399"
  },
  {
    "text": "this topic over here i want to store 10 gigabytes worth of data and when we hit 10 gigabytes then we start to get",
    "start": "988399",
    "end": "993440"
  },
  {
    "text": "rid of the old stuff we can say how to keep data based on time you say i want to keep seven days worth of clicks because after",
    "start": "993440",
    "end": "1000079"
  },
  {
    "text": "that they kind of like less useful i want to keep 10 years worth of orders i would in fact want to keep my customer",
    "start": "1000079",
    "end": "1005360"
  },
  {
    "text": "data forever and that's completely valid and plenty of people do that you say well actually whether or not",
    "start": "1005360",
    "end": "1011759"
  },
  {
    "text": "we've received an event for a given customer in the last 10 years doesn't matter i want to always have access to that",
    "start": "1011759",
    "end": "1017440"
  },
  {
    "text": "customer information you can do something called compacted topics where it says well actually we'll kind of",
    "start": "1017440",
    "end": "1022560"
  },
  {
    "text": "compact it down but we'll always keep the latest value for a given key but this is how you",
    "start": "1022560",
    "end": "1028400"
  },
  {
    "text": "manage when data leaves kafka when it gets deleted out of kafka it's completely separate from when",
    "start": "1028400",
    "end": "1034880"
  },
  {
    "text": "people read data from kafka and because of that this is super useful",
    "start": "1034880",
    "end": "1040319"
  },
  {
    "text": "because because of that we can have our original application so sally is our application here she's reading messages from the topic",
    "start": "1040319",
    "end": "1046720"
  },
  {
    "text": "for maybe the original use case that we envisaged but then someone else comes along and says oh you've got order data on a topic",
    "start": "1046720",
    "end": "1052960"
  },
  {
    "text": "like a live stream of all the data with historical orders as well that's really useful i'll use that also",
    "start": "1052960",
    "end": "1058720"
  },
  {
    "text": "so then someone else can come along and they can also read data from that topic maybe they just process new data maybe",
    "start": "1058720",
    "end": "1064559"
  },
  {
    "text": "they go back and scan through old stuff but we now build separate applications",
    "start": "1064559",
    "end": "1069679"
  },
  {
    "text": "reading the same data that are independent of each other so this is massively useful because we can",
    "start": "1069679",
    "end": "1074960"
  },
  {
    "text": "build systems where something happens in the world we produce it into a kafka topic",
    "start": "1074960",
    "end": "1080480"
  },
  {
    "text": "and now multiple different applications can independently use that data if one of those applications crashes the other two just",
    "start": "1080480",
    "end": "1087600"
  },
  {
    "text": "carry on happily the data will sit in the topic until it's being aged out and that way we can build nice flexible",
    "start": "1087600",
    "end": "1094000"
  },
  {
    "text": "loosely coupled systems the consumer api this is what it looks like in go and",
    "start": "1094000",
    "end": "1099200"
  },
  {
    "text": "go we can receive the messages on a different channel you simply say i'd like to subscribe to this particular topic and you receive",
    "start": "1099200",
    "end": "1105280"
  },
  {
    "text": "the messages as they arrive single consumer a single instance of a",
    "start": "1105280",
    "end": "1112160"
  },
  {
    "text": "consumer will receive data from all the partitions in a topic which is what you would want you don't want to say",
    "start": "1112160",
    "end": "1118320"
  },
  {
    "text": "give me the data and you need to get some of the data that would be fairly pointless so a single instance of a single",
    "start": "1118320",
    "end": "1123840"
  },
  {
    "text": "application gets all of the data from all the partitions we can have multiple",
    "start": "1123840",
    "end": "1129039"
  },
  {
    "text": "different consumers one consumer is sally is doing like fraud processing gets all of data from all partitions",
    "start": "1129039",
    "end": "1135120"
  },
  {
    "text": "another application is fred and he's doing analytics gets all of the data from all the",
    "start": "1135120",
    "end": "1140240"
  },
  {
    "text": "partitions now perhaps c1 here that blue box at the top of the screen",
    "start": "1140240",
    "end": "1145840"
  },
  {
    "text": "can't keep up with the processing of the throughput that we actually need to deliver so they're doing something super",
    "start": "1145840",
    "end": "1151919"
  },
  {
    "text": "complicated we've got very tight window in which we need to process the data we can horizontally scale",
    "start": "1151919",
    "end": "1157919"
  },
  {
    "text": "that application and create additional instances of it so in this case it looks like this",
    "start": "1157919",
    "end": "1164400"
  },
  {
    "text": "we have multiple instances of c1 and they form what's called a consumer",
    "start": "1164400",
    "end": "1170000"
  },
  {
    "text": "group so in this diagram here i realized this is kind of like ambiguous that c1 square is actually an instance",
    "start": "1170000",
    "end": "1176960"
  },
  {
    "text": "these are all c1 instances that are labeled ambiguously that's a silly idea for me to do that",
    "start": "1176960",
    "end": "1182320"
  },
  {
    "text": "but within this instance here this consumer group representing the whole consumer application you have four",
    "start": "1182320",
    "end": "1188080"
  },
  {
    "text": "instances of it we've scaled it out horizontally and so because we have four instances of that same logical",
    "start": "1188080",
    "end": "1194240"
  },
  {
    "text": "application and we have four partitions each instance gets data from one partition and kafka",
    "start": "1194240",
    "end": "1201200"
  },
  {
    "text": "does that for us kafka says okay you formed a consumer group i will sort out who gets which data and if we lose one",
    "start": "1201200",
    "end": "1208000"
  },
  {
    "text": "of those instances kafka says oh we've lost one of you i will make sure that someone else takes on that processing i use four partitions",
    "start": "1208000",
    "end": "1217120"
  },
  {
    "text": "and four instances of the application you don't have to have four and four you could have four partitions and two",
    "start": "1217120",
    "end": "1223679"
  },
  {
    "text": "instances so if you go from one to two you're gonna double your throughput each of those instances will get data",
    "start": "1223679",
    "end": "1229600"
  },
  {
    "text": "from two partitions if you have four and four then each one gets one partition if we have",
    "start": "1229600",
    "end": "1235200"
  },
  {
    "text": "five or if we have eight instances of the application and four partitions we don't process it any",
    "start": "1235200",
    "end": "1240960"
  },
  {
    "text": "faster than four because there are only four partitions partitions are our units of parallelism",
    "start": "1240960",
    "end": "1246640"
  },
  {
    "text": "partitions are a unit of scale which is why figuring out your partitioning strategy up front",
    "start": "1246640",
    "end": "1252400"
  },
  {
    "text": "is a good thing to do because if you start off with one partition you can't go anywhere if you start off",
    "start": "1252400",
    "end": "1257760"
  },
  {
    "text": "with like a million partitions that's probably a bad idea also so there's various rules of thumb out there which are a good idea to",
    "start": "1257760",
    "end": "1263760"
  },
  {
    "text": "follow it's give it i don't know i'm picking numbers here there are better ones out there i'll post on the slack channel afterwards give it 16 partitions to start off with",
    "start": "1263760",
    "end": "1271039"
  },
  {
    "text": "and then you've got room to grow you can still consume it with a single instance of instance of the application if you want to so the consumer api",
    "start": "1271039",
    "end": "1279679"
  },
  {
    "text": "a little bit more complicated because we can actually handle scaling within us we call it from our client applications",
    "start": "1279679",
    "end": "1285200"
  },
  {
    "text": "it reads messages from topics it can scan back and forth in the log messages don't get deleted",
    "start": "1285200",
    "end": "1290799"
  },
  {
    "text": "when we read them it's horizontally scalable we can just add in the distance additional instances of an application",
    "start": "1290799",
    "end": "1297360"
  },
  {
    "text": "to form a consumer group and again client languages available for numerous different uh languages clients libraries available",
    "start": "1297360",
    "end": "1304159"
  },
  {
    "text": "from numerous different languages and the rest proxy also we also need to think about where this",
    "start": "1304159",
    "end": "1310559"
  },
  {
    "text": "data's going to live we talked conceptually about a log and it's like very",
    "start": "1310559",
    "end": "1316000"
  },
  {
    "text": "abstract idea of like an immutable append only commit log but this log itself needs to",
    "start": "1316000",
    "end": "1321440"
  },
  {
    "text": "live somewhere and where it lives is on a broker so kafka brokers the actual jvm process",
    "start": "1321440",
    "end": "1327120"
  },
  {
    "start": "1324000",
    "end": "1407000"
  },
  {
    "text": "that you install and that you run and actually does this stuff that your clients are going to connect to",
    "start": "1327120",
    "end": "1332559"
  },
  {
    "text": "and it's a distributed system i've not really mentioned that up until this point but that commit log that we talked",
    "start": "1332559",
    "end": "1337919"
  },
  {
    "text": "about is actually a distributed commit log an append only immutable commit log that's distributed",
    "start": "1337919",
    "end": "1343600"
  },
  {
    "text": "across multiple brokers so like a distributed system you'll usually have three or more",
    "start": "1343600",
    "end": "1349360"
  },
  {
    "text": "instances of a broker so the data gets spread across them each of our partitions",
    "start": "1349360",
    "end": "1354559"
  },
  {
    "text": "gets allocated across the available brokers and each partition has what's called a leader partition but in kind of standard",
    "start": "1354559",
    "end": "1362080"
  },
  {
    "text": "distributed systems way we also have followers which are replicas of the primary of the leader so now",
    "start": "1362080",
    "end": "1370000"
  },
  {
    "text": "in the case of let's say partition three partition three has got its leader on broker three by coincidence that's",
    "start": "1370000",
    "end": "1375919"
  },
  {
    "text": "not kind of um um always always the way so partition three lives on broker three we also have",
    "start": "1375919",
    "end": "1382640"
  },
  {
    "text": "follower partitions on broker two and broker one if we lose broker three okay in a nice",
    "start": "1382640",
    "end": "1388960"
  },
  {
    "text": "little fire then then broker two well in this case we'll take over the responsibility for the leader",
    "start": "1388960",
    "end": "1394559"
  },
  {
    "text": "partition for that uh partition so a distributed system you'll have three or pretty many more brokers",
    "start": "1394559",
    "end": "1401360"
  },
  {
    "text": "to hold that data and to serve that data to the client applications",
    "start": "1401360",
    "end": "1407200"
  },
  {
    "start": "1407000",
    "end": "1457000"
  },
  {
    "text": "so this is kind of good we've got a distributed system it gives us the concept of an immutable",
    "start": "1407600",
    "end": "1413360"
  },
  {
    "text": "app and only commit log we send our events to it we can consume our events from this",
    "start": "1413360",
    "end": "1419679"
  },
  {
    "text": "but in a way we're only just getting started because what i've laid out here is",
    "start": "1420400",
    "end": "1426880"
  },
  {
    "text": "the foundations and the very very strong and solid foundations with some fantastic guarantees over how how it does things",
    "start": "1426880",
    "end": "1434080"
  },
  {
    "text": "but the foundations for working with events but when it comes to building applications with events it's kind of",
    "start": "1434080",
    "end": "1441200"
  },
  {
    "text": "tedious if we have to reinvent everything else all the time and this is where the ecosystem",
    "start": "1441200",
    "end": "1446880"
  },
  {
    "text": "bits of kafka comes in because the producer and the consumer apis and the brokers they're just like the starting blocks",
    "start": "1446880",
    "end": "1452960"
  },
  {
    "text": "what we've actually got within apache kafka is a lot more so let's take a look through that",
    "start": "1452960",
    "end": "1458799"
  },
  {
    "start": "1457000",
    "end": "2107000"
  },
  {
    "text": "the reasons why kafka does a lot more than just what i've shown you so far is that when you come to actually use it",
    "start": "1458799",
    "end": "1464159"
  },
  {
    "text": "you realize that there are common patterns and common requirements around what people built and one example",
    "start": "1464159",
    "end": "1469600"
  },
  {
    "text": "would be when people start building streaming pipelines when they say i've got data that's sat in one place",
    "start": "1469600",
    "end": "1475039"
  },
  {
    "text": "and i want to stream it and go and put it somewhere else i've got data that sat in a transactional system i want to",
    "start": "1475039",
    "end": "1480159"
  },
  {
    "text": "offload it to an analytical platform like s3 well because the data is not deleted when i",
    "start": "1480159",
    "end": "1485440"
  },
  {
    "text": "push it to s3 because it's stored in a topic i also want to take that same data and push it to hdfs",
    "start": "1485440",
    "end": "1490960"
  },
  {
    "text": "that's kind of useful we've got systems coming into kafka systems going out of kafka we could say actually i",
    "start": "1490960",
    "end": "1497039"
  },
  {
    "text": "want to build a new application and it's going to be a microservice and it's going to use kafka as its broker",
    "start": "1497039",
    "end": "1502480"
  },
  {
    "text": "and this application needs to be driven by events from an existing application on existing",
    "start": "1502480",
    "end": "1508159"
  },
  {
    "text": "applications a lot of the time we've got a database set underneath them and what we can actually do is without needing to modify",
    "start": "1508159",
    "end": "1514080"
  },
  {
    "text": "that existing application because it's a closed box third party or it's a legacy system in-house that we don't touch",
    "start": "1514080",
    "end": "1520080"
  },
  {
    "text": "we can actually hook into the database which that system that application is writing and what it's changes to",
    "start": "1520080",
    "end": "1526080"
  },
  {
    "text": "capture the events out of the database into a kafka topic and drive a new application from it and",
    "start": "1526080",
    "end": "1532240"
  },
  {
    "text": "in both of these examples whether it's applications or pipelines we're integrating different systems",
    "start": "1532240",
    "end": "1538080"
  },
  {
    "text": "we're saying stream data in from a database and push it to s3 stream data and capture events from this",
    "start": "1538080",
    "end": "1543120"
  },
  {
    "text": "database so we can drive an application from it and so we start thinking well that's interesting how am i going to get that",
    "start": "1543120",
    "end": "1548799"
  },
  {
    "text": "data from the database into kafka how i'm going to push that data from kafka to s3 and to hdfs well i saw this talk",
    "start": "1548799",
    "end": "1556960"
  },
  {
    "text": "at google tokyo once and there's this chap and he was talking about the producer api and the consumer api",
    "start": "1556960",
    "end": "1562080"
  },
  {
    "text": "so i guess i'm going to write a java program and i'm going to poll the database and use the producer api to write some records to a kafka topic",
    "start": "1562080",
    "end": "1568720"
  },
  {
    "text": "and then i'm going to have to take a program maybe i'll write in python and i'll connect i'll use the consumer api to connect to this topic and i'll",
    "start": "1568720",
    "end": "1575200"
  },
  {
    "text": "push the data to htfs and then we'll use something else and push it to s3 and well actually we've",
    "start": "1575200",
    "end": "1580400"
  },
  {
    "text": "got lots of integrations that we're doing here maybe we can write ourselves a framework maybe like we could actually do",
    "start": "1580400",
    "end": "1586080"
  },
  {
    "text": "something like we're going to sit down and go into a dark room for like six months and build this thing and it can like pull data in from all these places and",
    "start": "1586080",
    "end": "1592480"
  },
  {
    "text": "push data these places and we can make it configuration based and we can make it plugable and we can",
    "start": "1592480",
    "end": "1597760"
  },
  {
    "text": "handle schemas and we can handle restarts and scalability this will be a load of fun but the problem is that there's admiral",
    "start": "1597760",
    "end": "1604320"
  },
  {
    "text": "tellers it's a trap because in writing that framework you're not actually doing anything",
    "start": "1604320",
    "end": "1610159"
  },
  {
    "text": "different from what every other business is doing everyone's streaming data from kaf from a database into kafka",
    "start": "1610159",
    "end": "1616480"
  },
  {
    "text": "everyone needs to take data from kafka and push it to s3 or elasticsearch or snowflake or any number of other places everyone",
    "start": "1616480",
    "end": "1624240"
  },
  {
    "text": "writing frameworks would be kind of wasting their time and reinventing the whale because it exists",
    "start": "1624240",
    "end": "1629520"
  },
  {
    "text": "already so apache kafka includes the kafka connect api which lets you stream data in from",
    "start": "1629520",
    "end": "1635679"
  },
  {
    "text": "systems upstream into kafka push data from kafka topics downstream to any number of other places so we can",
    "start": "1635679",
    "end": "1643120"
  },
  {
    "text": "actually do this integration whether it's like end-to-end pipelines or streaming data in from sources to drive",
    "start": "1643120",
    "end": "1648799"
  },
  {
    "text": "our applications using kafka connect it's configuration based use set up",
    "start": "1648799",
    "end": "1654080"
  },
  {
    "text": "about json to describe what you want to do it's pluggable so you say i want to use this connector this technology",
    "start": "1654080",
    "end": "1659840"
  },
  {
    "text": "i want to serialize my data in this way it's also a distributed system so it's fault tolerant it's scalable it",
    "start": "1659840",
    "end": "1666000"
  },
  {
    "text": "manages schemas it does all of this stuff for you so as much fun as writing frameworks is",
    "start": "1666000",
    "end": "1671120"
  },
  {
    "text": "unfortunately kafka is stolen your thunder on this one so instead you get to go and write applications that actually deliver",
    "start": "1671120",
    "end": "1676640"
  },
  {
    "text": "business value you get all of the different connectors and so on from conference hub you type in the technology that you're",
    "start": "1676640",
    "end": "1682159"
  },
  {
    "text": "interested in and off you go now let's take another step back for a",
    "start": "1682159",
    "end": "1688159"
  },
  {
    "text": "moment and think about this data that we're actually talking about integrating we've said we're going to get data in",
    "start": "1688159",
    "end": "1693840"
  },
  {
    "text": "from a database i'm going to push data down to s3 but i also said well let's model these events as like just this humble little",
    "start": "1693840",
    "end": "1700399"
  },
  {
    "text": "square a rectangle on the screen and say it's just like a key value and we've got another key value we're appending these keys and values to the",
    "start": "1700399",
    "end": "1706640"
  },
  {
    "text": "log but actually what is it it's kind of fairly abstract saying that's a key and a value",
    "start": "1706640",
    "end": "1711679"
  },
  {
    "text": "and if we're thinking well this thing sounds useful let's go and build something in it we need to think about like well okay well tell us a bit more what actually is",
    "start": "1711679",
    "end": "1718080"
  },
  {
    "text": "inside that box now hopefully what's not inside the box is this but unfortunately sometimes it",
    "start": "1718080",
    "end": "1725039"
  },
  {
    "text": "is but kafka itself doesn't care what's inside that box it just says if you want to give me a key and then",
    "start": "1725039",
    "end": "1731279"
  },
  {
    "text": "give me a value and it's just bytes so as developers as data engineers we",
    "start": "1731279",
    "end": "1736799"
  },
  {
    "text": "need to decide how am i going to serialize that data to give kafka the balance that it needs",
    "start": "1736799",
    "end": "1742080"
  },
  {
    "text": "and hopefully we don't choose this option which if we eyeball that set of data i guess it's tab separated maybe i looks",
    "start": "1742080",
    "end": "1748880"
  },
  {
    "text": "like there's some address fields in there maybe a date maybe a latitude and longitude on it who knows",
    "start": "1748880",
    "end": "1754320"
  },
  {
    "text": "so what kafka enables you to do like we saw earlier is build loosely coupled systems",
    "start": "1754320",
    "end": "1760799"
  },
  {
    "text": "something an event happens produces data into a topic another application on numerous",
    "start": "1760799",
    "end": "1766240"
  },
  {
    "text": "applications can use that data and consume that data from the topic nice and loosely coupled",
    "start": "1766240",
    "end": "1771279"
  },
  {
    "text": "this is brilliant but then if we don't think about how we're serializing our data and specifically how we're managing our",
    "start": "1771279",
    "end": "1777039"
  },
  {
    "text": "schemas we start to build those couplings back together in terms of humans having to speak to each other",
    "start": "1777039",
    "end": "1782559"
  },
  {
    "text": "or email or smack each other and say that you've written onto a topic here like what on earth is that supposed to be saying and what are the different",
    "start": "1782559",
    "end": "1788720"
  },
  {
    "text": "fields and like did you just delete a field from that schema like without telling us and what's the default one is this like a big hint or is it a day",
    "start": "1788720",
    "end": "1794799"
  },
  {
    "text": "tonight all of those common things around schemers so there are good ways and there are bad",
    "start": "1794799",
    "end": "1800720"
  },
  {
    "text": "ways to serialize your data because if you want to build systems which are resilient",
    "start": "1800720",
    "end": "1806480"
  },
  {
    "text": "and don't like break every time you blink or think of coughing which you certainly shouldn't do in a",
    "start": "1806480",
    "end": "1811840"
  },
  {
    "text": "pandemic then you want to be thinking about your schemas because schemers act as the contract that is the api",
    "start": "1811840",
    "end": "1817679"
  },
  {
    "text": "between your services whether you're building pipelines or applications so if you use afro or protobuf or json schema",
    "start": "1817679",
    "end": "1824960"
  },
  {
    "text": "you actually have a schema which we store and persist and then consumers can use that schema",
    "start": "1824960",
    "end": "1831279"
  },
  {
    "text": "if you use json you can like well it looks like there's a schema but it's not actually explicit and declared",
    "start": "1831279",
    "end": "1836720"
  },
  {
    "text": "and if you use csv then i'll come and see me later i'll talk to about that so if we use avro protocol json schema",
    "start": "1836720",
    "end": "1843919"
  },
  {
    "text": "the confluence scheme registry is community licensed it will actually store the schema for you",
    "start": "1843919",
    "end": "1849200"
  },
  {
    "text": "it will enforce schema compatibility guarantees on the producer so once the producer said here is the",
    "start": "1849200",
    "end": "1854880"
  },
  {
    "text": "schema they can't subsequently say like oh and just write these messages to the topic as well even though they don't match it won't let you do that for good",
    "start": "1854880",
    "end": "1861120"
  },
  {
    "text": "reasons because the consumer when they come to read that data they'll go to the schema registry they'll fetch the schema they'll be able",
    "start": "1861120",
    "end": "1867360"
  },
  {
    "text": "to deserialize that data and those compatibility guarantees mean that consumer will be able to read that",
    "start": "1867360",
    "end": "1873039"
  },
  {
    "text": "data and it will know what the fields are within that data it'll know the data types and the field names so it's a very",
    "start": "1873039",
    "end": "1878640"
  },
  {
    "text": "very good idea to do the other piece of the puzzle in terms",
    "start": "1878640",
    "end": "1885200"
  },
  {
    "text": "of like the ecosystem and the common things that people build with event driven platforms",
    "start": "1885200",
    "end": "1890399"
  },
  {
    "text": "is what they're actually doing within these consumers so we've got data going in data coming",
    "start": "1890399",
    "end": "1896080"
  },
  {
    "text": "out and if we think about the kind of things that you do with data it often boils down into a",
    "start": "1896080",
    "end": "1901919"
  },
  {
    "text": "few repeating patterns and there's other stuff as well there's plenty of other things but some of these common patterns",
    "start": "1901919",
    "end": "1907279"
  },
  {
    "text": "include driving alerts based on events or wanting to take data and aggregate it up",
    "start": "1907279",
    "end": "1912559"
  },
  {
    "text": "and do something with that aggregate whether to look at kind of like uh particular trends and drive an alert",
    "start": "1912559",
    "end": "1917840"
  },
  {
    "text": "from that or simply to like take an aggregate and report on that or use it to drive a dashboard out to your",
    "start": "1917840",
    "end": "1923440"
  },
  {
    "text": "user or to take data and push it somewhere else for further analytics and as part of this is basically like",
    "start": "1923440",
    "end": "1930480"
  },
  {
    "text": "describing stream processing take some data join it aggregate it enrich it do these kind of things with it and so",
    "start": "1930480",
    "end": "1938559"
  },
  {
    "text": "over time as our events are occurring we have this stream coming in and we want to process it we want to do",
    "start": "1938559",
    "end": "1944880"
  },
  {
    "text": "some fairly standard patterns to it so maybe we've got widgets information our widgets and widgets have got",
    "start": "1944880",
    "end": "1950559"
  },
  {
    "text": "different colors they've got yellow widgets and red widgets we'd like to create a new stream as the",
    "start": "1950559",
    "end": "1956320"
  },
  {
    "text": "source data comes in based on our source events as they're arriving only about red widgets so various data",
    "start": "1956320",
    "end": "1962720"
  },
  {
    "text": "coming in we want to filter out data as it arrives and write it out to a new topic because that new topic is going to",
    "start": "1962720",
    "end": "1968240"
  },
  {
    "text": "be used to drive an alerting system which integrates with kafka because most things integrate with kafka nowadays",
    "start": "1968240",
    "end": "1973600"
  },
  {
    "text": "and we can do that using the kafka streams api so kafka streams is again part of apache",
    "start": "1973600",
    "end": "1979840"
  },
  {
    "text": "kafka it's a java library and so instead of doing your stream processing like on",
    "start": "1979840",
    "end": "1985120"
  },
  {
    "text": "a separate technology in a separate infrastructure a separate team somewhere else you can actually do your",
    "start": "1985120",
    "end": "1990320"
  },
  {
    "text": "stream processing within your java applications you can see i'm going to do my filtering my",
    "start": "1990320",
    "end": "1995919"
  },
  {
    "text": "aggregations my enrichment my transformations within my java application itself my",
    "start": "1995919",
    "end": "2002000"
  },
  {
    "text": "data comes in i filter it and so this is an abstraction on top of the produce from consumer apis",
    "start": "2002000",
    "end": "2008240"
  },
  {
    "text": "yes you could do like stream processing yourself and use producer and consumer apis but it's an awful lot simpler if you use",
    "start": "2008240",
    "end": "2014399"
  },
  {
    "text": "the library that's provided which is kafka streams so instead of writing consuming consumer",
    "start": "2014399",
    "end": "2019760"
  },
  {
    "text": "api applications we write streams applications now not everyone writes stream",
    "start": "2019760",
    "end": "2025600"
  },
  {
    "text": "processing application right so java sorry but they still want to do stream processing so in this case they",
    "start": "2025600",
    "end": "2031679"
  },
  {
    "text": "can use k sql db k sequel db again it's community licensed and here we use sql",
    "start": "2031679",
    "end": "2037840"
  },
  {
    "text": "to describe our stream processing transformations we can say take this stream of data select from",
    "start": "2037840",
    "end": "2043440"
  },
  {
    "text": "this stream where it matches this predicate and like in the database world you say create table as select in the streaming",
    "start": "2043440",
    "end": "2050560"
  },
  {
    "text": "world we say create stream as select and because the data and the source",
    "start": "2050560",
    "end": "2055599"
  },
  {
    "text": "is unbounded it's continuous it never stops so does that select statement because there's no end to it it's not like",
    "start": "2055599",
    "end": "2061520"
  },
  {
    "text": "selecting against relational data whether it's like a lump of data when you've selected from it it might take an age if you've not indexed it",
    "start": "2061520",
    "end": "2067280"
  },
  {
    "text": "but you can select from that relational data and then it returns eventually in a streaming world you can select from",
    "start": "2067280",
    "end": "2072638"
  },
  {
    "text": "that inbound stream that stream is unbounded so i select query runs continually so",
    "start": "2072639",
    "end": "2077760"
  },
  {
    "text": "that continual output from that select is written out to your target stream which is a kafka topic",
    "start": "2077760",
    "end": "2084960"
  },
  {
    "text": "so if we think about the kind of things that we want to do with that data we can write alerts against it we can",
    "start": "2084960",
    "end": "2090398"
  },
  {
    "text": "say i would like to take the stream of inbound data and apply a predicate to it i would like to take this inbound data",
    "start": "2090399",
    "end": "2096320"
  },
  {
    "text": "and aggregate it or aggregate it and apply a predicate or i'd like to take this data and i'd",
    "start": "2096320",
    "end": "2101599"
  },
  {
    "text": "like to use kafka connect to actually route it down to a target system",
    "start": "2101599",
    "end": "2106960"
  },
  {
    "text": "so i believe i have five minutes left i'll show you a very brief demo and then i'll share a couple of",
    "start": "2106960",
    "end": "2113440"
  },
  {
    "start": "2107000",
    "end": "2382000"
  },
  {
    "text": "resources to finish off with and we'll do questions on slack because i'm pretty sure i'll run out of time but",
    "start": "2113440",
    "end": "2118480"
  },
  {
    "text": "i do want to show you the demo because it's useful to get an idea of the kind of things you want to do the demo itself",
    "start": "2118480",
    "end": "2123520"
  },
  {
    "text": "is on github so again i'll post this on slack afterwards so demo scene and the whole script itself is also",
    "start": "2123520",
    "end": "2129280"
  },
  {
    "text": "there so you can just say docker compose up and it brings up the whole stack and you can actually go and try it out",
    "start": "2129280",
    "end": "2134640"
  },
  {
    "text": "for yourself so what we've got here is a kafka topic that's been populated",
    "start": "2134640",
    "end": "2141280"
  },
  {
    "text": "with a producer somewhere else which is over here and on this topic",
    "start": "2141280",
    "end": "2146880"
  },
  {
    "text": "we've got information about stock trades so connected into the broker we're saying show me the data from this particular",
    "start": "2146880",
    "end": "2152160"
  },
  {
    "text": "topic and it says like it's buyers and the cells and there's different symbols and kind of like fairly standard set of data",
    "start": "2152160",
    "end": "2158640"
  },
  {
    "text": "and i want to show you a couple of things we can do in terms of this stream processing so here we're using kc called db and",
    "start": "2158640",
    "end": "2165440"
  },
  {
    "text": "we're going to say i'd like to declare an object a stream on top of that topic that you showed me and because we're",
    "start": "2165440",
    "end": "2171680"
  },
  {
    "text": "using avro we don't actually have to type in the schema i can say describe trades and it says",
    "start": "2171680",
    "end": "2177440"
  },
  {
    "text": "here is your particular stream that you've created and there's the schema because we're using the schema registry",
    "start": "2177440",
    "end": "2183040"
  },
  {
    "text": "and afro i could say well we've got a schema so i can then project the fields from it i can say",
    "start": "2183040",
    "end": "2188720"
  },
  {
    "text": "select just a few of the fields so let's select the side and the quantity from trades where the symbol is this and",
    "start": "2188720",
    "end": "2195119"
  },
  {
    "text": "it says like okay every message which now arrives i'm going to apply that's predicate to it and i'm just going to echo out to the",
    "start": "2195119",
    "end": "2200880"
  },
  {
    "text": "screen those two fields that you've asked for and using key sql db we can build an",
    "start": "2200880",
    "end": "2207599"
  },
  {
    "text": "aggregate view of that data and actually kind of like a materialized view in effect it's actually built in this",
    "start": "2207599",
    "end": "2212880"
  },
  {
    "text": "state for aggregation internally k sequel db like kafka streams is a distributed system so you scale it",
    "start": "2212880",
    "end": "2218640"
  },
  {
    "text": "out for performance and for resilience and we're going to create an object in this case a table",
    "start": "2218640",
    "end": "2223920"
  },
  {
    "text": "which is going to hold the state like how many trades have there been what's the total value and so on every 15 minutes just for this",
    "start": "2223920",
    "end": "2230480"
  },
  {
    "text": "particular stock symbol and it creates that table and we can say select",
    "start": "2230480",
    "end": "2235520"
  },
  {
    "text": "star from that table and it says okay there is the contents",
    "start": "2235520",
    "end": "2241200"
  },
  {
    "text": "of the table and you can see it's got a windows started end which will show as an epoch um so these are the aggregate values",
    "start": "2241200",
    "end": "2248560"
  },
  {
    "text": "as new um values arrive on that source topic it's updating that aggregate and under",
    "start": "2248560",
    "end": "2254720"
  },
  {
    "text": "the covers if i say show tables it says we've got a table called sorry",
    "start": "2254720",
    "end": "2261359"
  },
  {
    "text": "kafka topic called the name of the table and we can actually consume from that so at this point now we've populated a",
    "start": "2261359",
    "end": "2267680"
  },
  {
    "text": "new kafka topic with the state of this aggregate so we can take that aggregate and drive an application from it we can also",
    "start": "2267680",
    "end": "2274800"
  },
  {
    "text": "hook it up to an external data target that's what we're going to do here let me show you kafka connect this is",
    "start": "2274800",
    "end": "2280079"
  },
  {
    "text": "obviously a super quick demo of some of the pieces but i will just give you a flavor of the kind of thing",
    "start": "2280079",
    "end": "2285280"
  },
  {
    "text": "that you can do so we've got an inbound stream of data we've said let's create an aggregate",
    "start": "2285280",
    "end": "2290320"
  },
  {
    "text": "that's being refreshed and maintained in real time i'm going to push this in this case down to a database so down to postgres",
    "start": "2290320",
    "end": "2296320"
  },
  {
    "text": "using kafka connect and now if i head over to photographs and first off i'm just",
    "start": "2296320",
    "end": "2301760"
  },
  {
    "text": "going to do a select from that stream so this is from that stream that we",
    "start": "2301760",
    "end": "2308079"
  },
  {
    "text": "created so this is the source data as it arrives so that's at the top of the screen there and at the bottom of the screen i'm",
    "start": "2308079",
    "end": "2313920"
  },
  {
    "text": "going to go into postgres i'm going to say select from this table that we're populating",
    "start": "2313920",
    "end": "2319920"
  },
  {
    "text": "so the top of the screen is the data as it arrives and it's actually flowing by there like loads and loads of stuff at the beginning",
    "start": "2319920",
    "end": "2325520"
  },
  {
    "text": "because i was reading all of the data from the topic in the beginning whereas now it's saying we've just caught up and now we're just",
    "start": "2325520",
    "end": "2330880"
  },
  {
    "text": "showing you any new rows that arrive so if you look at the data so 1045 is",
    "start": "2330880",
    "end": "2336000"
  },
  {
    "text": "the window most recently so 1045 in the uk right now 71 buys 67 cells if i re-run",
    "start": "2336000",
    "end": "2342880"
  },
  {
    "text": "that query let's move that up so you can see the column headings and so you're looking at this but here this the buy and the cell 61",
    "start": "2342880",
    "end": "2349200"
  },
  {
    "text": "and so 71 and 67 now 82 and 73 if i run it again 83 so as new messages",
    "start": "2349200",
    "end": "2356800"
  },
  {
    "text": "arrive on our source topic key sql db um updates the aggregates and in this case we're pushing that",
    "start": "2356800",
    "end": "2362240"
  },
  {
    "text": "aggregate down to a target but we can also query that aggregate directly from k sql db so you can do",
    "start": "2362240",
    "end": "2369760"
  },
  {
    "text": "some very very cool things but unfortunately i'm pretty much out of time so i'll share a couple of resources and then any other questions and if you",
    "start": "2369760",
    "end": "2376079"
  },
  {
    "text": "want more links that show this kind of stuff in detail and hit me up on slack and i can share as much as you'd like to see",
    "start": "2376079",
    "end": "2382320"
  },
  {
    "start": "2382000",
    "end": "2450000"
  },
  {
    "text": "so quick recap and super super quick recap events are all around us they model the",
    "start": "2382320",
    "end": "2388560"
  },
  {
    "text": "real world we store them as key values in an immutable app end only distributed commit log we have the log",
    "start": "2388560",
    "end": "2395839"
  },
  {
    "text": "at the heart of things we have produced from consumer apis we have kafka connect providing the integration api",
    "start": "2395839",
    "end": "2401680"
  },
  {
    "text": "and we have kafka streams giving us the stream processing capabilities and this gives us apache kafka around",
    "start": "2401680",
    "end": "2408560"
  },
  {
    "text": "apache kafka we have things like confluent platform which include kc called db and scheme registry to build out a more",
    "start": "2408560",
    "end": "2414480"
  },
  {
    "text": "complete ecosystem for be able to build applications and pipelines go and learn",
    "start": "2414480",
    "end": "2419599"
  },
  {
    "text": "more from these these books you can download for free from our website and go and try out confluent cloud and",
    "start": "2419599",
    "end": "2425200"
  },
  {
    "text": "there's various discount codes and or money off codes that you can use there and finally if you want to learn more",
    "start": "2425200",
    "end": "2431359"
  },
  {
    "text": "actually try this stuff out developer.confluence.io is where you can go for that",
    "start": "2431359",
    "end": "2439839"
  }
]