[
  {
    "text": "[Music]",
    "start": "2570",
    "end": "10170"
  },
  {
    "text": "so my name Tim alif I work for company data Stacks in analytic uh team and we",
    "start": "11400",
    "end": "18960"
  },
  {
    "text": "do analytic for the one of the new nosql distributed database that called",
    "start": "18960",
    "end": "25240"
  },
  {
    "text": "Cassandra and well actually uh before I start let me well a little bit",
    "start": "25240",
    "end": "31720"
  },
  {
    "text": "understand the auditory who know who works with any no sare database and okay",
    "start": "31720",
    "end": "38960"
  },
  {
    "text": "everybody know what is it and who know what Cassandra is oh well much better",
    "start": "38960",
    "end": "44920"
  },
  {
    "text": "who know what is a Hadoop is cool who use",
    "start": "44920",
    "end": "50360"
  },
  {
    "text": "it who knows what is spark is H okay who",
    "start": "51239",
    "end": "56359"
  },
  {
    "text": "use it well we got plenty of people who know and use it",
    "start": "56359",
    "end": "62559"
  },
  {
    "text": "well actually spark will replace map reduce and who know what the map redu is",
    "start": "62559",
    "end": "67960"
  },
  {
    "text": "and how to use it okay sure sure uh so I will talk about well",
    "start": "67960",
    "end": "74920"
  },
  {
    "text": "agenda agenda is really simple well actually when you build a when right now",
    "start": "74920",
    "end": "81439"
  },
  {
    "text": "anybody build a data floor they build something like this",
    "start": "81439",
    "end": "87920"
  },
  {
    "text": "when you get a uh b proing c when you you have data",
    "start": "87920",
    "end": "94479"
  },
  {
    "text": "sources on the left put it into the datab basis you have a data lake or",
    "start": "94479",
    "end": "100560"
  },
  {
    "text": "something like that on the all the all the thing the data Lake and you build uh",
    "start": "100560",
    "end": "107439"
  },
  {
    "text": "all the data then and then you process it by the night and put the data into the serving layer when you use your bi",
    "start": "107439",
    "end": "115119"
  },
  {
    "text": "and show it to the people it's not it's not a bi it could be any I don't know",
    "start": "115119",
    "end": "120479"
  },
  {
    "text": "recommendation engine your website with a catalog or something like that and well actually the new layer appear that",
    "start": "120479",
    "end": "126399"
  },
  {
    "text": "call speed layer or streaming when you stream data on memory and produce result",
    "start": "126399",
    "end": "133000"
  },
  {
    "text": "on on on a fly so well actually some guys from the Lambda architecture called",
    "start": "133000",
    "end": "139200"
  },
  {
    "text": "Lambda architecture and we'll draw the picture the other guys say that this data lake is all thing and you need only",
    "start": "139200",
    "end": "147160"
  },
  {
    "text": "the thin layer the small layer that called speed layer layer and you need to",
    "start": "147160",
    "end": "152680"
  },
  {
    "text": "do only speed processing and this kind of called COA",
    "start": "152680",
    "end": "159400"
  },
  {
    "text": "architecture but well actually it's a common way to do things and let's try to",
    "start": "159400",
    "end": "167480"
  },
  {
    "text": "build one of that kind of system B based on Cassandra uh a little bit about",
    "start": "167480",
    "end": "174080"
  },
  {
    "text": "Cassandra Cassandra is a new kind of databases as any other no",
    "start": "174080",
    "end": "180360"
  },
  {
    "text": "think it's scalable and well it's much better scale than most of the no sare te because it's",
    "start": "180360",
    "end": "186599"
  },
  {
    "text": "masterless everybody know uh equals everybody can be master and so you can",
    "start": "186599",
    "end": "193879"
  },
  {
    "text": "scale it liner for for a long long list",
    "start": "193879",
    "end": "198920"
  },
  {
    "text": "and actually the good feature of the Cassandra is that SQL language that is like SQL I will show you",
    "start": "198920",
    "end": "206440"
  },
  {
    "text": "later so actually it's a multi dat Center you can manage replication Factor on any",
    "start": "206440",
    "end": "213760"
  },
  {
    "text": "data center so you can have two data centers with the same data well all over",
    "start": "213760",
    "end": "219400"
  },
  {
    "text": "the world so the data replication is everybody know every note in a class you",
    "start": "219400",
    "end": "227120"
  },
  {
    "text": "know how the data layout how it's shed so it's shed and how",
    "start": "227120",
    "end": "235120"
  },
  {
    "text": "replicated so it's really s simple thing to do to to work with so and it's really",
    "start": "235120",
    "end": "244439"
  },
  {
    "text": "configurable it's a good way to use it and well my lovely feature of the Cassandra is that kind of language well",
    "start": "244439",
    "end": "252000"
  },
  {
    "text": "actually while a lot of no s system has the ability to scale linary have this",
    "start": "252000",
    "end": "258720"
  },
  {
    "text": "masterless systems but SQL language is a feature well actually you can Oracle",
    "start": "258720",
    "end": "264479"
  },
  {
    "text": "guys like this really well and well actually the only thing that help oral",
    "start": "264479",
    "end": "270039"
  },
  {
    "text": "guys to step into the No Square world is to write that kind of lines of code well",
    "start": "270039",
    "end": "275560"
  },
  {
    "text": "actually beside that it's still wide row like if you know what what I what this",
    "start": "275560",
    "end": "280880"
  },
  {
    "text": "mean database that allow you to add rows on the add column white column add",
    "start": "280880",
    "end": "288360"
  },
  {
    "text": "column on the on a on a go as you go so so what the spark is spark is a",
    "start": "288360",
    "end": "297639"
  },
  {
    "text": "new distributed anal IC engine that was well developed in UK uh",
    "start": "297639",
    "end": "305919"
  },
  {
    "text": "University of Berkeley well it's have to well it's",
    "start": "305919",
    "end": "312199"
  },
  {
    "text": "much simple and have to solve that kind of problem that Hadoop is hard to",
    "start": "312199",
    "end": "318000"
  },
  {
    "text": "solve and actually here is an example uh if you start if you take a",
    "start": "318000",
    "end": "323880"
  },
  {
    "text": "look at the performance the Hadoop will be hundreds times",
    "start": "323880",
    "end": "332560"
  },
  {
    "text": "slower than a spark on some tasks when you need to do a lot of iteration",
    "start": "332680",
    "end": "338039"
  },
  {
    "text": "against the same data for example in this case you see logistic regression",
    "start": "338039",
    "end": "344720"
  },
  {
    "text": "performance and you will see that Hadoop will read data for the each iteration",
    "start": "344720",
    "end": "350800"
  },
  {
    "text": "process data save it to the uh back to the hdf",
    "start": "350800",
    "end": "356160"
  },
  {
    "text": "fastest hdfs uh and we like the spark will do it in memory if it can in any",
    "start": "356160",
    "end": "363000"
  },
  {
    "text": "case it can get most of the data to memory process it and put back Get Next",
    "start": "363000",
    "end": "368319"
  },
  {
    "text": "Step so you can see that spark will be well much faster on a lot of well",
    "start": "368319",
    "end": "375479"
  },
  {
    "text": "actually Cloud era claims that they going to uh replace map reduce with a",
    "start": "375479",
    "end": "381440"
  },
  {
    "text": "spark as a default engine and so my produce engine will dial sooner or",
    "start": "381440",
    "end": "388560"
  },
  {
    "text": "later and well actually the other thing that spark bring into this world of data",
    "start": "388560",
    "end": "395639"
  },
  {
    "text": "analytic is Simplicity you can take a look at the well actually you if you know kup always",
    "start": "395639",
    "end": "403759"
  },
  {
    "text": "say see our first tutorial is word count let's count word in the system and you",
    "start": "403759",
    "end": "411000"
  },
  {
    "text": "well everybody who run right my produce task know that what",
    "start": "411000",
    "end": "417639"
  },
  {
    "text": "all these small lines mean in the had World well actually the right uh the",
    "start": "417639",
    "end": "424440"
  },
  {
    "text": "spark is three line of code to do the same thing we can well we can do well we",
    "start": "424440",
    "end": "431080"
  },
  {
    "text": "can go deeper into this lines of code later when I'll start start talking",
    "start": "431080",
    "end": "438759"
  },
  {
    "text": "about spark in details but it's spark is really much simple to cut well in this",
    "start": "438759",
    "end": "445560"
  },
  {
    "text": "case it's a python the scolar right on Scala oh the sparkk writing on Scala so I recommend",
    "start": "445560",
    "end": "453240"
  },
  {
    "text": "to use Scala not not a python but if you scar about Scala python is a another",
    "start": "453240",
    "end": "460160"
  },
  {
    "text": "Choice don't use Java well and what this picture well actually I I I already uh",
    "start": "460160",
    "end": "467639"
  },
  {
    "text": "talk about this picture how how the map reduce work read data from the disk map reduce",
    "start": "467639",
    "end": "474879"
  },
  {
    "text": "right data to the disk read data from the disk map reduce right data to the dis and well and so on and so far well",
    "start": "474879",
    "end": "482159"
  },
  {
    "text": "actually the spark build uh graph of",
    "start": "482159",
    "end": "489000"
  },
  {
    "text": "execution and execute it on the data so it's a lot data from given sources get",
    "start": "489000",
    "end": "496400"
  },
  {
    "text": "it by if it can get it all in the memory get it into the memory or read it step",
    "start": "496400",
    "end": "502479"
  },
  {
    "text": "by step by by Partition by partition read into the memory uh do the map join and all other",
    "start": "502479",
    "end": "511479"
  },
  {
    "text": "things that need to be done cat the data at the moment they can Feit into the",
    "start": "511479",
    "end": "517320"
  },
  {
    "text": "memory and do other transformation so you don't do only just map reduce but you do transformation",
    "start": "517320",
    "end": "524000"
  },
  {
    "text": "over the data and that well that also make code much more clear and",
    "start": "524000",
    "end": "532200"
  },
  {
    "text": "improve performance and really improve performance",
    "start": "532200",
    "end": "536920"
  },
  {
    "text": "and Spark IP well actually not only map reduce well we need it but not only M Ru",
    "start": "539839",
    "end": "548600"
  },
  {
    "text": "but all other it's not it's not the full list but you can see that all the",
    "start": "548600",
    "end": "558279"
  },
  {
    "text": "useful API function are is here Group by",
    "start": "558279",
    "end": "563760"
  },
  {
    "text": "sorts joints all all kind of joints uh a lot of data sources I",
    "start": "563760",
    "end": "570920"
  },
  {
    "text": "included into the are already included or provided for spark to get data from different",
    "start": "570920",
    "end": "576920"
  },
  {
    "text": "sources uh cross joints Co group reduced by K all other things and",
    "start": "576920",
    "end": "585160"
  },
  {
    "text": "then and save data to the different",
    "start": "585160",
    "end": "590240"
  },
  {
    "text": "uh uh different data sources again so writing back so how it work so the main",
    "start": "590240",
    "end": "598000"
  },
  {
    "text": "uh data structure in the spark is a reselling distributed data set",
    "start": "598000",
    "end": "605120"
  },
  {
    "text": "rdd and well actually it can be and it can be covered by data frame that add",
    "start": "605120",
    "end": "611240"
  },
  {
    "text": "names to the column or something like that but inside the system it's uh a uh",
    "start": "611240",
    "end": "617839"
  },
  {
    "text": "distributed data set that contains of the partition of the data each partition",
    "start": "617839",
    "end": "623320"
  },
  {
    "text": "contains a set of objects that can be processed by a map redu use and all",
    "start": "623320",
    "end": "630160"
  },
  {
    "text": "other functions one by one in parallel or can be sorted or and reduced",
    "start": "630160",
    "end": "636480"
  },
  {
    "text": "and so on so far and there are kind of operation over it transformation or action that get data",
    "start": "636480",
    "end": "644120"
  },
  {
    "text": "to the to the storage or get data to the to the your screen print it on a screen",
    "start": "644120",
    "end": "652079"
  },
  {
    "text": "something like that so there are two kind of Function One transform data and",
    "start": "652079",
    "end": "657959"
  },
  {
    "text": "another one save them printed to the screen so when you get this U2 rdd your",
    "start": "657959",
    "end": "666399"
  },
  {
    "text": "rdd defin you run a as I said a special graph you define a special graph that",
    "start": "666399",
    "end": "673320"
  },
  {
    "text": "run on a run over that",
    "start": "673320",
    "end": "679120"
  },
  {
    "text": "Rd so all your function compiled and sent for the to the all the nod to run",
    "start": "679120",
    "end": "684560"
  },
  {
    "text": "on the so we can see that we have two RS here and three straight three stages of",
    "start": "684560",
    "end": "692200"
  },
  {
    "text": "execution uh rgd contains uh consist of partitions of the data that can be",
    "start": "692560",
    "end": "698639"
  },
  {
    "text": "processed as a as one block of the data and it's s say we have RDC that map to",
    "start": "698639",
    "end": "706399"
  },
  {
    "text": "another rdd that filter to another rdd then join with rdd from other",
    "start": "706399",
    "end": "713560"
  },
  {
    "text": "stages uh some of the some of the partition of the Rd can be can be cached",
    "start": "713560",
    "end": "719480"
  },
  {
    "text": "and well actually the spark decide by itself what he want to cach or or you",
    "start": "719480",
    "end": "725920"
  },
  {
    "text": "need to explic uh to explicitly specify",
    "start": "725920",
    "end": "731120"
  },
  {
    "text": "that you need to this data to be cached and then because you are going to use it",
    "start": "731120",
    "end": "736279"
  },
  {
    "text": "once and once again it's really important for any kind of dictionary you use in a in a process",
    "start": "736279",
    "end": "745320"
  },
  {
    "text": "so why we are going why you need to use spark with Cassandra uh the main thing because",
    "start": "749639",
    "end": "757320"
  },
  {
    "text": "Cassandra doesn't have and well actually all new databases this kind of new scale",
    "start": "757320",
    "end": "763680"
  },
  {
    "text": "databases don't have joints Group by and all the things that you need to do",
    "start": "763680",
    "end": "769160"
  },
  {
    "text": "reporting to the management uh and well actually all of that things",
    "start": "769160",
    "end": "774880"
  },
  {
    "text": "that you need to do a data mining for example well actually all of them includ map producer kind of",
    "start": "774880",
    "end": "781079"
  },
  {
    "text": "them and Cassandra also have a producer but as I said before map producers really small really slow and not",
    "start": "781079",
    "end": "788279"
  },
  {
    "text": "convenient to Cod um the other thing that why we need",
    "start": "788279",
    "end": "796240"
  },
  {
    "text": "to use a Cassandra with a spark because",
    "start": "796240",
    "end": "802040"
  },
  {
    "text": "of that great uh driver that was built by data Stacks that and I will talk",
    "start": "802040",
    "end": "808440"
  },
  {
    "text": "about it that a little bit that allow it to map all the data from a from your",
    "start": "808440",
    "end": "814040"
  },
  {
    "text": "Cassandra to the to the to the spark and back with two line of code that really",
    "start": "814040",
    "end": "821399"
  },
  {
    "text": "important well to have less code you less code you have less bu you have you know",
    "start": "821399",
    "end": "829000"
  },
  {
    "text": "it's as I say there is a it's it's open",
    "start": "831279",
    "end": "836440"
  },
  {
    "text": "sourced so we can use it for free well actually we built our database",
    "start": "836440",
    "end": "843240"
  },
  {
    "text": "platform that includes spark and the driver and a lot of other security",
    "start": "843240",
    "end": "849639"
  },
  {
    "text": "stuff but you can use it for free and a lot of people use it use Cassandra with",
    "start": "849639",
    "end": "855720"
  },
  {
    "text": "well actually the classical um uh data flow that I see right now",
    "start": "855720",
    "end": "862079"
  },
  {
    "text": "looks like Kafka spark Cassandra back to",
    "start": "862079",
    "end": "867680"
  },
  {
    "text": "spark back to candra ra visualization and your and your website so it's a",
    "start": "867680",
    "end": "874000"
  },
  {
    "text": "classical data flow I see a lot of time in right at last at last",
    "start": "874000",
    "end": "880560"
  },
  {
    "text": "time so what the Cassandra spark driver do expose",
    "start": "880560",
    "end": "886240"
  },
  {
    "text": "cassander table as already so with one comment like I will",
    "start": "886240",
    "end": "891360"
  },
  {
    "text": "show you it read data right that from Cassandra in parallel that's important",
    "start": "891360",
    "end": "896639"
  },
  {
    "text": "so if you ccate uh spark nodes with Cassandra nodes you",
    "start": "896639",
    "end": "902519"
  },
  {
    "text": "will get data locality you will get uh parallel writing and reading with the into the",
    "start": "902519",
    "end": "909199"
  },
  {
    "text": "local node and Driver know about the partitioning about",
    "start": "909199",
    "end": "914600"
  },
  {
    "text": "the about the spark partitioning and about the uh Cassandra sharding so it's",
    "start": "914600",
    "end": "922360"
  },
  {
    "text": "do try to do everything with data locality in mind it have automatic m of",
    "start": "922360",
    "end": "929639"
  },
  {
    "text": "the Cassandra data tapes to the spark and back that also really",
    "start": "929639",
    "end": "936480"
  },
  {
    "text": "important so all all the data is supporting predic cut push down is",
    "start": "936480",
    "end": "942040"
  },
  {
    "text": "supported so we don't get all the data into the memory but all the filters that you write not all but most of the",
    "start": "942040",
    "end": "950360"
  },
  {
    "text": "filters that can be afforded by a Cassandra push down to the Cassandra and",
    "start": "950360",
    "end": "955880"
  },
  {
    "text": "we filter data on only that data that you really need for the analysis so that",
    "start": "955880",
    "end": "961360"
  },
  {
    "text": "so they can fit into the memory well supports spark streaming",
    "start": "961360",
    "end": "966519"
  },
  {
    "text": "Scala Java python so well actually",
    "start": "966519",
    "end": "972079"
  },
  {
    "text": "python is supported through G python in a spark so it's it's always",
    "start": "972079",
    "end": "978759"
  },
  {
    "text": "supported and well actually connections look really simple you just pass",
    "start": "979360",
    "end": "986440"
  },
  {
    "text": "the at least one note of the IP address of at least one not of",
    "start": "986440",
    "end": "991639"
  },
  {
    "text": "the Cassandra you pass the spark uh spark",
    "start": "991639",
    "end": "998759"
  },
  {
    "text": "meister and that's it so when you start it the next thing that we will do we are",
    "start": "998759",
    "end": "1004160"
  },
  {
    "text": "going we will ready we ready to get access to the data let's do simple example just create table and",
    "start": "1004160",
    "end": "1013040"
  },
  {
    "text": "put data into it well you can see you you can see SQL here right but it's not",
    "start": "1013040",
    "end": "1018920"
  },
  {
    "text": "SQL it's SQL so we can do it with Cassandra and well actually remember",
    "start": "1018920",
    "end": "1024038"
  },
  {
    "text": "that SQL not a real SQL it has some limitations after",
    "start": "1024039",
    "end": "1030558"
  },
  {
    "text": "all so now to get data from the Cassandra you need to write only the first line in the second",
    "start": "1031439",
    "end": "1038600"
  },
  {
    "text": "block uh Cassandra you Skype get data from Cassandra table and all the data",
    "start": "1038600",
    "end": "1044438"
  },
  {
    "text": "from the from the table from the cas space test and from the table words",
    "start": "1044439",
    "end": "1050360"
  },
  {
    "text": "we'll be get into the Rd and you can do all other things with a with it in this",
    "start": "1050360",
    "end": "1057240"
  },
  {
    "text": "case we just printed right we get the action to array and get and move data",
    "start": "1057240",
    "end": "1063000"
  },
  {
    "text": "from the uh can you see the Cod on the this so is it it's okay right",
    "start": "1063000",
    "end": "1070279"
  },
  {
    "text": "cool okay so we can do the so in in this case you get really simple thing right",
    "start": "1070600",
    "end": "1077039"
  },
  {
    "text": "you uh without any filtering without any map reduce we just print it for the well",
    "start": "1077039",
    "end": "1083480"
  },
  {
    "text": "and this is a Scala code so don't be afraid about Scala you can see the scalar code is really simple well actually when I do the talk they say oh",
    "start": "1083480",
    "end": "1090640"
  },
  {
    "text": "Scala that's strange language I write only Java I recommend not write on the",
    "start": "1090640",
    "end": "1096520"
  },
  {
    "text": "Java with Java owners because Java is too many words to do the simple",
    "start": "1096520",
    "end": "1102400"
  },
  {
    "text": "things espe especially in this in this in this",
    "start": "1102400",
    "end": "1107480"
  },
  {
    "text": "analytics side well when you I I I I I don't say that Java is bad Java is good",
    "start": "1107480",
    "end": "1113799"
  },
  {
    "text": "but for the scripting well for the small scripting don't use Java for the small scripting well",
    "start": "1113799",
    "end": "1120640"
  },
  {
    "text": "actually Java eight is much better U Lambda helps",
    "start": "1120640",
    "end": "1126799"
  },
  {
    "text": "uh saving data is really simple again it's only one comment like safe to",
    "start": "1126799",
    "end": "1133840"
  },
  {
    "text": "Cassandra and all the magic will happen inside the driver in this case well",
    "start": "1133840",
    "end": "1141360"
  },
  {
    "text": "let's see what what the case do I create an array a sequence in this",
    "start": "1141360",
    "end": "1146559"
  },
  {
    "text": "case that contains two tles w and count right so I have two",
    "start": "1146559",
    "end": "1153640"
  },
  {
    "text": "rows to put to the Cassandra in this case the parallelize command from the",
    "start": "1153640",
    "end": "1160240"
  },
  {
    "text": "spark context in a spark allow me to make this array parallel to it's make",
    "start": "1160240",
    "end": "1167960"
  },
  {
    "text": "its move do from array to to rdd so it become parallel so if",
    "start": "1167960",
    "end": "1175000"
  },
  {
    "text": "you want to do if you have any small uh arrays that you need to join",
    "start": "1175000",
    "end": "1181840"
  },
  {
    "text": "with a big data you do something like that you declare it or read it from file declare it and do parallel",
    "start": "1181840",
    "end": "1191399"
  },
  {
    "text": "Li and now we have four four l in our",
    "start": "1192600",
    "end": "1201158"
  },
  {
    "text": "table type match well but actually it's it's for the reference uh all types are mapped back",
    "start": "1201720",
    "end": "1209360"
  },
  {
    "text": "and forth from Cassandra to spark from spark to Cassandra and well it's you can",
    "start": "1209360",
    "end": "1216760"
  },
  {
    "text": "get it so it's it's it not need to be described uh the next thing that is very",
    "start": "1216760",
    "end": "1223960"
  },
  {
    "text": "interesting is that we can map",
    "start": "1223960",
    "end": "1229919"
  },
  {
    "text": "tables to objects and orm are in place and work",
    "start": "1229919",
    "end": "1235919"
  },
  {
    "text": "pretty as usual pretty simple so we let's say we have a table that describe",
    "start": "1235919",
    "end": "1244400"
  },
  {
    "text": "car we can create class that will describe the same thing but in",
    "start": "1244400",
    "end": "1251720"
  },
  {
    "text": "Scala in this case it's a case class uh it's scalar objects that have",
    "start": "1251720",
    "end": "1259240"
  },
  {
    "text": "some options for pattern matching and mapping but the code is as this and then you can",
    "start": "1259240",
    "end": "1269000"
  },
  {
    "text": "do candra table vehicle uh the this bracket means uh",
    "start": "1269000",
    "end": "1276039"
  },
  {
    "text": "generics it's not a generic in a Scala but in Java world it's called it's like a generic so I will get the rdd of a",
    "start": "1276039",
    "end": "1284760"
  },
  {
    "text": "vehicles of the vehicle object so nothing else need to be done for that",
    "start": "1284760",
    "end": "1291279"
  },
  {
    "text": "kind of simple mapping well actually the struct sets rays in an object also will",
    "start": "1291279",
    "end": "1298760"
  },
  {
    "text": "be mapped into the struct into the array set of the scalar world so the complex",
    "start": "1298760",
    "end": "1305640"
  },
  {
    "text": "data also mapped really",
    "start": "1305640",
    "end": "1309320"
  },
  {
    "text": "simple and well actually it's uh there is plugable interface for mapper in any",
    "start": "1312400",
    "end": "1317720"
  },
  {
    "text": "case",
    "start": "1317720",
    "end": "1320520"
  },
  {
    "text": "Ser uh predicate push down really important thing that I said before if",
    "start": "1323120",
    "end": "1328640"
  },
  {
    "text": "you need to select some data only some set of",
    "start": "1328640",
    "end": "1336200"
  },
  {
    "text": "fields you can do a you can Define the name of columns that you need to",
    "start": "1336200",
    "end": "1343240"
  },
  {
    "text": "select if you need to select draw you can also specify where that will will do",
    "start": "1343240",
    "end": "1348799"
  },
  {
    "text": "filtering for and filter only that column that you need okay so that was part about how to",
    "start": "1348799",
    "end": "1358480"
  },
  {
    "text": "connect Cassandra with spark and now we can talk uh a little bit more about what",
    "start": "1358480",
    "end": "1366919"
  },
  {
    "text": "libraries are provided on spark yeah questions yeah one of the things that uh",
    "start": "1366919",
    "end": "1373760"
  },
  {
    "text": "the problem of no SQL databases is that you are not allowed to do table scans",
    "start": "1373760",
    "end": "1379919"
  },
  {
    "text": "is there done anything in the driver to to kind of minimize the performance of",
    "start": "1379919",
    "end": "1386559"
  },
  {
    "text": "reading so much data out of it uh well uh could you repe the",
    "start": "1386559",
    "end": "1393480"
  },
  {
    "text": "question so what the predicate push down when you do where my my question is that",
    "start": "1393480",
    "end": "1399360"
  },
  {
    "text": "usually when you have no SQL databases is that it's very hard to do a query and",
    "start": "1399360",
    "end": "1404799"
  },
  {
    "text": "take out data because that would that's a big performance it that's least I'm M",
    "start": "1404799",
    "end": "1411279"
  },
  {
    "text": "men so that's where I come from uh okay so so spark takes out a lot of data so I",
    "start": "1411279",
    "end": "1417840"
  },
  {
    "text": "just is there anything inside the driver that takes care of of kind of taking out",
    "start": "1417840",
    "end": "1423720"
  },
  {
    "text": "the data in the a good way without killing performance okay the question was about bulk loading well actually all",
    "start": "1423720",
    "end": "1429679"
  },
  {
    "text": "no school data are not very good with getting bulk data out of the database",
    "start": "1429679",
    "end": "1435679"
  },
  {
    "text": "you know well actually they all have K value thing inside the database right",
    "start": "1435679",
    "end": "1441840"
  },
  {
    "text": "and if you want to get data fast you you you you drop K into the database and get",
    "start": "1441840",
    "end": "1447440"
  },
  {
    "text": "the row out of the database and that's all well actually all the all that kind",
    "start": "1447440",
    "end": "1452880"
  },
  {
    "text": "of indexing and all other things that happened in well any that kind of no",
    "start": "1452880",
    "end": "1458039"
  },
  {
    "text": "scol database works like you query index get case and then",
    "start": "1458039",
    "end": "1465200"
  },
  {
    "text": "get all the row by K by K right and when you do filtering or something",
    "start": "1465200",
    "end": "1471200"
  },
  {
    "text": "like that you get all the row and just filter it one by one and the question is how to do bul loading uh and what the",
    "start": "1471200",
    "end": "1478320"
  },
  {
    "text": "driver do for the bulk loading well actually for bulk processing everybody prod uh the the idea is that what the",
    "start": "1478320",
    "end": "1484520"
  },
  {
    "text": "driver do driver get architecture of the of the row of the of",
    "start": "1484520",
    "end": "1492640"
  },
  {
    "text": "the of the cycle of of Cassandra nodes and knows",
    "start": "1492640",
    "end": "1498600"
  },
  {
    "text": "what data what dat knows the shart what data resides on which which data resides",
    "start": "1498600",
    "end": "1504840"
  },
  {
    "text": "on which Cassandra node and you can decide well you get get it really clearly by uh a token toen ring ranges",
    "start": "1504840",
    "end": "1515159"
  },
  {
    "text": "in a Cassandra World well actually in other database do the same I will talk about Cassandra because it's really",
    "start": "1515159",
    "end": "1521799"
  },
  {
    "text": "simple to implement so you know data ranges now as you have a data locality",
    "start": "1521799",
    "end": "1527039"
  },
  {
    "text": "you can query the data from a particular cassander node in parallel sending that if you have the",
    "start": "1527039",
    "end": "1535520"
  },
  {
    "text": "same node of the spark on the same node you can query only this range of case",
    "start": "1535520",
    "end": "1542880"
  },
  {
    "text": "that iide to the to that Cassandra node so we can get all the data in parallel",
    "start": "1542880",
    "end": "1550840"
  },
  {
    "text": "from all the nodes yeah and it's much more so you don't do",
    "start": "1550840",
    "end": "1556000"
  },
  {
    "text": "it through the you don't do it through the drivers through the one point of access you do it in parallel on each",
    "start": "1556000",
    "end": "1562520"
  },
  {
    "text": "node locally and then well after that filter",
    "start": "1562520",
    "end": "1567960"
  },
  {
    "text": "that ensure that you will your query from a spark will hit only",
    "start": "1567960",
    "end": "1574279"
  },
  {
    "text": "one data node of Cassandra you also add the filtering so it the Cassandra will also",
    "start": "1574279",
    "end": "1582320"
  },
  {
    "text": "apply this where and filter and filter more data out of this range that it",
    "start": "1582320",
    "end": "1588880"
  },
  {
    "text": "canist so in a good way uh in a good in most cases no NV interaction happened so",
    "start": "1588880",
    "end": "1596840"
  },
  {
    "text": "Every Spark node talk directly to the its own cassander node and get data out of",
    "start": "1596840",
    "end": "1603398"
  },
  {
    "text": "them but this calculation of the Ring ranges and so slow down start start of",
    "start": "1604080",
    "end": "1610200"
  },
  {
    "text": "the of the of the execution so remember it's when you talk about spark I talk",
    "start": "1610200",
    "end": "1616520"
  },
  {
    "text": "about analytics not about spark streaming but about spark itself about",
    "start": "1616520",
    "end": "1621919"
  },
  {
    "text": "this B processing so right now we talk about B processing when we load a lot of data process it and save results it's",
    "start": "1621919",
    "end": "1629480"
  },
  {
    "text": "not really fast because it's need time to calculate all the things to optimize",
    "start": "1629480",
    "end": "1634520"
  },
  {
    "text": "the graph graph execution and while the all the optimization done it starts",
    "start": "1634520",
    "end": "1640399"
  },
  {
    "text": "processing fast and in bulk uh before well well actually it's a good time to",
    "start": "1640399",
    "end": "1647640"
  },
  {
    "text": "answer some question and then I will start talk about spark",
    "start": "1647640",
    "end": "1653120"
  },
  {
    "text": "modus questions more questions from the are you storing the data in a column",
    "start": "1653120",
    "end": "1661120"
  },
  {
    "text": "format yeah sure it's actually actually it's a column for format well well",
    "start": "1661120",
    "end": "1666360"
  },
  {
    "text": "actually it's uh or what's the DAT",
    "start": "1666360",
    "end": "1673000"
  },
  {
    "text": "format well well actually it's a uh it's a kind of sess tables where you saw the",
    "start": "1673000",
    "end": "1679120"
  },
  {
    "text": "key column name and the value so it's a it's a wide row so you",
    "start": "1679120",
    "end": "1685919"
  },
  {
    "text": "you you use St wide row with a k and a value and value contains column names",
    "start": "1685919",
    "end": "1691559"
  },
  {
    "text": "value of the column column names value of the column well actually there are in a Cassandra 3.0 will be introduced a new",
    "start": "1691559",
    "end": "1699080"
  },
  {
    "text": "uh storage engine that will be uh that will uh better fit cql",
    "start": "1699080",
    "end": "1707200"
  },
  {
    "text": "columns which question basically what he wants to knows can you load the columns directly that's what he was asking well",
    "start": "1707200",
    "end": "1713760"
  },
  {
    "text": "actually you can well actually in a you always wrot columns directly right yeah but you're cracking them out of the out",
    "start": "1713760",
    "end": "1720360"
  },
  {
    "text": "of the blob you're not taking them as a as a continuous column and loading them",
    "start": "1720360",
    "end": "1725559"
  },
  {
    "text": "in well actually all all the thing happened under the H in a in a Cassandra",
    "start": "1725559",
    "end": "1731000"
  },
  {
    "text": "engine yeah but they have to crack them so there's a performance problem yeah",
    "start": "1731000",
    "end": "1736679"
  },
  {
    "text": "sure but actually when you do it in parallel it's not it become much less",
    "start": "1736679",
    "end": "1741919"
  },
  {
    "text": "performance problem yeah M memory memory is not free",
    "start": "1741919",
    "end": "1748600"
  },
  {
    "text": "you know yeah the only important the only important thing in a in a in a current",
    "start": "1748600",
    "end": "1755200"
  },
  {
    "text": "Vol that memory is too expensive uh disc is too slow memory is",
    "start": "1755200",
    "end": "1762360"
  },
  {
    "text": "too expensive CPUs is for free right uh",
    "start": "1762360",
    "end": "1770039"
  },
  {
    "text": "okay more model so right now we talk about the basic of the",
    "start": "1773279",
    "end": "1778600"
  },
  {
    "text": "spark so the basic of the spark is that Rd Vault and this uh uh execution graph",
    "start": "1778600",
    "end": "1786360"
  },
  {
    "text": "processing machine and based on this execution graph uh based on this",
    "start": "1786360",
    "end": "1792640"
  },
  {
    "text": "execution we can uh the spark provide more convenience vent",
    "start": "1792640",
    "end": "1799039"
  },
  {
    "text": "interfaces the first one is a spark",
    "start": "1799039",
    "end": "1803278"
  },
  {
    "text": "SQL that well actually hi compatible so I hope everybody knows what the hi is",
    "start": "1805240",
    "end": "1811159"
  },
  {
    "text": "it's it's AQL engine in the hadu",
    "start": "1811159",
    "end": "1815080"
  },
  {
    "text": "world and well actually spark SQL contains esol query engine on top of the",
    "start": "1819240",
    "end": "1825640"
  },
  {
    "text": "spark that well right now in sp. 5 it's",
    "start": "1825640",
    "end": "1832720"
  },
  {
    "text": "around comparable with a TZ but please uh so if you need only",
    "start": "1832720",
    "end": "1838760"
  },
  {
    "text": "test or something like that probably you can stay on this test but if you need",
    "start": "1838760",
    "end": "1844039"
  },
  {
    "text": "other spark thing like a like a question right now I I think",
    "start": "1844039",
    "end": "1851000"
  },
  {
    "text": "right now spark spark compatible with test performance it has all the have",
    "start": "1851000",
    "end": "1858320"
  },
  {
    "text": "compatible thing like gtbc support gtbc server support UDF type metadata all",
    "start": "1858320",
    "end": "1865600"
  },
  {
    "text": "other things but support spark support uh in memory processing so you can cache",
    "start": "1865600",
    "end": "1871960"
  },
  {
    "text": "the table and it will be start in spark rgd and you well you will run you will",
    "start": "1871960",
    "end": "1878039"
  },
  {
    "text": "so you can get fully in memory database with a spark SQL but still",
    "start": "1878039",
    "end": "1884080"
  },
  {
    "text": "analytic analytic spark is SC don't expect m Mill so if you need",
    "start": "1884080",
    "end": "1889480"
  },
  {
    "text": "milliseconds in response you use Cassandra directly if you need any fast engine you",
    "start": "1889480",
    "end": "1896000"
  },
  {
    "text": "can use spark SQL and well actually push down predicate to candra well not not",
    "start": "1896000",
    "end": "1901760"
  },
  {
    "text": "not only to candra support most of the most of the databases that that have a",
    "start": "1901760",
    "end": "1908519"
  },
  {
    "text": "predicate you can push down predicate and actually example of",
    "start": "1908519",
    "end": "1916360"
  },
  {
    "text": "use uh SQL context to use spark SQL you need in",
    "start": "1916360",
    "end": "1921679"
  },
  {
    "text": "a program well the good way you you can you can use it in a program not only in",
    "start": "1921679",
    "end": "1927399"
  },
  {
    "text": "a command line so to use to Conn to use a Cassandra spark context you just",
    "start": "1927399",
    "end": "1934880"
  },
  {
    "text": "create Cassandra SQL context and then use SQL",
    "start": "1934880",
    "end": "1941279"
  },
  {
    "text": "to hi compatible squ it's not SQL in this case it's Esq SQL the",
    "start": "1941279",
    "end": "1949840"
  },
  {
    "text": "hsql SQL that allow you to do all this thing with a with a",
    "start": "1950559",
    "end": "1957639"
  },
  {
    "text": "databases uh not mentioned here but well if you like that kind of command line",
    "start": "1957639",
    "end": "1965559"
  },
  {
    "text": "like s SQL shell you can do it with SQL as well uh you can write run SQL shell",
    "start": "1965559",
    "end": "1973279"
  },
  {
    "text": "you can write gdbc server and connect it by gdbc driver by have gdbc driver and",
    "start": "1973279",
    "end": "1979240"
  },
  {
    "text": "get all the data out of",
    "start": "1979240",
    "end": "1982559"
  },
  {
    "text": "that uh most interesting part of the spark and well in",
    "start": "1984600",
    "end": "1991919"
  },
  {
    "text": "the data processing W right now is a spark",
    "start": "1991919",
    "end": "1997080"
  },
  {
    "text": "streaming so data streaming is when you get data from a stream from kfka stream",
    "start": "1998080",
    "end": "2003600"
  },
  {
    "text": "for any other processing on the Fly and store results or sh show results to the",
    "start": "2003600",
    "end": "2009880"
  },
  {
    "text": "user uh spark uh provide a and well actually",
    "start": "2009880",
    "end": "2015519"
  },
  {
    "text": "when when I when everybody start talk about streaming they start talking about",
    "start": "2015519",
    "end": "2021039"
  },
  {
    "text": "storm or something like samza and the main and the first thing",
    "start": "2021039",
    "end": "2026519"
  },
  {
    "text": "that you start to do in a storm for example you start do data buffering",
    "start": "2026519",
    "end": "2032840"
  },
  {
    "text": "because you cannot write data one by one into any databases",
    "start": "2032840",
    "end": "2038679"
  },
  {
    "text": "you start buffering the data into the Set uh small data sets and then store it",
    "start": "2038679",
    "end": "2043840"
  },
  {
    "text": "or process it in a small batches spark goes another way they",
    "start": "2043840",
    "end": "2050158"
  },
  {
    "text": "start batching data for you at the beginning so it put data into",
    "start": "2050159",
    "end": "2056599"
  },
  {
    "text": "microbes microbes organized into small rdd and so you don't get the stream of",
    "start": "2056599",
    "end": "2062720"
  },
  {
    "text": "the events but you get a stream of rdd so you get a the distributed stream",
    "start": "2062720",
    "end": "2070000"
  },
  {
    "text": "that contains a r that can be processed in parallel so after",
    "start": "2070000",
    "end": "2075679"
  },
  {
    "text": "that you back you get after this optimization you get back to",
    "start": "2075679",
    "end": "2083440"
  },
  {
    "text": "the all these functions that you get in a to work that we talked before so we",
    "start": "2083839",
    "end": "2090158"
  },
  {
    "text": "can do on a streaming in streaming qu you can do all the things like joining like uh group grp by filtering",
    "start": "2090159",
    "end": "2100040"
  },
  {
    "text": "and so on so far and let's see how it will how it's API will looks",
    "start": "2100040",
    "end": "2108160"
  },
  {
    "text": "like in this case we will read data out of the just out of the socket and then",
    "start": "2108440",
    "end": "2114960"
  },
  {
    "text": "do the same word count word count",
    "start": "2114960",
    "end": "2120280"
  },
  {
    "text": "example we def find the stream distributor stream like",
    "start": "2120280",
    "end": "2126400"
  },
  {
    "text": "Lines Just by getting the streaming context and defining the IP address when",
    "start": "2126400",
    "end": "2133800"
  },
  {
    "text": "we will listen for the data then we will do all the same thing we will map the data map rdd to the stream",
    "start": "2133800",
    "end": "2143000"
  },
  {
    "text": "by splitting it to the Ws map wordss to the tople W and number and",
    "start": "2143000",
    "end": "2151280"
  },
  {
    "text": "Reduce by K by sumon them so we will get a d stream back so the word count it's",
    "start": "2151280",
    "end": "2158599"
  },
  {
    "text": "also gstream that that will contains a group of data that will be produced uh once a",
    "start": "2158599",
    "end": "2167800"
  },
  {
    "text": "second if you can take a look at the first line that you'll get at once a second you will get the new new",
    "start": "2167800",
    "end": "2175560"
  },
  {
    "text": "rdd that will contains how much work to get at this second uh and you can and to save it to",
    "start": "2175560",
    "end": "2184280"
  },
  {
    "text": "the Cassandra is really simple you just save to Cassandra and you will get the new date in Cassandra each second if you",
    "start": "2184280",
    "end": "2191520"
  },
  {
    "text": "if you say it word count print you will get the output once once in a second",
    "start": "2191520",
    "end": "2199400"
  },
  {
    "text": "yeah sure in order to understand this example is this something that then continues to run as long as data coming",
    "start": "2199400",
    "end": "2205040"
  },
  {
    "text": "in yeah get run once no it's continuously run well actually as a stream as expected it's streaming it's",
    "start": "2205040",
    "end": "2212960"
  },
  {
    "text": "continuously run and once a second you get a result so but if",
    "start": "2212960",
    "end": "2218440"
  },
  {
    "text": "look at my debugger the await termination hangs there it blocks until there's no more input to the streen well",
    "start": "2218440",
    "end": "2225240"
  },
  {
    "text": "actually yes it will be it will be blocks and we'll wait till the new data come to the stream and we'll just",
    "start": "2225240",
    "end": "2231440"
  },
  {
    "text": "produce empty rdgs if you if you don't get data in a second you just get a",
    "start": "2231440",
    "end": "2237200"
  },
  {
    "text": "empty rdd and wait for the next one so each SEC each second you will get",
    "start": "2237200",
    "end": "2242720"
  },
  {
    "text": "data well it's important difference as I say against other system when you",
    "start": "2242720",
    "end": "2248920"
  },
  {
    "text": "get when you get event by event and process it event by event in this case",
    "start": "2248920",
    "end": "2254160"
  },
  {
    "text": "you get the time time bound block of the data per second you can say per minutes",
    "start": "2254160",
    "end": "2261599"
  },
  {
    "text": "and so on so far well actually there are windows functions that allow you to",
    "start": "2261599",
    "end": "2268079"
  },
  {
    "text": "get every second last 30 minutes of the data for example you can get state so",
    "start": "2268079",
    "end": "2275960"
  },
  {
    "text": "you can get a Des this stream when you get the state an update state for each",
    "start": "2275960",
    "end": "2282280"
  },
  {
    "text": "for each new event but after all well actually this streaming example important thing",
    "start": "2282280",
    "end": "2290520"
  },
  {
    "text": "that uh that way to do streaming is much more performance- wise because you get",
    "start": "2290520",
    "end": "2296760"
  },
  {
    "text": "buffering out of the out of the box and yeah but you lose the fact that",
    "start": "2296760",
    "end": "2303599"
  },
  {
    "text": "you've got something important in the Stream you're essentially batching them so you're you're pulling everything yeah",
    "start": "2303599",
    "end": "2309240"
  },
  {
    "text": "so if I've got a new stock price coming in and it's higher I'm going to have to wait until that window so I'm going to",
    "start": "2309240",
    "end": "2314760"
  },
  {
    "text": "miss it not be able to make yeah yeah sure well actually as I say it's a different kind of uh so if you're ready",
    "start": "2314760",
    "end": "2322880"
  },
  {
    "text": "to wait for say 100 milliseconds it's the smallest thing you can go get",
    "start": "2322880",
    "end": "2328720"
  },
  {
    "text": "probably 30 millisecond you can get the B with a 30 millisecond and configurable it's okay for you but if you if if you",
    "start": "2328720",
    "end": "2335720"
  },
  {
    "text": "want if you want react on event you use another tool well that's true you use a",
    "start": "2335720",
    "end": "2341839"
  },
  {
    "text": "say with kfka you use you use samza or storm something like",
    "start": "2341839",
    "end": "2348520"
  },
  {
    "text": "that in this case you do it event by event but well actually uh back to the",
    "start": "2348520",
    "end": "2354640"
  },
  {
    "text": "back back to the storm the first thing that everybody roll for storm is a",
    "start": "2354640",
    "end": "2360000"
  },
  {
    "text": "memory buffer to buffer data and write it in beses well actually I see it every",
    "start": "2360000",
    "end": "2365480"
  },
  {
    "text": "time uh except except stock prices but no one",
    "start": "2365480",
    "end": "2370680"
  },
  {
    "text": "well no one do stock prices on storm well actually stock pric is done",
    "start": "2370680",
    "end": "2376000"
  },
  {
    "text": "on by on the lcal level as I say it usually because every microsc means all",
    "start": "2376000",
    "end": "2384040"
  },
  {
    "text": "other things is much better when",
    "start": "2384040",
    "end": "2387560"
  },
  {
    "text": "budget okay next thing is um machine learning",
    "start": "2391480",
    "end": "2396920"
  },
  {
    "text": "uh and uh this well",
    "start": "2398000",
    "end": "2404000"
  },
  {
    "text": "probably the most scalable machine learning library I knows well actually",
    "start": "2404720",
    "end": "2411319"
  },
  {
    "text": "if you need the best machine learning libraries you use R and all this and all the",
    "start": "2411319",
    "end": "2417240"
  },
  {
    "text": "things uh but unfortunately R scalable not more than to the one machine and",
    "start": "2417240",
    "end": "2422400"
  },
  {
    "text": "well actually when you when someone say I use API with r i want to see that people",
    "start": "2422400",
    "end": "2427839"
  },
  {
    "text": "uh no one use it well usually you have a mathematicians who do R scripting on its",
    "start": "2427839",
    "end": "2434440"
  },
  {
    "text": "own small data set and then you have a Java developer who do it on might produce the same thing um",
    "start": "2434440",
    "end": "2442760"
  },
  {
    "text": "well uh spark uh spend a lot of time for",
    "start": "2442760",
    "end": "2448240"
  },
  {
    "text": "developing scalable algorithms it's still really small set",
    "start": "2448240",
    "end": "2454079"
  },
  {
    "text": "of algorithms from the r point of view but for the Practical usage they have",
    "start": "2454079",
    "end": "2460280"
  },
  {
    "text": "all all you need they have uh classification classification algorithm",
    "start": "2460280",
    "end": "2466599"
  },
  {
    "text": "basic one a little bit clustering uh all set of linear",
    "start": "2466599",
    "end": "2473599"
  },
  {
    "text": "regression uh recommendation engine can be built on the collaborative",
    "start": "2473599",
    "end": "2479640"
  },
  {
    "text": "filtering what we have is the time okay I have 20 minutes so uh so we have time",
    "start": "2479640",
    "end": "2485920"
  },
  {
    "text": "to talk about about them this is my lovely part I can talk about it for a long time di ruction",
    "start": "2485920",
    "end": "2493640"
  },
  {
    "text": "frequently pattern mapping and well actually they Implement work to work yep um what do you mean by theor work in",
    "start": "2493640",
    "end": "2503640"
  },
  {
    "text": "a distri way and you can scale across",
    "start": "2503640",
    "end": "2508760"
  },
  {
    "text": "the I was trying to work with workor and I found out that I have too many data in",
    "start": "2508760",
    "end": "2514920"
  },
  {
    "text": "the last step huge Matrix that actually everything to one and this",
    "start": "2514920",
    "end": "2523359"
  },
  {
    "text": "one cannot can you elaborate well actually well actually you select a bad bad",
    "start": "2523599",
    "end": "2530200"
  },
  {
    "text": "example word is not best implementation here and well actually I also was",
    "start": "2530200",
    "end": "2535760"
  },
  {
    "text": "surprised well actually I also was surprised that they that the last step is executed on the one machine but this",
    "start": "2535760",
    "end": "2543079"
  },
  {
    "text": "is how the work to work work unfortunately and I hope they will well",
    "start": "2543079",
    "end": "2549960"
  },
  {
    "text": "it's it's well there are only two implementation of work to work in the world right the native one and this one",
    "start": "2549960",
    "end": "2556839"
  },
  {
    "text": "so it's not this particular implementation is not perfect one but because I wonder if you promote like",
    "start": "2556839",
    "end": "2564119"
  },
  {
    "text": "distributed machine learning this one should not be on the list it's today",
    "start": "2564119",
    "end": "2570440"
  },
  {
    "text": "it's not possible well actually they do everything except the last step",
    "start": "2570440",
    "end": "2578880"
  },
  {
    "text": "distributed well well well actually for the for a lot of uh for a lot of machine learning well actually to scale machine",
    "start": "2578880",
    "end": "2584920"
  },
  {
    "text": "learning algorithm is not simple thing and well actually and and some of that algorithms up up there also have that",
    "start": "2584920",
    "end": "2592359"
  },
  {
    "text": "kind of the last step but it's still it's still faster than",
    "start": "2592359",
    "end": "2598359"
  },
  {
    "text": "than one step algorithm well it's it's a hard it's the hardest and most",
    "start": "2598359",
    "end": "2603520"
  },
  {
    "text": "interesting part of the of of the sparker and of the current world today",
    "start": "2603520",
    "end": "2608680"
  },
  {
    "text": "as as previous presenter told us so I will not show you a word",
    "start": "2608680",
    "end": "2617920"
  },
  {
    "text": "example let's solve let's see how the how the",
    "start": "2618559",
    "end": "2624640"
  },
  {
    "text": "classical classificational uh Al uh problem can be solved with a spark",
    "start": "2624640",
    "end": "2631559"
  },
  {
    "text": "machine learning algorithm I will solve one of that problem that we'll see on",
    "start": "2631559",
    "end": "2636640"
  },
  {
    "text": "previous uh in in a previous presentation in this case we will do",
    "start": "2636640",
    "end": "2644558"
  },
  {
    "text": "a uh select uh understanding what type of",
    "start": "2645359",
    "end": "2652040"
  },
  {
    "text": "areas we see so there is a classical data set for machine",
    "start": "2652040",
    "end": "2657680"
  },
  {
    "text": "learning examples that use that use arises and",
    "start": "2657680",
    "end": "2665160"
  },
  {
    "text": "four measures of the of this flower and let's uh teach our algorithm",
    "start": "2665160",
    "end": "2672960"
  },
  {
    "text": "to understand what flower he can see so what we will do we will read our data",
    "start": "2672960",
    "end": "2679680"
  },
  {
    "text": "set of the out of the Cassandra prepare the data and this is",
    "start": "2679680",
    "end": "2686599"
  },
  {
    "text": "well always the the longest well the hardest think always is to prepare data",
    "start": "2686599",
    "end": "2694000"
  },
  {
    "text": "in this case algorithm expects that we will get label at point so we need a",
    "start": "2694000",
    "end": "2699720"
  },
  {
    "text": "features set of arrays of features and a label in this case we will the label",
    "start": "2699720",
    "end": "2705800"
  },
  {
    "text": "will be the name of the species of the flower the other two steps are really",
    "start": "2705800",
    "end": "2712720"
  },
  {
    "text": "well actually I skip a lot of machine learning steps here and well we will we",
    "start": "2712720",
    "end": "2719119"
  },
  {
    "text": "can talk about them out of this discussion so the the simple step is",
    "start": "2719119",
    "end": "2724760"
  },
  {
    "text": "train the algorithm in this case it's in this case it's naive bias one of the multi classification",
    "start": "2724760",
    "end": "2733160"
  },
  {
    "text": "algorithm and then we can we can get model out of that and then we can do",
    "start": "2733160",
    "end": "2738359"
  },
  {
    "text": "just ask the model I have that kind of measurement what the flowers it is and",
    "start": "2738359",
    "end": "2745640"
  },
  {
    "text": "to do that I need to say predict and and the and Sy will say it's a that kind of",
    "start": "2745640",
    "end": "2754480"
  },
  {
    "text": "flower so it's look really s Le at this point of view",
    "start": "2754480",
    "end": "2761040"
  },
  {
    "text": "well in a in the latest spark they introduced spark",
    "start": "2761040",
    "end": "2766559"
  },
  {
    "text": "ml that allow it to build the more professionally",
    "start": "2766559",
    "end": "2773359"
  },
  {
    "text": "looking um machine learning pipeline so they allow to build pipelines with a",
    "start": "2773359",
    "end": "2780119"
  },
  {
    "text": "data preparation uh cross validation and getting results but the",
    "start": "2780119",
    "end": "2787920"
  },
  {
    "text": "simplest way you can do it well as you can see machine learning is really simple five lines of",
    "start": "2787920",
    "end": "2795079"
  },
  {
    "text": "code y questions another thing like for an practical approach you often want to",
    "start": "2795079",
    "end": "2800640"
  },
  {
    "text": "have your model trainer evaluate all the things that you just told us and then you want to apply it and I was wondering",
    "start": "2800640",
    "end": "2808559"
  },
  {
    "text": "how you do it like in thetic way to like how you deploy a model and then have it",
    "start": "2808559",
    "end": "2814720"
  },
  {
    "text": "as a service running like in the okay well actually to deploy this you",
    "start": "2814720",
    "end": "2820920"
  },
  {
    "text": "can well you can save model the model can be saved so the model save then you can low it on the",
    "start": "2820920",
    "end": "2828240"
  },
  {
    "text": "system and call predict if you if you run in Scala if you if you if you are",
    "start": "2828240",
    "end": "2834359"
  },
  {
    "text": "not if you are not if you're not running on Scala as usual you can save it into XML",
    "start": "2834359",
    "end": "2840119"
  },
  {
    "text": "ml they support that some kind of standard load it fix all the uh fix all",
    "start": "2840119",
    "end": "2845960"
  },
  {
    "text": "the problems with XML ml because it's not really compatible inter interpretable yeah yeah",
    "start": "2845960",
    "end": "2853800"
  },
  {
    "text": "but like when we the model now with our setup now we would store candra and then load it and get our",
    "start": "2853800",
    "end": "2861480"
  },
  {
    "text": "model I wondering how you set up a service within spark like like you still",
    "start": "2861480",
    "end": "2866800"
  },
  {
    "text": "in spark you're still on the RBD how do you like serve",
    "start": "2866800",
    "end": "2871839"
  },
  {
    "text": "that uh good question well as I say if as I said",
    "start": "2871839",
    "end": "2877800"
  },
  {
    "text": "before if you want to do that kind of Serv base I recommend to save it into the",
    "start": "2877800",
    "end": "2884119"
  },
  {
    "text": "XML and we'll apply it by another function so well actually to learn to to",
    "start": "2884119",
    "end": "2890359"
  },
  {
    "text": "teach the model you need a lot of power and you do it in and you do it in parallel to go back to apply model you",
    "start": "2890359",
    "end": "2898280"
  },
  {
    "text": "don't need such kind of you need to get well it's hundred of parameters usually",
    "start": "2898280",
    "end": "2904119"
  },
  {
    "text": "well actually if you talk about so line regression you get the just a equation",
    "start": "2904119",
    "end": "2910319"
  },
  {
    "text": "and linear equations that's really simple to to apply to a to to any system",
    "start": "2910319",
    "end": "2917880"
  },
  {
    "text": "by by by hands so in this case you will get really uh so you use usually use safe",
    "start": "2917880",
    "end": "2925800"
  },
  {
    "text": "model and apply it in production by another code not in a spark environment at",
    "start": "2925800",
    "end": "2931400"
  },
  {
    "text": "all or if you need a bch prediction you do it in spark save it save it back to",
    "start": "2931400",
    "end": "2937760"
  },
  {
    "text": "the databases and get for example recommendation is usually done in bch by",
    "start": "2937760",
    "end": "2943440"
  },
  {
    "text": "in in a spark so we do recommendation for the for the all users save the B to the Cassandra table and then feat",
    "start": "2943440",
    "end": "2952280"
  },
  {
    "text": "it okay 10 10 10 10 minutes till",
    "start": "2953720",
    "end": "2959920"
  },
  {
    "text": "lunch let's be patient um okay let me stop here uh what what did I miss here",
    "start": "2959920",
    "end": "2967799"
  },
  {
    "text": "is a as I say spark ml that's in a alpha that's an alpha stage and graph uh they",
    "start": "2967799",
    "end": "2976200"
  },
  {
    "text": "also have a graph engine that allow and a lot of people try to use it it's still",
    "start": "2976200",
    "end": "2982040"
  },
  {
    "text": "it's still in a alpha stage and well it's someone say that's good other other",
    "start": "2982040",
    "end": "2989480"
  },
  {
    "text": "ones say a lot of out of memory exceptions so I will not talk about that",
    "start": "2989480",
    "end": "2995280"
  },
  {
    "text": "because I don't have well uh it's need a lot of time to go into",
    "start": "2995280",
    "end": "3001000"
  },
  {
    "text": "the graph algorithm before I can show you uh simple API that Implement",
    "start": "3001000",
    "end": "3007400"
  },
  {
    "text": "that graph algorith I'm sorry and well because everybody about all talk about",
    "start": "3007400",
    "end": "3013520"
  },
  {
    "text": "page rang that not appliable in a real world only Google use page run sorry",
    "start": "3013520",
    "end": "3020359"
  },
  {
    "text": "guys uh let's go okay algorithms as I say uh API you",
    "start": "3020359",
    "end": "3028079"
  },
  {
    "text": "can use Scala and I recommend to use it as a native python python is a good one",
    "start": "3028079",
    "end": "3035040"
  },
  {
    "text": "well and all examples I see about machine learning is on python uh but it's G python so don't",
    "start": "3035040",
    "end": "3043119"
  },
  {
    "text": "expect a lot it's it's a python inside",
    "start": "3043119",
    "end": "3048520"
  },
  {
    "text": "Java uh there is a Java Drive well you can use Java but it's look ugly uh",
    "start": "3048520",
    "end": "3054319"
  },
  {
    "text": "especially if you don't use if you don't use a lambdas so Java Java I recommend Java eight8",
    "start": "3054319",
    "end": "3061240"
  },
  {
    "text": "only and well actually there is they introduced error interfaces but no closures only a",
    "start": "3061240",
    "end": "3069480"
  },
  {
    "text": "spark SQL I like so we cannot do error in par algorithm parallel you get the",
    "start": "3069480",
    "end": "3077720"
  },
  {
    "text": "error you get only the data in and out questions well actually what what we",
    "start": "3077720",
    "end": "3086680"
  },
  {
    "text": "have done we put I put I get our architecture",
    "start": "3086680",
    "end": "3094200"
  },
  {
    "text": "and put Cassandra and Spark on it and you can see that you can do streaming",
    "start": "3094200",
    "end": "3102160"
  },
  {
    "text": "with spark and get speed layer with spark and Cassandra you definitely could use",
    "start": "3102160",
    "end": "3108760"
  },
  {
    "text": "Cassandra as a serving layer especially if you",
    "start": "3108760",
    "end": "3113799"
  },
  {
    "text": "have a lot of users and a lot of well stuff to store and you can use spark and well",
    "start": "3113799",
    "end": "3122599"
  },
  {
    "text": "probably yes yet Hadoop especially if you use hads to store your data L data",
    "start": "3122599",
    "end": "3127920"
  },
  {
    "text": "and master data sets more",
    "start": "3127920",
    "end": "3133240"
  },
  {
    "text": "questions would you still recommend to have htfs in the St for like holding",
    "start": "3133799",
    "end": "3140559"
  },
  {
    "text": "like have all the benefits of the security of the hdfs or also there there",
    "start": "3140559",
    "end": "3146760"
  },
  {
    "text": "one thing that if it's HFS stores the sequence fil then you can make it so that it's still SD readable so in case",
    "start": "3146760",
    "end": "3154920"
  },
  {
    "text": "of a complete outage you can collect manually all your files and it's like text encoded which is not possible",
    "start": "3154920",
    "end": "3162200"
  },
  {
    "text": "candra and so how do you stand HS like what would you Rec well",
    "start": "3162200",
    "end": "3170359"
  },
  {
    "text": "actually uh Hadoop is the only way to store petabytes of data and well",
    "start": "3170359",
    "end": "3175799"
  },
  {
    "text": "actually and if you want to process data uh",
    "start": "3175799",
    "end": "3183480"
  },
  {
    "text": "sequentially Hadoop is still very good and well not Hadoop but hdfs so if you",
    "start": "3183480",
    "end": "3189799"
  },
  {
    "text": "if you want spark to process data sequentially and you have a petabytes or gigabytes of data that you just need to",
    "start": "3189799",
    "end": "3195960"
  },
  {
    "text": "just go through all the data you have like logs files it's still something",
    "start": "3195960",
    "end": "3201839"
  },
  {
    "text": "like hdfs so uh remember that uh all this Cassandra is for the random access",
    "start": "3201839",
    "end": "3209960"
  },
  {
    "text": "data so when you store everything sequentially for example but you need get only 5% of the data select selected",
    "start": "3209960",
    "end": "3217839"
  },
  {
    "text": "wisely or something like that but if you need to store petabyte of data all if",
    "start": "3217839",
    "end": "3223000"
  },
  {
    "text": "you want to stay store everything I have you still will use uh hdf not the Hadoop but hdfs as a",
    "start": "3223000",
    "end": "3230680"
  },
  {
    "text": "part of the hard but then you can run spark on it not not not M ruce something",
    "start": "3230680",
    "end": "3237880"
  },
  {
    "text": "like that and well and the other question what the was what",
    "start": "3237880",
    "end": "3243480"
  },
  {
    "text": "the DAT format use for that well actually it's depends after that it",
    "start": "3243480",
    "end": "3250599"
  },
  {
    "text": "depends more",
    "start": "3250599",
    "end": "3253839"
  },
  {
    "text": "questions I think well any questions from the",
    "start": "3256880",
    "end": "3261920"
  },
  {
    "text": "internet okay we have five minutes still so we can run a little bit but it's okay",
    "start": "3261920",
    "end": "3267640"
  },
  {
    "text": "no question yeah okay so thank you very much don't forget to use this one tool",
    "start": "3267640",
    "end": "3274520"
  },
  {
    "text": "for you",
    "start": "3274520",
    "end": "3277760"
  }
]