[
  {
    "start": "0",
    "end": "211000"
  },
  {
    "text": "[Music]",
    "start": "2540",
    "end": "11609"
  },
  {
    "text": "thank you thank you and good morning um I'm here to talk about two things and I hope at least one of them is interesting",
    "start": "12240",
    "end": "18240"
  },
  {
    "text": "to you number one is is Apache spark which I'm sure everyone in the room is by now heard of it's a the the new",
    "start": "18240",
    "end": "24240"
  },
  {
    "text": "hotness in the Hadoop ecosystem it's a new processing Paradigm and has a lot to offer I think for for the data scientists in the room as well but I",
    "start": "24240",
    "end": "31000"
  },
  {
    "text": "also want to talk about uh decision Forest which is a common and important algorithm in in machine learning and",
    "start": "31000",
    "end": "36879"
  },
  {
    "text": "data science and one I think everyone should um know a little bit about so uh so hopefully you're you're here to learn",
    "start": "36879",
    "end": "43039"
  },
  {
    "text": "about at least one maybe maybe both at the same time let's begin uh I want to start by",
    "start": "43039",
    "end": "49039"
  },
  {
    "text": "backing up a little bit and talking about the the roots of prediction or where we got this idea of predicting",
    "start": "49039",
    "end": "55320"
  },
  {
    "text": "some data from other data and I like to go back to this picture",
    "start": "55320",
    "end": "60760"
  },
  {
    "text": "um this is a this was drawn by Sir Francis gton in the the 18th century I",
    "start": "60760",
    "end": "66400"
  },
  {
    "text": "think uh he was studying the relationship between parents and children uh specifically their heights",
    "start": "66400",
    "end": "73320"
  },
  {
    "text": "and he noted that tall parents have tall children surprisingly uh what he was",
    "start": "73320",
    "end": "78479"
  },
  {
    "text": "really getting at though was that very tall parents have tall but not very tall",
    "start": "78479",
    "end": "83680"
  },
  {
    "text": "children and he called this a regression to the mean and we still do talk about regression to the mean but interestingly",
    "start": "83680",
    "end": "89720"
  },
  {
    "text": "he he tried to plot his data and he plotted parents I think he might have",
    "start": "89720",
    "end": "95119"
  },
  {
    "text": "flipped the axes relative to what we're used to but parent versus child height and uh found there was a sort of linear",
    "start": "95119",
    "end": "101759"
  },
  {
    "text": "relationship between the two and the slope was less than one um but I think this is really almost an example of an",
    "start": "101759",
    "end": "108320"
  },
  {
    "text": "early predictive model the idea that we could say something about the child's height given the the parents height as",
    "start": "108320",
    "end": "113880"
  },
  {
    "text": "input and this act of drawing a line to establish that linear relationship we still call that regression for this",
    "start": "113880",
    "end": "120240"
  },
  {
    "text": "reason even though it really has nothing to do with regression to the mean or or moving backwards or anything so there is",
    "start": "120240",
    "end": "125799"
  },
  {
    "text": "something interesting about this particular picture historically now if you think about it",
    "start": "125799",
    "end": "131800"
  },
  {
    "text": "it's it's a this gton maybe unwittingly was positing a black box there's this",
    "start": "131800",
    "end": "138000"
  },
  {
    "text": "chain of relationships correlations maybe causations between this input and",
    "start": "138000",
    "end": "143200"
  },
  {
    "text": "this output uh we don't know what they are I don't think he was trying to explain exactly physiologically why",
    "start": "143200",
    "end": "149000"
  },
  {
    "text": "parents have tall children but he thought there was a relationship there and he he showed the relationship he",
    "start": "149000",
    "end": "154760"
  },
  {
    "text": "tried to describe that black box in this linear relationship and really this is how most of machine learning or",
    "start": "154760",
    "end": "161840"
  },
  {
    "text": "supervised learning thinks of the world uh more particularly there's a a very common data set out there called the",
    "start": "161840",
    "end": "167840"
  },
  {
    "text": "iris data set that people have used uh in research for years and it's a data",
    "start": "167840",
    "end": "173280"
  },
  {
    "text": "set that describes a bunch of irises or flowers and gives their measurements their um seil and petal lengths and",
    "start": "173280",
    "end": "179239"
  },
  {
    "text": "widths and their species and we think that there's some relationship between the two that excuse me that given uh",
    "start": "179239",
    "end": "185080"
  },
  {
    "text": "some of these measurements that's causes or correlates with the the type of flower that it is and we don't know why",
    "start": "185080",
    "end": "191360"
  },
  {
    "text": "we don't need to know why in order to model that relationship or to describe that black box that connects those",
    "start": "191360",
    "end": "197840"
  },
  {
    "text": "inputs to that output and uh in fact that's the magic of of machine learning that we don't actually have to",
    "start": "197840",
    "end": "204159"
  },
  {
    "text": "understand the relationships we can we can just try to model them and learn them empirically from data",
    "start": "204159",
    "end": "210680"
  },
  {
    "text": "this is an example from the iris data set itself the data set's not important but it's a good example of just about",
    "start": "210680",
    "end": "216959"
  },
  {
    "start": "211000",
    "end": "393000"
  },
  {
    "text": "every normal machine learning data set out there you have a bunch of examples",
    "start": "216959",
    "end": "222080"
  },
  {
    "text": "of inputs plus outputs in this case it's four Columns of of numeric measures",
    "start": "222080",
    "end": "227360"
  },
  {
    "text": "these are the the lengths and centimeters of the SE and the SE and pedal lengths and widths and the species",
    "start": "227360",
    "end": "233400"
  },
  {
    "text": "the outputs and this is the thing that we're trying to predict given the inputs and again we believe there's this",
    "start": "233400",
    "end": "238680"
  },
  {
    "text": "connection between the two and we want to discover what it is not Discover it but describe it and model it and so we",
    "start": "238680",
    "end": "246079"
  },
  {
    "text": "given enough of this enough of these examples of the inputs and outputs we believe we can learn what how that",
    "start": "246079",
    "end": "251280"
  },
  {
    "text": "blackbox behaves and this is the essence of just about all standard conventional",
    "start": "251280",
    "end": "256919"
  },
  {
    "text": "machine learning given enough of these feature vectors we can build a model this is a an attempt to plot this",
    "start": "256919",
    "end": "265800"
  },
  {
    "text": "data set now it's really uh four-dimensional but here I think only",
    "start": "265800",
    "end": "271080"
  },
  {
    "text": "I've just plotted three or picked up a plot of three of the dimensions um uh so each point in space",
    "start": "271080",
    "end": "277240"
  },
  {
    "text": "three of its dimensions are determined by three of its inputs and the color and shape is the output or the the species",
    "start": "277240",
    "end": "283720"
  },
  {
    "text": "and you and I can see that the uh iosa exists in a fairly distinct area of",
    "start": "283720",
    "end": "289520"
  },
  {
    "text": "feature space intuitively we understand that when the measurements are put a point over there it's probably a",
    "start": "289520",
    "end": "296639"
  },
  {
    "text": "Sosa and indeed a lot of classic um classification algorithms in machine",
    "start": "296639",
    "end": "302120"
  },
  {
    "text": "learning are all about drawing lines uh trying to figure out what's the the",
    "start": "302120",
    "end": "307400"
  },
  {
    "text": "dividing line between this data of one type and that data of another type because if we could draw this line",
    "start": "307400",
    "end": "313440"
  },
  {
    "text": "effectively we could effectively say that stuff over here in feature space is probably this type of flower and this",
    "start": "313440",
    "end": "320800"
  },
  {
    "text": "type of output and that's a that's a form of predictive model so if you've heard of logistic regression logistic",
    "start": "320800",
    "end": "325840"
  },
  {
    "text": "regression is really a linear model that's trying to pick some best line or really really hyperplane in",
    "start": "325840",
    "end": "332039"
  },
  {
    "text": "space that's uh dividing some types of the outputs from other types of the",
    "start": "332039",
    "end": "337240"
  },
  {
    "text": "outputs if you've heard of support Vector machines it's also a linear model that tries to draw maybe a slightly",
    "start": "337240",
    "end": "343039"
  },
  {
    "text": "better hyperplane that's maximizing the the margins here between the the classes and the line and then if you've heard of",
    "start": "343039",
    "end": "349080"
  },
  {
    "text": "support Vector machines plus the the the kernel trick that's an attempt to draw",
    "start": "349080",
    "end": "354240"
  },
  {
    "text": "an even better line that's maybe not linear in the original feature space so we can draw curved lines in space and",
    "start": "354240",
    "end": "360720"
  },
  {
    "text": "again each of these uh the the idea behind each of these is let's just draw a better and better boundary between the",
    "start": "360720",
    "end": "367360"
  },
  {
    "text": "outputs we're trying to predict to discriminate them and this is good and this is powerful but this is not the",
    "start": "367360",
    "end": "372759"
  },
  {
    "text": "only form of algorithm or decision boundary we can construct to try to discriminate classes or",
    "start": "372759",
    "end": "379680"
  },
  {
    "text": "outputs and indeed uh decision trees are not of this form but there's still a",
    "start": "379680",
    "end": "385800"
  },
  {
    "text": "interesting and Powerful idea for classification so I want to introduce them briefly so to introduce decision trees I",
    "start": "385800",
    "end": "392840"
  },
  {
    "text": "want to give a a quick toy example so imagine you are a robot that works at a",
    "start": "392840",
    "end": "398800"
  },
  {
    "start": "393000",
    "end": "460000"
  },
  {
    "text": "pet store and you need to you know which pets are suitable for children and you",
    "start": "398800",
    "end": "403919"
  },
  {
    "text": "don't know why pets are suitable for children or not but the owner has told you the following you can you can",
    "start": "403919",
    "end": "409639"
  },
  {
    "text": "measure the following um attributes of the pets in the store and you've been told in some cases that they're yes",
    "start": "409639",
    "end": "415479"
  },
  {
    "text": "suitable for children or or no not now I think you and I can kind of intuitively",
    "start": "415479",
    "end": "421039"
  },
  {
    "text": "understand that a huge elephant is not a good pet yes uh maybe a a a poisonous",
    "start": "421039",
    "end": "426720"
  },
  {
    "text": "millipede is not a good pet yes but the robot doesn't know these things the robot can only learn on empirically on",
    "start": "426720",
    "end": "433680"
  },
  {
    "text": "on this data that we've given to it now one valid approach if you are the robot is to try to come up with some rules of",
    "start": "433680",
    "end": "440080"
  },
  {
    "text": "thumb based on this data that might discriminate the yeses from the nose for example you might indeed see that weight",
    "start": "440080",
    "end": "446039"
  },
  {
    "text": "seems to matter uh the you know very heavy pet P seems to not be a good pet",
    "start": "446039",
    "end": "451919"
  },
  {
    "text": "in in these two cases so so maybe that's a good place to start maybe we try to make a decision rule based on on",
    "start": "451919",
    "end": "459280"
  },
  {
    "text": "weight so we start there and maybe this is the first decision rule uh we come up",
    "start": "459280",
    "end": "464520"
  },
  {
    "start": "460000",
    "end": "600000"
  },
  {
    "text": "with if it's over 500 Kilograms it's not suitable if it's if otherwise it is and",
    "start": "464520",
    "end": "470960"
  },
  {
    "text": "that would be right in about half the cases actually that's not bad but it's not really any better than chance so",
    "start": "470960",
    "end": "476800"
  },
  {
    "text": "that that doesn't seem like a good enough decision rule so we could make this more elaborate maybe maybe try a",
    "start": "476800",
    "end": "483280"
  },
  {
    "text": "different threshold maybe um you know 100 kilograms is a is a better place to begin discriminating and maybe amongst",
    "start": "483280",
    "end": "490520"
  },
  {
    "text": "the smaller animals we need a further discrimination and and another rule that",
    "start": "490520",
    "end": "496039"
  },
  {
    "text": "uh based on color maybe so you can make a more elaborate decision process with several layers here and get more answers",
    "start": "496039",
    "end": "503120"
  },
  {
    "text": "correct for example this decision rule while still pretty simplistic gets seven of the nine examples correct and that's",
    "start": "503120",
    "end": "509199"
  },
  {
    "text": "that's better right and you can imagine that with a couple more steps here we could get 100% of the examples correct",
    "start": "509199",
    "end": "518080"
  },
  {
    "text": "which may or may not be a good thing actually we may we may overfit the data but I think the process is fairly clear",
    "start": "518080",
    "end": "525399"
  },
  {
    "text": "uh we want to keep um chopping up the uh making decision rules and layering them",
    "start": "525399",
    "end": "530480"
  },
  {
    "text": "in order to uh accurately reflect the input we have and there's some process here by which we could do that and the",
    "start": "530480",
    "end": "536800"
  },
  {
    "text": "nice thing about the output here is it's it's fairly intuitive to read off and in fact this is a a",
    "start": "536800",
    "end": "541920"
  },
  {
    "text": "classifier this is also a a model of the relationship between three inputs and",
    "start": "541920",
    "end": "547680"
  },
  {
    "text": "one output so this is a valid classification algorithm as well of course the question is how do we build",
    "start": "547680",
    "end": "552959"
  },
  {
    "text": "these trees and that is in fact all of the complexity of decision trees how do we pick the right tree how do we know",
    "start": "552959",
    "end": "559200"
  },
  {
    "text": "when to stop things like that so for the Curious uh if you think",
    "start": "559200",
    "end": "564360"
  },
  {
    "text": "about the rules that decision trees create simple decision trees end up",
    "start": "564360",
    "end": "569519"
  },
  {
    "text": "drawing decision boundaries like this in comparison um axis align boundaries they",
    "start": "569519",
    "end": "575160"
  },
  {
    "text": "keep chopping up space into into halves in this case into uh via binary decisions uh keep chopping up space",
    "start": "575160",
    "end": "582040"
  },
  {
    "text": "recursively along along certain Dimensions so that's qualitatively a different type of decision rule you can",
    "start": "582040",
    "end": "587399"
  },
  {
    "text": "get out of decision trees than you can out of a out of a linear model and this has in some cases this is more powerful",
    "start": "587399",
    "end": "593920"
  },
  {
    "text": "this is a more powerful way to to to model decision boundaries",
    "start": "593920",
    "end": "599560"
  },
  {
    "start": "600000",
    "end": "951000"
  },
  {
    "text": "here for illustration I've uh built two different classification models this is",
    "start": "601480",
    "end": "607160"
  },
  {
    "text": "just an R this is not spark uh on the left we have a support Vector Machine model on the right we have a decision",
    "start": "607160",
    "end": "614600"
  },
  {
    "text": "force model and in both cases we're just going to build uh build based on the iris data set which is so common it's",
    "start": "614600",
    "end": "621160"
  },
  {
    "text": "built into r as as Iris so very simple to do uh in r at this point in both",
    "start": "621160",
    "end": "627480"
  },
  {
    "text": "cases we build we build a model and it's a a perfectly fine classification model they both have high accuracy on this",
    "start": "627480",
    "end": "632839"
  },
  {
    "text": "data set but interestingly if you look at the model and try to interpret it",
    "start": "632839",
    "end": "638519"
  },
  {
    "text": "based on its string representation you'll find that linear models are a little bit hard to hard to reason about",
    "start": "638519",
    "end": "645079"
  },
  {
    "text": "um these are the support vectors and they have a meaning and we can we can understand them and maybe interpret them",
    "start": "645079",
    "end": "652000"
  },
  {
    "text": "but it does take a little bit of time to to think about what this means um the",
    "start": "652000",
    "end": "657560"
  },
  {
    "text": "meaning of this even changes based on what exactly you did in the support Factor machine so linear models are",
    "start": "657560",
    "end": "664079"
  },
  {
    "text": "maybe not trivial to interpret whereas decision trees are much more straightforward you can almost read off",
    "start": "664079",
    "end": "669680"
  },
  {
    "text": "the logic in in plain language uh at the top the decision Tree starts by asking",
    "start": "669680",
    "end": "675519"
  },
  {
    "text": "whether the pedal length is less than or equal to 1.9 CM if so I think this means predict the the first class the first of",
    "start": "675519",
    "end": "682440"
  },
  {
    "text": "the the three species otherwise it looks at the pedal width other and if that is",
    "start": "682440",
    "end": "688600"
  },
  {
    "text": "uh small enough it checks the pedal length and so on so you could you could easily reconstruct the logic and explain",
    "start": "688600",
    "end": "694440"
  },
  {
    "text": "to anyone what the the logic is that led you to one decision or the other and maybe as a botanist you could evaluate",
    "start": "694440",
    "end": "701839"
  },
  {
    "text": "whether these rules make sense sure but but at least we can all we can read them off and explain how the model is",
    "start": "701839",
    "end": "707440"
  },
  {
    "text": "relating inputs to an output and that's often also powerful especially in um",
    "start": "707440",
    "end": "713160"
  },
  {
    "text": "think about Finance or regulated Industries where you do need to explain why your process um approved someone",
    "start": "713160",
    "end": "719040"
  },
  {
    "text": "from for a loan or not so this is this is a non-trivial consideration in",
    "start": "719040",
    "end": "724720"
  },
  {
    "text": "modeling now of course as advertised I want to talk about decision trees as implemented in Sparks machine learning",
    "start": "724800",
    "end": "731399"
  },
  {
    "text": "sub project which is is um called MLB at least the classic machine learning component is called MLB and it has an",
    "start": "731399",
    "end": "737800"
  },
  {
    "text": "implementation of decision trees and actually decision forests as well which we'll get to so this is the part where",
    "start": "737800",
    "end": "744160"
  },
  {
    "text": "we start to show what it means to build a real decision tree at Large SC using",
    "start": "744160",
    "end": "749920"
  },
  {
    "text": "spark for this example I'm going to use a data set called the cover type data set which is by happy coincidence also",
    "start": "749920",
    "end": "757399"
  },
  {
    "text": "about trees and forests it's uh 580,000 examples um Each of which describes a",
    "start": "757399",
    "end": "765040"
  },
  {
    "text": "piece of land I think in in Colorado in the United States uh has a bunch of features about that piece of land like",
    "start": "765040",
    "end": "771079"
  },
  {
    "text": "its elevation and slope and I think humidity and distance to water and maybe sunlight and so on and it also has the",
    "start": "771079",
    "end": "779240"
  },
  {
    "text": "type of forest covering that piece of land is it no trees is it Aspen Spruce",
    "start": "779240",
    "end": "785160"
  },
  {
    "text": "Etc um this may may not be interesting to us as a as a problem but it'll make a good example can we actually predict",
    "start": "785160",
    "end": "792199"
  },
  {
    "text": "based on the attributes of the piece of land what type of forest is probably growing on that land this is again a",
    "start": "792199",
    "end": "800279"
  },
  {
    "text": "reasonably well-known data set in the machine learning um community so it's may make a good example as a benchmark",
    "start": "800279",
    "end": "806920"
  },
  {
    "text": "as well so let let's look at the data well the data is a bunch of feature",
    "start": "806920",
    "end": "813600"
  },
  {
    "text": "vectors examples and as before we have a bunch of inputs and one output and I've",
    "start": "813600",
    "end": "819199"
  },
  {
    "text": "printed an example here and formatted it in a particular way to make a few points about it the first 10 columns or 10",
    "start": "819199",
    "end": "826519"
  },
  {
    "text": "features are are numeric and they're measures of things like elevation and slope and I think uh distance to to",
    "start": "826519",
    "end": "833399"
  },
  {
    "text": "water and maybe to the nearest highway I forget what they are it almost doesn't matter um the next four features and the",
    "start": "833399",
    "end": "840639"
  },
  {
    "text": "next 40 are actually respectively just just two features two attributes these",
    "start": "840639",
    "end": "847480"
  },
  {
    "text": "four describe the Wilderness type um is it desert or arid or scrub I I forget",
    "start": "847480",
    "end": "855079"
  },
  {
    "text": "it's there's four types of wilderness that a piece of land can be so exactly one of these four columns is a one and",
    "start": "855079",
    "end": "860360"
  },
  {
    "text": "the others are zero so really this is a a categorical or discret value that's been encoded as a series of numbers and",
    "start": "860360",
    "end": "867320"
  },
  {
    "text": "we'll come back to this point but um likewise for these 40 this is actually one of 40 soil types so there's 40",
    "start": "867320",
    "end": "875079"
  },
  {
    "text": "features here but they're really conceptually one feature because all of them will be zero except for one which",
    "start": "875079",
    "end": "880440"
  },
  {
    "text": "will be a one it's like an indicator flag and last but not least we have the",
    "start": "880440",
    "end": "885800"
  },
  {
    "text": "cover type this takes on One of seven values and these are encoded as a number",
    "start": "885800",
    "end": "891600"
  },
  {
    "text": "from one to seven so interestingly there's there's really three non-numeric",
    "start": "891600",
    "end": "897839"
  },
  {
    "text": "values here in COD put it in two different ways such as life this is this is real",
    "start": "897839",
    "end": "903040"
  },
  {
    "text": "data and to begin to work with it we need to convert it into a a form and a representation that the spark MLB",
    "start": "903040",
    "end": "910560"
  },
  {
    "text": "implementations can consume and for spark that's called a labeled point which is really just a double and an",
    "start": "910560",
    "end": "917600"
  },
  {
    "text": "array of doubles uh the label is the output um and happily it wants even",
    "start": "917600",
    "end": "924360"
  },
  {
    "text": "categorical features encoded as a number and we've already got that okay and then these are the the rest of it is the",
    "start": "924360",
    "end": "931560"
  },
  {
    "text": "feature Vector all the inputs encoded as a as a vector which is really just a an array of doubles so thankfully this that",
    "start": "931560",
    "end": "939000"
  },
  {
    "text": "should be pretty straightforward that's already how this data is is encoded actually so this should be pretty",
    "start": "939000",
    "end": "944600"
  },
  {
    "text": "straightforward to to make objects like this out of the",
    "start": "944600",
    "end": "949680"
  },
  {
    "text": "input let's do it maybe a little bit hard to read but I'll I'll walk through this this is",
    "start": "949680",
    "end": "955680"
  },
  {
    "start": "951000",
    "end": "1180000"
  },
  {
    "text": "actual spark code this is using Spark's Scala API and if it looks like regular Scala it's because spark really is at",
    "start": "955680",
    "end": "962680"
  },
  {
    "text": "heart distributed Scala so a lot of the operations that are actually spark API calls here are going to look just like",
    "start": "962680",
    "end": "968920"
  },
  {
    "text": "Scala collection API calls and that's a good thing import some classes U first",
    "start": "968920",
    "end": "974480"
  },
  {
    "text": "thing we'll do is open or get a reference to this data set which may live on hdfs or your your clusters",
    "start": "974480",
    "end": "982319"
  },
  {
    "text": "distributed file system and in spark just about everything is an rdd a",
    "start": "982319",
    "end": "987600"
  },
  {
    "text": "resilient distributed data data set that's um an immutable distributed",
    "start": "987600",
    "end": "993360"
  },
  {
    "text": "collection of things and in the case of a text file that mean spark views that",
    "start": "993360",
    "end": "998480"
  },
  {
    "text": "as a an rdd of strings so a text file is just a big collection of of strings one",
    "start": "998480",
    "end": "1004399"
  },
  {
    "text": "per line right that's our that's so raw data is an rdd of strings we're going to",
    "start": "1004399",
    "end": "1012000"
  },
  {
    "text": "transform those strings into labeled points using a map here for every line",
    "start": "1012000",
    "end": "1017240"
  },
  {
    "text": "split on commas so we make a uh we tokenize it and we map each of those to a double so now I've turned my CSV",
    "start": "1017240",
    "end": "1025000"
  },
  {
    "text": "formatted line into an array of doubles uh I'm going to take all but the",
    "start": "1025000",
    "end": "1030240"
  },
  {
    "text": "last one and make a vector out of it so this is my feature Vector I'm going to take the last one as the output or label",
    "start": "1030240",
    "end": "1037319"
  },
  {
    "text": "and subtract one from it since spark wants them to be zero based not one based and there we go I make a labeled",
    "start": "1037319",
    "end": "1042600"
  },
  {
    "text": "point out of the label and feature vector and that's my transformation process so you can type this code into",
    "start": "1042600",
    "end": "1047880"
  },
  {
    "text": "the spark shell directly no need to compile a program and if you type that and you type that uh you'll find that",
    "start": "1047880",
    "end": "1053440"
  },
  {
    "text": "nothing really happens because there's been no action yet we've just described uh a process that would transform a data",
    "start": "1053440",
    "end": "1059679"
  },
  {
    "text": "on hdfs into this set of uh objects in memory but we haven't asked it to do anything",
    "start": "1059679",
    "end": "1065840"
  },
  {
    "text": "yet before we go and do something with it and build a decision tree model however we're going to actually put",
    "start": "1065840",
    "end": "1072000"
  },
  {
    "text": "aside about 10% of the data for for testing purposes that is we're going to train the model on most of the data and",
    "start": "1072000",
    "end": "1078960"
  },
  {
    "text": "hold back a little bit for for reasons I'll show you on the next slide um in order to test the model um last but not",
    "start": "1078960",
    "end": "1085720"
  },
  {
    "text": "least it's interesting to note that we should cache the training data this tells spark that once you compute the",
    "start": "1085720",
    "end": "1091360"
  },
  {
    "text": "rdd do keep that result in memory in the cluster so take those those uh labelled",
    "start": "1091360",
    "end": "1097120"
  },
  {
    "text": "Point objects and actually persist them uh across the the distributed cache that the spark executors have and this will",
    "start": "1097120",
    "end": "1103240"
  },
  {
    "text": "be important because this process is going to access that data over and over and if it's in memory that's fast and of",
    "start": "1103240",
    "end": "1109520"
  },
  {
    "text": "course this is why people say spark can be a lot faster than same M produce because it can use memory so use more",
    "start": "1109520",
    "end": "1117120"
  },
  {
    "text": "memory in order to gain speed and that's a good trade-off in general these days all that done all that set up aside",
    "start": "1117120",
    "end": "1124039"
  },
  {
    "text": "then building the decision tree model is just one method call here decision Tree Train classifier passes the input data",
    "start": "1124039",
    "end": "1130440"
  },
  {
    "text": "we have to tell it uh in this case that there are seven possible label values",
    "start": "1130440",
    "end": "1135760"
  },
  {
    "text": "and this we cover later this is concerns which values are categorical and we give it a couple hyper parameters which again",
    "start": "1135760",
    "end": "1141960"
  },
  {
    "text": "I'll I'll explain it in a moment and the output is a decision tree you can print",
    "start": "1141960",
    "end": "1147760"
  },
  {
    "text": "it and again as in the r code you see that uh we can sort of read off the logic here um given this API it doesn't",
    "start": "1147760",
    "end": "1155840"
  },
  {
    "text": "actually know what these features are we haven't given them names so all it can do is call them featur 0 1 two 3 but",
    "start": "1155840",
    "end": "1161880"
  },
  {
    "text": "again this is this is elevation for example so the first uh decision is is the elevation less than 342 ft and and",
    "start": "1161880",
    "end": "1168840"
  },
  {
    "text": "so on we we could uh read off the logic and and see this nested tree structure in the same way so that's nice so we've",
    "start": "1168840",
    "end": "1175840"
  },
  {
    "text": "gotten a nice decision tree out of this but of course we didn't want to",
    "start": "1175840",
    "end": "1182039"
  },
  {
    "start": "1180000",
    "end": "1376000"
  },
  {
    "text": "just build the tree to build the tree we wanted to build the tree to make predictions and looking at that decision",
    "start": "1182039",
    "end": "1188000"
  },
  {
    "text": "tree certainly we don't know whether it's a a good model um even I think if",
    "start": "1188000",
    "end": "1193320"
  },
  {
    "text": "we're an expert in the in the field I don't know that we could look at that and decide whether that is the best model so we need some way to evaluate",
    "start": "1193320",
    "end": "1199640"
  },
  {
    "text": "whether we've built a good model whether this thing actually will predict other Forest cover types from new data to do",
    "start": "1199640",
    "end": "1207000"
  },
  {
    "text": "that we need some some theory of evaluation and that's why we held out some of the test data sorry some of the",
    "start": "1207000",
    "end": "1212360"
  },
  {
    "text": "input data as a test set because that's new data or data the model's not seen before for which we have the right",
    "start": "1212360",
    "end": "1219000"
  },
  {
    "text": "answer and so we could see how often the model predicts the same answer that is",
    "start": "1219000",
    "end": "1224400"
  },
  {
    "text": "we know to be correct from this test set so that's what we're doing here I'm going to make a method that takes in a a",
    "start": "1224400",
    "end": "1230559"
  },
  {
    "text": "model I've built and a test set an rdd of labeled points and for each one I'm",
    "start": "1230559",
    "end": "1236720"
  },
  {
    "text": "going to take that point and map it to the result of predicting from the model",
    "start": "1236720",
    "end": "1242039"
  },
  {
    "text": "and the known correct answer so the result of this uh function or t PS or pairs of predicted answer and correct",
    "start": "1242039",
    "end": "1249440"
  },
  {
    "text": "answer and Spark even has a little bit of support for automatically analyzing those results to get some metrics out so",
    "start": "1249440",
    "end": "1255799"
  },
  {
    "text": "that's what I'm going to return and I'm going to use that on my model and test data and then print out some of the U",
    "start": "1255799",
    "end": "1263159"
  },
  {
    "text": "metrics that that come out from this this evaluation process so one thing I",
    "start": "1263159",
    "end": "1268520"
  },
  {
    "text": "can get out of this is a confusion Matrix and a confusion Matrix is effectively uh these are correct answers",
    "start": "1268520",
    "end": "1275279"
  },
  {
    "text": "by row so here's all the seven possible outputs and the Seven possible um",
    "start": "1275279",
    "end": "1281600"
  },
  {
    "text": "predictions the model could have made and ideally the model's always right and",
    "start": "1281600",
    "end": "1287240"
  },
  {
    "text": "therefore uh is the count the number of times uh that the correct answer was predicted to be such and such an answer",
    "start": "1287240",
    "end": "1293480"
  },
  {
    "text": "so ideally the only the diagonal would have the values and everything else would be zero that would mean every uh",
    "start": "1293480",
    "end": "1300679"
  },
  {
    "text": "in every case the model predicted the correct answer the the the correct cases are the diagonal counts here and that's",
    "start": "1300679",
    "end": "1306880"
  },
  {
    "text": "mostly true as you can see but not always and the confusion Matrix helps you read off some some attributes of the",
    "start": "1306880",
    "end": "1313320"
  },
  {
    "text": "the model like uh class five was never predicted for some reason don't know why",
    "start": "1313320",
    "end": "1318360"
  },
  {
    "text": "but maybe that's interesting so in the main it looks like the the model's doing something reasonable but but it's not",
    "start": "1318360",
    "end": "1324400"
  },
  {
    "text": "always right you can see a number of errors here and here for example but maybe the one single interesting value",
    "start": "1324400",
    "end": "1331200"
  },
  {
    "text": "out of this is the the accuracy how many times was the model correct and that's",
    "start": "1331200",
    "end": "1336320"
  },
  {
    "text": "that's about 70% that's effectively the sum of the diagonal over the sum of everything here um correct out of out of",
    "start": "1336320",
    "end": "1342799"
  },
  {
    "text": "the total 70% sounds all right um it's better than one in seven I guess but is",
    "start": "1342799",
    "end": "1352000"
  },
  {
    "text": "that good could we do better is this better than random and that's always a I think a good way to think about these",
    "start": "1352000",
    "end": "1357320"
  },
  {
    "text": "things um when you get a results it seems okay Benchmark it against something else something simpler maybe",
    "start": "1357320",
    "end": "1363760"
  },
  {
    "text": "after all if uh this is no better than random guessing why bother with a decision tree model let's just make",
    "start": "1363760",
    "end": "1368960"
  },
  {
    "text": "random guesses and and do as well so let's actually Benchmark this against random",
    "start": "1368960",
    "end": "1374120"
  },
  {
    "text": "guessing and to do that what we could do is uh look at our training uh set and",
    "start": "1374120",
    "end": "1379600"
  },
  {
    "start": "1376000",
    "end": "1614000"
  },
  {
    "text": "count up the number of times each of the seven outputs occurred and then guess in proportion to those counts for example",
    "start": "1379600",
    "end": "1386279"
  },
  {
    "text": "if class one was present in the data 50% of the time then 50% of the time I would",
    "start": "1386279",
    "end": "1392240"
  },
  {
    "text": "just guess that the answer is class one and some of the time uh that would be right just like a a broken clock is is",
    "start": "1392240",
    "end": "1398960"
  },
  {
    "text": "correct twice a day it might be might be even correct quite often who knows so we",
    "start": "1398960",
    "end": "1404440"
  },
  {
    "text": "can Implement that pretty easily by counting uh here's another uh example of spark in action counting the number of",
    "start": "1404440",
    "end": "1412480"
  },
  {
    "text": "times each label occurs so that's us taking the label out of each point and Counting them up um",
    "start": "1412480",
    "end": "1420200"
  },
  {
    "text": "and then uh just producing proportions or or um",
    "start": "1420200",
    "end": "1426520"
  },
  {
    "text": "probabilities if you will out of that so this is just uh you know counts divided by the sum of the counts yeah I do that",
    "start": "1426520",
    "end": "1432360"
  },
  {
    "text": "for the train and test set and then uh put them together so we know the train probabilities dictate how um how",
    "start": "1432360",
    "end": "1438880"
  },
  {
    "text": "frequently we'll guess one class versus the other and the test probabilities give us a distribution of the actual uh",
    "start": "1438880",
    "end": "1444919"
  },
  {
    "text": "correct answers in the test set and from that we could we could infer how often we'd be correct with this random",
    "start": "1444919",
    "end": "1450159"
  },
  {
    "text": "guessing and even make a pseudo confusion Matrix here out of this process which is going to be symmetric",
    "start": "1450159",
    "end": "1456440"
  },
  {
    "text": "of course and it is worse random guessing I guess not surprisingly is not",
    "start": "1456440",
    "end": "1462440"
  },
  {
    "text": "as good it's right about 38% of the time so okay so out of the box the decision",
    "start": "1462440",
    "end": "1468640"
  },
  {
    "text": "algorithm is doing better than a very simplistic random guessing algorithm so so that much is good so we must be doing",
    "start": "1468640",
    "end": "1475039"
  },
  {
    "text": "something right but can we do better and yes we can for",
    "start": "1475039",
    "end": "1480080"
  },
  {
    "text": "sure so let's let's do better part of doing better is tuning the model and I",
    "start": "1480080",
    "end": "1486960"
  },
  {
    "text": "told you there were three parameters here I'd explain later and these are these are hyperparameters to the model",
    "start": "1486960",
    "end": "1492520"
  },
  {
    "text": "they're called hyperparameters since they're not things the model learns but things you have to give as um",
    "start": "1492520",
    "end": "1499000"
  },
  {
    "text": "uh parameters to the model building process itself so whereas the model will learn which nodes to put where and which",
    "start": "1499000",
    "end": "1506080"
  },
  {
    "text": "uh thresholds to choose these are things we have to are inputs to the model building process itself that that we",
    "start": "1506080",
    "end": "1513200"
  },
  {
    "text": "choose maximum depth four here is easy to explain that's the maximum depth uh",
    "start": "1513200",
    "end": "1518360"
  },
  {
    "text": "that the tree will expand to and you want to limit that because as you go de",
    "start": "1518360",
    "end": "1523720"
  },
  {
    "text": "build a deeper and deeper tree that chops up the data into smaller and smaller subsets eventually you're only",
    "start": "1523720",
    "end": "1529360"
  },
  {
    "text": "fitting noise and that's not helping and it's only helping you overfit the the data you have in hand and is unlikely to",
    "start": "1529360",
    "end": "1535960"
  },
  {
    "text": "help you make good predictions in the future that end it takes extra time and it makes a bigger model which is takes",
    "start": "1535960",
    "end": "1541000"
  },
  {
    "text": "longer to score so this is the crudest way and the only way this implementation supports at the moment to no one to stop",
    "start": "1541000",
    "end": "1546480"
  },
  {
    "text": "expanding now you can also maybe uh look at um when expanding the node seems to",
    "start": "1546480",
    "end": "1552799"
  },
  {
    "text": "not help much because it doesn't gain you much information or decrease impurity much",
    "start": "1552799",
    "end": "1558559"
  },
  {
    "text": "and that brings us to impurity so um actually maybe I'll explain the",
    "start": "1558559",
    "end": "1564120"
  },
  {
    "text": "minimum bends first in the initial example I tried two different thresholds for the the weight uh should I try a",
    "start": "1564120",
    "end": "1571799"
  },
  {
    "text": "rule based on 500 kilograms or 100 kilograms and um in an in an algorithmic",
    "start": "1571799",
    "end": "1577480"
  },
  {
    "text": "process I'd want to try many more possibilities and pick the best one and that is indeed a key part of what the decision tree algorithm does it tries a",
    "start": "1577480",
    "end": "1584840"
  },
  {
    "text": "bunch of n no expansions and and decision rules and picks the best one and then moves on through the tree but",
    "start": "1584840",
    "end": "1590240"
  },
  {
    "text": "but what does best mean I mean yes I can tell it to try a hundred different values at every node but but which one",
    "start": "1590240",
    "end": "1597080"
  },
  {
    "text": "is best how do I know which threshold is the right one to choose out of those hundred that I try and that's where the",
    "start": "1597080",
    "end": "1602840"
  },
  {
    "text": "impurity measure comes in impurity is uh is going to be the",
    "start": "1602840",
    "end": "1607919"
  },
  {
    "text": "criteria by which we judge a a split its quality and there's two possible values",
    "start": "1607919",
    "end": "1614120"
  },
  {
    "start": "1614000",
    "end": "1752000"
  },
  {
    "text": "for the impurity measure one is Genie impurity and one is entropy um I put up the formulas not to because",
    "start": "1614120",
    "end": "1622000"
  },
  {
    "text": "it's so important but to illustrate that they're not actually that different uh even though they're interpretation is",
    "start": "1622000",
    "end": "1627080"
  },
  {
    "text": "probably quite different uh the geni impurity I think you could most quickly view it as one minus the probability",
    "start": "1627080",
    "end": "1634399"
  },
  {
    "text": "that a random guess is correct within that data set so what does that mean um",
    "start": "1634399",
    "end": "1640640"
  },
  {
    "text": "if the data set is very mixed and has a little bit of all of the outputs then a random guess is unlikely to be correct",
    "start": "1640640",
    "end": "1647039"
  },
  {
    "text": "and so this value is high and that's bad because we kind of want decision rules to break up the data into",
    "start": "1647039",
    "end": "1654640"
  },
  {
    "text": "regions where the output is clear there's only one type of label or output present in the subsets so we do want",
    "start": "1654640",
    "end": "1662399"
  },
  {
    "text": "lower impurity we want a decision role to take a high impurity set of data and",
    "start": "1662399",
    "end": "1667640"
  },
  {
    "text": "break it into two subsets with low impurity we could also use entropy as that criteria entropy is an information",
    "start": "1667640",
    "end": "1674120"
  },
  {
    "text": "Theory concept that measures um the in a way the mixedness of of a set of things",
    "start": "1674120",
    "end": "1680159"
  },
  {
    "text": "and again if you have a lot of a little bit of everything in a set that has high entropy in a sense you could say that's",
    "start": "1680159",
    "end": "1686279"
  },
  {
    "text": "because it's uh we it the predictability of the outcome at that point is is is high the unpredictability is high so",
    "start": "1686279",
    "end": "1693320"
  },
  {
    "text": "again we would love to make um subsets from a decision rule that have low entropy that have just one output or the",
    "start": "1693320",
    "end": "1699640"
  },
  {
    "text": "other output because then we'd be we'd be driving to a clear decision from the model so in both cases lower is better",
    "start": "1699640",
    "end": "1707159"
  },
  {
    "text": "and the formula I mean ignoring this constant one which doesn't matter they're both like a negative weighted",
    "start": "1707159",
    "end": "1714159"
  },
  {
    "text": "average of something in this case the log of the proportions in this case the proportions themselves so they're",
    "start": "1714159",
    "end": "1719799"
  },
  {
    "text": "they're in a way related um but this is a bit of u a sidebar um the question is",
    "start": "1719799",
    "end": "1725200"
  },
  {
    "text": "which one's the better measure for this data set and we don't know we actually don't know just looking at this so what",
    "start": "1725200",
    "end": "1731240"
  },
  {
    "text": "do we do I mean how do we know which measure to choose which Maximum depth is right or which number of bends to try",
    "start": "1731240",
    "end": "1738640"
  },
  {
    "text": "and this is a general problem in model building because models will have hyper parameters and we don't know necessarily",
    "start": "1738640",
    "end": "1744480"
  },
  {
    "text": "which ones are correct because the the best value is going to depend on the data set so the answer thankfully in",
    "start": "1744480",
    "end": "1750640"
  },
  {
    "text": "Hadoop is just try them all or try a lot of them and in spark and and",
    "start": "1750640",
    "end": "1756519"
  },
  {
    "start": "1752000",
    "end": "2300000"
  },
  {
    "text": "specifically the Scala API this is really easy to do so I can write a a",
    "start": "1756519",
    "end": "1762080"
  },
  {
    "text": "triply nested for Loop here that that's what this is in Scola U that's going to try all possible combin",
    "start": "1762080",
    "end": "1768640"
  },
  {
    "text": "of two values for these three hyper parameters so I'm going to try both Genie impurity and entropy I'm going to",
    "start": "1768640",
    "end": "1775399"
  },
  {
    "text": "try depth one which should be very shallow and depth 20 which should be very deep and maybe I'll try building 10",
    "start": "1775399",
    "end": "1781919"
  },
  {
    "text": "bins versus 300 bins and so there's eight possible combinations here and I'm going to build a model for each one of",
    "start": "1781919",
    "end": "1787679"
  },
  {
    "text": "them here and evaluate its accuracy just like before and return the three values",
    "start": "1787679",
    "end": "1794159"
  },
  {
    "text": "I used and the accuracy at the end and then then I can just take those and sort",
    "start": "1794159",
    "end": "1799919"
  },
  {
    "text": "them descending by this accuracy and print them out so if you run this I",
    "start": "1799919",
    "end": "1805640"
  },
  {
    "text": "found when I ran it that uh I got about 91% accuracy with entropy as an impurity",
    "start": "1805640",
    "end": "1811840"
  },
  {
    "text": "measure depth 20 and trying 300 BS that's not bad I mean clearly that's",
    "start": "1811840",
    "end": "1818159"
  },
  {
    "text": "better than uh this combination which is only 63% and it's better than my initial guess of Genie for 100 right so just uh",
    "start": "1818159",
    "end": "1827480"
  },
  {
    "text": "I think it's kind of nice that just uh running this process going and getting a cup of coffee you can let this expensive",
    "start": "1827480",
    "end": "1833679"
  },
  {
    "text": "cluster do the work for you and improve your your predictability and in fact you could do",
    "start": "1833679",
    "end": "1839080"
  },
  {
    "text": "this all day you could try many more combinations um you could be smarter about this exploration of the",
    "start": "1839080",
    "end": "1844240"
  },
  {
    "text": "hyperparameter space but the point is that um you can we can often throw",
    "start": "1844240",
    "end": "1850480"
  },
  {
    "text": "hardware and raw Brute Force at a problem like this and have it do some work that's otherwise very very complex",
    "start": "1850480",
    "end": "1856320"
  },
  {
    "text": "or hard for us to to know or guess and uh that's that is the premise and promise of of",
    "start": "1856320",
    "end": "1862519"
  },
  {
    "text": "Hado so I guess we might observe that um generally speaking trying more values",
    "start": "1862519",
    "end": "1868679"
  },
  {
    "text": "more bends is better which makes some sense right if I'm trying more possible thresholds I'm more likely to get a",
    "start": "1868679",
    "end": "1875600"
  },
  {
    "text": "better best answer and the expense there is simply that takes more computation it",
    "start": "1875600",
    "end": "1881200"
  },
  {
    "text": "takes 30 times longer to try 300 values versus 10 so that that trade up is more obvious but um the the max step than the",
    "start": "1881200",
    "end": "1888320"
  },
  {
    "text": "the information measure that is not nearly as clear which one to pick so it's nice to be able to appeal to brute",
    "start": "1888320",
    "end": "1894200"
  },
  {
    "text": "force on with the spark and with Hadoop and the nicer thing still is that uh I",
    "start": "1894200",
    "end": "1900519"
  },
  {
    "text": "mean this is a parallel process this will build in parallel across your cluster but if you wanted to really make",
    "start": "1900519",
    "end": "1906159"
  },
  {
    "text": "your cluster heat up you could stick a uh dot par in here somewhere in um in",
    "start": "1906159",
    "end": "1911919"
  },
  {
    "text": "Scala and ask the uh driver program in Scala to evaluate all of the at the same",
    "start": "1911919",
    "end": "1918519"
  },
  {
    "text": "time in parallel so in parallel build eight models that are themselves parallel processes on the cluster so you",
    "start": "1918519",
    "end": "1925399"
  },
  {
    "text": "could easily fill your whole cluster with this process all at once which is a good thing if you got the hardware it's nice to be able to type a few characters",
    "start": "1925399",
    "end": "1932240"
  },
  {
    "text": "and make full use of it to get an answer faster so parallelism within parallelism can we do better yes we can",
    "start": "1932240",
    "end": "1939360"
  },
  {
    "text": "do better to get better from here we might need to um not just apply Brute Force but think a little bit through the",
    "start": "1939360",
    "end": "1946320"
  },
  {
    "text": "the process and make some improvements too our data and a representation so that will also give us a",
    "start": "1946320",
    "end": "1951960"
  },
  {
    "text": "win and I want to come back to this idea of categorical variables so as I say uh",
    "start": "1951960",
    "end": "1958240"
  },
  {
    "text": "the really the output it's a categorical variable it's a so it's a cover type there's no ordering to it there's no",
    "start": "1958240",
    "end": "1964639"
  },
  {
    "text": "such thing as Spruce being greater than Aspen or uh Spruce being um one less",
    "start": "1964639",
    "end": "1970279"
  },
  {
    "text": "than oak but we've encoded it as a number and also uh there's two other",
    "start": "1970279",
    "end": "1975519"
  },
  {
    "text": "values in the data set the soil type and the Wilderness type that are also not ordered values but they we have to",
    "start": "1975519",
    "end": "1981120"
  },
  {
    "text": "represent them as numbers or we've had to and the way they've been encoded in this data set is called one hot or one",
    "start": "1981120",
    "end": "1987320"
  },
  {
    "text": "event encoding um the practice here is to take a set of uh a categorical value that takes on N possible values and",
    "start": "1987320",
    "end": "1994679"
  },
  {
    "text": "encode it as n indicator variables that are zero or ones so for example if if I",
    "start": "1994679",
    "end": "2001000"
  },
  {
    "text": "had a a categorical value that took on three possible values cat dog and fish I",
    "start": "2001000",
    "end": "2006360"
  },
  {
    "text": "could encode that as three features exactly one of which was one and the rest of which are zero so you can think",
    "start": "2006360",
    "end": "2012080"
  },
  {
    "text": "of this as like an is cat is dog is fish feature and it turns out that's a",
    "start": "2012080",
    "end": "2017279"
  },
  {
    "text": "reasonably principled way to encode non-numeric data as numeric for most",
    "start": "2017279",
    "end": "2022679"
  },
  {
    "text": "algorithms purposes that's a pretty good encoding the only downside is if this has a lot of distinct values this blows",
    "start": "2022679",
    "end": "2028919"
  },
  {
    "text": "up and becomes pretty hard to manage in this case it's it's pretty reasonable we had one uh value with 40 values but that",
    "start": "2028919",
    "end": "2035559"
  },
  {
    "text": "wasn't so bad now the thing is this process is necessary if you're using an",
    "start": "2035559",
    "end": "2041120"
  },
  {
    "text": "algorithm that can't consume categorical values directly for example uh the support Vector machine decision force",
    "start": "2041120",
    "end": "2048118"
  },
  {
    "text": "can consume these categoricals directly or they have a principled way to to",
    "start": "2048119",
    "end": "2053878"
  },
  {
    "text": "evaluate rules based on categoricals without this encoding so in a way this is um this is uh not optimal for a",
    "start": "2053879",
    "end": "2061280"
  },
  {
    "text": "decision tree so we can undo this encoding and then try again and see if that actually helped with a different",
    "start": "2061280",
    "end": "2066720"
  },
  {
    "text": "representation so conceptually we're going to collapse these four and these 40 features down to",
    "start": "2066720",
    "end": "2075118"
  },
  {
    "text": "two this is going to become the index that was a one in the set of four and",
    "start": "2075119",
    "end": "2081520"
  },
  {
    "text": "this is the index that was a one in the set of 40 so here we're again still",
    "start": "2081520",
    "end": "2086638"
  },
  {
    "text": "representing these non-numeric values as numbers so we we have to be a little careful to to realize these aren't",
    "start": "2086639",
    "end": "2092398"
  },
  {
    "text": "numbers um but we can tell the decision tree algorithm to treat those as categorical values and and and proceed",
    "start": "2092399",
    "end": "2100320"
  },
  {
    "text": "accordingly so that ETL process is is still pretty straightforward split the line um we're going to chop off the four",
    "start": "2100320",
    "end": "2108839"
  },
  {
    "text": "and 40 values figure out where the 1.0 is and add those back as additional features on our feature vector and",
    "start": "2108839",
    "end": "2115920"
  },
  {
    "text": "proceed as before to return a labeled Point easy and then try again uh the",
    "start": "2115920",
    "end": "2121320"
  },
  {
    "text": "only difference here is we've we're telling the input that that 10th feature and that 11th feature are not actually",
    "start": "2121320",
    "end": "2126480"
  },
  {
    "text": "numbers they're categorical values taking on four and 40 different values respectively so you have to tell this",
    "start": "2126480",
    "end": "2132880"
  },
  {
    "text": "particular implementation about that ahead of time other than that it's the same we can run the same U",
    "start": "2132880",
    "end": "2138880"
  },
  {
    "text": "hyperparameter selection process and come out with new answers so in this case the best result was about 94%",
    "start": "2138880",
    "end": "2145920"
  },
  {
    "text": "accuracy with um similar settings I'd say I tried some different values here",
    "start": "2145920",
    "end": "2151280"
  },
  {
    "text": "um that 30 300 bins so that's not bad I mean it's clearly seems clearly like uh",
    "start": "2151280",
    "end": "2157079"
  },
  {
    "text": "we're getting better results from from this process and I guess the reason is that we're losing less information by",
    "start": "2157079",
    "end": "2165240"
  },
  {
    "text": "collapsing conceptually one value down to one value and letting the algorithm treat it accordingly rather than",
    "start": "2165240",
    "end": "2171520"
  },
  {
    "text": "encoding it and sort of obscuring the the data so data representation does matter and this is a another argument",
    "start": "2171520",
    "end": "2178599"
  },
  {
    "text": "for decision trees that they can consume these non-numeric features directly can we do better yes we can",
    "start": "2178599",
    "end": "2185760"
  },
  {
    "text": "still do better than this now this is where decision forests come in so as you as you probably guessed a",
    "start": "2185760",
    "end": "2193520"
  },
  {
    "text": "decision Forest is a bunch of decision trees put together and in particular we're going to talk about random decision",
    "start": "2193520",
    "end": "2200240"
  },
  {
    "text": "forests now to set the stage for this I want you to think um take a guess don't",
    "start": "2200240",
    "end": "2206520"
  },
  {
    "text": "don't don't don't tell me don't tell your your neighbors how many black cabs operate in London think for yourself um I don't",
    "start": "2206520",
    "end": "2215760"
  },
  {
    "text": "know I took a guess like you are right now and I was wrong of course but then again most people are wrong but we've we",
    "start": "2215760",
    "end": "2221760"
  },
  {
    "text": "we all probably made some educated guesses so I the real answer is is about 19,000 I guess about",
    "start": "2221760",
    "end": "2228280"
  },
  {
    "text": "10,000 maybe you got closer than I did uh I asked around our office and got some really startingly bad estimates of",
    "start": "2228280",
    "end": "2236480"
  },
  {
    "text": "the number of taxis in London uh but interestingly if I took the the mean of",
    "start": "2236480",
    "end": "2242280"
  },
  {
    "text": "all the guesses the mean was well closer to the correct answer than my answer was",
    "start": "2242280",
    "end": "2249280"
  },
  {
    "text": "and in fact it was closer to the correct answer than most answers were in fact so",
    "start": "2249280",
    "end": "2254760"
  },
  {
    "text": "even though lots of people were quite wrong on average people were reasonably",
    "start": "2254760",
    "end": "2260480"
  },
  {
    "text": "right this is the so-called wisdom of the crowds if we uh average um combine",
    "start": "2260480",
    "end": "2265760"
  },
  {
    "text": "some partially educated guesses we end up with a better guess overall and so",
    "start": "2265760",
    "end": "2271200"
  },
  {
    "text": "this is the phenomenon we want to leverage in machine learning when we talk about um Ensemble techniques where",
    "start": "2271200",
    "end": "2276800"
  },
  {
    "text": "we actually putting together many classifiers that may have different opinions about the right answer and",
    "start": "2276800",
    "end": "2282760"
  },
  {
    "text": "producing one one answer out of that that's maybe better than in individual classifi",
    "start": "2282760",
    "end": "2288200"
  },
  {
    "text": "output so let's see if we can use this phenomenon to improve um improve on decision",
    "start": "2288200",
    "end": "2294280"
  },
  {
    "text": "trees yes so this should for us Leverage The the wisdom of the crowds that's the the key takeaway",
    "start": "2294280",
    "end": "2299560"
  },
  {
    "text": "here now the problem here is everything I've described so far as deterministic so if you uh give this algorithm the",
    "start": "2299560",
    "end": "2306119"
  },
  {
    "start": "2300000",
    "end": "2346000"
  },
  {
    "text": "same data and same hyperparameter you'll end up with the same tree every time and that doesn't help uh it doesn't",
    "start": "2306119",
    "end": "2312480"
  },
  {
    "text": "help to have 100 copies of the same tree giving a prediction because they're all the same it's as if I have just one tree",
    "start": "2312480",
    "end": "2319720"
  },
  {
    "text": "what we really need is um a diversity of opinion here we need a bunch of slightly different trees giving slightly",
    "start": "2319720",
    "end": "2325520"
  },
  {
    "text": "different opinions good guesses good predictions but maybe slightly different ones that we can average together into a",
    "start": "2325520",
    "end": "2332359"
  },
  {
    "text": "better overall prediction so how do we do that how do we actually get different answers out of",
    "start": "2332359",
    "end": "2339760"
  },
  {
    "text": "the same algorithm and same data two things number one we can",
    "start": "2339760",
    "end": "2347040"
  },
  {
    "start": "2346000",
    "end": "2391000"
  },
  {
    "text": "um handicap each tree a little bit by showing it only part of the data so for",
    "start": "2347040",
    "end": "2353400"
  },
  {
    "text": "example um you could make 10 trees Each of which look at a different 90% of the data and for which a different 10% of",
    "start": "2353400",
    "end": "2360359"
  },
  {
    "text": "the data is held out so each individual tree is going to be a little bit different since it's seen different to",
    "start": "2360359",
    "end": "2365880"
  },
  {
    "text": "input it's going to be a little bit worse off than a tree that looked at all the data since it has less input but",
    "start": "2365880",
    "end": "2372000"
  },
  {
    "text": "strangely the the average guess over all those 10 trees is going to be better in general than one tree built over all the",
    "start": "2372000",
    "end": "2379920"
  },
  {
    "text": "data that's a powerful idea um so and this is where the the idea the",
    "start": "2379920",
    "end": "2386760"
  },
  {
    "text": "motivation for random forests comes in we could also do the same thing but with",
    "start": "2386760",
    "end": "2391960"
  },
  {
    "start": "2391000",
    "end": "2466000"
  },
  {
    "text": "features you could have different trees look at different subsets of the input and make different predictions based on",
    "start": "2391960",
    "end": "2397760"
  },
  {
    "text": "based on that um for example this tree could look at just the elevation and the",
    "start": "2397760",
    "end": "2403920"
  },
  {
    "text": "slope and so on this one might be looking at soil and Wilderness type and they would get different answers indeed",
    "start": "2403920",
    "end": "2409200"
  },
  {
    "text": "but maybe overall the the combined answer is is inde better you can do both you can slice it up both ways in fact",
    "start": "2409200",
    "end": "2416200"
  },
  {
    "text": "you can slice it up uh to some degree randomly which also generates some degree of difference in uh the the the",
    "start": "2416200",
    "end": "2423040"
  },
  {
    "text": "outputs of the trees and this phenomenon makes decision for us the building",
    "start": "2423040",
    "end": "2428440"
  },
  {
    "text": "process especially nice for Hadoop Hadoop is fundamentally a data parallel",
    "start": "2428440",
    "end": "2434119"
  },
  {
    "text": "Paradigm where we have to uh live with data distributed all across a cluster",
    "start": "2434119",
    "end": "2439760"
  },
  {
    "text": "and in a world where it's relatively expensive to move data around so we need to do small operations on small subsets",
    "start": "2439760",
    "end": "2445560"
  },
  {
    "text": "of the data and combine the answers at the end and that's exactly how decision forests work the idea is to build uh",
    "start": "2445560",
    "end": "2452280"
  },
  {
    "text": "weaker trees on small looks at the data independently and then combine them into",
    "start": "2452280",
    "end": "2457440"
  },
  {
    "text": "Ensemble or Forest at the end so that's another reason decision forests are nice in a data parallel Paradigm like",
    "start": "2457440",
    "end": "2464000"
  },
  {
    "text": "K so if we do something like this we will find we get different trees uh not just different um thresholds throughout",
    "start": "2464000",
    "end": "2471200"
  },
  {
    "start": "2466000",
    "end": "2499000"
  },
  {
    "text": "the tree but totally different structures trying different features um making different predictions",
    "start": "2471200",
    "end": "2476800"
  },
  {
    "text": "Etc so we will get a diversity of opinion from these from these trees and we can make hundreds of them or even a",
    "start": "2476800",
    "end": "2482319"
  },
  {
    "text": "thousand if we want and then to make a prediction out of all of them we are simply",
    "start": "2482319",
    "end": "2487920"
  },
  {
    "text": "uh taking a majority vote for example asking each of the trees for a prediction and predicting the one that",
    "start": "2487920",
    "end": "2493000"
  },
  {
    "text": "occurs most frequently pretty simple stuff we can thankfully do that in mb as",
    "start": "2493000",
    "end": "2499880"
  },
  {
    "start": "2499000",
    "end": "3168000"
  },
  {
    "text": "well this this is also implemented in in spark very similar except that there are",
    "start": "2499880",
    "end": "2506000"
  },
  {
    "text": "two new parameters now one is the number of trees to build and another is called the feature subset",
    "start": "2506000",
    "end": "2511920"
  },
  {
    "text": "strategy this is really uh the means the how it chooses",
    "start": "2511920",
    "end": "2517680"
  },
  {
    "text": "how many and which different features to randomly try at each level and to some degree it also controls how the how the",
    "start": "2517680",
    "end": "2524280"
  },
  {
    "text": "examples are a portion to the trees so in a way this controls how um how",
    "start": "2524280",
    "end": "2529440"
  },
  {
    "text": "different pieces of the data get fed to different trees for purposes here we'll leave it at Auto but there are some",
    "start": "2529440",
    "end": "2534839"
  },
  {
    "text": "other settings and the rest is the same though same hyper parameters as for the decision tree model and if you go",
    "start": "2534839",
    "end": "2540800"
  },
  {
    "text": "through the same process and try a bunch of hyperparameters and try different numbers of trees uh you can get pretty easily",
    "start": "2540800",
    "end": "2547760"
  },
  {
    "text": "better accuracy than than we got last time I got about 96% accuracy with the same process with just a a couple hours",
    "start": "2547760",
    "end": "2555000"
  },
  {
    "text": "of uh of tuning on on my cluster I imagine if you tried a little harder you could get 97% I don't know I think the",
    "start": "2555000",
    "end": "2563040"
  },
  {
    "text": "the best um algorithms I've seen for for CUV type um have end up with a maybe a",
    "start": "2563040",
    "end": "2569319"
  },
  {
    "text": "2% error rate or so so this is pretty close to state-of-the-art for this particular data set that's not bad for a",
    "start": "2569319",
    "end": "2576880"
  },
  {
    "text": "a simple algorithm that's implemented for you um that you can use brute force and a bunch of cluster power to to tune",
    "start": "2576880",
    "end": "2582880"
  },
  {
    "text": "for you and get nearly state-of-the-art results and indeed this is um random forests are generally a pretty",
    "start": "2582880",
    "end": "2589559"
  },
  {
    "text": "competitive algorithm for a a large range of problems so definitely worth knowing about definitely worth learning",
    "start": "2589559",
    "end": "2595359"
  },
  {
    "text": "about in the context of a Hado and Spark now that's not the end of it",
    "start": "2595359",
    "end": "2602200"
  },
  {
    "text": "though uh let if you've gone to all this trouble of preparing your data and making labeled points you can easily try",
    "start": "2602200",
    "end": "2609599"
  },
  {
    "text": "other algorithms in MLB for example there is an implementation of the support Vector machine uh or logistic",
    "start": "2609599",
    "end": "2615559"
  },
  {
    "text": "regression and they take the the input as uh the original input we showed is is something you could easily feed into",
    "start": "2615559",
    "end": "2621920"
  },
  {
    "text": "these algorithms as well and you could easily put those into an evaluation loop as well and see what their accuracy is",
    "start": "2621920",
    "end": "2628680"
  },
  {
    "text": "like so that's another nice thing about using MLB it does have a fairly standard and cohesive set of implementations with",
    "start": "2628680",
    "end": "2635480"
  },
  {
    "text": "the same sort of input and same sort of out out puts so you could easily compare and contrast them another fun thing about spark is",
    "start": "2635480",
    "end": "2642680"
  },
  {
    "text": "that it's not just um for ETL it's not just for machine learning it has a streaming component called spark",
    "start": "2642680",
    "end": "2648280"
  },
  {
    "text": "streaming which is spark as applied to a stream of data coming off of a a message",
    "start": "2648280",
    "end": "2653440"
  },
  {
    "text": "CU like kofka for example and whatever you do in spark MLB can be used within",
    "start": "2653440",
    "end": "2659400"
  },
  {
    "text": "spark streaming so that decision tree model could be directly reused in a spark streaming job that would maybe",
    "start": "2659400",
    "end": "2666440"
  },
  {
    "text": "score data make predictions on data as it arrives in the cluster now maybe",
    "start": "2666440",
    "end": "2671839"
  },
  {
    "text": "that's not so useful for Forest cover type but it might be useful if your model is predicting fraud um if it's uh",
    "start": "2671839",
    "end": "2678880"
  },
  {
    "text": "detecting anomalies and sensor data for example so it's nice to be able to just directly uh Port that code move that",
    "start": "2678880",
    "end": "2685040"
  },
  {
    "text": "model into a streaming job with no with no change or rewriting and maybe it's also worth",
    "start": "2685040",
    "end": "2691559"
  },
  {
    "text": "noting here at the end to tie it back to the beginning that random decision forests are not just classification algorithms we've been using it as a",
    "start": "2691559",
    "end": "2697880"
  },
  {
    "text": "classifier here to predict a um a discrete categorical value but they can also be used for regression which is uh",
    "start": "2697880",
    "end": "2704440"
  },
  {
    "text": "means predicting um continuous numeric value so you can predict numbers as well as as categories with this exact same",
    "start": "2704440",
    "end": "2710800"
  },
  {
    "text": "technique and that's also implemented in in Sparky M lip worth",
    "start": "2710800",
    "end": "2718000"
  },
  {
    "text": "knowing that's all from me thank you very much um are there questions",
    "start": "2718200",
    "end": "2724800"
  },
  {
    "text": "objections sir you showed that the decision Tre a normal decision Tre is more explanatory than for example",
    "start": "2724800",
    "end": "2730920"
  },
  {
    "text": "support Factor machine or a logistic model um how about random Forest because",
    "start": "2730920",
    "end": "2737000"
  },
  {
    "text": "it seems to me that it's not really explanatory anymore right so the a",
    "start": "2737000",
    "end": "2742680"
  },
  {
    "text": "random decision Forest is really just a bunch of decision trees so it's more",
    "start": "2742680",
    "end": "2747960"
  },
  {
    "text": "complicated than that you're digging through a bunch of these trees not just one but underneath they're all just the",
    "start": "2747960",
    "end": "2753720"
  },
  {
    "text": "same decision trees um inside a forest but do you then have any uh does this",
    "start": "2753720",
    "end": "2759160"
  },
  {
    "text": "have any implications for example using this in practice because for example I work for HR company in nland and um I",
    "start": "2759160",
    "end": "2766599"
  },
  {
    "text": "know that when we have uh matching and searching models then people find it um",
    "start": "2766599",
    "end": "2772359"
  },
  {
    "text": "have difficulty mental difficulty to use it when they don't know what the results coming from do you uh is this harder for",
    "start": "2772359",
    "end": "2780559"
  },
  {
    "text": "uh decision Forest than this than you know this simpler decision Tre model I",
    "start": "2780559",
    "end": "2785680"
  },
  {
    "text": "think it's probably the inter interpretability of the model is about the same between a decision tree and decision Forest since a forest is just",
    "start": "2785680",
    "end": "2791359"
  },
  {
    "text": "many trees um but you're right that's exactly the kind of case where uh the the trees are nice because often people",
    "start": "2791359",
    "end": "2798599"
  },
  {
    "text": "either have to or just want to understand how the model's thinking and you can read that off directly in the",
    "start": "2798599",
    "end": "2804880"
  },
  {
    "text": "case of a forest or or trees yes and then one final question um how does the",
    "start": "2804880",
    "end": "2809960"
  },
  {
    "text": "how how do decision trees and random forest in general uh compared to say for example support support Factor machines",
    "start": "2809960",
    "end": "2816480"
  },
  {
    "text": "in in in a Cousy Precision recall things like that uh how does in general how",
    "start": "2816480",
    "end": "2821599"
  },
  {
    "text": "does this perform versus spms I don't think there's one answer to that um I guess on um interpretability aside",
    "start": "2821599",
    "end": "2828839"
  },
  {
    "text": "decision Forest have some advantage in that they can um do multiclass pretty easily and consume categorical variables",
    "start": "2828839",
    "end": "2835720"
  },
  {
    "text": "that said I generally speaking decision forests are a Brute Force approach which is okay in some cases not in others um",
    "start": "2835720",
    "end": "2843800"
  },
  {
    "text": "they tend to struggle a little bit with a very large number of features and svms have some tricks that make a very large",
    "start": "2843800",
    "end": "2850319"
  },
  {
    "text": "number of features work well so there I think they have just different properties there's no one answer thank",
    "start": "2850319",
    "end": "2859440"
  },
  {
    "text": "you question here yes sir so the question is when I",
    "start": "2859599",
    "end": "2866319"
  },
  {
    "text": "build a decision Forest am I using the same hyperparameters for every tree yes",
    "start": "2866319",
    "end": "2871480"
  },
  {
    "text": "so I'm making one call here to train the classifier and I'm giving it one set of hyper parameters I can build different",
    "start": "2871480",
    "end": "2876599"
  },
  {
    "text": "for Forest different hyper parameters but within one Forest um that's constant the the features and examples they look",
    "start": "2876599",
    "end": "2882720"
  },
  {
    "text": "at are different across the trees though okay and will there ever be an",
    "start": "2882720",
    "end": "2888839"
  },
  {
    "text": "option to change hyper parameters within a forest or isn't that a n would you ever want to use different hyper",
    "start": "2888839",
    "end": "2895079"
  },
  {
    "text": "parameters within one Forest um it's not crazy I think it's probably not that important you would generally want to",
    "start": "2895079",
    "end": "2901760"
  },
  {
    "text": "have one consistent um measure within a forest although I guess uh decision",
    "start": "2901760",
    "end": "2908760"
  },
  {
    "text": "Force an example of Ensemble learning where you don't care what the the the individual classifiers are you're just averaging their results so it wouldn't",
    "start": "2908760",
    "end": "2915520"
  },
  {
    "text": "be crazy to combine two forests that were built in different ways with different hyperparameters and average",
    "start": "2915520",
    "end": "2920839"
  },
  {
    "text": "all their results maybe not not as important or not as helpful but there's some use for it but uh but no this",
    "start": "2920839",
    "end": "2927680"
  },
  {
    "text": "doesn't exist in this API okay there's a question in the",
    "start": "2927680",
    "end": "2933720"
  },
  {
    "text": "back right so how is performance affected by the side of the forest so uh",
    "start": "2933720",
    "end": "2939520"
  },
  {
    "text": "certainly the prediction time grows linearly with the the number of trees since you have to run each example",
    "start": "2939520",
    "end": "2945400"
  },
  {
    "text": "through all of the trees you can do that in parallel on the plus side it also of course depends on the depth of the tree",
    "start": "2945400",
    "end": "2952119"
  },
  {
    "text": "but that's um that's less of a factor since that's kind of um you know that time grows logarithmically with the",
    "start": "2952119",
    "end": "2958040"
  },
  {
    "text": "number of decision nodes and that's that's not as generally a problem so generally I would describe decision",
    "start": "2958040",
    "end": "2964640"
  },
  {
    "text": "Forest as fairly fast to score they're not as fast as a linear model which is just a doing a DOT product maybe but",
    "start": "2964640",
    "end": "2971160"
  },
  {
    "text": "generally not not really a problem yeah I have a question uh would a",
    "start": "2971160",
    "end": "2977400"
  },
  {
    "text": "similar approach uh could be applied to not to trees but to to a linear model so",
    "start": "2977400",
    "end": "2983200"
  },
  {
    "text": "sure would that deliver some advantages in fact could you put together an ensemble of linear models yes definitely so decision forests are really just",
    "start": "2983200",
    "end": "2990319"
  },
  {
    "text": "ensembles for decision trees you can make an ensemble of support Vector machine classifiers as well and would",
    "start": "2990319",
    "end": "2995880"
  },
  {
    "text": "see a similar effect um yes I think that's done as well I think that the",
    "start": "2995880",
    "end": "3002000"
  },
  {
    "text": "benefit of ensembles is maybe more pronounced in decision trees or somehow is a better a nice fit for the way",
    "start": "3002000",
    "end": "3008280"
  },
  {
    "text": "decision trees work but there's nothing specific about that idea that's specific to decision trees yes one more okay uh",
    "start": "3008280",
    "end": "3017760"
  },
  {
    "text": "there is any relation between the number of features and the number of trees that should be used okay is there a relationship",
    "start": "3017760",
    "end": "3024359"
  },
  {
    "text": "between the number of input features and the number of trees that should be used uh that's an interesting one so it I",
    "start": "3024359",
    "end": "3030760"
  },
  {
    "text": "suppose it depends on how many different features each tree looks at because generally speaking each tree will not",
    "start": "3030760",
    "end": "3036680"
  },
  {
    "text": "consider all the features so for example for a classification problem often each tree will look at about the square root",
    "start": "3036680",
    "end": "3043160"
  },
  {
    "text": "of the number of features that's I think that's the default in R and most people have just followed followed that so",
    "start": "3043160",
    "end": "3049319"
  },
  {
    "text": "given that I suppose if you only had a couple trees uh and a large number of features you would probably end up not",
    "start": "3049319",
    "end": "3056200"
  },
  {
    "text": "ever looking looking at some of your your features just randomly because only a few are chosen so may I suppose you",
    "start": "3056200",
    "end": "3062319"
  },
  {
    "text": "want to have enough trees that you suspect you've used all your features at least once on",
    "start": "3062319",
    "end": "3069200"
  },
  {
    "text": "average one more was there any occurrence of a",
    "start": "3069960",
    "end": "3075280"
  },
  {
    "text": "deadlock because that can be a chance of contradiction of uh when you combine",
    "start": "3075280",
    "end": "3080520"
  },
  {
    "text": "different trees there can be contradiction of different uh subset of features so you know was is there any",
    "start": "3080520",
    "end": "3087319"
  },
  {
    "text": "deadlock I mean it was running for long and was there predictability predictability reached or did you face",
    "start": "3087319",
    "end": "3094200"
  },
  {
    "text": "anything like that is there a possibility of Deadlock in the implementation did did you get any deadlock I mean because when you compare",
    "start": "3094200",
    "end": "3101400"
  },
  {
    "text": "between different trees there can be a contradiction right because different divions by different features in One",
    "start": "3101400",
    "end": "3108520"
  },
  {
    "text": "Tree can be different from another tree so oh do the trees disagree yes absolutely so uh hopefully the trees",
    "start": "3108520",
    "end": "3115119"
  },
  {
    "text": "generally speaking agree on a on a on a prediction and that's your that's your final prediction but they won't all",
    "start": "3115119",
    "end": "3120280"
  },
  {
    "text": "agree in general I guess the the nice thing about that is you could return to the call or not just one answer but a",
    "start": "3120280",
    "end": "3126119"
  },
  {
    "text": "distribution of predictions uh 70% of the trees say it's class one 20% say",
    "start": "3126119",
    "end": "3131200"
  },
  {
    "text": "it's class two and in fact even the trees themselves can give you a a distribution over the categories and you",
    "start": "3131200",
    "end": "3137680"
  },
  {
    "text": "can average those distributions so yes there's certainly the trees will arrive at different answers sometimes but",
    "start": "3137680",
    "end": "3144079"
  },
  {
    "text": "that's fine that's to be expected um thank you very much",
    "start": "3144079",
    "end": "3150400"
  },
  {
    "text": "[Music]",
    "start": "3158350",
    "end": "3167409"
  }
]