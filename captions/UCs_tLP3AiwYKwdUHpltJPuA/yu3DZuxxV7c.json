[
  {
    "start": "0",
    "end": "132000"
  },
  {
    "text": "[Music] cool thank you um just so I know what",
    "start": "6990",
    "end": "13139"
  },
  {
    "text": "level to speak at raise your hands if you know who Bach is great raise your",
    "start": "13139",
    "end": "20550"
  },
  {
    "text": "hand if you know what a neural network is oh this is the perfect crowd awesome",
    "start": "20550",
    "end": "26779"
  },
  {
    "text": "if you don't know don't worry I'm going to cover the very basics of both so",
    "start": "26779",
    "end": "33600"
  },
  {
    "text": "let's talk about Bach I'm going to play to you some music [Music]",
    "start": "33600",
    "end": "49730"
  },
  {
    "text": "now what you just heard is what's known as a coral there are four parts to it a",
    "start": "49730",
    "end": "55500"
  },
  {
    "text": "soprano alto tenor bass playing at the exact same time and there's very regular",
    "start": "55500",
    "end": "60540"
  },
  {
    "text": "phrasing structure where you have the beginning of a phrase the determination of a phrase followed by the next phrase",
    "start": "60540",
    "end": "66350"
  },
  {
    "text": "except that wasn't Bach rather that was",
    "start": "66350",
    "end": "72120"
  },
  {
    "text": "a computer algorithm called Bach bot and that was one sample out of its outputs if you don't believe me it's on",
    "start": "72120",
    "end": "78450"
  },
  {
    "text": "soundcloud it's called sample one go listen for yourself so instead of",
    "start": "78450",
    "end": "83490"
  },
  {
    "text": "talking about box today I'm going to talk to you about Bach bot hi my name is",
    "start": "83490",
    "end": "89910"
  },
  {
    "text": "phiman and it's a pleasure to be here at Amsterdam and today we'll talk about autumn is automatic stylistic",
    "start": "89910",
    "end": "95250"
  },
  {
    "text": "composition using long short term memory so then a background about myself I'm",
    "start": "95250",
    "end": "102270"
  },
  {
    "text": "currently a software engineer at gigster where I walk at work on interesting automation problems regarding I'm taking",
    "start": "102270",
    "end": "108150"
  },
  {
    "text": "contracts divided them into sub contracts and then freelancing them out the work on Bach bot was done as part of",
    "start": "108150",
    "end": "114450"
  },
  {
    "text": "my master's thesis where which I did at the University of Cambridge with Microsoft Research Cambridge in line",
    "start": "114450",
    "end": "121470"
  },
  {
    "text": "with the track here I do not have a PhD and so and I still can do machine learning so this is the fact this is a",
    "start": "121470",
    "end": "127440"
  },
  {
    "text": "fact you can do machine learning without a PhD for those of you who just want to",
    "start": "127440",
    "end": "134370"
  },
  {
    "start": "132000",
    "end": "188000"
  },
  {
    "text": "know what's going to happen and then get out of here because it's not interesting here is the executive summary I'm going",
    "start": "134370",
    "end": "140310"
  },
  {
    "text": "to talk to you about how to train end to end starting from data sets preparation all the way to model tuning and",
    "start": "140310",
    "end": "145830"
  },
  {
    "text": "deployment of a deep recurrent neural network for music this neural network is",
    "start": "145830",
    "end": "151290"
  },
  {
    "text": "capable of polysemy multiple simultaneous voices at the same time it's capable automatic composition",
    "start": "151290",
    "end": "157110"
  },
  {
    "text": "generating a composition completely from scratch as well as harmonization given",
    "start": "157110",
    "end": "162600"
  },
  {
    "text": "some fixed parts such as the soprano line of the melody generate the remaining supporting parts this model",
    "start": "162600",
    "end": "169260"
  },
  {
    "text": "learns music theory without being told to do so providing empirical validation of what music theorists have been using",
    "start": "169260",
    "end": "175260"
  },
  {
    "text": "for centuries and finally it's evaluated on an online musical Turing test we're out of 1700",
    "start": "175260",
    "end": "182170"
  },
  {
    "text": "participants only nine percent are able to distinguish actual Bach from Bach Bach",
    "start": "182170",
    "end": "188970"
  },
  {
    "start": "188000",
    "end": "281000"
  },
  {
    "text": "when I set off on this research there were three primary goals the first",
    "start": "189360",
    "end": "194410"
  },
  {
    "text": "question I wanted to answer was what is the frontier of computational creativity now creativity is something we take to",
    "start": "194410",
    "end": "202000"
  },
  {
    "text": "be an 8 li human innately special in some sense computers shouldn't ought not to be able to replicate this about us is",
    "start": "202000",
    "end": "208420"
  },
  {
    "text": "this actually true can we have computers generate art that is convincingly human",
    "start": "208420",
    "end": "214200"
  },
  {
    "text": "the second question I wanted to answer was how much does deep learning impacted",
    "start": "214200",
    "end": "219310"
  },
  {
    "text": "automatic music composition now automatic music composition is a special field it has been dominated by symbolic",
    "start": "219310",
    "end": "225040"
  },
  {
    "text": "methods which utilize things like formal grammars or context-free grammars such as this parse tree we've seen",
    "start": "225040",
    "end": "231070"
  },
  {
    "text": "connectionist methods in the early 19th century however we have it however they",
    "start": "231070",
    "end": "236230"
  },
  {
    "text": "have they followed in popularity and most recent systems have used symbolic methods with the work here I wanted to",
    "start": "236230",
    "end": "242350"
  },
  {
    "text": "see did the new advances in deep learning in the last 10 years can they be transferred over to this particular",
    "start": "242350",
    "end": "248290"
  },
  {
    "text": "problem domain and finally the last question I wanted to look at is how do",
    "start": "248290",
    "end": "253750"
  },
  {
    "text": "we evaluate these generative models I mean we've seen we've seen in the previous talk a lot of a lot of models",
    "start": "253750",
    "end": "259630"
  },
  {
    "text": "they generate art we look at it and as the author we say oh that's convincing but oh that's beautiful and great that",
    "start": "259630",
    "end": "267340"
  },
  {
    "text": "might be a perfectly valid use case but it's not sufficient for publication to publish something we need to establish a",
    "start": "267340",
    "end": "272770"
  },
  {
    "text": "standardized benchmark and we need to be able to evaluate all of our models about it so we can objectively say which model",
    "start": "272770",
    "end": "278140"
  },
  {
    "text": "is better than the other now if you're still here I'm assuming you're",
    "start": "278140",
    "end": "284020"
  },
  {
    "start": "281000",
    "end": "346000"
  },
  {
    "text": "interested this is the outline we'll start with a quick primer on music theory giving you just the basic",
    "start": "284020",
    "end": "290290"
  },
  {
    "text": "terminology you need to understand the remainder of this presentation we'll talk about how to prepare a data set of",
    "start": "290290",
    "end": "295330"
  },
  {
    "text": "Bach Corral's well then gate will get the give a primer on recurrent neural networks which is the actual deep learning model",
    "start": "295330",
    "end": "301630"
  },
  {
    "text": "architecture used to build Bach Bach we'll talk about the Bach Bach model itself the tips and tricks and",
    "start": "301630",
    "end": "308110"
  },
  {
    "text": "techniques that we used in order to train it have it run successfully as well as deploy it and then we'll show the",
    "start": "308110",
    "end": "313760"
  },
  {
    "text": "results well show how this model is able to capture statistical regularities in box musical style and we'll prove a we won't",
    "start": "313760",
    "end": "320840"
  },
  {
    "text": "prove we'll provide very convincing evidence that music theory does have theoretical gesture empirical",
    "start": "320840",
    "end": "326180"
  },
  {
    "text": "justification and finally I'll show the results of the musical Turing test which",
    "start": "326180",
    "end": "331400"
  },
  {
    "text": "was our proposed evaluation methodology for saying yes this model has solves our research goal the the task of automatically composing",
    "start": "331400",
    "end": "339020"
  },
  {
    "text": "convincing Bach chorale is more closed than open of a problem as a result of Bach plot and if you're a hands-on type",
    "start": "339020",
    "end": "348230"
  },
  {
    "start": "346000",
    "end": "731000"
  },
  {
    "text": "of learner we've containerized the entire deployment so if you go to my website here I have a copy of the slides",
    "start": "348230",
    "end": "353780"
  },
  {
    "text": "which have all of these instructions you run this eight lines of code and it runs this entire and pipeline right here",
    "start": "353780",
    "end": "359930"
  },
  {
    "text": "where it takes the corrals it pre processes them puts them into a data store trains of trains the deep learning",
    "start": "359930",
    "end": "365600"
  },
  {
    "text": "model samples the deep learning model produces outputs that you can listen to",
    "start": "365600",
    "end": "371440"
  },
  {
    "text": "let's start with basic music theory now when people think of music this is",
    "start": "372729",
    "end": "380000"
  },
  {
    "text": "usually what you think about you got these bar lines you got notes and these notes are on different horizontal and",
    "start": "380000",
    "end": "385310"
  },
  {
    "text": "vertical positions some of them have interesting ties some of them of dots this is interesting little weird hat",
    "start": "385310",
    "end": "390470"
  },
  {
    "text": "looking thing we don't need all this we need three fundamental concepts the",
    "start": "390470",
    "end": "397039"
  },
  {
    "text": "first is pitch pitch is often referred to as how low or how high a note is so",
    "start": "397039",
    "end": "403640"
  },
  {
    "text": "if I play this we can distinguish that",
    "start": "403640",
    "end": "410630"
  },
  {
    "text": "some notes are lower and some notes are higher in frequency and that corresponds to the vertical axis here as the notes",
    "start": "410630",
    "end": "417470"
  },
  {
    "text": "of the notes sound ascending they appear ascending on the bar lines the second",
    "start": "417470",
    "end": "424160"
  },
  {
    "text": "attribute we need is duration and this is really how long a notice so this one",
    "start": "424160",
    "end": "429860"
  },
  {
    "text": "note these two notes these four and these eight all have equal total duration but they are they're having zuv",
    "start": "429860",
    "end": "437150"
  },
  {
    "text": "each other so if we take a listen",
    "start": "437150",
    "end": "440289"
  },
  {
    "text": "the general intuition is the more bars there are on these tides the faster the",
    "start": "443240",
    "end": "448860"
  },
  {
    "text": "notes appear with just those two concepts this is starting to make a",
    "start": "448860",
    "end": "454229"
  },
  {
    "text": "little bit more sense this right here is twice as fast as this note we can see this note is higher than this note and",
    "start": "454229",
    "end": "460460"
  },
  {
    "text": "you can generalize this to the remainder of this but there's still this funny hat",
    "start": "460460",
    "end": "465840"
  },
  {
    "text": "looking thing we'll get to the hat in a sec but with pitch and duration we can",
    "start": "465840",
    "end": "473610"
  },
  {
    "text": "rewrite the music like so rather than representing it using notes which may be kind of cryptic we show it here as a",
    "start": "473610",
    "end": "479940"
  },
  {
    "text": "matrix where on the x axis we have time so the duration and on the y-axis we",
    "start": "479940",
    "end": "486000"
  },
  {
    "text": "have pitch how high or low and frequency that note is and what we've done is we've taken the symbolic representation",
    "start": "486000",
    "end": "492270"
  },
  {
    "text": "of music and we've turned it into a digital computable format that we can train models on back to the hat looking",
    "start": "492270",
    "end": "500310"
  },
  {
    "text": "thing this is called a Fermata and Bach used it to denote the ends of phrases we",
    "start": "500310",
    "end": "506970"
  },
  {
    "text": "had originally said about this research completely neglecting for modest and we found that the phrases generated by the",
    "start": "506970",
    "end": "512250"
  },
  {
    "text": "model just kind of wandered they never seem to end there was no sense of resolution or conclusion and that was",
    "start": "512250",
    "end": "518099"
  },
  {
    "text": "unrealistic but by adding these four modest all of a sudden the model turned around and we and we suddenly found",
    "start": "518099",
    "end": "524459"
  },
  {
    "text": "realistic phrasing structure cool and",
    "start": "524459",
    "end": "529709"
  },
  {
    "text": "that's all the music you need to know the rest of it is machine learning now",
    "start": "529709",
    "end": "535740"
  },
  {
    "text": "the biggest part of a machine learning engineer's job is preparing their data sets this is a very painful task usually",
    "start": "535740",
    "end": "541950"
  },
  {
    "text": "have to scour the internet or find some standardized data set that you train and evaluate your models on that usually",
    "start": "541950",
    "end": "547320"
  },
  {
    "text": "these data sets have to be pre processed and massaged into a format that's amenable for learning upon and for us it",
    "start": "547320",
    "end": "554279"
  },
  {
    "text": "was no different box works however fortunately over the years have been",
    "start": "554279",
    "end": "559589"
  },
  {
    "text": "transcribed into excuse my German Bach worka Vera - Nix BW sorry",
    "start": "559589",
    "end": "567740"
  },
  {
    "text": "dwv is how I've been referring to this corpus it contains about all 438",
    "start": "567740",
    "end": "573089"
  },
  {
    "text": "harmonizations of Bach Corral's and conveniently it is available through the software package",
    "start": "573089",
    "end": "578520"
  },
  {
    "text": "called music21 this is a Python package that you can just tip install and then import it and",
    "start": "578520",
    "end": "584310"
  },
  {
    "text": "now you have an iterator over a collection of music the first",
    "start": "584310",
    "end": "590430"
  },
  {
    "text": "pre-processing step we did is we took the music the original music here and we did two things we transposed it and then",
    "start": "590430",
    "end": "598410"
  },
  {
    "text": "we quantize it in time now you can notice the transposition by looking at these accidentals right here these two",
    "start": "598410",
    "end": "604020"
  },
  {
    "text": "little funny backwards or forwards B's and then they're absent over here furthermore that note has shifted up by",
    "start": "604020",
    "end": "611730"
  },
  {
    "text": "half a line that's a little hard to see but it's happening and the reason why we",
    "start": "611730",
    "end": "617100"
  },
  {
    "text": "did this is we didn't want to learn key signature key signature is usually something decided by the author before",
    "start": "617100",
    "end": "622530"
  },
  {
    "text": "the pieces even begun to compose and so we can and so key signature itself can be injected as a pre-processing step",
    "start": "622530",
    "end": "627990"
  },
  {
    "text": "where we sample over all the keys Bach did use so we removed key fingers from the equation through transposition and",
    "start": "627990",
    "end": "634170"
  },
  {
    "text": "I'll justify why that's an okay thing to do in the next slide this first measure",
    "start": "634170",
    "end": "640200"
  },
  {
    "text": "is written is is a progression of five notes written in C major and then what I",
    "start": "640200",
    "end": "645540"
  },
  {
    "text": "did in the next measure is I just moved it up by five whole steps [Music]",
    "start": "645540",
    "end": "657399"
  },
  {
    "text": "so yeah the pitch did change it's relatively higher it's absolutely higher",
    "start": "657890",
    "end": "662910"
  },
  {
    "text": "on all accounts but the relations between the notes didn't change and the sensation the the",
    "start": "662910",
    "end": "669300"
  },
  {
    "text": "motifs that the music is bringing out those still remain fairly constant even after transposition quantization that",
    "start": "669300",
    "end": "677130"
  },
  {
    "text": "however is a different story if I go back to slides will notice quantization",
    "start": "677130",
    "end": "682230"
  },
  {
    "text": "to this 30-second note and turn it into a sixteenth note by removing that second bar we've distorted time is that a",
    "start": "682230",
    "end": "689610"
  },
  {
    "text": "problem it is it's not it's not perfect",
    "start": "689610",
    "end": "696920"
  },
  {
    "text": "but it's a very minor problem so over here I've plotted a histogram of all of",
    "start": "696920",
    "end": "702360"
  },
  {
    "text": "the durations inside of the corral corpus and this quantization affects only 0.2% of all the notes that we're",
    "start": "702360",
    "end": "709950"
  },
  {
    "text": "training on the reason that we do it is by quantizing in time we're able to get discrete representations in both time as",
    "start": "709950",
    "end": "716940"
  },
  {
    "text": "well as in pitch whereas working on a continuous time axis now you have to deal computers are discrete and are",
    "start": "716940",
    "end": "723540"
  },
  {
    "text": "unable to operate on the continuous representation has to be quantized into a digital format somehow the last",
    "start": "723540",
    "end": "732630"
  },
  {
    "start": "731000",
    "end": "804000"
  },
  {
    "text": "challenge polyphony so polysemy is the presence of multiple simultaneous voices",
    "start": "732630",
    "end": "738030"
  },
  {
    "text": "so far the examples that I've shown you you've just heard a single voice playing at any given time but a Corral has four",
    "start": "738030",
    "end": "744150"
  },
  {
    "text": "voices the soprano the alto the tenor the bass and so here's a question for you if I have four voices and they can",
    "start": "744150",
    "end": "752730"
  },
  {
    "text": "each represent 128 different pitches that's the constraint in MIDI representation of music how many",
    "start": "752730",
    "end": "759540"
  },
  {
    "text": "different chords can I construct very",
    "start": "759540",
    "end": "765180"
  },
  {
    "text": "good yes 128 ^ 4 that's correct I put a Big O because some like some",
    "start": "765180",
    "end": "771660"
  },
  {
    "text": "like you can rearrange the ordering but more or less yeah that's correct and why",
    "start": "771660",
    "end": "776940"
  },
  {
    "text": "is this a problem well this is the problem because most of these chords are actually never seen especially after you",
    "start": "776940",
    "end": "783900"
  },
  {
    "text": "transposed a C major a minor in fact looking at the data set we can see that just the first 20 chords or 20",
    "start": "783900",
    "end": "790950"
  },
  {
    "text": "notes rather occupy almost 90% of the entire dataset so if we were to",
    "start": "790950",
    "end": "796830"
  },
  {
    "text": "represent all of these we would have a ton of symbols in our vocabulary which we had never seen before the way we deal",
    "start": "796830",
    "end": "804870"
  },
  {
    "start": "804000",
    "end": "1251000"
  },
  {
    "text": "with this problem is by serializing so that is instead of representing all four notes as an individual symbol we",
    "start": "804870",
    "end": "812340"
  },
  {
    "text": "represent each individual note as a symbol itself and we serialized in soprano alto tenor bass order and so",
    "start": "812340",
    "end": "819480"
  },
  {
    "text": "what you end up getting is a reduction from 128 to the 4th all possible chords into just 128 possible pitches now this",
    "start": "819480",
    "end": "828780"
  },
  {
    "text": "may seem a little unjustified but this is actually done all the time with sequence processing if you took like",
    "start": "828780",
    "end": "834300"
  },
  {
    "text": "take a look at traditional on language models you can represent them either at the character level or at the word level",
    "start": "834300",
    "end": "840410"
  },
  {
    "text": "similarly you can represent music either at the note level or at the chord level",
    "start": "840410",
    "end": "845840"
  },
  {
    "text": "after serializing the the data looks like this we have assembled a noting the",
    "start": "845840",
    "end": "852390"
  },
  {
    "text": "start of a piece and this is used to initialize our model we then have the four chords soprano alto tenor bass",
    "start": "852390",
    "end": "859400"
  },
  {
    "text": "followed by a delimiter indicating the end of this frame and time has advanced one in the future followed by another",
    "start": "859400",
    "end": "865590"
  },
  {
    "text": "soprano alto tenor bass we also have these funny-looking dot things which I came up with to denote the self firmata",
    "start": "865590",
    "end": "872100"
  },
  {
    "text": "so that we can encode when the end of a phrases in our input training data after",
    "start": "872100",
    "end": "880380"
  },
  {
    "text": "all of our pre-processing our final corpus looks like this there's only 108",
    "start": "880380",
    "end": "885390"
  },
  {
    "text": "symbols left so not a hundred all hundred 28 pitches are used in Bach's works and there's about I would say four",
    "start": "885390",
    "end": "892710"
  },
  {
    "text": "hundred thousand total where we split three hundred and eighty thousand or three hundred and eighty thousand into a",
    "start": "892710",
    "end": "898020"
  },
  {
    "text": "training set and forty thousand into a validation set we split between training and validation in order to prevent",
    "start": "898020",
    "end": "904200"
  },
  {
    "text": "overfitting we don't want to just memorize box Corral's rather we want to be able to produce very similar samples",
    "start": "904200",
    "end": "911400"
  },
  {
    "text": "which are not exact identical and that's it with that you have the training set",
    "start": "911400",
    "end": "918930"
  },
  {
    "text": "and it's encapsulated by the first three commands on that slide I showed earlier with Bach",
    "start": "918930",
    "end": "924670"
  },
  {
    "text": "make data set Bach bot extract vocabulary the next step is to train the",
    "start": "924670",
    "end": "931780"
  },
  {
    "text": "recurrent neural network to talk about recurrent neural networks let's break the word down recurrent neural network",
    "start": "931780",
    "end": "939010"
  },
  {
    "text": "I'm going to start with neuro neural neural just means that we have very basic building blocks called neurons",
    "start": "939010",
    "end": "944770"
  },
  {
    "text": "which look like this they take a d-dimensional input x1 XD these are",
    "start": "944770",
    "end": "950590"
  },
  {
    "text": "numbers like 0.9 0.2 and they're all added together with a linear combination",
    "start": "950590",
    "end": "957010"
  },
  {
    "text": "so what you end up getting is this activation Z which is just the sum of these inputs weighted by WS so if a",
    "start": "957010",
    "end": "964330"
  },
  {
    "text": "neuron really cares about say X 2 W 2 W 1 and the rest will be zeros and so this",
    "start": "964330",
    "end": "970450"
  },
  {
    "text": "lets the neuron preferentially select which of its inputs that cares more about and allows to specialize for certain parts of its input this",
    "start": "970450",
    "end": "977560"
  },
  {
    "text": "activation is passed through this X shaped thing called an on called an activation function commonly a sigmoid",
    "start": "977560",
    "end": "983320"
  },
  {
    "text": "but all it does is it introduces a non-linearity into the network and allows you to explore expressive on the",
    "start": "983320",
    "end": "989350"
  },
  {
    "text": "types of functions you can approximate and we have the output called Y you take",
    "start": "989350",
    "end": "996040"
  },
  {
    "text": "these neurons you stack them horizontally and you get what's called a lair so here I'm just showing four",
    "start": "996040",
    "end": "1002460"
  },
  {
    "text": "neurons in this layer three neurons in this layer two neurons on this top layer and I represented the network like this",
    "start": "1002460",
    "end": "1010190"
  },
  {
    "text": "here we take the input X so this bottom part we multiply by a matrix now because",
    "start": "1010190",
    "end": "1016110"
  },
  {
    "text": "we've replicated the neurons horizontally and what w's represents the weights we pass it through this sigmoid",
    "start": "1016110",
    "end": "1022290"
  },
  {
    "text": "activation function to get these first layer outputs this is recursively done through all the layers until you get to",
    "start": "1022290",
    "end": "1028230"
  },
  {
    "text": "the very top where we have the final outputs of the model the W's here the weights those are the parameters of the",
    "start": "1028230",
    "end": "1034439"
  },
  {
    "text": "network and these are the things that we need to learn in order to train the neural network great",
    "start": "1034440",
    "end": "1041579"
  },
  {
    "text": "we know that feed-forward neural networks now let's introduce the word recurrent recurrent just means that the",
    "start": "1041580",
    "end": "1048630"
  },
  {
    "text": "previous input or the previous hidden states are used in the next time step the prediction so what I'm showing here",
    "start": "1048630",
    "end": "1054600"
  },
  {
    "text": "is again if you just pay attention to this input area and this layer right here and this",
    "start": "1054600",
    "end": "1059620"
  },
  {
    "text": "output this part right here is the same thing as this thing right here however",
    "start": "1059620",
    "end": "1066430"
  },
  {
    "text": "we've added this funny little loop coming back with this is electrical engineering notation for a unit time",
    "start": "1066430",
    "end": "1072520"
  },
  {
    "text": "delay and what this is saying is take the hidden state from time T minus 1 and",
    "start": "1072520",
    "end": "1077590"
  },
  {
    "text": "also include it as input into the next into the prime T predictions in",
    "start": "1077590",
    "end": "1082660"
  },
  {
    "text": "equations it looks like this the current hidden state is equal to the",
    "start": "1082660",
    "end": "1087910"
  },
  {
    "text": "act or the previous inputs plus the free or an activation of the previous inputs",
    "start": "1087910",
    "end": "1093670"
  },
  {
    "text": "waited plus the the weighted activations of the previous hidden states and the",
    "start": "1093670",
    "end": "1100390"
  },
  {
    "text": "outputs is only a function of just the current hidden states we can take this",
    "start": "1100390",
    "end": "1107110"
  },
  {
    "text": "loop right here oh sorry before I go there um this is called a Elmen type recurrent neural",
    "start": "1107110",
    "end": "1113290"
  },
  {
    "text": "network this memory cell is very basic it's just doing the exact same thing a normal neural network would do it turns",
    "start": "1113290",
    "end": "1120370"
  },
  {
    "text": "out there's some problems with just using the basic architecture and so the architecture that the field has been",
    "start": "1120370",
    "end": "1125530"
  },
  {
    "text": "converging towards is known as long short-term memory it looks really complicated it's not you",
    "start": "1125530",
    "end": "1133570"
  },
  {
    "text": "take the inputs and the hidden states and you put them into three spots right here the inputs an input gate a forget",
    "start": "1133570",
    "end": "1139510"
  },
  {
    "text": "gate and output gate and the point of adding all this art complexity is to solve a problem known as the vanishing",
    "start": "1139510",
    "end": "1145090"
  },
  {
    "text": "gradient problem where this constant error carousel of the hidden state being fed back to itself over and over and",
    "start": "1145090",
    "end": "1150460"
  },
  {
    "text": "over results in signals converging toward zero or diverging to infinity this is fortunately this is usually",
    "start": "1150460",
    "end": "1157420"
  },
  {
    "text": "available as just a black box implementation in most software packages you just specify I want to use an LS TM",
    "start": "1157420",
    "end": "1163180"
  },
  {
    "text": "and all of this is abstracted away from you now here if you squint you can kind",
    "start": "1163180",
    "end": "1172150"
  },
  {
    "text": "of see that the memory cell that I've shown previously where we have the inputs the hidden States hidden facing",
    "start": "1172150",
    "end": "1178660"
  },
  {
    "text": "back to itself to generate an output I distract it away like this and I've stacked it up on top of each other so",
    "start": "1178660",
    "end": "1185050"
  },
  {
    "text": "rather than just having the outputs come out of this H right here I've actually made it the inputs to get another memory",
    "start": "1185050",
    "end": "1190720"
  },
  {
    "text": "cell this is where the word deep comes from deep networks are just networks that have a lot of layers and by stacking I",
    "start": "1190720",
    "end": "1198460"
  },
  {
    "text": "get to use the word deep inside of my deep LS TM model but I'll show you later",
    "start": "1198460",
    "end": "1204160"
  },
  {
    "text": "that I'm not just doing it for the buzzword depth actually matters as well see in results another operation that's",
    "start": "1204160",
    "end": "1210400"
  },
  {
    "text": "important for LS CMS is unrolling and what unrolling does is it takes this unit time delay and it just replicates",
    "start": "1210400",
    "end": "1217090"
  },
  {
    "text": "the LS TM units over time so rather than show in this delay like this I've taken it I've shown the the - once hidden unit",
    "start": "1217090",
    "end": "1223540"
  },
  {
    "text": "passing state into the the t hidden unit passing stages the T plus first hidden unit your input is a variable length and",
    "start": "1223540",
    "end": "1230710"
  },
  {
    "text": "to train the network what you do is you expand this graph you unroll the lsdm so the same length as your variable length",
    "start": "1230710",
    "end": "1237070"
  },
  {
    "text": "input and in order to get these predictions up at the top great we know",
    "start": "1237070",
    "end": "1244810"
  },
  {
    "text": "all we need to know about music and rnns let's move on to a Bach bot have Bach Bach works to Train Bach bot we apply",
    "start": "1244810",
    "end": "1253510"
  },
  {
    "start": "1251000",
    "end": "1300000"
  },
  {
    "text": "sequential prediction criteria now I've stolen this from Andre carpet thieves github but the principles are the same",
    "start": "1253510",
    "end": "1260070"
  },
  {
    "text": "suppose we're given the input characters hello and we want to model it using a recurrent neural network the training",
    "start": "1260070",
    "end": "1267160"
  },
  {
    "text": "criteria is given the current input character and the previous hidden state predicts the next character so notice",
    "start": "1267160",
    "end": "1274210"
  },
  {
    "text": "down here I have a CH and I'm trying to predict e I've e and I'm trying to predict L I've L and I'm trying to",
    "start": "1274210",
    "end": "1279760"
  },
  {
    "text": "predict L and I have Allen I'm trying to predict oh if we take this analogy to",
    "start": "1279760",
    "end": "1285220"
  },
  {
    "text": "music I have all of the notes I've seen up until this point in time and I'm trying to predict the next note I can",
    "start": "1285220",
    "end": "1291070"
  },
  {
    "text": "iterate this process forwards to generate compositions the criteria we",
    "start": "1291070",
    "end": "1296470"
  },
  {
    "text": "want to use is and so the output layer here is actually a probability",
    "start": "1296470",
    "end": "1301600"
  },
  {
    "start": "1300000",
    "end": "1618000"
  },
  {
    "text": "distribution sorry so take in the previous slide and now I put it on top of my unrolled Network so given the",
    "start": "1301600",
    "end": "1308800"
  },
  {
    "text": "initial hidden state which we just initialized all zeroes because we have a unique start symbol used to initialize",
    "start": "1308800",
    "end": "1313930"
  },
  {
    "text": "our pieces and the RNN dynamics so this is the probability distribution over the",
    "start": "1313930",
    "end": "1320650"
  },
  {
    "text": "next state given the current state so this YT is for that and it's a function of the",
    "start": "1320650",
    "end": "1327400"
  },
  {
    "text": "currents the current input XT as well as the previous hidden states from t minus 1 we need to choose the r and n",
    "start": "1327400",
    "end": "1335680"
  },
  {
    "text": "parameters so these weight matrices the weights of all the connections between all the neurons in order to maximize",
    "start": "1335680",
    "end": "1341410"
  },
  {
    "text": "this probability right here the probability of the real Bach chorale so down here we have all the notes of the",
    "start": "1341410",
    "end": "1348370"
  },
  {
    "text": "real Bach chorale and up here we have the next notes of this of those in an",
    "start": "1348370",
    "end": "1353559"
  },
  {
    "text": "ideal world if we just initialize it with some Bach chorale it'll just memorize and return the remainder and that will that will do great on this",
    "start": "1353559",
    "end": "1360280"
  },
  {
    "text": "prediction criteria but that's not exactly what we want but nevertheless",
    "start": "1360280",
    "end": "1366570"
  },
  {
    "text": "once we have this criteria the way that the model is actually trained is by using the chain rule from calculus where",
    "start": "1366570",
    "end": "1372490"
  },
  {
    "text": "we take partial derivatives up here we have an error signal so I know this is the real Bach note the real note that",
    "start": "1372490",
    "end": "1378820"
  },
  {
    "text": "Bach used and this is the thing my model is predicting ok they're a little bit different how do I change the parameters",
    "start": "1378820",
    "end": "1385360"
  },
  {
    "text": "this weight matrix between the hidden state the outputs this weight matrix between the previous in stay in the current hidden state and this weight",
    "start": "1385360",
    "end": "1391450"
  },
  {
    "text": "matrix between the hidden state the inputs how can I change those around how do I wiggle those to make this output up",
    "start": "1391450",
    "end": "1396910"
  },
  {
    "text": "here closer to what Bach actually had produced now this training criteria can be just formalized",
    "start": "1396910",
    "end": "1402040"
  },
  {
    "text": "used by taking gradients using calculus and iterating and then optimization known as stochastic gradient descents",
    "start": "1402040",
    "end": "1407890"
  },
  {
    "text": "and when applied to neural networks it's an algorithm called back propagation well back propagation through time if",
    "start": "1407890",
    "end": "1414880"
  },
  {
    "text": "you want to get nitty-gritty because we've unrolled the neural network over time but again this is also abstraction",
    "start": "1414880",
    "end": "1420429"
  },
  {
    "text": "that need not concern you because this is also usually provided for you as a black box inside of common frameworks",
    "start": "1420429",
    "end": "1426280"
  },
  {
    "text": "such as tensor flow and caris we now",
    "start": "1426280",
    "end": "1431350"
  },
  {
    "text": "have all we now have the Bach bot model but there's a couple parameters that we need to look at I haven't told you",
    "start": "1431350",
    "end": "1436720"
  },
  {
    "text": "exactly how deep Bach bot is nor have I told you how big these layers are before",
    "start": "1436720",
    "end": "1443710"
  },
  {
    "text": "we start when optimizing models this is this is a very important learning and it's probably obvious by now GPUs are",
    "start": "1443710",
    "end": "1450820"
  },
  {
    "text": "very important for rapid experimentation I did a quick benchmark and I found that",
    "start": "1450820",
    "end": "1456250"
  },
  {
    "text": "a GPU delivers an 8x perform speed up making my training time goes down from 256 minutes down to just 28",
    "start": "1456250",
    "end": "1464260"
  },
  {
    "text": "minutes so if you want to iterate quickly getting a GPU will save you April like will make you eight times",
    "start": "1464260",
    "end": "1470590"
  },
  {
    "text": "more productive did I just put the word",
    "start": "1470590",
    "end": "1476020"
  },
  {
    "text": "deep onto my neural network because it was a good buzz word it turns out no depth actually matters what I'm showing",
    "start": "1476020",
    "end": "1484120"
  },
  {
    "text": "you here are the training losses as well as the validation losses as I change the depth the training loss is how well is",
    "start": "1484120",
    "end": "1491560"
  },
  {
    "text": "my model doing on the training data set which I'm letting it see and letting it tune its parameters to do better on and the validation loss is how well is my",
    "start": "1491560",
    "end": "1499060"
  },
  {
    "text": "model doing on data that I didn't let it see so how well is it generalizing beyond just memorizing its inputs and",
    "start": "1499060",
    "end": "1506050"
  },
  {
    "text": "what we notice here is that with just one layer the validation error is quite high and as we increase layers - it gets",
    "start": "1506050",
    "end": "1512800"
  },
  {
    "text": "you down here three gets you this red curve which is as low as it goes and if you keep going for with four it goes",
    "start": "1512800",
    "end": "1518890"
  },
  {
    "text": "back up should this be surprising it shouldn't and the reason why it",
    "start": "1518890",
    "end": "1524980"
  },
  {
    "text": "shouldn't is because as you add more layers you're adding more expressive power notice that we're here with four",
    "start": "1524980",
    "end": "1530800"
  },
  {
    "text": "layers you're actually doing just as good as the red curve so you're doing great on the training set but because your model is now so expressive you're",
    "start": "1530800",
    "end": "1537070"
  },
  {
    "text": "memorizing the inputs and so you generalize more poorly so a similar",
    "start": "1537070",
    "end": "1543790"
  },
  {
    "text": "story can be told about the hidden state sighs so how wide those memory cells are how many units do we have in them as we",
    "start": "1543790",
    "end": "1550150"
  },
  {
    "text": "increase the hidden state layer it's hidden state size we get performance improvements in generalization from this",
    "start": "1550150",
    "end": "1555280"
  },
  {
    "text": "blue curve all the way down until we get to 256 hidden units this green curve",
    "start": "1555280",
    "end": "1560790"
  },
  {
    "text": "after that we see the same kind of behavior where the training set error goes lower and lower but because you're",
    "start": "1560790",
    "end": "1567610"
  },
  {
    "text": "memorizing the inputs because your model is now too powerful you're out your generalization error actually gets worse",
    "start": "1567610",
    "end": "1575370"
  },
  {
    "text": "finally LST em they're pretty complicated the reason why I introduced it is because it's actually so critical",
    "start": "1575670",
    "end": "1582340"
  },
  {
    "text": "for your performance the the basic element type recurrent neural network or just reuses the standard recurrent",
    "start": "1582340",
    "end": "1588130"
  },
  {
    "text": "neural network architecture for the memory cell is shown here in side of this green curve right here",
    "start": "1588130",
    "end": "1593440"
  },
  {
    "text": "which actually doesn't do to both too badly but by using a long short term memory you get this yellow curve which",
    "start": "1593440",
    "end": "1599830"
  },
  {
    "text": "is at the very bottom it's doing as best as out of all the architectures we looked at in terms of memory cells gated",
    "start": "1599830",
    "end": "1606220"
  },
  {
    "text": "recurrent units are ass more simpler or simpler generalization of LF CMS they haven't been used as much and so there's",
    "start": "1606220",
    "end": "1612100"
  },
  {
    "text": "less literature about them but on this task they also appear to be doing quite well cool after all of this",
    "start": "1612100",
    "end": "1621070"
  },
  {
    "start": "1618000",
    "end": "1798000"
  },
  {
    "text": "experimentation and all of this manual grid search we finally arrived at a final architecture where notes are first",
    "start": "1621070",
    "end": "1627970"
  },
  {
    "text": "embedded into real numbers a 32 dimensional real or vector rather and then we have a three layer stacked",
    "start": "1627970",
    "end": "1634840"
  },
  {
    "text": "long short term memory recurrent neural network which processes these notes sequences over time and we trained it",
    "start": "1634840",
    "end": "1642700"
  },
  {
    "text": "using standard gradient descent with a couple tricks we use this thing called",
    "start": "1642700",
    "end": "1647830"
  },
  {
    "text": "drop out and we drop out with a setting of 30% and what this means is in between",
    "start": "1647830",
    "end": "1653530"
  },
  {
    "text": "subsequent connections between layers randomly turns 30% of the neurons off",
    "start": "1653530",
    "end": "1658770"
  },
  {
    "text": "that seems a little bit counterintuitive why might you want to do that it turns",
    "start": "1658770",
    "end": "1664720"
  },
  {
    "text": "out by turning off neurons during training you actually force the neurons to learn more robust features that are",
    "start": "1664720",
    "end": "1670840"
  },
  {
    "text": "independent of each other if the neurons are not always reliably avail if those connections are not always reliably available then there are",
    "start": "1670840",
    "end": "1678310"
  },
  {
    "text": "always reliably available then neurons may learn that to combine these two features and to happen so you end up",
    "start": "1678310",
    "end": "1684010"
  },
  {
    "text": "getting correlated features where to newer ons are actually learning the exact same feature with dropout we're",
    "start": "1684010",
    "end": "1690370"
  },
  {
    "text": "able we will actually show in the next slide that generalization improves as we increase this number to a certain point",
    "start": "1690370",
    "end": "1696030"
  },
  {
    "text": "we also conduct something called Bachelor Malaysian it basically just takes your data and centers it back",
    "start": "1696030",
    "end": "1701860"
  },
  {
    "text": "around zero and rescales the variance so that you don't have to worry about floating-point number overflows or under",
    "start": "1701860",
    "end": "1706900"
  },
  {
    "text": "flows and we use 128 kind step truncated back propagation through time again",
    "start": "1706900",
    "end": "1713980"
  },
  {
    "text": "another thing that your optimizer will handle for you but at a high level what this is doing is rather than unrolling",
    "start": "1713980",
    "end": "1720280"
  },
  {
    "text": "the entire network which over the entire input sequence which could be tens of thousands of notes long got tens of thousands thousands of notes",
    "start": "1720280",
    "end": "1726410"
  },
  {
    "text": "long we only unroll it 128 and we truncate the air signals we basically say after 120 time steps whatever you do",
    "start": "1726410",
    "end": "1733130"
  },
  {
    "text": "over here is not going to affect the future too much here's my promise slide about",
    "start": "1733130",
    "end": "1740150"
  },
  {
    "text": "drop out counter-intuitively as we turn that as we start dropping out or turning",
    "start": "1740150",
    "end": "1746090"
  },
  {
    "text": "off random neurons or random neuron connections we actually generalize better we see that without drop out the",
    "start": "1746090",
    "end": "1752750"
  },
  {
    "text": "model actually starts to overfit dramatically you know it gets better at generalizing that it gets worse and worse and worse at generalizing because",
    "start": "1752750",
    "end": "1758930"
  },
  {
    "text": "it's got so many connections it can learn so much you turn to and drop out up to 0.3 you get this purple curve at",
    "start": "1758930",
    "end": "1765440"
  },
  {
    "text": "the bottom where you've turned just to the right amount so that the features the model of learning are robust they",
    "start": "1765440",
    "end": "1770660"
  },
  {
    "text": "can generalize independently of other features and if you turn it up too high then now you're dropping up so much",
    "start": "1770660",
    "end": "1776600"
  },
  {
    "text": "you're injecting more noise than regularizing your model you actually don't generalize that well and the story",
    "start": "1776600",
    "end": "1782960"
  },
  {
    "text": "on the training side is also consistent as we increase dropout you do strictly worse on training and that makes sense",
    "start": "1782960",
    "end": "1788810"
  },
  {
    "text": "too because this isn't generalization this is just how well can the model memorize its input data and if you turn",
    "start": "1788810",
    "end": "1794420"
  },
  {
    "text": "inputs off you will memorize this good",
    "start": "1794420",
    "end": "1798760"
  },
  {
    "start": "1798000",
    "end": "1862000"
  },
  {
    "text": "great with the Train model we can do many things we can compose and we can",
    "start": "1799900",
    "end": "1806420"
  },
  {
    "text": "harmonize and the way we compose is the following we have the hidden states and",
    "start": "1806420",
    "end": "1813560"
  },
  {
    "text": "we have the inputs and we have the model weights and so we can use the model weights to form this predictive distribution what is the probability of",
    "start": "1813560",
    "end": "1820490"
  },
  {
    "text": "my current note given all of the previous notes I've seen before from",
    "start": "1820490",
    "end": "1825950"
  },
  {
    "text": "this probability distribution we just written we pick out a note according to how that distribution is parameterised",
    "start": "1825950",
    "end": "1831590"
  },
  {
    "text": "so up here this could be like I think L has the highest weight here and then so",
    "start": "1831590",
    "end": "1839870"
  },
  {
    "text": "after we sample it we just set XT equal to whatever we sampled out of there and we just treat it as truth we just assume",
    "start": "1839870",
    "end": "1845510"
  },
  {
    "text": "that whatever the output was right there is now the input for the next time step and then we iterate this process for",
    "start": "1845510",
    "end": "1850880"
  },
  {
    "text": "words so starting with no notes at all you sample the start symbol and then you just keep going until you sample the end",
    "start": "1850880",
    "end": "1856970"
  },
  {
    "text": "symbol and then that way we're able to generate novel automatic compositions harmonization is",
    "start": "1856970",
    "end": "1864529"
  },
  {
    "start": "1862000",
    "end": "2107000"
  },
  {
    "text": "actually a generalization of composition in composition what we basically did was I got a start symbol fill in the rest",
    "start": "1864529",
    "end": "1872679"
  },
  {
    "text": "harmonization is where you say I've got the melody I've got the baseline or I've",
    "start": "1872679",
    "end": "1878090"
  },
  {
    "text": "got these certain notes fill in the parts that I didn't specify and for this we actually proposed a suboptimal",
    "start": "1878090",
    "end": "1883700"
  },
  {
    "text": "strategy so I'm going to let alpha denote the stuff that we're given so it alpha could be like 1 3 7 the points in",
    "start": "1883700",
    "end": "1890899"
  },
  {
    "text": "time where the notes are fixed and the privatization problem is we need to",
    "start": "1890899",
    "end": "1896330"
  },
  {
    "text": "choose the notes that aren't fixed or we subdues the input the sequence X 1 to X",
    "start": "1896330",
    "end": "1901610"
  },
  {
    "text": "also we need to choose the entire composition such that the notes that",
    "start": "1901610",
    "end": "1907399"
  },
  {
    "text": "we're given X alpha are already fixed and so our decision variables are the things that are not in alpha and we need",
    "start": "1907399",
    "end": "1914090"
  },
  {
    "text": "to maximize this probability distribution my kind of greedy solution",
    "start": "1914090",
    "end": "1919370"
  },
  {
    "text": "which I've received a lot of criticism for is okay you're at this point in time just sample the the most likely thing at",
    "start": "1919370",
    "end": "1926570"
  },
  {
    "text": "the next point in time the reason why this gets criticized is because if you",
    "start": "1926570",
    "end": "1931789"
  },
  {
    "text": "greedily choose without looking at what influence this decision now could impact on your future you might choose",
    "start": "1931789",
    "end": "1937100"
  },
  {
    "text": "something that just doesn't make any sense in the future harmonic context but may sound really good right now it's",
    "start": "1937100",
    "end": "1943190"
  },
  {
    "text": "kind of like thinking it's kind of like acting without thinking about the consequences of your action but the",
    "start": "1943190",
    "end": "1950570"
  },
  {
    "text": "testament to how well this actually performs is not what could it how bad could it be theoretically it's actually",
    "start": "1950570",
    "end": "1956120"
  },
  {
    "text": "how well does it do empirically is this still convincing and we'll find out soon",
    "start": "1956120",
    "end": "1962289"
  },
  {
    "text": "but before we go there let's uncover the black box I've been talking about neural networks is just this thing which you",
    "start": "1963370",
    "end": "1969470"
  },
  {
    "text": "can just optimize throw data at it it'll learn things let's take a look inside and see what's actually going on and so",
    "start": "1969470",
    "end": "1976159"
  },
  {
    "text": "what I've done here is I've taken the various memory cells of my recurrent neural network and I've unrolled it over",
    "start": "1976159",
    "end": "1982970"
  },
  {
    "text": "time so on the x axis you see time and on the y axis I'm showing you the activations of all of the hidden units",
    "start": "1982970",
    "end": "1989870"
  },
  {
    "text": "so this is like neuron number one tuner on number 32 this is neuron number one - neuron number 256 in the",
    "start": "1989870",
    "end": "1997220"
  },
  {
    "text": "first hidden layer and similarly this is neuron number one - neuron number 256 in the second hidden layer these any",
    "start": "1997220",
    "end": "2004780"
  },
  {
    "text": "pattern there I don't I mean I kind of do I see like there's like this little",
    "start": "2004780",
    "end": "2010810"
  },
  {
    "text": "smear right here and it seems to show up everywhere as well as right here but there's not too much intuitive sense",
    "start": "2010810",
    "end": "2016990"
  },
  {
    "text": "that I can make out of this image and this is a common criticism of deep neural networks they're like black boxes",
    "start": "2016990",
    "end": "2022510"
  },
  {
    "text": "where we don't know how they really work on the inside but they seem to do awfully good as we get closer to the",
    "start": "2022510",
    "end": "2029740"
  },
  {
    "text": "output things start to make a little bit more sense so over so I previously was showing the hidden units of the first",
    "start": "2029740",
    "end": "2035440"
  },
  {
    "text": "and second layer now I'm showing the third layer as well as a linear combination of the third layer and",
    "start": "2035440",
    "end": "2040510"
  },
  {
    "text": "finally the outputs of the model and as you get towards the end you start seeing oh there's this little dotty pattern",
    "start": "2040510",
    "end": "2046090"
  },
  {
    "text": "this almost looks like a piano roll if you remember the representation of music",
    "start": "2046090",
    "end": "2051550"
  },
  {
    "text": "I showed earlier where we had time on the x-axis and pitch on the y-axis this",
    "start": "2051550",
    "end": "2056919"
  },
  {
    "text": "looks awfully similar to that and this isn't surprising either recall we",
    "start": "2056920",
    "end": "2062740"
  },
  {
    "text": "trained the neural network to predict the next note given the current note or all the previous notes if the network",
    "start": "2062740",
    "end": "2069220"
  },
  {
    "text": "was doing perfectly we would expect to just see the input here delayed by a single time step and so it's",
    "start": "2069220",
    "end": "2075970"
  },
  {
    "text": "unsurprising that we do see something that resembles the input but it's not quite exactly the input sometimes we see",
    "start": "2075970",
    "end": "2081429"
  },
  {
    "text": "like multiple predictions at one point in time and this is really representing the uncertainty inside of our",
    "start": "2081430",
    "end": "2086470"
  },
  {
    "text": "predictions so if I represented the probability distribution we're not just saying the next note is then is this rather we're saying we're pretty sure",
    "start": "2086470",
    "end": "2093399"
  },
  {
    "text": "than that next note is this with this probability but it could also be this with this probability that probability I",
    "start": "2093400",
    "end": "2099240"
  },
  {
    "text": "called this the probabilistic piano roll I don't know if that's standard terminology here's one of my most",
    "start": "2099240",
    "end": "2109360"
  },
  {
    "start": "2107000",
    "end": "2508000"
  },
  {
    "text": "interesting insights that I found from this model it appears to actually be learning music theory concepts so what",
    "start": "2109360",
    "end": "2116170"
  },
  {
    "text": "I'm showing here is some input that I provided to the model and here I picked out some neurons and oh no these neurons",
    "start": "2116170",
    "end": "2122680"
  },
  {
    "text": "are randomly selected so I didn't just go and I fished for the ones that like that rather I just ran a random number generator got eight of them out",
    "start": "2122680",
    "end": "2128259"
  },
  {
    "text": "and then I handed them off to my music dearest collaborator and I was like hey is there anything there and here's the",
    "start": "2128259",
    "end": "2134289"
  },
  {
    "text": "end here's the notes he made for me he said that neuron 64 this one and layer one neuron 138 this one they",
    "start": "2134289",
    "end": "2141519"
  },
  {
    "text": "appear to be picking out perfect Cadence's with root position chords in the tonic key more music theory than I",
    "start": "2141519",
    "end": "2148390"
  },
  {
    "text": "can understand but if I look up here it's like that shape right there on the piano roll looks like that shape on the",
    "start": "2148390",
    "end": "2154119"
  },
  {
    "text": "piano roll looks like that shape on the piano roll interesting neuron layer one",
    "start": "2154119",
    "end": "2160630"
  },
  {
    "text": "or neuron 151 I believe that is this one a minor Cadence's ending phrases two and",
    "start": "2160630",
    "end": "2168910"
  },
  {
    "text": "four no that's this one sorry and and again I look up here okay yeah that kind",
    "start": "2168910",
    "end": "2174579"
  },
  {
    "text": "of chord right there looks kind of like that chord right there they seem to be specializing to picking out specific",
    "start": "2174579",
    "end": "2180190"
  },
  {
    "text": "types of chords okay so it's learning Roman numeral analysis and tonics and root position chords and Cadence's and",
    "start": "2180190",
    "end": "2187799"
  },
  {
    "text": "the last one where one neuron eighty seven and layer two neuron 37 I believe",
    "start": "2187799",
    "end": "2192910"
  },
  {
    "text": "that's this one in this one they're picking out I six chords I have no idea",
    "start": "2192910",
    "end": "2198730"
  },
  {
    "text": "what that means so I showed you automatic composition at",
    "start": "2198730",
    "end": "2206710"
  },
  {
    "text": "the beginning of the presentation when I took some Bach Bach music and I allegedly claimed it was Bach I'll now",
    "start": "2206710",
    "end": "2211809"
  },
  {
    "text": "show you what harmonization sounds like and this is with the sub optimal strategy that I proposed so we take a",
    "start": "2211809",
    "end": "2217989"
  },
  {
    "text": "melody such as [Music] we tell the model this has to be the",
    "start": "2217989",
    "end": "2225609"
  },
  {
    "text": "soprano line what are the others likely to be like that's kind of convincing",
    "start": "2225609",
    "end": "2235660"
  },
  {
    "text": "it's almost like a baroque C major chord progression what's really interesting",
    "start": "2235660",
    "end": "2240700"
  },
  {
    "text": "though is that not only can we just harmonize simple melodies like that we can actually take popular tunes such as",
    "start": "2240700",
    "end": "2246280"
  },
  {
    "text": "this [Music]",
    "start": "2246280",
    "end": "2268199"
  },
  {
    "text": "we can generate a novel baroque harmonization of what Bach might have done had he heard twinkle twinkle little",
    "start": "2270939",
    "end": "2277609"
  },
  {
    "text": "star during his lifetime now I'm going off the track where it's",
    "start": "2277609",
    "end": "2283339"
  },
  {
    "text": "like oh this is my model it looks so good it sounds so realistic yeah but I",
    "start": "2283339",
    "end": "2288439"
  },
  {
    "text": "was just criticizing at the beginning of the talk my third research goal was actually how can we determine a standardized way to",
    "start": "2288439",
    "end": "2295309"
  },
  {
    "text": "quantitatively assess the performance of generative models for this particular task and one which I recommend for all",
    "start": "2295309",
    "end": "2301429"
  },
  {
    "text": "of automatic composition is to do a subjective listening experiment and so what we did is we built",
    "start": "2301429",
    "end": "2306799"
  },
  {
    "text": "vclav comm and it looks like this it's got a splash page and it's kind of",
    "start": "2306799",
    "end": "2312979"
  },
  {
    "text": "trying to go viral it's asking can you tell the difference between Bach and a computer they used to say man versus",
    "start": "2312979",
    "end": "2318289"
  },
  {
    "text": "machine but but the interface is simple you're given two choices one of them is",
    "start": "2318289",
    "end": "2325009"
  },
  {
    "text": "Bach one of them is Bach bot and you're asked to distinguish which one was the actual Bach we put this up out on the",
    "start": "2325009",
    "end": "2333379"
  },
  {
    "text": "Internet I've got around nineteen hundred participants from all around the world",
    "start": "2333379",
    "end": "2338949"
  },
  {
    "text": "participants tended to be within the eighteen to forty five age group the",
    "start": "2339189",
    "end": "2344509"
  },
  {
    "text": "district we got a surprisingly large number of expert users who decided to contribute we defined expert as a",
    "start": "2344509",
    "end": "2349879"
  },
  {
    "text": "researcher someone who is published or a teacher someone with professional accreditation as a music teacher",
    "start": "2349879",
    "end": "2355839"
  },
  {
    "text": "advanced as someone who has who have studied in a degree program for music and intermediate someone who plays an",
    "start": "2355839",
    "end": "2361009"
  },
  {
    "text": "instrument and here's how they did so I've coded these like I've coded these",
    "start": "2361009",
    "end": "2368659"
  },
  {
    "text": "with SAT B to represent the part that was asked to be harmonized so this is given the alto tenor bass harmonized",
    "start": "2368659",
    "end": "2375739"
  },
  {
    "text": "with soprano this year was given just the soprano wood bass harmonized the middle - and this is composed everything",
    "start": "2375739",
    "end": "2383359"
  },
  {
    "text": "I'm going to give you nothing this is the result that I've been coding this entire talk only participants are only",
    "start": "2383359",
    "end": "2391069"
  },
  {
    "text": "able to distinguish Bach from Bach bought 7% better than random chance but",
    "start": "2391069",
    "end": "2398029"
  },
  {
    "text": "there's some other interesting findings in here well I guess this isn't too surprising if you delete the soprano line then then",
    "start": "2398029",
    "end": "2404619"
  },
  {
    "text": "Bach bot is off to create a convincing melody and it doesn't do too well whereas if you delete the bass line",
    "start": "2404619",
    "end": "2410709"
  },
  {
    "text": "Bach lots of a lot better now I think this is actually a consequence of the",
    "start": "2410709",
    "end": "2416289"
  },
  {
    "text": "way I chose to deal with polyphony in the sense that I serialized the music from soprano alto tenor bass and so by",
    "start": "2416289",
    "end": "2422889"
  },
  {
    "text": "the time Bach Bach got to figuring out what the bass note might be it already seen the soprano alto and tenor note",
    "start": "2422889",
    "end": "2428949"
  },
  {
    "text": "within that time instant and so it already had a very strong harmonic context about what note might sound good",
    "start": "2428949",
    "end": "2434729"
  },
  {
    "text": "whereas if I whereas when I've got the soprano note Bach watt has no idea what",
    "start": "2434729",
    "end": "2440109"
  },
  {
    "text": "the alto tenor bass note might be and so just going to make a random guess that could be totally out of place to",
    "start": "2440109",
    "end": "2445989"
  },
  {
    "text": "validate this hypothesis which is a work left for the future you could serialize in a different order such as bass tenor",
    "start": "2445989",
    "end": "2452649"
  },
  {
    "text": "Alto soprano you could run this experiment again and you can see and you would expect to see it go down like this",
    "start": "2452649",
    "end": "2458589"
  },
  {
    "text": "if the hypothesis is true and differently if not here I've taken the",
    "start": "2458589",
    "end": "2466569"
  },
  {
    "text": "exact same plot from the previous plot except I've now broken it down by music experience unsurprisingly",
    "start": "2466569",
    "end": "2472659"
  },
  {
    "text": "you kind of see this curve where people are doing or doing better as they get more experienced so the novices are like",
    "start": "2472659",
    "end": "2479289"
  },
  {
    "text": "almost only three percent better where the experts are sixteen percent better they probably know Bach they've got it",
    "start": "2479289",
    "end": "2484569"
  },
  {
    "text": "memorized so they can tell the difference but the interesting one is here the experts do significantly worse",
    "start": "2484569",
    "end": "2492369"
  },
  {
    "text": "than random chance when getting when comparing Bach versus Bach bought bass harmonizations I actually don't have a",
    "start": "2492369",
    "end": "2499359"
  },
  {
    "text": "good reason why but it's surprising to me it seems that the experts think block",
    "start": "2499359",
    "end": "2505239"
  },
  {
    "text": "bot is more convincing than actual Bach so in conclusion I've presented a deep",
    "start": "2505239",
    "end": "2512679"
  },
  {
    "start": "2508000",
    "end": "2601000"
  },
  {
    "text": "long short term memory generative model for composing completing and generating",
    "start": "2512679",
    "end": "2517749"
  },
  {
    "text": "polyphonic music and this model isn't just like research that I'm talking about that no one ever gets to use it's",
    "start": "2517749",
    "end": "2524019"
  },
  {
    "text": "actually open source it's on my github and moreover Google's Google brains magenta project has actually integrated",
    "start": "2524019",
    "end": "2531009"
  },
  {
    "text": "it already into Google magenta so if you use the polyphonic recurrent neural network model at magenta and the tensor flow",
    "start": "2531009",
    "end": "2537460"
  },
  {
    "text": "projects you'll be using the bok-bok model the model appears to learn music",
    "start": "2537460",
    "end": "2543100"
  },
  {
    "text": "theory without any prior knowledge we didn't tell it this is a chord this is the cadence this is a tonic it just",
    "start": "2543100",
    "end": "2548320"
  },
  {
    "text": "decided to figure that out on its own in order to optimize performance on an automatic composition task to me this",
    "start": "2548320",
    "end": "2554680"
  },
  {
    "text": "suggests that music theory with all of its rules and all of its formalisms actually is useful for for comp",
    "start": "2554680",
    "end": "2561130"
  },
  {
    "text": "composing in fact it's so useful that a machine trained to optimize compose composition decided to specialize on",
    "start": "2561130",
    "end": "2567400"
  },
  {
    "text": "these concepts finally we conducted the largest musical Turing test to date with",
    "start": "2567400",
    "end": "2573960"
  },
  {
    "text": "1,700 participants only 7% of which performed better than random chance",
    "start": "2573960",
    "end": "2580590"
  },
  {
    "text": "obligatory note to my employer we do open slitter we do freelance outsourcing",
    "start": "2581310",
    "end": "2587140"
  },
  {
    "text": "if you need a development team let me know other than that thank you so much for your attention it was a pleasure",
    "start": "2587140",
    "end": "2592390"
  },
  {
    "text": "speaking to you all [Applause]",
    "start": "2592390",
    "end": "2601679"
  }
]