[
  {
    "start": "0",
    "end": "150000"
  },
  {
    "text": "[Music]",
    "start": "2550",
    "end": "11640"
  },
  {
    "text": "thank you the schedule said when you passed never mind does that mean I get",
    "start": "12799",
    "end": "18000"
  },
  {
    "text": "more time or less time uh right let's put this",
    "start": "18000",
    "end": "23960"
  },
  {
    "text": "away okay plug in make sure I'm on the right slide",
    "start": "24039",
    "end": "30720"
  },
  {
    "text": "how you doing today you good I brought the weather with me I normally bring rain everywhere I go it",
    "start": "30720",
    "end": "36360"
  },
  {
    "text": "rains but last two days has been amazing so this is fantastic I'm a little bit worried at Spring we kind of Miss spring",
    "start": "36360",
    "end": "42960"
  },
  {
    "text": "we gone straight to Summer uh and I like spring I like spring a lot oh there we go right let's make sure",
    "start": "42960",
    "end": "51280"
  },
  {
    "text": "my clicker Works dump my phone make sure my phone doesn't go off nobody phones me",
    "start": "51280",
    "end": "58440"
  },
  {
    "text": "anymore I don't do phone calls anymore I keep telling people I don't do phone calls and they keep phing me but right",
    "start": "58440",
    "end": "64239"
  },
  {
    "text": "okay hello everyone how you doing contain the schedule in or practical guide uh so I'm going to be talking",
    "start": "64239",
    "end": "69640"
  },
  {
    "text": "mostly about kubes uh I'm not going to be talking about do SW and things like that I think I should do actually",
    "start": "69640",
    "end": "77280"
  },
  {
    "text": "I'm kind of worried about what the timing is now because I have no idea when we're going to finish uh so let me",
    "start": "111880",
    "end": "117000"
  },
  {
    "text": "run through this very quickly uh this is me uh you can get me on Tech girl on Twitter uh and how many of you using",
    "start": "117000",
    "end": "124079"
  },
  {
    "text": "Google Plus still everyone last when I say that now I us to use it it's still really good",
    "start": "124079",
    "end": "130440"
  },
  {
    "text": "it's got more than 140 characters which is pretty cool so so uh you can also find my",
    "start": "130440",
    "end": "137640"
  },
  {
    "text": "Twitter handle on every single slide right at the bottom there and along with the goto stockh home and support the event the event is fantastic it's really",
    "start": "137640",
    "end": "144040"
  },
  {
    "text": "amazing that they're doing these events everywhere in Europe uh and let's talk about kubat get straight into I used to",
    "start": "144040",
    "end": "150519"
  },
  {
    "start": "150000",
    "end": "305000"
  },
  {
    "text": "talk about uh Borg our cluster schedu the thing that we have in Google uh and compare Bor and kubernetes together I'm",
    "start": "150519",
    "end": "158000"
  },
  {
    "text": "not going to do that today uh there are talks of me doing that online and there's also a guy called John wils he",
    "start": "158000",
    "end": "163400"
  },
  {
    "text": "talks about Bor uh he's one of the guys who builds our infrastructure uh and Bor is the thing that ultimately uh takes",
    "start": "163400",
    "end": "170879"
  },
  {
    "text": "all of the work that our software Engineers do and runs it on our clusters our large data centers uh and kubes is",
    "start": "170879",
    "end": "178319"
  },
  {
    "text": "kind of an attempt to make that kind of power available to everybody uh not just",
    "start": "178319",
    "end": "184599"
  },
  {
    "text": "a Google scale but a very small scale uh 10 machines five machines uh you don't need to have Google scale to do this uh",
    "start": "184599",
    "end": "191599"
  },
  {
    "text": "kuat ultimately is a strange word how many of you can say",
    "start": "191599",
    "end": "196640"
  },
  {
    "text": "kuat if you need lessons I'll be outside I will be teach you how to pronounce kubernetes",
    "start": "196640",
    "end": "202560"
  },
  {
    "text": "kubernetes it's a kind of Japanese word I'm using Japanese as well but it's really Greek it means homesman not going",
    "start": "202560",
    "end": "208920"
  },
  {
    "text": "to get into that uh but ultimately it goal is to run and manage containers and that's good because we ultimately",
    "start": "208920",
    "end": "215360"
  },
  {
    "text": "probably have a lot of containers by now uh because You' probably all discovered containers how many of you haven't discovered containers",
    "start": "215360",
    "end": "222680"
  },
  {
    "text": "yet yeah no put the hand up that's good see you end up with lots of containers and you need something ultimately to",
    "start": "223439",
    "end": "228640"
  },
  {
    "text": "manage them for you and that's what Kuban does and I mentioned bul earlier the thing that we have internally and",
    "start": "228640",
    "end": "234760"
  },
  {
    "text": "kubernetes is ultimately informed and inspired by Bor that thing that we have internally uh and all of the experience",
    "start": "234760",
    "end": "241239"
  },
  {
    "text": "that we have building that thing for the last 12 years or so uh it does support multiple clouds it runs on AWS just as",
    "start": "241239",
    "end": "248360"
  },
  {
    "text": "well as it does on Google Cloud platform uh so there's no lock in here there's nothing to do with Google this is a community project uh which is why I",
    "start": "248360",
    "end": "255280"
  },
  {
    "text": "don't have the Google logo anywhere maybe on the first slide I'm not sure uh",
    "start": "255280",
    "end": "260959"
  },
  {
    "text": "it supports bare metal environment as well so you can run this on Prem you can run it on AWS you can run it on Google Cloud you can run it on digital ocean",
    "start": "260959",
    "end": "267320"
  },
  {
    "text": "wherever you want to uh it also supports multiple container run times for some value of multiple uh we have a couple at",
    "start": "267320",
    "end": "274000"
  },
  {
    "text": "the moment we have rocket and Docker uh and now we have an open container format effectively we probably have more",
    "start": "274000",
    "end": "280120"
  },
  {
    "text": "container formats uh that we can run ultimately on uh kubernetes 100% open",
    "start": "280120",
    "end": "285560"
  },
  {
    "text": "source and it's written in go uh go is extremely popular for this type of uh",
    "start": "285560",
    "end": "290919"
  },
  {
    "text": "application and ultimately the goal is to manage applications and not have to worry about machines even if your",
    "start": "290919",
    "end": "296360"
  },
  {
    "text": "operations you shouldn't have to worry about machines too much uh most people nowadays I ask a question about whether your developers or operations people say",
    "start": "296360",
    "end": "303600"
  },
  {
    "text": "little bit both right so everything of Google runs in containers all the things that you use",
    "start": "303600",
    "end": "309960"
  },
  {
    "start": "305000",
    "end": "372000"
  },
  {
    "text": "all the services you interact with uh on Google running containers effectively the container was was a byproduct of",
    "start": "309960",
    "end": "316400"
  },
  {
    "text": "what we needed to do and ultimately we worked with Linux to create cgroups and Nam spaces uh which is the thing that",
    "start": "316400",
    "end": "323560"
  },
  {
    "text": "powers ultimately Docker and other container formats uh we run our map producers and our batch jobs in",
    "start": "323560",
    "end": "329080"
  },
  {
    "text": "containers and even our Google Cloud platform virtual machines effectively run in",
    "start": "329080",
    "end": "334199"
  },
  {
    "text": "containers so if you run a container on Google Cloud platform virtual machine you're run a container on a VM on a",
    "start": "334199",
    "end": "340960"
  },
  {
    "text": "container uh we launch over two billion containers per week but let's not think too much about what a container is let's",
    "start": "340960",
    "end": "347240"
  },
  {
    "text": "not worry about it too much it's a process uh it's a process that carries all of its dependencies around it and",
    "start": "347240",
    "end": "352360"
  },
  {
    "text": "has a tool chain and has an ecosystem But ultimately when you spin up a container you're spinning up a process",
    "start": "352360",
    "end": "357639"
  },
  {
    "text": "uh and most you can do more than one process but ultimately you care about one thing in a container uh so if you",
    "start": "357639",
    "end": "363840"
  },
  {
    "text": "think about two billion processes being uh two billion containers being two billion processes then it's not such a",
    "start": "363840",
    "end": "369199"
  },
  {
    "text": "big stretch to get to two uh so yeah so you discover this ultimately you've done all this work",
    "start": "369199",
    "end": "375440"
  },
  {
    "start": "372000",
    "end": "474000"
  },
  {
    "text": "this is replace all my container slides I used to have an explanation of what containers were uh 10 12 slides and I",
    "start": "375440",
    "end": "381479"
  },
  {
    "text": "replace it with this one slide which is containers are awesome so let's run lots of them and that's a natural conclusion",
    "start": "381479",
    "end": "387479"
  },
  {
    "text": "for most people to reach when it comes to uh developing applications today containers are good we want to run them",
    "start": "387479",
    "end": "393759"
  },
  {
    "text": "where do we run them how do we run them how do we manage them how do we do their updates how do we wire them together and",
    "start": "393759",
    "end": "398960"
  },
  {
    "text": "that's the kind of thing that kubat ultimately deals",
    "start": "398960",
    "end": "403280"
  },
  {
    "text": "with with cators you start with a cluster uh the cluster could be a laptop",
    "start": "405919",
    "end": "411479"
  },
  {
    "text": "so you could run using the vagrant provider uh you could spin cators up on your laptop uh using ultimately virtual",
    "start": "411479",
    "end": "417879"
  },
  {
    "text": "box as your virtual machine provider I think it supports VMware as well probably I've never tried that uh you",
    "start": "417879",
    "end": "424759"
  },
  {
    "text": "can also use multi Noe High availability clusters uh on Prem or in the cloud uh",
    "start": "424759",
    "end": "430800"
  },
  {
    "text": "they can be hosted or self-managed we have a managed version of kubernetes uh",
    "start": "430800",
    "end": "436120"
  },
  {
    "text": "but you could be managing yourself it could be on premise it could be on uh on the cloud you be running on bare metal or virtual machines we support pretty",
    "start": "436120",
    "end": "442879"
  },
  {
    "text": "much every operating system even uh",
    "start": "442879",
    "end": "448120"
  },
  {
    "text": "uh sorry I forgot what it's called exactly I've forgot what it's called so you lose your word right so we actually",
    "start": "448240",
    "end": "454680"
  },
  {
    "text": "implemented it on top of this rasby pie Custer uh and that's a five node cluster running kubernetes uh you probably don't",
    "start": "454680",
    "end": "460440"
  },
  {
    "text": "want to run that in production though uh and ultimately if you want to find out more about how to fit it to your needs",
    "start": "460440",
    "end": "467240"
  },
  {
    "text": "there's a cluster there's a matrix uh a cluster Matrix there at the bottom I link and you'll have the slides at the",
    "start": "467240",
    "end": "473639"
  },
  {
    "text": "end uh setting up a cluster is fairly straightforward uh I meant to update this we don't use this kind of bash",
    "start": "473639",
    "end": "479800"
  },
  {
    "start": "474000",
    "end": "527000"
  },
  {
    "text": "script uh pull down a bash script and run it immediately after pulling it down anymore nobody does that anymore uh so",
    "start": "479800",
    "end": "485840"
  },
  {
    "text": "that's a bit of a legacy so your pull down the tball and tiar it and then ultimately run the installer uh setting",
    "start": "485840",
    "end": "492680"
  },
  {
    "text": "your kubat provider to something like GCE or AWS or vagrant or coreos",
    "start": "492680",
    "end": "499159"
  },
  {
    "text": "depending on what you're running it on uh if we don't have an outof thebox way of installing it on your platform",
    "start": "499159",
    "end": "504240"
  },
  {
    "text": "there's plenty of recipes and that goes back to the Matrix on the other page uh we have well there are various dros red",
    "start": "504240",
    "end": "511639"
  },
  {
    "text": "red hat Atomic uh coros tectonic uh moranto uh Mor mortis Morano uh and",
    "start": "511639",
    "end": "519240"
  },
  {
    "text": "various others and yeah so there recipes and you can Al also use a hosted version",
    "start": "519240",
    "end": "524920"
  },
  {
    "text": "such as Google container engine and this is what kubat looks like ultimately we have a bunch of machines nodes uh we us",
    "start": "524920",
    "end": "532720"
  },
  {
    "start": "527000",
    "end": "664000"
  },
  {
    "text": "call them minions uh again Point things slaves and minions is a little bit not",
    "start": "532720",
    "end": "538000"
  },
  {
    "text": "so nice so now they called nodes uh and each node in the cluster runs this thing",
    "start": "538000",
    "end": "544240"
  },
  {
    "text": "called A kuet and A kuet has the credentials it needs to be able to connect to the cluster and become a member of the cluster uh and so when",
    "start": "544240",
    "end": "551040"
  },
  {
    "text": "these machines come up they find the master uh we have a single Master it",
    "start": "551040",
    "end": "556120"
  },
  {
    "text": "they connect to the master of ID the API uh and they become part of the cluster",
    "start": "556120",
    "end": "563000"
  },
  {
    "text": "and the schul uh which is also part of the master uh the slide probably needs a",
    "start": "563000",
    "end": "568760"
  },
  {
    "text": "little bit of update uh and that's wrong I'm not going to change that uh we use",
    "start": "568760",
    "end": "574680"
  },
  {
    "text": "XD for configuration management uh but it's raft based not pack based uh so uh",
    "start": "574680",
    "end": "579880"
  },
  {
    "text": "that's a mistake on the slides U that's kind of what bul does uh then we also have dashboards and we have controllers",
    "start": "579880",
    "end": "586440"
  },
  {
    "text": "we also have a container registry as well so because we care about running containers uh things like Docker or",
    "start": "586440",
    "end": "592000"
  },
  {
    "text": "rocket then we care about where those images are located and this container registry could be hub. doco.com it could",
    "start": "592000",
    "end": "599000"
  },
  {
    "text": "be uh some registry you created for yourself because it's fairly easy to set them up or it could be also on Google",
    "start": "599000",
    "end": "604720"
  },
  {
    "text": "Cloud platform if you're using Google container engine and ultimately what happens is we",
    "start": "604720",
    "end": "610440"
  },
  {
    "text": "push an image out to The Container registry uh we provide a configuration push that out to the KX Master uh the",
    "start": "610440",
    "end": "616920"
  },
  {
    "text": "Schuler comes along and looks at the state of the system uh what it is currently uh looks at the state of what",
    "start": "616920",
    "end": "622320"
  },
  {
    "text": "it should be as according to our positionist storage and it says Ah okay",
    "start": "622320",
    "end": "627680"
  },
  {
    "text": "there different let me fix that and so it will make the actual State the same as the desired State uh so we have this",
    "start": "627680",
    "end": "634040"
  },
  {
    "text": "notion of desired State and then the scheduler whoops the line's gone R back",
    "start": "634040",
    "end": "639839"
  },
  {
    "text": "the scheduler will uh decide where that work will run those resources will run",
    "start": "639839",
    "end": "646160"
  },
  {
    "text": "and it will talk to one of the machines uh and the Machine will pull over the binary or the image and then run that",
    "start": "646160",
    "end": "652320"
  },
  {
    "text": "container uh on that machine and the schu's work is what we're going to be talking about today uh and somehow that",
    "start": "652320",
    "end": "658600"
  },
  {
    "text": "Line's got uh stuck at the back there so that's basically how kubes Works uh but",
    "start": "658600",
    "end": "664240"
  },
  {
    "start": "664000",
    "end": "803000"
  },
  {
    "text": "let's go into some of the resources that kubes provides uh so the first is the uh the",
    "start": "664240",
    "end": "672399"
  },
  {
    "text": "the atom of scheduling we don't use containers directly uh to schedule work on kubernetes we use a thing called a",
    "start": "672399",
    "end": "678480"
  },
  {
    "text": "pod and a pod can ultimately run multiple containers uh it's a bit of a",
    "start": "678480",
    "end": "684480"
  },
  {
    "text": "convenience uh but if you think of it like a logical host uh if you create a machine and you run multiple processor",
    "start": "684480",
    "end": "690959"
  },
  {
    "text": "processes on that machine because they need to work together they have some kind of synergy then it makes sense to",
    "start": "690959",
    "end": "696560"
  },
  {
    "text": "have them on in a pod so effectively what you're going to do is take that machine which could be virtual or",
    "start": "696560",
    "end": "702040"
  },
  {
    "text": "logical and you're going to take it into a format and convert it into a pod that can run on a cluster and just like a",
    "start": "702040",
    "end": "709120"
  },
  {
    "text": "machine it also has its own IP address uh so we can be running multiple of these pods on a node but each of them",
    "start": "709120",
    "end": "715320"
  },
  {
    "text": "will have its own IP address which is not nated so we can talk to it directly where IP and all of the containers run",
    "start": "715320",
    "end": "721600"
  },
  {
    "text": "inside the Pod have a shared name space uh so the idea of the port name space is the same for multiple containers running",
    "start": "721600",
    "end": "728399"
  },
  {
    "text": "inside the Pod and they can talk to talk to each other via ports or they can talk to each other via ipcs which they have a",
    "start": "728399",
    "end": "734920"
  },
  {
    "text": "shared IP uh namespace IPC namespace as well so uh containers running in a pod",
    "start": "734920",
    "end": "742000"
  },
  {
    "text": "look like and think that they are running on a single machine uh so effectively we're creating these machines and we're scheduling them on",
    "start": "742000",
    "end": "748199"
  },
  {
    "text": "our cluster uh that's really cool and really powerful and we also have volumes the notion of",
    "start": "748199",
    "end": "755199"
  },
  {
    "text": "volumes this example shows two containers one of them is a very simple node.js uh application serving traffic",
    "start": "755199",
    "end": "762399"
  },
  {
    "text": "uh externally uh there's other mechanisms we can use to expose that externally which we'll see",
    "start": "762399",
    "end": "767519"
  },
  {
    "text": "shortly and we also have a git synchronizer and its job in the world is to synchronize with a git repo uh so",
    "start": "767519",
    "end": "774079"
  },
  {
    "text": "whenever you push changes to your git repo the git synchronizer or see the changes pull them down store them on the volume",
    "start": "774079",
    "end": "779959"
  },
  {
    "text": "and we can now serve that uh those updates to users so it's a kind of push",
    "start": "779959",
    "end": "785639"
  },
  {
    "text": "to deployer mechanism but again they're just two examples of containers uh this is what we call a sidecar pattern uh",
    "start": "785639",
    "end": "792440"
  },
  {
    "text": "where one where there's a Synergy between the containers there are also other patterns as well often you may",
    "start": "792440",
    "end": "797920"
  },
  {
    "text": "only run one container in a pod which is perfectly fine there's no overhead uh and we said that pods can",
    "start": "797920",
    "end": "806399"
  },
  {
    "start": "803000",
    "end": "853000"
  },
  {
    "text": "talk to each other by IP addresses directly uh that's the way it works we have uh an underlying overlay network uh",
    "start": "806399",
    "end": "813440"
  },
  {
    "text": "for Google Cloud platform that's our Advanced routing uh for AWS is Route tables we can use Calico or flannel or",
    "start": "813440",
    "end": "819680"
  },
  {
    "text": "weave or open V switch or cloud provider or open contrail and others as also one from Cisco Cisco contrive there's",
    "start": "819680",
    "end": "826040"
  },
  {
    "text": "probably some I've missed and if anybody's representing those companies I do apologize uh but yeah so it's fairly",
    "start": "826040",
    "end": "831360"
  },
  {
    "text": "easy to set up uh but again if we have multiple pods on a node and they're all",
    "start": "831360",
    "end": "836839"
  },
  {
    "text": "exposed in the same port that is fine they had their own port namespace effectively uh so if you have multiple",
    "start": "836839",
    "end": "843320"
  },
  {
    "text": "pods exposing port 8080 on the same node uh that's fine that will work uh these",
    "start": "843320",
    "end": "849279"
  },
  {
    "text": "are fundamental requirements we had to have this for kubes to work uh we also have the notion of",
    "start": "849279",
    "end": "855440"
  },
  {
    "start": "853000",
    "end": "916000"
  },
  {
    "text": "labels uh labels is the single grouping mechanism uh cators uh we can label pretty much everything uh in cators and",
    "start": "855440",
    "end": "862839"
  },
  {
    "text": "then we can build tools to read those labels a label is a key value pair uh it",
    "start": "862839",
    "end": "868120"
  },
  {
    "text": "doesn't have any meaning cators uh but it probably has semantic meaning to you",
    "start": "868120",
    "end": "874160"
  },
  {
    "text": "like the role equals something the type equals something the color equals something something that has semantic",
    "start": "874160",
    "end": "880720"
  },
  {
    "text": "meaning to you that will allow you to group these artifacts these resources within kubernetes uh here we see them",
    "start": "880720",
    "end": "888199"
  },
  {
    "text": "applied to pods we have pods with multiple labels uh resources can have multiple labels and we're building",
    "start": "888199",
    "end": "893959"
  },
  {
    "text": "dashboards and tools that will use those labels and the way we use labels is via",
    "start": "893959",
    "end": "899160"
  },
  {
    "text": "called a selector and so basically we just say to the API give me everything or all of the pods with this label uh",
    "start": "899160",
    "end": "905079"
  },
  {
    "text": "and we provide we this label base on a selector and we have services within",
    "start": "905079",
    "end": "910920"
  },
  {
    "text": "kubat that also use that mechanism to group pods and to group other things as",
    "start": "910920",
    "end": "916040"
  },
  {
    "start": "916000",
    "end": "997000"
  },
  {
    "text": "well uh the service is the way we get access to these pods that are running containers uh they expose a port",
    "start": "916040",
    "end": "922759"
  },
  {
    "text": "ultimately uh there's no nattin as we mentioned uh but ultimately we have three identical pods or running in the",
    "start": "922759",
    "end": "929240"
  },
  {
    "text": "same nodejs application engine X application uh but all identical uh then",
    "start": "929240",
    "end": "935759"
  },
  {
    "text": "we need to somehow Route traffic across them we need to communicate with it anyway even if there's only one pod and to do that we put a service in front of",
    "start": "935759",
    "end": "942360"
  },
  {
    "text": "it so the service construct effectively uh groups uh a number of pods from one",
    "start": "942360",
    "end": "948920"
  },
  {
    "text": "to many uh and the service provides a a stable virtual IP address and also a DNS",
    "start": "948920",
    "end": "954639"
  },
  {
    "text": "entry so clients can access that those running pods via that virtual endpoint",
    "start": "954639",
    "end": "961480"
  },
  {
    "text": "that we providing there and traffic coming in from the clients will be low balance across multiple pods uh and it's",
    "start": "961480",
    "end": "969079"
  },
  {
    "text": "done randomly currently used to be round Robbin so that's the service this can be used to discover Services internally uh",
    "start": "969079",
    "end": "976120"
  },
  {
    "text": "so we could be wiring things together internally or it also could ultimately expose an endpoint externally as well uh",
    "start": "976120",
    "end": "982240"
  },
  {
    "text": "so we have the notion of a low Balan endpoint and if we have a cloud provider uh such as Google Cloud platform or AWS",
    "start": "982240",
    "end": "988720"
  },
  {
    "text": "we we can actually expose the service on an external endpoint to the rest of the world uh so you have multiple different",
    "start": "988720",
    "end": "995040"
  },
  {
    "text": "types of services uh we also have the notion of replication controllers which which are",
    "start": "995040",
    "end": "1000880"
  },
  {
    "start": "997000",
    "end": "1117000"
  },
  {
    "text": "now been superseded pretty much by replica sets uh they are the same we're just trying to get away from well",
    "start": "1000880",
    "end": "1006680"
  },
  {
    "text": "they're mostly the same we're trying to get away from using controllers in the names of these things everything started",
    "start": "1006680",
    "end": "1012639"
  },
  {
    "text": "to be a controller and it kind of got confusing so now we've decided to confuse Everybody by changing their name",
    "start": "1012639",
    "end": "1019000"
  },
  {
    "text": "uh so now they're called replica sets uh but they are fundamentally the same as replica replication controllers so if",
    "start": "1019000",
    "end": "1025280"
  },
  {
    "text": "you're familiar with them uh then you won't see much change they do have a new generalized label semantics uh so rather",
    "start": "1025280",
    "end": "1032600"
  },
  {
    "text": "than just having uh equality for labels you can have inequality and you can also do sets uh so like a value in uh some",
    "start": "1032600",
    "end": "1041959"
  },
  {
    "text": "set of values uh so we can do labels a little bit differently with replica sets but fundamentally they're the same and",
    "start": "1041959",
    "end": "1048919"
  },
  {
    "text": "replica set ultimately is responsible for making sure we have our desired State uh for a given pod deployment uh",
    "start": "1048919",
    "end": "1057080"
  },
  {
    "text": "we provide a pod template uh which contains the containers uh ports labels and such like for creating pods we say",
    "start": "1057080",
    "end": "1065679"
  },
  {
    "text": "how many we want in this case we want two and well one in two in the left hand one and one and the right hand one and",
    "start": "1065679",
    "end": "1072280"
  },
  {
    "text": "we pass that in as a configuration to kubat and say make it so effectively so",
    "start": "1072280",
    "end": "1077760"
  },
  {
    "text": "we say we want number and we want this template the replication controller will wake up and look to see if that there",
    "start": "1077760",
    "end": "1084440"
  },
  {
    "text": "are any pods that match those labels that it has in this case it cares about label version equals V1 so it will look",
    "start": "1084440",
    "end": "1092159"
  },
  {
    "text": "to see if these exist if they don't it will make an API call and ask the API to create them for it and it will",
    "start": "1092159",
    "end": "1099559"
  },
  {
    "text": "continuously monitor to make sure there's always the right number running okay so if you've asked a two there'll always be two running uh and it's fairly",
    "start": "1099559",
    "end": "1106679"
  },
  {
    "text": "straightforward we can have multiple replica set many many different replica sets manag in different pods but they",
    "start": "1106679",
    "end": "1112120"
  },
  {
    "text": "may even be just different versions of your pod we got some scenarios coming up we'll show",
    "start": "1112120",
    "end": "1117400"
  },
  {
    "start": "1117000",
    "end": "1203000"
  },
  {
    "text": "you uh deployments are very new we have the ability to do a rolling update to those pods uh so once we change updated",
    "start": "1117400",
    "end": "1125840"
  },
  {
    "text": "uh our image our container image we may want to roll that out uh as an update a",
    "start": "1125840",
    "end": "1131880"
  },
  {
    "text": "rolling update to our service our production service and the way we used to do that was a thing called a rolling",
    "start": "1131880",
    "end": "1138000"
  },
  {
    "text": "update and it would basically create a new replication controller in those days",
    "start": "1138000",
    "end": "1143360"
  },
  {
    "text": "uh which would create one new pod take an old pod away create another new pod take an old pod away and eventually it",
    "start": "1143360",
    "end": "1150520"
  },
  {
    "text": "gets to a state where all of the pods have been replaced and it can remove the old replication controller but that was",
    "start": "1150520",
    "end": "1155840"
  },
  {
    "text": "all done on the client side so you could do control C at any point and you would be left in an unknown State uh you could",
    "start": "1155840",
    "end": "1162480"
  },
  {
    "text": "finish it off at some point later uh but it was fairly unpredictable and liable to problems so the deployment object is",
    "start": "1162480",
    "end": "1169440"
  },
  {
    "text": "something new that's introduced in 1.1 uh which is now in beta in 1.2 but it's also the default as we trust it so much",
    "start": "1169440",
    "end": "1177240"
  },
  {
    "text": "uh and this effectively does our rolling updates for us on the server side so we basically create the deployment I'll",
    "start": "1177240",
    "end": "1183039"
  },
  {
    "text": "demo shortly we create the deployment we push it out it will create the replica sets for us uh and if we want to update",
    "start": "1183039",
    "end": "1190360"
  },
  {
    "text": "we can just edit that configuration using the command line very very simple it's very powerful and then it will roll",
    "start": "1190360",
    "end": "1195960"
  },
  {
    "text": "out the updates to the pods and you don't have to worry about it anymore it's not happening on the client side so",
    "start": "1195960",
    "end": "1201480"
  },
  {
    "text": "that's deployments uh scaling is fairly straightforward we can just change the",
    "start": "1201480",
    "end": "1207039"
  },
  {
    "start": "1203000",
    "end": "1244000"
  },
  {
    "text": "number we can instruct the replication controller or the replication replica set to have more pods in this case we",
    "start": "1207039",
    "end": "1212880"
  },
  {
    "text": "say want you have two it will create another one or it will ask for another to be created if we say we want four it",
    "start": "1212880",
    "end": "1219039"
  },
  {
    "text": "will go ahead and instruct the API to create another two and you will then",
    "start": "1219039",
    "end": "1224200"
  },
  {
    "text": "continuously check to make sure there's always four so scaling is very very straightforward",
    "start": "1224200",
    "end": "1229240"
  },
  {
    "text": "and at the bottom here we have a service uh which is cares about labels with type",
    "start": "1229240",
    "end": "1234360"
  },
  {
    "text": "equals F Fe and all of the traffic coming in uh to this service will be routed across uh low balance across all",
    "start": "1234360",
    "end": "1240919"
  },
  {
    "text": "of those pods okay rolling updates are done like this",
    "start": "1240919",
    "end": "1247000"
  },
  {
    "start": "1244000",
    "end": "1314000"
  },
  {
    "text": "now so we have a this deployment object we have a replication controller uh again it's managed in two pods we have a",
    "start": "1247000",
    "end": "1253240"
  },
  {
    "text": "service down here which I haven't put the name in uh so these two pods here are being managed by this controller and",
    "start": "1253240",
    "end": "1258360"
  },
  {
    "text": "this deployment is managing this whole thing and when we update the deployment Maybe by changing the image uh what will",
    "start": "1258360",
    "end": "1264720"
  },
  {
    "text": "happen is that we will create a new replica Set uh and we will then create a or tell",
    "start": "1264720",
    "end": "1272279"
  },
  {
    "text": "it to scale to one scale the old one down and then scale the new one up to",
    "start": "1272279",
    "end": "1279400"
  },
  {
    "text": "the new number of PODS so ultimately we've done a rolling update now so we've updated all of our",
    "start": "1279400",
    "end": "1284640"
  },
  {
    "text": "pods to the new image uh and we're left with this thing this Fair replica set here this doesn't go away uh because",
    "start": "1284640",
    "end": "1291440"
  },
  {
    "text": "basically we create these replica sets based on a hash of the template that we use for creating pods uh so we hash that",
    "start": "1291440",
    "end": "1297799"
  },
  {
    "text": "template gives us a number and we add that to the name of the replica set in this case all we've done is scale it",
    "start": "1297799",
    "end": "1303360"
  },
  {
    "text": "down to zero and if we ever reuse that template uh the same template we can",
    "start": "1303360",
    "end": "1308640"
  },
  {
    "text": "reuse that replica Set uh but at the moment it just sits there with zero pods a canary is very simple uh so",
    "start": "1308640",
    "end": "1317919"
  },
  {
    "start": "1314000",
    "end": "1358000"
  },
  {
    "text": "sometimes you want to roll out an update to a partial update uh just to test it uh to do ab testing effectively in this",
    "start": "1317919",
    "end": "1325120"
  },
  {
    "text": "case we have two replication controllers uh we have one with version equals V1 pods and one with one version equals V2",
    "start": "1325120",
    "end": "1332000"
  },
  {
    "text": "pod and the service is aggregating all three of those and low balancing traffic across all three so 33% of our traffic",
    "start": "1332000",
    "end": "1339080"
  },
  {
    "text": "will go to this pod and 66% will go to these two pods and so we're effectively canaran we may have one pod out of 10 uh",
    "start": "1339080",
    "end": "1346600"
  },
  {
    "text": "to do an even smaller percentage But ultimately we can see the results come back for that new version if we like it",
    "start": "1346600",
    "end": "1352039"
  },
  {
    "text": "we can then roll out a deployment update to update it completely or we can roll it back so that's how we do",
    "start": "1352039",
    "end": "1359360"
  },
  {
    "text": "caneras and we also do auto scaling which I'm not going to really cover uh Auto scaling is based on uh performance",
    "start": "1359360",
    "end": "1365919"
  },
  {
    "text": "metrics or utilization uh and we can scale clusters based on that so that's very scale pods based on that that's",
    "start": "1365919",
    "end": "1372480"
  },
  {
    "text": "very easy uh jobs we also have the ability to run",
    "start": "1372480",
    "end": "1378520"
  },
  {
    "start": "1376000",
    "end": "1398000"
  },
  {
    "text": "short running jobs batch jobs effectively uh so these will run to completion uh and there's various rules that you can set for them so pods no",
    "start": "1378520",
    "end": "1386039"
  },
  {
    "text": "longer had to be long running uh there were ways to make them not long running before but now we have an official job",
    "start": "1386039",
    "end": "1392039"
  },
  {
    "text": "so you can effectively say I want to run this uh to completion you push it out the cumulates it will run and finish",
    "start": "1392039",
    "end": "1397559"
  },
  {
    "text": "that will be it and we also have livess and Readiness checks uh people ask about how we know whether the pods the running",
    "start": "1397559",
    "end": "1404919"
  },
  {
    "start": "1398000",
    "end": "1416000"
  },
  {
    "text": "pods are live where we have the ability to set up liveness and readiness uh both at the process level and also at",
    "start": "1404919",
    "end": "1411080"
  },
  {
    "text": "the app level but I'm not going to go into details with them but you'll see the slides later so you can get access to this and also the ability to",
    "start": "1411080",
    "end": "1418320"
  },
  {
    "start": "1416000",
    "end": "1452000"
  },
  {
    "text": "gracefully terminate pods if we're doing downscale events we're scaling downwards we need to be able to shut down the Pod",
    "start": "1418320",
    "end": "1423440"
  },
  {
    "text": "gracefully uh so we give it a 30C window of opportunity by sending it a signal uh so then the Pod can do its own cleanup",
    "start": "1423440",
    "end": "1430480"
  },
  {
    "text": "uh in the future we'll hopefully be able to do some kind of draining uh we'll be able to make lame ducts uh that can be",
    "start": "1430480",
    "end": "1436559"
  },
  {
    "text": "taken out of uh the actual pod management system from the replication controller's point of view and we can",
    "start": "1436559",
    "end": "1442960"
  },
  {
    "text": "take traffic stop traffic from going to it so it can finish processing its requests and once it's done we can remove it we don't have that currently",
    "start": "1442960",
    "end": "1449799"
  },
  {
    "text": "but we probably need it in the future and also we support multis Zone clusters now uh so if you have uh on",
    "start": "1449799",
    "end": "1457640"
  },
  {
    "start": "1452000",
    "end": "1487000"
  },
  {
    "text": "Google Cloud platform and on AWS you have no the notion of zones availability zones in AWS now you can have parts of",
    "start": "1457640",
    "end": "1464520"
  },
  {
    "text": "your cluster across multiple zones so if uh for example San Francisco or the Bay",
    "start": "1464520",
    "end": "1470000"
  },
  {
    "text": "Area was fall into the Pacific Ocean for some reason and you lose a Zone you'll be still be good because Europe would",
    "start": "1470000",
    "end": "1475200"
  },
  {
    "text": "still be up and running we don't want that to happen by the way so I'm going there next week or this week even so let's not have that happen uh so yeah so",
    "start": "1475200",
    "end": "1481279"
  },
  {
    "text": "that's a really good for availability and that's available in G now incat",
    "start": "1481279",
    "end": "1486760"
  },
  {
    "text": "1.2 and just going to run through scheduling very quickly I wanted to do my demo anybody know how long I've",
    "start": "1486760",
    "end": "1493760"
  },
  {
    "start": "1487000",
    "end": "1573000"
  },
  {
    "text": "got 10 minutes okay uh all right I'm just going to go through",
    "start": "1493760",
    "end": "1499679"
  },
  {
    "text": "the schuer okay so schedu in uh basically when we schedule things we",
    "start": "1499679",
    "end": "1505720"
  },
  {
    "text": "send some object to the API server and this scheduler will look for new objects",
    "start": "1505720",
    "end": "1510840"
  },
  {
    "text": "have been stalled in the configuration uh and it will schedule them it will make a decision ultimately uh it will",
    "start": "1510840",
    "end": "1516480"
  },
  {
    "text": "decide where it's going to go uh it's going to decide which node the arrow should have appeared afterwards but uh",
    "start": "1516480",
    "end": "1521520"
  },
  {
    "text": "it will then schedu on a node that matches its scheduling policies uh the scheduling policies can be controlled",
    "start": "1521520",
    "end": "1527159"
  },
  {
    "text": "and we'll go into them shortly uh but it has to make that decision on which node without auler uh nothing can",
    "start": "1527159",
    "end": "1533600"
  },
  {
    "text": "happen really uh so we can send something to cators and it won't know what to do with it because it has no",
    "start": "1533600",
    "end": "1538960"
  },
  {
    "text": "schedul but you could do it manually you could say I want to run it on this",
    "start": "1538960",
    "end": "1544120"
  },
  {
    "text": "specific node and you provide the node name uh when you send it say basically run this pod run this service run this",
    "start": "1544120",
    "end": "1550760"
  },
  {
    "text": "pod effectively on this node that's the old way of doing it and that's the mechanical way of doing it where you",
    "start": "1550760",
    "end": "1556000"
  },
  {
    "text": "have to identify a machine if you have a large dat sent it may be a machine in a rack a machine a single machine in a",
    "start": "1556000",
    "end": "1562799"
  },
  {
    "text": "rack in a cluster uh and decide which one you want to run it on and that's not",
    "start": "1562799",
    "end": "1567960"
  },
  {
    "text": "very good so that's why we have auler we have the schuer make all of those decisions for us uh kuat is understands resources uh",
    "start": "1567960",
    "end": "1576640"
  },
  {
    "text": "in terms of CPU and in terms of memory uh and we can request given levels of",
    "start": "1576640",
    "end": "1582320"
  },
  {
    "text": "CPU and memory for our pods uh so we can ask for uh two CPU CES or 2 and a half",
    "start": "1582320",
    "end": "1589559"
  },
  {
    "text": "CPU CES and uh 10 GB of RAM for a pod which we know that's what it's going to use uh and kubat can then basically",
    "start": "1589559",
    "end": "1598360"
  },
  {
    "text": "schedule based on the resource availability on those nodes because you have a certain amount of Cs available in",
    "start": "1598360",
    "end": "1603520"
  },
  {
    "text": "your cluster you have a certain amount of memory available in your cluster and cators can therefore make a decision as",
    "start": "1603520",
    "end": "1609000"
  },
  {
    "text": "to where this can go because it will fit on there it needs two CES where does it go where are where do I have two cores",
    "start": "1609000",
    "end": "1615080"
  },
  {
    "text": "available uh and also if you we can also set limits so if you go above your limit",
    "start": "1615080",
    "end": "1621360"
  },
  {
    "text": "uh we can then say I'm sorry you've run out you're you're using too much memory uh we can kick you off uh that's how we",
    "start": "1621360",
    "end": "1627919"
  },
  {
    "text": "handle memory because memory can't be compressed we cannot fott memory allocation we can fott CPU allocation we",
    "start": "1627919",
    "end": "1634880"
  },
  {
    "text": "can use CPU quoters in the Linux kernel to say ah we're going to control how much CPU you have access using quoters",
    "start": "1634880",
    "end": "1641559"
  },
  {
    "text": "but for memory we can't do that so CPU is compressible it can be throttled the memory can't be so if you use too much",
    "start": "1641559",
    "end": "1648200"
  },
  {
    "text": "CPU you'll just be throttled you won't be able to use too much CPU uh you've asked for so much you're only going to",
    "start": "1648200",
    "end": "1653279"
  },
  {
    "text": "get so much if you ask for too much memory you'll be kicked off because uh you're you're trying to do something",
    "start": "1653279",
    "end": "1658679"
  },
  {
    "text": "that we can't allow you may compromise other running processes yep uh and ultimately a request uh and a",
    "start": "1658679",
    "end": "1666600"
  },
  {
    "start": "1664000",
    "end": "1765000"
  },
  {
    "text": "limit are two things we have we can specify uh when we're asking for memory and CPU uh for a running pod uh a",
    "start": "1666600",
    "end": "1674840"
  },
  {
    "text": "request is basically how much you think you're going to need you for this and it may be",
    "start": "1674840",
    "end": "1680600"
  },
  {
    "text": "based on empirical evidence uh it probably will be based on empirical evidence this is what you think you're",
    "start": "1680600",
    "end": "1685640"
  },
  {
    "text": "going to need to be able to run this right so it's effectively the minimum you need uh and if you say you want this",
    "start": "1685640",
    "end": "1693399"
  },
  {
    "text": "much resource we will only run it on a place where there's that much resource available okay so if we we need two",
    "start": "1693399",
    "end": "1699720"
  },
  {
    "text": "cores we'll run it on a machine that has two cores available and we won't overcommit uh that so if you have lots",
    "start": "1699720",
    "end": "1706919"
  },
  {
    "text": "of pods requesting two CPU CES and you have eight CPU CES you can run four of them on it uh but you can't run any more",
    "start": "1706919",
    "end": "1714159"
  },
  {
    "text": "uh we will never overcommit a request so we're effectively guaranteeing that you have that amount available to you but we",
    "start": "1714159",
    "end": "1721039"
  },
  {
    "text": "also have the notion of a limit and the limit is the maximum amount of a resource you can request",
    "start": "1721039",
    "end": "1726440"
  },
  {
    "text": "and for a pod to be scheduled ultimately all we care about is that the amount of",
    "start": "1726440",
    "end": "1731720"
  },
  {
    "text": "request is available on a single node but if you ever hit the limit then ultimately we're going to kick you off",
    "start": "1731720",
    "end": "1737200"
  },
  {
    "text": "or we're going to throttle for you uh if you request zero then we can always schedule your pod but you may not get",
    "start": "1737200",
    "end": "1743840"
  },
  {
    "text": "anything you may not actually be able to do anything because you'll have no CPU shares uh but if you request zero and you go to a node that's empty it'll run",
    "start": "1743840",
    "end": "1750840"
  },
  {
    "text": "it'll do whatever it wants to it have as much as it needs uh but it will be compressed down to the point where it",
    "start": "1750840",
    "end": "1756000"
  },
  {
    "text": "may have nothing ultimately and it won't have any resources available to it at all and again we can't do that for C",
    "start": "1756000",
    "end": "1762600"
  },
  {
    "text": "memory but we can do it with CPU and ultimately we can use different",
    "start": "1762600",
    "end": "1769240"
  },
  {
    "start": "1765000",
    "end": "1835000"
  },
  {
    "text": "combinations of requests and limit to set resource guarantees or almost like protection how much protection does a",
    "start": "1769240",
    "end": "1775279"
  },
  {
    "text": "running pod have uh so this is for CPU and memory it effectively gives us quality of service uh if we specify the",
    "start": "1775279",
    "end": "1782440"
  },
  {
    "text": "request equal zero then we just saying run me whenever you can run this pod whenever you can if everything is maxed",
    "start": "1782440",
    "end": "1788440"
  },
  {
    "text": "out the whole class is maxed out then it won't be able to run it be scheduled but it won't be able to run because it has no CPU shares uh but if there's spare",
    "start": "1788440",
    "end": "1796799"
  },
  {
    "text": "capacity then it will be able to run and use whatever it needs if you say the request is less than the limit and",
    "start": "1796799",
    "end": "1802120"
  },
  {
    "text": "you're saying sometimes I know we need this much but sometimes we need to burst and have more this is the limit we're",
    "start": "1802120",
    "end": "1807240"
  },
  {
    "text": "going to go to and that's basically burstable we can run that we can't always guarantee it's going to have that available to it so it tries to you more",
    "start": "1807240",
    "end": "1814240"
  },
  {
    "text": "use more it may be frotted or it may even be kicked off for guaranteed we specified the",
    "start": "1814240",
    "end": "1819480"
  },
  {
    "text": "request and limit to be exactly the same so this is really the highest priority",
    "start": "1819480",
    "end": "1824519"
  },
  {
    "text": "it's not really priority currently but it's basically the protection guarantee so effectively you're saying this pod will always be able to run because it's",
    "start": "1824519",
    "end": "1831200"
  },
  {
    "text": "never going to want more than its limit and when you specify your",
    "start": "1831200",
    "end": "1837480"
  },
  {
    "start": "1835000",
    "end": "1859000"
  },
  {
    "text": "configuration it looks something like this uh you just specify the memory and a CPU for a request and a memory and CPU",
    "start": "1837480",
    "end": "1842840"
  },
  {
    "text": "for uh a limit on a pod uh which actually I should mention this is",
    "start": "1842840",
    "end": "1849159"
  },
  {
    "text": "applied on a container level not on a pod level so you have multiple containers you can set different uh",
    "start": "1849159",
    "end": "1854480"
  },
  {
    "text": "quality of service parameters for each container",
    "start": "1854480",
    "end": "1859760"
  },
  {
    "text": "and also pod scheduling uh we may care about other things as well we may care",
    "start": "1859760",
    "end": "1865120"
  },
  {
    "text": "about a particular machine has a particular thing that we need it may have a graphics card a GPU that we can",
    "start": "1865120",
    "end": "1870919"
  },
  {
    "text": "use it may have fast SSD disc as opposed to other uh uh spindle based disc uh so",
    "start": "1870919",
    "end": "1877960"
  },
  {
    "text": "there may be something about a machine that means we want to run it there uh so this is kind of like affinity and we can",
    "start": "1877960",
    "end": "1884559"
  },
  {
    "text": "do that with labels we'll talk about that in a second but we may want a certain of resource and we may also need certain dis uh we may have created a dis",
    "start": "1884559",
    "end": "1892480"
  },
  {
    "text": "uh that we need specifically or we may just need a certain amount of dis uh that can be provided to us so ultimately",
    "start": "1892480",
    "end": "1899279"
  },
  {
    "text": "with CPU memory uh when we try to schedule when the schedule is looking how to schedule this pod it's going to",
    "start": "1899279",
    "end": "1905799"
  },
  {
    "text": "look at the memory and CPU requirements and it's going to find a machine or",
    "start": "1905799",
    "end": "1910880"
  },
  {
    "text": "multiple machines that can actually have that resource available to it or if it's using a uh if it's using a specific disc",
    "start": "1910880",
    "end": "1918279"
  },
  {
    "text": "an example if I was able to do the demo which I'm not I'm going to get chance to uh we have a MySQL pod and a MySQL pod",
    "start": "1918279",
    "end": "1924960"
  },
  {
    "text": "mounts a disc and the dis has all of its data on it it's effectively an i inod b file system",
    "start": "1924960",
    "end": "1932279"
  },
  {
    "text": "and if that pod needs that dis then we have to schedule it on the place where that disc is available now the dis may",
    "start": "1932279",
    "end": "1939080"
  },
  {
    "text": "not be mounted at all in which case we can schedule it on any node and we could then Mount that disc on that node and",
    "start": "1939080",
    "end": "1944480"
  },
  {
    "text": "that's what kubat will do automatically for us it will schedule it on the on node and it will find the disc and mount",
    "start": "1944480",
    "end": "1950200"
  },
  {
    "text": "mount the disc for us if it's already mounted that disc then it will run it on that node because it needs that disc uh",
    "start": "1950200",
    "end": "1958279"
  },
  {
    "text": "and the same with we have a thing called persistent volumes so you need a certain amount of Ram uh certain r a certain",
    "start": "1958279",
    "end": "1965279"
  },
  {
    "text": "amount of dis we would have created these things called position volumes which you can mount in on",
    "start": "1965279",
    "end": "1971519"
  },
  {
    "text": "demand and also uh you may as I mentioned you may want to run it on a machine with fast SSD disc or a GP or",
    "start": "1971519",
    "end": "1978279"
  },
  {
    "start": "1972000",
    "end": "1999000"
  },
  {
    "text": "with other some other configuration you need uh it may also be that you have some kind of affinity need uh maybe you",
    "start": "1978279",
    "end": "1984399"
  },
  {
    "text": "have all of your machines in the cloud and some on premise which would be possible soon with a thing called ubben",
    "start": "1984399",
    "end": "1990399"
  },
  {
    "text": "and that way you can actually say I want to run it on",
    "start": "1990399",
    "end": "1994720"
  },
  {
    "text": "these clusters uh some on Prem some in Cloud you'll be able to use scheduling mechanisms for no definity to say only",
    "start": "1998480",
    "end": "2005600"
  },
  {
    "start": "1999000",
    "end": "2039000"
  },
  {
    "text": "run on machines on premise never run this uh in the cloud uh we also have this thing called",
    "start": "2005600",
    "end": "2013120"
  },
  {
    "text": "anti-affinity coming up as well so if you want to make sure it never runs on this or never runs on a machine with",
    "start": "2013120",
    "end": "2020000"
  },
  {
    "text": "another pod that might come into conflict with or various other reasons why you might not want to run two things",
    "start": "2020000",
    "end": "2027120"
  },
  {
    "text": "uh on the same node and you can use anti-affinity policies to make sure that never happens okay and that's coming up",
    "start": "2027120",
    "end": "2033440"
  },
  {
    "text": "probably in 1.3 but won't be GA in 1.3 uh but it will be available and ultimately what we have to",
    "start": "2033440",
    "end": "2040480"
  },
  {
    "start": "2039000",
    "end": "2065000"
  },
  {
    "text": "do is rank all of the potential nodes and decide which one to use uh and then we have different policies we can decide",
    "start": "2040480",
    "end": "2045639"
  },
  {
    "text": "on whether it's to balance the CPU in memory uh or make sure we have the machine we leave the machine with the",
    "start": "2045639",
    "end": "2051638"
  },
  {
    "text": "most free resources uh or even uh make sure that the pods that we have are",
    "start": "2051639",
    "end": "2057358"
  },
  {
    "text": "scheduled on different machines as much as possible so if we have 10 pods and 10 machines we should have one on each",
    "start": "2057359",
    "end": "2064560"
  },
  {
    "text": "node uh we have a bunch of things coming soon you'll see this in the slides when you get them uh velocity we're moving",
    "start": "2064560",
    "end": "2070919"
  },
  {
    "start": "2065000",
    "end": "2090000"
  },
  {
    "text": "very rapidly up to 1.2 we're at 12,000 stars now or something uh and 5,000",
    "start": "2070919",
    "end": "2077040"
  },
  {
    "text": "commits uh we're now at plus 50% unique contributors from what we were with 1.1",
    "start": "2077040",
    "end": "2082839"
  },
  {
    "text": "uh so kubat is dramatically uh rapidly uh growing in",
    "start": "2082839",
    "end": "2089358"
  },
  {
    "text": "popularity and also it's in the top 0.01% of all projects on GitHub which is",
    "start": "2089359",
    "end": "2096800"
  },
  {
    "start": "2090000",
    "end": "2121000"
  },
  {
    "text": "extremely significant right so uh two do a magnitude less than 1% or more than 1%",
    "start": "2096800",
    "end": "2103079"
  },
  {
    "text": "uh and so it's extremely popular 800 plus unique contributors uh we have many",
    "start": "2103079",
    "end": "2108280"
  },
  {
    "text": "projects that are based on kubernetes as well uh many companies contributing it's not just Google uh Red Hat are serious",
    "start": "2108280",
    "end": "2115000"
  },
  {
    "text": "contributors to this as are many of these others uh and Al many companies are already using cators in production",
    "start": "2115000",
    "end": "2121599"
  },
  {
    "start": "2121000",
    "end": "2148000"
  },
  {
    "text": "and in terms of plans we're at 1.2 currently uh 1.3 is in progress should",
    "start": "2121599",
    "end": "2126760"
  },
  {
    "text": "be in June uh we're trying to do a three month release Cadence and we also have container",
    "start": "2126760",
    "end": "2132800"
  },
  {
    "text": "engine which is Google's hosted platform it's the only time I'm going to mention Google stuff uh there's platforms as a",
    "start": "2132800",
    "end": "2138040"
  },
  {
    "text": "service Red Hat open shift Deus strus there's also destroyers such as cor",
    "start": "2138040",
    "end": "2143160"
  },
  {
    "text": "tectonic marantis Morano uh Red Hat Atomic mesos and finally it's open",
    "start": "2143160",
    "end": "2149160"
  },
  {
    "start": "2148000",
    "end": "2191000"
  },
  {
    "text": "source completely open source uh we are not governing this project uh the community is govern in this project I",
    "start": "2149160",
    "end": "2155960"
  },
  {
    "text": "used to work at Sun and we did this thing called open and it wasn't really very open source at all uh kuis is fully",
    "start": "2155960",
    "end": "2161560"
  },
  {
    "text": "open source and it is easy for you to contribute uh you can do poll requests you can uh get involved in issues uh and",
    "start": "2161560",
    "end": "2168440"
  },
  {
    "text": "there's many really good debates on there so if you're interested in helping kubes develop uh or you have a feature",
    "start": "2168440",
    "end": "2174240"
  },
  {
    "text": "you need you should go there and raise that question and talk to the engineers that are building it uh ku. is for the",
    "start": "2174240",
    "end": "2182079"
  },
  {
    "text": "documentation uh on GitHub it's kuat Cuates on slack slack. K8 io on Twitter",
    "start": "2182079",
    "end": "2189079"
  },
  {
    "text": "it's @ kuat I the slides for this talk uh well my can my canonical deck is",
    "start": "2189079",
    "end": "2196280"
  },
  {
    "start": "2191000",
    "end": "2637000"
  },
  {
    "text": "available there so it has more than these slides uh so if you look there you'll see all of the slides for this",
    "start": "2196280",
    "end": "2201640"
  },
  {
    "text": "talk and more uh and that's it basically uh I've got time for questions",
    "start": "2201640",
    "end": "2208920"
  },
  {
    "text": "or yes so uh we have a bunch of questions",
    "start": "2208920",
    "end": "2215560"
  },
  {
    "text": "actually we have a couple relating to uh um relating to region awareness so is",
    "start": "2215560",
    "end": "2224480"
  },
  {
    "text": "uh scheduling for region uh awareness available how do you do that do you use labels like you would for graphic cards",
    "start": "2224480",
    "end": "2230960"
  },
  {
    "text": "or is that a separate feature I think at a moment You' have to use labels uh there are annotations uh so we can make sure that persistent volumes and nodes",
    "start": "2230960",
    "end": "2237520"
  },
  {
    "text": "running the same place uh so we can actually identify where they're running through these annotations we didn't talk",
    "start": "2237520",
    "end": "2242760"
  },
  {
    "text": "about annotations but they're kind of like metadata that can be attached to running objects uh just like notes",
    "start": "2242760",
    "end": "2247960"
  },
  {
    "text": "basically uh so we can use annotations effectively uh but I think at the moment you'll probably have to use labels okay",
    "start": "2247960",
    "end": "2254200"
  },
  {
    "text": "what about so another question here about anti-affinity uh you mentioned that it's an upcoming feature but is",
    "start": "2254200",
    "end": "2259839"
  },
  {
    "text": "there any way to deal with this today if you're clever is there any way yeah so you can probably do it with uh you could",
    "start": "2259839",
    "end": "2266319"
  },
  {
    "text": "probably do it with labels I mean providing if you if you labels are not the best way but you're really kind of limiting the schedu to what your options",
    "start": "2266319",
    "end": "2272760"
  },
  {
    "text": "are uh when it comes to where it can schedule its pods uh and that's a bit of",
    "start": "2272760",
    "end": "2278240"
  },
  {
    "text": "a problem you're taking some of the power of it away from it so you could say only schedule these pods on these nodes only schedule these pods on these",
    "start": "2278240",
    "end": "2284720"
  },
  {
    "text": "nodes uh and so it's a problem of having maybe multi- tency problem that could be",
    "start": "2284720",
    "end": "2289839"
  },
  {
    "text": "a problem uh we also have names spacing but we can't schedule based on names spacing currently uh so probably the",
    "start": "2289839",
    "end": "2296119"
  },
  {
    "text": "best way is again use labels uh so group things by labels uh that will probably work for you okay um so do you see any",
    "start": "2296119",
    "end": "2304599"
  },
  {
    "text": "benefit of running kubernetes on top of uh yeah I think it's a good standards",
    "start": "2304599",
    "end": "2310760"
  },
  {
    "text": "based API so I I think ultimately we're not really in the B business of building clusters so if",
    "start": "2310760",
    "end": "2317480"
  },
  {
    "text": "there are other cluster mechanisms then that's great I mean so if you can build a cluster somewhere and then stick cators on top of it like you can do with",
    "start": "2317480",
    "end": "2324319"
  },
  {
    "text": "uh the dcos from uh mosphere uh then that's fine so any kind of cluster mechanism anything that can actually run",
    "start": "2324319",
    "end": "2330160"
  },
  {
    "text": "that stuff providing we can stick a kuber's layer on top of it then that's fine then we all we have to do is rely",
    "start": "2330160",
    "end": "2335760"
  },
  {
    "text": "on the cluster to do the schedu or to do carry out the instructions of theuer for",
    "start": "2335760",
    "end": "2340920"
  },
  {
    "text": "us that's what we need to do so yeah so I think it's very possible I don't see any did you say any benefits yeah",
    "start": "2340920",
    "end": "2347520"
  },
  {
    "text": "benefits yeah I don't think there's any benefits specifically well you could if if you have familiarity with mesos running mesos clusters and that's great",
    "start": "2347520",
    "end": "2354079"
  },
  {
    "text": "or you if you have mesos in your organization already and you want to use carbonet exactly if you're already using",
    "start": "2354079",
    "end": "2359760"
  },
  {
    "text": "mesos you can stick kuon as a shim on top of it effectively uh and then you can just move around as you need to uh",
    "start": "2359760",
    "end": "2365400"
  },
  {
    "text": "portability is a great uh one of the goals of kuat uh so I think that's having that API available but not having",
    "start": "2365400",
    "end": "2371800"
  },
  {
    "text": "to worry too much about the infrastructure underneath is really important in fact I mentioned B earlier they may I'd love to see a day when we",
    "start": "2371800",
    "end": "2378920"
  },
  {
    "text": "have at the moment we schedule stuff on Virtual machines uh we could EV we could effectively get rid of the virtual machines and just schedule them on Bard",
    "start": "2378920",
    "end": "2385560"
  },
  {
    "text": "directly uh in the future at some point so that's exactly the same thing we have a cluster we have a kubernetes interface",
    "start": "2385560",
    "end": "2392240"
  },
  {
    "text": "a facade almost and we send instructions to cators like we always would do but we don't we don't care about the mechanics",
    "start": "2392240",
    "end": "2398560"
  },
  {
    "text": "of what happens underneath the hood okay so uh another question here will console be supported at some",
    "start": "2398560",
    "end": "2406440"
  },
  {
    "text": "point uh I don't know will conso be supported I think you'd have to ask on uh you'd have to go to GitHub and find",
    "start": "2406440",
    "end": "2411960"
  },
  {
    "text": "out uh I'm not sure there's any plans but I think pretty much everything that can be done with kubats has been",
    "start": "2411960",
    "end": "2417119"
  },
  {
    "text": "discussed on uh the GitHub uh project because everybody's interested everybody's excited and everybody has",
    "start": "2417119",
    "end": "2423280"
  },
  {
    "text": "their own problems they want to solve of it so I think if somebody wants to run cons probably would raised it it may be",
    "start": "2423280",
    "end": "2428839"
  },
  {
    "text": "it went into a dead end uh maybe you could revive it and again I think that's the the great thing about it is that you",
    "start": "2428839",
    "end": "2434079"
  },
  {
    "text": "can just this is what I want how can we build this and people will be interested and they'll tell you why you can't or",
    "start": "2434079",
    "end": "2440480"
  },
  {
    "text": "why you can or how you can uh so I think getting involv in those discussions is really important what about something",
    "start": "2440480",
    "end": "2445960"
  },
  {
    "text": "like Auto scaling you we saw that you could scale up by manually going in and say I want two notes but could you write",
    "start": "2445960",
    "end": "2453000"
  },
  {
    "text": "is there any sort of support for that in kubernetes already or would you have to write it external program that would scale it up and down for you all right",
    "start": "2453000",
    "end": "2459760"
  },
  {
    "text": "so I would have talked about pod a scaling it's quite a 30 minutes is quite short for a talk so uh showing all these",
    "start": "2459760",
    "end": "2466280"
  },
  {
    "text": "things in a demo is hard uh and Autos scaling demos are really really terrible because Autos scaling is something",
    "start": "2466280",
    "end": "2471440"
  },
  {
    "text": "happens over a long period of time not over in not in a two-minute demo so it never really works properly uh but the",
    "start": "2471440",
    "end": "2477800"
  },
  {
    "text": "fact is is if you are if you have a one pod servicing traffic and you're monitoring the resource utilization of",
    "start": "2477800",
    "end": "2484000"
  },
  {
    "text": "that pod uh you can scale when it reaches a certain level and add more pods and those pods will be distributed",
    "start": "2484000",
    "end": "2489960"
  },
  {
    "text": "across the nodes and the traffic will be low balance via the service to each of those Pods at some point you may run out",
    "start": "2489960",
    "end": "2495839"
  },
  {
    "text": "of nodes you may not be able to schedule any more pods uh that's the whole point of kubat uh you could potentially run",
    "start": "2495839",
    "end": "2501839"
  },
  {
    "text": "out of nodes and that point if you're in the cloud uh you could also scale your",
    "start": "2501839",
    "end": "2507720"
  },
  {
    "text": "nodes for some providers so for Google container engine and Google Compu engine if you're on no o we can do it today",
    "start": "2507720",
    "end": "2514599"
  },
  {
    "text": "we're looking for other people to do other mechanisms that allow to scale dynamically uh but we can then scale",
    "start": "2514599",
    "end": "2520480"
  },
  {
    "text": "more nodes and add new no uh new pods onto those new nodes the one thing we can't do currently is rebalance so once",
    "start": "2520480",
    "end": "2526960"
  },
  {
    "text": "we've added nodes we can't move things so we can flatten out the cluster again",
    "start": "2526960",
    "end": "2532000"
  },
  {
    "text": "uh that would probably come in the future where we can reshape the cluster based on uh adding new nodes would you",
    "start": "2532000",
    "end": "2537920"
  },
  {
    "text": "be able to sked or uh Scale based on other metrics rather than just uh load",
    "start": "2537920",
    "end": "2543640"
  },
  {
    "text": "like the face of the moon or I think so that could be measured uh anything that",
    "start": "2543640",
    "end": "2548920"
  },
  {
    "text": "could be measured on a scale uh and be scal proportionately uh then I think that will be possible at the moment we",
    "start": "2548920",
    "end": "2556160"
  },
  {
    "text": "can do that with nodes already uh so we can do that on lots of different metrics uh so we can be monitoring the nodes and",
    "start": "2556160",
    "end": "2562079"
  },
  {
    "text": "Scale based on metrics that are happening on the nodes uh so it could be dis utilization or something like that",
    "start": "2562079",
    "end": "2567319"
  },
  {
    "text": "but we really want kuat to be the one that's responsible for asking the underlying runtime to scale not having",
    "start": "2567319",
    "end": "2574800"
  },
  {
    "text": "the runtime scale automatically on its own that kind of screws the schedule up a little bit so basically we should have",
    "start": "2574800",
    "end": "2580839"
  },
  {
    "text": "an interface from kubat onto the underlying runtime so that we can say hey we need more pods can you give me",
    "start": "2580839",
    "end": "2587280"
  },
  {
    "text": "more pods and it may create a purchase order and send out to to Dell so you get",
    "start": "2587280",
    "end": "2592359"
  },
  {
    "text": "another computer delivered on a forklift the next day and you stick it in and you add it to your your cluster and that's autoscaling uh a very manual process of",
    "start": "2592359",
    "end": "2600079"
  },
  {
    "text": "Autos scaling but yeah I think it's it's it's very possible today with container engine uh but again I that's not I'm not",
    "start": "2600079",
    "end": "2606720"
  },
  {
    "text": "recommending you choose container engine but it is one option for that today for",
    "start": "2606720",
    "end": "2612200"
  },
  {
    "text": "AWS and others a lot of people run kubernetes on AWS it's probably possible to hook it up to their Al scaler as well",
    "start": "2612200",
    "end": "2618160"
  },
  {
    "text": "but again you may be relying on the operate the cloud provider to do the scaling for you as opposed to having",
    "start": "2618160",
    "end": "2624040"
  },
  {
    "text": "cat's request the a scaling which is a bit different cool thank you Mandy thank you very much yes so",
    "start": "2624040",
    "end": "2632760"
  }
]