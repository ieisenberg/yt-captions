[
  {
    "text": "[Music] welcome to the go-to podcast each",
    "start": "280",
    "end": "7560"
  },
  {
    "text": "episode covers the brightest and boldest ideas from the world's leading experts in software development tune in for",
    "start": "7560",
    "end": "14080"
  },
  {
    "text": "practical lessons compelling theories and plenty of",
    "start": "14080",
    "end": "19240"
  },
  {
    "text": "inspiration go to gathers the brightest Minds in the software Community to help developers tackle projects today plan",
    "start": "19960",
    "end": "26720"
  },
  {
    "text": "for tomorrow and create a better future stay up to dat with the latest in Tech through goto's top rated events held",
    "start": "26720",
    "end": "33320"
  },
  {
    "text": "online and in person in cities like Amsterdam London Copenhagen and Chicago",
    "start": "33320",
    "end": "39160"
  },
  {
    "text": "and by subscribing to the go-to conferences YouTube channel where you can find thousands more highquality de",
    "start": "39160",
    "end": "44719"
  },
  {
    "text": "talks learn more otopia [Music]",
    "start": "44719",
    "end": "51320"
  },
  {
    "text": "Tech welcome everyone to the go to book club we are here in Amsterdam now and um",
    "start": "51320",
    "end": "59000"
  },
  {
    "text": "I'm ala G from Pam Sadam and thought works and I'm here to introduce you",
    "start": "59000",
    "end": "64280"
  },
  {
    "text": "Katherine jarmo hi yeah I'm Katherine I'm so excited to be here with you I'm also at thought Works um I work right",
    "start": "64280",
    "end": "71520"
  },
  {
    "text": "now as a principal data scientist um and we're here I think to talk about my recent O'Reilly book uh which is called",
    "start": "71520",
    "end": "78759"
  },
  {
    "text": "practical data privacy and is aimed towards data folks and Technical folks that want to learn about privacy so",
    "start": "78759",
    "end": "85560"
  },
  {
    "text": "really excited to chat with you about this today cool like I'm excited too probably all excited but anyway so um I",
    "start": "85560",
    "end": "93600"
  },
  {
    "text": "was curious what exactly inspired you to write this book in the first place yeah",
    "start": "93600",
    "end": "100439"
  },
  {
    "text": "I mean I think um I think probably a lot of things uh first and foremost I've",
    "start": "100439",
    "end": "105640"
  },
  {
    "text": "been working in the field of data privacy and data science and machine learning now for multiple years and I",
    "start": "105640",
    "end": "111920"
  },
  {
    "text": "think I learned a lot the hard way so when I first got interested and I started asking questions like okay is",
    "start": "111920",
    "end": "118759"
  },
  {
    "text": "privacy important in machine learning how are we going to do it um how would we even go about that a lot of the",
    "start": "118759",
    "end": "124600"
  },
  {
    "text": "material that was available was either really high level um really basic stuff",
    "start": "124600",
    "end": "129800"
  },
  {
    "text": "so like kind of older techniques maybe even sometimes broken techniques that we wouldn't recommend anymore or it was",
    "start": "129800",
    "end": "136560"
  },
  {
    "text": "Hardcore research it was like you're a PhD in differential privacy here's a paper let's go or here's a paper on",
    "start": "136560",
    "end": "144280"
  },
  {
    "text": "cryptography like figure figure it out yourself and I think that um through",
    "start": "144280",
    "end": "150000"
  },
  {
    "text": "that learning was probably a lot of battles I wouldn't have had to do so kind of the book the idea of the book is",
    "start": "150000",
    "end": "157080"
  },
  {
    "text": "really like a gift to me five six years ago like these are all the shortcuts",
    "start": "157080",
    "end": "162560"
  },
  {
    "text": "that you eventually find out the hard way like here's if I had to teach it to you again here's how I would teach it",
    "start": "162560",
    "end": "169519"
  },
  {
    "text": "and kind of the goal is to demystify the field a little bit of privacy engineering and basically just of",
    "start": "169519",
    "end": "175640"
  },
  {
    "text": "technical privacy in general and get more people into it because I think it's should be like more accessible easier to",
    "start": "175640",
    "end": "183239"
  },
  {
    "text": "understand uh fully like if we want to use the term democratize but available for more people wow I think that's nice",
    "start": "183239",
    "end": "191640"
  },
  {
    "text": "like it's it's a gift back to me and the rest of the folks I wish you know we",
    "start": "191640",
    "end": "196760"
  },
  {
    "text": "have more such beautiful Books available where you can just go oh I just need to tap to this knowledge ding ding ding",
    "start": "196760",
    "end": "202319"
  },
  {
    "text": "ready the next one that's your book right yeah fantastic so before let's say deep diving a little",
    "start": "202319",
    "end": "210439"
  },
  {
    "text": "bit more into this book I just want to highlight the specific terms that we're",
    "start": "210439",
    "end": "215920"
  },
  {
    "text": "going to use because based on my personal experience I noticed that there are a lot of terms that used",
    "start": "215920",
    "end": "222200"
  },
  {
    "text": "interchangeably so what I'm usually hearing uh in the room full of Enterprise Architects uh when the data",
    "start": "222200",
    "end": "229439"
  },
  {
    "text": "topic pops up it usually starts with um let's say we need to protect data of",
    "start": "229439",
    "end": "235040"
  },
  {
    "text": "course and then we go we need to protect data so what techniques we're going to use we're going to use pseudonymization",
    "start": "235040",
    "end": "240480"
  },
  {
    "text": "we're going to use anonymization we're going to use this and that and that and I have a feeling that it's all",
    "start": "240480",
    "end": "247920"
  },
  {
    "text": "different techniques it's all different meanings and the end result how the data",
    "start": "247920",
    "end": "253760"
  },
  {
    "text": "looks after this techniques that's what's usually really underestimated because we forgetting about data",
    "start": "253760",
    "end": "260160"
  },
  {
    "text": "consumer such as data scientist data Engineers data analyst and the rest of the fxs so could you probably walk me",
    "start": "260160",
    "end": "265280"
  },
  {
    "text": "through a little bit are there any difference between them or just you can use them interchangeably yeah absolutely and I",
    "start": "265280",
    "end": "272440"
  },
  {
    "text": "think you're you're highlighting a key point and I know it's been a pain point that you've probably experienced in your",
    "start": "272440",
    "end": "277680"
  },
  {
    "text": "life right yes um even like I want to even Zoom back to like data protection",
    "start": "277680",
    "end": "283199"
  },
  {
    "text": "versus data privacy those are actually also different fields right they're overlapping Fields but data privacy is",
    "start": "283199",
    "end": "290440"
  },
  {
    "text": "also like a social uh and cultural understanding of of privacy and maybe",
    "start": "290440",
    "end": "295800"
  },
  {
    "text": "also relates to individual experiences and feelings that and like how we want to share things and how we want to",
    "start": "295800",
    "end": "301880"
  },
  {
    "text": "change how we share things and then we have data protection which is really like a lot of the laws around data",
    "start": "301880",
    "end": "307320"
  },
  {
    "text": "privacy around data protection and maybe that also borders on data security which is also different and infos SEC which is",
    "start": "307320",
    "end": "315199"
  },
  {
    "text": "also different and so we have all these like neighboring things and I think if you're just an architect or a software",
    "start": "315199",
    "end": "322199"
  },
  {
    "text": "person you may just think like oh those are all the same words and they mean all the same stuff and that's not your fault",
    "start": "322199",
    "end": "328080"
  },
  {
    "text": "like probably lots of people have used them interchangeably with you but let's correct some of some of the problems",
    "start": "328080",
    "end": "335280"
  },
  {
    "text": "here I think um you point out Su imization versus anonymization that's",
    "start": "335280",
    "end": "340960"
  },
  {
    "text": "usually what just mixed up constantly massively different things massively different outcomes I mean technically uh",
    "start": "340960",
    "end": "348880"
  },
  {
    "text": "pseudonymization is all we're trying to do is we're trying to create some sort of placeholder or pseudonym for maybe",
    "start": "348880",
    "end": "356039"
  },
  {
    "text": "even personal identifiers so maybe we shouldn't release the name or the email",
    "start": "356039",
    "end": "361080"
  },
  {
    "text": "so we create a pseudonym we can use many different methods So within pseudonymization there's subfields like",
    "start": "361080",
    "end": "367280"
  },
  {
    "text": "masking tokenization of course you can always use reduction that's slightly different you can even do format",
    "start": "367280",
    "end": "373199"
  },
  {
    "text": "preserving pseudonymization using methods of format preserving encryption you can do all of these different things",
    "start": "373199",
    "end": "379360"
  },
  {
    "text": "but those is a subfield of simiz and then we have",
    "start": "379360",
    "end": "384440"
  },
  {
    "text": "anonymization and I'm here to break it to everybody and I'm sorry anonymization",
    "start": "384440",
    "end": "390000"
  },
  {
    "text": "actually doesn't exist like if if we collect data if we collect information",
    "start": "390000",
    "end": "395919"
  },
  {
    "text": "and we release information there is actually mathematically no guarantee",
    "start": "395919",
    "end": "400960"
  },
  {
    "text": "that we cannot have somebody learn something about the individual and so the true like the dictionary definition",
    "start": "400960",
    "end": "407919"
  },
  {
    "text": "of anonymization which is like you can't learn anything about the in individual",
    "start": "407919",
    "end": "413080"
  },
  {
    "text": "you can't have the potential to reidentifying using Ma math and logic",
    "start": "413080",
    "end": "420039"
  },
  {
    "text": "and information theory that is actually just impossible that's okay that's okay",
    "start": "420039",
    "end": "425240"
  },
  {
    "text": "because even though it's not technically possible we have now defined methods to",
    "start": "425240",
    "end": "432160"
  },
  {
    "text": "let's just use the term anonymize or to approximate the best way that we can",
    "start": "432160",
    "end": "437360"
  },
  {
    "text": "anonymize data um using techniques like differential privacy and I'm happy to",
    "start": "437360",
    "end": "443120"
  },
  {
    "text": "talk further about it but there's ways that we can try to think about the",
    "start": "443120",
    "end": "448759"
  },
  {
    "text": "information that we're giving out and think about what the implication of that information is for the privacy of",
    "start": "448759",
    "end": "455800"
  },
  {
    "text": "individuals and to rigorously Define and measure that and to tune that for as you",
    "start": "455800",
    "end": "461520"
  },
  {
    "text": "say the exact use case right because at the end of the day we have some users",
    "start": "461520",
    "end": "466840"
  },
  {
    "text": "they might be data analysts they might be business users they might be you know the users themselves they might be data",
    "start": "466840",
    "end": "474080"
  },
  {
    "text": "scientists or machine learning folks and they're the ones that actually have to then consume this data and make",
    "start": "474080",
    "end": "480440"
  },
  {
    "text": "decisions with it or potentially um lead the company in certain directions and we",
    "start": "480440",
    "end": "485840"
  },
  {
    "text": "need to be very clear like what's their needs too in this so we balance the user needs for privacy and the data needs for",
    "start": "485840",
    "end": "493840"
  },
  {
    "text": "information yeah okay so like short summary pseudonymization anonymization",
    "start": "493840",
    "end": "499599"
  },
  {
    "text": "it's not the same thing it's different and let's say the classic anonymization like 100% guarantee doesn't exist yeah",
    "start": "499599",
    "end": "507960"
  },
  {
    "text": "and just get back to that point I think we're going to Deep dive further into differential privacy of course but what",
    "start": "507960",
    "end": "514719"
  },
  {
    "text": "I usually see in the let's say boardrooms that it this a little bit binary approach to let's say how a data",
    "start": "514719",
    "end": "522518"
  },
  {
    "text": "could be secured it's like uh or it's secured or not and this is usually the",
    "start": "522519",
    "end": "529040"
  },
  {
    "text": "all conversations that we have with infas FX with compliance fog uh like",
    "start": "529040",
    "end": "534959"
  },
  {
    "text": "odit and and all that like and if it's a huge Enterprise you have of course a done a governance board and then it goes",
    "start": "534959",
    "end": "541399"
  },
  {
    "text": "more and more and more and more um but I think what is really fascinating with",
    "start": "541399",
    "end": "546680"
  },
  {
    "text": "differential privacy that this is the first time when you can think about",
    "start": "546680",
    "end": "551920"
  },
  {
    "text": "let's say the data protection as a specific scale so it's not let's say or",
    "start": "551920",
    "end": "557880"
  },
  {
    "text": "protected or not protected anymore so could you explain more about that because I think that's the most",
    "start": "557880",
    "end": "563279"
  },
  {
    "text": "tremendous concept that is introduced in the chapter two of your book yeah yeah",
    "start": "563279",
    "end": "568440"
  },
  {
    "text": "yeah and it's I think it's even almost bigger than that too because it's like simiz is also a method and that offers",
    "start": "568440",
    "end": "575800"
  },
  {
    "text": "some privacy guarantees that we go into anonymization differential privacy and then we can go even further of like",
    "start": "575800",
    "end": "582079"
  },
  {
    "text": "Federated use cases and and all this stuff you know this stuff from from our",
    "start": "582079",
    "end": "587399"
  },
  {
    "text": "conversations but it's all in the book too um but I think like it I love this",
    "start": "587399",
    "end": "592680"
  },
  {
    "text": "thing though it's like privacy isn't on or off data protection isn't on or off it isn't like I switched on the data",
    "start": "592680",
    "end": "599279"
  },
  {
    "text": "protection magically everything's protected and it probably never was so if unfortunately we have this kind of",
    "start": "599279",
    "end": "605800"
  },
  {
    "text": "idea do we implement the security do we implement the protection this is very binary mindset but that's not at all how",
    "start": "605800",
    "end": "613440"
  },
  {
    "text": "any of these Technologies work and and we know from our work in data it's also not how data and information works just",
    "start": "613440",
    "end": "619920"
  },
  {
    "text": "by default right like when you're working in machine learning as you and I have had a lot of experience in um",
    "start": "619920",
    "end": "626000"
  },
  {
    "text": "there's a variety of truthiness of the data and there's a variety of protection of that data depending on how you",
    "start": "626000",
    "end": "632839"
  },
  {
    "text": "implement your algorithms or your architectures and so I think cool thing about differential privacy it's all",
    "start": "632839",
    "end": "639680"
  },
  {
    "text": "tunable so it's by default tunable and um the base theory of differential",
    "start": "639680",
    "end": "645720"
  },
  {
    "text": "privacy is the idea of tuning the amount of information that somebody can get",
    "start": "645720",
    "end": "652120"
  },
  {
    "text": "from the results and tuning that and bounding it by a small probability",
    "start": "652120",
    "end": "657399"
  },
  {
    "text": "bounce so I give you answer yeah and um let's say the answer is 10 yeah whatever",
    "start": "657399",
    "end": "665399"
  },
  {
    "text": "whatever the answer is how many purchases uh users in this area made in this month or something like this yeah",
    "start": "665399",
    "end": "672200"
  },
  {
    "text": "it's 10 okay and then I add a person to the data set and maybe the person falls",
    "start": "672200",
    "end": "677480"
  },
  {
    "text": "in that query then you ask me again and the change in those answers so 10 let's",
    "start": "677480",
    "end": "685480"
  },
  {
    "text": "say then now I say 11 it's like if I just answer you 11 you could make a",
    "start": "685480",
    "end": "690760"
  },
  {
    "text": "pretty good inference that somebody got added and that person made a purchase yeah and the goal of differential",
    "start": "690760",
    "end": "696440"
  },
  {
    "text": "privacy is to add essentially some uncertainty in whether the answer was",
    "start": "696440",
    "end": "702760"
  },
  {
    "text": "actually the answer and in doing so allow there to be some uh some of this",
    "start": "702760",
    "end": "710240"
  },
  {
    "text": "probabilistic thinking in how certain am I that a person got added or",
    "start": "710240",
    "end": "716639"
  },
  {
    "text": "not and so we can think of this like a really good example I think is like salaries yeah so let's say there was",
    "start": "716639",
    "end": "722600"
  },
  {
    "text": "like a dashboard of all payroll right with some buckets probably exactly and then a new person got hired in your team",
    "start": "722600",
    "end": "729480"
  },
  {
    "text": "and you're of course quite curious because all people are curious and you're like I wonder what how much this",
    "start": "729480",
    "end": "734920"
  },
  {
    "text": "person is getting paid and maybe s question especially the European Union exactly and maybe the person doesn't",
    "start": "734920",
    "end": "741279"
  },
  {
    "text": "want to share or maybe you don't want to ask and whatever but if the dashboard just reports all of the actual numbers",
    "start": "741279",
    "end": "747399"
  },
  {
    "text": "there's a pretty good chance especially with some data thinking and probabilistic thinking or statistical",
    "start": "747399",
    "end": "752720"
  },
  {
    "text": "thinking that you could reverse engineer the salary and what differential privacy",
    "start": "752720",
    "end": "757839"
  },
  {
    "text": "tries to guaranteed by using differential privacy mechanisms is it tries to again add some level of noise",
    "start": "757839",
    "end": "764560"
  },
  {
    "text": "some level of bounding so constraining so that outliers are essentially non-existent because outliers leak a lot",
    "start": "764560",
    "end": "771160"
  },
  {
    "text": "of privacy yeah and then adding this um probabilistic noise so that even the",
    "start": "771160",
    "end": "777519"
  },
  {
    "text": "people running the system they can't determine how much noise was added and this",
    "start": "777519",
    "end": "783079"
  },
  {
    "text": "uncertainty um and this like the tuning of the noise can help you tune this",
    "start": "783079",
    "end": "788120"
  },
  {
    "text": "amount of privacy guarantees versus the amount of information and of course in",
    "start": "788120",
    "end": "793399"
  },
  {
    "text": "an internal use case maybe you want less noise U more information but then you also have less privacy and maybe if",
    "start": "793399",
    "end": "800360"
  },
  {
    "text": "you're releasing data to a partner to a third party or to the public maybe you want to actually tune up that noise and",
    "start": "800360",
    "end": "807040"
  },
  {
    "text": "it's okay that it's not 100% % accurate for whatever accurate means so could we",
    "start": "807040",
    "end": "813160"
  },
  {
    "text": "say that in this way we try to protect data from let's say reverse engineering from from privacy uh violations so",
    "start": "813160",
    "end": "820880"
  },
  {
    "text": "essentially what we're trying to do is we're trying to drisk the release of information for whatever we Define as an",
    "start": "820880",
    "end": "828279"
  },
  {
    "text": "adequate privacy risk this is where like the thinking and privacy really overlaps",
    "start": "828279",
    "end": "833560"
  },
  {
    "text": "with security and infosec thinking because it's about what is the actual",
    "start": "833560",
    "end": "838800"
  },
  {
    "text": "risk what's the threat model here when we release this data what are we worried about and how do we then adequately tune",
    "start": "838800",
    "end": "846880"
  },
  {
    "text": "the protections that we have or employ or you know implement the protections",
    "start": "846880",
    "end": "852399"
  },
  {
    "text": "that we have that's going to adequately mitigate that risk so we feel safe and",
    "start": "852399",
    "end": "857759"
  },
  {
    "text": "we feel like maybe our users or our citizens if you're a country are safe and we can release this data and so it's",
    "start": "857759",
    "end": "865040"
  },
  {
    "text": "pretty cool the library that I used to implement differential private see in the book um is tumult analytics it's the",
    "start": "865040",
    "end": "871800"
  },
  {
    "text": "open source library and they were the folks that helped the US Census release differentially private census data for",
    "start": "871800",
    "end": "878680"
  },
  {
    "text": "the first time in 2020 and they just last week and of course there'll be some some delay in the release of this video",
    "start": "878680",
    "end": "885240"
  },
  {
    "text": "but a few weeks ago now at this point in time uh they just released all of the Wikipedia data with differential privacy",
    "start": "885240",
    "end": "892959"
  },
  {
    "text": "which is pretty cool I think whoa and I just want to let's say to hold this",
    "start": "892959",
    "end": "898079"
  },
  {
    "text": "moment so so um we usually see that",
    "start": "898079",
    "end": "903399"
  },
  {
    "text": "especially in any data projects usually the Privacy this is like the last step",
    "start": "903399",
    "end": "909920"
  },
  {
    "text": "the compliance is the last step yeah and also take into account the upcoming you",
    "start": "909920",
    "end": "916199"
  },
  {
    "text": "AI regulation act which indicates specific let's say highrisk Industries",
    "start": "916199",
    "end": "922519"
  },
  {
    "text": "high risk use cases so I'm really wondering how you can",
    "start": "922519",
    "end": "929240"
  },
  {
    "text": "structure the process in the most mutually beneficial way for software",
    "start": "929240",
    "end": "935920"
  },
  {
    "text": "Engineers for infac and for data Fox so what you going to do let's say step by",
    "start": "935920",
    "end": "941720"
  },
  {
    "text": "step what is your recommendation on that and I think like we've seen a lot of this happen in governance and risk",
    "start": "941720",
    "end": "948839"
  },
  {
    "text": "conversations and I mean it's going to be different for every organization which is why it has to be led by those",
    "start": "948839",
    "end": "955160"
  },
  {
    "text": "experts and I think you you named them very appropriately it has to be the software infra architect side of the",
    "start": "955160",
    "end": "961600"
  },
  {
    "text": "house it has to be the data side of the house it has to be the security side of the house and if the company is large",
    "start": "961600",
    "end": "969079"
  },
  {
    "text": "enough as you referenced before has to be compliance audit privacy legal and",
    "start": "969079",
    "end": "974839"
  },
  {
    "text": "those people should all be sitting on the data governance board by them you know with talking with each other",
    "start": "974839",
    "end": "980000"
  },
  {
    "text": "regularly let's hope if not you know maybe that's the first step and that's the first chapter of the book is like if",
    "start": "980000",
    "end": "986480"
  },
  {
    "text": "you don't have functioning data governance you can't do any of this cool stuff so you need functioning data",
    "start": "986480",
    "end": "991600"
  },
  {
    "text": "governance first because as you point out there has to be a risk appetite set",
    "start": "991600",
    "end": "996639"
  },
  {
    "text": "so you have to Define what is privacy risk for us what is data protection risk what is information security risk how do",
    "start": "996639",
    "end": "1003560"
  },
  {
    "text": "we Define that how do we Define it on a data science or a data caseby case basis",
    "start": "1003560",
    "end": "1008880"
  },
  {
    "text": "so for certain things do we have a a larger risk appetite like are we willing to take more privacy risk on to do a new",
    "start": "1008880",
    "end": "1016959"
  },
  {
    "text": "cool machine learning thing that we want want to test out but that all has to be defined at the organizational level and",
    "start": "1016959",
    "end": "1023800"
  },
  {
    "text": "once that starts to get defined you can actually work with teams to prioritize experiments and that's often you know",
    "start": "1023800",
    "end": "1030839"
  },
  {
    "text": "that's very much the thought works way is identify a priority identify a thin",
    "start": "1030839",
    "end": "1035959"
  },
  {
    "text": "slice and then iterate on it and as very agile methodologies as well and I think",
    "start": "1035959",
    "end": "1042280"
  },
  {
    "text": "that that actually works really well for these new technologies because they're so tunable because um some of them have",
    "start": "1042280",
    "end": "1049880"
  },
  {
    "text": "different requirements in terms of how to deploy and scale them and so forth that on a caseby Case basis you",
    "start": "1049880",
    "end": "1056960"
  },
  {
    "text": "experiment with some of these new technologies um you iterate and learn as you always do and then maybe as you",
    "start": "1056960",
    "end": "1064240"
  },
  {
    "text": "iterate and learn you can build them into platform Services maybe you can build them into reusable functionality",
    "start": "1064240",
    "end": "1071240"
  },
  {
    "text": "that your teams can leverage and that requires obviously the software folks in the house and the platform folks in the",
    "start": "1071240",
    "end": "1077919"
  },
  {
    "text": "house to really think through how do we optimize these systems for scaled use",
    "start": "1077919",
    "end": "1083520"
  },
  {
    "text": "and for the types of users that we have and so it can be this huge iterative process and then you go back to the",
    "start": "1083520",
    "end": "1089000"
  },
  {
    "text": "governance board and say okay here's our learnings like what's the next priority and it can it can iterate that way too",
    "start": "1089000",
    "end": "1096240"
  },
  {
    "text": "okay so let's for example zoom in into the use case so you're in the room there's specific use case we're not",
    "start": "1096240",
    "end": "1101640"
  },
  {
    "text": "talking about Mission critical scenario right now we're just talking about something that we would prefer to do",
    "start": "1101640",
    "end": "1107120"
  },
  {
    "text": "better let's uh good boundaries it's for internal usage so um it's still let say",
    "start": "1107120",
    "end": "1113480"
  },
  {
    "text": "private information but like we're not relasing this information somewhere external of the company so what will be",
    "start": "1113480",
    "end": "1120760"
  },
  {
    "text": "the Hands-On approach you briefly mentioned the library that you use so how you going to go so you have this use",
    "start": "1120760",
    "end": "1126480"
  },
  {
    "text": "case you have this like software folks in the room uh data folks in the room um infra privacy folks in the room so what",
    "start": "1126480",
    "end": "1133799"
  },
  {
    "text": "are you going to do yeah so I mean there might already be policies and standards that say like for these use cases this",
    "start": "1133799",
    "end": "1140200"
  },
  {
    "text": "is what we recommend and depending on the org you're at they might be super specific and say like use this library",
    "start": "1140200",
    "end": "1146440"
  },
  {
    "text": "and with these parameters and so on so they you can even Define a per exactly",
    "start": "1146440",
    "end": "1151480"
  },
  {
    "text": "if if you wanted to right and that's usually for organizations that have been doing this for a long time that's why",
    "start": "1151480",
    "end": "1156799"
  },
  {
    "text": "they don't have to have so much experimental they've already experimented they've codified it in their policies or their standards but",
    "start": "1156799",
    "end": "1163520"
  },
  {
    "text": "there's also a lot of orgs that don't want to cify it in standards because they know technology change over time so",
    "start": "1163520",
    "end": "1169720"
  },
  {
    "text": "you definitely go to The Guiding standards and policies um you kind of hopefully by that point in time read and",
    "start": "1169720",
    "end": "1175760"
  },
  {
    "text": "understand a lot of them and then to actually Implement them um if you can leverage a platform that's already there",
    "start": "1175760",
    "end": "1182480"
  },
  {
    "text": "if you can leverage a tool that's already there always do that nobody should be rolling their own of any of",
    "start": "1182480",
    "end": "1188200"
  },
  {
    "text": "these things in my opinion unless you have you know a privacy engineering team that's in charge of this um and in youra",
    "start": "1188200",
    "end": "1196039"
  },
  {
    "text": "in the case that you described so like internal data use not going to be released publicly you might even just",
    "start": "1196039",
    "end": "1201720"
  },
  {
    "text": "decide simiz is enough okay or you might decide like aggregation Plus suud imization for these particular",
    "start": "1201720",
    "end": "1208760"
  },
  {
    "text": "categories is enough so there can be I think particularly for internal use",
    "start": "1208760",
    "end": "1214520"
  },
  {
    "text": "cases a much larger risk appetite because presumably with your internal",
    "start": "1214520",
    "end": "1219840"
  },
  {
    "text": "users there's a high level of trust yeah if you're in a huge org um and this is one of the things that they did at",
    "start": "1219840",
    "end": "1226480"
  },
  {
    "text": "Google is for dat a scientist you have to first prove that your experiment idea",
    "start": "1226480",
    "end": "1232000"
  },
  {
    "text": "works with differentially private data wow before you can get access to the",
    "start": "1232000",
    "end": "1238840"
  },
  {
    "text": "real data and this for the sensitive you know pii and other types of sensitive data and I think that's a really cool",
    "start": "1238840",
    "end": "1245440"
  },
  {
    "text": "idea that could be used in in in all sorts of Industry where it's like look maybe you're just in the Eda step the",
    "start": "1245440",
    "end": "1252400"
  },
  {
    "text": "exploratory data analysis step yeah maybe you don't need the raw data maybe you just need some artifacts of the raw",
    "start": "1252400",
    "end": "1258559"
  },
  {
    "text": "data to just test out the idea and then if you want to go further maybe there's a process to apply to get access to the",
    "start": "1258559",
    "end": "1265360"
  },
  {
    "text": "raw data and maybe there's a time frame of experimentation so maybe instead of",
    "start": "1265360",
    "end": "1270400"
  },
  {
    "text": "accidentally I think a lot of times accidentally having access for like two years oh yeah and then you accidentally",
    "start": "1270400",
    "end": "1276480"
  },
  {
    "text": "pull production or something like this like you don't want to do that either so like it helps the users and it helps the",
    "start": "1276480",
    "end": "1283640"
  },
  {
    "text": "the organization to like have a little bit more structure around how some of",
    "start": "1283640",
    "end": "1289080"
  },
  {
    "text": "this more exploratory data uh works and then to Define okay once the experiments",
    "start": "1289080",
    "end": "1295240"
  },
  {
    "text": "are run and we know the approach and maybe we want to move into staging or even production then we essentially",
    "start": "1295240",
    "end": "1301799"
  },
  {
    "text": "decide to give less access to the individual data user and more of a systems level mlops I know near and dear",
    "start": "1301799",
    "end": "1309640"
  },
  {
    "text": "to your heart mlops layer of of access to the data which is great right because",
    "start": "1309640",
    "end": "1315320"
  },
  {
    "text": "then the chance of somebody accidentally pulling production or accidentally exposing users is much less yeah",
    "start": "1315320",
    "end": "1322840"
  },
  {
    "text": "definitely and I think that's that's the most difficult part because usually it's like yeah we can do this because we",
    "start": "1322840",
    "end": "1329120"
  },
  {
    "text": "don't have a s production data we couldn't create synthetic data based on that so um I'm just wondering how in",
    "start": "1329120",
    "end": "1335760"
  },
  {
    "text": "this case um let's say spec let's say specificity of um uh let's say uh of the",
    "start": "1335760",
    "end": "1345000"
  },
  {
    "text": "usage of the specific libraries uh and the tuning of the specific parameters that just mentioned probably we can zoom",
    "start": "1345000",
    "end": "1351039"
  },
  {
    "text": "in a little bit more so I just want okay so imagine we have specific I don't know like the database from this database",
    "start": "1351039",
    "end": "1356400"
  },
  {
    "text": "like we operational database on a specific moment we just scoop specific amount of data we have let's say um a",
    "start": "1356400",
    "end": "1362799"
  },
  {
    "text": "data processing pipeline which has as we guess some embedded um data verification",
    "start": "1362799",
    "end": "1369400"
  },
  {
    "text": "checks and also apply probably like application of differential privacy and",
    "start": "1369400",
    "end": "1374520"
  },
  {
    "text": "then we have let's say the end result so what type of this perameters you can",
    "start": "1374520",
    "end": "1380200"
  },
  {
    "text": "tweak that's what I'm really curious so if you look at the differential privacy what are the main concepts of it and",
    "start": "1380200",
    "end": "1387880"
  },
  {
    "text": "this concept let's say then turned into parameters and what could be uh literally um explained let's say in the",
    "start": "1387880",
    "end": "1395360"
  },
  {
    "text": "simple words because if you go into all the Privacy budget with zepel and how for it could be too much but I would say",
    "start": "1395360",
    "end": "1400880"
  },
  {
    "text": "if I need to explain it to my software colleagues what I'm going to do yeah yeah yeah so um perfect example right",
    "start": "1400880",
    "end": "1408320"
  },
  {
    "text": "and um one of the problems and also beautiful things about differential privacy is now we have actually hundreds",
    "start": "1408320",
    "end": "1414679"
  },
  {
    "text": "of definitions um and those definitions have different parameters and there's",
    "start": "1414679",
    "end": "1420400"
  },
  {
    "text": "some ones that you can use as kind of like hyperparameters to optimize uh different definitions and so it can get",
    "start": "1420400",
    "end": "1426600"
  },
  {
    "text": "quite complicated uh Daman de fontin his work on it and his article series on it",
    "start": "1426600",
    "end": "1432240"
  },
  {
    "text": "he helped me with uh the differential privacy chapters as a technical advisor um and uh helped implement the Google",
    "start": "1432240",
    "end": "1438919"
  },
  {
    "text": "differential privacy stuff and now is at toml analytics place and he and uh a",
    "start": "1438919",
    "end": "1444320"
  },
  {
    "text": "co-researcher whose name now I unfortunately forget uh they did a survey paper and they they parameterized",
    "start": "1444320",
    "end": "1451960"
  },
  {
    "text": "all of the parameters and there was like aund different parameters just to",
    "start": "1451960",
    "end": "1457559"
  },
  {
    "text": "Dimension this data exactly yes yes yes absolutely so but let's Zoom back like",
    "start": "1457559",
    "end": "1463320"
  },
  {
    "text": "let's go to more a simpler definition and then depending on the Privacy",
    "start": "1463320",
    "end": "1469039"
  },
  {
    "text": "expertise or if you if you start hiring privacy engineers at your org then they can go dive into the deep end yeah but",
    "start": "1469039",
    "end": "1475919"
  },
  {
    "text": "from a simple standpoint we're going back to these queries that you're running what you want to understand is",
    "start": "1475919",
    "end": "1482360"
  },
  {
    "text": "what's the probability of learning something before in the first query and then what's the probability of learning",
    "start": "1482360",
    "end": "1488760"
  },
  {
    "text": "something in the second query and you want those probabilities to be very close to each other because the closer",
    "start": "1488760",
    "end": "1495840"
  },
  {
    "text": "they are together the less information that you've essentially given away and the less information you've given away",
    "start": "1495840",
    "end": "1502080"
  },
  {
    "text": "the less of a chance of privacy loss yeah so privacy loss of the individual",
    "start": "1502080",
    "end": "1507240"
  },
  {
    "text": "is Information Gain of the person querying and the these are modeled in",
    "start": "1507240",
    "end": "1512279"
  },
  {
    "text": "like a threat modeling sense like the person quering is like trying to attack and therefore they can be relaxed for",
    "start": "1512279",
    "end": "1518360"
  },
  {
    "text": "different types of use cases if you're releasing data publicly you should probably think about a very high you",
    "start": "1518360",
    "end": "1524559"
  },
  {
    "text": "know high level of security and low level of trust with the people using the data but if it's internally we can think",
    "start": "1524559",
    "end": "1532039"
  },
  {
    "text": "of much we can broaden that and one of those parameters that you can tune is that closeness of those responses do",
    "start": "1532039",
    "end": "1539760"
  },
  {
    "text": "they need to be tightly coupled so we have very high uh high security but also probably a lot of noise right so the",
    "start": "1539760",
    "end": "1547720"
  },
  {
    "text": "noise distribution that we're pulling from is much larger in a sense and therefore we have much higher",
    "start": "1547720",
    "end": "1553320"
  },
  {
    "text": "uncertainty of if the answer that we got the first time or the answer that we got second time if there's if there's any",
    "start": "1553320",
    "end": "1561000"
  },
  {
    "text": "information there um but at some point in time it also gets too noisy right and then we need to uh then we need to",
    "start": "1561000",
    "end": "1568360"
  },
  {
    "text": "determine how to do that and so I think like the higher accuracy that you want means less of a chance of privacy for",
    "start": "1568360",
    "end": "1576000"
  },
  {
    "text": "the individuals um so if you absolutely 100% need the right answer and it's",
    "start": "1576000",
    "end": "1581480"
  },
  {
    "text": "Mission critical that you get the right answer then probably differential privacy is not for your use case but an",
    "start": "1581480",
    "end": "1587960"
  },
  {
    "text": "approximation of the right answer and this is like a also theoretical debate in data is um what is ground truth and",
    "start": "1587960",
    "end": "1597640"
  },
  {
    "text": "do we ever have accurate data and all of these things and so I think there is something to be said of how much do we",
    "start": "1597640",
    "end": "1605440"
  },
  {
    "text": "uh understand the data that we're collecting and is there not already",
    "start": "1605440",
    "end": "1610600"
  },
  {
    "text": "error in the data we collecting and does the insertion of differential privacy",
    "start": "1610600",
    "end": "1616399"
  },
  {
    "text": "related error so errors so we can ensure privacy for the individuals which is",
    "start": "1616399",
    "end": "1621480"
  },
  {
    "text": "essentially what we do as part of this process is we insert error and we can decide what type of error so we can",
    "start": "1621480",
    "end": "1626679"
  },
  {
    "text": "choose the distributions that we insert so we can use gaan um which we would",
    "start": "1626679",
    "end": "1632159"
  },
  {
    "text": "normally expect anyways in data we collect so as data scientists we expect like normally distributed error in most",
    "start": "1632159",
    "end": "1638520"
  },
  {
    "text": "of the data we collect anyways and so inserting some more normally distributed error is probably not the end of the",
    "start": "1638520",
    "end": "1644960"
  },
  {
    "text": "world if as as long as we're doing robust data science but this is um these",
    "start": "1644960",
    "end": "1650200"
  },
  {
    "text": "are like all things to think about from from from a software perspective you need to think about this trade-off",
    "start": "1650200",
    "end": "1656039"
  },
  {
    "text": "between the accuracy of the response or the information the response and the Privacy you can guarantee and the",
    "start": "1656039",
    "end": "1662080"
  },
  {
    "text": "mechanism in and of itself and what you can tune is this tension between those two points so either you're getting a",
    "start": "1662080",
    "end": "1670279"
  },
  {
    "text": "more privacy and also more error and noise and also for the attacker less",
    "start": "1670279",
    "end": "1675679"
  },
  {
    "text": "certainty that they know for sure what what the data is or who's in it or you're getting you know higher",
    "start": "1675679",
    "end": "1683120"
  },
  {
    "text": "accuracy closer to the actual numbers but the chance that somebody learns that somebody got added to the data or that",
    "start": "1683120",
    "end": "1689840"
  },
  {
    "text": "somebody got removed from the data or certain things about the people in the data is much higher so therefore you're",
    "start": "1689840",
    "end": "1695559"
  },
  {
    "text": "dealing with much riskier data science or data release okay so literally let's",
    "start": "1695559",
    "end": "1701640"
  },
  {
    "text": "say in my head you know it's like the slider which go is like how much privacy",
    "start": "1701640",
    "end": "1707120"
  },
  {
    "text": "lost we could afford to get meaningful results at the end versus how more we",
    "start": "1707120",
    "end": "1713880"
  },
  {
    "text": "prefer to lose to get let's say better results so that's let's say the space where we operating absolutely okay okay",
    "start": "1713880",
    "end": "1722320"
  },
  {
    "text": "yeah that's I think it's usually like everything is trade-off in the majority of the cases that's exactly that the",
    "start": "1722320",
    "end": "1728200"
  },
  {
    "text": "tradeoff on which sensitivity of data we need I just want to get a little bit back I think what triggered me when you",
    "start": "1728200",
    "end": "1734399"
  },
  {
    "text": "mentioned that uh this Google approach is really interesting so we're getting back to the story of let's say again",
    "start": "1734399",
    "end": "1741760"
  },
  {
    "text": "data in different environment so usually usually of course you're not allowed like highrisk uh sensitive Pia data to",
    "start": "1741760",
    "end": "1748960"
  },
  {
    "text": "be available in Dev or staging you prefer to keep this data uh in production but what I saw um with my own",
    "start": "1748960",
    "end": "1756120"
  },
  {
    "text": "eyes like two eyes that usually like's only one environment um it's only one environment",
    "start": "1756120",
    "end": "1761360"
  },
  {
    "text": "and let's say then there is um no understanding where the data FS could be",
    "start": "1761360",
    "end": "1766440"
  },
  {
    "text": "put in uh to do their experiments they're not allowed to do their experiments in production uh because",
    "start": "1766440",
    "end": "1771640"
  },
  {
    "text": "production there is no by the way there is no infra to do this of course for all the hungry computations and then um",
    "start": "1771640",
    "end": "1779200"
  },
  {
    "text": "usually it goes like if I can access the data then I find someone who help me to access the data right so we go this way",
    "start": "1779200",
    "end": "1784559"
  },
  {
    "text": "oh probably you can send me this three gigabytes you know as like attachment to the email and imagine if it's piia data",
    "start": "1784559",
    "end": "1791279"
  },
  {
    "text": "good luck with you so again like in this like chaotic situation um what will be a",
    "start": "1791279",
    "end": "1797919"
  },
  {
    "text": "piece of advice it doesn't matter like right now the scale of the company but",
    "start": "1797919",
    "end": "1803039"
  },
  {
    "text": "imagine they don't have anything and they just want to start how you going to approach this",
    "start": "1803039",
    "end": "1809519"
  },
  {
    "text": "yeah yeah absolutely and I think like this goes back to the very basics of data governance and the fact that like",
    "start": "1809519",
    "end": "1817159"
  },
  {
    "text": "if you have a system that cannot support an entire department or team of people",
    "start": "1817159",
    "end": "1823360"
  },
  {
    "text": "um except for going around the data governance rules you haven't done it",
    "start": "1823360",
    "end": "1828799"
  },
  {
    "text": "right and we see this a lot even at scaled companies where data governance",
    "start": "1828799",
    "end": "1834480"
  },
  {
    "text": "comes in and with very strong hard rules of like no no access for data teams no",
    "start": "1834480",
    "end": "1841000"
  },
  {
    "text": "this no that um although that might be a great idea for information security",
    "start": "1841000",
    "end": "1846720"
  },
  {
    "text": "people are clever and people are social and social and clever people will find",
    "start": "1846720",
    "end": "1852760"
  },
  {
    "text": "and socially engineer new ways to access data so just saying no all the time is",
    "start": "1852760",
    "end": "1858720"
  },
  {
    "text": "not going to stop data from being used instead implementing privacy engineering",
    "start": "1858720",
    "end": "1864960"
  },
  {
    "text": "at an organization even if right now you call it infra engineering you put it in the architecture you put it in software",
    "start": "1864960",
    "end": "1871760"
  },
  {
    "text": "wherever you put it the concepts of privacy engineering which is making these Technologies easier to use and",
    "start": "1871760",
    "end": "1878320"
  },
  {
    "text": "more available for the entire organization that stuff has to be built in to avoid these shadow it systems and",
    "start": "1878320",
    "end": "1886320"
  },
  {
    "text": "these workarounds that people find because people need to do their jobs right so just saying no and blocking",
    "start": "1886320",
    "end": "1893080"
  },
  {
    "text": "access to production data or something is not going to stop data scientists from figuring out a way to do that or",
    "start": "1893080",
    "end": "1899159"
  },
  {
    "text": "leaving the orc even worse getting so frustrated that they can't get access to",
    "start": "1899159",
    "end": "1904639"
  },
  {
    "text": "data that they leave the org and then you don't have any data scientists anymore or you don't have data",
    "start": "1904639",
    "end": "1910039"
  },
  {
    "text": "scientists that want to do daily data science right which is also problematic so I think you have to go back to an",
    "start": "1910039",
    "end": "1917720"
  },
  {
    "text": "organizational decision we're going to invest in ways and if you're a small or",
    "start": "1917720",
    "end": "1923440"
  },
  {
    "text": "you can you can make these small ways right this can be a simple interface",
    "start": "1923440",
    "end": "1929080"
  },
  {
    "text": "that uh allows for suiz that allows for tokenization or something like this or",
    "start": "1929080",
    "end": "1934360"
  },
  {
    "text": "an interface that allows for an aggregate dump from production to go through a differential privacy pipeline",
    "start": "1934360",
    "end": "1940159"
  },
  {
    "text": "or something like this doesn't have to you don't have to do everything at once but you have to give people the tools",
    "start": "1940159",
    "end": "1946080"
  },
  {
    "text": "they need to do their job job and the tools they need to do their job safely and and with privacy respecting uh tools",
    "start": "1946080",
    "end": "1954320"
  },
  {
    "text": "so yeah I do I do believe as well that it's like from the day one it should be built in if it's there then it's easier",
    "start": "1954320",
    "end": "1962240"
  },
  {
    "text": "to expand it later but there is nothing here uh if there are even no categorization of the data like",
    "start": "1962240",
    "end": "1968919"
  },
  {
    "text": "regarding what the sensitivity of specific data and then a basic let's say the the basic let's say the way how the",
    "start": "1968919",
    "end": "1975880"
  },
  {
    "text": "users can access the data without specific like role if you have just like you know oh we give everyone root access",
    "start": "1975880",
    "end": "1982000"
  },
  {
    "text": "so good luck with this later for the use cases yeah that's that's that's really I",
    "start": "1982000",
    "end": "1987960"
  },
  {
    "text": "think the hard hard thing I also wondering if I look back for example",
    "start": "1987960",
    "end": "1993240"
  },
  {
    "text": "pseudonymization usually the vector of of attack is usually let's say this like",
    "start": "1993240",
    "end": "1998440"
  },
  {
    "text": "linkage between pseudonymized data and let's say the original data or it's like some some dictionary or any other thing",
    "start": "1998440",
    "end": "2006519"
  },
  {
    "text": "so that's usually what a Tracker try to to get access to what will be the vector of attack in case of differential",
    "start": "2006519",
    "end": "2013840"
  },
  {
    "text": "privacy yeah I mean I think uh so differential privacy you tend to release",
    "start": "2013840",
    "end": "2019639"
  },
  {
    "text": "Aggregates uh things like histograms things like count sketches and other things like this but you could also",
    "start": "2019639",
    "end": "2026480"
  },
  {
    "text": "release a result right so you can release an average or or something like this um interestingly enough some of the",
    "start": "2026480",
    "end": "2033240"
  },
  {
    "text": "core differential privacy attacks that have been proven are basically implementation errors oh wow so um",
    "start": "2033240",
    "end": "2041320"
  },
  {
    "text": "Damien the same researcher who who's done a bunch of work on differential privacy uh we'll be presenting at bside",
    "start": "2041320",
    "end": "2047440"
  },
  {
    "text": "Zur this year some work that the tomal team did on leveraging floating Point",
    "start": "2047440",
    "end": "2053520"
  },
  {
    "text": "attacks um against differential privacy systems we can think about this like when we're in flo floating Point um and",
    "start": "2053520",
    "end": "2061240"
  },
  {
    "text": "and we're already like then that's already an abstraction on how computers uh store data right there no real",
    "start": "2061240",
    "end": "2067919"
  },
  {
    "text": "floating Point data type at the the core processing level and so we're dealing",
    "start": "2067919",
    "end": "2073760"
  },
  {
    "text": "with some sort of abstraction and what has been proven before and what they further proved is um depending on how",
    "start": "2073760",
    "end": "2081440"
  },
  {
    "text": "the sensitivity of how you're sampling this noise um the computer can sometimes",
    "start": "2081440",
    "end": "2086720"
  },
  {
    "text": "not be truly pseudo random these are some of the same problems we have in cryptography and so forth as well is",
    "start": "2086720",
    "end": "2093040"
  },
  {
    "text": "like with computers we don't actually often have a true source of r Randomness um at least from a mathematical",
    "start": "2093040",
    "end": "2099599"
  },
  {
    "text": "perspective and so um let's say that you're an attacker of a differential privacy system if it's quite obvious um",
    "start": "2099599",
    "end": "2108640"
  },
  {
    "text": "that you're you're running a query that has a particular distribution and the sampling of that distribution is",
    "start": "2108640",
    "end": "2114520"
  },
  {
    "text": "predictable you can start to reverse engineer we can think of this like from basan or probabilistic thinking you can",
    "start": "2114520",
    "end": "2121520"
  },
  {
    "text": "start to reverse engineer these processes and you could potentially even start to guess how much noise was added",
    "start": "2121520",
    "end": "2129119"
  },
  {
    "text": "um obviously if you can guess how much noise was added with any certainty you immediately remove all of the Privacy",
    "start": "2129119",
    "end": "2136000"
  },
  {
    "text": "guarantees because the Privacy guarantees come completely from the fact that nobody can Pro prob ballistically",
    "start": "2136000",
    "end": "2143800"
  },
  {
    "text": "infer how much noise was added and so the whole cars fall apart if um if you",
    "start": "2143800",
    "end": "2150280"
  },
  {
    "text": "could do that but it's more interestingly enough it's less from like data analysis and it's more from like computers and systems analysis that um",
    "start": "2150280",
    "end": "2157920"
  },
  {
    "text": "have been most of the attack factors against differential privacy yeah so now I guess I get it why there is no such a",
    "start": "2157920",
    "end": "2165160"
  },
  {
    "text": "thing this like 100 like percentage guarantee of anonymization so with all",
    "start": "2165160",
    "end": "2171000"
  },
  {
    "text": "being said like we tried to walk through let's say different chapters a little bit touch different things so I want to",
    "start": "2171000",
    "end": "2179280"
  },
  {
    "text": "go to summarize all of this so could you briefly walk me through let's say what",
    "start": "2179280",
    "end": "2185119"
  },
  {
    "text": "is the more or less content of the chapters like how many chapters the book has with the content and this is like",
    "start": "2185119",
    "end": "2190800"
  },
  {
    "text": "Hands-On privacy implementation book so just highlight this and I think that",
    "start": "2190800",
    "end": "2195960"
  },
  {
    "text": "will conclude our session for today yeah thank you yeah so um as you're familiar",
    "start": "2195960",
    "end": "2201319"
  },
  {
    "text": "with already and as uh hopefully viewers will will be as they read the book is um",
    "start": "2201319",
    "end": "2206920"
  },
  {
    "text": "the chapters are structured so there's Theory first and then there's handson stuff in the later half of the chapter",
    "start": "2206920",
    "end": "2213200"
  },
  {
    "text": "and this is because I'm a practitioner like yourself like I think it's really important to not",
    "start": "2213200",
    "end": "2218760"
  },
  {
    "text": "just teach people how something works but how they can use it you know okay",
    "start": "2218760",
    "end": "2223839"
  },
  {
    "text": "that sounds great but like what am I gonna type when I sit down like how am I supposed to do this and so each chapter",
    "start": "2223839",
    "end": "2230960"
  },
  {
    "text": "has open source libraries um so we start with data governance and we talk a little bit about pseudonymization and so",
    "start": "2230960",
    "end": "2237280"
  },
  {
    "text": "forth and then move on to differential privacy you get to play around with the toml analytics Library there's also some",
    "start": "2237280",
    "end": "2244040"
  },
  {
    "text": "like implementing differential privacy by scratch which by the way you should never do unless you're remember don't do",
    "start": "2244040",
    "end": "2249800"
  },
  {
    "text": "that do I I teach it to folks so they can play with it right because it's important to like play with these",
    "start": "2249800",
    "end": "2255680"
  },
  {
    "text": "mechanisms and kind of reverse engineer how they work so you can reason about them um then it moves into Data",
    "start": "2255680",
    "end": "2262359"
  },
  {
    "text": "pipelines data engineering so how do you put privacy Technologies into Data pipeline work and data engineering work",
    "start": "2262359",
    "end": "2269720"
  },
  {
    "text": "uh then it moves on to attacking privacy systems so how can we like because we",
    "start": "2269720",
    "end": "2274920"
  },
  {
    "text": "need to know how to attack stuff if we want to know how to protect it to protect um and then we move into",
    "start": "2274920",
    "end": "2280640"
  },
  {
    "text": "Federated learning Federated and distributed data analysis which to talk to today but yeah then you use flower",
    "start": "2280640",
    "end": "2287119"
  },
  {
    "text": "which I love flower to do this um and then we go into encrypted computation",
    "start": "2287119",
    "end": "2292960"
  },
  {
    "text": "and this is just Computing only on encrypted data without decrypting it um encrypted machine learning encrypted",
    "start": "2292960",
    "end": "2299280"
  },
  {
    "text": "data processing and how do how does one do that and then I sort of go into The Human Side so how do you read policies",
    "start": "2299280",
    "end": "2306280"
  },
  {
    "text": "how you work with lawyers all of these things and and then some FAQs and some use cases so it's a pretty fun uh wild",
    "start": "2306280",
    "end": "2314319"
  },
  {
    "text": "ride I hope I do hope open for feedback if there's open questions um and yeah",
    "start": "2314319",
    "end": "2321240"
  },
  {
    "text": "it's been really exciting to chat with you and thank you so much for for being my host with the",
    "start": "2321240",
    "end": "2328640"
  },
  {
    "text": "most yeah thanks a lot and I guess the folks can um get your book because it's",
    "start": "2328640",
    "end": "2333760"
  },
  {
    "text": "right now available so you can find it a digital C your hard copy whatever you prefer thanks a lot katri thank you so",
    "start": "2333760",
    "end": "2340680"
  },
  {
    "text": "much thanks for listening to this episode of the goto podcast head over to cop. Tech to discover lots more content",
    "start": "2340680",
    "end": "2348000"
  },
  {
    "text": "from the brightest minds and software [Music]",
    "start": "2348000",
    "end": "2361079"
  },
  {
    "text": "development",
    "start": "2361079",
    "end": "2364079"
  }
]