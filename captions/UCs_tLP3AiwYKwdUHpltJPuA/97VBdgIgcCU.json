[
  {
    "start": "0",
    "end": "133000"
  },
  {
    "text": "so I'm going to talk about running Netflix on Cassandra and the cloud but I'm going to start off by just",
    "start": "359",
    "end": "5440"
  },
  {
    "text": "mentioning something for the locals which is that uh Netflix announced a few",
    "start": "5440",
    "end": "10559"
  },
  {
    "text": "months ago that we're coming to the Nordic countries in Q4 and it's October",
    "start": "10559",
    "end": "16640"
  },
  {
    "text": "now so it's Q4 so at some point in the relatively short future you'll be able",
    "start": "16640",
    "end": "23000"
  },
  {
    "text": "to get Netflix and right now if you go there they just want to collect your email address but I do want to highlight",
    "start": "23000",
    "end": "29119"
  },
  {
    "text": "this quite nicely little Point down here is if you look down here it says EU",
    "start": "29119",
    "end": "35320"
  },
  {
    "text": "west1 DF 69b e09 C DK that's that's an",
    "start": "35320",
    "end": "41960"
  },
  {
    "text": "instance Amazon instance running in Ireland uh and it's it's figured out",
    "start": "41960",
    "end": "48239"
  },
  {
    "text": "we're in Denmark so anyway that's um just thought I'd highlight that and hopefully there are local",
    "start": "48239",
    "end": "55800"
  },
  {
    "text": "people who will sign up for this thing um so yesterday I gave a talk and I went",
    "start": "55800",
    "end": "62079"
  },
  {
    "text": "through a bunch of the stuff about Netflix and cloud and whatever and I assumed that some of you would have been",
    "start": "62079",
    "end": "67640"
  },
  {
    "text": "in that talk as well so I'm not going to go over that much in great detail talk was videoed you'll be able to find the",
    "start": "67640",
    "end": "73720"
  },
  {
    "text": "slides anyway um I actually have a t-shirt that says blah blah Cloud blah on it but today I decided to be chaos",
    "start": "73720",
    "end": "81079"
  },
  {
    "text": "monkey um lots and lots of slides at slideshare.net Netflix this is one of the slides from",
    "start": "81079",
    "end": "88040"
  },
  {
    "text": "yesterday just to give a bit of context for people that weren't here uh yesterday we have over the last 3 years",
    "start": "88040",
    "end": "95880"
  },
  {
    "text": "basically moved all the different pieces of Netflix to the cloud and all of the blue boxes on here are basically backed",
    "start": "95880",
    "end": "104040"
  },
  {
    "text": "by Cassandra yeah all of those there's like a purple one that's my sequel that",
    "start": "104040",
    "end": "109200"
  },
  {
    "text": "one is go in the process of being moved to Cassandra and there actually there's a",
    "start": "109200",
    "end": "114840"
  },
  {
    "text": "bit of content management and some of the stuff on that First Column should actually be Cassandra as well well",
    "start": "114840",
    "end": "120960"
  },
  {
    "text": "nowadays so I'm going to talk about Cassandra running on AWS um which for us is a highly",
    "start": "120960",
    "end": "128319"
  },
  {
    "text": "available and durable deployment pattern which looks something like this what we have",
    "start": "128319",
    "end": "135319"
  },
  {
    "start": "133000",
    "end": "257000"
  },
  {
    "text": "here is all of the client services that want to look up this particular piece of",
    "start": "135319",
    "end": "141560"
  },
  {
    "text": "information uh they all make rest calls to a service tier with a you know",
    "start": "141560",
    "end": "147040"
  },
  {
    "text": "standard rest interface that then makes is the only thing that talks to our Cassandra cluster I think the one that I",
    "start": "147040",
    "end": "154480"
  },
  {
    "text": "grabbed here is I think there's 24 nodes here and probably about 50 here and then some of these there are like f some of",
    "start": "154480",
    "end": "161360"
  },
  {
    "text": "these little boxes have 500 machines so these are distinct services not distinct",
    "start": "161360",
    "end": "166800"
  },
  {
    "text": "machines right so we have I don't know 300 different services and some of them",
    "start": "166800",
    "end": "172000"
  },
  {
    "text": "are backed um this middle tier rest service also synchronizes with the data",
    "start": "172000",
    "end": "178319"
  },
  {
    "text": "center to post things back",
    "start": "178319",
    "end": "181840"
  },
  {
    "text": "and an engineer built that service and if it ever goes red and has bad response times we know exactly who to call and",
    "start": "240000",
    "end": "247360"
  },
  {
    "text": "this Cassandra cluster serves only that one purpose right so we've partitioned everything out so what that means is",
    "start": "247360",
    "end": "254159"
  },
  {
    "text": "that in production we actually have you know lots of these things there's more than 50 distinct Cassandra clusters um",
    "start": "254159",
    "end": "262520"
  },
  {
    "start": "257000",
    "end": "312000"
  },
  {
    "text": "they add up to more than 500 Cassandra nodes of various various sizes the",
    "start": "262520",
    "end": "267880"
  },
  {
    "text": "smallest cluster was we do a six modes uh the biggest we have is 72 our daily backup volume and that's",
    "start": "267880",
    "end": "275639"
  },
  {
    "text": "compressed backups is about 30 terabytes so that's what we're backing up on a daily basis um and one of these clusters",
    "start": "275639",
    "end": "285160"
  },
  {
    "text": "is doing over 250,000 rights per second um and it's actually mostly logging data",
    "start": "285160",
    "end": "291199"
  },
  {
    "text": "we have quite a few high traffic clusters and we have another cluster that has about 10 terabytes of data in",
    "start": "291199",
    "end": "296800"
  },
  {
    "text": "it so we have one very big cluster one very high throughput cluster and lots of stuff somewhere in between so that's",
    "start": "296800",
    "end": "304160"
  },
  {
    "text": "kind of where We've Ended up so the talk a bit about the rest of the architecture",
    "start": "304160",
    "end": "311560"
  },
  {
    "text": "here we get high availability by storing three copies of the data on the local",
    "start": "311560",
    "end": "317560"
  },
  {
    "start": "312000",
    "end": "392000"
  },
  {
    "text": "instances on Cassandra we use the local internal discs that are ephemeral that",
    "start": "317560",
    "end": "322919"
  },
  {
    "text": "go away if the machine dies okay so we have three copies of that and those three copies are in different data",
    "start": "322919",
    "end": "329160"
  },
  {
    "text": "centers and different buildings in fact the rights go to RAM and then it acts",
    "start": "329160",
    "end": "334199"
  },
  {
    "text": "back and says that's fine you're done you can get a fast right by saying",
    "start": "334199",
    "end": "340479"
  },
  {
    "text": "let me know when one of the clients when one of the Cassandra nodes is completed you can get a a durable right by doing a",
    "start": "340479",
    "end": "348039"
  },
  {
    "text": "two out of three Quorum right and a two out of three Quorum read after if you want to get read after WR",
    "start": "348039",
    "end": "354440"
  },
  {
    "text": "consistency these availability zones are separate building separate power they're",
    "start": "354440",
    "end": "360160"
  },
  {
    "text": "I'm not sure how far apart but they're a few miles apart and they are about a millisecond apart which means that if",
    "start": "360160",
    "end": "366360"
  },
  {
    "text": "you want to get a fast request you call you talk to the Cassandra node that's in",
    "start": "366360",
    "end": "371599"
  },
  {
    "text": "the same building as you because the clients are also spread across these three zones right so think of three",
    "start": "371599",
    "end": "377240"
  },
  {
    "text": "different buildings if you're in one building there is a Cassandra cluster node in your building that has a copy of",
    "start": "377240",
    "end": "383960"
  },
  {
    "text": "your data if you want consistency you have to talk to one of the other buildings as well to make sure everyone",
    "start": "383960",
    "end": "389240"
  },
  {
    "text": "has the right data so we triple replicate we take our little map of all the services talking",
    "start": "389240",
    "end": "396080"
  },
  {
    "start": "392000",
    "end": "483000"
  },
  {
    "text": "to each other and we have three copies of it in three different buildings and then we have Cassandra backend replicas",
    "start": "396080",
    "end": "403080"
  },
  {
    "text": "and Cassandra is doing the side to side replication that basically keeps everything in sync so if you write some",
    "start": "403080",
    "end": "409319"
  },
  {
    "text": "data it gets copied sideways and then the load balancers send the traffic into the",
    "start": "409319",
    "end": "416319"
  },
  {
    "text": "Top If we're doing maintenance on Cassandra which we do for upgrading the version of Cassandra or doing uh repairs",
    "start": "418400",
    "end": "426360"
  },
  {
    "text": "or or or compaction sometimes depending on on what we'll talk L later about why",
    "start": "426360",
    "end": "432000"
  },
  {
    "text": "we do that um we take a particular Casandra instance out of service now",
    "start": "432000",
    "end": "438039"
  },
  {
    "text": "that means that I only have two copies of that particular data I mean there may be this is a part of the data for this",
    "start": "438039",
    "end": "443759"
  },
  {
    "text": "tier um one Cassandra instance comes out of service what I do is stop replicating",
    "start": "443759",
    "end": "450319"
  },
  {
    "text": "to it and then all the machines that are in this clust in this uh availability",
    "start": "450319",
    "end": "456080"
  },
  {
    "text": "Zone talk to Cassandra in the other zones so that that's normal operation we're doing that continuously we have",
    "start": "456080",
    "end": "462440"
  },
  {
    "text": "Jenkins jobs that continuously walk around our Cassandra clusters running",
    "start": "462440",
    "end": "467800"
  },
  {
    "text": "repairs and uh compactions and doing software updates for new versions of",
    "start": "467800",
    "end": "473720"
  },
  {
    "text": "Cassandra so that's rolling continuously so the individual nodes I'm",
    "start": "473720",
    "end": "479199"
  },
  {
    "text": "going to look at how they're operate how we set them up so we have a base Ami it's currently Centos we're gradually",
    "start": "479199",
    "end": "485639"
  },
  {
    "start": "483000",
    "end": "577000"
  },
  {
    "text": "moving towards Ubuntu but we've been spending most of that this year doing that we haven't got there yet it's",
    "start": "485639",
    "end": "490919"
  },
  {
    "text": "fairly low priority job because Centos basically Works uh we run mixture of java 6 and",
    "start": "490919",
    "end": "497360"
  },
  {
    "text": "Java 7 the Cassandra clusters mostly run seven now um within the main",
    "start": "497360",
    "end": "503680"
  },
  {
    "text": "jvm we have Cassandra 1.09 currently we're just moving to 1.1 there's an app",
    "start": "503680",
    "end": "510400"
  },
  {
    "text": "agent monitoring it which does instruments the Java bite codes here and we have some garbage and thread dump",
    "start": "510400",
    "end": "516518"
  },
  {
    "text": "logging on the side here outside the main Java virtual machine we have",
    "start": "516519",
    "end": "521760"
  },
  {
    "text": "monitoring log rotation a machine agent that's telling us how busy Linux is",
    "start": "521760",
    "end": "527000"
  },
  {
    "text": "things like that and we have this service here called prium which is our Cassandra management process that's",
    "start": "527000",
    "end": "534080"
  },
  {
    "text": "running in a tomcat server which is our standard for our platform every instance we have based basically almost every",
    "start": "534080",
    "end": "541079"
  },
  {
    "text": "instance runs a tomcat server normally we deploy code into tomcat and that would be the main jvm on the machine",
    "start": "541079",
    "end": "547760"
  },
  {
    "text": "consuming most of the RAM for our Cassandra nodes it's a little different because we run Cassandra in a separate",
    "start": "547760",
    "end": "553200"
  },
  {
    "text": "jvm that consumes most of the Ram or well actually we let's say this is a 64",
    "start": "553200",
    "end": "559519"
  },
  {
    "text": "68 gab machine we run Cassandra in about six or 7 GB Heap prium and these other",
    "start": "559519",
    "end": "565800"
  },
  {
    "text": "things are probably using a gigabyte and the rest of memory is file system so that's that's roughly how we",
    "start": "565800",
    "end": "571800"
  },
  {
    "text": "configure it so PR is a available on GitHub this is the the secret Source or",
    "start": "571800",
    "end": "579240"
  },
  {
    "start": "577000",
    "end": "817000"
  },
  {
    "text": "not secret I guess public Source um which we now have for doing zero touch",
    "start": "579240",
    "end": "584440"
  },
  {
    "text": "order configuration of our Cassandra cluster so for me to provision a Cassandra cluster I just have to say",
    "start": "584440",
    "end": "592120"
  },
  {
    "text": "please make 24 nodes those 24 nodes will self-organize into a 24 node Cassandra a",
    "start": "592120",
    "end": "599720"
  },
  {
    "text": "cluster and from me deciding to do it to have a running cluster that I can write to takes about 5 minutes five minutes",
    "start": "599720",
    "end": "608920"
  },
  {
    "text": "okay and three of those minutes is uh Amazon creating the instances and booting them up and then there's like a",
    "start": "608920",
    "end": "615120"
  },
  {
    "text": "minute or two of of PR starting you got to talk tomcat and then it's got to decide what to do configure things and",
    "start": "615120",
    "end": "621120"
  },
  {
    "text": "start uh the Cassandra noes themselves so what that's doing is uh",
    "start": "621120",
    "end": "628040"
  },
  {
    "text": "figuring out what the Cassandra configuration files should have in them",
    "start": "628040",
    "end": "633320"
  },
  {
    "text": "um figuring out is this new I'm a new Cassandra node I just came into existence am I joining an existing",
    "start": "633320",
    "end": "640240"
  },
  {
    "text": "cluster should I be repairing that cluster you know because I pre I'm rep",
    "start": "640240",
    "end": "646120"
  },
  {
    "text": "replacing a broken node that's one case um if I'm a brand new cluster I set up",
    "start": "646120",
    "end": "651600"
  },
  {
    "text": "as an empty system or I can actually also do a restore from backup and you",
    "start": "651600",
    "end": "657000"
  },
  {
    "text": "just tell it what time you want to restore from and it pulls all data up to that point in time so it's doing token allocation",
    "start": "657000",
    "end": "665440"
  },
  {
    "text": "assignment I've talked about this um so the other thing that PR does is",
    "start": "665440",
    "end": "671399"
  },
  {
    "text": "continuously archive into S3 all the data that's written into Cassandra is",
    "start": "671399",
    "end": "676680"
  },
  {
    "text": "logged to disk as single as large immutable files whenever one of those files is written we copy that data into",
    "start": "676680",
    "end": "684320"
  },
  {
    "text": "S3 and we compress it as it goes and then we can restore that into from from",
    "start": "684320",
    "end": "689560"
  },
  {
    "text": "S3 the other finally you can also grow and shrink the ring you can tell it to double or half currently and I'll talk",
    "start": "689560",
    "end": "696519"
  },
  {
    "text": "more about that later and those are all functions which are implemented with PRI PR also has",
    "start": "696519",
    "end": "703000"
  },
  {
    "text": "some monitoring uh interfaces think of this as a rest based user interface to",
    "start": "703000",
    "end": "709720"
  },
  {
    "text": "Cassandra on a per node basis we have a client library that",
    "start": "709720",
    "end": "715279"
  },
  {
    "text": "we've also written is also open source called ASX all the stuff's on GitHub um this is",
    "start": "715279",
    "end": "722320"
  },
  {
    "text": "an improvement of the the previous uh Java client Library which is called Hector um obviously we're all very",
    "start": "722320",
    "end": "729440"
  },
  {
    "text": "familiar with Greek mythology it's part now of the interview questions at Netflix um so Cassandra was a Greek a",
    "start": "729440",
    "end": "736639"
  },
  {
    "text": "woman in Greek mythology uh Hector was her boyfriend and asona was the son of",
    "start": "736639",
    "end": "741920"
  },
  {
    "text": "Hector but not actually Cassandra's son I think I'm not sure um and later on",
    "start": "741920",
    "end": "747800"
  },
  {
    "text": "we'll run into some more Greek thought Greek uh characters so asteronx gives",
    "start": "747800",
    "end": "754120"
  },
  {
    "text": "has a much better abstraction and is for the RPC protocol handling and the thread",
    "start": "754120",
    "end": "759680"
  },
  {
    "text": "handling uh has a nice fluent style API I'll show you an example on the next slide um it has a much better retry and",
    "start": "759680",
    "end": "766600"
  },
  {
    "text": "back off semantics and it's also token aware and explain what that means with asex comes a bunch of useful recipes",
    "start": "766600",
    "end": "773839"
  },
  {
    "text": "because our developers keep doing stuff and keep asking the same questions so we've built standard recipes for things",
    "start": "773839",
    "end": "780600"
  },
  {
    "text": "like distributed roock which you can do with zookeeper but we figured out how to do it without multi- dat Center rooll",
    "start": "780600",
    "end": "786920"
  },
  {
    "text": "lock uniqueness multi constraints and then this large file storage if you want to store like 10 gigabyte chunks into",
    "start": "786920",
    "end": "794079"
  },
  {
    "text": "Cassandra which yeah seems like a reasonable thing to do I guess then the",
    "start": "794079",
    "end": "799279"
  },
  {
    "text": "problem is that if it fails near the end of writing a 10 gigabyte chunk you have to write the whole chunk again so what",
    "start": "799279",
    "end": "804880"
  },
  {
    "text": "we have is a chunked and threaded thing which breaks your large right into lots of little right and they can all be individually retried",
    "start": "804880",
    "end": "811800"
  },
  {
    "text": "and that's a standard feature of the ASX client this is what a query example",
    "start": "811800",
    "end": "817639"
  },
  {
    "start": "817000",
    "end": "855000"
  },
  {
    "text": "looks like um we're not using any SQL like language where you're sitting there cutting and pasting strings together to",
    "start": "817639",
    "end": "824160"
  },
  {
    "text": "create a query we just create it directly um you can see it say fluent style thing do thing do thing do thing",
    "start": "824160",
    "end": "832040"
  },
  {
    "text": "um so this basically says give my you know here's my query is preparing the",
    "start": "832040",
    "end": "837160"
  },
  {
    "text": "query get the key I want to paginate it it um you set it all up and then you just see it get the result zip through",
    "start": "837160",
    "end": "844680"
  },
  {
    "text": "you're done right it's nice easy Java level uh",
    "start": "844680",
    "end": "850120"
  },
  {
    "text": "API um so let's look at what TR Cassandra right flows look",
    "start": "850120",
    "end": "855279"
  },
  {
    "start": "855000",
    "end": "939000"
  },
  {
    "text": "like we've got um the clients in the middle they pick a cassand this is a six",
    "start": "855279",
    "end": "861399"
  },
  {
    "text": "node Cassandra cluster with triple replication so I've got two red item two",
    "start": "861399",
    "end": "867519"
  },
  {
    "text": "red nodes in zone B two yellow nodes in zone a and two blue nodes in zone C okay",
    "start": "867519",
    "end": "873160"
  },
  {
    "text": "and the clients from whichever Zone they're in pick a node at random send it a request there's a fairly small chance",
    "start": "873160",
    "end": "880720"
  },
  {
    "text": "that the machine well in fact in this case there's a 50% chance that that node is actually contains the data you asked",
    "start": "880720",
    "end": "887360"
  },
  {
    "text": "for but the client has no idea whereas the cluster gets bigger the pro probability you hit the right node",
    "start": "887360",
    "end": "893720"
  },
  {
    "text": "reduces um that node acts as a coordinator uh figures out where the data lives talks to the other three",
    "start": "893720",
    "end": "900240"
  },
  {
    "text": "nodes writes the data to all of them and you can see the sequence I sort of walked through here the key thing is",
    "start": "900240",
    "end": "906360"
  },
  {
    "text": "that you get an act as soon as you're in memory on the other nodes um and about 10 seconds later it gets flushed to dis",
    "start": "906360",
    "end": "913959"
  },
  {
    "text": "and you could say well that's not fully durable but I have three different buildings that I have data in memory and",
    "start": "913959",
    "end": "920040"
  },
  {
    "text": "if I lose a building I still have two more copies in memory it's quite hard to take out three buildings within a 10-second period without having a",
    "start": "920040",
    "end": "926759"
  },
  {
    "text": "problem that's big enough that you no longer care about whether Cassandra is running or not you know if you drop an atom bomb on the east side of the US",
    "start": "926759",
    "end": "933800"
  },
  {
    "text": "last thing you care about is whether Netflix is still running right so what asex does is that the",
    "start": "933800",
    "end": "942319"
  },
  {
    "start": "939000",
    "end": "999000"
  },
  {
    "text": "client goes and pulls a describe ring every now and again and knows what the token ranges for each client are and",
    "start": "942319",
    "end": "949440"
  },
  {
    "text": "what it can do is say well this data I know is in the red node here and it's not in this red node here so I'm going",
    "start": "949440",
    "end": "956319"
  },
  {
    "text": "to go directly to that one and then that will also coordinate the other two copies but what it means is that if",
    "start": "956319",
    "end": "962199"
  },
  {
    "text": "you're trying to do a read one or a write one which is the fastest request",
    "start": "962199",
    "end": "967279"
  },
  {
    "text": "type in this case I actually have two hops to get to my data there's two",
    "start": "967279",
    "end": "972920"
  },
  {
    "text": "Network hops which you know every time you go across the network there's delay so it means that there's less latency",
    "start": "972920",
    "end": "979440"
  },
  {
    "text": "and there's less that can go wrong every time you go across the network it could also fail so by having a single Network",
    "start": "979440",
    "end": "984759"
  },
  {
    "text": "operation we reduce our latency so all of this gives us an improve movement over the previous Hector library that",
    "start": "984759",
    "end": "991560"
  },
  {
    "text": "gives us more resilience and lower latency now we run Cassandra in multi",
    "start": "991560",
    "end": "997120"
  },
  {
    "text": "region mode and in that case we have a cluster in Europe and a cluster in the",
    "start": "997120",
    "end": "1003040"
  },
  {
    "start": "999000",
    "end": "1174000"
  },
  {
    "text": "US so this is running in Amazon Ireland this is running in Amazon East Coast so",
    "start": "1003040",
    "end": "1008319"
  },
  {
    "text": "here's a US client writing some data this is the previous diagram but the coordinator this time knows it's got a",
    "start": "1008319",
    "end": "1014680"
  },
  {
    "text": "bunch of buddies in Europe so it sends one copy of the data to Europe that coordinator in Europe makes the other",
    "start": "1014680",
    "end": "1020560"
  },
  {
    "text": "two copies and then those two nodes act back to this coordinator and you finally get a act the actually the find the ACT",
    "start": "1020560",
    "end": "1027880"
  },
  {
    "text": "happens early for these clients it happens as soon as it's consistent locally the remote coordinator holds a",
    "start": "1027880",
    "end": "1035558"
  },
  {
    "text": "copy of the data until it's heard from the remote cluster so this this copy is not delaying the right but it's making",
    "start": "1035559",
    "end": "1043319"
  },
  {
    "text": "it resilient to downtime so it'll it's is called hinted handoff and it basically sits there and says until I've",
    "start": "1043319",
    "end": "1049440"
  },
  {
    "text": "heard that you've got the data I'm going to keep a copy of it and once I hear from everyone they've got it I can drop",
    "start": "1049440",
    "end": "1054600"
  },
  {
    "text": "that so what that means is the overhead of having another remote cluster is a little bit of extra memory thread",
    "start": "1054600",
    "end": "1061200"
  },
  {
    "text": "handling and traffic here but there's no latency overhead to the local cluster",
    "start": "1061200",
    "end": "1066280"
  },
  {
    "text": "and that applies symmetrically so if you sign up in Europe in a few weeks time whenever it is that we have Denmark",
    "start": "1066280",
    "end": "1071840"
  },
  {
    "text": "support you will enter your data we will log you as a local customer here your data will then move back to the US and",
    "start": "1071840",
    "end": "1079080"
  },
  {
    "text": "then if you happen to visit the US one second later for example um because you",
    "start": "1079080",
    "end": "1084600"
  },
  {
    "text": "have a tight very fast aircraft or something um you'll find you're a customer in the US we we recognize you",
    "start": "1084600",
    "end": "1091320"
  },
  {
    "text": "you're a full every everyone every member of Netflix is a global member of Netflix it doesn't matter where you sign",
    "start": "1091320",
    "end": "1096679"
  },
  {
    "text": "up you get local content for whatever content we've we've licensed in the US or the UK or or whatever depends what",
    "start": "1096679",
    "end": "1103240"
  },
  {
    "text": "country you're in but it is a global membership model when we were booting this up",
    "start": "1103240",
    "end": "1109600"
  },
  {
    "text": "we had the US region running with Cassandra and with no downtime at all we",
    "start": "1109600",
    "end": "1115200"
  },
  {
    "text": "set up the European cluster and linked everything together so to get there we",
    "start": "1115200",
    "end": "1120600"
  },
  {
    "text": "actually took backups and copied the backups to Europe and did a restore here so this was mostly up to date but a",
    "start": "1120600",
    "end": "1126400"
  },
  {
    "text": "little bit running behind then we did a global repair to get everything back in sync uh but that what that meant was we",
    "start": "1126400",
    "end": "1133280"
  },
  {
    "text": "were it was a little faster and it put a little less traffic on the US to go from a backup um but we we could if it's a",
    "start": "1133280",
    "end": "1139799"
  },
  {
    "text": "small cluster with a small amount of data in it we just do a sync this particular thing was for clusters that",
    "start": "1139799",
    "end": "1145400"
  },
  {
    "text": "had a lot of data we actually used the backup to try and just accelerate the thing so let's talk a bit more about the",
    "start": "1145400",
    "end": "1151799"
  },
  {
    "text": "backups oh I like my analogy here by the way this is like taking a 737 on a domestic flight you know and while it's",
    "start": "1151799",
    "end": "1158240"
  },
  {
    "text": "flying strapping on some more engines of bigger fuel tanks and flying it to Europe okay without Landing it and it",
    "start": "1158240",
    "end": "1164840"
  },
  {
    "text": "was that worked out pretty well we've done this for we have approximately 10 or 15 clusters that are globally uh run",
    "start": "1164840",
    "end": "1171440"
  },
  {
    "text": "run in a global mode okay for backups we have",
    "start": "1171440",
    "end": "1177360"
  },
  {
    "start": "1174000",
    "end": "1248000"
  },
  {
    "text": "um let's see these are all the nodes what they do is they have a they're",
    "start": "1177360",
    "end": "1182600"
  },
  {
    "text": "configured with an Amazon S3 bucket and whenever they write to disk they write an immutable SS table file which was",
    "start": "1182600",
    "end": "1189960"
  },
  {
    "text": "described in the previous talk and that SS table is compressed using Snappy",
    "start": "1189960",
    "end": "1195080"
  },
  {
    "text": "compression which is nice low overhead method gives you reasonable amount of compression and is copied to",
    "start": "1195080",
    "end": "1200880"
  },
  {
    "text": "S3 uh that happens incrementally as everything writes and we do a daily full",
    "start": "1200880",
    "end": "1206039"
  },
  {
    "text": "dump where we clear out the incrementals afterwards but we've got then a full snapshot so we have a daily fall and an",
    "start": "1206039",
    "end": "1211919"
  },
  {
    "text": "incremental every time you write an SS table once a day we also take a copy of",
    "start": "1211919",
    "end": "1217559"
  },
  {
    "text": "everything that's there and copy it to the other side of the country in a different region with a different Amazon",
    "start": "1217559",
    "end": "1223360"
  },
  {
    "text": "account and uh we encrypt it and all that kind of stuff because you get paranoid and this is you know business",
    "start": "1223360",
    "end": "1230799"
  },
  {
    "text": "protection basically this is the master copy of Netflix's customer database I don't want",
    "start": "1230799",
    "end": "1237760"
  },
  {
    "text": "to lose it okay and I probably don't want you to steal it um so we have a lot",
    "start": "1237760",
    "end": "1244679"
  },
  {
    "text": "of these clusters um we're going to be open sourcing this soon but we built a",
    "start": "1244679",
    "end": "1249720"
  },
  {
    "start": "1248000",
    "end": "1325000"
  },
  {
    "text": "Cassandra Explorer and you can see this is an old list of some of the Clusters that we have we just have an endless",
    "start": "1249720",
    "end": "1256960"
  },
  {
    "text": "number of them so why do we have so many clusters because we made it really easy for developers in a few minutes to",
    "start": "1256960",
    "end": "1263480"
  },
  {
    "text": "create their own cluster and we encouraged developers to have their own cluster that they access that doesn't",
    "start": "1263480",
    "end": "1269799"
  },
  {
    "text": "get in anyone else's way no one else hitting there's no one else sending traffic to it so it's separation of",
    "start": "1269799",
    "end": "1275240"
  },
  {
    "text": "concerns makes everything very identifiable so what this well the problem with that is you end up with too",
    "start": "1275240",
    "end": "1280960"
  },
  {
    "text": "many clusters to Monitor and what we built was our own tooling that gives you a list of clusters you can go in and you",
    "start": "1280960",
    "end": "1286520"
  },
  {
    "text": "can look at the schemas and key spaces this is a sandbox with a bunch of test stuff in it running in our test account",
    "start": "1286520",
    "end": "1293640"
  },
  {
    "text": "so the Explorers is based on in a framework it's a uh I think the client",
    "start": "1293640",
    "end": "1298919"
  },
  {
    "text": "side it's mostly based on groovy uh gr's back end and I think it's D3 is the the",
    "start": "1298919",
    "end": "1305480"
  },
  {
    "text": "front end gooey stuff so there's there's a bunch of basic Explorer Technologies we've used in many cases to build these",
    "start": "1305480",
    "end": "1312559"
  },
  {
    "text": "kinds of dashboards for managing stuff and that framework should be out in the next few months so another problem we have is",
    "start": "1312559",
    "end": "1321080"
  },
  {
    "text": "because we've got so many Cassandra clusters is ETL normally you go to your big Oracle on MySQL back end that's got",
    "start": "1321080",
    "end": "1328080"
  },
  {
    "start": "1325000",
    "end": "1473000"
  },
  {
    "text": "all your tables in and you suck everything out and you do your integration queries and then you spit",
    "start": "1328080",
    "end": "1333600"
  },
  {
    "text": "everything into business intelligence but the trouble is we've denormalized our data set across a huge number of",
    "start": "1333600",
    "end": "1339360"
  },
  {
    "text": "clusters and there is no place where you can do a join and um in some cases we're",
    "start": "1339360",
    "end": "1346400"
  },
  {
    "text": "logging data directly for business intelligence but in some cases they just want everything like they want the entire membership database to cross link",
    "start": "1346400",
    "end": "1353159"
  },
  {
    "text": "with everything else the more event stream stuff they'll take as a log but sometimes some of the things they need",
    "start": "1353159",
    "end": "1359120"
  },
  {
    "text": "is the complete set so we built a Hadoop input formatter that reads Cassandra SS",
    "start": "1359120",
    "end": "1366159"
  },
  {
    "text": "tables there is no actual Cassandra running but it consumes the backups and",
    "start": "1366159",
    "end": "1371240"
  },
  {
    "text": "this is called aisus aisus in Greek mythology is the person that killed Cassandra because there is no actual",
    "start": "1371240",
    "end": "1377360"
  },
  {
    "text": "Cassandra here a dead cluster is imported by aisus and we basically in",
    "start": "1377360",
    "end": "1384880"
  },
  {
    "text": "bulk we have a continuously running single C single Hadoop job Hadoop cluster which sucks in large amounts of",
    "start": "1384880",
    "end": "1391960"
  },
  {
    "text": "Cassandra does the joins and the ETL and all that stuff and then sticks it into our backend terrod data we have a little",
    "start": "1391960",
    "end": "1398640"
  },
  {
    "text": "Terra data it's not getting any bigger we're surrounding it by more and more Hadoop and the master copy of our",
    "start": "1398640",
    "end": "1404919"
  },
  {
    "text": "business intelligence software is now moving from the being the ter",
    "start": "1404919",
    "end": "1410519"
  },
  {
    "text": "basically for managing test and production and things like that so the projects that we have um the",
    "start": "1468760",
    "end": "1476080"
  },
  {
    "start": "1473000",
    "end": "1838000"
  },
  {
    "text": "red ones here are things that are currently on GitHub the blue one this is Cassandra which is already out there but",
    "start": "1476080",
    "end": "1482200"
  },
  {
    "text": "we've put a lot of we've checked in a lot of code into Cassandra specifically for managing um well we're a major",
    "start": "1482200",
    "end": "1489679"
  },
  {
    "text": "contributor to Cassandra basically particularly for ec2 support um aisus is",
    "start": "1489679",
    "end": "1494960"
  },
  {
    "text": "currently uh a blog post that we you know we haven't had anyone actually ask us to open source it yet but and then we",
    "start": "1494960",
    "end": "1502120"
  },
  {
    "text": "have these two which have been described in blog posts but we're now planning to release them as open- Source things on",
    "start": "1502120",
    "end": "1509720"
  },
  {
    "text": "GitHub um circuit breaker is the you know release it pattern if you if you've",
    "start": "1509720",
    "end": "1515320"
  },
  {
    "text": "been listening to uh Michael nygard's um talks and a lot of other talks been talking about circuit breaker pattern so",
    "start": "1515320",
    "end": "1521799"
  },
  {
    "text": "this is a Java implementation of that that we'll be releasing soon so let me",
    "start": "1521799",
    "end": "1527799"
  },
  {
    "text": "see PR I talked the these two I talked about cast J meter is a load test Suite",
    "start": "1527799",
    "end": "1533480"
  },
  {
    "text": "we use for checking out benchmarking and testing new versions of Cassandra I'll show you that again a little bit later",
    "start": "1533480",
    "end": "1539640"
  },
  {
    "text": "we use zookeeper for distributed coordination so we have zookeeper patterns that's called curator that's",
    "start": "1539640",
    "end": "1544720"
  },
  {
    "text": "probably the most popular project we have it's got nothing to do with Cloud if you're using zookeeper you probably",
    "start": "1544720",
    "end": "1550320"
  },
  {
    "text": "want to be using curator um exhibitor is like prium but",
    "start": "1550320",
    "end": "1556399"
  },
  {
    "text": "for keeping zookeepers running in the cloud and doing backups and things like that um let's see we have a service",
    "start": "1556399",
    "end": "1563760"
  },
  {
    "text": "directory called Eureka which is we also internally call the discovery service we have a Dynamic Property Service we call",
    "start": "1563760",
    "end": "1570279"
  },
  {
    "text": "archaias which is actually a uh it's a kind of a gecko it's some lizardy thing",
    "start": "1570279",
    "end": "1575919"
  },
  {
    "text": "it's got nothing to do with Greek mythology although it sounds like it should um order scaling scripts and",
    "start": "1575919",
    "end": "1581760"
  },
  {
    "text": "things Asgard and the chaos monkey I'm wearing the shirt obviously but this thing is a robustness verification if",
    "start": "1581760",
    "end": "1588559"
  },
  {
    "text": "you look at the chaos monkey code it looks overly complicated for what it does the reason is it's a framework for",
    "start": "1588559",
    "end": "1594480"
  },
  {
    "text": "supporting about 10 different varieties of monkey that are all specialized for different tasks and there are two that",
    "start": "1594480",
    "end": "1601440"
  },
  {
    "text": "we're currently working on a latency monkey and janitor monkey latency monkey doesn't go around killing things it goes",
    "start": "1601440",
    "end": "1607760"
  },
  {
    "text": "around doing uh latency and error injection so instead of killing a service it causes the service to return",
    "start": "1607760",
    "end": "1613600"
  },
  {
    "text": "an arbitrary error code or to delay the response at some random frequenc",
    "start": "1613600",
    "end": "1619600"
  },
  {
    "text": "um that has been extremely useful for finding all kinds of interesting bugs janitor monkey cleans up unused",
    "start": "1619600",
    "end": "1625440"
  },
  {
    "text": "resources it notices that you have an autoscale group that's not been touched for a week or so and just it's basically",
    "start": "1625440",
    "end": "1631600"
  },
  {
    "text": "it sort of harvesting things they both use uh a database which we've internally",
    "start": "1631600",
    "end": "1637919"
  },
  {
    "text": "called entry points for obscure reasons think of collecting everything about",
    "start": "1637919",
    "end": "1643279"
  },
  {
    "text": "your your infrastructure you you walk through the Amazon instances that you have and you do describe instance on",
    "start": "1643279",
    "end": "1649520"
  },
  {
    "text": "everything that's there you can enumerate everything in your infrastructure this means you can make",
    "start": "1649520",
    "end": "1655520"
  },
  {
    "text": "strong assertions about your infrastructure which is a very interesting concept for security but",
    "start": "1655520",
    "end": "1660559"
  },
  {
    "text": "that's not really the subject of this talk entry points is where we log that data so entry points is walking the",
    "start": "1660559",
    "end": "1667399"
  },
  {
    "text": "describe calls against all the AWS entities it's also talking to um Eureka",
    "start": "1667399",
    "end": "1675880"
  },
  {
    "text": "and pulling our service directory and all of our described data and a few other things and it builds a lot a",
    "start": "1675880",
    "end": "1682519"
  },
  {
    "text": "complete view of the world but the interesting thing is it doesn't throw it away it remembers the differences so now",
    "start": "1682519",
    "end": "1690080"
  },
  {
    "text": "I can get the view of what my Amazon environment looked like a week ago and I",
    "start": "1690080",
    "end": "1695559"
  },
  {
    "text": "can difference that to now and see what changed which is how the janitor monkey works it's how it knows that you didn't",
    "start": "1695559",
    "end": "1701200"
  },
  {
    "text": "touch this thing for a week because it can see the state of it hasn't changed and we can do interesting things now",
    "start": "1701200",
    "end": "1707600"
  },
  {
    "text": "because we have a VI into history so again that service is coming out it's basically a big pile of Json objects",
    "start": "1707600",
    "end": "1715120"
  },
  {
    "text": "that are being indexed in interesting ways and you know the Fairly obvious implementation of that is to use mongodb",
    "start": "1715120",
    "end": "1721440"
  },
  {
    "text": "because that's kind of what mongodb is so that's you know it's a big Json integration slice and dice",
    "start": "1721440",
    "end": "1728200"
  },
  {
    "text": "thing um explorers I mentioned I showed you uh we're doing uh we're rebuilding",
    "start": "1728200",
    "end": "1734360"
  },
  {
    "text": "all of our internal code using juice as dependency injection framework um that's",
    "start": "1734360",
    "end": "1739760"
  },
  {
    "text": "going to come out soon it's called governator is the name of the library that it's it's a it's like a extended",
    "start": "1739760",
    "end": "1745240"
  },
  {
    "text": "version of juice um Odin is coming from the same team that did Asgard as you may",
    "start": "1745240",
    "end": "1751039"
  },
  {
    "text": "be able to guess which is doing workflow orchestration as logging ly",
    "start": "1751039",
    "end": "1756600"
  },
  {
    "text": "injection uh bits and pieces here oh the dinos slaves this is part of the build",
    "start": "1756600",
    "end": "1761679"
  },
  {
    "text": "system so we have a bakery which is how we build our Amis dinos slave is a a way",
    "start": "1761679",
    "end": "1767120"
  },
  {
    "text": "to Cloud first your jenin system that's sitting in the mean your build system your continuous integration build system",
    "start": "1767120",
    "end": "1773480"
  },
  {
    "text": "sitting in the data center but every now and again you have to do a big build and it takes too long so what a dinos slave",
    "start": "1773480",
    "end": "1778799"
  },
  {
    "text": "does is create build slaves in the cloud as part of an Amazon VPC extension of",
    "start": "1778799",
    "end": "1784320"
  },
  {
    "text": "your data center to to handle the burst and then they disappear again and fall back to your your data center based sort",
    "start": "1784320",
    "end": "1790840"
  },
  {
    "text": "of Baseline so that's basically it okay so that's uh",
    "start": "1790840",
    "end": "1799159"
  },
  {
    "text": "most of the architectural stuff now I'm going to talk about some benchmarks and scalability um some point about a year",
    "start": "1799159",
    "end": "1806600"
  },
  {
    "text": "ago almost a year ago we ran some scalability benchmarks so we took the version of Prius that we had at the time",
    "start": "1806600",
    "end": "1812720"
  },
  {
    "text": "we took the version of Cassandra that we had at the time with the biggest cluster we were running was sort of 24 to 48",
    "start": "1812720",
    "end": "1819200"
  },
  {
    "text": "nodes we were just firing up 48 nodes and I said well you know what happens when you scale and get a bigger and",
    "start": "1819200",
    "end": "1825440"
  },
  {
    "text": "bigger Cassandra cluster and how far can you take it and I said well let's keep going until we get a big number and I",
    "start": "1825440",
    "end": "1833039"
  },
  {
    "text": "decided that a million rights a second was a big number so we did that and data",
    "start": "1833039",
    "end": "1840519"
  },
  {
    "start": "1838000",
    "end": "1959000"
  },
  {
    "text": "stack started advertising that you could do a million rights per second in the cloud I mean I was getting popup ads for",
    "start": "1840519",
    "end": "1846000"
  },
  {
    "text": "my Benchmark just if you click through data staes site you ended up on the Netflix blog site so that was cool um it",
    "start": "1846000",
    "end": "1854440"
  },
  {
    "text": "turns out that the universal unit of marketing is a million whatever you're doing scale it",
    "start": "1854440",
    "end": "1861080"
  },
  {
    "text": "so it was a million of right you know units of per second per hour per day",
    "start": "1861080",
    "end": "1866159"
  },
  {
    "text": "what if you say it's a billion it sounds like you're trying trying too hard and if it's only a thousand it's not impressive right I mean even if it's a",
    "start": "1866159",
    "end": "1873080"
  },
  {
    "text": "th000 per micr second and you know a billion per month and a million right so you want to pick a million and this is a",
    "start": "1873080",
    "end": "1881320"
  },
  {
    "text": "you know I did 48 uh 96 144 and then we just went for it and did 288 nodes uh",
    "start": "1881320",
    "end": "1888159"
  },
  {
    "text": "these are running fairly small nodes because I was trying scalability I didn't want to have I wasn't going for vertical I was going for horizontal so",
    "start": "1888159",
    "end": "1895000"
  },
  {
    "text": "yeah you could do a million rights per second with a lot fewer nodes but I wanted to see was it linear and this is",
    "start": "1895000",
    "end": "1900240"
  },
  {
    "text": "a pretty much as a straight line as you'll see on a scalability graph um and we got bored at this point I mean I",
    "start": "1900240",
    "end": "1906320"
  },
  {
    "text": "don't know it probably goes up here somewhere right thousands of nodes it sounds plausible the individual nodes",
    "start": "1906320",
    "end": "1912480"
  },
  {
    "text": "were about as busy at this point as they were at this point so the overhead of adding more capacity goes up this is for",
    "start": "1912480",
    "end": "1920360"
  },
  {
    "text": "100% wrs now reads is a different story and but particularly Cassandra's always",
    "start": "1920360",
    "end": "1926080"
  },
  {
    "text": "been good at wrs it's getting better at reads that's just that's the best way of thinking about it this Benchmark was run",
    "start": "1926080",
    "end": "1932840"
  },
  {
    "text": "in about an hour only needed 288 machines for an hour cost a few hundred",
    "start": "1932840",
    "end": "1937960"
  },
  {
    "text": "gave it all back we're done right this is the beauty of benchmarking in the cloud so the next Benchmark we did was",
    "start": "1937960",
    "end": "1945360"
  },
  {
    "text": "more recent and we've been looking at this for a while and um you know where",
    "start": "1945360",
    "end": "1950639"
  },
  {
    "text": "is the future where where are we going in terms of cloud and storage and we'd been twisting Amazon's arms for a while",
    "start": "1950639",
    "end": "1956840"
  },
  {
    "text": "over solid state discs so what we've been running on mostly to now is is",
    "start": "1956840",
    "end": "1962039"
  },
  {
    "start": "1959000",
    "end": "2016000"
  },
  {
    "text": "these machines they're M2 Forex large they have 68 gig of RAM a 1 gig Network",
    "start": "1962039",
    "end": "1967240"
  },
  {
    "text": "you can do maybe 500 iops there's two internal discs you can you can get about",
    "start": "1967240",
    "end": "1972600"
  },
  {
    "text": "100 megabytes per second on the discs and yeah they're not they're they're the",
    "start": "1972600",
    "end": "1977960"
  },
  {
    "text": "relatively expensive but you get a decent amount of memory and a reasonable amount of this is the CPU so what they",
    "start": "1977960",
    "end": "1983880"
  },
  {
    "text": "came out with was these uh SSD based instances so you get two terabytes of built-in SSD there stripes of smaller in",
    "start": "1983880",
    "end": "1992039"
  },
  {
    "text": "smaller ssds it's got a bit more CPU power about the same Ram 10 gig Network",
    "start": "1992039",
    "end": "1998039"
  },
  {
    "text": "100,000 iops and a gigabyte a second two dis and a gigabyte per second on the network so it's s a balanced throughput",
    "start": "1998039",
    "end": "2004919"
  },
  {
    "text": "gigabyte per second server that you can do 100,000 Diop at and and we're okay",
    "start": "2004919",
    "end": "2010720"
  },
  {
    "text": "give us hundreds of those and we'll move all our Cassandra cluster to it so that was the general idea so we posted a",
    "start": "2010720",
    "end": "2016720"
  },
  {
    "start": "2016000",
    "end": "2172000"
  },
  {
    "text": "benchmark we did a load test driver we had the same rest application service",
    "start": "2016720",
    "end": "2022720"
  },
  {
    "text": "and this is what we're currently running we're running 36 M2 extra large mcash DS",
    "start": "2022720",
    "end": "2028760"
  },
  {
    "text": "because Cassandra is not always that good at read performance um and then we",
    "start": "2028760",
    "end": "2034159"
  },
  {
    "text": "have 48 M4 extra large Cassandra to have enough IO bandwidth to handle the reads",
    "start": "2034159",
    "end": "2040279"
  },
  {
    "text": "and writs this is handling the bookmarks where everybody that ever um watches a",
    "start": "2040279",
    "end": "2046519"
  },
  {
    "text": "movie it remembers where you are in that movie and that's updated once a minute so there's a large amount of traffic on",
    "start": "2046519",
    "end": "2052398"
  },
  {
    "text": "this read and write um what we benchmarked it against was a much smaller cluster of Cassandra nodes with",
    "start": "2052399",
    "end": "2059320"
  },
  {
    "text": "no memcache in between and the same service hitting it so this is the comparison that was approximately",
    "start": "2059320",
    "end": "2066158"
  },
  {
    "text": "equivalent and the since there's some many fewer of these and we don't need any M caches to get good read",
    "start": "2066159",
    "end": "2071480"
  },
  {
    "text": "performance uh it worked out about half the cost so we currently have about 100",
    "start": "2071480",
    "end": "2076839"
  },
  {
    "text": "uh SSD instances that Amazon has finally given us that we can put into production they are pretty rare right now um",
    "start": "2076839",
    "end": "2083200"
  },
  {
    "text": "there's a it's a bit easier to get them in the European clusters but um that's a",
    "start": "2083200",
    "end": "2088960"
  },
  {
    "text": "this this was this is kind of the future this is a system that has an excess of I/O capacity and it always used to be",
    "start": "2088960",
    "end": "2095520"
  },
  {
    "text": "that in the cloud you didn't have enough I/O capacity so um there's a bunch of things that we",
    "start": "2095520",
    "end": "2101640"
  },
  {
    "text": "don't do this a slide I used yesterday but it's one of my favorite slides right",
    "start": "2101640",
    "end": "2107079"
  },
  {
    "text": "particularly when we're benchmarking but some of the one of the things we do do and this is going back to the Cassandra",
    "start": "2107079",
    "end": "2113079"
  },
  {
    "text": "Summit uh a few months ago I ran a benchmark live on stage just to like show what it's really like to run",
    "start": "2113079",
    "end": "2119960"
  },
  {
    "text": "Cassandra um but I'm just going to rerun the results of that I'm not actually going to run it live and um this is what",
    "start": "2119960",
    "end": "2126920"
  },
  {
    "text": "I actually put up says okay I'm about to start this Benchmark and know YOLO means you only live once this happened a week",
    "start": "2126920",
    "end": "2133920"
  },
  {
    "text": "after another event this is an event that redefined the meaning of the word hard for an engineering project remember",
    "start": "2133920",
    "end": "2141400"
  },
  {
    "text": "what that project was what what happened this Summer that sounded like you know",
    "start": "2141400",
    "end": "2147160"
  },
  {
    "text": "that's hard you know it was it's kind of hard you know let's let's let's just",
    "start": "2147160",
    "end": "2152400"
  },
  {
    "text": "land a truck on Mars from a from a sky crane you know that's hard what I'm doing isn't hard that's hard right so",
    "start": "2152400",
    "end": "2160400"
  },
  {
    "text": "that was my okay you know if we can if if they can land a sky crane on Mars I can do a live demo that was my attitude",
    "start": "2160400",
    "end": "2167440"
  },
  {
    "text": "there was a guy from JPL in the audience so who liked that slide so that was cool too um so we have a Jenkins automation",
    "start": "2167440",
    "end": "2175240"
  },
  {
    "start": "2172000",
    "end": "2235000"
  },
  {
    "text": "we have the J meter code that we put up we have a Jenkins stapper around it the traffic was reading and writing 100",
    "start": "2175240",
    "end": "2181200"
  },
  {
    "text": "column rows we have 25 million row Keys which is sort of rough our sort of customer base sized benchmark we ran it",
    "start": "2181200",
    "end": "2188319"
  },
  {
    "text": "for 10 minutes and then doubled the size of the cluster so 10 I mean it takes me",
    "start": "2188319",
    "end": "2194200"
  },
  {
    "text": "5 minutes to create this cluster temp and I run traffic into it for 10 minutes and then I tell it to double the size",
    "start": "2194200",
    "end": "2199680"
  },
  {
    "text": "and I run it for another 10 minutes wrap up the whole thing in about 30 40 minutes and I'm done so let's see if I",
    "start": "2199680",
    "end": "2206280"
  },
  {
    "text": "can get through this um first thing this is Asgard before I start running ignore",
    "start": "2206280",
    "end": "2211680"
  },
  {
    "text": "the one in the middle I have an M2 test and a SSD test uh the M2 is the non SSD",
    "start": "2211680",
    "end": "2217520"
  },
  {
    "text": "one and I have no instances and no order scale groups and Dennis set this up for",
    "start": "2217520",
    "end": "2222920"
  },
  {
    "text": "me here's my Jenkins I have two build scripts and I checked it this was I",
    "start": "2222920",
    "end": "2228960"
  },
  {
    "text": "tested it a few times to see if it worked and Dennis built this stuff for me um and debugged it so this is the build",
    "start": "2228960",
    "end": "2236319"
  },
  {
    "start": "2235000",
    "end": "2431000"
  },
  {
    "text": "parameters when you hit run you get to look at all this stuff I won't go through it in detail this is basically",
    "start": "2236319",
    "end": "2241760"
  },
  {
    "text": "controlling how many J how many jmeter drivers we need so it's a build job that",
    "start": "2241760",
    "end": "2247160"
  },
  {
    "text": "creates a benchmark and it creates the the jmeter drivers and then it the output of",
    "start": "2247160",
    "end": "2254839"
  },
  {
    "text": "the build job is the graphs that show what happened during The Benchmark right",
    "start": "2254839",
    "end": "2259960"
  },
  {
    "text": "so the output of the build is a bunch of plots and the machines no longer exist",
    "start": "2259960",
    "end": "2265119"
  },
  {
    "text": "they gave them back again so it just runs to completion um so after running it a bit",
    "start": "2265119",
    "end": "2272040"
  },
  {
    "text": "here we have three asgs one per Zone we have 12 instances I got two 12 node clusters",
    "start": "2272040",
    "end": "2279200"
  },
  {
    "text": "um this is another one of our Cassandra uh monitoring Explorer things it's called Klo which is Greek for circles",
    "start": "2279200",
    "end": "2286560"
  },
  {
    "text": "and these are this is I I had 12 nodes running and then as we were scaling it",
    "start": "2286560",
    "end": "2292079"
  },
  {
    "text": "up you could see the nodes appearing this thing was updating every few seconds and it was showing the new nodes",
    "start": "2292079",
    "end": "2297480"
  },
  {
    "text": "coming in so the the red ones are ones where Cassandra is not yet running but",
    "start": "2297480",
    "end": "2303480"
  },
  {
    "text": "it's declared that it's prium has told has basically grabbed a token there's",
    "start": "2303480",
    "end": "2309400"
  },
  {
    "text": "one going to be here um it's grabbed a slot this is bootstrapping pulling the data uh the green ones are in service",
    "start": "2309400",
    "end": "2317040"
  },
  {
    "text": "and there's a when they garbage collect or it basically goes yellow so you can see if you've got too much garbage",
    "start": "2317040",
    "end": "2322160"
  },
  {
    "text": "collecting going on a little bit later um this one for some reason had finished",
    "start": "2322160",
    "end": "2327839"
  },
  {
    "text": "a little earlier this one was coming up um and if you Mouse over something you can see how much data it has on it and",
    "start": "2327839",
    "end": "2335040"
  },
  {
    "text": "uh what else is going on there's a bunch of other things so out of this whole talk when we gave it last time everyone",
    "start": "2335040",
    "end": "2341200"
  },
  {
    "text": "just wanted this tool turns out okay so that's and it's coming soon um so this",
    "start": "2341200",
    "end": "2347319"
  },
  {
    "text": "is what happened you know it was a 1hour talk uh this was where I kicked off the the machines and this is how busy the",
    "start": "2347319",
    "end": "2354359"
  },
  {
    "text": "machines were and you know this is the CPU this is the network and this is the iops so you can see here we couldn't",
    "start": "2354359",
    "end": "2361319"
  },
  {
    "text": "really drive the M2 4X larges above 500 iops but the ssds were up at 3,000 di",
    "start": "2361319",
    "end": "2368560"
  },
  {
    "text": "ups at Peak so this is the initial Running of The Benchmark this is the",
    "start": "2368560",
    "end": "2373920"
  },
  {
    "text": "autoscaling well scaling it up and then this was I guess Drive doing the resyncs",
    "start": "2373920",
    "end": "2379400"
  },
  {
    "text": "and things okay and this is the output of",
    "start": "2379400",
    "end": "2384440"
  },
  {
    "text": "the build which is a whole lot of graphs um this is one of the graphs which shows how many instances there were per",
    "start": "2384440",
    "end": "2391560"
  },
  {
    "text": "availability zone so I have four and it ended up at eight which is good um and",
    "start": "2391560",
    "end": "2397280"
  },
  {
    "text": "here is the read latency in right yeah the read latency between the two so this is in microsc so this is half a",
    "start": "2397280",
    "end": "2404680"
  },
  {
    "text": "millisecond so you can see that the read latency for the SSD instance stayed under a millisecond in the whole time it",
    "start": "2404680",
    "end": "2411520"
  },
  {
    "text": "sort of ramped up as we sent more traffic to it but it stayed pretty flat the read latency for the one using",
    "start": "2411520",
    "end": "2418000"
  },
  {
    "text": "regular discs was about 50 milliseconds at Peak and basically you know it was happy until we tried autoscaling it's a",
    "start": "2418000",
    "end": "2425280"
  },
  {
    "text": "pretty nasty thing to do to autoscale a Cassandra cluster you have to shuffle all the data around so the next",
    "start": "2425280",
    "end": "2431720"
  },
  {
    "start": "2431000",
    "end": "2638000"
  },
  {
    "text": "steps we're migrating our production Cassandra to solid state discs um",
    "start": "2431720",
    "end": "2437760"
  },
  {
    "text": "several clusters are done about 100 running the next step from a code point of view is to take prum and start",
    "start": "2437760",
    "end": "2443480"
  },
  {
    "text": "autoscaling Cassandra um in 1.2 Cassandra has v- noodes um basically right now the only",
    "start": "2443480",
    "end": "2451000"
  },
  {
    "text": "sensible way of scaling Cassandra is to double or half the size of the ring so that each node splits to to a buddy or",
    "start": "2451000",
    "end": "2458680"
  },
  {
    "text": "coales is two two to one uh v- noes is an extra level of indirection in the",
    "start": "2458680",
    "end": "2464000"
  },
  {
    "text": "token mapping that lets you add one node and move just some data to it without",
    "start": "2464000",
    "end": "2469160"
  },
  {
    "text": "affecting too much uh of the rest of the system it's part of the original Dynamo paper it's something that reac already",
    "start": "2469160",
    "end": "2475400"
  },
  {
    "text": "has so it's it's not new technology it's something that was just missing from the original implementation of Cassandra so",
    "start": "2475400",
    "end": "2481560"
  },
  {
    "text": "with v- noes we can start adding and shrinking our cluster by one node at a time and what we'll end up with is a",
    "start": "2481560",
    "end": "2487920"
  },
  {
    "text": "cluster where you just launch it when you start writing data to it we'll say let's have no more than 100 gbt or 500",
    "start": "2487920",
    "end": "2495119"
  },
  {
    "text": "gigabytes of data per node and once it once it fills to that point it will just start growing because capacity growth so",
    "start": "2495119",
    "end": "2502280"
  },
  {
    "text": "you never run out of dis space your cluster just gets bigger um and then as",
    "start": "2502280",
    "end": "2507760"
  },
  {
    "text": "you send it traffic it will grow to the level of traffic additionally right so if you need send it enough traffic it",
    "start": "2507760",
    "end": "2513720"
  },
  {
    "text": "will end up having half the data so if you send it twice the traffic now every night at 4:00 a.m. we need much you know",
    "start": "2513720",
    "end": "2520000"
  },
  {
    "text": "we'd probably need much smaller clusters than we would at 700 p.m. so what we can do is have our Cassandra clusters order",
    "start": "2520000",
    "end": "2525720"
  },
  {
    "text": "scaling and growing and shrinking every night and and one way of thinking about this",
    "start": "2525720",
    "end": "2531400"
  },
  {
    "text": "is um historically we've a lot a lot of the advances in technology come by",
    "start": "2531400",
    "end": "2537560"
  },
  {
    "text": "learning how to waste something some some resource that used to be really really difficult to find and was",
    "start": "2537560",
    "end": "2543720"
  },
  {
    "text": "constrained suddenly becomes abundant and here iops were a constraint now",
    "start": "2543720",
    "end": "2549920"
  },
  {
    "text": "they're abundant so what I'm doing is figuring out if how can I waste iops I'm",
    "start": "2549920",
    "end": "2555160"
  },
  {
    "text": "wasting iops here in order to save something else and what I can do here is waste iops by autoscaling a backend",
    "start": "2555160",
    "end": "2561760"
  },
  {
    "text": "stateful server and save on money by having fewer instances and also make the",
    "start": "2561760",
    "end": "2567280"
  },
  {
    "text": "whole automation better so there can be some I mean ssds are game changers for",
    "start": "2567280",
    "end": "2572960"
  },
  {
    "text": "and they going there's going to be new applications new things that you couldn't do before",
    "start": "2572960",
    "end": "2578640"
  },
  {
    "text": "okay so I've got a few minutes left and I'm going to talk about a a Netflix internal hack Day project which um",
    "start": "2578640",
    "end": "2585800"
  },
  {
    "text": "luckily was not actually implemented um it's",
    "start": "2585800",
    "end": "2592079"
  },
  {
    "text": "Skynet okay so Sky Skynet is the computer from the Terminator movies and",
    "start": "2592079",
    "end": "2599839"
  },
  {
    "text": "luckily it's only been implemented in PowerPoint but I figured out how to implement Skynet in Cassandra so those",
    "start": "2599839",
    "end": "2605920"
  },
  {
    "text": "of you that don't know what I'm talking about the the plot the bits of the plot that matter is Skynet is a sentient",
    "start": "2605920",
    "end": "2612240"
  },
  {
    "text": "computer that defends itself if you try and turn it off okay that was the sort",
    "start": "2612240",
    "end": "2617440"
  },
  {
    "text": "of the initial part of the plot was they tried to shut it down and it decided didn't want to be shut down and since it",
    "start": "2617440",
    "end": "2622480"
  },
  {
    "text": "was in charge of the Weapons Systems of America it fought back right there's a guy called Connor who kills it",
    "start": "2622480",
    "end": "2629240"
  },
  {
    "text": "eventually and then there's Terminator who sent back in time to kill Connor and a bunch of stuff like that right so",
    "start": "2629240",
    "end": "2635280"
  },
  {
    "text": "those are the basic that's the plot and it was a hack so these are the haors",
    "start": "2635280",
    "end": "2640599"
  },
  {
    "start": "2638000",
    "end": "2718000"
  },
  {
    "text": "so we have a Cassandra cluster that detects it's going being attacked and responds we have a Conor monkey that",
    "start": "2640599",
    "end": "2647200"
  },
  {
    "text": "kills Skynet nodes and we have a Terminator monkey that kills the the Conor monkey nodes right and there's",
    "start": "2647200",
    "end": "2653880"
  },
  {
    "text": "Skynet there's the counter guy and there's the Terminator guy so how how are we going to implement",
    "start": "2653880",
    "end": "2660839"
  },
  {
    "text": "this well we're going to have cast Sky inet store a history of its world everything it ever sees it'll just log",
    "start": "2660839",
    "end": "2667000"
  },
  {
    "text": "in into a memory list uh and then it will have like a brain that uh has",
    "start": "2667000",
    "end": "2673280"
  },
  {
    "text": "actions um and the action okay if it loses a node it will obviously replace",
    "start": "2673280",
    "end": "2678400"
  },
  {
    "text": "that node but it said you know if I'm losing nodes I should get bigger so I have more nodes to start with so the",
    "start": "2678400",
    "end": "2684640"
  },
  {
    "text": "reaction to start to killing off a Skynet cluster is it gets bigger and bigger and bigger the more you try and",
    "start": "2684640",
    "end": "2690040"
  },
  {
    "text": "kill it right um also if it's being targeted in one zone it should be able",
    "start": "2690040",
    "end": "2695480"
  },
  {
    "text": "to replicate itself to another Zone and another region and so this cluster and then split so that there's now replicas",
    "start": "2695480",
    "end": "2703119"
  },
  {
    "text": "of of this Skynet cluster sort of floating around in Amazon sort of lurking in different places and trying",
    "start": "2703119",
    "end": "2708800"
  },
  {
    "text": "to hide and also if it sees a Conor monkey it starts up a Terminator monkey",
    "start": "2708800",
    "end": "2713839"
  },
  {
    "text": "to go shoot that kind of monkey right so the implementation plan here was in prum",
    "start": "2713839",
    "end": "2719480"
  },
  {
    "start": "2718000",
    "end": "3106000"
  },
  {
    "text": "we already have Auto replacing missing nodes we know how to grow the cluster um",
    "start": "2719480",
    "end": "2724880"
  },
  {
    "text": "we actually need to figure out how to replicate to a new Zone but that's actually pretty simple code and and move",
    "start": "2724880",
    "end": "2732680"
  },
  {
    "text": "to a new region that we've got some automation there but it's not quite done yet so there's a little bit of new code there we have a couple of key spaces",
    "start": "2732680",
    "end": "2739880"
  },
  {
    "text": "actions and memory and then maybe a Cron job once a minute that just reads actions and runs whatever's there right",
    "start": "2739880",
    "end": "2746640"
  },
  {
    "text": "another interesting thing is if if you split this off and you actually successfully kill some of the nodes it",
    "start": "2746640",
    "end": "2752680"
  },
  {
    "text": "would actually forget things right if the only parts of this keyspace were in a certain not it would forget but if it",
    "start": "2752680",
    "end": "2758440"
  },
  {
    "text": "ran into another copy of itself they could merge and they could repair each other's memory so you could have a self-repairing self-learning so that",
    "start": "2758440",
    "end": "2765240"
  },
  {
    "text": "means I can insect a new action into one Cassandra Skynet node and as it ran around it would infect all of the other",
    "start": "2765240",
    "end": "2771400"
  },
  {
    "text": "nodes with that new action um and then the chaos monkey is pretty obvious so here's my simulation",
    "start": "2771400",
    "end": "2779640"
  },
  {
    "text": "This Is Us East with four skynets running in it there's Conor monkey who's popped up in Amazon Brazil because it",
    "start": "2779640",
    "end": "2786359"
  },
  {
    "text": "was far enough way to draw the picture um he zaps zet and takes one out um",
    "start": "2786359",
    "end": "2792760"
  },
  {
    "text": "here's the Terminator monkey who obviously is in California because that's where the governator lives um and",
    "start": "2792760",
    "end": "2800559"
  },
  {
    "text": "Skynet repairs itself and then he goes and he takes out the uh yeah so that's",
    "start": "2800559",
    "end": "2806920"
  },
  {
    "text": "it's a wonderful simulation isn't it anyway so I thought that would be amusing for for everybody but that's",
    "start": "2806920",
    "end": "2813319"
  },
  {
    "text": "basically it I'm done with the slides um I'll take some questions now and as you can see it's it's a good idea",
    "start": "2813319",
    "end": "2819760"
  },
  {
    "text": "that we good job we haven't actually implemented this yet so the question is do we anticipate problems with SSD",
    "start": "2819760",
    "end": "2826359"
  },
  {
    "text": "wearout uh since we're right heavy um there's two things one is the our workload isn't right heavy we have some",
    "start": "2826359",
    "end": "2833880"
  },
  {
    "text": "uh some we have 50 different clusters we have everything from Almost 100% write",
    "start": "2833880",
    "end": "2839240"
  },
  {
    "text": "to Almost 100% read and everything in between um on the other side what does",
    "start": "2839240",
    "end": "2845160"
  },
  {
    "text": "what does SSD wear out mean when I'm renting them by the hour I don't care that's Amazon's",
    "start": "2845160",
    "end": "2852160"
  },
  {
    "text": "problem it's B baked into the cost right if if if an SSD wears out kill the node",
    "start": "2852160",
    "end": "2857240"
  },
  {
    "text": "and get another one right they you know if I get if I find that one of the I",
    "start": "2857240",
    "end": "2862559"
  },
  {
    "text": "might need to put some measurement in there once they get a bit older that I could detect a bad one and just kill that node and keep replacing until I get",
    "start": "2862559",
    "end": "2868880"
  },
  {
    "text": "good ones but I it's not the same model is data center where you buy stuff and keep it for 3 years right I don't care",
    "start": "2868880",
    "end": "2874960"
  },
  {
    "text": "about SSD wearout basic and the other thing actually with Cassandra there's no right amplification",
    "start": "2874960",
    "end": "2881680"
  },
  {
    "text": "in Cassandra's uh rights to dis it's basically the perfect SSD workload it it",
    "start": "2881680",
    "end": "2887359"
  },
  {
    "text": "does a single right it's a big right it's sequential and it never writes it again it it never updates so and and",
    "start": "2887359",
    "end": "2895240"
  },
  {
    "text": "it's a big right so and then a bit later it does a merge and deletes that file so it's a very clean interface where if you",
    "start": "2895240",
    "end": "2901240"
  },
  {
    "text": "put a b Tree on top of an SSD you're working it really hard and it will wear out much quicker the standard Amazon",
    "start": "2901240",
    "end": "2908680"
  },
  {
    "text": "instances have two to four discs spinning rust and you can do you know",
    "start": "2908680",
    "end": "2913760"
  },
  {
    "text": "they spin it like 100 H Herz right so you get you get a few hundred iops that's all you get two discs two",
    "start": "2913760",
    "end": "2920400"
  },
  {
    "text": "spindles worth of capacity um on the other side it's a stripe of a large",
    "start": "2920400",
    "end": "2925680"
  },
  {
    "text": "number of ssds and ssds there's nothing spinning I mean even one SSD will do um",
    "start": "2925680",
    "end": "2932359"
  },
  {
    "text": "yeah cheap ssds do 10 to 20,000 iops easily and this is a this is an array of",
    "start": "2932359",
    "end": "2937720"
  },
  {
    "text": "small ssds that have been abstracted up to as a volume for from Amazon's point of view so ssds it's just re it's random",
    "start": "2937720",
    "end": "2945480"
  },
  {
    "text": "reads flash right so that's that you know we measured it we got 100,000 reads",
    "start": "2945480",
    "end": "2951559"
  },
  {
    "text": "100,000 you know you can do 100,000 iops reads or writes um depends you know the",
    "start": "2951559",
    "end": "2957359"
  },
  {
    "text": "rights the right patterns matter but the am the the Cassandra right pattern works really well so yes it is a Rags to",
    "start": "2957359",
    "end": "2964359"
  },
  {
    "text": "Riches thing I now have instead of being constru trained at a few hundred iops I now have just a ridiculous number like",
    "start": "2964359",
    "end": "2971079"
  },
  {
    "text": "my my I drove them to 3,000 right I have 100,000 I I got as far as 3,000 um some",
    "start": "2971079",
    "end": "2977520"
  },
  {
    "text": "of the the worst we did so far I think we got up to about 40,000 iops um after",
    "start": "2977520",
    "end": "2983520"
  },
  {
    "text": "doing some particularly pathological stuff to one of our clusters which was a bug right but the system was just going",
    "start": "2983520",
    "end": "2990319"
  },
  {
    "text": "yeah I can do that right so the question is if I'm sending a remote copy of the data do I do",
    "start": "2990319",
    "end": "2997520"
  },
  {
    "text": "anti-entropy on it well Cassandra has several anti-entropy systems in it so the first thing is hinted handoff so as",
    "start": "2997520",
    "end": "3004880"
  },
  {
    "text": "I mentioned when I send data remotely uh I hold the copy of that data locally until I've heard acknowledgement that it",
    "start": "3004880",
    "end": "3012079"
  },
  {
    "text": "arrived if I don't hear the acknowledgement you know after a long period of time maybe this node crashes",
    "start": "3012079",
    "end": "3017359"
  },
  {
    "text": "or something and you've lost the lost it but that covers shortterm loss right um for you know days even um we also the",
    "start": "3017359",
    "end": "3027119"
  },
  {
    "text": "the the two maintenance operations for Cassandra and and these maintenance operations are relatively IO intensive",
    "start": "3027119",
    "end": "3033799"
  },
  {
    "text": "one is compaction which compresses the SS tables into fewer bigger ones the",
    "start": "3033799",
    "end": "3039160"
  },
  {
    "text": "other one is repair so repair takes all the nodes and it builds Merkel trees of",
    "start": "3039160",
    "end": "3044839"
  },
  {
    "text": "all the data that's in each node and then it ships those around and then Compares them and on the regular disc",
    "start": "3044839",
    "end": "3051960"
  },
  {
    "text": "based nodes it's painful because it overloads the nodes and you get IO",
    "start": "3051960",
    "end": "3057319"
  },
  {
    "text": "shortages and you get you know it affects your read performance when you when you're doing this with the SSD ones",
    "start": "3057319",
    "end": "3063760"
  },
  {
    "text": "we have enough IO bandwidth that we can run repairs and uh compactions without",
    "start": "3063760",
    "end": "3069200"
  },
  {
    "text": "taking the nodes offline we typically take a node offline to do compaction and",
    "start": "3069200",
    "end": "3074319"
  },
  {
    "text": "Repair on it um the trouble with repair is you're comparing all three copies of",
    "start": "3074319",
    "end": "3079440"
  },
  {
    "text": "the data so if you're trying to get good read band with you have three copies that are all busy so there's a thing",
    "start": "3079440",
    "end": "3084599"
  },
  {
    "text": "called staggered repair that tries to stagger the hard work and it sort of helps a bit doesn't help completely the",
    "start": "3084599",
    "end": "3090640"
  },
  {
    "text": "real answer is to run on ssds okay right thanks very",
    "start": "3090640",
    "end": "3096240"
  },
  {
    "text": "much",
    "start": "3104960",
    "end": "3107960"
  }
]