[
  {
    "start": "0",
    "end": "104000"
  },
  {
    "text": "[Music]",
    "start": "3510",
    "end": "6660"
  },
  {
    "text": "so yeah thanks for coming I know uh end of the the day is interesting see who actually shows up um I'm Jeff esel so I",
    "start": "12759",
    "end": "19720"
  },
  {
    "text": "work at elastic I've been there five and a half years I used to work in trading firms I used to work at um E Trade",
    "start": "19720",
    "end": "25880"
  },
  {
    "text": "before on the engineering side and operation side so done a lot of stuff the these days I tend to focus on search",
    "start": "25880",
    "end": "31759"
  },
  {
    "text": "and gen and all the buzz words that that entails uh a couple quick let's say",
    "start": "31759",
    "end": "37000"
  },
  {
    "text": "housekeeping slides so there's the app if you haven't downloaded it it's nice you can do the QR code um and then I",
    "start": "37000",
    "end": "43640"
  },
  {
    "text": "have two elastic plugs as well just to get it out um so if you're familiar with",
    "start": "43640",
    "end": "49520"
  },
  {
    "text": "elastic if you're not you may have seen our marketing blogs over the years and they're marketing great but if you're a",
    "start": "49520",
    "end": "54680"
  },
  {
    "text": "developer and you're interested more in the technical content um if you go to search lab so elastic.co search Dash",
    "start": "54680",
    "end": "60680"
  },
  {
    "text": "laabs these are where our real technical content is um it's written by you know the engineering team I'm in the field so",
    "start": "60680",
    "end": "67119"
  },
  {
    "text": "field team developers um again really technically focused not just kind of marketing um things there's also then",
    "start": "67119",
    "end": "73920"
  },
  {
    "text": "the associated GitHub so there's um full applications you can clone and download and get up and running real fast jupyter",
    "start": "73920",
    "end": "79400"
  },
  {
    "text": "notebooks all that fun stuff and then a even more personal plug uh my boss and I at the be I guess the end of last year",
    "start": "79400",
    "end": "85920"
  },
  {
    "text": "we wrote a book on using um operationalizing Vector search for elastic so if you're interested in this",
    "start": "85920",
    "end": "93399"
  },
  {
    "text": "it's from the operational perspective right so actually using things it's not the uh ml research books there's other",
    "start": "93399",
    "end": "99720"
  },
  {
    "text": "better ones on that but if you want to actually use this stuff you can check that out all right with that said so I",
    "start": "99720",
    "end": "105759"
  },
  {
    "start": "104000",
    "end": "208000"
  },
  {
    "text": "know there's probably a wide range of experience here um so we're going to do a couple little laying the foundation",
    "start": "105759",
    "end": "113159"
  },
  {
    "text": "things so lexical 101 so with elastic when you do termbase search these are",
    "start": "113159",
    "end": "118399"
  },
  {
    "text": "the things we've been doing for a long time uh you go through your input it goes through tokenizers analyzers and",
    "start": "118399",
    "end": "124439"
  },
  {
    "text": "and things like that to clean it up and break it apart and then we store it in the vered index so for example if your",
    "start": "124439",
    "end": "130080"
  },
  {
    "text": "input is jumping foxes goes through the analyzers and other components and you wind up with jump Fox which then gets",
    "start": "130080",
    "end": "137280"
  },
  {
    "text": "broken up into jump or Fox and then we store it in the inverted index so this",
    "start": "137280",
    "end": "143720"
  },
  {
    "text": "is things again we've been doing I think elastic is 12 or 13 years at this point um but it's based on Decades of research",
    "start": "143720",
    "end": "149959"
  },
  {
    "text": "W be Beyond elastic and it continues to actually be tuned and optimized so if you take for example two",
    "start": "149959",
    "end": "156440"
  },
  {
    "text": "documents you have the quick brown fox jumps over the lazy dog and you have dogs can jump",
    "start": "156440",
    "end": "162480"
  },
  {
    "text": "dogs whatever means whatever that means um when we process it we store it in the",
    "start": "162480",
    "end": "167560"
  },
  {
    "text": "vert indic index um and so you have the column so you have your terms Brown and",
    "start": "167560",
    "end": "172959"
  },
  {
    "text": "dog you have the doc frequency how many documents that appears in and then you have the postings field which has things",
    "start": "172959",
    "end": "178720"
  },
  {
    "text": "like again the doc ID uh term frequency positions and so this is how we can search pyte scale at at",
    "start": "178720",
    "end": "186000"
  },
  {
    "text": "you know millisecond speed because you're not doing a full teex search over all the actual documents you're going to",
    "start": "186000",
    "end": "191879"
  },
  {
    "text": "that inverted index in the back and just looking for the word and then jumping to the dock allows it to work pretty fast",
    "start": "191879",
    "end": "197040"
  },
  {
    "text": "and because of you know that need there is the necessary complexity but it all is optimized again for Speed and and",
    "start": "197040",
    "end": "204239"
  },
  {
    "text": "efficiency and it works really well that was the shortest version of",
    "start": "204239",
    "end": "209840"
  },
  {
    "start": "208000",
    "end": "560000"
  },
  {
    "text": "lexical search I could ever uh make so again the reason we kind of poting that is then to show the the opposing view or",
    "start": "209840",
    "end": "216920"
  },
  {
    "text": "the newer view I guess of semantical search so if anyone is a really deep in",
    "start": "216920",
    "end": "223920"
  },
  {
    "text": "the field on this you don't jump on me that there's I put only two there are definitely many many subtypes of models",
    "start": "223920",
    "end": "230480"
  },
  {
    "text": "but we're going to talk about two high General types of of vector models today so dense Vector models are the ones that",
    "start": "230480",
    "end": "237360"
  },
  {
    "text": "you almost certainly use if you've used Vector search or you're talking to chat apps this is what you know today Vector",
    "start": "237360",
    "end": "245120"
  },
  {
    "text": "search uses for the vast majority and on the output side you have just this array",
    "start": "245120",
    "end": "250319"
  },
  {
    "text": "of floats on the left side sparse vectors are I'm not going to say newer",
    "start": "250319",
    "end": "255519"
  },
  {
    "text": "but they're starting to become a little more common um and the middle layers",
    "start": "255519",
    "end": "260840"
  },
  {
    "text": "you'll see are the same but the output is a token and a weight and so I",
    "start": "260840",
    "end": "266320"
  },
  {
    "text": "mentioned this because when we're talking about sparse vectors here we're talking about semantic IC sparse vectors",
    "start": "266320",
    "end": "271520"
  },
  {
    "text": "um so one example is Elser model that's elastics elastic learn sparse encoder",
    "start": "271520",
    "end": "276720"
  },
  {
    "text": "we'll talk a little bit more about the details of in a second but it's important to know that you know these models share the kind of beginning and",
    "start": "276720",
    "end": "284240"
  },
  {
    "text": "middle same architecture so they're both based on uh Transformer models for",
    "start": "284240",
    "end": "289520"
  },
  {
    "text": "example Bert um the middle layers use the semantic relationship in the attention mechanism and so this is how",
    "start": "289520",
    "end": "295080"
  },
  {
    "text": "when you insert um you know bring in a a text field or something all these middle layers learn their relationships on how",
    "start": "295080",
    "end": "301639"
  },
  {
    "text": "words are formed their importance of the position how they relate to other words and you're basically generating the",
    "start": "301639",
    "end": "307000"
  },
  {
    "text": "models understanding of you know what the meaning is of the document and this",
    "start": "307000",
    "end": "312960"
  },
  {
    "text": "is then where they differ so D vectors then again are dense one reason call",
    "start": "312960",
    "end": "319319"
  },
  {
    "text": "them dense is because they output a fixed number of Dimensions every single time so you'll see things like 786 or",
    "start": "319319",
    "end": "325960"
  },
  {
    "text": "1536 Dimension 768 um so every time a model that outputs 768 Dimensions it",
    "start": "325960",
    "end": "332319"
  },
  {
    "text": "always outputs that if you think of them kind of as slots every slot is filled every slot has a a value that's",
    "start": "332319",
    "end": "338560"
  },
  {
    "text": "important conversely then sparse vectors are sparse because they have these dictionaries of potentially hundreds of",
    "start": "338560",
    "end": "344639"
  },
  {
    "text": "thousands of words and the vast majority of those values are zero because they're they're not relevant to whatever your",
    "start": "344639",
    "end": "350080"
  },
  {
    "text": "input was so it's sparsely populated and in this case we output again the that",
    "start": "350080",
    "end": "355240"
  },
  {
    "text": "that token and the weight uh dense vectors for the vast",
    "start": "355240",
    "end": "360680"
  },
  {
    "text": "majority are the vector you get is the representation of the whole input so the",
    "start": "360680",
    "end": "365759"
  },
  {
    "text": "output Vector represents it sentence a paragraph whatever that's its representation of that whole input it's",
    "start": "365759",
    "end": "372560"
  },
  {
    "text": "not every single token has its own input now there are very there special um you",
    "start": "372560",
    "end": "378919"
  },
  {
    "text": "can't see it behind me little special Corner called coar and there's a couple other ones that have multi-dimension",
    "start": "378919",
    "end": "384960"
  },
  {
    "text": "almost things like that we're not talking about that today but sparse vectors on the other hand do what we",
    "start": "384960",
    "end": "390120"
  },
  {
    "text": "call late interaction so it goes through that that model and then the scoring token based happens later on at the end",
    "start": "390120",
    "end": "397080"
  },
  {
    "text": "so when we up with the token um we weigh the importance of it so typical semantical",
    "start": "397080",
    "end": "404599"
  },
  {
    "text": "search go to that you have the meaning you're trying to get to the meaning of what the document is you have a model in",
    "start": "404599",
    "end": "411479"
  },
  {
    "text": "the index so jumping foxes goes through the ml model and then goes to the index",
    "start": "411479",
    "end": "417759"
  },
  {
    "text": "and in this case we're going to store factors so these have been around again for a long time um been doing research",
    "start": "417759",
    "end": "424720"
  },
  {
    "text": "it's been mostly in the research Academia field though it's only been the last couple years they kind of broke out and you know with the The Bert model and",
    "start": "424720",
    "end": "432000"
  },
  {
    "text": "and things like that where they've kind of changed everything we've been doing lately uh but they're usually tuned by",
    "start": "432000",
    "end": "437400"
  },
  {
    "text": "you know machines running scripts and reinforcement learning things that there's some human in the loop it points",
    "start": "437400",
    "end": "442680"
  },
  {
    "text": "there but vast majority is automatic um tuning so when you have now this",
    "start": "442680",
    "end": "447759"
  },
  {
    "text": "document the quick brown fox it goes to the ml model and if it's a dense Vector",
    "start": "447759",
    "end": "452800"
  },
  {
    "text": "model you store the dense Vector so we store again the orinal value the vector",
    "start": "452800",
    "end": "458280"
  },
  {
    "text": "itself but then to make it search performant we store um The Neighbors in",
    "start": "458280",
    "end": "464479"
  },
  {
    "text": "the graph so we store um we generate What's called the hnsw graph the hierarchical navigable small worlds",
    "start": "464479",
    "end": "470080"
  },
  {
    "text": "graph um you can think of it the best way I can think of it is if you're on like Google Maps you open up Google",
    "start": "470080",
    "end": "476919"
  },
  {
    "text": "Earth and you say okay I'm going to Sears Tower it's called the Sears Tower",
    "start": "476919",
    "end": "483120"
  },
  {
    "text": "and I live you know I live nearby so I want to see where I am so I'm I'm it and",
    "start": "483120",
    "end": "488360"
  },
  {
    "text": "I zoom in and I want to get rid of things that aren't relevant to my search I want to find where this location is then I zoom in again and I can quickly",
    "start": "488360",
    "end": "494879"
  },
  {
    "text": "drop and I can take out a whole another blast because I'm only looking at Chicago then I'm looking you know I took Metra down so I'm I'm looking within a",
    "start": "494879",
    "end": "501159"
  },
  {
    "text": "couple blocks of Metra this way with the the hierarchal navigable small worlds graph you're able to quickly Traverse",
    "start": "501159",
    "end": "507120"
  },
  {
    "text": "these graphs and find nearest neighbors so in instead of matching keywords you're looking for what are the nearest",
    "start": "507120",
    "end": "513640"
  },
  {
    "text": "semantically relevant documents and there's a lot of math involved so what's in a vector kind of whatever the model",
    "start": "513640",
    "end": "519880"
  },
  {
    "text": "thinks it is um this is where the the training and tuning comes into play um there's these foundational models and",
    "start": "519880",
    "end": "526480"
  },
  {
    "text": "then you can do fine tuning of them for specific use cases um they're generally trained on document and query pairs some",
    "start": "526480",
    "end": "532760"
  },
  {
    "text": "of the new models even have the ability to tweak what they're going to do so this one's from coh here where depending",
    "start": "532760",
    "end": "539279"
  },
  {
    "text": "on if you tell it if it's a a search document or a search query document it'll actually output differently",
    "start": "539279",
    "end": "545120"
  },
  {
    "text": "because it's has kind of internal tuning for which way you're going to use it but",
    "start": "545120",
    "end": "550360"
  },
  {
    "text": "vectors are an approximation um they're not the exact thing as matching individual keywords you're always",
    "start": "550360",
    "end": "556800"
  },
  {
    "text": "matching the model's interpretation of what the input is okay with that said",
    "start": "556800",
    "end": "561880"
  },
  {
    "start": "560000",
    "end": "736000"
  },
  {
    "text": "we're going to look at Elser a little bit the sparse Vector model so sparse Vector models as I",
    "start": "561880",
    "end": "569000"
  },
  {
    "text": "mentioned um is based on our our model is based on a paper called Spade that came out last",
    "start": "569000",
    "end": "574560"
  },
  {
    "text": "year it's a um model designed to kind of piix some of the limitations of of dense",
    "start": "574560",
    "end": "579959"
  },
  {
    "text": "vectors one of those is for dense vectors at least with when you're using in um when you're using things like",
    "start": "579959",
    "end": "587399"
  },
  {
    "text": "Lucine where you're generating these graphs for the search to be performant you have to store it in memory it's",
    "start": "587399",
    "end": "593720"
  },
  {
    "text": "expensive off heat memory works really well it's extremely fast but if you have hundreds and hundreds of millions of",
    "start": "593720",
    "end": "598959"
  },
  {
    "text": "vectors you're going to have to have a lot of off keep Ram so we can use the ability",
    "start": "598959",
    "end": "604680"
  },
  {
    "text": "of the inverted index which uses much less RAM and output those tokens and",
    "start": "604680",
    "end": "610360"
  },
  {
    "text": "store them in this um Elsa right now is version uh two it is English only but it",
    "start": "610360",
    "end": "616440"
  },
  {
    "text": "works really really well at what we call out of domain uh learning so it's not fine-tuned for say you know a legal data",
    "start": "616440",
    "end": "625519"
  },
  {
    "text": "set or a medical data set I kind of think of it as if if it's an English document that any normal person can read",
    "start": "625519",
    "end": "631959"
  },
  {
    "text": "and understand elser's going to be great at it it does output again those tokens",
    "start": "631959",
    "end": "637480"
  },
  {
    "text": "I'll point out real quick just in case curious you'll see like hash hash and then oid um that's where the internal",
    "start": "637480",
    "end": "644240"
  },
  {
    "text": "dictionary didn't have whatever word ring the word Droid so it had to split",
    "start": "644240",
    "end": "649519"
  },
  {
    "text": "it so that's just kind of its marking that these are two um so text expansion were people getting",
    "start": "649519",
    "end": "655839"
  },
  {
    "text": "into trouble with uh sparse vectors in general L service play whatever is that",
    "start": "655839",
    "end": "660920"
  },
  {
    "text": "they think it's just kind of like an auto synonym machine because we output tokens that look like words and they are",
    "start": "660920",
    "end": "666760"
  },
  {
    "text": "words usually but there is that neural network expansion layer so it is important to understand that there is a",
    "start": "666760",
    "end": "672600"
  },
  {
    "text": "encoded meaning in in the way that these um tokens get generated but you go",
    "start": "672600",
    "end": "678720"
  },
  {
    "text": "through your first layer internal layers and then we do what's called token expansion so the um the tokens that are",
    "start": "678720",
    "end": "687120"
  },
  {
    "text": "input we then find which tokens are can be expanded out similar to to synonyms",
    "start": "687120",
    "end": "692600"
  },
  {
    "text": "but not the same um and then we store those and then the weights are added in so how important these individual tokens",
    "start": "692600",
    "end": "699040"
  },
  {
    "text": "are now to the um the input and then at query time you do the same thing so you",
    "start": "699040",
    "end": "704320"
  },
  {
    "text": "you run the input through the model you generate weights and then the search",
    "start": "704320",
    "end": "709440"
  },
  {
    "text": "phase looks for matches and then we do some super complicated math no it's",
    "start": "709440",
    "end": "715639"
  },
  {
    "text": "pretty pretty simple math add it up and that's your score and this again one of the big advantages",
    "start": "715639",
    "end": "721399"
  },
  {
    "text": "of of sparse vectors is that you're using the inverted index so you're you're faster and less memory um is",
    "start": "721399",
    "end": "729560"
  },
  {
    "text": "required um again it works really well on our you know all the standard benchmarks and everything out there but",
    "start": "729560",
    "end": "736279"
  },
  {
    "start": "736000",
    "end": "905000"
  },
  {
    "text": "there is a query problem we have with this uh Ben's one of our ml Engineers who I stole a couple of the next slide",
    "start": "736279",
    "end": "742480"
  },
  {
    "text": "from so if even if you're not a search engineer search expert you should",
    "start": "742480",
    "end": "749240"
  },
  {
    "text": "probably figure that when we do um token expansion matching if I tell you that",
    "start": "749240",
    "end": "754800"
  },
  {
    "text": "we're doing 30 to 50 sh Clauses that seems like a lot of sh Clauses for any query uh most you know lexical bm25",
    "start": "754800",
    "end": "761760"
  },
  {
    "text": "search you're doing you don't have 25 to 30 individual tokens in there you're doing much smaller um and you can see on this chart",
    "start": "761760",
    "end": "769160"
  },
  {
    "text": "that has no labels I'm sorry about that that along the the x-axis you have um",
    "start": "769160",
    "end": "775600"
  },
  {
    "text": "basically all the the tokens that are across so the purple one is is a lexical search and kind of the the frequency",
    "start": "775600",
    "end": "782560"
  },
  {
    "text": "across of all the documents potentially um they're not all bunched up and they're not a lot of them are not",
    "start": "782560",
    "end": "788680"
  },
  {
    "text": "relevant to an input whereas semantic search like with Elser the red one um",
    "start": "788680",
    "end": "794199"
  },
  {
    "text": "because you're doing this this um kind of token expansion most of the documents",
    "start": "794199",
    "end": "800000"
  },
  {
    "text": "have similar the similar tokens um with bm25 for example when you're doing",
    "start": "800000",
    "end": "805800"
  },
  {
    "text": "lexical search if if a word appears in every single document or the vast majority of",
    "start": "805800",
    "end": "812120"
  },
  {
    "text": "the documents you bm25 will score that lower because it's it's not probably not",
    "start": "812120",
    "end": "818160"
  },
  {
    "text": "super important so we do inverse document frequency that's the orange line um all this chart that is this",
    "start": "818160",
    "end": "825320"
  },
  {
    "text": "crazy drawing is trying to say that Elser and sparse vectors don't pay attention to things like um how",
    "start": "825320",
    "end": "832240"
  },
  {
    "text": "frequency the term all the the terms are weighted equally important and because we have so many",
    "start": "832240",
    "end": "838040"
  },
  {
    "text": "terms if you just run a Straight Out of the Box even though you're now getting you know a semantic search which is great because allows people to do",
    "start": "838040",
    "end": "843800"
  },
  {
    "text": "natural language you do then pay a penalty on speed which isn't great so what can we do about this well we can",
    "start": "843800",
    "end": "850519"
  },
  {
    "text": "just drop a bunch of tokens um you might think I'm kidding but actually it works pretty well so we have these two",
    "start": "850519",
    "end": "855800"
  },
  {
    "text": "arguments um token frequency ratio and token weight and these allow you to tune",
    "start": "855800",
    "end": "862480"
  },
  {
    "text": "the kind of lower weighted ones that aren't super important and you can drop those off and what we've actually found",
    "start": "862480",
    "end": "868880"
  },
  {
    "text": "is is so the control row uh with Max W disabled is the the regular one without",
    "start": "868880",
    "end": "875440"
  },
  {
    "text": "it pruned and then the pruned row is dropping um certain percent of of tokens",
    "start": "875440",
    "end": "881759"
  },
  {
    "text": "by dropping the lower weighted ones you can speed it up between 3x and 7x um and",
    "start": "881759",
    "end": "887480"
  },
  {
    "text": "get pretty close to normal bm25 search and recall is almost unaffected sometimes it's a little better sometimes",
    "start": "887480",
    "end": "893560"
  },
  {
    "text": "a little worse but it depends on your data set but it's largely unaffected so you get again that semantic search that",
    "start": "893560",
    "end": "899160"
  },
  {
    "text": "you can use for rag which we're going to get to and then um you know all a great speed up okay a couple other things",
    "start": "899160",
    "end": "906240"
  },
  {
    "start": "905000",
    "end": "987000"
  },
  {
    "text": "we're going to do that we're going to use when I show the the rag app that we're going to build um to make things",
    "start": "906240",
    "end": "912399"
  },
  {
    "text": "easy these things are all things to make things easier so when you're connecting you know generating embeddings or you're",
    "start": "912399",
    "end": "919160"
  },
  {
    "text": "doing chat completion or things like this you have to you know connect to models in some way so one way we can do",
    "start": "919160",
    "end": "926759"
  },
  {
    "text": "is use an inference API endpoint you can then create an endpoint that connects to either a model running locally in",
    "start": "926759",
    "end": "933600"
  },
  {
    "text": "elastic you can connect to um your favorite embedding service of choice and",
    "start": "933600",
    "end": "939040"
  },
  {
    "text": "then you just have a nice one endpoint that you hit you can use this at you know index time or query time or since",
    "start": "939040",
    "end": "944360"
  },
  {
    "text": "it is an endpoint you can just programmatically hit it um this is nice when you have again if you're you know",
    "start": "944360",
    "end": "950240"
  },
  {
    "text": "if you're developers or developers other people are going to use that data set you don't have to give them you know",
    "start": "950240",
    "end": "956120"
  },
  {
    "text": "which embedding model individual they have to use what's certainly what's the key to this model and everything if it's",
    "start": "956120",
    "end": "962199"
  },
  {
    "text": "an external service you just tell them the endpoint service and then they can connect and they're already if they're already using elastic for data it's all",
    "start": "962199",
    "end": "969079"
  },
  {
    "text": "kind of in the same place um again nothing super complicated here you hit the",
    "start": "969079",
    "end": "974199"
  },
  {
    "text": "endpoint it then passes the requests to the external service there's some you know retry logic and buffering and and",
    "start": "974199",
    "end": "980880"
  },
  {
    "text": "things like that um BT calls and then we will return it for",
    "start": "980880",
    "end": "986720"
  },
  {
    "text": "you so I've been doing you know things with Vector search and rag you know since a couple years now and if you if I",
    "start": "986800",
    "end": "995079"
  },
  {
    "start": "987000",
    "end": "1196000"
  },
  {
    "text": "was giving this talk a year ago even we would have gone through a whole thing on how complicated it can be to set up and",
    "start": "995079",
    "end": "1001319"
  },
  {
    "text": "tuning dense Vector search can be a real pain um not still can be but what we've",
    "start": "1001319",
    "end": "1006839"
  },
  {
    "text": "learned over the last year from you know real users experience is a lot of kind of sane defaults that are nice to get up",
    "start": "1006839",
    "end": "1013120"
  },
  {
    "text": "and running if all you want to do is get something running quickly kind of have a not weird data set or or even if you",
    "start": "1013120",
    "end": "1019639"
  },
  {
    "text": "just want to PC things we want to make it easy for you to get up you want to make a couple API calls you're up and running um so we have now we call the",
    "start": "1019639",
    "end": "1027360"
  },
  {
    "text": "semantic text field it does automatic chunking and will generate vectors for you again it's kind of these same",
    "start": "1027360",
    "end": "1034240"
  },
  {
    "text": "defaults where um we have right now there's one chunky mythology there will be more where you",
    "start": "1034240",
    "end": "1041240"
  },
  {
    "text": "can do it you don't have to you specify um as part of the field mapping sorry I'll come back to that",
    "start": "1041240",
    "end": "1047558"
  },
  {
    "text": "insane slide in a sec you specify your inference API uh endpoint is part of the field mapping so",
    "start": "1047559",
    "end": "1053960"
  },
  {
    "text": "if you've ever set this up before um with us or whichever service usually every time you make a call you have to",
    "start": "1053960",
    "end": "1060480"
  },
  {
    "text": "know your model ID it's kind of a pain especially when you have to tell other people or if you change the backend model um by doing it this way only at",
    "start": "1060480",
    "end": "1067880"
  },
  {
    "text": "creation time of your index where you're going to store the documents you link it to an embedding service either locally",
    "start": "1067880",
    "end": "1073440"
  },
  {
    "text": "or externally and then every other time after that whether you're querying or you're indexing documents you use it like every other other field it's kind",
    "start": "1073440",
    "end": "1079880"
  },
  {
    "text": "of like when you use you know keyword search bm25 I don't have to specify you know bm25 model in there anywhere I",
    "start": "1079880",
    "end": "1086720"
  },
  {
    "text": "don't even know where it is I just use it and it works all the box as I mentioned we do automatic chunking um",
    "start": "1086720",
    "end": "1094039"
  },
  {
    "text": "there's the first one they got out is 250 word chunks with 100 overlap the way I explain elastic to people who might",
    "start": "1094039",
    "end": "1101039"
  },
  {
    "text": "not be familiar with it is our engineering team like to make things",
    "start": "1101039",
    "end": "1106720"
  },
  {
    "text": "embedded and you know into the product is as much as possible we put it at Lucine layer usually where that's our",
    "start": "1106720",
    "end": "1113200"
  },
  {
    "text": "underlying Library technology and then we build into elastic and the reason we do it this way is we want it to work",
    "start": "1113200",
    "end": "1120120"
  },
  {
    "text": "with all our features and functionality not just be like a bolted on part that's kind of a pain to use and kind of works",
    "start": "1120120",
    "end": "1125760"
  },
  {
    "text": "kind of doesn't so because of that what we'll do is we'll release something that's works well but then you have this",
    "start": "1125760",
    "end": "1133440"
  },
  {
    "text": "list of feature requests over here that's great I mean this is one way to do chunking but we want to do a lot more",
    "start": "1133440",
    "end": "1138840"
  },
  {
    "text": "so this is just an example where you know 250 chunks with 100 overlap it might not be optimal for everyone it's a",
    "start": "1138840",
    "end": "1144120"
  },
  {
    "text": "good kind of starting point I know the next one that's coming out they're going to do um sentence chunking so you specify how many sentences you put",
    "start": "1144120",
    "end": "1150679"
  },
  {
    "text": "together you can say do you want chunking or overlap or not the reason to do overlapping if you're not familiar um",
    "start": "1150679",
    "end": "1157400"
  },
  {
    "text": "so if I have these chunks three and four since I'm not doing anything clever like",
    "start": "1157400",
    "end": "1163760"
  },
  {
    "text": "actually finding where the paragraph boundary is or where this you know",
    "start": "1163760",
    "end": "1169039"
  },
  {
    "text": "whatever topic I'm talking about I just find the words and I'm breaking on them it's almost certain that these two",
    "start": "1169039",
    "end": "1174600"
  },
  {
    "text": "chunks were talking about the same thing so you want to kind of be able to pull some meaning from the previous or",
    "start": "1174600",
    "end": "1180720"
  },
  {
    "text": "surrounding chunks into it um because you're matching on on what does the chunk mean so by pulling in some of the",
    "start": "1180720",
    "end": "1186960"
  },
  {
    "text": "surrounding you know meaning for lack of a better term um it kind of tends to get better",
    "start": "1186960",
    "end": "1193960"
  },
  {
    "text": "results cool all right so get to rag which is in my title I see now I'm going",
    "start": "1195679",
    "end": "1203320"
  },
  {
    "start": "1196000",
    "end": "1397000"
  },
  {
    "text": "to assume everyone has been seeing rag around in a thousand different ways I",
    "start": "1203320",
    "end": "1209240"
  },
  {
    "text": "even I know there's like five other talks or three other talks about it which is great um I'm glad to see people are using it this is my very quick",
    "start": "1209240",
    "end": "1216280"
  },
  {
    "text": "overview for people who aren't familiar um you're the happy smiley guy up there",
    "start": "1216280",
    "end": "1223360"
  },
  {
    "text": "and we have a chat application where humans interacting with it so we're going to go uh counterclockwise today so",
    "start": "1223360",
    "end": "1229799"
  },
  {
    "text": "the user asks a question we have some code layer whatever you want to write your code in and we want to retrieve",
    "start": "1229799",
    "end": "1237400"
  },
  {
    "text": "some documents so the the little app I'm going to show in a few minutes it has a restaurant review kind of like Yelp but",
    "start": "1237400",
    "end": "1243640"
  },
  {
    "text": "you know some open source day said and if I want to see where's the the tastiest pizza around um I'll ask my",
    "start": "1243640",
    "end": "1251600"
  },
  {
    "text": "question and I want to retrieve contextual documents I want to find documents perhaps that are you know",
    "start": "1251600",
    "end": "1258200"
  },
  {
    "text": "based on reviews that are you know in the last year I want to find places that",
    "start": "1258200",
    "end": "1263559"
  },
  {
    "text": "are within a half mile of of here of the you know search Tower because I don't want to go you know drive out somewhere",
    "start": "1263559",
    "end": "1270480"
  },
  {
    "text": "far and you can use semantic search you can use lexical search you can combine them together um doing something called",
    "start": "1270480",
    "end": "1276880"
  },
  {
    "text": "reciprocal rank Fusion that's where you run two searches semantic search and lexical search and then based on the the",
    "start": "1276880",
    "end": "1284200"
  },
  {
    "text": "rank that they come back in there's a little formula it's pretty straightforward we rank results together",
    "start": "1284200",
    "end": "1289960"
  },
  {
    "text": "um this is when you have a data set or a users where sometimes they might ask questions that are just keyword-based",
    "start": "1289960",
    "end": "1296080"
  },
  {
    "text": "you know people have been kind of trained for decades to just play the keyword game where I'm like what",
    "start": "1296080",
    "end": "1302120"
  },
  {
    "text": "keywords are in this document I want other people are starting to get used to asking questions naturally um but if I",
    "start": "1302120",
    "end": "1308240"
  },
  {
    "text": "know a very specific keyword that's in my document that's going to hit far better than just kind of trying to",
    "start": "1308240",
    "end": "1313520"
  },
  {
    "text": "describe it um so anyways that's how um you can combine the results together so that's the retrieval part",
    "start": "1313520",
    "end": "1319640"
  },
  {
    "text": "then we return the documents to the app your whatever your middle War app is or your back end and we create a prompt so",
    "start": "1319640",
    "end": "1326360"
  },
  {
    "text": "this is the augmented part so when you're interacting with llms they're all just taking text input",
    "start": "1326360",
    "end": "1334360"
  },
  {
    "text": "to explain what's going on or tell them what to do so we take this input and we",
    "start": "1334360",
    "end": "1339480"
  },
  {
    "text": "you know you you can get as super creative as you want with these but the the basic foundations of it is you say",
    "start": "1339480",
    "end": "1345159"
  },
  {
    "text": "okay the user is asking this question you give them the question um use the information we provide because the",
    "start": "1345159",
    "end": "1351520"
  },
  {
    "text": "answers are somewhere in this document and usually you're going to say something around the lines of you know",
    "start": "1351520",
    "end": "1356880"
  },
  {
    "text": "don't use your training to come up with the answer yourself um because models",
    "start": "1356880",
    "end": "1362840"
  },
  {
    "text": "are going to just predict out the next token whether it's actually the right answer or not so if it's if it has just",
    "start": "1362840",
    "end": "1368919"
  },
  {
    "text": "finished training on restaurants yesterday it's probably going to be pretty good but if I'm looking for reviews and it's you know a little",
    "start": "1368919",
    "end": "1375320"
  },
  {
    "text": "older um you don't want it just making up answers so generally if you tell it to only use the documents you provide",
    "start": "1375320",
    "end": "1380919"
  },
  {
    "text": "and not make up an answer you have a higher chance of it not hallucinating so",
    "start": "1380919",
    "end": "1386000"
  },
  {
    "text": "that's the augmented part you kind of tell it what to do and then generation is obviously the the chat model",
    "start": "1386000",
    "end": "1391120"
  },
  {
    "text": "generating an answer and then we pass it back to the user and hopefully it all works out",
    "start": "1391120",
    "end": "1396320"
  },
  {
    "text": "well uh a couple little I don't want to say Advanced but different less common things you can do when you're thinking",
    "start": "1396320",
    "end": "1402279"
  },
  {
    "start": "1397000",
    "end": "1566000"
  },
  {
    "text": "about rag applications um one of them is you know generative caching now there's a couple different generative caching",
    "start": "1402279",
    "end": "1407960"
  },
  {
    "text": "things um the one I'm going to talk about is generating I'm G to do this in reverse order",
    "start": "1407960",
    "end": "1413799"
  },
  {
    "text": "um caching the responses from an llm so",
    "start": "1413799",
    "end": "1419440"
  },
  {
    "text": "where this is really useful is things like knowledge based or document stor so",
    "start": "1419440",
    "end": "1424919"
  },
  {
    "text": "if you have you know users asking questions about like how do I do something for your company or it's an",
    "start": "1424919",
    "end": "1432240"
  },
  {
    "text": "internal thing on how does some process work right and you have kind of the top 100,000",
    "start": "1432240",
    "end": "1438880"
  },
  {
    "text": "you know 200 whatever your amount is but the vast majority of your questions are these people asking these questions over",
    "start": "1438880",
    "end": "1444200"
  },
  {
    "text": "and over what you can do is store those answers so I kind of walk through this this workflow um it's little we but so",
    "start": "1444200",
    "end": "1451679"
  },
  {
    "text": "the first time you ask a question let's say the cash is empty user your application queries elastic there's",
    "start": "1451679",
    "end": "1457679"
  },
  {
    "text": "nothing in there so it gets back no hit you go the normal rag route you get some contextual documents you build your",
    "start": "1457679",
    "end": "1464799"
  },
  {
    "text": "prompt you pass it to the LM you get back your your answer you send it back up to the user and and hopefully they're",
    "start": "1464799",
    "end": "1470840"
  },
  {
    "text": "happy what you do in parallel then is um the bottom in here send the prompt so",
    "start": "1470840",
    "end": "1479279"
  },
  {
    "text": "I'm going to store the the generated prompt in text I'm going to store the users's question in text but I'm going",
    "start": "1479279",
    "end": "1486440"
  },
  {
    "text": "to generate a dense Vector for the user's question the reason I'm going to do this is then I'm going to run",
    "start": "1486440",
    "end": "1491840"
  },
  {
    "text": "similarity search because if I ask everyone in this room to ask a chatbot",
    "start": "1491840",
    "end": "1496919"
  },
  {
    "text": "where's the best pizza place in your own words if everyone asks basically the",
    "start": "1496919",
    "end": "1502640"
  },
  {
    "text": "exact same thing we're going to have I don't know how many people are in there let's say 50 different slightly",
    "start": "1502640",
    "end": "1508720"
  },
  {
    "text": "variations on how that is is phrased so you can use semantic search approxim",
    "start": "1508720",
    "end": "1513760"
  },
  {
    "text": "near neighbor search to find questions that are semantically similar now because approximate nearest neighbor is",
    "start": "1513760",
    "end": "1519960"
  },
  {
    "text": "approximate and it's the nearest neighbor if someone asks something completely off the wall it will you know",
    "start": "1519960",
    "end": "1526399"
  },
  {
    "text": "Vector search will return the nearest neighbor so you can control um kind of the closeness of the question by a",
    "start": "1526399",
    "end": "1532520"
  },
  {
    "text": "similarity parameter and you can tune that but essentially if the next person comes around and asks basically the same",
    "start": "1532520",
    "end": "1539080"
  },
  {
    "text": "question and if it's close enough this first one is going to do um",
    "start": "1539080",
    "end": "1544640"
  },
  {
    "text": "query oh sorry it's up there um for semantically cached answers based on the",
    "start": "1544640",
    "end": "1550080"
  },
  {
    "text": "user's question if that's a close enough match I just return my previously",
    "start": "1550080",
    "end": "1555720"
  },
  {
    "text": "generated answer back to the user so to the user it seems like the llm",
    "start": "1555720",
    "end": "1561279"
  },
  {
    "text": "generated the answer because it did it just happened to do it sometime in the past um and so reasons for doing this as",
    "start": "1561279",
    "end": "1568039"
  },
  {
    "start": "1566000",
    "end": "1639000"
  },
  {
    "text": "I mentioned it works really well in some scenarios definitely not all scenarios some keep in mind um increased response",
    "start": "1568039",
    "end": "1573880"
  },
  {
    "text": "speed even if you're using an extremely fast um llm GPT 40 right it's pretty",
    "start": "1573880",
    "end": "1579679"
  },
  {
    "text": "quick um returning a document from your",
    "start": "1579679",
    "end": "1585200"
  },
  {
    "text": "vector database is all always going to be faster than going the chat",
    "start": "1585200",
    "end": "1590960"
  },
  {
    "text": "route um so you get faster responses um reduce token cost all this stuff costs",
    "start": "1590960",
    "end": "1596279"
  },
  {
    "text": "money um if you're using a hosted service certainly you're paying token costs if you're using a self-hosted one",
    "start": "1596279",
    "end": "1602039"
  },
  {
    "text": "you know you're paying gpus and spikes and all that stuff um the other thing you can do is",
    "start": "1602039",
    "end": "1607960"
  },
  {
    "text": "kind of have your prev vetted answers even if you use all the prompting techniques in the world sometimes the",
    "start": "1607960",
    "end": "1613520"
  },
  {
    "text": "alms go off the rails a little bit and we've all seen where like car dealers are giving away free cars and stuff",
    "start": "1613520",
    "end": "1619320"
  },
  {
    "text": "right um it'd be nice to prev some of these answers so again if you had a 100 or 200 top questions that you want to",
    "start": "1619320",
    "end": "1625279"
  },
  {
    "text": "answer you can sit down and generate the answers or you can just type them up and it you know it looks like a generated",
    "start": "1625279",
    "end": "1631120"
  },
  {
    "text": "answer but then you still allow for those kind of less common questions to be run so a lot of different uses for it",
    "start": "1631120",
    "end": "1637840"
  },
  {
    "text": "um some keep in mind and then two other ways you can use the um llm to help out",
    "start": "1637840",
    "end": "1643840"
  },
  {
    "start": "1639000",
    "end": "1879000"
  },
  {
    "text": "your rag app is using it for or will Zoom um using it to generate",
    "start": "1643840",
    "end": "1652159"
  },
  {
    "text": "queries for you and then generate the um uh conversation memory so the first",
    "start": "1652159",
    "end": "1658799"
  },
  {
    "text": "thing you can do is have it generate the query to elastic search your vector data",
    "start": "1658799",
    "end": "1664559"
  },
  {
    "text": "store for it the reasons for doing this is generally what you're going to do when you're quering for contextual",
    "start": "1664559",
    "end": "1670480"
  },
  {
    "text": "documents you're going to template out your query you have a query or maybe two that you want you know you C up and you",
    "start": "1670480",
    "end": "1677600"
  },
  {
    "text": "fill in the keywords that the users ask but you can almost always create a more",
    "start": "1677600",
    "end": "1682880"
  },
  {
    "text": "custom um query and get you know more relevant documents if based on what the",
    "start": "1682880",
    "end": "1689000"
  },
  {
    "text": "user is asking maybe based on where they are time and things like this so instead of you know obviously",
    "start": "1689000",
    "end": "1695519"
  },
  {
    "text": "human is not going to do this what you can do is you can pass first a prompt to the LM and say again the user is asking",
    "start": "1695519",
    "end": "1701640"
  },
  {
    "text": "this question this is the index or multiple indices that are available these are the relevant fields",
    "start": "1701640",
    "end": "1708679"
  },
  {
    "text": "you can give it examples of queries if it needs like this is lexical this is a a hybrid this is one using aggregations",
    "start": "1708679",
    "end": "1715480"
  },
  {
    "text": "blah blah like all the filters the models are pretty good at re generating these more um tuned query for it and",
    "start": "1715480",
    "end": "1723559"
  },
  {
    "text": "then you get that back and you do the normal rag approach you take that query",
    "start": "1723559",
    "end": "1728600"
  },
  {
    "text": "you you know get contextual documents and you pass it back the other thing you can build in um and there's lots of",
    "start": "1728600",
    "end": "1735519"
  },
  {
    "text": "different ways to do this is updating memory so we're all kind of used to using you know well J gbt and all the",
    "start": "1735519",
    "end": "1744519"
  },
  {
    "text": "other services that are out there now and when you're talking to these models you know it kind of seems like they they",
    "start": "1744519",
    "end": "1750000"
  },
  {
    "text": "have a memory of what's going on you know you can say what's the best pizza place and you can say what time is it",
    "start": "1750000",
    "end": "1755600"
  },
  {
    "text": "open like I'm not specifying a specific I'm just saying it right and that's how normal language works you remember a",
    "start": "1755600",
    "end": "1761279"
  },
  {
    "text": "little bit and then I probably forget but the way this really works is the",
    "start": "1761279",
    "end": "1766679"
  },
  {
    "text": "models aren't updating in real time you have to pass the memory of the",
    "start": "1766679",
    "end": "1771720"
  },
  {
    "text": "conversation to the model in the prompt it's something the users generally never see but to do that if you have a short",
    "start": "1771720",
    "end": "1779399"
  },
  {
    "text": "conversation it's usually not a big deal but if they're talking back and forth for a while eventually you're going to hit the model's context window so this",
    "start": "1779399",
    "end": "1785640"
  },
  {
    "text": "is the amount of tokens you know input that the model can handle at once everything outside of that it either is",
    "start": "1785640",
    "end": "1792080"
  },
  {
    "text": "going to drop it or throw an error you know whichever way it's set up um so one",
    "start": "1792080",
    "end": "1797399"
  },
  {
    "text": "thing we've kind of I've been playing with is you have it summarize a conversation so for example you know if",
    "start": "1797399",
    "end": "1805039"
  },
  {
    "text": "I'm talking to one of you the most recent two or three back and forth are I'm probably going to ask something",
    "start": "1805039",
    "end": "1810519"
  },
  {
    "text": "about that like I'll ask you a follow-up right away to something you asked um and so you might want to keep those two or",
    "start": "1810519",
    "end": "1816600"
  },
  {
    "text": "three interactions for both like I want to store them exactly so I put them in an array or dictionary but then",
    "start": "1816600",
    "end": "1822679"
  },
  {
    "text": "everything after that it's important but it kind of trails off in in importance so you can test the LM hey this is",
    "start": "1822679",
    "end": "1828640"
  },
  {
    "text": "everything we've talked about so far I want you to create a summary that you know the you the LM is going to use is",
    "start": "1828640",
    "end": "1836360"
  },
  {
    "text": "followup um you can tell at things like the user is never going to see this so it doesn't have to be nice complete sentences it just make it as Compact and",
    "start": "1836360",
    "end": "1843240"
  },
  {
    "text": "efficient as possible and you give it a window right like you have 300 characters pick your whatever you want",
    "start": "1843240",
    "end": "1849960"
  },
  {
    "text": "um and then every time you make a call um you pass back the answer you ask the LM to update it again lots of approaches",
    "start": "1849960",
    "end": "1856519"
  },
  {
    "text": "to this but this is one way you can kind of keep having that back and forth and allow your users to ask follow-up",
    "start": "1856519",
    "end": "1861919"
  },
  {
    "text": "questions about the conversation okay talked for a while so I do have a a",
    "start": "1861919",
    "end": "1869360"
  },
  {
    "text": "short demo and then we'll do some questions and",
    "start": "1869360",
    "end": "1874120"
  },
  {
    "text": "answers I say this all the time but like I'm not a big slide person but then I get stuck with 51 slides so sorry okay",
    "start": "1876760",
    "end": "1885799"
  },
  {
    "start": "1879000",
    "end": "2600000"
  },
  {
    "text": "um the code's not super important here what we're going to do",
    "start": "1885799",
    "end": "1891240"
  },
  {
    "text": "is this bigger okay um I'm going to do that restaurant review thing I said so I need",
    "start": "1891240",
    "end": "1898240"
  },
  {
    "text": "to clean up some things to make",
    "start": "1898240",
    "end": "1903600"
  },
  {
    "text": "sure views me trying to prove it's not",
    "start": "1903600",
    "end": "1909000"
  },
  {
    "text": "all and delete so I'm deleting the index I'm deleting the data view that just lets you look at the document going to",
    "start": "1909000",
    "end": "1915159"
  },
  {
    "text": "delete the endpoint to generate embeddings delete the endpoint to talk to um in this case Azure open AI where",
    "start": "1915159",
    "end": "1922760"
  },
  {
    "text": "we have our tenant okay so we talked about some of this in all the the slide buildup I was doing the first thing I",
    "start": "1922760",
    "end": "1928519"
  },
  {
    "text": "want to do is generate a inference API endpoint so I can generate these sparse vector or sparse embedding tokens um I",
    "start": "1928519",
    "end": "1936039"
  },
  {
    "text": "specify that it's um Elser and allocations and number of",
    "start": "1936039",
    "end": "1941159"
  },
  {
    "text": "threads these are just resources that we're going to give to the to the deployment um allocations are essentially how many concurrent request",
    "start": "1941159",
    "end": "1948039"
  },
  {
    "text": "can I do at a time and then number of threads is number of threads per allocation so um of course we're",
    "start": "1948039",
    "end": "1955600"
  },
  {
    "text": "changing the API so you get a little warning there but or notice but what this has done in the background is it's",
    "start": "1955600",
    "end": "1962200"
  },
  {
    "text": "automatically deployed my model for me I have a new deployment set up and now I have an endpoint called my Elser",
    "start": "1962200",
    "end": "1968559"
  },
  {
    "text": "endpoint great we'll test that in a second next thing I want to do is set up an inference API to talk to again Azure",
    "start": "1968559",
    "end": "1976120"
  },
  {
    "text": "open AI in this case where I'm going to um have my chat completion so we'll test",
    "start": "1976120",
    "end": "1984039"
  },
  {
    "text": "so there's my model or inpoint rather here's my model all I'm looking for here is that",
    "start": "1984039",
    "end": "1991679"
  },
  {
    "text": "it's deployed on some node I don't care what node serverless um I'm going to then test",
    "start": "1991679",
    "end": "1998600"
  },
  {
    "text": "generating some embeddings so again I like to ask ridiculous questions tell lims to entertain myself um how many",
    "start": "1998600",
    "end": "2005159"
  },
  {
    "text": "ducks so this one is generating the embeddings that I could store and as you can see there's the tokens Ducks",
    "start": "2005159",
    "end": "2011760"
  },
  {
    "text": "football Mall must have got oh yeah Mard um field duck anyways so we're storing",
    "start": "2011760",
    "end": "2018519"
  },
  {
    "text": "those and then the chat completion that's running I can ask it",
    "start": "2018519",
    "end": "2024679"
  },
  {
    "text": "again just to test that it's working so it's making the it's passing the call to open Ai and",
    "start": "2024679",
    "end": "2030840"
  },
  {
    "text": "um it's making it um do this to me it's entertaining to use like the most",
    "start": "2030840",
    "end": "2036799"
  },
  {
    "text": "powerful tools we have available to do ridiculous things um so approximately",
    "start": "2036799",
    "end": "2042720"
  },
  {
    "text": "57,000 ducks can fit interestingly once in a while I get 26,000 I don't know",
    "start": "2042720",
    "end": "2049240"
  },
  {
    "text": "it's the same model but same question okay so we have set up our API endpoints",
    "start": "2049240",
    "end": "2054760"
  },
  {
    "text": "and we're ready to roll um what I'm going to do is go and upload it's not",
    "start": "2054760",
    "end": "2060760"
  },
  {
    "text": "what I want to do [Music]",
    "start": "2060760",
    "end": "2069408"
  },
  {
    "text": "Chang I'm in serverless and we push to serverless all the time so I think uh",
    "start": "2075399",
    "end": "2081158"
  },
  {
    "text": "Monday is push day see what happens okay uh I have a small restaurant view",
    "start": "2081159",
    "end": "2087320"
  },
  {
    "text": "again this is super small just for the EV so what I'm doing now is you know uploading this this CSV file you can do",
    "start": "2087320",
    "end": "2093480"
  },
  {
    "text": "PDFs or anything else you want I'm going to so what the what this",
    "start": "2093480",
    "end": "2099280"
  },
  {
    "text": "does is it just kind of analyzes your data you can see distributions things like that um if you're not familiar with",
    "start": "2099280",
    "end": "2105839"
  },
  {
    "text": "what you're you're working with it's helpful but I'm going to hit import call",
    "start": "2105839",
    "end": "2112960"
  },
  {
    "text": "It restaurant R views I'm going to go to Advanced this is where I can add my semantic text so I want to run semantic",
    "start": "2114560",
    "end": "2123119"
  },
  {
    "text": "queries on reviews so I can ask things about pizza and then you can see I have",
    "start": "2123119",
    "end": "2128320"
  },
  {
    "text": "my inference service that I previously set up so if I add that if I look down in mappings you'll know you'll see um I",
    "start": "2128320",
    "end": "2136760"
  },
  {
    "text": "have my new field review semantic it's a semantic text field and it points to that so that's all it has to happen hit",
    "start": "2136760",
    "end": "2142800"
  },
  {
    "text": "import um again if you wanted to change things",
    "start": "2142800",
    "end": "2148078"
  },
  {
    "text": "like I change that doesn't matter but you can update your mapping",
    "start": "2148200",
    "end": "2154400"
  },
  {
    "text": "as you want it's going to upload the data and it's kind of this part is again we're indexing the",
    "start": "2154400",
    "end": "2160319"
  },
  {
    "text": "document we're automatically chunking it into small chunks and we're generating in this case Spar and Bings for every",
    "start": "2160319",
    "end": "2166640"
  },
  {
    "text": "one of those chunks there's always one failure with this data set because I haven't fixed it um but then you can go",
    "start": "2166640",
    "end": "2173440"
  },
  {
    "text": "on and start using it you go to playground so playground is um you may",
    "start": "2173440",
    "end": "2178920"
  },
  {
    "text": "have seen other playgrounds all kind of similar right where it's kind of your rag testing ground um you can since we",
    "start": "2178920",
    "end": "2186160"
  },
  {
    "text": "we created the service you can connect to different models I'm going to leave as four and I can ask it",
    "start": "2186160",
    "end": "2192680"
  },
  {
    "text": "again so we're passing it and then we're getting back so the reason one of the things you can do on playground is again",
    "start": "2196960",
    "end": "2203319"
  },
  {
    "text": "we're we're testing like the documents that come back so we'll say we'll show you what",
    "start": "2203319",
    "end": "2209880"
  },
  {
    "text": "are the contextual documents that we're getting back um let's talk about pizza in here obviously you can see the scores",
    "start": "2209880",
    "end": "2216040"
  },
  {
    "text": "and how they were passed so these are the documents that then we would pass to the the LM to kind of ground the model",
    "start": "2216040",
    "end": "2223440"
  },
  {
    "text": "and say use these documents as part of your response to us the playground lets",
    "start": "2223440",
    "end": "2228680"
  },
  {
    "text": "you do other things where you can change number of documents to send um if you",
    "start": "2228680",
    "end": "2233800"
  },
  {
    "text": "have a different contextual field but also allows you to to change the query so right now I'm I'm querying",
    "start": "2233800",
    "end": "2241359"
  },
  {
    "text": "um that review semantic but if I had multiple fields that were relevant you can turn them on so maybe I wanted to to",
    "start": "2241359",
    "end": "2248200"
  },
  {
    "text": "query the review and the restaurant name looking for you know things like that you can go ahead and and change like",
    "start": "2248200",
    "end": "2254480"
  },
  {
    "text": "that and basically just play around um and see which works but you can also",
    "start": "2254480",
    "end": "2259720"
  },
  {
    "text": "then change",
    "start": "2259720",
    "end": "2262440"
  },
  {
    "text": "it um again for if you haven't worked with with Rags or llm a lot so there's",
    "start": "2264760",
    "end": "2270599"
  },
  {
    "text": "there's the user part of the prompt that tells the the LM like what what's the user asking but then you can provide",
    "start": "2270599",
    "end": "2276079"
  },
  {
    "text": "instructions or system instructions on how the model should behave these are really useful when you want to say how",
    "start": "2276079",
    "end": "2281280"
  },
  {
    "text": "it should behave who it's talking to is it talking to you know an executive or is it talking to a developer those are two vastly different answers that you",
    "start": "2281280",
    "end": "2287960"
  },
  {
    "text": "probably want to give um you can tell things to talk about not to talk about if I regenerate this now it'll say you",
    "start": "2287960",
    "end": "2296480"
  },
  {
    "text": "know it's looking for tasty pizza and it just kind of pirate talk but you know",
    "start": "2296480",
    "end": "2303599"
  },
  {
    "text": "you want to do something more useful with your application so once you're kind of happy with what's going on here we can then um autogenerate some code",
    "start": "2303599",
    "end": "2311720"
  },
  {
    "text": "for you so in this case it's python code and you could take this as is and run it",
    "start": "2311720",
    "end": "2319200"
  },
  {
    "text": "you'd have a you know a base kind of functional app you can build out it talks oops talks about um here's you",
    "start": "2319200",
    "end": "2326520"
  },
  {
    "text": "create the prompt interestingly with the prompt for examp you know we have I put in a pilot who loves to talk about gold",
    "start": "2326520",
    "end": "2332480"
  },
  {
    "text": "um but we also put in again these are some same things where you're kind of adding in information that are",
    "start": "2332480",
    "end": "2337560"
  },
  {
    "text": "developers thought were useful feel free to use them feel free to delete them um and then generate the response and so on",
    "start": "2337560",
    "end": "2344640"
  },
  {
    "text": "the only thing I actually want to use today for this is the query copy that and go vs code um so",
    "start": "2344640",
    "end": "2353520"
  },
  {
    "text": "I'll show a couple of things in here this is a python backend running",
    "start": "2353520",
    "end": "2360880"
  },
  {
    "text": "fast API and then a react frontend I am not a front-end developer nor am I good",
    "start": "2360880",
    "end": "2368280"
  },
  {
    "text": "at it it's something I'm working on so you're going to be blown away by the front end in a second um real quick I'll",
    "start": "2368280",
    "end": "2375960"
  },
  {
    "text": "show I I'll show this first I'll start",
    "start": "2375960",
    "end": "2380520"
  },
  {
    "text": "it running running host restart this okay it's not",
    "start": "2383160",
    "end": "2391079"
  },
  {
    "text": "the worst I've seen But like okay um where can",
    "start": "2391079",
    "end": "2397760"
  },
  {
    "text": "uh I'm not doing streaming so there's going to be a delay in the answer coming back usually you want to implement streaming so it looks like you know the",
    "start": "2399440",
    "end": "2405680"
  },
  {
    "text": "things typing at you you don't have the the thing but again you're going to get back then based on information provided",
    "start": "2405680",
    "end": "2411960"
  },
  {
    "text": "an answer um there's a lot of things we could we could tune up here obviously it's trying to send me markdown but I'm",
    "start": "2411960",
    "end": "2417079"
  },
  {
    "text": "not printing it in markdown you know it's just a de short demo app",
    "start": "2417079",
    "end": "2423480"
  },
  {
    "text": "um what's the rating of the first",
    "start": "2425000",
    "end": "2429319"
  },
  {
    "text": "restaurant um sometimes it works",
    "start": "2431040",
    "end": "2437318"
  },
  {
    "text": "doesn't usually it gives me an answer what I wanted to show was I ask",
    "start": "2440400",
    "end": "2447079"
  },
  {
    "text": "enough",
    "start": "2447079",
    "end": "2449480"
  },
  {
    "text": "questions this is where streaming comes into play it makes it much better demo",
    "start": "2452680",
    "end": "2458640"
  },
  {
    "text": "um where can I just getting a couple interactions so",
    "start": "2458640",
    "end": "2464680"
  },
  {
    "text": "then I can show the memory thing um but again every time we're making a call here or typing a question it's it's",
    "start": "2464680",
    "end": "2472079"
  },
  {
    "text": "doing a couple things it's quering elastic sech to find documents that areally similar I'm also running the",
    "start": "2472079",
    "end": "2477480"
  },
  {
    "text": "history update every time um certainly things could be you know sped up but in",
    "start": "2477480",
    "end": "2483119"
  },
  {
    "text": "in doing this and then if I ask something like the the duck question let see if it gives me an",
    "start": "2483119",
    "end": "2490200"
  },
  {
    "text": "answer okay could it worked you want to have some guard rails around it don't let it talk about crazy stuff um there's",
    "start": "2497440",
    "end": "2505240"
  },
  {
    "text": "obviously real Advanced ways to do this but just telling you to focus on one thing is the best um going for both one",
    "start": "2505240",
    "end": "2512400"
  },
  {
    "text": "thing I wanted to show that history um this is returning all the",
    "start": "2512400",
    "end": "2517480"
  },
  {
    "text": "information um is",
    "start": "2517480",
    "end": "2521720"
  },
  {
    "text": "doing bigger I can't really make it bigger um",
    "start": "2523880",
    "end": "2528960"
  },
  {
    "text": "but when I ask the question it's going through and returning the documents and",
    "start": "2528960",
    "end": "2535079"
  },
  {
    "text": "then find summary so when I ask the the summary",
    "start": "2535079",
    "end": "2543760"
  },
  {
    "text": "question sec",
    "start": "2544800",
    "end": "2548800"
  },
  {
    "text": "I was printing it out earlier but I'll show you again when I'm trying to ask you the question I'm telling it you know you're a conversation specializing in",
    "start": "2552480",
    "end": "2560079"
  },
  {
    "text": "restaurants um summarize the conversation prioritize the mention and then you kind of have this you're",
    "start": "2560079",
    "end": "2565480"
  },
  {
    "text": "telling it almost the spec that you want it to to PR in so I wanted the summary I want the key restaurants relevant",
    "start": "2565480",
    "end": "2571760"
  },
  {
    "text": "details and topics so every time I go through the conversation I'm making this call behind the scenes",
    "start": "2571760",
    "end": "2577880"
  },
  {
    "text": "and continually building a history update so um yeah that's uh that's my",
    "start": "2577880",
    "end": "2584000"
  },
  {
    "text": "talk for the last part I'm happy to answer questions um again I I'll mention that if you're interested in in this",
    "start": "2584000",
    "end": "2592200"
  },
  {
    "text": "more information and you want to read and not listen me talk um if you go to to search Labs I'll leave this up here",
    "start": "2592200",
    "end": "2598839"
  },
  {
    "text": "while I answer questions again all these um I wrote this one there's jupyter notebooks",
    "start": "2598839",
    "end": "2606359"
  },
  {
    "start": "2600000",
    "end": "2631000"
  },
  {
    "text": "sample apps and all that all those things to kind of help you get more information up and running and and just general information so but yeah thanks",
    "start": "2606359",
    "end": "2612280"
  },
  {
    "text": "for everyone for coming and uh enjoy your time",
    "start": "2612280",
    "end": "2617640"
  }
]