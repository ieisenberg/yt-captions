[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "text": "[Applause] [Music]",
    "start": "1690",
    "end": "10639"
  },
  {
    "text": "um so yeah today I'll be talking about um some context and some practical",
    "start": "10639",
    "end": "17800"
  },
  {
    "text": "specifics about how we buildt stronger consistency guarantees into our existing key value store at",
    "start": "17800",
    "end": "25199"
  },
  {
    "text": "Twitter my name is Boaz um I am on the core storage team at Twitter uh we're",
    "start": "26840",
    "end": "32520"
  },
  {
    "text": "one of the teams uh that provides as a managed service storage to the engineers",
    "start": "32520",
    "end": "39719"
  },
  {
    "text": "at the company who are building their applications uh I'll give you a second to follow me on",
    "start": "39719",
    "end": "45039"
  },
  {
    "text": "Twitter um one of the ways that we do that and I've been doing that for a while now is with uh a database that we",
    "start": "45039",
    "end": "52320"
  },
  {
    "text": "call Manhattan it is our distributed uh key value store it is the primary uh key",
    "start": "52320",
    "end": "58920"
  },
  {
    "text": "value store at Twitter that we built from the ground up um about 4 years ago Manhattan didn't",
    "start": "58920",
    "end": "65040"
  },
  {
    "text": "exist and today it is the canonical store for a lot of very important data at the company things like tweets DMS",
    "start": "65040",
    "end": "73360"
  },
  {
    "text": "Advertiser data and a whole longtail uh of other",
    "start": "73360",
    "end": "78840"
  },
  {
    "text": "applications I'll give you some context uh about the database um among other reasons uh one of the things that we",
    "start": "78840",
    "end": "85520"
  },
  {
    "start": "82000",
    "end": "142000"
  },
  {
    "text": "built Manhattan for was to be very easy for developers to adopt storage and get",
    "start": "85520",
    "end": "91320"
  },
  {
    "text": "into production quickly with their services uh in order to do that we emphasize a few things uh we provide uh",
    "start": "91320",
    "end": "99159"
  },
  {
    "text": "self-service uh access to the database through a web interface we try to hide as many",
    "start": "99159",
    "end": "105439"
  },
  {
    "text": "infrastructure complexities as we can from our customers the other engineers at Twitter uh things like making",
    "start": "105439",
    "end": "111880"
  },
  {
    "text": "transparent Global replication across all of the regions of our data centers uh we allow customers to share space on",
    "start": "111880",
    "end": "119200"
  },
  {
    "text": "large clust ERS this reduces the provisioning time when you ask ask for storage and it reduces the burden on the",
    "start": "119200",
    "end": "125920"
  },
  {
    "text": "developers from having to set up and manage new clusters whenever someone has a new use case and also we provide some",
    "start": "125920",
    "end": "132160"
  },
  {
    "text": "Nifty automatic visibility uh and observability and alerting so that people have visibility to how they're",
    "start": "132160",
    "end": "138920"
  },
  {
    "text": "using the database now like with any uh successful",
    "start": "138920",
    "end": "145319"
  },
  {
    "start": "142000",
    "end": "201000"
  },
  {
    "text": "Service uh as it gets more popular and gets more adoption people get used to using it and they want to start using it",
    "start": "145319",
    "end": "152959"
  },
  {
    "text": "for use cases for which it wasn't exactly designed um in our case that",
    "start": "152959",
    "end": "158680"
  },
  {
    "text": "could mean um that someone who uh was using this eventually consistent key value store wants to start using it for",
    "start": "158680",
    "end": "166840"
  },
  {
    "text": "uh parts of their data that need stronger consistency guarantees when they try to do this uh",
    "start": "166840",
    "end": "172680"
  },
  {
    "text": "and they can't with our system it causes a lot of pain um it means that they have",
    "start": "172680",
    "end": "177879"
  },
  {
    "text": "to make trade-offs that they they don't want to make or that they're not happy with um and either they make those",
    "start": "177879",
    "end": "183959"
  },
  {
    "text": "trade-offs through using our system or through using a different system like in rdbms that doesn't provide some of the",
    "start": "183959",
    "end": "189519"
  },
  {
    "text": "guarantees that we provide so you as an engineer whose job it is uh to solve these problems for",
    "start": "189519",
    "end": "196040"
  },
  {
    "text": "your customers um are presented with a choice a choice that we see a lot which is do you build a new service or use an",
    "start": "196040",
    "end": "204080"
  },
  {
    "start": "201000",
    "end": "348000"
  },
  {
    "text": "existing uh new service alongside in parallel to the to the service that you",
    "start": "204080",
    "end": "209439"
  },
  {
    "text": "already have have to to solve these problems or do you build new features into the existing service",
    "start": "209439",
    "end": "215879"
  },
  {
    "text": "today now this is a scale and I don't need to tell you that you don't want to be on the far side of the scale uh where",
    "start": "215879",
    "end": "223159"
  },
  {
    "text": "you build bloated uh software that's not very good at anything um but also when",
    "start": "223159",
    "end": "228519"
  },
  {
    "text": "you're building infrastructure you don't always want to build services that are just too specific uh and there's a few",
    "start": "228519",
    "end": "236000"
  },
  {
    "text": "reasons for that one is um running multiple systems in production can be",
    "start": "236000",
    "end": "242360"
  },
  {
    "text": "really painful every system is going to have a different operational profile every system is going to have its own",
    "start": "242360",
    "end": "247879"
  },
  {
    "text": "little quirks and it's going to be painful for your on call Engineers to know every system in",
    "start": "247879",
    "end": "254120"
  },
  {
    "text": "depth another another is that for complex infrastructure a lot of things are going to be uh common between these",
    "start": "254120",
    "end": "260600"
  },
  {
    "text": "services for a database for instance um you're always going to need to manage your nodes you're always going to need",
    "start": "260600",
    "end": "266400"
  },
  {
    "text": "to move data around and you're going to have to either re build these components",
    "start": "266400",
    "end": "271440"
  },
  {
    "text": "for different systems or try to continuously abstract components out of",
    "start": "271440",
    "end": "276680"
  },
  {
    "text": "your systems which can have diminishing returns and probably the biggest reason",
    "start": "276680",
    "end": "282160"
  },
  {
    "text": "um that you don't want lots of separate systems is that I guarantee you that your customers do not want to talk to",
    "start": "282160",
    "end": "287880"
  },
  {
    "text": "five different databases with five different apis each one that provides a slightly different uh guarantee or",
    "start": "287880",
    "end": "295919"
  },
  {
    "text": "feature so in reality you want to be um somewhere here on the scale where you're",
    "start": "295919",
    "end": "302120"
  },
  {
    "text": "building specialized systems that are good at what they do um but they uh can",
    "start": "302120",
    "end": "307240"
  },
  {
    "text": "evolve in a logical and thoughtful way to provide more functionality to your customers so that's what we wanted to do",
    "start": "307240",
    "end": "314440"
  },
  {
    "text": "to solve this problem for our customers and uh the engineers at Twitter and uh in order to talk about",
    "start": "314440",
    "end": "320720"
  },
  {
    "text": "how we did that I'll give you guys an overview of the archit architecture of our eventually consistent uh database",
    "start": "320720",
    "end": "328240"
  },
  {
    "text": "I'm going to go pretty quickly over a lot of uh general concepts um if",
    "start": "328240",
    "end": "333800"
  },
  {
    "text": "you are familiar with distributed systems or with distributed databases a lot of this is going to look very familiar but uh if not feel free to ask",
    "start": "333800",
    "end": "341479"
  },
  {
    "text": "questions after the talk or find me at the conference as good a place to start as",
    "start": "341479",
    "end": "347960"
  },
  {
    "text": "any with the database is with the data model Um Manhattan as I said is a key value store and we split our keys into",
    "start": "347960",
    "end": "355280"
  },
  {
    "start": "348000",
    "end": "399000"
  },
  {
    "text": "two components a partition key or PE key and a local key or L key local keys are",
    "start": "355280",
    "end": "361800"
  },
  {
    "text": "stored uh together on the same machine underneath a partition key they have locality that's why we call them that uh",
    "start": "361800",
    "end": "369440"
  },
  {
    "text": "and that means that you can do uh interesting range type queries uh for instance in this data set if the P key",
    "start": "369440",
    "end": "376680"
  },
  {
    "text": "is a user ID you can say something like get me all of the L keys that start with the word profile for this particular",
    "start": "376680",
    "end": "383000"
  },
  {
    "text": "user and you would get the user's profile this should look familiar to you um if you've used Cassandra or Big T as",
    "start": "383000",
    "end": "389960"
  },
  {
    "text": "a row and column scheme or if you've used Dynamo DB as a hash key and a range",
    "start": "389960",
    "end": "398199"
  },
  {
    "text": "key so we take these Keys uh and we distribute them around the cluster the way we do this is by hashing the",
    "start": "398199",
    "end": "404759"
  },
  {
    "start": "399000",
    "end": "416000"
  },
  {
    "text": "partitioning key placing it into uh randomly into a Shard and then",
    "start": "404759",
    "end": "410120"
  },
  {
    "text": "replicating those shards and distributing them to the nodes in the cluster when you want to make a request",
    "start": "410120",
    "end": "418199"
  },
  {
    "start": "416000",
    "end": "458000"
  },
  {
    "text": "uh you send a client sends their request to one of many um stateless symmetrical",
    "start": "418199",
    "end": "424199"
  },
  {
    "text": "coordinator nodes those nodes take the request take the key from it find The",
    "start": "424199",
    "end": "429800"
  },
  {
    "text": "Shard that that key belongs to find the storage nodes that that Shard lives on",
    "start": "429800",
    "end": "435680"
  },
  {
    "text": "and then forwards those requests in parallel to all of those storage nodes waiting for a response that response",
    "start": "435680",
    "end": "442360"
  },
  {
    "text": "could be a success if it's a right uh it could be the latest value if it's a read",
    "start": "442360",
    "end": "449639"
  },
  {
    "text": "now when I say that the coordinator nodes forward this data in parallel to all of the storage nodes that should be",
    "start": "449639",
    "end": "456360"
  },
  {
    "text": "a giveaway immediately that I'm talking about a quorum based eventually consistent system specifically in this",
    "start": "456360",
    "end": "462800"
  },
  {
    "start": "458000",
    "end": "557000"
  },
  {
    "text": "case we use Dynamo style eventual consistency as in the Amazon Dynamo paper and we have last right wins",
    "start": "462800",
    "end": "469720"
  },
  {
    "text": "conflict resolution by doing this we can forward the requests in parallel to all of the",
    "start": "469720",
    "end": "476080"
  },
  {
    "text": "hosts and those requests can succeed and fail independ ly of each other without",
    "start": "476080",
    "end": "481240"
  },
  {
    "text": "necessarily affecting the overall success of the entire request and as long as you are making",
    "start": "481240",
    "end": "487840"
  },
  {
    "text": "requests to a quorum of hosts if that's important to you in this case that would be two out of",
    "start": "487840",
    "end": "493639"
  },
  {
    "text": "three and those requests succeed you're guaranteed to get certain consistency uh",
    "start": "493639",
    "end": "501479"
  },
  {
    "text": "levels such as read your own rights in this case so if these are rights to the database and one of them fails because",
    "start": "501479",
    "end": "508520"
  },
  {
    "text": "of a network partic or because of a rolling restart and subsequently you do a read from a database um and it Al the",
    "start": "508520",
    "end": "515760"
  },
  {
    "text": "read also succeeds against a quorum of hosts even if one of those hosts previously failed you're guaranteed to",
    "start": "515760",
    "end": "521120"
  },
  {
    "text": "see at least one version of the data that is the latest version that you had written and of course we can't call it",
    "start": "521120",
    "end": "529160"
  },
  {
    "text": "eventual consistency if we don't eventually make the data consistent across hosts so we have an anti-entropy",
    "start": "529160",
    "end": "535839"
  },
  {
    "text": "process that runs in the background called replica reconciliation this goes and compares the shards across",
    "start": "535839",
    "end": "542760"
  },
  {
    "text": "every replica making sure that they have the same data and if they don't it copies that data",
    "start": "542760",
    "end": "548959"
  },
  {
    "text": "over so these are all some pretty standard straightforward Concepts why would we choose to build a",
    "start": "548959",
    "end": "555959"
  },
  {
    "text": "system this way the primary benefit that you get from building a system like this is",
    "start": "555959",
    "end": "562160"
  },
  {
    "start": "557000",
    "end": "648000"
  },
  {
    "text": "super high availability because all of the storage nodes uh are equal to each other we",
    "start": "562160",
    "end": "569000"
  },
  {
    "text": "don't have a mastership relationship we don't have latency and availability hiccups from things like failover and",
    "start": "569000",
    "end": "576200"
  },
  {
    "text": "master re-election and because we chose to send the request in parallel to all of these",
    "start": "576200",
    "end": "582720"
  },
  {
    "text": "hosts this is a latency throughput trade-off you're essentially waiting for a best of n responses from these",
    "start": "582720",
    "end": "590200"
  },
  {
    "text": "replicas and you're getting the best possible latency that the system can provide at any",
    "start": "590200",
    "end": "595560"
  },
  {
    "text": "time and you also get Simplicity and the Simplicity is in",
    "start": "595560",
    "end": "600640"
  },
  {
    "text": "exchange for maybe a slightly less than intuitive consistency model um you get",
    "start": "600640",
    "end": "607360"
  },
  {
    "text": "last right wins or really with any kind of hands-off conflict",
    "start": "607360",
    "end": "612519"
  },
  {
    "text": "resolution a way to transparently move data a asynchronously replicate data",
    "start": "612519",
    "end": "618680"
  },
  {
    "text": "from one set of hosts to another and have that data be seamlessly integrated into the existing data in the system so",
    "start": "618680",
    "end": "625839"
  },
  {
    "text": "if I copy data over through replica reconciliation or if I replicate data over from a different",
    "start": "625839",
    "end": "631760"
  },
  {
    "text": "region that just combines with the data that's already on the system in a way",
    "start": "631760",
    "end": "637440"
  },
  {
    "text": "that makes our cross region replication story very nice very easy for people to use and you don't have to think about",
    "start": "637440",
    "end": "643600"
  },
  {
    "text": "how my cross region data is working in the database but as I said for some people",
    "start": "643600",
    "end": "651360"
  },
  {
    "start": "648000",
    "end": "797000"
  },
  {
    "text": "it's not quite enough when is it not enough um one use",
    "start": "651360",
    "end": "657000"
  },
  {
    "text": "case is much like in any kind of concurrent programming when you need a uniqueness or checkin set constraint in",
    "start": "657000",
    "end": "664959"
  },
  {
    "text": "the face of concurrent updates to the database so an example of that could be if two users are trying to both reserve",
    "start": "664959",
    "end": "672160"
  },
  {
    "text": "a username not only do I need uh only one of these users to ultimately succeed in",
    "start": "672160",
    "end": "679040"
  },
  {
    "text": "reserving this username I can't have races that result in both of them",
    "start": "679040",
    "end": "684880"
  },
  {
    "text": "thinking that they succeeded in reserving the username because this can break the business logic of your",
    "start": "684880",
    "end": "690399"
  },
  {
    "text": "application so for instance if both of these users tried to reserve they both saw that the uh username was free they",
    "start": "690399",
    "end": "697560"
  },
  {
    "text": "both uh said assign it to me if only if they both got a success on that",
    "start": "697560",
    "end": "703160"
  },
  {
    "text": "assignment but only the second person who happened to make that request",
    "start": "703160",
    "end": "708240"
  },
  {
    "text": "succeeded this is very um awkward behavior for your application to have to",
    "start": "708240",
    "end": "715480"
  },
  {
    "text": "overcome and it it also applies uh across regions globally so two people",
    "start": "715480",
    "end": "722519"
  },
  {
    "text": "might be trying to reserve a a username one of them is in Chicago one of them is in",
    "start": "722519",
    "end": "728000"
  },
  {
    "text": "Nairobi even if they're talking to different data centers they should still get these same",
    "start": "728000",
    "end": "734040"
  },
  {
    "text": "guarantees and in the same example you want one other thing which is atomicity of your",
    "start": "735320",
    "end": "741880"
  },
  {
    "text": "updates if I uh send an update to the database to reserve this username in the",
    "start": "741880",
    "end": "748199"
  },
  {
    "text": "case of failure I want it to fail everywhere in the case of success I want it to succeed everywhere I can't get",
    "start": "748199",
    "end": "754839"
  },
  {
    "text": "into a state in a failure scenario where a minority of hosts think that a",
    "start": "754839",
    "end": "760120"
  },
  {
    "text": "username is reserved and I can't guarantee a predictable read when I go back and try to see who has this",
    "start": "760120",
    "end": "766600"
  },
  {
    "text": "username this is a state that you can get into uh with eventual consistency",
    "start": "766600",
    "end": "772000"
  },
  {
    "text": "and that state will be resolved either in One Direction or the other depending on whether this existing host dies or",
    "start": "772000",
    "end": "778079"
  },
  {
    "text": "not so um it's unacceptable for some use",
    "start": "778079",
    "end": "784079"
  },
  {
    "text": "cases so now we know that we have this kind of genuine need for a stronger consistency",
    "start": "784079",
    "end": "790279"
  },
  {
    "text": "model how do we adapt our current architecture in order to support",
    "start": "790279",
    "end": "796959"
  },
  {
    "start": "797000",
    "end": "1270000"
  },
  {
    "text": "this we can add a new component into our request path which is logs specifically",
    "start": "799240",
    "end": "807560"
  },
  {
    "text": "sequential replicated distributed logs um in the form for us of Twitter",
    "start": "807560",
    "end": "814399"
  },
  {
    "text": "distributed log which is a recently open-sourced log Library built on top of a patchy",
    "start": "814399",
    "end": "820800"
  },
  {
    "text": "bookkeeper um the purpose of a sequential log is that all of the updates that go into this uh system into",
    "start": "820800",
    "end": "829040"
  },
  {
    "text": "this log are able to be read in the same order from by all consumers on the way",
    "start": "829040",
    "end": "835600"
  },
  {
    "text": "out it gets complicated when you replicate this log for dur ability and you distribute this log for",
    "start": "835600",
    "end": "842560"
  },
  {
    "text": "resilience and scalability and the log has to deal with Concepts like consensus uh or Master",
    "start": "842560",
    "end": "848759"
  },
  {
    "text": "ship with failover and fencing uh and it has to get all these complicated Concepts",
    "start": "848759",
    "end": "854639"
  },
  {
    "text": "right now this kind of component uh is becoming more and more popular uh in",
    "start": "854639",
    "end": "860240"
  },
  {
    "text": "lots of different kinds of infrastructure if you were at the last talk here uh then you saw Kafka which is",
    "start": "860240",
    "end": "865440"
  },
  {
    "text": "one uh type of distributed log um and so I won't talk about how these logs are",
    "start": "865440",
    "end": "871079"
  },
  {
    "text": "implemented but I will talk about how we use this log what are some of the",
    "start": "871079",
    "end": "876639"
  },
  {
    "text": "complexities of interaction when we introduce uh this new kind of component",
    "start": "876639",
    "end": "881759"
  },
  {
    "text": "into our request path and have this new kind of consistency",
    "start": "881759",
    "end": "887240"
  },
  {
    "text": "model so I said earlier that all of the keys are partitioned into shards this",
    "start": "888720",
    "end": "895440"
  },
  {
    "text": "Shard is this uh ultimate granular unit",
    "start": "895440",
    "end": "901000"
  },
  {
    "text": "of uh partitioning and distribution in the system so we can move shards around in between hosts what I can choose to",
    "start": "901000",
    "end": "908360"
  },
  {
    "text": "do is say that all operations that are going to update or act on a Shard have",
    "start": "908360",
    "end": "914360"
  },
  {
    "text": "to go through a log and I can make this log per Shard",
    "start": "914360",
    "end": "919639"
  },
  {
    "text": "infrastructure and then I can have all the storage nodes that own this Shard",
    "start": "919639",
    "end": "925000"
  },
  {
    "text": "consume all of their updates for that Shard from the log and I have this guarantee that all of those updates will",
    "start": "925000",
    "end": "931480"
  },
  {
    "text": "be guaranteed to be delivered and be delivered in order from the log and now once I have this I can make",
    "start": "931480",
    "end": "939839"
  },
  {
    "text": "these operations pretty much every anything that I want them to be they can be reads or writs as they were before or",
    "start": "939839",
    "end": "946519"
  },
  {
    "text": "I can start sending compare and set operations I can start sending increment operations I can send any kind of",
    "start": "946519",
    "end": "952519"
  },
  {
    "text": "operation that is deterministic based on the data that arrives in the operation",
    "start": "952519",
    "end": "957560"
  },
  {
    "text": "itself and the data that already exists in the database which is guaranteed to be the same on every storage node at the",
    "start": "957560",
    "end": "965399"
  },
  {
    "text": "time that that operation is processed so we can already see that by",
    "start": "965399",
    "end": "971440"
  },
  {
    "text": "introducing this we can solve some of the problems that we were worried about",
    "start": "971440",
    "end": "976519"
  },
  {
    "text": "before let's take the reservation example uh I can model a reservation of",
    "start": "976519",
    "end": "983000"
  },
  {
    "text": "a username as a checkin set operation where I say if a username is free assign",
    "start": "983000",
    "end": "989199"
  },
  {
    "text": "my user ID to that username and two users who are concurrently making this request will",
    "start": "989199",
    "end": "995560"
  },
  {
    "text": "appear in this log in some order which order doesn't really matter because they're doing it concurrently um and",
    "start": "995560",
    "end": "1001720"
  },
  {
    "text": "either order is valid but what's important is that all of the storage nodes all of the replicas that process",
    "start": "1001720",
    "end": "1008519"
  },
  {
    "text": "this request see the same order so what will end up happening is that the first",
    "start": "1008519",
    "end": "1013639"
  },
  {
    "text": "one that enters the log will be able to reserve that username and the second",
    "start": "1013639",
    "end": "1019880"
  },
  {
    "text": "one will fail its check condition and not reserve the username and see a happy",
    "start": "1019880",
    "end": "1027000"
  },
  {
    "text": "application Level error this kind of simple uh change to",
    "start": "1027000",
    "end": "1033760"
  },
  {
    "text": "the messaging model allows our guarantees to be strengthened quite a bit so let's zoom out a little",
    "start": "1033760",
    "end": "1040720"
  },
  {
    "text": "bit and see how this fits into the larger architecture now when a client sends a",
    "start": "1040720",
    "end": "1046959"
  },
  {
    "text": "request to a coordinator instead of forwarding that request uh directly to the storage nodes the coordinator",
    "start": "1046959",
    "end": "1053559"
  },
  {
    "text": "forwards it to the log that owns The Shard for which that key lives on and",
    "start": "1053559",
    "end": "1059480"
  },
  {
    "text": "then the storage nodes can consume from that log play back the operation and",
    "start": "1059480",
    "end": "1065960"
  },
  {
    "text": "respond with uh again the success uh if it's a write uh the value if it's a read",
    "start": "1065960",
    "end": "1073400"
  },
  {
    "text": "or the whatever the result is of your more comp complex operation back over to",
    "start": "1073400",
    "end": "1079400"
  },
  {
    "text": "the coordinator now if you",
    "start": "1079400",
    "end": "1084960"
  },
  {
    "text": "notice I have a new guarantee here that I didn't have before which is",
    "start": "1084960",
    "end": "1091760"
  },
  {
    "text": "that if the right enters the log I know that the that the storage nodes will see",
    "start": "1091760",
    "end": "1097280"
  },
  {
    "text": "it and if a storage node processes an operation I know that all the other storage nodes that process that",
    "start": "1097280",
    "end": "1103600"
  },
  {
    "text": "operation will return the same response because they have the same data so now I can start to do more",
    "start": "1103600",
    "end": "1110280"
  },
  {
    "text": "interesting things with my request model so that I can keep the same guarantees",
    "start": "1110280",
    "end": "1115720"
  },
  {
    "text": "that I want to have while lowering the latency uh on the client side for any given",
    "start": "1115720",
    "end": "1121840"
  },
  {
    "text": "request so for example let's say I know for sure that all of my rights and all",
    "start": "1121840",
    "end": "1127640"
  },
  {
    "text": "of my reads to an uh to my data set are going to go through this sequential log",
    "start": "1127640",
    "end": "1133559"
  },
  {
    "text": "well now I can say if I'm sending a right as soon as that right enters the log I'm done I know that that right will",
    "start": "1133559",
    "end": "1141200"
  },
  {
    "text": "be eventually be applied onto the storage nodes so I only need to wait for the log to act that it has this right",
    "start": "1141200",
    "end": "1148400"
  },
  {
    "text": "durable before I can return a success back to the client and I know that any subsequent read will be cued behind that",
    "start": "1148400",
    "end": "1155159"
  },
  {
    "text": "right in the log therefore it will see the right as I expected",
    "start": "1155159",
    "end": "1160520"
  },
  {
    "text": "to and when I send that read or if I send a more complex operation that needs a",
    "start": "1160520",
    "end": "1166000"
  },
  {
    "text": "result I only need to wait for one of the stor St nodes to respond because I know that all the other storage nodes",
    "start": "1166000",
    "end": "1172280"
  },
  {
    "text": "will respond with the same data so this makes uh certain kinds of",
    "start": "1172280",
    "end": "1178159"
  },
  {
    "text": "strong operations happen a little bit faster we either get to wait for the log response or the fastest response from a",
    "start": "1178159",
    "end": "1184559"
  },
  {
    "text": "host but I can still do something interesting I can say I was waiting for",
    "start": "1184559",
    "end": "1189880"
  },
  {
    "text": "a quorum of responses before when my eventually consistent model how about I",
    "start": "1189880",
    "end": "1195000"
  },
  {
    "text": "wait for a quorum of responses now so if I send a write or if I send a Cass or an",
    "start": "1195000",
    "end": "1200120"
  },
  {
    "text": "increment or something I'll wait for a quorum of responses to come back and if I do that I know that that uh request is",
    "start": "1200120",
    "end": "1209039"
  },
  {
    "text": "durable and consistent on the storage nodes themselves and when I send a subsequent",
    "start": "1209039",
    "end": "1216200"
  },
  {
    "text": "read because I didn't change my request path I only added a new option to the",
    "start": "1216200",
    "end": "1222600"
  },
  {
    "text": "path that requests can take in this database I can send a read through the normal eventually consistent fan out",
    "start": "1222600",
    "end": "1228919"
  },
  {
    "text": "path and when I do that I get all of the benefits that I was getting before from",
    "start": "1228919",
    "end": "1234159"
  },
  {
    "text": "an eventually consistent request uh High availability low latency without eating uh any of the",
    "start": "1234159",
    "end": "1241600"
  },
  {
    "text": "cost of going through the log um but I still get to do updates in",
    "start": "1241600",
    "end": "1246799"
  },
  {
    "text": "this more strongly consistent fashion so this is an interesting",
    "start": "1246799",
    "end": "1254280"
  },
  {
    "text": "model um and in a nutshell this is how we can add add uh this log component",
    "start": "1254280",
    "end": "1261320"
  },
  {
    "text": "into the database to add stronger consistency so let's see what are the results that come out of",
    "start": "1261320",
    "end": "1267400"
  },
  {
    "text": "this well first we've gained what it is that we were trying to gain because updates are ordered per",
    "start": "1267400",
    "end": "1274120"
  },
  {
    "start": "1270000",
    "end": "1583000"
  },
  {
    "text": "Shard and keys are hashed randomly into shards but consistently into shards we've effectively gained in order",
    "start": "1274120",
    "end": "1281200"
  },
  {
    "text": "updates on a per key basis linearizability for our keys um technically we have",
    "start": "1281200",
    "end": "1289039"
  },
  {
    "text": "in order updates for everything in The Shard but because you as a user don't know for sure if any two keys will end",
    "start": "1289039",
    "end": "1294679"
  },
  {
    "text": "up in the same Shard you don't really rely on that uh it's important to note that we",
    "start": "1294679",
    "end": "1300440"
  },
  {
    "text": "don't have full distributed transactions across shards that's a more complex",
    "start": "1300440",
    "end": "1306679"
  },
  {
    "text": "problem and it's something that would have to be built on top of this more basic system we also have or we can say we",
    "start": "1306679",
    "end": "1314320"
  },
  {
    "text": "kept failure isolation at The Shard level because all of our rights uh go through logs that are owned by",
    "start": "1314320",
    "end": "1320679"
  },
  {
    "text": "shards if we have a hiccup or an issue or a problem with an individual log that",
    "start": "1320679",
    "end": "1327240"
  },
  {
    "text": "problem is constrained to a very small sliver of the key space um that belongs to that",
    "start": "1327240",
    "end": "1333919"
  },
  {
    "text": "Shard and also because we've only added this New Path into our architecture instead of replacing it completely we",
    "start": "1333919",
    "end": "1340679"
  },
  {
    "text": "have this ability to do a couple things not only can we mix strong and eventually consistent data sets within",
    "start": "1340679",
    "end": "1348360"
  },
  {
    "text": "the the same cluster we can actually to some extent mix strong and eventually",
    "start": "1348360",
    "end": "1354080"
  },
  {
    "text": "consistent requests within the same data set and this is what I mean when I say",
    "start": "1354080",
    "end": "1360120"
  },
  {
    "text": "that we're providing flexible consistency levels a user can move back",
    "start": "1360120",
    "end": "1365600"
  },
  {
    "text": "and forth across um this Continuum between super high availability and",
    "start": "1365600",
    "end": "1372400"
  },
  {
    "text": "pretty strong per object linearizability by just modifying the kind of request",
    "start": "1372400",
    "end": "1379440"
  },
  {
    "text": "they make on a per request basis to the database and that's really powerful for our customers and allows you to get",
    "start": "1379440",
    "end": "1385760"
  },
  {
    "text": "exactly what you want into the system at exactly the time that you want it of course we made trade-offs first",
    "start": "1385760",
    "end": "1394240"
  },
  {
    "text": "trade off is an obvious one we've added a new component it's not a particularly simple component into the request path",
    "start": "1394240",
    "end": "1400520"
  },
  {
    "text": "and so on average we're going to add some amount of latency in the local data center that could be a few milliseconds",
    "start": "1400520",
    "end": "1406279"
  },
  {
    "text": "of latency if you're talking about um doing coordination across regions uh",
    "start": "1406279",
    "end": "1413679"
  },
  {
    "text": "that is constrained by the disappointingly slow speed of light and",
    "start": "1413679",
    "end": "1418720"
  },
  {
    "text": "so you're going to have uh an order of magnitude more latency added to your requests that have to coordinate far",
    "start": "1418720",
    "end": "1424600"
  },
  {
    "text": "across the globe we've also as much as we tried to isolate failures um exposed oursel to",
    "start": "1424600",
    "end": "1432159"
  },
  {
    "text": "this thing that we were trying to avoid which is latency hiccups so when there are some kind of problems or",
    "start": "1432159",
    "end": "1437440"
  },
  {
    "text": "re-elections on an indiv individual log that slow down the updates for an individual",
    "start": "1437440",
    "end": "1442720"
  },
  {
    "text": "Shard uh a customer will see in their High percentile latencies an increase",
    "start": "1442720",
    "end": "1448080"
  },
  {
    "text": "and that has to be okay for you that is a trade-off that you have to be okay with as an application",
    "start": "1448080",
    "end": "1453960"
  },
  {
    "text": "developer uh and finally an interesting one because we said that all of these",
    "start": "1453960",
    "end": "1460600"
  },
  {
    "text": "updates have to happen in order and they have to be applied in order to maintain the Integrity of the database and the",
    "start": "1460600",
    "end": "1466640"
  },
  {
    "text": "data on the database then we have the situation where if for",
    "start": "1466640",
    "end": "1474520"
  },
  {
    "text": "some reason uh replica cannot apply an operation that it reads from a stream",
    "start": "1474520",
    "end": "1480720"
  },
  {
    "text": "from a log um the only option that we have is",
    "start": "1480720",
    "end": "1486320"
  },
  {
    "text": "to completely halt operations from that log so why why might we not be able to apply an operation normally everything",
    "start": "1486320",
    "end": "1493279"
  },
  {
    "text": "should be applied but operating at scale we have to take into account that there might be bugs we have to take into",
    "start": "1493279",
    "end": "1500440"
  },
  {
    "text": "account that there might be some kind of exceptional corruption issue and that individual replica now",
    "start": "1500440",
    "end": "1506960"
  },
  {
    "text": "cannot continue to uh process uh operations from that log and we have a",
    "start": "1506960",
    "end": "1513039"
  },
  {
    "text": "form of unavailability this can be scary certainly sounds scary and it's where",
    "start": "1513039",
    "end": "1520080"
  },
  {
    "text": "things like um excellent observability excellent alerting excellent tooling",
    "start": "1520080",
    "end": "1526200"
  },
  {
    "text": "come into play to make sure that if this happens um because you have to account for all",
    "start": "1526200",
    "end": "1531440"
  },
  {
    "text": "possibilities we know about it immediately and we resolve it as quickly as",
    "start": "1531440",
    "end": "1538000"
  },
  {
    "text": "possible [Music] so that's what it looks like when we add",
    "start": "1540200",
    "end": "1546159"
  },
  {
    "text": "uh this new path in and um it's important now that you",
    "start": "1546159",
    "end": "1552520"
  },
  {
    "text": "have it to think about how is this new model that we've added different from the existing",
    "start": "1552520",
    "end": "1560159"
  },
  {
    "text": "model and how do we um use this new model and operate our clusters with this",
    "start": "1560159",
    "end": "1567279"
  },
  {
    "text": "new model in a way that isn't special or unique that just works with the system",
    "start": "1567279",
    "end": "1573399"
  },
  {
    "text": "that we already had so that for us and our customers we're adding something",
    "start": "1573399",
    "end": "1578840"
  },
  {
    "text": "without losing too much so when I think about how these",
    "start": "1578840",
    "end": "1584880"
  },
  {
    "start": "1583000",
    "end": "1903000"
  },
  {
    "text": "event this eventually consistent and this uh strongly consistent model are different the first thing that comes to",
    "start": "1584880",
    "end": "1590960"
  },
  {
    "text": "mind is where the authority of your data comes from in the Dynamo style",
    "start": "1590960",
    "end": "1596919"
  },
  {
    "text": "eventually consistent model the authority of your data is delivered alongside your",
    "start": "1596919",
    "end": "1603080"
  },
  {
    "text": "values in the form of a version uh or",
    "start": "1603080",
    "end": "1609720"
  },
  {
    "text": "timestamp this version information is what's applied to the back end and decides if the new data can override the",
    "start": "1609720",
    "end": "1616919"
  },
  {
    "text": "old data so in this example example we have values um we have a key with value X at version one already on these notes",
    "start": "1616919",
    "end": "1623960"
  },
  {
    "text": "and a new key is uh the uh update to that key is coming in with value Y at version two and the only reason reason",
    "start": "1623960",
    "end": "1631200"
  },
  {
    "text": "that that update is accepted is because the version is",
    "start": "1631200",
    "end": "1635720"
  },
  {
    "text": "newer this is interesting because it effectively um decouples the authority",
    "start": "1636720",
    "end": "1643159"
  },
  {
    "text": "of data from the order in which that data comes in uh it's gives us that conflict",
    "start": "1643159",
    "end": "1650720"
  },
  {
    "text": "resolution uh it's because of the conflict resolution it's what gives us the the nice properties that we got",
    "start": "1650720",
    "end": "1655960"
  },
  {
    "text": "before it's what gives us the item potent of the updates that go into the",
    "start": "1655960",
    "end": "1661039"
  },
  {
    "text": "database um but it's interesting to reason about uh even for the benefits",
    "start": "1661039",
    "end": "1667640"
  },
  {
    "text": "that it provides the new model is completely different the order of the updates is",
    "start": "1667640",
    "end": "1674880"
  },
  {
    "text": "all that matters uh for the data in the back ends and if I do want to keep around some",
    "start": "1674880",
    "end": "1682440"
  },
  {
    "text": "virsion information then uh I have to maintain that independently on every single host",
    "start": "1682440",
    "end": "1690840"
  },
  {
    "text": "and if I want that version information to be the same on uh across the hosts",
    "start": "1690840",
    "end": "1696679"
  },
  {
    "text": "which is something that I do want if I still want to be able to make those eventually consistent style Quorum",
    "start": "1696679",
    "end": "1702960"
  },
  {
    "text": "requests then I have to do that as an implementation detail of how I maintain version",
    "start": "1702960",
    "end": "1708919"
  },
  {
    "text": "because it's not an intrinsic property of the system that the data has",
    "start": "1708919",
    "end": "1714519"
  },
  {
    "text": "versioning and when I'm applying this data I have to store it locally so that",
    "start": "1714519",
    "end": "1719720"
  },
  {
    "text": "I know that I'm applying everything exactly once I have to store where I am in the",
    "start": "1719720",
    "end": "1727519"
  },
  {
    "text": "logs so how does this",
    "start": "1727519",
    "end": "1731559"
  },
  {
    "text": "manifest one interesting thing that comes out of this is that because I have",
    "start": "1733679",
    "end": "1739320"
  },
  {
    "text": "this guarantee that I will apply all of the data that comes in on the log in this order that it comes in I have no",
    "start": "1739320",
    "end": "1746559"
  },
  {
    "text": "more need for this replica reconciliation process in fact if I was to go and",
    "start": "1746559",
    "end": "1753159"
  },
  {
    "text": "compare the data on shards and try to proactively correct the data that exists on the shards I would be introducing",
    "start": "1753159",
    "end": "1758640"
  },
  {
    "text": "more consistency issues than I would be solving because of the way the system",
    "start": "1758640",
    "end": "1764840"
  },
  {
    "text": "works so that's cool but it also turns out that we still have a need for a very",
    "start": "1764840",
    "end": "1770760"
  },
  {
    "text": "similar type of system that goes through and compares the data between hosts because when you have a cluster with",
    "start": "1770760",
    "end": "1777480"
  },
  {
    "text": "terabytes upon terabytes of data in it you have to constantly be checking for things like bit rot for um potential",
    "start": "1777480",
    "end": "1785640"
  },
  {
    "text": "bugs and potentially correcting them so it's a savings but not an",
    "start": "1785640",
    "end": "1791440"
  },
  {
    "text": "enormous savings another thing that Chang changes",
    "start": "1791440",
    "end": "1799919"
  },
  {
    "text": "um is when you're doing data migrations so at Twitter like at most places when",
    "start": "1799919",
    "end": "1806080"
  },
  {
    "text": "we build a new system we generally want to take advantage of the benefits uh that we built it for by migrating some",
    "start": "1806080",
    "end": "1812799"
  },
  {
    "text": "services from the old systems that they were on onto the new system and to do this for a database uh",
    "start": "1812799",
    "end": "1820120"
  },
  {
    "text": "and to keep that service live and not have any downtime what you generally do is send",
    "start": "1820120",
    "end": "1827559"
  },
  {
    "text": "requests especially WR requests that are coming into that datab uh that are coming into the service to both",
    "start": "1827559",
    "end": "1834519"
  },
  {
    "text": "databases and then in the background you backfill old information from the old database system to the new",
    "start": "1834519",
    "end": "1841120"
  },
  {
    "text": "one in an eventually consistent uh key value store with conflict resolution",
    "start": "1841120",
    "end": "1848240"
  },
  {
    "text": "this turns out to be pretty trivial to do you simply send rights to both of these systems you copy the data over",
    "start": "1848240",
    "end": "1855679"
  },
  {
    "text": "however you want to copy it the conflict the conflict resolution takes care of everything for you when you're done",
    "start": "1855679",
    "end": "1861399"
  },
  {
    "text": "copying you can just switch your reads over to the new system pretty simple for the more strongly consistent",
    "start": "1861399",
    "end": "1868399"
  },
  {
    "text": "model this is no longer the case if I want to do something like move an application that was on my SQL or an",
    "start": "1868399",
    "end": "1874880"
  },
  {
    "text": "rdbms onto Manhattan we might have operations",
    "start": "1874880",
    "end": "1880320"
  },
  {
    "text": "running that rely on the data that exists in the system while also updating that data things like compare and set so",
    "start": "1880320",
    "end": "1887960"
  },
  {
    "text": "so I have to write extra code on a per application basis to make sure that the",
    "start": "1887960",
    "end": "1894159"
  },
  {
    "text": "data is populated correctly between the two systems and that we maintain integrity and",
    "start": "1894159",
    "end": "1900559"
  },
  {
    "start": "1903000",
    "end": "2050000"
  },
  {
    "text": "correctness a change that we had to make in the same vein is to the process of",
    "start": "1904039",
    "end": "1909720"
  },
  {
    "text": "how we add and remove nodes from the cluster um which is something that we",
    "start": "1909720",
    "end": "1915720"
  },
  {
    "text": "call a topology transition so again an eventually consistent system",
    "start": "1915720",
    "end": "1921519"
  },
  {
    "text": "doing a topology transition moving A Shard from an old set of hosts to a new set of hosts is pretty",
    "start": "1921519",
    "end": "1926639"
  },
  {
    "text": "easy you start directing rights that belong on that Shard to the new replica",
    "start": "1926639",
    "end": "1931760"
  },
  {
    "text": "set as well as the old one you snapshot the data you copy it over when it's done",
    "start": "1931760",
    "end": "1936799"
  },
  {
    "text": "being copied over you move all of the wrs and reads to the new replica",
    "start": "1936799",
    "end": "1943320"
  },
  {
    "text": "set because order doesn't matter this works out uh on its",
    "start": "1944159",
    "end": "1950480"
  },
  {
    "text": "own but with the more strongly consistent model you have to think a little bit",
    "start": "1950480",
    "end": "1955559"
  },
  {
    "text": "harder because you can't just start sending rights to the new set of hosts you have to um change the topology",
    "start": "1955559",
    "end": "1966279"
  },
  {
    "text": "State machine the transition state machine that manages the addition of",
    "start": "1966279",
    "end": "1971399"
  },
  {
    "text": "hosts to accommodate this these new components so the first thing you do now",
    "start": "1971399",
    "end": "1977559"
  },
  {
    "text": "is say instead of accepting rights just say I have a new set of replicas in the cluster and I care about the updates",
    "start": "1977559",
    "end": "1984880"
  },
  {
    "text": "that are coming in off of this log for this chard so at the very least don't delete them from the log before I have a",
    "start": "1984880",
    "end": "1991159"
  },
  {
    "text": "chance to read them then you snapshot the data and copy it over as",
    "start": "1991159",
    "end": "1996440"
  },
  {
    "text": "normal and then you add this new state to the state machine which for",
    "start": "1996440",
    "end": "2001919"
  },
  {
    "text": "eventually consistent data is a noop but for strongly consistent data that reads from a log says give me a chance to",
    "start": "2001919",
    "end": "2009240"
  },
  {
    "text": "catch up all of the updates that I missed from the log from the point where I got the",
    "start": "2009240",
    "end": "2015679"
  },
  {
    "text": "snapshot you need to do this catch up before you start accepting requests otherwise as soon as you start",
    "start": "2015679",
    "end": "2022960"
  },
  {
    "text": "accepting requests from these hosts uh and saying that they're trying to listen",
    "start": "2022960",
    "end": "2028519"
  },
  {
    "text": "for the responses that these hosts send back to the coordinator you're suddenly will be taking a request that was taking 10",
    "start": "2028519",
    "end": "2036039"
  },
  {
    "text": "milliseconds and making it take multiple minutes or however long it takes for",
    "start": "2036039",
    "end": "2041240"
  },
  {
    "text": "these nodes to catch up to the current state on the",
    "start": "2041240",
    "end": "2046679"
  },
  {
    "start": "2050000",
    "end": "2249000"
  },
  {
    "text": "log one last change that we made was in how we in how we implemented",
    "start": "2050839",
    "end": "2059760"
  },
  {
    "text": "ttls so a TTL or a time to live uh is a really useful feature where you can",
    "start": "2059760",
    "end": "2066280"
  },
  {
    "text": "indicate that a value should should no longer be readable after a certain point in",
    "start": "2066280",
    "end": "2071398"
  },
  {
    "text": "time you might use a TTL because you're collecting some kind of user data and",
    "start": "2071399",
    "end": "2076720"
  },
  {
    "text": "it's not relevant anymore after a few days so you don't want to be able to read it you want it to get cleaned up",
    "start": "2076720",
    "end": "2082040"
  },
  {
    "text": "you might use it because a user is uh active for 30 days or something so you",
    "start": "2082040",
    "end": "2088480"
  },
  {
    "text": "want to when the user logs in you'll say uh this user is active set the TTL for 30 days automatically after 30 days the",
    "start": "2088480",
    "end": "2095240"
  },
  {
    "text": "user won't look active anymore if they haven't loed again",
    "start": "2095240",
    "end": "2100800"
  },
  {
    "text": "so in the eventually consistent model it's implemented in a pretty straightforward way if you read a piece",
    "start": "2100800",
    "end": "2106800"
  },
  {
    "text": "of data from a node and that node thinks that that piece of data has expired it simply doesn't return it to",
    "start": "2106800",
    "end": "2113040"
  },
  {
    "text": "you this is going to be subject to the normal amount of clock skew that happens",
    "start": "2113040",
    "end": "2118240"
  },
  {
    "text": "across nodes in a data center but it turns out that for our eventually consistent model that's okay it doesn't",
    "start": "2118240",
    "end": "2124520"
  },
  {
    "text": "break anything everything still works as expect it to",
    "start": "2124520",
    "end": "2130359"
  },
  {
    "text": "work but of course for a strongly consistent model it's no longer okay because the expiration of this",
    "start": "2131000",
    "end": "2137560"
  },
  {
    "text": "data is in data manipulating event just like any other and we know that all",
    "start": "2137560",
    "end": "2143960"
  },
  {
    "text": "these such events have to happen in the same order so you can imagine relying on the",
    "start": "2143960",
    "end": "2151000"
  },
  {
    "text": "clock of the node and these nodes having clock SK and doing something like a",
    "start": "2151000",
    "end": "2156119"
  },
  {
    "text": "check and set operation and if on some nodes you're doing a check and set operation against the",
    "start": "2156119",
    "end": "2161839"
  },
  {
    "text": "value that exists and on some nodes you're doing it against the value that the node thinks doesn't",
    "start": "2161839",
    "end": "2167760"
  },
  {
    "text": "exist you've now introduced a Divergence into the into your data you have an",
    "start": "2167760",
    "end": "2173280"
  },
  {
    "text": "inconsistency between your replicas and this is exactly the case that we were trying to",
    "start": "2173280",
    "end": "2179599"
  },
  {
    "text": "avoid so in order to get around that we decided to no longer trust an individual",
    "start": "2179599",
    "end": "2186880"
  },
  {
    "text": "con no concept of the time instead of relying on the noes",
    "start": "2186880",
    "end": "2193119"
  },
  {
    "text": "clock we send with every operation a time stamp indicating what time the node",
    "start": "2193119",
    "end": "2199520"
  },
  {
    "text": "should think it is we can do this because uh our distributed log architecture allows a",
    "start": "2199520",
    "end": "2206400"
  },
  {
    "text": "single Master a single writer for every log and so we can ensure that this concept of time that this clock is",
    "start": "2206400",
    "end": "2213119"
  },
  {
    "text": "always monotonically increasing so when we send uh each",
    "start": "2213119",
    "end": "2219960"
  },
  {
    "text": "operation the node updates its concept of time and now only across operation",
    "start": "2219960",
    "end": "2225240"
  },
  {
    "text": "boundaries can data be expired due to TTL and that way we guarantee that all",
    "start": "2225240",
    "end": "2231800"
  },
  {
    "text": "these things happen in the same order it's a pretty interesting solution",
    "start": "2231800",
    "end": "2237280"
  },
  {
    "text": "uh it's uh a very popular use of our database and it's not generally TTL is not something that you see in more",
    "start": "2237280",
    "end": "2243400"
  },
  {
    "text": "strongly consistent systems",
    "start": "2243400",
    "end": "2247160"
  },
  {
    "start": "2249000",
    "end": "2380000"
  },
  {
    "text": "so um one last thing I'll say is that at the beginning of the talk uh we",
    "start": "2250480",
    "end": "2256079"
  },
  {
    "text": "discussed what it is that the users of our database wanted and in order to support what they",
    "start": "2256079",
    "end": "2263800"
  },
  {
    "text": "wanted we made all of these changes um to the back end of the database we",
    "start": "2263800",
    "end": "2268920"
  },
  {
    "text": "introduced two quite differing consistency models living side by side and the question is did we succeed",
    "start": "2268920",
    "end": "2277119"
  },
  {
    "text": "in add in functionality without disrupting the user's",
    "start": "2277119",
    "end": "2282480"
  },
  {
    "text": "experience so in order to use this uh new mode of the database all the user",
    "start": "2282480",
    "end": "2287839"
  },
  {
    "text": "has to do is when provisioning the data set in the web UI they choose a stronger consistency",
    "start": "2287839",
    "end": "2294240"
  },
  {
    "text": "type and when making a request through our client they set a stronger consistency",
    "start": "2294240",
    "end": "2301359"
  },
  {
    "text": "guarantee or if they're doing an operation that didn't exist before like a compare and set they can use a simple",
    "start": "2301359",
    "end": "2307520"
  },
  {
    "text": "API for that and not just that if they try to do",
    "start": "2307520",
    "end": "2313160"
  },
  {
    "text": "something that we don't think is safe to do against this data for instance if",
    "start": "2313160",
    "end": "2319079"
  },
  {
    "text": "they send a request that is trying to update in a fan out eventually",
    "start": "2319079",
    "end": "2324359"
  },
  {
    "text": "consistent way a data set that's strongly consistent it's trying to send a right",
    "start": "2324359",
    "end": "2330480"
  },
  {
    "text": "not through the replicated log we inspect that request we decide that it's",
    "start": "2330480",
    "end": "2336319"
  },
  {
    "text": "not safe and and then we send back a very long but clear error message saying",
    "start": "2336319",
    "end": "2342160"
  },
  {
    "text": "what it is they're trying to do why that's not okay and what they might do",
    "start": "2342160",
    "end": "2347520"
  },
  {
    "text": "instead and in this way we maintain one of the most important things uh about",
    "start": "2347520",
    "end": "2353440"
  },
  {
    "text": "this database that we were trying to achieve when we built it which is that it's very usable and easy for people so",
    "start": "2353440",
    "end": "2359440"
  },
  {
    "text": "that they can adopt quickly without bringing our team into the loop for most issues",
    "start": "2359440",
    "end": "2367000"
  },
  {
    "text": "thank you very much [Music] [Applause]",
    "start": "2370079",
    "end": "2378420"
  }
]