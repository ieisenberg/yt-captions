[
  {
    "text": "so I've been doing big data consulting for a while before i joined typesafe to",
    "start": "6010",
    "end": "11059"
  },
  {
    "text": "work on tools like spark and I pretty real I realized pretty quickly that's the tools were terrible so I started",
    "start": "11059",
    "end": "16880"
  },
  {
    "text": "trolling the community back in you know 2012 or so when I started back in those",
    "start": "16880",
    "end": "22279"
  },
  {
    "text": "days typically if you wanted to do big data science at scale what you would",
    "start": "22279",
    "end": "28160"
  },
  {
    "text": "typically do is like you create your models in Python and so forth and then throw them over the wall to some Java",
    "start": "28160",
    "end": "34250"
  },
  {
    "text": "developers to write MapReduce code which kind of sucked and I didn't talk about",
    "start": "34250",
    "end": "40219"
  },
  {
    "text": "three years ago at the Northeast Scala symposium where I argued that big data needs to be functional and what I meant",
    "start": "40219",
    "end": "46850"
  },
  {
    "text": "by that is that he you know i love what Martin and todd said this morning about",
    "start": "46850",
    "end": "52850"
  },
  {
    "text": "functional programming it can be a bit religious and dogmatic at times or at least certain people in the community but it does give us the right",
    "start": "52850",
    "end": "59629"
  },
  {
    "text": "abstractions for working with data and thinking of it as immutable values that we transform and so forth and it turns",
    "start": "59629",
    "end": "66110"
  },
  {
    "text": "out that SQL is actually a functional language of sorts and that's always",
    "start": "66110",
    "end": "71270"
  },
  {
    "text": "served us really well so you know first of all you might say well why use the",
    "start": "71270",
    "end": "76280"
  },
  {
    "text": "JVM well you know I think we all know that it's it's a well-established platform it's it does scale extremely",
    "start": "76280",
    "end": "82939"
  },
  {
    "text": "well it's just a lot of tools we've had for a while different programming languages now like Scala and closure in",
    "start": "82939",
    "end": "89299"
  },
  {
    "text": "addition to Java those are math libraries on the right for in Scala algebra inspire that I won't really talk",
    "start": "89299",
    "end": "95540"
  },
  {
    "text": "about and then we've got you know mature development environments and these days",
    "start": "95540",
    "end": "101090"
  },
  {
    "text": "we've got a very mature ecosystem of tools mostly around the Hadoop ecosystem but not necessarily all of them that let",
    "start": "101090",
    "end": "108170"
  },
  {
    "text": "us you know approach different kinds of data problems at scale you know there's",
    "start": "108170",
    "end": "113570"
  },
  {
    "text": "just a laundry list of some of them here we're going to talk about spark a little bit a predecessor to spark that was very",
    "start": "113570",
    "end": "119329"
  },
  {
    "text": "influential was scalding at Twitter wrote which sat on top of MapReduce and that birds actually there's their",
    "start": "119329",
    "end": "126530"
  },
  {
    "text": "summing bird which is a tool that abstract / scalding and storm so mostly",
    "start": "126530",
    "end": "133430"
  },
  {
    "text": "what I'm going to talk about is in the context of hit do but it's not specific to Hadoop areli but it sort of sets the",
    "start": "133430",
    "end": "138860"
  },
  {
    "text": "stage for what we have to do at scale I think actually how many of you have actually used to do before okay most of you so",
    "start": "138860",
    "end": "145849"
  },
  {
    "text": "I'm not going to spend a lot of time talking about hit do because it's mostly familiar but just to give you a sense is",
    "start": "145849",
    "end": "151069"
  },
  {
    "text": "kind of you know how you would approach data science at scale and very schematically Hadoop cluster looks",
    "start": "151069",
    "end": "157519"
  },
  {
    "text": "something like this where you have masternodes that that own the the jobs",
    "start": "157519",
    "end": "163340"
  },
  {
    "text": "that you're going to run on the on the system so for example this resource manager is the current a dupe way of",
    "start": "163340",
    "end": "169610"
  },
  {
    "text": "managing a job that you submit it figures out when it has resources to run your job and it breaks it into tasks",
    "start": "169610",
    "end": "176239"
  },
  {
    "text": "that are run in the slave nodes and it delegates to these node managers you know the actual execution and monitoring",
    "start": "176239",
    "end": "183049"
  },
  {
    "text": "of your tasks locally on each machine and then underneath Hadoop has always been this distributed file system called",
    "start": "183049",
    "end": "189290"
  },
  {
    "text": "HDFS for Hadoop distributed file system it has its own services the name note is the master for the whole file system and",
    "start": "189290",
    "end": "196220"
  },
  {
    "text": "those all the metadata about files and then it has data notes that manage all the blocks that are on hard drives",
    "start": "196220",
    "end": "202250"
  },
  {
    "text": "around the cluster and you know traditionally you would write MapReduce jobs that you submit to the cluster",
    "start": "202250",
    "end": "208670"
  },
  {
    "text": "through to the resource manager today and they would then be scheduled and run",
    "start": "208670",
    "end": "213739"
  },
  {
    "text": "and that's great if you can write MapReduce code but it's not really such an ideal scenario for a lot of people so",
    "start": "213739",
    "end": "221120"
  },
  {
    "text": "what does MapReduce how does it work briefly very conceptually one of the examples I'm going to go through but not",
    "start": "221120",
    "end": "226970"
  },
  {
    "text": "actually in MapReduce I'll talk about it in the context of spark is a fairly famous algorithm called the inverted",
    "start": "226970",
    "end": "232940"
  },
  {
    "text": "index and it's it's sort of what you do is a minimal implementation for something like a search appliance or",
    "start": "232940",
    "end": "239989"
  },
  {
    "text": "Google or whatever which is you know I have all these pages on the internet or some document repository I want to",
    "start": "239989",
    "end": "248230"
  },
  {
    "text": "create an index they call it reverse index or inverted because I'm going to find all the words in this document and",
    "start": "248230",
    "end": "254900"
  },
  {
    "text": "build an index of word to all of the locations that word can be found and I even want to know the counts because",
    "start": "254900",
    "end": "261440"
  },
  {
    "text": "obviously I'm probably more interested in documents that talk in great depth about a topic not ones that mention it",
    "start": "261440",
    "end": "268099"
  },
  {
    "text": "in passing so we would like to calculate this if you actually want to see sort of the",
    "start": "268099",
    "end": "273979"
  },
  {
    "text": "particulars of how this works in MapReduce I have some bonus slides at the end of the talk that I didn't want",
    "start": "273979",
    "end": "279229"
  },
  {
    "text": "to go through now but the idea is that I'm just going to pass it through some miracle some cluster and then it's going",
    "start": "279229",
    "end": "286639"
  },
  {
    "text": "to pop out the other end and what has to happen as a precursor step is we're",
    "start": "286639",
    "end": "291889"
  },
  {
    "text": "going to have web crawlers that go around the internet and find all these documents and write an initial data set",
    "start": "291889",
    "end": "297020"
  },
  {
    "text": "that has as it like a two column table if you will the location where it was",
    "start": "297020",
    "end": "303080"
  },
  {
    "text": "found or some ID representing that location and then the contents and then once again the inverted index that comes",
    "start": "303080",
    "end": "309680"
  },
  {
    "text": "out the other end will be indexed by words and then a list of places where",
    "start": "309680",
    "end": "314719"
  },
  {
    "text": "the word is found in it you know frequency and just to zoom in a little",
    "start": "314719",
    "end": "319819"
  },
  {
    "text": "bit this is what you know kind of the data would look like if we were crawling wikipedia it might look something like this but I won't I don't want to belabor",
    "start": "319819",
    "end": "327559"
  },
  {
    "text": "the point too much and the output would yeah I forgot about this of bill so",
    "start": "327559",
    "end": "333469"
  },
  {
    "text": "anyway getting onto the issue the problem with MapReduce that we ran into and the reason I was you know trolling",
    "start": "333469",
    "end": "338839"
  },
  {
    "text": "it a couple years ago was it's actually really hard to map problems down to a low level very coarse-grained you have",
    "start": "338839",
    "end": "346249"
  },
  {
    "text": "this map step that does initial processing and a reduced step that takes that temporary data and you know",
    "start": "346249",
    "end": "351879"
  },
  {
    "text": "collapses it down to your final result that again I skipped over the details for time you can find them at the end of",
    "start": "351879",
    "end": "358550"
  },
  {
    "text": "the talk if you want and this this is one of the reasons why most data scientist never bothered really learning",
    "start": "358550",
    "end": "364089"
  },
  {
    "text": "MapReduce because it was a waste of their time to master the arcana of such a low level system instead you know they",
    "start": "364089",
    "end": "371329"
  },
  {
    "text": "would let Java experts Java engineers worry about that it really actually became a sub specialty is knowing how to",
    "start": "371329",
    "end": "378769"
  },
  {
    "text": "write MapReduce code how to actually use that API so in fact if you actually go",
    "start": "378769",
    "end": "385069"
  },
  {
    "text": "through the Java code it looks something like this I know you can't read this at six point font but it's it's like",
    "start": "385069",
    "end": "390800"
  },
  {
    "text": "something like I think even a way what was it like 80 lines of code but it was very detailed arcane code the kind of",
    "start": "390800",
    "end": "397789"
  },
  {
    "text": "stuff he would spend hours writing unit tests thoroughly that was another thing was hard to do in MapReduce not",
    "start": "397789",
    "end": "404120"
  },
  {
    "text": "something that you really want to do for what is conceptual a very simple algorithm so we want to see a better way to do that in one ways",
    "start": "404120",
    "end": "412610"
  },
  {
    "text": "is well can we layer some higher level abstractions on top of this to at least make it easier for people to work in",
    "start": "412610",
    "end": "418310"
  },
  {
    "text": "conceptually in like data flows or sequel and the first one that became",
    "start": "418310",
    "end": "423350"
  },
  {
    "text": "extremely popular was a sequel query tool that would generate MapReduce jobs",
    "start": "423350",
    "end": "428360"
  },
  {
    "text": "for you where you're basically using sequel as a domain-specific language for the problem and you could write things",
    "start": "428360",
    "end": "434930"
  },
  {
    "text": "like this create some table load data into it and then right you know select queries on the data it wasn't a",
    "start": "434930",
    "end": "441320"
  },
  {
    "text": "transactional database what's still around so it's not like in past tense it was really purely for putting the",
    "start": "441320",
    "end": "447710"
  },
  {
    "text": "queries back in SQL and it wasn't designed to like do record level stuff it was designed for table scans I love",
    "start": "447710",
    "end": "454310"
  },
  {
    "text": "this tool so much even though I don't consider myself a sequel person that i co-wrote a book on it because I wanted to make sure people knew how to use it",
    "start": "454310",
    "end": "460790"
  },
  {
    "text": "because it was so useful for sort of the 8020 problem of the kind of things people did for at least basic analytics",
    "start": "460790",
    "end": "467870"
  },
  {
    "text": "but the problem with it was that you had to write any extensions that you might want you so called user-defined",
    "start": "467870",
    "end": "473750"
  },
  {
    "text": "functions and Java which again brought you back to working with the engineers and figuring how to deploy this and all",
    "start": "473750",
    "end": "479510"
  },
  {
    "text": "this kind of stuff you couldn't just like embed a Java function or class inside your hive query and then just",
    "start": "479510",
    "end": "486920"
  },
  {
    "text": "have it work you had to go through a lot of rigmarole to extend hive there was another tool that got popular called Pig",
    "start": "486920",
    "end": "493700"
  },
  {
    "text": "this is written at Yahoo and the name was sort of an accident and just kind of stuck around and the dialect is called",
    "start": "493700",
    "end": "498950"
  },
  {
    "text": "Pig Latin okay this is a dataflow tool it's it's designed a little better for",
    "start": "498950",
    "end": "505100"
  },
  {
    "text": "the sort of scenario where I'm going to just walk data through a series of steps of transformation may be forking the",
    "start": "505100",
    "end": "511310"
  },
  {
    "text": "data and you know they have two pipelines that are doing different things this is actually basically the",
    "start": "511310",
    "end": "516710"
  },
  {
    "text": "same thing i just showed a simple situation where i load some data and I project out just one column so to speak",
    "start": "516710",
    "end": "524420"
  },
  {
    "text": "of that data pig is kind of a strange language in terms of its not familiar if you know sequel it doesn't kind of look",
    "start": "524420",
    "end": "530660"
  },
  {
    "text": "like Java and it's not turing-complete so once again you are kind of stuck with a situation of you know if I want to",
    "start": "530660",
    "end": "536600"
  },
  {
    "text": "extend it I have to write code in another language in a link attend although in this case you could use Python and Ruby even and JavaScript if",
    "start": "536600",
    "end": "544430"
  },
  {
    "text": "anything that could be run is in a JVM that you could actually use you weren't just limited to Java which was nice but",
    "start": "544430",
    "end": "552170"
  },
  {
    "text": "my favorite in this early stage again around the 2012 time frame was a this",
    "start": "552170",
    "end": "557860"
  },
  {
    "text": "scalding library that yes written by by Twitter it's actually a scholar language",
    "start": "557860",
    "end": "564200"
  },
  {
    "text": "and I'll just show you a quick code example of this that actually was sitting on top of another nice API",
    "start": "564200",
    "end": "569750"
  },
  {
    "text": "called cascading which gave you more data flow abstractions even though it still had kind of the verbosity of pre",
    "start": "569750",
    "end": "576590"
  },
  {
    "text": "Java 8 joven by that I mean it really could have used lambda sore anonymous",
    "start": "576590",
    "end": "582830"
  },
  {
    "text": "functions which we now have in Java but this was you know again several years ago and that would abstract a lot of the",
    "start": "582830",
    "end": "588920"
  },
  {
    "text": "hassles a lot of the low-level details of writing MapReduce code so that you",
    "start": "588920",
    "end": "594470"
  },
  {
    "text": "could write very high level language kind of stuff and this is what it looks like in scalding I won't go through all",
    "start": "594470",
    "end": "601100"
  },
  {
    "text": "the details really I want you to get a sense of how concise this code is that I've suddenly taken a software",
    "start": "601100",
    "end": "606470"
  },
  {
    "text": "engineering problem and turned it into a script a lot of people when you say the word script they think you know that's",
    "start": "606470",
    "end": "612650"
  },
  {
    "text": "kind of throw away you know real developers don't use scripting languages you know you kind of hear that mindset",
    "start": "612650",
    "end": "618110"
  },
  {
    "text": "instead they write hundreds and hundreds of lines java code or whatever but the beauty part about this is you know i can",
    "start": "618110",
    "end": "624650"
  },
  {
    "text": "very quickly read this and once I know the API it's got a few little idiosyncrasies and know the kind of",
    "start": "624650",
    "end": "631310"
  },
  {
    "text": "concepts that go back to functional programming of like mapping and filtering and flat mapping and what all",
    "start": "631310",
    "end": "636560"
  },
  {
    "text": "those mean and i'll explain flat mapping in a little bit if you don't know what it means but basically i'm going to load",
    "start": "636560",
    "end": "642320"
  },
  {
    "text": "some tab-separated data i'm going to split it into words this would be like the contents of my Wikipedia pages and",
    "start": "642320",
    "end": "650300"
  },
  {
    "text": "I'm going to spit out some new tuples and you can write tuples literally with parentheses in Scala it'll have a word",
    "start": "650300",
    "end": "656180"
  },
  {
    "text": "and the ID so I've already done a flip right there came in with an ID and then text now it's word an ID and then the",
    "start": "656180",
    "end": "663800"
  },
  {
    "text": "last steps where I do group by is where I bring all the common words together and do final counts to get the inverted",
    "start": "663800",
    "end": "669680"
  },
  {
    "text": "index and the count on each document actually i forgot i'm actually not do the counts here when you see the spark",
    "start": "669680",
    "end": "675920"
  },
  {
    "text": "example it's actually a little more complicated than this or let's yeah complicated is a good word only because",
    "start": "675920",
    "end": "682190"
  },
  {
    "text": "it's actually adding the ability to count the words occurrences per document which again is what you really want so",
    "start": "682190",
    "end": "688100"
  },
  {
    "text": "this is slightly simplified but I think it gives you the sense that knowing this API or something like it and there's a",
    "start": "688100",
    "end": "694460"
  },
  {
    "text": "closure API that's like this to you can just bang out this stuff really fast you can experiment you can play with it",
    "start": "694460",
    "end": "700940"
  },
  {
    "text": "there's a local mode for running things on your laptop and cascading this is the word we want to get to as far as the",
    "start": "700940",
    "end": "706970"
  },
  {
    "text": "kind of non-sequel like data processing that we like to do but it didn't solve",
    "start": "706970",
    "end": "713240"
  },
  {
    "text": "all our problems and one of them was that MapReduce was only ever designed for doing batch mode processing and when",
    "start": "713240",
    "end": "719150"
  },
  {
    "text": "you hear that word in the big data or at least the Hadoop context it always means I've already got the data parked somewhere you're some massive file",
    "start": "719150",
    "end": "725900"
  },
  {
    "text": "system and I want to just ingest it all or some big subset of it and do you know a big batch of processing you know not",
    "start": "725900",
    "end": "732170"
  },
  {
    "text": "live not real time and so forth as opposed to streaming where I have data",
    "start": "732170",
    "end": "737420"
  },
  {
    "text": "coming in and I want to analyze it as quickly as possible and actually my favorite example of why streaming is",
    "start": "737420",
    "end": "742730"
  },
  {
    "text": "important and growing important is actually this one we just talked about inverted index in part because that's",
    "start": "742730",
    "end": "749000"
  },
  {
    "text": "something you can certainly do in batch mode and that's obviously the way that the you know the search engines used to",
    "start": "749000",
    "end": "754610"
  },
  {
    "text": "do it but there is definitely a competitive advantage if somebody makes an edit on their web page and a web",
    "start": "754610",
    "end": "760670"
  },
  {
    "text": "crawler hits it you know say within 20 minutes or something you'd really like that change to show up immediately in",
    "start": "760670",
    "end": "767060"
  },
  {
    "text": "your search index and not you know three hours from now or tonight when you run",
    "start": "767060",
    "end": "772850"
  },
  {
    "text": "your batch job so streaming even makes sense for things that we don't think of so much as streams like this data set",
    "start": "772850",
    "end": "778790"
  },
  {
    "text": "but it also makes sense for things like a monitoring the Twitter fire hose I'm monitoring my log files for signs of",
    "start": "778790",
    "end": "785270"
  },
  {
    "text": "anomalies in the system you know that kind of stuff so we need streaming and the performance needs to be a lot better",
    "start": "785270",
    "end": "791690"
  },
  {
    "text": "you know once again it was fine at the beginning when things were crude to throw more hardware at the problem we",
    "start": "791690",
    "end": "797900"
  },
  {
    "text": "all want to say power we all want to optimize the resources so we need better performance and that's where spark comes",
    "start": "797900",
    "end": "805670"
  },
  {
    "text": "in so about two years ago TWEN 13 Cloudera the big Hadoop vendor said",
    "start": "805670",
    "end": "810800"
  },
  {
    "text": "you know we've kind of reached a point where MapReduce really has gone as far as we can take it this spark tool has",
    "start": "810800",
    "end": "815870"
  },
  {
    "text": "been incubating for a while first at Berkeley and then it's an open source Apache project we think that's kind of",
    "start": "815870",
    "end": "822590"
  },
  {
    "text": "the future for what the compute engine underneath everything ought to be in Hadoop so they embraced it and sort of",
    "start": "822590",
    "end": "828440"
  },
  {
    "text": "put it on a rocketship it's now maybe the most active open source project in the world I believe it definitely is on",
    "start": "828440",
    "end": "834470"
  },
  {
    "text": "github and apache but you know maybe not for Linux or compared to Linux or something so what does it give us well",
    "start": "834470",
    "end": "840350"
  },
  {
    "text": "this will see in a minute it does give us very concise ap is that expose the kind of abstractions we need for",
    "start": "840350",
    "end": "846650"
  },
  {
    "text": "composing data flows whether they're sort of traditional sequel and there's even a sequel dialect you can use for",
    "start": "846650",
    "end": "852980"
  },
  {
    "text": "this which we'll see or I want to do something that's maybe non-trivial like an ETL process where I'm going to do",
    "start": "852980",
    "end": "859100"
  },
  {
    "text": "various staging and validation and transformation and splitting streams and so forth the sort of things that you",
    "start": "859100",
    "end": "865370"
  },
  {
    "text": "wouldn't really think of doing with a sequel query so much and it actually supports for AP is right now the r r1 is",
    "start": "865370",
    "end": "872570"
  },
  {
    "text": "actually coming in a future release but it's basically ready but python skull and java this actually made it possible",
    "start": "872570",
    "end": "879140"
  },
  {
    "text": "in fact a recent change in the API finally made it possible for the Python",
    "start": "879140",
    "end": "884630"
  },
  {
    "text": "code to be just as fast as the skull and Java code so it's completely no longer",
    "start": "884630",
    "end": "889880"
  },
  {
    "text": "necessary for the data science guys to throw their code over the wall just some engineers to port it to java or scala",
    "start": "889880",
    "end": "897350"
  },
  {
    "text": "now of course there's always still the usual issues about deploying something to production you know testing it before",
    "start": "897350",
    "end": "902930"
  },
  {
    "text": "it gets there the usual software engineering challenges that we all know we have to you know address so I don't",
    "start": "902930",
    "end": "908540"
  },
  {
    "text": "want to minimize those kind of aspects but it's not necessary anymore to simply port things over to is what is perceived",
    "start": "908540",
    "end": "915410"
  },
  {
    "text": "as the real systems language like it used to be and it actually has a sequel",
    "start": "915410",
    "end": "921380"
  },
  {
    "text": "dialect so we can actually write sequel queries even interactively when that's the right way to work with the data so",
    "start": "921380",
    "end": "929780"
  },
  {
    "text": "we do have this interactive shell it's basically the scala shell and it gives us it doesn't give us java but we can",
    "start": "929780",
    "end": "935450"
  },
  {
    "text": "write in these three languages and we can also write sequel queries in fact the example are shown a little bit",
    "start": "935450",
    "end": "941220"
  },
  {
    "text": "beb sequel queries in a larger Scala program but there's even a variant of the shell that just brings up a sequel",
    "start": "941220",
    "end": "947579"
  },
  {
    "text": "prompts you just write sequel queries like you would with hive or any of the database tools that you've ever used",
    "start": "947579",
    "end": "954089"
  },
  {
    "text": "which is really really good for I've even worked the product managers people that are not technical at all that know",
    "start": "954089",
    "end": "960449"
  },
  {
    "text": "how to write sequel queries because they've just learned how to do this as part of their jobs and enabling those folks that just log into your Hadoop",
    "start": "960449",
    "end": "966660"
  },
  {
    "text": "cluster and write sequel queries is pretty powerful and do it at reasonably good speed not the slow speed that",
    "start": "966660",
    "end": "972959"
  },
  {
    "text": "MapReduce forced on you with hive and because of these composable primitives",
    "start": "972959",
    "end": "980040"
  },
  {
    "text": "that I mentioned it's really easy to implement a wide class of algorithms and you get the recently good performance so",
    "start": "980040",
    "end": "986759"
  },
  {
    "text": "you can do even iterative algorithms like graph traversals like training a model machine learning and that sort of",
    "start": "986759",
    "end": "993060"
  },
  {
    "text": "thing it used to be that these were very hard to do with MapReduce and you'd have to either work around the system or",
    "start": "993060",
    "end": "999449"
  },
  {
    "text": "tolerate poor performance but now we can do that a lot better some other really",
    "start": "999449",
    "end": "1005629"
  },
  {
    "text": "interesting things they do MapReduce never actually knew what the long sequence of jobs were that you wrote",
    "start": "1005629",
    "end": "1011000"
  },
  {
    "text": "we're trying to accomplish that it had no visibility into the pipeline it just knew about the map step and reduced step",
    "start": "1011000",
    "end": "1016879"
  },
  {
    "text": "that might be say you know in any of these sort of coarse-grained steps here spark actually builds up a lazy directed",
    "start": "1016879",
    "end": "1024589"
  },
  {
    "text": "acyclic graph of the computation and he can do things like you know squash some",
    "start": "1024589",
    "end": "1029870"
  },
  {
    "text": "of them together in the same jvm process so you can conceptually right things separately but he'd get the optimization",
    "start": "1029870",
    "end": "1036110"
  },
  {
    "text": "of having things rolled together this also benefits the data recovery so in",
    "start": "1036110",
    "end": "1041808"
  },
  {
    "text": "fact if you lose like a partition of data spark can go back to wherever the last checkpoint or data source was and",
    "start": "1041809",
    "end": "1048558"
  },
  {
    "text": "reconstruct that partition because it understands the lineage of all the data because it has this directed acyclic",
    "start": "1048559",
    "end": "1054679"
  },
  {
    "text": "graph in memory and I actually mentioned this already there's a new data frame",
    "start": "1054679",
    "end": "1060950"
  },
  {
    "text": "API that's inspired by Python and our data frame concepts that AB strays a--'s",
    "start": "1060950",
    "end": "1066559"
  },
  {
    "text": "the abstraction level even a little farther than the core of spark but also enables query optimizations and the",
    "start": "1066559",
    "end": "1074000"
  },
  {
    "text": "ability to use any of these four languages I mentioned including actually five including sequel",
    "start": "1074000",
    "end": "1079670"
  },
  {
    "text": "and still get the same performance which i think is a huge step forward and",
    "start": "1079670",
    "end": "1085090"
  },
  {
    "text": "because of its ability to be fairly efficient spark can actually support a",
    "start": "1085090",
    "end": "1090200"
  },
  {
    "text": "streaming model which is basically a mini batch model so spark actually evolved also as a batch mode system like",
    "start": "1090200",
    "end": "1096890"
  },
  {
    "text": "mapreduce you know he started it was about 2008-2009 but because of its",
    "start": "1096890",
    "end": "1102770"
  },
  {
    "text": "relative efficiency they realized that they could actually support sort of the 8020 solution to what most people mean",
    "start": "1102770",
    "end": "1109670"
  },
  {
    "text": "when they really mean streaming which is I just need to process the data fairly quickly it doesn't have to be perfect or",
    "start": "1109670",
    "end": "1115730"
  },
  {
    "text": "you know instantaneous like milliseconds like my inverted index example where it",
    "start": "1115730",
    "end": "1120860"
  },
  {
    "text": "yeah I don't have to respond to a user immediately you know when the index is updated but sure it'd be great if I",
    "start": "1120860",
    "end": "1126290"
  },
  {
    "text": "could do it within a minute or whatever so they invented this mini batch process",
    "start": "1126290",
    "end": "1131780"
  },
  {
    "text": "called I actually have a slight of the next slide will show it a little bit to",
    "start": "1131780",
    "end": "1137510"
  },
  {
    "text": "other important benefits though one is that because it's a mini batch you actually can just repurpose the same",
    "start": "1137510",
    "end": "1142850"
  },
  {
    "text": "code you've written for your batch jobs inside a stream processing system and if",
    "start": "1142850",
    "end": "1148100"
  },
  {
    "text": "you ever use the lambda architecture for example this really solves a huge problem that is typical the lambda",
    "start": "1148100",
    "end": "1154160"
  },
  {
    "text": "architecture which is in other tools you often find yourself re implementing the same logic twice once in your streaming",
    "start": "1154160",
    "end": "1161510"
  },
  {
    "text": "tool like storm let's say and watch in your badge to like mapreduce so now you don't have to do that even you can just",
    "start": "1161510",
    "end": "1167660"
  },
  {
    "text": "write one your core code base and then you'll repurpose it for those two different scenarios and in addition to",
    "start": "1167660",
    "end": "1174260"
  },
  {
    "text": "reusing batch code you get the ability to do window functions so it conceptually it looks something like",
    "start": "1174260",
    "end": "1180140"
  },
  {
    "text": "this each of these blue boxes is a resilient distributed data set that's",
    "start": "1180140",
    "end": "1185510"
  },
  {
    "text": "the distributed collection that's behind the scenes always holding your data it's resilient in the sense that if a",
    "start": "1185510",
    "end": "1191600"
  },
  {
    "text": "partition is lost like I said it can go back and reconstruct it it's distributed and that the data will be partitioned",
    "start": "1191600",
    "end": "1197600"
  },
  {
    "text": "across your cluster so that you get the benefits of parallelism and in the case of a streaming process they use",
    "start": "1197600",
    "end": "1204410"
  },
  {
    "text": "something called a discretized stream or D stream where each time window and is something you specify you know down",
    "start": "1204410",
    "end": "1210840"
  },
  {
    "text": "to about a second is the minimum resolution it's going to put that batch",
    "start": "1210840",
    "end": "1216720"
  },
  {
    "text": "of data into an RDD and let you process on it or even do things over windows of",
    "start": "1216720",
    "end": "1221910"
  },
  {
    "text": "them so if you're like computing moving averages or you know what are the most frequent tweets or whatever you can do",
    "start": "1221910",
    "end": "1227370"
  },
  {
    "text": "it this way okay now i am going to show some Scala code even though this is data",
    "start": "1227370",
    "end": "1233490"
  },
  {
    "text": "science talk and I'm going to use this my you know source for the permission to do this Vitaly Gordon this actually is a",
    "start": "1233490",
    "end": "1240450"
  },
  {
    "text": "link to a talk he gave at scala days but it was convenient that he talked before he elfin advocates using scala and",
    "start": "1240450",
    "end": "1247380"
  },
  {
    "text": "especially in this sort of pre spark 13 mode where you didn't have the ability",
    "start": "1247380",
    "end": "1252660"
  },
  {
    "text": "to run Python at the same performance as Java code so i'll just use scallop and",
    "start": "1252660",
    "end": "1258630"
  },
  {
    "text": "mostly because that's the only thing I know how to do anymore so anyway let's actually look at the inverted index and you know I assume that you don't know",
    "start": "1258630",
    "end": "1265020"
  },
  {
    "text": "Scala how many of you've actually used Scala just out of cure all right more than I thought the purpose isn't really",
    "start": "1265020",
    "end": "1271080"
  },
  {
    "text": "did teach you skull of course but to give you a sense of what it's like to create a data flow of processing data in",
    "start": "1271080",
    "end": "1277440"
  },
  {
    "text": "a language that or toolkit that gives you the right functional abstractions namely immutable data operators I string",
    "start": "1277440",
    "end": "1284850"
  },
  {
    "text": "together to get from A to B with you making all the little micro steps that I need to make the Python API is almost",
    "start": "1284850",
    "end": "1292050"
  },
  {
    "text": "identical it's amazing how similar Python code looks to this in fact I'll",
    "start": "1292050",
    "end": "1297210"
  },
  {
    "text": "show an example of that in a second the Java code isn't quite as concise but now",
    "start": "1297210",
    "end": "1302550"
  },
  {
    "text": "because of Java 8 you can do things like embed anonymous functions which is very important for code concision so anyway",
    "start": "1302550",
    "end": "1310560"
  },
  {
    "text": "you start always with importing stuff whether it's java or scala the same idea and then scholar let you declare",
    "start": "1310560",
    "end": "1316770"
  },
  {
    "text": "singleton objects and that's what you would put your main routines in so that's what i'm doing here to calculate the inverted index obviously that's my",
    "start": "1316770",
    "end": "1323910"
  },
  {
    "text": "entry point when this thing runs you always start a spark program by creating a spark context that's your entry point",
    "start": "1323910",
    "end": "1330660"
  },
  {
    "text": "it the first argument actually is the cluster I'm talking to in this case I'm just talking to my laptop so it's just",
    "start": "1330660",
    "end": "1337080"
  },
  {
    "text": "local mode and there's that there's very various ways you can construct these things but now let's actually start",
    "start": "1337080",
    "end": "1343080"
  },
  {
    "text": "processing this data so the first thing I'm going to do is assume that my kraal data is just in text files so I'm going",
    "start": "1343080",
    "end": "1348720"
  },
  {
    "text": "to load that and again actually this could be a directory of you know terabytes of data but I but it will",
    "start": "1348720",
    "end": "1354840"
  },
  {
    "text": "figure out how many tasks to spawn for me and the first thing I want to do and",
    "start": "1354840",
    "end": "1360330"
  },
  {
    "text": "it's we're a little cut off at the tub but I think you can still see it I'm going to map over each line and so this",
    "start": "1360330",
    "end": "1366000"
  },
  {
    "text": "is an anonymous function i'm passing to the map method and you can use curly braces instead of parenthesis for",
    "start": "1366000",
    "end": "1371130"
  },
  {
    "text": "function arguments because it gives you this a nice block structure and also",
    "start": "1371130",
    "end": "1376289"
  },
  {
    "text": "actually while I think of it notice their dots at the beginning of these red things the red things are always methods",
    "start": "1376289",
    "end": "1381960"
  },
  {
    "text": "because that does the real work this is actually one giant expression where I'm just building up a pipeline you could",
    "start": "1381960",
    "end": "1387960"
  },
  {
    "text": "certainly have intermediate variables for each of these stages but when I'm just doing it all at once you know I",
    "start": "1387960",
    "end": "1393690"
  },
  {
    "text": "don't like to declare variables where I have to come up with a name anyway so I'm going to split the line on tabs and",
    "start": "1393690",
    "end": "1400289"
  },
  {
    "text": "then return a tuple containing the first element in the second element I'm actually splitting into two elements",
    "start": "1400289",
    "end": "1406440"
  },
  {
    "text": "that's what the two is four because an obviously text could have tabs in it I don't want to treat those as fields so",
    "start": "1406440",
    "end": "1413220"
  },
  {
    "text": "this is now I have a couple of my document ID and the contents of my document you one record per line flat",
    "start": "1413220",
    "end": "1422039"
  },
  {
    "text": "map is basically like map but instead of being one to one its 02 many outputs to hear what I'm doing is taking each line",
    "start": "1422039",
    "end": "1428580"
  },
  {
    "text": "and splitting the text into words and then outputting another tupple which will be the word which I've now put in a",
    "start": "1428580",
    "end": "1435210"
  },
  {
    "text": "key position by default it uses the first field is the key and then the path because I need to keep track of where",
    "start": "1435210",
    "end": "1441179"
  },
  {
    "text": "this word was found and it's i'm using flat map because obviously i'll get many of these per line and flat map will just",
    "start": "1441179",
    "end": "1448110"
  },
  {
    "text": "automatically compress that nested collection of words per document into one giant a collection of word path",
    "start": "1448110",
    "end": "1455940"
  },
  {
    "text": "pairs that's of course what we want this is my favorite little bit of code this is kind of a classic spark idiom for",
    "start": "1455940",
    "end": "1463380"
  },
  {
    "text": "counting I'm taking those tuples and I'm going to you still use the word and path tupple as a key so it will be nested",
    "start": "1463380",
    "end": "1470130"
  },
  {
    "text": "inside a larger tupple that has a seed count of one for each tuple and then the next step we reduced by key is",
    "start": "1470130",
    "end": "1477090"
  },
  {
    "text": "is an optimization and spark where it'll just it's basically grouped by over all",
    "start": "1477090",
    "end": "1482490"
  },
  {
    "text": "of those word path keys and then it just sums up the counts by whatever function I pass it so I'm just going to grab the",
    "start": "1482490",
    "end": "1489360"
  },
  {
    "text": "two counts n1 and n2 sum them up and I'll end up with data like in that gray",
    "start": "1489360",
    "end": "1494400"
  },
  {
    "text": "box where now I'll have unique word path pairs and then some count that's one or",
    "start": "1494400",
    "end": "1499800"
  },
  {
    "text": "more for each word all this thing does",
    "start": "1499800",
    "end": "1506040"
  },
  {
    "text": "is it rearranges the print you know what I love about this line this this would be like nasty job it'd be like you know",
    "start": "1506040",
    "end": "1511530"
  },
  {
    "text": "five or six or ten lines of Java code all I want to do is get to back to using",
    "start": "1511530",
    "end": "1516570"
  },
  {
    "text": "the word is the key so I just shifted the parenthesis of these tuples in the nested tuples so that now I have word is",
    "start": "1516570",
    "end": "1524250"
  },
  {
    "text": "the key and then path and count as the value and that's why I just love about this the reason the word case is here",
    "start": "1524250",
    "end": "1531450"
  },
  {
    "text": "this is actually a pattern match idiom and Scala it lets me reach in and tear the tupple apart just without having to",
    "start": "1531450",
    "end": "1538740"
  },
  {
    "text": "you know like call get element get element kind of stuff so I'm pattern matching on that nested word paths",
    "start": "1538740",
    "end": "1544250"
  },
  {
    "text": "couple the original key and count and then creating a new temple with the word is the key and then path and an this the",
    "start": "1544250",
    "end": "1551460"
  },
  {
    "text": "count as the value of this of the temple now we do a traditional group x over the",
    "start": "1551460",
    "end": "1557430"
  },
  {
    "text": "key and then this last bit all it's really doing is formatting the output",
    "start": "1557430",
    "end": "1562530"
  },
  {
    "text": "and it on the fly sorting by count descending so that's what we're it's his",
    "start": "1562530",
    "end": "1567810"
  },
  {
    "text": "sort by and then I have another pattern match all I'm really doing is saying all right when you're doing this sorting",
    "start": "1567810",
    "end": "1573000"
  },
  {
    "text": "treat the count is negative so it will actually be descending and do a secondary sword on the path actually not",
    "start": "1573000",
    "end": "1578910"
  },
  {
    "text": "necessary it but it's useful for unit testing and then i'll just make a string out of those and then save all of this",
    "start": "1578910",
    "end": "1585120"
  },
  {
    "text": "to a text file and then stop when we're done everything except that stop at the",
    "start": "1585120",
    "end": "1591750"
  },
  {
    "text": "bottom and the initial input through text files i could take that code put it in a library and do it in batch or",
    "start": "1591750",
    "end": "1597720"
  },
  {
    "text": "streaming on the fly if I wanted to so I get that reuse so anyway you know",
    "start": "1597720",
    "end": "1603480"
  },
  {
    "text": "whether you understood all those details I hope you got the sense of how the AP and AP I that exposes the right",
    "start": "1603480",
    "end": "1608730"
  },
  {
    "text": "abstractions in this case you know design for data really lets his crank this out quickly I wrote this in 30 minutes the",
    "start": "1608730",
    "end": "1614880"
  },
  {
    "text": "first time I wrote this thing now it's evolved a lot since but it was because I understood those abstractions and I",
    "start": "1614880",
    "end": "1621179"
  },
  {
    "text": "wasn't thinking about really really low level gritty details that MapReduce would force on me and I was able to",
    "start": "1621179",
    "end": "1626910"
  },
  {
    "text": "crank this out quickly and this is a 12 point font I think so you can actually",
    "start": "1626910",
    "end": "1632730"
  },
  {
    "text": "almost read the whole thing maybe those of you in the front can okay well once",
    "start": "1632730",
    "end": "1637950"
  },
  {
    "text": "again amazing Combinator's that's sort of a functional programming term i'm one of those uppity guys on the top hat that",
    "start": "1637950",
    "end": "1644370"
  },
  {
    "text": "anyway operators is a better word really but that's why i use the red even though it sort of fades out on this projector",
    "start": "1644370",
    "end": "1650970"
  },
  {
    "text": "because those are doing the real work those are the heavy lifters and I just tell them you know okay when you reduce",
    "start": "1650970",
    "end": "1656549"
  },
  {
    "text": "by keyway reducing by when you're mapping all right how do i want you to map these tuples and so forth so let's",
    "start": "1656549",
    "end": "1665970"
  },
  {
    "text": "look at the slightly broader aspect of the spark ecosystem as a sub ecosystem of Hadoop or you can actually run spark",
    "start": "1665970",
    "end": "1672630"
  },
  {
    "text": "outside of Hadoop on may so sore even as a standalone cluster shameless plug",
    "start": "1672630",
    "end": "1677730"
  },
  {
    "text": "typesafe is offering commercial support for those latter two options so talk to me afterwards if you want to give me",
    "start": "1677730",
    "end": "1683429"
  },
  {
    "text": "your money alright we can write sequel queries so let's look at that first of",
    "start": "1683429",
    "end": "1689549"
  },
  {
    "text": "all very important for a lot of people in amateur Hadoop ecosystem is they've",
    "start": "1689549",
    "end": "1694799"
  },
  {
    "text": "already got a lot of stuff in hive they've been using hive tables forever spark and interoperate with hive so you",
    "start": "1694799",
    "end": "1700380"
  },
  {
    "text": "can get faster queries or you can so my favorite example of this actually is you spark for that etl cleanup you know",
    "start": "1700380",
    "end": "1706740"
  },
  {
    "text": "munching process and then just write hive tables and then your data analysts you know as soon as that process is",
    "start": "1706740",
    "end": "1712590"
  },
  {
    "text": "finished they can go right there queries to see you know what's the latest tweets or whatever it has an internal query",
    "start": "1712590",
    "end": "1719160"
  },
  {
    "text": "optimizer called catalyst and so basically the last two lines you have",
    "start": "1719160",
    "end": "1724590"
  },
  {
    "text": "two options you can either write real sequel or you can use this new data frame API which is sort of like a you",
    "start": "1724590",
    "end": "1730860"
  },
  {
    "text": "know a like a builder pattern for expressing the queries in a more type safe way as opposed to embedded strings",
    "start": "1730860",
    "end": "1737600"
  },
  {
    "text": "which actually is the preferred way today so let's actually see an example of how you might use this with high",
    "start": "1737600",
    "end": "1744390"
  },
  {
    "text": "and if you wanted to use sparks own sequel dialect the code would be almost the same but it's not quite as mature or",
    "start": "1744390",
    "end": "1751230"
  },
  {
    "text": "as feature complete as the high version so this is still really popular so",
    "start": "1751230",
    "end": "1758370"
  },
  {
    "text": "there's some stuff that got cut off and just an import statement it doesn't really matter create a spark context as",
    "start": "1758370",
    "end": "1764310"
  },
  {
    "text": "before wrap that in a hive context or use the sequel context if I don't if I'm not talking to hive and I can basically",
    "start": "1764310",
    "end": "1771210"
  },
  {
    "text": "write the same kind of queries we saw earlier I can create a table I just I have this sequel method and I have to",
    "start": "1771210",
    "end": "1777750"
  },
  {
    "text": "pass strings so the downside is it's not type checked until runtime but that's okay if this is actually an interactive",
    "start": "1777750",
    "end": "1784230"
  },
  {
    "text": "show where I'll just tell I can tell right away if I messed it up and redo it I can load data this is classic high for",
    "start": "1784230",
    "end": "1790980"
  },
  {
    "text": "just loading data that I have you know stored in my like local file system into HDFS and then I can start writing",
    "start": "1790980",
    "end": "1797430"
  },
  {
    "text": "queries this last show method is just a data frame method that's convenient for",
    "start": "1797430",
    "end": "1803280"
  },
  {
    "text": "seeing like the first 10 records and to see what you've got so whenever when I did this what do they do like is it off",
    "start": "1803280",
    "end": "1811470"
  },
  {
    "text": "the screen actually when I do these queries they return something called a data frame which has this higher level",
    "start": "1811470",
    "end": "1817730"
  },
  {
    "text": "abstraction for sequel awareness schema awareness of the data and then I can just I can actually use this in the",
    "start": "1817730",
    "end": "1824100"
  },
  {
    "text": "regular spark API if I want to take the like the results of this query and do some munching on it and create a new",
    "start": "1824100",
    "end": "1830280"
  },
  {
    "text": "hive table or whatever it's just all like all integrated right there and if you don't like the nested query strings",
    "start": "1830280",
    "end": "1835950"
  },
  {
    "text": "well there's this alternative that we'll see in just a second so it turns out actually when you do this either in",
    "start": "1835950",
    "end": "1841740"
  },
  {
    "text": "Python or source kala there's most of your code looks it's going to look",
    "start": "1841740",
    "end": "1846900"
  },
  {
    "text": "really similar all you'll do in this particular example I showed you which is even a trivial change as you replace to",
    "start": "1846900",
    "end": "1853110"
  },
  {
    "text": "statement or one statement the import is replaced with a from PI sparks equal import hype context and then you just",
    "start": "1853110",
    "end": "1859770"
  },
  {
    "text": "delete the Val keywords from the spark of the code I just showed you actually were there any I guess there were at the",
    "start": "1859770",
    "end": "1865500"
  },
  {
    "text": "beginning for the spark context and then the code is actually Python code so that's really nice you know in a way",
    "start": "1865500",
    "end": "1871470"
  },
  {
    "text": "that's great too because it means that if you know python and somebody gives you a scala example to look OH",
    "start": "1871470",
    "end": "1877950"
  },
  {
    "text": "where you can probably figure out what it's doing pretty easily so let's look at the alternative which is the data",
    "start": "1877950",
    "end": "1883320"
  },
  {
    "text": "frame API and I'll basically do kind of the same code this will be more type",
    "start": "1883320",
    "end": "1888809"
  },
  {
    "text": "safe because it will be doing checking at compile time and that is an advantage the disadvantage for anybody who already",
    "start": "1888809",
    "end": "1895350"
  },
  {
    "text": "know sequel is they have to kind of learn this dsl as opposed to just writing with the language they already know but it starts off the same you know",
    "start": "1895350",
    "end": "1903059"
  },
  {
    "text": "we create this spark context and this time I'm going to use the sequel context and not deal with hive at all I'm going",
    "start": "1903059",
    "end": "1910169"
  },
  {
    "text": "to load some data and by default it uses part k which is sort of the hotness in terms of data formats in Hadoop file",
    "start": "1910169",
    "end": "1916679"
  },
  {
    "text": "system it's a column-oriented storage it actually compresses columns does some other nice things that are pretty handy",
    "start": "1916679",
    "end": "1923730"
  },
  {
    "text": "this is a something Twitter helped developed actually but you can load Jason and it can parse JSON on the fly",
    "start": "1923730",
    "end": "1929580"
  },
  {
    "text": "for you and infer the schema you can certainly use some other file formats that are popular including text even",
    "start": "1929580",
    "end": "1936269"
  },
  {
    "text": "though Martin and Todd convinced us never to use text again for the rest of our natural lives so maybe I want to",
    "start": "1936269",
    "end": "1944240"
  },
  {
    "text": "actually the WC this is supposed to be like a word count data or just think of it as inverted index and maybe I want to",
    "start": "1944240",
    "end": "1950909"
  },
  {
    "text": "see the counts descending I want to see the most frequently occurring words I can just do data frame order by and then",
    "start": "1950909",
    "end": "1958169"
  },
  {
    "text": "this funky little syntax with a dollar sign that's a that's one of several ways you can express I want you to find the",
    "start": "1958169",
    "end": "1964889"
  },
  {
    "text": "count column in this data and sorted descending so it's it's a bit idiomatic",
    "start": "1964889",
    "end": "1970320"
  },
  {
    "text": "scala that they had to use for this particular API but you get used to it really fast and then I can see the first",
    "start": "1970320",
    "end": "1976919"
  },
  {
    "text": "ten very easily just to make sure it looks sensible and then cash is an optimization spark exposes to you to say",
    "start": "1976919",
    "end": "1983820"
  },
  {
    "text": "I'm going to keep going over the state over and over so cash it in memory if you can so that you don't just recompute",
    "start": "1983820",
    "end": "1990149"
  },
  {
    "text": "the whole graph every time otherwise it would actually go back to the and reload the data every time which is going to be",
    "start": "1990149",
    "end": "1996419"
  },
  {
    "text": "inefficient I could do things like you show me all the words that are greater that are longer than 20 characters again",
    "start": "1996419",
    "end": "2002840"
  },
  {
    "text": "using that idiomatic syntax this time I'm passing a predicate is an argument to filter and then I could save the",
    "start": "2002840",
    "end": "2008629"
  },
  {
    "text": "results back to park a if I want actually I put in the extension just to show that it's parquet but it doesn't",
    "start": "2008629",
    "end": "2014630"
  },
  {
    "text": "matter it's going to write by default parque and read part k i just use that as a naming convention here so very",
    "start": "2014630",
    "end": "2021320"
  },
  {
    "text": "simple actually i guess it's not quite like the previous example but if you prefer working in an API that's type",
    "start": "2021320",
    "end": "2027440"
  },
  {
    "text": "checked at compile time and and also gives you really good performance it's it's an excellent tool so what about",
    "start": "2027440",
    "end": "2034880"
  },
  {
    "text": "machine learning and graph processing let's finish up with those and how am i doing for time pretty well ok so there's",
    "start": "2034880",
    "end": "2040970"
  },
  {
    "text": "a ml live which is part of spark it's a rapidly growing library of standard machine learning algorithms and it's",
    "start": "2040970",
    "end": "2047690"
  },
  {
    "text": "built on this core infrastructure that gives us really good performance for the most part by the way I just thought of",
    "start": "2047690",
    "end": "2054858"
  },
  {
    "text": "something if you want to post questions post them on the app and also remember",
    "start": "2054859",
    "end": "2060440"
  },
  {
    "text": "to review the app I meant to say that when I started let's look at a kind of",
    "start": "2060440",
    "end": "2065690"
  },
  {
    "text": "an interesting example so i mentioned streaming is really hot well a lot of things that we do a machine-learning we've been doing in batch mode but",
    "start": "2065690",
    "end": "2072648"
  },
  {
    "text": "there's some things we'd really like to be doing in real time and i use the word real time with air quotes of course and",
    "start": "2072649",
    "end": "2078260"
  },
  {
    "text": "one of them is finding clusters and data as it's coming in the other one they support right now is logistic and linear",
    "start": "2078260",
    "end": "2085820"
  },
  {
    "text": "regression so you can like train models on the fly and do predictions on the fly the example i'm going to show you is",
    "start": "2085820",
    "end": "2091580"
  },
  {
    "text": "actually from the spark distribution itself so you can just download spark and you'll find this code in there",
    "start": "2091580",
    "end": "2098390"
  },
  {
    "text": "there's a bunch of messy imports but mostly what i wanted to show you is i'm importing an example and I've aletta the",
    "start": "2098390",
    "end": "2104359"
  },
  {
    "text": "org dot Apache on the front here that's not legal scholar but I just do so it",
    "start": "2104359",
    "end": "2109700"
  },
  {
    "text": "would fit there's some built-in linear algebra stuff for like representing vectors of features you know like a",
    "start": "2109700",
    "end": "2116080"
  },
  {
    "text": "classic example would be like you know Sam rating like housing prices I might",
    "start": "2116080",
    "end": "2122540"
  },
  {
    "text": "have a vector for the price number of rooms those kind of features and what",
    "start": "2122540",
    "end": "2127730"
  },
  {
    "text": "else here then labeled point if I've data that's already pre labeled as being part of a certain cluster or grouping",
    "start": "2127730",
    "end": "2134570"
  },
  {
    "text": "then I'm going to use that to test how accurate this is so then we so I've done",
    "start": "2134570",
    "end": "2140630"
  },
  {
    "text": "these imports now i'm going to import a streaming context so we'll make both machine learning and streaming this",
    "start": "2140630",
    "end": "2146870"
  },
  {
    "text": "also wraps I think I've gotta build here let's get through that so there we go so",
    "start": "2146870",
    "end": "2152060"
  },
  {
    "text": "I once again create a spark contacts create a streaming context and hear the second argument is important it tells me",
    "start": "2152060",
    "end": "2158510"
  },
  {
    "text": "how many times are you know what's the length in some time window that I want to capture data and process it as a mini",
    "start": "2158510",
    "end": "2165530"
  },
  {
    "text": "batch for various technical reasons that we won't get into this kind of minimum",
    "start": "2165530",
    "end": "2171560"
  },
  {
    "text": "is really about a second you don't want to go in you can't really go below that and have reliable performance you can go",
    "start": "2171560",
    "end": "2177320"
  },
  {
    "text": "up to really long times if you want like if you have very slow streaming and data",
    "start": "2177320",
    "end": "2182570"
  },
  {
    "text": "you know you can have several minutes if you want the only risk there of course is that data is going to be sitting in memory and could you know get lost there",
    "start": "2182570",
    "end": "2189920"
  },
  {
    "text": "are some ways that they're adding resilience so you don't get data loss on the event that the job happens to crash",
    "start": "2189920",
    "end": "2195320"
  },
  {
    "text": "why you've got data that's been sitting there ready for the next iteration we're",
    "start": "2195320",
    "end": "2201380"
  },
  {
    "text": "going to load two data sets one of which will be the training data that's already labeled no it's not labeled we want it",
    "start": "2201380",
    "end": "2207740"
  },
  {
    "text": "we want to use those for predicting but then we're gonna have some tests table that test data that is labeled it will",
    "start": "2207740",
    "end": "2213890"
  },
  {
    "text": "also load and what we're going to do when we actually start processing is as the data comes in on the training stream",
    "start": "2213890",
    "end": "2220280"
  },
  {
    "text": "we're going to your train or look for clusters you know with k-means and then",
    "start": "2220280",
    "end": "2226520"
  },
  {
    "text": "we're going to take that data and see how well we did at predicting the location of these training sets so the",
    "start": "2226520",
    "end": "2233780"
  },
  {
    "text": "training data already knows where it should be we're going to see how close we got to it so txt file stream means",
    "start": "2233780",
    "end": "2239780"
  },
  {
    "text": "we're going to just listen on a socket for data to come in yes that's correct",
    "start": "2239780",
    "end": "2246050"
  },
  {
    "text": "there's also like directories you could watch directories and so forth we'll treat each record will be like a single",
    "start": "2246050",
    "end": "2251930"
  },
  {
    "text": "line of text and then this is a classic idiom of just mapping over that data and passing it to some parse method that",
    "start": "2251930",
    "end": "2257870"
  },
  {
    "text": "knows how to tokenize that line of text and turn it into an object like a vector in this case or a labeled point it's",
    "start": "2257870",
    "end": "2265220"
  },
  {
    "text": "also a vector but else it has a label attached to it and I love this because",
    "start": "2265220",
    "end": "2272000"
  },
  {
    "text": "it's so concise to actually use streaming k-means we just instantiate one and we set some property",
    "start": "2272000",
    "end": "2278030"
  },
  {
    "text": "now all these things in yellow would normally be like command-line options so that you can vary them but just for simplicity I treated them as constants",
    "start": "2278030",
    "end": "2284840"
  },
  {
    "text": "so we're going to find like say five clusters in the data all the time the",
    "start": "2284840",
    "end": "2289970"
  },
  {
    "text": "decay factor says should I remember previous data and mix that in or should I forget it and basically it's a",
    "start": "2289970",
    "end": "2296240"
  },
  {
    "text": "weighting in this case one actually means forget all previous data treat each new batch as brand new data and",
    "start": "2296240",
    "end": "2302930"
  },
  {
    "text": "find its clusters zero would mean use all data in the past which is kind of dangerous because obviously you're going",
    "start": "2302930",
    "end": "2308750"
  },
  {
    "text": "to be accumulating data and then it we need to initialize centers and we're",
    "start": "2308750",
    "end": "2313850"
  },
  {
    "text": "going to assume that the end features is actually the size of our vectors so we",
    "start": "2313850",
    "end": "2319130"
  },
  {
    "text": "let's say we know that we've got 20 features in our vectors that we're going to do predictions with this is a",
    "start": "2319130",
    "end": "2325640"
  },
  {
    "text": "function in Scala for just transforming a labeled point into at uppal that has",
    "start": "2325640",
    "end": "2331520"
  },
  {
    "text": "the label as the first element and the rest of the features in a vector is the second element I need that for actually",
    "start": "2331520",
    "end": "2339050"
  },
  {
    "text": "well one of the next functions here so this is the way you could declare an anonymous function or actually a named",
    "start": "2339050",
    "end": "2344810"
  },
  {
    "text": "function in Scala would look similar in like Python and Java now as well now",
    "start": "2344810",
    "end": "2352460"
  },
  {
    "text": "everything I've done up to this point in fact even at this point nothing's actually happening I'm building up a",
    "start": "2352460",
    "end": "2357680"
  },
  {
    "text": "pipeline and it's not until the next line on this when I say start that will actually run so as i said i'm going to",
    "start": "2357680",
    "end": "2364100"
  },
  {
    "text": "have these two streams coming in i'm going to train on the training data on each batch iteration and then predict",
    "start": "2364100",
    "end": "2369890"
  },
  {
    "text": "from that data using what came out of the test data with my f function that",
    "start": "2369890",
    "end": "2375080"
  },
  {
    "text": "does it can extract the fields that i need and Prentiss just really it's a debug statement that they have in the",
    "start": "2375080",
    "end": "2380990"
  },
  {
    "text": "API it'll show you like the first 20 results of this returned rdd a resilient",
    "start": "2380990",
    "end": "2387860"
  },
  {
    "text": "distribute data set so normally you wouldn't see that in production you'd be writing this out to a file or a database",
    "start": "2387860",
    "end": "2393380"
  },
  {
    "text": "or something and then finally we start it and then wait for it to just terminate which could be forever",
    "start": "2393380",
    "end": "2399100"
  },
  {
    "text": "hopefully okay the last one I want to talk about and then we'll wrap up is graph processing so this is the idea and",
    "start": "2399100",
    "end": "2407000"
  },
  {
    "text": "there's a API called graphics well there's a whole bunch of these in the real world Vitaly talked about some actually at",
    "start": "2407000",
    "end": "2413600"
  },
  {
    "text": "LinkedIn your social network your Twitter followers epidemics are an interesting case the interwebs they form",
    "start": "2413600",
    "end": "2420920"
  },
  {
    "text": "a graph I did not misspell teh actually but it used to be that distributed graph",
    "start": "2420920",
    "end": "2427220"
  },
  {
    "text": "processing was actually sort of a research problem and we didn't know how to do it well so most people just sort",
    "start": "2427220",
    "end": "2432590"
  },
  {
    "text": "of hand coded specialized versions of whatever algorithm they were trying to compute using MapReduce or whatever but",
    "start": "2432590",
    "end": "2439490"
  },
  {
    "text": "now we actually have the ability to build more general purpose graph engines that can be distributed in this case",
    "start": "2439490",
    "end": "2444950"
  },
  {
    "text": "they're built on top of spark quickly I'll go through the code a lot of it is",
    "start": "2444950",
    "end": "2450860"
  },
  {
    "text": "just set up but I wanted to I wanted to give you a complete example so that you know the first thing is a bunch of imports again and I will this will not",
    "start": "2450860",
    "end": "2458090"
  },
  {
    "text": "even say any more about that I want to do 20 partitions again of something I",
    "start": "2458090",
    "end": "2463370"
  },
  {
    "text": "would actually make a config option there's several ways I could partition the data actually in this case we're",
    "start": "2463370",
    "end": "2468860"
  },
  {
    "text": "going to actually duplicate some of the vertices whenever an edge is going to cross a partition I think I got this",
    "start": "2468860",
    "end": "2474980"
  },
  {
    "text": "right actually we want vertices to be clustered together more than edges so we'll actually be duplicating vertices",
    "start": "2474980",
    "end": "2481700"
  },
  {
    "text": "instead of just splitting edges this is this particular model and these two next",
    "start": "2481700",
    "end": "2487220"
  },
  {
    "text": "two values too fast these next two ones are just like try to save everything in memories or processing and then the",
    "start": "2487220",
    "end": "2493910"
  },
  {
    "text": "tolerance we're actually going to do page rank is I didn't really say that yet we're going to page ranks when we iterate on computing the weights of our",
    "start": "2493910",
    "end": "2500750"
  },
  {
    "text": "nodes in our you know social graph or whatever we'll wait until we get a",
    "start": "2500750",
    "end": "2506000"
  },
  {
    "text": "tolerance down to point zero zero one and that'll be the end of the iteration so you'll just run to completion with",
    "start": "2506000",
    "end": "2511880"
  },
  {
    "text": "this tolerance and then somewhere we're going to get the data that'll be our input okay so this is the real meat of",
    "start": "2511880",
    "end": "2517790"
  },
  {
    "text": "it once again spark context create that use a static method edge list file where",
    "start": "2517790",
    "end": "2523940"
  },
  {
    "text": "we give it the spark context our input and then those other values we just set",
    "start": "2523940",
    "end": "2529250"
  },
  {
    "text": "that tells it now how many partitions and so forth and then cash that in memory is the last step here that's",
    "start": "2529250",
    "end": "2536600"
  },
  {
    "text": "actually the first step here will be to give us unpartitioned graph data once again i'm setting up a pipeline I",
    "start": "2536600",
    "end": "2542330"
  },
  {
    "text": "haven't done anything yet this fold left will be used to take that unpartitioned data and partition it by",
    "start": "2542330",
    "end": "2549160"
  },
  {
    "text": "the partitioner that I told it to use somewhere actually where did I tell",
    "start": "2549160",
    "end": "2554830"
  },
  {
    "text": "anyway it's somewhere in here maybe I left it out and then the last thing we'll do is actually print out how many",
    "start": "2554830",
    "end": "2560620"
  },
  {
    "text": "nodes and edges we have in this graph and then finally we can just run page rank with one line of code cache the",
    "start": "2560620",
    "end": "2568360"
  },
  {
    "text": "vertices because we're going to keep going over those this last step in just computes the total rank it sums up all",
    "start": "2568360",
    "end": "2574390"
  },
  {
    "text": "of the ranks of the individual nodes just a little bit of Scala isms here for that and then finally we can just save",
    "start": "2574390",
    "end": "2581080"
  },
  {
    "text": "this as tab delimited data back to txt files and then stop so once again actually most of this is setup and",
    "start": "2581080",
    "end": "2587530"
  },
  {
    "text": "teardown that there's just like maybe 10 lines of code they do all the real work and the rest of this is like",
    "start": "2587530",
    "end": "2592690"
  },
  {
    "text": "configuration and input and output I think that's my last example it is i want to mention a couple of other things",
    "start": "2592690",
    "end": "2599290"
  },
  {
    "text": "that you should watch if you're a data scientist one of them are going to get a talk on i think it's the next one actually are no is going to talk about",
    "start": "2599290",
    "end": "2605440"
  },
  {
    "text": "h2o which is a really amazing in-memory compute engine where a spark can handle",
    "start": "2605440",
    "end": "2610510"
  },
  {
    "text": "not only in memory but also data that has to be flushed to disk how many of you looked at the julia language by",
    "start": "2610510",
    "end": "2616120"
  },
  {
    "text": "chance so Julia as sort of an aspires to replace our it's it's a relatively new",
    "start": "2616120",
    "end": "2622660"
  },
  {
    "text": "but it's rapidly evolving it's a really elegant language you might want to check out so we have maybe five minutes for",
    "start": "2622660",
    "end": "2628420"
  },
  {
    "text": "questions",
    "start": "2628420",
    "end": "2630900"
  }
]