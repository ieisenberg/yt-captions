[
  {
    "text": "my name is Dean Wampler and this is a talk about going Beyond The Big Data hype and actually getting something",
    "start": "3240",
    "end": "8440"
  },
  {
    "text": "useful out of Technologies like Hadoop and nosql and so forth um it's based on",
    "start": "8440",
    "end": "14480"
  },
  {
    "text": "my experience as a consultant doing um Hado Consulting mostly and some nosql",
    "start": "14480",
    "end": "19840"
  },
  {
    "text": "the last few years and I've been doing Enterprise Java since the pene era um",
    "start": "19840",
    "end": "25119"
  },
  {
    "text": "and what I've seen is a lot of people are just really anxious to jump on on this bandwagon they don't want to be left out they don't want to miss all the",
    "start": "25119",
    "end": "31439"
  },
  {
    "text": "cool stuff they want to mine gold from Twitter sentiment analysis and stuff",
    "start": "31439",
    "end": "36559"
  },
  {
    "text": "like that but a lot of times they really don't know what they're doing uh they don't really know that they're caught up in a hype cycle and what they really",
    "start": "36559",
    "end": "43520"
  },
  {
    "text": "need to be doing is figuring out what value they want to extract from their information and that's the goal of this",
    "start": "43520",
    "end": "48800"
  },
  {
    "text": "talk what I'm going to do is start by defining what Big Data means to me uh so",
    "start": "48800",
    "end": "54559"
  },
  {
    "text": "we uh put put a definition on this buzzword uh and then talk about the",
    "start": "54559",
    "end": "60039"
  },
  {
    "text": "current ecosystem sort of focusing on Hadoop what's good about it and what isn't uh and talk about um particular",
    "start": "60039",
    "end": "67520"
  },
  {
    "text": "use cases that uh you may need to solve and what are the best tools and approaches for those uh use cases and",
    "start": "67520",
    "end": "74479"
  },
  {
    "text": "then speculate a little bit about the future so let me Begin by defining Big Data you may not be able to read this",
    "start": "74479",
    "end": "80000"
  },
  {
    "text": "from the back but there's two definitions here big data is anything which is Crash Excel or uh small data is when fit in",
    "start": "80000",
    "end": "87240"
  },
  {
    "text": "Ram Big Data is when it's crashed because it's not fit in Ram that's one definition or two actually uh",
    "start": "87240",
    "end": "94119"
  },
  {
    "text": "maybe a more realistic definition and I'll let you do the math on the right to make sure that's correct um big data is",
    "start": "94119",
    "end": "100640"
  },
  {
    "text": "data that's uh really so big that our traditional tools and approaches kind of break down either they get too expensive",
    "start": "100640",
    "end": "106119"
  },
  {
    "text": "or they simply cannot scale to the sizes that we need to scale to so we've invented alternative",
    "start": "106119",
    "end": "112880"
  },
  {
    "text": "approaches underlying all this stuff are uh three Trends uh one of which I think",
    "start": "112880",
    "end": "118159"
  },
  {
    "text": "is obvious and the other two maybe not so obvious the obvious one is the data",
    "start": "118159",
    "end": "123320"
  },
  {
    "text": "sizes are growing we're capturing more log data we're capturing more data about users um we're we're not throwing as as",
    "start": "123320",
    "end": "131160"
  },
  {
    "text": "much data away as we used to uh and so in general we just have more to manage and this is not anything that none of",
    "start": "131160",
    "end": "137720"
  },
  {
    "text": "you have not heard before what may be new is the idea that maybe we don't really want to be so hard",
    "start": "137720",
    "end": "145959"
  },
  {
    "text": "about schema anymore that maybe we need to relax our notion of schemas be more flexible and adaptable in part because",
    "start": "145959",
    "end": "154120"
  },
  {
    "text": "first of all uh the schema that applies to our data today is going to be evolving to something else pretty",
    "start": "154120",
    "end": "159480"
  },
  {
    "text": "quickly that's pretty common and the other thing that happens is that people want to bring together uh wildly",
    "start": "159480",
    "end": "165879"
  },
  {
    "text": "different kinds of data and and uh extract information extract value out of",
    "start": "165879",
    "end": "171200"
  },
  {
    "text": "the uh the intersection of those data sets so being able to be very flexible about data schema has turned out to be",
    "start": "171200",
    "end": "177599"
  },
  {
    "text": "really important and the last big trend is perhaps the most interesting in terms",
    "start": "177599",
    "end": "182800"
  },
  {
    "text": "of technical Innovation and that's the idea that we're deemphasizing the idea that our programs are Masters of the",
    "start": "182800",
    "end": "189480"
  },
  {
    "text": "Universe that they understand everything about our domain and instead we're writing generic algorithms that are",
    "start": "189480",
    "end": "195400"
  },
  {
    "text": "driven by data instead and this is a picture of the second generation self-driving car that Google built",
    "start": "195400",
    "end": "202000"
  },
  {
    "text": "called Stanley uh which is a great example of a data driven car it's using some fundamental probabilistic",
    "start": "202000",
    "end": "208840"
  },
  {
    "text": "algorithms to Fig figure out where it is to detect pedestrians uh and it turns",
    "start": "208840",
    "end": "214200"
  },
  {
    "text": "out they've driven these things several hundred thousand miles around the United States and the only accident the these",
    "start": "214200",
    "end": "221000"
  },
  {
    "text": "cars have ever had is when a grad student was actually driving the car so they it works very very",
    "start": "221000",
    "end": "227680"
  },
  {
    "text": "well uh another interesting example for me of this whole debate about formal",
    "start": "227680",
    "end": "233239"
  },
  {
    "text": "models versus just probabilistic uh figuring out structure is an argument",
    "start": "233239",
    "end": "238720"
  },
  {
    "text": "that broke out between the famous linguist gome Chomsky and Peter norvig",
    "start": "238720",
    "end": "243760"
  },
  {
    "text": "who's the director of research at Google and one of the Pioneers in probabilistic methods for machine learning and of",
    "start": "243760",
    "end": "250000"
  },
  {
    "text": "course n Chomsky is famous for building you very sophisticated models of how language works you know at a fundamental",
    "start": "250000",
    "end": "255959"
  },
  {
    "text": "level and then we layer on top of that English and Dutch and den Danish and you",
    "start": "255959",
    "end": "261320"
  },
  {
    "text": "know whatever whereas what has actually turned out to be most effective at building like machine translation tools",
    "start": "261320",
    "end": "269600"
  },
  {
    "text": "uh sentiment analysis has been building up probabilistic models that are relatively stupid about what's actually",
    "start": "269600",
    "end": "275400"
  },
  {
    "text": "happening in terms of meaning but probabilistically we infer meaning just by throwing lots of data at the problem",
    "start": "275400",
    "end": "282360"
  },
  {
    "text": "and so this sort of uh public debate erupted between these two guys I don't think they actually met face to face per",
    "start": "282360",
    "end": "288560"
  },
  {
    "text": "se but you know competing blog post sort of thing and interviews where they debated the merits of formal grammars",
    "start": "288560",
    "end": "295680"
  },
  {
    "text": "versus probabilistic reasoning about U the way things really work so those are",
    "start": "295680",
    "end": "301560"
  },
  {
    "text": "the three Trends and that sort of drives architectural decisions you know it used to be we all",
    "start": "301560",
    "end": "308639"
  },
  {
    "text": "built applications like this where uh some event like a query or a user login",
    "start": "308639",
    "end": "315000"
  },
  {
    "text": "or whatever triggered some information that or a query run against a database",
    "start": "315000",
    "end": "320120"
  },
  {
    "text": "uh we brought that in through an object relational mapping tool we put that in our perfect world model of java objects",
    "start": "320120",
    "end": "326520"
  },
  {
    "text": "or whatever in in middleware we must dodged it and then we sent it off to a",
    "start": "326520",
    "end": "331680"
  },
  {
    "text": "browser or whatever but now we're moving more towards this model which is you",
    "start": "331680",
    "end": "336960"
  },
  {
    "text": "know forget all the object stuff in the middle or at least this idea that we should have a a complete representation",
    "start": "336960",
    "end": "342479"
  },
  {
    "text": "of the world in objects in the middle and instead let's use the fact that our",
    "start": "342479",
    "end": "347520"
  },
  {
    "text": "databases are returning basically collections of data technically the term is a result set but it's really like a",
    "start": "347520",
    "end": "354280"
  },
  {
    "text": "giant a list array whatever hashmap why don't we just Embrace those colle ctions",
    "start": "354280",
    "end": "360560"
  },
  {
    "text": "maintain minimal knowledge of what it actually what the larger context is of the particular scenario we're",
    "start": "360560",
    "end": "366479"
  },
  {
    "text": "implementing and and stay lean and mean as a result because we just know only what we need to know to transform that",
    "start": "366479",
    "end": "372759"
  },
  {
    "text": "data in some way in the middle and then send it off to the browser which can also be functional if we're using",
    "start": "372759",
    "end": "378520"
  },
  {
    "text": "JavaScript in the right way and so forth so it's sort of forced us to focus",
    "start": "378520",
    "end": "383639"
  },
  {
    "text": "Less on domain specific objects and more on fundamental data structures you know",
    "start": "383639",
    "end": "388720"
  },
  {
    "text": "like the usual thing that we all know and love and work with those and stay",
    "start": "388720",
    "end": "394160"
  },
  {
    "text": "agnostic which goes back to this idea of like the probabilistic data structures",
    "start": "394160",
    "end": "399199"
  },
  {
    "text": "versus formal grammar models where our our code no longer knows a whole lot",
    "start": "399199",
    "end": "404720"
  },
  {
    "text": "about the domain it's working in it is just driven by data and actually the nosql movement",
    "start": "404720",
    "end": "410880"
  },
  {
    "text": "sort of fits into this in the same way where we're deemphasizing the schema deemphasizing domain knowledge that's",
    "start": "410880",
    "end": "417080"
  },
  {
    "text": "embedded in in the logic of our SCH schas and instead working with more fundamental data that uh is Rel you know",
    "start": "417080",
    "end": "424680"
  },
  {
    "text": "the database system is relatively agnostic about the data we're just working with uh the minimal",
    "start": "424680",
    "end": "430199"
  },
  {
    "text": "understanding we need and then you know moving with as much scalability and as much speed as we can in order to scale",
    "start": "430199",
    "end": "436400"
  },
  {
    "text": "but using the logic only where we need to to get the answers we need so an",
    "start": "436400",
    "end": "441759"
  },
  {
    "text": "unintended side effect of the object approach was that it was very difficult in Practical terms to decouple things",
    "start": "441759",
    "end": "448000"
  },
  {
    "text": "into small pieces so almost all of the mature Enterprise Java applications I've",
    "start": "448000",
    "end": "453400"
  },
  {
    "text": "ever seen were basically like this this big wad of code in the middle through which everything had to pass because uh",
    "start": "453400",
    "end": "460199"
  },
  {
    "text": "that big wad had all knowledge of the world but when we decouple things and focus on minimal abstractions and",
    "start": "460199",
    "end": "467560"
  },
  {
    "text": "minimal understanding and use a more functional approach then it turns out it's a lot easier to De decompose things",
    "start": "467560",
    "end": "473919"
  },
  {
    "text": "into smaller focused processes and guess what now we can you know make copies of",
    "start": "473919",
    "end": "479039"
  },
  {
    "text": "them so can run them concurrently uh we can run in parallel in other ways as well and hence we get better scalability",
    "start": "479039",
    "end": "485560"
  },
  {
    "text": "and it's a lot easier to maintain stuff like this that's well decoupled we can upgrade process to if we need to and and",
    "start": "485560",
    "end": "492680"
  },
  {
    "text": "keep going so that gives us smaller focused middleware and all of this structure",
    "start": "492680",
    "end": "499080"
  },
  {
    "text": "sort of fits back into the three driving forces where it scales better when we can scale horizontally with smaller",
    "start": "499080",
    "end": "505479"
  },
  {
    "text": "pieces rather than big wads of code uh it deemphasizing schema in the",
    "start": "505479",
    "end": "511280"
  },
  {
    "text": "sense that our objects are no longer you know models of the world but instead are",
    "start": "511280",
    "end": "516680"
  },
  {
    "text": "minimally uh focused uh manipulations of fundamental data fundamental Collections",
    "start": "516680",
    "end": "522240"
  },
  {
    "text": "and so forth and this better supports the idea that we're going to let the Data Drive the emerging Behavior rather",
    "start": "522240",
    "end": "528600"
  },
  {
    "text": "than try to build a model of the world that uh we have to force fit our data",
    "start": "528600",
    "end": "533640"
  },
  {
    "text": "into and the tools that we have available that support this today one of them is actually map ruce and the",
    "start": "533640",
    "end": "540279"
  },
  {
    "text": "distributed file system that goes with it called hdfs for Hadoop distributed file system now map Ru is actually",
    "start": "540279",
    "end": "547040"
  },
  {
    "text": "Pioneer to Google uh although of course the concept of map and reduce is basically old",
    "start": "547040",
    "end": "552800"
  },
  {
    "text": "mathematics uh and Google also had a distributed file system and then the the Hado project was a clean room clone of",
    "start": "552800",
    "end": "558480"
  },
  {
    "text": "this but it does fit the model nicely where we're going to have this generic infrastructure that knows uh only how to",
    "start": "558480",
    "end": "564320"
  },
  {
    "text": "map things and how to reduce things and we program it to do the actual Transformations we need it scales really",
    "start": "564320",
    "end": "570560"
  },
  {
    "text": "well horizontally the distributed file system uh in principle can can be unbounded in size although right now",
    "start": "570560",
    "end": "577519"
  },
  {
    "text": "there's some practical limitations in size and it turns out that uh companies like Facebook have 600 pedabytes now",
    "start": "577519",
    "end": "584519"
  },
  {
    "text": "resident in Hadoop clusters now one no one cluster could hold that much data",
    "start": "584519",
    "end": "589600"
  },
  {
    "text": "but you can easily uh scale up to several pedabytes in one single Hado cluster over several thousand",
    "start": "589600",
    "end": "596480"
  },
  {
    "text": "machines and there's other approaches besides Hadoop there's other kinds of middleware that we might write some",
    "start": "596480",
    "end": "602240"
  },
  {
    "text": "custom some off the shelf and then there's often people using no SQL databases as the uh durable tier",
    "start": "602240",
    "end": "608920"
  },
  {
    "text": "underneath and we'll talk about scenarios where you might use one versus the",
    "start": "608920",
    "end": "614680"
  },
  {
    "text": "other and one interesting example I think is worth mentioning is that suddenly we can actually use JavaScript",
    "start": "614680",
    "end": "621000"
  },
  {
    "text": "from top to bottom because of uh middleware capabilities like node.js and",
    "start": "621000",
    "end": "626519"
  },
  {
    "text": "Json based document databases like uh and of course we've had Jason in the",
    "start": "626519",
    "end": "632200"
  },
  {
    "text": "browser forever um well forever in dog years I guess anyway um we can now have",
    "start": "632200",
    "end": "638200"
  },
  {
    "text": "JavaScript you know all the way from top to bottom if we really want to which is actually pretty uh pretty compelling to",
    "start": "638200",
    "end": "644720"
  },
  {
    "text": "be able to use one tool for the whole thing although certainly there some people hate node so I'm not necessarily",
    "start": "644720",
    "end": "650680"
  },
  {
    "text": "saying node is the right thing to use but nevertheless I think you get the point well let me take just a moment to",
    "start": "650680",
    "end": "656800"
  },
  {
    "text": "describe what M produces in case you don't know what it is you've probably all heard the term by now but if you",
    "start": "656800",
    "end": "662720"
  },
  {
    "text": "don't actually know what it means it's worth just a moment to talk about it because it actually has some great strengths but also great",
    "start": "662720",
    "end": "670360"
  },
  {
    "text": "weaknesses basically a map reduced job is literally a map phase and a reduced phase and then some magic that happens",
    "start": "670360",
    "end": "677040"
  },
  {
    "text": "in the middle um in the map phase you actually read in the files off your",
    "start": "677040",
    "end": "682639"
  },
  {
    "text": "distributed file system and it turns out what Hadoop likes to do is store files in very large blocks Ty typically a",
    "start": "682639",
    "end": "689680"
  },
  {
    "text": "multiple of 128 megabytes whereas the underlying file systems that we're all used to like whatever's underneath the",
    "start": "689680",
    "end": "697079"
  },
  {
    "text": "had distributed file system the file system on my computer you know typically a block size is like five kilobytes or",
    "start": "697079",
    "end": "703000"
  },
  {
    "text": "something but in Hadoop they like to use hundreds of kilobyte block sizes and the reason is it's a very practical reason",
    "start": "703000",
    "end": "709800"
  },
  {
    "text": "going back to physics or at least hard drives um and we have these hard drives that are spinning rust uh iron oxide",
    "start": "709800",
    "end": "717639"
  },
  {
    "text": "basically and the this little head that's traveling over the top and that head movement is by far the slowest part",
    "start": "717639",
    "end": "723000"
  },
  {
    "text": "of the process it's you know like 10 milliseconds or something to move ahead across a hard drive what we want to do",
    "start": "723000",
    "end": "730000"
  },
  {
    "text": "is Park the hard that head and then just scan all the data that's spinning underneath it and and that's why you",
    "start": "730000",
    "end": "735440"
  },
  {
    "text": "want to have big contiguous blocks of memory and not uh stuff broken up like",
    "start": "735440",
    "end": "740880"
  },
  {
    "text": "you would have on a normal file system so Hadoop from the very beginning is very much oriented towards I have a lot",
    "start": "740880",
    "end": "747040"
  },
  {
    "text": "of data I want to scan all of it at once or maybe a big chunk of it and so I want it to be able to read it continuously",
    "start": "747040",
    "end": "753399"
  },
  {
    "text": "off off the hard drive I think the rule of thumb is that you can drain like a terabyte hard drive in 10 minutes it",
    "start": "753399",
    "end": "758760"
  },
  {
    "text": "takes that long to read a terabyte off a hard drive these days as fast as they",
    "start": "758760",
    "end": "763920"
  },
  {
    "text": "are it's a huge bottleneck so anyway uh the way Hadoop",
    "start": "763920",
    "end": "769079"
  },
  {
    "text": "works and this may be different for like Google Map ruce but you'll you'll spawn a jvm task for each uh block on the file",
    "start": "769079",
    "end": "776839"
  },
  {
    "text": "system and that initial task will do some sort of mapping over that data and to be specific let me just describe what",
    "start": "776839",
    "end": "783480"
  },
  {
    "text": "you would do with something called word count word count is like the hello world of Hadoop space it's the first program",
    "start": "783480",
    "end": "790120"
  },
  {
    "text": "everybody writes because it's a simple algorithm and so you can focus on the nasty API that we're going to look at in",
    "start": "790120",
    "end": "795320"
  },
  {
    "text": "a few minutes but basically in word count I want to read U you know a corpus of documents let's say",
    "start": "795320",
    "end": "801360"
  },
  {
    "text": "Wikipedia and I want to find all the words in that Corpus and then I want to count the occurrence of each word so you",
    "start": "801360",
    "end": "807279"
  },
  {
    "text": "know I'm going to see the word Shakespeare and the word Denmark and the word Hadoop and so forth and uh so I",
    "start": "807279",
    "end": "813720"
  },
  {
    "text": "want to find that across all of these documents and I've got a bunch of map tasks in parallel reading chunks of",
    "start": "813720",
    "end": "819880"
  },
  {
    "text": "Wikipedia and what they're going to spit out in the middle uh towards this thing that's called sort Shuffle here is some",
    "start": "819880",
    "end": "826360"
  },
  {
    "text": "key value Pairs and those in this case this algorithm the key value pairs will be each word it finds and then a count",
    "start": "826360",
    "end": "833240"
  },
  {
    "text": "of those words you know in the data that it processed and as these key value pairs",
    "start": "833240",
    "end": "838560"
  },
  {
    "text": "are coming out out they'll be sorted in the same process not globally but in the same process and then they're shuffled",
    "start": "838560",
    "end": "845959"
  },
  {
    "text": "which means that I want to make sure that all the occurrences of Denmark show up in let's say the second reduced task",
    "start": "845959",
    "end": "852560"
  },
  {
    "text": "which is another jvm process running in my cluster somewhere maybe all the Shakespeare key value pairs will show up",
    "start": "852560",
    "end": "858600"
  },
  {
    "text": "on the first one and so forth and you can see where this is going the reducer is then going to get all of the",
    "start": "858600",
    "end": "863959"
  },
  {
    "text": "Shakespeare counts and all the and another one's going to get all the Denmark counts and it's going to do a final total count and output the global",
    "start": "863959",
    "end": "871720"
  },
  {
    "text": "count for each of those words that is really all there is to a map ruce job it's you know these three steps the one",
    "start": "871720",
    "end": "879000"
  },
  {
    "text": "in the middle that Hado does for you the sort Shuffle and then the two on the map and the reduced task that you write",
    "start": "879000",
    "end": "885880"
  },
  {
    "text": "yourself now it turns out if you want to do something like a SQL join or a group ey that's where things get really hairy",
    "start": "885880",
    "end": "892000"
  },
  {
    "text": "because even though this is very generic and flexible in in some respects it's also very hard to map a lot of highlevel",
    "start": "892000",
    "end": "898399"
  },
  {
    "text": "Concepts to to these low-level apis and most of the time in more complicated",
    "start": "898399",
    "end": "903519"
  },
  {
    "text": "scenarios like a group ey followed by a sort and that kind of SQL statement you might write you'll actually have to",
    "start": "903519",
    "end": "908920"
  },
  {
    "text": "sequence a bunch of these jobs now that leads to one of the important limitations of Hadoop after each of",
    "start": "908920",
    "end": "915199"
  },
  {
    "text": "these map reduce tasks it's going to write everything back to disk and if I've got a bunch of job sequenced",
    "start": "915199",
    "end": "920279"
  },
  {
    "text": "together then I'm going to read all that stuff back into memory so there's a bit of inefficiency here but it's a sort of",
    "start": "920279",
    "end": "925759"
  },
  {
    "text": "inefficiency that's been tolerated because if I need to process 100 terabytes of data then I'd rather throw",
    "start": "925759",
    "end": "932079"
  },
  {
    "text": "a thousand machines at it or whatever and and even though it's each process is inefficient at least in the aggregate I",
    "start": "932079",
    "end": "938720"
  },
  {
    "text": "get results as fast as I want uh so that's sort of the gist of",
    "start": "938720",
    "end": "945319"
  },
  {
    "text": "how map reduce Works uh this is an example of something a map reduce job without a reducer um",
    "start": "945319",
    "end": "953720"
  },
  {
    "text": "I'll let you figure out if that makes any sense well it turns out right now Hadoop is really the best generic tool we have",
    "start": "953720",
    "end": "960399"
  },
  {
    "text": "for this kind of general purpose arbitrary uh massive data set",
    "start": "960399",
    "end": "966360"
  },
  {
    "text": "manipulation um you know it's free although free is as in not quite as in",
    "start": "966360",
    "end": "971759"
  },
  {
    "text": "beer because you do have to do a lot of work to make it to tune it and all that stuff but it's it's a sort of thing a lot of people can adopt fairly",
    "start": "971759",
    "end": "977920"
  },
  {
    "text": "inexpensively right now uh for if you look at the cost per terabyte Hadoop is dramatically cheaper than almost any",
    "start": "977920",
    "end": "983680"
  },
  {
    "text": "other option we have except some nosql databases but as I descri cribed it",
    "start": "983680",
    "end": "989319"
  },
  {
    "text": "really wants to do everything in batch mode it really wants to take that data that you've been collecting and do some big analysis all at once it doesn't at",
    "start": "989319",
    "end": "996360"
  },
  {
    "text": "all want to work with stuff incrementally it's you can't write incrementally to the file system",
    "start": "996360",
    "end": "1001759"
  },
  {
    "text": "conveniently uh the way that it scales up and the overhead of trying to be massively concurrent uh doesn't let you",
    "start": "1001759",
    "end": "1008560"
  },
  {
    "text": "do incremental uh event processing very well and we'll come back to that as a",
    "start": "1008560",
    "end": "1013639"
  },
  {
    "text": "problem that we need to solve so it's not so great for stream uh streams",
    "start": "1013639",
    "end": "1019360"
  },
  {
    "text": "events it's also no good for transactions because once again it doesn't want to work with individual",
    "start": "1019360",
    "end": "1024640"
  },
  {
    "text": "records for some definition of record it just wants to work with all the data at once so transactions is another problem",
    "start": "1024640",
    "end": "1030880"
  },
  {
    "text": "you might need to solve for big data and that API that I walked you through sort of conceptually is somewhat",
    "start": "1030880",
    "end": "1037360"
  },
  {
    "text": "coar grained it you would like something that's smaller and more composable so that you can combine things together to",
    "start": "1037360",
    "end": "1043319"
  },
  {
    "text": "get the kind of behaviors you want whereas trying to force fit everything into you know one map in one reduced",
    "start": "1043319",
    "end": "1049760"
  },
  {
    "text": "phase or a sequence of those is actually difficult it's it's one of those things that becomes a special skill that",
    "start": "1049760",
    "end": "1055240"
  },
  {
    "text": "certain developers have and the rest of us are doing everything we can to avoid having to acquire that skill because we",
    "start": "1055240",
    "end": "1061280"
  },
  {
    "text": "don't want to spend the time on it so one of the first Alternatives I want to mention today is this tool called spark",
    "start": "1061280",
    "end": "1068320"
  },
  {
    "text": "which started as a research project at Berkeley University in California uh and it's very similar in",
    "start": "1068320",
    "end": "1075640"
  },
  {
    "text": "the sense that it's designed to be highly distributed uh to let you do um",
    "start": "1075640",
    "end": "1080840"
  },
  {
    "text": "the sort of general purpose computation that Hadoop is good at but it has a lot of advantages over Hadoop at least now",
    "start": "1080840",
    "end": "1087679"
  },
  {
    "text": "it hasn't been battle tested at at hundreds of pedabytes of data but at least by design it has a lot of",
    "start": "1087679",
    "end": "1093600"
  },
  {
    "text": "advantages for example it caches data in memory between steps and so sometimes",
    "start": "1093600",
    "end": "1098919"
  },
  {
    "text": "it's actually easy to get jobs that run 30 to even 100 times faster in Spark",
    "start": "1098919",
    "end": "1104520"
  },
  {
    "text": "versus map ruce but it is something that needs more battle testing to be as uh",
    "start": "1104520",
    "end": "1109880"
  },
  {
    "text": "robust as Hadoop is in terms of being proven and and known reliable now another problem with Hadoop",
    "start": "1109880",
    "end": "1116679"
  },
  {
    "text": "that's really specific to Hadoop is that the low-level Java API is very verbose and difficult to use so for example",
    "start": "1116679",
    "end": "1123240"
  },
  {
    "text": "here's word count in that API and I know that nobody can read this and that's sort of by Design um the actual",
    "start": "1123240",
    "end": "1130360"
  },
  {
    "text": "interesting logic that I describ verbally a minute ago is just in a relatively few lines of code and",
    "start": "1130360",
    "end": "1135880"
  },
  {
    "text": "actually I'm not even showing you the whole program I haven't shown you the main routine which is another 20 or 30",
    "start": "1135880",
    "end": "1141360"
  },
  {
    "text": "lines of setup so this is this is a lot of work excuse me uh besides the fact",
    "start": "1141360",
    "end": "1147520"
  },
  {
    "text": "that it's kind of verbose there's a lot of setup to do and then if I want to do a non-trivial algorithm I really have to",
    "start": "1147520",
    "end": "1153080"
  },
  {
    "text": "develop an expertise of how to map that highlevel concept to this lowlevel",
    "start": "1153080",
    "end": "1158400"
  },
  {
    "text": "API in fact I think it looks a lot like ejb's uh when I first started reading this code I thought man we're well it's",
    "start": "1158400",
    "end": "1164760"
  },
  {
    "text": "it should the '90s was a little early it's really like the early 2000s but uh anyway um it really feels like we're back to",
    "start": "1164760",
    "end": "1171159"
  },
  {
    "text": "the ejb days and you know ejbs so everybody likes to complain about them now because they were kind of a failed",
    "start": "1171159",
    "end": "1176919"
  },
  {
    "text": "experiment but they did get people thinking about Enterprise Java they solved some problems just not especially",
    "start": "1176919",
    "end": "1183440"
  },
  {
    "text": "well the problem with ejbs were they it was a inefficient protocol it was very invasive at the API level in your",
    "start": "1183440",
    "end": "1190400"
  },
  {
    "text": "application and it took some rethinking by people like the sprang framework and Rod Johnson to figure out the right way",
    "start": "1190400",
    "end": "1196440"
  },
  {
    "text": "to do Enterprise Java and I think the same thing is going to happen with a dupe so in fact if you use a particular",
    "start": "1196440",
    "end": "1203919"
  },
  {
    "text": "scalar based tool for writing this this job then you get something that looks a whole lot smaller and in fact without",
    "start": "1203919",
    "end": "1210480"
  },
  {
    "text": "going into a lot of details about the code it's pretty easy to understand what's going on here once you understand",
    "start": "1210480",
    "end": "1216400"
  },
  {
    "text": "a few little basics of the scalding API the scalding is a project that Twitter wrote It's it is actually a Hadoop API",
    "start": "1216400",
    "end": "1223679"
  },
  {
    "text": "it sets actually two levels above the lowlevel API I described but you get",
    "start": "1223679",
    "end": "1228880"
  },
  {
    "text": "back to something that's much more uh what we really want which is to express our algorithms using the terminology",
    "start": "1228880",
    "end": "1236120"
  },
  {
    "text": "that is appropriate for the level that we're working so whenever people are learning Hadoop when I'm mentoring",
    "start": "1236120",
    "end": "1241640"
  },
  {
    "text": "people on Hadoop I tell them you know adopt scalding or if you're a closure guy adopt CasCal things like that but",
    "start": "1241640",
    "end": "1247840"
  },
  {
    "text": "avoid the low-level API and just for convenience spark gives you something very similar this is the",
    "start": "1247840",
    "end": "1253600"
  },
  {
    "text": "same API in spark and it gives the same kind of concise uh very descriptive",
    "start": "1253600",
    "end": "1259159"
  },
  {
    "text": "and intuitive kind of explanation once you understand things like flat mapping and mapping and reduction then it's",
    "start": "1259159",
    "end": "1265960"
  },
  {
    "text": "really easy to use these apis which is the way it should be so that's one problem we need to solve in Big Data is",
    "start": "1265960",
    "end": "1271880"
  },
  {
    "text": "expressive apis that let us express our intent as concisely as possible and",
    "start": "1271880",
    "end": "1277080"
  },
  {
    "text": "these are examples that are taking us in that direction okay but let's look at a few specific usage scenarios problems that",
    "start": "1277080",
    "end": "1283880"
  },
  {
    "text": "we need to solve where we think we have a big data problem and what that means for us and first I want to contrast when",
    "start": "1283880",
    "end": "1290360"
  },
  {
    "text": "you would use Hadoop versus a database either SQL or no SQL for that matter",
    "start": "1290360",
    "end": "1295880"
  },
  {
    "text": "well Hadoop is great when you really do have a very flexible need for computation um you know machine learning",
    "start": "1295880",
    "end": "1301840"
  },
  {
    "text": "algorithms don't really fit a relational model query very well but they're easy",
    "start": "1301840",
    "end": "1307679"
  },
  {
    "text": "relatively speaking to implement in a tool like Hadoop Hadoop is great if you",
    "start": "1307679",
    "end": "1312799"
  },
  {
    "text": "really do have massive data sets that you need to do scan scans of to do a sort of global anal",
    "start": "1312799",
    "end": "1319120"
  },
  {
    "text": "itics and it's okay or great if if all if you do all this in batch mode in other words it's not an online system",
    "start": "1319120",
    "end": "1325440"
  },
  {
    "text": "that has to be fast because you've got a user setting at a browser it's something where you run it overnight to analyze",
    "start": "1325440",
    "end": "1330799"
  },
  {
    "text": "your pedabytes of log data or whatever whereas on the other hand if you have a",
    "start": "1330799",
    "end": "1336039"
  },
  {
    "text": "a problem that does fit a particular data model whether it's relational or document oriented or key value store",
    "start": "1336039",
    "end": "1343559"
  },
  {
    "text": "then databases are fantastic for this and in particular if you need transactional Behavior of some kind if",
    "start": "1343559",
    "end": "1350039"
  },
  {
    "text": "you need record level access as opposed to Full Table scans and even I have a event driven",
    "start": "1350039",
    "end": "1357039"
  },
  {
    "text": "here because one thing that people use these databases for is to solve the event problem where I treat each you",
    "start": "1357039",
    "end": "1362480"
  },
  {
    "text": "know interaction with a database as an event so it's great for these sorts of things and in fact most large",
    "start": "1362480",
    "end": "1369480"
  },
  {
    "text": "organizations need both kinds of feature sets and so they often combine the two and one of the interesting combinations",
    "start": "1369480",
    "end": "1376000"
  },
  {
    "text": "is using this database called hbas which actually sits on top of the Hadoop distributed file system when you need",
    "start": "1376000",
    "end": "1382640"
  },
  {
    "text": "that kind of record level access and fast updates and deletes and so forth but you want to do Global analytics over",
    "start": "1382640",
    "end": "1389600"
  },
  {
    "text": "that data uh each night or whatever then hbas plus a dup is a good",
    "start": "1389600",
    "end": "1395360"
  },
  {
    "text": "combination well the other way that we're seeing Hadoop in particular used is as a lowcost data warehouse option so",
    "start": "1395360",
    "end": "1403039"
  },
  {
    "text": "I worked with a client once who could only keep six months of the data that they were accumulating and they accumulated a lot of data the only way",
    "start": "1403039",
    "end": "1410480"
  },
  {
    "text": "they were going to get beyond that six-month window was to actually drop another million dollars on a unnamed um",
    "start": "1410480",
    "end": "1416799"
  },
  {
    "text": "data warehouse suppliance and they were they did not want to spend a million dollars when they could uh use something",
    "start": "1416799",
    "end": "1422320"
  },
  {
    "text": "that was on10th a cost and what we did for this customer actually is migrate them to Hadoop and now they they have",
    "start": "1422320",
    "end": "1429400"
  },
  {
    "text": "like several years of data in a cluster that's I think 160 nodes or something like that well the advantages though of a",
    "start": "1429400",
    "end": "1436679"
  },
  {
    "text": "traditional data warehouse here PA datas and so forth is that they are mature",
    "start": "1436679",
    "end": "1442080"
  },
  {
    "text": "Technologies they give you a very nice environment they tend to be fast they do scale Beyond traditional uh analytical",
    "start": "1442080",
    "end": "1448799"
  },
  {
    "text": "databases or online databases rather and they give you rich SQL and query",
    "start": "1448799",
    "end": "1454039"
  },
  {
    "text": "semantics which is great and so if your data set isn't petabyte size then maybe",
    "start": "1454039",
    "end": "1459440"
  },
  {
    "text": "you just pay the money for one of these things because they they're just mature and they work well but they are extremely expensive they can be up to a",
    "start": "1459440",
    "end": "1466000"
  },
  {
    "text": "100 times more expensive at at a terabyte cost level compared to Hado and",
    "start": "1466000",
    "end": "1471559"
  },
  {
    "text": "they do have this limit that when you're getting to those really big data set sizes they don't scale so",
    "start": "1471559",
    "end": "1476799"
  },
  {
    "text": "well well one thing you might consider doing is what if we used a nosql database as a replacement for a data",
    "start": "1476799",
    "end": "1482640"
  },
  {
    "text": "warehouse and this will work in some scenarios but the fact is that when you think about the environment you're",
    "start": "1482640",
    "end": "1488360"
  },
  {
    "text": "dropping these things into you really need to support SQL uh a lot of the Enterprises I go into you'll have like a",
    "start": "1488360",
    "end": "1495080"
  },
  {
    "text": "few it's sort of like an inverted pyramid you'll have like a few administrator ERS at the bottom you know",
    "start": "1495080",
    "end": "1500120"
  },
  {
    "text": "a larger group of developers and then you'll have this massive team of data analysts that know how to write SQL",
    "start": "1500120",
    "end": "1505559"
  },
  {
    "text": "queries or know how to work with SAS or r or one of those tools and so unless",
    "start": "1505559",
    "end": "1510960"
  },
  {
    "text": "you have something that gives them a SQL abstraction you're probably dead in the water um now there are some nosql",
    "start": "1510960",
    "end": "1517559"
  },
  {
    "text": "databases quote unquote now that are offering SQL query capabilities like Cassandra for example but for the most",
    "start": "1517559",
    "end": "1524640"
  },
  {
    "text": "part people are going to need a SQL answer for this problem and it turns out even though Hadoop is designed to",
    "start": "1524640",
    "end": "1531480"
  },
  {
    "text": "be it's not working all of a sudden all right let's try this even though Hadoop is designed to be uh this very generic",
    "start": "1531480",
    "end": "1538159"
  },
  {
    "text": "data platform it's actually quite good for doing SQL kind of stuff as we'll see in a minute and once again we're talking",
    "start": "1538159",
    "end": "1544520"
  },
  {
    "text": "about uh data warehouse problems not online transactions where I've you know I've got this data set and I need to do",
    "start": "1544520",
    "end": "1551039"
  },
  {
    "text": "offline analytics on it so I don't need the I mean put it another way the batch mode nature of Hadoop is okay I can",
    "start": "1551039",
    "end": "1558799"
  },
  {
    "text": "tolerate that because I'm I'm doing offline analytics so even though we have a lot",
    "start": "1558799",
    "end": "1564480"
  },
  {
    "text": "of benefits of traditional data warehouses which we just discussed uh huba also has an interesting trade-off",
    "start": "1564480",
    "end": "1570600"
  },
  {
    "text": "to offer now it is less mature and it's less sophisticated as a SQL environment which is kind of shocking to people",
    "start": "1570600",
    "end": "1576720"
  },
  {
    "text": "unless you warn them about this but the sequel story is getting better uh as we'll discuss in a second",
    "start": "1576720",
    "end": "1583600"
  },
  {
    "text": "so it actually is a viable option for data warehousing and the cost and the",
    "start": "1583600",
    "end": "1588679"
  },
  {
    "text": "scalability cannot be beaten so that's why one of the most common scenarios for using Hadoop now is",
    "start": "1588679",
    "end": "1595520"
  },
  {
    "text": "as a data warehouse replacement so let's talk for a minute then about SQL on Hadoop and what that means it's not",
    "start": "1595520",
    "end": "1602240"
  },
  {
    "text": "really a contradiction in terms well it turns out you know uh Facebook was one of the first companies to really scale",
    "start": "1602240",
    "end": "1608240"
  },
  {
    "text": "Hadoop in a big way Yahoo maybe being the other big one and uh Facebook had the problem I just described where they",
    "start": "1608240",
    "end": "1614440"
  },
  {
    "text": "had a lot of data analysts they were collecting all this data about their users maybe more than we realized or",
    "start": "1614440",
    "end": "1620120"
  },
  {
    "text": "whatever um and they wanted to these data analysts wanted to use SQL to quiriat so Facebook actually wrote a",
    "start": "1620120",
    "end": "1626440"
  },
  {
    "text": "hive query tool on top of Hadoop called uh sorry a SQL query tool called Hive",
    "start": "1626440",
    "end": "1632440"
  },
  {
    "text": "and it's it doesn't give you transactions there's no crud operations create read update and delete it really",
    "start": "1632440",
    "end": "1637880"
  },
  {
    "text": "is just scan this table which in this case is just files on hdfs and use Query",
    "start": "1637880",
    "end": "1644120"
  },
  {
    "text": "abstractions that are then turned into map reduce jobs so in a sense it's a DSL a domain specific language for Hadoop",
    "start": "1644120",
    "end": "1651320"
  },
  {
    "text": "but it's been extremely successful as a result in fact it's so important one of the books I wrote was unive even though",
    "start": "1651320",
    "end": "1657240"
  },
  {
    "text": "I'm not really a SQL person uh I realized just from talking to customers that it was essential to have good",
    "start": "1657240",
    "end": "1662679"
  },
  {
    "text": "documentation on this tool so uh I helped write a book on it and there's some new options emerging there's a port",
    "start": "1662679",
    "end": "1669399"
  },
  {
    "text": "of Hive to spark called Shark um there's a new version of Hive a sort of Hive",
    "start": "1669399",
    "end": "1676320"
  },
  {
    "text": "that's called Impala where they're replaced the slow map reduced back end which a with a much faster query engine",
    "start": "1676320",
    "end": "1682440"
  },
  {
    "text": "and then there's even now a much more anti compliant SQL version of a tool called lingual that's based on an API",
    "start": "1682440",
    "end": "1690159"
  },
  {
    "text": "called cascading so it's sort of like a thousand flowers or blooming in the SQL",
    "start": "1690159",
    "end": "1695200"
  },
  {
    "text": "space on Hadoop and it's in it's in fact you can do things like even write word count in",
    "start": "1695200",
    "end": "1701159"
  },
  {
    "text": "Hive that so this is actually the entire program for doing word count on hive uh",
    "start": "1701159",
    "end": "1706440"
  },
  {
    "text": "it's just about as concise is that scalding example and it demonstrates that when you have a SQL or rather a",
    "start": "1706440",
    "end": "1712440"
  },
  {
    "text": "query problem it's really hard to beat the concision of Sequel and that's why people really want to be able to use",
    "start": "1712440",
    "end": "1720760"
  },
  {
    "text": "it well just quickly uh I don't want to spend a lot of time on these but hi was this the pioneer of this movement it was",
    "start": "1720840",
    "end": "1727440"
  },
  {
    "text": "invented at Facebook to solve their problems but it does run M Rue so no quer is going to come back you know",
    "start": "1727440",
    "end": "1732600"
  },
  {
    "text": "instantaneously over a small set it's going to take seconds to minutes to get anything back",
    "start": "1732600",
    "end": "1738799"
  },
  {
    "text": "shark actually gives you much better performance because spark is much faster than m ruce in most cases but it's not",
    "start": "1738799",
    "end": "1744840"
  },
  {
    "text": "as proven in terms of U you know deployments and and battle hardening and",
    "start": "1744840",
    "end": "1750159"
  },
  {
    "text": "then there's this improved uh query engine underneath the hive uh SQL language that's going to make Hive",
    "start": "1750159",
    "end": "1756679"
  },
  {
    "text": "somewhat obsolete but it's going to take a while for uh the Impala tool to catch up with all of hives",
    "start": "1756679",
    "end": "1762960"
  },
  {
    "text": "features so one of the things that's happening though is that people are starting to recognize that map while",
    "start": "1762960",
    "end": "1768480"
  },
  {
    "text": "it's gotten us a long ways and it's been a powerful enabler it really does have a lot of disadvantages a lot of",
    "start": "1768480",
    "end": "1773640"
  },
  {
    "text": "limitations and so when wherever we can replace it with better compute engines we should do it and this is a good",
    "start": "1773640",
    "end": "1780399"
  },
  {
    "text": "example of a query specific compute engine that's being used in by Impala",
    "start": "1780399",
    "end": "1785960"
  },
  {
    "text": "instead of mauce and I think we'll see this as a trend where M ruce will slowly be segmented to a smaller and smaller",
    "start": "1785960",
    "end": "1793120"
  },
  {
    "text": "section of the compute environment if you will and we'll have alternative compute engines running",
    "start": "1793120",
    "end": "1798919"
  },
  {
    "text": "and then finally this alternative if you really want more ansy compliance SQL this tool called lingual gives you that",
    "start": "1798919",
    "end": "1804760"
  },
  {
    "text": "more than Hive does well another scenario that people need to solve is search so if you think",
    "start": "1804760",
    "end": "1811480"
  },
  {
    "text": "about what you're doing when you search Google they've actually done a index over the inter interwebs so that when",
    "start": "1811480",
    "end": "1817720"
  },
  {
    "text": "you do a query for Denmark or Shakespeare whatever you get the most relevant Pages back and that's the",
    "start": "1817720",
    "end": "1824279"
  },
  {
    "text": "general problem of enabling search and it could be search of I want to index the Weeki in our corporate environment I",
    "start": "1824279",
    "end": "1831159"
  },
  {
    "text": "want to index customer records that have like transcripts of phone calls things",
    "start": "1831159",
    "end": "1836200"
  },
  {
    "text": "like that this is an example of a specific solution being added uh to",
    "start": "1836200",
    "end": "1841840"
  },
  {
    "text": "solve a problem rather than going with a general purpose framework like M ruce and the two best known examples is U",
    "start": "1841840",
    "end": "1848960"
  },
  {
    "text": "Lucine with solar or elastic search elastic search I think has a talk this afternoon but these are great examples",
    "start": "1848960",
    "end": "1855919"
  },
  {
    "text": "of a much more domain specific Solution that's tailored to a particular problem uh and it solves it in a much better way",
    "start": "1855919",
    "end": "1862600"
  },
  {
    "text": "than you could solve it just using generic Hadoop and so forth now I mentioned that",
    "start": "1862600",
    "end": "1869639"
  },
  {
    "text": "um Hadoop is a batch mode system but there's a lot of problems that are more like real-time event processing so how",
    "start": "1869639",
    "end": "1876600"
  },
  {
    "text": "do we solve those um if we can't do it with the dup because it's not so great at solving these problems then what are",
    "start": "1876600",
    "end": "1882919"
  },
  {
    "text": "we going to use well there are several Alternatives and one of them that I'll mention is called storm uh this is very",
    "start": "1882919",
    "end": "1889919"
  },
  {
    "text": "much it's it's sort of conceptually like Hadoop in the sense that it's designed to provide distributed processing of",
    "start": "1889919",
    "end": "1896360"
  },
  {
    "text": "events but it is very much focused on event processing as opposed to batch mode",
    "start": "1896360",
    "end": "1902080"
  },
  {
    "text": "processing and it tries to do this with distributed reliable services so that you can trust this thing in a high",
    "start": "1902080",
    "end": "1908320"
  },
  {
    "text": "volume highs scale environment uh conceptually a spark deployment looks like this where they",
    "start": "1908320",
    "end": "1914159"
  },
  {
    "text": "treat their data sources as spouts and you implement spouts for file systems for message cues whatever and then the",
    "start": "1914159",
    "end": "1921840"
  },
  {
    "text": "processing is done by bolts where the messages are channeled to these things they make decisions about the data they",
    "start": "1921840",
    "end": "1927399"
  },
  {
    "text": "transform it and so forth uh and then you know send it off to wherever it may land could be landing in hdfs it could",
    "start": "1927399",
    "end": "1934559"
  },
  {
    "text": "be landing in some other message CU or whatever so to compare these two um both",
    "start": "1934559",
    "end": "1942519"
  },
  {
    "text": "of them are cheap both of them are highly scalable uh Hadoop has much better commercial support in battl test",
    "start": "1942519",
    "end": "1948080"
  },
  {
    "text": "in in real deployments than storm does at this point but that's going to change storm is just newer and less mature and",
    "start": "1948080",
    "end": "1955000"
  },
  {
    "text": "doesn't have a a it's actually commercial support is starting to emerge now in fact the data Stacks I think is",
    "start": "1955000",
    "end": "1960279"
  },
  {
    "text": "one company that's offering it now but again Hadoop is great for these batch mode problems storm is great for",
    "start": "1960279",
    "end": "1966720"
  },
  {
    "text": "event stream processing well if you have individual events coming in you know that's sort of",
    "start": "1966720",
    "end": "1972480"
  },
  {
    "text": "what a online database transactional database will do for you so maybe you could throw a database and and let that",
    "start": "1972480",
    "end": "1978720"
  },
  {
    "text": "come to your rescue now I have to talk about this picture for a second so this is literally outside my condo window in",
    "start": "1978720",
    "end": "1984919"
  },
  {
    "text": "Chicago I I live on the 34th floor and this helicopter is below ey level uh so",
    "start": "1984919",
    "end": "1990039"
  },
  {
    "text": "why is a helicopter flying between the buildings well it turns out they use these heavy lift helicopters to lift",
    "start": "1990039",
    "end": "1995679"
  },
  {
    "text": "like new Air Conditioning and Heating units off the street and put them on top of buildings when they're upgrading or",
    "start": "1995679",
    "end": "2001200"
  },
  {
    "text": "replacing old units so some every now and then on a Sunday morning you'll hear this whack whack whack whack and you",
    "start": "2001200",
    "end": "2006559"
  },
  {
    "text": "wake up and look out the window and there a helicopter busing by so anyway so well what about a database",
    "start": "2006559",
    "end": "2013919"
  },
  {
    "text": "well it turns out that you know obviously seel databases are great at transactional create read update and",
    "start": "2013919",
    "end": "2019000"
  },
  {
    "text": "delete operations so a lot of times that's what people use but in this sort of Big Data world it's become popular to",
    "start": "2019000",
    "end": "2025720"
  },
  {
    "text": "combine and no SQL database like Cassandra for this kind of processing to store stuff very quickly or if you",
    "start": "2025720",
    "end": "2032360"
  },
  {
    "text": "really are a Hadoop Centric shop where you want to be able to run map reduce jobs a lot of people just use hbase",
    "start": "2032360",
    "end": "2038159"
  },
  {
    "text": "because hbas uses uh hdfs the file system as this durable storage so you",
    "start": "2038159",
    "end": "2043279"
  },
  {
    "text": "can be doing transactional stuff with hbas and then run your map reduce jobs that are analyzing the same data so",
    "start": "2043279",
    "end": "2049878"
  },
  {
    "text": "that's a pretty common com combination scenario okay so one uh other use case",
    "start": "2049879",
    "end": "2056878"
  },
  {
    "text": "that is kind of driving the excitement about big data is machine learning so how many times if you heard someone say",
    "start": "2056879",
    "end": "2063079"
  },
  {
    "text": "boy I wish I could mine Twitter for sentiment to find out you know what people want to buy or what they think of",
    "start": "2063079",
    "end": "2068480"
  },
  {
    "text": "my company well that's exactly what we're talking about is doing things like",
    "start": "2068480",
    "end": "2074320"
  },
  {
    "text": "um you know recommendation engines like the famous Netflix way that it recommends movies to you based on what",
    "start": "2074320",
    "end": "2080040"
  },
  {
    "text": "you've seen already Amazon does this for products uh spam filters are probably the most valuable machine learning tool",
    "start": "2080040",
    "end": "2086638"
  },
  {
    "text": "that all of us rely on every day imagine if you had to deal with Spam as much as we used to and so forth down the line a whole",
    "start": "2086639",
    "end": "2093919"
  },
  {
    "text": "bunch of machine learning things that you can do well you can Implement these algorithms in map ruce but again it's",
    "start": "2093919",
    "end": "2099880"
  },
  {
    "text": "not such a great model for it one reason actually in this particular case is a lot of these machine learning algorithms are very iterative especially when",
    "start": "2099880",
    "end": "2106280"
  },
  {
    "text": "you're training the system uh for example there's a mo there's an approach",
    "start": "2106280",
    "end": "2111800"
  },
  {
    "text": "called stochastic gradient descent or where you're uh you have an equation that you can't solve analytically and",
    "start": "2111800",
    "end": "2117920"
  },
  {
    "text": "you just have to guess the solution and iterate towards the actual Solution that's not so great if you have to do",
    "start": "2117920",
    "end": "2123560"
  },
  {
    "text": "this like in a map reduced job in a map produced job and so forth to figured out workarounds for",
    "start": "2123560",
    "end": "2128960"
  },
  {
    "text": "these problems but map reduce is not the the best tool and especially programming",
    "start": "2128960",
    "end": "2134200"
  },
  {
    "text": "these algorithms is difficult well it turns out there's some interesting Alternatives that have emerged and one",
    "start": "2134200",
    "end": "2139400"
  },
  {
    "text": "of them is a a tool called pattern where you build predictive models in a tool",
    "start": "2139400",
    "end": "2144520"
  },
  {
    "text": "like SAS or r one of these tools that a lot of machine learning experts like to use you export them in a markup language",
    "start": "2144520",
    "end": "2151920"
  },
  {
    "text": "and then you can just run them without modification on one of these uh Platforms in this case cascading is the",
    "start": "2151920",
    "end": "2157839"
  },
  {
    "text": "platform and underneath that is map reduce so there's a lot of ways people are solving this problem of implementing",
    "start": "2157839",
    "end": "2164119"
  },
  {
    "text": "machine learning algorithms in a non-trivial uh platform like AUP and it",
    "start": "2164119",
    "end": "2169480"
  },
  {
    "text": "turns out spark which we mentioned already is actually originally designed for map produce so it's actually an",
    "start": "2169480",
    "end": "2174839"
  },
  {
    "text": "easier tool to use at the API level if you're writing map produce or sorry machine learning",
    "start": "2174839",
    "end": "2180160"
  },
  {
    "text": "algorithms because that was one of the original design goals of spark uh and just finally for",
    "start": "2180160",
    "end": "2186280"
  },
  {
    "text": "completeness how this often works and practice is you might use Hado to train your models because you're you're",
    "start": "2186280",
    "end": "2191680"
  },
  {
    "text": "training it over a very large data set and then store the model data you know the coefficients of the equations if or",
    "start": "2191680",
    "end": "2197960"
  },
  {
    "text": "whatever in some faster storage medium like no SQL so that in real time when",
    "start": "2197960",
    "end": "2203040"
  },
  {
    "text": "you're making recommendations you're you're not asking Hado for the answer you're asking a database for the pre-computed",
    "start": "2203040",
    "end": "2210079"
  },
  {
    "text": "answers uh the last one I want to talk about is um a common data scenario which",
    "start": "2210079",
    "end": "2216480"
  },
  {
    "text": "is like analyzing social network data so Google invented map produce in",
    "start": "2216480",
    "end": "2222599"
  },
  {
    "text": "part to implement algorithms like their famous page rank where they build basically if you think about the web it's just a giant graph right of links",
    "start": "2222599",
    "end": "2229520"
  },
  {
    "text": "from one page to the next and other great examples of course are your followers and those people following you",
    "start": "2229520",
    "end": "2235160"
  },
  {
    "text": "on Twitter your friends on Facebook Etc well it turns out these are giant graphs",
    "start": "2235160",
    "end": "2240280"
  },
  {
    "text": "so you want to use graph algorithms to analyze them but once again map ruce is not a terribly great fit because in the",
    "start": "2240280",
    "end": "2246800"
  },
  {
    "text": "naive implement mentation you'd start at a node you'd run a map ruce job to do the processing to to its neighbors",
    "start": "2246800",
    "end": "2252400"
  },
  {
    "text": "whatever messages and then State updates required then the next messages would propagate or or the traversals whatever",
    "start": "2252400",
    "end": "2258640"
  },
  {
    "text": "you're doing a note at a time or rather an edge at a time and if you were doing this one map Produce job at a time it",
    "start": "2258640",
    "end": "2264520"
  },
  {
    "text": "take forever so people have gained the system in amazing ways to actually get this to work in map ruce even though",
    "start": "2264520",
    "end": "2270040"
  },
  {
    "text": "it's a horrible compute model for graphs so what people are trying to do",
    "start": "2270040",
    "end": "2275240"
  },
  {
    "text": "and I say trying because this is actually cutting Edge kind of research is how do we actually Implement graph",
    "start": "2275240",
    "end": "2280599"
  },
  {
    "text": "algorithms in a distributed way so that the the graphs themselves and the traversal algorithms are first class",
    "start": "2280599",
    "end": "2287599"
  },
  {
    "text": "citizens well Google uh worked on a system called pragal prael is the name of the river that runs through I think",
    "start": "2287599",
    "end": "2293720"
  },
  {
    "text": "it was konigsburg Germany and it the invention of graph theory was done by Leonard Oiler a mathematician who",
    "start": "2293720",
    "end": "2300599"
  },
  {
    "text": "noticed there were Seven Bridges Crossing this River and could you actually cross them without going uh you",
    "start": "2300599",
    "end": "2306240"
  },
  {
    "text": "repeating a crossing and without going into a lot of detail it turns out you couldn't but it was the",
    "start": "2306240",
    "end": "2311640"
  },
  {
    "text": "first example of doing an analysis of graphs and that's why they named this thing this but essentially they they",
    "start": "2311640",
    "end": "2318319"
  },
  {
    "text": "were trying to treat uh graphs as a first class Citizen and then uh do traversal over the graph and there's",
    "start": "2318319",
    "end": "2324319"
  },
  {
    "text": "some other uh as usual as you know the history of the last 10 years has been Google does something writes a paper a",
    "start": "2324319",
    "end": "2330400"
  },
  {
    "text": "few years later and then people try to imitate it and there's several imitators giraffe and Hama and then there's a very",
    "start": "2330400",
    "end": "2336640"
  },
  {
    "text": "interesting alternative database called Titan which is probably the most sophisticated of the three for doing",
    "start": "2336640",
    "end": "2343079"
  },
  {
    "text": "distributed graphs but the problem is nobody really knows how to do this well yet so even Google is still using map",
    "start": "2343079",
    "end": "2349720"
  },
  {
    "text": "ruce for a lot of graph algorithms and a lot of the lore that's built up about how to program map ruce is focused on",
    "start": "2349720",
    "end": "2356359"
  },
  {
    "text": "how do you do inter iterative algorithms like graph reversals at the map ruce level so this is a not entirely solved",
    "start": "2356359",
    "end": "2364000"
  },
  {
    "text": "problem even though we'd love to be able to do this uh do have some mature graph databases like Neo forj which is really",
    "start": "2364000",
    "end": "2370760"
  },
  {
    "text": "a single machine database so it's not the sort of thing you'd throw at a massively uh scaled up graph but it is a",
    "start": "2370760",
    "end": "2378000"
  },
  {
    "text": "more mature graph system if you want to use it so anyway that's an unsolved problem is how do we do distributed",
    "start": "2378000",
    "end": "2383680"
  },
  {
    "text": "graphs we're sort of hacking our way through it right now okay so if we just recap where we are so Hadoop is fine for",
    "start": "2383680",
    "end": "2391160"
  },
  {
    "text": "batch mode analytics uh if if you have a you know I I need to scan my tables",
    "start": "2391160",
    "end": "2396359"
  },
  {
    "text": "quote unquote for some definition of table and do some analytics and it's great for that but it's got a lot of",
    "start": "2396359",
    "end": "2402000"
  },
  {
    "text": "performance problems the API is kind of nasty it's really the interprise Java beans of our time so it's I I see it as",
    "start": "2402000",
    "end": "2408400"
  },
  {
    "text": "somewhat transitional but it's it certainly solves some important problems for us no SQL obviously scales well it fits",
    "start": "2408400",
    "end": "2415480"
  },
  {
    "text": "a lot of the problems that we have it there are times when we need SQL but uh",
    "start": "2415480",
    "end": "2421800"
  },
  {
    "text": "a lot of times we don't and we'll get better scalability I mean we all know this story from the last decade or so",
    "start": "2421800",
    "end": "2427880"
  },
  {
    "text": "we're also seeing a lot of uh purpose-built tools like elastic search and solar that are solving a focused",
    "start": "2427880",
    "end": "2433680"
  },
  {
    "text": "problem but and doing it very effectively rather than relying on a generic solution like a dupe and then there's this emerging",
    "start": "2433680",
    "end": "2440359"
  },
  {
    "text": "vision of how to com uh combine batch mode processing with event stream processing and really the definitive",
    "start": "2440359",
    "end": "2446880"
  },
  {
    "text": "book on this right now is uh the inventor of storm is Nathan Mars and he's he's writing a very good book on",
    "start": "2446880",
    "end": "2453200"
  },
  {
    "text": "this sort of vision of of how to combine uh Real Time Event processing with batch",
    "start": "2453200",
    "end": "2458319"
  },
  {
    "text": "mod and a lot of this came out of the time you was at Twitter and and their",
    "start": "2458319",
    "end": "2463400"
  },
  {
    "text": "experiences so for example he would recommend using Hado for the batch mode stuff real-time updates with storm so",
    "start": "2463400",
    "end": "2470079"
  },
  {
    "text": "that you could you have current data at your disposal and you don't have to wait for that nightly run over the Hado",
    "start": "2470079",
    "end": "2477480"
  },
  {
    "text": "process and then whatever persistence tier you need and so forth I obviously can't summarize what he's doing in just",
    "start": "2477480",
    "end": "2483720"
  },
  {
    "text": "a few minutes I do want to emphasize though something that people are I think not as aware of as they should be and",
    "start": "2483720",
    "end": "2489880"
  },
  {
    "text": "that is sequel is roaring back even though we went through the you know the sort of the romance with no Sequel and",
    "start": "2489880",
    "end": "2495200"
  },
  {
    "text": "it's obviously got its place and we like Hado because it's general purpose people",
    "start": "2495200",
    "end": "2500839"
  },
  {
    "text": "really need SQL it's what they know it's what they want to use so if you sort of ignore SQL with whatever solution You're",
    "start": "2500839",
    "end": "2507400"
  },
  {
    "text": "Building you're probably going to fail long term if unless you address this problem and and my personal view is that",
    "start": "2507400",
    "end": "2513800"
  },
  {
    "text": "Hadoop would not be nearly as popular uh something that everybody wants to do if it was not for the fact that we have a",
    "start": "2513800",
    "end": "2520359"
  },
  {
    "text": "crude but effective SQL tool called Hive and even a lot of the so-called",
    "start": "2520359",
    "end": "2526400"
  },
  {
    "text": "nosql databases have query engines Cassandra's is a SQL dialect um is",
    "start": "2526400",
    "end": "2532079"
  },
  {
    "text": "is entirely different but it nonetheless gives you similar kind of of uh relational semantics over documents in",
    "start": "2532079",
    "end": "2538079"
  },
  {
    "text": "this case and now there's new uh generation databases that are SQL again",
    "start": "2538079",
    "end": "2543960"
  },
  {
    "text": "and they're applying the lessons of no SQL and they've been called new SQL everyone everything has to have some",
    "start": "2543960",
    "end": "2549119"
  },
  {
    "text": "sort of buzzword right but they're trying to bring the performance of no SQL back to SQL uh a really famous",
    "start": "2549119",
    "end": "2556240"
  },
  {
    "text": "example and it's worth reading the papers about this is what Google has done with spanner and F1 which are basically globally synchronized and when",
    "start": "2556240",
    "end": "2562839"
  },
  {
    "text": "I say globally I'm not kidding they actually use atomic clocks and GPS units and all kinds of crazy stuff to",
    "start": "2562839",
    "end": "2568599"
  },
  {
    "text": "synchronize the time clocks on these databases new ODB Vault DB or a couple",
    "start": "2568599",
    "end": "2573839"
  },
  {
    "text": "of others and the last very last thing I want to talk talk about is this a sort of way in the distance Trend that I",
    "start": "2573839",
    "end": "2580160"
  },
  {
    "text": "think is very interesting and could actually be transformative over time and",
    "start": "2580160",
    "end": "2585200"
  },
  {
    "text": "that is that a lot of the stuff that's underlying machine learning that's underlying this debate between Nome",
    "start": "2585200",
    "end": "2590520"
  },
  {
    "text": "Chomsky and Peter norvig is using probabilistic data structures to represent data and make predictions to",
    "start": "2590520",
    "end": "2597119"
  },
  {
    "text": "infer what it's going to do and so forth that stuff's hard to program and it takes a fair degree of expertise to even",
    "start": "2597119",
    "end": "2604119"
  },
  {
    "text": "understand what's going on so there's actually a movement starting to build languages that embed probabilistic",
    "start": "2604119",
    "end": "2610000"
  },
  {
    "text": "reasoning within them so it's a lot easier for Mortals people like the rest of us to write these kinds of programs",
    "start": "2610000",
    "end": "2616240"
  },
  {
    "text": "that use basian networks and Markov chains and other buzzwords so that's something to keep in",
    "start": "2616240",
    "end": "2622079"
  },
  {
    "text": "mind and that's it so I believe we have questions probably on the little app and",
    "start": "2622079",
    "end": "2627720"
  },
  {
    "text": "I'll take those now yeah so can Hado be used uh on data that's hard to",
    "start": "2627720",
    "end": "2633200"
  },
  {
    "text": "decompose like I.E aggregating interpolating over time series data you know the the general answer is if",
    "start": "2633200",
    "end": "2639880"
  },
  {
    "text": "you're willing to work hard enough you can pretty much do anything uh the time series problem is an interesting one I",
    "start": "2639880",
    "end": "2645880"
  },
  {
    "text": "didn't really talk about it but that's actually a good example of special purpose approaches there's some time",
    "start": "2645880",
    "end": "2651200"
  },
  {
    "text": "series databases that people are building on top of Hadoop mostly through actually hbas and",
    "start": "2651200",
    "end": "2658040"
  },
  {
    "text": "Cassandra um and so in general and I didn't apply say this either but one of the things that Hop's",
    "start": "2658040",
    "end": "2664440"
  },
  {
    "text": "really great at is you can just pretty much throw any kind of data you want into the file system and then figure out",
    "start": "2664440",
    "end": "2669720"
  },
  {
    "text": "a way to analyze it whether it's relational whether it's key value or rather comma delimited data or whatever",
    "start": "2669720",
    "end": "2675520"
  },
  {
    "text": "and so I think in the case of Time series data people are solving that problem either with a special purpose",
    "start": "2675520",
    "end": "2681400"
  },
  {
    "text": "database or uh figuring out an ad hoc way to represent the data and I'll mention one database you should take a",
    "start": "2681400",
    "end": "2687599"
  },
  {
    "text": "look at that I think is going to also be disruptive and that's datomic which is the one that Chris rather Rich hickey is",
    "start": "2687599",
    "end": "2693520"
  },
  {
    "text": "working on the guy who created closure the thing about datomic is it remembers everything that ever happened it treats",
    "start": "2693520",
    "end": "2699800"
  },
  {
    "text": "the database as a value so when you like write an update to a record you're actually creating a new virtual copy of",
    "start": "2699800",
    "end": "2706280"
  },
  {
    "text": "the database of course they don't really copy the whole thing they share data but for time series data I could see that as",
    "start": "2706280",
    "end": "2712760"
  },
  {
    "text": "being really revolutionary for solving that problem and that was the only one",
    "start": "2712760",
    "end": "2717960"
  },
  {
    "text": "that was the only question all right well I'll be around the rest of the day so feel free to come ask me questions thanks a",
    "start": "2717960",
    "end": "2724838"
  },
  {
    "text": "lot",
    "start": "2725960",
    "end": "2728960"
  }
]