[
  {
    "start": "0",
    "end": "138000"
  },
  {
    "text": "thank you very much so today I'd like to share a little bit about how honeycomb has managed to keep",
    "start": "6500",
    "end": "13259"
  },
  {
    "text": "our service up in the face of escalating demands in terms of both number of users requesting our services as well as",
    "start": "13259",
    "end": "19980"
  },
  {
    "text": "feature sprawl and all those lovely and wonderful things that come with software development",
    "start": "19980",
    "end": "25140"
  },
  {
    "text": "so the field of observability is evolving really quickly in particular what we've seen is that",
    "start": "25140",
    "end": "31080"
  },
  {
    "text": "people are starting to really understand this idea that we need to be able to debug and interrogate our software",
    "start": "31080",
    "end": "37140"
  },
  {
    "text": "systems in order to be able to tame and manage complexity in the field and not only that open Telemetry has",
    "start": "37140",
    "end": "44640"
  },
  {
    "text": "gone from a niche thing three years ago to being something that is now one of the top cncf projects we are now the",
    "start": "44640",
    "end": "52200"
  },
  {
    "text": "second most popular cncf project with the highest commit velocity Beyond kubernetes",
    "start": "52200",
    "end": "60539"
  },
  {
    "text": "so what if this boils down to is that practitioners in the field require velocity reliability and scalability",
    "start": "60539",
    "end": "68220"
  },
  {
    "text": "and that these are things that we can help them with but how did we manage to do this with",
    "start": "68220",
    "end": "73619"
  },
  {
    "text": "only about 50 engineers and how did we manage to keep up with companies that were shipping features",
    "start": "73619",
    "end": "80880"
  },
  {
    "text": "and software with 500 Engineers that were competing with us so part of how we do this is that we",
    "start": "80880",
    "end": "87060"
  },
  {
    "text": "make sure that we're always deploying all the time not you know deploying once a week or",
    "start": "87060",
    "end": "92880"
  },
  {
    "text": "even four times a week we deploy up to 14 times per day every single day of the",
    "start": "92880",
    "end": "98400"
  },
  {
    "text": "week and when I mentioned earlier about traffic increasing traffic has really",
    "start": "98400",
    "end": "104280"
  },
  {
    "text": "really gone up since the last time that I was here for Yao in 2019 we've seen a",
    "start": "104280",
    "end": "109979"
  },
  {
    "text": "growth of 10x both in terms of the number of average data points ingested per second",
    "start": "109979",
    "end": "115799"
  },
  {
    "text": "going from about 200 000 query points adjusted per second to over 2 million",
    "start": "115799",
    "end": "121259"
  },
  {
    "text": "data points ingested per second we're also seeing the number of queries from our users increased by a factor of",
    "start": "121259",
    "end": "128039"
  },
  {
    "text": "10 because it turns out that during the pandemic everything moved online and people depend upon their online",
    "start": "128039",
    "end": "134700"
  },
  {
    "text": "services to conduct their day-to-day activities so how do we manage to ship reliably and",
    "start": "134700",
    "end": "142500"
  },
  {
    "start": "138000",
    "end": "176000"
  },
  {
    "text": "manage everything in the face of Chaos well first of all we have to have a definition of what reliability is for us",
    "start": "142500",
    "end": "148980"
  },
  {
    "text": "so we have to measure what expected level of reliability our customers want from us",
    "start": "148980",
    "end": "154920"
  },
  {
    "text": "and then we have to plan we have to think about what might jeopardize our reliability to our customers",
    "start": "154920",
    "end": "160739"
  },
  {
    "text": "and how can we actually verify that our system has a safety properties that we're looking for",
    "start": "160739",
    "end": "165840"
  },
  {
    "text": "and then finally we actually go ahead and make sure that we are paying down our technical debt backlog rather than",
    "start": "165840",
    "end": "172379"
  },
  {
    "text": "have it grow and grow and grow until Things fall over so let's talk first about what our",
    "start": "172379",
    "end": "178860"
  },
  {
    "start": "176000",
    "end": "388000"
  },
  {
    "text": "definition of reliability is for an observability provider like honeycomb so this goes back to the idea of service",
    "start": "178860",
    "end": "185400"
  },
  {
    "text": "level objectives and measuring what two broken is from the perspective of our customers",
    "start": "185400",
    "end": "191159"
  },
  {
    "text": "so fortunately I don't have to explain this in a lot of detail because there's loads of reading material but in short a",
    "start": "191159",
    "end": "198060"
  },
  {
    "text": "service level objective is this core concept of measuring critical user Journeys and determining what percentage",
    "start": "198060",
    "end": "204959"
  },
  {
    "text": "of them need to be completed successfully in order for user expectations to be met overall",
    "start": "204959",
    "end": "211379"
  },
  {
    "text": "and the core idea is essentially that you cannot achieve 100 uptime so we shouldn't try to achieve 100 we should",
    "start": "211379",
    "end": "218159"
  },
  {
    "text": "try to achieve something that's more realistic that meets our customer needs and slos are really this opportunity for",
    "start": "218159",
    "end": "224220"
  },
  {
    "text": "us to negotiate with our customers with our product management team to really get down this idea of okay reliability",
    "start": "224220",
    "end": "231480"
  },
  {
    "text": "is a product feature how do we actually ship to that rather than to a goal that's too high or too low and turns out",
    "start": "231480",
    "end": "238260"
  },
  {
    "text": "the last time that I was here in Brisbane that's exactly what I talked about so when we have slos slos enable us to",
    "start": "238260",
    "end": "246060"
  },
  {
    "text": "really focus only on things that are customer impacting and not have the Sea of noisy alerts it's really fantastic",
    "start": "246060",
    "end": "252299"
  },
  {
    "text": "so what does it look like for us as honeycomb well there's a lot of moving pieces in this but essentially honeycomb",
    "start": "252299",
    "end": "260040"
  },
  {
    "text": "users send us something like 2 million data points per second at Peak and our job is to reliably serialize them onto",
    "start": "260040",
    "end": "267780"
  },
  {
    "text": "disk so that we can index them and retrieve them on demand within seconds so that our customers can understand",
    "start": "267780",
    "end": "274199"
  },
  {
    "text": "what's happening in their software systems when they need it the most when your site is down we need to be there",
    "start": "274199",
    "end": "281100"
  },
  {
    "text": "for you you need to feel that you can confidently rely upon us to help you debug your problems",
    "start": "281100",
    "end": "287340"
  },
  {
    "text": "so under the hood we're using a lot of cloud computing Technologies like serverless like Cloud object storage",
    "start": "287340",
    "end": "294180"
  },
  {
    "text": "these are all things that work in service of that but we are fundamentally the owners of",
    "start": "294180",
    "end": "300300"
  },
  {
    "text": "that ingest Pipeline and we have to make sure that it works correctly so in terms of user flows what this",
    "start": "300300",
    "end": "307259"
  },
  {
    "text": "boils down is three things first of all that if you want a snapshot of what's",
    "start": "307259",
    "end": "312300"
  },
  {
    "text": "happening in your production system we need to give that to you like at a split less than the amount of time that would",
    "start": "312300",
    "end": "318600"
  },
  {
    "text": "take you to Blink if you run a query of arbitrary complexity it should take no more than",
    "start": "318600",
    "end": "324840"
  },
  {
    "text": "the amount of time that it takes to take a sip of your coffee if you have time to get up and go make a",
    "start": "324840",
    "end": "330180"
  },
  {
    "text": "new pot of coffee before the query returns that's way too slow but most importantly we only get one",
    "start": "330180",
    "end": "336240"
  },
  {
    "text": "shot to ingest your data because after that point if we fail to ingest your data you're going to have to drop that",
    "start": "336240",
    "end": "342780"
  },
  {
    "text": "data in order to make room for new data you can't afford to keep on buffering that data in Ram until it out of",
    "start": "342780",
    "end": "348960"
  },
  {
    "text": "memories your processes so that's how we think about our service level objectives",
    "start": "348960",
    "end": "354120"
  },
  {
    "text": "we think about the idea that for core ingest we have to achieve four nines of",
    "start": "354120",
    "end": "359280"
  },
  {
    "text": "reliability or less than 4.5 minutes of full downtime if we had a full downtime",
    "start": "359280",
    "end": "364620"
  },
  {
    "text": "incident per month we're a little bit more lenient about the query workflows because you can always refresh the page and try again if",
    "start": "364620",
    "end": "371699"
  },
  {
    "text": "we only get that one shot on your telemetry so this is what our expectation of",
    "start": "371699",
    "end": "377820"
  },
  {
    "text": "reliability looks like and how we measure it day-to-day we have of course another copy of honeycomb called dog",
    "start": "377820",
    "end": "383940"
  },
  {
    "text": "food which measures production honeycomb and that's how we generate these statistics",
    "start": "383940",
    "end": "389419"
  },
  {
    "start": "388000",
    "end": "563000"
  },
  {
    "text": "but it's all in well to measure a Target but how do we actually think about achieving it how do we actually design",
    "start": "389520",
    "end": "395940"
  },
  {
    "text": "to meet that four Nine's reliability guarantee and what happens if we don't achieve it",
    "start": "395940",
    "end": "403800"
  },
  {
    "text": "so the devops research organization accelerate state of devops report says",
    "start": "403800",
    "end": "409020"
  },
  {
    "text": "that Elite teams deploy on-demand multiple times per day and it's not necessarily that those",
    "start": "409020",
    "end": "415259"
  },
  {
    "text": "teams are failing less often in their deploys is that they're recovering very",
    "start": "415259",
    "end": "420360"
  },
  {
    "text": "very quickly from outages less than an hour to deploy or to recover from a service degradation",
    "start": "420360",
    "end": "426780"
  },
  {
    "text": "so that's really where we've focused our energy is how do we make it so that we cannot permanently break our system only",
    "start": "426780",
    "end": "433620"
  },
  {
    "text": "temporarily for a small fraction of users so in practice this means that we have",
    "start": "433620",
    "end": "439080"
  },
  {
    "text": "to think about reliability throughout our entire software development life cycle we have to iterate and incorporate",
    "start": "439080",
    "end": "445500"
  },
  {
    "text": "future flag development so that in case something goes wrong it's a matter of Five Seconds to flip off the feature",
    "start": "445500",
    "end": "451740"
  },
  {
    "text": "flag and we might have only impacted one or two percent of users rather than every user",
    "start": "451740",
    "end": "457020"
  },
  {
    "text": "we also invest in automatic integration and make sure that it does not get in the way of our software developers",
    "start": "457020",
    "end": "463319"
  },
  {
    "text": "writing our own code in particular here's a graph of the latency involved in the build pipeline",
    "start": "463319",
    "end": "470160"
  },
  {
    "text": "for our product and you can see that in in July and August of this year it got a little bit",
    "start": "470160",
    "end": "476699"
  },
  {
    "text": "slower than we would have liked that it was reaching as high as 14 to 15 minutes",
    "start": "476699",
    "end": "481919"
  },
  {
    "text": "that to us was not acceptable so we invested sometime in September in making sure that we're predictably achieving a",
    "start": "481919",
    "end": "488940"
  },
  {
    "text": "build cycle time of less than 10 minutes so that software is not just sitting",
    "start": "488940",
    "end": "494280"
  },
  {
    "text": "there rotting in the build queue that it's actually getting billed and shipped on a reliable schedule",
    "start": "494280",
    "end": "499919"
  },
  {
    "text": "and you can see that as of the past month we're doing considerably better we're basically bang on 10 minutes every",
    "start": "499919",
    "end": "505500"
  },
  {
    "text": "single time and that really makes a difference to our developer productivity while ensuring that we're still",
    "start": "505500",
    "end": "511620"
  },
  {
    "text": "delivering high quality software and speaking of delivering high quality software we have confidence in the",
    "start": "511620",
    "end": "518279"
  },
  {
    "text": "combination of human code review and release testing and the human code",
    "start": "518279",
    "end": "523320"
  },
  {
    "text": "review is focused on that element of how we're going to observe when it's working in production is this instrumented",
    "start": "523320",
    "end": "529260"
  },
  {
    "text": "correctly and the automation takes care of all of the regression testing for us so we can be confident that anything",
    "start": "529260",
    "end": "535860"
  },
  {
    "text": "that passes the human tests and the automated tests is safe to deploy automatically to production",
    "start": "535860",
    "end": "541800"
  },
  {
    "text": "and it automatically goes out within an hour or two so sure we might not necessarily Deploy",
    "start": "541800",
    "end": "547740"
  },
  {
    "text": "on Friday at 6 pm but we definitely will Deploy on Friday at 3 pm",
    "start": "547740",
    "end": "553860"
  },
  {
    "text": "because we have that predictability of when things go out and we make sure that developers are sticking around to",
    "start": "553860",
    "end": "560040"
  },
  {
    "text": "observe the behavior and production of what they're shipping so that's how things work with the",
    "start": "560040",
    "end": "567060"
  },
  {
    "start": "563000",
    "end": "824000"
  },
  {
    "text": "normal software development cycle but as we all know things don't always go exactly according to plan",
    "start": "567060",
    "end": "573899"
  },
  {
    "text": "so how do we make sure that our software is meeting the safety expectations",
    "start": "573899",
    "end": "579779"
  },
  {
    "text": "well we experiment using leftover error budget only if there's error budget left of course if your service is already on",
    "start": "579779",
    "end": "587399"
  },
  {
    "text": "fire you should be focusing on figuring out why it is on fire and putting out those fires but if your service is not",
    "start": "587399",
    "end": "594000"
  },
  {
    "text": "currently on fire and has a good track record you should always be poking at and then trying to understand what might",
    "start": "594000",
    "end": "600240"
  },
  {
    "text": "cause me to fail more than I expect in the future so when we design chaos engineering",
    "start": "600240",
    "end": "606000"
  },
  {
    "text": "experiments there's usually some kind of hypothesis that we're trying to validate what are we trying to learn from doing",
    "start": "606000",
    "end": "611820"
  },
  {
    "text": "this experiment in production and we make sure that there's always some kind of a safety button to press in",
    "start": "611820",
    "end": "618420"
  },
  {
    "text": "order to make sure that we're able to restore to a known good state so as I mentioned feature Flags can be",
    "start": "618420",
    "end": "624480"
  },
  {
    "text": "really powerful if we're trying to test something on a on a small set of users",
    "start": "624480",
    "end": "629820"
  },
  {
    "text": "but unfortunately for us not everything can be tested with feature Flags",
    "start": "629820",
    "end": "635459"
  },
  {
    "text": "because at the end of the day honeycomb is a big data problem where we need to",
    "start": "635459",
    "end": "640500"
  },
  {
    "text": "persist the data and make sure that we have durability and reliability for our customers",
    "start": "640500",
    "end": "646920"
  },
  {
    "text": "so well the top tier of things that I'm showing on the screen doodle poodle and Shepard those are all stateless services",
    "start": "646920",
    "end": "653700"
  },
  {
    "text": "that we can deploy on demand that we can feature flag everything below that on the diagram",
    "start": "653700",
    "end": "660120"
  },
  {
    "text": "those are all stateful services that store customer data on disk",
    "start": "660120",
    "end": "665579"
  },
  {
    "text": "and that we can't necessarily just switch off or can we",
    "start": "665579",
    "end": "671880"
  },
  {
    "text": "so this is where it pays to understand some of the Primitives of how persistence works at rest",
    "start": "671880",
    "end": "680399"
  },
  {
    "text": "when we ingest that into honeycomb it comes as a stream of events that flow into Shepherd which is our ingest",
    "start": "680399",
    "end": "687420"
  },
  {
    "text": "service and Shepherd decides to send that data to one or multiple different Kafka",
    "start": "687420",
    "end": "694079"
  },
  {
    "text": "partitions because we utilize Kafka to provide that durability layer between stateless ingest and mostly stateful",
    "start": "694079",
    "end": "702440"
  },
  {
    "text": "indexing but we'll talk about that so at the end of the day what Kafka is giving us is this ordered stream of",
    "start": "702440",
    "end": "709079"
  },
  {
    "text": "events and we can go ahead and make sure that we are replaying or reading from that",
    "start": "709079",
    "end": "715200"
  },
  {
    "text": "ordered stream of events and then performing the Transformations needed to write it to S3",
    "start": "715200",
    "end": "721620"
  },
  {
    "text": "so a couple of things make this really challenging and like our normal software development process that pushes changes",
    "start": "721620",
    "end": "727800"
  },
  {
    "text": "every hour we don't necessarily make a major change that would require restarting Kafka every hour",
    "start": "727800",
    "end": "735000"
  },
  {
    "text": "we have to think about the picture of how do we ensure data integrity and consistency for things that happen not",
    "start": "735000",
    "end": "741480"
  },
  {
    "text": "very often right how often do we lose a Kafka node if AWS is doing its job that's not going",
    "start": "741480",
    "end": "748860"
  },
  {
    "text": "to be very often because we're not utilizing spot instances for that we're using on-demand instances which are",
    "start": "748860",
    "end": "753959"
  },
  {
    "text": "supposed to stay up and when things do wind up degrading availability or degrading the amount of",
    "start": "753959",
    "end": "761100"
  },
  {
    "text": "redundancy in the system there are all kinds of delicate failover dances that are required by Kafka or by",
    "start": "761100",
    "end": "767639"
  },
  {
    "text": "our durable storage systems that need to happen to restore that redundancy it's not just a matter of restart the service",
    "start": "767639",
    "end": "774000"
  },
  {
    "text": "that comes back up so let's suppose for instance that we lose one particular Kafka worker node",
    "start": "774000",
    "end": "781920"
  },
  {
    "text": "what is supposed to happen after that well in theory we get a brand new Kafka",
    "start": "781920",
    "end": "788220"
  },
  {
    "text": "we get a brand new Kafka broker and the existing Kafka broker should help restore the state of the partition onto",
    "start": "788220",
    "end": "795240"
  },
  {
    "text": "that Kafka node and we should be back at replication factor three ideally",
    "start": "795240",
    "end": "800820"
  },
  {
    "text": "similarly if we lose one of our indexing workers in theory we should be able to grab a fresh snapshot of the previous",
    "start": "800820",
    "end": "808019"
  },
  {
    "text": "indexing workers hardness contents off S3 and the Costco offset it was running from and Replay that forward",
    "start": "808019",
    "end": "816120"
  },
  {
    "text": "so we'll go ahead and just replay the stream which should be in the same consistent order from the same offset",
    "start": "816120",
    "end": "821700"
  },
  {
    "text": "and get the same bytes on disk but how do we actually verify that right",
    "start": "821700",
    "end": "827880"
  },
  {
    "start": "824000",
    "end": "1159000"
  },
  {
    "text": "like all that sounds good in theory right that looks great on a white paper how do we actually make sure that it works when we need it the most",
    "start": "827880",
    "end": "834779"
  },
  {
    "text": "the answer for us is that we have to test those properties we have to restart one server and one service at a time",
    "start": "834779",
    "end": "842040"
  },
  {
    "text": "we don't restart everything right like we don't necessarily uh run Netflix's chaos monkey like on our entire fleet",
    "start": "842040",
    "end": "848339"
  },
  {
    "text": "but we do want to be thoughtful about making sure that we can tolerate the loss of a Kafka broker or tolerate the",
    "start": "848339",
    "end": "854279"
  },
  {
    "text": "loss of a retriever indexing node and we also want to control the timing of this we want it to happen at 3 pm not",
    "start": "854279",
    "end": "861240"
  },
  {
    "text": "at 3 AM this is for a variety of reasons first of all it ensures that most of our",
    "start": "861240",
    "end": "866519"
  },
  {
    "text": "team is on deck and when I say 3 pm I I mean 3 P.M in the US mostly because",
    "start": "866519",
    "end": "871620"
  },
  {
    "text": "that's where most of our team is based um I'm yours as a safety net in case things do go wrong out of hours but in",
    "start": "871620",
    "end": "878160"
  },
  {
    "text": "general we prefer for most of the team to be awake but the other important thing is to do this during Peak traffic load right the worst time to lose",
    "start": "878160",
    "end": "885300"
  },
  {
    "text": "redundancy is when you're receiving the most traffic because then you have the least bandwidth available to repopulate",
    "start": "885300",
    "end": "892019"
  },
  {
    "text": "existing nodes and we have to check we have to make sure that if we replace a given Kafka",
    "start": "892019",
    "end": "898620"
  },
  {
    "text": "node and we're doing choosing to do it rather than Amazon making the decision as to when it happens we want to make",
    "start": "898620",
    "end": "903899"
  },
  {
    "text": "sure that there are no blips induced to our ingest pipeline that for instance the shepherd nodes that are the",
    "start": "903899",
    "end": "910560"
  },
  {
    "text": "stateless indust workers that they're noticing that the Costco partition they're trying to write to has gone",
    "start": "910560",
    "end": "916199"
  },
  {
    "text": "temporarily and available and they need to make sure to redirect that traffic to a Kafka partition that is working",
    "start": "916199",
    "end": "921720"
  },
  {
    "text": "correctly at least until we can get that partition up and running again and in the event something does go wrong",
    "start": "921720",
    "end": "928260"
  },
  {
    "text": "we have to have the observability to understand what's gone wrong and how do",
    "start": "928260",
    "end": "933300"
  },
  {
    "text": "we make sure that that doesn't happen again the next time we do this experiment or heaven forbid the next time that it happens in production",
    "start": "933300",
    "end": "940440"
  },
  {
    "text": "this means that we need to make sure that our Telemetry is working correctly and when we're running an experiment if",
    "start": "940440",
    "end": "946620"
  },
  {
    "text": "we don't see the blip that we expected that's the sign that we're missing Telemetry right we can't just say you",
    "start": "946620",
    "end": "952500"
  },
  {
    "text": "know oh like nothing went wrong therefore you know we're not going to do anything right like if we do an",
    "start": "952500",
    "end": "957839"
  },
  {
    "text": "experiment and we don't see the little crater that we expect to see that's a problem too",
    "start": "957839",
    "end": "963440"
  },
  {
    "text": "and over time we've gotten to the point where we're able to verify these fixes by repeating them over and over so we",
    "start": "963779",
    "end": "969660"
  },
  {
    "text": "don't just kill Kafka Brokers you know once in every blue moon we actually kill one Kafka broker every",
    "start": "969660",
    "end": "976800"
  },
  {
    "text": "Wednesday morning in Australia we kill one uh one indexing worker every Tuesday",
    "start": "976800",
    "end": "982260"
  },
  {
    "text": "Morning in Australia so that enables us to verify situations like what happens if we suddenly lose an",
    "start": "982260",
    "end": "988139"
  },
  {
    "text": "indexing worker did we accidentally break the brute straw process or more recently we actually found an issue",
    "start": "988139",
    "end": "994740"
  },
  {
    "text": "where we did not have enough spare capacity in our system to tolerate the loss of a Kafka broker that we caught it",
    "start": "994740",
    "end": "1002480"
  },
  {
    "text": "before it became something where the Kafka worker was struggling to keep up at all when it was coming back up we",
    "start": "1002480",
    "end": "1008779"
  },
  {
    "text": "caught it just at the Tipping Point where the restore process was taking something like 12 to 16 hours not the",
    "start": "1008779",
    "end": "1015199"
  },
  {
    "text": "six hours that we expected and that was a sign that we needed to know to upscale",
    "start": "1015199",
    "end": "1020660"
  },
  {
    "text": "our Kafka cluster before it became a serious problem so by verifying these things this helps",
    "start": "1020660",
    "end": "1026959"
  },
  {
    "text": "us identify all kinds of interesting edge cases for instance we had a situation before we adopted",
    "start": "1026959",
    "end": "1032900"
  },
  {
    "text": "kubernetes cron jobs where we needed to run alerts that honeycomb users were",
    "start": "1032900",
    "end": "1038240"
  },
  {
    "text": "expecting to run once a minute and it turns out that we had multiple nodes that could potentially do that and",
    "start": "1038240",
    "end": "1044120"
  },
  {
    "text": "ideally one worker would elect itself leader and uh and that would be the only one to run that minute",
    "start": "1044120",
    "end": "1050720"
  },
  {
    "text": "but we tried restarting notes from our zookeeper cluster and it we discovered that If you restarted The zeroth",
    "start": "1050720",
    "end": "1056480"
  },
  {
    "text": "Zookeeper node neither a learning worker would succeed at getting the lock and no alerts would run oops",
    "start": "1056480",
    "end": "1063260"
  },
  {
    "text": "and turns out that all of the redundancy in the world in your zookeeper cluster does not matter",
    "start": "1063260",
    "end": "1068840"
  },
  {
    "text": "if your clients that are talking to zookeeper are not reaching the Quorum of all zookeepers and are just talking to",
    "start": "1068840",
    "end": "1074600"
  },
  {
    "text": "the zeroth worker so after we fix that bug we were able to safely restart our zookeeper cluster and",
    "start": "1074600",
    "end": "1081140"
  },
  {
    "text": "continue to have alerts work so by using both design and automation we've been able to de-risk",
    "start": "1081140",
    "end": "1088940"
  },
  {
    "text": "our systems and ensure that they're able to work correctly even if something chaotic happens in production whether it",
    "start": "1088940",
    "end": "1094820"
  },
  {
    "text": "be us inducing that chaos or AWS doing it for us so for instance we now believe that we",
    "start": "1094820",
    "end": "1102140"
  },
  {
    "text": "can safely lose indexing workers even lose an entire availability Zone at a time and actually have things restore",
    "start": "1102140",
    "end": "1108799"
  },
  {
    "text": "correctly and that continuous verification process is really important",
    "start": "1108799",
    "end": "1113900"
  },
  {
    "text": "because if you don't test these things yourself you're going to have those things break",
    "start": "1113900",
    "end": "1120320"
  },
  {
    "text": "anyways in production if you're using AWS spot nodes if you're using kubernetes right like kubernetes",
    "start": "1120320",
    "end": "1126380"
  },
  {
    "text": "reserves the right to reschedule your workload there's not necessarily a guarantee that things are going to keep running on the same node forever",
    "start": "1126380",
    "end": "1132919"
  },
  {
    "text": "so by ensuring that things survive being moved around you're giving yourself a",
    "start": "1132919",
    "end": "1138320"
  },
  {
    "text": "lot more flexibility because you know your system is designed to tolerate that level of Chaos",
    "start": "1138320",
    "end": "1144200"
  },
  {
    "text": "so overall chaos testing creates stronger platforms rather than eroding the safety of your platforms it's",
    "start": "1144200",
    "end": "1150860"
  },
  {
    "text": "helping you discover what your safety margins of your systems are so that you then have confidence to build on top of",
    "start": "1150860",
    "end": "1157520"
  },
  {
    "text": "them but not every experiment succeeds right like I talk a big game about what",
    "start": "1157520",
    "end": "1162919"
  },
  {
    "start": "1159000",
    "end": "1606000"
  },
  {
    "text": "happens if you restart a Costco broker right and our answer was it works no problem right",
    "start": "1162919",
    "end": "1168140"
  },
  {
    "text": "but I would be remiss to make you think that everything is sunshine roses so let's talk about three things that went",
    "start": "1168140",
    "end": "1173299"
  },
  {
    "text": "wrong so once upon a time uh we had a service",
    "start": "1173299",
    "end": "1179299"
  },
  {
    "text": "we have the ninja service only supporting ingesting Json over http but",
    "start": "1179299",
    "end": "1184700"
  },
  {
    "text": "we wanted to add support for adding for open Telemetry over grpc",
    "start": "1184700",
    "end": "1190460"
  },
  {
    "text": "so our developers with the best intentions said okay let's have a separate port to listen for grpc",
    "start": "1190460",
    "end": "1196220"
  },
  {
    "text": "requests and let's go ahead and test that locally hey it works okay great let's go ahead and roll out production",
    "start": "1196220",
    "end": "1204140"
  },
  {
    "text": "and then everything crashed the new build started rolling out but it was failing to health check it was failing",
    "start": "1204140",
    "end": "1211100"
  },
  {
    "text": "to start up correctly so we're eroding the capacity in our existing Auto scaling groups as the deploy went",
    "start": "1211100",
    "end": "1216620"
  },
  {
    "text": "onwards we're able to stop the deploy in time so that we weren't having a hundred percent",
    "start": "1216620",
    "end": "1222320"
  },
  {
    "text": "outrage what happened that we weren't aware of was that our safety mechanism",
    "start": "1222320",
    "end": "1227660"
  },
  {
    "text": "for stopping releases did not stop new worker nodes that were spinning up in response to increased demand from",
    "start": "1227660",
    "end": "1234320"
  },
  {
    "text": "getting the newest build rather than the current golden build that they're supposed to be running",
    "start": "1234320",
    "end": "1240559"
  },
  {
    "text": "so when traffic increased even though we'd Frozen the deploy our system became",
    "start": "1240559",
    "end": "1245780"
  },
  {
    "text": "unstable and latency shot up and we blew our SLO now the Google Sree book says you're",
    "start": "1245780",
    "end": "1252140"
  },
  {
    "text": "supposed to stop all deploys and win for your soul to recover but we're a startup we don't have that luxury so we could",
    "start": "1252140",
    "end": "1258080"
  },
  {
    "text": "have said you know okay so let's not do that we thought about delaying the launch but it turned out that this was",
    "start": "1258080",
    "end": "1263900"
  },
  {
    "text": "something that was really important it was a partner announcement with AWS so we instead decided to get creative right",
    "start": "1263900",
    "end": "1269720"
  },
  {
    "text": "like the point of the SLO is to prevent you from violating the safety constraints",
    "start": "1269720",
    "end": "1274940"
  },
  {
    "text": "multiple times to damage this rest of your users so what we chose to do was we chose to",
    "start": "1274940",
    "end": "1280220"
  },
  {
    "text": "mitigate any future impact of this feature work by using a separate worker",
    "start": "1280220",
    "end": "1285559"
  },
  {
    "text": "pool ensuring that the load balancer upstream was making the routing decision about whether he used the experimental",
    "start": "1285559",
    "end": "1292280"
  },
  {
    "text": "pool or the stable pool so the experimental pool is running grpc but the stable pool is running everything",
    "start": "1292280",
    "end": "1298280"
  },
  {
    "text": "else and that way we could protect our users traffic while still enabling the grpc experiment to succeed",
    "start": "1298280",
    "end": "1306340"
  },
  {
    "text": "here's the second failure use case Costco is our main a persistence layer as I talked about and we wanted to make",
    "start": "1306380",
    "end": "1313940"
  },
  {
    "text": "sure that we're running it as efficiently as possible while preserving stability and safety",
    "start": "1313940",
    "end": "1320539"
  },
  {
    "text": "so for a long time we've been using confluent Kafka and confluent Kafka released this feature called uh",
    "start": "1320539",
    "end": "1327740"
  },
  {
    "text": "called Data tiering where you could tier older data that was less recently accessed",
    "start": "1327740",
    "end": "1333080"
  },
  {
    "text": "and our goal here was to stop running you know 30 or 40 different uh Costco",
    "start": "1333080",
    "end": "1338299"
  },
  {
    "text": "brokers and reduce the number of confident brokers who are running to a more manageable six or nine because it",
    "start": "1338299",
    "end": "1345440"
  },
  {
    "text": "turned out that we didn't need 100 of that data to be fresh on disk",
    "start": "1345440",
    "end": "1351320"
  },
  {
    "text": "we merely needed a safety margin of something like uh of something like one",
    "start": "1351320",
    "end": "1356419"
  },
  {
    "text": "to two hours to be able to do our replay safely rather than needing the full 24 hours if we need the past 24 hours of",
    "start": "1356419",
    "end": "1364100"
  },
  {
    "text": "data that's a sign that something's gone further wrong that can take a little bit more time to restore it doesn't have to",
    "start": "1364100",
    "end": "1369799"
  },
  {
    "text": "be sitting on local SSD so as you can see for a while we are",
    "start": "1369799",
    "end": "1375140"
  },
  {
    "text": "running a very high number of workers you're running as as many as I think 30 or 40 i3en x large nodes",
    "start": "1375140",
    "end": "1383419"
  },
  {
    "text": "and we tried to say okay we're going to tear all of the older data off and therefore we only need the amount of SSD",
    "start": "1383419",
    "end": "1390500"
  },
  {
    "text": "corresponding to the number of disks the amount of disks they set the most recent hour will take right",
    "start": "1390500",
    "end": "1396799"
  },
  {
    "text": "but it turns out that there are five different dimensions in a Kafka cluster you need to care about CPU disk size spindles RAM and Network",
    "start": "1396799",
    "end": "1405980"
  },
  {
    "text": "and we kept on bumping into two Dimensions we did not think we're going to go wrong we thought we were right",
    "start": "1405980",
    "end": "1412220"
  },
  {
    "text": "sizing disk size and CPU but that created bottlenecks on disk speed and Network",
    "start": "1412220",
    "end": "1418340"
  },
  {
    "text": "and ultimately we decided that it was not worth trying to min max the whole thing the safety of the system and the",
    "start": "1418340",
    "end": "1425299"
  },
  {
    "text": "safety of our people to have enough time to react in case something went wrong that was more important to us than a few",
    "start": "1425299",
    "end": "1432320"
  },
  {
    "text": "dollars and it took until December of 2021 about a year ago until we were able to",
    "start": "1432320",
    "end": "1438380"
  },
  {
    "text": "actually move into our preferred configuration once Amazon released the right instance shapes to help meet the",
    "start": "1438380",
    "end": "1445880"
  },
  {
    "text": "use case that we had so we briefly went to having the",
    "start": "1445880",
    "end": "1451940"
  },
  {
    "text": "previous instance generation type that we were using before just fewer of them and eventually we're able to run the",
    "start": "1451940",
    "end": "1459080"
  },
  {
    "text": "correct uh the correct uh architecture the correct uh size of machine that was",
    "start": "1459080",
    "end": "1465200"
  },
  {
    "text": "going to meet our needs and provided adequate safety margin so safety of your people should always",
    "start": "1465200",
    "end": "1471679"
  },
  {
    "text": "always take a precedence when you're dealing with any kind of chaos engineering any kind of experiments or",
    "start": "1471679",
    "end": "1477080"
  },
  {
    "text": "emergencies finally I want to talk about the use case of being able to make",
    "start": "1477080",
    "end": "1484100"
  },
  {
    "text": "changes at your infrastructure level using feature Flags we use AWS Lambda in order to power that",
    "start": "1484100",
    "end": "1491480"
  },
  {
    "text": "ability to return data in less than 10 seconds no matter how how many million files on disk you're querying",
    "start": "1491480",
    "end": "1497419"
  },
  {
    "text": "and it turns out to be our most expensive uh our most expensive back-end",
    "start": "1497419",
    "end": "1503000"
  },
  {
    "text": "back-end service spend but AWS had released graviton ec2",
    "start": "1503000",
    "end": "1509419"
  },
  {
    "text": "instances powered by arm processors which worked great for us we're really thrilled with them but we wanted to",
    "start": "1509419",
    "end": "1514820"
  },
  {
    "text": "migrate our Lambda usage over to graviton as well in order to Avail ourselves with better",
    "start": "1514820",
    "end": "1520460"
  },
  {
    "text": "performance and lower cost so we tried turning it on with feature Flags we said let's just try an A B test",
    "start": "1520460",
    "end": "1526760"
  },
  {
    "text": "let's just try running 50 arm 50 x 86 and see what happens right",
    "start": "1526760",
    "end": "1532520"
  },
  {
    "text": "and it fell over it fell over and actually made cost worse performance",
    "start": "1532520",
    "end": "1537620"
  },
  {
    "text": "worse it made users experiencing about 1.5 x latency and it was not",
    "start": "1537620",
    "end": "1542720"
  },
  {
    "text": "satisfactory so we had to turn it off right quickly",
    "start": "1542720",
    "end": "1547820"
  },
  {
    "text": "but fortunately because we designed things with feature Flags you'll notice the time stamp there 6 49 pm",
    "start": "1547820",
    "end": "1554480"
  },
  {
    "text": "at 6 49 PM we were able to flip it back to one percent without any problems",
    "start": "1554480",
    "end": "1560059"
  },
  {
    "text": "so having that safety made it possible for us to do this experiment knowing that even if it failed the impact to",
    "start": "1560059",
    "end": "1565820"
  },
  {
    "text": "users would be minimal and we're able ultimately to make Ford progress to optimize the things that",
    "start": "1565820",
    "end": "1572480"
  },
  {
    "text": "were potentially at risk of breakage and to solve the performance bottlenecks that were causing us to see that higher",
    "start": "1572480",
    "end": "1577700"
  },
  {
    "text": "than expected latency and today you know after a few false starts right you know we had to run at",
    "start": "1577700",
    "end": "1583760"
  },
  {
    "text": "30 and then we went up to 80 90 we were like uh okay that's not quite doing what we need we need to scale it back down we",
    "start": "1583760",
    "end": "1591020"
  },
  {
    "text": "finally got to running more than 99 arm Lambda we just have a one percent carve out to make sure to our point about",
    "start": "1591020",
    "end": "1597679"
  },
  {
    "text": "maintaining safety margins that if we have to run an x86 we can safely do so",
    "start": "1597679",
    "end": "1602840"
  },
  {
    "text": "still so that's the reason why that does not say 100 percent so in summary you don't have to pick",
    "start": "1602840",
    "end": "1610039"
  },
  {
    "start": "1606000",
    "end": "1745000"
  },
  {
    "text": "between fast delivery and reliable delivery it's a little bit like riding a bicycle if you're moving fast enough",
    "start": "1610039",
    "end": "1616460"
  },
  {
    "text": "you'll be both fast and and not wobbly so think about your service level",
    "start": "1616460",
    "end": "1622580"
  },
  {
    "text": "objectives through the full life cycle manage things using future Flags to make sure that you're minimizing the impact",
    "start": "1622580",
    "end": "1629539"
  },
  {
    "text": "to users in case things do go wrong mitigate risk and discover what could go",
    "start": "1629539",
    "end": "1634640"
  },
  {
    "text": "wrong by actually inducing those failures and verifying that it meets your assumptions",
    "start": "1634640",
    "end": "1639799"
  },
  {
    "text": "but if things do go catastrophically wrong don't necessarily stop the presses and stop everything",
    "start": "1639799",
    "end": "1645260"
  },
  {
    "text": "think about what can you do to move safely forward the next time",
    "start": "1645260",
    "end": "1650299"
  },
  {
    "text": "and you have to include in your error budget room for both known risks as well as unknown risks",
    "start": "1650299",
    "end": "1656779"
  },
  {
    "text": "for instance you might have things like your SSL certificate suddenly expiring or log4j happens right like things like",
    "start": "1656779",
    "end": "1663500"
  },
  {
    "text": "that will happen so discover early and often through testing can you actually",
    "start": "1663500",
    "end": "1668720"
  },
  {
    "text": "do a fleet-wide upgrade of all of your instances or all of your applications can you rebuild those because you're",
    "start": "1668720",
    "end": "1675380"
  },
  {
    "text": "going to have to at some point in the future whether you like it or not we all have various stakeholders you're",
    "start": "1675380",
    "end": "1682340"
  },
  {
    "text": "always going to have things that push the limits of what one stakeholder or another is willing tolerate so that's an",
    "start": "1682340",
    "end": "1688700"
  },
  {
    "text": "opportunity for us to have a conversation about what did we learn how can we do things better rather than",
    "start": "1688700",
    "end": "1693860"
  },
  {
    "text": "resorting to blame so having this flexibility and experimentation and freedom to replace",
    "start": "1693860",
    "end": "1700880"
  },
  {
    "text": "any instance at any time that flexibility to try out things with feature flags that enables us to run a",
    "start": "1700880",
    "end": "1707600"
  },
  {
    "text": "more agile infrastructure as well as to ship code on demand a dozen times per",
    "start": "1707600",
    "end": "1713000"
  },
  {
    "text": "day in particular one thing I wanted to highlight is that in the past month we've succeeded at replacing a hundred",
    "start": "1713000",
    "end": "1719659"
  },
  {
    "text": "percent of our kubernetes nodes with graviton 3 kubernetes nodes and that would not have been possible without our",
    "start": "1719659",
    "end": "1726740"
  },
  {
    "text": "chaos experimentation to verify that it is safe for us to replace nodes and kind",
    "start": "1726740",
    "end": "1732020"
  },
  {
    "text": "of do these rolling restarts on demand and also to test and understand what are",
    "start": "1732020",
    "end": "1737179"
  },
  {
    "text": "the limits how hot can we run our workers to minimize the number of kubernetes of kubernetes nodes that we",
    "start": "1737179",
    "end": "1743480"
  },
  {
    "text": "need to run if you're interested in learning more about how we think about Building",
    "start": "1743480",
    "end": "1748760"
  },
  {
    "start": "1745000",
    "end": "1776000"
  },
  {
    "text": "Systems for observability with good observability you can check out a copy",
    "start": "1748760",
    "end": "1754159"
  },
  {
    "text": "of her book for free here which was recently published in May by O'Reilly",
    "start": "1754159",
    "end": "1759980"
  },
  {
    "text": "and finally I encourage you to go ahead try this out in your infrastructure just",
    "start": "1759980",
    "end": "1765080"
  },
  {
    "text": "make sure that you have the tools to manage that risk and iterate",
    "start": "1765080",
    "end": "1770120"
  },
  {
    "text": "you can see more resources at the QR codes on screen and thank you very much for your attention I look forward to",
    "start": "1770120",
    "end": "1775460"
  },
  {
    "text": "taking any questions that you have [Applause]",
    "start": "1775460",
    "end": "1784920"
  },
  {
    "start": "1776000",
    "end": "2423000"
  },
  {
    "text": "all right any questions",
    "start": "1785779",
    "end": "1790059"
  },
  {
    "text": "hands anyone yes and keep your hands up afterwards so I can run to the next person please",
    "start": "1791240",
    "end": "1798380"
  },
  {
    "text": "um yeah hi thank you for the talk uh I'm just trying to understand where you're talking about error budgets and",
    "start": "1798380",
    "end": "1805100"
  },
  {
    "text": "experimenting how does that work in practice at what point in your 30-day cycle do you decide we have enough",
    "start": "1805100",
    "end": "1810140"
  },
  {
    "text": "budget to experiment yeah so the way that we think about when",
    "start": "1810140",
    "end": "1815539"
  },
  {
    "text": "we have enough error budget to experiment versus when we don't relates to that question of if you have an SLO that's say you know",
    "start": "1815539",
    "end": "1822980"
  },
  {
    "text": "your SLO says you should be 99.9 reliable if you're at least 99.95",
    "start": "1822980",
    "end": "1828620"
  },
  {
    "text": "reliable EJ you have about 50 of your error budget left or more that's a sign that you're under utilizing that budget",
    "start": "1828620",
    "end": "1835220"
  },
  {
    "text": "and you can go a little bit faster and run more experiments if you're getting down to 10 or 20 of your air budget left",
    "start": "1835220",
    "end": "1840559"
  },
  {
    "text": "at the end of the month that's a sign of okay like you know any one minor incident could blow us out of SLO that's",
    "start": "1840559",
    "end": "1846380"
  },
  {
    "text": "that's not so good right so it kind of depends upon what granularity and tools you have to see what's happening with",
    "start": "1846380",
    "end": "1853279"
  },
  {
    "text": "your SLO what's the rate of burn down how many incidents have we had how many more similar incidents can we tolerate",
    "start": "1853279",
    "end": "1859279"
  },
  {
    "text": "before we might blow that SLO I think the most obvious case of don't run an experiment is uh if your escrow",
    "start": "1859279",
    "end": "1865580"
  },
  {
    "text": "is blown by a factor of 100 or 200 or more right like you have your work cut out for you you know what you need to be doing",
    "start": "1865580",
    "end": "1871520"
  },
  {
    "text": "or if not you should be figuring that out um but yeah basically anything between 50 and",
    "start": "1871520",
    "end": "1877100"
  },
  {
    "text": "90 of Arrow budget leftover is like okay we've we've got plenty to experiment with and if you have over 90 of your",
    "start": "1877100",
    "end": "1884240"
  },
  {
    "text": "error budget left at the end of the month that's honestly a sign that your SLO is misad right that's a sign that you are over dramatically over",
    "start": "1884240",
    "end": "1890659"
  },
  {
    "text": "delivering unreliability it's possible your target is not aggressive enough it's part possible you're measuring the",
    "start": "1890659",
    "end": "1896059"
  },
  {
    "text": "wrong thing or it's just possible that um you might need to so this is a story",
    "start": "1896059",
    "end": "1901700"
  },
  {
    "text": "from my uh from my time at Google there's a service called uh Global chubby that was basically This Global",
    "start": "1901700",
    "end": "1907760"
  },
  {
    "text": "Lock Service um where people would store config in global Trevi and expect you know hey I poke this file and it updates magically",
    "start": "1907760",
    "end": "1914720"
  },
  {
    "text": "for me and people grew to over rely on it so that team would actually deliberately",
    "start": "1914720",
    "end": "1920960"
  },
  {
    "text": "induce a 10 minute outage of that every quarter uh if there had not been a 10 minute outage that quarter from other",
    "start": "1920960",
    "end": "1926120"
  },
  {
    "text": "causes just to make sure that people did not over depend upon it so if you have way too much error budget",
    "start": "1926120",
    "end": "1932659"
  },
  {
    "text": "deliberately break it if at least if it's an internal facing service that you don't have people who come to depend",
    "start": "1932659",
    "end": "1938299"
  },
  {
    "text": "upon undocumented Behavior about oh it's always safe to keep my config in global",
    "start": "1938299",
    "end": "1943340"
  },
  {
    "text": "chubby guess what that's that's not true",
    "start": "1943340",
    "end": "1947440"
  },
  {
    "text": "next question yes",
    "start": "1948440",
    "end": "1954100"
  },
  {
    "text": "what I'm interested to find out is how do you convince like business uh you",
    "start": "1962919",
    "end": "1969320"
  },
  {
    "text": "know we're gonna fall so we're going to make customers not a great get a great experience you know maybe a small amount but to for them to understand the",
    "start": "1969320",
    "end": "1976820"
  },
  {
    "text": "importance of you know doing these experiments but there is the risk that you will impact customers",
    "start": "1976820",
    "end": "1983000"
  },
  {
    "text": "yeah I think that the risk of customer impact you can make the argument that",
    "start": "1983000",
    "end": "1989120"
  },
  {
    "text": "you would much rather Discover it at 3 P.M not at 3am free like you'd much rather Discover it when people are",
    "start": "1989120",
    "end": "1994820"
  },
  {
    "text": "available to work on it rather than have it emerge and become a sudden emergency in the middle of the night",
    "start": "1994820",
    "end": "2000940"
  },
  {
    "text": "um I think that's kind of the main way to emphasize it to stakeholders is",
    "start": "2000940",
    "end": "2006580"
  },
  {
    "text": "that it's a mitigation technique to discover things before you would naturally run into them anyways so it's",
    "start": "2006580",
    "end": "2012940"
  },
  {
    "text": "kind of playing with the timing of when the failures is going to happen so it's not that you're adding new classes of",
    "start": "2012940",
    "end": "2018700"
  },
  {
    "text": "failure it's just that you're pulling forward the window of time in which you're discovering them",
    "start": "2018700",
    "end": "2024220"
  },
  {
    "text": "I think the other thing to emphasize is users have some degree of expectations",
    "start": "2024220",
    "end": "2030640"
  },
  {
    "text": "right like you shouldn't always be experimenting on the same pool of users or if you are those should be users who",
    "start": "2030640",
    "end": "2037360"
  },
  {
    "text": "have deliberately opted in right like if you have a beta channel right like those those are those are the users who have said you know I'm willing to tolerate a",
    "start": "2037360",
    "end": "2043539"
  },
  {
    "text": "little bit more risk to see the bleeding edge features right so I think that's basically what it what it boils down to",
    "start": "2043539",
    "end": "2048760"
  },
  {
    "text": "is if you have to pick a percentage of users to experiment on right either pick the users who are the most adventurous or make sure you're rotating that",
    "start": "2048760",
    "end": "2055658"
  },
  {
    "text": "potential impact around and exclude from your experiments right like if you have shards that have been particularly",
    "start": "2055659",
    "end": "2061480"
  },
  {
    "text": "problematic right those users have seen enough pain don't induce more pain on them but if you have users that have",
    "start": "2061480",
    "end": "2066878"
  },
  {
    "text": "been having an abnormally High degree of user experience I think it's okay to run your",
    "start": "2066879",
    "end": "2073599"
  },
  {
    "text": "experiments against those users in order to enter the safety of the whole system at Large",
    "start": "2073599",
    "end": "2079179"
  },
  {
    "text": "so it's kind of that that issue of if you don't have enough data it can feel dangerous",
    "start": "2079179",
    "end": "2084220"
  },
  {
    "text": "that's why the first job might be to gather that Baseline data on how much error budget do you have how",
    "start": "2084220",
    "end": "2090220"
  },
  {
    "text": "much have you spent on which users who is the safest to run experiments with so you can have that confidence and even if",
    "start": "2090220",
    "end": "2096700"
  },
  {
    "text": "you decide not to run experiment after that guess what you've still done something super valuable right you now understand",
    "start": "2096700",
    "end": "2101820"
  },
  {
    "text": "the happiness of your users who wants to make me run around next",
    "start": "2101820",
    "end": "2109980"
  },
  {
    "text": "I can stand up on here on stage and not run around it's fantastic",
    "start": "2111400",
    "end": "2116640"
  },
  {
    "text": "no more questions yes",
    "start": "2117400",
    "end": "2121740"
  },
  {
    "text": "um I'm just having a quick question around agile principles and modeling",
    "start": "2128079",
    "end": "2134380"
  },
  {
    "text": "um agile says release small and release quick do you believe in that",
    "start": "2134380",
    "end": "2141940"
  },
  {
    "text": "yeah I think that it's as I said we deploy 14 times per day right like and",
    "start": "2141940",
    "end": "2148240"
  },
  {
    "text": "one of the main problems that we have actually right now is that the hourly deploy is now constraining us previously",
    "start": "2148240",
    "end": "2154660"
  },
  {
    "text": "when we had a dozen software developers there was not a problem right every every release contained exactly one change list now every release contains",
    "start": "2154660",
    "end": "2161619"
  },
  {
    "text": "three or four change lists because we're batching them up every hour and two us that's not satisfactory that actually was the primary motivation for us to",
    "start": "2161619",
    "end": "2168880"
  },
  {
    "text": "switch away from ec2 raw VMS onto kubernetes uh was that we wanted that flexibility to be able to do deployments",
    "start": "2168880",
    "end": "2175660"
  },
  {
    "text": "and have central control over them so that we could pipeline them rather than have kind of cron jobs running on each",
    "start": "2175660",
    "end": "2181720"
  },
  {
    "text": "machine that were just pulling the latest golden build every hour so we do think that kind of small incremental",
    "start": "2181720",
    "end": "2187060"
  },
  {
    "text": "changes can reduce risk um but again right like that's a thing that",
    "start": "2187060",
    "end": "2192220"
  },
  {
    "text": "you have to validate you have to you have to actually make it safe to roll out those changes right if you're rolling out changes once a week or once",
    "start": "2192220",
    "end": "2198700"
  },
  {
    "text": "a month right every deploy is going to be a potential bomb that's that's that's going to explode right whereas if you're",
    "start": "2198700",
    "end": "2205839"
  },
  {
    "text": "regularly doing these deploys that's going to be good right in the sense that every deploy is going to be",
    "start": "2205839",
    "end": "2211839"
  },
  {
    "text": "at least nominally some somewhat safe the downside is that starts to expose you to some risks of what happens if",
    "start": "2211839",
    "end": "2218800"
  },
  {
    "text": "your load balancer is misconfigured and there's a problem with every time you do a deploy one percent of your traffic",
    "start": "2218800",
    "end": "2224740"
  },
  {
    "text": "gets dropped because it's sent to an instance that's shutting down right that causes you to have to think a little bit",
    "start": "2224740",
    "end": "2229780"
  },
  {
    "text": "harder about those cases of if you have a deploy in Flight if you have servers that are in lame duck mode or shutting",
    "start": "2229780",
    "end": "2235780"
  },
  {
    "text": "down and they're not properly reporting lame duck status right like how do you make sure that those are behaving correctly right so it's kind of not",
    "start": "2235780",
    "end": "2243060"
  },
  {
    "text": "you're right that there is kind of some risk involved with moving faster but I think that those are things that we can",
    "start": "2243060",
    "end": "2248440"
  },
  {
    "text": "mitigate for did I answer your question great see a thumbs up cool",
    "start": "2248440",
    "end": "2256200"
  },
  {
    "text": "thanks for the great talk um I was wondering uh there's been a lot of uh I guess discussions about feature",
    "start": "2268780",
    "end": "2276040"
  },
  {
    "text": "flags and I guess a similar problem exists with feature Flags to microservices when you get Beyond a",
    "start": "2276040",
    "end": "2282579"
  },
  {
    "text": "point of scale especially with multiple multiple environments with multiple different feature Flags across a stack",
    "start": "2282579",
    "end": "2289720"
  },
  {
    "text": "of Technology how do you see ways of combating that in an effective",
    "start": "2289720",
    "end": "2296079"
  },
  {
    "text": "manner yeah I think that feature flags are fun this is actually a",
    "start": "2296079",
    "end": "2302500"
  },
  {
    "text": "I have a ringer in the audience asking asking me awesome questions right like so okay I I think right like the set of",
    "start": "2302500",
    "end": "2309700"
  },
  {
    "text": "modern technologies that are going to kind of empower the next generation of software delivery right like we understand we should all be doing",
    "start": "2309700",
    "end": "2315400"
  },
  {
    "text": "microservice here like I hope so at least right like but I think the next generation of things we need to pay attention to are CI CD future flags",
    "start": "2315400",
    "end": "2323020"
  },
  {
    "text": "observability that those are kind of the three things that really really work really well together and create a",
    "start": "2323020",
    "end": "2328180"
  },
  {
    "text": "virtuous cycle where cicd will help you push the code that contains your feature Flags out",
    "start": "2328180",
    "end": "2333880"
  },
  {
    "text": "faster feature Flags help you kind of have the right knobs to turn on individual features at a time for specific subsets of users",
    "start": "2333880",
    "end": "2340300"
  },
  {
    "text": "but you have to have observability to do that kind of big data analytics to understand",
    "start": "2340300",
    "end": "2345520"
  },
  {
    "text": "really really quickly if you are seeing one or two percent errors which users which feature slides are most strongly",
    "start": "2345520",
    "end": "2351339"
  },
  {
    "text": "correlated across which particular sets of microservices right so I think that",
    "start": "2351339",
    "end": "2356859"
  },
  {
    "text": "yes feature Flags introduce complexity and introduce common heterical complexity",
    "start": "2356859",
    "end": "2362560"
  },
  {
    "text": "but observability is well positioned to be able to answer those questions about combinatorical complexity and to pull",
    "start": "2362560",
    "end": "2369880"
  },
  {
    "text": "out those high cardinality and high dimensionality values to help you understand really quickly where is this",
    "start": "2369880",
    "end": "2375640"
  },
  {
    "text": "coming from right you're 100 correct that kind of I'm a little bit skeptical right of you know people are like you",
    "start": "2375640",
    "end": "2381339"
  },
  {
    "text": "know not just you know metrics logs traces but you know now suddenly adding events as a category of you know I have",
    "start": "2381339",
    "end": "2386619"
  },
  {
    "text": "to find out that every time a feature flag changes I have to find out every time right like you know X or Y or Z changes my systems right",
    "start": "2386619",
    "end": "2392020"
  },
  {
    "text": "and turns out that a linear timeline of events that have happened in your system can be useful for putting together a",
    "start": "2392020",
    "end": "2397839"
  },
  {
    "text": "retrospective but it's not going to give you the operational data to understand which feature flags were in effect for",
    "start": "2397839",
    "end": "2403660"
  },
  {
    "text": "which users at the time that those users started experiencing failures right so being able to have the data and the",
    "start": "2403660",
    "end": "2410740"
  },
  {
    "text": "analysis to correlate that and understand that and to surface that as a hey you might want to know about this",
    "start": "2410740",
    "end": "2416500"
  },
  {
    "text": "right like that's kind of how we augment human understanding in order to cope with that complexity",
    "start": "2416500",
    "end": "2423359"
  }
]