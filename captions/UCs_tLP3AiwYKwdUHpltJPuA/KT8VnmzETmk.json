[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "text": "[Music]",
    "start": "2550",
    "end": "10170"
  },
  {
    "text": "thank you hello so hello Copenhagen uh in this",
    "start": "12160",
    "end": "17640"
  },
  {
    "text": "talk we're going to illustrate the evolution of Hadoop at Spotify so regardless of the reason why you're here",
    "start": "17640",
    "end": "23840"
  },
  {
    "text": "and maybe you're here because you have your own cluster or maybe you want to build your new cluster or maybe you just want to learn about Hadoop well we hope",
    "start": "23840",
    "end": "30800"
  },
  {
    "text": "that this talk was going to be useful for you because we're going to bring up a real world examples of incidents and",
    "start": "30800",
    "end": "36640"
  },
  {
    "text": "issues and bugs that we had and how we tackle them at Spotify but please keep in mind though that every single cluster",
    "start": "36640",
    "end": "43039"
  },
  {
    "text": "is different so what worked for us may not work for you but we hope that next",
    "start": "43039",
    "end": "48079"
  },
  {
    "text": "time you see similar issue you can take this knowledge and make more data driven decision about your issues before we",
    "start": "48079",
    "end": "54640"
  },
  {
    "text": "dive into the details though let's first introduce ourselves so I'm Rafal and this is Josh Josh is the product owner",
    "start": "54640",
    "end": "60760"
  },
  {
    "text": "and I'm an engineer at at Hadoop Squad and we both kind of fall in love with Hadoop about two years ago when we moved",
    "start": "60760",
    "end": "67159"
  },
  {
    "text": "to our beautiful Stockholm office and we started supporting this one of the biggest Hadoop clusters in Europe and",
    "start": "67159",
    "end": "73360"
  },
  {
    "text": "that was a huge responsibility but also great privilege so that now that you know who we are let's take a look at the",
    "start": "73360",
    "end": "80360"
  },
  {
    "text": "agenda of the talk so first we're going to talk about the issues the pain points how we were trying to H keep our cluster",
    "start": "80360",
    "end": "88000"
  },
  {
    "start": "82000",
    "end": "160000"
  },
  {
    "text": "up and running then we'll gain a Focus we'll stabilize the cluster and then at the end we'll finish it hard with the",
    "start": "88000",
    "end": "94240"
  },
  {
    "text": "future and our current features of our infrastructure so let's get started sure",
    "start": "94240",
    "end": "99479"
  },
  {
    "text": "so to start off with as Rafal mentioned we're going to start talking about the beginning of uh Big Data at Spotify uh",
    "start": "99479",
    "end": "106119"
  },
  {
    "text": "when we first started with Hadoop um but before we do that we're just going to go back up a second and talk about what",
    "start": "106119",
    "end": "111719"
  },
  {
    "text": "Spotify is I realized it's probably uh pretty well known in Scandinavia but just in case so we're all on the same",
    "start": "111719",
    "end": "117200"
  },
  {
    "text": "page uh Spotify is a music streaming service uh some people think of it as like iTunes on the cloud it's uh it was",
    "start": "117200",
    "end": "124640"
  },
  {
    "text": "launched in 2008 there's two main tiers a free tier and a premium tier that costs around â‚¬ 10 a month it gives full",
    "start": "124640",
    "end": "131440"
  },
  {
    "text": "access to all our cataloges of songs uh it's available in 58 countries I just",
    "start": "131440",
    "end": "137200"
  },
  {
    "text": "looked in over 48 of those countries stream average stream over 1 million streams per day uh some other numbers we",
    "start": "137200",
    "end": "144400"
  },
  {
    "text": "have over 75 million monthly active users uh those users chat choose from",
    "start": "144400",
    "end": "150040"
  },
  {
    "text": "catalog of over 30 million songs over 4 million artists and they combin to play",
    "start": "150040",
    "end": "155599"
  },
  {
    "text": "around 1 million streams every day so that's a lot of data um and because it's",
    "start": "155599",
    "end": "161159"
  },
  {
    "start": "160000",
    "end": "249000"
  },
  {
    "text": "a lot of data we have a pretty massive data infrastructure to process over it we just upgraded we added another three",
    "start": "161159",
    "end": "168319"
  },
  {
    "text": "or 400 nodes we got up to around 1,700 nodes we think we're probably the biggest Hadoop cluster in Europe if",
    "start": "168319",
    "end": "174400"
  },
  {
    "text": "you're not we'd love to hear how big your Hadoop cluster is we have 62 pedabytes of storage",
    "start": "174400",
    "end": "180519"
  },
  {
    "text": "uh we get over 30 terabytes of logs every day incoming generated by users uh",
    "start": "180519",
    "end": "186959"
  },
  {
    "text": "we run about 20,000 Hadoop jobs per day and those jobs generate over 400 terabytes of",
    "start": "186959",
    "end": "193599"
  },
  {
    "text": "data uh so that's a lot of data and you might be wondering what we do with all that data now let me ask a question real",
    "start": "193599",
    "end": "199519"
  },
  {
    "text": "quick um is there anybody in here that's a runner okay well if you use Spotify",
    "start": "199519",
    "end": "206840"
  },
  {
    "text": "you're in luck a few months ago we released this new application um a new",
    "start": "206840",
    "end": "212120"
  },
  {
    "text": "feature inside the application that's a running app and what's really cool about this is it gives you a custom play list",
    "start": "212120",
    "end": "218760"
  },
  {
    "text": "based off your running Temple uh we found that actually running with a beat uh when you're in rhythm with your",
    "start": "218760",
    "end": "224920"
  },
  {
    "text": "Cadence improves performance as much as 15% and what's really great is that we",
    "start": "224920",
    "end": "230879"
  },
  {
    "text": "combine your lisening habits your personal listing habits and the characteristics of all the songs that you like to listen to things like the",
    "start": "230879",
    "end": "237360"
  },
  {
    "text": "mood and the energy uh along with the song beat per minute to generate a bunch of different customized running",
    "start": "237360",
    "end": "243239"
  },
  {
    "text": "playlists of familiar and interesting music at every beat per minute for all our active",
    "start": "243239",
    "end": "248640"
  },
  {
    "text": "users another feature that we've recently launched uh that you might be familiar with if you're a Spotify user",
    "start": "248640",
    "end": "254480"
  },
  {
    "start": "249000",
    "end": "443000"
  },
  {
    "text": "is called Discovery weekly it's two hours of personalized recommended music delivered every Monday for all of our 75",
    "start": "254480",
    "end": "261400"
  },
  {
    "text": "million uh monthly active users and this has really been one of our most successful features that we've ever",
    "start": "261400",
    "end": "267080"
  },
  {
    "text": "launched every Monday we get a flurry of new like Twitter activities that say how",
    "start": "267080",
    "end": "272160"
  },
  {
    "text": "great their playlist is this week and how much better it was from the best or if we're late with the data we get a",
    "start": "272160",
    "end": "277800"
  },
  {
    "text": "bunch of Twitter complaints about how their playlist isn't there yet so we know that it's really important to our",
    "start": "277800",
    "end": "283600"
  },
  {
    "text": "users behind the scenes it's running a variety of different uh machine learning techniques that's all based off of",
    "start": "283600",
    "end": "290400"
  },
  {
    "text": "extracted features from the data yeah these features are pretty cool but",
    "start": "290400",
    "end": "295600"
  },
  {
    "text": "before we dive into them um let's first take a look at a very simple one so this is the top list for Denmark top 5050",
    "start": "295600",
    "end": "301800"
  },
  {
    "text": "songs that you that users are listening to in Denmark uh so let's take let's",
    "start": "301800",
    "end": "307680"
  },
  {
    "text": "spend two seconds and think about how would you basically implement this feature so if you think about it the",
    "start": "307680",
    "end": "314199"
  },
  {
    "text": "most simple the most Brute Force solution will be probably to just like throw this data into database and then",
    "start": "314199",
    "end": "319560"
  },
  {
    "text": "run a simple query like this one over here so you select some track ID some artist ID you count it uh you go by you",
    "start": "319560",
    "end": "326520"
  },
  {
    "text": "have a limit you have voila you have your feature right pretty simple the problem is that as soon as you start",
    "start": "326520",
    "end": "332639"
  },
  {
    "text": "getting this amount of data that Josh was talking about remember 30 terab um of data per day that we have to ingest",
    "start": "332639",
    "end": "339960"
  },
  {
    "text": "then that starts to be an issue basically so we knew right from the very beginning that we we need something that",
    "start": "339960",
    "end": "346680"
  },
  {
    "text": "will scale better with our scale with the amount of data that we're getting and also toess is just one type of",
    "start": "346680",
    "end": "353080"
  },
  {
    "text": "report we have to create lots of different reports lots of different calculations that we have to do on the data so we have to be flexible when it",
    "start": "353080",
    "end": "359880"
  },
  {
    "text": "comes to the type of calculations we're doing and the third reason we're getting different uh data from different sources",
    "start": "359880",
    "end": "366479"
  },
  {
    "text": "in a different structure so we have to be flexible about the data and the structure of it we basically did",
    "start": "366479",
    "end": "372000"
  },
  {
    "text": "something like Hadoop and that's why we started playing with Hado in 2009 so",
    "start": "372000",
    "end": "377479"
  },
  {
    "text": "very very you know very early in the in the life of Hado um and we had a couple",
    "start": "377479",
    "end": "382720"
  },
  {
    "text": "of learnings about it at the very beginning so basically now we know that it's kind of it scales pretty well and then it's flexible when it comes to what",
    "start": "382720",
    "end": "389199"
  },
  {
    "text": "kind of sworks you want to run on top of it and also with the schema on R principle it's actually flexible when it",
    "start": "389199",
    "end": "394759"
  },
  {
    "text": "comes to what kind of data I want to store on it but then there was also another cool findings uh which is Hado",
    "start": "394759",
    "end": "400759"
  },
  {
    "text": "streaming and Hado streaming is a feature of Hadoop that allows you to implement M redu jobs in a language",
    "start": "400759",
    "end": "406280"
  },
  {
    "text": "different than Java and that was important for for Spotify because Spotify back then was a big python shop",
    "start": "406280",
    "end": "412360"
  },
  {
    "text": "so that allows us to take this extensive knowledge of python and put it on top of Hadoop and then we started implementing",
    "start": "412360",
    "end": "418879"
  },
  {
    "text": "this this intensive pipelines uh using Python and basically we got allot of",
    "start": "418879",
    "end": "424000"
  },
  {
    "text": "insights and data and that allows us to make data driven decisions and make Spotify so successful the way it is",
    "start": "424000",
    "end": "430440"
  },
  {
    "text": "today but to be honest I'm missing one important piece of the story I'm saying that yes you can Rite these pipelines",
    "start": "430440",
    "end": "437160"
  },
  {
    "text": "and you can get these insights and you can do lots of knowledge but first problem that you're probably going to get is how to move the data to Hadoop so",
    "start": "437160",
    "end": "444479"
  },
  {
    "start": "443000",
    "end": "567000"
  },
  {
    "text": "that sounds like a simple problem well we just you know store it on hdfs That's The Simple Solution but well it's",
    "start": "444479",
    "end": "450599"
  },
  {
    "text": "actually not that easy so let's take a look at this problem From spotify's perspective so at Spotify we have this",
    "start": "450599",
    "end": "456919"
  },
  {
    "text": "notion of access points which are machines that all the clients connect to so every time you open your client and",
    "start": "456919",
    "end": "463599"
  },
  {
    "text": "you want to request a playlist or anything like that you want to get any backand service you go through the",
    "start": "463599",
    "end": "468720"
  },
  {
    "text": "access point and every time you request something from the access point access point will log that on that specific",
    "start": "468720",
    "end": "473919"
  },
  {
    "text": "machine in a tab separated format and because of that that's that introduces a",
    "start": "473919",
    "end": "478960"
  },
  {
    "text": "lot of a specific type of issues so the first one is that that data is pretty",
    "start": "478960",
    "end": "484080"
  },
  {
    "text": "raw and dirty so we have to make sure that our ETL pipelines are rock solid and that we make sure that every single",
    "start": "484080",
    "end": "491159"
  },
  {
    "text": "record is clean and that actually is not so simple of tap separated values another thing if you have a tab",
    "start": "491159",
    "end": "497159"
  },
  {
    "text": "separated value well the schema evolution is not that simple and it's specifically difficult at Spotify",
    "start": "497159",
    "end": "503240"
  },
  {
    "text": "because we have lots of small teams and if you want to change something in the structure if you want to add a field or",
    "start": "503240",
    "end": "508599"
  },
  {
    "text": "remove the field well you have to talk to lots of different teams and it's not uncommon for a team to change something",
    "start": "508599",
    "end": "514560"
  },
  {
    "text": "without talking to anyone and then you learn about it only by uh having an incident Downstream in the pipeline so",
    "start": "514560",
    "end": "520719"
  },
  {
    "text": "that's another issue there's another set of issues uh that we're having because we have a had cluster in only one data",
    "start": "520719",
    "end": "526600"
  },
  {
    "text": "center in London we have to move all the data to one single place in London and that introduces issues with uh",
    "start": "526600",
    "end": "533519"
  },
  {
    "text": "networking and what happens if you have to H duplicate the data we have to duplicate it later so on so",
    "start": "533519",
    "end": "540279"
  },
  {
    "text": "on uh so given the problem uh we started oh there's one more important issue also",
    "start": "540279",
    "end": "547040"
  },
  {
    "text": "so at Spotify we treat our logs very carefully and make we have to make sure that we get all of them because we treat",
    "start": "547040",
    "end": "553320"
  },
  {
    "text": "them like Financial um transactions because we have to pay back to our artist and make sure that they are",
    "start": "553320",
    "end": "558720"
  },
  {
    "text": "satisfied with Spotify so logs are super important for us so with that problem we",
    "start": "558720",
    "end": "563959"
  },
  {
    "text": "implemented our first iteration uh which was loog archiver and that solution",
    "start": "563959",
    "end": "569600"
  },
  {
    "start": "567000",
    "end": "787000"
  },
  {
    "text": "lasted very long so that solution lasted from 2009 to 2013 and you will be surprised because it was very trivial it",
    "start": "569600",
    "end": "576320"
  },
  {
    "text": "was just a set of Python scripts that would basically compress logs and then ring or RCP files between machines all",
    "start": "576320",
    "end": "582800"
  },
  {
    "text": "the way to uh to data center in London um and that jobs would basically be",
    "start": "582800",
    "end": "588160"
  },
  {
    "text": "crowned and basically it had a lot of issues and what kind of issues well uh",
    "start": "588160",
    "end": "594120"
  },
  {
    "text": "because it was CR and because it was trivial and it was set of Python scripts it would fail a lot there was no proper",
    "start": "594120",
    "end": "599640"
  },
  {
    "text": "monitoring there was no proper alerting so every time there was an issue our engines would have to go to a specific",
    "start": "599640",
    "end": "605519"
  },
  {
    "text": "data center SCP files manually all the way to uh to London and make sure that all the locks are there um if we had to",
    "start": "605519",
    "end": "612560"
  },
  {
    "text": "scale our access points we had to add more machines or we had to remove an access point that would be another",
    "start": "612560",
    "end": "617760"
  },
  {
    "text": "manual uh manual work for our Engineers so it was a huge magnificent",
    "start": "617760",
    "end": "623680"
  },
  {
    "text": "failure but before you bemon it too much it was also I must also introduce an",
    "start": "623680",
    "end": "628920"
  },
  {
    "text": "important part of spotify's culture and that's best summarized by two words Embrace failure uh the idea is pretty",
    "start": "628920",
    "end": "636880"
  },
  {
    "text": "simple basically when you you're failing you're learning a lot uh you're learning a lot more than if you're being very",
    "start": "636880",
    "end": "642560"
  },
  {
    "text": "conservative and moving slow and being afraid to fail so in the case of log archiver we actually learned a lot we",
    "start": "642560",
    "end": "649120"
  },
  {
    "text": "learned a lot about what not to do um with log delivery for example you don't make it rely on KRON um if you want to",
    "start": "649120",
    "end": "656680"
  },
  {
    "text": "expand your nodes and scale up all around the world um the failures and learning or",
    "start": "656680",
    "end": "663320"
  },
  {
    "text": "learnings guided our next generation of log delivery which we built on top of Kafka now let me ask another question is",
    "start": "663320",
    "end": "670720"
  },
  {
    "text": "anybody in is anybody using Kafka in their production environments in this room is is is anybody familiar with it",
    "start": "670720",
    "end": "680920"
  },
  {
    "text": "okay so Kafka is basically a messaging queuing system that was open sourc by LinkedIn uh that has a publish subscribe",
    "start": "680920",
    "end": "688320"
  },
  {
    "text": "model or Kafka turns producer consumer uh so we tried a few different",
    "start": "688320",
    "end": "693480"
  },
  {
    "text": "messaging systems when we were evaluating the next generation of our log delivery and we found pretty early",
    "start": "693480",
    "end": "699160"
  },
  {
    "text": "on that kafa just worked the best uh we had latency go down from our access",
    "start": "699160",
    "end": "704639"
  },
  {
    "text": "point as raal mentioned uh to our hgfs cluster go from hours down to seconds",
    "start": "704639",
    "end": "709839"
  },
  {
    "text": "and this opened up a a wide variety of new use cases that we could do with the data for example real-time processing",
    "start": "709839",
    "end": "716040"
  },
  {
    "text": "with Apache storm uh so this is a bit a simple picture of what our architecture looks",
    "start": "716040",
    "end": "722079"
  },
  {
    "text": "like uh so we have the 30 terabytes of data that's generated from users connecting to our access points uh",
    "start": "722079",
    "end": "729720"
  },
  {
    "text": "that's passed alongside to site local groupers that consume all the events",
    "start": "729720",
    "end": "734760"
  },
  {
    "text": "produced from the access point they compress them they encrypt them they send it over the internet to London uh",
    "start": "734760",
    "end": "741040"
  },
  {
    "text": "where it gets consumed into our Hadoop cluster so we'll be pretty honest we've",
    "start": "741040",
    "end": "746199"
  },
  {
    "text": "had a lot of problems uh with even this log delivery system uh but that's",
    "start": "746199",
    "end": "751440"
  },
  {
    "text": "related a lot to the Kafka system that we're using we're using Kafka 0.7 which is a little bit older architecture and",
    "start": "751440",
    "end": "758519"
  },
  {
    "text": "it's also due to the fact that we have a end to end delivery system that we built on top of Kafka uh that really allows us",
    "start": "758519",
    "end": "765519"
  },
  {
    "text": "to get the reliable delivery because these logs as we said are pretty important um but our log delivery system",
    "start": "765519",
    "end": "773000"
  },
  {
    "text": "is constantly evolving as at every new uh issue that we hit and in fact right",
    "start": "773000",
    "end": "778399"
  },
  {
    "text": "now we're in the process of evaluating a bunch of different log Delivery Systems uh so we can improve it and fix some of",
    "start": "778399",
    "end": "784000"
  },
  {
    "text": "the bugs that uh exist in our system so now that we have data inside of a dup um you might want to start",
    "start": "784000",
    "end": "791760"
  },
  {
    "start": "787000",
    "end": "1057000"
  },
  {
    "text": "doing data you might want to start doing something with data so you run a few jobs and you might schedule them uh for",
    "start": "791760",
    "end": "798560"
  },
  {
    "text": "example like in cron like this and run the jobs that predictable",
    "start": "798560",
    "end": "803760"
  },
  {
    "text": "times uh this might work initially but what happens if your previous job fails",
    "start": "803760",
    "end": "810920"
  },
  {
    "text": "you don't want to process over incomplete data because that's actually worse than process than not processing",
    "start": "810920",
    "end": "815959"
  },
  {
    "text": "at all because you'll have inconsistent results down your pipeline so faced with this challenge",
    "start": "815959",
    "end": "821760"
  },
  {
    "text": "one of the early engineers at Spotify his name was Eric bernhardson tackled it by creating a new tool it was called",
    "start": "821760",
    "end": "829600"
  },
  {
    "text": "Luigi uh because it handles a plumbing of Hadoop jobs and also because it's",
    "start": "829600",
    "end": "834959"
  },
  {
    "text": "green and Spotify is green and that's cool so it's Luigi's a work flow orchestrator that's written in Python",
    "start": "834959",
    "end": "841120"
  },
  {
    "text": "that allows you to Define job dependencies programmatically so if for example in",
    "start": "841120",
    "end": "846199"
  },
  {
    "text": "the royalty calculation uh your royalty calculation pipeline depends Upstream on",
    "start": "846199",
    "end": "851320"
  },
  {
    "text": "some ETL jobs it's going to make sure that those dependencies are complete before it runs the royy calculation jobs",
    "start": "851320",
    "end": "858800"
  },
  {
    "text": "if it's not complete it's going to schedule them so it's been a very successful project that was first open",
    "start": "858800",
    "end": "864240"
  },
  {
    "text": "source by Spotify in 2011 it's used all over the world at hundreds of different",
    "start": "864240",
    "end": "869320"
  },
  {
    "text": "companies we're finding new ones each and every day including some really big ones like four square and stripe",
    "start": "869320",
    "end": "876440"
  },
  {
    "text": "unfortunately we don't have so many so much time to go down into the details of Luigi but if you're interested in this",
    "start": "876440",
    "end": "882160"
  },
  {
    "text": "we encourage you to go to the GitHub page down there and check it out so now that we have data on htfs and",
    "start": "882160",
    "end": "888920"
  },
  {
    "text": "we have a scheduler that we can schedule the job with um we started getting more and more Engineers into crunching the",
    "start": "888920",
    "end": "895360"
  },
  {
    "text": "data that was pretty cool and what comes uh with engineers at Spotify there is",
    "start": "895360",
    "end": "900800"
  },
  {
    "text": "feedback and there's a good pie positive feedback and there also constru construct constructive negative feedback",
    "start": "900800",
    "end": "907440"
  },
  {
    "text": "and one of it was that there's no data catalog so it's hard to find data sets on hdfs and every now and then an",
    "start": "907440",
    "end": "914480"
  },
  {
    "text": "engineer would have to go to hdfs unless some unless some directories then maybe at the end will cut it so like in this",
    "start": "914480",
    "end": "920639"
  },
  {
    "text": "case looking for the boat on the data Lake uh every single execution of this hdfs client would take a couple of",
    "start": "920639",
    "end": "927399"
  },
  {
    "text": "seconds because it has to load JPM has to load the libraries and so on so on and that was a pain point for our",
    "start": "927399",
    "end": "932800"
  },
  {
    "text": "engineers and because we like to experiment we've decided that we'll experiment with the RPC protocol in hdfs",
    "start": "932800",
    "end": "939440"
  },
  {
    "text": "and that's how another open source project at SP IFI was born and that was snakebite snakebite was originally",
    "start": "939440",
    "end": "945560"
  },
  {
    "text": "created but by vouter Deb and in the nutshell it's basically a pure python",
    "start": "945560",
    "end": "950959"
  },
  {
    "text": "hdfs client which means that everything happens inside python there's no Java uh",
    "start": "950959",
    "end": "956319"
  },
  {
    "text": "happening in this client uh and it's actually very simple and um",
    "start": "956319",
    "end": "961959"
  },
  {
    "text": "very intuitive when it comes to simple read operations on hdfs and how fast it this let me show you on another slide so",
    "start": "961959",
    "end": "968399"
  },
  {
    "text": "here you can see 100 executions of vanilla hdfs client on top and Below you can see 100 executions of snake bite",
    "start": "968399",
    "end": "974279"
  },
  {
    "text": "client and you can see that it's roughly 10 times faster than vanilla hdfs CLI",
    "start": "974279",
    "end": "980360"
  },
  {
    "text": "which is pretty cool but what it's also uh nice is it it will actually use less",
    "start": "980360",
    "end": "986199"
  },
  {
    "text": "resources than vanilla hdfs client it will use less memory and less CPU and that is actually pretty handy especially",
    "start": "986199",
    "end": "992040"
  },
  {
    "text": "if you have a service that interacts with hdfs a lot and at Spotify we actually have such service and it's",
    "start": "992040",
    "end": "997880"
  },
  {
    "text": "Luigi because Luigi actually to schedule the jobs it has to do a lot of existenc checks on",
    "start": "997880",
    "end": "1004079"
  },
  {
    "text": "hdfs and when you start running tens of thousands of jobs that can actually overload the machines that you run the",
    "start": "1004079",
    "end": "1009920"
  },
  {
    "text": "schuer on and we actually had this issue so we've decided that we will move from hdfs client to snake bike client and",
    "start": "1009920",
    "end": "1016319"
  },
  {
    "text": "what we've noticed immediately afterwards is that our checks are more stable are faster and scheduling works",
    "start": "1016319",
    "end": "1022759"
  },
  {
    "text": "more promptly which is pretty cool and I encourage you to take a look at the GitHub page and a search for snake",
    "start": "1022759",
    "end": "1030319"
  },
  {
    "text": "bite so now that we had our Hadoop cluster up and running and we have the kafa that's loading data into Hadoop and",
    "start": "1030319",
    "end": "1036959"
  },
  {
    "text": "we have some tools like Luigi and snake bite so that all developers around Spotify could could access data and run",
    "start": "1036959",
    "end": "1044199"
  },
  {
    "text": "Hadoop jobs uh we started to run into a new issue and that was that Developers actually were running a lot of jobs they",
    "start": "1044199",
    "end": "1050360"
  },
  {
    "text": "were running more and more jobs and they were the importance of those jobs were just increasing over",
    "start": "1050360",
    "end": "1058039"
  },
  {
    "start": "1057000",
    "end": "1113000"
  },
  {
    "text": "time and the problem was that the team that managed the Hadoop cluster was also",
    "start": "1058039",
    "end": "1063840"
  },
  {
    "text": "the team that was writing jobs too and they were also developing and supporting tools like snake bite and",
    "start": "1063840",
    "end": "1070440"
  },
  {
    "text": "Luigi and they didn't have a lot of time for Hadoop maintenance and if you're familiar with Hadoop a Hadoop cluster",
    "start": "1070440",
    "end": "1076600"
  },
  {
    "text": "without time for maintenance is an accident waiting to happen and after a particularly particularly hairy incident",
    "start": "1076600",
    "end": "1084000"
  },
  {
    "text": "uh that caused multi-day outage uh we decided we had to change something and the decision was to form a team so the",
    "start": "1084000",
    "end": "1090760"
  },
  {
    "text": "team started with Rafal myself and another engineer at Spotify and we had a very simple Mission uh when we",
    "start": "1090760",
    "end": "1098480"
  },
  {
    "text": "started uh the first part was that we had to migrate to a new distribution of Hadoop that included yarn uh the second",
    "start": "1098480",
    "end": "1106039"
  },
  {
    "text": "part and the most important part was that we had to make Hadoop reliable and so you might be wondering",
    "start": "1106039",
    "end": "1112320"
  },
  {
    "text": "how we did so let me show you in this graph uh so in the first section this is when",
    "start": "1112320",
    "end": "1118919"
  },
  {
    "start": "1113000",
    "end": "1232000"
  },
  {
    "text": "Hadoop was essentially owner list at Spotify we had a lot of random issues and outages that caused downtime around",
    "start": "1118919",
    "end": "1125679"
  },
  {
    "text": "the company Hadoop was kind of like a dirty word because even though it was really easy to write and run jobs on the",
    "start": "1125679",
    "end": "1130880"
  },
  {
    "text": "cluster you weren't so sure if it was going to be up uh when you actually really needed the",
    "start": "1130880",
    "end": "1136919"
  },
  {
    "text": "results uh the second section is actually when we started the Hadoop team at Spotify uh we addressed a lot of the",
    "start": "1136919",
    "end": "1142840"
  },
  {
    "text": "lwh hanging fruit and started to improve reliability right away the third section",
    "start": "1142840",
    "end": "1148159"
  },
  {
    "text": "is when we upgraded our Hadoop cluster to a distribution that included yarn so there were some complications involved",
    "start": "1148159",
    "end": "1154520"
  },
  {
    "text": "in that it was also Complicated by changing distributions um in the first quarter of",
    "start": "1154520",
    "end": "1160720"
  },
  {
    "text": "2014 we added name node High availability and that also introduced the problems of their own because of our",
    "start": "1160720",
    "end": "1166880"
  },
  {
    "text": "our this the scale of our cluster and the size just nothing's ever as easy as the Hadoop book says it",
    "start": "1166880",
    "end": "1173320"
  },
  {
    "text": "is so in the third section uh this is when or the four section I guess uh this",
    "start": "1173320",
    "end": "1179240"
  },
  {
    "text": "is when we actually started to get pretty reliable and predictable um and we were actually",
    "start": "1179240",
    "end": "1184520"
  },
  {
    "text": "around we we did this by by making our puppet configurations that we used to control the Hadoop cluster really rock",
    "start": "1184520",
    "end": "1190880"
  },
  {
    "text": "solid solid we added a lot of monitoring alerting so that before issues happen we",
    "start": "1190880",
    "end": "1196240"
  },
  {
    "text": "were aware of the fact and could prevent them uh and we also built some infrastructure",
    "start": "1196240",
    "end": "1202320"
  },
  {
    "text": "to make upgrades easier so we're actually we're starting to be seen as a model team around Spotify uh because we",
    "start": "1202320",
    "end": "1209200"
  },
  {
    "text": "were really improving the reliability and addressing our users biggest",
    "start": "1209200",
    "end": "1215159"
  },
  {
    "text": "problems that was all cool except at the end you can see the last bar and this is the last quarter and you can see a",
    "start": "1215159",
    "end": "1221799"
  },
  {
    "text": "significant drop in availability of our hup cluster and that is mostly because of um our process of SC ing our Hardo",
    "start": "1221799",
    "end": "1229320"
  },
  {
    "text": "cluster from 12200 to 1700 notes and as Josh said yes we had the puppet we had",
    "start": "1229320",
    "end": "1237240"
  },
  {
    "start": "1232000",
    "end": "1487000"
  },
  {
    "text": "uh monitoring and alerting and we could prevent something that we knew about but as it turns out there was another class",
    "start": "1237240",
    "end": "1243840"
  },
  {
    "text": "of issues that we were starting to heit there was a class of issues um were basically bugs in Hadoop code so the the",
    "start": "1243840",
    "end": "1252080"
  },
  {
    "text": "lesson is that when we when you scale Hado you also scale the hidden bugs that",
    "start": "1252080",
    "end": "1257120"
  },
  {
    "text": "come in the code and in our case they come um we can see them as soon as we",
    "start": "1257120",
    "end": "1262520"
  },
  {
    "text": "start scaling over let's say 1,200 notes so in this case you can see two specific",
    "start": "1262520",
    "end": "1267919"
  },
  {
    "text": "issues that are super deadly to us but in terms of small clusters they wouldn't",
    "start": "1267919",
    "end": "1273400"
  },
  {
    "text": "really be that deadly or they wouldn't cost downtime in our cases both of them basically brought the whole cluster down",
    "start": "1273400",
    "end": "1279240"
  },
  {
    "text": "for more than one day uh so let's take a look at the first issue for example so",
    "start": "1279240",
    "end": "1284720"
  },
  {
    "text": "in the first issue um it's an issue with open files and fail over so when you fail over one name Noe to another one in",
    "start": "1284720",
    "end": "1291400"
  },
  {
    "text": "the ha setup uh the to be active name mode will have to go through all the open files make sure that they are still",
    "start": "1291400",
    "end": "1297799"
  },
  {
    "text": "open and valid that process was implemented in very poorly way it was very in inefficient way to implement",
    "start": "1297799",
    "end": "1304880"
  },
  {
    "text": "that and the whole description is in the code in a small cluster that wouldn't be an issue in our case where we have",
    "start": "1304880",
    "end": "1310919"
  },
  {
    "text": "thousands of open tens of thousands of open files at any given point that basically brings the whole cluster down",
    "start": "1310919",
    "end": "1317360"
  },
  {
    "text": "what is funny is that when we that issue actually made us upgrade from one uh version to another from 2.2 to 2.6",
    "start": "1317360",
    "end": "1324640"
  },
  {
    "text": "actually right after the upgrade we run into another issue the second one that Al actually also brought the whole cluster down and it's also related to a",
    "start": "1324640",
    "end": "1332080"
  },
  {
    "text": "failover uh so remember when you scale a Hardo cluster you also scale the hidden bugs in the",
    "start": "1332080",
    "end": "1338440"
  },
  {
    "text": "code yeah so there were a lot of uh bugs that we ran into that were in the Hadoop",
    "start": "1338440",
    "end": "1343480"
  },
  {
    "text": "cut there was also a lot of earlier challenges that we ran into just with our poor configuration and are needed to",
    "start": "1343480",
    "end": "1349720"
  },
  {
    "text": "constantly uh adapt to change uh but there were also some some",
    "start": "1349720",
    "end": "1355000"
  },
  {
    "text": "things some issues that we ran into that were totally 100% preventable as I'll talk about right now so uh little bit",
    "start": "1355000",
    "end": "1363600"
  },
  {
    "text": "over a year ago um the Hadoop team was doing a pretty good job and we were pretty proud of of the reliability of",
    "start": "1363600",
    "end": "1369279"
  },
  {
    "text": "the Hadoop cluster and the success that we've had and around the company you know people were coming up to us and they were saying you're doing such a",
    "start": "1369279",
    "end": "1375440"
  },
  {
    "text": "great job so we we thought we'd go celebrate uh and we decided to go out to",
    "start": "1375440",
    "end": "1381120"
  },
  {
    "text": "a bar in in Stockholm and you know have a good a few beers and just celebrate our success so when we were walking out",
    "start": "1381120",
    "end": "1388760"
  },
  {
    "text": "to this bar uh we all got a message uh coming in on our phones we opened it up",
    "start": "1388760",
    "end": "1394799"
  },
  {
    "text": "and the title said something like this um I think I made a mistake now when we",
    "start": "1394799",
    "end": "1400360"
  },
  {
    "text": "open up the email we realized that one of the users in New York had run a command uh on the cluster that looks",
    "start": "1400360",
    "end": "1407360"
  },
  {
    "text": "something like this now maybe someone can see what's wrong here maybe",
    "start": "1407360",
    "end": "1413880"
  },
  {
    "text": "now maybe now so when I when I realized the issue",
    "start": "1413880",
    "end": "1419080"
  },
  {
    "text": "I looked up and I saw rafal's face and it was like this mother of god what have they done so it turned out that the user",
    "start": "1419080",
    "end": "1427200"
  },
  {
    "text": "in New York had accidentally put a space in the command uh that he was running in between his team's name and the folder",
    "start": "1427200",
    "end": "1433880"
  },
  {
    "text": "that he actually wanted to delete and he had wiped out his entire team's director of data this was over a petabyte of data",
    "start": "1433880",
    "end": "1442120"
  },
  {
    "text": "that was collected over months and months of pretty intense haduk processing so we were standing there on",
    "start": "1442120",
    "end": "1448520"
  },
  {
    "text": "the sidewalk in Sunny Stockholm and we're trying to decide what do we do should we go back to the office shut",
    "start": "1448520",
    "end": "1454200"
  },
  {
    "text": "down Hadoop try to recover some of the blocks before they get permanently deleted or do we just continue on to the",
    "start": "1454200",
    "end": "1459880"
  },
  {
    "text": "bar and you know deal with it tomorrow and pretend we didn't see",
    "start": "1459880",
    "end": "1465080"
  },
  {
    "text": "it fortunately we didn't act have to make that decision uh because another",
    "start": "1466159",
    "end": "1471279"
  },
  {
    "text": "user from uh the same team of this guy in New York replied to the thread saying don't worry about it Ed we can actually",
    "start": "1471279",
    "end": "1477840"
  },
  {
    "text": "regenerate some of the most critical data uh in just a few days you know hooray we just saved a lot of space on",
    "start": "1477840",
    "end": "1484080"
  },
  {
    "text": "Hadoop cluster so from this we learned a few really important lessons the first one",
    "start": "1484080",
    "end": "1490960"
  },
  {
    "start": "1487000",
    "end": "1733000"
  },
  {
    "text": "is is from our colleague vouter uh he always says this sit on your hands before you type you know especially if",
    "start": "1490960",
    "end": "1497799"
  },
  {
    "text": "you're uh removing skipping trash in hdfs or you're using some kind of super",
    "start": "1497799",
    "end": "1502840"
  },
  {
    "text": "user you know before you hit that enter key make sure what you typed is actually what you want to run on the",
    "start": "1502840",
    "end": "1509120"
  },
  {
    "text": "cluster the second one is that users always want to retain their data and as we found out in this specific case you",
    "start": "1509120",
    "end": "1516120"
  },
  {
    "text": "know this team could regenerate all their recit critical data and they've really only needed a little part portion",
    "start": "1516120",
    "end": "1521520"
  },
  {
    "text": "of that one paby of data and if we knew this beforehand if we had actually challenged them a little bit harder uh",
    "start": "1521520",
    "end": "1527720"
  },
  {
    "text": "we could have saved a lot of money and space on a cluster and costs the third one is that you should",
    "start": "1527720",
    "end": "1533960"
  },
  {
    "text": "remove super users from your Edge note if you're familiar with super users they basically have Global access they have",
    "start": "1533960",
    "end": "1540320"
  },
  {
    "text": "super permissions on your Hardo cluster um and it's Spotify we have this you",
    "start": "1540320",
    "end": "1545520"
  },
  {
    "text": "know Swedish ideal of equality and and we have which means that all engineers at the company have pseudo access on the",
    "start": "1545520",
    "end": "1552279"
  },
  {
    "text": "machines that they have access to so if this user had used the super user and",
    "start": "1552279",
    "end": "1558440"
  },
  {
    "text": "had that space a little bit earlier in the command you know me and Rafal might not be up here giving this talk",
    "start": "1558440",
    "end": "1565279"
  },
  {
    "text": "today so the fourth lesson we learned is that moving to trash is actually a client side implementation in kop and in",
    "start": "1565279",
    "end": "1571080"
  },
  {
    "text": "snake bite we hadn't quite implemented it so we celebrated a little failure we",
    "start": "1571080",
    "end": "1577440"
  },
  {
    "text": "did a little hacking and you can now safely remove from snake bite that was a pretty cool incident and",
    "start": "1577440",
    "end": "1585039"
  },
  {
    "text": "a funny one uh but a few weeks afterwards there was another one so we had this external consultant over at",
    "start": "1585039",
    "end": "1591840"
  },
  {
    "text": "Spotify and his goal was basically to certify our cluster which means that he would go through all the different parts",
    "start": "1591840",
    "end": "1597240"
  },
  {
    "text": "of the cluster in configuration at the end say this cluster is healthy or not and this is what you have to improve the",
    "start": "1597240",
    "end": "1604200"
  },
  {
    "text": "first few days weren't actually pretty smoothed and we were kind of like happy about ourselves and proud because you",
    "start": "1604200",
    "end": "1609360"
  },
  {
    "text": "know he didn't find anything so we're like yes we're doing great uh but on the day number three due to miscommunication",
    "start": "1609360",
    "end": "1615679"
  },
  {
    "text": "and misconfiguration one of the teammates killed killed our standby name node and that was fine because it's a",
    "start": "1615679",
    "end": "1621159"
  },
  {
    "text": "standby name node except then there was another miscommunication and we killed our active name node which means that",
    "start": "1621159",
    "end": "1626840"
  },
  {
    "text": "there was no master node in hdfs which means there's no hdfs which means there's no Hadoop there's no processing",
    "start": "1626840",
    "end": "1633080"
  },
  {
    "text": "and that at our scales means around two hours of downtime and that was specifically bad because there was this",
    "start": "1633080",
    "end": "1638760"
  },
  {
    "text": "external consultant over right but that it wasn't as nearly as bad as day number four so on day number four we're sitting",
    "start": "1638760",
    "end": "1646559"
  },
  {
    "text": "in another room there's this there's the team and there's our managers and we're talking about the incident and overall",
    "start": "1646559",
    "end": "1652440"
  },
  {
    "text": "about the certification and the consultant is saying something around the lines of the fact that our our",
    "start": "1652440",
    "end": "1659159"
  },
  {
    "text": "testing and deployment procedures are like wild wide west and that was very",
    "start": "1659159",
    "end": "1664279"
  },
  {
    "text": "difficult to listen to uh but in the end he was right so and we knew that so right after the meeting the whole team",
    "start": "1664279",
    "end": "1670120"
  },
  {
    "text": "went to a room and we've decided we're not going to leave the room until we come up with a plan to basically solve",
    "start": "1670120",
    "end": "1675600"
  },
  {
    "text": "this issue and we came up with something that may be actually pretty simple and obvious to you which is a pre-production",
    "start": "1675600",
    "end": "1682159"
  },
  {
    "text": "cluster and pre-production cluster is made out of the same class of machines exactly the same class of machines very",
    "start": "1682159",
    "end": "1688440"
  },
  {
    "text": "similar configuration almost identical and we created a set of smoke tests that we can run that pre-production cluster",
    "start": "1688440",
    "end": "1695399"
  },
  {
    "text": "make sure that every uh every part of that system is well integrated and works perfectly fine and then we can use that",
    "start": "1695399",
    "end": "1702279"
  },
  {
    "text": "both smoke test and pre-production cluster to deploy changes first to pre-production then run the smoke test",
    "start": "1702279",
    "end": "1708480"
  },
  {
    "text": "get instant feedback and then decide whether we want to actually deploy or not that actually changed the way we",
    "start": "1708480",
    "end": "1714799"
  },
  {
    "text": "test and Deploy on production which worked pretty well specifically for example for our uh recent upgrade we're",
    "start": "1714799",
    "end": "1721399"
  },
  {
    "text": "able to discover through two issues that would probably most probably cause a",
    "start": "1721399",
    "end": "1726480"
  },
  {
    "text": "serious uh incident on the production if we have not uh discovered it before",
    "start": "1726480",
    "end": "1733240"
  },
  {
    "start": "1733000",
    "end": "1801000"
  },
  {
    "text": "um while the Hadoop Squad was trying to make the infrastructure stable I was",
    "start": "1733240",
    "end": "1738760"
  },
  {
    "text": "working on pre-production and all that kind of stuff there was another uh there was another effort ongoing in data and",
    "start": "1738760",
    "end": "1745279"
  },
  {
    "text": "that was to move from python to jvm so as we said before Spotify was a big",
    "start": "1745279",
    "end": "1750799"
  },
  {
    "text": "python shop and we implemented lots of pipelines in Python on top of Hadoop streaming and over the time we realized",
    "start": "1750799",
    "end": "1757559"
  },
  {
    "text": "that that is an issue because we started seeing lots of lots of failures of this kind of pipelines because every time",
    "start": "1757559",
    "end": "1763559"
  },
  {
    "text": "someone made a change in the python code it was very hard to change test it so people will basically throw it at the",
    "start": "1763559",
    "end": "1769679"
  },
  {
    "text": "cluster wa waste resources just to get feedback whether they made typo in a code or not or maybe there's a mismatch",
    "start": "1769679",
    "end": "1777000"
  },
  {
    "text": "in the type because of the nature of python so that was one class of issue another one was that uh there was",
    "start": "1777000",
    "end": "1783440"
  },
  {
    "text": "basically no testing infrastructure so people would throw basically a pipelines at the at the cluster and also the",
    "start": "1783440",
    "end": "1789399"
  },
  {
    "text": "performance was not there so that is because of the nature and architecture of Hado streaming and also python itself",
    "start": "1789399",
    "end": "1796159"
  },
  {
    "text": "so one of the engineers at Spotify David whitning did an extensive overview of",
    "start": "1796159",
    "end": "1801880"
  },
  {
    "start": "1801000",
    "end": "1867000"
  },
  {
    "text": "all the Frameworks you can use on Hadoop and you can find the links over here and then afterwards we've made we there was",
    "start": "1801880",
    "end": "1808240"
  },
  {
    "text": "a discussion about uh all the Frameworks and we've decided that we will choose Apache crunch as a supported framework",
    "start": "1808240",
    "end": "1815360"
  },
  {
    "text": "for running map produce uh batch jobs at Spotify and there was a couple of reasons behind it I'm going to bring up",
    "start": "1815360",
    "end": "1822000"
  },
  {
    "text": "three of them uh which we think are the most important ones so the first one is that you get real type so you get type",
    "start": "1822000",
    "end": "1827480"
  },
  {
    "text": "safety which means you can get a compile time errors you don't have to throw the job at the cluster to verify that it's",
    "start": "1827480",
    "end": "1834600"
  },
  {
    "text": "it's a proper job without typos or a schema mismatch another one you get a",
    "start": "1834600",
    "end": "1840159"
  },
  {
    "text": "high Lev API which means that you can start thinking in terms of joints Group B cor groups on all kind of fancy",
    "start": "1840159",
    "end": "1846480"
  },
  {
    "text": "functions instead of thinking in terms of this old map and redu Paradigm which is very nice and makes the whole",
    "start": "1846480",
    "end": "1852720"
  },
  {
    "text": "pipeline uh less veros and easier to maintain over time uh by different engineers then the third uh reason is",
    "start": "1852720",
    "end": "1860000"
  },
  {
    "text": "the performance itself uh that comes from jvm and let's take a look at the graph over here so on this graph you can",
    "start": "1860000",
    "end": "1866760"
  },
  {
    "text": "see uh The Benchmark of crunch and Hadoop streaming and this is a benchmark",
    "start": "1866760",
    "end": "1872519"
  },
  {
    "start": "1867000",
    "end": "1916000"
  },
  {
    "text": "for our production workload so this is not a synthetic one this is our these are our production workloads and this",
    "start": "1872519",
    "end": "1879320"
  },
  {
    "text": "specific graph is a map throughput in megabytes per second on the left you can see crunch on the right you can see Hado",
    "start": "1879320",
    "end": "1886159"
  },
  {
    "text": "streaming and you can see that Apache crunch is roughly eight times faster on average and almost 75% of all the Apache",
    "start": "1886159",
    "end": "1894440"
  },
  {
    "text": "crunch jobs are faster than all the had of streaming jobs which is a pretty good reason to move to",
    "start": "1894440",
    "end": "1899760"
  },
  {
    "text": "jvm and also we were able to came up with a pretty neat um testing environment for Apache crunch on top of",
    "start": "1899760",
    "end": "1906840"
  },
  {
    "text": "a mini cluster which basically means now we can enforce our developers to create tests for their pipelines which is also",
    "start": "1906840",
    "end": "1913639"
  },
  {
    "text": "very nice yeah so that was a lot of uh evolution uh so let's just quickly review what we",
    "start": "1913639",
    "end": "1920080"
  },
  {
    "start": "1916000",
    "end": "1980000"
  },
  {
    "text": "talked about so at the beginning we discussed the difficulties of even getting data to our hdfs cluster and how",
    "start": "1920080",
    "end": "1927519"
  },
  {
    "text": "we solved that using Kafka uh then we talked about some of the challenges that we had uh when we were first starting",
    "start": "1927519",
    "end": "1933600"
  },
  {
    "text": "out in writing Hadoop jobs and using KRON as the schedule and how we fix those by using tools like uh Luigi and",
    "start": "1933600",
    "end": "1939840"
  },
  {
    "text": "snake bite um then we talked about some of the issues that we had early on with",
    "start": "1939840",
    "end": "1944960"
  },
  {
    "text": "availability and how we solved that uh creating a team that was really focused on the Hadoop infrastructure only um and",
    "start": "1944960",
    "end": "1952519"
  },
  {
    "text": "also how we improved the reliability by doing things like mod proper monitoring and",
    "start": "1952519",
    "end": "1957600"
  },
  {
    "text": "alerting uh then Rafal recently talked about how we we started to really focus on performance and improve uh by moving",
    "start": "1957600",
    "end": "1964919"
  },
  {
    "text": "things from Python and Hadoop streaming uh to Apache Crunch and this last",
    "start": "1964919",
    "end": "1970200"
  },
  {
    "text": "section we're going to talk about the future you know what we're working on now what we're planning on planning to",
    "start": "1970200",
    "end": "1976080"
  },
  {
    "text": "focus on the next 6 to 12 months at Hadoop at Spotify so this is a graph of the growth",
    "start": "1976080",
    "end": "1982679"
  },
  {
    "start": "1980000",
    "end": "2008000"
  },
  {
    "text": "of Hadoop versus Spotify end users since 2012 when Spotify had just crossed over",
    "start": "1982679",
    "end": "1988600"
  },
  {
    "text": "10 million users so since that time spotify's grown 650 which is pretty great growth uh but",
    "start": "1988600",
    "end": "1996440"
  },
  {
    "text": "the user growth doesn't compare at all to Hadoop usage growth the growth in demand for compute resources at",
    "start": "1996440",
    "end": "2003600"
  },
  {
    "text": "Spotify so it's which has grown over 4,000% um as you see um so what caused",
    "start": "2003600",
    "end": "2010480"
  },
  {
    "start": "2008000",
    "end": "2199000"
  },
  {
    "text": "so much growth now we attribute it to three main things the first one is",
    "start": "2010480",
    "end": "2015760"
  },
  {
    "text": "pretty obvious with increased Spotify end users you're going to have a lot more data all those users are listening",
    "start": "2015760",
    "end": "2021960"
  },
  {
    "text": "to a lot more song which means that all the pipelines that you have that you used to have uh things like the royalty",
    "start": "2021960",
    "end": "2028240"
  },
  {
    "text": "pip pipelines and the the top charts have to process over a lot more data and they need a lot more compute resour",
    "start": "2028240",
    "end": "2034799"
  },
  {
    "text": "resources to do that the second one is increase use cases so when we started",
    "start": "2034799",
    "end": "2040760"
  },
  {
    "text": "out with Hadoop at Spotify we used it for just analytics and our our reporting",
    "start": "2040760",
    "end": "2046639"
  },
  {
    "text": "pipelines we didn't use it for a ton of things these days we use it to power features uh things like the discovery",
    "start": "2046639",
    "end": "2052878"
  },
  {
    "text": "weekly is is all at the end driven by Hadoop and and all these massive machine learning and graph processing",
    "start": "2052879",
    "end": "2059878"
  },
  {
    "text": "jobs and we've really seen that just all the different use cases means you're going to be running a lot more jobs and",
    "start": "2059879",
    "end": "2065599"
  },
  {
    "text": "you're going to need a lot more processing power the last one is is actually really interesting so we've increased a lot of",
    "start": "2065599",
    "end": "2072520"
  },
  {
    "text": "Engineers at Spotify that process over data and that's that's driven growth a",
    "start": "2072520",
    "end": "2078079"
  },
  {
    "text": "lot in 2014 Spotify acquired this company called the eonis that's based in Boston in the US um they're they're",
    "start": "2078079",
    "end": "2085599"
  },
  {
    "text": "Music Company that's just obsessed with music intelligence and before they came to Spotify they actually never used",
    "start": "2085599",
    "end": "2091919"
  },
  {
    "text": "Hadoop uh but when they started it and had access to a tror trove of of user data they they became",
    "start": "2091919",
    "end": "2098240"
  },
  {
    "text": "and now they're some of our heaviest users and run some of the most massive jobs now we learned through all this",
    "start": "2098240",
    "end": "2105200"
  },
  {
    "text": "growth is what we learned through this growth is an important realization that's that scaling machines is actually",
    "start": "2105200",
    "end": "2112560"
  },
  {
    "text": "kind of easy you know we have alerts we have monitoring proper monitoring we have really solid puppet configuration",
    "start": "2112560",
    "end": "2119320"
  },
  {
    "text": "that allows us to add new machines you know it it's it's allowed us to go from 120 machines a little bit over two years",
    "start": "2119320",
    "end": "2126119"
  },
  {
    "text": "ago uh to the 1700 almost 1,700 machines that we have today without much trouble",
    "start": "2126119",
    "end": "2132160"
  },
  {
    "text": "now we have run into some issues as Ral mentioned with the recent uh adding of 400 nodes and going or 500 going from",
    "start": "2132160",
    "end": "2138640"
  },
  {
    "text": "1200 to 1700 but for the most part it's been pretty smooth but scaling people",
    "start": "2138640",
    "end": "2145320"
  },
  {
    "text": "that's that's actually really hard now we're still a relatively small team at Spotify um but we support hundreds of",
    "start": "2145320",
    "end": "2152720"
  },
  {
    "text": "users that are processing data and they have different levels of expertise down from the beginning to the expert and we",
    "start": "2152720",
    "end": "2159280"
  },
  {
    "text": "have trouble keeping up with all their problems uh so you might wonder what we're planning on doing about",
    "start": "2159280",
    "end": "2165640"
  },
  {
    "text": "that we're starting to automate feedback we want to provide information about",
    "start": "2165640",
    "end": "2170720"
  },
  {
    "text": "Hadoop jobs to users immediately after job completion things like did your job fail with the specific error maybe we",
    "start": "2170720",
    "end": "2177480"
  },
  {
    "text": "know about that error can link to the Jura ticket and maybe the workaround for it uh maybe your job is launching with",
    "start": "2177480",
    "end": "2184359"
  },
  {
    "text": "the incorrect user permissions so we'll try to put up warning sign before you run your job so you don't waste all the",
    "start": "2184359",
    "end": "2190359"
  },
  {
    "text": "compute resources only to come up with nothing but while we're working on this",
    "start": "2190359",
    "end": "2195400"
  },
  {
    "text": "we've deployed a few things that are already helping us and might help you too if you're running a dup the first is",
    "start": "2195400",
    "end": "2201359"
  },
  {
    "start": "2199000",
    "end": "2238000"
  },
  {
    "text": "a project called inviso uh this was written and released by Netflix a little bit over uh around a",
    "start": "2201359",
    "end": "2207880"
  },
  {
    "text": "year ago viso allows you to see what's going on with the cluster in real time and provide some really nice uh",
    "start": "2207880",
    "end": "2214119"
  },
  {
    "text": "visualizations that make it obvious for example if a sing Le job is dominating the cluster and using all the",
    "start": "2214119",
    "end": "2221119"
  },
  {
    "text": "resources it also allows it also contains some pretty cool visualization tools that you can go down in in the",
    "start": "2221119",
    "end": "2227800"
  },
  {
    "text": "individual job level and see how how the life of the job and and how it's performing and you can use that to",
    "start": "2227800",
    "end": "2234079"
  },
  {
    "text": "improve job performance I'd really encourage you to check it out sorry the other thing we've been",
    "start": "2234079",
    "end": "2241880"
  },
  {
    "start": "2238000",
    "end": "2325000"
  },
  {
    "text": "doing seems so obvious that we've been really surprised at how effective it's been so every quarter uh we publish a",
    "start": "2241880",
    "end": "2249440"
  },
  {
    "text": "newsletter that contains all kinds of different information and statistics about Hadoop",
    "start": "2249440",
    "end": "2254480"
  },
  {
    "text": "jobs things like the growth of had uh compute demand the increase of storage",
    "start": "2254480",
    "end": "2260280"
  },
  {
    "text": "uh the top failing jobs of the cluster in that quarter or in the month or plain",
    "start": "2260280",
    "end": "2266000"
  },
  {
    "text": "old Hadoop reliability now we always get really great feedback around this",
    "start": "2266000",
    "end": "2272240"
  },
  {
    "text": "newsletter sorry I'm getting over cold um but a few months ago it was even more eff Ive because it identified a single",
    "start": "2272240",
    "end": "2279680"
  },
  {
    "text": "job that was running every day and using wasting over 10% of the cluster's",
    "start": "2279680",
    "end": "2285160"
  },
  {
    "text": "resources because it was always failing now before we published a newsletter um the user that owned this",
    "start": "2285160",
    "end": "2291680"
  },
  {
    "text": "job didn't even realize his job was running every day on the cluster and failing but when he saw his name on the",
    "start": "2291680",
    "end": "2297480"
  },
  {
    "text": "newsletter the social peer pressure alone uh caused him to to immediately go",
    "start": "2297480",
    "end": "2304240"
  },
  {
    "text": "unschedule his job and vow to us that he'd never launch a job on the Hadoop cluster without properly testing it um",
    "start": "2304240",
    "end": "2311680"
  },
  {
    "text": "and without kind of certifying with us so just from the social peer pressure and improved user performance uh which",
    "start": "2311680",
    "end": "2317599"
  },
  {
    "text": "was great for us because we didn't have to go in and and you know manually tell him that he has to stop doing that stuff",
    "start": "2317599",
    "end": "2323599"
  },
  {
    "text": "which was great yes since this is a talk uh data talk we have to mention spark so there's",
    "start": "2323599",
    "end": "2331640"
  },
  {
    "start": "2325000",
    "end": "2410000"
  },
  {
    "text": "there's spark slide uh so yeah we we are evaluating spark and we've been playing",
    "start": "2331640",
    "end": "2337720"
  },
  {
    "text": "with it since the very beginning we had some issues with it and pretty recently we started to uh experiment with um",
    "start": "2337720",
    "end": "2345280"
  },
  {
    "text": "aache Zeppelin on top of Apache spark so you've probably heard about spark but how many of you have heard about",
    "start": "2345280",
    "end": "2352640"
  },
  {
    "text": "Zeppelin cool so there's a couple of people great so you can think of zapin as the I python on steroids on top of",
    "start": "2352640",
    "end": "2360240"
  },
  {
    "text": "spark right so what it gives you is that you can basically dive into Data slice",
    "start": "2360240",
    "end": "2366319"
  },
  {
    "text": "and dice a little bit and get the result in a nice visual way right so it's like an like a notebook experience on top of",
    "start": "2366319",
    "end": "2372480"
  },
  {
    "text": "your spark and the way we want to use it is you want to use it as a glue tool",
    "start": "2372480",
    "end": "2377960"
  },
  {
    "text": "where you can connect to all the different pieces of our infrastructure fetch the data in quickly process it get",
    "start": "2377960",
    "end": "2384200"
  },
  {
    "text": "the nice visual um result and then the engineer can decide whether there's",
    "start": "2384200",
    "end": "2389839"
  },
  {
    "text": "value in that data and whether there's a need to deploy a proper production ready Pipeline on top of scolding or crunch or",
    "start": "2389839",
    "end": "2397400"
  },
  {
    "text": "anything like that so that's where we see it we have a we have some good results from the very beginning I encourage you strongly to take a look at",
    "start": "2397400",
    "end": "2404800"
  },
  {
    "text": "a aachi zeppelin uh this actually brings us to uh almost the end uh which is two",
    "start": "2404800",
    "end": "2412200"
  },
  {
    "start": "2410000",
    "end": "3108000"
  },
  {
    "text": "takeaways uh so this is kind of the most important slide of the of the talk um so",
    "start": "2412200",
    "end": "2418960"
  },
  {
    "text": "two takeaways um one is that there's no golden path uh especially when it comes",
    "start": "2418960",
    "end": "2424079"
  },
  {
    "text": "to Big Data there are no patterns there are kind of emerging patterns patterns but the engineers are kind of defining",
    "start": "2424079",
    "end": "2431000"
  },
  {
    "text": "it or up to Define it so when you have a problem when you have to design a system uh that will deal with data uh we would",
    "start": "2431000",
    "end": "2438480"
  },
  {
    "text": "encourage you to take a look at the problem and try to implement in the most simple solution that that you can come up with so try to avoid premature",
    "start": "2438480",
    "end": "2445119"
  },
  {
    "text": "optimization as much as possible if it's an anti pattern generally then in Big Data this is like the root of all evil",
    "start": "2445119",
    "end": "2452160"
  },
  {
    "text": "uh and then the second takeaway is that evolution is an ongoing process so",
    "start": "2452160",
    "end": "2457359"
  },
  {
    "text": "whatever you're going to plan to design or Implement has to be uh plan in a way design in a way that makes it easy to",
    "start": "2457359",
    "end": "2464160"
  },
  {
    "text": "iterate over it so basically you create something then you will have to most likely just throw it away very shortly",
    "start": "2464160",
    "end": "2469760"
  },
  {
    "text": "right after and Implement something on top of it and you have to implement in a way that makes it easy to to swap it",
    "start": "2469760",
    "end": "2475200"
  },
  {
    "text": "basically so these are two kind of simple takeaways kind of kind of generic",
    "start": "2475200",
    "end": "2480640"
  },
  {
    "text": "a little bit too but if you kind of live up to them uh that will make your life much easier as a Hado administrator or",
    "start": "2480640",
    "end": "2487839"
  },
  {
    "text": "data engineer and that brings us to the last slide which is we are hiring and",
    "start": "2487839",
    "end": "2493800"
  },
  {
    "text": "you know we need engineers in uh New York Stockholm so if you want to join you can talk to us afterwards or you can",
    "start": "2493800",
    "end": "2500720"
  },
  {
    "text": "uh write to us on Twitter that's the end all right thank you very much I think we have time for",
    "start": "2500720",
    "end": "2509200"
  },
  {
    "text": "questions all right thanks guys uh so who want wants to",
    "start": "2513319",
    "end": "2518640"
  },
  {
    "text": "start this one because you're you happen to be close hi um how do we deal with Shima",
    "start": "2518640",
    "end": "2527079"
  },
  {
    "text": "changes you mentioned it that is is a problem with the",
    "start": "2527079",
    "end": "2533079"
  },
  {
    "text": "format uh how do we do with schema changes the question is how do we do with schema changes um okay uh so most",
    "start": "2533079",
    "end": "2540119"
  },
  {
    "text": "of it is actually implemented as a schema repository right now uh where um",
    "start": "2540119",
    "end": "2547240"
  },
  {
    "text": "we have a project called lock uh lock parser uh which basically defines what are the fields and then every time you",
    "start": "2547240",
    "end": "2554640"
  },
  {
    "text": "you make a change you can change that in that loog parser but you have to make sure that everyone knows about that",
    "start": "2554640",
    "end": "2560079"
  },
  {
    "text": "change and all the pipelines that are Downstream are up to date with that change um that not that doesn't really",
    "start": "2560079",
    "end": "2566359"
  },
  {
    "text": "work all the time and in the most recent um architecture we're working on the",
    "start": "2566359",
    "end": "2571880"
  },
  {
    "text": "more centralized schema repository where you would fetch the schema from uh from that reposit story and that would",
    "start": "2571880",
    "end": "2578599"
  },
  {
    "text": "basically be dependent on the data that you fetch so let's say that you want to process something that is from 2 years",
    "start": "2578599",
    "end": "2583680"
  },
  {
    "text": "ago then the schema will be that in that rep you can fetch but it's not in there in there it's not there yet and I can't",
    "start": "2583680",
    "end": "2589800"
  },
  {
    "text": "remember if we mentioned but we do use a pat AO uh so that helps out a",
    "start": "2589800",
    "end": "2596400"
  },
  {
    "text": "lot okay there's one over here uh you mentioned that the cluster",
    "start": "2596400",
    "end": "2604119"
  },
  {
    "text": "was not stable in the beginning what was the reason for that at before the Hadoop team was started",
    "start": "2604119",
    "end": "2611680"
  },
  {
    "text": "yeah yeah so we had a bunch of different issues um a lot of them were really low",
    "start": "2611680",
    "end": "2617160"
  },
  {
    "text": "hanging fruit like nobody was managing uh the data's growth uh so every once in",
    "start": "2617160",
    "end": "2622319"
  },
  {
    "text": "a while we we'd be expanding the cluster and we'd run out of disc space uh and there was really no it was always",
    "start": "2622319",
    "end": "2628240"
  },
  {
    "text": "reactive that was the problem uh so we'd run into 90% discs utilization which is",
    "start": "2628240",
    "end": "2633440"
  },
  {
    "text": "kind of the danger zone which because it means that individual discs are 100% % full or you'll have individual nodes",
    "start": "2633440",
    "end": "2639240"
  },
  {
    "text": "that are 100% full when it hasn't properly balanced and that'll cause a bunch of job failures and event it'll",
    "start": "2639240",
    "end": "2645800"
  },
  {
    "text": "just run into all kinds of crazy uh edge cases in Hadoop so that was one of the big reasons um there were also some",
    "start": "2645800",
    "end": "2653079"
  },
  {
    "text": "other really interesting issues that we ran into I i' I'd have to go back and look um but some bugs some",
    "start": "2653079",
    "end": "2660480"
  },
  {
    "text": "misconfigurations because Hadoop is not the best documented code there was even",
    "start": "2660480",
    "end": "2666720"
  },
  {
    "text": "some configuration parameters where uh it said this this parameter is milliseconds but it actually turned out",
    "start": "2666720",
    "end": "2673119"
  },
  {
    "text": "to be seconds and that like really I remember that was one massive issue um",
    "start": "2673119",
    "end": "2679119"
  },
  {
    "text": "yeah so it was Al all sorts of little small yeah it was kind of like a small like Josh is saying kind of small stuff",
    "start": "2679119",
    "end": "2685040"
  },
  {
    "text": "like there was no alerting so for example would run out of space on on the dis drives on name notes would be like",
    "start": "2685040",
    "end": "2691240"
  },
  {
    "text": "you know name will die uh and then puppet was not running so every time there was a change for example examp in",
    "start": "2691240",
    "end": "2697359"
  },
  {
    "text": "network that was kind of serious and it would not be propagated deployed on name node all of a sudden no one can connect",
    "start": "2697359",
    "end": "2702920"
  },
  {
    "text": "to name node which is also a problem um all like diss would fail like we would",
    "start": "2702920",
    "end": "2709079"
  },
  {
    "text": "not have a right setup on uh on name noes and another problem so kind of simple stuff that you can you can do",
    "start": "2709079",
    "end": "2715000"
  },
  {
    "text": "kind of long hang hanging fruits actually there were alerts there were just way way too many alerts and they",
    "start": "2715000",
    "end": "2720200"
  },
  {
    "text": "were totally useless I remember when I first started at Spotify you get your email starts like a a few weeks before",
    "start": "2720200",
    "end": "2726720"
  },
  {
    "text": "you actually your first day and I had like over 10,000 emails and like all of them were you know standby name no disc",
    "start": "2726720",
    "end": "2734160"
  },
  {
    "text": "is full or you know just we had the over alerting problem so so many alerts that it's absolutely useless um and they",
    "start": "2734160",
    "end": "2741040"
  },
  {
    "text": "weren't very specific like we made later and there was a there was a dead log in data Notes too and I remember",
    "start": "2741040",
    "end": "2747920"
  },
  {
    "text": "that we've made a change in the in the time that it takes for name note to Mark",
    "start": "2747920",
    "end": "2753240"
  },
  {
    "text": "the data node is dead and that one was the I think the the milliseconds seconds so in the way we wanted to make it",
    "start": "2753240",
    "end": "2759160"
  },
  {
    "text": "longer but we've made it shorter uh which made the whole thing",
    "start": "2759160",
    "end": "2764520"
  },
  {
    "text": "worse um yeah what else was there but there was",
    "start": "2764520",
    "end": "2770160"
  },
  {
    "text": "there were some bugs there were some misconfigurations there were Lo L low hanging",
    "start": "2770160",
    "end": "2776240"
  },
  {
    "text": "FRS okay some more",
    "start": "2777680",
    "end": "2781520"
  },
  {
    "text": "questions all right",
    "start": "2783000",
    "end": "2787200"
  },
  {
    "text": "how big is your pre-production cluster right now it's 12 nodes and",
    "start": "2793000",
    "end": "2799000"
  },
  {
    "text": "two we expanded it to I think around 40 nodes now",
    "start": "2799000",
    "end": "2804680"
  },
  {
    "text": "okay oh so we recently it was 12 noes for a long time but then we we recently",
    "start": "2804680",
    "end": "2809720"
  },
  {
    "text": "had to upgrade about was a month and a half ago or even a month ago uh so we had to bump up capacity cuz we were uh",
    "start": "2809720",
    "end": "2816599"
  },
  {
    "text": "12 nodes and 1,300 1,700 nodes um it was",
    "start": "2816599",
    "end": "2822960"
  },
  {
    "text": "hard to actually move data there to do pre-production tests uh so we had to bump up capacity I think to around 30 or",
    "start": "2822960",
    "end": "2829960"
  },
  {
    "text": "40 nodes but it's pretty elastic we might have have gone back down to 12 I don't",
    "start": "2829960",
    "end": "2838520"
  },
  {
    "text": "know okay more questions for the guys running the biggest Hadoop cluster in",
    "start": "2839520",
    "end": "2845040"
  },
  {
    "text": "Europe you're gonna you guys are just going to be kicking yourselves when you get home",
    "start": "2845040",
    "end": "2851160"
  },
  {
    "text": "and you realize that question you wanted to ask all right everyone's like it's it's",
    "start": "2851160",
    "end": "2857240"
  },
  {
    "text": "time it's time over there so the question is whether we use",
    "start": "2857240",
    "end": "2863680"
  },
  {
    "text": "physical nodes or virtual one so we have physical cluster in in London uh",
    "start": "2863680",
    "end": "2871359"
  },
  {
    "text": "yeah yep I we actually had a slide of the specs but we we decided to remove it",
    "start": "2871359",
    "end": "2876960"
  },
  {
    "text": "uh yeah but we we run Dell servers uh 1700 uh we have I think three different",
    "start": "2876960",
    "end": "2882680"
  },
  {
    "text": "generations of servers um our oldest ones are about three or four years our",
    "start": "2882680",
    "end": "2887720"
  },
  {
    "text": "newest ones we got I don't know three months ago delivered um they're pretty",
    "start": "2887720",
    "end": "2893359"
  },
  {
    "text": "beefy they run we had we have uh 12 discs per machine four terabytes of",
    "start": "2893359",
    "end": "2901880"
  },
  {
    "text": "disc yeah about 24 cores yeah depend yeah",
    "start": "2901880",
    "end": "2908760"
  },
  {
    "text": "okay more",
    "start": "2908960",
    "end": "2911400"
  },
  {
    "text": "questions about backups yeah so it's It's Kind of a",
    "start": "2914800",
    "end": "2921520"
  },
  {
    "text": "Funny Story so log ey cover we talked about before uh actually did our backups too so it would write to htfs would also",
    "start": "2921520",
    "end": "2928640"
  },
  {
    "text": "write to uh Amazon I think it was S3 uh so that if we needed to restore we could",
    "start": "2928640",
    "end": "2934760"
  },
  {
    "text": "just reprocess the data from raw you know if we ever actually had to do it though it would be a different story",
    "start": "2934760",
    "end": "2939799"
  },
  {
    "text": "because actually getting the data from S3 and and reprocessing it with old formats and stuff would be a nightmare",
    "start": "2939799",
    "end": "2947119"
  },
  {
    "text": "um yeah so when we killed it we had to come up with a new solution uh so what we do now is is um",
    "start": "2947119",
    "end": "2956240"
  },
  {
    "text": "we don't copy our raw data because we save like the recent history in inside Kafka there's some mechanic mechanisms",
    "start": "2956240",
    "end": "2963760"
  },
  {
    "text": "inside Kafka that allow you to uh persist data for a certain certain period of time um so we so all of our",
    "start": "2963760",
    "end": "2970079"
  },
  {
    "text": "core data sets uh we make backups of we we copy to the we encrypt them and we",
    "start": "2970079",
    "end": "2975160"
  },
  {
    "text": "copi them to the cloud uh so now if we want to restore them hopefully it'll it'll be a lot easier we can you know",
    "start": "2975160",
    "end": "2980599"
  },
  {
    "text": "find out what's been deleted just copy that data set and reprocess from there instead of going all the way back to the",
    "start": "2980599",
    "end": "2986359"
  },
  {
    "text": "raw logs we we also use uh snapshots yes",
    "start": "2986359",
    "end": "2991599"
  },
  {
    "text": "that's right which also is another like there's a cool feature of SN snapshots which basically means that you cannot",
    "start": "2991599",
    "end": "2998000"
  },
  {
    "text": "really delete snapshots easily which basically prevents you from deleting everything at least you will get you",
    "start": "2998000",
    "end": "3003640"
  },
  {
    "text": "know warning no error that you know you cannot delete everything because there's a snapshot there uh so that's also like",
    "start": "3003640",
    "end": "3009920"
  },
  {
    "text": "a cool feature of snapshot but we use snapshots too yeah funny little story",
    "start": "3009920",
    "end": "3015040"
  },
  {
    "text": "about snapshots we were so we we um we're at at shop we do kind of scrummage",
    "start": "3015040",
    "end": "3020240"
  },
  {
    "text": "style uh and we had a Sprint demo with a lot of our stakeholders in the room and we were talking about how we did snapshots and how it make it harder for",
    "start": "3020240",
    "end": "3027960"
  },
  {
    "text": "some a user to accidentally delete his entire team's directory uh so we implement this feature and you know we",
    "start": "3027960",
    "end": "3034040"
  },
  {
    "text": "had test it out in our pre-production environment it worked fine uh we decided that in our demo we were going to launch",
    "start": "3034040",
    "end": "3039599"
  },
  {
    "text": "it on the production environment but we we didn't had talk about was actually should we test it on a production",
    "start": "3039599",
    "end": "3045200"
  },
  {
    "text": "environment too so one of our Engineers kind of went a little rogue gido uh and he decided you know you know I've read",
    "start": "3045200",
    "end": "3052520"
  },
  {
    "text": "about it it's going to work uh so he's right there in front of all everybody body is like okay delete",
    "start": "3052520",
    "end": "3059520"
  },
  {
    "text": "root and there was about a second or two uh we were sitting there like oh oh no",
    "start": "3059520",
    "end": "3064960"
  },
  {
    "text": "oh no what's going to happen and luckily it said you know you can't do this because it's a snapshot so it was a successful demo",
    "start": "3064960",
    "end": "3072440"
  },
  {
    "text": "but probably a little bit unnecessary we could have done that on in pre-production just as easy all I think",
    "start": "3072440",
    "end": "3077920"
  },
  {
    "text": "we're glad that you didn't have to learn from failure there because then we wouldn't get to hear you today so all right everyone thank you please remember",
    "start": "3077920",
    "end": "3085319"
  },
  {
    "text": "uh to submit a rating and also remember if you have other questions or feedback to give these guys um you can put it in",
    "start": "3085319",
    "end": "3091200"
  },
  {
    "text": "the app and it will'll get to them all right thank you thanks everyone",
    "start": "3091200",
    "end": "3097119"
  }
]