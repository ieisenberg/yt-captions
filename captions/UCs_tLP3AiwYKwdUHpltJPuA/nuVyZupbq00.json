[
  {
    "start": "0",
    "end": "135000"
  },
  {
    "text": "[Music]",
    "start": "970",
    "end": "7838"
  },
  {
    "text": "robert crowe i'm a google developer engineer and i'm going to be talking today about",
    "start": "13440",
    "end": "19920"
  },
  {
    "text": "a journey a journey from experimentation to production machine learning um",
    "start": "19920",
    "end": "27039"
  },
  {
    "text": "well we start with the world uh fundamentally when we do machine learning we're trying to model",
    "start": "27039",
    "end": "33760"
  },
  {
    "text": "part of the world a process an event a goal a behavior",
    "start": "33760",
    "end": "41520"
  },
  {
    "text": "something we're trying to model something some part of the world",
    "start": "41520",
    "end": "47280"
  },
  {
    "text": "so we look for information things that will help our model understand the thing that we're modeling",
    "start": "49039",
    "end": "56000"
  },
  {
    "text": "things we can measure or that are already being measured we look",
    "start": "56000",
    "end": "61280"
  },
  {
    "text": "for data and this is the same thing that we as humans do when we're trying to",
    "start": "61280",
    "end": "68320"
  },
  {
    "text": "understand our world our model is no different except that it's not quite as smart as",
    "start": "68320",
    "end": "74880"
  },
  {
    "text": "we are in a general sense so as data scientists or machine",
    "start": "74880",
    "end": "81439"
  },
  {
    "text": "learning engineers we use our data to train our model",
    "start": "81439",
    "end": "86960"
  },
  {
    "text": "we start by experimenting typically that starts in a jupiter notebook or a similar tool often of course we",
    "start": "86960",
    "end": "94640"
  },
  {
    "text": "discover in the process that we need to change our data or look for better data but eventually",
    "start": "94640",
    "end": "101200"
  },
  {
    "text": "we hopefully get to a model that does a reasonably good job of modeling the thing that we're trying to",
    "start": "101200",
    "end": "108320"
  },
  {
    "text": "model and if we're in academia or in research",
    "start": "108320",
    "end": "114880"
  },
  {
    "text": "we turn in the assignment or publish the paper and we're done",
    "start": "114880",
    "end": "122079"
  },
  {
    "text": "but if we're creating our model to be part of a product or service that we're going to offer to",
    "start": "122719",
    "end": "127920"
  },
  {
    "text": "the world our journey has really only just begun",
    "start": "127920",
    "end": "134080"
  },
  {
    "text": "to put any piece of software into production to use it in a product or service",
    "start": "134879",
    "end": "142239"
  },
  {
    "start": "135000",
    "end": "341000"
  },
  {
    "text": "we need a production infrastructure and process",
    "start": "142239",
    "end": "148400"
  },
  {
    "text": "first we need all the things that any machine learning development needs along",
    "start": "148480",
    "end": "153599"
  },
  {
    "text": "with some special considerations because we're working in a production environment",
    "start": "153599",
    "end": "159599"
  },
  {
    "text": "so assuming we're doing supervised learning and most of the time we are we we're going to need labeled data we need",
    "start": "159599",
    "end": "167519"
  },
  {
    "text": "to make sure that that data covers the same feature space as the prediction requests that",
    "start": "167519",
    "end": "172800"
  },
  {
    "text": "our model will receive we need to minimize the dimensionality because",
    "start": "172800",
    "end": "179040"
  },
  {
    "text": "we want the simplest possible model that is the has the lowest cost to run and",
    "start": "179040",
    "end": "185760"
  },
  {
    "text": "requires the least cpu uh infrastructure to to",
    "start": "185760",
    "end": "191040"
  },
  {
    "text": "run that model but we want to maximize the predictive",
    "start": "191040",
    "end": "196080"
  },
  {
    "text": "information in that data and we need to consider things like",
    "start": "196080",
    "end": "202319"
  },
  {
    "text": "fairness and how we serve different parts of the people who are going to be using",
    "start": "202319",
    "end": "207360"
  },
  {
    "text": "our model different groups within our maybe our customers or our users",
    "start": "207360",
    "end": "212400"
  },
  {
    "text": "or our business process and in many domains we need to consider",
    "start": "212400",
    "end": "219200"
  },
  {
    "text": "rare conditions so especially in things like healthcare we we may have very rare conditions that",
    "start": "219200",
    "end": "226319"
  },
  {
    "text": "we want to make sure that we model well because they're important",
    "start": "226319",
    "end": "231840"
  },
  {
    "text": "and we need to think about our data life cycle so over the life of our application",
    "start": "231840",
    "end": "238959"
  },
  {
    "text": "we need to constantly collect new data and we'll talk more about that in a",
    "start": "238959",
    "end": "244799"
  },
  {
    "text": "little bit but along with all of that as if that weren't enough",
    "start": "244799",
    "end": "250879"
  },
  {
    "text": "we also need to do what we need to do anytime we put a piece of software into",
    "start": "250879",
    "end": "256239"
  },
  {
    "text": "production so we need to consider scalability can",
    "start": "256239",
    "end": "261680"
  },
  {
    "text": "can we scale our infrastructure as our needs go up or down",
    "start": "261680",
    "end": "268320"
  },
  {
    "text": "can we extend our our uh application and potentially our model",
    "start": "268320",
    "end": "275360"
  },
  {
    "text": "as our business needs change or our organization if we're doing something like a non-profit",
    "start": "275360",
    "end": "283040"
  },
  {
    "text": "can we control and and and uh change our our infrastructure and our",
    "start": "283040",
    "end": "289199"
  },
  {
    "text": "model through configuration or do we actually have to write code to do that",
    "start": "289199",
    "end": "294800"
  },
  {
    "text": "is our are our results consistent are they reproducible if we give it the same data do we get",
    "start": "294800",
    "end": "300960"
  },
  {
    "text": "the same predictions have we designed a modular system where",
    "start": "300960",
    "end": "306000"
  },
  {
    "text": "we can plug and unplug different parts of our system to improve or change things",
    "start": "306000",
    "end": "313520"
  },
  {
    "text": "and have we followed best practices we need to do all of that in",
    "start": "313520",
    "end": "320080"
  },
  {
    "text": "in a in a production machine learning environment which really means what we need is is to",
    "start": "320080",
    "end": "327600"
  },
  {
    "text": "apply devops principles to machine learning use cases",
    "start": "327600",
    "end": "339840"
  },
  {
    "start": "341000",
    "end": "381000"
  },
  {
    "text": "so we need ml ops there are various ways to define mlaps",
    "start": "341680",
    "end": "347199"
  },
  {
    "text": "uh but i think this one says it well emblops is a ml engineering culture and",
    "start": "347199",
    "end": "352479"
  },
  {
    "text": "practice that aims at unifying ml system development dev and",
    "start": "352479",
    "end": "357919"
  },
  {
    "text": "ml system operation ops practicing ml ops means that you advocate for automation",
    "start": "357919",
    "end": "364240"
  },
  {
    "text": "and monitoring at all steps of the ml system construction including integration testing releasing",
    "start": "364240",
    "end": "371120"
  },
  {
    "text": "deployment and infrastructure management and there's a link to a paper there that talks about that in some detail",
    "start": "371120",
    "end": "381440"
  },
  {
    "start": "381000",
    "end": "449000"
  },
  {
    "text": "but continuous integration is not the same as it would be in normal devops",
    "start": "381440",
    "end": "388160"
  },
  {
    "text": "testing and validating code uh and components is still part of",
    "start": "388160",
    "end": "394240"
  },
  {
    "text": "continuous integration but it also includes testing and validating data data schemas",
    "start": "394240",
    "end": "400160"
  },
  {
    "text": "and models continuous deployment is not just a",
    "start": "400160",
    "end": "406400"
  },
  {
    "text": "single piece of software or service but a whole system an ml training pipeline",
    "start": "406400",
    "end": "412000"
  },
  {
    "text": "we'll talk about ml pipelines more in a little bit and that needs to automatically deploy",
    "start": "412000",
    "end": "418319"
  },
  {
    "text": "another service the model prediction service so we have one service that is going to deploy another",
    "start": "418319",
    "end": "425680"
  },
  {
    "text": "service and we have continuous training too",
    "start": "425680",
    "end": "431039"
  },
  {
    "text": "which we don't have in devops continuous training is a new process that's unique to ml systems and",
    "start": "431039",
    "end": "437759"
  },
  {
    "text": "and it's concerned with automatically gathering and labeling new data retraining our",
    "start": "437759",
    "end": "444080"
  },
  {
    "text": "model and serving new models",
    "start": "444080",
    "end": "448720"
  },
  {
    "start": "449000",
    "end": "542000"
  },
  {
    "text": "so many teams have data scientists and ml researchers who can build state-of-the-art models",
    "start": "449360",
    "end": "455360"
  },
  {
    "text": "but their process for building and deploying ml models is often entirely manual as shown in the diagram on this",
    "start": "455360",
    "end": "463759"
  },
  {
    "text": "slide and we consider this to be a basic level of maturity or",
    "start": "463759",
    "end": "469120"
  },
  {
    "text": "level zero it's a manual script-driven and interactive process",
    "start": "469120",
    "end": "476319"
  },
  {
    "text": "every step is manual including data analysis and data preparation model training",
    "start": "476319",
    "end": "483199"
  },
  {
    "text": "and validation the process assumes that your data science team manages a",
    "start": "483199",
    "end": "488800"
  },
  {
    "text": "few models that don't change frequently either changing model implementation or",
    "start": "488800",
    "end": "495919"
  },
  {
    "text": "retraining the model with new data a new version is deployed maybe a couple",
    "start": "495919",
    "end": "501440"
  },
  {
    "text": "of times a year there's no ci cd and of course",
    "start": "501440",
    "end": "506560"
  },
  {
    "text": "no continuous training model performance is evaluated infrequently",
    "start": "506560",
    "end": "512240"
  },
  {
    "text": "and new training data is only gathered and labeled when the model is going to be retrained",
    "start": "512240",
    "end": "518080"
  },
  {
    "text": "and when models do need to be retrained it's a fairly heavy and expensive process",
    "start": "518080",
    "end": "524240"
  },
  {
    "text": "requiring ml engineers to manually gather data and retrain models",
    "start": "524240",
    "end": "531920"
  },
  {
    "text": "but why isn't one model good enough why can't we just train our models and",
    "start": "532000",
    "end": "537839"
  },
  {
    "text": "be done well let's let's do a little thought experiment",
    "start": "537839",
    "end": "543440"
  },
  {
    "start": "542000",
    "end": "731000"
  },
  {
    "text": "so imagine that you're an online retailer and you're using a model",
    "start": "543440",
    "end": "549040"
  },
  {
    "text": "of your click-through rates to help you decide how much inventory to order",
    "start": "549040",
    "end": "554720"
  },
  {
    "text": "okay that that seems like it's a reasonable idea but suddenly your auc and prediction",
    "start": "554720",
    "end": "561839"
  },
  {
    "text": "accuracy have dropped not on your whole inventory but on one part of your inventory men's",
    "start": "561839",
    "end": "569839"
  },
  {
    "text": "dress shoes well why why did why did our model suddenly",
    "start": "569839",
    "end": "576959"
  },
  {
    "text": "start doing badly and more importantly how do we even know",
    "start": "576959",
    "end": "583519"
  },
  {
    "text": "that we have a problem if all we have is our training data and our current click-through data how",
    "start": "583519",
    "end": "590800"
  },
  {
    "text": "do we know that our model is no longer predicting our inventory ordering well",
    "start": "590800",
    "end": "597120"
  },
  {
    "text": "well unfortunately we often find out the hard way we we either order way too much",
    "start": "597120",
    "end": "603760"
  },
  {
    "text": "inventory or not enough this is not the way that a",
    "start": "603760",
    "end": "609279"
  },
  {
    "text": "business wants to discover that it has a problem with its model so we need a process to make sure this",
    "start": "609279",
    "end": "617040"
  },
  {
    "text": "doesn't happen and an infrastructure to support it",
    "start": "617040",
    "end": "622079"
  },
  {
    "text": "but going back to the question why why why did our model suddenly start doing badly when it",
    "start": "622079",
    "end": "628880"
  },
  {
    "text": "worked fine before well the answer is change the world",
    "start": "628880",
    "end": "634640"
  },
  {
    "text": "changes our data changes our business changes if we're doing something like shoes",
    "start": "634640",
    "end": "641200"
  },
  {
    "text": "styles change we have we may have new competitors we may have different suppliers we",
    "start": "641200",
    "end": "649519"
  },
  {
    "text": "we may have it's a different season of the year things change and our model has to",
    "start": "649519",
    "end": "655680"
  },
  {
    "text": "change too so in many domains we need to constantly",
    "start": "655680",
    "end": "662000"
  },
  {
    "text": "collect and label new data and retrain our models",
    "start": "662000",
    "end": "667279"
  },
  {
    "text": "but these things aren't free and we only want to spend our limited resources when",
    "start": "667279",
    "end": "672560"
  },
  {
    "text": "we need to so how do we know when we need more data",
    "start": "672560",
    "end": "677600"
  },
  {
    "text": "or we need to retrain our model what do we need to build to make this happen",
    "start": "677600",
    "end": "684880"
  },
  {
    "text": "well as data scientists or machine learning engineers we tend to focus on our models and",
    "start": "685839",
    "end": "692720"
  },
  {
    "text": "that's natural because really the modeling is what makes machine learning",
    "start": "692720",
    "end": "698560"
  },
  {
    "text": "unique it's it's where the magic happens but when we move to production ml we",
    "start": "698560",
    "end": "706160"
  },
  {
    "text": "discover that there's a lot more that is required to be successful at google we've we've measured the ml",
    "start": "706160",
    "end": "713600"
  },
  {
    "text": "model code to be something like five percent of the total code",
    "start": "713600",
    "end": "718800"
  },
  {
    "text": "necessary to train and deploy a model in production and keep it running all the other tools",
    "start": "718800",
    "end": "726079"
  },
  {
    "text": "and infrastructure that you see here are critical and unfortunately many",
    "start": "726079",
    "end": "732160"
  },
  {
    "start": "731000",
    "end": "782000"
  },
  {
    "text": "people who are just starting out find this out the hard way this is kind of a famous tweet now where",
    "start": "732160",
    "end": "739760"
  },
  {
    "text": "they trained a model in three weeks and 11 months later it's still not deployed",
    "start": "739760",
    "end": "746000"
  },
  {
    "text": "unfortunately this is all too common this whole topic was written about over",
    "start": "746000",
    "end": "752399"
  },
  {
    "text": "five years ago now in a landmark paper called the hidden technical debt in machine learning systems and if you",
    "start": "752399",
    "end": "759440"
  },
  {
    "text": "haven't read it before i highly recommend it and there's a link there on the slide",
    "start": "759440",
    "end": "765440"
  },
  {
    "text": "okay so we have all these requirements both machine",
    "start": "765839",
    "end": "771760"
  },
  {
    "text": "learning requirements and infrastructure requirements and ml ops requirements how do we implement all",
    "start": "771760",
    "end": "779519"
  },
  {
    "text": "of this well i'm from google so i can tell you",
    "start": "779519",
    "end": "784720"
  },
  {
    "start": "782000",
    "end": "868000"
  },
  {
    "text": "about what we do we use ml everywhere in google in nearly every",
    "start": "784720",
    "end": "791360"
  },
  {
    "text": "product or service from google and for the vast majority of it we use",
    "start": "791360",
    "end": "797200"
  },
  {
    "text": "tensorflow extended or tfx so that's what i'm going to be talking",
    "start": "797200",
    "end": "803360"
  },
  {
    "text": "about today gfx is not the only infrastructure that you can use for production ml",
    "start": "803360",
    "end": "809279"
  },
  {
    "text": "but it's the one that i know the most about and and so i can explain to you how it works but",
    "start": "809279",
    "end": "816480"
  },
  {
    "text": "all production ml is going to be similar it's going to have the same needs and the same requirements so it's going",
    "start": "816480",
    "end": "823519"
  },
  {
    "text": "to have to fill those somehow and we use tfx and uh",
    "start": "823519",
    "end": "829519"
  },
  {
    "text": "it's used across uh uh really the the alphabet companies and you've",
    "start": "829519",
    "end": "835519"
  },
  {
    "text": "probably been using it or really using products or services that are implemented using tfx",
    "start": "835519",
    "end": "843839"
  },
  {
    "text": "and it's sorry and it's used by by our partners as well",
    "start": "843839",
    "end": "850639"
  },
  {
    "text": "so you probably recognize some names here",
    "start": "850639",
    "end": "855680"
  },
  {
    "text": "and there's a nice quote from twitter about the productivity gains that they've seen",
    "start": "855680",
    "end": "862000"
  },
  {
    "text": "since adopting tensorflow and tfx",
    "start": "862000",
    "end": "866800"
  },
  {
    "text": "so tfx implements ml pipelines which are the heart of a production ml deployment",
    "start": "867680",
    "end": "873040"
  },
  {
    "start": "868000",
    "end": "1003000"
  },
  {
    "text": "infrastructure ml pipelines operationalize the ml training",
    "start": "873040",
    "end": "878480"
  },
  {
    "text": "and serving processes and this is a conceptual view of a tfx pipeline at the top are the",
    "start": "878480",
    "end": "885839"
  },
  {
    "text": "tasks that we need to perform which probably look familiar they're the same",
    "start": "885839",
    "end": "891360"
  },
  {
    "text": "that you do whenever you're going to train a a model for ml",
    "start": "891360",
    "end": "897440"
  },
  {
    "text": "so it starts on the left with ingesting data and proceeds through training and serving the model",
    "start": "897440",
    "end": "903839"
  },
  {
    "text": "in the middle are open source libraries these this layer here",
    "start": "903839",
    "end": "910639"
  },
  {
    "text": "those are open source libraries that are used by tfx for particular parts of the process",
    "start": "910639",
    "end": "916480"
  },
  {
    "text": "like validating data and analyzing model performance in orange are tfx components which are",
    "start": "916480",
    "end": "924000"
  },
  {
    "text": "what you use to define the pipeline itself",
    "start": "924000",
    "end": "928560"
  },
  {
    "text": "and this is a different view of a tfx pipeline it's it's what i call the hello world of tfx",
    "start": "930480",
    "end": "936959"
  },
  {
    "text": "so it's a pipeline that you start with when you're just doing a pip install but then from here you can",
    "start": "936959",
    "end": "943839"
  },
  {
    "text": "add to your pipeline or even remove some of these components or replace them in orange",
    "start": "943839",
    "end": "951839"
  },
  {
    "text": "you see the components for a training pipeline and in green you see components for a",
    "start": "951839",
    "end": "958320"
  },
  {
    "text": "batch inference pipeline which is one way to serve the model that you train",
    "start": "958320",
    "end": "964720"
  },
  {
    "text": "the other ways to serve it are over here on the left where we have a repository like",
    "start": "964720",
    "end": "971360"
  },
  {
    "text": "tensorflow hub if you're going to use your model for something like transfer learning or generating embeddings",
    "start": "971360",
    "end": "977759"
  },
  {
    "text": "tensorflow lite or tensorflow js if you're going to use your model in a javascript maybe in a web browser",
    "start": "977759",
    "end": "985920"
  },
  {
    "text": "or a mobile application or iot device or tensorflow serving if you're going to",
    "start": "985920",
    "end": "991920"
  },
  {
    "text": "serve your model uh online or or in a cluster",
    "start": "991920",
    "end": "997040"
  },
  {
    "text": "or wherever you can serve it a model really",
    "start": "997040",
    "end": "1002560"
  },
  {
    "start": "1003000",
    "end": "1100000"
  },
  {
    "text": "so i've mentioned components a couple of times let's take a look at what exactly is a component there are three parts",
    "start": "1003519",
    "end": "1012320"
  },
  {
    "text": "there's a driver an executor and a publisher the driver receives the input",
    "start": "1012320",
    "end": "1020000"
  },
  {
    "text": "for the component and supplies that to the executor where it really does the work of the",
    "start": "1020000",
    "end": "1025760"
  },
  {
    "text": "component that's what makes different components unique and then the publisher takes the results",
    "start": "1025760",
    "end": "1031918"
  },
  {
    "text": "of the executor and puts it into metadata which we'll talk about in a second",
    "start": "1031919",
    "end": "1039839"
  },
  {
    "text": "so this is a different view of it there's a configuration for each component there's the component",
    "start": "1040160",
    "end": "1046079"
  },
  {
    "text": "itself and it's getting its input from the metadata store",
    "start": "1046079",
    "end": "1051520"
  },
  {
    "text": "which we'll talk about some more but what it's getting are artifacts and it gets those over channels so it",
    "start": "1051520",
    "end": "1058640"
  },
  {
    "text": "gets its input over input channels it takes its results",
    "start": "1058640",
    "end": "1063919"
  },
  {
    "text": "uses an output channel to put it back into metadata",
    "start": "1063919",
    "end": "1069760"
  },
  {
    "text": "so that's how different components communicate with each other you have an upstream component that",
    "start": "1069760",
    "end": "1076080"
  },
  {
    "text": "generates a result as an artifact that's put into metadata downstream components",
    "start": "1076080",
    "end": "1083280"
  },
  {
    "text": "that depend on those artifacts then pull those uh from metadata and use them and",
    "start": "1083280",
    "end": "1089840"
  },
  {
    "text": "generate their results and put them into metadata and then it flows downstream to",
    "start": "1089840",
    "end": "1097200"
  },
  {
    "text": "the next component so what that means is we need orchestration we need to sequence",
    "start": "1097200",
    "end": "1104400"
  },
  {
    "start": "1100000",
    "end": "1156000"
  },
  {
    "text": "and synchronize our tasks and in tfx there's different orchestrators that are",
    "start": "1104400",
    "end": "1110960"
  },
  {
    "text": "available and you can add your own just out of the box it comes with",
    "start": "1110960",
    "end": "1117520"
  },
  {
    "text": "support for airflow cube flow and interactive orchestrator that we'll",
    "start": "1117520",
    "end": "1122720"
  },
  {
    "text": "talk about a little bit or a local orchestrator you can run all of this just on your laptop",
    "start": "1122720",
    "end": "1130399"
  },
  {
    "text": "but regardless of which orchestrator you use the the pipeline itself is going to be",
    "start": "1131679",
    "end": "1137520"
  },
  {
    "text": "the same it's just going to look differently and you're going to have a different ui for it but the the",
    "start": "1137520",
    "end": "1144720"
  },
  {
    "text": "directed acyclic graph or dag that that forms a pipeline is going to be the",
    "start": "1144720",
    "end": "1150880"
  },
  {
    "text": "same just in a different tool",
    "start": "1150880",
    "end": "1155519"
  },
  {
    "start": "1156000",
    "end": "1380000"
  },
  {
    "text": "one of the questions that we sometimes get is what is the difference between tfx and cube flow pipelines",
    "start": "1156559",
    "end": "1164480"
  },
  {
    "text": "and the difference is really that they're they they work together tfx can run in a",
    "start": "1164480",
    "end": "1170880"
  },
  {
    "text": "number of different environments including uh on your laptop or",
    "start": "1170880",
    "end": "1176320"
  },
  {
    "text": "on a server or what have you it can also run in a containerized environment using",
    "start": "1176320",
    "end": "1182960"
  },
  {
    "text": "kubeflow pipelines and kubeflow pipelines itself forms the basis",
    "start": "1182960",
    "end": "1188320"
  },
  {
    "text": "of uh an offering from google cloud ai platform pipelines",
    "start": "1188320",
    "end": "1193440"
  },
  {
    "text": "but you can also run uh keyflow pipelines just on your own system so it's also an open source uh framework",
    "start": "1193440",
    "end": "1200720"
  },
  {
    "text": "that's available and it's it's used a lot as kubernetes is used a lot",
    "start": "1200720",
    "end": "1207520"
  },
  {
    "text": "and tfx runs on top of it so the bottom line here is that when you use tfx",
    "start": "1207520",
    "end": "1213760"
  },
  {
    "text": "and qfl pipelines together you have the best of both worlds and if",
    "start": "1213760",
    "end": "1219600"
  },
  {
    "text": "you're running in a containerized environment it's it's a good option to have",
    "start": "1219600",
    "end": "1226080"
  },
  {
    "text": "we also have orchestration inside a notebook so this is uh for doing experiments",
    "start": "1227200",
    "end": "1234320"
  },
  {
    "text": "or for doing iterative development you you build a pipeline in a in a notebook if you're using colab",
    "start": "1234320",
    "end": "1240880"
  },
  {
    "text": "you don't even have to install anything on your your laptop it just runs",
    "start": "1240880",
    "end": "1246080"
  },
  {
    "text": "it all in a web browser there's an interactive context that maintains the state the context",
    "start": "1246080",
    "end": "1253679"
  },
  {
    "text": "so it maintains it it does it handles metadata management and it helps you visualize the artifacts",
    "start": "1253679",
    "end": "1261360"
  },
  {
    "text": "and once you've developed your pipeline then you you're going to move it to some other environment to run it in",
    "start": "1261360",
    "end": "1268159"
  },
  {
    "text": "production this is really only for development and experimentation",
    "start": "1268159",
    "end": "1273919"
  },
  {
    "text": "so i've mentioned metadata a few times now let's let's look at what that is",
    "start": "1274480",
    "end": "1281039"
  },
  {
    "text": "as you run through the steps of an ml training process",
    "start": "1281679",
    "end": "1286720"
  },
  {
    "text": "so ingesting data and calculating statistics and doing feature engineering and",
    "start": "1286720",
    "end": "1293520"
  },
  {
    "text": "training and measuring your metrics and so forth you generate new data objects which are",
    "start": "1293520",
    "end": "1300559"
  },
  {
    "text": "known as metadata artifacts so for tfx we store our metadata artifacts in a",
    "start": "1300559",
    "end": "1308000"
  },
  {
    "text": "metadata store and so trained models for example are",
    "start": "1308000",
    "end": "1313360"
  },
  {
    "text": "metadata artifacts and they have properties they have a type for example which in this case is trained",
    "start": "1313360",
    "end": "1319120"
  },
  {
    "text": "model we also grouped those artifacts by the execution run that passed through",
    "start": "1319120",
    "end": "1326320"
  },
  {
    "text": "the pipeline that generated each of those artifacts and that helps us",
    "start": "1326320",
    "end": "1331760"
  },
  {
    "text": "later when we're trying to understand and analyze how the training went and and understand",
    "start": "1331760",
    "end": "1337840"
  },
  {
    "text": "the artifacts that were generated so for example it allows us to trace",
    "start": "1337840",
    "end": "1343679"
  },
  {
    "text": "back through our pipeline and understand the sequence of operations that generated something so",
    "start": "1343679",
    "end": "1350720"
  },
  {
    "text": "for example if we're looking at a metric and we're trying to understand why it's different",
    "start": "1350720",
    "end": "1356080"
  },
  {
    "text": "than the metric that we had before we can trace that back to the model that generated that metric",
    "start": "1356080",
    "end": "1362799"
  },
  {
    "text": "and then from there we can trace that back to the data that we use to train that model and from there to",
    "start": "1362799",
    "end": "1370000"
  },
  {
    "text": "the statistics on that data so it helps us understand and monitor and analyze the process",
    "start": "1370000",
    "end": "1378480"
  },
  {
    "text": "so to do a lot of this it can potentially require a lot of",
    "start": "1379200",
    "end": "1385520"
  },
  {
    "start": "1380000",
    "end": "1528000"
  },
  {
    "text": "compute resources especially when you're working with large models or large data sets",
    "start": "1385520",
    "end": "1392240"
  },
  {
    "text": "and to make that manageable we use a distributed pipeline to run",
    "start": "1392240",
    "end": "1399919"
  },
  {
    "text": "many of these processes on a compute cluster and the pipeline that we use is called",
    "start": "1399919",
    "end": "1405120"
  },
  {
    "text": "apache beam which is an open source framework for doing distributed processing",
    "start": "1405120",
    "end": "1413039"
  },
  {
    "text": "apache beam is a unified batch and stream distributed processing api it's a layer",
    "start": "1414320",
    "end": "1420400"
  },
  {
    "text": "on top of other distributed processing platforms like apache spark or flank",
    "start": "1420400",
    "end": "1428080"
  },
  {
    "text": "or samsa or google cloud data flow or what's called a direct runner if",
    "start": "1428080",
    "end": "1434159"
  },
  {
    "text": "you're just running it all on your laptop",
    "start": "1434159",
    "end": "1438639"
  },
  {
    "text": "so the way this works is we have a number of different uh underlying clusters or",
    "start": "1439600",
    "end": "1446480"
  },
  {
    "text": "frameworks that we can run on like spark or cloud data flow and beam has",
    "start": "1446480",
    "end": "1453279"
  },
  {
    "text": "a number of different sdks in different languages which we can use and for tfx we're using",
    "start": "1453279",
    "end": "1459520"
  },
  {
    "text": "python beam sits in the middle of that and it",
    "start": "1459520",
    "end": "1465200"
  },
  {
    "text": "takes operations and data using the the beam",
    "start": "1465200",
    "end": "1470840"
  },
  {
    "text": "sdk it translates that into the native sdk for whatever framework you're",
    "start": "1470840",
    "end": "1476720"
  },
  {
    "text": "running on and then a result is generated",
    "start": "1476720",
    "end": "1482080"
  },
  {
    "text": "or looking at it a little differently we have our component and remember we talked about the executor where the work is done",
    "start": "1482080",
    "end": "1489360"
  },
  {
    "text": "that could be running on a full operating system or in a docker container",
    "start": "1489360",
    "end": "1495120"
  },
  {
    "text": "we have beam we create a beam pipeline and send that off",
    "start": "1495120",
    "end": "1501840"
  },
  {
    "text": "that gets translated to the native sdk for whatever processing cluster",
    "start": "1501840",
    "end": "1507840"
  },
  {
    "text": "we're using it generates a result and that's translated back",
    "start": "1507840",
    "end": "1513360"
  },
  {
    "text": "to the beam sdk and delivered back to our executor so",
    "start": "1513360",
    "end": "1519440"
  },
  {
    "text": "we're able to run those that processing on our cluster and take advantage of those compute",
    "start": "1519440",
    "end": "1525680"
  },
  {
    "text": "[Music] resources okay you might remember a little while",
    "start": "1525680",
    "end": "1531039"
  },
  {
    "start": "1528000",
    "end": "1553000"
  },
  {
    "text": "ago we talked about the hello world pipeline that that the components in that are",
    "start": "1531039",
    "end": "1536159"
  },
  {
    "text": "referred to as the standard components and it looked like this so i'm going to go through these uh",
    "start": "1536159",
    "end": "1542880"
  },
  {
    "text": "fairly quickly just so that you have a feel for it but you'll notice that this mirrors a typical ml development process",
    "start": "1542880",
    "end": "1552640"
  },
  {
    "text": "so we start with ingesting our data right that's what we always start with and for for tfx we use a component",
    "start": "1552640",
    "end": "1559919"
  },
  {
    "start": "1553000",
    "end": "1697000"
  },
  {
    "text": "called examplegen to ingest our data and it actually uses apache beam",
    "start": "1559919",
    "end": "1565200"
  },
  {
    "text": "to split that data and and it's going to ingest different data formats as well so there's a",
    "start": "1565200",
    "end": "1572080"
  },
  {
    "text": "a long fairly long list of different formats that are available here and you can see the configuration is",
    "start": "1572080",
    "end": "1578320"
  },
  {
    "text": "very simple this is for csv it would be different for for different um",
    "start": "1578320",
    "end": "1584080"
  },
  {
    "text": "data formats or sources of data one of the things it does uh well it",
    "start": "1584080",
    "end": "1590720"
  },
  {
    "text": "splits our data into however many splits we need usually at least training and evaluation it also",
    "start": "1590720",
    "end": "1598400"
  },
  {
    "text": "splits it into spans so that we can can",
    "start": "1598400",
    "end": "1605360"
  },
  {
    "text": "if we have a large amount of data we can process it as individual spans across the data set and this helps us",
    "start": "1605360",
    "end": "1613840"
  },
  {
    "text": "with you know large amounts of data that we wouldn't want to process the whole thing at once it would be difficult to do that",
    "start": "1613840",
    "end": "1622399"
  },
  {
    "text": "the next component is statistics gen and it uses a library called tensorflow data",
    "start": "1623120",
    "end": "1629520"
  },
  {
    "text": "validation to calculate descriptive statistics for our data set",
    "start": "1629520",
    "end": "1635520"
  },
  {
    "text": "also using beam for processing so it's things like the median and mean and",
    "start": "1635520",
    "end": "1640640"
  },
  {
    "text": "and you know standard deviation the normal things that we use to to examine and understand our data",
    "start": "1640640",
    "end": "1647919"
  },
  {
    "text": "and there's a visualization tool that comes with it that we can use in a notebook to understand our",
    "start": "1647919",
    "end": "1654559"
  },
  {
    "text": "different features so this is looking at one particular feature and we can see we might need some more",
    "start": "1654559",
    "end": "1660240"
  },
  {
    "text": "data here at six in the morning the next component is schema gen which",
    "start": "1660240",
    "end": "1667440"
  },
  {
    "text": "tries to infer the types of our features now",
    "start": "1667440",
    "end": "1672640"
  },
  {
    "text": "it went too far it does it it makes the best effort to",
    "start": "1672640",
    "end": "1679520"
  },
  {
    "text": "do that but you may be smarter you may know more than schema gen does about your",
    "start": "1679520",
    "end": "1684960"
  },
  {
    "text": "domain and your data so you you want to take a look at the schema that it generates",
    "start": "1684960",
    "end": "1690880"
  },
  {
    "text": "and you know potentially make adjustments to curate that schema",
    "start": "1690880",
    "end": "1697520"
  },
  {
    "start": "1697000",
    "end": "1905000"
  },
  {
    "text": "example validator takes the statistics and the schema and it looks for problems",
    "start": "1697520",
    "end": "1702799"
  },
  {
    "text": "so it looks for things like examples where you have the wrong data type for for a feature or",
    "start": "1702799",
    "end": "1710640"
  },
  {
    "text": "the wrong category for a categorical feature things like that",
    "start": "1710640",
    "end": "1716720"
  },
  {
    "text": "transform is where the feature engineering is done and the code for transform is really",
    "start": "1717039",
    "end": "1722799"
  },
  {
    "text": "going to depend on the kinds of feature engineering that you need to do",
    "start": "1722799",
    "end": "1728240"
  },
  {
    "text": "it's going to use the schema and and the statistics that were generated but",
    "start": "1728240",
    "end": "1735200"
  },
  {
    "text": "you're going to be writing code here to do your feature engineering one of the important things that",
    "start": "1735200",
    "end": "1741279"
  },
  {
    "text": "transform does is it takes the transformations that you give it and it creates a tensorflow graph from",
    "start": "1741279",
    "end": "1748799"
  },
  {
    "text": "that from those transformations so that graph is then prepended to your",
    "start": "1748799",
    "end": "1756320"
  },
  {
    "text": "data when you train your model it's prepended to your model rather",
    "start": "1756320",
    "end": "1762240"
  },
  {
    "text": "so that you're using that graph those transformations to do the feature engineering",
    "start": "1762240",
    "end": "1768159"
  },
  {
    "text": "for the data that you supply to the model and that gets bundled with the model",
    "start": "1768159",
    "end": "1773679"
  },
  {
    "text": "when it's deployed to serving so you have exactly the same transformations",
    "start": "1773679",
    "end": "1778880"
  },
  {
    "text": "and it eliminates the potential for a training serving skew the difference",
    "start": "1778880",
    "end": "1785039"
  },
  {
    "text": "have having different transformations because of different code paths between training and serving",
    "start": "1785039",
    "end": "1792399"
  },
  {
    "text": "the next component is trainer and trainer does what you imagine it trains a model",
    "start": "1792399",
    "end": "1799440"
  },
  {
    "text": "so when it does it it's going to use your model code and it's going to save the results as a",
    "start": "1799440",
    "end": "1805840"
  },
  {
    "text": "saved model that gets then used when you deploy your",
    "start": "1805840",
    "end": "1812320"
  },
  {
    "text": "model and it also gets used for analyzing the performance of your model using tensorflow model",
    "start": "1812320",
    "end": "1818640"
  },
  {
    "text": "analysis the the configuration for",
    "start": "1818640",
    "end": "1824399"
  },
  {
    "text": "your model is about what you think it's it's things like the numbers number of steps and uh you need to give",
    "start": "1824399",
    "end": "1831440"
  },
  {
    "text": "it the schema that you generated and the data that you've you've uh you've",
    "start": "1831440",
    "end": "1837120"
  },
  {
    "text": "created your data set you also need to give it your model",
    "start": "1837120",
    "end": "1842720"
  },
  {
    "text": "so your model could be a tensorflow model using either the keras or estimator api",
    "start": "1842720",
    "end": "1849279"
  },
  {
    "text": "or just you know low level tensorflow if you want to do that it could be a scikit-learn model if",
    "start": "1849279",
    "end": "1855360"
  },
  {
    "text": "you're training on claudia platform you could use xgboost or pi torch",
    "start": "1855360",
    "end": "1860720"
  },
  {
    "text": "or actually we have experimental support for that just in native tfx as",
    "start": "1860720",
    "end": "1866880"
  },
  {
    "text": "well there are advantages to using tensorflow some of the some of the tools and",
    "start": "1866880",
    "end": "1872399"
  },
  {
    "text": "libraries work better with tensorflow but it's really up to you you can train models",
    "start": "1872399",
    "end": "1879039"
  },
  {
    "text": "using any of those frameworks um one of the advantages in using",
    "start": "1879039",
    "end": "1885200"
  },
  {
    "text": "tensorflow is the ability to use tensorboard which is a great tool to understand",
    "start": "1885200",
    "end": "1890480"
  },
  {
    "text": "your training process that includes comparing different training runs so you may have",
    "start": "1890480",
    "end": "1897679"
  },
  {
    "text": "trained a model uh say last month and you want to compare that to the model that you're",
    "start": "1897679",
    "end": "1902880"
  },
  {
    "text": "training now tuner uses the keras tuning library",
    "start": "1902880",
    "end": "1909919"
  },
  {
    "start": "1905000",
    "end": "1971000"
  },
  {
    "text": "to train your to tune your hyper parameters for your model",
    "start": "1909919",
    "end": "1916158"
  },
  {
    "text": "evaluator does deep analysis of your model performance",
    "start": "1916559",
    "end": "1921679"
  },
  {
    "text": "so what i mean by deep analysis is it's it's going to do analysis of your your top uh level",
    "start": "1921679",
    "end": "1929840"
  },
  {
    "text": "uh metrics but it's also going to look at slices of data that you define to",
    "start": "1929840",
    "end": "1935760"
  },
  {
    "text": "understand performance for different parts of your data so going back to our",
    "start": "1935760",
    "end": "1941120"
  },
  {
    "text": "example of shoes you would slice out men's dress shoes to understand the",
    "start": "1941120",
    "end": "1946240"
  },
  {
    "text": "performance for that particular part of your inventory",
    "start": "1946240",
    "end": "1951440"
  },
  {
    "text": "and there's a visualization tool that helps you understand that performance",
    "start": "1951440",
    "end": "1957679"
  },
  {
    "text": "infra validator is used to make sure that you can actually run your model on the infrastructure that you have",
    "start": "1957840",
    "end": "1965279"
  },
  {
    "text": "which is important because you want to make sure that it'll run before you deploy it",
    "start": "1965279",
    "end": "1970960"
  },
  {
    "start": "1971000",
    "end": "2017000"
  },
  {
    "text": "if both infra validator and evaluator decide that your model is ready",
    "start": "1971039",
    "end": "1976399"
  },
  {
    "text": "pusher will push it to whatever deployment target that you're using with your model",
    "start": "1976399",
    "end": "1983279"
  },
  {
    "text": "so it could be a mobile application or a web browser or a serving cluster or a cloud",
    "start": "1983279",
    "end": "1993200"
  },
  {
    "text": "bulk infer which is one of the components in green in the in the diagram that we had before",
    "start": "1993919",
    "end": "2000480"
  },
  {
    "text": "is used for doing batch inference and it uses again apache beam to do that so you can uh",
    "start": "2000480",
    "end": "2008559"
  },
  {
    "text": "distribute uh that that that batch inference across your compute cluster",
    "start": "2008559",
    "end": "2016559"
  },
  {
    "start": "2017000",
    "end": "2083000"
  },
  {
    "text": "gfx also has pipeline nodes which are not components",
    "start": "2017279",
    "end": "2022320"
  },
  {
    "text": "so there are special purpose classes for performing advanced metadata operations things like",
    "start": "2022320",
    "end": "2028240"
  },
  {
    "text": "importing external artifacts in into metadata or doing queries of metadata",
    "start": "2028240",
    "end": "2034720"
  },
  {
    "text": "so importer node is one and it's used for just that for importing external",
    "start": "2034720",
    "end": "2041120"
  },
  {
    "text": "data or artifacts into your metadata store to use them with your pipeline resolver node is used",
    "start": "2041120",
    "end": "2049040"
  },
  {
    "text": "to query the metadata that you have so a typical case for this is",
    "start": "2049040",
    "end": "2054398"
  },
  {
    "text": "something that evaluator does it it looks at the latest model and pulls it from metadata so that it",
    "start": "2054399",
    "end": "2060878"
  },
  {
    "text": "can compare it to the model that you just trained there's really two types of resolver",
    "start": "2060879",
    "end": "2066480"
  },
  {
    "text": "nodes the the latest artifacts resolver that looks at whatever kinds of artifacts that you're you're interested",
    "start": "2066480",
    "end": "2073118"
  },
  {
    "text": "in and pulls the latest end and the blessed model resolver that pulls the latest",
    "start": "2073119",
    "end": "2078398"
  },
  {
    "text": "blessed model or the model that's in production now there are also custom components so you",
    "start": "2078399",
    "end": "2086320"
  },
  {
    "start": "2083000",
    "end": "2169000"
  },
  {
    "text": "can create really three ways there's to create custom components you can use a python function with a",
    "start": "2086320",
    "end": "2093280"
  },
  {
    "text": "decorator we'll look at that in a second you can run components create components using containers",
    "start": "2093280",
    "end": "2099760"
  },
  {
    "text": "or you can just extend existing component classes using normal python",
    "start": "2099760",
    "end": "2105839"
  },
  {
    "text": "at the end of the day when you're when you create your component you use it just like any other component",
    "start": "2105839",
    "end": "2111440"
  },
  {
    "text": "in a tfx pipeline so a python based component",
    "start": "2111440",
    "end": "2116800"
  },
  {
    "text": "looks something like this you have a decorator this at component at the top here",
    "start": "2116800",
    "end": "2122160"
  },
  {
    "text": "you have some annotations on your parameters and your output there's an input and output artifact and",
    "start": "2122160",
    "end": "2129280"
  },
  {
    "text": "you can have an arbitrary number of parameters and then there's an output",
    "start": "2129280",
    "end": "2135040"
  },
  {
    "text": "a container based component is very similar except that it's wrapped in this call",
    "start": "2136000",
    "end": "2141119"
  },
  {
    "text": "here to create container component but i use these these artifacts to",
    "start": "2141119",
    "end": "2148560"
  },
  {
    "text": "declare the inputs i also need to give it the the container image to start from and a",
    "start": "2148560",
    "end": "2155040"
  },
  {
    "text": "command to create that container",
    "start": "2155040",
    "end": "2159839"
  },
  {
    "text": "we also run on the cloud in in google although you can also run on other clouds as well",
    "start": "2160160",
    "end": "2166320"
  },
  {
    "text": "using cloud ai platform pipelines this is a very high level look at the",
    "start": "2166320",
    "end": "2171520"
  },
  {
    "start": "2169000",
    "end": "2223000"
  },
  {
    "text": "architecture of it it's it's using tensorflow uh and that's being used by tensorflow",
    "start": "2171520",
    "end": "2177440"
  },
  {
    "text": "extended or tfx that's running on kubeflow pipelines like we talked about earlier that's running on a kubernetes",
    "start": "2177440",
    "end": "2185119"
  },
  {
    "text": "engine in the google cloud it's google google kubernetes engine which is using all the other cloud",
    "start": "2185119",
    "end": "2190800"
  },
  {
    "text": "services like like bigquery and data flow and what have you",
    "start": "2190800",
    "end": "2196559"
  },
  {
    "text": "so that's tfx there are standard components that you start with you can apply flexible",
    "start": "2196800",
    "end": "2203119"
  },
  {
    "text": "orchestration using one of the orchestrators that we supply there's or your own orchestrator there's",
    "start": "2203119",
    "end": "2210160"
  },
  {
    "text": "there's metadata that helps you understand your processes and you can extend it which you often",
    "start": "2210160",
    "end": "2217760"
  },
  {
    "text": "almost always will really with with custom components",
    "start": "2217760",
    "end": "2222880"
  },
  {
    "start": "2223000",
    "end": "2300000"
  },
  {
    "text": "so that's our production ml journey we've looked at our world we found ways",
    "start": "2224240",
    "end": "2231119"
  },
  {
    "text": "to measure and collect data to understand it and we've modeled it to give us a product or",
    "start": "2231119",
    "end": "2237599"
  },
  {
    "text": "service that would otherwise be impossible to create in pure software alone and if you're",
    "start": "2237599",
    "end": "2244320"
  },
  {
    "text": "worried about the robot apocalypse don't be the machine hasn't really learned anything",
    "start": "2244320",
    "end": "2250880"
  },
  {
    "text": "we have one of the things we've learned is create tools that we can use to",
    "start": "2250880",
    "end": "2256960"
  },
  {
    "text": "create things that we value products and services that we want",
    "start": "2256960",
    "end": "2262720"
  },
  {
    "text": "that includes tools like tfx which enable us to take our experiments",
    "start": "2262720",
    "end": "2268480"
  },
  {
    "text": "and use ml to create robust sustainable products and services and",
    "start": "2268480",
    "end": "2274800"
  },
  {
    "text": "offer them to the world here's our website if you haven't seen it before tensorflow.org",
    "start": "2274800",
    "end": "2280720"
  },
  {
    "text": "tfx and here's some more links to help you get started",
    "start": "2280720",
    "end": "2299599"
  },
  {
    "text": "you",
    "start": "2299599",
    "end": "2301680"
  }
]