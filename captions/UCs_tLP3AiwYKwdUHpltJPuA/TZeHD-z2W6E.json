[
  {
    "text": "so hello good afternoon I'm Brian McAllister of the special guest and",
    "start": "0",
    "end": "7500"
  },
  {
    "text": "today we want to talk about processing very large event streams at groupon is",
    "start": "7500",
    "end": "12990"
  },
  {
    "text": "the mic coming through clearly if I hold it about this distance good so there is",
    "start": "12990",
    "end": "18949"
  },
  {
    "text": "myself to co-workers AJ and Eric weathers who are actually have all the",
    "start": "18949",
    "end": "25710"
  },
  {
    "text": "fun meaty parts of the presentation I get to do the setup and sort of talk",
    "start": "25710",
    "end": "31019"
  },
  {
    "text": "about the problem because i'm going to start coughing in about ten minutes so at groupon we have a whole lot of events",
    "start": "31019",
    "end": "37940"
  },
  {
    "text": "that we process be they web logs mobile logs application level events sort of",
    "start": "37940",
    "end": "47219"
  },
  {
    "text": "operational metric type events you know and if you just talk about ones feeding",
    "start": "47219",
    "end": "52320"
  },
  {
    "text": "into the bulk of our storm topologies which is going to center around a lot you know it's on the order of you know",
    "start": "52320",
    "end": "59820"
  },
  {
    "text": "100 to 200 thousand events per second sort of nominally throughout the",
    "start": "59820",
    "end": "65250"
  },
  {
    "text": "interesting part of the day which isn't gargantuan but it's big enough that you",
    "start": "65250",
    "end": "70979"
  },
  {
    "text": "have to start doing some interesting things particularly when you couple it with dozens of different ways of",
    "start": "70979",
    "end": "76320"
  },
  {
    "text": "processing this that you want to do in an sometimes creating new streams out of those streams to feed back in and so",
    "start": "76320",
    "end": "82560"
  },
  {
    "text": "this talk is going to focus on what we have learned in doing this and what we",
    "start": "82560",
    "end": "87570"
  },
  {
    "text": "have done to make it work well sort of developmentally and operationally and so",
    "start": "87570",
    "end": "92759"
  },
  {
    "text": "the starting point for almost all of our stream processing is storm which came",
    "start": "92759",
    "end": "99630"
  },
  {
    "text": "out of back type and Twitter few years ago I don't want to go too deeply into",
    "start": "99630",
    "end": "104850"
  },
  {
    "text": "hail storm work so I'm going to do a quick poll who here is comfortable with the basic abstractions of storm that's",
    "start": "104850",
    "end": "113970"
  },
  {
    "text": "not as big a cause okay so I'm going to go into them a little bit more so storm",
    "start": "113970",
    "end": "120259"
  },
  {
    "text": "works on what it calls a topology and it's apology is made up of spouts which",
    "start": "120259",
    "end": "128340"
  },
  {
    "text": "send events into the system you know so in our case the starting source of",
    "start": "128340",
    "end": "133830"
  },
  {
    "text": "is almost always Kafka so there's something called a Kafka spout that reads out of actually hold on quick show of hands who does not know Kafka does",
    "start": "133830",
    "end": "141570"
  },
  {
    "text": "not know kafka kafka is a high-throughput a pub sub system that",
    "start": "141570",
    "end": "147990"
  },
  {
    "text": "works pretty well and you can just think about it that way it's a high-throughput pub/sub that works pretty well as",
    "start": "147990",
    "end": "153750"
  },
  {
    "text": "compared to you high-throughput pub/sub that doesn't work well which is the bulk of them including the ones I've written",
    "start": "153750",
    "end": "160460"
  },
  {
    "text": "so spouts feed things into storm you then have processors that an event feeds",
    "start": "160460",
    "end": "167400"
  },
  {
    "text": "in as what storm calls a tuple this is probably because it started its life enclosure and closure hates types so",
    "start": "167400",
    "end": "174270"
  },
  {
    "text": "everything is a tuple although it gets complicated now that most people write their stuff in Java or even Ruby um",
    "start": "174270",
    "end": "180290"
  },
  {
    "text": "within storm and conceptually a bolt does it a little bit of processing and",
    "start": "180290",
    "end": "185760"
  },
  {
    "text": "then emits more tuples that go to another bowl that emits more tuples that go to another bolt so you can start",
    "start": "185760",
    "end": "191670"
  },
  {
    "text": "gluing together how you want to process the data a lot like piping things together in the shell you know a way of",
    "start": "191670",
    "end": "197550"
  },
  {
    "text": "thinking about dealing with streams of data that most developers are pretty familiar with and you can omit multiple",
    "start": "197550",
    "end": "204780"
  },
  {
    "text": "ones this sort of a little tiny topology i have here doesn't really do that but",
    "start": "204780",
    "end": "211230"
  },
  {
    "text": "we will see much more complicated topologies later in the talk and so the thing that storm does is it provides a",
    "start": "211230",
    "end": "217320"
  },
  {
    "text": "programming model for doing this that's pleasant to work in as a developer and very importantly it also assumes that",
    "start": "217320",
    "end": "223530"
  },
  {
    "text": "you can't do all of this home one machine because you just have too much data or too much processing and it",
    "start": "223530",
    "end": "229260"
  },
  {
    "text": "provides very good error recovery error handling mechanisms to replay streams of",
    "start": "229260",
    "end": "234390"
  },
  {
    "text": "tuples from points in time to handle failures that's really the main benefit that storm brings over something",
    "start": "234390",
    "end": "241230"
  },
  {
    "text": "hand-rolled is that it has some clever things for handling error in recovery in",
    "start": "241230",
    "end": "247170"
  },
  {
    "text": "a distributed system which we're not going to actually go into that except to say that we you turn them off when you",
    "start": "247170",
    "end": "252840"
  },
  {
    "text": "want high throughput so from a developer perspective what are you writing a bolt",
    "start": "252840",
    "end": "259109"
  },
  {
    "text": "it really is you know a tuple comes in you do some stuff like here you know we",
    "start": "259109",
    "end": "265560"
  },
  {
    "text": "get a span this is actually a subset of a topology for processing",
    "start": "265560",
    "end": "271480"
  },
  {
    "text": "Zipkin Street Zipkin spans we do some stuff with it you do some stuff with it",
    "start": "271480",
    "end": "278050"
  },
  {
    "text": "and then you emit in a different tuple and you'll just glue together a bunch of",
    "start": "278050",
    "end": "283090"
  },
  {
    "text": "these things typically and you know this is you know going in so when you act a",
    "start": "283090",
    "end": "289570"
  },
  {
    "text": "lot of the colors really don't show up on the projector this is all colorized that's not going to help you little too",
    "start": "289570",
    "end": "296020"
  },
  {
    "text": "subtle I apologize so because we don't want to go into how storm works to a",
    "start": "296020",
    "end": "301120"
  },
  {
    "text": "large degree from a programmer perspective we want to talk about how to do it when you've got lots and lots of",
    "start": "301120",
    "end": "306790"
  },
  {
    "text": "teams using it lots and lots of events and how do you do that reasonably efficiently it's important to understand the core abstractions that storm uses",
    "start": "306790",
    "end": "314650"
  },
  {
    "text": "because we're going to talk about them and then we're going to twist them around sideways and put them through a wringer and do well Eric's going to do",
    "start": "314650",
    "end": "319690"
  },
  {
    "text": "horrible things to them so when you look at storm there's a few other things like",
    "start": "319690",
    "end": "324970"
  },
  {
    "text": "a zookeeper cluster because you can't have a distributed system without zookeeper in 2015 with Nimbus the Nimbus",
    "start": "324970",
    "end": "332080"
  },
  {
    "text": "is conceptually the controller for the whole thing it's sort of assigns you submit your topology which is your graph",
    "start": "332080",
    "end": "338440"
  },
  {
    "text": "of processing you know to the Nimbus and it puts it out to a bunch of worker nodes you know so this is like three",
    "start": "338440",
    "end": "344410"
  },
  {
    "text": "different servers now the worker nodes you know have a supervisor running and",
    "start": "344410",
    "end": "350440"
  },
  {
    "text": "this is similar to an or lying supervisor but just different enough to be confusing where a supervisor runs a",
    "start": "350440",
    "end": "357430"
  },
  {
    "text": "worker a worker contains executives and the workers are separate processes their",
    "start": "357430",
    "end": "363190"
  },
  {
    "text": "child processes of the supervisor they started yet it's it's own JVM the executor is not a Java executor despite",
    "start": "363190",
    "end": "370150"
  },
  {
    "text": "being in Java it is a single execution thread in a scheduling thread and you",
    "start": "370150",
    "end": "377470"
  },
  {
    "text": "usually if you put tasks for a topology into an executor um you can technically",
    "start": "377470",
    "end": "386410"
  },
  {
    "text": "put more than one task on an executor running in a worker but you don't ever want to do that because it there's no",
    "start": "386410",
    "end": "392650"
  },
  {
    "text": "it's bad and if anybody actually knows why you'd ever put more than one task into an executor find me afterwards and",
    "start": "392650",
    "end": "398740"
  },
  {
    "text": "explain it because I do not understand it's possible so it's apology then that",
    "start": "398740",
    "end": "404179"
  },
  {
    "text": "we saw is split up into a whole bunch of little tasks that work so if you think",
    "start": "404179",
    "end": "410239"
  },
  {
    "text": "about the spout the spout is set up you run three instances of the spout here you know say this is Kafka we have three",
    "start": "410239",
    "end": "417649"
  },
  {
    "text": "partitions that you can actually get three simultaneous readers that spew things in then you have a stream of",
    "start": "417649",
    "end": "423229"
  },
  {
    "text": "stuff going from the spout pushing into boltay in bolt X and it you know you can",
    "start": "423229",
    "end": "429019"
  },
  {
    "text": "do various things to say try and put it on the same machine but if you can't put it somewhere else and it's always one to",
    "start": "429019",
    "end": "435349"
  },
  {
    "text": "one so this will emit a tuple into here this will admit at uppal into here or",
    "start": "435349",
    "end": "440679"
  },
  {
    "text": "into here and storm takes care of plumbing the tuples all the way through the system the worker you know takes",
    "start": "440679",
    "end": "448729"
  },
  {
    "text": "care of running all the tasks for a topology and here we have what's storm out of the box approximately looks like",
    "start": "448729",
    "end": "456829"
  },
  {
    "text": "trying to run just that one topology now we have dozens of topologies so it gets",
    "start": "456829",
    "end": "462799"
  },
  {
    "text": "even more complicated which is what the rest of the talk is about all half of",
    "start": "462799",
    "end": "468019"
  },
  {
    "text": "the rest of the talk is about um I do want to stop here though because I went",
    "start": "468019",
    "end": "473360"
  },
  {
    "text": "through the explanation quickly and i want to say if you do not understand the different components here a physical",
    "start": "473360",
    "end": "479329"
  },
  {
    "text": "server or virtual machine runs a supervisor a supervisor runs workers",
    "start": "479329",
    "end": "486439"
  },
  {
    "text": "possibly more than one a worker runs executives which run tasks associated",
    "start": "486439",
    "end": "493489"
  },
  {
    "text": "with a topology there's a lot of layers",
    "start": "493489",
    "end": "498919"
  },
  {
    "text": "of sort of nesting of control here and it's going to get murky so this is your opportunity to ask the questions before",
    "start": "498919",
    "end": "505669"
  },
  {
    "text": "it gets murky I'm Way better at",
    "start": "505669",
    "end": "510919"
  },
  {
    "text": "explaining that I thought I was if there's no questions going once going",
    "start": "510919",
    "end": "518240"
  },
  {
    "text": "twice okay then so that's the basics of storm out of the box now to talk about",
    "start": "518240",
    "end": "525110"
  },
  {
    "text": "how we actually make this work for a large number of teams a large number of topologies on a slightly smaller set of",
    "start": "525110",
    "end": "532309"
  },
  {
    "text": "hardware we have Eric weathers who leads the sort of storm on mezzos work at Groupon so",
    "start": "532309",
    "end": "543240"
  },
  {
    "text": "just working yeah cool okay so as Brian",
    "start": "543240",
    "end": "548610"
  },
  {
    "text": "said we have a ton of different topologies that are running inside of Groupon and it used to be that",
    "start": "548610",
    "end": "553970"
  },
  {
    "text": "individual teams would have to manage their own storm clusters gain all the expertise on running storm learn about",
    "start": "553970",
    "end": "560070"
  },
  {
    "text": "all the different operational aspects and then we also have inefficiencies where we have underutilized clusters",
    "start": "560070",
    "end": "567390"
  },
  {
    "text": "because of general issues with needing to acquire hardware in advance to",
    "start": "567390",
    "end": "573410"
  },
  {
    "text": "provision your machines for clusters and also that storm itself doesn't lend",
    "start": "573410",
    "end": "579840"
  },
  {
    "text": "itself to efficient utilization of underlying hardware because of the resource scheduling model of storm which",
    "start": "579840",
    "end": "587130"
  },
  {
    "text": "I'll get into here in a second so first I want to introduce the concept of May soaps and how it works in general and",
    "start": "587130",
    "end": "595680"
  },
  {
    "text": "then I'll apply it specifically to our use case of storm so if no one's heard of May sews or I'll do the survey thing",
    "start": "595680",
    "end": "601980"
  },
  {
    "text": "as anyone is how many people are familiar with mesas so that's actually",
    "start": "601980",
    "end": "607260"
  },
  {
    "text": "pretty cool so I don't have to go into super detail but basically may sews does",
    "start": "607260",
    "end": "614670"
  },
  {
    "text": "resource scheduling on worker nodes so given an application framework like",
    "start": "614670",
    "end": "620010"
  },
  {
    "text": "Hadoop or storm or MPI or anything else you can imagine it might want to run on",
    "start": "620010",
    "end": "626160"
  },
  {
    "text": "many different hosts spread across a cluster and may sews allows you to",
    "start": "626160",
    "end": "632400"
  },
  {
    "text": "schedule your tasks on to these various worker nodes it has its own mechanisms",
    "start": "632400",
    "end": "638700"
  },
  {
    "text": "for maintaining state for the master so that it can be a che and it two of the",
    "start": "638700",
    "end": "646050"
  },
  {
    "text": "core concepts of may so that need to be understood for you to really grasp the rest of the talk my section at least are",
    "start": "646050",
    "end": "652590"
  },
  {
    "text": "the scheduler and the executor so each framework storm Hadoop whatever has a",
    "start": "652590",
    "end": "658770"
  },
  {
    "text": "scheduler which is a thing that makes those talks to and it says like hey do you want to run anything on mesa sand",
    "start": "658770",
    "end": "664350"
  },
  {
    "text": "the schedules like yes please run these tasks then may sews will realize those tasks on to the actual worker",
    "start": "664350",
    "end": "671230"
  },
  {
    "text": "slave notes and there's different ways of doing it but the standard way that we also using in storm is to run another",
    "start": "671230",
    "end": "678249"
  },
  {
    "text": "executor and this is where we get into a little bit of terminology confusion because I've talked about tasks and",
    "start": "678249",
    "end": "683769"
  },
  {
    "text": "executives here and Brian also talked about tasks and executor and these are different things so again going back to",
    "start": "683769",
    "end": "690910"
  },
  {
    "text": "the storm model you have executives and tasks these are the most low-level things in this whole system they operate",
    "start": "690910",
    "end": "697089"
  },
  {
    "text": "on the individual tuples that are flowing through the system they might do things like aggregating together two",
    "start": "697089",
    "end": "702699"
  },
  {
    "text": "different log lines whatever you can imagine and they just run in this threaded model of the executor the tasks",
    "start": "702699",
    "end": "709660"
  },
  {
    "text": "do now form a sauce going back the executor was the actual controller of",
    "start": "709660",
    "end": "716920"
  },
  {
    "text": "everything that's running on a particular slave for your framework and it's going to run individual processes",
    "start": "716920",
    "end": "722350"
  },
  {
    "text": "for your framework as makes those tasks so this is kind of the matryoshka doll I",
    "start": "722350",
    "end": "730959"
  },
  {
    "text": "forgot how you said it but babushka dolls I don't know it of the different layers so the outside you have a worker",
    "start": "730959",
    "end": "736660"
  },
  {
    "text": "host within that you have a storm supervisor which is actually the mace",
    "start": "736660",
    "end": "741730"
  },
  {
    "text": "executor within that we have the storm worker which is actually the mezzos task",
    "start": "741730",
    "end": "746980"
  },
  {
    "text": "process then you have the storm executor and then the storm task so whenever you're talking about tasks and",
    "start": "746980",
    "end": "753129"
  },
  {
    "text": "executives with respect to these two different systems you need to be very clear about which level you're talking",
    "start": "753129",
    "end": "761339"
  },
  {
    "text": "so to bridge together this two different systems we leveraged an outside or",
    "start": "761339",
    "end": "767649"
  },
  {
    "text": "rather existing open source project for creating a framework on top of mesos for",
    "start": "767649",
    "end": "773079"
  },
  {
    "text": "storm there's actually written by the original author of storm nathan martes",
    "start": "773079",
    "end": "778209"
  },
  {
    "text": "and so we've just run it for a while unadulterated and we've recently started",
    "start": "778209",
    "end": "784389"
  },
  {
    "text": "making changes to it so the way it works is that it interfaces between storm and maces by implementing various Java",
    "start": "784389",
    "end": "791019"
  },
  {
    "text": "interfaces and I have well get to it a",
    "start": "791019",
    "end": "796420"
  },
  {
    "text": "little bit about exactly how the interface work but the most important thing to understand really for thinking about",
    "start": "796420",
    "end": "803820"
  },
  {
    "text": "storm vs. mesos is that they're very different in terms that are resource scheduling storm has no knowledge about",
    "start": "803820",
    "end": "809670"
  },
  {
    "text": "CPUs and memory and things like that that Mesa AZ knows all about because how mesas works is it looks at a given host",
    "start": "809670",
    "end": "816089"
  },
  {
    "text": "and it's like okay we have 20 cores we have 20 gigabytes of RAM available and",
    "start": "816089",
    "end": "821640"
  },
  {
    "text": "it offers that out to the schedulers saying hey you guys want to use this but storm doesn't know anything about that",
    "start": "821640",
    "end": "827040"
  },
  {
    "text": "all it knows is this concept of a slot which I'll explain so imagine this top",
    "start": "827040",
    "end": "832620"
  },
  {
    "text": "box is a single host it has four slots we're really all they are is a place",
    "start": "832620",
    "end": "838230"
  },
  {
    "text": "where it can store that store but run a worker process because storms Nimbus has",
    "start": "838230",
    "end": "844470"
  },
  {
    "text": "these work or processes that are going to do all the processing it needs to sit them on two nodes so what it does is it",
    "start": "844470",
    "end": "850170"
  },
  {
    "text": "asks the cluster hey how many slots do I have available and it's like okay I have four from this host and it's like okay",
    "start": "850170",
    "end": "856740"
  },
  {
    "text": "I'll just shove some topologies into it so that's what I represent below where topology red is in the top left and then",
    "start": "856740",
    "end": "863910"
  },
  {
    "text": "the peel one is on the right with two different worker processes in the single",
    "start": "863910",
    "end": "869100"
  },
  {
    "text": "host and there's a third topology in the bottom left but one of the bad things about this whole model is it's very",
    "start": "869100",
    "end": "874800"
  },
  {
    "text": "static you have as you set up your storm cluster you have to decide how many worker processes you're going to create",
    "start": "874800",
    "end": "881519"
  },
  {
    "text": "you can't make it dynamic to the actual demands of the various topologies are going to be running within your system so it's pretty inflexible and if you're",
    "start": "881519",
    "end": "889410"
  },
  {
    "text": "allocating different types of machines you have to think about well this is a much be free machine how many slots does",
    "start": "889410",
    "end": "895320"
  },
  {
    "text": "this really represent so we'd prefer to have a model that instead takes into account the actual real resources within",
    "start": "895320",
    "end": "902459"
  },
  {
    "text": "the slave nodes and allows us to schedule what the actual topologies need the other thing all reference is that",
    "start": "902459",
    "end": "909180"
  },
  {
    "text": "when you're running inside of a standard storm cluster you have no real incentive to think about how many CPUs or how much",
    "start": "909180",
    "end": "917459"
  },
  {
    "text": "memory do I need per worker process so it can lead to some inefficiencies in terms of how you're tuning your topology",
    "start": "917459",
    "end": "924899"
  },
  {
    "text": "you might just throw a bunch of hard metal at it and not really think about like oh this process really doesn't need",
    "start": "924899",
    "end": "932010"
  },
  {
    "text": "to ton of memory it's all it's only doing CPU work but when we put this stuff on",
    "start": "932010",
    "end": "938200"
  },
  {
    "text": "top of May sews it forces us to start thinking of this and gains more efficiencies in terms of usage of the",
    "start": "938200",
    "end": "944170"
  },
  {
    "text": "hardware so I already kind of covered this but basically mesas thinks of CPU",
    "start": "944170",
    "end": "949240"
  },
  {
    "text": "and memory storm doesn't make those deals with allocating the CPU and memory",
    "start": "949240",
    "end": "956050"
  },
  {
    "text": "for tasks on toast storm again only thinks about slots so this is how storm",
    "start": "956050",
    "end": "966100"
  },
  {
    "text": "this is like another picture of the earlier representation Brian showed up storm where you have a worker host you",
    "start": "966100",
    "end": "972370"
  },
  {
    "text": "have a supervisor and you have different topologies that run under supervision of that single supervisor in the storm of",
    "start": "972370",
    "end": "979930"
  },
  {
    "text": "mesas model it's actually different so we allocate different supervisors for",
    "start": "979930",
    "end": "985480"
  },
  {
    "text": "every topology that run on a particular host the advantage of this is that you can isolate the topologies from one",
    "start": "985480",
    "end": "992350"
  },
  {
    "text": "another you can decide that this topology needs more ram this topology",
    "start": "992350",
    "end": "998260"
  },
  {
    "text": "needs more CPU and you can allocate them appropriately and the other thing that's",
    "start": "998260",
    "end": "1004680"
  },
  {
    "text": "really important to note about the something that's kind of hidden this diagram is that these purple boxes also",
    "start": "1004680",
    "end": "1010230"
  },
  {
    "text": "represent Mesa executor 'he's which actually if you're using c groups are containers so they have hard bounds on",
    "start": "1010230",
    "end": "1017760"
  },
  {
    "text": "the amount of memory or CPU or other stuff as May sews improves it some",
    "start": "1017760",
    "end": "1023660"
  },
  {
    "text": "resource handling so you can avoid different topologies impacting each",
    "start": "1023660",
    "end": "1028709"
  },
  {
    "text": "other whereas in this former model topology see could potentially take over all the RAM on the box and squeeze out",
    "start": "1028709",
    "end": "1034530"
  },
  {
    "text": "topology a and B so that's one advantage so this this diagram is a little bit",
    "start": "1034530",
    "end": "1040829"
  },
  {
    "text": "busy present is that we need to somehow",
    "start": "1040829",
    "end": "1046020"
  },
  {
    "text": "map the resources that may so snow is about into slots and storm has this",
    "start": "1046020",
    "end": "1052920"
  },
  {
    "text": "model where it knows beforehand I have X slots but in the maze world there's no such thing there's not a pre-existing",
    "start": "1052920",
    "end": "1059700"
  },
  {
    "text": "number of slots we have to somehow take the existing amount of resources that been offered up by May sews and munge",
    "start": "1059700",
    "end": "1065850"
  },
  {
    "text": "them in do a set of slots for the worker processes to get scheduled on to you by storm so this is representing how the",
    "start": "1065850",
    "end": "1073480"
  },
  {
    "text": "two systems interface they go through Java interfaces and resources come in",
    "start": "1073480",
    "end": "1078760"
  },
  {
    "text": "from asos get calculated into slots and then storm schedules worker processes onto those slots sending it back through",
    "start": "1078760",
    "end": "1087220"
  },
  {
    "text": "the interface again into Mesa swear supervisors are created that then start",
    "start": "1087220",
    "end": "1094030"
  },
  {
    "text": "worker processes so let me talk a little bit I'm going like too fast I guess but",
    "start": "1094030",
    "end": "1100720"
  },
  {
    "text": "let me talk about how Groupon storm as a service actually looks on top of this",
    "start": "1100720",
    "end": "1106390"
  },
  {
    "text": "existing technology that is out there in the real world so we have a dedicated",
    "start": "1106390",
    "end": "1111550"
  },
  {
    "text": "mason's cluster where we run storm on we created a submitter application where we",
    "start": "1111550",
    "end": "1116620"
  },
  {
    "text": "can gate the topologies that are coming into our system so that we can control and kind of know who's running",
    "start": "1116620",
    "end": "1122920"
  },
  {
    "text": "topologies eventually we could use this to block topologies from even launching if they're going beyond certain like",
    "start": "1122920",
    "end": "1129400"
  },
  {
    "text": "resource request limits we've overcome something that I haven't touched on yet",
    "start": "1129400",
    "end": "1134440"
  },
  {
    "text": "but you might have noticed in an earlier slide here that this is a spa single",
    "start": "1134440",
    "end": "1140890"
  },
  {
    "text": "point of failure the nimbus if it goes down then storm is unable to keep",
    "start": "1140890",
    "end": "1147270"
  },
  {
    "text": "launching new workers to notice that workers are down and needs to create new ones to take a new topologies and launch",
    "start": "1147270",
    "end": "1153700"
  },
  {
    "text": "them so you have to work hard to get the numbers back up as soon as possible if",
    "start": "1153700",
    "end": "1159280"
  },
  {
    "text": "it goes down storm as Brian reference does store a bunch of state in zookeeper but it does have state that isn't in",
    "start": "1159280",
    "end": "1166450"
  },
  {
    "text": "zookeeper such as the storm jars themselves for your topology so that's why it's Singapore failure right now and",
    "start": "1166450",
    "end": "1173200"
  },
  {
    "text": "doesn't have a che so that leads us to the are sinking thing so we are synced",
    "start": "1173200",
    "end": "1179080"
  },
  {
    "text": "from our current master Nimbus on to a backup one so that we can bring up that one pretty quickly because all the state",
    "start": "1179080",
    "end": "1185290"
  },
  {
    "text": "is either on that house or in zookeeper and we've gone through that exercise a couple of times and it works well we've",
    "start": "1185290",
    "end": "1192940"
  },
  {
    "text": "also had to created had to create a couple of libraries to give us consistent",
    "start": "1192940",
    "end": "1198580"
  },
  {
    "text": "visibility into the system to allow our users to be able to use the system nicely so we allow you to log out into",
    "start": "1198580",
    "end": "1206019"
  },
  {
    "text": "Kafka or splunk using our libraries we also provided a metric system so that",
    "start": "1206019",
    "end": "1212140"
  },
  {
    "text": "people can get a view as to how their topology is performing like how many",
    "start": "1212140",
    "end": "1220779"
  },
  {
    "text": "tuples per second are going through different bolts how many acts are coming back you can also put application level",
    "start": "1220779",
    "end": "1227830"
  },
  {
    "text": "metrics out into the system so that you could graph various application level",
    "start": "1227830",
    "end": "1233039"
  },
  {
    "text": "interesting things for for your particular topology and we implemented",
    "start": "1233039",
    "end": "1238510"
  },
  {
    "text": "this just on top of a built-in storm thing that's not very well-documented called the I metrics consumer yeah so",
    "start": "1238510",
    "end": "1247240"
  },
  {
    "text": "the pros for using this system instead of using native storm are that provides",
    "start": "1247240",
    "end": "1252610"
  },
  {
    "text": "true isolation storm does provide something called the isolation scheduler however it statically partitions the",
    "start": "1252610",
    "end": "1259799"
  },
  {
    "text": "cluster so the way it works is that you would dedicate particular worker nodes",
    "start": "1259799",
    "end": "1265960"
  },
  {
    "text": "to a topology so this topology would only ever run in those nodes as those would only overrun that topologies",
    "start": "1265960",
    "end": "1272549"
  },
  {
    "text": "processes so we didn't really like that that's an advantage of using the Mason",
    "start": "1272549",
    "end": "1277659"
  },
  {
    "text": "space system provides us flexibility for increasing or decreasing the number and size of the worker processes that you",
    "start": "1277659",
    "end": "1284649"
  },
  {
    "text": "have for your storm cluster or topology rather in cluster it avoids us having a",
    "start": "1284649",
    "end": "1293169"
  },
  {
    "text": "bunch of separate underutilized clusters that everybody's having to manage themselves look into the future and try",
    "start": "1293169",
    "end": "1298929"
  },
  {
    "text": "to predict how stuff is growing and have to request from our operations team extra hosts it just it it reduces a lot",
    "start": "1298929",
    "end": "1305740"
  },
  {
    "text": "of the burden it also gives us some consistent visibility into the",
    "start": "1305740",
    "end": "1310899"
  },
  {
    "text": "operations of our storm clusters so that we can compare across topologies and be like oh well this topology is behaving",
    "start": "1310899",
    "end": "1317620"
  },
  {
    "text": "well this one isn't so it really is something about your topology and not the underlying system that we've built",
    "start": "1317620",
    "end": "1324840"
  },
  {
    "text": "well I guess I will yield the floor to aj at this place all right turn myself",
    "start": "1324840",
    "end": "1334390"
  },
  {
    "text": "off all right so this is cat we like",
    "start": "1334390",
    "end": "1340630"
  },
  {
    "text": "cats a groupon but this is a bullet cat and called a cat tuple so the griddle",
    "start": "1340630",
    "end": "1345910"
  },
  {
    "text": "part of the talk so the griddle part of the talk is going to be focused on not",
    "start": "1345910",
    "end": "1352990"
  },
  {
    "text": "just throughput but latency so storm is really focused on getting the most",
    "start": "1352990",
    "end": "1358930"
  },
  {
    "text": "number of tuples as you can flowing through the topology if we go back to that storm picture with all of those",
    "start": "1358930",
    "end": "1365320"
  },
  {
    "text": "boxes both Brian and Eric talked a lot about how a tuple will leave the Machine and go somewhere else okay so how about",
    "start": "1365320",
    "end": "1372520"
  },
  {
    "text": "cases where latency matters and you don't want to bear the cost of serializing deserialising network I oh",
    "start": "1372520",
    "end": "1378660"
  },
  {
    "text": "okay so this is called griddle griddle",
    "start": "1378660",
    "end": "1384910"
  },
  {
    "text": "basically gives two teams that want to use storm a DSL which expresses with a",
    "start": "1384910",
    "end": "1394810"
  },
  {
    "text": "DSP like workflow what I mean by that is just P traditionally as tiny little things that you want to combine together",
    "start": "1394810",
    "end": "1401050"
  },
  {
    "text": "to make the sort of Gestalt behavior they should be easier it should be easy to mix and match the syntax on the dsl",
    "start": "1401050",
    "end": "1408070"
  },
  {
    "text": "is like adjacency graph to Claire verdict says you declare your Jason sees",
    "start": "1408070",
    "end": "1413640"
  },
  {
    "text": "the topologies that I work with which are specifically place data business data we do lots of normalization so",
    "start": "1413640",
    "end": "1420730"
  },
  {
    "text": "we're really focused on how do we get storm to be useful for data scientists or machine learning folks and also we",
    "start": "1420730",
    "end": "1429820"
  },
  {
    "text": "talk a lot about or I can talk about about mechanical sympathy which is how",
    "start": "1429820",
    "end": "1435280"
  },
  {
    "text": "about cases where you know you want those things to be in CPU cache you want to run as fast as possible maintence e",
    "start": "1435280",
    "end": "1441670"
  },
  {
    "text": "matters ok all right so now we'll talk about some pictures and they're just",
    "start": "1441670",
    "end": "1446920"
  },
  {
    "text": "going to be DAGs and this is a very small portion of the place data topology",
    "start": "1446920",
    "end": "1456510"
  },
  {
    "text": "having very complex topologies in store storm is a bit rough it's hard to declare them",
    "start": "1456690",
    "end": "1462610"
  },
  {
    "text": "so our place topology is i think twenty to thirty nodes which is a lot for storm",
    "start": "1462610",
    "end": "1469540"
  },
  {
    "text": "it also means that you have 20 to 30 cues okay all right so let's just talk about this so we want to normalize the",
    "start": "1469540",
    "end": "1475990"
  },
  {
    "text": "country code we want to do some things with the website postcode this stands for out of business not out of band and",
    "start": "1475990",
    "end": "1484330"
  },
  {
    "text": "we geocode so this special little blue border is indicating that this is the only note here that needs to do any I oh",
    "start": "1484330",
    "end": "1492840"
  },
  {
    "text": "ok so it goes out to our happy clock because that cloud is getting data so it's happy ok and then we want to do",
    "start": "1492840",
    "end": "1498910"
  },
  {
    "text": "things with the phone numbers alright so this is very abstract it doesn't really capture some really important business",
    "start": "1498910",
    "end": "1504610"
  },
  {
    "text": "use cases which are described here whoa okay so I recognize that this picture is",
    "start": "1504610",
    "end": "1510160"
  },
  {
    "text": "ugly it's intentionally ugly because it's trying to express that in the real world things get complicated so what",
    "start": "1510160",
    "end": "1516280"
  },
  {
    "text": "this box up here means is that we have a whole sub graph which until we have qaid",
    "start": "1516280",
    "end": "1523660"
  },
  {
    "text": "the effect of these vertices on a",
    "start": "1523660",
    "end": "1529120"
  },
  {
    "text": "country's data that we don't actually want to run them we would like to leave the data as it is because we're not so",
    "start": "1529120",
    "end": "1535240"
  },
  {
    "text": "like let's say we have a nun bored at a particular country yet so place data we get from all over the world and we've",
    "start": "1535240",
    "end": "1541030"
  },
  {
    "text": "optimized the architecture so that we can add country by country by country okay so let's say we want to add a new",
    "start": "1541030",
    "end": "1547480"
  },
  {
    "text": "country let's say we're doing Spain for example and let's say these don't QA",
    "start": "1547480",
    "end": "1553419"
  },
  {
    "text": "well okay but we don't want to delay the business from being able to do to call",
    "start": "1553419",
    "end": "1559030"
  },
  {
    "text": "businesses in Spain so we're just going to do this part okay or it may be be the",
    "start": "1559030",
    "end": "1564340"
  },
  {
    "text": "case that we're starting to accept countries that we haven't even started training on okay so that's fine so",
    "start": "1564340",
    "end": "1570910"
  },
  {
    "text": "they'll go through here too all right so go back to hear this is a really simple",
    "start": "1570910",
    "end": "1577990"
  },
  {
    "text": "graph every vertex has an indegree of one every edge is going to get walked",
    "start": "1577990",
    "end": "1585309"
  },
  {
    "text": "all right so I'll sudden we're here we now have our texts that have in degree",
    "start": "1585309",
    "end": "1591580"
  },
  {
    "text": "of two and not all vertexes are going to be walked in fact let's say we do want to process",
    "start": "1591580",
    "end": "1600100"
  },
  {
    "text": "our country well then we're going to go here or go go here post condition exists",
    "start": "1600100",
    "end": "1606160"
  },
  {
    "text": "just to be like yep we did or we didn't this okay if we don't want to process this because we don't appreciate like",
    "start": "1606160",
    "end": "1612790"
  },
  {
    "text": "the quality of these normalizations well then we're going to come here so that's",
    "start": "1612790",
    "end": "1619720"
  },
  {
    "text": "interesting in the sense that those two edge sets are not disjoint okay alright",
    "start": "1619720",
    "end": "1628150"
  },
  {
    "text": "so let's go ahead and maybe obviate these things to make this graph look at",
    "start": "1628150",
    "end": "1633820"
  },
  {
    "text": "least a little more manageable first thing we're going to do is we're going to say that we have this thing called an",
    "start": "1633820",
    "end": "1638860"
  },
  {
    "text": "active edge chooser we also want to state that that means that this country",
    "start": "1638860",
    "end": "1645460"
  },
  {
    "text": "code normalizer it has no idea what vertices is actually going to so UD",
    "start": "1645460",
    "end": "1651640"
  },
  {
    "text": "couple the behavior this can change independently from this so that's nice we also know have a couple different",
    "start": "1651640",
    "end": "1658290"
  },
  {
    "text": "types of vertexes okay so all three of these have an in degree of two okay but",
    "start": "1658290",
    "end": "1664960"
  },
  {
    "text": "these two are different because until they get data on each of their edges they need to block and they need to wait",
    "start": "1664960",
    "end": "1671680"
  },
  {
    "text": "so like I said that's very different from this there's no waiting necessary here there's no joining there's there's",
    "start": "1671680",
    "end": "1677800"
  },
  {
    "text": "none of this business all right okay so this is these kinds of joints are also",
    "start": "1677800",
    "end": "1686650"
  },
  {
    "text": "very difficult in a storm topologies so now what I'd like to do is jump to what",
    "start": "1686650",
    "end": "1695410"
  },
  {
    "text": "is this looks like in a griddle declaration well just the whole thing briefly and then we're going to start to",
    "start": "1695410",
    "end": "1701200"
  },
  {
    "text": "dive in so this is a dsl and there is a compiler all right so let's get started",
    "start": "1701200",
    "end": "1708660"
  },
  {
    "text": "this is a subset of the vertices that we declare okay we're going to see you start okay very simple syntax so this is",
    "start": "1708660",
    "end": "1717070"
  },
  {
    "text": "what I was really trying to go to with you okay a why a dsl and B if you're going to do a DSL make sure you don't force people to learn a bunch of weird",
    "start": "1717070",
    "end": "1723190"
  },
  {
    "text": "random stuff should make a lot of sense this vertex we name it and we say with",
    "start": "1723190",
    "end": "1728230"
  },
  {
    "text": "the type is okay alright so now we want to talk about",
    "start": "1728230",
    "end": "1733310"
  },
  {
    "text": "adjacencies okay so we're saying that we omit to post code and website from begin",
    "start": "1733310",
    "end": "1740550"
  },
  {
    "text": "country dependent so if we go back so you could consider the conditional to be",
    "start": "1740550",
    "end": "1745590"
  },
  {
    "text": "the country dependent here's postcode website okay all right so these are all",
    "start": "1745590",
    "end": "1755750"
  },
  {
    "text": "pretty basic and by basic what I mean is that we omit on every out edge okay and",
    "start": "1755750",
    "end": "1766190"
  },
  {
    "text": "here's a really simple little wing all right let's go to a complicated one",
    "start": "1766190",
    "end": "1773690"
  },
  {
    "text": "alright so here's our active edge chooser so this is basically saying you",
    "start": "1773690",
    "end": "1780740"
  },
  {
    "text": "omit to any subset of this adjacency",
    "start": "1780740",
    "end": "1787410"
  },
  {
    "text": "list from start and you use the chooser of this type okay and by any said I",
    "start": "1787410",
    "end": "1794100"
  },
  {
    "text": "literally mean any set it can be empty as well which could alternate so this is",
    "start": "1794100",
    "end": "1800610"
  },
  {
    "text": "just making sure this is obvious so let's go back to the picture again",
    "start": "1800610",
    "end": "1805520"
  },
  {
    "text": "alright so we're saying we omit to",
    "start": "1805670",
    "end": "1811220"
  },
  {
    "text": "conditional post conditional or out of business from country code okay but what",
    "start": "1811220",
    "end": "1819030"
  },
  {
    "text": "we're doing is this active edge choose your nose okay we've normalized the country code what is the country oh we",
    "start": "1819030",
    "end": "1825210"
  },
  {
    "text": "support that one so I'm going to go to these two okay or no we don't split that one so I'm gonna go to these two okay",
    "start": "1825210",
    "end": "1831200"
  },
  {
    "text": "country code has no idea about that edge choice all right 02 here's a picture",
    "start": "1831200",
    "end": "1845130"
  },
  {
    "text": "I could show the cat again too okay so",
    "start": "1845130",
    "end": "1850860"
  },
  {
    "text": "as any time there's a DSL you know people very reasonably say like why do",
    "start": "1850860",
    "end": "1856800"
  },
  {
    "text": "you do that what does it do so i'll give you because it better give you something okay so there's a compiler and it",
    "start": "1856800",
    "end": "1862440"
  },
  {
    "text": "creates a binary graph okay and that binary graph is then processed by a",
    "start": "1862440",
    "end": "1868740"
  },
  {
    "text": "griddle runtime framework okay and the runtime framework is simply a framework it's not a process so that you can put",
    "start": "1868740",
    "end": "1876090"
  },
  {
    "text": "it in any sort of server you want to and that that process provides optimal",
    "start": "1876090",
    "end": "1882780"
  },
  {
    "text": "concurrency for the graph that you've expressed okay so basically what we have",
    "start": "1882780",
    "end": "1888810"
  },
  {
    "text": "now in a groupon is that for our big placed apology where we have areas of",
    "start": "1888810",
    "end": "1895880"
  },
  {
    "text": "bolts that really don't need any i/o if there were on the same CPU we would get",
    "start": "1895880",
    "end": "1902310"
  },
  {
    "text": "a huge latency increase we've been porting them to what we call the griddle",
    "start": "1902310",
    "end": "1908040"
  },
  {
    "text": "operator or griddle bolt which is a bolt running over a compiled griddle graph",
    "start": "1908040",
    "end": "1916250"
  },
  {
    "text": "and getting this really good concurrency and we've had empirically i should say",
    "start": "1916250",
    "end": "1923100"
  },
  {
    "text": "every time we sort of pull a bolt into a griddle graph we get back at least 10",
    "start": "1923100",
    "end": "1928410"
  },
  {
    "text": "milliseconds because of the queue um so i'm not saying that is either a good or a bad thing it's a bad thing for latency",
    "start": "1928410",
    "end": "1935730"
  },
  {
    "text": "it's something you don't really care about for throughput right because if you want to say like we can do 10,000 a",
    "start": "1935730",
    "end": "1940950"
  },
  {
    "text": "second that means that every second you have 10,000 be processed okay that doesn't say anything about how long it",
    "start": "1940950",
    "end": "1946140"
  },
  {
    "text": "takes all right so now I'm going to cross my fingers dive into the deep end",
    "start": "1946140",
    "end": "1951840"
  },
  {
    "text": "and we'll i'll show you this picture that talks a little bit about the threading model if what I say is",
    "start": "1951840",
    "end": "1957480"
  },
  {
    "text": "confusing just interrupt me because I really do want to motivate like why would we create another DSL alright so",
    "start": "1957480",
    "end": "1966800"
  },
  {
    "text": "good the colors showed up kind of okay um so this is a pretty simple graph it's",
    "start": "1966800",
    "end": "1975660"
  },
  {
    "text": "got six vertices those are blue and you've got all your",
    "start": "1975660",
    "end": "1982409"
  },
  {
    "text": "edges and basic and then these little red circles are basically indicating",
    "start": "1982409",
    "end": "1988559"
  },
  {
    "text": "that what a griddle graph processes is a domain value okay basically these are",
    "start": "1988559",
    "end": "1995190"
  },
  {
    "text": "unary functions they have domain they have range okay as your domain value is moving through the graph until it gets",
    "start": "1995190",
    "end": "2002480"
  },
  {
    "text": "spit out obviously it's going to change state okay so if we think of that place like the website is going to change geocoding is going to change postcodes",
    "start": "2002480",
    "end": "2009470"
  },
  {
    "text": "going to get normalized things like that okay alright so here we are the yellow",
    "start": "2009470",
    "end": "2017269"
  },
  {
    "text": "boxes our vertex label and a thread",
    "start": "2017269",
    "end": "2022899"
  },
  {
    "text": "indicator for what thread will be running that node okay one thing I",
    "start": "2022899",
    "end": "2031039"
  },
  {
    "text": "should indicate here is that I haven't even I don't dive into the conditional",
    "start": "2031039",
    "end": "2036710"
  },
  {
    "text": "pass most I just want to talk about the concurrency okay so the the green",
    "start": "2036710",
    "end": "2044600"
  },
  {
    "text": "rounded rectangle is saying that there's a scheduler thread and so what that means is that scheduling does not get in",
    "start": "2044600",
    "end": "2051378"
  },
  {
    "text": "the way of any work that's already being done okay all right so we have d0 it's a",
    "start": "2051379",
    "end": "2056780"
  },
  {
    "text": "pretty trivial one it's just trying to get this graph started ok so we trigger these three to get started okay",
    "start": "2056780",
    "end": "2063919"
  },
  {
    "text": "obviously this is handled by to not but now we have G naught 2 1 T 2 we have",
    "start": "2063919",
    "end": "2069470"
  },
  {
    "text": "three threads ok these these emit to range values and we're able to transform",
    "start": "2069470",
    "end": "2076429"
  },
  {
    "text": "d naught into D 1 ok this is one of these joint vertexes ok in fact I didn't",
    "start": "2076429",
    "end": "2083510"
  },
  {
    "text": "didn't explicate it but within the",
    "start": "2083510",
    "end": "2090220"
  },
  {
    "text": "syntax we have this idea of aggregates inputs ok which means you need to wait",
    "start": "2091960",
    "end": "2097490"
  },
  {
    "text": "for all of your inputs any vertex that doesn't have that one input will trigger it to continue and that becomes",
    "start": "2097490",
    "end": "2104960"
  },
  {
    "text": "interesting with this one for example you'll notice since it doesn't have an iron box if it gets in here",
    "start": "2104960",
    "end": "2111410"
  },
  {
    "text": "there if it gets in here it goes to there okay all right so we're now using",
    "start": "2111410",
    "end": "2122090"
  },
  {
    "text": "t not so now we're back down to only having two threads and then we come to here and we just have one final thread",
    "start": "2122090",
    "end": "2131020"
  },
  {
    "text": "Brian and I were talking this morning and based on the real implementation one could very recently criticized this the",
    "start": "2131020",
    "end": "2139490"
  },
  {
    "text": "fact that the T naught is running through this path is I probably shouldn't have done okay because we are",
    "start": "2139490",
    "end": "2145940"
  },
  {
    "text": "not scheduling paths we are scheduling vertices that are ready to do work ok so",
    "start": "2145940",
    "end": "2153710"
  },
  {
    "text": "I could have just well said T naught T naught 2 1 T 2 ok and this is very",
    "start": "2153710",
    "end": "2165320"
  },
  {
    "text": "important because what it means is that as a vertex is ready so let's take this",
    "start": "2165320",
    "end": "2171590"
  },
  {
    "text": "example so let's say r1 takes a millisecond but our two takes 10",
    "start": "2171590",
    "end": "2177380"
  },
  {
    "text": "milliseconds I don't know actually I help doesn't because that doesn't sound very good latency so as soon as this is",
    "start": "2177380",
    "end": "2184220"
  },
  {
    "text": "then ready it's going to come up and be scheduled because it says I'm ready to do some work so that ability of being",
    "start": "2184220",
    "end": "2193910"
  },
  {
    "text": "ready isn't related to the particular path thats related to the state of the vertex all right questions",
    "start": "2193910",
    "end": "2203800"
  }
]