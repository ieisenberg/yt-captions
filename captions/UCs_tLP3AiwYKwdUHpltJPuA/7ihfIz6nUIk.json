[
  {
    "text": "[Music] so welcome and today um I will try to um",
    "start": "6990",
    "end": "13880"
  },
  {
    "text": "to walk you through um a little example uh of deep learning using",
    "start": "13880",
    "end": "19160"
  },
  {
    "text": "tensorflow let's Jump Right In so the example we will be using is uh this data",
    "start": "19160",
    "end": "26359"
  },
  {
    "text": "set this these are handwritten characters and it's it's of a Hello World in uh in uh in in neural networks",
    "start": "26359",
    "end": "32840"
  },
  {
    "text": "and deep learning uh but apart being kind of an easy example it is also a",
    "start": "32840",
    "end": "38280"
  },
  {
    "text": "data set that has been that has kept uh scientists busy for the past two decades",
    "start": "38280",
    "end": "43879"
  },
  {
    "text": "if you go to the website over there you will actually see 20 years of scientific",
    "start": "43879",
    "end": "49320"
  },
  {
    "text": "Publications on this and uh we'll go through that evolution in 40 40 minutes",
    "start": "49320",
    "end": "55920"
  },
  {
    "text": "so let's start what is the simplest possible neural network you can build to recognize those",
    "start": "55920",
    "end": "62199"
  },
  {
    "text": "images so it's a simple one layer Network how does this work you take your",
    "start": "62199",
    "end": "67280"
  },
  {
    "text": "images over there that's an eight and the first thing you do uh you you you just consider it as a bag of pixels you",
    "start": "67280",
    "end": "75320"
  },
  {
    "text": "put all the pixels on one line and then on the line below you have neurons so",
    "start": "75320",
    "end": "80640"
  },
  {
    "text": "what do we call a neuron in everything we will see today a neuron is something",
    "start": "80640",
    "end": "86400"
  },
  {
    "text": "that does a weighted sum of all of its inputs and then feeds the result through",
    "start": "86400",
    "end": "91920"
  },
  {
    "text": "some nonlinear function okay that's it weighted sum with weights we will see",
    "start": "91920",
    "end": "98159"
  },
  {
    "text": "what those weights are and then the result you feed it through some nonlinear function which we will call an",
    "start": "98159",
    "end": "104119"
  },
  {
    "text": "activation function so here uh we have 10 neurons why 10 well we are trying to",
    "start": "104119",
    "end": "110680"
  },
  {
    "text": "recognize digits from 0 to 9 so we are hoping that out of those 10 neurons one",
    "start": "110680",
    "end": "116880"
  },
  {
    "text": "will light up very strongly and tell us this is an eight that's what we are hoping",
    "start": "116880",
    "end": "122000"
  },
  {
    "text": "for we have to choose the activation function appropriately and for",
    "start": "122000",
    "end": "127360"
  },
  {
    "text": "classification examples like here the one that is very often used is called Soft Max so beyond the equation what it",
    "start": "127360",
    "end": "134239"
  },
  {
    "text": "is is simply that you've got your 10 weighted sums and you take the exponential of them and then you",
    "start": "134239",
    "end": "140879"
  },
  {
    "text": "normalize the whole Vector normalize means you divide by its size Norm so the",
    "start": "140879",
    "end": "146920"
  },
  {
    "text": "the effect of that the exponential goes up very steeply so the effect of that is that if one of",
    "start": "146920",
    "end": "152280"
  },
  {
    "text": "those neurons is slightly higher than the other after the exponential he will be a lot higher and as you divide by the",
    "start": "152280",
    "end": "158519"
  },
  {
    "text": "norm he will end up still with a significant value while all the others will go down so it's kind of a soft way",
    "start": "158519",
    "end": "165879"
  },
  {
    "text": "of taking the maximum out of those neurons so now we have to write this as",
    "start": "165879",
    "end": "172879"
  },
  {
    "text": "a matrix multiply so please rewind to your University years and remember what",
    "start": "172879",
    "end": "179120"
  },
  {
    "text": "is a matrix mul multiply um and we are not going to do this on just one um",
    "start": "179120",
    "end": "186360"
  },
  {
    "text": "image actually let's do a matrix multiply that processes a 100 images at",
    "start": "186360",
    "end": "191480"
  },
  {
    "text": "a time or any number so at the bottom there you have our 100 images they're",
    "start": "191480",
    "end": "198599"
  },
  {
    "text": "all flattened okay it's one image per line and now let's take our Matrix of",
    "start": "198599",
    "end": "203799"
  },
  {
    "text": "weights and we using the First Column of Weights we do a weighted sum of all the",
    "start": "203799",
    "end": "209879"
  },
  {
    "text": "pixels in the first image okay we have the result of our first neuron now",
    "start": "209879",
    "end": "215360"
  },
  {
    "text": "taking the second um column of Weights we do a weighted sum of all the pixels",
    "start": "215360",
    "end": "221480"
  },
  {
    "text": "in the first image and so on and we have the the weighted sum for the second the third until the 10th",
    "start": "221480",
    "end": "228879"
  },
  {
    "text": "neuron now we finish this Matrix multiply which gives us oh sorry and",
    "start": "228879",
    "end": "235280"
  },
  {
    "text": "first and before that I have to add one more thing which is that",
    "start": "235280",
    "end": "240400"
  },
  {
    "text": "the input of the neuron is the weighted sum but we also add a constant which is",
    "start": "240400",
    "end": "246239"
  },
  {
    "text": "just an additional degree of Freedom it's called a bias there is one bias per neuron okay we will try to find out what",
    "start": "246239",
    "end": "254120"
  },
  {
    "text": "the good value for that bias is later as for the weights so now we have our 10",
    "start": "254120",
    "end": "259160"
  },
  {
    "text": "weighted sums for the 10 neurons we add our 10 biases and then if we want to finish the The Matrix multiply well we",
    "start": "259160",
    "end": "267160"
  },
  {
    "text": "just continue it and we get the um all of the weighted sum for the 99",
    "start": "267160",
    "end": "274560"
  },
  {
    "text": "remaining images now of course we would like to write this with a simple formula like",
    "start": "274560",
    "end": "281400"
  },
  {
    "text": "like here but it's just a little Quirk this the plus here doesn't quite work",
    "start": "281400",
    "end": "286440"
  },
  {
    "text": "you see this Matrix here is 10 * 100 and",
    "start": "286440",
    "end": "291600"
  },
  {
    "text": "this Vector is just 10 elements you can't do a plus between those never mind",
    "start": "291600",
    "end": "297400"
  },
  {
    "text": "let us redefine Plus well actually that is the standard way",
    "start": "297400",
    "end": "303720"
  },
  {
    "text": "uh in which this works in data science and specifically in Python which we will be using today in numpy which is the",
    "start": "303720",
    "end": "310840"
  },
  {
    "text": "most used scientific library in Python uh they they they call it a broadcasting",
    "start": "310840",
    "end": "317840"
  },
  {
    "text": "plus and it's just a slight extension it means if you can't add those matrices",
    "start": "317840",
    "end": "324240"
  },
  {
    "text": "because the the size don't match well you try to replicate this one as as much as possible until it matches if that",
    "start": "324240",
    "end": "331720"
  },
  {
    "text": "works you can do a plus it just so happens that is exactly what we want here those 10 biases we want them added",
    "start": "331720",
    "end": "339479"
  },
  {
    "text": "on each line so let's call it a plus it's a broadcasting plus and we can",
    "start": "339479",
    "end": "346120"
  },
  {
    "text": "write this formula for one layer of a neural",
    "start": "346120",
    "end": "352680"
  },
  {
    "text": "network as a simple expression so let's go through it again here we have our 100",
    "start": "352680",
    "end": "359319"
  },
  {
    "text": "images one image per line each image is 28 by 28 which is 784",
    "start": "359319",
    "end": "366599"
  },
  {
    "text": "pixels we do we multiply by our weights which create for each image our 10",
    "start": "366599",
    "end": "372800"
  },
  {
    "text": "weighted sums so here we have again 784",
    "start": "372800",
    "end": "377919"
  },
  {
    "text": "* 10 weights now on each line we add our 10",
    "start": "377919",
    "end": "383360"
  },
  {
    "text": "biases and line by line we feed this through the softmax function the soft",
    "start": "383360",
    "end": "389840"
  },
  {
    "text": "mat function which line by line I reminder a line is the 10 weighted",
    "start": "389840",
    "end": "398639"
  },
  {
    "text": "sums plus b for our 10 neurons so line by line we take the exponential of all",
    "start": "398639",
    "end": "403720"
  },
  {
    "text": "the values and normalize and that is why",
    "start": "403720",
    "end": "409280"
  },
  {
    "text": "here a matrix of 100 lines one per image and for each image we have the output of",
    "start": "409280",
    "end": "417360"
  },
  {
    "text": "our 10 neurons from uh 0 to 9 which tells us which picture was recognized",
    "start": "417360",
    "end": "425240"
  },
  {
    "text": "which number which digit was recognized uh of course all of this",
    "start": "425240",
    "end": "430639"
  },
  {
    "text": "works only if the biases and the weights are good so how do we know they are good",
    "start": "430639",
    "end": "437599"
  },
  {
    "text": "I haven't spoken at all about how we are going to determine them yet but how do",
    "start": "437599",
    "end": "442840"
  },
  {
    "text": "we know they are good this is how you would write it in",
    "start": "442840",
    "end": "448720"
  },
  {
    "text": "tensor flow in Python and tensor flow you see not very",
    "start": "448720",
    "end": "454400"
  },
  {
    "text": "different so how do we know they are good when we we are going to train this",
    "start": "454400",
    "end": "461360"
  },
  {
    "text": "system and when we train it we give it the handwritten digits but we know what",
    "start": "461360",
    "end": "466560"
  },
  {
    "text": "these digits are we have labels for them and so we are simply going to compare what the neural network is telling us",
    "start": "466560",
    "end": "473159"
  },
  {
    "text": "with what the label says that this was so here is a possible label uh we encod",
    "start": "473159",
    "end": "479159"
  },
  {
    "text": "it like this it's called one hot it's just a vocabulary uh a six is encoded by",
    "start": "479159",
    "end": "484440"
  },
  {
    "text": "putting a one in the sixth box here okay we encode it like this because it looks very much like the predictions that our",
    "start": "484440",
    "end": "491560"
  },
  {
    "text": "system is outputting these are the outputs of our neurons and here as well we expect that one of them will have a",
    "start": "491560",
    "end": "497720"
  },
  {
    "text": "high value and and and and tell us that this is a six so now we have to",
    "start": "497720",
    "end": "503000"
  },
  {
    "text": "calculate a dense distance between those two which frankly could be any distance",
    "start": "503000",
    "end": "509800"
  },
  {
    "text": "uh ukian distance will work okay no problem but again for classification",
    "start": "509800",
    "end": "515800"
  },
  {
    "text": "problems a lot of research has shown that this distance called the cross",
    "start": "515800",
    "end": "521000"
  },
  {
    "text": "entropy is actually a little bit better so that's the distance we will be using",
    "start": "521000",
    "end": "527000"
  },
  {
    "text": "it's these numbers multiply by the logarithm of of these numbers that multiplied by the logarithm of this one",
    "start": "527000",
    "end": "533680"
  },
  {
    "text": "and summed across the vector so this will be our Quality",
    "start": "533680",
    "end": "540279"
  },
  {
    "text": "Control Function it's also called a loss function and",
    "start": "540279",
    "end": "547120"
  },
  {
    "text": "now let's leave mathematics and let's see a demo let's try to uh train a",
    "start": "547120",
    "end": "555079"
  },
  {
    "text": "system so here I'm training this neural network I what you see here are my training digits I'm I'm throwing 100",
    "start": "555079",
    "end": "562279"
  },
  {
    "text": "digits at a time at the system and in red you have those that are badly",
    "start": "562279",
    "end": "567640"
  },
  {
    "text": "recognized but in white you have those that it has already learned how to recognize from the training",
    "start": "567640",
    "end": "574399"
  },
  {
    "text": "digits um you see here the the loss function is going down okay our quality",
    "start": "574399",
    "end": "581279"
  },
  {
    "text": "is going up it's the reverse so this just shows us that the training kind of",
    "start": "581279",
    "end": "586720"
  },
  {
    "text": "works we were seeing this here as well but here we have it Quantified accuracy is simply the percentage of recognized",
    "start": "586720",
    "end": "594440"
  },
  {
    "text": "digits and now we also want to test if",
    "start": "594440",
    "end": "599519"
  },
  {
    "text": "this neural network is actually recognizing something in the real world okay we are not interested in the test",
    "start": "599519",
    "end": "605279"
  },
  {
    "text": "digits that we are in in the training digits we are interested in the digit that I'm going to write tomorrow because",
    "start": "605279",
    "end": "612680"
  },
  {
    "text": "this is supposed to be used for handwritten digit recognition so we have set aside 10,000 additional digits to",
    "start": "612680",
    "end": "620360"
  },
  {
    "text": "test the performance here you only see 1,000 of them so imagine nine more",
    "start": "620360",
    "end": "625920"
  },
  {
    "text": "screens but I sorted the red ones the non-recognized ones at the top okay and",
    "start": "625920",
    "end": "632680"
  },
  {
    "text": "a little scale that tells you how what is the percentage out of the 10,000 of recognized digits and lo and behold with",
    "start": "632680",
    "end": "639920"
  },
  {
    "text": "this one layer the naive approach we are already recognizing 92% of our digits we have",
    "start": "639920",
    "end": "648600"
  },
  {
    "text": "taught the system to recognize more than one in 10 handwritten",
    "start": "648600",
    "end": "653839"
  },
  {
    "text": "digits finally what what is this so I told you this only works if weights and biases are",
    "start": "653839",
    "end": "660839"
  },
  {
    "text": "good so this training mechanism actually changes gradually our weights and biases",
    "start": "660839",
    "end": "668839"
  },
  {
    "text": "uh so that the loss function goes down and after a certain number of iterations",
    "start": "668839",
    "end": "676160"
  },
  {
    "text": "we have quite good performance what you see here on these two graphs uh this is kind of a histogram of all the all the",
    "start": "676160",
    "end": "682279"
  },
  {
    "text": "weights and all the biases in the system you mostly see that they are changing",
    "start": "682279",
    "end": "687519"
  },
  {
    "text": "and you also see that they are not exploding they stay between min-2 and two and min-1 and one over there these",
    "start": "687519",
    "end": "695320"
  },
  {
    "text": "are just two handy graphs to see that the system is not diverging",
    "start": "695320",
    "end": "700920"
  },
  {
    "text": "completely so what we have seen here is How To Train A",
    "start": "700920",
    "end": "706880"
  },
  {
    "text": "system that you need a loss function that you will try to minimize that you need to define a model which has weights",
    "start": "706880",
    "end": "713160"
  },
  {
    "text": "which has three parameters weights and biases that you are trying to adjust to",
    "start": "713160",
    "end": "718680"
  },
  {
    "text": "produce a prediction that is good in terms of the loss function so let's see",
    "start": "718680",
    "end": "725079"
  },
  {
    "text": "how this is actually done",
    "start": "725079",
    "end": "728360"
  },
  {
    "text": "now here we",
    "start": "731000",
    "end": "734360"
  },
  {
    "text": "go let's do this in tensor flow so now we are diving into tensor flow and",
    "start": "736800",
    "end": "743760"
  },
  {
    "text": "python so first in tensor flow you need to Define what is called variables those",
    "start": "743760",
    "end": "749440"
  },
  {
    "text": "are the degrees of freedom of your system is what you are asking the system to determine for you through training so",
    "start": "749440",
    "end": "757079"
  },
  {
    "text": "in our case it's the weights and the biases we say it's",
    "start": "757079",
    "end": "762240"
  },
  {
    "text": "variables we have seen that we have our weight Matrix is 784 by 10 that we have",
    "start": "762240",
    "end": "767639"
  },
  {
    "text": "seen previously our bias Vector is 10 values then during training you will",
    "start": "767639",
    "end": "775720"
  },
  {
    "text": "need to feed in training data for that in tensor flow you define a placeholder",
    "start": "775720",
    "end": "782399"
  },
  {
    "text": "a placeholder means it doesn't have any value values yet I will I will feed",
    "start": "782399",
    "end": "788120"
  },
  {
    "text": "values during training so just look at the size of",
    "start": "788120",
    "end": "793839"
  },
  {
    "text": "this tensor here by the way what is a tensor okay in first approximation it's",
    "start": "793839",
    "end": "800560"
  },
  {
    "text": "just a multi-dimensional matrix that's it and here I have the number of",
    "start": "800560",
    "end": "805920"
  },
  {
    "text": "dimensions of this Matrix so let's start from the end end uh this is the number of values per",
    "start": "805920",
    "end": "812839"
  },
  {
    "text": "pixel okay we have grayscale images so it's one value value per pixel uh it's not needed here but I just put it in",
    "start": "812839",
    "end": "820360"
  },
  {
    "text": "case you had RGB um images this would be three three values per pixel uh our",
    "start": "820360",
    "end": "827560"
  },
  {
    "text": "images are are 28x 28 and this is how many images we will have uh well it's",
    "start": "827560",
    "end": "833639"
  },
  {
    "text": "not known at this point it will be known when we actually feed the images how many we feed at one at one",
    "start": "833639",
    "end": "840199"
  },
  {
    "text": "time all right so variables for free degrees of freedom and placeholders for",
    "start": "840199",
    "end": "846800"
  },
  {
    "text": "whatever data you will feed during training now this is our model this is",
    "start": "846800",
    "end": "852440"
  },
  {
    "text": "what we have seen previously there is only one Quirk reshape you you remember our images 28",
    "start": "852440",
    "end": "858800"
  },
  {
    "text": "by 28 28 but we first have to put all the pixels on one line to go into our",
    "start": "858800",
    "end": "864399"
  },
  {
    "text": "model this is a very simplistic model that is what reshape does okay it flattens everything on one line and we",
    "start": "864399",
    "end": "871839"
  },
  {
    "text": "tell it well I want everything on one line the minus one here will end up being the number of images minus one",
    "start": "871839",
    "end": "878320"
  },
  {
    "text": "means there is only one possible number so figure it out um we also need a placeholder for",
    "start": "878320",
    "end": "885399"
  },
  {
    "text": "our known labels we will be feeding images and labels a picture of an eight",
    "start": "885399",
    "end": "891880"
  },
  {
    "text": "and a label saying this is an eight picture of a seven label saying this is a seven and now we compute our loss",
    "start": "891880",
    "end": "899560"
  },
  {
    "text": "function the cross entropy so we have seen this is this multiply is an elementwise",
    "start": "899560",
    "end": "905440"
  },
  {
    "text": "multiply we multiply the bits of the label by the logarithm of our",
    "start": "905440",
    "end": "912880"
  },
  {
    "text": "predictions okay it's what we have seen previously on this picture",
    "start": "912880",
    "end": "917920"
  },
  {
    "text": "here we multiply the bits of the label by this number label by this number and",
    "start": "917920",
    "end": "924519"
  },
  {
    "text": "we sum across so reduce sum is a sum across the",
    "start": "924519",
    "end": "931880"
  },
  {
    "text": "vector so now we have our L function and this look at it if you if you if you want to understand it's slightly more",
    "start": "931880",
    "end": "939079"
  },
  {
    "text": "complicated this is just the computation of the percentage of correctly recognized images for display okay so I",
    "start": "939079",
    "end": "947680"
  },
  {
    "text": "I won't get into the details of that it's how many images were correctly recognized and now we go to the heart of",
    "start": "947680",
    "end": "956160"
  },
  {
    "text": "how neural networks work we take we we tell tensor flow well please use this",
    "start": "956160",
    "end": "962800"
  },
  {
    "text": "Optimizer and optimize my loss function what is this going to",
    "start": "962800",
    "end": "968279"
  },
  {
    "text": "do well this loss function we have seen it it depends from our weights our",
    "start": "968279",
    "end": "973720"
  },
  {
    "text": "biases and also from our uh images so what this what what the",
    "start": "973720",
    "end": "981240"
  },
  {
    "text": "general learning algorithm does is that it is going to compute the partial",
    "start": "981240",
    "end": "986560"
  },
  {
    "text": "derivative of your loss function relatively to all the weights and all the",
    "start": "986560",
    "end": "991880"
  },
  {
    "text": "biases now thank you tensorflow for doing this for me because remember how",
    "start": "991880",
    "end": "997720"
  },
  {
    "text": "many weights we had 7,000 and something so this is a vector of partial",
    "start": "997720",
    "end": "1003680"
  },
  {
    "text": "derivatives which has 7,000 values okay so it's it's a good thing that tensor",
    "start": "1003680",
    "end": "1010000"
  },
  {
    "text": "flow does this derivation for me and it's it's also a very good thing because it's a very eror prone thing to actually",
    "start": "1010000",
    "end": "1018279"
  },
  {
    "text": "implement by hand and it's it's a real um formal",
    "start": "1018279",
    "end": "1023360"
  },
  {
    "text": "derivation it's not a numerical derivation it's a real formal uh derivation so this Vector of partial",
    "start": "1023360",
    "end": "1030120"
  },
  {
    "text": "derivatives what is it it's called the gradient and the nice property of a gradient is to",
    "start": "1030120",
    "end": "1037240"
  },
  {
    "text": "point point where somebody knows where are the the gradient",
    "start": "1037240",
    "end": "1043438"
  },
  {
    "text": "points up we want to go down so we put a minus sign on it and that's our Direction",
    "start": "1043439",
    "end": "1050880"
  },
  {
    "text": "where to go when I say Direction where are we in which space are we we are in the space of weights and biases so if",
    "start": "1050880",
    "end": "1057600"
  },
  {
    "text": "the gradient tells us well this is the direction in which you you you want to go to minimize your cross entropy it",
    "start": "1057600",
    "end": "1063640"
  },
  {
    "text": "means it just gave me all the small Deltas to add to my weights and biases to go to a region where the loss is",
    "start": "1063640",
    "end": "1070600"
  },
  {
    "text": "smaller so let's add those deltas and go there and repeat and that is the training compute gradient follow",
    "start": "1070600",
    "end": "1078000"
  },
  {
    "text": "gradient which modifies weights and biases compute uh pumping next set of",
    "start": "1078000",
    "end": "1084000"
  },
  {
    "text": "images of of training images and training labels compute gradient uh follow gradient and so on just one last",
    "start": "1084000",
    "end": "1091840"
  },
  {
    "text": "Quirk is this one here if you jump by the whole gradient",
    "start": "1091840",
    "end": "1097600"
  },
  {
    "text": "that's a bit too much uh you you to take a metaphor imagine you're in the",
    "start": "1097600",
    "end": "1103320"
  },
  {
    "text": "mountains uh somewhere on on the top of the mountain and you're heading for the bottom of the valley now if you have",
    "start": "1103320",
    "end": "1110159"
  },
  {
    "text": "seven leag boots and and you you make huge strides you will be jumping from",
    "start": "1110159",
    "end": "1115280"
  },
  {
    "text": "one side of the valley to the other without reaching the bottom so you need to go slow even if you know where the",
    "start": "1115280",
    "end": "1121679"
  },
  {
    "text": "direction of steepest descent is you need to go slow to actually reach the bottom so this is just something by",
    "start": "1121679",
    "end": "1129360"
  },
  {
    "text": "which you multiply the gradient before adding it to to your weights and biases in order to go slow it's called a",
    "start": "1129360",
    "end": "1136039"
  },
  {
    "text": "learning rate all",
    "start": "1136039",
    "end": "1141000"
  },
  {
    "text": "right what next well now we have to write the training Loop so one thing to",
    "start": "1141200",
    "end": "1147320"
  },
  {
    "text": "understand here about tensorflow is that tensorflow has a deferred execution model all right everything we have",
    "start": "1147320",
    "end": "1154840"
  },
  {
    "text": "written so far everything that starts with TF something uh when it is executed it's",
    "start": "1154840",
    "end": "1162120"
  },
  {
    "text": "not actually producing result it's just building a computation graph in memory",
    "start": "1162120",
    "end": "1168000"
  },
  {
    "text": "when you want to actually run this computation graph you have to start a session but over there session and then",
    "start": "1168000",
    "end": "1175840"
  },
  {
    "text": "to compute a piece of a graph you you go session. run and one Noe of your graph",
    "start": "1175840",
    "end": "1183039"
  },
  {
    "text": "so what's the purpose of this the purpose U is not to make your life difficult the purpose is to allow uh",
    "start": "1183039",
    "end": "1191200"
  },
  {
    "text": "distributed training tensor flow was built with distributed execution in mind",
    "start": "1191200",
    "end": "1196679"
  },
  {
    "text": "so that's why it has to know everything about your computation graph beforehand",
    "start": "1196679",
    "end": "1201720"
  },
  {
    "text": "before you start working because then it can do magic plus boring logistics for",
    "start": "1201720",
    "end": "1208640"
  },
  {
    "text": "you when you distribute the work to different nodes one example of something",
    "start": "1208640",
    "end": "1213679"
  },
  {
    "text": "that it knows how to do if if you tell it to to do one piece of computation here and another one there since it",
    "start": "1213679",
    "end": "1220400"
  },
  {
    "text": "knows the graph it knows which data has to be shuffled between those two",
    "start": "1220400",
    "end": "1225600"
  },
  {
    "text": "servers and the graph is also useful to do the ACT ual U formal uh gradient",
    "start": "1225600",
    "end": "1231400"
  },
  {
    "text": "computation as well so let's start our session let's start our Loop and let's",
    "start": "1231400",
    "end": "1238640"
  },
  {
    "text": "do the training Loop so here I'm loading 100 additional training images and",
    "start": "1238640",
    "end": "1244080"
  },
  {
    "text": "labels I'm running the train step what is the train step you remember what is",
    "start": "1244080",
    "end": "1249120"
  },
  {
    "text": "the train step it's what we got here in when we this is when we asked an",
    "start": "1249120",
    "end": "1255039"
  },
  {
    "text": "Optimizer to minimize our cross entropy we got a train step so the train step is",
    "start": "1255039",
    "end": "1260280"
  },
  {
    "text": "actually the thing that computes a gradient takes a fraction of the gradient adds it to to our weights and",
    "start": "1260280",
    "end": "1268320"
  },
  {
    "text": "biases that's what it does",
    "start": "1268320",
    "end": "1273480"
  },
  {
    "text": "so so you don't you Loop you you you you Loop until you're",
    "start": "1275720",
    "end": "1282760"
  },
  {
    "text": "happy with the result it's not infinite you you will see actually what we have seen previously here is that very",
    "start": "1282760",
    "end": "1288840"
  },
  {
    "text": "quickly you get into diminishing returns and you see that it's not worth uh",
    "start": "1288840",
    "end": "1294440"
  },
  {
    "text": "looping anymore uh you see on this example I had my cross entropy in the",
    "start": "1294440",
    "end": "1299799"
  },
  {
    "text": "first 100 iteration this is going down quite nicely and then well it's no point",
    "start": "1299799",
    "end": "1306360"
  },
  {
    "text": "iterating more actually from here on I will keep this keep this going for 2000",
    "start": "1306360",
    "end": "1312279"
  },
  {
    "text": "iteration could have been 20,000 but you you see on the curves that it flattens out so at one point you",
    "start": "1312279",
    "end": "1321120"
  },
  {
    "text": "stop so we run this uh training step and then everything else here is just for",
    "start": "1326320",
    "end": "1333559"
  },
  {
    "text": "display so we compute the percentage of correctly recognized images the crossentropy so that we can put it on",
    "start": "1333559",
    "end": "1339720"
  },
  {
    "text": "the screen but the core of the algorithm is this one load 100 training images and",
    "start": "1339720",
    "end": "1345679"
  },
  {
    "text": "labels run one training step and that makes the weights and biases evolve and we put stuff on the screen to see if it",
    "start": "1345679",
    "end": "1352600"
  },
  {
    "text": "is evolving in the right direction okay so here is our basic",
    "start": "1352600",
    "end": "1357760"
  },
  {
    "text": "model on one screen initialization variables for degrees of",
    "start": "1357760",
    "end": "1363760"
  },
  {
    "text": "freedom placeholders for your training data here our model here our loss",
    "start": "1363760",
    "end": "1370840"
  },
  {
    "text": "function uh this is the computation of the accuracy the percentage of correctly recognized images over there our",
    "start": "1370840",
    "end": "1379200"
  },
  {
    "text": "Optimizer and we ask it to Mo to to optimize the loss function and here in",
    "start": "1379200",
    "end": "1385640"
  },
  {
    "text": "this Loop we take training images and labels and run the training step which",
    "start": "1385640",
    "end": "1391520"
  },
  {
    "text": "makes our weights and biases evolve until our system is capable of recognizing a handwritten digits with a",
    "start": "1391520",
    "end": "1398799"
  },
  {
    "text": "92% accuracy all",
    "start": "1398799",
    "end": "1403880"
  },
  {
    "text": "right so we've seen some of some of the things that go into the PO when designing a neural network uh the the",
    "start": "1403880",
    "end": "1411240"
  },
  {
    "text": "crossentropy which is the usual uh loss function for classification problem softmax which is the usual activation",
    "start": "1411240",
    "end": "1418240"
  },
  {
    "text": "function for a classification problem and mini batching as we've seen here we",
    "start": "1418240",
    "end": "1423520"
  },
  {
    "text": "are feeding the uh training images 100 at a time but is 92% really",
    "start": "1423520",
    "end": "1431360"
  },
  {
    "text": "good come on it's awful if you were to use this in in in an everyday system I",
    "start": "1431360",
    "end": "1437799"
  },
  {
    "text": "don't know in the post Stu is to recognize um uh postal codes well you're",
    "start": "1437799",
    "end": "1444320"
  },
  {
    "text": "missing almost one digit out of 10 how terrible can this be we have to we have",
    "start": "1444320",
    "end": "1449640"
  },
  {
    "text": "to do this a lot better so the it was called Deep learning so yeah let's go",
    "start": "1449640",
    "end": "1454880"
  },
  {
    "text": "deep how do we go deep we simply add layers so the way to add layers into a",
    "start": "1454880",
    "end": "1461600"
  },
  {
    "text": "neural network is to stack them and here you have seen these neurons do weighted",
    "start": "1461600",
    "end": "1468840"
  },
  {
    "text": "sum of all the pixels well it's equally simple for a second layer of neurons to do a weighted sum of the outputs of the",
    "start": "1468840",
    "end": "1475919"
  },
  {
    "text": "previous layer so that's how you will stack uh layers in a neural",
    "start": "1475919",
    "end": "1481120"
  },
  {
    "text": "network now we will change the activation function for the intermediate",
    "start": "1481120",
    "end": "1486679"
  },
  {
    "text": "layers actually softmax is really good only on the last layer uh and the the",
    "start": "1486679",
    "end": "1492600"
  },
  {
    "text": "very usual activation function is is the sigmoid which is a pretty boring",
    "start": "1492600",
    "end": "1497679"
  },
  {
    "text": "function that go goes from 0 to one uh but that is nonlinear that's the main property we're looking",
    "start": "1497679",
    "end": "1504200"
  },
  {
    "text": "for all right so how do we write this in tensor flow well first of all we will need one",
    "start": "1504200",
    "end": "1512120"
  },
  {
    "text": "weight Matrix and one bias Vector per layer okay and uh it's actually good",
    "start": "1512120",
    "end": "1520279"
  },
  {
    "text": "practice to initialize those weights with random values so truncated normal",
    "start": "1520279",
    "end": "1527279"
  },
  {
    "text": "is just a complicated word to say random so we have our for our inter for",
    "start": "1527279",
    "end": "1534240"
  },
  {
    "text": "all of our layers we have a weight Matrix and a bias vector and now this is",
    "start": "1534240",
    "end": "1539840"
  },
  {
    "text": "what our model is going to look like maybe you recognize this first one",
    "start": "1539840",
    "end": "1545520"
  },
  {
    "text": "okay that's exactly the formula we had previously with X here the inputs this",
    "start": "1545520",
    "end": "1550799"
  },
  {
    "text": "is our Matrix holding our 100 images well now on the second line we do",
    "start": "1550799",
    "end": "1556240"
  },
  {
    "text": "exactly the same thing but as the input we take the output of the previous layer",
    "start": "1556240",
    "end": "1561880"
  },
  {
    "text": "and that's it of course we use the correct activation function so sigmoid on the",
    "start": "1561880",
    "end": "1569200"
  },
  {
    "text": "intermediate L layers and soft Max on the outer layer uh to produce something",
    "start": "1569200",
    "end": "1575480"
  },
  {
    "text": "that looks like you know a probability uh so that we can classify our",
    "start": "1575480",
    "end": "1581480"
  },
  {
    "text": "images apart from this I'm not changing anything else to the code uh the cross entropy remains as it is the the",
    "start": "1581480",
    "end": "1588520"
  },
  {
    "text": "training Loop remains as it is I just changed the model and I initialized the the additional weights and biases that",
    "start": "1588520",
    "end": "1594760"
  },
  {
    "text": "go into this model so if we launch this first of all",
    "start": "1594760",
    "end": "1601919"
  },
  {
    "text": "you will realize that the starts okay it's only 300 iterations the start is a",
    "start": "1601919",
    "end": "1607039"
  },
  {
    "text": "little bit slow and it's not so bad on this example but actually as you go deep",
    "start": "1607039",
    "end": "1612760"
  },
  {
    "text": "the the sigmoid activation function I mentioned it for historical reasons",
    "start": "1612760",
    "end": "1617799"
  },
  {
    "text": "because it's really the historical activation function for neural networks but as we go deep it's no good and",
    "start": "1617799",
    "end": "1625039"
  },
  {
    "text": "people invented a new one called re which is actually super simple",
    "start": "1625039",
    "end": "1633360"
  },
  {
    "text": "F yes can you hear me in the back yes so",
    "start": "1634559",
    "end": "1640279"
  },
  {
    "text": "this activation function is simply zero for all negative values and identity for all positive values how much simpler",
    "start": "1640279",
    "end": "1647679"
  },
  {
    "text": "could it be uh for the little story actually both of those activation functions sigmoid and",
    "start": "1647679",
    "end": "1653960"
  },
  {
    "text": "this one uh come from biology and it was thought that biological neurons were",
    "start": "1653960",
    "end": "1661559"
  },
  {
    "text": "actually working like this so if all the synapses",
    "start": "1661559",
    "end": "1666960"
  },
  {
    "text": "were pumping in signal from one threshold of signal it it would start",
    "start": "1666960",
    "end": "1672159"
  },
  {
    "text": "puming something on its outputs well now biologists think that biological neurons",
    "start": "1672159",
    "end": "1680519"
  },
  {
    "text": "are working more more like this so this is all biology inspired but we actually",
    "start": "1680519",
    "end": "1685600"
  },
  {
    "text": "don't care about biology this actually works better for deep neural",
    "start": "1685600",
    "end": "1691480"
  },
  {
    "text": "nence why well we will certainly see plenty of Articles explaining why the",
    "start": "1691480",
    "end": "1697720"
  },
  {
    "text": "reality is that someone tried it and it worked",
    "start": "1697720",
    "end": "1701960"
  },
  {
    "text": "bad okay no problem so that's the first thing we do and as you use uh this U alternative uh",
    "start": "1702919",
    "end": "1711960"
  },
  {
    "text": "activation function you see that the start of our learning is much faster and",
    "start": "1711960",
    "end": "1717039"
  },
  {
    "text": "actually the the cross entropy goes down much quicker so that's one thing that we can do then let's push this to 10,000",
    "start": "1717039",
    "end": "1725159"
  },
  {
    "text": "iterations so first of all look at it 98% accuracy we jumped from 92 to9 and",
    "start": "1725159",
    "end": "1732840"
  },
  {
    "text": "this is fantastic but those curves look really",
    "start": "1732840",
    "end": "1738440"
  },
  {
    "text": "really ugly really noisy and you see here the the red curve which is the",
    "start": "1738440",
    "end": "1743960"
  },
  {
    "text": "accuracy on our test data the the only thing that we actually care about",
    "start": "1743960",
    "end": "1749200"
  },
  {
    "text": "actually goes up and down by a full percent that's not good this is actually",
    "start": "1749200",
    "end": "1754720"
  },
  {
    "text": "a sign of what what I was talking about with the Seven League Boots we are going too fast we're jumping across the valley",
    "start": "1754720",
    "end": "1762200"
  },
  {
    "text": "that we are trying to find the bottom of so we could go slower but that would",
    "start": "1762200",
    "end": "1768880"
  },
  {
    "text": "mean 10 times slower would mean that our learning 10 uh takes 10 times more time",
    "start": "1768880",
    "end": "1775559"
  },
  {
    "text": "we don't want that so the good approach is actually to um gradually decrease the",
    "start": "1775559",
    "end": "1783480"
  },
  {
    "text": "learning rate as we go and let me show you the result of",
    "start": "1783480",
    "end": "1788640"
  },
  {
    "text": "that it's quite spectacular this is what you get without decreasing the learning",
    "start": "1788640",
    "end": "1795039"
  },
  {
    "text": "rate this is what you get with learning rate DK okay it's a small trick but it's",
    "start": "1795039",
    "end": "1800679"
  },
  {
    "text": "it's spectacular how good it makes things all the noise is gone and look at the blue curve look at it this is our",
    "start": "1800679",
    "end": "1806840"
  },
  {
    "text": "training accuracy it's stuck at 100% for thousands of iterations here towards the",
    "start": "1806840",
    "end": "1812720"
  },
  {
    "text": "end for the first time we have designed a neural network that has learned all the training examples and we have 60,000",
    "start": "1812720",
    "end": "1820919"
  },
  {
    "text": "of them so it has learned all the 60,000 training examples perfectly it doesn't",
    "start": "1820919",
    "end": "1827000"
  },
  {
    "text": "mean that it knows how to recognize all real world handwritten digits but at",
    "start": "1827000",
    "end": "1832240"
  },
  {
    "text": "least on training examples now it's perfect um what else well the other",
    "start": "1832240",
    "end": "1839360"
  },
  {
    "text": "thing you see is is this here kind of weird the the loss is is going down on",
    "start": "1839360",
    "end": "1846440"
  },
  {
    "text": "Test example examples the ones we care about on training examples is going all",
    "start": "1846440",
    "end": "1851720"
  },
  {
    "text": "the way down well that is normal we are actually optimizing this one we are actively optimizing this one we are not",
    "start": "1851720",
    "end": "1858039"
  },
  {
    "text": "actively optimizing this one okay the algorithm is only working on training examples it never touches test examples",
    "start": "1858039",
    "end": "1865720"
  },
  {
    "text": "so it's kind of a good thing that the test performance follows but un without",
    "start": "1865720",
    "end": "1872960"
  },
  {
    "text": "surprise at one point it stops following and goes and disconnects from the",
    "start": "1872960",
    "end": "1879960"
  },
  {
    "text": "training performance that is a a bit of a problem",
    "start": "1879960",
    "end": "1885799"
  },
  {
    "text": "it's a problem that people usually call overfitting and if you look up a a manual uh people will tell you well you",
    "start": "1885799",
    "end": "1893039"
  },
  {
    "text": "need normalization and the standard normalization is called Dropout and Dropout is actually a really",
    "start": "1893039",
    "end": "1901080"
  },
  {
    "text": "brutal technique that works like this you take your neural network and during training",
    "start": "1901080",
    "end": "1907880"
  },
  {
    "text": "at each in each round of training you are actually going to shoot a certain percentage of your neurons to discard",
    "start": "1907880",
    "end": "1916120"
  },
  {
    "text": "them from your model so you do this at random at each round",
    "start": "1916120",
    "end": "1921679"
  },
  {
    "text": "you take a probability for example here 25% probability of a neuron dying and at",
    "start": "1921679",
    "end": "1928639"
  },
  {
    "text": "each round in the iteration you shoot 25% of your neurons of course when you",
    "start": "1928639",
    "end": "1934919"
  },
  {
    "text": "evaluate the final performance of your system you put them all back okay you",
    "start": "1934919",
    "end": "1940480"
  },
  {
    "text": "you don't evaluate on a half uh Dead uh brain",
    "start": "1940480",
    "end": "1945919"
  },
  {
    "text": "so this technique uh this is how you write it in tensor flow so there is a",
    "start": "1945919",
    "end": "1952200"
  },
  {
    "text": "handy function to that takes the outputs of the layer and when you add Dropout what it simply does that it replaces",
    "start": "1952200",
    "end": "1959480"
  },
  {
    "text": "some of those outputs by zeros and it boosts the remaining ones by something",
    "start": "1959480",
    "end": "1966399"
  },
  {
    "text": "that's just so that to make sure that the the the total amount of activation",
    "start": "1966399",
    "end": "1971960"
  },
  {
    "text": "that flows from one layer to the next is not changed but well that's just a detail so so what is the effect of this",
    "start": "1971960",
    "end": "1981080"
  },
  {
    "text": "so let's see from the very beginning we had our sigmoid activation function okay",
    "start": "1981080",
    "end": "1988600"
  },
  {
    "text": "um we which we replace by the relu so faster start and we actually got a",
    "start": "1988600",
    "end": "1994240"
  },
  {
    "text": "little bit more performance uh for it in terms of percentage of recognized",
    "start": "1994240",
    "end": "1999679"
  },
  {
    "text": "images then we added the learning rate Decay which cleaned up all these curves",
    "start": "1999679",
    "end": "2005480"
  },
  {
    "text": "and actually helped with the the accuracy as well we were at",
    "start": "2005480",
    "end": "2010760"
  },
  {
    "text": "98.2 Peak and we are now at 98.2 sustained we still have this problem",
    "start": "2010760",
    "end": "2017919"
  },
  {
    "text": "here so let us add Dropout",
    "start": "2017919",
    "end": "2023279"
  },
  {
    "text": "boom Dropout actually tamed our loss test",
    "start": "2023279",
    "end": "2028320"
  },
  {
    "text": "loss curve here which is good that's what it is for according to the textbooks but actually",
    "start": "2028320",
    "end": "2035399"
  },
  {
    "text": "in this case well it did didn't have really an effect on the",
    "start": "2035399",
    "end": "2041240"
  },
  {
    "text": "accuracy so it was a good try but now let's try to",
    "start": "2041240",
    "end": "2046440"
  },
  {
    "text": "revisit um what overfitting actually means so if you ask the Specialists this",
    "start": "2046440",
    "end": "2053560"
  },
  {
    "text": "is what they will tell you overfitting at the most at the core is that a neural",
    "start": "2053560",
    "end": "2060040"
  },
  {
    "text": "network has so many degrees of freedom so many neurons that you could imagine",
    "start": "2060040",
    "end": "2065760"
  },
  {
    "text": "to store your entire training data set in those degrees of freedom and and just",
    "start": "2065760",
    "end": "2071158"
  },
  {
    "text": "recognize that using a kind of pattern magic of course if the neural network",
    "start": "2071159",
    "end": "2076560"
  },
  {
    "text": "ends up doing that and it will end up doing that if it has if it has too many degrees of freedom it will be absolutely",
    "start": "2076560",
    "end": "2083118"
  },
  {
    "text": "pathetic in terms of accuracy and performance on real world data so in a",
    "start": "2083119",
    "end": "2088440"
  },
  {
    "text": "neural network you actually want to constrain the degrees of freedom to force the neural network to create",
    "start": "2088440",
    "end": "2095480"
  },
  {
    "text": "internally generalizations things that describe the training data and that also generalize",
    "start": "2095480",
    "end": "2102720"
  },
  {
    "text": "to data it hasn't seen before you need to restrain the number of degrees of freedom um the parallel of that is even",
    "start": "2102720",
    "end": "2110560"
  },
  {
    "text": "if you have a small network if you have very little data it will it will still be able to store it entirely so you",
    "start": "2110560",
    "end": "2117880"
  },
  {
    "text": "always need a lot of data to train your networks on and then if you did everything right",
    "start": "2117880",
    "end": "2125280"
  },
  {
    "text": "like here okay um we I actually designed this network um",
    "start": "2125280",
    "end": "2132000"
  },
  {
    "text": "with maybe too a little bit too many layers but it's not that bad I have plenty of data 60,000 training um",
    "start": "2132000",
    "end": "2141520"
  },
  {
    "text": "images and I added drop out which kind of assures me that even if I had a",
    "start": "2141520",
    "end": "2147520"
  },
  {
    "text": "little bit too many degrees of freedom drop out the way it works the fact that you are killing neurons in the middle it",
    "start": "2147520",
    "end": "2154359"
  },
  {
    "text": "kind of prevents it from using those degrees of freedom too bad badly and ensures that as we have",
    "start": "2154359",
    "end": "2162520"
  },
  {
    "text": "seen the overfitting so the disconnect between the test loss and the training loss is kind of tamed so we've done",
    "start": "2162520",
    "end": "2170839"
  },
  {
    "text": "everything we could and we are still stuck at 98%",
    "start": "2170839",
    "end": "2177760"
  },
  {
    "text": "performance the only remaining possible explanation is that our network is is",
    "start": "2177760",
    "end": "2184520"
  },
  {
    "text": "bad so why is that anybody here knows has identified",
    "start": "2184520",
    "end": "2191839"
  },
  {
    "text": "something that we did at the very beginning that was completely stupid we did something dumber than",
    "start": "2191839",
    "end": "2200000"
  },
  {
    "text": "Earth exactly what is an image what is a number it's it's it's a shape it's lines",
    "start": "2200000",
    "end": "2206000"
  },
  {
    "text": "it's circles it's it's a shape and we just took all the pixels and and put them in one bag without any order any",
    "start": "2206000",
    "end": "2212440"
  },
  {
    "text": "any idea of shape we just threw away all the shape information that's terrible and that's why it's not it's not doing",
    "start": "2212440",
    "end": "2218720"
  },
  {
    "text": "well here so exactly we need to design a neural network that will",
    "start": "2218720",
    "end": "2224079"
  },
  {
    "text": "actually uh use the shape information that we have in our images and those are",
    "start": "2224079",
    "end": "2229319"
  },
  {
    "text": "called convolutional neural networks they they work like this so here I'm back to the general example of a of of a",
    "start": "2229319",
    "end": "2236079"
  },
  {
    "text": "color image that's why I have three layers of data in my image and now one neuron will do a",
    "start": "2236079",
    "end": "2244319"
  },
  {
    "text": "weighted sum of only a small p patch of pixels above it okay and now this neuron",
    "start": "2244319",
    "end": "2252880"
  },
  {
    "text": "I'm going to slide it over the picture so the first difference with",
    "start": "2252880",
    "end": "2258240"
  },
  {
    "text": "what we have seen previously is that one neuron does a weighted sum of only a small patch the second difference is",
    "start": "2258240",
    "end": "2265079"
  },
  {
    "text": "that I'm reusing the same weights for all the neurons it's just one little",
    "start": "2265079",
    "end": "2271040"
  },
  {
    "text": "patch of Weights which I'm sliding across the picture to produce my weighted sums okay",
    "start": "2271040",
    "end": "2278720"
  },
  {
    "text": "so now let's slide it over the whole picture of course I need some padding on",
    "start": "2278720",
    "end": "2284079"
  },
  {
    "text": "the sides and but in the end I obtain as many weighted sums I as I had",
    "start": "2284079",
    "end": "2292079"
  },
  {
    "text": "pixels in my original image okay so we could say this is the",
    "start": "2292079",
    "end": "2298480"
  },
  {
    "text": "way of doing a convolutional neural network but we have one problem here look how many weights we have so 4 by 4",
    "start": "2298480",
    "end": "2307720"
  },
  {
    "text": "by 3 which is exactly 48 we had 7,000 degrees of freedom",
    "start": "2307720",
    "end": "2313760"
  },
  {
    "text": "previously 48 that's not going to work we need more degrees of freedom so we",
    "start": "2313760",
    "end": "2319119"
  },
  {
    "text": "will simply do this the same operation again using a different set of weights",
    "start": "2319119",
    "end": "2324680"
  },
  {
    "text": "and obtain a second layer of of weighted sums and since we are working with tensors those two tensors can be written",
    "start": "2324680",
    "end": "2332440"
  },
  {
    "text": "as one simply by adding a dimension here uh here where zero will be for this one",
    "start": "2332440",
    "end": "2338160"
  },
  {
    "text": "and one will be for this one and we have here the general shape of the weight",
    "start": "2338160",
    "end": "2346119"
  },
  {
    "text": "tensor for a convolutional layer in a neural network so let's look at it the first",
    "start": "2346119",
    "end": "2355680"
  },
  {
    "text": "two numbers is 4x4 that's the size of the of the patch and the last two",
    "start": "2355680",
    "end": "2362920"
  },
  {
    "text": "numbers are three and two that's the number of channels in the input and the",
    "start": "2362920",
    "end": "2368359"
  },
  {
    "text": "number of channels in the output so now it's very general it means that now we can actually stack",
    "start": "2368359",
    "end": "2375880"
  },
  {
    "text": "them let's stack them but we still need to to to solve one problem well at the",
    "start": "2375880",
    "end": "2383119"
  },
  {
    "text": "very bottom of the stack we want 10 numbers okay so we need to boil this",
    "start": "2383119",
    "end": "2389280"
  },
  {
    "text": "information down in some way and so the the traditional way of of doing this",
    "start": "2389280",
    "end": "2394800"
  },
  {
    "text": "boiling down was to add a subsampling layer um which is not used anymore but",
    "start": "2394800",
    "end": "2400599"
  },
  {
    "text": "it's it's useful to understand because it gives you the idea uh the idea is that you know this little patch of",
    "start": "2400599",
    "end": "2407720"
  },
  {
    "text": "Weights will evolve to become some kind of shape recognizer the weights will will evolve to produce a high output for",
    "start": "2407720",
    "end": "2415800"
  },
  {
    "text": "a horizontal line and for another patch they will evolve to produce a high output for a vertical line or for a",
    "start": "2415800",
    "end": "2422119"
  },
  {
    "text": "small circle and what you do to to boil the",
    "start": "2422119",
    "end": "2427880"
  },
  {
    "text": "information down is you take those values by groups of four 2 by two and",
    "start": "2427880",
    "end": "2434040"
  },
  {
    "text": "you only keep the maximum one which tells you well here above this one I've",
    "start": "2434040",
    "end": "2439240"
  },
  {
    "text": "reacted strongly to the shape I'm tuned to recognize so you just take the maximum one and you you you get that",
    "start": "2439240",
    "end": "2445760"
  },
  {
    "text": "information that one of those shapes is present above well so that was subsampling as it",
    "start": "2445760",
    "end": "2452800"
  },
  {
    "text": "was done before today people realize that if you play with the stride of the",
    "start": "2452800",
    "end": "2459440"
  },
  {
    "text": "sliding of the convolution you get exactly the same result and you can do convolutional networks with only",
    "start": "2459440",
    "end": "2465880"
  },
  {
    "text": "convolutional layers okay playing with the stride means that if you slide the",
    "start": "2465880",
    "end": "2471960"
  },
  {
    "text": "patches not pixel by pixel but two pixels by two pixels you mechanically",
    "start": "2471960",
    "end": "2477040"
  },
  {
    "text": "obtain twice less points well four times less points in the output so that is other way of boiling the information",
    "start": "2477040",
    "end": "2484000"
  },
  {
    "text": "down and that's what we will do so this this is the network that we are going to build we have on the top there we have",
    "start": "2484000",
    "end": "2491920"
  },
  {
    "text": "our image now it's back to a grayscale image our real example we apply a first",
    "start": "2491920",
    "end": "2498440"
  },
  {
    "text": "patch of 5x five and we actually apply four of those patches so you see 5 by",
    "start": "2498440",
    "end": "2504520"
  },
  {
    "text": "five one channel as an input because it's a grayscale image and I I apply four of those patches which means I",
    "start": "2504520",
    "end": "2510880"
  },
  {
    "text": "obtain four channels of values here of weighted sums the next",
    "start": "2510880",
    "end": "2517760"
  },
  {
    "text": "layer now let's go for a patch of 4x4 I had four channels in the input here and",
    "start": "2517760",
    "end": "2524760"
  },
  {
    "text": "let's let's apply eight of those patches so I will have eight channels in the in in the output with a stride of two which",
    "start": "2524760",
    "end": "2532480"
  },
  {
    "text": "means that instead of having a 28x 28 array of values I end up with a 14x 14",
    "start": "2532480",
    "end": "2538760"
  },
  {
    "text": "array of values here and a third Channel now let's keep the patch size at 4x4 I",
    "start": "2538760",
    "end": "2545480"
  },
  {
    "text": "have eight channels here so I put an here and I I I I do this 12 times so I I",
    "start": "2545480",
    "end": "2551000"
  },
  {
    "text": "will have 12 output channels again a stride of two which means that from 14",
    "start": "2551000",
    "end": "2557400"
  },
  {
    "text": "by 14 I go down to 7 by S and now I apply a fully connected layer so a fully",
    "start": "2557400",
    "end": "2564160"
  },
  {
    "text": "connected layer is what we have seen in the previous section one neuron in a fully connected",
    "start": "2564160",
    "end": "2570720"
  },
  {
    "text": "layer okay does a weighted sum of all the values in this cube of values and",
    "start": "2570720",
    "end": "2577200"
  },
  {
    "text": "the next one does the same and so on that is a fully connected layer it's what we have seen in the first section",
    "start": "2577200",
    "end": "2583359"
  },
  {
    "text": "of uh of this talk so let's see if this does something",
    "start": "2583359",
    "end": "2590839"
  },
  {
    "text": "better uh well first we need to implement it so one uh weight Matrix one bias",
    "start": "2590839",
    "end": "2600240"
  },
  {
    "text": "Vector per layer the weight matrices have the shape that we have been",
    "start": "2600240",
    "end": "2605559"
  },
  {
    "text": "explaining size of the p PCH number of input channels number of output channels okay the output and input channels have",
    "start": "2605559",
    "end": "2611720"
  },
  {
    "text": "to match here k k l l m and so on so I have three of",
    "start": "2611720",
    "end": "2617760"
  },
  {
    "text": "those then I have my fully connected layer okay my fully connected layer the",
    "start": "2617760",
    "end": "2624040"
  },
  {
    "text": "the the size of its uh weight Matrix is inputs multiplied by outputs so in the",
    "start": "2624040",
    "end": "2632000"
  },
  {
    "text": "inputs here I had that was on the little drawing before",
    "start": "2632000",
    "end": "2637720"
  },
  {
    "text": "I had 7 by 7 by 12 values to read these values this little Cube and in the",
    "start": "2637720",
    "end": "2644319"
  },
  {
    "text": "outputs well I decide how many neurons I want so here I chose 2,00 200 so I put",
    "start": "2644319",
    "end": "2651880"
  },
  {
    "text": "200 neurons in n and then I have my last layer which are my 10 Soft Max neurons",
    "start": "2651880",
    "end": "2658760"
  },
  {
    "text": "to actually produce a prediction those are the initializations",
    "start": "2658760",
    "end": "2664559"
  },
  {
    "text": "and now here is the model itself so tensor flow has a handy conf 2D function",
    "start": "2664559",
    "end": "2671040"
  },
  {
    "text": "into which you can pass uh the the weights the weight tensor for a convolutional layer and it will do the",
    "start": "2671040",
    "end": "2678800"
  },
  {
    "text": "sliding in two directions it's just a double Loop that's all it does it does the sliding in both directions across",
    "start": "2678800",
    "end": "2684720"
  },
  {
    "text": "your inputs uh don't mind the complex um",
    "start": "2684720",
    "end": "2690359"
  },
  {
    "text": "Syntax for the strides okay I highlighted in red the two numbers that need to be one or two to get a stride of",
    "start": "2690359",
    "end": "2698280"
  },
  {
    "text": "one or two you can read the documentation about the other numbers uh the padding padding strategy so you",
    "start": "2698280",
    "end": "2704760"
  },
  {
    "text": "remember on the sides we need to extend our images a little bit uh but well here they are on a white background so we add",
    "start": "2704760",
    "end": "2711359"
  },
  {
    "text": "more white background that's called same and these are our three",
    "start": "2711359",
    "end": "2717319"
  },
  {
    "text": "convolutional layers then we have reshape why is the reshaped there well",
    "start": "2717319",
    "end": "2722680"
  },
  {
    "text": "it's this one you know we have this cube of values here and we need to put it all",
    "start": "2722680",
    "end": "2727839"
  },
  {
    "text": "in one line so that these neurons can uh use them as inputs so that is the",
    "start": "2727839",
    "end": "2736319"
  },
  {
    "text": "reshape okay reshape here and then the same formula as we had previously for",
    "start": "2736319",
    "end": "2741400"
  },
  {
    "text": "one fully connected layer and the soft Max layer again I changed the",
    "start": "2741400",
    "end": "2748040"
  },
  {
    "text": "initializations of the weights and biases I changed the model I didn't change anything else uh the training",
    "start": "2748040",
    "end": "2754240"
  },
  {
    "text": "Loop the definition of the Cross entropy and all that let's try let's run",
    "start": "2754240",
    "end": "2760640"
  },
  {
    "text": "this convolutional run",
    "start": "2760640",
    "end": "2766000"
  },
  {
    "text": "run here so the first thing you're going to notice is that well we're asking the",
    "start": "2766000",
    "end": "2773359"
  },
  {
    "text": "system to compute a little bit more so it's not as fast as as it was in the beginning the second thing you notice is",
    "start": "2773359",
    "end": "2780960"
  },
  {
    "text": "that look at this the accuracy actually in the first 100 iterations it already show shot all the",
    "start": "2780960",
    "end": "2787559"
  },
  {
    "text": "way up to 92 in the second 100 iterations is already at 95 and it's not",
    "start": "2787559",
    "end": "2793400"
  },
  {
    "text": "stopping there it's going up and up and up actually if",
    "start": "2793400",
    "end": "2800200"
  },
  {
    "text": "I uh I will I'm going to play you the video because it would be a bit slow",
    "start": "2800200",
    "end": "2805720"
  },
  {
    "text": "otherwise but actually if you do this for all 10,000",
    "start": "2805720",
    "end": "2810800"
  },
  {
    "text": "iterations you end up over there I have to zoom you see 99%",
    "start": "2810800",
    "end": "2816520"
  },
  {
    "text": "is over there there and it's it's it's edging it's edging it's edging close closely on this",
    "start": "2816520",
    "end": "2823680"
  },
  {
    "text": "99 but in the end I get 99.8 damn",
    "start": "2825160",
    "end": "2831839"
  },
  {
    "text": "it I really want you to beat the 99 come on okay let's beat the 99% that's kind",
    "start": "2831839",
    "end": "2838880"
  },
  {
    "text": "of a limit we have to get over 99 so let's do something and looking at looking at",
    "start": "2838880",
    "end": "2845480"
  },
  {
    "text": "these curves first who can tell me that something is wrong and who can tell me how to make it right look",
    "start": "2845480",
    "end": "2853200"
  },
  {
    "text": "here overfitting yes look at the test loss it's going up like crazy what can",
    "start": "2853200",
    "end": "2859240"
  },
  {
    "text": "we do about overfitting shoot some neurons exactly I",
    "start": "2859240",
    "end": "2865480"
  },
  {
    "text": "like that let's shoot neurons let's apply Dropout what we will do actually",
    "start": "2865480",
    "end": "2870599"
  },
  {
    "text": "is a little bit more than that uh it's kind of a general method",
    "start": "2870599",
    "end": "2875800"
  },
  {
    "text": "when you are trying to find the ideal Network for a given problem it's kind of",
    "start": "2875800",
    "end": "2881640"
  },
  {
    "text": "a method or a trick uh is that to restrain the network until it hurts",
    "start": "2881640",
    "end": "2887599"
  },
  {
    "text": "until you know that you can do slightly better that's what I I've done here I know I can go go above 99 but actually I",
    "start": "2887599",
    "end": "2895480"
  },
  {
    "text": "I have not given it enough degrees of freedom so from there I will increase the degrees of freedom a little bit and",
    "start": "2895480",
    "end": "2902839"
  },
  {
    "text": "add the Dropout to make sure that it's not going to overfit because of those",
    "start": "2902839",
    "end": "2907920"
  },
  {
    "text": "degrees of freedom so that's what I will do here look we go first the patches a",
    "start": "2907920",
    "end": "2914400"
  },
  {
    "text": "little bit bigger 6655 instead of 5544 and a lot more channels so I had 4",
    "start": "2914400",
    "end": "2920480"
  },
  {
    "text": "8 and 12 I go to 6 12 and 24 okay so that's more degrees of freedom and I",
    "start": "2920480",
    "end": "2928119"
  },
  {
    "text": "will add Dropout on this layer on the other ones I'm a bit cautious with the Dropout there are not so many degrees of",
    "start": "2928119",
    "end": "2935400"
  },
  {
    "text": "freedom up there I don't want to to start shooting neurons up there here I have plenty okay how many 7x 7 by 24",
    "start": "2935400",
    "end": "2945640"
  },
  {
    "text": "that's like 10,000 or something so that's a lot I can shoot neurons there so let's see how this",
    "start": "2945640",
    "end": "2952920"
  },
  {
    "text": "performs what do you think uh actually let's keep it on video because of the",
    "start": "2952920",
    "end": "2960240"
  },
  {
    "text": "time what do you think let's go so it's shooting up right there I have to zoom let's zoom in come on Zoom",
    "start": "2960280",
    "end": "2967720"
  },
  {
    "text": "now you see what's going on look this is 99% and it's above 99% we did it we did",
    "start": "2967720",
    "end": "2973040"
  },
  {
    "text": "the 99% actually if we let this finish it will",
    "start": "2973040",
    "end": "2979680"
  },
  {
    "text": "end up at 99.3% the world record if you go to the",
    "start": "2979680",
    "end": "2985079"
  },
  {
    "text": "manist website is 99.7 so you see here in for in a 45 minute session we have actually done the",
    "start": "2985079",
    "end": "2992240"
  },
  {
    "text": "20 odd years of R&D into this problem almost uh to the end that's not to say that the",
    "start": "2992240",
    "end": "2999880"
  },
  {
    "text": "scientists have been uh playing around for 20 years actually they've done something fantastic they have tried",
    "start": "2999880",
    "end": "3007520"
  },
  {
    "text": "everything that doesn't work and published the stuff that works uh fully",
    "start": "3007520",
    "end": "3013640"
  },
  {
    "text": "connected networks reu soft soft Maxs uh convolutional networks Dropout we now",
    "start": "3013640",
    "end": "3020960"
  },
  {
    "text": "know what works and we now know how to add layers while still keeping our",
    "start": "3020960",
    "end": "3026160"
  },
  {
    "text": "Network converging and that's why we can solve those problems that's why we can",
    "start": "3026160",
    "end": "3031760"
  },
  {
    "text": "solve other problems like recognizing cats and dogs in a",
    "start": "3031760",
    "end": "3036880"
  },
  {
    "text": "picture so just to finish here is what Dropout did uh in this",
    "start": "3036880",
    "end": "3044680"
  },
  {
    "text": "case so actually you see this is without Dropout but it's already with a slightly",
    "start": "3044680",
    "end": "3050359"
  },
  {
    "text": "bigger Network and with learning rate Decay so we are already above 99% but we",
    "start": "3050359",
    "end": "3056400"
  },
  {
    "text": "still still have this overfitting problem adding Dropout the overfitting is is largely",
    "start": "3056400",
    "end": "3063280"
  },
  {
    "text": "resolved and we just won 210 of of a percent here which if you think about it",
    "start": "3063280",
    "end": "3070400"
  },
  {
    "text": "we're fighting for the last percent so 210th with just a simple normalization function is",
    "start": "3070400",
    "end": "3076760"
  },
  {
    "text": "huge and that's all I wanted to show to you today thank",
    "start": "3076760",
    "end": "3082558"
  },
  {
    "text": "you and before you go before you go uh tensorflow is open source so you can use",
    "start": "3084119",
    "end": "3090000"
  },
  {
    "text": "it but if you want to run tensorflow on Google's uh Hardware you actually have a cloud ml service that is designed",
    "start": "3090000",
    "end": "3096880"
  },
  {
    "text": "specifically for that fast learning on Google's hardware and if you don't want",
    "start": "3096880",
    "end": "3101920"
  },
  {
    "text": "to build your own models we have a couple of pre-built models we have an image recognition API uh a speech",
    "start": "3101920",
    "end": "3108839"
  },
  {
    "text": "recognition API a natural language processing API and then Google translate which has been around for ages but which",
    "start": "3108839",
    "end": "3115799"
  },
  {
    "text": "is heavily heavily based on neural networks today so all of that is available as an API uh but if you have",
    "start": "3115799",
    "end": "3123040"
  },
  {
    "text": "if you have to build your own neural network uh you can use this infrastructure to actually speed up this",
    "start": "3123040",
    "end": "3130000"
  },
  {
    "text": "work considerably thank [Applause]",
    "start": "3130000",
    "end": "3138190"
  },
  {
    "text": "you",
    "start": "3141599",
    "end": "3144599"
  }
]