[
  {
    "start": "0",
    "end": "136000"
  },
  {
    "text": "[Music]",
    "start": "6990",
    "end": "10108"
  },
  {
    "text": "okay so I'm going to talk about oh thanks going to talk about stream all",
    "start": "12679",
    "end": "19240"
  },
  {
    "text": "things because it's how everything in the world is streams and also how every",
    "start": "19240",
    "end": "25320"
  },
  {
    "text": "problem could be solved with streams or almost a lot of them could and about about modern ways of poring data you'll",
    "start": "25320",
    "end": "32599"
  },
  {
    "text": "see It'll be cool um I want to start by introducing my co-presenter uh rainbow a",
    "start": "32599",
    "end": "38640"
  },
  {
    "text": "courtesy of Holden who noticed that I'm speaking without a stuffed animal which is apparently no longer allowed H so I'm",
    "start": "38640",
    "end": "46480"
  },
  {
    "text": "very grateful for that I am Gwen shapira I'm working for confluent which is a",
    "start": "46480",
    "end": "52039"
  },
  {
    "text": "company that well I'm no longer allowed to say that it's the company behind the pic afka because it's all a community",
    "start": "52039",
    "end": "58879"
  },
  {
    "text": "but we do have a suspiciously large number of apachi afka committers working for us working on Apachi afka and",
    "start": "58879",
    "end": "64920"
  },
  {
    "text": "contributing to the project H I do a lot of stuff so um I wrote a nice book on",
    "start": "64920",
    "end": "70960"
  },
  {
    "text": "Kafka so if Kafka is your thing you should totally read it getting reviewed by Jesse who was here earlier um I am",
    "start": "70960",
    "end": "79320"
  },
  {
    "text": "product manager manager right now which is weird because the team do no longer accept my pull requests they're like no",
    "start": "79320",
    "end": "85799"
  },
  {
    "text": "no no you do the PowerPoint we'll write a code um I I used to write code I used",
    "start": "85799",
    "end": "91320"
  },
  {
    "text": "to be a real engineer and I used to do a lot a lot a lot of consultant like if you ask me what's the one thing you did",
    "start": "91320",
    "end": "97759"
  },
  {
    "text": "most in your career is go around tell talk to companies help them build do stuff with their data H which means that",
    "start": "97759",
    "end": "104759"
  },
  {
    "text": "uh I've seen how C how data management evolved over the years and I can kind of",
    "start": "104759",
    "end": "110640"
  },
  {
    "text": "I'm going to talk a lot to that and then I tweet way more than I should I even",
    "start": "110640",
    "end": "115799"
  },
  {
    "text": "got a book from go to Chicago for tweeting so much and I do what while I not supposed to",
    "start": "115799",
    "end": "123920"
  },
  {
    "text": "quote I still do and a lot of the stuff I'll talk about here I wrote examples for and you can go to my GitHub account",
    "start": "123920",
    "end": "131000"
  },
  {
    "text": "and kind of look up Kafka streams examples and you'll see a bunch of them so what happened to data management",
    "start": "131000",
    "end": "139239"
  },
  {
    "start": "136000",
    "end": "383000"
  },
  {
    "text": "like that's kind of over the course of my career which started somewhere around year 2000 yes I'm old um until now so we",
    "start": "139239",
    "end": "147360"
  },
  {
    "text": "started out I started as a DBA and IED as a data warehouse DBA and data",
    "start": "147360",
    "end": "152680"
  },
  {
    "text": "warehouse DBA was kind of a difficult job and the biggest the most difficult part was getting the data model right if",
    "start": "152680",
    "end": "158200"
  },
  {
    "text": "you got it right everything was supposed to be perfect but nobody ever got it right in just was a real pain there was",
    "start": "158200",
    "end": "164159"
  },
  {
    "text": "those huge batch jobs loading data as it would break every single night and we",
    "start": "164159",
    "end": "169319"
  },
  {
    "text": "always had to scrumble to still get the business reports in time even though those huge bat jobs kept failing we'd",
    "start": "169319",
    "end": "175080"
  },
  {
    "text": "had a lot of techniques about optimizing those nightly lad jobs like that that was was like 99% of the work and then",
    "start": "175080",
    "end": "182760"
  },
  {
    "text": "because modeling the data and loading data was so hard we never caught up with new applications so some data was always",
    "start": "182760",
    "end": "189200"
  },
  {
    "text": "missing and some and every single time someone wanted a report and he couldn't really do everything he wanted to do a",
    "start": "189200",
    "end": "196239"
  },
  {
    "text": "limited throughput was a pretty big issue like you could only load that much",
    "start": "196239",
    "end": "201920"
  },
  {
    "text": "data in eight hours window you had every night and once that you once you turned",
    "start": "201920",
    "end": "207480"
  },
  {
    "text": "the hell out of that it was a like we didn't even what to do and obviously nobody even thought that you could do it",
    "start": "207480",
    "end": "213400"
  },
  {
    "text": "not in large batches and then fast forward 209",
    "start": "213400",
    "end": "219319"
  },
  {
    "text": "2010 we had hadoop's coming along like a brand new way to do it so Hadoop is",
    "start": "219319",
    "end": "225080"
  },
  {
    "text": "still batch only but stuff was a lot better now we could skip the whole data modeling which was totally a pain in the",
    "start": "225080",
    "end": "230799"
  },
  {
    "text": "ass and skip the most of the in just pains in just tooop is easy you can just dump all the data in there and someone",
    "start": "230799",
    "end": "236720"
  },
  {
    "text": "will figure it out which is um DBA was a huge Improvement in life quality H but",
    "start": "236720",
    "end": "243959"
  },
  {
    "text": "then I was a Hadoop admin I help people set up their Hadoop and you immediately run into whole new host of issues which",
    "start": "243959",
    "end": "250799"
  },
  {
    "text": "is that nobody can actually use the data like now the mo that modeling of the",
    "start": "250799",
    "end": "256160"
  },
  {
    "text": "data understanding how to use it and what to do with it and writing those fantastic Mar produce and later spark",
    "start": "256160",
    "end": "261239"
  },
  {
    "text": "jobs was left for every single engineer in the company and it turns out that there weren't any better at it than I",
    "start": "261239",
    "end": "267000"
  },
  {
    "text": "was as a DBA it is still just really difficult problem and it was still messy and also a lot of time people just",
    "start": "267000",
    "end": "274199"
  },
  {
    "text": "couldn't find the data like they knew the data is somewhere in Hadoop but how the hell do we know what it is where it is what to do with it it was just it the",
    "start": "274199",
    "end": "282000"
  },
  {
    "text": "pains were still there they just moved from one team to another as often happens and now I'm looking at where the",
    "start": "282000",
    "end": "289479"
  },
  {
    "text": "companies that I'm talking to now and the D lake is still there people definitely still do this kinde of",
    "start": "289479",
    "end": "294720"
  },
  {
    "text": "processing but because of it still batchy nature people are also moving a lot of the data processing to something",
    "start": "294720",
    "end": "300880"
  },
  {
    "text": "that's more real time and obviously with much lower latency but still very",
    "start": "300880",
    "end": "306360"
  },
  {
    "text": "scalable like the hads used to be and the thing that's scary there is that it's a completely different model like",
    "start": "306360",
    "end": "312720"
  },
  {
    "text": "streaming and real time and doing all the data processing H live is just",
    "start": "312720",
    "end": "318000"
  },
  {
    "text": "pretty new and pretty scary for everyone I met someone at a very large uh online cloud service that pretty much probably",
    "start": "318000",
    "end": "324759"
  },
  {
    "text": "everyone in the room is is using or has used at some point and he's like I'm using Google flow and I'm like fantastic",
    "start": "324759",
    "end": "331039"
  },
  {
    "text": "how is that working for you and he's like well but I'm using it in batch mode and I'm like huh okay why are you doing",
    "start": "331039",
    "end": "338199"
  },
  {
    "text": "that and he like well because I'm scared of doing things extremes and I'm like you know I kind of get you I mean it is",
    "start": "338199",
    "end": "344600"
  },
  {
    "text": "scary like how to debug it like it's it it always goes on you don't have the 16 hours of day of looking at stuff and",
    "start": "344600",
    "end": "351720"
  },
  {
    "text": "fixing things it's really scary so I kind of get the whole yes stre is scary part and we are kind of still feeling",
    "start": "351720",
    "end": "358080"
  },
  {
    "text": "our ways around it and but it is good to see that companies are doing streams not everyone is totally afraid of it so I'm",
    "start": "358080",
    "end": "364800"
  },
  {
    "text": "basically going to talk about the patterns like that we discovered so it's not like Silver Bullet this is how you do things everything works that way it's",
    "start": "364800",
    "end": "371479"
  },
  {
    "text": "like I've talked to all those companies I've seen what people are doing here are the patterns that emerg that seems to be",
    "start": "371479",
    "end": "377400"
  },
  {
    "text": "things that people find useful and didn't burn anything",
    "start": "377400",
    "end": "382880"
  },
  {
    "text": "yet and one of the things I noticed throughout this whole transition is that",
    "start": "382919",
    "end": "388280"
  },
  {
    "start": "383000",
    "end": "468000"
  },
  {
    "text": "some of the not just the way we do things changed but who does things changed so as you remember I started my",
    "start": "388280",
    "end": "395000"
  },
  {
    "text": "career as a DBA but we had the ETL Engineers they used Informatica and data stages and then at some point talend D",
    "start": "395000",
    "end": "402039"
  },
  {
    "text": "and pintau and all those and you know did Dr and drop and build the whole ETL",
    "start": "402039",
    "end": "407199"
  },
  {
    "text": "Pipeline and that sort of continued into the data Lake and and people like you",
    "start": "407199",
    "end": "413000"
  },
  {
    "text": "know Informatica for the data Lake thingies I guess H but then at some point they disappeared and now I Prett",
    "start": "413000",
    "end": "419720"
  },
  {
    "text": "much like when someone is building a modern pip data pipeline that uses streams I don't see ETL Engineers",
    "start": "419720",
    "end": "425720"
  },
  {
    "text": "anywhere anymore and at first I thought it's just because the job went away right like if you dump all the data into Hadoop what the hell do you need an ETL",
    "start": "425720",
    "end": "431840"
  },
  {
    "text": "engineer for which is absolutely correct but then I looked around the notice something even scarier it's all",
    "start": "431840",
    "end": "438080"
  },
  {
    "text": "Engineers now everyone is gone I can't find and all the people I used to work with when I started my career I had",
    "start": "438080",
    "end": "444840"
  },
  {
    "text": "those QA people and the people doing data modeling and the CIS admins and the network admins and the storage admins I",
    "start": "444840",
    "end": "450440"
  },
  {
    "text": "used to hate the guts of my storage admin I think he's working for Amazon now because those are the only people",
    "start": "450440",
    "end": "456160"
  },
  {
    "text": "who still hire storage admins in the world so pretty much everything changed so I'm not surprised that software",
    "start": "456160",
    "end": "462440"
  },
  {
    "text": "Engineers are doing those data pipelines and data processing because are doing everything in the",
    "start": "462440",
    "end": "468639"
  },
  {
    "start": "468000",
    "end": "511000"
  },
  {
    "text": "organization so when you have a software engineer doing the job it's not just that it's a software engineer who is",
    "start": "468759",
    "end": "475159"
  },
  {
    "text": "doing the same things that the ETL engineer did earlier like the whole thing changes like the whole perspective it's a different person and everything",
    "start": "475159",
    "end": "482240"
  },
  {
    "text": "is different so if you're a software engineer you're not into let's get data from here to there you're into I'm",
    "start": "482240",
    "end": "488520"
  },
  {
    "text": "building an application I usually have a business goal in mind like that's the thing I need to do and you're also not",
    "start": "488520",
    "end": "495120"
  },
  {
    "text": "really into drag and dropping like you're an engineer right I know how to write Java or python or SQL or whatever",
    "start": "495120",
    "end": "500240"
  },
  {
    "text": "I have a debugger like what the is those point and clicks right",
    "start": "500240",
    "end": "505960"
  },
  {
    "text": "so so so a lot in the space just completely",
    "start": "505960",
    "end": "511440"
  },
  {
    "start": "511000",
    "end": "552000"
  },
  {
    "text": "changed and those patterns that emerged that we were look going to look at is first of all the fact that we're now",
    "start": "511440",
    "end": "517479"
  },
  {
    "text": "streaming everything right like that's the whole transition that we're doing everything in streams and in real time",
    "start": "517479",
    "end": "523000"
  },
  {
    "text": "it's about how do we keep compatibility that's a problem that kind of kept popping up starting from the keynote",
    "start": "523000",
    "end": "528519"
  },
  {
    "text": "yesterday like Martin followers mentioned that compatibility is a big issue when you building applications",
    "start": "528519",
    "end": "535360"
  },
  {
    "text": "that move data around and it's still a big deal and I'm going to talk about how to some processing is just ridiculously",
    "start": "535360",
    "end": "542399"
  },
  {
    "text": "simple and now we can do it at huge scale I'm going to talk a bit about that and then also how to solve a really",
    "start": "542399",
    "end": "548079"
  },
  {
    "text": "interesting problem of doing streaming data enrichment and everything is going to go",
    "start": "548079",
    "end": "554040"
  },
  {
    "text": "through an example of it's kind of a merge of two customers I worked with in the last few years and they're both",
    "start": "554040",
    "end": "560839"
  },
  {
    "text": "large hotel chains uh each doing it things slightly differently but a lot of things are in common because it's the",
    "start": "560839",
    "end": "566279"
  },
  {
    "text": "same problem domain and then I don't know like haat is here in Chicago but those are actually competitors are not",
    "start": "566279",
    "end": "572959"
  },
  {
    "text": "the same thing but I hope it will look familiar so let's start with something that when I wrote the slide I thought",
    "start": "572959",
    "end": "579600"
  },
  {
    "start": "576000",
    "end": "668000"
  },
  {
    "text": "it's a bold claim that all the data is event streams I didn't expect that the starting from the keyote and every",
    "start": "579600",
    "end": "586040"
  },
  {
    "text": "single presentation afterwards will also repeat every is event streams and by everything I mean that okay you're the",
    "start": "586040",
    "end": "592399"
  },
  {
    "text": "hotel chain and you have the website right and people look at pages and you want to know what people are looking at",
    "start": "592399",
    "end": "597720"
  },
  {
    "text": "definitely events right so you can capture those events and people look at specific rooms those are definitely",
    "start": "597720",
    "end": "603720"
  },
  {
    "text": "events and people have emails and they click on links to see your promotions those are events searches on the website",
    "start": "603720",
    "end": "610720"
  },
  {
    "text": "are event someone booked a room it's an event when you get to um if someone is a",
    "start": "610720",
    "end": "616079"
  },
  {
    "text": "loyal customer and he changes his email address on your database an event if a",
    "start": "616079",
    "end": "621560"
  },
  {
    "text": "hotel is under maintenance it's an event if you get to the hotel and check in it's an event if you go to your room and",
    "start": "621560",
    "end": "627240"
  },
  {
    "text": "open the door you know how you do it I bet that that's an event too if you H go and to the mini bar and take out a beer",
    "start": "627240",
    "end": "633360"
  },
  {
    "text": "it has to be an event otherwise how in the world will they Bild you so all how much internet do you use in your room",
    "start": "633360",
    "end": "638920"
  },
  {
    "text": "it's an event it's a bunch of events I have to measure it otherwise it can't really do any kind of capacity planning Etc everything is measure everything is",
    "start": "638920",
    "end": "646279"
  },
  {
    "text": "track that's the kind of the kind of world we're in today I bet that they also collect information from your phone",
    "start": "646279",
    "end": "651959"
  },
  {
    "text": "through the Wi-Fi somehow but I don't want to dig into that so I hope I",
    "start": "651959",
    "end": "657040"
  },
  {
    "text": "convinced you everything everything that happens in your database is an event because you can do a change data capture",
    "start": "657040",
    "end": "662480"
  },
  {
    "text": "in the database everything with a sensor everything in a log everything in application all",
    "start": "662480",
    "end": "668440"
  },
  {
    "start": "668000",
    "end": "683000"
  },
  {
    "text": "events and we look at one of them we'll start by looking at one of them which is the page view event pretty much every",
    "start": "668440",
    "end": "674320"
  },
  {
    "text": "every company in the world has a website everyone cares about what do people do everyone cares about page views and if",
    "start": "674320",
    "end": "680600"
  },
  {
    "text": "you look inside the page view event you kind of get um something",
    "start": "680600",
    "end": "685880"
  },
  {
    "start": "683000",
    "end": "727000"
  },
  {
    "text": "that's pretty similar right you have like a session identifier usually from the cookies to kind of track how the",
    "start": "685880",
    "end": "691000"
  },
  {
    "text": "person moved around you have a time stamp because everything in the world should have a time stamp you have kind",
    "start": "691000",
    "end": "696079"
  },
  {
    "text": "of what they're viewing right now he's viewing a property since he viewing property we also have the property ID so",
    "start": "696079",
    "end": "702160"
  },
  {
    "text": "we'll know what he's looking at and because he's logged into our website we also has his loyalty ID number so we",
    "start": "702160",
    "end": "708600"
  },
  {
    "text": "actually have a lot more information we we offer reward uh rewards and things so",
    "start": "708600",
    "end": "714000"
  },
  {
    "text": "we will get this all this extra information Associated that's super important if you're a hotel and then we",
    "start": "714000",
    "end": "719600"
  },
  {
    "text": "know that he's on a website because he clicked on a link from a promotion that we sent him by email and tons and tons",
    "start": "719600",
    "end": "725079"
  },
  {
    "text": "of those metad doas what do we do with those events well a lot of services are",
    "start": "725079",
    "end": "730200"
  },
  {
    "start": "727000",
    "end": "798000"
  },
  {
    "text": "interested in them right that's the event driven microservices architecture that's what it's all about you have a",
    "start": "730200",
    "end": "735279"
  },
  {
    "text": "lot of people interest interested in those events you want to have a feature on the website that says people who",
    "start": "735279",
    "end": "741480"
  },
  {
    "text": "looked at this room also looked on those five other slightly more expensive rooms maybe you're interested in those or 20",
    "start": "741480",
    "end": "748600"
  },
  {
    "text": "other people are also looking this room please book it fast because who knows how long it will last right you want to",
    "start": "748600",
    "end": "753639"
  },
  {
    "text": "do ab tests you want to know that people who search for this and the D booking the other saying you want to know how",
    "start": "753639",
    "end": "760320"
  },
  {
    "text": "relevant your search results are pro if your email promotion is actually working",
    "start": "760320",
    "end": "765519"
  },
  {
    "text": "affiliate programs these days you know how someone may blog about their fantastic vacation and you'll have a bunch of links to the place they stay at",
    "start": "765519",
    "end": "772320"
  },
  {
    "text": "and if you click on the link the Blogger and you actually book the Blogger will get paid that's very important to track",
    "start": "772320",
    "end": "778639"
  },
  {
    "text": "page just for that kind of system never believe anything written in blogs that's",
    "start": "778639",
    "end": "783720"
  },
  {
    "text": "definitely not mine but yeah that's a it's I it always scares me how much of",
    "start": "783720",
    "end": "789800"
  },
  {
    "text": "the ongoing uh lifestyle blogging ends up being marketing so we have all those",
    "start": "789800",
    "end": "796120"
  },
  {
    "text": "events and we have all those Services who wants to get these events and if you kind of naively every time you have",
    "start": "796120",
    "end": "801680"
  },
  {
    "start": "798000",
    "end": "825000"
  },
  {
    "text": "someone generating data and you have someone who wants the data have them talk to each other kind of HTTP request",
    "start": "801680",
    "end": "806800"
  },
  {
    "text": "response kind of thing that gets really messy really hard to operationalize I usually dig a lot into it but I feel",
    "start": "806800",
    "end": "812639"
  },
  {
    "text": "like this entire um event was all about how hard it is to debug troubleshoot",
    "start": "812639",
    "end": "818680"
  },
  {
    "text": "operationalize a bunch of events who talk to each other with ephemeral messages that nobody can really track so",
    "start": "818680",
    "end": "825320"
  },
  {
    "start": "825000",
    "end": "890000"
  },
  {
    "text": "pattern number one the way I really like doing it is basically publish everything",
    "start": "825320",
    "end": "831399"
  },
  {
    "text": "into Kafka and cfco was written originally as kind of a published subscribed message bus on steroids just",
    "start": "831399",
    "end": "838800"
  },
  {
    "text": "open source and meant to scale really really really well which is why companies like LinkedIn can actually",
    "start": "838800",
    "end": "844160"
  },
  {
    "text": "afford to publish everything in dufka because it it is the only thing that's capable of scaling um they tried active",
    "start": "844160",
    "end": "851480"
  },
  {
    "text": "mq earlier and that didn't end up very well and then you have all those subscribers who can read from Kafka the",
    "start": "851480",
    "end": "857560"
  },
  {
    "text": "other cool property of Kafka is that it scales really well with a high number of subscribers that's another thing that",
    "start": "857560",
    "end": "863519"
  },
  {
    "text": "mqs tend to not do that amazingly and it also SC with different type of",
    "start": "863519",
    "end": "870360"
  },
  {
    "text": "subscribers so if you have batch subscribers that just read stuff every hour or every day cfco will actually",
    "start": "870360",
    "end": "875720"
  },
  {
    "text": "handle them pretty well as opposed to message cues that has to keep state in memory for all those subscribers that's",
    "start": "875720",
    "end": "882079"
  },
  {
    "text": "why we can get that to Hadoop and to the data warehouse as well as to stuff that's more real time so that's kind of",
    "start": "882079",
    "end": "888680"
  },
  {
    "text": "the idea if we look slightly inside Kafka we can see why it's a good idea so",
    "start": "888680",
    "end": "894160"
  },
  {
    "start": "890000",
    "end": "987000"
  },
  {
    "text": "the basic unit of information in Kafka is the log H if you have read my CEO",
    "start": "894160",
    "end": "901639"
  },
  {
    "text": "wrote a really nice book called I love logs and before that there was a really nice oilly blog post everything a",
    "start": "901639",
    "end": "909480"
  },
  {
    "text": "software developer has to know about logs and it talks about logs as an obstruction because really logs is a the",
    "start": "909480",
    "end": "915160"
  },
  {
    "text": "basis of everything right it's not just your application logs where you write oh this happened this happened oh exception",
    "start": "915160",
    "end": "921160"
  },
  {
    "text": "it's also a if you are a database every database in the world every no SQL SQL",
    "start": "921160",
    "end": "926759"
  },
  {
    "text": "whatever they have the right ah head log right they keep writing everything that happens every single update insert",
    "start": "926759",
    "end": "932519"
  },
  {
    "text": "delete commit roll back everything goes into this log everything in the world is",
    "start": "932519",
    "end": "937920"
  },
  {
    "text": "a log of events so Kafka is a very good obstruction that allows you to model pretty much everything that's going on",
    "start": "937920",
    "end": "943920"
  },
  {
    "text": "in your organization so the way this log works is that we have this Ser sequence",
    "start": "943920",
    "end": "949839"
  },
  {
    "text": "of events and you keep writing and every time we write it comes to the end and cfut guarantees that it preserves the",
    "start": "949839",
    "end": "955600"
  },
  {
    "text": "order of writing across this one log and then you have readers and the way it SC is that each reader",
    "start": "955600",
    "end": "961319"
  },
  {
    "text": "basically remembers that I'm now at message five which means that I read message 0 to four and now at message six",
    "start": "961319",
    "end": "967120"
  },
  {
    "text": "and now I'm message seven so Kafka doesn't have to keep track of those messages were acknowledged but those",
    "start": "967120",
    "end": "973240"
  },
  {
    "text": "messages weren't which is most of the load on an traditional mq thingy so this",
    "start": "973240",
    "end": "978880"
  },
  {
    "text": "is pretty uh useful it's fast it scales it's easy to understand which is important as someone who debugs Kafka",
    "start": "978880",
    "end": "984839"
  },
  {
    "text": "easy to understand is important I like simple H but obviously One log can only",
    "start": "984839",
    "end": "990720"
  },
  {
    "start": "987000",
    "end": "1047000"
  },
  {
    "text": "fit on one dis maybe if we try to keep the whole sequential thing and we actually want it to scale so the way",
    "start": "990720",
    "end": "998680"
  },
  {
    "text": "Kafka works is it has topics and topics have partitions and each partition is a",
    "start": "998680",
    "end": "1003720"
  },
  {
    "text": "log so you scale by saying okay all the page of events go to this one topic and",
    "start": "1003720",
    "end": "1009040"
  },
  {
    "text": "this one topic has a 100 a thousand whatever partitions and then each we",
    "start": "1009040",
    "end": "1014680"
  },
  {
    "text": "have maybe 10 Brokers each broker gets 10 partitions and that's how things are kind of load balanced and distributed",
    "start": "1014680",
    "end": "1021079"
  },
  {
    "text": "and so on order is still maintained in each portation so you kind of have to make sure that all the events that are",
    "start": "1021079",
    "end": "1027480"
  },
  {
    "text": "relevant together for example all the page views from one session all the page views from one user they go to the same",
    "start": "1027480",
    "end": "1034079"
  },
  {
    "text": "uh log so you can process them in the with the same reader and the way we do",
    "start": "1034079",
    "end": "1040280"
  },
  {
    "text": "it is basically we have keys and we use them as partition keys so that's pretty straightforward like the way most",
    "start": "1040280",
    "end": "1045959"
  },
  {
    "text": "systems would work uh so back to our page view and all those consumers and we now know that we",
    "start": "1045959",
    "end": "1053160"
  },
  {
    "text": "have Kafka page View events go into Kafka all those things are consumers reading data from Kafka and processing",
    "start": "1053160",
    "end": "1059240"
  },
  {
    "start": "1059000",
    "end": "1087000"
  },
  {
    "text": "it and this is kind of the way it usually looks you have those events going into Kafka and then you have all",
    "start": "1059240",
    "end": "1065640"
  },
  {
    "text": "those services so maybe someone is getting the events into a datab base he does create tail Bell we have the time",
    "start": "1065640",
    "end": "1070880"
  },
  {
    "text": "stamp it's a number fantastic you can see the time stamp is indeed a long number of seconds 1970 the usual you",
    "start": "1070880",
    "end": "1077320"
  },
  {
    "text": "have a search uh something it he needs to pass the date no worries new date give it a time stamp everything",
    "start": "1077320",
    "end": "1084039"
  },
  {
    "text": "is fantastic that's how it works and then one day a developer says that hey",
    "start": "1084039",
    "end": "1089960"
  },
  {
    "text": "number of seconds since H 1970 is horrible to debug it's unreadable we",
    "start": "1089960",
    "end": "1095000"
  },
  {
    "text": "need an easier format let's go with something more readable and he makes the change and immediately everything across",
    "start": "1095000",
    "end": "1102360"
  },
  {
    "text": "the organization breaks that's the way life is yeah it's really sad I know I'm sorry",
    "start": "1102360",
    "end": "1109440"
  },
  {
    "text": "so and that's you can think I'm making this specific example with the time stamp format app but I've actually seen",
    "start": "1109440",
    "end": "1116480"
  },
  {
    "text": "it in one of my bigger customers how this small change blows up everything and they ended up having to reprocess",
    "start": "1116480",
    "end": "1122880"
  },
  {
    "text": "something like four terabytes of data to overcome the problem I've then I've think that okay my",
    "start": "1122880",
    "end": "1129880"
  },
  {
    "text": "customer is uniquely crazy but I've been at the Strat talk by Uber and they talked about the exact same problem so",
    "start": "1129880",
    "end": "1136400"
  },
  {
    "text": "it's not just that my customer recoup is crazy this happens to people who are supposed to be really good Engineers so",
    "start": "1136400",
    "end": "1143120"
  },
  {
    "text": "I guess if it happened to you don't feel ashamed it's a thing but we still need to prevent it from happening so which",
    "start": "1143120",
    "end": "1149400"
  },
  {
    "start": "1148000",
    "end": "1159000"
  },
  {
    "text": "brings us to partner number two the fact that we need to somehow maintain compatibility right you write an event",
    "start": "1149400",
    "end": "1155480"
  },
  {
    "text": "you need to to make sure that people are capable of actually reading it after you wrote it and that's something I feel very",
    "start": "1155480",
    "end": "1162280"
  },
  {
    "start": "1159000",
    "end": "1245000"
  },
  {
    "text": "strongly about because all this conference I heard people talk about uh",
    "start": "1162280",
    "end": "1168320"
  },
  {
    "text": "uh comp compatibility and they talked about compatibility in terms of services talking to each other but if we take",
    "start": "1168320",
    "end": "1174520"
  },
  {
    "text": "accept the premise that Services talking to each other V rest API is actually not that good of a way to scale and not that",
    "start": "1174520",
    "end": "1180880"
  },
  {
    "text": "good of a way to process data and then compatibility at that level is not what you should be worried",
    "start": "1180880",
    "end": "1187559"
  },
  {
    "text": "about you should worry about it but that's not the big thing the big thing is that if you're doing event processing",
    "start": "1187559",
    "end": "1193200"
  },
  {
    "text": "or stream processing your the messages in the log are the schema right that",
    "start": "1193200",
    "end": "1198400"
  },
  {
    "text": "this is the they're the schema and they are the API between the different",
    "start": "1198400",
    "end": "1203840"
  },
  {
    "text": "Services right that's the contract you produce an event it has a certain schema to it it has a certain H types you",
    "start": "1203840",
    "end": "1211120"
  },
  {
    "text": "everyone has to read it the contract is what the event contains so it's super super important to main this is your API",
    "start": "1211120",
    "end": "1217240"
  },
  {
    "text": "this is the thing you need to control compatibility for and it's even worse than just those apis with messages",
    "start": "1217240",
    "end": "1223760"
  },
  {
    "text": "because those events can stick around for very long so if you decide to save",
    "start": "1223760",
    "end": "1229080"
  },
  {
    "text": "data for a month in Kafka or three months or three years which has been known to happen you're are now committed",
    "start": "1229080",
    "end": "1235720"
  },
  {
    "text": "to allowing people to read data with the schema that you've used three years ago so that's pretty",
    "start": "1235720",
    "end": "1242120"
  },
  {
    "text": "crazy and we need to make to at least know when we did bad things and one of",
    "start": "1242120",
    "end": "1247440"
  },
  {
    "start": "1245000",
    "end": "1363000"
  },
  {
    "text": "the important things about compatibility that people tend to miss is that you need to do it as early as possible like",
    "start": "1247440",
    "end": "1254840"
  },
  {
    "text": "a lot of people talk about keeping compatibility in production and having safe ground s around production and not",
    "start": "1254840",
    "end": "1260559"
  },
  {
    "text": "allowing stuff to that breaks to write events in production which is great but",
    "start": "1260559",
    "end": "1265679"
  },
  {
    "text": "if you're if you deploy to Productions three times a day it's probably fine but",
    "start": "1265679",
    "end": "1271120"
  },
  {
    "text": "I'm not all organizations deploy data three times a day some of them deploy data once a month for example like",
    "start": "1271120",
    "end": "1277919"
  },
  {
    "text": "changes so you really want to hopefully be able to catch schema changes when",
    "start": "1277919",
    "end": "1283840"
  },
  {
    "text": "someone makes the first commit to his Branch or if it's not on the first commit to his branch maybe when you do",
    "start": "1283840",
    "end": "1289640"
  },
  {
    "text": "AEM merge and if it's not when you do emerge well you do have nightly tests running via something like Jenkins right",
    "start": "1289640",
    "end": "1294919"
  },
  {
    "text": "so that would be a good time to verify that schemas are still compatible and if you don't even do that maybe you have a",
    "start": "1294919",
    "end": "1300200"
  },
  {
    "text": "staging environment that will also be a really nice time to Cas schemas are compatible one of those don't wait until",
    "start": "1300200",
    "end": "1306440"
  },
  {
    "text": "production because that's kind of like Last Resort kind of thing like if you have to roll back a change from",
    "start": "1306440",
    "end": "1312159"
  },
  {
    "text": "production that's kind of ugly H so you kind of want to integrate with all those wonderful development tools we all have",
    "start": "1312159",
    "end": "1318799"
  },
  {
    "text": "you know git plugins and Maven plugins and jenin plugins and all those things make sure you compatibility there so",
    "start": "1318799",
    "end": "1326039"
  },
  {
    "text": "klent has an open source schema registry and we have Maven plugins and we also have the things that keeps it safe in",
    "start": "1326039",
    "end": "1331679"
  },
  {
    "text": "production which can also be used in test and staging Etc I usually don't recommend actually running these kind of",
    "start": "1331679",
    "end": "1337799"
  },
  {
    "text": "schemas and compatibility checks on the development environment itself because usually when you just develop in your",
    "start": "1337799",
    "end": "1343080"
  },
  {
    "text": "own environment you want to be able to iterate and test a bunch of things I can on worrying about it at the stage page",
    "start": "1343080",
    "end": "1348720"
  },
  {
    "text": "of after the first commit or nightly test seems like a reasonable compromise for me anyway you don't have to use",
    "start": "1348720",
    "end": "1354760"
  },
  {
    "text": "Confluence schem registry but do some things that keep things compatible that's that's the key Point",
    "start": "1354760",
    "end": "1362919"
  },
  {
    "start": "1363000",
    "end": "1403000"
  },
  {
    "text": "here third pattern uh having ridiculously simple sometimes you just",
    "start": "1363480",
    "end": "1369440"
  },
  {
    "text": "have to want to do really simple changes to a messages so for example you have",
    "start": "1369440",
    "end": "1375880"
  },
  {
    "text": "some you happen to to have the credit card transaction this is completely H kind of H made up right you wouldn't",
    "start": "1375880",
    "end": "1383360"
  },
  {
    "text": "really have in a page you event a credit card number but suppose you did and you wanted to mask it just imagine this is",
    "start": "1383360",
    "end": "1389480"
  },
  {
    "text": "not a page view H there is a this is pretty simple right you read an event you change one field you write an event",
    "start": "1389480",
    "end": "1396279"
  },
  {
    "text": "back kind of thing uh this is really simple so there could be bunch of ways",
    "start": "1396279",
    "end": "1401600"
  },
  {
    "text": "to uh do it uh the easiest is basically to run a",
    "start": "1401600",
    "end": "1406679"
  },
  {
    "start": "1403000",
    "end": "1500000"
  },
  {
    "text": "Kafka consumer and it's just you know C consumer is basically an API that you do",
    "start": "1406679",
    "end": "1413480"
  },
  {
    "text": "subscribe to a topic PLL you get an array of events you iterate you make the changes and now you need to also do a",
    "start": "1413480",
    "end": "1420400"
  },
  {
    "text": "produce because or do something with the data that you just got to make sure that the credit card is gone but uh it's it's",
    "start": "1420400",
    "end": "1427679"
  },
  {
    "text": "very straightforward and the thing that makes it very very ridiculously scalable",
    "start": "1427679",
    "end": "1433159"
  },
  {
    "text": "is that you can basically write one C consumer and if you just deploy one of them it and say read data from the page",
    "start": "1433159",
    "end": "1439640"
  },
  {
    "text": "view topic it will just read from all the partitions in the topic but what if you have a thousand partitions and",
    "start": "1439640",
    "end": "1445080"
  },
  {
    "text": "millions of event per second and this one poor consumer can't handle it all well you just start another one and they",
    "start": "1445080",
    "end": "1451360"
  },
  {
    "text": "magically and by magically I mean through a complex CFA protocol H we talk to each other and figure out that hey",
    "start": "1451360",
    "end": "1458720"
  },
  {
    "text": "I'm doing those three partitions you're doing the other 997 partition no they",
    "start": "1458720",
    "end": "1463960"
  },
  {
    "text": "try to do it 50/50 and you say oh it still do scale you start another one okay I'm doing",
    "start": "1463960",
    "end": "1470399"
  },
  {
    "text": "these partitions you're doing the other partitions and this goes on until you have one consumer per partition you",
    "start": "1470399",
    "end": "1477200"
  },
  {
    "text": "can't have two consumer partition because then you can't you we don't guarantee order and any of that but and",
    "start": "1477200",
    "end": "1483320"
  },
  {
    "text": "so that's why I kind of like have a lot of partitions but until you exhausted that part then it's um it scales really",
    "start": "1483320",
    "end": "1491320"
  },
  {
    "text": "really well and as you can see you can do consume from a topic do stuff produce",
    "start": "1491320",
    "end": "1496720"
  },
  {
    "text": "to a topic it scales really well this would be the Hipster stream",
    "start": "1496720",
    "end": "1501799"
  },
  {
    "start": "1500000",
    "end": "1611000"
  },
  {
    "text": "processing pattern and the reason we call it a hipster stream processing is that over in San Francisco land where we",
    "start": "1501799",
    "end": "1508760"
  },
  {
    "text": "don't drink bad light with [Laughter]",
    "start": "1508760",
    "end": "1514320"
  },
  {
    "text": "lime we we have those um hipster people and they ride um fixes which are bikes",
    "start": "1514320",
    "end": "1522919"
  },
  {
    "text": "not just without gears but without a free whe you just have to keep pedaling they have no brakes it's really really",
    "start": "1522919",
    "end": "1528480"
  },
  {
    "text": "really weird it is and the things that they love about",
    "start": "1528480",
    "end": "1534039"
  },
  {
    "text": "those bags is they're super simple which is true and that's the thing that I like about hipster stream processing is that you just have a consumer and a producer",
    "start": "1534039",
    "end": "1540720"
  },
  {
    "text": "very small up it scales well super simple I love Simplicity and this works great until you hit the first big hill",
    "start": "1540720",
    "end": "1547679"
  },
  {
    "text": "in San Francisco you hit the first big hill really really soon and that's where Simplicity no longer works right like",
    "start": "1547679",
    "end": "1553960"
  },
  {
    "text": "you don't want a fixie on a large Hill if well no matter if it goes up or down you're",
    "start": "1553960",
    "end": "1559840"
  },
  {
    "text": "um what you and that's pretty much so what are the heels when stream processing the heels are things that are",
    "start": "1559840",
    "end": "1565840"
  },
  {
    "text": "stateful things that are stateful would be either joints which are kind of hard",
    "start": "1565840",
    "end": "1571039"
  },
  {
    "text": "they would be Aggregates like window Aggregates you need to maintain a state if you crush you still need to somehow",
    "start": "1571039",
    "end": "1576520"
  },
  {
    "text": "restore the state and figure out what to do with it that's really hard and the other things that's kind of hard and",
    "start": "1576520",
    "end": "1583080"
  },
  {
    "text": "people don't think about it is that Dynamic configuration is really hard in a distributed system because you have all those",
    "start": "1583080",
    "end": "1589039"
  },
  {
    "text": "consumers and now you want to change a config somehow and you need them to all basically start together you don't",
    "start": "1589039",
    "end": "1595000"
  },
  {
    "text": "unless you especially built for it half and half is really difficult so Dynamic config changes is another hard problem",
    "start": "1595000",
    "end": "1602360"
  },
  {
    "text": "so kind of like so stre proc hipster stream processing scals really well for simple problems not so much for harder",
    "start": "1602360",
    "end": "1611039"
  },
  {
    "start": "1611000",
    "end": "1634000"
  },
  {
    "text": "problems and then there is also like what do we do if there is a data store involved like we actually I'll show",
    "start": "1611039",
    "end": "1617399"
  },
  {
    "text": "later why but but you happen to need to get some data from a database or to get data to elastic can we do it in a",
    "start": "1617399",
    "end": "1624360"
  },
  {
    "text": "simpler and the answer is that there is CA connect exists to make it even simpler and the reason it's awesome and",
    "start": "1624360",
    "end": "1631720"
  },
  {
    "text": "so simple is that you don't have to write anything basically there's just",
    "start": "1631720",
    "end": "1637240"
  },
  {
    "start": "1634000",
    "end": "1672000"
  },
  {
    "text": "like we have like 50 plus connectors already out there so of course the",
    "start": "1637240",
    "end": "1642640"
  },
  {
    "text": "easiest to write software the fastest with the list bugs is the one you never had to write yourself uh um so that's",
    "start": "1642640",
    "end": "1649559"
  },
  {
    "text": "kind that's not a bad option at all like there's a lot of data system out there but I'm betting with something like 80%",
    "start": "1649559",
    "end": "1656480"
  },
  {
    "text": "certainty that you have MySQL or postgress or Oracle or Cassandra or",
    "start": "1656480",
    "end": "1663200"
  },
  {
    "text": "mongodb or elastic like one of those six you probably have somewhere around so it's like the 8020 Rule and we have 50",
    "start": "1663200",
    "end": "1669080"
  },
  {
    "text": "connectors so some one of those you'll probably have around and how does it work why do do I",
    "start": "1669080",
    "end": "1675240"
  },
  {
    "text": "claim that it massively scales well it kind of runs on own worker nodes which can be Docker containers or whatever and",
    "start": "1675240",
    "end": "1682960"
  },
  {
    "text": "it has a rest so you install a bunch of those nodes and you have a rest API and you can say hey please get me data from",
    "start": "1682960",
    "end": "1688799"
  },
  {
    "text": "a log and from an mqtt this kind of an iot sing uh service and it goes to the",
    "start": "1688799",
    "end": "1695320"
  },
  {
    "text": "respective data stores and say hey how it doesn't say give me the data yet because that wouldn't scale right I only",
    "start": "1695320",
    "end": "1701039"
  },
  {
    "text": "have one of each it says how much data do you have for me how many indexes how many logs how many files how much data",
    "start": "1701039",
    "end": "1707960"
  },
  {
    "text": "and and then it looks at the data and says huh so I need to have that many tasks and you kind of configure how many",
    "start": "1707960",
    "end": "1713200"
  },
  {
    "text": "tasks you can have maximum but the idea is that you'll have like a task per CPU so each one is one thread single thread",
    "start": "1713200",
    "end": "1719399"
  },
  {
    "text": "and now it's all paralyzed and you didn't have to do any of it but it's fully par parallelized and another nice",
    "start": "1719399",
    "end": "1725039"
  },
  {
    "text": "benefit because of our magic coordination protocol if that one",
    "start": "1725039",
    "end": "1732200"
  },
  {
    "text": "dies the work will the task will magically move its way",
    "start": "1732200",
    "end": "1739960"
  },
  {
    "text": "to an available worker so that's very convenient and by the way I you 90% of",
    "start": "1739960",
    "end": "1746600"
  },
  {
    "text": "the time working on the presentation was the animation I hate PowerPoint Kafka is easy Kafka streams",
    "start": "1746600",
    "end": "1753720"
  },
  {
    "text": "is super easy PowerPoint is hard so that's kind of cool but okay so",
    "start": "1753720",
    "end": "1759720"
  },
  {
    "text": "now we have data from databases we have those simple processing we got rid of credit card numbers now our hotel has a",
    "start": "1759720",
    "end": "1765799"
  },
  {
    "start": "1765000",
    "end": "1790000"
  },
  {
    "text": "new business case we want to do a promotion we have Platinum members and we have a",
    "start": "1765799",
    "end": "1771840"
  },
  {
    "text": "new Beachside property in Hawaii that we want to promote and we decid that if Platinum member looks at any kind of",
    "start": "1771840",
    "end": "1778919"
  },
  {
    "text": "Beach property anywhere in the world we will send him an email about the awesome deals he can get in Hawai because we",
    "start": "1778919",
    "end": "1784640"
  },
  {
    "text": "want him to book hotel in Hawai okay fair enough that's pretty normal a lot of hotels do it all the",
    "start": "1784640",
    "end": "1790519"
  },
  {
    "start": "1790000",
    "end": "1808000"
  },
  {
    "text": "time and this means that we take a page viw event that have session and time",
    "start": "1790519",
    "end": "1795559"
  },
  {
    "text": "stamp and loyalty ID and property ID but what what we really need to know is things like is it Platinum is it a Beach",
    "start": "1795559",
    "end": "1802120"
  },
  {
    "text": "property like we need an enriched event we need more our service that has to decide has to get more information so",
    "start": "1802120",
    "end": "1809600"
  },
  {
    "start": "1808000",
    "end": "1855000"
  },
  {
    "text": "our architecture excuse my drawing ends up being a bit like that you have those",
    "start": "1809600",
    "end": "1815320"
  },
  {
    "text": "events going into Kafka you have your still super simple consumers that get them but then you need to enrich the",
    "start": "1815320",
    "end": "1821360"
  },
  {
    "text": "event and then you need to pass it on to decide whether or not it's a promotion",
    "start": "1821360",
    "end": "1826640"
  },
  {
    "text": "suppose that you also write your decision to elastic so data scientists can do AB testing and make decisions and",
    "start": "1826640",
    "end": "1832440"
  },
  {
    "text": "then you have a producer sending an email to notific sending sorry A message",
    "start": "1832440",
    "end": "1838360"
  },
  {
    "text": "an event to a notification service saying please notify the user H send an",
    "start": "1838360",
    "end": "1843600"
  },
  {
    "text": "email Etc so you kind of have this pass through thingy and now the question the only",
    "start": "1843600",
    "end": "1849440"
  },
  {
    "text": "question is how do I write the enrichment service how do I get I have this event and I did an event with more",
    "start": "1849440",
    "end": "1856279"
  },
  {
    "start": "1855000",
    "end": "1952000"
  },
  {
    "text": "data so our first attempt is to say well we have a database with all the",
    "start": "1856279",
    "end": "1862519"
  },
  {
    "text": "information let's go and get information from the database or if you are really really nice you say oh we have a service",
    "start": "1862519",
    "end": "1868840"
  },
  {
    "text": "that provides those apis that give us information let's go and talk to the service and have him talk to the database and give us all the information",
    "start": "1868840",
    "end": "1875519"
  },
  {
    "text": "which is slightly better but not that much because we still have a pretty big issue and the biggest issue is latency",
    "start": "1875519",
    "end": "1883880"
  },
  {
    "text": "that just takes time the second biggest issue is throughput like just because Kafka scales to gazillion events per",
    "start": "1883880",
    "end": "1889679"
  },
  {
    "text": "second doesn't mean that every service in your organization is going to scale the same thing so you have a huge fire hose trying to go into a pipe that big",
    "start": "1889679",
    "end": "1896519"
  },
  {
    "text": "especially important if you go directly to a database databases can scale 10,000",
    "start": "1896519",
    "end": "1902120"
  },
  {
    "text": "events per second like without killing production accidentally like your dbas will kill you if you try to do 100,000",
    "start": "1902120",
    "end": "1907720"
  },
  {
    "text": "or a million of them per second kafa can do a lot more so it's you have to be very careful not to DDOS your own",
    "start": "1907720",
    "end": "1913399"
  },
  {
    "text": "database accidentally of course if you're using Amazon something just go to town you pay for all of the events they",
    "start": "1913399",
    "end": "1919760"
  },
  {
    "text": "they will love you um the other problem is availability like if you just talk to one service no big deal one of my",
    "start": "1919760",
    "end": "1926440"
  },
  {
    "text": "customers needs to to enrich each event he needs to talk to 11 different services and I heard that's not even the",
    "start": "1926440",
    "end": "1932399"
  },
  {
    "text": "worst thing he has to do so so if you have 11 different Services",
    "start": "1932399",
    "end": "1937720"
  },
  {
    "text": "one of them will have an issue at any given time like 90% probability one of them will have an issue what do we do",
    "start": "1937720",
    "end": "1943120"
  },
  {
    "text": "now like I have an event I have to enrich it like I okay so I have to keep retrying and have to manage all the r r",
    "start": "1943120",
    "end": "1949159"
  },
  {
    "text": "it's a pain we can do better than that guys we can do a lot better than that enter the streams table Join one side we",
    "start": "1949159",
    "end": "1956639"
  },
  {
    "start": "1952000",
    "end": "1992000"
  },
  {
    "text": "have a table with the state of all our um members and what is their account",
    "start": "1956639",
    "end": "1961840"
  },
  {
    "text": "status and on the other side we have a stream of events with idas that we need we somehow need to join both of",
    "start": "1961840",
    "end": "1969720"
  },
  {
    "text": "them and what the way we wanted to end up is that we have a local copy of",
    "start": "1969720",
    "end": "1975120"
  },
  {
    "text": "database that's a pattern that everyone in this conference kept talking about right right you have a local readon in",
    "start": "1975120",
    "end": "1980679"
  },
  {
    "text": "memory cache of the data that your application needs and the way we created",
    "start": "1980679",
    "end": "1986720"
  },
  {
    "text": "M obviously cashes has the problem of getting stale and we don't want it to get stale we want it to stay alive and",
    "start": "1986720",
    "end": "1992279"
  },
  {
    "start": "1992000",
    "end": "2052000"
  },
  {
    "text": "active so we add another consumer his entire job is to get the events about",
    "start": "1992279",
    "end": "1998880"
  },
  {
    "text": "all those updates to the table from the from Kafka and keep on maintaining this",
    "start": "1998880",
    "end": "2005240"
  },
  {
    "text": "uh cash fair enough but how does those events from the database even end up in cfco in the first place that's called",
    "start": "2005240",
    "end": "2011840"
  },
  {
    "text": "change data capture you take stuff out of a database and basically you read a",
    "start": "2011840",
    "end": "2017080"
  },
  {
    "text": "commit log update insert update insert delete delete and you turn it into a log",
    "start": "2017080",
    "end": "2022760"
  },
  {
    "text": "of events basically turn a state into a log that's that's a super super",
    "start": "2022760",
    "end": "2027840"
  },
  {
    "text": "important pattern a CA connect has a bunch of connectors specifically for that or if you're into Oracle you have",
    "start": "2027840",
    "end": "2033399"
  },
  {
    "text": "Golden Gate you have a tunities there's a lot of ways to do change dat capture super wellknown p from the ETL days so",
    "start": "2033399",
    "end": "2040559"
  },
  {
    "text": "we used all those things to take a state in the database into a stream of events in cka and then we turn it back into a",
    "start": "2040559",
    "end": "2047799"
  },
  {
    "text": "state in the application so we kind of do this back and forth pretty much what",
    "start": "2047799",
    "end": "2053919"
  },
  {
    "text": "it looks like we take the state turn to a stream back into a state of course now that you have a stream of events you can",
    "start": "2053919",
    "end": "2060079"
  },
  {
    "text": "also do a lot of stuff you couldn't do before right because now you you have the entire history of your database at",
    "start": "2060079",
    "end": "2065118"
  },
  {
    "text": "your fingertips you can see how many people book prop properties and then immediately cancel them you couldn't before because database only has a",
    "start": "2065119",
    "end": "2071320"
  },
  {
    "text": "specific state state at any given time so you can do a lot cool stuff with",
    "start": "2071320",
    "end": "2077119"
  },
  {
    "start": "2075000",
    "end": "2196000"
  },
  {
    "text": "that am I completely Running Out of Time how you good I'm good cool",
    "start": "2079119",
    "end": "2085599"
  },
  {
    "text": "okay um okay so this is one of the things that is a it kind of sounds good",
    "start": "2085599",
    "end": "2091200"
  },
  {
    "text": "when I'm waving my hands all over but it turns out that doing a stream to table join in a reality is not not always",
    "start": "2091200",
    "end": "2098560"
  },
  {
    "text": "trivial like just you have to maintain the table in one thread and you have to have those things coordinate and then",
    "start": "2098560",
    "end": "2105359"
  },
  {
    "text": "you have this cash if you fail over you have to recreate it from the stream H you if you want to be fast about it you",
    "start": "2105359",
    "end": "2111240"
  },
  {
    "text": "need to somehow maintain snapshots which is kind of a its own kind of challenge so there's a lot of stuff that you have",
    "start": "2111240",
    "end": "2117240"
  },
  {
    "text": "to worry about if you implement it like it's always like it's all fun and games until you have to think about what happens if something crashes and then",
    "start": "2117240",
    "end": "2124320"
  },
  {
    "text": "you find out that it's actually an hour downtime until we managed to recreate the state from this stream of events and",
    "start": "2124320",
    "end": "2130680"
  },
  {
    "text": "nobody wants that so oh we actually need an inmemory database oh that inmemory database has to persist so we use rocks",
    "start": "2130680",
    "end": "2137079"
  },
  {
    "text": "Deb to persist it and then we have to say okay but if this crashes and starts on another machine what do we do like",
    "start": "2137079",
    "end": "2143440"
  },
  {
    "text": "there is there a way to make make it stateful in kubernetes we still either way anyone knows if there's a way to",
    "start": "2143440",
    "end": "2149400"
  },
  {
    "text": "have stateful uh containers in kubernetes because yeah I'll find out before next time I'm doing this",
    "start": "2149400",
    "end": "2155359"
  },
  {
    "text": "presentation uh so so Kafka streams tries to make it easier by hiding a lot",
    "start": "2155359",
    "end": "2160599"
  },
  {
    "text": "of the oh we have a local um uh Rock Deb uh instance and we take",
    "start": "2160599",
    "end": "2168400"
  },
  {
    "text": "snapshots and we try to make recovery faster we just give you a very nice API uh you basically it's called K table",
    "start": "2168400",
    "end": "2175400"
  },
  {
    "text": "we're saying this is a stream that represents a table and then we say okay do a left join and my the example is on",
    "start": "2175400",
    "end": "2182520"
  },
  {
    "text": "our website and you can kind of uh take a look and there is a Blog that just explain exp how it works",
    "start": "2182520",
    "end": "2189720"
  },
  {
    "text": "in either delicious or excruciating detail depending on how much you like",
    "start": "2189720",
    "end": "2196200"
  },
  {
    "start": "2196000",
    "end": "2232000"
  },
  {
    "text": "details okay so I have a bonus content",
    "start": "2196319",
    "end": "2201480"
  },
  {
    "text": "on how to join streams to each other which is kind of cool H but I can also",
    "start": "2201480",
    "end": "2206720"
  },
  {
    "text": "take questions in the remaining the 10 minutes or so um anyone has preferences",
    "start": "2206720",
    "end": "2212760"
  },
  {
    "text": "like if you are burning with questions I'll go to questions and if you're not then I can just keep I'm talking anyone",
    "start": "2212760",
    "end": "2218760"
  },
  {
    "text": "who really really okay question sorry oh you want the bonus content okay",
    "start": "2218760",
    "end": "2226240"
  },
  {
    "text": "anyone has burning questions that he feels will he will not sleep if I don't answer okay we're",
    "start": "2226240",
    "end": "2231960"
  },
  {
    "text": "good bonus content so we have another app we want to decide on search",
    "start": "2231960",
    "end": "2237359"
  },
  {
    "start": "2232000",
    "end": "2266000"
  },
  {
    "text": "relevance the way search works is that the user searches for hotels in",
    "start": "2237359",
    "end": "2242640"
  },
  {
    "text": "Philadelphia and what we really want him to have most of the people just click on the first result and book it and then",
    "start": "2242640",
    "end": "2249319"
  },
  {
    "text": "maybe few more in the third second result and the rest kind of in the first page we don't really want anyone to keep",
    "start": "2249319",
    "end": "2254560"
  },
  {
    "text": "looking because that means that our search absolutely sucks H in reality well how do we even know right like we",
    "start": "2254560",
    "end": "2260880"
  },
  {
    "text": "need to somehow measure what do people click on when they search so now we have two",
    "start": "2260880",
    "end": "2266400"
  },
  {
    "text": "events we have page View events which uh are the usual page views and we have",
    "start": "2266400",
    "end": "2272960"
  },
  {
    "text": "search events which is what people search for session Tim stamp user ID have it and the search term so now we",
    "start": "2272960",
    "end": "2280160"
  },
  {
    "text": "know what people searched on this search term and we know how often they search for and we know what people viewed but",
    "start": "2280160",
    "end": "2285880"
  },
  {
    "text": "we need to somehow put it together right like we have those two strings of events that we need to get together we really",
    "start": "2285880",
    "end": "2291960"
  },
  {
    "start": "2289000",
    "end": "2339000"
  },
  {
    "text": "want to say that this search caused this click right so it's a kind of we're trying to infer a casual",
    "start": "2291960",
    "end": "2298960"
  },
  {
    "text": "relationship and obviously it's just inference but a good inference would be that if the click comes within let's say",
    "start": "2298960",
    "end": "2305800"
  },
  {
    "text": "20 30 seconds of the search by the same user then it's a match and yes he",
    "start": "2305800",
    "end": "2311839"
  },
  {
    "text": "searched and then clicked if the click comes half an hour later it's probably he searched through Google and somehow",
    "start": "2311839",
    "end": "2318040"
  },
  {
    "text": "found something in another way maybe Expedia or something so we have this kind of",
    "start": "2318040",
    "end": "2324200"
  },
  {
    "text": "inference which means that we have to do a join on two things right the first is the the session ID to make sure that",
    "start": "2324200",
    "end": "2330079"
  },
  {
    "text": "we're actually looking at the same user and the second is time we need to do a join based on time right we have this",
    "start": "2330079",
    "end": "2335839"
  },
  {
    "text": "10c window in which stuff still counts I see I was slightly optimistic",
    "start": "2335839",
    "end": "2341760"
  },
  {
    "start": "2339000",
    "end": "2388000"
  },
  {
    "text": "this is a five minute window but we basically what we do is now we have two caches one cache looks at five minute",
    "start": "2341760",
    "end": "2348920"
  },
  {
    "text": "window of one stream the other cache keeps five minute window of the other stream and we basically keep updating",
    "start": "2348920",
    "end": "2355440"
  },
  {
    "text": "those two caches and trying to join them so we have kind of a hash table with events from both that we keep matching",
    "start": "2355440",
    "end": "2361839"
  },
  {
    "text": "and also keeps aging stuff out which is a very nice example of um you know the",
    "start": "2361839",
    "end": "2368200"
  },
  {
    "text": "kind how the kind of data structures that you really hate in job interviews end up being really really useful in the",
    "start": "2368200",
    "end": "2373920"
  },
  {
    "text": "real world like if someone asks you to maintain a a hash table that allows you",
    "start": "2373920",
    "end": "2379000"
  },
  {
    "text": "to do hash joins and also allows you to expire events as they get older you'd probably say those crazy interviewers",
    "start": "2379000",
    "end": "2385079"
  },
  {
    "text": "nobody would ever need it in real life and here we",
    "start": "2385079",
    "end": "2389599"
  },
  {
    "text": "are H so and as time goes by you keep just maintaining those hash windows and",
    "start": "2390400",
    "end": "2396040"
  },
  {
    "text": "keep joining and keep results of the join fairly easy the only question is that time is",
    "start": "2396040",
    "end": "2403680"
  },
  {
    "start": "2400000",
    "end": "2482000"
  },
  {
    "text": "obviously really really critical here if everyone been in the beam I'm sure you've heard it before but uh time is",
    "start": "2403680",
    "end": "2411040"
  },
  {
    "text": "really really critical and there is a lot of ideas of what time is it that exist in a streaming system it can be",
    "start": "2411040",
    "end": "2417040"
  },
  {
    "text": "the time that the user actually clicked something in his browser like the user really did something or the web app got",
    "start": "2417040",
    "end": "2423480"
  },
  {
    "text": "got the click and loaded the page and created an event or or the producer in the w web app created an event to send",
    "start": "2423480",
    "end": "2430480"
  },
  {
    "text": "into Kafka that's slightly later than when the web app first created the event probably when the producer actually did",
    "start": "2430480",
    "end": "2435800"
  },
  {
    "text": "descend we may be doing some batching and then there's the times that the event really go to Kafka which can be a",
    "start": "2435800",
    "end": "2441440"
  },
  {
    "text": "long time later who knows what network partitions exist and then it can be the time that the stream processing app is",
    "start": "2441440",
    "end": "2446880"
  },
  {
    "text": "reading the event which can be months or years later for that matter because who knows when we decided to add this kind",
    "start": "2446880",
    "end": "2453040"
  },
  {
    "text": "of application so we need to be a bit careful about it the best time is obviously the ones that your application",
    "start": "2453040",
    "end": "2458839"
  },
  {
    "text": "put in h in Li of that Kafka producers always put in a time stamp just in case",
    "start": "2458839",
    "end": "2463960"
  },
  {
    "text": "and if you have an old producer that doesn't do it Kafka itself will put in a time stamp also just in case so you have",
    "start": "2463960",
    "end": "2470160"
  },
  {
    "text": "a lot of places just don't do the whole if now it's 9:30 and my web applic and",
    "start": "2470160",
    "end": "2475839"
  },
  {
    "text": "my streams application just got the event then the event is at 9:30 this is pretty much never ever correct don't",
    "start": "2475839",
    "end": "2483800"
  },
  {
    "start": "2482000",
    "end": "2514000"
  },
  {
    "text": "don't okay and then if due to network partitions gotten late events our system",
    "start": "2483800",
    "end": "2489560"
  },
  {
    "text": "can is actually capable of joining to an earlier hash table as long as we actually keep them in memory so you can",
    "start": "2489560",
    "end": "2495520"
  },
  {
    "text": "even say something like please keep all the hash tables live in memory especially if they're small enough for",
    "start": "2495520",
    "end": "2500640"
  },
  {
    "text": "maybe half an hour because some of our applications end up reporting stuff a bit later so that's that's a nice thing",
    "start": "2500640",
    "end": "2507839"
  },
  {
    "text": "that you can do right you can just basically take an event say oh it's old let's join it with an older hash table",
    "start": "2507839",
    "end": "2514839"
  },
  {
    "start": "2514000",
    "end": "2595000"
  },
  {
    "text": "it sounds super complicated and it is actually very Challen alling to implement ask me how I know H so you",
    "start": "2514839",
    "end": "2520400"
  },
  {
    "text": "need those two buffers and then again you maintaining State fail over all those things that makes life fun again",
    "start": "2520400",
    "end": "2527119"
  },
  {
    "text": "kafa streams makes it Easy by the way I'm saying that kafa streams makes it easy because I know CFA streams really",
    "start": "2527119",
    "end": "2533000"
  },
  {
    "text": "well and I wrote All My examples in Kafka streams I'm not saying that oh it's really hard in Flink or oh it's",
    "start": "2533000",
    "end": "2538800"
  },
  {
    "text": "really hard in spark or oh it's really hard in beam because I wouldn't know I don't know Flink I don't know vest I",
    "start": "2538800",
    "end": "2545040"
  },
  {
    "text": "know spark from like three years ago it definitely didn't have it then but who knows what they have now nope I guess",
    "start": "2545040",
    "end": "2550800"
  },
  {
    "text": "nope H Flink may have it there are slightly more streams forward H maybe",
    "start": "2550800",
    "end": "2556720"
  },
  {
    "text": "gold dataflow probably has it because they keep talking about it so I'm guessing they have it so this is an",
    "start": "2556720",
    "end": "2561800"
  },
  {
    "text": "example don't take it as we have it and they don't kind of thing we we love all",
    "start": "2561800",
    "end": "2567200"
  },
  {
    "text": "stream processing systems that's kind of the thing with Kafka right because we work with everyone and then we can uh",
    "start": "2567200",
    "end": "2574200"
  },
  {
    "text": "you can see um an example of how I do it in my H GitHub by the way H Kafka streams",
    "start": "2574200",
    "end": "2582400"
  },
  {
    "text": "is still at the point where apis sometimes change H so at the time when you look at the example there is some",
    "start": "2582400",
    "end": "2588359"
  },
  {
    "text": "chance it wouldn't actually work pool requests are really really welcome it's just small API",
    "start": "2588359",
    "end": "2595040"
  },
  {
    "start": "2595000",
    "end": "2629000"
  },
  {
    "text": "changes okay going back stream all things into CA keep compatibility if you",
    "start": "2595040",
    "end": "2601640"
  },
  {
    "text": "listen to nothing else from this presentation please keep compatibility nothing else will screw you over as as",
    "start": "2601640",
    "end": "2607280"
  },
  {
    "text": "hard as breaking compatibility in events um use whatever you can to make simple",
    "start": "2607280",
    "end": "2613040"
  },
  {
    "text": "stuff simple and then if you need to do complex stuff then there is this these nice enrichment patterns please don't",
    "start": "2613040",
    "end": "2619880"
  },
  {
    "text": "write stream processing applications that have to query 11 services this will bite you slightly less hard than",
    "start": "2619880",
    "end": "2625920"
  },
  {
    "text": "compatibility but not zero pain if you like this kind of stuff we are doing",
    "start": "2625920",
    "end": "2631240"
  },
  {
    "start": "2629000",
    "end": "2652000"
  },
  {
    "text": "Kafka Summit and it's in New York which is kind of close by if you're in Chicago",
    "start": "2631240",
    "end": "2636640"
  },
  {
    "text": "and you have this nice discount and it's next week so it's kind of last minute if you didn't register yet but it's going to be a lot of fun and D said he's going",
    "start": "2636640",
    "end": "2643200"
  },
  {
    "text": "to be there by the way so so he's a totally good H person to meet even if",
    "start": "2643200",
    "end": "2650440"
  },
  {
    "text": "you if you happen to be in New York",
    "start": "2650440",
    "end": "2654000"
  },
  {
    "start": "2652000",
    "end": "2919000"
  },
  {
    "text": "City um yeah so I'm done I know it was fast do we have time left for questions",
    "start": "2657960",
    "end": "2663200"
  },
  {
    "text": "yeah few so um earlier there was talk I mentioned it as well the idea of all of these streaming engines supporting beam",
    "start": "2663200",
    "end": "2670440"
  },
  {
    "text": "as like a top level API has there been any discussion about that in the cop streams I think we've gotten questions",
    "start": "2670440",
    "end": "2678400"
  },
  {
    "text": "and but we got questions about a lot of things like we get questions about when will we support Scala and when we",
    "start": "2678400",
    "end": "2684280"
  },
  {
    "text": "support Python and when we support go and when we support net and when we support nodejs so we have a lot of like",
    "start": "2684280",
    "end": "2691359"
  },
  {
    "text": "just prioritizing the whole thing is really hard H so right now we don't",
    "start": "2691359",
    "end": "2696400"
  },
  {
    "text": "obviously in the open source Community the easiest way to highly prioritize your request is to have someone go ahead",
    "start": "2696400",
    "end": "2703160"
  },
  {
    "text": "and just contribute a nice bunch of code to the project H so that's Al that's a",
    "start": "2703160",
    "end": "2710880"
  },
  {
    "text": "way to get high so I know like for python a bunch of people kept asking and we kept like well we'll get to it but",
    "start": "2710880",
    "end": "2716200"
  },
  {
    "text": "we're not really there yet and then they just went and opened the gaab project Kafka streams python where they're doing",
    "start": "2716200",
    "end": "2722079"
  },
  {
    "text": "their stuff H so I'm guessing that at some point if enough people are interested there will be a Kafka streams",
    "start": "2722079",
    "end": "2728920"
  },
  {
    "text": "beam kind of showing up probably in beam right because they host most of the Integrations right they don't really put",
    "start": "2728920",
    "end": "2734359"
  },
  {
    "text": "it in the other project like they I think they host the spark streaming one and so on so yeah I'm guessing one day",
    "start": "2734359",
    "end": "2741079"
  },
  {
    "text": "either we will gather first or someone else will get it her first but yeah I'm guessing that it will happen I'm very",
    "start": "2741079",
    "end": "2746520"
  },
  {
    "text": "excited about the idea that again I'm XDP I wrote a lot of SQL and I moved from oracl to SQL Server to mySQL",
    "start": "2746520",
    "end": "2754839"
  },
  {
    "text": "basically almost no h except they how the hell do I take backups because a lot of my scripts in Sequel still worked and",
    "start": "2754839",
    "end": "2761200"
  },
  {
    "text": "I kind of expect be to end up I hope it will end up doing the same thing for",
    "start": "2761200",
    "end": "2767359"
  },
  {
    "text": "us yes is there an off shell feature that I can things in I have to oh that's",
    "start": "2768640",
    "end": "2776920"
  },
  {
    "text": "an excellent question because earlier this uh week a the EMP poent producer",
    "start": "2776920",
    "end": "2783599"
  },
  {
    "text": "has been merged into Apachi Kafka which means that you can actually avoid duplicates in Kafka itself you can have",
    "start": "2783599",
    "end": "2790760"
  },
  {
    "text": "a producers that basically checks for lack of duplicates and it does sequence numbers it does pretty complex stuff",
    "start": "2790760",
    "end": "2797720"
  },
  {
    "text": "before it writes thata to Kafka so we think and then streams itself is using",
    "start": "2797720",
    "end": "2803200"
  },
  {
    "text": "those commits and checkpoints to make sure that it's exactly once so we think that we reduced drastically the need for",
    "start": "2803200",
    "end": "2811599"
  },
  {
    "text": "um the duping at least we hope so and the other cool thing about D producer is",
    "start": "2811599",
    "end": "2817400"
  },
  {
    "text": "that we added transactions so you can now write to a lot of different topics and partitions and you say begin",
    "start": "2817400",
    "end": "2822839"
  },
  {
    "text": "transaction right right right right right commit and you can have read committed consumers that only read the",
    "start": "2822839",
    "end": "2830480"
  },
  {
    "text": "stuff that was committed and if something got aborted they will not see partial transactions which is absolutely",
    "start": "2830480",
    "end": "2835960"
  },
  {
    "text": "mind breakingly changes everything in my word kind of thing and I I'm super excited about it a Kafka Summit this the",
    "start": "2835960",
    "end": "2843960"
  },
  {
    "text": "one we almost missed you will have a talk about exactly how it's done and how to use it and all the goas which",
    "start": "2843960",
    "end": "2849559"
  },
  {
    "text": "apparently there are gochas the biggest one people keep asking me is it slower if I use transactions because everyone knows transactions are slower it's",
    "start": "2849559",
    "end": "2856280"
  },
  {
    "text": "actually significantly faster not because transactions are faster we didn't break physics or anything but",
    "start": "2856280",
    "end": "2862160"
  },
  {
    "text": "while we implemented transactions we H also um optimized the data protocol of",
    "start": "2862160",
    "end": "2869960"
  },
  {
    "text": "Kafka by a lot so basically if you produce more than two events every 3",
    "start": "2869960",
    "end": "2876200"
  },
  {
    "text": "milliseconds or or something it you're saving a you you sending a lot smaller H",
    "start": "2876200",
    "end": "2882400"
  },
  {
    "text": "batches even if you don't compress anything it will just send you smaller batches and uh that magically makes",
    "start": "2882400",
    "end": "2888760"
  },
  {
    "text": "everything faster because apparently it's always the network that is the bottleneck or it is a network or dis you",
    "start": "2888760",
    "end": "2894280"
  },
  {
    "text": "save either way okay um I'm here tonight and then",
    "start": "2894280",
    "end": "2901960"
  },
  {
    "text": "flying out really early I have Twitter gwap if anyone feels like chatting about the data integration stuff I'm very much",
    "start": "2901960",
    "end": "2908960"
  },
  {
    "text": "into it [Applause]",
    "start": "2908960",
    "end": "2920130"
  }
]