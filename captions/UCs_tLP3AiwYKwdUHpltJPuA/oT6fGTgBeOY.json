[
  {
    "start": "0",
    "end": "83000"
  },
  {
    "text": "hi everyone and welcome to my talk i'm really super excited to be here uh it's super good",
    "start": "13519",
    "end": "19760"
  },
  {
    "text": "that conference are back to being in person and uh well",
    "start": "19760",
    "end": "25599"
  },
  {
    "text": "i'm a data scientist working at nvidia and my day-to-day job is",
    "start": "25599",
    "end": "30960"
  },
  {
    "text": "actually supporting our partners and customers in developing and deploying efficient ai",
    "start": "30960",
    "end": "37600"
  },
  {
    "text": "video ai video analytics applications and when doing this i'm aiming at making",
    "start": "37600",
    "end": "45600"
  },
  {
    "text": "it as efficient and as possible so i'm focusing on",
    "start": "45600",
    "end": "51840"
  },
  {
    "text": "supporting the development and deployment of ai models and over the past couple of years i",
    "start": "51840",
    "end": "59520"
  },
  {
    "text": "collected uh well a set of tips and tricks on how to make",
    "start": "59520",
    "end": "65040"
  },
  {
    "text": "uh development and deployment of ai models as efficient as possible and the goal of my today's box is to share with",
    "start": "65040",
    "end": "72080"
  },
  {
    "text": "you these tips and tricks and i called my talk best practices for real-time intelligent video analytics",
    "start": "72080",
    "end": "79920"
  },
  {
    "text": "um so i didn't start and the first question i'm going to answer",
    "start": "79920",
    "end": "86720"
  },
  {
    "start": "83000",
    "end": "387000"
  },
  {
    "text": "is uh why uh video analytics and if i post here for a moment and let you",
    "start": "86720",
    "end": "94000"
  },
  {
    "text": "think uh what is the most popular iot sensor these days",
    "start": "94000",
    "end": "99600"
  },
  {
    "text": "and probably even without hesitation uh like many of you would answer that it's a video camera",
    "start": "99600",
    "end": "106079"
  },
  {
    "text": "and you will be right because video cameras are everywhere we have video cameras and subway trains we have video",
    "start": "106079",
    "end": "112880"
  },
  {
    "text": "cameras in the airports um well there's a bunch of cameras there's a bunch of cameras even here",
    "start": "112880",
    "end": "119439"
  },
  {
    "text": "so all these cameras they generate huge amounts of data and huge amounts of data",
    "start": "119439",
    "end": "128160"
  },
  {
    "text": "means that we have so many insights to get out of this data and it means that well we need",
    "start": "128160",
    "end": "136000"
  },
  {
    "text": "some tools to do that and the best tool for that is ai so that's why we say intelligent video",
    "start": "136000",
    "end": "142319"
  },
  {
    "text": "analytics so it's ai video analytics and working with working very closely to our",
    "start": "142319",
    "end": "148640"
  },
  {
    "text": "partners and customers i am privileged to being exposed to",
    "start": "148640",
    "end": "154480"
  },
  {
    "text": "well different use cases we can have in intelligent video analytics so if you",
    "start": "154480",
    "end": "159680"
  },
  {
    "text": "look here we have applications in traffic management think of the applications like",
    "start": "159680",
    "end": "166239"
  },
  {
    "text": "smart parking where you can well have your cameras set up around your parking",
    "start": "166239",
    "end": "171280"
  },
  {
    "text": "house and you can intelligently say that this parking house has like five free",
    "start": "171280",
    "end": "177120"
  },
  {
    "text": "spots available and then you can guide your drivers immediately to find the",
    "start": "177120",
    "end": "182400"
  },
  {
    "text": "spots this is next application in manufacturing we can have ai for defect",
    "start": "182400",
    "end": "190159"
  },
  {
    "text": "detection and for quality control in production and many of our customers are doing that",
    "start": "190159",
    "end": "197280"
  },
  {
    "text": "and they're deploying these models in the real world in retail uh think of the applications",
    "start": "197280",
    "end": "202720"
  },
  {
    "text": "like uh shopping a shop analytics so where you put the cameras into your big",
    "start": "202720",
    "end": "208319"
  },
  {
    "text": "supermarket and you can identify which are the hot and cold areas of your supermarket",
    "start": "208319",
    "end": "214480"
  },
  {
    "text": "which is very helpful for a smart product placement the second application in retail can be",
    "start": "214480",
    "end": "221760"
  },
  {
    "text": "autonomous shopping and you may have heard of applications like amazon go is the most popular one",
    "start": "221760",
    "end": "228400"
  },
  {
    "text": "where your customer walks into your store grabs the product and leaves the store without going to the cashier",
    "start": "228400",
    "end": "235840"
  },
  {
    "text": "so which means that well the money is being charged from the account automatically and the whole process is",
    "start": "235840",
    "end": "242799"
  },
  {
    "text": "going seamlessly with the help of ai and the next application area is health care",
    "start": "242799",
    "end": "249680"
  },
  {
    "text": "and hospitals and here think about such an application is for example surgery assistance and often",
    "start": "249680",
    "end": "257120"
  },
  {
    "text": "uh it happens that surgical instruments being left in patients body imagine that it happens",
    "start": "257120",
    "end": "263360"
  },
  {
    "text": "so we can use ai assistant which helped during the surgery and",
    "start": "263360",
    "end": "269440"
  },
  {
    "text": "it will prevent of such accidents of happening uh in the factories uh we are really uh",
    "start": "269440",
    "end": "277199"
  },
  {
    "text": "aiming at uh safety in factories and imagine the situation where autonomous machines working closely with human",
    "start": "277199",
    "end": "284639"
  },
  {
    "text": "workers and we want we don't want this autonomous machines colliding with human",
    "start": "284639",
    "end": "290160"
  },
  {
    "text": "workers so we implement ai to protect our human workers",
    "start": "290160",
    "end": "295360"
  },
  {
    "text": "another application in factories we can see if the",
    "start": "295360",
    "end": "300800"
  },
  {
    "text": "workers are following the guidelines and wearing hard hats of protective vests for example",
    "start": "300800",
    "end": "307440"
  },
  {
    "text": "which brings me to the topic of public spaces so in the public spaces we can do all kinds",
    "start": "307440",
    "end": "313440"
  },
  {
    "text": "of crowd analytics and recently during the pandemics many of our",
    "start": "313440",
    "end": "319039"
  },
  {
    "text": "customers have actually introduced this into their cctv systems is",
    "start": "319039",
    "end": "325520"
  },
  {
    "text": "detecting if people are wearing protective masks for covet so it's a really very important",
    "start": "325520",
    "end": "332000"
  },
  {
    "text": "applications these days or for example seeing if people are practicing social",
    "start": "332000",
    "end": "337039"
  },
  {
    "text": "distances right um in the warehouse and logistics we can",
    "start": "337039",
    "end": "342400"
  },
  {
    "text": "have uh autonomous robots also like robotic arm pickers to automatically",
    "start": "342400",
    "end": "348320"
  },
  {
    "text": "grab the packages read the barcodes and send the packages in the right direction",
    "start": "348320",
    "end": "354960"
  },
  {
    "text": "or we can have delivery robots which would go and we should take the products from the",
    "start": "354960",
    "end": "361039"
  },
  {
    "text": "shelves and package them also automatically it's another application of ai in the warehouse and logistics and",
    "start": "361039",
    "end": "367759"
  },
  {
    "text": "finally in transportation hubs so here again uh we put safety is very important in",
    "start": "367759",
    "end": "374080"
  },
  {
    "text": "transportation hubs and uh many of our clients are big airports and",
    "start": "374080",
    "end": "380400"
  },
  {
    "text": "they developed and deploy ai solutions for safety in the airports",
    "start": "380400",
    "end": "386560"
  },
  {
    "start": "387000",
    "end": "529000"
  },
  {
    "text": "and as a data scientist you might know that there are some challenges associated",
    "start": "387039",
    "end": "393199"
  },
  {
    "text": "with development and deployment process and the first biggest challenge is",
    "start": "393199",
    "end": "398960"
  },
  {
    "text": "accuracy and here i tend to say that it's a twofold challenge so the first one you",
    "start": "398960",
    "end": "405199"
  },
  {
    "text": "need to design your model to be as accurate as possible and as suitable as possible for your",
    "start": "405199",
    "end": "413120"
  },
  {
    "text": "application and another challenge here is in collecting right data for your to",
    "start": "413120",
    "end": "420560"
  },
  {
    "text": "train this model so your data set has to be has to have a good representation and",
    "start": "420560",
    "end": "426080"
  },
  {
    "text": "coverage of all possible use cases it needs to be well balanced and",
    "start": "426080",
    "end": "431280"
  },
  {
    "text": "of course it needs to be annotated and annotation is a costly process",
    "start": "431280",
    "end": "437199"
  },
  {
    "text": "uh the next challenge is throughput so imagine now you have created a super",
    "start": "437199",
    "end": "442400"
  },
  {
    "text": "accurate model and you're ready to deploy it but now you're facing the next challenge you put your model in your",
    "start": "442400",
    "end": "450240"
  },
  {
    "text": "device in your tiny robot or on your mobile phone and you notice that it's slow it doesn't",
    "start": "450240",
    "end": "456880"
  },
  {
    "text": "perform well enough uh in in the given hardware so the next challenge here is",
    "start": "456880",
    "end": "462560"
  },
  {
    "text": "to take your model and to optimize it to make it as fast and as efficient as",
    "start": "462560",
    "end": "468160"
  },
  {
    "text": "possible to in terms of throughput and latency and the third challenge here so imagine",
    "start": "468160",
    "end": "475840"
  },
  {
    "text": "now you came up with the optimization techniques and you optimize your model it's accurate it's fast",
    "start": "475840",
    "end": "482560"
  },
  {
    "text": "the third challenge the final challenge now is how do you deploy it at scale so imagine you have in the whole world",
    "start": "482560",
    "end": "489360"
  },
  {
    "text": "thousands of installations and you need to update your software you need to update your models so sometimes some",
    "start": "489360",
    "end": "496560"
  },
  {
    "text": "some of the models you might know are seasonal for example you constantly need",
    "start": "496560",
    "end": "501759"
  },
  {
    "text": "to re-update this model or like the data is evolving so this is very important",
    "start": "501759",
    "end": "507520"
  },
  {
    "text": "and this is the third challenge and at nvidia we work hard to tackle all of",
    "start": "507520",
    "end": "513760"
  },
  {
    "text": "these challenges and we have a set of solutions and set of",
    "start": "513760",
    "end": "518880"
  },
  {
    "text": "best practices which we recommend to everyone and today i'm going to share with you these best practices and",
    "start": "518880",
    "end": "525440"
  },
  {
    "text": "solutions",
    "start": "525440",
    "end": "528000"
  },
  {
    "start": "529000",
    "end": "540000"
  },
  {
    "text": "and i will start with the simplest technique ever you",
    "start": "531120",
    "end": "536880"
  },
  {
    "text": "can implement you can apply when developing an ai system which is called transfer",
    "start": "536880",
    "end": "543120"
  },
  {
    "start": "540000",
    "end": "663000"
  },
  {
    "text": "learning uh the way it works so you take a model pre-trained on some kind of data set",
    "start": "543120",
    "end": "549920"
  },
  {
    "text": "let's say not necessarily the data set representing your own data and not necessarily uh representing your use",
    "start": "549920",
    "end": "556800"
  },
  {
    "text": "case and you train your model well you don't train your model you take this model trained by somebody else and you retrain",
    "start": "556800",
    "end": "564560"
  },
  {
    "text": "this model adapting it to your own data set it is it makes the whole development",
    "start": "564560",
    "end": "571839"
  },
  {
    "text": "process much faster because you don't have to train for let's say a thousand epochs you only need",
    "start": "571839",
    "end": "577760"
  },
  {
    "text": "about 100 epochs to find tune a model right and it also uh well uh you need now less",
    "start": "577760",
    "end": "584880"
  },
  {
    "text": "data so you don't need you don't need the whole big data set for that so smaller data set is sufficient and why",
    "start": "584880",
    "end": "591839"
  },
  {
    "text": "it works is because we when we are working for example with images uh also with textual data as well it applies",
    "start": "591839",
    "end": "599519"
  },
  {
    "text": "so in images we can have similar patterns like these are blobs some corners uh",
    "start": "599519",
    "end": "605920"
  },
  {
    "text": "some structures the same with words like we have letters we have words uh so",
    "start": "605920",
    "end": "612160"
  },
  {
    "text": "and uh it helps us well to reuse these structures learned from one data set and",
    "start": "612160",
    "end": "619200"
  },
  {
    "text": "reapply them to your own model so definitely use transfer learning whatever models you develop and i'll say",
    "start": "619200",
    "end": "625440"
  },
  {
    "text": "more here i used to work in uh well medical image processing where you know like you process uh x-ray data which are",
    "start": "625440",
    "end": "633360"
  },
  {
    "text": "grayscale images right and uh just out of fun i thought uh what if i take for",
    "start": "633360",
    "end": "638959"
  },
  {
    "text": "example a model trained on the imagenet data set which is you may know that it's a data set of like general objects",
    "start": "638959",
    "end": "646959"
  },
  {
    "text": "and you know what the performance of my uh grayscale model it actually went up",
    "start": "646959",
    "end": "653279"
  },
  {
    "text": "so it definitely helps also in terms of performance and even if the data is has nothing to do with what you have in your",
    "start": "653279",
    "end": "659920"
  },
  {
    "text": "own data set and the second technique is data augmentation and data",
    "start": "659920",
    "end": "666160"
  },
  {
    "start": "663000",
    "end": "739000"
  },
  {
    "text": "augmentation is also very similar but also very powerful what it means so you take your",
    "start": "666160",
    "end": "672880"
  },
  {
    "text": "previously created data set and you modify it slightly applying some very",
    "start": "672880",
    "end": "678959"
  },
  {
    "text": "cheap to use computer vision techniques like blurring spatial transformations and some color",
    "start": "678959",
    "end": "685680"
  },
  {
    "text": "transformations and turns out when you take your data set and you",
    "start": "685680",
    "end": "690800"
  },
  {
    "text": "distort it this way you make it you make a model robuster and",
    "start": "690800",
    "end": "696720"
  },
  {
    "text": "also robust two different like kinds of variations in your data like translation",
    "start": "696720",
    "end": "702720"
  },
  {
    "text": "uh scaling and all this so also use data augmentation and you can use data",
    "start": "702720",
    "end": "708320"
  },
  {
    "text": "augmentation in two ways uh you can pre uh pre",
    "start": "708320",
    "end": "713519"
  },
  {
    "text": "pre pre-create your augmented data set uh before the training but you also can do",
    "start": "713519",
    "end": "719839"
  },
  {
    "text": "it uh before every uh before creating every new batch so there are two ways and i recommend",
    "start": "719839",
    "end": "726160"
  },
  {
    "text": "the second way because which is called online data augmentation",
    "start": "726160",
    "end": "731360"
  },
  {
    "text": "which makes it even more random",
    "start": "731360",
    "end": "735920"
  },
  {
    "text": "the next technique is also associated with training it is designed to make a",
    "start": "737440",
    "end": "742560"
  },
  {
    "start": "739000",
    "end": "843000"
  },
  {
    "text": "training more efficient and just out of curiosity how many of you have heard about automated mix precision",
    "start": "742560",
    "end": "749519"
  },
  {
    "text": "just raise your hand if you hurry about it okay not very many uh so",
    "start": "749519",
    "end": "755920"
  },
  {
    "text": "apparently well this technique is super helpful when you want to run your training faster and you want to iterate",
    "start": "755920",
    "end": "762000"
  },
  {
    "text": "faster and the way it works so you take your data and you start training in floating point 32 precision",
    "start": "762000",
    "end": "769440"
  },
  {
    "text": "as a next step you convert your data to floating point 16 and you run through all the",
    "start": "769440",
    "end": "776320"
  },
  {
    "text": "convolutions in floating point 16. when you want to compute your loss and",
    "start": "776320",
    "end": "782720"
  },
  {
    "text": "this is like a very sensitive process you need the highest accuracy here",
    "start": "782720",
    "end": "787760"
  },
  {
    "text": "you convert the data back to the floating point 32 and do the optimization step in",
    "start": "787760",
    "end": "793279"
  },
  {
    "text": "floating.32 when you're ready to backpropagate you convert your model back to point 16",
    "start": "793279",
    "end": "800720"
  },
  {
    "text": "and back propagating floating point 16. that all makes the training up to",
    "start": "800720",
    "end": "806440"
  },
  {
    "text": "1.5 times faster which means you can iterate faster you can try different",
    "start": "806440",
    "end": "812160"
  },
  {
    "text": "hyper parameters faster and also you can well reducing precision actually allows",
    "start": "812160",
    "end": "819279"
  },
  {
    "text": "you to train bigger models or use a bigger batch size so it's a very powerful technique and if",
    "start": "819279",
    "end": "825760"
  },
  {
    "text": "your gpu supports it definitely use it and it's it's really easy to use like in",
    "start": "825760",
    "end": "831600"
  },
  {
    "text": "popular frameworks like tensorflow or pi torch it's just a matter of adding like two extra lines of code",
    "start": "831600",
    "end": "837920"
  },
  {
    "text": "and it is it is very helpful the next technique",
    "start": "837920",
    "end": "844560"
  },
  {
    "start": "843000",
    "end": "978000"
  },
  {
    "text": "called quantization and it also as you may notice has to do with",
    "start": "844560",
    "end": "850399"
  },
  {
    "text": "floating point precision so typically after you have trained your",
    "start": "850399",
    "end": "855600"
  },
  {
    "text": "model it is in floating.32 precision and uh if you want to deploy it on some",
    "start": "855600",
    "end": "863040"
  },
  {
    "text": "tiny embedded edge device uh you typically want to reduce uh this",
    "start": "863040",
    "end": "868240"
  },
  {
    "text": "floating point precision also for deployment so the way you do it you just round up",
    "start": "868240",
    "end": "873839"
  },
  {
    "text": "all the bits until you have floating point 16 first let's say 1.16",
    "start": "873839",
    "end": "879040"
  },
  {
    "text": "and your model already performs faster and you typically don't lose much of accuracy here",
    "start": "879040",
    "end": "885199"
  },
  {
    "text": "but then our engineers thought further like what if we actually reduce our bits",
    "start": "885199",
    "end": "891120"
  },
  {
    "text": "even to lower precision let's say integer eight and uh while reducing to integer 8 means",
    "start": "891120",
    "end": "898480"
  },
  {
    "text": "that we cannot just round up our floating point numbers because if we lose the whole decimal numbers it means",
    "start": "898480",
    "end": "905839"
  },
  {
    "text": "we'll lose a lot of precision uh so then we came up with a so-called",
    "start": "905839",
    "end": "911440"
  },
  {
    "text": "mapping where we take our dynamic range and map it to the range of from",
    "start": "911440",
    "end": "916959"
  },
  {
    "text": "minus 128 to plus 128 for that you need to find the mean and max in the weight",
    "start": "916959",
    "end": "923920"
  },
  {
    "text": "ranges in your model and to do that you can do it in two ways the first way to do it is you take some",
    "start": "923920",
    "end": "931120"
  },
  {
    "text": "calibration data set and run a couple of epochs to find this means and max and",
    "start": "931120",
    "end": "936880"
  },
  {
    "text": "create this mapping it's called post-training quantization or calibration",
    "start": "936880",
    "end": "942880"
  },
  {
    "text": "and the second way uh apparently you can actually do this when you do your training and it's",
    "start": "942880",
    "end": "948720"
  },
  {
    "text": "called quantization aware training and by doing that you actually model",
    "start": "948720",
    "end": "954320"
  },
  {
    "text": "your mapping uh in addition to like estimating normal loss you also estimate loss for",
    "start": "954320",
    "end": "961279"
  },
  {
    "text": "calibration so there are two ways to do it and usually when you apply quantization",
    "start": "961279",
    "end": "968240"
  },
  {
    "text": "it helps you to use well the model footprint in size but it also helps you to increase the",
    "start": "968240",
    "end": "974560"
  },
  {
    "text": "throughput of your model and the next technique is network",
    "start": "974560",
    "end": "980399"
  },
  {
    "start": "978000",
    "end": "1090000"
  },
  {
    "text": "pruning so network pruning the idea behind is that you can well from your",
    "start": "980399",
    "end": "986000"
  },
  {
    "text": "network you can remove the neurons which do not participate in the final outcome",
    "start": "986000",
    "end": "992079"
  },
  {
    "text": "and the way to do it usually you well estimate uh you run the inference and you see like",
    "start": "992079",
    "end": "999440"
  },
  {
    "text": "on the test data set and you see which neurons are kind of lazy compared to some threshold so you rank this neurons",
    "start": "999440",
    "end": "1006399"
  },
  {
    "text": "and you remove all those which do not participate in final decision and the",
    "start": "1006399",
    "end": "1011920"
  },
  {
    "text": "picture here it shows a simplified model which is a perceptron and",
    "start": "1011920",
    "end": "1017040"
  },
  {
    "text": "it's it's just a simplification so usually pruning works on convolutional level so",
    "start": "1017040",
    "end": "1023199"
  },
  {
    "text": "uh applying pruning you can reduce the size of your model like sometimes up to 10 times but you need to",
    "start": "1023199",
    "end": "1030480"
  },
  {
    "text": "bear in mind that it's a well deep learning is a stochastic process and",
    "start": "1030480",
    "end": "1035839"
  },
  {
    "text": "with everything random it's hard to predict uh if you get 10 10 times",
    "start": "1035839",
    "end": "1041199"
  },
  {
    "text": "increase decrease in size or a little bit less but it definitely worth trying and pruning is also supported in popular",
    "start": "1041199",
    "end": "1048160"
  },
  {
    "text": "frameworks like tensorflow pytorch uh also when you reduce the connections",
    "start": "1048160",
    "end": "1054400"
  },
  {
    "text": "between neurons it also makes your model faster and it's important to say that pruning",
    "start": "1054400",
    "end": "1060240"
  },
  {
    "text": "is a lossy process so usually after you remove some neurons you modify your network structure and some accuracy gets",
    "start": "1060240",
    "end": "1066960"
  },
  {
    "text": "lost but hopefully not too much if you chose the threshold wisely",
    "start": "1066960",
    "end": "1072240"
  },
  {
    "text": "uh so for that you need to do some retraining like you need to run a couple",
    "start": "1072240",
    "end": "1078080"
  },
  {
    "text": "of iterations retraining and the good thing you can combine it for example with something like quantization aware",
    "start": "1078080",
    "end": "1083760"
  },
  {
    "text": "training and have two-in-one optimizations done",
    "start": "1083760",
    "end": "1089919"
  },
  {
    "start": "1090000",
    "end": "1191000"
  },
  {
    "text": "the techniques i covered before they are widely available",
    "start": "1090880",
    "end": "1096320"
  },
  {
    "text": "in all popular frameworks and are really super simple to use the following couple of techniques would",
    "start": "1096320",
    "end": "1102960"
  },
  {
    "text": "require more knowledge on cuda level and programming",
    "start": "1102960",
    "end": "1108160"
  },
  {
    "text": "uh in depth with cuda but these are also powerful techniques and they like if",
    "start": "1108160",
    "end": "1114880"
  },
  {
    "text": "implemented wisely they help to improve your performance even further",
    "start": "1114880",
    "end": "1120559"
  },
  {
    "text": "and the first one is called layer intense fusion and if you see the image here",
    "start": "1120559",
    "end": "1126080"
  },
  {
    "text": "you may recognize the popular inception architecture and you see here that",
    "start": "1126080",
    "end": "1132080"
  },
  {
    "text": "the image on the left uh it has the same structure but",
    "start": "1132080",
    "end": "1138400"
  },
  {
    "text": "some of the components like relu bias and convolution they are merged so they",
    "start": "1138400",
    "end": "1144160"
  },
  {
    "text": "are fused horizontally and vertically and the reason we do that is that uh",
    "start": "1144160",
    "end": "1150960"
  },
  {
    "text": "well calling a cuda kernel well coulda coulda kernels are super fast and uh",
    "start": "1150960",
    "end": "1156960"
  },
  {
    "text": "combination with these kudo kernels is super fast but passing the data from the cpu from well",
    "start": "1156960",
    "end": "1164240"
  },
  {
    "text": "from your code to this kernel is actually a bottleneck and in order to avoid this bottleneck we",
    "start": "1164240",
    "end": "1170400"
  },
  {
    "text": "try to reduce the number of cuda kernels and instead of having like three cuda",
    "start": "1170400",
    "end": "1176000"
  },
  {
    "text": "kernel calls like for reload for bias and convolution we can have a one combined in a single cbr structure",
    "start": "1176000",
    "end": "1183840"
  },
  {
    "text": "and this helps to reduce latency a lot",
    "start": "1183840",
    "end": "1189600"
  },
  {
    "text": "the next techniques called the next technique called kernel auto tuning",
    "start": "1190960",
    "end": "1196720"
  },
  {
    "start": "1191000",
    "end": "1252000"
  },
  {
    "text": "and for that well the reason behind is that we have so many different",
    "start": "1196720",
    "end": "1202559"
  },
  {
    "text": "hardware products to deploy our models at the right hand side we also have",
    "start": "1202559",
    "end": "1208960"
  },
  {
    "text": "so many different algorithms for every operation so there are many ways to implement the convolution",
    "start": "1208960",
    "end": "1215919"
  },
  {
    "text": "and all these algorithms they are well not all but most of them they perform",
    "start": "1215919",
    "end": "1221440"
  },
  {
    "text": "differently on different hardware uh take also this in a combination with",
    "start": "1221440",
    "end": "1226480"
  },
  {
    "text": "different batch size with different input size so the idea here is to sort of brute",
    "start": "1226480",
    "end": "1233600"
  },
  {
    "text": "force and to let your model run on your target",
    "start": "1233600",
    "end": "1238799"
  },
  {
    "text": "hardware but with a different combination of these algorithms and to select the best performant one for that",
    "start": "1238799",
    "end": "1245360"
  },
  {
    "text": "reason you need to perform this on the target hardware device",
    "start": "1245360",
    "end": "1251120"
  },
  {
    "text": "and the next technique the next optimization technique it's been inspired by a register allocation in",
    "start": "1251200",
    "end": "1258320"
  },
  {
    "start": "1252000",
    "end": "1293000"
  },
  {
    "text": "compiling which is the process of assigning large number of target program variables onto a smaller",
    "start": "1258320",
    "end": "1265520"
  },
  {
    "text": "number of regions and applied to deep learning it means combining tensors into regions",
    "start": "1265520",
    "end": "1272320"
  },
  {
    "text": "so region lifetime is a section of a network execution time and then we have",
    "start": "1272320",
    "end": "1278559"
  },
  {
    "text": "blocks so we assign regions to blocks and the regions assigned to a block",
    "start": "1278559",
    "end": "1284159"
  },
  {
    "text": "have a disjoint lifetimes so this is called dynamic tensor memory upon inference",
    "start": "1284159",
    "end": "1291840"
  },
  {
    "text": "and the next technique called multi-stream concurrent execution",
    "start": "1292320",
    "end": "1298640"
  },
  {
    "start": "1293000",
    "end": "1324000"
  },
  {
    "text": "and turns out you don't have to perform all operations sequentially so you can parallelize things like for example the",
    "start": "1298640",
    "end": "1306240"
  },
  {
    "text": "memory copy and the next kernel call so by stacking",
    "start": "1306240",
    "end": "1311280"
  },
  {
    "text": "them in this way depicted here you can actually also win some latency here for your model",
    "start": "1311280",
    "end": "1318720"
  },
  {
    "text": "so this were the techniques",
    "start": "1318720",
    "end": "1322320"
  },
  {
    "start": "1324000",
    "end": "1390000"
  },
  {
    "text": "and now i want to talk about free and when i say free i mean free for commercial use",
    "start": "1324400",
    "end": "1331120"
  },
  {
    "text": "free software products developed by nvidia to make your life as a data scientist as an engineer much",
    "start": "1331120",
    "end": "1338880"
  },
  {
    "text": "easier and most of these techniques which i well all of these techniques which i actually",
    "start": "1338880",
    "end": "1345200"
  },
  {
    "text": "described now they are implemented in our software products and i'll tell you how to use them",
    "start": "1345200",
    "end": "1353440"
  },
  {
    "text": "and i want to start this part quoting our ceo jensen huang who said",
    "start": "1353440",
    "end": "1359360"
  },
  {
    "text": "that nvidia is not a gpu company it's a platform company and by saying that he",
    "start": "1359360",
    "end": "1365840"
  },
  {
    "text": "means that well many people know us as a company who develops gpus or some people",
    "start": "1365840",
    "end": "1372000"
  },
  {
    "text": "think we are gaming company but we actually do a lot in ai and we actually",
    "start": "1372000",
    "end": "1377360"
  },
  {
    "text": "do a lot in software and in fact nvidia has even more software engineers than",
    "start": "1377360",
    "end": "1382960"
  },
  {
    "text": "hardware engineers can you imagine that so and our goal here is to basically",
    "start": "1382960",
    "end": "1389840"
  },
  {
    "text": "develop the platforms which will be used for many different industries and use",
    "start": "1389840",
    "end": "1395600"
  },
  {
    "start": "1390000",
    "end": "1532000"
  },
  {
    "text": "cases and applied to video analytics use case we came up with this end-to-end workflow",
    "start": "1395600",
    "end": "1405280"
  },
  {
    "text": "which starts here with our pre-trained model library or some people call it model zoo",
    "start": "1405280",
    "end": "1412400"
  },
  {
    "text": "and we basically train these models on our own proprietary data sets and",
    "start": "1412400",
    "end": "1420240"
  },
  {
    "text": "make them available for free for anyone to use and we usually store these models on our",
    "start": "1420240",
    "end": "1426880"
  },
  {
    "text": "nvidia gpu cloud it's a website it's not a cloud we're not a cloud provider but",
    "start": "1426880",
    "end": "1432000"
  },
  {
    "text": "it's called nvidia gpu cloud which is in fact is just a repository where we store",
    "start": "1432000",
    "end": "1437360"
  },
  {
    "text": "our gpu optimized software it is usually containerized",
    "start": "1437360",
    "end": "1442640"
  },
  {
    "text": "and well you can just pull this docker containers and deploy it on different hardware be it x86 gpus or",
    "start": "1442640",
    "end": "1450480"
  },
  {
    "text": "our embedded arm based jetson devices and the next step you can take these",
    "start": "1450480",
    "end": "1456840"
  },
  {
    "text": "models these pre-trained models and deploy it directly or if you think that well you need some",
    "start": "1456840",
    "end": "1463840"
  },
  {
    "text": "further training you can take these models and adapt it adapt these models to your own use case using your own data",
    "start": "1463840",
    "end": "1472080"
  },
  {
    "text": "and for that we provide a tool it's a zero coding tool called tower toolkit so",
    "start": "1472080",
    "end": "1477360"
  },
  {
    "text": "tau stands for train adapt and optimize uh after that you will have your own",
    "start": "1477360",
    "end": "1484080"
  },
  {
    "text": "model it it's it's your own it's uh nvidia doesn't have any claims to your model and you can",
    "start": "1484080",
    "end": "1490400"
  },
  {
    "text": "use it for production uh and to deploy a model uh we also",
    "start": "1490400",
    "end": "1495600"
  },
  {
    "text": "provide some sdks and the one sdk which we",
    "start": "1495600",
    "end": "1501120"
  },
  {
    "text": "will develop for video analytics is called deep stream and i'll also cover it in the following",
    "start": "1501120",
    "end": "1507520"
  },
  {
    "text": "slides and in addition to deep stream sdk we also provide",
    "start": "1507520",
    "end": "1513760"
  },
  {
    "text": "open source applications which we like post on github which everybody can",
    "start": "1513760",
    "end": "1519039"
  },
  {
    "text": "take and use and modify for your own use cases so this is",
    "start": "1519039",
    "end": "1525039"
  },
  {
    "text": "the end-to-end workflow in the nutshell and let's just take a look close at its components",
    "start": "1525039",
    "end": "1532400"
  },
  {
    "start": "1532000",
    "end": "1725000"
  },
  {
    "text": "and the first one i want to cover is the tower toolkit and here in the bottom",
    "start": "1532400",
    "end": "1539200"
  },
  {
    "text": "line you can see the supported hardware so when running tower toolkit you can train",
    "start": "1539200",
    "end": "1545600"
  },
  {
    "text": "your models on any of our data center or workstation gpus",
    "start": "1545600",
    "end": "1551840"
  },
  {
    "text": "x86 gpus we do not recommend training on our arm-based devices they are not for that",
    "start": "1551840",
    "end": "1559279"
  },
  {
    "text": "so as on the deployment side you can use here now our jetson devices and in",
    "start": "1559279",
    "end": "1564960"
  },
  {
    "text": "addition to all our data center gpus then on top of our hardware stack uh",
    "start": "1564960",
    "end": "1570320"
  },
  {
    "text": "there comes cuda x stack uh including nvidia container runtime uh cuda cuda for deep learning kdnn and",
    "start": "1570320",
    "end": "1578480"
  },
  {
    "text": "tens rt which is our optimizing compiler and i will cover it also",
    "start": "1578480",
    "end": "1583520"
  },
  {
    "text": "in the following um and then the tau stack itself comes and",
    "start": "1583520",
    "end": "1591120"
  },
  {
    "text": "what does it include so it includes data preparation and augmentation so",
    "start": "1591120",
    "end": "1597200"
  },
  {
    "text": "the technique i mentioned so first of all it includes transfer learning then you can use data augmentation",
    "start": "1597200",
    "end": "1604720"
  },
  {
    "text": "for that of course training and we also included pruning based on",
    "start": "1604720",
    "end": "1610799"
  },
  {
    "text": "our appropriate pruning algorithm from nvidia research and you can also run quantization aware",
    "start": "1610799",
    "end": "1617120"
  },
  {
    "text": "training for example you print your model and you do the retraining so you can do this together with quantization",
    "start": "1617120",
    "end": "1623120"
  },
  {
    "text": "aware training in the outcome you have your deployable model and a calibration",
    "start": "1623120",
    "end": "1628240"
  },
  {
    "text": "file which you can then use further to take your model on the target hardware",
    "start": "1628240",
    "end": "1633600"
  },
  {
    "text": "and optimize it further with tensority for example",
    "start": "1633600",
    "end": "1638720"
  },
  {
    "text": "it's like tao it's been called tower since uh a couple of months i would say so before",
    "start": "1638960",
    "end": "1645039"
  },
  {
    "text": "that you may have heard of a tool called transfer learning toolkit so it's the",
    "start": "1645039",
    "end": "1650320"
  },
  {
    "text": "previous name of tower toolkit and we renamed it and",
    "start": "1650320",
    "end": "1655840"
  },
  {
    "text": "we are also working on developing a tao ui it's not there yet but it's coming and",
    "start": "1655840",
    "end": "1662880"
  },
  {
    "text": "hopefully next year we'll have tau ui as white as well",
    "start": "1662880",
    "end": "1668720"
  },
  {
    "text": "tower toolkit works with vision ai applications but we recently also extended it to conversational ai so we",
    "start": "1668720",
    "end": "1676320"
  },
  {
    "text": "have a set of conversational models also supported by tao um and also many people uh struggle like",
    "start": "1676320",
    "end": "1685679"
  },
  {
    "text": "i experienced with my customers uh when training um models using multi-gpu setups",
    "start": "1685679",
    "end": "1692640"
  },
  {
    "text": "and in tao toolkit it is as easy as just setting a parameter in the command line",
    "start": "1692640",
    "end": "1698159"
  },
  {
    "text": "so use multi-gpu and tower toolkit will automatically pick your all of your gpus",
    "start": "1698159",
    "end": "1704480"
  },
  {
    "text": "you want to use uh for your training it also supports automated mix precision",
    "start": "1704480",
    "end": "1710480"
  },
  {
    "text": "so you don't have to write any code here just also set a parameter use amp true",
    "start": "1710480",
    "end": "1716159"
  },
  {
    "text": "and it will also then you also benefit from from automated mix precision",
    "start": "1716159",
    "end": "1723399"
  },
  {
    "text": "so this is a little bit more about our pre-trained models and it's necessary to",
    "start": "1724960",
    "end": "1731840"
  },
  {
    "start": "1725000",
    "end": "1802000"
  },
  {
    "text": "say that all these pre-trained models they are performance tuned for our",
    "start": "1731840",
    "end": "1736880"
  },
  {
    "text": "hardware so we usually take some state-of-the-art algorithms and modify them well",
    "start": "1736880",
    "end": "1744080"
  },
  {
    "text": "we keep the idea the mathematical the mathematics behind but we try to",
    "start": "1744080",
    "end": "1749520"
  },
  {
    "text": "implement it as most efficient way for our gpus and the",
    "start": "1749520",
    "end": "1755760"
  },
  {
    "text": "models in our portfolio which we offer they include some models for people detection for gaze estimation for body",
    "start": "1755760",
    "end": "1763120"
  },
  {
    "text": "pose estimation there are a couple of models for autonomous automotive use",
    "start": "1763120",
    "end": "1768159"
  },
  {
    "text": "cases like car detection car classification license plates recogni",
    "start": "1768159",
    "end": "1773360"
  },
  {
    "text": "license plate recognition and we also provide on the github we provide our open source applications",
    "start": "1773360",
    "end": "1780399"
  },
  {
    "text": "which show you how to deploy these models in the tools like deep stream which i also mentioned later",
    "start": "1780399",
    "end": "1787200"
  },
  {
    "text": "they're usually very accurate like on average they're trained for over 80",
    "start": "1787200",
    "end": "1793039"
  },
  {
    "text": "accuracy and as i already also mentioned you can take these models and adapt them on your",
    "start": "1793039",
    "end": "1798640"
  },
  {
    "text": "own data set uh but what's more in addition to these",
    "start": "1798640",
    "end": "1805360"
  },
  {
    "start": "1802000",
    "end": "1848000"
  },
  {
    "text": "purpose-built models uh you're super flexible in terms of taking all these",
    "start": "1805360",
    "end": "1810399"
  },
  {
    "text": "combinations of popular backbones and state-of-the-art networks",
    "start": "1810399",
    "end": "1815679"
  },
  {
    "text": "and you can also take them and train them on your own data the models which we offer",
    "start": "1815679",
    "end": "1821840"
  },
  {
    "text": "in our tile toolkit in in the ngc uh they've been trained on public data",
    "start": "1821840",
    "end": "1827120"
  },
  {
    "text": "sets and uh the most of them were trained on open image data set which is also publicly available",
    "start": "1827120",
    "end": "1834080"
  },
  {
    "text": "and you can be really creative here so you can take image segmentation image",
    "start": "1834080",
    "end": "1839360"
  },
  {
    "text": "detection and image classification and take this back bones and train your models",
    "start": "1839360",
    "end": "1846640"
  },
  {
    "text": "and you may wonder what is the accuracy of these models and we are actually aiming at keeping the accuracy at the",
    "start": "1847039",
    "end": "1853440"
  },
  {
    "start": "1848000",
    "end": "1870000"
  },
  {
    "text": "state-of-the-art level and as you see on this diagram uh like for",
    "start": "1853440",
    "end": "1858559"
  },
  {
    "text": "most of the models on public data sets we we are on par and the state-of-the-art",
    "start": "1858559",
    "end": "1864159"
  },
  {
    "text": "accuracy or even exceeding it",
    "start": "1864159",
    "end": "1868240"
  },
  {
    "start": "1870000",
    "end": "1968000"
  },
  {
    "text": "and the next the next tool i want to introduce is nvidia 10rt",
    "start": "1870880",
    "end": "1876320"
  },
  {
    "text": "and nvidia 1030 is basically two things uh first one is the optimizing compiler",
    "start": "1876320",
    "end": "1882880"
  },
  {
    "text": "uh it's the tool like if i remember the techniques i said you need to know uh cuda to implement them",
    "start": "1882880",
    "end": "1890399"
  },
  {
    "text": "so all of these techniques including lay intensive fusion uh",
    "start": "1890399",
    "end": "1896159"
  },
  {
    "text": "weight calibration kernel auto tuning dynamic tensor memory and multi-stream",
    "start": "1896159",
    "end": "1901360"
  },
  {
    "text": "execution they are all implemented in this tensor to optimizing compiler so",
    "start": "1901360",
    "end": "1907840"
  },
  {
    "text": "for you it is easy as just pressing the button and saying optimize and",
    "start": "1907840",
    "end": "1913760"
  },
  {
    "text": "you will come from your own model into the same model with this absolutely the same accuracy",
    "start": "1913760",
    "end": "1920399"
  },
  {
    "text": "but which is much faster in this throughput so uh like in my experience it can be up",
    "start": "1920399",
    "end": "1927360"
  },
  {
    "text": "to ten times uh performance increase it's a tendency is super fantastic",
    "start": "1927360",
    "end": "1932480"
  },
  {
    "text": "it's actually my favorite uh tool from nvidia if you ask me",
    "start": "1932480",
    "end": "1937519"
  },
  {
    "text": "uh and the second part of tensorrt is tensority runtime it's something which",
    "start": "1937519",
    "end": "1943440"
  },
  {
    "text": "you well basically incorporate in your software product when you want to deploy",
    "start": "1943440",
    "end": "1948480"
  },
  {
    "text": "your model and basically the runtime takes a model it deserializes it and",
    "start": "1948480",
    "end": "1954240"
  },
  {
    "text": "runs inference for you you can have python integration we have python api",
    "start": "1954240",
    "end": "1959760"
  },
  {
    "text": "for tensor t and of course c plus so there are two ways to deploy tensor t",
    "start": "1959760",
    "end": "1965760"
  },
  {
    "text": "optimized models and basically the workflow of working with tensor t is as follows",
    "start": "1965760",
    "end": "1971600"
  },
  {
    "start": "1968000",
    "end": "2036000"
  },
  {
    "text": "you take a model which you trained and put it on your target device if it's an",
    "start": "1971600",
    "end": "1977039"
  },
  {
    "text": "x86 gpu you need to do it on x86 gpu and if it's",
    "start": "1977039",
    "end": "1983200"
  },
  {
    "text": "if you're deploying on arm based jetson device you need to put it on the jetson device",
    "start": "1983200",
    "end": "1988720"
  },
  {
    "text": "good news here you don't have to do it for every device but you need to do it at least once for each product family",
    "start": "1988720",
    "end": "1995679"
  },
  {
    "text": "and the process is you take your model run it through the optimizer and in the output you will have a plan",
    "start": "1995679",
    "end": "2002559"
  },
  {
    "text": "file and by saying plan i mean you have weights the original weights from your model but",
    "start": "2002559",
    "end": "2009679"
  },
  {
    "text": "also the instructions how to run this model on the given hardware and the second step is deployment so",
    "start": "2009679",
    "end": "2017679"
  },
  {
    "text": "here is the place where you take a model the optimize plan file you put it in",
    "start": "2017679",
    "end": "2022880"
  },
  {
    "text": "your application and the tensority runtime will take your model will",
    "start": "2022880",
    "end": "2028000"
  },
  {
    "text": "deserialize this plan file and perform the inference",
    "start": "2028000",
    "end": "2034640"
  },
  {
    "start": "2036000",
    "end": "2232000"
  },
  {
    "text": "the second tool also used for deployment is try it an inference server",
    "start": "2036080",
    "end": "2042240"
  },
  {
    "text": "and try to inference server also can work in well two major scenarios so",
    "start": "2042240",
    "end": "2048079"
  },
  {
    "text": "first one it's actually a server so uh if you well if you deployed something",
    "start": "2048079",
    "end": "2054398"
  },
  {
    "text": "with applications like flask it's probably the closest analogy so you have your model deployed remotely",
    "start": "2054399",
    "end": "2062158"
  },
  {
    "text": "and you have your model repository which can be also remote and then you have your clients uh which",
    "start": "2062159",
    "end": "2068638"
  },
  {
    "text": "do not necessarily have to be on the same hardware so you can have your mobile clients or",
    "start": "2068639",
    "end": "2074480"
  },
  {
    "text": "users from laptops from all parts of the words from the world which will access your",
    "start": "2074480",
    "end": "2080000"
  },
  {
    "text": "servers through http or grpc endpoint and the server will then take you well let's",
    "start": "2080000",
    "end": "2087440"
  },
  {
    "text": "say image or text processing task it will do the inference on the server",
    "start": "2087440",
    "end": "2094560"
  },
  {
    "text": "side on the server hardware and it will send you the response from the server",
    "start": "2094560",
    "end": "2100320"
  },
  {
    "text": "it includes also load balancing and tricks like concurrent model execution",
    "start": "2100320",
    "end": "2107119"
  },
  {
    "text": "or dynamic batching concurrent model execution is super cool it means that you can use a single gpu",
    "start": "2107119",
    "end": "2113440"
  },
  {
    "text": "to run multiple inferences in parallel which leads to full gpu utilization",
    "start": "2113440",
    "end": "2120240"
  },
  {
    "text": "and dynamic batching means that instead of performing inference many many many",
    "start": "2120240",
    "end": "2125520"
  },
  {
    "text": "times for each of the requests you batch all incoming requests in a one",
    "start": "2125520",
    "end": "2130640"
  },
  {
    "text": "big batch and run inference only one time and i've heard from one of the customers who said oh my god",
    "start": "2130640",
    "end": "2138000"
  },
  {
    "text": "it will help us saving a lot of money when we run this cloud provider they charge chargers per inference so in fact",
    "start": "2138000",
    "end": "2145200"
  },
  {
    "text": "yes so instead of running like 10 times you just create a batch of 10 and",
    "start": "2145200",
    "end": "2150720"
  },
  {
    "text": "pay 18 times 10 times less and important to say that trading inference server is completely open",
    "start": "2150720",
    "end": "2157119"
  },
  {
    "text": "source so we put the whole project in the github and i'm proud to say that i'm also",
    "start": "2157119",
    "end": "2164000"
  },
  {
    "text": "contributing in this project the second scenario of",
    "start": "2164000",
    "end": "2169680"
  },
  {
    "text": "using trident is actually you compile it as a shared library",
    "start": "2169680",
    "end": "2175200"
  },
  {
    "text": "and place it in your application as a basically a triton runtime",
    "start": "2175200",
    "end": "2181119"
  },
  {
    "text": "and well in this scenario it works similar way as tensority runtime",
    "start": "2181119",
    "end": "2187599"
  },
  {
    "text": "but in addition to 1030 models so yeah in triton you can also deploy 1030 models as well",
    "start": "2187599",
    "end": "2194160"
  },
  {
    "text": "you can also deploy any kinds of models so you can deploy tensorflow models pytorch models uh",
    "start": "2194160",
    "end": "2201119"
  },
  {
    "text": "well you can deploy onyx rapids which is our own framework uh tanzanite i",
    "start": "2201119",
    "end": "2206800"
  },
  {
    "text": "mentioned you can implement your custom backends so you can even deploy your own custom",
    "start": "2206800",
    "end": "2212960"
  },
  {
    "text": "models and finally you can also deploy openvino models yes triton works on cpus",
    "start": "2212960",
    "end": "2219280"
  },
  {
    "text": "so with trident we support gpus and cpus both so with this solution you're super",
    "start": "2219280",
    "end": "2226640"
  },
  {
    "text": "flexible and the next uh tool",
    "start": "2226640",
    "end": "2233359"
  },
  {
    "start": "2232000",
    "end": "2337000"
  },
  {
    "text": "which is our powerful solution for video analytics is dipstream sdk",
    "start": "2233359",
    "end": "2239680"
  },
  {
    "text": "and for dipstim sdk you can run it on all kinds of hardware",
    "start": "2239680",
    "end": "2246000"
  },
  {
    "text": "in terms of gpus yes and deep stream encapsulates",
    "start": "2246000",
    "end": "2251760"
  },
  {
    "text": "both triton and tensor t you're flexible to choose between deploying with triton",
    "start": "2251760",
    "end": "2257280"
  },
  {
    "text": "or tensor t in deep stream and also important to say that deep stream is based on the open source",
    "start": "2257280",
    "end": "2264240"
  },
  {
    "text": "project called g streamer so if not necessarily you work on ai but you",
    "start": "2264240",
    "end": "2270640"
  },
  {
    "text": "worked with video analytics you may have heard about gstreamer and gstreamer consists of a collection of different",
    "start": "2270640",
    "end": "2277440"
  },
  {
    "text": "plugins for encoding decoding so we took the gstreamer and we augmented it with",
    "start": "2277440",
    "end": "2283440"
  },
  {
    "text": "our own plugins our own hardware optimized plugins which include hardware based decoding",
    "start": "2283440",
    "end": "2290320"
  },
  {
    "text": "and encoding multimedia libraries the inference plugins and also some",
    "start": "2290320",
    "end": "2296160"
  },
  {
    "text": "analytics as well and on top of that so yeah it runs",
    "start": "2296160",
    "end": "2302160"
  },
  {
    "text": "containerized on our embedded and data center devices",
    "start": "2302160",
    "end": "2308800"
  },
  {
    "text": "there are three ways to develop for deep stream you can develop natively in c or",
    "start": "2308800",
    "end": "2313839"
  },
  {
    "text": "c plus plus since recently we also introduced python wrappers for that",
    "start": "2313839",
    "end": "2319680"
  },
  {
    "text": "and since very recently like the last iteration of deep stream six",
    "start": "2319680",
    "end": "2324960"
  },
  {
    "text": "we also included graph composer which is a ui tool and you basically can",
    "start": "2324960",
    "end": "2330640"
  },
  {
    "text": "drag and drop from building blocks and create your pipelines for that",
    "start": "2330640",
    "end": "2335920"
  },
  {
    "text": "on this slide i'm showing a typical video analytics pipeline which starts uh",
    "start": "2335920",
    "end": "2342000"
  },
  {
    "start": "2337000",
    "end": "2483000"
  },
  {
    "text": "so here you can see like the from the camera and we also support audio processing so",
    "start": "2342000",
    "end": "2348800"
  },
  {
    "text": "in addition to video processing we also support audio processing and the first step to do is to",
    "start": "2348800",
    "end": "2355680"
  },
  {
    "text": "capture and decode and this is usually happening on your uh well device so we have nvdac which is",
    "start": "2355680",
    "end": "2363040"
  },
  {
    "text": "a special part in your gpu which does uh the next step is uh image",
    "start": "2363040",
    "end": "2369760"
  },
  {
    "text": "preprocessing and you do here what you do here you can do scaling cropping",
    "start": "2369760",
    "end": "2375440"
  },
  {
    "text": "sometimes you need to do warping of your image and i'll process all possible",
    "start": "2375440",
    "end": "2381280"
  },
  {
    "text": "operations which do not require deep learning the next one is batching so actually",
    "start": "2381280",
    "end": "2387359"
  },
  {
    "text": "with deep stream you can process multiple streams in parallel and",
    "start": "2387359",
    "end": "2393520"
  },
  {
    "text": "you can also do batching of the same stream in order to make it also more efficient and uh",
    "start": "2393520",
    "end": "2399680"
  },
  {
    "text": "to increase the throughput of your pipeline and the next step is inference so in the",
    "start": "2399680",
    "end": "2405200"
  },
  {
    "text": "interference you do either tenzarity or triton inference and this is usually running on",
    "start": "2405200",
    "end": "2411520"
  },
  {
    "text": "gpu or if you're deploying a jetson you can also use deep learning accelerator which",
    "start": "2411520",
    "end": "2417520"
  },
  {
    "text": "is an extra accelerator in addition to gpu which we have on our jetson device",
    "start": "2417520",
    "end": "2423200"
  },
  {
    "text": "then we have a couple of trackers available which means that you can keep your object in the scene and you can",
    "start": "2423200",
    "end": "2429200"
  },
  {
    "text": "track your object assigning a unique id to the object and until recently we had like three",
    "start": "2429200",
    "end": "2435040"
  },
  {
    "text": "very simplistic trackers which are klt intersectional union and another",
    "start": "2435040",
    "end": "2440160"
  },
  {
    "text": "proprietary tracker since the last iteration we included a deep sort tracker which is a deep",
    "start": "2440160",
    "end": "2446480"
  },
  {
    "text": "learning based tracker and it's supposed to be more robust for example for object trading re-identification in the scene",
    "start": "2446480",
    "end": "2455440"
  },
  {
    "text": "and the next step is well you can apply analytics you can apply a visualization of your",
    "start": "2458960",
    "end": "2465280"
  },
  {
    "text": "object so for example you can pick on-screen display and you can visualize the bounding boxes around the",
    "start": "2465280",
    "end": "2472560"
  },
  {
    "text": "objects you can write classification labels and you can also",
    "start": "2472560",
    "end": "2477920"
  },
  {
    "text": "set up rtsp output of your data and in this slide i'm showing the",
    "start": "2477920",
    "end": "2485200"
  },
  {
    "start": "2483000",
    "end": "2539000"
  },
  {
    "text": "the secret sauce behind dipstream what makes it actually so efficient and it's called a zero zero memory copy",
    "start": "2485200",
    "end": "2494000"
  },
  {
    "text": "pipeline and typically the major bottleneck in video",
    "start": "2494000",
    "end": "2499680"
  },
  {
    "text": "processing is putting the data from the cpu to the gpu and deep stream",
    "start": "2499680",
    "end": "2506319"
  },
  {
    "text": "enables doing this step only once so you record your data with your camera",
    "start": "2506319",
    "end": "2511920"
  },
  {
    "text": "the data enters the cpu and then you do decoding which puts your data in the gpu",
    "start": "2511920",
    "end": "2517520"
  },
  {
    "text": "and until the end of the pipeline the whole operations are performed in the gpu",
    "start": "2517520",
    "end": "2523839"
  },
  {
    "text": "by avoiding moving the data back and forth you actually achieve the highest",
    "start": "2523839",
    "end": "2529040"
  },
  {
    "text": "speed up possible and this is yeah this is the main",
    "start": "2529040",
    "end": "2535119"
  },
  {
    "text": "this is what makes deep stream so efficient and this is basically the visualization",
    "start": "2535119",
    "end": "2540960"
  },
  {
    "start": "2539000",
    "end": "2552000"
  },
  {
    "text": "of graph composer how it looks like so these are the building blocks and we have like over 150 building blocks which",
    "start": "2540960",
    "end": "2547680"
  },
  {
    "text": "you can drag and drop and connect in this pipeline and a couple of demos just",
    "start": "2547680",
    "end": "2553920"
  },
  {
    "start": "2552000",
    "end": "2634000"
  },
  {
    "text": "so that you see how it looks like so this is the peoplenet model",
    "start": "2553920",
    "end": "2558960"
  },
  {
    "text": "pulled directly from the ngc and inferred with deep stream so it works",
    "start": "2558960",
    "end": "2565359"
  },
  {
    "text": "real time and of course this is the rendering but it still works real time",
    "start": "2565359",
    "end": "2571200"
  },
  {
    "text": "believe me this demo shows you that you can actually stack multiple models so you",
    "start": "2571200",
    "end": "2577280"
  },
  {
    "text": "can have a model for car detection and then this you can stack it with this",
    "start": "2577280",
    "end": "2582480"
  },
  {
    "text": "this is the second model for car classification which will say it's a van or track or sedan",
    "start": "2582480",
    "end": "2589280"
  },
  {
    "text": "uh and you can also see that there are some object ids here which are kept persistent in the scene",
    "start": "2589280",
    "end": "2596640"
  },
  {
    "text": "and that's actually me in my home office and i developed and open sourced this demo a",
    "start": "2596640",
    "end": "2602880"
  },
  {
    "text": "couple of months ago and for this demo i took a people detection model and i retrained it uh to",
    "start": "2602880",
    "end": "2609520"
  },
  {
    "text": "detect hand and for that i used some public data set",
    "start": "2609520",
    "end": "2615040"
  },
  {
    "text": "and the second model i used is a mod is a gesture classification model which i",
    "start": "2615040",
    "end": "2620319"
  },
  {
    "text": "took just out of the box from the ngc and i cascaded it with the first hand",
    "start": "2620319",
    "end": "2625440"
  },
  {
    "text": "detector using deep stream and you can see that it also works real time and",
    "start": "2625440",
    "end": "2630880"
  },
  {
    "text": "it's quite robust at last well the summary is that uh",
    "start": "2630880",
    "end": "2637520"
  },
  {
    "start": "2634000",
    "end": "2718000"
  },
  {
    "text": "so basically when deploying and developing efficient video analytic applications you need to consider three",
    "start": "2637520",
    "end": "2644079"
  },
  {
    "text": "things first of all when training do it in the most efficient way utilize your",
    "start": "2644079",
    "end": "2650000"
  },
  {
    "text": "gpu as much as possible if you have multi gpus use all of them for your training if your gpu supports mixed",
    "start": "2650000",
    "end": "2657359"
  },
  {
    "text": "precision training use it use automated misprecision it will save you much a lot of time",
    "start": "2657359",
    "end": "2663440"
  },
  {
    "text": "and consider doing things like pruning quantization aware training even if you are not using tools like total kit",
    "start": "2663440",
    "end": "2670400"
  },
  {
    "text": "you can still use your favorite frameworks and apply these techniques",
    "start": "2670400",
    "end": "2676160"
  },
  {
    "text": "then you should consider doing optimization and for that we provide our great tools like tensorrt and this costs",
    "start": "2676160",
    "end": "2682960"
  },
  {
    "text": "you nothing to do that and it's basically by clicking this button optimize",
    "start": "2682960",
    "end": "2688880"
  },
  {
    "text": "you will get a huge performance boost and finally for deployment uh well consider uh well",
    "start": "2688880",
    "end": "2696880"
  },
  {
    "text": "having zero memory copies and uh well to make it efficient for you",
    "start": "2696880",
    "end": "2702880"
  },
  {
    "text": "we provide our deep stream sdk which is yeah all the frameworks are free for",
    "start": "2702880",
    "end": "2708400"
  },
  {
    "text": "commercial use so i'm not trying to sell you anything and",
    "start": "2708400",
    "end": "2714400"
  },
  {
    "text": "at last but not least i'm just sharing some resources where you can find it and we'll be sharing the",
    "start": "2714400",
    "end": "2720560"
  },
  {
    "start": "2718000",
    "end": "2769000"
  },
  {
    "text": "slides with you uh if you have any questions or troubles with our products please post on our",
    "start": "2720560",
    "end": "2727040"
  },
  {
    "text": "developer forums uh they have been constantly monitored by our engineers",
    "start": "2727040",
    "end": "2732720"
  },
  {
    "text": "and uh well they make sure to provide you feedback as fast as possible uh we also",
    "start": "2732720",
    "end": "2739599"
  },
  {
    "text": "have our nvidia deep learning institute is there like uh you can take like a",
    "start": "2739599",
    "end": "2744880"
  },
  {
    "text": "daily workshops and take courses there for any particular",
    "start": "2744880",
    "end": "2750079"
  },
  {
    "text": "topics in deep learning and actually our folks from the deep learning institute they told me that",
    "start": "2750079",
    "end": "2756240"
  },
  {
    "text": "well they noticed that i'm presenting here at this event and they actually said that there will be the book which",
    "start": "2756240",
    "end": "2762319"
  },
  {
    "text": "they recently published sold in the stores at this event so if you want to learn deep learning check out the",
    "start": "2762319",
    "end": "2768480"
  },
  {
    "text": "bookstore with that said thank you very much for coming to my talk and don't forget for",
    "start": "2768480",
    "end": "2774400"
  },
  {
    "start": "2769000",
    "end": "2792000"
  },
  {
    "text": "to vote for this session [Applause]",
    "start": "2774400",
    "end": "2782230"
  },
  {
    "text": "you",
    "start": "2790800",
    "end": "2792880"
  }
]