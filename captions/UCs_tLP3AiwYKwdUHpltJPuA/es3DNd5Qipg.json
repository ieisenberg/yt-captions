[
  {
    "start": "0",
    "end": "128000"
  },
  {
    "text": "thank you",
    "start": "6180",
    "end": "8839"
  },
  {
    "text": "so I am Jess humble I am a site reliability engineer working at Google Cloud",
    "start": "11760",
    "end": "17820"
  },
  {
    "text": "um who knows about who's heard of site reliability engineering before okay quite a lot of you not everyone so",
    "start": "17820",
    "end": "24300"
  },
  {
    "text": "site reliability engineering we don't do traditional Ops at Google what we do is we get engineers and have them work in",
    "start": "24300",
    "end": "30960"
  },
  {
    "text": "Ops and the idea is rather than operations being kind of like clicking on stuff and manual work we work to",
    "start": "30960",
    "end": "36840"
  },
  {
    "text": "automate things and make sure that as the platform scales we don't need to scale the size of the team that manages",
    "start": "36840",
    "end": "42899"
  },
  {
    "text": "and operates that platform so we apply software engineering solutions to the problem of running and maintaining",
    "start": "42899",
    "end": "49079"
  },
  {
    "text": "Platforms in production I am a tech lead on the serverless SRE",
    "start": "49079",
    "end": "54539"
  },
  {
    "text": "team where ironically I manage a very large number of servers um the serverless platform is Google app",
    "start": "54539",
    "end": "61920"
  },
  {
    "text": "engine Cloud run Cloud functions so Google app engine was a cloud service before Google Cloud existed so it was",
    "start": "61920",
    "end": "68820"
  },
  {
    "text": "you know the predated Google Cloud uh it's still running still a great platform",
    "start": "68820",
    "end": "74520"
  },
  {
    "text": "um and uh have a lot of fun working on it so I'm going to start by saying that what I'm presenting today is the work of",
    "start": "74520",
    "end": "81060"
  },
  {
    "text": "a lot of people you can see here these are all my colleagues on the serverless SRE team which is in San Francisco and",
    "start": "81060",
    "end": "87600"
  },
  {
    "text": "London um so we are all on call for the platform we have Encore rotations uh I'm",
    "start": "87600",
    "end": "93659"
  },
  {
    "text": "on the San Francisco side uh we are on call from 10 a.m to 10 p.m and then I get the pager gets handed over to the",
    "start": "93659",
    "end": "99540"
  },
  {
    "text": "London team uh 6 a.m London time to 6 p.m London time and then the pager gets handed back to San Francisco so these",
    "start": "99540",
    "end": "105360"
  },
  {
    "text": "are all the people that are on call the work that I'm talking about today the ideas were invented by the Kraken team",
    "start": "105360",
    "end": "112399"
  },
  {
    "text": "led by Narayan Desai and a whole bunch of other people from",
    "start": "112399",
    "end": "117659"
  },
  {
    "text": "inside and outside Google contributed to it so all that to say I work with a lot of really great smart people who I'm",
    "start": "117659",
    "end": "122759"
  },
  {
    "text": "proud to work with and what I'm presenting today is the work of a lot of people not me so the serverless platform is really",
    "start": "122759",
    "end": "130080"
  },
  {
    "start": "128000",
    "end": "356000"
  },
  {
    "text": "cool you can deploy containers you can deploy codes from the command line and",
    "start": "130080",
    "end": "135120"
  },
  {
    "text": "we take care of scaling and infrastructure for you so you don't have to it scales down to zero and up to",
    "start": "135120",
    "end": "141060"
  },
  {
    "text": "thousands of instances in seconds which is really cool but basically what we're doing with that is we're giving our",
    "start": "141060",
    "end": "146580"
  },
  {
    "text": "customers the ability to apply severe load to our platform so that can be",
    "start": "146580",
    "end": "151920"
  },
  {
    "text": "quite painful we have some very big customers and some of those customers do things that involve",
    "start": "151920",
    "end": "159660"
  },
  {
    "text": "a sudden arrival of an extremely large amount of QPS on our platform so for example major sporting events you could",
    "start": "159660",
    "end": "166800"
  },
  {
    "text": "have millions of people tuning in to watch a match at the same time and that's going to instantaneously spike up",
    "start": "166800",
    "end": "172440"
  },
  {
    "text": "QPS to very high levels in our platform so we definitely feel that we can see that happening we can feel it it's a",
    "start": "172440",
    "end": "178319"
  },
  {
    "text": "shared platform it's an amazing platform but you know that has an effect on it",
    "start": "178319",
    "end": "183440"
  },
  {
    "text": "despite that it works really well but you know not to say there aren't issues",
    "start": "183440",
    "end": "188879"
  },
  {
    "text": "so I'm just going to briefly describe how the platform works at a very high level so when you deploy an app it's the",
    "start": "188879",
    "end": "196560"
  },
  {
    "text": "control plane that takes care of taking that app and getting it running on the",
    "start": "196560",
    "end": "201780"
  },
  {
    "text": "data plane and managing the life cycle of that app so every time you deploy an app that",
    "start": "201780",
    "end": "206879"
  },
  {
    "text": "creates a new version and you can split traffic between the current version and old versions and migrate traffic and so forth and it's",
    "start": "206879",
    "end": "213239"
  },
  {
    "text": "the data plane that does the serving so we have multiple regions those regions are isolated from each other each region",
    "start": "213239",
    "end": "219599"
  },
  {
    "text": "has a number of cells which are independent data centers that your app will run on within that region",
    "start": "219599",
    "end": "226200"
  },
  {
    "text": "and then when a user sends a request to your app that goes firstly through a bunch of networking stuff near the user",
    "start": "226200",
    "end": "232500"
  },
  {
    "text": "which could be in a completely different part of the world from the app which is serving that request so there's a networking layer here which works out",
    "start": "232500",
    "end": "238860"
  },
  {
    "text": "where that request is going to be served forwards it to another networking component in the same region that the",
    "start": "238860",
    "end": "246239"
  },
  {
    "text": "app is going to serve your request from and then it sends your requests to an",
    "start": "246239",
    "end": "251879"
  },
  {
    "text": "instance of your app which is running on an app server an app server is a job that runs on Borg our compute",
    "start": "251879",
    "end": "257280"
  },
  {
    "text": "infrastructure which hosts you know large numbers of instances and there's large numbers of app servers running in",
    "start": "257280",
    "end": "263280"
  },
  {
    "text": "every cell this is obviously not to scale there's you know a huge amount of stuff going on here but at a high level",
    "start": "263280",
    "end": "268919"
  },
  {
    "text": "this is kind of what it looks like this networking stuff near your app",
    "start": "268919",
    "end": "274860"
  },
  {
    "text": "um in fact all of this stuff but in particular we care about this networking component in the region",
    "start": "274860",
    "end": "279900"
  },
  {
    "text": "um emits a large amount of metadata a large number of metrics so one of the things that we're going to be talking",
    "start": "279900",
    "end": "285840"
  },
  {
    "text": "about later we capture time stamps for every request as it goes through the",
    "start": "285840",
    "end": "291540"
  },
  {
    "text": "stack so we capture timestamp for when your requests hits this part of the stack we capture the timestamp of when",
    "start": "291540",
    "end": "297900"
  },
  {
    "text": "we start negotiating to work out which app server is going to serve your request we capture the timestamp of when",
    "start": "297900",
    "end": "303180"
  },
  {
    "text": "your request gets forwarded to the app server when you're when your code starts executing when your code finishes",
    "start": "303180",
    "end": "309000"
  },
  {
    "text": "executing when it sends the request back to the networking layer and when it exits the networking layer to send the",
    "start": "309000",
    "end": "314520"
  },
  {
    "text": "request back to you so that metadata and a whole bunch of other metadata firstly",
    "start": "314520",
    "end": "319680"
  },
  {
    "text": "goes into logs so we have amazing structured logs at Google it's not just a timestamp and a",
    "start": "319680",
    "end": "325800"
  },
  {
    "text": "string it's a timestamp and a ton of protos that are stuck into that log entry and it all goes into a big data",
    "start": "325800",
    "end": "331440"
  },
  {
    "text": "warehouse and you can query it with SQL it's pretty amazing and that metadata also goes to our",
    "start": "331440",
    "end": "340259"
  },
  {
    "text": "in-memory Global time series data store called Monarch so Monarch stores this",
    "start": "340259",
    "end": "346680"
  },
  {
    "text": "data as well and Monarch is where we get all the data we use for our monitoring observability platform when we're",
    "start": "346680",
    "end": "352500"
  },
  {
    "text": "running stuff in prod so who here Works in platform",
    "start": "352500",
    "end": "357960"
  },
  {
    "start": "356000",
    "end": "494000"
  },
  {
    "text": "engineering he's run kubernetes or something like that okay have you ever had this experience",
    "start": "357960",
    "end": "363720"
  },
  {
    "text": "where one of your customers is like hey my app's low anyone had that and how do you feel",
    "start": "363720",
    "end": "370620"
  },
  {
    "text": "about that when that happens so that's about 25 of the audience like so when that happens I'm like oh my God because",
    "start": "370620",
    "end": "377039"
  },
  {
    "text": "it's really hard to work out what's going on one possibility is that your customer",
    "start": "377039",
    "end": "382080"
  },
  {
    "text": "changed some codes and uh caused the problem and that's the cause of the problem",
    "start": "382080",
    "end": "387300"
  },
  {
    "text": "they've changed their code they changed some config something like that but it could be a ton of other things",
    "start": "387300",
    "end": "392580"
  },
  {
    "text": "um any reasonable app has a whole bunch of dependencies and those dependencies might be running slowly for some reason",
    "start": "392580",
    "end": "398520"
  },
  {
    "text": "there could be some problem with that you might call out to a database you might call out to some other service and",
    "start": "398520",
    "end": "404759"
  },
  {
    "text": "that could be behaving differently from usual there could be a change in the pattern of the traffic to an app an apps",
    "start": "404759",
    "end": "411000"
  },
  {
    "text": "often behave non-linearly if you go above a certain number of QPS you might run out of memory on your machine and",
    "start": "411000",
    "end": "416220"
  },
  {
    "text": "then it might just grind to a hole so there's kind of non-linear effects with traffic patterns to your app or to the",
    "start": "416220",
    "end": "422220"
  },
  {
    "text": "platform also some networking layer somewhere we make changes to the platform all the time we're always deploying new changes to the platform",
    "start": "422220",
    "end": "429360"
  },
  {
    "text": "that could be a conflict change somewhere in Google that affects one of our dependencies in some way that could",
    "start": "429360",
    "end": "434940"
  },
  {
    "text": "change something um sometimes although it's a very robust platform if there's a huge number of QPS",
    "start": "434940",
    "end": "440280"
  },
  {
    "text": "that comes into a particular app that might cause problems for other app servers running for other instances",
    "start": "440280",
    "end": "445500"
  },
  {
    "text": "running on the same app server or there might be other contention of resources somewhere else in the stack and that can",
    "start": "445500",
    "end": "450900"
  },
  {
    "text": "cause problems denial of service attacks we get those we have very robust infrastructure in place to deal with",
    "start": "450900",
    "end": "456660"
  },
  {
    "text": "denial of service attacks but it definitely can have Noisy Neighbor issues sometimes if a particular clone",
    "start": "456660",
    "end": "462060"
  },
  {
    "text": "gets a big hit of QPS that can consume a lot of resources on that app server and",
    "start": "462060",
    "end": "467580"
  },
  {
    "text": "the bin packing of clones into app servers can become sub-optimal and we might need some new stuff around so",
    "start": "467580",
    "end": "473340"
  },
  {
    "text": "there's like a ton of stuff that can happen and trying to work out which of those things is causing your app to be slow is",
    "start": "473340",
    "end": "480240"
  },
  {
    "text": "not a trivial task it's going to take a lot of debugging and understanding uh",
    "start": "480240",
    "end": "485400"
  },
  {
    "text": "unless it's something obvious and SRE are basically the people who get called when it's not something obvious",
    "start": "485400",
    "end": "490500"
  },
  {
    "text": "so you know you're in for a long night so you know if something's really badly",
    "start": "490500",
    "end": "496860"
  },
  {
    "start": "494000",
    "end": "569000"
  },
  {
    "text": "wrong you know people obviously will say well is it us or is it is it is it Google and",
    "start": "496860",
    "end": "502199"
  },
  {
    "text": "if you get a customer ticket from someone um you know if it's just one ticket you're like well it's probably you know",
    "start": "502199",
    "end": "508620"
  },
  {
    "text": "some configuration change or something that they've they've made to their app but when you start getting tickets from a whole bunch of customers at that point",
    "start": "508620",
    "end": "515159"
  },
  {
    "text": "you start to think well maybe it's not an individual customer problem maybe there's something going on with the platform",
    "start": "515159",
    "end": "520260"
  },
  {
    "text": "um and what you would like to do is find out before customers you don't",
    "start": "520260",
    "end": "527519"
  },
  {
    "text": "want customers telling you there's a problem with your system you want to know there's a problem so that when the customer requests come in you can say",
    "start": "527519",
    "end": "533459"
  },
  {
    "text": "okay we know what's going on here it's this thing or we don't know what's going on here so it's probably not us or at",
    "start": "533459",
    "end": "538860"
  },
  {
    "text": "least we need to do some more investigation to find out if it's us or not so when you get a whole bunch of",
    "start": "538860",
    "end": "545279"
  },
  {
    "text": "customer tickets or Pages you want to know you know what's changed in the behavior",
    "start": "545279",
    "end": "551700"
  },
  {
    "text": "of the system as a whole from a customer perspective so from a customer point of view what's changed",
    "start": "551700",
    "end": "558600"
  },
  {
    "text": "the problem is with you know my app is slow we're looking at latency right and latency on a platform is dominated",
    "start": "558600",
    "end": "566100"
  },
  {
    "text": "by the behavior of the app so I actually went and ran some stats on this so this",
    "start": "566100",
    "end": "573060"
  },
  {
    "start": "569000",
    "end": "654000"
  },
  {
    "text": "is like the end-to-end latency for requests to customer apps and there's you know this is a logarithmic scale so",
    "start": "573060",
    "end": "580560"
  },
  {
    "text": "the biggest bucket of request latencies is in the one second 10 second range here and there's like a whole bunch of",
    "start": "580560",
    "end": "587040"
  },
  {
    "text": "buckets where the latency is much smaller and then there's this Bucket from 10 seconds to 100 seconds and then it drops off but doesn't go to zero so",
    "start": "587040",
    "end": "593820"
  },
  {
    "text": "you can see in the normal operation of the platform we're talking about five orders of magnitude in terms of request",
    "start": "593820",
    "end": "600720"
  },
  {
    "text": "latency for an app and that's you know normal behavior some apps they're doing something that's very computationally",
    "start": "600720",
    "end": "606779"
  },
  {
    "text": "expensive and it takes them several seconds to process your request and that's expected Behavior so",
    "start": "606779",
    "end": "613260"
  },
  {
    "text": "you don't know um I mean this is this is really hard to manage where do you set I mean typically",
    "start": "613260",
    "end": "618660"
  },
  {
    "text": "when you're thinking about the behavior of the platform we use slos a lot service level objectives and service",
    "start": "618660",
    "end": "624420"
  },
  {
    "text": "level objectives for error rates are reasonably straightforward conceptually but where would you set the SLA for",
    "start": "624420",
    "end": "629519"
  },
  {
    "text": "latency will you come there's no like good latency here if my app takes 100",
    "start": "629519",
    "end": "634640"
  },
  {
    "text": "milliseconds to respond and I set the SLO at 900 milliseconds obviously that",
    "start": "634640",
    "end": "640320"
  },
  {
    "text": "that's useless so slos uh are problematic in this context",
    "start": "640320",
    "end": "645480"
  },
  {
    "text": "so what we've traditionally done is we've taken a part of that end to end latency and established an SLA for that",
    "start": "645480",
    "end": "651899"
  },
  {
    "text": "so what we look at the first graph we look at is request delivery latency and that's the latency from when the request",
    "start": "651899",
    "end": "658200"
  },
  {
    "start": "654000",
    "end": "757000"
  },
  {
    "text": "hits the networking stack in the cell that the request will be served from to the time that an app is actually",
    "start": "658200",
    "end": "664800"
  },
  {
    "text": "starting to serve that request and that we can establish a latency slo4 and you",
    "start": "664800",
    "end": "670740"
  },
  {
    "text": "can see this is one of the graphs that we look at and what this is showing is these different colors are different",
    "start": "670740",
    "end": "675899"
  },
  {
    "text": "cells within the region and we can look at the number of projects",
    "start": "675899",
    "end": "680940"
  },
  {
    "text": "for which the latency is above the threshold that we've established as the S alone so we've got a threshold limit",
    "start": "680940",
    "end": "687720"
  },
  {
    "text": "and we can see that there's kind of this noise floor here of about naught to five apps where the request delivery latency",
    "start": "687720",
    "end": "693420"
  },
  {
    "text": "is under the threshold it's a bit over the threshold and then it kind of spikes here and here's where you would expect",
    "start": "693420",
    "end": "699480"
  },
  {
    "text": "for example you know maybe there's a networking components somewhere or a networking job somewhere that's got",
    "start": "699480",
    "end": "704940"
  },
  {
    "text": "overloaded or if an app server somewhere has got overloaded maybe that's what's going on here so it gives you some clue",
    "start": "704940",
    "end": "710459"
  },
  {
    "text": "what the problem is but it's not going to catch issues where there's",
    "start": "710459",
    "end": "715880"
  },
  {
    "text": "an increase in latency in some other phase for example",
    "start": "715880",
    "end": "721079"
  },
  {
    "text": "um calling out to a dependency or during clone execution if there's a problem that won't get caught by this so there's",
    "start": "721079",
    "end": "726120"
  },
  {
    "text": "a lot of problems that can occur where this graph isn't going to move and isn't going to tell you that's a problem",
    "start": "726120",
    "end": "731459"
  },
  {
    "text": "and because it doesn't catch those problems we can't measure the true customer impacts of something going",
    "start": "731459",
    "end": "736980"
  },
  {
    "text": "wrong on the platform and more importantly it doesn't actually represent the customer experience in",
    "start": "736980",
    "end": "742200"
  },
  {
    "text": "terms of the end-to-end latency for when the from running user requests hits the the platform to when it goes back",
    "start": "742200",
    "end": "749880"
  },
  {
    "text": "so this is it's something but it's it's not really good enough",
    "start": "749880",
    "end": "756600"
  },
  {
    "text": "what we actually want is a metric that represents the customer experience we want it to be combinable across",
    "start": "756600",
    "end": "762959"
  },
  {
    "start": "757000",
    "end": "819000"
  },
  {
    "text": "projects across cells and across regions we want to use it to detect anomalies",
    "start": "762959",
    "end": "768480"
  },
  {
    "text": "that potentially can impact multiple customers so things that are going to be",
    "start": "768480",
    "end": "774660"
  },
  {
    "text": "to tell us whether it's a platform issue or just a customer issue and if it's a platform issue is it an issue with you",
    "start": "774660",
    "end": "781440"
  },
  {
    "text": "know which part of the platform it's an issue with and how many people are being impacted by that",
    "start": "781440",
    "end": "787880"
  },
  {
    "text": "we want something that's computationally cheap we get an extremely large number of QPS as you can imagine and we want to",
    "start": "788639",
    "end": "794760"
  },
  {
    "text": "be able to get the data on what the impact is in their real time and we want it to be principle based so",
    "start": "794760",
    "end": "802019"
  },
  {
    "text": "you know that request delivery latency we've chosen a number for that requested delivery latency that's fundamentally",
    "start": "802019",
    "end": "808019"
  },
  {
    "text": "empirically determined it's not something that's principle based and the SLO we choose is empirical as well",
    "start": "808019",
    "end": "813899"
  },
  {
    "text": "so what principles do we care about in terms of the reliability of the platform",
    "start": "813899",
    "end": "819240"
  },
  {
    "start": "819000",
    "end": "1029000"
  },
  {
    "text": "well there's three things fundamentally that we care about firstly is my service available is it there when I need it",
    "start": "819240",
    "end": "826680"
  },
  {
    "text": "secondly how effectively is the work being performed by the platform is it",
    "start": "826680",
    "end": "831800"
  },
  {
    "text": "happening in a reasonable amount of time and then thirdly is the service correct",
    "start": "831800",
    "end": "836940"
  },
  {
    "text": "is it doing is it behaving in the way that customers expect it to behave",
    "start": "836940",
    "end": "842160"
  },
  {
    "text": "and there's standard techniques that we use to measure those things so for availability you might count for example",
    "start": "842160",
    "end": "849839"
  },
  {
    "text": "the error ratio the ratio of failed requests to normal requests and if that goes above a certain amount we might say",
    "start": "849839",
    "end": "855839"
  },
  {
    "text": "that the service is not fully available for performance what we often do is we",
    "start": "855839",
    "end": "861720"
  },
  {
    "text": "have probes so probos are apps that behave in you know the exerciser",
    "start": "861720",
    "end": "867660"
  },
  {
    "text": "relatively simple workflow that's predictable and has a predictable latency and so we monitor for example",
    "start": "867660",
    "end": "873420"
  },
  {
    "text": "the P99 latency of the paper apps and and alert if that goes below a certain",
    "start": "873420",
    "end": "879120"
  },
  {
    "text": "threshold and then for correctness uh we're very big on tests automated testing in Google every time a developer",
    "start": "879120",
    "end": "885839"
  },
  {
    "text": "commits some code they have to commit a test as well to show that that code behaves according to expectations so we",
    "start": "885839",
    "end": "892139"
  },
  {
    "text": "have a ton of automated tests you run an extremely large number of automated tests uh continuously and developers get",
    "start": "892139",
    "end": "899339"
  },
  {
    "text": "fast feedback if those tests are failing so we've got great test coverage and we do Canary analysis when we roll out new",
    "start": "899339",
    "end": "904740"
  },
  {
    "text": "binaries or new versions of binaries to make sure that we're not impacting customer metrics that we care about",
    "start": "904740",
    "end": "910440"
  },
  {
    "text": "but even with all this stuff um it that there's more we can do and there's problems with those existing",
    "start": "910440",
    "end": "915959"
  },
  {
    "text": "things so for example there's an error is it an error that's a user error or is it an error caused by something going",
    "start": "915959",
    "end": "921720"
  },
  {
    "text": "wrong in the platform well we've got a pretty good idea most of the time but there can definitely be places where you",
    "start": "921720",
    "end": "927240"
  },
  {
    "text": "know we've said it's the platform error where it's a customer error or vice versa because you know fundamentally determining what's an error or not is",
    "start": "927240",
    "end": "933600"
  },
  {
    "text": "subjective it's something that we decide there's no platonic ideal of an error it's fundamentally you know part of the",
    "start": "933600",
    "end": "940500"
  },
  {
    "text": "platform designed to determine what's an error and what's not and what's the user error and what's not so there's always",
    "start": "940500",
    "end": "945540"
  },
  {
    "text": "some ambiguity there those deadlines things can time out without giving an error but there can be",
    "start": "945540",
    "end": "950820"
  },
  {
    "text": "a problem sometimes there's malformed requests which cause unexpected behavior and then",
    "start": "950820",
    "end": "955920"
  },
  {
    "text": "people Implement retries and then when you retry in the face of an error that can magnify errors so availability it's",
    "start": "955920",
    "end": "962880"
  },
  {
    "text": "not as simple as as just you know putting an SLO in something and calling it done um we do have an enormous number of slos",
    "start": "962880",
    "end": "969420"
  },
  {
    "text": "there's some there's one service in Google that has like quarter of a million slos and that has its own problem but trying to work out when",
    "start": "969420",
    "end": "975480"
  },
  {
    "text": "those slos are out of bounds and when they are what's actually wrong is a problem in its own right with performance we have these probers",
    "start": "975480",
    "end": "983220"
  },
  {
    "text": "but slos tend to be work workload dependent and often the probes are narrow they're just testing one",
    "start": "983220",
    "end": "989699"
  },
  {
    "text": "particular Journey Through the platform the platform provides an awful lot of functionality you're not going to write progress that can possibly test all the",
    "start": "989699",
    "end": "995880"
  },
  {
    "text": "different functionality and the interactions between those functionalities so the probe is a narrow in terms of the actual functionality",
    "start": "995880",
    "end": "1001519"
  },
  {
    "text": "they're testing and you're not going to find all the problems by doing that and in terms of correctness you know",
    "start": "1001519",
    "end": "1007759"
  },
  {
    "text": "we've got great test coverage but production platforms their geometry is",
    "start": "1007759",
    "end": "1013519"
  },
  {
    "text": "different the traffic going through it is different the dependencies you're going to have prod versions of",
    "start": "1013519",
    "end": "1019220"
  },
  {
    "text": "dependencies and and so you're never going to find all the problems with automated testing and hope is not the",
    "start": "1019220",
    "end": "1024860"
  },
  {
    "text": "strategy of course so what can we do",
    "start": "1024860",
    "end": "1029438"
  },
  {
    "start": "1029000",
    "end": "1119000"
  },
  {
    "text": "from the point of view of the operator of a platform basically what you have",
    "start": "1031100",
    "end": "1036798"
  },
  {
    "text": "is events and in the case of the serverless platform these events are requests",
    "start": "1036799",
    "end": "1042740"
  },
  {
    "text": "and you can see you know this is the latency on the uh on the y-axis and this",
    "start": "1042740",
    "end": "1048500"
  },
  {
    "text": "is time so fundamentally what we're seeing come in to the platform is a whole bunch of events",
    "start": "1048500",
    "end": "1053840"
  },
  {
    "text": "but you can't really make sense out of this like you can't look at this or even analyze this and say whether it looks",
    "start": "1053840",
    "end": "1060740"
  },
  {
    "text": "normal or not but from a customer perspective it normally is pretty obvious whether something's okay or not",
    "start": "1060740",
    "end": "1066160"
  },
  {
    "text": "a particular customer workloads which in our case is a version",
    "start": "1066160",
    "end": "1071299"
  },
  {
    "text": "of an app that's deployed will typically have pretty predictable Behavior it'll have something that you can look at and",
    "start": "1071299",
    "end": "1077960"
  },
  {
    "text": "say that looks good or it doesn't and certainly most customers know what looks good and what doesn't and fundamentally",
    "start": "1077960",
    "end": "1084200"
  },
  {
    "text": "what you want as a customer is that my app behaves the same way today as it did yesterday",
    "start": "1084200",
    "end": "1090559"
  },
  {
    "text": "so a customer is expecting you know whatever the performance is I want it to stay the same and if it changes I'm",
    "start": "1090559",
    "end": "1096500"
  },
  {
    "text": "going to send you a ticket so the customer expectation is you know this is how our application performs I want it",
    "start": "1096500",
    "end": "1101539"
  },
  {
    "text": "to continue behaving in the same way and so when you break it down to the individual workloads we have this idea",
    "start": "1101539",
    "end": "1107000"
  },
  {
    "text": "of reliability which is actually that the individual workload should perform in the same way over time",
    "start": "1107000",
    "end": "1113179"
  },
  {
    "text": "and we can give this a statistical meaning so if you take an individual workload and you plot that workload in",
    "start": "1113179",
    "end": "1120679"
  },
  {
    "start": "1119000",
    "end": "1171000"
  },
  {
    "text": "terms of the latency on the x-axis and the number of requests on the y-axis",
    "start": "1120679",
    "end": "1126340"
  },
  {
    "text": "this is a log normal distribution and you can see this is a pretty good fit for the log normal distribution and",
    "start": "1126340",
    "end": "1133100"
  },
  {
    "text": "what we're looking for statistically in terms of reliability is that this",
    "start": "1133100",
    "end": "1138679"
  },
  {
    "text": "distribution should stay the same over time the workload should continue to behave",
    "start": "1138679",
    "end": "1143960"
  },
  {
    "text": "the same way tomorrow as it did today that is to say that the distribution of latencies should continue to be the same",
    "start": "1143960",
    "end": "1149960"
  },
  {
    "text": "over time and in statistics this is a characteristic called stationarity so what we're looking for is stationarity",
    "start": "1149960",
    "end": "1155840"
  },
  {
    "text": "of workloads basically that that's what's going to tell us that everything's okay",
    "start": "1155840",
    "end": "1160940"
  },
  {
    "text": "and so armed with this concept of stationarity we can then go on and Define reliability in a way that we can",
    "start": "1160940",
    "end": "1167780"
  },
  {
    "text": "actually use to fulfill those goals that I talked about earlier using What's called the two Sigma technique so it'll become clear why it's",
    "start": "1167780",
    "end": "1175400"
  },
  {
    "start": "1171000",
    "end": "1470000"
  },
  {
    "text": "called the two Sigma technique in in just a minute our hypothesis as we've said is that",
    "start": "1175400",
    "end": "1181720"
  },
  {
    "text": "workloads that are self-similar so workloads that are well-defined should have consistent performance over time",
    "start": "1181720",
    "end": "1190720"
  },
  {
    "text": "what we're going to do is we're going to take all our workloads on the platform and partition them into cohorts where a",
    "start": "1190940",
    "end": "1196400"
  },
  {
    "text": "cohort is something which is going to be self-similar over time and then we're going to build baselines for each of",
    "start": "1196400",
    "end": "1203059"
  },
  {
    "text": "those cohorts so for each workloads each cohort we're going to determine a baseline which is basically we're going",
    "start": "1203059",
    "end": "1210020"
  },
  {
    "text": "to model the distribution that we care about and then for every new event that comes in we can look at that event compare it",
    "start": "1210020",
    "end": "1217580"
  },
  {
    "text": "to the distribution and see whether or not or to what extent it's anomalous and basically test for stationarity of our",
    "start": "1217580",
    "end": "1224360"
  },
  {
    "text": "new incoming data for requests versus the established Baseline of its past",
    "start": "1224360",
    "end": "1229460"
  },
  {
    "text": "Behavior and then the result is that we end up",
    "start": "1229460",
    "end": "1235039"
  },
  {
    "text": "with a set of events with predicted likelihoods what's the probability of this request having this latency",
    "start": "1235039",
    "end": "1240799"
  },
  {
    "text": "compared to the Baseline and we can create a Time series for of that where the time series is showing you for every",
    "start": "1240799",
    "end": "1246980"
  },
  {
    "text": "request to what extent it's deviation from the expected performance of that",
    "start": "1246980",
    "end": "1252500"
  },
  {
    "text": "workload how do we do this in practice well there's two pieces to it firstly we've",
    "start": "1252500",
    "end": "1258860"
  },
  {
    "text": "got to establish the baselines the model so what we do here is we have a batch process that looks at historical service",
    "start": "1258860",
    "end": "1265340"
  },
  {
    "text": "data and partitions it into cohorts on the serverless platform those cohorts are",
    "start": "1265340",
    "end": "1271580"
  },
  {
    "text": "app versions so every app version We deploy we create a baseline for",
    "start": "1271580",
    "end": "1276620"
  },
  {
    "text": "this technique was developed actually not in serverless it was developed by the bigquery team bigquery is Google",
    "start": "1276620",
    "end": "1282980"
  },
  {
    "text": "Cloud's data warehouse platform and so actually they're looking at basically",
    "start": "1282980",
    "end": "1288860"
  },
  {
    "text": "SQL requests you send SQL in you get query results back so they're using it it was invented to deal with a",
    "start": "1288860",
    "end": "1294380"
  },
  {
    "text": "completely different domain and it works great there and the cohorts there are not the same I'll talk a little bit about that later but for the for the app",
    "start": "1294380",
    "end": "1301520"
  },
  {
    "text": "server platform the cohorts are basically app versions that's what we expect to be sell similar over time and",
    "start": "1301520",
    "end": "1307159"
  },
  {
    "text": "then for each workload we try and fit",
    "start": "1307159",
    "end": "1312500"
  },
  {
    "text": "um a distribution to it this is showing a log a normal distribution in practice we",
    "start": "1312500",
    "end": "1317900"
  },
  {
    "text": "actually use log normal but you can see you know here's a workload in yellow where you can fit a normal distribution",
    "start": "1317900",
    "end": "1323480"
  },
  {
    "text": "pretty well to it this green one is less good and then this red one is a bimodal distribution and that's not going to fit",
    "start": "1323480",
    "end": "1329120"
  },
  {
    "text": "a normal curve at all and that's what we find in practice we find that for the serverless platform about 50 of our",
    "start": "1329120",
    "end": "1335840"
  },
  {
    "text": "workloads can be modeled with a log normal curve and then the other 50 we reject because they're not a good fit we",
    "start": "1335840",
    "end": "1341960"
  },
  {
    "text": "can't fit a log normal curve to to that distribution very well and then what we do is we store the",
    "start": "1341960",
    "end": "1347600"
  },
  {
    "text": "parameters of the distribution in a database along with the cohort identifier so every cohort has an",
    "start": "1347600",
    "end": "1353900"
  },
  {
    "text": "identifier and we store like what's the mean what's the standard deviation basically in a database so that's our",
    "start": "1353900",
    "end": "1359179"
  },
  {
    "text": "baselines then for every request that comes in when the networking when the request is",
    "start": "1359179",
    "end": "1364640"
  },
  {
    "text": "complete the networking stack emits those metrics that we care about and we grab the enter and latency",
    "start": "1364640",
    "end": "1371720"
  },
  {
    "text": "for that request and we compare it against the Baseline for that cohort and we compute what's called a z-score",
    "start": "1371720",
    "end": "1378020"
  },
  {
    "text": "so a z-score is essentially how many standard deviations away from the mean",
    "start": "1378020",
    "end": "1384620"
  },
  {
    "text": "is that request's latency and this is like if you were falling",
    "start": "1384620",
    "end": "1389780"
  },
  {
    "text": "asleep this is the point to wake up and pay attention because it's by Computing the Z score that we can actually get",
    "start": "1389780",
    "end": "1394940"
  },
  {
    "text": "something which is normalized across all the different workloads and we can get basically a graph of Z scores over time",
    "start": "1394940",
    "end": "1401539"
  },
  {
    "text": "and so what we expect is the Z scores should maintain the same shape across",
    "start": "1401539",
    "end": "1406940"
  },
  {
    "text": "projects across cells across regions because the Zed score should follow a",
    "start": "1406940",
    "end": "1414200"
  },
  {
    "text": "normal distribution that's what we're expecting so this is the technique by which we",
    "start": "1414200",
    "end": "1419659"
  },
  {
    "text": "take all this stuff and we normalize it into a graph where we can see for every request how far is this request away",
    "start": "1419659",
    "end": "1425480"
  },
  {
    "text": "from what we expect and is it an outlier and What proportion of our requests are",
    "start": "1425480",
    "end": "1431360"
  },
  {
    "text": "actually outliers and we can use this to take that intent which is you know app dependent and turn it to something where",
    "start": "1431360",
    "end": "1438860"
  },
  {
    "text": "the intent is not app dependent and we can combine that and we can aggregate it and we can calculate across the platform",
    "start": "1438860",
    "end": "1444679"
  },
  {
    "text": "as a whole how it's behaving so that's kind of the important bit so that is pretty computationally cheap",
    "start": "1444679",
    "end": "1452000"
  },
  {
    "text": "and then how do we turn this into a data product that we can actually use for monitoring and alerting and Performance",
    "start": "1452000",
    "end": "1457280"
  },
  {
    "text": "Management purposes well",
    "start": "1457280",
    "end": "1462220"
  },
  {
    "text": "here's what we do we aggregate those Z scores across workloads and then what we look at is the fraction",
    "start": "1462620",
    "end": "1470299"
  },
  {
    "start": "1470000",
    "end": "1601000"
  },
  {
    "text": "of requests where the z-score is above two so more than two standard deviations away from what we expect",
    "start": "1470299",
    "end": "1477860"
  },
  {
    "text": "from the mean so any of you who's familiar with science you might know that like five",
    "start": "1477860",
    "end": "1483799"
  },
  {
    "text": "Sigma is the you know the threshold for something to be established as as you know scientific uh",
    "start": "1483799",
    "end": "1491179"
  },
  {
    "text": "as valid in the context of science but that's something very unlikely for a platform we're not looking for extremely",
    "start": "1491179",
    "end": "1497600"
  },
  {
    "text": "unlikely Black Swan events we're looking for something which is abnormal which might indicate that something's wrong that's going to give us a good",
    "start": "1497600",
    "end": "1503720"
  },
  {
    "text": "prediction that you know customers are going to start sending us tickets and what we found empirically is that two",
    "start": "1503720",
    "end": "1508820"
  },
  {
    "text": "Sigma is a good cutoff for that so what we look at is the fraction of requests that we get that are above two",
    "start": "1508820",
    "end": "1515840"
  },
  {
    "text": "Sigma from the mean and we expect based on the statistics of a normal distribution that that will be",
    "start": "1515840",
    "end": "1522020"
  },
  {
    "text": "about two to five percent of requests that's what a normal distribution is when that number goes above 10 percent",
    "start": "1522020",
    "end": "1530419"
  },
  {
    "text": "basically we know there's a problem we're expecting that you know two to five percent of requests will be",
    "start": "1530419",
    "end": "1536299"
  },
  {
    "text": "two Sigma away or less away from the mean when that starts going above 10 we know there's a problem and again we can",
    "start": "1536299",
    "end": "1542900"
  },
  {
    "text": "aggregate this across the cell so we can see when a cell is is starting to regress and there's some kind of",
    "start": "1542900",
    "end": "1549020"
  },
  {
    "text": "performance regression in the cell and so detection is based on the fraction of workloads that are",
    "start": "1549020",
    "end": "1555559"
  },
  {
    "text": "exhibiting a regression",
    "start": "1555559",
    "end": "1558880"
  },
  {
    "text": "so what data products uh what alerting and monitoring do we actually create based",
    "start": "1562220",
    "end": "1569059"
  },
  {
    "text": "on this well the main data product that we create from this is a graph which",
    "start": "1569059",
    "end": "1575419"
  },
  {
    "text": "uh creates a distribution across all the different projects running in a region",
    "start": "1575419",
    "end": "1580880"
  },
  {
    "text": "so we take all the projects in the region and we stuck them up into a distribution and typically we use like a",
    "start": "1580880",
    "end": "1587539"
  },
  {
    "text": "15 minute window for that and so there's a lot of services that are low TPS and",
    "start": "1587539",
    "end": "1592640"
  },
  {
    "text": "so the z-score is going to be either zero or one and so we stack all the projects within a particular cell up and",
    "start": "1592640",
    "end": "1599179"
  },
  {
    "text": "calculate the median Z score and actually a bunch of other different parts of the distribution as well so",
    "start": "1599179",
    "end": "1605779"
  },
  {
    "start": "1601000",
    "end": "1688000"
  },
  {
    "text": "what you can see here is the the first graph that you look at when you get an alert and what this shows is that 50 of",
    "start": "1605779",
    "end": "1613520"
  },
  {
    "text": "projects are experiencing J scores of at least eight what does that mean so J",
    "start": "1613520",
    "end": "1618980"
  },
  {
    "text": "scores of at least eight means that at least eight percent of requests",
    "start": "1618980",
    "end": "1624620"
  },
  {
    "text": "for that project are more than two Sigma above mean so that's you know it's not horrible but",
    "start": "1624620",
    "end": "1632659"
  },
  {
    "text": "it's definitely above the two point two to five percent that we're expecting so 50 of the projects are experiencing a jscore release date that's a sign that",
    "start": "1632659",
    "end": "1639679"
  },
  {
    "text": "you know fifty percent of our customers are experiencing something that they might notice is a regression and then we",
    "start": "1639679",
    "end": "1645500"
  },
  {
    "text": "also have a 30 um line as well here so 30 of projects that are experiencing J scores of at",
    "start": "1645500",
    "end": "1651799"
  },
  {
    "text": "least 30. so this is the jscore the anomaly percentage the percentage of requests that are more than two Sigma",
    "start": "1651799",
    "end": "1657200"
  },
  {
    "text": "away from the mean and so like if 30 of customers are experiencing A J school where thirty percent of requests",
    "start": "1657200",
    "end": "1664100"
  },
  {
    "text": "are deviating more than two Sigma from the mean that like people are people can",
    "start": "1664100",
    "end": "1669440"
  },
  {
    "text": "see that people can spot that and we know there's a problem so it's very easy to see when there's a problem",
    "start": "1669440",
    "end": "1675020"
  },
  {
    "text": "and again this is end to end for requests um so it's an accurate reflection of the",
    "start": "1675020",
    "end": "1680600"
  },
  {
    "text": "customer experience and then the second thing we want to determine is there a problem what's the",
    "start": "1680600",
    "end": "1685760"
  },
  {
    "text": "impact of that problem so the next graph that we have is um this one and what this is showing you",
    "start": "1685760",
    "end": "1692299"
  },
  {
    "start": "1688000",
    "end": "1743000"
  },
  {
    "text": "is the number of projects where the J score is above 10 which means the number",
    "start": "1692299",
    "end": "1698059"
  },
  {
    "text": "of projects where the percentage of requests two Sigma or more above mean is 10 or",
    "start": "1698059",
    "end": "1704840"
  },
  {
    "text": "more so that gives you you can look at that and you can immediately see the number of projects that are impacted by that so that's the second piece of",
    "start": "1704840",
    "end": "1711500"
  },
  {
    "text": "information I need is an on-caller to determine that there's an incident that I need to respond to and calculate the",
    "start": "1711500",
    "end": "1717320"
  },
  {
    "text": "severity of that incident so we can decide if we're going to externalize it and tell other people or if we're going",
    "start": "1717320",
    "end": "1722600"
  },
  {
    "text": "to just deal with it on a per customer basis so very easy to see at a glance",
    "start": "1722600",
    "end": "1728559"
  },
  {
    "text": "firstly is this a real problem is it a problem with the platform secondly how",
    "start": "1728559",
    "end": "1733580"
  },
  {
    "text": "many customers are impacted by this do I need to escalate this so at this point people normally have a",
    "start": "1733580",
    "end": "1739220"
  },
  {
    "text": "lot of questions because I've used a lot of stats words so I'm going to try and head them off a little bit firstly",
    "start": "1739220",
    "end": "1745159"
  },
  {
    "start": "1743000",
    "end": "1916000"
  },
  {
    "text": "do performance metrics actually follow normal distributions Jess no um you know you're always fitting a",
    "start": "1745159",
    "end": "1751880"
  },
  {
    "text": "curve uh and so the curve is never going to be a perfect fit um actually we use log normal",
    "start": "1751880",
    "end": "1757279"
  },
  {
    "text": "distributions as I said as I said earlier so log normal distribution um well I show you that earlier",
    "start": "1757279",
    "end": "1763720"
  },
  {
    "text": "and that gives you a better variety of fits we can we can actually fit about 50",
    "start": "1763720",
    "end": "1769460"
  },
  {
    "text": "of our workloads using a log normal distribution um",
    "start": "1769460",
    "end": "1774500"
  },
  {
    "text": "do we know if the approximations hold will we test that we use case stats in fact to test the extent to which the",
    "start": "1774500",
    "end": "1780559"
  },
  {
    "text": "curve fits the actual distribution and we reject baselines if the if the case",
    "start": "1780559",
    "end": "1786320"
  },
  {
    "text": "stats don't fit well so we have a threshold that we use for that and as I say about 50 of our workloads meets",
    "start": "1786320",
    "end": "1792559"
  },
  {
    "text": "statistical tests for whether the distribution fits the actual Behavior",
    "start": "1792559",
    "end": "1799000"
  },
  {
    "text": "how do you Define cohorts well in the serverless platform is pretty",
    "start": "1799220",
    "end": "1804320"
  },
  {
    "text": "straightforward a cohort is an app version now you could get more sophisticated than that because different parts within an app might have",
    "start": "1804320",
    "end": "1810740"
  },
  {
    "text": "different behaviors in practice that doesn't seem to be a problem in terms of actually uh the the use cases that we",
    "start": "1810740",
    "end": "1818899"
  },
  {
    "text": "use these metrics for it works fine for bigquery they use something different they have different",
    "start": "1818899",
    "end": "1823940"
  },
  {
    "text": "features and they calculate like a cross-productive features to Define cohorts so depending on your application",
    "start": "1823940",
    "end": "1829520"
  },
  {
    "text": "of this process you're going to Define cohorts differently but that's one of the first things you've got to work out when you use this technique is how are",
    "start": "1829520",
    "end": "1835640"
  },
  {
    "text": "we going to Define our cohorts but for app server it's pretty straightforward",
    "start": "1835640",
    "end": "1841898"
  },
  {
    "text": "and then how do we deal with Singleton or infrequent workloads well the answer to that is is twofold firstly if you",
    "start": "1848179",
    "end": "1856880"
  },
  {
    "text": "have low QPS then it you know it's quite hard to actually fit a distribution and",
    "start": "1856880",
    "end": "1862039"
  },
  {
    "text": "be confident that that distribution is a good fit because it will be underdetermined by the data but also",
    "start": "1862039",
    "end": "1867080"
  },
  {
    "text": "more practically if someone's not sending a lot of QPS they probably don't have very strong expectations about the",
    "start": "1867080",
    "end": "1873080"
  },
  {
    "text": "behavior of it so we just we don't care about that if we're getting less than 30",
    "start": "1873080",
    "end": "1878179"
  },
  {
    "text": "requests to a particular adversion over the one month period with Baseline over we just we just don't bother baselining",
    "start": "1878179",
    "end": "1885380"
  },
  {
    "text": "that or we don't use the Baseline so if it's infrequent probably the customer doesn't have strong expectations about",
    "start": "1885380",
    "end": "1890659"
  },
  {
    "text": "the behavior and so we don't care about it and then obviously you're like okay Jazz",
    "start": "1890659",
    "end": "1896179"
  },
  {
    "text": "that's a lot of words does this really work so the next thing we do is we go and do a whole bunch of back testing so",
    "start": "1896179",
    "end": "1902059"
  },
  {
    "text": "we go over like the last uh three four months of incidents and we go and look",
    "start": "1902059",
    "end": "1907100"
  },
  {
    "text": "and see could we find a signal in our data from the two Sigma process that",
    "start": "1907100",
    "end": "1913640"
  },
  {
    "text": "shows this problem occurring and actually we find it's a really good fit which is why we're using it because it",
    "start": "1913640",
    "end": "1919700"
  },
  {
    "start": "1916000",
    "end": "2001000"
  },
  {
    "text": "works so this is the graph that we use traditionally for request delivery latency that that metric that we we use",
    "start": "1919700",
    "end": "1926600"
  },
  {
    "text": "which is the latency from um when it hits the networking layer in the cell to when it",
    "start": "1926600",
    "end": "1933080"
  },
  {
    "text": "hits your app um and we have you know 90 for 90 of projects 95 99",
    "start": "1933080",
    "end": "1940520"
  },
  {
    "text": "um and you can see here this is this is a regression that's hitting um that's",
    "start": "1940520",
    "end": "1945620"
  },
  {
    "text": "taking us below SLO for 99 of projects so that's like a that's a problem that we would care",
    "start": "1945620",
    "end": "1950960"
  },
  {
    "text": "about this is fine tunes obviously because we have a parameter for what the SLO is and we've got these different",
    "start": "1950960",
    "end": "1957140"
  },
  {
    "text": "things um that we're looking at um but this isn't fine-tuned at all this is just like",
    "start": "1957140",
    "end": "1962179"
  },
  {
    "text": "throw the data at the thing and it shows very clearly that there's a problem so it's super easy to see problems without",
    "start": "1962179",
    "end": "1968360"
  },
  {
    "text": "any fine tuning of the parameters of the model um it's just very very clear and we",
    "start": "1968360",
    "end": "1974480"
  },
  {
    "text": "found that the signal that came I love it when this happens you're like doing a lot of stats and a lot of programming and then you're like oh this really",
    "start": "1974480",
    "end": "1980779"
  },
  {
    "text": "works yay so that was very satisfying when I went through or we went through all the",
    "start": "1980779",
    "end": "1986120"
  },
  {
    "text": "incidents and we could see this really clear signal uh in in the data that we were producing that matched up with the",
    "start": "1986120",
    "end": "1991640"
  },
  {
    "text": "data that are being gathered from all the incidents uh in the course of investigating those incidents so it",
    "start": "1991640",
    "end": "1997279"
  },
  {
    "text": "works really well actually which is great which is why I'm here um there are limitations firstly I had",
    "start": "1997279",
    "end": "2003700"
  },
  {
    "start": "2001000",
    "end": "2113000"
  },
  {
    "text": "to explain a lot of stats words uh who in this audience had heard of Zed scores before and was like oh geez you're using",
    "start": "2003700",
    "end": "2009220"
  },
  {
    "text": "Zed scores well actually okay you're you're my people amazing about half of you great",
    "start": "2009220",
    "end": "2015039"
  },
  {
    "text": "um so you know if you understand Zed scores and J scores it's going to be a fairly simple uh Journey for you but if",
    "start": "2015039",
    "end": "2020500"
  },
  {
    "text": "you don't know what those things are then there's a bunch of stats words you have to learn and then when you see that graph you have to do a bunch of thinking",
    "start": "2020500",
    "end": "2026140"
  },
  {
    "text": "in your head which is not normally the first thing that you want to do when you're in the middle of an instant um so that's an issue but in practice we",
    "start": "2026140",
    "end": "2033220"
  },
  {
    "text": "found with a bit of training that people get it and it's pretty easy to grok uh we get about 40 to 60 of coverage",
    "start": "2033220",
    "end": "2040960"
  },
  {
    "text": "um based on the fit of the curves and QPS and we find that's actually",
    "start": "2040960",
    "end": "2046600"
  },
  {
    "text": "absolutely fine we have some services that have a much lower level of coverage of around five percent which is what we",
    "start": "2046600",
    "end": "2052240"
  },
  {
    "text": "got when we started using it with just a normal distribution instead and in practice you can still discover platform",
    "start": "2052240",
    "end": "2058179"
  },
  {
    "text": "regressions just fine using a much lower coverage um 50 to 40 60 is great",
    "start": "2058179",
    "end": "2065440"
  },
  {
    "text": "um this in implementation doesn't tell you why there's a problem it just tells you that's a problem in the context of",
    "start": "2065440",
    "end": "2070898"
  },
  {
    "text": "operability this is actually fine um this is called symptom-based alerting and that's actually better than cause",
    "start": "2070899",
    "end": "2077378"
  },
  {
    "text": "based alerting we have a lot of course based alerts and the problem with them is they're noisy and you get a lot of false",
    "start": "2077379",
    "end": "2083320"
  },
  {
    "text": "positives because yes this thing may have gone wrong but what you actually care about is did it have a customer impact and so you then go and have to go",
    "start": "2083320",
    "end": "2089919"
  },
  {
    "text": "and hunt you know did this have a customer impact how do we measure the customer impact is this the actual cause of that customer impact or is it not so",
    "start": "2089919",
    "end": "2097240"
  },
  {
    "text": "um they're noisy they're problematic we try and get rid of course based alerts in favor of symptom-based alerts and",
    "start": "2097240",
    "end": "2102520"
  },
  {
    "text": "this is an amazing symptom based alert because it's reflecting the customer experience and as you can see we can",
    "start": "2102520",
    "end": "2108160"
  },
  {
    "text": "very easily determine the impact on customers so that's our application but actually",
    "start": "2108160",
    "end": "2113500"
  },
  {
    "start": "2113000",
    "end": "2120000"
  },
  {
    "text": "this has a ton of other applications um so the bigquery team actually used this",
    "start": "2113500",
    "end": "2119800"
  },
  {
    "text": "not just to diagnose problems but also not only to detect problems but also to diagnose problems so the total time for",
    "start": "2119800",
    "end": "2127900"
  },
  {
    "start": "2120000",
    "end": "2226000"
  },
  {
    "text": "a bigquery request you can split into queuing time versus execution time and you can take the execution time and",
    "start": "2127900",
    "end": "2134260"
  },
  {
    "text": "split that into i o time and compute time and so forth and what they actually do is they calculate the latencies of",
    "start": "2134260",
    "end": "2140079"
  },
  {
    "text": "those individual phases and then Chuck them into exactly the same engine and so here's an example of something from one",
    "start": "2140079",
    "end": "2147520"
  },
  {
    "text": "of the bigquery cells where you can actually see there's this actually multi-day regression of performance and",
    "start": "2147520",
    "end": "2154480"
  },
  {
    "text": "then they split that this is i o time whenever you see a square wave on the graph in production that's terrifying",
    "start": "2154480",
    "end": "2161339"
  },
  {
    "text": "but you can see very clearly that this regression is dominated by IO time so in",
    "start": "2161339",
    "end": "2167079"
  },
  {
    "text": "that situation like okay it's the i o that's a problem and what they were able to do is uh and this is something you",
    "start": "2167079",
    "end": "2173140"
  },
  {
    "text": "can do in a multi so all Google cloud services within a region served out of",
    "start": "2173140",
    "end": "2179200"
  },
  {
    "text": "multiple cells so what you do is you just drain this cell so that you're not serving from the cell because this IO problem is limited to a single cell so",
    "start": "2179200",
    "end": "2185740"
  },
  {
    "text": "you drain the cell to mitigate it while you've worked to resolve the problem so you can definitely use this technique",
    "start": "2185740",
    "end": "2191440"
  },
  {
    "text": "for diagnosis as well and and there's a lot of work being done at the moment to basically look to look for correlations",
    "start": "2191440",
    "end": "2198400"
  },
  {
    "text": "so here are all the dependencies let's find correlations in the dependencies what you're trying to do fundamentally",
    "start": "2198400",
    "end": "2203619"
  },
  {
    "text": "with a distributed system is decorate everything that's the purpose of a distributed system is to decorate things",
    "start": "2203619",
    "end": "2209800"
  },
  {
    "text": "so you remove single points of failure so anytime you see correlations that's normally a bad sign that's a sign that",
    "start": "2209800",
    "end": "2215920"
  },
  {
    "text": "your distributed system is not doing what it's supposed to do so correlations are actually very useful to know about",
    "start": "2215920",
    "end": "2221380"
  },
  {
    "text": "when you're trying to diagnose a problem in production we have an experiments framework in",
    "start": "2221380",
    "end": "2226839"
  },
  {
    "start": "2226000",
    "end": "2253000"
  },
  {
    "text": "Google that we use to push out new features and you can actually use this for looking at experiments and doing a b",
    "start": "2226839",
    "end": "2233500"
  },
  {
    "text": "testing for experiments you can look at the impact of an experiment on cohorts and see if it shifts the things you care",
    "start": "2233500",
    "end": "2240400"
  },
  {
    "text": "about the behavior that you care about with an A B test as well so that's something we've been using it for and",
    "start": "2240400",
    "end": "2245980"
  },
  {
    "text": "investigating as well so that to say there's a ton of different applications for this that we're only really",
    "start": "2245980",
    "end": "2251140"
  },
  {
    "text": "scratching the surface of right now so just to sum up we have this technique",
    "start": "2251140",
    "end": "2257079"
  },
  {
    "start": "2253000",
    "end": "2402000"
  },
  {
    "text": "we can use it to reliably detect and measure the impact of platform regressions and know if we have a",
    "start": "2257079",
    "end": "2262960"
  },
  {
    "text": "platform problem versus a customer problem and and work out the severity of that so",
    "start": "2262960",
    "end": "2268720"
  },
  {
    "text": "we know kind of what level of action we need to take about it um fundamentally the model of",
    "start": "2268720",
    "end": "2274720"
  },
  {
    "text": "reliability we're using reliability is a shared property between customer and service and customers know what they",
    "start": "2274720",
    "end": "2281140"
  },
  {
    "text": "expect and what we want to do is be able to model customer expectation and make sure that that thing behaves the same",
    "start": "2281140",
    "end": "2287380"
  },
  {
    "text": "way over time that's fundamentally the property we care about and that's hence what we're modeling using this technique",
    "start": "2287380",
    "end": "2293920"
  },
  {
    "text": "um and the key thing is that transformation of latencies into Z scores so that we",
    "start": "2293920",
    "end": "2299619"
  },
  {
    "text": "can combine those Z scores and aggregate them across aggregation levels that we care about such as projects and cells",
    "start": "2299619",
    "end": "2306220"
  },
  {
    "text": "and regions and actually we use the number of minutes for which the jscore for a cell",
    "start": "2306220",
    "end": "2313300"
  },
  {
    "text": "as a whole is about 10 as a measure of um we we call that metric overloaded",
    "start": "2313300",
    "end": "2318820"
  },
  {
    "text": "minutes and we use that as a measure of the reliability as a one of the measurements of the reliability of the",
    "start": "2318820",
    "end": "2324040"
  },
  {
    "text": "platform as well what customers care about fundamentally is variability like is this thing staying the same or",
    "start": "2324040",
    "end": "2330880"
  },
  {
    "text": "is it not which is what we're measuring and if you extend this technique and apply it to a",
    "start": "2330880",
    "end": "2337180"
  },
  {
    "text": "bunch of different things within the platform you can look at the correlation and detect decoration which is often a",
    "start": "2337180",
    "end": "2343000"
  },
  {
    "text": "good way of looking for causes of problems um this two single method we think there's",
    "start": "2343000",
    "end": "2349240"
  },
  {
    "text": "actually a ton of applications of this we think it's very widely applicable I'm not going to go through all of this now",
    "start": "2349240",
    "end": "2355300"
  },
  {
    "text": "because I want to leave some time for questions but you know fundamentally what we can do is we can look for invariants of the system",
    "start": "2355300",
    "end": "2361540"
  },
  {
    "text": "we can look for things that we expect to stay the same where we expect the distribution to be stationary over time",
    "start": "2361540",
    "end": "2367240"
  },
  {
    "text": "any characteristic of your system that is like that where we expect the behavior to stay the same and we can",
    "start": "2367240",
    "end": "2373599"
  },
  {
    "text": "model it statistically and look for stationarity you can use this technique to model that so we think it's very",
    "start": "2373599",
    "end": "2379240"
  },
  {
    "text": "widely applicable and very powerful and we've shown that there's a way to implement it that's computationally",
    "start": "2379240",
    "end": "2386079"
  },
  {
    "text": "relatively cheap and scales up to very high QPS and then what you can use at the platform level",
    "start": "2386079",
    "end": "2391540"
  },
  {
    "text": "there's a talk by Narayan Desai and Brent Brian who are two of the creators",
    "start": "2391540",
    "end": "2396640"
  },
  {
    "text": "of this idea from srecon last year and you should go and check that out if you want to know more I work at Google we",
    "start": "2396640",
    "end": "2403480"
  },
  {
    "start": "2402000",
    "end": "2424000"
  },
  {
    "text": "have a whole bunch of books that you can get for free by going to sre.google forward slash resources to find out more",
    "start": "2403480",
    "end": "2408940"
  },
  {
    "text": "that's it",
    "start": "2408940",
    "end": "2412078"
  }
]