[
  {
    "text": "all right good afternoon it is so lovely to be here in Melbourne so when I talk about what I do",
    "start": "7279",
    "end": "16278"
  },
  {
    "text": "I don't usually use the term Ai and that's because that term means different",
    "start": "16279",
    "end": "21920"
  },
  {
    "text": "things to different people so the problem that I've been concerned with is how do we deploy and build autonomous",
    "start": "21920",
    "end": "30640"
  },
  {
    "text": "agents that work in this beautiful mess that we call the real world and I've been working on this",
    "start": "30640",
    "end": "38480"
  },
  {
    "text": "problem over the backdrop of the rise of deep learning and I recently stumbled",
    "start": "38480",
    "end": "43879"
  },
  {
    "text": "across this post by a prominent researcher in the field Andre carpet and",
    "start": "43879",
    "end": "50520"
  },
  {
    "text": "he characterizes deep neural networks not as another tool in the toolbox of a",
    "start": "50520",
    "end": "56760"
  },
  {
    "text": "software engineer but as a brand new paradigm for thinking about how to solve",
    "start": "56760",
    "end": "62280"
  },
  {
    "text": "problems in software and some amazing strides have been made in recent years to solve problems that we didn't think",
    "start": "62280",
    "end": "68799"
  },
  {
    "text": "would be solved by machines anytime soon and so the question on my mind as he",
    "start": "68799",
    "end": "75040"
  },
  {
    "text": "brings us up is will this get us to the point where this software 2.0 Paradigm",
    "start": "75040",
    "end": "80600"
  },
  {
    "text": "get us to the point where we can actually deploy these autonomous agents and the jury is out but we have",
    "start": "80600",
    "end": "88240"
  },
  {
    "text": "kind of a few choice words from some of the luminaries in the field so this is",
    "start": "88240",
    "end": "94119"
  },
  {
    "text": "some quotes from Jeff Hinton who is one of the Godfathers of deep learning and he says he's quite suspicious of back",
    "start": "94119",
    "end": "101680"
  },
  {
    "text": "propagation in particular because it requires many many many labeled training",
    "start": "101680",
    "end": "107439"
  },
  {
    "text": "examples and he's quite sure and I think many of us would agree that is not how",
    "start": "107439",
    "end": "112680"
  },
  {
    "text": "we humans learn well and also if you ask Yan Lun",
    "start": "112680",
    "end": "117799"
  },
  {
    "text": "another Godfather of deep learning he'll say that unsupervised learning the way",
    "start": "117799",
    "end": "123000"
  },
  {
    "text": "that we humans and other biological life forms learn is really the dark matter of",
    "start": "123000",
    "end": "129640"
  },
  {
    "text": "AI and I think what he means by this is that AI or the dark the the the learning",
    "start": "129640",
    "end": "137720"
  },
  {
    "text": "that we do doesn't match how machines learn today and",
    "start": "137720",
    "end": "143959"
  },
  {
    "text": "furthermore we don't understand it and yet it Ma it makes up most of how we learn so just like we don't understand",
    "start": "143959",
    "end": "150519"
  },
  {
    "text": "dark matter in the universe we don't really understand unsupervised learning yet Fran Chalet one more example he is",
    "start": "150519",
    "end": "158920"
  },
  {
    "text": "author of curus which is a deep learning library very popular and he's kind of taking this idea even further and saying",
    "start": "158920",
    "end": "165519"
  },
  {
    "text": "I'm not even sure that we've framed the question properly of that will lead us",
    "start": "165519",
    "end": "171280"
  },
  {
    "text": "down a path of inquiry to develop these intelligent autonomous",
    "start": "171280",
    "end": "176400"
  },
  {
    "text": "systems so pondering all of this what I kind of realized is that the",
    "start": "176400",
    "end": "183760"
  },
  {
    "text": "problem space I've been working on is not actually a 2.0 problem space it's",
    "start": "183760",
    "end": "188879"
  },
  {
    "text": "actually software 3.0 and so the way that I would describe it would be",
    "start": "188879",
    "end": "194720"
  },
  {
    "text": "autonomous agents acting in the real world this could be a physical agent",
    "start": "194720",
    "end": "200159"
  },
  {
    "text": "maybe it's a robot or a self-driving car or perhaps it's a virtual agent that manages your Cloud deployment for",
    "start": "200159",
    "end": "206440"
  },
  {
    "text": "example and so the way that that I envision them being deployed in the future is you would Define the interface",
    "start": "206440",
    "end": "214680"
  },
  {
    "text": "between the agent and the world I.E its sensors and its actuators and you would",
    "start": "214680",
    "end": "220760"
  },
  {
    "text": "instantiate an agent and it would learn on the job and the way that it should learn is through domain expertise from a",
    "start": "220760",
    "end": "229200"
  },
  {
    "text": "human say Mentor rather than from somebody that's tweaking model so I very",
    "start": "229200",
    "end": "234920"
  },
  {
    "text": "much expect on the job learning the other thing if we're talking about a",
    "start": "234920",
    "end": "240519"
  },
  {
    "text": "hardware um system I believe that we should be able to get away with not",
    "start": "240519",
    "end": "246439"
  },
  {
    "text": "having high-end very very sophisticated sensory devices because if you think",
    "start": "246439",
    "end": "251720"
  },
  {
    "text": "about it you know us meat wear devices they're actually very low end right if",
    "start": "251720",
    "end": "256880"
  },
  {
    "text": "you think about um the human Vision there's only a very narrow range at which we have high fidelity vision and",
    "start": "256880",
    "end": "263759"
  },
  {
    "text": "the rest of the picture is filled in inside of our brains and then going",
    "start": "263759",
    "end": "269440"
  },
  {
    "text": "forward forward in terms of the the rise of big data I think that big data will end up becoming experience so rather",
    "start": "269440",
    "end": "276720"
  },
  {
    "text": "than saying how big of a data set can we accumulate how much experience can we accumulate in an",
    "start": "276720",
    "end": "282520"
  },
  {
    "text": "agent so that's the the the problem space and I'm going to in this talk boil",
    "start": "282520",
    "end": "289400"
  },
  {
    "text": "it down to a canonical question which is why don't we have robot Butlers in our",
    "start": "289400",
    "end": "294919"
  },
  {
    "text": "homes today right this is something that we've been promised for decades and why is that not a reality",
    "start": "294919",
    "end": "300440"
  },
  {
    "text": "and once we get into that I'm going to look at is there another framework that we're not already thinking about that",
    "start": "300440",
    "end": "306360"
  },
  {
    "text": "will get us there so let's talk about our robot a little bit what kind what what is it",
    "start": "306360",
    "end": "312039"
  },
  {
    "text": "going to do so let's let's go deep into a particular use case which is you're",
    "start": "312039",
    "end": "317520"
  },
  {
    "text": "thirsty and you want this robot to bring you a beverage pretty simple right so",
    "start": "317520",
    "end": "324680"
  },
  {
    "text": "we'll add some constraints we're going to assume that the hardware is taken care of I'm not Hardware person so",
    "start": "324680",
    "end": "330919"
  },
  {
    "text": "that's my assumption there um and we're also going to assume that we are not going to be using rule-based methods",
    "start": "330919",
    "end": "337479"
  },
  {
    "text": "we've been trying for decades and they don't work so we're going to focus on learning methods in this",
    "start": "337479",
    "end": "344160"
  },
  {
    "text": "talk okay so if I had to describe the problem in a very abstract",
    "start": "344160",
    "end": "350680"
  },
  {
    "text": "way the problem that our robot is trying to solve is finding a way right given some goal that the human is happily",
    "start": "350680",
    "end": "357400"
  },
  {
    "text": "drinking his or her beverage the agent needs to navigate a path from where it",
    "start": "357400",
    "end": "362880"
  },
  {
    "text": "is to this goal and I may and it may in fact be literally a path right going",
    "start": "362880",
    "end": "369000"
  },
  {
    "text": "from your office through the hallway to the kitchen but it might be a path in very AB more abstract states of the",
    "start": "369000",
    "end": "375800"
  },
  {
    "text": "world high level planning this kind of thing but in all senses we're trying to find this path through the states of the",
    "start": "375800",
    "end": "383199"
  },
  {
    "text": "world so what's the interface look like between our agent and the world it's",
    "start": "383199",
    "end": "388919"
  },
  {
    "text": "pretty simple the agent is making a series of observations a continuous stream right",
    "start": "388919",
    "end": "394880"
  },
  {
    "text": "this is not a data set this is experience and the agent is generating action which can affect the state of the",
    "start": "394880",
    "end": "402560"
  },
  {
    "text": "world and there's some challenges okay given given that what are some of the obstacles in our place in in in place so",
    "start": "402560",
    "end": "410319"
  },
  {
    "text": "the first is the paths that the agent needs to take are not known at the",
    "start": "410319",
    "end": "417199"
  },
  {
    "text": "outset the agent must discover those paths the second is just because the",
    "start": "417199",
    "end": "423440"
  },
  {
    "text": "agent learned how to do something in one environment the environment itself can change you might remodel your house or",
    "start": "423440",
    "end": "431240"
  },
  {
    "text": "maybe you're going to take your robot to your friend's house to show off your cool Gadget right so paths that used to",
    "start": "431240",
    "end": "437199"
  },
  {
    "text": "work don't actually work in anymore and then the other thing is the observations",
    "start": "437199",
    "end": "442560"
  },
  {
    "text": "themselves are imperfect and noisy if you went to Brian's talk on cman folding",
    "start": "442560",
    "end": "448360"
  },
  {
    "text": "you would have seen that that when you're measuring say GPS data that you're not going to get the same reading",
    "start": "448360",
    "end": "454160"
  },
  {
    "text": "every time it's going to be noisy and there's going to be uncertainty about it so how do you take those observations",
    "start": "454160",
    "end": "459560"
  },
  {
    "text": "and infer the state of the world based on that so let's talk about the behavior we",
    "start": "459560",
    "end": "466680"
  },
  {
    "text": "want our robot to have one that often gets overlooked is",
    "start": "466680",
    "end": "472960"
  },
  {
    "text": "we need our agent to be able to resolve ambiguity or uncertainty about the world in a purposeful manner",
    "start": "472960",
    "end": "479960"
  },
  {
    "text": "so for example if you want your a your agent to go get you your beverage it",
    "start": "479960",
    "end": "485960"
  },
  {
    "text": "might not know where the beverage is it might go well maybe it's in the kitchen or maybe it's in the dining room what",
    "start": "485960",
    "end": "491720"
  },
  {
    "text": "you don't want to have it do is start a random grid search and look in the bathroom first because that just doesn't",
    "start": "491720",
    "end": "497199"
  },
  {
    "text": "make any sense and you won't really get timely results the other thing that we need our",
    "start": "497199",
    "end": "503039"
  },
  {
    "text": "agent to do is pursue multiple goals simultaneously in parallel so",
    "start": "503039",
    "end": "510000"
  },
  {
    "text": "the high level goal is get me my beverage but in order to do that you need to actually fulfill a number of",
    "start": "510000",
    "end": "517240"
  },
  {
    "text": "needs at the lower level so keep the battery from running out for example um",
    "start": "517240",
    "end": "525080"
  },
  {
    "text": "for example keep the agent from running into obstacles or running into the dog for so avoid",
    "start": "525080",
    "end": "533040"
  },
  {
    "text": "damage so other behaviors that we need are the ability to generalize we want skills right we don't",
    "start": "533040",
    "end": "541440"
  },
  {
    "text": "want specific um things that work only in one situation so we want the agent to be",
    "start": "541440",
    "end": "548240"
  },
  {
    "text": "able to grasp any type of cup you know a wine glass or a mug what have you we",
    "start": "548240",
    "end": "553959"
  },
  {
    "text": "want the agent to be risk sensitive that is avoid really bad States so we don't",
    "start": "553959",
    "end": "560680"
  },
  {
    "text": "want to do something stupid like jump off a balcony if that happens to be the shortest path to its goal and then",
    "start": "560680",
    "end": "568040"
  },
  {
    "text": "finally we need the agent to recover from adversity and when when I say adversity I mean situations that it",
    "start": "568040",
    "end": "573200"
  },
  {
    "text": "didn't anticipate when it first made its plan so that could be anything from the",
    "start": "573200",
    "end": "579200"
  },
  {
    "text": "cup gets knocked over to the agent decides to walk down the hallway and",
    "start": "579200",
    "end": "584600"
  },
  {
    "text": "gets blocked by a pile of laundry and if the agent is anything like me instead of",
    "start": "584600",
    "end": "589680"
  },
  {
    "text": "going around it'll forget about getting the cup and just get the laundry and do the laundry and forget about",
    "start": "589680",
    "end": "595000"
  },
  {
    "text": "it um okay so let's get down to how we can tackle this problem and we're going",
    "start": "595000",
    "end": "601279"
  },
  {
    "text": "to look at two approaches we're first going to start with reinforcement learning and this is very much a",
    "start": "601279",
    "end": "606720"
  },
  {
    "text": "software 2.0 approach the most popular sort of um approach to it is really in",
    "start": "606720",
    "end": "612640"
  },
  {
    "text": "deep learning at the moment and then we're going to talk about active inference which is a framework that I",
    "start": "612640",
    "end": "617920"
  },
  {
    "text": "believe will solve the 3.0 problem so just to show of hands how many of you would call yourselves",
    "start": "617920",
    "end": "624560"
  },
  {
    "text": "somewhat familiar with reinforcement learning okay a few hands",
    "start": "624560",
    "end": "629839"
  },
  {
    "text": "but I'll I'll kind of describe the basics enough that uh you'll you'll be familiar enough to understand this so",
    "start": "629839",
    "end": "637320"
  },
  {
    "text": "before we dive in we have to describe this interface because I just lied to",
    "start": "637320",
    "end": "643920"
  },
  {
    "text": "you because I said there was an observation and action but actually the way that reinforcement learning problems",
    "start": "643920",
    "end": "650240"
  },
  {
    "text": "are set up is that there's this additional piece of the interface which is called a reward and a reward is",
    "start": "650240",
    "end": "657760"
  },
  {
    "text": "really a number and it's an arbitrary number that measures the goodness of the states of",
    "start": "657760",
    "end": "664920"
  },
  {
    "text": "the world and so as a as a designer if you're coming up with you know um your",
    "start": "664920",
    "end": "671240"
  },
  {
    "text": "environment for your agent you want to design a reward function that rewards behaviors that you want it to do and you",
    "start": "671240",
    "end": "678519"
  },
  {
    "text": "want the behavior that maximizes the reward to be doing exactly what you want",
    "start": "678519",
    "end": "686760"
  },
  {
    "text": "because that's all the agent's going to do it's going to try and Maxim the reward through trial and error it's going to try all these different things",
    "start": "686760",
    "end": "693200"
  },
  {
    "text": "and it's going to figure out what's going to be the most rewarding so a note here on deep",
    "start": "693200",
    "end": "698760"
  },
  {
    "text": "reinforcement learning this is a very very cartoonish picture of what it looks like um but essentially in deep",
    "start": "698760",
    "end": "705839"
  },
  {
    "text": "reinforcement learning there's one or more neural networks inside of the agent",
    "start": "705839",
    "end": "711160"
  },
  {
    "text": "and the basic idea is the observations come in at the bottom and out at the top",
    "start": "711160",
    "end": "716279"
  },
  {
    "text": "come in action this would be like a video game up down left right right and there's somewhere in here this policy",
    "start": "716279",
    "end": "723000"
  },
  {
    "text": "it's usually uh Ed the symbol pi to describe it that that selects the action",
    "start": "723000",
    "end": "728760"
  },
  {
    "text": "that affects the environment and then the reward comes back after that so",
    "start": "728760",
    "end": "733920"
  },
  {
    "text": "there are many many ways you might solve this problem with deep neural networks",
    "start": "733920",
    "end": "739680"
  },
  {
    "text": "and at this point you might be expecting me to start talking about all the different flavors of how you might do that but I'm not going to do that I'm",
    "start": "739680",
    "end": "747800"
  },
  {
    "text": "going to instead talk about this reward function because I believe that this is fundamentally limiting our view on the",
    "start": "747800",
    "end": "755079"
  },
  {
    "text": "problem and limiting what we can do with these agents okay so let's look at an example",
    "start": "755079",
    "end": "763680"
  },
  {
    "text": "of how a reward function might be constructed so this is from a paper in",
    "start": "763680",
    "end": "768760"
  },
  {
    "text": "2015 and the goal was for different bodies either a humanoid here or say",
    "start": "768760",
    "end": "775920"
  },
  {
    "text": "something with our quadraped learn how to locomote right learn how to move forward",
    "start": "775920",
    "end": "782240"
  },
  {
    "text": "as fast as you can and so in this environment the reward function is using",
    "start": "782240",
    "end": "788040"
  },
  {
    "text": "these kind of internal or hidden states of the world the velocity forward how",
    "start": "788040",
    "end": "793959"
  },
  {
    "text": "fast the agent is moving is a big component of the reward So the faster the agent moves the more reward it gets",
    "start": "793959",
    "end": "801320"
  },
  {
    "text": "however you also don't want the agent to put undue forces on its body so there's",
    "start": "801320",
    "end": "807880"
  },
  {
    "text": "this kind of negative re W coming out of impact forces and joint torqus and so at",
    "start": "807880",
    "end": "814079"
  },
  {
    "text": "the end of the day the agent actually learns how to walk or to crawl or to run",
    "start": "814079",
    "end": "820800"
  },
  {
    "text": "quite well so question for all of you guys if we take this agent that learned",
    "start": "820800",
    "end": "827519"
  },
  {
    "text": "how to locomote and put it in our homes into our robot would would it actually",
    "start": "827519",
    "end": "833839"
  },
  {
    "text": "work raise your hand if you think this actually would work well okay no one person two people okay",
    "start": "833839",
    "end": "842399"
  },
  {
    "text": "not very many people think this this would work well so I mean the question is why wouldn't it",
    "start": "842399",
    "end": "847519"
  },
  {
    "text": "work well um if you noticed you probably did that this world here is a entirely",
    "start": "847519",
    "end": "854160"
  },
  {
    "text": "featureless flat plane there's absolutely no obstacles no Dynamics in",
    "start": "854160",
    "end": "859880"
  },
  {
    "text": "this world whatsoever so the first time it encounters even a wall it's probably",
    "start": "859880",
    "end": "865160"
  },
  {
    "text": "not going to go so well so I can think of just a few adaptations that we want so if the agent",
    "start": "865160",
    "end": "873360"
  },
  {
    "text": "is uh facing a staircase ahead of it you don't want it to go as fast as it can you want it to actually lift its knees",
    "start": "873360",
    "end": "880920"
  },
  {
    "text": "if it's carrying your glass of water maybe it's going to be a little bit more careful or even needs to kind of",
    "start": "880920",
    "end": "886880"
  },
  {
    "text": "counterbalance the weight um if a predator appears I don't know anybody have a 2-year-old in their house you",
    "start": "886880",
    "end": "893839"
  },
  {
    "text": "might want the robot to run away from that so these are just a few adaptations",
    "start": "893839",
    "end": "899320"
  },
  {
    "text": "that I came up with off the top of my head and so the question is well how would we make all of that fit into the",
    "start": "899320",
    "end": "905920"
  },
  {
    "text": "reward function so that the agent does the right thing well one thing we could do is like add more inputs to the reward",
    "start": "905920",
    "end": "914560"
  },
  {
    "text": "function uh I don't exactly know how you would do this but let's say we could do it you would add terrain features you'd",
    "start": "914560",
    "end": "923079"
  },
  {
    "text": "add the other actors you would add um you know Predators right all this kind",
    "start": "923079",
    "end": "928360"
  },
  {
    "text": "of good stuff and depending on what immediate environment the agent is in there might",
    "start": "928360",
    "end": "934319"
  },
  {
    "text": "be more or fewer of these different features so that's a pretty high-dimensional problem so that's one",
    "start": "934319",
    "end": "941920"
  },
  {
    "text": "problem um the other problem is if we could come up with a hypothesis of what we think would generate the right",
    "start": "941920",
    "end": "947560"
  },
  {
    "text": "Behavior the only way to find out is through trial and error we need to train the agent from scratch which could take",
    "start": "947560",
    "end": "955279"
  },
  {
    "text": "hours hours or days even on many machines um so that's quite expensive and then",
    "start": "955279",
    "end": "962800"
  },
  {
    "text": "finally what if we get our agent into its real environment and we come up with",
    "start": "962800",
    "end": "969199"
  },
  {
    "text": "a particular context that we just didn't think of right it's a model models are imperfect and so I don't know about you",
    "start": "969199",
    "end": "975920"
  },
  {
    "text": "but to me this is starting to feel a lot like the early days of computer vision",
    "start": "975920",
    "end": "981519"
  },
  {
    "text": "where it was all about feature engineering that's a very laborious process and so I don't I don't want to",
    "start": "981519",
    "end": "988399"
  },
  {
    "text": "get into feature engineering my reward function now there is an active area of",
    "start": "988399",
    "end": "993880"
  },
  {
    "text": "research that's called inverse reinforcement learning and the aim of that is actually based on human",
    "start": "993880",
    "end": "1001720"
  },
  {
    "text": "demonstrations to infer what the reward function might be there's a lot of open",
    "start": "1001720",
    "end": "1007800"
  },
  {
    "text": "questions to that uh one of the questions is if you could do that in an",
    "start": "1007800",
    "end": "1013920"
  },
  {
    "text": "efficient manner how would you kind of get this notion of skills separated we need our",
    "start": "1013920",
    "end": "1021759"
  },
  {
    "text": "agent to be able to you know grasp different objects we need it to be able to navigate we need to be able to",
    "start": "1021759",
    "end": "1027720"
  },
  {
    "text": "locomote and do those things at the right time so assuming we got all of",
    "start": "1027720",
    "end": "1034400"
  },
  {
    "text": "these kind of skills learned how do you actually decide when to employ which skill I mean do we have one reward",
    "start": "1034400",
    "end": "1042199"
  },
  {
    "text": "function per skill the other problem is we're talking about neural networks that are trained",
    "start": "1042199",
    "end": "1048480"
  },
  {
    "text": "using back propagation and the way that back prop propagation works is you",
    "start": "1048480",
    "end": "1056240"
  },
  {
    "text": "propagate this error back through the network and tweak every single parameter by a small amount on each",
    "start": "1056240",
    "end": "1063720"
  },
  {
    "text": "trial which means you're forgetting something so if you're trying to learn a",
    "start": "1063720",
    "end": "1069240"
  },
  {
    "text": "new skill forgetting what you already knew is not a good strategy so human beings",
    "start": "1069240",
    "end": "1075679"
  },
  {
    "text": "and biological organisms in general we do something different we build structure in order to represent",
    "start": "1075679",
    "end": "1082120"
  },
  {
    "text": "new skills so I mean there there's still a number of unanswered questions",
    "start": "1082120",
    "end": "1088400"
  },
  {
    "text": "here okay here's another dimension to it which is this tradeoff between",
    "start": "1088400",
    "end": "1094120"
  },
  {
    "text": "exploration versus exploitation this is important exploring is necessary because",
    "start": "1094120",
    "end": "1101880"
  },
  {
    "text": "it helps gain information to improve our model which improves our ability to predict",
    "start": "1101880",
    "end": "1109280"
  },
  {
    "text": "reward in the future and so the information that you gain through exploration is known as epistemic",
    "start": "1109280",
    "end": "1117480"
  },
  {
    "text": "value and you need to do that at the right time the problem is that reward",
    "start": "1117480",
    "end": "1123760"
  },
  {
    "text": "functions don't include epistemic value so what do you do well you got to add",
    "start": "1123760",
    "end": "1130000"
  },
  {
    "text": "something on top of that so there's many methods one of the most popular ones",
    "start": "1130000",
    "end": "1137320"
  },
  {
    "text": "right now is called Epsilon greedy and this method essentially",
    "start": "1137320",
    "end": "1142720"
  },
  {
    "text": "entails coming up with a parameter Epsilon between 0 and one and that is the probability of performing a random",
    "start": "1142720",
    "end": "1150640"
  },
  {
    "text": "action on any given time step so this is from the Atari Deep Mind paper of 2015",
    "start": "1150640",
    "end": "1157200"
  },
  {
    "text": "the famous one where they solved the Atari video games and they chose to start Epsilon at one and then gradually",
    "start": "1157200",
    "end": "1163280"
  },
  {
    "text": "over a million time steps move it down to 0.1 so every 10 time steps you're exploring and then kind of level out",
    "start": "1163280",
    "end": "1170919"
  },
  {
    "text": "from there okay what's wrong with this again",
    "start": "1170919",
    "end": "1176760"
  },
  {
    "text": "we don't have inherent value in Exploration and the so and the problem is then that we don't get purposeful",
    "start": "1176760",
    "end": "1183520"
  },
  {
    "text": "exploration we get random exploration and why haven't we heard more about this",
    "start": "1183520",
    "end": "1188919"
  },
  {
    "text": "problem is that most of the experiments that you're seeing in reinforcement learning are happening in Virtual",
    "start": "1188919",
    "end": "1195559"
  },
  {
    "text": "environments in which it's completely acceptable to die millions of times or",
    "start": "1195559",
    "end": "1201280"
  },
  {
    "text": "even th thousands or millions of times um before learning something if you're talking about an agent operating in the",
    "start": "1201280",
    "end": "1208280"
  },
  {
    "text": "real world how acceptable is it to die many many times so while this might",
    "start": "1208280",
    "end": "1214000"
  },
  {
    "text": "converge it doesn't converge fast enough for real world situations so we might",
    "start": "1214000",
    "end": "1219240"
  },
  {
    "text": "get an agent that explores when it's about to die and that's not good the other problem though is that once you",
    "start": "1219240",
    "end": "1226640"
  },
  {
    "text": "deploy the agent into its environment it there's no reason for it to start",
    "start": "1226640",
    "end": "1232000"
  },
  {
    "text": "exploring again so if the world changes at this point it moves into a new environment how do you make it",
    "start": "1232000",
    "end": "1239679"
  },
  {
    "text": "explore okay so there's definitely some big open questions here and I want to",
    "start": "1239679",
    "end": "1246039"
  },
  {
    "text": "end this section with another question which is what if the agent generated its",
    "start": "1246039",
    "end": "1252000"
  },
  {
    "text": "own reward and what if in fact that reward included an imperative to explore this",
    "start": "1252000",
    "end": "1258840"
  },
  {
    "text": "epistemic value and that brings me to active inference so at this point we're",
    "start": "1258840",
    "end": "1264720"
  },
  {
    "text": "going to take a deep breath because we're going to forget",
    "start": "1264720",
    "end": "1271000"
  },
  {
    "text": "everything that we just learned about reinforcement learning um this is a different way of thinking about the",
    "start": "1271000",
    "end": "1276760"
  },
  {
    "text": "problem a paradigm shift but I think you'll find some familiar Concepts in here as",
    "start": "1276760",
    "end": "1281960"
  },
  {
    "text": "well so in order to introduce active inference I need to tell you a little bit about the free energy principle",
    "start": "1281960",
    "end": "1289320"
  },
  {
    "text": "and this is the work of Professor Carl friston out of University College London",
    "start": "1289320",
    "end": "1294679"
  },
  {
    "text": "and Carl friston is a rather famous neuroscientist who came up with this breakthrough method of statistical",
    "start": "1294679",
    "end": "1301480"
  },
  {
    "text": "mapping for fmis so he's not really well known in the machine Learning Community",
    "start": "1301480",
    "end": "1307200"
  },
  {
    "text": "um but he realized as he developed this method that he was actually describing an organizing principle for intelligent",
    "start": "1307200",
    "end": "1315440"
  },
  {
    "text": "life now that sounds pretty big doesn't it what does that actually mean so he's used this principle to describe many",
    "start": "1315440",
    "end": "1323520"
  },
  {
    "text": "emergent behaviors or phenomena in biological life and now he's taken that",
    "start": "1323520",
    "end": "1329360"
  },
  {
    "text": "and described in very precise mathematical detail this framework called active inference that would allow",
    "start": "1329360",
    "end": "1335919"
  },
  {
    "text": "you to implement an agent that minimizes this quantity called free energy and",
    "start": "1335919",
    "end": "1341080"
  },
  {
    "text": "we'll get to why that's important in a minute but first why am I excited about the free",
    "start": "1341080",
    "end": "1347799"
  },
  {
    "text": "energy principle the reason is that all of these behaviors that I described in the",
    "start": "1347799",
    "end": "1353520"
  },
  {
    "text": "beginning about what we want our robot to do",
    "start": "1353520",
    "end": "1358679"
  },
  {
    "text": "emerge by virtue of this principle the other thing is that it has",
    "start": "1358679",
    "end": "1364720"
  },
  {
    "text": "many ties to well-known Concepts so if you're familiar with you know basan",
    "start": "1364720",
    "end": "1370520"
  },
  {
    "text": "methods variational inference statistical physics information Theory",
    "start": "1370520",
    "end": "1376159"
  },
  {
    "text": "and in fact under certain assumptions the Markoff decision process which is",
    "start": "1376159",
    "end": "1382640"
  },
  {
    "text": "the underlying the process or description of reinforcement learning problems can be",
    "start": "1382640",
    "end": "1389200"
  },
  {
    "text": "as posed as a special case of active inference so it's pretty exciting let's",
    "start": "1389200",
    "end": "1395240"
  },
  {
    "text": "get into some intuition about how this principle works so you go to the store",
    "start": "1395240",
    "end": "1401600"
  },
  {
    "text": "and you get an egg it's just an ordinary unfertilized egg and you're hungry so",
    "start": "1401600",
    "end": "1407880"
  },
  {
    "text": "you get want some breakfast so you get out your spatula and you scramble the egg and during this time the egg goes",
    "start": "1407880",
    "end": "1414960"
  },
  {
    "text": "through this kind of irreversible transition from a state of wholeness to a state of",
    "start": "1414960",
    "end": "1421440"
  },
  {
    "text": "scrambled and this is kind of one way to describe the second love thermodynamics because the probability of the molecules",
    "start": "1421440",
    "end": "1428039"
  },
  {
    "text": "in this egg sort of spontaneously reforming to be this whole egg is infantes small and this is kind of how",
    "start": "1428039",
    "end": "1434880"
  },
  {
    "text": "we describe entropy or disorder increasing in the UN universe as time goes on okay so what if we had a special",
    "start": "1434880",
    "end": "1444080"
  },
  {
    "text": "egg and this egg had a face and it had a pair of legs and this time it sees the",
    "start": "1444080",
    "end": "1450400"
  },
  {
    "text": "spatula coming and it runs away so what did this egg do it avoided Crossing this",
    "start": "1450400",
    "end": "1457480"
  },
  {
    "text": "boundary here it stayed in this good state that is compatible with his own",
    "start": "1457480",
    "end": "1463159"
  },
  {
    "text": "behavior because it can not only sense but act upon the environment so to to",
    "start": "1463159",
    "end": "1469080"
  },
  {
    "text": "zoom out at the 30,000 foot view we have intelligent agents",
    "start": "1469080",
    "end": "1475120"
  },
  {
    "text": "maintaining very narrow sets of states to more or less at least temporarily",
    "start": "1475120",
    "end": "1481880"
  },
  {
    "text": "defy the second law of Thermodynamics defy all of these dispersion forces acting upon us right we we stay in a",
    "start": "1481880",
    "end": "1488840"
  },
  {
    "text": "warm room to stay warm right we eat food to keep our systems running nicely so",
    "start": "1488840",
    "end": "1495320"
  },
  {
    "text": "we're acting on the World to maintain this very small set of states and a very obvious kind of result of that statement",
    "start": "1495320",
    "end": "1503159"
  },
  {
    "text": "is that these states are preferred by the agent so one point about that which is",
    "start": "1503159",
    "end": "1511200"
  },
  {
    "text": "that preferred is in the eye of the beholder we have an environment here containing two agents one of which is in",
    "start": "1511200",
    "end": "1518840"
  },
  {
    "text": "a very very preferred State and the other of which is really in a dead state",
    "start": "1518840",
    "end": "1524720"
  },
  {
    "text": "so the the states that you would find fish and birds in are are quite different and when you really think",
    "start": "1524720",
    "end": "1531159"
  },
  {
    "text": "about that you start to wonder you know how much sense does it really make to",
    "start": "1531159",
    "end": "1537799"
  },
  {
    "text": "pose the problem in such a way that it requires a reward function coming from",
    "start": "1537799",
    "end": "1542919"
  },
  {
    "text": "the environment I mean isn't this more of a property of the agent",
    "start": "1542919",
    "end": "1548360"
  },
  {
    "text": "itself and that brings me to the free energy principle which is essentially inversion of what I just said states",
    "start": "1548360",
    "end": "1555720"
  },
  {
    "text": "that are incompatible with your survival are those very states that are surprising or",
    "start": "1555720",
    "end": "1562760"
  },
  {
    "text": "improbable surprising meaning you didn't expect them and so intelligent agents",
    "start": "1562760",
    "end": "1569679"
  },
  {
    "text": "need to minimize surprise and I mean a very specific thing when I say surprise you know what",
    "start": "1569679",
    "end": "1574960"
  },
  {
    "text": "does this actually mean if you're again if you're kind of familiar with um you know base theorem surprise is",
    "start": "1574960",
    "end": "1582240"
  },
  {
    "text": "essentially this curve this negative log probability of being in some state given",
    "start": "1582240",
    "end": "1588720"
  },
  {
    "text": "your model of the world and so if you're at probability zero that is you don't expect to be in the state your surprise",
    "start": "1588720",
    "end": "1595520"
  },
  {
    "text": "is essentially infinite whereas if you're at probability one like you believe the world is completely static",
    "start": "1595520",
    "end": "1601880"
  },
  {
    "text": "then your surprise is is zero the problem with surprise is we can't",
    "start": "1601880",
    "end": "1607480"
  },
  {
    "text": "actually measure it because it requires knowledge of hidden states of the world to which we don't have",
    "start": "1607480",
    "end": "1614000"
  },
  {
    "text": "access fortunately we can Define this quantity called free energy which is an upper Bound for surprise because it",
    "start": "1614000",
    "end": "1620840"
  },
  {
    "text": "includes this kind of Divergent term that's non- negative that is this",
    "start": "1620840",
    "end": "1627039"
  },
  {
    "text": "difference between what states you expect to be in and what your sensors are telling you you're in and so the the",
    "start": "1627039",
    "end": "1634720"
  },
  {
    "text": "the smaller you can make this Divergent Divergence the better you estimate your surprise the more intelligent you",
    "start": "1634720",
    "end": "1643679"
  },
  {
    "text": "become so I keep seeing using the word model over and over again because we all",
    "start": "1643679",
    "end": "1650840"
  },
  {
    "text": "need to build a model of the world in order to do this so this particular",
    "start": "1650840",
    "end": "1656440"
  },
  {
    "text": "formulation of free energy includes both a complexity term and an accuracy term",
    "start": "1656440",
    "end": "1664440"
  },
  {
    "text": "so we we minimize free energy not just on one level we actually minimize it on several levels we minimize it on our",
    "start": "1664440",
    "end": "1671159"
  },
  {
    "text": "perceptions of the world which are hypotheses about the world these can actually vary moment to moment we also",
    "start": "1671159",
    "end": "1678200"
  },
  {
    "text": "Al minimize the free energy on our policies these are paths through the states of the world this is the Dynamics",
    "start": "1678200",
    "end": "1684760"
  },
  {
    "text": "of the world we have a predictive model that predicts how the world evolves and",
    "start": "1684760",
    "end": "1690039"
  },
  {
    "text": "then finally we minimize free energy on the model itself on not only the",
    "start": "1690039",
    "end": "1695760"
  },
  {
    "text": "parameters of the model but the structures of the model as well and the best way I can probably think of to give",
    "start": "1695760",
    "end": "1701320"
  },
  {
    "text": "you an intuition about this is um raise your hand if you're familiar with the concept of overfitting in machine",
    "start": "1701320",
    "end": "1707880"
  },
  {
    "text": "learning learning okay a few people okay so the basic idea is when you're",
    "start": "1707880",
    "end": "1715120"
  },
  {
    "text": "training a neural network you're giving it a training data set and it's possible",
    "start": "1715120",
    "end": "1721200"
  },
  {
    "text": "if you give your neural network enough parameters to get 100% accuracy on that",
    "start": "1721200",
    "end": "1726760"
  },
  {
    "text": "training data set the problem is the resulting kind of nonlinear function",
    "start": "1726760",
    "end": "1733000"
  },
  {
    "text": "that your neural network is approximating is incredibly complex which means your model is not able to",
    "start": "1733000",
    "end": "1739679"
  },
  {
    "text": "generalize well if you give the model some new data it's never seen it's probably going to give you some answer",
    "start": "1739679",
    "end": "1746559"
  },
  {
    "text": "that doesn't make any sense so the the in machine learning what you would do is you would start to introduce",
    "start": "1746559",
    "end": "1753320"
  },
  {
    "text": "regularization and that more or less reduces the complexity of this nonlinear",
    "start": "1753320",
    "end": "1759559"
  },
  {
    "text": "function so when you're minimizing FR energy on a Model what you automatically get is kind of this Max maximization of",
    "start": "1759559",
    "end": "1766600"
  },
  {
    "text": "accuracy while minimizing your complexity of the model and that allows",
    "start": "1766600",
    "end": "1771960"
  },
  {
    "text": "you to generalize to be able to learn skills like holding different types of Cups separating the notion of what",
    "start": "1771960",
    "end": "1780600"
  },
  {
    "text": "versus where so let's go into active inference",
    "start": "1780600",
    "end": "1786399"
  },
  {
    "text": "and sort of compare that to um reinforcement learning so the idea is in active inference there is no such thing",
    "start": "1786399",
    "end": "1793039"
  },
  {
    "text": "as a reward function coming between the agent and the environment instead the reward",
    "start": "1793039",
    "end": "1798600"
  },
  {
    "text": "is intrinsic in this model by virtue of minimizing free energy and we can",
    "start": "1798600",
    "end": "1804960"
  },
  {
    "text": "actually Implement free energy minimization under a framework known as predictive coding and the currency of",
    "start": "1804960",
    "end": "1811840"
  },
  {
    "text": "that model is prediction error right so predictions come from above from high",
    "start": "1811840",
    "end": "1817399"
  },
  {
    "text": "level to low level they form your hypotheses or perceptions about the world and any kind of mismatch between",
    "start": "1817399",
    "end": "1824360"
  },
  {
    "text": "that and your sensory observations creates prediction error which you can think of as free energy so the goal is",
    "start": "1824360",
    "end": "1831120"
  },
  {
    "text": "minimize your prediction error okay so one thing that's really",
    "start": "1831120",
    "end": "1838600"
  },
  {
    "text": "fascinating about the act of predicting right this is a predictive hierarchical",
    "start": "1838600",
    "end": "1844720"
  },
  {
    "text": "model of the world you're trying to anticipate what your next sensation is going to be and prediction actually",
    "start": "1844720",
    "end": "1851679"
  },
  {
    "text": "underactive inference does two things the first thing is it's forming perceptions so if what you're seeing",
    "start": "1851679",
    "end": "1859679"
  },
  {
    "text": "doesn't match what you believe you're seeing you can adjust your beliefs or your perceptions about what you're",
    "start": "1859679",
    "end": "1865840"
  },
  {
    "text": "looking at but you can also given some prediction change the",
    "start": "1865840",
    "end": "1872600"
  },
  {
    "text": "world to make your prediction come true so when the predictions reach all the way down to the sensory level and in",
    "start": "1872600",
    "end": "1879360"
  },
  {
    "text": "particular I'm talking about these sensors called proceptive sensors these are attached to your actuators you can",
    "start": "1879360",
    "end": "1886159"
  },
  {
    "text": "actually make them come true so just by by way of example I see my finger out in front of my face and I predict I'm going",
    "start": "1886159",
    "end": "1892799"
  },
  {
    "text": "to sense that my arm is actually touching my finger here and my nervous system makes that true so this is a",
    "start": "1892799",
    "end": "1899639"
  },
  {
    "text": "pretty profound simplification of a number of problems",
    "start": "1899639",
    "end": "1904679"
  },
  {
    "text": "in reinforcement learning that I unfortunately don't have time to do justice to but I wanted to point it out",
    "start": "1904679",
    "end": "1909960"
  },
  {
    "text": "uh while we're here in case you're curious to learn more let's get back to",
    "start": "1909960",
    "end": "1915039"
  },
  {
    "text": "our robot okay so we first need to describe",
    "start": "1915039",
    "end": "1920399"
  },
  {
    "text": "what it is we want our robot to do we need to give it some kind of goals and",
    "start": "1920399",
    "end": "1925880"
  },
  {
    "text": "earlier I talked about this notion of States compatible with survival are those that are preferred so we need to",
    "start": "1925880",
    "end": "1932919"
  },
  {
    "text": "to to Define preferences there's a there's a couple places where you might get preferences",
    "start": "1932919",
    "end": "1938639"
  },
  {
    "text": "one is the experience of your ancestors what states were they in and you get",
    "start": "1938639",
    "end": "1944960"
  },
  {
    "text": "those passed down through your genome but another way you can get preferences",
    "start": "1944960",
    "end": "1950919"
  },
  {
    "text": "is through your experience as you experience life you start to prefer the",
    "start": "1950919",
    "end": "1957360"
  },
  {
    "text": "states that you most occupy but we're talking about an artificial agent so we",
    "start": "1957360",
    "end": "1962480"
  },
  {
    "text": "don't have to wait for evolution we can actually tell the agent what it prefers",
    "start": "1962480",
    "end": "1967720"
  },
  {
    "text": "when it's appropriate so we can say you prefer this High battery and so again",
    "start": "1967720",
    "end": "1972799"
  },
  {
    "text": "this is really a probability distribution it's not like a a single number but it's saying hey given uh my",
    "start": "1972799",
    "end": "1979600"
  },
  {
    "text": "model I prefer to be in this state and the the beauty of this formulation of",
    "start": "1979600",
    "end": "1989000"
  },
  {
    "text": "preferences is that it's completely independent of the environment that is",
    "start": "1989000",
    "end": "1994039"
  },
  {
    "text": "it doesn't say in this environment you prefer this in this environment you prefer that what it's saying is whatever",
    "start": "1994039",
    "end": "1999840"
  },
  {
    "text": "environment you're in figure out how to get there the other thing about preferences",
    "start": "1999840",
    "end": "2006279"
  },
  {
    "text": "is that they can be high dimensional battery level is one thing for an agent",
    "start": "2006279",
    "end": "2012080"
  },
  {
    "text": "but avoiding damage would be yet another level and keeping the human happy would be yet another you know you might have a",
    "start": "2012080",
    "end": "2018279"
  },
  {
    "text": "big red button that says hey I'm happy or I'm not happy right as giving giving feedback there so given that we have",
    "start": "2018279",
    "end": "2025600"
  },
  {
    "text": "this hierarchical predictive model of the world we can actually balance all",
    "start": "2025600",
    "end": "2031080"
  },
  {
    "text": "across all of these different goals or needs simultaneously right this this",
    "start": "2031080",
    "end": "2036559"
  },
  {
    "text": "model Works in par and when these guys are competing we can actually do kind of a cost sensitive",
    "start": "2036559",
    "end": "2043799"
  },
  {
    "text": "prioritization across these different preferences let's get back to this kind",
    "start": "2043799",
    "end": "2048919"
  },
  {
    "text": "of notion of finding a way getting from where you are to your goal um an agent",
    "start": "2048919",
    "end": "2055200"
  },
  {
    "text": "that that implements active inference has learns policies which are",
    "start": "2055200",
    "end": "2060280"
  },
  {
    "text": "essentially paths through the states of the world and it has a memory right so",
    "start": "2060280",
    "end": "2065320"
  },
  {
    "text": "it knows kind of here I am now here's where I was recently and here's where I expect to be and because where I expect",
    "start": "2065320",
    "end": "2073240"
  },
  {
    "text": "to be includes the goal that's a good thing let's look at a couple of scenarios for how this plays out and how",
    "start": "2073240",
    "end": "2079960"
  },
  {
    "text": "we get the behavior we're looking for here the first is overcoming adversity",
    "start": "2079960",
    "end": "2086000"
  },
  {
    "text": "so the robot's in the kitchen it knocks over the cup or maybe the cat knocked it over and it has to recover from that so",
    "start": "2086000",
    "end": "2093040"
  },
  {
    "text": "in this case let's say the robot was sort of chose a policy to get it to its goal which was make the human happy and",
    "start": "2093040",
    "end": "2100520"
  },
  {
    "text": "it it predicted it was going to be in the state where it was looking at its hand holding the cup but instead where",
    "start": "2100520",
    "end": "2107280"
  },
  {
    "text": "it found itself was actually the cup is knocked over so this formulation of free",
    "start": "2107280",
    "end": "2114920"
  },
  {
    "text": "energy this is this is free energy um minimized across a path includes this",
    "start": "2114920",
    "end": "2121599"
  },
  {
    "text": "kind of complexity term which becomes very high when the observations don't match what the policy predicts what",
    "start": "2121599",
    "end": "2128839"
  },
  {
    "text": "which means this policy that it was following becomes totally improbable it",
    "start": "2128839",
    "end": "2134839"
  },
  {
    "text": "stops performing that policy and instead chooses this other one which ex which",
    "start": "2134839",
    "end": "2140960"
  },
  {
    "text": "now aligns well with where it's at and the goal as well so let's talk about risk",
    "start": "2140960",
    "end": "2149560"
  },
  {
    "text": "sensitivity um the the previous slide I was talking about this quantity free energy which is kind of these paths",
    "start": "2149560",
    "end": "2155200"
  },
  {
    "text": "through the states and this one I'm I'm going to talk briefly about this quantity called expected free energy",
    "start": "2155200",
    "end": "2161760"
  },
  {
    "text": "which is very much related but it it is evaluated with respect to outcomes and",
    "start": "2161760",
    "end": "2167400"
  },
  {
    "text": "in particular these preferred outcomes so if there's sort of a high difference",
    "start": "2167400",
    "end": "2173040"
  },
  {
    "text": "between where the policy predicts the agent's going to go and where the preferences are it's going to say that's",
    "start": "2173040",
    "end": "2180200"
  },
  {
    "text": "too high of a cost that's too high of a risk I'm not going to do that so in this case there's two possible paths to the",
    "start": "2180200",
    "end": "2186960"
  },
  {
    "text": "goal one of which involves let's say climbing over an obstacle which means oh my God my battery is going to drain and",
    "start": "2186960",
    "end": "2193480"
  },
  {
    "text": "so this one has high expected free energy because this cost of the state is just so incredibly High whereas this",
    "start": "2193480",
    "end": "2200720"
  },
  {
    "text": "path involves going around and the cost is much lower so the agent chooses this",
    "start": "2200720",
    "end": "2207000"
  },
  {
    "text": "path instead okay and finally let's talk briefly about ambiguity",
    "start": "2207000",
    "end": "2213520"
  },
  {
    "text": "resolution if the agent is uncertain about the state of the World it can actually resolve that in a",
    "start": "2213520",
    "end": "2221599"
  },
  {
    "text": "very purposeful way so in this case the agent has reached the kitchen door in",
    "start": "2221599",
    "end": "2227720"
  },
  {
    "text": "trying to bring you the beverage but it doesn't yet know what's the state of the kitchen going to be is the cup going to",
    "start": "2227720",
    "end": "2234280"
  },
  {
    "text": "be on the counter or is the cup going to be not on the counter so if you look at",
    "start": "2234280",
    "end": "2240160"
  },
  {
    "text": "these states as a probability distribution maybe this would be 50% and maybe that would be 50% but the in terms",
    "start": "2240160",
    "end": "2246920"
  },
  {
    "text": "of the next step to resolve that ambiguity either way it needs to open the kitchen door so it's going to open",
    "start": "2246920",
    "end": "2252839"
  },
  {
    "text": "the kitchen door and then you'll either see close to a 100% this state or 100% that state and then the path it's going",
    "start": "2252839",
    "end": "2259520"
  },
  {
    "text": "to take will become very very clear so this expected ambiguity although it doesn't know any know where it's going",
    "start": "2259520",
    "end": "2265720"
  },
  {
    "text": "to be now is actually low because it expects I'm going to resolve my uncertainty about the World by opening",
    "start": "2265720",
    "end": "2271839"
  },
  {
    "text": "the kitchen door and then finally I just want to uh point out another another kind of",
    "start": "2271839",
    "end": "2278240"
  },
  {
    "text": "rearrangement of expected free energy if we take the the negative of that we get",
    "start": "2278240",
    "end": "2284040"
  },
  {
    "text": "value right this is this is how valuable is a particular policy and if we",
    "start": "2284040",
    "end": "2289240"
  },
  {
    "text": "rearrange it we exactly get extrinsic and behavior extrinsic",
    "start": "2289240",
    "end": "2295880"
  },
  {
    "text": "value and epistemic value so by Nature agents that minimize free energy",
    "start": "2295880",
    "end": "2304319"
  },
  {
    "text": "explore naturally and they balance these two things out out naturally so my conclusion here is that",
    "start": "2304319",
    "end": "2313240"
  },
  {
    "text": "if we Implement agents under active inference we're going to get these emerging behaviors that really describe",
    "start": "2313240",
    "end": "2319680"
  },
  {
    "text": "intelligent life but more importantly for us get us the robot Butlers that we want um and we also don't have to solve",
    "start": "2319680",
    "end": "2326960"
  },
  {
    "text": "all these problems around reward coming from the environment finally by Nature our agent",
    "start": "2326960",
    "end": "2333480"
  },
  {
    "text": "is going to be an Explorer so just to kind of wrap up why don't we have these robot Butlers in our",
    "start": "2333480",
    "end": "2339240"
  },
  {
    "text": "house I believe it's because we're investing most of our efforts toward a",
    "start": "2339240",
    "end": "2345200"
  },
  {
    "text": "framework that leaves a lot of open questions that doesn't allow these",
    "start": "2345200",
    "end": "2350400"
  },
  {
    "text": "behaviors to emerge and that would be reinforcement learning so I think that we need a new framework and I'm pretty",
    "start": "2350400",
    "end": "2357000"
  },
  {
    "text": "convinced at this point that active inference is that framework so I hope that I've made you",
    "start": "2357000",
    "end": "2363400"
  },
  {
    "text": "curious about active inference because I've been working on this problem for",
    "start": "2363400",
    "end": "2368520"
  },
  {
    "text": "the past couple years I'm really excited to continue and I think we need more Minds on it so I'd love to hear your",
    "start": "2368520",
    "end": "2374200"
  },
  {
    "text": "thoughts about this I'll be around so thank you very much and oh and finally",
    "start": "2374200",
    "end": "2380720"
  },
  {
    "text": "um I I mentioned a lot of you know sort of papers and blog posts and so forth so",
    "start": "2380720",
    "end": "2386400"
  },
  {
    "text": "if you go to my GitHub page here I put a whole bunch of links on there to give",
    "start": "2386400",
    "end": "2391960"
  },
  {
    "text": "you all the details surrounding this so thank you very much appreciate your time",
    "start": "2391960",
    "end": "2399319"
  }
]