[
  {
    "text": "hi everyone I'm very excited to be here in orus it's my uh second time",
    "start": "7319",
    "end": "12519"
  },
  {
    "text": "presenting here and uh last year when I was here I talked about Hadoop and the",
    "start": "12519",
    "end": "18000"
  },
  {
    "text": "Hadoop ecosystem and it spun up a lot of interesting questions so I guess I was",
    "start": "18000",
    "end": "24039"
  },
  {
    "text": "invited back to address some of them maybe um how many of you know Hadoop",
    "start": "24039",
    "end": "31439"
  },
  {
    "text": "okay how many of you have heard about Big Data that's a big difference already",
    "start": "31439",
    "end": "37960"
  },
  {
    "text": "from last year and the year before um so I guess you're here but why are you here",
    "start": "37960",
    "end": "44440"
  },
  {
    "text": "I'm I'm evand dren as Dean said I work for a company called claer uh it's most well known for its",
    "start": "44440",
    "end": "51840"
  },
  {
    "text": "distribution of Hadoop but I'm not here to talk about our product line I'm here to talk about big data and the landscape",
    "start": "51840",
    "end": "60239"
  },
  {
    "text": "um so why are you here well if you if you feel something like this cat a",
    "start": "60239",
    "end": "66320"
  },
  {
    "text": "little bit overwhelmed and confused by Big Data you know what it is where the",
    "start": "66320",
    "end": "71840"
  },
  {
    "text": "where all these technology belongs where it comes from uh you're in the right place this is a talk about uh a kind of",
    "start": "71840",
    "end": "79960"
  },
  {
    "text": "a historical perspective a 10,000 ft view of the landscape uh what the",
    "start": "79960",
    "end": "86320"
  },
  {
    "text": "drivers are to rethink data uh where Hado comes from the industry Trends who",
    "start": "86320",
    "end": "93159"
  },
  {
    "text": "are the vendors what pocket of Big Data uh do they land in and when to use which",
    "start": "93159",
    "end": "99880"
  },
  {
    "text": "tool those are the most common questions I get so I I I chose to focus the first",
    "start": "99880",
    "end": "105880"
  },
  {
    "text": "part on that second part will be more focused on bringing it down back to Earth again a very concrete small",
    "start": "105880",
    "end": "114040"
  },
  {
    "text": "example of Big Data uh trying to highlight the actual values Beyond just",
    "start": "114040",
    "end": "119920"
  },
  {
    "text": "large data size processing at scale uh really highlighting that there are other",
    "start": "119920",
    "end": "125200"
  },
  {
    "text": "aspects of Big Data as a concept as well after that a quick Q&A and a break after",
    "start": "125200",
    "end": "131560"
  },
  {
    "text": "which Dean Wampler will take the third part of this journey actually starting to digging under the surface of the",
    "start": "131560",
    "end": "138599"
  },
  {
    "text": "Earth into a deep dive around spark and also why SQL has this new uh fuss and",
    "start": "138599",
    "end": "147319"
  },
  {
    "text": "and uh interest around it so let's get started with uh a",
    "start": "147319",
    "end": "154840"
  },
  {
    "text": "perspective so what are the drivers in the Big Data space well um I'm sure",
    "start": "154840",
    "end": "161000"
  },
  {
    "text": "you've heard about it and are somewhat familiar but just to set the same foundation for the rest of the talk I'm",
    "start": "161000",
    "end": "167480"
  },
  {
    "text": "going to walk you through the four perspectives as I see has led to the",
    "start": "167480",
    "end": "173159"
  },
  {
    "text": "situation we're in today where we need to start rethink data management data",
    "start": "173159",
    "end": "178680"
  },
  {
    "text": "processing and data storage so first of all we",
    "start": "178680",
    "end": "184080"
  },
  {
    "text": "have moved to world where we do most of our business online all the transactions",
    "start": "184080",
    "end": "191200"
  },
  {
    "text": "all the purchases that we do has increased a lot over the last decade",
    "start": "191200",
    "end": "196760"
  },
  {
    "text": "right and with all these new businesses new Services there's a whole new set of data",
    "start": "196760",
    "end": "204080"
  },
  {
    "text": "types that are served and managed and processed and all all new sets of",
    "start": "204080",
    "end": "209280"
  },
  {
    "text": "Technology around serving these data to the consumers us so that cost us a lot of different",
    "start": "209280",
    "end": "216680"
  },
  {
    "text": "types of data the second driver is what we've seen",
    "start": "216680",
    "end": "223879"
  },
  {
    "text": "escalating there are no uh longer just mobile devices generating logs about you",
    "start": "223879",
    "end": "230080"
  },
  {
    "text": "know how the cell phone itself is doing or how your laptop is doing Etc there's",
    "start": "230080",
    "end": "235599"
  },
  {
    "text": "a whole new set of innovation in the device space it's your car your GPS uh",
    "start": "235599",
    "end": "243959"
  },
  {
    "text": "your your clothes your glasses your watches the things in your homes",
    "start": "243959",
    "end": "250799"
  },
  {
    "text": "starting to communicate sending back important information to the vendors to",
    "start": "250799",
    "end": "256519"
  },
  {
    "text": "the providers and it again it's not only about health of the device it's also",
    "start": "256519",
    "end": "263479"
  },
  {
    "text": "about okay when do you use it how often do you use it where are you when you use",
    "start": "263479",
    "end": "268759"
  },
  {
    "text": "it are you nearby something else connected in my product stack who are you interacting with what profiles are",
    "start": "268759",
    "end": "276000"
  },
  {
    "text": "you interacting with when you use my product all that information that",
    "start": "276000",
    "end": "281880"
  },
  {
    "text": "context around the customer is also generated by the devices",
    "start": "281880",
    "end": "287400"
  },
  {
    "text": "today the internet of things it's a it's a enormous set of",
    "start": "287400",
    "end": "293160"
  },
  {
    "text": "data that we are looking at coming towards us if we want to understand our users",
    "start": "293160",
    "end": "298639"
  },
  {
    "text": "more and that has exploded in innovation in the device world now third driver is",
    "start": "298639",
    "end": "306520"
  },
  {
    "text": "the not only business that we do online it's also our social lives we",
    "start": "306520",
    "end": "312759"
  },
  {
    "text": "interact with our friends we connect and and talk and plan our",
    "start": "312759",
    "end": "318039"
  },
  {
    "text": "careers uh we get entertained we plan our vacation you know all the talk about",
    "start": "318039",
    "end": "324000"
  },
  {
    "text": "where you go what you do it leaves a trace about the customer type you are",
    "start": "324000",
    "end": "330360"
  },
  {
    "text": "what you like to do what you prefer that movie you watched last Friday that you tweeted about suddenly we know something",
    "start": "330360",
    "end": "336639"
  },
  {
    "text": "about you we can profile you especially if we don't look at the individual but",
    "start": "336639",
    "end": "342840"
  },
  {
    "text": "we we aggregate tons of data about individuals to a profile of a market",
    "start": "342840",
    "end": "349400"
  },
  {
    "text": "segment so people who don't like this movie also drives trucks people who do",
    "start": "349400",
    "end": "355560"
  },
  {
    "text": "like this movie like to drive a minivan or something like that okay can we",
    "start": "355560",
    "end": "361800"
  },
  {
    "text": "correlate some marketing program where we present you know uh near the roots the routes that minan drivers often take",
    "start": "361800",
    "end": "369400"
  },
  {
    "text": "you know maybe we should Market those kind of movies on those kind of routes it it's more contextual about who you",
    "start": "369400",
    "end": "376880"
  },
  {
    "text": "are what you like and you leave traces everywhere whenever you leave uh you know a note on Facebook or Twitter or in",
    "start": "376880",
    "end": "383639"
  },
  {
    "text": "some Forum we know something more about you we're not even aware all the time",
    "start": "383639",
    "end": "389759"
  },
  {
    "text": "all the information we share and that's a big gold mine for Enterprises out there what if we can",
    "start": "389759",
    "end": "397080"
  },
  {
    "text": "find more information about our consumer markets to fine-tune our marketing to reach our customers in less intrusive",
    "start": "397080",
    "end": "404280"
  },
  {
    "text": "ways if you start complaining about things or or you know more timely manner if you want to actually know about a",
    "start": "404280",
    "end": "411280"
  },
  {
    "text": "special offer while driving on the way uh to work and you just need to pick up",
    "start": "411280",
    "end": "417080"
  },
  {
    "text": "something on the way and suddenly you know con convenience there's something there on",
    "start": "417080",
    "end": "422800"
  },
  {
    "text": "sale convenience and leading more business to your services all that Trace",
    "start": "422800",
    "end": "428960"
  },
  {
    "text": "around you on the web is generating a lot of information that isn't really structured it's not like a Tim stamp and",
    "start": "428960",
    "end": "436919"
  },
  {
    "text": "a value right it's actually some context that needs to be analyzed what you",
    "start": "436919",
    "end": "442240"
  },
  {
    "text": "actually think some sentiment to it the fourth driver is the technology itself",
    "start": "442240",
    "end": "448759"
  },
  {
    "text": "since all these Enterprises want to extract information from all these sets of massively generated data and variety",
    "start": "448759",
    "end": "456919"
  },
  {
    "text": "of data in the number of data types we need to serve Technology Innovation has been",
    "start": "456919",
    "end": "462759"
  },
  {
    "text": "driven around collecting data at speed how do we scale",
    "start": "462759",
    "end": "467840"
  },
  {
    "text": "collecting all these data information types and and sources and how do we also",
    "start": "467840",
    "end": "474159"
  },
  {
    "text": "process it in a timely manner and serve information how do we extract analyze the dat data extract information out of",
    "start": "474159",
    "end": "481919"
  },
  {
    "text": "it has spun off a trend in data processing technology",
    "start": "481919",
    "end": "487280"
  },
  {
    "text": "itself how do we store this and that has put pressure on",
    "start": "487280",
    "end": "493479"
  },
  {
    "text": "traditional systems traditional systems are very well optimized for what we built them",
    "start": "493479",
    "end": "499400"
  },
  {
    "text": "for they've been dominating the market for many decades and rightfully so they",
    "start": "499400",
    "end": "504759"
  },
  {
    "text": "have served us a good need right uh when you know the questions when you know",
    "start": "504759",
    "end": "510440"
  },
  {
    "text": "like okay what products sold this year you know you need to store some information about products about",
    "start": "510440",
    "end": "516320"
  },
  {
    "text": "transactions and then you do a bi report and you're done and you know something where to invest your money right but if",
    "start": "516320",
    "end": "522880"
  },
  {
    "text": "you also want to correlate it with new data like but what does my customer say about that product and how frequently do",
    "start": "522880",
    "end": "530160"
  },
  {
    "text": "they use it and in what Regional areas suddenly you might actually have to",
    "start": "530160",
    "end": "535560"
  },
  {
    "text": "collect other data sets and that becomes a challenge for your traditional systems because new data types and large data",
    "start": "535560",
    "end": "543200"
  },
  {
    "text": "sets to be correlated is a challenge it's not easy to scale uh when when your",
    "start": "543200",
    "end": "549760"
  },
  {
    "text": "data is 10x or or you know exponentially larger or when you have to handle new",
    "start": "549760",
    "end": "556160"
  },
  {
    "text": "data types that your original structure which is stored with the data in the database isn't modeled to handle that",
    "start": "556160",
    "end": "564120"
  },
  {
    "text": "new type of data coming in and remodeling data is also a costly business for the Enterprises who want to",
    "start": "564120",
    "end": "570800"
  },
  {
    "text": "extract new information from that gold mine and interestingly enough um I mean",
    "start": "570800",
    "end": "578240"
  },
  {
    "text": "as a round number 10 to 20% of our data is structured really structured the",
    "start": "578240",
    "end": "583920"
  },
  {
    "text": "other data out there being generated today think about the devices think about the social",
    "start": "583920",
    "end": "589399"
  },
  {
    "text": "lives it's unstructured or semi-structured it's not really modeled",
    "start": "589399",
    "end": "594880"
  },
  {
    "text": "into a predetermined world where you can model it into a specific structure and",
    "start": "594880",
    "end": "601079"
  },
  {
    "text": "then stick to that business question Forever After you can't change your mind you've logged into one model and will",
    "start": "601079",
    "end": "607480"
  },
  {
    "text": "experience a lot of cost if you have to move forward with new business needs new data types and unstructured or",
    "start": "607480",
    "end": "613480"
  },
  {
    "text": "semi-structured",
    "start": "613480",
    "end": "616320"
  },
  {
    "text": "data okay so this is a wise word from",
    "start": "620959",
    "end": "626200"
  },
  {
    "text": "one of my early managers in my career um a smart engineer comes up with a great",
    "start": "626200",
    "end": "632720"
  },
  {
    "text": "solution a wise engineer knows to Google it",
    "start": "632720",
    "end": "638000"
  },
  {
    "text": "first uh so where does Hadoop come from well it doesn't come just from random",
    "start": "638360",
    "end": "645320"
  },
  {
    "text": "stance um it actually what the industry did was to look at okay we see these",
    "start": "645320",
    "end": "650800"
  },
  {
    "text": "challenges coming let's look at a company that's already solved data scale",
    "start": "650800",
    "end": "656839"
  },
  {
    "text": "and how we can do it and processing data and handling all these unstructured",
    "start": "656839",
    "end": "661920"
  },
  {
    "text": "information or semi-structured data coming our way and a company who had already done",
    "start": "661920",
    "end": "667600"
  },
  {
    "text": "that was Google and they had for the last 10 years published papers",
    "start": "667600",
    "end": "673320"
  },
  {
    "text": "ideas that they've implemented somehow in house on how to store process and",
    "start": "673320",
    "end": "680120"
  },
  {
    "text": "access data at scale and handling multi types",
    "start": "680120",
    "end": "685519"
  },
  {
    "text": "right the first paper came out in 2003 this is a timeline of their paper",
    "start": "685519",
    "end": "692000"
  },
  {
    "text": "publishing um it was about Google file system a distributed uh fail safe",
    "start": "692000",
    "end": "698839"
  },
  {
    "text": "self-healing data storage system distributed across regular Hardware as",
    "start": "698839",
    "end": "705120"
  },
  {
    "text": "equally sized chunks that are replicated so if one you know diss go down the data",
    "start": "705120",
    "end": "711959"
  },
  {
    "text": "exists elsewhere and you can continue without interruption right A year later map ruce paper came",
    "start": "711959",
    "end": "718800"
  },
  {
    "text": "out and that was uh kind of a new way of",
    "start": "718800",
    "end": "725160"
  },
  {
    "text": "thinking about how to process data it was distributed it was a simple distributed API for read and wrs uh",
    "start": "725160",
    "end": "732920"
  },
  {
    "text": "around key value Pairs and it was mapped onto this distributed file system so you",
    "start": "732920",
    "end": "738680"
  },
  {
    "text": "could actually process data at scale since each data size data block was",
    "start": "738680",
    "end": "744560"
  },
  {
    "text": "equally sized you can process it in parallel and the processing of equally sized data would take the same time so",
    "start": "744560",
    "end": "751440"
  },
  {
    "text": "you can basically linearly scale out the processing of data two years later they",
    "start": "751440",
    "end": "756920"
  },
  {
    "text": "publish the big table paper that actually brought some interesting aspects to this stack uh by bringing",
    "start": "756920",
    "end": "765880"
  },
  {
    "text": "table likee semantics you can actually Access Data as if it was a",
    "start": "765880",
    "end": "773120"
  },
  {
    "text": "table so a key value column oriented um data store where you can get random",
    "start": "773120",
    "end": "781160"
  },
  {
    "text": "access to data sparsely populated data then it took some years and then a new",
    "start": "781160",
    "end": "787480"
  },
  {
    "text": "little series of of papers came out around more real time oriented access we",
    "start": "787480",
    "end": "793959"
  },
  {
    "text": "have a Dremel and tensing that brought SQL real SQL interfaces to the very same",
    "start": "793959",
    "end": "800399"
  },
  {
    "text": "data store and the percolator that brought real-time indexing so you can search over your data as",
    "start": "800399",
    "end": "806680"
  },
  {
    "text": "well and then more recently the spanner project which is really interesting so",
    "start": "806680",
    "end": "812160"
  },
  {
    "text": "why am I talking about Google aren't you here to learn about you know hop and and Big Data technology",
    "start": "812160",
    "end": "819480"
  },
  {
    "text": "well this is ideas someone with this need went and looked at these ideas and",
    "start": "819480",
    "end": "825519"
  },
  {
    "text": "then decided to make it accessible for for everybody since the the the data big",
    "start": "825519",
    "end": "832839"
  },
  {
    "text": "data problem is hitting everybody it's written on the wall it's coming our way why don't we make it open source and if",
    "start": "832839",
    "end": "839240"
  },
  {
    "text": "you map in the Hadoop timeline with uh Apache licensed projects uh popping up",
    "start": "839240",
    "end": "846519"
  },
  {
    "text": "it's very well timed I thought this was interesting so I'm sharing it with you it's very well timed with the Hadoop",
    "start": "846519",
    "end": "854240"
  },
  {
    "text": "ecosystem Hadoop came out um 2000 end of 2005 2006 and uh it's basically a",
    "start": "854240",
    "end": "863079"
  },
  {
    "text": "distributed file system with map reduce processing on it and then you have age BAS and and uh Impala and solar Cloud",
    "start": "863079",
    "end": "872160"
  },
  {
    "text": "move last year uh the realtime access Frameworks so you see it's a very good",
    "start": "872160",
    "end": "879680"
  },
  {
    "text": "mapping and it's very interesting where it comes",
    "start": "879680",
    "end": "884959"
  },
  {
    "text": "from so if we use the same Hadoop timeline and and start looking at the vendors in the space the ecosystem the",
    "start": "884959",
    "end": "892839"
  },
  {
    "text": "landscape um you see I think you can see um that it started around",
    "start": "892839",
    "end": "899720"
  },
  {
    "text": "maybe 2007 2008 and it has escalated you",
    "start": "899720",
    "end": "904800"
  },
  {
    "text": "see more and more vendors jumping into the pot trying to get a piece of this",
    "start": "904800",
    "end": "910839"
  },
  {
    "text": "and it's interesting because when big vendor names as you see up there who's joined this infrastructure technology",
    "start": "910839",
    "end": "919839"
  },
  {
    "text": "Evolution and change when you see them jump on board on something you know at",
    "start": "919839",
    "end": "925800"
  },
  {
    "text": "least in my mind I think you know that it's real it's it's not just a a",
    "start": "925800",
    "end": "932399"
  },
  {
    "text": "temporary Trend that will go away when they engage they actually validate that this is a disruptive technology and they",
    "start": "932399",
    "end": "940519"
  },
  {
    "text": "are planning to do something about it and just to explain the dotted lines",
    "start": "940519",
    "end": "946040"
  },
  {
    "text": "here um you see uh so green plum was acquired by EMC and then through a joint",
    "start": "946040",
    "end": "953639"
  },
  {
    "text": "partnership with VMware uh they turned into pivotal which is an independent",
    "start": "953639",
    "end": "960040"
  },
  {
    "text": "subsidiary uh you have Oracle they decided to join the race by adopting",
    "start": "960040",
    "end": "965199"
  },
  {
    "text": "Cloud era's distribution and embedded in their solution and then you have Microsoft who joined in a technical",
    "start": "965199",
    "end": "971920"
  },
  {
    "text": "partnership to uh move Hadoop to AER and then you have Intel who first",
    "start": "971920",
    "end": "979240"
  },
  {
    "text": "started with their own distribution but uh earlier this year decided to join",
    "start": "979240",
    "end": "984360"
  },
  {
    "text": "claar in a technical partnership and merge the distribution technology and",
    "start": "984360",
    "end": "989560"
  },
  {
    "text": "ideas together so maybe it's too early to say",
    "start": "989560",
    "end": "994600"
  },
  {
    "text": "but some solutions have started to consolidate but um we're definitely in",
    "start": "994600",
    "end": "1002519"
  },
  {
    "text": "in that exciting phase of a disruptive technology and the market has embraced",
    "start": "1002519",
    "end": "1007600"
  },
  {
    "text": "it it's not just a a interesting idea anymore it's actually real and and as",
    "start": "1007600",
    "end": "1015480"
  },
  {
    "text": "you can see if you take a wider view of the vendors in this place is not only Hadoop vendors right if you take a step",
    "start": "1015480",
    "end": "1022440"
  },
  {
    "text": "back and look at the wider Market of Big Data technology uh providers A lot of them base it on Open",
    "start": "1022440",
    "end": "1029640"
  },
  {
    "text": "Source Technologies it's very important that the core is open source so you don't miss out on any new innovation in",
    "start": "1029640",
    "end": "1036640"
  },
  {
    "text": "such a rapid disruptive market and Technology space but then you build value ad on top of it you have the",
    "start": "1036640",
    "end": "1044400"
  },
  {
    "text": "traditional databases I I I chose to keep them here",
    "start": "1044400",
    "end": "1049880"
  },
  {
    "text": "um because they're still very important in our data centers to handle uh well",
    "start": "1049880",
    "end": "1056200"
  },
  {
    "text": "optimized ecosystem around the more transactional workflows uh but then you see other",
    "start": "1056200",
    "end": "1062640"
  },
  {
    "text": "vendors in the more Cloud space where they provide this uh scalable data",
    "start": "1062640",
    "end": "1068080"
  },
  {
    "text": "processing as a service and cloud has actually taken off a little bit more uh",
    "start": "1068080",
    "end": "1074280"
  },
  {
    "text": "recent years in the data processing space as more and more Enterprises have",
    "start": "1074280",
    "end": "1079520"
  },
  {
    "text": "figured out what use cases to serve in the cloud and what to keep on Prem and",
    "start": "1079520",
    "end": "1084760"
  },
  {
    "text": "we see hybrid uh models growing in the Enterprise space where I I work with",
    "start": "1084760",
    "end": "1090159"
  },
  {
    "text": "many of the customers there and then we have and these two buckets are of course overlapping there",
    "start": "1090159",
    "end": "1097280"
  },
  {
    "text": "they're no straight lines it's kind of fuzzy we see analytics vendors in the operational space and we see operational",
    "start": "1097280",
    "end": "1104280"
  },
  {
    "text": "vendors in the analytic space so don't don't take this as the truth it's it's",
    "start": "1104280",
    "end": "1110159"
  },
  {
    "text": "one view of it to simplify the world and uh you see more operational",
    "start": "1110159",
    "end": "1117280"
  },
  {
    "text": "specialized Singletons uh in the in one bucket there while the distributed multi",
    "start": "1117280",
    "end": "1123720"
  },
  {
    "text": "multi-purpose uh Frameworks are are in the analytics bucket there and on top of",
    "start": "1123720",
    "end": "1129440"
  },
  {
    "text": "this we see a rapid movement of applications what do I mean with that",
    "start": "1129440",
    "end": "1136360"
  },
  {
    "text": "well traditional data applications such as um I don't know",
    "start": "1136360",
    "end": "1144520"
  },
  {
    "text": "SAS uh micro strategy Tableau Oracle",
    "start": "1144520",
    "end": "1149919"
  },
  {
    "text": "Microsoft are moving applications not only to process data and and give",
    "start": "1149919",
    "end": "1155080"
  },
  {
    "text": "application access to data in databases but also on top of these other",
    "start": "1155080",
    "end": "1161280"
  },
  {
    "text": "alternative backends today they're already moved to integrate the two and",
    "start": "1161280",
    "end": "1166960"
  },
  {
    "text": "allow end users the same familiar experience to handle their data but with",
    "start": "1166960",
    "end": "1172240"
  },
  {
    "text": "the benefits of these newer um Innovative data processing",
    "start": "1172240",
    "end": "1177880"
  },
  {
    "text": "systems and then you see a whole bunch of new companies popping up one example",
    "start": "1177880",
    "end": "1185200"
  },
  {
    "text": "is hexa dat which will actually give a talk tomorrow I think if the schedule",
    "start": "1185200",
    "end": "1190240"
  },
  {
    "text": "hasn't changed uh so you can go and learn how to do Predictive Analytics",
    "start": "1190240",
    "end": "1195640"
  },
  {
    "text": "using their framework on top of some of these backends and then you see other types of new",
    "start": "1195640",
    "end": "1202240"
  },
  {
    "text": "vendors such as Zoom data for instance which is a very interesting new way of",
    "start": "1202240",
    "end": "1208360"
  },
  {
    "text": "visualizing data where you can fast forward and fast backward and zoom into your different data use cases and sets",
    "start": "1208360",
    "end": "1216880"
  },
  {
    "text": "and see it visualized in many many different ways just uh you know beyond",
    "start": "1216880",
    "end": "1222080"
  },
  {
    "text": "bar charts and and regular timelines right and this is the space where I find",
    "start": "1222080",
    "end": "1229120"
  },
  {
    "text": "really interesting this is the space that will evolve next what kind of",
    "start": "1229120",
    "end": "1234600"
  },
  {
    "text": "applications can be built next to combine the values of these new backends",
    "start": "1234600",
    "end": "1239880"
  },
  {
    "text": "that can handle the challenges of Big Data what is the next application out",
    "start": "1239880",
    "end": "1245919"
  },
  {
    "text": "there and we see a lot of startups I I live in Silicon Valley and just by",
    "start": "1245919",
    "end": "1251880"
  },
  {
    "text": "osmosis you get connected with a lot of startups and Venture capitalists and where they are looking right now is in",
    "start": "1251880",
    "end": "1259000"
  },
  {
    "text": "this layer they've already moved past the infrastructure layer there's enough",
    "start": "1259000",
    "end": "1264559"
  },
  {
    "text": "movement and vendors well there's never enough but the interest has moved one",
    "start": "1264559",
    "end": "1269799"
  },
  {
    "text": "step above that's where the venture capital of funds are are looking to find the next app what's the new way we need",
    "start": "1269799",
    "end": "1277480"
  },
  {
    "text": "to process and access data on top of these new established now established",
    "start": "1277480",
    "end": "1285000"
  },
  {
    "text": "backends that's where things are happening and of course spoiler spoiler alert um",
    "start": "1285000",
    "end": "1294279"
  },
  {
    "text": "you know I thought I was really smart coming up with my own bucketing around vendors and trying to give some clarity",
    "start": "1294279",
    "end": "1300799"
  },
  {
    "text": "of the landscape and then I Googled it and I found this much better chart so",
    "start": "1300799",
    "end": "1306440"
  },
  {
    "text": "wise engineer might have found this um and this is thanks to Matt Turk and and",
    "start": "1306440",
    "end": "1312240"
  },
  {
    "text": "shiven zillis um I found it online it's much more granular sorting of the",
    "start": "1312240",
    "end": "1318880"
  },
  {
    "text": "vendors in the space some might actually have disappeared it's a very changing",
    "start": "1318880",
    "end": "1324240"
  },
  {
    "text": "space so apologies for that uh but I think it's kind of giving a better view",
    "start": "1324240",
    "end": "1329840"
  },
  {
    "text": "of how much activity there is in this space today it's not new",
    "start": "1329840",
    "end": "1336480"
  },
  {
    "text": "anymore it's actually flourishing right and I'm leaving it here for your",
    "start": "1336480",
    "end": "1342840"
  },
  {
    "text": "records it's kind of an ey chart so don't don't bother reading it now and then as a last point to this um",
    "start": "1342840",
    "end": "1351919"
  },
  {
    "text": "it's not only me standing here saying this it's actually you know where many",
    "start": "1351919",
    "end": "1357600"
  },
  {
    "text": "industry uh decision makers cios go is to look at these analyst reports and",
    "start": "1357600",
    "end": "1364240"
  },
  {
    "text": "understand you know what what does everybody else do an analysts talk with everybody else",
    "start": "1364240",
    "end": "1370840"
  },
  {
    "text": "right so what happened this year and I think this is a very interesting",
    "start": "1370840",
    "end": "1376200"
  },
  {
    "text": "Milestone is that three Hadoop or um you know new data management vendors popped",
    "start": "1376200",
    "end": "1386200"
  },
  {
    "text": "up in the traditional chart over uh data management and and data Enterprise uh",
    "start": "1386200",
    "end": "1394120"
  },
  {
    "text": "Warehouse technology this is the Gartner report uh to the right containing three",
    "start": "1394120",
    "end": "1399799"
  },
  {
    "text": "of those vendors it's also kind of a ey chart I didn't mean to point in Market",
    "start": "1399799",
    "end": "1405080"
  },
  {
    "text": "three particular vendors the point is they are there this year they weren't last",
    "start": "1405080",
    "end": "1410720"
  },
  {
    "text": "year so it's kind of a validation",
    "start": "1410720",
    "end": "1415240"
  },
  {
    "text": "Point all right we've talked about the technology",
    "start": "1416120",
    "end": "1421919"
  },
  {
    "text": "where it comes from where it is uh how the vendor spaces happening and are here",
    "start": "1421919",
    "end": "1428039"
  },
  {
    "text": "to stay um but it is interesting to reflect on the architectural effects of this big data",
    "start": "1428039",
    "end": "1436080"
  },
  {
    "text": "movement as well and where companies are looking to",
    "start": "1436080",
    "end": "1444799"
  },
  {
    "text": "go is they have started talking I mean I sit in discussions with you know the",
    "start": "1444880",
    "end": "1450720"
  },
  {
    "text": "global uh 500 Fortune 500 companies and talk with their strategy teams they're",
    "start": "1450720",
    "end": "1456880"
  },
  {
    "text": "it planning for the next two to five years and I I'm I'm fascinated by their",
    "start": "1456880",
    "end": "1464520"
  },
  {
    "text": "conversations and their insights but one thing that's common across verticals is",
    "start": "1464520",
    "end": "1470760"
  },
  {
    "text": "that they've started talking about information driven on every level in the company",
    "start": "1470760",
    "end": "1478080"
  },
  {
    "text": "what does this mean well every role every organization every team needs",
    "start": "1478080",
    "end": "1484240"
  },
  {
    "text": "access to data and to be truly flexible to new needs in the market you need to",
    "start": "1484240",
    "end": "1490240"
  },
  {
    "text": "make sure that every decis decision is based on data so you make the right",
    "start": "1490240",
    "end": "1495480"
  },
  {
    "text": "decision but that all the data that everybody in your organization has access to is the same then you can be",
    "start": "1495480",
    "end": "1502480"
  },
  {
    "text": "truly independent of each other right so if if organization a has the same data",
    "start": "1502480",
    "end": "1509559"
  },
  {
    "text": "as organization B you can make the same conclusions if you are data",
    "start": "1509559",
    "end": "1515880"
  },
  {
    "text": "driven so to become data driven information",
    "start": "1515880",
    "end": "1521480"
  },
  {
    "text": "driven it's kind of challenging in the Enterprise architecture deployments",
    "start": "1521480",
    "end": "1527360"
  },
  {
    "text": "today this is what what we see when we go in there it's a lot of different",
    "start": "1527360",
    "end": "1533399"
  },
  {
    "text": "backends where data lives in multiple copies or you're moving data around and transform it on the way it's very",
    "start": "1533399",
    "end": "1540320"
  },
  {
    "text": "complex and it's no judgment it's what's happened over the years as companies grow new use cases come in instead of",
    "start": "1540320",
    "end": "1548799"
  },
  {
    "text": "remodeling some old system that nobody wants to touch because the guy who wrote it has already left or the G who wrote",
    "start": "1548799",
    "end": "1555240"
  },
  {
    "text": "it has moved on in the organization you know you add a new system to serve new use cases or new",
    "start": "1555240",
    "end": "1563360"
  },
  {
    "text": "business questions needed to be asked you add new systems that are specialized for that kind of use case right and over",
    "start": "1563360",
    "end": "1571279"
  },
  {
    "text": "time through acquisition or other growth factors your data center has come to look like this where you have to serve a",
    "start": "1571279",
    "end": "1579039"
  },
  {
    "text": "multitude of audiences of this very heterogenous environment how do you do that cost efficiently right how do you",
    "start": "1579039",
    "end": "1585880"
  },
  {
    "text": "do that in a speedy timely manner also when your data volumes coming in might be growing forward",
    "start": "1585880",
    "end": "1593000"
  },
  {
    "text": "right well the concept the category it's an",
    "start": "1593000",
    "end": "1598919"
  },
  {
    "text": "architecture it's a it's like a Enterprise data warehouse or a data Mark",
    "start": "1598919",
    "end": "1604600"
  },
  {
    "text": "that kind of category that has emerged is called the Enterprise data Hub it's a",
    "start": "1604600",
    "end": "1611240"
  },
  {
    "text": "place where all your data can land so everybody has access to the same",
    "start": "1611240",
    "end": "1616760"
  },
  {
    "text": "truth and then you serve it either natively from that data Hub to a variety",
    "start": "1616760",
    "end": "1621960"
  },
  {
    "text": "of audiences using different tools that can operate integrated with the Hub or",
    "start": "1621960",
    "end": "1627960"
  },
  {
    "text": "you use the Hub to pre-process it and serve the results through your optimized",
    "start": "1627960",
    "end": "1633440"
  },
  {
    "text": "systems and then of course not all use cases can move immediately over to",
    "start": "1633440",
    "end": "1638720"
  },
  {
    "text": "Enterprise data Hub you can use these systems that are optimized for",
    "start": "1638720",
    "end": "1644360"
  },
  {
    "text": "transactional workload still for that but you can open up space on them enabling more volume to actually be more",
    "start": "1644360",
    "end": "1653360"
  },
  {
    "text": "served from the data Hub so this is a new category that has",
    "start": "1653360",
    "end": "1660880"
  },
  {
    "text": "come from the same drivers Trends and Technology needs um that we've just",
    "start": "1660880",
    "end": "1668720"
  },
  {
    "text": "discussed and another way of looking at it is of course uh the real architecture of the Enterprise data Hub is actually a",
    "start": "1668720",
    "end": "1675600"
  },
  {
    "text": "file system or a way to access that data as a kind of database like system you",
    "start": "1675600",
    "end": "1683799"
  },
  {
    "text": "have a shared Resource Management you have a shared security model you have",
    "start": "1683799",
    "end": "1689519"
  },
  {
    "text": "governance and data inest and management you have production visibility you have",
    "start": "1689519",
    "end": "1694880"
  },
  {
    "text": "all that those requirements have not gone away they are still needed just",
    "start": "1694880",
    "end": "1699919"
  },
  {
    "text": "like for other data management systems but what's different is that different Frameworks interoperate directly with",
    "start": "1699919",
    "end": "1707320"
  },
  {
    "text": "this shared data store right bring workloads natively and instead of moving",
    "start": "1707320",
    "end": "1714399"
  },
  {
    "text": "data around to different audiences you can just choose a framework to serve a particular",
    "start": "1714399",
    "end": "1721480"
  },
  {
    "text": "audience so now we're starting the landing phase here we're coming back to Earth so",
    "start": "1722279",
    "end": "1729519"
  },
  {
    "text": "buckle up your seat belts because this is the last face of the 10,000 ft view I",
    "start": "1729519",
    "end": "1736679"
  },
  {
    "text": "get the question of what what should I use I mean Hadoop is a wide ecosystem of weirdly named projects and I I don't",
    "start": "1736679",
    "end": "1744360"
  },
  {
    "text": "know what to do with which well this is my very simple reference",
    "start": "1744360",
    "end": "1749840"
  },
  {
    "text": "guide of what to use for what so if you have the same data store",
    "start": "1749840",
    "end": "1757640"
  },
  {
    "text": "you can choose from a variety of Frameworks to process and serve your data if you have real time query",
    "start": "1757640",
    "end": "1765039"
  },
  {
    "text": "needs if you want to do bi reports the ones you already do or new ones as new",
    "start": "1765039",
    "end": "1770559"
  },
  {
    "text": "needs come in over combined data sets or larger data",
    "start": "1770559",
    "end": "1776000"
  },
  {
    "text": "sets then a real-time query engine is what you need and one example",
    "start": "1776000",
    "end": "1783399"
  },
  {
    "text": "is Impala right if you rather have some long",
    "start": "1783399",
    "end": "1789360"
  },
  {
    "text": "running ETL workload where you want to started and it should run overnight it",
    "start": "1789360",
    "end": "1794399"
  },
  {
    "text": "should be very very fail safe fail over all interm mediate data set should be",
    "start": "1794399",
    "end": "1799640"
  },
  {
    "text": "written to dis and persisted Etc you need a more you know batch oriented",
    "start": "1799640",
    "end": "1805919"
  },
  {
    "text": "workload and pig and Hive are excellent tools for that but Pig and Hive are not really",
    "start": "1805919",
    "end": "1812640"
  },
  {
    "text": "good when you want interactive quick speed of thought analytical queries",
    "start": "1812640",
    "end": "1819200"
  },
  {
    "text": "because then you have to sit and wait for hours for that you know vast framework uh map ruce to spin up and",
    "start": "1819200",
    "end": "1825720"
  },
  {
    "text": "process data and you know aggregate map produce is the foundation of hi and",
    "start": "1825720",
    "end": "1833200"
  },
  {
    "text": "pig if you have another type of query use case where it's more about fuzzy",
    "start": "1833200",
    "end": "1839600"
  },
  {
    "text": "matching or random uh terms and and your SQL queries",
    "start": "1839600",
    "end": "1845200"
  },
  {
    "text": "for it starts looking like a mess with uh you know 25 to 100 different like uh",
    "start": "1845200",
    "end": "1851640"
  },
  {
    "text": "function calls then you might want to consider to use a search engine instead",
    "start": "1851640",
    "end": "1857679"
  },
  {
    "text": "of use an indexing engine to process your data and serve it so that you can",
    "start": "1857679",
    "end": "1863519"
  },
  {
    "text": "do phonetic search or dictionary search or shape based search there's a variety",
    "start": "1863519",
    "end": "1870000"
  },
  {
    "text": "of powerful tools uh just within a search engine that might be tricky to",
    "start": "1870000",
    "end": "1875519"
  },
  {
    "text": "accomplish in a traditional SQL query engine if you want to serve a web",
    "start": "1875519",
    "end": "1882840"
  },
  {
    "text": "service in real time and do some quick analytics over um you know sparsely",
    "start": "1882840",
    "end": "1889399"
  },
  {
    "text": "populated data let's say you have a website and while a session is going on",
    "start": "1889399",
    "end": "1894880"
  },
  {
    "text": "you want to serve ads or suggestions of what else to look at while the customer",
    "start": "1894880",
    "end": "1900600"
  },
  {
    "text": "is still walking through your website you can um do analytics over those web",
    "start": "1900600",
    "end": "1906919"
  },
  {
    "text": "click streams and profile other users that have looked at the same items using",
    "start": "1906919",
    "end": "1912760"
  },
  {
    "text": "a data store like hbase it can serve in real time",
    "start": "1912760",
    "end": "1917960"
  },
  {
    "text": "suggestions and you can we have many many uh use cases where recommendation engines or ad engines are built on top",
    "start": "1917960",
    "end": "1925360"
  },
  {
    "text": "of HB right and then I'm not going to go in",
    "start": "1925360",
    "end": "1932480"
  },
  {
    "text": "deeply to spark since we're going to have a deep dive on it next session uh but if you want to",
    "start": "1932480",
    "end": "1940080"
  },
  {
    "text": "implement some kind of analytics be it graph uh strength or some machine",
    "start": "1940080",
    "end": "1946120"
  },
  {
    "text": "learning or some other kind of pattern extraction and the data set",
    "start": "1946120",
    "end": "1951919"
  },
  {
    "text": "is fairly neat it can actually fit into memory uh into the ram of those machines",
    "start": "1951919",
    "end": "1959679"
  },
  {
    "text": "you can access for this use case then spark is a very very efficient realtime",
    "start": "1959679",
    "end": "1965519"
  },
  {
    "text": "analytics framework and if you have realtime",
    "start": "1965519",
    "end": "1970600"
  },
  {
    "text": "streaming data there's a spark streaming that is evolving to handle those kind of",
    "start": "1970600",
    "end": "1976360"
  },
  {
    "text": "event processing uh workloads as well but if you have that again",
    "start": "1976360",
    "end": "1983880"
  },
  {
    "text": "ETL long-term running you want to persist every step the data set might be",
    "start": "1983880",
    "end": "1990279"
  },
  {
    "text": "you know one petabyte you want to do some analytics over that maybe a",
    "start": "1990279",
    "end": "1995320"
  },
  {
    "text": "framework like map ruce would be better in that case so this is the quick reference guide you know just to give an",
    "start": "1995320",
    "end": "2002760"
  },
  {
    "text": "idea what the tools are there for and again think about that visual of the",
    "start": "2002760",
    "end": "2008120"
  },
  {
    "text": "Enterprise data Hub where all these workloads can actually work against the same data store and fully integrated",
    "start": "2008120",
    "end": "2015200"
  },
  {
    "text": "with each other so we're at the last part and this",
    "start": "2015200",
    "end": "2022639"
  },
  {
    "text": "is a simple example we're going to walk through big data values um exemplified through a data",
    "start": "2022639",
    "end": "2031760"
  },
  {
    "text": "corporation that I made up um it's a product provider and it's mediumsized it has",
    "start": "2031760",
    "end": "2039840"
  },
  {
    "text": "most Revenue coming in online and there is a lot of customer transactions and",
    "start": "2039840",
    "end": "2045880"
  },
  {
    "text": "they store in a traditional relational database and they kind of think like yeah business as usual um we don't",
    "start": "2045880",
    "end": "2053638"
  },
  {
    "text": "really have any challenges other than the market might get more uh more and more crowded how we how do we drive new",
    "start": "2053639",
    "end": "2061240"
  },
  {
    "text": "traffic to our uh products and our website and it sounds like pretty much",
    "start": "2061240",
    "end": "2066320"
  },
  {
    "text": "any company right and the head of it at this company says",
    "start": "2066320",
    "end": "2074440"
  },
  {
    "text": "this I only have like 100 gigabytes I don't have a big data",
    "start": "2074440",
    "end": "2079480"
  },
  {
    "text": "problem I hear this a lot and the examples I'm",
    "start": "2079480",
    "end": "2085560"
  },
  {
    "text": "doing is yeah of course if you have big data if you have large sizes of data the",
    "start": "2085560",
    "end": "2091839"
  },
  {
    "text": "the platform and the Technologies and the architectures I've been talking about can address that but I'm I'm going",
    "start": "2091839",
    "end": "2097880"
  },
  {
    "text": "to take a different Spin and highlight some other aspects of Big Data through",
    "start": "2097880",
    "end": "2103000"
  },
  {
    "text": "my examples so pretend you work for this head of it and you're",
    "start": "2103000",
    "end": "2110040"
  },
  {
    "text": "like okay I'm pretty smart or wise your your",
    "start": "2110040",
    "end": "2115160"
  },
  {
    "text": "preference uh and you have a 10 no CDH cluster running CDH is cloud",
    "start": "2115160",
    "end": "2120359"
  },
  {
    "text": "distribution over hiop in Amazon just for the fun of",
    "start": "2120359",
    "end": "2125760"
  },
  {
    "text": "it the first step you would do is to try something you already know just to",
    "start": "2125760",
    "end": "2131839"
  },
  {
    "text": "familiarize yourself with a new environment a new platform and prove that you can do what your managers or",
    "start": "2131839",
    "end": "2138720"
  },
  {
    "text": "this head of it actually expects you to do so that's what we're going to do we're going to do the same thing same",
    "start": "2138720",
    "end": "2145560"
  },
  {
    "text": "product reports calculating the top most sold products but in",
    "start": "2145560",
    "end": "2151720"
  },
  {
    "text": "Hadoop so the approach we would take would be load the pro product sales data",
    "start": "2151720",
    "end": "2156920"
  },
  {
    "text": "into Hadoop um and we're going to use scoop for that because scoop is a nice tool for",
    "start": "2156920",
    "end": "2162319"
  },
  {
    "text": "structure data transport and we're going to convert it to Avro just for the fun of it and to",
    "start": "2162319",
    "end": "2169680"
  },
  {
    "text": "prepare it for other workflows that might be using the same data forward you don't know but a is an optimized file",
    "start": "2169680",
    "end": "2177079"
  },
  {
    "text": "format that uh takes away the complexity of having to serialize and the serialized data uh for map reduce for",
    "start": "2177079",
    "end": "2185040"
  },
  {
    "text": "example so we're just going to do that to spice up our example and then we're going to use Hive",
    "start": "2185040",
    "end": "2192119"
  },
  {
    "text": "to create tables to serve the questions at hand the tables might be different than",
    "start": "2192119",
    "end": "2198640"
  },
  {
    "text": "the tables you have but you know in this first use case they're probably much the same and then we're going to use Impala",
    "start": "2198640",
    "end": "2206160"
  },
  {
    "text": "to query because we don't want to sit at our computer and wait forever so same use case different",
    "start": "2206160",
    "end": "2213079"
  },
  {
    "text": "platform and here are some code examples and it's just examples uh the first one",
    "start": "2213079",
    "end": "2218599"
  },
  {
    "text": "is to show that scoop is a command line interface you uh launch scoop and you",
    "start": "2218599",
    "end": "2225319"
  },
  {
    "text": "put some parameters in there that transports your tables to htfs and then",
    "start": "2225319",
    "end": "2231040"
  },
  {
    "text": "there are two commands exampling how to look at your tables and",
    "start": "2231040",
    "end": "2236319"
  },
  {
    "text": "also how to look at the data that serves a particular table in our case a bunch of a files",
    "start": "2236319",
    "end": "2243440"
  },
  {
    "text": "right so this is how hard it is to get dat data into hop using",
    "start": "2243440",
    "end": "2251400"
  },
  {
    "text": "scoop step two create tables uh we launch the hi command line shell if we",
    "start": "2251400",
    "end": "2258240"
  },
  {
    "text": "want and we create some tables and I can't show all the tables you need for this example that would take too much",
    "start": "2258240",
    "end": "2264920"
  },
  {
    "text": "time in my slides but it's an example to Showcase that you can use the same SQL",
    "start": "2264920",
    "end": "2271240"
  },
  {
    "text": "language you're familiar with to create the table but it happens to be a hive command shell once you have your tables",
    "start": "2271240",
    "end": "2277880"
  },
  {
    "text": "that you want to do uh the query over and note here the tables live separate from your data your data is ingested",
    "start": "2277880",
    "end": "2285720"
  },
  {
    "text": "tables is just a structure for that particular use case you're looking at at that particular time structure separate",
    "start": "2285720",
    "end": "2293480"
  },
  {
    "text": "from data makes it very flexible and we'll see that in the next",
    "start": "2293480",
    "end": "2298520"
  },
  {
    "text": "example so we can use Hue which is a UI part of",
    "start": "2299200",
    "end": "2305359"
  },
  {
    "text": "the drro and you type your query in there on your tables in the hive query",
    "start": "2305359",
    "end": "2312960"
  },
  {
    "text": "sorry in the Impala query application because we didn't want to sit there forever and you get your results set",
    "start": "2312960",
    "end": "2318359"
  },
  {
    "text": "back and what we see is like okay it seems to be sports goods that are the top 10 most selling",
    "start": "2318359",
    "end": "2324800"
  },
  {
    "text": "products and it's all good and we're done so that wasn't hard the point here",
    "start": "2324800",
    "end": "2330839"
  },
  {
    "text": "you can do the same thing but in this new platform and it's not hard and then keep in mind you can do it",
    "start": "2330839",
    "end": "2338400"
  },
  {
    "text": "over larger sets of data as well the second step is a new angle of",
    "start": "2338400",
    "end": "2344240"
  },
  {
    "text": "big data that I hope to share some light over what if we have the same business",
    "start": "2344240",
    "end": "2351200"
  },
  {
    "text": "question what product should we invest in but use a different data set to",
    "start": "2351200",
    "end": "2357400"
  },
  {
    "text": "answer that question really getting big data value",
    "start": "2357400",
    "end": "2364000"
  },
  {
    "text": "from correlating data sets that's another angle of big data so",
    "start": "2364000",
    "end": "2369920"
  },
  {
    "text": "what we do is lo load some web log data we want to see what people look at while",
    "start": "2369920",
    "end": "2375599"
  },
  {
    "text": "they're on our website and you know see if it correlates with what we actually",
    "start": "2375599",
    "end": "2380920"
  },
  {
    "text": "sell so we're going to use um Flume a",
    "start": "2380920",
    "end": "2386119"
  },
  {
    "text": "realtime injust framework that subscribes to events and posts to Hadoop",
    "start": "2386119",
    "end": "2391880"
  },
  {
    "text": "uh to ingest these web click streams in in uh in real time and and then we're",
    "start": "2391880",
    "end": "2397640"
  },
  {
    "text": "going to use Hive to create tables over that semi structure data Hive is the",
    "start": "2397640",
    "end": "2403400"
  },
  {
    "text": "same tool though we just saw how it was used and then we're going to use Hue and Impala to query that same way we did",
    "start": "2403400",
    "end": "2410880"
  },
  {
    "text": "different kind of data set though",
    "start": "2410880",
    "end": "2415160"
  },
  {
    "text": "right so Flume it's a pubsub ingestion framework it is very flexible you can um",
    "start": "2416079",
    "end": "2424040"
  },
  {
    "text": "deploy it as a hierarchy where you do mini transf formations on the way if you will you want to Route certain parts of",
    "start": "2424040",
    "end": "2430599"
  },
  {
    "text": "the data somewhere while other parts somewhere else um it basically has a source that you configure to consume uh",
    "start": "2430599",
    "end": "2438480"
  },
  {
    "text": "the events be CIS log tweets whatever real time generated event kind of data",
    "start": "2438480",
    "end": "2444079"
  },
  {
    "text": "that that you have in our case the web click streams and then you do some",
    "start": "2444079",
    "end": "2449280"
  },
  {
    "text": "optional logic if you want to cleanse it or whatever you don't have to it's optional and then you publish it either",
    "start": "2449280",
    "end": "2455720"
  },
  {
    "text": "to Hadoop in our case we or somewhere",
    "start": "2455720",
    "end": "2460599"
  },
  {
    "text": "else and here is I'm going to wait with The",
    "start": "2460880",
    "end": "2466480"
  },
  {
    "text": "Flume configuration because I'm going to do it again in example three um but",
    "start": "2466480",
    "end": "2472280"
  },
  {
    "text": "imagine we have the data coming in streaming through Flume we do some",
    "start": "2472280",
    "end": "2477839"
  },
  {
    "text": "tables through Hive again same way and then we go to Hue to query that",
    "start": "2477839",
    "end": "2484240"
  },
  {
    "text": "data and Tada what do we find well the top 10 most viewed products has",
    "start": "2484240",
    "end": "2490880"
  },
  {
    "text": "a diff if you correlate it with the top 10 most sold products and suddenly we",
    "start": "2490880",
    "end": "2496400"
  },
  {
    "text": "know something and we can ask a new question why and we can go and start",
    "start": "2496400",
    "end": "2503000"
  },
  {
    "text": "investigating this well maybe we priced the product wrong so people actually turn away from our website for that",
    "start": "2503000",
    "end": "2510760"
  },
  {
    "text": "product that it's viewed but not bought it's a it's a kids football okay maybe",
    "start": "2510760",
    "end": "2516720"
  },
  {
    "text": "maybe we can can create a new marketing program for families maybe it's kids viewing the product and they don't",
    "start": "2516720",
    "end": "2522480"
  },
  {
    "text": "really have any purchasing power but we can talk to the parents and maybe that can help if especially if we find that",
    "start": "2522480",
    "end": "2529200"
  },
  {
    "text": "we can bundle it with products that parents look look at at the same time there's there's interesting information",
    "start": "2529200",
    "end": "2536079"
  },
  {
    "text": "and insight to make Make a Better Business decision by correlating two different data sets structured and",
    "start": "2536079",
    "end": "2543680"
  },
  {
    "text": "semi-structured in the same platform and it's not that difficult ult",
    "start": "2543680",
    "end": "2550000"
  },
  {
    "text": "right so business question number two this is the third and last example before we",
    "start": "2551359",
    "end": "2557599"
  },
  {
    "text": "wrap up so why is sales suddenly dropping well that's a new business",
    "start": "2557599",
    "end": "2565000"
  },
  {
    "text": "question suddenly something happens with the sales data coming in and we're going to use the same data set as we just set",
    "start": "2565000",
    "end": "2571720"
  },
  {
    "text": "up with Flume the realtime web clickstream to figure out if something",
    "start": "2571720",
    "end": "2577319"
  },
  {
    "text": "going on with our uh online store so same data for new business",
    "start": "2577319",
    "end": "2583160"
  },
  {
    "text": "question that's another big data value because you don't have to move the",
    "start": "2583160",
    "end": "2588559"
  },
  {
    "text": "data to different infrastructure to serve it the data is already there you can just use a different framework",
    "start": "2588559",
    "end": "2595160"
  },
  {
    "text": "process it and serve it that's the key so we're going to use the same data",
    "start": "2595160",
    "end": "2602079"
  },
  {
    "text": "we're going to use a indexing engine solar Cloud to index it but we're going",
    "start": "2602079",
    "end": "2607520"
  },
  {
    "text": "to do that in real time so we're going to reconfigure our flu engent to post events to solar and then we're going to use Hue",
    "start": "2607520",
    "end": "2615520"
  },
  {
    "text": "again but the Search application H to free text search our web clickstream",
    "start": "2615520",
    "end": "2622200"
  },
  {
    "text": "data so first we create an empty solar index configuration and then we introspect the",
    "start": "2622200",
    "end": "2629400"
  },
  {
    "text": "solar scheme and change it to the fields we're actually interested searching over at that particular time for that use",
    "start": "2629400",
    "end": "2636079"
  },
  {
    "text": "case you can always change your mind that's the idea",
    "start": "2636079",
    "end": "2641359"
  },
  {
    "text": "right and then we upload our configuration to zookeeper who keeps",
    "start": "2641359",
    "end": "2646599"
  },
  {
    "text": "track of the truth in case one of the solar nodes goes down it's fail safe right so zookeeper keeps the truth no",
    "start": "2646599",
    "end": "2655000"
  },
  {
    "text": "risk of split brain in a search cluster on Hadoop and then we tells solar to start",
    "start": "2655000",
    "end": "2662079"
  },
  {
    "text": "serving up uh a collection and start accepting data to index it for it",
    "start": "2662079",
    "end": "2668359"
  },
  {
    "text": "and then we're going to configure the data using Flume again to post to solar",
    "start": "2668359",
    "end": "2673920"
  },
  {
    "text": "the incoming events so solar can accept them and index them as they arrive and in our Flume",
    "start": "2673920",
    "end": "2681040"
  },
  {
    "text": "sync we use a framework called morph lines which is a mini ETL uh pre-built",
    "start": "2681040",
    "end": "2687400"
  },
  {
    "text": "Java library that looks and smells like Unix pipelines so you can process your",
    "start": "2687400",
    "end": "2692599"
  },
  {
    "text": "data as if it was a line by line uh command line uh",
    "start": "2692599",
    "end": "2698319"
  },
  {
    "text": "Transformations and then we publish it solar cloud and you can also do custom Morlin",
    "start": "2698319",
    "end": "2706200"
  },
  {
    "text": "you can instead of using these prepared grock and whatever pre-built uh Transformations for your data uh needs",
    "start": "2706200",
    "end": "2713839"
  },
  {
    "text": "you can also Implement your own Mor lines and use them that's what we're going to do here's an example and then the lower part here is",
    "start": "2713839",
    "end": "2721760"
  },
  {
    "text": "the actual Flume configuration part where you point to what morls you use to transform your data extract the fields",
    "start": "2721760",
    "end": "2728720"
  },
  {
    "text": "and map them into the solar schema and also what solar cluster on Hadoop you're going to publish them",
    "start": "2728720",
    "end": "2735160"
  },
  {
    "text": "to and Tada now you can go to the Hue Search application and drill down into",
    "start": "2735160",
    "end": "2741599"
  },
  {
    "text": "your web clickstream data and analyze maybe it was a DDOS attack going on so",
    "start": "2741599",
    "end": "2748319"
  },
  {
    "text": "you can address that and suddenly your sales goes back up another tool another use case with same data",
    "start": "2748319",
    "end": "2756440"
  },
  {
    "text": "without mov moving it around and if you want to try these examples on your own we are going to",
    "start": "2756440",
    "end": "2763280"
  },
  {
    "text": "launch uh the six of October free trial clusters that you can play with but",
    "start": "2763280",
    "end": "2768920"
  },
  {
    "text": "that's a side note um so I leave that behind the key takeaways from today is",
    "start": "2768920",
    "end": "2776680"
  },
  {
    "text": "that information driven businesses is what organizations are striving for and",
    "start": "2776680",
    "end": "2782599"
  },
  {
    "text": "it's essential if you're going to survive in a more competitive market um Hadoop is a dis disruptive technology",
    "start": "2782599",
    "end": "2792359"
  },
  {
    "text": "and the drivers in the market has opened up for this technology and more and more vendors has",
    "start": "2792359",
    "end": "2799079"
  },
  {
    "text": "popped up but the big vendors are there too and it's established it's",
    "start": "2799079",
    "end": "2804839"
  },
  {
    "text": "validated it's not going to go away yet and uh use the right load for the right",
    "start": "2804839",
    "end": "2811720"
  },
  {
    "text": "workload right tool for the right workload and also remember that big data",
    "start": "2811720",
    "end": "2817800"
  },
  {
    "text": "is not only about the data size it's about correlating various kinds of data sets",
    "start": "2817800",
    "end": "2824559"
  },
  {
    "text": "or serving multiple use cases of the same data without having to move it",
    "start": "2824559",
    "end": "2831640"
  },
  {
    "text": "around and I hope that you now feel like this kitten and that you actually",
    "start": "2831640",
    "end": "2838640"
  },
  {
    "text": "learned something and if you did please vote please vote anyway um and let me know what you think",
    "start": "2838640",
    "end": "2845920"
  },
  {
    "text": "any questions I don't even see you",
    "start": "2845920",
    "end": "2852720"
  },
  {
    "text": "guys no questions you must be",
    "start": "2854200",
    "end": "2859359"
  },
  {
    "text": "brilliant all right after the break Dean Wampler will",
    "start": "2859359",
    "end": "2864839"
  },
  {
    "text": "be back and deep dive into spark and the new Hypes around a new era of",
    "start": "2864839",
    "end": "2870559"
  },
  {
    "text": "SQL thank",
    "start": "2870559",
    "end": "2873760"
  },
  {
    "text": "you name is Dean wler I get to introduce",
    "start": "2875839",
    "end": "2883000"
  },
  {
    "text": "myself um if you attended the last session Eva andreon gave sort of a a",
    "start": "2883000",
    "end": "2888960"
  },
  {
    "text": "broad overview of the Big Data landscape and talked about a few scenarios for ingesting data and doing analysis on it",
    "start": "2888960",
    "end": "2895400"
  },
  {
    "text": "in this session I'm going to dive into two specific areas that are kind of very hot right now the first is a general",
    "start": "2895400",
    "end": "2901480"
  },
  {
    "text": "purpose compute engine called spark that I'm very excited about uh for reasons I hope you'll see by the you know 40 20",
    "start": "2901480",
    "end": "2908920"
  },
  {
    "text": "minutes from now whatever and then I want to talk about how SQL is making a big comeback in the Big Data space uh",
    "start": "2908920",
    "end": "2914599"
  },
  {
    "text": "not that no sequel's been invalidated in any sense but that as these things go we tend to swing in pendulums you know we",
    "start": "2914599",
    "end": "2921240"
  },
  {
    "text": "swung hard over to the no sequel site got to the point of even dismissing SQL and now we're coming back and in a way",
    "start": "2921240",
    "end": "2927559"
  },
  {
    "text": "those comments will be sort of complimentary to what Mark Matson was talking about earlier today if you",
    "start": "2927559",
    "end": "2933000"
  },
  {
    "text": "visited his talk and I think it would be worth looking at that on online if you missed this talk and you're interested",
    "start": "2933000",
    "end": "2938480"
  },
  {
    "text": "in you what's the role of SQL SQL uh in the uh data world",
    "start": "2938480",
    "end": "2944440"
  },
  {
    "text": "today my name is Dean wler as I said I work for typesafe typesafe is not a big",
    "start": "2944440",
    "end": "2949520"
  },
  {
    "text": "data company per se even though that's what I've been doing the last few years but we're starting to you know grow into",
    "start": "2949520",
    "end": "2954839"
  },
  {
    "text": "that area we mostly developed a scholar language and tools on top of it um you",
    "start": "2954839",
    "end": "2961079"
  },
  {
    "text": "can I haven't posted this talk yet on my polyglot programming site but I have some other uh talk talks there if you're",
    "start": "2961079",
    "end": "2967400"
  },
  {
    "text": "interested and of course this talk will be on the conference website all these three books are reasonably priced",
    "start": "2967400",
    "end": "2973040"
  },
  {
    "text": "outside um although I actually don't recommend you buy the second one the big one because this is actually the second",
    "start": "2973040",
    "end": "2978880"
  },
  {
    "text": "edition that's coming out next month and they have the first edition so I don't want anybody to be disappointed um",
    "start": "2978880",
    "end": "2985760"
  },
  {
    "text": "anyway so it's two it's 2014 I always use pretty pictures of my hiking trips",
    "start": "2985760",
    "end": "2991839"
  },
  {
    "text": "because if you get bored with the material at least you'll have something to do um and this is Colorado in the",
    "start": "2991839",
    "end": "2997119"
  },
  {
    "text": "Western United States anyway at this point Hadoop has",
    "start": "2997119",
    "end": "3002400"
  },
  {
    "text": "been really successful uh I'm sure sure all of you have heard of Hado by now I won't even ask because it's kind of a",
    "start": "3002400",
    "end": "3007480"
  },
  {
    "text": "given um but like everything it's a first generation technology it's solved a lot of really good problems and really",
    "start": "3007480",
    "end": "3015000"
  },
  {
    "text": "it's pieces of first generation Technologies and we're getting to the point where we're starting to see second",
    "start": "3015000",
    "end": "3020400"
  },
  {
    "text": "generation Technologies emerge and that's really kind of the theme of this this talk it it's not perfect",
    "start": "3020400",
    "end": "3027640"
  },
  {
    "text": "um there have been some warts that have uh become increasingly important to fix and maybe the most important one was the",
    "start": "3027640",
    "end": "3033839"
  },
  {
    "text": "compute engine called M ruce and I'll explain what M produce is in a second but we kind of realized that we needed",
    "start": "3033839",
    "end": "3039920"
  },
  {
    "text": "to fix it or or replace it and that's kind of what's happened here the problem",
    "start": "3039920",
    "end": "3045079"
  },
  {
    "text": "with map well first let's talk about briefly what it is so it's really the two things stuck together mapping over",
    "start": "3045079",
    "end": "3051359"
  },
  {
    "text": "data and reducing it what that basically means it'll be a little clear as we walk through some examples but",
    "start": "3051359",
    "end": "3057280"
  },
  {
    "text": "in parallel I might read a bunch of data that's kind of off the screen on the left in these so-called map processes",
    "start": "3057280",
    "end": "3062640"
  },
  {
    "text": "and that would do initial maybe filtering you know transformation whatever it spits out key value pairs on",
    "start": "3062640",
    "end": "3069200"
  },
  {
    "text": "the back end those keys are you know whatever those key value pairs are depends on the algorithm you're implementing and then there's a process",
    "start": "3069200",
    "end": "3075920"
  },
  {
    "text": "where the keys are sorted locally on each process those map tasks or jvm processes and then they're shuffled over",
    "start": "3075920",
    "end": "3082839"
  },
  {
    "text": "the cluster that's all done for you so that you know if you're doing like word count that is you want to count the",
    "start": "3082839",
    "end": "3088359"
  },
  {
    "text": "occurrences of all words and documents then all the words all the occurrences of Hadoop as a key might show up in that",
    "start": "3088359",
    "end": "3094280"
  },
  {
    "text": "first reducer task and then you'll just add up all the ones you get that sort of thing well it'll be more clear as we go",
    "start": "3094280",
    "end": "3100440"
  },
  {
    "text": "but just so you know where the word map reduce comes from so you can impress your friends at the uh reception after",
    "start": "3100440",
    "end": "3106079"
  },
  {
    "text": "the talk uh so there's a bunch of problems with it though I mean even though it's",
    "start": "3106079",
    "end": "3111240"
  },
  {
    "text": "been great it was invented at Google so it has to be good uh and it served us well for all these years actually 10",
    "start": "3111240",
    "end": "3116839"
  },
  {
    "text": "years now really uh problems have come up it's kind of a limited programming model actually if all you have is map",
    "start": "3116839",
    "end": "3123079"
  },
  {
    "text": "and reduce you kind of have to hammer your algorithm into it and some things just don't work so well like it turns",
    "start": "3123079",
    "end": "3128559"
  },
  {
    "text": "out iterative algorithms like training a machine learning model is really hard to do in this model at least to do",
    "start": "3128559",
    "end": "3136119"
  },
  {
    "text": "efficiently and it turns out the Hadoop Java API is kind of a nightmare to work with I'm I have high aesthetic opinions",
    "start": "3136119",
    "end": "3143440"
  },
  {
    "text": "about what software should look like and when I saw the dup API was really rather offended by it and really wanted",
    "start": "3143440",
    "end": "3149760"
  },
  {
    "text": "something better fortunately that's what we're getting but just to give you an example just to kind of show the before",
    "start": "3149760",
    "end": "3155440"
  },
  {
    "text": "and after if you will let's talk about another algorithm called inverted index and this is actually the basis of search",
    "start": "3155440",
    "end": "3161359"
  },
  {
    "text": "engines at least in the simplest case where what I want to do is I'm going to",
    "start": "3161359",
    "end": "3166760"
  },
  {
    "text": "uh like crawl the web and I want to find say all the documents on the web and here I have some madeup examples from",
    "start": "3166760",
    "end": "3172280"
  },
  {
    "text": "Wikipedia and what I'm going to do is build up a data set that these webcrawlers are going to write for me",
    "start": "3172280",
    "end": "3177720"
  },
  {
    "text": "you all the time they're just running constantly right and it'll just be two columns it'll be some ID for what this",
    "start": "3177720",
    "end": "3184240"
  },
  {
    "text": "document is in this case I'm just using the name or the URL and then just the contents of the document maybe I",
    "start": "3184240",
    "end": "3189799"
  },
  {
    "text": "stripped out HTML tags maybe not doesn't really matter that's so that's sort of",
    "start": "3189799",
    "end": "3194920"
  },
  {
    "text": "my index of documents to words I want to invert it to have words to",
    "start": "3194920",
    "end": "3200880"
  },
  {
    "text": "documents so some sort of Magic's going to happen in the middle and we'll look at the Magic in a second or Miracle I",
    "start": "3200880",
    "end": "3206240"
  },
  {
    "text": "guess and the output we want is what you would want if you were trying to like Implement a search engine if you wanted",
    "start": "3206240",
    "end": "3211839"
  },
  {
    "text": "to be the next Google where you would want to tokenize those contents of documents into words and then find all",
    "start": "3211839",
    "end": "3219440"
  },
  {
    "text": "the occurrences of of a given word in all the documents so you know for example uh I in my madeup example the",
    "start": "3219440",
    "end": "3225680"
  },
  {
    "text": "word hbas appeared in two documents uh you know I used ellipses for the full",
    "start": "3225680",
    "end": "3231000"
  },
  {
    "text": "paths because they're kind of long and I might want to know how many times the word occurred in there because obviously",
    "start": "3231000",
    "end": "3236520"
  },
  {
    "text": "a page that's devoted to hbas will talk about it a lot and that's probably the document you want to see first if you",
    "start": "3236520",
    "end": "3242559"
  },
  {
    "text": "were returning results to a user so we need to do this miracle part in the",
    "start": "3242559",
    "end": "3248119"
  },
  {
    "text": "middle and just for completeness here's the whole thing together so this is what it looks like",
    "start": "3248119",
    "end": "3253640"
  },
  {
    "text": "in the Hadoop API and if you don't like looking at Java it's a good time to avert your gaze you know you bury your",
    "start": "3253640",
    "end": "3259520"
  },
  {
    "text": "eyes or something I'm not going to talk through all this code it goes on endlessly um you what you'll notice is",
    "start": "3259520",
    "end": "3266480"
  },
  {
    "text": "that uh I used yellow for functions and if you just sort of pay attention a little bit you notice that all the",
    "start": "3266480",
    "end": "3271760"
  },
  {
    "text": "functions in this code don't really do a whole lot of work well maybe main does because it does everything but things",
    "start": "3271760",
    "end": "3278079"
  },
  {
    "text": "like you know set a value get a value I mean it's kind of boring you know low content takes up a lot of space doesn't",
    "start": "3278079",
    "end": "3284559"
  },
  {
    "text": "deliver a lot of value and in the sort of functional way of thinking about the world we really want our functions to",
    "start": "3284559",
    "end": "3290119"
  },
  {
    "text": "pack a lot of punch you know when I do something like a map I want it to do a major amount of work for me so I write",
    "start": "3290119",
    "end": "3296760"
  },
  {
    "text": "as little code as possible so that's sort of where we're going with this but anyway you know the first page here is",
    "start": "3296760",
    "end": "3302200"
  },
  {
    "text": "just telling Hadoop uh configuration stuff like here's the name of the class for mapping and for reducing and here's",
    "start": "3302200",
    "end": "3309119"
  },
  {
    "text": "the types I'm going to input and output and all kinds of stuff that it apparently can't figure out on its own",
    "start": "3309119",
    "end": "3314559"
  },
  {
    "text": "finally I run the job after configuring it and I have a you know an exception clause for whatever purpose I'm that",
    "start": "3314559",
    "end": "3320480"
  },
  {
    "text": "really sure then I start my class that does mapping remember I said we do these Mapp and reduce steps and and I have to",
    "start": "3320480",
    "end": "3326720"
  },
  {
    "text": "implement this method map and there's a whole bunch of stuff I passed to the argument a lot of it is ceremony it's",
    "start": "3326720",
    "end": "3333160"
  },
  {
    "text": "infrastructure like this thing that's called a reporter and this output collector is where I'm going to write my",
    "start": "3333160",
    "end": "3338440"
  },
  {
    "text": "key value pairs I talked about the interesting bit is that key and Val uh thing that I'm showing here you know",
    "start": "3338440",
    "end": "3345079"
  },
  {
    "text": "that's like well let's not even worry about what it is just think of it just the document right now the document",
    "start": "3345079",
    "end": "3350440"
  },
  {
    "text": "contents but then I got to write a lot of java code to split this thing and uh you know write these key Val pairs I",
    "start": "3350440",
    "end": "3356960"
  },
  {
    "text": "mentioned in the middle and then it just keeps going now here's my reducer and I've got a whole bunch of stuff on the",
    "start": "3356960",
    "end": "3362720"
  },
  {
    "text": "screen and your you know your eyes are bleeding and uh you know I'm just building up basically all I'm really",
    "start": "3362720",
    "end": "3368000"
  },
  {
    "text": "doing now is getting all those occurrences of the word hbas and the word Hadoop together and writing those",
    "start": "3368000",
    "end": "3373559"
  },
  {
    "text": "little you know tuples of like document ID count document ID count so there's",
    "start": "3373559",
    "end": "3378880"
  },
  {
    "text": "just a whole lot of hoo-ha here and then it's done mercifully and the thing is this is like",
    "start": "3378880",
    "end": "3386480"
  },
  {
    "text": "a software engineering project and it's not a this is a simple algorithm it's not the simplest I could write but it's",
    "start": "3386480",
    "end": "3393280"
  },
  {
    "text": "a very simple algorithm and it you know it fits on the screen with sixo font which even I can't read when I look at",
    "start": "3393280",
    "end": "3398960"
  },
  {
    "text": "the screen um but I have to go through all the usual software engineering stuff it takes you know a couple hours to write",
    "start": "3398960",
    "end": "3405039"
  },
  {
    "text": "this even if you know the API if you don't it's going to take a couple days and it's a simple algorithm and if it's",
    "start": "3405039",
    "end": "3411280"
  },
  {
    "text": "not a simple algorithm it just the complexity just exponentially ramps up so it's terrible",
    "start": "3411280",
    "end": "3418000"
  },
  {
    "text": "um and it also only lets me do batch mode analysis so you know that was great when Google invented map ruce you know",
    "start": "3418000",
    "end": "3424000"
  },
  {
    "text": "back in the day um you know it's probably okay if they had these web crawlers building up that data set maybe",
    "start": "3424000",
    "end": "3430039"
  },
  {
    "text": "once a day you know they would calculate a new index for searching but that sucks",
    "start": "3430039",
    "end": "3435160"
  },
  {
    "text": "right because as soon as you make an edit to your web page you know your great e-commerce site whatever you want",
    "start": "3435160",
    "end": "3441200"
  },
  {
    "text": "the search results to reflect those changes like you know immediately or within maybe you know few minutes or an",
    "start": "3441200",
    "end": "3446480"
  },
  {
    "text": "hour at least so you'd really like this to be running constantly you know as events come in of changes you'd really",
    "start": "3446480",
    "end": "3452960"
  },
  {
    "text": "like to have your index reflect those as quickly as possible and that's a you know trivial case more realistic or",
    "start": "3452960",
    "end": "3459200"
  },
  {
    "text": "serious cases are you're like watching log dat and all of a sudden a server is screaming I'm on fire you don't want to",
    "start": "3459200",
    "end": "3465240"
  },
  {
    "text": "wait till you know tonight to figure out that your server is on fire when you run this batch mode job so we really want to",
    "start": "3465240",
    "end": "3471359"
  },
  {
    "text": "do event streaming data too and map reduce is not at all designed for that",
    "start": "3471359",
    "end": "3477160"
  },
  {
    "text": "there's also a serious performance problem well there a lot of them really but the one that's the worst is now the",
    "start": "3477160",
    "end": "3483839"
  },
  {
    "text": "example I showed I could write one map reduce job uh you know map tasks reduced",
    "start": "3483839",
    "end": "3489440"
  },
  {
    "text": "tasks uh to implement it but a lot of algorithms you can't do it quite in that few steps you kind of have to sequence",
    "start": "3489440",
    "end": "3495880"
  },
  {
    "text": "some of these jobs together unfortunately map ruce doesn't know you're doing this so I might have",
    "start": "3495880",
    "end": "3501440"
  },
  {
    "text": "terabytes of intermediate data out of one of those steps but I have to write it all to disc you know flush it out of",
    "start": "3501440",
    "end": "3507440"
  },
  {
    "text": "memory even if the next job that's going to start immediately it's going to read it all back in and it turns out that",
    "start": "3507440",
    "end": "3513440"
  },
  {
    "text": "just fixing that like smart caching of that intermediate data can easily give you a 100x performance gain in a lot of",
    "start": "3513440",
    "end": "3520640"
  },
  {
    "text": "algorithms so we need to fix that problem and that's where Spark came in",
    "start": "3520640",
    "end": "3526799"
  },
  {
    "text": "so about a year ago now maybe a little longer than that the uh major Hado",
    "start": "3526799",
    "end": "3531960"
  },
  {
    "text": "vendors realized you know we really need to embrace something new that seems to be proven or at least have the potential",
    "start": "3531960",
    "end": "3537079"
  },
  {
    "text": "to take over that fixes all these problems and that turned out to be a Berkeley University project that had",
    "start": "3537079",
    "end": "3543640"
  },
  {
    "text": "been kind of incubating for really since 2009 or so had had already become an",
    "start": "3543640",
    "end": "3548760"
  },
  {
    "text": "Apache project and so they they sort of wholesale decided this is the next Generation now I should say if you",
    "start": "3548760",
    "end": "3555359"
  },
  {
    "text": "really actually have big data like if you're Twitter or you know Facebook or whatever map uh spark probably isn't",
    "start": "3555359",
    "end": "3562039"
  },
  {
    "text": "quite ready for your massive data sets but for most of us the sort of intermediate data that we might be",
    "start": "3562039",
    "end": "3567760"
  },
  {
    "text": "working with it's probably perfectly fine but you know as always you should be paranoid and test everything to make",
    "start": "3567760",
    "end": "3573440"
  },
  {
    "text": "sure you can make this switch but I hope you'll see within you know the next few minutes why this is such a great thing",
    "start": "3573440",
    "end": "3579680"
  },
  {
    "text": "why it's going to be worth it when it's finally ready or maybe ready already for your needs the first thing it does is it",
    "start": "3579680",
    "end": "3586480"
  },
  {
    "text": "gives us a really elegant and concise programming model we're going to shrink that code I showed you before down to",
    "start": "3586480",
    "end": "3593079"
  },
  {
    "text": "basically 10 or so lines with maybe a little bit of setup up and tear down that you have to have in most languages",
    "start": "3593079",
    "end": "3599839"
  },
  {
    "text": "and we're going to get rid of most of the ceremony so that we can focus on the",
    "start": "3599839",
    "end": "3604880"
  },
  {
    "text": "problem what else have we got it is written in Scola I mentioned I work for typ safe so I might have a little bias",
    "start": "3606079",
    "end": "3612119"
  },
  {
    "text": "here but I hope you'll see that it's actually not bad Scala if you've ever been you know if you dislike Scala like",
    "start": "3612119",
    "end": "3618480"
  },
  {
    "text": "that's okay you can use python or Java and actually the r language is coming there there's a team working on that if",
    "start": "3618480",
    "end": "3624640"
  },
  {
    "text": "you're interested but I'll show you scholar code and I you know I'll explain just enough so you get the gist of what's going on obviously we don't have",
    "start": "3624640",
    "end": "3631319"
  },
  {
    "text": "enough time to teach schola but I hope you'll get the sense for how concise you can make statements that do exactly what",
    "start": "3631319",
    "end": "3638280"
  },
  {
    "text": "you want to do and don't require a whole lot of stuff uh wrapped around",
    "start": "3638280",
    "end": "3643599"
  },
  {
    "text": "it and they also uh spark also leverages the power of functional programming the",
    "start": "3643599",
    "end": "3649119"
  },
  {
    "text": "so-called combinators and basically they're functions that don't have side effects they take inputs they do some",
    "start": "3649119",
    "end": "3655079"
  },
  {
    "text": "sort of transformation they output stuff and it turns out you can just sequence those together like a pipeline like",
    "start": "3655079",
    "end": "3661119"
  },
  {
    "text": "we're basically going to build plumbing and then turn on the spigot and then run the data through it and it's uh really",
    "start": "3661119",
    "end": "3667599"
  },
  {
    "text": "in my opinion this is the most effective reuse technique we've ever invented in software well combinators were actually",
    "start": "3667599",
    "end": "3673480"
  },
  {
    "text": "invented math mathematics but nevertheless far better than objects far better than almost anything else we've",
    "start": "3673480",
    "end": "3679559"
  },
  {
    "text": "come up with and you'll I think hope you'll see what I mean when we get to the example here in a minute another",
    "start": "3679559",
    "end": "3685720"
  },
  {
    "text": "nice feature of spark is that you're not limited to map or rather Hadoop even though that's probably the way most",
    "start": "3685720",
    "end": "3690760"
  },
  {
    "text": "people are going to use it for the foreseeable future uh you can actually run it on this new uh clustering",
    "start": "3690760",
    "end": "3696160"
  },
  {
    "text": "framework called mesos that uh Twitter has been working on it's another Berkeley project they hired the grad",
    "start": "3696160",
    "end": "3702160"
  },
  {
    "text": "student he didn't get his PhD as a result uh but he did finish mesos so that maybe that was better for all of us",
    "start": "3702160",
    "end": "3708599"
  },
  {
    "text": "you can run it in ec2 if you're uh already working in Amazon and there's even a little simple Standalone mode",
    "start": "3708599",
    "end": "3714000"
  },
  {
    "text": "where you can set up kind of a static cluster that's just you know special purpose and doesn't require a lot of ceremony like disaster recovery and all",
    "start": "3714000",
    "end": "3721720"
  },
  {
    "text": "that sort of thing like that's what you would do in development or maybe a research lab where you're prototyping",
    "start": "3721720",
    "end": "3726760"
  },
  {
    "text": "algorithms or doing basic data analytics the the core abstraction of",
    "start": "3726760",
    "end": "3733079"
  },
  {
    "text": "spark upon which everything else is based which is why I pick these kind of rocks here it's kind of the foundation",
    "start": "3733079",
    "end": "3739200"
  },
  {
    "text": "anyway that's that's what I was thinking um it's called a resilient distributed data set so what we're going to do we're",
    "start": "3739200",
    "end": "3744400"
  },
  {
    "text": "going to read in this data that we want to put in memory and we're going to Shard it over the cluster so that's the",
    "start": "3744400",
    "end": "3749760"
  },
  {
    "text": "distributed part although it you know virtually it appears like it's one big collection and the same way the file system like hdfs the Hado file system",
    "start": "3749760",
    "end": "3757520"
  },
  {
    "text": "looks like one big file system even though it's actually over a bunch of",
    "start": "3757520",
    "end": "3763400"
  },
  {
    "text": "servers the resilient part is not so much that they actually like make redundant copies if a node goes down",
    "start": "3763400",
    "end": "3769640"
  },
  {
    "text": "you've got data elsewhere uh to save space they actually adopted this model is they keep track of where all the data",
    "start": "3769640",
    "end": "3775920"
  },
  {
    "text": "came from like if you're in the midpoint of some you know Pipeline and you lose a node they can actually go back and",
    "start": "3775920",
    "end": "3781839"
  },
  {
    "text": "recreate that data on another node of course it's going to take some time to do that uh there's reasons why you may",
    "start": "3781839",
    "end": "3787079"
  },
  {
    "text": "not want to always do that but when you think about the fact that yes servers do crash but it really isn't that often you",
    "start": "3787079",
    "end": "3792880"
  },
  {
    "text": "know most of the time this is going to work just fine so uh you know schematically it",
    "start": "3792880",
    "end": "3799279"
  },
  {
    "text": "looks something like this I have this one rdd but it's broken into partitions across different nodes in my cluster",
    "start": "3799279",
    "end": "3807760"
  },
  {
    "text": "okay uh this is Scala code coming up so you know fair warning you know cover your eyes if you don't like Scala it",
    "start": "3808079",
    "end": "3814200"
  },
  {
    "text": "it'll go quickly but uh this is the inverted index and skull actually this implementation does a little bit more",
    "start": "3814200",
    "end": "3819599"
  },
  {
    "text": "than the other one does but it uses a lot less code anyway so uh like most jvm languages we",
    "start": "3819599",
    "end": "3826920"
  },
  {
    "text": "have to do some imports at the start but in this case we just import this thing called a spark context that's kind of",
    "start": "3826920",
    "end": "3832000"
  },
  {
    "text": "like our driver you know that's that's the entry point that we start with um Scola has this notion of um the",
    "start": "3832000",
    "end": "3839000"
  },
  {
    "text": "Singleton design patterns actually baked into into the language so where I say object inverted index just think class",
    "start": "3839000",
    "end": "3844599"
  },
  {
    "text": "inverted index and there's a Singleton instance that I don't have to manage myself and that's where we put things",
    "start": "3844599",
    "end": "3850279"
  },
  {
    "text": "like the main routine so like the previous example we're just going to run all this in main okay the first thing I do is I",
    "start": "3850279",
    "end": "3856799"
  },
  {
    "text": "create a spark context and then I use this text file method to read the data from that path and and text file means",
    "start": "3856799",
    "end": "3863880"
  },
  {
    "text": "it's going to assume that each line in the file is is a single field record and then we'll you know parse it as we see",
    "start": "3863880",
    "end": "3870119"
  },
  {
    "text": "fit from there and then we're going to map over it and notice there's a dot in front of the map and also in front of",
    "start": "3870119",
    "end": "3875640"
  },
  {
    "text": "the flat map I'm basically building up this chain of of method invocations that are forming my pipeline you know I could",
    "start": "3875640",
    "end": "3881319"
  },
  {
    "text": "assign intermediate variables if I wanted but usually you just do it this way and that thing in curly braces is a",
    "start": "3881319",
    "end": "3887680"
  },
  {
    "text": "function an anonymous function I passed to map and it says for each line you know coming out of the input file I want",
    "start": "3887680",
    "end": "3894480"
  },
  {
    "text": "you to first split it on the first tab you find so this we'll assume it's the crawl data that we generated you know",
    "start": "3894480",
    "end": "3900760"
  },
  {
    "text": "separately as tab to limited but of course there would be tabs in the text so they only want to split into two and",
    "start": "3900760",
    "end": "3906160"
  },
  {
    "text": "just grab on the first Tab and then Scola also has a nice Syntax for creating a tuple I don't have",
    "start": "3906160",
    "end": "3912559"
  },
  {
    "text": "to declare a class that returns two things like you do in Java which just annoys the hell out of me I could just",
    "start": "3912559",
    "end": "3918760"
  },
  {
    "text": "say you know parentheses first element of the array second element of the array and I have a little Two element Tuple",
    "start": "3918760",
    "end": "3924559"
  },
  {
    "text": "and I can just sling it around the the types are inferred here there's very few types in this program and that's because",
    "start": "3924559",
    "end": "3930680"
  },
  {
    "text": "Scala even though it's a statically typed language is inferring type so actually I think spark and another big",
    "start": "3930680",
    "end": "3936960"
  },
  {
    "text": "data uh uh Scola API called scalding do a pretty nice job hiding the",
    "start": "3936960",
    "end": "3942359"
  },
  {
    "text": "complexities of Scala and letting you write stuff where all the most of the types are inferred except where you might want them we'll see an example in",
    "start": "3942359",
    "end": "3949200"
  },
  {
    "text": "a minute where we're going to use a class to specify a schema for the data that we want to work with all right so",
    "start": "3949200",
    "end": "3955640"
  },
  {
    "text": "so I mapped over it to do that uh splitting into the what was the document path and then the the contents of the",
    "start": "3955640",
    "end": "3962599"
  },
  {
    "text": "document flat map is an extension of map where now I want to split the contents which is called well I'm calling it text",
    "start": "3962599",
    "end": "3969400"
  },
  {
    "text": "here I'm going to split on everything that isn't a alpha numeric character so that'll be how I tokenize into words yes",
    "start": "3969400",
    "end": "3975720"
  },
  {
    "text": "it's a crude way to do it but it's good enough for now uh so what what flatmap does is",
    "start": "3975720",
    "end": "3981440"
  },
  {
    "text": "instead of like a Ono one thing I'm actually going to generate like an array and an AR array and array for each input",
    "start": "3981440",
    "end": "3987640"
  },
  {
    "text": "text but then the flat part is where I'm just going to flatten it so I just have this one long array of these two element",
    "start": "3987640",
    "end": "3994559"
  },
  {
    "text": "tuples again that's going to be a word and that original path or document ID so",
    "start": "3994559",
    "end": "3999640"
  },
  {
    "text": "now I've got word document ID Word document ID Etc and then the next few lines is",
    "start": "3999640",
    "end": "4005359"
  },
  {
    "text": "really just oops I went a little too fast there is really kind of tupple hacking I actually need to do a little",
    "start": "4005359",
    "end": "4010880"
  },
  {
    "text": "rearranging to do group by effectively and so all I need to do is like in this next map step is do a pattern match on",
    "start": "4010880",
    "end": "4018200"
  },
  {
    "text": "the that word and path and then just rearrange things a bit actually what I'm doing here is I'm nesting that inside",
    "start": "4018200",
    "end": "4024079"
  },
  {
    "text": "another tupple with a count of one that little nested word and path is now going to be a key that I use for like join",
    "start": "4024079",
    "end": "4030799"
  },
  {
    "text": "finding all the things that are the same so I want to find all the words and paths that match",
    "start": "4030799",
    "end": "4036640"
  },
  {
    "text": "together and then they have this built-in thing called Reduce by key where it's going to take that first you",
    "start": "4036640",
    "end": "4042119"
  },
  {
    "text": "know each of those word path Pairs and then sum up the thing that's left which is just the count of one so that's how I",
    "start": "4042119",
    "end": "4048359"
  },
  {
    "text": "get the total count of each of those word path Pairs and then I do some more rearranging of the tuples to get it back",
    "start": "4048359",
    "end": "4054640"
  },
  {
    "text": "the way I want I you know the output I want is you know here's the word and then here's this list of paths and",
    "start": "4054640",
    "end": "4060039"
  },
  {
    "text": "counts and so that's what this last map statement does and then finally I do a",
    "start": "4060039",
    "end": "4065440"
  },
  {
    "text": "group by you know our old friend from SQL uh do a group by over effectively the word and that's going to give me um",
    "start": "4065440",
    "end": "4072480"
  },
  {
    "text": "you know that long list of all of the documents and count that were a given word occurred and the",
    "start": "4072480",
    "end": "4078520"
  },
  {
    "text": "last map statement is just rearranging the output so there's not much going on it looks complicated but it all it's",
    "start": "4078520",
    "end": "4083640"
  },
  {
    "text": "really doing it's just pretty fing the output the way I want it and then finally notice all these dot somethings",
    "start": "4083640",
    "end": "4089520"
  },
  {
    "text": "dot somethings these heavyweight functions that are doing a hell of a lot of work for me pardon my French anyway U",
    "start": "4089520",
    "end": "4095640"
  },
  {
    "text": "now I'm going to save all this to uh a text file and I forgot to fix the argument to the text file this was a CO",
    "start": "4095640",
    "end": "4101960"
  },
  {
    "text": "copy paste thing you know it's some path that I want to write to and then we say stop when we're done when we've done",
    "start": "4101960",
    "end": "4108080"
  },
  {
    "text": "this processing so right now we're still in batch mode land remember I said that we'd like to be able to do streaming",
    "start": "4108080",
    "end": "4114080"
  },
  {
    "text": "well we'll see that you could actually take this code and almost use it without modification in a streaming context too",
    "start": "4114080",
    "end": "4119318"
  },
  {
    "text": "but right now we're just you know we got these files and we want to process them to build an",
    "start": "4119319",
    "end": "4124400"
  },
  {
    "text": "index so I want you to appreciate even if the you're totally confused by the schola isms here just sort of the",
    "start": "4124400",
    "end": "4130758"
  },
  {
    "text": "Elegance of this thing you know I'm just you know step step step you know just little functions inside each step that",
    "start": "4130759",
    "end": "4136679"
  },
  {
    "text": "tell it what to do like you know shuffle the tupple around or whatever it it's extremely composable these little",
    "start": "4136679",
    "end": "4143560"
  },
  {
    "text": "operations are themselves very powerful and yet I'm combining them to do some non-trivial work very concisely and for",
    "start": "4143560",
    "end": "4150318"
  },
  {
    "text": "me this is what gets me excited this is what gets me out of bed in the morning whereas in the days when I was riding",
    "start": "4150319",
    "end": "4156000"
  },
  {
    "text": "with a crappy map reduce API I would sort of have to drag my carcass out of bed every morning so so I'm a happy",
    "start": "4156000",
    "end": "4162679"
  },
  {
    "text": "camper and it reminds me of this anybody know what this the Maxwell equations that I hear",
    "start": "4162679",
    "end": "4168640"
  },
  {
    "text": "somebody say so I'm an old physicist I'm a recovering physicist I'm you know been in recovery now for 30 years I think um",
    "start": "4168640",
    "end": "4177159"
  },
  {
    "text": "anyway but I remember when I first learned about these you know this is a DSL for electricity magnetism that's all",
    "start": "4177159",
    "end": "4182920"
  },
  {
    "text": "you really need to know but gosh isn't that beautiful and for me when a code is beautiful that I'm happy and it tells me",
    "start": "4182920",
    "end": "4189080"
  },
  {
    "text": "I can get stuff done and I'm not going to you know be fighting bugs and fighting apis so anyway that's a shout",
    "start": "4189080",
    "end": "4196520"
  },
  {
    "text": "out to all you fellow physicists out there now it you know may you might actually be able to read this from the",
    "start": "4196520",
    "end": "4201760"
  },
  {
    "text": "back I don't know the font is maybe 12 point something but anyway it's a smaller program but it's actually doing",
    "start": "4201760",
    "end": "4207880"
  },
  {
    "text": "more work and there's almost no ceremony I got rid of the ceremony at the very beginning when I created a spark context",
    "start": "4207880",
    "end": "4215080"
  },
  {
    "text": "and then I'm just you know doing these Transformations you can tell I'm excited about this stuff I really love this",
    "start": "4215080",
    "end": "4221320"
  },
  {
    "text": "stuff and it took me like 30 minutes to write that thing when I first wrote it now of course I knew the API already so",
    "start": "4221320",
    "end": "4228400"
  },
  {
    "text": "it would take you maybe 40 minutes if you didn't know the API but all right well so uh one of the",
    "start": "4228400",
    "end": "4237120"
  },
  {
    "text": "reasons spark is really pretty important though is not only is it really cool for writing code like we just saw but it",
    "start": "4237120",
    "end": "4243199"
  },
  {
    "text": "actually creates a really good foundation upon which we're able to build other tools and other apis that",
    "start": "4243199",
    "end": "4248320"
  },
  {
    "text": "Express different kinds of uh let's call them paradigms for lack of a better word",
    "start": "4248320",
    "end": "4253560"
  },
  {
    "text": "I guess uh so I I'll mention three of them quickly and then we'll dive into a few",
    "start": "4253560",
    "end": "4259760"
  },
  {
    "text": "uh the first one is there's a really nice machine learning library so spark is much better if you have a you know an iterative algorithm like you're training",
    "start": "4259760",
    "end": "4266440"
  },
  {
    "text": "something with stochastic gradient descent or something if you know what that is um and that it's a small library",
    "start": "4266440",
    "end": "4272080"
  },
  {
    "text": "at this point but it's growing rapidly I'm really excited that Cliff click is integrating uh H2O with uh uh spark",
    "start": "4272080",
    "end": "4280760"
  },
  {
    "text": "maybe you heard that if you heard his talk earlier I think it was today because he they've got a really great",
    "start": "4280760",
    "end": "4286120"
  },
  {
    "text": "machine learning library so that's going to be a pretty killer combination if you have a graph problem you might use Neo",
    "start": "4286120",
    "end": "4292000"
  },
  {
    "text": "for J but if if if your algorithm won't fit in the size that Neo forj can handle",
    "start": "4292000",
    "end": "4297400"
  },
  {
    "text": "then graphx is designed to be a distributed graph system and tachon is",
    "start": "4297400",
    "end": "4302719"
  },
  {
    "text": "uh a little hard to explain right now but um it's basically taking Spark's",
    "start": "4302719",
    "end": "4308120"
  },
  {
    "text": "intelligent ability to Cache this data in memory between stages and let that go",
    "start": "4308120",
    "end": "4314040"
  },
  {
    "text": "in into a separate process and be shared among applications and they're actually describing this as a inmemory file",
    "start": "4314040",
    "end": "4321080"
  },
  {
    "text": "system so they're going of you know they're implementing file system semantics I'm talking about this now",
    "start": "4321080",
    "end": "4326400"
  },
  {
    "text": "it's sort of alpha quality stuff but I actually think this is going to be really disruptive when it's actually ready because it's going to transform",
    "start": "4326400",
    "end": "4332360"
  },
  {
    "text": "the way we write Big Data applications in my opinion you heard it here first let's put it that",
    "start": "4332360",
    "end": "4338320"
  },
  {
    "text": "way okay um one of the really cool things is that you with this nice",
    "start": "4338320",
    "end": "4343960"
  },
  {
    "text": "programming uh BAS we can actually Implement uh structured query language semantics on top now this isn't going to",
    "start": "4343960",
    "end": "4350520"
  },
  {
    "text": "be full database we're not going to have transactions we're not going to have inserts and updates but we're going to",
    "start": "4350520",
    "end": "4356520"
  },
  {
    "text": "put the queue back in SQL where we're just going to focus on writing SQL queries to interrogate our data and you",
    "start": "4356520",
    "end": "4362360"
  },
  {
    "text": "can do some things like create new tables and whatnot but it's not really designed to be a replacement for a relational",
    "start": "4362360",
    "end": "4368199"
  },
  {
    "text": "database but anyway let's see how this works so we're we're going to basically get the best of both worlds you know",
    "start": "4368199",
    "end": "4374400"
  },
  {
    "text": "sometimes a SQL query have you ever hit this point where you're writing a SQL query and all of a sudden you slam into a wall because you can't quite Express",
    "start": "4374400",
    "end": "4380520"
  },
  {
    "text": "what you want to express with SQL we've all been there right well now you can just sort of all right do your SQL where",
    "start": "4380520",
    "end": "4387199"
  },
  {
    "text": "that works but then flip back over to that Turing complete API to fill in the gaps and just sort of mix and",
    "start": "4387199",
    "end": "4394159"
  },
  {
    "text": "match also if you have uh data in Hive so Hive was the original SQL query tool",
    "start": "4394159",
    "end": "4400120"
  },
  {
    "text": "in Hadoop and a lot of people have data organized in Hive tables it's really just files under into the hood and hdfs",
    "start": "4400120",
    "end": "4407159"
  },
  {
    "text": "but if you have that kind of infrastructure you can easily use spark SQL with it so that's that's really",
    "start": "4407159",
    "end": "4412880"
  },
  {
    "text": "useful and they even have an ability for uh to work with Json so if you have documents records formatted as Json they",
    "start": "4412880",
    "end": "4419840"
  },
  {
    "text": "can very intelligently figure out the schema uh even write to Jason and so",
    "start": "4419840",
    "end": "4425040"
  },
  {
    "text": "forth so that's a nice little feature that's kind of useful so let's uh let's look at example so what I did for",
    "start": "4425040",
    "end": "4431120"
  },
  {
    "text": "Simplicity is I just took that crawl data that we were talking about as the output that would go into inverted index",
    "start": "4431120",
    "end": "4437880"
  },
  {
    "text": "the reason I didn't use the inverted index data is it's it was just a little too complicated to work with a variable",
    "start": "4437880",
    "end": "4443760"
  },
  {
    "text": "number of things in the second column because you know obviously the number of documents will vary depending on the word it was just a little too",
    "start": "4443760",
    "end": "4450199"
  },
  {
    "text": "complicated for this talked so some of the Imports look the same but now we've got some new things",
    "start": "4450199",
    "end": "4456440"
  },
  {
    "text": "that like the with the word SQL in them uh and schema rdd uh this case class thing this is a",
    "start": "4456440",
    "end": "4463440"
  },
  {
    "text": "fancy way of just declaring like a record type it's really just a regular class in Java isms but the the keyword",
    "start": "4463440",
    "end": "4469960"
  },
  {
    "text": "case adds a bunch of functions and things but basically we're going to say our crawl record is a two element thing",
    "start": "4469960",
    "end": "4477800"
  },
  {
    "text": "uh the first element of type string which is the document ID and the second element is also typ string it's the",
    "start": "4477800",
    "end": "4483080"
  },
  {
    "text": "contents and then I'm adding this little uh once again one of those Singleton objects that with a parse method this is",
    "start": "4483080",
    "end": "4489480"
  },
  {
    "text": "just details for how I might parse just an arbitrary bit of text into one one of those crawl record things for each line",
    "start": "4489480",
    "end": "4496719"
  },
  {
    "text": "that I'm going to read in so you know the details aren't that important but just for completeness and then I've",
    "start": "4496719",
    "end": "4501960"
  },
  {
    "text": "added this do SQL method this is just a helper method the key thing is that the SQL method is something that comes with",
    "start": "4501960",
    "end": "4509159"
  },
  {
    "text": "spark SQL I I'm going to pass in a string that's a SQL query as you'll see in a minute and then the rest of this",
    "start": "4509159",
    "end": "4515280"
  },
  {
    "text": "bit is just say all right take the first 100 elements from the result set and print them out that's all that's the rest of it's doing so it's just a helper",
    "start": "4515280",
    "end": "4521840"
  },
  {
    "text": "function the interesting bit is that SQL function there that near the",
    "start": "4521840",
    "end": "4527599"
  },
  {
    "text": "bottom all right so I'll uh now it's near the top uh I'm going to you know",
    "start": "4527679",
    "end": "4532719"
  },
  {
    "text": "crawl the data from some path I'm going to create a spark context like I did before I didn't really talk about the",
    "start": "4532719",
    "end": "4538719"
  },
  {
    "text": "arguments but they're not that important right now uh and then I have a little idiom for using a basically a for Loop",
    "start": "4538719",
    "end": "4544520"
  },
  {
    "text": "to read each line of text uh call that parse method on it to return what will be one of those rdds so so crawl the",
    "start": "4544520",
    "end": "4551800"
  },
  {
    "text": "type of crawl will be an rdd that's parameterized by that crawl record so",
    "start": "4551800",
    "end": "4556880"
  },
  {
    "text": "it'll be typed Fields instead of just a bunch of strings or whatever uh there's some fancy stuff I'm doing here that",
    "start": "4556880",
    "end": "4563280"
  },
  {
    "text": "handles like bad records um you know if you end up with a blank line or something that automatically just kind of throws it away um but anyway the",
    "start": "4563280",
    "end": "4570800"
  },
  {
    "text": "details aren't that important what's cool though is that once I've read in this data that has this schema I can",
    "start": "4570800",
    "end": "4577600"
  },
  {
    "text": "call this thing registers table which is sort of a fiction really it just basically creates sort of an in-memory",
    "start": "4577600",
    "end": "4583560"
  },
  {
    "text": "set of metadata about it I can't get at it from anywhere else it's not going to persist once the job is done but it lets",
    "start": "4583560",
    "end": "4589880"
  },
  {
    "text": "me write SQL queries against it uh for any rdd you can cach it in memory if you know you're going to be going over and",
    "start": "4589880",
    "end": "4595920"
  },
  {
    "text": "over it's sort of a hint to the system like this is important data I'm going to keep reading it so keep it in cash if",
    "start": "4595920",
    "end": "4601120"
  },
  {
    "text": "you can and there's a nice uh little method to print the schema just to the console the interesting bit though is",
    "start": "4601120",
    "end": "4607800"
  },
  {
    "text": "you know look at that do sequel hopefully this looks very familiar I'm assuming everybody here has probably written a SQL query at some point in",
    "start": "4607800",
    "end": "4614040"
  },
  {
    "text": "your life so notice that it knows that doc ID and contents are two fields that",
    "start": "4614040",
    "end": "4619120"
  },
  {
    "text": "were in that crawl record class I probably should have called that out earlier and it knows that by that first",
    "start": "4619120",
    "end": "4625199"
  },
  {
    "text": "register is table I I'm going to call my table crawl so I'm just writing SQL I could have written this with the regular",
    "start": "4625199",
    "end": "4631520"
  },
  {
    "text": "rdd API of course but if you know SQL you know you just bang these things out really fast and in fact it's really nice",
    "start": "4631520",
    "end": "4638520"
  },
  {
    "text": "when you get to things like joins and group buys and stuff like that um now the next bit is",
    "start": "4638520",
    "end": "4645400"
  },
  {
    "text": "uh so the the first output isn't all that interesting because it's just going to print like the document URL and then",
    "start": "4645400",
    "end": "4650600"
  },
  {
    "text": "that long list of uh of text what I did in the second bit of fancy looking code",
    "start": "4650600",
    "end": "4656880"
  },
  {
    "text": "is I just went ahead and created another table that's actually going to put the document ID in each word uh repeated so",
    "start": "4656880",
    "end": "4664600"
  },
  {
    "text": "I'll have a lot of duplication of document IDs one for each word so that's all I'm doing in the second bit it's a",
    "start": "4664600",
    "end": "4670000"
  },
  {
    "text": "little fancy looking but and I'm I'm just reusing the crawl record too you can see that word disappeared again once",
    "start": "4670000",
    "end": "4676760"
  },
  {
    "text": "again I'll register this I probably should have cached it because this is actually the one I'm querying over most",
    "start": "4676760",
    "end": "4682639"
  },
  {
    "text": "but now I'm just writing a bunch of queries you know another one is Select star statement select distinct and look",
    "start": "4682639",
    "end": "4688920"
  },
  {
    "text": "for the word management you know which documents have that word in it and you can all do all this from a reppel too I",
    "start": "4688920",
    "end": "4694520"
  },
  {
    "text": "don't have to write a program compile it you know walk over to the IT guy and hand it to them on a you know a paper",
    "start": "4694520",
    "end": "4700520"
  },
  {
    "text": "tape or whatever I used to have to do when I learned how to program um no I can just actually bring up a redev Val",
    "start": "4700520",
    "end": "4706880"
  },
  {
    "text": "print Loop an interactive console and actually type these things in uh and the last one I'm doing a group",
    "start": "4706880",
    "end": "4714120"
  },
  {
    "text": "by so and you know actually I actually left off cut off the slide at the end",
    "start": "4714120",
    "end": "4719679"
  },
  {
    "text": "there but um the point is I can mix and match the API that looks so nice I can",
    "start": "4719679",
    "end": "4725480"
  },
  {
    "text": "write SQL when that's what I know um I can give the SQL console there's actually even a custom SQL console where",
    "start": "4725480",
    "end": "4732080"
  },
  {
    "text": "it looks just like any SQL console you've ever used used give that to my data analysts if they want to play with",
    "start": "4732080",
    "end": "4737719"
  },
  {
    "text": "this data and they don't have a clue what Scola is and it just all works all right I also said that we need",
    "start": "4737719",
    "end": "4744440"
  },
  {
    "text": "to have streaming support so let's have a look at that let me see how I'm doing for",
    "start": "4744440",
    "end": "4750760"
  },
  {
    "text": "time so um we'd like to be able to process events as they come in and you",
    "start": "4750760",
    "end": "4756320"
  },
  {
    "text": "may have heard of tools like storm and there's message cues and so forth that are specifically designed for handling",
    "start": "4756320",
    "end": "4762040"
  },
  {
    "text": "like individual events as they arrive and doing some logic the the the tact they took to kind of",
    "start": "4762040",
    "end": "4767840"
  },
  {
    "text": "reuse all this infrastructure they already built is well what if we just handle the case where it's okay to wait",
    "start": "4767840",
    "end": "4774080"
  },
  {
    "text": "a few seconds or even a few minutes and just capture all the events that come in you know in certain time slices and then",
    "start": "4774080",
    "end": "4781800"
  },
  {
    "text": "put each of those time slices in one of these rdd things and then I've got all this the infrastructure that I can apply",
    "start": "4781800",
    "end": "4787679"
  },
  {
    "text": "to it that we've just seen including the SQL stuff so that's the idea they came up with so you wouldn't use this if you",
    "start": "4787679",
    "end": "4794280"
  },
  {
    "text": "really do have to handle each event in some unique way or or very quickly but if if all you're doing is like you know",
    "start": "4794280",
    "end": "4800560"
  },
  {
    "text": "updating running averages or whatever then this is perfect so each of these slices uh will",
    "start": "4800560",
    "end": "4808679"
  },
  {
    "text": "be an rdd they use the term slice or batch for those time slices and they",
    "start": "4808679",
    "end": "4813960"
  },
  {
    "text": "also give us some window function so if I want to do like a moving average over the last 10 windows or something I can",
    "start": "4813960",
    "end": "4819239"
  },
  {
    "text": "easily do that in fact that's what we'll see so schematically it looks like this events coming in from the left",
    "start": "4819239",
    "end": "4825239"
  },
  {
    "text": "it's called a dam for discretized stream the number of events in each time slice",
    "start": "4825239",
    "end": "4830480"
  },
  {
    "text": "will vary depending on you know the burst and traffic and whatnot and then I'm showing how I might have a moving",
    "start": "4830480",
    "end": "4836040"
  },
  {
    "text": "window that I'm doing some calculations over so that's sort of the extension I get on top of um rdds all right so now",
    "start": "4836040",
    "end": "4843639"
  },
  {
    "text": "now let's imagine that actually this crawl data is coming in live instead of it's been parked on a hard drive for 6",
    "start": "4843639",
    "end": "4849120"
  },
  {
    "text": "hours or whatever and now I want to process it so how might it look if I want to work with it live well well in",
    "start": "4849120",
    "end": "4854920"
  },
  {
    "text": "fact most of the code is pretty similar to what we already saw uh there's some stuff at the top about um well first I",
    "start": "4854920",
    "end": "4861800"
  },
  {
    "text": "created a spark context as before and then I wrap it in a streaming context and that gives me these extra bits for",
    "start": "4861800",
    "end": "4868880"
  },
  {
    "text": "streaming but I can also wrap it in the SQL context I was using in the last example so I can write SQL queries as",
    "start": "4868880",
    "end": "4875360"
  },
  {
    "text": "well as use rdd functions uh there's some logic you might put in about you know a stream",
    "start": "4875360",
    "end": "4880400"
  },
  {
    "text": "listener that would listen for the end of the stream or the stream fails unexpected ly that kind of",
    "start": "4880400",
    "end": "4886639"
  },
  {
    "text": "stuff um and then I have this little bit of flat map stuff where because I'm getting bites coming in over a socket is",
    "start": "4886639",
    "end": "4893480"
  },
  {
    "text": "what I'm going to read I'm just going to split each uh you chunk of bites on new lines and that'll be my records so",
    "start": "4893480",
    "end": "4900320"
  },
  {
    "text": "that's all that's doing now the next bits are pretty much what we saw before uh this for Loop",
    "start": "4900320",
    "end": "4906520"
  },
  {
    "text": "actually combines that stuff where I initially had the each line parsed and",
    "start": "4906520",
    "end": "4911920"
  },
  {
    "text": "then I later split it into words I'm just doing it all in one step so each of these yield crawl record things that's",
    "start": "4911920",
    "end": "4919400"
  },
  {
    "text": "going to be the uh document ID and the word document ID word and so forth and",
    "start": "4919400",
    "end": "4925520"
  },
  {
    "text": "then I set up a window function so every uh I didn't oops I kind of skipped past something important uh one of the",
    "start": "4925520",
    "end": "4932000"
  },
  {
    "text": "arguments that you give when you set up the streaming context is how wide you want these time slices to be the the",
    "start": "4932000",
    "end": "4937880"
  },
  {
    "text": "smallest is about a second that's that reasonably works so you couldn't do like millisecond",
    "start": "4937880",
    "end": "4943360"
  },
  {
    "text": "resolution but but you know 60 seconds or longer is fine so I'm going to do every 60 seconds and I'm going to do uh",
    "start": "4943360",
    "end": "4950400"
  },
  {
    "text": "when functions over the last five of those is what this is setting up and then I use this special function for",
    "start": "4950400",
    "end": "4956440"
  },
  {
    "text": "each rdd go ahead and make a table out of it and then write this query to this group by query that we saw on the",
    "start": "4956440",
    "end": "4962760"
  },
  {
    "text": "previous slide with the line that I cut off that's the last line that I left off on the previous",
    "start": "4962760",
    "end": "4968520"
  },
  {
    "text": "slide uh and here there's an explicit statement to start the stream processing and then wait for it to finish",
    "start": "4968520",
    "end": "4975239"
  },
  {
    "text": "so there's a little bit of more setup before and after but then in the middle you've got basically all the same stuff",
    "start": "4975239",
    "end": "4980400"
  },
  {
    "text": "that we've been using all along uh so you get a lot of reuse that",
    "start": "4980400",
    "end": "4986600"
  },
  {
    "text": "way all right so that's uh my little um I don't know propaganda maybe for spark",
    "start": "4986600",
    "end": "4994239"
  },
  {
    "text": "let's talk about SQL or SQL so it was kind of a typical Java",
    "start": "4994239",
    "end": "4999360"
  },
  {
    "text": "developer this is a confession actually um until you know like say four five",
    "start": "4999360",
    "end": "5004800"
  },
  {
    "text": "years ago when I started doing big data Consulting and I was like a lot of Java developers where I was kind of dismissive of databases you know I knew",
    "start": "5004800",
    "end": "5011880"
  },
  {
    "text": "you had to have them but uh I I really wanted to you know suck that stuff right into my object model in memory through",
    "start": "5011880",
    "end": "5017679"
  },
  {
    "text": "the OM and then just you do the joins in memory or anything to to avoid writing",
    "start": "5017679",
    "end": "5022920"
  },
  {
    "text": "SQL but what I really realized is just how powerful and concise and useful SQL",
    "start": "5022920",
    "end": "5028280"
  },
  {
    "text": "is uh when I was doing big data Consulting so I became sort of a late believer I guess but here's the",
    "start": "5028280",
    "end": "5033880"
  },
  {
    "text": "interesting thing so the basic thing is you know why did we actually say or start to think negatively of SQL you",
    "start": "5033880",
    "end": "5040280"
  },
  {
    "text": "know besides dealing with dbas or whatever um you what was it that made us think that no SQL was important and then",
    "start": "5040280",
    "end": "5046960"
  },
  {
    "text": "how do we go back or or why would we go back so let's talk about that just quickly why did we actually invent no",
    "start": "5046960",
    "end": "5053199"
  },
  {
    "text": "SQL in the first place well the first reason was that the early Pioneers in",
    "start": "5053199",
    "end": "5058400"
  },
  {
    "text": "the internet uh the you know the uh yahoos and Amazon and then later Googles",
    "start": "5058400",
    "end": "5063800"
  },
  {
    "text": "realized that they could not manage the amount of data they were getting with",
    "start": "5063800",
    "end": "5069120"
  },
  {
    "text": "traditional relational technology at the time uh most of the SQL databases scale",
    "start": "5069120",
    "end": "5074600"
  },
  {
    "text": "a lot better now than they did you know in the mid 9s but it just wasn't practical you just couldn't even do it",
    "start": "5074600",
    "end": "5080040"
  },
  {
    "text": "in most cases uh never mind what the cost was so they needed some way to manage all this data but keep the cost",
    "start": "5080040",
    "end": "5087239"
  },
  {
    "text": "to a reasonable size and the other thing is we we it really became apparent that we needed to be more flexible about how",
    "start": "5087239",
    "end": "5094080"
  },
  {
    "text": "we thought about things like consistency and availability so the famous cap theorem came about where you know if I",
    "start": "5094080",
    "end": "5101320"
  },
  {
    "text": "like cut the cable between uh Europe and North America and let's say Amazon's",
    "start": "5101320",
    "end": "5106840"
  },
  {
    "text": "catalog is only in North America you know am I just going to take Amazon offline in Europe because I can't be",
    "start": "5106840",
    "end": "5112119"
  },
  {
    "text": "sure the catalog is up to date or is it better to just go ahead and show stale data Maybe you know some people are",
    "start": "5112119",
    "end": "5118159"
  },
  {
    "text": "going to buy something that's already sold out but you know I'll just give them a gift certificate or something but they can still do work and and once the",
    "start": "5118159",
    "end": "5125480"
  },
  {
    "text": "cable is restored then I can you know eventually get things consistent so that was sort of the trade-off we realized",
    "start": "5125480",
    "end": "5131440"
  },
  {
    "text": "that we needed to make so a lot of the nosql databases give us that that ability to either in some cases even",
    "start": "5131440",
    "end": "5137520"
  },
  {
    "text": "tune whether we want consistency or availability in the face of partition tolerance but um you know the relational",
    "start": "5137520",
    "end": "5144840"
  },
  {
    "text": "databases are hard over on the idea that you're not available if you can't be consistent consistency rules whereas you",
    "start": "5144840",
    "end": "5151600"
  },
  {
    "text": "know databases like key value stores you even things like Cassandra and so forth often you'll want to just keep running",
    "start": "5151600",
    "end": "5157960"
  },
  {
    "text": "even if you're inconsistent and then eventually bring the consistency back so that was another thing we needed and it",
    "start": "5157960",
    "end": "5164960"
  },
  {
    "text": "you know not all data is relational many of you have probably used things like react where you're just storing massive basically key it's",
    "start": "5164960",
    "end": "5172040"
  },
  {
    "text": "basically a giant distributed hashmap you know keys and some sort of values not everything has to be",
    "start": "5172040",
    "end": "5178400"
  },
  {
    "text": "relational people like because they like storing Json documents and so forth so that was another reason",
    "start": "5178400",
    "end": "5184760"
  },
  {
    "text": "but on the other hand a lot of data actually does have structure and it turns out we kind of figured out pretty",
    "start": "5184760",
    "end": "5190360"
  },
  {
    "text": "quickly that yeah that the the nosql databases are solving important problems and they're not going away but there are",
    "start": "5190360",
    "end": "5196639"
  },
  {
    "text": "times when we actually do have structured data and there's nothing like SQL to talk to that data and um if you",
    "start": "5196639",
    "end": "5203320"
  },
  {
    "text": "go into a lot of companies where maybe there's you know a few dozen developers and then you know rooms of hundreds of",
    "start": "5203320",
    "end": "5208520"
  },
  {
    "text": "data analysts those those folks are not going to write Java or even Scola but they are going to write SQL very easily",
    "start": "5208520",
    "end": "5215199"
  },
  {
    "text": "because that's what they know so it was actually Facebook that had this problem first and they invented this tool Hive",
    "start": "5215199",
    "end": "5221080"
  },
  {
    "text": "that I mentioned that gave you SQL query semantics on top of data that was just",
    "start": "5221080",
    "end": "5226239"
  },
  {
    "text": "in a file system so it sort of reversed another model which was that the relational database could impose",
    "start": "5226239",
    "end": "5231639"
  },
  {
    "text": "consistency when you wrote the data it had total control over it you know the you had no idea how it was storing the",
    "start": "5231639",
    "end": "5237440"
  },
  {
    "text": "data what it was doing to it now we've flipped that so that maybe you know exactly where the data is you know at",
    "start": "5237440",
    "end": "5243920"
  },
  {
    "text": "the schema is it may or may not actually always be a you know according to that schema and these tools are going to try",
    "start": "5243920",
    "end": "5251199"
  },
  {
    "text": "their best to assume the scheme is right but you know have pre robust recovery",
    "start": "5251199",
    "end": "5256560"
  },
  {
    "text": "when a record shows up that's completely bogus so it gave us a new model so we end",
    "start": "5256560",
    "end": "5262960"
  },
  {
    "text": "actually ended up with two new approaches to SQL this is supposed to be sort of a frowny face it's a Lykan",
    "start": "5262960",
    "end": "5269840"
  },
  {
    "text": "anyway um the first was kind of what I just showed was spark SQL where let's layer a query engine on top of uh mostly",
    "start": "5269840",
    "end": "5278639"
  },
  {
    "text": "hdfs the distributed file system but these things are also now talking to databases like hbas and Cassandra and so",
    "start": "5278639",
    "end": "5287280"
  },
  {
    "text": "forth so if you have a Hado cluster that looks roughly like this where you've got some Masters where you submit jobs and",
    "start": "5287600",
    "end": "5294960"
  },
  {
    "text": "they're also responsible for the file system that's what that name note thing does and all the data in all the these",
    "start": "5294960",
    "end": "5300360"
  },
  {
    "text": "dis Farms basically that are managed by these other services",
    "start": "5300360",
    "end": "5305440"
  },
  {
    "text": "then I could have these query engines that are maybe written in map ruce maybe written in spark or maybe something",
    "start": "5305440",
    "end": "5310600"
  },
  {
    "text": "custom that's designed to be much more efficient than like a general purpose tool and that's in fact what happened",
    "start": "5310600",
    "end": "5317679"
  },
  {
    "text": "well first we're going to whatever we use we're going to submit jobs to this cluster and it's going to figure out how",
    "start": "5317679",
    "end": "5323080"
  },
  {
    "text": "to schedule things and then you know do this these tasks over all the",
    "start": "5323080",
    "end": "5328840"
  },
  {
    "text": "data yeah the data is there all right well as I said a second ago and I",
    "start": "5328840",
    "end": "5335040"
  },
  {
    "text": "forget how many builds I have here there we go so Hive was the the like the prototypical thing it was written in Ma",
    "start": "5335040",
    "end": "5340800"
  },
  {
    "text": "ruce so Hive queries are always slow because map reduce is slow but they can",
    "start": "5340800",
    "end": "5346000"
  },
  {
    "text": "work over pedabytes of data if you're willing to wait and actually there's a new query",
    "start": "5346000",
    "end": "5353119"
  },
  {
    "text": "there's an alternative to spark called tez that probably won't really take off because spark has all the you know the",
    "start": "5353119",
    "end": "5359360"
  },
  {
    "text": "marketing attention I guess but they have replaced the map produce inter with",
    "start": "5359360",
    "end": "5364600"
  },
  {
    "text": "TZ and so it is actually even Hive now is faster than it used to be which is a good thing but then spark actually originally",
    "start": "5364600",
    "end": "5372080"
  },
  {
    "text": "actually ported The Hive query engine on top of spark and they called that shark because they're clever um and that that",
    "start": "5372080",
    "end": "5378840"
  },
  {
    "text": "was actually really great for a while you could just take your uh all the things you did with Hive replace shark as your query engine suddenly things",
    "start": "5378840",
    "end": "5385199"
  },
  {
    "text": "were 30 to 100 times faster because of that in-memory caching I mentioned but they actually got tired of",
    "start": "5385199",
    "end": "5391719"
  },
  {
    "text": "working with the Facebook uh uh d or rather code that was inside Hive so they've actually deprecated that and",
    "start": "5391719",
    "end": "5398719"
  },
  {
    "text": "this new SE spark sequel thing I showed you is the new hotness as far as the shark or spark people are concerned but",
    "start": "5398719",
    "end": "5406239"
  },
  {
    "text": "then there's also been some custom query engines the the the current champion for performance is cloudas Impala which is",
    "start": "5406239",
    "end": "5413280"
  },
  {
    "text": "lightning fast you basically create your tables with Hive but then when you query them with Impala you know you get",
    "start": "5413280",
    "end": "5419040"
  },
  {
    "text": "queries that come back you know in milliseconds if for small data sets or seconds for really large ones and",
    "start": "5419040",
    "end": "5425080"
  },
  {
    "text": "there's some competitors that have emerged drill and Presto just for completeness there's even a few I left",
    "start": "5425080",
    "end": "5430119"
  },
  {
    "text": "off but but anyway the point being that um this has been a very rich field for",
    "start": "5430119",
    "end": "5435400"
  },
  {
    "text": "innovation of getting the Best of Both Worlds data that's parked in hdfs or hbas or whatever I can run SQL queries",
    "start": "5435400",
    "end": "5443159"
  },
  {
    "text": "against it I can run map produce or spark jobs against the same data it's a very rich way of doing things lots of",
    "start": "5443159",
    "end": "5449000"
  },
  {
    "text": "options but there's also this sort of new SQL movement where a lot of experts",
    "start": "5449000",
    "end": "5454440"
  },
  {
    "text": "especially indust rather academic um luminaries like Stone breaker and those kind of people have taken the lessons",
    "start": "5454440",
    "end": "5461600"
  },
  {
    "text": "from what we learned about scaling nosql databases and brought it back to uh relational models and distributed",
    "start": "5461600",
    "end": "5467440"
  },
  {
    "text": "transactions and created this new buzzword new SQL and I'm just going to list these without going into detail but",
    "start": "5467440",
    "end": "5474199"
  },
  {
    "text": "it um they should have F1 next to spanner but Google now has this a incredible globally distributed database",
    "start": "5474199",
    "end": "5480520"
  },
  {
    "text": "with global transactions they put atomic clocks in in the data center so they can",
    "start": "5480520",
    "end": "5485679"
  },
  {
    "text": "get as close as you can get to coordinating time across uh you know the speed of light time gaps and crazy stuff",
    "start": "5485679",
    "end": "5492639"
  },
  {
    "text": "like that but there's some others here that uh some of them are open source most of them are commercial that are trying to take us to the next level but",
    "start": "5492639",
    "end": "5499560"
  },
  {
    "text": "bring us back to the relational model when that's the right thing so that's a pretty cool thing to watch so to kind of",
    "start": "5499560",
    "end": "5505600"
  },
  {
    "text": "wrap this up what are some uh conclusion or rather what are some things we might be able to speculate about looking",
    "start": "5505600",
    "end": "5511040"
  },
  {
    "text": "forward you into the fog of the future I guess um I mentioned that you can run spark on",
    "start": "5511040",
    "end": "5518199"
  },
  {
    "text": "this new distributed computing platform called mesos and I actually think that that's going to grow in importance that",
    "start": "5518199",
    "end": "5523800"
  },
  {
    "text": "people that don't necessarily need Hadoop will actually start using mesos because it gives them a lot more",
    "start": "5523800",
    "end": "5529560"
  },
  {
    "text": "flexibility both to run spark with other kinds of applications that have nothing to do with big data but run fine on",
    "start": "5529560",
    "end": "5535960"
  },
  {
    "text": "mesos but even for some deployments and so-called Big Data scenarios I think it'll be a of growing",
    "start": "5535960",
    "end": "5542840"
  },
  {
    "text": "importance uh and that's further supports this uh what's already really nice about spark is we get a lot of",
    "start": "5542840",
    "end": "5548560"
  },
  {
    "text": "flexibility about how we deploy uh infrastructure we can deploy spark in a lot of different you know cloud or uh",
    "start": "5548560",
    "end": "5556520"
  },
  {
    "text": "actual Hardware environments I think you should pay attention to tach on if you're interested in this space because I think",
    "start": "5556520",
    "end": "5562440"
  },
  {
    "text": "that will be disruptive when it's finally uh you know production ready that's that thing that generalizes the",
    "start": "5562440",
    "end": "5568080"
  },
  {
    "text": "cach to be something that a bunch of uh applications could share data in memory but sort of a a file system",
    "start": "5568080",
    "end": "5576840"
  },
  {
    "text": "model uh I mentioned H H2O it's actually open source it's a",
    "start": "5576840",
    "end": "5582600"
  },
  {
    "text": "really wonderful compute engine for certain kinds of problems really good machine learning libraries and Cliff",
    "start": "5582600",
    "end": "5588880"
  },
  {
    "text": "click talked about it I think earlier today so to",
    "start": "5588880",
    "end": "5594560"
  },
  {
    "text": "recap um spark is replacing map reduce because we really needed to do",
    "start": "5594560",
    "end": "5601560"
  },
  {
    "text": "it uh it gives us the ability to do both the traditional batch mode processing and",
    "start": "5601560",
    "end": "5607639"
  },
  {
    "text": "streaming uh it integrates it creates a foundation for building a bunch of other compute models like SQL like graph",
    "start": "5607639",
    "end": "5614320"
  },
  {
    "text": "Theory you know like machine learning libraries but it also works really well",
    "start": "5614320",
    "end": "5620119"
  },
  {
    "text": "in Hadoop and it'll you there's some rough edges that are getting fixed but it's it really is the future for writing",
    "start": "5620119",
    "end": "5625800"
  },
  {
    "text": "jobs in Hadoop in my opinion and we're much better off for it but also keep in",
    "start": "5625800",
    "end": "5631239"
  },
  {
    "text": "mind that people are fixing SQL to make it more relevant for large data sets or distributed environments and so forth so",
    "start": "5631239",
    "end": "5637480"
  },
  {
    "text": "there's a lot of interesting innovation in this space as well but that doesn't mean that no",
    "start": "5637480",
    "end": "5644480"
  },
  {
    "text": "sequel's gone away I think we're just sort sort of really appreciating the strengths and weaknesses and reassessing and the Pendulum is kind of swinging",
    "start": "5644480",
    "end": "5650880"
  },
  {
    "text": "back which is always a good thing and uh I think we'll see mesos",
    "start": "5650880",
    "end": "5656080"
  },
  {
    "text": "grow in importance I actually have um some white",
    "start": "5656080",
    "end": "5661440"
  },
  {
    "text": "papers on the Apache site sorry the typ safe site and uh I'm I'm doing a one-day",
    "start": "5661440",
    "end": "5667520"
  },
  {
    "text": "workshop on spark if you want to learn about it also we have this thing called activator there's a spark Workshop you",
    "start": "5667520",
    "end": "5672560"
  },
  {
    "text": "can do on your own time as well if if you're interested in this you can talk to me later but uh that's it thank you",
    "start": "5672560",
    "end": "5678840"
  },
  {
    "text": "very [Applause]",
    "start": "5678840",
    "end": "5686909"
  },
  {
    "text": "much",
    "start": "5687280",
    "end": "5690280"
  }
]