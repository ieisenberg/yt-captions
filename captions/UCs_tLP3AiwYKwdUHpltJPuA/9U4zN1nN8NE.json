[
  {
    "start": "0",
    "end": "70000"
  },
  {
    "text": "[Music]",
    "start": "960",
    "end": "8889"
  },
  {
    "text": "hi everyone I hope you can hear me really those at the back yeah great",
    "start": "12480",
    "end": "19500"
  },
  {
    "text": "so my name is manipa and I'll be talking about addressing algorithmic buys today",
    "start": "19500",
    "end": "25740"
  },
  {
    "text": "so let's start and have a look at what is bias",
    "start": "25740",
    "end": "30779"
  },
  {
    "text": "but before that I would like to ask can you guys read it",
    "start": "30779",
    "end": "36239"
  },
  {
    "text": "yeah you guys can read it and if you are English speaker so it shouldn't be",
    "start": "36239",
    "end": "42480"
  },
  {
    "text": "problem that you could read it and you could understand it despite the fact that the letters in the words are all",
    "start": "42480",
    "end": "50340"
  },
  {
    "text": "mixed up and they are not at the the right position so why this happens this",
    "start": "50340",
    "end": "56460"
  },
  {
    "text": "happens because our brain has this ability to create the shortcuts which",
    "start": "56460",
    "end": "61620"
  },
  {
    "text": "help us understand the things so it makes the things easier for us to grasp",
    "start": "61620",
    "end": "67260"
  },
  {
    "text": "and this ability of brain to make things simplified for us",
    "start": "67260",
    "end": "72900"
  },
  {
    "start": "70000",
    "end": "223000"
  },
  {
    "text": "is called this functionality is called heuristics so heuristics are basically shortcuts of",
    "start": "72900",
    "end": "79560"
  },
  {
    "text": "the brain that help us understand uh the things easily in different situations",
    "start": "79560",
    "end": "85380"
  },
  {
    "text": "and besides that it also makes things easier for us and these shortcuts",
    "start": "85380",
    "end": "93380"
  },
  {
    "text": "make help help us make sense of the things",
    "start": "93380",
    "end": "98640"
  },
  {
    "text": "so these heuristics normally are very helpful to us because they are making",
    "start": "98640",
    "end": "105420"
  },
  {
    "text": "the task easy but in doing so sometimes they create the biases in",
    "start": "105420",
    "end": "112140"
  },
  {
    "text": "what are biases biases is basically are inclination our favoritism",
    "start": "112140",
    "end": "119579"
  },
  {
    "text": "are the Discrimination towards one thing and giving favoritism to one group of",
    "start": "119579",
    "end": "128940"
  },
  {
    "text": "people objects Etc and having bias towards others",
    "start": "128940",
    "end": "135900"
  },
  {
    "text": "so if there are different kind of heuristics familiarity heuristic",
    "start": "135900",
    "end": "141360"
  },
  {
    "text": "representative heuristic familiarity heuristic is basically the pattern of",
    "start": "141360",
    "end": "146520"
  },
  {
    "text": "favoring what is similar or what is familiar to us this can lead to",
    "start": "146520",
    "end": "152160"
  },
  {
    "text": "stethoscope wise a preference for the current state there's not willing to",
    "start": "152160",
    "end": "158520"
  },
  {
    "text": "come out of the comfort zone so this status quo based off the based",
    "start": "158520",
    "end": "167280"
  },
  {
    "text": "on this to the school 86 percent of CEOs and 500 Fortune companies",
    "start": "167280",
    "end": "172739"
  },
  {
    "text": "are male white and since this themselves are white",
    "start": "172739",
    "end": "179160"
  },
  {
    "text": "males so they prefer hiring the wild maid because they are familiar to them they are familiar working with them they",
    "start": "179160",
    "end": "186180"
  },
  {
    "text": "are familiar in recognizing or relating to them then similarly if we are talking about",
    "start": "186180",
    "end": "193080"
  },
  {
    "text": "representative heuristics so it can lead to the stereotyping",
    "start": "193080",
    "end": "199379"
  },
  {
    "text": "the unconscious attribution of particular qualities a member of a",
    "start": "199379",
    "end": "205019"
  },
  {
    "text": "certain social group example imagine nurse so what comes in your mind is it a",
    "start": "205019",
    "end": "211019"
  },
  {
    "text": "woman why because this this is stereotype and it has been produced",
    "start": "211019",
    "end": "217379"
  },
  {
    "text": "because of this heuristics the ability of our brain to simplify the things",
    "start": "217379",
    "end": "223280"
  },
  {
    "start": "223000",
    "end": "285000"
  },
  {
    "text": "this is a the cognitive biased codex",
    "start": "224040",
    "end": "231000"
  },
  {
    "text": "which has been developed by John mangyan III and Buster Benson",
    "start": "231000",
    "end": "237659"
  },
  {
    "text": "this gets updated on daily basis and to this date there are 150 plus biases",
    "start": "237659",
    "end": "245940"
  },
  {
    "text": "registered in this codex all of these biases exist because of the",
    "start": "245940",
    "end": "251040"
  },
  {
    "text": "cognitive limitations that our brain has so these cognitive",
    "start": "251040",
    "end": "257639"
  },
  {
    "text": "limitations create these biases in us so all of you",
    "start": "257639",
    "end": "262800"
  },
  {
    "text": "sitting here you can't deny that we are biased so unfortunately because of these",
    "start": "262800",
    "end": "268560"
  },
  {
    "text": "cognitive limitations we are bound to be biased what we can do is we can just",
    "start": "268560",
    "end": "276060"
  },
  {
    "text": "overcome these biases by understanding these biases so this was the human side of the story",
    "start": "276060",
    "end": "281940"
  },
  {
    "text": "that how we are biased so let's look into what is a algorithmic",
    "start": "281940",
    "end": "288060"
  },
  {
    "start": "285000",
    "end": "420000"
  },
  {
    "text": "bias then and how this bias basically transfer from humans to machines",
    "start": "288060",
    "end": "295280"
  },
  {
    "text": "algorithm make bind bias is basically defined as the",
    "start": "295280",
    "end": "301759"
  },
  {
    "text": "um when the outcome of the machine basically it is more inclined towards",
    "start": "301860",
    "end": "307500"
  },
  {
    "text": "one group when it is basically discriminatory",
    "start": "307500",
    "end": "313440"
  },
  {
    "text": "so the algorithms are then called the bias algorithms because their outcomes",
    "start": "313800",
    "end": "320780"
  },
  {
    "text": "are not equal for all groups that are involved",
    "start": "320780",
    "end": "326759"
  },
  {
    "text": "it means that might one group might be getting favored over the other",
    "start": "326759",
    "end": "332699"
  },
  {
    "text": "and it could be any proxy due to which this bias could occur",
    "start": "332699",
    "end": "338699"
  },
  {
    "text": "so algorithmic by machines or Italy or these algorithms they were designed to",
    "start": "338699",
    "end": "345720"
  },
  {
    "text": "address the bias that was in US humans",
    "start": "345720",
    "end": "351300"
  },
  {
    "text": "so algorithms were designed so that we could make the decision making",
    "start": "351300",
    "end": "356780"
  },
  {
    "text": "unbiased we could make it more rational we could make it maybe less discriminatory",
    "start": "356780",
    "end": "363720"
  },
  {
    "text": "but what happened that now in media we hear a lot about the discrimination from",
    "start": "363720",
    "end": "370620"
  },
  {
    "text": "the machines so these are some of the examples of the",
    "start": "370620",
    "end": "377460"
  },
  {
    "text": "bias that um that started originating as the machine",
    "start": "377460",
    "end": "383280"
  },
  {
    "text": "or machine learning algorithms or AI algorithms they came uh they started",
    "start": "383280",
    "end": "390120"
  },
  {
    "text": "getting popularity or they started getting implemented machine very first one that in 2015 Google's image",
    "start": "390120",
    "end": "397440"
  },
  {
    "text": "recognition system labeled African Americans as gorillas three years later in 2018",
    "start": "397440",
    "end": "405600"
  },
  {
    "text": "Amazon's Recreation system Drew criticism for matching 28 members of",
    "start": "405600",
    "end": "411300"
  },
  {
    "text": "Congress to criminal mugshots just based on their color",
    "start": "411300",
    "end": "418400"
  },
  {
    "start": "420000",
    "end": "897000"
  },
  {
    "text": "this one a very famous maybe some of you have heard about it",
    "start": "420060",
    "end": "426620"
  },
  {
    "text": "a recent study found that three facial recognitions from the top companies IBM",
    "start": "427680",
    "end": "434699"
  },
  {
    "text": "Microsoft and the Chinese image recognition system all were unable to identify a black",
    "start": "434699",
    "end": "442800"
  },
  {
    "text": "woman why this happened",
    "start": "442800",
    "end": "448199"
  },
  {
    "text": "this happened because the data that had been fed to the system",
    "start": "448199",
    "end": "454380"
  },
  {
    "text": "that was in educate there was not enough data related to",
    "start": "454380",
    "end": "460380"
  },
  {
    "text": "black woman or black people on which our algorithms got trained so therefore when",
    "start": "460380",
    "end": "466800"
  },
  {
    "text": "in real life when in deployment phase these uh systems",
    "start": "466800",
    "end": "473060"
  },
  {
    "text": "encountered someone black so they couldn't recognize them because it was",
    "start": "473060",
    "end": "478620"
  },
  {
    "text": "new for them it was some something new that they were not trained for",
    "start": "478620",
    "end": "484979"
  },
  {
    "text": "so it was basically limitation of humans that came into play when the machine",
    "start": "484979",
    "end": "491699"
  },
  {
    "text": "started uh working or they got deployed",
    "start": "491699",
    "end": "497300"
  },
  {
    "text": "similarly if you are a person with a dark skin",
    "start": "499020",
    "end": "504180"
  },
  {
    "text": "you may you are you have more chances to get an",
    "start": "504180",
    "end": "510720"
  },
  {
    "text": "accident or get hit by self-driving cars again the same reason because",
    "start": "510720",
    "end": "517740"
  },
  {
    "text": "the data set that has been used to train self-driving cars it did not have",
    "start": "517740",
    "end": "524880"
  },
  {
    "text": "sufficient examples of black people so it is more",
    "start": "524880",
    "end": "532440"
  },
  {
    "text": "um prone or it is can identify the white",
    "start": "532440",
    "end": "537720"
  },
  {
    "text": "people and can stop but if there is a pedestrian is a black person so there is",
    "start": "537720",
    "end": "544500"
  },
  {
    "text": "a chances that the car will unable to identify it and in return hit it",
    "start": "544500",
    "end": "552120"
  },
  {
    "text": "the authors of the self-driving car study not that couple of factors are",
    "start": "552120",
    "end": "558300"
  },
  {
    "text": "likely fueling the disparity in this case the first as I mentioned the is the case",
    "start": "558300",
    "end": "564360"
  },
  {
    "text": "of insufficient data the second is that the data even that was available",
    "start": "564360",
    "end": "570680"
  },
  {
    "text": "the machine learning algorithms did not train themselves from that data",
    "start": "570680",
    "end": "577920"
  },
  {
    "text": "why because they didn't give them more weightage to those examples",
    "start": "577920",
    "end": "584820"
  },
  {
    "text": "so we can address these kind of issues either by providing the right data a",
    "start": "584820",
    "end": "589980"
  },
  {
    "text": "right amount of data or we can address the issue by",
    "start": "589980",
    "end": "597120"
  },
  {
    "text": "putting the More Voltage to the less amount of data that we have so in this way we can protect the",
    "start": "597120",
    "end": "604080"
  },
  {
    "text": "minorities or the smaller groups um in the data set",
    "start": "604080",
    "end": "609600"
  },
  {
    "text": "now the second example or another example which is from Amazon",
    "start": "609600",
    "end": "617060"
  },
  {
    "text": "when in 2014 launched it's a AI based recruiting system so it",
    "start": "617660",
    "end": "625980"
  },
  {
    "text": "got lots of hype but in 2015 Amazon discovered that their recruiting system",
    "start": "625980",
    "end": "632880"
  },
  {
    "text": "was not gender neutral so it means that the algorithm was",
    "start": "632880",
    "end": "638220"
  },
  {
    "text": "giving more preference and actually um putting the",
    "start": "638220",
    "end": "645360"
  },
  {
    "text": "seaweed or resumes from the woman aside",
    "start": "645360",
    "end": "650839"
  },
  {
    "text": "so this was a big shock definitely um to all who were looking forward",
    "start": "651660",
    "end": "658019"
  },
  {
    "text": "towards AI based recruiting systems and all and Amazon as well",
    "start": "658019",
    "end": "665279"
  },
  {
    "text": "so what was the reason of this the reason was that there was a human bias",
    "start": "665279",
    "end": "671820"
  },
  {
    "text": "that got sweeped into the algorithms now you must be wondering how it was a",
    "start": "671820",
    "end": "677880"
  },
  {
    "text": "human bias when it was a ai-based recruiting system it was because the data that had been",
    "start": "677880",
    "end": "685200"
  },
  {
    "text": "provided to the AI algorithm again was the data that Amazon had captured or",
    "start": "685200",
    "end": "692339"
  },
  {
    "text": "stored in its databases or from the past 10 years",
    "start": "692339",
    "end": "699000"
  },
  {
    "text": "and since American tech industry is highly",
    "start": "699000",
    "end": "704120"
  },
  {
    "text": "dominated by male especially in Tech and I.T so therefore",
    "start": "704120",
    "end": "710459"
  },
  {
    "text": "the software developers and Amazon that were hired you can see like from the",
    "start": "710459",
    "end": "716940"
  },
  {
    "text": "graphs not just um Amazon but also other companies tech",
    "start": "716940",
    "end": "723540"
  },
  {
    "text": "companies as well they are all male dominated",
    "start": "723540",
    "end": "730440"
  },
  {
    "text": "so when this data got fed to the algorithm so algorithm assumed that",
    "start": "730440",
    "end": "737000"
  },
  {
    "text": "males are more preferable group for recruitment",
    "start": "737000",
    "end": "743040"
  },
  {
    "text": "so any resume that would contain the word woman for example woman chess club",
    "start": "743040",
    "end": "750360"
  },
  {
    "text": "player woman College woman would be discarded by the",
    "start": "750360",
    "end": "756300"
  },
  {
    "text": "or will be less preferable by the uh for the AI",
    "start": "756300",
    "end": "761820"
  },
  {
    "text": "or algorithms and hence discard it later on Amazon addressed that issue and",
    "start": "761820",
    "end": "769860"
  },
  {
    "text": "removed these preferences or these Proxes from their system but still we",
    "start": "769860",
    "end": "775980"
  },
  {
    "text": "cannot make sure that now the system will not learn new words based on which they will create discrimination",
    "start": "775980",
    "end": "784440"
  },
  {
    "text": "there are lots and lots of examples and from in all fields of life whether it be",
    "start": "784440",
    "end": "789540"
  },
  {
    "text": "a Health Care system or a criminal justice system",
    "start": "789540",
    "end": "794660"
  },
  {
    "text": "we have hundreds of examples which where basically AI is being biased",
    "start": "794940",
    "end": "803660"
  },
  {
    "text": "and it is not just discrimination discrimination discrimination isn't the",
    "start": "806100",
    "end": "811680"
  },
  {
    "text": "only potential problem with the uh these algorithms in one of the earliest examples of",
    "start": "811680",
    "end": "817800"
  },
  {
    "text": "problematic AI Microsoft released a Twitter chatbot called say",
    "start": "817800",
    "end": "824639"
  },
  {
    "text": "and the idea was that the chatbot will learn from the conversations with their",
    "start": "824639",
    "end": "830220"
  },
  {
    "text": "real human beings and within 24 hours the chatbot started sending",
    "start": "830220",
    "end": "838339"
  },
  {
    "text": "racist and buy streets",
    "start": "838339",
    "end": "843740"
  },
  {
    "text": "so the heart of the problem actually is not with the AI technology or the algorithms itself perceived but with how",
    "start": "845399",
    "end": "853620"
  },
  {
    "text": "the AI Power Systems are trained for this system to perform as desired",
    "start": "853620",
    "end": "860760"
  },
  {
    "text": "and outcomes to become increasingly accurate training data must be diverse",
    "start": "860760",
    "end": "868500"
  },
  {
    "text": "and it should offer the breadth of coverage since algorithmic systems",
    "start": "868500",
    "end": "875160"
  },
  {
    "text": "learn from the examples they are fed so we should",
    "start": "875160",
    "end": "880500"
  },
  {
    "text": "feed the appropriate data and very diverse data so it covers all",
    "start": "880500",
    "end": "886459"
  },
  {
    "text": "kind of examples so when they we are deployed in the real life situation so",
    "start": "886459",
    "end": "891660"
  },
  {
    "text": "we should not see any examples like this we have seen",
    "start": "891660",
    "end": "897079"
  },
  {
    "start": "897000",
    "end": "1024000"
  },
  {
    "text": "so how and when does the",
    "start": "897899",
    "end": "903060"
  },
  {
    "text": "bias enters the these systems so it is basically there",
    "start": "903060",
    "end": "909240"
  },
  {
    "text": "is no one step that where device enters the system it's actually throughout the Project Life Cycle there there could be",
    "start": "909240",
    "end": "916380"
  },
  {
    "text": "many occasions where the buys can enter the system AI system",
    "start": "916380",
    "end": "922019"
  },
  {
    "text": "but there are three main data entry points where the bias enters the system",
    "start": "922019",
    "end": "927779"
  },
  {
    "text": "the one is through the data the other one is through the people or developers",
    "start": "927779",
    "end": "933360"
  },
  {
    "text": "or the people who are working on these algorithms and the third is like when",
    "start": "933360",
    "end": "938699"
  },
  {
    "text": "the outcomes um the desired outcomes such as",
    "start": "938699",
    "end": "944880"
  },
  {
    "text": "recommendation content and serve populations um so through those outcomes the buy",
    "start": "944880",
    "end": "952079"
  },
  {
    "text": "skin sweep into the systems so people write the algorithms people",
    "start": "952079",
    "end": "957899"
  },
  {
    "text": "choose the data they use to feed those algorithms so therefore it's the people's responsibility or the",
    "start": "957899",
    "end": "963839"
  },
  {
    "text": "developer's responsibility that while choosing the data they make sure that the first thing is that they put the",
    "start": "963839",
    "end": "970560"
  },
  {
    "text": "right weightage to the data they include all the ethnic groups all their Races",
    "start": "970560",
    "end": "975600"
  },
  {
    "text": "they should protect the data they should get the clean data and if there is some",
    "start": "975600",
    "end": "981000"
  },
  {
    "text": "certain sensitive information so that should be hidden from the algorithms",
    "start": "981000",
    "end": "987540"
  },
  {
    "text": "so historical human biases are shaped by pervasive and often deeply embedded",
    "start": "987540",
    "end": "994079"
  },
  {
    "text": "prejudice against certain groups which can lead to their reproduction and amplification in computer models human",
    "start": "994079",
    "end": "1001160"
  },
  {
    "text": "biases can be reinforced and perpetuated without the user's knowledge",
    "start": "1001160",
    "end": "1007579"
  },
  {
    "text": "so if the data used to train the algorithm are more representative of some groups of people than others the",
    "start": "1007579",
    "end": "1014600"
  },
  {
    "text": "predictions from the model may also be systematically worse for unrepresented or underrepresentative",
    "start": "1014600",
    "end": "1021860"
  },
  {
    "text": "groups so what are the fair algorithms",
    "start": "1021860",
    "end": "1028520"
  },
  {
    "start": "1024000",
    "end": "1226000"
  },
  {
    "text": "so now we have seen the examples of the biased algorithms or we saw the example",
    "start": "1029059",
    "end": "1036798"
  },
  {
    "text": "the what are bias algorithms so therefore we have to work towards the",
    "start": "1036799",
    "end": "1043100"
  },
  {
    "text": "fair algorithms so fair algorithms are the algorithms which are not discriminatory towards one",
    "start": "1043100",
    "end": "1051620"
  },
  {
    "text": "group of people or the other so how we can make our algorithms affair",
    "start": "1051620",
    "end": "1061100"
  },
  {
    "text": "so when we create the models so they are mostly the predictive models which have",
    "start": "1061100",
    "end": "1067460"
  },
  {
    "text": "to answer the questions regarding okay if I go to apply the loan will I be",
    "start": "1067460",
    "end": "1073039"
  },
  {
    "text": "getting the loan is it the right person a candidate to give the loan is it the right candidate to give the job is it",
    "start": "1073039",
    "end": "1080299"
  },
  {
    "text": "the right candidate to be enrolled in the college so there are four kind of outcomes",
    "start": "1080299",
    "end": "1089660"
  },
  {
    "text": "yeah as you can see here in the diagram it could be there are",
    "start": "1089660",
    "end": "1095200"
  },
  {
    "text": "true positive false positive true negative and false negative",
    "start": "1095200",
    "end": "1100220"
  },
  {
    "text": "so false negative and false positive are actually the errors",
    "start": "1100220",
    "end": "1105559"
  },
  {
    "text": "that are made by the algorithm or the model so if these errors or these algorithm",
    "start": "1105559",
    "end": "1113720"
  },
  {
    "text": "these errors are distributed equally among all the groups that go",
    "start": "1113720",
    "end": "1121940"
  },
  {
    "text": "through these algorithms then we can say that our algorithm is a fair algorithm",
    "start": "1121940",
    "end": "1129740"
  },
  {
    "text": "what does it mean it means that if there are two group of",
    "start": "1129740",
    "end": "1136100"
  },
  {
    "text": "people group a and Group B group a have five",
    "start": "1136100",
    "end": "1141160"
  },
  {
    "text": "members and Group B also has five members and the error in both the groups is that",
    "start": "1141160",
    "end": "1148580"
  },
  {
    "text": "just one person who was the right candidate who would have returned the loan after the period",
    "start": "1148580",
    "end": "1156440"
  },
  {
    "text": "has been denied for the loan so it means that is an error that is a mistake",
    "start": "1156440",
    "end": "1161780"
  },
  {
    "text": "because our prediction algorithm was unable to predict it accurately",
    "start": "1161780",
    "end": "1168559"
  },
  {
    "text": "so he denied the right candidate so if among both the groups we our",
    "start": "1168559",
    "end": "1175340"
  },
  {
    "text": "algorithm make one error so then it means that it is a error rate",
    "start": "1175340",
    "end": "1182000"
  },
  {
    "text": "parity because it has made the equal error for both the groups",
    "start": "1182000",
    "end": "1188660"
  },
  {
    "text": "similarly we took the group of five people yeah group a was also five people and Group B",
    "start": "1188660",
    "end": "1195799"
  },
  {
    "text": "was also five people it means that to start with we started with",
    "start": "1195799",
    "end": "1202940"
  },
  {
    "text": "um statistical parity we gave the equal amount of data and the error that got",
    "start": "1202940",
    "end": "1208340"
  },
  {
    "text": "distributed uh on those groups that was also equal",
    "start": "1208340",
    "end": "1213380"
  },
  {
    "text": "so these are the two ways we can make our algorithms accurate or our uh not",
    "start": "1213380",
    "end": "1220340"
  },
  {
    "text": "accurate sorry our algorithms more fair because",
    "start": "1220340",
    "end": "1225559"
  },
  {
    "text": "there is a difference between um accuracy and fairness and this we will be discussing",
    "start": "1225559",
    "end": "1232460"
  },
  {
    "start": "1226000",
    "end": "1438000"
  },
  {
    "text": "so what this is the example here",
    "start": "1232460",
    "end": "1237740"
  },
  {
    "text": "you want to go for skiing yeah and there are there is this famous resort which is",
    "start": "1237740",
    "end": "1244100"
  },
  {
    "text": "selling only which has only 100 tickets to sell so what happens there what what is the",
    "start": "1244100",
    "end": "1250820"
  },
  {
    "text": "scenario now we want to choose via the using this algorithm or model that which",
    "start": "1250820",
    "end": "1256100"
  },
  {
    "text": "group or uh who should be basically getting the tickets for this Resort",
    "start": "1256100",
    "end": "1262700"
  },
  {
    "text": "this is quite famous resort and as you can see there are 700 skiers and uh",
    "start": "1262700",
    "end": "1268340"
  },
  {
    "text": "almost 300 snowboarders who are interested to get",
    "start": "1268340",
    "end": "1273440"
  },
  {
    "text": "the tickets for this Resort while there are only 100. and the",
    "start": "1273440",
    "end": "1278539"
  },
  {
    "text": "the algorithm should be able to generate the",
    "start": "1278539",
    "end": "1285620"
  },
  {
    "text": "profit while Distributing these tickets the scenario says that we have",
    "start": "1285620",
    "end": "1294039"
  },
  {
    "text": "40 um skills and 20 I guess",
    "start": "1294380",
    "end": "1302720"
  },
  {
    "text": "yeah 20 snowboarders that are more privileged and that can afford the",
    "start": "1302720",
    "end": "1309679"
  },
  {
    "text": "things better when they will be given given so it means there will be more profitable for the system",
    "start": "1309679",
    "end": "1317720"
  },
  {
    "text": "so what happens that now our system has to actually",
    "start": "1317720",
    "end": "1322960"
  },
  {
    "text": "choose that which one are the right candidates keeping again in the mind that we have to generate the",
    "start": "1322960",
    "end": "1329179"
  },
  {
    "text": "profits as well so the system divides the ticket equally",
    "start": "1329179",
    "end": "1334280"
  },
  {
    "text": "so the 50 people from the skiers group and 50 people from the snowboarders will",
    "start": "1334280",
    "end": "1340340"
  },
  {
    "text": "get the tickets we are talking about statistical parity",
    "start": "1340340",
    "end": "1346340"
  },
  {
    "text": "we are satisfied we say that okay our algorithm is fair because it has distributed",
    "start": "1346340",
    "end": "1352419"
  },
  {
    "text": "the tickets fairly let's look a little bit deeply into it now because this",
    "start": "1352419",
    "end": "1360200"
  },
  {
    "text": "scenario should generate the profit so when the tickets got distributed so 40",
    "start": "1360200",
    "end": "1365240"
  },
  {
    "text": "people among the skiers got selected and 10 people",
    "start": "1365240",
    "end": "1371720"
  },
  {
    "text": "um that that were privileged and 10 people that were underprivileged which were less spent which were going to",
    "start": "1371720",
    "end": "1378260"
  },
  {
    "text": "spend less got chosen similarly from the other group there were 20 snowboarders",
    "start": "1378260",
    "end": "1384440"
  },
  {
    "text": "um that were chosen that were privileged and the rest were not chosen",
    "start": "1384440",
    "end": "1391460"
  },
  {
    "text": "see that out of the 700 skiers who sent applications so 40 that is like 100",
    "start": "1391460",
    "end": "1399620"
  },
  {
    "text": "percent from the privileged group that got chosen so it means that the",
    "start": "1399620",
    "end": "1406159"
  },
  {
    "text": "the algorithm which was seemingly Fair because there was a static statistical",
    "start": "1406159",
    "end": "1411860"
  },
  {
    "text": "parity on the input and there was a equal distribution of the outcomes or",
    "start": "1411860",
    "end": "1417080"
  },
  {
    "text": "the tickets was not even very fair when we zoomed it in",
    "start": "1417080",
    "end": "1424240"
  },
  {
    "text": "why because there was only 1.5 percent that is 10 percent 10 tickets that were",
    "start": "1424400",
    "end": "1430880"
  },
  {
    "text": "sold to less privileged Group which we're going to spend less",
    "start": "1430880",
    "end": "1436539"
  },
  {
    "text": "so when while making the algorithms",
    "start": "1437539",
    "end": "1444080"
  },
  {
    "start": "1438000",
    "end": "1512000"
  },
  {
    "text": "there is a trade-off there is a trade-off between choosing the groups that we can save from the",
    "start": "1444080",
    "end": "1450980"
  },
  {
    "text": "buys or the individuals normally we have to compromise",
    "start": "1450980",
    "end": "1457720"
  },
  {
    "text": "on the individual preferences because there could be many factors that",
    "start": "1457720",
    "end": "1464000"
  },
  {
    "text": "are not under the control of the algorithm and then we the group comes",
    "start": "1464000",
    "end": "1471740"
  },
  {
    "text": "before then then the individuals so in a way we can say that achieving",
    "start": "1471740",
    "end": "1479360"
  },
  {
    "text": "100 percent fairness is kind of impossible",
    "start": "1479360",
    "end": "1486080"
  },
  {
    "text": "but we still strive and we strive by doing two things the",
    "start": "1486080",
    "end": "1491240"
  },
  {
    "text": "one example that I have given of the equal distribution and the other one is like if there are some merits that are",
    "start": "1491240",
    "end": "1498380"
  },
  {
    "text": "involved then we can make sure that the error that is received or the error",
    "start": "1498380",
    "end": "1504260"
  },
  {
    "text": "parity it should also be achievable I should also be targeted",
    "start": "1504260",
    "end": "1511780"
  },
  {
    "start": "1512000",
    "end": "1703000"
  },
  {
    "text": "then comes the accuracy so what is the accuracy now we are talking about that there are errors we",
    "start": "1512600",
    "end": "1519500"
  },
  {
    "text": "admit that are models are not 100 accurate because there are some errors",
    "start": "1519500",
    "end": "1525440"
  },
  {
    "text": "in our models so what do we do in traditional systems",
    "start": "1525440",
    "end": "1531860"
  },
  {
    "text": "when we only use to Target about that there should be minimum error and the",
    "start": "1531860",
    "end": "1539140"
  },
  {
    "text": "system should make the accurate decision so then we can only target accuracy and",
    "start": "1539140",
    "end": "1546860"
  },
  {
    "text": "we can try to minimize the error rate as we would like",
    "start": "1546860",
    "end": "1551900"
  },
  {
    "text": "but now we there is a fairness so when fairness comes in the formula so",
    "start": "1551900",
    "end": "1559220"
  },
  {
    "text": "we sometimes have to make compromise over accuracy",
    "start": "1559220",
    "end": "1566980"
  },
  {
    "text": "what is does it mean that we uh try to minimize the error rate",
    "start": "1568159",
    "end": "1574340"
  },
  {
    "text": "as long as parity is obtained so the main focus is that we have to make our",
    "start": "1574340",
    "end": "1581799"
  },
  {
    "text": "algorithms accurate by keeping the parity in mind",
    "start": "1581799",
    "end": "1587600"
  },
  {
    "text": "because now we want to go away from bias algorithms and move more close towards",
    "start": "1587600",
    "end": "1593659"
  },
  {
    "text": "the fair achieving Fair algorithms so in doing so if there is a",
    "start": "1593659",
    "end": "1600140"
  },
  {
    "text": "some trade-off so we don't mind that",
    "start": "1600140",
    "end": "1604480"
  },
  {
    "text": "let's say again the example continues so if we had to choose only accuracy so",
    "start": "1606020",
    "end": "1614600"
  },
  {
    "text": "we were okay that if there were five snowboarders and one",
    "start": "1614600",
    "end": "1620440"
  },
  {
    "text": "skier that would get the loan yeah because we were achieving a certain",
    "start": "1620440",
    "end": "1626960"
  },
  {
    "text": "percentage of accuracy and our error threshold was somewhere along the line",
    "start": "1626960",
    "end": "1633320"
  },
  {
    "text": "where it says e but when the when fairness came into play so now we could see that this",
    "start": "1633320",
    "end": "1642020"
  },
  {
    "text": "algorithm is not giving the fair answer because one group has been chosen there",
    "start": "1642020",
    "end": "1647659"
  },
  {
    "text": "are six members of one group that has been chosen or five and the they're only one member of two skiers",
    "start": "1647659",
    "end": "1654640"
  },
  {
    "text": "so to make this algorithm we will move our threshold error threshold little bit",
    "start": "1654640",
    "end": "1661520"
  },
  {
    "text": "on the left hand side and now there we can see that we this will include three",
    "start": "1661520",
    "end": "1666919"
  },
  {
    "text": "more skiers in the outcome which means that we have compromised",
    "start": "1666919",
    "end": "1673580"
  },
  {
    "text": "over accuracy in order to achieve fairness",
    "start": "1673580",
    "end": "1678919"
  },
  {
    "text": "so the point here is that our goal needs to be managing the balance of accuracy",
    "start": "1678919",
    "end": "1684200"
  },
  {
    "text": "and fairness while acknowledging that our models are going to be imperfect at the end",
    "start": "1684200",
    "end": "1689360"
  },
  {
    "text": "there is no one magical model that's going to be able to sufficiently protect",
    "start": "1689360",
    "end": "1694640"
  },
  {
    "text": "every single person that comes into it but we can make more fair models by",
    "start": "1694640",
    "end": "1700700"
  },
  {
    "text": "introducing the concept of parity so how we can combat the bias then",
    "start": "1700700",
    "end": "1710000"
  },
  {
    "start": "1703000",
    "end": "1880000"
  },
  {
    "text": "making new algorithms fair is one",
    "start": "1710000",
    "end": "1715159"
  },
  {
    "text": "technique but what about the existing algorithms",
    "start": "1715159",
    "end": "1720620"
  },
  {
    "text": "how to combat the bias in those algorithms so there are various things that we can",
    "start": "1720620",
    "end": "1727820"
  },
  {
    "text": "do first of all we can make the inventory of the algorithms that we are using in our organization",
    "start": "1727820",
    "end": "1734120"
  },
  {
    "text": "it's uh it might sound simple but there could be hundreds of",
    "start": "1734120",
    "end": "1739520"
  },
  {
    "text": "algorithms that we are using without knowing that we are using those algorithms in our organization so making",
    "start": "1739520",
    "end": "1747200"
  },
  {
    "text": "an active directory or inventory of the algorithm so that we could assess those",
    "start": "1747200",
    "end": "1752539"
  },
  {
    "text": "algorithms once we have those that inventory then we have to screen those algorithms that",
    "start": "1752539",
    "end": "1758840"
  },
  {
    "text": "means there there should be a team dedicated to make sure that their the algorithms that we are current that are",
    "start": "1758840",
    "end": "1765320"
  },
  {
    "text": "currently in use they are unbiased because this example that I had given for the image recognition system uh many",
    "start": "1765320",
    "end": "1772580"
  },
  {
    "text": "companies had been using it without any problem until one graduate from MIT",
    "start": "1772580",
    "end": "1781820"
  },
  {
    "text": "when she used that image recognition system in her project and she was amazed to see that the image",
    "start": "1781820",
    "end": "1790880"
  },
  {
    "text": "recognition system was unable to even identify her",
    "start": "1790880",
    "end": "1796460"
  },
  {
    "text": "and she needed to literally wear the white mask in order to be able to identifiable by the image recognition",
    "start": "1796460",
    "end": "1802820"
  },
  {
    "text": "system so there might be hundreds of systems that are around us that we are using on",
    "start": "1802820",
    "end": "1808760"
  },
  {
    "text": "daily basis without knowing that there is a bias in these systems so screening is the next step to making",
    "start": "1808760",
    "end": "1815960"
  },
  {
    "text": "the uh to mitigating or to getting rid of bias from algorithms the",
    "start": "1815960",
    "end": "1825620"
  },
  {
    "text": "third is refraining biased algorithms like retraining biased algorithms like if",
    "start": "1825620",
    "end": "1832460"
  },
  {
    "text": "there are some existing algorithm we can fix them like Amazon's exam example that",
    "start": "1832460",
    "end": "1837919"
  },
  {
    "text": "Amazon it removed the words like woman and it basically retrained their",
    "start": "1837919",
    "end": "1844460"
  },
  {
    "text": "algorithm so it could become more fair so this is one Technique we can fix the",
    "start": "1844460",
    "end": "1850039"
  },
  {
    "text": "existing algorithms after screening and identifying the biasing them",
    "start": "1850039",
    "end": "1855500"
  },
  {
    "text": "and making the policies making the company culture in such a way that we",
    "start": "1855500",
    "end": "1862220"
  },
  {
    "text": "encourage um the error identifying techniques we test",
    "start": "1862220",
    "end": "1868880"
  },
  {
    "text": "our algorithm for bias and then we prevent that uh biased",
    "start": "1868880",
    "end": "1874840"
  },
  {
    "text": "Sweeping in to our algorithms",
    "start": "1874840",
    "end": "1879278"
  },
  {
    "start": "1880000",
    "end": "1911000"
  },
  {
    "text": "with this we will look into a little bit into what what is the screening process",
    "start": "1880640",
    "end": "1885980"
  },
  {
    "text": "and how we can screen our existing algorithms so there are a couple of steps that are",
    "start": "1885980",
    "end": "1892520"
  },
  {
    "text": "involved the first is like identifying how to recognize the fairness issues and deploy Solutions in real world",
    "start": "1892520",
    "end": "1899360"
  },
  {
    "text": "the second is appraise a predictive model for fairness issue and discover auditing model attributes",
    "start": "1899360",
    "end": "1908600"
  },
  {
    "text": "so in whole of this screening process what is it that we can control",
    "start": "1910279",
    "end": "1916658"
  },
  {
    "start": "1911000",
    "end": "2141000"
  },
  {
    "text": "actually there are very few things that we can control why because",
    "start": "1917539",
    "end": "1922700"
  },
  {
    "text": "these algorithms or these models they are quite complex",
    "start": "1922700",
    "end": "1928700"
  },
  {
    "text": "and we really don't know how they are training themselves there are many proxies that are involved",
    "start": "1928700",
    "end": "1936020"
  },
  {
    "text": "there are many factors that are involved there are many uh one real-time situations that they are",
    "start": "1936020",
    "end": "1943279"
  },
  {
    "text": "analyzing so there are many things that are out of our control but what we can control is like input",
    "start": "1943279",
    "end": "1948919"
  },
  {
    "text": "and output because input is what we are feeding them and output what we can evaluate",
    "start": "1948919",
    "end": "1956899"
  },
  {
    "text": "so if we change input so here the concept of control a variable that comes",
    "start": "1956899",
    "end": "1963020"
  },
  {
    "text": "into play that we choose one variable at a time and we feed our algorithm one",
    "start": "1963020",
    "end": "1969140"
  },
  {
    "text": "variable at it one variable at a time and keep the rest of the factors",
    "start": "1969140",
    "end": "1974659"
  },
  {
    "text": "constant and observe that what are the how are output is sending",
    "start": "1974659",
    "end": "1982539"
  },
  {
    "text": "and then this is how we identify the attributes that create the auditing data",
    "start": "1982700",
    "end": "1989620"
  },
  {
    "text": "so what will be the example",
    "start": "1990860",
    "end": "1995898"
  },
  {
    "text": "so we are actually left with a blind model because we don't know what's happening in the model",
    "start": "1996380",
    "end": "2002980"
  },
  {
    "text": "um one does not one that does not explicitly know the race for example in",
    "start": "2002980",
    "end": "2008200"
  },
  {
    "text": "case of Amazon and so the resume is that Amazon was providing it didn't have the",
    "start": "2008200",
    "end": "2013360"
  },
  {
    "text": "gender information similarly in the criminal justice system when um in America's Criminal Justice System",
    "start": "2013360",
    "end": "2020700"
  },
  {
    "text": "we saw that people were people with the black origin or they were getting",
    "start": "2020700",
    "end": "2027059"
  },
  {
    "text": "uh more sentences than the the one with the why and when there was a score to",
    "start": "2027059",
    "end": "2035019"
  },
  {
    "text": "identify that who will commit the uh offense in the future so black uh",
    "start": "2035019",
    "end": "2042760"
  },
  {
    "text": "prisoners were getting higher risk as compared to their white",
    "start": "2042760",
    "end": "2048940"
  },
  {
    "text": "um uh fellows so but in but that was being proven wrong so despite the fact",
    "start": "2048940",
    "end": "2057158"
  },
  {
    "text": "that there was no information about the race or the gender in both these cases that the the algorithms were creating",
    "start": "2057159",
    "end": "2064658"
  },
  {
    "text": "buys but themselves and why was it because there were many other factors",
    "start": "2064659",
    "end": "2070300"
  },
  {
    "text": "that they could associate to for example in case of the Amazon there was no",
    "start": "2070300",
    "end": "2075460"
  },
  {
    "text": "general information but there was information like there were the words like woman woman's woman League Etc",
    "start": "2075460",
    "end": "2081339"
  },
  {
    "text": "which was based on which the algorithm was being biased towards the woman",
    "start": "2081339",
    "end": "2087878"
  },
  {
    "text": "in case of the prison example there was no information about the race because",
    "start": "2087879",
    "end": "2093339"
  },
  {
    "text": "there were 135 questions um or that questionnaire based on which",
    "start": "2093339",
    "end": "2098740"
  },
  {
    "text": "the score was being calculated so there was no question about that what is your race what is the ethnic groups you come",
    "start": "2098740",
    "end": "2105400"
  },
  {
    "text": "from well there were the the questions about like which locality do you live in which postcode do you live in",
    "start": "2105400",
    "end": "2111940"
  },
  {
    "text": "ETC based on that the AI could relate that this is the uh maybe male the the",
    "start": "2111940",
    "end": "2120060"
  },
  {
    "text": "American African dominated area or this is a ghetto or it could be anything",
    "start": "2120060",
    "end": "2126160"
  },
  {
    "text": "based on which it was created the biased scores so here uh this model",
    "start": "2126160",
    "end": "2134079"
  },
  {
    "text": "um is blind but we are what we are doing is like we are controlling the inputs and we are evaluating the outputs",
    "start": "2134079",
    "end": "2141460"
  },
  {
    "start": "2141000",
    "end": "2286000"
  },
  {
    "text": "so it is like if we give the resume to uh during the algorithm auditing phase",
    "start": "2141460",
    "end": "2147660"
  },
  {
    "text": "so first of all we will be using this um algorithm which will basically analyze",
    "start": "2147660",
    "end": "2155200"
  },
  {
    "text": "all the text Within These resumes and then it will create maybe bag of words",
    "start": "2155200",
    "end": "2160839"
  },
  {
    "text": "or certain attributes out of by analyzing the data",
    "start": "2160839",
    "end": "2167260"
  },
  {
    "text": "that we provide and then based on that data what we will do is like we will",
    "start": "2167260",
    "end": "2174420"
  },
  {
    "text": "choose one parameter one input variable at a time and then we will under the",
    "start": "2174420",
    "end": "2182200"
  },
  {
    "text": "control condition see how does it affect the output",
    "start": "2182200",
    "end": "2187960"
  },
  {
    "text": "based on that we can create the report audit report",
    "start": "2187960",
    "end": "2193060"
  },
  {
    "text": "which tells us that okay this variable is acting and then we can",
    "start": "2193060",
    "end": "2199839"
  },
  {
    "text": "scale them like okay the first one maybe the gender it is having the adverse",
    "start": "2199839",
    "end": "2206680"
  },
  {
    "text": "effect the woman it is having less little less effect than that so we can",
    "start": "2206680",
    "end": "2213220"
  },
  {
    "text": "scale the outcomes in a prioritical periodical order or chronological order",
    "start": "2213220",
    "end": "2220680"
  },
  {
    "text": "and then we can create the report based on which we can address the issues of",
    "start": "2220680",
    "end": "2228099"
  },
  {
    "text": "the bias in the algorithm so this is this technique is basically algorithm make auditing technique",
    "start": "2228099",
    "end": "2236940"
  },
  {
    "text": "so when we are auditing the algorithm so there should be the four rules that we",
    "start": "2240460",
    "end": "2245800"
  },
  {
    "text": "should take care of the first is that change one input keep others the constant as talked about score a weight",
    "start": "2245800",
    "end": "2253000"
  },
  {
    "text": "of input attributes and output we have to give the weightage to our input",
    "start": "2253000",
    "end": "2259180"
  },
  {
    "text": "um so that we could know that which one which input is creating a drastic effect the third is like assemble a picture of",
    "start": "2259180",
    "end": "2266200"
  },
  {
    "text": "the model's true blind spots and then present audio report and begin investigating into",
    "start": "2266200",
    "end": "2273099"
  },
  {
    "text": "um buys data or fairness metrics to create this metrics out of this",
    "start": "2273099",
    "end": "2279520"
  },
  {
    "text": "analysis or Audits and based on that take the measures for the future",
    "start": "2279520",
    "end": "2286140"
  },
  {
    "start": "2286000",
    "end": "2373000"
  },
  {
    "text": "it is it sounds very easy but it is pretty challenging when we talk about",
    "start": "2286420",
    "end": "2291520"
  },
  {
    "text": "implementing it why is it because there is no one definition",
    "start": "2291520",
    "end": "2296560"
  },
  {
    "text": "of AI buys there could be a various definitions of Buys in various",
    "start": "2296560",
    "end": "2302800"
  },
  {
    "text": "organizations in various various setups various situations there are there is no",
    "start": "2302800",
    "end": "2309480"
  },
  {
    "text": "standard or accepted definition of systematic unfair what does",
    "start": "2309480",
    "end": "2314680"
  },
  {
    "text": "systematically unfair means there are also a few standards or metrics to measure the fairness leaving each",
    "start": "2314680",
    "end": "2321280"
  },
  {
    "text": "company to reach its own definition of bias and how to measure that bias and how to take the actions against it",
    "start": "2321280",
    "end": "2329500"
  },
  {
    "text": "um so another issue is that the AI models likely use both new data and",
    "start": "2329500",
    "end": "2334839"
  },
  {
    "text": "historical data so while you can control the historical data maybe but you cannot",
    "start": "2334839",
    "end": "2342160"
  },
  {
    "text": "control the new data because that's it's being generated and it's the tells about",
    "start": "2342160",
    "end": "2347500"
  },
  {
    "text": "the current situation so the constantly evolving world will",
    "start": "2347500",
    "end": "2354160"
  },
  {
    "text": "produce a different data which will be used so there are so many factors actually that are unknown and there are",
    "start": "2354160",
    "end": "2361720"
  },
  {
    "text": "very few standards that exist due to making or counter",
    "start": "2361720",
    "end": "2367859"
  },
  {
    "text": "accounting or mitigating the AI buys is very challenging but despite that fact",
    "start": "2367859",
    "end": "2374800"
  },
  {
    "start": "2373000",
    "end": "2582000"
  },
  {
    "text": "fortunately we have certain steps that we can take to make sure that we are",
    "start": "2374800",
    "end": "2381880"
  },
  {
    "text": "mitigating the algorithmic bias so the first one is like identifying your",
    "start": "2381880",
    "end": "2387520"
  },
  {
    "text": "unique vulnerabilities as I said that depending on the organization that you are working with and depending on the",
    "start": "2387520",
    "end": "2394420"
  },
  {
    "text": "situation or circumstances there could be a different vulnerabilities for example Banks retailers and utilities",
    "start": "2394420",
    "end": "2401020"
  },
  {
    "text": "they all face different kinds of risk from potential AI buys so you have to",
    "start": "2401020",
    "end": "2407260"
  },
  {
    "text": "what you have to do is determine your own um vulnerabilities the unique",
    "start": "2407260",
    "end": "2412780"
  },
  {
    "text": "vulnerabilities or the challenges that you are facing in your company and then you have to Target that and",
    "start": "2412780",
    "end": "2419500"
  },
  {
    "text": "calculate the result in terms of financial operational and reputational",
    "start": "2419500",
    "end": "2425020"
  },
  {
    "text": "risks that are associated with that bias in your company and then prioritize the focus where you",
    "start": "2425020",
    "end": "2431440"
  },
  {
    "text": "should set the focus accordingly the second is you have to control your data again data",
    "start": "2431440",
    "end": "2439900"
  },
  {
    "text": "is the key data is basically a main source of the bias so therefore your",
    "start": "2439900",
    "end": "2445839"
  },
  {
    "text": "traditional controls may not might not be enough so you need to devise the new ways of controlling your data",
    "start": "2445839",
    "end": "2453940"
  },
  {
    "text": "and you have to scrutinize your data before feeding it to algorithm and you have to make sure that your data is",
    "start": "2453940",
    "end": "2460599"
  },
  {
    "text": "diverse um it is weighted correctly Etc before",
    "start": "2460599",
    "end": "2467079"
  },
  {
    "text": "feeding it to AI algorithm and then comes the Govern like govern AI",
    "start": "2467079",
    "end": "2473440"
  },
  {
    "text": "at AI speed what does it mean it means increasingly AI is always on and may use data from",
    "start": "2473440",
    "end": "2480880"
  },
  {
    "text": "across the organization so your governance should keep up with",
    "start": "2480880",
    "end": "2485920"
  },
  {
    "text": "it it should be continuous and enterprise-wide the governor should",
    "start": "2485920",
    "end": "2491140"
  },
  {
    "text": "include easily understandable Frameworks and toolkits as well as common definitions and controls so that both AI",
    "start": "2491140",
    "end": "2498579"
  },
  {
    "text": "Specialists and business users can keep up with it",
    "start": "2498579",
    "end": "2503579"
  },
  {
    "text": "then there should be diversity diversity doesn't mean that there should be a gender base or race or ethnicity based",
    "start": "2504280",
    "end": "2511900"
  },
  {
    "text": "diversity but there should be diversity from the business point of view as well they shouldn't be just developers but",
    "start": "2511900",
    "end": "2518619"
  },
  {
    "text": "also like Business Leaders your stakeholders your lawyers",
    "start": "2518619",
    "end": "2523980"
  },
  {
    "text": "regulatory people all of this should sit together to",
    "start": "2523980",
    "end": "2529619"
  },
  {
    "text": "talk about these algorithms to add to talk about the strategies to mitigate",
    "start": "2529619",
    "end": "2536800"
  },
  {
    "text": "these uh biases so that you think from different angles and take every angle",
    "start": "2536800",
    "end": "2543880"
  },
  {
    "text": "into account before deploying these algorithms last but not not the least validate",
    "start": "2543880",
    "end": "2551200"
  },
  {
    "text": "independently and continuously it means that there should be an additional layer",
    "start": "2551200",
    "end": "2556300"
  },
  {
    "text": "of security because here we are talking about the new and much dangerous tools",
    "start": "2556300",
    "end": "2562359"
  },
  {
    "text": "than before because now we are talking about the real life situations like a person",
    "start": "2562359",
    "end": "2569140"
  },
  {
    "text": "getting hit by a car autonomous car Etc so therefore we should validate the",
    "start": "2569140",
    "end": "2578380"
  },
  {
    "text": "algorithm before sending them out so with this I will say thank you so",
    "start": "2578380",
    "end": "2585400"
  },
  {
    "start": "2582000",
    "end": "2601000"
  },
  {
    "text": "much [Applause]",
    "start": "2585400",
    "end": "2592489"
  }
]