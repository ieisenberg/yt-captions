[
  {
    "start": "0",
    "end": "59000"
  },
  {
    "text": "[Music]",
    "start": "3550",
    "end": "7840"
  },
  {
    "text": "good afternoon so I'm Julius one of the co-creators of the open source monitoring system called",
    "start": "12190",
    "end": "18529"
  },
  {
    "text": "Prometheus I usually speak about this in front of a bit more infrastructure heavy audience so I'm curious who here",
    "start": "18529",
    "end": "25130"
  },
  {
    "text": "actually uses Prometheus oh yeah so you'll all be bored by this because it's",
    "start": "25130",
    "end": "30410"
  },
  {
    "text": "it's going to be an introductory talk and who here has heard about it but is",
    "start": "30410",
    "end": "35420"
  },
  {
    "text": "not using it yet oh okay maybe it's gonna be cool but might be useful okay",
    "start": "35420",
    "end": "41059"
  },
  {
    "text": "so this talk is first of all a tiny bit of an introduction to what systems monitoring is and why we do it and then",
    "start": "41059",
    "end": "48920"
  },
  {
    "text": "I'll deep a bit I'll dive a bit deeper into Prometheus specifically so first of",
    "start": "48920",
    "end": "56059"
  },
  {
    "text": "all a primer about systems monitoring you know nowadays IT systems are",
    "start": "56059",
    "end": "61789"
  },
  {
    "start": "59000",
    "end": "120000"
  },
  {
    "text": "everywhere and you know from if you want to sell shoes to if you're a large tech",
    "start": "61789",
    "end": "67670"
  },
  {
    "text": "corporation you're going to have IT systems either your own or in the cloud with software and hardware et cetera and",
    "start": "67670",
    "end": "73880"
  },
  {
    "text": "the goals for these systems are typically similar you want stuff to be available like a website should be",
    "start": "73880",
    "end": "80350"
  },
  {
    "text": "should load it should load fast as well it should show the correct contents and",
    "start": "80350",
    "end": "85520"
  },
  {
    "text": "then maybe something less user visible but still important to you as an operator is that you're running stuff",
    "start": "85520",
    "end": "92090"
  },
  {
    "text": "efficiently so that you're not wasting a bunch of money or CPU time etc under the",
    "start": "92090",
    "end": "97130"
  },
  {
    "text": "hood of your systems in reality though anyone who has run any kind of",
    "start": "97130",
    "end": "103039"
  },
  {
    "text": "complicated complex distributed system will know that things are always in a",
    "start": "103039",
    "end": "108259"
  },
  {
    "text": "state of partial degradation and everything is kind of exploding left and right and the challenge is to make the",
    "start": "108259",
    "end": "114979"
  },
  {
    "text": "overall system look ok to the outside world so for example a couple of",
    "start": "114979",
    "end": "121700"
  },
  {
    "start": "120000",
    "end": "241000"
  },
  {
    "text": "potential problems you might see might range from hardware to software issues to like other issues for example full",
    "start": "121700",
    "end": "129800"
  },
  {
    "text": "disk might prevent a database from storing more data which will then maybe fail some write requests that a user",
    "start": "129800",
    "end": "135590"
  },
  {
    "text": "sees software bugs temperature etc the last point here in the list is a bit",
    "start": "135590",
    "end": "141379"
  },
  {
    "text": "different from the others for example if you have a container scheduler system like kubernetes and you",
    "start": "141379",
    "end": "147140"
  },
  {
    "text": "reserved way too much RAM or memory for each container of a certain type and the",
    "start": "147140",
    "end": "152720"
  },
  {
    "text": "container actually only uses like 10% of that then the unused memory that you're",
    "start": "152720",
    "end": "158090"
  },
  {
    "text": "reserving cannot be used by other stuff so you're kind of wasting money on on a",
    "start": "158090",
    "end": "164540"
  },
  {
    "text": "Mis configuration of your memory reservation which is also something you want to catch and do something about so",
    "start": "164540",
    "end": "174680"
  },
  {
    "text": "this is where monitoring comes in I use the term monitoring very broadly so some",
    "start": "174680",
    "end": "180799"
  },
  {
    "text": "people there's also the term observability and some other terms I just say monitoring for anything that",
    "start": "180799",
    "end": "187370"
  },
  {
    "text": "kind of gets information or signals from your infrastructure and services in any way and then allows you to get inside",
    "start": "187370",
    "end": "195170"
  },
  {
    "text": "and act on the information and if you have something that looks like a website",
    "start": "195170",
    "end": "200360"
  },
  {
    "text": "or something that serves requests the typical signals that you want to collect information on are how many requests per",
    "start": "200360",
    "end": "207440"
  },
  {
    "text": "second are you getting what's the latency distribution how many",
    "start": "207440",
    "end": "212600"
  },
  {
    "text": "percent like what's the error percentage of the requests etc etc but also stuff like memory usage CPU usage underlying",
    "start": "212600",
    "end": "220239"
  },
  {
    "text": "resource usage metrics and really anything you can imagine measuring and then you want to react when something",
    "start": "220239",
    "end": "228530"
  },
  {
    "text": "doesn't look like you expect so there's",
    "start": "228530",
    "end": "233630"
  },
  {
    "text": "a couple of different types of monitoring some were more popular in the past and some are still popular nowadays",
    "start": "233630",
    "end": "239889"
  },
  {
    "text": "so a typical old one here who has suffered from using Nagios or who's",
    "start": "239889",
    "end": "246230"
  },
  {
    "start": "241000",
    "end": "346000"
  },
  {
    "text": "running nodules so the of course system that did its job mainly in the 90s and",
    "start": "246230",
    "end": "252260"
  },
  {
    "text": "still nowadays in some legacy installations this is basically you know",
    "start": "252260",
    "end": "258070"
  },
  {
    "text": "very heavily doing check based monitoring which means you have",
    "start": "258070",
    "end": "264139"
  },
  {
    "text": "statically configured hosts that have you know one has a database server one has a web server and so on and you run",
    "start": "264139",
    "end": "270080"
  },
  {
    "text": "regular check scripts that just check right now every minute is the database",
    "start": "270080",
    "end": "275270"
  },
  {
    "text": "server running the web server running is the CPU temperature right now okay and then",
    "start": "275270",
    "end": "280430"
  },
  {
    "text": "these checks might output the status like okay warning or critical and alert people if",
    "start": "280430",
    "end": "286520"
  },
  {
    "text": "something looks bad so this is definitely better than nothing it's",
    "start": "286520",
    "end": "293180"
  },
  {
    "text": "pretty much focused on probing a system from the outside in a very primitive way though for example like checking if a",
    "start": "293180",
    "end": "299389"
  },
  {
    "text": "process is running CPU temperature and so on not not so good for looking deeply",
    "start": "299389",
    "end": "304879"
  },
  {
    "text": "into a process what exactly is happening it's also focused around doing checks on",
    "start": "304879",
    "end": "312110"
  },
  {
    "text": "individual machines in a kind of local context and only focusing on the right now moment I know later on these systems",
    "start": "312110",
    "end": "319399"
  },
  {
    "text": "also gained a bit more capabilities beyond that but we're really focused on really these simple types of checks and",
    "start": "319399",
    "end": "326259"
  },
  {
    "text": "especially they were configured in a static way like this is my database server it has the following checks and",
    "start": "326259",
    "end": "332839"
  },
  {
    "text": "this kind of breaks down in environment where you have for example kubernetes",
    "start": "332839",
    "end": "338889"
  },
  {
    "text": "scheduling containers and workload shifting around a lot another way to get",
    "start": "338889",
    "end": "347240"
  },
  {
    "start": "346000",
    "end": "442000"
  },
  {
    "text": "insight into your applications and do some the do stuff about it is logs logs",
    "start": "347240",
    "end": "352699"
  },
  {
    "text": "just means your application emits either a structured log line with key value",
    "start": "352699",
    "end": "358669"
  },
  {
    "text": "pairs about an event that just happened or an unstructured log line like set text line which you then would have two",
    "start": "358669",
    "end": "364849"
  },
  {
    "text": "powers later and make sense of and you know examples for this is you might just",
    "start": "364849",
    "end": "370849"
  },
  {
    "text": "log it to local disk or you might ship it to a remote system like elasticsearch or like in flux TB where you can then do",
    "start": "370849",
    "end": "378319"
  },
  {
    "text": "further analysis and actions based on the data you've collected logs are",
    "start": "378319",
    "end": "383449"
  },
  {
    "text": "really cool because you give you the highest were kind of the highest possible amount of detail about all the",
    "start": "383449",
    "end": "389599"
  },
  {
    "text": "events that are happening in your processes and systems and they're also really simple to produce it's usually just a line of code to emit a log event",
    "start": "389599",
    "end": "398319"
  },
  {
    "text": "they do scale in cost with a rate of",
    "start": "398319",
    "end": "403939"
  },
  {
    "text": "your events though so if you're getting 1 million requests per second and you",
    "start": "403939",
    "end": "409099"
  },
  {
    "text": "want to law all of them as events then you have to log a million events per second and that can get really really expensive they",
    "start": "409099",
    "end": "416820"
  },
  {
    "text": "also don't solve the problem of correlating different log lines from different services that correspond to",
    "start": "416820",
    "end": "423750"
  },
  {
    "text": "the same request that maybe went to different services so it's a bit tough",
    "start": "423750",
    "end": "432090"
  },
  {
    "text": "due to the cost here to base your main operational systems monitoring on logs",
    "start": "432090",
    "end": "437700"
  },
  {
    "text": "for high traffic services at least so this is where metrics help a bit",
    "start": "437700",
    "end": "443970"
  },
  {
    "start": "442000",
    "end": "526000"
  },
  {
    "text": "metrics or time series track individual numeric values over time and you may be",
    "start": "443970",
    "end": "450420"
  },
  {
    "text": "it could be a temperature memory usage the number of requests that a certain",
    "start": "450420",
    "end": "455700"
  },
  {
    "text": "process has served since it started and these are all numbers and you sample",
    "start": "455700",
    "end": "461010"
  },
  {
    "text": "them every 15 seconds or every 30 seconds or so and store them as a series over time examples of these are for",
    "start": "461010",
    "end": "469440"
  },
  {
    "text": "example stats the open TCB's on and prometheus is also in this category so a",
    "start": "469440",
    "end": "477750"
  },
  {
    "text": "good thing is metrics are way way way cheaper than logs imagine again like",
    "start": "477750",
    "end": "483120"
  },
  {
    "text": "storing a million events per second versus just storing a counter value",
    "start": "483120",
    "end": "488130"
  },
  {
    "text": "every 15 seconds which is a single number for that of course you know it gives you way less detail it doesn't",
    "start": "488130",
    "end": "494730"
  },
  {
    "text": "give you the full detail about every event that happened anymore but it's",
    "start": "494730",
    "end": "499770"
  },
  {
    "text": "still for usual cases good enough to give you an aggregate view of your system and whether it's healthy and then",
    "start": "499770",
    "end": "505950"
  },
  {
    "text": "for doing detailed debugging you still often want to have logs for jumping you",
    "start": "505950",
    "end": "511350"
  },
  {
    "text": "know to the logs of certain select systems - to figure out what exactly happened and yeah also relatively simple",
    "start": "511350",
    "end": "521130"
  },
  {
    "text": "to produce also lacking the inter-service correlation so for the inter-service correlation that's where",
    "start": "521130",
    "end": "527400"
  },
  {
    "start": "526000",
    "end": "640000"
  },
  {
    "text": "request tracing really helps this is about tracking a single request through",
    "start": "527400",
    "end": "532890"
  },
  {
    "text": "an entire stack of micro services that serves a request so the ideas a request",
    "start": "532890",
    "end": "539730"
  },
  {
    "text": "comes in at the load balancer gets some trace idea signed and the trace ID gets passed along the",
    "start": "539730",
    "end": "545940"
  },
  {
    "text": "entire chain of all the different sub services that handle a user request and each sub service records the time span",
    "start": "545940",
    "end": "553670"
  },
  {
    "text": "which would spent handling a certain sub part of handing that request and then",
    "start": "553670",
    "end": "558779"
  },
  {
    "text": "sends that span to a remote system to record it and later to analyze it so",
    "start": "558779",
    "end": "565529"
  },
  {
    "text": "this is really great for getting a good intuition and also insight into the life",
    "start": "565529",
    "end": "571079"
  },
  {
    "text": "of an individual request where it spends how much time and where it met encounter errors of course this can also this is",
    "start": "571079",
    "end": "581699"
  },
  {
    "text": "kind of like logging but on steroids so you have to make it not too expensive typically people sample this so they say",
    "start": "581699",
    "end": "588209"
  },
  {
    "text": "like maybe log every thousand thousandth can can't pronounce that one in every",
    "start": "588209",
    "end": "595410"
  },
  {
    "text": "thousand requests and and of course the more you sample the more the less useful",
    "start": "595410",
    "end": "601259"
  },
  {
    "text": "it becomes but the cheaper it becomes so it's a trade-off tracing is a bit harder to add to your",
    "start": "601259",
    "end": "607529"
  },
  {
    "text": "infrastructure because you need to make every node in the path cooperate everyone needs to pass on this trace ID",
    "start": "607529",
    "end": "613980"
  },
  {
    "text": "so you can correlate everything in the end and it's really mostly suitable for",
    "start": "613980",
    "end": "619920"
  },
  {
    "text": "tracking request style information so you know metrics also help you record",
    "start": "619920",
    "end": "624990"
  },
  {
    "text": "stuff like CPU temperatures and other gages and histograms and so on which don't fit so well into this model so now",
    "start": "624990",
    "end": "634259"
  },
  {
    "text": "let's get to prometheus as an example for a metrics based monitoring system so",
    "start": "634259",
    "end": "640439"
  },
  {
    "start": "640000",
    "end": "674000"
  },
  {
    "text": "Prometheus is a monitoring system that's based on metrics and also can create",
    "start": "640439",
    "end": "647430"
  },
  {
    "text": "alerts based on metrics we give you tools for the entire range of what you",
    "start": "647430",
    "end": "652769"
  },
  {
    "text": "want to do so getting instrumentation getting data out of the things you care about then collecting the data and then",
    "start": "652769",
    "end": "659970"
  },
  {
    "text": "doing useful stuff with the data for example generating alerts or doing dashboards and it's especially",
    "start": "659970",
    "end": "665310"
  },
  {
    "text": "well-suited for dynamic cloud environments like kubernetes where containers and applications and so on",
    "start": "665310",
    "end": "671069"
  },
  {
    "text": "just move around a lot we try to keep things simple and",
    "start": "671069",
    "end": "676529"
  },
  {
    "start": "674000",
    "end": "830000"
  },
  {
    "text": "explicitly don't do certain other things so logging and tracing which I just mentioned earlier we don't do in",
    "start": "676529",
    "end": "682870"
  },
  {
    "text": "Prometheus we only do time series we still think that logging and tracing are useful but you will have to use separate",
    "start": "682870",
    "end": "689199"
  },
  {
    "text": "systems from Prometheus to do those we allow you to specify a lodging rules",
    "start": "689199",
    "end": "694360"
  },
  {
    "text": "which can be potentially complex but have to be very explicit so we don't do automatic anomaly detection where the",
    "start": "694360",
    "end": "700839"
  },
  {
    "text": "system just looks at some data and sees this is something it looks different than usual let's ping someone also",
    "start": "700839",
    "end": "709120"
  },
  {
    "text": "Prometheus itself for simplicity only has a local storage which naturally has",
    "start": "709120",
    "end": "714430"
  },
  {
    "text": "some limitations of you know horizontal scalability and so on but there's ways",
    "start": "714430",
    "end": "719589"
  },
  {
    "text": "to build more scalable and durable storage around Prometheus so Prometheus",
    "start": "719589",
    "end": "726880"
  },
  {
    "text": "started in 2012 when me and another guy both met proud both came from Google to",
    "start": "726880",
    "end": "734050"
  },
  {
    "text": "SoundCloud and SoundCloud already had built their own cluster scheduler before docker even existed and obviously before",
    "start": "734050",
    "end": "740079"
  },
  {
    "text": "kubernetes existed and all the existing open source monitoring tools back then",
    "start": "740079",
    "end": "746019"
  },
  {
    "text": "were not really suitable anymore for this kind of dynamic environment and we had trouble finding out what was going",
    "start": "746019",
    "end": "751449"
  },
  {
    "text": "on we're in suitable enough detail to make the site stable and fast and so",
    "start": "751449",
    "end": "758230"
  },
  {
    "text": "we're kind of thinking back and saying well we've worked with a cluster scheduler at Google already so and obviously Google had a monitoring system",
    "start": "758230",
    "end": "765130"
  },
  {
    "text": "that worked well with that so prometheus we started building it an hour free time",
    "start": "765130",
    "end": "770440"
  },
  {
    "text": "inspired by Google's internal monitoring system called Bachmann and then",
    "start": "770440",
    "end": "775660"
  },
  {
    "text": "gradually introduced it at SoundCloud and yeah it's we made it open source and the",
    "start": "775660",
    "end": "782230"
  },
  {
    "text": "world started using it Prometheus has been part of the cloud native computing",
    "start": "782230",
    "end": "788620"
  },
  {
    "text": "foundation since 2016 we were the second project after kubernetes in there and",
    "start": "788620",
    "end": "793649"
  },
  {
    "text": "very important like we're bit different from some of the other open source projects out there we are not a company",
    "start": "793649",
    "end": "800259"
  },
  {
    "text": "so we're one of the most independent projects we have at the moment around 20",
    "start": "800259",
    "end": "807560"
  },
  {
    "text": "voting team members in the core Prometheus team and they're all work for different companies some are like",
    "start": "807560",
    "end": "813080"
  },
  {
    "text": "redhead others are Griffin alabs one is at Google wants a digital ocean and so on I'm an independent freelancer also in",
    "start": "813080",
    "end": "820310"
  },
  {
    "text": "that team so you know there's not one single company driving where this project is going and you can find us at",
    "start": "820310",
    "end": "827180"
  },
  {
    "text": "fermitas IO alright let's look at the",
    "start": "827180",
    "end": "833089"
  },
  {
    "start": "830000",
    "end": "961000"
  },
  {
    "text": "actual core system architecture of Prometheus first you start out with some",
    "start": "833089",
    "end": "838430"
  },
  {
    "text": "with some things you care about we call them targets in the best case these might be services where you control the",
    "start": "838430",
    "end": "845360"
  },
  {
    "text": "source code directly that's the best case because then you can just take a Prometheus client library add it to your",
    "start": "845360",
    "end": "851960"
  },
  {
    "text": "code and add an HTTP endpoint because Prometheus is a pull based monitoring",
    "start": "851960",
    "end": "857210"
  },
  {
    "text": "system that exposes metrics over HTTP and that also helps you track metrics so",
    "start": "857210",
    "end": "863990"
  },
  {
    "text": "it helps keep track of the state of internal counters gauges histograms and",
    "start": "863990",
    "end": "869000"
  },
  {
    "text": "summary metrics and this allows you to get very good white box instrumentation",
    "start": "869000",
    "end": "875089"
  },
  {
    "text": "meaning really instrumentation and metrics where the process itself keeps",
    "start": "875089",
    "end": "880280"
  },
  {
    "text": "track in high detail of what it is doing inside then there's things where you",
    "start": "880280",
    "end": "885980"
  },
  {
    "text": "cannot you know add an HTTP endpoint with previous metrics directly to the",
    "start": "885980",
    "end": "892070"
  },
  {
    "text": "code it might still be a while until Linux the Linux kernel has an HTTP server serving fermitas metrics directly",
    "start": "892070",
    "end": "898960"
  },
  {
    "text": "or a my sequel server and for these purposes we have the concept of an",
    "start": "898960",
    "end": "905420"
  },
  {
    "text": "exporter a little sidecar process that you run next to or on on top of the",
    "start": "905420",
    "end": "910760"
  },
  {
    "text": "thing that you want to actually monitor and then Prometheus gathers metrics from",
    "start": "910760",
    "end": "916640"
  },
  {
    "text": "the exporter and the exporter in the background just live contacts the back-end system gathers metrics in",
    "start": "916640",
    "end": "922130"
  },
  {
    "text": "whatever proprietary format that is and translates them back to Prometheus metrics so then you have as the heart of",
    "start": "922130",
    "end": "930830"
  },
  {
    "text": "the ecosystem the Prometheus server starts out in its simplest form as a",
    "start": "930830",
    "end": "935959"
  },
  {
    "text": "single node system and then later you can build more scalable topologies and the",
    "start": "935959",
    "end": "941150"
  },
  {
    "text": "from easy server you configure to scrape or pull metrics from all the targets",
    "start": "941150",
    "end": "947450"
  },
  {
    "text": "that you have configured in a regular interval and it then stores those as",
    "start": "947450",
    "end": "953210"
  },
  {
    "text": "time series on local disk at first so let's do a little bit of an excursion",
    "start": "953210",
    "end": "959660"
  },
  {
    "text": "into what actually gets transferred over the wire here this is what the exposition format looks like when",
    "start": "959660",
    "end": "966830"
  },
  {
    "start": "961000",
    "end": "1049000"
  },
  {
    "text": "Prometheus asks one of the endpoint hey what is the current value of each one of your time series the endpoint answers",
    "start": "966830",
    "end": "972950"
  },
  {
    "text": "with something that looks like this I'll talk about the data format a bit more later but basically it is one sample per",
    "start": "972950",
    "end": "979970"
  },
  {
    "text": "line and the line gives the identity of the time series and then the current sample value and this is all that's ever",
    "start": "979970",
    "end": "986810"
  },
  {
    "text": "being returned so it's only ever the current value of each series that is currently being tracked alright so how",
    "start": "986810",
    "end": "998240"
  },
  {
    "text": "does prometheus know where all the targets are obviously nowadays we have quite dynamic environments so we need to",
    "start": "998240",
    "end": "1005020"
  },
  {
    "text": "integrate with service discovery or some source of truth in your infrastructure to tell Prometheus where all the things",
    "start": "1005020",
    "end": "1012220"
  },
  {
    "text": "should be and where they are and then Prometheus can dynamically all the time figure out what it should scrape from",
    "start": "1012220",
    "end": "1020610"
  },
  {
    "text": "you can then build dashboards using your fauna or prometheus is built in web UI automation against the collected data if",
    "start": "1020610",
    "end": "1027699"
  },
  {
    "text": "you want and also define alerts that Prometheus calculates but then dispatches over a separate component",
    "start": "1027700",
    "end": "1033670"
  },
  {
    "text": "called the alert manager and the alert manager and groups over time and over different dimensions and eventually",
    "start": "1033670",
    "end": "1040420"
  },
  {
    "text": "sends out notifications as email pager Duty ops genie slack and some other mechanisms and you can build your own so",
    "start": "1040420",
    "end": "1049750"
  },
  {
    "start": "1049000",
    "end": "1094000"
  },
  {
    "text": "let's go into some of my favorite features and features which I think made Prometheus as successful as it is",
    "start": "1049750",
    "end": "1055870"
  },
  {
    "text": "nowadays and maybe these features are not so exciting or unusual anymore",
    "start": "1055870",
    "end": "1063040"
  },
  {
    "text": "nowadays because they're being adopted by more and more other systems but this really helped set apart Prometheus in",
    "start": "1063040",
    "end": "1069190"
  },
  {
    "text": "the beginning so the dimensional data model to help track things metrics in detail a good crew",
    "start": "1069190",
    "end": "1077270"
  },
  {
    "text": "language to go along with that data model being able to start simply and efficient with a single parameter server",
    "start": "1077270",
    "end": "1083660"
  },
  {
    "text": "and then integrating with service discovery to make this whole thing work in dynamic environments and I'll go into",
    "start": "1083660",
    "end": "1091730"
  },
  {
    "text": "each one of these now so first of all the data model promises fundamentally stores time series the",
    "start": "1091730",
    "end": "1099350"
  },
  {
    "start": "1094000",
    "end": "1215000"
  },
  {
    "text": "time series and Prometheus has some kind of identifier and then we just add time series value ten to ten stem value times",
    "start": "1099350",
    "end": "1105890"
  },
  {
    "text": "time value times time value pairs to those identifiers as the series evolves",
    "start": "1105890",
    "end": "1111800"
  },
  {
    "text": "over time now the timestamp is actually always just in in 64 milliseconds since",
    "start": "1111800",
    "end": "1119300"
  },
  {
    "text": "the UNIX epoch the values are all float 64 turns out to actually work really",
    "start": "1119300",
    "end": "1124580"
  },
  {
    "text": "well for operational systems monitoring now the difference back then was how do we identify time series that's that's",
    "start": "1124580",
    "end": "1131690"
  },
  {
    "text": "one of the most interesting parts here we identify time series first by a",
    "start": "1131690",
    "end": "1139070"
  },
  {
    "text": "central aspect of a system that we're monitoring called the metric name in this case HTTP requests total the total",
    "start": "1139070",
    "end": "1146060"
  },
  {
    "text": "number of HTTP requests that a given process has handled since it started up and then to further further",
    "start": "1146060",
    "end": "1153260"
  },
  {
    "text": "differentiate sub dimensions we add key value pairs that we call labels and these say for example the past that a",
    "start": "1153260",
    "end": "1160040"
  },
  {
    "text": "request happened on the status code or the process that it came from that's the instance here so you see that some of",
    "start": "1160040",
    "end": "1168410"
  },
  {
    "text": "these labels get added by the instrumentation within the process when it handles events and other labels then",
    "start": "1168410",
    "end": "1174740"
  },
  {
    "text": "get attached by parameters when it scrapes your process so it attaches more",
    "start": "1174740",
    "end": "1179780"
  },
  {
    "text": "label saying like this is where the metric actually came from yeah the key",
    "start": "1179780",
    "end": "1187400"
  },
  {
    "text": "value based nature of this data model of the labels makes it pretty flexible it's not a hierarchy like some systems that",
    "start": "1187400",
    "end": "1194450"
  },
  {
    "text": "came before it stats T or graphite for example where the metric name looked a bit like a tree in a in a directory hierarchy where it",
    "start": "1194450",
    "end": "1202820"
  },
  {
    "text": "was then a bit more implicit you don't have to know which component means what and where to new dimensions if you wanted to add or",
    "start": "1202820",
    "end": "1209660"
  },
  {
    "text": "remove dimensions so this makes a little bit more implicit and flexible so now",
    "start": "1209660",
    "end": "1216470"
  },
  {
    "start": "1215000",
    "end": "1415000"
  },
  {
    "text": "you have this data model you collect data in it and you want to do useful stuff Prometheus brings its own query language",
    "start": "1216470",
    "end": "1223250"
  },
  {
    "text": "to do that it's called prompt QL and it's explicitly not a sequel style language some other systems have tried to build",
    "start": "1223250",
    "end": "1230870"
  },
  {
    "text": "sequel star languages for this kind of purpose and it ended up that many of the",
    "start": "1230870",
    "end": "1237200"
  },
  {
    "text": "typical computations you want to do on time series then either become impossible or very cumbersome and",
    "start": "1237200",
    "end": "1243410"
  },
  {
    "text": "unwieldy with with a sequel style language so premiere prom Carol is a bit",
    "start": "1243410",
    "end": "1249560"
  },
  {
    "text": "different and it's optimized towards the common computations that are useful on time series here's just a couple of",
    "start": "1249560",
    "end": "1257300"
  },
  {
    "text": "examples imagine you have this one type of exporter running on each node in your infrastructure called the node exporter",
    "start": "1257300",
    "end": "1263990"
  },
  {
    "text": "and one metric it exposes is the size of every partition that you have or every",
    "start": "1263990",
    "end": "1270080"
  },
  {
    "text": "file system and it has a couple of labels on each of these time series what's the mount point what's the device what's the machine and came from and so",
    "start": "1270080",
    "end": "1276650"
  },
  {
    "text": "on and then as a sample value it has the size now if you want to just get an",
    "start": "1276650",
    "end": "1282020"
  },
  {
    "text": "overview over infrastructure say like hey give me all partitions greater than 100 gigabytes that are not the route mount point you could start out with a",
    "start": "1282020",
    "end": "1289100"
  },
  {
    "text": "metric name doing a negative filter on the mount point label divided by a billion to roughly get from Giga bytes",
    "start": "1289100",
    "end": "1295790"
  },
  {
    "text": "to 2 bytes to get from bytes to Giga bytes and then you filter this list of",
    "start": "1295790",
    "end": "1301550"
  },
  {
    "text": "time series by hundreds to only get the ones that are larger than 100 and you",
    "start": "1301550",
    "end": "1306920"
  },
  {
    "text": "would get a labeled list of output series here another common query you",
    "start": "1306920",
    "end": "1313070"
  },
  {
    "text": "would see is the ratio of the rates of 500 status codes for example divided by",
    "start": "1313070",
    "end": "1318890"
  },
  {
    "text": "the ratio but by the rate of all requests in this case as averaged over",
    "start": "1318890",
    "end": "1324650"
  },
  {
    "text": "the last 5 minutes so this is one expression and there's a division operator right here now this is just a",
    "start": "1324650",
    "end": "1334370"
  },
  {
    "text": "single number in the output but these binary operations become really magic once you add",
    "start": "1334370",
    "end": "1340490"
  },
  {
    "text": "once you have still some dimensionality preserved by the summing on each of the",
    "start": "1340490",
    "end": "1346040"
  },
  {
    "text": "sides for example we might preserve the path label by adding a modifier and then",
    "start": "1346040",
    "end": "1352460"
  },
  {
    "text": "the binary operator automatically does a join on the path label so basically it",
    "start": "1352460",
    "end": "1360530"
  },
  {
    "text": "will look for label sets that are identical on the left-hand side and on the right-hand side of the binary",
    "start": "1360530",
    "end": "1366110"
  },
  {
    "text": "operator divide those by each other and propagate an equally labeled result into",
    "start": "1366110",
    "end": "1371960"
  },
  {
    "text": "the result and yeah so now you get for",
    "start": "1371960",
    "end": "1377000"
  },
  {
    "text": "example the ratio for each path another example if you track request latency Zin",
    "start": "1377000",
    "end": "1385970"
  },
  {
    "text": "what we call a histogram metric then we have a function that allows you to calculate during query time for example",
    "start": "1385970",
    "end": "1391549"
  },
  {
    "text": "the 99th give me the 99th percentile latency summed over all my instances and",
    "start": "1391549",
    "end": "1396980"
  },
  {
    "text": "as averaged over the last 5 minutes and we do that in a statistically valid way",
    "start": "1396980",
    "end": "1403360"
  },
  {
    "text": "of course it's a bit of an estimation going from a histogram to quantiles but",
    "start": "1403360",
    "end": "1409220"
  },
  {
    "text": "that depends on how you choose your packets how big the error will be ok so",
    "start": "1409220",
    "end": "1414980"
  },
  {
    "text": "now you know this language can get way more complicated and complex and you",
    "start": "1414980",
    "end": "1420110"
  },
  {
    "start": "1415000",
    "end": "1437000"
  },
  {
    "text": "have to learn it it's a bit of a steep learning curve admittedly but then it really pays off so now you can start",
    "start": "1420110",
    "end": "1425299"
  },
  {
    "text": "using it either to look at stuff right now in the built-in expression browser in Prometheus like what is the current",
    "start": "1425299",
    "end": "1432620"
  },
  {
    "text": "value of all Chinese views outputted by a given expression you can graph it over time but if you want to build real",
    "start": "1432620",
    "end": "1439340"
  },
  {
    "start": "1437000",
    "end": "1443000"
  },
  {
    "text": "dashboards that you can save and share with your colleagues and make it look nice we would recommend always using",
    "start": "1439340",
    "end": "1444679"
  },
  {
    "start": "1443000",
    "end": "1456000"
  },
  {
    "text": "Gravano Pravana is the most popular open source dashboard builder and it supports",
    "start": "1444679",
    "end": "1450440"
  },
  {
    "text": "all kind of backends including Prometheus now the cool thing and that",
    "start": "1450440",
    "end": "1457730"
  },
  {
    "start": "1456000",
    "end": "1539000"
  },
  {
    "text": "was also new at the time is how alerting and collecting transferees were no",
    "start": "1457730",
    "end": "1465049"
  },
  {
    "text": "longer separate systems we're now basing alerting directly on the times use data",
    "start": "1465049",
    "end": "1470240"
  },
  {
    "text": "that we collect so the idea with primitives is really collect everything has a time series first even if it just looks like a boolean value or",
    "start": "1470240",
    "end": "1477210"
  },
  {
    "text": "an enum so you know even just an up or down state you might model as a 0 or 1 sample value and that actually gets",
    "start": "1477210",
    "end": "1483360"
  },
  {
    "text": "compressed and stored really efficiently in primitives and then once you have collected that data then you can have",
    "start": "1483360",
    "end": "1490700"
  },
  {
    "text": "you know central Alert rules in your Prometheus server that act on the data here's one example of an alerting rule",
    "start": "1490700",
    "end": "1497750"
  },
  {
    "text": "that you would configure into Prometheus it takes an arbitrary prompt ql",
    "start": "1497750",
    "end": "1503520"
  },
  {
    "text": "expression and then when the prompt ula expression returns any result time series those time series become alerts",
    "start": "1503520",
    "end": "1510840"
  },
  {
    "text": "so a good alerting expression is one which has an empty result in this case we're taking an expression from earlier",
    "start": "1510840",
    "end": "1517350"
  },
  {
    "text": "we're kind of taking the ratio of 500 requests to total requests multiplying",
    "start": "1517350",
    "end": "1522780"
  },
  {
    "text": "by 100 to get a percentage and then filtering that list of paths by the ones that have an error rate that's above 5%",
    "start": "1522780",
    "end": "1529710"
  },
  {
    "text": "and then each one of these paths that has a too high error rate will become an alert and will be shipped to alert",
    "start": "1529710",
    "end": "1535590"
  },
  {
    "text": "manager just briefly Prometheus is",
    "start": "1535590",
    "end": "1541800"
  },
  {
    "start": "1539000",
    "end": "1577000"
  },
  {
    "text": "pretty simple to get started with operationally you know you can start on a local node just rights to a local data",
    "start": "1541800",
    "end": "1549450"
  },
  {
    "text": "directory it's written in goal which is also quite convenient for operations especially before the container",
    "start": "1549450",
    "end": "1556220"
  },
  {
    "text": "containerization started you can still get high availability by just running",
    "start": "1556220",
    "end": "1561900"
  },
  {
    "text": "two identically configured from easy servers which pull the same data calculate the same alerting rules and",
    "start": "1561900",
    "end": "1567300"
  },
  {
    "text": "then the alert manager will notice that there's the same alerts by their label sets and you will only get one",
    "start": "1567300",
    "end": "1573500"
  },
  {
    "text": "notification a single server can also get quite efficient if you put it on a",
    "start": "1573500",
    "end": "1580020"
  },
  {
    "start": "1577000",
    "end": "1643000"
  },
  {
    "text": "big iron machine well you know a big server we've managed to well in kind of",
    "start": "1580020",
    "end": "1586050"
  },
  {
    "text": "synthetic benchmarks get a million samples per second ingested but typically big promises servers have no",
    "start": "1586050",
    "end": "1592890"
  },
  {
    "text": "trouble storing a couple of million concurrently active time series at the",
    "start": "1592890",
    "end": "1598650"
  },
  {
    "text": "same time that means like when you do a full scrape of all your different targets there might be a couple of",
    "start": "1598650",
    "end": "1604260"
  },
  {
    "text": "million different answers that you're tracking in that well scrape the on disk storage format",
    "start": "1604260",
    "end": "1610090"
  },
  {
    "text": "is also really efficient typically its each sample takes around between like",
    "start": "1610090",
    "end": "1615190"
  },
  {
    "text": "one or two bytes on disk the local storage is good for keeping a couple of weeks maybe months of data there's some",
    "start": "1615190",
    "end": "1621730"
  },
  {
    "text": "people who are very courageous and put years in there but we don't really we",
    "start": "1621730",
    "end": "1627160"
  },
  {
    "text": "don't really recommend it because it's a single disk like you can backup it you can even take consistent snapshots and",
    "start": "1627160",
    "end": "1632650"
  },
  {
    "text": "then back them up and so on if you want to do that but of course it's not a clustered replicated horizontally",
    "start": "1632650",
    "end": "1639490"
  },
  {
    "text": "scalable system if you want to do that just briefly there's a way to do",
    "start": "1639490",
    "end": "1646060"
  },
  {
    "start": "1643000",
    "end": "1680000"
  },
  {
    "text": "decoupled remote storage either with this remote write and read interface that we have in prometheus or another",
    "start": "1646060",
    "end": "1652720"
  },
  {
    "text": "project that's called Panos made by friends of Prometheus also core core",
    "start": "1652720",
    "end": "1658990"
  },
  {
    "text": "people of Prometheus actually which integrates in a bit different way with",
    "start": "1658990",
    "end": "1664060"
  },
  {
    "text": "existing Prometheus deployments to add long-term storage and durability and",
    "start": "1664060",
    "end": "1670690"
  },
  {
    "text": "then giving you unified view over different primitive servers so I can really recommend checking this one out",
    "start": "1670690",
    "end": "1679110"
  },
  {
    "start": "1680000",
    "end": "1721000"
  },
  {
    "text": "so the last point is how does Prometheus work well together here with dynamic",
    "start": "1680040",
    "end": "1686890"
  },
  {
    "text": "environments so nowadays like first came to the ends then came the micro services",
    "start": "1686890",
    "end": "1692290"
  },
  {
    "text": "in the cluster schedulers and they're all kind of built layers and there's more and more moving parts to keep track",
    "start": "1692290",
    "end": "1697960"
  },
  {
    "text": "of and for a monitoring system it became harder and harder to still know what",
    "start": "1697960",
    "end": "1704620"
  },
  {
    "text": "should currently be where and then also gather the data reliably and define",
    "start": "1704620",
    "end": "1711760"
  },
  {
    "text": "alerts that are on the right aggregation level etc so how to make sense of these",
    "start": "1711760",
    "end": "1718780"
  },
  {
    "text": "dynamic environments the one answer that Prometheus has to this is of course service discovery integration Prometheus",
    "start": "1718780",
    "end": "1726250"
  },
  {
    "start": "1721000",
    "end": "1850000"
  },
  {
    "text": "supports talking to different sources of truth in your infrastructure for example the most prominent one is nowadays",
    "start": "1726250",
    "end": "1733650"
  },
  {
    "text": "talking to the kubernetes api server saying hey give me all the pods of a certain annotation or tie",
    "start": "1733650",
    "end": "1740470"
  },
  {
    "text": "that's on give me all the end points ingresses etc and Prometheus then uses",
    "start": "1740470",
    "end": "1748030"
  },
  {
    "text": "this information for three distinct but related purposes and I think it's important to understand that these are",
    "start": "1748030",
    "end": "1753900"
  },
  {
    "text": "different ones of course they're related first of all a monitoring system should",
    "start": "1753900",
    "end": "1759130"
  },
  {
    "text": "always know what should be there sometimes this question is a bit ignored for example in some push based",
    "start": "1759130",
    "end": "1766120"
  },
  {
    "text": "monitoring systems if a node or service never reports in maybe the maybe the",
    "start": "1766120",
    "end": "1773650"
  },
  {
    "text": "monitoring system will never know that something should have reported in not sure you still have to have some kind of",
    "start": "1773650",
    "end": "1779770"
  },
  {
    "text": "service discovery integration to correlate incoming data with what should",
    "start": "1779770",
    "end": "1785620"
  },
  {
    "text": "be coming in with pool and service discovery you get that kind of out-of-the-box so you know if parameters",
    "start": "1785620",
    "end": "1792370"
  },
  {
    "text": "cannot actually find something reach something that should be there then you can already use that for automatic",
    "start": "1792370",
    "end": "1798340"
  },
  {
    "text": "alerting then on a purely technical level the service discovery should give",
    "start": "1798340",
    "end": "1803380"
  },
  {
    "text": "from me to some information about how to actually pull data from the thing you just discovered so this is the actual",
    "start": "1803380",
    "end": "1809500"
  },
  {
    "text": "HTTP endpoint and maybe you need certain parameters to scrape from it and then",
    "start": "1809500",
    "end": "1816070"
  },
  {
    "text": "last but not least a good service discovery mechanism also gives you metadata about the object you discovered",
    "start": "1816070",
    "end": "1822840"
  },
  {
    "text": "so for example you know premiere kubernetes is a pretty good one that gives you all kinds of metadata about",
    "start": "1822840",
    "end": "1829900"
  },
  {
    "text": "what you discovered for example pod labels and annotations and so on and you can then choose in Prometheus VR built",
    "start": "1829900",
    "end": "1836289"
  },
  {
    "text": "in configuration language to map that metadata into the time series that you collect so now you know this time series",
    "start": "1836289",
    "end": "1843490"
  },
  {
    "text": "came from a part of a certain type and with a certain app name etc so in",
    "start": "1843490",
    "end": "1851230"
  },
  {
    "start": "1850000",
    "end": "1886000"
  },
  {
    "text": "conclusion yeah Prometheus works pretty well in these modern dynamic environments by giving",
    "start": "1851230",
    "end": "1857230"
  },
  {
    "text": "you you know a detailed data model to collect data in detail query language to make good use of it getting started easy",
    "start": "1857230",
    "end": "1864789"
  },
  {
    "text": "with simplicity and efficiency and then integrating with service discovery to actually track dynamic containers",
    "start": "1864789",
    "end": "1873880"
  },
  {
    "text": "etc in your infrastructure thank you and I'm open for questions",
    "start": "1873880",
    "end": "1881070"
  },
  {
    "text": "[Applause]",
    "start": "1881070",
    "end": "1886950"
  }
]