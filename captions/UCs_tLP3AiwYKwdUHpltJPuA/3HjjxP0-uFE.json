[
  {
    "start": "0",
    "end": "87000"
  },
  {
    "text": "[Music]",
    "start": "980",
    "end": "7849"
  },
  {
    "text": "good afternoon everyone my name is Kelly Robinson and I'm really excited to be here today goto is consistently one of my favorite",
    "start": "13059",
    "end": "20060"
  },
  {
    "text": "conferences so thank you for having me I have spent most of my career as a scholar developer including about three",
    "start": "20060",
    "end": "26359"
  },
  {
    "text": "of the last years doing data engineering at an advertising technology company when I started there in 2015 we were",
    "start": "26359",
    "end": "33739"
  },
  {
    "text": "running maybe spark 1.1 and so it's been really interesting to see how the",
    "start": "33739",
    "end": "39140"
  },
  {
    "text": "project has developed since then now we're in the spark 2 dot X series and",
    "start": "39140",
    "end": "44269"
  },
  {
    "text": "there's been a lot of developments in the language in the framework or project or whatever you want to call it since",
    "start": "44269",
    "end": "49699"
  },
  {
    "text": "the time that I started with that as you can imagine ad tech has a lot of data so I spend most of my time at that company",
    "start": "49699",
    "end": "55909"
  },
  {
    "text": "writing data pipelines using Scala and SPARC and we were doing that for our extract transform and load or ETL jobs",
    "start": "55909",
    "end": "62500"
  },
  {
    "text": "they're almost all of that was done in SPARC so dead today I'm going to talk to you about some of the things that I",
    "start": "62500",
    "end": "67729"
  },
  {
    "text": "learned from working with SPARC with the added benefit of analyzing some owned passwords pwned with the P is actually",
    "start": "67729",
    "end": "74750"
  },
  {
    "text": "pronounced own just say it with a piece silent and hackers will think you're cool just trust me I got out of AD Tech",
    "start": "74750",
    "end": "81140"
  },
  {
    "text": "about six months ago these days I'm a developer evangelist at Twilio where I get to work on a lot of fun and interesting things if you're not",
    "start": "81140",
    "end": "87590"
  },
  {
    "start": "87000",
    "end": "143000"
  },
  {
    "text": "familiar Twilio is a communications company that allows you as developers to add communications into your",
    "start": "87590",
    "end": "92780"
  },
  {
    "text": "applications with our API s for things like voice video SMS and authentication",
    "start": "92780",
    "end": "98680"
  },
  {
    "text": "with identity being so closely tied to phone numbers Twilio got into the security world by acquiring offi a few",
    "start": "98680",
    "end": "105560"
  },
  {
    "text": "years ago which many of you may know is a two-factor authentication provider this is like the last I'm gonna talk",
    "start": "105560",
    "end": "110630"
  },
  {
    "text": "about Twilio but since I work on our authentication products I spend most of my day thinking about how we can",
    "start": "110630",
    "end": "115670"
  },
  {
    "text": "validate and secure our online identities and passwords have been the way that we've done that on the internet",
    "start": "115670",
    "end": "121909"
  },
  {
    "text": "for most of the last thirty years so when I was writing this talk I needed some data to work with to show off spark",
    "start": "121909",
    "end": "127070"
  },
  {
    "text": "that was a little bit more interesting than advertising clickstream data because I don't know about you but I",
    "start": "127070",
    "end": "132200"
  },
  {
    "text": "don't really care how many times somebody has looked at an ad unfortunately or unfortunately for us there are about 500 million breached",
    "start": "132200",
    "end": "139250"
  },
  {
    "text": "credentials that are available for our perusing so before we get into that I'm",
    "start": "139250",
    "end": "144890"
  },
  {
    "start": "143000",
    "end": "169000"
  },
  {
    "text": "going to start by introducing apache spark look at why people are using it and how the language has been developed",
    "start": "144890",
    "end": "150820"
  },
  {
    "text": "and we'll give a bit of background on the state of passwords before we dive into that data then I'm going to show",
    "start": "150820",
    "end": "156860"
  },
  {
    "text": "you how spark works a little bit of live code and then finally we're gonna spend some time talking about the implications",
    "start": "156860",
    "end": "162080"
  },
  {
    "text": "at the intersection of Big Data and security so we can start by looking at",
    "start": "162080",
    "end": "167150"
  },
  {
    "text": "the apache spark project so spark markets itself as a fast unified",
    "start": "167150",
    "end": "173240"
  },
  {
    "start": "169000",
    "end": "215000"
  },
  {
    "text": "analytics engine for big data and machine learning and it provides a functional interface for doing this",
    "start": "173240",
    "end": "179720"
  },
  {
    "text": "which is going to be really familiar to a lot of Scala developers but you can do this with languages and data processing",
    "start": "179720",
    "end": "185810"
  },
  {
    "text": "in languages like Python even sequel and other languages too so it's generally",
    "start": "185810",
    "end": "191870"
  },
  {
    "text": "thought of as an improvement on top of Hadoop and MapReduce in fact it was built on top of Hadoop and MapReduce but",
    "start": "191870",
    "end": "197690"
  },
  {
    "text": "it's up to a hundred times faster than Hadoop and so that's why it's gained a lot of popularity lately is because it",
    "start": "197690",
    "end": "203420"
  },
  {
    "text": "makes things so much quicker to process data but we're using it to process big",
    "start": "203420",
    "end": "209480"
  },
  {
    "text": "data so what do I mean when I talk about Big Data I mean data that can't fit on a single machine although machine capabilities",
    "start": "209480",
    "end": "218600"
  },
  {
    "start": "215000",
    "end": "313000"
  },
  {
    "text": "are getting better every day so according to Gary here we don't even need spark we can all just go home talk",
    "start": "218600",
    "end": "223940"
  },
  {
    "text": "over just spin up this instance on AWS and everybody will be fine of course there are some companies that",
    "start": "223940",
    "end": "229549"
  },
  {
    "text": "are doing data analysis that petabyte scale that running AWS large-scale Ram instances are not necessarily going to",
    "start": "229549",
    "end": "236269"
  },
  {
    "text": "be the best option for you but when people ask me about big data like one of the things that I like to question",
    "start": "236269",
    "end": "242390"
  },
  {
    "text": "people on is like is your data actually that big do you need a tool like spark to work with it we're gonna assume that",
    "start": "242390",
    "end": "248690"
  },
  {
    "text": "you do for the sake of this presentation otherwise you wouldn't be here but I do like to point that out that one of the",
    "start": "248690",
    "end": "254480"
  },
  {
    "text": "things in working with big data is knowing the size of the data you're working with and how you can trim it down so I digress one of the major",
    "start": "254480",
    "end": "261620"
  },
  {
    "text": "benefits of spark is that it has built-in support from multiple languages and extensions I've already talked about the language a little bit but it also",
    "start": "261620",
    "end": "267919"
  },
  {
    "text": "has these extensions built in for things like structured streaming and machine learning it's really been promoting its",
    "start": "267919",
    "end": "274400"
  },
  {
    "text": "capabilities for a machine in AI lately but as my friend Ryan likes to say machine learning is just the",
    "start": "274400",
    "end": "280319"
  },
  {
    "text": "carrot that you dangle in front of engineers to get them to do the data engineering work so I think that's one",
    "start": "280319",
    "end": "285780"
  },
  {
    "text": "of the things that spark has a big benefit for is that it allows you to do both on the same platform and so I think",
    "start": "285780",
    "end": "291030"
  },
  {
    "text": "that's really valuable to a team that needs to do data science and data engineering because data science is not",
    "start": "291030",
    "end": "297449"
  },
  {
    "text": "all the sexy machine learning modeling that you might think of it as you know if you get into an any kind of data",
    "start": "297449",
    "end": "302819"
  },
  {
    "text": "science team a lot of the time you're going to spend is doing the data engineering data cleaning and getting",
    "start": "302819",
    "end": "307889"
  },
  {
    "text": "things into an analytics environment that you can start looking at and analyzing your data on the data",
    "start": "307889",
    "end": "314699"
  },
  {
    "start": "313000",
    "end": "360000"
  },
  {
    "text": "engineering side spark has two major abstractions and so we started with our D DS and our D D is a resilient",
    "start": "314699",
    "end": "320759"
  },
  {
    "text": "distributed datasets these are the major API it was a collection API for working",
    "start": "320759",
    "end": "325860"
  },
  {
    "text": "with data in spark and so you have a lazily evaluated list essentially that",
    "start": "325860",
    "end": "331199"
  },
  {
    "text": "is sparks major abstraction for working with data and processing data with a lot of functional interfaces arteezy's",
    "start": "331199",
    "end": "338969"
  },
  {
    "text": "aren't going away but for several reasons that will cover data frames and data sets are now the major abstraction",
    "start": "338969",
    "end": "345419"
  },
  {
    "text": "that most people are encouraged to use completely unrelated Lee it drives me absolutely insane that the camel casing",
    "start": "345419",
    "end": "351569"
  },
  {
    "text": "is not consistent on these this is not a typo the essent data sets is lowercase",
    "start": "351569",
    "end": "356759"
  },
  {
    "text": "and I really wish they would fix it but they're not going to so RTD is a little",
    "start": "356759",
    "end": "361889"
  },
  {
    "start": "360000",
    "end": "393000"
  },
  {
    "text": "bit more about those the distributed collections with a functional interface and they have a functional API that like",
    "start": "361889",
    "end": "367020"
  },
  {
    "text": "I mentioned is going to be familiar with for a lot of Scala developers like I mentioned my background is mostly in",
    "start": "367020",
    "end": "372449"
  },
  {
    "text": "Scala and so as a scala team moving to spark was pretty straightforward and you can see how that API works here in this",
    "start": "372449",
    "end": "379050"
  },
  {
    "text": "little bit of code and so we're a flat mapping mapping and reducing in this chained function for getting the word",
    "start": "379050",
    "end": "385710"
  },
  {
    "text": "count on whatever this data set is again our TDS are not going away they still",
    "start": "385710",
    "end": "392219"
  },
  {
    "text": "have their uses SPARC is easy to use but early on it was discovered that users of",
    "start": "392219",
    "end": "398879"
  },
  {
    "start": "393000",
    "end": "540000"
  },
  {
    "text": "rdd's were doing some bad things unintentionally and the creators of the",
    "start": "398879",
    "end": "404610"
  },
  {
    "text": "language or the creators of the project really wanted to make it easy to use and they were aware of this and so we're working on making it better one of",
    "start": "404610",
    "end": "411509"
  },
  {
    "text": "these big things that people were doing was using this group by key method and it turns out that this is really slow",
    "start": "411509",
    "end": "417090"
  },
  {
    "text": "and not performant and there are usually better ways to get the thing that you want and so usually people were doing",
    "start": "417090",
    "end": "422909"
  },
  {
    "text": "this to do some kind of counting or to do some kind of analysis on similar data same and that you would think of a doing",
    "start": "422909",
    "end": "429689"
  },
  {
    "text": "a group by and a sequel query but group by key was really really slow and so it slowed down your program execution aloft",
    "start": "429689",
    "end": "436080"
  },
  {
    "text": "and so this is from a data brings best practices guide and data bricks is the company that was created by the creators",
    "start": "436080",
    "end": "443039"
  },
  {
    "text": "of spark to offer a managed services platform on top of spark so you can run spark in the cloud and so data bricks is",
    "start": "443039",
    "end": "450060"
  },
  {
    "text": "seen as an authoritative source and so for them to publish a best practices guy that says avoid using this API that we",
    "start": "450060",
    "end": "456300"
  },
  {
    "text": "wrote for you especially in a language like Scala which if you here here for the Kotlin talked a lot of things in",
    "start": "456300",
    "end": "462900"
  },
  {
    "text": "scholar done with IDs you would have an RDD in your IDE and then you would see",
    "start": "462900",
    "end": "468479"
  },
  {
    "text": "that this method is available on your ID RDD and then you would say oh this sounds like something I would need this",
    "start": "468479",
    "end": "474270"
  },
  {
    "text": "sounds like a useful thing and you might not know that it's a bad thing to use again fortunately the team was aware of",
    "start": "474270",
    "end": "480990"
  },
  {
    "text": "this and working on ways to make this better so how can you do things like group by in a more efficient way and that's what",
    "start": "480990",
    "end": "487680"
  },
  {
    "text": "led us to data frames and data sets and so these have been around data frames have been around since about 1.6 but",
    "start": "487680",
    "end": "495210"
  },
  {
    "text": "data frames and data sets have really only been first class citizens in the spark world since about spark 2.0 and so",
    "start": "495210",
    "end": "502979"
  },
  {
    "text": "it was started by developing the data frames API and that mirrors something like data frames and pythons pandas or",
    "start": "502979",
    "end": "509879"
  },
  {
    "text": "Python pandas or a language like R and so for people that were familiar with those kind of data processing languages",
    "start": "509879",
    "end": "516479"
  },
  {
    "text": "this is very very similar to that and then data sets were developed more recently on top of that to offer a",
    "start": "516479",
    "end": "522690"
  },
  {
    "text": "strongly typed API for languages like Scala and Java that offer compile time",
    "start": "522690",
    "end": "528029"
  },
  {
    "text": "safety and this API works with structured or semi structured data and",
    "start": "528029",
    "end": "533699"
  },
  {
    "text": "contains a lot of optimizations under the hood for making this a lot faster than working with rdd's and because data",
    "start": "533699",
    "end": "543040"
  },
  {
    "start": "540000",
    "end": "602000"
  },
  {
    "text": "it's effectively require a schema depending on what you're doing there about three times faster than using",
    "start": "543040",
    "end": "549760"
  },
  {
    "text": "rdd's so if you're doing if you're working with structured data this is definitely the way to go they also",
    "start": "549760",
    "end": "556300"
  },
  {
    "text": "provide a sequel like DSL so you can do some of those group by queries that you",
    "start": "556300",
    "end": "562390"
  },
  {
    "text": "would want to do on your data and have those be quite fast and you can see that",
    "start": "562390",
    "end": "567610"
  },
  {
    "text": "sequel like DSL in this code example will look more at that later but you can even use raw sequel on top of that on",
    "start": "567610",
    "end": "574030"
  },
  {
    "text": "top of data sets and again this goes back to the fact that like you know SPARC is a cool new thing sequel is not",
    "start": "574030",
    "end": "581350"
  },
  {
    "text": "a cool new thing but it turns out sequel is really really useful and some of the best data scientists that I've worked",
    "start": "581350",
    "end": "587320"
  },
  {
    "text": "with are just really really good with sequel because that's the universal language for working with data and so I",
    "start": "587320",
    "end": "594070"
  },
  {
    "text": "think it's really great that spark has has recognized that and is integrating",
    "start": "594070",
    "end": "599770"
  },
  {
    "text": "that very closely into its API so you remember this ecosystem and the fact that starts for it's multiple languages",
    "start": "599770",
    "end": "606220"
  },
  {
    "text": "including languages like R in Python I do want to point out that Scala still",
    "start": "606220",
    "end": "611860"
  },
  {
    "text": "has the most robust language API it's still faster when using rdd's and it",
    "start": "611860",
    "end": "617770"
  },
  {
    "text": "also supports things like the strongly typed datasets that you're just not going to get in a language like Python because Python doesn't have compile time",
    "start": "617770",
    "end": "624670"
  },
  {
    "text": "type checking I mean it kind of does in Python 3 but I digress it's also almost all scala on to the",
    "start": "624670",
    "end": "630640"
  },
  {
    "text": "hood for spark and so if you need to dig into the source code you will have a benefit for knowing Scala there and",
    "start": "630640",
    "end": "636240"
  },
  {
    "text": "additionally a lot of the new things and new features that are released in spark come first for Scala and so you might",
    "start": "636240",
    "end": "642610"
  },
  {
    "text": "have to wait to get that in a language like Python whereas Scala will support it earlier",
    "start": "642610",
    "end": "648600"
  },
  {
    "text": "one of the big disadvantages early on them was Python was just so much slower than Scala and so this is one of the",
    "start": "648720",
    "end": "655210"
  },
  {
    "text": "reasons that I mentioned for Scala being a little bit better as a language choice for working with SPARC and so you would",
    "start": "655210",
    "end": "661210"
  },
  {
    "text": "have people that would learn Scala just to use SPARC so they could get some quick performance improvements without",
    "start": "661210",
    "end": "667780"
  },
  {
    "text": "having to learn about spark cluster tuning if any of you are familiar with Deane Wampler he has a very popular",
    "start": "667780",
    "end": "673240"
  },
  {
    "text": "tutorial that's called just enough Scala for SPARC and I think that's pretty awesome but as",
    "start": "673240",
    "end": "678970"
  },
  {
    "text": "you can see with the with the data frames API all of that performance kind of went out the window the performance",
    "start": "678970",
    "end": "685210"
  },
  {
    "text": "differences rather went out the window and now it's just as fast to run with the data frames api and python or r as",
    "start": "685210",
    "end": "692200"
  },
  {
    "text": "it is with scala and that's because the way that that API is constructed they're all executing the same code under the",
    "start": "692200",
    "end": "698020"
  },
  {
    "text": "hood it basically creates a sequel like execution layer and it's able to do that",
    "start": "698020",
    "end": "704590"
  },
  {
    "text": "because you provided it with a schema with the data because you're working with some kind of structured data I'm",
    "start": "704590",
    "end": "711070"
  },
  {
    "text": "really excited to see where the data community goes with this I think we've seen a lot of people using SPARC mostly with Scala but even as a Scala engineer",
    "start": "711070",
    "end": "718720"
  },
  {
    "text": "and some of the benefits that I already mentioned I think pythons gonna have a huge advantage moving forward just",
    "start": "718720",
    "end": "723940"
  },
  {
    "text": "because of the community support and some of the libraries are already supported in the Python data science",
    "start": "723940",
    "end": "728950"
  },
  {
    "text": "community and toolset so that's some of the basics of SPARC and where we're at",
    "start": "728950",
    "end": "735790"
  },
  {
    "start": "732000",
    "end": "802000"
  },
  {
    "text": "with the project today I wanted to spend just a couple of minutes talking about the state of password since it's gonna",
    "start": "735790",
    "end": "740890"
  },
  {
    "text": "help inform some of the analysis that we're doing like I mentioned I spend a lot of time thinking about authentication and we have a big problem",
    "start": "740890",
    "end": "747640"
  },
  {
    "text": "when it comes to password security and that's that people reuse passwords and they use passwords that are short and",
    "start": "747640",
    "end": "755740"
  },
  {
    "text": "obvious and easy to guess and this is a problem because even if you don't care if your myspace account gets hacked and",
    "start": "755740",
    "end": "762070"
  },
  {
    "text": "every myspace account got hacked recently like three years ago if that's recent this is a problem because if",
    "start": "762070",
    "end": "768370"
  },
  {
    "text": "you're using the same set of credentials from MySpace as you were for your email or your Bitcoin account you're gonna",
    "start": "768370",
    "end": "774730"
  },
  {
    "text": "have a bad time and that's because hackers like this guy and you know this guy is good because on top of all that",
    "start": "774730",
    "end": "781390"
  },
  {
    "text": "encryption he got the word hacked to read you know hackers like this guy are",
    "start": "781390",
    "end": "786400"
  },
  {
    "text": "gonna buy your leaked credentials off the internet and use them across sites on the Internet to try to make money and",
    "start": "786400",
    "end": "794020"
  },
  {
    "text": "people still do this because there's a lot of creative ways or even not so creative ways that they can make money off of your account credentials so you",
    "start": "794020",
    "end": "803140"
  },
  {
    "start": "802000",
    "end": "862000"
  },
  {
    "text": "might hope that nobody does this nobody use as a password as silly as one two three four five six nobody writes it on",
    "start": "803140",
    "end": "808270"
  },
  {
    "text": "their sticky note but of course people do in fact this password one-two-three",
    "start": "808270",
    "end": "813800"
  },
  {
    "text": "four-five-six has been seen over 20 million times and this is according to",
    "start": "813800",
    "end": "819350"
  },
  {
    "text": "the site have I been owned which I hope a lot of you have checked out already and this project from security",
    "start": "819350",
    "end": "825020"
  },
  {
    "text": "researcher Troy hunt Troy Hunt has access to a lot of password data from hundreds of breached",
    "start": "825020",
    "end": "831580"
  },
  {
    "text": "data breaches in the last several years and he provides a web interface for searching whether your email has been",
    "start": "831580",
    "end": "838160"
  },
  {
    "text": "included in one of those breaches there's also an interface for searching whether your password has been included in one of those breaches and there's a",
    "start": "838160",
    "end": "845120"
  },
  {
    "text": "couple other ways that you can access this data there's an API for it so you can call that API with a raw password or",
    "start": "845120",
    "end": "850550"
  },
  {
    "text": "a hashed password or you can download all of the password data and that's what we'll be using today to show off apache",
    "start": "850550",
    "end": "856880"
  },
  {
    "text": "spark so i'm gonna switch over can",
    "start": "856880",
    "end": "865460"
  },
  {
    "start": "862000",
    "end": "939000"
  },
  {
    "text": "everybody see that okay font big enough okay so the data that we're gonna be",
    "start": "865460",
    "end": "872540"
  },
  {
    "text": "looking at is the data dump from apache or from the own password site and this",
    "start": "872540",
    "end": "878270"
  },
  {
    "text": "data comes in a a specific format and so it comes with hashed passwords and the",
    "start": "878270",
    "end": "883370"
  },
  {
    "text": "count of the number of times the password has been seen for the sake of time I've already joined that data with",
    "start": "883370",
    "end": "890300"
  },
  {
    "text": "some data of like known plaintext passwords and so we have a smaller data set here that we can work with because",
    "start": "890300",
    "end": "896600"
  },
  {
    "text": "working with hash passwords is only so interesting so we're gonna be looking at the plaintext passwords the tool that",
    "start": "896600",
    "end": "901820"
  },
  {
    "text": "I'm using here is called Zeppelin this is a open-source alternative to something like data bricks that allows",
    "start": "901820",
    "end": "907910"
  },
  {
    "text": "you to run spark either locally or on a machine that you choose and so this is a deployment that I have just running on",
    "start": "907910",
    "end": "913520"
  },
  {
    "text": "localhost it's kind of like a Jupiter notebook so you can do something with this and it has a built in spark interpreter and so it's a really nice",
    "start": "913520",
    "end": "920180"
  },
  {
    "text": "way to get up and running and demo spark for the sake of something like this and so I'm gonna go ahead and read in our",
    "start": "920180",
    "end": "926690"
  },
  {
    "text": "password data I already wrote that out to a CSV and so I can pull in all of our",
    "start": "926690",
    "end": "932930"
  },
  {
    "text": "passwords now and let's go ahead and see how many passwords we have let that run",
    "start": "932930",
    "end": "940610"
  },
  {
    "start": "939000",
    "end": "1095000"
  },
  {
    "text": "so we have about 700,000 passwords that work with right now that sounds about right",
    "start": "940610",
    "end": "946610"
  },
  {
    "text": "you'll notice that we can also do a schema check on this you can see a little bit of that in the output that we",
    "start": "946610",
    "end": "953810"
  },
  {
    "text": "already have but we can print the schema using spark this isn't exactly what we want our CSV has a header so we can go",
    "start": "953810",
    "end": "962660"
  },
  {
    "text": "ahead and use that header by setting the option on that to interpret our header",
    "start": "962660",
    "end": "968050"
  },
  {
    "text": "and once we do that we'll get a little bit nicer field names so we have our",
    "start": "968050",
    "end": "973610"
  },
  {
    "text": "password our hash and our count the one thing that we still don't have is the",
    "start": "973610",
    "end": "978680"
  },
  {
    "text": "correct types for all those and so count is going to not be a string file we want us to be able to support that by numbers",
    "start": "978680",
    "end": "984410"
  },
  {
    "text": "and so we can tell it to infer the schema and now we have the schema that",
    "start": "984410",
    "end": "995960"
  },
  {
    "text": "we want we have passwords and hashes which are strings and counts which are integers before we actually print these",
    "start": "995960",
    "end": "1001780"
  },
  {
    "text": "out though I need to do something else and that's because there's a lot of",
    "start": "1001780",
    "end": "1007480"
  },
  {
    "text": "really naughty things that people include in passwords and this is a beautiful public forum and I don't want any of those things showing up on this",
    "start": "1007480",
    "end": "1013510"
  },
  {
    "text": "screen so I didn't already filter those out I'm gonna do that now and so I have a text file with a bunch of bad words in",
    "start": "1013510",
    "end": "1019600"
  },
  {
    "text": "it that we don't want to include and we",
    "start": "1019600",
    "end": "1024819"
  },
  {
    "text": "can read in that text file explicitly there's 63 words in there use your",
    "start": "1024820",
    "end": "1030339"
  },
  {
    "text": "imaginations for what those include and we'll go ahead and join these data sets together and so I can join our bad words",
    "start": "1030339",
    "end": "1036760"
  },
  {
    "text": "and I'm going to join that on our password column and if that contains our",
    "start": "1036760",
    "end": "1047319"
  },
  {
    "text": "bad word and then instead of doing a",
    "start": "1047320",
    "end": "1053440"
  },
  {
    "text": "inner join we're going to specify that this is a left ante join and so the",
    "start": "1053440",
    "end": "1060190"
  },
  {
    "text": "schema looks the same but I want to print a count here to make sure that we have less and yeah so we got rid of",
    "start": "1060190",
    "end": "1068830"
  },
  {
    "text": "about 20,000 passwords by doing that in are already a little bit small data set and that's a lot of passwords that",
    "start": "1068830",
    "end": "1074110"
  },
  {
    "text": "contain some naughty stuff so use your imagination like I said we're not going to be showing that up here so now that we have that we can go",
    "start": "1074110",
    "end": "1081340"
  },
  {
    "text": "ahead and show what our most popular passwords are and we're gonna be using this sequel like syntax to do this and",
    "start": "1081340",
    "end": "1087280"
  },
  {
    "text": "so we can go ahead and order by our count call that the descending and then we can show these and so it has a handy",
    "start": "1087280",
    "end": "1094120"
  },
  {
    "text": "function to show the top 20 things this will interpret everything because we have to order by things anyway but if",
    "start": "1094120",
    "end": "1101380"
  },
  {
    "start": "1095000",
    "end": "1127000"
  },
  {
    "text": "you didn't have any kind of ordering operation it would evaluate this lazily and only pull the first 20 things and so",
    "start": "1101380",
    "end": "1107890"
  },
  {
    "text": "these are about what you would expect sequential numbers all one's the word",
    "start": "1107890",
    "end": "1114100"
  },
  {
    "text": "password these are really disappointing results the word password has appeared over three million times and data sets",
    "start": "1114100",
    "end": "1120940"
  },
  {
    "text": "and so that's really fun to know please don't use these as your passwords but",
    "start": "1120940",
    "end": "1126580"
  },
  {
    "text": "there we can do some more interesting stuff with this now and so one of the things that I want to do is find out what the most common length of a",
    "start": "1126580",
    "end": "1132340"
  },
  {
    "start": "1127000",
    "end": "1147000"
  },
  {
    "text": "password is and so we can add a column to this using the width column operator and I'm gonna call that column length",
    "start": "1132340",
    "end": "1138460"
  },
  {
    "text": "sequel or spark sequel has a built in length function so we can go ahead and use that will get the length of all of",
    "start": "1138460",
    "end": "1145390"
  },
  {
    "text": "our passwords and so now that we have that one of the cool things that we can do is go ahead and run some raw sequel",
    "start": "1145390",
    "end": "1153040"
  },
  {
    "start": "1147000",
    "end": "1197000"
  },
  {
    "text": "on our password table and so this is really awesome if you're working with any kind of team that might know how to",
    "start": "1153040",
    "end": "1159310"
  },
  {
    "text": "have programmers on it but it has business analysts and no sequel and so you can go ahead and write sequel like",
    "start": "1159310",
    "end": "1164350"
  },
  {
    "text": "you absolutely always would have so you can write your CEO and your some of counts and you could grab this from your",
    "start": "1164350",
    "end": "1171490"
  },
  {
    "text": "passwords table and then I'm going to group by and order by our length and I",
    "start": "1171490",
    "end": "1181420"
  },
  {
    "text": "did not save the password table as a table so we need to actually create a view for this and we do need to give it",
    "start": "1181420",
    "end": "1190930"
  },
  {
    "text": "a name so that it knows what we're selecting from and so let's go ahead and do that and then we can select from our",
    "start": "1190930",
    "end": "1196420"
  },
  {
    "text": "table and once we run this Zeppelin provides some built-in functionality",
    "start": "1196420",
    "end": "1202570"
  },
  {
    "text": "that's pretty cool so you can do things like have built-in graphs pie charts you",
    "start": "1202570",
    "end": "1208030"
  },
  {
    "text": "can think about how scatter charts might be interesting or scatter plots would be interest for certain type of modeling or outlier",
    "start": "1208030",
    "end": "1213850"
  },
  {
    "text": "data I think for our purposes of finding the most common lengths the bar charts pretty easy to read and so here you can",
    "start": "1213850",
    "end": "1220120"
  },
  {
    "text": "see that 6 character passwords are the most common that doesn't surprise me as",
    "start": "1220120",
    "end": "1225610"
  },
  {
    "text": "the fact that six and eight character password minimum requirements are very common at Twilio our password minimum",
    "start": "1225610",
    "end": "1231520"
  },
  {
    "text": "requirement is 14 characters and that annoys a lot of people but it makes passwords way more secure and it also",
    "start": "1231520",
    "end": "1238180"
  },
  {
    "text": "encourages people who use password managers um cool so now that we've done",
    "start": "1238180",
    "end": "1244570"
  },
  {
    "start": "1242000",
    "end": "1275000"
  },
  {
    "text": "that one of the other things I want to show about Zeppelin really quick it has some very cool built-in analysis tools",
    "start": "1244570",
    "end": "1251200"
  },
  {
    "text": "so you can do something like filtering the password on a password like claws and so you could do this for something a",
    "start": "1251200",
    "end": "1257770"
  },
  {
    "text": "word like Chicago let's go ahead and put that in quotes and so you could do that",
    "start": "1257770",
    "end": "1267040"
  },
  {
    "text": "but then you can take it a step further and make this a configurable field and",
    "start": "1267040",
    "end": "1272230"
  },
  {
    "text": "so we'll need to specify that this is a field we're expecting that on and so now you can see that this this text box",
    "start": "1272230",
    "end": "1278530"
  },
  {
    "text": "comes up and so you can see all these passwords that contain the word Chicago obviously the minimum length there is",
    "start": "1278530",
    "end": "1283540"
  },
  {
    "text": "going to be 10 but I could change this to be my name then we'd have a lot of",
    "start": "1283540",
    "end": "1289060"
  },
  {
    "text": "people that are 11,000 people named Kelly that's weird or you could you know make this Scala whatever you want I",
    "start": "1289060",
    "end": "1295810"
  },
  {
    "text": "would be surprised if there's anyone oh my gosh it's probably somewhere that contains the word Scala but anyway so",
    "start": "1295810",
    "end": "1301570"
  },
  {
    "text": "there's a lot of cool things that you can do with this and I think this is a really cool kind of sharing tool that you could deploy and use within a team",
    "start": "1301570",
    "end": "1307770"
  },
  {
    "text": "and it's all open-source and so you would have to manage to the deployment of Zeppelin but there this is actively",
    "start": "1307770",
    "end": "1313840"
  },
  {
    "text": "being developed and I think there's some really cool potential for using a tool like this but that's you know some",
    "start": "1313840",
    "end": "1319900"
  },
  {
    "start": "1318000",
    "end": "1348000"
  },
  {
    "text": "metadata about passwords what's really fun is if you actually dig into the password data one of the obvious things",
    "start": "1319900",
    "end": "1325630"
  },
  {
    "text": "that you can start to look at about passwords is like what they contain in the actual like words and so you can",
    "start": "1325630",
    "end": "1332200"
  },
  {
    "text": "look at proper nouns and you could approach this as a few different data sets you could look at like celebrity name",
    "start": "1332200",
    "end": "1337350"
  },
  {
    "text": "or geographical places I'm going to use dog names because I love dogs I can't",
    "start": "1337350",
    "end": "1343890"
  },
  {
    "text": "have one in my apartment I'm not bitter that's fine so I can read in our dog",
    "start": "1343890",
    "end": "1350210"
  },
  {
    "start": "1348000",
    "end": "1382000"
  },
  {
    "text": "JSON and we can print the schema on this and this is going to be popular dog",
    "start": "1350210",
    "end": "1355620"
  },
  {
    "text": "names from I think 2006 and so you can see that it's does schema inference on our JSON file too I'm gonna go ahead and",
    "start": "1355620",
    "end": "1362400"
  },
  {
    "text": "pull that up and so this is what our JSON looks like one of the cool things I",
    "start": "1362400",
    "end": "1367620"
  },
  {
    "text": "think about the schema inference from spark and especially the spark sequel and data frames and data sets API is if",
    "start": "1367620",
    "end": "1373799"
  },
  {
    "text": "I go in and add Max's favorite food here let's say it's chicken we want to",
    "start": "1373799",
    "end": "1379380"
  },
  {
    "text": "minimize that again because spark doesn't like multiple line JSON if I go ahead and print the schema on this again",
    "start": "1379380",
    "end": "1385890"
  },
  {
    "text": "it's recognized that food is now part of the schema even though this is going to be a null column for every other value",
    "start": "1385890",
    "end": "1391799"
  },
  {
    "text": "in there and so this is pretty cool because our data set here that were working with is pretty small but you can",
    "start": "1391799",
    "end": "1397260"
  },
  {
    "text": "imagine if you have JSON log data that is very inconsistent with the type of fields that it has if it's a semi-structured schema there you can do",
    "start": "1397260",
    "end": "1404910"
  },
  {
    "text": "the schema inference to understand if those fields exist you can use that to inspect your data and spark will do the",
    "start": "1404910",
    "end": "1411390"
  },
  {
    "text": "heavy lifting there you don't have to build all the encoders for understanding what kind of fields are in your JSON and",
    "start": "1411390",
    "end": "1416850"
  },
  {
    "text": "it also allows you to change your JSON a little bit more seamlessly all right so now that we have that we can go ahead",
    "start": "1416850",
    "end": "1422700"
  },
  {
    "text": "and rejoin these things together with our passwords and see what is contained in those so we want our passwords",
    "start": "1422700",
    "end": "1429450"
  },
  {
    "text": "password column and that contains our",
    "start": "1429450",
    "end": "1434960"
  },
  {
    "text": "dog column and then I'm just going to",
    "start": "1434960",
    "end": "1439980"
  },
  {
    "text": "default this to be an inner join you don't have to provide the type of join that it has we'll go ahead and show that",
    "start": "1439980",
    "end": "1446419"
  },
  {
    "text": "well let's first only select the fields that we want and so I want the name oops",
    "start": "1446419",
    "end": "1453870"
  },
  {
    "text": "the password and then the couch and",
    "start": "1453870",
    "end": "1459780"
  },
  {
    "text": "let's also order this by a descending count",
    "start": "1459780",
    "end": "1465169"
  },
  {
    "text": "that's because I didn't type the right field alright so now that we have this",
    "start": "1466560",
    "end": "1472810"
  },
  {
    "start": "1470000",
    "end": "1514000"
  },
  {
    "text": "we'll get the most common name dog names that are in passwords our friend Charlie here is very loved you can see that he's",
    "start": "1472810",
    "end": "1479290"
  },
  {
    "text": "shows up in about 15,000 passwords one other thing that I want to show you now is you can so we looked at the length",
    "start": "1479290",
    "end": "1486310"
  },
  {
    "text": "function which is a built-in function for sparks equal but you can define your own UDF's or user-defined functions and",
    "start": "1486310",
    "end": "1492850"
  },
  {
    "text": "so I'm going to find one that's lowercase and that's a UDF and the syntax for this is a little weird so",
    "start": "1492850",
    "end": "1498580"
  },
  {
    "text": "this is takes in a string and then we'll output whatever you tell it to and then",
    "start": "1498580",
    "end": "1504940"
  },
  {
    "text": "you can write this in in scholar code and so now that I have that I can go",
    "start": "1504940",
    "end": "1510430"
  },
  {
    "text": "ahead and lowercase my dog name and see if that's more common you see how we",
    "start": "1510430",
    "end": "1515770"
  },
  {
    "start": "1514000",
    "end": "1543000"
  },
  {
    "text": "have our user-defined function there and so now our passwords jumped up from fifteen thousand two hundred and seventy",
    "start": "1515770",
    "end": "1522850"
  },
  {
    "text": "eight thousand times that Charlie has been used in password names so that's you know a little sad and happy at the",
    "start": "1522850",
    "end": "1530200"
  },
  {
    "text": "same time means Charlie's very loved but probably don't use your dog name in your password I'm gonna stop the live code",
    "start": "1530200",
    "end": "1536320"
  },
  {
    "text": "there because there's a couple other things that I want to wrap up all right",
    "start": "1536320",
    "end": "1543240"
  },
  {
    "start": "1543000",
    "end": "1560000"
  },
  {
    "text": "yes if nothing else I think we've learned not to include our dog names in",
    "start": "1543240",
    "end": "1548740"
  },
  {
    "text": "our passwords really in this whole talk revolved I'm me using a dog rights tweet I did modify the text of this one he",
    "start": "1548740",
    "end": "1555070"
  },
  {
    "text": "hasn't actually texted about using passwords huh unfortunately maybe someday but in all",
    "start": "1555070",
    "end": "1561070"
  },
  {
    "start": "1560000",
    "end": "1597000"
  },
  {
    "text": "seriousness some of the benefits of using spark that I think we've seen so not only is it fast and flexible but",
    "start": "1561070",
    "end": "1566290"
  },
  {
    "text": "it's really good for exploration and it's proven for large systems and so I really love it that you can take",
    "start": "1566290",
    "end": "1571690"
  },
  {
    "text": "something like spark that is really robust and use something as proven and tried as a language like sequel and use",
    "start": "1571690",
    "end": "1578410"
  },
  {
    "text": "it in the same way I think this is going to be really powerful for teams that work together in data science and data",
    "start": "1578410",
    "end": "1584290"
  },
  {
    "text": "engineering capacities I know I've done that at different companies and it's very very valuable to have a toolset that can work for both teams and any",
    "start": "1584290",
    "end": "1592210"
  },
  {
    "text": "kind of toolset that is going to support that is going to have a huge advantage of course we're not without challenges",
    "start": "1592210",
    "end": "1599149"
  },
  {
    "start": "1597000",
    "end": "1650000"
  },
  {
    "text": "and I want to highlight some of the challenges and operationalizing because",
    "start": "1599149",
    "end": "1604249"
  },
  {
    "text": "while it's pretty easy for me to spin up Zeppelin on my local machine it's definitely not as easy to do that across a data cluster and that's the entire",
    "start": "1604249",
    "end": "1610700"
  },
  {
    "text": "reason that this is useful is because you want to be running something like this on distributed data it's still a",
    "start": "1610700",
    "end": "1617570"
  },
  {
    "text": "young project though like I mentioned it's only been GA for about three or four years and all this is getting",
    "start": "1617570",
    "end": "1623299"
  },
  {
    "text": "better all the time there's always new tooling that's coming out the documentation is currently is consistently being improved the sub note",
    "start": "1623299",
    "end": "1630739"
  },
  {
    "text": "here that you can find these in my slides when I post them Heather Miller who is a computer science professor and",
    "start": "1630739",
    "end": "1636019"
  },
  {
    "text": "does a lot of research and work with SPARC and Scala just published this great blog post earlier this month on",
    "start": "1636019",
    "end": "1641269"
  },
  {
    "text": "how to deploy SPARC on a cluster because this isn't something that really existed before earlier this month and so she's a",
    "start": "1641269",
    "end": "1647869"
  },
  {
    "text": "great tutorial for that if you want to check that out I included this error message on the off chance of my life",
    "start": "1647869",
    "end": "1654049"
  },
  {
    "start": "1650000",
    "end": "1688000"
  },
  {
    "text": "coding went perfectly but this is something that we didn't see which is you can get these really nested error",
    "start": "1654049",
    "end": "1659779"
  },
  {
    "text": "messages that are really crazy and obscure and hard to follow when you're working with a language like SPARC this",
    "start": "1659779",
    "end": "1666109"
  },
  {
    "text": "is something when I was initially exploring this data set again just on my local machine I was trying to join two",
    "start": "1666109",
    "end": "1672859"
  },
  {
    "text": "tables together and I was getting a really obscure joint message that was saying the join key couldn't be found",
    "start": "1672859",
    "end": "1678590"
  },
  {
    "text": "but it wasn't telling me what join key I was looking for and so there's things like that that are still a little",
    "start": "1678590",
    "end": "1683720"
  },
  {
    "text": "challenging to parse through and understand where you're actually making mistakes with regards to documentation I",
    "start": "1683720",
    "end": "1692539"
  },
  {
    "start": "1688000",
    "end": "1745000"
  },
  {
    "text": "do want to give a shout out to this documentation that's been compiled by Yacht sack and so he has documentation",
    "start": "1692539",
    "end": "1699409"
  },
  {
    "text": "for mastering apart apache spark and a companion guide for mastering spark sequel you might have noticed during the",
    "start": "1699409",
    "end": "1706159"
  },
  {
    "text": "live code portion that there's a few different ways that we kind of looked at what data columns we were trying to pull",
    "start": "1706159",
    "end": "1713480"
  },
  {
    "text": "so there was that dollar sign quote method which is an alias for accessing a column which is an alias for accessing",
    "start": "1713480",
    "end": "1719570"
  },
  {
    "text": "the table that acts as a column there's like a lot of different ways you can do this and none of it is terribly well documented but Yap psych has done all",
    "start": "1719570",
    "end": "1726350"
  },
  {
    "text": "that documentation for us so I'm very grateful for him I know that's somebody that's been working with spark a lot that I have",
    "start": "1726350",
    "end": "1732259"
  },
  {
    "text": "referenced this a lot and it's not super easy to find because it's not part of the official documentation so if you're",
    "start": "1732259",
    "end": "1738229"
  },
  {
    "text": "part of the spark community and you are working with this kind of stuff definitely contribute back as much as you can and finally I want to leave you",
    "start": "1738229",
    "end": "1748969"
  },
  {
    "start": "1745000",
    "end": "1763000"
  },
  {
    "text": "thinking about some of the security implications of working with big data it's a really useful tool for finding",
    "start": "1748969",
    "end": "1756109"
  },
  {
    "text": "bad things and for analyzing large systems and gaining insights into your business but you could also be the",
    "start": "1756109",
    "end": "1762469"
  },
  {
    "text": "target of the next compromised database I think data privacy is on everybody's mind lately",
    "start": "1762469",
    "end": "1767479"
  },
  {
    "start": "1763000",
    "end": "1815000"
  },
  {
    "text": "whether it's gdpr and as I say that some of you might cringe because that's all you've been thinking yeah I saw that",
    "start": "1767479",
    "end": "1773599"
  },
  {
    "text": "it's all you've been thinking about lately and these are European privacy regulations that are going into effect",
    "start": "1773599",
    "end": "1778759"
  },
  {
    "text": "next month that's why all of you these companies are emailing you right now saying that they've updated their Terms",
    "start": "1778759",
    "end": "1784069"
  },
  {
    "text": "of Service and that's because of these privacy regulations and this is something that's been very very fresh on",
    "start": "1784069",
    "end": "1790399"
  },
  {
    "text": "everyone's mind especially last year with things like the Equifax breach more recently even with the Cambridge",
    "start": "1790399",
    "end": "1796549"
  },
  {
    "text": "analytical scandal everybody's thinking about InfoSec and how it affects your day to day and at this point we really",
    "start": "1796549",
    "end": "1803329"
  },
  {
    "text": "can't separate security and software development so if you if you're working on any kind of engineering team whether",
    "start": "1803329",
    "end": "1809690"
  },
  {
    "text": "that's the security team or not please be mindful of the data you store and how you're accessing it because",
    "start": "1809690",
    "end": "1816349"
  },
  {
    "start": "1815000",
    "end": "1875000"
  },
  {
    "text": "nobody's security is perfect if you have influence over the security at your company including if your company and",
    "start": "1816349",
    "end": "1823489"
  },
  {
    "text": "especially if your company is too small to have a dedicated security team please",
    "start": "1823489",
    "end": "1828769"
  },
  {
    "text": "make sure that you're sparring passwords or other personally identifiable information securely a lot of the",
    "start": "1828769",
    "end": "1835369"
  },
  {
    "text": "reasons that password data is accessible is because all of those data breaches were storing passwords in a way that",
    "start": "1835369",
    "end": "1840379"
  },
  {
    "text": "could be decoded into something that was referenceable or something that could be",
    "start": "1840379",
    "end": "1846349"
  },
  {
    "text": "searchable and same goes with email addresses that's all their PII that people can use to to breach other",
    "start": "1846349",
    "end": "1853759"
  },
  {
    "text": "accounts this tweet if you haven't seen it came from a conversation earlier this month on Twitter where it was revealed",
    "start": "1853759",
    "end": "1860359"
  },
  {
    "text": "that t-mobile or at least two mobile Austria was story in plain text passwords and unfortunately this support",
    "start": "1860359",
    "end": "1867410"
  },
  {
    "text": "agent really really doubled down on that not being a problem which of course we",
    "start": "1867410",
    "end": "1873140"
  },
  {
    "text": "know is not true so that's from the company side of things what can you do",
    "start": "1873140",
    "end": "1878540"
  },
  {
    "start": "1875000",
    "end": "1925000"
  },
  {
    "text": "as a consumer probably preaching to the choir and an audience like this but use a password manager turn on to FA",
    "start": "1878540",
    "end": "1885290"
  },
  {
    "text": "wherever it's offered those two things alone will offer you a huge amount of security and your personal identity",
    "start": "1885290",
    "end": "1891470"
  },
  {
    "text": "online and as engineers and people that might be a little bit more versed in these things you can also do a lot to",
    "start": "1891470",
    "end": "1898070"
  },
  {
    "text": "encourage your friends and family to start adapting these practices - my dad",
    "start": "1898070",
    "end": "1903320"
  },
  {
    "text": "uses a password-protected Excel spreadsheet to store his passwords and honestly I'm pretty proud of him because",
    "start": "1903320",
    "end": "1909920"
  },
  {
    "text": "well we'll get him on LastPass eventually but the important thing about that is the security mindset is there",
    "start": "1909920",
    "end": "1916100"
  },
  {
    "text": "and a lot of times that can be the biggest hurdle in convincing somebody that they need to think about their",
    "start": "1916100",
    "end": "1921260"
  },
  {
    "text": "online privacy a little bit more strongly so thank you for letting me",
    "start": "1921260",
    "end": "1927320"
  },
  {
    "start": "1925000",
    "end": "1941000"
  },
  {
    "text": "spend some time with you this afternoon diving into spark and some of the through the lens of own passwords I love",
    "start": "1927320",
    "end": "1934610"
  },
  {
    "text": "chatting about this stuff so hopefully I've inspired you to dig into your own data with Apache spark if you have big",
    "start": "1934610",
    "end": "1942620"
  },
  {
    "start": "1941000",
    "end": "1965000"
  },
  {
    "text": "data and especially I think spark and especially spark sequel are really great tools for doing a lot of powerful",
    "start": "1942620",
    "end": "1950000"
  },
  {
    "text": "analysis I've also decided that this I mean stock photos of hackers are just",
    "start": "1950000",
    "end": "1955790"
  },
  {
    "text": "hilarious and I've decided this lady skeleton stock photo whatever she is is",
    "start": "1955790",
    "end": "1961250"
  },
  {
    "text": "my alter ego so that's why I'm closing with this I will post these slides to my",
    "start": "1961250",
    "end": "1966830"
  },
  {
    "start": "1965000",
    "end": "1993000"
  },
  {
    "text": "Twitter SlideShare is apparently having some problems right now but I will have them ready for you soon I'm gonna have a",
    "start": "1966830",
    "end": "1972800"
  },
  {
    "text": "tutorial ready on my blog soon to show you how to get started on with your own data with Zeppelin and Apache spark come",
    "start": "1972800",
    "end": "1980120"
  },
  {
    "text": "find me after this if you have any questions about Scala spark or security once again my name is Kelly Robinson",
    "start": "1980120",
    "end": "1985780"
  },
  {
    "text": "please use a password manager and thank you for listening",
    "start": "1985780",
    "end": "1990580"
  },
  {
    "start": "1993000",
    "end": "2374000"
  },
  {
    "text": "Thank You Kelly and mad props for the live coding we actually have a little time for some questions from the audience if we have any if anyone",
    "start": "1994490",
    "end": "2001010"
  },
  {
    "text": "submitted some from the go to app or if just cause anything so in my company",
    "start": "2001010",
    "end": "2007940"
  },
  {
    "text": "we're using not only spark with Scala but also PI spark okay so when you get",
    "start": "2007940",
    "end": "2013880"
  },
  {
    "text": "errors it is amazing because you get both stack stack calls in Python and",
    "start": "2013880",
    "end": "2019850"
  },
  {
    "text": "Chavez and in Java at the same time wait oh no yay how do you deep out things",
    "start": "2019850",
    "end": "2026000"
  },
  {
    "text": "like when something goes wrong especially if I'm teaching someone do you have some way to figures like Oh",
    "start": "2026000",
    "end": "2031040"
  },
  {
    "text": "start from the beginning or start from the N or that's a really good question I don't have a ton of experience working",
    "start": "2031040",
    "end": "2036080"
  },
  {
    "text": "with Pi spark so I don't have as much to offer on debugging Python stack traces that sounds really challenging I mean a",
    "start": "2036080",
    "end": "2043700"
  },
  {
    "text": "lot of times it's understanding what your schemas are what the type of data that you're working with is using things",
    "start": "2043700",
    "end": "2051800"
  },
  {
    "text": "as much as possible like data sets I didn't actually I forgot to go into the data set side of thing but you can use",
    "start": "2051800",
    "end": "2057858"
  },
  {
    "text": "strongly typed api's that will help you do a little bit of the a little bit of compile time safety as much as possible",
    "start": "2057859",
    "end": "2064580"
  },
  {
    "text": "again that's not gonna be possible with something like Python but honestly like a lot of print statements seeing what",
    "start": "2064580",
    "end": "2070190"
  },
  {
    "text": "your data looks like iteratively one of the things that you can do with spark and Scala especially and it might be similar in Python is you saw me kind of",
    "start": "2070190",
    "end": "2077118"
  },
  {
    "text": "chaining methods together and so this is something that a lot of people will do is so you have one operation right after",
    "start": "2077119",
    "end": "2082429"
  },
  {
    "text": "the other and then breaking that up as much as possible to see where something breaks unfortunately I don't have much",
    "start": "2082429",
    "end": "2087980"
  },
  {
    "text": "else to offer than that you mentioned Dean wampler 'he's just enough Scala for",
    "start": "2087980",
    "end": "2093530"
  },
  {
    "text": "Apache spark yeah I found that can be a good resource I've had clients where I'll just sent their poet point their",
    "start": "2093530",
    "end": "2100010"
  },
  {
    "text": "data analysts who have a Python background at that and teach them enough Scala to write Scala SPARC just because the debugging",
    "start": "2100010",
    "end": "2105950"
  },
  {
    "text": "is so much easier cool any other questions from the audience or people putting things in on",
    "start": "2105950",
    "end": "2111470"
  },
  {
    "text": "the app otherwise I have two questions first of all is is Apache Zeppelin",
    "start": "2111470",
    "end": "2116480"
  },
  {
    "text": "something I could download and install on my own laptop if I wanted to run cool spark demos like that live you",
    "start": "2116480",
    "end": "2122090"
  },
  {
    "text": "absolutely so Zeppelin is open-source it's available so search Apache Zeppelin and",
    "start": "2122090",
    "end": "2127240"
  },
  {
    "text": "you will be able to set that up they have some tutorials on their website for getting it set up on a cluster",
    "start": "2127240",
    "end": "2133359"
  },
  {
    "text": "I was following somebody's blog that was how to deploy Apache spark on a local machine so you might be able to search",
    "start": "2133359",
    "end": "2139299"
  },
  {
    "text": "for that to get it set up and I will include that in my upcoming tutorial too and do any of the prominent cloud",
    "start": "2139299",
    "end": "2145920"
  },
  {
    "text": "infrastructure providers have a spark offerings that I could use absolutely so",
    "start": "2145920",
    "end": "2151240"
  },
  {
    "text": "I don't know about Google cloud platform if anybody here works for Google and wants to correct me so when I was",
    "start": "2151240",
    "end": "2160030"
  },
  {
    "text": "running this in production in my last company we were using AWS as elastic MapReduce which is a tool for running",
    "start": "2160030",
    "end": "2166990"
  },
  {
    "text": "spark in the cloud and you can run it with your existing AWS s3 so we had all of our logs stored in s3 and then you",
    "start": "2166990",
    "end": "2172569"
  },
  {
    "text": "can use if you want to talk about authentication I am prevent provisioning or credentialing to access different",
    "start": "2172569",
    "end": "2179020"
  },
  {
    "text": "data sets and use stuff like that if you're running something like that in a cluster you are going to need some kind",
    "start": "2179020",
    "end": "2184359"
  },
  {
    "text": "of scheduler to do that and so you're gonna be needing to use something like yarn or may sews and then there are also",
    "start": "2184359",
    "end": "2190030"
  },
  {
    "text": "managed services like I mentioned so things like data bricks will allow you to do that without having to spin up your own clusters but those come with",
    "start": "2190030",
    "end": "2196510"
  },
  {
    "text": "the costs and that cost is they are expensive cool and I think we got time for a few more anything else from the",
    "start": "2196510",
    "end": "2203290"
  },
  {
    "text": "audience otherwise I've got one or two more",
    "start": "2203290",
    "end": "2206760"
  },
  {
    "text": "just to repeat the question it's is there a good profiler that can explore a spark memory usage I haven't used",
    "start": "2210680",
    "end": "2217950"
  },
  {
    "text": "anything on AWS I know data bricks has some of that built into their application we were",
    "start": "2217950",
    "end": "2223710"
  },
  {
    "text": "running stuff on a mixture of AWS and data bricks so you can look at how you're in data bricks you can see the",
    "start": "2223710",
    "end": "2231300"
  },
  {
    "text": "memory inspection you can also dig into that in the logs when something is running on EMR and depending on how you",
    "start": "2231300",
    "end": "2237960"
  },
  {
    "text": "have that set up if it's a persistent cluster or a temporary cluster if you're doing batch jobs temporary clusters",
    "start": "2237960",
    "end": "2243240"
  },
  {
    "text": "sometimes we'll lose the log data after the fact if so you have to be looking at it while it's actually running one of",
    "start": "2243240",
    "end": "2250860"
  },
  {
    "text": "the things that a lot of people do when they get it to the point of needing to do spark tuning is you just add more machines because sometimes that can be",
    "start": "2250860",
    "end": "2257070"
  },
  {
    "text": "easier than trying to be a spark tuning expert so that was generally our solution one thing that came to mind for",
    "start": "2257070",
    "end": "2265230"
  },
  {
    "text": "me is one thing that's great about the spark ecosystem is it has data connectors for just about every file system and database Under the Sun right",
    "start": "2265230",
    "end": "2272520"
  },
  {
    "text": "from neo4j to relational databases to an s3 and HDFS hmm I was wondering if anecdotally there",
    "start": "2272520",
    "end": "2278100"
  },
  {
    "text": "were any of those that you found worked particularly well or were particularly painful yes so are a lot of our data was",
    "start": "2278100",
    "end": "2285540"
  },
  {
    "text": "in JSON format when we were reading it in and so that was just kind of an existing thing that we had to deal with",
    "start": "2285540",
    "end": "2290690"
  },
  {
    "text": "but it we found it was really fast to write data to park' format and that's another interpreter that SPARC supports",
    "start": "2290690",
    "end": "2297000"
  },
  {
    "text": "and so that's a really really efficient type of a data connector that you can",
    "start": "2297000",
    "end": "2302730"
  },
  {
    "text": "hook in with with SPARC were you doing that in s3 or just an HDFS or local files we were doing that in s3 and C we",
    "start": "2302730",
    "end": "2310020"
  },
  {
    "text": "were saving part K files and then you you do have to the the disadvantage with saving something as part K is that it's",
    "start": "2310020",
    "end": "2315720"
  },
  {
    "text": "not as easy to just like read the data if you just want to inspect like an individual file which you can do if",
    "start": "2315720",
    "end": "2321150"
  },
  {
    "text": "you're saving and then something like JSON format but it is a lot faster because it's compressed cool oh and we",
    "start": "2321150",
    "end": "2330870"
  },
  {
    "text": "got one question from the audience spark ml lib or tensorflow pros and cons",
    "start": "2330870",
    "end": "2337260"
  },
  {
    "text": "or or can you run tensorflow on spark somehow I am NOT a data scientist on top",
    "start": "2337260",
    "end": "2342540"
  },
  {
    "text": "of SPARC I've worked with data science so I'm not the best person to answer that so I'll defer that to somebody else",
    "start": "2342540",
    "end": "2348760"
  },
  {
    "text": "does anybody else have any experience with that I don't know about tensorflow but what I will say is ml lib is against",
    "start": "2348760",
    "end": "2355540"
  },
  {
    "text": "the RTD API I think and SPARC ml is the data frame equivalent that's a little higher level I think I tend to get",
    "start": "2355540",
    "end": "2362980"
  },
  {
    "text": "better results working in spark ml cool thanks everyone",
    "start": "2362980",
    "end": "2369670"
  },
  {
    "text": "all right another round of applause for for Kelly [Applause]",
    "start": "2369670",
    "end": "2374889"
  }
]