[
  {
    "start": "0",
    "end": "21000"
  },
  {
    "text": "[Music] brief context my name is Arman you'll",
    "start": "6990",
    "end": "12400"
  },
  {
    "text": "find me all around the internet as also just Arman so relatively easy to find uh and as Nikki said I'm one of the",
    "start": "12400",
    "end": "18880"
  },
  {
    "text": "co-founders and CTO of Hashi Cororp we're probably better known for",
    "start": "18880",
    "end": "24000"
  },
  {
    "start": "21000",
    "end": "41000"
  },
  {
    "text": "our tools than for the company name uh many of you are probably familiar or have used Vagrant um one of our more",
    "start": "24000",
    "end": "30800"
  },
  {
    "text": "popular tools we also make Terraform and Packer and Vault uh among sort of others",
    "start": "30800",
    "end": "36399"
  },
  {
    "text": "that that Nikki mentioned um for for folks like HMRC uh but today I want to spend some",
    "start": "36399",
    "end": "42879"
  },
  {
    "start": "41000",
    "end": "59000"
  },
  {
    "text": "time talking about Nomad which is our cluster manager anduler uh these are",
    "start": "42879",
    "end": "48079"
  },
  {
    "text": "both sort of relatively new domains so I'd like to sort of explain what a scheduler is as I think this really",
    "start": "48079",
    "end": "54640"
  },
  {
    "text": "makes it a lot more useful when we when we talk about Nomad and its design what it's trying to do at sort of the highest",
    "start": "54640",
    "end": "61359"
  },
  {
    "start": "59000",
    "end": "138000"
  },
  {
    "text": "level what is the purpose of anyuler uh auler ultimately is there to map a set",
    "start": "61359",
    "end": "66960"
  },
  {
    "text": "of work onto a different set of resources right and so a concrete example something that we're all",
    "start": "66960",
    "end": "73040"
  },
  {
    "text": "relatively familiar with or if we're not familiar with we still use without realizing all the time is a CPU",
    "start": "73040",
    "end": "78400"
  },
  {
    "text": "scheduler so if you Think about any machine it has some limited number of physical cores right there's a physical",
    "start": "78400",
    "end": "85280"
  },
  {
    "text": "reality that we have only you know a core or two maybe it can hyperthread and there's sort of virtual threads running",
    "start": "85280",
    "end": "90560"
  },
  {
    "text": "but effectively there's a physical limit to our resources but there is sort of an unlimited number of workloads that we",
    "start": "90560",
    "end": "96640"
  },
  {
    "text": "might be able to place on this right it's not inconceivable to have thousands of threads uh scheduled on an operating",
    "start": "96640",
    "end": "102600"
  },
  {
    "text": "system and so this responsibility of mapping this workload whether it's you know web server threads kernel threads",
    "start": "102600",
    "end": "109439"
  },
  {
    "text": "multiple applications onto the physical resources is the job of our CPU scheduler and so over time the scheduler",
    "start": "109439",
    "end": "116560"
  },
  {
    "text": "is trying to optimize for different things right for most types of CPU schedulers they're optimizing for some",
    "start": "116560",
    "end": "122000"
  },
  {
    "text": "amount of fair use of the CPU if it's a desktopuler for you know you know OSX",
    "start": "122000",
    "end": "127280"
  },
  {
    "text": "it's going to optimize for some level of responsiveness so it's going to do things like shuffle around what's actually running on these physical",
    "start": "127280",
    "end": "133440"
  },
  {
    "text": "resources either to give fair access or to optimize for",
    "start": "133440",
    "end": "138640"
  },
  {
    "start": "138000",
    "end": "179000"
  },
  {
    "text": "responsiveness this is just one example of you know auler that we use and interact with all the time the reality",
    "start": "138760",
    "end": "144800"
  },
  {
    "text": "is schedulers are absolutely everywhere uh but their function is basically the same it's just they're mapping different",
    "start": "144800",
    "end": "150800"
  },
  {
    "text": "sets of work onto different sets of resources so our CPU schedulers are mapping virtual threads onto physical",
    "start": "150800",
    "end": "156720"
  },
  {
    "text": "cores you can really think about clouds like EC2 or OpenStack Nova as",
    "start": "156720",
    "end": "162160"
  },
  {
    "text": "effectively just being schedulers mapping VMs onto hypervisors uh you have IO schedulers that are mapping it onto",
    "start": "162160",
    "end": "168720"
  },
  {
    "text": "disk spindles uh and the sort of the examples go on and on when we talk about cluster schedulers their sort of domain",
    "start": "168720",
    "end": "174959"
  },
  {
    "text": "is mapping applications onto a set of servers and so generally speaking why do",
    "start": "174959",
    "end": "181519"
  },
  {
    "start": "179000",
    "end": "254000"
  },
  {
    "text": "we use schedulers at all what are the advantages of using them in sort of any setting h it really comes down to three",
    "start": "181519",
    "end": "187840"
  },
  {
    "text": "different primary benefits one of them is higher utilization of the underlying resources whether that's a hypervisor",
    "start": "187840",
    "end": "194640"
  },
  {
    "text": "whether that's a physical CPU core um it's a decoupling of work from resources",
    "start": "194640",
    "end": "200000"
  },
  {
    "text": "and it's better quality of service where better sort of depends on your application and the techniques we use to",
    "start": "200000",
    "end": "207040"
  },
  {
    "text": "to get there is sort of shared across every type of scheduler uh to get higher",
    "start": "207040",
    "end": "212159"
  },
  {
    "text": "levels of utilization of our resources we do things like bin packing so if you think about a hypervisor you have your",
    "start": "212159",
    "end": "218080"
  },
  {
    "text": "physical machine with 64 cores you spin up many VMs on it you're packing multiple machines onto the same workload",
    "start": "218080",
    "end": "223680"
  },
  {
    "text": "you're effectively binacking we do things like overs subscribe so it might be that you actually allocate more",
    "start": "223680",
    "end": "229599"
  },
  {
    "text": "memory to those VMs than the physical machine has RAM because you're sort you're relatively confident that your VMs aren't going to consume all of the",
    "start": "229599",
    "end": "235760"
  },
  {
    "text": "memory so we can sort of overs subscribe the system and are optimistic that we're not actually going to exhaust ex exhaust",
    "start": "235760",
    "end": "241799"
  },
  {
    "text": "resource we can do things like job queuing so if we have more work than we have physical resources we can put these",
    "start": "241799",
    "end": "246879"
  },
  {
    "text": "in a queue such that the moment we free up some available resource we can start running the new thing right away so we",
    "start": "246879",
    "end": "251920"
  },
  {
    "text": "minimize idle resources then we can talk about sort of the decoupling benefits this is done",
    "start": "251920",
    "end": "257840"
  },
  {
    "start": "254000",
    "end": "296000"
  },
  {
    "text": "through a few different folds one of these is it allows us to abstract away details of the physical resource away",
    "start": "257840",
    "end": "264160"
  },
  {
    "text": "from the logical work in a way that sort of frees us from having to care about some of the underlying details so for",
    "start": "264160",
    "end": "270400"
  },
  {
    "text": "example when we're writing an application we don't really care you know what is the cache alignment of my",
    "start": "270400",
    "end": "275520"
  },
  {
    "text": "CPU and am I sort of sharing a cache line and am I crossing a memory boundary these are sort of details that are",
    "start": "275520",
    "end": "280800"
  },
  {
    "text": "abstracted away from us by the CPUuler it sort of allows us to work at a higher level uh without having to be sort of",
    "start": "280800",
    "end": "286560"
  },
  {
    "text": "bothered by low-level abstractions and so the way we ultimately get this is by having an API contract that our",
    "start": "286560",
    "end": "292880"
  },
  {
    "text": "scheduler gives us and standardizing around that and the last one is better quality",
    "start": "292880",
    "end": "298479"
  },
  {
    "start": "296000",
    "end": "384000"
  },
  {
    "text": "of service and this really depends on sort of what the goals of your application are if my application is",
    "start": "298479",
    "end": "305120"
  },
  {
    "text": "something like Hadoop and I'm just running you know thousands or hundreds of thousands of shards of work you know",
    "start": "305120",
    "end": "310240"
  },
  {
    "text": "my quality of service is something around you know what time does it take to complete all of my workload right and",
    "start": "310240",
    "end": "316160"
  },
  {
    "text": "so the best scheduling decisions are probably something around fair use if I'm a desktop scheduler and I'm",
    "start": "316160",
    "end": "322000"
  },
  {
    "text": "scheduling you know Safari and an interactive interface I care more about responsiveness so I care about the",
    "start": "322000",
    "end": "327120"
  },
  {
    "text": "latency of user interaction being handled right and if I'm a moonlander I I want I really care that like my",
    "start": "327120",
    "end": "333039"
  },
  {
    "text": "rockets fire in time so I don't crash into the earth uh and as opposed to you know did I fairly schedule the",
    "start": "333039",
    "end": "338960"
  },
  {
    "text": "background you know data upload process so the notion of quality of service really depends on what your domain is",
    "start": "338960",
    "end": "344880"
  },
  {
    "text": "because it could be very different what you care about but ultimately it really comes down to three different mechanisms",
    "start": "344880",
    "end": "350400"
  },
  {
    "text": "that get applied one is priority so you know is the you know is the rocket uh",
    "start": "350400",
    "end": "355759"
  },
  {
    "text": "you know firing program a higher priority than the telemetry upload program then like make sure it gets priority access to resource it's",
    "start": "355759",
    "end": "362479"
  },
  {
    "text": "resource isolation so even if you have multiple resources uh multiple physical resources do you have low priority work",
    "start": "362479",
    "end": "369360"
  },
  {
    "text": "that's happening to steal resources away from high priority work so we want to be able to isolate that and it's",
    "start": "369360",
    "end": "374479"
  },
  {
    "text": "preeemption so if all of a sudden high priority work shows up but we're busy doing a bunch of low priority stuff can",
    "start": "374479",
    "end": "380080"
  },
  {
    "text": "we get rid of the low priority stuff and switch over to doing high priority work so the concept of using something",
    "start": "380080",
    "end": "387360"
  },
  {
    "start": "384000",
    "end": "413000"
  },
  {
    "text": "like a clusteruler to achieve these three uh goals isn't new uh it's been done for decades um Google for example",
    "start": "387360",
    "end": "394560"
  },
  {
    "text": "has famously used Borg since the early 2000s if we really think about AWS as just being a giantuler it's been about a",
    "start": "394560",
    "end": "401680"
  },
  {
    "text": "decade there netflix and Twitter more recently uh both have their own schedulers that they use so this is sort",
    "start": "401680",
    "end": "407440"
  },
  {
    "text": "of a common approach we see done at large scale to achieve those sort of efficient economies of",
    "start": "407440",
    "end": "413319"
  },
  {
    "start": "413000",
    "end": "454000"
  },
  {
    "text": "scale so with that background that that context that sort of brings us now to Nomad which is our clusteruler",
    "start": "413319",
    "end": "420000"
  },
  {
    "text": "so when we're talking about a clusteruler it's important to remember what we're doing is mapping applications onto a set of servers right and so in",
    "start": "420000",
    "end": "427039"
  },
  {
    "text": "doing this we have a number of goals in designing and building Nomad one of these things that it's easy to deploy",
    "start": "427039",
    "end": "432800"
  },
  {
    "text": "applications that being sort of the primary function of theuler two that it's operationally simple to run this",
    "start": "432800",
    "end": "438720"
  },
  {
    "text": "thing right so as you might imagine schedulers much like you know a CPU your clusterul ends up being sort of the",
    "start": "438720",
    "end": "445039"
  },
  {
    "text": "center of the universe for your data center and that you want this thing to be built for scale that it you don't want Nomad to ever become the",
    "start": "445039",
    "end": "451280"
  },
  {
    "text": "scalability limit of your broader application so where this starts for us",
    "start": "451280",
    "end": "456960"
  },
  {
    "start": "454000",
    "end": "485000"
  },
  {
    "text": "is with the job specification is how do we provide that workload how do we specify what applications we actually",
    "start": "456960",
    "end": "463440"
  },
  {
    "text": "want Nomad to deploy and where this starts with Nomad uh is a job file the job file is meant to be human editable",
    "start": "463440",
    "end": "470560"
  },
  {
    "text": "human readable simple to write and understandable and this is a totally valid example of a job file that would",
    "start": "470560",
    "end": "475919"
  },
  {
    "text": "deploy Reddus so here we're defining a job naming it Reddus basically specifying a Docker image to run and",
    "start": "475919",
    "end": "481599"
  },
  {
    "text": "providing some set of resources we want Nomad to dedicate to our job before we sort of get into the",
    "start": "481599",
    "end": "487599"
  },
  {
    "start": "485000",
    "end": "536000"
  },
  {
    "text": "details of it what's really important with the job job file is that it declares what we want to run right and",
    "start": "487599",
    "end": "493919"
  },
  {
    "text": "importantly what it doesn't do is tell it where to run it or how to run it these are details that are left to Nomad",
    "start": "493919",
    "end": "500879"
  },
  {
    "text": "right and so this kind of goes to that second goal of a scheduler which is how do we decouple the work uh from the",
    "start": "500879",
    "end": "507199"
  },
  {
    "text": "underlying machines right and so if our developers were telling us specifically what machines to put it on or how it",
    "start": "507199",
    "end": "512560"
  },
  {
    "text": "should be run there's really no decoupling the developer has there is no abstraction basically so the key here is",
    "start": "512560",
    "end": "518399"
  },
  {
    "text": "by not allowing people to provide this this is what allows Nomad to abstract the work from the underlying resources",
    "start": "518399",
    "end": "524080"
  },
  {
    "text": "so now Nomad can make decisions as to where the work should run how it should run should it be preempted should it be",
    "start": "524080",
    "end": "529360"
  },
  {
    "text": "moved somewhere else And so this is super key to have this decoupling this abstraction so that we can have an API",
    "start": "529360",
    "end": "535800"
  },
  {
    "text": "contract in terms of the type of workloads you might run on top of Nomad it you know the example happened to be a",
    "start": "535800",
    "end": "541839"
  },
  {
    "start": "536000",
    "end": "586000"
  },
  {
    "text": "Docker container but it can sort of be anything right from Nomad's perspective we're mapping a set of applications",
    "start": "541839",
    "end": "547200"
  },
  {
    "text": "whether they're containerized or virtualized or standalone onto a set of servers it doesn't particularly matter",
    "start": "547200",
    "end": "552560"
  },
  {
    "text": "what the type of application is underneath it's going to use some amount of storage compute and networking and",
    "start": "552560",
    "end": "558160"
  },
  {
    "text": "that's really what we're accounting for here so if we're running containerized workloads like Docker and and Rock I'm",
    "start": "558160",
    "end": "563360"
  },
  {
    "text": "sorry Docker and Rocket those are supported natively virtual workloads through qimu standalone things like Java",
    "start": "563360",
    "end": "569680"
  },
  {
    "text": "jars or statically linked binaries are all supported today the system is designed to have a very flexible",
    "start": "569680",
    "end": "576000"
  },
  {
    "text": "pluggable architecture so it would be you know relatively straightforward to extend it to support Windows server",
    "start": "576000",
    "end": "581279"
  },
  {
    "text": "containers zen HyperV the list sort of goes on as you might imagine",
    "start": "581279",
    "end": "586959"
  },
  {
    "start": "586000",
    "end": "639000"
  },
  {
    "text": "so where this really starts for our developers is with this declarative job file it allows them to express their",
    "start": "586959",
    "end": "592480"
  },
  {
    "text": "workload in an infrastructure as code way these job files are meant to be version controlled they're meant to be",
    "start": "592480",
    "end": "597920"
  },
  {
    "text": "peer-reviewed you can do pull requests and talk about you know and use your normal code process to to evolve how you",
    "start": "597920",
    "end": "604000"
  },
  {
    "text": "define and run your application and importantly it lets us remove imperative logic so as we do things like move from",
    "start": "604000",
    "end": "610640"
  },
  {
    "text": "AWS cloud to Google cloud or shift traffic from on-remise to the cloud these things can then be abstracted away from Nomad because we haven't",
    "start": "610640",
    "end": "616800"
  },
  {
    "text": "imperatively declared dependencies on them but what about our external dependencies what about all of the other",
    "start": "616800",
    "end": "622640"
  },
  {
    "text": "things an app needs to do other than just start right so questions around okay how do we connect to our service",
    "start": "622640",
    "end": "628959"
  },
  {
    "text": "discovery registry how do we health monitor these applications how do we get application secrets to them how do we",
    "start": "628959",
    "end": "635519"
  },
  {
    "text": "deal with our stateful applications they don't particularly like being moved around and so there's sort of answers",
    "start": "635519",
    "end": "641519"
  },
  {
    "start": "639000",
    "end": "673000"
  },
  {
    "text": "through each of these but the answer is going to sort of be repetitive in the sense that you put it in the job file you you declaratively define what you",
    "start": "641519",
    "end": "648079"
  },
  {
    "text": "want to do and it'll be Nomad's job to make sure that takes place so for example how do we declare that we want",
    "start": "648079",
    "end": "653920"
  },
  {
    "text": "to be part of a service discovery index well we declare that that the thing that we're running our application is a",
    "start": "653920",
    "end": "659760"
  },
  {
    "text": "service itself so we might say we have a service we'd like to expose our HTTP",
    "start": "659760",
    "end": "664959"
  },
  {
    "text": "port to the rest of the cluster and by the way we have a health check as well so please hit the health endpoint every",
    "start": "664959",
    "end": "670399"
  },
  {
    "text": "5 seconds and that's going to tell us whether or not this app is healthy so what this will do when we provide that",
    "start": "670399",
    "end": "676079"
  },
  {
    "start": "673000",
    "end": "760000"
  },
  {
    "text": "specification to Nomad is all of a sudden inform it that oh by the way you need to interface with console and",
    "start": "676079",
    "end": "681839"
  },
  {
    "text": "notify it that the thing we're running is a service and why don't we just do this automatically well it could be the",
    "start": "681839",
    "end": "687279"
  },
  {
    "text": "case that what we're running is a batch workload so for example if we're running a massive Hadoop job or Spark job it",
    "start": "687279",
    "end": "692560"
  },
  {
    "text": "might have you know tens of thousands of shards that aren't services we would ever want to discover right so it would",
    "start": "692560",
    "end": "697920"
  },
  {
    "text": "just sort of be it would just pollute the namespace of our service discovery if we were registering sort of anonymous batch jobs so we want to be explicit in",
    "start": "697920",
    "end": "705040"
  },
  {
    "text": "registering services when it makes sense so here what's going to happen is Nomad is very much decoupled from console",
    "start": "705040",
    "end": "711519"
  },
  {
    "text": "they're just interfacing with each other over an API so Nomad is constrained to just caring about scheduling problems so",
    "start": "711519",
    "end": "718000"
  },
  {
    "text": "Nomad is going to schedule the application potentially placing it uh on a client alongside any number of other",
    "start": "718000",
    "end": "723120"
  },
  {
    "text": "apps that it's been packed and then because we've declared this as a service Nomad will inform the local console",
    "start": "723120",
    "end": "728240"
  },
  {
    "text": "agent that by the way app one is a service and here's its health check information and then console will take care of registering the service",
    "start": "728240",
    "end": "734639"
  },
  {
    "text": "monitoring its health and making sure this is available to the rest of the cluster one of the advantages of",
    "start": "734639",
    "end": "739680"
  },
  {
    "text": "decoupling these concerns is that we can have applications that live inside of Nomad and are scheduled and are sort of",
    "start": "739680",
    "end": "745360"
  },
  {
    "text": "living the dream uh alongside applications that are you know legacy VM based monoliths that are interfacing",
    "start": "745360",
    "end": "751519"
  },
  {
    "text": "over console so console can sort of act as a shared service discovery bus uh for",
    "start": "751519",
    "end": "756560"
  },
  {
    "text": "applications inside and outside of Nomad so that brings us to secret",
    "start": "756560",
    "end": "762720"
  },
  {
    "start": "760000",
    "end": "781000"
  },
  {
    "text": "distribution sort of at a high level when I say secret distribution it's really answering the question of how do our end applications get their API",
    "start": "762720",
    "end": "769680"
  },
  {
    "text": "credentials for example to talk to a you know S3 database credentials SSL and TLS",
    "start": "769680",
    "end": "775440"
  },
  {
    "text": "certificates these are sort of highly sensitive pieces of information that our application",
    "start": "775440",
    "end": "781079"
  },
  {
    "start": "781000",
    "end": "822000"
  },
  {
    "text": "needs and sort of the worst case scenario but you know is very common very easy to do is you know well you",
    "start": "781079",
    "end": "786959"
  },
  {
    "text": "just you know pass it to the application as an environment variable right the challenge is if we're moving towards",
    "start": "786959",
    "end": "792639"
  },
  {
    "text": "things like infrastructure as code this file lives in GitHub right or this file is being you know templated by by",
    "start": "792639",
    "end": "799120"
  },
  {
    "text": "something else that then is pulling from GitHub and so ultimately this thing is living in plain text in systems that",
    "start": "799120",
    "end": "805120"
  },
  {
    "text": "have very limited access control right so how many developers in your org have access to GitHub and can go and see what",
    "start": "805120",
    "end": "810560"
  },
  {
    "text": "the username and password to the database is so it becomes very hard to reason about who has access to these things how do we restrict that if it's",
    "start": "810560",
    "end": "817519"
  },
  {
    "text": "compromised who could have made use of that information it's very hard to reason about it and so for this reason",
    "start": "817519",
    "end": "823920"
  },
  {
    "start": "822000",
    "end": "858000"
  },
  {
    "text": "uh we created vault uh so it's a different product we make its goal is to really tackle the secret management",
    "start": "823920",
    "end": "829920"
  },
  {
    "text": "space uh and do so in a way that gives you auditability gives you fine grain access control ensures secrets are",
    "start": "829920",
    "end": "836399"
  },
  {
    "text": "encrypted in transit and at rest so we don't have a lot of time to go into the details of vault but at a high level you",
    "start": "836399",
    "end": "842720"
  },
  {
    "text": "know its goal is really doing secret storage uh it has you know ways of dealing with static secrets it can",
    "start": "842720",
    "end": "848399"
  },
  {
    "text": "dynamically generate credentials for things like databases and cloud providers um and it does it in a way",
    "start": "848399",
    "end": "853839"
  },
  {
    "text": "that gives you rich access control auditing and and client authentication mechanisms so at the highest level",
    "start": "853839",
    "end": "860079"
  },
  {
    "start": "858000",
    "end": "899000"
  },
  {
    "text": "though much like any type of database there's sort of a login flow to it you know before you can request data from",
    "start": "860079",
    "end": "865920"
  },
  {
    "text": "MySQL you first establish a connection and do a login sort of handshake vault is very similar in that respect so",
    "start": "865920",
    "end": "872320"
  },
  {
    "text": "anytime we're starting to talk to Vault there's an initial login flow where we're providing some set of credentials to it if that goes well Vault will",
    "start": "872320",
    "end": "878560"
  },
  {
    "text": "return a vault token back to us very similar to you know how a web server for example would return a session cookie",
    "start": "878560",
    "end": "883920"
  },
  {
    "text": "that says \"Great you've logged in.\" And every time we perform an operation we're basically threading that cookie that",
    "start": "883920",
    "end": "889360"
  },
  {
    "text": "token back into Vault and if you know we're still logged in Vault will bother to actually respond to our operation so",
    "start": "889360",
    "end": "895680"
  },
  {
    "text": "the question then becomes great as we're moving into something like Nomad how do",
    "start": "895680",
    "end": "900880"
  },
  {
    "text": "we get these tokens to our application right and sort of a bad answer would be well just provide it in as a you know",
    "start": "900880",
    "end": "907360"
  },
  {
    "text": "environment variable and paste that into uh the job file the reason this becomes a bad answer is it's effectively only",
    "start": "907360",
    "end": "913279"
  },
  {
    "text": "one degree of separation away from actually just putting the database password in there right so it's like yes the database password might be living in",
    "start": "913279",
    "end": "919680"
  },
  {
    "text": "vault but the credentials needed to access the database password are living in GitHub so we're still only sort of",
    "start": "919680",
    "end": "924959"
  },
  {
    "text": "one step shy of it so as we might expect the answer is well there's a declarative block that",
    "start": "924959",
    "end": "930480"
  },
  {
    "start": "926000",
    "end": "957000"
  },
  {
    "text": "lets us do this uh and get away from this problem so the latest version of Nomad allows us to instead just specify",
    "start": "930480",
    "end": "936720"
  },
  {
    "text": "what set of vault policies our application needs so we don't actually need to now provide the vault token",
    "start": "936720",
    "end": "941920"
  },
  {
    "text": "itself we just say we need a token that has at least this level of privilege and we can put in a list of all of the",
    "start": "941920",
    "end": "947040"
  },
  {
    "text": "policies that we need so if we know okay my app needs access to S3 and you know the database we can define a policy for",
    "start": "947040",
    "end": "953759"
  },
  {
    "text": "this and just say my app needs that policy by the time it's running so then what vault will do is take care",
    "start": "953759",
    "end": "959759"
  },
  {
    "start": "957000",
    "end": "1023000"
  },
  {
    "text": "of doing the flow around that for us at runtime so that we don't have to hardcode these secrets anymore so when",
    "start": "959759",
    "end": "965920"
  },
  {
    "text": "our users submit a job now to Nomad uh they can provide their own vault token vault will then I'm sorry Nomad will",
    "start": "965920",
    "end": "972000"
  },
  {
    "text": "verify does Arman actually have sufficient privilege right like if I'm only allowed to perform 10 operations",
    "start": "972000",
    "end": "978480"
  },
  {
    "text": "that I just submit an application that needs to run with root level privilege in the system uh and then all of a",
    "start": "978480",
    "end": "983680"
  },
  {
    "text": "sudden we can reject that and say no you're trying to basically escalate your privilege in the system so Nomad will make sure we're not escalating our",
    "start": "983680",
    "end": "989759"
  },
  {
    "text": "privilege if we're still launching an app with you know equal or less than privilege than our self then Nomad will",
    "start": "989759",
    "end": "995040"
  },
  {
    "text": "go ahead and schedule the application somewhere so great it places app one alongside some other apps and then the",
    "start": "995040",
    "end": "1000639"
  },
  {
    "text": "there will be sort of a transparent flow to our application where Nomad will take care of generating a token with",
    "start": "1000639",
    "end": "1005839"
  },
  {
    "text": "sufficient privilege and renewing it behind the scenes and it will just magically appear as a environment",
    "start": "1005839",
    "end": "1011199"
  },
  {
    "text": "variable to our app so our app still just gets its vault token as an environment variable but we didn't have to hardcode that anywhere and this is",
    "start": "1011199",
    "end": "1017839"
  },
  {
    "text": "sort of uh in memory everywhere uh and never sort of uh no no real risk with it",
    "start": "1017839",
    "end": "1023920"
  },
  {
    "start": "1023000",
    "end": "1050000"
  },
  {
    "text": "so the key to the native integration is we can remove these secrets out of our job files and just talk about policies which are safe to store inside of a",
    "start": "1023920",
    "end": "1030558"
  },
  {
    "text": "version control system our secrets never hit clients disk so Nomad goes out of its way to sort of use temp fs systems",
    "start": "1030559",
    "end": "1037199"
  },
  {
    "text": "and keep things in memory and even when it's writing to disk it's real still writing to RAM and we can minimize the",
    "start": "1037199",
    "end": "1042720"
  },
  {
    "text": "trust throughout our system the Nomad servers don't have to maintain tokens clients are maintaining things in memory",
    "start": "1042720",
    "end": "1047760"
  },
  {
    "text": "things don't have to live in GitHub cool so what about our stateful",
    "start": "1047760",
    "end": "1052799"
  },
  {
    "start": "1050000",
    "end": "1128000"
  },
  {
    "text": "applications those that we can't easily just nuke and blow away the way I like to describe statefulness is really not",
    "start": "1052799",
    "end": "1059440"
  },
  {
    "text": "as a binary it's not totally stateless or totally stateful is it's really the spectrum right and different apps will",
    "start": "1059440",
    "end": "1064960"
  },
  {
    "text": "live in different places on this so for example you know on the very left we tend to have applications like API",
    "start": "1064960",
    "end": "1071200"
  },
  {
    "text": "servers web servers caches they tend to be sort of more stateless than not uh with some things like cache you might",
    "start": "1071200",
    "end": "1077440"
  },
  {
    "text": "actually imagine inching them further up towards the stateful side of the spectrum um they're not totally",
    "start": "1077440",
    "end": "1082559"
  },
  {
    "text": "stateless in the sense that they are accumulating some state it's just that for most purposes we don't really care if we lose them uh they're sort of",
    "start": "1082559",
    "end": "1088880"
  },
  {
    "text": "ephemeral by design then more towards the middle of the spectrum you have systems like HDFS",
    "start": "1088880",
    "end": "1094640"
  },
  {
    "text": "Cassandra these are you know what you might call cloudnative databases they expect to be running in a",
    "start": "1094640",
    "end": "1100640"
  },
  {
    "text": "distributed setup they expect to have their data replicated you know end times across data in the cluster and they",
    "start": "1100640",
    "end": "1106799"
  },
  {
    "text": "expect machine failure and that's sort of the fundamental difference is their design expecting machines",
    "start": "1106799",
    "end": "1112840"
  },
  {
    "text": "fail at the very end of your spectrum you have your traditional SQL system so",
    "start": "1112840",
    "end": "1117919"
  },
  {
    "text": "my SQL Postgress MSSQL you know go down the list and these systems are highly",
    "start": "1117919",
    "end": "1122960"
  },
  {
    "text": "stable they basically expect that if they call write an F-sync that data is never lost right and so as a result the",
    "start": "1122960",
    "end": "1130320"
  },
  {
    "start": "1128000",
    "end": "1239000"
  },
  {
    "text": "way we can handle each of these is sort of goes from easy to very hard right and the reason it goes from easy to very",
    "start": "1130320",
    "end": "1135840"
  },
  {
    "text": "hard is at the very hard end of the spectrum you have assumptions that you know sort of don't match reality right",
    "start": "1135840",
    "end": "1142720"
  },
  {
    "text": "the the reality is if you call f-sync and then assume you'll never lose data again well you've sort of made an",
    "start": "1142720",
    "end": "1147760"
  },
  {
    "text": "assumption that hardware never fails and it turns out that's actually not the case uh especially if you're running in",
    "start": "1147760",
    "end": "1152960"
  },
  {
    "text": "a cloud environment then hardware definitely fails at a at a elevated rate and so the promise that these",
    "start": "1152960",
    "end": "1158720"
  },
  {
    "text": "applications are expecting the API contract if you will is very ownorous right it's very ownorous to maintain",
    "start": "1158720",
    "end": "1164960"
  },
  {
    "text": "whether you're running physical metal because you have to go out of your way with RAID controllers and multiple discs",
    "start": "1164960",
    "end": "1170160"
  },
  {
    "text": "and sort of doing redundancy underneath the database because the database that I called f-sync your problem as the",
    "start": "1170160",
    "end": "1175520"
  },
  {
    "text": "operator right so this is sort of hard in a non-scheduler world and usually requires a lot of hardware to sort of",
    "start": "1175520",
    "end": "1181360"
  },
  {
    "text": "hack around this but then as you move into auler world this contract becomes especially tough right because now we",
    "start": "1181360",
    "end": "1187360"
  },
  {
    "text": "don't necessarily have the luxury of RAID controllers and redundant drives on every single machine uh or using storage",
    "start": "1187360",
    "end": "1193200"
  },
  {
    "text": "attached networks we're running in a cloud there is no storage attached network everything is ephemeral uh and",
    "start": "1193200",
    "end": "1198240"
  },
  {
    "text": "things fail all the time so this contract becomes a lot harder so in practice what you have to do is work",
    "start": "1198240",
    "end": "1203679"
  },
  {
    "text": "around this by using distributed file systems so you provide what appears to be uh you know a local file system but",
    "start": "1203679",
    "end": "1210160"
  },
  {
    "text": "really underneath the hood you're trapping out to EBS ebs is writing to multiple discs it's using eraser coding",
    "start": "1210160",
    "end": "1215520"
  },
  {
    "text": "so it's going above and beyond to sort of simulate the fact that hardware doesn't fail",
    "start": "1215520",
    "end": "1222160"
  },
  {
    "text": "so where Nomad really tries to focus is sort of on the easy and medium today and says you know let's punt this to RDS and",
    "start": "1222160",
    "end": "1228799"
  },
  {
    "text": "you know the traditional mechanisms that we've sort of battle hardened for it and really focus on you know the",
    "start": "1228799",
    "end": "1233840"
  },
  {
    "text": "non-stateful to the moderately stful those that kind of work with reality a little better and so if we're just working with",
    "start": "1233840",
    "end": "1240960"
  },
  {
    "start": "1239000",
    "end": "1281000"
  },
  {
    "text": "those type of applications what we can give Nomad is a hint that says you know what my app isn't totally stateless i'd",
    "start": "1240960",
    "end": "1247280"
  },
  {
    "text": "prefer not to lose my data my data is sort of sticky alongside me and what this will let Nomad do is it gives it",
    "start": "1247280",
    "end": "1253600"
  },
  {
    "text": "the right hint that says you know what if I do an application update or something and I'm changing you know my",
    "start": "1253600",
    "end": "1258720"
  },
  {
    "text": "version one to version two I'd prefer not to lose my data so what Nomad will try and do at this point is great I'm",
    "start": "1258720",
    "end": "1264880"
  },
  {
    "text": "upgrading from V1 to V2 if we can place the app on the same machine then we'll simply move the data between the two",
    "start": "1264880",
    "end": "1270559"
  },
  {
    "text": "tasks so stop the old version of Cassandra move the data start the new version of Cassandra great we didn't have to do a full rebuild of that",
    "start": "1270559",
    "end": "1277360"
  },
  {
    "text": "Cassandra node by you know pulling in data from the other replicas if that machine is full or for",
    "start": "1277360",
    "end": "1284320"
  },
  {
    "start": "1281000",
    "end": "1347000"
  },
  {
    "text": "some reason we've changed the constraints of our job so it no longer fits there for example we've gone from version one of Cassandra to version two",
    "start": "1284320",
    "end": "1290480"
  },
  {
    "text": "but we changed it and said actually we really need to be running on you know the version four Linux kernel and Cassandra was previously running on",
    "start": "1290480",
    "end": "1296799"
  },
  {
    "text": "version three Nomad can no longer put it on the same machine because we've changed what's where what machines are",
    "start": "1296799",
    "end": "1302080"
  },
  {
    "text": "eligible so Nomad will find a new machine that has aail available space to run it and it will copy the data from the old machine to the new machine",
    "start": "1302080",
    "end": "1308240"
  },
  {
    "text": "before starting Cassandra so we can still sort of allow our data to migrate alongside of our app and avoid doing a",
    "start": "1308240",
    "end": "1314320"
  },
  {
    "text": "full rebuild of of Cassandra state potentially the key is this is a best",
    "start": "1314320",
    "end": "1319520"
  },
  {
    "text": "effort replication so if we actually just lost that machine the machine that was running Cassandra previously just",
    "start": "1319520",
    "end": "1324880"
  },
  {
    "text": "died AWS decided to kill it then there's no machine to copy it from so Nomad will still find a new machine and start",
    "start": "1324880",
    "end": "1330400"
  },
  {
    "text": "Cassandra somewhere else but it won't be able to migrate the data so the key is the sticky migration is really best",
    "start": "1330400",
    "end": "1336640"
  },
  {
    "text": "effort right and so if we're sort of running these medium stateful apps that can expect and tolerate data failure",
    "start": "1336640",
    "end": "1342400"
  },
  {
    "text": "then they will begin their expensive application level rebuild of state so from the perspective of trying",
    "start": "1342400",
    "end": "1349360"
  },
  {
    "start": "1347000",
    "end": "1379000"
  },
  {
    "text": "to make deployments easy this really starts at the top with declarative job files uh it goes into making sure it's",
    "start": "1349360",
    "end": "1355840"
  },
  {
    "text": "flexible enough to support different workloads so if you have existing VM workloads or you're migrating to",
    "start": "1355840",
    "end": "1361120"
  },
  {
    "text": "containers or you're like I just have a lot of standalone go binaries and I really don't need to bring in uh more heavyweight uh tooling then you can run",
    "start": "1361120",
    "end": "1368240"
  },
  {
    "text": "all of that um through nomad sort of flexible interfaces by bringing in things like console integration vault",
    "start": "1368240",
    "end": "1373919"
  },
  {
    "text": "integration and sticky volumes we can kind of handle a much broader set of application needs and challenges so this brings us to the",
    "start": "1373919",
    "end": "1380799"
  },
  {
    "start": "1379000",
    "end": "1414000"
  },
  {
    "text": "operating side of this thing it's great from the side of developers that it's easy to use and they can get the power they need but our operators still are",
    "start": "1380799",
    "end": "1387039"
  },
  {
    "text": "running this thing our platform teams are responsible for it so how do we make their life easy uh this starts by making",
    "start": "1387039",
    "end": "1393120"
  },
  {
    "text": "sure Nomad is just a single binary with no external dependencies so whether we're running it in client mode and we",
    "start": "1393120",
    "end": "1399280"
  },
  {
    "text": "have thousands of instances of it where we're scheduling work onto whether we're running it in server mode that's actually the brains of the cluster uh",
    "start": "1399280",
    "end": "1405760"
  },
  {
    "text": "it's a single binary we're just flipping flags on it there's no external state storage like CcD or Zookeeper it's",
    "start": "1405760",
    "end": "1411440"
  },
  {
    "text": "self-contained as a system in terms of trying to learn about how do we build a system that's",
    "start": "1411440",
    "end": "1417600"
  },
  {
    "start": "1414000",
    "end": "1462000"
  },
  {
    "text": "operationally reliable where we really started was by building on the experience we have with Surf and Console",
    "start": "1417600",
    "end": "1423840"
  },
  {
    "text": "uh these are two other tools we make surf is a cluster management tool uh",
    "start": "1423840",
    "end": "1429440"
  },
  {
    "text": "that's much lower level uh it's really a gossip based peer-to-peer tool that gives us three major features one is",
    "start": "1429440",
    "end": "1435600"
  },
  {
    "text": "membership which is what other machines are in the cluster uh this is sort of a tricky problem when you're in a highly",
    "start": "1435600",
    "end": "1441600"
  },
  {
    "text": "elastic environment where servers are coming and going you have autoscale groups things are failing so the failure",
    "start": "1441600",
    "end": "1447039"
  },
  {
    "text": "detection is key as it lets us know okay which machines are both in the cluster and currently alive and the last one is",
    "start": "1447039",
    "end": "1452880"
  },
  {
    "text": "an event system so how do we broadcast a message to the entire cluster efficiently uh this gets to be a more",
    "start": "1452880",
    "end": "1458400"
  },
  {
    "text": "challenging problem when you have hundreds or thousands of machines and so surf has been proven at",
    "start": "1458400",
    "end": "1463919"
  },
  {
    "start": "1462000",
    "end": "1500000"
  },
  {
    "text": "very large scale it's a bat battle hardened subsystem that we can sort of use and build from the San Diego",
    "start": "1463919",
    "end": "1469600"
  },
  {
    "text": "supercomputing center has a 10,000 node surf cluster that gets used uh that uses surf fastly uses it uh to orchestrate",
    "start": "1469600",
    "end": "1476320"
  },
  {
    "text": "their CDN so it has a lot of very large scale uh production usage and what we can use this for underneath the hood of",
    "start": "1476320",
    "end": "1482880"
  },
  {
    "text": "Nomad is to give us very simple clustering and federation so if you've ever used console and did you know a",
    "start": "1482880",
    "end": "1488240"
  },
  {
    "text": "console join you're like \"Oh that was kind of easy we didn't have to think about what were the IPs of all the thousands of nodes in our cluster.\"",
    "start": "1488240",
    "end": "1493600"
  },
  {
    "text": "That's handled by surf underneath the hood and so we can bring that over to Nomad and make it easy to run these very large scale",
    "start": "1493600",
    "end": "1499320"
  },
  {
    "text": "clusters the other thing we started with was console which is our service discovery tool and really it provides a",
    "start": "1499320",
    "end": "1505200"
  },
  {
    "start": "1500000",
    "end": "1546000"
  },
  {
    "text": "handful of functions service discovery you know where are the API servers are they healthy what's the configuration of",
    "start": "1505200",
    "end": "1511039"
  },
  {
    "text": "those API servers are the feature flags you know enabled or disabled are we doing shadow traffic uh it lets us do",
    "start": "1511039",
    "end": "1516400"
  },
  {
    "text": "high level coordination so for example distributed locking and it operates very similarly in the centralized server",
    "start": "1516400",
    "end": "1522799"
  },
  {
    "text": "distributed client model so what we were sort of able to learn and bring in as learnings from console is how do we",
    "start": "1522799",
    "end": "1528799"
  },
  {
    "text": "support multi-data center setups how do we have a robust raft implementation so that we have these servers that are",
    "start": "1528799",
    "end": "1534400"
  },
  {
    "text": "highly available they have consistent state and making sure that the system actually operates with you know in a",
    "start": "1534400",
    "end": "1539440"
  },
  {
    "text": "central server distributed client model at very large scale so we're able to borrow a lot of our learnings from console when building Nomad",
    "start": "1539440",
    "end": "1546799"
  },
  {
    "start": "1546000",
    "end": "1582000"
  },
  {
    "text": "and so the the sort of highlevel operational goals with was making sure it's a single binary with no external",
    "start": "1546799",
    "end": "1552320"
  },
  {
    "text": "dependencies as self-contained as possible part of this is you know reducing how you have to reason about a",
    "start": "1552320",
    "end": "1558000"
  },
  {
    "text": "potential failure right if it's a system that depends on many other systems and you have a failure it's sort of hard to",
    "start": "1558000",
    "end": "1563200"
  },
  {
    "text": "debug and understand where this lies where if we can make the system sort of as self-contained as possible uh it can",
    "start": "1563200",
    "end": "1568559"
  },
  {
    "text": "minimize the surface area we have to reason about and it's highly available out of the box so there's no special configuration you just run multiple",
    "start": "1568559",
    "end": "1575360"
  },
  {
    "text": "servers and it will replicate as highly available the clients know how to discover the new servers and deal with",
    "start": "1575360",
    "end": "1581559"
  },
  {
    "text": "that and so the last one was building for scale and this goal is really to ensure if they're putting something like",
    "start": "1581559",
    "end": "1587679"
  },
  {
    "start": "1582000",
    "end": "1602000"
  },
  {
    "text": "Nomad at the heart of your data center right it's scheduling your services it's scheduling your batch jobs it can become",
    "start": "1587679",
    "end": "1593679"
  },
  {
    "text": "the central bottleneck by which all other orchestration is waiting on so how do we make sure the system is sort of",
    "start": "1593679",
    "end": "1599120"
  },
  {
    "text": "never the limiting factor to what you're doing and so where we started on was building with surf and console but the",
    "start": "1599120",
    "end": "1605600"
  },
  {
    "start": "1602000",
    "end": "1620000"
  },
  {
    "text": "problem with this approach is neither of those has any scheduling logic right like they provide low-level tools to",
    "start": "1605600",
    "end": "1610720"
  },
  {
    "text": "know what what's in the cluster and highle system architecture and multi-data center routing but they don't",
    "start": "1610720",
    "end": "1616400"
  },
  {
    "text": "actually schedule jobs which turns out to be the bottleneck so where we turned to was research to",
    "start": "1616400",
    "end": "1622400"
  },
  {
    "start": "1620000",
    "end": "1642000"
  },
  {
    "text": "understand you know what is the state-of-the-art what's been what's being done out there and who can we learn from and the two groups that are",
    "start": "1622400",
    "end": "1628880"
  },
  {
    "text": "sort of most influential to our thinking is the Berkeley AMP lab where they've done a lot of work on things like map",
    "start": "1628880",
    "end": "1634320"
  },
  {
    "text": "produce and yarn and mos and spark as well as Google which is probably you know the preeminent scheduling shop in",
    "start": "1634320",
    "end": "1641720"
  },
  {
    "text": "industry and so there's probably four major papers that mostly influenced our thinking um one of them and the biggest",
    "start": "1641720",
    "end": "1648400"
  },
  {
    "start": "1642000",
    "end": "1683000"
  },
  {
    "text": "and most important really is or uh it's Google's Google's primary internal scheduler today um and so this paper was",
    "start": "1648400",
    "end": "1655840"
  },
  {
    "text": "instrumental in sort of how Google thinks about their large scale workloads uh their latest enhancements to bore",
    "start": "1655840",
    "end": "1662000"
  },
  {
    "text": "come in the form of Omega which is an updated system architecture and then from AMPLAB we learned about sort of how",
    "start": "1662000",
    "end": "1667520"
  },
  {
    "text": "do we do very large volume low latency scheduling of batch jobs from Sparrow as",
    "start": "1667520",
    "end": "1672640"
  },
  {
    "text": "well as from MSOS how do you do flexible you know flexible scheduling when you have different types of workloads so you",
    "start": "1672640",
    "end": "1678399"
  },
  {
    "text": "know your services very different types of workloads than uh than things like batch jobs and so what we ended up with was",
    "start": "1678399",
    "end": "1685520"
  },
  {
    "start": "1683000",
    "end": "1748000"
  },
  {
    "text": "sort of a an architecture that looks very similar to console if you're familiar with it you have a set of central servers they elect leaders",
    "start": "1685520",
    "end": "1692360"
  },
  {
    "text": "internally so that you know in this case we have our leader is in the middle it's being it's replicating data to the other",
    "start": "1692360",
    "end": "1698320"
  },
  {
    "text": "servers the other servers are forwarding requests to the leader that's sort of servicing most of the operations and",
    "start": "1698320",
    "end": "1703600"
  },
  {
    "text": "then we have clients that span many different data centers so we can have you know data center 1 2 and three all slaved into a single set of central",
    "start": "1703600",
    "end": "1711559"
  },
  {
    "text": "controllers if we need to distribute our controllers across multiple regions for example you know maybe we have a US",
    "start": "1711559",
    "end": "1717840"
  },
  {
    "text": "region and an EU region and we've decided that that's how we decide we want to split our workloads we might split it and say we have these two",
    "start": "1717840",
    "end": "1724240"
  },
  {
    "text": "separate regions and in the US have three data centers and in the EU have three data centers but have separate control planes for them but although we",
    "start": "1724240",
    "end": "1730880"
  },
  {
    "text": "have separate control planes we still want our developer to be able to just submit a job to any one of their data centers any one of their regions and so",
    "start": "1730880",
    "end": "1737039"
  },
  {
    "text": "the servers federate together so that a developer doesn't have to think about it they don't have to worry about which is the endpoint for the EU servers versus",
    "start": "1737039",
    "end": "1743120"
  },
  {
    "text": "the US servers they just submit it to anywhere and it will forward the request as",
    "start": "1743120",
    "end": "1748399"
  },
  {
    "start": "1748000",
    "end": "1798000"
  },
  {
    "text": "appropriate one of the goals of having this sort of multi-reion architecture is that it also becomes an isolation domain",
    "start": "1748520",
    "end": "1754720"
  },
  {
    "text": "for failure right so you can think about this sort of like the bulkhead of a ship right if we take an outage in one of our",
    "start": "1754720",
    "end": "1759840"
  },
  {
    "text": "regions the other regions should continue running and so this gives us sort of an isolation boundary that we",
    "start": "1759840",
    "end": "1764960"
  },
  {
    "text": "can reason about okay where do we want to draw our failure lines and so because we can have any number of data centers",
    "start": "1764960",
    "end": "1770240"
  },
  {
    "text": "per region it's sort of up to us to be able to model how we want to do this so maybe we say you know what we're going",
    "start": "1770240",
    "end": "1775279"
  },
  {
    "text": "to run one set of servers and have 12 data centers around the world and the entire world is our isolation boundary",
    "start": "1775279",
    "end": "1780799"
  },
  {
    "text": "basically if we lose our central servers we won't be able to schedule anywhere or we might go to the very opposite extreme",
    "start": "1780799",
    "end": "1785840"
  },
  {
    "text": "and say every single data center should be fully independent and should be its own region and schedule separately and",
    "start": "1785840",
    "end": "1791440"
  },
  {
    "text": "great we can do that if we want to have a onetoone ratio of data centers to to region controllers that's",
    "start": "1791440",
    "end": "1798120"
  },
  {
    "start": "1798000",
    "end": "1823000"
  },
  {
    "text": "fine so ultimately though the design goal of the system was to be able to support hundreds of regions because we",
    "start": "1798120",
    "end": "1804559"
  },
  {
    "text": "have you know users and customers that have you know several dozen data centers and so what if they want to model each",
    "start": "1804559",
    "end": "1810799"
  },
  {
    "text": "one of those data centers as an independent failure domain that should be within scope for Nomad each of these data centers should be able to handle",
    "start": "1810799",
    "end": "1817120"
  },
  {
    "text": "tens of thousands of clients uh and each region should be able to handle thousands of jobs",
    "start": "1817120",
    "end": "1823000"
  },
  {
    "start": "1823000",
    "end": "1854000"
  },
  {
    "text": "running and so the ultimate architecture of the system is most heavily influenced by Google Omega uh this means it's",
    "start": "1823000",
    "end": "1829679"
  },
  {
    "text": "optimistically concurrent in its scheduling meaning it can make many parallel scheduling decisions at the same time and it uses several controls",
    "start": "1829679",
    "end": "1836640"
  },
  {
    "text": "to make sure this is done safely uh and part of this changes the way we do state coordination so this means the system",
    "start": "1836640",
    "end": "1841840"
  },
  {
    "text": "keeps all of its state internally it frees frees sort of external dependencies from the system but it",
    "start": "1841840",
    "end": "1847120"
  },
  {
    "text": "allows us to support service and batch workloads even though they have separate types of logic separate types of quality",
    "start": "1847120",
    "end": "1852720"
  },
  {
    "text": "of service concerns and so at the very highest level the kind of mental model the",
    "start": "1852720",
    "end": "1858799"
  },
  {
    "start": "1854000",
    "end": "1899000"
  },
  {
    "text": "architecture of the system is basically only these four types of nouns there's two sort of inputs from the outside",
    "start": "1858799",
    "end": "1865120"
  },
  {
    "text": "world to nomad one is the job workloads so developers who are submitting work to the system what are the things we need",
    "start": "1865120",
    "end": "1870799"
  },
  {
    "text": "to run this is sort of the work and then there's operators who are providing resources so what are the underlying",
    "start": "1870799",
    "end": "1875840"
  },
  {
    "text": "loads uh nodes that we can actually schedule work onto the mapping table of",
    "start": "1875840",
    "end": "1881200"
  },
  {
    "text": "work onto resources is through an allocation so we've allocated work to a machine and the way Nomad decides these",
    "start": "1881200",
    "end": "1887600"
  },
  {
    "text": "things is by doing an evaluation so we sort of evaluate okay a developer has submitted a job what is the available",
    "start": "1887600",
    "end": "1893440"
  },
  {
    "text": "resources how many resources do they need let's create an allocation that maps these things to each",
    "start": "1893440",
    "end": "1898760"
  },
  {
    "text": "other and so roughly we can think about an evaluation as taking place anytime there's a state change so something has",
    "start": "1898760",
    "end": "1905120"
  },
  {
    "text": "changed in the real world a node has booted you know or a job has been created updated deleted nodes have come",
    "start": "1905120",
    "end": "1911360"
  },
  {
    "text": "and failed allocations have failed or finished so something has changed in the outside world and now we need to",
    "start": "1911360",
    "end": "1916880"
  },
  {
    "text": "reconcile that we need to sort of bring the world back into the desired state",
    "start": "1916880",
    "end": "1922240"
  },
  {
    "text": "and so the way we can think about the function of an evaluation or I'm sorry the function of a scheduler is it's",
    "start": "1922240",
    "end": "1927519"
  },
  {
    "text": "really a function that takes an evaluation which says hey something about the world has changed please generate a set of updates to the world",
    "start": "1927519",
    "end": "1934000"
  },
  {
    "text": "that will kind of bring us back in line right so in sort of Terraform's case it might be you know the the sort of",
    "start": "1934000",
    "end": "1939360"
  },
  {
    "text": "equivalent is you've asked for a VM that VM doesn't exist so Terraform's job is to create the VM Nomad's equivalent is",
    "start": "1939360",
    "end": "1945679"
  },
  {
    "text": "you just asked for 10 web servers there's currently zero web servers so it job is to create 10 new web servers somewhere",
    "start": "1945679",
    "end": "1951519"
  },
  {
    "text": "So this is how we can think about sort of the job of the scheduler but the logic within theuler of deciding where",
    "start": "1951519",
    "end": "1957360"
  },
  {
    "text": "we place it you know the constraints we want to do the quality of service we enforce this function is sort of free to",
    "start": "1957360",
    "end": "1963039"
  },
  {
    "text": "do things however it wants as long as it's ultimately generating these contracts and so you might imagine",
    "start": "1963039",
    "end": "1968159"
  },
  {
    "text": "having different schedulers in the system that prioritize doing things in different ways so in Nomad's case today",
    "start": "1968159",
    "end": "1973279"
  },
  {
    "text": "we ship with three distinct types of schedulers the serviceuler the batch scheduler and the systemuler and so a",
    "start": "1973279",
    "end": "1980159"
  },
  {
    "text": "batchuler expects that the things it's going to run terminate eventually right and it also expects that it's doing",
    "start": "1980159",
    "end": "1985360"
  },
  {
    "text": "extremely high volume things that are generally relatively short-lived so you might actually want to do things like",
    "start": "1985360",
    "end": "1991120"
  },
  {
    "text": "optimize for placement speed over finding the perfect placement right so if I have tens of thousands of machines",
    "start": "1991120",
    "end": "1996240"
  },
  {
    "text": "in my cluster but you submit a Hadoop job with you know a million components you know what's more important finding",
    "start": "1996240",
    "end": "2001840"
  },
  {
    "text": "the kind of perfect location to run each one of those million pieces of the job or just quickly scheduling them because",
    "start": "2001840",
    "end": "2007760"
  },
  {
    "text": "most of them will be finished in 10 seconds anyway so the batchuler prioritizes doing things faster where the serviceuler assumes that your jobs",
    "start": "2007760",
    "end": "2014399"
  },
  {
    "text": "will be long running and so it's worth spending more time to find an optimal",
    "start": "2014399",
    "end": "2019480"
  },
  {
    "start": "2019000",
    "end": "2145000"
  },
  {
    "text": "placement so at a very high level without really diving into it the system architecture looks something like this",
    "start": "2019480",
    "end": "2025120"
  },
  {
    "text": "these are totally internal details that uh if you enjoy nerding out with are are kind of fun the pipeline in terms of how",
    "start": "2025120",
    "end": "2031919"
  },
  {
    "text": "Nomad deals with things is things happen in the real world so there's external events jobs are being created nodes are",
    "start": "2031919",
    "end": "2037600"
  },
  {
    "text": "coming and going so on and so forth that results in evaluations being created so these are things that the scheduler has",
    "start": "2037600",
    "end": "2043039"
  },
  {
    "text": "to deal with and reconcile those all go into a queue so the evaluation broker cues these up and then applies priority",
    "start": "2043039",
    "end": "2049679"
  },
  {
    "text": "ordering so for example a high priority job is more important to reconcile than a low priority job",
    "start": "2049679",
    "end": "2055358"
  },
  {
    "text": "so this thing will provide some sense of ordering and uh and then we'll fan out the work to any number of our servers so",
    "start": "2055359",
    "end": "2060800"
  },
  {
    "text": "for each server that we're running every single one of its cores is running a newuler so if we have you know",
    "start": "2060800",
    "end": "2066638"
  },
  {
    "text": "reasonably beefy hardware we might be making hundreds of scheduling decisions in parallel so this will get fanned out",
    "start": "2066639",
    "end": "2072720"
  },
  {
    "text": "across these things that are optimistically running at the same time there's no pessimistic locking taking place each of them can be invoking",
    "start": "2072720",
    "end": "2079040"
  },
  {
    "text": "different types of logic so the serviceuler the batchuler a customuler if you want and then they're submitting",
    "start": "2079040",
    "end": "2084800"
  },
  {
    "text": "plans and so this is how we deal with the fact that we're optimistically making decisions and there's races right",
    "start": "2084800",
    "end": "2089919"
  },
  {
    "text": "if you're making a hundred decisions in parallel you might race and say you know five different schedulers are deciding to put work on the same machine so how",
    "start": "2089919",
    "end": "2096158"
  },
  {
    "text": "do we fix that up and this is where the optimistic coordination comes in is in this plan queue all of our schedulers",
    "start": "2096159",
    "end": "2102800"
  },
  {
    "text": "submit plans and they may be fully applied meaning great you've submitted you know your 10 web servers there's",
    "start": "2102800",
    "end": "2108000"
  },
  {
    "text": "free room on those machines start running those things create the set of allocations or it might be the case that there was a conflict so there might have",
    "start": "2108000",
    "end": "2114240"
  },
  {
    "text": "been a race and you place that web server alongside an API server because another scheduler beat you to it uh and",
    "start": "2114240",
    "end": "2119680"
  },
  {
    "text": "there's no more space to run your web server so that might get rejected so the plan Q allows sort of partial admission",
    "start": "2119680",
    "end": "2124800"
  },
  {
    "text": "of scheduling work and it will kick things back to the schedulers uh if they don't get things 100% right the first",
    "start": "2124800",
    "end": "2130160"
  },
  {
    "text": "time around so for very large scale workloads they can sort of you know 900 out of the thousand might get scheduled",
    "start": "2130160",
    "end": "2135920"
  },
  {
    "text": "in the first try 99 out of the next hundred in the second try and that final one in the third try and so allows us to",
    "start": "2135920",
    "end": "2141440"
  },
  {
    "text": "make progress even though we're making decisions sort of in parallel so ultimately as a result of",
    "start": "2141440",
    "end": "2147920"
  },
  {
    "start": "2145000",
    "end": "2173000"
  },
  {
    "text": "this sort of higher level architecture it's very much heavily molded around omega uh what this enables us to do is",
    "start": "2147920",
    "end": "2153760"
  },
  {
    "text": "make these optimistic scheduling decisions um and do these hundreds of decisions in parallel uh without",
    "start": "2153760",
    "end": "2159359"
  },
  {
    "text": "sacrificing the correctness of the system so you don't have to worry about Nomad sort of you know quadruple overallocating a node's resources that",
    "start": "2159359",
    "end": "2166240"
  },
  {
    "text": "sort of corrects for this stuff internally so that the user never sees sort of the the issues with the optimistic challenges",
    "start": "2166240",
    "end": "2173520"
  },
  {
    "start": "2173000",
    "end": "2238000"
  },
  {
    "text": "so the question is you know going through all of this work of building a system that's optimistically concurrent and dealing with all this stuff and",
    "start": "2173520",
    "end": "2178960"
  },
  {
    "text": "adopting the omega challenge the question becomes you know did we succeed right we set all of this engineering",
    "start": "2178960",
    "end": "2184400"
  },
  {
    "text": "effort like was it for all for not so a challenge that we sort of brought up for ourselves is to say well how do we know",
    "start": "2184400",
    "end": "2190400"
  },
  {
    "text": "this thing worked how do we know it actually met its design goals because we set out these lofty goals where we said we want tens of thousands of clients and",
    "start": "2190400",
    "end": "2196960"
  },
  {
    "text": "thousands of jobs running and can the system actually do that and so we came up with a challenge for ourselves that",
    "start": "2196960",
    "end": "2202720"
  },
  {
    "text": "we called the nomad million container challenge which was basically can we just run a million containers on the",
    "start": "2202720",
    "end": "2207760"
  },
  {
    "text": "system and see how fast we can do this right and so sort of the ultimate stress test of the",
    "start": "2207760",
    "end": "2213000"
  },
  {
    "text": "system and so we worked closely with our friends at uh GCP to do this they gave us a bunch of resources so we spun up",
    "start": "2213000",
    "end": "2218960"
  },
  {
    "text": "5,000 machines on GCE and then basically submitted a million containers splitting it and saying we're going to submit a",
    "start": "2218960",
    "end": "2224720"
  },
  {
    "text": "thousand jobs each job will contain a thousand containers uh so that we're sort of creating you know a slightly",
    "start": "2224720",
    "end": "2231680"
  },
  {
    "text": "more difficult workload for the scheduler to have to deal with uh and so that the end total should be that we're running a million",
    "start": "2231680",
    "end": "2237640"
  },
  {
    "text": "containers and what we saw is that the system performs admirably uh is that it actually does give you that nearly",
    "start": "2237640",
    "end": "2244079"
  },
  {
    "text": "linear scaling of having an Omega architecture uh that it is able to make you know the hundreds of decisions in",
    "start": "2244079",
    "end": "2250240"
  },
  {
    "text": "parallel and so even though we submitted this you know absolutely brutal workload for the system it's still able to",
    "start": "2250240",
    "end": "2255920"
  },
  {
    "text": "schedule all million containers successfully in under five minutes right and so it's sustaining a scheduling rate",
    "start": "2255920",
    "end": "2262000"
  },
  {
    "text": "of you know initially it's a little bit faster but eventually it plateaus down to you know north of 2200 per",
    "start": "2262000",
    "end": "2268040"
  },
  {
    "text": "second uh and so this is pretty pretty uh pretty impressive for a system to be able to to maintain this and so you know",
    "start": "2268040",
    "end": "2274880"
  },
  {
    "start": "2274000",
    "end": "2333000"
  },
  {
    "text": "one of the things that you might notice is actually that there's random dips as it's going is that it's not a perfectly monotonic line uh one of the interesting",
    "start": "2274880",
    "end": "2282160"
  },
  {
    "text": "things you actually see when you're doing you know benchmarks this large is that things fail in the real world so as",
    "start": "2282160",
    "end": "2288240"
  },
  {
    "text": "Nomad was running through this and and scheduling all of these jobs in practice we saw that machines were failing in GCP",
    "start": "2288240",
    "end": "2294000"
  },
  {
    "text": "so Nomad would schedule it onto machine that machine would fail uh and so then a new machine would be brought up and",
    "start": "2294000",
    "end": "2299040"
  },
  {
    "text": "Nomad would reschedule the work so Nomad was sort of fighting the fact that some machines are being lost as it's doing this along the way we also uncovered",
    "start": "2299040",
    "end": "2306400"
  },
  {
    "text": "some bugs uh in the Docker engine u just because the the rate at which we were scheduling to the engine was so high",
    "start": "2306400",
    "end": "2312000"
  },
  {
    "text": "that exposed some edge conditions and so we'd lose probably about 2% of our Docker engines due to a race condition",
    "start": "2312000",
    "end": "2318160"
  },
  {
    "text": "and so Nomad would also work around that so it would detect that those machines and jobs had been lost and reschedule them so there's these sort of bumps as",
    "start": "2318160",
    "end": "2324880"
  },
  {
    "text": "it's sort of losing machines and rescheduling but even though it's losing machines as it's going it still managed",
    "start": "2324880",
    "end": "2330480"
  },
  {
    "text": "to finish the million containers in less than five minutes and so I think the most common reaction",
    "start": "2330480",
    "end": "2335920"
  },
  {
    "start": "2333000",
    "end": "2414000"
  },
  {
    "text": "you know for us this was sort of like mission accomplished the system did what we promised it would do uh but I think the general reaction we got from people",
    "start": "2335920",
    "end": "2342160"
  },
  {
    "text": "when we talked to them about it was you know something along this which was like well 640 KB should be enough for anybody",
    "start": "2342160",
    "end": "2348079"
  },
  {
    "text": "why would you ever need a scheduler uh that can do anything remotely close to this and you know the the way I like to",
    "start": "2348079",
    "end": "2354960"
  },
  {
    "text": "think about this is you know what if I you know what if when Oracle created you know one of the original RDBMS systems they said well it can do two",
    "start": "2354960",
    "end": "2360720"
  },
  {
    "text": "transactions a second like why would you ever need a database that could do more than two transactions per second right if you're if you have tools that",
    "start": "2360720",
    "end": "2367599"
  },
  {
    "text": "constrain what you can actually build on top of them then you're not going to go and build Twitter on top of a database",
    "start": "2367599",
    "end": "2372640"
  },
  {
    "text": "that can do two transactions per second right like it's going to constrain the possibilities of what you'll do above it",
    "start": "2372640",
    "end": "2377839"
  },
  {
    "text": "so when we're thinking about workloads of this scale we're thinking about things like well what if you have Lambda function type workloads right what if",
    "start": "2377839",
    "end": "2383599"
  },
  {
    "text": "you're doing batch jobs with millions of pieces what if you want to reach the point where you don't even have longunning services and you spin them up",
    "start": "2383599",
    "end": "2389280"
  },
  {
    "text": "on demand so within the request flow you're starting and stopping your services because youruler's fast enough",
    "start": "2389280",
    "end": "2394800"
  },
  {
    "text": "to do that so why bother reserve resources for a machine that does five requests a day and so to get to those",
    "start": "2394800",
    "end": "2400480"
  },
  {
    "text": "levels of capability you can't do two transactions per second you have to be able to operate at these really immense",
    "start": "2400480",
    "end": "2406359"
  },
  {
    "text": "volumes and so you know this you know many of those things are sort of you know phantom future workloads that",
    "start": "2406359",
    "end": "2411440"
  },
  {
    "text": "you're like okay well when we get there we'll cross that bridge so one of the interesting things for us to be able to",
    "start": "2411440",
    "end": "2416640"
  },
  {
    "start": "2414000",
    "end": "2488000"
  },
  {
    "text": "talk about is a customer whose workload is actually even bigger than the C1M challenge um so shortly after you know",
    "start": "2416640",
    "end": "2423280"
  },
  {
    "text": "we published this benchmark we had a chance to talk with the folks at Citadel for those who are unfamiliar they're the",
    "start": "2423280",
    "end": "2428720"
  },
  {
    "text": "world's second largest hedge fund they manage over $160 billion and they reached out to us and said you know what",
    "start": "2428720",
    "end": "2433920"
  },
  {
    "text": "the C1M is sort of cute uh but we have some workloads that you know our own scheduler in-house can do uh and it's",
    "start": "2433920",
    "end": "2440079"
  },
  {
    "text": "five times larger than what your C1M does and uh we're curious if it will hold up to our challenges and we're like",
    "start": "2440079",
    "end": "2445839"
  },
  {
    "text": "okay interesting this is sort of a very different type of workload because it's heavily batched so you know in our",
    "start": "2445839",
    "end": "2451040"
  },
  {
    "text": "benchmark once Nomad had scheduled the Docker containers they were long running services they didn't have to be rescheduled where in theirs it's a",
    "start": "2451040",
    "end": "2457680"
  },
  {
    "text": "continuous stream of batch work that's being finished uh and so it's not that we can Nomad can schedule it and be done",
    "start": "2457680",
    "end": "2463040"
  },
  {
    "text": "because 30 seconds later that thing finishes a new workload arrives for the system and so in their benchmark they",
    "start": "2463040",
    "end": "2468319"
  },
  {
    "text": "ran a cluster with 18,000 CPU cores and for five continuous hours basically",
    "start": "2468319",
    "end": "2473359"
  },
  {
    "text": "continued to submit workloads to this thing and Nomad continued to schedule them at over 2200 containers a second",
    "start": "2473359",
    "end": "2478880"
  },
  {
    "text": "the entire time so I'll let you do the math on that but just in the course of their burn-in for their workload was",
    "start": "2478880",
    "end": "2485040"
  },
  {
    "text": "something over 40 million allocations and so sort of at a high",
    "start": "2485040",
    "end": "2490720"
  },
  {
    "start": "2488000",
    "end": "2690000"
  },
  {
    "text": "level this is Nomad right it's uh it's really designed to be a cluster scheduler and provide the those key",
    "start": "2490720",
    "end": "2495760"
  },
  {
    "text": "benefits of higher resource utilization decoupling of work and resources better quality of service and to do this in",
    "start": "2495760",
    "end": "2502000"
  },
  {
    "text": "sort of a way that is making it easy for our end developers to self-service and deploy their own applications so how do",
    "start": "2502000",
    "end": "2507359"
  },
  {
    "text": "we sort of free them from having to talk to our operators and just let them submit whatever jobs they want at the",
    "start": "2507359",
    "end": "2512640"
  },
  {
    "text": "same time there's people at the other side of this who have to keep our platforms and our systems operational so how do we make their life easy so",
    "start": "2512640",
    "end": "2518240"
  },
  {
    "text": "they're not being paged continuously and then how do we build it for scale so that you know when you do try and build",
    "start": "2518240",
    "end": "2523280"
  },
  {
    "text": "lambda style workloads and batch style systems on top of this thing that it's not your scale bottleneck and so that's",
    "start": "2523280",
    "end": "2528560"
  },
  {
    "text": "Nomad in a nutshell uh that's all I had thank you guys so much",
    "start": "2528560",
    "end": "2534519"
  },
  {
    "text": "i think we have probably like three minutes for questions yeah I I will start with the first question that comes",
    "start": "2538000",
    "end": "2543599"
  },
  {
    "text": "through the app is Nomad Nomad aware of the utilization of the underlying",
    "start": "2543599",
    "end": "2548880"
  },
  {
    "text": "resources in other words can Nomad help me avoid hot resources by moving jobs",
    "start": "2548880",
    "end": "2554960"
  },
  {
    "text": "around that's a good question so it's sort of two different questions one of those is is Nomad aware of it the answer",
    "start": "2554960",
    "end": "2561040"
  },
  {
    "text": "is yes so for any job you can basically ask Nomad give me the allocation status and it'll tell you you know memory usage",
    "start": "2561040",
    "end": "2566800"
  },
  {
    "text": "CPU usage etc and it'll give you kind of fine grain resource utilization the second question is can Nomad basically",
    "start": "2566800",
    "end": "2573040"
  },
  {
    "text": "automatically mitigate the noisy neighbor problem so I happen to be colllocated with an app that is really",
    "start": "2573040",
    "end": "2578160"
  },
  {
    "text": "noisy uh can it just move my application to deal with that not today uh it's certainly something that could be done",
    "start": "2578160",
    "end": "2584000"
  },
  {
    "text": "we sort of have the right feedback loops for it we have the underlying telemetry uh there is just you know no process",
    "start": "2584000",
    "end": "2589760"
  },
  {
    "text": "today by which it's it's looking for these hotspots and doing a migration but it's uh very conceivable that it could",
    "start": "2589760",
    "end": "2595440"
  },
  {
    "text": "and because it's all APIdriven you could actually imagine building an app yourself that does this right you could just simply query you know for your",
    "start": "2595440",
    "end": "2601680"
  },
  {
    "text": "allocations you care about figure out okay it has a noisy neighbor i'm just going to like you know taint that one and make Nomad force Nomad to basically",
    "start": "2601680",
    "end": "2608000"
  },
  {
    "text": "migrate it somewhere else so you could imagine actually just using its APIs today to to build it yourself",
    "start": "2608000",
    "end": "2615200"
  },
  {
    "text": "more questions",
    "start": "2615200",
    "end": "2618440"
  },
  {
    "text": "hello um so in the uh example you gave of using uh KVM as a um underlying",
    "start": "2622160",
    "end": "2631240"
  },
  {
    "text": "platform when you speak about the slightly stateful applications you",
    "start": "2631240",
    "end": "2637040"
  },
  {
    "text": "spoke about and the ability to copy data does that expect shared storage in order",
    "start": "2637040",
    "end": "2642480"
  },
  {
    "text": "to work uh good question so the question is when we're doing the copying of data for the semi-stateful applications does",
    "start": "2642480",
    "end": "2648480"
  },
  {
    "text": "it expect shared storage no so it doesn't depend on using like a SAN or network attached storage is it's",
    "start": "2648480",
    "end": "2654240"
  },
  {
    "text": "literally like copying the data from one machine to another machine it doesn't expect that there sort of have some underlying shared storage and so it'll",
    "start": "2654240",
    "end": "2661440"
  },
  {
    "text": "prefer to keep you on the same machine so Nomad would rather not copy over the network and instead find room on the",
    "start": "2661440",
    "end": "2667119"
  },
  {
    "text": "same machine uh but if it has to or you've changed your constraints then it'll slave it over the network",
    "start": "2667119",
    "end": "2673799"
  },
  {
    "text": "cool any other questions awesome well thank you guys so much thank you",
    "start": "2673920",
    "end": "2681079"
  }
]