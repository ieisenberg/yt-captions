[
  {
    "start": "0",
    "end": "141000"
  },
  {
    "text": "imagine you have built this simple servoless event driven architecture in the cloud you pick the services you want",
    "start": "12639",
    "end": "20320"
  },
  {
    "text": "to use you connected them together and everything just works like magic so much so that you decide to go",
    "start": "20320",
    "end": "27080"
  },
  {
    "text": "to production with it and all of a sudden Hell Breaks Loose and everything starts to fail in all possible shapes",
    "start": "27080",
    "end": "33960"
  },
  {
    "text": "and forms and there's nothing you have done wrong that's what you think well",
    "start": "33960",
    "end": "40440"
  },
  {
    "text": "maybe you didn't read all the extensive documentation of all the services and all the libraries that you are using but",
    "start": "40440",
    "end": "46800"
  },
  {
    "text": "come on who does that nowadays anyway right so what's the next logical",
    "start": "46800",
    "end": "52640"
  },
  {
    "text": "step maybe you just swear of using services or architecture in question ever again because well they just don't",
    "start": "52640",
    "end": "59519"
  },
  {
    "text": "work and what's wrong with the cloud anyway and servess is just an expensive fat",
    "start": "59519",
    "end": "65680"
  },
  {
    "text": "maybe just let's all go back on Prem and use good gold monoliths or what do you",
    "start": "65680",
    "end": "71920"
  },
  {
    "text": "think about it though purely statistically speaking we should assume that the problem is not",
    "start": "71920",
    "end": "78520"
  },
  {
    "text": "necessarily the cloud maybe not the cloud providers maybe not even the services or the architecture that you",
    "start": "78520",
    "end": "84320"
  },
  {
    "text": "have built so what is it then as Murphy's law says anything that",
    "start": "84320",
    "end": "90400"
  },
  {
    "text": "can go wrong will go wrong though I personally prefer the more extended version of it that says anything that",
    "start": "90400",
    "end": "96320"
  },
  {
    "text": "can go wrong will go wrong at the worst possible time my name is Anahit I'm a lead Cloud",
    "start": "96320",
    "end": "103479"
  },
  {
    "text": "software engineer at the company called Solita and I'm personally based in Finland and as Julian said I'm also an",
    "start": "103479",
    "end": "109320"
  },
  {
    "text": "AWS data hero and there's this funny thing I noticed after becoming an AWS",
    "start": "109320",
    "end": "114439"
  },
  {
    "text": "hero people for some reason started to come to me with the smirk on their faces and saying so tell us now what's wrong",
    "start": "114439",
    "end": "120840"
  },
  {
    "text": "with the cloud why doesn't it work so I'm finally here to answer that or maybe",
    "start": "120840",
    "end": "127479"
  },
  {
    "text": "not that but I want us to look together at something that we as humans usually",
    "start": "127479",
    "end": "132760"
  },
  {
    "text": "don't feel comfortable looking at failures and I hope that this talk helps",
    "start": "132760",
    "end": "139640"
  },
  {
    "text": "you to become a bit more aware maybe to spot patterns that others don't",
    "start": "139640",
    "end": "144720"
  },
  {
    "start": "141000",
    "end": "172000"
  },
  {
    "text": "necessarily see and to have the tools to ask questions and make ious and critical",
    "start": "144720",
    "end": "150840"
  },
  {
    "text": "decisions take control into your own hands rather than believing in Magic and",
    "start": "150840",
    "end": "156000"
  },
  {
    "text": "we will see soon what I mean and become a tiny bit paranoid but in a good way",
    "start": "156000",
    "end": "162680"
  },
  {
    "text": "because to borrow words of Martin kman in distributed systems suspicion pessimism and paranoia pay",
    "start": "162680",
    "end": "170400"
  },
  {
    "text": "off but before we start talking about distributed systems and failures any further let's get back for a second to",
    "start": "170400",
    "end": "176560"
  },
  {
    "start": "172000",
    "end": "325000"
  },
  {
    "text": "our fic fictional story with failing serverless architecture it actually had",
    "start": "176560",
    "end": "182360"
  },
  {
    "text": "a prequel to it so once upon a time you were a developer who started developing",
    "start": "182360",
    "end": "188040"
  },
  {
    "text": "software that was probably supposed to run on a single machine somewhere on Prem and the only thing you actually",
    "start": "188040",
    "end": "195440"
  },
  {
    "text": "cared about were the so-called functional requirements so you made sure that your code worked and had as little",
    "start": "195440",
    "end": "201680"
  },
  {
    "text": "bugs and failures as possible and that was your definition of reliability of course there were some",
    "start": "201680",
    "end": "208640"
  },
  {
    "text": "occasional uh failures Hardware failures but didn't care much about them mostly things were pretty nicely deterministic",
    "start": "208640",
    "end": "215480"
  },
  {
    "text": "everything either worked or it didn't and next thing you know you jump",
    "start": "215480",
    "end": "221840"
  },
  {
    "text": "over to the cloud maybe you start using I don't know why this fails you start",
    "start": "221840",
    "end": "227720"
  },
  {
    "text": "using um virtual machines and you start developing software that requires you to",
    "start": "227720",
    "end": "232879"
  },
  {
    "text": "think about the so-called non-functional requirements so things like certain",
    "start": "232879",
    "end": "238200"
  },
  {
    "text": "levels of availability scalability also reliability and resilience get a whole new",
    "start": "238200",
    "end": "244680"
  },
  {
    "text": "meaning and you still have all your functional requirements and you still need to make sure that things work but",
    "start": "244680",
    "end": "252239"
  },
  {
    "text": "the complexity level just went up Notch and you need to take care of so much more and also the failures are becoming",
    "start": "252239",
    "end": "259000"
  },
  {
    "text": "that much more pronounced and less deterministic welcome to the dark side",
    "start": "259000",
    "end": "265199"
  },
  {
    "text": "the wonderful world of distributed systems where with great power comes great respon",
    "start": "265199",
    "end": "271720"
  },
  {
    "text": "responsibility but of course things didn't stop there and next thing we know you jump over to the seress and fully",
    "start": "271759",
    "end": "277440"
  },
  {
    "text": "managed World wherever that line goes nowadays and everything seems to be",
    "start": "277440",
    "end": "282520"
  },
  {
    "text": "simple again you just pick Services you connect them together and everything just works like magic and you don't",
    "start": "282520",
    "end": "289680"
  },
  {
    "text": "really see any machines around moreover the word seress suggest that you don't need to be looking for any machines",
    "start": "289680",
    "end": "295919"
  },
  {
    "text": "around and the cloud provider takes care of most of those ilities for you so",
    "start": "295919",
    "end": "301160"
  },
  {
    "text": "reliability availability scalability and you're once again back to just being a developer who cares about the code",
    "start": "301160",
    "end": "308160"
  },
  {
    "text": "working and not having box though we kind of know that's not how our story ends",
    "start": "308160",
    "end": "314320"
  },
  {
    "text": "right because anything that can go wrong will go wrong so what is it that can go",
    "start": "314320",
    "end": "323160"
  },
  {
    "text": "wrong so let's look at the serverless cloud distributed systems in really simplified terms distributed system just",
    "start": "323880",
    "end": "331560"
  },
  {
    "start": "325000",
    "end": "391000"
  },
  {
    "text": "a bunch of machines connected with a network and while it comes with a lot of",
    "start": "331560",
    "end": "336639"
  },
  {
    "text": "new and exciting ways to build Solutions solve problems it also comes with a lot of new and exciting ways for things to",
    "start": "336639",
    "end": "343199"
  },
  {
    "text": "go wrong because the resources you are using are not limited anymore to a single machine they are distributed",
    "start": "343199",
    "end": "349560"
  },
  {
    "text": "across various servers maybe server RS data centers and instead of just one",
    "start": "349560",
    "end": "354840"
  },
  {
    "text": "machine that can fail now you have plenty and all of those failures can",
    "start": "354840",
    "end": "360120"
  },
  {
    "text": "happen in a most non-deterministic way completely independently of each other and failures can be on any level it can",
    "start": "360120",
    "end": "366400"
  },
  {
    "text": "be Hardware software failures maybe operating system hard drive network adapter anything can fail at any given",
    "start": "366400",
    "end": "373759"
  },
  {
    "text": "moment but the worst thing is that all of those machines are talking to each",
    "start": "373759",
    "end": "378800"
  },
  {
    "text": "other over a network and the network is known for one thing in particular wherever there is any communication",
    "start": "378800",
    "end": "385520"
  },
  {
    "text": "going over a network it will eventually fail",
    "start": "385520",
    "end": "390800"
  },
  {
    "text": "and any cloud is built on top of such distributed systems that's where the superpowers come from the cloud",
    "start": "390800",
    "end": "396000"
  },
  {
    "start": "391000",
    "end": "424000"
  },
  {
    "text": "providers take care of managing the underlying distributed infrastructure for us obstructing the lowlevel",
    "start": "396000",
    "end": "402639"
  },
  {
    "text": "complexities away from us and giving us access to this huge pool of shared resources that we can use storage",
    "start": "402639",
    "end": "409360"
  },
  {
    "text": "Network compute while doing this at a truly massive scale that no individual user could ever achieve but especially",
    "start": "409360",
    "end": "416120"
  },
  {
    "text": "at the bigger scale if something has a tiny little chance of happening it most certainly",
    "start": "416120",
    "end": "422560"
  },
  {
    "text": "will and servoless and fully managed services are a step up in an obstruction",
    "start": "422560",
    "end": "428360"
  },
  {
    "start": "424000",
    "end": "527000"
  },
  {
    "text": "ladder they kind of make the underlying distributed infrastructure seem almost invisible almost magical so much so that",
    "start": "428360",
    "end": "436680"
  },
  {
    "text": "we even sometimes forget is there but by using seress fully managed Services we",
    "start": "436680",
    "end": "442039"
  },
  {
    "text": "didn't just magically teleport to a different reality we are still living in this very same messy physical world with",
    "start": "442039",
    "end": "449840"
  },
  {
    "text": "all the underlying messy infrastructure with all its limitations and trade-offs and this higher level of",
    "start": "449840",
    "end": "456919"
  },
  {
    "text": "complexity of course makes a lot of things easier just like higher level programming language would but it also",
    "start": "456919",
    "end": "463400"
  },
  {
    "text": "comes with a danger because being seemingly simple to use it might also",
    "start": "463400",
    "end": "469639"
  },
  {
    "text": "give you this false sense of security and this might make sporting potential failures that much harder because",
    "start": "469639",
    "end": "476560"
  },
  {
    "text": "they're also obstructed away from you and the reality is failures didn't",
    "start": "476560",
    "end": "482280"
  },
  {
    "text": "really disappear anyway anywhere they are still embedded in the very distributed system that you are using",
    "start": "482280",
    "end": "487879"
  },
  {
    "text": "and waiting to show up and as Leslie Lampert said in already 1987 a distributed system in is one in",
    "start": "487879",
    "end": "495240"
  },
  {
    "text": "which the failure of a computer you didn't even know existed can render your own computer",
    "start": "495240",
    "end": "500840"
  },
  {
    "text": "unusable now we could kind of rephrase it for serverless applications or architectures a serverless architecture",
    "start": "500840",
    "end": "507440"
  },
  {
    "text": "is one in which a failure of a computer you didn't know was there can render your entire architecture",
    "start": "507440",
    "end": "513959"
  },
  {
    "text": "unusable but with surus of course we don't see failures as Hardware failures or blue screens failures manifest in a",
    "start": "513959",
    "end": "521680"
  },
  {
    "text": "bit more subtle way now let's climb one last step in our",
    "start": "521680",
    "end": "529880"
  },
  {
    "start": "527000",
    "end": "730000"
  },
  {
    "text": "obstruction ladder and talk about building distributed applications on top of distributed systems and managed and",
    "start": "529880",
    "end": "536880"
  },
  {
    "text": "services and serverless so in in essence what we do is we split a problem at hand into",
    "start": "536880",
    "end": "542440"
  },
  {
    "text": "smaller pieces we pick services and resources we want to use for each of",
    "start": "542440",
    "end": "547560"
  },
  {
    "text": "them and then we connect them together with things like messages HTTP requests",
    "start": "547560",
    "end": "552720"
  },
  {
    "text": "events all of those things are using network in some shape or form so you might have noticed that in",
    "start": "552720",
    "end": "560640"
  },
  {
    "text": "essence the distributed application is mirroring the underlying distributed system right they also give us this",
    "start": "560640",
    "end": "566560"
  },
  {
    "text": "ability to build Solutions in a completely different way but just like the underlying distributed system it",
    "start": "566560",
    "end": "572720"
  },
  {
    "text": "will come with some trade-offs the architectures that you are going to build are likely going to be complex and",
    "start": "572720",
    "end": "580000"
  },
  {
    "text": "every piece will be failing at any given moment in the most non-deterministic way",
    "start": "580000",
    "end": "585120"
  },
  {
    "text": "possible and whenever there is any communication happening over Network it",
    "start": "585120",
    "end": "590200"
  },
  {
    "text": "will eventually fail so how do we make our distributed",
    "start": "590200",
    "end": "596600"
  },
  {
    "text": "applications more resilient in the face of all those impending failures well we",
    "start": "596600",
    "end": "602120"
  },
  {
    "text": "are mirroring the underlying distributed system so let's take a look at how Cloud providers are dealing with all those",
    "start": "602120",
    "end": "608680"
  },
  {
    "text": "failures because they do have quite some experience with that and of course there's a lot of complex mechanisms and",
    "start": "608680",
    "end": "614519"
  },
  {
    "text": "algorithms at play that we are not going to go into today but surprisingly the two of the most effective tools for",
    "start": "614519",
    "end": "621120"
  },
  {
    "text": "better resilience in distributed applications architectures are also seemingly",
    "start": "621120",
    "end": "627200"
  },
  {
    "text": "simple they are timeouts and retries and those are the things that we",
    "start": "627200",
    "end": "633680"
  },
  {
    "text": "absolutely need to be aware of when we are building our distributed applications I call them these",
    "start": "633680",
    "end": "639279"
  },
  {
    "text": "superpowers because they can be extremely powerful but just like with",
    "start": "639279",
    "end": "644560"
  },
  {
    "text": "superpowers we need to be very careful how we use them and we'll get back to that in a",
    "start": "644560",
    "end": "650130"
  },
  {
    "text": "[Music] second so you might have noticed that so far I haven't even mentioned any Cloud",
    "start": "650130",
    "end": "656079"
  },
  {
    "text": "providers or any Services because those are the things that are pretty much Universal to any of",
    "start": "656079",
    "end": "663360"
  },
  {
    "text": "them but now it's finally time to move on from fiction to reality and our",
    "start": "663360",
    "end": "669360"
  },
  {
    "text": "fictional story wasn't that fictional after all actually something that happened to me to some degree so I was",
    "start": "669360",
    "end": "676279"
  },
  {
    "text": "working with a customer where we're building this simple powerful distributed architecture for new",
    "start": "676279",
    "end": "681519"
  },
  {
    "text": "realtime data streaming at a pretty big scale on a quiet day we would have something like over half a terabyte of",
    "start": "681519",
    "end": "687680"
  },
  {
    "text": "data coming in and we wanted to capture that data to store it and to process it later so for that we had our producer",
    "start": "687680",
    "end": "694800"
  },
  {
    "text": "application that got the data and then we wrote the data to Kinesis data stream and on the other end of it we connected",
    "start": "694800",
    "end": "701680"
  },
  {
    "text": "the Lambda function to it to read the data and just like that we built ourselves a simple serverless data",
    "start": "701680",
    "end": "708560"
  },
  {
    "text": "processing Pipeline and it was great until one day we realized that we are actually losing",
    "start": "708560",
    "end": "716120"
  },
  {
    "text": "data in this Pipeline and we had no idea it was happening thank you higher level of abstraction so",
    "start": "716120",
    "end": "723399"
  },
  {
    "text": "what exactly was going on there and there were actually several things we will get to them in a moment so first of",
    "start": "723399",
    "end": "729920"
  },
  {
    "text": "all Kinesis data streams if you are not familiar it's a fully managed massively SC scalable service on AWS to stream",
    "start": "729920",
    "end": "737440"
  },
  {
    "start": "730000",
    "end": "849000"
  },
  {
    "text": "data after you write the data to the stream it's available for you in milliseconds and it stays in a stream",
    "start": "737440",
    "end": "742720"
  },
  {
    "text": "for at least 24 hours or up to a year if you configure it to be so and during",
    "start": "742720",
    "end": "748000"
  },
  {
    "text": "that entire time you can consume or read that data from the stream as many times as you want in whatever form or shape",
    "start": "748000",
    "end": "753480"
  },
  {
    "text": "that you want but whatever you do you cannot delete the data from the stream once it's in the Stream it stays there",
    "start": "753480",
    "end": "758839"
  },
  {
    "text": "for at least 24 hours and Kinesis is a very powerful tool also for event driven",
    "start": "758839",
    "end": "765079"
  },
  {
    "text": "architectures though it's mainly uh used for data applications but it's also very powerful for Avenger architectures and",
    "start": "765079",
    "end": "772399"
  },
  {
    "text": "there are no servers or clusters to manage and it scales pretty massively to",
    "start": "772399",
    "end": "777959"
  },
  {
    "text": "achieve that massive SC scalability it uses a concept of A Shard which is probably familiar to you from different",
    "start": "777959",
    "end": "784480"
  },
  {
    "text": "areas but in Kinesis it means an ordered cue so you can think of it as stream",
    "start": "784480",
    "end": "790360"
  },
  {
    "text": "being composed of multiple ordered cues and when you write data to the stream they end up in those cues or shards and",
    "start": "790360",
    "end": "797880"
  },
  {
    "text": "each shart would come with certain limitations you can write up to one megabyte or 1,000 records of incoming",
    "start": "797880",
    "end": "805760"
  },
  {
    "text": "data per second but the number of sharts you can have in in each stream is virtually unlimited so you can add as",
    "start": "805760",
    "end": "812240"
  },
  {
    "text": "many shards as you want to stream as much data as you need and to get all that data to Kinesis",
    "start": "812240",
    "end": "819839"
  },
  {
    "text": "you actually have two ways of doing it or in essence two distinct API calls you can either write individual records or",
    "start": "819839",
    "end": "826720"
  },
  {
    "text": "you can batch up to 500 records and send them in a single put records API call",
    "start": "826720",
    "end": "832639"
  },
  {
    "text": "and in general batching is a more effective and less resource intensive way of making API Cod especially at a",
    "start": "832639",
    "end": "840079"
  },
  {
    "text": "bigger scale but usually when something comes with Benefits it also comes with some extra responsibilities and we'll",
    "start": "840079",
    "end": "847360"
  },
  {
    "text": "get back to that in a moment so we have established by now that failures are going to happen but",
    "start": "847360",
    "end": "855199"
  },
  {
    "start": "849000",
    "end": "1172000"
  },
  {
    "text": "how does that manifest at this higher level of abstraction that we are using with services like Kinesis for example",
    "start": "855199",
    "end": "862120"
  },
  {
    "text": "well that's actually pretty straightforward because when we interact with services like Kinesis from our code",
    "start": "862120",
    "end": "868360"
  },
  {
    "text": "we are in essence using API calls and every API call can fail at any given",
    "start": "868360",
    "end": "875480"
  },
  {
    "text": "moment now the good news here is that if we are using AWS SDK to make those API",
    "start": "875480",
    "end": "881120"
  },
  {
    "text": "calls it will actually take care of most of that failures for us after AWS does know firsth hands firsthand that the",
    "start": "881120",
    "end": "887759"
  },
  {
    "text": "failures will happen so they have built into the SDK one of those essential tools for better resilience the r rise",
    "start": "887759",
    "end": "894959"
  },
  {
    "text": "or superpow as we know it now the trouble with rri in general",
    "start": "894959",
    "end": "901440"
  },
  {
    "text": "is that now we have a potential of turning maybe a small intermittent failure like some Network glitch into a",
    "start": "901440",
    "end": "908839"
  },
  {
    "text": "massive one because retri can have really an unexpected blast radius they",
    "start": "908839",
    "end": "914600"
  },
  {
    "text": "can spread this uh ripple effect of cascading failures through the system",
    "start": "914600",
    "end": "919839"
  },
  {
    "text": "and ultimately bring the entire system down because retri are inherently",
    "start": "919839",
    "end": "927360"
  },
  {
    "text": "selfish it's just like hitting the refresh button on your browser we all know we shouldn't do it but we do it",
    "start": "927360",
    "end": "933319"
  },
  {
    "text": "anyway because retri imply that our request is more important and more",
    "start": "933319",
    "end": "938959"
  },
  {
    "text": "valuable than anybody else right and we are ready to consume extra resources to",
    "start": "938959",
    "end": "946279"
  },
  {
    "text": "add more load to maybe incur more costs on the underlying systems just to make",
    "start": "946279",
    "end": "951759"
  },
  {
    "text": "sure that our request goes through no matter what but retri are not always effective",
    "start": "951759",
    "end": "958519"
  },
  {
    "text": "neither are safe first of all which failures do we even retry in the first place and it's a very complicated",
    "start": "958519",
    "end": "966800"
  },
  {
    "text": "question let's say a downstream system like an API or a database is under a heavy load and that's what causing",
    "start": "966800",
    "end": "973160"
  },
  {
    "text": "failures well then if you start R trying the chances are pretty high you are just going to make matters",
    "start": "973160",
    "end": "978560"
  },
  {
    "text": "worse or if the failure is caused by the timeout and then you are not ready to",
    "start": "978560",
    "end": "984519"
  },
  {
    "text": "wait the time that it will take to make the retry maybe you have your own SLA requirements for the time and then",
    "start": "984519",
    "end": "990959"
  },
  {
    "text": "retrying means that you are just being selfish you are just waiting resources for nothing just like hitting refresh",
    "start": "990959",
    "end": "996600"
  },
  {
    "text": "and just closing browser right and what if the downstream system",
    "start": "996600",
    "end": "1002800"
  },
  {
    "text": "itself has its own retries built in and what if they are using a distributed architecture with r rise on many",
    "start": "1002800",
    "end": "1008319"
  },
  {
    "text": "different levels our R rise will then multiply and they will amplify that",
    "start": "1008319",
    "end": "1014120"
  },
  {
    "text": "possible negative effect that retries can have and this is where we can see that skaing failure effect through the",
    "start": "1014120",
    "end": "1021079"
  },
  {
    "text": "entire system and ultimately bring the system down and this is especially true if we start retrying right away without",
    "start": "1021079",
    "end": "1028038"
  },
  {
    "text": "giving the underlying system a chance to recover and what if the operation that",
    "start": "1028039",
    "end": "1033880"
  },
  {
    "text": "you are retrying actually has side effects let's say it's updating a database well then retrying might mean",
    "start": "1033880",
    "end": "1040199"
  },
  {
    "text": "that you will have unpredictable results so there are many different considerations but the bottom line here",
    "start": "1040199",
    "end": "1046600"
  },
  {
    "text": "is that we need to be extremely careful about how we are retrying we need to be",
    "start": "1046600",
    "end": "1052080"
  },
  {
    "text": "mindful about the superpower that we are given luckily in case of AWS SDK does",
    "start": "1052080",
    "end": "1057960"
  },
  {
    "text": "come with some builtin safety measures so if a request to a service like Kinesis for example fails SDK will only",
    "start": "1057960",
    "end": "1065400"
  },
  {
    "text": "retry or handle the so-called retryable failures so things like service unavailable timeouts other transient",
    "start": "1065400",
    "end": "1073039"
  },
  {
    "text": "errors and for those retryable failures it will retry them behind the scenes on your behalf but it will stop after",
    "start": "1073039",
    "end": "1080080"
  },
  {
    "text": "certain amount of attempts and between those attempts it will use the so-called exponential backoff so that the time",
    "start": "1080080",
    "end": "1087360"
  },
  {
    "text": "between retry attempts will be increasing exponentially and those are seemingly",
    "start": "1087360",
    "end": "1092559"
  },
  {
    "text": "simple but very crucial details that can either Make It or Break It And they can",
    "start": "1092559",
    "end": "1098159"
  },
  {
    "text": "actually turn R rise from being a very powerful tool for better resilience into the main cause of a system outage",
    "start": "1098159",
    "end": "1105000"
  },
  {
    "text": "because we only want to retry if it can actually improve the situation so transient failures and if we do retry we",
    "start": "1105000",
    "end": "1112600"
  },
  {
    "text": "also want to limit the retry attempts and stop retrying when it doesn't improve the situation anymore to avoid",
    "start": "1112600",
    "end": "1118600"
  },
  {
    "text": "that further reple effect of cascading failures and we want to spread the retry attempts as uniformly as possible to",
    "start": "1118600",
    "end": "1126960"
  },
  {
    "text": "avoid this burst of retries to the underlying system to avoid overwhelming the underlying",
    "start": "1126960",
    "end": "1132679"
  },
  {
    "text": "system now with aw ssdk we have the safety measures built in but we are also",
    "start": "1132679",
    "end": "1137880"
  },
  {
    "text": "given the opportunity to configure some of those parameters and this is just an example of how Javas JavaScript SDK",
    "start": "1137880",
    "end": "1145440"
  },
  {
    "text": "allows you to configure them every language will have their own ways and their own default values but all of them",
    "start": "1145440",
    "end": "1150880"
  },
  {
    "text": "will allow you to configure those some of those parameters and the same way they will allow you to configure this",
    "start": "1150880",
    "end": "1156600"
  },
  {
    "text": "other superpower that we have the timeout related values now if you are thinking that",
    "start": "1156600",
    "end": "1162440"
  },
  {
    "text": "timeouts don't sound that much as a superpower well I have a news for you because distributed systems time outes",
    "start": "1162440",
    "end": "1169000"
  },
  {
    "text": "are pretty much given so let's once again take a look at",
    "start": "1169000",
    "end": "1174480"
  },
  {
    "start": "1172000",
    "end": "1256000"
  },
  {
    "text": "the underlying system in very simplified terms so when we interact with a service",
    "start": "1174480",
    "end": "1179679"
  },
  {
    "text": "no matter how serverless or not it is from our code we are in essence using API method calls that are obstructed",
    "start": "1179679",
    "end": "1186080"
  },
  {
    "text": "away as SDK method calls and do method calls look exactly the same as any other",
    "start": "1186080",
    "end": "1192400"
  },
  {
    "text": "local method invocations but let's not let that fool us because we know that the network is still there it's just",
    "start": "1192400",
    "end": "1199880"
  },
  {
    "text": "abstracted away from us and any request that you send over the network like an",
    "start": "1199880",
    "end": "1204960"
  },
  {
    "text": "API call to Kinesis for example can fail in many different shapes and forms",
    "start": "1204960",
    "end": "1210280"
  },
  {
    "text": "moreover it's usually impossible to tell if the request actually went through or not because that failure can happen on",
    "start": "1210280",
    "end": "1216440"
  },
  {
    "text": "many different levels so maybe sending the request fails or receiving the request fails or maybe the request just",
    "start": "1216440",
    "end": "1222400"
  },
  {
    "text": "waits in the queue to be processed because the service is busy or actually it was processed but you just never got",
    "start": "1222400",
    "end": "1229440"
  },
  {
    "text": "the response back so there's plenty of different options but the end result is still the same you might be stuck",
    "start": "1229440",
    "end": "1235919"
  },
  {
    "text": "waiting for something that will never happen and this is true for any service no matter how serverless or not it is",
    "start": "1235919",
    "end": "1242520"
  },
  {
    "text": "it's true for Kinesis as well and to avoid wa waiting forever AWS has built",
    "start": "1242520",
    "end": "1248200"
  },
  {
    "text": "into the SDK this second superpower or essential tool for better resilience the",
    "start": "1248200",
    "end": "1253440"
  },
  {
    "text": "timeouts and this ability to configure timeouts is our superpower that we can use but again just like with retries we",
    "start": "1253440",
    "end": "1260760"
  },
  {
    "start": "1256000",
    "end": "1826000"
  },
  {
    "text": "need to be especially mindful about how we do this because picking the right",
    "start": "1260760",
    "end": "1266159"
  },
  {
    "text": "time out value is actually not an easy task at all I would argue it's one of the hardest task that we have just like",
    "start": "1266159",
    "end": "1273000"
  },
  {
    "text": "any other decisions in our architecture they will come with trade-offs because if timeouts are too long well they are",
    "start": "1273000",
    "end": "1279600"
  },
  {
    "text": "going to be ineffective they also can increase latencies and in consume",
    "start": "1279600",
    "end": "1285640"
  },
  {
    "text": "resources and if the timeouts are too short well we are likely retrying too early we're not giving the original",
    "start": "1285640",
    "end": "1292559"
  },
  {
    "text": "request a chance to continue or to complete and hence we are increasing the load on the underlying system and we can",
    "start": "1292559",
    "end": "1299240"
  },
  {
    "text": "end up in this situation with this rle effect of cascading failures and on top of all that of",
    "start": "1299240",
    "end": "1306159"
  },
  {
    "text": "course an appropriate timeout value will be different for every service that you are using every operation that you are",
    "start": "1306159",
    "end": "1312240"
  },
  {
    "text": "making but with JavaScript SDK at least I have been scaring people",
    "start": "1312240",
    "end": "1317679"
  },
  {
    "text": "about that for for a long time with JavaScript SDK it will for every service call it will wait for two entire minutes",
    "start": "1317679",
    "end": "1325720"
  },
  {
    "text": "before it times out the request so if you think about it we are",
    "start": "1325720",
    "end": "1331440"
  },
  {
    "text": "probably dealing with something low latency like Dynamo for example or Kinesis in this case and it takes SDK 2",
    "start": "1331440",
    "end": "1338960"
  },
  {
    "text": "minutes to just decide that okay let's time out that request and let's try all",
    "start": "1338960",
    "end": "1344279"
  },
  {
    "text": "over so it's a horribly long time well since then things have changed",
    "start": "1344279",
    "end": "1350240"
  },
  {
    "text": "things have evolved SDK has moved from version two to version three they have",
    "start": "1350240",
    "end": "1355279"
  },
  {
    "text": "updated a default timeouts now it's infinite there are no timeouts so",
    "start": "1355279",
    "end": "1363279"
  },
  {
    "text": "prepare to be stuck waiting for a really long time if you don't configure those timeout values",
    "start": "1363279",
    "end": "1370480"
  },
  {
    "text": "yourself and this brings us to the very first reason for losing data in our story if you remember we had some really",
    "start": "1370480",
    "end": "1376760"
  },
  {
    "text": "big issues there and this was before the infinite timeout still but but even in that case waiting for too long for",
    "start": "1376760",
    "end": "1383440"
  },
  {
    "text": "request to time out can exhaust resources of your producer application and you will not be able to consume any",
    "start": "1383440",
    "end": "1389799"
  },
  {
    "text": "new requests that come in and you will inevitably lose the data and that's exactly what we were seeing so because",
    "start": "1389799",
    "end": "1396799"
  },
  {
    "text": "of probably some intermittent glitch with network or a service that was causing the timeouts now we suddenly had",
    "start": "1396799",
    "end": "1403640"
  },
  {
    "text": "a complete system outage which is in essence the opposite from what resilient system be we should mask we should",
    "start": "1403640",
    "end": "1410360"
  },
  {
    "text": "recover from Individual failures and here we just amplify them now this obviously doesn't sound",
    "start": "1410360",
    "end": "1418200"
  },
  {
    "text": "too great I know but if you are thinking that the solution is to have short time out values well I have bad news for you",
    "start": "1418200",
    "end": "1424919"
  },
  {
    "text": "once again because in my personal experience short timeouts or two short",
    "start": "1424919",
    "end": "1429960"
  },
  {
    "text": "timeouts can be even more dangerous than two long timeouts especially if they are",
    "start": "1429960",
    "end": "1435240"
  },
  {
    "text": "combined with the retries because once once again if you have too short time out you don't give a",
    "start": "1435240",
    "end": "1441640"
  },
  {
    "text": "request a chance to complete you retry it over and over again just like this refresh button that you are hitting and",
    "start": "1441640",
    "end": "1448159"
  },
  {
    "text": "you're inevitably increasing load on the underlying system and there's plenty of fun things that can come out of that you",
    "start": "1448159",
    "end": "1454799"
  },
  {
    "text": "will increase latencies you will see a lot of Errors you will increase costs and ultimately you will bring the entire",
    "start": "1454799",
    "end": "1460600"
  },
  {
    "text": "system down and once again if we are Del developing a resilient architecture",
    "start": "1460600",
    "end": "1467159"
  },
  {
    "text": "which we usually should be we need to mask and recover from Individual failures we need to make sure",
    "start": "1467159",
    "end": "1473000"
  },
  {
    "text": "that the system works as a whole even if individual failures happen and here is where wrongly configured timeouts and",
    "start": "1473000",
    "end": "1479600"
  },
  {
    "text": "retries can be a real match made in hell so once again even if we have these",
    "start": "1479600",
    "end": "1487520"
  },
  {
    "text": "superpowers free tries and timeouts we need to be extremely careful about how",
    "start": "1487520",
    "end": "1492679"
  },
  {
    "text": "we use them we never want to go with a defaults defaults are very dangerous",
    "start": "1492679",
    "end": "1500120"
  },
  {
    "text": "so when you go back to your code and your architecture please check all the libraries that make any calls over a",
    "start": "1500120",
    "end": "1507200"
  },
  {
    "text": "network Let It Be SDK calls or maybe you have an external API that you are calling if there is a call going over a",
    "start": "1507200",
    "end": "1513919"
  },
  {
    "text": "network and you are using library with defaults which you probably are make sure that you know those defaults make",
    "start": "1513919",
    "end": "1521039"
  },
  {
    "text": "sure that they are not either too short or too long take control in your hands",
    "start": "1521039",
    "end": "1526760"
  },
  {
    "text": "especially if they are combined bined with the rich rise okay so so far I've been talking",
    "start": "1526760",
    "end": "1533120"
  },
  {
    "text": "about all these inevitable failures in distributed systems but there's also a special type of failures that actually",
    "start": "1533120",
    "end": "1539760"
  },
  {
    "text": "Cloud providers cause on purpose and those are the failures that are related to service limits and",
    "start": "1539760",
    "end": "1546240"
  },
  {
    "text": "throttling and this can be especially confusing in the servoless world because",
    "start": "1546240",
    "end": "1552279"
  },
  {
    "text": "we are promis scalability right and somehow we tend to assume infinite",
    "start": "1552279",
    "end": "1557559"
  },
  {
    "text": "scalability but of course if something sounds too good to be truth it's probably just that",
    "start": "1557559",
    "end": "1563399"
  },
  {
    "text": "and we better face the reality rather sooner than later because of course no resource is infinite and cloud is not",
    "start": "1563399",
    "end": "1569960"
  },
  {
    "text": "infinite but moreover we don't have the entire Cloud at our own disposal we are sharing the underlying Resources with",
    "start": "1569960",
    "end": "1576880"
  },
  {
    "text": "everybody else and sharing resources comes with trade-offs on one hand we do have this",
    "start": "1576880",
    "end": "1583039"
  },
  {
    "text": "huge pool of resources that we can use but on the other hand this might mean that an individual user might either on",
    "start": "1583039",
    "end": "1590919"
  },
  {
    "text": "purpose or by accident try to monopolize a resource and while it's not infinite",
    "start": "1590919",
    "end": "1595960"
  },
  {
    "text": "well it will cause degradation of service for everybody else so to avoid that from happening there are service",
    "start": "1595960",
    "end": "1602760"
  },
  {
    "text": "limits and Thro link is just a tool to enforce those service limits and for",
    "start": "1602760",
    "end": "1608919"
  },
  {
    "text": "example in case of Kinesis there's a service limit related to how much data you can write to an individual Shard and",
    "start": "1608919",
    "end": "1615600"
  },
  {
    "text": "when you exceed that limit your request quests will be throttled they will",
    "start": "1615600",
    "end": "1621360"
  },
  {
    "text": "fail and this brings us to the next reason for losing data in our story so I",
    "start": "1621360",
    "end": "1627640"
  },
  {
    "text": "said that AWS SDK is great handling the failures for us right but of course",
    "start": "1627640",
    "end": "1633320"
  },
  {
    "text": "there is a catch because in case of batch operations like we have here the put records operation instead of just",
    "start": "1633320",
    "end": "1639640"
  },
  {
    "text": "handling the failure of an entire operation we should also handle the so-called partial",
    "start": "1639640",
    "end": "1645640"
  },
  {
    "text": "failures because this batch operations are not Atomic it's not like either all the requests go through or all the",
    "start": "1645640",
    "end": "1652080"
  },
  {
    "text": "records go through or everything fails it might happen so that part of the",
    "start": "1652080",
    "end": "1657559"
  },
  {
    "text": "records actually go to the stream successfully and part of them fail and you still get a success response back",
    "start": "1657559",
    "end": "1664440"
  },
  {
    "text": "back from the SDK and it's your responsibility to detect those failures and to handle them",
    "start": "1664440",
    "end": "1670799"
  },
  {
    "text": "and if you are not doing that you are losing data and well that was us",
    "start": "1670799",
    "end": "1676360"
  },
  {
    "text": "moreover every single request in a batch can fail and you will still get a",
    "start": "1676360",
    "end": "1681600"
  },
  {
    "text": "success response back so the main reason for these kind",
    "start": "1681600",
    "end": "1687200"
  },
  {
    "text": "of failures is throttling or exceeding service limits for for because of let's",
    "start": "1687200",
    "end": "1692279"
  },
  {
    "text": "say uh traffic spikes and luckily we already know that there is this wonderful superpower that we can use for",
    "start": "1692279",
    "end": "1699919"
  },
  {
    "text": "transient errors which occasional spikes in traffics are the retries and when",
    "start": "1699919",
    "end": "1706679"
  },
  {
    "text": "implementing retries for partti failures or any other failures we know that there are three key things that we need to",
    "start": "1706679",
    "end": "1712720"
  },
  {
    "text": "keep in mind so let's go over them one more time we only retry for transient failures or for retryable",
    "start": "1712720",
    "end": "1720440"
  },
  {
    "text": "failures and we always set an upper limit we stop retrying where it doesn't improve anything and we use proper back",
    "start": "1720440",
    "end": "1728399"
  },
  {
    "text": "off between retry ATMs and exponential back off and cheater is a very powerful",
    "start": "1728399",
    "end": "1734200"
  },
  {
    "text": "tool to spread those rich attemps uniformally I actually cheated a little bit because SDK uses exponential back",
    "start": "1734200",
    "end": "1740039"
  },
  {
    "text": "off and Jeter I just didn't mention it before and Jeter is just a way to randomize those uh delays so that the",
    "start": "1740039",
    "end": "1747000"
  },
  {
    "text": "retry attempts will be spread as uniformly as possible because we don't want to overwhelm the system that we are",
    "start": "1747000",
    "end": "1753519"
  },
  {
    "text": "sending retries to especially if it's already under a lot of stress because we are seeing throttling so we are already",
    "start": "1753519",
    "end": "1759480"
  },
  {
    "text": "close to the Limit we don't want to push it over the limit and surprisingly a simple tool like exponential back off",
    "start": "1759480",
    "end": "1766159"
  },
  {
    "text": "and Jitter can be extremely powerful it can actually increase your Su success rate dramatically and decrease number of",
    "start": "1766159",
    "end": "1773279"
  },
  {
    "text": "retries that you need to send before the records go through so I always say that if you remember anything from my talks",
    "start": "1773279",
    "end": "1780320"
  },
  {
    "text": "it's exponential back off and Jer timeouts retries of partial failures or",
    "start": "1780320",
    "end": "1788080"
  },
  {
    "text": "partial failures in general so those are the things that are not related to Kinesis or any particular service those",
    "start": "1788080",
    "end": "1795399"
  },
  {
    "text": "are very common things for any any distributed application that you are going to build and you need to be aware",
    "start": "1795399",
    "end": "1801320"
  },
  {
    "text": "as a developer of all of them to borrow words of Gregor ho retri",
    "start": "1801320",
    "end": "1808399"
  },
  {
    "text": "have brought more distributed systems down that all the other causes together",
    "start": "1808399",
    "end": "1813600"
  },
  {
    "text": "and of course this doesn't mean that we should stop retrying but as I've been saying we should be very careful and",
    "start": "1813600",
    "end": "1819039"
  },
  {
    "text": "mindful about how we do those retries we don't want to kill the system while we try to fix",
    "start": "1819039",
    "end": "1825080"
  },
  {
    "text": "it now this brings us to the next part of losing data in our story um let's",
    "start": "1825080",
    "end": "1830840"
  },
  {
    "start": "1826000",
    "end": "2024000"
  },
  {
    "text": "move to the consumer part so if you remember there was a Lambda function reading from our stream and it turns out",
    "start": "1830840",
    "end": "1837399"
  },
  {
    "text": "with Lambda functions thing things can also escalate pretty quickly if we just let the matter",
    "start": "1837399",
    "end": "1842480"
  },
  {
    "text": "slide so Lambda function itself is actually a prime representative of these",
    "start": "1842480",
    "end": "1848080"
  },
  {
    "text": "distributed architectures because it consists of many different components that work together behind the scenes to",
    "start": "1848080",
    "end": "1854519"
  },
  {
    "text": "make it so very powerful and my personal favorite component is the Event Source",
    "start": "1854519",
    "end": "1859600"
  },
  {
    "text": "mapping many don't know it exist but it's actually very very powerful component that is very well hinded",
    "start": "1859600",
    "end": "1866039"
  },
  {
    "text": "underneath the Lambda abstraction layer and for example in case of Kinesis when you attach Lambda function to read from",
    "start": "1866039",
    "end": "1872679"
  },
  {
    "text": "a stream you are actually attaching an Event Source mapping to that stream and Event Source mapping will be pulling",
    "start": "1872679",
    "end": "1877720"
  },
  {
    "text": "reading records from that stream batching them and invoking your Lambda function for you and it also works with",
    "start": "1877720",
    "end": "1884440"
  },
  {
    "text": "some other event sources and Event Source mapping will be picking",
    "start": "1884440",
    "end": "1890360"
  },
  {
    "text": "those records from every shard in your stream in parallel and you can configure to have as many as 10 lambdas reading",
    "start": "1890360",
    "end": "1897559"
  },
  {
    "text": "from each chart in your stream which effectively means that you can have up to 10 times as many concurrent lambdas",
    "start": "1897559",
    "end": "1904840"
  },
  {
    "text": "reading from your stream as you have shards in your stream now this is where",
    "start": "1904840",
    "end": "1910240"
  },
  {
    "text": "the power of parallel processing kicks in because now we can parallelize reading data from the stream we can have",
    "start": "1910240",
    "end": "1916480"
  },
  {
    "text": "up to 10 lambdas reading concurrently from each chart in your stream but of",
    "start": "1916480",
    "end": "1922600"
  },
  {
    "text": "course power responsibility nothing is infinite",
    "start": "1922600",
    "end": "1927720"
  },
  {
    "text": "Lambda is not infinite neither and it has its own limitations and this one we can come across really easily especially",
    "start": "1927720",
    "end": "1934360"
  },
  {
    "text": "with paralyzation factor and this is the Lambda concurrency limit and I always bring it up because it it's a limit that",
    "start": "1934360",
    "end": "1940559"
  },
  {
    "text": "can have an extremely big blast radius so if you're not familiar Lambda concurrency limit it just means that you",
    "start": "1940559",
    "end": "1947159"
  },
  {
    "text": "can have limited amount of concurrent Lambda inv vacations in the same account in the same region so limited amount of",
    "start": "1947159",
    "end": "1953760"
  },
  {
    "text": "Lambda instan is running at the same time essentially and by default it's 1,000 though I'm hearing that new",
    "start": "1953760",
    "end": "1959799"
  },
  {
    "text": "accounts only get 100 it's a soft limit you can create a ticket you can increase",
    "start": "1959799",
    "end": "1965039"
  },
  {
    "text": "that limit but whatever you do you still are going to have some limit and once you reach that limit all the new Lambda",
    "start": "1965039",
    "end": "1971639"
  },
  {
    "text": "invocations in that account in that region will be throttled they will fail",
    "start": "1971639",
    "end": "1977519"
  },
  {
    "text": "so imagine a scenario you have a kesy stream let's say with 100 chards and then you set a paralyzation factor to 10",
    "start": "1977519",
    "end": "1984559"
  },
  {
    "text": "so this means that you have 10 * 100 1,000 Lambda invocations just constantly",
    "start": "1984559",
    "end": "1989799"
  },
  {
    "text": "reading from your stream and that's great I mean everything works perfectly but then some other Lambda somewhere",
    "start": "1989799",
    "end": "1995080"
  },
  {
    "text": "else in your account in your region probably one that does extremely important stuff that Lambda starts to",
    "start": "1995080",
    "end": "2000559"
  },
  {
    "text": "fail and this is because your stream consumer has run out of this concurrency",
    "start": "2000559",
    "end": "2006360"
  },
  {
    "text": "limit so this is a limit that can have a really big blast radius that goes Way",
    "start": "2006360",
    "end": "2011760"
  },
  {
    "text": "Beyond your own architecture so this it can cause this reple effect of cascading failures well outside of your own",
    "start": "2011760",
    "end": "2018600"
  },
  {
    "text": "architecture so we need to be very very careful about it but getting back to actually reading",
    "start": "2018600",
    "end": "2025200"
  },
  {
    "start": "2024000",
    "end": "2375000"
  },
  {
    "text": "data from the stream so what happens if there's a failure so once again there's good news",
    "start": "2025200",
    "end": "2031960"
  },
  {
    "text": "bad news situation the good news is that the Event Source mapping actually comes with a lot of handy features to handle",
    "start": "2031960",
    "end": "2038960"
  },
  {
    "text": "errors there's a lot of capabilities but to use them you need to know that they are there so the chances are very very",
    "start": "2038960",
    "end": "2046320"
  },
  {
    "text": "high just like with default timeouts that you are going with the defaults and",
    "start": "2046320",
    "end": "2053079"
  },
  {
    "text": "we know by now that defaults are really dangerous so what happens by default if",
    "start": "2053079",
    "end": "2059520"
  },
  {
    "text": "Lambda fails to process a batch of Records let's say there was a bad record some corrupt data you didn't Implement",
    "start": "2059520",
    "end": "2065839"
  },
  {
    "text": "proper error handling because well what can can go wrong and then eventually your Lambda fails because of that so",
    "start": "2065839",
    "end": "2071839"
  },
  {
    "text": "what happens after that although in this case no amount of retries will fix anything because we",
    "start": "2071839",
    "end": "2078599"
  },
  {
    "text": "just have a bad record from somewhere but by default Lambda will be retrying that one batch of Records over and over",
    "start": "2078599",
    "end": "2086240"
  },
  {
    "text": "and over and over again until it Le succeeds which obviously it never will or the data in the batch expires and the",
    "start": "2086240",
    "end": "2094599"
  },
  {
    "text": "data in batch expires in at least 24 hours so it's at least full day of repeated",
    "start": "2094599",
    "end": "2101880"
  },
  {
    "text": "useless Lambda invocations they are not free you are paying for them they are still consuming",
    "start": "2101880",
    "end": "2107760"
  },
  {
    "text": "your Lambda concurrency limit and they are doing absolutely nothing for",
    "start": "2107760",
    "end": "2112880"
  },
  {
    "text": "you and moreover all those useless invocations and useless retries will",
    "start": "2112880",
    "end": "2118040"
  },
  {
    "text": "probably cause multiple reprocessing of the same data because you see from",
    "start": "2118040",
    "end": "2123200"
  },
  {
    "text": "perspective of the Event Source mapping either the entire batch succeeds or the entire batch fails which means that even",
    "start": "2123200",
    "end": "2129960"
  },
  {
    "text": "if you already manage to process some of the records in that batch successfully you will be still reprocessing them over",
    "start": "2129960",
    "end": "2135839"
  },
  {
    "text": "and over and over again like records one two and three there they will be just reprocessed for 24 hours over and over",
    "start": "2135839",
    "end": "2142960"
  },
  {
    "text": "again and bad things don't stop there because while all of that insanity is",
    "start": "2142960",
    "end": "2148200"
  },
  {
    "text": "happening Lambda is not picking any new records from that chart so it continues",
    "start": "2148200",
    "end": "2154680"
  },
  {
    "text": "reading from other sharts there nothing ever happened but but that one particular Shard is stock it's just",
    "start": "2154680",
    "end": "2160920"
  },
  {
    "text": "waiting for that one batch to go through and this is actually a very kind of logical way of working because Kinesis",
    "start": "2160920",
    "end": "2168280"
  },
  {
    "text": "promises us ordering on the level of individual shards so if we think about it if Lambda would just jump over and",
    "start": "2168280",
    "end": "2174839"
  },
  {
    "text": "start processing other records and then maybe coming back to these failed records we would lose that order or",
    "start": "2174839",
    "end": "2180560"
  },
  {
    "text": "ordering guarantee but doesn't that doesn't make the situation much better",
    "start": "2180560",
    "end": "2186000"
  },
  {
    "text": "so now because of just one bad record we have an entire shart that is stuck with",
    "start": "2186000",
    "end": "2193000"
  },
  {
    "text": "absolutely nothing and that's why this kind of a bed record is often referred to as a poison peel record and another",
    "start": "2193000",
    "end": "2200000"
  },
  {
    "text": "problem with a poison peel is that especially in data applications data loses its value really really fast so",
    "start": "2200000",
    "end": "2207680"
  },
  {
    "text": "probably 24hour old data is going to be pretty much useless for you so it's not",
    "start": "2207680",
    "end": "2213640"
  },
  {
    "text": "only physically losing the data but also the value of the data disappears while you are stuck",
    "start": "2213640",
    "end": "2221319"
  },
  {
    "text": "retrying but okay 24 hours go by and finally the batch is discarded from the",
    "start": "2221319",
    "end": "2226680"
  },
  {
    "text": "stream some of the records stay unprocessed but that's life you should continue your living and at least lamb",
    "start": "2226680",
    "end": "2233160"
  },
  {
    "text": "can now pick those other records from The Shard but the problem here is that",
    "start": "2233160",
    "end": "2238599"
  },
  {
    "text": "probably your Shard is filled with records that were written around the same time as the expired ones so they",
    "start": "2238599",
    "end": "2244440"
  },
  {
    "text": "also expire around the same time so so the end result is that you might be just",
    "start": "2244440",
    "end": "2249839"
  },
  {
    "text": "losing the data without Lambda having a chance to process that data because there's just too much it doesn't have",
    "start": "2249839",
    "end": "2255920"
  },
  {
    "text": "time I compare it to this overflowing syn analogy you just don't have enough time to drain the",
    "start": "2255920",
    "end": "2261680"
  },
  {
    "text": "syn so we started with just one bad corrupted record and we ended up losing",
    "start": "2261680",
    "end": "2268680"
  },
  {
    "text": "a lot of valid and valuable data so one small problem intermittent problem and",
    "start": "2268680",
    "end": "2275200"
  },
  {
    "text": "we turned it to an complete system",
    "start": "2275200",
    "end": "2280040"
  },
  {
    "text": "outage we saw that in our story of course um just like with badly",
    "start": "2280640",
    "end": "2286839"
  },
  {
    "text": "configured timeouts and retries we just went with a default and ended up with complete system outage we were losing",
    "start": "2286839",
    "end": "2293280"
  },
  {
    "text": "data we were seeing duplicates we were seeing delays we were seeing costs",
    "start": "2293280",
    "end": "2298400"
  },
  {
    "text": "piling up and we were consuming extra resources of course but to no use there",
    "start": "2298400",
    "end": "2305000"
  },
  {
    "text": "was no outcome to that and well of course it happened because",
    "start": "2305000",
    "end": "2310440"
  },
  {
    "text": "we didn't know better and just went with the good old defaults when it comes to failure handling and once",
    "start": "2310440",
    "end": "2317119"
  },
  {
    "text": "again wonderful quote by Gregor we need to be very mindful about how we do the r",
    "start": "2317119",
    "end": "2322599"
  },
  {
    "text": "rise and luckily with Lambda and Event Source mapping there's many ways of how we can be smarter about our",
    "start": "2322599",
    "end": "2329720"
  },
  {
    "text": "retries and by now we know that probably the most important things are to set limits to set limits for timeouts to set",
    "start": "2329720",
    "end": "2336560"
  },
  {
    "text": "limits for retri Dev and we can do both of that with Event Source mapping but both of them are set to minus one by",
    "start": "2336560",
    "end": "2343319"
  },
  {
    "text": "default and minus one means you have no limits so it's your responsibility to",
    "start": "2343319",
    "end": "2349480"
  },
  {
    "text": "set that limits and there are also many other useful configurations that you can use with eventsource mapping to improve",
    "start": "2349480",
    "end": "2356000"
  },
  {
    "text": "eror handling I'm not going to go through them today but you need to know they are there you can use them in any",
    "start": "2356000",
    "end": "2362839"
  },
  {
    "text": "way you want in any config uh combination that you want but please please please please don't ever go what",
    "start": "2362839",
    "end": "2368640"
  },
  {
    "text": "a defaults and if you want to know more about all those a handling capabilities",
    "start": "2368640",
    "end": "2375319"
  },
  {
    "start": "2375000",
    "end": "2407000"
  },
  {
    "text": "or maybe you want to know more about Lambda processing Kinesis records or maybe Kinesis in general I've written",
    "start": "2375319",
    "end": "2382000"
  },
  {
    "text": "this uh user manuals as I lovingly call them very long blog post about the subject so please go ahead um check them",
    "start": "2382000",
    "end": "2389160"
  },
  {
    "text": "out if you're interested at all it goes in a lot of details about what I told",
    "start": "2389160",
    "end": "2394319"
  },
  {
    "text": "today and also what I didn't have time to talk about",
    "start": "2394319",
    "end": "2399318"
  },
  {
    "start": "2407000",
    "end": "2574000"
  },
  {
    "text": "all right so we have seen today that sometimes we can actually cause more problems while trying to fix them and",
    "start": "2408839",
    "end": "2415119"
  },
  {
    "text": "this is particularly true if we don't make conscious critical informed decisions about handling",
    "start": "2415119",
    "end": "2421720"
  },
  {
    "text": "failers and every such decision will naturally come with tradeoffs or things like timeouts and retries they can be",
    "start": "2421720",
    "end": "2427680"
  },
  {
    "text": "extremely powerful but if we just let the matter slide if we just go with the defaults we can turn them from making",
    "start": "2427680",
    "end": "2435760"
  },
  {
    "text": "our architecture more resilient which is our end goal into doing the complete",
    "start": "2435760",
    "end": "2440960"
  },
  {
    "text": "opposite so next time you are building a distributed application I encourage you",
    "start": "2440960",
    "end": "2446960"
  },
  {
    "text": "to be brave to face the messy reality to take control into your own hands rather",
    "start": "2446960",
    "end": "2452280"
  },
  {
    "text": "than believing in Magic because there's no magic but that's a good thing we have",
    "start": "2452280",
    "end": "2458760"
  },
  {
    "text": "control over things and distributed systems and applications are very powerful but they",
    "start": "2458760",
    "end": "2465760"
  },
  {
    "text": "are also complex which doesn't make them neither inherently good nor bad and the",
    "start": "2465760",
    "end": "2470920"
  },
  {
    "text": "cloud especially servoless abstracts away a lot of that complexity from us",
    "start": "2470920",
    "end": "2476359"
  },
  {
    "text": "which again doesn't mean that the complex complexity is not there and it doesn't make them neither inherently",
    "start": "2476359",
    "end": "2481760"
  },
  {
    "text": "good nor bad and we don't know don't need to know every single detail about every single",
    "start": "2481760",
    "end": "2488560"
  },
  {
    "text": "service that we are using it's w online impossible but there are fundamental",
    "start": "2488560",
    "end": "2493920"
  },
  {
    "text": "things that are inherent to distributed systems and the clouds in general things",
    "start": "2493920",
    "end": "2498960"
  },
  {
    "text": "like service limits timeouts retries partial failures backoffs and those are",
    "start": "2498960",
    "end": "2506079"
  },
  {
    "text": "all fundamental things that we absolutely need to be aware of when we are building our distributed",
    "start": "2506079",
    "end": "2511880"
  },
  {
    "text": "applications and without that we are just moving in the dark with our eyes closed and just hoping that everything's",
    "start": "2511880",
    "end": "2518680"
  },
  {
    "text": "going to be fine it's not and finally on a more philosophical",
    "start": "2518680",
    "end": "2525359"
  },
  {
    "text": "note distributed systems and architectures are hard but they can also teach us this very valuable skill of",
    "start": "2525359",
    "end": "2533160"
  },
  {
    "text": "embracing the chaos of the real world because every failure can be an opportunity to make our architecture",
    "start": "2533160",
    "end": "2539240"
  },
  {
    "text": "better to make it even more resilient and though it is impossible to build something that never fails there's one",
    "start": "2539240",
    "end": "2546680"
  },
  {
    "text": "thing that we can do we can learn and grow from each individual failure and as Dr ber wus likes to say",
    "start": "2546680",
    "end": "2554599"
  },
  {
    "text": "everything fails all the time and that's just the reality of things so either in",
    "start": "2554599",
    "end": "2559760"
  },
  {
    "text": "life in general or with AWS services in particular or with Eda I'd argue that",
    "start": "2559760",
    "end": "2566040"
  },
  {
    "text": "the best thing that we can do is be prepared and stay calm when those failures happen because they",
    "start": "2566040",
    "end": "2573040"
  },
  {
    "text": "will and that's it for me for today thank you so very much for listening thank you very much",
    "start": "2573040",
    "end": "2580720"
  }
]