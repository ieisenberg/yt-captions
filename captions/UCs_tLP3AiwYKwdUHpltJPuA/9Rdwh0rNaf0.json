[
  {
    "start": "0",
    "end": "83000"
  },
  {
    "text": "thanks for coming I hope you're enjoying the conference I am Gabor Sash and I'm here to talk about the duck DB system a",
    "start": "12080",
    "end": "19039"
  },
  {
    "text": "little bit about my background I did a PhD back in Hungary in something called the Ft srg default tolerance systems",
    "start": "19039",
    "end": "26119"
  },
  {
    "text": "research group and we focused on critical systems and models of critical systems which eventually drove me to",
    "start": "26119",
    "end": "33239"
  },
  {
    "text": "work on graph databases where you can store these models once I finished my PhD I moved here to Amsterdam I worked",
    "start": "33239",
    "end": "39920"
  },
  {
    "text": "at CWI the Dutch national Research Center for computer science where python was invented and D algorithm was",
    "start": "39920",
    "end": "46800"
  },
  {
    "text": "invented and there I worked on graph databases graph analytics and database benchmarks and while I was doing this in",
    "start": "46800",
    "end": "54320"
  },
  {
    "text": "the offices near me Mark Rasel and hes mizen invented something called duck DB",
    "start": "54320",
    "end": "59640"
  },
  {
    "text": "which was an open source project that spun off into a Labs company which is",
    "start": "59640",
    "end": "65760"
  },
  {
    "text": "duck DB Labs which is where I followed them in 2023 and I worked there as a",
    "start": "65760",
    "end": "70840"
  },
  {
    "text": "developer relations Advocate and in this talk I would like to give you an overview of duck DB and also some of the",
    "start": "70840",
    "end": "76960"
  },
  {
    "text": "internals that will help you understand why it's fast and how you can make your workloads fast in duct DB so what is",
    "start": "76960",
    "end": "84000"
  },
  {
    "start": "83000",
    "end": "168000"
  },
  {
    "text": "duck DB in a sentence duck DB is an analytical database management system that's designed to process access 100 GB",
    "start": "84000",
    "end": "91520"
  },
  {
    "text": "Plus data sets on end user devices such as laptops and the main observation here",
    "start": "91520",
    "end": "97119"
  },
  {
    "text": "is that in the last decade vendors have been coming out with excellent laptops you can now go to a store and buy a new",
    "start": "97119",
    "end": "104439"
  },
  {
    "text": "laptop with 8 12 or 16 cores quite a lot of memory and a very fast nvme disc and",
    "start": "104439",
    "end": "111399"
  },
  {
    "text": "this is true for both Apple laptops and laptops by Microsoft Lenovo Dell you name it these laptops are actually",
    "start": "111399",
    "end": "118640"
  },
  {
    "text": "developing faster in terms of speed than cloud machines and they are definitely better in terms of price performance",
    "start": "118640",
    "end": "125439"
  },
  {
    "text": "ratio if you browse the cloud prices the cloud didn't really get much cheaper in",
    "start": "125439",
    "end": "130879"
  },
  {
    "text": "the last decade while laptops have gotten at the same price a lot faster so there is a lot of performance that you",
    "start": "130879",
    "end": "137560"
  },
  {
    "text": "can extract from a laptop if you program it correctly ddb's key values are that it's",
    "start": "137560",
    "end": "144000"
  },
  {
    "text": "simple to use it's free to use because it's open source it's fast and it's feature Rich and I think it's best to",
    "start": "144000",
    "end": "151360"
  },
  {
    "text": "demo this with a data set so there is this app in the Netherlands which is",
    "start": "151360",
    "end": "156560"
  },
  {
    "text": "about train information and they release a bunch of very high quality open data",
    "start": "156560",
    "end": "162440"
  },
  {
    "text": "sets these include train Services stations and distances so I made a little demo with",
    "start": "162440",
    "end": "169599"
  },
  {
    "start": "168000",
    "end": "309000"
  },
  {
    "text": "this and the motivation of the demo is that I have moved here in 2020 and ever",
    "start": "169599",
    "end": "174640"
  },
  {
    "text": "since then everyone I know has been complaining about DNS everyone is saying well the trains are very expensive and",
    "start": "174640",
    "end": "181720"
  },
  {
    "text": "they're getting worse by the year so I grabbed five years of data sets this is",
    "start": "181720",
    "end": "186920"
  },
  {
    "text": "between January 2019 and April 2024 this is about 15 gigabyt of CSV files and I",
    "start": "186920",
    "end": "194760"
  },
  {
    "text": "just started the loading process the loading command is very simple we say",
    "start": "194760",
    "end": "200360"
  },
  {
    "text": "create table services from the services CSV file that's it and then we return",
    "start": "200360",
    "end": "205680"
  },
  {
    "text": "the number of rows that we have just loaded into the system and you can see that on this very laptop which is kind",
    "start": "205680",
    "end": "212640"
  },
  {
    "text": "of a off-the-shelf MacBook Pro we loaded 15 GB of CSV files into duck DB's own",
    "start": "212640",
    "end": "219319"
  },
  {
    "text": "format in 11 seconds so at more than 1 Gigabyte per second and if you want to",
    "start": "219319",
    "end": "224920"
  },
  {
    "text": "ask did the trains get any slower over the years then we can just say well",
    "start": "224920",
    "end": "230280"
  },
  {
    "text": "let's extract the Year from the service date where the train was running and",
    "start": "230280",
    "end": "235640"
  },
  {
    "text": "let's do an average of the arrival delay which is encoded in minut so I will just convert it to seconds so that it's",
    "start": "235640",
    "end": "242560"
  },
  {
    "text": "easier to read and that's it it's a couple of lines and this rounds on 11 15 million rows in 0.2 seconds so we got a",
    "start": "242560",
    "end": "251319"
  },
  {
    "text": "bunch of numbers and you can kind of see that yeah the delay is increasing but it would be nicer to actually visualize",
    "start": "251319",
    "end": "258320"
  },
  {
    "text": "this now in many databases it would mean you have to return it in some sort of a CSV format read it into pandas and then",
    "start": "258320",
    "end": "265720"
  },
  {
    "text": "do a visualization not with duck DB though with duck DB we we can just root",
    "start": "265720",
    "end": "271199"
  },
  {
    "text": "the result of a query into a data frame well it took .1 seconds and then",
    "start": "271199",
    "end": "277759"
  },
  {
    "text": "we can use this data frame in a python code we can say plot bar and there we",
    "start": "277759",
    "end": "284400"
  },
  {
    "text": "have it it's uh all the complaints actually have some ground truth ever",
    "start": "284400",
    "end": "290639"
  },
  {
    "text": "since 2020 where trains were running quite on schedule because there were",
    "start": "290639",
    "end": "296039"
  },
  {
    "text": "very few of them the delays have been increasing steadily and this year Ser is on track to be one of the worst so what",
    "start": "296039",
    "end": "303880"
  },
  {
    "text": "did we just do or mainly what did we not do in this demo but first because ddb is",
    "start": "303880",
    "end": "309919"
  },
  {
    "start": "309000",
    "end": "408000"
  },
  {
    "text": "simple we did not have to fiddle with any registration I'm sure all of you have used cloud services where you have",
    "start": "309919",
    "end": "316400"
  },
  {
    "text": "to register account or you have to start a server then you have to configure your client with an IP address maybe fiddle",
    "start": "316400",
    "end": "322880"
  },
  {
    "text": "with a firewall set up some ports we didn't have to do any of that duck DB just ran with Import ddb in python we",
    "start": "322880",
    "end": "331280"
  },
  {
    "text": "also did not enter any credit card details we didn't have to register for a trial and we didn't have to do a quota",
    "start": "331280",
    "end": "337960"
  },
  {
    "text": "increase which is one of the worst things when you use the cloud and just want to get your analysis done what we",
    "start": "337960",
    "end": "344400"
  },
  {
    "text": "did do is that we analyze 15 gabyt of data in a matter of seconds of course it",
    "start": "344400",
    "end": "350000"
  },
  {
    "text": "could be more data I actually took this data set and boosted it from 5 years to 200 years of Ray Services which resulted",
    "start": "350000",
    "end": "357759"
  },
  {
    "text": "in 5.4 billion rows of CSV files and what I noticed is that on this laptop",
    "start": "357759",
    "end": "365080"
  },
  {
    "text": "you can still work with 600 GB of CSV files granted this is nowhere near as",
    "start": "365080",
    "end": "370720"
  },
  {
    "text": "brisk as the demo so you need a medium length coffee break of 16 minutes to load it but once you did that the",
    "start": "370720",
    "end": "377639"
  },
  {
    "text": "aggregation is still reasonably Snappy you can wait 22 seconds and this is still a lot faster than moving all this",
    "start": "377639",
    "end": "383000"
  },
  {
    "text": "data to the cloud setting up a computer and then waiting for the computation to finish and the final feature I talked",
    "start": "383000",
    "end": "390639"
  },
  {
    "text": "about is being feature Rich and this means that duck DB has full SQL support",
    "start": "390639",
    "end": "396599"
  },
  {
    "text": "and it doesn't mean joins aggregations and filtering only we actually do many",
    "start": "396599",
    "end": "401680"
  },
  {
    "text": "of the advanced SQL features plus a bunch of sequel extensions and to show",
    "start": "401680",
    "end": "407840"
  },
  {
    "text": "these I go back to the demo where I have a couple of queries prepared for you so",
    "start": "407840",
    "end": "413440"
  },
  {
    "start": "408000",
    "end": "669000"
  },
  {
    "text": "one of the questions that I had when I started to look at the data set is which are the busiest stations in the",
    "start": "413440",
    "end": "419720"
  },
  {
    "text": "Netherlands anyone has a guess which is the busiest station that's mix of UT and Amsterdam",
    "start": "419720",
    "end": "427199"
  },
  {
    "text": "so uh let's see the data we can do high level statistics of services per month",
    "start": "427199",
    "end": "433879"
  },
  {
    "text": "in uh the data set but then SQL has a bit of a difficult thing when it comes",
    "start": "433879",
    "end": "441080"
  },
  {
    "text": "to top three you can do top one but to do top three you have to do something called a window function a window",
    "start": "441080",
    "end": "447240"
  },
  {
    "text": "function has this awkward syntax where you go go through something using the over keyword you partition the data",
    "start": "447240",
    "end": "454240"
  },
  {
    "text": "based on month ordering by the number of R services and then you rank them using",
    "start": "454240",
    "end": "460120"
  },
  {
    "text": "this function this is something I can never remember from the top of my head but this is standard SQL and if we add",
    "start": "460120",
    "end": "467159"
  },
  {
    "text": "the rest of the query which is constructing the month name and then filtering for the summer month we can",
    "start": "467159",
    "end": "474440"
  },
  {
    "text": "indeed confirm that utrax Central is leading the big business competition for",
    "start": "474440",
    "end": "480840"
  },
  {
    "text": "every summer month followed by sometimes Amsterdam Central and sometimes by the airport so this is again something that",
    "start": "480840",
    "end": "489400"
  },
  {
    "text": "barely took a few milliseconds and while this is not a particular in ice query this is standard SQL and it's portable",
    "start": "489400",
    "end": "496199"
  },
  {
    "text": "between SQL systems now a couple of important operations when dealing with data sets",
    "start": "496199",
    "end": "502520"
  },
  {
    "text": "and data frames are pivot and unpivot pivot means we turn a long table into a",
    "start": "502520",
    "end": "508120"
  },
  {
    "text": "v table so for example example this table that we just had we would like to turn it into a way where we have the",
    "start": "508120",
    "end": "515080"
  },
  {
    "text": "train station names as column names and then the traffic as the column values",
    "start": "515080",
    "end": "520719"
  },
  {
    "text": "and luckily docdb has a very simple Syntax for it we first filter for a long",
    "start": "520719",
    "end": "527040"
  },
  {
    "text": "table to get these values and then we can just say let's pivot on the station",
    "start": "527040",
    "end": "532519"
  },
  {
    "text": "and create the number of services as a summary statistic and that's it again",
    "start": "532519",
    "end": "538839"
  },
  {
    "text": "took minimal time and a very simple concise syntax sometimes we want to unpivot and",
    "start": "538839",
    "end": "544760"
  },
  {
    "text": "for this I actually used another data set so we have this data set called the tariffs which encode the tariffs and the",
    "start": "544760",
    "end": "552560"
  },
  {
    "text": "distances between 400 train stations in the Netherlands you can see that this is",
    "start": "552560",
    "end": "557920"
  },
  {
    "text": "kind of a weird table because it encodes a symmetric Matrix so it has Nan values",
    "start": "557920",
    "end": "564880"
  },
  {
    "text": "all across the diagonal of the Matrix and then it has the distance between every pair of train stations in the",
    "start": "564880",
    "end": "571200"
  },
  {
    "text": "country now this is really unwieldy to work with in SQL but we can use",
    "start": "571200",
    "end": "577079"
  },
  {
    "text": "something called unpivot to turn this into a long table now to do this in",
    "start": "577079",
    "end": "583519"
  },
  {
    "text": "traditional SQL systems we would have to spell out all the column names AC ah ahp",
    "start": "583519",
    "end": "589600"
  },
  {
    "text": "all to zmo which is really inconvenient but luckily dgdb has something called",
    "start": "589600",
    "end": "595519"
  },
  {
    "text": "columns star and column star will list all of the column names we can then",
    "start": "595519",
    "end": "600839"
  },
  {
    "text": "exclude the station column the First Column and the rest is just trivial to write so once we on this we have this",
    "start": "600839",
    "end": "608640"
  },
  {
    "text": "very convenient table saying from this station this other station is this many",
    "start": "608640",
    "end": "614320"
  },
  {
    "text": "kilometers away and if we want to find something like the largest distance",
    "start": "614320",
    "end": "620160"
  },
  {
    "text": "between two stations we can just say well let's do joins let's make sure that",
    "start": "620160",
    "end": "627079"
  },
  {
    "text": "it's running from train station into another NE lands to another train station in the Netherlands we break the",
    "start": "627079",
    "end": "632519"
  },
  {
    "text": "Symmetry with this filter and we can retrieve as Haven to fingan which means",
    "start": "632519",
    "end": "638959"
  },
  {
    "text": "we can take a 426 kilom train route using just trains in the Netherlands I",
    "start": "638959",
    "end": "644920"
  },
  {
    "text": "actually confirm this with Google Maps so if you are a train geek and looking for a weon trip this is the longest that",
    "start": "644920",
    "end": "652200"
  },
  {
    "text": "you can do without using any bus in the country so these are a couple of",
    "start": "652200",
    "end": "657680"
  },
  {
    "text": "advanced CQ features but ddb has more interesting characteristics that allow",
    "start": "657680",
    "end": "663120"
  },
  {
    "text": "this performance and this usability the first one is the duck DB is in process",
    "start": "663120",
    "end": "669880"
  },
  {
    "start": "669000",
    "end": "824000"
  },
  {
    "text": "this is quite a fundamental shift because it was very widely acknowledged",
    "start": "669880",
    "end": "675639"
  },
  {
    "text": "back in the data warehousing and data analytics word that the client server architecture is necessary for data",
    "start": "675639",
    "end": "682880"
  },
  {
    "text": "analytics and the reason for this is that the data analytics itself had to",
    "start": "682880",
    "end": "688079"
  },
  {
    "text": "run on some Big Iron it had to run on a big server that could be in the on",
    "start": "688079",
    "end": "693880"
  },
  {
    "text": "premises cluster of the company or in the cloud but it had to be something that's a big machine and there are many",
    "start": "693880",
    "end": "700399"
  },
  {
    "text": "problems with this one is that the big machine is going to be expensive two the",
    "start": "700399",
    "end": "705920"
  },
  {
    "text": "software running on the big machine is probably going to come with an expensive license someone has to set it up someone",
    "start": "705920",
    "end": "712839"
  },
  {
    "text": "has to operate and even once we are done with all those you have to deal with these arrows in the middle you have to",
    "start": "712839",
    "end": "720200"
  },
  {
    "text": "make sure that your client can connect to the server the client will then communicate over a slow Network and",
    "start": "720200",
    "end": "728200"
  },
  {
    "text": "potentially over a slow database protocol so this is all quite unwieldy",
    "start": "728200",
    "end": "735720"
  },
  {
    "text": "and dgdb said well the laptop on the left shouldn't be just a glorified terminal it should be something that",
    "start": "735720",
    "end": "741760"
  },
  {
    "text": "actually does the data processing so we took the database and we moved it into",
    "start": "741760",
    "end": "747000"
  },
  {
    "text": "the client process that's running on the data on the laptop and serverless has a bad name as",
    "start": "747000",
    "end": "754000"
  },
  {
    "text": "a term because people say yeah AWS Lambda is serverless while there is",
    "start": "754000",
    "end": "759279"
  },
  {
    "text": "definitely a server doing some work in the background well ddb is truly serverless we remove the server and",
    "start": "759279",
    "end": "765639"
  },
  {
    "text": "there is no more server but what we do have is we have a",
    "start": "765639",
    "end": "770920"
  },
  {
    "text": "single file database format that makes it very easy to work with duck DB and then stop working maybe pass your",
    "start": "770920",
    "end": "777680"
  },
  {
    "text": "partial work Al belong to a colleague you can just attach it to an email upload it to an S3 bucket and this makes",
    "start": "777680",
    "end": "784440"
  },
  {
    "text": "it very convenient to use duck DB if you are familiar with other",
    "start": "784440",
    "end": "789480"
  },
  {
    "text": "database systems this quadrant image is very helpful for categorizing duck DB",
    "start": "789480",
    "end": "795519"
  },
  {
    "text": "the impress deployment model itself is not really novel you probably know sqlite is the most deployed database on",
    "start": "795519",
    "end": "802880"
  },
  {
    "text": "the word and sqlite is in process but it targets a transactional workload where you add a couple of rows query for a",
    "start": "802880",
    "end": "810079"
  },
  {
    "text": "couple of rows and then maybe delete a couple of rows but du DB is more in line with bulky analytical processing",
    "start": "810079",
    "end": "817079"
  },
  {
    "text": "similarly to data warehouses like snowflake vertica teradata data brakes",
    "start": "817079",
    "end": "822480"
  },
  {
    "text": "and so on so what is this internal working how",
    "start": "822480",
    "end": "827800"
  },
  {
    "start": "824000",
    "end": "1161000"
  },
  {
    "text": "do these Data Warehouse Systems work well it all starts with the storage the",
    "start": "827800",
    "end": "833160"
  },
  {
    "text": "transactional systems sqlite MySQL and postgress they use something called a row based storage so if we use the raway",
    "start": "833160",
    "end": "840680"
  },
  {
    "text": "data set where we have a date an ID a type and a train station then row BAS",
    "start": "840680",
    "end": "847560"
  },
  {
    "text": "system would group these together a given date a given ID a given type and a given station and store those together",
    "start": "847560",
    "end": "854360"
  },
  {
    "text": "on disk and this is great because it allows for easy updates and easy queries for a given row but the column based",
    "start": "854360",
    "end": "861680"
  },
  {
    "text": "system stores it by column which means that you can compress really nicely on a data that is of the same type in",
    "start": "861680",
    "end": "869800"
  },
  {
    "text": "consequence and you can store these on disk together physically and it also allows you to just throw away columns so",
    "start": "869800",
    "end": "876959"
  },
  {
    "text": "if you need to do some statistics based on date and station which we already did today then the ID and the type columns",
    "start": "876959",
    "end": "884480"
  },
  {
    "text": "do not have to be read from dis so there are lots of savings to be done there the next step is the",
    "start": "884480",
    "end": "892079"
  },
  {
    "text": "execution traditionally systems did something called a tle at a time execution this is very easy on memory",
    "start": "892079",
    "end": "900399"
  },
  {
    "text": "because you just have to get one tle in the memory process it and then the next tle the next tle and then the next apple",
    "start": "900399",
    "end": "908120"
  },
  {
    "text": "but this is more aching to working like writing a line of program and then",
    "start": "908120",
    "end": "913160"
  },
  {
    "text": "changing to your email client writing another line of program writing another email we know this is not the best of",
    "start": "913160",
    "end": "919360"
  },
  {
    "text": "office productivity what's better is to do all the programming and then all your emails and this is column as a Time",
    "start": "919360",
    "end": "926040"
  },
  {
    "text": "execution and this works really well for analytics because you can deal with this",
    "start": "926040",
    "end": "931839"
  },
  {
    "text": "one column in one big go the problem with column at a time is that you can run out of memory if your columns have a",
    "start": "931839",
    "end": "939800"
  },
  {
    "text": "billion or more rows your system is likely to crash and this is one of the reasons why systems like pandas often",
    "start": "939800",
    "end": "946120"
  },
  {
    "text": "run out of memory pandas is column at a time and you can run out of memory now",
    "start": "946120",
    "end": "951839"
  },
  {
    "text": "there must be some better interim gold Delux approach and this is called vectorized execution vectorized",
    "start": "951839",
    "end": "958360"
  },
  {
    "text": "execution means that you process the data in vectors where the vector is maybe",
    "start": "958360",
    "end": "963800"
  },
  {
    "text": "248 items and this allows you to do some level of bulky processing and at the",
    "start": "963800",
    "end": "970600"
  },
  {
    "text": "same time not run out of memory and it's even better than not running out of memory because as we will see in the",
    "start": "970600",
    "end": "976959"
  },
  {
    "text": "next slide these vectors are selected to the size where they actually fit into the L1 cache of the CPU so what we see",
    "start": "976959",
    "end": "985279"
  },
  {
    "text": "here is eight rows they are split into two row groups 1 to four and then 5 to8",
    "start": "985279",
    "end": "991519"
  },
  {
    "text": "and we split them between the CPU threads based on the row group so we first send these to the two CPU threads",
    "start": "991519",
    "end": "998880"
  },
  {
    "text": "and then we start doing the processing based on the vectors we then process we first process the first pair of vectors",
    "start": "998880",
    "end": "1007120"
  },
  {
    "text": "and then we process the second pair of vectors and because modern CPUs have L1",
    "start": "1007120",
    "end": "1012440"
  },
  {
    "text": "caches around 32 to 128 kiloby per core this 2014 item in the vector and related",
    "start": "1012440",
    "end": "1021959"
  },
  {
    "text": "columns most of the time fit into the L1 cache which is much much faster even compared to main memory of course this",
    "start": "1021959",
    "end": "1029839"
  },
  {
    "text": "sounds very neat and you may ask why didn't everyone do this before I mean this is how you should get the most",
    "start": "1029839",
    "end": "1036038"
  },
  {
    "text": "performance out of your system well the problem was that this is really difficult to Port because you have an",
    "start": "1036039",
    "end": "1041959"
  },
  {
    "text": "Intel CPU with ssc2 you have a n xon with AVX 512 and then you have an arm",
    "start": "1041959",
    "end": "1047918"
  },
  {
    "text": "CPU and would have to manually program all of them spe uh particularly with",
    "start": "1047919",
    "end": "1054000"
  },
  {
    "text": "simd single instruction multiple data set instructions and this makes portability really difficult but luckily",
    "start": "1054000",
    "end": "1061960"
  },
  {
    "text": "over the last 15 years or so the main compilers GCC and clang have started to",
    "start": "1061960",
    "end": "1068760"
  },
  {
    "text": "autov vectorize code so if you write code in a way where you use tight for Loops there are no branches in the code",
    "start": "1068760",
    "end": "1075440"
  },
  {
    "text": "and even when there are they are made out of the for Loops the CPU the",
    "start": "1075440",
    "end": "1081120"
  },
  {
    "text": "compiler will be able to create code that is using SD instructions and that",
    "start": "1081120",
    "end": "1087159"
  },
  {
    "text": "will allow you to get the best performance of the CPU while maintaining portability and of course things are not",
    "start": "1087159",
    "end": "1094120"
  },
  {
    "text": "as simple as on this slide sometimes you have to build some hash tables do some",
    "start": "1094120",
    "end": "1099400"
  },
  {
    "text": "larger state in memory but this is the key idea that allows ddb to be fast the other idea that works really",
    "start": "1099400",
    "end": "1106720"
  },
  {
    "text": "well in Duck DB and it's probably not talked enough talked about enough is zone maps so duck DB creates zone maps",
    "start": "1106720",
    "end": "1115080"
  },
  {
    "text": "also known as minmax indexes for each column in each row group and this works",
    "start": "1115080",
    "end": "1120679"
  },
  {
    "text": "really well if the column is ordered or almost completely ordered for example you can see that for date for the first",
    "start": "1120679",
    "end": "1127679"
  },
  {
    "text": "row group we here created min max May 30 and May 31st and in row group two we",
    "start": "1127679",
    "end": "1134320"
  },
  {
    "text": "created 1st of June and second of June so if someone says I only want summer month then duck DB can immediately just",
    "start": "1134320",
    "end": "1141919"
  },
  {
    "text": "throw away row group F because there cannot be a potential match in row group",
    "start": "1141919",
    "end": "1147120"
  },
  {
    "text": "form this also works well for IDs and it doesn't really work for Strings so for",
    "start": "1147120",
    "end": "1152200"
  },
  {
    "text": "the type and the station columns this doesn't work that well but in many of the cases zone maps is all that you need",
    "start": "1152200",
    "end": "1159520"
  },
  {
    "text": "for indexing so I talked about portability between different architectures but",
    "start": "1159520",
    "end": "1166559"
  },
  {
    "start": "1161000",
    "end": "1256000"
  },
  {
    "text": "having autov vectorization in the code is not not sufficient what you really need is a code base that's written in a",
    "start": "1166559",
    "end": "1173159"
  },
  {
    "text": "single language we selected C++ 11 because it has a compiler available on most platforms but we also had to be",
    "start": "1173159",
    "end": "1179480"
  },
  {
    "text": "very stringent on not using any external dependencies dependencies are notoriously difficult to port and we",
    "start": "1179480",
    "end": "1186159"
  },
  {
    "text": "cannot just rely on some sort of regx library or an SSH Library being on the",
    "start": "1186159",
    "end": "1191960"
  },
  {
    "text": "machine of the user however we also cannot rely on completely redeveloping RX Li or SSH for",
    "start": "1191960",
    "end": "1199960"
  },
  {
    "text": "that matter that's just a recipe for disaster so we have a bunch of inline dependencies that are part of the code",
    "start": "1199960",
    "end": "1206360"
  },
  {
    "text": "base we vender those dependencies into the code base it's still C++ 11 in its",
    "start": "1206360",
    "end": "1212520"
  },
  {
    "text": "entirety and this means the DU DB is very portable it doesn't have any dependencies to hold it down and thanks",
    "start": "1212520",
    "end": "1220640"
  },
  {
    "text": "to this we have clients for many platforms and many programming languages we have a python R node.js Java rust go",
    "start": "1220640",
    "end": "1229080"
  },
  {
    "text": "and many different clients also a standalone CLI application and our Crown Jewel which is Du DB in the browser on",
    "start": "1229080",
    "end": "1237159"
  },
  {
    "text": "the left you can see it running on a desktop computer and on the right you can see it running on a phone so DD is",
    "start": "1237159",
    "end": "1243280"
  },
  {
    "text": "so portable that in your phone in your browser there is a web assembly client and du DB runs the webshell in your",
    "start": "1243280",
    "end": "1251159"
  },
  {
    "text": "phone and works reasonably well for mediumsized data sets so let's talk about ergonomics",
    "start": "1251159",
    "end": "1258200"
  },
  {
    "start": "1256000",
    "end": "1488000"
  },
  {
    "text": "because this this is something databases have not always excelled in the past well duck DB supports many formats and",
    "start": "1258200",
    "end": "1265960"
  },
  {
    "text": "protocols so it can load CSV par and Json as you would expect but also now",
    "start": "1265960",
    "end": "1272240"
  },
  {
    "text": "the data Lake formats like Delta and Iceberg and it supports the https",
    "start": "1272240",
    "end": "1277919"
  },
  {
    "text": "protocol the AWS S3 protocol and the aour blob storage and these work in",
    "start": "1277919",
    "end": "1284799"
  },
  {
    "text": "tandem together so for example if you do something like a request for a PAR file",
    "start": "1284799",
    "end": "1291679"
  },
  {
    "text": "that's sitting in an httpfs endpoint then duck DB will not fetch the entire",
    "start": "1291679",
    "end": "1297799"
  },
  {
    "text": "PAR file load it over the Vier and then do the query instead if we read the metadata of the PAR file it will say oh",
    "start": "1297799",
    "end": "1304960"
  },
  {
    "text": "I only want summer month and I only want the date and I only want the station name and then it will fetch those",
    "start": "1304960",
    "end": "1312279"
  },
  {
    "text": "columns using HTTP range requests and this can potentially save a lot of traffic I wrote a block post about it",
    "start": "1312279",
    "end": "1319840"
  },
  {
    "text": "and it turns out that if you query for summer month and a couple of columns it only does about",
    "start": "1319840",
    "end": "1326440"
  },
  {
    "text": "6% of the data set size as Network traffic so you can really cut down on",
    "start": "1326440",
    "end": "1332000"
  },
  {
    "text": "transfer costs and egress costs if you're using the cloud continuing with the protocols duck",
    "start": "1332000",
    "end": "1338679"
  },
  {
    "text": "DB can directly connect to the transactional databases so we understand that a lot of data is in mysq postgress",
    "start": "1338679",
    "end": "1345880"
  },
  {
    "text": "and sqlite and while you can export a CSV file from these systems it is not",
    "start": "1345880",
    "end": "1352039"
  },
  {
    "text": "the easiest thing to do and to parse it back it's much easier to just connect directly to these systems and query",
    "start": "1352039",
    "end": "1358240"
  },
  {
    "text": "their tables and finally a unique feature of duck DB is that it integrates",
    "start": "1358240",
    "end": "1363360"
  },
  {
    "text": "with things like the pond library numpy and D player in R and we can do this",
    "start": "1363360",
    "end": "1369279"
  },
  {
    "text": "specifically because of the impress setup of du DB here's a small example where we import duck DB and import",
    "start": "1369279",
    "end": "1375960"
  },
  {
    "text": "pandas and what we do is that we Define something called mycore DF which is that",
    "start": "1375960",
    "end": "1382520"
  },
  {
    "text": "just a pandas data frame with two numbers and then we run a du DB query and what's happening here is that dgdb",
    "start": "1382520",
    "end": "1388919"
  },
  {
    "text": "is querying from using the SQL from close the my DF table but this table",
    "start": "1388919",
    "end": "1394799"
  },
  {
    "text": "doesn't exist so duck DB says well let's try to do a replacement it's called a replacement scan it's looking around in",
    "start": "1394799",
    "end": "1402200"
  },
  {
    "text": "the python environment and then it finds that there is something called mycore DF",
    "start": "1402200",
    "end": "1407320"
  },
  {
    "text": "and then it reads that Punda data frame and there are a couple of important things here first that this read is",
    "start": "1407320",
    "end": "1413640"
  },
  {
    "text": "happening without creating a copy in Duck DB in the Big Data word it's in general bad idea to copy things between",
    "start": "1413640",
    "end": "1421559"
  },
  {
    "text": "systems because it's just a lot of performance loss for no good reason so we do a zero copy access but duck DB is",
    "start": "1421559",
    "end": "1429720"
  },
  {
    "text": "parallel so duck DB is essentially reading pandas data frames faster than pandas itself and once you're done with",
    "start": "1429720",
    "end": "1437360"
  },
  {
    "text": "the computation and as you have already seen in the demo you can just take the result that du DB gave and turn it into",
    "start": "1437360",
    "end": "1444279"
  },
  {
    "text": "a pandas data frame and we made this integration specifically to make it easier to introduce duck DB in a pandas",
    "start": "1444279",
    "end": "1450919"
  },
  {
    "text": "code base we understand that there are large pandas code bases most of them work perfectly fine but sometimes there",
    "start": "1450919",
    "end": "1457000"
  },
  {
    "text": "is heavy hitting operation and maybe you just want to do that one operation with duck DB and that is perfectly good and",
    "start": "1457000",
    "end": "1464039"
  },
  {
    "text": "that's a valid use case for duck DB if you want to use duck DB there are now lots of IDs supporting it daver is a",
    "start": "1464039",
    "end": "1471039"
  },
  {
    "text": "classic system there are new systems like Q Studio or harleen terminal user interface and it works really well in",
    "start": "1471039",
    "end": "1478279"
  },
  {
    "text": "Jupiter notebooks there is jupy SQL support and you can return tables you",
    "start": "1478279",
    "end": "1483320"
  },
  {
    "text": "can integrate with pandas as you havec so this is all very ergonomic what's the state of duck DB",
    "start": "1483320",
    "end": "1490240"
  },
  {
    "start": "1488000",
    "end": "1558000"
  },
  {
    "text": "well June has been a busy month for us uh we had this upsh short sling at the",
    "start": "1490240",
    "end": "1496080"
  },
  {
    "text": "end of the star diagram in our adoption so we have about 19k GitHub Stars 30k",
    "start": "1496080",
    "end": "1503159"
  },
  {
    "text": "followers on LinkedIn and Twitter we have now more than million visits per month and 4 million plus piie in",
    "start": "1503159",
    "end": "1510520"
  },
  {
    "text": "stores and at the beginning of the month we have just released something called snow duck this is an homage to the apple",
    "start": "1510520",
    "end": "1519039"
  },
  {
    "text": "snow leopard system where they released a Mac OS version with the slogan that",
    "start": "1519039",
    "end": "1524080"
  },
  {
    "text": "the feature of the system the new features is that there are no new features so in snow duck we focused on",
    "start": "1524080",
    "end": "1531840"
  },
  {
    "text": "stability we said this is a duck DB version where if you create a database",
    "start": "1531840",
    "end": "1538240"
  },
  {
    "text": "new versions will be backwards compatible with those and you will be able to read this duck DB file for the",
    "start": "1538240",
    "end": "1543960"
  },
  {
    "text": "next 5 to 10 years and to celebrate this we actually had these big data on your",
    "start": "1543960",
    "end": "1549360"
  },
  {
    "text": "laptop posters scattered across Amsterdam you can see the very nicely composed picture with the canals the",
    "start": "1549360",
    "end": "1555880"
  },
  {
    "text": "bike and the DU DB poster du DB has an extensive way for",
    "start": "1555880",
    "end": "1562159"
  },
  {
    "start": "1558000",
    "end": "1630000"
  },
  {
    "text": "extensions we said that we don't want dependencies but sometimes you just have to package something that's really heavy",
    "start": "1562159",
    "end": "1568480"
  },
  {
    "text": "hitting and we don't really want to compromise our core with having big bulky dependencies and having the system",
    "start": "1568480",
    "end": "1575679"
  },
  {
    "text": "swell up to 100 megabyte plus so we have a powerful extension mechanism where you",
    "start": "1575679",
    "end": "1580799"
  },
  {
    "text": "can register new functions new types new data formats operators SQL syntax even",
    "start": "1580799",
    "end": "1586000"
  },
  {
    "text": "memory allocators and we are having dock fooding this so many of the key features that you see with duck DB are extensions",
    "start": "1586000",
    "end": "1593159"
  },
  {
    "text": "this includes httpfs Json and parket many of them are built in but these are",
    "start": "1593159",
    "end": "1598200"
  },
  {
    "text": "still extensions that you can turn on or off and what we want to do going forward",
    "start": "1598200",
    "end": "1604039"
  },
  {
    "text": "from version 1.2 is an extension ecosystem where duck DB is in the middle it acts as kind of the fabric between",
    "start": "1604039",
    "end": "1610880"
  },
  {
    "text": "different extensions and you can do something like httpfs request over Park",
    "start": "1610880",
    "end": "1616640"
  },
  {
    "text": "and these work in tandem and these integrate with other du DB client D DB",
    "start": "1616640",
    "end": "1621960"
  },
  {
    "text": "extensions one of them may be something for a data format the other one may be vsss for Vector similarity",
    "start": "1621960",
    "end": "1629640"
  },
  {
    "text": "search so what are the use cases for Duc DB I have collected a couple of use",
    "start": "1629640",
    "end": "1634760"
  },
  {
    "start": "1630000",
    "end": "1894000"
  },
  {
    "text": "cases over the past few month and by far the first one is reducing costs people",
    "start": "1634760",
    "end": "1641200"
  },
  {
    "text": "are a bit fed up with cloud data warehouses because even though they are ergonomic they are very scalable they",
    "start": "1641200",
    "end": "1647840"
  },
  {
    "text": "are extremely expensive so replacing a proprietary system or even replacing",
    "start": "1647840",
    "end": "1653640"
  },
  {
    "text": "partially and just doing some computations locally will save you a lot of money and if you replace distributed",
    "start": "1653640",
    "end": "1660960"
  },
  {
    "text": "systems like Apache spark ddb will save you there on the efficiency front this",
    "start": "1660960",
    "end": "1666240"
  },
  {
    "text": "is a slide that hannus also showed in his talk this is a tpch experiment on",
    "start": "1666240",
    "end": "1672240"
  },
  {
    "text": "Park FES where it's roughly 40 gabes of par files and the tpch workload from",
    "start": "1672240",
    "end": "1678480"
  },
  {
    "text": "quiry 1 to quiry 22 and you can see that spark on two machines is completing it",
    "start": "1678480",
    "end": "1683919"
  },
  {
    "text": "in about 8 minutes and do DB is taking 1 minute and 16 seconds now the thing is",
    "start": "1683919",
    "end": "1690200"
  },
  {
    "text": "you can scale spark you can add more and more machines but because of the",
    "start": "1690200",
    "end": "1695279"
  },
  {
    "text": "overhead that is introduced by running distributedly and all the coordination",
    "start": "1695279",
    "end": "1700679"
  },
  {
    "text": "that those nodes need to be doing spark will never catch up with duck DB so even though you spend 30 time 32 times as",
    "start": "1700679",
    "end": "1708279"
  },
  {
    "text": "much on the costs of the machines it will still be slower than duck DB on a",
    "start": "1708279",
    "end": "1713960"
  },
  {
    "text": "single machine of the same type of course I'm not here to tell you",
    "start": "1713960",
    "end": "1720200"
  },
  {
    "text": "that spark is bad sometimes you have 100 terabytes of lock files and there is no way you can deal with those with duck DB",
    "start": "1720200",
    "end": "1727279"
  },
  {
    "text": "on your laptop then we recommend you to keep using Spark Run it on 100 machine",
    "start": "1727279",
    "end": "1733200"
  },
  {
    "text": "cluster and usually you can throw away most of the data and save just a bunch",
    "start": "1733200",
    "end": "1738440"
  },
  {
    "text": "of Statistics so maybe you will have a 5050 GB summary table then you can take that as a PAR file put it on your laptop",
    "start": "1738440",
    "end": "1746120"
  },
  {
    "text": "and do the last steps of the analytics very interactively very conveniently with do DB there are a bunch of",
    "start": "1746120",
    "end": "1752000"
  },
  {
    "text": "companies now like evidence real data and streamlit and they take this very idea and they build dashboards on top of",
    "start": "1752000",
    "end": "1758840"
  },
  {
    "text": "docdb and these dashboards run in the browser so they run duck DB in the web assembly shell and this is a very Snappy",
    "start": "1758840",
    "end": "1767600"
  },
  {
    "text": "very nice user experience they call it 120 FPS FPS dashboards because it's very",
    "start": "1767600",
    "end": "1773760"
  },
  {
    "text": "convenient that you click and you get the result immediately people do local prototyping",
    "start": "1773760",
    "end": "1779279"
  },
  {
    "text": "all the time uh there are now libraries that translate between the dialect of snowflake and data bricks to duck DB you",
    "start": "1779279",
    "end": "1786640"
  },
  {
    "text": "can program in your local environment it's completely free it's very responsive and because it's a standard",
    "start": "1786640",
    "end": "1794039"
  },
  {
    "text": "SQL dialect the portability is reasonably good even if there is is no automated",
    "start": "1794039",
    "end": "1800600"
  },
  {
    "text": "solution one thing that really drove me to use Duc DB is that it's something",
    "start": "1800600",
    "end": "1807159"
  },
  {
    "text": "that you can use as a building block so you have a larger system it's maybe nothing to do with data processing but",
    "start": "1807159",
    "end": "1813720"
  },
  {
    "text": "there is a step in between where you can use docdb maybe you have a PAR file and",
    "start": "1813720",
    "end": "1819039"
  },
  {
    "text": "you need to turn it into a CSV or maybe you have to do reading 100 million",
    "start": "1819039",
    "end": "1824159"
  },
  {
    "text": "records and you don't want to use some Java library that does it like line by line uh here are two use cases that I",
    "start": "1824159",
    "end": "1831360"
  },
  {
    "text": "did during my academic work with students and these were Java code bases",
    "start": "1831360",
    "end": "1836799"
  },
  {
    "text": "where we just notice that you do something in a for loop with some very simple data we rewrote the whole thing",
    "start": "1836799",
    "end": "1844519"
  },
  {
    "text": "in SQL ripped out the original Java code and this LEDs to parallel processing",
    "start": "1844519",
    "end": "1850000"
  },
  {
    "text": "which is much faster and also these very satisfying comets where we could just throw away 5,000 lines and repeat",
    "start": "1850000",
    "end": "1856279"
  },
  {
    "text": "replace it with 1600 lines in Duck DB the interesting thing about this is that",
    "start": "1856279",
    "end": "1861320"
  },
  {
    "text": "this is totally not a database use case you would never use snowflake for this or vertica for this but with duck DB",
    "start": "1861320",
    "end": "1867799"
  },
  {
    "text": "because it's so portable it can be used as a building block and finally duck DB can be used in education it's very easy",
    "start": "1867799",
    "end": "1874480"
  },
  {
    "text": "to install for the students it's open source it uses de facto standards and you don't need to have a DBA who has to",
    "start": "1874480",
    "end": "1880799"
  },
  {
    "text": "be phoned out of bed during the weekend because the students crashed the cluster again this did happen when I was",
    "start": "1880799",
    "end": "1887240"
  },
  {
    "text": "teaching data bases with duck DB there is no server there is no DBA no configuration so this works really well",
    "start": "1887240",
    "end": "1895360"
  },
  {
    "start": "1894000",
    "end": "1977000"
  },
  {
    "text": "so what are some limitations of duck DB well one big limitation is that because",
    "start": "1895360",
    "end": "1901639"
  },
  {
    "text": "it's in process and because it's focus on analytics we do have transactions but",
    "start": "1901639",
    "end": "1907279"
  },
  {
    "text": "you cannot do multiple writers so if you try to connect to the same database instance with multiple writers it will",
    "start": "1907279",
    "end": "1914120"
  },
  {
    "text": "throw an error you can of course read and write a single datab and you can",
    "start": "1914120",
    "end": "1919279"
  },
  {
    "text": "read the same database from multiple duck DB clients but this is a limitation",
    "start": "1919279",
    "end": "1925000"
  },
  {
    "text": "in concurrency control and the second is the duck DB is limited to single node",
    "start": "1925000",
    "end": "1930519"
  },
  {
    "text": "execution duck DB can scale up and the cloud has gone a long way in the last",
    "start": "1930519",
    "end": "1937120"
  },
  {
    "text": "couple of years you can rent machines up to 36 terabytes although the pricing is",
    "start": "1937120",
    "end": "1942440"
  },
  {
    "text": "a bit of a joke so I just put that in uh in Gray but you can easily get enough",
    "start": "1942440",
    "end": "1948519"
  },
  {
    "text": "memory and enough CPUs if you want to go back to Big Iron sty processing in the",
    "start": "1948519",
    "end": "1954080"
  },
  {
    "text": "cloud and one thing that is very good for duck DB is that you don't have to pay these outrageous prices every hour",
    "start": "1954080",
    "end": "1961440"
  },
  {
    "text": "you can just store your data in S3 bucket or even better an S3 Express phone bucket and spin up a computer run",
    "start": "1961440",
    "end": "1969440"
  },
  {
    "text": "the short ver of workload save the data back to S3 and then you still get a",
    "start": "1969440",
    "end": "1974960"
  },
  {
    "text": "pretty good bang for your buck so what's the business model of duck DB well duck DB Labs is a company",
    "start": "1974960",
    "end": "1982799"
  },
  {
    "start": "1977000",
    "end": "2103000"
  },
  {
    "text": "that's based in Amsterdam and one of our principles is that we are funded by Revenue so we didn't take any Venture",
    "start": "1982799",
    "end": "1989240"
  },
  {
    "text": "Capital but we kept the company rather small and kept the growth at a",
    "start": "1989240",
    "end": "1994399"
  },
  {
    "text": "reasonable rate and we consult and support for duck DB so we work with a",
    "start": "1994399",
    "end": "1999559"
  },
  {
    "text": "bunch of companies like data bricks five Trend real data hugging phase and so on and we provide services for them around",
    "start": "1999559",
    "end": "2006440"
  },
  {
    "text": "duck DB including premium support the duck DB Foundation is a",
    "start": "2006440",
    "end": "2013000"
  },
  {
    "text": "nonprofit the idea of it being here is that even though we do not want to be acquired it is possible that at sometime",
    "start": "2013000",
    "end": "2019000"
  },
  {
    "text": "in the future big tech company X will just buy duck DB labs and close down the source but the source is actually not",
    "start": "2019000",
    "end": "2026159"
  },
  {
    "text": "owned by Duck DB Labs the source is with the ddb foundation the ddb foundation",
    "start": "2026159",
    "end": "2031760"
  },
  {
    "text": "owns the intellectual property so it cannot be relicensed through the company",
    "start": "2031760",
    "end": "2036799"
  },
  {
    "text": "the ddb foundation is something you can think of as an insurance policy where",
    "start": "2036799",
    "end": "2042080"
  },
  {
    "text": "the supporters ensure that duck DB still has maintenance even if the original project for some reason does not",
    "start": "2042080",
    "end": "2049520"
  },
  {
    "text": "continue and we have companies building on Duck DB so I mentioned that we do not",
    "start": "2049520",
    "end": "2054638"
  },
  {
    "text": "do multiple writers and we do not really scale that much well we have a partner company called mother duck they are",
    "start": "2054639",
    "end": "2061040"
  },
  {
    "text": "venture capital founded because they are building a cloud data warehouse and you cannot really build that based on",
    "start": "2061040",
    "end": "2067000"
  },
  {
    "text": "Revenue and they are based in Seattle with offices also in San Francisco New York",
    "start": "2067000",
    "end": "2074200"
  },
  {
    "text": "and here in Amsterdam and mother dock do a cloud version of duck DB with many interesting",
    "start": "2074200",
    "end": "2081638"
  },
  {
    "text": "ideas such as hybrid query execution where part of the query runs on your laptop and the other part runs on the",
    "start": "2081639",
    "end": "2087638"
  },
  {
    "text": "server so mother duck just reached General availability as today so they went to the open market and we hope it's",
    "start": "2087639",
    "end": "2094760"
  },
  {
    "text": "going to work out really well for them they have a really nice product with quite interesting ideas and very strong",
    "start": "2094760",
    "end": "2100880"
  },
  {
    "text": "ties to the duck DB ecosystem so to sum up if you think",
    "start": "2100880",
    "end": "2106280"
  },
  {
    "start": "2103000",
    "end": "2166000"
  },
  {
    "text": "about the spectrum of data processing systems you have really small data sets",
    "start": "2106280",
    "end": "2111680"
  },
  {
    "text": "like Excel data sets and also pandas on the left and you have really big 10",
    "start": "2111680",
    "end": "2116760"
  },
  {
    "text": "terabyte Plus data sets on the right where Spark makes perfect sense but we believe that there is this Middle Ground",
    "start": "2116760",
    "end": "2122960"
  },
  {
    "text": "between say 10 gabt to 1 terabyte when p is not scalable enough and Spark is a",
    "start": "2122960",
    "end": "2129599"
  },
  {
    "text": "bit too much to set up and ddb fits really nicely into this Gap and we are",
    "start": "2129599",
    "end": "2135400"
  },
  {
    "text": "really proud that duck DB is an old school system in the sense that we don't do business software license we have a",
    "start": "2135400",
    "end": "2142680"
  },
  {
    "text": "very permissive MIT license we are very portable like the good old Unix tools we",
    "start": "2142680",
    "end": "2148359"
  },
  {
    "text": "build an Open Standards everything works offline even the documentation is something that you can just grab here's",
    "start": "2148359",
    "end": "2154200"
  },
  {
    "text": "a PDF and then you can work on the train without being permanently connected to some annoying endpoint in the cloud we",
    "start": "2154200",
    "end": "2161240"
  },
  {
    "text": "do not track you we do not use cookies we don't have Telemetry and so on so if uh you found this interesting",
    "start": "2161240",
    "end": "2168680"
  },
  {
    "start": "2166000",
    "end": "2190000"
  },
  {
    "text": "then head to db. org and uh give it a try and also stay in touch thank you so",
    "start": "2168680",
    "end": "2174040"
  },
  {
    "text": "much",
    "start": "2174040",
    "end": "2177040"
  }
]