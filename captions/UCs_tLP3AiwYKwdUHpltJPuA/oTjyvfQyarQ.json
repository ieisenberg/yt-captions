[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "thanks for joining me today everybody as Chris said I am an engineering manager at a small software company in San",
    "start": "5759",
    "end": "12120"
  },
  {
    "text": "Francisco called one signal we're a omni-channel messaging company so our",
    "start": "12120",
    "end": "17220"
  },
  {
    "text": "customers are website Builders app Builders who want to send messages to their user base using uh push",
    "start": "17220",
    "end": "23640"
  },
  {
    "text": "notifications email in-app messages Etc so this is kind of a bug retrospective",
    "start": "23640",
    "end": "29160"
  },
  {
    "text": "so we need to sort of jump back in time a little bit so I can set the stage for you and",
    "start": "29160",
    "end": "35399"
  },
  {
    "start": "35000",
    "end": "233000"
  },
  {
    "text": "talk about what's going to happen here so at this time we were sending around 8 billion push notifications every day we",
    "start": "35399",
    "end": "43500"
  },
  {
    "text": "had about 10 Engineers working on our back-end team who are managing all the systems that did this and when you're",
    "start": "43500",
    "end": "50460"
  },
  {
    "text": "operating at that like sort of large-ish scale with that kind of smallish number of Engineers you have to make a lot of",
    "start": "50460",
    "end": "56699"
  },
  {
    "text": "concessions in the name of performance and you know meeting the goals that you would like to make right",
    "start": "56699",
    "end": "63960"
  },
  {
    "text": "um so the problem that we are trying to solve today is we are fronting the API that updates data properties for our",
    "start": "63960",
    "end": "71580"
  },
  {
    "text": "customers subscriptions so our customers are going to be sending us HTTP requests like this that",
    "start": "71580",
    "end": "78540"
  },
  {
    "text": "correspond with a single app which is like a single customer's data set and a",
    "start": "78540",
    "end": "84780"
  },
  {
    "text": "subscription which is a single device so we allow customers to store key value",
    "start": "84780",
    "end": "89820"
  },
  {
    "text": "pairs arbitrary key value pairs associated with all their subscriptions in this case you know we have John Smith",
    "start": "89820",
    "end": "96840"
  },
  {
    "text": "in there as a customer's name so this would allow our customer to do something like you know hi John please come back",
    "start": "96840",
    "end": "102900"
  },
  {
    "text": "to the app and you know check out this great sale that we have right now um so a single person might have multiple",
    "start": "102900",
    "end": "109920"
  },
  {
    "text": "subscriptions associated with them you know they might have multiple subscriptions on a single device like a",
    "start": "109920",
    "end": "115799"
  },
  {
    "text": "mobile push subscription as well as an SMS subscription and then in a different device on a web browser they might have",
    "start": "115799",
    "end": "121920"
  },
  {
    "text": "you know a web push subscription and these would all be stored under different different records in our",
    "start": "121920",
    "end": "127259"
  },
  {
    "text": "database because our data storage model was at this time very device Centric",
    "start": "127259",
    "end": "134160"
  },
  {
    "text": "yeah so you can see all these records have independent storage for this account type key value pair on them",
    "start": "134160",
    "end": "140959"
  },
  {
    "text": "um we started out with a you know pretty simple API it was uh just a rest server that was sending",
    "start": "141239",
    "end": "147599"
  },
  {
    "text": "updates directly into postgres and you know that worked for a time but then we we started to get to a certain scale of",
    "start": "147599",
    "end": "154140"
  },
  {
    "text": "user base that when we got to like the hour and the half hour marks we were just getting hammered with tons of traffic because everybody loves to",
    "start": "154140",
    "end": "160680"
  },
  {
    "text": "schedule all their stuff to happen an hour and a half hour marks nobody's nobody's scheduling stuff to happen at",
    "start": "160680",
    "end": "165959"
  },
  {
    "text": "1102 in the morning right um so we we started to get hammered on our",
    "start": "165959",
    "end": "172500"
  },
  {
    "text": "postgres updates we started to like get a bunch of CPU CPU utilization you know lock contention on rows things like this",
    "start": "172500",
    "end": "179160"
  },
  {
    "text": "so what were we going to do with these updates we we decided to put them stick",
    "start": "179160",
    "end": "184860"
  },
  {
    "text": "them in a queue because cues are a lot easier to control right uh cues uh just",
    "start": "184860",
    "end": "191040"
  },
  {
    "text": "like the Q you weighed in at the at the cafe you enqueue stuff at one end you DQ stuff at the other end when it's ready",
    "start": "191040",
    "end": "196140"
  },
  {
    "text": "to be processed when we had this variability in the enqueue rate at the you know hour and a half hour marks we",
    "start": "196140",
    "end": "202379"
  },
  {
    "text": "were able to keep the DQ rate the same so instead of saturating our postgres",
    "start": "202379",
    "end": "207959"
  },
  {
    "text": "connections and you know taking our servers down we just built up a little bit of lag so our updates were you know",
    "start": "207959",
    "end": "214860"
  },
  {
    "text": "slightly delayed we could deal with you know a five minute delay in updates that's preferable to knocking out our",
    "start": "214860",
    "end": "221159"
  },
  {
    "text": "our web servers knocking out our postgres servers right um but in the real world we don't use",
    "start": "221159",
    "end": "227000"
  },
  {
    "text": "just a queue right we use like real implementations of the abstract data",
    "start": "227000",
    "end": "232080"
  },
  {
    "text": "structure of a queue in our case we're using Apache Kafka to cue these updates so this is an event streaming platform",
    "start": "232080",
    "end": "238280"
  },
  {
    "start": "233000",
    "end": "535000"
  },
  {
    "text": "uh it's very widely used um and yeah so so the way Coffee Works more",
    "start": "238280",
    "end": "247500"
  },
  {
    "text": "or less we have a we have a queue we have a topic which is like a logical grouping of messages",
    "start": "247500",
    "end": "253819"
  },
  {
    "text": "uh messages all have incrementing IDs which are called offsets in this case we",
    "start": "253819",
    "end": "259560"
  },
  {
    "text": "have offsets zero one two three and four um messages are added onto the topic by",
    "start": "259560",
    "end": "265979"
  },
  {
    "text": "a process called a producer they are pulled off of the topic by a process called a consumer and the way that Kafka",
    "start": "265979",
    "end": "273120"
  },
  {
    "text": "like keeps track of where you are in the queue is not by mutating the queue but it's by actually just changing which",
    "start": "273120",
    "end": "280620"
  },
  {
    "text": "offset you're like currently looking at so it's just sort of a pointer to that queue of messages",
    "start": "280620",
    "end": "287220"
  },
  {
    "text": "um and then when we're done processing a message you send Kafka 8 commit which will change which pointer you're you're",
    "start": "287220",
    "end": "292979"
  },
  {
    "text": "currently pointing to and this is very convenient because it means that you can actually have multiple consumers which",
    "start": "292979",
    "end": "298199"
  },
  {
    "text": "are all consuming from the exact same queue so in our case you know we're we're fronting basically postgres rights",
    "start": "298199",
    "end": "304620"
  },
  {
    "text": "or we started out by fronting postgres rights to update these user properties right but at a certain point we realize",
    "start": "304620",
    "end": "311160"
  },
  {
    "text": "that oh these these user property updates could actually be useful in a bunch of different product areas for us so we actually have many different",
    "start": "311160",
    "end": "317340"
  },
  {
    "text": "coffee consumers which are reading from the same uh the same user property update stream and we use that to to",
    "start": "317340",
    "end": "325020"
  },
  {
    "text": "sort of Feed the data intake for many of our different features",
    "start": "325020",
    "end": "330440"
  },
  {
    "text": "and that's you know that's basically only possible because we are because Kafka is storing just a pointer in there",
    "start": "330660",
    "end": "336840"
  },
  {
    "text": "for each one of the consumer groups um yes and then if you want to scale uh",
    "start": "336840",
    "end": "342600"
  },
  {
    "text": "Kafka horizontally because we we all love to scale horizontally um we do that using something called a",
    "start": "342600",
    "end": "349080"
  },
  {
    "text": "partition so each Kafka topic is divided up into numbered partitions and these",
    "start": "349080",
    "end": "355320"
  },
  {
    "text": "each have independent numbering for all of the offsets of messages in there so",
    "start": "355320",
    "end": "360600"
  },
  {
    "text": "in this case we can see that partition 0 is currently looking at oh that number",
    "start": "360600",
    "end": "366720"
  },
  {
    "text": "is wrong I'm sorry partition zero is currently looking at message number one and partition one is currently a",
    "start": "366720",
    "end": "372960"
  },
  {
    "text": "processing message with the offset number two so uh something that's sort of relevant",
    "start": "372960",
    "end": "379199"
  },
  {
    "text": "to us that I touched on briefly was latency the number of messages that are sitting in Kafka and waiting to be",
    "start": "379199",
    "end": "385139"
  },
  {
    "text": "processed right so in this case partition zero is going to have four messages in leg in latency it's got one",
    "start": "385139",
    "end": "392639"
  },
  {
    "text": "two three and four on it and partition one just has a single message in lag",
    "start": "392639",
    "end": "399680"
  },
  {
    "text": "um yeah and there's a couple of ways that you can distribute messages to the",
    "start": "400979",
    "end": "406440"
  },
  {
    "text": "different Kafka partitions that you have you can do that you can you know basically let Kafka do this for you if",
    "start": "406440",
    "end": "411720"
  },
  {
    "text": "you just have your producer dump the message into Kafka it will sort of round robin messages between all the",
    "start": "411720",
    "end": "417000"
  },
  {
    "text": "partitions and that that might be okay for you it might not be okay um but in our case we were fronting",
    "start": "417000",
    "end": "423120"
  },
  {
    "text": "postgres rights and these postgres rights were going to one of several servers inside of a large cluster of",
    "start": "423120",
    "end": "429000"
  },
  {
    "text": "postgres servers and the the sharding scheme for those postgres servers was",
    "start": "429000",
    "end": "434280"
  },
  {
    "text": "consistent it was based on these sort of uh the app ID which is like the customer",
    "start": "434280",
    "end": "439860"
  },
  {
    "text": "data set ID um and so we actually would not use that round robin method we were doing",
    "start": "439860",
    "end": "446220"
  },
  {
    "text": "explicit assignment of messages to different Kafka partitions so at the time we were producing the message into",
    "start": "446220",
    "end": "451800"
  },
  {
    "text": "Kafka we would say all right this is going to which data set um all right dump it into into this",
    "start": "451800",
    "end": "457560"
  },
  {
    "text": "partition so each partition was going to have messages for only one postgres",
    "start": "457560",
    "end": "463139"
  },
  {
    "text": "server in it this meant that if one postgres server had an availability issue",
    "start": "463139",
    "end": "468539"
  },
  {
    "text": "um it would not spread to sort of other postgres servers so we wanted to try and keep our blast radius for postgres",
    "start": "468539",
    "end": "475080"
  },
  {
    "text": "downtime as small as possible it also meant that if one app was",
    "start": "475080",
    "end": "480300"
  },
  {
    "text": "spamming us with a lot of updates hopefully that isn't going to be causing latency issues for us across multiple",
    "start": "480300",
    "end": "486240"
  },
  {
    "text": "partitions the other nice thing about this is",
    "start": "486240",
    "end": "493080"
  },
  {
    "text": "allows us to pretty easily control the concurrency on postgres rights so",
    "start": "493080",
    "end": "499259"
  },
  {
    "text": "if we had say two postgres servers and we wanted to perform two concurrent rights to our postcast servers we could",
    "start": "499259",
    "end": "505800"
  },
  {
    "text": "create a Kafka topic which had four partitions and have each of those partitions performing a right to one of",
    "start": "505800",
    "end": "512880"
  },
  {
    "text": "the postgres servers so in this case we have you know two partitions which are sending rights to the red server and two",
    "start": "512880",
    "end": "518820"
  },
  {
    "text": "partitions which are sending rights to the green server um and this got us pretty far we we used",
    "start": "518820",
    "end": "525600"
  },
  {
    "text": "this for for a while and it worked pretty well but you know the numbers always keep going up and so you have to",
    "start": "525600",
    "end": "532560"
  },
  {
    "text": "you have to get creative as time goes on um and we started to run into a couple of problems with this",
    "start": "532560",
    "end": "539640"
  },
  {
    "start": "535000",
    "end": "640000"
  },
  {
    "text": "times that we wanted to control the currency a little bit more finely grained right we wanted to uh increase",
    "start": "539640",
    "end": "547680"
  },
  {
    "text": "the number of concurrent rights that we're performing to postgres this system started to show itself as pretty",
    "start": "547680",
    "end": "553140"
  },
  {
    "text": "inflexible because if you wanted to say go from two concurrent postgres rights",
    "start": "553140",
    "end": "558660"
  },
  {
    "text": "to three the way that you would accomplished that was by doing a repartitioning of Kafka",
    "start": "558660",
    "end": "564060"
  },
  {
    "text": "and if you have ever been a part of a that process it's mildly annoying uh and",
    "start": "564060",
    "end": "570480"
  },
  {
    "text": "time consuming so the the way that you do this basically is to create a new topic which has the desired number of",
    "start": "570480",
    "end": "577260"
  },
  {
    "text": "partitions and you dump all the messages from the old topic into the new topic and then you shift your producers over",
    "start": "577260",
    "end": "584220"
  },
  {
    "text": "from the old topic to the new topic and if you you know care about which partition a message is assigned to that",
    "start": "584220",
    "end": "591420"
  },
  {
    "text": "that producer cut over is is quite annoying as well and if you care about the ordering",
    "start": "591420",
    "end": "596580"
  },
  {
    "text": "between the old messages and the newly produced messages then you need to get even like more so creative with it um",
    "start": "596580",
    "end": "604200"
  },
  {
    "text": "and you may trying to introduce this in a way that also doesn't contribute massively to a",
    "start": "604200",
    "end": "611100"
  },
  {
    "text": "bunch of lag in Kafka is quite frustrating so this is the thing that we wanted to sort of architect our way out",
    "start": "611100",
    "end": "617459"
  },
  {
    "text": "of having to solve all the time because you know what if we what if we get better CPUs on our database servers what",
    "start": "617459",
    "end": "624060"
  },
  {
    "text": "if we decide that we want to prioritize reads over writes and we're okay with a little bit more latency on this topic we",
    "start": "624060",
    "end": "631140"
  },
  {
    "text": "wanted to be able to be more flexible and not be quite so constrained on on having to care about our Kafka",
    "start": "631140",
    "end": "637140"
  },
  {
    "text": "partitioning scheme so the way that we accomplished this was uh was something that we called sub partition processing",
    "start": "637140",
    "end": "643560"
  },
  {
    "start": "640000",
    "end": "902000"
  },
  {
    "text": "so inside of a Kafka partition right basically doing uh concurrent processing",
    "start": "643560",
    "end": "649680"
  },
  {
    "text": "of individual messages normally the Kafka folks say don't do that",
    "start": "649680",
    "end": "655640"
  },
  {
    "text": "but we did it anyways um so what does this look like so we have a",
    "start": "655640",
    "end": "663120"
  },
  {
    "text": "number of workers a number of processors that are a part of each partition and each of those can process a message",
    "start": "663120",
    "end": "669120"
  },
  {
    "text": "concurrently because they're all associated with the same underlying kafka-q this means that",
    "start": "669120",
    "end": "675839"
  },
  {
    "text": "we can control the number of workers very easily it's you know just a number in a config file and we can change this",
    "start": "675839",
    "end": "681480"
  },
  {
    "text": "by just redeploying the consumer and you know if we want it to be really Dynamic about it we could even make this a",
    "start": "681480",
    "end": "688200"
  },
  {
    "text": "property that was able to be live updated um yeah so this got us even further however",
    "start": "688200",
    "end": "696019"
  },
  {
    "text": "this this in and of itself introduces another problem that we need to solve so if your processing messages concurrently",
    "start": "696019",
    "end": "702540"
  },
  {
    "text": "you're going to eventually process some messages out of order right so if we have messages uh with with offsets zero",
    "start": "702540",
    "end": "710579"
  },
  {
    "text": "one two and three what happens if messages zero and three are completed you know at around the same time",
    "start": "710579",
    "end": "717320"
  },
  {
    "text": "uh under the normal semantics of a Kafka consumer you do a commit after a message",
    "start": "717320",
    "end": "722399"
  },
  {
    "text": "is processed so when we perform the commit for message zero that's fine",
    "start": "722399",
    "end": "728339"
  },
  {
    "text": "um because Kafka is like a linear log of messages right",
    "start": "728339",
    "end": "734399"
  },
  {
    "text": "um so when we commit zero that's fine because it's at the it's at the beginning of the topic but when we run",
    "start": "734399",
    "end": "739860"
  },
  {
    "text": "the commit for the message with offset three this presents a problem because Kafka does not store",
    "start": "739860",
    "end": "746220"
  },
  {
    "text": "like each message's commit status yes this was committed no this wasn't committed it only stores for each uh",
    "start": "746220",
    "end": "754079"
  },
  {
    "text": "consumer group and each partition the last message which was committed or the greatest offset that was commit",
    "start": "754079",
    "end": "760440"
  },
  {
    "text": "committed so that means when we send a commit for the offset three it assumes that one and",
    "start": "760440",
    "end": "767399"
  },
  {
    "text": "two were also completed and also committed now these messages are still being processed inside the consumer so",
    "start": "767399",
    "end": "773519"
  },
  {
    "text": "they're really not completed and it's okay for the sort of interim time period",
    "start": "773519",
    "end": "778860"
  },
  {
    "text": "where these messages are still being processed but if the consumer was to get a rebalance or if it was to be restarted",
    "start": "778860",
    "end": "785639"
  },
  {
    "text": "by kubernetes or something like that we're going to run into a problem right because then these messages might never",
    "start": "785639",
    "end": "792839"
  },
  {
    "text": "end up being completed and we don't really want to drop anybody's postgres rights on the floor",
    "start": "792839",
    "end": "798380"
  },
  {
    "text": "uh so how do we solve this the solution that we came up with was to",
    "start": "798380",
    "end": "803579"
  },
  {
    "text": "use something called a commit buffer so instead of sending the instead of",
    "start": "803579",
    "end": "809459"
  },
  {
    "text": "sending the offsets of completed messages directly to Kafka we instead dumped them in memory into this data",
    "start": "809459",
    "end": "815220"
  },
  {
    "text": "structure which was storing is the message completed yes no and",
    "start": "815220",
    "end": "821399"
  },
  {
    "text": "after doing this we would scan from the beginning of the commit buffer uh down and we would check to see if",
    "start": "821399",
    "end": "827700"
  },
  {
    "text": "there was a contiguous block of completed messages starting at the beginning right so in this case we do",
    "start": "827700",
    "end": "833519"
  },
  {
    "text": "have a completed we do have a contiguous block just containing one message right just message with offset zero uh so we",
    "start": "833519",
    "end": "841980"
  },
  {
    "text": "would go ahead and send that commit to Kafka just the message with offset zero so one and two are now at the front of",
    "start": "841980",
    "end": "849240"
  },
  {
    "text": "that block we sort of re-index we shift the block up um one and two are now at the front and",
    "start": "849240",
    "end": "854820"
  },
  {
    "text": "those are not completed so we can't do any more commits so message three is just going to sort of sit there in that",
    "start": "854820",
    "end": "861000"
  },
  {
    "text": "commit buffer waiting for waiting for that block to finish before it can move on",
    "start": "861000",
    "end": "866040"
  },
  {
    "text": "uh and at a certain point in time the messages with offsets one and two are going to be completed right and at this",
    "start": "866040",
    "end": "873120"
  },
  {
    "text": "point we are going to have a contiguous block of messages from one two and three",
    "start": "873120",
    "end": "878220"
  },
  {
    "text": "so we can then send Kafka a commit for the message with offset three and it now",
    "start": "878220",
    "end": "883560"
  },
  {
    "text": "correctly knows that messages zero through three are completed",
    "start": "883560",
    "end": "889500"
  },
  {
    "text": "uh so once again this bought us another like layer of stability and another layer of performance gains right",
    "start": "889500",
    "end": "897600"
  },
  {
    "text": "um but we started to run into problems with this as well uh this has you know a big concession",
    "start": "897600",
    "end": "905399"
  },
  {
    "start": "902000",
    "end": "972000"
  },
  {
    "text": "with it which is you are getting at least once delivery or sorry yes at least once delivery there's a number of",
    "start": "905399",
    "end": "912240"
  },
  {
    "text": "sort of models that Kafka consumers operate on there's at least once delivery there's at most once delivery",
    "start": "912240",
    "end": "918060"
  },
  {
    "text": "where a message might not be read at all but it's not going to be read more than once and there is uh exactly once",
    "start": "918060",
    "end": "924060"
  },
  {
    "text": "delivery which is basically just a big bucket that you pour money into and your time is much better spent writing a",
    "start": "924060",
    "end": "931620"
  },
  {
    "text": "document about why you actually don't need exactly one's delivery or why you should pick something that isn't Kafka",
    "start": "931620",
    "end": "938160"
  },
  {
    "text": "um yeah so we're doing at least once delivery this means messages",
    "start": "938160",
    "end": "943740"
  },
  {
    "text": "at a certain point are probably going to be replayed and so you have to sort of design your system around this you have",
    "start": "943740",
    "end": "950339"
  },
  {
    "text": "to well you have to either decide that it's okay if values are inconsistent or you",
    "start": "950339",
    "end": "956519"
  },
  {
    "text": "have to design a way that your system is not going to you know do that right a second time",
    "start": "956519",
    "end": "962279"
  },
  {
    "text": "I'm not going to delve too much into how we designed our system around this but I can answer questions about it later if",
    "start": "962279",
    "end": "968459"
  },
  {
    "text": "if that's desired yeah so let's talk about what we've",
    "start": "968459",
    "end": "974940"
  },
  {
    "start": "972000",
    "end": "1084000"
  },
  {
    "text": "what we've got so far right we have a Kafka topic which is composed of multiple partitions and these are",
    "start": "974940",
    "end": "981779"
  },
  {
    "text": "independent cues of messages each message has an incrementing offset starting at zero and going up",
    "start": "981779",
    "end": "987959"
  },
  {
    "text": "we have a producer that enqueues messages into the topic and we have a consumer which dqs messages out of the",
    "start": "987959",
    "end": "994019"
  },
  {
    "text": "topic we can control the concurrency of that consumer using the number of Kafka",
    "start": "994019",
    "end": "1000259"
  },
  {
    "text": "partitions as well as the number of sub-partition workers and then this particular consumer in",
    "start": "1000259",
    "end": "1006980"
  },
  {
    "text": "this case is performing postgres rights um yes let's talk about these postgres",
    "start": "1006980",
    "end": "1013220"
  },
  {
    "text": "rights for a moment what happens if if we perform two",
    "start": "1013220",
    "end": "1019579"
  },
  {
    "text": "concurrent postgres rights which are targeting the same row right what happens if a customer you",
    "start": "1019579",
    "end": "1026298"
  },
  {
    "text": "know rapidly sends us to update requests one to set a property to ten and another",
    "start": "1026299",
    "end": "1032298"
  },
  {
    "text": "one to set the same property to 20. if we're processing those two updates concurrently",
    "start": "1032299",
    "end": "1037760"
  },
  {
    "text": "we might at some point process the the message that the customer",
    "start": "1037760",
    "end": "1043220"
  },
  {
    "text": "technically sent a second before the one that they sent us first right so we",
    "start": "1043220",
    "end": "1048799"
  },
  {
    "text": "might in postgres set the property to 20 and then you know a short time later set",
    "start": "1048799",
    "end": "1054140"
  },
  {
    "text": "the property to 10. maybe this is acceptable but we decided that you know we can't Intuit what the customer wanted",
    "start": "1054140",
    "end": "1061160"
  },
  {
    "text": "so sort of the best thing we could do in this strange situation was like maintain the the ordering guarantees that the",
    "start": "1061160",
    "end": "1067640"
  },
  {
    "text": "customer provided to us so we we decided that this was not acceptable we didn't",
    "start": "1067640",
    "end": "1073280"
  },
  {
    "text": "want to process you know updates for the same row concurrently we wanted to",
    "start": "1073280",
    "end": "1078380"
  },
  {
    "text": "maintain the same ordering that the customer provided to us and the way we did this",
    "start": "1078380",
    "end": "1084679"
  },
  {
    "start": "1084000",
    "end": "1333000"
  },
  {
    "text": "or sorry we identified a couple goals of this process right we wanted to maximize",
    "start": "1084679",
    "end": "1090020"
  },
  {
    "text": "concurrency we wanted to you know saturate our postgres connections up to the limits that we had defined do as",
    "start": "1090020",
    "end": "1096860"
  },
  {
    "text": "many operations as possible concurrently while minimizing contention for individual rows and never performing",
    "start": "1096860",
    "end": "1103520"
  },
  {
    "text": "concurrent updates on a single row right so the way that we accomplished this was",
    "start": "1103520",
    "end": "1110960"
  },
  {
    "text": "with another layer of queuing because it's cues all the way down I'm afraid",
    "start": "1110960",
    "end": "1117500"
  },
  {
    "text": "um so we did this using something that we called sub partition queuing so we have a partition that is you know a real",
    "start": "1117500",
    "end": "1125120"
  },
  {
    "text": "cue that Kafka maintains and what we did was we took the subscription ID which is like the ID of",
    "start": "1125120",
    "end": "1132380"
  },
  {
    "text": "the individual device and we hashed that and we used it to put",
    "start": "1132380",
    "end": "1138440"
  },
  {
    "text": "bucket it put it into a queue that was feeding into one of the processors into one of the workers",
    "start": "1138440",
    "end": "1144440"
  },
  {
    "text": "so these cues are matched one to one with the the workers and because which queue we're assigning to is computed as",
    "start": "1144440",
    "end": "1151100"
  },
  {
    "text": "a hash of the subscription ID which is a row ID we're never going to process",
    "start": "1151100",
    "end": "1156200"
  },
  {
    "text": "updates for a single row at the same time so assuming that you know the",
    "start": "1156200",
    "end": "1162440"
  },
  {
    "text": "number of updates that we're getting is like distributed across all of our rows in the database we're we're going to",
    "start": "1162440",
    "end": "1168679"
  },
  {
    "text": "have cues that our processing messages concurrently but those updates are never going to be for the same postgres row",
    "start": "1168679",
    "end": "1176260"
  },
  {
    "text": "so if we put all of this together and look at like what one of our Kafka consumers looks like right we have a",
    "start": "1178520",
    "end": "1183980"
  },
  {
    "text": "producer that's dumping messages into Kafka based and it's picking a partition based",
    "start": "1183980",
    "end": "1189919"
  },
  {
    "text": "on the ID of the application the customer's data set um Kafka is storing those messages",
    "start": "1189919",
    "end": "1197539"
  },
  {
    "text": "is being pulled off by a consumer that consumer is Distributing the messages to an in-memory queue",
    "start": "1197539",
    "end": "1204820"
  },
  {
    "text": "based on the row ID we have a commit buffer that's storing",
    "start": "1204820",
    "end": "1210440"
  },
  {
    "text": "the in-process commits and then everything is going to postgres",
    "start": "1210440",
    "end": "1216620"
  },
  {
    "text": "eventually",
    "start": "1216620",
    "end": "1219620"
  },
  {
    "text": "and and suddenly for for a sec everything was fine",
    "start": "1222679",
    "end": "1228260"
  },
  {
    "text": "and we got on for like a really long time without any major problems in this system but you know when everything is",
    "start": "1228260",
    "end": "1235880"
  },
  {
    "text": "fine for a while usually at a certain point it ceases to become fine and and",
    "start": "1235880",
    "end": "1241280"
  },
  {
    "text": "that's what I want to really want to talk about today um is the point where it stopped being fine right",
    "start": "1241280",
    "end": "1248740"
  },
  {
    "text": "um so we started to get paged on huge spikes in message latency in our",
    "start": "1249080",
    "end": "1254240"
  },
  {
    "text": "consumers right so we're talking about like latency going from you know maybe 5",
    "start": "1254240",
    "end": "1261200"
  },
  {
    "text": "000 messages at a time as like our base rate to tens of millions of messages at",
    "start": "1261200",
    "end": "1267020"
  },
  {
    "text": "these Peaks and we really had a tough time figuring out why um we did notice that the latency was",
    "start": "1267020",
    "end": "1274039"
  },
  {
    "text": "not associated with a single partition it wasn't associated with all partitions it was smeared across all of the",
    "start": "1274039",
    "end": "1281480"
  },
  {
    "text": "partitions which were assigned to like a single consumer instance it's like one of our kubernetes pots at all of its",
    "start": "1281480",
    "end": "1287840"
  },
  {
    "text": "partitions that were lagging so we looked at the we looked at the stats for this one pod right we expected",
    "start": "1287840",
    "end": "1295520"
  },
  {
    "text": "to see you know elevated CPU usage we expected to see every postgres connection checked out of the pool and",
    "start": "1295520",
    "end": "1301520"
  },
  {
    "text": "Performing rights as quickly as possible and what we actually saw was was the complete opposite of that right we saw",
    "start": "1301520",
    "end": "1308419"
  },
  {
    "text": "CPU usage at nearly zero and we saw every single postgres connection except",
    "start": "1308419",
    "end": "1313580"
  },
  {
    "text": "one sitting in our connection pool doing nothing so we're like okay there's",
    "start": "1313580",
    "end": "1319580"
  },
  {
    "text": "there's a virtually infinite amount of work that you could be doing right now but you're just sitting there doing nothing it's just like me",
    "start": "1319580",
    "end": "1328299"
  },
  {
    "text": "just like me okay uh so we're like what what is happening",
    "start": "1331039",
    "end": "1336260"
  },
  {
    "start": "1333000",
    "end": "1647000"
  },
  {
    "text": "right now what's going on uh so we we tried to answer this question using the observability tools",
    "start": "1336260",
    "end": "1342320"
  },
  {
    "text": "that we had available to us which at this time unfortunately was basically just metrics now metrics are great they",
    "start": "1342320",
    "end": "1349460"
  },
  {
    "text": "can get you very far you can build great alerts off metrics but they're pretty limited when you're trying to like debug",
    "start": "1349460",
    "end": "1356840"
  },
  {
    "text": "something that's going wrong in production right uh we did have logs but they were scattered they were like",
    "start": "1356840",
    "end": "1363200"
  },
  {
    "text": "physically on they were like physically on the boxes they were unstructured we had no centralized no structured logs",
    "start": "1363200",
    "end": "1370280"
  },
  {
    "text": "whatsoever this is something that I was like trying",
    "start": "1370280",
    "end": "1375320"
  },
  {
    "text": "to get implemented at the company sort of since I joined because I came from a much much larger company that had a",
    "start": "1375320",
    "end": "1381080"
  },
  {
    "text": "great practice of of centralized structured logging and you know I joined the team and I was like all right great",
    "start": "1381080",
    "end": "1386720"
  },
  {
    "text": "where do I go to see the logs you SSH onto the Box no no I don't do that",
    "start": "1386720",
    "end": "1392659"
  },
  {
    "text": "um so at this point when we're dealing with this this this problem I'm like all",
    "start": "1392659",
    "end": "1397700"
  },
  {
    "text": "right we need to get better observability in place um to solve this problem right so I use",
    "start": "1397700",
    "end": "1403820"
  },
  {
    "text": "this as like a jumping off point for for getting centralized logging centralized observability setup",
    "start": "1403820",
    "end": "1410120"
  },
  {
    "text": "um so we had all of our consumers now dumping structured logs into a centralized logging provider and we were",
    "start": "1410120",
    "end": "1416960"
  },
  {
    "text": "able to then do some analytics based on that now we weren't sending a lot of information for each event but all of",
    "start": "1416960",
    "end": "1423679"
  },
  {
    "text": "our events were sending you know which app ID which customer data set which subscription ID which is like the",
    "start": "1423679",
    "end": "1429440"
  },
  {
    "text": "postgres row the SQL that we were using uh sending to postgres and the host name",
    "start": "1429440",
    "end": "1435380"
  },
  {
    "text": "of the consumer instance right and the first thing we did was like all",
    "start": "1435380",
    "end": "1440960"
  },
  {
    "text": "right let's let's group this by the data set let's group this by the app ID and we immediately noticed something",
    "start": "1440960",
    "end": "1447380"
  },
  {
    "text": "very striking we noticed that of the small amount of updates that were being performed nearly all of them were",
    "start": "1447380",
    "end": "1454039"
  },
  {
    "text": "associated with a single customer data set which we're going to call clothesly",
    "start": "1454039",
    "end": "1459340"
  },
  {
    "text": "which is not a real website so you know if closely was doing maybe",
    "start": "1459340",
    "end": "1464840"
  },
  {
    "text": "500 updates per second the next largest customer was maybe doing five and all",
    "start": "1464840",
    "end": "1470900"
  },
  {
    "text": "other customers together on that one pod were doing maybe 30. so just enormous",
    "start": "1470900",
    "end": "1477159"
  },
  {
    "text": "enormous Gap in in the scale here and we were remember at this time sending like",
    "start": "1477159",
    "end": "1483140"
  },
  {
    "text": "8 billion push notifications per day our sdks are in something like 9 million mobile apps so for for One customer to",
    "start": "1483140",
    "end": "1491059"
  },
  {
    "text": "be dominating like one of our boxes that indicated that something was pretty wrong",
    "start": "1491059",
    "end": "1497240"
  },
  {
    "text": "and since we had it available to us we also grouped by the subscription ID the the row ID right",
    "start": "1497240",
    "end": "1502880"
  },
  {
    "text": "and then we noticed something really strange uh most of the updates were going not just",
    "start": "1502880",
    "end": "1509299"
  },
  {
    "text": "to closely as as a data set they're actually going to a single subscription",
    "start": "1509299",
    "end": "1514760"
  },
  {
    "text": "a single device and close these data set and they had around 5 million devices",
    "start": "1514760",
    "end": "1521299"
  },
  {
    "text": "so a data set of five million devices in you know our entire database of",
    "start": "1521299",
    "end": "1526940"
  },
  {
    "text": "billions of devices and we noticed one of them as as causing problems for us so this was very surprising to us like",
    "start": "1526940",
    "end": "1534980"
  },
  {
    "text": "you know why is this one device showing up why would why do I care about it",
    "start": "1534980",
    "end": "1540679"
  },
  {
    "text": "um so we started looking at what updates were happening right uh",
    "start": "1540679",
    "end": "1546919"
  },
  {
    "text": "we could have just you know grabbed a subscription ID and like disabled updates to this one device and you know",
    "start": "1546919",
    "end": "1553400"
  },
  {
    "text": "moved on and said you know they're doing something weird and we don't care moving on um but we wanted to figure out what was",
    "start": "1553400",
    "end": "1560539"
  },
  {
    "text": "happening with this app so we could try and prevent it from happening with our other customers right we looked at what",
    "start": "1560539",
    "end": "1566960"
  },
  {
    "text": "the updates that we're sending to this one record and it was it was tons of updates to individual fields and these",
    "start": "1566960",
    "end": "1573200"
  },
  {
    "text": "updates seem to be incompatible with each other right so the same field was being constantly reset to very very",
    "start": "1573200",
    "end": "1580039"
  },
  {
    "text": "different values things like the location record was moving like Chicago to Tokyo to San Francisco to Brisbane",
    "start": "1580039",
    "end": "1588440"
  },
  {
    "text": "um it was it was very uh very confusing and we we knew that the customer like",
    "start": "1588440",
    "end": "1593900"
  },
  {
    "text": "must be doing something very wrong in their sdks but we really wanted to try and understand what it was",
    "start": "1593900",
    "end": "1600380"
  },
  {
    "text": "so we looked at the we looked at the row in postgres right and we're just like constantly spamming you know select star",
    "start": "1600380",
    "end": "1607279"
  },
  {
    "text": "from this row to try and see what's happening in these fields we noticed that you know basically every field was",
    "start": "1607279",
    "end": "1612679"
  },
  {
    "text": "changing constantly there were a couple fields that maintained uh consistency",
    "start": "1612679",
    "end": "1617779"
  },
  {
    "text": "was always marked as an email device type and the the identifier which is",
    "start": "1617779",
    "end": "1622820"
  },
  {
    "text": "like the way that we actually contact the device so for a push device this is like the token we get from Apple or",
    "start": "1622820",
    "end": "1628460"
  },
  {
    "text": "Google or whoever but for an email record it's just the email that we use to to send out",
    "start": "1628460",
    "end": "1634940"
  },
  {
    "text": "uh the identifier was always set to admin at closely and this started setting off alarm bells",
    "start": "1634940",
    "end": "1642320"
  },
  {
    "text": "for us it started to sort of Click into place and let me let me explain why so one single started as a push",
    "start": "1642320",
    "end": "1648980"
  },
  {
    "start": "1647000",
    "end": "1814000"
  },
  {
    "text": "notification company so a lot of the stuff that we have is very Centric around push notifications and and the",
    "start": "1648980",
    "end": "1654980"
  },
  {
    "text": "device and the SDK right but uh we started branching out from there we tried to try to be like more than just",
    "start": "1654980",
    "end": "1661580"
  },
  {
    "text": "push notifications and uh you know we're trying to let people do omni-channel",
    "start": "1661580",
    "end": "1667279"
  },
  {
    "text": "messaging to their customers because it's a very it's very cool and inward right now um and the first platform that we added",
    "start": "1667279",
    "end": "1673940"
  },
  {
    "text": "that wasn't just push was email and the way that we let our customers access this",
    "start": "1673940",
    "end": "1679279"
  },
  {
    "text": "um was through a method in the sdks called set email and when you call the set email method",
    "start": "1679279",
    "end": "1686240"
  },
  {
    "text": "you provided it with an email address and what it would do is it would send a",
    "start": "1686240",
    "end": "1691400"
  },
  {
    "text": "request to our servers that would either look up or create if it didn't exist an",
    "start": "1691400",
    "end": "1697279"
  },
  {
    "text": "email record with a given email address um and then it would link the push",
    "start": "1697279",
    "end": "1703520"
  },
  {
    "text": "notification record that's associated with that SDK install to the email record",
    "start": "1703520",
    "end": "1709159"
  },
  {
    "text": "um by setting a property on the push record to point to the email record and then it's going to store that uh the",
    "start": "1709159",
    "end": "1716360"
  },
  {
    "text": "email subscription also in the SDK and every time the SDK issues an update",
    "start": "1716360",
    "end": "1722539"
  },
  {
    "text": "like of a property it's going to send a duplicate update that's associated with the email",
    "start": "1722539",
    "end": "1728539"
  },
  {
    "text": "subscription so we checked this like parent property",
    "start": "1728539",
    "end": "1735440"
  },
  {
    "text": "we checked to see okay how many subscriptions does this closely have they have around 5 million subscriptions",
    "start": "1735440",
    "end": "1740480"
  },
  {
    "text": "and how many of those subscriptions have their parent properties set to this one admin at closely record it was almost",
    "start": "1740480",
    "end": "1748039"
  },
  {
    "text": "every single one of them right almost every single one of them so what does this mean it means that on every",
    "start": "1748039",
    "end": "1754640"
  },
  {
    "text": "single SDK install that they had they had a line that just said like set email admin that closely right so they",
    "start": "1754640",
    "end": "1762740"
  },
  {
    "text": "probably misunderstood our docs they probably thought you were supposed to call set email with like an email of the",
    "start": "1762740",
    "end": "1768080"
  },
  {
    "text": "administrators of the app that you know one signal could use to contact you if there was a problem or something right it's kind of an understandable",
    "start": "1768080",
    "end": "1775179"
  },
  {
    "text": "misunderstanding to make but it was not one that we had ever thought somebody would do right",
    "start": "1775179",
    "end": "1782240"
  },
  {
    "text": "so yeah they just had a ton of Records which were all linked to this one Central email record",
    "start": "1782240",
    "end": "1788659"
  },
  {
    "text": "so why why is this why is this really a problem what do we care why am I talking about it right",
    "start": "1788659",
    "end": "1795200"
  },
  {
    "text": "we have a Kafka queue that's full of a lot of updates for this one individual",
    "start": "1795200",
    "end": "1802460"
  },
  {
    "text": "subscription this one individual Rove in postgres right",
    "start": "1802460",
    "end": "1807679"
  },
  {
    "text": "how does that translate into you know basically all of our concurrency going to nil",
    "start": "1807679",
    "end": "1814640"
  },
  {
    "start": "1814000",
    "end": "2137000"
  },
  {
    "text": "why is this a problem imagine for a moment that we have you",
    "start": "1814640",
    "end": "1820399"
  },
  {
    "text": "know our three sub-partition workers and they have a basically Fair distribution of messages on them they're all going to",
    "start": "1820399",
    "end": "1826460"
  },
  {
    "text": "you know the different cues and as messages are completed on one cue",
    "start": "1826460",
    "end": "1831740"
  },
  {
    "text": "you know as say this red message is completed here at the beginning of the queue we can shift all the cues forward",
    "start": "1831740",
    "end": "1837380"
  },
  {
    "text": "grab some more messages and hydrate the rest of the cues right but what happens if we put our thumb on the scale kind of",
    "start": "1837380",
    "end": "1843500"
  },
  {
    "text": "like closely did here what if what if we have a ton of updates that are just going to to one subscription right",
    "start": "1843500",
    "end": "1849860"
  },
  {
    "text": "they're all going to be hashing to the same value they're all going to wind up on the same Q right so this means that",
    "start": "1849860",
    "end": "1856640"
  },
  {
    "text": "the the other sub partition cues that we have are going to be pretty dry there's not going to be a lot of messages in",
    "start": "1856640",
    "end": "1862220"
  },
  {
    "text": "there so in this case Q2 up here just has a single message in it right when that",
    "start": "1862220",
    "end": "1868460"
  },
  {
    "text": "message is processed If the message at the beginning of the queue which is associated with with q0",
    "start": "1868460",
    "end": "1874940"
  },
  {
    "text": "is not finished processing by then Q2 is just going to be dry there's going to be no messages in there and the",
    "start": "1874940",
    "end": "1881659"
  },
  {
    "text": "worker is not going to have anything to process right um",
    "start": "1881659",
    "end": "1887240"
  },
  {
    "text": "so this means that your your consumer is going to be sitting there and all of the beautiful workers that you have waiting",
    "start": "1887240",
    "end": "1893539"
  },
  {
    "text": "for more messages to process are not going to have anything to do and your process rate is going to go way down",
    "start": "1893539",
    "end": "1902120"
  },
  {
    "text": "um and in reality it was slightly worse than this because the the limit on the",
    "start": "1902120",
    "end": "1909200"
  },
  {
    "text": "number of messages that we would store in memory was applied not at the partition level but it was actually",
    "start": "1909200",
    "end": "1914600"
  },
  {
    "text": "applied at the consumer level at the process level so this meant that as these cues started",
    "start": "1914600",
    "end": "1920539"
  },
  {
    "text": "to back up this this one partition was sort of eating up all the memory capacity of our",
    "start": "1920539",
    "end": "1927860"
  },
  {
    "text": "consumers so the rest of the partitions that we had assigned were also having",
    "start": "1927860",
    "end": "1932960"
  },
  {
    "text": "all of their cues run dry so basically the only thing this you know this one",
    "start": "1932960",
    "end": "1938899"
  },
  {
    "text": "consumer instance was doing was processing updates for this one row",
    "start": "1938899",
    "end": "1944059"
  },
  {
    "text": "and you know we had thousands of workers that were associated with this one process and only one of those workers",
    "start": "1944059",
    "end": "1949520"
  },
  {
    "text": "was really doing anything uh yeah so so that was that was not so good",
    "start": "1949520",
    "end": "1955640"
  },
  {
    "text": "so what did we do how did we how do we like solve this problem immediately and how did we fix it like sort of more",
    "start": "1955640",
    "end": "1961880"
  },
  {
    "text": "broadly long term right the initial thing we did was skip these update messages we're like all right if you",
    "start": "1961880",
    "end": "1967220"
  },
  {
    "text": "have this subscription ID we're throwing your update out like sorry",
    "start": "1967220",
    "end": "1973100"
  },
  {
    "text": "um this this road definitely does not need to know that you know this email record",
    "start": "1973100",
    "end": "1979279"
  },
  {
    "text": "is constantly having its location change that's not relevant it's not important and when we did this the process time",
    "start": "1979279",
    "end": "1985399"
  },
  {
    "text": "you know for each message went from like 20 milliseconds to you know under one millisecond so that meant we were able",
    "start": "1985399",
    "end": "1992179"
  },
  {
    "text": "to churn through that leg very very quickly you're not able unfortunately to just delete messages from Kafka you just",
    "start": "1992179",
    "end": "1998179"
  },
  {
    "text": "have to sort of play them and and skip over them but that's very very fast",
    "start": "1998179",
    "end": "2004720"
  },
  {
    "text": "uh the next thing we did was we fixed the limiting of messages so we took the",
    "start": "2004720",
    "end": "2009820"
  },
  {
    "text": "we took that like Global message cap and we scoped it in to be a per partition message cap so if this were to happen",
    "start": "2009820",
    "end": "2016720"
  },
  {
    "text": "again in the future we would not let all of the partitions have their cues run dry it would just be scoped into a",
    "start": "2016720",
    "end": "2023260"
  },
  {
    "text": "single partition so we we took the sort of blast radius in that problem and we made it much smaller",
    "start": "2023260",
    "end": "2029019"
  },
  {
    "text": "and the next thing that we did was we said okay how were they able to enter into this situation in the first place well they they linked too many records",
    "start": "2029019",
    "end": "2037360"
  },
  {
    "text": "together so we we set limits on how many records you could you could link together because we didn't really think",
    "start": "2037360",
    "end": "2042640"
  },
  {
    "text": "it was reasonable to have you know 5 million subscriptions with the same email address associated with them right",
    "start": "2042640",
    "end": "2050560"
  },
  {
    "text": "and uh what did we learn what did what did we sort of take away from this uh",
    "start": "2050560",
    "end": "2055599"
  },
  {
    "text": "users are basically always more creative than engineering and product teams right they will find really fun and",
    "start": "2055599",
    "end": "2062618"
  },
  {
    "text": "interesting ways to misuse your system that you never considered in the past uh",
    "start": "2062619",
    "end": "2069280"
  },
  {
    "text": "so the best you can sort of do is like put limits on on what the users do and when you're building those limits",
    "start": "2069280",
    "end": "2075878"
  },
  {
    "text": "you want to try and scope them in as small as possible so that you know a limit in one place isn't going to have a",
    "start": "2075879",
    "end": "2081099"
  },
  {
    "text": "a larger effect than intended and the other thing we we gained was like the centralized observability the",
    "start": "2081099",
    "end": "2087580"
  },
  {
    "text": "centralized locking and the structured logging that we put in place was was really instrumental to solving this problem",
    "start": "2087580",
    "end": "2094618"
  },
  {
    "text": "um so if if you or your company is interested in uh sending push",
    "start": "2095500",
    "end": "2100720"
  },
  {
    "text": "notifications and uh not linking 5 million email addresses to uh to a",
    "start": "2100720",
    "end": "2107440"
  },
  {
    "text": "single user uh check out one signal uh we're at onesignal.com and if you're",
    "start": "2107440",
    "end": "2112960"
  },
  {
    "text": "interested in contacting myself uh all my information is on my website at",
    "start": "2112960",
    "end": "2118619"
  },
  {
    "text": "lillimara.xyz and I think with that we have a couple minutes for questions",
    "start": "2118619",
    "end": "2124390"
  },
  {
    "text": "[Applause] thank you",
    "start": "2124390",
    "end": "2130000"
  },
  {
    "text": "okay who's got a question",
    "start": "2130000",
    "end": "2133320"
  },
  {
    "start": "2137000",
    "end": "2522000"
  },
  {
    "text": "hi Lily uh great talk thank you thank you uh you mentioned that you had merely",
    "start": "2137619",
    "end": "2143260"
  },
  {
    "text": "or mostly merely metrics for observability um have you",
    "start": "2143260",
    "end": "2148420"
  },
  {
    "text": "built any more observability into your Kafka yes so after after this sort of",
    "start": "2148420",
    "end": "2155079"
  },
  {
    "text": "debacle I like use this as a jumping off point to be like you know observability is important and we can't like ignore it",
    "start": "2155079",
    "end": "2160780"
  },
  {
    "text": "anymore so we started off with just structured centralized logging and then uh maybe about a year and a half ago we",
    "start": "2160780",
    "end": "2168460"
  },
  {
    "text": "started investing really heavily in tracing um using open Telemetry and sending all",
    "start": "2168460",
    "end": "2174339"
  },
  {
    "text": "of our stuff to Honeycomb and that has been just like a total game changer for",
    "start": "2174339",
    "end": "2179380"
  },
  {
    "text": "us um it's been it's been really instrumental in not just like debugging",
    "start": "2179380",
    "end": "2186040"
  },
  {
    "text": "stuff that's causing problems for us in production but like even in the development process as we try and",
    "start": "2186040",
    "end": "2191380"
  },
  {
    "text": "understand how Stuff moves through our big distributed system so yeah tracing",
    "start": "2191380",
    "end": "2196660"
  },
  {
    "text": "logs metrics all very important thank you very much",
    "start": "2196660",
    "end": "2202020"
  },
  {
    "text": "uh hi thanks for the presentation so you mentioned commit batching uh sorry what",
    "start": "2208240",
    "end": "2213460"
  },
  {
    "text": "was that you mentioned commit batching to write this to to the postgres database",
    "start": "2213460",
    "end": "2219240"
  },
  {
    "text": "and um and you promise to go into the details how you do uh how you make sure item",
    "start": "2219240",
    "end": "2227380"
  },
  {
    "text": "potency is yes maintained if your memory memory cage fails sure yeah so basically",
    "start": "2227380",
    "end": "2234099"
  },
  {
    "text": "each subscription Row in our database also has a field on it called Kafka offset right and so every time we write",
    "start": "2234099",
    "end": "2243700"
  },
  {
    "text": "a row we do one of these update calls we also set the um",
    "start": "2243700",
    "end": "2248980"
  },
  {
    "text": "we also set this Kafka offset property to be the offset of the message that's",
    "start": "2248980",
    "end": "2255460"
  },
  {
    "text": "performing that update right and then in that call we we have a condition that's",
    "start": "2255460",
    "end": "2261339"
  },
  {
    "text": "like you know the the wear condition for that call looks like you know where app ID equals this where subscription ID",
    "start": "2261339",
    "end": "2267220"
  },
  {
    "text": "equals that and Kafka offset is under this value right so if we have already",
    "start": "2267220",
    "end": "2274720"
  },
  {
    "text": "played a message uh for that row that has that offset or has a greater offset",
    "start": "2274720",
    "end": "2280480"
  },
  {
    "text": "then we're just going to skip it okay",
    "start": "2280480",
    "end": "2288099"
  },
  {
    "text": "it sounds like you had to build a kind of a Contraption around Kafka to make it do what you wanted to do if you were",
    "start": "2294640",
    "end": "2300940"
  },
  {
    "text": "starting again today would you start with Kafka or would you do something else with some some other system",
    "start": "2300940",
    "end": "2306700"
  },
  {
    "text": "that's a good question that is not something that I've thought about um",
    "start": "2306700",
    "end": "2311740"
  },
  {
    "text": "I think we have developed the system to be pretty robust and you know very high",
    "start": "2311740",
    "end": "2318099"
  },
  {
    "text": "performance uh I think the benefits that we get from coffee you know this talk was mostly",
    "start": "2318099",
    "end": "2323380"
  },
  {
    "text": "about like how complicated it is and how smart we are for coming up with this great system but like really",
    "start": "2323380",
    "end": "2329020"
  },
  {
    "text": "um you know as most talks are right but um you know really Kafka has been quite",
    "start": "2329020",
    "end": "2334900"
  },
  {
    "text": "beneficial for us like one of the key benefits uh that I you know sort of briefly touched on was like the fact",
    "start": "2334900",
    "end": "2341320"
  },
  {
    "text": "that we can dump all of our updates into one place and then have lots of things that are consuming that same update",
    "start": "2341320",
    "end": "2346780"
  },
  {
    "text": "stream has been really really beneficial for us right so this is powering things like data Integrations with with",
    "start": "2346780",
    "end": "2352720"
  },
  {
    "text": "third-party providers it's powering these property updates it's powering",
    "start": "2352720",
    "end": "2358060"
  },
  {
    "text": "um like an events driven like drag and drop uh node programming tool basically",
    "start": "2358060",
    "end": "2365020"
  },
  {
    "text": "that our customers can use uh so it's it's really been quite beneficial for us",
    "start": "2365020",
    "end": "2370599"
  },
  {
    "text": "and we have also had essentially zero operational issues with Kafka itself",
    "start": "2370599",
    "end": "2377859"
  },
  {
    "text": "they've all just been sort of around like our our consumers like the things reading off of it",
    "start": "2377859",
    "end": "2383920"
  },
  {
    "text": "um versus something like you know uh postgres which we also operate at a",
    "start": "2383920",
    "end": "2389500"
  },
  {
    "text": "pretty large scale um that is just like constantly on fire and and needs like constant you know",
    "start": "2389500",
    "end": "2396099"
  },
  {
    "text": "hand holding and tuning so I think the benefits that we've seen from Kafka outweigh the costs that we've",
    "start": "2396099",
    "end": "2401800"
  },
  {
    "text": "paid to use it yeah other questions",
    "start": "2401800",
    "end": "2407859"
  },
  {
    "text": "I have a follow-up question on that yeah early on you mentioned the uh the mild annoyance of uh repetitioning your your",
    "start": "2407859",
    "end": "2415119"
  },
  {
    "text": "captain scream does that require you to to basically shut down the producers or",
    "start": "2415119",
    "end": "2420280"
  },
  {
    "text": "consumers and for how long does it does it take for that kind of operation yeah",
    "start": "2420280",
    "end": "2425500"
  },
  {
    "text": "so I guess it requires you to shut down consumers for a very brief time like you",
    "start": "2425500",
    "end": "2430839"
  },
  {
    "text": "basically just have to to redeploy your consumers right so you can you can take your consumers and point them from the",
    "start": "2430839",
    "end": "2437260"
  },
  {
    "text": "old topic uh to the new topic and then start up your process that's dumping",
    "start": "2437260",
    "end": "2444040"
  },
  {
    "text": "messages from the old topic into the new topic so I guess you know as long as it takes to start that process that's",
    "start": "2444040",
    "end": "2450040"
  },
  {
    "text": "dumping messages from the old topic into the new topic um",
    "start": "2450040",
    "end": "2455079"
  },
  {
    "text": "I don't recall exactly how much downtime was associated with that you know",
    "start": "2455079",
    "end": "2461280"
  },
  {
    "text": "any other questions",
    "start": "2462940",
    "end": "2465900"
  },
  {
    "text": "did your customer ever solve the issue and what was the impact to them with having all these emails like being the",
    "start": "2472540",
    "end": "2479920"
  },
  {
    "text": "same I don't know if our support team was ever able to get in contact with them yeah so I don't I don't know if they",
    "start": "2479920",
    "end": "2486400"
  },
  {
    "text": "ever figured out that they were doing something really wrong or not I think I remember talking to them maybe",
    "start": "2486400",
    "end": "2492339"
  },
  {
    "text": "a month after we discovered this problem and they said yeah we've never been able to get a response from them so I don't",
    "start": "2492339",
    "end": "2497740"
  },
  {
    "text": "think we ever heard back",
    "start": "2497740",
    "end": "2500880"
  },
  {
    "text": "we have a we have a large number of customers who are you know just using our free plan I don't remember if these",
    "start": "2503260",
    "end": "2509680"
  },
  {
    "text": "guys were paying us or not but um it's it's fairly common for us to like not be able to get in contact",
    "start": "2509680",
    "end": "2517180"
  },
  {
    "text": "with our customers and we have you know just huge numbers of of customers",
    "start": "2517180",
    "end": "2523140"
  },
  {
    "start": "2522000",
    "end": "2532000"
  },
  {
    "text": "anybody else awesome well thank you so much for for attending everybody",
    "start": "2523540",
    "end": "2530350"
  },
  {
    "text": "[Applause]",
    "start": "2530350",
    "end": "2532889"
  }
]