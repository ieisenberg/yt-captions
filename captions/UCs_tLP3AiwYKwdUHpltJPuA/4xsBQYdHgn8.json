[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "[Music] so I'm going to try a slightly different",
    "start": "6990",
    "end": "12920"
  },
  {
    "text": "format for presenting this information than I normally do in part because I got an opportunity to Dr up my stuffed",
    "start": "12920",
    "end": "18439"
  },
  {
    "text": "animals in outfits um and it's why there isn't Magic uh which is kind of",
    "start": "18439",
    "end": "23760"
  },
  {
    "text": "depressing but then we'll we'll look at the magic and it's really more about how the magic uh in programming inevitably",
    "start": "23760",
    "end": "30320"
  },
  {
    "text": "steals our lunch money and makes our jobs fail um so there is some magic it",
    "start": "30320",
    "end": "35719"
  },
  {
    "text": "just it comes at a price uh and we should understand that price um so yeah I'm Holden U my preferred pronouns are",
    "start": "35719",
    "end": "42440"
  },
  {
    "start": "39000",
    "end": "97000"
  },
  {
    "text": "she or her I work for IBM spark Technology Center in San Francisco I'm really glad they pay me money to come",
    "start": "42440",
    "end": "48920"
  },
  {
    "text": "and talk to wonderful people like you um and I get to do a lot of Open Source software and I'm a a paty spark",
    "start": "48920",
    "end": "55879"
  },
  {
    "text": "committer in part because they pay me and I get to have this time to focus on spark so I'm really happy for that uh",
    "start": "55879",
    "end": "63079"
  },
  {
    "text": "I've worked with a bunch of other companies like Alpine data bricks Google and for square uh and I'm a co-author of",
    "start": "63079",
    "end": "68439"
  },
  {
    "text": "several spark books my latest one though is high performance spark and I do get the most royalties for that so that is",
    "start": "68439",
    "end": "74479"
  },
  {
    "text": "the best spark book um and uh I really hope you buy it if",
    "start": "74479",
    "end": "80159"
  },
  {
    "text": "you want to follow me on Twitter for a Canadian who's wondering why she's living in America uh you can find me on",
    "start": "80159",
    "end": "86159"
  },
  {
    "text": "Twitter and the slides from today's talk I'll give them to Dean but I'll also put them on my slides share and I've got",
    "start": "86159",
    "end": "93119"
  },
  {
    "text": "some other more sort of standard links if anyone's interested um also I want to be like",
    "start": "93119",
    "end": "98680"
  },
  {
    "start": "97000",
    "end": "129000"
  },
  {
    "text": "super clear I'm like trans I'm queer I'm Canadian I'm here on a work visa I'm on an H1B um and I also consider myself a",
    "start": "98680",
    "end": "106040"
  },
  {
    "text": "part of the leather community and if you don't know other people like me in software you should come out to San Francisco more often um but also like",
    "start": "106040",
    "end": "113880"
  },
  {
    "text": "we're here and like we're nice people we're maybe not as scary as television implies I'm not stealing your job I I",
    "start": "113880",
    "end": "119960"
  },
  {
    "text": "press the same buttons that you do um and you know maybe I end my sentences in",
    "start": "119960",
    "end": "126039"
  },
  {
    "text": "a more often but we're fine we can all be friends um so oh yeah skipped a few",
    "start": "126039",
    "end": "132680"
  },
  {
    "start": "129000",
    "end": "194000"
  },
  {
    "text": "slides um I should not skip the slide from my glorious employer who can do no wrong um the IBM Spar Technology Center",
    "start": "132680",
    "end": "140000"
  },
  {
    "text": "uh in San Francisco is amazing uh I I actually honestly really like it um",
    "start": "140000",
    "end": "145160"
  },
  {
    "text": "there's a lot of developers who work with me there and we all focus on open source um not all of us are directly on",
    "start": "145160",
    "end": "151040"
  },
  {
    "text": "spark but a bunch of people work on things that are built on top of spark um and we also have a design team",
    "start": "151040",
    "end": "157040"
  },
  {
    "text": "associated with us and they do a lot of work on sort of related projects uh notebook type things and and experiences",
    "start": "157040",
    "end": "164480"
  },
  {
    "text": "they tend to work on things which are a little more commercially relevant so you know when the layoffs come I'll be off",
    "start": "164480",
    "end": "169959"
  },
  {
    "text": "in the corner and they'll still be around um anyway sorry badly off joke um",
    "start": "169959",
    "end": "177159"
  },
  {
    "text": "yeah if you buy things from IBM if you have have a Mainframe if you have a support contract keep doing that um",
    "start": "177159",
    "end": "184799"
  },
  {
    "text": "someone told me that the system 360 stuff essentially pays my salary so if you are in any way involved in that in a",
    "start": "184799",
    "end": "190799"
  },
  {
    "text": "bank just keep buying that it's amazing um right okay so you you've been",
    "start": "190799",
    "end": "196519"
  },
  {
    "start": "194000",
    "end": "268000"
  },
  {
    "text": "laughing at my jokes so you're clearly nice people or at least willing to pretend to be um I'm really curious how",
    "start": "196519",
    "end": "202560"
  },
  {
    "text": "many people are new to Apache spark this is like your first intro to spark oh wow that's a lot more people than I thought",
    "start": "202560",
    "end": "208400"
  },
  {
    "text": "this is really fun and exciting okay yes um I am a little worried though I hope this talk does not scare you away",
    "start": "208400",
    "end": "215000"
  },
  {
    "text": "from spark uh because we are going to talk about the parts of spark which don't often get mentioned because they",
    "start": "215000",
    "end": "221560"
  },
  {
    "text": "don't work so well um but most of spark works much better than the pieces we're going to be talking about today we're",
    "start": "221560",
    "end": "227799"
  },
  {
    "text": "talking about it shortcomings and in learning that you can hopefully learn that the other parts are pretty awesome",
    "start": "227799",
    "end": "233799"
  },
  {
    "text": "too um how many people are Scala Developers okay Dean yeah thank you um",
    "start": "233799",
    "end": "242640"
  },
  {
    "text": "how many people call themselves Java developers good representation and",
    "start": "242640",
    "end": "248120"
  },
  {
    "text": "python developers less than I thought but we python developers we can be friends um I",
    "start": "248120",
    "end": "254560"
  },
  {
    "text": "work in Scola mostly but I I do a lot of work with the python Community okay cool thank you um there's going to be a",
    "start": "254560",
    "end": "260479"
  },
  {
    "text": "mixture of examples and if there's something that you don't understand because it's in a different programming language stop me and we'll go over it",
    "start": "260479",
    "end": "266520"
  },
  {
    "text": "together it's fine um so for people who are new to spark uh you're presumably here because",
    "start": "266520",
    "end": "272680"
  },
  {
    "start": "268000",
    "end": "384000"
  },
  {
    "text": "like you saw something cool about spark um but if you're not super sold it's really awesome it's it's a general",
    "start": "272680",
    "end": "278000"
  },
  {
    "text": "purpose distributed system it's not just for like solving one particular task you can do whatever you want with it um you",
    "start": "278000",
    "end": "284039"
  },
  {
    "text": "can provide arbitrary functions and do stuff uh it's an Apache project which I think is really important it means that",
    "start": "284039",
    "end": "289960"
  },
  {
    "text": "it's not exactly controlled by a single company um although there are components",
    "start": "289960",
    "end": "295680"
  },
  {
    "text": "of it which are more controlled by individual companies than others um but it means that there sort of this",
    "start": "295680",
    "end": "300840"
  },
  {
    "text": "continuation if any of the big companies that are working on spark decide to go away there's still a foundation behind",
    "start": "300840",
    "end": "306639"
  },
  {
    "text": "it um and I think that's really important for sort of just you know trusting your software um a lot of",
    "start": "306639",
    "end": "311720"
  },
  {
    "text": "people come to spark because they're like wow my Ma's job is taking 18 hours um and so they go how can I make spark",
    "start": "311720",
    "end": "318160"
  },
  {
    "text": "fast how can I make my job faster and then they see this thing which claims to be 100 times faster which is obviously",
    "start": "318160",
    "end": "324080"
  },
  {
    "text": " um but they go like well a hundred they probably wouldn't lie that",
    "start": "324080",
    "end": "329160"
  },
  {
    "text": "much um and then they try spark and then then we have them yes um okay the other",
    "start": "329160",
    "end": "336120"
  },
  {
    "text": "way that I think people find spark a lot is they're like My panda's data frame is running out of memory I want to",
    "start": "336120",
    "end": "341520"
  },
  {
    "text": "distributed data frame and then they search for distributed data frame on the internet and then the spark people they're like come yes do not look too",
    "start": "341520",
    "end": "349319"
  },
  {
    "text": "closely um and then we we sneak in the python users and it's delightful um and there's sort of two",
    "start": "349319",
    "end": "355680"
  },
  {
    "text": "core abstractions to spark the there's this sort of Legacy one called Rd s um that are still going to be very much",
    "start": "355680",
    "end": "361720"
  },
  {
    "text": "maintained and developed um and they they just allow us to do sort of really raw types of computation and then",
    "start": "361720",
    "end": "367759"
  },
  {
    "text": "there's data sets which is the new name for data frames because software people love changing the names of things um so",
    "start": "367759",
    "end": "374440"
  },
  {
    "text": "we have data sets and they they give us a data frame like interface although there's many many asterisks after the",
    "start": "374440",
    "end": "379919"
  },
  {
    "text": "word like um and possibly you know a very sad cat um or two so right yeah",
    "start": "379919",
    "end": "385840"
  },
  {
    "start": "384000",
    "end": "435000"
  },
  {
    "text": "people come to spark because they're like how hard could it be to learn Spark while I'm waiting for my map ruce job to",
    "start": "385840",
    "end": "391639"
  },
  {
    "text": "finish um and honestly you can get the basics in the time it takes for your map",
    "start": "391639",
    "end": "396880"
  },
  {
    "text": "ruce job to finish right like you don't even have to ask your boss for training just like read this while you're waiting",
    "start": "396880",
    "end": "403160"
  },
  {
    "text": "for your map ruce job to finish are there any map produ devs in the room okay well please don't murder me um",
    "start": "403160",
    "end": "411639"
  },
  {
    "text": "and yeah this is the other one my my data doesn't fit in memory on my fancy machine anymore and they won't let me",
    "start": "411639",
    "end": "416960"
  },
  {
    "text": "buy bigger machines or I there's not a larger Amazon instance type how do I solve my problems now um and the other",
    "start": "416960",
    "end": "424840"
  },
  {
    "text": "reason they come is there's a little bit of magic um and the magic will bite Us in the posterior um but it's it'll be",
    "start": "424840",
    "end": "431919"
  },
  {
    "text": "fun um until it explodes right so what is the magic of",
    "start": "431919",
    "end": "438080"
  },
  {
    "start": "435000",
    "end": "556000"
  },
  {
    "text": "spark right um the magic of Spark all sort of stems from this we call it the",
    "start": "438080",
    "end": "443160"
  },
  {
    "text": "directed acyclic graph which makes it sound all fancy in computer sciency um very good for writing papers uh The Spar",
    "start": "443160",
    "end": "449680"
  },
  {
    "text": "paper actually won like a best paper award um used a lot of instances of the word graph very solid um but we can",
    "start": "449680",
    "end": "457120"
  },
  {
    "text": "really just think of this as spark is a is a lazy system and instead of doing computation immediately when we ask it",
    "start": "457120",
    "end": "464199"
  },
  {
    "text": "what it does is it builds up this like idea of all the things we've asked it to do and then once we sort of force it to",
    "start": "464199",
    "end": "469919"
  },
  {
    "text": "do work much like a teenager um it collapses all of that into the absolute minimum required to not get grounded um",
    "start": "469919",
    "end": "478360"
  },
  {
    "text": "in data frames instead of calling this a directed a cyclic graph we call it a query plan um because it's more like SQL",
    "start": "478360",
    "end": "485599"
  },
  {
    "text": "um and in some ways there's there's some very important differences and we'll look at those um so this this dag in",
    "start": "485599",
    "end": "492080"
  },
  {
    "text": "addition to being used for sort of this really cool Optimizer which takes all",
    "start": "492080",
    "end": "497120"
  },
  {
    "text": "the things we ask spark to do and and shove it down into one lazy thing is we use this to recover from errors in",
    "start": "497120",
    "end": "503919"
  },
  {
    "text": "distributed systems our our really big problem is that failure is a first class citizen right like we can make",
    "start": "503919",
    "end": "510840"
  },
  {
    "text": "distributed systems really easily that run on Reliable Hardware but it turns out Reliable Hardware is expensive and",
    "start": "510840",
    "end": "516599"
  },
  {
    "text": "the reliable network does not exist um and so we have to be able to recover",
    "start": "516599",
    "end": "521760"
  },
  {
    "text": "from failures and so spark uses the same information that it uses for the optimizer to go like I encountered an",
    "start": "521760",
    "end": "528279"
  },
  {
    "text": "error I'm going to recompute this piece of data and this is very different than the map ruce approach which is when I",
    "start": "528279",
    "end": "534399"
  },
  {
    "text": "compute something I'm going to write it out to three different discs distributed across my data center uh and IO is kind",
    "start": "534399",
    "end": "540079"
  },
  {
    "text": "of slow and io on three remote discs is like really slow so using this uh dag to",
    "start": "540079",
    "end": "546240"
  },
  {
    "text": "do our recompute is going to be really fast awesome um and then there's some other",
    "start": "546240",
    "end": "552120"
  },
  {
    "text": "magic it's not super exciting uh we'll we'll we'll look at some STS okay all",
    "start": "552120",
    "end": "557200"
  },
  {
    "start": "556000",
    "end": "646000"
  },
  {
    "text": "right so rdd is is the core bit it's the resilient distributed data set um notably it is not the say it's not",
    "start": "557200",
    "end": "564200"
  },
  {
    "text": "saying that the data set is not resilient um this is just the wonders of software Developers owed to name things",
    "start": "564200",
    "end": "570800"
  },
  {
    "text": "um data sets are actually built on top of rdds despite what the naming would imply um if there's pandz users and",
    "start": "570800",
    "end": "578560"
  },
  {
    "text": "you're seeing like data frame you're probably really excited and you're like I can Port my code super easily and no",
    "start": "578560",
    "end": "584920"
  },
  {
    "text": "no that's not going to work uh data frames totally have some data frame like operations but they don't have the same",
    "start": "584920",
    "end": "591399"
  },
  {
    "text": "ones that you're used to in pandas not not by a long shot uh we we'll talk about this but let's let's dive into the",
    "start": "591399",
    "end": "598279"
  },
  {
    "text": "first piece of magic together let's let's look at our dag so in spark rather",
    "start": "598279",
    "end": "604079"
  },
  {
    "text": "than sort of manipulating collections by mutating them our our collections are distributed and immutable and so what we",
    "start": "604079",
    "end": "611880"
  },
  {
    "text": "do when we want to like compute something we we obviously want to compute something um is we get a new",
    "start": "611880",
    "end": "617839"
  },
  {
    "text": "collection back so we transform our data and then we get back a new collection and then this collection doesn't really",
    "start": "617839",
    "end": "624560"
  },
  {
    "text": "exist um it's this collection is just like this plan of how to make this",
    "start": "624560",
    "end": "629880"
  },
  {
    "text": "collection if we ever get caught with the fact that we didn't make our collection um and the best collections",
    "start": "629880",
    "end": "635440"
  },
  {
    "text": "are the ones where you never write them out because I don't have to compute anything and you won't find any bugs in my software and I'm really excited about",
    "start": "635440",
    "end": "642079"
  },
  {
    "text": "that um but let's let's look at what one of these plans looks like um so this is maybe not the right",
    "start": "642079",
    "end": "649720"
  },
  {
    "start": "646000",
    "end": "709000"
  },
  {
    "text": "font size I'm sorry about that uh but we can see here that the dag is like all of these different Transformations are",
    "start": "649720",
    "end": "655959"
  },
  {
    "text": "linked together in a very just one after the other um and now this is this is a",
    "start": "655959",
    "end": "661680"
  },
  {
    "text": "really simple dag we could have an rdd which was comprised of many different input rdds um so we could join right",
    "start": "661680",
    "end": "669279"
  },
  {
    "text": "joins are pretty basic and then our our dag starts to get more complicated and the the stuff that we can do by",
    "start": "669279",
    "end": "675639"
  },
  {
    "text": "collapsing this down becomes even more important um but but the nice thing to think about is that even though we've",
    "start": "675639",
    "end": "681519"
  },
  {
    "text": "asked it to do a whole bunch of different things it's going to compute this in sort of a single Passover the data um we can see these stage",
    "start": "681519",
    "end": "688000"
  },
  {
    "text": "boundaries are are sort of where the differences um and we can look at query plans and we can see that query plans",
    "start": "688000",
    "end": "694200"
  },
  {
    "text": "have a little bit more information not information that you can read at a distance I'm sorry um but you can see",
    "start": "694200",
    "end": "701160"
  },
  {
    "text": "that there is a little more going on spark has a better idea of what's going on when we're working with data",
    "start": "701160",
    "end": "708240"
  },
  {
    "text": "sets um right so we're going to look at the stereotypical word count example um",
    "start": "708240",
    "end": "713880"
  },
  {
    "start": "709000",
    "end": "808000"
  },
  {
    "text": "and that is because as a licensed Big Data instructor I am required to include word count in every talk which may have",
    "start": "713880",
    "end": "720639"
  },
  {
    "text": "a new student and I am definitely required to include it if it has returning students um and this is this",
    "start": "720639",
    "end": "726959"
  },
  {
    "text": "is why word count is everywhere um but more seriously word count is actually a pretty good example for for Big Data um",
    "start": "726959",
    "end": "734120"
  },
  {
    "text": "because of this reduced by key step um but we can we can see our our first bit",
    "start": "734120",
    "end": "739160"
  },
  {
    "text": "is just loading our data um and this in this case it's loading a text file uh this text file could be terabytes of",
    "start": "739160",
    "end": "745440"
  },
  {
    "text": "data on hdfs or it could be kilobytes of data on your local MacBook uh either way",
    "start": "745440",
    "end": "751600"
  },
  {
    "text": "I get paid the same so I don't really care um then we go ahead and we tokenize your data um in this case we're using",
    "start": "751600",
    "end": "758720"
  },
  {
    "text": "flat map so we're just saying for each one of our inputs we could have multiple outputs and uh this is our rule to",
    "start": "758720",
    "end": "764480"
  },
  {
    "text": "tokenize it it works really well for English when I go overseas I pretend that I don't know that other languages",
    "start": "764480",
    "end": "769600"
  },
  {
    "text": "exist um and then it's it's almost reasonable um and then I've got this",
    "start": "769600",
    "end": "776079"
  },
  {
    "text": "word and then I need to make the word and the number one and then I tell spark like hey what's up sum up all these",
    "start": "776079",
    "end": "781120"
  },
  {
    "text": "numbers one and save it out um and this Reduce by key step is really important because this is the one step which",
    "start": "781120",
    "end": "787399"
  },
  {
    "text": "requires inter node communication if we just had all of the previous steps and we saved it out each node could just",
    "start": "787399",
    "end": "793399"
  },
  {
    "text": "compute the data locally and they wouldn't have to talk to each other and it would be really fast and like that",
    "start": "793399",
    "end": "798800"
  },
  {
    "text": "would be amazing but here we we actually word count does have something tricky in it just not that tricky",
    "start": "798800",
    "end": "806880"
  },
  {
    "text": "um and the important the important important thing is that until we get to this action spark hasn't done anything I",
    "start": "806880",
    "end": "814480"
  },
  {
    "start": "808000",
    "end": "862000"
  },
  {
    "text": "could literally tell it to load a file that doesn't exist from a input format which doesn't exist either and it would",
    "start": "814480",
    "end": "821279"
  },
  {
    "text": "just go like okay cool um and then it's only when we get to hear that I'll be like oh wait yeah you asked me to take",
    "start": "821279",
    "end": "827440"
  },
  {
    "text": "out the garbage but the garbage doesn't exist um so I'm done um and so this is this is the action which forces spark to",
    "start": "827440",
    "end": "834199"
  },
  {
    "text": "evaluate the rdd uh and this is really cool because we could do a lot of different maps and if you're used to",
    "start": "834199",
    "end": "840320"
  },
  {
    "text": "working in map reduce you're maybe used to spending a lot of time thinking about collapsing all of your things down into",
    "start": "840320",
    "end": "845639"
  },
  {
    "text": "single stages but in this case the computer does it for me and it's mostly right so that's okay that's good enough",
    "start": "845639",
    "end": "853320"
  },
  {
    "text": "um this is Magic okay not very convincing magic",
    "start": "853320",
    "end": "861519"
  },
  {
    "text": "um yeah so what what about this word count example is awesome our our map",
    "start": "861519",
    "end": "867040"
  },
  {
    "start": "862000",
    "end": "904000"
  },
  {
    "text": "stages got combined together our flat map stage got combined together um if one of our nodes failed in the middle it",
    "start": "867040",
    "end": "873759"
  },
  {
    "text": "would just go and be like that node was processing the information of this slice of the file I'm just going to go and",
    "start": "873759",
    "end": "879199"
  },
  {
    "text": "have one of my other nodes do this recompute and we can even do like really cool things like sort of if one of the",
    "start": "879199",
    "end": "885079"
  },
  {
    "text": "nodes is taking a long time we've got a straggler and like maybe it's actually 386 that someone snuck into my cluster",
    "start": "885079",
    "end": "890440"
  },
  {
    "text": "as a prank um it'll go and like preemptively start a second task on another machine and and start Computing",
    "start": "890440",
    "end": "896440"
  },
  {
    "text": "this for me um so I think this is pretty awesome awesome there's there's some limitations though that come from this",
    "start": "896440",
    "end": "904560"
  },
  {
    "start": "904000",
    "end": "1016000"
  },
  {
    "text": "design so the biggest problem with spark is that at the end of the day spark is not a compiler um it doesn't know what",
    "start": "904560",
    "end": "912360"
  },
  {
    "text": "your program looks like until it's running it um and it sort of gets a view",
    "start": "912360",
    "end": "917720"
  },
  {
    "text": "at each instance of the word action and then we try and do our best when we encounter an action right when we try",
    "start": "917720",
    "end": "923880"
  },
  {
    "text": "and save the data out we we do our best to optimize it but I don't know what you're going to be doing right",
    "start": "923880",
    "end": "929279"
  },
  {
    "text": "afterwards right if maybe you've done this like really awesome complex thing to train a model and then you have",
    "start": "929279",
    "end": "934839"
  },
  {
    "text": "another thing to extract all of the errors I don't know that you're coming through and extracting the errors afterwards so I'll compute all of this",
    "start": "934839",
    "end": "941600"
  },
  {
    "text": "and then promptly throw it away um and you might actually want to reuse some of those pieces right and so we need to",
    "start": "941600",
    "end": "947480"
  },
  {
    "text": "help spark out and let it know when we're going to be reusing data just because we don't have this whole program",
    "start": "947480",
    "end": "953680"
  },
  {
    "text": "view going on um one of the other things is combining these things together is",
    "start": "953680",
    "end": "959279"
  },
  {
    "text": "really awesome for performance and really terrible for debugging um so if if my tokenization thing was actually",
    "start": "959279",
    "end": "966120"
  },
  {
    "text": "something complex and I screwed up in there like I had a bad statistical model for like Chinese tokenization and I",
    "start": "966120",
    "end": "972079"
  },
  {
    "text": "started throwing exceptions I wouldn't find out until I saved it out and at that point I wouldn't necessarily know",
    "start": "972079",
    "end": "978399"
  },
  {
    "text": "which one of my individual map tasks had caused my error because I'm just getting an exception from my save right and like",
    "start": "978399",
    "end": "985800"
  },
  {
    "text": "save should work um this most often happens with people going count and they're like why can't I just count my",
    "start": "985800",
    "end": "991079"
  },
  {
    "text": "data counting should work and it's like yeah counting should work but it has to do all of the other work um and so it's",
    "start": "991079",
    "end": "997759"
  },
  {
    "text": "there there's some sad bits of magic in here but it's okay we can we can work around much of our",
    "start": "997759",
    "end": "1002839"
  },
  {
    "text": "sadness and the the last one is is sort of the limitation with rdds um spark is",
    "start": "1002839",
    "end": "1008639"
  },
  {
    "text": "has this really awesome Optimizer but it can't look inside of the functions that we're passing to it right so if we if we",
    "start": "1008639",
    "end": "1015800"
  },
  {
    "text": "jump back to slides um can't see that we're tokenizing the data all it knows",
    "start": "1015800",
    "end": "1021839"
  },
  {
    "start": "1016000",
    "end": "1039000"
  },
  {
    "text": "is that we're doing something with the data um and it's going to return zero or more results for each element and then",
    "start": "1021839",
    "end": "1028160"
  },
  {
    "text": "for the next one it knows it's going to get a onetoone mapping but it doesn't know what's happening inside of that it",
    "start": "1028160",
    "end": "1033678"
  },
  {
    "text": "doesn't know if it's going to be really complicated or really simple or if maybe we do something interesting um so this",
    "start": "1033679",
    "end": "1040720"
  },
  {
    "text": "is this is frustrating and data sets went like I want to make a more awesome Optimizer but I don't want to spend my",
    "start": "1040720",
    "end": "1046720"
  },
  {
    "text": "life uh trying to decompile Java bite code or python bite code right um the",
    "start": "1046720",
    "end": "1052760"
  },
  {
    "text": "spk people were like I want to make more awesome tools but I don't want to become a compiler",
    "start": "1052760",
    "end": "1058080"
  },
  {
    "text": "writer um and yeah that's cool the other piece of magic is that inside of all of",
    "start": "1058080",
    "end": "1063400"
  },
  {
    "start": "1059000",
    "end": "1135000"
  },
  {
    "text": "this our data is magically distributed um there are some catches which happen inside of reduced by key but when we're",
    "start": "1063400",
    "end": "1069160"
  },
  {
    "text": "loading our data in from hdfs we we have our data magically distributed across our cluster um if we have a local",
    "start": "1069160",
    "end": "1076000"
  },
  {
    "text": "collection and we want to like sort of send it out everywhere spark will take that collection and slice it up and send",
    "start": "1076000",
    "end": "1081360"
  },
  {
    "text": "it to all of the different computers um and we don't have to think about this too much except when it",
    "start": "1081360",
    "end": "1087760"
  },
  {
    "text": "breaks um and this normally happens inside of the shuffle step the shuffle",
    "start": "1087760",
    "end": "1093360"
  },
  {
    "text": "step is the tricky part of word count um and it's the tricky part of most distributed applications that are built",
    "start": "1093360",
    "end": "1100039"
  },
  {
    "text": "in the same way um and our Shuffle requires that we have a known",
    "start": "1100039",
    "end": "1105320"
  },
  {
    "text": "partitioner inside of spark and what that means is when we're loading our data in spark doesn't really have to",
    "start": "1105320",
    "end": "1110520"
  },
  {
    "text": "make a lot of assumptions about what our data looks like but when it shuffles the data for word count for example it needs",
    "start": "1110520",
    "end": "1116080"
  },
  {
    "text": "to send all of the information about everything which has the same word to the same machine um and this could you",
    "start": "1116080",
    "end": "1123080"
  },
  {
    "text": "know very quickly overwhelm spark right so obviously there's some solutions to",
    "start": "1123080",
    "end": "1128120"
  },
  {
    "text": "these problems but this is part of where the magic breaks down our partitioners have to be",
    "start": "1128120",
    "end": "1134799"
  },
  {
    "text": "deterministic um and key skew is essentially the part which breaks our magic automatic partitioning of your",
    "start": "1134799",
    "end": "1141159"
  },
  {
    "start": "1135000",
    "end": "1149000"
  },
  {
    "text": "data um I like to think of it as the anti- rescue it could also be the Wicked Witch of the West doesn't really matter",
    "start": "1141159",
    "end": "1148440"
  },
  {
    "text": "um but it turns out most data that we have has kyew in it um or at least mod",
    "start": "1148440",
    "end": "1154840"
  },
  {
    "start": "1149000",
    "end": "1189000"
  },
  {
    "text": "this data that I have that I care about has kyq in it um and if your customers tell you that your data doesn't have kyq",
    "start": "1154840",
    "end": "1161000"
  },
  {
    "text": "in it uh they're lying um probably not intentionally but they're lying to you",
    "start": "1161000",
    "end": "1167039"
  },
  {
    "text": "um if you have something thing with like null inside of it ever you're probably going to find out like 80% of those",
    "start": "1167039",
    "end": "1173280"
  },
  {
    "text": "records actually have null at one point in your processing Pipeline and and it's going to be sad and you're going to",
    "start": "1173280",
    "end": "1178679"
  },
  {
    "text": "start sending all of your data to this one machine and your magic distributed system becomes as effective as that",
    "start": "1178679",
    "end": "1183960"
  },
  {
    "text": "single machine which is really not a lot of fun that's not what we want to do um so inside of shuffles when we do",
    "start": "1183960",
    "end": "1191720"
  },
  {
    "start": "1189000",
    "end": "1256000"
  },
  {
    "text": "something like sort by key or group by key which essentially just requires that all of the data with the same key goes",
    "start": "1191720",
    "end": "1197799"
  },
  {
    "text": "to the same machine um we can we can make this explode um this is one of my favorite examples it",
    "start": "1197799",
    "end": "1204200"
  },
  {
    "text": "is about my intentions to open a handlebar mustache wax shop in San Francisco uh well I'm not convinced it's",
    "start": "1204200",
    "end": "1210520"
  },
  {
    "text": "in San Francisco yet but I want to open an artisanal handlebar mustash Wax Shop",
    "start": "1210520",
    "end": "1215679"
  },
  {
    "text": "I think that its time has really come I see a lot of hipsters um although not as",
    "start": "1215679",
    "end": "1221480"
  },
  {
    "text": "many in the room as I expected I should maybe reconsider my mustache wax shop um",
    "start": "1221480",
    "end": "1227520"
  },
  {
    "text": "but you know maybe I'm going to purchase some Market data and then I'm going to try and figure out where I should be",
    "start": "1227520",
    "end": "1233360"
  },
  {
    "text": "sending my flyers for people about these exciting once in a lifetime opportunities to save 20% on their",
    "start": "1233360",
    "end": "1239159"
  },
  {
    "text": "mustache wax um or where I should open my shop right and so I've got zip code",
    "start": "1239159",
    "end": "1244440"
  },
  {
    "text": "data because it turns out that Precision is pretty cheap and we can pretend that it doesn't de anonymize the user even",
    "start": "1244440",
    "end": "1250720"
  },
  {
    "text": "though it totally does but you know whatever people will sell me that data for about $5 um and so I shuffle it but then it",
    "start": "1250720",
    "end": "1258280"
  },
  {
    "start": "1256000",
    "end": "1322000"
  },
  {
    "text": "turns out that there's so many hipsters in San Francisco that my distributed computer fails and I send all of my data",
    "start": "1258280",
    "end": "1265679"
  },
  {
    "text": "for like this isn't even all of San Francisco this is just the Mission District and it turns out we have a lot of",
    "start": "1265679",
    "end": "1271200"
  },
  {
    "text": "mustaches um and so red red is sad um and this happens because when we sort",
    "start": "1271200",
    "end": "1277360"
  },
  {
    "text": "our data by key spark says okay I'm gonna I'm going to sample the data I'm going to try and figure out what some",
    "start": "1277360",
    "end": "1283200"
  },
  {
    "text": "reasonable range intervals are for my data but it can't break it if the key",
    "start": "1283200",
    "end": "1290600"
  },
  {
    "text": "doesn't change we can't send one key to two different computers right now um and this is really useful from when we're",
    "start": "1290600",
    "end": "1296880"
  },
  {
    "text": "doing things Downstream like joins um because if I know where all of the data for one key is when I'm doing my join",
    "start": "1296880",
    "end": "1303039"
  },
  {
    "text": "this can be much more efficient I can do my shuffles more efficiently I can maybe only Shuffle one of my two pieces um and",
    "start": "1303039",
    "end": "1309360"
  },
  {
    "text": "if like one of them is big and one of them is small that can make a huge difference right so this is this gives",
    "start": "1309360",
    "end": "1314760"
  },
  {
    "text": "us some really cool performance benefits but it comes at the cost of I can't f figure out where to open my mustache wax",
    "start": "1314760",
    "end": "1320159"
  },
  {
    "text": "shop which is a pretty sad cost so there's a hack um oh did I fix it in",
    "start": "1320159",
    "end": "1326880"
  },
  {
    "start": "1322000",
    "end": "1362000"
  },
  {
    "text": "this example whatever we'll pretend I did um there might be one record which is shuffled incorrectly by hand I'm",
    "start": "1326880",
    "end": "1333400"
  },
  {
    "text": "sorry I I did this by hand for for the literally bites of data um but we can we",
    "start": "1333400",
    "end": "1339919"
  },
  {
    "text": "can sort of take our key and we can append some junk to it um and this is kind of hacky but it lets us do our",
    "start": "1339919",
    "end": "1345760"
  },
  {
    "text": "distributed shuffles I can figure out where to send my mustache wax flyers and",
    "start": "1345760",
    "end": "1351159"
  },
  {
    "text": "you know everyone's going to be happy my distributed system is still going to magically",
    "start": "1351159",
    "end": "1356200"
  },
  {
    "text": "work um and so that's that's one way that the shuffle can make things sad um and this is one way we can make it happy",
    "start": "1356200",
    "end": "1362760"
  },
  {
    "start": "1362000",
    "end": "1428000"
  },
  {
    "text": "but there's other ways we can make the shuffle sad and happy at the same time um we were sort of talking about how",
    "start": "1362760",
    "end": "1369480"
  },
  {
    "text": "spark can't see inside of the Lambda Expressions right um and this can come together with our good friend keu to",
    "start": "1369480",
    "end": "1376360"
  },
  {
    "text": "make an even bigger problem um so Group by key is this thing which sounds really",
    "start": "1376360",
    "end": "1381600"
  },
  {
    "text": "safe how many people think Group by key sounds like a safe API to use okay only two people how many people",
    "start": "1381600",
    "end": "1389720"
  },
  {
    "text": "think it sounds like a dangerous API to use before I started talking on the",
    "start": "1389720",
    "end": "1395080"
  },
  {
    "text": "slide okay three people and a lot of laughs so that's pretty good thanks",
    "start": "1395080",
    "end": "1400240"
  },
  {
    "text": "boo um so yeah I mean it's clearly not safe because this is a slide labeled more sadness um but I think Group by key",
    "start": "1400240",
    "end": "1408840"
  },
  {
    "text": "when I first came to spark I was like yeah I want to group my data together by key that makes sense um and the magic",
    "start": "1408840",
    "end": "1415279"
  },
  {
    "text": "Optimizer will look and see what I'm doing next and understand and be smart and it'll save me from my",
    "start": "1415279",
    "end": "1421880"
  },
  {
    "text": "laziness um or actually it will save me from my laziness with laziness okay no one like that one um",
    "start": "1421880",
    "end": "1429799"
  },
  {
    "start": "1428000",
    "end": "1461000"
  },
  {
    "text": "here is bad word count um and I think that if we hadn't talked about why Group",
    "start": "1429799",
    "end": "1436000"
  },
  {
    "text": "by key was sad um or is sad we wouldn't necessarily this wouldn't jump out to us",
    "start": "1436000",
    "end": "1442120"
  },
  {
    "text": "immediately as non-spark developers as being a bad pattern right I've got my",
    "start": "1442120",
    "end": "1447640"
  },
  {
    "text": "data I tokenize it I do the word the number one thing and then I group it together for each word uh and then I go",
    "start": "1447640",
    "end": "1455039"
  },
  {
    "text": "ahead and I compute the sum oh that seems fine and then I save the result out um but that there actually is a",
    "start": "1455039",
    "end": "1461480"
  },
  {
    "start": "1461000",
    "end": "1478000"
  },
  {
    "text": "difference um in this case I did this with literally kilobytes of data cuz I was on an airplane um but the the",
    "start": "1461480",
    "end": "1469000"
  },
  {
    "text": "shuffled read here is this group I key yeah the shuffled read here is like 48.7 kilobytes and the shuffled right is",
    "start": "1469000",
    "end": "1477720"
  },
  {
    "text": "42.4 um and so that's that's like oh right for an input essentially for our",
    "start": "1477720",
    "end": "1483240"
  },
  {
    "start": "1478000",
    "end": "1503000"
  },
  {
    "text": "input we ended up writing out exactly as much as our output was during the shuffle stage and that's interesting and",
    "start": "1483240",
    "end": "1489399"
  },
  {
    "text": "that could be fine but because we're going to be sending potentially all of",
    "start": "1489399",
    "end": "1494880"
  },
  {
    "text": "the records for one word to one machine this could be problematic a large portion of that shuffled right could be",
    "start": "1494880",
    "end": "1500919"
  },
  {
    "text": "going to a single machine so instead what we did in our earlier example is we",
    "start": "1500919",
    "end": "1506120"
  },
  {
    "start": "1503000",
    "end": "1627000"
  },
  {
    "text": "used Reduce by key um and Reduce by key allows spark to understand what's going",
    "start": "1506120",
    "end": "1511679"
  },
  {
    "text": "on because inside of our map values Spark all it knows is we're Computing something it doesn't know if there's a",
    "start": "1511679",
    "end": "1518240"
  },
  {
    "text": "partial aggregation of that something that exists right it can't like compute that for like the keys on one machine",
    "start": "1518240",
    "end": "1524840"
  },
  {
    "text": "and then send the closure to another machine and then do some craziness like that's just is not going to go very well",
    "start": "1524840",
    "end": "1530320"
  },
  {
    "text": "um but reduced by key tells spark like hey for all the data with this key this",
    "start": "1530320",
    "end": "1535799"
  },
  {
    "text": "is my combine rule I want you to combine the data using this Rule and then on each worker it combines the data for",
    "start": "1535799",
    "end": "1542279"
  },
  {
    "text": "each key locally and then it shuffles the combined values and so at most we have number of workers um instances of",
    "start": "1542279",
    "end": "1551200"
  },
  {
    "text": "each individual key being shuffled to the different machines and unless you have more workers than you have memory",
    "start": "1551200",
    "end": "1557080"
  },
  {
    "text": "on your workers in which case please buy an IBM support contract um last time I",
    "start": "1557080",
    "end": "1563080"
  },
  {
    "text": "said cladera and that was just awkward um but please buy a support contract",
    "start": "1563080",
    "end": "1569200"
  },
  {
    "text": "from a Hadoop vendor in that case um but if you have more than two gigabytes worth of machines you're going to run",
    "start": "1569200",
    "end": "1575240"
  },
  {
    "text": "into other problems um but reduced by key essentially guarantees that we're going to have a reasonable result",
    "start": "1575240",
    "end": "1582120"
  },
  {
    "text": "provided that our reduction isn't just creating a list right like if we create a list inside of our reduced by key",
    "start": "1582120",
    "end": "1587919"
  },
  {
    "text": "there's not going to be any memory decrease and we're still going to essentially have this this path of sadness but if I'm summing the numbers I",
    "start": "1587919",
    "end": "1593919"
  },
  {
    "text": "mean integer plus integer takes up the space of an integer I'm pretty happy or even if I'm doing like a hash set like",
    "start": "1593919",
    "end": "1601080"
  },
  {
    "text": "I'm probably going to have duplicate values I'm going to be pretty happy too um an aggregate by key is like word",
    "start": "1601080",
    "end": "1606440"
  },
  {
    "text": "count part two uh which we normally leave to the advanced talks um and that allows us to do things that aren't word",
    "start": "1606440",
    "end": "1613159"
  },
  {
    "text": "count um so that's that's definitely a v- talk uh but you can use it for things where you're Computing uh stuff where",
    "start": "1613159",
    "end": "1619960"
  },
  {
    "text": "the types aren't quite the same if you're doing like an INT into a hash map you can do it more easily with with",
    "start": "1619960",
    "end": "1625799"
  },
  {
    "text": "aggregate by key um and we can see the Reduce by key version you know the shuffled read went",
    "start": "1625799",
    "end": "1631799"
  },
  {
    "text": "down by a factor of two and you can trust me because no one can read this slide I'm sorry about that um but the",
    "start": "1631799",
    "end": "1637440"
  },
  {
    "text": "slides will be posted and you can double check that at the very least I used Ms paint to fake these",
    "start": "1637440",
    "end": "1642679"
  },
  {
    "text": "slides um yeah cool uh so another potential fix",
    "start": "1642679",
    "end": "1650000"
  },
  {
    "start": "1646000",
    "end": "1696000"
  },
  {
    "text": "instead of like intentionally remembering not to use Group by key which to be fair isn't that hard but",
    "start": "1650000",
    "end": "1655600"
  },
  {
    "text": "instead of expressing all of my sort of aggregate statistics as reduced by key operations I could use data frames um",
    "start": "1655600",
    "end": "1662760"
  },
  {
    "text": "and data frames allow spark to understand what's going on so inside of our instead of having this map values I",
    "start": "1662760",
    "end": "1669679"
  },
  {
    "text": "can get this group data structure and I can tell spark what sums I want it to compute and then spark can do really",
    "start": "1669679",
    "end": "1675399"
  },
  {
    "text": "awesome things for us um I can even have things where I do select statements to",
    "start": "1675399",
    "end": "1680760"
  },
  {
    "text": "narrow down the data that I'm reading and then spark will only read the parts of data that I need um if I have a",
    "start": "1680760",
    "end": "1687120"
  },
  {
    "text": "filter or if I select only certain columns this can make a huge difference if your data is appropriately",
    "start": "1687120",
    "end": "1692440"
  },
  {
    "text": "partitioned um and this this is really important right the other thing that I",
    "start": "1692440",
    "end": "1698080"
  },
  {
    "text": "think is really cool about data sets is that it lets me write in this DSL that spark can understand um which looks a",
    "start": "1698080",
    "end": "1704919"
  },
  {
    "text": "lot like SQL um but at the end of the day I'm a functional program progamming nerd who isn't very good with SQL um so",
    "start": "1704919",
    "end": "1711080"
  },
  {
    "text": "I still want to be able to put my random functional programming stuff in sort of the middle of these SQL like statements",
    "start": "1711080",
    "end": "1716720"
  },
  {
    "text": "right in this case I'm loading some information about pandas probably most pandas are pretty sad honestly I think",
    "start": "1716720",
    "end": "1722679"
  },
  {
    "text": "they live in zoos it's kind of depressing but I can process this data a lot faster since I only care about the",
    "start": "1722679",
    "end": "1729200"
  },
  {
    "text": "happy ones um assuming I've got my data appropriately partitioned and so I can have this filter statement and Spark can",
    "start": "1729200",
    "end": "1735600"
  },
  {
    "text": "see that I only care about happy pandas and it'll just completely ignore all of the sad pandas in the world um which is",
    "start": "1735600",
    "end": "1744399"
  },
  {
    "text": "depressing sorry okay um here's depressing with",
    "start": "1744399",
    "end": "1750360"
  },
  {
    "text": "annotations uh right okay so let's let's go back to something more fun let's say",
    "start": "1750360",
    "end": "1756720"
  },
  {
    "start": "1752000",
    "end": "1811000"
  },
  {
    "text": "I'm really curious about how fuzzy pandas are in aggregate but I really suck at writing sequel like statements",
    "start": "1756720",
    "end": "1763840"
  },
  {
    "text": "um and I've got this array attribute and I just want to sum all of it um and so this is this is a functional programming",
    "start": "1763840",
    "end": "1770000"
  },
  {
    "text": "sum uh the underscore is should really you can rewrite the underscore into Lambda X colon X greater than zero and",
    "start": "1770000",
    "end": "1777600"
  },
  {
    "text": "your life will be fine um I'm only interested in positive fuzzy pandas uh if a panda is negatively fuzzy I think",
    "start": "1777600",
    "end": "1784240"
  },
  {
    "text": "that's a data error um so we can just throw that out or that Panda has some issues um and I can keep my Su and I get",
    "start": "1784240",
    "end": "1792080"
  },
  {
    "text": "back a data set and I think this is cool because I get to still write my like nerdy functional programming stuff and",
    "start": "1792080",
    "end": "1798080"
  },
  {
    "text": "pretend that my employer is getting good value for money um but I can intermix",
    "start": "1798080",
    "end": "1803679"
  },
  {
    "text": "with sort of squel like Expressions that spark can really easily optimize and do cool stuff with um and it's a lot faster",
    "start": "1803679",
    "end": "1812039"
  },
  {
    "text": "right um if I'm Computing some Aggregates I mean we know Group by key is bad because I told you it's bad uh",
    "start": "1812039",
    "end": "1818240"
  },
  {
    "text": "big is bad here small is good unlike the amount of money uh which people should pay me uh where we would inverse these",
    "start": "1818240",
    "end": "1825000"
  },
  {
    "text": "two um but we can see Group by key performance pretty terribly and we can see that reduced by key performs like",
    "start": "1825000",
    "end": "1831799"
  },
  {
    "text": "pretty okay but if we ask spark data frames to do the same stuff they're able to do it a lot faster because it's able",
    "start": "1831799",
    "end": "1838679"
  },
  {
    "text": "to understand what we're asking it to do and it can pipeline a lot of these things a lot more efficiently it also",
    "start": "1838679",
    "end": "1844320"
  },
  {
    "text": "gets a much better understanding of the types um data frames have this sort of restricted set of types that we can work",
    "start": "1844320",
    "end": "1850360"
  },
  {
    "text": "in um pretty much any native type that you're used to working with is fine but Reds have to take sort of arbitrary Java",
    "start": "1850360",
    "end": "1857440"
  },
  {
    "text": "objects and arbitary python objects and we use the built-in serializers for those things and that works about as",
    "start": "1857440",
    "end": "1863760"
  },
  {
    "text": "well as you'd think uh at least it's not XML um and so this is this is or Json um",
    "start": "1863760",
    "end": "1871399"
  },
  {
    "text": "so this is a lot faster because the optimizer is able to see what we're doing and also our stuff just takes up a",
    "start": "1871399",
    "end": "1877720"
  },
  {
    "text": "lot less memory while we're doing it um and so this is pretty cool and you should definitely consider using data",
    "start": "1877720",
    "end": "1883279"
  },
  {
    "text": "sets um which relatedly is only covered in my new book um",
    "start": "1883279",
    "end": "1889919"
  },
  {
    "text": "coincidence I assure you um right so we're going to do one final bit of magic",
    "start": "1890039",
    "end": "1895120"
  },
  {
    "start": "1892000",
    "end": "1944000"
  },
  {
    "text": "for the like seven or eight python people in the room uh and we're going to make spark work with python uh I mean",
    "start": "1895120",
    "end": "1903399"
  },
  {
    "text": "we're going to someone else did this already but let's figure out how they did it um they drank a lot of coffee and",
    "start": "1903399",
    "end": "1911360"
  },
  {
    "text": "they said to themselves I know how to do an interprocess communication I use Unix",
    "start": "1911360",
    "end": "1916639"
  },
  {
    "text": "pipes um um and then they were like okay well that's kind of slow um but you can",
    "start": "1916639",
    "end": "1923279"
  },
  {
    "text": "just buy twice as many machines and life will be okay um and that's the state of the world um but let's look at it let's",
    "start": "1923279",
    "end": "1929559"
  },
  {
    "text": "look at how this works kind of like this playground um if",
    "start": "1929559",
    "end": "1935320"
  },
  {
    "text": "you send your process down here it will probably succeed somehow but if it dies it's not my fault and you signed a",
    "start": "1935320",
    "end": "1941159"
  },
  {
    "text": "liability wer um or you know by a support contract",
    "start": "1941159",
    "end": "1946559"
  },
  {
    "start": "1944000",
    "end": "2056000"
  },
  {
    "text": "so on our driver side program which is sort of where we've Define all of our Transformations and we Define all of our",
    "start": "1946559",
    "end": "1953240"
  },
  {
    "text": "selects or whatever it is we're doing on our data um spark needs a way to communicate from python to the jvm so",
    "start": "1953240",
    "end": "1960360"
  },
  {
    "text": "that the jvm can schedule these jobs for it um and we do this using something called pi4j and pi4j is amazing and",
    "start": "1960360",
    "end": "1968960"
  },
  {
    "text": "terrible um it gives you delightful error messages like object is not",
    "start": "1968960",
    "end": "1974159"
  },
  {
    "text": "callable which is it's just like that means something has has gone wrong um",
    "start": "1974159",
    "end": "1980399"
  },
  {
    "text": "from anything to like object is literally not callable to package does not exist to my class path got deleted",
    "start": "1980399",
    "end": "1987240"
  },
  {
    "text": "by accident um all gets you the same error message which is delightful because when you Google it you can see",
    "start": "1987240",
    "end": "1992840"
  },
  {
    "text": "all of the ponential flaws um but you know it's great because it lets our code work and there's really nothing better",
    "start": "1992840",
    "end": "2000159"
  },
  {
    "text": "um in P spark what we do is we take our data and we pickle it and then we store",
    "start": "2000159",
    "end": "2006799"
  },
  {
    "text": "it in an rdd uh I don't work so well um but it works kind of surprisingly um it turns out",
    "start": "2006799",
    "end": "2014120"
  },
  {
    "text": "that trying to figure out what key a pickled object has in Java is kind of expensive so it's this isn't exactly",
    "start": "2014120",
    "end": "2021320"
  },
  {
    "text": "true but it's close enough uh we do some magic to make it so the jvm can understand the Pickled",
    "start": "2021320",
    "end": "2027080"
  },
  {
    "text": "objects about as well as I understand French um which is to say I can say",
    "start": "2027080",
    "end": "2032760"
  },
  {
    "text": "hello and the number four um oh also I can assure you that you're a",
    "start": "2032760",
    "end": "2038840"
  },
  {
    "text": "is it a pineapple no a banana um anyways uh so yeah so the jvm can sort of",
    "start": "2038840",
    "end": "2045399"
  },
  {
    "text": "understand these pickled objects just enough to figure out where they should be going the jvm's responsible for all",
    "start": "2045399",
    "end": "2051118"
  },
  {
    "text": "the network communication happening so it has to understand what the keys are of our data um and this top quality",
    "start": "2051119",
    "end": "2057919"
  },
  {
    "text": "architectural diagram which shows you that I have not managed to convince the designers to work on my slides um shows",
    "start": "2057919",
    "end": "2064280"
  },
  {
    "text": "you pretty much how this works on our on our driver program we use pi4j it's not amazing but it works and on our worker",
    "start": "2064280",
    "end": "2071320"
  },
  {
    "text": "programs we copy the data from the jvm into python we do all of our magic computation and we copy it",
    "start": "2071320",
    "end": "2078158"
  },
  {
    "text": "back um this does have some unfortunate implications though um I'll wait for you",
    "start": "2078159",
    "end": "2084480"
  },
  {
    "text": "to finish taking a photo and okay cool um so how does this",
    "start": "2084480",
    "end": "2090000"
  },
  {
    "start": "2088000",
    "end": "2170000"
  },
  {
    "text": "break essentially this talk is all of the bad things that we've done in spark that you know have to deal with and for",
    "start": "2090000",
    "end": "2095358"
  },
  {
    "text": "the python users the biggest one is that we have to serialize the data multiple times um and we we essentially serialize",
    "start": "2095359",
    "end": "2102760"
  },
  {
    "text": "it send it to Java send it back from java to Python and deserialize it and it just gets kind of sad because we spend",
    "start": "2102760",
    "end": "2108680"
  },
  {
    "text": "all this time essentially double serializing this data and it's it's a little depressing um the other parts",
    "start": "2108680",
    "end": "2114520"
  },
  {
    "text": "aren't super important uh one thing that I left off here is that the error messages make no sense has anyone ever",
    "start": "2114520",
    "end": "2121160"
  },
  {
    "text": "understood a pisar error message and no one okay ful yeah so the",
    "start": "2121160",
    "end": "2128920"
  },
  {
    "text": "error messages in pice Park literally are things like my iterator is like invalid and that can be your Lambda",
    "start": "2128920",
    "end": "2136160"
  },
  {
    "text": "expression in one of your Maps is wrong um and that's just",
    "start": "2136160",
    "end": "2141320"
  },
  {
    "text": "great uh if you're interested in sorry I should leave you with a solution rather than like everything is terrible and on",
    "start": "2141320",
    "end": "2146960"
  },
  {
    "text": "fire um I I I keep forgetting to put the happy part in um now now thankfully",
    "start": "2146960",
    "end": "2153760"
  },
  {
    "text": "there are ways to understand the error messages uh but that is a talk onto itself for python people and so I have a",
    "start": "2153760",
    "end": "2160160"
  },
  {
    "text": "python Focus talk on how to read python spark error messages and it's on YouTube",
    "start": "2160160",
    "end": "2165520"
  },
  {
    "text": "um I'm sorry that it's like a 40-minute talk on how to read your error messages but you should really use data",
    "start": "2165520",
    "end": "2172839"
  },
  {
    "start": "2170000",
    "end": "2218000"
  },
  {
    "text": "sets if you're in Python because it saves us from the biggest downside of",
    "start": "2172839",
    "end": "2178200"
  },
  {
    "text": "this delightful architecture that we've chosen um and that is instead of taking",
    "start": "2178200",
    "end": "2183319"
  },
  {
    "text": "the data and serializing it back and forth all the time with data sets when we write are things in this DSL that",
    "start": "2183319",
    "end": "2189040"
  },
  {
    "text": "spark can understand spark just compiles a DSL of java and just runs it there and we're fine right we don't even have to",
    "start": "2189040",
    "end": "2196240"
  },
  {
    "text": "execute anything in Python except for you know our our nice little driver program which tells the jvm what to do",
    "start": "2196240",
    "end": "2203079"
  },
  {
    "text": "uh notably this all comes crashing down once we actually need to use numpy uh and then we go back to the double",
    "start": "2203079",
    "end": "2208640"
  },
  {
    "text": "serialization cost because it turns out that the Java version of numpy is called does not exist um and",
    "start": "2208640",
    "end": "2216520"
  },
  {
    "text": "yeah anyways um so I think I'm getting close to the end there are some delightful",
    "start": "2216520",
    "end": "2223359"
  },
  {
    "start": "2218000",
    "end": "2246000"
  },
  {
    "text": "spark videos uh for the people who are new to spark if I didn't scare you off too much pacco has a really wonderful",
    "start": "2223359",
    "end": "2230079"
  },
  {
    "text": "introduction to Apache spark if you have an expense account please buy it from O'Reilly it is $100 um but if you're",
    "start": "2230079",
    "end": "2236920"
  },
  {
    "text": "deciding between buying that and my book he does have a free version of his talk on YouTube and you should buy my book",
    "start": "2236920",
    "end": "2244240"
  },
  {
    "text": "um I keep talking to people who have not tested Ted their spark code and this is",
    "start": "2244240",
    "end": "2249680"
  },
  {
    "start": "2246000",
    "end": "2294000"
  },
  {
    "text": "terrifying just because it's in a notebook doesn't mean you don't have to test it I'm not looking at anyone in",
    "start": "2249680",
    "end": "2256040"
  },
  {
    "text": "particular but like please do not kill me or lead to",
    "start": "2256040",
    "end": "2261200"
  },
  {
    "text": "the next financial collapse then I end up in one of those engineering textbooks in 20 years about someone who was killed",
    "start": "2261200",
    "end": "2267040"
  },
  {
    "text": "by her own software and that is not what I want my legacy to be um so here's a",
    "start": "2267040",
    "end": "2272920"
  },
  {
    "text": "bunch of testing libraries don't screw it up and one of them is written by Jesse well there's a post on how to use",
    "start": "2272920",
    "end": "2278160"
  },
  {
    "text": "it written by Jesse who is amazing Jesse raise your hand yay Jesse um and you",
    "start": "2278160",
    "end": "2284640"
  },
  {
    "text": "should definitely read it if you're a Java user I think it's it's really good admittedly he tells you to use my testing Library so there's some",
    "start": "2284640",
    "end": "2292319"
  },
  {
    "text": "symbiotic relationship there as it were uh here are spark books I receive royalties for many but not all of these",
    "start": "2292319",
    "end": "2299400"
  },
  {
    "start": "2294000",
    "end": "2332000"
  },
  {
    "text": "um if you're new to spark learning spark is great uh learning py Spark by Denny I don't get any Roy uh Denny and thas I",
    "start": "2299400",
    "end": "2306680"
  },
  {
    "text": "don't get any Royal from but it's also pretty cool and I owe Denny a beer so",
    "start": "2306680",
    "end": "2312000"
  },
  {
    "text": "check out learning pisar um and drop him a line that said you bought it from me",
    "start": "2312000",
    "end": "2317280"
  },
  {
    "text": "and if like six people do that I'm pretty sure I don't have to buy him budlight lime uh oh no I forgot my",
    "start": "2317280",
    "end": "2322520"
  },
  {
    "text": "budlight lime okay sorry um I have some",
    "start": "2322520",
    "end": "2329240"
  },
  {
    "text": "post-conference traditions which you know uh but most importantly buy several",
    "start": "2329240",
    "end": "2334960"
  },
  {
    "start": "2332000",
    "end": "2369000"
  },
  {
    "text": "copies of this book um my uh my corporate compliance",
    "start": "2334960",
    "end": "2341920"
  },
  {
    "text": "training was very clear that I'm not allowed to accept $20 bills in a brown envelope anymore um and they said it",
    "start": "2341920",
    "end": "2348440"
  },
  {
    "text": "didn't even actually matter what color the envelope was um but I am allowed to",
    "start": "2348440",
    "end": "2353560"
  },
  {
    "text": "ask you to buy my book which I receive royalties from not $20 mind you but please please buy this um it will be",
    "start": "2353560",
    "end": "2361680"
  },
  {
    "text": "finished soon but do not let that stop you from giving me your money um",
    "start": "2361680",
    "end": "2368720"
  },
  {
    "text": "if you're looking for some places to come I will be doing office hours tomorrow at 11:00 a.m. at intelligencia",
    "start": "2368720",
    "end": "2374720"
  },
  {
    "start": "2369000",
    "end": "2416000"
  },
  {
    "text": "these are unofficial not goto sponsored or anything like that I just like drinking coffee and if other people show",
    "start": "2374720",
    "end": "2380760"
  },
  {
    "text": "up I can put it on my aforementioned expense account um if no one shows up my",
    "start": "2380760",
    "end": "2386160"
  },
  {
    "text": "boss is getting suspicious that Buu is not a real person um so I probably can't expense it uh if anyone is from Europe",
    "start": "2386160",
    "end": "2394800"
  },
  {
    "text": "I'll be in Lisbon and Barcelona and London and Israel um for the rest of this month I",
    "start": "2394800",
    "end": "2401520"
  },
  {
    "text": "am sure I will go home in June when spark Summit happens in San",
    "start": "2401520",
    "end": "2406560"
  },
  {
    "text": "Francisco uh so come say hi at One of These Fine events um and I can buy you",
    "start": "2406560",
    "end": "2412119"
  },
  {
    "text": "coffee on my expense account um especially if you rate this talk really",
    "start": "2412119",
    "end": "2418280"
  },
  {
    "start": "2416000",
    "end": "2504000"
  },
  {
    "text": "highly um remember this talk was amazing and the stuffed animals were super cute",
    "start": "2418280",
    "end": "2425240"
  },
  {
    "text": "um so thank you for for coming I I guess I can do questions is there time for",
    "start": "2425240",
    "end": "2430280"
  },
  {
    "text": "questions okay does anyone have questions great that's a that's a wonderful question um and so essentially",
    "start": "2430280",
    "end": "2436599"
  },
  {
    "text": "the question was I'm gonna shorten this I'm doing something normally when I'm doing it I have to store it as a sparts",
    "start": "2436599",
    "end": "2442079"
  },
  {
    "text": "matrix uh but in spark like I have this magic distributed system do I still have to do sparse matrices um and the answer",
    "start": "2442079",
    "end": "2449000"
  },
  {
    "text": "is no but you will be buying a lot of machines um and that is fine from my",
    "start": "2449000",
    "end": "2454640"
  },
  {
    "text": "point of view I work at a hardware vendor um um from the point of view of like future",
    "start": "2454640",
    "end": "2461359"
  },
  {
    "text": "Holden who may not work as a hardware vendor uh and will work at a cloud service also fine um from the point of",
    "start": "2461359",
    "end": "2467800"
  },
  {
    "text": "view of someone who may eventually pay the bill oh God please use a sports Matrix um there's a distributed row",
    "start": "2467800",
    "end": "2473520"
  },
  {
    "text": "Matrix format in spark it's not great but it has a built-in sparse Vector representation you can use and that'll",
    "start": "2473520",
    "end": "2479560"
  },
  {
    "text": "probably give you the right level of sparsity and that'll probably work but like don't hold me to",
    "start": "2479560",
    "end": "2486240"
  },
  {
    "text": "it okay awesome I think that's it for questions um so thank you all for coming I hope this was useful and I didn't",
    "start": "2486240",
    "end": "2493079"
  },
  {
    "text": "scare you away from using spark um if I did please don't tell my",
    "start": "2493079",
    "end": "2498560"
  },
  {
    "text": "[Applause]",
    "start": "2498560",
    "end": "2503800"
  },
  {
    "text": "boss",
    "start": "2503800",
    "end": "2506800"
  }
]