[
  {
    "start": "0",
    "end": "91000"
  },
  {
    "text": "Okay, why don't we start? Thanks for coming. My name is Dean wampler on the track host and the first speaker",
    "start": "9730",
    "end": "16110"
  },
  {
    "text": "And I have to say I'm really excited about this track you know with the exception of the first speaker I think we put together a good lineup",
    "start": "16330",
    "end": "22480"
  },
  {
    "text": "A couple of them are here in the audience Glen and Jesse that's the only ones I notice but really I think this will be my talk will be like an introduction to the kind of a",
    "start": "22680",
    "end": "31980"
  },
  {
    "text": "Streaming landscape as it's kind of emerging in Sort of the big Data world I mean really we've always been writing streaming apps even restful. Apps. They're kind of if you think about it",
    "start": "31980",
    "end": "41339"
  },
  {
    "text": "They're kind of you know data keeps coming and never stops and it's sort of the same at its scale of course and then subsequent",
    "start": "41339",
    "end": "48239"
  },
  {
    "text": "talks today, we'll dive into some of the particulars of Beam and Kafka and",
    "start": "48239",
    "end": "52858"
  },
  {
    "text": "Spark and and so forth so I should be pretty exciting. I hope I hope you'll enjoy it",
    "start": "53710",
    "end": "57960"
  },
  {
    "text": "all the pictures in this thing I took and they're basically pictures of water because we're talking about streaming, so",
    "start": "59920",
    "end": "65400"
  },
  {
    "text": "And you can get this talk and it obviously be on the conference website But I keep all my talks and my vanity website polyglotprogramming.com",
    "start": "66070",
    "end": "74218"
  },
  {
    "text": "Okay, so as always please rate the sessions when you're done Because I'm the speaker. I can't really look at the app that easily did four questions",
    "start": "75909",
    "end": "84209"
  },
  {
    "text": "So please just ask me questions at the end if you don't mind And once again, please rate the session",
    "start": "84210",
    "end": "90209"
  },
  {
    "start": "91000",
    "end": "113000"
  },
  {
    "text": "Most of what I'm going to talk about is Go into more depth in this little O'Reilly report. It's like a 30 40 page report so it's not too painful to digest",
    "start": "91660",
    "end": "101220"
  },
  {
    "text": "The only catch is you have to give us your email address at Lightbend hopefully you won't mind too much",
    "start": "101920",
    "end": "106900"
  },
  {
    "text": "But it goes into more detail, but what I'm going to describe today",
    "start": "107499",
    "end": "111359"
  },
  {
    "text": "So let's talk with some start with some context about streaming This is the area near Sacramento actually some of the cool wind farms there",
    "start": "112630",
    "end": "122159"
  },
  {
    "start": "113000",
    "end": "124000"
  },
  {
    "text": "And this happens to be in the sierras anyway The Hadoop classic architecture is kind of a good starting point",
    "start": "123759",
    "end": "129989"
  },
  {
    "start": "124000",
    "end": "134000"
  },
  {
    "text": "So how many of you have worked with the doop a lot of you probably okay? I'm not gonna spend a lot of time on this, but you know if you think about its architecture. It's essentially three big pieces",
    "start": "129990",
    "end": "139220"
  },
  {
    "start": "134000",
    "end": "167000"
  },
  {
    "text": "It's a distributed file system. You know that's HDFS. It's something that to do computation over the files",
    "start": "140020",
    "end": "146440"
  },
  {
    "text": "You know was a Mapreduce now It's more likely spark than anything else and and then there's a simple structure called the YARN that knows how to manage all these",
    "start": "146440",
    "end": "153940"
  },
  {
    "text": "Resources and allocate you know tasks across the cluster for your job and all that kind of stuff, and then there's obviously support tools",
    "start": "154100",
    "end": "161760"
  },
  {
    "text": "That's kind of what's on the bottom left corner to get data in and out and so forth and Etc",
    "start": "161770",
    "end": "166329"
  },
  {
    "text": "Yeah, but the general characteristics of hadoop. You know at least in classical terms there It's obviously been evolving a lot, but it was you know originally just a bachelor oriented system",
    "start": "167030",
    "end": "175269"
  },
  {
    "text": "Let's just get this data in here, and then do massive analysis over it I'm kind of glossing over the role of hbase which is more of a traditional database",
    "start": "175270",
    "end": "183100"
  },
  {
    "text": "cheap massive storage You know he's kind of an artifact of the unprecedented sized data sets we acquired in the internet, Era",
    "start": "184520",
    "end": "193269"
  },
  {
    "text": "back in the Pleistocene But it did support things like multi-user jobs",
    "start": "194210",
    "end": "199059"
  },
  {
    "text": "Diversity of Workflows and so forth and in a common use case for it turned out to be data warehouse replacement",
    "start": "199610",
    "end": "205570"
  },
  {
    "text": "Especially if you used hive as a sequel query engine That was really nice for that in terms of lowering cost and but maybe giving up some maturity and sophistication that the kind of data",
    "start": "206030",
    "end": "215679"
  },
  {
    "text": "warehouses are good at But now we're kind of transitioning to more of a streaming world, and it really boils down to time equals money",
    "start": "215750",
    "end": "223600"
  },
  {
    "start": "217000",
    "end": "245000"
  },
  {
    "text": "You know if I have valuable information in my data, the sooner I extract that value the better off I am.",
    "start": "223600",
    "end": "228880"
  },
  {
    "text": "It also has other benefits like if I'm running massive batch jobs. I need a lot of infrastructure",
    "start": "229460",
    "end": "235030"
  },
  {
    "text": "Maybe a few times a day, but if I kind of you know bleed it out over the day You know amortize it the data out then it you know. I need less infrastructure, so that's kind of a side benefit as well",
    "start": "235030",
    "end": "244569"
  },
  {
    "start": "245000",
    "end": "647000"
  },
  {
    "text": "So this diagram is from that little booklet and that the numbers correspond to points made in the booklet",
    "start": "245930",
    "end": "251409"
  },
  {
    "text": "But I'll walk through the details a little bit There's a lot on the slide, but hopefully it'll make sense as we go through it",
    "start": "251420",
    "end": "256539"
  },
  {
    "text": "This is what I see is kind of the canonical emerging streaming Data architecture today",
    "start": "257030",
    "end": "262959"
  },
  {
    "text": "Obviously, it's going to start with Data coming in So I kind of drew three different bubbles of data coming in to sort of represent",
    "start": "263690",
    "end": "271270"
  },
  {
    "text": "classic restful requests like someone is authorizing a credit card against your website or that kind of thing it could also be the",
    "start": "271729",
    "end": "278889"
  },
  {
    "text": "service consoles for your big data system And those are typically going to be handled by what let's just call them micro service toolkits",
    "start": "279169",
    "end": "286539"
  },
  {
    "text": "Which is sort of that you know the state of the art today? You know whatever. Tool you use for that purpose today",
    "start": "286539",
    "end": "291369"
  },
  {
    "text": "and then the bottom two blocks are meant to represent you know large volumes of data coming in you know a",
    "start": "292069",
    "end": "297848"
  },
  {
    "text": "You know maybe in very large streams, but in some sense anonymous records And I'm basically used sockets to represent anything coming from the outside that could be iot telemetry that could be",
    "start": "298430",
    "end": "309788"
  },
  {
    "text": "The Twitter firehose anything like that that you're ingesting Maybe partner data",
    "start": "311569",
    "end": "316610"
  },
  {
    "text": "And then I use logs to kind of really kind of represent anything internal like the exhaust logs that come off your server",
    "start": "316610",
    "end": "322418"
  },
  {
    "text": "So you might want to do analysis it could also be like Clickstream logs for your website that you might want to do you know?",
    "start": "322419",
    "end": "327908"
  },
  {
    "text": "usability studies on anything like that, but anyway the way It's tending to play out these days is it all ends up in Kafka at least most of the streaming data",
    "start": "328819",
    "end": "337688"
  },
  {
    "text": "Maybe not so much the rest data. Let's talk about that one first, so again typical micro Service Toolkit kind of stuff",
    "start": "337689",
    "end": "344620"
  },
  {
    "text": "We could be anything including our bar p up there arp is the reactive platform. That's light bends of",
    "start": "345020",
    "end": "350948"
  },
  {
    "text": "Spin on react on micro services, but it could be go no Js' you know whatever your favorite thing is",
    "start": "351649",
    "end": "357548"
  },
  {
    "text": "And I'll mention and I'm gonna come back in a minute to why I have microservices on a slide about big data",
    "start": "359659",
    "end": "364509"
  },
  {
    "text": "Well, I want to come back to that point because I think there's something interesting going on here ZooKeepers one of these things that everybody has especially if you're running Kafka, it's required by Kafka",
    "start": "365000",
    "end": "374620"
  },
  {
    "text": "But Kafka is really kind of the backplane of this whole system, and when it's going to talk a lot about this later today a bit.",
    "start": "375260",
    "end": "380580"
  },
  {
    "text": "It's really a great tool for ingesting Data in a reliable durable way Not really intended for long term storage although some people are kind of pushing the envelope about what that means normally",
    "start": "381529",
    "end": "391179"
  },
  {
    "text": "data lasts for about seven days by default in a Kafka cluster And that means anybody that wants to consume a down stream even like to replace something because something failed or maybe",
    "start": "391789",
    "end": "401349"
  },
  {
    "text": "Decided to change how the analysis was done. You can just go back and reread the data out of Kafka",
    "start": "401569",
    "end": "406329"
  },
  {
    "text": "organized by topic like a message queue one crucial difference though is that unlike most message queues where as soon as somebody reads it they delete the message",
    "start": "406849",
    "end": "416349"
  },
  {
    "text": "Kafka doesn't do it that way they took the harder route But actually more effective of any consumer on a given topic will see the whole thing they won't have to coordinate with other",
    "start": "416839",
    "end": "427238"
  },
  {
    "text": "Consumers who were each consuming messages on the topic? Instead everybody gets to read the whole thing so if I have like a dashboard they get to see it all if I'm doing offline",
    "start": "427430",
    "end": "437108"
  },
  {
    "text": "Analytics they get to see it all training Machine Learning ETc Etc And then there's some mechanisms for basically timing out the data in which case. It's garbage collected as it were",
    "start": "437330",
    "end": "446619"
  },
  {
    "text": "Another important benefit of Kafka for architectural purposes is that it gets rid of deliberately ugly architectures like this",
    "start": "447439",
    "end": "454419"
  },
  {
    "text": "So I I drew every possible combination at least it won't say it's a bipartite graph",
    "start": "454419",
    "end": "459429"
  },
  {
    "text": "so anything on the left and anything on the right is connected and this is obviously messy to manage and messy to draw so if you're the architect",
    "start": "459430",
    "end": "467889"
  },
  {
    "text": "Responsible for all this you know you like the architect and the Matrix it looks like me except I'm not wearing a suit, and this is not such a great thing",
    "start": "468379",
    "end": "475088"
  },
  {
    "text": "The other problem with this is let's say service two crashes then all of a sudden I have all these broken connections",
    "start": "476059",
    "end": "481539"
  },
  {
    "text": "And you know what's going to happen to the data that's coming in from this services on the left is it just lost",
    "start": "481539",
    "end": "485979"
  },
  {
    "text": "Is you know service disrupted how does that work? well if you put Kafka in the middle Then you're basically using the the that famous computer science dictum that you can solve any problem with another level of indirection",
    "start": "486649",
    "end": "498337"
  },
  {
    "text": "So here you go, and I mean does several things for us it cleans up the architecture",
    "start": "498469",
    "end": "503587"
  },
  {
    "text": "it gives us kind of a common protocol if you will I mean if you push this to the extreme then the only thing you know I have to know how to do is talk to Kafka. Whatever service you're doing.",
    "start": "503589",
    "end": "513060"
  },
  {
    "text": "Not necessarily what you want to do all the time, but you could do it this way And then of course because you have this thing in the middle. It's fairly durable if service one goes down",
    "start": "513640",
    "end": "521678"
  },
  {
    "text": "Let's say then you're not going to necessarily lose data as long as you can recover within seven days or whatever not always assured",
    "start": "521690",
    "end": "528399"
  },
  {
    "text": "But hopefully seven days is long enough So Kafka is pretty important in this architecture, but then you've got data in Kafka now. What are we going to do with it?",
    "start": "528399",
    "end": "537429"
  },
  {
    "text": "Well, obviously we need to process it so basically the analog of Mapreduce And the hadoop world is streaming engines if you there's there was a great blog post about a year ago",
    "start": "537430",
    "end": "546770"
  },
  {
    "text": "And I really should put a link in it in this talk some guy Looked at all the apache projects and found there like eleven of them that claim to be streaming engines",
    "start": "546770",
    "end": "554839"
  },
  {
    "text": "So you know the Paradox of choice here? You walk into you know Best Buy, and you see a hundred refrigerators",
    "start": "555080",
    "end": "560340"
  },
  {
    "text": "And you walk out because I'd want to buy the wrong refrigerator. I've got all these choices",
    "start": "560360",
    "end": "564500"
  },
  {
    "text": "So how do you decide what to pick well what I'm going to do is argue that there's maybe four Engines that you really ought to focus on because I think they're the ones that both you know meet all the criteria you could imagine",
    "start": "565470",
    "end": "575480"
  },
  {
    "text": "Like the full spectrum of what you might want to do and streaming, and they're like viable projects",
    "start": "575480",
    "end": "581180"
  },
  {
    "text": "That are going to live more than you know the lifetime of some company that's struggling to survive that sort of thing",
    "start": "581180",
    "end": "586480"
  },
  {
    "text": "But there's certainly others that we could put up on this slide Of course, I'm these are all gonna be open source one, so I'll come back and talk about these in a few minutes",
    "start": "586760",
    "end": "594738"
  },
  {
    "text": "But just to finish this diagram You know these are basically all open source components, so that you can do whatever persistence",
    "start": "594740",
    "end": "600619"
  },
  {
    "text": "You want you can you know write to file systems? Object stores like S3. You know Elasticsearch, and that sort of thing as well as traditional databases",
    "start": "600620",
    "end": "610840"
  },
  {
    "text": "And then the other thing that's nice about this architecture is it does free us up to consider different deployment?",
    "start": "613040",
    "end": "618519"
  },
  {
    "text": "Scenarios so we happen to be big fans of Mesos at Lightbend But some people just run on cloud platforms and leverage whatever they provide for resource management",
    "start": "619080",
    "end": "627859"
  },
  {
    "text": "ETC Kubernetes is now emerging as important in this space and people are doing it on Hadoop YARN as well",
    "start": "628140",
    "end": "634670"
  },
  {
    "text": "but you have a lot of flexibility with the tools I put up here basically everything I drew up here of course I didn't give a lot of details about the persistence tier but",
    "start": "634950",
    "end": "643160"
  },
  {
    "text": "You're not necessarily tied to hit dupe with any of these tools today Ok let's go back and talk about the streaming engines a little bit because there's a lot to consider here when making this choice this",
    "start": "643230",
    "end": "654199"
  },
  {
    "start": "647000",
    "end": "674000"
  },
  {
    "text": "This is a friend of mine on a backpacking trip trying not to fall in the the creek as he retrieves water",
    "start": "654510",
    "end": "659689"
  },
  {
    "text": "So what are some of the things you might want to consider? I said that I put four streaming engines on this slide, so earlier",
    "start": "660800",
    "end": "666959"
  },
  {
    "text": "So why four what what kind of trade-offs do I need to consider when I'm picking one let's say for a particular task",
    "start": "666980",
    "end": "672620"
  },
  {
    "text": "I have to Well one is what is your latency requirement if you're using Spark Streaming you can't get under like 200 milliseconds latency",
    "start": "672620",
    "end": "680909"
  },
  {
    "start": "674000",
    "end": "728000"
  },
  {
    "text": "That's not so great if you're trying to authorize credit cards. Let's say on the fly I talked to somebody to bank a few months ago. Who said that you know when you click buy on a ecommerce site",
    "start": "681009",
    "end": "691949"
  },
  {
    "text": "you know there's something like you know the two usual 200 millisecond response is like the rule of thumb for",
    "start": "692470",
    "end": "697319"
  },
  {
    "text": "When people want to start to get annoyed if they don't get a response on the webpage? So 200 milliseconds that sounds like that would fit within the Spark",
    "start": "697660",
    "end": "704639"
  },
  {
    "text": "Streaming time frame, but no because there's a lot of stuff going on. You know in that",
    "start": "705189",
    "end": "709769"
  },
  {
    "text": "Cycle from you clicking all the way down and back and this guy said they get about 10 milliseconds to make a decision",
    "start": "710500",
    "end": "715769"
  },
  {
    "text": "If their credit card off system has a 10 millisecond budget to decide Yeah, or nay so obviously that's not going to work if you're trying to do that with spark streaming",
    "start": "715930",
    "end": "724769"
  },
  {
    "text": "But the other engines that we'll talk about could handle that just fine So just to walk through some of these scenarios if you're really down at the bleeding edge and some of you are in",
    "start": "724769",
    "end": "733500"
  },
  {
    "start": "728000",
    "end": "761000"
  },
  {
    "text": "finance and may have worked in high frequency trading none of the tools I'm talking about really address this need I mean you're talking about custom hardware. You know Kernel bypass",
    "start": "733750",
    "end": "741539"
  },
  {
    "text": "Network cards and stuff like this Not the sort of thing that is we're really talking about people use the word real-time a lot in this space",
    "start": "742089",
    "end": "750120"
  },
  {
    "text": "And they you know are butchering the definition of the term real-time is more like this you know picoseconds or microseconds",
    "start": "750120",
    "end": "756239"
  },
  {
    "text": "But most people in the big data world are really talking about milliseconds when they're saying real time So we're not even going to talk anymore about that",
    "start": "756759",
    "end": "763740"
  },
  {
    "text": "If you you know Maybe at less than 100 microseconds You could do this with some of the very",
    "start": "764050",
    "end": "769169"
  },
  {
    "text": "high-Performance a JVM based message handlers like the L max disrupt Or if any of you abuse that akka actors can can give you this kind of turnaround?",
    "start": "769569",
    "end": "777839"
  },
  {
    "text": "But you really have to know what you're doing to avoid like garbage collection pauses and so forth Mostly that we tend to work more and maybe the 10 millisecond range or something like that",
    "start": "777880",
    "end": "787049"
  },
  {
    "start": "783000",
    "end": "797000"
  },
  {
    "text": "Where we have a wide range of choices and pretty much all the tools I'm going to talk about with the exception of spark",
    "start": "787180",
    "end": "792719"
  },
  {
    "text": "Streaming could handle the this scenario this sort of latency requirement",
    "start": "792790",
    "end": "796589"
  },
  {
    "start": "797000",
    "end": "810000"
  },
  {
    "text": "and then if you when you're getting into like hundreds of Milliseconds now you might be thinking about like windowing things like I'm going to do a sequel query over the data",
    "start": "798040",
    "end": "806099"
  },
  {
    "text": "And do some window functions group buys that kind of thing and then when we get to",
    "start": "806370",
    "end": "812170"
  },
  {
    "start": "810000",
    "end": "834000"
  },
  {
    "text": "Much longer time frames then you get into the realm of like I want to train a machine learning model",
    "start": "812170",
    "end": "817050"
  },
  {
    "text": "Incrementally as data arrives, but it doesn't I don't have to have the model exactly up-to-date to the millisecond. Maybe every five minutes. I'll",
    "start": "817570",
    "end": "825239"
  },
  {
    "text": "Swap in a new machine Learning model into my stream processor to do recommendations or spam filtering or whatever",
    "start": "825910",
    "end": "832469"
  },
  {
    "start": "834000",
    "end": "891000"
  },
  {
    "text": "And you know actually when if you get to that point where you're talking minutes? I think you really ought to consider Just running batch jobs with cron or some equivalent because one of the cases. I want to argue here",
    "start": "834670",
    "end": "845459"
  },
  {
    "text": "is that stream processing is as bad as Micro-service processing where you've got something you're going to deploy and it could run for months and",
    "start": "845460",
    "end": "853800"
  },
  {
    "text": "If you wait long enough anything is going to happen you're gonna lose hard drives from that time network partitions",
    "start": "854080",
    "end": "859259"
  },
  {
    "text": "You'll you know maybe an S3 availability zone will go down It's happened right so you're gonna run into any kind of contingency plus you know the usual ups and downs and scalability requirements",
    "start": "859810",
    "end": "871200"
  },
  {
    "text": "So a lot of times it's actually easier just to run a lot of batch job So that you know they only have to be good for the few minutes",
    "start": "871200",
    "end": "877049"
  },
  {
    "text": "They're gonna run or maybe hours they go away, then when the next one starts it can be scaled appropriately or whatever",
    "start": "877050",
    "end": "883560"
  },
  {
    "text": "But if you're gonna run something that's a stream engine that's gonna run for months then that kind of raises the bar on your engineering",
    "start": "883779",
    "end": "889979"
  },
  {
    "text": "requirements Another criterion would be what's the kind of volume that? I want to process",
    "start": "889980",
    "end": "895259"
  },
  {
    "start": "891000",
    "end": "909000"
  },
  {
    "text": "Yeah If you're doing you know lots and lots of data like you're ingesting the Twitter firehose that's going to be put some pretty serious",
    "start": "895450",
    "end": "902459"
  },
  {
    "text": "requirements on horsepower Compared to just handling. Maybe rest requests for a moderately busy website",
    "start": "902830",
    "end": "908610"
  },
  {
    "text": "Once again just to put some numbers here if it's less than say 10,000 or so events per second You know we have a lot of tools that can handle this pretty well for classic rest requests",
    "start": "910420",
    "end": "919349"
  },
  {
    "text": "It's not not really you considered a high scale problem when you're getting into the range of hundreds of thousands",
    "start": "919350",
    "end": "925380"
  },
  {
    "start": "922000",
    "end": "936000"
  },
  {
    "text": "Well, you can still do like non-blocking rest or something like this But you're gonna have to be very careful that you don't have bottlenecks in the system",
    "start": "925380",
    "end": "932400"
  },
  {
    "text": "That limit scalability with Amdahl's law and those kind of things but then when you're really getting up into like the millions of events",
    "start": "932400",
    "end": "939390"
  },
  {
    "start": "936000",
    "end": "950000"
  },
  {
    "text": "Records whatever per second then you're probably going to want to do something use something that can actually just you know",
    "start": "939490",
    "end": "944560"
  },
  {
    "text": "partition your data over a cluster and do things in parallel like spark or flank or so forth",
    "start": "944720",
    "end": "949779"
  },
  {
    "start": "950000",
    "end": "977000"
  },
  {
    "text": "And just to wrap up a few other things to think about what kind of integration Do I need with other tools if I want to connect directly to a database for my streaming engine?",
    "start": "950990",
    "end": "958719"
  },
  {
    "text": "Kafka streams for example It is really designed to work within like with Kafka topics reading and writing not so much connecting to other things",
    "start": "958970",
    "end": "965889"
  },
  {
    "text": "There's other tools for that purpose for example, and that's really why this is here because most of the tools",
    "start": "965890",
    "end": "970839"
  },
  {
    "text": "Otherwise will usually provide some sort of api to talk to whatever it is you need to talk to",
    "start": "971720",
    "end": "975910"
  },
  {
    "start": "977000",
    "end": "1001000"
  },
  {
    "text": "And then the last one is really basically maybe the most important question What are you actually trying to do with the data if you're trying to write sequel queries and here?",
    "start": "977089",
    "end": "984909"
  },
  {
    "text": "I just sort of mocked up a little spark example using the spark",
    "start": "984910",
    "end": "989019"
  },
  {
    "text": "Api version of a sequel query then that's that obviously that's going to limit your tools spark is the most mature sequel option",
    "start": "990110",
    "end": "997269"
  },
  {
    "text": "Whereas for example akka streams doesn't provide a sequel capability at all But then on the other hand if you're doing like ETL stuff, then you know",
    "start": "997610",
    "end": "1005729"
  },
  {
    "start": "1001000",
    "end": "1030000"
  },
  {
    "text": "I think this is a wonderful example of for Kafka streams where I just like I have raw log data",
    "start": "1005730",
    "end": "1011699"
  },
  {
    "text": "We're not supposed to use strings for log messages anymore according to the experts, but let's say let's suppose",
    "start": "1011700",
    "end": "1017489"
  },
  {
    "text": "You're old-school and you're still logging strings But you actually want to parse them into some record format for downstream consumption that kind of stuff is one example",
    "start": "1017490",
    "end": "1025079"
  },
  {
    "text": "That's fantastic for a tool like Kafka streams",
    "start": "1025079",
    "end": "1028469"
  },
  {
    "start": "1030000",
    "end": "1042000"
  },
  {
    "text": "and then another like maybe the opposite extreme in terms of Computation is what if I do want to train machine learning models, dynamically rather than just do a big batch training periodically?",
    "start": "1031420",
    "end": "1041459"
  },
  {
    "start": "1042000",
    "end": "1075000"
  },
  {
    "text": "Okay, so I'm kind of belaboring a bunch of points here, but the last I think the last important one is",
    "start": "1043059",
    "end": "1049169"
  },
  {
    "text": "In terms of the kind of processing I'd like to think of events You know like server on fire or something like that to have an identity where I may need to process it individually",
    "start": "1049960",
    "end": "1059969"
  },
  {
    "text": "Whereas something like the Twitter firehose where it's all tweets Yeah, sure, you could drill in and say that who's the person doing this tweet or whatever?",
    "start": "1060790",
    "end": "1068309"
  },
  {
    "text": "But mostly we tend to process that kind of stuff in bulk, and think of it as like anonymous records in a sense",
    "start": "1068309",
    "end": "1073699"
  },
  {
    "start": "1075000",
    "end": "1088000"
  },
  {
    "text": "and then actually some of these engines are better at Individual event processing especially at smaller volumes than they are at bulk and vice versa, okay?",
    "start": "1075929",
    "end": "1084110"
  },
  {
    "text": "so that's a long list of Reasons for picking one tool or the other let's actually look at the the four tools that I I had on this chart",
    "start": "1084110",
    "end": "1091579"
  },
  {
    "start": "1092000",
    "end": "1097000"
  },
  {
    "text": "Okay, so I'm this is sort of the right side of that diagram It turns out that google has been doing stream processing for a long time",
    "start": "1093149",
    "end": "1101689"
  },
  {
    "start": "1097000",
    "end": "1165000"
  },
  {
    "text": "You know who knew? But it turns out that they tend to everything like you know last twenty years everything that we know now is something Google",
    "start": "1101690",
    "end": "1107990"
  },
  {
    "text": "invented and then either Described in a paper or actually a recently open sourced and a good example of the recent phenomenon is they've had several iterations of stream",
    "start": "1108059",
    "end": "1117559"
  },
  {
    "text": "Processing tools, and they thought about all the kind of scenarios you need to worry about in stream processing and the open sourced",
    "start": "1117750",
    "end": "1123289"
  },
  {
    "text": "part of Google Dataflow as apache beam The part that they didn't over open source was the actual Runner",
    "start": "1123840",
    "end": "1130580"
  },
  {
    "text": "Except for like a test runner that just runs in a single VM, and that's actually an interesting Place where other tools like spark and flink are stepping up to function as runners for Data flows defined with apache beam",
    "start": "1130649",
    "end": "1142730"
  },
  {
    "text": "What's great about beam? Is that they as I say they've really thought through all of the sophisticated",
    "start": "1143549",
    "end": "1148788"
  },
  {
    "text": "Semantics that you might run into and stream processing especially if you want to go beyond, just like yeah",
    "start": "1149039",
    "end": "1154128"
  },
  {
    "text": "Let's just kind of compute averages over like the last hour of sales per store But I don't really actually need like an accounting quality number",
    "start": "1154130",
    "end": "1161479"
  },
  {
    "text": "I just want the average but what if you actually wanted to do everything in a stream processing scenario?",
    "start": "1161480",
    "end": "1167209"
  },
  {
    "start": "1165000",
    "end": "1267000"
  },
  {
    "text": "Then you'd have to handle a scenario like this where let's suppose that you know every whatever that time is I said minutes here",
    "start": "1167309",
    "end": "1173628"
  },
  {
    "text": "Let's let's suppose every minute. I want to do some sort of aggregation over some datum",
    "start": "1173630",
    "end": "1179569"
  },
  {
    "text": "That's occurring in my system some metric well You know time-of-flight is obviously not zero for even stuff going with him my cluster in a normally operating cluster",
    "start": "1179570",
    "end": "1189799"
  },
  {
    "text": "There's always going to be some delay which I'm trying to show with the arrows that sort of trend to the right",
    "start": "1189799",
    "end": "1194719"
  },
  {
    "text": "But then you're going to have real problems when you have like a network partition where data might show up ten minutes late Or maybe a day late or something so the question is if I'm if I'm actually let's say feeding a dashboard",
    "start": "1195360",
    "end": "1205669"
  },
  {
    "text": "But also feeding my counting system when do I decide I can actually it's safe to do the Calculation when do I know I have all the data or if I know that I may not have all the data",
    "start": "1205669",
    "end": "1214850"
  },
  {
    "text": "When is it okay to do maybe a provisional calculation with some facility for posting a correction later on if Data arrives late?",
    "start": "1214920",
    "end": "1221779"
  },
  {
    "text": "Or when do I decide that data can be ignored if it arrives too late? These are the kind of questions that you really get into if you think about what it would take to make a streaming engine",
    "start": "1221780",
    "end": "1230900"
  },
  {
    "text": "Like you know I'd like to say accounting quality something that actually is you know 100 100 percent accurate in some sense of the word",
    "start": "1231210",
    "end": "1237260"
  },
  {
    "text": "as opposed to approximate and Google has thought about these sort of things and beam has is really kind of the state of the art in terms of defining these",
    "start": "1237260",
    "end": "1244579"
  },
  {
    "text": "kinds of semantics that you might want to do there's a I think in the speaker notes for the slides which I always post with the slides",
    "start": "1244580",
    "end": "1252050"
  },
  {
    "text": "I think I have some links to some really good talks by people from the beam team if you're interested in More details about all of this, and I think jess you might get into it a little bit later as well",
    "start": "1252050",
    "end": "1261020"
  },
  {
    "text": "But anyway that's sort of the state of the art for a stream data processing and One of the reasons that I really like flank which meant you have actually heard of link by the way",
    "start": "1262050",
    "end": "1271430"
  },
  {
    "start": "1267000",
    "end": "1319000"
  },
  {
    "text": "It's maybe the Lisa I know quite a few that's great one of the reasons I like flank is a it's",
    "start": "1271430",
    "end": "1276690"
  },
  {
    "text": "It it really strives to be an effective runner for beam semantics and by that I mean that all of these Runners",
    "start": "1276690",
    "end": "1283250"
  },
  {
    "text": "Usually do a subset of the available semantics, you know based on what they're capable of and that's obviously a moving Target",
    "start": "1283650",
    "end": "1289550"
  },
  {
    "text": "But flink is ahead of everybody else. I would say other than Google's own dataflow Which is if you're in Google Cloud actually you can use their data flow",
    "start": "1290100",
    "end": "1298380"
  },
  {
    "text": "But if link is also designed, in contrast to Spark, to be a low latency processing engine",
    "start": "1298760",
    "end": "1304480"
  },
  {
    "text": "but still at high volume you know with you know partition Data pipelines and so forth.",
    "start": "1304480",
    "end": "1309140"
  },
  {
    "text": "And they do have early but evolving support for things like SQL over streams and machine learning and so forth but not as mature as Spark.",
    "start": "1309660",
    "end": "1318380"
  },
  {
    "start": "1319000",
    "end": "1406000"
  },
  {
    "text": "So Akka Streams is a streaming api on top of the Akka Actor Model",
    "start": "1319860",
    "end": "1325779"
  },
  {
    "text": "Akka is one of the products that Lightbend supports it, but again. It's open source",
    "start": "1326300",
    "end": "1330480"
  },
  {
    "text": "I really like Akka actors as a concurrency primitives that removed the need for me to worry about thread safety",
    "start": "1332780",
    "end": "1338960"
  },
  {
    "text": "And those kind of basically an actor is like really It's like an object where it is guaranteed to be thread-safe by the implementation",
    "start": "1338960",
    "end": "1345549"
  },
  {
    "text": "And you send it messages for work to do and then it processes them one at a time So it's a really nice primitive, but it is a pretty low level primitive",
    "start": "1345770",
    "end": "1353400"
  },
  {
    "text": "So if you're really thinking about a stream processing scenario then it's nice to have a streaming API",
    "start": "1353600",
    "end": "1358539"
  },
  {
    "text": "And that's what Akka streams is designed to do So because of the fact it's built in on actors. It's very very low latency per event",
    "start": "1358640",
    "end": "1367100"
  },
  {
    "text": "It's really great for complex event processing and I should mention Flink has a growing complex event processing",
    "start": "1367720",
    "end": "1373480"
  },
  {
    "text": "component as well This one is one of those engines where if you don't have massive datasets?",
    "start": "1374560",
    "end": "1379420"
  },
  {
    "text": "It's actually very efficient on a per event basis as opposed to something like Spark where even though Spark is generally thought of as efficient",
    "start": "1379880",
    "end": "1387220"
  },
  {
    "text": "If you throw like you know thousands of bytes at it Then you're just going to have a lot of overhead that's going to swamp out the the actual computation work so you don't get much",
    "start": "1387640",
    "end": "1396490"
  },
  {
    "text": "amortization over the Data But if you need something, that's good at its small data up to missing medium sized data then Akka Streams is great for that",
    "start": "1396560",
    "end": "1404859"
  },
  {
    "start": "1406000",
    "end": "1471000"
  },
  {
    "text": "Kafka Streams is a tool that's really oriented towards the Kafka ecosystem. So if you're really based on Kafka as your core",
    "start": "1407210",
    "end": "1414789"
  },
  {
    "text": "You know your data pipeline in the middle your back playing and Kafka Streams is A really good engine for writing a lot of stuff I mentioned that ETL scenario already another nice abstraction",
    "start": "1415340",
    "end": "1426249"
  },
  {
    "text": "They have in Kafka Streams is something called KTables Or it basically it's a table abstraction again think about this scenario where I have like a dashboard",
    "start": "1426380",
    "end": "1434709"
  },
  {
    "text": "And I just want to see like aggregations like average per minute or what's the last high-water mark for a given key?",
    "start": "1434710",
    "end": "1441039"
  },
  {
    "text": "sort of sort of a dual case of instead of seeing every event I just want to see some roll-up of that like a typical database hence the name \"KTable\". It's really nice for that",
    "start": "1441820",
    "end": "1451320"
  },
  {
    "text": "Kafka Streams does not try to be like the solution to every streaming problem",
    "start": "1452060",
    "end": "1457320"
  },
  {
    "text": "But it's really good for a lot of scenarios where I just I need to read some stuff out of a Kafka topic",
    "start": "1457330",
    "end": "1462460"
  },
  {
    "text": "manipulate it in some way and write it back to Kafka for downstream consumption like that log ETL scenario. I mentioned earlier",
    "start": "1462860",
    "end": "1469780"
  },
  {
    "start": "1471000",
    "end": "1571000"
  },
  {
    "text": "And then there's Spark which probably is the one that you all have heard about the most.",
    "start": "1472620",
    "end": "1476540"
  },
  {
    "text": "Spark originally started as a batch mode system just like Mapreduce, but because it's relatively efficient they came up with this clever hack. You know, what if we just capture windows of data",
    "start": "1477800",
    "end": "1488960"
  },
  {
    "text": "You know like fixed time intervals And then just run little batch jobs over that data and that actually works pretty well for the most part",
    "start": "1489440",
    "end": "1496340"
  },
  {
    "text": "But it does limit the latency I said 200 milliseconds earlier. That's like the absolute lowest limit It's more realistic to be like half a second latency that would be like the this window size",
    "start": "1496340",
    "end": "1506080"
  },
  {
    "text": "but with Spark you get lots of very rich sequel semantics and capabilities a",
    "start": "1507260",
    "end": "1512300"
  },
  {
    "text": "pretty nice Machine Learning library And the and there is a group I think led by some people at Cloudera to actually implement those Beam semantics on top of Spark Streaming",
    "start": "1513300",
    "end": "1522640"
  },
  {
    "text": "This is one of the areas where Spark is really evolving Quickly because they know that this latency is actually a drawback for them because it opens the door for tools like Flink",
    "start": "1522740",
    "end": "1531140"
  },
  {
    "text": "So we expect that over time they'll replace the core of Spark So that it can actually be a true streaming engine and not a mini-batch engine",
    "start": "1531380",
    "end": "1539810"
  },
  {
    "text": "But I would typically turn to spark you know in a complex architecture like this Use Spark Streaming for things like training my Machine Learning models for example even though",
    "start": "1540060",
    "end": "1549860"
  },
  {
    "text": "I may not be able to actually score data with those models if I have lower latency requirements",
    "start": "1549860",
    "end": "1554148"
  },
  {
    "text": "So in fact one of the things we're thinking about at Lightbend is, well If I do that if I score with say Flink, but I want to train with Spark",
    "start": "1555300",
    "end": "1562759"
  },
  {
    "text": "How do I share those models and there are a lot of ways to do it but right now? You kind of have to roll your own so we're thinking about how to make that easier",
    "start": "1562760",
    "end": "1570480"
  },
  {
    "start": "1571000",
    "end": "1593000"
  },
  {
    "text": "And as I said because it was started out as a batch engine you can actually",
    "start": "1571760",
    "end": "1575800"
  },
  {
    "text": "you know write code maybe the core of your code that does some sort of processing and then run it in a batch mode or run it in a semi batch or mini batch mode, so",
    "start": "1576870",
    "end": "1586998"
  },
  {
    "text": "If you know the lambda architecture this actually makes it easier to implement the Lambda Architecture",
    "start": "1587700",
    "end": "1592309"
  },
  {
    "start": "1593000",
    "end": "1619000"
  },
  {
    "text": "Okay I want to talk a little bit about",
    "start": "1593520",
    "end": "1599180"
  },
  {
    "text": "this idea of Microservices and fast data and the kind of Synergy that's emerging as I see it",
    "start": "1599710",
    "end": "1605850"
  },
  {
    "text": "This is a cool sliced rock in a pond up in the sierras. They're actually a big lake",
    "start": "1606280",
    "end": "1611430"
  },
  {
    "text": "So I thought they look like Micro-services. I meant many lifts instead of Macro Lifts or something",
    "start": "1612400",
    "end": "1617490"
  },
  {
    "start": "1619000",
    "end": "1650000"
  },
  {
    "text": "Okay, so the question is this how is something like this Look like something like this, and this is just sort of a made-up diagram of a typical",
    "start": "1620290",
    "end": "1628860"
  },
  {
    "text": "Micro service deployment where you know my ecommerce site has separate micro services You know one for order processing one for account Management etc each of them has its own storage",
    "start": "1629860",
    "end": "1640739"
  },
  {
    "text": "That's usually a pattern that's supposed to be good and they communicate through Messaging to get services from each other as needed so just you know Strawman micro service",
    "start": "1640930",
    "end": "1649470"
  },
  {
    "start": "1650000",
    "end": "1691000"
  },
  {
    "text": "Well, I think there's actually some interesting synergies here The first is that a typical data application like that ETL job that you're gonna write in Kafka Streams",
    "start": "1650380",
    "end": "1660059"
  },
  {
    "text": "Or maybe your daily roll-ups that you're doing in and spark sequel they tend to be",
    "start": "1660060",
    "end": "1665429"
  },
  {
    "text": "Something that has a single responsibility. They're small and for some definition of small You know Spark may be massive these jobs might be running with you know 10 gigabyte heaps or something",
    "start": "1666360",
    "end": "1677659"
  },
  {
    "text": "But the code you ride tends to look really small like a very simple Microservice for some definition of micro",
    "start": "1677669",
    "end": "1683299"
  },
  {
    "text": "Much like micro services themselves are supposed to be single responsibility and and delegate to other micro services for other things",
    "start": "1684580",
    "end": "1690960"
  },
  {
    "start": "1691000",
    "end": "1706000"
  },
  {
    "text": "You know as I mentioned at the beginning both of them are gonna have to be able to process? data that never stops arriving whether it's requests for service or",
    "start": "1691930",
    "end": "1699749"
  },
  {
    "text": "Its actual you know the Twitter firehose or something like that So they have to remain responsive you know they have to be available all the time they have to be",
    "start": "1700360",
    "end": "1709020"
  },
  {
    "start": "1706000",
    "end": "1719000"
  },
  {
    "text": "Scalable and resilient all these things there's this idea of reactive programming that is kind of designed to encapsulate these sort of core",
    "start": "1709210",
    "end": "1716699"
  },
  {
    "text": "requirements and micro services so I'm gonna argue also that if you think about a successful company building micro services",
    "start": "1717370",
    "end": "1726809"
  },
  {
    "start": "1719000",
    "end": "1755000"
  },
  {
    "text": "Eventually data becomes their dominant problem, and when Twitter started it was basically a three-tier web app. You know for a bunch of guys within",
    "start": "1727150",
    "end": "1734769"
  },
  {
    "text": "frigga what the name of the company was that Twitter actually started as But of course his people signed up for Twitter then suddenly",
    "start": "1735409",
    "end": "1741939"
  },
  {
    "text": "They had the the so-called Justin Bieber problem where every time he tweeted something they had to you know",
    "start": "1741940",
    "end": "1747429"
  },
  {
    "text": "Broadcast to millions of followers what he said and that just raised the bar on making their architecture more data centric",
    "start": "1747980",
    "end": "1754119"
  },
  {
    "start": "1755000",
    "end": "1787000"
  },
  {
    "text": "So I actually think that we're seeing this happen that if you think about the past where we had like Monolithic services and big data where there really wasn't a whole lot of overlap between them",
    "start": "1755330",
    "end": "1764199"
  },
  {
    "text": "They the engineers let's say in each of these worlds tended to think about different problems",
    "start": "1764200",
    "end": "1768939"
  },
  {
    "text": "That now they're starting to look more alike that Microservice people are kind of overwhelmed by Data if they're successful and fast data people have to be really good at writing highly available",
    "start": "1769789",
    "end": "1781359"
  },
  {
    "text": "durable resilient etc services, so I think there's a lot of overlap here",
    "start": "1781640",
    "end": "1786000"
  },
  {
    "start": "1787000",
    "end": "1868000"
  },
  {
    "text": "So just a few shameless plugs from from Lightbend. I'm actually leading the team that's building a platform and I did it in color because you know it's a commercial product, so",
    "start": "1787100",
    "end": "1795460"
  },
  {
    "text": "Yeah, I actually drew a piece, but anyway, we're trying to think about these problems, and how would we help people build these architectures that?",
    "start": "1798919",
    "end": "1806617"
  },
  {
    "text": "blend Microservice, and and in fast Data streaming tools in a coherent whole",
    "start": "1807230",
    "end": "1812499"
  },
  {
    "text": "We've talked about these pieces circled in red obviously a commercial product like this you would expect to have",
    "start": "1813289",
    "end": "1821949"
  },
  {
    "text": "Production tooling, so we have monitoring tooling and management tooling as well And we're actually focused on Enterprise DC/OS which is the commercial version Mesos as our deployment platform?",
    "start": "1822860",
    "end": "1833840"
  },
  {
    "text": "And we also think there's a really interesting opportunity for Machine Learning In this situation because as I mentioned these services have to run forever and so, how are they going to remain reliable?",
    "start": "1834620",
    "end": "1845120"
  },
  {
    "text": "Well, I think the way to do that as much as possible make them the cluster self managing and self-healing",
    "start": "1845389",
    "end": "1851079"
  },
  {
    "text": "So we're spending a lot of time you know building services around that idea to make it. You know truly",
    "start": "1851080",
    "end": "1857199"
  },
  {
    "text": "Let's face it once you get used to installing these clusters and writing these apps then maybe you don't need a commercial distribution much",
    "start": "1857720",
    "end": "1864470"
  },
  {
    "text": "But we hope this will actually keep it valuable forever Well, I've actually finished a little early. So here's the link again for that that book and I thank you very much",
    "start": "1864470",
    "end": "1876229"
  },
  {
    "text": "Any questions Yes the question was to repeat for the video",
    "start": "1883590",
    "end": "1890360"
  },
  {
    "start": "1886000",
    "end": "2015000"
  },
  {
    "text": "How do you deal with merging streams especially when they're like a different velocities different rates?",
    "start": "1890360",
    "end": "1896089"
  },
  {
    "text": "well if you watch ghost busters, you should never cross your streams, so Just professional tip there",
    "start": "1896460",
    "end": "1903260"
  },
  {
    "text": "Yeah, that's that's an interesting one because most of these tools do provide some notion of a join",
    "start": "1904350",
    "end": "1910250"
  },
  {
    "text": "And that is a really interesting question when you don't have all the data What does it actually mean to do a join between say two tables in that sense?",
    "start": "1910250",
    "end": "1917780"
  },
  {
    "text": "again these are some of the semantics that people like the Beam team have thought about And there's just a whole bunch of solutions like for example",
    "start": "1919400",
    "end": "1926760"
  },
  {
    "text": "if some of the data is small a lot of these engines are now building the capabilities to keep moving state in memory and with maybe backing store of some kind so that you could",
    "start": "1926779",
    "end": "1937189"
  },
  {
    "text": "Let's say you have it's some sort of lookup table, and it's slightly evolving. I don't know let's say It's like a SKU catalog in your store",
    "start": "1937380",
    "end": "1944090"
  },
  {
    "text": "And you want to actually do joins against the real-time traffic in the store against the current catalog or something like that",
    "start": "1944090",
    "end": "1950148"
  },
  {
    "text": "That kind of stuff you would I think is really doing some amazing stuff in this area right now to make it easy to get this kind of",
    "start": "1950360",
    "end": "1958560"
  },
  {
    "text": "out-of-band data into the system or even write it out of the system as you go. But a lot of times you can actually think of these as a stream itself that you're just going to",
    "start": "1959240",
    "end": "1968880"
  },
  {
    "text": "You know like to go back the example the SKU catalog is going to be a stream And I've got my you know traffic as a stream",
    "start": "1969240",
    "end": "1976340"
  },
  {
    "text": "And I'm just going to do joins and as they come in so then you'd have to figure out well",
    "start": "1976340",
    "end": "1980929"
  },
  {
    "text": "Okay, that seems that makes sense if it's maybe Delta's to the catalog that I want to update",
    "start": "1981570",
    "end": "1987138"
  },
  {
    "text": "But what if I just want the whole thing? What does that actually mean in this case, so I think it?",
    "start": "1987139",
    "end": "1991549"
  },
  {
    "text": "Brings up some examples to really point out that it probably depends a lot on the scenario",
    "start": "1992400",
    "end": "1997100"
  },
  {
    "text": "but these are all problems that these streaming engines are trying to address in one way or the other and",
    "start": "1997420",
    "end": "2002580"
  },
  {
    "text": "In a particular case you'd end up you know figuring out what makes sense. And a lot of times",
    "start": "2002580",
    "end": "2007640"
  },
  {
    "text": "what would make sense depends on the amount of Data you're talking about for either one or both streams.",
    "start": "2007640",
    "end": "2013250"
  },
  {
    "start": "2015000",
    "end": "2076000"
  },
  {
    "text": "Yeah, so the question was what is the runner do in the beam case if we've already got orchestrators? And I assume you mean like meso sand yarn and so forth. It's really well if you think about it, it's",
    "start": "2015870",
    "end": "2025400"
  },
  {
    "text": "It's it's the way you define the data flow and how you actually run it how you how you go from like a good analog would be how you write an",
    "start": "2026460",
    "end": "2034759"
  },
  {
    "text": "Sequel query in the abstract and turn it into a physical representation that's actually run against real Data files real tasks distribute across a cluster if you've ever looked at",
    "start": "2034950",
    "end": "2046070"
  },
  {
    "text": "Like the described command of a sequel query you know it'll show you this breakdown from like a logical query down to a physical query and basically what you're what you're",
    "start": "2046950",
    "end": "2055219"
  },
  {
    "text": "Putting in there with the runner is sort of that physical manifestation of this more logical Data flow",
    "start": "2055320",
    "end": "2061369"
  },
  {
    "text": "So that's that's sort thuggin all to how those tests are going to be scheduled in at runtime and and managed basically",
    "start": "2061649",
    "end": "2069259"
  },
  {
    "text": "anybody Else Yes",
    "start": "2070440",
    "end": "2075110"
  },
  {
    "start": "2076000",
    "end": "2186000"
  },
  {
    "text": "Apache Storm that's a really good question I should probably add a slide about this I think pragmatically speaking",
    "start": "2076780",
    "end": "2082979"
  },
  {
    "text": "Storm seems to be vanishing from the landscape independent of its qualities as a as an engine",
    "start": "2084250",
    "end": "2090209"
  },
  {
    "text": "You know it's it's it's sort of the usual dilemma where it's not always the best Thing that wins or the thing that's the newest tends to be you get the attention",
    "start": "2090210",
    "end": "2099810"
  },
  {
    "text": "What I would do if I were really devoted to storm is I would take a look at a Twitter Heron which was their complete rewrite. That's api compatible I",
    "start": "2100270",
    "end": "2108930"
  },
  {
    "text": "Don't know a lot about it I've talked to the engineers at Twitter about it a little bit and and they've they've addressed some of the limitations of storms such as",
    "start": "2109510",
    "end": "2117689"
  },
  {
    "text": "The problem of trying to evolve the the the graph of nodes, so I forget the topology",
    "start": "2118540",
    "end": "2124499"
  },
  {
    "text": "And I think they've enabled that ability so that it can now be more dynamic at runtime if you think about the way Storm",
    "start": "2125230",
    "end": "2131909"
  },
  {
    "text": "typically works you kind of lay out the topology of the nodes and then you run data through them",
    "start": "2132130",
    "end": "2137190"
  },
  {
    "text": "Whereas what tools like flink and spark are doing instead is sort of dynamically building that topology based on a kind of abstraction",
    "start": "2137370",
    "end": "2144870"
  },
  {
    "text": "Which is the program you write as opposed to having dedicated nodes that are more statically oriented?",
    "start": "2145150",
    "end": "2151260"
  },
  {
    "text": "That's my understanding of the drawbacks of storm the way I tend to think of it is it's it's like the first generation",
    "start": "2151480",
    "end": "2156090"
  },
  {
    "text": "Stream processing engine and a really good one, but we've kind of learned a lot in the last you know",
    "start": "2156490",
    "end": "2161939"
  },
  {
    "text": "It's only been like seven years which is basically 49 dog years. I guess so it feels like a long time but",
    "start": "2161940",
    "end": "2167369"
  },
  {
    "text": "And I think some of the newer entrants are trying to You know leverage what we've learned and be more adaptable, but yeah, I certainly wouldn't remove storm",
    "start": "2168490",
    "end": "2177539"
  },
  {
    "text": "Just because you know it's not the sexy thing right now Anybody else yes?",
    "start": "2177540",
    "end": "2184320"
  },
  {
    "start": "2186000",
    "end": "2264000"
  },
  {
    "text": "NiFi I yeah, so this is a really interesting Apache project. The question is about NiFi.",
    "start": "2186400",
    "end": "2191549"
  },
  {
    "text": "It's an interesting Apache project. Where you basically graphically draw out your stream flow and then it generates the tasks for you",
    "start": "2192820",
    "end": "2200070"
  },
  {
    "text": "The only thing I dislike about it. Is that they do use their own proprietary Engines it but it's still open source of propriety in the sense that it's not like spark or something",
    "start": "2201760",
    "end": "2210309"
  },
  {
    "text": "I think that's a really great idea Especially for novice users that want to define streams the only thing I'm really wonder about it",
    "start": "2210950",
    "end": "2219130"
  },
  {
    "text": "though actually is Where as SQL queries are the sort of thing that everybody wants to be able to write no matter what their level of expertise?",
    "start": "2219130",
    "end": "2226360"
  },
  {
    "text": "I'm not entirely sure that people are going to define stream processing flows",
    "start": "2226360",
    "end": "2230229"
  },
  {
    "text": "Arbitrarily that seems more like an engineering job to me I'd like to be proven wrong at the very least what I would really love to see NiFi",
    "start": "2231380",
    "end": "2238930"
  },
  {
    "text": "evolved into is actually both the definition tool for streams but also like a visualizer of runtime behavior of the stream if you could have the same thing in one view",
    "start": "2238930",
    "end": "2248920"
  },
  {
    "text": "Then I would use it for sure Because then I think it would be really useful to you know both see what I wrote and then see it in runtime",
    "start": "2249140",
    "end": "2255130"
  },
  {
    "text": "Yeah",
    "start": "2257600",
    "end": "2259600"
  },
  {
    "start": "2264000",
    "end": "2364000"
  },
  {
    "text": "Next year. Yeah, that's again seven dog years so it's a little hard to know",
    "start": "2264859",
    "end": "2268689"
  },
  {
    "text": "What would it look like? I think actually that you'll see Beam a lot more",
    "start": "2269860",
    "end": "2275240"
  },
  {
    "text": "I think it's kind of one of those it's interesting about Beam is I was talking with Jessie about this yesterday",
    "start": "2275260",
    "end": "2280720"
  },
  {
    "text": "They're going to adopt a Scala API that Spotify wrote right yeah, and",
    "start": "2281000",
    "end": "2287580"
  },
  {
    "text": "Yeah, I'm a Scala developer as you may know Working for a Lightbend may be no surprise, so I'm really a bigot about Scala",
    "start": "2288529",
    "end": "2294818"
  },
  {
    "text": "It's a strong word But I actually don't like the Beam API that much, at least the Java one, compared to the Scala API's",
    "start": "2295300",
    "end": "2301700"
  },
  {
    "text": "So I would I Would love to see a Scala API and but but the reason I mentioned this is because what seems to be emerging is that Beam",
    "start": "2302080",
    "end": "2309340"
  },
  {
    "text": "might be the meta-API that sits on top of all of these others so in a year",
    "start": "2309340",
    "end": "2314390"
  },
  {
    "text": "We might be talking about not using the Spark API or Kafka Streams or whatever we might be all writing to the Beam API",
    "start": "2314390",
    "end": "2321068"
  },
  {
    "text": "and then materializing underneath with a runner that makes sense for whatever the like the data",
    "start": "2321140",
    "end": "2327909"
  },
  {
    "text": "format looks like or some per units whatever that kind of stuff So I think beam is going to be the thing to watch in that regard",
    "start": "2328430",
    "end": "2335348"
  },
  {
    "text": "That's a really good question of other things I think you'll definitely see more Machine Learning in a streaming",
    "start": "2337039",
    "end": "2341779"
  },
  {
    "text": "context and more SQL in a streaming context because once again everybody loves to write SQL and they don't want to give that up just",
    "start": "2342040",
    "end": "2348260"
  },
  {
    "text": "because their stream processing now instead of Batch processing So that's always a good way out of a question like that. It's just more of the same",
    "start": "2348280",
    "end": "2356079"
  },
  {
    "text": "if there's a question here",
    "start": "2357410",
    "end": "2359410"
  },
  {
    "start": "2364000",
    "end": "2435000"
  },
  {
    "text": "What about turning legacy data sets and sources into streams? Well, I think the if you ask the Beam guys or any and also the Flink guys this question what they'll say is that really?",
    "start": "2365000",
    "end": "2375409"
  },
  {
    "text": "A batch job is just a finite stream And that's how they actually implement batch in flank for example is just treat the stream is finite",
    "start": "2375409",
    "end": "2382849"
  },
  {
    "text": "So that's one way to do it You could certainly connect a stream job to a legacy database and have it just process say the whole table",
    "start": "2383040",
    "end": "2390679"
  },
  {
    "text": "And then it's finished or you could I even I don't know if I've seen anybody do this But you like here's the interesting use for Spark Streaming",
    "start": "2390680",
    "end": "2397860"
  },
  {
    "text": "Maybe suppose that you have periodic updates to a database And you want Spark to watch that and then",
    "start": "2397880",
    "end": "2403440"
  },
  {
    "text": "immediately grab those deltas and then process them for some downstream use so so I think you're gonna see a lot more of that sort",
    "start": "2403620",
    "end": "2409729"
  },
  {
    "text": "of thing it kind of pushes a little hard on what databases are used to dealing with in terms of connections and the kind of",
    "start": "2409729",
    "end": "2415840"
  },
  {
    "text": "processing but The other cool thing about these open source projects is you know if your legacy batch stuff is just like HDFS files",
    "start": "2416040",
    "end": "2424440"
  },
  {
    "text": "You know it's no different as far as Spark or Flink are concerned You're just going to read them in in and process them and maybe join them against a live data as well",
    "start": "2424980",
    "end": "2433460"
  },
  {
    "start": "2435000",
    "end": "2542000"
  },
  {
    "text": "Yeah, so it to clarify you clarify this question for the video that you know what about I'm going to convert like a batch oriented",
    "start": "2435540",
    "end": "2442729"
  },
  {
    "text": "applications to a more stream processing And a couple comments about that I've been talking with people",
    "start": "2444180",
    "end": "2449920"
  },
  {
    "text": "You know a lot of times you're in an industry where that is really hard Just because you might only be getting data once a day like medical is really bad about this you know they'll still FTP a file",
    "start": "2450200",
    "end": "2460250"
  },
  {
    "text": "to you once a day, so there's no way you're gonna do stream processing anyway you know it's kind of defeats the point so if but if you can make that transformation then the next question",
    "start": "2460250",
    "end": "2470179"
  },
  {
    "text": "I would ask is really. What is your latency requirement? you know is it a nice to have to have like subset and",
    "start": "2470180",
    "end": "2476119"
  },
  {
    "text": "Processing or is really 10 seconds or a minute or whatever fast enough because if it is then you have a lot more",
    "start": "2476339",
    "end": "2482089"
  },
  {
    "text": "Options like maybe just running those batch jobs more frequently on smaller Data Data sets that are arriving",
    "start": "2482220",
    "end": "2488419"
  },
  {
    "text": "You know new or whatever and then you avoid all of this effort. I mentioned about keeping a streaming job happy for months on end",
    "start": "2488420",
    "end": "2495048"
  },
  {
    "text": "But if you really do get to the especially at larger volumes where you really want to process stuff on the fly",
    "start": "2496220",
    "end": "2501520"
  },
  {
    "text": "Maybe because you'd like to have up-to-date and like Search engines are a great example of this yeah breaking news you want to see it on your search engine right away, then",
    "start": "2501550",
    "end": "2511058"
  },
  {
    "text": "What I would probably do is I would avoid the lambda architecture actually because I think it's too hard to implement both a stream",
    "start": "2512480",
    "end": "2519129"
  },
  {
    "text": "Processing in a batch mode if you can avoid it But a lot of these batch jobs if you the logic can actually be repurposed as streaming",
    "start": "2519290",
    "end": "2527349"
  },
  {
    "text": "But you might have to use a new tool set to do it Like and this is something Spark is actually very handy for if you're using Spark already, so that was kind of a rambling answer",
    "start": "2527349",
    "end": "2536660"
  },
  {
    "text": "But hopefully it gave you some thoughts maybe one more question go ahead",
    "start": "2536680",
    "end": "2540500"
  },
  {
    "start": "2542000",
    "end": "2717000"
  },
  {
    "text": "Yeah, sure, so the question was about I made this comment about the challenge of sharing models between the training engine like maybe Spark Mini-batch and",
    "start": "2542680",
    "end": "2551040"
  },
  {
    "text": "you're scoring engine which could be Flink or Akka or something like that So here's there's a bunch of ways to do this one is if you're using a lot of these",
    "start": "2551360",
    "end": "2561250"
  },
  {
    "text": "big Machine Learning systems like Deeplearning4j Or TensorFlow you can run them as servers",
    "start": "2561680",
    "end": "2567818"
  },
  {
    "text": "So if you can tolerate the overhead of making a restful call to a server to do the scoring",
    "start": "2567890",
    "end": "2572859"
  },
  {
    "text": "So we're on the scoring side now. You've got tensorflow also training the models in some process and now I've got data",
    "start": "2573349",
    "end": "2580149"
  },
  {
    "text": "I want to score against the the model You can use tensorflow as a server so at least everything from the machine learning perspective is in one place. It's in tensor flow",
    "start": "2580150",
    "end": "2588669"
  },
  {
    "text": "if it's a fairly simple model like logistic regression or something like that where you could actually just take the",
    "start": "2589579",
    "end": "2595209"
  },
  {
    "text": "parameters that were trained and then just reimplementation Over in your scoring engine",
    "start": "2595430",
    "end": "2601339"
  },
  {
    "text": "Then you could move the parameters over there's several ways that are kind of Emerging to do this, and I'll just reel them off one is you could write those to a database?",
    "start": "2601339",
    "end": "2609480"
  },
  {
    "text": "And you have some process that periodically refreshes from the database with Flink they have this",
    "start": "2609480",
    "end": "2615280"
  },
  {
    "text": "Side Input and Output feature. I think that's the name of it for a member that lets you you read data",
    "start": "2615820",
    "end": "2621660"
  },
  {
    "text": "you know you have the stream processing thing going on and then periodically you read Data from the And that's how you could bring in your parameters",
    "start": "2622460",
    "end": "2629060"
  },
  {
    "text": "And you could do that with a database you could do that writing to a Kafka topic and then bring that in this bit might be another example of the other gentleman's question about maybe I'm actually I'm sort of doing a",
    "start": "2630580",
    "end": "2640330"
  },
  {
    "text": "pseudo-join where I have here's my model parameters coming in here's my data, and I somehow joining them",
    "start": "2640330",
    "end": "2646520"
  },
  {
    "text": "that's a bit awkward to do actually but And then the last one, I'll mention is for really big models like Neural Network models",
    "start": "2646820",
    "end": "2655329"
  },
  {
    "text": "especially in a distributed context there are these parameter servers that are emerging as a way of as fast as they're like a customized database really for",
    "start": "2655370",
    "end": "2664799"
  },
  {
    "text": "Serving the parameters of a model to engine so I so for example. Maybe in TensorFlow I train a neural network. I write to this parameter server, and then I pull them into a different model over in Flink",
    "start": "2665440",
    "end": "2677440"
  },
  {
    "text": "let's say. And the last one that's kind of related is there are these interchange formats like",
    "start": "2677440",
    "end": "2682359"
  },
  {
    "text": "PMML and PFA and although they have their critics and their limitations",
    "start": "2682550",
    "end": "2686560"
  },
  {
    "text": "in that sense you're really just exchanging the Metadata about the model or the parameters and then once again if you have a",
    "start": "2687620",
    "end": "2695049"
  },
  {
    "text": "way of reproducing the model in both environments either through a library or a separate implementation, then you could also do it that way",
    "start": "2695810",
    "end": "2702519"
  },
  {
    "text": "So that was off the top of my head just a bunch of ways that it can be done",
    "start": "2702980",
    "end": "2707710"
  },
  {
    "text": "Okay, thanks again",
    "start": "2708160",
    "end": "2710160"
  }
]