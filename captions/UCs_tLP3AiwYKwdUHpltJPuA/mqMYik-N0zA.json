[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "[Music] all right welcome everybody thanks for",
    "start": "6990",
    "end": "13480"
  },
  {
    "text": "joining this session so we're talking about data science being delivered",
    "start": "13480",
    "end": "19510"
  },
  {
    "text": "continuously today and this Christian Christian is chief architect at",
    "start": "19510",
    "end": "26550"
  },
  {
    "text": "autoscout24 but he actually joined autoscout as a mere developer and then made his way to his current role and we",
    "start": "26550",
    "end": "35170"
  },
  {
    "text": "as taught works we've worked quite a lot with Christian over the time at Scout and I think I can",
    "start": "35170",
    "end": "41620"
  },
  {
    "text": "say that we've enjoyed each other's company quite a bit so and I have with",
    "start": "41620",
    "end": "48190"
  },
  {
    "start": "45000",
    "end": "127000"
  },
  {
    "text": "me Arif and I will agree on the complement that was a pleasure working with sort workcenters Arif on this",
    "start": "48190",
    "end": "55300"
  },
  {
    "text": "project we both were there from the beginning assumed so",
    "start": "55300",
    "end": "60430"
  },
  {
    "text": "Arif is senior consultant at sort works Scala is his favorite language of choice",
    "start": "60430",
    "end": "67469"
  },
  {
    "text": "especially in the context of big data applications and before joining sort",
    "start": "67469",
    "end": "72610"
  },
  {
    "text": "works he has been in academia doing research on applying functional programming techniques to data",
    "start": "72610",
    "end": "78130"
  },
  {
    "text": "synchronization and I assume you know autoscout how's the code 24 is a",
    "start": "78130",
    "end": "85590"
  },
  {
    "text": "used-car listing platform it's the",
    "start": "85590",
    "end": "91150"
  },
  {
    "text": "biggest to use car marketplace europe-wide we are present in in 18 countries and everything has to do with",
    "start": "91150",
    "end": "98259"
  },
  {
    "text": "use listings that are offered by dealers and private sellers and for sale and you can contact those who are trying to sell",
    "start": "98259",
    "end": "105490"
  },
  {
    "text": "a car we have about 2.4 million listings on our platform at every given time and",
    "start": "105490",
    "end": "111729"
  },
  {
    "text": "this is the data we will be talking about what we can do with this data and we have a lot of users on the platform",
    "start": "111729",
    "end": "118540"
  },
  {
    "text": "so typically all our engineering problems involve some form of scalability issues movie or which we",
    "start": "118540",
    "end": "125259"
  },
  {
    "text": "will also see later on yes so christian",
    "start": "125259",
    "end": "130569"
  },
  {
    "text": "outlined autoscout has a lot of data and specifically our scott has a lot of data",
    "start": "130569",
    "end": "138099"
  },
  {
    "text": "about how cars and at what prices and now our task when",
    "start": "138099",
    "end": "144280"
  },
  {
    "text": "we started this particular project was to turn all this data into something actually useful for the end-user so our",
    "start": "144280",
    "end": "153130"
  },
  {
    "text": "task was to create a consumer facing data product where users can quickly",
    "start": "153130",
    "end": "158530"
  },
  {
    "text": "estimate the current value of their used car and its product that we built works",
    "start": "158530",
    "end": "164800"
  },
  {
    "text": "as follows so here you see the German input page of that product and you see",
    "start": "164800",
    "end": "172480"
  },
  {
    "text": "you put in some basic information about your car like make the year of which the",
    "start": "172480",
    "end": "178540"
  },
  {
    "text": "first registration the model fuel type few other things and then importantly",
    "start": "178540",
    "end": "183820"
  },
  {
    "text": "the mileage because it's about used cars right and then you go to the next page",
    "start": "183820",
    "end": "191080"
  },
  {
    "text": "and you can optionally indicate extra equipment or the condition of the car",
    "start": "191080",
    "end": "196810"
  },
  {
    "text": "and then you get a price range so that is what we believe the car is still",
    "start": "196810",
    "end": "204040"
  },
  {
    "text": "worse so here we say that this particular Forks were angles you can",
    "start": "204040",
    "end": "210610"
  },
  {
    "text": "still get something between 11,000 and 12,000 700 euro for this of course this",
    "start": "210610",
    "end": "218410"
  },
  {
    "text": "is just an estimate so therefore we show a range here because we only have so",
    "start": "218410",
    "end": "223630"
  },
  {
    "text": "much information about the car especially the condition yeah can vary quite a lot so this is what we built now",
    "start": "223630",
    "end": "234280"
  },
  {
    "start": "233000",
    "end": "431000"
  },
  {
    "text": "what we actually had when we started this project was a prediction model",
    "start": "234280",
    "end": "241330"
  },
  {
    "text": "already or a prototype of a prediction model because that was what the data",
    "start": "241330",
    "end": "246400"
  },
  {
    "text": "scientists at Otto Scott had already built a prototype and the language that",
    "start": "246400",
    "end": "252700"
  },
  {
    "text": "they used to build this was our and the approach the data science approach that",
    "start": "252700",
    "end": "258940"
  },
  {
    "text": "they selected for this problem is called random forest who of you has heard of",
    "start": "258940",
    "end": "265480"
  },
  {
    "text": "random forest before okay quite a few nice they actually changed over the",
    "start": "265480",
    "end": "271390"
  },
  {
    "text": "years so so let's have a look at how this roughly works so in this case we take",
    "start": "271390",
    "end": "282389"
  },
  {
    "text": "the car listings of the last two years for instance from the platform all the",
    "start": "282389",
    "end": "288580"
  },
  {
    "text": "car listings that have been on the platform and then we use this to train a prediction model and what you get out of",
    "start": "288580",
    "end": "295719"
  },
  {
    "text": "this are many of such decision trees and you can see decision tree works like",
    "start": "295719",
    "end": "303460"
  },
  {
    "text": "this so you have a decision here at the very top of the tree for instance",
    "start": "303460",
    "end": "308949"
  },
  {
    "text": "whether the mileage is more than fifty thousand kilometers and then let's say",
    "start": "308949",
    "end": "314169"
  },
  {
    "text": "this is the case then the next decision might be whether the fuel type is diesel and then if this is the case you already",
    "start": "314169",
    "end": "320709"
  },
  {
    "text": "get a price and you can already see that there's a hierarchy in those decisions",
    "start": "320709",
    "end": "327580"
  },
  {
    "text": "so it actually matters which of those decisions come first because the one",
    "start": "327580",
    "end": "332889"
  },
  {
    "text": "most on the top here is the most important one because it's decided before the other ones now random forest",
    "start": "332889",
    "end": "340569"
  },
  {
    "text": "is the algorithm that basically analyzes the data or strength with the data and",
    "start": "340569",
    "end": "346180"
  },
  {
    "text": "thereby decides how to arrange those decisions in this tree and beyond that",
    "start": "346180",
    "end": "353759"
  },
  {
    "text": "random forest is a approach or a technique that works against overfitting",
    "start": "353759",
    "end": "362459"
  },
  {
    "text": "so what is overfitting overfitting means that a prediction model a trained model",
    "start": "362459",
    "end": "369689"
  },
  {
    "text": "all the works really well on pretty much exactly the data it has been trained on",
    "start": "369689",
    "end": "376469"
  },
  {
    "text": "but produces rather bad results on data or input that is different from what it",
    "start": "376469",
    "end": "383199"
  },
  {
    "text": "was trained with and obviously as the main use case here is to predict prices",
    "start": "383199",
    "end": "389919"
  },
  {
    "text": "of cars that we have not seen exactly like this on the platform and this",
    "start": "389919",
    "end": "395619"
  },
  {
    "text": "wouldn't be very good so therefore what random forests does and that's why it's called forest and",
    "start": "395619",
    "end": "402370"
  },
  {
    "text": "Autry is it generates many of those this trees and it generates them kind of",
    "start": "402370",
    "end": "409000"
  },
  {
    "text": "randomly by randomly selecting subsets of the training data so that all those trees are slightly different and then it",
    "start": "409000",
    "end": "416440"
  },
  {
    "text": "basically asks all those decision trees to make a prediction and then more or",
    "start": "416440",
    "end": "422650"
  },
  {
    "text": "less averages of all those results and that's a pretty effective technique against overfitting and this is what we",
    "start": "422650",
    "end": "428670"
  },
  {
    "text": "wanted to have here so up until here the",
    "start": "428670",
    "end": "434470"
  },
  {
    "text": "data scientists were working on their own and coming up with this prediction model prototype of the prediction model",
    "start": "434470",
    "end": "441520"
  },
  {
    "text": "and now our task was to actually bring this into our high performance web application so now we have a lot of",
    "start": "441520",
    "end": "448120"
  },
  {
    "text": "non-functional requirements for this page it needs to be available it needs to be answering in this fast response",
    "start": "448120",
    "end": "456010"
  },
  {
    "text": "times and it needs to be need to be able to operate this without too much",
    "start": "456010",
    "end": "461650"
  },
  {
    "text": "maintenance so at that point in time we looked into whether it's possible to put",
    "start": "461650",
    "end": "469630"
  },
  {
    "text": "our directly into production and this is some years ago and we didn't found a",
    "start": "469630",
    "end": "475780"
  },
  {
    "text": "good solution the open source version of a far it's not multi-threading so it",
    "start": "475780",
    "end": "481210"
  },
  {
    "text": "wouldn't handle the concurrent requests and it was also from memory consumption and other concerns it was not a valid",
    "start": "481210",
    "end": "488560"
  },
  {
    "text": "choice for us to actually put on into production this could have peaked loads",
    "start": "488560",
    "end": "494620"
  },
  {
    "text": "when we when we start a marketing campaign or something like this and we don't want to see our cluster going down",
    "start": "494620",
    "end": "502240"
  },
  {
    "text": "so what is the potential solution a potential solution would be and we have",
    "start": "502240",
    "end": "508960"
  },
  {
    "text": "seen companies doing this take the our model transfer it into a programming",
    "start": "508960",
    "end": "514990"
  },
  {
    "text": "lead language where engineers are capable of putting this into production and that can handle the load in our",
    "start": "514990",
    "end": "521860"
  },
  {
    "text": "example it would also be chava so an engineer would transfer the model from",
    "start": "521860",
    "end": "528070"
  },
  {
    "text": "our into java code and we could then put this in production we have the performance characteristic and all the",
    "start": "528070",
    "end": "534460"
  },
  {
    "text": "engineering practices around a well understood run unlike Java the downside with this is",
    "start": "534460",
    "end": "541520"
  },
  {
    "text": "it's a manual task transforming the thing so that there can be errors in",
    "start": "541520",
    "end": "546800"
  },
  {
    "text": "there which you need to find out and also you might have the problem and we",
    "start": "546800",
    "end": "551900"
  },
  {
    "text": "actually also had the situation that the data scientist might change their model so in our inception for this project",
    "start": "551900",
    "end": "559820"
  },
  {
    "text": "they already mentioned we started out this linear regression then we tried this model currently we are favoring",
    "start": "559820",
    "end": "565850"
  },
  {
    "text": "random forests but we might change this model again if you learn that we can improve on the model so having a manual",
    "start": "565850",
    "end": "573970"
  },
  {
    "text": "transformation of the programming model into a different language would be not a good idea if the model changes yes so",
    "start": "573970",
    "end": "585700"
  },
  {
    "start": "582000",
    "end": "658000"
  },
  {
    "text": "therefore obviously we wanted to automate this right especially in cases",
    "start": "585700",
    "end": "592070"
  },
  {
    "text": "that Christian just described if the model drastically changes and but also",
    "start": "592070",
    "end": "598730"
  },
  {
    "text": "generally when the model is retrained obviously so um therefore we looked how",
    "start": "598730",
    "end": "604010"
  },
  {
    "text": "can we automate this and a technology that helped us quite a bit with this was",
    "start": "604010",
    "end": "609410"
  },
  {
    "text": "h2o one of you has heard of h2o before okay a few",
    "start": "609410",
    "end": "615070"
  },
  {
    "text": "so h2o is a Java based analytics engine that can be programmed using our and",
    "start": "615070",
    "end": "621410"
  },
  {
    "text": "there was something in that case that our data scientist liked and and that",
    "start": "621410",
    "end": "626600"
  },
  {
    "text": "was really the important piece here for us h2o provides the possibility to export a",
    "start": "626600",
    "end": "633950"
  },
  {
    "text": "fully trained model to Java source code interestingly it's not compiled Java",
    "start": "633950",
    "end": "640459"
  },
  {
    "text": "code but Java source code and actually quite a lot of it but we'll talk about",
    "start": "640459",
    "end": "646400"
  },
  {
    "text": "that later of all over this then really",
    "start": "646400",
    "end": "651410"
  },
  {
    "text": "allowed us to integrate this entire model generation into a continuous delivery pipeline so on now let's look",
    "start": "651410",
    "end": "660020"
  },
  {
    "start": "658000",
    "end": "931000"
  },
  {
    "text": "at a very simplistic version of a continuous delivery pipeline to get through the basics we are talking about",
    "start": "660020",
    "end": "666290"
  },
  {
    "text": "here a typical continuous delivery pipeline starts with",
    "start": "666290",
    "end": "671499"
  },
  {
    "text": "repository in our case typically a github repository version control system we have the code for your application",
    "start": "671499",
    "end": "678739"
  },
  {
    "text": "for your service then at the commit stage you take all the source code",
    "start": "678739",
    "end": "684769"
  },
  {
    "text": "compile it if you have a compiled language run your unit test do all the verifications you need to make sure that",
    "start": "684769",
    "end": "691759"
  },
  {
    "text": "you have a proper version of your code and generate an artifact out of it which",
    "start": "691759",
    "end": "697129"
  },
  {
    "text": "you then store in an artifact repository in our case this is as simple as an s3",
    "start": "697129",
    "end": "702259"
  },
  {
    "text": "repository and then comes the second part of the continuous delivery pipeline",
    "start": "702259",
    "end": "707959"
  },
  {
    "text": "and you actually take this just compiled artifact and deploy it on to your",
    "start": "707959",
    "end": "714259"
  },
  {
    "text": "servers hopefully you do this without interruption there are many techniques out there like Bluegreen delivery and",
    "start": "714259",
    "end": "720769"
  },
  {
    "text": "all of those things to actually make sure that your roller the new version without interrupting what what the user",
    "start": "720769",
    "end": "726920"
  },
  {
    "text": "is doing and bring the new version in front of the user why are we doing",
    "start": "726920",
    "end": "732410"
  },
  {
    "text": "continuous delivery like this we are doing this because we want to bring",
    "start": "732410",
    "end": "737749"
  },
  {
    "text": "changes to our application to a code and in this case also to a model in front of",
    "start": "737749",
    "end": "745040"
  },
  {
    "text": "our users to get feedback and to us as often as possible and with less friction",
    "start": "745040",
    "end": "751939"
  },
  {
    "text": "as possible and such a continuous delivery pipeline needs to be repeatable",
    "start": "751939",
    "end": "757579"
  },
  {
    "text": "every time I run through this pipeline the same result should be delivered",
    "start": "757579",
    "end": "762860"
  },
  {
    "text": "meaning this need to be fully automated manual processes will not work there it's also traceable in our case this is",
    "start": "762860",
    "end": "770449"
  },
  {
    "text": "not so important but in general all changes you can trace back to a commit",
    "start": "770449",
    "end": "776449"
  },
  {
    "text": "in your version control system and it should be reliable so the changes you",
    "start": "776449",
    "end": "783619"
  },
  {
    "text": "bring through the pipeline you should have confidence in it that you actually are not breaking have broken services in",
    "start": "783619",
    "end": "791809"
  },
  {
    "text": "production so you need to verify and make sure that this is working properly",
    "start": "791809",
    "end": "797379"
  },
  {
    "text": "from this most idealistic version of our delivery pipeline now to how our",
    "start": "797379",
    "end": "803419"
  },
  {
    "text": "continuous delivery I'm firm model or setup so we have to pay status this is the historical data",
    "start": "803419",
    "end": "810110"
  },
  {
    "text": "of our listings 2 years of listings are stored in there and the our scripts from",
    "start": "810110",
    "end": "816950"
  },
  {
    "text": "the data science make the cleaning of the data train the model using h2o and",
    "start": "816950",
    "end": "824030"
  },
  {
    "text": "then have a proper model as an as a result into the verification on top of",
    "start": "824030",
    "end": "830630"
  },
  {
    "text": "that then we generate the Java code as Arif already mentioned this is exported",
    "start": "830630",
    "end": "837260"
  },
  {
    "text": "out of h2o a big Java classes and many of them they are compiled into a char",
    "start": "837260",
    "end": "845180"
  },
  {
    "text": "and stored on a stream so whenever the data scientists now",
    "start": "845180",
    "end": "850990"
  },
  {
    "text": "change something in the model implementation or we want to just to update the model and train it with newer",
    "start": "850990",
    "end": "857360"
  },
  {
    "text": "base data the whole delivery pipeline is run through and the model is created and",
    "start": "857360",
    "end": "863570"
  },
  {
    "text": "stored on s3 this is what we call our prediction model pipeline in addition we",
    "start": "863570",
    "end": "869480"
  },
  {
    "text": "have the more traditional web server pipeline where we have in our case a",
    "start": "869480",
    "end": "875870"
  },
  {
    "text": "scholar play application which also is in the commit stage built into a web app",
    "start": "875870",
    "end": "883850"
  },
  {
    "text": "char which is then deployed as seen in the continuous delivery pipeline model",
    "start": "883850",
    "end": "889640"
  },
  {
    "text": "on to ec2 instances and the application has started so now and when the",
    "start": "889640",
    "end": "896030"
  },
  {
    "text": "application is started it's also in pulling down the the char from from the",
    "start": "896030",
    "end": "902930"
  },
  {
    "text": "h2o model from s3 and this also do loading this into the classpath so with",
    "start": "902930",
    "end": "908240"
  },
  {
    "text": "that then have a web application that has the model in process and can",
    "start": "908240",
    "end": "914690"
  },
  {
    "text": "directly access the model there and the idea is that whenever the web application changes the delivery",
    "start": "914690",
    "end": "921800"
  },
  {
    "text": "pipeline is run and a new version is deployed or when something in the prediction model pipeline changes this",
    "start": "921800",
    "end": "928070"
  },
  {
    "text": "is also delivered to the instances yes oh yeah so Cristiano said basically",
    "start": "928070",
    "end": "936359"
  },
  {
    "start": "931000",
    "end": "1296000"
  },
  {
    "text": "we want to fully automatically deploy a new prediction model true production",
    "start": "936359",
    "end": "943289"
  },
  {
    "text": "when something changes there when the data sign does improve something or it's",
    "start": "943289",
    "end": "949239"
  },
  {
    "text": "trained when you data so if you really want to do this without any manual intervention you have to have enough",
    "start": "949239",
    "end": "956589"
  },
  {
    "text": "confidence to actually do so so to automatically push it through to production and have users see it",
    "start": "956589",
    "end": "962439"
  },
  {
    "text": "basically and therefore we built an extensive model validation workflow so",
    "start": "962439",
    "end": "970679"
  },
  {
    "text": "yeah let's see and let's start with how a model is usually trained and how the",
    "start": "970679",
    "end": "976899"
  },
  {
    "text": "success of a training is evaluated so let's say you are a data scientist then",
    "start": "976899",
    "end": "982749"
  },
  {
    "text": "what you usually do in the beginning is that you divide your historical data",
    "start": "982749",
    "end": "988209"
  },
  {
    "text": "that you have into training data and test data and importantly those two",
    "start": "988209",
    "end": "994529"
  },
  {
    "text": "datasets need to be disjunct and I'll explain why in a minute",
    "start": "994529",
    "end": "999569"
  },
  {
    "text": "and then yeah typical relation is maybe",
    "start": "999569",
    "end": "1004949"
  },
  {
    "text": "that this is 80% of the data and this is 20% something that yet so then you take the",
    "start": "1004949",
    "end": "1011729"
  },
  {
    "text": "training data use it to train your prediction model and then what you get",
    "start": "1011729",
    "end": "1017249"
  },
  {
    "text": "out in our case here is an a trained h2o prediction model and then we take the",
    "start": "1017249",
    "end": "1026038"
  },
  {
    "text": "test data or specifically we take the input data from the test data for",
    "start": "1026039",
    "end": "1031829"
  },
  {
    "text": "instance the age of the car the mileage model and so on and then ask the fully trained model",
    "start": "1031829",
    "end": "1039980"
  },
  {
    "text": "what price do you predict for that car and then we get those tests estimation",
    "start": "1039980",
    "end": "1046230"
  },
  {
    "text": "results here and now we can compare those tests estimation results or the",
    "start": "1046230",
    "end": "1052320"
  },
  {
    "text": "prices here with the actual prices here in this test data because this is",
    "start": "1052320",
    "end": "1058950"
  },
  {
    "text": "supervised learning so we actually know what the perfect answer is right",
    "start": "1058950",
    "end": "1064720"
  },
  {
    "text": "so these the results here will never be exactly the same that's the perfect",
    "start": "1064720",
    "end": "1070779"
  },
  {
    "text": "results in here but this way we can compare them with each other and thereby",
    "start": "1070779",
    "end": "1076330"
  },
  {
    "text": "get a score of how well the model is trained and now you may see why it's",
    "start": "1076330",
    "end": "1083590"
  },
  {
    "text": "important that those two data sets here are distract because if you would if you",
    "start": "1083590",
    "end": "1089879"
  },
  {
    "text": "trained the or if you tested the model",
    "start": "1089879",
    "end": "1094929"
  },
  {
    "text": "with data you actually used to train it you would get perfect results in here",
    "start": "1094929",
    "end": "1101049"
  },
  {
    "text": "they would be identical to the ones in here but unfortunately this would not",
    "start": "1101049",
    "end": "1106600"
  },
  {
    "text": "tell you anything about the actual behavior of the prediction model to new",
    "start": "1106600",
    "end": "1112090"
  },
  {
    "text": "data so you would have a great score but it would actually not give you any real",
    "start": "1112090",
    "end": "1117309"
  },
  {
    "text": "information about how this thing would act in the world so um yeah therefore those need to be separated and this",
    "start": "1117309",
    "end": "1124960"
  },
  {
    "text": "score in this case that we get here is basically the first quality gate it shows us whether the prediction quality",
    "start": "1124960",
    "end": "1131200"
  },
  {
    "text": "is still good enough so only if this is beyond a certain threshold then we go",
    "start": "1131200",
    "end": "1137110"
  },
  {
    "text": "further and now we also want to check",
    "start": "1137110",
    "end": "1145710"
  },
  {
    "text": "that this train prediction model that we then export first to Java source code",
    "start": "1145710",
    "end": "1153039"
  },
  {
    "text": "then compile and then actually integrate into our web application so loaded into",
    "start": "1153039",
    "end": "1160000"
  },
  {
    "text": "the class loader of the web application and so on we want to make sure that this compiled trained prediction model",
    "start": "1160000",
    "end": "1168070"
  },
  {
    "text": "actually behaves exactly the same as the original h2 all trained model because it",
    "start": "1168070",
    "end": "1178240"
  },
  {
    "text": "doesn't help us much if here we got a good score and see okay that's great and",
    "start": "1178240",
    "end": "1183899"
  },
  {
    "text": "the data scientists think okay that's really good what the model is doing but then things go down go back down the",
    "start": "1183899",
    "end": "1191019"
  },
  {
    "text": "line here and the web application is actually doing something entirely different so that's why we have to check",
    "start": "1191019",
    "end": "1197740"
  },
  {
    "text": "that again and here we use those tests estimation results again and now simply check",
    "start": "1197740",
    "end": "1203349"
  },
  {
    "text": "whether this compiled prediction model gives us exactly the same results as the",
    "start": "1203349",
    "end": "1209559"
  },
  {
    "text": "test estimation results so we know that really nothing has gone wrong down the line and then finally we also make sure",
    "start": "1209559",
    "end": "1220239"
  },
  {
    "text": "that the prediction model fulfills all the requirements that the web",
    "start": "1220239",
    "end": "1226539"
  },
  {
    "text": "application poses at it and this is called consumer driven contract testing so in this case the web application is",
    "start": "1226539",
    "end": "1235629"
  },
  {
    "text": "the consumer and it consumes the prediction model and therefore because",
    "start": "1235629",
    "end": "1241929"
  },
  {
    "text": "it's consumer driven the web application states in a contract what it expects",
    "start": "1241929",
    "end": "1247659"
  },
  {
    "text": "from the prediction model and in this case this is a lot about the interface",
    "start": "1247659",
    "end": "1253389"
  },
  {
    "text": "so it's what are the names of the parameters yeah what data types etc and",
    "start": "1253389",
    "end": "1261429"
  },
  {
    "text": "that was really useful in the beginning because yeah they simply really happened",
    "start": "1261429",
    "end": "1266589"
  },
  {
    "text": "quite often that the interface changed all those things and then things would break and production so this really",
    "start": "1266589",
    "end": "1272289"
  },
  {
    "text": "makes sure that this doesn't happen so only if all those three things are good",
    "start": "1272289",
    "end": "1278589"
  },
  {
    "text": "the general quality of the prediction it's made sure that it's doing the same",
    "start": "1278589",
    "end": "1284440"
  },
  {
    "text": "thing as a compiled prediction model and it works well together with the web application only in this case if all",
    "start": "1284440",
    "end": "1291009"
  },
  {
    "text": "those gates are green then we actually release automatically to production so",
    "start": "1291009",
    "end": "1298299"
  },
  {
    "start": "1296000",
    "end": "1531000"
  },
  {
    "text": "you have seen now what I would like to call data def ops you've seen the data",
    "start": "1298299",
    "end": "1304239"
  },
  {
    "text": "science part where the model was predicted and we had some engineering to",
    "start": "1304239",
    "end": "1309369"
  },
  {
    "text": "actually take this model into a web application in India and we also take",
    "start": "1309369",
    "end": "1314589"
  },
  {
    "text": "care how to put the things into production so what we really learned",
    "start": "1314589",
    "end": "1320259"
  },
  {
    "text": "from this project that in the beginning we actually worked all together this was",
    "start": "1320259",
    "end": "1326339"
  },
  {
    "text": "cross-functional teams data scientists were in the same stage",
    "start": "1326339",
    "end": "1332080"
  },
  {
    "text": "up next to the engineers discussing pipelines discussing model changes and",
    "start": "1332080",
    "end": "1338580"
  },
  {
    "text": "because we also are really touching all of those points and this is what all in",
    "start": "1338580",
    "end": "1344289"
  },
  {
    "text": "the end the idea of a consumer facing product and the feedback cycles around",
    "start": "1344289",
    "end": "1349600"
  },
  {
    "text": "improving this product and the model and the software and the deliverer is around because then those people actually owned",
    "start": "1349600",
    "end": "1356769"
  },
  {
    "text": "a whole pipeline they understand it and therefore can bring changes fast into",
    "start": "1356769",
    "end": "1361870"
  },
  {
    "text": "production and reason about all the parts there are no handovers in between there's no data scientists producing a",
    "start": "1361870",
    "end": "1369429"
  },
  {
    "text": "model an engineer writing an application for it and then an ops guy bring it in",
    "start": "1369429",
    "end": "1374980"
  },
  {
    "text": "production and the obstacle would not know if the predictions are wrong but somebody would call him in the night",
    "start": "1374980",
    "end": "1380620"
  },
  {
    "text": "there is something wrong with the prediction so this is very valuable for us especially then working together with",
    "start": "1380620",
    "end": "1388659"
  },
  {
    "text": "data scientists we are software engineers I have a software engineer background I learned a lot from data",
    "start": "1388659",
    "end": "1395320"
  },
  {
    "text": "scientist so it's fascinating to learn how random forests work and what they are optimizing for and also the quirks",
    "start": "1395320",
    "end": "1403059"
  },
  {
    "text": "of the model how does then actually impact the product that is coming out there and what can be improved from for",
    "start": "1403059",
    "end": "1411070"
  },
  {
    "text": "example what data are we collecting from the user to actually make this prediction and how do we transform it",
    "start": "1411070",
    "end": "1416860"
  },
  {
    "text": "that it's meaningful for a model and for the data scientists they learned a lot",
    "start": "1416860",
    "end": "1423370"
  },
  {
    "text": "about our engineering practices they learned that you actually can do unit",
    "start": "1423370",
    "end": "1428590"
  },
  {
    "text": "testing in RN justice actually makes sense and actually also found errors in",
    "start": "1428590",
    "end": "1433809"
  },
  {
    "text": "our clean up script where we were losing data and some points also as I Reeve already mentioned the",
    "start": "1433809",
    "end": "1441010"
  },
  {
    "text": "notion of stable interfaces so up until that project the data scientists were in",
    "start": "1441010",
    "end": "1446860"
  },
  {
    "text": "exploratory mode they could change their model that parameters everything to their liking",
    "start": "1446860",
    "end": "1452250"
  },
  {
    "text": "but now for our prediction model in interacting with the application we needed to have no stable contracts they",
    "start": "1452250",
    "end": "1459220"
  },
  {
    "text": "couldn't just go ahead and rename their dimensions and features because then",
    "start": "1459220",
    "end": "1464809"
  },
  {
    "text": "the application will break lucky enough we had those tests in place we found out those things but yes this is things data",
    "start": "1464809",
    "end": "1472970"
  },
  {
    "text": "scientists learned also the usage of version control system for four there",
    "start": "1472970",
    "end": "1478129"
  },
  {
    "text": "are scripts and all of those things and what also was very valuable to them is",
    "start": "1478129",
    "end": "1483139"
  },
  {
    "text": "to actually get feedback of the impact of their work so not building up a model",
    "start": "1483139",
    "end": "1488499"
  },
  {
    "text": "handing it over and doing them the next project but actually staying with a project learning and improving on the",
    "start": "1488499",
    "end": "1494779"
  },
  {
    "text": "model and actually seeing how real users use it and how the rest of the of the application is working with the model so",
    "start": "1494779",
    "end": "1504470"
  },
  {
    "text": "in essence working closely together make those fast iterations possible",
    "start": "1504470",
    "end": "1510590"
  },
  {
    "text": "especially at the beginning of the project to be fair in later times when",
    "start": "1510590",
    "end": "1516769"
  },
  {
    "text": "the engineering part was more dominant engineers were not so deeply involved",
    "start": "1516769",
    "end": "1523190"
  },
  {
    "text": "but you need to take care and actually be we need to recognize when Windows",
    "start": "1523190",
    "end": "1528559"
  },
  {
    "text": "close collaboration is actually required all right so um now before going a bit",
    "start": "1528559",
    "end": "1535549"
  },
  {
    "start": "1531000",
    "end": "1791000"
  },
  {
    "text": "into some really technical lessons learned during that project I just",
    "start": "1535549",
    "end": "1541070"
  },
  {
    "text": "simply want to emphasis what I'm sorry what what Christian just said really this if you take one thing home from",
    "start": "1541070",
    "end": "1548799"
  },
  {
    "text": "this talk for me it would really be make the data scientists and engineers",
    "start": "1548799",
    "end": "1554809"
  },
  {
    "text": "software engineers work together so all the technical stuff is minor this is",
    "start": "1554809",
    "end": "1560299"
  },
  {
    "text": "really D take away message so looking a bit into the internal technical",
    "start": "1560299",
    "end": "1565879"
  },
  {
    "text": "challenges that we had many of them had to do with actually generating gigabytes",
    "start": "1565879",
    "end": "1573080"
  },
  {
    "text": "of Java code and that is quite a challenge if not to say an abuse to the",
    "start": "1573080",
    "end": "1578779"
  },
  {
    "text": "JVM I mean you have to imagine those this this generated code from random",
    "start": "1578779",
    "end": "1585350"
  },
  {
    "text": "forests it's basically huge if-else trees and that's it",
    "start": "1585350",
    "end": "1590809"
  },
  {
    "text": "so it's absolutely huge amounts of stupid source code also doesn't look very nice",
    "start": "1590809",
    "end": "1597650"
  },
  {
    "text": "and anyway even compiled it's still really big so one lesson that we learned",
    "start": "1597650",
    "end": "1604580"
  },
  {
    "text": "at that time was that we had to use the back then not default g1 garbage",
    "start": "1604580",
    "end": "1611000"
  },
  {
    "text": "collector by now it's default and this was not a case because the the older",
    "start": "1611000",
    "end": "1616400"
  },
  {
    "text": "garbage collector it would literally take seconds to sweep through all this all this data and obviously if you have",
    "start": "1616400",
    "end": "1624409"
  },
  {
    "text": "a web application running in production yeah you don't want the application to",
    "start": "1624409",
    "end": "1629690"
  },
  {
    "text": "freeze for one or two seconds so this the g1 garbage collector is actually not",
    "start": "1629690",
    "end": "1636500"
  },
  {
    "text": "that much quicker to go through this but you can set a maximum amount of how long",
    "start": "1636500",
    "end": "1643070"
  },
  {
    "text": "a sweep can take and that really helped us a lot because then it may occur more",
    "start": "1643070",
    "end": "1648830"
  },
  {
    "text": "often but we can control how much it effects the performance and",
    "start": "1648830",
    "end": "1654080"
  },
  {
    "text": "responsiveness of the application so that was rather straightforward this one",
    "start": "1654080",
    "end": "1661419"
  },
  {
    "text": "we really learned this the hard way cleared compilation is a pretty smart",
    "start": "1661419",
    "end": "1667700"
  },
  {
    "text": "feature of the JVM and it continuously optimizes the generated machine code",
    "start": "1667700",
    "end": "1675289"
  },
  {
    "text": "basically on usage patterns and so on the problem is if it tries to optimize",
    "start": "1675289",
    "end": "1682510"
  },
  {
    "text": "gigabytes of Java code continuously it literally did not come back out of",
    "start": "1682510",
    "end": "1688970"
  },
  {
    "text": "optimization any more and and we actually only found out about this when",
    "start": "1688970",
    "end": "1694700"
  },
  {
    "text": "we switched from Java 6 to Java 7 because in Java 6 this feature was",
    "start": "1694700",
    "end": "1699980"
  },
  {
    "text": "already there but it was turned off by default in a Java 7 it was turned on by default and we basically thought a while",
    "start": "1699980",
    "end": "1707270"
  },
  {
    "text": "what can go wrong switching from 6 to 7 and nothing worked anymore so I name was",
    "start": "1707270",
    "end": "1713570"
  },
  {
    "text": "really quite tricky to actually find out what a lot of lack it was that ya made",
    "start": "1713570",
    "end": "1719029"
  },
  {
    "text": "everything being on hold there and then the final lesson here was to do",
    "start": "1719029",
    "end": "1727520"
  },
  {
    "text": "extensive warm-ups with the prediction models and it's also because the JVM is rather",
    "start": "1727520",
    "end": "1732830"
  },
  {
    "text": "smart as it only tries to load into memory what is actually being used and",
    "start": "1732830",
    "end": "1739250"
  },
  {
    "text": "it generally makes sense but again if it only loads a huge amount of code into",
    "start": "1739250",
    "end": "1747020"
  },
  {
    "text": "memory at the time first users hitting that particular part of the model then",
    "start": "1747020",
    "end": "1753890"
  },
  {
    "text": "it simply takes too long and the application again is not responsive so yeah we're really kind of engineering",
    "start": "1753890",
    "end": "1761860"
  },
  {
    "text": "quite effective warm-up procedures here in fact we even used the test data that",
    "start": "1761860",
    "end": "1770690"
  },
  {
    "text": "I showed before to really make sure we have a good distribution for those",
    "start": "1770690",
    "end": "1776780"
  },
  {
    "text": "warm-ups in order to really make sure that we get all the all the data into",
    "start": "1776780",
    "end": "1781910"
  },
  {
    "text": "memory because yeah basically those those prediction models they are programmed code and data in one write so",
    "start": "1781910",
    "end": "1793280"
  },
  {
    "start": "1791000",
    "end": "1945000"
  },
  {
    "text": "this is an example of for warmup time this is the CPU utilization of an",
    "start": "1793280",
    "end": "1799490"
  },
  {
    "text": "instant that's just started and as a reef mentioned the warm-up phase kicks",
    "start": "1799490",
    "end": "1804830"
  },
  {
    "text": "in and this is quite sometimes it takes so this is over an hour takes to warm up",
    "start": "1804830",
    "end": "1810890"
  },
  {
    "text": "just easy to instance on AWS and it's not a small instance we are using there",
    "start": "1810890",
    "end": "1816940"
  },
  {
    "text": "but you also see that after the warm up the CPU utilization and the normal load",
    "start": "1816940",
    "end": "1824360"
  },
  {
    "text": "is then quite good so this is then not a heavily used model we have additional",
    "start": "1824360",
    "end": "1830690"
  },
  {
    "text": "use cases that I mentioned soon we're actually distanced and feels like being",
    "start": "1830690",
    "end": "1836150"
  },
  {
    "text": "warm up again the warmup time in itself is something we could live this but it's still one of the most problematic parts",
    "start": "1836150",
    "end": "1843020"
  },
  {
    "text": "now in this project as we were talking about fast feedback and continuous",
    "start": "1843020",
    "end": "1848420"
  },
  {
    "text": "delivery and now everything is fast and at the end of the journey you have to",
    "start": "1848420",
    "end": "1853580"
  },
  {
    "text": "wait for an hour for instance to come up and additionally as this should be a",
    "start": "1853580",
    "end": "1858980"
  },
  {
    "text": "highly available application in case of a disaster if",
    "start": "1858980",
    "end": "1864950"
  },
  {
    "text": "of those instances goes down or god forbid two of those instances go down to do some configuration error the time to",
    "start": "1864950",
    "end": "1872240"
  },
  {
    "text": "recovery is quite high so we've changed the project a little bit the first time we had an incident we had the instance",
    "start": "1872240",
    "end": "1879230"
  },
  {
    "text": "up and running again hooray but then we had to wait for an hour before the instance was wound up so",
    "start": "1879230",
    "end": "1884539"
  },
  {
    "text": "there are some lessons to learn and of course of all the advantages we already have talked about this is the big",
    "start": "1884539",
    "end": "1891769"
  },
  {
    "text": "disadvantage we have in there in addition still having the chavvy mm",
    "start": "1891769",
    "end": "1898190"
  },
  {
    "text": "production is a very good thing if the cherry M is warm she's very very fast and if you would",
    "start": "1898190",
    "end": "1905269"
  },
  {
    "text": "have are in production one of the later use cases would not would not have worked we are now one of all I will talk",
    "start": "1905269",
    "end": "1913760"
  },
  {
    "text": "about this later but currently we are also not using this app for the standalone web application",
    "start": "1913760",
    "end": "1919519"
  },
  {
    "text": "but we are also estimating the prices for all the listings we have in stock",
    "start": "1919519",
    "end": "1924980"
  },
  {
    "text": "and we also have this needs to be repossessed for xena quite aggressive SLA I believe half an hour so putting",
    "start": "1924980",
    "end": "1933309"
  },
  {
    "text": "2.4 million listings against those models in in half an hour and something that JVM can handle and we have not",
    "start": "1933309",
    "end": "1941120"
  },
  {
    "text": "tried this Bazaar but I believe this would not be such an easy thing to do yeah so basically when the model is was",
    "start": "1941120",
    "end": "1952190"
  },
  {
    "start": "1945000",
    "end": "2158000"
  },
  {
    "text": "warmed up we were really seeing response times for a prediction of low",
    "start": "1952190",
    "end": "1958700"
  },
  {
    "text": "single-digit milliseconds so really like two three milliseconds to get a prediction and yeah I mean the warm-up",
    "start": "1958700",
    "end": "1965870"
  },
  {
    "text": "definitely has its drawbacks but this is really an incredible performance then I mean it's kind of makes sense because",
    "start": "1965870",
    "end": "1972350"
  },
  {
    "text": "the yeah JVM only needs to go through those if-else statements in bytecode",
    "start": "1972350",
    "end": "1979130"
  },
  {
    "text": "more or less so it's really quick so",
    "start": "1979130",
    "end": "1984610"
  },
  {
    "text": "then beyond those technical lessons I also generally learned then after this",
    "start": "1984610",
    "end": "1993409"
  },
  {
    "text": "project when I switched to the to emo billion Scouty and berlin other",
    "start": "1993409",
    "end": "1999670"
  },
  {
    "text": "Scott's sister company and was doing other kind of data science in production",
    "start": "1999670",
    "end": "2006240"
  },
  {
    "text": "projects that this approach of applying",
    "start": "2006240",
    "end": "2011250"
  },
  {
    "text": "continuous delivery principles to data science projects is really really useful",
    "start": "2011250",
    "end": "2016400"
  },
  {
    "text": "independently of the specific technology that you use so in this case we were",
    "start": "2016400",
    "end": "2023550"
  },
  {
    "text": "using an entirely different text deck it was done using peyten and spark and we",
    "start": "2023550",
    "end": "2031070"
  },
  {
    "text": "saw similar advantages of creating such delivery pipeline with quality gates and",
    "start": "2031070",
    "end": "2038520"
  },
  {
    "text": "so on and was even more useful in this case it was also entirely different data science approach it was about",
    "start": "2038520",
    "end": "2044250"
  },
  {
    "text": "recommendations so in this case model evolution was much quicker if we look at",
    "start": "2044250",
    "end": "2051510"
  },
  {
    "text": "this price prediction use case we were",
    "start": "2051510",
    "end": "2056520"
  },
  {
    "text": "looking at basically a sliding window of 24 months so yeah if you update this",
    "start": "2056520",
    "end": "2064679"
  },
  {
    "text": "sliding window of 24 months with a couple of days the prediction will not change dramatically right but in this",
    "start": "2064679",
    "end": "2071940"
  },
  {
    "text": "case that we were doing at the mobian scout we were actually using user interaction data as input for the",
    "start": "2071940",
    "end": "2079230"
  },
  {
    "text": "prediction model and this changes much more quickly and here yeah it simply",
    "start": "2079230",
    "end": "2084480"
  },
  {
    "text": "doesn't work without having everything autumn eyes of retraining the model and adapting it so this was what's really",
    "start": "2084480",
    "end": "2092550"
  },
  {
    "text": "useful here as well interestingly not only the advanced which is very similar",
    "start": "2092550",
    "end": "2098820"
  },
  {
    "text": "but also the challenges were also similar so I'm quite often after his talk I get asked like why didn't you",
    "start": "2098820",
    "end": "2106410"
  },
  {
    "text": "simply do this in Titan and and I mean it also worked quite well this way but",
    "start": "2106410",
    "end": "2114750"
  },
  {
    "text": "it was very similar we had similar challenges there also when we got for advanced modeling techniques and we're",
    "start": "2114750",
    "end": "2121710"
  },
  {
    "text": "loading a lot of data into the prediction models the models got big then we need to make sure those models",
    "start": "2121710",
    "end": "2129600"
  },
  {
    "text": "are loaded into memory to get acceptable performance and indeed we did not have",
    "start": "2129600",
    "end": "2134870"
  },
  {
    "text": "those really JVM specific issues where we have basically to fight against optimization features of the JVM but",
    "start": "2134870",
    "end": "2142940"
  },
  {
    "text": "then on the other hand we also did not get or never came to the point that we had debt superb performance of the JVM",
    "start": "2142940",
    "end": "2149060"
  },
  {
    "text": "so it was actually be a trade-off which you can obviously consider what makes sense in which situation and yeah it's",
    "start": "2149060",
    "end": "2161270"
  },
  {
    "start": "2158000",
    "end": "2211000"
  },
  {
    "text": "why we come here again to audit to the general conclusions continuous delivery",
    "start": "2161270",
    "end": "2166550"
  },
  {
    "text": "really allows us to bring prediction model changes life very quickly and this",
    "start": "2166550",
    "end": "2173120"
  },
  {
    "text": "is only possible with extensive and automated end-to-end tests that really",
    "start": "2173120",
    "end": "2180230"
  },
  {
    "text": "provide you with confidence to deploy for to production automatically yeah especially what I show you before that",
    "start": "2180230",
    "end": "2187100"
  },
  {
    "text": "you also then again have to test whether the compiled model behaves the same way as to one before those things and then",
    "start": "2187100",
    "end": "2195590"
  },
  {
    "text": "this specific technical approach of Java code generation has its drawbacks but it",
    "start": "2195590",
    "end": "2202670"
  },
  {
    "text": "also allowed us for very low response times and in general excellent scalability then because of those",
    "start": "2202670",
    "end": "2208940"
  },
  {
    "text": "response times and actually by preparing",
    "start": "2208940",
    "end": "2215030"
  },
  {
    "text": "this talk I looked up at the h2 old page and then now have more or less",
    "start": "2215030",
    "end": "2222130"
  },
  {
    "text": "paraphrasing what we are told telling you here that there is a problem with the big models and they have an",
    "start": "2222130",
    "end": "2228650"
  },
  {
    "text": "additional approach now where they're not in addition to exporting at this",
    "start": "2228650",
    "end": "2234020"
  },
  {
    "text": "Java code they're also now allowing it to export a model and a Java code so",
    "start": "2234020",
    "end": "2240170"
  },
  {
    "text": "that they're actually using the model data in the java code so it's you you",
    "start": "2240170",
    "end": "2245450"
  },
  {
    "text": "still have the same general approach but with less of the drawbacks we haven't",
    "start": "2245450",
    "end": "2250670"
  },
  {
    "text": "not tried this in production but this could be something we try out so we are",
    "start": "2250670",
    "end": "2255980"
  },
  {
    "text": "not the only one who are suffering from big Java like sizes but now let's look",
    "start": "2255980",
    "end": "2262370"
  },
  {
    "text": "at what afterwards came so we talked about the standalone application people",
    "start": "2262370",
    "end": "2267950"
  },
  {
    "text": "entering their data now we are also using the this model to predict the",
    "start": "2267950",
    "end": "2273349"
  },
  {
    "text": "price when users are creating your listing on our site to actually give him",
    "start": "2273349",
    "end": "2278569"
  },
  {
    "text": "an indication with this data but our model says is a good price for the for",
    "start": "2278569",
    "end": "2285319"
  },
  {
    "text": "the car to sell and we also and this I already mentioned that we are now",
    "start": "2285319",
    "end": "2290650"
  },
  {
    "text": "estimating the prices for all the listings we have on stock and put them into categories so is this as arive",
    "start": "2290650",
    "end": "2298819"
  },
  {
    "text": "already mentioned there is a price range coming out of there that the model prediction and then can say is the price",
    "start": "2298819",
    "end": "2305240"
  },
  {
    "text": "of the current car in comparison to the other comparable listings is just a top",
    "start": "2305240",
    "end": "2311210"
  },
  {
    "text": "price is a good price is a fair price is it a bad price so we can put categories",
    "start": "2311210",
    "end": "2317240"
  },
  {
    "text": "on top of it so now I believe 90% of the listings on our site have just price",
    "start": "2317240",
    "end": "2323359"
  },
  {
    "text": "labels and then the price estimation attached to them when you search for them these are the categories where the",
    "start": "2323359",
    "end": "2330770"
  },
  {
    "text": "listing is then entered into and this is now also usable as a criteria for for",
    "start": "2330770",
    "end": "2337339"
  },
  {
    "text": "searching so that you can only search for top prices or a good prices on on",
    "start": "2337339",
    "end": "2342680"
  },
  {
    "text": "your listings which extends on the usage of of this model and as already",
    "start": "2342680",
    "end": "2347690"
  },
  {
    "text": "mentioned this now is when re indexing or reevaluating all the all the listings",
    "start": "2347690",
    "end": "2354220"
  },
  {
    "text": "puts an additional burden on the model services to actually answer in a",
    "start": "2354220",
    "end": "2359420"
  },
  {
    "text": "meaningful time so this is the end of our presentations thank you very much",
    "start": "2359420",
    "end": "2366880"
  },
  {
    "start": "2362000",
    "end": "2498000"
  },
  {
    "text": "[Applause]",
    "start": "2366880",
    "end": "2373850"
  },
  {
    "text": "please remember to rate the sessions and now we are here for questions I assume I",
    "start": "2373850",
    "end": "2380190"
  },
  {
    "text": "guess yes first of all there's a comment from the audience here one saying this",
    "start": "2380190",
    "end": "2385500"
  },
  {
    "text": "is finally a real case finally it's",
    "start": "2385500",
    "end": "2393930"
  },
  {
    "text": "interesting to see the theory meeting the limitations of the real world here quite a lot of the questions here attack",
    "start": "2393930",
    "end": "2402990"
  },
  {
    "text": "the size of the generated code asking",
    "start": "2402990",
    "end": "2408810"
  },
  {
    "text": "questions like that you try to optimize it did you look for duplicated code so",
    "start": "2408810",
    "end": "2414540"
  },
  {
    "text": "in this case this as a Reeve already mentioned the initial version using your random forest directly this is a lot of",
    "start": "2414540",
    "end": "2421320"
  },
  {
    "text": "if-else statements with if mileage is like this if this so of course you can",
    "start": "2421320",
    "end": "2427940"
  },
  {
    "text": "reduce the code size by changing the model parameters how deep is the tree",
    "start": "2427940",
    "end": "2433740"
  },
  {
    "text": "how many of those trees are in the in the in the random forest the biggest improvement which we did not mentioned",
    "start": "2433740",
    "end": "2439830"
  },
  {
    "text": "in the targets that we actually switch the model once again and we are now using radium boosted machines which are",
    "start": "2439830",
    "end": "2446720"
  },
  {
    "text": "solving some of the problems so the code size gets smaller nevertheless this was",
    "start": "2446720",
    "end": "2452490"
  },
  {
    "text": "necessary because the model size grew to a point where we actually for certain",
    "start": "2452490",
    "end": "2459000"
  },
  {
    "text": "make model combinations were not longer able to actually export it out of h2o because they hit the 1 gigabyte limit so",
    "start": "2459000",
    "end": "2467490"
  },
  {
    "text": "we need to switch the model still by increasing the number of features in",
    "start": "2467490",
    "end": "2473490"
  },
  {
    "text": "there that they're still quite big but there of course that there are trade-offs you you can do there if if if",
    "start": "2473490",
    "end": "2479280"
  },
  {
    "text": "this would go to two or three hours warm-up time I believe we would just adjust the model accordingly",
    "start": "2479280",
    "end": "2484410"
  },
  {
    "text": "but typically refactorings or something like that is not doable because it's",
    "start": "2484410",
    "end": "2489720"
  },
  {
    "text": "just the model as code and the sets as big as the trees and of and in the tree",
    "start": "2489720",
    "end": "2496440"
  },
  {
    "text": "sizes there I think with those optimizations was possible to basically bring down the most",
    "start": "2496440",
    "end": "2502090"
  },
  {
    "text": "to about the tenth of the original size but it's more or less in the same",
    "start": "2502090",
    "end": "2507220"
  },
  {
    "text": "ballpark right it's not a thousandth of the of the original model because as",
    "start": "2507220",
    "end": "2512680"
  },
  {
    "text": "Christiane said it contains the data that's the thing you're yeah kind of putting data into compiled code so it's",
    "start": "2512680",
    "end": "2519580"
  },
  {
    "text": "pretty much like drowning at 100 meters or centimeters yeah you explicitly check",
    "start": "2519580",
    "end": "2530260"
  },
  {
    "text": "the compiled model a to comply compiled model for correctness do you have a",
    "start": "2530260",
    "end": "2535540"
  },
  {
    "text": "reason to believe that this is necessary why wouldn't it be correct I mean so if",
    "start": "2535540",
    "end": "2543430"
  },
  {
    "text": "we changed everything breaks at some point in time and this is one of those",
    "start": "2543430",
    "end": "2550090"
  },
  {
    "text": "things you're transferring from technology a to technology B and that at",
    "start": "2550090",
    "end": "2555520"
  },
  {
    "text": "some point in time things go wrong and it's not only about that the model itself might break but it's also how we",
    "start": "2555520",
    "end": "2561730"
  },
  {
    "text": "interface with the model so that coming out of there is a predict function in",
    "start": "2561730",
    "end": "2566800"
  },
  {
    "text": "Java with a lot of parameters and if we just mix up the parameters we pass in",
    "start": "2566800",
    "end": "2573520"
  },
  {
    "text": "their due term some refactoring or something else then this would also break so it's not only about is h2o bug",
    "start": "2573520",
    "end": "2580360"
  },
  {
    "text": "free which I believe the question is hinting at so I'll be verifying that h2o is a good job but also the compiled",
    "start": "2580360",
    "end": "2586930"
  },
  {
    "text": "model and how we interface with this model needs to be verified so we have not found a back in h2o to be honest",
    "start": "2586930",
    "end": "2593860"
  },
  {
    "text": "and obviously this actually happened that we mixed up the numb the position",
    "start": "2593860",
    "end": "2600760"
  },
  {
    "text": "of the parameters and then things were wrong and yeah so we actually also needed this this kind of validation at",
    "start": "2600760",
    "end": "2608110"
  },
  {
    "text": "the integration into our application is correct at the end of the day this is about selling cars and giving the right",
    "start": "2608110",
    "end": "2615250"
  },
  {
    "text": "price so do you have any feedback from the receivers from these prices did they",
    "start": "2615250",
    "end": "2621550"
  },
  {
    "text": "increase the sales or did they did the customers accept the prices or there is",
    "start": "2621550",
    "end": "2628810"
  },
  {
    "text": "a lot of learning involved so I will not go into many of those details because",
    "start": "2628810",
    "end": "2634060"
  },
  {
    "text": "this is not something on camera I will talk about what all please turn off the camera what we",
    "start": "2634060",
    "end": "2641950"
  },
  {
    "text": "learned No okay up there but it's very very interesting if you put something",
    "start": "2641950",
    "end": "2648280"
  },
  {
    "text": "like this in front of a user who also believes he knows the so that the users",
    "start": "2648280",
    "end": "2654670"
  },
  {
    "text": "who believe they know something about cars and the particle prices and users that don't know about car prices and how",
    "start": "2654670",
    "end": "2660550"
  },
  {
    "text": "do they react and so first of all the initial version they believe this isn't",
    "start": "2660550",
    "end": "2665830"
  },
  {
    "text": "bought advertising so because of just a top price there they don't believe that this is actually data science trying to",
    "start": "2665830",
    "end": "2673420"
  },
  {
    "text": "predict the price but they just believe this is like Google Ads or something like this this is why we'd in later also",
    "start": "2673420",
    "end": "2679420"
  },
  {
    "text": "introduced that they're not so good categories which increase the trust level because then users actually see a",
    "start": "2679420",
    "end": "2685900"
  },
  {
    "text": "there are also labeling bad prices therefore this seems to be more",
    "start": "2685900",
    "end": "2691060"
  },
  {
    "text": "trustworthy and over time the trust has increased from from from the user their",
    "start": "2691060",
    "end": "2696370"
  },
  {
    "text": "desire using this it must be a very dynamic world I mean it's not just only",
    "start": "2696370",
    "end": "2702060"
  },
  {
    "text": "Volkswagen it's every model every every type of car there must be new new models",
    "start": "2702060",
    "end": "2710110"
  },
  {
    "text": "hitting the market all sorts of things affecting the prices so it must be a",
    "start": "2710110",
    "end": "2716530"
  },
  {
    "text": "very dynamic type yes for example now we have the diesel gate and thus the",
    "start": "2716530",
    "end": "2722740"
  },
  {
    "text": "digital gate actually influenced the model and so yes and no the thing where",
    "start": "2722740",
    "end": "2730030"
  },
  {
    "text": "you can react and make the model more dynamic or less dynamic is how much of",
    "start": "2730030",
    "end": "2735070"
  },
  {
    "text": "historical data you are using so if you using a lot of historic later the model does not change as fast because it's",
    "start": "2735070",
    "end": "2741700"
  },
  {
    "text": "also taking into account previous data and if use not you're not going some way",
    "start": "2741700",
    "end": "2749320"
  },
  {
    "text": "back in history then the model becomes more dynamic so you have actually have an influence in how the model reacts to",
    "start": "2749320",
    "end": "2756730"
  },
  {
    "text": "print price changes that are happening in reality but of course we are lagging so because we are not using the current",
    "start": "2756730",
    "end": "2762940"
  },
  {
    "text": "stock but we are also using historical data we are lagging behind if something changes for",
    "start": "2762940",
    "end": "2768230"
  },
  {
    "text": "sample like in dieselgate exactly and we just learned that the results do not get",
    "start": "2768230",
    "end": "2773690"
  },
  {
    "text": "better than the assumptions yes dictate so of course and did you ever consider",
    "start": "2773690",
    "end": "2779840"
  },
  {
    "text": "to use some feedback mechanisms or to apply deep learning did you try that",
    "start": "2779840",
    "end": "2785710"
  },
  {
    "text": "before you chose your strategy here so from from my data's I just the last week",
    "start": "2785710",
    "end": "2793970"
  },
  {
    "text": "to to the data scientist who's currently involved with the project and in the next iteration of course we will try out",
    "start": "2793970",
    "end": "2800600"
  },
  {
    "text": "new models so they are looking into all kinds of models to actually change the",
    "start": "2800600",
    "end": "2806930"
  },
  {
    "text": "crucian and the behavior of the model but to be very honest the the total last",
    "start": "2806930",
    "end": "2814250"
  },
  {
    "text": "of accuracy is not so much of a problem of the model problem but also of a data",
    "start": "2814250",
    "end": "2819470"
  },
  {
    "text": "quality problem so if you have all the parameters for a car and you don't know",
    "start": "2819470",
    "end": "2825530"
  },
  {
    "text": "exactly the condition the car is in because the one who's putting listing online did not fill out the form",
    "start": "2825530",
    "end": "2831530"
  },
  {
    "text": "properly or even we didn't get gave him the opportunity to properly formulate the condition of the car or is not",
    "start": "2831530",
    "end": "2838640"
  },
  {
    "text": "giving you all the parameters you can't make good prediction so it's it's a",
    "start": "2838640",
    "end": "2843800"
  },
  {
    "text": "trade of you it's a good indication for users how how does behaves but it will not be exact truth and and you can't go",
    "start": "2843800",
    "end": "2850850"
  },
  {
    "text": "the last mile that to actually say justice this is a an authoritative price you're giving there this out seeing the",
    "start": "2850850",
    "end": "2856070"
  },
  {
    "text": "car which is also some some of the critiques we are getting there how can you predict the price of a car without",
    "start": "2856070",
    "end": "2862190"
  },
  {
    "text": "actually seeing the current seeing its condition but yes it's artificial",
    "start": "2862190",
    "end": "2868040"
  },
  {
    "text": "machine learning and not real-life inspection thank you so much",
    "start": "2868040",
    "end": "2875710"
  },
  {
    "text": "[Applause]",
    "start": "2875820",
    "end": "2881340"
  }
]