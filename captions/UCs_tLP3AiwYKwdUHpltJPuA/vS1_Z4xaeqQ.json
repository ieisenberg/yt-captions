[
  {
    "start": "0",
    "end": "44000"
  },
  {
    "text": "Hi, everybody. This is Bruce Tate for GOTO, and \nI'm with Groxio Learning, but I have a special   guest for you today. I've got Sean Moriarity, \nand he's the creator of the Axon project and  ",
    "start": "4080",
    "end": "15059"
  },
  {
    "text": "the co-creator of the Nx project as well. This \nis all things machine learning in Elixir. You  ",
    "start": "15060",
    "end": "21000"
  },
  {
    "text": "wanna tell the audience about yourself, Sean?\nSure. So, my name is Sean Moriarity. Originally  ",
    "start": "21000",
    "end": "26940"
  },
  {
    "text": "from Philadelphia, Pennsylvania. I am the author \nof \"Genetic Algorithms in Elixir,\" and I also have  ",
    "start": "26940",
    "end": "33360"
  },
  {
    "text": "a new publication out called \"Machine Learning in \nElixir.\" Worked in the machine learning ecosystem  ",
    "start": "33360",
    "end": "38400"
  },
  {
    "text": "in Elixir for quite some time now, and I'm \nexcited to talk a little bit about my new project. ",
    "start": "38400",
    "end": "42420"
  },
  {
    "start": "44000",
    "end": "247000"
  },
  {
    "text": "This is an interesting story because, as many of \nyou might know, Elixir has not always been known  ",
    "start": "44340",
    "end": "50940"
  },
  {
    "text": "as a great language for machine learning. \nI think that you might have knocked José,  ",
    "start": "50940",
    "end": "56699"
  },
  {
    "text": "the creator of Elixir, out of his chair \nwhen you wrote the first book about   genetic algorithms and machine learning. \nCould you tell us a little bit about that? ",
    "start": "56700",
    "end": "64680"
  },
  {
    "text": "I was interested in genetic algorithms \nand evolutionary algorithms in college,  ",
    "start": "66840",
    "end": "72359"
  },
  {
    "text": "mainly for applications in, like, \nsports betting in particular, because  ",
    "start": "72360",
    "end": "76740"
  },
  {
    "text": "sports betting is very similar to, like, financial \ntheory. Investing. So, portfolio theory. There are  ",
    "start": "77820",
    "end": "84600"
  },
  {
    "text": "a lot of tie-ins with using genetic algorithms and \nevolutionary algorithms for optimizing a portfolio  ",
    "start": "84600",
    "end": "90360"
  },
  {
    "text": "for risk and reward. I was interested in genetic \nalgorithms, and evolutionary algorithms, and I was  ",
    "start": "90360",
    "end": "97200"
  },
  {
    "text": "also really interested in Elixir, but Elixir was \nnot a good language for doing any of this stuff. ",
    "start": "97200",
    "end": "102240"
  },
  {
    "text": "The BEAM, the virtual machine that Elixir runs \non, the Erlang virtual machine, is not good  ",
    "start": "102240",
    "end": "108960"
  },
  {
    "text": "for numerical computations. It's well-suited \nfor, concurrency, and building fault-tolerant  ",
    "start": "108960",
    "end": "114300"
  },
  {
    "text": "applications, but numerical applications were \nnot something that it was very good for. But I   decided I was just gonna write genetic algorithms \nin Elixir anyway. And so, I created a project for  ",
    "start": "114300",
    "end": "124740"
  },
  {
    "text": "creating genetic algorithms and solving some toy \nproblems, basically, with genetic algorithms. I  ",
    "start": "124740",
    "end": "130200"
  },
  {
    "text": "thought it was something that other people might \nalso be interested in, so I kind of threw a Hail   Mary over to the Pragmatic Bookshelf, who's \nthe publisher behind the Pragmatic Programmer,  ",
    "start": "130200",
    "end": "140760"
  },
  {
    "text": "about a book pitch, basically, \nfor genetic algorithms in Elixir,   and miraculously, they ended up accepting it.\nThe book came out in, I think, February of 2020,  ",
    "start": "141720",
    "end": "152160"
  },
  {
    "text": "or maybe it might've been October of 2020. And \nafter that, I got in touch with José Valim,  ",
    "start": "153120",
    "end": "160140"
  },
  {
    "text": "who's the creator of Elixir, and he was like, \n\"Hey, so, I thought this was pretty interesting.   Do you wanna start working on machine learning \nprojects in Elixir?\" And I thought that would be  ",
    "start": "160140",
    "end": "168120"
  },
  {
    "text": "pretty awesome. So we started, and kicked off that \nproject around the same time, in October 2020. And   three years on now, we've got everything \nfrom deep learning to traditional machine  ",
    "start": "168120",
    "end": "178019"
  },
  {
    "text": "learning. You can use pre-trained transformers \non a lot of pre-trained models, just directly  ",
    "start": "178020",
    "end": "183720"
  },
  {
    "text": "from Elixir itself. And our performance is \npretty competitive with the Python ecosystem.  ",
    "start": "183720",
    "end": "188940"
  },
  {
    "text": "And we have some pretty good abstractions for \ndeploying machine-learning models as well.  I wanna slow you down a little bit because we've \ngone over some pretty interesting things there.  ",
    "start": "188940",
    "end": "197820"
  },
  {
    "text": "So, I've gotta tell you, I've gotta make an \nadmission for the first time on this podcast.  ",
    "start": "197820",
    "end": "204000"
  },
  {
    "text": "I saw that initial book proposal come through, \nand I said, \"This is the wrong thing for Elixir.\"  ",
    "start": "204000",
    "end": "212280"
  },
  {
    "text": "I was the editor, or kind of the line editor of \nthe Elixir line of books at the time. And I said,  ",
    "start": "212280",
    "end": "218880"
  },
  {
    "text": "\"This is really interesting stuff, but this is \nmore interesting from an academic perspective.\" So  ",
    "start": "218880",
    "end": "225060"
  },
  {
    "text": "I said, \"You know, I don't wanna kill this, but I \nalso don't wanna kind of give people false hope,\"  ",
    "start": "225060",
    "end": "233640"
  },
  {
    "text": "right? So, what I did is I said, \"Okay, this is \ninteresting academically, so does anybody else  ",
    "start": "233640",
    "end": "240780"
  },
  {
    "text": "wanna take a shot at this?\" And another editor \nstepped up and, you know, the rest is history. But one interesting thing is that \nthere's a pretty blinding moment in time,  ",
    "start": "240780",
    "end": "253680"
  },
  {
    "start": "247000",
    "end": "632000"
  },
  {
    "text": "that just went very quickly, where Elixir was not \na good language for these kinds of applications,  ",
    "start": "253680",
    "end": "260160"
  },
  {
    "text": "to the time that Elixir became that. And \nthat's what Nx is about. Could you tell  ",
    "start": "260160",
    "end": "265560"
  },
  {
    "text": "us a little bit about what Nx does, before we \nget into Axon and the machine learning stack? ",
    "start": "265560",
    "end": "270960"
  },
  {
    "text": "Nx stands for Numerical Elixir, which is \nessentially the foundation for the Elixir machine  ",
    "start": "272520",
    "end": "277860"
  },
  {
    "text": "learning ecosystem. The Nx project started... \nSo, early on, we were trying to decide on how  ",
    "start": "277860",
    "end": "285719"
  },
  {
    "text": "we were gonna design the libraries. Elixir and \nErlang, have this concept of native-implemented  ",
    "start": "285720",
    "end": "291960"
  },
  {
    "text": "functions. They're essentially just a way you can \nwrite C bindings to some native library, and then  ",
    "start": "291960",
    "end": "297539"
  },
  {
    "text": "get it to work with the Erlang virtual machine. \nOne of the paths we could have taken was saying,  ",
    "start": "297540",
    "end": "303240"
  },
  {
    "text": "\"Okay, well these libraries, like TensorFlow and \nPyTorch, offer C and C++ libraries. We can just  ",
    "start": "303240",
    "end": "309840"
  },
  {
    "text": "essentially build directly on top of that.\" And \nit would've been a pretty quick win. We could've   gotten something up and running very quickly.\nThe problem with marrying yourself to another  ",
    "start": "309840",
    "end": "319140"
  },
  {
    "text": "ecosystem is you are essentially blocked anytime \nthey have an issue. And as a smaller consumer of  ",
    "start": "319140",
    "end": "327780"
  },
  {
    "text": "their native libraries, you might not necessarily \nhave the biggest influence over the bugs they're   gonna fix and the things they're gonna \nprioritize. So we made a very intentional  ",
    "start": "327780",
    "end": "336720"
  },
  {
    "text": "decision to not build the library dependent on \nother ecosystems upstream like that. So, Nx,  ",
    "start": "336720",
    "end": "344100"
  },
  {
    "text": "for those that are familiar with the Python \necosystem, is very, very similar to NumPy,  ",
    "start": "344100",
    "end": "349680"
  },
  {
    "text": "but Nx has the additional abstraction of, like, \npluggable compilers and backends, which means  ",
    "start": "349680",
    "end": "357419"
  },
  {
    "text": "that Nx itself just implements a behavior which \nis, I guess, similar to, like, an abstract class,  ",
    "start": "357420",
    "end": "363660"
  },
  {
    "text": "for those coming from object-oriented programming. \nEssentially, it's just a contract for people to  ",
    "start": "363660",
    "end": "369480"
  },
  {
    "text": "implement their implementations of some of \nthe numerical routines that we have in Nx. ",
    "start": "369480",
    "end": "374640"
  },
  {
    "text": "For example, Nx has something like Nx.cosine, \nor Nx.cos in this case, and library backend and  ",
    "start": "374640",
    "end": "382380"
  },
  {
    "text": "compiler implementers can implement their \nversions of cosine for targeted hardware,  ",
    "start": "382380",
    "end": "387660"
  },
  {
    "text": "or specialized routines that are accelerated in \nsome way. The first compiler we implemented was  ",
    "start": "387660",
    "end": "396240"
  },
  {
    "text": "XLA, which is Google's accelerated linear algebra. \nIt's a machine learning compiler for taking these  ",
    "start": "396240",
    "end": "403199"
  },
  {
    "text": "numerical programs and compiling them to the CPU, \nthe GPU, and Google's TPUs, these accelerators.  ",
    "start": "403200",
    "end": "409440"
  },
  {
    "text": "So, Nx serves as the foundation for our \nentire ecosystem. It also implements  ",
    "start": "410820",
    "end": "416760"
  },
  {
    "text": "automatic differentiation, which is important for \nimplementing some of the optimization routines  ",
    "start": "416760",
    "end": "422040"
  },
  {
    "text": "used in Axon, which is a deep-learning library, \nbut it sounds like we're gonna get to that a   little later. So, that's pretty much it about Nx.\nI love the idea that since you're snapping out  ",
    "start": "422040",
    "end": "433620"
  },
  {
    "text": "this whole numerical model, and then, kind \nof, the whole way that you think about storing  ",
    "start": "433620",
    "end": "439919"
  },
  {
    "text": "tensors, right? These are concepts that are just \nkind of ripped out of the language and snapped in.  ",
    "start": "440520",
    "end": "447720"
  },
  {
    "text": "And to make that surfaceable, you did something \nfairly brilliant in there, and that's taking  ",
    "start": "447720",
    "end": "453420"
  },
  {
    "text": "a traditional function definition, and then \noffering an alternative implementation.  ",
    "start": "454980",
    "end": "460200"
  },
  {
    "text": "Can you tell us just a little bit about defn?\nNx introduces this idea of numerical definitions.  ",
    "start": "460200",
    "end": "468420"
  },
  {
    "text": "So, in Elixir, functions are declared with the def \nkeyword. In Nx, you get something called defn. So,  ",
    "start": "468420",
    "end": "476220"
  },
  {
    "text": "it's just literally, like, \"def\" and then an \nn at the end. That's a numerical definition.   These numerical definitions support a tiny \nsubset of the Elixir programming language,  ",
    "start": "476220",
    "end": "484740"
  },
  {
    "text": "so, it's a little more strict. One of the things \nthat we noticed, we tried to model a lot of the   library off of JAX, which, you know, it's a \nlibrary. It offers the NumPy API, jax.numpy,  ",
    "start": "484740",
    "end": "496680"
  },
  {
    "text": "but it supports just-in-time compilation.\nAs a part of that just-in-time compilation,  ",
    "start": "497880",
    "end": "502980"
  },
  {
    "text": "you get some interesting behavior. So, JAX \nfunctions, JAX jitted functions, they have to be  ",
    "start": "503940",
    "end": "511140"
  },
  {
    "text": "completely pure. So, any side effects that happen, \nonly really happen once in these programs. And,  ",
    "start": "511140",
    "end": "517680"
  },
  {
    "text": "you know, they have to have some interesting, \nlike, static shape and type constraints.   And it was difficult for people transitioning \nto write JAX to get around, like, hey,  ",
    "start": "517680",
    "end": "528600"
  },
  {
    "text": "Python has these flexible abstractions. Like, \npeople like writing Python because it's flexible,  ",
    "start": "528600",
    "end": "534420"
  },
  {
    "text": "and JAX JIT was not flexible. So we made \nkind of an intentional decision to have a  ",
    "start": "534420",
    "end": "540000"
  },
  {
    "text": "completely separate abstraction from traditional \nElixir functions because we wanted people to   understand that when you write a numerical \ndefinition, it's gonna get JIT compiled.  ",
    "start": "540000",
    "end": "548280"
  },
  {
    "text": "You know, anything that's, like, side-effecting \nis not gonna work very well with what you wanna  ",
    "start": "549120",
    "end": "554460"
  },
  {
    "text": "do. And so, we wanted to keep that abstraction \ncompletely separate from the core language. ",
    "start": "554460",
    "end": "560040"
  },
  {
    "text": "When you write a numerical definition, \nit essentially gets immediately compiled,   and targeted to whatever the compiler you \nchoose is. So, in this case, if you're using  ",
    "start": "561000",
    "end": "570660"
  },
  {
    "text": "something like EXLA, which uses Google's XLA, \nyour numerical definition will get compiled to  ",
    "start": "570660",
    "end": "577019"
  },
  {
    "text": "the CPU or the GPU, depending on the client that \nyou choose, for the inputs that you give it. And  ",
    "start": "577020",
    "end": "584340"
  },
  {
    "text": "it's a very interesting abstraction because it's \nsomething that just extends the language. Like,   it didn't require any changes to Elixir upstream. \nElixir itself is just a flexible language, with  ",
    "start": "584340",
    "end": "595200"
  },
  {
    "text": "meta-programming and some of the other things you \ncan do. So we didn't have to make any changes to   Elixir upstream. It's just something that we were \nable to natively support, given what Elixir has. ",
    "start": "595200",
    "end": "604020"
  },
  {
    "text": "Which is beautiful, right? It's like you can \nsnap these guardrails right onto the system,  ",
    "start": "605160",
    "end": "611279"
  },
  {
    "text": "and everything just works, right? I hope that \nthe listeners are starting to get a sense that  ",
    "start": "611280",
    "end": "618540"
  },
  {
    "text": "rather than building something quick and dirty and \nmaking a library-type decision, this is something  ",
    "start": "618540",
    "end": "625620"
  },
  {
    "text": "a little bit more related to the infrastructure, \nin building up layer by layer, slowly. ",
    "start": "625620",
    "end": "632279"
  },
  {
    "start": "632000",
    "end": "1376000"
  },
  {
    "text": "Let's talk a little bit about the next \nlayer, the Axon layer. So, what is Axon? ",
    "start": "632880",
    "end": "638040"
  },
  {
    "text": "After the Nx project started to show some \ninitial successes, we wanted to get, like, real  ",
    "start": "639240",
    "end": "645180"
  },
  {
    "text": "applications of what we were building. The first, \nI would say, real concrete application was neural  ",
    "start": "645180",
    "end": "651060"
  },
  {
    "text": "networks because deep learning and neural networks \ntoday are almost synonymous with machine learning.  ",
    "start": "651060",
    "end": "657000"
  },
  {
    "text": "People say machine learning, and, like, 90% of the \ntime, people are talking about deep learning, just   because of the popularity of large language models \nand some of these other pre-trained models for  ",
    "start": "657000",
    "end": "666300"
  },
  {
    "text": "computer vision and natural language processing.\nWhile there is traditional machine learning,   and we do support traditional machine learning, \nwe targeted neural networks, because, at the time,  ",
    "start": "666300",
    "end": "675720"
  },
  {
    "text": "they were very, very popular, and that was the \nfirst thing. We wanted to prove we were able to do   that, because if we were able to do that, then we \nwere essentially able to do anything we wanted to.  ",
    "start": "675720",
    "end": "683760"
  },
  {
    "text": "So, Axon is a library for creating and training \nneural networks in Elixir. It has a very similar  ",
    "start": "685200",
    "end": "691980"
  },
  {
    "text": "API to Keras, TensorFlow Keras, as well as some \nother ideas stolen from the PyTorch ecosystem. So,  ",
    "start": "691980",
    "end": "698339"
  },
  {
    "text": "I am a, I would say, machine learning \nframework junkie and I spend a lot of   time just reading about different approaches to \nsolving machine learning problems, and different,  ",
    "start": "698340",
    "end": "709440"
  },
  {
    "text": "like, library design decisions that the \ncreators of PyTorch and Keras and some of   the projects in those ecosystems have made.\nLooking at the complaints of people trying  ",
    "start": "709440",
    "end": "718980"
  },
  {
    "text": "out different things and seeing what works. And \nso, Axon borrows a lot of ideas from these other  ",
    "start": "718980",
    "end": "724019"
  },
  {
    "text": "ecosystems, to make it easy to create composable \nneural networks and then also train these neural  ",
    "start": "724020",
    "end": "729660"
  },
  {
    "text": "networks. And it's a fundamentally different \napproach than what you see in the Python   ecosystem, just completely out of necessity. \nSo, Python supports those object-oriented  ",
    "start": "729660",
    "end": "740460"
  },
  {
    "text": "abstractions, and Axon, being built on top \nof a functional programming language, has to  ",
    "start": "740460",
    "end": "745860"
  },
  {
    "text": "build on functional constructs. So, it's a little \ndifficult, in terms of, like, comparing apples to  ",
    "start": "745860",
    "end": "752100"
  },
  {
    "text": "apples, you know, something that's implemented in \nKeras and something that's implemented in Axon,   but it is very similar. It will feel very \nsimilar to someone coming from another  ",
    "start": "752100",
    "end": "760380"
  },
  {
    "text": "ecosystem directly into the Elixir ecosystem.\nThis is cool, right? So, one of the things  ",
    "start": "760380",
    "end": "767160"
  },
  {
    "text": "that I've noticed is that by slowing down, \nwe're hitting this point where everything  ",
    "start": "767160",
    "end": "773940"
  },
  {
    "text": "seems to be happening at once. It seems like \nhaving the Elixir infrastructure underneath,  ",
    "start": "773940",
    "end": "779400"
  },
  {
    "text": "by slowing down and getting the abstractions \nright, all those things can be brought to bear   on the overall project. So, could you talk about \nthe impact of Axon and Nx on the Elixir ecosystem? ",
    "start": "780420",
    "end": "792600"
  },
  {
    "text": "First I wanna hit on the point that, like, \nstarting slowly, how fundamentally important that   was to what we wanted to do because I think this \nis happening a lot in the Python ecosystem now.  ",
    "start": "794340",
    "end": "803880"
  },
  {
    "text": "PyTorch, PyTorch 2.0, has made a huge, \nlike, effort to rewrite a lot of its  ",
    "start": "804780",
    "end": "810180"
  },
  {
    "text": "internals in Python, and there are \na lot of reasons for that decision.  ",
    "start": "810180",
    "end": "814380"
  },
  {
    "text": "But, you know, one of them is just, like, from a \nmaintainability perspective. Python, for better  ",
    "start": "815220",
    "end": "822360"
  },
  {
    "text": "or worse, is much more approachable than C++ as, \nlike, a, you know, language for writing compilers.  ",
    "start": "822360",
    "end": "827700"
  },
  {
    "text": "And for someone who's, you know, just coming \ninto the PyTorch ecosystem, it's a lot easier  ",
    "start": "828360",
    "end": "834180"
  },
  {
    "text": "for them to just look at some Python code that \nmaybe implements, like, some of their backends  ",
    "start": "834180",
    "end": "839339"
  },
  {
    "text": "for writing these numerical programs than \ntrying to figure out, like, decipher C++. W ",
    "start": "839340",
    "end": "845460"
  },
  {
    "text": "We had that kind of same inclination from the \nbeginning, that we should keep as much of what  ",
    "start": "846900",
    "end": "853860"
  },
  {
    "text": "we were building in Elixir as possible because \nit's more maintainable, it's more approachable,   and it was gonna just be easier for us to work \nwith, and to work fast and build on top of,  ",
    "start": "853860",
    "end": "864480"
  },
  {
    "text": "because neither myself nor José nor any of the \nmaintainers who have come on after the fact of the  ",
    "start": "865380",
    "end": "870780"
  },
  {
    "text": "original project, like Paolo and Jonathan, none of \nus are C++ people. We all are Elixir programmers. ",
    "start": "870780",
    "end": "878100"
  },
  {
    "text": "And so, keeping everything in Elixir allows us \nto work a lot faster than we traditionally would   have because we're a very small team of people \nwriting this. It's four, or five people who are  ",
    "start": "878100",
    "end": "888899"
  },
  {
    "text": "the core maintainers of the Nx project. We're \nable to implement features significantly faster,   just because we're working in Elixir, and, \nyou know, only reach into C++ and C and Rust  ",
    "start": "888900",
    "end": "898560"
  },
  {
    "text": "and whatever when it's necessary. Now, the \noverall impact that Nx and Axon have had  ",
    "start": "898560",
    "end": "905400"
  },
  {
    "text": "on the Elixir ecosystem, I would say it's been \npretty large, especially in the amount of time  ",
    "start": "905400",
    "end": "912480"
  },
  {
    "text": "that the projects have been out there. So, we are \nonly around three years into these projects, and  ",
    "start": "912480",
    "end": "918660"
  },
  {
    "text": "there are already some successful applications \nof these libraries being used in production. ",
    "start": "919200",
    "end": "923280"
  },
  {
    "text": "People I think are excited about the \nprospect of using machine learning in Elixir,   especially for companies that are using Elixir \nfor their actual, like, deployment environment,  ",
    "start": "924360",
    "end": "933360"
  },
  {
    "text": "their backend services, and stuff. It's a lot, \nI would say, easier for someone like them to  ",
    "start": "933360",
    "end": "939420"
  },
  {
    "text": "maybe get thrown a model from the Python ecosystem \nfrom their data science or machine learning team,   and then to implement, you know, essentially \nan inference pipeline directly in Elixir,  ",
    "start": "939420",
    "end": "949980"
  },
  {
    "text": "without having to call out to another service or \nbuild on top of some complex, like, microservices  ",
    "start": "949980",
    "end": "956639"
  },
  {
    "text": "stack. So, it has had a pretty large impact, and \nI'm excited to see where the ecosystem grows. ",
    "start": "956640",
    "end": "962340"
  },
  {
    "text": "We're starting to see all these little \npop-up projects, right? And that's always   an indication that you're doing something \nwell on the abstractions end. Right?  ",
    "start": "964140",
    "end": "972960"
  },
  {
    "text": "So, we've talked a little bit about the \nimpact of machine learning on Elixir,   and the idea that this is unexpected, and \npretty exciting, and has hit this critical mass,  ",
    "start": "974100",
    "end": "985560"
  },
  {
    "text": "where everything is rolling now. But we haven't \ntalked about the impact that we might see of  ",
    "start": "985560",
    "end": "992460"
  },
  {
    "text": "introducing Elixir to machine learning. Can you \ntalk about why that might be interesting to us? ",
    "start": "992460",
    "end": "998100"
  },
  {
    "text": "Anytime you try to, I guess, like, penetrate an \nadditional market from a programming language  ",
    "start": "999660",
    "end": "1007519"
  },
  {
    "text": "perspective, you have to, I would say, like, do \nit carefully, and think hard about why someone  ",
    "start": "1007520",
    "end": "1013760"
  },
  {
    "text": "would choose to use your language for whatever it \nis that they're doing, over what they're used to.  ",
    "start": "1013760",
    "end": "1020480"
  },
  {
    "text": "And particularly, like, in machine learning, \nPython is so entrenched, and it's for good reason.  ",
    "start": "1021440",
    "end": "1026900"
  },
  {
    "text": "There are a lot of really great abstractions \nand great libraries in the Python ecosystem.   It's friendly for, you know, beginner programmers, \nwho might have, like, you know, an academic  ",
    "start": "1026900",
    "end": "1035900"
  },
  {
    "text": "background, and they're interested in some aspect \nof, like, numerical computing or machine learning.  ",
    "start": "1035900",
    "end": "1040400"
  },
  {
    "text": "It's very easy to, you know, pick \nup Python and just run with it.  So, when we first started these projects, I think \na lot of people thought we were kind of crazy,  ",
    "start": "1041180",
    "end": "1050960"
  },
  {
    "text": "because, you know, trying to target something so \nentrenched, like Python is in the machine learning  ",
    "start": "1050960",
    "end": "1056720"
  },
  {
    "text": "ecosystem, you know, other languages have tried \nto do this, and it doesn't always have the best   results. And so, we were trying to, I guess, tread \ncarefully from the very beginning, that, you know,  ",
    "start": "1056720",
    "end": "1067460"
  },
  {
    "text": "we don't necessarily see these projects as \novertaking Python as, like, the primary language  ",
    "start": "1067460",
    "end": "1072559"
  },
  {
    "text": "for machine learning. But we wanted to give people \nwho were using Elixir and who were interested also  ",
    "start": "1072560",
    "end": "1077780"
  },
  {
    "text": "in Elixir kind of an alternative to some of the \noriginal workflows. As the projects have matured,   we're kind of identifying areas where our projects \ncould have a significant advantage over some of  ",
    "start": "1077780",
    "end": "1091279"
  },
  {
    "text": "the same projects in the Python ecosystem.\nI think one of those is in our serving   abstraction, which is a... Servings, in the \nworld of machine learning, are just, like,  ",
    "start": "1091280",
    "end": "1100340"
  },
  {
    "text": "an inference. It's essentially just a fancy way \nto say that we're gonna get inferences from the  ",
    "start": "1100340",
    "end": "1106880"
  },
  {
    "text": "model in production. And in the Python ecosystem, \nthere are, like, five or six serving projects,  ",
    "start": "1106880",
    "end": "1114140"
  },
  {
    "text": "and they're all separate services, like \nTorchServe, TensorFlow Serving, Kserve,  ",
    "start": "1114140",
    "end": "1119660"
  },
  {
    "text": "which is, like, a Kubernetes thing. There are all \nthese abstractions for essentially overcoming,  ",
    "start": "1119660",
    "end": "1125180"
  },
  {
    "text": "I think, some of the shortcomings that Python \nhas as a language for deploying machine   learning infrastructure. Whereas in the Elixir \necosystem, we don't necessarily have some of the  ",
    "start": "1125180",
    "end": "1134360"
  },
  {
    "text": "same shortcomings. So, we have this abstraction, \nwhich is Nx. Serving. It is essentially a data  ",
    "start": "1134360",
    "end": "1141679"
  },
  {
    "text": "structure or behavior that wraps up what you would \nsee in the production inference pipeline. So,  ",
    "start": "1141680",
    "end": "1147920"
  },
  {
    "text": "it encapsulates pre-processing, actual inference \nof the model, and then post-processing. And the   Nx. Serving abstraction is very, very simple, \nbut it supports some pretty insane things,  ",
    "start": "1147920",
    "end": "1157160"
  },
  {
    "text": "like, because of the way Elixir is built \non top of the Erlang virtual machine,  ",
    "start": "1157160",
    "end": "1162020"
  },
  {
    "text": "Nx. Serving supports distribution just natively.\nIf you have a cluster, you can spin up,  ",
    "start": "1162920",
    "end": "1169700"
  },
  {
    "text": "multiple servings, and they're load-balanced \nautomatically between the nodes in your cluster,   or, if you have, let's say, like, multiple GPUs, \nyou can partition inferences between multiple  ",
    "start": "1169700",
    "end": "1180140"
  },
  {
    "text": "GPUs. And it's a very scalable application. \nIt's a very scalable abstraction. It's also,  ",
    "start": "1180140",
    "end": "1186560"
  },
  {
    "text": "you get all of the goodies that you would get from \nbuilding on top of the Erlang virtual machine,   to begin with, like, you know, fault \ntolerance, and good concurrency and,  ",
    "start": "1187220",
    "end": "1195500"
  },
  {
    "text": "you know, the ability to build robust machine \nlearning applications on a battle-tested and  ",
    "start": "1195500",
    "end": "1201560"
  },
  {
    "text": "production-ready virtual machine.\nSo, it sounds kind of like there are  ",
    "start": "1201560",
    "end": "1207140"
  },
  {
    "text": "reasons to use Elixir, and there are \nreasons to do machine learning. And   so, the reasons to use Elixir don't go away just \nbecause you're entering this other space, right?  ",
    "start": "1207140",
    "end": "1218060"
  },
  {
    "text": "So, in some ways, all the things that Elixir \ndoes are becoming table stakes, right? And all  ",
    "start": "1218060",
    "end": "1225260"
  },
  {
    "text": "the things that machine learning does are becoming \ntable stakes. Once we can bring those two things  ",
    "start": "1225260",
    "end": "1232220"
  },
  {
    "text": "together, some pretty exciting things happen.\nExactly. We try as best as we possibly can to make  ",
    "start": "1232220",
    "end": "1239059"
  },
  {
    "text": "the ecosystem, you know, interop well with the \nPython ecosystem. So, we have this library called  ",
    "start": "1239060",
    "end": "1245600"
  },
  {
    "text": "Bumblebee, which supports a lot of pre-trained \nmachine-learning models in the Python ecosystem.  ",
    "start": "1245600",
    "end": "1253039"
  },
  {
    "text": "So, essentially what Bumblebee is, is it's very \nsimilar to the Hugging Face Transformers library,   which has a ton of pre-trained transformer models, \nand some other, like, computer vision-based  ",
    "start": "1253040",
    "end": "1262280"
  },
  {
    "text": "models, like ResNets and whatnot. And we built \nBumblebee as kind of, like, an intermediary  ",
    "start": "1262280",
    "end": "1267620"
  },
  {
    "text": "between ourselves and the Python ecosystem. \nSo, we're able to take pre-trained, like,  ",
    "start": "1267620",
    "end": "1272720"
  },
  {
    "text": "PyTorch models, convert them to what you would \nneed to use in Elixir, and then use them directly  ",
    "start": "1272720",
    "end": "1278900"
  },
  {
    "text": "in your Elixir applications. We also support a lot \nof the same tasks as you would see in the Python  ",
    "start": "1278900",
    "end": "1284660"
  },
  {
    "text": "ecosystem. So, Hugging Face has, like, pipelines \nis what they call them. We call them servings.  These pipelines support anything from the named \nentity recognition, to text classification, image  ",
    "start": "1284660",
    "end": "1294380"
  },
  {
    "text": "classification, to text generation. We support \nall those as well. So, if you are working with  ",
    "start": "1294380",
    "end": "1299900"
  },
  {
    "text": "a data science team that, you know, they're \nnot gonna wanna switch right away from using,   you know, Elixir, or from using Python to Elixir. \nSo they can still work in Python, they train their  ",
    "start": "1299900",
    "end": "1308419"
  },
  {
    "text": "models in Python, and then as long as you have, \nlike, trained or saved weights, you can kind of  ",
    "start": "1308420",
    "end": "1313580"
  },
  {
    "text": "throw them over to your backend team, and they \ncan write the inference pipeline in Elixir. ",
    "start": "1313580",
    "end": "1319159"
  },
  {
    "text": "So, we're very intentionally, I think, you \nknow, friendly, or try to be friendly and  ",
    "start": "1319160",
    "end": "1324440"
  },
  {
    "text": "supportive of the Python ecosystem, because in \nsome ways it's a necessity. There's no reason   to completely disregard all of the incredible \nwork that's been done in the Python ecosystem  ",
    "start": "1324440",
    "end": "1334160"
  },
  {
    "text": "because it's just unrealistic to think that we \nwould be able to catch up with 30-plus years of  ",
    "start": "1334160",
    "end": "1339980"
  },
  {
    "text": "a head start in this space, right? So, we try to \nsupport interop. We also support ONNX, Open Neural  ",
    "start": "1339980",
    "end": "1347720"
  },
  {
    "text": "Network Exchange, so you can transfer...or you can \nessentially take ONNX models that you've trained,  ",
    "start": "1347720",
    "end": "1354020"
  },
  {
    "text": "you know, in the Python ecosystem and run \nthem with some of the Nx abstractions. So,   we have what's called, like, a storage-only \nbackend, where you use the Nx. Serving  ",
    "start": "1354020",
    "end": "1363020"
  },
  {
    "text": "abstraction as a way to implement an inference \npipeline, and it's backed by the ONNX runtime. So,  ",
    "start": "1363020",
    "end": "1367820"
  },
  {
    "text": "there are a lot of reasons to use Elixir without \nactually having to switch from using Python.",
    "start": "1368480",
    "end": "1375260"
  },
  {
    "start": "1376000",
    "end": "1531000"
  },
  {
    "text": "Can we talk about some of the use cases \nthat you might have seen in Bumblebee?  ",
    "start": "1376100",
    "end": "1380960"
  },
  {
    "text": "What can this thing do, and where... Well, \nfirst let's talk about what it can do. ",
    "start": "1381980",
    "end": "1386600"
  },
  {
    "text": "Bumblebee can do a lot of the same things that \nthe Hugging Face Transformers library can do. So,   we support a pretty large, I would say, relative \nto the size of the Transformers library,  ",
    "start": "1388220",
    "end": "1399980"
  },
  {
    "text": "we have a pretty large coverage of \nthe pre-trained models, at least the   ones that are the most popular. You could take \nsomething like a pre-trained BERT and do text  ",
    "start": "1399980",
    "end": "1410960"
  },
  {
    "text": "classification with that, and you can fine-tune \nthe models and, you know, pre-trained models   from Bumblebee for downstream applications. We \nalso support, like I said, these servings. So,  ",
    "start": "1410960",
    "end": "1419540"
  },
  {
    "text": "one example that I've seen used is in entity \nrecognition, essentially extracting, like, proper  ",
    "start": "1420320",
    "end": "1428059"
  },
  {
    "text": "nouns out of some structured or unstructured \ndata, like text, and identifying, like, hey,  ",
    "start": "1428060",
    "end": "1433700"
  },
  {
    "text": "this is a person, this is a place, this is, you \nknow, an organization. And then we also support,  ",
    "start": "1433700",
    "end": "1440419"
  },
  {
    "text": "like, text generation. So, we do support some of \nthe latest and greatest chat models out there. ",
    "start": "1440420",
    "end": "1446840"
  },
  {
    "text": "So, LLaMA, for example, is one of the ones that \nwe do support. You can build on large language  ",
    "start": "1446840",
    "end": "1452720"
  },
  {
    "text": "models in Elixir without having to actually, you \nknow, shell out to Python or something else. So,   there are a lot of very powerful applications. \nAnd one of the strengths of Bumblebee is that  ",
    "start": "1452720",
    "end": "1461660"
  },
  {
    "text": "it's a very low-code, I would say, library. \nGetting up and running with a text generation  ",
    "start": "1461660",
    "end": "1467420"
  },
  {
    "text": "pipeline is probably, like, four lines of code \nand you're ready to go. That's pretty powerful,  ",
    "start": "1467420",
    "end": "1472880"
  },
  {
    "text": "especially for us, because, in the Elixir \necosystem, we don't have a ton of people with   machine learning experience. So, Bumblebee \ncan give them access to, like, a quick win. ",
    "start": "1472880",
    "end": "1480860"
  },
  {
    "text": "I've talked to several people who have started \nto make some Bumblebee contributions. They said   it's remarkably easy. So, it seems like, \nonce again, the abstractions are good. ",
    "start": "1481940",
    "end": "1493039"
  },
  {
    "text": "Yes And Bumblebee builds... It's in \na 100% Elixir library, so really,  ",
    "start": "1493040",
    "end": "1498320"
  },
  {
    "text": "the only libraries we have that touch any sort of \nnative code are compilers for Nx. Nx itself is a  ",
    "start": "1498320",
    "end": "1507080"
  },
  {
    "text": "100% Elixir library, and then it's the compilers \nthat Nx touches that are written in C C++ Rust,  ",
    "start": "1507080",
    "end": "1513740"
  },
  {
    "text": "and some of the other native languages. But \neverything, from Nx to Axon to Bumblebee,  ",
    "start": "1513740",
    "end": "1519559"
  },
  {
    "text": "it's 100% Elixir. So it's a very approachable \nlibrary. The abstractions are very, I think,  ",
    "start": "1519560",
    "end": "1525500"
  },
  {
    "text": "easy to understand once you kind of peel \nback the layers, so it's very powerful.",
    "start": "1525500",
    "end": "1530000"
  },
  {
    "start": "1531000",
    "end": "1853000"
  },
  {
    "text": "So, we've spent a little bit of time embracing \nPython, and I'm not gonna say take your shot now,  ",
    "start": "1531140",
    "end": "1536540"
  },
  {
    "text": "but what are some of the things that Elixir \ndoes that may make it maybe even better for  ",
    "start": "1536540",
    "end": "1542600"
  },
  {
    "text": "machine learning than some of the \nother machine learning languages?  I think the obvious one here is concurrency. \nPython, with the GIL, it's kind of difficult  ",
    "start": "1542600",
    "end": "1554420"
  },
  {
    "text": "to achieve the same, I would say, level \nof concurrency that you can achieve in an   Elixir application. Elixir is good for building, \nlike, robust, fault-tolerant, highly concurrent  ",
    "start": "1554420",
    "end": "1566120"
  },
  {
    "text": "applications. One of the things that originally \ndrew me to trying to do machine learning in Elixir  ",
    "start": "1566120",
    "end": "1572120"
  },
  {
    "text": "was there's a book from the Pragmatic Bookshelf as \nwell, \"Concurrent Data Processing in Elixir.\" It's  ",
    "start": "1572120",
    "end": "1577940"
  },
  {
    "text": "about building these robust data pipelines, highly \nconcurrent data pipelines. That's something that  ",
    "start": "1577940",
    "end": "1586279"
  },
  {
    "text": "you see a lot in machine learning workloads, \nand achieving some of the same, I guess,  ",
    "start": "1586280",
    "end": "1591440"
  },
  {
    "text": "throughput that you would get in the Python \necosystem in the Elixir ecosystem is just trivial.   There's a lot of, like, abstractions in the \nPython ecosystem that are essentially just,  ",
    "start": "1591440",
    "end": "1601460"
  },
  {
    "text": "like, wrappers around C and C++ implementations, \nwhereas, you know, you don't have to do the  ",
    "start": "1601460",
    "end": "1607760"
  },
  {
    "text": "same thing in the Elixir ecosystem.\nFor example, like, tf. data is a data  ",
    "start": "1607760",
    "end": "1612800"
  },
  {
    "text": "input pipeline for TensorFlow. And the same things \nyou can do in tf.data, you can just do natively  ",
    "start": "1612800",
    "end": "1621860"
  },
  {
    "text": "with Elixir, because it just supports this, you \nknow, concurrent data processing out of the box.  ",
    "start": "1621860",
    "end": "1626059"
  },
  {
    "text": "Then, a lot of, like, the, you know, OTP \nabstractions in Elixir are, I would say, very,  ",
    "start": "1627860",
    "end": "1635299"
  },
  {
    "text": "very well-suited for building these, like, robust \nmachine learning applications. So, I'm just now,  ",
    "start": "1635300",
    "end": "1641300"
  },
  {
    "text": "because, you know, I was not... Getting into \nthe language, I liked Elixir aesthetically,   but I didn't necessarily appreciate the OTP \nabstractions as much. And now recently, I'm  ",
    "start": "1641300",
    "end": "1650240"
  },
  {
    "text": "starting to get really into the OTP abstractions, \nand building more on what the language is designed  ",
    "start": "1650240",
    "end": "1656300"
  },
  {
    "text": "to do, and seeing how it, you know, connects \nwell with some of the things you wanna do in,  ",
    "start": "1656300",
    "end": "1661760"
  },
  {
    "text": "like, the MLOps lifecycle, which is, you know, the \nlifecycle for deploying machine learning models   and trying to identify, I guess, use cases for, \noh, this is, like, you know, really powerful,  ",
    "start": "1661760",
    "end": "1671240"
  },
  {
    "text": "and this is how this benefits the machine learning \necosystem. So, there's a lot of things that I   think Elixir does better than Python, just out \nof, you know, the circumstances of the language.  ",
    "start": "1671240",
    "end": "1680300"
  },
  {
    "text": "The language is designed for telecom platforms, \nright? Or built on top of a language designed for   telecom platforms. It turns out those abstractions \nare also really good for building, robust web  ",
    "start": "1680300",
    "end": "1689000"
  },
  {
    "text": "applications. That's one thing, or those are some \nof the things I think that Elixir does just better  ",
    "start": "1689000",
    "end": "1694100"
  },
  {
    "text": "than Python as a consequence of how it's built.\nWhat about immutability? Does that play a role,  ",
    "start": "1694100",
    "end": "1700039"
  },
  {
    "text": "or does the lack of immutability play a role in \nthe way that you've had to build Axon in layers,  ",
    "start": "1700040",
    "end": "1707420"
  },
  {
    "text": "versus the way you might have done \nit with something like Python?  I think immutability is an interesting one, \nbecause for, like, mathematicians and people  ",
    "start": "1707420",
    "end": "1716000"
  },
  {
    "text": "who are coming from, like, an academic machine \nlearning background, the immutability and,   like, the functional style of writing things in \nElixir kind of fits better with what, you know,  ",
    "start": "1716000",
    "end": "1726620"
  },
  {
    "text": "you would be used to seeing, like, mathematically. \nImmutability, I think, helps a lot in reasoning  ",
    "start": "1726620",
    "end": "1733280"
  },
  {
    "text": "about some of these more complex, highly \nconcurrent data pipelines. But then from, like,  ",
    "start": "1733280",
    "end": "1739340"
  },
  {
    "text": "just the, I guess, aesthetic perspective, writing \na program, a numerical program functionally,  ",
    "start": "1739340",
    "end": "1746480"
  },
  {
    "text": "I think makes a lot more sense than some of the \nthings you would do in the Python ecosystem. ",
    "start": "1746480",
    "end": "1751820"
  },
  {
    "text": "For example, TensorFlow and PyTorch both \nsupport what are called in-place operations,  ",
    "start": "1752420",
    "end": "1757520"
  },
  {
    "text": "where essentially you have a tensor that's backed \nby some buffer, and you can perform an in-place  ",
    "start": "1757520",
    "end": "1765560"
  },
  {
    "text": "sort, where that data is completely changed, \ncompletely overwritten. And I've had, you know,  ",
    "start": "1765560",
    "end": "1772460"
  },
  {
    "text": "experiences in the Python ecosystem where I \ndo something in place, and then you get some  ",
    "start": "1772460",
    "end": "1778460"
  },
  {
    "text": "pretty wonky results because you don't realize \nthat you are mutating some data, like, four or   five lines up, or, you know, somewhere at the \nbeginning of the program. You don't necessarily  ",
    "start": "1778460",
    "end": "1786260"
  },
  {
    "text": "have that same problem in the Elixir ecosystem \nbecause everything is immutable by default. ",
    "start": "1786260",
    "end": "1790580"
  },
  {
    "text": "From a performance perspective, it's something \nthat kind of hindered Elixir from the beginning,  ",
    "start": "1791660",
    "end": "1798200"
  },
  {
    "text": "because, with immutability, that, like, \nkind of implies some additional copies. But,  ",
    "start": "1798200",
    "end": "1803779"
  },
  {
    "text": "with this compiler, this JIT compilation \nconcept that we introduced with Nx, we kind of  ",
    "start": "1803780",
    "end": "1811220"
  },
  {
    "text": "completely bypass any of the issues we have \nwith immutability, because Nx works on, like,  ",
    "start": "1811940",
    "end": "1817759"
  },
  {
    "text": "a multi-staged programming model. So, when you \nwrite a numerical definition, it gets lowered to  ",
    "start": "1817760",
    "end": "1823820"
  },
  {
    "text": "an Nx expression, and then that gets compiled into \na program. So, it's not eager by default. It's a  ",
    "start": "1823820",
    "end": "1829519"
  },
  {
    "text": "very, you know, I would say, static workflow. You \ndon't necessarily have the same performance hits  ",
    "start": "1829520",
    "end": "1836120"
  },
  {
    "text": "with immutability that you would if you were \njust, you know, working natively in Elixir.",
    "start": "1836120",
    "end": "1840800"
  },
  {
    "text": "You just kind of carved down what your \nprimitive operations are, right? You   expand those a little bit, and contract them \nin other places, right? And that's pretty cool.  ",
    "start": "1841820",
    "end": "1851660"
  },
  {
    "text": "So, I have a couple of more questions for \nyou. Do you have some favorite moments,  ",
    "start": "1852440",
    "end": "1857179"
  },
  {
    "start": "1853000",
    "end": "2243000"
  },
  {
    "text": "you know, of where this whole rollercoaster ride \nhas taken you? Are there some favorite moments? ",
    "start": "1859280",
    "end": "1866180"
  },
  {
    "text": "Yeah, there's a lot of, like, stories \nrelated to these projects. Early on,   I don't think we... There was a lot of, I would \nsay, initial roadblocks to success. And, like,  ",
    "start": "1866180",
    "end": "1877100"
  },
  {
    "text": "we have come a long way, but in the beginning, \nthere was not, like, a guarantee that the projects   were gonna work out. It was kind of just more of \nan experiment. So, I can distinctly remember some  ",
    "start": "1877100",
    "end": "1885559"
  },
  {
    "text": "of the initial, like, trials and tribulations \nwith these projects. One, for example, was  ",
    "start": "1885560",
    "end": "1889940"
  },
  {
    "text": "the first time we got Nx and, like, to compile a \nprogram to the GPU, which was a little rough. So,  ",
    "start": "1890780",
    "end": "1899780"
  },
  {
    "text": "I go into a deep dive on Twitter about this, but \nthe Erlang virtual machine does some interesting  ",
    "start": "1900380",
    "end": "1906740"
  },
  {
    "text": "things, like, intentionally, and when you're \ndealing with subprocesses, and external programs,  ",
    "start": "1906740",
    "end": "1913820"
  },
  {
    "text": "you can run into some problems.\nSo, I have a deep dive on Twitter about that,   but I do distinctly remember it took, like, a \nfew days to track down some issues we were having  ",
    "start": "1914780",
    "end": "1921320"
  },
  {
    "text": "with, you know, why could we not compile a program \nto the GPU? And then we had kind of a breakthrough   moment, we were able to... And I think it was \nthe program we were compiling was just, like,  ",
    "start": "1921320",
    "end": "1928340"
  },
  {
    "text": "one plus one, or something simple. Like, \nit was nothing crazy. But that was pretty   awesome when that first happened. And then I \nremember we had some initial difficulties with  ",
    "start": "1928340",
    "end": "1938720"
  },
  {
    "text": "autograds. So, José wrote a lot of the autograd, \nlike, infrastructure. I think he honestly has  ",
    "start": "1941120",
    "end": "1947240"
  },
  {
    "text": "probably rewritten it, like, six or seven times \nin, you know, the life of the project. I remember  ",
    "start": "1947240",
    "end": "1953540"
  },
  {
    "text": "how, just how frustrating it was at times \nto get some of the things to work, because   autograd is not necessarily...well, automatic \ndifferentiation is not necessarily something  ",
    "start": "1953540",
    "end": "1961159"
  },
  {
    "text": "that's, like, straightforward to implement, \nand straightforward to implement efficiently.  I remember the first neural network we trained, \nwhen we finally got, the automatic differentiation  ",
    "start": "1961160",
    "end": "1971600"
  },
  {
    "text": "system working, and we had written a pure Nx \nneural network. It was just trained on MNIST,  ",
    "start": "1971600",
    "end": "1978200"
  },
  {
    "text": "and I remember, I think that was, like, maybe six \nor seven months into the project when that first   worked, and that was pretty awesome. Then, some \nof the first benchmarks we had were, you know,  ",
    "start": "1978200",
    "end": "1989180"
  },
  {
    "text": "we showed that the GPU-compiled program, with \nNx and EXLA were, like, 4,000 times faster than  ",
    "start": "1989180",
    "end": "1996020"
  },
  {
    "text": "anything you can do natively in Elixir, and \nsharing some of those benchmarks, and people   were like, \"This is crazy. I can't believe \nthis is happening.\" That was a lot of fun too. ",
    "start": "1996020",
    "end": "2005260"
  },
  {
    "text": "Yes. With the coy messaging, you know, this thing \nthat we're working on is X% faster, that was kind  ",
    "start": "2006460",
    "end": "2013240"
  },
  {
    "text": "of a lot of fun to watch too.\nYes.  And what about some moments that were particularly \nfrustrating for you? What have been some of the  ",
    "start": "2013240",
    "end": "2024040"
  },
  {
    "text": "hard ones to break through? You mentioned \nthe earlier one, with kind of established  ",
    "start": "2024040",
    "end": "2029920"
  },
  {
    "text": "that initial compilation, but what were some \nof the other ones that were pretty difficult? ",
    "start": "2030940",
    "end": "2034059"
  },
  {
    "text": "The GPU one, in particular, I just remember \nbeing incredibly frustrating. So, I guess,   like, high-level, essentially, what was going on \nthere is that, to compile a program to the GPU,  ",
    "start": "2036340",
    "end": "2047560"
  },
  {
    "text": "specifically NVIDIA GPUs, TensorFlow, and XLA were \nusing something called PTX, which is, like, an  ",
    "start": "2047560",
    "end": "2053860"
  },
  {
    "text": "NVIDIA assembler, essentially. They were calling \nit out from a subprocess, using, I think, like,  ",
    "start": "2053860",
    "end": "2060520"
  },
  {
    "text": "waitpid, or something particular. And there's, \nlike, something where, on Linux systems,  ",
    "start": "2062440",
    "end": "2068679"
  },
  {
    "text": "the Erlang virtual machine sets SIGCHLD to, \nI think, sig ignore or something specific,  ",
    "start": "2068680",
    "end": "2076059"
  },
  {
    "text": "which essentially just results in the \ncalling process of waitpid to completely,  ",
    "start": "2076060",
    "end": "2082300"
  },
  {
    "text": "like, ignore the result of the program and \nreturn negative one or something insane. And,   like, tracking that down took forever. And \nthat's pretty obscure about the Erlang,  ",
    "start": "2082300",
    "end": "2092200"
  },
  {
    "text": "you know, virtual machine, that, like, only \na few people would know off the bat, right? ",
    "start": "2092200",
    "end": "2096280"
  },
  {
    "text": "There was another very frustrating segmentation \nfault we were getting with convolutions on NVIDIA  ",
    "start": "2097240",
    "end": "2104560"
  },
  {
    "text": "GPUs. And I remember just how frustrating it was, \nkind of, working with NVIDIA and trying to figure   out what the deal was. It turned out the reason \nfor the segmentation fault was the default stack  ",
    "start": "2104560",
    "end": "2114040"
  },
  {
    "text": "size for the Dirty NIFs that we were using, which \nis kind of, it's an Elixir-specific thing. The  ",
    "start": "2114040",
    "end": "2121180"
  },
  {
    "text": "Dirty NIFs we were using, the default stack size \nwas too small. And I had talked to some of the  ",
    "start": "2122140",
    "end": "2127420"
  },
  {
    "text": "core team for Erlang a few times about this, and \nthey kept saying, well if you just set the stack  ",
    "start": "2127420",
    "end": "2132640"
  },
  {
    "text": "size a little bigger, does it work? And I was \nlike, \"Yeah, I've tried that. It doesn't work,\"   but I was setting the flag wrong or something. So, \nit took me, like, four or five months to realize  ",
    "start": "2132640",
    "end": "2139299"
  },
  {
    "text": "that I was setting the flag wrong. I think NVIDIA \nhad been telling me the same thing too. They were   like, \"Well, what's the stack size? Like, is the \nstack size too small?\" So that was frustrating,  ",
    "start": "2139300",
    "end": "2147280"
  },
  {
    "text": "but that was probably more frustrating for me, to \nrealize that I was making a silly mistake. It was,   like, a typo or something, and you know how \nfrustrating it can be when you realize that you  ",
    "start": "2147280",
    "end": "2156520"
  },
  {
    "text": "have a typo in your code that's been contributing \nto a bug for, like, four or five months now.   That was frustrating. And there's been some, I \nguess, difficult moments in implementing Axon. ",
    "start": "2156520",
    "end": "2167619"
  },
  {
    "text": "I think we have the benefit of being able to test \nagainst correct implementations in PyTorch and  ",
    "start": "2169360",
    "end": "2174520"
  },
  {
    "text": "TensorFlow, but when you're implementing numerical \nalgorithms, getting exact, like, correctness,  ",
    "start": "2175180",
    "end": "2183640"
  },
  {
    "text": "numerical correctness, is very, very difficult. \nThere can be very subtle bugs that just pop up,  ",
    "start": "2183640",
    "end": "2189819"
  },
  {
    "text": "and they have a drastic impact on the \nstability of, like, training different  ",
    "start": "2189820",
    "end": "2195460"
  },
  {
    "text": "models and the predictions you get with different \nmodels. And so, that can be very frustrating as   well. I remember we implemented... So, one of \nthe things you can do with Bumblebee is Stable  ",
    "start": "2195460",
    "end": "2204099"
  },
  {
    "text": "Diffusion. You can do image generation. I just \nremember working for, like, two weeks straight,  ",
    "start": "2204100",
    "end": "2209620"
  },
  {
    "text": "trying to get the outputs we were getting from \nStable Diffusion to match, like, within a very  ",
    "start": "2209620",
    "end": "2215620"
  },
  {
    "text": "small precision of what you get in Python. I also \nremember, like, thinking how insane, or, you know,  ",
    "start": "2215620",
    "end": "2223480"
  },
  {
    "text": "how much admiration I have for people that are \nimplementing these algorithms from scratch,   without any reference implementation, and \nwithout any reference tests or anything,  ",
    "start": "2224020",
    "end": "2232660"
  },
  {
    "text": "to say, like, you know, this is the correct \nimplementation of whatever this algorithm is. So,  ",
    "start": "2232660",
    "end": "2237280"
  },
  {
    "text": "it is always very frustrating to try to track \ndown those small bugs in numerical correctness.",
    "start": "2238000",
    "end": "2242260"
  },
  {
    "start": "2243000",
    "end": "2328000"
  },
  {
    "text": "So, I have two more questions. They're wrap-up \nquestions. One of them is, to make your last  ",
    "start": "2243940",
    "end": "2251619"
  },
  {
    "text": "pitch for machine learning with Elixir.\nFor, I think those that are deploying...  ",
    "start": "2251620",
    "end": "2260260"
  },
  {
    "text": "Well, first, I honestly think that Elixir for \nmachine learning startups is probably the best   language that you can have because you can do \neverything in Elixir. And not just everything  ",
    "start": "2260980",
    "end": "2271600"
  },
  {
    "text": "from a machine learning perspective, but also \nfrom an application development perspective. So,  ",
    "start": "2271600",
    "end": "2277420"
  },
  {
    "text": "with LiveView, you can write your front end in \nElixir. With Phoenix, you can write scalable  ",
    "start": "2277420",
    "end": "2283180"
  },
  {
    "text": "backends with Elixir. You can have your entire, \nyou know, inference pipeline written in Elixir,  ",
    "start": "2283180",
    "end": "2288280"
  },
  {
    "text": "and it's fault-tolerant and scalable. You can \ntrain your models in Elixir. You can deploy  ",
    "start": "2288280",
    "end": "2293800"
  },
  {
    "text": "your models in Elixir. I think, for a startup \nthat's, you know, undermanned, or, you know,  ",
    "start": "2293800",
    "end": "2299260"
  },
  {
    "text": "it doesn't necessarily have large teams, you can \nbuild at a very high velocity. I think that's  ",
    "start": "2299260",
    "end": "2304600"
  },
  {
    "text": "something that you just don't necessarily get \nfrom another ecosystem. So, if I was a small team,   that is obvious...I think that would be the \nfirst language I would reach for, would be,  ",
    "start": "2304600",
    "end": "2314020"
  },
  {
    "text": "you know, building an application in Elixir, \nbecause you can punch above your weight. I like that. So, you take JavaScript off \nthe table, you take Python off the table,  ",
    "start": "2314020",
    "end": "2323380"
  },
  {
    "text": "and you centralize everything on one language. \nThat's wonderful. And I have one last question. Is  ",
    "start": "2323380",
    "end": "2329019"
  },
  {
    "start": "2328000",
    "end": "2419000"
  },
  {
    "text": "there anything else that you want your listeners \nto know? What's coming up? What's happening? ",
    "start": "2329020",
    "end": "2334060"
  },
  {
    "text": "I guess, we've kind of ranted a little bit \nabout machine learning in Elixir, but I want the  ",
    "start": "2335980",
    "end": "2341740"
  },
  {
    "text": "listeners to know, if you're not familiar with \nthe machine learning in the Elixir ecosystem,   we have, I think the perfect treat for you. \nSo, I just recently released a book, \"Machine  ",
    "start": "2341740",
    "end": "2349840"
  },
  {
    "text": "Learning in Elixir.\" It's out in beta now, where \nyou can learn the fundamentals of the machine  ",
    "start": "2349840",
    "end": "2355720"
  },
  {
    "text": "learning ecosystem in Elixir from the ground up.\nSo, if you're not familiar with Elixir, it's a  ",
    "start": "2355720",
    "end": "2362680"
  },
  {
    "text": "great way to learn the language. And then, if you \nare familiar with Elixir and you're not familiar   with machine learning, it's a great way to learn \nmachine learning. \"Machine Learning in Elixir\" is,  ",
    "start": "2362680",
    "end": "2370059"
  },
  {
    "text": "I would say, designed to be the authoritative \nsource on everything you can do with machine   learning in Elixir. And it's got a lot of \ninspiration from some of my favorite machine  ",
    "start": "2370060",
    "end": "2378880"
  },
  {
    "text": "learning books, such as, you know, François \nChollet's \"Deep Learning with Python\" which was   kind of the first book I ever read in machine \nlearning. And then some of the other popular  ",
    "start": "2378880",
    "end": "2387760"
  },
  {
    "text": "machine learning textbooks out there, like \"Deep \nLearning,\" by Ian Goodfellow and Yoshua Bengio   and the other co-authors they have there.\nSo, I would highly recommend any listeners  ",
    "start": "2387760",
    "end": "2397360"
  },
  {
    "text": "of these to go check out that book. Send me \nany errata you find, because it is in beta,   so obviously, there's gonna be some issues as \nwe upgrade the libraries and things change. But  ",
    "start": "2397360",
    "end": "2407080"
  },
  {
    "text": "yeah, I think that's one of my key focuses now \nis just building out some of the educational  ",
    "start": "2407740",
    "end": "2413020"
  },
  {
    "text": "material for people in the ecosystem to \nkind of, you know, cut their teeth on. ",
    "start": "2413020",
    "end": "2417520"
  },
  {
    "text": "Wonderful. That's a beautiful conversation \ngoing from genetic algorithms to machine   learning in Elixir. And so, for Bruce \nTate and Sean Moriarity, we've been  ",
    "start": "2418600",
    "end": "2428920"
  },
  {
    "start": "2419000",
    "end": "2466000"
  },
  {
    "text": "talking for the GOTO Book Club, \nand we're signing off. Thank you.  Thank you, everyone.\nIt was a lot of  ",
    "start": "2428920",
    "end": "2445720"
  },
  {
    "text": "fun.",
    "start": "2463600",
    "end": "2463660"
  }
]