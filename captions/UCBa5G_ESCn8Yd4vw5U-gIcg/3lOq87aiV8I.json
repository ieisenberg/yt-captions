[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "i'm just going to dive right in because there is a lot that we can talk about with ethical ai and i",
    "start": "11040",
    "end": "18560"
  },
  {
    "text": "consider it um a cornerstone in in safe ai although i",
    "start": "18560",
    "end": "23760"
  },
  {
    "text": "am highly biased as well because this is what i do but i'm going to go over some of the um",
    "start": "23760",
    "end": "29279"
  },
  {
    "text": "the main principles in ethical ai and go into some details on how you can",
    "start": "29279",
    "end": "34800"
  },
  {
    "text": "integrate it into your work all right so the principles of ethical",
    "start": "34800",
    "end": "39840"
  },
  {
    "start": "39000",
    "end": "211000"
  },
  {
    "text": "ai i love this little infographic it was created by harvard and basically each one of these",
    "start": "39840",
    "end": "46559"
  },
  {
    "text": "different spokes represent a an organization so somebody like",
    "start": "46559",
    "end": "53680"
  },
  {
    "text": "salesforce for instance but a government institutions so like gdpr is in here i",
    "start": "53680",
    "end": "59920"
  },
  {
    "text": "believe and there are across industry like ieee has a set of standards and",
    "start": "59920",
    "end": "65840"
  },
  {
    "text": "each one of these little circles is a category or principle and what these boil down to this is a little",
    "start": "65840",
    "end": "72400"
  },
  {
    "text": "bit more nuanced than what i have here on the side is around um fairness accountability",
    "start": "72400",
    "end": "79600"
  },
  {
    "text": "transparency privacy security and human rights and if the circle is lit then",
    "start": "79600",
    "end": "85439"
  },
  {
    "text": "basically the organization has included it and one of the reasons i like to point",
    "start": "85439",
    "end": "90640"
  },
  {
    "text": "this out is that you know there are differences in which principles are most",
    "start": "90640",
    "end": "96640"
  },
  {
    "text": "important for a different organization so when i was at intel they create computer chips and so like",
    "start": "96640",
    "end": "103840"
  },
  {
    "text": "their their goals and their um concerns are different than when i was at facebook which is like a social media",
    "start": "103840",
    "end": "110240"
  },
  {
    "text": "platform which has a ridiculous amount of data and lots of different issues than what i'm uh working with",
    "start": "110240",
    "end": "117119"
  },
  {
    "text": "now at salesforce which is um a c2c no a b2b a business to business type of",
    "start": "117119",
    "end": "124719"
  },
  {
    "text": "company so we sell our products to other businesses who then use it to",
    "start": "124719",
    "end": "130720"
  },
  {
    "text": "make sure that their users are happy with what they're offering in terms of products or emails or",
    "start": "130720",
    "end": "136400"
  },
  {
    "text": "non-profit institutions use it too and so what uh",
    "start": "136400",
    "end": "142560"
  },
  {
    "text": "we've done here at salesforce and you know like what you should think about in your research in your labs uh when you",
    "start": "142560",
    "end": "149200"
  },
  {
    "text": "go off and you know maybe create a startup or join a company is what are the principles that are most important to",
    "start": "149200",
    "end": "155760"
  },
  {
    "text": "your group and for what you're doing so for us um this laid around a responsible",
    "start": "155760",
    "end": "161920"
  },
  {
    "text": "accountable transparent empowering um and inclusive so the",
    "start": "161920",
    "end": "167680"
  },
  {
    "text": "empowering one i think is a little bit more unique for for our organization",
    "start": "167680",
    "end": "174239"
  },
  {
    "text": "because you know we're wanting to be able to promote growth and employment for our customers and their users etc so",
    "start": "174239",
    "end": "182480"
  },
  {
    "text": "you know empowering our customers to use ai responsibly is is something that's really really important because a lot of times um",
    "start": "182480",
    "end": "190000"
  },
  {
    "text": "these customers are coming with their own data and they need to be able to say hey i know that you checked this model",
    "start": "190000",
    "end": "195760"
  },
  {
    "text": "when it was in your how when this ai model was in your shop but now i have changed it so is it still oh my god hi",
    "start": "195760",
    "end": "202239"
  },
  {
    "text": "panda this is my cat he does this i've just learned to accept it",
    "start": "202239",
    "end": "207920"
  },
  {
    "text": "okay cool so why are ai ethics important um these are",
    "start": "207920",
    "end": "213840"
  },
  {
    "start": "211000",
    "end": "292000"
  },
  {
    "text": "some headlines and basically every week there will be a new headline uh about some sort of technology that's",
    "start": "213840",
    "end": "220640"
  },
  {
    "text": "gone awry and this could be things like patrol robots in singapore",
    "start": "220640",
    "end": "226239"
  },
  {
    "text": "and actually this i just saw a tweet about this with the recent lockdowns you know people are afraid of their privacy",
    "start": "226239",
    "end": "233040"
  },
  {
    "text": "of their safety etc and then there's other issues like with",
    "start": "233040",
    "end": "239120"
  },
  {
    "text": "the ai itself like with the labels that it might be putting on so facebook",
    "start": "239120",
    "end": "244480"
  },
  {
    "text": "had an issue google had the same issue of uh ai putting the primate label on a",
    "start": "244480",
    "end": "251120"
  },
  {
    "text": "video or an image of a black person it's like you know",
    "start": "251120",
    "end": "256320"
  },
  {
    "text": "it's like that's not okay and you know if you're if you know computer vision you can say oh well yeah i understand",
    "start": "256320",
    "end": "262320"
  },
  {
    "text": "why it made that mistake potentially like the lighting was weird we didn't have enough samples but at the end of",
    "start": "262320",
    "end": "268800"
  },
  {
    "text": "the day it's just not an okay thing to happen um so it's important you we we have to make",
    "start": "268800",
    "end": "276560"
  },
  {
    "text": "our algorithms work for everybody um in a way that's reliable and",
    "start": "276560",
    "end": "282639"
  },
  {
    "text": "doesn't you know like perpetuate the systematic biases that we've had in our society is what it boils down to",
    "start": "282639",
    "end": "290720"
  },
  {
    "text": "so and one of the the biggest",
    "start": "291520",
    "end": "296720"
  },
  {
    "start": "292000",
    "end": "389000"
  },
  {
    "text": "issues around this though is that you know like technology is dual use so we talked a little bit about you know like",
    "start": "296720",
    "end": "302720"
  },
  {
    "text": "image labeling can have some very large ethical issues you know and you might",
    "start": "302720",
    "end": "308479"
  },
  {
    "text": "ask okay well maybe we should just not do image categorization or personal",
    "start": "308479",
    "end": "313520"
  },
  {
    "text": "identification is a great example um it's super helpful to unlock your phone",
    "start": "313520",
    "end": "320000"
  },
  {
    "text": "or devices i use it frequently for that and it has also been used to id",
    "start": "320000",
    "end": "325840"
  },
  {
    "text": "human traffic victims or people uh that are actually",
    "start": "325840",
    "end": "331520"
  },
  {
    "text": "criminally minded etc but and at the same time it can be used",
    "start": "331520",
    "end": "337520"
  },
  {
    "text": "for person tracking it can lead to a lack of anonymity a denial of entry or service for different individuals",
    "start": "337520",
    "end": "344880"
  },
  {
    "text": "etc so there's both of these coins so what do you do about",
    "start": "344880",
    "end": "350840"
  },
  {
    "text": "this um you should ask you know is my project",
    "start": "350840",
    "end": "356400"
  },
  {
    "text": "ethical and one way that we go about that uh at salesforce on a regular basis and",
    "start": "356400",
    "end": "363199"
  },
  {
    "text": "many others also do is to conduct like a consequence scanning brainstorm uh this",
    "start": "363199",
    "end": "368560"
  },
  {
    "text": "comes from.everyone.org and there's there's varieties of uh",
    "start": "368560",
    "end": "374400"
  },
  {
    "text": "types like this but basically what you're asking is what are all of the",
    "start": "374400",
    "end": "379520"
  },
  {
    "text": "intended and unintended consequence both positive and negative and for those that are negative how can",
    "start": "379520",
    "end": "385520"
  },
  {
    "text": "i avoid this this risk basically so i'm going to point i'm going to like",
    "start": "385520",
    "end": "392080"
  },
  {
    "start": "389000",
    "end": "463000"
  },
  {
    "text": "walk you through a thought experiment let's say we have a factory where we're creating cars or",
    "start": "392080",
    "end": "398960"
  },
  {
    "text": "something and the the manufacturing company is considering installing pose estimates",
    "start": "398960",
    "end": "404720"
  },
  {
    "text": "pose estimation software in their plans and they're doing this because they want to",
    "start": "404720",
    "end": "409919"
  },
  {
    "text": "be able to detect if there's going to be a collision between a person and the associated machinery so like you",
    "start": "409919",
    "end": "416479"
  },
  {
    "text": "know these cars go forward stuff can drop down there's a lot of moving parts and pieces",
    "start": "416479",
    "end": "423280"
  },
  {
    "text": "and this could detect fatigue which you know together reduces the",
    "start": "423280",
    "end": "429039"
  },
  {
    "text": "number of accidents by prompting you know an additional break maybe it",
    "start": "429039",
    "end": "434479"
  },
  {
    "text": "tells the plant owner hey there's been a lot of like potential incidents or incidents in this area maybe we want to rejigger it",
    "start": "434479",
    "end": "442160"
  },
  {
    "text": "at the same time as you know like preventing these accidents if installed all employees would be",
    "start": "442160",
    "end": "447759"
  },
  {
    "text": "monitored for the duration of their shifts and you know the the plan owners have decided yeah we should",
    "start": "447759",
    "end": "454160"
  },
  {
    "text": "delete data after 72 hours except for if an incident arose but there still could be some issues here so",
    "start": "454160",
    "end": "462639"
  },
  {
    "text": "what uh i've done here with with a couple of others in my group is ask you",
    "start": "462639",
    "end": "468639"
  },
  {
    "start": "463000",
    "end": "1080000"
  },
  {
    "text": "know like what are some of the intended positive and negative consequences so basically on the positive side we could have fewer",
    "start": "468639",
    "end": "475280"
  },
  {
    "text": "accidents on the site lower worker fatigue you know company has to pay lower workman's compensation overall so",
    "start": "475280",
    "end": "482479"
  },
  {
    "text": "here i'm looking at both the positive consequences to um an individual you",
    "start": "482479",
    "end": "488080"
  },
  {
    "text": "know like no one well very few people would actually want to get hurt on site i probably can't say no",
    "start": "488080",
    "end": "494319"
  },
  {
    "text": "one because there might be somebody um and the company also benefits",
    "start": "494319",
    "end": "499919"
  },
  {
    "text": "but you know some of the negatives is that now our employees are under constant surveillance",
    "start": "499919",
    "end": "505360"
  },
  {
    "text": "and there is a cost to install and maintain the ai system",
    "start": "505360",
    "end": "510400"
  },
  {
    "text": "and then once i like go through some of the intended consequences i want to think about some of these unintended",
    "start": "510400",
    "end": "515518"
  },
  {
    "text": "ones so these are you know like the secondary consequences that might occur if",
    "start": "515519",
    "end": "521440"
  },
  {
    "text": "i didn't initially think about them and some of the ways that you can come around to this is talking to a lot of",
    "start": "521440",
    "end": "528480"
  },
  {
    "text": "people that's you know 90 of my job even though i'm a data scientist is",
    "start": "528480",
    "end": "534399"
  },
  {
    "text": "talking to people trying to figure out like what is this system actually doing talking to the stakeholders trying to",
    "start": "534399",
    "end": "541200"
  },
  {
    "text": "figure out like okay what are you concerned about what are you excited about with this system and then you know",
    "start": "541200",
    "end": "546720"
  },
  {
    "text": "having a discussion so one of the positives is you could have higher worker diligence and",
    "start": "546720",
    "end": "552720"
  },
  {
    "text": "productivity um because the people know that they are being monitored",
    "start": "552720",
    "end": "558080"
  },
  {
    "text": "or simply because they're excited about the system etc",
    "start": "558080",
    "end": "563360"
  },
  {
    "text": "but a negative is that you know like the um workers might not have any say in this and so they could lose their job if",
    "start": "563360",
    "end": "570240"
  },
  {
    "text": "they're not comfortable with the technology if you know the company says thou shalt agree or leave",
    "start": "570240",
    "end": "577600"
  },
  {
    "text": "that could have some very negative repercussions on the workers it could be easily misused by estimating",
    "start": "577600",
    "end": "584160"
  },
  {
    "text": "worker productivity and salaries could be impacted um but this we have seen in uh",
    "start": "584160",
    "end": "591680"
  },
  {
    "text": "in shops especially um workshops especially in china or we've heard anecdotally i don't know",
    "start": "591680",
    "end": "597920"
  },
  {
    "text": "if we can prove it quote unquote but uh also the system might not work as accurately for all um video",
    "start": "597920",
    "end": "606240"
  },
  {
    "text": "systems import so anything around the computer vision",
    "start": "606240",
    "end": "611360"
  },
  {
    "text": "arena works best for those with lighter skin tones than those with darker skin",
    "start": "611360",
    "end": "616399"
  },
  {
    "text": "tones in general and also if you have um if you're",
    "start": "616399",
    "end": "621519"
  },
  {
    "text": "you know disabled in any way shape or form then it might not work as well for you especially with pose estimation",
    "start": "621519",
    "end": "626880"
  },
  {
    "text": "software and then another one you have to think about is hackers could access this video",
    "start": "626880",
    "end": "632560"
  },
  {
    "text": "stream somehow you know it's it is not out of the realm of possibilities",
    "start": "632560",
    "end": "639360"
  },
  {
    "text": "so you start to think about this and and you you go through and try to figure out um",
    "start": "639360",
    "end": "645120"
  },
  {
    "text": "how likely is it going to be to happen how many users are going to be impacted",
    "start": "645120",
    "end": "650959"
  },
  {
    "text": "the frequency that it could happen and basically these other types of like con risk",
    "start": "650959",
    "end": "656640"
  },
  {
    "text": "um forecasting to try to figure out you know like which one should i focus on",
    "start": "656640",
    "end": "663519"
  },
  {
    "text": "most to rate them and then looking from there to say okay",
    "start": "663519",
    "end": "668880"
  },
  {
    "text": "well what are some strategies that we could mitigate the negative consequence so one of the biggest one is acquire work",
    "start": "668880",
    "end": "675200"
  },
  {
    "text": "consent and input um make sure that it doesn't recognize people uh maybe it blurs out the faces",
    "start": "675200",
    "end": "681440"
  },
  {
    "text": "of people it alerts the station that a break should happen soon instead of an individual or",
    "start": "681440",
    "end": "687040"
  },
  {
    "text": "something doesn't log any information on worker fatigue uh different things like this you know can",
    "start": "687040",
    "end": "693200"
  },
  {
    "text": "help um mitigate the risks uh",
    "start": "693200",
    "end": "699040"
  },
  {
    "text": "so yeah that's basically like what you can do um and this sort of addresses",
    "start": "699040",
    "end": "706320"
  },
  {
    "text": "both like overall like ethics as well as it it starts to talk about like human rights",
    "start": "706320",
    "end": "712240"
  },
  {
    "text": "um but yeah it's a really interesting exercise to go through and i",
    "start": "712240",
    "end": "717920"
  },
  {
    "text": "i highly encourage it okay i am going to move on to bias but i",
    "start": "717920",
    "end": "724079"
  },
  {
    "text": "might pause for a minute to see if there are questions and i'll go and i actually",
    "start": "724079",
    "end": "730079"
  },
  {
    "text": "have a quick question um so this this checklist procedure um seems like a really great idea and i was just",
    "start": "730079",
    "end": "735600"
  },
  {
    "text": "wondering like is this something at salesforce that that is done widely like on any of the the projects and then is",
    "start": "735600",
    "end": "743040"
  },
  {
    "text": "it facilitated by someone who has experience in it or is it something that like teams just kind of conduct on their own as a norm",
    "start": "743040",
    "end": "749680"
  },
  {
    "text": "for uh yeah no that's a great question um it is done regularly and it's done for a",
    "start": "749680",
    "end": "757440"
  },
  {
    "text": "variety of projects whether it is an ai project or just a new feature",
    "start": "757440",
    "end": "763120"
  },
  {
    "text": "which could be ml or rule based or whatever and we do train",
    "start": "763120",
    "end": "768480"
  },
  {
    "text": "moderators to help guide teams and i've i have found it useful",
    "start": "768480",
    "end": "774480"
  },
  {
    "text": "to sit in the room for a couple of them just to get the sense and feel of how",
    "start": "774480",
    "end": "779519"
  },
  {
    "text": "how these types of workshops are done um but you know the",
    "start": "779519",
    "end": "785760"
  },
  {
    "text": "the way that it's structured and you know like dot everyone.org did a really",
    "start": "785760",
    "end": "791360"
  },
  {
    "text": "lovely job of creating enough material to support somebody um in",
    "start": "791360",
    "end": "798320"
  },
  {
    "text": "in this type of endeavor the other thing i would definitely say with it is like",
    "start": "798320",
    "end": "804560"
  },
  {
    "text": "there can be a ton of imposter syndrome around this like if you're not a trained ethicist or whatever then",
    "start": "804560",
    "end": "812160"
  },
  {
    "text": "you might be saying oh i can't moderate one of these but you know just by bringing up the questions and asking",
    "start": "812160",
    "end": "819199"
  },
  {
    "text": "you know is this project okay to do and getting people's input um makes you",
    "start": "819199",
    "end": "825440"
  },
  {
    "text": "like an ethic like an ml or an ai ethicist or whatever you want to say um",
    "start": "825440",
    "end": "832160"
  },
  {
    "text": "so so yeah yeah anyone can do it long story short",
    "start": "832160",
    "end": "838959"
  },
  {
    "text": "yeah hello a question over here um hello um i guess i don't know exactly how to",
    "start": "840839",
    "end": "847279"
  },
  {
    "text": "phrase it but how about this like it seems to me like a lot of the problem here is that if there is more information available for a decision",
    "start": "847279",
    "end": "854000"
  },
  {
    "text": "making system um we can't guarantee that it won't be misused and it's kind of similar to like other",
    "start": "854000",
    "end": "860079"
  },
  {
    "text": "things like so like you were saying um if you're monitoring people like a hacker can see it but if you're",
    "start": "860079",
    "end": "865519"
  },
  {
    "text": "monitoring people you can't prevent management in five years from doing something exploitative yeah kind of in",
    "start": "865519",
    "end": "871839"
  },
  {
    "text": "the same way where like i don't know if if people are carrying firearms you can't prevent that from firearms from ending up in the wrong hands yeah is",
    "start": "871839",
    "end": "878560"
  },
  {
    "text": "there room for having access to more information but protecting it",
    "start": "878560",
    "end": "885440"
  },
  {
    "text": "and guaranteeing that like good agents are using it or is that kind of like is this the fundamental issue",
    "start": "885440",
    "end": "891519"
  },
  {
    "text": "yeah no that's a great question and i do think it is a fundamental issue and",
    "start": "891519",
    "end": "897040"
  },
  {
    "text": "there are things that you could do so",
    "start": "897040",
    "end": "903600"
  },
  {
    "text": "you can make sure that when you are you know loading in the video before your data is even stored in any way",
    "start": "903600",
    "end": "910720"
  },
  {
    "text": "shape or form that you have like a face blurring or a face blocking type of algorithm so that you",
    "start": "910720",
    "end": "916880"
  },
  {
    "text": "have um protected some of the privacy you could have very strong",
    "start": "916880",
    "end": "923199"
  },
  {
    "text": "access control list type of scenarios so that you are ensuring that only those",
    "start": "923199",
    "end": "929600"
  },
  {
    "text": "who absolutely need to get this data get it um and then you could also",
    "start": "929600",
    "end": "936160"
  },
  {
    "text": "get into the realm of like legality so basically",
    "start": "936160",
    "end": "942480"
  },
  {
    "text": "uh signing having like the lawyers of your company draft and then have you sign",
    "start": "942480",
    "end": "948959"
  },
  {
    "text": "something saying like this data shall only be used for these types of things and basically making a contract with",
    "start": "948959",
    "end": "954079"
  },
  {
    "text": "yourself that's legally binding that's really strong um and",
    "start": "954079",
    "end": "959199"
  },
  {
    "text": "makes companies nervous sometimes but you can do stuff like that too",
    "start": "959199",
    "end": "965680"
  },
  {
    "text": "but yeah it's it's hard because a lot of times when the data is out there um like i'm a data",
    "start": "965839",
    "end": "972320"
  },
  {
    "text": "scientist and i love data and i love new data and i love glomming on data together because it's",
    "start": "972320",
    "end": "978320"
  },
  {
    "text": "it's fun and interesting and tells me things that i didn't know beforehand and tells like whoever i'm helping out",
    "start": "978320",
    "end": "985759"
  },
  {
    "text": "new things but like the awareness part of it also has to be there",
    "start": "985759",
    "end": "991040"
  },
  {
    "text": "one thing i'll talk about later are model cards um and data cards and so those also go into like",
    "start": "991040",
    "end": "996880"
  },
  {
    "text": "um aspects of the intended use of data sets and models",
    "start": "996880",
    "end": "1002240"
  },
  {
    "text": "and unintended etc so that that type of communication is really helpful too",
    "start": "1002240",
    "end": "1008079"
  },
  {
    "text": "thank you now thank you for your question hi um i was wondering at sale for",
    "start": "1008079",
    "end": "1015120"
  },
  {
    "text": "salesforce in particular as a b2b company um how much of this is like weighing your own products you're",
    "start": "1015120",
    "end": "1020160"
  },
  {
    "text": "building versus considering your clients and who's actually going to be using your products and how is that different",
    "start": "1020160",
    "end": "1027120"
  },
  {
    "text": "yeah so um i guess like there's two things in there like we have a lot of global models",
    "start": "1027120",
    "end": "1033678"
  },
  {
    "text": "and so basically those are models that we train on all of our data and so then we make certain that",
    "start": "1033679",
    "end": "1039918"
  },
  {
    "text": "well a lot of that comes around bias you know all honesty but we make sure that the the usages and the examples that we",
    "start": "1039919",
    "end": "1047600"
  },
  {
    "text": "give to our customers are um are ethical or as ethical as possible",
    "start": "1047600",
    "end": "1054160"
  },
  {
    "text": "and then we also provide them tools in the form of um training modules and i'll",
    "start": "1054160",
    "end": "1061600"
  },
  {
    "text": "talk about our acceptable use policy but well maybe i'll talk about it now but basically like things that we",
    "start": "1061600",
    "end": "1068480"
  },
  {
    "text": "they they may not do with our technology so for instance um this",
    "start": "1068480",
    "end": "1074160"
  },
  {
    "text": "oh oh it's sorry fast the you know face re-identification example that i give",
    "start": "1074160",
    "end": "1080400"
  },
  {
    "start": "1080000",
    "end": "1370000"
  },
  {
    "text": "that's something that our other businesses can't use with our",
    "start": "1080400",
    "end": "1085679"
  },
  {
    "text": "software because we just found it to be to ethically issue have too many issues basically so",
    "start": "1085679",
    "end": "1092000"
  },
  {
    "text": "um but yeah we focus on both i guess thanks yeah totally thanks for your",
    "start": "1092000",
    "end": "1098160"
  },
  {
    "text": "question if i can ask one more question",
    "start": "1098160",
    "end": "1103520"
  },
  {
    "text": "yeah um so i feel like often in this ethically ambiguous domains one argument",
    "start": "1103520",
    "end": "1109200"
  },
  {
    "text": "that that some companies make is uh we don't build the product you know this more extreme way another company is",
    "start": "1109200",
    "end": "1115200"
  },
  {
    "text": "going to build the product take over the market and so right now or maybe also in 10",
    "start": "1115200",
    "end": "1120240"
  },
  {
    "text": "years from now where do you see like you know maybe the law stepping in at some point and how much law do we require or",
    "start": "1120240",
    "end": "1125440"
  },
  {
    "text": "how much can the company still kind of bring from their own sites no for sure that's a great question and",
    "start": "1125440",
    "end": "1131280"
  },
  {
    "text": "it's something that we've actually seen shift a little bit with um gdpr",
    "start": "1131280",
    "end": "1136640"
  },
  {
    "text": "which came out i don't know how many years ago probably four ish years ago by this point in",
    "start": "1136640",
    "end": "1144000"
  },
  {
    "text": "europe which restricts um companies that operated in the eu i believe how they",
    "start": "1144000",
    "end": "1150960"
  },
  {
    "text": "handle data um and so one of the things i think that was",
    "start": "1150960",
    "end": "1156240"
  },
  {
    "text": "a side benefit for us even though we don't live there is a lot of a lot more explicit",
    "start": "1156240",
    "end": "1162559"
  },
  {
    "text": "consent around data that websites collect this idea of being the right to be",
    "start": "1162559",
    "end": "1167600"
  },
  {
    "text": "forgotten et cetera but there are also minimal uh",
    "start": "1167600",
    "end": "1173919"
  },
  {
    "text": "types of requirement around transparency and bias testing those are getting more and more um",
    "start": "1173919",
    "end": "1182400"
  },
  {
    "text": "there's more and more regulation that's coming down the pipe in those areas uh",
    "start": "1182400",
    "end": "1187600"
  },
  {
    "text": "but yeah as sort of like a software provider like because we don't own the data there are",
    "start": "1187600",
    "end": "1195679"
  },
  {
    "text": "less things that salesforce is legally compliant",
    "start": "1195679",
    "end": "1200720"
  },
  {
    "text": "to act upon um and i'm not a lawyer so but i talked to a lot of lawyers all the",
    "start": "1200720",
    "end": "1206400"
  },
  {
    "text": "time uh and but at the same time we're like yeah we",
    "start": "1206400",
    "end": "1211600"
  },
  {
    "text": "still need to be responsible we still need to ensure that this is happening um",
    "start": "1211600",
    "end": "1216640"
  },
  {
    "text": "because at some point down the line it's it's not going to just be like the",
    "start": "1216640",
    "end": "1221679"
  },
  {
    "text": "data owners that have these requirements um imposed upon them but",
    "start": "1221679",
    "end": "1226720"
  },
  {
    "text": "but other organizations like ourselves as well so and we're in a weird little gray area where we do",
    "start": "1226720",
    "end": "1232880"
  },
  {
    "text": "create some algorithms too so um it's a little bit of both",
    "start": "1232880",
    "end": "1238080"
  },
  {
    "text": "yeah but hopefully that answers your question yeah thank you",
    "start": "1238080",
    "end": "1244240"
  },
  {
    "text": "alrighty um so i'm going to go into bias now and",
    "start": "1244480",
    "end": "1249760"
  },
  {
    "text": "bias is really interesting and there is it's a term that's like overly used so i wanted to do some definitions here",
    "start": "1249760",
    "end": "1256880"
  },
  {
    "text": "so basically you have some algorithmic bias so your systematic errors in the model's prediction so",
    "start": "1256880",
    "end": "1263600"
  },
  {
    "text": "you probably have seen more than one powerpoint presentation of um looking at like accuracy versus bias",
    "start": "1263600",
    "end": "1271280"
  },
  {
    "text": "with uh either darts on a dartboard or like arrows in a in a target",
    "start": "1271280",
    "end": "1277760"
  },
  {
    "text": "but then you can also have things like societal bias so this is preconceptions of a person or group of people that can",
    "start": "1277760",
    "end": "1284559"
  },
  {
    "text": "lead to systematic advantages or disadvantages um",
    "start": "1284559",
    "end": "1289679"
  },
  {
    "text": "so this could be things around housing loans where uh",
    "start": "1289679",
    "end": "1295440"
  },
  {
    "text": "black and",
    "start": "1295440",
    "end": "1298559"
  },
  {
    "text": "asian and other minority groups have had disadvantages in certain regions or",
    "start": "1300480",
    "end": "1307120"
  },
  {
    "text": "across like the us in particular getting the same types of loans as the caucasian",
    "start": "1307120",
    "end": "1313200"
  },
  {
    "text": "individuals now when you talk about fairness it's basically your systematic bias for a",
    "start": "1313200",
    "end": "1320400"
  },
  {
    "text": "specific subpopulation which mirrors or exacerbates social bias so basically what can happen",
    "start": "1320400",
    "end": "1326320"
  },
  {
    "text": "is if you create like a loan application ai and it's trained on your historically",
    "start": "1326320",
    "end": "1333039"
  },
  {
    "text": "historical data and that's biased then your ai system could be as bad or even worse",
    "start": "1333039",
    "end": "1340880"
  },
  {
    "text": "we've seen in some cases um as as your um historical bias",
    "start": "1340880",
    "end": "1348159"
  },
  {
    "text": "and i talk about subpopulations here it's these these are basically groups of protected class variables which could be",
    "start": "1348159",
    "end": "1355120"
  },
  {
    "text": "race gender age uh or other variables of concern like socio-economic class some",
    "start": "1355120",
    "end": "1361679"
  },
  {
    "text": "of this comes from the partnership on ai and they're a great resource so um",
    "start": "1361679",
    "end": "1366880"
  },
  {
    "text": "yeah one of the really uh important aspects to note though is that",
    "start": "1366880",
    "end": "1373760"
  },
  {
    "start": "1370000",
    "end": "1507000"
  },
  {
    "text": "not everyone has the same ethical principles and our conversation in what is fair and what is unfair",
    "start": "1373760",
    "end": "1380880"
  },
  {
    "text": "is ongoing it's complex it's messy and it can be really heated and then it gets",
    "start": "1380880",
    "end": "1386000"
  },
  {
    "text": "even more complex we are in practice uh this is a image from",
    "start": "1386000",
    "end": "1392159"
  },
  {
    "text": "the good place which is a glorious and hilarious show by the way but basically",
    "start": "1392159",
    "end": "1397440"
  },
  {
    "text": "in here um chidiana gonier who is an ethics professor is put into a real life trolley problem",
    "start": "1397440",
    "end": "1405520"
  },
  {
    "text": "and the the trolley problem has a lot of different variants but basically",
    "start": "1405520",
    "end": "1411200"
  },
  {
    "text": "it it asks the questions like you are a bystander on the road",
    "start": "1411200",
    "end": "1416640"
  },
  {
    "text": "and a trolley is coming down the tracks now you can pull this lever and",
    "start": "1416640",
    "end": "1423679"
  },
  {
    "text": "switch the the trolley from one track to the other if you don't pull the lever then one",
    "start": "1423679",
    "end": "1428720"
  },
  {
    "text": "person or no five people will be killed um because they're on the track and they",
    "start": "1428720",
    "end": "1434400"
  },
  {
    "text": "can't get off if you do pull the lever then one person will be killed so what what do you do it's it's basically",
    "start": "1434400",
    "end": "1442159"
  },
  {
    "text": "a question of objectives and like uh would you actually intervene etc and",
    "start": "1442159",
    "end": "1448080"
  },
  {
    "text": "there's there's a large number of variants but you know like",
    "start": "1448080",
    "end": "1453200"
  },
  {
    "text": "in the real world it's like you can answer this in a classroom and you might get a different answer than if you're",
    "start": "1453200",
    "end": "1459760"
  },
  {
    "text": "actually standing there on the street",
    "start": "1459760",
    "end": "1464080"
  },
  {
    "text": "and ai can be biased in like a lot of ways basically it's you know",
    "start": "1464960",
    "end": "1471039"
  },
  {
    "text": "bias can be all the way down sort of like turtles are all the way down but",
    "start": "1471039",
    "end": "1476400"
  },
  {
    "text": "who gets access to the benefits and service um what is the purpose of the ai you know like who's going to be impacted",
    "start": "1476400",
    "end": "1483120"
  },
  {
    "text": "your training data is a big one so the representation of everyone",
    "start": "1483120",
    "end": "1488559"
  },
  {
    "text": "impacted your historical bias and then uh your model too if",
    "start": "1488559",
    "end": "1493679"
  },
  {
    "text": "you include uh subpopulation sensitive data in your",
    "start": "1493679",
    "end": "1498799"
  },
  {
    "text": "model then it could exacerbate it in a way then if you um leave it out",
    "start": "1498799",
    "end": "1505520"
  },
  {
    "text": "so this diagram sort of goes into",
    "start": "1505520",
    "end": "1510720"
  },
  {
    "start": "1507000",
    "end": "1684000"
  },
  {
    "text": "a little bit more data representation of our sources of bias um and i went",
    "start": "1510720",
    "end": "1516799"
  },
  {
    "text": "through them but uh this suresh and guitar paper is is helpful and if i haven't already",
    "start": "1516799",
    "end": "1523279"
  },
  {
    "text": "given y'all a list of references i'll make sure to do that so you have it",
    "start": "1523279",
    "end": "1529279"
  },
  {
    "text": "but yeah bias can get and everywhere and what you want to do is is check so",
    "start": "1529279",
    "end": "1535840"
  },
  {
    "text": "you know check to see what are some historical biases that could be occurring when i'm designing the system",
    "start": "1535840",
    "end": "1543200"
  },
  {
    "text": "how am i selecting my data if i'm just scraping off of twitter or off of facebook then who are the users of those",
    "start": "1543200",
    "end": "1550240"
  },
  {
    "text": "systems and does that represent all the people that i'm wanting to talk to or is it not",
    "start": "1550240",
    "end": "1556480"
  },
  {
    "text": "representative quick question um we're wondering if you guys actually",
    "start": "1556480",
    "end": "1562080"
  },
  {
    "text": "bake any of the bias testing into whatever cicd pipelines y'all have",
    "start": "1562080",
    "end": "1567440"
  },
  {
    "text": "it is in some places but that's that's also my job is to make it into more of the places",
    "start": "1567440",
    "end": "1572720"
  },
  {
    "text": "so we have a tool called einstein discovery and that one has baked in um a lot of really",
    "start": "1572720",
    "end": "1580799"
  },
  {
    "text": "good tools for the end user one of the really interesting things there is that it's basically like um",
    "start": "1580799",
    "end": "1587679"
  },
  {
    "text": "a logistic regression fairly simple type of algorithm well i shouldn't say",
    "start": "1587679",
    "end": "1593200"
  },
  {
    "text": "simple it there's a lot on the back end but it helps step a user through to make",
    "start": "1593200",
    "end": "1599919"
  },
  {
    "text": "sure that they do have representative data um and then it does some bias checks on the model that it then gives",
    "start": "1599919",
    "end": "1606720"
  },
  {
    "text": "to the to the like data owners so a lot of times like at salesforce we don't actually see any",
    "start": "1606720",
    "end": "1612320"
  },
  {
    "text": "of the data but but yeah we've been systematically going through a lot of our",
    "start": "1612320",
    "end": "1618480"
  },
  {
    "text": "other uh models and trying to check for it too so do you do like unit testing or is it",
    "start": "1618480",
    "end": "1624799"
  },
  {
    "text": "just like um like is it just going down a checklist would you actually run it through like test cases yeah so some of",
    "start": "1624799",
    "end": "1630960"
  },
  {
    "text": "it could be done in unit tests um a lot of the questions that actually of concern that actually",
    "start": "1630960",
    "end": "1638080"
  },
  {
    "text": "crop up more than the final like is this model biased or not come earlier so like doing",
    "start": "1638080",
    "end": "1644240"
  },
  {
    "text": "the consequent scanning workshops looking at uh who is applying the labels",
    "start": "1644240",
    "end": "1650320"
  },
  {
    "text": "how is the data acquired those can't really be unit tested so um instead we use like sort of",
    "start": "1650320",
    "end": "1657600"
  },
  {
    "text": "more of a checklist form right but you could test um model behavior on simulated data",
    "start": "1657600",
    "end": "1663279"
  },
  {
    "text": "yeah no for sure yeah no and that and that's a great idea to do and um",
    "start": "1663279",
    "end": "1669360"
  },
  {
    "text": "there's a lot of really good tools uh is that my next slide it might be my next slide",
    "start": "1669360",
    "end": "1674720"
  },
  {
    "text": "uh no it's not i'm just gonna skip that slide for now and i'll come back to",
    "start": "1674720",
    "end": "1679760"
  },
  {
    "text": "it but uh some of the open source tools that are out there",
    "start": "1679760",
    "end": "1685600"
  },
  {
    "start": "1684000",
    "end": "1717000"
  },
  {
    "text": "are really helpful for exactly what you were uh describing like ibm watson has a",
    "start": "1685600",
    "end": "1691760"
  },
  {
    "text": "fairness 360 and explainability 360 toolkit that can help you look to see",
    "start": "1691760",
    "end": "1698559"
  },
  {
    "text": "you know like are are these models fair based on these various subpopulations but there are",
    "start": "1698559",
    "end": "1704240"
  },
  {
    "text": "many more too so google has quite a few and then these are just a smattering of",
    "start": "1704240",
    "end": "1709520"
  },
  {
    "text": "other github repositories one of the tricky things though too that",
    "start": "1709520",
    "end": "1715120"
  },
  {
    "text": "you have to do with them um i'll go back to this one is that there's like a ridiculous number of definitions of",
    "start": "1715120",
    "end": "1720399"
  },
  {
    "start": "1717000",
    "end": "1774000"
  },
  {
    "text": "fairness and the reason for that is different situations um",
    "start": "1720399",
    "end": "1727120"
  },
  {
    "text": "are harmful in different ways to an end user so you might if you're doing sort of like a",
    "start": "1727120",
    "end": "1733679"
  },
  {
    "text": "hiring or admissions type of model statistical parity might be the",
    "start": "1733679",
    "end": "1738880"
  },
  {
    "text": "definition of fairness that's best because you know it's basically saying that there should be equal prediction",
    "start": "1738880",
    "end": "1744240"
  },
  {
    "text": "rates between certain categories but that might not always be true for other types of models um",
    "start": "1744240",
    "end": "1751600"
  },
  {
    "text": "so you sort of have to think of think and figure out like what is",
    "start": "1751600",
    "end": "1756720"
  },
  {
    "text": "the definition of fairness and therefore like the um [Music] the end",
    "start": "1756720",
    "end": "1762799"
  },
  {
    "text": "metric that you are checking for in your unit test",
    "start": "1762799",
    "end": "1767840"
  },
  {
    "text": "cool and there is a ton of research in this",
    "start": "1770720",
    "end": "1776640"
  },
  {
    "start": "1774000",
    "end": "1923000"
  },
  {
    "text": "area and it spans all types of data so there's a lot in nlp",
    "start": "1776640",
    "end": "1783679"
  },
  {
    "text": "computer vision as well as categorical types of data and there's a good number",
    "start": "1783679",
    "end": "1789200"
  },
  {
    "text": "of data sets out there now which is helpful so some of these are pseudo benchmark types of data sets um and some",
    "start": "1789200",
    "end": "1797600"
  },
  {
    "text": "of them are built to test specific scenarios and we have",
    "start": "1797600",
    "end": "1803440"
  },
  {
    "text": "an ongoing list thanks to my my manager kathy baxter keeps this blog up to date",
    "start": "1803440",
    "end": "1811039"
  },
  {
    "text": "cool so if you're looking for research things that's a good one",
    "start": "1812960",
    "end": "1817919"
  },
  {
    "text": "okay i'm gonna press onward i don't want to run out of time but",
    "start": "1818159",
    "end": "1823760"
  },
  {
    "text": "the the next topic that i want to chat about is around accountability and basically",
    "start": "1823760",
    "end": "1830720"
  },
  {
    "text": "once you're ready to deploy you want to ask you know who is ultimately responsible for how a system is used and",
    "start": "1830720",
    "end": "1836559"
  },
  {
    "text": "the decision it makes and if there's a mistake that's made how can it get corrected",
    "start": "1836559",
    "end": "1842399"
  },
  {
    "text": "how can a user appeal decisions that were made by the ai system in this appeal decisions that's another",
    "start": "1842399",
    "end": "1849840"
  },
  {
    "text": "regulation that is has been enacted in the gdpr",
    "start": "1849840",
    "end": "1855440"
  },
  {
    "text": "there's a little bit less csmds research in this area but it's",
    "start": "1855440",
    "end": "1861120"
  },
  {
    "text": "it should be baked into every type of project that you do um",
    "start": "1861120",
    "end": "1866399"
  },
  {
    "text": "and to sort of highlight this i'm going to go through a case study so sharks you know",
    "start": "1866399",
    "end": "1872640"
  },
  {
    "text": "uh basically they're changing where they swim and breed because of climate change and",
    "start": "1872640",
    "end": "1878799"
  },
  {
    "text": "so what we wanted to do is um there's a benioff",
    "start": "1878799",
    "end": "1884240"
  },
  {
    "text": "ocean institute it's founded and run or something by our",
    "start": "1884240",
    "end": "1890080"
  },
  {
    "text": "ceo mark benioff and they came to us with the question is like how can we study sharks and protect them while also",
    "start": "1890080",
    "end": "1897200"
  },
  {
    "text": "helping humans and sharks safely share the ocean basically we want less",
    "start": "1897200",
    "end": "1902720"
  },
  {
    "text": "shark human incidences on beaches so mj",
    "start": "1902720",
    "end": "1908559"
  },
  {
    "text": "here started this idea of shark eye so basically use ai to detect great white",
    "start": "1908559",
    "end": "1915120"
  },
  {
    "text": "sharks to learn about their biology and help people share the ocean with marine wildlife",
    "start": "1915120",
    "end": "1921039"
  },
  {
    "text": "and how accountability comes into place here is you know figuring out where a human",
    "start": "1921039",
    "end": "1927440"
  },
  {
    "start": "1923000",
    "end": "2010000"
  },
  {
    "text": "should be in this process so a dispatcher can define territories",
    "start": "1927440",
    "end": "1933120"
  },
  {
    "text": "and the skills that are required to collect data um oversee",
    "start": "1933120",
    "end": "1938720"
  },
  {
    "text": "that type of work etc and you know there's data collectors so uh",
    "start": "1938720",
    "end": "1944399"
  },
  {
    "text": "professional drone pilots who are assigned territories and scheduled to",
    "start": "1944399",
    "end": "1949519"
  },
  {
    "text": "routinely survey areas of beaches sounds like sort of a lovely job",
    "start": "1949519",
    "end": "1955679"
  },
  {
    "text": "to have um and then you know like how do we even make sure that you know like the",
    "start": "1955679",
    "end": "1962720"
  },
  {
    "text": "final ui that the drone dispatcher or the um expert at the end of the day is able",
    "start": "1962720",
    "end": "1969679"
  },
  {
    "text": "to look at this video and say okay like do i trust it or not because ai is going to make mistakes you know ai is not",
    "start": "1969679",
    "end": "1976399"
  },
  {
    "text": "infallible so by showing you know like the bounding box the prediction and um a percentage",
    "start": "1976399",
    "end": "1983600"
  },
  {
    "text": "you're able to really quickly convey a lot of information because sometimes it'll capture kelp as a shark and that's",
    "start": "1983600",
    "end": "1990640"
  },
  {
    "text": "not correct but you know it sort of looks like a shark in some instances the other thing it's not doing is making",
    "start": "1990640",
    "end": "1998080"
  },
  {
    "text": "any predictions about the person on the paddleboard so it's not this model",
    "start": "1998080",
    "end": "2003200"
  },
  {
    "text": "wasn't trained on people just on sharks so again trying to like preserve privacy",
    "start": "2003200",
    "end": "2009760"
  },
  {
    "text": "then the last um group of people that are included are safety",
    "start": "2009760",
    "end": "2015840"
  },
  {
    "start": "2010000",
    "end": "2030000"
  },
  {
    "text": "workers um and these are dispatched when positive sightings of sharks are made to",
    "start": "2015840",
    "end": "2020960"
  },
  {
    "text": "ensure people are safe you know saying hey the beach swim beach is closed there's a shark in the area et cetera",
    "start": "2020960",
    "end": "2028639"
  },
  {
    "start": "2030000",
    "end": "2091000"
  },
  {
    "text": "okay so some of the questions we asked um is like how can this be used with",
    "start": "2030320",
    "end": "2035600"
  },
  {
    "text": "either malice or alarming stupidity and i like the phrasing of that because yeah you know",
    "start": "2035600",
    "end": "2041519"
  },
  {
    "text": "it could be done on purpose but it could also be done on terrible accident who could be harmed so this is both like",
    "start": "2041519",
    "end": "2048240"
  },
  {
    "text": "the people on the beach the sharks themselves um other wildlife like the seagulls",
    "start": "2048240",
    "end": "2053839"
  },
  {
    "text": "there's there's a lot of stuff that goes on on the beach do we have enough perspective to understand the risks and if not",
    "start": "2053839",
    "end": "2060638"
  },
  {
    "text": "who can we bring in to supplement it so this is where we brought in the benny off ocean institute we aren't shark",
    "start": "2060639",
    "end": "2065919"
  },
  {
    "text": "experts we're like data scientists it's like yeah if you give me enough data of",
    "start": "2065919",
    "end": "2071040"
  },
  {
    "text": "like oceans and beaches with sharks i can totally build you a classifier that can predict them um and it'll be fun and",
    "start": "2071040",
    "end": "2078000"
  },
  {
    "text": "it'll be glorious but i want to make sure that i have enough information like what else could be in the water",
    "start": "2078000",
    "end": "2085358"
  },
  {
    "text": "how can we ensure privacy and yeah what if there's drones everywhere",
    "start": "2085359",
    "end": "2090480"
  },
  {
    "text": "so some of the considerations that we made to like mitigate this this is basically going through that consequence scanning",
    "start": "2090480",
    "end": "2096240"
  },
  {
    "start": "2091000",
    "end": "2186000"
  },
  {
    "text": "workshop again is you know we're going to begin our recordings over the water only because that's the area",
    "start": "2096240",
    "end": "2102720"
  },
  {
    "text": "of interest and the ai isn't trained on people we have certified drone",
    "start": "2102720",
    "end": "2108320"
  },
  {
    "text": "operators employed by the maniav ocean institute so they're the ones that say okay yeah these people can fly on a",
    "start": "2108320",
    "end": "2115280"
  },
  {
    "text": "beach and you know there are high-risk beaches are scheduled and",
    "start": "2115280",
    "end": "2122480"
  },
  {
    "text": "making sure that there isn't like too many people on the beach flying drones at the same time",
    "start": "2122480",
    "end": "2130040"
  },
  {
    "text": "cool another part way to um have accountability is",
    "start": "2130160",
    "end": "2137440"
  },
  {
    "text": "making sure that you are transparent and that you have enough explainability",
    "start": "2137440",
    "end": "2142640"
  },
  {
    "text": "and one of the biggest things i've learned in this area is that just because i understand what's going",
    "start": "2142640",
    "end": "2148160"
  },
  {
    "text": "on doesn't mean that everyone else will and there's like differing levels of specif specificity technical details um",
    "start": "2148160",
    "end": "2156880"
  },
  {
    "text": "that are necessary in your documentation so you might need to have something very broad for interested public versus like",
    "start": "2156880",
    "end": "2164640"
  },
  {
    "text": "your users external collaborators or auditors and the most information to your internal",
    "start": "2164640",
    "end": "2170640"
  },
  {
    "text": "team members like if i submit a pr request or if i submit a feature request i want",
    "start": "2170640",
    "end": "2177760"
  },
  {
    "text": "to provide as much information as i can",
    "start": "2177760",
    "end": "2183119"
  },
  {
    "text": "so this is where uh model cards come in and this is",
    "start": "2184400",
    "end": "2189599"
  },
  {
    "start": "2186000",
    "end": "2266000"
  },
  {
    "text": "with uh accountability where accountability also comes into play and the reason i really like model cards",
    "start": "2189599",
    "end": "2197440"
  },
  {
    "text": "is that they're very high level overview of",
    "start": "2197440",
    "end": "2202480"
  },
  {
    "text": "when a model was trained why i was trained how it was trained um",
    "start": "2202480",
    "end": "2208079"
  },
  {
    "text": "some of its ethical considerations where it should be used where it shouldn't be used so in you know like the great white",
    "start": "2208079",
    "end": "2215200"
  },
  {
    "text": "shark the shark eye example i want to say this is trained on great white sharks or this is trained on tiger",
    "start": "2215200",
    "end": "2221440"
  },
  {
    "text": "sharks and hammerhead sharks and so then if um a researcher from another",
    "start": "2221440",
    "end": "2226720"
  },
  {
    "text": "institute is using my model and they have",
    "start": "2226720",
    "end": "2232559"
  },
  {
    "text": "more sharks whale sharks then they would know that they were able",
    "start": "2233359",
    "end": "2238640"
  },
  {
    "text": "or weren't able to use my model um in which we retrain it or if they saw that the model date was from like five years",
    "start": "2238640",
    "end": "2244960"
  },
  {
    "text": "ago and they knew that the technology the cameras on their systems were much better they would also change it",
    "start": "2244960",
    "end": "2252400"
  },
  {
    "text": "cool any questions on accountability and model cards because i'm switching just a",
    "start": "2253599",
    "end": "2259440"
  },
  {
    "text": "tiny bit",
    "start": "2259440",
    "end": "2262078"
  },
  {
    "text": "another um i had one mark for transparency there is",
    "start": "2265119",
    "end": "2271040"
  },
  {
    "start": "2266000",
    "end": "2533000"
  },
  {
    "text": "a lot of transparency research and i it's fascinating so especially when you get into neural",
    "start": "2271040",
    "end": "2277599"
  },
  {
    "text": "networks or these really complex ai algorithms",
    "start": "2277599",
    "end": "2283040"
  },
  {
    "text": "you need to start to figure out okay well what in the world is going on and there are some uh",
    "start": "2283040",
    "end": "2288720"
  },
  {
    "text": "applications for feature importances like lime and shop as well as attention layers this uh",
    "start": "2288720",
    "end": "2296160"
  },
  {
    "text": "website from distil is utilizing attention layers i think it seems like",
    "start": "2296160",
    "end": "2302880"
  },
  {
    "text": "and basically it's highlighting where in an image it thinks there's a labrador or",
    "start": "2302880",
    "end": "2308160"
  },
  {
    "text": "a tiger cat the other really interesting thing i feel like with transparency is that",
    "start": "2308160",
    "end": "2314560"
  },
  {
    "text": "depending on how much transparency you need it might sort of direct you to",
    "start": "2314560",
    "end": "2320880"
  },
  {
    "text": "a different type of model so for instance cancer research there are a lot of models that are starting to be able",
    "start": "2320880",
    "end": "2327599"
  },
  {
    "text": "to identify tumors or other types of diseases in the body and if you",
    "start": "2327599",
    "end": "2333119"
  },
  {
    "text": "have a simple classifier that says okay in this image i see cancer or i don't see cancer well you",
    "start": "2333119",
    "end": "2340400"
  },
  {
    "text": "know like that's sort of useful but if you instead have like um a segmentation model where you're saying",
    "start": "2340400",
    "end": "2347599"
  },
  {
    "text": "in this specific image of the picture i see cancer or i see a problem then that",
    "start": "2347599",
    "end": "2354000"
  },
  {
    "text": "can really help a a doctor or a technician much more to",
    "start": "2354000",
    "end": "2359760"
  },
  {
    "text": "be able to clue in and say yeah i agree with the aai or i don't think the ai is correct",
    "start": "2359760",
    "end": "2365599"
  },
  {
    "text": "so basically like helping ai be a force multiplier for you can",
    "start": "2365599",
    "end": "2370720"
  },
  {
    "text": "can really change what type of model you want to deploy",
    "start": "2370720",
    "end": "2375440"
  },
  {
    "text": "and then i know there are a number of students as well as um other per you",
    "start": "2376960",
    "end": "2383119"
  },
  {
    "text": "know professionals in here and yeah ethics can be your career so there are more and more startup companies uh",
    "start": "2383119",
    "end": "2390560"
  },
  {
    "text": "in both that are doing this this is a smattering of a few of them and there's also lots",
    "start": "2390560",
    "end": "2396800"
  },
  {
    "text": "of large companies that have ethicists because you know we as we've seen ai be utilized",
    "start": "2396800",
    "end": "2404480"
  },
  {
    "text": "in so many different ways we have really realized that you know we need to do it",
    "start": "2404480",
    "end": "2410240"
  },
  {
    "text": "deploy ai in a safe um an ethical manner",
    "start": "2410240",
    "end": "2415359"
  },
  {
    "text": "and then if you know conferences and academia are your thing and you know",
    "start": "2415359",
    "end": "2421520"
  },
  {
    "text": "then this is also really important because it's focused on at conferences more and more so these are a few of them",
    "start": "2421520",
    "end": "2428800"
  },
  {
    "text": "that have highlighted it but um nurips and icml in particular were a",
    "start": "2428800",
    "end": "2435200"
  },
  {
    "text": "couple of the first to require impact statements so basically if you're submitting a paper you need to now say",
    "start": "2435200",
    "end": "2440720"
  },
  {
    "text": "like what are the ethical considerations of of this work",
    "start": "2440720",
    "end": "2446079"
  },
  {
    "text": "and a lot of the things that i pointed out during this can be helpful okay so last couple minutes i",
    "start": "2446079",
    "end": "2455040"
  },
  {
    "text": "want to leave you all with some helpful tools and resources uh ethical ai is a huge realm um so you",
    "start": "2455040",
    "end": "2463839"
  },
  {
    "text": "know there are semester-long courses that deal with this and actually i",
    "start": "2463839",
    "end": "2469280"
  },
  {
    "text": "was given a handful of these by juan carlos niebulles who is also a professor",
    "start": "2469280",
    "end": "2474720"
  },
  {
    "text": "at stanford but one of the tools that i always suggest is dion",
    "start": "2474720",
    "end": "2481520"
  },
  {
    "text": "dion is a basic cheat checklist it's created by driven data they're actually located in",
    "start": "2481520",
    "end": "2487839"
  },
  {
    "text": "the bay and it's a checklist that's jupiter notebook or readme ready so this can increase your",
    "start": "2487839",
    "end": "2494880"
  },
  {
    "text": "accountability and your transparency again but it goes through a bunch of the questions that i raised and didn't raise and you know",
    "start": "2494880",
    "end": "2502160"
  },
  {
    "text": "helped you ask okay if i'm collecting data did they give an",
    "start": "2502160",
    "end": "2507200"
  },
  {
    "text": "informed consent no is there pii exposure what is what about our",
    "start": "2507200",
    "end": "2512560"
  },
  {
    "text": "collection bias etc all through all the way through deployment and this is the entire checklist so it can help you to",
    "start": "2512560",
    "end": "2518880"
  },
  {
    "text": "start to ask the questions that are necessary",
    "start": "2518880",
    "end": "2523839"
  },
  {
    "text": "other tools i talked a little bit already about the 360 toolkit um so i'm",
    "start": "2524800",
    "end": "2529839"
  },
  {
    "text": "going to skip that one and these are some of the stanford class classes that juan carlos",
    "start": "2529839",
    "end": "2536800"
  },
  {
    "start": "2533000",
    "end": "2571000"
  },
  {
    "text": "pointed me out to these are all i guess in cs it looks like",
    "start": "2536800",
    "end": "2542160"
  },
  {
    "text": "but especially things around ethics public policy and technological change",
    "start": "2542160",
    "end": "2548079"
  },
  {
    "text": "things like this will really delve deeper into the history of um ai ethics why",
    "start": "2548079",
    "end": "2555280"
  },
  {
    "text": "it's important what we are doing about it etc and there's very there's",
    "start": "2555280",
    "end": "2561200"
  },
  {
    "text": "more specific ones around nlp and i'm sure in the coming years there would be even",
    "start": "2561200",
    "end": "2567200"
  },
  {
    "text": "more and yeah salesforce also of course has some",
    "start": "2567200",
    "end": "2572480"
  },
  {
    "start": "2571000",
    "end": "3278000"
  },
  {
    "text": "things too the one that i mentioned a little bit earlier is this trailhead so base this is a series",
    "start": "2572480",
    "end": "2580640"
  },
  {
    "text": "of courses that are self-guided and free where you can um",
    "start": "2580640",
    "end": "2586160"
  },
  {
    "text": "learn a little bit more about ethical ai what you can do in your product and when i say product that's the same",
    "start": "2586160",
    "end": "2592319"
  },
  {
    "text": "as if you're doing a research project to me they they can be very analogous",
    "start": "2592319",
    "end": "2598319"
  },
  {
    "text": "but yeah i think that is all of them um",
    "start": "2598960",
    "end": "2605200"
  },
  {
    "text": "definitely happy to take questions and or dive deeper into anything",
    "start": "2605200",
    "end": "2611440"
  },
  {
    "text": "that y'all would like wonderful thank you so much anna that was a fantastic presentation",
    "start": "2611440",
    "end": "2619800"
  },
  {
    "text": "so maybe i'll kick us off with a question so um i think much has been made about explainability and",
    "start": "2623359",
    "end": "2629760"
  },
  {
    "text": "transparency tools um for neural networks and i've seen lots of really cool examples of it",
    "start": "2629760",
    "end": "2635839"
  },
  {
    "text": "working um i wonder to what extent do you see them being kind of",
    "start": "2635839",
    "end": "2641119"
  },
  {
    "text": "practically used when you know like a data scientist is actually creating a model uh do they are",
    "start": "2641119",
    "end": "2647119"
  },
  {
    "text": "they actually do they have real utility there and are people using them frequently",
    "start": "2647119",
    "end": "2652240"
  },
  {
    "text": "that's a great question um i",
    "start": "2652240",
    "end": "2657200"
  },
  {
    "text": "i think that the way that they could be used the best and i have seen this in some",
    "start": "2657280",
    "end": "2663200"
  },
  {
    "text": "instances is looking at failure cases uh but even then",
    "start": "2663200",
    "end": "2670480"
  },
  {
    "text": "the tools aren't as useful as just looking at the images so basically like",
    "start": "2670480",
    "end": "2675920"
  },
  {
    "text": "trying to figure out okay we're if i'm doing a cat versus dog you know",
    "start": "2675920",
    "end": "2681119"
  },
  {
    "text": "classifier or whatever and i look at all my failure cases and i find out that all my chihuahuas are being classified as",
    "start": "2681119",
    "end": "2688079"
  },
  {
    "text": "cats because they're small [Music] then i might look to see okay well",
    "start": "2688079",
    "end": "2694720"
  },
  {
    "text": "what's going on with my training data what's going on with um",
    "start": "2694720",
    "end": "2700319"
  },
  {
    "text": "yeah i had to look at my training data basically in that instance and then be able to say okay well how can i fix this and for that",
    "start": "2700319",
    "end": "2708160"
  },
  {
    "text": "i probably wouldn't need you know any of these fancy schmancy",
    "start": "2708160",
    "end": "2713440"
  },
  {
    "text": "transparency um uh tools because my eyeballs",
    "start": "2713440",
    "end": "2720319"
  },
  {
    "text": "are able to figure it out faster and better than anything that we've built um",
    "start": "2720319",
    "end": "2726560"
  },
  {
    "text": "but in other cases it might be helpful uh yeah",
    "start": "2726560",
    "end": "2734560"
  },
  {
    "text": "i don't know i think a lot of times what i mentioned earlier of using a different type of model so a",
    "start": "2734560",
    "end": "2740880"
  },
  {
    "text": "segmentation model versus just a peer classification model is is more powerful and indicative to like",
    "start": "2740880",
    "end": "2747520"
  },
  {
    "text": "the end user as well as the uh",
    "start": "2747520",
    "end": "2753359"
  },
  {
    "text": "model designer so yeah that's a good question got it thank you yeah",
    "start": "2753359",
    "end": "2760640"
  },
  {
    "text": "yeah there's um a researcher at google being kim and she's been researching this for",
    "start": "2760640",
    "end": "2768560"
  },
  {
    "text": "researching like explainability and in deep learning models quite a bit so she might have a much different take on",
    "start": "2768560",
    "end": "2775440"
  },
  {
    "text": "it than i do but yeah",
    "start": "2775440",
    "end": "2781440"
  },
  {
    "text": "great other questions for anna",
    "start": "2782560",
    "end": "2787160"
  },
  {
    "text": "so i actually have another one um so you know a lot of folks here i think are are researchers as well and a lot of",
    "start": "2790160",
    "end": "2797119"
  },
  {
    "text": "times that the things that we uh build aren't necessarily going to be um",
    "start": "2797119",
    "end": "2803280"
  },
  {
    "text": "kind of deployed very widely uh and you know be actually interfacing with",
    "start": "2803280",
    "end": "2808640"
  },
  {
    "text": "people we will work on like toy problems and stuff like that so i'm just wondering like uh there's probably a continuum of you",
    "start": "2808640",
    "end": "2815760"
  },
  {
    "text": "know how much consideration you maybe need to put into",
    "start": "2815760",
    "end": "2820880"
  },
  {
    "text": "the design i think there should always be some but maybe you know you should be very much more certain when you're actually deploying uh a system and stuff",
    "start": "2820880",
    "end": "2826400"
  },
  {
    "text": "like that i'm just wondering if you have thoughts on like the continuum from kind of academic interests all the way up through like deployment and industry and",
    "start": "2826400",
    "end": "2832880"
  },
  {
    "text": "how maybe we can think about these things yeah totally that's a great question and um",
    "start": "2832880",
    "end": "2838960"
  },
  {
    "text": "i think there can be research which even if it's uh",
    "start": "2838960",
    "end": "2844960"
  },
  {
    "text": "just in academia can be quite harmful uh and this",
    "start": "2844960",
    "end": "2850720"
  },
  {
    "text": "mostly revolves around the ethical use and not well some bias too but",
    "start": "2850720",
    "end": "2856079"
  },
  {
    "text": "some of the more recent examples is uh things like gaydar so basically trying",
    "start": "2856079",
    "end": "2862400"
  },
  {
    "text": "to predict if from a picture if somebody is gay or not gay or even just",
    "start": "2862400",
    "end": "2867520"
  },
  {
    "text": "predicting um yeah it's so bad i saw some cringy faces which i was glad to see um",
    "start": "2867520",
    "end": "2875599"
  },
  {
    "text": "even predicting like uh age or race or uh gender can be really problematic",
    "start": "2875599",
    "end": "2882960"
  },
  {
    "text": "because these are sociological constructs um",
    "start": "2882960",
    "end": "2888079"
  },
  {
    "text": "versus something that can really have ground truth labels and that you know",
    "start": "2888079",
    "end": "2894160"
  },
  {
    "text": "a image can really capture on for a lot of different reasons",
    "start": "2894160",
    "end": "2899760"
  },
  {
    "text": "so like you know avoiding research like that is good for sure",
    "start": "2899760",
    "end": "2905040"
  },
  {
    "text": "um but then if you get into questions of bias you know like there",
    "start": "2905040",
    "end": "2910720"
  },
  {
    "text": "have been instances where uh especially around reinforcement",
    "start": "2910720",
    "end": "2916480"
  },
  {
    "text": "learning you can create images and if i think it was",
    "start": "2916480",
    "end": "2922480"
  },
  {
    "text": "i don't exactly know what i think it was going from like a pixelated image to",
    "start": "2922480",
    "end": "2928240"
  },
  {
    "text": "like a more full image or anyways someone was able to",
    "start": "2928240",
    "end": "2933599"
  },
  {
    "text": "uh use research where the model was you know",
    "start": "2933599",
    "end": "2938960"
  },
  {
    "text": "put on to github etc and then pass in pictures of barack obama as well as",
    "start": "2938960",
    "end": "2944319"
  },
  {
    "text": "other black individuals and lighten the skin so i think it might have been like a",
    "start": "2944319",
    "end": "2950800"
  },
  {
    "text": "glamour filters or yeah filters can have problems too",
    "start": "2950800",
    "end": "2956960"
  },
  {
    "text": "so so yeah there's there's definitely areas where even if it is just research",
    "start": "2956960",
    "end": "2965200"
  },
  {
    "text": "it can harm a community if you are",
    "start": "2965200",
    "end": "2970400"
  },
  {
    "text": "basically playing off of stereotype historical stereotypes which have been",
    "start": "2970400",
    "end": "2975440"
  },
  {
    "text": "harmful in the past um or aren't true for the whole population or you're trying to",
    "start": "2975440",
    "end": "2982400"
  },
  {
    "text": "basically pigeonhole individuals into societal norms that they don't believe",
    "start": "2982400",
    "end": "2987920"
  },
  {
    "text": "in um one of the ones actually that i heard about yesterday i think was",
    "start": "2987920",
    "end": "2994960"
  },
  {
    "text": "an age something that aged people it's like what do i look like when i'm in 50 years or something",
    "start": "2994960",
    "end": "3003040"
  },
  {
    "text": "and it was removing people's tattoos uh maybe because older individuals didn't",
    "start": "3003040",
    "end": "3008800"
  },
  {
    "text": "have as many tattoos unclear but that was also uh harmful to communities where tattoos",
    "start": "3008800",
    "end": "3016319"
  },
  {
    "text": "were part of their heritage and their culture um so yeah",
    "start": "3016319",
    "end": "3022160"
  },
  {
    "text": "yeah it's it's tricky um and nlp has all sorts of",
    "start": "3022160",
    "end": "3028079"
  },
  {
    "text": "biases but um yeah uh well word models they're getting better",
    "start": "3028079",
    "end": "3034400"
  },
  {
    "text": "i think i think we're starting to solve some of those but basically like you just grab things off the web and",
    "start": "3034400",
    "end": "3041520"
  },
  {
    "text": "it's reflected in the work that you do so yeah think about that and basically",
    "start": "3041520",
    "end": "3047359"
  },
  {
    "text": "think about the worst tweet that could be made of your paper and be like okay how do i avoid that for me yeah i like",
    "start": "3047359",
    "end": "3054400"
  },
  {
    "text": "that as a metric yeah",
    "start": "3054400",
    "end": "3057440"
  },
  {
    "text": "yeah i have a quick question so we have a few more minutes so today we have kind of an interface in this conversation",
    "start": "3059680",
    "end": "3065920"
  },
  {
    "text": "right now between kind of academia and industry and i think everybody has definitely a part of this you know uh",
    "start": "3065920",
    "end": "3072400"
  },
  {
    "text": "kind of ethical ai direction but what would you like or what would you think uh would be great to see from academia",
    "start": "3072400",
    "end": "3078480"
  },
  {
    "text": "as like a great result that maybe an industry it's a little bit harder you know to work for this direction",
    "start": "3078480",
    "end": "3084240"
  },
  {
    "text": "yeah that's a great question um",
    "start": "3084240",
    "end": "3088400"
  },
  {
    "text": "you know one of the i don't know",
    "start": "3090240",
    "end": "3094480"
  },
  {
    "text": "oh one of the biggest issues is how to",
    "start": "3096240",
    "end": "3102000"
  },
  {
    "text": "a lot of the bias work in particular requires information on",
    "start": "3102000",
    "end": "3107839"
  },
  {
    "text": "subgroup information so i need to know was this tweet that i'm predicting",
    "start": "3107839",
    "end": "3114079"
  },
  {
    "text": "sentiment on like what it was the age the gender the race the socioeconomic",
    "start": "3114079",
    "end": "3119119"
  },
  {
    "text": "group etc of the person so that i can check to see",
    "start": "3119119",
    "end": "3124240"
  },
  {
    "text": "if my sentiment classifier for instance is biased against these different groups",
    "start": "3124240",
    "end": "3129520"
  },
  {
    "text": "but when i'm thinking about privacy and safety i don't want any of that information",
    "start": "3129520",
    "end": "3135200"
  },
  {
    "text": "because it could be then personally identifiable and i don't really want to store it and there's things i can do to to make",
    "start": "3135200",
    "end": "3142400"
  },
  {
    "text": "sure that that data is safe but you know like how how can i",
    "start": "3142400",
    "end": "3148240"
  },
  {
    "text": "you know like in an aggregate way check for bias",
    "start": "3148240",
    "end": "3154960"
  },
  {
    "text": "when i may not have the metadata of this information or i",
    "start": "3154960",
    "end": "3160079"
  },
  {
    "text": "want to occlude that metadata from anybody else in my organization",
    "start": "3160079",
    "end": "3168480"
  },
  {
    "text": "so if you can solve that problem that would be great but so if i understand that correctly what you think the role",
    "start": "3168480",
    "end": "3174640"
  },
  {
    "text": "of here is to develop new kind of algorithms or mathematical",
    "start": "3174640",
    "end": "3180079"
  },
  {
    "text": "frameworks maybe so to solve a class of problems like the one you describe and this is more i would agree also that",
    "start": "3180079",
    "end": "3186720"
  },
  {
    "text": "this is more of a kind of an academia thing and then the industry would come and kind of put this into the place in the in the world okay yeah",
    "start": "3186720",
    "end": "3193920"
  },
  {
    "text": "sounds sounds great yeah no for sure and and there are other algorithms in terms of transparency or in terms of",
    "start": "3193920",
    "end": "3201920"
  },
  {
    "text": "even though admittedly i sort of said oh no transparency methods aren't useful",
    "start": "3201920",
    "end": "3207040"
  },
  {
    "text": "except for model cards but um yeah there's there's a lot a lot that we",
    "start": "3207040",
    "end": "3213839"
  },
  {
    "text": "can do around that area as well as better mitigation strategies so",
    "start": "3213839",
    "end": "3220000"
  },
  {
    "text": "like ai 360 has a couple of like mitigation algorithms in it which",
    "start": "3220000",
    "end": "3225119"
  },
  {
    "text": "basically reweight training data coming in or things coming out uh",
    "start": "3225119",
    "end": "3231440"
  },
  {
    "text": "but what else can we do is that how can we",
    "start": "3231440",
    "end": "3236480"
  },
  {
    "text": "optimize the accuracy of like the full model",
    "start": "3236480",
    "end": "3241839"
  },
  {
    "text": "while trying to get our quality metrics up because sometimes",
    "start": "3241839",
    "end": "3247119"
  },
  {
    "text": "there's a trade-off you're trying to build build up the predictions of a minority group and that",
    "start": "3247119",
    "end": "3252800"
  },
  {
    "text": "might make the majority group fall a little bit which makes your your end accuracy number fall a bit",
    "start": "3252800",
    "end": "3258559"
  },
  {
    "text": "which makes you know my pms and product owners like freak out a little bit",
    "start": "3258559",
    "end": "3264800"
  },
  {
    "text": "yeah all right so i think we're right at time so let's let's think again again for",
    "start": "3264800",
    "end": "3270079"
  },
  {
    "text": "that wonderful presentation thank you so much for being here",
    "start": "3270079",
    "end": "3275160"
  }
]