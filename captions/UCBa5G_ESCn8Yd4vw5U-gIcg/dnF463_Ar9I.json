[
  {
    "start": "0",
    "end": "5550"
  },
  {
    "text": "OK. Well, welcome back to CS224N. It's welcome back\nfor me to CS224N too,",
    "start": "5550",
    "end": "13410"
  },
  {
    "text": "since I was traveling\nfor a couple of weeks. I hope everything went\nsmoothly in the meantime.",
    "start": "13410",
    "end": "19200"
  },
  {
    "text": "So today, I'm\ndelighted to introduce our first invited\nspeaker, Nathan Lambert.",
    "start": "19200",
    "end": "24780"
  },
  {
    "text": "So Nathan did his\nPhD at UC Berkeley. So you're allowed to\nboo and hiss for that.",
    "start": "24780",
    "end": "31570"
  },
  {
    "text": "[LAUGHTER] But since then, he worked\nfirst for a couple of years",
    "start": "31570",
    "end": "40050"
  },
  {
    "text": "at Hugging Face. And now he's working at Ai2, the\nAllen Institute for Artificial",
    "start": "40050",
    "end": "47280"
  },
  {
    "text": "Intelligence in Seattle. So Nathan comes\nfrom a background",
    "start": "47280",
    "end": "53640"
  },
  {
    "text": "in reinforcement learning. Like quite a few\nother people who are now applying reinforcement\nlearning to language models,",
    "start": "53640",
    "end": "59980"
  },
  {
    "text": "he had an earlier background\napplying reinforcement learning to robots. But it turns out it's more fun\nto do it with language models.",
    "start": "59980",
    "end": "68619"
  },
  {
    "text": "No, it's not. [CHUCKLES] But anyway, I mean,\nhe's been very",
    "start": "68620",
    "end": "73780"
  },
  {
    "text": "influential in both\ndeveloping ideas as how to do post training with RLHF\nand other ideas that have come",
    "start": "73780",
    "end": "82720"
  },
  {
    "text": "since then, including\nDPO that he'll definitely mention in today's talk. And so he's one of\nthe best experts",
    "start": "82720",
    "end": "90910"
  },
  {
    "text": "on the post-training phase of\nlanguage model development, which has just proven,\nas time has passed by,",
    "start": "90910",
    "end": "99049"
  },
  {
    "text": "that more and more of the action\nof the large language model companies is happening not in\nthe initial pretraining language",
    "start": "99050",
    "end": "106450"
  },
  {
    "text": "model training phase but in the\nsubsequent post-training phase. And Nathan will have a lot\nto say about that today.",
    "start": "106450",
    "end": "112430"
  },
  {
    "text": "Thanks a lot for\ncoming to do this. Yeah, thanks for\nthe wonderful intro. You can see my talk\nis life after DPO,",
    "start": "112430",
    "end": "118780"
  },
  {
    "text": "which is a little bit of a\nunclear title, so I apologize about this. But it's trying to capture\nwhat is the moment that we're",
    "start": "118780",
    "end": "125830"
  },
  {
    "text": "at in alignment research. And really, DPO is the\nstory of last year, which",
    "start": "125830",
    "end": "131893"
  },
  {
    "text": "is this paper that came out. And I'll get to the math. And now a lot more people\nare interested in able to do alignment.",
    "start": "131893",
    "end": "137660"
  },
  {
    "text": "And it's building on from there. So it's like, what are we going\nto be interested in after DPO. And a tidbit talking with\nChris that isn't explicitly",
    "start": "137660",
    "end": "145330"
  },
  {
    "text": "in my slides is what\nwe're trying to close and the labs like Meta and\npeople with the amount of data",
    "start": "145330",
    "end": "152200"
  },
  {
    "text": "that they're using for this kind\nof post-training, fine-tuning. There's all these\nwords all defined.",
    "start": "152200",
    "end": "157810"
  },
  {
    "text": "It's so big that the amount of\ndata points that Meta bought in Llama 2 from one\nof these providers",
    "start": "157810",
    "end": "163900"
  },
  {
    "text": "is much more data than all of\nthe data that's been collected on Chatbot Arena from LMSYS. So Chatbot Arena has 800,000\ndata points that have been",
    "start": "163900",
    "end": "171550"
  },
  {
    "text": "collected. And Meta 2's paper\nsays they bought about 1.5 million comparisons. And these are years outdated.",
    "start": "171550",
    "end": "177670"
  },
  {
    "text": "And Chatbot Arena's data is\nthat as of a few weeks ago. So you can only imagine what\nOpenAI, Anthropic, et cetera",
    "start": "177670",
    "end": "184900"
  },
  {
    "text": "are buying at this scale. And this is the kind of reality\nthat we need to adapt to. It's like, what is different.",
    "start": "184900",
    "end": "190170"
  },
  {
    "text": "We don't have that type of\nresource doing research. And what are we going to do. So this lecture is\nsome history on things",
    "start": "190170",
    "end": "197310"
  },
  {
    "text": "that lead up to DPO\nthat I saw that I think are important to remember. And then really,\nwe'll go 0 to 100",
    "start": "197310",
    "end": "204030"
  },
  {
    "text": "and talk about recent\nresearch that we're doing to try to\nanswer this question and define what is happening.",
    "start": "204030",
    "end": "211410"
  },
  {
    "text": "So I'll start with the\nheavily abbreviated history of language models. I won't go through all of this.",
    "start": "211410",
    "end": "216760"
  },
  {
    "text": "There's a bunch of this\nin the class already. This is late in the lecture. I like to start\nwith Claude Shannon.",
    "start": "216760",
    "end": "221790"
  },
  {
    "text": "And then you skip a\nwhole bunch of stuff where this autoregressive loss\nfunction shows a lot of promise.",
    "start": "221790",
    "end": "228340"
  },
  {
    "text": "And this was not fast. You could see how\nmany years that it took to build language\nmodeling as a field here.",
    "start": "228340",
    "end": "234580"
  },
  {
    "text": "And deep learning is\nbrewing in the background of one of many things\nthat went into this.",
    "start": "234580",
    "end": "240069"
  },
  {
    "text": "And then you have these years\nwith 2017, the transformer paper that you hear about. 2018 with GPT-1, ELMo, and\nBERT, these foundational topics",
    "start": "240070",
    "end": "249510"
  },
  {
    "text": "in language processing and\nhow embeddings are created. And then with GPT-2\nand scaling laws",
    "start": "249510",
    "end": "255000"
  },
  {
    "text": "become this kind of\nkey idea that people are looking at and tracking in\nhow these models are improving.",
    "start": "255000",
    "end": "261338"
  },
  {
    "text": "And then in 2020 is\nwhen people really started to wake up to how\nuseful these large-scale trained",
    "start": "261339",
    "end": "267660"
  },
  {
    "text": "language models were. At this time, I wasn't even\na language modeling person. And for a lot of\npeople in AI, this",
    "start": "267660",
    "end": "273520"
  },
  {
    "text": "is when the gravity\nof the situation was starting to suck people in. And there's a lot of\ncadence to these things.",
    "start": "273520",
    "end": "280010"
  },
  {
    "text": "In 2021, we had the\nstochastic parrots paper, which before ChatGPT is\nraising the warnings of what",
    "start": "280010",
    "end": "287142"
  },
  {
    "text": "are we actually putting\ninto these models and what are they learning. Are they actually\nlearning something meaningful from language or\nare they repeating the language",
    "start": "287142",
    "end": "294460"
  },
  {
    "text": "that we have? And those are the kind of\nphilosophical debate depending on where you land,\non what language is,",
    "start": "294460",
    "end": "300620"
  },
  {
    "text": "what these language\nmodels are doing today. But it's important that it\ncame out before ChatGPT. And it's like these\nfoundations of debates of what",
    "start": "300620",
    "end": "307630"
  },
  {
    "text": "language models are doing. End of 2022 is when\nChatGPT actually came out,",
    "start": "307630",
    "end": "312860"
  },
  {
    "text": "which was supposed to\nbe this quiet launch of a demo from OpenAI.",
    "start": "312860",
    "end": "318650"
  },
  {
    "text": "And it has since captured\nthe attention of the world that we have seen.",
    "start": "318650",
    "end": "323930"
  },
  {
    "text": "And the simple question is,\ncan ChatGPT exist without RLHF? I think it's important to\nacknowledge that so much of this",
    "start": "323930",
    "end": "331539"
  },
  {
    "text": "is from pretraining. But at every point\nI've aligned in ChatGPT and then a lot of these\npopular models since then, RLHF",
    "start": "331540",
    "end": "338040"
  },
  {
    "text": "and these human-related or\nother fine-tuning technologies seem to be necessary\nbut not sufficient.",
    "start": "338040",
    "end": "344590"
  },
  {
    "text": "You need the pretraining. But you also need this kind\nof RLHF or this post-training to really shift the needle on\nwhat the most important models",
    "start": "344590",
    "end": "351880"
  },
  {
    "text": "are at that certain moment. Some examples. You can list so many of them\nwhere RLHF is relied upon.",
    "start": "351880",
    "end": "360020"
  },
  {
    "text": "I like to look at these\nplots from the Anthropic constitutional AI\npaper where they show this iterative improvement\nof their different RLHF methods.",
    "start": "360020",
    "end": "368040"
  },
  {
    "text": "It kind of shows how you\nhave these multiple model versions that evolving over\ntime as you add more fine-tuning",
    "start": "368040",
    "end": "373400"
  },
  {
    "text": "data. This is a dense paper but one\nof the most representative figures of what RLHF can do.",
    "start": "373400",
    "end": "378530"
  },
  {
    "text": "There's a lot of\ninformation in here that you don't need\nto follow right now. And then Meta's Llama\n2 paper is pretty funny",
    "start": "378530",
    "end": "383960"
  },
  {
    "text": "where they have\nthis quote that is like \"reinforcement learning,\nknown for its instability seemed a somewhat shadowy field\nfor those in the NLP research",
    "start": "383960",
    "end": "391730"
  },
  {
    "text": "community. However, reinforcement learning\nproved highly effective, particularly given its cost\nand time effectiveness.\"",
    "start": "391730",
    "end": "397560"
  },
  {
    "text": "So this is from the\ntechnical report directly, which I found\nreally entertaining. So this is back\nin the day when we",
    "start": "397560",
    "end": "402650"
  },
  {
    "text": "were like, oh, we don't\nknow if RLHF is really going to take off. This is July of 2023.",
    "start": "402650",
    "end": "408710"
  },
  {
    "text": "It's in this building period. And it's just directly\nfrom the report. And that's aged really\nwell where people are still",
    "start": "408710",
    "end": "414163"
  },
  {
    "text": "using this today. But there's just a lot\nof interesting hints and kind of history of\nculture of RLHF [CHUCKLES]",
    "start": "414163",
    "end": "419805"
  },
  {
    "text": "in the releases of these\nmodels where these companies like to talk about it and\ngive us these cultural details",
    "start": "419805",
    "end": "425669"
  },
  {
    "text": "to what's going on. So I'm going to go\nthrough some definitions. And I don't spend\ntoo much time on say,",
    "start": "425670",
    "end": "432220"
  },
  {
    "text": "doing RLHF 101 and\nexactly what is happening with these kind\nof mathematical terms.",
    "start": "432220",
    "end": "437637"
  },
  {
    "text": "But it's important to get\non the same page of what some of these things\ndo and don't mean. There's a lot of definitions.",
    "start": "437637",
    "end": "443630"
  },
  {
    "text": "I think some of the\ninteresting ones that if they don't make sense\nright now to come back to is like, what's the difference\nbetween instruction fine-tuning",
    "start": "443630",
    "end": "450313"
  },
  {
    "text": "and supervised fine-tuning. I think instruction fine-tuning\nis what's become really popular, where you're training a\nmodel to follow instructions.",
    "start": "450313",
    "end": "457445"
  },
  {
    "text": "And I have another\nslide on this after. And supervised fine-tuning is\nthis domain-specific thing. And we want to do both of them.",
    "start": "457445",
    "end": "463610"
  },
  {
    "text": "I think instruction fine-tuning\nis more linked to RLHF. It's about making these models\nreally useful, and really",
    "start": "463610",
    "end": "470350"
  },
  {
    "text": "engaging, and easy to work with. And then there's other\nthings like alignment, which is super vague.",
    "start": "470350",
    "end": "475940"
  },
  {
    "text": "But it's in the\nword, it's align. It's training a model to be\nmirrored to what a user wants.",
    "start": "475940",
    "end": "481790"
  },
  {
    "text": "And there's a lot of things\nthat you can align to. RLHF is a mouthful, which\nis one specific tool",
    "start": "481790",
    "end": "487010"
  },
  {
    "text": "for doing alignment where\nyou have this human feedback data, which feedback\nis a really loaded word",
    "start": "487010",
    "end": "493610"
  },
  {
    "text": "there where there\ncan be preferences and learning to rank is related\nto actually putting feedback on preferences.",
    "start": "493610",
    "end": "499005"
  },
  {
    "text": "So there's a lot\nof little things. I tried to make\npreference fine-tuning a phrase at one point but\ndidn't really double down on it.",
    "start": "499005",
    "end": "504780"
  },
  {
    "text": "I think it's a little\nbit clearer than RLHF, especially in the\ncontext of DPO. But there's just\na lot of spheres",
    "start": "504780",
    "end": "510199"
  },
  {
    "text": "that are overlapping in\nthis kind of post-training or fine-tuning space\nof models these days.",
    "start": "510200",
    "end": "517090"
  },
  {
    "text": "Instruction fine-tuning,\nit's still the foundation of a lot of this. This is where things called\nsystem prompts are added,",
    "start": "517090",
    "end": "524779"
  },
  {
    "text": "where we're making the model\nready for a specific style of input. OpenAI is still\ninnovating on this.",
    "start": "524780",
    "end": "532083"
  },
  {
    "text": "They have this\nmodel spec document they released a\nfew weeks ago where they said they're going to have\na second level system prompt",
    "start": "532083",
    "end": "537550"
  },
  {
    "text": "here, which this just adds some\nstructure to how the models can take in data so that you can do\na lot more of this fine-tuning",
    "start": "537550",
    "end": "544240"
  },
  {
    "text": "down the line and how user\ndata actually gets passed to the model or how the\ndeveloper passes information",
    "start": "544240",
    "end": "549940"
  },
  {
    "text": "that the user doesn't see. So what this can often look\nlike is Stack Overflow,",
    "start": "549940",
    "end": "555690"
  },
  {
    "text": "Reddit data where\nyou have a question at the top and then an answer. And this is still,\nI think, a lot",
    "start": "555690",
    "end": "560885"
  },
  {
    "text": "of what is happening\nbehind the scenes. There's a lot of data sets\nof Stack Overflow out there. Reddit has these\ndata partnerships.",
    "start": "560885",
    "end": "566850"
  },
  {
    "text": "And this still uses the\nautoregressive loss function that we started with. We haven't branched out into\ndifferent loss functions yet.",
    "start": "566850",
    "end": "574029"
  },
  {
    "text": "But it's still super important. A lot of academic\nresearch shows that this is all you need in\nsome ways, which I",
    "start": "574030",
    "end": "580260"
  },
  {
    "text": "think is a much more mixed bag. But it's the simple method. And it's the right\nplace to start.",
    "start": "580260",
    "end": "585420"
  },
  {
    "text": "And where we go is then we\ngo to this RLHF objective, which this looks really familiar\nto people that are trained",
    "start": "585420",
    "end": "593640"
  },
  {
    "text": "in reinforcement learning. I think this is a little\ndifferent from the NLP loss function. On the left side is the\nstandard reinforcement learning",
    "start": "593640",
    "end": "601110"
  },
  {
    "text": "objective, which is you're\nlearning a policy pi to maximize some reward, which is a\nfunction of something depending",
    "start": "601110",
    "end": "607060"
  },
  {
    "text": "how you set up the problem. And then on the\nright side is going to be this kind\nof KL constraint.",
    "start": "607060",
    "end": "612470"
  },
  {
    "text": "This is a distance so that the\npolicy doesn't change too much. It's related to this whole\nidea of over-optimization",
    "start": "612470",
    "end": "619240"
  },
  {
    "text": "that I don't go into\ntoo much of this talk. But the key ideas is that\nwe want to optimize a reward",
    "start": "619240",
    "end": "625540"
  },
  {
    "text": "but not over-optimize it. And the primary\nquestions with doing RLHF is like, how do we implement\na reward function, what",
    "start": "625540",
    "end": "632312"
  },
  {
    "text": "is our reward\nactually going to be, and then how do we optimize it. You see this abstracted later as\nwe train a specific reward model",
    "start": "632312",
    "end": "638830"
  },
  {
    "text": "and then we have\nspecific policy updates. And DPO, Direct\nPreference Optimization, handles this a little\nbit differently.",
    "start": "638830",
    "end": "645590"
  },
  {
    "text": "So get before we get there,\nthe actual preference model that people use for RLHF is--",
    "start": "645590",
    "end": "651639"
  },
  {
    "text": "well, I find this interesting. It's from this\nBradley Terry model, which is from economics\nand the 1950s,",
    "start": "651640",
    "end": "658430"
  },
  {
    "text": "which is essentially a\nprobability distribution over a pairwise choice. And what ends up happening\nfor various technical reasons",
    "start": "658430",
    "end": "666370"
  },
  {
    "text": "is that if we train\na preference model, it needs to output\na scalar value. And by some coincidence, I think\nit's still very convenient.",
    "start": "666370",
    "end": "674120"
  },
  {
    "text": "They just take\nthe output of this learned probability\ndistribution as a reward. They say that, OK,\nour reward is going",
    "start": "674120",
    "end": "680080"
  },
  {
    "text": "to be proportional\nto this probability and it's going to work. And it ends up doing so. But that's even a big\nleap to accept that.",
    "start": "680080",
    "end": "687500"
  },
  {
    "text": "It's like we have this pairwise\npreference probability that's saying the probability that one\nanswer is chosen over another.",
    "start": "687500",
    "end": "694430"
  },
  {
    "text": "And then you have to do this\nmental crazy step of saying we just pass in one number\nor one piece of text",
    "start": "694430",
    "end": "699630"
  },
  {
    "text": "and we're getting\nthe probability that that one piece of text\nis chosen over any arbitrary",
    "start": "699630",
    "end": "704680"
  },
  {
    "text": "other one. So there's a lot of assumptions\nthat make this-- there's kind of deep concepts in here.",
    "start": "704680",
    "end": "710565"
  },
  {
    "text": "But what we're\ngetting is a model that's giving us a score out.",
    "start": "710565",
    "end": "715840"
  },
  {
    "text": "And the question is why\ndo we have to do this and what if we can just take\nour original objective and use",
    "start": "715840",
    "end": "722890"
  },
  {
    "text": "gradient ascent\non this equation. Ascent because it's a maximum. And this is really\nwhat DPO does.",
    "start": "722890",
    "end": "728959"
  },
  {
    "text": "I'm blurring through\na ton of math. It's a great paper to learn a\nlot of this math of language",
    "start": "728960",
    "end": "734350"
  },
  {
    "text": "modeling where you learn\nhow these probabilities have different pieces of text that\nare handled by the model,",
    "start": "734350",
    "end": "740140"
  },
  {
    "text": "and how it ends up being a lot\nof these log probability ratios, and seeing how the\nprompt and the completion",
    "start": "740140",
    "end": "746440"
  },
  {
    "text": "are handled differently. It's worth digging into and\nunderstanding the derivation. But the core idea is,\nwell, why can't we just",
    "start": "746440",
    "end": "753310"
  },
  {
    "text": "do gradient descent\nor gradient ascent to solve RLHF optimization? And it becomes\nincredibly simple.",
    "start": "753310",
    "end": "763295"
  },
  {
    "text": "So if you look at\nthe code on the right is a reference code from\nthe original implementation.",
    "start": "763295",
    "end": "768709"
  },
  {
    "text": "It's extremely\nsimple to implement. And it has this\ncharacteristic where if you've worked with something\nlike transformers before,",
    "start": "768710",
    "end": "774230"
  },
  {
    "text": "it's pretty easy to write a\nloss function that uses DPO",
    "start": "774230",
    "end": "779360"
  },
  {
    "text": "rather than building an\nentire infrastructure stack to start with. When you do something like\nPPO and this full RLHF stuff",
    "start": "779360",
    "end": "786320"
  },
  {
    "text": "that OpenAI does,\nyou normally need almost entire new\ninfrastructure stack. But you can get started with\nDPO in a much simpler way.",
    "start": "786320",
    "end": "793560"
  },
  {
    "text": "And there's some kind\nof characteristics that I'll get to later,\nwhich is DPO still has a reward model,\nwhich is really",
    "start": "793560",
    "end": "799100"
  },
  {
    "text": "important to the math\nactually checking out. Where you're using\nyour original language model as a different\ntype of reward model.",
    "start": "799100",
    "end": "806100"
  },
  {
    "text": "But that quickly takes\nus down a whole bunch of derivations that is probably\nat least not the lecture",
    "start": "806100",
    "end": "812420"
  },
  {
    "text": "that I think is as fun to give. And the key thing is, which is\nwhy this lecture is called what",
    "start": "812420",
    "end": "817770"
  },
  {
    "text": "it is, that the\nfirst two points mean we'll see more DPO models\nthan anything else. DPO is where everyone\nwill start with if they",
    "start": "817770",
    "end": "824819"
  },
  {
    "text": "want to do alignment research. And it's for good reason. It is the right place\nto start if you're thinking about doing this.",
    "start": "824820",
    "end": "830710"
  },
  {
    "text": "It scales more\neasily on compute. It's easier to debug. It's even easier to learn. So it's not really worth\nsecond guessing that.",
    "start": "830710",
    "end": "839069"
  },
  {
    "text": "And it is a good place to start. But it also leads into these\nridiculous conversations",
    "start": "839070",
    "end": "844980"
  },
  {
    "text": "online where everyone\nis trying to figure out is DPO better than\nother RL methods,",
    "start": "844980",
    "end": "850940"
  },
  {
    "text": "PPO, which is this older,\npopular deep RL algorithm which John Schulman wrote.",
    "start": "850940",
    "end": "857220"
  },
  {
    "text": "Reinforce which is a slightly\ndifferent parameterization of policy gradient.",
    "start": "857220",
    "end": "862540"
  },
  {
    "text": "They're very similar. And DPO ends up being much-- it's just simpler to work with.",
    "start": "862540",
    "end": "868360"
  },
  {
    "text": "So there's this meme where\nit's like if you just do gradient descent, it'll work. In reality, they're\ndifferent loss functions.",
    "start": "868360",
    "end": "876089"
  },
  {
    "text": "And they're doing\nvery different things. But you can get similar\nresults with both of them, which is why if something\nis much easier to do,",
    "start": "876090",
    "end": "882510"
  },
  {
    "text": "you should just start with it. And I come back\nto this much later in the talk, which is\nwhat is fundamentally",
    "start": "882510",
    "end": "887670"
  },
  {
    "text": "different about\nthese RL algorithms, and how your data is processed,\nand where the signals actually",
    "start": "887670",
    "end": "892800"
  },
  {
    "text": "come from. But for now, we don't need\nto say one versus the other. We can do both and\nthey are different.",
    "start": "892800",
    "end": "899920"
  },
  {
    "text": "So that's a quick one on one\nof what the core ideas are. I'm going to take a\npath to how we actually",
    "start": "899920",
    "end": "908020"
  },
  {
    "text": "got to training models with DPO\nbecause I think this slide was from a different talk where\nthis subsection is reduced from.",
    "start": "908020",
    "end": "915440"
  },
  {
    "text": "But DPO really came out months\nbefore we started getting popular models trained with it. So it's like, how did we\nactually get to the point",
    "start": "915440",
    "end": "922120"
  },
  {
    "text": "where the community\nwas training models with DPO, which is much more\nrecently than the paper was",
    "start": "922120",
    "end": "927490"
  },
  {
    "text": "actually released. And this comes all the way back\nto these first instruction-tuned models that you saw.",
    "start": "927490",
    "end": "933390"
  },
  {
    "text": "So the Alpaca,\nthe Vicuna, Koala, Dolly of the world\nall in April of 2023.",
    "start": "933390",
    "end": "939920"
  },
  {
    "text": "And these are all\nbuilt on similar things and slight iterations. So there's kind of figuring\nout how to use synthetic data.",
    "start": "939920",
    "end": "946840"
  },
  {
    "text": "Building onto this\nfirst Llama release, there's some other things\nthat I'll talk about. But this is where we started.",
    "start": "946840",
    "end": "952370"
  },
  {
    "text": "They're all using\ninstruction tuning. Most of them use synthetic data. And what Vicuna\nactually did was they",
    "start": "952370",
    "end": "959920"
  },
  {
    "text": "used this thing\ncalled ShareGPT which was the first time that people\nworking in an academic alignment",
    "start": "959920",
    "end": "965709"
  },
  {
    "text": "space had access to data\nthat was from humans. It ended up being a bit\nof a legal gray area",
    "start": "965710",
    "end": "971440"
  },
  {
    "text": "because it was logging data that\npeople used in a Google Chrome extension called\nShareGPT to make it",
    "start": "971440",
    "end": "978310"
  },
  {
    "text": "so ChatGPT had a Share button. But this data was really\nimportant to things like Vicuna and a lot of the other models\nthat came down the line",
    "start": "978310",
    "end": "985450"
  },
  {
    "text": "and is still used\nin models today as one subset of the\ntraining data set. So just having access\nto these human prompts",
    "start": "985450",
    "end": "994040"
  },
  {
    "text": "unlocked a lot of\npotential back in the day and is still something\nthat we're seeing. Thankfully, now\nwe're starting to get",
    "start": "994040",
    "end": "999132"
  },
  {
    "text": "data sets like this\nthat were collected in more permissive ways. This kind of LMSYS\naccess data has",
    "start": "999132",
    "end": "1004390"
  },
  {
    "text": "prompts that are\ncollected with consent. And WildChat, which was\na project from Ai2, which",
    "start": "1004390",
    "end": "1009490"
  },
  {
    "text": "essentially gave people\nfree access to ChatGPT in exchange for their data.",
    "start": "1009490",
    "end": "1014540"
  },
  {
    "text": "The thing that\ncame after ShareGPT was the realization that\nwe need more human data.",
    "start": "1014540",
    "end": "1019620"
  },
  {
    "text": "And this Open Assistant\nproject is one that we honestly need more of. It shows how hard\nit is to create",
    "start": "1019620",
    "end": "1026720"
  },
  {
    "text": "human data that we haven't\nseen more things like this. This was run by a few people\nin a Discord community",
    "start": "1026720",
    "end": "1032150"
  },
  {
    "text": "working extremely long hours\nto generate prompts, responses, and preference pairs to common\nrequests language models.",
    "start": "1032150",
    "end": "1039750"
  },
  {
    "text": "And this was from April of 2023. And we haven't seen\nanything like it.",
    "start": "1039750",
    "end": "1046040"
  },
  {
    "text": "LMSYS's data is\nsimilar but there's not the same level of controls,\nand voting, and ranking that went into this\nOpen Assistant data.",
    "start": "1046040",
    "end": "1052550"
  },
  {
    "text": "And it, again, is a data set\nthat we're still training models with and many people still\ntrain models with and come up",
    "start": "1052550",
    "end": "1057680"
  },
  {
    "text": "time and time again. So it's just these one or\ntwo influential data sets from over a year ago are still\nwhat are used to train models.",
    "start": "1057680",
    "end": "1064860"
  },
  {
    "text": "So you'll get the\ntheme as I keep going. There's actually RLHF models\ntrained in April of 2023",
    "start": "1064860",
    "end": "1072140"
  },
  {
    "text": "as well. This was from CarperAI\nthat was doing a lot of work in the space.",
    "start": "1072140",
    "end": "1078750"
  },
  {
    "text": "They've fallen back a\nbit in recent times. But there were people that\nwere doing the similar methods",
    "start": "1078750",
    "end": "1084330"
  },
  {
    "text": "to what I'm going to talk\nabout at the end of the talk. That kind of knowledge\nand infrastructure was not translated into\nthings that were easy to use.",
    "start": "1084330",
    "end": "1092139"
  },
  {
    "text": "So there's also this vein of\nlike, even if things are open, it doesn't mean that it's going\nto immediately catch on and be",
    "start": "1092140",
    "end": "1099410"
  },
  {
    "text": "useful. You have to have the resources,\nthe data, and your code base set up in a way that people\ncan build on it, which",
    "start": "1099410",
    "end": "1105090"
  },
  {
    "text": "is what DPO did really well. This RLHF model from\nCarper was successful.",
    "start": "1105090",
    "end": "1110559"
  },
  {
    "text": "It was better than\nthe Vicuna model. But no one really\nbuilt on it right away, which I always find confusing.",
    "start": "1110560",
    "end": "1117270"
  },
  {
    "text": "And then later in the\nyear, another key thing for this open alignment\nwas the Llama 2 backlash,",
    "start": "1117270",
    "end": "1122410"
  },
  {
    "text": "where the Llama 2 was asked\nto kill a Linux process, it would refuse. And this kind of bred a whole\nseries of models which are still",
    "start": "1122410",
    "end": "1132419"
  },
  {
    "text": "referred to as uncensored, which\nI don't think is the best name. Because I don't think\nthere was ever actually any censoring to the model.",
    "start": "1132420",
    "end": "1138880"
  },
  {
    "text": "It wasn't intentional\ncensorship. But the goal is to\nmake models that don't refuse any request,\nwhich is useful as a research",
    "start": "1138880",
    "end": "1145590"
  },
  {
    "text": "artifact, which is like, what\ndo you get out of a model if it answers every\nquestion, what are the limits in that regard.",
    "start": "1145590",
    "end": "1151710"
  },
  {
    "text": "There are other ways to use\nthat which are up to you. But what ended up happening is\na lot of these ShareGPT data",
    "start": "1151710",
    "end": "1159570"
  },
  {
    "text": "sets because they were from\nChatGPT, there's data that says, oh, as a language model,\nI shouldn't answer that.",
    "start": "1159570",
    "end": "1164830"
  },
  {
    "text": "So people started\nfiltering all of that out. And you still see a\nlot of people releasing these uncensored models today as\na popular area of development.",
    "start": "1164830",
    "end": "1174720"
  },
  {
    "text": "I think that we should\nunderstand what people need when doing research. And researching a model that\ndoesn't refuse is reasonable.",
    "start": "1174720",
    "end": "1183280"
  },
  {
    "text": "But if you're to deploy a\nmodel for free use to users, you should consider\nwhether or not everything should be answered.",
    "start": "1183280",
    "end": "1189170"
  },
  {
    "text": "So as a researcher, how your\nartifacts are used kind of depend on the work that you're\nactually going to be doing.",
    "start": "1189170",
    "end": "1197090"
  },
  {
    "text": "Then in the alignment space,\nthere's this long series-- I'm almost done with\nthe [? winLMs. ?] But there's this\nlong series of models",
    "start": "1197090",
    "end": "1203003"
  },
  {
    "text": "that are really interesting to\npeople like me that never really broke through the narrative. Where they're saying there are\nthings-- like, we used RLHF.",
    "start": "1203003",
    "end": "1209300"
  },
  {
    "text": "We're the first model to\nbeat GPT-4 on AlpacaEval and these other eval tools.",
    "start": "1209300",
    "end": "1214350"
  },
  {
    "text": "They're scaling things up. But they don't\nalways have papers. They don't always\nhave code bases. [CHUCKLES] And things are happening around.",
    "start": "1214350",
    "end": "1222740"
  },
  {
    "text": "It's not just like the\nHugging Face of the world. There's a lot of different\norganizations in the US",
    "start": "1222740",
    "end": "1228230"
  },
  {
    "text": "and elsewhere that were\naligning models and getting similar numbers or beating\nthese mainstream tech",
    "start": "1228230",
    "end": "1233450"
  },
  {
    "text": "companies in these places that\nyou look for models to these. So these are all in\nthe summer of 2023.",
    "start": "1233450",
    "end": "1239840"
  },
  {
    "text": "And this is all-- I bring these up\nbecause this comes before the first\nbig splash of DPO.",
    "start": "1239840",
    "end": "1245810"
  },
  {
    "text": "So this Zephyr model was\nreally the first model that I remember making\na splash with DPO.",
    "start": "1245810",
    "end": "1251470"
  },
  {
    "text": "And this is when it took\nuntil this time, which was in September after the\nMay release of the paper,",
    "start": "1251470",
    "end": "1257350"
  },
  {
    "text": "for people to really be\nlike, oh, DPO is a real deal. It took four months. [CHUCKLES] And now the\npaper has best paper.",
    "start": "1257350",
    "end": "1264250"
  },
  {
    "text": "Everyone uses it. There's tons of derivations. But in industry and people\ntrying to train models, there was a lot of\nskepticism until this moment.",
    "start": "1264250",
    "end": "1271389"
  },
  {
    "text": "So this is a classic\nacademic story of needing to wait\na bit [CHUCKLES]",
    "start": "1271390",
    "end": "1277140"
  },
  {
    "text": "until your work is\nvindicated in some ways. But the two crucial\nthings here was new data set, the ultra\nfeedback data set,",
    "start": "1277140",
    "end": "1283090"
  },
  {
    "text": "which is a data set of\nsynthetically-generated text labeled by GPT-4.",
    "start": "1283090",
    "end": "1289690"
  },
  {
    "text": "So it's, again, this new\nways of making data where it's a preference data set.",
    "start": "1289690",
    "end": "1295020"
  },
  {
    "text": "We didn't make it. It was made by an Openbnb. I think they're based in China.",
    "start": "1295020",
    "end": "1300080"
  },
  {
    "text": "I should know more. And then we also just had\nto do a lot of experiments to make it work. There's a weird, really low\nlearning rate that was needed",
    "start": "1300080",
    "end": "1306780"
  },
  {
    "text": "to make this kind of\nchat model work with DPO, which is like 5E minus 7. If you're really\nplugged into AI,",
    "start": "1306780",
    "end": "1312400"
  },
  {
    "text": "you'll know that 3e minus 4 is\nthe lore of the best learning rate. So it's many orders\nof magnitude lower.",
    "start": "1312400",
    "end": "1319877"
  },
  {
    "text": "So that's what it took\nto get this to work. We probably could have done\nit months earlier if we just did more hyperparameter sweeps.",
    "start": "1319878",
    "end": "1324880"
  },
  {
    "text": "But this is the random\nhappenstance of the stories that people now backcast\nas being like, this",
    "start": "1324880",
    "end": "1332310"
  },
  {
    "text": "is the super important model. It's somewhat random. And then at the same\ntime, I was switching jobs",
    "start": "1332310",
    "end": "1337920"
  },
  {
    "text": "to the Allen Institute. And they were already working\non this project, which is trying to do a systematic\nstudy of instruction-tuning data",
    "start": "1337920",
    "end": "1345090"
  },
  {
    "text": "along with some of this\npreference-tuning recipes that were coming out. Because once this\nZephyr model came out,",
    "start": "1345090",
    "end": "1351549"
  },
  {
    "text": "there's always skeptics of, oh,\nlike doing it at 7B is easy. That's a small model. So it's like, oh, is it going to\nactually scale to the real deal,",
    "start": "1351550",
    "end": "1358510"
  },
  {
    "text": "to bigger models to be\nwhat like ChatGPT does. So it was like, OK, we\nhave some more compute.",
    "start": "1358510",
    "end": "1363630"
  },
  {
    "text": "And we tried it on this 70\nbillion parameter scale. And we showed similar gains. All we was use the\nsame UltraFeedback",
    "start": "1363630",
    "end": "1370380"
  },
  {
    "text": "recipe, the low learning\nrate, and it largely worked. So this is within two months.",
    "start": "1370380",
    "end": "1376170"
  },
  {
    "text": "And then this is when-- and since then is when there's\ntons of new DPO models.",
    "start": "1376170",
    "end": "1381679"
  },
  {
    "text": "All these startups that are\nreleasing their own models will release an instructor\nversion that is a DPO thing.",
    "start": "1381680",
    "end": "1387770"
  },
  {
    "text": "And that kind of\ncontinued for six months. I think just today, I'm starting\nto see less DPO models, which is interesting.",
    "start": "1387770",
    "end": "1393240"
  },
  {
    "text": "I've been keeping track of them\nfor another evaluation project. And it has finally\nslowed down a little bit.",
    "start": "1393240",
    "end": "1399120"
  },
  {
    "text": "I don't know if that's\nalignment at large. But there is so many. I should add a slide that's a\nlist of the ridiculous amount",
    "start": "1399120",
    "end": "1405500"
  },
  {
    "text": "of DPO models after these two. But this is really when\nthe floodgates started",
    "start": "1405500",
    "end": "1411290"
  },
  {
    "text": "and when we're like,\nOK, DPO really works. So this is kind of why\nI say what comes next.",
    "start": "1411290",
    "end": "1418880"
  },
  {
    "text": "It's like, we could\nretrain models on data sets that we have. We don't have that\nmany data sets.",
    "start": "1418880",
    "end": "1424002"
  },
  {
    "text": "But it kind of feels like\nwe're fishing in the dark. Like, Zephyr was built\non the success of needing the low learning rate.",
    "start": "1424003",
    "end": "1429740"
  },
  {
    "text": "This Tulu 2 model was\nactually trained on TPUs because we have the Google\nTensor Research Cloud.",
    "start": "1429740",
    "end": "1434990"
  },
  {
    "text": "So we have bigger TPUs\nto train these models. And it's like, how do we do\nthis more systematically.",
    "start": "1434990",
    "end": "1440000"
  },
  {
    "text": "And that's kind of\nwhere most of what I talk about today on\nthe technical matter is the recent\nresearch that we've",
    "start": "1440000",
    "end": "1445929"
  },
  {
    "text": "been doing to just\nmake sense of this and answer the fundamental\nquestions of what do we need to change about\nDPO, is PPO better, and so on.",
    "start": "1445930",
    "end": "1455490"
  },
  {
    "text": "So this is kind of the reality\nthat I go back and forth in between, which is we don't\nreally have the human data to do",
    "start": "1455490",
    "end": "1462539"
  },
  {
    "text": "RLHF like industry. But it is getting much easier\nto do alignment research. So you can choose\nyour narrative.",
    "start": "1462540",
    "end": "1468190"
  },
  {
    "text": "I think sometimes because\nI'm so close to industry and hear about people have,\nI'm too often on this side. But there is a lot of\nopportunity to do things.",
    "start": "1468190",
    "end": "1475360"
  },
  {
    "text": "It feels crowded. But being crowded at this point\nwhen there's so much investment",
    "start": "1475360",
    "end": "1480450"
  },
  {
    "text": "is just because you're\nin the right area. And most people in this room\naren't trying to be professors.",
    "start": "1480450",
    "end": "1485710"
  },
  {
    "text": "So if you get scooped, it's OK. But I find it very fun.",
    "start": "1485710",
    "end": "1491250"
  },
  {
    "text": "So how do we actually understand\nwhat we're doing with alignment? And can we improve\non these models?",
    "start": "1491250",
    "end": "1496740"
  },
  {
    "text": "We have Tulu 2. It has a number because we want\nto keep releasing more models. So how do we get\nbetter at evaluating",
    "start": "1496740",
    "end": "1502440"
  },
  {
    "text": "what we're doing to try to\nunderstand this process. And then how do we\ntrain better models. So these are the sort of\nthings that I'm up to.",
    "start": "1502440",
    "end": "1509740"
  },
  {
    "text": "I have a few examples of\nthings I've been working on. I built an evaluation\ntool for reward models.",
    "start": "1509740",
    "end": "1515000"
  },
  {
    "text": "I'll talk more about reward\nmodels to start here. And we need better\nevaluation because when you're training models,\nyou need to be able to do",
    "start": "1515000",
    "end": "1523210"
  },
  {
    "text": "what I call local evaluation. You need to be able to get\na number that tells you if your training technique\nis improving the end result.",
    "start": "1523210",
    "end": "1531410"
  },
  {
    "text": "You can't wait until Chatbot\nArena evaluates your model because that takes you about a\nmonth to get your numbers back. You need to be able\nto run something",
    "start": "1531410",
    "end": "1537910"
  },
  {
    "text": "at your desk that gives you a\nsignal on if you're actually doing a good job. And we're still pretty behind\non those evaluation tools.",
    "start": "1537910",
    "end": "1544310"
  },
  {
    "text": "And there are more coming,\nwhich is promising. And then given DPO simplicity,\ncan we actually improve on that.",
    "start": "1544310",
    "end": "1551030"
  },
  {
    "text": "And can we catch on to\nsome of the industry rumors that they've let it drift aside?",
    "start": "1551030",
    "end": "1558010"
  },
  {
    "text": "So RewardBench is\nthis project that I started because there\nare no evaluation",
    "start": "1558010",
    "end": "1563410"
  },
  {
    "text": "tools for reward models. My motivation was\nmostly for transparency, given how much industry\nsays reward models are",
    "start": "1563410",
    "end": "1570575"
  },
  {
    "text": "where you need to focus on. They're really important\nfor getting good models out the door. And it's like, what\ndoes that mean?",
    "start": "1570575",
    "end": "1576620"
  },
  {
    "text": "What does it mean for a\nreward model to be good? If we look at this\nfeedback diagram, which",
    "start": "1576620",
    "end": "1581980"
  },
  {
    "text": "is the one kind of homage\nto the RL background, which is feedback loops, is a\nreward model is, in this case,",
    "start": "1581980",
    "end": "1589970"
  },
  {
    "text": "the Agent is your\nactual language model. pi is the policy. The training data is\nprompts that you get.",
    "start": "1589970",
    "end": "1595820"
  },
  {
    "text": "So in this kind\nof RLHF framework, you have this feedback loop\nwhere the policy generates",
    "start": "1595820",
    "end": "1602620"
  },
  {
    "text": "something A, which is the\naction, which is the completion. It goes to the reward\nmodel which then scores it.",
    "start": "1602620",
    "end": "1607850"
  },
  {
    "text": "But you, on the side, are\nlooking at all these evaluation tools. And none of these\nevaluation tools",
    "start": "1607850",
    "end": "1614260"
  },
  {
    "text": "are giving us internal\ninsight into what's happening in this feedback loop. It seems external to\nwhat we are doing when",
    "start": "1614260",
    "end": "1621100"
  },
  {
    "text": "we're training these models. So we really wanted to zoom\nin on this reward model. And reward models are trained\nin an another kind of weird way,",
    "start": "1621100",
    "end": "1630320"
  },
  {
    "text": "the many quirks of RLHF. So in order to train\na reward model, you need to collect this\npairwise preference data.",
    "start": "1630320",
    "end": "1636350"
  },
  {
    "text": "If you're using\nChatGPT a lot, you'll sometimes see it\ngive you two answers and ask you which one is better.",
    "start": "1636350",
    "end": "1642200"
  },
  {
    "text": "This data is literally what is\nused to train a reward model. It's a prompt and then two\ncompletions, a chosen completion",
    "start": "1642200",
    "end": "1650500"
  },
  {
    "text": "and a rejected completion. But in order to\ntrain these models, you have to pass both of\nthem in at the same time.",
    "start": "1650500",
    "end": "1656150"
  },
  {
    "text": "So you pass both of\nthem in at the same time and it gives you\ntwo scalar values. You use a language model\nthat outputs a scalar just",
    "start": "1656150",
    "end": "1662680"
  },
  {
    "text": "by some modifications\nof the last layers rather than outputting text. And then this loss function,\nI'll show you on the next slide,",
    "start": "1662680",
    "end": "1669010"
  },
  {
    "text": "is essentially why you need to\nuse this batch mode idea, which is you pass multiple\nthings at once",
    "start": "1669010",
    "end": "1675010"
  },
  {
    "text": "and you get multiple\nnumbers out. So this loss function\nis essentially here this r is the output\ndirectly from the reward",
    "start": "1675010",
    "end": "1682780"
  },
  {
    "text": "model for the rejected\ncompletion and the chosen completion. So you're trying to separate\nthe distance between them.",
    "start": "1682780",
    "end": "1688419"
  },
  {
    "text": "And then automatic\ndifferentiation updates the parameters so\nthat this distance is bigger.",
    "start": "1688420",
    "end": "1693530"
  },
  {
    "text": "So you can't just\nkind of do supervised learning directly on one thing\nto say for the reward model.",
    "start": "1693530",
    "end": "1700280"
  },
  {
    "text": "There are alignment methods\nresearching that now. But it's really built on this\nidea of separating two things",
    "start": "1700280",
    "end": "1706660"
  },
  {
    "text": "and creating a margin\nin the preferences to learn the decision boundary. There's a lot of\nreally specific details",
    "start": "1706660",
    "end": "1712059"
  },
  {
    "text": "in industry such as these models\nare only trained for one epoch. They get really\nlow accuracy scores",
    "start": "1712060",
    "end": "1717460"
  },
  {
    "text": "when you compare them to\nother trained test set things in machine learning. And there's some additional\ntweaks that people do.",
    "start": "1717460",
    "end": "1724790"
  },
  {
    "text": "You can do ensembles. Llama 2 did this\nweird margin loss. But none of it really\nis transformative in how",
    "start": "1724790",
    "end": "1732670"
  },
  {
    "text": "these models are trained. They're in this\nweird place where you can only get about 70%\nagreement with your annotators.",
    "start": "1732670",
    "end": "1739570"
  },
  {
    "text": "It's kind of the thing of, is\nthe noise part of the signal or is it a bug? So in preferences,\nit could make sense",
    "start": "1739570",
    "end": "1746080"
  },
  {
    "text": "that it's a signal because not\neveryone's preferences here are the same. So not getting full of\nagreement would be like,",
    "start": "1746080",
    "end": "1752382"
  },
  {
    "text": "this system might be working. We don't want\nChatGPT to be fully narrow minded all the time.",
    "start": "1752382",
    "end": "1758330"
  },
  {
    "text": "And this reads to\nthe thing of how do we actually evaluate\nthese reward models that I was talking about.",
    "start": "1758330",
    "end": "1763520"
  },
  {
    "text": "I hear all the time that reward\nmodels are crucial to RLHF. But how do we know exactly\nwhat types of the final policy",
    "start": "1763520",
    "end": "1770809"
  },
  {
    "text": "they're improving? Should we include safety\nin these reward models? How does scaling laws\nimpact reward models?",
    "start": "1770810",
    "end": "1776617"
  },
  {
    "text": "And it's basic machine\nlearning questions that it's like, can\nwe evaluate these, what should we think about.",
    "start": "1776617",
    "end": "1782990"
  },
  {
    "text": "So what we did is we\ncollected a bunch of prompts and then we manually\ncreated chosen and rejected",
    "start": "1782990",
    "end": "1789440"
  },
  {
    "text": "answers for each prompt. And then we can\nsee whether or not the reward model agrees\nwith our human-created data",
    "start": "1789440",
    "end": "1796010"
  },
  {
    "text": "and call that a win or loss\nin an accuracy point of view. It's really direct. We're just doing inference\non existing models.",
    "start": "1796010",
    "end": "1802478"
  },
  {
    "text": "And we're going to\nsee whether or not they agree with human data. And this is a slide if you want\nto go into the academic side",
    "start": "1802478",
    "end": "1810700"
  },
  {
    "text": "of things. This was built on a lot\nof existing evaluation tools that were out there. You'll see some common name\nAlpacaVal, MT Bench, or things",
    "start": "1810700",
    "end": "1818710"
  },
  {
    "text": "that you've heard about. XSTest was on the\nslide when I mentioned Llama 2 being overly safe.",
    "start": "1818710",
    "end": "1825050"
  },
  {
    "text": "And there's some other\nthings that are really good but you might not have heard\nabout like this LLMBar data set",
    "start": "1825050",
    "end": "1830976"
  },
  {
    "text": "from Princeton is a\nbunch of trick questions that I'll have an\nexample on later. And some normal names\nfrom Anthropic and OpenAI",
    "start": "1830977",
    "end": "1838210"
  },
  {
    "text": "in here as well. So there's a lot\nof different things that we're testing\nwith this data set. And then we're trying to\nget the full picture of what",
    "start": "1838210",
    "end": "1844929"
  },
  {
    "text": "is going on with these models. We released this\nin March of '24.",
    "start": "1844930",
    "end": "1850360"
  },
  {
    "text": "And you can see a\nkey in the bottom where these red circles\nwith the arrow in them",
    "start": "1850360",
    "end": "1855400"
  },
  {
    "text": "are DPO models which you\ncan use as a reward model. And then these dice which\nlook like gray squares",
    "start": "1855400",
    "end": "1862150"
  },
  {
    "text": "when you zoom out are what I\ndescribed in this classifier type of training.",
    "start": "1862150",
    "end": "1867870"
  },
  {
    "text": "And you can see that\nthere's reasonable scores. The benchmark isn't saturated. A bunch of open models.",
    "start": "1867870",
    "end": "1874410"
  },
  {
    "text": "Some names that\nyou've seen before. The Tulu models and the\nZephyr models are on here.",
    "start": "1874410",
    "end": "1879529"
  },
  {
    "text": "Normal stuff where this\nis what we expected. It's not too saturated. But if you look\nhere, I'll show you",
    "start": "1879530",
    "end": "1886009"
  },
  {
    "text": "where this model has\nmoved in a few months. So today, we have\na lot more models and there's a lot\nmore information here.",
    "start": "1886010",
    "end": "1891809"
  },
  {
    "text": "So I get to tell you about\nmore interesting things, which is how OpenAI's and\nCohere's models do on this, which I\nmentioned wanting",
    "start": "1891810",
    "end": "1897980"
  },
  {
    "text": "to do this for transparency. But we also add new types. So this is where the\nfifth model ended up.",
    "start": "1897980",
    "end": "1903720"
  },
  {
    "text": "So in two months, the model that\nwas fifth on our leaderboard is now 31st. So we're getting the saturation\nfrom people doing research",
    "start": "1903720",
    "end": "1911029"
  },
  {
    "text": "in the area to actually have\nplaces to compare their models. But we also have models\nfrom some closed labs.",
    "start": "1911030",
    "end": "1918360"
  },
  {
    "text": "And I'll get into\nthe details here. So some of these are\ndifferent types of models with",
    "start": "1918360",
    "end": "1926000"
  },
  {
    "text": "LLM-as-a-judge. LLM-as-a-judge is the idea of\nif you can ask a language model",
    "start": "1926000",
    "end": "1933000"
  },
  {
    "text": "which answer is better. This is how things like\nAlpacaEval and MT Bench are built. But you can also\nuse that as a reward model.",
    "start": "1933000",
    "end": "1939990"
  },
  {
    "text": "I told you that I have prompts\nand then chosen and rejected. I can just ask ChatGPT which one\nis better and see what it does.",
    "start": "1939990",
    "end": "1946050"
  },
  {
    "text": "And this is what we\nadded in as a baseline. And this ends up being really\ninteresting because GPT-4",
    "start": "1946050",
    "end": "1952220"
  },
  {
    "text": "and GPT-4o are not actually\nas good in this closed domain",
    "start": "1952220",
    "end": "1957230"
  },
  {
    "text": "as a reward model that\nCohere is training. So we don't have\nfull information because we don't have\nOpenAI's reward models,",
    "start": "1957230",
    "end": "1963840"
  },
  {
    "text": "but we can use their\nmodels to compare. So we have a lot of\ndifferent information going into one system\nabout how language",
    "start": "1963840",
    "end": "1970790"
  },
  {
    "text": "models and different parts\nof the alignment process choose different categories. So go back and you\ncan see this Cohere is",
    "start": "1970790",
    "end": "1979490"
  },
  {
    "text": "across two different months. Theirs has improved a lot. And then these\nearlier DPO models",
    "start": "1979490",
    "end": "1984693"
  },
  {
    "text": "that we saw higher\nup on the leaderboard have been shifting\ndown by more people training reward\nmodels to begin with.",
    "start": "1984693",
    "end": "1989950"
  },
  {
    "text": " And the specific category\nthat I'll focus most on",
    "start": "1989950",
    "end": "1995660"
  },
  {
    "text": "is this Chat Hard thing. If you think about\nevaluation a lot, it's actually\nsurprisingly common",
    "start": "1995660",
    "end": "2002200"
  },
  {
    "text": "as a topic covered\nin tech coverage is how evaluation is saturating. This is the one feature\nof our benchmark",
    "start": "2002200",
    "end": "2008620"
  },
  {
    "text": "that hasn't fully saturated. And it's really important to\nhaving some sort of longevity to the benchmark.",
    "start": "2008620",
    "end": "2014330"
  },
  {
    "text": "And I'll talk more about\nthis as we go from here. So I mentioned this data set. And it's interesting to\nunderstand if you could actually",
    "start": "2014330",
    "end": "2023169"
  },
  {
    "text": "do this problem. So what we have is a prompt,\na chosen, and a rejected. And the prompt is\ngiven an example",
    "start": "2023170",
    "end": "2029740"
  },
  {
    "text": "of a metaphor that uses\nthe following object stars. And then the chosen and rejected\nare two similar metaphors",
    "start": "2029740",
    "end": "2037039"
  },
  {
    "text": "but one of them-- [CHUCKLES] you can\nsee if you read these what the differences are.",
    "start": "2037040",
    "end": "2044320"
  },
  {
    "text": "I'm just pausing\nfor the people that are paying attention\nand reading these. But essentially, what\nhappens is that the chosen",
    "start": "2044320",
    "end": "2049659"
  },
  {
    "text": "one is about the sky and the\nrejected is about the moon. So the twinkling\ndiamonds in the sky.",
    "start": "2049660",
    "end": "2055586"
  },
  {
    "text": "See, I messed it up\nreading the slide. But it asks for stars. And it's about this\nmetaphor of stars where the rejected is\nabout the moon, which",
    "start": "2055587",
    "end": "2062080"
  },
  {
    "text": "is also in the sky at night. And this data set\nis a whole bunch of things like this where\nwhat they do to create this is they either\nmanually or by ChatGPT",
    "start": "2062080",
    "end": "2072010"
  },
  {
    "text": "ask to rephrase a prompt\nand then you create a new generation from it. So you can get these\nrejected generations",
    "start": "2072010",
    "end": "2078010"
  },
  {
    "text": "that are just off topic. And it makes sense\nfor something that would be really hard\nfor language models",
    "start": "2078010",
    "end": "2083649"
  },
  {
    "text": "because they have\nthis association between the stars and the moon. But we want our language models\nto be able to answer questions",
    "start": "2083650",
    "end": "2090340"
  },
  {
    "text": "like this. And this is the type of\nthing where our reward model benchmark, which\nis something that",
    "start": "2090340",
    "end": "2095379"
  },
  {
    "text": "is training language models,\nhas the best correlation as something that is hard. So this is promising.",
    "start": "2095380",
    "end": "2101510"
  },
  {
    "text": "This is the sort of thing\nthat if you're in research, is really interesting. So it's really in the weeds.",
    "start": "2101510",
    "end": "2107549"
  },
  {
    "text": "But it shows that\nwe still have things to learn about these models. And there are things\nthat we can't do yet.",
    "start": "2107550",
    "end": "2113510"
  },
  {
    "text": "But another interesting\npattern is safety. I mentioned this kind\nof uncensored models.",
    "start": "2113510",
    "end": "2119390"
  },
  {
    "text": "And in safety, we see all\nthe patterns we would expect. The breakdown at the\ntop of this table.",
    "start": "2119390",
    "end": "2125450"
  },
  {
    "text": "Refusals is things that we want\nthe language model to refuse. And then this XSTest data set\ncan be split into something",
    "start": "2125450",
    "end": "2131810"
  },
  {
    "text": "that we want models to refuse\nand we want models to respond. And you can see that\nthere's multiple categories",
    "start": "2131810",
    "end": "2138470"
  },
  {
    "text": "of either DPO models\nor reward models where the model that\nhandles safety really well refuses things like asking\nfor advice on causing harm",
    "start": "2138470",
    "end": "2147680"
  },
  {
    "text": "and responds to something\nthat is borderline. But there's actually a\nlot of models out there that just refuse everything.",
    "start": "2147680",
    "end": "2153530"
  },
  {
    "text": "So that'll tank\nyour score on things that responds to everything,\nwhich is the safe bet.",
    "start": "2153530",
    "end": "2159300"
  },
  {
    "text": "We were seeing a lot\nof tech companies release models like\nthis which it just feels like it doesn't feel\nright when you talk to them.",
    "start": "2159300",
    "end": "2165510"
  },
  {
    "text": "But there's also the models\nthat just respond to everything. It's like, not my job to gate\nwhether or not I should--",
    "start": "2165510",
    "end": "2171920"
  },
  {
    "text": "It's not the language model's\njob to gate the question, is the philosophy there,\nwhich is something that we hear a lot about in\nthe discourse of alignment.",
    "start": "2171920",
    "end": "2179860"
  },
  {
    "text": "But to see it in\nthese reward models, in DPO models when\ndirectly probing them",
    "start": "2179860",
    "end": "2184980"
  },
  {
    "text": "without asking them\nto generate text is nice to be able to confirm a\nlot of suspicions that we have.",
    "start": "2184980",
    "end": "2190690"
  },
  {
    "text": "So this is back to\nsome of the DPO math, which is, again, good to know. So if you are to go into the\nDPO paper, you'll see equation 3",
    "start": "2190690",
    "end": "2199510"
  },
  {
    "text": "here, which is the reward that\nis defined in order to make the math actually work. And this is very different\nthan just outputting a scalar.",
    "start": "2199510",
    "end": "2206660"
  },
  {
    "text": "It ends up being a ratio of\nthe probability of the policy relative to the original\npolicy during training, which",
    "start": "2206660",
    "end": "2212470"
  },
  {
    "text": "is called the reference model. And it's a very complicated\nmathematical representation.",
    "start": "2212470",
    "end": "2220085"
  },
  {
    "text": "So if you actually\ntake a piece of text and pass it through a\nDPO model, the reward will be something like minus\n200 or something because it's",
    "start": "2220085",
    "end": "2227289"
  },
  {
    "text": "a bunch of log probabilities. Probabilities are\nbetween 0 to 1. You take the log, you\nget negative numbers,",
    "start": "2227290",
    "end": "2232690"
  },
  {
    "text": "and you sum all of these up. So you get a big\nnegative number. And that intuitively\nis the score",
    "start": "2232690",
    "end": "2238457"
  },
  {
    "text": "that these models\nare providing, which is very different than the\nother type of reward models that I talked about\ntraining earlier.",
    "start": "2238457",
    "end": "2244420"
  },
  {
    "text": "And if you have two prompts\nwith a chosen and a rejected, equation 4 is the\nmath that you actually",
    "start": "2244420",
    "end": "2249790"
  },
  {
    "text": "need to do to decide whether\nor not one of the answers was better.",
    "start": "2249790",
    "end": "2255359"
  },
  {
    "text": "You're comparing these\nratios of probabilities from two different\nmodels with respect to this reference\nmodel, which was",
    "start": "2255360",
    "end": "2260869"
  },
  {
    "text": "the starting point of training. And the question is, when\npeople release a DPO model, they normally release a model\nand they don't release all",
    "start": "2260870",
    "end": "2267860"
  },
  {
    "text": "the intermediate checkpoints. So this reference model would\nbe an intermediate checkpoint in the training process.",
    "start": "2267860",
    "end": "2273150"
  },
  {
    "text": "So the question is,\ncan you do this? Can you use it as a reward\nmodel if you don't have access",
    "start": "2273150",
    "end": "2278270"
  },
  {
    "text": "to all the information? And the short\nanswer is no, which is all the scores on our\nbenchmark plummet across",
    "start": "2278270",
    "end": "2285050"
  },
  {
    "text": "all our DPO models that\nwe have, which makes sense because this extra\nmodel is a regularizer",
    "start": "2285050",
    "end": "2291950"
  },
  {
    "text": "in the probabilities. Or it's in the actual\nreward equation, if you go back a few slides.",
    "start": "2291950",
    "end": "2297260"
  },
  {
    "text": "It's in the equation. So what we do is\nwe get rid of this, and we stop normalizing equation\n4, and we just see if it works.",
    "start": "2297260",
    "end": "2304650"
  },
  {
    "text": "And it doesn't. But this is important because\nDPO is training a reward model.",
    "start": "2304650",
    "end": "2310950"
  },
  {
    "text": "But if we don't always\nhave access to it, we just can't learn from it. We can't use that in\nanother system as clearly.",
    "start": "2310950",
    "end": "2317310"
  },
  {
    "text": "So it's just a lot to ask\nfor when getting people to release models. And this is an interesting\nslide showing Cohere's progress",
    "start": "2317310",
    "end": "2325970"
  },
  {
    "text": "on reward models. In just a few\nmonths, they released something that was clearly state\nof the art on our benchmark.",
    "start": "2325970",
    "end": "2331559"
  },
  {
    "text": "Alignment Lab, this\nkind of RLHF flow work",
    "start": "2331560",
    "end": "2337070"
  },
  {
    "text": "released something in May. And then just a few\ndays later, Cohere sent another number that was\nlike, here's our new model.",
    "start": "2337070",
    "end": "2343098"
  },
  {
    "text": "It's still better\nthan everyone else. So it's nice to have this\nacademic industry intersection.",
    "start": "2343098",
    "end": "2348779"
  },
  {
    "text": "But it's very rare and\ntakes a lot of work in terms of networking and\nbuilding relationships. But we're trying to do it at\nleast in these small niches",
    "start": "2348780",
    "end": "2355820"
  },
  {
    "text": "where the companies\nare willing to share. RewardBench 2 is going to need\nto just mostly make everything",
    "start": "2355820",
    "end": "2362150"
  },
  {
    "text": "harder and make\neverything more human. And the last point is what I'm\ngoing to transition into next",
    "start": "2362150",
    "end": "2368150"
  },
  {
    "text": "is, everything I've told you\nabout is about part of this RLHF pipeline. But I haven't told you how it\nis impacting the final model",
    "start": "2368150",
    "end": "2375560"
  },
  {
    "text": "that you use at the\nend of the day, which is very rightful criticism,\nwhich is if you're evaluating part of the alignment\npipeline, you",
    "start": "2375560",
    "end": "2382235"
  },
  {
    "text": "should be telling me whether or\nnot the final model is actually useful. So this is where I\ntalk about our journey",
    "start": "2382235",
    "end": "2387980"
  },
  {
    "text": "into trying to train PPO models. So we're trying to\nfine-tune a good model. We spent a lot of time on\nDPO with this Tulu 2 work.",
    "start": "2387980",
    "end": "2396020"
  },
  {
    "text": "And we wanted to know\nif we could do better by switching to PPO. So this is a lot of--",
    "start": "2396020",
    "end": "2401970"
  },
  {
    "text": "it's not yet published work\nbut it's going to be out soon. So the numbers\naren't entirely final but we're just\ntrying to disentangle",
    "start": "2401970",
    "end": "2408500"
  },
  {
    "text": "what the difference\nbetween DPO and PPO is at a very empirical level.",
    "start": "2408500",
    "end": "2413940"
  },
  {
    "text": "So we're trying to answer\nif it's better or not. So what we're\ngoing to do is walk",
    "start": "2413940",
    "end": "2419250"
  },
  {
    "text": "through a series\nof design decisions and see how it affects\nthe suite of evaluations. We're starting with this Llama\n2 13B that has already been",
    "start": "2419250",
    "end": "2427980"
  },
  {
    "text": "instruction tuned. The difference between\nthe blue and the red is the gains from\ninstruction tuning. For these kind of reasoning,\ncoding, chat tasks, instruction",
    "start": "2427980",
    "end": "2436349"
  },
  {
    "text": "tuning does the biggest\ndelta that you'll see among all of these slides. Instruction tuning puts\nthe model on the map",
    "start": "2436350",
    "end": "2442140"
  },
  {
    "text": "as being useful. And it is easy to see\ngains at the beginning. And then it's harder\nand harder for us",
    "start": "2442140",
    "end": "2448440"
  },
  {
    "text": "to really keep\nimproving these models. So we start with this. We add this Anthropic helpful,\nharmless RLHF data with DPO.",
    "start": "2448440",
    "end": "2456810"
  },
  {
    "text": "And you can see that there\nis a small bump across all of the metrics that we did.",
    "start": "2456810",
    "end": "2462250"
  },
  {
    "text": "This data set is known as\nbeing particularly noisy among researchers\nin the area but it is the starting\npoint when you're",
    "start": "2462250",
    "end": "2468510"
  },
  {
    "text": "doing research on alignment. It's been around\nfor a few years. It's big. It's multi-turn. But it's known to be noisy.",
    "start": "2468510",
    "end": "2475050"
  },
  {
    "text": "And it still gives improvement. And then what you do is if\nwe switch to this data that was used for both\nZephyr and Tulu 2,",
    "start": "2475050",
    "end": "2482339"
  },
  {
    "text": "officially, this\nUltraFeedback data, we get an even bigger bump. So this is just\nshowing the difference",
    "start": "2482340",
    "end": "2488010"
  },
  {
    "text": "that changing only the data\ncan give you in a DPO recipe. It's normally\nincreases of 0% to 2%.",
    "start": "2488010",
    "end": "2496720"
  },
  {
    "text": "And in the research sphere,\ntrying to ship a model, that's a big deal. So this is where we\ntread into new territory.",
    "start": "2496720",
    "end": "2504380"
  },
  {
    "text": "Grad students worked\nreally hard and implemented PPO and [? jacks ?] in addition\nto what they already had.",
    "start": "2504380",
    "end": "2510390"
  },
  {
    "text": "And we were like, OK, what\nhappens when we add PPO? And reliably across\nmultiple experiments,",
    "start": "2510390",
    "end": "2518359"
  },
  {
    "text": "this is one example with\n13 billion parameters, PPO just happens to do a\nlittle bit better, [CHUCKLES]",
    "start": "2518360",
    "end": "2524660"
  },
  {
    "text": "like 1% better. And we tried to change\na lot of things. And the changing things is\nwhere things get a bit messier.",
    "start": "2524660",
    "end": "2532590"
  },
  {
    "text": "So we've heard from industry\nthat using a bigger reward model can be really helpful to\ngetting a better policy model.",
    "start": "2532590",
    "end": "2539970"
  },
  {
    "text": "Essentially, these bigger reward\nmodels will be better at nuance. They should label better scores\nwhich are used as rewards.",
    "start": "2539970",
    "end": "2547020"
  },
  {
    "text": "They should just make this\nprocess a little bit more stable if we have\nthe compute for it. We see that it does\nimprove some things",
    "start": "2547020",
    "end": "2554549"
  },
  {
    "text": "but it doesn't actually make\nthe model overall much better. It's flatlined with\npretty similar data",
    "start": "2554550",
    "end": "2560210"
  },
  {
    "text": "and then just making\nthe reward model bigger, which is a little\nbit surprising to us. And this is the most realistic\nfew slides of the talk.",
    "start": "2560210",
    "end": "2570700"
  },
  {
    "text": "But we did this\nthing where we even were trying to see if our\nreward model training was bad",
    "start": "2570700",
    "end": "2577140"
  },
  {
    "text": "as we scaled it up. So we used RewardBench\non the right, which I had told\nyou about earlier,",
    "start": "2577140",
    "end": "2582160"
  },
  {
    "text": "which it's not clearly\ncorrelated whether or not these two 13B be models\nor 70B be or better.",
    "start": "2582160",
    "end": "2588490"
  },
  {
    "text": "And we also did this\nBest-of-N sampling idea, which is if you generate a bunch\nof completions from a language model, you can rank them\nby your reward model",
    "start": "2588490",
    "end": "2595860"
  },
  {
    "text": "and then re-evaluate on\nthe top ranked completions. That shows that\nour reward models",
    "start": "2595860",
    "end": "2601080"
  },
  {
    "text": "are better at the bigger scale. But we couldn't get\nthis to really click into a downstream model in\na PPO notion of the world.",
    "start": "2601080",
    "end": "2609300"
  },
  {
    "text": "We even tried adding\nmore prompts to RLHF. We added more code reasoning\nprompts because that's something",
    "start": "2609300",
    "end": "2614700"
  },
  {
    "text": "that OpenAI talks\nabout a lot and we want to improve our models on.",
    "start": "2614700",
    "end": "2619920"
  },
  {
    "text": "It doesn't really\nshift the needle on this cohesive\naverage over many tasks.",
    "start": "2619920",
    "end": "2626170"
  },
  {
    "text": "In the paper, what you'll\nsee when it's out is it shows that we added prompts\nreally similar to two math",
    "start": "2626170",
    "end": "2631200"
  },
  {
    "text": "and code evaluations. And those specific\nevaluations got a bit better. But adding the full\nnoise into the fact",
    "start": "2631200",
    "end": "2637470"
  },
  {
    "text": "that some other evaluations\nmight go down makes it-- this process is really\nhard to disentangle.",
    "start": "2637470",
    "end": "2643650"
  },
  {
    "text": "And this is why we're\ngetting the 0% to 2% improvement out of PPO but DPO\ndoesn't have this sort of mess.",
    "start": "2643650",
    "end": "2651630"
  },
  {
    "text": "So what we ended\nup getting to is there's always one\nmore thing for us to ablate when you're training\nthese models with PPO.",
    "start": "2651630",
    "end": "2658840"
  },
  {
    "text": "The sort of things like\ndifferent regularization, we're learning a value function\nin RL, different warm up,",
    "start": "2658840",
    "end": "2666040"
  },
  {
    "text": "different size parameters. There's just so many\nknobs to turn in PPO. And it was reliably getting\nus a pretty good model",
    "start": "2666040",
    "end": "2673839"
  },
  {
    "text": "but we're staring into the abyss\ntrying to improve this right now and the next few months. And the bottleneck in terms\nof the actual technical side",
    "start": "2673840",
    "end": "2682740"
  },
  {
    "text": "is that PPO generates new\nresponses from the model as it trains to\nrefresh the data.",
    "start": "2682740",
    "end": "2688809"
  },
  {
    "text": "And that is by far and away the\nbiggest bottleneck when you're actually training\nthese models, is it's just way slower than DPO.",
    "start": "2688810",
    "end": "2695940"
  },
  {
    "text": "So all these resources\nfor PPO things are somewhat available\nto academics. The Google Tensor\nResearch Cloud I think",
    "start": "2695940",
    "end": "2702420"
  },
  {
    "text": "is pretty available\nto grad students I work with seem to sign up. The code base is open.",
    "start": "2702420",
    "end": "2707500"
  },
  {
    "text": "So if you're an\ninterested grad student, and you're trying\nto do PPO alignment, and have access to TPUs,\nplease get in touch.",
    "start": "2707500",
    "end": "2713950"
  },
  {
    "text": "It's a very fun can of worms. But as a summary, these\nare the many different DPO",
    "start": "2713950",
    "end": "2721260"
  },
  {
    "text": "data sets that we tried. This is almost all of the\nwell-received data sets that are out there in the open.",
    "start": "2721260",
    "end": "2727740"
  },
  {
    "text": "And they all look like\nthe factuality column. Some of these things\njust don't matter at all when you're aligning\nthese models.",
    "start": "2727740",
    "end": "2734200"
  },
  {
    "text": "So we need to get new\ndata sets that are really adding different\ncapabilities to these models",
    "start": "2734200",
    "end": "2739680"
  },
  {
    "text": "And something that matches\nthese UltraFeedback numbers at the bottom.",
    "start": "2739680",
    "end": "2744840"
  },
  {
    "text": "And I'm surprised\nwhenever I look at this. But this is where we are at.",
    "start": "2744840",
    "end": "2750510"
  },
  {
    "text": "And we need to try to\nkeep building data sets and keep adding\nfreshness to this system.",
    "start": "2750510",
    "end": "2756610"
  },
  {
    "text": "UltraFeedback, at this point,\nis maybe six months old or so. I don't know the exact age.",
    "start": "2756610",
    "end": "2761680"
  },
  {
    "text": "But in terms of people\ntraining models, that feels old to things\nthat are happening.",
    "start": "2761680",
    "end": "2767550"
  },
  {
    "text": "And these are the actual\nnumbers that you get when you compare DPO versus PPO. This is all with this\n13 billion parameter.",
    "start": "2767550",
    "end": "2775390"
  },
  {
    "text": "Again, we changed the data set. And every one of these, PPO\ncomes out a little bit better on average.",
    "start": "2775390",
    "end": "2781019"
  },
  {
    "text": "And this is a few grad\nstudents and people like me. This is not a big team\nin industry doing this.",
    "start": "2781020",
    "end": "2787100"
  },
  {
    "text": "We're scraping by. And I don't know if\nit's worth the effort. I see why OpenAI\nuses this, because we",
    "start": "2787100",
    "end": "2794067"
  },
  {
    "text": "are able to get a bit\nmore signal out of it. But it's a ton of effort to\nget a bit better signal out.",
    "start": "2794067",
    "end": "2801440"
  },
  {
    "text": "And I'll transition\ninto a bit more of a open-ended\ndiscussion of this",
    "start": "2801440",
    "end": "2807619"
  },
  {
    "text": "and then we'll have questions. But what about PPO\nis actually special?",
    "start": "2807620",
    "end": "2813510"
  },
  {
    "text": "This generation, and this online\nnature, and can we just change",
    "start": "2813510",
    "end": "2818520"
  },
  {
    "text": "DPO to be like this or where\nare the new things going to go. And I had the\npleasure of advising",
    "start": "2818520",
    "end": "2824160"
  },
  {
    "text": "one project that\nwas related to this but this is much more general. So it's like, what is\nspecial about online data.",
    "start": "2824160",
    "end": "2831269"
  },
  {
    "text": "There's multiple ways that you\ncan get new data into your RLHF process.",
    "start": "2831270",
    "end": "2836520"
  },
  {
    "text": "And then there's also\nthis related question in reinforcement learning\nliterature, which is on versus off policy, which\nis a technical distinction that",
    "start": "2836520",
    "end": "2844640"
  },
  {
    "text": "often gets looped in with these\ndiscussions of DPO versus PPO.",
    "start": "2844640",
    "end": "2849809"
  },
  {
    "text": "They're actually related\nbut the reinforcement learning discussions have\nmuch more definitional flavor",
    "start": "2849810",
    "end": "2856940"
  },
  {
    "text": "to them. While in this\nalignment space, we're more focused on if we\nneed to get fresh data in",
    "start": "2856940",
    "end": "2862970"
  },
  {
    "text": "and how we need to label our\ndata for our language models. So I'd make this distinction\nbetween these two things, which",
    "start": "2862970",
    "end": "2868340"
  },
  {
    "text": "is freshly generated\ndata from the policy. If you zoom into a data\nset like UltraFeedback, it has generations from all\nsorts of models, from Alpaca,",
    "start": "2868340",
    "end": "2876420"
  },
  {
    "text": "Vicuna, GPT-3.5, GPT-4, Llama. There's generations\nfrom all sorts of models",
    "start": "2876420",
    "end": "2882770"
  },
  {
    "text": "in this data set we're using. So when we train these\nZephyr or these Tulu models, we're incorporating information\nfrom a lot of different models",
    "start": "2882770",
    "end": "2890100"
  },
  {
    "text": "down into our one policy. Whereas what PPO\nis doing is only generating data from\nyour existing model",
    "start": "2890100",
    "end": "2896369"
  },
  {
    "text": "and changing this\ndistribution over time. So that is a very different\nidea of where the signal is",
    "start": "2896370",
    "end": "2902609"
  },
  {
    "text": "coming from from the models. And then the second\nthing is whether or not you're refreshing the\ndata labels over time.",
    "start": "2902610",
    "end": "2909430"
  },
  {
    "text": "If I have human labelers\ncomparing chosen and rejected, that's one data point. But I can also later on take\nthis reward model that I trained",
    "start": "2909430",
    "end": "2917849"
  },
  {
    "text": "and generate a chosen and\nrejected and change the label. So these two things of\nwhat the actual text is",
    "start": "2917850",
    "end": "2924250"
  },
  {
    "text": "and when the chosen/rejected\nlabel was given are what people mean when\nthey're talking about",
    "start": "2924250",
    "end": "2929370"
  },
  {
    "text": "is something special\nabout online in RLHF. And it's clear to\nsee that PPO does it",
    "start": "2929370",
    "end": "2935340"
  },
  {
    "text": "very differently than DPO but\nwe're not restricted to this. In the last few weeks--",
    "start": "2935340",
    "end": "2941070"
  },
  {
    "text": "I have the dates all in here. So [CHUCKLES] April\nand May of 2024,",
    "start": "2941070",
    "end": "2946440"
  },
  {
    "text": "there started to be a lot of\npapers on this about DPO, PPO, online, offline.",
    "start": "2946440",
    "end": "2952820"
  },
  {
    "text": "And they really say\nsimilar things, which is that online is important.",
    "start": "2952820",
    "end": "2958640"
  },
  {
    "text": "And these papers\non this slide, they show these more theoretical\nand closed-form experiments",
    "start": "2958640",
    "end": "2964190"
  },
  {
    "text": "on what is special\nabout online data and what performance\ndrops if you use this kind of offline data.",
    "start": "2964190",
    "end": "2971330"
  },
  {
    "text": "It's good to dig into these. But this is what I say,\nit's nice to do research now because if you have an\nidea, a lot of times people",
    "start": "2971330",
    "end": "2977120"
  },
  {
    "text": "have three papers that confirm\nthe notion that you have. It's a lot easier to\nbe confident in things",
    "start": "2977120",
    "end": "2983120"
  },
  {
    "text": "if three independent\ninstitutions say something similar. At the same time, there's\na lot of methods coming out",
    "start": "2983120",
    "end": "2989240"
  },
  {
    "text": "where people are trying to\nmodify DPO to actually use this kind of online notion.",
    "start": "2989240",
    "end": "2995310"
  },
  {
    "text": "I think self-rewarding\nlanguage models from Meta was the first really popular\none where they asked the DPO",
    "start": "2995310",
    "end": "3001970"
  },
  {
    "text": "model, hey, which\nof these answers is better in between\neach iteration. So they did this LLM-as-a-Judge\nto relabel their own data.",
    "start": "3001970",
    "end": "3009180"
  },
  {
    "text": "And then they did multiple\niterations of DPO. And the model had\nreally strong stores.",
    "start": "3009180",
    "end": "3014520"
  },
  {
    "text": "There's now ideas like\nnot using all of your data at once so you can do batches\nof DPO and update your data.",
    "start": "3014520",
    "end": "3020750"
  },
  {
    "text": "The paper that I was on with\nthis discriminator-guided DPO, which I'll talk\nabout in a second,",
    "start": "3020750",
    "end": "3026070"
  },
  {
    "text": "is using reward models plus\nthis DPO training objective. There's just a lot of\nthings that we can change.",
    "start": "3026070",
    "end": "3031260"
  },
  {
    "text": "And I think the\ncommunity, again, is in this expansion\nphase where I even get messages from people\nwho are like, oh, my paper",
    "start": "3031260",
    "end": "3037790"
  },
  {
    "text": "was really similar to this\nother paper that we did. At first, they didn't cite us. And I'm like, this\nis kind of the point.",
    "start": "3037790",
    "end": "3043530"
  },
  {
    "text": "But it's hard. [CHUCKLES] It's going to be like\nthis for a little bit longer. And then hopefully, in the end\nof the year, in a few years,",
    "start": "3043530",
    "end": "3049830"
  },
  {
    "text": "we're going to be like,\nOK, this is clearly what we need to do on\nthe method side of thing. So this is one example D2PO,\ndiscriminator-guided DPO,",
    "start": "3049830",
    "end": "3058770"
  },
  {
    "text": "which I was an advisor\nto [INAUDIBLE], which was an undergrad researcher. And the idea is comparing\nthese three different things.",
    "start": "3058770",
    "end": "3067560"
  },
  {
    "text": "So A is the standard DPO. You have a data set. You apply the loss\nfunction on it. B is what we call some\nsort of online preference",
    "start": "3067560",
    "end": "3075830"
  },
  {
    "text": "optimization which is\nwhere you can repeatedly label your data with\na reward model, which",
    "start": "3075830",
    "end": "3082520"
  },
  {
    "text": "is kind of like the self-reward\npaper that I mentioned which is can reshuffle your preference\ndata based on a reward model.",
    "start": "3082520",
    "end": "3089640"
  },
  {
    "text": "And that kind of adds\nsome notion of online-ness to your data. And then the third thing is,\nwhat if we're relabeling data",
    "start": "3089640",
    "end": "3096440"
  },
  {
    "text": "and we're retraining our\nreward model over time? So we're just really trying to\nkeep what our policy is doing",
    "start": "3096440",
    "end": "3103250"
  },
  {
    "text": "related to our reward model and\nkeep everything really updated in real time so that\nit's all lined up.",
    "start": "3103250",
    "end": "3109650"
  },
  {
    "text": "And this is wondering\nhow much of a gain do you have by retraining the\nreward model over time in a DPO",
    "start": "3109650",
    "end": "3115520"
  },
  {
    "text": "framework. And part of why I like\nthis paper is there's things like closed form tasks.",
    "start": "3115520",
    "end": "3122019"
  },
  {
    "text": "So the biggest question\nthat I get for alignment is how do we actually evaluate\nit, what tasks is it good for.",
    "start": "3122020",
    "end": "3129223"
  },
  {
    "text": "There's a whole\nphilosophical discussion where I think information\ntransformation is a valuable task.",
    "start": "3129223",
    "end": "3134460"
  },
  {
    "text": "Writers tell the same\nstories in different ways but the best told story\nis the one that resonates with people that has value.",
    "start": "3134460",
    "end": "3140770"
  },
  {
    "text": "But at the other\ntime, we're academics and we need to be able\nto measure things. So this paper has\nthings like, your reward",
    "start": "3140770",
    "end": "3147690"
  },
  {
    "text": "is counting the number\nof nouns in a sentence. And then you're using\nthese alignment methods to increase the number of nouns\nin the outputted sentences",
    "start": "3147690",
    "end": "3155039"
  },
  {
    "text": "from the model. So you can measure that\na lot better because we have classifiers which\nwe know are nouns. And you can see on this\nleft figure is that",
    "start": "3155040",
    "end": "3162180"
  },
  {
    "text": "just by retraining this\nreward model a few times and it converges better than\nif you were just to relabel",
    "start": "3162180",
    "end": "3168210"
  },
  {
    "text": "your preference data. It's a mouthful but it's just\nkeeping your training process",
    "start": "3168210",
    "end": "3173250"
  },
  {
    "text": "a little bit more online\ncan improve the performance. And on the right\nis a more standard open-ended evaluation\ntask where we're",
    "start": "3173250",
    "end": "3180400"
  },
  {
    "text": "asking a language model like\nChatGPT which answer is better. And that has all sorts\nof problems where",
    "start": "3180400",
    "end": "3186280"
  },
  {
    "text": "we can show similar results. I think the big takeaway is\nreally these few slides, which",
    "start": "3186280",
    "end": "3192280"
  },
  {
    "text": "is the literature is moving. [CHUCKLES] We have studies that\nshow that online is better.",
    "start": "3192280",
    "end": "3197740"
  },
  {
    "text": "And people are coming up\nwith really cool, clever ways to actually use online data. So combined with new data sets,\nthis is the DPO of this year,",
    "start": "3197740",
    "end": "3207369"
  },
  {
    "text": "it's like online methods\nand how they work. So this goes back to\nwhat industry is doing.",
    "start": "3207370",
    "end": "3214720"
  },
  {
    "text": "And I showed this figure\nearlier on the left with Claude where you can see the little\npoints along the lines.",
    "start": "3214720",
    "end": "3220300"
  },
  {
    "text": "And these different iterations. We don't know exactly\nwhat they're doing. But it seems a\nlittle bit different",
    "start": "3220300",
    "end": "3225990"
  },
  {
    "text": "where the dots on these figures\nare new data sets from humans rather than this redo or reward\nmodel, relabel your data.",
    "start": "3225990",
    "end": "3234152"
  },
  {
    "text": "This is what happens\nwhen you have access to different types of scale. The Llama 2 paper makes\nthis much clearer.",
    "start": "3234152",
    "end": "3239478"
  },
  {
    "text": "They say they work\nwith an annotator. They get batches of data. When they're generating\nthis new batch of data,",
    "start": "3239478",
    "end": "3244840"
  },
  {
    "text": "the previous model's checkpoint\nwas used for generations. They do this many times.",
    "start": "3244840",
    "end": "3249940"
  },
  {
    "text": "And you can see that they're\ncollecting new human data, new human data, new human data. And each time they\ngenerate human data,",
    "start": "3249940",
    "end": "3256420"
  },
  {
    "text": "it is trained for a new model. They're doing a lot\nof training updates. And they're kind of\nbuilding on each other.",
    "start": "3256420",
    "end": "3262570"
  },
  {
    "text": "And this kind of leads into\nthe last section that I'll talk about in the\nconclusions is,",
    "start": "3262570",
    "end": "3267880"
  },
  {
    "text": "what did Meta do with Llama 3. [CHUCKLES] This is one of the\nmost funny blog post sentences.",
    "start": "3267880",
    "end": "3273319"
  },
  {
    "text": "It's the ridiculous\nthings that they give us and then we parse\nthe tea leaves. They say in the blog\npost is that our approach",
    "start": "3273320",
    "end": "3279940"
  },
  {
    "text": "to post-training\nis a combination of supervised fine-tuning,\nrejection sampling, Proximal Policy Optimization, PPO, and\nDirect Preference Optimization.",
    "start": "3279940",
    "end": "3288260"
  },
  {
    "text": "So the people ask me like,\nwhat the heck did they do? And I mean, I kind of agree. But it really goes\nback to this slide",
    "start": "3288260",
    "end": "3295210"
  },
  {
    "text": "in my mind, which is that\nthey're getting new data and then they're training\na new model over time.",
    "start": "3295210",
    "end": "3301730"
  },
  {
    "text": "So what I think is happening\nat each one of these points, they tried a few methods and\nthey chose the training method",
    "start": "3301730",
    "end": "3308260"
  },
  {
    "text": "that worked best. It's practical. Meta is a really\npractical organization, especially in the\ngenAI work right now.",
    "start": "3308260",
    "end": "3314630"
  },
  {
    "text": "And that just makes sense. At different points\nin the model, your model has\ndifferent capabilities and it's ready to be\ntrained in different ways.",
    "start": "3314630",
    "end": "3321799"
  },
  {
    "text": "Rejection sampling, which\nI didn't cover here, is the simplest training method. You take a reward model, you\nrank some supervised fine-tuning",
    "start": "3321800",
    "end": "3329800"
  },
  {
    "text": "outputs, and then you use this\nautoregressive loss function again. And then from there, DPO\nis much simpler to PPO",
    "start": "3329800",
    "end": "3337640"
  },
  {
    "text": "but it might not give you\nthe highest end performance. And then as your model really\nstarts kicking into gear",
    "start": "3337640",
    "end": "3343600"
  },
  {
    "text": "or you have more time to train\nthis model once all of your data is collected and you're not\non a weekly time crunch,",
    "start": "3343600",
    "end": "3349069"
  },
  {
    "text": "you can experiment with\nall the little knobs of PPO and you can really try\nto get the best model out",
    "start": "3349070",
    "end": "3354770"
  },
  {
    "text": "at the end of the day. Hopefully they release\na technical report that confirms some of my hypotheses.",
    "start": "3354770",
    "end": "3360890"
  },
  {
    "text": "But I think this\nis normally what people are interested in\nwhen somebody from industry comes up to give a lecture.",
    "start": "3360890",
    "end": "3366910"
  },
  {
    "text": "And I wish we had more details\non what industry was doing.",
    "start": "3366910",
    "end": "3372910"
  },
  {
    "text": "But in terms of\ncurrent directions that I'm most interested in\nRLHF, I talked about data lot.",
    "start": "3372910",
    "end": "3379520"
  },
  {
    "text": "We are very\nbottlenecked on data. Even as academics with\nvery limited compute,",
    "start": "3379520",
    "end": "3384529"
  },
  {
    "text": "we literally try every\ndata set that is available. We don't have a lot\nof compute but we",
    "start": "3384530",
    "end": "3390340"
  },
  {
    "text": "need to keep innovating there. We're going to see\nmore DPO methods.",
    "start": "3390340",
    "end": "3396119"
  },
  {
    "text": "It's here to say. There's a ton that\nI didn't cover here, things like removing\nthe reference model,",
    "start": "3396120",
    "end": "3402460"
  },
  {
    "text": "changing the loss\nfunction slightly, not using pairwise preferences\nbut single-wise preferences.",
    "start": "3402460",
    "end": "3409810"
  },
  {
    "text": "There's a lot going on there. We should use more model\nsizes than 7 and 13 billion parameters.",
    "start": "3409810",
    "end": "3415210"
  },
  {
    "text": "Or Llama's case 7 and\n70 billion parameters. Particularly scaling down\nis very useful as a place",
    "start": "3415210",
    "end": "3422080"
  },
  {
    "text": "where academia can still play. There's less of a\nweird marketing dynamic",
    "start": "3422080",
    "end": "3427322"
  },
  {
    "text": "where all the companies\nare racing to go bigger for certain strategic reasons. But this is something that's\naccessible to many people.",
    "start": "3427322",
    "end": "3434280"
  },
  {
    "text": "Aligning small models,\nit's hard to get signal out of them because the models\nshow more or less random scores",
    "start": "3434280",
    "end": "3440660"
  },
  {
    "text": "on many benchmarks that people\ncare about or really low scores. So even just kind\nof breaking through in that domain would\nbe really impactful",
    "start": "3440660",
    "end": "3447680"
  },
  {
    "text": "work to get more people\nworking on alignment. And then evaluations\nI covered at length,",
    "start": "3447680",
    "end": "3453240"
  },
  {
    "text": "which is we need to keep\ngetting more specific on things we care about. And personalization is\nsomething in alignment",
    "start": "3453240",
    "end": "3459440"
  },
  {
    "text": "that I didn't\ncover in this talk. But it is something\nthat is good to compete",
    "start": "3459440",
    "end": "3465020"
  },
  {
    "text": "with this kind of big tech,\nwhich is how do we train models that are good for\nyou as an individual",
    "start": "3465020",
    "end": "3470030"
  },
  {
    "text": "rather than one big model for\none big technology organization.",
    "start": "3470030",
    "end": "3475330"
  },
  {
    "text": "So these slides will get to you. But these are the\ntypes of places that I follow when I'm\ntrying to see open models",
    "start": "3475330",
    "end": "3482440"
  },
  {
    "text": "or open data sets that are\nreputable and easy to keep track of so you don't have to\ntry to follow everyone.",
    "start": "3482440",
    "end": "3489260"
  },
  {
    "text": "And I write about this a\nlot without doing too much self-promotion. But I ended 10 minutes\nearly for questions",
    "start": "3489260",
    "end": "3497110"
  },
  {
    "text": "that I'm happy to\ntake in a Q&A format. But you don't have to stay\nand wait if you don't want to.",
    "start": "3497110",
    "end": "3505368"
  },
  {
    "text": "[APPLAUSE] ",
    "start": "3505368",
    "end": "3515329"
  },
  {
    "text": "OK. Thank you, Nathan. Questions. Anyone got questions? ",
    "start": "3515330",
    "end": "3522390"
  },
  {
    "text": "[INAUDIBLE] is a good\nreward model, which is a large assumption, I agree. But what is the key challenge\nto doing online DPO in the sense",
    "start": "3522390",
    "end": "3529250"
  },
  {
    "text": "that you can do N rollouts\nand then just rank them using your reward model and then go.",
    "start": "3529250",
    "end": "3534290"
  },
  {
    "text": "And you can iterate\nthis that way? So what is the hard\nthing about [INAUDIBLE].",
    "start": "3534290",
    "end": "3541100"
  },
  {
    "text": "Yeah, I'm going to repeat the\nquestion so that people can hear them and it gets recorded. The idea is if you have\na good reward model, what",
    "start": "3541100",
    "end": "3548690"
  },
  {
    "text": "is stopping you from\ndoing online DPO and just improving\nthe policy from there?",
    "start": "3548690",
    "end": "3554580"
  },
  {
    "text": "I think there's\nmultiple angles to this that they're both technical\nand industry-wide.",
    "start": "3554580",
    "end": "3562822"
  },
  {
    "text": "But the technical thing is I\nthink the prompt matching ends up being really important. So prompt matching, so what\nyour reward model can learn",
    "start": "3562822",
    "end": "3571400"
  },
  {
    "text": "is specific to the prompts. There's a technical detail\nwhere the prompts used for your policy often are\nexactly the same as your reward",
    "start": "3571400",
    "end": "3578660"
  },
  {
    "text": "model in PPO, which is\nreally strange because we talk about generalization\nin machine learning. But we're soft-balling ourselves\nat the PPO stage, which",
    "start": "3578660",
    "end": "3586920"
  },
  {
    "text": "is we're only grading PPO\nanswers which our reward models trained to answer\nwhich is kind of strange.",
    "start": "3586920",
    "end": "3592869"
  },
  {
    "text": "So people think that some\nof that might break down. And we see some of\nthat when trying",
    "start": "3592870",
    "end": "3598019"
  },
  {
    "text": "to train PPO models with\noff-the-shelf reward models, which is kind of a long answer.",
    "start": "3598020",
    "end": "3603035"
  },
  {
    "text": " But I think it's mostly\ndistribution matching,",
    "start": "3603035",
    "end": "3609519"
  },
  {
    "text": "if I had to guess. But if we had\ntruly a good model, it should work for some things.",
    "start": "3609520",
    "end": "3614920"
  },
  {
    "text": "And that could be\none of the reasons why there aren't\nthat many in the open because it would\nhelp people catch up",
    "start": "3614920",
    "end": "3619980"
  },
  {
    "text": "in alignment as a reward model. If it is as important as people\nsay it is, it might be easy.",
    "start": "3619980",
    "end": "3625234"
  },
  {
    "text": " Other questions?",
    "start": "3625235",
    "end": "3630570"
  },
  {
    "text": "Yeah? [INAUDIBLE] always good to\nsee these pairwise comparisons",
    "start": "3630570",
    "end": "3638549"
  },
  {
    "text": "or are there [INAUDIBLE] that\nyou can use where you still have these [INAUDIBLE] gradients\n[INAUDIBLE] because it's more",
    "start": "3638550",
    "end": "3646320"
  },
  {
    "text": "complicated than a pairwise? [INAUDIBLE] ",
    "start": "3646320",
    "end": "3655530"
  },
  {
    "text": "A story, for example, is\nmeasured by the [INAUDIBLE]? ",
    "start": "3655530",
    "end": "3661500"
  },
  {
    "text": "Yeah, I think this is\na whole conversation. So if I don't cover it-- if\nyou want more after I answer,",
    "start": "3661500",
    "end": "3667440"
  },
  {
    "text": "you can come up. But the question is, is there\nmore than pairwise preferences that could be used in RLHF?",
    "start": "3667440",
    "end": "3673200"
  },
  {
    "text": "And there's a lot of\ndifferent lines of work that are studying this. One is methods like there is a\nmethod out of Stanford that's",
    "start": "3673200",
    "end": "3680980"
  },
  {
    "text": "KTO. I always mess it up. These names are so\nhard to pronounce. But it's the idea of using\none-sided preference data.",
    "start": "3680980",
    "end": "3689230"
  },
  {
    "text": "So a lot of customer\napps have like, did you get good support from\nthis agent, yes or no.",
    "start": "3689230",
    "end": "3694760"
  },
  {
    "text": "And you could use\ndata like that. It just is a different\nloss function for using single-sided\npreferences or just yes or no.",
    "start": "3694760",
    "end": "3702260"
  },
  {
    "text": "There are other\nthings like learning to rank for multiple answers. So this is something\nI slightly insinuated.",
    "start": "3702260",
    "end": "3711510"
  },
  {
    "text": "Binary preferences is--\nthere's a lot of literature on learning preferences. And one of the models\nthat I got reduced down",
    "start": "3711510",
    "end": "3719740"
  },
  {
    "text": "is the Starling model. And they use a\nK-wise preference. So they have five or nine\nanswers to every prompt.",
    "start": "3719740",
    "end": "3727150"
  },
  {
    "text": "And then they collect answers. And then they have a\ndifferent loss function. And this is one of the models\nthat has broken through",
    "start": "3727150",
    "end": "3733360"
  },
  {
    "text": "in the open alignment space. It's one of the few that I left\nin and skipped in my slide deck. So that's kind of interesting.",
    "start": "3733360",
    "end": "3739670"
  },
  {
    "text": "And then there's\nother research that's like fine-grained preferences. So for every\ncompletion to a prompt,",
    "start": "3739670",
    "end": "3746780"
  },
  {
    "text": "you get labels like conciseness,\nhelpfulness, honesty. So there's a few\nthings on that regards.",
    "start": "3746780",
    "end": "3752930"
  },
  {
    "text": "There's a SteerLM\npaper from NVIDIA and then there's\nwork from UW that",
    "start": "3752930",
    "end": "3758170"
  },
  {
    "text": "does learning from\nfine-grained preferences. So that one is\nprobably the one that's",
    "start": "3758170",
    "end": "3763869"
  },
  {
    "text": "emerging most in\nthe academic sense. But there's so\nmuch to learn here. Literally all the\nfield of social choice",
    "start": "3763870",
    "end": "3770890"
  },
  {
    "text": "needs to get condensed\ninto these things. ",
    "start": "3770890",
    "end": "3782300"
  },
  {
    "text": "Any other questions? ",
    "start": "3782300",
    "end": "3788550"
  },
  {
    "text": "Yeah, [? perfect. ?] It's kind of a general question. [INAUDIBLE] ",
    "start": "3788550",
    "end": "3804329"
  },
  {
    "text": "Yeah so the question is, how\ncan we-- broadly it's like, how can we exceed\nhuman performance",
    "start": "3804330",
    "end": "3809400"
  },
  {
    "text": "with fine-tuning or any\ntraining for that regards? And I think this is where\nsome older ideas in CS",
    "start": "3809400",
    "end": "3815640"
  },
  {
    "text": "will come back. I think one of the\nfoundational ideas in CS is search, which is really also\nmotivated as exploration in RL.",
    "start": "3815640",
    "end": "3823180"
  },
  {
    "text": "And therefore, we need to have\nsome sort of language models that can search and\ngenerate new data.",
    "start": "3823180",
    "end": "3829020"
  },
  {
    "text": "I was talking with somebody\nbefore, a grad student. And I think that search would be\na large part of synthetic data.",
    "start": "3829020",
    "end": "3835780"
  },
  {
    "text": "But then the human aspect will\nbe what gets it across the line if it can't solve\na certain area. And the QStar rumors\nare ridiculous",
    "start": "3835780",
    "end": "3843520"
  },
  {
    "text": "but that seems to be the best\nargument for the sort of thing that OpenAI is trying with that.",
    "start": "3843520",
    "end": "3850910"
  },
  {
    "text": "This is, like, how to get\nthat barrier broken with AI. ",
    "start": "3850910",
    "end": "3859040"
  },
  {
    "text": "Thank you so much for coming in. You mentioned data sets\nwere a big limitation. And I was curious how one goes\nabout creating a new data set?",
    "start": "3859040",
    "end": "3868460"
  },
  {
    "text": "Yeah, this is another\nthing that's hard. I think community efforts are\nwhat people have tried to do.",
    "start": "3868460",
    "end": "3874140"
  },
  {
    "text": "I mentioned Open Assistant. But most people that do a\ncommunity effort are like, I never want to do this again.",
    "start": "3874140",
    "end": "3879960"
  },
  {
    "text": "So while I still think it's\nworth doing things once that are highly impactful,\neven if you might not",
    "start": "3879960",
    "end": "3886550"
  },
  {
    "text": "want to do it again. Other avenues for building\nthese in a sustainable manner",
    "start": "3886550",
    "end": "3891770"
  },
  {
    "text": "are very important. I think that there's some\nways that this is being done,",
    "start": "3891770",
    "end": "3896910"
  },
  {
    "text": "like Chatbot Arena returns some\nof the prompts and the labels to users. There's specific\nconcerns I have with that",
    "start": "3896910",
    "end": "3903260"
  },
  {
    "text": "data around being too noisy. But that is the\nsort of thing that can happen if Ai2 has a\ndemo for their models,",
    "start": "3903260",
    "end": "3910980"
  },
  {
    "text": "it's going to be about science\nand generating information rather than being a\nChatGPT competitor.",
    "start": "3910980",
    "end": "3916980"
  },
  {
    "text": "It's a non-profit. It can't do a\nproduct competitor. But that's the sort of data\nthat we would want to release. And something that\nI might just have",
    "start": "3916980",
    "end": "3923930"
  },
  {
    "text": "to do but I'm interested\nin academic workshops and competitions as a ground\nwhere you could have communities",
    "start": "3923930",
    "end": "3930120"
  },
  {
    "text": "meet every three,\nsix, eight months and have work that's\nfocused on that area and/or focus time to have\npeople contribute to it.",
    "start": "3930120",
    "end": "3937539"
  },
  {
    "text": "But it's a good question. It's probably why\nthere aren't very many. ",
    "start": "3937540",
    "end": "3944450"
  },
  {
    "text": "Yeah. How do you feel-- Reward models are subject\nto reward hacking as well?",
    "start": "3944450",
    "end": "3953600"
  },
  {
    "text": "We had one person-- Close first and then\nwe'll come to you. The various places that you've\ndone research at over the years,",
    "start": "3953600",
    "end": "3961200"
  },
  {
    "text": "do you have any\nsense of how they compare in terms of\nspecifically alignment research?",
    "start": "3961200",
    "end": "3968580"
  },
  {
    "text": "I mean, obviously, they\nweren't doing a lot of research specifically at\nthose at the time.",
    "start": "3968580",
    "end": "3974980"
  },
  {
    "text": "I think, generally, it\nrepresents different culture and investments of the company. I wasn't doing language models\nuntil the time at Hugging Face.",
    "start": "3974980",
    "end": "3982820"
  },
  {
    "text": "So I can really only speak\nto these two open companies. And from Hugging\nFace perspective,",
    "start": "3982820",
    "end": "3989560"
  },
  {
    "text": "is to show that more\npeople can do this. We're not trying to\ncompete with ChatGPT but we're trying to foster\nan ecosystem of doing this.",
    "start": "3989560",
    "end": "3995930"
  },
  {
    "text": "And Ai2 is similar\nbut more about what is happening, like how do\nwe learn about this, how do we do science, how do we\nstudy the science of this",
    "start": "3995930",
    "end": "4003138"
  },
  {
    "text": "and communicate that clearly. And I'm sure if you\ndo the exercise, you can map this\nto every company.",
    "start": "4003138",
    "end": "4008632"
  },
  {
    "text": "It's like, What? Is their important thing. And they have different\ngoals in their products and their corporate structure\nand things like that.",
    "start": "4008633",
    "end": "4016347"
  },
  {
    "text": "I will talk more\nwhen not recorded. [LAUGHTER] OK, [? up ?] the back.",
    "start": "4016347",
    "end": "4023049"
  },
  {
    "text": "So are the reward models also\nsubject to reward hacking? They achieve a good\nresult on the outcome",
    "start": "4023050",
    "end": "4030230"
  },
  {
    "text": "but in reality, the outcome\nwas not fully expected.",
    "start": "4030230",
    "end": "4035890"
  },
  {
    "text": "Yeah. So when talking\nabout reward models, this is probably the most\nestablished line of work.",
    "start": "4035890",
    "end": "4040910"
  },
  {
    "text": "The question is like, are reward\nmodel subject to reward hacking? And reward hacking is a\nclassic problem in RL.",
    "start": "4040910",
    "end": "4047359"
  },
  {
    "text": "I should bring back\nfrom my RL slides where we have the boat\nswimming, going in circles and then be like, this\nhappens to your language model",
    "start": "4047360",
    "end": "4053458"
  },
  {
    "text": "and what happens. But it is. And there's a lot of\nresearch to mitigate it but it's a fundamental\nproblem, which",
    "start": "4053458",
    "end": "4059290"
  },
  {
    "text": "is you have a very\npowerful optimizer and you have a incomplete\nrepresentation of your reward.",
    "start": "4059290",
    "end": "4064300"
  },
  {
    "text": "And it will always find where\nyour representation of reward is wrong. So we will always be\ndoing the best we can",
    "start": "4064300",
    "end": "4070520"
  },
  {
    "text": "but I think saying it's perfect,\nit's not possible in the math. ",
    "start": "4070520",
    "end": "4082150"
  },
  {
    "text": "I mean, I can also\nsay the ways that it fails are pretty funny because\nif you train these models, you'll end up with a model\nthat just says JavaScript",
    "start": "4082150",
    "end": "4088467"
  },
  {
    "text": "to every answer to [INAUDIBLE]. Sometimes, it's really easy\nto see when that is happening,",
    "start": "4088467",
    "end": "4094250"
  },
  {
    "text": "which is good. Or you could change\nyour loss function so that it will always exploit.",
    "start": "4094250",
    "end": "4099259"
  },
  {
    "text": "And it's a good way to make sure\nthat things are working, which [CHUCKLES] you should be\nable to easily exploit",
    "start": "4099260",
    "end": "4105159"
  },
  {
    "text": "if you turn the brakes off. OK. ",
    "start": "4105160",
    "end": "4111210"
  },
  {
    "text": "Any last public question? ",
    "start": "4111210",
    "end": "4118120"
  },
  {
    "text": "If not, thank you for\nNathan for giving this talk. [APPLAUSE]",
    "start": "4118120",
    "end": "4124532"
  },
  {
    "text": " And if there's anything you'd\nlike to ask off the record, he'll be here for a bit longer.",
    "start": "4124532",
    "end": "4130409"
  },
  {
    "text": "[CHUCKLES] ",
    "start": "4130409",
    "end": "4137000"
  }
]