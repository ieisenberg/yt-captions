[
  {
    "start": "0",
    "end": "5600"
  },
  {
    "text": "All right, let's get started. So the plan for\ntoday is to finish up",
    "start": "5600",
    "end": "12290"
  },
  {
    "text": "the material we didn't\ncover in the last lecture on autoregressive models, and\nthen we'll talk about learning.",
    "start": "12290",
    "end": "20120"
  },
  {
    "text": "So towards the end\nof the last lecture, we talked about RNNs\nas being another way",
    "start": "20120",
    "end": "26510"
  },
  {
    "text": "to parameterize\nautoregressive models. And remember, the\nkey idea is that you have a small number of\nparameters, actually",
    "start": "26510",
    "end": "34250"
  },
  {
    "text": "a constant number of\nparameters, with respect to the length of the sequence\nyou're trying to model.",
    "start": "34250",
    "end": "40309"
  },
  {
    "text": "And you're going to use these\nparameters to basically keep track of the context\nthat you use to predict,",
    "start": "40310",
    "end": "49400"
  },
  {
    "text": "basically, the next\ntoken or the next pixel. And you keep track of\nall this information",
    "start": "49400",
    "end": "54620"
  },
  {
    "text": "through a single\nhidden vector that is supposed to summarize all\nthe information that you've",
    "start": "54620",
    "end": "61443"
  },
  {
    "text": "seen so far and that\nyou're going to use to make the next prediction. Like, in this example here,\nwhere I'm looking at, let's say,",
    "start": "61443",
    "end": "68240"
  },
  {
    "text": "building an RNN to model text. So you have tokens, and\nyou might have some prefix,",
    "start": "68240",
    "end": "74840"
  },
  {
    "text": "like \"My friend open\nthe,\" and then you're going to use all\nthis information.",
    "start": "74840",
    "end": "80740"
  },
  {
    "text": "You pass it through\nyour RNN, and the RNN will update its state,\nits hidden vector, and you end up with a\nhidden vector H4 here.",
    "start": "80740",
    "end": "89330"
  },
  {
    "text": "And then you're going\nto use that vector to predict the next token. And maybe if you're\ndoing a good job,",
    "start": "89330",
    "end": "94750"
  },
  {
    "text": "then you'll put high\nprobability to reasonable ways to continue this sentence,\nlike the door or the window,",
    "start": "94750",
    "end": "101170"
  },
  {
    "text": "and you're going to put\nlow probability to things that don't make sense. And as we've seen,\nthese RNN models",
    "start": "101170",
    "end": "107530"
  },
  {
    "text": "work reasonably well, even if\nyou build them at the character level, which is pretty hard. One challenge is that this\nsingle hidden vector that you",
    "start": "107530",
    "end": "116890"
  },
  {
    "text": "have here, basically, has to\nsummarize all the information that you've seen,\nso far, and that's",
    "start": "116890",
    "end": "123369"
  },
  {
    "text": "the only thing you can use\nto make the next prediction. And that can be a\nproblem because you",
    "start": "123370",
    "end": "129070"
  },
  {
    "text": "have to do a pretty good job\nof summarizing the meaning. Let's say if you're\nbuilding a language model,",
    "start": "129070",
    "end": "134430"
  },
  {
    "text": "this single vector has to\ncapture all the entire meaning of all the previous elements\nin the sequence, which",
    "start": "134430",
    "end": "143360"
  },
  {
    "text": "can be challenging. The other problem of RNNs\nis that, basically, you",
    "start": "143360",
    "end": "148670"
  },
  {
    "text": "have to unroll the\ncomputation, if you want to compute\nthese probabilities, and you want to come up with\nreasonable losses at training",
    "start": "148670",
    "end": "155720"
  },
  {
    "text": "time, which makes them pretty\nslow and pretty hard to train. And the other problem\nis that, yeah, they",
    "start": "155720",
    "end": "162110"
  },
  {
    "text": "can be a bit problematic\nto train because you have",
    "start": "162110",
    "end": "167240"
  },
  {
    "text": "this long dependencies\nfrom, let's say, early on in the sequence\ntowards the, let's say,",
    "start": "167240",
    "end": "174050"
  },
  {
    "text": "the present. It can take many, many\nupdates to get there. And this can lead to exploding\nor vanishing gradients,",
    "start": "174050",
    "end": "180930"
  },
  {
    "text": "and it can be problematic. So this is now what's actually\nbeen used in state-of-the-art",
    "start": "180930",
    "end": "186440"
  },
  {
    "text": "language model--\nautoregressive language models. Existing state-of-the-art\nmodels use attention,",
    "start": "186440",
    "end": "192290"
  },
  {
    "text": "and the basic idea is that they\nlook more like a NADE or like a MADE, and these other\nmodels that we've seen before,",
    "start": "192290",
    "end": "200630"
  },
  {
    "text": "where you essentially are able\nto use the entire sequence",
    "start": "200630",
    "end": "205700"
  },
  {
    "text": "of inputs up to time T to\nmake the next prediction. And so instead of just\nusing the hidden vector,",
    "start": "205700",
    "end": "213140"
  },
  {
    "text": "corresponding to the last time\nstep to make the prediction, you look at all the hidden\nvectors from previous time steps",
    "start": "213140",
    "end": "221720"
  },
  {
    "text": "to predict what's\ngoing to come next. And the way to make this\neffective in practice",
    "start": "221720",
    "end": "228110"
  },
  {
    "text": "is to use an attention\nmechanism to try to figure out which parts, which elements\nof this sequence are useful",
    "start": "228110",
    "end": "237620"
  },
  {
    "text": "and which ones are not, which\none you should pay attention to, and which one you\nshouldn't pay attention to when you make a prediction.",
    "start": "237620",
    "end": "244860"
  },
  {
    "text": "And so roughly, at\na very high level, the way these methods work is\nthat there is some attention",
    "start": "244860",
    "end": "250490"
  },
  {
    "text": "mechanism that will tell you\nhow relevant a query vector is,",
    "start": "250490",
    "end": "258648"
  },
  {
    "text": "with respect to a key vector. So this is similar to when\nyou search in a database.",
    "start": "258649",
    "end": "265099"
  },
  {
    "text": "You have a query. You have a set of keys,\nand you want to figure out want to do retrieval. This has a similar flavor,\nand it will basically tell you",
    "start": "265100",
    "end": "272240"
  },
  {
    "text": "how relevant is\nthe hidden vector, let's say, corresponding\nto the first time step,",
    "start": "272240",
    "end": "277460"
  },
  {
    "text": "with respect to\nthe hidden vector that you have at the\ncurrent time step. And this could be something\nas similar as just",
    "start": "277460",
    "end": "283889"
  },
  {
    "text": "taking a dot product\nbetween the two vectors. Once you have the\nsimilarity vectors, then you turn them into\nan attention distribution,",
    "start": "283890",
    "end": "291442"
  },
  {
    "text": "which is the thing that we\nwere talking about before, the thing that tells you which\nelements of the sequence matter",
    "start": "291442",
    "end": "297150"
  },
  {
    "text": "and which ones don't. And one simple way to do it is\nto just take all these attention scores and pass them\nthrough a softmax",
    "start": "297150",
    "end": "304110"
  },
  {
    "text": "to get an actual distribution. Yeah? ",
    "start": "304110",
    "end": "309702"
  },
  {
    "text": "[INAUDIBLE] Are we not assuming\nany conditional independence?",
    "start": "309702",
    "end": "315680"
  },
  {
    "text": "So the question is\nwhether this kind of model assumes conditional\nindependence? If you build a model\nlike this, again, there",
    "start": "315680",
    "end": "322610"
  },
  {
    "text": "is no conditional independence\nexplicitly stated. Because in principle,\nas long as this",
    "start": "322610",
    "end": "331490"
  },
  {
    "text": "is just an autoregressive\nmodel, and we're just parameterizing the\nconditionals using",
    "start": "331490",
    "end": "336590"
  },
  {
    "text": "a function that has a very\nspecific functional form. And so we're not going\nto be able to capture",
    "start": "336590",
    "end": "342620"
  },
  {
    "text": "all possible dependencies,\nbut we're not explicitly making any\nconditional independence",
    "start": "342620",
    "end": "347630"
  },
  {
    "text": "assumption, so far.  Well, if you were to make\nconditional independence",
    "start": "347630",
    "end": "354230"
  },
  {
    "text": "assumptions, yet,\ntypically, performance would drop significantly. As we'll see, the nice thing\nabout this architecture",
    "start": "354230",
    "end": "361400"
  },
  {
    "text": "is that it allows you to take\ninto account the full context when you make a prediction,\nwhile, at the same time,",
    "start": "361400",
    "end": "368360"
  },
  {
    "text": "being selective and be able\nto ignore things that are not relevant and pay attention\nto things that are relevant.",
    "start": "368360",
    "end": "374870"
  },
  {
    "text": "For example, in this simplified\nversion of an attention mechanism, what you\ncould do is, you",
    "start": "374870",
    "end": "380840"
  },
  {
    "text": "could take an average\nof the hidden vectors that you've seen\nbefore in your RNN.",
    "start": "380840",
    "end": "386210"
  },
  {
    "text": "And you weigh them with the\nattention distribution scores that you have.",
    "start": "386210",
    "end": "391370"
  },
  {
    "text": "You average them, and\nyou get a new vector, then you're going to combine\nit with the current vector to make a prediction\nfor the next token.",
    "start": "391370",
    "end": "398220"
  },
  {
    "text": "And you see that now. We're no longer bottlenecked. We're not just using this green\nvector to make the prediction.",
    "start": "398220",
    "end": "403650"
  },
  {
    "text": "We're able to use\nthe whole history. So we're able to really compare\nevery pair, essentially,",
    "start": "403650",
    "end": "409310"
  },
  {
    "text": "of tokens in the sequence. And that's pretty,\npretty powerful. And as you can see, for example,\nin this little example here,",
    "start": "409310",
    "end": "420060"
  },
  {
    "text": "I have a robot must\nobey the orders given.",
    "start": "420060",
    "end": "425460"
  },
  {
    "text": "And then you need to\nmake a prediction. And if you want to\nmake a prediction,",
    "start": "425460",
    "end": "430509"
  },
  {
    "text": "you need to figure out\nwhat \"it\" refers to. And the attention\nmechanism can help",
    "start": "430510",
    "end": "435970"
  },
  {
    "text": "you to figure out that this\n\"it\" is probably referring to, that when you're trying to\nfigure out what \"it\" means,",
    "start": "435970",
    "end": "441880"
  },
  {
    "text": "you should pay attention to\nthese two tokens, a robot. And so that's the flavor\nof why this attention",
    "start": "441880",
    "end": "448930"
  },
  {
    "text": "mechanism is helpful. Because you can take advantage\nof the whole sequence.",
    "start": "448930",
    "end": "454840"
  },
  {
    "text": "As usual, in practice,\nyou need to be careful about making sure that\nthe model is autoregressive.",
    "start": "454840",
    "end": "461910"
  },
  {
    "text": "So you cannot pay attention to\nfuture vectors when you do these",
    "start": "461910",
    "end": "467320"
  },
  {
    "text": "kind of things. So you have to use\na mask mechanism, just like you made, just\nlike in these other models",
    "start": "467320",
    "end": "473259"
  },
  {
    "text": "so that you can only\nbasically pay attention to the tokens or the\nrandom variables that",
    "start": "473260",
    "end": "479380"
  },
  {
    "text": "come before it in the\nsequence, in the ordering.",
    "start": "479380",
    "end": "484390"
  },
  {
    "text": "The other thing\nthat is important is that in an actual system\nthat is used in practice,",
    "start": "484390",
    "end": "489860"
  },
  {
    "text": "you would not use any\nrecurrent architecture. So you wouldn't\nyou wouldn't even",
    "start": "489860",
    "end": "495260"
  },
  {
    "text": "need this recurrent\ncomputation here, where you update the state,\nrecursively using an RNN.",
    "start": "495260",
    "end": "503870"
  },
  {
    "text": "You just use feed\nforward computation. You stack multiple\nlayers of attention.",
    "start": "503870",
    "end": "509690"
  },
  {
    "text": "And the key advantage\nof this is that we're back to the previous MADE-like\nsetting, where you can actually",
    "start": "509690",
    "end": "518030"
  },
  {
    "text": "evaluate. You can evaluate the\narchitecture in parallel,",
    "start": "518030",
    "end": "523909"
  },
  {
    "text": "so you can do the\ncomputation necessary to make a prediction at every index,\nin parallel across indexes.",
    "start": "523909",
    "end": "533930"
  },
  {
    "text": "This is a training\ntime, of course. And this is really what\nmakes these systems, these models good in\npractice compared to an RNN.",
    "start": "533930",
    "end": "541290"
  },
  {
    "text": "I think, actually, an RNN\nwould be reasonably good in terms of modeling power.",
    "start": "541290",
    "end": "546650"
  },
  {
    "text": "It's just too slow to train. And these transformers,\nbecause they allow for massive\nparallelism, and that was--",
    "start": "546650",
    "end": "554620"
  },
  {
    "text": "and we'll come back to\nthis when we talk exactly how these models are trained. But the key advantage is\nthat you can basically",
    "start": "554620",
    "end": "560250"
  },
  {
    "text": "evaluate the loss\nvery efficiently without having to unroll\nthe recursion corresponding to an RNN.",
    "start": "560250",
    "end": "566230"
  },
  {
    "text": "And that's why they\nare one of the reasons they've achieved this\ngreat success in practice is because they can be\nevaluated in parallel.",
    "start": "566230",
    "end": "573420"
  },
  {
    "text": "They can take advantage of\nGPUs, and you can scale them to very large sizes.",
    "start": "573420",
    "end": "578840"
  },
  {
    "text": "And you can see some of the\ndemos of the systems that-- like the GPT, GPT-2, 3, 4 that\nwe've seen in the first lecture,",
    "start": "578840",
    "end": "590630"
  },
  {
    "text": "that the amazing LLMs that\neverybody's talking about.",
    "start": "590630",
    "end": "596630"
  },
  {
    "text": "Llama, other systems that\nare available online, you can play around\nwith, are essentially based on these kind of\non this architecture.",
    "start": "596630",
    "end": "604000"
  },
  {
    "text": "Autoregressive models using\nthis self-attention mechanism that we're going to\ntalk about more in one",
    "start": "604000",
    "end": "609620"
  },
  {
    "text": "of the section that is\ngoing to be dedicated to two neural architectures. So this is the high-level idea\nof one of the key ingredients",
    "start": "609620",
    "end": "619730"
  },
  {
    "text": "that is behind state-of-the-art\nlanguage models. ",
    "start": "619730",
    "end": "627089"
  },
  {
    "text": "Cool. Now back to RNNs. People have been using\nthem not only for text.",
    "start": "627090",
    "end": "633240"
  },
  {
    "text": "You can use them\nto model images. So you can just think of an\nimage as a sequence of pixels. You can generate them in top\nleft to bottom right, one",
    "start": "633240",
    "end": "643080"
  },
  {
    "text": "at a time, and you can\nuse RNN to basically model all the conditionals in\nyour autoregressive model.",
    "start": "643080",
    "end": "650370"
  },
  {
    "text": "So each pixel, you're going to\nhave one conditional per pixel, giving you the\ndistribution of that pixel,",
    "start": "650370",
    "end": "656070"
  },
  {
    "text": "given all the ones that come\nbefore it in the sequence. And each conditional\nis going to be",
    "start": "656070",
    "end": "662519"
  },
  {
    "text": "a categorical distribution\nover the colors that, that pixel can take. And if you're modeling\npixels using an RGB encoding,",
    "start": "662520",
    "end": "670560"
  },
  {
    "text": "then you have three channels,\nred, green, and blue. And so you need to\ncapture the distribution",
    "start": "670560",
    "end": "676380"
  },
  {
    "text": "over the colors of a pixel,\ngiven all the previous pixels. And one way to do it is to use\nan autoregressive structure",
    "start": "676380",
    "end": "685560"
  },
  {
    "text": "inside every pixel, every\nconditional, defining the pixel.",
    "start": "685560",
    "end": "691680"
  },
  {
    "text": "So a pixel is going to involve\nthree random variables, the red, the green, and the blue channel.",
    "start": "691680",
    "end": "697180"
  },
  {
    "text": "And you can generate them,\nlet's say, in that order. So you can compute the\nconditional probability of the red channel, given\nthe previous context.",
    "start": "697180",
    "end": "703870"
  },
  {
    "text": "And you can do the\ngreen channel, given the previous context, and\nthe value of the red channel,",
    "start": "703870",
    "end": "709080"
  },
  {
    "text": "and so forth. And in practice, you can\nbasically use an RNN style",
    "start": "709080",
    "end": "714390"
  },
  {
    "text": "architecture with some masking,\nthe same kind of masking we've seen in MADE that\nenforces this ordering.",
    "start": "714390",
    "end": "721509"
  },
  {
    "text": "So first, you try to compute\nthe conditional probability of the red pixel, and that can\ndepend on everything you've seen",
    "start": "721510",
    "end": "727990"
  },
  {
    "text": "before, but you cannot pick. You cannot look at the green\nchannel or the blue channel.",
    "start": "727990",
    "end": "733089"
  },
  {
    "text": "When you try to predict\nthe green channel, it's fine to look at the\nvalue of the red channel",
    "start": "733090",
    "end": "738610"
  },
  {
    "text": "for that pixel and so forth. And so again, it's\nbasically the same idea,",
    "start": "738610",
    "end": "744220"
  },
  {
    "text": "but you're going\nto use some masking to enforce\nautoregressive structure. And this was one-- these are\nsome examples of the results you",
    "start": "744220",
    "end": "752680"
  },
  {
    "text": "can get from an RNN at the pixel\nlevel, trained on ImageNet,",
    "start": "752680",
    "end": "757930"
  },
  {
    "text": "downscaled ImageNet. Again, you can see that\nthese results are not great,",
    "start": "757930",
    "end": "764350"
  },
  {
    "text": "but they're pretty decent. Like, what you see here\nis you take an image. You see the rightmost\ncolumn is an actual image.",
    "start": "764350",
    "end": "772780"
  },
  {
    "text": "And then what you\ndo is you can remove the top, the bottom\nhalf, and then you can let the model complete.",
    "start": "772780",
    "end": "779270"
  },
  {
    "text": "So it's similar to\na language model. You have a prompt,\nwhich, in this case, is going to be just the\ntop half of the image.",
    "start": "779270",
    "end": "784720"
  },
  {
    "text": "And then you let your\nautoregressive model generate the next pixel,\nand then the next pixel, and then the next\npixel, and so forth.",
    "start": "784720",
    "end": "791572"
  },
  {
    "text": "And you can see\nthat it's coming up with somewhat\nreasonable completions,, has the right structure.",
    "start": "791572",
    "end": "797680"
  },
  {
    "text": "It has the right symmetries. It's doing a reasonable job\nof capturing the dependencies",
    "start": "797680",
    "end": "803320"
  },
  {
    "text": "between the pixels. There is some variability\nin the samples, like here, this one\nversus this one.",
    "start": "803320",
    "end": "810133"
  },
  {
    "text": "Of course, there\nis stochasticity. So if you sample\nfrom the-- even given the same initial\ncondition, if you sample,",
    "start": "810133",
    "end": "816699"
  },
  {
    "text": "there is randomness\nin the way you sample, so you can generate different\ncompletions every time.",
    "start": "816700",
    "end": "821962"
  },
  {
    "text": "Every time you\nsample, you're going to get a different possible\nway of completing that image.",
    "start": "821962",
    "end": "827269"
  },
  {
    "text": "And you can see that\nthey have not always-- I mean, some of them\ndon't make a lot of sense, but some of the completions\nare actually decent,",
    "start": "827270",
    "end": "834550"
  },
  {
    "text": "and there is some\nvariability, which is good. The challenge is that,\nagain, because you",
    "start": "834550",
    "end": "842650"
  },
  {
    "text": "have to evaluate the probability\nof an image sequentially,",
    "start": "842650",
    "end": "848200"
  },
  {
    "text": "you have to unroll\nthe recursion. These models are very slow. And so in practice, what tends\nto works much better on images",
    "start": "848200",
    "end": "855520"
  },
  {
    "text": "is convolutional architectures. These are the kind\nof architectures that work well when you're\nbuilding classification models.",
    "start": "855520",
    "end": "862130"
  },
  {
    "text": "And so it would\nbe natural to try to use a convolutional\narchitecture to build a generative model of images.",
    "start": "862130",
    "end": "869507"
  },
  {
    "text": "The challenge, once\nagain, is that you need to make sure\nthat the model is consistent with an\nautoregressive one.",
    "start": "869507",
    "end": "877100"
  },
  {
    "text": "So what you need to\nmake sure is that when you make a prediction\nfor a pixel,",
    "start": "877100",
    "end": "882490"
  },
  {
    "text": "you only use information that\nis consistent with the ordering you've chosen.",
    "start": "882490",
    "end": "887690"
  },
  {
    "text": "So if the ordering\nis, once again, from top left to\nbottom right, when you make a prediction\nfor this pixel,",
    "start": "887690",
    "end": "893770"
  },
  {
    "text": "it's fine to use information\nfrom all the shaded area in the image.",
    "start": "893770",
    "end": "898820"
  },
  {
    "text": "But you cannot pick. You cannot look at information\ncoming from the future or coming from any of the white\nregion of the image.",
    "start": "898820",
    "end": "908080"
  },
  {
    "text": "And the way to do it is, once\nagain, relatively simple. It's always masking\nat the end of the day.",
    "start": "908080",
    "end": "914170"
  },
  {
    "text": "So when you think\nabout if you want to enforce autoregressive\nstructure, one way to do it",
    "start": "914170",
    "end": "919630"
  },
  {
    "text": "is to set up the kernels of your\nconvolutions, to be consistent,",
    "start": "919630",
    "end": "924640"
  },
  {
    "text": "to have zeros in\nthe right places",
    "start": "924640",
    "end": "930730"
  },
  {
    "text": "so that the way the\ncomputation occurs is consistent with the\nautoregressive nature",
    "start": "930730",
    "end": "936310"
  },
  {
    "text": "of the model. So if you have a simple 3\nby 3 convolutional kernel,",
    "start": "936310",
    "end": "942430"
  },
  {
    "text": "and you zero out all these\nentries in the kernel, then if you look at\nthe computation graph,",
    "start": "942430",
    "end": "948370"
  },
  {
    "text": "whenever you make a\nprediction for this red pixel, you're only going to use\nthe blue pixels to make",
    "start": "948370",
    "end": "954670"
  },
  {
    "text": "that prediction. And so that's consistent with\nthe ordering that we had before.",
    "start": "954670",
    "end": "960410"
  },
  {
    "text": "So again, it's very\nsimilar to MADE. It's very similar to\ntransformers or self-attention. You basically mask to\nmake sure that things are",
    "start": "960410",
    "end": "967040"
  },
  {
    "text": "consistent with the ordering. I was wondering\nwith regards to the. [INAUDIBLE] convolutional\napproach [INAUDIBLE]",
    "start": "967040",
    "end": "978640"
  },
  {
    "text": "over the pixels. Do you recover something, like\na convolutional structure, where you get more attention for\nthe pixels in the neighborhood?",
    "start": "978640",
    "end": "986020"
  },
  {
    "text": "Yeah, so the question is\nwhether you can use, I think, attention or self-attention\nfor modeling images",
    "start": "986020",
    "end": "993160"
  },
  {
    "text": "and whether that would recover\nthe right inductive biases. And yeah, you can use masked,\nonce again, attention on images.",
    "start": "993160",
    "end": "1003100"
  },
  {
    "text": "And there have been\nautoregressive models that are essentially using the\ntransformer-like architecture",
    "start": "1003100",
    "end": "1010860"
  },
  {
    "text": "on images. And they've been\nvery, very successful. As far as I know, they are\nnot in the public domain.",
    "start": "1010860",
    "end": "1017620"
  },
  {
    "text": "So these have been\nbuilt in industry, but they have not been\nactually released.",
    "start": "1017620",
    "end": "1022680"
  },
  {
    "text": "I think they tend to\nbe more computationally intensive to train. And so other models seem\nto-- diffusion models",
    "start": "1022680",
    "end": "1029102"
  },
  {
    "text": "that we're going to\ntalk about later tend to work better in practice. But there has been\nreported in the literature",
    "start": "1029102",
    "end": "1034679"
  },
  {
    "text": "some good success using\ntransformer-based architectures on images.",
    "start": "1034680",
    "end": "1039939"
  },
  {
    "text": "How do you model\nautoregressive architecture for data that's not\nvery clearly sequential?",
    "start": "1039940",
    "end": "1046047"
  },
  {
    "text": "So if you have a\nlanguage model, it's obviously token, token, token. But I can generate an image is\na matter of how we're actually",
    "start": "1046047",
    "end": "1052570"
  },
  {
    "text": "starting off at the\norder that we're doing. Yeah, that's a great question. The question is, what's the\nright ordering for images.",
    "start": "1052570",
    "end": "1058210"
  },
  {
    "text": "For text, maybe left to\nright seems reasonable. But for images, what's\nthe right order? That's a great question.",
    "start": "1058210",
    "end": "1063730"
  },
  {
    "text": "And we don't have\na great answer. Right now, the typical ordering\nis top left to bottom right.",
    "start": "1063730",
    "end": "1069280"
  },
  {
    "text": "But as you said, it's\nprobably not the right one. And you could imagine\na different mechanism.",
    "start": "1069280",
    "end": "1075290"
  },
  {
    "text": "There are people,\nand there's been research where people have tried\nto learn the optimal ordering.",
    "start": "1075290",
    "end": "1080930"
  },
  {
    "text": "Like you can imagine, there's\na combinatorially large number of orderings, but you\ncould try to somehow set up an optimization problem,\nwhere you search",
    "start": "1080930",
    "end": "1087730"
  },
  {
    "text": "for the right ordering first. And then you find the\nautoregressive model consistent with that order\nthat maximizes the data fit",
    "start": "1087730",
    "end": "1096280"
  },
  {
    "text": "with moderate success. And incidentally, as far as\nI know, even for language, you can model right to\nleft, and it works OK, too.",
    "start": "1096280",
    "end": "1104300"
  },
  {
    "text": "So maybe the ordering is\nnot that important, even for language.",
    "start": "1104300",
    "end": "1109960"
  },
  {
    "text": "Even if we're using a CNN right\nnow, since it's autoregressive, if we're trying to evaluate the\nlikelihood of, say, an image,",
    "start": "1109960",
    "end": "1118060"
  },
  {
    "text": "don't we still have to unroll\nthe chain rule and results in a really long computation?",
    "start": "1118060",
    "end": "1123100"
  },
  {
    "text": "So the question is whether\nthese convolutional models can be evaluated in parallel?",
    "start": "1123100",
    "end": "1128950"
  },
  {
    "text": "And to some extent,\nconvolutions can be evaluated pretty efficiently.",
    "start": "1128950",
    "end": "1136720"
  },
  {
    "text": "Components can be\nevaluated in, basically, just matrix multiplications. And they can be done very\nefficiently on modern hardware.",
    "start": "1136720",
    "end": "1145250"
  },
  {
    "text": "In fact, that's another\nway to build very efficient language models is actually\nbased on convolutions,",
    "start": "1145250",
    "end": "1150380"
  },
  {
    "text": "one deconvolutions. You can get pretty close to\ntransformers like models, using convolutions\nthat are, of course--",
    "start": "1150380",
    "end": "1158043"
  },
  {
    "text": "of course, they\nneed to be causal, so you cannot look\ninto the future. You can only look into the past. But using convolutional\nmodels has",
    "start": "1158043",
    "end": "1165279"
  },
  {
    "text": "shown to work reasonably\nwell on language as well. It matches the performance\nof transformers.",
    "start": "1165280",
    "end": "1171130"
  },
  {
    "text": "So that's another way to get\nfast parallel computation and reasonably good\nmodeling performance.",
    "start": "1171130",
    "end": "1180670"
  },
  {
    "text": "[INAUDIBLE] problem is\nfundamentally different. Because I guess in any\n[INAUDIBLE] industry,",
    "start": "1180670",
    "end": "1187450"
  },
  {
    "text": "I use outside context. I'm curious, if you\npre-train with-- I mean, you can still\ngenerate or if that's",
    "start": "1187450",
    "end": "1193360"
  },
  {
    "text": "only applicable\nto the case where you have only certain classes? Yeah, so the question\nis whether you",
    "start": "1193360",
    "end": "1199840"
  },
  {
    "text": "could train a generative model\nbased on inpainting, where you maybe mask out\nparts of an image, and you train a model to\npredict the remaining parts.",
    "start": "1199840",
    "end": "1207980"
  },
  {
    "text": "And in general, that wouldn't\ngive you a generative model. Although, there are\nways to generate",
    "start": "1207980",
    "end": "1214299"
  },
  {
    "text": "samples from that architecture. Because in some\nsense, it's still trying to learned something. You need to learn\nsomething about the joint,",
    "start": "1214300",
    "end": "1220870"
  },
  {
    "text": "if you want to do well at that. But it doesn't give you directly\na way to generate samples,",
    "start": "1220870",
    "end": "1227090"
  },
  {
    "text": "at least left to right. You would need to use\nmore expensive sampling procedures that make\nthese models harder",
    "start": "1227090",
    "end": "1234490"
  },
  {
    "text": "to use in practice. Although, there\nare variants like masked out encoders that\nare used generatively,",
    "start": "1234490",
    "end": "1240580"
  },
  {
    "text": "but that's a little\nbit more complicated. Because, obviously, on\ntransformers, they're",
    "start": "1240580",
    "end": "1246760"
  },
  {
    "text": "very good for\nparallelization, but ignoring the computational\nefficiency, just in terms of the\nraw representation",
    "start": "1246760",
    "end": "1253929"
  },
  {
    "text": "power of these\nmodels, can we say that transformers, the\nrepresentation space, like [INAUDIBLE] is more\npowerful than LSTM-based models.",
    "start": "1253930",
    "end": "1263890"
  },
  {
    "text": "I think that would\nbe hard to claim. So the question is\nwhether transformers are more powerful than an RNN. And I think that's\na little bit tricky",
    "start": "1263890",
    "end": "1270730"
  },
  {
    "text": "because another an RNN,\nby itself, is already Turing complete in general,\nso it can implement any function at\nrelatively small end,",
    "start": "1270730",
    "end": "1278710"
  },
  {
    "text": "in theory, it could do that. So it's been proven that they\nare essentially arbitrarily.",
    "start": "1278710",
    "end": "1286090"
  },
  {
    "text": "Yeah. So it's really probably more\nabout the efficiency of training or maybe inductive biases.",
    "start": "1286090",
    "end": "1292660"
  },
  {
    "text": "Then there is not a\ngood understanding about the flexibility by itself.",
    "start": "1292660",
    "end": "1298779"
  },
  {
    "text": "Why would anyone be using an\nRNN today versus the transformer architecture?",
    "start": "1298780",
    "end": "1303820"
  },
  {
    "text": "The question is why\nwould you use an RNN? One advantage is that\nthen at inference time,",
    "start": "1303820",
    "end": "1310010"
  },
  {
    "text": "having keeping track\nof a single state is actually pretty\ngood because you don't have to do a\nlot of computation",
    "start": "1310010",
    "end": "1316280"
  },
  {
    "text": "over and over, if you had a\nvanilla model, where nothing is tied, like you need to\ndo a lot of computation",
    "start": "1316280",
    "end": "1322670"
  },
  {
    "text": "at inference time. An RNN is nice because\nall you have to do",
    "start": "1322670",
    "end": "1327992"
  },
  {
    "text": "is you keep track\nof a small state, and you can throw\naway everything. All the past doesn't matter. You just need to keep\ntrack of the hidden state,",
    "start": "1327992",
    "end": "1334970"
  },
  {
    "text": "and you just keep on\nfolding the computation. I mean, sequential,\nbut all these models are sequential anyways.",
    "start": "1334970",
    "end": "1341330"
  },
  {
    "text": "But the fact that you have\nthis very small vector, and that's the\nonly thing you need to keep track of with respect\nto the state is very appealing.",
    "start": "1341330",
    "end": "1348221"
  },
  {
    "text": "So that's why people are\ntrying to actually get back to RNN-like architectures\nbecause they could be much more",
    "start": "1348222",
    "end": "1353810"
  },
  {
    "text": "efficient at inference. ",
    "start": "1353810",
    "end": "1358870"
  },
  {
    "text": "Cool. So yeah, the other thing\nyou have to keep in mind is if you do this\nmask convolution is",
    "start": "1358870",
    "end": "1363900"
  },
  {
    "text": "that you might end up with\nthis blind spot thing, where if you look at the\nreceptive field",
    "start": "1363900",
    "end": "1370200"
  },
  {
    "text": "that you get when you use\nkernels that are masked, when you make a prediction, if you\nhave a stack of convolutions,",
    "start": "1370200",
    "end": "1378240"
  },
  {
    "text": "and you make a prediction\nfor this pixel, you're not actually going to\ntake into account this grayed out pixels because\nof the blind spot.",
    "start": "1378240",
    "end": "1385950"
  },
  {
    "text": "If you see what happens, if\nyou recurse on this computation",
    "start": "1385950",
    "end": "1391139"
  },
  {
    "text": "structure and you do a\nbunch of convolution one on top of each other, you\nend up with this blind spot.",
    "start": "1391140",
    "end": "1396670"
  },
  {
    "text": "And so there are\nsome other tricks that you have to do at the\nlevel of the architecture to basically combine\nmultiple convolutions",
    "start": "1396670",
    "end": "1404520"
  },
  {
    "text": "with different masking\nto solve that issue.",
    "start": "1404520",
    "end": "1411340"
  },
  {
    "text": "And here, you can\nsee some samples that it tends to work well. If you replace the\nnet with a CNN,",
    "start": "1411340",
    "end": "1417910"
  },
  {
    "text": "you get significantly better\nsamples, and it's much faster. And right, maybe--\nand these models",
    "start": "1417910",
    "end": "1428310"
  },
  {
    "text": "tend to actually not only\ngenerate reasonable samples, but they seem to get a pretty\ngood understanding of what",
    "start": "1428310",
    "end": "1435840"
  },
  {
    "text": "is the structure of the images\nthat they see at training time. And one indication that\nis, indeed, the case",
    "start": "1435840",
    "end": "1445350"
  },
  {
    "text": "is that you can use them\nto do anomaly detection. So you might have heard that\nmachine learning models are",
    "start": "1445350",
    "end": "1451080"
  },
  {
    "text": "pretty vulnerable to adversarial\nexamples, adversarial attacks. So you take an\nimage like this one.",
    "start": "1451080",
    "end": "1456300"
  },
  {
    "text": "That would be classified\nas a dog image. And then you add this noise,\nyou get back an image that looks",
    "start": "1456300",
    "end": "1462210"
  },
  {
    "text": "identical to the original one\nbut would be classified with very high confidence by\nstate-of-the-art models to be",
    "start": "1462210",
    "end": "1467670"
  },
  {
    "text": "something completely wrong. So these two images\nare different, but in very subtle ways.",
    "start": "1467670",
    "end": "1473703"
  },
  {
    "text": "And there is a natural\nquestion of whether you can detect these\ndifferences in the images.",
    "start": "1473703",
    "end": "1479255"
  },
  {
    "text": "And if you could\ndo it, maybe you can build more robust\nmachine learning models. And one way to do it is\nto try to fit in these two",
    "start": "1479255",
    "end": "1487570"
  },
  {
    "text": "types of inputs, like natural\nimages and adversarial attacks into a pre-trained\ngenerative model",
    "start": "1487570",
    "end": "1493780"
  },
  {
    "text": "and see whether they would\nassign different probabilities to these two types of inputs.",
    "start": "1493780",
    "end": "1498790"
  },
  {
    "text": "If the model is\ndoing a good job, it might be able to detect\nthat this is a natural image. It should be assigned fairly\nhigh probability versus this one",
    "start": "1498790",
    "end": "1507610"
  },
  {
    "text": "has something weird\nis going on here. And so, it should be\nassigned a lower probability. And indeed, a pre-train\nPixelCNN model",
    "start": "1507610",
    "end": "1516130"
  },
  {
    "text": "does a pretty good\njob at discriminating between natural images and ones\nthat have been tampered with.",
    "start": "1516130",
    "end": "1523630"
  },
  {
    "text": "And so what you see\nhere is basically a histogram of the likelihoods. I guess, they are written\nin bits per dimension,",
    "start": "1523630",
    "end": "1530620"
  },
  {
    "text": "but it's the same thing\nas the probability that the different samples\nare given by the model",
    "start": "1530620",
    "end": "1536049"
  },
  {
    "text": "is on the x-axis. And on the y-axis is\nyou see how frequently different images, let's\nsay, in the training set",
    "start": "1536050",
    "end": "1543340"
  },
  {
    "text": "are given that\nprobability by the model. And you see that the\ntrain and test set, they are here, while the\nadversarial attack are",
    "start": "1543340",
    "end": "1552659"
  },
  {
    "text": "significantly separated\nfrom the natural images,",
    "start": "1552660",
    "end": "1557970"
  },
  {
    "text": "meaning they are assigned much\nlower probability by the model. So if you use a threshold to\ntry to distinguish and say",
    "start": "1557970",
    "end": "1564870"
  },
  {
    "text": "if the probability of my input\nis significantly lower than one I'm expected to,\nthen I can maybe",
    "start": "1564870",
    "end": "1570570"
  },
  {
    "text": "say that's an\nadversarial attack. And I can reject\nit, and this model seem to perform\nreasonably well, which",
    "start": "1570570",
    "end": "1575850"
  },
  {
    "text": "means that they are no\nlonger getting the high level",
    "start": "1575850",
    "end": "1581220"
  },
  {
    "text": "semantics of the\nimage, but they really are able to understand\nthe subtle dependencies",
    "start": "1581220",
    "end": "1586290"
  },
  {
    "text": "between the pixel values\nthat exist in natural images.",
    "start": "1586290",
    "end": "1592140"
  },
  {
    "text": "And yeah, question? Can people do adversarial\nattack if they don't have access to the model.",
    "start": "1592140",
    "end": "1599153"
  },
  {
    "text": "The question is\nwhether people can do adversarial attacks if they\ndon't have access to the model. To some extent, yes.",
    "start": "1599153",
    "end": "1604590"
  },
  {
    "text": "It depends. There are different kinds\nof adversarial methods. You can assume that\nyou have exact-- you know the weights,\nmaybe you can",
    "start": "1604590",
    "end": "1610398"
  },
  {
    "text": "only the outputs of the model. Sometimes, you don't even\nhave access to anything. And you have to somehow hope\nthat an attack built for a model",
    "start": "1610398",
    "end": "1618450"
  },
  {
    "text": "transfers to a different one. So to some extent there\nhave been some success even in black box settings.",
    "start": "1618450",
    "end": "1624890"
  },
  {
    "text": "[INAUDIBLE] regressive\narchitecture are more robust to\nadversarial examples?",
    "start": "1624890",
    "end": "1630810"
  },
  {
    "text": "So the question-- so\nthe question is whether the autoregressive--",
    "start": "1630810",
    "end": "1635820"
  },
  {
    "text": "is it the auto regressive? [INAUDIBLE] defend against\nadversarial attacks",
    "start": "1635820",
    "end": "1642880"
  },
  {
    "text": "better than other models. Yeah, so it's not\nnecessarily better. I think the idea is\nthat this is just",
    "start": "1642880",
    "end": "1648940"
  },
  {
    "text": "to show that the generative\nmodel, the pixel CNN that was just trained by maximizing\nthe likelihood of a data set",
    "start": "1648940",
    "end": "1656019"
  },
  {
    "text": "is able to understand\nthe structure of the images and the\nlikelihood itself is useful.",
    "start": "1656020",
    "end": "1662300"
  },
  {
    "text": "So it's not just a matter\nof sampling from the model, but the likelihood\ncan actually be used to discriminate between\ndifferent kinds of inputs.",
    "start": "1662300",
    "end": "1671140"
  },
  {
    "text": "And in order to do\nwell, you really need to understand\nthe relationship between all the pixels. You need to figure out\nthat this image is actually",
    "start": "1671140",
    "end": "1677830"
  },
  {
    "text": "different from this image. And so it means that\nthose conditionals that you learn through\nthe autoregressive model",
    "start": "1677830",
    "end": "1683470"
  },
  {
    "text": "are actually doing\na pretty good job at discriminating this\nvery subtle differences.",
    "start": "1683470",
    "end": "1689440"
  },
  {
    "text": "So for the challenge model,\nhow we evaluate the P is half?",
    "start": "1689440",
    "end": "1695970"
  },
  {
    "text": "[AUDIO OUT]",
    "start": "1695970",
    "end": "1702350"
  },
  {
    "text": "So you're basically just--\njust the same thing. I think I have it here. Basically, if you want to\ncompute the probability,",
    "start": "1702350",
    "end": "1709190"
  },
  {
    "text": "you just use the autoregressive\nchain rule computation. And so you evaluate\nthe probability",
    "start": "1709190",
    "end": "1714350"
  },
  {
    "text": "of the first pixel, the second\npixel, given the first one. Just multiply all\nthose things together, and that gives you\nthe likelihood.",
    "start": "1714350",
    "end": "1720200"
  },
  {
    "text": "That's the formula from\nan autoregressive model. And you do that for every\ninput image, the same logic,",
    "start": "1720200",
    "end": "1726260"
  },
  {
    "text": "the same function. And then you get\ndifferent results because the images are different\nin some fundamental way.",
    "start": "1726260",
    "end": "1736250"
  },
  {
    "text": "Yeah? Can you explain\none more time what the x-dimension and y-dimension\non this graph represents.",
    "start": "1736250",
    "end": "1742890"
  },
  {
    "text": "Yeah, so the x\ndimension is essentially p of x, the different\nprobability values that",
    "start": "1742890",
    "end": "1749150"
  },
  {
    "text": "are assigned by the model. And it's in bits per\ndimension because it's normalized by the\nnumber of dimensions that you have in the images.",
    "start": "1749150",
    "end": "1756990"
  },
  {
    "text": "But think of it as p of\nx, rescaled so that it's a little bit more meaningful. But roughly, it's\nthe probability.",
    "start": "1756990",
    "end": "1763710"
  },
  {
    "text": "And on the y-axis, you have the\nhow many images are assigned. It's a histogram.",
    "start": "1763710",
    "end": "1769260"
  },
  {
    "text": "How many images are assigned\ndifferent probability values. And so you get\nthis Gaussian where",
    "start": "1769260",
    "end": "1775330"
  },
  {
    "text": "even all the images\nin the training set, they are given different\nprobability values, but roughly, they range--",
    "start": "1775330",
    "end": "1781179"
  },
  {
    "text": "they are usually in this\nrange between 1 and 4. And if you look at\nadversarial attacks,",
    "start": "1781180",
    "end": "1787480"
  },
  {
    "text": "they are significantly\nseparated. So they're different\nin probability. ",
    "start": "1787480",
    "end": "1794610"
  },
  {
    "text": "Cool. And then they can also\nbe used for speech. But let me skip that. And the summary is that\nautoregressive models",
    "start": "1794610",
    "end": "1802260"
  },
  {
    "text": "are pretty general. They're good because it's\neasy to sample from them. It's easy to evaluate\nprobabilities,",
    "start": "1802260",
    "end": "1808690"
  },
  {
    "text": "which are useful in itself\nbecause you can do things like anomaly detection.",
    "start": "1808690",
    "end": "1814410"
  },
  {
    "text": "You can extend it to\ncontinuous variables. One issue with\nautoregressive models is that there is not really\na natural way to cluster data",
    "start": "1814410",
    "end": "1822120"
  },
  {
    "text": "points or get features. We'll see that latent variable\nmodels are going to be much more",
    "start": "1822120",
    "end": "1828450"
  },
  {
    "text": "natural for that. And now the next\nthing is learning,",
    "start": "1828450",
    "end": "1835840"
  },
  {
    "text": "which is going to be\npretty straightforward for autoregressive models. You can probably guess\nhow you would train them.",
    "start": "1835840",
    "end": "1843600"
  },
  {
    "text": "An autoregressive model\nis just a sequence of basically\nclassifiers, and then you just train all the\nclassifiers the usual way,",
    "start": "1843600",
    "end": "1851800"
  },
  {
    "text": "essentially. And at a high level, remember,\nwe have your model family, which could be\nautoregressive models.",
    "start": "1851800",
    "end": "1858860"
  },
  {
    "text": "You have data to\ntrain the model. You have to specify\nsome notion of distance. So how good your model\ndistribution is, how similar",
    "start": "1858860",
    "end": "1866950"
  },
  {
    "text": "it is to the data distribution. And we've seen how to define\na set of distributions",
    "start": "1866950",
    "end": "1872919"
  },
  {
    "text": "using neural networks. And now the question\nis, how do you optimize the parameters\nof the neural network",
    "start": "1872920",
    "end": "1878500"
  },
  {
    "text": "to become as close as possible\nto the data distribution?",
    "start": "1878500",
    "end": "1883810"
  },
  {
    "text": "And the setting is\none where we assume we're given a data\nset of samples",
    "start": "1883810",
    "end": "1888940"
  },
  {
    "text": "from the data distribution. And each sample is basically an\nassignment to all the variables",
    "start": "1888940",
    "end": "1893980"
  },
  {
    "text": "in the model. So it could be the\npixel intensities, every pixel intensity in\neach image in the model",
    "start": "1893980",
    "end": "1901240"
  },
  {
    "text": "or which is the same as\na standard classification problem where you might\nhave features and label. You get to see the values\nof all the random variables.",
    "start": "1901240",
    "end": "1910600"
  },
  {
    "text": "And the assumption is\nthat each data point is coming from the\nsame distribution, so they all sampled from\nthe same data distribution.",
    "start": "1910600",
    "end": "1917260"
  },
  {
    "text": "So they are identically\ndistributed, and they are independent\nof each other, which",
    "start": "1917260",
    "end": "1922700"
  },
  {
    "text": "is a standard assumption\nin machine learning. And then you're given\na family of models,",
    "start": "1922700",
    "end": "1929450"
  },
  {
    "text": "and the goal is to pick a\ngood model in this family. So the model family could be all\nBayesian networks with a given",
    "start": "1929450",
    "end": "1937279"
  },
  {
    "text": "structure, or it could be fully\nvisible sigmoid belief network",
    "start": "1937280",
    "end": "1942590"
  },
  {
    "text": "where you can think of a\nbunch of logistic regression classifiers. They each have a\nbunch of parameters. And the question is, how\ndo choose the parameters,",
    "start": "1942590",
    "end": "1949669"
  },
  {
    "text": "such that you get a good model. Well, when the only\nthing you have access to",
    "start": "1949670",
    "end": "1954799"
  },
  {
    "text": "is a bunch of samples from\nsome unknown data distribution.",
    "start": "1954800",
    "end": "1960180"
  },
  {
    "text": "And the goal is to come\nup with a model that is a good approximation to this\nunknown data-generating process.",
    "start": "1960180",
    "end": "1969090"
  },
  {
    "text": "And the problem is that you\ndon't know what data is. I cannot evaluate data\non an arbitrary input.",
    "start": "1969090",
    "end": "1975700"
  },
  {
    "text": "The only thing I\nhave access to is a bunch of samples\nfrom this distribution.",
    "start": "1975700",
    "end": "1980850"
  },
  {
    "text": "And in general, this\nis pretty tricky because you can imagine\nsamples tell us something",
    "start": "1980850",
    "end": "1989430"
  },
  {
    "text": "about which axis, let's say,\nare likely under the data distribution. But there is a\nlot of information",
    "start": "1989430",
    "end": "1994919"
  },
  {
    "text": "that is just loss that we're\njust losing whenever we just sample from our distribution.",
    "start": "1994920",
    "end": "2001650"
  },
  {
    "text": "All right, so let's\nsay that we're trying to model MNIST again. And so, let's say, modeling\n784 binary variables,",
    "start": "2001650",
    "end": "2011070"
  },
  {
    "text": "black and white pixels. And what I claim is that this\nis a really, really hard problem",
    "start": "2011070",
    "end": "2016900"
  },
  {
    "text": "because x is so\nhigh-dimensional that there is just so many different\npossible images, that even,",
    "start": "2016900",
    "end": "2025990"
  },
  {
    "text": "basically, regardless how\nlarge your training set is, this is a really,\nreally hard problem.",
    "start": "2025990",
    "end": "2032169"
  },
  {
    "text": "If you think about it, how\nmany possible images are there? If we have binary variables,\nyou have 784 of them.",
    "start": "2032170",
    "end": "2041830"
  },
  {
    "text": "There is 2 to the 784,\nwhich is roughly 10 to the 236 different images.",
    "start": "2041830",
    "end": "2049547"
  },
  {
    "text": "And somehow, you need to be\nable to assign a probability to each one of them. So let's say that you have\nmaybe 10 million training",
    "start": "2049547",
    "end": "2057819"
  },
  {
    "text": "examples or 100 million or\na billion training examples. There is still such a huge gap\nbetween however many samples",
    "start": "2057820",
    "end": "2066550"
  },
  {
    "text": "you have and all the possible\nthings that can happen, that this is just fundamentally\na really, really hard problem.",
    "start": "2066550",
    "end": "2073730"
  },
  {
    "text": "This is way more than the\nnumber of atoms in the universe. So there's just so many\ndifferent possible combinations.",
    "start": "2073730",
    "end": "2080997"
  },
  {
    "text": "And somehow, you need to be\nable to assign a probability value to each one of them.",
    "start": "2080997",
    "end": "2086710"
  },
  {
    "text": "And so you have sparse coverage. And so, this is just\nfundamentally a pretty hard problem.",
    "start": "2086710",
    "end": "2092210"
  },
  {
    "text": "And then, there are\ncomputational reasons, even if you had\ninfinite data, training, these models might not be--",
    "start": "2092210",
    "end": "2098079"
  },
  {
    "text": "might still be challenging, just\nbecause you have finite compute. And so somehow we'll have to\nbe OK with approximations,",
    "start": "2098080",
    "end": "2110130"
  },
  {
    "text": "and we'll still try to find,\ngiven the data we have, we're going to try to\nfind a good approximation.",
    "start": "2110130",
    "end": "2118019"
  },
  {
    "text": "And so the natural question\nis, what do we mean by best?",
    "start": "2118020",
    "end": "2123220"
  },
  {
    "text": "What's a good approximation? What should we\neven try to achieve to do, try to\nachieve here, given",
    "start": "2123220",
    "end": "2129600"
  },
  {
    "text": "that there are fundamental\nlimits on what we can do. And so the setting,\nwhat best means really",
    "start": "2129600",
    "end": "2138260"
  },
  {
    "text": "depends on what you want to do. One goal could be to just\ndo density estimation. So if you think about anomaly\ndetection, we just talked about,",
    "start": "2138260",
    "end": "2146120"
  },
  {
    "text": "you really care about\nbeing able to assign reasonable probabilities\nto every possible inputs",
    "start": "2146120",
    "end": "2151339"
  },
  {
    "text": "because you care about-- because, let's say,\nyou care about that. And if you are really\nable to estimate",
    "start": "2151340",
    "end": "2156740"
  },
  {
    "text": "this full joint probability\ndistribution accurately, then you can do\nmany other things.",
    "start": "2156740",
    "end": "2162090"
  },
  {
    "text": "Then, you can condition on\na subset of the variables. You can infer the others. You can do basically\neverything you want.",
    "start": "2162090",
    "end": "2169580"
  },
  {
    "text": "But it's a pretty tall order. It's a pretty\nchallenging problem, as we've just seen before.",
    "start": "2169580",
    "end": "2175790"
  },
  {
    "text": "Another thing you\ncan do is maybe you have a specific\ntask in mind, right? If you already know how you're\ngoing to use this model,",
    "start": "2175790",
    "end": "2182600"
  },
  {
    "text": "perhaps, you can try to train\na model that performs well at that particular task.",
    "start": "2182600",
    "end": "2188240"
  },
  {
    "text": "If you only care about\nclassifying images in spam versus not spam,\nthen maybe you actually want",
    "start": "2188240",
    "end": "2193490"
  },
  {
    "text": "to build a discriminative model\nthat just predicts y given x. Or if you know\nthat you just care",
    "start": "2193490",
    "end": "2199760"
  },
  {
    "text": "about captioning an image\nor generating images, given captions,\nthen maybe you don't",
    "start": "2199760",
    "end": "2205820"
  },
  {
    "text": "need to learn a joint\ndistribution between images and captions. You just need to learn the\nconditional distribution of what",
    "start": "2205820",
    "end": "2212300"
  },
  {
    "text": "you're trying to\npredict, given what you have access to what test time. That can make your life a little\nbit easier because you don't",
    "start": "2212300",
    "end": "2219980"
  },
  {
    "text": "think about density estimation. You're saying, I don't have\nany preference about the task",
    "start": "2219980",
    "end": "2228440"
  },
  {
    "text": "the model is going to be given. I want to do well at every\nsingle possible task.",
    "start": "2228440",
    "end": "2234119"
  },
  {
    "text": "But if you know that there\nis a very specific way you're going to\nuse the model, then you might want to train the\nmodel so that it does well",
    "start": "2234120",
    "end": "2241740"
  },
  {
    "text": "at that specific\ntask you care about. Other times you might\ncare about structure,",
    "start": "2241740",
    "end": "2247083"
  },
  {
    "text": "like knowledge,\ndiscovery, but we're not going to talk about\nthat in this class.",
    "start": "2247083",
    "end": "2252110"
  },
  {
    "text": "And so, we'll see,\nfirst, how to do one, and then we'll\nsee how to do two.",
    "start": "2252110",
    "end": "2258920"
  },
  {
    "text": "And so let's say that\nreally what you want to do is you want to learn\na joint probability",
    "start": "2258920",
    "end": "2264440"
  },
  {
    "text": "distribution of the\nrandom variables that is as good as possible--\nas good an approximation as",
    "start": "2264440",
    "end": "2269480"
  },
  {
    "text": "possible to the data\ndistribution that generated your data. How do you do that?",
    "start": "2269480",
    "end": "2276600"
  },
  {
    "text": "This is basically\ndensity estimation. It's a regression problem. You can think of it as\nan estimation problem",
    "start": "2276600",
    "end": "2283020"
  },
  {
    "text": "because, again, you want to be\nable to assign a probability value to every possible\nassignment of values",
    "start": "2283020",
    "end": "2288660"
  },
  {
    "text": "to the random\nvariables you have. You're trying to\nbuild a model over. And so at this point,\nreally, we just",
    "start": "2288660",
    "end": "2296580"
  },
  {
    "text": "want the joint, defined\nby the data distribution, which is unknown. But we have access to samples\nto be close to this model",
    "start": "2296580",
    "end": "2305550"
  },
  {
    "text": "that to some distribution\nin your, in your model family, P theta.",
    "start": "2305550",
    "end": "2311150"
  },
  {
    "text": "And so the setting is like this. So there is this unknown P data. There is a bunch of samples\nthat you have access to it.",
    "start": "2311150",
    "end": "2317610"
  },
  {
    "text": "There is a bunch of\ndistributions in this set, say, all the distributions\nthat you can get, as you change parameters of your\nlogistic regression classifiers",
    "start": "2317610",
    "end": "2325730"
  },
  {
    "text": "or your transformer model. It doesn't matter. And somehow, we want to find a\npoint that is close with respect",
    "start": "2325730",
    "end": "2332870"
  },
  {
    "text": "to some notion of\nsimilarity or distance to the true underlying\ndata distribution.",
    "start": "2332870",
    "end": "2340250"
  },
  {
    "text": "So the first question is, how\ndo we evaluate, whether or not",
    "start": "2340250",
    "end": "2345800"
  },
  {
    "text": "to joint probability\ndistributions are similar to each other? And there is many ways to do it.",
    "start": "2345800",
    "end": "2351300"
  },
  {
    "text": "And as we'll see, we're going to\nget different generative models by changing the way we\nmeasure similarity between two",
    "start": "2351300",
    "end": "2358580"
  },
  {
    "text": "probability distributions. There are some ways of comparing\nprobability distributions that",
    "start": "2358580",
    "end": "2364490"
  },
  {
    "text": "are more information theoretic. We're going to see today,\nlike maximum likelihood based on compression that will give\nyou certain kinds of models.",
    "start": "2364490",
    "end": "2372197"
  },
  {
    "text": "There's going to be other\nways that are more based on if you can generate-- you\ncould say, OK, if I generate",
    "start": "2372197",
    "end": "2378000"
  },
  {
    "text": "samples from P data, and I\ngenerate samples from P theta, you should not be able to\ndistinguish between the two.",
    "start": "2378000",
    "end": "2385330"
  },
  {
    "text": "That would give\nrise to something like a generative,\nadversarial network. So there's going to be different\nways of defining similarities",
    "start": "2385330",
    "end": "2391870"
  },
  {
    "text": "between distributions, and\nthat will be one of the axes, one of the ingredients\nthat you can use to define different\ntypes of generative models.",
    "start": "2391870",
    "end": "2398519"
  },
  {
    "text": " For autoregressive\nmodels, a natural way",
    "start": "2398520",
    "end": "2403980"
  },
  {
    "text": "to build a notion\nof similarity is to use the likelihood\nbecause we have access to it.",
    "start": "2403980",
    "end": "2413430"
  },
  {
    "text": "And so we can use a\nnotion of similarity that is known as the\nKL-divergence, which",
    "start": "2413430",
    "end": "2419609"
  },
  {
    "text": "is defined like this. The KL-divergence between\ndistribution p and q is just basically\nthis expectation",
    "start": "2419610",
    "end": "2425640"
  },
  {
    "text": "with respect to all the\npossible things that can happen. All the possible\nthings that can happen",
    "start": "2425640",
    "end": "2430650"
  },
  {
    "text": "are weighted with respect\nto the probability under p. And then you look at the log of\nthe ratio of the probabilities",
    "start": "2430650",
    "end": "2437190"
  },
  {
    "text": "assigned by p and q. And it turns out that this\nquantity is non-negative,",
    "start": "2437190",
    "end": "2444630"
  },
  {
    "text": "and it's zero if and\nonly if p is equal to q.",
    "start": "2444630",
    "end": "2449980"
  },
  {
    "text": "And so it's a reasonable\nnotion of similarity because it tells\nyou, if you, somehow,",
    "start": "2449980",
    "end": "2456610"
  },
  {
    "text": "are able to choose one of\nthem, let's say, p to be p data, q to be your\nmodel distribution.",
    "start": "2456610",
    "end": "2463060"
  },
  {
    "text": "If you are able to derive this\nquantity as small as possible, then it means that you're\ntrying to make your model closer",
    "start": "2463060",
    "end": "2470320"
  },
  {
    "text": "to the data. And if you are able to\nderive this loss to zero, then you know that you\nhave a perfect model. ",
    "start": "2470320",
    "end": "2479800"
  },
  {
    "text": "Well, I have a one-line proof,\nbut I'm going to skip it, showing that it's non-negative.",
    "start": "2479800",
    "end": "2486150"
  },
  {
    "text": "The important thing is that\nthis quantity is asymmetric. So the KL-divergence\nbetween p and q",
    "start": "2486150",
    "end": "2493020"
  },
  {
    "text": "is not the same as the\nKL-divergence between q and p. In fact, the KL-divergence, if\nyou use one versus the other,",
    "start": "2493020",
    "end": "2500460"
  },
  {
    "text": "it's going to give us\nboth reasonable ways of comparing similarity. One will give us maximum\nlikelihood training.",
    "start": "2500460",
    "end": "2507819"
  },
  {
    "text": "One will be more natural\nand will come up again when we talk about generative\nadversarial networks.",
    "start": "2507820",
    "end": "2514092"
  },
  {
    "text": "It's going to be harder to\ndeal with computationally, but it's also like a\nreasonable way of comparing similarity between p and q.",
    "start": "2514092",
    "end": "2519790"
  },
  {
    "text": " Yeah, so they are symmetric. And the intuition, as\nI mentioned before,",
    "start": "2519790",
    "end": "2526259"
  },
  {
    "text": "is these kind of quantity\nhas an information theoretic interpretation, and\nit tells you something",
    "start": "2526260",
    "end": "2534480"
  },
  {
    "text": "to do with compression. So the idea is that when you're\nbuilding a generative model. You're essentially trying\nto learn a distribution.",
    "start": "2534480",
    "end": "2542289"
  },
  {
    "text": "If you have access to a good\nprobability distribution over all the possible\nthings that can happen, then you also have access to a\ngood way of compressing data.",
    "start": "2542290",
    "end": "2550960"
  },
  {
    "text": "And essentially, the\nKL-divergence between p and q tells you how well compression\nschemes based on p versus q",
    "start": "2550960",
    "end": "2561520"
  },
  {
    "text": "would perform. And so specifically,\nit's telling you,",
    "start": "2561520",
    "end": "2567460"
  },
  {
    "text": "if the data is\ntruly coming from p, and you use an optimization,\na compression scheme that",
    "start": "2567460",
    "end": "2573640"
  },
  {
    "text": "is optimized for q, how\nmuch worse is it going to be than a compression\nscheme that was actually",
    "start": "2573640",
    "end": "2580000"
  },
  {
    "text": "based on the true\ndistribution of the data? So intuitively, as\nI mentioned, knowing",
    "start": "2580000",
    "end": "2589820"
  },
  {
    "text": "the distribution that\ngenerates the data is useful for compression.",
    "start": "2589820",
    "end": "2596720"
  },
  {
    "text": "And so imagine that you have\n100 binary random variables,",
    "start": "2596720",
    "end": "2602920"
  },
  {
    "text": "coin flips. If the coin flips are\nunbiased, so 50/50,",
    "start": "2602920",
    "end": "2608630"
  },
  {
    "text": "heads, tails, then there\nis not much you can do. The best way to\ncompress the result",
    "start": "2608630",
    "end": "2614720"
  },
  {
    "text": "of flipping this coin 100 times\nis to basically use one bit. Let's see zero to encode\nhead, one to encode tails,",
    "start": "2614720",
    "end": "2623360"
  },
  {
    "text": "and on average, you're going\nto use one bit per sample. And that's the best\nthing you can do.",
    "start": "2623360",
    "end": "2629030"
  },
  {
    "text": "But imagine now that\nthe coin is biased. So imagine that heads is\nmuch more likely than tail.",
    "start": "2629030",
    "end": "2634910"
  },
  {
    "text": "Then you know that you are going\nto-- out of this 100 flips, you're expecting to see\nmany more heads than tails.",
    "start": "2634910",
    "end": "2642390"
  },
  {
    "text": "So it might make\nsense to come up with a compression\nscheme that assigns low short codes\nto things that are",
    "start": "2642390",
    "end": "2650700"
  },
  {
    "text": "going to be much more frequent. So you could say that you\ncould batch things together,",
    "start": "2650700",
    "end": "2656900"
  },
  {
    "text": "and you could say\nsequences like HHHH are going to be much more\ncommon than sequences like TTTT.",
    "start": "2656900",
    "end": "2663260"
  },
  {
    "text": "And so you might want to assign\na short code to sequences that you know are\ngoing to be frequent",
    "start": "2663260",
    "end": "2668450"
  },
  {
    "text": "and a long code to\nsequences that you think are going to be infrequent. And that gives you to\nsavings in practice.",
    "start": "2668450",
    "end": "2674630"
  },
  {
    "text": "So an example that many of\nyou are probably familiar with is Morse code. That's a way to encode\nletters to symbols,",
    "start": "2674630",
    "end": "2683390"
  },
  {
    "text": "like dots and dashes. And if you think about\nit, there is a reason",
    "start": "2683390",
    "end": "2688430"
  },
  {
    "text": "why the vowels, like\nE and A are assigned to this very short code,\nwhile a letter like U",
    "start": "2688430",
    "end": "2697650"
  },
  {
    "text": "is a sign, a very long\ncode with four elements.",
    "start": "2697650",
    "end": "2702869"
  },
  {
    "text": "And that's because vowels are\nmuch more common in English, so you are much\nmore likely to use-- if are trying to send\na message to somebody,",
    "start": "2702870",
    "end": "2709600"
  },
  {
    "text": "you are much more\nlikely to use vowels. And so if you want to minimize\nthe length of the message, you want to use a short\nencoding for frequent letters",
    "start": "2709600",
    "end": "2719010"
  },
  {
    "text": "and a long encoding\nfor infrequent letters. And so all this to say\nis that KL-divergence",
    "start": "2719010",
    "end": "2727310"
  },
  {
    "text": "has this interpretation,\nand it's basically saying if the data is truly\ndistributed according to p,",
    "start": "2727310",
    "end": "2734779"
  },
  {
    "text": "and you try to\nbuild a compression scheme that is optimized for q,\nyou're going to be suboptimal.",
    "start": "2734780",
    "end": "2742630"
  },
  {
    "text": "Maybe in your\nmodel of the world, the vowels are\nmuch more frequent than are much more\ninfrequent than q.",
    "start": "2742630",
    "end": "2749920"
  },
  {
    "text": "So you have a bad\ngenerative model for text. Then if you try to\noptimize, come up",
    "start": "2749920",
    "end": "2755050"
  },
  {
    "text": "with a scheme based on\nthis wrong assumption, it's not going to be as\nefficient as the one based",
    "start": "2755050",
    "end": "2761020"
  },
  {
    "text": "on the true frequencies\nof the characters. And how much more\nineffective your code is,",
    "start": "2761020",
    "end": "2767750"
  },
  {
    "text": "is exactly the KL-divergence. So the KL-divergence\nexactly measures how much more inefficient\nyour compression scheme",
    "start": "2767750",
    "end": "2775180"
  },
  {
    "text": "is going to be. And so if you try to\noptimize KL-divergence,",
    "start": "2775180",
    "end": "2781910"
  },
  {
    "text": "you're equivalently trying\nto optimize for compression. So you're trying\nto build a model,",
    "start": "2781910",
    "end": "2786960"
  },
  {
    "text": "such that you can\ncompress data pretty well or as well as possible,\nwhich is, again,",
    "start": "2786960",
    "end": "2795070"
  },
  {
    "text": "a reasonable kind\nof way of thinking about modeling the world. Because, in some sense, if\nyou can compress well, then",
    "start": "2795070",
    "end": "2801873"
  },
  {
    "text": "it means that\nyou're understanding the structure of the data, you\nknow, which things are common, and which ones are not.",
    "start": "2801873",
    "end": "2808010"
  },
  {
    "text": "And that's the\nphilosophy that you take if you train a\nmodel using KL-divergence",
    "start": "2808010",
    "end": "2814150"
  },
  {
    "text": "as the objective function. So now that we've\nchosen KL-divergence",
    "start": "2814150",
    "end": "2825560"
  },
  {
    "text": "as one of the ways of\nmeasuring similarity between distributions, we can\nset up our learning problem",
    "start": "2825560",
    "end": "2833960"
  },
  {
    "text": "as saying, OK, there is a\ntrue data generating process. There is a family\nof distributions",
    "start": "2833960",
    "end": "2839540"
  },
  {
    "text": "that I can choose from. I can measure how\nsimilar my model is to the data distribution\nby looking at this object.",
    "start": "2839540",
    "end": "2848022"
  },
  {
    "text": "And so, intuitively, if you\nthink about this formula, this thing is saying, I'm going\nto look at all possible, let's",
    "start": "2848022",
    "end": "2853109"
  },
  {
    "text": "say, images that could come\nfrom the data distribution. And I'm going to look at the\nratio of probability assigned",
    "start": "2853110",
    "end": "2859619"
  },
  {
    "text": "by the data distribution\nand the model. So I care about how\ndifferent the probabilities",
    "start": "2859620",
    "end": "2866280"
  },
  {
    "text": "are under the model and\nunder the data distribution. If those two match,\nso if they assign",
    "start": "2866280",
    "end": "2872550"
  },
  {
    "text": "exactly the same probability,\nthen this ratio becomes one.",
    "start": "2872550",
    "end": "2877570"
  },
  {
    "text": "The logarithm of 1 is 0. And you see that the\nKL-divergence is exactly zero. So you have a perfect model.",
    "start": "2877570",
    "end": "2884579"
  },
  {
    "text": "If you assign exactly the\nsame probability to every x, then you have a perfect model.",
    "start": "2884580",
    "end": "2890940"
  },
  {
    "text": "Otherwise, you're\ngoing to pay a price. And that price depends on how\nlikely x is under the data",
    "start": "2890940",
    "end": "2897210"
  },
  {
    "text": "distribution and how far off\nyour estimated probability is from the true probability\nunder the data distribution.",
    "start": "2897210",
    "end": "2907350"
  },
  {
    "text": "Yeah? [INAUDIBLE] through\ndensity estimation.",
    "start": "2907350",
    "end": "2912720"
  },
  {
    "text": "Great question. The question is, OK,\nthis looks reasonable, but how do you\ncompute this quantity?",
    "start": "2912720",
    "end": "2918850"
  },
  {
    "text": "How do you optimize it? It looks like it depends on\nthe true probability assigned",
    "start": "2918850",
    "end": "2924060"
  },
  {
    "text": "under the data distribution,\nwhich we don't have access to, so it doesn't look like\nsomething we can optimize. And we'll see that it\nsimplifies into something",
    "start": "2924060",
    "end": "2931050"
  },
  {
    "text": "that we can actually optimize. Yeah, a great question. So what would the\ninterpretation be",
    "start": "2931050",
    "end": "2938040"
  },
  {
    "text": "of the KL-divergence of T\ntheta, like two lines of theta,",
    "start": "2938040",
    "end": "2943830"
  },
  {
    "text": "just flipped it. And what is that-- It be more like-- the question is, what happens\nif we flip the argument of here,",
    "start": "2943830",
    "end": "2951390"
  },
  {
    "text": "and we have what's\ncalled as the reverse KL. So the Kl-divergence\nbetween P theta and P data. So it would be the same thing.",
    "start": "2951390",
    "end": "2957600"
  },
  {
    "text": "But in that case, we would be\nlooking at all possible things that can happen. We would weigh them with\nrespect to the model P theta,",
    "start": "2957600",
    "end": "2965760"
  },
  {
    "text": "and then the ratio here\nwould again be flipped. So we care about the ratios, but\nin a different sign, basically.",
    "start": "2965760",
    "end": "2974640"
  },
  {
    "text": "And so the quantity\nwould be zero if and only if they are identical.",
    "start": "2974640",
    "end": "2980310"
  },
  {
    "text": "But you can see that it\nhas a different flavor. Because if you look at this\nexpression, we're saying,",
    "start": "2980310",
    "end": "2987950"
  },
  {
    "text": "it does what happens\noutside, let's say, of the support of\nthe data distribution doesn't matter with\nrespect to these laws.",
    "start": "2987950",
    "end": "2995360"
  },
  {
    "text": "Well, if you had P theta\nhere, then you would say, I really care about\nthe loss that I achieve",
    "start": "2995360",
    "end": "3002230"
  },
  {
    "text": "on things I generate myself. And if you think about\nhow these models are used,",
    "start": "3002230",
    "end": "3007510"
  },
  {
    "text": "that actually seems like a\nmore reasonable thing to do. Because maybe it really matters.",
    "start": "3007510",
    "end": "3013930"
  },
  {
    "text": "You really want to score the\ngenerations that you produce, as opposed to what's\navailable in the training set.",
    "start": "3013930",
    "end": "3020470"
  },
  {
    "text": "But it will turn out that\nthe nice property that we're going to see soon that\nmakes this tractable",
    "start": "3020470",
    "end": "3025990"
  },
  {
    "text": "doesn't hold when you do\nreverse KL-divergence. So that's why you can't really\noptimize it in practice.",
    "start": "3025990",
    "end": "3031125"
  },
  {
    "text": " Yeah? So even though, say, the\nKL-divergence of p and q is not",
    "start": "3031125",
    "end": "3039910"
  },
  {
    "text": "equal to KL-divergence\nof q and p, so if you have two\nsets of p2q1q1,",
    "start": "3039910",
    "end": "3045970"
  },
  {
    "text": "if KL p1q1 is smaller than\np2q2, with the reverse--",
    "start": "3045970",
    "end": "3051099"
  },
  {
    "text": "with a KL q2q1q1, those will\nbe smaller than KL of q2p2p2.",
    "start": "3051100",
    "end": "3057820"
  },
  {
    "text": "As far as I know,\nnot necessarily. Yeah. ",
    "start": "3057820",
    "end": "3064289"
  },
  {
    "text": "Yeah. I wonder, do we\nuse other metrics",
    "start": "3064290",
    "end": "3071190"
  },
  {
    "text": "to evaluate the distance between\ntwo distributions, let's say, do integrations between\nthe two distributions?",
    "start": "3071190",
    "end": "3076750"
  },
  {
    "text": "Yeah, so the question is, do we\never want to use other metrics? Yes, we'll see that\nin future lectures.",
    "start": "3076750",
    "end": "3082119"
  },
  {
    "text": "We'll get different\ngenerative models, simply by changing this one ingredient. So you can still define\nyour family in any way",
    "start": "3082120",
    "end": "3088290"
  },
  {
    "text": "you want, but we might\nchange the way we compare distributions. Because, at the end of the\nday, here, we're saying",
    "start": "3088290",
    "end": "3094513"
  },
  {
    "text": "we care about compression,\nwhich might or might not be what you want. If you just care about\ngenerating pretty images,",
    "start": "3094513",
    "end": "3100005"
  },
  {
    "text": "maybe you don't care\nabout compression. Maybe you care about\nsomething else. And we'll see that\nthere is going",
    "start": "3100005",
    "end": "3105020"
  },
  {
    "text": "to be other types of learning\nobjectives that are reasonable.",
    "start": "3105020",
    "end": "3110210"
  },
  {
    "text": "And they give rise to\ngenerative models that tend to work well in practice. ",
    "start": "3110210",
    "end": "3117400"
  },
  {
    "text": "I'm just curious, for this\none, for this P data key,",
    "start": "3117400",
    "end": "3123849"
  },
  {
    "text": "you mean this order,\ndoes it make more-- does it have more meaning\nbecause the P data is really",
    "start": "3123850",
    "end": "3130030"
  },
  {
    "text": "the empirical distribution. But if you flip it, and then\ndoes it make it less meaningful,",
    "start": "3130030",
    "end": "3136760"
  },
  {
    "text": "meaning although it seems\nto be like a symmetrical, but it doesn't make it much\nless meaningful because we",
    "start": "3136760",
    "end": "3144550"
  },
  {
    "text": "don't really trust-- like\nthe Monte Carlo simulation, you yourself imagined out of--",
    "start": "3144550",
    "end": "3151630"
  },
  {
    "text": "So the question\nis, again, should the expectation, with respect\nto the true data distribution,",
    "start": "3151630",
    "end": "3158170"
  },
  {
    "text": "or should we with respect\nto the model, which is what you would get if you\nwere to flip the order here?",
    "start": "3158170",
    "end": "3165609"
  },
  {
    "text": "And the quantities will be zero. Both of them will\nbe zero if and only",
    "start": "3165610",
    "end": "3171609"
  },
  {
    "text": "if you have perfect matching. But in the real world, where\nyou would have finite data,",
    "start": "3171610",
    "end": "3178460"
  },
  {
    "text": "you would have limited\nmodeling capacity. You would not have\nperfect optimization.",
    "start": "3178460",
    "end": "3184070"
  },
  {
    "text": "You would get very\ndifferent results. And in fact, you\nget a much more-- if you were to do the\nKL-divergence between P theta",
    "start": "3184070",
    "end": "3191660"
  },
  {
    "text": "and P data, you would get a much\nmore mode seeking-like behavior,",
    "start": "3191660",
    "end": "3197390"
  },
  {
    "text": "where you can imagine if\nyou put all the probability mass into a single mode.",
    "start": "3197390",
    "end": "3204863"
  },
  {
    "text": "It might look like you're still\nperforming pretty well according to this objective. So it tends to have a much more\nmode-seeking objective compared",
    "start": "3204863",
    "end": "3213079"
  },
  {
    "text": "to the KL-divergence, which\nis forcing you to spread out all the probability mass over\nall the possible things that",
    "start": "3213080",
    "end": "3219109"
  },
  {
    "text": "can happen. So if there is an x that\nis possible under P data,",
    "start": "3219110",
    "end": "3224390"
  },
  {
    "text": "and you assign it\nzero probability, you're going to get\nan infinite loss. So it's going to\nbe very, very bad.",
    "start": "3224390",
    "end": "3230960"
  },
  {
    "text": "So you're forced to spread\nout the probability mass. If you do reverse KL,\nthat is an incentive to concentrate the\nprobability mass.",
    "start": "3230960",
    "end": "3238680"
  },
  {
    "text": "So the behavior, as you said,\nare going to be very different. And depending on\nwhat you want, one",
    "start": "3238680",
    "end": "3243860"
  },
  {
    "text": "might be better than the other. So we've been discussing\nabout the flavor",
    "start": "3243860",
    "end": "3249410"
  },
  {
    "text": "when we do one way\nof KL-divergence and the flipped way. Is it analogous to\nprecision and recall?",
    "start": "3249410",
    "end": "3257450"
  },
  {
    "text": "Because in one case, we're\nconsidering the denominator is the all true things.",
    "start": "3257450",
    "end": "3264890"
  },
  {
    "text": "And another, we are considering\nall the positive predictions. That happens. The question is, does\nthis have the flavor",
    "start": "3264890",
    "end": "3271970"
  },
  {
    "text": "of precision and recall? And yes, it has a very similar. It's not exactly\nprecision and recall.",
    "start": "3271970",
    "end": "3277110"
  },
  {
    "text": "It's a softer thing, but\nit has the flavor of you care more about\nprecision versus recall. ",
    "start": "3277110",
    "end": "3285440"
  },
  {
    "text": "It's a good way to put it. All right, so we\nhave this loss, which",
    "start": "3285440",
    "end": "3291560"
  },
  {
    "text": "you can expand the expectation. That's something like this. And now we know\nthat this divergence",
    "start": "3291560",
    "end": "3298320"
  },
  {
    "text": "is zero if and only if the\ndistributions are the same. So if you can optimize\nthis as a function of theta to make it as small\nas possible, it's",
    "start": "3298320",
    "end": "3305210"
  },
  {
    "text": "a reasonable learning objective. Measure compression loss. The challenge is, as\nwas mentioned before,",
    "start": "3305210",
    "end": "3312980"
  },
  {
    "text": "is that it might look like it\ndepends on something you cannot even compute it because it\ndepends on the probability",
    "start": "3312980",
    "end": "3320330"
  },
  {
    "text": "assigned to all the possible\nthings that can happen under the true model-- under the true\ndata distribution,",
    "start": "3320330",
    "end": "3327430"
  },
  {
    "text": "which you don't know. But if you just\ndecompose the log of the ratio as the\ndifference of the logs,",
    "start": "3327430",
    "end": "3334510"
  },
  {
    "text": "you get an expression\nthat looks like this. And now you can note\nthat the first term here",
    "start": "3334510",
    "end": "3340060"
  },
  {
    "text": "does not depend on theta. It's just like a shift. It's a constant that is\nindependent on how you choose",
    "start": "3340060",
    "end": "3348400"
  },
  {
    "text": "the parameters of your model. And so for the purposes\nof optimizing theta,",
    "start": "3348400",
    "end": "3353780"
  },
  {
    "text": "you can ignore the first term. Actually, if you're trying\nto make this quantity as",
    "start": "3353780",
    "end": "3360270"
  },
  {
    "text": "small as possible, regardless\nof how you choose theta, this is going to be the same,\nas you can effectively ignore it",
    "start": "3360270",
    "end": "3365880"
  },
  {
    "text": "for the purposes\nof optimization.  And so if you try\nto find a theta that",
    "start": "3365880",
    "end": "3374320"
  },
  {
    "text": "minimizes this expression\nbecause there is a minus here, the best thing you can do is to\nbasically make this thing here",
    "start": "3374320",
    "end": "3382670"
  },
  {
    "text": "as large as possible,\nwhat I have here. And this term here should\nbe somewhat familiar.",
    "start": "3382670",
    "end": "3390369"
  },
  {
    "text": "What we're saying\nis that we should pick the distribution that\nassigns basically the highest",
    "start": "3390370",
    "end": "3396040"
  },
  {
    "text": "probability to the axes that\nare sampled from the data distribution.",
    "start": "3396040",
    "end": "3402310"
  },
  {
    "text": "And so this is really a\nmaximum likelihood estimation. We're trying to\nchoose a model that",
    "start": "3402310",
    "end": "3408100"
  },
  {
    "text": "puts high probability\non the things that you have in the\ntraining set, essentially,",
    "start": "3408100",
    "end": "3414839"
  },
  {
    "text": "which is the training objective\nthat you've seen before, probably in other classes of\ntrying to pick parameters that",
    "start": "3414840",
    "end": "3424170"
  },
  {
    "text": "basically maximize the\nprobability of observing a particular data set.",
    "start": "3424170",
    "end": "3429390"
  },
  {
    "text": "We're trying to\nchoose parameters, such that, in expectation,\nthe average log likelihood of the data is as\nhigh as possible.",
    "start": "3429390",
    "end": "3438850"
  },
  {
    "text": "So you can see that that's\nequivalent to minimizing our KL-divergence,\nwhich, as we've seen,",
    "start": "3438850",
    "end": "3444700"
  },
  {
    "text": "is the same as trying\nto do as well as you can at this compression task.",
    "start": "3444700",
    "end": "3449980"
  },
  {
    "text": "And one caveat here is because\nwe've ignored this term,",
    "start": "3449980",
    "end": "3455500"
  },
  {
    "text": "it's possible to\ncompare two models. So you have a theta\n1 and a theta 2. I can tell you which one\nis doing a better job,",
    "start": "3455500",
    "end": "3462160"
  },
  {
    "text": "but you can never know\nhow close you truly are to the data distribution.",
    "start": "3462160",
    "end": "3467560"
  },
  {
    "text": "You can only evaluate the\nloss up to a constant. So you'll never know how much\nbetter could it have been.",
    "start": "3467560",
    "end": "3475270"
  },
  {
    "text": "You can't really evaluate that. And that's one of\nthe problems here, is that we don't know how much\nbetter could we have been,",
    "start": "3475270",
    "end": "3483520"
  },
  {
    "text": "because there's always this\nshift that cannot be evaluated. And for those who have\nseen this in other classes,",
    "start": "3483520",
    "end": "3490839"
  },
  {
    "text": "that's basically the entropy\nof the data distribution. And that's telling you how\nhard is it to model the data",
    "start": "3490840",
    "end": "3497960"
  },
  {
    "text": "distribution or what's the-- yeah, how random is the data\ndistribution to begin with,",
    "start": "3497960",
    "end": "3504260"
  },
  {
    "text": "how hard is it to model\nthe data distribution, if you had access to\nthe perfect model.",
    "start": "3504260",
    "end": "3509720"
  },
  {
    "text": "That doesn't affect how well\nyour particular model is doing, but it's the thing you need\nto know how close you are",
    "start": "3509720",
    "end": "3518150"
  },
  {
    "text": "truly to the data distribution. As you mentioned, you could\ncompare two models using this,",
    "start": "3518150",
    "end": "3524270"
  },
  {
    "text": "but you'd only get the\ndivergence between the two, and you still wouldn't\nknow how they [INAUDIBLE]",
    "start": "3524270",
    "end": "3530180"
  },
  {
    "text": "So you could get--\nif you take the-- let's say, you have a P\ntheta 1 and a P theta 2,",
    "start": "3530180",
    "end": "3535730"
  },
  {
    "text": "and you take the difference\nbetween the KL-divergence between data and P theta 1 minus\nthe KL-divergence between theta",
    "start": "3535730",
    "end": "3543140"
  },
  {
    "text": "and P theta 2, the\nconstant cancels out. And so you know which one is\ncloser to the data distribution.",
    "start": "3543140",
    "end": "3549200"
  },
  {
    "text": "But you never know\nhow close you are. So going back to this picture, I\nguess what I'm saying is that--",
    "start": "3549200",
    "end": "3556310"
  },
  {
    "text": "maybe it's too many-- given two points\nin here, you can tell which one is closer\nto the data distribution,",
    "start": "3556310",
    "end": "3563150"
  },
  {
    "text": "but you never know the\nlength of this segment, like you don't know how\nclose you actually are.",
    "start": "3563150",
    "end": "3570000"
  },
  {
    "text": "[INAUDIBLE] we have two\nmodels and the second term is [INAUDIBLE].",
    "start": "3570000",
    "end": "3575779"
  },
  {
    "text": "Is there any way to objectively\ncompare which of the two is better, relative to\nthe [INAUDIBLE] other",
    "start": "3575780",
    "end": "3582470"
  },
  {
    "text": "than to just have zero. Yeah, so if you have\nmodels that achieve exactly the same average log\nlikelihood, which one is better?",
    "start": "3582470",
    "end": "3589700"
  },
  {
    "text": "Occam's razor would tell\nyou pick the simplest one. And that's usually a\ngood inductive bias.",
    "start": "3589700",
    "end": "3595400"
  },
  {
    "text": " OK. ",
    "start": "3595400",
    "end": "3602800"
  },
  {
    "text": "Now, one further problem\nis that this quantity here",
    "start": "3602800",
    "end": "3610300"
  },
  {
    "text": "still involves an\nexpectation, with respect to the data distribution, which\nwe still don't have access to.",
    "start": "3610300",
    "end": "3616530"
  },
  {
    "text": "So you can't still\noptimize this quantity.",
    "start": "3616530",
    "end": "3621730"
  },
  {
    "text": "However, we can approximate\nthe expected log likelihood",
    "start": "3621730",
    "end": "3627730"
  },
  {
    "text": "with the empirical log\nlikelihood or the average log likelihood on the training set.",
    "start": "3627730",
    "end": "3634250"
  },
  {
    "text": "So remember that what we\nwould really care about is the average log likelihood,\nwith respect to all the things",
    "start": "3634250",
    "end": "3641420"
  },
  {
    "text": "that can possibly happen\nwhen you weight them with the probability given\nby the data distribution",
    "start": "3641420",
    "end": "3647000"
  },
  {
    "text": "that we don't have access to. But we can approximate that\nby going through our data set",
    "start": "3647000",
    "end": "3653420"
  },
  {
    "text": "and checking the log\nprobabilities assigned by the model to all the\ndata points in the data set.",
    "start": "3653420",
    "end": "3660170"
  },
  {
    "text": "And to the extent that the\ndata set is sufficiently large, I claim that this is a good\napproximation to the expected",
    "start": "3660170",
    "end": "3667880"
  },
  {
    "text": "value. And the intuition is that\nyou have an expectation. You have a sample average. To the extent that you take\nan average with respect",
    "start": "3667880",
    "end": "3674030"
  },
  {
    "text": "to a large enough\nnumber of samples, the sample average will be\npretty close to the expectation.",
    "start": "3674030",
    "end": "3679490"
  },
  {
    "text": "And now this is a loss\nthat you can compute. You just go through\nyour training set.",
    "start": "3679490",
    "end": "3684510"
  },
  {
    "text": "You look what's the likelihood\nassigned by the model to every data point,\nand you try to make that as large as possible.",
    "start": "3684510",
    "end": "3692670"
  },
  {
    "text": "And so that's maximum\nlikelihood learning. That's the thing that\nyou've seen before.",
    "start": "3692670",
    "end": "3698670"
  },
  {
    "text": "Try to find the distribution\nthat maximizes the average log probability over all the data\npoints in your training set D.",
    "start": "3698670",
    "end": "3709440"
  },
  {
    "text": "And as usual, you can\nignore this one over D. That's just a constant. It doesn't involve,\ndoesn't depend on theta.",
    "start": "3709440",
    "end": "3715779"
  },
  {
    "text": "And so you get the\nusual loss function. And note, this is exactly\nthe same thing as saying",
    "start": "3715780",
    "end": "3723319"
  },
  {
    "text": "because the data\npoints are independent, maximizing this\nexpression is exactly",
    "start": "3723320",
    "end": "3728900"
  },
  {
    "text": "the same thing as\nmaximizing the probability of observing the data set\nthat you have access to.",
    "start": "3728900",
    "end": "3735438"
  },
  {
    "text": "So this is a reasonable\nlearning objective. You have a bunch\nof data, and you're trying to find the\nparameters that maximize",
    "start": "3735438",
    "end": "3741569"
  },
  {
    "text": "the probability of\nsampling data set, like the one you have access to.",
    "start": "3741570",
    "end": "3747540"
  },
  {
    "text": "If you take a log of this\nexpression, the log of a product becomes a sum of\nlogs, and then you get that these two things\nare exactly the same.",
    "start": "3747540",
    "end": "3756549"
  },
  {
    "text": "So again, very reasonable\ntraining objective. Let's find parameters\nthat maximize the probability of observing the\ndata set that we have access to.",
    "start": "3756550",
    "end": "3765930"
  },
  {
    "text": "Yeah? We use similar numerical methods\nto estimate that constant term",
    "start": "3765930",
    "end": "3772470"
  },
  {
    "text": "from before, where\nyou could-- like in your data set, based\non x, and then estimate the entropy of that.",
    "start": "3772470",
    "end": "3780142"
  },
  {
    "text": "Yeah, so the question is,\ncan you use similar tricks to estimate this? So you can certainly\nestimate the expectation,",
    "start": "3780143",
    "end": "3785800"
  },
  {
    "text": "but then the problem is\nthis log probability. And that one is much\nharder to estimate. And you can try to do\nkernel density estimates,",
    "start": "3785800",
    "end": "3792569"
  },
  {
    "text": "or you could even\nuse P theta in there, if you believe you have\na good approximation,",
    "start": "3792570",
    "end": "3798180"
  },
  {
    "text": "then you can plug it\nin, but you'll never know how far off you are. So there's always\napproximations.",
    "start": "3798180",
    "end": "3804300"
  },
  {
    "text": "So obviously, there's\na little thumb that says samples of p\ntheta of x as close to zero weigh heavily in the objective.",
    "start": "3804300",
    "end": "3810732"
  },
  {
    "text": "In practice, two\npeople do anything to maybe alleviate that a bit. I think for discrete\ndistributions, maybe you have a softmax\nthat will make it",
    "start": "3810732",
    "end": "3817380"
  },
  {
    "text": "a bit further away from zero. Is that something that happens\nin continuous distributions, or how do you help\nalleviate that?",
    "start": "3817380",
    "end": "3823819"
  },
  {
    "text": "Yeah, so that's goes back\nto what we were saying, what is this model doing? It's trying to make sure\nthat if something is possible",
    "start": "3823820",
    "end": "3830970"
  },
  {
    "text": "happen in the\ntraining set, you're going to be forced to put\nsome probability mass there, which is a good thing, right?",
    "start": "3830970",
    "end": "3836980"
  },
  {
    "text": "You're going to be forced to\nspread out the probability mass so that the entire\nsupport of the entire data set",
    "start": "3836980",
    "end": "3843569"
  },
  {
    "text": "is covered by your model. Now, the problem is\nthat you're going to always have finite\nmodeling capacity, right?",
    "start": "3843570",
    "end": "3850160"
  },
  {
    "text": "So if you put\nprobability mass there, you might be forced to put\nprobability mass somewhere that you didn't want to.",
    "start": "3850160",
    "end": "3856930"
  },
  {
    "text": "And maybe then your model will\nhallucinate weird stuff that was not in the\ntraining set, but you",
    "start": "3856930",
    "end": "3862890"
  },
  {
    "text": "have to generate them because\nyou're forced by this objective to spread out the\nprobability mass.",
    "start": "3862890",
    "end": "3868340"
  },
  {
    "text": "Again, back to precision recall. You need to have a\nvery high recall. Everything in the training set\nhas to be non-zero probability.",
    "start": "3868340",
    "end": "3876430"
  },
  {
    "text": "And as a result,\nmaybe your precision goes down because then you start\nto generate stuff that should not have been generating.",
    "start": "3876430",
    "end": "3882220"
  },
  {
    "text": "So that's the\ntakeaway of that line. ",
    "start": "3882220",
    "end": "3887819"
  },
  {
    "text": "Cool. Now, why does this work?",
    "start": "3887820",
    "end": "3892960"
  },
  {
    "text": "That's an example of why can\nyou approximate this expectation with this sample average.",
    "start": "3892960",
    "end": "3898450"
  },
  {
    "text": "This is something that is\nbasically a Monte Carlo estimate. You might have seen it before.",
    "start": "3898450",
    "end": "3904510"
  },
  {
    "text": "The idea is that if you have an\nexpectation of some function, there's a random variable x. There is a function g of x.",
    "start": "3904510",
    "end": "3910860"
  },
  {
    "text": "You want to get the\nexpected value of g of x, which is just this thing. You can approximate\nthis by just taking--",
    "start": "3910860",
    "end": "3920390"
  },
  {
    "text": "the true thing would look at\nall the things that can happen, and it would weight them\nwith the probability under p.",
    "start": "3920390",
    "end": "3927089"
  },
  {
    "text": "Alternatively,\nwhat you can do is you can just\ngenerate T scenarios, T samples, and look at\nthe average value of G",
    "start": "3927090",
    "end": "3935910"
  },
  {
    "text": "under these T samples. And that should be a reasonable\napproximation, right?",
    "start": "3935910",
    "end": "3941730"
  },
  {
    "text": "You can approximate\nthe expectation by looking at the value\nof the function on this T",
    "start": "3941730",
    "end": "3948730"
  },
  {
    "text": "representative samples. And these g hat is\na random variable",
    "start": "3948730",
    "end": "3957849"
  },
  {
    "text": "because it depends on this\nsamples, x1 through xt.",
    "start": "3957850",
    "end": "3964160"
  },
  {
    "text": "But it has good\nproperties in the sense that in expectation, it gives\nyou back what you wanted.",
    "start": "3964160",
    "end": "3975580"
  },
  {
    "text": "So although this g hat\nis now a random variable,",
    "start": "3975580",
    "end": "3981280"
  },
  {
    "text": "in expectation this\nrandom variable has the right value, which\nis the true expectation",
    "start": "3981280",
    "end": "3989530"
  },
  {
    "text": "of the function, the thing\nyou wanted to compute. And the more samples you get,\nthe better the approximation is.",
    "start": "3989530",
    "end": "3998880"
  },
  {
    "text": "So although g hat is\nrandom, as you increase the number of samples T, g\nhat converges pretty strongly",
    "start": "3998880",
    "end": "4009130"
  },
  {
    "text": "to this expected value. So the more samples you take,\nthe less randomness there is,",
    "start": "4009130",
    "end": "4017270"
  },
  {
    "text": "and the more likely\nyou are to get close to the true answer\nyou're looking for, which is the expectation\nof the function.",
    "start": "4017270",
    "end": "4025180"
  },
  {
    "text": "And the variance also\ngoes down as the number of samples increases.",
    "start": "4025180",
    "end": "4031030"
  },
  {
    "text": "So you have a random variable\nthat an expectation gives you the answer you want. And as you increase\nthe number of samples,",
    "start": "4031030",
    "end": "4037120"
  },
  {
    "text": "the variance of this\nrandom variables becomes smaller\nand smaller, which means that your approximation\nbecomes more and more reliable.",
    "start": "4037120",
    "end": "4045400"
  },
  {
    "text": "The less unlikely you\nare that the estimate you have is wildly off.",
    "start": "4045400",
    "end": "4051520"
  },
  {
    "text": "And that's exactly\nwhat we're doing here. We are approximating.",
    "start": "4051520",
    "end": "4057610"
  },
  {
    "text": "This expectation is a number. It's not random. We're approximating it\nwith a quantity that",
    "start": "4057610",
    "end": "4063370"
  },
  {
    "text": "depends on the training set,\nso different training sets would give you\ndifferent answers.",
    "start": "4063370",
    "end": "4069220"
  },
  {
    "text": "But if the training set\nis sufficiently large, this sample average would be\nvery close to the expectation.",
    "start": "4069220",
    "end": "4077190"
  },
  {
    "text": "And the larger the training\nset is, the more likely it is that this sample average\nthat you get on the data set",
    "start": "4077190",
    "end": "4084569"
  },
  {
    "text": "is actually going to be pretty\nclose to the true expected value that you care about.",
    "start": "4084570",
    "end": "4089705"
  },
  {
    "text": " Cool.",
    "start": "4089705",
    "end": "4095369"
  },
  {
    "text": "And we'll see this idea\ncome up often, this idea that there is an intractable\nexpectation that you",
    "start": "4095370",
    "end": "4102792"
  },
  {
    "text": "have to deal with,\nand you're going to approximate it using\nsamples from the distribution. It's a pretty\nconvenient way of making",
    "start": "4102792",
    "end": "4111189"
  },
  {
    "text": "algorithm is more\ncomputationally tractable, essentially.",
    "start": "4111189",
    "end": "4116699"
  },
  {
    "text": "Was there a question?  OK, right.",
    "start": "4116700",
    "end": "4122939"
  },
  {
    "text": "So now back to learning. I mean, you've probably seen\nmaximum likelihood learning",
    "start": "4122939",
    "end": "4128389"
  },
  {
    "text": "and examples, like learning\nthe parameters of a Bernoulli random variable. So let's say you have two\noutcomes, heads and tails.",
    "start": "4128390",
    "end": "4137450"
  },
  {
    "text": "You have a data set. So you've seen that you\nflipped the coin five times,",
    "start": "4137450",
    "end": "4143449"
  },
  {
    "text": "and the first two\ntimes were heads, then you have a tail,\nthen a heads and a tail.",
    "start": "4143450",
    "end": "4149120"
  },
  {
    "text": "You assume that there is some\nunderlying data distribution that produced the results\nof this experiment",
    "start": "4149120",
    "end": "4156170"
  },
  {
    "text": "that you did with five\ntosses of the coin. And then you model all these\nBernoulli distributions.",
    "start": "4156170",
    "end": "4163439"
  },
  {
    "text": "And then, again, you\njust need one parameter to describe the\nprobability of heads versus the probability of tail.",
    "start": "4163439",
    "end": "4169759"
  },
  {
    "text": "And then you could\ntry to fit, and you try to find a model\nof the world that is as close as possible to the\ntrue data generating process.",
    "start": "4169760",
    "end": "4180799"
  },
  {
    "text": "For example, you might see that\nthere is three heads out of five of coin flips, and\nthen you try to find",
    "start": "4180800",
    "end": "4186229"
  },
  {
    "text": "a good model for this data. And a way to do it is\nmaximum likelihood.",
    "start": "4186229",
    "end": "4193318"
  },
  {
    "text": "So in this case, P theta would\nbe really, really simple. It's just a single\nBernoulli random variable.",
    "start": "4193319",
    "end": "4199162"
  },
  {
    "text": "You have one parameter, which\nis the probability two heads. One minus theta is the\nprobability of tails. And then you have\nyour data set, which",
    "start": "4199162",
    "end": "4206389"
  },
  {
    "text": "is three heads and two\ntails, and then you can evaluate the\nlikelihood of the data.",
    "start": "4206390",
    "end": "4212060"
  },
  {
    "text": "And it's just that expression. So you have theta,\ntheta, 1 minus theta,",
    "start": "4212060",
    "end": "4217100"
  },
  {
    "text": "because the third result\nis a tail and so forth. And now this is a\nfunction of theta,",
    "start": "4217100",
    "end": "4223790"
  },
  {
    "text": "as you change theta,\nthe probability that your model assigns\nto the data set changes.",
    "start": "4223790",
    "end": "4229199"
  },
  {
    "text": "And if you plot it,\nit has the shape. And then maximum\nlikelihood would",
    "start": "4229200",
    "end": "4235599"
  },
  {
    "text": "tell you pick the\ntheta that maximizes the probability of observing\nthis particular data set.",
    "start": "4235600",
    "end": "4241179"
  },
  {
    "text": "And that basically\ncorresponds to trying to find a maximum\nof this function.",
    "start": "4241180",
    "end": "4246390"
  },
  {
    "text": "And in this case,\nwhat's the solution? That's 0.6, right?",
    "start": "4246390",
    "end": "4252485"
  },
  {
    "text": "In this case, you can actually\nsolve this in closed form, and you can work out what\nis the optimal theta, and it's going to be 0.6.",
    "start": "4252485",
    "end": "4260080"
  },
  {
    "text": "And so we're basically going\nto do the same thing now but for autoregressive models,\nso this is the same idea.",
    "start": "4260080",
    "end": "4268000"
  },
  {
    "text": "You have, except that now\ntheta is very high-dimensional. It's all possible parameters\nof a neural network,",
    "start": "4268000",
    "end": "4275440"
  },
  {
    "text": "but the y-axis is the same. It's basically the\nprobability that your model",
    "start": "4275440",
    "end": "4280540"
  },
  {
    "text": "assigns to the data set. And then you try\nto find theta that maximizes the probability\nof observing the data set",
    "start": "4280540",
    "end": "4287110"
  },
  {
    "text": "that you have access to. And the good news is that\nin an autoregressive model,",
    "start": "4287110",
    "end": "4292810"
  },
  {
    "text": "evaluating likelihoods\nis relatively easy. If you want to evaluate\nthe probability",
    "start": "4292810",
    "end": "4298420"
  },
  {
    "text": "that the model assigns to a\nparticular image or sentence or whatever, the probability of\nx is just given by chain rule.",
    "start": "4298420",
    "end": "4307870"
  },
  {
    "text": "It's the product of the\nconditional probabilities. And so evaluating the\nprobability of a single data",
    "start": "4307870",
    "end": "4315960"
  },
  {
    "text": "point is very easy. It's exactly the\nsame computation we did before when we were\ntrying to do anomaly detection.",
    "start": "4315960",
    "end": "4323130"
  },
  {
    "text": "We just go through\nall the conditionals, and you multiply them together.",
    "start": "4323130",
    "end": "4329190"
  },
  {
    "text": "And how to evaluate the\nprobability of a data set.",
    "start": "4329190",
    "end": "4335390"
  },
  {
    "text": "While the probability\nof the data set is just the product\nof the probabilities of the individual data points,\nand the individual data points",
    "start": "4335390",
    "end": "4343990"
  },
  {
    "text": "are just obtained\nthrough chain rule. And so, again, it's\nall pretty simple.",
    "start": "4343990",
    "end": "4350890"
  },
  {
    "text": "If you want to maximize the\nprobability of observing the data set that\nyou have access to,",
    "start": "4350890",
    "end": "4356890"
  },
  {
    "text": "you can also take a log. And you can maximize\nthe log likelihood. And you get an\nexpression that when",
    "start": "4356890",
    "end": "4366460"
  },
  {
    "text": "you can turn the log of a\nproduct into a sum of logs. But we no longer have\na closed form solution.",
    "start": "4366460",
    "end": "4373429"
  },
  {
    "text": "So before, for the\nBernoulli coin flips, you all knew the answer is 0.6.",
    "start": "4373430",
    "end": "4379010"
  },
  {
    "text": "If you have a deep\nneural network here, you no longer have a closed\nform way of choosing theta,",
    "start": "4379010",
    "end": "4384850"
  },
  {
    "text": "and you have to rely on\nsome optimization algorithm to try to make this objective\nfunction as high as possible.",
    "start": "4384850",
    "end": "4394300"
  },
  {
    "text": "You negate it and try to\nmake it as small as possible. And so, for example, you\ncan use gradient descent.",
    "start": "4394300",
    "end": "4402560"
  },
  {
    "text": "So that's the objective function\nthat we're trying to optimize. And if you take a log,\nI guess it boils down",
    "start": "4402560",
    "end": "4411020"
  },
  {
    "text": "to this, which is\nmuch more natural. So you go through\nall the data points.",
    "start": "4411020",
    "end": "4416480"
  },
  {
    "text": "You go through all the\nvariables in each data point, and you look at the log\nprobability assigned",
    "start": "4416480",
    "end": "4423110"
  },
  {
    "text": "of that variable,\ngiven all the ones that come before it\nin that data point.",
    "start": "4423110",
    "end": "4428300"
  },
  {
    "text": "So equivalently,\nwhat you're doing is-- remember that this P\nneural here are basically classifiers that try to\npredict the next value, given",
    "start": "4428300",
    "end": "4436460"
  },
  {
    "text": "everything before it. So this loss is\nbasically just evaluating the average loss of all these\nclassifiers across data points",
    "start": "4436460",
    "end": "4445700"
  },
  {
    "text": "and across variables. And so again, basically\nminimizing KL-divergence",
    "start": "4445700",
    "end": "4452650"
  },
  {
    "text": "is the same as maximizing\nlog likelihood, which is the same\nas basically trying to make these classifiers\nperform as well as they can.",
    "start": "4452650",
    "end": "4461179"
  },
  {
    "text": "They should do a pretty good\njob at predicting overall data points. J overall variables I. They\nshould do a pretty good job",
    "start": "4461180",
    "end": "4467740"
  },
  {
    "text": "at predicting the\nnext variable, given what they've seen so far for\nthat particular data point.",
    "start": "4467740",
    "end": "4476650"
  },
  {
    "text": "And so all of this is\nbasically boiling down to let's try to make these\nclassifiers that predict",
    "start": "4476650",
    "end": "4483310"
  },
  {
    "text": "the next variable, given\nthe ones before it, as efficient, as good\nas possible in terms of the essentially\ncross entropy.",
    "start": "4483310",
    "end": "4491699"
  },
  {
    "text": "So one way to do it is you can\ninitialize all the parameters at random, and then you can\ncompute gradients on this loss",
    "start": "4491700",
    "end": "4498900"
  },
  {
    "text": "by backpropagation, and then\nyou just do gradient ascent on this thing.",
    "start": "4498900",
    "end": "4506100"
  },
  {
    "text": "It's non-convex. But in practice,\nbasically, that's how you would train\nall these models. And it tends to work\npretty well in practice.",
    "start": "4506100",
    "end": "4516090"
  },
  {
    "text": "One thing to note\nis that as written, this quantity involves a\nsum over an entire data set.",
    "start": "4516090",
    "end": "4526380"
  },
  {
    "text": "Like, if you want to know\nwhat's the effect of changing the parameter of one\nof these classifiers.",
    "start": "4526380",
    "end": "4532510"
  },
  {
    "text": "Let's say you want to get\nthe gradient of the loss with respect of, let's\nsay, theta I, where theta I",
    "start": "4532510",
    "end": "4539460"
  },
  {
    "text": "is basically the parameters of\nthe i-th conditional, you would have to sum over the whole data\nset to get this gradient, which",
    "start": "4539460",
    "end": "4550230"
  },
  {
    "text": "would be, of course,\nway too expensive. Because you would have to\ngo through the whole data set to figure out how\nto adjust the parameters",
    "start": "4550230",
    "end": "4557070"
  },
  {
    "text": "of your classifier. And that's tricky.",
    "start": "4557070",
    "end": "4563679"
  },
  {
    "text": "But well, here I'm actually-- OK, the good news is,\nOK, each condition",
    "start": "4563680",
    "end": "4570780"
  },
  {
    "text": "can be optimized separately, if\nthere is no parameter sharing. In practice, there is\nalways parameter sharing. ",
    "start": "4570780",
    "end": "4578330"
  },
  {
    "text": "The challenge is that you have\nthis big sum over all the data points in the data set.",
    "start": "4578330",
    "end": "4584600"
  },
  {
    "text": "But again, what we can do is, we\ncan use a Monte Carlo estimate. So instead of going\nthrough the whole data set,",
    "start": "4584600",
    "end": "4592240"
  },
  {
    "text": "we can try to estimate\nwhat is the gradient, just by looking at a small\nsample of data points.",
    "start": "4592240",
    "end": "4598240"
  },
  {
    "text": "Just like before, we were\napproximating an expectation with a sample average.",
    "start": "4598240",
    "end": "4603579"
  },
  {
    "text": "We can think of this sum over\nm over all the data points. We can multiply by\nm and divide by 1/m,",
    "start": "4603580",
    "end": "4611230"
  },
  {
    "text": "and then we can think of this\nsum over 1/m as an expectation, with respect to a uniform\ndistribution over the data",
    "start": "4611230",
    "end": "4618909"
  },
  {
    "text": "points in the data set.  And so you can write\ndown the gradient",
    "start": "4618910",
    "end": "4625810"
  },
  {
    "text": "as the expectation of the\ngradient, with respect to a uniform distribution\nover the data set.",
    "start": "4625810",
    "end": "4633719"
  },
  {
    "text": "And so far, we haven't\ngained anything. But now you can do. Monte Carlo.",
    "start": "4633720",
    "end": "4639210"
  },
  {
    "text": "You can approximate\nthis expectation by taking a bunch of samples\nand evaluating the gradient only",
    "start": "4639210",
    "end": "4645450"
  },
  {
    "text": "on those samples. And that's basically stochastic\ngradient descent or mini batch,",
    "start": "4645450",
    "end": "4650880"
  },
  {
    "text": "where you would basically select\na small subset of data points.",
    "start": "4650880",
    "end": "4657040"
  },
  {
    "text": "You will evaluate the\ngradient on those data points, and you would update\nyour model accordingly.",
    "start": "4657040",
    "end": "4663650"
  },
  {
    "text": "And so we see another layer of\nMonte Carlo simulation or Monte",
    "start": "4663650",
    "end": "4669469"
  },
  {
    "text": "Carlo estimate, where instead\nof evaluating the full gradient, you evaluate the gradient\non a subset of data points",
    "start": "4669470",
    "end": "4675380"
  },
  {
    "text": "to make things scalable.  And what else?",
    "start": "4675380",
    "end": "4682468"
  },
  {
    "text": "Yeah, the other\nthing to keep in mind is that, well, there is\nalways the risk of overfitting that came up before.",
    "start": "4682468",
    "end": "4688450"
  },
  {
    "text": "If you just blindly\noptimize that objective, you could just\nmemorize the data set.",
    "start": "4688450",
    "end": "4694590"
  },
  {
    "text": "So the data becomes the model. You're going to perform pretty\nwell at this prediction task,",
    "start": "4694590",
    "end": "4702809"
  },
  {
    "text": "but that's not what we want. So we don't care about the\nperformance on the data set.",
    "start": "4702810",
    "end": "4710540"
  },
  {
    "text": "We care about performance\non unseen samples that come from the same\ndistribution as the one",
    "start": "4710540",
    "end": "4716889"
  },
  {
    "text": "we've used for training. So the same problems\nthat we've seen when you train a machine\nlearning model, apply here.",
    "start": "4716890",
    "end": "4723790"
  },
  {
    "text": "Blindly minimizing\nthis loss might not do what we want because you can\ndo very well on the training",
    "start": "4723790",
    "end": "4729880"
  },
  {
    "text": "set, but you might not\nbe doing well in general. You might not be generalizing.",
    "start": "4729880",
    "end": "4736030"
  },
  {
    "text": "And so what you\nwould need to do is to, somehow, restrict the\nhypothesis space or regularize",
    "start": "4736030",
    "end": "4742420"
  },
  {
    "text": "the model somehow so\nthat this doesn't happen. So that it doesn't just\nmemorize the training set,",
    "start": "4742420",
    "end": "4747460"
  },
  {
    "text": "and it doesn't-- and so you don't get this\noverfitting behavior.",
    "start": "4747460",
    "end": "4753760"
  },
  {
    "text": "And the problem-- and then you\nget the usual bias variance",
    "start": "4753760",
    "end": "4759130"
  },
  {
    "text": "trade-offs, where if you limit\nthe model too much, if you",
    "start": "4759130",
    "end": "4764739"
  },
  {
    "text": "restrict the modeling capacity\ntoo much, instead of using deep neural network, you\nuse logistic regressions,",
    "start": "4764740",
    "end": "4770080"
  },
  {
    "text": "or you make very strong\nconditional independence assumptions. Your modeling capacity\nor hypothesis space",
    "start": "4770080",
    "end": "4777730"
  },
  {
    "text": "becomes too limited,\nand you might not be able to do well at minimizing\nthat loss on the training set.",
    "start": "4777730",
    "end": "4784270"
  },
  {
    "text": "And this is basically bias\nbecause it limits how well you can approximate the\ntarget distribution,",
    "start": "4784270",
    "end": "4790540"
  },
  {
    "text": "even if you could optimize\nas well as you could. And then the trade off\nhere is that if you",
    "start": "4790540",
    "end": "4798220"
  },
  {
    "text": "choose model families that\nare too flexible, then you end you encounter the\nother issue, which is variance.",
    "start": "4798220",
    "end": "4806010"
  },
  {
    "text": "So your model might\nbe fitting too well. It might be fitting even\nbetter than the true model that",
    "start": "4806010",
    "end": "4812770"
  },
  {
    "text": "generated the data. And even small changes\nto the data set could have huge changes to the\nparameters that you output.",
    "start": "4812770",
    "end": "4821680"
  },
  {
    "text": "And that's variance. And so you want to\nfind a sweet spot",
    "start": "4821680",
    "end": "4826720"
  },
  {
    "text": "where you balance\nthe effect of bias and variance on the\nperformance of your model.",
    "start": "4826720",
    "end": "4834260"
  },
  {
    "text": "And visually, I think,\nthis is an example.",
    "start": "4834260",
    "end": "4839989"
  },
  {
    "text": "Let's say that you have\na bunch of data points, and you're trying\nto fit a curve, trying to predict y from x.",
    "start": "4839990",
    "end": "4846160"
  },
  {
    "text": "If you choose a very simple\nspace of possible relationships, like all linear models, you\ncan do very well at fitting.",
    "start": "4846160",
    "end": "4853650"
  },
  {
    "text": "But somehow, if the model\nclass is too simple, you're not going to\nbe able to capture the true trend in the data.",
    "start": "4853650",
    "end": "4859980"
  },
  {
    "text": "And so the bias here\nwill hurt you too much. If you say underfits--",
    "start": "4859980",
    "end": "4866599"
  },
  {
    "text": "if you choose a very flexible\nmodel, lots of parameters, you're going to be fitting\nthe data set extremely well,",
    "start": "4866600",
    "end": "4874560"
  },
  {
    "text": "but you can see that, perhaps,\nit's too flexible, the model. If you were to change one\nsingle data point a little bit,",
    "start": "4874560",
    "end": "4881210"
  },
  {
    "text": "the predictions would\nchange drastically. And that's maybe overfitting.",
    "start": "4881210",
    "end": "4886530"
  },
  {
    "text": "And so you want, maybe,\nthat sweet spot, where you have a low degree\npolynomial that fits the data, A little bit\nworse than this higH-degree",
    "start": "4886530",
    "end": "4893510"
  },
  {
    "text": "polynomial, but it\nwill generalize. And it will perform\nOK in practice.",
    "start": "4893510",
    "end": "4898849"
  },
  {
    "text": "And yeah, question? Is there any way we can-- because when we\ntrain our models,",
    "start": "4898850",
    "end": "4905270"
  },
  {
    "text": "we will see those\nhyperparameters fixed, and then we train the model. So it's like the\ncase you show here.",
    "start": "4905270",
    "end": "4911010"
  },
  {
    "text": "We say this is a polynomial\nwith order three. Is there any way for us to\nturn the model parameters",
    "start": "4911010",
    "end": "4918650"
  },
  {
    "text": "into trainable or have a\nvery elegant framework? Well, yeah. So there's a few\nthings you can do.",
    "start": "4918650",
    "end": "4924670"
  },
  {
    "text": "One is to prevent overfitting\nis you could be Bayesian, but that's very hard\ncomputationally.",
    "start": "4924670",
    "end": "4930800"
  },
  {
    "text": "Another thing you\ncan do is you can try to do cross\nvalidation, where you're",
    "start": "4930800",
    "end": "4936050"
  },
  {
    "text": "keep some held out\ndata to evaluate the performance of your model. And if you see that there is a\nbig gap between the performance",
    "start": "4936050",
    "end": "4942890"
  },
  {
    "text": "that you had at training\ntime versus what you had at on the\nvalidation set, then you know\nyou're overfitting,",
    "start": "4942890",
    "end": "4948330"
  },
  {
    "text": "and so maybe you want to reduce\nthe complexity of your model. And so, yeah, one\nthing you can do",
    "start": "4948330",
    "end": "4955875"
  },
  {
    "text": "is you can reduce the complexity\nof your neural networks, reduce the number of parameters,\nshare parameters, make the set",
    "start": "4955875",
    "end": "4963020"
  },
  {
    "text": "smaller in some way. Another thing that was\nmentioned before is you",
    "start": "4963020",
    "end": "4968580"
  },
  {
    "text": "could try to use some soft\npreference for simpler models so that if you have two models\nthat fit the data equally well,",
    "start": "4968580",
    "end": "4975810"
  },
  {
    "text": "they achieve the same loss. Maybe you have a\nregularization term that says prefer the simpler one.",
    "start": "4975810",
    "end": "4981750"
  },
  {
    "text": "Maybe the one with fewer\nparameters or the one where the magnitude of\nthe parameters is smaller.",
    "start": "4981750",
    "end": "4988445"
  },
  {
    "text": " And the other thing is\nwhat I just mentioned.",
    "start": "4988445",
    "end": "4995300"
  },
  {
    "text": "You can always\nevaluate performance on some held out validation set. This is actually what\npeople do in practice.",
    "start": "4995300",
    "end": "5001980"
  },
  {
    "text": "And you can check if there\nis a big gap between training and validation loss, then\nyou know that you're probably",
    "start": "5001980",
    "end": "5009060"
  },
  {
    "text": "overfitting. And maybe you want to\nreduce the size of the set, or you want to do something\nto prevent that overfitting.",
    "start": "5009060",
    "end": "5018630"
  },
  {
    "text": "And yeah, I think that's\nprobably a good place to stop. I think training\nconditional models",
    "start": "5018630",
    "end": "5024690"
  },
  {
    "text": "is pretty much the same thing. ",
    "start": "5024690",
    "end": "5032000"
  }
]