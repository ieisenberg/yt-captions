[
  {
    "start": "0",
    "end": "100000"
  },
  {
    "text": "Welcome back to the third lecture of CS229 So in the first two lectures,",
    "start": "4880",
    "end": "11129"
  },
  {
    "text": "we've been mostly going over the course prerequisites, linear algebra probability. So on Wednesday's lecture,",
    "start": "11130",
    "end": "17880"
  },
  {
    "text": "just- just to recap, uh, we went over the, um, concept of determinant, its geometrical interpretation.",
    "start": "17880",
    "end": "25125"
  },
  {
    "text": "And we went through two different kinds of decomposition of a matrix, the eigenvalue decomposition and the singular value decomposition.",
    "start": "25125",
    "end": "34785"
  },
  {
    "text": "Um, after that, we, we, uh, quickly went through some matrix calculus.",
    "start": "34785",
    "end": "41790"
  },
  {
    "text": "Um, the different ways in which linear algebra plays a role in calculus when we are dealing with, uh, multivariate functions.",
    "start": "41790",
    "end": "50829"
  },
  {
    "text": "And after that, we switched gears into probability to review some basics of probability theory.",
    "start": "50830",
    "end": "57289"
  },
  {
    "text": "Um, today we will [NOISE] finish up the review of probability theory and maybe in a few,",
    "start": "57290",
    "end": "64245"
  },
  {
    "text": "you know, just, uh, spend a couple of minutes on, on some basics of statistics of why statistics plays a role in machine learning.",
    "start": "64245",
    "end": "71195"
  },
  {
    "text": "And after that, we will jump right into our first learning algorithm, uh, linear regression.",
    "start": "71195",
    "end": "77869"
  },
  {
    "text": "That's the plan for today. Any questions about what we've covered so far before we get started?",
    "start": "77870",
    "end": "83970"
  },
  {
    "text": "All right. Okay. So, um, we were reviewing probability, um,",
    "start": "84020",
    "end": "91020"
  },
  {
    "text": "and maybe let me just go back a few slides to just touch upon a few,",
    "start": "91020",
    "end": "96969"
  },
  {
    "text": "um, important concepts, right? So we covered, um,",
    "start": "96970",
    "end": "103845"
  },
  {
    "start": "100000",
    "end": "133000"
  },
  {
    "text": "what's a sample space? Sample space are, um, samples are basically the collection of outcomes of random experiments.",
    "start": "103845",
    "end": "112080"
  },
  {
    "text": "And events are subsets of the sample space. Um, we assign probabilities to events,",
    "start": "112080",
    "end": "122094"
  },
  {
    "text": "not to random outcomes, but to, uh, um, sets of random outcome, uh, er, the sets of, uh,",
    "start": "122094",
    "end": "128289"
  },
  {
    "text": "outcomes, and those are the, uh, axioms of probability.",
    "start": "128290",
    "end": "134445"
  },
  {
    "text": "We went over what is, uh, the meaning of independence with respect to events.",
    "start": "134445",
    "end": "141165"
  },
  {
    "text": "Um, an event is independent of the other if the probability of their intersection is the- is the,",
    "start": "141165",
    "end": "148760"
  },
  {
    "text": "is the product of their individual probabilities. And then we, um,",
    "start": "148760",
    "end": "154000"
  },
  {
    "start": "152000",
    "end": "245000"
  },
  {
    "text": "discussed this concept of a random variable. A random variable is something that maps the,",
    "start": "154000",
    "end": "160489"
  },
  {
    "text": "the space of outcomes, which could be anything. It could be a string of heads and tails.",
    "start": "160490",
    "end": "165560"
  },
  {
    "text": "It could be the color of a dye, it could be, uh, anything whatsoever. And we map it on to the real line.",
    "start": "165560",
    "end": "171545"
  },
  {
    "text": "And that's when we can start doing mathematics with, with, with, with randomness. You know, the concept of a random variable maps outcomes which could be in,",
    "start": "171545",
    "end": "179555"
  },
  {
    "text": "in any sample space onto the real line, right?",
    "start": "179555",
    "end": "185564"
  },
  {
    "text": "For example, we could have, uh, a random variable that counts the number of heights on- in a given event,",
    "start": "185565",
    "end": "191224"
  },
  {
    "text": "uh, and to map it- map that, uh, event, uh, I'm so- outcome.",
    "start": "191225",
    "end": "196680"
  },
  {
    "text": "Let, let me, um, uh, say that again. So here we have an example of a random variable which, uh, takes as input.",
    "start": "196680",
    "end": "202715"
  },
  {
    "text": "Um, the outcome, the outcome, uh, in this case is a string of 10 heads or tail, uh, coin flips.",
    "start": "202715",
    "end": "209970"
  },
  {
    "text": "And that is just a string of, you know, um, heads or tails symbols, it's not a number. And a random variable- an example of a random variable in this case,",
    "start": "209970",
    "end": "218105"
  },
  {
    "text": "is a random variable that just counts the number of, um, heads in, in, um,",
    "start": "218105",
    "end": "224204"
  },
  {
    "text": "in that sequence of 10, ah, coin flips and maps it to, uh, uh, a real number, okay?",
    "start": "224205",
    "end": "231670"
  },
  {
    "text": "And, um, by, by, uh, values of x, we mean the set of all possible real values that the random variable can possibly map,",
    "start": "232190",
    "end": "242000"
  },
  {
    "text": "uh, any of the, uh, outcomes into, okay?",
    "start": "242000",
    "end": "247080"
  },
  {
    "start": "245000",
    "end": "355000"
  },
  {
    "text": "And then we, uh, spoke about the cumulative distribution function. So, uh, previously we saw that, uh,",
    "start": "247080",
    "end": "253120"
  },
  {
    "text": "a probability measure is defined on events. But once we map it to the real line, once we convert, uh, to, uh,",
    "start": "253120",
    "end": "258790"
  },
  {
    "text": "define a random variable, the probability, the probability measure, kind of, induces some,",
    "start": "258790",
    "end": "264720"
  },
  {
    "text": "kind of, uh, uh, a probability measure on the real line, right? And that can be captured by, uh,",
    "start": "264720",
    "end": "271160"
  },
  {
    "text": "something called as the cumulative distribution function. It is basically the measure or the, the, um,",
    "start": "271160",
    "end": "277540"
  },
  {
    "text": "probability assigned to the set of all outcomes which get mapped to a value less than or equal to the,",
    "start": "277540",
    "end": "286540"
  },
  {
    "text": "uh, the desired threshold, right? Um, so this is how, um, um,",
    "start": "286540",
    "end": "293130"
  },
  {
    "text": "a CDF would, you know, uh, look for a continuous random variable. So the, the height of this function is measuring the amount of,",
    "start": "293130",
    "end": "302000"
  },
  {
    "text": "uh, probability assigned to the set of all events. The set of all, um, uh,",
    "start": "302000",
    "end": "309560"
  },
  {
    "text": "the set of all outcomes that map less than or equal to the, the, uh, uh, desired threshold.",
    "start": "309560",
    "end": "316129"
  },
  {
    "text": "So that's why- what we have here. So it is, er, the P over here is now measuring an event,",
    "start": "316130",
    "end": "323625"
  },
  {
    "text": "a set of outcomes, and the set of out- outcomes is defined as the pre-image of the random variable, right?",
    "start": "323625",
    "end": "329585"
  },
  {
    "text": "The random variable maps outcomes to, to the real line. And, um, if you look at the pre-image of that function,",
    "start": "329585",
    "end": "335720"
  },
  {
    "text": "that is the set of all inputs that get mapped, uh, to a value less than, uh, or equal to the, the, uh, uh,",
    "start": "335720",
    "end": "342755"
  },
  {
    "text": "desired threshold, that will give you a probability and that's going to be a value between zero and one. And which is why, you know,",
    "start": "342755",
    "end": "349699"
  },
  {
    "text": "this, this, um, uh, the values of, of the CDF lie between zero and one, right?",
    "start": "349700",
    "end": "355965"
  },
  {
    "text": "And, uh, we spoke about discrete versus continuous variables. Discrete variables have a probability mass function,",
    "start": "355965",
    "end": "361925"
  },
  {
    "text": "and continuous variables have a probability density function, right? Um, a CDF or the cumulative, uh, the,",
    "start": "361925",
    "end": "368580"
  },
  {
    "text": "um, distribution function exists for all random variables. Um, that is the, the CDF that we,",
    "start": "368580",
    "end": "374599"
  },
  {
    "text": "um, saw over here. This- a CDF exists for all random variables, whether it's discrete or continuous.",
    "start": "374600",
    "end": "380480"
  },
  {
    "text": "But, um, the density function exists only for, uh,",
    "start": "380480",
    "end": "386030"
  },
  {
    "text": "continuous distributions, which is basically the derivativ e of the, the, um, CDF.",
    "start": "386030",
    "end": "392130"
  },
  {
    "text": "It's important to note that the density of, um, the, the value returned by the density function is not the probability.",
    "start": "392130",
    "end": "401450"
  },
  {
    "text": "Uh, what I mean by that is if, um, suppose this is x and we have some density function,",
    "start": "401450",
    "end": "410044"
  },
  {
    "text": "and suppose this is 0.3, okay? And this has some height, right?",
    "start": "410045",
    "end": "418020"
  },
  {
    "text": "It's important to, to note that, uh, the probability of x equals 0.3. is not equal to this height, right?",
    "start": "418020",
    "end": "429550"
  },
  {
    "text": "The probability for any continuous random variable, uh, the probability that though- the random variable takes a specific value is always 0.",
    "start": "429550",
    "end": "440380"
  },
  {
    "text": "That's, uh, that sounds, that may sound a little counter-intuitive if you're, um, if you're, uh, hearing this for the first time.",
    "start": "440380",
    "end": "446974"
  },
  {
    "text": "But, you know, uh, let, let me say that again. If we have a discrete probability, um, uh, uh,",
    "start": "446975",
    "end": "452530"
  },
  {
    "text": "discrete random variable with the probability mass function, I think 1, 2,",
    "start": "452530",
    "end": "458730"
  },
  {
    "text": "3, then the probability that the random variable takes on the value 3 is given by the height of the probability mass function.",
    "start": "458730",
    "end": "466180"
  },
  {
    "text": "But the probability density function is something that's fundamentally, uh, different in the sense the height of the, of, of,",
    "start": "466180",
    "end": "475020"
  },
  {
    "text": "or the, the density measured at that point is not the probability that the random variable takes on the value of,",
    "start": "475020",
    "end": "481720"
  },
  {
    "text": "for example, 0.3, right? Probabilities in case of continuous random variables are only defined on intervals,",
    "start": "481720",
    "end": "490164"
  },
  {
    "text": "or sets of intervals. So the area under the probability density function for a given,",
    "start": "490165",
    "end": "499185"
  },
  {
    "text": "uh, range of your, uh, input, the area under the probability density function is",
    "start": "499185",
    "end": "504500"
  },
  {
    "text": "the probability that the random variable takes a value in this range, right?",
    "start": "504500",
    "end": "509810"
  },
  {
    "text": "But the probability that a continuous random variable takes any specific value is always zero, okay?",
    "start": "509810",
    "end": "517410"
  },
  {
    "text": "So that's, that's, uh, that, that's- yes question? [BACKGROUND].",
    "start": "517870",
    "end": "550080"
  },
  {
    "text": "Well, so the, the, the question is, um, if I understand you correctly, let me rephrase it and then you can tell me if I",
    "start": "550080",
    "end": "556464"
  },
  {
    "text": "understood your question correctly, is, uh, we don't know where the, uh, what value the continuous random variable is going to take,",
    "start": "556465",
    "end": "563740"
  },
  {
    "text": "so how can you define, um, the probability to be zero? Well, so this is, you know,",
    "start": "563740",
    "end": "570375"
  },
  {
    "text": "uh, you can think of probability as the, the, um, so there are two,",
    "start": "570375",
    "end": "575970"
  },
  {
    "text": "two, kinds of, um, uh, interpretations to probabilities. One is, um, looking historically what's been, like,",
    "start": "575970",
    "end": "582779"
  },
  {
    "text": "the distribution of, of, uh, um, you know, the- your true random variable, uh, taking on in the past.",
    "start": "582780",
    "end": "588855"
  },
  {
    "text": "And then based on that, you can probably construct some kind of a density or estimate some kind of a density. What it actually means is just because if you take a continuous random variable,",
    "start": "588855",
    "end": "598550"
  },
  {
    "text": "the number of possible values it can take is uncountably infinite,",
    "start": "598550",
    "end": "604144"
  },
  {
    "text": "and that makes it, um, impossible. Or, or a continuous random variable is, is, um,",
    "start": "604145",
    "end": "611880"
  },
  {
    "text": "does not have point masses in the sense, uh, any given value in this,",
    "start": "611880",
    "end": "617089"
  },
  {
    "text": "among this infinite, infinite, uncountably infinite number of, um, um, possible values, any, any, um,",
    "start": "617090",
    "end": "624725"
  },
  {
    "text": "any given value of, of, of, these has zero probability of occurrence. But if you take a range,",
    "start": "624725",
    "end": "630245"
  },
  {
    "text": "you get a probability for the value falling in that range, right?",
    "start": "630245",
    "end": "637005"
  },
  {
    "text": "Okay, so that's, uh, discrete versus continuous probability. And, and then we, we, uh,",
    "start": "637005",
    "end": "643190"
  },
  {
    "text": "started talking about this very important concept called expectation, right?",
    "start": "643190",
    "end": "648450"
  },
  {
    "text": "Expectation is, um, expectation is, um, ah, um, a concept that's only associated with a random variable, right?",
    "start": "648450",
    "end": "657140"
  },
  {
    "text": "So, uh, we spoke about, um, outcomes.",
    "start": "657140",
    "end": "662040"
  },
  {
    "text": "And we spoke about events, right? And we define a probability measure on events.",
    "start": "662390",
    "end": "670800"
  },
  {
    "text": "So if we limit ourselves to, uh, just these concepts, then the concept of expectation does not come into picture.",
    "start": "672260",
    "end": "679324"
  },
  {
    "text": "Yes. Question. [BACKGROUND] Lights.",
    "start": "679325",
    "end": "688060"
  },
  {
    "text": "Is it okay? Okay. Can you see the slides as well?",
    "start": "688070",
    "end": "695010"
  },
  {
    "start": "693000",
    "end": "1261000"
  },
  {
    "text": "Is it too bright for the slides? All right. Okay. Okay. So, um, if we are only, uh,",
    "start": "695010",
    "end": "700380"
  },
  {
    "text": "limiting ourselves to the concepts of outcomes and events, then there is no such thing as an expectation. But once we define a random variable, okay,",
    "start": "700380",
    "end": "707959"
  },
  {
    "text": "a random variable, uh, as we saw last time was- it, um- we're going to draw the, um,",
    "start": "707960",
    "end": "714420"
  },
  {
    "text": "outcome space as- or the sample space with a squiggly line just because it's- it's- it's not like a linearly ordered line.",
    "start": "714420",
    "end": "720720"
  },
  {
    "text": "And then we define a random variable. So assume this is some random variable x that maps outcomes to the real line. All right.",
    "start": "720720",
    "end": "732185"
  },
  {
    "text": "You could have another random variable which maps the same sample space to the real line in a different way.",
    "start": "732185",
    "end": "740910"
  },
  {
    "text": "Let's call this y of Omega. Now, uh, only after we define this concept called a random variable,",
    "start": "740910",
    "end": "748454"
  },
  {
    "text": "the concept of expectation comes into picture. So, uh, expectation is, what is the,",
    "start": "748454",
    "end": "755399"
  },
  {
    "text": "uh- well, an informal way to think about expectation is, what value does this random variable take on average?",
    "start": "755400",
    "end": "763545"
  },
  {
    "text": "Right? That's- that's, uh, uh,- uh, so we have a function that maps, um, um,",
    "start": "763545",
    "end": "770490"
  },
  {
    "text": "outcomes to the real line and, um, generally we- we kind of flip this axis over and we- we- we,",
    "start": "770490",
    "end": "778590"
  },
  {
    "text": "uh- we- we start with the vertical line, uh, flipped over. And we ask ourselves, suppose we,",
    "start": "778590",
    "end": "787080"
  },
  {
    "text": "uh, there's a function g defined on top of this random variable, so let this be, you know,",
    "start": "787080",
    "end": "792420"
  },
  {
    "text": "some function g of x. [NOISE] And this is x.",
    "start": "792420",
    "end": "798930"
  },
  {
    "text": "And the values over here are- those values are- are- are coming from random out- uh,",
    "start": "798930",
    "end": "807270"
  },
  {
    "text": "coming from outcomes that got mapped onto the real line. And then, uh, the expectation is now asking,",
    "start": "807270",
    "end": "812730"
  },
  {
    "text": "what is the average value that, uh, g- g will return? Right? And the average value that g returns is gonna depend on",
    "start": "812730",
    "end": "820935"
  },
  {
    "text": "the way on- depend on what values we- we are feeding into x. Right? And the, uh- one expectation informally is,",
    "start": "820935",
    "end": "829710"
  },
  {
    "text": "if we feed values into g, according to the distribution,",
    "start": "829710",
    "end": "835425"
  },
  {
    "text": "that is induced by x. Okay? So events happen, you get sample events, all right,",
    "start": "835425",
    "end": "841935"
  },
  {
    "text": "map it to x through the random variable, feed that- that value of x into g and you get some value.",
    "start": "841935",
    "end": "847995"
  },
  {
    "text": "All right? So suppose this- this- this, uh, um, outcome happened, map it to x, right,",
    "start": "847995",
    "end": "855735"
  },
  {
    "text": "x maps it to g, and this is the value of g. Right? And similarly, uh, you repeat this experiment over and over. All right?",
    "start": "855735",
    "end": "864240"
  },
  {
    "text": "Different outcomes map you to different values of x and different values of x map you to different values of g of x.",
    "start": "864240",
    "end": "870225"
  },
  {
    "text": "And you're measuring what, you know- the various values of g. And you're trying to ask the question,",
    "start": "870225",
    "end": "875835"
  },
  {
    "text": "what's the average value of g in general? What- right. And you can ask the question of average value",
    "start": "875835",
    "end": "881774"
  },
  {
    "text": "only because now you have things on the real line, right? On sample space, there is no such thing as an average.",
    "start": "881775",
    "end": "887579"
  },
  {
    "text": "[NOISE] [LAUGHTER] There's no such thing as, uh, an average sample because,",
    "start": "887580",
    "end": "894194"
  },
  {
    "text": "you know, you have a die with six colors, what's the average color? You know, it's- it's- it's not meaningful. Once you map them to numbers,",
    "start": "894195",
    "end": "899250"
  },
  {
    "text": "you can ask what's- what's the- what's the, um, uh, average value? All right. And, um, for a discrete random variable,",
    "start": "899250",
    "end": "907830"
  },
  {
    "text": "the expectation is, uh- is defined like this, right? Sum over all the possible values x can take.",
    "start": "907830",
    "end": "915885"
  },
  {
    "text": "And for- for- for each of those values multiply g of x, that is the value, uh,",
    "start": "915885",
    "end": "922050"
  },
  {
    "text": "g of x returns times a probability x takes that value, right? And it- this is just a weighted sum of the different values of g of x",
    "start": "922050",
    "end": "930540"
  },
  {
    "text": "according to the probability with which x can occur. All right. Very- very intuitive definition of, uh, expectation.",
    "start": "930540",
    "end": "937709"
  },
  {
    "text": "And similarly the, uh- for continuous random variables,",
    "start": "937709",
    "end": "942779"
  },
  {
    "text": "um, it is defined like this. In place of the- the, uh, summation, we have an integral.",
    "start": "942780",
    "end": "948945"
  },
  {
    "text": "Right? Now, you're integrating over all possible values of x, right, and for each value of x,",
    "start": "948945",
    "end": "955370"
  },
  {
    "text": "we measure the density. Right? Not the probability, but the density. And because we are now using densities,",
    "start": "955370",
    "end": "961639"
  },
  {
    "text": "we- we now have an integral instead of a sum. Yes, question? So do you [inaudible] this is like a distribution of [inaudible]?",
    "start": "961640",
    "end": "970350"
  },
  {
    "text": "X. So what- the question is, what's g of x? So x is a random variable and g of x is some function of x.",
    "start": "970350",
    "end": "977640"
  },
  {
    "text": "It's just some function of x, right? Uh, G is defined here. G is some- some function that takes a real value as input and outputs a real value.",
    "start": "977640",
    "end": "986204"
  },
  {
    "text": "Right? And you can- if- g is just a function. You- you know, um, in this case,",
    "start": "986205",
    "end": "992445"
  },
  {
    "text": "the input of x- in- input to g is going to be the random variable, which means,",
    "start": "992445",
    "end": "997515"
  },
  {
    "text": "you know, uh, it- it can take on different values according to the, uh, outcome of some random- random experiment.",
    "start": "997515",
    "end": "1003800"
  },
  {
    "text": "Okay. So that's the expectation of, uh, g of x. And we saw, uh, uh,",
    "start": "1003800",
    "end": "1009290"
  },
  {
    "text": "uh, an important, uh, concept, uh, last, uh- in the last lecture that you can approximate g of x by taking",
    "start": "1009290",
    "end": "1018790"
  },
  {
    "text": "random samples of x and calculating the average of g of x for that sample, all right?",
    "start": "1018790",
    "end": "1026059"
  },
  {
    "text": "And that's called the Monte Carlo estimate of the expectation. And as the number of samples tends to infinity, right?",
    "start": "1026060",
    "end": "1034355"
  },
  {
    "text": "As you- as you estimate the expectation with more and more number of samples, the- the Monte Carlo estimate converges to the true ex- true expectation, right?",
    "start": "1034355",
    "end": "1044929"
  },
  {
    "text": "And that's basically the law of large numbers. All right. Any questions about that?",
    "start": "1044930",
    "end": "1051060"
  },
  {
    "text": "Okay. So that was still, um- that- that's- that's, uh, what we covered, um,",
    "start": "1051220",
    "end": "1057304"
  },
  {
    "text": "on Wednesday and today, we're going to continue further. Now, we talk- we're gonna talk about variance.",
    "start": "1057305",
    "end": "1063710"
  },
  {
    "text": "So what's the variance of x? Uh, variance of x is defined as- is there a question? All right.",
    "start": "1063710",
    "end": "1071285"
  },
  {
    "text": "So the variance of x is- is defined as the expectation of x minus expectation of x squared.",
    "start": "1071285",
    "end": "1077195"
  },
  {
    "text": "Now what's- what's- what's happening here? It basically means, um, suppose x has some distribution.",
    "start": "1077195",
    "end": "1083660"
  },
  {
    "text": "For example, let's suppose this is the probability density of- of- uh- of x.",
    "start": "1083660",
    "end": "1091050"
  },
  {
    "text": "The expectation is also, uh, the point that's kind of,",
    "start": "1091930",
    "end": "1098225"
  },
  {
    "text": "uh- the point that's- you can- you can- you can think of it as kind of the,",
    "start": "1098225",
    "end": "1104780"
  },
  {
    "text": "uh- if- if you imagine this probability density function to have some kind of a shape,",
    "start": "1104780",
    "end": "1111545"
  },
  {
    "text": "then the expectation is that point on x, which- which is basically like the center of gravity.",
    "start": "1111545",
    "end": "1117304"
  },
  {
    "text": "Like- right? If you- if you- if you- if you take an actual physical thing of- of this shape and you try to balance it on the- on the expectation,",
    "start": "1117305",
    "end": "1125810"
  },
  {
    "text": "then, you know, your- your- that shape will kind of stay balanced, right? It's like the center of gravity, right?",
    "start": "1125810",
    "end": "1131059"
  },
  {
    "text": "That's- that's the intuition for expectation. Uh, the, uh- the variance is kind of trying to measure how spread",
    "start": "1131060",
    "end": "1137810"
  },
  {
    "text": "apart is this random variable around its, um- around its mean. So this is going to be, um, the expectation.",
    "start": "1137810",
    "end": "1146750"
  },
  {
    "text": "Um, the- the- the variance is trying to, uh, measure how spread apart is this,",
    "start": "1146750",
    "end": "1152120"
  },
  {
    "text": "uh, distribution in general. All right. Uh, there is also, uh, another concept called the median of the distribution.",
    "start": "1152120",
    "end": "1159875"
  },
  {
    "text": "So the median of the distribution is- supposing this is the median. The median of the distribution is, uh,",
    "start": "1159875",
    "end": "1168590"
  },
  {
    "text": "the point that divides the mass into- [NOISE] into- into,",
    "start": "1168590",
    "end": "1176570"
  },
  {
    "text": "uh, uh, equal masses on either side. All right. And for symmetric distribution,",
    "start": "1176570",
    "end": "1182090"
  },
  {
    "text": "the median and the expectation are- are- are the same, right? However, uh, there could be distributions which are- which are- you know,",
    "start": "1182090",
    "end": "1190700"
  },
  {
    "text": "for example, you have a distribution like this, right? In this case, the median could be somewhere over here,",
    "start": "1190700",
    "end": "1197735"
  },
  {
    "text": "but the expectation could be, uh, uh,- so this could be the median and this could be the expectation,",
    "start": "1197735",
    "end": "1203900"
  },
  {
    "text": "because the- the- the point where you balance a mass is not necessarily the point where, you know,",
    "start": "1203900",
    "end": "1209825"
  },
  {
    "text": "you have equal masses on both sides, because, you know, um, I think- I think that's called the, uh, moment of inertia or something like that.",
    "start": "1209825",
    "end": "1215795"
  },
  {
    "text": "But if you have a mass that's- that kinda spread, uh, all the way on the other side, then that has more effect. So if you want to balance things,",
    "start": "1215795",
    "end": "1221015"
  },
  {
    "text": "that's the point where- that's- that's the, uh, mean or the expectation and median is the thing that divides it into,",
    "start": "1221015",
    "end": "1226715"
  },
  {
    "text": "uh, equal masses on both sides. All right. And the variance is trying to measure how spread apart is this.",
    "start": "1226715",
    "end": "1231725"
  },
  {
    "text": "If you could have a distribution that has, uh, a given expectation and a small variance,",
    "start": "1231725",
    "end": "1239150"
  },
  {
    "text": "or you could have a distribution that has a given expectation and a large variance. Right? [NOISE] These are two different expressions for,",
    "start": "1239150",
    "end": "1251195"
  },
  {
    "text": "uh, uh- for- for, uh, the variance of x and they're- they're, uh, both equivalent.",
    "start": "1251195",
    "end": "1257450"
  },
  {
    "text": "That's, uh- showing this is- is pretty simple. It's in the notes. Um, and here are some examples of various, uh,",
    "start": "1257450",
    "end": "1263740"
  },
  {
    "start": "1261000",
    "end": "1381000"
  },
  {
    "text": "distributions and their parameters. All right? Parameters. Now, this is the first time we're talking about parameters.",
    "start": "1263740",
    "end": "1270255"
  },
  {
    "text": "What's a parameter? Right? When we're talking about distributions there are kind of two concepts.",
    "start": "1270255",
    "end": "1276080"
  },
  {
    "text": "One is the- the, uh, space over which the distribution is being defined. Supposing it's a random variable,",
    "start": "1276080",
    "end": "1282470"
  },
  {
    "text": "it is, you know, um, the real line on- on which the- the distribution is being defined.",
    "start": "1282470",
    "end": "1288320"
  },
  {
    "text": "And then you have a parameter. A parameter is basically a number that tries to,",
    "start": "1288320",
    "end": "1295530"
  },
  {
    "text": "uh, summarize the shape of the distribution. All right. For example, um,",
    "start": "1295530",
    "end": "1302810"
  },
  {
    "text": "if you have a- a Gaussian random variable, which is [NOISE] also called the normal distribution or the bell curve.",
    "start": "1302810",
    "end": "1314240"
  },
  {
    "text": "[NOISE]",
    "start": "1314240",
    "end": "1320100"
  },
  {
    "text": "It has two- two parameters.",
    "start": "1320100",
    "end": "1326475"
  },
  {
    "text": "So one is the, uh, so this is the real line.",
    "start": "1326475",
    "end": "1331630"
  },
  {
    "text": "And this is the, uh, Gaussian, uh, uh, C- PDF and Mu is the- the mean",
    "start": "1332450",
    "end": "1339150"
  },
  {
    "text": "of the Gaussian distribution and Sigma is like- it's- it's called the, uh, uh, standard deviation, right?",
    "start": "1339150",
    "end": "1349125"
  },
  {
    "text": "Uh, so a Gaussian distribution is- is, uh, summarized by a Mu and, um, um,",
    "start": "1349125",
    "end": "1354810"
  },
  {
    "text": "and the variance Sigma squared, where Sigma is the standard deviation. Similarly, there are, you know,",
    "start": "1354810",
    "end": "1360945"
  },
  {
    "text": "many different, uh, uh, uh, distributions and each of them have- have, um, you know, parameters.",
    "start": "1360945",
    "end": "1366735"
  },
  {
    "text": "Some distributions have just one parameter, some have two, some have three. Uh, but most of the distributions we're gonna be, uh, uh,",
    "start": "1366735",
    "end": "1373409"
  },
  {
    "text": "interested in- in this course are- are- are, you know, gonna have one or two parameters.",
    "start": "1373410",
    "end": "1379270"
  },
  {
    "start": "1381000",
    "end": "1597000"
  },
  {
    "text": "All right? You can have, um, you can look at two random variables, um, jointly, right?",
    "start": "1381050",
    "end": "1390060"
  },
  {
    "text": "And for that, um, what that means is supposing you have a variable X and variable Y,",
    "start": "1390060",
    "end": "1396825"
  },
  {
    "text": "together, they have a joint CDF, which means the probability that X- the variable X,",
    "start": "1396825",
    "end": "1404250"
  },
  {
    "text": "is less than the X threshold and the probability that Y is less than the Y threshold, right?",
    "start": "1404250",
    "end": "1409710"
  },
  {
    "text": "So you think of the comma over here as- as- as the logical and, right?",
    "start": "1409710",
    "end": "1415995"
  },
  {
    "text": "Um, similarly, you have a- a bivariate, uh, probability mass function, which is, you know,",
    "start": "1415995",
    "end": "1421590"
  },
  {
    "text": "the probability that X, uh, in this case, X and Y are discrete. In- in the, uh, continuous,",
    "start": "1421590",
    "end": "1426900"
  },
  {
    "text": "they were, uh, here, they were continuous. So this is the probability that,",
    "start": "1426900",
    "end": "1432330"
  },
  {
    "text": "uh, X takes the value, you know, small x and the random variable Y takes,",
    "start": "1432330",
    "end": "1437775"
  },
  {
    "text": "uh, the value small y. Given to, uh, given the, uh, joint P,",
    "start": "1437775",
    "end": "1444870"
  },
  {
    "text": "uh, PDF of two random variables, you can construct their marginals, right? So if you have two random variables,",
    "start": "1444870",
    "end": "1452170"
  },
  {
    "text": "let's call it, um, X and Y,",
    "start": "1453320",
    "end": "1458475"
  },
  {
    "text": "the- the, uh, the distribution that- that- that,",
    "start": "1458475",
    "end": "1465419"
  },
  {
    "text": "uh, captures the most information of these two is the joint distribution of the two. And that will have, uh,",
    "start": "1465420",
    "end": "1471600"
  },
  {
    "text": "assuming they are continuous, you know, you write it as P of x, y, right?",
    "start": "1471600",
    "end": "1478185"
  },
  {
    "text": "That's the, um, um, the, um, joint, uh, distribution of the two.",
    "start": "1478185",
    "end": "1483255"
  },
  {
    "text": "Similarly, you can have the marginal distributions of, uh, the two random variables.",
    "start": "1483255",
    "end": "1489014"
  },
  {
    "text": "And the marginal distributions is gonna be, um, written as P of x or P of y.",
    "start": "1489015",
    "end": "1496215"
  },
  {
    "text": "And the way you get this is to- what's- what's called as marginalizing out,",
    "start": "1496215",
    "end": "1501510"
  },
  {
    "text": "or summing out, or integrating out the other variables. So sum of all the Y,",
    "start": "1501510",
    "end": "1506865"
  },
  {
    "text": "P of x, y, and you get this. Or similarly, if it's a continuous random variable,",
    "start": "1506865",
    "end": "1513090"
  },
  {
    "text": "then you integrate out P of x, y dy, right?",
    "start": "1513090",
    "end": "1519674"
  },
  {
    "text": "And- and this is just the other way. So here you sum out x, if it is, um, discrete.",
    "start": "1519675",
    "end": "1526380"
  },
  {
    "text": "So this is discrete and this is continuous, right?",
    "start": "1526380",
    "end": "1532050"
  },
  {
    "text": "And here it would be summing all, right?",
    "start": "1532050",
    "end": "1538170"
  },
  {
    "text": "Now the, uh, the way to think about the- the, uh, joint, uh,",
    "start": "1538170",
    "end": "1543840"
  },
  {
    "text": "PDF is that the- the joint PDF or the joint PMF has to satisfy the property, right?",
    "start": "1543840",
    "end": "1552669"
  },
  {
    "text": "Right? So and if you integrate out both the variables, the- the joint PDF has to integrate to one, right?",
    "start": "1558080",
    "end": "1566415"
  },
  {
    "text": "And, um, if you integrate out only one variable, then you get a distribution with respect to the other variable, right?",
    "start": "1566415",
    "end": "1575370"
  },
  {
    "text": "And that's called the marginal distribution, right?",
    "start": "1575370",
    "end": "1584415"
  },
  {
    "text": "So, uh, given- given these concepts of, uh, the joint and the marginal,",
    "start": "1584415",
    "end": "1590595"
  },
  {
    "text": "we can define the base theorem. The base theorem is probably one of the- the most important,",
    "start": "1590595",
    "end": "1596025"
  },
  {
    "text": "uh, theorems in- in probability theory. It's gonna show up all over the place, especially in this course and even beyond.",
    "start": "1596025",
    "end": "1602160"
  },
  {
    "start": "1597000",
    "end": "1854000"
  },
  {
    "text": "And the base theorem basically, uh, gives you, uh, uh,",
    "start": "1602160",
    "end": "1608189"
  },
  {
    "text": "the relation between the conditional distribution, the joint distribution, and the marginal distribution, right?",
    "start": "1608189",
    "end": "1614804"
  },
  {
    "text": "So the, um, base- the base theorem looks like this.",
    "start": "1614805",
    "end": "1623445"
  },
  {
    "text": "So P of x, y. You can write this as P of x times P of y given x, right?",
    "start": "1623445",
    "end": "1636015"
  },
  {
    "text": "So this is the joint, this is the marginal,",
    "start": "1636015",
    "end": "1642430"
  },
  {
    "text": "and this is the conditional probability, right?",
    "start": "1643480",
    "end": "1649799"
  },
  {
    "text": "So the- the way you wanna remember this is the joint is the product of the marginal and the conditional, right?",
    "start": "1649800",
    "end": "1655559"
  },
  {
    "text": "But you can also write this as P of y times P of x given y, right?",
    "start": "1655560",
    "end": "1663690"
  },
  {
    "text": "And the way you decompose the joint into- into these two parts is called the chain rule, right?",
    "start": "1663690",
    "end": "1670290"
  },
  {
    "text": "So there's a chain rule in calculus, there's the chain rule of probability theory, right? So the chain rule, it tells you that you can- you can",
    "start": "1670290",
    "end": "1676335"
  },
  {
    "text": "always- for any pair of random variables, you can decompose the joint into the marginal times the conditional, right?",
    "start": "1676335",
    "end": "1684705"
  },
  {
    "text": "And a- a- a trivial consequence of this, uh, chain rule is the base theorem.",
    "start": "1684705",
    "end": "1690300"
  },
  {
    "text": "So the base theorem, uh, basically equate these two. So if you write P of x times P of y given x equals P of y times P of x given y, right?",
    "start": "1690300",
    "end": "1704260"
  },
  {
    "text": "And assuming P of x is never 0,",
    "start": "1704300",
    "end": "1709605"
  },
  {
    "text": "you can divide both sides by P of x, and that gives you P of y given x equals P of y times P of x given y over P of x, right?",
    "start": "1709605",
    "end": "1722040"
  },
  {
    "text": "So this is base theorem, right? It's- it's- it's a very simple consequence of- of the,",
    "start": "1722040",
    "end": "1729179"
  },
  {
    "text": "uh, uh, of chain rule. You apply the chain rule in two different ways, equate the two different ways and divide,",
    "start": "1729180",
    "end": "1736335"
  },
  {
    "text": "uh, divide over by one of the modules, and you get the base rule. Yeah, so here we're, um, um,",
    "start": "1736335",
    "end": "1742410"
  },
  {
    "text": "writing this in terms of x, F, but, you know, um, um, um, you can- you- you can use F or P, doesn't matter, right?",
    "start": "1742410",
    "end": "1750735"
  },
  {
    "text": "And another way, um, in which Bayes- the, uh, Bayes theorem is, um,",
    "start": "1750735",
    "end": "1756135"
  },
  {
    "text": "is also commonly written is retain the numerator as it is,",
    "start": "1756135",
    "end": "1762255"
  },
  {
    "text": "P of y times P of x given y. And, uh, the- the denominator is- instead of writing it as P of x,",
    "start": "1762255",
    "end": "1772515"
  },
  {
    "text": "you're gonna write it as- assuming it's- it's a discrete, sum over y prime,",
    "start": "1772515",
    "end": "1778425"
  },
  {
    "text": "P of y prime times P of x given y prime.",
    "start": "1778425",
    "end": "1785745"
  },
  {
    "text": "And these two are the same. Why are they the same? Because P of y prime times P of x given y prime is the joint.",
    "start": "1785745",
    "end": "1793620"
  },
  {
    "text": "All right? It's- it's the joint. And you're marginaling- marginalizing out y prime from this,",
    "start": "1793620",
    "end": "1800430"
  },
  {
    "text": "which gives you P of x, right? May- you know, just- just to add,",
    "start": "1800430",
    "end": "1805815"
  },
  {
    "text": "this is basically equal to summing over y prime of P of x, y prime.",
    "start": "1805815",
    "end": "1813340"
  },
  {
    "text": "You're gonna use this form of the Bayes theorem also",
    "start": "1815180",
    "end": "1820260"
  },
  {
    "text": "in this course, right?",
    "start": "1820260",
    "end": "1828720"
  },
  {
    "text": "Any questions. So some of you sitting over here,",
    "start": "1828720",
    "end": "1834990"
  },
  {
    "text": "you may not be able to see this, uh, uh, read this because this, kind of, comes in the way. So maybe you can move over next time or you can move over",
    "start": "1834990",
    "end": "1841710"
  },
  {
    "text": "even now if you want to, um, right?",
    "start": "1841710",
    "end": "1847065"
  },
  {
    "text": "Example of Bayes' rule, let's skip over this. Independence- so the independence of two random variables is very similar to,",
    "start": "1847065",
    "end": "1854580"
  },
  {
    "start": "1854000",
    "end": "2046000"
  },
  {
    "text": "uh, the independence of- of two events. They are distinct concepts. Independence, uh, we- we- we spoke about independence of random events a few slides ago.",
    "start": "1854580",
    "end": "1865785"
  },
  {
    "text": "Here we're talking about independence of two random variables, right? And independence of two random variables is defined like this, that the, uh,",
    "start": "1865785",
    "end": "1873645"
  },
  {
    "text": "the joint probability is equal to the product of the marginal probabilities, right?",
    "start": "1873645",
    "end": "1879690"
  },
  {
    "text": "Or the joint density is equal to the product of the marginal densities. Uh, that also, you know, um,",
    "start": "1879690",
    "end": "1886230"
  },
  {
    "text": "that's basically telling, you know, if it is independent, right?",
    "start": "1886230",
    "end": "1891375"
  },
  {
    "text": "You're saying P of- this is equal to P of x times P of y. So if- if two random variables are independent, then,",
    "start": "1891375",
    "end": "1898995"
  },
  {
    "text": "you know, you- P of y must be equal to P of y given x, right? We saw something very similar for random,",
    "start": "1898995",
    "end": "1905330"
  },
  {
    "text": "um, um, for independence of random events. It's, uh, you know, it's very, very, uh, uh, very similar.",
    "start": "1905330",
    "end": "1911700"
  },
  {
    "text": "So P of y given, uh, x of two variables will be equal to, uh, P of y? Yes, question.",
    "start": "1911700",
    "end": "1917180"
  },
  {
    "text": "[NOISE] [inaudible]",
    "start": "1917180",
    "end": "1925905"
  },
  {
    "text": "The- so the question is, if two random variables are independent, can we say that the, uh, events that they are, um, um.",
    "start": "1925905",
    "end": "1933570"
  },
  {
    "text": "Defined over. Defined over are also independent? I- I.",
    "start": "1933570",
    "end": "1940809"
  },
  {
    "text": "Yeah, it is [inaudible]. The- it- the- it- it- it- it must be the case, right? Yeah. Yeah. Yes, question? Yes, question?",
    "start": "1941090",
    "end": "1950850"
  },
  {
    "text": "Can you go back a slide? Go back a slide. Yeah. This is defined [inaudible]",
    "start": "1950850",
    "end": "1960149"
  },
  {
    "text": "This is defined when f of x is not equal to 0. [OVERLAPPING]",
    "start": "1960150",
    "end": "1973349"
  },
  {
    "text": "[inaudible]\nSo the, um, when- when, uh, the probability, uh, so assuming we are talking about discrete, um,",
    "start": "1973349",
    "end": "1980010"
  },
  {
    "text": "um, in the discrete case, the uh, conditional of y given x itself is not defined if the probability of x is 0.",
    "start": "1980010",
    "end": "1988480"
  },
  {
    "text": "Right? Assuming that the condition is defined, you know, the def- um, um, the Bayes' theorem tells you that,",
    "start": "1989300",
    "end": "1996285"
  },
  {
    "text": "you know, you can represent it like this. [NOISE] Does that make sense? So- so the conditional probability itself is defined only when, uh, you know,",
    "start": "1996285",
    "end": "2004315"
  },
  {
    "text": "uh, conditional of y of x, uh, is defined only when x has a non-zero probability. Can you explain the interpretation for the function f of y given that,",
    "start": "2004315",
    "end": "2015500"
  },
  {
    "text": "what does it actually mean [inaudible]? So, um, over here, this is the um,",
    "start": "2015500",
    "end": "2021035"
  },
  {
    "text": "conditional distribution of y given x takes on some,",
    "start": "2021035",
    "end": "2026840"
  },
  {
    "text": "uh, you know, a- a given value.",
    "start": "2026840",
    "end": "2029850"
  },
  {
    "text": "Right? [NOISE] Right?",
    "start": "2034690",
    "end": "2042679"
  },
  {
    "text": "Next we're gonna talk about this, uh, you know, uh, so yeah, independence.",
    "start": "2042680",
    "end": "2048169"
  },
  {
    "start": "2046000",
    "end": "2201000"
  },
  {
    "text": "So we're gonna- I don't- independent random variables are- are, um, are super important because it lets you decompose",
    "start": "2048169",
    "end": "2055970"
  },
  {
    "text": "the joint probability into just product of the marginals. And this independence makes,",
    "start": "2055970",
    "end": "2062628"
  },
  {
    "text": "um, this independence, uh, um, is what makes a lot of machine learning theory kind of,",
    "start": "2062629",
    "end": "2068990"
  },
  {
    "text": "you know, easy to- to, um, uh, work with. We- we- we make, we're going to be making, um, assumptions about,",
    "start": "2068990",
    "end": "2077319"
  },
  {
    "text": "you know, your training examples, um, which you think of them as random variables being independent of each other.",
    "start": "2077320",
    "end": "2083020"
  },
  {
    "text": "So the concept of independence is- is, uh, is- is- is very important. [NOISE] Let's skip all this.",
    "start": "2083020",
    "end": "2091385"
  },
  {
    "text": "Right. This is the expectation of two random variables. Uh, [NOISE] you can go over these on your slides.",
    "start": "2091385",
    "end": "2097460"
  },
  {
    "text": "Um, the- just like the concept of, um,",
    "start": "2097460",
    "end": "2103190"
  },
  {
    "text": "um, of variance, there is the concept of covariance when we're talking about two random variables.",
    "start": "2103190",
    "end": "2109460"
  },
  {
    "text": "Right? Uh, and the covariance, um, of- of, ah, random variables are defined like this.",
    "start": "2109460",
    "end": "2115070"
  },
  {
    "text": "So, um, so the variance of- [NOISE]",
    "start": "2115070",
    "end": "2127970"
  },
  {
    "text": "so the variance of x was defined as expectation",
    "start": "2127970",
    "end": "2134315"
  },
  {
    "text": "of x square minus expectation of x square.",
    "start": "2134315",
    "end": "2142775"
  },
  {
    "text": "We saw this on a few slides ago. Similarly, uh, the covariance of x comma y is defined as expectation of x,",
    "start": "2142775",
    "end": "2154595"
  },
  {
    "text": "uh, times y minus expectation of x into expectation of y,",
    "start": "2154595",
    "end": "2163805"
  },
  {
    "text": "which basically tells you that, um, the covariance of x with x is just the variance of x.",
    "start": "2163805",
    "end": "2170870"
  },
  {
    "text": "[NOISE] Okay? [NOISE] All right.",
    "start": "2170870",
    "end": "2180860"
  },
  {
    "text": "The multivariate Gaussian. So this is, uh, one of the most commonly used distributions defined",
    "start": "2180860",
    "end": "2187340"
  },
  {
    "text": "over a collection of random variables, right? And this is something that's gonna to show up a whole lot in this course.",
    "start": "2187340",
    "end": "2194705"
  },
  {
    "text": "And hopefully towards the end of the course, uh, if you're woken up in the middle of your sleep and asked you",
    "start": "2194705",
    "end": "2201650"
  },
  {
    "start": "2201000",
    "end": "2568000"
  },
  {
    "text": "what's the probability density of a multivariate Gaussian? You'll be able to say this, you know, instinctively, right?",
    "start": "2201650",
    "end": "2208280"
  },
  {
    "text": "Um, this looks- looks a little, um, uh, scary at first, but let's- let's,",
    "start": "2208280",
    "end": "2214385"
  },
  {
    "text": "um, let's kind of diss- uh, dissect it a little bit. So- so x is",
    "start": "2214385",
    "end": "2223730"
  },
  {
    "text": "in some n-dimensional space, and the probability, uh,",
    "start": "2223730",
    "end": "2231214"
  },
  {
    "text": "the density of x given two parameters, Mu and Sigma is defined as 1 over 2 pi to",
    "start": "2231215",
    "end": "2245090"
  },
  {
    "text": "the n by 2 times the determinant of Sigma to the power half times the exponent",
    "start": "2245090",
    "end": "2255440"
  },
  {
    "text": "of minus half x minus",
    "start": "2255440",
    "end": "2260900"
  },
  {
    "text": "Mu transpose Sigma inverse x minus Mu.",
    "start": "2260900",
    "end": "2268400"
  },
  {
    "text": "Wow. So, um, first of all,",
    "start": "2268400",
    "end": "2273664"
  },
  {
    "text": "let's see what's happening here, uh, and add some color to it. So x is the space over which we are defining,",
    "start": "2273665",
    "end": "2283970"
  },
  {
    "text": "ah, the- the- the, uh, probably density. So we see that x appears only here inside the exponent, right?",
    "start": "2283970",
    "end": "2294215"
  },
  {
    "text": "And you have two parameters, Mu and- and- and Sigma. The Sigma is called the covariance matrix and Mu is the- is the uh,",
    "start": "2294215",
    "end": "2302920"
  },
  {
    "text": "uh, is the mean. And the covariance matrix shows up over here, so let's [NOISE]",
    "start": "2302920",
    "end": "2324290"
  },
  {
    "text": "right? And then we have the mean. And the mean- [NOISE]",
    "start": "2324290",
    "end": "2335359"
  },
  {
    "text": "maybe I'll use black for this. [NOISE] Mean, mean, mean. All right.",
    "start": "2335360",
    "end": "2355595"
  },
  {
    "text": "So, uh, this is the- is the, um, um, the joint, er,",
    "start": "2355595",
    "end": "2362570"
  },
  {
    "text": "the- the joint distribution of a multivariate, uh, Gaussian, and we right away recognize a few things that",
    "start": "2362570",
    "end": "2369230"
  },
  {
    "text": "we've seen in our linear algebra course before, right? So first of all, uh, Sigma is a matrix, right?",
    "start": "2369230",
    "end": "2375800"
  },
  {
    "text": "It's called a covariance matrix. It turns out to be a positive semi-definite matrix.",
    "start": "2375800",
    "end": "2381740"
  },
  {
    "text": "Uh, any covariance matrix for any- any, uh, uh, joint distribution, no matter what the distribution is,",
    "start": "2381740",
    "end": "2388055"
  },
  {
    "text": "it's always going to be positive semi-definite, right? So this and this is a positive semi-definite matrix,",
    "start": "2388055",
    "end": "2394115"
  },
  {
    "text": "and- we- it is going to be full rank, which means the inverse will exist.",
    "start": "2394115",
    "end": "2399140"
  },
  {
    "text": "Right? And recognize a few more things. So this is the- is the determinant of the matrix.",
    "start": "2399140",
    "end": "2406970"
  },
  {
    "text": "Right? That's the, uh, notation for, uh, the determinant, and it's- it's like the square root of the determinant.",
    "start": "2406970",
    "end": "2413360"
  },
  {
    "text": "Um, and we also see this form of it over here. Does anybody recognize this form?",
    "start": "2413360",
    "end": "2419870"
  },
  {
    "text": "[BACKGROUND] This is the quadratic form. So x minus Mu is sum vector, right?",
    "start": "2419870",
    "end": "2425600"
  },
  {
    "text": "And what we're doing is x minus Mu transpose sum matrix times x minus Mu, right?",
    "start": "2425600",
    "end": "2430640"
  },
  {
    "text": "So this is the quadratic form that we also spent some time on, right? So all- all the- all the, uh, uh,",
    "start": "2430640",
    "end": "2437750"
  },
  {
    "text": "review that we did with linear algebra is gonna, um, um, apply on this, uh, shortly, right?",
    "start": "2437750",
    "end": "2444050"
  },
  {
    "text": "And this term over here, uh, is just some constant. It has no mean, no- no data,",
    "start": "2444050",
    "end": "2450950"
  },
  {
    "text": "no, uh, uh, covariance, th- this is just some constant. And the purpose of this 1 over,",
    "start": "2450950",
    "end": "2456110"
  },
  {
    "text": "uh, 2 Pi^n by 2, uh, the purpose of its existence is to only make this- the integral of this with respect to x be equal to 1.",
    "start": "2456110",
    "end": "2465050"
  },
  {
    "text": "So this- this is just some normalizing constant, right? Yes, question. [BACKGROUND] Yeah.",
    "start": "2465050",
    "end": "2473270"
  },
  {
    "text": "[BACKGROUND] Exactly. So the question is,",
    "start": "2473270",
    "end": "2478790"
  },
  {
    "text": "uh, if this is a multivariate distribution, what does the integral with respect to x mean? It- it means, uh, exactly what you say.",
    "start": "2478790",
    "end": "2484880"
  },
  {
    "text": "You're integrating out all x1 through xn. All right.",
    "start": "2484880",
    "end": "2489750"
  },
  {
    "text": "You hear me? Okay. Yeah. Okay, sorry.",
    "start": "2490690",
    "end": "2496520"
  },
  {
    "text": "Um, so, um, the- the integral would- would look something like this.",
    "start": "2496520",
    "end": "2501950"
  },
  {
    "text": "You would have, um, p of x Mu Sigma squared times dx_1, dx_2.",
    "start": "2501950",
    "end": "2513330"
  },
  {
    "text": "Right? Its- it- it would be, uh, a double integral or a triple integral, or, uh-.",
    "start": "2513330",
    "end": "2520160"
  },
  {
    "text": "Right? Right. So this is the- the, uh, the multivariate Gaussian density,",
    "start": "2520160",
    "end": "2527224"
  },
  {
    "text": "and we're gonna be working a whole lot with this, so, you know, you- you'll get familiar with this, uh,",
    "start": "2527225",
    "end": "2532474"
  },
  {
    "text": "if you- if you, uh, if you're not already. Okay? And here are some intuitions about what the mean",
    "start": "2532475",
    "end": "2539450"
  },
  {
    "text": "and covariance matrix of a multivariate Gaussian, uh, looks like. So what we have in the, uh,",
    "start": "2539450",
    "end": "2546470"
  },
  {
    "text": "in the left are examples of particular instances of Mu and Sigma.",
    "start": "2546470",
    "end": "2552530"
  },
  {
    "text": "The figure on the center is a plot of the probability density function.",
    "start": "2552530",
    "end": "2559835"
  },
  {
    "text": "Um, here it- it's a two-dimensional Gaussian, and the- the figure on the rightmost column",
    "start": "2559835",
    "end": "2565715"
  },
  {
    "text": "is what's also called a contour plot or a heat map, where a lighter color,",
    "start": "2565715",
    "end": "2572255"
  },
  {
    "start": "2568000",
    "end": "3045000"
  },
  {
    "text": "a color of orange, means, um, that that region has a higher value and blue means it has a low value.",
    "start": "2572255",
    "end": "2579275"
  },
  {
    "text": "So imagine, um, looking at the- the- the plot in",
    "start": "2579275",
    "end": "2584569"
  },
  {
    "text": "the middle directly from the top and using different colors to represent different heights, right?",
    "start": "2584570",
    "end": "2590330"
  },
  {
    "text": "That's- that's the, uh, uh, uh, plot on the right. Right? So let's just get a feel for, you know,",
    "start": "2590330",
    "end": "2596750"
  },
  {
    "text": "what the different, uh, um, um, different parameters mean in terms of the probability density.",
    "start": "2596750",
    "end": "2603230"
  },
  {
    "text": "So a Mu equal to 0 means the center of the distribution,",
    "start": "2603230",
    "end": "2608645"
  },
  {
    "text": "or- or- or the- the distribution is going to be centered around 0, 0, right?",
    "start": "2608645",
    "end": "2613820"
  },
  {
    "text": "We see this to be the, uh, you know, 0, 0 over here and the- the peak of the distribution is over 0, 0 in this case.",
    "start": "2613820",
    "end": "2622565"
  },
  {
    "text": "And the- the covariance matrix, the diagonals of the covariance matrix are",
    "start": "2622565",
    "end": "2629150"
  },
  {
    "text": "the variances of the marginals of x1 and x2, right? So it- it- what- what this actually means is that x,",
    "start": "2629150",
    "end": "2637490"
  },
  {
    "text": "the variable, uh, x1, so if you- if you just look at x1 over here, it's gonna have like a standard deviation of 1.",
    "start": "2637490",
    "end": "2645005"
  },
  {
    "text": "You know, it- it- that looks roughly correct. Um, uh, if you- if you imagine this to be, uh, the x direction,",
    "start": "2645005",
    "end": "2651320"
  },
  {
    "text": "it has like a- a- a- a standard deviation of 1 and x2 also has a standard deviation of 1.",
    "start": "2651320",
    "end": "2658349"
  },
  {
    "text": "And the cross entries in the covariance matrix are the actual covariances between those two random variables.",
    "start": "2658350",
    "end": "2665935"
  },
  {
    "text": "A covariance of 0, uh, here roughly means that there is no, um, strong relation between x1 and x2,",
    "start": "2665935",
    "end": "2673970"
  },
  {
    "text": "which means it take, um, a spherical shape. Um, moving on, so here's another example where the, um,",
    "start": "2673970",
    "end": "2685450"
  },
  {
    "text": "the covariance matrix has smaller diagonal elements, which means the distribution is still centered around 0,",
    "start": "2685450",
    "end": "2693785"
  },
  {
    "text": "but it is more concentrated. It has a smaller variance, right? Which means it is more peaked.",
    "start": "2693785",
    "end": "2700445"
  },
  {
    "text": "Okay? It is more peaked compared to, uh, this distribution.",
    "start": "2700445",
    "end": "2705470"
  },
  {
    "text": "And in terms of the, uh, contour plot, um, the- the- the circles just appear to be,",
    "start": "2705470",
    "end": "2711830"
  },
  {
    "text": "uh, smaller. [NOISE] Right? And similarly, if you increase the variance of, um,",
    "start": "2711830",
    "end": "2719630"
  },
  {
    "text": "of your distribution, the covariances are still zero, which means that it is still spherically shaped, right?",
    "start": "2719630",
    "end": "2725690"
  },
  {
    "text": "Um, now, it- it- it looks more flattened, right? Your probability density looks more flattened,",
    "start": "2725690",
    "end": "2731225"
  },
  {
    "text": "and the concentric circles are also more, uh, expanded. Okay? Let's look at a few more examples.",
    "start": "2731225",
    "end": "2740345"
  },
  {
    "text": "Now, um, over here, you know, this is still the- the,",
    "start": "2740345",
    "end": "2745609"
  },
  {
    "text": "uh, the, uh, first picture. Now- now let us scale the x1 to have a variance 0.6 and x2 to have variance 1.",
    "start": "2745610",
    "end": "2755750"
  },
  {
    "text": "[NOISE] So x1 is now more, um, compressed, but x2 still maintains the same variance.",
    "start": "2755750",
    "end": "2764075"
  },
  {
    "text": "So you see from the contour plot over here, um, that it- it- it is more,",
    "start": "2764075",
    "end": "2769865"
  },
  {
    "text": "uh, vertically, uh, aligned. Probably the slides- the shape- the prediction of the slides make it look a little different,",
    "start": "2769865",
    "end": "2778400"
  },
  {
    "text": "but that's- that's the, um, you know, it seems on the- on- on- on the picture over there, this looks like a circle,",
    "start": "2778400",
    "end": "2784250"
  },
  {
    "text": "but it's supposed to be, you know, compressed vertically and- and it's- it's, uh, it's supposed to be longer.",
    "start": "2784250",
    "end": "2789920"
  },
  {
    "text": "Now, instead of shrinking x1, if we expand x1, right? Now, you see that it's- it's- it's stretched across more widely.",
    "start": "2789920",
    "end": "2797750"
  },
  {
    "text": "Right? But still there is no correlation between, um, between x1 and x2, uh,",
    "start": "2797750",
    "end": "2804095"
  },
  {
    "text": "so you don't see any particular, you know, um, um, correlation over there.",
    "start": "2804095",
    "end": "2809340"
  },
  {
    "text": "Now let's add some correlation, right? Now, um, x1, and, um,",
    "start": "2809560",
    "end": "2814625"
  },
  {
    "text": "in- in this case, x1 and x2 still have a variance of 1 and, uh,",
    "start": "2814625",
    "end": "2820625"
  },
  {
    "text": "1, but now we've added a correlation of 0.5, which means, um, as you increase x1,",
    "start": "2820625",
    "end": "2827555"
  },
  {
    "text": "x2 also tends to increase. That's- that's the, uh, general idea that x1 and x2 tend to co-occur,",
    "start": "2827555",
    "end": "2834440"
  },
  {
    "text": "uh, you know, uh, uh, more of, uh, you know, as- as you increase the values of x. If you increase the, uh,",
    "start": "2834440",
    "end": "2840905"
  },
  {
    "text": "covariance further, you know, the- the relation becomes even tighter, even stronger, right?",
    "start": "2840905",
    "end": "2846680"
  },
  {
    "text": "Uh, the marginals in this case are still 1 and 1, which means from, uh,",
    "start": "2846680",
    "end": "2852275"
  },
  {
    "text": "if- if you- if you were to integrate out x1,",
    "start": "2852275",
    "end": "2857315"
  },
  {
    "text": "you would get, uh, a probability density of- of a unit, uh, of- of, uh, mean 0, uh,",
    "start": "2857315",
    "end": "2863134"
  },
  {
    "text": "variance 1 on- on x2, and similarly, if you were- were to integrate out x2, you would still get, you know,",
    "start": "2863135",
    "end": "2868775"
  },
  {
    "text": "a normal curve on- on- on the x1 axis. Right? So that- this, um, you know,",
    "start": "2868775",
    "end": "2875660"
  },
  {
    "text": "intuition for how a multivariate Gaussian, uh, would- would- would look.",
    "start": "2875660",
    "end": "2881119"
  },
  {
    "text": "You know, with- with two dimensions it's easy to- to, uh, um, visualize and get some, uh, intuitions.",
    "start": "2881120",
    "end": "2887970"
  },
  {
    "text": "Right? So, um, if the, er, this is the next slide, and, uh,",
    "start": "2889180",
    "end": "2894560"
  },
  {
    "text": "if the correlation is negative, now, what- what- what this means is, uh, larger values of x- x1 tend to co-occur with smaller values of x2, right?",
    "start": "2894560",
    "end": "2906079"
  },
  {
    "text": "So, um, for example, over here, for x- x1 equal to, you know, uh,",
    "start": "2906080",
    "end": "2913174"
  },
  {
    "text": "for x1 to have a particular color, you know, um, x2 has to be, you know, negative.",
    "start": "2913175",
    "end": "2919420"
  },
  {
    "text": "Uh, that- that's what it means. So- so you see- you see a reverse relation between, um, uh, x1 going up and x2, uh, going down.",
    "start": "2919420",
    "end": "2927390"
  },
  {
    "text": "And if you, again, uh, make it negative and make it a bigger number, you see a stronger correlation between x1 and x2,",
    "start": "2927390",
    "end": "2934415"
  },
  {
    "text": "but in the- in the- in the, uh, reverse direction. Similarly, uh, for a given covariance matrix, you can, uh,",
    "start": "2934415",
    "end": "2942425"
  },
  {
    "text": "move your, you know, use a different Mu, which means, uh, the covariance matrix is deciding the shape and orientation of the- of the,",
    "start": "2942425",
    "end": "2952385"
  },
  {
    "text": "um, of this function, and Mu, uh, tells you where to place it,",
    "start": "2952385",
    "end": "2957395"
  },
  {
    "text": "where to center it, right? So, uh, this is basically the same, uh, uh, bell-shaped but placed at x1 equal to 0 and x2 equals 0.5.",
    "start": "2957395",
    "end": "2967820"
  },
  {
    "text": "So here you see it's just- in terms of x2, it's moved up a little bit.",
    "start": "2967820",
    "end": "2974030"
  },
  {
    "text": "Right? And- and, uh, this- this is, uh, another example where x1 is 1.5. The- the- the center of the distribution is that x1 equals 1.5 and x2 equals minus 0.5.",
    "start": "2974030",
    "end": "2982445"
  },
  {
    "text": "That's where the center is, and the, uh, covariance matrix decides the shape and orientation.",
    "start": "2982445",
    "end": "2988200"
  },
  {
    "text": "Right? Any questions? Yes, question. [BACKGROUND]",
    "start": "2988770",
    "end": "3008420"
  },
  {
    "text": "So if you- if you, uh, the question is, uh, this is, uh, with respect to two variables and what if",
    "start": "3008420",
    "end": "3014070"
  },
  {
    "text": "we have more variables? Is tha- is that the question? [BACKGROUND] Yeah, so if you have,",
    "start": "3014070",
    "end": "3021310"
  },
  {
    "text": "uh, a three-dimensional Gaussian, then you would have a three-by-three covariance matrix [NOISE], and each of the, uh, uh,",
    "start": "3021310",
    "end": "3027744"
  },
  {
    "text": "pairwise correlations will be the values in the corresponding cell.",
    "start": "3027745",
    "end": "3032060"
  },
  {
    "text": "All right. Okay, so conditional probability,",
    "start": "3034770",
    "end": "3040780"
  },
  {
    "text": "uh, um, we went over this already. Um, so, uh, here's a useful,",
    "start": "3040780",
    "end": "3048510"
  },
  {
    "start": "3045000",
    "end": "3568000"
  },
  {
    "text": "uh, identity, which is the conditional expectation, right? So the conditional expectation is another important concept in- in, uh, probability.",
    "start": "3048510",
    "end": "3056880"
  },
  {
    "text": "And the conditional, um, expectation is, uh, defined like this.",
    "start": "3056880",
    "end": "3062930"
  },
  {
    "text": "And the, uh, conditional expectation is actually- is actually somewhat subtle, right?",
    "start": "3063360",
    "end": "3070885"
  },
  {
    "text": "So conditional expectation, um, is- [NOISE]",
    "start": "3070885",
    "end": "3094210"
  },
  {
    "text": "so the conditional expectation of [NOISE] E of X given Y.",
    "start": "3094210",
    "end": "3102040"
  },
  {
    "text": "So you think of the conditional expectation in this form where X and Y are random variables to",
    "start": "3102040",
    "end": "3107860"
  },
  {
    "text": "be a random variable [NOISE], right?",
    "start": "3107860",
    "end": "3115450"
  },
  {
    "text": "The expectation of X [NOISE] is a constant [NOISE], right?",
    "start": "3115450",
    "end": "3122079"
  },
  {
    "text": "But the conditional variable of expectation, uh, the conditional expectation of X given Y is another random variable, right?",
    "start": "3122080",
    "end": "3129145"
  },
  {
    "text": "X is random- X is random, Y is random, expectation of X is just a constant,",
    "start": "3129145",
    "end": "3134410"
  },
  {
    "text": "it is not random, right? The conditional expectation of X given Y is a random variable, right?",
    "start": "3134410",
    "end": "3142825"
  },
  {
    "text": "It's a random variable defined over the space of Y. [NOISE] However,",
    "start": "3142825",
    "end": "3149050"
  },
  {
    "text": "conditional [NOISE] expectation of",
    "start": "3149050",
    "end": "3156415"
  },
  {
    "text": "X given Y equals small y. This is a function [NOISE] of small y, right?",
    "start": "3156415",
    "end": "3170715"
  },
  {
    "text": "The conditional expectation of X given Y, where you're, you know, you're- you're- you're just placing the random variable over",
    "start": "3170715",
    "end": "3176580"
  },
  {
    "text": "here is another random variable, right? It doesn't look like it, but X given Y- co- a conditional expectation of X given Y is another random variable.",
    "start": "3176580",
    "end": "3185185"
  },
  {
    "text": "But conditional expectation of X given Y equals some specific Y,",
    "start": "3185185",
    "end": "3190225"
  },
  {
    "text": "is a function of that specific Y. Yes, question? [BACKGROUND] These two are not the same in the sense,",
    "start": "3190225",
    "end": "3202930"
  },
  {
    "text": "this is still a random variable. The- this whole thing is a random variable, and this whole thing is a function of small y, right?",
    "start": "3202930",
    "end": "3213610"
  },
  {
    "text": "This is- this is, uh, um, a subtle concept and you may want to go back to your probability book to,",
    "start": "3213610",
    "end": "3220240"
  },
  {
    "text": "kind of, uh, uh, go through this again. But, uh, this can be- this can be relevant in some of your homeworks.",
    "start": "3220240",
    "end": "3225474"
  },
  {
    "text": "So that, you know, uh, you- you want to have a good- good- good, uh, clear understanding, uh,",
    "start": "3225475",
    "end": "3230710"
  },
  {
    "text": "that or- or view these as distinct things. Conditional expectation of X given Y,oo",
    "start": "3230710",
    "end": "3236140"
  },
  {
    "text": "where Y is still random variable is another random variable. It is basically, you're saying,",
    "start": "3236140",
    "end": "3241420"
  },
  {
    "text": "you're feeding a random variable Y as input, and this becomes a function of Y,",
    "start": "3241420",
    "end": "3247119"
  },
  {
    "text": "and therefore, it's still a random variable. You can think of it that way. Yes, question? [BACKGROUND]",
    "start": "3247120",
    "end": "3265900"
  },
  {
    "text": "Exactly. So the question is, if this is a random variable, then can we apply all the- all the rules that we've seen so far on this?",
    "start": "3265900",
    "end": "3272305"
  },
  {
    "text": "You- exactly, you can, but you cannot on this one. Yes, question? So is the random variable going to be defined [inaudible] as only a function of X and Y?",
    "start": "3272305",
    "end": "3281980"
  },
  {
    "text": "So over here, the random variable is defined over the space of Y only. Yes, question?",
    "start": "3281980",
    "end": "3291090"
  },
  {
    "text": "[BACKGROUND]",
    "start": "3291090",
    "end": "3300070"
  },
  {
    "text": "Random variables are functions already. This is a function of y,",
    "start": "3300070",
    "end": "3305935"
  },
  {
    "text": "of- of- of, uh, you know, of small y. But, you know, this is a random variable in the sense, this is a function over your sample space,",
    "start": "3305935",
    "end": "3312775"
  },
  {
    "text": "you know, you can- you can think of it that way. [BACKGROUND] This is just,",
    "start": "3312775",
    "end": "3321220"
  },
  {
    "text": "uh, over the sample space of y alone. [BACKGROUND] So may- may-",
    "start": "3321220",
    "end": "3330070"
  },
  {
    "text": "maybe the next- next slide might make it, uh, a little more clear. Uh, so maybe, you know, hold your question until the end of the next slide.",
    "start": "3330070",
    "end": "3337119"
  },
  {
    "text": "Uh, and- and there- was there one more question? Yes, question? [BACKGROUND]",
    "start": "3337120",
    "end": "3357910"
  },
  {
    "text": "Yeah. So, uh, in- in the case of Gaussian, you can think of it like this, right? So, uh, assume this is the contour plot of-",
    "start": "3357910",
    "end": "3363295"
  },
  {
    "text": "of- of Gaussians where it is centered on some, let's call this a, uh, uh,",
    "start": "3363295",
    "end": "3369744"
  },
  {
    "text": "plus 1 and plus 2. And the expecta- and let's call this X and let's call this Y.",
    "start": "3369745",
    "end": "3376630"
  },
  {
    "text": "So, uh, the expectation of- of, uh, X given Y equals, you know,",
    "start": "3376630",
    "end": "3384654"
  },
  {
    "text": "say, some small value y and assume the small value y is this, right?",
    "start": "3384655",
    "end": "3390955"
  },
  {
    "text": "Uh, what it basically tells us if- if you, kind of, limit yourself with this slice of the- of the Gaussian,",
    "start": "3390955",
    "end": "3397555"
  },
  {
    "text": "and that's going to be, you know, that's going to have some kind of a bell shape and what's the expectation over here? Yes, question?",
    "start": "3397555",
    "end": "3406960"
  },
  {
    "text": "Did you say it's a function of small y? So, um, this is a function of small y, expectation of-",
    "start": "3406960",
    "end": "3413590"
  },
  {
    "text": "But here you said it's a function of big Y? That's a typo, that should be small y, sorry.",
    "start": "3413590",
    "end": "3419665"
  },
  {
    "text": "Yeah, this over here should be small y. Thank you. Yeah, so you, um, um, so now we have,",
    "start": "3419665",
    "end": "3428740"
  },
  {
    "text": "uh, something called the law of total expectation, right? So the law of total expectation tells us that,",
    "start": "3428740",
    "end": "3435205"
  },
  {
    "text": "expectation of X can be written as the expectation of- expectation",
    "start": "3435205",
    "end": "3446049"
  },
  {
    "text": "of X given Y, right? Now, this holds true for any X and any Y, right?",
    "start": "3446050",
    "end": "3458214"
  },
  {
    "text": "Y could be completely independent of X, it could be dependent on X. But the expectation of X can always be decomposed into this nested form where,",
    "start": "3458215",
    "end": "3468369"
  },
  {
    "text": "um, you condition on Y, you get a new random variable, right?",
    "start": "3468370",
    "end": "3473845"
  },
  {
    "text": "And you take the expectation of- of this random variable and you get back the expectation of X. This is called the law of total expectation.",
    "start": "3473845",
    "end": "3480610"
  },
  {
    "text": "Um, and- and, um, while this holds true for any Y in general,",
    "start": "3480610",
    "end": "3488349"
  },
  {
    "text": "you have the choice of choosing the Y, and you want to choose a Y that makes your problem easier to solve by,",
    "start": "3488350",
    "end": "3495174"
  },
  {
    "text": "you know, breaking it down this way, right? And- and the choice of what Y you want to choose is more of an art and you got to, um,",
    "start": "3495175",
    "end": "3501130"
  },
  {
    "text": "uh, when you're asked to calculate, you know, the expectation of X and it's- it's, you know, it's- it's pretty, uh,",
    "start": "3501130",
    "end": "3506650"
  },
  {
    "text": "it's pretty complex, uh, you want to use some- some, uh, uh, creativity in choosing a Y to defining this, uh,",
    "start": "3506650",
    "end": "3514740"
  },
  {
    "text": "um, random variable, and- and then take the expectation of that, and by breaking it down into, like,",
    "start": "3514740",
    "end": "3520300"
  },
  {
    "text": "sub-problems, uh, you can- you can, uh, solve some complex problems. [NOISE] So, uh, the expectation of X given Y,",
    "start": "3520300",
    "end": "3527730"
  },
  {
    "text": "where Y is left unspecified is a random variable. And that's what shows up here in the law of total, uh, expectation.",
    "start": "3527730",
    "end": "3534690"
  },
  {
    "text": "[NOISE] And when you take the expectation of that, you get back the expectation of X itself. Yes, question?",
    "start": "3534690",
    "end": "3539850"
  },
  {
    "text": "[inaudible] [NOISE] X given Y, that is a random variable for the function of big Y, right?",
    "start": "3539850",
    "end": "3548265"
  },
  {
    "text": "This one? Yeah. Yeah, you- you- you- you can think of it as a function of big Y, uh,",
    "start": "3548265",
    "end": "3553470"
  },
  {
    "text": "that's one way to think of it or- or just think of this as a random variable, right? This is a random variable like Z,",
    "start": "3553470",
    "end": "3559420"
  },
  {
    "text": "you know, think of this as Z, you know, some random variable. [NOISE] But here's a proof,",
    "start": "3559420",
    "end": "3569470"
  },
  {
    "start": "3568000",
    "end": "3579000"
  },
  {
    "text": "uh, to show, you know, this is just proving, uh, law of total expectation. I'm not going to go with this, you can, uh,",
    "start": "3569470",
    "end": "3574750"
  },
  {
    "text": "uh go through it yourself pretty straightforward. There's- there's nothing, uh, complex going on here.",
    "start": "3574750",
    "end": "3580180"
  },
  {
    "text": "And there's one more version of Bayes' rule, that's going to be, uh, helpful for you. Uh, [NOISE]",
    "start": "3580180",
    "end": "3603730"
  },
  {
    "text": "Right? So- so let's write the Bayes' rule that we've already seen.",
    "start": "3603730",
    "end": "3610795"
  },
  {
    "text": "P of a given b is equal to P of b given",
    "start": "3610795",
    "end": "3618099"
  },
  {
    "text": "a times P of a over P of b.",
    "start": "3618100",
    "end": "3628900"
  },
  {
    "text": "Right? So this is the Bayes' rule that is already, right? And the Bayes' rule basically also allows you",
    "start": "3628900",
    "end": "3635710"
  },
  {
    "text": "to have another variable that is conditioned throughout, right? So you can think of this as the conditional Bayes' Rule,",
    "start": "3635710",
    "end": "3642190"
  },
  {
    "text": "which means P of, um, b comma c. So you- you can have c condition in all the terms.",
    "start": "3642190",
    "end": "3655480"
  },
  {
    "text": "And this is also Bayes' rule, the- the expression that you, uh, see over here. All right?",
    "start": "3655480",
    "end": "3662710"
  },
  {
    "text": "Similarly you can also- um, so this is the Bayes' rule where we are kind of swapping a and b or here.",
    "start": "3662710",
    "end": "3669010"
  },
  {
    "text": "Um, similarly, you could have also just as well kept b conditioned throughout and just swapped a and c. And- and that's also,",
    "start": "3669010",
    "end": "3676450"
  },
  {
    "text": "um, um, you can think of that as Bayes' rule. All right, anymore- any questions?",
    "start": "3676450",
    "end": "3683570"
  },
  {
    "text": "Okay. With this- okay. So here's a proof for this. You can go over this, but, you know, you can just- you can just remember it this way and you can apply it.",
    "start": "3684750",
    "end": "3692380"
  },
  {
    "text": "You- you know, this proof is just to convince yourself, we're not gonna test you on these kind of proofs.",
    "start": "3692380",
    "end": "3698605"
  },
  {
    "text": "Ah, with that, ah, we finish the- the review of probability. Any questions?",
    "start": "3698605",
    "end": "3706255"
  },
  {
    "text": "No. I wanna take a few moments to, um, to kind of give you a- a- a-",
    "start": "3706255",
    "end": "3713800"
  },
  {
    "text": "a slightly a big picture overview of the role statistics plays in machine learning, right?",
    "start": "3713800",
    "end": "3720050"
  },
  {
    "text": "Um, what we saw in the slide so far, right?",
    "start": "3720090",
    "end": "3726055"
  },
  {
    "text": "We had, um, parameters. [NOISE] Okay. For example,",
    "start": "3726055",
    "end": "3734800"
  },
  {
    "text": "Mu Sigma of the Gaussian, okay? And we had observations.",
    "start": "3734800",
    "end": "3741940"
  },
  {
    "text": "[NOISE] The parameters and we had observations,",
    "start": "3741940",
    "end": "3749290"
  },
  {
    "text": "for example, in the multivariate Gaussian, you had x in some R_n, right?",
    "start": "3749290",
    "end": "3755955"
  },
  {
    "text": "So probability basically is the field",
    "start": "3755955",
    "end": "3762280"
  },
  {
    "text": "where you're trying to make statements about the observations given your parameters, right?",
    "start": "3762280",
    "end": "3773480"
  },
  {
    "text": "Probability. So given- given parameters,",
    "start": "3775950",
    "end": "3781270"
  },
  {
    "text": "you are trying to, um, make statements about- about the data, you know, what is the probability of this?",
    "start": "3781270",
    "end": "3786490"
  },
  {
    "text": "What's going to be the marginal probability? What's the conditional probability? What's the joint probability? Things like that.",
    "start": "3786490",
    "end": "3792085"
  },
  {
    "text": "Um, so given obser- given- given- given parameters, ah, we assume that the parameters are given or they're fixed,",
    "start": "3792085",
    "end": "3799525"
  },
  {
    "text": "and you're making statements about observations or- or data. You can think of this as data. All right?",
    "start": "3799525",
    "end": "3808000"
  },
  {
    "text": "And statistics is kind of doing like the opposite, right?",
    "start": "3808000",
    "end": "3813475"
  },
  {
    "text": "You're given data, you start with data, and you're trying to make statements about your parameters. [NOISE] All right?",
    "start": "3813475",
    "end": "3822760"
  },
  {
    "text": "You make some assumptions in statistics about what the distribution is, um, given- given- given the data.",
    "start": "3822760",
    "end": "3830890"
  },
  {
    "text": "And you take the data that's given to you, right? You're not making statements about, you know,",
    "start": "3830890",
    "end": "3836560"
  },
  {
    "text": "what the probability of some other data is or, um- or anything like that. But given the data you are trying to make statements about the parameters, right?",
    "start": "3836560",
    "end": "3845140"
  },
  {
    "text": "And- and- and that's statistics. And you're- you're trying to, um- um, generally the- the statements that you make about",
    "start": "3845140",
    "end": "3852850"
  },
  {
    "text": "the parameters in statistics are about things like, is this parameter value equal to 0?",
    "start": "3852850",
    "end": "3857905"
  },
  {
    "text": "or is this parameter value much farther away from 0, right? And you- you're going to assign,",
    "start": "3857905",
    "end": "3863395"
  },
  {
    "text": "you know, p-values for those observations. You're going to define confidence intervals where, you know, what- what you believe,",
    "start": "3863395",
    "end": "3869830"
  },
  {
    "text": "ah, the- the range for those parameters are, right? So you start from- from data and make statements about the parameters and statistics,",
    "start": "3869830",
    "end": "3876300"
  },
  {
    "text": "but in probability you're kind of, you know, going the other way. And- and these two are kind of, you know, like, yin and yang, they're kind of couple to each other, right?",
    "start": "3876300",
    "end": "3885000"
  },
  {
    "text": "And there are many- many techniques to, um, estimate parameters, um, given- given your data, right?",
    "start": "3885000",
    "end": "3892590"
  },
  {
    "text": "So, ah, things like method of moments or maximum likelihood estimation.",
    "start": "3892590",
    "end": "3902730"
  },
  {
    "text": "[NOISE]",
    "start": "3902730",
    "end": "3913450"
  },
  {
    "text": "And it is this approach that's kind of relevant for us in this course and for most of machine learning.",
    "start": "3913450",
    "end": "3921100"
  },
  {
    "text": "And, um, using these techniques, you- you kind of feed in x as the input for these techniques and it's going to",
    "start": "3921100",
    "end": "3928930"
  },
  {
    "text": "output some- some- some parameters for you, right? Now, where does machine-learning come into the picture?",
    "start": "3928930",
    "end": "3936474"
  },
  {
    "text": "So with machine learning, what we are trying to do is you're given what's called as a training data.",
    "start": "3936475",
    "end": "3945160"
  },
  {
    "text": "All right? Your training data is going to be some collection of x- um,",
    "start": "3945160",
    "end": "3951970"
  },
  {
    "text": "you know, you can think of them as x, y pairs for supervised learning, and you're given a collection of them, right?",
    "start": "3951970",
    "end": "3959035"
  },
  {
    "text": "And given the- given your training data, you want to learn the parameters of your model.",
    "start": "3959035",
    "end": "3966850"
  },
  {
    "text": "You're going to define some kind of a model for- for this data and learn the parameters.",
    "start": "3966850",
    "end": "3972310"
  },
  {
    "text": "And using the learned parameters, so for machine learning,",
    "start": "3972310",
    "end": "3978010"
  },
  {
    "text": "you know, feed training data, right?",
    "start": "3978010",
    "end": "3985180"
  },
  {
    "text": "And you're going to learn your model. And using those models,",
    "start": "3985180",
    "end": "3990414"
  },
  {
    "text": "you want to make predictions about future data, [NOISE] right?",
    "start": "3990414",
    "end": "3997570"
  },
  {
    "text": "So machine-learning, we are actually- ah, the goal that we are interested in is actually given the training data,",
    "start": "3997570",
    "end": "4004260"
  },
  {
    "text": "make predictions about future data, right? But incidentally, we kind of",
    "start": "4004260",
    "end": "4009795"
  },
  {
    "text": "take statistical approaches where we build a model and use the model to make, um- um- um, predictions on future data.",
    "start": "4009795",
    "end": "4017205"
  },
  {
    "text": "And that's where kind of statistics comes into the picture where, you know, you're given- given training data. We use tools from statistics like maximum likelihood estimation,",
    "start": "4017205",
    "end": "4025545"
  },
  {
    "text": "um- um, and other things to learn the model. Even though we're using the same tools of statistics,",
    "start": "4025545",
    "end": "4033809"
  },
  {
    "text": "the goals are very different. In statistics, the goal is to make statements about the parameters itself, right?",
    "start": "4033810",
    "end": "4041040"
  },
  {
    "text": "In machine learning, we really don't care about the parameters at all. All we care is how good is this model in making predictions about future data?",
    "start": "4041040",
    "end": "4049650"
  },
  {
    "text": "And that's how we measure the quality of machine learning methods. You know, how good is it in making predictions about- um,",
    "start": "4049650",
    "end": "4057270"
  },
  {
    "text": "about- about future data? Whereas in statistics, you know, the game ends here. We measure how good the model is or how good our method is in terms of",
    "start": "4057270",
    "end": "4064634"
  },
  {
    "text": "making proper statements about the parameters. All right? In machine learning we, this is- this is a black box for us, right?",
    "start": "4064635",
    "end": "4071895"
  },
  {
    "text": "We- it does estimate your parameters, but they are not of direct interest to us. They're only- we are only interested in the parameters",
    "start": "4071895",
    "end": "4078599"
  },
  {
    "text": "because they help us make predictions about future data, right? [NOISE] And, um, this- this part of the cycle is also what is called as,",
    "start": "4078600",
    "end": "4090960"
  },
  {
    "text": "um, learning or training or fitting;",
    "start": "4090960",
    "end": "4098384"
  },
  {
    "text": "fit a model with data, right? And in- in classical statistics,",
    "start": "4098385",
    "end": "4105824"
  },
  {
    "text": "you also call this statistical inference. All right?",
    "start": "4105825",
    "end": "4111449"
  },
  {
    "text": "You're inferring what the parameters are given- um, given the data. And once you have data,",
    "start": "4111450",
    "end": "4118424"
  },
  {
    "text": "this is called prediction. You're making predictions about- about future data,",
    "start": "4118425",
    "end": "4125775"
  },
  {
    "text": "about unseen things- things that you haven't seen before, about data that you haven't encountered before, right?",
    "start": "4125775",
    "end": "4132299"
  },
  {
    "text": "And- which- which brings us to maximum likelihood estimation, right? In maximum likelihood estimation,",
    "start": "4132300",
    "end": "4139380"
  },
  {
    "text": "um, I'm going to call it MLE. [NOISE] Now, MLE, um,",
    "start": "4141340",
    "end": "4152750"
  },
  {
    "text": "we- let's start with a very simple example of MLE to- to kind of give you the flavor of,",
    "start": "4152750",
    "end": "4158745"
  },
  {
    "text": "um- uh- let's- let's start with Gaussians- Gaussian data.",
    "start": "4158745",
    "end": "4166140"
  },
  {
    "text": "So in the- here we are going to talk about a simple situation where we are given data x in R_d,",
    "start": "4166140",
    "end": "4173444"
  },
  {
    "text": "and you're given a collection of them. I'm going to write them as x_1 through x_n.",
    "start": "4173445",
    "end": "4182730"
  },
  {
    "text": "In this notation, what I mean is each x_i is the i-th example.",
    "start": "4182730",
    "end": "4190904"
  },
  {
    "text": "It does not mean x raised to the power of i or x raised to the power of n. Ah, whenever I write the superscript with parentheses,",
    "start": "4190905",
    "end": "4198855"
  },
  {
    "text": "it means it's the i-th example, right? Ah, and there are n such examples.",
    "start": "4198855",
    "end": "4206670"
  },
  {
    "text": "Each example is in R_d. So it's each- each x_i is a d-dimensional vector, right?",
    "start": "4206670",
    "end": "4215655"
  },
  {
    "text": "And we are given- um, let's assume we are given, um- um, n such examples, right?",
    "start": "4215655",
    "end": "4222345"
  },
  {
    "text": "Now, the probability- we already saw the probability density of,",
    "start": "4222345",
    "end": "4231310"
  },
  {
    "text": "um, of the Gaussian, which is P of x given Mu Sigma is equal to",
    "start": "4231320",
    "end": "4240554"
  },
  {
    "text": "1 over 2 pi to the d by 2 [NOISE] sigma half exponent [NOISE]",
    "start": "4240555",
    "end": "4263204"
  },
  {
    "text": "Right. So this is the probability [NOISE] density.",
    "start": "4263205",
    "end": "4274380"
  },
  {
    "text": "Okay. Now, we are now gonna define something called as the likelihood function.",
    "start": "4274380",
    "end": "4282060"
  },
  {
    "text": "[NOISE] All right. So let's define the likelihood function of the parameters,",
    "start": "4282060",
    "end": "4289215"
  },
  {
    "text": "mu comma sigma [NOISE] given- given the data X.",
    "start": "4289215",
    "end": "4299925"
  },
  {
    "text": "Okay. [NOISE] And for notation, let's- let's call X as the- as the collection of,",
    "start": "4299925",
    "end": "4310139"
  },
  {
    "text": "uh, your N examples. And over here, the semicolon means, um,",
    "start": "4310140",
    "end": "4316530"
  },
  {
    "text": "things to the right are kind of, um, um, think of them as parameters or think of them as- as, you know, given values.",
    "start": "4316530",
    "end": "4323310"
  },
  {
    "text": "And the function is being defined on the variables that are to the left of the, uh, semi- semicolon, right?",
    "start": "4323310",
    "end": "4329995"
  },
  {
    "text": "In order to define the likelihood function, we are gonna make yet another assumption.",
    "start": "4329995",
    "end": "4335765"
  },
  {
    "text": "The assumption here is that each of these are sampled [NOISE]",
    "start": "4335765",
    "end": "4343580"
  },
  {
    "text": "independently and [NOISE] identically distributed.",
    "start": "4343580",
    "end": "4354780"
  },
  {
    "text": "[NOISE] Right? So this is also commonly called [NOISE] IID.",
    "start": "4354780",
    "end": "4361395"
  },
  {
    "text": "Um, and this is gonna be a very common assumption in all our-",
    "start": "4361395",
    "end": "4368415"
  },
  {
    "text": "in all machine learning methods that- that- the- the training data that you have are sampled in an IID fashion,",
    "start": "4368415",
    "end": "4374880"
  },
  {
    "text": "which means X^1 through X^n are all independent of each other. They were sampled independently.",
    "start": "4374880",
    "end": "4380235"
  },
  {
    "text": "Right? Now the like- uh, de- we define the, uh, likelihood, uh, function as- so, um,",
    "start": "4380235",
    "end": "4391750"
  },
  {
    "text": "so this was for one example. Let's make this, um,",
    "start": "4391940",
    "end": "4397440"
  },
  {
    "text": "for all the examples. Um, s- so the probability of X^1",
    "start": "4397440",
    "end": "4404350"
  },
  {
    "text": "through X^n mu comma sigma is gonna be- we though- we- we thought independence,",
    "start": "4405200",
    "end": "4416415"
  },
  {
    "text": "um, that P of X^1 through X^n,",
    "start": "4416415",
    "end": "4423410"
  },
  {
    "text": "if X^1 to X^n are independent of each other, we can write this as the product of i equals 1 through n, P of X^i.",
    "start": "4423410",
    "end": "4436410"
  },
  {
    "text": "Right. This is- this means product. You're- you're- you're- you're multiplying the- the terms over here.",
    "start": "4436410",
    "end": "4443625"
  },
  {
    "text": "Okay? Yes. Question?",
    "start": "4443625",
    "end": "4446410"
  },
  {
    "text": "Can you explain about [inaudible]",
    "start": "4449060",
    "end": "4454620"
  },
  {
    "text": "All right. Correct. Let- let- let me- let me- I- I- I haven't, uh, finished this yet. So let- let me come back to this in a moment.",
    "start": "4454620",
    "end": "4460455"
  },
  {
    "text": "Okay? So the, uh, the joint probability of independent, um,",
    "start": "4460455",
    "end": "4466785"
  },
  {
    "text": "independent pieces of data is the product of the individual marginals. Right? Now, if you want to define the,",
    "start": "4466785",
    "end": "4474120"
  },
  {
    "text": "uh, probability density of your full data set, X^1 through X^n mu sigma,",
    "start": "4474120",
    "end": "4484830"
  },
  {
    "text": "[NOISE] you're gonna write this as the product of i equals 1 through n,",
    "start": "4484830",
    "end": "4491475"
  },
  {
    "text": "1 over 2 Pi^d by",
    "start": "4491475",
    "end": "4497120"
  },
  {
    "text": "2 [NOISE] half exponent [NOISE] minus half,",
    "start": "4497120",
    "end": "4505155"
  },
  {
    "text": "here X^i [NOISE] minus mu [NOISE] transpose.",
    "start": "4505155",
    "end": "4510570"
  },
  {
    "text": "[NOISE]",
    "start": "4510570",
    "end": "4520320"
  },
  {
    "text": "Does it make sense? Because we are making, um, think of this as, um, um, this was for the case of one example.",
    "start": "4520320",
    "end": "4527909"
  },
  {
    "text": "But if you are given, you know, uh, a- an entire training set, the probability density of the full training set.",
    "start": "4527910",
    "end": "4534045"
  },
  {
    "text": "Because of the IID assumption, we can break it down into the- the probabilities of individual examples and just,",
    "start": "4534045",
    "end": "4541425"
  },
  {
    "text": "uh, uh, multiply over them. Yes. Question? [inaudible] [OVERLAPPING]",
    "start": "4541425",
    "end": "4547679"
  },
  {
    "text": "X is a vector. D dimensions. E- Is a vector of d dimensions, yes. [inaudible]",
    "start": "4547680",
    "end": "4555210"
  },
  {
    "text": "So, this- in this case, X is also a vector. Right. So you have X minus mu transpose,",
    "start": "4555210",
    "end": "4562244"
  },
  {
    "text": "you know, this- this is still in a- in a- in a vector setting. Okay. So this is for one example,",
    "start": "4562244",
    "end": "4569925"
  },
  {
    "text": "that the example is still vector-valued and this is, um, an example where each of the example is still the d dimension. Yes. Question?",
    "start": "4569925",
    "end": "4578474"
  },
  {
    "text": "What is the semicolon, sir? So the semicolon is a way of saying things that come to the right.",
    "start": "4578475",
    "end": "4583949"
  },
  {
    "text": "You're treating them as given. Uh, you don't think of them as variables. They are just some- some- some- some constants.",
    "start": "4583950",
    "end": "4590270"
  },
  {
    "text": "[NOISE] Okay. Now, um, this is the probability density and it is",
    "start": "4590270",
    "end": "4597270"
  },
  {
    "text": "precisely this IID assumption that lets us break down a complex problem into smaller pieces, right?",
    "start": "4597270",
    "end": "4606014"
  },
  {
    "text": "It's, um, representing this as one large Gaussian, would have- would- just makes it a- a harder problem.",
    "start": "4606015",
    "end": "4611970"
  },
  {
    "text": "We don't even know, um, you know, what this distribution might look like if there is, you know, um, um, uh, if the- if the interactions are pretty complex.",
    "start": "4611970",
    "end": "4620115"
  },
  {
    "text": "So instead, when we make the IID assumption that your training data just came to you indep- each example was- was generated independently,",
    "start": "4620115",
    "end": "4627824"
  },
  {
    "text": "we break it down into, um, you know, smaller problems. Now, once we have this, uh,",
    "start": "4627825",
    "end": "4633795"
  },
  {
    "text": "you know, this as the probability density, we're gonna now define the likelihood function.",
    "start": "4633795",
    "end": "4639989"
  },
  {
    "text": "[NOISE] So the likelihood function is, you know,",
    "start": "4639990",
    "end": "4647940"
  },
  {
    "text": "in- in the spirit of this, a probability density function takes X as your data,",
    "start": "4647940",
    "end": "4653805"
  },
  {
    "text": "uh, X as the- your data as the variable, and assumes your parameters to be given.",
    "start": "4653805",
    "end": "4659070"
  },
  {
    "text": "But for going the other way, you're gonna take your data to be, you know, given and your parameters are the thing you wanna, um, uh, uh,",
    "start": "4659070",
    "end": "4668100"
  },
  {
    "text": "make a statement or- so over here, the likelihood function is gonna be over mu and",
    "start": "4668100",
    "end": "4673260"
  },
  {
    "text": "sigma X^1 through X^n.",
    "start": "4673260",
    "end": "4682159"
  },
  {
    "text": "Right. And, you know, uh, to no surprise,",
    "start": "4682160",
    "end": "4688110"
  },
  {
    "text": "this is gonna look exactly like this. [NOISE] Yes. Question?",
    "start": "4688110",
    "end": "4695790"
  },
  {
    "text": "[inaudible] [OVERLAPPING]",
    "start": "4695790",
    "end": "4701850"
  },
  {
    "text": "Yeah. Ho- Hol- Hold that question for a moment. Yeah. Okay. Um, what we see is that this expression over here,",
    "start": "4701850",
    "end": "4708585"
  },
  {
    "text": "in case of the Gaussian, this- the- the- the- uh, what we thought of as the density function.",
    "start": "4708585",
    "end": "4714760"
  },
  {
    "text": "Also, you- we can repurpose it as our likelihood function.",
    "start": "4714770",
    "end": "4719820"
  },
  {
    "text": "The difference is that in case of- when we- when we think of this as,",
    "start": "4719820",
    "end": "4725474"
  },
  {
    "text": "uh, density function, right, [NOISE] the data is",
    "start": "4725475",
    "end": "4738780"
  },
  {
    "text": "kind of- the function is defined over the data space. Right? But when we define it as a likelihood function,",
    "start": "4738780",
    "end": "4745155"
  },
  {
    "text": "um, product from i equals 1 to n, 1 over 2 Pi^d by 2 sigma [NOISE] to",
    "start": "4745155",
    "end": "4757860"
  },
  {
    "text": "the half exponent of [NOISE] half X^i minus mu inverse",
    "start": "4757860",
    "end": "4771735"
  },
  {
    "text": "X^i minus [NOISE]",
    "start": "4771735",
    "end": "4779074"
  },
  {
    "text": "It is basically the same expression, but we are interpreting it differently.",
    "start": "4779075",
    "end": "4784445"
  },
  {
    "text": "We consider in the likely, uh, when we see this as a likelihood function, we think of the very- the,",
    "start": "4784445",
    "end": "4790685"
  },
  {
    "text": "the parameters to be the variables, right? And your data to be given, right?",
    "start": "4790685",
    "end": "4795950"
  },
  {
    "text": "When we think of- reinterpret this as the probability density function, we think of the data as the variables and your parameters to be fixed. Yes, question.",
    "start": "4795950",
    "end": "4805460"
  },
  {
    "text": "[inaudible]",
    "start": "4805460",
    "end": "4815120"
  },
  {
    "text": "Right, so the question is, um, when you think- when, when you write probability of Mu, Sigma given x.",
    "start": "4815120",
    "end": "4821750"
  },
  {
    "text": "So for- so, er, er, a small correction over there when we, we use the term probability",
    "start": "4821750",
    "end": "4834930"
  },
  {
    "text": "of data, right? We never use the term probability of your parameters.",
    "start": "4836560",
    "end": "4843755"
  },
  {
    "text": "And we use the term likelihood of your parameters, right?",
    "start": "4843755",
    "end": "4853310"
  },
  {
    "text": "The, the, the correct phrases to use his probability of the data given parameters,",
    "start": "4853310",
    "end": "4859410"
  },
  {
    "text": "and the likelihood of the parameters given data, right?",
    "start": "4859470",
    "end": "4867115"
  },
  {
    "text": "You wanna, you wanna- [inaudible] That is, that is the only difference in terms of,",
    "start": "4867115",
    "end": "4872675"
  },
  {
    "text": "uh, you know, uh, um, mathematically, but semantically, we are treating the, the, um,",
    "start": "4872675",
    "end": "4878659"
  },
  {
    "text": "um, the data as, as the variable here, but the parameters as variable here.",
    "start": "4878660",
    "end": "4883940"
  },
  {
    "text": "Other than, other than that, the expression is just the same. And, um, the, the- one of",
    "start": "4883940",
    "end": "4889640"
  },
  {
    "text": "the obvious- another obvious way in which they are different is if, if you integrate your probability density over its variables,",
    "start": "4889640",
    "end": "4897514"
  },
  {
    "text": "you get one, right? But if you integrate your likelihood with respect to,",
    "start": "4897514",
    "end": "4902735"
  },
  {
    "text": "you know, for your parameter space, it may not even exist. Yes, question.",
    "start": "4902735",
    "end": "4909739"
  },
  {
    "text": "[inaudible]",
    "start": "4909740",
    "end": "4916490"
  },
  {
    "text": "Does the likelihood form one particular slice in the, in the- that, that's a good question.",
    "start": "4916490",
    "end": "4922610"
  },
  {
    "text": "So let's, let's, let's, kind of, go over that again. [NOISE]",
    "start": "4922610",
    "end": "4936440"
  },
  {
    "text": "So assume this is your probability density function, right? Now, the shape of this curve and,",
    "start": "4936440",
    "end": "4942215"
  },
  {
    "text": "and the position and, you know, everything about this curve is, kind of, fixed by the Mu and Sigma, right?",
    "start": "4942215",
    "end": "4949205"
  },
  {
    "text": "And the x axis is data or observation, right?",
    "start": "4949205",
    "end": "4956420"
  },
  {
    "text": "Now, the likelihood, uh, uh, for this is going to be Mu,",
    "start": "4956420",
    "end": "4968360"
  },
  {
    "text": "Sigma and this is likelihood, and here it was the probability density.",
    "start": "4968360",
    "end": "4973940"
  },
  {
    "text": "And the likelihood function is going to take, you know, some shape, and your training data is going to decide what the shape of this is, right?",
    "start": "4973940",
    "end": "4988430"
  },
  {
    "text": "So that's, that's, that's, you know, you've got to think of it this way where the, um, this would probably again,",
    "start": "4988430",
    "end": "4996830"
  },
  {
    "text": "be some, some, some,",
    "start": "4996830",
    "end": "5001960"
  },
  {
    "text": "kind of, uh, uh, um, up- up- upward bending.",
    "start": "5001960",
    "end": "5007465"
  },
  {
    "text": "But, um, the x axis over here is data. The x axis over here is Mu and Sigma.",
    "start": "5007465",
    "end": "5014110"
  },
  {
    "text": "So this is probability, and this is likelihood.",
    "start": "5014110",
    "end": "5020215"
  },
  {
    "text": "So if you are given a different collection of training data, your likelihood function is going to take a different shape,",
    "start": "5020215",
    "end": "5027955"
  },
  {
    "text": "be located somewhere else. It may take a different, you know, uh, it, it may look very different and this may not even integrate over to 1,",
    "start": "5027955",
    "end": "5034855"
  },
  {
    "text": "you know, um, the area under this or the volume under this, un- under the likelihood function. But for a probability density function, the,",
    "start": "5034855",
    "end": "5043255"
  },
  {
    "text": "the parameters will decide the shape and the volume under,",
    "start": "5043255",
    "end": "5048280"
  },
  {
    "text": "under this function will always integrate to 1. So, um, yeah, yeah.",
    "start": "5048280",
    "end": "5053425"
  },
  {
    "text": "It's, it's, it's the, the, uh, the analogy of taking a slice, it doesn't quite hold, um, you know,",
    "start": "5053425",
    "end": "5059755"
  },
  {
    "text": "they- you- they're just two different, you know, think of them as two different functions. Yeah. [inaudible]",
    "start": "5059755",
    "end": "5085710"
  },
  {
    "text": "Exactly. So, um, the, the, the problem that we're gonna attack with the likelihood function is,",
    "start": "5085710",
    "end": "5091335"
  },
  {
    "text": "given the training data that we have, we want to choose some Mu and Sigma that maximizes the likelihood the most, right?",
    "start": "5091335",
    "end": "5098500"
  },
  {
    "text": "And that's, that's, that- that's basically, uh, um, like, the big picture of what we do in statistics.",
    "start": "5098500",
    "end": "5104200"
  },
  {
    "text": "You are given a data, that data is gonna, you know, implicitly define some, kind of, a likelihood function, and now we want to find the parameters that make the given data,",
    "start": "5104200",
    "end": "5113350"
  },
  {
    "text": "you know, most probable. [inaudible]",
    "start": "5113350",
    "end": "5122230"
  },
  {
    "text": "It's, it's, it's, uh, I'm not intentionally not gonna go into the details of, you know, will a maximum always exist or not.",
    "start": "5122230",
    "end": "5129355"
  },
  {
    "text": "Um, but this is the intention- intuition you wanna have. There are technical conditions that make a maximum exist,",
    "start": "5129355",
    "end": "5136420"
  },
  {
    "text": "uh, uh, and things like that, but, you know, that's beyond the scope. You know, for most of our problems,",
    "start": "5136420",
    "end": "5141745"
  },
  {
    "text": "you know, you know, this, this is the intuition that you want to have. You know, the, the, um, the likelihood is a function defined over the parameter space.",
    "start": "5141745",
    "end": "5150370"
  },
  {
    "text": "And the shape of the function is gonna be different according to the data that we have.",
    "start": "5150370",
    "end": "5155845"
  },
  {
    "text": "And the probability, you know, which is- which is like the other way, it is defined over the observations on",
    "start": "5155845",
    "end": "5161140"
  },
  {
    "text": "the data and different parameters is gonna give you, you know, different probability density functions.",
    "start": "5161140",
    "end": "5167110"
  },
  {
    "text": "There was another question? Yes. [inaudible]",
    "start": "5167110",
    "end": "5182800"
  },
  {
    "text": "We- again, hold that thought. We're gonna, we gonna, uh, uh, um, um, talk about what we're, you know,",
    "start": "5182800",
    "end": "5189010"
  },
  {
    "text": "how we're gonna extend this into machine learning. But this is, you know, gene- uh, you know, classical statistics.",
    "start": "5189010",
    "end": "5194020"
  },
  {
    "text": "What's likelihood? What's probability, right? Yes, question? [inaudible]",
    "start": "5194020",
    "end": "5207640"
  },
  {
    "text": "Yes. Yes. So, uh, the question was when we, er, you know, uh, based on, you know, in- in based on the, uh,",
    "start": "5207640",
    "end": "5215455"
  },
  {
    "text": "training data that we're given, we can estimate a Mu and Sigma and that Mu and Sigma is going to define some probability distribution.",
    "start": "5215455",
    "end": "5222160"
  },
  {
    "text": "And that's the- and using this distribution is what we're going to use for making predictions over future data.",
    "start": "5222160",
    "end": "5227440"
  },
  {
    "text": "That's- that's the, uh, rough idea, right? So that's- that's, uh, maximum likelihood.",
    "start": "5227440",
    "end": "5235090"
  },
  {
    "text": "Um, and [NOISE] the procedure, you know,",
    "start": "5235090",
    "end": "5244510"
  },
  {
    "text": "that we're going to follow is, um, in some kind of, you know, uh, an abstract,",
    "start": "5244510",
    "end": "5253450"
  },
  {
    "text": "let- you know in an abstract way, if L Theta given x is the likelihood function,",
    "start": "5253450",
    "end": "5263047"
  },
  {
    "text": "x on form where x is uh, um, so suppose we are given something data and it-",
    "start": "5263047",
    "end": "5270385"
  },
  {
    "text": "we- we make an assumption that it belongs to some distribution, for example, the Gaussian, right? The likelihood is going to take some form and that's going to be of",
    "start": "5270385",
    "end": "5278230"
  },
  {
    "text": "the form of some, you know, product of i equals 1_n,",
    "start": "5278230",
    "end": "5284560"
  },
  {
    "text": "you know, L _Theta x_i.",
    "start": "5284560",
    "end": "5289615"
  },
  {
    "text": "I'm intentionally not expanding out what L is. It could be a Gaussian, um, like that.",
    "start": "5289615",
    "end": "5295000"
  },
  {
    "text": "And the maximum likelihood procedure basically tells us that Theta hat- so in general,",
    "start": "5295000",
    "end": "5303040"
  },
  {
    "text": "we use, um, the, uh, the- the notation that we're going to use is, we're going to put a hat over things that are estimated, right?",
    "start": "5303040",
    "end": "5310810"
  },
  {
    "text": "Things that are given to us like training data are, you know, we just use them as variables, but things that we estimate as the output of some- some estimation process,",
    "start": "5310810",
    "end": "5318865"
  },
  {
    "text": "we're going to put a hat over it, right? And Theta hat and I'm going to use the subscript",
    "start": "5318865",
    "end": "5324010"
  },
  {
    "text": "MLE to say that we use the maximum likelihood procedure to estimate this. This is going to be arg max of Theta,",
    "start": "5324010",
    "end": "5333880"
  },
  {
    "text": "uh, of i equals 1_n.",
    "start": "5333880",
    "end": "5339250"
  },
  {
    "text": "Likelihood of Theta for each x_i, right?",
    "start": "5339250",
    "end": "5344720"
  },
  {
    "text": "So think of, you know, for example, in case of the Gaussian, you can think of this expression coming over- coming in over here, right?",
    "start": "5346230",
    "end": "5355975"
  },
  {
    "text": "And further, wha- I'm going to make this claim.",
    "start": "5355975",
    "end": "5362860"
  },
  {
    "text": "And this is always going to be the same as arg max of",
    "start": "5362860",
    "end": "5368510"
  },
  {
    "text": "log of product of i equals 1_n L theta.",
    "start": "5368510",
    "end": "5375900"
  },
  {
    "text": "What will I do here? There was this- this function,",
    "start": "5378200",
    "end": "5383770"
  },
  {
    "text": "the likelihood function defined over all your training examples. And I make a claim here that instead of maximizing this,",
    "start": "5383770",
    "end": "5392035"
  },
  {
    "text": "the product of these, I'm going to maximize the log of the product of these, okay? And why is this true?",
    "start": "5392035",
    "end": "5399500"
  },
  {
    "text": "Log is a monotonically increasing function and if we want to the- the,",
    "start": "5401130",
    "end": "5409465"
  },
  {
    "text": "uh, the Theta that we obtain with this will always be the Theta,",
    "start": "5409465",
    "end": "5415599"
  },
  {
    "text": "uh, will always be the same as the Theta that we obtained with this, okay? Because it is a monotonically increasing function. Yes, question.",
    "start": "5415600",
    "end": "5422650"
  },
  {
    "text": "[inaudible].",
    "start": "5422650",
    "end": "5443830"
  },
  {
    "text": "Okay. Let- let- let- let- let's keep that question for- for, uh, uh, uh, uh, you know, let- I'm going to answer that after that.",
    "start": "5443830",
    "end": "5449905"
  },
  {
    "text": "Let, uh, uh, just because I'm running short of time, I want to, you know, uh, cover a few more things. Uh, but in general, we're going to assume some form,",
    "start": "5449905",
    "end": "5457105"
  },
  {
    "text": "some probability, density and just going to work with that, right? Now, um, here the, um, um,",
    "start": "5457105",
    "end": "5465265"
  },
  {
    "text": "so we replaced the, uh, uh, the thing that we want to, uh, maximize with- with the log of- of the same thing.",
    "start": "5465265",
    "end": "5473665"
  },
  {
    "text": "Um, it's important to note that here we are calculating the arg max. You know, if you were just calculating the max,",
    "start": "5473665",
    "end": "5479760"
  },
  {
    "text": "then of course these two are different. But here we are calculating the arg max. So the, uh, the Theta that maximizes this",
    "start": "5479760",
    "end": "5486510"
  },
  {
    "text": "will also be the same Theta that maximizes the log of this, right? Because we're doing the arg max.",
    "start": "5486510",
    "end": "5492034"
  },
  {
    "text": "Um, and this can now be written as arg max of Theta,",
    "start": "5492035",
    "end": "5500000"
  },
  {
    "text": "sum of i equals 1_n small l_Theta x_i.",
    "start": "5500010",
    "end": "5511400"
  },
  {
    "text": "Where l_Theta is equal to log of L of the other theta, right?",
    "start": "5511400",
    "end": "5521739"
  },
  {
    "text": "So the log of a product of a few terms is the sum of the log of the individual terms, right?",
    "start": "5521740",
    "end": "5528090"
  },
  {
    "text": "And, um, the- the- the- and we replace the- the function L with the log of itself, right?",
    "start": "5528090",
    "end": "5537830"
  },
  {
    "text": "Now, um, what would happen if L were to be a Gaussian?",
    "start": "5538620",
    "end": "5544820"
  },
  {
    "text": "If- if- if we assume that our training data x is coming from a multivariate Gaussian,",
    "start": "5545370",
    "end": "5551155"
  },
  {
    "text": "we would then plug in- so- so far, it's a standard template. No matter what problem that we have, you know,",
    "start": "5551155",
    "end": "5558580"
  },
  {
    "text": "this is the recipe that we're going to follow for maximum likelihood estimation. And to take this further,",
    "start": "5558580",
    "end": "5566244"
  },
  {
    "text": "we now have to get into the functional form of what the likelihood function actually is for that problem, right?",
    "start": "5566245",
    "end": "5573100"
  },
  {
    "text": "And in this case, for Gaussian, it's going to be arg max of.",
    "start": "5573100",
    "end": "5582445"
  },
  {
    "text": "Now, so far we were, uh, just talking about Theta in- in general.",
    "start": "5582445",
    "end": "5588264"
  },
  {
    "text": "But now we're going to talk about the parameters of the Gaussian. Here we were, you know, by Theta, we were just, uh,",
    "start": "5588265",
    "end": "5594625"
  },
  {
    "text": "mentioning the parameters of whatever distribution that we're going to use in an abstract way.",
    "start": "5594625",
    "end": "5600170"
  },
  {
    "text": "And sum over i equals 1_n log of this whole thing,",
    "start": "5600600",
    "end": "5610225"
  },
  {
    "text": "1 over 2 Pi to the d by 2. This is going to look a little big,",
    "start": "5610225",
    "end": "5619120"
  },
  {
    "text": "but it's going to simplify, I promise. V_2 exponent of minus half x minus Mu transpose",
    "start": "5619120",
    "end": "5632485"
  },
  {
    "text": "Sigma inverse x minus Mu.",
    "start": "5632485",
    "end": "5642110"
  },
  {
    "text": "So can we simplify this further? Yes, we can. So there is an exponent and there's a log, right?",
    "start": "5643810",
    "end": "5650330"
  },
  {
    "text": "So the- the exponent and log cancel out. And it's the log of the product of two terms, you know,",
    "start": "5650330",
    "end": "5655670"
  },
  {
    "text": "and that can be broken down into the sum of the logs of the individual terms, right? And here I'm gonna use this as Mu hat, Sigma hat.",
    "start": "5655670",
    "end": "5665875"
  },
  {
    "text": "And this is um, our max Mu Sigma sum over I equals 1_n.",
    "start": "5665875",
    "end": "5678554"
  },
  {
    "text": "So I'm gonna break this down into three parts, right? 1 over 2 pi to the d by 2, and the log of that.",
    "start": "5678555",
    "end": "5685940"
  },
  {
    "text": "It's gonna be some constant, at some constant, there are no variables there. So I'm just going to write that as some constant k, right?",
    "start": "5685940",
    "end": "5694040"
  },
  {
    "text": "Plus log or half",
    "start": "5694040",
    "end": "5700789"
  },
  {
    "text": "log of the determinant of Sigma, right?",
    "start": "5700789",
    "end": "5705950"
  },
  {
    "text": "Is it plus half or minus half, sorry, minus half plus log of the exponent,",
    "start": "5705950",
    "end": "5714949"
  },
  {
    "text": "they're going to cancel and there's a minus, minus half x minus Mu transpose",
    "start": "5714950",
    "end": "5723440"
  },
  {
    "text": "Sigma inverse x minus Mu, right.",
    "start": "5723440",
    "end": "5732485"
  },
  {
    "text": "And now this is some function, this whole thing is some function of Mu and Sigma.",
    "start": "5732485",
    "end": "5741455"
  },
  {
    "text": "We're going to think of x as- as given. Oh, this should be xi.",
    "start": "5741455",
    "end": "5747060"
  },
  {
    "text": "Now how do we do the argmax? Calculus, right.",
    "start": "5749830",
    "end": "5755585"
  },
  {
    "text": "Take the- take the derivative, set the derivative equal to zero and solve for Mu and Sigma.",
    "start": "5755585",
    "end": "5761630"
  },
  {
    "text": "Right. And do we have time to do this today? Yeah, we have some time.",
    "start": "5761630",
    "end": "5766835"
  },
  {
    "text": "So, here- here, um,",
    "start": "5766835",
    "end": "5772400"
  },
  {
    "text": "and this is where the things we reviewed over the last couple of days are gonna, you know, ah, come in handy.",
    "start": "5772400",
    "end": "5778340"
  },
  {
    "text": "We saw how to take the derivative of quadratic forms. We saw how to take the derivative of a log of the determinant of a matrix.",
    "start": "5778340",
    "end": "5788550"
  },
  {
    "text": "Right. And those things are gonna be used right away over here. [inaudible]",
    "start": "5788550",
    "end": "5802220"
  },
  {
    "text": "What's the question again? [inaudible]",
    "start": "5802220",
    "end": "5807530"
  },
  {
    "text": "So this is the- the- the summation applies for the whole thing. The summation is for all the terms in there.",
    "start": "5807530",
    "end": "5815730"
  },
  {
    "text": "So I'm going to write it out here. So we need to maximize this with respect to two variables,",
    "start": "5861280",
    "end": "5868115"
  },
  {
    "text": "Mu and Sigma jointly. So we're going to take partial derivatives.",
    "start": "5868115",
    "end": "5873905"
  },
  {
    "text": "So let's- let's solve it separately. So this is for Mu and this is for Sigma, right?",
    "start": "5873905",
    "end": "5880895"
  },
  {
    "text": "And so the derivative with respect to Mu [NOISE] of summation I equals 1",
    "start": "5880895",
    "end": "5895790"
  },
  {
    "text": "to n, k times minus half log Sigma",
    "start": "5895790",
    "end": "5904445"
  },
  {
    "text": "minus half xi minus Mu",
    "start": "5904445",
    "end": "5910610"
  },
  {
    "text": "transpose Sigma inverse xi minus Mu, right.",
    "start": "5910610",
    "end": "5918985"
  },
  {
    "text": "And when we take the derivative, um, and this is gonna be sum over I equals 1",
    "start": "5918985",
    "end": "5924880"
  },
  {
    "text": "to n. Derivative of k with respect to Mu is 0. Derivative of log determinant of this with respect to Mu is 0.",
    "start": "5924880",
    "end": "5932600"
  },
  {
    "text": "And over to your left it is d, derivative with respect to the quadratic formula, right.",
    "start": "5932600",
    "end": "5940220"
  },
  {
    "text": "And so the- we saw that the derivative of x with respect to,",
    "start": "5940220",
    "end": "5946025"
  },
  {
    "text": "uh, x transpose x is equal to 2ax.",
    "start": "5946025",
    "end": "5951920"
  },
  {
    "text": "If it is symmetric and Sigma is symmetric, it's a covariance matrix. So covariance matrices are always symmetric.",
    "start": "5952270",
    "end": "5959150"
  },
  {
    "text": "Right. And that is a half and there's, um- and the half is gonna cancel with the 2, right?",
    "start": "5959150",
    "end": "5965195"
  },
  {
    "text": "And this will give us, uh, minus xi minus Mu.",
    "start": "5965195",
    "end": "5972600"
  },
  {
    "text": "So let- let me expand this out to- to- to, uh, uh- in case it caused any confusion.",
    "start": "5975550",
    "end": "5982710"
  },
  {
    "text": "We're gonna expand this out. So that will give us, um, so minus half is a common, is a common for everything.",
    "start": "5983410",
    "end": "5990454"
  },
  {
    "text": "Xi transpose Sigma inverse xi.",
    "start": "5990455",
    "end": "5995670"
  },
  {
    "text": "So this is just a distributive property of- of, uh, multiplication. You take every- every pair and write it out, uh,",
    "start": "5996040",
    "end": "6004000"
  },
  {
    "text": "minus xi transpose sigma inverse Mu, and the other way,",
    "start": "6004000",
    "end": "6012640"
  },
  {
    "text": "minus Mu transpose Sigma inverse xi plus Mu transpose Sigma inverse Mu.",
    "start": "6012640",
    "end": "6023600"
  },
  {
    "text": "Is it clear how we went from this step to this step? It's just you know the distribution- distributive property of multiplication.",
    "start": "6023730",
    "end": "6032260"
  },
  {
    "text": "Um, and now you still wanna take this with respect to Mu. Right. And this is equal to sum over I equals 1_n minus half.",
    "start": "6032260",
    "end": "6044000"
  },
  {
    "text": "Right. This has no Mu in it. So this is just gonna be 0.",
    "start": "6044190",
    "end": "6050650"
  },
  {
    "text": "This and this are the same thing because it's a scaler and you can take the transpose of each other.",
    "start": "6050650",
    "end": "6056815"
  },
  {
    "text": "Right. So I'm gonna write this as- let me still write it with respect to Mu,",
    "start": "6056815",
    "end": "6062515"
  },
  {
    "text": "um, it's gonna be Sigma inverse.",
    "start": "6062515",
    "end": "6072710"
  },
  {
    "text": "And the transpose of that is- is, uh, uh, the same, xi transpose Mu minus 2 of that",
    "start": "6072960",
    "end": "6083710"
  },
  {
    "text": "plus Mu transpose sigma inverse Mu, right.",
    "start": "6083710",
    "end": "6090790"
  },
  {
    "text": "And now take the derivative in. It's gonna be I equals 1_n um,",
    "start": "6090790",
    "end": "6096190"
  },
  {
    "text": "minus half and um, the two cancel out. And this will be- this will be,",
    "start": "6096190",
    "end": "6104665"
  },
  {
    "text": "uh, uh- with respect to Mu, so this is just Sigma inverse xi.",
    "start": "6104665",
    "end": "6109610"
  },
  {
    "text": "And this is the quadratic form. And that's gonna come with a 2.",
    "start": "6109710",
    "end": "6115840"
  },
  {
    "text": "The 2 and the 2 cancel, that's gonna be a minus 2- the 2 cancels,",
    "start": "6115840",
    "end": "6122349"
  },
  {
    "text": "sorry, Sigma inverse Mu.",
    "start": "6122349",
    "end": "6127520"
  },
  {
    "text": "Right. And then we set it equal to 0 and solve for Mu.",
    "start": "6127620",
    "end": "6134005"
  },
  {
    "text": "And this would basically give us that n times- because it's a summation over n,",
    "start": "6134005",
    "end": "6140020"
  },
  {
    "text": "n times Sigma inverse Mu equals summation I equals 1_n Sigma inverse xi.",
    "start": "6140020",
    "end": "6152410"
  },
  {
    "text": "And Sigma inverse you can- you can, uh, ah, take it out. Um, and I'm gonna continue it here.",
    "start": "6152410",
    "end": "6159625"
  },
  {
    "text": "And that- that will give us basically Sigma inverse Mu is equal to Sigma",
    "start": "6159625",
    "end": "6166269"
  },
  {
    "text": "inverse 1 over ni equals 1_n xi.",
    "start": "6166269",
    "end": "6175065"
  },
  {
    "text": "Right. And multiply it with Sigma on both sides, we get Mu equals 1 over n I equals 1_n xi.",
    "start": "6175065",
    "end": "6187220"
  },
  {
    "text": "Right? So Mu is just the average of the xs that are given to us.",
    "start": "6191340",
    "end": "6197290"
  },
  {
    "text": "It's- it's- it's a- it's a- it looks pretty tedious. It's something you want to, you know,",
    "start": "6197290",
    "end": "6204775"
  },
  {
    "text": "go back home and- and rederive it on your own just to make sure you're comfortable with it. And this is going to be probably the last time we're going to do",
    "start": "6204775",
    "end": "6212050"
  },
  {
    "text": "such detailed deri- derivations on board, right? Um, going forward, you know,",
    "start": "6212050",
    "end": "6217900"
  },
  {
    "text": "I'm just going to defer to you to verify the derivations, but this is the, um, um, essence.",
    "start": "6217900",
    "end": "6224200"
  },
  {
    "text": "Now, what about Sigma? [NOISE] We want to estimate um,",
    "start": "6224200",
    "end": "6232120"
  },
  {
    "text": "the parameter Sigma, but we see here that we have a Sigma inverse, right?",
    "start": "6232120",
    "end": "6238675"
  },
  {
    "text": "How do you take- you know, that makes it a little complex. Right? [BACKGROUND] Sorry, uh,",
    "start": "6238675",
    "end": "6246160"
  },
  {
    "text": "uh, what's the question? Is Sigma inverse the same as 1 by Sigma? Well, so the Sigma inverse is not the same as 1 by Sigma because Sigma is a matrix.",
    "start": "6246160",
    "end": "6253720"
  },
  {
    "text": "It- what is 1 over a matrix? There is no such thing as 1 over a matrix, right? Uh, you have a matrix inverse,",
    "start": "6253720",
    "end": "6259510"
  },
  {
    "text": "which is another matrix, right? [BACKGROUND] Right? So, uh, now we- for- for doing it with respect to Sigma,",
    "start": "6259510",
    "end": "6265885"
  },
  {
    "text": "we're going to use a change- of- variable trick, which means we are going to [NOISE] consider a variable S,",
    "start": "6265885",
    "end": "6273309"
  },
  {
    "text": "[NOISE] which is equal to Sigma inverse, and make the observation that",
    "start": "6273310",
    "end": "6281305"
  },
  {
    "text": "the- the gradient of sum function L with respect to S is equal to 0,",
    "start": "6281305",
    "end": "6288925"
  },
  {
    "text": "if and only if, gradient [NOISE] of- with respect to Sigma inverse of L equals 0.",
    "start": "6288925",
    "end": "6296150"
  },
  {
    "text": "Does it make sense? [NOISE] Right?",
    "start": "6297510",
    "end": "6303144"
  },
  {
    "text": "And then we're going to solve it for x- solve for S, and once we get the S, we're going to invert it,",
    "start": "6303145",
    "end": "6308485"
  },
  {
    "text": "and the inverse of that is going to be the Sigma that we want. Right? This is a- a- a pretty standard classical change of variable trick that you do with calculus.",
    "start": "6308485",
    "end": "6319300"
  },
  {
    "text": "Right? So this thing over here- so now we're going to take the derivative with",
    "start": "6319300",
    "end": "6326530"
  },
  {
    "text": "respect to S [NOISE] i equals 1 to n [NOISE] half log.",
    "start": "6326530",
    "end": "6334079"
  },
  {
    "text": "There's a minus, and um- so this will make it [NOISE] similar to S and minus",
    "start": "6334080",
    "end": "6340469"
  },
  {
    "text": "[NOISE] half x minus Mu transpose S, x minus Mu.",
    "start": "6340469",
    "end": "6348250"
  },
  {
    "text": "[NOISE] It's going to be x^i, [NOISE] x^i.",
    "start": "6348250",
    "end": "6353730"
  },
  {
    "text": "Right? And the derivative of a log determinant with respect to the matrix is?",
    "start": "6353730",
    "end": "6360130"
  },
  {
    "text": "[BACKGROUND] S inverse, S inverse. It's not 1 by S, 1 over S, it is S inverse.",
    "start": "6360130",
    "end": "6366460"
  },
  {
    "text": "Um, and we're going to, um, sum over n, such things.",
    "start": "6366460",
    "end": "6372445"
  },
  {
    "text": "So that is going to be n times S inverse and there's a half outside,",
    "start": "6372445",
    "end": "6379179"
  },
  {
    "text": "[NOISE] so- minus- what's the- what's the-",
    "start": "6379180",
    "end": "6386140"
  },
  {
    "text": "[inaudible] with respect to A of x transpose A_x.",
    "start": "6386140",
    "end": "6393595"
  },
  {
    "text": "So we saw what it is with respect to S, what is it with respect to A?",
    "start": "6393595",
    "end": "6397790"
  },
  {
    "text": "I'm going to leave it as an exercise and you will have to verify that [NOISE] this is just x x transpose.",
    "start": "6398670",
    "end": "6406000"
  },
  {
    "text": "[NOISE] Right? [BACKGROUND] Yes, question?",
    "start": "6406000",
    "end": "6411160"
  },
  {
    "text": "[BACKGROUND]",
    "start": "6411160",
    "end": "6422590"
  },
  {
    "text": "So I took a half- as outside is there another negative? There's probably another negative. [BACKGROUND]",
    "start": "6422590",
    "end": "6437380"
  },
  {
    "text": "Oh, yeah.\nSo that's where the negative comes in so that the- the log- so log of 1 over something becomes the negative of the log of something.",
    "start": "6437380",
    "end": "6445480"
  },
  {
    "text": "[BACKGROUND] Right?",
    "start": "6445480",
    "end": "6450550"
  },
  {
    "text": "So that's where the- the- the- the negative got canceled out. [BACKGROUND] I'm sorry. [BACKGROUND] I mean, there's one,",
    "start": "6450550",
    "end": "6458610"
  },
  {
    "text": "um, but this should be- yeah, that makes sense.",
    "start": "6458610",
    "end": "6465060"
  },
  {
    "text": "Sorry. And minus sum i equals 1 to n,",
    "start": "6465060",
    "end": "6471580"
  },
  {
    "text": "x^i minus Mu, x^i minus Mu transpose.",
    "start": "6471740",
    "end": "6480140"
  },
  {
    "text": "Which means S, and- and you set this equal to 0,",
    "start": "6480140",
    "end": "6485710"
  },
  {
    "text": "the half cancels and you get S inverse equals 1 over n,",
    "start": "6485710",
    "end": "6492145"
  },
  {
    "text": "i equals 1 to n, x^i minus Mu, x^i minus Mu transpose.",
    "start": "6492145",
    "end": "6502850"
  },
  {
    "text": "And S inverse is equal to Sigma.",
    "start": "6503280",
    "end": "6509300"
  },
  {
    "text": "Right? So Sigma is 1 over n, x^i minus Mu x^i minus Mu transpose.",
    "start": "6509610",
    "end": "6517405"
  },
  {
    "text": "And this, you know, it's- it's similar to expectation of",
    "start": "6517405",
    "end": "6522625"
  },
  {
    "text": "x minus expectation of x square, right?",
    "start": "6522625",
    "end": "6529645"
  },
  {
    "text": "You can kind of see the similar, exactly. And- and that's- that's your covariance matrix. Yes, question.",
    "start": "6529645",
    "end": "6535930"
  },
  {
    "text": "[BACKGROUND]",
    "start": "6535930",
    "end": "6552940"
  },
  {
    "text": "There is a minus that got- that absorbed the 1 over s s denominator. [BACKGROUND] Yeah, right?",
    "start": "6552940",
    "end": "6560094"
  },
  {
    "text": "So anyway, so if you have questions about this, you know, come up to stage after the lecture. But this is- this is- you know,",
    "start": "6560095",
    "end": "6566125"
  },
  {
    "text": "this gives you a flavor of how to do maximum likelihood estimation, right? And the- the- you know, um,",
    "start": "6566125",
    "end": "6572440"
  },
  {
    "text": "you're expected to be comfortable to do this kind of matrix calculus to derive maximum likelihood estimates, right?",
    "start": "6572440",
    "end": "6579130"
  },
  {
    "text": "And- and we're going to be doing a whole lot of such matrix calculus, you know, throughout the course for different kinds of models.",
    "start": "6579130",
    "end": "6585355"
  },
  {
    "text": "And this is just, uh, a simple model, a multivariate Gaussian, where, uh, you know, uh, where this gets used.",
    "start": "6585355",
    "end": "6592225"
  },
  {
    "text": "Now, what are we going to be, uh, uh, doing for linear regression? Uh, I don't think we have time for linear regression,",
    "start": "6592225",
    "end": "6597670"
  },
  {
    "text": "but maybe, you know, I'll just give you a- a- a flavor of what linear regression is about. So for linear regression,",
    "start": "6597670",
    "end": "6604285"
  },
  {
    "text": "it's a supervised learning problem. And a supervised learning problem is a problem in which we want to learn relations between x and y.",
    "start": "6604285",
    "end": "6613480"
  },
  {
    "text": "So we are going to be given training sets in the form of x^i, y^i pairs, right?",
    "start": "6613480",
    "end": "6624940"
  },
  {
    "text": "You're going to be given pairs of x^i, y^i, where x^i is [NOISE] some kind of a vector- a d-dimensional vector.",
    "start": "6624940",
    "end": "6635515"
  },
  {
    "text": "And we're going to be given n such pairs right?",
    "start": "6635515",
    "end": "6641620"
  },
  {
    "text": "So this terminology means this is- it's- it's- it's a pair of x, um, um.",
    "start": "6641620",
    "end": "6647230"
  },
  {
    "text": "x^i and y^i, and you have, you know, i from 1 to n, and you have n such pairs, right? That's- that's the terminology we use over here.",
    "start": "6647230",
    "end": "6653844"
  },
  {
    "text": "And y [NOISE] is going to be in R and you want to learn a function, right?",
    "start": "6653845",
    "end": "6659950"
  },
  {
    "text": "We're going to call that function as [NOISE] h of x. And we want h of x to output y, right?",
    "start": "6659950",
    "end": "6668005"
  },
  {
    "text": "And this h is going to be parameterized by some Theta, and in this case,",
    "start": "6668005",
    "end": "6673780"
  },
  {
    "text": "Theta was Mu and- and Sigma over here, Theta is going to be something else that we're going to see.",
    "start": "6673780",
    "end": "6680230"
  },
  {
    "text": "And from the tra- given training, um, uh, data set we want to learn, the function h Theta of x,",
    "start": "6680230",
    "end": "6687370"
  },
  {
    "text": "which means you are given a training set. [NOISE] Right?",
    "start": "6687370",
    "end": "6694809"
  },
  {
    "text": "From this training set, you're going to construct h Theta.",
    "start": "6694810",
    "end": "6701275"
  },
  {
    "text": "So this is called the learning process, the learning algorithm, right?",
    "start": "6701275",
    "end": "6711400"
  },
  {
    "text": "This was x, y pairs, and the output of the learning process is an algorithm, right?",
    "start": "6711400",
    "end": "6719199"
  },
  {
    "text": "So the h Theta is the output of- of the learning algorithm itself. And into h Theta we're going to feed new x- let's call it x star unseen xs.",
    "start": "6719200",
    "end": "6729475"
  },
  {
    "text": "And it's supposed to output ys for the unseen xs. Thi- thi- this is like the- the- the macro setting of regression problems,",
    "start": "6729475",
    "end": "6738870"
  },
  {
    "text": "or supervised learning problems. And in the next lecture, we're going to, uh, um see a couple of algorithms for- for, uh,",
    "start": "6738870",
    "end": "6748179"
  },
  {
    "text": "uh, learning linear relationships between x and y, and that's going to be, uh, linear regression.",
    "start": "6748180",
    "end": "6754280"
  }
]