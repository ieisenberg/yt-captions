[
  {
    "text": "thanks a lot for inviting me over and thanks everybody for coming it's always exciting to",
    "start": "11180",
    "end": "17630"
  },
  {
    "text": "yeah and it's fun this is Ian video type auditorium great match because I'm",
    "start": "17630",
    "end": "22890"
  },
  {
    "text": "mostly going to talk about stuff that we've actually done at the Nvidia lab over the last year's just a quick",
    "start": "22890",
    "end": "29369"
  },
  {
    "text": "background I start at two and a half years ago first with an 80% position at",
    "start": "29369",
    "end": "35370"
  },
  {
    "text": "Nvidia starting a robotics lab and now I'm currently on a half half an half position with the universities so back",
    "start": "35370",
    "end": "41790"
  },
  {
    "text": "back to teaching as well and this all came about when I had a meeting with with Chancellor Wong the CEO of Nvidia",
    "start": "41790",
    "end": "48899"
  },
  {
    "text": "talking about robotics and what are the the hot areas where this new kind of",
    "start": "48899",
    "end": "55019"
  },
  {
    "text": "robotics really has an impact on industrial settings and what's of course already happening right now is really",
    "start": "55019",
    "end": "60960"
  },
  {
    "text": "robots in warehouses moving around navigating through hospitals hotels of",
    "start": "60960",
    "end": "67470"
  },
  {
    "text": "course of driving cars it's the big elephant in the room always but what we",
    "start": "67470",
    "end": "73500"
  },
  {
    "text": "wanted to look at in the research side is really robots that are not just about navigating around but robots that",
    "start": "73500",
    "end": "79640"
  },
  {
    "text": "physically interact with the environment so if you match in self-driving cars of course the main focus is not to",
    "start": "79640",
    "end": "85709"
  },
  {
    "text": "physically interact with your environment maybe with the tires but otherwise you don't want to crash into anything so if you want to have a look",
    "start": "85709",
    "end": "94319"
  },
  {
    "text": "at these kind of robots and it's really about interacting with the environment touching things doing this in close",
    "start": "94319",
    "end": "101340"
  },
  {
    "text": "proximity to people around you and application domains of these kind of robots is of course industrial",
    "start": "101340",
    "end": "109140"
  },
  {
    "text": "manufacturing and you've heard some recent exciting news about where these",
    "start": "109140",
    "end": "114420"
  },
  {
    "text": "kind of deep learning technologies for example I have an impact on manipulation settings in industry but also people",
    "start": "114420",
    "end": "122459"
  },
  {
    "text": "with physical disabilities a colleague at Georgia Tech Charlie Kemp is investigating for example how robots can",
    "start": "122459",
    "end": "128220"
  },
  {
    "text": "help people with physical disabilities here in a case even where the robot might help shaving the person of pulling",
    "start": "128220",
    "end": "134970"
  },
  {
    "text": "up a blanket when the purse is lying in the bed so you can imagine that in these cases the robot really has to kind of be",
    "start": "134970",
    "end": "140400"
  },
  {
    "text": "in contact with a person and all of this has to be safe but at the same time the robot has to have kind of the perception",
    "start": "140400",
    "end": "146610"
  },
  {
    "text": "capabilities to reason about what is going on around right any general helping people elderly",
    "start": "146610",
    "end": "153069"
  },
  {
    "text": "people in hospitals so this is kind of the goal for this kind of work and we",
    "start": "153069",
    "end": "158140"
  },
  {
    "text": "realize of course there's still a lot of research to be done or to do this so we started the robotics lab in Seattle two",
    "start": "158140",
    "end": "165370"
  },
  {
    "text": "and a half years ago we're currently 12 research scientists we have over the",
    "start": "165370",
    "end": "170500"
  },
  {
    "text": "summer for example we had 20 interns last year several of them are here in the room from Stanford it was really fun",
    "start": "170500",
    "end": "176860"
  },
  {
    "text": "working with with all these smart people what I want to talk about today though",
    "start": "176860",
    "end": "182290"
  },
  {
    "text": "is one project that we started in the lab and that is about getting a robot to",
    "start": "182290",
    "end": "189010"
  },
  {
    "text": "do certain tasks in a kitchen and we're clearly not doing cooking yet but we",
    "start": "189010",
    "end": "194110"
  },
  {
    "text": "chose kitchen kind of s and integrate a test environment right in which we can",
    "start": "194110",
    "end": "199320"
  },
  {
    "text": "look at different aspects of this manipulation problem or puzzle right",
    "start": "199320",
    "end": "204630"
  },
  {
    "text": "this is the kitchen we have the Nakia kitchen just bought down the road we also have a simulation model of that and",
    "start": "204630",
    "end": "211620"
  },
  {
    "text": "the idea for the kitchen is really that again we can integrate different manipulation components into it we can",
    "start": "211620",
    "end": "218440"
  },
  {
    "text": "do task planning motion planning and things like that but also you can make it in a staged way more and more complex",
    "start": "218440",
    "end": "224680"
  },
  {
    "text": "right so initially actually as you'll see today it's mostly very simple tasks like just pick up an object and move it",
    "start": "224680",
    "end": "230980"
  },
  {
    "text": "somewhere but then on the longer run you can make it more and more complicated by introducing people into the kitchen so",
    "start": "230980",
    "end": "237640"
  },
  {
    "text": "the robot has to start interacting with them has to understand what the person wants to do and maybe help the person",
    "start": "237640",
    "end": "244150"
  },
  {
    "text": "cooking and even maybe helping with preparing ingredients or cooking ultimately itself but the idea is that",
    "start": "244150",
    "end": "250750"
  },
  {
    "text": "many of these tasks really just stand in for many of these other application events you could imagine for example",
    "start": "250750",
    "end": "256120"
  },
  {
    "text": "also in industrial manufacturing the specific thing we wanted to look at",
    "start": "256120",
    "end": "261669"
  },
  {
    "text": "initially it's really that let's assume we have this kitchen here and we make",
    "start": "261669",
    "end": "268120"
  },
  {
    "text": "the very strong assumption that we have a 3d model of the kitchen which means we know where the drawers are we know where the doors are and we even have 3d models",
    "start": "268120",
    "end": "275320"
  },
  {
    "text": "of the objects you might have seen the famous spam can is kind of from the ycb",
    "start": "275320",
    "end": "280660"
  },
  {
    "text": "object data set I'm kind of tire of it but just looking at it not even eating I haven't eaten any of them",
    "start": "280660",
    "end": "287100"
  },
  {
    "text": "but also yeah the cheesy box these kind of objects so the idea was under these",
    "start": "287100",
    "end": "292180"
  },
  {
    "text": "super-strong assumptions can we build a system that can do let's say reasonably simple pick-and-place tasks like just",
    "start": "292180",
    "end": "299320"
  },
  {
    "text": "opening a drawer and getting an object out of out of it on the perception side",
    "start": "299320",
    "end": "305430"
  },
  {
    "text": "we use actually in this case just because we didn't want me to put actually depth camera on the robot",
    "start": "305430",
    "end": "312100"
  },
  {
    "text": "itself we have like two depth cameras and looking at the kitchen and then the idea is use them to to track what's",
    "start": "312100",
    "end": "318940"
  },
  {
    "text": "going on in there okay the robot base is sakata that is used by nvidia by the",
    "start": "318940",
    "end": "325810"
  },
  {
    "text": "Issac SDK team that is focusing so for more on navigation system building a navigation stack also with very strong",
    "start": "325810",
    "end": "333340"
  },
  {
    "text": "support of course for deep learning and GPU processing and we put a franca arm on top and for some of the more recent",
    "start": "333340",
    "end": "340090"
  },
  {
    "text": "work also we could we put a depth camera onto the wrist of the robot so that you can get close up if you want to pick up",
    "start": "340090",
    "end": "346180"
  },
  {
    "text": "objects so if you think about what's involved in such a manipulation task",
    "start": "346180",
    "end": "351190"
  },
  {
    "text": "then on the high level EF is what people call task and motion planning so a sub part of their Waskom is focusing on this",
    "start": "351190",
    "end": "357639"
  },
  {
    "text": "where you say you know where objects are for example in the world you have the geometric models of everything and then",
    "start": "357639",
    "end": "364180"
  },
  {
    "text": "you wanna for example let's say if I set the table then you say the target configuration is that the plate in the",
    "start": "364180",
    "end": "370450"
  },
  {
    "text": "fork a knife on the table and then you have a high level let's say more logic",
    "start": "370450",
    "end": "375669"
  },
  {
    "text": "planning based descriptions or where you say well for the table for example if the table is in the drawer in order for",
    "start": "375669",
    "end": "382389"
  },
  {
    "text": "the robot to be able to pick up the plate not not the table if the plate is in the drawer for the robot to get the",
    "start": "382389",
    "end": "389320"
  },
  {
    "text": "plate or the drawer the drawer has to be open in things like that so you have preconditions on actions and then you",
    "start": "389320",
    "end": "394330"
  },
  {
    "text": "can chain these and can come up with a plan that achieves a desired configuration okay this is tasks in",
    "start": "394330",
    "end": "400690"
  },
  {
    "text": "motion planning and what you do jointly with this high-level planning is also",
    "start": "400690",
    "end": "405940"
  },
  {
    "text": "the actual motion planning for the robot so that it can achieve these tasks and",
    "start": "405940",
    "end": "411370"
  },
  {
    "text": "the other thing of course if you want to execute it in the real world you need state estimation you need to know actually what is the status of the world where",
    "start": "411370",
    "end": "417760"
  },
  {
    "text": "the objects are and I'm going to talk about that a bit more then you need to be able to grasp and place objects and",
    "start": "417760",
    "end": "424360"
  },
  {
    "text": "again here we take advantage of the fact that we have 3d models of objects and we can more or less pre specify all these",
    "start": "424360",
    "end": "431530"
  },
  {
    "text": "different components and then the final ingredient is then on the manipulator itself you need a controller that",
    "start": "431530",
    "end": "437590"
  },
  {
    "text": "actually executes all these planned motions ok so in this talk I just want to go through some of these these areas and",
    "start": "437590",
    "end": "444820"
  },
  {
    "text": "give you an example for the kind of work we've done in in debt and then later I'll give one more example on how we",
    "start": "444820",
    "end": "451150"
  },
  {
    "text": "expand the grasping to unknown objects ok so the first part is we've done a lot",
    "start": "451150",
    "end": "457270"
  },
  {
    "text": "of work on 60 object post estimation and I know this is some really cool and exciting work of course going on here as",
    "start": "457270",
    "end": "463570"
  },
  {
    "text": "well so we're kind of working ourselves a long side to this problem and the idea is of",
    "start": "463570",
    "end": "468850"
  },
  {
    "text": "course if you have an object and you want to manipulate it let's say this is our famous cheesy box and you have a 3d",
    "start": "468850",
    "end": "476020"
  },
  {
    "text": "model of the object and you want the robot for example to pick it up then one",
    "start": "476020",
    "end": "481450"
  },
  {
    "text": "approaches that you want to estimate where it is relative to the robot ok so the task is you get an image and the",
    "start": "481450",
    "end": "489190"
  },
  {
    "text": "option is in the image and you want to know for example in that specific image what is the 3d position and 3d",
    "start": "489190",
    "end": "495130"
  },
  {
    "text": "orientation of the object that's why it's a 60 estimation problem okay so it's orientation and translation and",
    "start": "495130",
    "end": "502020"
  },
  {
    "text": "this of course a lot of work has been going on in the computer vision community especially been focusing on",
    "start": "502020",
    "end": "508540"
  },
  {
    "text": "this problem for quite a while there's many standard data sets on how to evaluate your technique a lot of the",
    "start": "508540",
    "end": "515020"
  },
  {
    "text": "focus has actually been on kind of single image estimation which means you get a single image of the scene and then",
    "start": "515020",
    "end": "521830"
  },
  {
    "text": "you immediately go to an estimate for the 60 pose of the object and again a",
    "start": "521830",
    "end": "528220"
  },
  {
    "text": "lot of really cool work coming out of out of Stanford here and the typical",
    "start": "528220",
    "end": "533470"
  },
  {
    "text": "approach most recently has also been for example on just regressing more or less the image to a 60 pose estimate right",
    "start": "533470",
    "end": "540010"
  },
  {
    "text": "using deep learning and using large-scale training data sets the focus of these is",
    "start": "540010",
    "end": "547090"
  },
  {
    "text": "also that you really want to regress the process to a unique post of the robot and it",
    "start": "547090",
    "end": "552640"
  },
  {
    "text": "turns out if you have symmetric objects then the question is whether that even makes sense whether whether they say",
    "start": "552640",
    "end": "557680"
  },
  {
    "text": "uniquely defined 60 posts and I'll talk about it on the next slide a bit more and then there's other work on trekking",
    "start": "557680",
    "end": "564280"
  },
  {
    "text": "but again there it's also just kind of a unimodal estimate Kalman filter style and there's one exception where they use",
    "start": "564280",
    "end": "570880"
  },
  {
    "text": "the particle filter but it turns out that if you just use a standard particle filter for sampling this full 60 space",
    "start": "570880",
    "end": "577300"
  },
  {
    "text": "then it also boils down more or less to just being able to do tracking because the 60 post space of these objects is",
    "start": "577300",
    "end": "584320"
  },
  {
    "text": "pretty complicated as I'll show you next so what we wanted to focus on is really",
    "start": "584320",
    "end": "589990"
  },
  {
    "text": "estimating now the full 60 post distribution and uncertainty and doing",
    "start": "589990",
    "end": "595690"
  },
  {
    "text": "that even for symmetric objects okay so why is orientation uncertainty actually",
    "start": "595690",
    "end": "602800"
  },
  {
    "text": "not as simple as you might think let's imagine this is an object like tunic and",
    "start": "602800",
    "end": "608680"
  },
  {
    "text": "I think in this case and this is the only observation you get right so now",
    "start": "608680",
    "end": "614920"
  },
  {
    "text": "the question is what is the orientation of the of the object or the the",
    "start": "614920",
    "end": "619990"
  },
  {
    "text": "equivalent question is what is the camera viewpoint onto that object given",
    "start": "619990",
    "end": "625480"
  },
  {
    "text": "that image okay and it turns out of course that you have quite some",
    "start": "625480",
    "end": "630790"
  },
  {
    "text": "uncertainty in the orientation that you cannot resolve based on that single image because you could be anywhere the",
    "start": "630790",
    "end": "637120"
  },
  {
    "text": "camera could be any way around the object right or it could even be in this case if it's symmetric you could kind of",
    "start": "637120",
    "end": "642280"
  },
  {
    "text": "view the object from the bottom part now you might say that that is just because",
    "start": "642280",
    "end": "649630"
  },
  {
    "text": "people are using depth cameras and they don't know how to use texture and visual",
    "start": "649630",
    "end": "654790"
  },
  {
    "text": "information so if you now use a textured model of the object and you also use",
    "start": "654790",
    "end": "660490"
  },
  {
    "text": "color then you might be able to actually resolve that uncertainty and you have",
    "start": "660490",
    "end": "666220"
  },
  {
    "text": "kind of a unimodal distribution of where you are relative to the object does it make sense okay so on the one hand side",
    "start": "666220",
    "end": "673660"
  },
  {
    "text": "of course color helps you resolve some uncertainties but then even in this case even if you have that object model if",
    "start": "673660",
    "end": "680950"
  },
  {
    "text": "you're looking at some point at the object from the then even in this situation you still",
    "start": "680950",
    "end": "686680"
  },
  {
    "text": "don't know where your are relative to it even if you use if you have a textured object right so to summarize this part",
    "start": "686680",
    "end": "694660"
  },
  {
    "text": "is really that on the one hand side these symmetries result in pretty complex actually distributions and you",
    "start": "694660",
    "end": "701590"
  },
  {
    "text": "cannot predefined actually the shape of these distributions in advance because it often depends on your specific",
    "start": "701590",
    "end": "708430"
  },
  {
    "text": "viewpoint and it depends on certain occlusions you have in the scene right you might have specific uncertainties",
    "start": "708430",
    "end": "714820"
  },
  {
    "text": "just because a specific part of an object is occluded and because of that you can't resolve it so what we wanted",
    "start": "714820",
    "end": "721360"
  },
  {
    "text": "to do is we want to do 60 pulse estimation where we keep track of these full distributions here the first",
    "start": "721360",
    "end": "729460"
  },
  {
    "text": "approach here is and that is actually borrowing the idea of Santa Maya and in",
    "start": "729460",
    "end": "735220"
  },
  {
    "text": "colleagues they had there was actually best paper at ECC V in 2018 and they",
    "start": "735220",
    "end": "742090"
  },
  {
    "text": "kind of their focus was specifically on orientation uncertainty over objects",
    "start": "742090",
    "end": "747700"
  },
  {
    "text": "okay I just want to give you a brief summary on how that works imagine you have this object and what they're learning is kind",
    "start": "747700",
    "end": "754990"
  },
  {
    "text": "of a a code book for certain views onto",
    "start": "754990",
    "end": "760060"
  },
  {
    "text": "this object so the idea is that you have an auto encoder and you train this with",
    "start": "760060",
    "end": "767020"
  },
  {
    "text": "pairs of input images and output images okay so the goal of the auto encoder is",
    "start": "767020",
    "end": "772030"
  },
  {
    "text": "to take a noisy image of the object a view onto the object from a let's say",
    "start": "772030",
    "end": "777150"
  },
  {
    "text": "normalized distance to the object and it has to learn to encode this in this case",
    "start": "777150",
    "end": "784060"
  },
  {
    "text": "at 128 dimensional codeword draw such that you can decode it into a clean",
    "start": "784060",
    "end": "790030"
  },
  {
    "text": "image of the object okay and you do this with many pairs right different",
    "start": "790030",
    "end": "797560"
  },
  {
    "text": "viewpoints and in the sense what the network learns is to kind of take input",
    "start": "797560",
    "end": "802750"
  },
  {
    "text": "or not noise the image and encode it such that you can specifically also recover the viewpoint onto the object I",
    "start": "802750",
    "end": "810330"
  },
  {
    "text": "hope that makes sense and again you do this for many pairs and",
    "start": "810330",
    "end": "816389"
  },
  {
    "text": "once it's trained you can now go and in their case",
    "start": "816389",
    "end": "821439"
  },
  {
    "text": "you just can't generate ninety two thousand something views different",
    "start": "821439",
    "end": "827559"
  },
  {
    "text": "viewpoints onto the object and for each viewpoint you just store the code that",
    "start": "827559",
    "end": "833980"
  },
  {
    "text": "the encoder generates for that view okay and now during testing if someone",
    "start": "833980",
    "end": "841179"
  },
  {
    "text": "gives you an image of the object you throw away the decoder because actually you don't need that you encode that and",
    "start": "841179",
    "end": "847240"
  },
  {
    "text": "then what you do is you just compare the code word with your codebook use",
    "start": "847240",
    "end": "853389"
  },
  {
    "text": "adjusted cosine similarity measure then that similarity tells you which view from which view you're looking at the",
    "start": "853389",
    "end": "859779"
  },
  {
    "text": "object yeah and then what they did in their work it just shows the most",
    "start": "859779",
    "end": "867100"
  },
  {
    "text": "similar and said like okay that is the orientation of the object okay the nice thing is if you have let's say a",
    "start": "867100",
    "end": "873040"
  },
  {
    "text": "symmetric object like this one that if you look at it from here then it will",
    "start": "873040",
    "end": "878649"
  },
  {
    "text": "match very well every view that is around this object that has the same that looks the same right so it very",
    "start": "878649",
    "end": "885579"
  },
  {
    "text": "naturally recovers or encodes this notion of that objects might look the same from different orientations great",
    "start": "885579",
    "end": "893559"
  },
  {
    "text": "the similar the the limitations of that work is that if they only really cared",
    "start": "893559",
    "end": "898959"
  },
  {
    "text": "about getting the single orientation out of it and they didn't worry about different translations of the objects",
    "start": "898959",
    "end": "904809"
  },
  {
    "text": "and also it was just a single image in a single estimate but this is kind of the foundation for what we did then where we",
    "start": "904809",
    "end": "912279"
  },
  {
    "text": "said we want to incorporate that into a full 60 post tracking framework and for",
    "start": "912279",
    "end": "920829"
  },
  {
    "text": "that we developed this technique called post our BPF our vp f stands for raw black rice particle filtering so we",
    "start": "920829",
    "end": "927730"
  },
  {
    "text": "thought we're gonna bring in some good old particle filtering into here where the ideas particle filtering is you'd um",
    "start": "927730",
    "end": "933759"
  },
  {
    "text": "represent densities or uncertainties by samples that represent then the the high",
    "start": "933759",
    "end": "942189"
  },
  {
    "text": "density areas by having most samples in that area and the idea of",
    "start": "942189",
    "end": "947779"
  },
  {
    "text": "this specific route black oval eyes particle filters this is just rather than saying for example I'm going to sample this full 60 post space what I'm",
    "start": "947779",
    "end": "955879"
  },
  {
    "text": "going to do is I'm going to sample certain parts of the state space and others I'm gonna solve analytically",
    "start": "955879",
    "end": "961389"
  },
  {
    "text": "conditioned on the samples okay people have used this in different contexts like activity recognition you",
    "start": "961389",
    "end": "968209"
  },
  {
    "text": "might have heard here from stanford sebastian front of nicola they introduced fast slam where what they",
    "start": "968209",
    "end": "973759"
  },
  {
    "text": "they decouple the slam problem by sampling trajectories of the robot and for each sample trajectory they would",
    "start": "973759",
    "end": "980869"
  },
  {
    "text": "then build a map right so now you have a sample based representation that encapsulate both uncertainty of the path",
    "start": "980869",
    "end": "987290"
  },
  {
    "text": "and uncertainty of the map that corresponds to that in our case we use the same idea but here what we say is",
    "start": "987290",
    "end": "994309"
  },
  {
    "text": "we're gonna sample the translation of the object which is XYZ and we're gonna solve for the orientation using just a",
    "start": "994309",
    "end": "1001839"
  },
  {
    "text": "discrete distribution so here's how this works each particle now has a",
    "start": "1001839",
    "end": "1009369"
  },
  {
    "text": "translation again XYZ where's the object and the full distribution over the rotation conditioned on the translation",
    "start": "1009369",
    "end": "1016869"
  },
  {
    "text": "of that object okay so what that means is we first can take an image like this",
    "start": "1016869",
    "end": "1023739"
  },
  {
    "text": "one here and if we have a particle that has a specific translation that will",
    "start": "1023739",
    "end": "1029558"
  },
  {
    "text": "give us a specific bounding box for the object in the image if the translation",
    "start": "1029559",
    "end": "1035380"
  },
  {
    "text": "says that the object is further away then the bounding box will be smaller for example things like that right and it moves throughout the image and then",
    "start": "1035380",
    "end": "1042788"
  },
  {
    "text": "we can feed this region of interest that corresponds to that single particle we",
    "start": "1042789",
    "end": "1048159"
  },
  {
    "text": "can encode it and compare it to the same code book that you saw by the Sunnah",
    "start": "1048159",
    "end": "1055600"
  },
  {
    "text": "Meyer work in our case we have even more bins for the annotation you 191,000",
    "start": "1055600",
    "end": "1061330"
  },
  {
    "text": "which is 191,000 orientations okay and then that gives you the likelihood and",
    "start": "1061330",
    "end": "1068169"
  },
  {
    "text": "that likelihood is what you use to weight your particles and then you do over time you can do the resampling and",
    "start": "1068169",
    "end": "1074980"
  },
  {
    "text": "very standard party filter techniques right there's nothing special about it the only insight was",
    "start": "1074980",
    "end": "1080220"
  },
  {
    "text": "here really to say we do the seemingly crazy thing of having every particle",
    "start": "1080220",
    "end": "1085230"
  },
  {
    "text": "with like these almost two hundred thousand bins that represent the possible orientations and we can do this",
    "start": "1085230",
    "end": "1091379"
  },
  {
    "text": "of course efficiently because we only need to compute the code once per",
    "start": "1091379",
    "end": "1096539"
  },
  {
    "text": "particle and then the comparison with the codebook that's of course on a GPU you can do this all in parallel so then",
    "start": "1096539",
    "end": "1103259"
  },
  {
    "text": "why we can do all of this then in real time okay let me just show you an example on the right side you see the",
    "start": "1103259",
    "end": "1111419"
  },
  {
    "text": "particle do the kind of the orientation uncertainty and these are the different",
    "start": "1111419",
    "end": "1119039"
  },
  {
    "text": "bounding boxes of the different particles okay and what's interesting",
    "start": "1119039",
    "end": "1124529"
  },
  {
    "text": "here is that initially when the handle of the mark let me play this again when",
    "start": "1124529",
    "end": "1130919"
  },
  {
    "text": "the handle of the mug is not visible the uncertainty is actually much larger in the orientation right because you don't",
    "start": "1130919",
    "end": "1137730"
  },
  {
    "text": "know where it is exactly but then when the handle comes into view the uncertainty really goes down",
    "start": "1137730",
    "end": "1143789"
  },
  {
    "text": "significantly okay so it really nicely handles all these kind of different",
    "start": "1143789",
    "end": "1148889"
  },
  {
    "text": "viewpoints another thing of course you can do now is even though this is kind of a technique that is only based on",
    "start": "1148889",
    "end": "1155220"
  },
  {
    "text": "kind of this let's say local region of image of interest images you can't just do global localization which means you",
    "start": "1155220",
    "end": "1161850"
  },
  {
    "text": "can do detection with this technique as well where what you do is you just randomly throw out how many five",
    "start": "1161850",
    "end": "1169139"
  },
  {
    "text": "thousand particles this is these are the bounding boxes of these five thousand particles and because we don't need to",
    "start": "1169139",
    "end": "1175950"
  },
  {
    "text": "sample the orientation we do it analytically again through this full distribution it very quickly actually",
    "start": "1175950",
    "end": "1182460"
  },
  {
    "text": "converges then to the right pose and then we can even reduce the number of particles and then track the object",
    "start": "1182460",
    "end": "1188609"
  },
  {
    "text": "through time okay and again this can be",
    "start": "1188609",
    "end": "1193619"
  },
  {
    "text": "done pretty faster with at the end the tracking fifty particles was twenty frames per second and now I think",
    "start": "1193619",
    "end": "1199080"
  },
  {
    "text": "gingka this wasn't Gingka did this as an intern he has it to more than 30 frames",
    "start": "1199080",
    "end": "1204450"
  },
  {
    "text": "per second actually if they're tracking okay so now what we have is we can estimate",
    "start": "1204450",
    "end": "1211600"
  },
  {
    "text": "the poses of these individual objects but of course in this let's say in this kitchen setting you also want to",
    "start": "1211600",
    "end": "1217990"
  },
  {
    "text": "estimate where the doors the drawers are and things like that for that we use a technique that my student Tanishq it",
    "start": "1217990",
    "end": "1224200"
  },
  {
    "text": "introduced a while ago it's called dart thence articulated real-time tracking and the idea here is that if you have a",
    "start": "1224200",
    "end": "1231190"
  },
  {
    "text": "3d shape model of an articulated object could be a robotic manipulator could",
    "start": "1231190",
    "end": "1236500"
  },
  {
    "text": "also be furniture or things like that then it's a very fast again GPU based implementation of if you have a depth",
    "start": "1236500",
    "end": "1243760"
  },
  {
    "text": "based observation of a scene you can then match your depth information against the model and estimate the",
    "start": "1243760",
    "end": "1249850"
  },
  {
    "text": "articulation okay so here the problem is that the estimation is not just let's",
    "start": "1249850",
    "end": "1255520"
  },
  {
    "text": "say a 60 pose of a hand but we also need to estimate the joints of the hand as well okay and the nice thing is in this",
    "start": "1255520",
    "end": "1264160"
  },
  {
    "text": "case since it's just purely model-based it's very general so it can be applied to any of these settings okay so you can",
    "start": "1264160",
    "end": "1272590"
  },
  {
    "text": "see the TREC model alongside with the point load and we've done it also for some robot manipulation tasks at that",
    "start": "1272590",
    "end": "1279250"
  },
  {
    "text": "time already the key limitation is it needs to be initialized right because",
    "start": "1279250",
    "end": "1286420"
  },
  {
    "text": "the search space is too high dimensional and also that it's reasonably brittle",
    "start": "1286420",
    "end": "1291880"
  },
  {
    "text": "which means if for example your model doesn't match well if you don't have a correct model then the tracking might fail or if tracking fails reinitialize",
    "start": "1291880",
    "end": "1299470"
  },
  {
    "text": "ation is also not that easy so actually nowadays of course also you can do the",
    "start": "1299470",
    "end": "1304900"
  },
  {
    "text": "follow-up for many of these tasks you can do it of course through just deep learning if you have enough training",
    "start": "1304900",
    "end": "1309940"
  },
  {
    "text": "data okay but once we have that framework now we can kind of apply to",
    "start": "1309940",
    "end": "1315460"
  },
  {
    "text": "the whole kitchen where what you see here is kind of this fuzzy stuff is",
    "start": "1315460",
    "end": "1321400"
  },
  {
    "text": "going to be the point cloud that we see from the depth camera data and the coloring indicates what object the",
    "start": "1321400",
    "end": "1328510"
  },
  {
    "text": "different points are associated with and what we can do is we can use the technique that I just told you to get",
    "start": "1328510",
    "end": "1334630"
  },
  {
    "text": "the initial pose of objects and here we take just the most likely orientation and then we can use this to initialize",
    "start": "1334630",
    "end": "1341530"
  },
  {
    "text": "start so we're running dart to track these individual objects and we're",
    "start": "1341530",
    "end": "1346540"
  },
  {
    "text": "running dart to track also where the robot is in the scene and also to track the status of the furniture kind of of",
    "start": "1346540",
    "end": "1354220"
  },
  {
    "text": "the cabinets so you'll see here on the upper aides I might not be perfectly time synth but you can see the ball but",
    "start": "1354220",
    "end": "1360880"
  },
  {
    "text": "now moving in and we're using now the",
    "start": "1360880",
    "end": "1367750"
  },
  {
    "text": "dart estimates to tell the manipulator where it should move in order to pick up",
    "start": "1367750",
    "end": "1372820"
  },
  {
    "text": "the object yeah this one is a bit",
    "start": "1372820",
    "end": "1380650"
  },
  {
    "text": "lagging behind I'm sorry and again this only works of course if",
    "start": "1380650",
    "end": "1386680"
  },
  {
    "text": "you have a good model off of that scene right but we can do all of this now",
    "start": "1386680",
    "end": "1392860"
  },
  {
    "text": "fully autonomously and now and one",
    "start": "1392860",
    "end": "1398860"
  },
  {
    "text": "problem was for example with this initial system is if the depth camera is too far away you're not going to get the",
    "start": "1398860",
    "end": "1404830"
  },
  {
    "text": "object post actually accurately enough so that you could really always grab it so now what we have is we have a depth",
    "start": "1404830",
    "end": "1411070"
  },
  {
    "text": "camera also on the on the wrist of the arm and then you get a more accurate pulse estimate for the object that you",
    "start": "1411070",
    "end": "1417190"
  },
  {
    "text": "actually want to pick up okay but the nice thing is we can do all this",
    "start": "1417190",
    "end": "1422590"
  },
  {
    "text": "tracking in a single joint optimization over all the poses of these objects and the robot and also you can incorporate",
    "start": "1422590",
    "end": "1430930"
  },
  {
    "text": "physical constraints into that estimation problem such as for example objects shouldn't interpenetrate and",
    "start": "1430930",
    "end": "1436420"
  },
  {
    "text": "things like that okay so the final piece to the puzzle here then the manipulator",
    "start": "1436420",
    "end": "1444010"
  },
  {
    "text": "control and for that we're using a technique that Nathan Ratliff introduced it's called this Romanian motion",
    "start": "1444010",
    "end": "1449590"
  },
  {
    "text": "policies where the problem is that let's say if you have a rough trajectory of how the robot arm should move then you",
    "start": "1449590",
    "end": "1457600"
  },
  {
    "text": "what you need to generate at a very high frequency you need to generate control commands for the manipulator and of",
    "start": "1457600",
    "end": "1465070"
  },
  {
    "text": "course that's been one of the some of the most fundamental work has happened here also especially with Ossama Khatib",
    "start": "1465070",
    "end": "1470560"
  },
  {
    "text": "slap and then the idea is that you wanna do reasoning actually in the",
    "start": "1470560",
    "end": "1476279"
  },
  {
    "text": "task space in the geometric space that is going on here but you need to generate control commands for the motors for the joints",
    "start": "1476279",
    "end": "1482940"
  },
  {
    "text": "of the robot okay so how this approach is working is you can define you define",
    "start": "1482940",
    "end": "1489210"
  },
  {
    "text": "points for example on the robot just give you some examples",
    "start": "1489210",
    "end": "1494990"
  },
  {
    "text": "okay and for these points you can now define or you can compute for example",
    "start": "1494990",
    "end": "1502710"
  },
  {
    "text": "features in the environment such as what is the closest obstacle to that point um",
    "start": "1502710",
    "end": "1508070"
  },
  {
    "text": "multiple of them you can also define for example for this one for the gripper you",
    "start": "1508070",
    "end": "1514230"
  },
  {
    "text": "can define for example what is the direction of the goal point where you want the the end effector to actually go",
    "start": "1514230",
    "end": "1519690"
  },
  {
    "text": "in the workspace yeah and what it gives you is for each of these points",
    "start": "1519690",
    "end": "1526250"
  },
  {
    "text": "depending on for example the proximity of these obstacles and things like that it's going to give you a desired",
    "start": "1526250",
    "end": "1531720"
  },
  {
    "text": "acceleration where that point would like to move and these accelerations can also take into account the velocity so for",
    "start": "1531720",
    "end": "1539340"
  },
  {
    "text": "example when you're moving along an obstacle that it will not penalize that so it will kind of allow you to smoothly",
    "start": "1539340",
    "end": "1546600"
  },
  {
    "text": "move for long obstacles and then once you have defined all of these then you can combine them all together and",
    "start": "1546600",
    "end": "1553289"
  },
  {
    "text": "compute the accelerations of then actually in the controls in the joint space of the robot okay so you define",
    "start": "1553289",
    "end": "1559590"
  },
  {
    "text": "things in the operational space and then you pull them back into the joint space and here's just an example kind of this",
    "start": "1559590",
    "end": "1565350"
  },
  {
    "text": "is a robot that doesn't like open drawers and this is Nathan in the background and what we're doing here is",
    "start": "1565350",
    "end": "1571679"
  },
  {
    "text": "we're using dart to track the status of the cabinet right and then the robot as you can see can generate these pretty",
    "start": "1571679",
    "end": "1577230"
  },
  {
    "text": "nice smooth motions important thing to",
    "start": "1577230",
    "end": "1584580"
  },
  {
    "text": "mention Scott that his armies it's still a local technique right it's still kind of just mapping from the current state",
    "start": "1584580",
    "end": "1589830"
  },
  {
    "text": "in velocities to accelerations so you still have to kind of have on top of",
    "start": "1589830",
    "end": "1595110"
  },
  {
    "text": "that a global trajectory optimizer or a path window here's another example now",
    "start": "1595110",
    "end": "1602940"
  },
  {
    "text": "all of this put together this is work that Chris Paxton did how you now let's assume if you do",
    "start": "1602940",
    "end": "1609580"
  },
  {
    "text": "higher-level planning and you want a robot to execute such a plan then one",
    "start": "1609580",
    "end": "1615850"
  },
  {
    "text": "technique that people are using is called this behavior trees which are kind of just observing the current state",
    "start": "1615850",
    "end": "1621610"
  },
  {
    "text": "and then based on that they're generating what kind of discrete mode for example the system should be in and",
    "start": "1621610",
    "end": "1628889"
  },
  {
    "text": "Chris developed these logical dynamical systems that then for some in this case",
    "start": "1628889",
    "end": "1634029"
  },
  {
    "text": "let's assume the robot wants to put the spam into the drawer and there's pretty",
    "start": "1634029",
    "end": "1641019"
  },
  {
    "text": "robust to these kind of perturb insists right so it's constantly just kind of",
    "start": "1641019",
    "end": "1646179"
  },
  {
    "text": "checking the state of the system of the environment and then sees if for example preconditions for certain actions are",
    "start": "1646179",
    "end": "1653200"
  },
  {
    "text": "still fulfilled",
    "start": "1653200",
    "end": "1655830"
  },
  {
    "text": "so it pretty smoothly and quickly right can't react to all of that and we can later on of course we can have a",
    "start": "1664820",
    "end": "1670940"
  },
  {
    "text": "discussion on that like you could not argue you can do all of this in an end-to-end way but I think the advantage",
    "start": "1670940",
    "end": "1678259"
  },
  {
    "text": "of the model-based techniques is really that you can now really put on top of this a really long term task in motion",
    "start": "1678259",
    "end": "1684830"
  },
  {
    "text": "planner right that it's very general it can and can generate these these",
    "start": "1684830",
    "end": "1690230"
  },
  {
    "text": "behaviors let me just give you one more example this is a project that Cailin",
    "start": "1690230",
    "end": "1695299"
  },
  {
    "text": "Garrett he's a student MIT with Leslie kelp Lee and Thomas Lozano press working on tasks and motion planning which is",
    "start": "1695299",
    "end": "1701570"
  },
  {
    "text": "exactly this higher-level reasoning and actually most of the work that you see these days is still happening purely in",
    "start": "1701570",
    "end": "1708950"
  },
  {
    "text": "simulation which means that task more spending is they generate these scenarios and then they evaluate how",
    "start": "1708950",
    "end": "1714769"
  },
  {
    "text": "fast they can do the planning in this scenarios but often they don't really work in the real world yet because the",
    "start": "1714769",
    "end": "1720740"
  },
  {
    "text": "perception isn't good enough but now with having a system here that has pretty good perception we can actually",
    "start": "1720740",
    "end": "1726049"
  },
  {
    "text": "start running these tasks and motion planning systems in the real world so in this case the task is in the focus of",
    "start": "1726049",
    "end": "1732350"
  },
  {
    "text": "that paper is also on dealing with with uncertainty or not knowing certain things so in this case the robot knows",
    "start": "1732350",
    "end": "1739159"
  },
  {
    "text": "that this I think it's miss Pam again yes spam again is in one of the two top drawers",
    "start": "1739159",
    "end": "1744950"
  },
  {
    "text": "and it should figure out in which one and then put it in the other one okay",
    "start": "1744950",
    "end": "1750159"
  },
  {
    "text": "and in this case the motion is actually he didn't use the RM piece but the full",
    "start": "1750159",
    "end": "1758059"
  },
  {
    "text": "perception system that we have so you can reason about okay so it's gotta be the upper drawer and then detects it in",
    "start": "1758059",
    "end": "1763399"
  },
  {
    "text": "the Aqua drawer and then you can do all the kind of planning with again the preconditions and all of that so you can",
    "start": "1763399",
    "end": "1769549"
  },
  {
    "text": "specify these tasks at a very high level of abstraction then the planner figures out the rest and then we can execute it",
    "start": "1769549",
    "end": "1775759"
  },
  {
    "text": "now actually in the real world okay and",
    "start": "1775759",
    "end": "1781399"
  },
  {
    "text": "we're currently putting all the pieces together so that we can also do some longer-term benchmarking of that system",
    "start": "1781399",
    "end": "1787549"
  },
  {
    "text": "so you could imagine if you run this on whatever hundreds of tasks for many hours how often is it gonna fail because",
    "start": "1787549",
    "end": "1794120"
  },
  {
    "text": "I'm sure it's gonna fail once in a while and because what are these failure cases right so far the failure cases that",
    "start": "1794120",
    "end": "1800270"
  },
  {
    "text": "we've seen when the cameras are just off ball is really if the post estimation of these objects isn't accurate enough and",
    "start": "1800270",
    "end": "1806419"
  },
  {
    "text": "things like that and that brings me also now to the next point where actually the",
    "start": "1806419",
    "end": "1813890"
  },
  {
    "text": "idea is can we get away with without this explicit 60 pulse estimation for",
    "start": "1813890",
    "end": "1819020"
  },
  {
    "text": "manipulation tasks okay so so far again we assume that we have 3d models of",
    "start": "1819020",
    "end": "1824660"
  },
  {
    "text": "these objects and if you have that model and you can detect where it is then you can also determine which grasp you want",
    "start": "1824660",
    "end": "1831710"
  },
  {
    "text": "to use for example but of course ultimately you want to be able to do all of this without object models right and",
    "start": "1831710",
    "end": "1837409"
  },
  {
    "text": "I want to talk a bit about some of the more recent work we've done in that direction in order to cross unknown",
    "start": "1837409",
    "end": "1843110"
  },
  {
    "text": "objects on the one hand side of course you want to use training data for deep",
    "start": "1843110",
    "end": "1850880"
  },
  {
    "text": "networks if you want to solve this problem with deep learning and here we used flex similar later in video system",
    "start": "1850880",
    "end": "1858309"
  },
  {
    "text": "where the idea is you have a model of an object and you can just sample thousands",
    "start": "1858309",
    "end": "1864530"
  },
  {
    "text": "of possible grasps and then you run the simulator in order to evaluate whether that's a good grasp or not right and you",
    "start": "1864530",
    "end": "1871340"
  },
  {
    "text": "can do this all in parallel on the GPU with with with many instances of this class and you can do this with many",
    "start": "1871340",
    "end": "1877730"
  },
  {
    "text": "objects and then you do some perturbations on it to figure out whether the grasp is actually also",
    "start": "1877730",
    "end": "1883070"
  },
  {
    "text": "stable or robust to these perturbations okay and then what you get is for",
    "start": "1883070",
    "end": "1889070"
  },
  {
    "text": "example maybe for these objects you get some training data that tells you for that object these are the 60 graphs that",
    "start": "1889070",
    "end": "1894710"
  },
  {
    "text": "actually worked yeah and you can do this now for whatever thousands of objects",
    "start": "1894710",
    "end": "1899919"
  },
  {
    "text": "and in this specific piece of work that most probably ICCB with by arsalan last",
    "start": "1899919",
    "end": "1907520"
  },
  {
    "text": "year and I want to note the code and everything is available for that if you want to look into this",
    "start": "1907520",
    "end": "1913010"
  },
  {
    "text": "it's the idea is that we train a deep network that looks at the point load of",
    "start": "1913010",
    "end": "1919970"
  },
  {
    "text": "an object and then samples grass that have a high chance of succeeding okay",
    "start": "1919970",
    "end": "1925600"
  },
  {
    "text": "how we do this is let's assume you have an input image and we use the point loud",
    "start": "1925600",
    "end": "1931850"
  },
  {
    "text": "what makes this problem with trickier then if you do it in the in the simulated world is that we have",
    "start": "1931850",
    "end": "1938059"
  },
  {
    "text": "occlusions and things like that right so we want to train a network to actually just observe the point loss so these the",
    "start": "1938059",
    "end": "1943370"
  },
  {
    "text": "backside of the object for example you can't even see okay so now what we do",
    "start": "1943370",
    "end": "1948470"
  },
  {
    "text": "with this is we first have a deep network and I'll go a bit into the detail of that we have a deep network",
    "start": "1948470",
    "end": "1953900"
  },
  {
    "text": "that takes that as input and samples grass for it okay and since the grass",
    "start": "1953900",
    "end": "1961220"
  },
  {
    "text": "same place actually might make a mistake because it's not trivial we have another",
    "start": "1961220",
    "end": "1968030"
  },
  {
    "text": "network that kind of an evaluator discriminator that takes us in put a configuration of a sampled grasp and the",
    "start": "1968030",
    "end": "1976880"
  },
  {
    "text": "object point cloud so we're using pointing at plus plus further for the deep networks here and it just is",
    "start": "1976880",
    "end": "1982370"
  },
  {
    "text": "trained to predict whether that's going to be a successful grasp or not okay",
    "start": "1982370",
    "end": "1988250"
  },
  {
    "text": "so you can imagine this as being something that's cleaning up some of the sampled grass and and refines them okay",
    "start": "1988250",
    "end": "1994730"
  },
  {
    "text": "and we can also use this as a set for refinement where we say actually the",
    "start": "1994730",
    "end": "2000640"
  },
  {
    "text": "grasp evaluator since it's a deep network you can also propagate backwards",
    "start": "2000640",
    "end": "2006130"
  },
  {
    "text": "through the network to increase the probability of success with respect to the 60 poles of the gripper and by",
    "start": "2006130",
    "end": "2012250"
  },
  {
    "text": "iterating through this you can actually move the gripper into a configuration that has a higher chance of succeeding",
    "start": "2012250",
    "end": "2017760"
  },
  {
    "text": "yeah let me just go into the grass sampling here and again this is one of these order encoders in this case a",
    "start": "2017760",
    "end": "2023980"
  },
  {
    "text": "conditional variational order encoder where the idea is through the simulation",
    "start": "2023980",
    "end": "2029049"
  },
  {
    "text": "we get a successful grasp okay and we",
    "start": "2029049",
    "end": "2034210"
  },
  {
    "text": "encode this as the point cloud of the object along with the 60 gripper pose",
    "start": "2034210",
    "end": "2039600"
  },
  {
    "text": "relative to the object so it's all centered around the object coordinate frame but as seen from the camera so we",
    "start": "2039600",
    "end": "2046240"
  },
  {
    "text": "don't know the internal coordinate frame of the object itself okay and then we",
    "start": "2046240",
    "end": "2052118"
  },
  {
    "text": "want to encode this into in this case the 2d latent space such that the 2d",
    "start": "2052119",
    "end": "2060128"
  },
  {
    "text": "value along with the object model again can be decoded into the 60 grasp okay so the idea is we want",
    "start": "2060129",
    "end": "2070000"
  },
  {
    "text": "to be able instead if that Layton's face was 60 then of course you could just copy that value over right but the idea",
    "start": "2070000",
    "end": "2076840"
  },
  {
    "text": "is we can actually compress the 60 grasp face into a 2d space if we condition it",
    "start": "2076840",
    "end": "2083200"
  },
  {
    "text": "on the object point lot okay and to train this and then the decoder of",
    "start": "2083200",
    "end": "2091750"
  },
  {
    "text": "course again generates that a grasp kind of reconstruction and the loss is then",
    "start": "2091750",
    "end": "2098980"
  },
  {
    "text": "just the difference between the position that the network generated versus the position you put in okay and you train",
    "start": "2098980",
    "end": "2106510"
  },
  {
    "text": "this again on many of these successful grasps and the network then learns to go",
    "start": "2106510",
    "end": "2113500"
  },
  {
    "text": "from the weight from the point cloud yes",
    "start": "2113500",
    "end": "2121450"
  },
  {
    "text": "to to kind of encode the 60 space into the 2d space okay and then I'll tell you",
    "start": "2121450",
    "end": "2127630"
  },
  {
    "text": "how we're going to use this now once we have trained that latent space we can",
    "start": "2127630",
    "end": "2134470"
  },
  {
    "text": "then later on kind of throw in this case we throw away the encoder in the",
    "start": "2134470",
    "end": "2139600"
  },
  {
    "text": "previous work with this particle filtering we threw away the decoder here we can throw away the encoder because",
    "start": "2139600",
    "end": "2145330"
  },
  {
    "text": "later what we can do is we can then take a point loud sample a latent value and",
    "start": "2145330",
    "end": "2150910"
  },
  {
    "text": "the network is gonna give us a 60 grasp for that okay and we also actually have",
    "start": "2150910",
    "end": "2157840"
  },
  {
    "text": "a more recent version that is working even better that it's just using actually the again approach for that",
    "start": "2157840",
    "end": "2166060"
  },
  {
    "text": "yeah that's again formulation that just came out last year that works actually even better but the idea is is the high",
    "start": "2166060",
    "end": "2174940"
  },
  {
    "text": "level ideas very very similar it's just a more robust training for this embedding space okay so here's what this",
    "start": "2174940",
    "end": "2181030"
  },
  {
    "text": "might look like then for example here's this bowl and what Athlon did here if",
    "start": "2181030",
    "end": "2186160"
  },
  {
    "text": "you move through this 2d latent space then these are grasps that the network would sample conditioned on the latent",
    "start": "2186160",
    "end": "2192640"
  },
  {
    "text": "value okay and",
    "start": "2192640",
    "end": "2198380"
  },
  {
    "text": "if you for example choose a different trajectory let's say through that space you're getting kind of this wrapping around the rim of the bowl and since we",
    "start": "2198380",
    "end": "2207590"
  },
  {
    "text": "train it on different objects different object types the same latent space can be used on different objects and then",
    "start": "2207590",
    "end": "2213620"
  },
  {
    "text": "for example in this case this would be an area where it's trying to generate grass around the handle so this is the",
    "start": "2213620",
    "end": "2221270"
  },
  {
    "text": "grass sampler and then to clean up this grass again we have another network that takes this input the point cloud of the",
    "start": "2221270",
    "end": "2229820"
  },
  {
    "text": "object along with a point cloud representing the gripper relative to the",
    "start": "2229820",
    "end": "2235400"
  },
  {
    "text": "object okay and then it's a point nipple of not plus plus let's just trained to say whether that's going to be",
    "start": "2235400",
    "end": "2241040"
  },
  {
    "text": "successful or not and we're using the same simulated training data for that so essentially what it does is if you",
    "start": "2241040",
    "end": "2248120"
  },
  {
    "text": "look at this would be an example for example as input to the network and then the score would be awkward 1 3 and again",
    "start": "2248120",
    "end": "2254600"
  },
  {
    "text": "you can now propagate that through the network with respect to the post of the",
    "start": "2254600",
    "end": "2260210"
  },
  {
    "text": "gripper so that you can increase the score and you can do this iteratively right where",
    "start": "2260210",
    "end": "2270320"
  },
  {
    "text": "then the network generates a grass that has a higher chance of success makes sense and then we can put this all",
    "start": "2270320",
    "end": "2280640"
  },
  {
    "text": "together and here's just some results there was at the time of the ICCB submission we had 88% grass success rate",
    "start": "2280640",
    "end": "2288440"
  },
  {
    "text": "this is actually 88% on the very first attempt right which means of course if",
    "start": "2288440",
    "end": "2294380"
  },
  {
    "text": "you allow it to regress then we're getting much closer to a hundred and these were objects that were not first",
    "start": "2294380",
    "end": "2301760"
  },
  {
    "text": "of all it was only trained in simulation and the models of these objects were not",
    "start": "2301760",
    "end": "2307130"
  },
  {
    "text": "in the training data right and now the most recent version actually we trained",
    "start": "2307130",
    "end": "2312830"
  },
  {
    "text": "a new network which is a single network that we trained on more than 8,000 objects and more than 200 shapener",
    "start": "2312830",
    "end": "2320030"
  },
  {
    "text": "classes so it's a very general network so it's not focus on a single object",
    "start": "2320030",
    "end": "2325670"
  },
  {
    "text": "type or so you can also actually train it on a specific object type but then of",
    "start": "2325670",
    "end": "2331010"
  },
  {
    "text": "course it doesn't general as well hey one extension to that is now",
    "start": "2331010",
    "end": "2338320"
  },
  {
    "text": "on a Miss if the timing works too cluttered scene and something that's",
    "start": "2338320",
    "end": "2343940"
  },
  {
    "text": "going to come up at liquor where the idea is imagine you have this scene so far we assume that we have only the",
    "start": "2343940",
    "end": "2350120"
  },
  {
    "text": "single object okay the point load of this object but if you have a more cluttered scene then what we can do is",
    "start": "2350120",
    "end": "2356810"
  },
  {
    "text": "we had a paper call last year where you can segment the scene you see even if",
    "start": "2356810",
    "end": "2362840"
  },
  {
    "text": "the objects are not known so unknown object instant segmentation and let's assume someone tells you hey pick up",
    "start": "2362840",
    "end": "2369860"
  },
  {
    "text": "object number three doesn't matter which class we can now of course first of all we can kind of cut out that part around",
    "start": "2369860",
    "end": "2376820"
  },
  {
    "text": "that object from the scene which is this",
    "start": "2376820",
    "end": "2383060"
  },
  {
    "text": "kind of red rectangle here and now we",
    "start": "2383060",
    "end": "2388220"
  },
  {
    "text": "can take this crop point cloud first we can segment out in this case it's the",
    "start": "2388220",
    "end": "2394640"
  },
  {
    "text": "master we can segment that mastered out and can feed that into the grass there to generate good grass for it",
    "start": "2394640",
    "end": "2401260"
  },
  {
    "text": "and then we have another net work another network the collision net that's",
    "start": "2401260",
    "end": "2409280"
  },
  {
    "text": "not going to look at this grass and predict which of those are going to be in collision or not okay now you might",
    "start": "2409280",
    "end": "2414800"
  },
  {
    "text": "say that you don't need this collision net because you just do collision checks on your point load but it turns out that",
    "start": "2414800",
    "end": "2422030"
  },
  {
    "text": "that is actually much better at detecting collisions with occluded areas right because there was also trained on",
    "start": "2422030",
    "end": "2428210"
  },
  {
    "text": "these full object models so it learned to kind of predict collisions even if you don't see it in the point cloud",
    "start": "2428210",
    "end": "2436720"
  },
  {
    "text": "yeah so it's significantly better than just using the raw observation data and",
    "start": "2436720",
    "end": "2442340"
  },
  {
    "text": "once you have that you can now of course choose graphs that are not in collision with anything and have a much higher",
    "start": "2442340",
    "end": "2448820"
  },
  {
    "text": "chance of succeeding even in clutter so here's an example situation where let's say this is the outside view on the raw but this is the",
    "start": "2448820",
    "end": "2455900"
  },
  {
    "text": "view from the robots wrist camera and I think we want to pick up in this case the the white mark and we do the",
    "start": "2455900",
    "end": "2464090"
  },
  {
    "text": "segmentation the goal we do the segmentation of the scene and then we can check which of",
    "start": "2464090",
    "end": "2469750"
  },
  {
    "text": "these are mostly contributing to the coalition of the grass that we want to",
    "start": "2469750",
    "end": "2475060"
  },
  {
    "text": "execute in this case it would pick the one that was mostly contributing to this",
    "start": "2475060",
    "end": "2480370"
  },
  {
    "text": "and then they can just do grass blending on that object and you can get this out of the way and then just the same check",
    "start": "2480370",
    "end": "2488140"
  },
  {
    "text": "on the scene again and now it can say ok I can actually pick up the the white cup to another segmentation round and you",
    "start": "2488140",
    "end": "2496870"
  },
  {
    "text": "can pick it up okay so now we can start even without having any object model in",
    "start": "2496870",
    "end": "2502390"
  },
  {
    "text": "there right no 3d model of existing objects or reasoning or 60 pose we can",
    "start": "2502390",
    "end": "2507610"
  },
  {
    "text": "really do start doing more and more complex actually manipulation tasks in this setting okay and you can also",
    "start": "2507610",
    "end": "2515170"
  },
  {
    "text": "imagine that you can use a very similar approach to do placement right where car",
    "start": "2515170",
    "end": "2521200"
  },
  {
    "text": "placement is very related to the grasping problem right where you would say I have a point cloud that I'm",
    "start": "2521200",
    "end": "2527170"
  },
  {
    "text": "observing and you can now start sampling stable placements for an object on a",
    "start": "2527170",
    "end": "2532540"
  },
  {
    "text": "surface and you can now then you can actually do the full kind of cross felling where you're reason about picking an option up and putting it on",
    "start": "2532540",
    "end": "2539050"
  },
  {
    "text": "another stable position all all without using any 60/60 object poses also and",
    "start": "2539050",
    "end": "2546760"
  },
  {
    "text": "also what Arthur is looking at right now doing this also with visual feedback in real time I wanna just briefly say",
    "start": "2546760",
    "end": "2554200"
  },
  {
    "text": "something about the role of simulation I think of course simulation is going to",
    "start": "2554200",
    "end": "2561010"
  },
  {
    "text": "be a really really big plus for robotics right I think these simulators are",
    "start": "2561010",
    "end": "2566890"
  },
  {
    "text": "nowadays are becoming more and more realistic both on the physics side but also on the photorealistic rendering",
    "start": "2566890",
    "end": "2572170"
  },
  {
    "text": "side and training robots in in simulation is gonna be very powerful just to save time make it awesome more",
    "start": "2572170",
    "end": "2579910"
  },
  {
    "text": "safe and and clearly more efficient and in simulation you can provide much more",
    "start": "2579910",
    "end": "2586260"
  },
  {
    "text": "details on the training data than if you would do it just in the real world where",
    "start": "2586260",
    "end": "2591460"
  },
  {
    "text": "you often don't have access to the internal state for example of the objects so one example here's of course how we're training",
    "start": "2591460",
    "end": "2598370"
  },
  {
    "text": "most of our visual detection systems is just in simulation where you can just",
    "start": "2598370",
    "end": "2604590"
  },
  {
    "text": "place your objects into a scene and you can randomize over the lighting conditions and then we see if this video",
    "start": "2604590",
    "end": "2612570"
  },
  {
    "text": "wants to play whatever he wants to write oh yeah",
    "start": "2612570",
    "end": "2619640"
  },
  {
    "text": "Oh interesting huh",
    "start": "2630280",
    "end": "2634980"
  },
  {
    "text": "okay not playing it not not you get the",
    "start": "2635460",
    "end": "2641069"
  },
  {
    "text": "idea so you can the good thing you can put these Octavia's also seen where you can put the objects into drawers and",
    "start": "2641069",
    "end": "2647069"
  },
  {
    "text": "things like that so that you can simulate more realistic viewpoints if the objects are in cluttered scenes there's a lot of of course like",
    "start": "2647069",
    "end": "2653700"
  },
  {
    "text": "randomizing the colors and things like that so that your detectors become robust I would like to be able to say",
    "start": "2653700",
    "end": "2659849"
  },
  {
    "text": "that we solve this problem now which means we can train everything purely in simulation it's gonna work right away",
    "start": "2659849",
    "end": "2665790"
  },
  {
    "text": "perfectly well in the real world we have not figured it out yet so that it's",
    "start": "2665790",
    "end": "2671760"
  },
  {
    "text": "really working as well as if you would add some additional real-world data to",
    "start": "2671760",
    "end": "2677250"
  },
  {
    "text": "it and I haven't seen any techniques that really perfectly transfer from the",
    "start": "2677250",
    "end": "2682319"
  },
  {
    "text": "simulator to the real world even on the perception side okay so what we have a",
    "start": "2682319",
    "end": "2687809"
  },
  {
    "text": "paper coming out with Ikra where the robot then collects a small amount of additional training data for the for",
    "start": "2687809",
    "end": "2694770"
  },
  {
    "text": "example for the detection networks and that improves the results them still over just doing it in simulation in the",
    "start": "2694770",
    "end": "2701430"
  },
  {
    "text": "same of course for control where we can now train robots and policies in",
    "start": "2701430",
    "end": "2707849"
  },
  {
    "text": "simulated environments and there again similar these ideas with domain randomization where in this case you",
    "start": "2707849",
    "end": "2714089"
  },
  {
    "text": "might not know how long that rope is or you might not know the mass and the size of that little cylinder that you want to",
    "start": "2714089",
    "end": "2720299"
  },
  {
    "text": "put in there you can randomize over that and train policies for this this is work",
    "start": "2720299",
    "end": "2726329"
  },
  {
    "text": "that Afghan chebotar did and probably Aramis is now working on a technique",
    "start": "2726329",
    "end": "2732299"
  },
  {
    "text": "that's called base sim where the idea is we do most of the training in the in the",
    "start": "2732299",
    "end": "2737790"
  },
  {
    "text": "simulation and then we go into the real world to a small number of rollouts",
    "start": "2737790",
    "end": "2743160"
  },
  {
    "text": "in order to hopefully refine the simulation and then get away with training something that works well in",
    "start": "2743160",
    "end": "2749040"
  },
  {
    "text": "the real world but only requires a minimum amount of real-world experience let's say ok and we see a lot of work in",
    "start": "2749040",
    "end": "2757589"
  },
  {
    "text": "this direction going on in general right but what this work by Fabio Ramos was at",
    "start": "2757589",
    "end": "2762599"
  },
  {
    "text": "ours last year does is really phrase this as a Bayesian estimation problem where you have a sim",
    "start": "2762599",
    "end": "2768280"
  },
  {
    "text": "later that has certain parameters you treat the simulator as a black box but it has certain parameters and you",
    "start": "2768280",
    "end": "2773590"
  },
  {
    "text": "generate many rollouts and with that you learn an invertible model that goes from",
    "start": "2773590",
    "end": "2778830"
  },
  {
    "text": "statistics over these rollouts back to distributions over likelihood over the parameters and then you can update them",
    "start": "2778830",
    "end": "2785650"
  },
  {
    "text": "over time and refine your sampling distributions using the real-world rollouts okay so I think simulation is",
    "start": "2785650",
    "end": "2793540"
  },
  {
    "text": "gonna be really useful for perception for control but also just for testing your whole robot system so here's an",
    "start": "2793540",
    "end": "2799510"
  },
  {
    "text": "example of our robot in the real kitchen and one of them is actually a simulated",
    "start": "2799510",
    "end": "2804850"
  },
  {
    "text": "version of that kitchen so we worked with the Nvidia the content creation team that generated a pretty nice",
    "start": "2804850",
    "end": "2811270"
  },
  {
    "text": "realistic simulation of the kitchen I'm sure by now most of you have seen which one is which but I'm sure a little bit",
    "start": "2811270",
    "end": "2820060"
  },
  {
    "text": "more work might have made it even more similar but it's not trivial to see anymore right so that kind of hints at",
    "start": "2820060",
    "end": "2826480"
  },
  {
    "text": "what is possible with these kind of simulators right also on the photorealistic rendering side okay so",
    "start": "2826480",
    "end": "2832690"
  },
  {
    "text": "the goal would be that you can actually run your robot control system again this against the simulator and as we know",
    "start": "2832690",
    "end": "2840010"
  },
  {
    "text": "there might be some times you might have to do some debugging and things like that right so you can do this against",
    "start": "2840010",
    "end": "2845680"
  },
  {
    "text": "the simulator if the simulation is realistic enough and it's still actually a major effort to develop a simpler",
    "start": "2845680",
    "end": "2853000"
  },
  {
    "text": "photorealistic and especially also a physically realistic simulation of such an environment so how to do content",
    "start": "2853000",
    "end": "2859450"
  },
  {
    "text": "creation for these kind of settings it's also one of the big open questions",
    "start": "2859450",
    "end": "2864660"
  },
  {
    "text": "alright I'm coming to my last slide here mainly and I think we've seen the huge",
    "start": "2864990",
    "end": "2872320"
  },
  {
    "text": "huge progress especially also thanks to deep learning on the perception side on individual components and what I talked",
    "start": "2872320",
    "end": "2878920"
  },
  {
    "text": "about today is kind of this attempt of bringing these different pieces together so that we can solve kind of larger",
    "start": "2878920",
    "end": "2884320"
  },
  {
    "text": "scale problems right that go all the way from the low-level control to the high-level task planning and all the",
    "start": "2884320",
    "end": "2889810"
  },
  {
    "text": "perception and I think it will be really useful to move more towards these",
    "start": "2889810",
    "end": "2895510"
  },
  {
    "text": "systems because often by doing that you really learn about what's working what's not working and for that you really need to",
    "start": "2895510",
    "end": "2902050"
  },
  {
    "text": "integrate these different components and I think as a robotics community we need to start thinking more about",
    "start": "2902050",
    "end": "2907080"
  },
  {
    "text": "benchmarking environments in which we can really test our different algorithms against each other not only on let's say",
    "start": "2907080",
    "end": "2913720"
  },
  {
    "text": "simple tasks like just picking up an object but much more in the context of larger scale tasks I think being in the",
    "start": "2913720",
    "end": "2920110"
  },
  {
    "text": "nvidia auditorium of course and videos super well-suited I think for really driving a lot of the progress in",
    "start": "2920110",
    "end": "2925840"
  },
  {
    "text": "robotics right on the one hand side it's clearly of course GPU acceleration for for deep learning and inference",
    "start": "2925840",
    "end": "2932590"
  },
  {
    "text": "perception these robots will need more and more perception and learning so that that's a clear case but also I think the",
    "start": "2932590",
    "end": "2939670"
  },
  {
    "text": "simulation video has of course a lot of experience on physics based simulation photorealistic rendering from gaming and",
    "start": "2939670",
    "end": "2946810"
  },
  {
    "text": "computer graphics and I think that's gonna be really important moving all of this forward various open questions I",
    "start": "2946810",
    "end": "2954880"
  },
  {
    "text": "think one interesting events that really keeps on coming up is how do we represent these environments you saw",
    "start": "2954880",
    "end": "2960370"
  },
  {
    "text": "today there's one the first part was more about really explicit representations of the world right",
    "start": "2960370",
    "end": "2965890"
  },
  {
    "text": "explicit shape models explicit reasoning about things like 60 posts and everything and an alternative is more",
    "start": "2965890",
    "end": "2973000"
  },
  {
    "text": "like the second part that I described which is more implicit right like the 60 grasped net that generous grasp of an",
    "start": "2973000",
    "end": "2979540"
  },
  {
    "text": "object on the one hand side you might say the network has no idea of what it's doing but obviously if it can generate",
    "start": "2979540",
    "end": "2985960"
  },
  {
    "text": "good grass for different objects it must implicitly have these kind of reasoning",
    "start": "2985960",
    "end": "2991270"
  },
  {
    "text": "capabilities right and the questioner is we're on the long run should we be on on",
    "start": "2991270",
    "end": "2996520"
  },
  {
    "text": "that right like how much 3d knowledge for example should we inject into the deep networks in order to do the",
    "start": "2996520",
    "end": "3003360"
  },
  {
    "text": "learning better and I think and clearly we've seen some also nice recent work on",
    "start": "3003360",
    "end": "3008550"
  },
  {
    "text": "3d voxel-based representations that are not explicitly recovering 3d shape but",
    "start": "3008550",
    "end": "3013880"
  },
  {
    "text": "just recover kind of a 3d shape feature space that is then well-suited for",
    "start": "3013880",
    "end": "3018930"
  },
  {
    "text": "certain tasks like matching objects into a scene and things like that so I think the earth this is a really one of the",
    "start": "3018930",
    "end": "3026040"
  },
  {
    "text": "big questions that the earth that I'd be happy to discuss some more also the same",
    "start": "3026040",
    "end": "3031680"
  },
  {
    "text": "of course on the controls right we need to learn representations obviously that are not just good for a",
    "start": "3031680",
    "end": "3037319"
  },
  {
    "text": "specific task a labeling task but they have to be good in the context of",
    "start": "3037319",
    "end": "3042569"
  },
  {
    "text": "control tasks and of course like the work the Chelsea is doing here is also really interesting direction this in",
    "start": "3042569",
    "end": "3048179"
  },
  {
    "text": "this really interesting work in this direction of course simulation how close",
    "start": "3048179",
    "end": "3053400"
  },
  {
    "text": "are we there's still many different components that I think are still missing there right how do we do seem to real better",
    "start": "3053400",
    "end": "3059819"
  },
  {
    "text": "content creation is going to be a big problem if you want to train our robots in thousands of environments they have",
    "start": "3059819",
    "end": "3065489"
  },
  {
    "text": "to look pretty good so who's going to do this for us of course the furniture manufacturers",
    "start": "3065489",
    "end": "3070589"
  },
  {
    "text": "they often have 3d models if they would be willing to share with us all this months I think that would be really cool touch sensing is a really big area still",
    "start": "3070589",
    "end": "3079140"
  },
  {
    "text": "on the one hand side on the hardware itself right and I saw some exciting work today kind of building better touch",
    "start": "3079140",
    "end": "3085919"
  },
  {
    "text": "sensors but also then how to take advantage of visit because it could should clearly be connected to this and",
    "start": "3085919",
    "end": "3091859"
  },
  {
    "text": "again I think benchmarking is going to be a big question moving forward so with",
    "start": "3091859",
    "end": "3098459"
  },
  {
    "text": "that thank you very much also this is the lab and of course many of the great interns we've been working with again",
    "start": "3098459",
    "end": "3105119"
  },
  {
    "text": "several of them here if I have one more minute I can just show you one think that I didn't turn out is like something",
    "start": "3105119",
    "end": "3110339"
  },
  {
    "text": "like this just to show you that we're not only doing kitchen kind of stuff but this is work that uncle Honda has done",
    "start": "3110339",
    "end": "3115559"
  },
  {
    "text": "on that expired it's gonna be at across where the ideas he built with multiple def cameras are I could hand tracking",
    "start": "3115559",
    "end": "3122189"
  },
  {
    "text": "system and then they can in real time well 1/6 of real-time here want to be",
    "start": "3122189",
    "end": "3129659"
  },
  {
    "text": "really open about this but you can actually get the robot to do pretty cool",
    "start": "3129659",
    "end": "3135089"
  },
  {
    "text": "stuff so the idea is that he's standing next to the robot and he's kind of moving the hand that then in real-time",
    "start": "3135089",
    "end": "3141319"
  },
  {
    "text": "gets first you track it but you cannot just copy the joints over but you have",
    "start": "3141319",
    "end": "3147539"
  },
  {
    "text": "to cut because the shape of the allegro hand is different so you have to do some real reading to that but then you can",
    "start": "3147539",
    "end": "3154199"
  },
  {
    "text": "actually get this hand to do pretty cool things right that in the fully automated way is just not possible yet and you can",
    "start": "3154199",
    "end": "3161039"
  },
  {
    "text": "use this then moving forward maybe also for imitation learning and things that and of course touch sensing year",
    "start": "3161039",
    "end": "3167369"
  },
  {
    "text": "we've done good bunch of touch sensing work using the biotech sensors here to",
    "start": "3167369",
    "end": "3172710"
  },
  {
    "text": "pick up objects but also just to learn kind of models of these touch sensors right so that you can go from the in",
    "start": "3172710",
    "end": "3179130"
  },
  {
    "text": "this case electron measurements to contact points and contact directions and things like that alright with that",
    "start": "3179130",
    "end": "3186769"
  },
  {
    "text": "thank you for attention [Applause]",
    "start": "3186769",
    "end": "3198439"
  },
  {
    "text": "between my eyesight and my touch bugs I",
    "start": "3217009",
    "end": "3223970"
  },
  {
    "text": "think it's a combination of both you really want to use and I think that's also what what we do right like for",
    "start": "3223970",
    "end": "3229229"
  },
  {
    "text": "example the visual perception gives me clearly first of all it tells me where I have to put my head move my hand in",
    "start": "3229229",
    "end": "3235859"
  },
  {
    "text": "order to grasp it and it also gives me a pretty good prior on where I should grab it but then of course at some point",
    "start": "3235859",
    "end": "3241529"
  },
  {
    "text": "clearly it's touch that takes over right so I can clearly then close my eyes and I can do all of these things so I think",
    "start": "3241529",
    "end": "3247589"
  },
  {
    "text": "it's not either/or it's a combination of those clearly",
    "start": "3247589",
    "end": "3251989"
  },
  {
    "text": "not a lot and they're also Jeanette and the men people are doing some really cool work in that domain I think as",
    "start": "3255530",
    "end": "3263100"
  },
  {
    "text": "you've seen we we were able to we're also moving in this direction but it's clearly not soft yet and again one of",
    "start": "3263100",
    "end": "3269160"
  },
  {
    "text": "the problems also we just don't have robot hands that have touch sensing on the palm and everything we're still kind",
    "start": "3269160",
    "end": "3275580"
  },
  {
    "text": "of stuck to kind of fingertip kind of sensing on that side and then again I",
    "start": "3275580",
    "end": "3282390"
  },
  {
    "text": "think a lot of the work that we are seeing is kind of still pretty isolated way it's about like learning a",
    "start": "3282390",
    "end": "3288330"
  },
  {
    "text": "predictive model of kind of what happens if I rotate my finger and things like that or slip detection but I I haven't",
    "start": "3288330",
    "end": "3295410"
  },
  {
    "text": "really seen a lot of work so that you can really feel like yeah this is working for arbitrary objects or things",
    "start": "3295410",
    "end": "3301080"
  },
  {
    "text": "like that on Edwin in hand manipulation is also a problem yeah yeah that's an",
    "start": "3301080",
    "end": "3330270"
  },
  {
    "text": "interesting sexually with the one video I shot that was actually exactly that case wait this one here so we're",
    "start": "3330270",
    "end": "3340280"
  },
  {
    "text": "not your play yeah when we actually wanna grab the white coffee cup right",
    "start": "3344220",
    "end": "3350520"
  },
  {
    "text": "and we can segment the scene and then we can reason about these other segments and we can reason about how much are",
    "start": "3350520",
    "end": "3356730"
  },
  {
    "text": "they in the way of being able to grasp it and I think actually so on the one hand side what you could do is you could",
    "start": "3356730",
    "end": "3363270"
  },
  {
    "text": "do now the full complex planning where you say exactly in which order do I need",
    "start": "3363270",
    "end": "3368369"
  },
  {
    "text": "to pick which object in order to grab than this one and what might be the optimal ordering I think for many of",
    "start": "3368369",
    "end": "3375390"
  },
  {
    "text": "these scenes reasonably simple heuristics might actually work pretty well and I'm pretty sure that I am using",
    "start": "3375390",
    "end": "3381900"
  },
  {
    "text": "very simple heuristics where you just say okay here the object and you say well I can't grab it now this looks like",
    "start": "3381900",
    "end": "3388050"
  },
  {
    "text": "it's in the way so what you're doing then you just drive it and get it all the way and then you look again and you say hmm okay there's another one in the",
    "start": "3388050",
    "end": "3395250"
  },
  {
    "text": "way right so kind of simply your six where you just say take them away kind of one after the other might get us",
    "start": "3395250",
    "end": "3401130"
  },
  {
    "text": "actually pretty far already and of course then the idea would be that these heuristics you can train in very large",
    "start": "3401130",
    "end": "3409910"
  },
  {
    "text": "simulation so that the deep networks for example can decode these heuristics well",
    "start": "3409910",
    "end": "3415760"
  },
  {
    "text": "so I'm I'm not sure we need really very complicated all the rearrangement planning for that we can do it often",
    "start": "3415760",
    "end": "3426839"
  },
  {
    "text": "yeah so beyond so you're using it on the",
    "start": "3426839",
    "end": "3440910"
  },
  {
    "text": "robot and also are there other sensors they can intercept in the you ideally want to have that maybe today I",
    "start": "3440910",
    "end": "3451290"
  },
  {
    "text": "think you could also mention a combination but clearly for example for",
    "start": "3451290",
    "end": "3456700"
  },
  {
    "text": "the home setting where you don't want to put death cameras everywhere or if for",
    "start": "3456700",
    "end": "3462640"
  },
  {
    "text": "example if you want to get an object out of a drawer then clearly it's good to have a camera on the robot so that it can actually look in through the drawer",
    "start": "3462640",
    "end": "3468940"
  },
  {
    "text": "we also carry our cameras with us it turns out actually it's tricky where to place them right if you have of course",
    "start": "3468940",
    "end": "3475750"
  },
  {
    "text": "on the grip is one but if you want to have something closer to let's say a human head position on a robot on a",
    "start": "3475750",
    "end": "3482380"
  },
  {
    "text": "mobile base there it's actually pretty terrible because you have this fixed viewpoint and sometimes you want to be",
    "start": "3482380",
    "end": "3488350"
  },
  {
    "text": "able to kind of to lean over and look into something so it's very difficult actually to where to best place these",
    "start": "3488350",
    "end": "3493930"
  },
  {
    "text": "cameras in industrial settings you can imagine also putting more like cameras",
    "start": "3493930",
    "end": "3499060"
  },
  {
    "text": "outside because the industrial robots they don't have any any perception right",
    "start": "3499060",
    "end": "3504370"
  },
  {
    "text": "now on them which touch I think is I think the one I would love most having",
    "start": "3504370",
    "end": "3511060"
  },
  {
    "text": "much better like touch skin on this robot it's not only on the fingers but also on the arms and everywhere right",
    "start": "3511060",
    "end": "3516430"
  },
  {
    "text": "and people are looking at that but it's still I don't think it's ready for prime time yet there will be my dream sensor",
    "start": "3516430",
    "end": "3525090"
  },
  {
    "text": "[Applause]",
    "start": "3527520",
    "end": "3531699"
  }
]