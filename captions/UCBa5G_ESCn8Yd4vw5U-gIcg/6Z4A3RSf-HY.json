[
  {
    "start": "0",
    "end": "97000"
  },
  {
    "text": "Okay. Hi everyone. Let's get started [NOISE] Okay.",
    "start": "4850",
    "end": "11430"
  },
  {
    "text": "So, so for today's lecture, what we're gonna do is look at the topic of having Tree Recursive Neural Networks.",
    "start": "11430",
    "end": "20025"
  },
  {
    "text": "I mean, this is actually, uh, a topic which I feel especially fond of and attached to,",
    "start": "20025",
    "end": "26660"
  },
  {
    "text": "because actually when we started doing deep learning for NLP here at Stanford in 2010,",
    "start": "26660",
    "end": "32975"
  },
  {
    "text": "really for the sort of period from 2010 to 2015, the dominant set of ideas that we were working on was this topic of how you",
    "start": "32975",
    "end": "42320"
  },
  {
    "text": "could build a recur- recursive tree structure into neural networks. So in a way, it's kind of funny that I'm only getting to it now.",
    "start": "42320",
    "end": "50645"
  },
  {
    "text": "I mean, there are sort of reasons for that, but I think there are a bunch of interesting ideas",
    "start": "50645",
    "end": "56719"
  },
  {
    "text": "here which relate closely to linguistic structure, and so it's good stuff to have seen.",
    "start": "56720",
    "end": "62720"
  },
  {
    "text": "But in practice, um, these ideas have proven kind of hard to scale",
    "start": "62720",
    "end": "68105"
  },
  {
    "text": "and not necessarily to work better in practice than the kind of things that we've spent more time on",
    "start": "68105",
    "end": "75140"
  },
  {
    "text": "meaning things like looking at LSTMs and looking at transformers, and things like that.",
    "start": "75140",
    "end": "80570"
  },
  {
    "text": "And so that's kinda why we sort of shunted them towards the end of the curriculum.",
    "start": "80570",
    "end": "85860"
  },
  {
    "text": "But I want to sort of say something about the motivations, and the ways you can build tree structures,",
    "start": "85860",
    "end": "91189"
  },
  {
    "text": "and neural networks, and look at some of the possibilities, um, we explored um, in during this class.",
    "start": "91190",
    "end": "98170"
  },
  {
    "text": "Um, another fact about this class is actually this is the last class I'm going to give.",
    "start": "98170",
    "end": "104689"
  },
  {
    "text": "Um, so two more classes next week. Don't forget about next week, um, CS224N classes, um,",
    "start": "104690",
    "end": "112275"
  },
  {
    "text": "but on Tuesday, um, we've gotten the final invited speaker, ,",
    "start": "112275",
    "end": "117280"
  },
  {
    "text": "who's a great speaker and has tons of interesting stuff to say about fairness and ethics in NLP and AI.",
    "start": "117280",
    "end": "124865"
  },
  {
    "text": "And then for the final lecture, one of my- another of my PhD students  is gonna give that and talk about some of the recent,",
    "start": "124865",
    "end": "133269"
  },
  {
    "text": "what's been happening in deep learning in 2018, '19, of some of the sort of recent developments in NLP and deep learning. Um,",
    "start": "133270",
    "end": "140974"
  },
  {
    "text": "so, um, let's- I'll say my farewells at the end of this one. Um, so hopefully, everyone has submitted, um,",
    "start": "140975",
    "end": "148730"
  },
  {
    "text": "their, um, milestone for their final project. If you haven't, you should really begin your milestone in- um,",
    "start": "148730",
    "end": "157310"
  },
  {
    "text": "you know, it's inevitable that somewhere around here, there start to be problems that people have the situation that nothing works,",
    "start": "157310",
    "end": "166290"
  },
  {
    "text": "and everything is too slow, and you panic. Um, and, um, this happens.",
    "start": "166290",
    "end": "172095"
  },
  {
    "text": "Um, I wish you luck, of course. I mean, what can you do about it? I mean, it can be really hard when you have things that don't work as to work out,",
    "start": "172095",
    "end": "180784"
  },
  {
    "text": "why they don't work, and how to fix them. I mean, I think often the best thing to do is really to go back to something",
    "start": "180785",
    "end": "188660"
  },
  {
    "text": "simple that you can get working and to work forward from there again.",
    "start": "188660",
    "end": "194405"
  },
  {
    "text": "It also really helps to have really small data sets. I really recommend the strategy of sort of having a 10-item,",
    "start": "194405",
    "end": "203540"
  },
  {
    "text": "or 20-item data set and checking that your model works perfectly, over-trains to 100 percent accuracy on that kind of data",
    "start": "203540",
    "end": "211319"
  },
  {
    "text": "set saves you huge amounts of time, and it's sort of after you've gotten something simple working on a small amount of data,",
    "start": "211320",
    "end": "219170"
  },
  {
    "text": "that's the right time to sort of then, um, expand forward again.",
    "start": "219170",
    "end": "224600"
  },
  {
    "text": "Um, you should definitely always make sure that you can completely overfit on your training data set.",
    "start": "224600",
    "end": "229970"
  },
  {
    "text": "That's sort of, um, not quite a proof, but it's at least a first good requirement for your model being implemented properly.",
    "start": "229970",
    "end": "236780"
  },
  {
    "text": "Um, you, you know part of the trick of being a successful deep learning researcher is actually",
    "start": "236780",
    "end": "243920"
  },
  {
    "text": "managing to get things done and not wasting a ton of time. And so it definitely always helps just to be, you know,",
    "start": "243920",
    "end": "250300"
  },
  {
    "text": "plotting as you go along your training and dev errors so that you can sort of tell if things are working,",
    "start": "250300",
    "end": "256160"
  },
  {
    "text": "or if things aren't working, and you should abandon and start again with a new experiment, tha- that just things like that save you hours and get you, uh, more done.",
    "start": "256160",
    "end": "265505"
  },
  {
    "text": "And so then once things are working, there's sort of a whole bunch of things to make it work better. There's regularization with L2 and Dropout,",
    "start": "265505",
    "end": "273409"
  },
  {
    "text": "there's time to do hyperparameter search, um, and, you know,",
    "start": "273410",
    "end": "278530"
  },
  {
    "text": "often doing these things and make quite a lot of difference to what your final results are and so it's good to have time to do those things.",
    "start": "278530",
    "end": "286820"
  },
  {
    "text": "But clearly, you want to get things, um, working first before you go on to that, um, and sort of really encourage people to still",
    "start": "286820",
    "end": "294380"
  },
  {
    "text": "stop by in office hours if you've got any problems, and we'll try our best to help out here within",
    "start": "294380",
    "end": "300530"
  },
  {
    "text": "the limitations of what we can do from just being hit cold with problems. Okay, um, yeah.",
    "start": "300530",
    "end": "308240"
  },
  {
    "start": "305000",
    "end": "465000"
  },
  {
    "text": "So, I wanted to sort of just say some general remarks about, um,",
    "start": "308240",
    "end": "313595"
  },
  {
    "text": "language and theories of language, um, that, in the context that motivate these tree recursive networks.",
    "start": "313595",
    "end": "322789"
  },
  {
    "text": "Um, so this is an art installation at Carnegie Mellon University. And as an NLP person,",
    "start": "322790",
    "end": "329150"
  },
  {
    "text": "I really love this art installation. Um, so we need better art installations around the Stanford School of Engineering.",
    "start": "329150",
    "end": "336805"
  },
  {
    "text": "Um, so this is the bag-of-words art Installation. There's the bag with a lot of words in it.",
    "start": "336805",
    "end": "342710"
  },
  {
    "text": "And you see down here, there were the stop words, the the, and the us, that had fallen out of the bag,",
    "start": "342710",
    "end": "349135"
  },
  {
    "text": "and are represented on the ground as the stop words. Beautiful artwork, right? So, um,",
    "start": "349135",
    "end": "355729"
  },
  {
    "text": "one of the interesting things that has been found about NLP models of language,",
    "start": "355730",
    "end": "362690"
  },
  {
    "text": "and I think this is even more true in the deep learning world than it used to be previously is,",
    "start": "362690",
    "end": "368600"
  },
  {
    "text": "boy, you can do a lot with bag-of-words models, right? That you can just often get a lot of power by saying,",
    "start": "368600",
    "end": "375920"
  },
  {
    "text": "well, let's get our neural word vectors, we're gonna average them or max pool them, or something like this,",
    "start": "375920",
    "end": "381975"
  },
  {
    "text": "and do nothing more, and that gives me a pretty good sentence representation or document representation that I could use in a classifier or something.",
    "start": "381975",
    "end": "390034"
  },
  {
    "text": "And sometimes, you can do not much more than that and get even better. So people have done things like deep averaging networks where you're taking",
    "start": "390035",
    "end": "397909"
  },
  {
    "text": "the output of a bag-of-words model and sort of feeding it through a couple more layers and improving things.",
    "start": "397910",
    "end": "403475"
  },
  {
    "text": "So that is in complete distinction to what's been dominant in linguistics of looking at language structure.",
    "start": "403475",
    "end": "411560"
  },
  {
    "text": "That typically in linguistics the emphasis has been on identifying kind of",
    "start": "411560",
    "end": "418340"
  },
  {
    "text": "huge amounts of structure of linguistic utterances through very complex formalisms.",
    "start": "418340",
    "end": "425480"
  },
  {
    "text": "I guess this is sort of a bit of a picture of a Chomsky minimalism syntactic tree, and the one up at the top is a bit of a picture of head-driven phrase structure grammar.",
    "start": "425480",
    "end": "435259"
  },
  {
    "text": "Which was a theory that was predominantly, um, developed at Stanford in the '90s.",
    "start": "435260",
    "end": "441935"
  },
  {
    "text": "Um, but sort of very complex data structures and articulated structures used to describe linguistics.",
    "start": "441935",
    "end": "448895"
  },
  {
    "text": "And there's a huge gap between these two things. And you might think that, you know, surely,",
    "start": "448895",
    "end": "456259"
  },
  {
    "text": "there's some good points in the middle where we have a certain amount of structure, and that's going to help us do what we want.",
    "start": "456260",
    "end": "463460"
  },
  {
    "text": "And so in particular, um, that if we're wanting to semantically interpret language,",
    "start": "463460",
    "end": "469745"
  },
  {
    "start": "465000",
    "end": "684000"
  },
  {
    "text": "it seems like we don't just want to have word vectors, we want to have meanings of bigger phrases.",
    "start": "469745",
    "end": "476120"
  },
  {
    "text": "So here's the snowboarders leaping over a mogul, and a person on a snowboard jumps into the air.",
    "start": "476120",
    "end": "483169"
  },
  {
    "text": "And what we'd like to be able to say is that the snowboarder means basically the same thing as a person on the snowboard.",
    "start": "483170",
    "end": "490790"
  },
  {
    "text": "So we wanted to have these chunks of language which in linguistics will be constituent  phrases,",
    "start": "490790",
    "end": "497255"
  },
  {
    "text": "and say that they have a meaning, and we'd like to be able to compare their meaning. Now, we've looked at at least one tool that allows us to have chunks of language, right?",
    "start": "497255",
    "end": "506870"
  },
  {
    "text": "Because we looked at convolutional neural networks where you could take three words and make a representation of the convolutional neural network,",
    "start": "506870",
    "end": "514805"
  },
  {
    "text": "but the fundamental difference is that in human languages you have these chunks that have meaning,",
    "start": "514805",
    "end": "521254"
  },
  {
    "text": "that are of different sizes. So we'd like to say the snowboarder is pretty much semantically equivalent to a person on the snowboard,",
    "start": "521254",
    "end": "530120"
  },
  {
    "text": "but the top one is two words long, and the bottom one is five words long.",
    "start": "530120",
    "end": "535295"
  },
  {
    "text": "And so if we're gonna be able to do that, um, we somehow wanted to have these sort of constituent chunks and be",
    "start": "535295",
    "end": "543230"
  },
  {
    "text": "able to work with and represent them in neural networks. And that's sort of, um,",
    "start": "543230",
    "end": "548630"
  },
  {
    "text": "the central idea of what motivated some of the sort of tree structured neural networks that I'm about to show you.",
    "start": "548630",
    "end": "556235"
  },
  {
    "text": "There's another related thing that you might wanna think about is, you know,",
    "start": "556235",
    "end": "561380"
  },
  {
    "text": "a person on a snowboard, how do human beings manage to understand what that means?",
    "start": "561380",
    "end": "567950"
  },
  {
    "text": "And then a person on a snowboard jumps into the air, how does people manage to understand what that means?",
    "start": "567950",
    "end": "575654"
  },
  {
    "text": "And it sort of seems like the only possible answer to",
    "start": "575655",
    "end": "581030"
  },
  {
    "text": "this is what's normally referred to as the principle of compositionality.",
    "start": "581030",
    "end": "586595"
  },
  {
    "text": "That people know the word person, they know the word on, they know the word snowboard, therefore,",
    "start": "586595",
    "end": "592760"
  },
  {
    "text": "they can work out what on a snowboard means, um, and they can work out what person on a snowboard means by knowing",
    "start": "592760",
    "end": "600170"
  },
  {
    "text": "the meanings of components and putting them together into bigger pieces.",
    "start": "600170",
    "end": "605899"
  },
  {
    "text": "There's a f- there's a famous, um, applied mathematician statistician, um,",
    "start": "605900",
    "end": "612500"
  },
  {
    "text": "at Brown University, Stu Geman, and I guess the way he summarized this is, either the principle of compositionality is true, or God exists.",
    "start": "612500",
    "end": "621680"
  },
  {
    "text": "Um, for [LAUGHTER] which he was, um, well you can take that as- as you want but, you know,",
    "start": "621680",
    "end": "627199"
  },
  {
    "text": "um, I think what he meant was well, you know, you can just make these infinite number of",
    "start": "627200",
    "end": "632880"
  },
  {
    "text": "infinitely long sentences and human beings understand them, that it just has to be that people can know about",
    "start": "632880",
    "end": "639365"
  },
  {
    "text": "words and ways to combine meanings and-and make bigger meanings because, you know, how else could it possibly work that people could understand sentences.",
    "start": "639365",
    "end": "647945"
  },
  {
    "text": "And so we want to be able to do that. We want to be able to work out semantic compositions of smaller elements,",
    "start": "647945",
    "end": "654664"
  },
  {
    "text": "to work out the meanings of bigger pieces. And that this obviously isn't only a linguistic thing,",
    "start": "654664",
    "end": "661199"
  },
  {
    "text": "compositionality, um, appears in other places as well, right. So, um, if you want to understand how some piece of machinery works,",
    "start": "661200",
    "end": "670620"
  },
  {
    "text": "what you kind of wanna know is it has different sub-components. And if you can understand how",
    "start": "670620",
    "end": "676140"
  },
  {
    "text": "the different sub-components work and how they're fitted together, um, then you might have some understanding of how the whole scene works.",
    "start": "676140",
    "end": "684605"
  },
  {
    "start": "684000",
    "end": "714000"
  },
  {
    "text": "Um, and, um, compositionality seems to be wor- at work in vision as well.",
    "start": "684605",
    "end": "691170"
  },
  {
    "text": "So here is a scene and again it seems like this scene has parts. So there are little parts that go together, right.",
    "start": "691170",
    "end": "698650"
  },
  {
    "text": "So there are people that go together into a crowd of people, and there's a roof and a second floor and another bit of roof.",
    "start": "698650",
    "end": "704860"
  },
  {
    "text": "and a first floor that go together into a picture of this church. And so this is also kind of a compositional scene in which pieces go together.",
    "start": "704860",
    "end": "714274"
  },
  {
    "start": "714000",
    "end": "730000"
  },
  {
    "text": "So it sort of seems like certainly for language understanding, and then really for a lot of the other things that we use for intelligence,",
    "start": "714275",
    "end": "722810"
  },
  {
    "text": "that we somehow need to be able to understand bigger things from knowing about smaller parts.",
    "start": "722810",
    "end": "729335"
  },
  {
    "text": "Um, yeah, so computational- so the most- I mentioned this earlier,",
    "start": "729335",
    "end": "734870"
  },
  {
    "start": "730000",
    "end": "827000"
  },
  {
    "text": "sometime the most famous, um, linguist is Noam Chomsky at MIT and,",
    "start": "734870",
    "end": "740480"
  },
  {
    "text": "um, you know, really computational linguists, a lot of the time haven't been that friendly to, um,",
    "start": "740480",
    "end": "748355"
  },
  {
    "text": "linguistics linguists and in particular some of Noam Chomsky's, um, theories of language because really he's never been",
    "start": "748355",
    "end": "756250"
  },
  {
    "text": "sympathetic to the idea of machine learning. Or in general does some of the empirical ability to learn from data.",
    "start": "756250",
    "end": "764074"
  },
  {
    "text": "He's sort of always been, um, [NOISE] wanting to refuse to that exists. But, um, if we nevertheless look for a little bit of,",
    "start": "764075",
    "end": "771470"
  },
  {
    "text": "um, insight on that. Um, you know, this is a recent paper of Chomsky's with authors and that they're sort",
    "start": "771470",
    "end": "778115"
  },
  {
    "text": "of trying to give a version of what is unique about human language.",
    "start": "778115",
    "end": "783120"
  },
  {
    "text": "And essentially what they, um, zero in on is that well, if you're sort of looking at, you know,",
    "start": "783120",
    "end": "789910"
  },
  {
    "text": "humans versus other fairly intelligent creatures. They suggest that the defining difference of human beings, um,",
    "start": "789910",
    "end": "797790"
  },
  {
    "text": "is that they have this ability to model recursion. And so the- this paper argues that the- the singular distinction that allowed",
    "start": "797790",
    "end": "807449"
  },
  {
    "text": "language to develop in human beings was that we could put together smaller parts to make bigger things,",
    "start": "807450",
    "end": "813555"
  },
  {
    "text": "in a recursive process and that that was the sort of defining new ability. Um, not sure I- not sure I believe that or not,",
    "start": "813555",
    "end": "821135"
  },
  {
    "text": "um, [LAUGHTER] you can decide what you think. But what I think, um,",
    "start": "821135",
    "end": "826325"
  },
  {
    "text": "is certainly the case is that- it's just incontrovertible that",
    "start": "826325",
    "end": "831390"
  },
  {
    "start": "827000",
    "end": "1090000"
  },
  {
    "text": "the structure of human language sentences have these pieces,",
    "start": "831390",
    "end": "836990"
  },
  {
    "text": "um, constituents that then form together hierarchically or recursively into bigger pieces as you go up in the tree.",
    "start": "836990",
    "end": "845555"
  },
  {
    "text": "And in particular you get this recursion where you get a little noun phrase meat,",
    "start": "845555",
    "end": "851190"
  },
  {
    "text": "which then appears in a bigger noun phrase like spaghetti with meat. And you can repeat that several times,",
    "start": "851190",
    "end": "857640"
  },
  {
    "text": "giving you a recursive structure. And I have an example of that in blue up at the top.",
    "start": "857640",
    "end": "862710"
  },
  {
    "text": "So the person standing next to the man from the company that purchased the firm that you used to work at,",
    "start": "862710",
    "end": "868035"
  },
  {
    "text": "um, that whole thing is a big noun phrase. Um, but inside that there's a noun phrase,",
    "start": "868035",
    "end": "876279"
  },
  {
    "text": "the man from the company that purchased the firm that you used to work at, which is another big noun phrase.",
    "start": "876280",
    "end": "881880"
  },
  {
    "text": "And well inside that, um, there are smaller noun phrases like,",
    "start": "881880",
    "end": "887260"
  },
  {
    "text": "the company that purchased the firm you used to work at. But, you know, it's still got inside that noun phrases like,",
    "start": "887260",
    "end": "893190"
  },
  {
    "text": "the firm that you used to work at. And actually even that's got it inside, the smaller noun phrase,",
    "start": "893190",
    "end": "899365"
  },
  {
    "text": "which is just the word you. So an individual pronoun is also a noun phrase.",
    "start": "899365",
    "end": "906944"
  },
  {
    "text": "Um, so just kind of structuring of language where you get this sort of",
    "start": "906945",
    "end": "913565"
  },
  {
    "text": "hierarchical structure and the same kind of things inside them. I think that's just sort of totally, totally correct.",
    "start": "913565",
    "end": "920465"
  },
  {
    "text": "Um, the- the claim then that, you know, our language is recursive, I mean,",
    "start": "920465",
    "end": "926210"
  },
  {
    "text": "in a formal sense is not quite clear that that's, uh, it's a clear thing.",
    "start": "926210",
    "end": "933365"
  },
  {
    "text": "And that's the reason- to say something is recursive, it has to repeat out to infinity, right.",
    "start": "933365",
    "end": "939500"
  },
  {
    "text": "So as soon as you put any bound on something, and you say, \"Look that's a noun phrase you just gave me with five levels of nesting.\"",
    "start": "939500",
    "end": "948819"
  },
  {
    "text": "That's pretty implausible that someone is going to say that. And so as soon as you sort of,",
    "start": "948819",
    "end": "954630"
  },
  {
    "text": "um, want to make an argument like, okay even if they said that, no one is going to say a noun phrase with 10 levels of nesting.",
    "start": "954630",
    "end": "961110"
  },
  {
    "text": "And if you put some hard limit on it like that, um, then in some sense it's not truly recursive because it doesn't go out to infinity.",
    "start": "961110",
    "end": "968970"
  },
  {
    "text": "Um, but, you know, regardless what you think about that, that doesn't negate the basic argument that you get this hierarchical",
    "start": "968970",
    "end": "976090"
  },
  {
    "text": "structuring with the same kinds of things like noun phrases, sentences, verb phrases, appearing inside each other in a way that has no clear bound.",
    "start": "976090",
    "end": "986780"
  },
  {
    "text": "Like to the extent that I show you a complex sentence, you can say I can make that an even bigger, more complex sentence by putting it inside,",
    "start": "986780",
    "end": "995430"
  },
  {
    "text": "you said to me that, and then saying, um, my sentence, right. So that's the sense in which it does appear to be a recursive generative process,",
    "start": "995430",
    "end": "1004880"
  },
  {
    "text": "even though practically there are limits to how complex sentences people say.",
    "start": "1004880",
    "end": "1010510"
  },
  {
    "text": "And so that's the kind of structure that gets captured in these constituency, um, structure trees.",
    "start": "1010510",
    "end": "1017605"
  },
  {
    "text": "So before the early time when we talked about parsing and you guys did some of it,",
    "start": "1017605",
    "end": "1022779"
  },
  {
    "text": "I emphasized dependency parsing. Um, but the other kind of parsing which is actually",
    "start": "1022780",
    "end": "1028220"
  },
  {
    "text": "the kind that the models I'm going to talk about today was using, was this idea of what's often called constituency",
    "start": "1028220",
    "end": "1035605"
  },
  {
    "text": "parsing or linguists often call it phrase structure grammars, um, or in sort of computer science formal language theory.",
    "start": "1035605",
    "end": "1044635"
  },
  {
    "text": "These are context-free grammars, where, um, we're having, um, these, um, non-terminals like noun phrase,",
    "start": "1044635",
    "end": "1052129"
  },
  {
    "text": "and verb phrase, and that's inside another noun phrases, it's inside another verb phrase, which is inside more verb phrases,",
    "start": "1052129",
    "end": "1058878"
  },
  {
    "text": "heading up the sentence. And so these are our constituency grammars.",
    "start": "1058879",
    "end": "1063895"
  },
  {
    "text": "And when we've occasionally mentioned the Penn Treebank tree, this was kind of an original Penn Treebank tree which is basically, uh,",
    "start": "1063895",
    "end": "1072495"
  },
  {
    "text": "phrase structure grammar like, this with sort of various extra annotations, um, put on the nodes.",
    "start": "1072495",
    "end": "1078850"
  },
  {
    "text": "Okay, so what did seem- what- what do you- to capture some of these properties,",
    "start": "1078850",
    "end": "1084775"
  },
  {
    "text": "it seems like we'd like to have a neural model that can make use of some of this same kind of tree structure.",
    "start": "1084775",
    "end": "1091405"
  },
  {
    "start": "1090000",
    "end": "1143000"
  },
  {
    "text": "And so what we'd like to do for working out semantic similarity of constituents,",
    "start": "1091405",
    "end": "1097720"
  },
  {
    "text": "is we want to not only have a word vector space like we started off with right at the beginning of the quarter,",
    "start": "1097720",
    "end": "1105185"
  },
  {
    "text": "but we'd like to be able to take bigger constituents like noun phrases, the country of my birth,",
    "start": "1105185",
    "end": "1111565"
  },
  {
    "text": "and the place where I was born, and also give them a meaning. And so it seems like what we'd like to do is have a method of",
    "start": "1111565",
    "end": "1119370"
  },
  {
    "text": "computing the meaning of any phrase in a compositional manner, such that the end result is also that",
    "start": "1119370",
    "end": "1127320"
  },
  {
    "text": "these phrases could be stuck inside our vector space models.",
    "start": "1127320",
    "end": "1132409"
  },
  {
    "text": "So we're still going to stick with our vector space semantics of phrases, and we wanna comp- compute the meanings of phrases.",
    "start": "1132410",
    "end": "1139625"
  },
  {
    "text": "And so then the question is, how could we go about doing that?",
    "start": "1139625",
    "end": "1144715"
  },
  {
    "start": "1143000",
    "end": "1203000"
  },
  {
    "text": "And well answer number one is we're gonna use the principle of compositionality since we're sure it's right,",
    "start": "1144715",
    "end": "1151375"
  },
  {
    "text": "and so, well, what the principle of compositionality essentially says, if you want to work out the meaning- or here it says of a sentence.",
    "start": "1151375",
    "end": "1160170"
  },
  {
    "text": "But the meaning of any phrase, any constituent is you're going to build it by knowing the meanings of its words,",
    "start": "1160170",
    "end": "1169049"
  },
  {
    "text": "and then having rules that combine these meanings. So starting off with the country of my birth,",
    "start": "1169050",
    "end": "1174279"
  },
  {
    "text": "I should be able to calculate a meaning of my birth, and meaning of the country, and meaning of of the- my birth and then a meaning of the country of my birth.",
    "start": "1174280",
    "end": "1183540"
  },
  {
    "text": "So we'd have meaning composition rules which will let us calculate meanings upwards for larger constituents or sentences.",
    "start": "1183540",
    "end": "1192520"
  },
  {
    "text": "Um, so that seems kind of the right thing to do. And so then the question is well, can we, um,",
    "start": "1192520",
    "end": "1200140"
  },
  {
    "text": "then build a model of how to do that? Well, here's sort of a straightforward way of doing this, okay.",
    "start": "1200140",
    "end": "1208630"
  },
  {
    "text": "So we- we have word vectors for the words that we've calculated.",
    "start": "1208630",
    "end": "1216105"
  },
  {
    "text": "And what we'd like to do is work out, um- Then a meaning representation of this sentence.",
    "start": "1216105",
    "end": "1223625"
  },
  {
    "text": "And at this point we sort of have two things to do. We have parsing to do of working out what's the right structure of the sentence,",
    "start": "1223625",
    "end": "1231590"
  },
  {
    "text": "and then we have meaning computation to do of working out what is the meaning representation of this sentence.",
    "start": "1231590",
    "end": "1239515"
  },
  {
    "text": "Um, so for parsing we'd sort of be building, sort of noun phrase, prepositional phrase,",
    "start": "1239515",
    "end": "1245280"
  },
  {
    "text": "verb phrase, sentence kind of units, um, to get \"the cat sat on the mat\", and then will, what,",
    "start": "1245280",
    "end": "1251380"
  },
  {
    "text": "we, if we had that, we could then run some kind of meaning computation program,",
    "start": "1251380",
    "end": "1257220"
  },
  {
    "text": "and give us sort of a vector space, um, meaning of these sentences. So that's kind of what we want,",
    "start": "1257220",
    "end": "1262970"
  },
  {
    "text": "is to do both of those, and in a little bit I'll show you an example of the kind of one way that you go about approaching that.",
    "start": "1262970",
    "end": "1270660"
  },
  {
    "text": "But before I do that, just sort of stepping back for a moment as to what's different here, right?",
    "start": "1270660",
    "end": "1275934"
  },
  {
    "text": "That here we had our recurrent neural network which in some sense has been",
    "start": "1275935",
    "end": "1281630"
  },
  {
    "text": "our workhorse tool in this class up to now, and it gives you, it gives you a representation of the meaning of the country of my birth sort of,",
    "start": "1281630",
    "end": "1290605"
  },
  {
    "text": "you could either say that's the meaning of, um, the country of my birth, or we talked about other tricks like,",
    "start": "1290605",
    "end": "1296980"
  },
  {
    "text": "doing max pooling across all of these, or you could have a separate node out here,",
    "start": "1296980",
    "end": "1302820"
  },
  {
    "text": "which so does attention over these. So it does give you a sort of representation, um,",
    "start": "1302820",
    "end": "1309240"
  },
  {
    "text": "of the meaning of this, of any, um, sub-sequence of words as well.",
    "start": "1309240",
    "end": "1314784"
  },
  {
    "text": "Um, but they, they're sort of different, right? That this what, the top, the tree recursive neural network,",
    "start": "1314785",
    "end": "1321435"
  },
  {
    "text": "it requires a sentence or any kind of phrase to have a tree structure.",
    "start": "1321435",
    "end": "1327775"
  },
  {
    "text": "So we know what its component parts are, but then we're working out meaning representations",
    "start": "1327775",
    "end": "1334935"
  },
  {
    "text": "for the phrase that is sensitive to what its syntactic structure is,",
    "start": "1334935",
    "end": "1340800"
  },
  {
    "text": "that how the words go together to build phrases. Whereas for the recurrent neural network we're",
    "start": "1340800",
    "end": "1347875"
  },
  {
    "text": "just in an oblivious way running a sequence model along, and say and compute things,",
    "start": "1347875",
    "end": "1353500"
  },
  {
    "text": "and in the obvious, it doesn't in any obvious way give a meaning representation of, of my birth, or my birth contained inside that.",
    "start": "1353500",
    "end": "1361970"
  },
  {
    "text": "We sort of only have a meaning representation for the whole sequence, whereas if we're doing things this way, um,",
    "start": "1361970",
    "end": "1368679"
  },
  {
    "text": "we do have meaning representations for the different meaningful parts of the sentence.",
    "start": "1368680",
    "end": "1374705"
  },
  {
    "text": "Okay. That makes sense of what we're trying to do? Okay. So how could we do,",
    "start": "1374705",
    "end": "1381625"
  },
  {
    "text": "go about doing that? Um, well, the idea of how we could go about doing that is,",
    "start": "1381625",
    "end": "1388385"
  },
  {
    "text": "if we work bottom-up, at the very bottom we have word vectors,",
    "start": "1388385",
    "end": "1394245"
  },
  {
    "text": "and so we want to recursively compute the meaning of bigger constituents.",
    "start": "1394245",
    "end": "1399890"
  },
  {
    "text": "So if we wanted to compute the meaning of \"on the mat\" what we can do is say,",
    "start": "1399890",
    "end": "1405280"
  },
  {
    "text": "well, we have, already have a meaning representation of, on and mat. So if we could feed those into a neural network, because that's our",
    "start": "1405280",
    "end": "1413910"
  },
  {
    "text": "one tool, we could maybe get out of it two things. We could get out of it a goodness score.",
    "start": "1413910",
    "end": "1422054"
  },
  {
    "text": "So this is what we're going to use for parsing. We're going to say, \"Do you belie- do you believe you can put together \"on\" and the",
    "start": "1422055",
    "end": "1429450"
  },
  {
    "text": "\"mat\" to form a good constituent that's part of a parse tree?",
    "start": "1429450",
    "end": "1435054"
  },
  {
    "text": "And this will be a big positive number if the answer is true, and negative if it's not true, and then we have a meaning composition device,",
    "start": "1435055",
    "end": "1443304"
  },
  {
    "text": "which says, \"Okay, um, if you put together these two things, what would be the meaning representation of what we put together?\"",
    "start": "1443305",
    "end": "1451965"
  },
  {
    "text": "And so this is the first model that we explored which was doing this in a pretty simple way, right?",
    "start": "1451965",
    "end": "1459659"
  },
  {
    "text": "So here was our meaning composition, um, device that we concatenated the two vectors of the constituents,",
    "start": "1459660",
    "end": "1467665"
  },
  {
    "text": "we multiply them by a matrix, add a bias as usual, put it through a tan h. Uh,",
    "start": "1467665",
    "end": "1473955"
  },
  {
    "text": "this work is old enough, it's sort of before things, like, ReLUs became popular, but maybe it's better to have a tan h anyway, um, fit more like,",
    "start": "1473955",
    "end": "1481735"
  },
  {
    "text": "a recurrent neural network, and so this was our meaning composition that gave the meaning of the parent.",
    "start": "1481735",
    "end": "1487580"
  },
  {
    "text": "And then to the side, what the score of it was as to whether this was a good phrase, we were taking that parent vector representation,",
    "start": "1487580",
    "end": "1495370"
  },
  {
    "text": "and multiplying it by another vector, and that was giving us out a number.",
    "start": "1495370",
    "end": "1501990"
  },
  {
    "text": "Um, if you think about it a bit while we're doing this, you might think that this isn't quite a perfect model of meaning composition,",
    "start": "1502100",
    "end": "1510699"
  },
  {
    "text": "and later on in the class I'll talk about some more complex models, um, that we then started to explore.",
    "start": "1510699",
    "end": "1517880"
  },
  {
    "text": "Um, but this is sort of enough to get us going, and this gave us a way of building",
    "start": "1517880",
    "end": "1524519"
  },
  {
    "text": "a recursive neural network parser which both found parsers,",
    "start": "1524520",
    "end": "1529805"
  },
  {
    "text": "and worked out a meaning representation for them. And so the way we did this was in the simplest possible way really,",
    "start": "1529805",
    "end": "1537565"
  },
  {
    "start": "1532000",
    "end": "1626000"
  },
  {
    "text": "which was to have a greedy parser. So if we start off with the \"cat sat on the mat\",",
    "start": "1537565",
    "end": "1542620"
  },
  {
    "text": "what we could do is say, well, maybe you should join \"the\" and \"cat\" together. Let's try that.",
    "start": "1542620",
    "end": "1548220"
  },
  {
    "text": "Run it through our neural network, it'll get a score and a meaning representation,",
    "start": "1548220",
    "end": "1553315"
  },
  {
    "text": "and while we could try doing that for \"cat\" and \"sat\" we could try doing it for \"sat\" and \"on\".",
    "start": "1553315",
    "end": "1558634"
  },
  {
    "text": "We could try doing it for \"on\" and \"the\" we could try doing it for \"the\" and \"mat\". And then at this point we'd say, okay, well the,",
    "start": "1558635",
    "end": "1566665"
  },
  {
    "text": "the best phrase that we can make combining these word vectors is the one for \"the cat\".",
    "start": "1566665",
    "end": "1572725"
  },
  {
    "text": "So let's just commit to that one, and it has this semantic representation, and at this point we can essentially repeat.",
    "start": "1572725",
    "end": "1580825"
  },
  {
    "text": "Now, all the work we did over there we can just reuse because nothing has changed, but we can also consider now joining the \"cat\"",
    "start": "1580825",
    "end": "1588850"
  },
  {
    "text": "as a constituent with \"sat\" and get a score for that. And so at this point we decide, okay,",
    "start": "1588850",
    "end": "1594929"
  },
  {
    "text": "the mat is the best constituent to build, commit to that, calculate a meaning representation for \"on the mat\".",
    "start": "1594930",
    "end": "1601455"
  },
  {
    "text": "That looks good, commit to that, and kind of keep on chugging up, and so we've got a mechanism for sort of choosing a parse of a sentence in a,",
    "start": "1601455",
    "end": "1611200"
  },
  {
    "text": "in a greedy manner. But, you know, when we looked at the dependency parsing, we're also doing that greedily, right?",
    "start": "1611200",
    "end": "1617230"
  },
  {
    "text": "Um, and coming up with a meaning representation. Okay. So that was our first model of having a tree recursive neural network,",
    "start": "1617230",
    "end": "1626105"
  },
  {
    "text": "and using it for parsing. Um, there are a few more details here,",
    "start": "1626105",
    "end": "1632630"
  },
  {
    "text": "some of which probably aren't super, um, important at this point, right?",
    "start": "1632630",
    "end": "1638335"
  },
  {
    "text": "So we could score a tree by summing the scores at each node, um, for working out,",
    "start": "1638335",
    "end": "1645290"
  },
  {
    "text": "for the optimization we were working out, we're using this kind of max-margin loss that we've looked at in other places.",
    "start": "1645290",
    "end": "1653544"
  },
  {
    "text": "Um, the simplest way to do things is completely greedily. You just, um, find the best local decision at each point,",
    "start": "1653545",
    "end": "1661825"
  },
  {
    "text": "and make that structure, and keep on going. But if you wanna do things a bit better, and we explored this,",
    "start": "1661825",
    "end": "1667100"
  },
  {
    "text": "um, you could say, um, we could do beam search. We could explore out several good ways of merging,",
    "start": "1667100",
    "end": "1674020"
  },
  {
    "text": "and then decide later higher up the tree as to which was the best way, um, to merge.",
    "start": "1674020",
    "end": "1679480"
  },
  {
    "text": "Um, we haven't talked about it in this class, but just to mention, um,",
    "start": "1679480",
    "end": "1685240"
  },
  {
    "text": "something in case people have seen it is, um, traditional constituency parsing where you have symbols here,",
    "start": "1685240",
    "end": "1692545"
  },
  {
    "text": "like, NP or VP. Um, there exist efficient dynamic programming algorithms where you can",
    "start": "1692545",
    "end": "1699240"
  },
  {
    "text": "find the optimal parse of a sentence in polynomial time.",
    "start": "1699240",
    "end": "1704660"
  },
  {
    "text": "So in, in cubic time. So if you have a regular context-free grammar, and well, so regular probabilistic context-free grammar, um,",
    "start": "1704660",
    "end": "1712615"
  },
  {
    "text": "and if you want to know what is the best parse of the sentence according to the probabilistic context-free grammar,",
    "start": "1712615",
    "end": "1718385"
  },
  {
    "text": "you can write a cubic time dynamic programming algorithm and you can find it. That's good. And in the old days of CS224N,",
    "start": "1718385",
    "end": "1727560"
  },
  {
    "text": "um, before neural networks we used to have everyone do that. The, the most, the most brain-breaking assignment of the old CS224N",
    "start": "1727560",
    "end": "1737240"
  },
  {
    "text": "was writing this dynamic program to do context-free grammar parsing of a sentence.",
    "start": "1737240",
    "end": "1742380"
  },
  {
    "text": "Um, the slightly sad fact is, once you go to these kind of neural network representations,",
    "start": "1742380",
    "end": "1748809"
  },
  {
    "text": "you can't write clever dynamic programming algorithms anymore, because clever dynamic programming algorithms only work when you have symbols",
    "start": "1748810",
    "end": "1758094"
  },
  {
    "text": "from a reasonably small set for your non-terminals because if that's the case,",
    "start": "1758094",
    "end": "1763580"
  },
  {
    "text": "you can, you kind of have collisions, right? You have lots of ways of parsing stuff lower down,",
    "start": "1763580",
    "end": "1769075"
  },
  {
    "text": "which kind of, uh, turn out to be different ways to make a noun phrase, or different ways to make a prepositional phrase,",
    "start": "1769075",
    "end": "1775850"
  },
  {
    "text": "and therefore you can save work with dynamic programming. If you've got a model like this, since everything that you build is going through layers of neural network,",
    "start": "1775850",
    "end": "1784490"
  },
  {
    "text": "and you've got a meaning representation, some high-dimensional vector, things are never going to collide,",
    "start": "1784490",
    "end": "1789760"
  },
  {
    "text": "and so you can never save work by doing dynamic programming. And so, um, you're either doing exponential work to explore out everything,",
    "start": "1789760",
    "end": "1798519"
  },
  {
    "text": "or else you're using some kind of beam to explore a bunch of likely stuff.",
    "start": "1798520",
    "end": "1803950"
  },
  {
    "text": "Yeah. Um, we actually also applied this,",
    "start": "1804090",
    "end": "1809260"
  },
  {
    "text": "um, to vision at the same time. So it wasn't just sort of completely",
    "start": "1809260",
    "end": "1814330"
  },
  {
    "start": "1812000",
    "end": "1872000"
  },
  {
    "text": "a vague motivation of, um, visual scenes have parts that we actually started exploring that well you could take, um,",
    "start": "1814330",
    "end": "1823255"
  },
  {
    "text": "these pieces of scenes and then work out, um, representations for scenes using a similar form of compositionality.",
    "start": "1823255",
    "end": "1832885"
  },
  {
    "text": "And so in particular, um, there was sort of this dataset that was being used for, um,",
    "start": "1832885",
    "end": "1840715"
  },
  {
    "text": "multi-class segmentation in vision, where you start off with very small patches and then you wanna combine them",
    "start": "1840715",
    "end": "1848830"
  },
  {
    "text": "up into parts of a scene of sort of recognizing which part of the picture was the building,",
    "start": "1848830",
    "end": "1854170"
  },
  {
    "text": "the sky, the road, various other classes. And we were actually at the time able to do this really rather well, um,",
    "start": "1854170",
    "end": "1862435"
  },
  {
    "text": "using one of these tree recursive structured neural networks better than preceding work in vision had done in the late 2000s decade.",
    "start": "1862435",
    "end": "1871435"
  },
  {
    "text": "Okay. So how can we- how can we build neural networks,",
    "start": "1871435",
    "end": "1877225"
  },
  {
    "start": "1872000",
    "end": "2152000"
  },
  {
    "text": "um, that do this kind of stuff? Um, so when- when we started off exploring these tree structured neural networks, um,",
    "start": "1877225",
    "end": "1885730"
  },
  {
    "text": "we thought that this was a cool original idea and no one had worked on tree structured neural networks successfully before.",
    "start": "1885730",
    "end": "1893304"
  },
  {
    "text": "Um, but it turned out we were wrong, that there were a couple of Germans in the mid-1990s,",
    "start": "1893305",
    "end": "1899560"
  },
  {
    "text": "um, had actually started looking at tree structured neural networks and had worked out,",
    "start": "1899560",
    "end": "1904765"
  },
  {
    "text": "um, the math of them. So corresponding to the backpropagation through time algorithm, um, that Abby talked about when we were doing recurrent neural networks.",
    "start": "1904765",
    "end": "1912865"
  },
  {
    "text": "They worked out the tree structured case which they called backpropagation, um, through structure.",
    "start": "1912865",
    "end": "1918310"
  },
  {
    "text": "Um, there are several slides on this in the slides but I think I'm gonna sort of skip them.",
    "start": "1918310",
    "end": "1927505"
  },
  {
    "text": "If anyone wants to look at them, they're on the web and you can look at them. I mean, there isn't actually anything that's new.",
    "start": "1927505",
    "end": "1934720"
  },
  {
    "text": "So if you remember with- with bad scarring or something that was early lectures of this class of working out,",
    "start": "1934720",
    "end": "1941725"
  },
  {
    "text": "um, the derivatives of neural networks and how it worked with recurrent neural networks. It's sort of the same, right.",
    "start": "1941725",
    "end": "1948130"
  },
  {
    "text": "You have this recurrent matrix at different levels of tree structure. You're summing the derivatives of everywhere it turns up.",
    "start": "1948130",
    "end": "1957220"
  },
  {
    "text": "The only difference is sort of because we now have tree structure, you're sort of splitting things downwards.",
    "start": "1957220",
    "end": "1963040"
  },
  {
    "text": "Um, so yes. So forward prop we kind of compute it forwards.",
    "start": "1963040",
    "end": "1968560"
  },
  {
    "text": "And then when we're doing back prop, when we've had the backward propagation we have the error signal coming from above.",
    "start": "1968560",
    "end": "1975910"
  },
  {
    "text": "We then, um, combine it, um, with the calculations at this node. And then we're sort of sending it back in a tree structure",
    "start": "1975910",
    "end": "1983350"
  },
  {
    "text": "down to each of the branches underneath us. So that was our first version of things and we got some decent results.",
    "start": "1983350",
    "end": "1991960"
  },
  {
    "text": "We got this good vision results that I showed you and it sort of seemed to do,",
    "start": "1991960",
    "end": "1996985"
  },
  {
    "text": "um, some good for, um, language both for parsing and doing- We had",
    "start": "1996985",
    "end": "2003030"
  },
  {
    "text": "some results I haven't actually included here of sort of doing paraphrase, um, judgment between sentences and it- it modeled things, um, fairly well.",
    "start": "2003030",
    "end": "2013545"
  },
  {
    "text": "But once we started thinking about it more it seemed like that very simple neural net function couldn't possibly",
    "start": "2013545",
    "end": "2021780"
  },
  {
    "text": "compute the kind of meanings that we wanted to compute for sentence meanings.",
    "start": "2021780",
    "end": "2026955"
  },
  {
    "text": "And so we then sort of set about trying to come up with some more complex ways of working out kind",
    "start": "2026955",
    "end": "2032970"
  },
  {
    "text": "of meaning composition functions and nodes that could then be used to build a better neural network.",
    "start": "2032970",
    "end": "2039630"
  },
  {
    "text": "And sort of some- some of the essence of that is on this slide. But, you know, for the first version we just",
    "start": "2039630",
    "end": "2047280"
  },
  {
    "text": "didn't have enough complexity of neural network, frankly, right? So when we had two constituents we concatenated",
    "start": "2047280",
    "end": "2053730"
  },
  {
    "text": "them and multiply that by a weight, uh, weight matrix.",
    "start": "2053730",
    "end": "2059040"
  },
  {
    "text": "Um, and that was sort of essentially all we had. And, um, as I hope you've gotten more of a sense of in this class.",
    "start": "2059040",
    "end": "2067740"
  },
  {
    "text": "If you just concatenate and multiply by a weight matrix, you're not actually modeling the interaction between these two vectors, right.",
    "start": "2067740",
    "end": "2076200"
  },
  {
    "text": "Because you can think of this weight matrix as just sort of being divided in two and half of it multiplies this vector,",
    "start": "2076200",
    "end": "2083490"
  },
  {
    "text": "and half of it multiplies this vector. So the meanings of these two things don't act on each other.",
    "start": "2083490",
    "end": "2089685"
  },
  {
    "text": "And so somehow you have to make your neural network, um, more complex than that. But the other way in which this seemed too simple is in the first model,",
    "start": "2089685",
    "end": "2099944"
  },
  {
    "text": "we had just one weight matrix which we use for everything. And, ah, at least if you're a linguist and you're",
    "start": "2099944",
    "end": "2108119"
  },
  {
    "text": "thinking about the structure of language you might start thinking of well, wait a minute, sometimes you're gonna be putting",
    "start": "2108120",
    "end": "2114630"
  },
  {
    "text": "together a verb and an object noun phrase. Um, hit the ball. Sometimes you're gonna be putting together an article and a noun, uh, ball.",
    "start": "2114630",
    "end": "2124125"
  },
  {
    "text": "Sometimes you're gonna be doing adjectival modification blue ball.",
    "start": "2124125",
    "end": "2129150"
  },
  {
    "text": "These things are very different in their semantics. Can it really be the case that you can just have one weight matrix that is",
    "start": "2129150",
    "end": "2136980"
  },
  {
    "text": "this universal composition function for putting together the meaning of phrases? Could that possibly work?",
    "start": "2136980",
    "end": "2143280"
  },
  {
    "text": "And you sort of might suspect, um, it doesn't work. Um, and so I'm gonna go on and, um,",
    "start": "2143280",
    "end": "2149355"
  },
  {
    "text": "show, um, some of those different things. But really, um, before I show the different things,",
    "start": "2149355",
    "end": "2157964"
  },
  {
    "start": "2152000",
    "end": "2372000"
  },
  {
    "text": "um, I'm gonna show one more version that's sort of related to the first thing, which actually gave a pretty successful and good parser,",
    "start": "2157965",
    "end": "2167355"
  },
  {
    "text": "um, for doing, um, context-free style constituency parsing.",
    "start": "2167355",
    "end": "2174195"
  },
  {
    "text": "And so this was another way of getting away from the parsing being completely greedy.",
    "start": "2174195",
    "end": "2181950"
  },
  {
    "text": "Um, which was to actually split apart the two parts of g. We have to come up with a tree structure for our sentence from,",
    "start": "2181950",
    "end": "2191880"
  },
  {
    "text": "'Let's compute the meaning of the sentence'. And so the thinking was, well,",
    "start": "2191880",
    "end": "2197280"
  },
  {
    "text": "in terms of deciding what's a good tree structure for a sentence,",
    "start": "2197280",
    "end": "2202965"
  },
  {
    "text": "that's actually something you can do pretty well with the symbolic grammar. But the problems with symbolic grammars aren't",
    "start": "2202965",
    "end": "2209940"
  },
  {
    "text": "that they can't put tree structures over sentences. The problems you have with those grammars is that,",
    "start": "2209940",
    "end": "2215790"
  },
  {
    "text": "they can't compute meaning representation and they're not very good at choosing between alternative tree structures.",
    "start": "2215790",
    "end": "2223710"
  },
  {
    "text": "But we can divide up the two parts. So what we can do is say, well,",
    "start": "2223710",
    "end": "2228750"
  },
  {
    "text": "let's just use a regular probabilistic Context-Free Grammar to generate possible tree structures for sentences.",
    "start": "2228750",
    "end": "2236580"
  },
  {
    "text": "We can generate a k best list and say, what are the 50 best, um, context-free grammar structures for this sentence?",
    "start": "2236580",
    "end": "2244230"
  },
  {
    "text": "And that's something we can do very efficiently with dynamic programming algorithms. And then we can work out a neural net,",
    "start": "2244230",
    "end": "2253125"
  },
  {
    "text": "um, that will work out the meaning representation of the sentence. Um, and so that led to this, um,",
    "start": "2253125",
    "end": "2261675"
  },
  {
    "text": "what's called syntactically untied recursive neural network. Um, so essentially what this is saying is that we",
    "start": "2261675",
    "end": "2271260"
  },
  {
    "text": "ha- for each node and the sentence it's got a category, um, of a symbolic context-free grammar.",
    "start": "2271260",
    "end": "2278940"
  },
  {
    "text": "So they're category A and B and C. So when we put things together we'll be able to say, okay.",
    "start": "2278940",
    "end": "2286500"
  },
  {
    "text": "We've got a rule that says, um, X goes to BC,",
    "start": "2286500",
    "end": "2292619"
  },
  {
    "text": "so that licenses this node here. So that part of the parsing is symbolic.",
    "start": "2292620",
    "end": "2298800"
  },
  {
    "text": "Then- then we want to, um, work out the meaning of this phrase.",
    "start": "2298800",
    "end": "2304110"
  },
  {
    "text": "Um, and well, the second problem I talked about was surely just having a one way of doing composition",
    "start": "2304110",
    "end": "2312075"
  },
  {
    "text": "is expecting a lot too much to be able to have sort of verb and object versus adjective and noun composed the same way.",
    "start": "2312075",
    "end": "2320535"
  },
  {
    "text": "So we have this idea of well, since we now know about the syntactic categories of",
    "start": "2320535",
    "end": "2326190"
  },
  {
    "text": "the children that we maybe know that this is an adjective and this is a noun.",
    "start": "2326190",
    "end": "2331214"
  },
  {
    "text": "What we could do is have different weight matrices for composition depending on what the categories are.",
    "start": "2331215",
    "end": "2338895"
  },
  {
    "text": "So rather than where before there was just this one universal weight matrix which was meant to do all meaning composition.",
    "start": "2338895",
    "end": "2347325"
  },
  {
    "text": "Here we can have, this is the weight matrix for combining together the meanings of an adjective and a noun and it will compute,",
    "start": "2347325",
    "end": "2355439"
  },
  {
    "text": "um, the meaning of this constituent. But then we'll have a different weight matrix for combining",
    "start": "2355439",
    "end": "2361590"
  },
  {
    "text": "together the meanings of a determiner and a noun phrase or something like that.",
    "start": "2361590",
    "end": "2367870"
  },
  {
    "text": "Okay. Um, yes. So I sort of always said this one I guess,",
    "start": "2370090",
    "end": "2377359"
  },
  {
    "start": "2372000",
    "end": "2452000"
  },
  {
    "text": "um, we wanted to be able to do things quickly. And so our solution to be able to do that is we sort of",
    "start": "2377360",
    "end": "2384829"
  },
  {
    "text": "used a probabilistic context-free grammar to find likely parses, um, and then only worked out our meaning for ones that were, um, quite probable.",
    "start": "2384830",
    "end": "2395119"
  },
  {
    "text": "And so we call this result a compositional vector grammar which was a combination of a PCFG and a tree recursive neural network.",
    "start": "2395120",
    "end": "2403745"
  },
  {
    "text": "Um, and yeah. So, um, essentially at the time,",
    "start": "2403745",
    "end": "2411010"
  },
  {
    "text": "this actually gave a pretty good constituency parser. So there are sort of lots of results here.",
    "start": "2411010",
    "end": "2416845"
  },
  {
    "text": "The top ones are kind of our classic older, um, Stanford Parser which is a PCFG,  the kind of parsers that people had built.",
    "start": "2416845",
    "end": "2425285"
  },
  {
    "text": "This is our compositional vector grammar that the time of this being done in 2013,",
    "start": "2425285",
    "end": "2432079"
  },
  {
    "text": "it wasn't the very best parser available. There had been some better work by Eugene Charniak at Brown.",
    "start": "2432080",
    "end": "2438155"
  },
  {
    "text": "But we actually had a pretty good parser coming out of that system. But what was perhaps a bit more interesting was we,",
    "start": "2438155",
    "end": "2446494"
  },
  {
    "text": "we didn't only have a parser that was meant to give the right parse trees. We are also computing meaning representations of nodes.",
    "start": "2446495",
    "end": "2454849"
  },
  {
    "start": "2452000",
    "end": "3080000"
  },
  {
    "text": "And as a kind of a consequence of that, you can look at not only meaning representations of nodes.",
    "start": "2454850",
    "end": "2462140"
  },
  {
    "text": "You could learn about the weight matrices that these models were learning, um, when they combine together meanings.",
    "start": "2462140",
    "end": "2468980"
  },
  {
    "text": "So remember we had these sort of category-specific W matrices, that were going together with the children to work out the meaning.",
    "start": "2468980",
    "end": "2477455"
  },
  {
    "text": "Um, so these are a little bit hard to interpret. But the deal is, when we load these matrices,",
    "start": "2477455",
    "end": "2484130"
  },
  {
    "text": "we initialize them as a pair of diagonal matrices. So these are sort of two by one rectangular matrices because there are two children.",
    "start": "2484130",
    "end": "2492125"
  },
  {
    "text": "Um, so half of it is, um, multiplying the left child, the other half is multiplying the right child.",
    "start": "2492125",
    "end": "2499535"
  },
  {
    "text": "And we initialize them as sort of like a compi- two identity matrices next to each",
    "start": "2499535",
    "end": "2505520"
  },
  {
    "text": "other which would give us the sort of default semantics of just averaging until something different was learned in the,",
    "start": "2505520",
    "end": "2512840"
  },
  {
    "text": "in the, in the weight vectors. And to the extent that sort of nothing interesting has been learned by the model,",
    "start": "2512840",
    "end": "2522069"
  },
  {
    "text": "you'll get yellow along the diagonal and this sort of sky blue in the rest of the field.",
    "start": "2522070",
    "end": "2528325"
  },
  {
    "text": "And to the extent that it's learned something interesting to take out of the semantics of a child,",
    "start": "2528325",
    "end": "2534355"
  },
  {
    "text": "you will then start to see reds and oranges on the diagonal, and dark blues and greens and stuff in the rest of the field.",
    "start": "2534355",
    "end": "2542465"
  },
  {
    "text": "So what you find is that if you train this model, it's learning about which children of a phrase are actually the important ones.",
    "start": "2542465",
    "end": "2554224"
  },
  {
    "text": "Um, so these ones are saying that if you're combining together a noun phrase and the coordination,",
    "start": "2554225",
    "end": "2559310"
  },
  {
    "text": "so something like \"the cat and\", that most of the semantics have to be found in \"the cat\"",
    "start": "2559310",
    "end": "2565055"
  },
  {
    "text": "and not much of the semantics is going to be found in \"and\". Whereas if you are combining together a possessive pronoun,",
    "start": "2565055",
    "end": "2572825"
  },
  {
    "text": "something like her or his, um, with a noun phrase inside it like,",
    "start": "2572825",
    "end": "2578615"
  },
  {
    "text": "um, her tabby cat or something like that. Then most of the meaning is to be found inside the tabby cat constituent.",
    "start": "2578615",
    "end": "2586595"
  },
  {
    "text": "So it's actually learning where the important semantics of sentences is. Um, and there're lots of examples of that. Um, yeah.",
    "start": "2586595",
    "end": "2598460"
  },
  {
    "text": "This one sort of- so this one shows a variety of modification structures where adjectives or adverbs,",
    "start": "2598460",
    "end": "2606575"
  },
  {
    "text": "um, modify either a noun phrase or an adjective phrase or just a single adjective is multiplying a noun phrase.",
    "start": "2606575",
    "end": "2615995"
  },
  {
    "text": "And the thing that you seem to notice is that there are particular dimensions which are kind of capturing sort of modification meanings.",
    "start": "2615995",
    "end": "2624200"
  },
  {
    "text": "So dimension 6 and dimension 11 is sort of showing up in these different,",
    "start": "2624200",
    "end": "2630395"
  },
  {
    "text": "um, combinations here, as sort of capturing meaning components. So that was kind of neat.",
    "start": "2630395",
    "end": "2635645"
  },
  {
    "text": "And so this slightly more complex model actually worked pretty well at capturing a meaning of phrases and sentences.",
    "start": "2635645",
    "end": "2645035"
  },
  {
    "text": "So in this test here, we were giving it- the system a test sentence and saying well,",
    "start": "2645035",
    "end": "2651920"
  },
  {
    "text": "what are the other- what are sentences that are most similar in meaning,",
    "start": "2651920",
    "end": "2657785"
  },
  {
    "text": "nearest to paraphrases in our corpus for this sentence? So if all the figures are adjusted for seasonal variations,",
    "start": "2657785",
    "end": "2665990"
  },
  {
    "text": "the two most similar other sentences in the corpus were, all the numbers are adjusted for seasonal vet fluctuation.",
    "start": "2665990",
    "end": "2672994"
  },
  {
    "text": "That's a pretty easy one. Or all the figures are adjusted to remove usual seasonal patterns. So that seems to be working pretty well.",
    "start": "2672995",
    "end": "2680240"
  },
  {
    "text": "\"Knight-Ridder wouldn't a comment on the author, Harsco declined to say what country placed the order.\"",
    "start": "2680240",
    "end": "2686360"
  },
  {
    "text": "The semantics there are a bit more different but it seems like it is capturing something similar.",
    "start": "2686360",
    "end": "2691490"
  },
  {
    "text": "\"Um, Coastal wouldn't disclose the terms.\" That's kind of a really interesting one, because that one is actually very similar in meaning but it's expressed in",
    "start": "2691490",
    "end": "2700009"
  },
  {
    "text": "a very different way in terms of the words and the syntactic structure that are used.",
    "start": "2700010",
    "end": "2705215"
  },
  {
    "text": "Okay, so that was progress because now we could have different matrices for different constituent types.",
    "start": "2705215",
    "end": "2714815"
  },
  {
    "text": "Um, but there's still some reason to think that we didn't have enough power,",
    "start": "2714815",
    "end": "2722015"
  },
  {
    "text": "and that was we're still at heart using this very simple compositional structure",
    "start": "2722015",
    "end": "2728224"
  },
  {
    "text": "where we're just concatenating the two children's vectors and multiplying it by a matrix.",
    "start": "2728225",
    "end": "2734960"
  },
  {
    "text": "So that means the two words, um, didn't interact with each other in terms of their meaning.",
    "start": "2734960",
    "end": "2741470"
  },
  {
    "text": "Um, but, um, it seems like we want to have them interact in their meaning, right?",
    "start": "2741470",
    "end": "2749450"
  },
  {
    "text": "So in particular if you if you think about human languages and the kind of things that people look at in linguistic semantics,",
    "start": "2749450",
    "end": "2759305"
  },
  {
    "text": "you get words that appear to be kind of modifiers or operators.",
    "start": "2759305",
    "end": "2764464"
  },
  {
    "text": "So the word very, sort of doesn't mean much by itself.",
    "start": "2764465",
    "end": "2769580"
  },
  {
    "text": "I mean it means something like strengthening or more so or something like that,",
    "start": "2769580",
    "end": "2774980"
  },
  {
    "text": "but you know, it doesn't really have a meaning, right? It doesn't have any denotation.",
    "start": "2774980",
    "end": "2780049"
  },
  {
    "text": "You can't show me very things, right? You can show me chairs and pens and, um,",
    "start": "2780050",
    "end": "2785135"
  },
  {
    "text": "children but you can't show me very things, that the meaning of very seems to be that something comes after it, good.",
    "start": "2785135",
    "end": "2792694"
  },
  {
    "text": "And this has a sort of an operator meaning of increase on the scale of this thing,",
    "start": "2792695",
    "end": "2799580"
  },
  {
    "text": "and it can increase on the scale in either direction. You can have very good or very bad.",
    "start": "2799580",
    "end": "2805115"
  },
  {
    "text": "So if we want to capture that kind of semantics, it seems like we can't capture that kind of semantics by just",
    "start": "2805115",
    "end": "2813425"
  },
  {
    "text": "concatenating two vectors and multiplying them by a matrix. It seems like what we really want to say is very is gonna grab",
    "start": "2813425",
    "end": "2824300"
  },
  {
    "text": "hold of the meaning of good and modify it in some ways to produce a new meaning for very good.",
    "start": "2824300",
    "end": "2830914"
  },
  {
    "text": "And indeed, that's the kind of approach that's typically, um, been done in linguistic semantics.",
    "start": "2830915",
    "end": "2837980"
  },
  {
    "text": "So in linguistic theories of semantic, you would normally say, okay, good has a meaning,",
    "start": "2837980",
    "end": "2843620"
  },
  {
    "text": "very is a function that takes in the meaning of good and returns a meaning of very good.",
    "start": "2843620",
    "end": "2849530"
  },
  {
    "text": "And so we wanted to have, um, a way of putting that into a neural network.",
    "start": "2849530",
    "end": "2855050"
  },
  {
    "text": "And so to try and come up with a new composition function as to how to do that.",
    "start": "2855050",
    "end": "2860750"
  },
  {
    "text": "And there are various ways that you could think about doing that and other people have had a couple of different attempts.",
    "start": "2860750",
    "end": "2868340"
  },
  {
    "text": "But essentially what was in our head is well, we have word vectors,",
    "start": "2868340",
    "end": "2875120"
  },
  {
    "text": "and if we want to say that very takes the meaning of good and returns a new meaning,",
    "start": "2875120",
    "end": "2881990"
  },
  {
    "text": "the kind of obvious thing to do is to say very has a matrix attached to it because then we could use the,",
    "start": "2881990",
    "end": "2888829"
  },
  {
    "text": "the very matrix and multiply it by the good vector and we get a new,",
    "start": "2888830",
    "end": "2893990"
  },
  {
    "text": "um, vector coming out. And so then, well,",
    "start": "2893990",
    "end": "2899045"
  },
  {
    "text": "the problem is, uh, which- then which words have vectors and which words have matrices?",
    "start": "2899045",
    "end": "2905810"
  },
  {
    "text": "And that's kind of, um, hard to know the answer to. I mean, in particular, um,",
    "start": "2905810",
    "end": "2912484"
  },
  {
    "text": "words that act as operators can, um, often themselves be modified.",
    "start": "2912485",
    "end": "2919550"
  },
  {
    "text": "Um, and so, um, that you know, good can also- good also is a operator, right?",
    "start": "2919550",
    "end": "2929680"
  },
  {
    "text": "So that from a sort of a person, you can have a good person and that's sort of also an operator,",
    "start": "2929680",
    "end": "2936115"
  },
  {
    "text": "and very is modifying that good. So the idea we came up with is let's not try and predetermine all of this.",
    "start": "2936115",
    "end": "2943460"
  },
  {
    "text": "Why don't we say that every word and every phrase has connected to it both matrix and vector.",
    "start": "2943460",
    "end": "2952384"
  },
  {
    "text": "So here's our very good movie. So for each word, we have a vector meaning and it has a matrix meaning,",
    "start": "2952385",
    "end": "2959945"
  },
  {
    "text": "and then as we start to build up phrases like very good, they're also going to have a vector meaning and a matrix meaning.",
    "start": "2959945",
    "end": "2968795"
  },
  {
    "text": "And so what we proposed was, um, so first of all,",
    "start": "2968795",
    "end": "2974390"
  },
  {
    "text": "we, we would like to be able, um, to calculate, um, the vector meanings.",
    "start": "2974390",
    "end": "2980765"
  },
  {
    "text": "So to work out the vector meaning of a phrase like very good.",
    "start": "2980765",
    "end": "2987005"
  },
  {
    "text": "Each word has a matrix meaning. And so we're going to combine their opposing matrix and vector meaning.",
    "start": "2987005",
    "end": "2993680"
  },
  {
    "text": "So we're going to take the matrix meaning of good and multiply it by the vector meaning of very.",
    "start": "2993680",
    "end": "3000610"
  },
  {
    "text": "And we're going to take the matrix meaning of very and multiply it by the vector meaning of good.",
    "start": "3000610",
    "end": "3007525"
  },
  {
    "text": "And so we're going to have both of those two things. And then we're going to have a neural network layer like before that combine those together.",
    "start": "3007525",
    "end": "3017620"
  },
  {
    "text": "And so that's sort of in the red box. Then those two things were concatenated, and put through the kind of neural network layer we had before to give us",
    "start": "3017620",
    "end": "3025839"
  },
  {
    "text": "a final vector meaning on this, for the phrase. And then we also needed a matrix meaning for the phrase.",
    "start": "3025840",
    "end": "3034675"
  },
  {
    "text": "And so for the matrix meaning for the phrase, um. We did this kind of simple model which maybe actually wasn't very good which was to say,",
    "start": "3034675",
    "end": "3044185"
  },
  {
    "text": "let's just concatenate the two matrices of the- um,",
    "start": "3044185",
    "end": "3050260"
  },
  {
    "text": "the constituents, multiply them by another matrix and that's then going to give us a matrix,",
    "start": "3050260",
    "end": "3057280"
  },
  {
    "text": "um, version of the parent node. And so this was gave us our new more compo- more powerful composition procedure.",
    "start": "3057280",
    "end": "3065605"
  },
  {
    "text": "Um, this did seem like it could do some kind of good things that captured,",
    "start": "3065605",
    "end": "3071619"
  },
  {
    "text": "uh, uh, sort of operator semantics where one word modified the meaning of another word.",
    "start": "3071620",
    "end": "3077980"
  },
  {
    "text": "Um, so here's a kind of a neat thing that we were able to do with this.",
    "start": "3077980",
    "end": "3085609"
  },
  {
    "text": "Um, that we are wanting to be able to work out the semantics of an operator modifying another word.",
    "start": "3085620",
    "end": "3094330"
  },
  {
    "text": "So unbelievably annoying, unbelievably awesome, unbelievably sad.",
    "start": "3094330",
    "end": "3100450"
  },
  {
    "text": "Um, not annoying, not awesome, not sad. [NOISE] And so this was contrasting,",
    "start": "3100450",
    "end": "3108340"
  },
  {
    "text": "our, um, old model versus the new model.",
    "start": "3108340",
    "end": "3114205"
  },
  {
    "text": "And this scale is a scale of positive to negative. So this is completely negative to completely positive, all right?",
    "start": "3114205",
    "end": "3123210"
  },
  {
    "text": "And so the kind of contrast you get, uh, that for, um,",
    "start": "3123210",
    "end": "3129910"
  },
  {
    "text": "not annoying that the simple model thought that this was pretty negative,",
    "start": "3129910",
    "end": "3135289"
  },
  {
    "text": "whereas the new model thinks this is pretty neutral in meaning, and that seems to be reasonably correct.",
    "start": "3135290",
    "end": "3142615"
  },
  {
    "text": "Um, but not sad, that means it's a little bit positive and both models were trying to capt- capture that,",
    "start": "3142615",
    "end": "3151180"
  },
  {
    "text": "that- you know, the results here are a little bit ambivalent, but- but it sort of seems that they sort of go a",
    "start": "3151180",
    "end": "3156970"
  },
  {
    "text": "little bit in the direction of what we want. Yes. What is the ground truth in the \"not sad\" example?",
    "start": "3156970",
    "end": "3162640"
  },
  {
    "text": "Oh, yeah. So this ground truth was- we actually asked a whole bunch of human beings to say,",
    "start": "3162640",
    "end": "3169510"
  },
  {
    "text": "um, rate the [LAUGHTER] meaning of not sad, on this scale of 1 to 10.",
    "start": "3169510",
    "end": "3175210"
  },
  {
    "text": "Maybe this wasn't a very good clear task because as you can see it, bounced around a lot [LAUGHTER] as to,",
    "start": "3175210",
    "end": "3181630"
  },
  {
    "text": "um, what kind of ratings we were getting for things. But yeah, that was actually kind of getting human judgments.",
    "start": "3181630",
    "end": "3188230"
  },
  {
    "text": "Um, we also then use this,",
    "start": "3188230",
    "end": "3193463"
  },
  {
    "start": "3190000",
    "end": "3383000"
  },
  {
    "text": "um, model to say, \"Well, could we do, um, semantic classification tasks?\"",
    "start": "3193464",
    "end": "3198910"
  },
  {
    "text": "So if we wanted to understand relations between different noun phrases,",
    "start": "3198910",
    "end": "3204964"
  },
  {
    "text": "so this was a dataset where, um, there were relations marked between two noun phrases.",
    "start": "3204965",
    "end": "3212694"
  },
  {
    "text": "My apartment has a pretty large kitchen that that was seen as an example of a component-whole, a part of relationship between the two noun phrases,",
    "start": "3212695",
    "end": "3223839"
  },
  {
    "text": "and there were other relationships between different kinds of noun phrases.",
    "start": "3223840",
    "end": "3229210"
  },
  {
    "text": "So if it was the movie showed wars, um, that that was then a message topic,",
    "start": "3229210",
    "end": "3234535"
  },
  {
    "text": "so there's some communication medium that contains some topic relationship. And so we were using this kind of",
    "start": "3234535",
    "end": "3241930"
  },
  {
    "text": "neural network to sort of build our meaning representations and then putting them through",
    "start": "3241930",
    "end": "3246940"
  },
  {
    "text": "another neural network layer as a classifier to see how well we did.",
    "start": "3246940",
    "end": "3252395"
  },
  {
    "text": "And so we got some sort of fairly good results on that. So this was a dataset that people had worked on with",
    "start": "3252395",
    "end": "3258970"
  },
  {
    "text": "traditional NLP systems of different kinds of machine learning methods.",
    "start": "3258970",
    "end": "3264070"
  },
  {
    "text": "But in some sense, you know, what we were interested in was we seem to be making progress in having",
    "start": "3264070",
    "end": "3269710"
  },
  {
    "text": "a better semantic composition system that our old recursive neural network was getting about 75 percent,",
    "start": "3269710",
    "end": "3277180"
  },
  {
    "text": "and then our new one was getting about 79 percent, which we could sort of push up further by putting more features into our system.",
    "start": "3277180",
    "end": "3285279"
  },
  {
    "text": "So that was progress, um, but we didn't stop there.",
    "start": "3285280",
    "end": "3290815"
  },
  {
    "text": "We kept on trying to come up with better ways of doing things. And so even though things worked fairly well here,",
    "start": "3290815",
    "end": "3299660"
  },
  {
    "text": "it sort of seemed like this way of doing matrices wasn't necessarily very good.",
    "start": "3299660",
    "end": "3307500"
  },
  {
    "text": "It sort of had two problems. One problem was it introduced a humongous number of parameters because,",
    "start": "3307500",
    "end": "3315910"
  },
  {
    "text": "you know, for just about everything that we've done, otherwise, words have had a vector and well,",
    "start": "3315910",
    "end": "3322359"
  },
  {
    "text": "maybe sometimes we use quite high dimensional vectors like 1,024,",
    "start": "3322360",
    "end": "3328090"
  },
  {
    "text": "um, [NOISE] but, you know, that's a relatively modest number of parameters. Whereas once we introduce this matrix here,",
    "start": "3328090",
    "end": "3335305"
  },
  {
    "text": "we've got that number of squared additional parameters for every word.",
    "start": "3335305",
    "end": "3340540"
  },
  {
    "text": "And essentially because of that number of parameters to be able to compute this model at all,",
    "start": "3340540",
    "end": "3346690"
  },
  {
    "text": "we were making the vector size small. So what we were actually using was that these were just 25-dimensional vectors so that the 25 squared,",
    "start": "3346690",
    "end": "3355570"
  },
  {
    "text": "625, still safe, sort of decently within the range in which we could compute.",
    "start": "3355570",
    "end": "3361495"
  },
  {
    "text": "So that was the first problem. The second problem is, we didn't really have very good ways of",
    "start": "3361495",
    "end": "3368620"
  },
  {
    "text": "sort of building up the matrix meaning of bigger phrases. I mean, you know,",
    "start": "3368620",
    "end": "3374349"
  },
  {
    "text": "this sort of seems something simple we could do but it didn't, you know, feel a very good way of getting a matrix meaning of a phrase.",
    "start": "3374350",
    "end": "3381970"
  },
  {
    "text": "So we sort of wanted to come up with some other way of doing things that could fix both of those problems.",
    "start": "3381970",
    "end": "3388375"
  },
  {
    "start": "3383000",
    "end": "3419000"
  },
  {
    "text": "And then, that led into work on recursive neural tensor networks.",
    "start": "3388375",
    "end": "3393940"
  },
  {
    "text": "Um, and there's a kind of a nice idea here of these neural tensors,",
    "start": "3393940",
    "end": "3399415"
  },
  {
    "text": "which is an idea that's actually been used in other places including, um,",
    "start": "3399415",
    "end": "3404590"
  },
  {
    "text": "work on sort of putting vector embeddings of knowledge graphs and so on, which is a kind of a bit of a nice idea.",
    "start": "3404590",
    "end": "3411550"
  },
  {
    "text": "So I wanted to sort of show a bit of how this model works. Um, and but just to say, first,",
    "start": "3411550",
    "end": "3418930"
  },
  {
    "text": "a place where we applied this model was on the problem of sentiment analysis. Now, I think the term sentiment analysis has come up a few times as something you can",
    "start": "3418930",
    "end": "3428650"
  },
  {
    "start": "3419000",
    "end": "3573000"
  },
  {
    "text": "do and actually which I then mentioned in the last, um, lecture.",
    "start": "3428650",
    "end": "3433720"
  },
  {
    "text": "But I think we've never really talked for five minutes, um, in this class on sentiment analysis,",
    "start": "3433720",
    "end": "3439450"
  },
  {
    "text": "so I'll, um, give you this as an example of that. Um, sentiment analysis has actually been",
    "start": "3439450",
    "end": "3444910"
  },
  {
    "text": "a really common and important application in natural language processing.",
    "start": "3444910",
    "end": "3450940"
  },
  {
    "text": "Um, you're looking at a piece of text and you're sort of saying, \"Is it, um, positive or negative?\"",
    "start": "3450940",
    "end": "3457810"
  },
  {
    "text": "Um, and that's just something that's very useful for lots of, um, commercial applications of looking at product reviews or doing brand,",
    "start": "3457810",
    "end": "3466954"
  },
  {
    "text": "um, awareness and things like that of sort of looking at sentiment connected to things. And to some extent doing sentiment analysis is easy, right?",
    "start": "3466955",
    "end": "3475540"
  },
  {
    "text": "That you can kind of say, \"Well, look at a piece of text. If you see words like loved,",
    "start": "3475540",
    "end": "3480700"
  },
  {
    "text": "great, impressed, marvelous, then it's positive. It's a positive review. And if it's saying, bad and awful,",
    "start": "3480700",
    "end": "3486880"
  },
  {
    "text": "then it's a negative review.\" And to some extent that's the baseline of sentiment analysis that you can use",
    "start": "3486880",
    "end": "3493420"
  },
  {
    "text": "just either selected word features or all words in a bag of words.",
    "start": "3493420",
    "end": "3498805"
  },
  {
    "text": "And if you do that, you don't actually do that badly, um, in sentiment analysis.",
    "start": "3498805",
    "end": "3505150"
  },
  {
    "text": "If you have longer documents, just looking at bags of words can give you 90 percent in sentiment analysis.",
    "start": "3505150",
    "end": "3511135"
  },
  {
    "text": "But on the other hand, things often do get trickier, right? So, um, this is from Rotten Tomatoes.",
    "start": "3511135",
    "end": "3518019"
  },
  {
    "text": "With this cast and the subject matter, the movie should have been funnier and more entertaining.",
    "start": "3518020",
    "end": "3523480"
  },
  {
    "text": "And if you sort of pretend you're a bag of words model, the only words in this that are sort of clearly sentiment-laden words, uh,",
    "start": "3523480",
    "end": "3532839"
  },
  {
    "text": "entertaining and funnier, and both of those are pretty positive words,",
    "start": "3532840",
    "end": "3537969"
  },
  {
    "text": "um, but it's fairly obvious that this actually is meant to be a bad review of the movie.",
    "start": "3537969",
    "end": "3544615"
  },
  {
    "text": "And so well, how are we meant to know that? Well, it sort of seems again like what we have to do is meaning composition.",
    "start": "3544615",
    "end": "3551320"
  },
  {
    "text": "We have to get sort of phrases like \"should have been funnier,\" and then realized that that's actually a negative meaning for a phrase.",
    "start": "3551320",
    "end": "3561070"
  },
  {
    "text": "And so we wanted to explore how we could look at those sort of meanings for phrases and explore building up those meanings as doing meaning composition over trees.",
    "start": "3561070",
    "end": "3573310"
  },
  {
    "start": "3573000",
    "end": "3600000"
  },
  {
    "text": "Um, so the first thing we did, um, was we built a treebank of sentiment trees where we got people to rate sentiment.",
    "start": "3573310",
    "end": "3582490"
  },
  {
    "text": "And so this led to the Stanford Sentiment Treebank, which is still a dataset you often see used in, um,",
    "start": "3582490",
    "end": "3589674"
  },
  {
    "text": "various of evaluations with a whole bunch of datasets. Indeed, it showed up in decaNLP last week.",
    "start": "3589675",
    "end": "3597175"
  },
  {
    "text": "Um, so what we were doing in this was taking, um, sentences which were Rotten Tomatoes sentences from movies.",
    "start": "3597175",
    "end": "3606265"
  },
  {
    "text": "We were parsing them to give tree structure and then we were asking Mechanical Turkers to",
    "start": "3606265",
    "end": "3613450"
  },
  {
    "text": "rate the different phra- the different words and phrases on a sentiment scale of very positive to very negative.",
    "start": "3613450",
    "end": "3621460"
  },
  {
    "text": "So lots of stuff is white because it's just not sentiment-laden, right? There's words that are the,",
    "start": "3621460",
    "end": "3627575"
  },
  {
    "text": "and there's phrases like the movie and the movie was- which don't really have any sentiment,",
    "start": "3627575",
    "end": "3633325"
  },
  {
    "text": "but then you have pieces that are sort of very positives pieces of tree and negative pieces of tree that are then shown in the blue and the red.",
    "start": "3633325",
    "end": "3642025"
  },
  {
    "text": "And- so typically in sentiment datasets, people have only labeled the entire sentence to say,",
    "start": "3642025",
    "end": "3649720"
  },
  {
    "text": "\"This is a positive sentence or a very positive sentence. This is a negative sentence or a very negative sentence.\"",
    "start": "3649720",
    "end": "3655839"
  },
  {
    "text": "Crucially, what we were doing differently here is every phrase in the sentence",
    "start": "3655840",
    "end": "3661810"
  },
  {
    "text": "according to our tree structure was being given a label for its positivity or negativity.",
    "start": "3661810",
    "end": "3668170"
  },
  {
    "text": "Um, and perhaps not surprisingly, just the fact that you have a lot more annotations like that, um,",
    "start": "3668170",
    "end": "3674990"
  },
  {
    "text": "just improves the behavior of classifiers because you kind of can",
    "start": "3674990",
    "end": "3680400"
  },
  {
    "text": "do better attribution of which words in a sentence are positive or negative. Um.",
    "start": "3680400",
    "end": "3686734"
  },
  {
    "text": "So, these were um were results of sort of preceding models.",
    "start": "3686735",
    "end": "3692810"
  },
  {
    "text": "So the green is a Naive Bayes model except it not only uses individual words,",
    "start": "3692810",
    "end": "3699650"
  },
  {
    "text": "but it uses pairs of words. It turns out if you're building a traditional classifier and you",
    "start": "3699650",
    "end": "3705589"
  },
  {
    "text": "wanna do sentiment analysis as opposed to something like topic classification, you get a lot better results if you also use word pair features.",
    "start": "3705590",
    "end": "3714500"
  },
  {
    "text": "And that's because it does a baby bit of um composition for you. You don't only have features for not an interesting,",
    "start": "3714500",
    "end": "3721659"
  },
  {
    "text": "but you can have a feature for not interesting and that lets you model a certain amount of stuff.",
    "start": "3721659",
    "end": "3727030"
  },
  {
    "text": "Um, and then these are our older generations of neural networks, our ori- original tree structured neural network and our matrix vector one.",
    "start": "3727030",
    "end": "3734934"
  },
  {
    "text": "And so simply having- for these sort of fixed models, simply having the richer supervision that comes from our new treebank,",
    "start": "3734935",
    "end": "3743810"
  },
  {
    "text": "it's sort of moved up the performance of every model. So even um, for just the um,",
    "start": "3743810",
    "end": "3749450"
  },
  {
    "text": "Naive Bayes model's performances going up about four percent um, because of the fact um,",
    "start": "3749450",
    "end": "3755940"
  },
  {
    "text": "that it now knows more about which particular words are positive or negative in the sentences.",
    "start": "3755940",
    "end": "3762005"
  },
  {
    "text": "Um, but still none of these performances are really great.",
    "start": "3762005",
    "end": "3767075"
  },
  {
    "text": "Um, so we still thought that well can we build better models of how to do this.",
    "start": "3767075",
    "end": "3773119"
  },
  {
    "text": "Um, in particular, if you look at sentences with sort of various kinds of negation you know,",
    "start": "3773120",
    "end": "3779450"
  },
  {
    "text": "things like should've been funnier, these models in general still couldn't capture the right meanings for them.",
    "start": "3779450",
    "end": "3786170"
  },
  {
    "text": "And so that led into our fourth model of how to do this,",
    "start": "3786170",
    "end": "3791599"
  },
  {
    "text": "which is this idea of recursive neural tensor networks. Um, and so what we wanted to be able to do is go back to just having um,",
    "start": "3791600",
    "end": "3802550"
  },
  {
    "text": "meanings of words be vectors, but nevertheless despite that to be able to",
    "start": "3802550",
    "end": "3810335"
  },
  {
    "text": "have a meaningful phrase where the two vectors um, acted on each other.",
    "start": "3810335",
    "end": "3816140"
  },
  {
    "text": "And well, you know, this kind of, this is the picture of what we did when we were",
    "start": "3816140",
    "end": "3821810"
  },
  {
    "text": "doing attention in a bi-linear way, right? We had vectors for two words. We stuck a matrix in between and we used",
    "start": "3821810",
    "end": "3830330"
  },
  {
    "text": "that and gave an attention and got an attention score out. So that let these two vectors interact with each other,",
    "start": "3830330",
    "end": "3839244"
  },
  {
    "text": "but it only produced one number as the output. But there's a way to fix that,",
    "start": "3839245",
    "end": "3844630"
  },
  {
    "text": "which is to say well rather than having a matrix here,",
    "start": "3844630",
    "end": "3850295"
  },
  {
    "text": "what we could stick here is a three-dimensional cube, which physicists and deep learning people now call a tensor, right?",
    "start": "3850295",
    "end": "3859220"
  },
  {
    "text": "So a tensor is just higher multi-dimensional array um, in computer science terms.",
    "start": "3859220",
    "end": "3864560"
  },
  {
    "text": "Um, so if we sort of made that a tensor, you know, it's like we have sort of multiple layers of matrix here.",
    "start": "3864560",
    "end": "3873035"
  },
  {
    "text": "And so the end result of that is we get one number here and one number here.",
    "start": "3873035",
    "end": "3878795"
  },
  {
    "text": "So in total, we get out a size two vector, which is all we need in my baby example where",
    "start": "3878795",
    "end": "3886355"
  },
  {
    "text": "baby examples, where we only have these two component vectors for words. But in general, we have a tensor with",
    "start": "3886355",
    "end": "3892250"
  },
  {
    "text": "the extra mention dimension of the size of our word vector. And so therefore, we will get a word vector, w hat,",
    "start": "3892250",
    "end": "3898984"
  },
  {
    "text": "we will get a phrase vector out from the composition that's the same size of the input vectors and will allow them to",
    "start": "3898985",
    "end": "3906980"
  },
  {
    "text": "interact with each other in working out the meaning of the entire thing.",
    "start": "3906980",
    "end": "3912450"
  },
  {
    "text": "Okay. Um, all right. So at that point um,",
    "start": "3912910",
    "end": "3919220"
  },
  {
    "text": "we use the resulting vectors um um,",
    "start": "3919220",
    "end": "3923250"
  },
  {
    "text": "so we had our neural tensor network. We actually combined it together with the sort of previous kind of layer we used to have,",
    "start": "3924610",
    "end": "3933694"
  },
  {
    "text": "our sort of first RNN, maybe you  didn't need to do this, but we just decided to add that in as well,",
    "start": "3933695",
    "end": "3939485"
  },
  {
    "text": "put things through a nonlinearity and that was then giving us our new representation of phrases.",
    "start": "3939485",
    "end": "3945770"
  },
  {
    "text": "We built that up the tree and then at the end, we could classify the meaning of any phrase um,",
    "start": "3945770",
    "end": "3953119"
  },
  {
    "text": "in the same kind of way with softmax regression and we could train these weights with gradient descent to predict sentiment.",
    "start": "3953120",
    "end": "3960585"
  },
  {
    "text": "And so this actually worked pretty nicely. I mean in particular, it didn't so really work any better with just the sentence labels.",
    "start": "3960585",
    "end": "3969820"
  },
  {
    "text": "But if we train the model with our treebank, we could then get a kind of- of whatever",
    "start": "3969820",
    "end": "3975820"
  },
  {
    "text": "that is about another couple of percent in performance, and so that seemed good. And so in particular,",
    "start": "3975820",
    "end": "3981880"
  },
  {
    "text": "it seemed to do a much better job of actually understanding meaning composition.",
    "start": "3981880",
    "end": "3986920"
  },
  {
    "text": "So here's the kind of sentence where you have there are slow and repetitive parts,",
    "start": "3986920",
    "end": "3992095"
  },
  {
    "text": "but it has just enough spice to keep it interesting. And the model seen here is pretty good at understanding.",
    "start": "3992095",
    "end": "3998470"
  },
  {
    "text": "Okay, this part of the sentence is negative, this part of the sentence is positive,",
    "start": "3998470",
    "end": "4003880"
  },
  {
    "text": "and actually when you stick the two halves together, the end result is a sentence that's positive in meaning.",
    "start": "4003880",
    "end": "4010450"
  },
  {
    "text": "But focusing in a little bit more what seems like it's especially good was for the first time this actually",
    "start": "4010450",
    "end": "4018609"
  },
  {
    "text": "did seem like it could do a better job of working out sort of what happens when you do things like negation.",
    "start": "4018610",
    "end": "4027220"
  },
  {
    "text": "So here we have it's just incredibly dull and it's definitely not dull. So if it's definitely not dull,",
    "start": "4027220",
    "end": "4034000"
  },
  {
    "text": "that's actually means it's good, right? Can we work out the meaning of, it's definitely not dull?",
    "start": "4034000",
    "end": "4040480"
  },
  {
    "text": "And so um, these, this is sort of",
    "start": "4040480",
    "end": "4045609"
  },
  {
    "text": "showing sort of what happens when you have a negative,",
    "start": "4045610",
    "end": "4051505"
  },
  {
    "text": "a negative sentence that's further negated. So if you go from um,",
    "start": "4051505",
    "end": "4059890"
  },
  {
    "text": "so if you sort of do a annex- negation of a negative thing should become moderately positive, right?",
    "start": "4059890",
    "end": "4068710"
  },
  {
    "text": "So that if you have dull is negative and if you say not dull,",
    "start": "4068710",
    "end": "4074065"
  },
  {
    "text": "it doesn't mean it's fantastic, but it means it's moderately positive. And so for either a kind of Naive Bayes model or our preceding models,",
    "start": "4074065",
    "end": "4084069"
  },
  {
    "text": "they weren't capable of capturing that of sort of going from dull to not dull your,",
    "start": "4084070",
    "end": "4089755"
  },
  {
    "text": "your meaning computation did not come out any more positive. Whereas this sort of neural tensor network was",
    "start": "4089755",
    "end": "4097000"
  },
  {
    "text": "capturing the fact that not dull meant that it was reasonably good.",
    "start": "4097000",
    "end": "4102470"
  },
  {
    "text": "So that was progress. Um, yes. So I think that's as much as I'll um show you really now about applying",
    "start": "4102750",
    "end": "4111460"
  },
  {
    "text": "these tree structured neural networks um, to natural language.",
    "start": "4111460",
    "end": "4117589"
  },
  {
    "text": "Um, you know, I think the summary I sort of said at the beginning um is that I",
    "start": "4117810",
    "end": "4123190"
  },
  {
    "text": "think you know, they're kind of interesting ideas and linguistic connections here.",
    "start": "4123190",
    "end": "4128275"
  },
  {
    "text": "I mean, for various reasons, these ideas haven't been um,",
    "start": "4128275",
    "end": "4135100"
  },
  {
    "text": "pursued a ton in recent years of natural language processing. You know, one is in all honesty people have found that um,",
    "start": "4135100",
    "end": "4144609"
  },
  {
    "text": "once you have high dimensional vectors in things like the kind of sequence models that we've looked at,",
    "start": "4144610",
    "end": "4151480"
  },
  {
    "text": "whether it's meaning things like the sort of LSTM models or any of the more recent contextual language models, those work incredibly well um,",
    "start": "4151480",
    "end": "4161200"
  },
  {
    "text": "and it's not, it's not clear that overall these models work better. The second reason is sort of a computational reason,",
    "start": "4161200",
    "end": "4168399"
  },
  {
    "text": "which is um, GPUs work great when you're doing uniform computation.",
    "start": "4168399",
    "end": "4175495"
  },
  {
    "text": "And the beauty of having something like a sequence model is that there's uh, there's just one determinant computation you are doing",
    "start": "4175495",
    "end": "4183594"
  },
  {
    "text": "along the sequence or in the convolutional neural network, there's one determinant um,",
    "start": "4183595",
    "end": "4189009"
  },
  {
    "text": "computation you're doing up um, through your convolutional layers and therefore,",
    "start": "4189010",
    "end": "4194170"
  },
  {
    "text": "things can be represented and computed efficiently on a GPU. The huge problem with these kind of models was what computations you are",
    "start": "4194170",
    "end": "4203020"
  },
  {
    "text": "going to do depended on which structure you are assigning to the sentence, and every sentence was going to have a different structure, and so therefore,",
    "start": "4203020",
    "end": "4212110"
  },
  {
    "text": "there was no way to batch the computations over a group of sentences and have the same computations being done for",
    "start": "4212110",
    "end": "4218860"
  },
  {
    "text": "different sentences, it sort of undermined the ability to sort of efficiently build these models in the large.",
    "start": "4218860",
    "end": "4226365"
  },
  {
    "text": "The thing I thought I'd just sort of say a moment about at the end. Um, the funny thing is that although these haven't been used much for,",
    "start": "4226365",
    "end": "4236195"
  },
  {
    "text": "um, language in the last few years, um, that they've actually had some use and found different applications in different places,",
    "start": "4236195",
    "end": "4245650"
  },
  {
    "text": "um, which is just sort of seen kind of cute. Um, so this is actually an application from physics.",
    "start": "4245650",
    "end": "4252849"
  },
  {
    "text": "Um, and I think I'm going to just have to read this and so I have no idea what half the words mean.",
    "start": "4252850",
    "end": "4258890"
  },
  {
    "text": "Um, but, um, what it says is by far the most common structures seen in collisions at",
    "start": "4258890",
    "end": "4264295"
  },
  {
    "text": "the Large Hadron Collider are collimated sprays of energetic hadrons referred to as jets.",
    "start": "4264295",
    "end": "4270295"
  },
  {
    "text": "These jets are produced from the fragmentation and hadronization of quarks and gluons as described by quantum chromodynamics.",
    "start": "4270295",
    "end": "4279199"
  },
  {
    "text": "Anyone knows what that means? Um, I hope you're following along here. Um, one compelling physics challenge is to search for",
    "start": "4279200",
    "end": "4286969"
  },
  {
    "text": "highly boosted standard model particles decaying hadronically.",
    "start": "4286970",
    "end": "4292000"
  },
  {
    "text": "Unfortunately there's a large background from jets produced by more mundane, um, QCD, that's quantum chromodynamics processes.",
    "start": "4292000",
    "end": "4301090"
  },
  {
    "text": "In this work, we propose instead a solution for jet classification based on an analogy between",
    "start": "4301090",
    "end": "4308215"
  },
  {
    "text": "quantum chromodynamics and natural languages as inspired by several works from natural language, um, processing.",
    "start": "4308215",
    "end": "4316775"
  },
  {
    "text": "Much like a sentence is composed of words following a syntactic structure organized as a parse tree,",
    "start": "4316775",
    "end": "4322865"
  },
  {
    "text": "a jet is also composed of 4-momenta following a structure dictated by",
    "start": "4322865",
    "end": "4328050"
  },
  {
    "text": "QCD and organized via the clustering history of a sequential co- combination jet algorithm.",
    "start": "4328050",
    "end": "4334099"
  },
  {
    "text": "Um, so anyway um, yeah with these jets you see they're getting a tree structure over them and they're using the tree recursive neural network,",
    "start": "4334100",
    "end": "4343794"
  },
  {
    "text": "um, to model it. Um, well that's a little bit far afield but to show you just one more example, um,",
    "start": "4343794",
    "end": "4351435"
  },
  {
    "text": "that another place where these models have actually being quite useful is for doing things in programming languages.",
    "start": "4351435",
    "end": "4359840"
  },
  {
    "text": "And I think in part this, this is because the application is easier in programming languages.",
    "start": "4359840",
    "end": "4366545"
  },
  {
    "text": "So unlike in natural language where we have this uncertainty as to what is the correct parse tree because there's a lot of ambiguity in natural language,",
    "start": "4366545",
    "end": "4375775"
  },
  {
    "text": "in programming languages, um, the parse trees are actually pretty determinant.",
    "start": "4375775",
    "end": "4381175"
  },
  {
    "text": "Um, so a group of people at Berkeley, Dawn Song and her students have worked on doing",
    "start": "4381175",
    "end": "4387559"
  },
  {
    "text": "programming language translation by building tree recursive neural network encoder-decoders.",
    "start": "4387560",
    "end": "4394490"
  },
  {
    "text": "So that you're building up a tree structured neural network representation of a program in one language.",
    "start": "4394490",
    "end": "4402120"
  },
  {
    "text": "This is a CoffeeScript program and then you're wanting to build a tree to tree model which is then translating that to a program in a different language.",
    "start": "4402120",
    "end": "4411760"
  },
  {
    "text": "And they've been able to do that and get good results. Um, I was too lazy to retype this table.",
    "start": "4411760",
    "end": "4418760"
  },
  {
    "text": "So this is probably a bit, bit hard to read. But what's it's contrasting is for a number of programs this is the sort of",
    "start": "4418760",
    "end": "4426320"
  },
  {
    "text": "CoffeeScript to JavaScript, um, um, translation.",
    "start": "4426320",
    "end": "4431650"
  },
  {
    "text": "They're comparing using tree to tree models. Um, and then using sequence to sequence",
    "start": "4431650",
    "end": "4437130"
  },
  {
    "text": "models and then they tried both other combinations, sequence to tree and tree to sequence.",
    "start": "4437130",
    "end": "4443345"
  },
  {
    "text": "Um, and what they find is you can get the best results with the tree to tree neural network models.",
    "start": "4443345",
    "end": "4450175"
  },
  {
    "text": "And in particular these tree to tree models are augmented with attention so they have attention like we talked about",
    "start": "4450175",
    "end": "4457345"
  },
  {
    "text": "the sequence to sequence models where you're then being able to do attention back to nodes in the tree structure which is a pretty natural way of doing translation.",
    "start": "4457345",
    "end": "4466990"
  },
  {
    "text": "And indeed what these results show is if you don't have- that's right these results show is if you don't have the attention operation it doesn't work at all.",
    "start": "4466990",
    "end": "4476034"
  },
  {
    "text": "It's too difficult, um, to get things, um, sort of done if you've just sort of trying to create",
    "start": "4476035",
    "end": "4481050"
  },
  {
    "text": "a single tree representation and then say generate the tra- the translation from that. But if you can do it with this sort of putting attention",
    "start": "4481050",
    "end": "4488245"
  },
  {
    "text": "into the different modes, um, that's great. Um, you might- If you know what CoffeeScript is you might, um,",
    "start": "4488245",
    "end": "4496385"
  },
  {
    "text": "feel like wait that's cheating slightly because CoffeeScript is a bit too similar to JavaScript.",
    "start": "4496385",
    "end": "4502125"
  },
  {
    "text": "Um, but they've also, um, done it in other languages. So this is going between Java and C# and this is a sort of",
    "start": "4502125",
    "end": "4511090"
  },
  {
    "text": "handwritten Java to C# converter that you can download from GitHub if you want but it doesn't actually work that well.",
    "start": "4511090",
    "end": "4518820"
  },
  {
    "text": "Um, and they're able to show, the- they're able to build a far better, um, Java to C# translator,",
    "start": "4518820",
    "end": "4525580"
  },
  {
    "text": "um, doing that. Um, so that's actually kind of cool. And it's good to know that tree structured recursive neural networks",
    "start": "4525580",
    "end": "4533110"
  },
  {
    "text": "are good for some things. Um, so I'm pleased to see work like this. Okay. I'm, I'm, just about done but I thought,",
    "start": "4533110",
    "end": "4541135"
  },
  {
    "text": "um, before, um, finishing, I'd just mention one other [NOISE] thing which is sort of nothing to do",
    "start": "4541135",
    "end": "4547135"
  },
  {
    "text": "with natural language processing precisely but it's about AI. Um, but I wanted to sort of put in a little bit of advertisement.",
    "start": "4547135",
    "end": "4554570"
  },
  {
    "text": "Um, that's something that a number of us have been working on very hard for the last year or so,",
    "start": "4554570",
    "end": "4560855"
  },
  {
    "text": "is developing, um, a new Stanford Institute for Human-Centered Artificial Intelligence.",
    "start": "4560855",
    "end": "4566710"
  },
  {
    "text": "And actually the launch of this institute is going to be on Monday of exam week,",
    "start": "4566710",
    "end": "4571880"
  },
  {
    "text": "just when you're maximally concentrating on things such as this. Um, but our hope is that we can have a lot of",
    "start": "4571880",
    "end": "4579680"
  },
  {
    "text": "new activity around artificial intelligence, taking a much broader perspective to artificial intelligence, um,",
    "start": "4579680",
    "end": "4587510"
  },
  {
    "text": "which is centrally viewing it from the viewpoint of humans and working out, um,",
    "start": "4587510",
    "end": "4594769"
  },
  {
    "text": "I'll- exploring a much broader range of issues that embrace a lot of the interests of the rest of the university whether",
    "start": "4594770",
    "end": "4601490"
  },
  {
    "text": "it's the social sciences and humanities, or also variously in professional schools like the law school and the business school.",
    "start": "4601490",
    "end": "4608945"
  },
  {
    "text": "Um, so let's just quickly say a minute about that. Um, that the, the sort of motivating idea is that sort of for most of my life sort",
    "start": "4608945",
    "end": "4618980"
  },
  {
    "text": "of AI seemed like a kind of a fun intellectual quest as to whether you could write bits of software that did anything,",
    "start": "4618980",
    "end": "4626420"
  },
  {
    "text": "um, halfway intelligent but that's clearly not what's going to be, what's happening for the next 25 years.",
    "start": "4626420",
    "end": "4632820"
  },
  {
    "text": "That we're now at this point in which artificial intelligence systems are being unleashed on society.",
    "start": "4632820",
    "end": "4639680"
  },
  {
    "text": "Um, and well hopefully they do some good things but as we've increasingly been seeing there are lots of",
    "start": "4639680",
    "end": "4646120"
  },
  {
    "text": "also lots of opportunities for them to do bad things. And even if we're not imagining Terminator scenarios,",
    "start": "4646120",
    "end": "4652510"
  },
  {
    "text": "there are just lots of places where people are using machine learning and AI algorithms for making decisions.",
    "start": "4652510",
    "end": "4659545"
  },
  {
    "text": "Some of the worst ones are things like sentencing guidelines in courts where you have very biased algorithms making bad decisions and",
    "start": "4659545",
    "end": "4666949"
  },
  {
    "text": "people are starting to become a lot more aware of the issues and so effectively we are wanting to have this institute sort of",
    "start": "4666950",
    "end": "4674074"
  },
  {
    "text": "embracing a lot of the work of social scientists, the ethicists and other people to actually explore how to have an AI",
    "start": "4674075",
    "end": "4681420"
  },
  {
    "text": "that's really improving human lives rather than having the opposite effect. And so the three themes,",
    "start": "4681420",
    "end": "4687860"
  },
  {
    "text": "um, that we're, um, mainly emphasizing for this institute is the first one in the top left is",
    "start": "4687860",
    "end": "4695870"
  },
  {
    "text": "developing AI technologies but we're particularly interested in making linkages back to human intelligence.",
    "start": "4695870",
    "end": "4702770"
  },
  {
    "text": "So cognitive science and neuroscience that when a lot of the early formative work in AI was",
    "start": "4702770",
    "end": "4708925"
  },
  {
    "text": "done including all of the early work in neural networks like the development of back propagation,",
    "start": "4708925",
    "end": "4715330"
  },
  {
    "text": "it was actually largely done in the context of cognitive science. Right? And that was sort of a linkage that tended to get lost in",
    "start": "4715330",
    "end": "4722090"
  },
  {
    "text": "the '90s and 2000s statistical machine learning emphasis.",
    "start": "4722090",
    "end": "4727139"
  },
  {
    "text": "And I think it would be good to renew that. Um, the top right, um, there's paying much more attention to",
    "start": "4727140",
    "end": "4733910"
  },
  {
    "text": "the human and societal impact of AI and so this is looking at legal issues,",
    "start": "4733910",
    "end": "4739310"
  },
  {
    "text": "economic issues, labor forces, ethics, um, green power, politics, whatever you are.",
    "start": "4739310",
    "end": "4745905"
  },
  {
    "text": "But then down at the bottom is something where it seems like there's just kind of enormous opportunities to do more which is,",
    "start": "4745905",
    "end": "4753725"
  },
  {
    "text": "um, how can we build technology that actually augments human lives.",
    "start": "4753725",
    "end": "4758825"
  },
  {
    "text": "Like to some extent here tech- we've got technology with AI augmenting human lives.",
    "start": "4758825",
    "end": "4765860"
  },
  {
    "text": "So all of your cell phones have speech recognition in them now. So you know that's AI, um,",
    "start": "4765860",
    "end": "4771235"
  },
  {
    "text": "that can augment, um, your human lives. But there's a sense of which not very much of",
    "start": "4771235",
    "end": "4777205"
  },
  {
    "text": "artificial intelligence has actually been put into the service of augmenting human lives.",
    "start": "4777205",
    "end": "4783055"
  },
  {
    "text": "Like most of what a cell phone has on it is still sort of clever and cute stuff done by",
    "start": "4783055",
    "end": "4788159"
  },
  {
    "text": "HCI people and designers which is very nice a lot of the time when you're using your map program or something but we don't really have",
    "start": "4788160",
    "end": "4795470"
  },
  {
    "text": "much AI inside these devices helping to make people's lives better. And so we're hoping not only for individuals when applications like health care,",
    "start": "4795470",
    "end": "4805880"
  },
  {
    "text": "um, to be doing much more of sort of putting artificial intelligence into human-centered applications.",
    "start": "4805880",
    "end": "4812420"
  },
  {
    "text": "Um, anyway that's my brief advertisement. Um, look it out for this while you're not studying for your exams.",
    "start": "4812420",
    "end": "4817840"
  },
  {
    "text": "And I think there'll be sort of lots of opportunities, um, for students and others to be getting more involved in this in the coming months.",
    "start": "4817840",
    "end": "4824315"
  },
  {
    "text": "Okay. Thank you very much. Um, and I will see you later, um, at the poster session.",
    "start": "4824315",
    "end": "4837290"
  },
  {
    "text": "[APPLAUSE].",
    "start": "4837290",
    "end": "4837400"
  }
]