[
  {
    "start": "0",
    "end": "75000"
  },
  {
    "start": "0",
    "end": "4158"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4158",
    "end": "5700"
  },
  {
    "text": "This is part 5 in our series\non natural language inference.",
    "start": "5700",
    "end": "8000"
  },
  {
    "text": "We're going to be talking\nabout attention mechanisms.",
    "start": "8000",
    "end": "10208"
  },
  {
    "text": "Attention was an important\nsource of innovation in the NLI",
    "start": "10208",
    "end": "12830"
  },
  {
    "text": "literature, and, of\ncourse, it's only",
    "start": "12830",
    "end": "14570"
  },
  {
    "text": "grown in prominence since then.",
    "start": "14570",
    "end": "17160"
  },
  {
    "text": "Let's begin with\nsome guiding ideas.",
    "start": "17160",
    "end": "18960"
  },
  {
    "text": "In the context of\nthe NLI problem,",
    "start": "18960",
    "end": "20849"
  },
  {
    "text": "we might have an intuition that\nwe just need more connections",
    "start": "20850",
    "end": "24060"
  },
  {
    "text": "for a lot of our architectures\nbetween the premise",
    "start": "24060",
    "end": "26580"
  },
  {
    "text": "and hypothesis, right?",
    "start": "26580",
    "end": "28110"
  },
  {
    "text": "Possibly, in processing\nthe hypothesis,",
    "start": "28110",
    "end": "30270"
  },
  {
    "text": "we just need the model to\nhave some reminders about what",
    "start": "30270",
    "end": "33360"
  },
  {
    "text": "the premise actually contained.",
    "start": "33360",
    "end": "35220"
  },
  {
    "text": "And whatever summary\nrepresentation we have",
    "start": "35220",
    "end": "37260"
  },
  {
    "text": "of that premise\nmight just not be",
    "start": "37260",
    "end": "38910"
  },
  {
    "text": "enough from the point of view\nof processing the hypothesis",
    "start": "38910",
    "end": "42300"
  },
  {
    "text": "and feeding the representation\ninto the classifier layer.",
    "start": "42300",
    "end": "46230"
  },
  {
    "text": "Relatedly, there's a persistent\nintuition in the NLI literature",
    "start": "46230",
    "end": "50160"
  },
  {
    "text": "that it's useful to softly align\nthe premise and the hypothesis",
    "start": "50160",
    "end": "53520"
  },
  {
    "text": "to find corresponding words\nand phrases between those two",
    "start": "53520",
    "end": "56700"
  },
  {
    "text": "texts.",
    "start": "56700",
    "end": "57480"
  },
  {
    "text": "It can be difficult to do\nthat at a mechanical level,",
    "start": "57480",
    "end": "60180"
  },
  {
    "text": "but attention\nmechanisms might allow",
    "start": "60180",
    "end": "62760"
  },
  {
    "text": "us, via our data-driven\nlearning process,",
    "start": "62760",
    "end": "65128"
  },
  {
    "text": "to find soft connections in\nthe weights for these attention",
    "start": "65129",
    "end": "68220"
  },
  {
    "text": "layers between the\npremise and hypothesis",
    "start": "68220",
    "end": "70590"
  },
  {
    "text": "and achieve some of the\neffects that we would get",
    "start": "70590",
    "end": "72899"
  },
  {
    "text": "from a real alignment process.",
    "start": "72900",
    "end": "76430"
  },
  {
    "start": "75000",
    "end": "216000"
  },
  {
    "text": "So let's begin with\nglobal attention.",
    "start": "76430",
    "end": "78170"
  },
  {
    "text": "This is the simplest\nattention mechanism",
    "start": "78170",
    "end": "79900"
  },
  {
    "text": "that you see in\nthe NLI literature,",
    "start": "79900",
    "end": "81720"
  },
  {
    "text": "but it's already quite powerful.",
    "start": "81720",
    "end": "83058"
  },
  {
    "text": "And as you'll see, it has deep\nconnections with the attention",
    "start": "83058",
    "end": "85600"
  },
  {
    "text": "mechanisms in the transformer.",
    "start": "85600",
    "end": "88159"
  },
  {
    "text": "So to make this concrete, let's\nstart with a simple example.",
    "start": "88160",
    "end": "90760"
  },
  {
    "text": "We have \"every dog\ndanced\" as our premise,",
    "start": "90760",
    "end": "93460"
  },
  {
    "text": "\"some poodle danced\"\nas our hypothesis,",
    "start": "93460",
    "end": "95830"
  },
  {
    "text": "and they're fit together into\nthis chained RNN model for NLI.",
    "start": "95830",
    "end": "99770"
  },
  {
    "text": "Now, standardly,\nwhat we would do",
    "start": "99770",
    "end": "101200"
  },
  {
    "text": "is take this final\nrepresentation, hC,",
    "start": "101200",
    "end": "104079"
  },
  {
    "text": "as the summary representation\nfor the entire sequence",
    "start": "104080",
    "end": "106840"
  },
  {
    "text": "and feed that directly\ninto the classifier.",
    "start": "106840",
    "end": "109840"
  },
  {
    "text": "What we're going to do when\nwe add attention mechanisms is",
    "start": "109840",
    "end": "112689"
  },
  {
    "text": "instead offer some connections\nback from this state",
    "start": "112690",
    "end": "116080"
  },
  {
    "text": "into the premise states.",
    "start": "116080",
    "end": "117920"
  },
  {
    "text": "The way that\nprocess gets started",
    "start": "117920",
    "end": "119380"
  },
  {
    "text": "is via a series of dot products.",
    "start": "119380",
    "end": "120922"
  },
  {
    "text": "So we're going to take\nour target vector,",
    "start": "120922",
    "end": "122630"
  },
  {
    "text": "hC, and take its dot\nproduct with each one",
    "start": "122630",
    "end": "125770"
  },
  {
    "text": "of the hidden representations\ncorresponding to tokens",
    "start": "125770",
    "end": "128888"
  },
  {
    "text": "in the premise.",
    "start": "128889",
    "end": "130210"
  },
  {
    "text": "And that gives us this vector\nof unnormalized scores,",
    "start": "130210",
    "end": "132910"
  },
  {
    "text": "just the dot products.",
    "start": "132910",
    "end": "134570"
  },
  {
    "text": "And it's common,\nthen, to softmax",
    "start": "134570",
    "end": "136210"
  },
  {
    "text": "normalize those scores into\nour attention weights, alpha.",
    "start": "136210",
    "end": "139970"
  },
  {
    "text": "What we do with alpha is then\ncreate our context vector,",
    "start": "139970",
    "end": "142557"
  },
  {
    "text": "and the way that\nhappens is that we're",
    "start": "142557",
    "end": "144140"
  },
  {
    "text": "going to get a weighted view\nof all those premise states.",
    "start": "144140",
    "end": "147580"
  },
  {
    "text": "Each one-- h1, h2, and h3--",
    "start": "147580",
    "end": "149660"
  },
  {
    "text": "is weighted by its corresponding\nattention weight, which",
    "start": "149660",
    "end": "152240"
  },
  {
    "text": "is capturing its kind\nof unnormalized notion",
    "start": "152240",
    "end": "154970"
  },
  {
    "text": "of similarity with\nour target vector hC.",
    "start": "154970",
    "end": "158143"
  },
  {
    "text": "And then to get a fixed\ndimensional version of that,",
    "start": "158143",
    "end": "160310"
  },
  {
    "text": "we take the mean-- or\nit could be the sum--",
    "start": "160310",
    "end": "162830"
  },
  {
    "text": "of all of those weighted\nviews of the premise.",
    "start": "162830",
    "end": "165970"
  },
  {
    "text": "Next, we get our attention\ncombination layer,",
    "start": "165970",
    "end": "168280"
  },
  {
    "text": "and there are various\nways to do this.",
    "start": "168280",
    "end": "170160"
  },
  {
    "text": "One simple one would be to\nsimply concatenate our context",
    "start": "170160",
    "end": "172870"
  },
  {
    "text": "vector with our\noriginal context--",
    "start": "172870",
    "end": "175450"
  },
  {
    "text": "target vector hC and feed those\nthrough a kind of dense layer",
    "start": "175450",
    "end": "179080"
  },
  {
    "text": "of learned parameters.",
    "start": "179080",
    "end": "180868"
  },
  {
    "text": "Another perspective,\nkind of similar,",
    "start": "180868",
    "end": "182410"
  },
  {
    "text": "is to give the context\nvector and our target vector",
    "start": "182410",
    "end": "185410"
  },
  {
    "text": "hC, each one their\nown weights, and have",
    "start": "185410",
    "end": "187720"
  },
  {
    "text": "an additive combination\nof those two,",
    "start": "187720",
    "end": "189590"
  },
  {
    "text": "and, again, feed it through\nsome kind of non-linearity.",
    "start": "189590",
    "end": "193370"
  },
  {
    "text": "And you could think of various\nother designs for this.",
    "start": "193370",
    "end": "195620"
  },
  {
    "text": "And that gives us this\nattention combination, h tilde.",
    "start": "195620",
    "end": "199330"
  },
  {
    "text": "And then, finally,\nthe classifier layer",
    "start": "199330",
    "end": "201610"
  },
  {
    "text": "is a simple dense layer, just\nas before, except instead",
    "start": "201610",
    "end": "204820"
  },
  {
    "text": "of using just hC, we now use\nthis h tilde representation,",
    "start": "204820",
    "end": "208990"
  },
  {
    "text": "which incorporates both hC and\nthat kind of weighted mixture",
    "start": "208990",
    "end": "213700"
  },
  {
    "text": "of premise states.",
    "start": "213700",
    "end": "216980"
  },
  {
    "start": "216000",
    "end": "333000"
  },
  {
    "text": "It might be useful\nto go through this",
    "start": "216980",
    "end": "218659"
  },
  {
    "text": "with some specific\nnumerical values here.",
    "start": "218660",
    "end": "220783"
  },
  {
    "text": "So what I've done\nis, just imagine",
    "start": "220783",
    "end": "222200"
  },
  {
    "text": "that we have two-dimensional\nrepresentations for all",
    "start": "222200",
    "end": "224510"
  },
  {
    "text": "of these vectors.",
    "start": "224510",
    "end": "225620"
  },
  {
    "text": "And you can see, what I've done\nhere is kind of ensure that,",
    "start": "225620",
    "end": "228440"
  },
  {
    "text": "proportionally, \"every\" is a lot\nlike this final representation",
    "start": "228440",
    "end": "232520"
  },
  {
    "text": "here.",
    "start": "232520",
    "end": "233590"
  },
  {
    "text": "And then that kind of\nsimilarity drops off",
    "start": "233590",
    "end": "235757"
  },
  {
    "text": "as we move through\nthe premise states,",
    "start": "235757",
    "end": "237340"
  },
  {
    "text": "and you'll see what happens when\nwe take the dot products here.",
    "start": "237340",
    "end": "240150"
  },
  {
    "text": "So the first step gives us\nthe unnormalized scores,",
    "start": "240150",
    "end": "242750"
  },
  {
    "text": "and you can see that the highest\nunnormalized similarity is",
    "start": "242750",
    "end": "245930"
  },
  {
    "text": "with the first token, followed\nby the second, and then",
    "start": "245930",
    "end": "248659"
  },
  {
    "text": "the third.",
    "start": "248660",
    "end": "249440"
  },
  {
    "text": "The softmax normalization step\nkind of just flattens out those",
    "start": "249440",
    "end": "252650"
  },
  {
    "text": "dot products a\nlittle bit, but we",
    "start": "252650",
    "end": "254239"
  },
  {
    "text": "get the same proportional\nranking with respect to hC.",
    "start": "254240",
    "end": "258902"
  },
  {
    "text": "Here's that context\nvector, and you",
    "start": "258902",
    "end": "260838"
  },
  {
    "text": "can see it's just a mean of\nthe weighted values of all",
    "start": "260839",
    "end": "263960"
  },
  {
    "text": "of these vectors.",
    "start": "263960",
    "end": "265069"
  },
  {
    "text": "That gives us k.",
    "start": "265070",
    "end": "266540"
  },
  {
    "text": "And that k is then fed into this\nattention combination layer.",
    "start": "266540",
    "end": "269708"
  },
  {
    "text": "And you can see, in\norange here, this",
    "start": "269708",
    "end": "271250"
  },
  {
    "text": "is the context vector,\ntwo dimensions.",
    "start": "271250",
    "end": "273770"
  },
  {
    "text": "Down here, we have hC,\njust faithfully repeated.",
    "start": "273770",
    "end": "276560"
  },
  {
    "text": "And then this\nmatrix of weights Wk",
    "start": "276560",
    "end": "279320"
  },
  {
    "text": "is going to give us, in the\nend, after this non-linearity, h",
    "start": "279320",
    "end": "282830"
  },
  {
    "text": "tilde.",
    "start": "282830",
    "end": "284120"
  },
  {
    "text": "And then the classifier\nis as before.",
    "start": "284120",
    "end": "286860"
  },
  {
    "text": "So that's a simple\nworked example",
    "start": "286860",
    "end": "288349"
  },
  {
    "text": "of how these attention\nmechanisms work.",
    "start": "288350",
    "end": "290180"
  },
  {
    "text": "And the idea is that we are\nkind of fundamentally weighting",
    "start": "290180",
    "end": "294229"
  },
  {
    "text": "this target representation\nhC by its similarity",
    "start": "294230",
    "end": "298310"
  },
  {
    "text": "with the previous\npremise states.",
    "start": "298310",
    "end": "300650"
  },
  {
    "text": "But all of them are mixed\nin, and the influence",
    "start": "300650",
    "end": "302810"
  },
  {
    "text": "is kind of proportional to\nthat unnormalized similarity.",
    "start": "302810",
    "end": "307042"
  },
  {
    "text": "There are other scoring\nfunctions that you could use,",
    "start": "307042",
    "end": "309250"
  },
  {
    "text": "of course.",
    "start": "309250",
    "end": "309750"
  },
  {
    "text": "We've just done a simple\ndot product up here,",
    "start": "309750",
    "end": "312080"
  },
  {
    "text": "but you can also imagine having\nlearned parameters in there",
    "start": "312080",
    "end": "315370"
  },
  {
    "text": "or doing concatenation of\nthe learned parameters.",
    "start": "315370",
    "end": "317449"
  },
  {
    "text": "This does a kind\nof bilinear form,",
    "start": "317450",
    "end": "319300"
  },
  {
    "text": "and this is just a concatenation\nof those two states fed",
    "start": "319300",
    "end": "322270"
  },
  {
    "text": "through these learned weights.",
    "start": "322270",
    "end": "323590"
  },
  {
    "text": "And once you see this\nkind of design space,",
    "start": "323590",
    "end": "325449"
  },
  {
    "text": "you can imagine there\nare a lot of other ways",
    "start": "325450",
    "end": "327550"
  },
  {
    "text": "in which you could\nmix in parameters",
    "start": "327550",
    "end": "329379"
  },
  {
    "text": "and have different views of\nthis global attention mechanism.",
    "start": "329380",
    "end": "334220"
  },
  {
    "text": "We could go one\nstep further here.",
    "start": "334220",
    "end": "335900"
  },
  {
    "text": "That was global attention.",
    "start": "335900",
    "end": "337220"
  },
  {
    "text": "In word-by-word\nattention, we're going",
    "start": "337220",
    "end": "338960"
  },
  {
    "text": "to have a lot more learned\nparameters and a lot",
    "start": "338960",
    "end": "341360"
  },
  {
    "text": "more connections between\nthe hypothesis back",
    "start": "341360",
    "end": "344389"
  },
  {
    "text": "into the premise.",
    "start": "344390",
    "end": "345920"
  },
  {
    "text": "So to make this\nkind of tractable,",
    "start": "345920",
    "end": "347510"
  },
  {
    "text": "I've picked one pretty simple\nview of how this could work.",
    "start": "347510",
    "end": "350900"
  },
  {
    "text": "And the way we should\ntrack these computations",
    "start": "350900",
    "end": "352970"
  },
  {
    "text": "is focus on this vector\nB here, because we're",
    "start": "352970",
    "end": "356405"
  },
  {
    "text": "going to move through time.",
    "start": "356405",
    "end": "357530"
  },
  {
    "text": "But let's imagine that we've\nalready processed the A state,",
    "start": "357530",
    "end": "360770"
  },
  {
    "text": "and we will subsequently\nprocess the C state.",
    "start": "360770",
    "end": "363470"
  },
  {
    "text": "So we're focused on B.",
    "start": "363470",
    "end": "365390"
  },
  {
    "text": "And the way we establish\nthese connections",
    "start": "365390",
    "end": "367250"
  },
  {
    "text": "is by taking the\nprevious context",
    "start": "367250",
    "end": "369227"
  },
  {
    "text": "vector that we've created.",
    "start": "369227",
    "end": "370310"
  },
  {
    "text": "That's kA here.",
    "start": "370310",
    "end": "372200"
  },
  {
    "text": "We're going to multiply that by\nrepeated copies of the B state,",
    "start": "372200",
    "end": "375170"
  },
  {
    "text": "and that's simply so that we\nget the same dimensionality",
    "start": "375170",
    "end": "378200"
  },
  {
    "text": "as we have in the\npremise over here,",
    "start": "378200",
    "end": "380030"
  },
  {
    "text": "where I've simply copied\nover into a matrix all three",
    "start": "380030",
    "end": "382639"
  },
  {
    "text": "of those states.",
    "start": "382640",
    "end": "384200"
  },
  {
    "text": "And we have a matrix of\nlearned parameters here",
    "start": "384200",
    "end": "386930"
  },
  {
    "text": "and an additive\ncombination of the two,",
    "start": "386930",
    "end": "388940"
  },
  {
    "text": "followed by a non-linearity.",
    "start": "388940",
    "end": "390560"
  },
  {
    "text": "That's going to give us this M\nhere, which kind of corresponds",
    "start": "390560",
    "end": "393440"
  },
  {
    "text": "to the attention weights in\nthe previous global attention",
    "start": "393440",
    "end": "396080"
  },
  {
    "text": "mechanisms.",
    "start": "396080",
    "end": "397909"
  },
  {
    "text": "We're going to softmax\nnormalize those,",
    "start": "397910",
    "end": "399603"
  },
  {
    "text": "and that literally\ngives us the weights.",
    "start": "399603",
    "end": "401270"
  },
  {
    "text": "And you can see that there\nare some additional parameters",
    "start": "401270",
    "end": "403645"
  },
  {
    "text": "in here to create the\nright dimensionalities.",
    "start": "403645",
    "end": "407280"
  },
  {
    "text": "And then, finally, we\nhave the context at B.",
    "start": "407280",
    "end": "409550"
  },
  {
    "text": "So that's going to be a repeated\nview of all these premises,",
    "start": "409550",
    "end": "412050"
  },
  {
    "text": "weighted by our context\nvector, as before,",
    "start": "412050",
    "end": "414509"
  },
  {
    "text": "and then fed through some\nadditional parameters Wa here.",
    "start": "414510",
    "end": "418500"
  },
  {
    "text": "And that gives us, as you\ncan see here, the context",
    "start": "418500",
    "end": "421180"
  },
  {
    "text": "representation for the state B.",
    "start": "421180",
    "end": "423870"
  },
  {
    "text": "When we move to\nstate C, of course,",
    "start": "423870",
    "end": "425729"
  },
  {
    "text": "that will be used in\nthe place of A here.",
    "start": "425730",
    "end": "428160"
  },
  {
    "text": "And C will go in for\nall these purple values,",
    "start": "428160",
    "end": "430380"
  },
  {
    "text": "and the computation\nwill proceed as before.",
    "start": "430380",
    "end": "432563"
  },
  {
    "text": "And in that way, because we have\nall of these additional learned",
    "start": "432563",
    "end": "435229"
  },
  {
    "text": "parameters, we can\nmeaningfully move",
    "start": "435230",
    "end": "437280"
  },
  {
    "text": "through the entire sequence,\nupdating our parameters",
    "start": "437280",
    "end": "440250"
  },
  {
    "text": "and learning connections from\neach hypothesis token back",
    "start": "440250",
    "end": "443370"
  },
  {
    "text": "into the premise.",
    "start": "443370",
    "end": "444510"
  },
  {
    "text": "So it's much more powerful\nthan the previous view, where",
    "start": "444510",
    "end": "447000"
  },
  {
    "text": "we had relatively few learned\nparameters in our attention",
    "start": "447000",
    "end": "450420"
  },
  {
    "text": "mechanisms, and, therefore, we\ncould only really meaningfully",
    "start": "450420",
    "end": "453210"
  },
  {
    "text": "connect that from the\nstate that we're going",
    "start": "453210",
    "end": "455550"
  },
  {
    "text": "to feed into the classifier.",
    "start": "455550",
    "end": "457080"
  },
  {
    "text": "So this is much more\nexpressive, right?",
    "start": "457080",
    "end": "460500"
  },
  {
    "text": "And then once we have done the\nentire sequence processing,",
    "start": "460500",
    "end": "463570"
  },
  {
    "text": "finally, we get the\nrepresentation for C",
    "start": "463570",
    "end": "466005"
  },
  {
    "text": "here, as fed through\nthese mechanisms,",
    "start": "466005",
    "end": "467910"
  },
  {
    "text": "and that becomes the\ninput to the classifier",
    "start": "467910",
    "end": "470280"
  },
  {
    "text": "that we ultimately use.",
    "start": "470280",
    "end": "473440"
  },
  {
    "text": "The connection with the\ntransformer should be apparent.",
    "start": "473440",
    "end": "475768"
  },
  {
    "text": "This is going to return us\nback to the global attention",
    "start": "475768",
    "end": "478060"
  },
  {
    "text": "mechanism.",
    "start": "478060",
    "end": "478770"
  },
  {
    "text": "Recall that for\nthe transformer, we",
    "start": "478770",
    "end": "480490"
  },
  {
    "text": "have these sequences\nof tokens with",
    "start": "480490",
    "end": "482990"
  },
  {
    "text": "their positional encodings.",
    "start": "482990",
    "end": "484599"
  },
  {
    "text": "That gives us an embedding here.",
    "start": "484600",
    "end": "486410"
  },
  {
    "text": "And at that point,\nwe establish a lot",
    "start": "486410",
    "end": "488350"
  },
  {
    "text": "of dot product connections.",
    "start": "488350",
    "end": "490017"
  },
  {
    "text": "And I showed you in the\nlecture on the transformer",
    "start": "490017",
    "end": "492100"
  },
  {
    "text": "that the mechanisms here are\nidentical to the mechanisms",
    "start": "492100",
    "end": "495640"
  },
  {
    "text": "that we used for dot\nproduct attention.",
    "start": "495640",
    "end": "497420"
  },
  {
    "text": "It's just that in the\ncontext of the transformer,",
    "start": "497420",
    "end": "499730"
  },
  {
    "text": "we do it from every state\nto every other state.",
    "start": "499730",
    "end": "501990"
  },
  {
    "start": "501990",
    "end": "505568"
  },
  {
    "text": "And then, of course,\nthe computations",
    "start": "505568",
    "end": "507110"
  },
  {
    "text": "proceed through subsequent\nsteps in the transformer layer",
    "start": "507110",
    "end": "510289"
  },
  {
    "text": "and on through multiple\ntransformer layers,",
    "start": "510290",
    "end": "512510"
  },
  {
    "text": "potentially.",
    "start": "512510",
    "end": "514892"
  },
  {
    "start": "514000",
    "end": "601000"
  },
  {
    "text": "And there are some\nother variants, right?",
    "start": "514892",
    "end": "516599"
  },
  {
    "text": "This is just the beginning\nof a very large design",
    "start": "516600",
    "end": "518909"
  },
  {
    "text": "space for attention mechanisms.",
    "start": "518909",
    "end": "520340"
  },
  {
    "text": "Let me just mention a few.",
    "start": "520340",
    "end": "521700"
  },
  {
    "text": "We could have local attention.",
    "start": "521700",
    "end": "523135"
  },
  {
    "text": "This was actually an\nearly contribution",
    "start": "523135",
    "end": "524760"
  },
  {
    "text": "in the context of\nmachine translation.",
    "start": "524760",
    "end": "526990"
  },
  {
    "text": "And this would build connections\nbetween selected points",
    "start": "526990",
    "end": "529560"
  },
  {
    "text": "and the premise and\nhypothesis, based",
    "start": "529560",
    "end": "531630"
  },
  {
    "text": "on some possibly a\npriori notion we have",
    "start": "531630",
    "end": "534480"
  },
  {
    "text": "of which things are likely to\nbe important for our problem.",
    "start": "534480",
    "end": "538029"
  },
  {
    "text": "Word-by-word attention,\nas I've said,",
    "start": "538030",
    "end": "539680"
  },
  {
    "text": "can be set up in many ways, with\nmany more learned parameters.",
    "start": "539680",
    "end": "542920"
  },
  {
    "text": "And the classic paper\nis the one that I'm",
    "start": "542920",
    "end": "544810"
  },
  {
    "text": "recommending for reading\nfor this unit, Rocktaschel",
    "start": "544810",
    "end": "547060"
  },
  {
    "text": "et al., where they do a\nreally pioneering view of this",
    "start": "547060",
    "end": "551110"
  },
  {
    "text": "in using even more complex\nattention mechanisms than I",
    "start": "551110",
    "end": "554140"
  },
  {
    "text": "presented under\nword-by-word attention",
    "start": "554140",
    "end": "556180"
  },
  {
    "text": "but following a lot of the\nsame intuitions, I would say.",
    "start": "556180",
    "end": "560339"
  },
  {
    "text": "The attention\nrepresentation at a time t",
    "start": "560340",
    "end": "562230"
  },
  {
    "text": "could be appended to the\nhidden representation",
    "start": "562230",
    "end": "564180"
  },
  {
    "text": "at time t plus 1.",
    "start": "564180",
    "end": "565335"
  },
  {
    "text": "This would give us another\nway of moving sequentially",
    "start": "565335",
    "end": "567930"
  },
  {
    "text": "through the sequence,\nhaving meaningful attention",
    "start": "567930",
    "end": "570450"
  },
  {
    "text": "at each one of those\npoints, as opposed",
    "start": "570450",
    "end": "572462"
  },
  {
    "text": "to the global attention,\nwhich would just",
    "start": "572462",
    "end": "574170"
  },
  {
    "text": "be for that final state.",
    "start": "574170",
    "end": "576172"
  },
  {
    "text": "And then there are\nother connections even",
    "start": "576172",
    "end": "577880"
  },
  {
    "text": "further afield.",
    "start": "577880",
    "end": "578840"
  },
  {
    "text": "For example, memory\nnetworks can be",
    "start": "578840",
    "end": "580550"
  },
  {
    "text": "used to address similar\nissues, and they",
    "start": "580550",
    "end": "582320"
  },
  {
    "text": "have similar intuitions behind\nthem as attention mechanisms",
    "start": "582320",
    "end": "585980"
  },
  {
    "text": "as applied to the NLI problem.",
    "start": "585980",
    "end": "587870"
  },
  {
    "text": "And that's kind of more\nexplicitly drawing on this idea",
    "start": "587870",
    "end": "590300"
  },
  {
    "text": "that we might, in late\nstates in processing,",
    "start": "590300",
    "end": "593029"
  },
  {
    "text": "need a bit of a\nreminder about what",
    "start": "593030",
    "end": "594980"
  },
  {
    "text": "was in the previous\ncontext that we processed.",
    "start": "594980",
    "end": "598420"
  },
  {
    "start": "598420",
    "end": "602000"
  }
]