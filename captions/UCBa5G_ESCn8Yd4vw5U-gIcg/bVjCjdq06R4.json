[
  {
    "start": "0",
    "end": "5520"
  },
  {
    "text": "Before we get started, a\ncouple of logistical things. Homework Zero is due\ntonight at 11:59.",
    "start": "5520",
    "end": "12690"
  },
  {
    "text": "Homework One has\nalso been posted now and it will be due on\nWednesday next week.",
    "start": "12690",
    "end": "18990"
  },
  {
    "text": "We are also going to be\nposting a number of resources for your project today. First, we're going to be posting\na number of project ideas.",
    "start": "18990",
    "end": "26320"
  },
  {
    "text": "So if you're not\nsure what to work on and you want some ideas, that\nshould be pretty helpful. We're also going to post\nsome example projects",
    "start": "26320",
    "end": "32549"
  },
  {
    "text": "from last year\nthat could give you a little bit of a flavor of the\nkinds of things that have done, people have done for\nthe class in the past.",
    "start": "32549",
    "end": "38520"
  },
  {
    "text": "And then lastly,\nsome people mentioned that they'd love some help\nconnecting with others in the class that have\nsimilar interests,",
    "start": "38520",
    "end": "44790"
  },
  {
    "text": "and so we're also going to be\nposting a form that allows you to put down your interests\nand whether or not you're",
    "start": "44790",
    "end": "52440"
  },
  {
    "text": "looking for project\ncollaborators. And we'll also post the\nresponses to that form",
    "start": "52440",
    "end": "59100"
  },
  {
    "text": "today, as well. And so it'll just help\nyou connect with people who have similar interests. This is an optional form.",
    "start": "59100",
    "end": "65069"
  },
  {
    "text": "It's more just to\nhelp you find people, if you're looking for\npeople to work with. ",
    "start": "65069",
    "end": "71970"
  },
  {
    "text": "One other important note,\nas well, on logistics is that these days there are\nsome pretty fancy AI based code",
    "start": "71970",
    "end": "79860"
  },
  {
    "text": "completion tools like GitHub-- actually, sorry, that's a typo-- Co-Pilot.",
    "start": "79860",
    "end": "84930"
  },
  {
    "text": "There's also a more recent\none that's even open source. There aren't-- these are pretty\nnew and so there aren't a lot",
    "start": "84930",
    "end": "91890"
  },
  {
    "text": "of standard course policies\naround using these tools. We'd like to say\nthat it's OK for you",
    "start": "91890",
    "end": "97470"
  },
  {
    "text": "to use these kind of\nthings for the project. But we'd say that it's\nnot OK for using it for the assignments\nbecause the assignments,",
    "start": "97470",
    "end": "103470"
  },
  {
    "text": "you should really be able to\nunderstand the code that you're writing rather than\ntrying to ask an AI to do",
    "start": "103470",
    "end": "108703"
  },
  {
    "text": "your homework for you. ",
    "start": "108703",
    "end": "113760"
  },
  {
    "text": "Cool. And then the last\nlogistical item also, as well, is\nthat your feedback",
    "start": "113760",
    "end": "119930"
  },
  {
    "text": "is really important\nto helping us help you to make sure the class\nis a great experience for you. And so one thing that\nwe're going to be doing",
    "start": "119930",
    "end": "126320"
  },
  {
    "text": "is called high resolution\nfeedback starting this week, and it will-- every week, a\nrandom subset of the class",
    "start": "126320",
    "end": "133730"
  },
  {
    "text": "will be getting a form,\na feedback form, where you can tell us how things\nare going and so forth,",
    "start": "133730",
    "end": "138860"
  },
  {
    "text": "and we'll use this\nto improve the class. So not all of you will\nget this this week. It will be a different\nsubset every week",
    "start": "138860",
    "end": "145462"
  },
  {
    "text": "so that we're not\nputting a ton of burden on you to fill out\nsurveys every single week but we can also still get\nfeedback throughout the course.",
    "start": "145462",
    "end": "152720"
  },
  {
    "text": "Great. And then lastly,\nwe've also finalized the guest lectures for\nthe end of the course. I'm really excited that we'll\nhave Hanie and Percy give",
    "start": "152720",
    "end": "160130"
  },
  {
    "text": "guest lectures. Hanie is at Google. She does a lot of really\ncool work on transfer",
    "start": "160130",
    "end": "165140"
  },
  {
    "text": "learning and understanding\ndeep learning, and Percy is here\nat Stanford and he does a lot of work\non foundation models,",
    "start": "165140",
    "end": "170329"
  },
  {
    "text": "natural language processing,\nand understanding emergent few-shot learning. So I'm excited to\nsee those lectures",
    "start": "170330",
    "end": "176600"
  },
  {
    "text": "at the end of the course. Great. So those are all the logistics.",
    "start": "176600",
    "end": "182397"
  },
  {
    "text": "Any questions on all that? So a lot. All of this is on-- well, these things are\non the course website.",
    "start": "182397",
    "end": "188800"
  },
  {
    "text": "The project resources will\nbe posted on Ed, so, yeah. Nothing is only here.",
    "start": "188800",
    "end": "195830"
  },
  {
    "text": "Awesome. So last lecture on\nWednesday last week,",
    "start": "195830",
    "end": "200900"
  },
  {
    "text": "we talked about\nmulti task learning and we defined what a task is\nas a set of data generating",
    "start": "200900",
    "end": "206330"
  },
  {
    "text": "distributions from which a\ntraining set and a test set is sampled. The task also has a\ncorresponding loss function.",
    "start": "206330",
    "end": "214849"
  },
  {
    "text": "And we didn't\nexplicitly cover this, but you can think of learning\na task as essentially taking as input a data set and\npredicting a set of parameters.",
    "start": "214850",
    "end": "225300"
  },
  {
    "text": "And those parameters would be\nthe parameters of your model. We also talked about\nmulti task learning, where we're going to be\ntraining a neural network that's",
    "start": "225300",
    "end": "231739"
  },
  {
    "text": "conditioned on a\ndescription of the task, z, and is going to be making\npredictions for inputs given",
    "start": "231740",
    "end": "237260"
  },
  {
    "text": "that descriptor of the task. We talked about how basically\nthe choice of conditioning",
    "start": "237260",
    "end": "244610"
  },
  {
    "text": "on z in different ways affects\nhow the parameters are shared and different design choices\nwith respect to that choice.",
    "start": "244610",
    "end": "251940"
  },
  {
    "text": "So if you observe\nnegative transfer, you might consider\nsharing less information and designing architectures\nand designing ways to condition",
    "start": "251940",
    "end": "258799"
  },
  {
    "text": "on z that affect that. And likewise, if you\nobserve overfitting, you may want to try\nsharing more and have",
    "start": "258800",
    "end": "266030"
  },
  {
    "text": "more shared parameters. Lastly, we also talked about\nthis objective function for multi task learning and how\nif you normalize your labels,",
    "start": "266030",
    "end": "275930"
  },
  {
    "text": "then just adding up the losses\nand optimizing like that is a great choice. But you may also want to\nchoose the task weightings",
    "start": "275930",
    "end": "282680"
  },
  {
    "text": "in a way that affects the\nprioritization of the tasks. Great. So that's a brief recap\nof Wednesday's lecture.",
    "start": "282680",
    "end": "290850"
  },
  {
    "text": "Now the plan for\ntoday is we're going to talk about transfer\nlearning and the problem",
    "start": "290850",
    "end": "296160"
  },
  {
    "text": "formulation of\nthat, and then we're going to actually start\ntalking about meta learning and the\nproblem formulation",
    "start": "296160",
    "end": "302010"
  },
  {
    "text": "and the way that we\ncan think about what meta learning actually is. This will actually get into\nthe start of Homework One",
    "start": "302010",
    "end": "310349"
  },
  {
    "text": "and the lecture on\nWednesday will kind of cover the rest of the\ncontent that's needed for completing Homework One.",
    "start": "310350",
    "end": "318599"
  },
  {
    "text": "Cool. And then from there,\nthe cool things that we'll cover today in\nterms of learning goals are thinking about how you can\ntransfer things from one task",
    "start": "318600",
    "end": "325610"
  },
  {
    "text": "to another task, what does\nit mean for multiple tasks to have some sort of\nshared structure, which",
    "start": "325610",
    "end": "331460"
  },
  {
    "text": "is this somewhat nebulous\nnotion that we've been talking about for the last week, and\nalso what is meta learning.",
    "start": "331460",
    "end": "339900"
  },
  {
    "text": "Awesome. So let's get started by talking\nabout transfer learning. So in lecture before,\nwe talked about trying",
    "start": "339900",
    "end": "346850"
  },
  {
    "text": "to solve multiple tasks at\nonce, and in transfer learning, things are a little\nbit different.",
    "start": "346850",
    "end": "352830"
  },
  {
    "text": "So in transfer\nlearning, our goal is typically to solve a\nparticular target task",
    "start": "352830",
    "end": "358160"
  },
  {
    "text": "after having previously\nsolved a source task or a set of source tasks.",
    "start": "358160",
    "end": "363610"
  },
  {
    "text": "And the goal here is, when we\ntry to solve this target task, we want to transfer some of\nwhat was learned on task a",
    "start": "363610",
    "end": "371620"
  },
  {
    "text": "when trying to solve task b. And a common assumption\nhere is that typically you",
    "start": "371620",
    "end": "378790"
  },
  {
    "text": "can't access the\ndata from task a when you're trying\nto solve task b.",
    "start": "378790",
    "end": "384580"
  },
  {
    "text": "And so you basically want to\ncondense all your information or knowledge about task\na into some parameters",
    "start": "384580",
    "end": "391660"
  },
  {
    "text": "and then use that\ncondensed knowledge when trying to address task b.",
    "start": "391660",
    "end": "398639"
  },
  {
    "text": "Now it's worth mentioning here\nthat transfer learning-- you can think of it as a valid\nsolution to multi task",
    "start": "398640",
    "end": "403780"
  },
  {
    "text": "learning, because if you\nwant to learn two tasks, then you can learn\ntask one and then transfer that to more\neffectively learn task two.",
    "start": "403780",
    "end": "410710"
  },
  {
    "text": "That will give you a solution\nto task one and task two. However, because of this\npretty common assumption,",
    "start": "410710",
    "end": "418570"
  },
  {
    "text": "multi task learning\nis not usually thought of as a valid\nsolution to transfer learning because\nyou can't access",
    "start": "418570",
    "end": "425259"
  },
  {
    "text": "the data for one of the tasks\nduring the transfer process. Don't access the data sets of\nthe two tasks at the same time.",
    "start": "425260",
    "end": "432699"
  },
  {
    "text": " Cool. So from there, I have\na question for you.",
    "start": "432700",
    "end": "439159"
  },
  {
    "text": "So now that I've introduced\nthese two problem statements, I'm curious if you\nhave some ideas for what kinds of problems you\nmight run into where transfer",
    "start": "439160",
    "end": "447819"
  },
  {
    "text": "learning might\nmake a lot of sense and might make more sense than\nrunning multi task learning.",
    "start": "447820",
    "end": "454919"
  },
  {
    "text": "Yeah. Your source has way more\ndata than your chart. Yeah. So if your source\nhas a ton of data",
    "start": "454920",
    "end": "461700"
  },
  {
    "text": "that you don't want to have to\nkeep this around and retrain on it when you're\nsolving task b, you want to condense that\nknowledge down and try",
    "start": "461700",
    "end": "468300"
  },
  {
    "text": "to transfer it. So that's one setting\nwhere you might want to use transfer learning. Other scenarios?",
    "start": "468300",
    "end": "474889"
  },
  {
    "text": "Yeah. There are certain tasks may not\nhave a lot of data associated with it. So you want to learn\nfrom previous tasks.",
    "start": "474890",
    "end": "483180"
  },
  {
    "text": "Yeah. So if you don't\nhave a lot of tasks, a lot of data for\nyour target task, then something like\ntransfer learning",
    "start": "483180",
    "end": "488509"
  },
  {
    "text": "might make sense there. I also think that something\nlike multi task learning could probably also be\napplicable to that sort of setting, as well.",
    "start": "488510",
    "end": "495450"
  },
  {
    "text": "But it is a setting where\ntransfer learning is often, is often used.",
    "start": "495450",
    "end": "500930"
  },
  {
    "text": "Yeah. Would you mean that\ntask a and task b have some sort of\nco-relationships?",
    "start": "500930",
    "end": "508300"
  },
  {
    "text": "For example, once you\nlocate [INAUDIBLE] maybe first you need to\nrecognize or identify,",
    "start": "508300",
    "end": "516429"
  },
  {
    "text": "so then-- because\npatients can be task a, their location can be task\nb, something like that.",
    "start": "516429",
    "end": "522079"
  },
  {
    "text": "Yeah. So if the tasks have a\nlot of shared structure. That may also be a\ngood scenario to use transfer learning\nand also a scenario",
    "start": "522080",
    "end": "527511"
  },
  {
    "text": "where multi task\nlearning might apply. Yeah. [INAUDIBLE] so you don't\nhave to [INAUDIBLE]..",
    "start": "527512",
    "end": "536540"
  },
  {
    "text": "Yeah, exactly. So have you already learned\ntask a and the weights, you can just download\nfrom the internet. Maybe you haven't\ntrained it yourself,",
    "start": "536540",
    "end": "542773"
  },
  {
    "text": "maybe someone else did,\nand they put their weights on the internet,\nthen you don't even actually have to solve task a.",
    "start": "542773",
    "end": "547850"
  },
  {
    "text": "You can just take their\nweights, take their solution, and use that for task b. And so that's another scenario\nwhere it's very commonly used.",
    "start": "547850",
    "end": "557170"
  },
  {
    "text": "One more. Maybe you're deploying this\ninto an environment where you don't have the capacity\nfor very much once you're there",
    "start": "557170",
    "end": "564880"
  },
  {
    "text": "but only then will you\nunderstand what task b really is. Yeah. Exactly. So you might be in\na scenario where",
    "start": "564880",
    "end": "570298"
  },
  {
    "text": "you don't know what all\nthe tasks are up front. You're in a scenario where maybe\nyou want to very quickly adapt",
    "start": "570298",
    "end": "576790"
  },
  {
    "text": "to a new task. Maybe you're trying\nto adapt to-- like on a cell\nphone, for example-- to a particular user.",
    "start": "576790",
    "end": "582620"
  },
  {
    "text": "In that sort of scenario,\nsomething like transfer learning makes a lot of sense. Cool.",
    "start": "582620",
    "end": "587680"
  },
  {
    "text": "So those are, like-- yeah,\nlots of really, really great examples. I had a couple examples\nthat I put on the slide.",
    "start": "587680",
    "end": "593873"
  },
  {
    "text": "The first was mentioned, if\nyou have a really large source data set, you don't want to\nhave to retain this and retrain",
    "start": "593873",
    "end": "599800"
  },
  {
    "text": "on it with multi task learning. And then the second\ncase that I mentioned",
    "start": "599800",
    "end": "605470"
  },
  {
    "text": "is you may actually\nhave a scenario where you don't actually care about\nsolving both of the tasks simultaneously.",
    "start": "605470",
    "end": "612880"
  },
  {
    "text": "For example, if ultimately\nyour goal is to kind of deploy a model on different\npeople's phones,",
    "start": "612880",
    "end": "618012"
  },
  {
    "text": "you don't need to have\na model that works for everyone at the same time. You just need to\nbe able to adapt the model to a user at any\nparticular point in time.",
    "start": "618012",
    "end": "627570"
  },
  {
    "text": "Yeah. [INAUDIBLE] we cannot\nusually [INAUDIBLE]",
    "start": "627570",
    "end": "642790"
  },
  {
    "text": "--weights all from the model A? Yeah. Typically what you\nwill have about task a is something like the weights\nor something that you learned.",
    "start": "642790",
    "end": "649263"
  },
  {
    "text": "It could be more\nthan the weights. Maybe you have some information\nabout the optimizer, for example, that\nmaybe you have stored.",
    "start": "649263",
    "end": "655860"
  },
  {
    "text": "But typically, it's\nthe weights that you have trained on task a. ",
    "start": "655860",
    "end": "664060"
  },
  {
    "text": "Cool. So now that we've talked\nabout the problem set up,",
    "start": "664060",
    "end": "669382"
  },
  {
    "text": "we'll talk about\nfine tuning, which is basically the go to\napproach for transfer learning.",
    "start": "669382",
    "end": "675340"
  },
  {
    "text": "And the way that fine\ntuning works is we'll take the weights that\nwere trained on task a--",
    "start": "675340",
    "end": "682690"
  },
  {
    "text": "I'll denote this with theta-- and we will initialize a neural\nnetwork with those weights",
    "start": "682690",
    "end": "688930"
  },
  {
    "text": "and then run gradient descent\non the target task initialized at those weights.",
    "start": "688930",
    "end": "695530"
  },
  {
    "text": "And so if D train\nis the training data for your new task, then\nyou'll evaluate the gradient,",
    "start": "695530",
    "end": "701080"
  },
  {
    "text": "apply that gradient at the--\nthat work initialized at theta. This is showing just one\nstep of gradient descent,",
    "start": "701080",
    "end": "707470"
  },
  {
    "text": "but in practice,\nyou would typically do this for a number\nof gradient steps. And then the result\nof this process",
    "start": "707470",
    "end": "713889"
  },
  {
    "text": "is you'll get a set\nof parameters phi that are hopefully actually\nmuch better at task b",
    "start": "713890",
    "end": "720430"
  },
  {
    "text": "than if you were to\nrandomly initialize theta. And in fact, if you\nactually compare",
    "start": "720430",
    "end": "728110"
  },
  {
    "text": "using randomly initialized\nparameters versus parameters preinitialized with a data\nset that is effective,",
    "start": "728110",
    "end": "734800"
  },
  {
    "text": "is useful, for the\ntarget task, we can see that we get\nmuch better performance. So in particular, here's an\nexample where a neural network",
    "start": "734800",
    "end": "742149"
  },
  {
    "text": "is pretrained on ImageNet, or\nit is initialized randomly, and the target data set is these\ntwo data sets, PASCAL and SUN,",
    "start": "742150",
    "end": "749860"
  },
  {
    "text": "which are both image\nrecognition data sets. And we see that we're able\nto do remarkably better if we",
    "start": "749860",
    "end": "756790"
  },
  {
    "text": "pretrain on ImageNet\ncompared to pretraining from scratch on the\norder of like 16% to 17%.",
    "start": "756790",
    "end": "763640"
  },
  {
    "text": " So this is really cool.",
    "start": "763640",
    "end": "768790"
  },
  {
    "text": "Essentially what\nthis is doing is that it's taking all the\nrich information that exists in the ImageNet\ndata set, leveraging",
    "start": "768790",
    "end": "775330"
  },
  {
    "text": "that in the context\nof transferring to these new tasks. Yeah.",
    "start": "775330",
    "end": "781222"
  },
  {
    "text": "[INAUDIBLE] transfer\ndoesn't make sense between two\n[INAUDIBLE] So,",
    "start": "781222",
    "end": "789440"
  },
  {
    "text": "like, how do I look\nat the [INAUDIBLE]",
    "start": "789440",
    "end": "796350"
  },
  {
    "text": "Yeah. So the question is, is\nthere some criterion that could tell us if\ntransfer learning will work,",
    "start": "796350",
    "end": "802940"
  },
  {
    "text": "if it will be helpful. And, in general, kind of similar\nto the multi task learning",
    "start": "802940",
    "end": "808367"
  },
  {
    "text": "scenario where it's difficult\nto tell if training on two tasks together will be helpful,\nit's also very difficult",
    "start": "808367",
    "end": "814070"
  },
  {
    "text": "to just develop some criterion. And so, in general,\nthere are some kind of general common\nwisdom for getting",
    "start": "814070",
    "end": "821210"
  },
  {
    "text": "these things to work well. And in general, for example,\nif intuitively the tasks seem related, then it\ncan be a good choice.",
    "start": "821210",
    "end": "829190"
  },
  {
    "text": "But in practice, there\nisn't any hard rule that will tell us if\nit will work or not.",
    "start": "829190",
    "end": "835280"
  },
  {
    "text": "And so I guess I should\nsay that what I just said is dissatisfying to me.",
    "start": "835280",
    "end": "840580"
  },
  {
    "text": "I think it would\nbe really awesome if we had something that\ncould tell us basically whether this would work. And we'll talk a little bit\nabout some recent research",
    "start": "840580",
    "end": "848620"
  },
  {
    "text": "in some of the coming slides. But also investigating\nthis sort of thing for like a course\nproject I think",
    "start": "848620",
    "end": "854140"
  },
  {
    "text": "could be quite interesting. Yeah. I assume in this case, you're\nretraining all the layers?",
    "start": "854140",
    "end": "862790"
  },
  {
    "text": "Yeah. So in this case, we're\nretraining the entire parameter vector. I guess I'll try\nto finish the slide",
    "start": "862790",
    "end": "868000"
  },
  {
    "text": "and then we can get\nto more questions. So kind of related to,\nlike, when will this",
    "start": "868000",
    "end": "875139"
  },
  {
    "text": "work, when will this not work,\nthere's a number of choices that you can select for\nother pretrained parameters.",
    "start": "875140",
    "end": "881930"
  },
  {
    "text": "Things like ImageNet\nclassification or models trained on a very\nlarge language corpora are common choices in\ncomputer vision NLP.",
    "start": "881930",
    "end": "889480"
  },
  {
    "text": "And in general, if you can\npretrain on a diverse data set that covers\nthe kinds of data",
    "start": "889480",
    "end": "895390"
  },
  {
    "text": "that you'll be seeing\nduring fine tuning, this, that's kind of\ngenerally a good choice.",
    "start": "895390",
    "end": "902480"
  },
  {
    "text": "It doesn't even need\nto necessarily cover the distribution of what\nyou'll see at test time.",
    "start": "902480",
    "end": "908649"
  },
  {
    "text": "So, for example, in\nImageNet classification, people have found\nthat you actually get very general purpose\nvisual features from those.",
    "start": "908650",
    "end": "915529"
  },
  {
    "text": "And you can take\nthose visual features and actually apply it to-- try to transfer them\nto satellite images",
    "start": "915530",
    "end": "921160"
  },
  {
    "text": "or try to transfer\nthem to medical images, and those visual features are\nactually still pretty useful, even though those images\nare out of distribution",
    "start": "921160",
    "end": "928180"
  },
  {
    "text": "compared to what ImageNet\nimages look like. Yeah. ",
    "start": "928180",
    "end": "934089"
  },
  {
    "text": "Don't you think that\nthis is doing something like creating a dictionary\nof latent variables",
    "start": "934090",
    "end": "939790"
  },
  {
    "text": "[INAUDIBLE] approximate\ndictionary and then fine tuning is basically updating the\ndictionary a little bit?",
    "start": "939790",
    "end": "945770"
  },
  {
    "text": "Yeah. So the question is\ncan we think of this as, the pretraining process, as\nlearning a dictionary of latent",
    "start": "945770",
    "end": "951400"
  },
  {
    "text": "variables and then the\nfine tuning process is essentially as updating the--",
    "start": "951400",
    "end": "957580"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "957580",
    "end": "964503"
  },
  {
    "text": "I guess I'm not fully sure\nwhat fixing it towards the task exactly means. I mean, the--",
    "start": "964503",
    "end": "970240"
  },
  {
    "text": "I think that\nintuitively there's-- I guess, I think there's a\nlot of intuitive explanations for what it may be doing.",
    "start": "970240",
    "end": "976900"
  },
  {
    "text": "One intuitive\nexplanation is that it's giving you good features. So it's giving you a good\nrepresentation of images,",
    "start": "976900",
    "end": "982960"
  },
  {
    "text": "for example. It gives you a good way to\nlook at images or a good way to read text. So that's one explanation,\nand once you have,",
    "start": "982960",
    "end": "989410"
  },
  {
    "text": "once you kind of roughly know\nhow to see at a course level, then that makes it\nmuch easier to be able to recognize other images.",
    "start": "989410",
    "end": "996910"
  },
  {
    "text": "And then a completely\nseparate explanation might be if you think about\nit from the standpoint of optimization, you have this\nreally complex optimization",
    "start": "996910",
    "end": "1004500"
  },
  {
    "text": "landscape that is\nnon convex, then you can possibly think\nof this as trying to put you in the right basin\nof that optimization landscape.",
    "start": "1004500",
    "end": "1012630"
  },
  {
    "text": "And once you're in\nthe right basin, then if you run gradient\ndescent from that basin, you'll get to a better solution\nthan if you started off",
    "start": "1012630",
    "end": "1020130"
  },
  {
    "text": "kind of at a random spot\nin that optimization or in that landscape. Yeah.",
    "start": "1020130",
    "end": "1025480"
  },
  {
    "text": "So can this be related\nto the idea where you have a simple\nmodel within that model",
    "start": "1025480",
    "end": "1030520"
  },
  {
    "text": "and then initialize a more\ncomplex model with weights learned from a simpler model? Or does this fall\noutside that paradigm?",
    "start": "1030520",
    "end": "1037930"
  },
  {
    "text": "So starting with a\nsimple model and going towards a more complex model. So in general,\nsomething like that I",
    "start": "1037930",
    "end": "1043119"
  },
  {
    "text": "would refer to as perhaps a form\nof curriculum learning where you move from kind of simpler\ntasks towards more and more",
    "start": "1043119",
    "end": "1050470"
  },
  {
    "text": "difficult tasks. And things like\ntransfer learning can be applied to that,\nbut that doesn't always",
    "start": "1050470",
    "end": "1055847"
  },
  {
    "text": "have to be the case. You don't have to start\nwith a simple task and move to a more complex task. In fact, oftentimes you actually\nmove from a more complex task",
    "start": "1055847",
    "end": "1063490"
  },
  {
    "text": "to a simpler one, like ImageNet\nclassification is actually a really, really hard task\nto learn from scratch. And then you might\nmove to something",
    "start": "1063490",
    "end": "1069730"
  },
  {
    "text": "that's a little bit\nsimpler, like maybe you want to be able to classify\nbetween cats and dogs or something like that.",
    "start": "1069730",
    "end": "1075386"
  },
  {
    "text": "Yeah. So if you have a large\namount of data sets, does it make sense to\npretrain a model from scratch?",
    "start": "1075386",
    "end": "1082040"
  },
  {
    "text": "Or do you think that fine\ntuning would generally maybe still provide some benefits? Yeah. So if you have a\nlarge enough data set,",
    "start": "1082040",
    "end": "1088135"
  },
  {
    "text": "does pretraining\nmake sense or can you just train from scratch? If you do have a large data set,\nthen it's, from a performance",
    "start": "1088135",
    "end": "1095840"
  },
  {
    "text": "standpoint, it may be\nthat you can already do quite well training from\nscratch on that large data set.",
    "start": "1095840",
    "end": "1101390"
  },
  {
    "text": "There are still some\npotential benefits. One very obvious\npotential benefit",
    "start": "1101390",
    "end": "1106429"
  },
  {
    "text": "is that you may not need as much\ncompute to get to the solution. So even if you have\na large data set,",
    "start": "1106430",
    "end": "1113330"
  },
  {
    "text": "if you can initialize\nwith the pretrained model, then you may need many\nfewer gradient steps to--",
    "start": "1113330",
    "end": "1118737"
  },
  {
    "text": "and many fewer compute\ncycles-- to get to a good solution\non that task compared to if you were to train\nwithout any prior knowledge.",
    "start": "1118737",
    "end": "1124955"
  },
  {
    "text": " Cool.",
    "start": "1124955",
    "end": "1130400"
  },
  {
    "text": "So kind of the common wisdom\nwith these pretrained models is generally trying to\npretrain on diverse data sets.",
    "start": "1130400",
    "end": "1138127"
  },
  {
    "text": "There's also some unsupervised\nlearning techniques that you can use\nfor pretraining, and we'll cover those in some\nof the upcoming lectures.",
    "start": "1138127",
    "end": "1145400"
  },
  {
    "text": "Yeah. [INAUDIBLE] considered that\nthe task, the new task, is related to the old\ntask or the new data set",
    "start": "1145400",
    "end": "1152970"
  },
  {
    "text": "is related to the\nlarge repository? I mean, is it, is the relation\nbased on the type of the data",
    "start": "1152970",
    "end": "1159570"
  },
  {
    "text": "or the type of the task? Yeah. So the question was-- like, generally we'll-- this\nmay be part of the answer, too--",
    "start": "1159570",
    "end": "1166630"
  },
  {
    "text": "but generally we want the source\ntask and target task to be similar, to be related\nto one another.",
    "start": "1166630",
    "end": "1172356"
  },
  {
    "text": "And you were asking\nif they need to be related in terms of what task\nit is versus what the data is. And in reality, it's both-- it's\nboth what the data looks like",
    "start": "1172357",
    "end": "1179759"
  },
  {
    "text": "and what you're trying\nto do with that data-- that will affect how\nsuccessful transfer is.",
    "start": "1179760",
    "end": "1186485"
  },
  {
    "text": "I'm going to move\non a little bit to at least get to the\nrest of this slide. So, but all these\nare great questions.",
    "start": "1186485",
    "end": "1193122"
  },
  {
    "text": "Now one of the things\nthat was mentioned before is that one\nof the things that's really awesome about\ntransfer learning",
    "start": "1193123",
    "end": "1199320"
  },
  {
    "text": "is it's not just that\nyou can do better by leveraging this\nprior knowledge, but that oftentimes\nsomeone has already",
    "start": "1199320",
    "end": "1205620"
  },
  {
    "text": "distilled that prior\nknowledge into a model and put that model\non the internet so that you don't even\nhave to train on ImageNet",
    "start": "1205620",
    "end": "1213090"
  },
  {
    "text": "or train on some\nlanguage corpora. And this actually\nsignificantly improves, I think, the accessibility\nof things like deep learning",
    "start": "1213090",
    "end": "1220590"
  },
  {
    "text": "because it means that\nanyone can download a model and use it on their\nproblem, on their data set,",
    "start": "1220590",
    "end": "1226560"
  },
  {
    "text": "as long as it's at least\nsomewhat sufficiently similar to these common\npretrained models. ",
    "start": "1226560",
    "end": "1234610"
  },
  {
    "text": "Cool. Now, in this example,\nwe-- this is kind of the-- I think one of the most\ncommon versions of fine",
    "start": "1234610",
    "end": "1242350"
  },
  {
    "text": "tuning where you fine\ntune the entire network. But in practice\nthere's actually a lot of different design\nchoices that come up when",
    "start": "1242350",
    "end": "1249520"
  },
  {
    "text": "thinking about fine tuning. And a lot of these\ndesign choices kind of revolve around\nthinking about how",
    "start": "1249520",
    "end": "1257540"
  },
  {
    "text": "to not destroy the prior\nknowledge and prior information in your model and balance\nthat prior knowledge",
    "start": "1257540",
    "end": "1263780"
  },
  {
    "text": "with the knowledge\nfrom your new data set. And in particular, if you\nhave a neural network that",
    "start": "1263780",
    "end": "1273620"
  },
  {
    "text": "has some layers-- say, for example, maybe you're\ntrying to pretrain on ImageNet",
    "start": "1273620",
    "end": "1280430"
  },
  {
    "text": "and now you're trying\nto fine tune on a task like classifying\nbetween cats and dogs--",
    "start": "1280430",
    "end": "1285658"
  },
  {
    "text": "or maybe-- I think cats and dogs\nare probably in the ImageNet data set, so we can\nmaybe pick something more obscure,\nlike, I don't know,",
    "start": "1285658",
    "end": "1291950"
  },
  {
    "text": "classifying between\nwhiteboard markers and whiteboard erasers\nor something like that.",
    "start": "1291950",
    "end": "1298320"
  },
  {
    "text": "In that case, you have a\nnetwork pretrained on ImageNet. And the output image\nhas 1,000 classes",
    "start": "1298320",
    "end": "1305929"
  },
  {
    "text": "and so the output layer is\ngoing to be 1,000 dimensional. And your target task\nhas two possible labels,",
    "start": "1305930",
    "end": "1312860"
  },
  {
    "text": "one for whiteboard markers and\none for whiteboard erasers. And so what you need\nto do, in that case,",
    "start": "1312860",
    "end": "1318049"
  },
  {
    "text": "is to essentially kind of\nreinitialize the last layer, because you can't-- or at\nleast use some part of this.",
    "start": "1318050",
    "end": "1326307"
  },
  {
    "text": "You can't just directly\nuse this last thing. You could either use\ntwo of the classes and then throw\naway the last half,",
    "start": "1326307",
    "end": "1332220"
  },
  {
    "text": "or just reinitialize\na new network on top of these features right here. Now, one thing that\ncomes up is let's",
    "start": "1332220",
    "end": "1337980"
  },
  {
    "text": "say you reinitialize to instead\nhave a last layer of two,",
    "start": "1337980",
    "end": "1343770"
  },
  {
    "text": "a much smaller last layer. And these are now randomly\ninitialized weights",
    "start": "1343770",
    "end": "1349290"
  },
  {
    "text": "whereas everything\nbefore this is weights pretrained on ImageNet. Now in that case, if you\nback propagate your gradients",
    "start": "1349290",
    "end": "1355860"
  },
  {
    "text": "through the network, if this is\na randomly initialized weight matrix, then you're\nessentially going to be multiplying your\ngradients by some random numbers",
    "start": "1355860",
    "end": "1362880"
  },
  {
    "text": "right here and then\napplying those gradients to the rest of the network. And so then you're going to be\nhitting these weight matrices",
    "start": "1362880",
    "end": "1370080"
  },
  {
    "text": "with numbers that have a\nlot more randomness in them, and that might actually destroy\na lot of the really great information in these layers.",
    "start": "1370080",
    "end": "1377530"
  },
  {
    "text": "And so one common practice-- or there's a few\ndifferent common practices that people do.",
    "start": "1377530",
    "end": "1383828"
  },
  {
    "text": "One is to fine tune with\na smaller learning rate, especially for the earlier\nlayers of the network, because you don't\nwant these features",
    "start": "1383828",
    "end": "1389760"
  },
  {
    "text": "to be destroyed by the gradients\ncoming in from the network. ",
    "start": "1389760",
    "end": "1396000"
  },
  {
    "text": "Smaller learning rate\nfor earlier layers. Sometimes also you might\nfreeze these earlier layers",
    "start": "1396000",
    "end": "1401250"
  },
  {
    "text": "and only train the latter parts\nof the network or only train-- start by only training the\ngradual part, the last parts,",
    "start": "1401250",
    "end": "1407820"
  },
  {
    "text": "of the network and\ngradually unfreeze from the back of the network. ",
    "start": "1407820",
    "end": "1413790"
  },
  {
    "text": "Reinitialize the last layer,\nwhich we talked about. So I guess these four things\nare fairly common design",
    "start": "1413790",
    "end": "1420630"
  },
  {
    "text": "choices when fine tuning. In terms of how to pick\nbetween those design choices, you can kind of search over\nthose design choices and hyper",
    "start": "1420630",
    "end": "1427980"
  },
  {
    "text": "parameters by running cross\nvalidation on your target task.",
    "start": "1427980",
    "end": "1435110"
  },
  {
    "text": "And I guess the last thing\nworth mentioning here is that the architecture\nthat you choose to fine tune will also affect the\ntransfer performance.",
    "start": "1435110",
    "end": "1442920"
  },
  {
    "text": "And so, in particular,\nresidual networks which have residual\nconnections between layers,",
    "start": "1442920",
    "end": "1450350"
  },
  {
    "text": "these networks are often more\neffective for fine tuning because you get kind\nof a bit of a highway",
    "start": "1450350",
    "end": "1457100"
  },
  {
    "text": "throughout the network. And so it's easier to get\ngradients to all of the layers",
    "start": "1457100",
    "end": "1462410"
  },
  {
    "text": "rather than having to use the-- well-- rather than having to\nkind of go one by one by layer.",
    "start": "1462410",
    "end": "1468410"
  },
  {
    "text": " OK.",
    "start": "1468410",
    "end": "1474600"
  },
  {
    "text": "So now that we've gone over\nthe basics of fine tuning, are there any more questions?",
    "start": "1474600",
    "end": "1480240"
  },
  {
    "text": "Yeah. Would these design\nchoices depend on the complexity of that\nsecond transfer learning task?",
    "start": "1480240",
    "end": "1489330"
  },
  {
    "text": "Let's say just, you\nknow, cats versus dogs. Maybe all you would need\nto do is a simpler change",
    "start": "1489330",
    "end": "1496020"
  },
  {
    "text": "for that last layer, right? But if it's a really\ncomplicated [INAUDIBLE]..",
    "start": "1496020",
    "end": "1502372"
  },
  {
    "text": "Yeah, absolutely. So the question was, like,\nif the target task is simple",
    "start": "1502372",
    "end": "1507480"
  },
  {
    "text": "versus complex, does that\naffect how you choose to fine tune the network?",
    "start": "1507480",
    "end": "1513000"
  },
  {
    "text": "And, yeah. If you have a\nreally simple task, then you may just only need\nto train this last layer. You may not even need to change\nthe features at all, especially",
    "start": "1513000",
    "end": "1520050"
  },
  {
    "text": "if that task is very\nrelated to the source task. Whereas if it's very\ncomplex or very different,",
    "start": "1520050",
    "end": "1525345"
  },
  {
    "text": "you may want to actually\nkind of reinitialize more than one layer on top of\nthe features or kind of fine",
    "start": "1525345",
    "end": "1530760"
  },
  {
    "text": "tune the entire network\nor something like that. Yeah. If you don't have\nmuch [INAUDIBLE],,",
    "start": "1530760",
    "end": "1537760"
  },
  {
    "text": "how do you prevent\noverfitting on that data? Like, is there a principled\nway of doing [INAUDIBLE]??",
    "start": "1537760",
    "end": "1543470"
  },
  {
    "text": "Yeah. So if we have a\nsmall amount of data, how do we prevent overfitting? The most common thing\nto do is early stopping,",
    "start": "1543470",
    "end": "1549910"
  },
  {
    "text": "which is also done in standard\nmachine learning, which is instead of fine tuning\nthis for a very long time,",
    "start": "1549910",
    "end": "1555490"
  },
  {
    "text": "fine tune it for\nfewer gradient steps, watch your validation\nloss on your target task, and stop fine tuning once\nyou reach a good solution.",
    "start": "1555490",
    "end": "1564159"
  },
  {
    "text": "You can also fine tune\nfewer layers, as well. So if you fine tune\nonly the last layer, then you'll probably\noverfit less than",
    "start": "1564160",
    "end": "1569470"
  },
  {
    "text": "if you fine tune\nthe whole network.  Yeah. Just out of curiosity, if you\ntook, like, a pretrained model,",
    "start": "1569470",
    "end": "1576640"
  },
  {
    "text": "like, ImageNet for example,\nand would just fine tune it on more exotic sort\nof, more, like, more",
    "start": "1576640",
    "end": "1582130"
  },
  {
    "text": "specific instantiations of\ncertain objects or categories, would the network\nstill sort of show",
    "start": "1582130",
    "end": "1590500"
  },
  {
    "text": "an understanding of, like,\nsemantic distinctions between different patterns? You know, maybe two specific\ninstantiations of cat",
    "start": "1590500",
    "end": "1598659"
  },
  {
    "text": "would be closer to each other by\nthe space than, like, you know, a specific kind of dog?",
    "start": "1598660",
    "end": "1605130"
  },
  {
    "text": "Yeah. So the question is\nif you fine tune on a more fine grained\nclassification task, like maybe classifying between\ndifferent species of birds",
    "start": "1605130",
    "end": "1611650"
  },
  {
    "text": "or something like that, then\nwould the features still-- like, would you get a good\nfeature space for that task",
    "start": "1611650",
    "end": "1618940"
  },
  {
    "text": "such that similar species of\nbirds are grouped together? Is that what you're asking? Yeah. Yeah. So if you fine tune only the\nlast layer of the network,",
    "start": "1618940",
    "end": "1626590"
  },
  {
    "text": "that won't affect the\nfeatures in any way. And so you'll just get the\noriginal ImageNet features. But if you do fine tune\nthe entire network,",
    "start": "1626590",
    "end": "1632559"
  },
  {
    "text": "then that should also give you\nbetter features for that target task. Of course, if you have a\nreally small target task,",
    "start": "1632560",
    "end": "1639080"
  },
  {
    "text": "then the features-- you may not benefit-- well,\nyou may overfit to that,",
    "start": "1639080",
    "end": "1644870"
  },
  {
    "text": "and so you may not get\ngood features of that or it may just be\nbeneficial not to fine tune for very long because you\nhave a small data set.",
    "start": "1644870",
    "end": "1650530"
  },
  {
    "text": "So it will depend a bit\non your data set size. But if you fine tune\nthe whole thing, it will adjust the\nfeatures and it",
    "start": "1650530",
    "end": "1656259"
  },
  {
    "text": "should learn a good space for\nthat, a good set of features. ",
    "start": "1656260",
    "end": "1664000"
  },
  {
    "text": "Yeah. [INAUDIBLE] What is the\nnotation for the arrow--",
    "start": "1664000",
    "end": "1673510"
  },
  {
    "text": "is it a new loss function\nfor the new task? Yeah. So the arrow on the\nleft, it is kind",
    "start": "1673510",
    "end": "1679060"
  },
  {
    "text": "of an assignment operation. So we're saying\nthat we're defining theta-- we're defining phi to\nbe the right hand side of that.",
    "start": "1679060",
    "end": "1686660"
  },
  {
    "text": "You can sort of think\nof it as an equal sign, although it's more directional. Okay, so that means\nthat we're literally",
    "start": "1686660",
    "end": "1692491"
  },
  {
    "text": "moving the parameter from\nthe mode of the first task",
    "start": "1692491",
    "end": "1698240"
  },
  {
    "text": "to the mode of second task by\nusing the new loss function on the new training data. Yeah.",
    "start": "1698240",
    "end": "1703445"
  },
  {
    "text": "And do the same, [INAUDIBLE]\nof the second task.",
    "start": "1703445",
    "end": "1710304"
  },
  {
    "text": "Yeah. OK, OK. Then in the case,\nin these, two models",
    "start": "1710305",
    "end": "1716059"
  },
  {
    "text": "should have the\nsame architecture. Yeah. Yeah. So when you fine tune,\nyou're going to be--",
    "start": "1716060",
    "end": "1722060"
  },
  {
    "text": "if you fine tune\nthe whole network-- well, in general, you should\nbe using the same architecture. You can reinitialize\nthe last layer",
    "start": "1722060",
    "end": "1727640"
  },
  {
    "text": "or reinitialize\nparts of the network and change that part\nof the architecture. But, in general,\nespecially the first layers",
    "start": "1727640",
    "end": "1733550"
  },
  {
    "text": "will need to have the\nsame architecture. OK. And in the second half,\nwe don't put more layers,",
    "start": "1733550",
    "end": "1739100"
  },
  {
    "text": "then how we initialize\nthe parameter [INAUDIBLE]",
    "start": "1739100",
    "end": "1746669"
  },
  {
    "text": "Yeah. So if you have more\nlayers, then you'll need to randomly initialize,\nthough there isn't a great way to initialize them.",
    "start": "1746670",
    "end": "1753150"
  },
  {
    "text": "Yeah. Can we also use transfer\nlearning for multi tasks just by separating\nthe last few layers?",
    "start": "1753150",
    "end": "1760870"
  },
  {
    "text": "Yeah. So you could also transfer to\nmultiple different downstream tasks.",
    "start": "1760870",
    "end": "1766170"
  },
  {
    "text": "So you could have multiple\nheads, for example. Or you could pass in the Z here.",
    "start": "1766170",
    "end": "1771690"
  },
  {
    "text": "One thing that's a\nlittle bit tricky is if you want to pass in\na Z earlier in the network or pass in some other input\nearlier into the network that",
    "start": "1771690",
    "end": "1778230"
  },
  {
    "text": "wasn't there during\npretraining, then you need to make sure\nthat you kind of actually update this layer\nduring fine tuning.",
    "start": "1778230",
    "end": "1785500"
  },
  {
    "text": "But in general, something like\nthat is definitely possible. Cool. Was there one more\nquestion in the back?",
    "start": "1785500",
    "end": "1791267"
  },
  {
    "text": "Yeah. So if you have the\nsource [INAUDIBLE]",
    "start": "1791267",
    "end": "1802320"
  },
  {
    "text": "for the pretrain\nmodel, are there any methods to, like, we\ncould do multi task learning",
    "start": "1802320",
    "end": "1807600"
  },
  {
    "text": "for this particular\nfine-tuning [INAUDIBLE]",
    "start": "1807600",
    "end": "1813210"
  },
  {
    "text": "the task we're training for? So you're asking if we have\nthe source data available, is that helpful during\nthe fine tuning process?",
    "start": "1813210",
    "end": "1821889"
  },
  {
    "text": "So in general-- so you could\njust do multi task learning and train on both of the tasks\nand maybe up-weight the target",
    "start": "1821890",
    "end": "1829290"
  },
  {
    "text": "task if you care\nmore about that. That can be helpful if you don't\nwant to forget the source task.",
    "start": "1829290",
    "end": "1836070"
  },
  {
    "text": "It can be somewhat helpful\nto regularize when trying to solve the target task. Although, in practice,\nit depends a little bit",
    "start": "1836070",
    "end": "1844049"
  },
  {
    "text": "on the scenario, and in practice\nactually just oftentimes fine tuning is actually often\nbetter than keeping it around.",
    "start": "1844050",
    "end": "1849965"
  },
  {
    "text": "The other thing\nthat you could do is you can regularize towards\nthe initial parameters and that you can do actually\nwithout the source data, which",
    "start": "1849965",
    "end": "1856500"
  },
  {
    "text": "is nice, and there are some\nworks that have found benefits for that in some scenarios. ",
    "start": "1856500",
    "end": "1864290"
  },
  {
    "text": "Yeah. So if you're working in some\nsort of few-shot regime, is there--",
    "start": "1864290",
    "end": "1869929"
  },
  {
    "text": "has there ever\nbeen, has there been shown, like, any way to sort\nof do a full fine tuning that",
    "start": "1869930",
    "end": "1875270"
  },
  {
    "text": "isn't, just, like explicitly\nworse than your evaluation? ",
    "start": "1875270",
    "end": "1882500"
  },
  {
    "text": "So you're asking in\nthe setting where you have a very small\namount of target data, is it ever better to fine tune\nthe entire network rather than",
    "start": "1882500",
    "end": "1889610"
  },
  {
    "text": "just a small thing on top? Like, just the final layer. Yeah. All-- in two slides, I'll get\nto something that I think will",
    "start": "1889610",
    "end": "1897740"
  },
  {
    "text": "somewhat-- it won't fully\naddress that, but it will show some scenarios\nthat are better than just",
    "start": "1897740",
    "end": "1903950"
  },
  {
    "text": "fine tuning the last layer. Yeah.",
    "start": "1903950",
    "end": "1909530"
  },
  {
    "text": "OK. So in general, all the\nthings on this slide are--",
    "start": "1909530",
    "end": "1915450"
  },
  {
    "text": "I would-- or all the\nthings on the bottom, I would refer to as\nkind of common wisdom around fine tuning.",
    "start": "1915450",
    "end": "1921400"
  },
  {
    "text": "And it's also worth mentioning\nthat this common wisdom, I think, is changing and\nit's not fully set in stone.",
    "start": "1921400",
    "end": "1930490"
  },
  {
    "text": "And so in particular,\non these next two slides I want to cover two\npapers that maybe you",
    "start": "1930490",
    "end": "1938169"
  },
  {
    "text": "might consider as old by\nmachine learning standpoint, by machine learning terms,\nwhich is that they both came out",
    "start": "1938170",
    "end": "1943690"
  },
  {
    "text": "last week. And the first paper is a\npaper by some folks at CMU,",
    "start": "1943690",
    "end": "1952480"
  },
  {
    "text": "I believe, and they\nfound that actually that unsupervised-- or\ngenerally pretraining-- doesn't",
    "start": "1952480",
    "end": "1959799"
  },
  {
    "text": "necessarily need a\nreally diverse data set, specifically unsupervised\npretraining methods.",
    "start": "1959800",
    "end": "1965440"
  },
  {
    "text": "And so they found\nthat if you randomly initialize the network, you get\n72% success on the target task.",
    "start": "1965440",
    "end": "1971920"
  },
  {
    "text": "If you pretrain on a book\ncorpus that's much larger, you get 81%.",
    "start": "1971920",
    "end": "1977020"
  },
  {
    "text": "So we do see a benefit\nfrom pretraining here. But they also found\nthat if you pretrain",
    "start": "1977020",
    "end": "1982660"
  },
  {
    "text": "with this unsupervised objective\non the fine tuning data set,",
    "start": "1982660",
    "end": "1988570"
  },
  {
    "text": "you actually get 80.96% success\nor average accuracy, which",
    "start": "1988570",
    "end": "1994210"
  },
  {
    "text": "is actually very\nclose to the 81.3%. And this, I think,\nbreaks the common wisdom",
    "start": "1994210",
    "end": "1999823"
  },
  {
    "text": "because the common wisdom\nis that typically we need a very diverse\npretraining data set, and this suggests\nthat when you have",
    "start": "1999823",
    "end": "2005610"
  },
  {
    "text": "an unsupervised\npretraining objective, you may not actually need a\nreally diverse pretraining objective-- or\npretraining data set.",
    "start": "2005610",
    "end": "2011820"
  },
  {
    "text": "You can possibly even just\npretrain on the target data set itself.",
    "start": "2011820",
    "end": "2019000"
  },
  {
    "text": "So this is kind of breaking\nthe common knowledge, or the common wisdom, here. If you have a supervised\npretraining task,",
    "start": "2019000",
    "end": "2026429"
  },
  {
    "text": "then this won't work\nout, because you-- like running supervised learning\non your fine tuning data set and then running fine tuning\non your fine tuning data set,",
    "start": "2026430",
    "end": "2034770"
  },
  {
    "text": "those are going to be\nthe same exact thing. So this is-- I would\nexpect to only hold in the unsupervised\npretraining case.",
    "start": "2034770",
    "end": "2042000"
  },
  {
    "text": "But it's something that\nbreaks the common wisdom and suggests that we don't\nhave everything figured out, even when it comes\nto fine tuning.",
    "start": "2042000",
    "end": "2049020"
  },
  {
    "text": "Yeah. [INAUDIBLE] ",
    "start": "2049020",
    "end": "2057739"
  },
  {
    "text": "This is only in stages\nversus can you-- with this-- you're\nasking if this would also hold in the multi task setting?",
    "start": "2057739",
    "end": "2063552"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2063552",
    "end": "2069669"
  },
  {
    "text": "I think that they\nonly ran experiments in the pretraining\nand fine tuning phase. But you could read\nthe paper to check.",
    "start": "2069670",
    "end": "2078219"
  },
  {
    "text": "And because it\ncame out last week, I don't think anyone\nhas built on it yet. I should mention\nthat this is averaged",
    "start": "2078219",
    "end": "2083919"
  },
  {
    "text": "over a number of different-- they ran this on a number\nof different target tasks.",
    "start": "2083920",
    "end": "2089408"
  },
  {
    "text": "This was all in the NLP domain. But there's actually\nanother paper that came out\nactually a little bit",
    "start": "2089409",
    "end": "2094468"
  },
  {
    "text": "before this that actually showed\na similar result in computer vision tasks, as well. ",
    "start": "2094468",
    "end": "2101820"
  },
  {
    "text": "Now the second paper is actually\na paper that was co-authored by Yoonho and others--",
    "start": "2101820",
    "end": "2107420"
  },
  {
    "text": "Yoonho is a TA in the class. He looks like this. And I want to give a little\nbit of the kind of the thought",
    "start": "2107420",
    "end": "2114090"
  },
  {
    "text": "process behind the\nresearch, because I said I would try to mention\nthat a little bit in the course. And the thought process\nwas that there's",
    "start": "2114090",
    "end": "2120390"
  },
  {
    "text": "a lot of scenarios where fine\ntuning only the last layer works really well. So that's great.",
    "start": "2120390",
    "end": "2126150"
  },
  {
    "text": "But is there anything\nactually like that special about the last layer? You can maybe think that\nit's somewhat special",
    "start": "2126150",
    "end": "2132420"
  },
  {
    "text": "and that it's kind of closest\nto the labels, in some sense. But in many ways, it's also\njust like any other layer",
    "start": "2132420",
    "end": "2138600"
  },
  {
    "text": "in the neural network. And so his thought process\nwas, well, maybe for, kind of--",
    "start": "2138600",
    "end": "2145050"
  },
  {
    "text": "if we're trying to fine tune\nto a pretty low level shift, maybe there are scenarios\nwhere the last layer wouldn't",
    "start": "2145050",
    "end": "2150330"
  },
  {
    "text": "work better but actually maybe\nother layers in the network, like the first layer,\nmight actually work better.",
    "start": "2150330",
    "end": "2155550"
  },
  {
    "text": "And he actually\nalready had something, some experiments setup\nto run fine tuning on image corruptions.",
    "start": "2155550",
    "end": "2161460"
  },
  {
    "text": "And so he pretrained on\nthe CIFAR 10 data set, which is an image\nclassification data set,",
    "start": "2161460",
    "end": "2167400"
  },
  {
    "text": "fine tuned it on\nCIFAR 10 C, which is a corrupted version of\nthat data set where there are",
    "start": "2167400",
    "end": "2173730"
  },
  {
    "text": "small, low level image\ncorruptions applied to the data, and he found\nthat if you fine tune the whole network, you\nget around 79.9% accuracy,",
    "start": "2173730",
    "end": "2182130"
  },
  {
    "text": "and if you fine tune only the\nfirst layer of the network, you get a higher accuracy\non the target task.",
    "start": "2182130",
    "end": "2189498"
  },
  {
    "text": "And then from there, the\nthought process was, well, OK, if there are some scenarios\nwhere the first layer works, maybe there are scenarios where\nsome, like, only fine tuning",
    "start": "2189498",
    "end": "2196000"
  },
  {
    "text": "the middle layer works. And indeed, there\nare some scenarios where actually fine\ntuning a middle layer",
    "start": "2196000",
    "end": "2201508"
  },
  {
    "text": "is actually better than fine\ntuning the whole network. And so this is another example\nof breaking the common wisdom,",
    "start": "2201508",
    "end": "2207510"
  },
  {
    "text": "because the common\nwisdom is typically to fine tune from the\nback of the network. But there are actually\nscenarios where",
    "start": "2207510",
    "end": "2212693"
  },
  {
    "text": "that's not the best option. Of course, these\ndifferences are still, they're kind of 2% to 4%.",
    "start": "2212693",
    "end": "2218790"
  },
  {
    "text": "So full fine tuning still does\nwork pretty well in practice. But it says that we don't\nhave a full understanding",
    "start": "2218790",
    "end": "2225240"
  },
  {
    "text": "of fine tuning. Yeah. I was just wondering\nhow these layers, like, have different\nnumbers of parameters,",
    "start": "2225240",
    "end": "2231170"
  },
  {
    "text": "how that would be integrated\nin this sort of analysis? Or how do you, you\nknow, correct for that?",
    "start": "2231170",
    "end": "2236550"
  },
  {
    "text": "In preparing the events-- Yeah. So the question is\nthat different layers have different\nnumbers of parameters,",
    "start": "2236550",
    "end": "2242579"
  },
  {
    "text": "and so it may be\nthat, like, maybe that is accounting for some\nof the differences here. Maybe that has, like,\na regularizing effect",
    "start": "2242580",
    "end": "2248692"
  },
  {
    "text": "or something like that. In general, I cannot remember\noff the top of my head",
    "start": "2248692",
    "end": "2254220"
  },
  {
    "text": "the numbers of parameters\nfor each of these blocks. But the, they did run\nexperiments fairly extensively",
    "start": "2254220",
    "end": "2262560"
  },
  {
    "text": "on this. They weren't\nexplicitly controlling for the number of parameters,\nbut the results, I do think,",
    "start": "2262560",
    "end": "2268440"
  },
  {
    "text": "suggested that it had more to\ndo with where in the network you were fine tuning compared\nto the number of parameters.",
    "start": "2268440",
    "end": "2275220"
  },
  {
    "text": "And I should also mention\nthat for full fine tuning, the learning rate and\nthe early stopping",
    "start": "2275220",
    "end": "2282720"
  },
  {
    "text": "were both determined\nwith cross validation. And so this is a pretty\nwell tuned baseline.",
    "start": "2282720",
    "end": "2289250"
  },
  {
    "text": "Yeah. So for some tasks\nlike the ImageNet",
    "start": "2289250",
    "end": "2294740"
  },
  {
    "text": "data set where we start off\nwith a color image as the input",
    "start": "2294740",
    "end": "2299900"
  },
  {
    "text": "and we now want a task where\nwe just have black and white. It would make sense\nto just retrain",
    "start": "2299900",
    "end": "2305510"
  },
  {
    "text": "the first layer because\nit's like a change in, like, the inputs. Would we still want to do,\nlike, the fundamentally similar",
    "start": "2305510",
    "end": "2312110"
  },
  {
    "text": "processing? Yeah. Is there, like, any\ngood intuition for when we would choose, like, a middle\nlayer and which middle layer",
    "start": "2312110",
    "end": "2319090"
  },
  {
    "text": "to choose? Yeah. So the question is-- yeah. Like, what's some\nof the intuition here, especially with\nregard to the middle layers.",
    "start": "2319090",
    "end": "2326300"
  },
  {
    "text": "And so, and some of Yoonho's\nintuition for trying this, also, was that you\ncould sort of think of--",
    "start": "2326300",
    "end": "2333620"
  },
  {
    "text": "so there's sort of kind of\na causal process underlying the process of going from a\nlabel to generating an image,",
    "start": "2333620",
    "end": "2339740"
  },
  {
    "text": "and you can sort of\nthink of neural networks as trying to reverse that\ncausal process, where,",
    "start": "2339740",
    "end": "2345050"
  },
  {
    "text": "like, when you are, for\nexample, trying to generate-- I think, like, this one, you're\ntrying to predict vegetables,",
    "start": "2345050",
    "end": "2351470"
  },
  {
    "text": "for example, you may first go\nfrom whether it's a vegetable or not to something that's,\nlike, what type of vegetable",
    "start": "2351470",
    "end": "2358730"
  },
  {
    "text": "is it to, like, what is the\nposition of those vegetables and the appearance of that\nvegetable and then down to, like, what do pixels look\nlike for that kind of thing.",
    "start": "2358730",
    "end": "2366690"
  },
  {
    "text": "And when you change some\npart of that causal process-- if you think of that\nas the entire chain, it could be that maybe\nif you change something",
    "start": "2366690",
    "end": "2373640"
  },
  {
    "text": "in the middle of\nthat chain, then fine tuning the middle\nparts of the network might be the best choice\ninsofar as neural networks",
    "start": "2373640",
    "end": "2380000"
  },
  {
    "text": "might be kind of reversing\nthe causal process of image generation.",
    "start": "2380000",
    "end": "2385410"
  },
  {
    "text": "So that was some of\nour intuition there, but it's also something that\nisn't, like, still fully",
    "start": "2385410",
    "end": "2392690"
  },
  {
    "text": "sorted out. And I think that one\nof the things that makes this result\ninteresting is that it is different for\ndifferent kinds of shifts",
    "start": "2392690",
    "end": "2399332"
  },
  {
    "text": "between source and target.  Yeah. [INAUDIBLE] works\nbetter than full time.",
    "start": "2399332",
    "end": "2412520"
  },
  {
    "text": "Would it also compare that it\nworks better than just finding the last layer, as well? Yeah. Yeah.",
    "start": "2412520",
    "end": "2417790"
  },
  {
    "text": "So in these cases, it was also-- well, in this case,\nlast layer was best. But in the first\ntwo cases, these",
    "start": "2417790",
    "end": "2423589"
  },
  {
    "text": "were also better than fine\ntuning just the last layer. Yeah. So [INAUDIBLE] different types\nof corruptions or does it--",
    "start": "2423590",
    "end": "2437460"
  },
  {
    "text": "even in degrees for some\nspecific corruption because-- Yeah. [INAUDIBLE]",
    "start": "2437460",
    "end": "2444839"
  },
  {
    "text": "Yeah. So the question\nis it helpful-- is the first layer good for all\ndifferent kinds of corruptions.",
    "start": "2444840",
    "end": "2450600"
  },
  {
    "text": "I believe it was better for\nalmost all of the corruptions",
    "start": "2450600",
    "end": "2458170"
  },
  {
    "text": "that we tried. I don't think-- I'm not sure\nif we tried all 30 of them. I think there's a lot of them.",
    "start": "2458170",
    "end": "2463542"
  },
  {
    "text": "But I think that for, at least\nfor most of the ones that we tried, it was the best. Yeah. ",
    "start": "2463542",
    "end": "2470870"
  },
  {
    "text": "Cool. OK. So now that we know that\nthe common wisdom is-- well the common wisdom is OK,\nbut it's still something",
    "start": "2470870",
    "end": "2478430"
  },
  {
    "text": "that's kind of being developed. I do want to give you\nsomewhat of a default, because if you do\nwant to actually use",
    "start": "2478430",
    "end": "2485480"
  },
  {
    "text": "fine tuning in\npractice, you don't want to have to navigate this\nentire space of design choices.",
    "start": "2485480",
    "end": "2490910"
  },
  {
    "text": "And so despite some of the\nresults that we've seen, I do think that if\nyou want something",
    "start": "2490910",
    "end": "2496850"
  },
  {
    "text": "that is pretty reliable, I think\nsomething like first training the last layer of the\nnetwork and then training the whole thing is generally\na really good place to start",
    "start": "2496850",
    "end": "2506250"
  },
  {
    "text": "and generally works\nwell in practice. And the reason for that is\nexactly what we talked about before, where you don't want--\nyou want to avoid destroying",
    "start": "2506250",
    "end": "2514232"
  },
  {
    "text": "your early features\nand so you want to train this last layer, the\nlast set of layers, first. And then fine tuning\nthe whole thing usually",
    "start": "2514232",
    "end": "2521000"
  },
  {
    "text": "is helpful in practice. Yeah. How do you know when to\nstop for your last layer",
    "start": "2521000",
    "end": "2527210"
  },
  {
    "text": "and then go into [INAUDIBLE]? Yeah. So in terms of when to stop\ntraining the last layer,",
    "start": "2527210",
    "end": "2532323"
  },
  {
    "text": "typically when you\ntrain the last layer, you typically don't\noverfit because it's a pretty small\nnumber of parameters.",
    "start": "2532323",
    "end": "2539900"
  },
  {
    "text": "And so you can mostly just\nlook at until you converge and then once you roughly\nconverge, once you see the loss",
    "start": "2539900",
    "end": "2545720"
  },
  {
    "text": "function not going\ndown anymore, you can start fine tuning\nthe whole layer. You can also look at when the\nvalidation loss starts to even,",
    "start": "2545720",
    "end": "2554390"
  },
  {
    "text": "start to level out, as well. ",
    "start": "2554390",
    "end": "2559480"
  },
  {
    "text": "Cool. Yeah. [INAUDIBLE] applicable\nalso to online learning?",
    "start": "2559480",
    "end": "2566520"
  },
  {
    "text": "Will this be applicable\nto online learning?  Yeah.",
    "start": "2566520",
    "end": "2571750"
  },
  {
    "text": "I mean you could certainly kind\nof do this repeatedly, like reinitialize the last layer\nand fine tune that before fine",
    "start": "2571750",
    "end": "2577450"
  },
  {
    "text": "tuning the whole thing and then\nkind of do that repeatedly. Yeah. Although, if are seeing\nkind of a gradual shift",
    "start": "2577450",
    "end": "2584523"
  },
  {
    "text": "in an online\nlearning setting, you may just want to keep on fine\ntuning the whole network. ",
    "start": "2584523",
    "end": "2591619"
  },
  {
    "text": "Cool. And then the last thing\nthat we'll talk about with fine tuning is looking at\nwhat fine tuning performance",
    "start": "2591620",
    "end": "2598070"
  },
  {
    "text": "looks like when you have varying\namounts of target task data. And this is just one example.",
    "start": "2598070",
    "end": "2603540"
  },
  {
    "text": "And so in particular,\nwhat this is looking at is the x-axis is the\nnumber of training examples in your target\ndata set and the y-axis",
    "start": "2603540",
    "end": "2611329"
  },
  {
    "text": "is the error, the\nvalidation error rate, on that target task. And so as we see,\nas we have more",
    "start": "2611330",
    "end": "2617780"
  },
  {
    "text": "target task data, the better\nwe do, as we would expect. The blue line is\ntraining from scratch.",
    "start": "2617780",
    "end": "2624560"
  },
  {
    "text": "The green and orange line are\ndifferent pretrained models. And one thing we note is that\nif we have only 100 target task",
    "start": "2624560",
    "end": "2634160"
  },
  {
    "text": "data points, the performance\nisn't nearly as good as if we had a bit more than that.",
    "start": "2634160",
    "end": "2641540"
  },
  {
    "text": "And in general, this is\nstill doing pretty good. Like, in this\nexample, we only have, like-- if we only\nhave 1,000 examples,",
    "start": "2641540",
    "end": "2647710"
  },
  {
    "text": "we still do really, really well. But it does start\nto get much worse if we have only 100 examples.",
    "start": "2647710",
    "end": "2654280"
  },
  {
    "text": "And this is where things like\nmeta learning can be useful. ",
    "start": "2654280",
    "end": "2660520"
  },
  {
    "text": "Cool. So now let's, with that\nin mind, let's transition to talking about meta learning. So how do we get from transfer\nlearning to meta learning?",
    "start": "2660520",
    "end": "2668982"
  },
  {
    "text": "In transfer learning,\nwe talked about how we'll initialize\nthe model and hope that initializing from there\nhelps on the target task.",
    "start": "2668982",
    "end": "2678010"
  },
  {
    "text": "The kind of key intuition\nbehind meta learning is instead of hoping\nthat it will help, what if we explicitly\noptimize for transferability.",
    "start": "2678010",
    "end": "2689330"
  },
  {
    "text": "And what I mean by that\nis if we have not just one source task but a\nset of source tasks,",
    "start": "2689330",
    "end": "2696690"
  },
  {
    "text": "can we optimize for\nthe ability to quickly learn these tasks such that we\ncan learn new tasks quickly,",
    "start": "2696690",
    "end": "2704380"
  },
  {
    "text": "as well. So if we learn how to quickly\nlearn a set of tasks already,",
    "start": "2704380",
    "end": "2709690"
  },
  {
    "text": "that means that we should\nbe able to also learn new tasks quickly. And so you can also\nthink of this as--",
    "start": "2709690",
    "end": "2717295"
  },
  {
    "text": "if we think about learning\nas going from a data set to a set of\nparameters, and we want",
    "start": "2717295",
    "end": "2722420"
  },
  {
    "text": "to be able to learn\nquickly, then we can think about\nessentially trying",
    "start": "2722420",
    "end": "2727490"
  },
  {
    "text": "to optimize for this function\nso that we can learn well",
    "start": "2727490",
    "end": "2732530"
  },
  {
    "text": "even with small data\nsets or even when we have a small compute budget. ",
    "start": "2732530",
    "end": "2739280"
  },
  {
    "text": "So that's the intuition\nbehind meta learning.  There's two different ways to\nview meta learning algorithms.",
    "start": "2739280",
    "end": "2748830"
  },
  {
    "text": "The first is more of\na mechanistic view and the second is more\nof a probabilistic view.",
    "start": "2748830",
    "end": "2753890"
  },
  {
    "text": "Related to the\nidea of optimizing that learning process, you\ncan-- the mechanistic view",
    "start": "2753890",
    "end": "2759230"
  },
  {
    "text": "is that you can think about\na deep neural network that takes as input a data set and\nmakes predictions for new data",
    "start": "2759230",
    "end": "2766190"
  },
  {
    "text": "points and you just\nwant to optimize this deep neural network using\nthis kind of form of metadata",
    "start": "2766190",
    "end": "2772400"
  },
  {
    "text": "set and optimize\nit over those tasks so that if you give it a\ndata set for a new task,",
    "start": "2772400",
    "end": "2777940"
  },
  {
    "text": "it can give you parameters\nfor that new task. So that's the more\nmechanistic view.",
    "start": "2777940",
    "end": "2783520"
  },
  {
    "text": "That's how you might\nkind of implement one of these algorithms. The more probabilistic\nview is if we",
    "start": "2783520",
    "end": "2789910"
  },
  {
    "text": "have a set of source\ntasks, we could try to extract the shared\nknowledge, the shared",
    "start": "2789910",
    "end": "2795460"
  },
  {
    "text": "structure, from those tasks\nin a way that allows us to efficiently learn new tasks. ",
    "start": "2795460",
    "end": "2803150"
  },
  {
    "text": "And so then when\nwe have a new task, we'll try to use\nthat prior knowledge to infer the parameters\nfor the target task.",
    "start": "2803150",
    "end": "2812300"
  },
  {
    "text": " So with these two\nviews in mind, I",
    "start": "2812300",
    "end": "2821200"
  },
  {
    "text": "want to try to\nfirst kind of talk a little bit about this\nprobabilistic view, which will help us think about what\nit means for tasks to share",
    "start": "2821200",
    "end": "2829420"
  },
  {
    "text": "structure, and then we'll go\nback to the mechanistic view. ",
    "start": "2829420",
    "end": "2835190"
  },
  {
    "text": "Cool. So let's start with\nthe probabilistic view, thinking about what Bayes\nwould think of meta learning.",
    "start": "2835190",
    "end": "2842950"
  },
  {
    "text": "And for this, it'd be helpful to\ngo through the graphical model. So to start off,\nhow many of you are",
    "start": "2842950",
    "end": "2853540"
  },
  {
    "text": "familiar with Bayesian networks\nor equivalently directed graphical models?",
    "start": "2853540",
    "end": "2858940"
  },
  {
    "text": "Raise your hand? OK. And how many of you are not\nfamiliar with Bayesian networks or directed graphical models?",
    "start": "2858940",
    "end": "2865380"
  },
  {
    "text": "OK. It's about 50/50. So Bayes nets or graphical\nmodels, are covered,",
    "start": "2865380",
    "end": "2872570"
  },
  {
    "text": "they're covered in CS 109. And so for people who did\nundergrad at Stanford,",
    "start": "2872570",
    "end": "2878750"
  },
  {
    "text": "you probably have\nlearned about them, although you may be a little\nbit rusty, so we'll kind of walk through it a little bit.",
    "start": "2878750",
    "end": "2885120"
  },
  {
    "text": "So in-- I don't know if--",
    "start": "2885120",
    "end": "2891810"
  },
  {
    "text": "I think we'll call them directed\nor graphical models in general. They're often called\nBayes nets as well.",
    "start": "2891810",
    "end": "2897040"
  },
  {
    "text": "So in graphical models,\nrandom variables are denoted with circles. So if we have a random variable\nX, we put X in a circle,",
    "start": "2897040",
    "end": "2906890"
  },
  {
    "text": "we may also have a\nseparate random variable Y. So these are just two\ndifferent random variables.",
    "start": "2906890",
    "end": "2913240"
  },
  {
    "text": "And dependencies\nbetween random variables are represented with arrows.",
    "start": "2913240",
    "end": "2918710"
  },
  {
    "text": "And so, for example, if you have\na distribution P of Y given X,",
    "start": "2918710",
    "end": "2924589"
  },
  {
    "text": "this arrow kind of\nrepresents this dependency. And if, for example,\nthis is equal to P of Y,",
    "start": "2924590",
    "end": "2934825"
  },
  {
    "text": "that means that\nthere is actually no real dependency\non X and so you wouldn't draw an arrow here. ",
    "start": "2934825",
    "end": "2943267"
  },
  {
    "text": "So actually, I'll leave that up. So now, the other\nthing about Bayes nets is that it tells\nyou a little bit--",
    "start": "2943267",
    "end": "2949330"
  },
  {
    "text": "you can read off kind\nof whether or not two variables are independent. And so if you have--",
    "start": "2949330",
    "end": "2955870"
  },
  {
    "text": "maybe you have a\nBayesian network that looks something like this.",
    "start": "2955870",
    "end": "2962430"
  },
  {
    "text": "You can look at\nthis and figure out if two variables\nare independent.",
    "start": "2962430",
    "end": "2967510"
  },
  {
    "text": "So, for example, A and Y are\nnot independent from each other",
    "start": "2967510",
    "end": "2974800"
  },
  {
    "text": "because they have a\nline that connects them. And A and B are actually\nalso not independent",
    "start": "2974800",
    "end": "2981010"
  },
  {
    "text": "because they have a path that\nkind of goes from A to B. And so any set of variables\nthat have a path between them",
    "start": "2981010",
    "end": "2986890"
  },
  {
    "text": "are not independent. They have some dependency. Whereas if there's\nno path between them, then those two random\nvariables are independent.",
    "start": "2986890",
    "end": "2992930"
  },
  {
    "text": "So for example, A and D are\nindependent from one another. ",
    "start": "2992930",
    "end": "2998640"
  },
  {
    "text": "Cool. So that's kind of a very-- basically all you need to\nknow about graphical models,",
    "start": "2998640",
    "end": "3004830"
  },
  {
    "text": "at least for now. ",
    "start": "3004830",
    "end": "3015309"
  },
  {
    "text": "So now I want to draw what\nI'll call the graphical model for single task learning.",
    "start": "3015310",
    "end": "3022660"
  },
  {
    "text": "So in particular, say we have\nsome parameters, some labels,",
    "start": "3022660",
    "end": "3032160"
  },
  {
    "text": "and some inputs\nthat we could think",
    "start": "3032160",
    "end": "3038190"
  },
  {
    "text": "of having a graphical model\nof basically to predict Y, it's going to be a function\nof X and our parameters",
    "start": "3038190",
    "end": "3044220"
  },
  {
    "text": "because you can-- we have this\nkind of relationship of F of Y",
    "start": "3044220",
    "end": "3049560"
  },
  {
    "text": "given X and phi.",
    "start": "3049560",
    "end": "3055620"
  },
  {
    "text": "So you can think of this as a\ngraphical model for single task learning.",
    "start": "3055620",
    "end": "3061270"
  },
  {
    "text": "I should also note that if you\nare thinking about causality, sometimes people might--",
    "start": "3061270",
    "end": "3066960"
  },
  {
    "text": "this arrow-- or the kind of\nrelationship between X and Y,",
    "start": "3066960",
    "end": "3072240"
  },
  {
    "text": "you could possibly,\nlike, flip these arrows to be in different directions. But for the standpoint\nof this lecture,",
    "start": "3072240",
    "end": "3078700"
  },
  {
    "text": "it will be helpful to\nconsider this direction and we won't really\nbe considering things from the kind of\ncausality standpoint.",
    "start": "3078700",
    "end": "3086201"
  },
  {
    "text": " Cool. And you can think of phi as\nessentially the parameters of Y",
    "start": "3086201",
    "end": "3093412"
  },
  {
    "text": "given X. I mean, these\nare the parameters that we're going to be trying\nto infer in machine learning or in single task learning.",
    "start": "3093412",
    "end": "3099410"
  },
  {
    "text": " Now, actually, OK, I lied.",
    "start": "3099410",
    "end": "3104980"
  },
  {
    "text": "There's one more thing\nthat's helpful to know about graphical models,\nand this actually may not have been covered\nin 109, which is what's",
    "start": "3104980",
    "end": "3112390"
  },
  {
    "text": "referred to as plate notation. And in particular, if\nyou have multiple data points-- so say this is just\none input and one output,",
    "start": "3112390",
    "end": "3120653"
  },
  {
    "text": "then it'd be nice to\nhave something that could denote the entire\ndata set, not just",
    "start": "3120653",
    "end": "3126030"
  },
  {
    "text": "one input and one output. And so that's called\nplate notation, which is that instead of drawing\nall of the different data",
    "start": "3126030",
    "end": "3132270"
  },
  {
    "text": "points, which would\ntake me a long time, we'll instead kind of\ndraw a plate around here.",
    "start": "3132270",
    "end": "3140880"
  },
  {
    "text": "We'll have an i here\nthat will denote that this plate is over i and\nthis means that basically this",
    "start": "3140880",
    "end": "3146310"
  },
  {
    "text": "makes copies of everything\ninside the plate, indexed by i. ",
    "start": "3146310",
    "end": "3153640"
  },
  {
    "text": "Cool. Any questions on\nthis graphical model? Does this make sense?",
    "start": "3153640",
    "end": "3161120"
  },
  {
    "text": "Cool. So now if we get into\nmulti task learning and think about the\ngraphical model there,",
    "start": "3161120",
    "end": "3168425"
  },
  {
    "text": "first that means that we're\ngoing to have multiple tasks. And so we're going to have--",
    "start": "3168425",
    "end": "3175060"
  },
  {
    "text": "can index the tasks by j. And then we will have\nanother plate around here,",
    "start": "3175060",
    "end": "3182720"
  },
  {
    "text": "which will just mean that we\nhave multiple tasks that go, that are indexed by j.",
    "start": "3182720",
    "end": "3187930"
  },
  {
    "text": "So phi j is the\nparameters for task one, task two, task\nthree, and so forth.",
    "start": "3187930",
    "end": "3194060"
  },
  {
    "text": "And these parameters have\nsome shared structure. And in particular,\nthere is some dependency",
    "start": "3194060",
    "end": "3201700"
  },
  {
    "text": "on these parameters phi theta. ",
    "start": "3201700",
    "end": "3209360"
  },
  {
    "text": "Yeah. [INAUDIBLE] phi? Why does Y only depend on phi?",
    "start": "3209360",
    "end": "3215810"
  },
  {
    "text": "So Y depends on both phi\nand X. So there's an arrow from phi to Y and X to Y.",
    "start": "3215810",
    "end": "3221950"
  },
  {
    "text": "[INAUDIBLE] is Y a\nprediction or our label?",
    "start": "3221950",
    "end": "3231460"
  },
  {
    "text": "Y is-- This is where it\ngets a little bit messy. I guess Y-- so Y is the label\nand phi is the true parameters.",
    "start": "3231460",
    "end": "3241250"
  },
  {
    "text": "Oh, OK. Yeah. Yeah. What is the-- is there a\nrelation between i and j",
    "start": "3241250",
    "end": "3248720"
  },
  {
    "text": "because the plate on j, like,\ngoes over the plate on i? Yeah.",
    "start": "3248720",
    "end": "3253970"
  },
  {
    "text": "So you can think of i is over\ndata points, j is over tasks. And so this means that for\nevery task, there is another,",
    "start": "3253970",
    "end": "3261569"
  },
  {
    "text": "there is a set of data points. OK. Yeah. Yeah? [INAUDIBLE] because if\nyou wanted to, like,",
    "start": "3261570",
    "end": "3268940"
  },
  {
    "text": "the same data point for-- Yeah. So you may actually\nhave something where X is outside of this plate. It's more like-- actually, no.",
    "start": "3268940",
    "end": "3276700"
  },
  {
    "text": "So, yeah. It may not be true. And so this is the more general\ncase where they're different",
    "start": "3276700",
    "end": "3282950"
  },
  {
    "text": "but in practice-- and I'm\nactually not sure how I would draw it if I were to-- it's a little bit\ntricky to draw.",
    "start": "3282950",
    "end": "3289160"
  },
  {
    "text": "Yeah. Yeah? Is theta in this case\njust like the kind",
    "start": "3289160",
    "end": "3295640"
  },
  {
    "text": "of like the unifying\nmodel that is able to do all of these different tasks? So theta, theta is kind\nof an interesting thing.",
    "start": "3295640",
    "end": "3303300"
  },
  {
    "text": "So theta is-- you can think\nof theta as this shared information between the tasks\nand it's only the stuff that's",
    "start": "3303300",
    "end": "3311570"
  },
  {
    "text": "shared between the tasks. And I use the word latent\nhere because latent",
    "start": "3311570",
    "end": "3316970"
  },
  {
    "text": "means unobserved. So in particular, the data\npoints X and Y are observed.",
    "start": "3316970",
    "end": "3326030"
  },
  {
    "text": "And so in graphical models,\nif you shade something in, which is a little hard\nto do on whiteboards,",
    "start": "3326030",
    "end": "3331610"
  },
  {
    "text": "that means that\nthey're observed, whereas we don't observe\nthe true parameters and we don't observe what\nthe shared structure is.",
    "start": "3331610",
    "end": "3339230"
  },
  {
    "text": "And so it's worth mentioning\nthat if there is no dependency here, then that means the tasks\ndon't share any structure,",
    "start": "3339230",
    "end": "3346880"
  },
  {
    "text": "whereas if there is some\ndependency on the shared structure, then they do actually\nhave some shared information",
    "start": "3346880",
    "end": "3354230"
  },
  {
    "text": "between them.  Yeah. [INAUDIBLE] samples that are\nbeing sampled from [INAUDIBLE]",
    "start": "3354230",
    "end": "3365520"
  },
  {
    "text": "why do you need a\nplate around them? I'm just using this to denote\nthe kind of individual data",
    "start": "3365520",
    "end": "3370815"
  },
  {
    "text": "points in our training data set. [INAUDIBLE]",
    "start": "3370815",
    "end": "3378010"
  },
  {
    "start": "3378010",
    "end": "3386172"
  },
  {
    "text": "That's a good question. ",
    "start": "3386172",
    "end": "3392180"
  },
  {
    "text": "Yeah. So you could alternatively\nthink of X and Y as the random variable,\nlike, the random variable X",
    "start": "3392180",
    "end": "3401319"
  },
  {
    "text": "and the random variable\nY. And in that case, you wouldn't need\nthe plate here. You would still need\nthis plate, but you",
    "start": "3401320",
    "end": "3406630"
  },
  {
    "text": "wouldn't need this plate. Yeah. Yeah. So is this like the equivalent\nof a discriminative model,",
    "start": "3406630",
    "end": "3413180"
  },
  {
    "text": "since you're taking X, like\nthe X to Y is like a given. And, like, a more general\ncase, is there also",
    "start": "3413180",
    "end": "3418730"
  },
  {
    "text": "something else like some noise\naffecting the X and Y as input? Yeah. So there could be cases where\nthere is other things that",
    "start": "3418730",
    "end": "3426860"
  },
  {
    "text": "affect X and other things that\naffect Y. I'm leaving those out for simplicity. But, for example, Y\nmay not be perfectly--",
    "start": "3426860",
    "end": "3435350"
  },
  {
    "text": "like there could be some\nnoise that affects Y-- if you have label noise, for example. There could be other things\nthat affect X. But, yeah.",
    "start": "3435350",
    "end": "3442587"
  },
  {
    "text": "For simplicity, I'm\nleaving those out. ",
    "start": "3442587",
    "end": "3447770"
  },
  {
    "text": "Cool. So here I just kind\nof drew this, like,",
    "start": "3447770",
    "end": "3455210"
  },
  {
    "text": "for the training data points. It's helpful in some cases\nto actually write it out in a way that separately\nrepresents the test data",
    "start": "3455210",
    "end": "3461720"
  },
  {
    "text": "points. And I mention that because\nthe test data points are not",
    "start": "3461720",
    "end": "3468530"
  },
  {
    "text": "fully observed. The labels are not\nfully observed. If you have a set\nof training tasks, then you can observe the\nlabels for the training tasks,",
    "start": "3468530",
    "end": "3476960"
  },
  {
    "text": "but we'll go into that\nmore in a future lecture. ",
    "start": "3476960",
    "end": "3482148"
  },
  {
    "text": "But now, let's get into\na little bit about theta. So if you condition on the\nshared information on theta,",
    "start": "3482148",
    "end": "3489390"
  },
  {
    "text": "then it's worth noting\nthat the task parameters--",
    "start": "3489390",
    "end": "3496200"
  },
  {
    "text": "well, so first,\nthe task parameters are not independent\nright now because there's kind of a path between them.",
    "start": "3496200",
    "end": "3503230"
  },
  {
    "text": "But if you condition on\ntheta, the task parameters become independent. ",
    "start": "3503230",
    "end": "3510160"
  },
  {
    "text": "And so what that means is that\nif you condition on theta, you'll actually\nhave a lower entropy",
    "start": "3510160",
    "end": "3517420"
  },
  {
    "text": "distribution over your\ntask specific parameters, phi I, compared to if--",
    "start": "3517420",
    "end": "3524240"
  },
  {
    "text": "compared to just your\nkind of prior over phi i. ",
    "start": "3524240",
    "end": "3529810"
  },
  {
    "text": "And so what that means is that-- well, actually no. Maybe I'll ask you this. So, so if you can identify-- if\nyou know the shared structure,",
    "start": "3529810",
    "end": "3540090"
  },
  {
    "text": "if you can identify\nthe shared structure, theta, then when\nshould learning phi",
    "start": "3540090",
    "end": "3545875"
  },
  {
    "text": "I be faster than if you didn't\nknow that shared structure?",
    "start": "3545875",
    "end": "3551975"
  },
  {
    "text": "People have thoughts? ",
    "start": "3551975",
    "end": "3567512"
  },
  {
    "text": "I can also walk through the\nslide a little bit again. So I guess the first\nstep is that if we condition on this variable,\nthen these become independent.",
    "start": "3567512",
    "end": "3579970"
  },
  {
    "text": "And if you have\ninformation about theta, that lowers your entropy\nestimate of phi because the--",
    "start": "3579970",
    "end": "3586599"
  },
  {
    "text": " essentially you can\nkind of narrow down",
    "start": "3586600",
    "end": "3592690"
  },
  {
    "text": "what the value of\nphi is once you have information about theta. ",
    "start": "3592690",
    "end": "3599050"
  },
  {
    "text": "And so from there, if we\ncan identify information about theta, then\nwith this dependency,",
    "start": "3599050",
    "end": "3607260"
  },
  {
    "text": "learning phi should be faster\nbecause we have fewer bits to uncover from our\ntraining data points.",
    "start": "3607260",
    "end": "3616299"
  },
  {
    "text": "Yeah. As long as you really understand\nthe relationship between theta [INAUDIBLE], right?",
    "start": "3616300",
    "end": "3622069"
  },
  {
    "text": "Yeah. So if you have\ninformation about theta and this dependency exists and\nyou understand that dependency,",
    "start": "3622070",
    "end": "3628510"
  },
  {
    "text": "then learning phi\nshould be faster than if you didn't have\nthat information.",
    "start": "3628510",
    "end": "3633940"
  },
  {
    "text": "Exactly. Basically, you need fewer,\nless information about the data points to infer phi once you\nhave information about theta.",
    "start": "3633940",
    "end": "3642339"
  },
  {
    "start": "3642340",
    "end": "3649410"
  },
  {
    "text": "One other thought\nexercise that builds on this a little bit more is--",
    "start": "3649410",
    "end": "3657539"
  },
  {
    "text": "so we talked about how, if you\nhave information about theta that tells us-- that lowers our\nentropy, that gives us",
    "start": "3657540",
    "end": "3663750"
  },
  {
    "text": "more information about phi,\nnow what if the entropy of phi,",
    "start": "3663750",
    "end": "3670140"
  },
  {
    "text": "given theta, is zero? ",
    "start": "3670140",
    "end": "3677508"
  },
  {
    "text": "[INAUDIBLE] Yeah. We already have a model which\ncan perform all the tasks.",
    "start": "3677508",
    "end": "3683269"
  },
  {
    "text": "Yeah. So if, basically if your\nentropy goes to zero,",
    "start": "3683270",
    "end": "3688450"
  },
  {
    "text": "then you actually\nfully know phi. You, like, you have full\ninformation about phi.",
    "start": "3688450",
    "end": "3693952"
  },
  {
    "text": "And at that point,\nyou can actually just fully solve the tasks. Yeah.",
    "start": "3693952",
    "end": "3699370"
  },
  {
    "text": "And that means\nthat you don't even need any additional\ndata to solve the task. ",
    "start": "3699370",
    "end": "3706520"
  },
  {
    "text": "Cool. So in general, I think that this\nsort of Bayesian perspective",
    "start": "3706520",
    "end": "3713510"
  },
  {
    "text": "I think is a useful framework\nfor thinking about what it means for some tasks\nto share structure",
    "start": "3713510",
    "end": "3719869"
  },
  {
    "text": "and specifically in the kind of\nthe form of this variable here. And you can think\nabout these kind of different\nmathematical relations",
    "start": "3719870",
    "end": "3728410"
  },
  {
    "text": "as when we might\nexpect basically how much shared structure\nis versus how much data you need to learn a given task.",
    "start": "3728410",
    "end": "3734484"
  },
  {
    "text": " Cool.",
    "start": "3734485",
    "end": "3740170"
  },
  {
    "text": "Two other exercises-- or, well,\none exercise with two examples. So say that we have a\nset of sinusoid tasks.",
    "start": "3740170",
    "end": "3749530"
  },
  {
    "text": "So task one is a sinusoid\nwith an amplitude of, like, 5",
    "start": "3749530",
    "end": "3759990"
  },
  {
    "text": "and kind of a phase of pi,\nand then maybe task two",
    "start": "3759990",
    "end": "3765750"
  },
  {
    "text": "is an amplitude of 1 and a\nphase of pi over 2 or something like that.",
    "start": "3765750",
    "end": "3772120"
  },
  {
    "text": "And all of your tasks\nhave different amplitudes in different phases.",
    "start": "3772120",
    "end": "3777180"
  },
  {
    "text": "Then in that scenario, what\ninformation does theta contain? Yeah?",
    "start": "3777180",
    "end": "3782820"
  },
  {
    "text": "Different amplitudes and phases?  Not quite.",
    "start": "3782820",
    "end": "3788190"
  },
  {
    "text": "So the amplitude and phase--\nthis is for task one, this is for task two. The question is what is\nkind of-- what is theta?",
    "start": "3788190",
    "end": "3793610"
  },
  {
    "text": "What is the shared\nstructure between them? Yeah. Like the fact that\nthey're all sinusoidal?",
    "start": "3793610",
    "end": "3800450"
  },
  {
    "text": "Yeah. Exactly. So everything but the\namplitude and phase. So it kind of corresponds to\nthe family of sinusoid functions",
    "start": "3800450",
    "end": "3809830"
  },
  {
    "text": "that once we have\nthat shared structure, we just need to infer these\ntwo values using our data set.",
    "start": "3809830",
    "end": "3815230"
  },
  {
    "text": " One more example. So say that our tasks\nare machine translation",
    "start": "3815230",
    "end": "3825680"
  },
  {
    "text": "and our goal is to translate\nbetween two languages, and one task is to translate\nfrom French to English, another task is to translate\nfrom Japanese to Spanish,",
    "start": "3825680",
    "end": "3833750"
  },
  {
    "text": "or something like that. In this case, what does\ntheta correspond to? ",
    "start": "3833750",
    "end": "3841400"
  },
  {
    "text": "Yeah? Possible translation\n[INAUDIBLE]?? So you're saying it's\npossible translation",
    "start": "3841400",
    "end": "3847700"
  },
  {
    "text": "between any pair of languages? Any family of two languages. Any family of two languages.",
    "start": "3847700",
    "end": "3854390"
  },
  {
    "text": "So basically you're saying\nthat the shared structure is kind of a universal translator?",
    "start": "3854390",
    "end": "3860510"
  },
  {
    "text": "Yeah, that's close,\nalthough not quite. Yeah. Universal language\nrules [INAUDIBLE]..",
    "start": "3860510",
    "end": "3868330"
  },
  {
    "text": "Universal language rules. Yeah. So you're saying things\nlike adverbs and verbs and stuff like that?",
    "start": "3868330",
    "end": "3873740"
  },
  {
    "text": "Yeah. So this is, I guess both of\nthese are, like, mostly right.",
    "start": "3873740",
    "end": "3878830"
  },
  {
    "text": "It's basically going\nto be everything-- and I guess, I mean, sort\nof the first one, too.",
    "start": "3878830",
    "end": "3883990"
  },
  {
    "text": "It basically tells\nus information about the family of\nall language pairs, although it shouldn't contain\nall of the information needed",
    "start": "3883990",
    "end": "3891940"
  },
  {
    "text": "to translate between one\npair of languages because--",
    "start": "3891940",
    "end": "3897339"
  },
  {
    "text": "well, ideally the\nthings that are-- only the things that are shared\nand not the things that are needed to actually\nsolve the task.",
    "start": "3897340",
    "end": "3905710"
  },
  {
    "text": "Yeah. [INAUDIBLE] parameters since\nlanguages are so diverse?",
    "start": "3905710",
    "end": "3912470"
  },
  {
    "text": "Yeah. So this will be, this will\nbe, like, relatively small.",
    "start": "3912470",
    "end": "3917800"
  },
  {
    "text": "It won't include things like\nvocabulary of a given model. But it will contain\nthings like adverbs,",
    "start": "3917800",
    "end": "3924220"
  },
  {
    "text": "like the kind of\ngrammatical structure that you often see in languages.",
    "start": "3924220",
    "end": "3929380"
  },
  {
    "text": "[INAUDIBLE] across languages? ",
    "start": "3929380",
    "end": "3935960"
  },
  {
    "text": "Right. So the specifics of, like,\nwhat, like, what order do you put-- do you put the\nadjective before the noun",
    "start": "3935960",
    "end": "3941589"
  },
  {
    "text": "or after the noun--\nthose sorts of specifics won't be contained in theta. But the general notion of those\nthings is somewhat shared.",
    "start": "3941590",
    "end": "3948550"
  },
  {
    "text": "This is also sort\nof a hard question because it's something\nthat's rather vague and hard to put into words. Yeah.",
    "start": "3948550",
    "end": "3953970"
  },
  {
    "text": "So can you, like, translate this\nsort of set up into something like a network structure?",
    "start": "3953970",
    "end": "3959410"
  },
  {
    "text": "Would this, would theta\nbe like the early parts that are all shared\nbetween [INAUDIBLE] task?",
    "start": "3959410",
    "end": "3967170"
  },
  {
    "text": "Or-- like, yeah, like where\ndoes that sort of divide between theta [INAUDIBLE]? Yeah.",
    "start": "3967170",
    "end": "3972460"
  },
  {
    "text": "So the question was, like,\nin practice, what is-- like, does theta correspond\nto earlier layers",
    "start": "3972460",
    "end": "3977530"
  },
  {
    "text": "or, like, what is\nthis theta thing? So here we're really, like,\njust thinking conceptually.",
    "start": "3977530",
    "end": "3984130"
  },
  {
    "text": "In practice, this can, it\ncan correspond to a number of different things.",
    "start": "3984130",
    "end": "3989710"
  },
  {
    "text": "I chose the notation theta and-- I chose the notation\ntheta in part because one thing\nit can represent",
    "start": "3989710",
    "end": "3996460"
  },
  {
    "text": "is the initialization\nof fine tuning. You can think of it as kind\nof, the initialization, as kind of prior knowledge\nor kind of source",
    "start": "3996460",
    "end": "4002760"
  },
  {
    "text": "shared structure. But we'll see this\nmuch more concretely when we get to the lecture\non Bayesian meta learning.",
    "start": "4002760",
    "end": "4011950"
  },
  {
    "text": "Yeah. [INAUDIBLE]",
    "start": "4011950",
    "end": "4017900"
  },
  {
    "text": " Oh, is a meta learning\ntask and a task equivalent?",
    "start": "4017900",
    "end": "4025330"
  },
  {
    "text": "Or? [INAUDIBLE] Oh. Yeah. So this, this\ngraphical model will",
    "start": "4025330",
    "end": "4032042"
  },
  {
    "text": "be the same for both multi task\nlearning and meta learning. Yeah. Anytime you-- this is\nbasically the graphical model",
    "start": "4032042",
    "end": "4037320"
  },
  {
    "text": "for a set of tasks. It doesn't really cover-- and both multi task\nlearning and meta learning consider a set of tasks.",
    "start": "4037320",
    "end": "4044380"
  },
  {
    "text": "[INAUDIBLE] entropy is zero,\nthat means [INAUDIBLE]??",
    "start": "4044380",
    "end": "4054380"
  },
  {
    "text": "Yeah. So this last part-- so what does entropy\nof zero mean? So entropy of zero means that\nyou, your distribution over phi",
    "start": "4054380",
    "end": "4065170"
  },
  {
    "text": "is basically just deterministic. It just has a single value. Whereas if we had\na non-zero entropy,",
    "start": "4065170",
    "end": "4072580"
  },
  {
    "text": "that would mean that we have\nsome uncertainty around phi. Our distribution would\nbe a wider distribution.",
    "start": "4072580",
    "end": "4078110"
  },
  {
    "text": "And so when the\nentropy is 0, that means that we know exactly-- there's only one value that\nthe distribution covers.",
    "start": "4078110",
    "end": "4085120"
  },
  {
    "text": "There's only one value that the\nrandom variable can take on. And so in this case when the\nentropy of phi given theta",
    "start": "4085120",
    "end": "4092320"
  },
  {
    "text": "is zero, that means that phi\ncan only take on one value",
    "start": "4092320",
    "end": "4097359"
  },
  {
    "text": "once we have the\ninformation in theta. And that means that we\nkind of have already-- that means that once we\nhave this information,",
    "start": "4097359",
    "end": "4103460"
  },
  {
    "text": "we can fully recover\nthe parameters for all of our tasks. ",
    "start": "4103460",
    "end": "4109818"
  },
  {
    "text": "Yeah. How can we formulate the\ngoal for meta learning in this specific context?",
    "start": "4109819",
    "end": "4114950"
  },
  {
    "text": "Because meta learning wants\nto maximize transferability, but would that translate\ninto something of, like,",
    "start": "4114950",
    "end": "4123210"
  },
  {
    "text": "the relationships\nbetween the phi's? Yeah. So you can think of meta\nlearning as basically",
    "start": "4123210",
    "end": "4128270"
  },
  {
    "text": "trying to infer theta. So try to maximize for the\nability to learn a new task",
    "start": "4128270",
    "end": "4133549"
  },
  {
    "text": "and when we have as-- well, ideally if we--\nlike, it'd be nice if we could just\nget phi directly.",
    "start": "4133550",
    "end": "4139229"
  },
  {
    "text": "But if we're trying to\noptimize for the ability to learn a new task,\ngiven a set of tasks,",
    "start": "4139229",
    "end": "4144439"
  },
  {
    "text": "the best that we can\ndo is recover theta and once we have\ntheta, then that will allow us to learn new\ntasks from that distribution",
    "start": "4144439",
    "end": "4153109"
  },
  {
    "text": "more quickly. Yeah. ",
    "start": "4153109",
    "end": "4158490"
  },
  {
    "text": "So. Yeah. So basically, yeah. You can think of\nmeta learning is trying to infer, infer theta.",
    "start": "4158490",
    "end": "4166549"
  },
  {
    "text": "Yeah. [INAUDIBLE] as well?",
    "start": "4166550",
    "end": "4172660"
  },
  {
    "text": "Like, are they already\npart of the different-- Yeah. So you can think\nof meta learning as trying to learn\ninductive biases",
    "start": "4172660",
    "end": "4179479"
  },
  {
    "text": "or trying to learn,\nlike, structure. And there's separately\nchoices of structure",
    "start": "4179479",
    "end": "4185870"
  },
  {
    "text": "that we build in ourselves\nas humans and those can be represented--",
    "start": "4185870",
    "end": "4192210"
  },
  {
    "text": "I guess the way that\nI might represent that is by having\nsome other variable",
    "start": "4192210",
    "end": "4199130"
  },
  {
    "text": "here, which is like the human\nbuilt in inductive bias, and that maybe also has some\nprior over-- like, maybe we",
    "start": "4199130",
    "end": "4206720"
  },
  {
    "text": "have some guess at what phi is\nand that's going to-- this will kind of denote\nthat sort of guess.",
    "start": "4206720",
    "end": "4212960"
  },
  {
    "text": "And so, yeah. Something like that. Yeah. Can you clarify\nwhat you mean when",
    "start": "4212960",
    "end": "4218900"
  },
  {
    "text": "you say that phi is independent\non conditions are in the past? Because I can imagine\na situation where",
    "start": "4218900",
    "end": "4225110"
  },
  {
    "text": "we're training where there's,\nlike, three main tasks. Two of the tasks are very\nsimilar to each other, but the third one is,\nlike, completely different.",
    "start": "4225110",
    "end": "4232140"
  },
  {
    "text": "So just, like, so there's\nno shared knowledge in theta that's universal\nacross all tasks?",
    "start": "4232140",
    "end": "4238280"
  },
  {
    "text": "But still, like-- so in\nthis case, would phi 1 and phi 2 still be, like,\ndependent on each other?",
    "start": "4238280",
    "end": "4245630"
  },
  {
    "text": "But, like-- yeah. So you're saying\nthat in the case--",
    "start": "4245630",
    "end": "4251240"
  },
  {
    "text": "so this isn't a\nproblem with two tasks. You're saying in the\ncase of three tasks,",
    "start": "4251240",
    "end": "4256880"
  },
  {
    "text": "there may be\nscenarios where there",
    "start": "4256880",
    "end": "4266810"
  },
  {
    "text": "may be scenarios where the-- like, there's a lot of\nshared structure between two",
    "start": "4266810",
    "end": "4272630"
  },
  {
    "text": "of the tasks, a lot\nof shared structure between the third and then kind\nof the least common denominator is pretty small in that case.",
    "start": "4272630",
    "end": "4277875"
  },
  {
    "text": "And so then when you\ncondition on that, the first two tasks\nare not independent. Yeah. So I think that when you--",
    "start": "4277875",
    "end": "4283579"
  },
  {
    "text": " yeah. I think that cases like that\nend up getting more complicated.",
    "start": "4283580",
    "end": "4290010"
  },
  {
    "text": "I think it's cleanest to think\nabout in the case with two tasks and when you have\nmore than two tasks-- ",
    "start": "4290010",
    "end": "4297000"
  },
  {
    "text": "yeah. It's, yeah, a little bit more\ncomplicated to think about. Happy to discuss that more,\nlike, in office hours.",
    "start": "4297000",
    "end": "4302350"
  },
  {
    "text": " Cool. We have about five more minutes.",
    "start": "4302350",
    "end": "4308025"
  },
  {
    "text": " Trying to think about-- so let's talk a little bit\nabout the mechanistic view.",
    "start": "4308025",
    "end": "4315570"
  },
  {
    "text": "And, yeah. We may not quite\nfinish it, but we're going to start, we're going\nto talk a lot more about meta",
    "start": "4315570",
    "end": "4321090"
  },
  {
    "text": "learning next lecture, so\nit's OK if we don't finish. So we covered this\nprobabilistic view,",
    "start": "4321090",
    "end": "4328289"
  },
  {
    "text": "which is meta\nlearning as basically trying to recover\nthis shared structure or recover this theta.",
    "start": "4328290",
    "end": "4335213"
  },
  {
    "text": "For the rest of\nthe lecture, we're going to talk a little bit\nabout the mechanistic view.",
    "start": "4335213",
    "end": "4340260"
  },
  {
    "text": "And Bayes will come back later\nin the Bayesian meta learning lecture. ",
    "start": "4340260",
    "end": "4346690"
  },
  {
    "text": "Cool. So from the standpoint\nof the mechanistic view,",
    "start": "4346690",
    "end": "4352020"
  },
  {
    "text": "say your goal is\nto classify images. And we have, in this\ncase, a really tiny data",
    "start": "4352020",
    "end": "4358110"
  },
  {
    "text": "set of five examples. And our goal is to take this\ntraining data set and classify",
    "start": "4358110",
    "end": "4363590"
  },
  {
    "text": "new examples. So kind of going back\nto our conceptual view, these are going to\nbe kind of examples",
    "start": "4363590",
    "end": "4368690"
  },
  {
    "text": "for a new task for X and Y. Now if we want to solve\nthis task from scratch,",
    "start": "4368690",
    "end": "4377140"
  },
  {
    "text": "it won't work very well. And so we want to have-- we want to leverage\nprevious information.",
    "start": "4377140",
    "end": "4383199"
  },
  {
    "text": "And specifically, if we\nhave data from other tasks, then we should be able\nto use that to help",
    "start": "4383200",
    "end": "4389590"
  },
  {
    "text": "us solve this few-shot\nclassification problem. And in particular,\nwhat we can do",
    "start": "4389590",
    "end": "4394840"
  },
  {
    "text": "is we can take data\nfrom other image classes and construct them into tasks,\neach with their own train set",
    "start": "4394840",
    "end": "4402580"
  },
  {
    "text": "and test set. And so in particular,\nhere might be one task where instead\nof classifying things",
    "start": "4402580",
    "end": "4409010"
  },
  {
    "text": "like lions and dogs and bulls,\nour task is to classify, like, birds, pianos, mushrooms,\nand a different breed of dog.",
    "start": "4409010",
    "end": "4416575"
  },
  {
    "text": "Or, in this case, we can\nconstruct a different task with its own training\nset and test set where our goal is to classify\nbetween landscapes, gymnasts,",
    "start": "4416575",
    "end": "4423320"
  },
  {
    "text": "and carousels, for example. And so on and so forth. And what we can do\nis we can construct",
    "start": "4423320",
    "end": "4429233"
  },
  {
    "text": "all of these different\ntasks, ideally lots of different tasks.",
    "start": "4429233",
    "end": "4434270"
  },
  {
    "text": "These will be used-- these will be using a set\nof training image classes and we want to construct\nthem in a way that",
    "start": "4434270",
    "end": "4440600"
  },
  {
    "text": "allows us to learn how\nto quickly learn each of these tasks such that\nwhen we're given examples",
    "start": "4440600",
    "end": "4447710"
  },
  {
    "text": "of new image classes,\nwe can also learn a classifier for that task. ",
    "start": "4447710",
    "end": "4455180"
  },
  {
    "text": "So you can think of this top\nprocess as the meta training process where we\nare kind of learning how to learn these tasks\nand the bottom part",
    "start": "4455180",
    "end": "4463060"
  },
  {
    "text": "as the meta test, meta\ntesting part where we're trying to learn a new task. ",
    "start": "4463060",
    "end": "4470320"
  },
  {
    "text": "This is an example with\nimage classification where all the tasks are these\nimage classification problems.",
    "start": "4470320",
    "end": "4476920"
  },
  {
    "text": "But you can replace\nimage classification with really any other kind\nof machine learning task. So it doesn't have\nto be an image task.",
    "start": "4476920",
    "end": "4483290"
  },
  {
    "text": "It could be a regression task. It could be a language\ngeneration task. It could be trying\nto learn a skill.",
    "start": "4483290",
    "end": "4489489"
  },
  {
    "text": "Really any of the kind of\ntasks that we saw before, and multi task\nlearning could also be used to replace\nthe tasks here.",
    "start": "4489490",
    "end": "4495520"
  },
  {
    "text": " Now then kind more formally,\nthe goal of meta learning",
    "start": "4495520",
    "end": "4504520"
  },
  {
    "text": "is to try to, given data\nfrom a set of tasks, try to solve a new test\ntask more quickly, more",
    "start": "4504520",
    "end": "4510850"
  },
  {
    "text": "proficiently, or more stably. Oftentimes, in a lot of the use\ncases we'll see in this course",
    "start": "4510850",
    "end": "4516670"
  },
  {
    "text": "are to try to learn more\nquickly with fewer examples, but in principle,\nall these ideas could also be trying to\noptimize for other aspects",
    "start": "4516670",
    "end": "4523870"
  },
  {
    "text": "of the learning process,\nlike performance and the stability of\nthe learning process. ",
    "start": "4523870",
    "end": "4532070"
  },
  {
    "text": "Now one really key\nassumption here is that we have a set\nof training tasks.",
    "start": "4532070",
    "end": "4538520"
  },
  {
    "text": "We're going to\nassume that our test task is drawn from the same\ndistribution as our training tasks.",
    "start": "4538520",
    "end": "4545080"
  },
  {
    "text": "And so in particular, we'll\nhave some broader distribution",
    "start": "4545080",
    "end": "4550330"
  },
  {
    "text": "over tasks. It can be a little bit hard\nto think about what a task distribution is, but there\nis some broader distribution over those tasks and we need\nto assume that the training",
    "start": "4550330",
    "end": "4557650"
  },
  {
    "text": "task and the test task are both\ndrawn from that distribution such that when we're given\nenough samples of our training tasks, we can naturally\nexpect to generalize and learn",
    "start": "4557650",
    "end": "4566980"
  },
  {
    "text": "a new test task from\nthat distribution. So this is analogous to the\nstandard assumption in machine learning where we assume that\nour train set and our test set",
    "start": "4566980",
    "end": "4574340"
  },
  {
    "text": "are drawn from the\nsame data distribution. ",
    "start": "4574340",
    "end": "4580520"
  },
  {
    "text": "And like before,\nwe probably want these tasks to share structure. If this task distribution is a\ncompletely random distribution,",
    "start": "4580520",
    "end": "4587300"
  },
  {
    "text": "then we won't expect\nto be able to learn a new task because these\ntasks are drawing completely",
    "start": "4587300",
    "end": "4592610"
  },
  {
    "text": "from random.  Cool.",
    "start": "4592610",
    "end": "4597800"
  },
  {
    "text": "And then the task can compare\nto a number of-- can actually correspond to a number\nof different things, basically correspond to\nthe same kind of task",
    "start": "4597800",
    "end": "4604400"
  },
  {
    "text": "that we saw in\nmulti task learning. And one example that\nwe'll see in homework one is to recognize handwritten\ndigits from different languages",
    "start": "4604400",
    "end": "4611900"
  },
  {
    "text": "where we might want to be able\nto recognize new digits that we haven't seen before. ",
    "start": "4611900",
    "end": "4618870"
  },
  {
    "text": "I'll skip through this\nfor the sake of time. And I think I'll skip\nthe terminology, too.",
    "start": "4618870",
    "end": "4624690"
  },
  {
    "text": "I think that we can\ncover the terminology in Wednesday's lecture. ",
    "start": "4624690",
    "end": "4632410"
  },
  {
    "text": "Cool. So to start to recap,\nin this lecture, we talked about transfer\nlearning and meta learning.",
    "start": "4632410",
    "end": "4638375"
  },
  {
    "text": "We only got to the very\nbeginning of meta learning, but in transfer learning the\ngoal was to solve a target",
    "start": "4638375",
    "end": "4643950"
  },
  {
    "text": "task after having\nsolved a source task and you can think\nof meta learning as a subset of the\ntransfer learning problem",
    "start": "4643950",
    "end": "4651480"
  },
  {
    "text": "where we have a\nset of source tasks and we want to\ntransfer information",
    "start": "4651480",
    "end": "4656789"
  },
  {
    "text": "from that to a new test task. And so really it's\nbasically the same problem, except we're going to\nassume that we have not",
    "start": "4656790",
    "end": "4663150"
  },
  {
    "text": "just one source task but\nmultiple source tasks. ",
    "start": "4663150",
    "end": "4668780"
  },
  {
    "text": "Generally in both\nof these cases, it's fairly impractical to\naccess data from the source tasks.",
    "start": "4668780",
    "end": "4674738"
  },
  {
    "text": "And in all of these\nsettings, we want to have some sort\nof shared structure.",
    "start": "4674738",
    "end": "4680250"
  },
  {
    "text": "Then we'll skip some\nof these two slides.",
    "start": "4680250",
    "end": "4686110"
  },
  {
    "text": "Yeah. And then to provide a recap\nbeyond just the problem settings, today we\ntalked about transferring",
    "start": "4686110",
    "end": "4693150"
  },
  {
    "text": "via fine tuning by initializing\nand then optimizing on the target task and trying\nto be careful not to destroy",
    "start": "4693150",
    "end": "4698668"
  },
  {
    "text": "the features that were\ninitialized in the network by using a smaller learning\nrate or by training the last layer first.",
    "start": "4698668",
    "end": "4705630"
  },
  {
    "text": "We talked about\nthis graphical model which can give us some\nconceptual intuition for what it means for tasks\nto share structure",
    "start": "4705630",
    "end": "4712200"
  },
  {
    "text": "by having this statistical\ndependence on this shared latent information. And then lastly, we talked about\nhow meta learning is aiming",
    "start": "4712200",
    "end": "4719910"
  },
  {
    "text": "to try to actually infer\nwhat this shared structure is and use it to learn\ntasks more quickly. ",
    "start": "4719910",
    "end": "4728070"
  },
  {
    "text": "Cool. So that covers the\nplan for today. In terms of the next lectures,\nthe next five lectures",
    "start": "4728070",
    "end": "4734280"
  },
  {
    "text": "will be on really core\nmethods for meta learning and unsupervised pretraining,\nand these will also",
    "start": "4734280",
    "end": "4740940"
  },
  {
    "text": "be covered in homeworks\none, two, and three. And then, yeah, lastly,\na couple of reminders.",
    "start": "4740940",
    "end": "4747150"
  },
  {
    "text": "Homework Zero is due tonight,\nso make sure you get that in. ",
    "start": "4747150",
    "end": "4755000"
  }
]