[
  {
    "start": "0",
    "end": "16000"
  },
  {
    "start": "0",
    "end": "5050"
  },
  {
    "text": "CHRIS POTTS: Welcome, everyone,\nto our first screencast",
    "start": "5050",
    "end": "7450"
  },
  {
    "text": "on contextual word\nrepresentations.",
    "start": "7450",
    "end": "9400"
  },
  {
    "text": "My goal here is to give you\nan overview for this unit",
    "start": "9400",
    "end": "12280"
  },
  {
    "text": "and also give you a sense\nfor the conceptual landscape.",
    "start": "12280",
    "end": "15849"
  },
  {
    "text": "Let's start with the\nassociated materials.",
    "start": "15850",
    "end": "17740"
  },
  {
    "start": "16000",
    "end": "150000"
  },
  {
    "text": "You might think that the name\nof the game for this unit",
    "start": "17740",
    "end": "20170"
  },
  {
    "text": "is to get you to the point\nwhere you can work productively",
    "start": "20170",
    "end": "23020"
  },
  {
    "text": "with this notebook\ncalled finetuning,",
    "start": "23020",
    "end": "24910"
  },
  {
    "text": "which shows you how to fine-tune\ncontextual word representations",
    "start": "24910",
    "end": "28240"
  },
  {
    "text": "for classification problems.",
    "start": "28240",
    "end": "29619"
  },
  {
    "text": "I think that could be a\nvery powerful mode for you",
    "start": "29620",
    "end": "32110"
  },
  {
    "text": "as you work on the current\nassignment and bakeoff.",
    "start": "32110",
    "end": "35910"
  },
  {
    "text": "For background and intuitions,\nI highly recommend this paper",
    "start": "35910",
    "end": "38670"
  },
  {
    "text": "by Noah Smith.",
    "start": "38670",
    "end": "40879"
  },
  {
    "text": "The beating heart of\nthis unit is really",
    "start": "40880",
    "end": "42830"
  },
  {
    "text": "the Transformer\narchitecture, which",
    "start": "42830",
    "end": "44510"
  },
  {
    "text": "was introduced by Vaswani,\net al., 2017 in a paper",
    "start": "44510",
    "end": "47510"
  },
  {
    "text": "called \"Attention\nis All You Need.\"",
    "start": "47510",
    "end": "49550"
  },
  {
    "text": "It's a highly readable paper.",
    "start": "49550",
    "end": "51380"
  },
  {
    "text": "But I recommend that\nif you want to read it,",
    "start": "51380",
    "end": "53240"
  },
  {
    "text": "you instead read Sasha Rush's\noutstanding contribution,",
    "start": "53240",
    "end": "56330"
  },
  {
    "text": "\"The Annotated Transformer.\"",
    "start": "56330",
    "end": "58280"
  },
  {
    "text": "What this does is literally\nreproduce the text of Vaswani,",
    "start": "58280",
    "end": "61610"
  },
  {
    "text": "et al., 2017 with\nPyTorch code woven in,",
    "start": "61610",
    "end": "66020"
  },
  {
    "text": "culminating in a\ncomplete implementation",
    "start": "66020",
    "end": "68299"
  },
  {
    "text": "of the Transformer as\napplied to problems",
    "start": "68300",
    "end": "70490"
  },
  {
    "text": "in machine translation.",
    "start": "70490",
    "end": "72005"
  },
  {
    "text": "This is a wonderful\ncontribution in the sense",
    "start": "72005",
    "end": "73880"
  },
  {
    "text": "that to the extent that\nthere are points of unclarity",
    "start": "73880",
    "end": "76088"
  },
  {
    "text": "or uncertainty in\nthe original text,",
    "start": "76088",
    "end": "78110"
  },
  {
    "text": "they are fully resolved\nby Sasha's code.",
    "start": "78110",
    "end": "81080"
  },
  {
    "text": "And of course, this can give\nyou a really good example",
    "start": "81080",
    "end": "83330"
  },
  {
    "text": "of how to do efficient and\neffective implementation",
    "start": "83330",
    "end": "85730"
  },
  {
    "text": "of model architectures\nlike this using PyTorch.",
    "start": "85730",
    "end": "90095"
  },
  {
    "text": "In practical terms,\nwe're going to make",
    "start": "90095",
    "end": "91720"
  },
  {
    "text": "extensive use of the\nHugging Face Transformers",
    "start": "91720",
    "end": "93940"
  },
  {
    "text": "library, which has\nreally opened up access",
    "start": "93940",
    "end": "96370"
  },
  {
    "text": "to a wide range of pretrained\ntransformer models.",
    "start": "96370",
    "end": "99610"
  },
  {
    "text": "It's very exciting.",
    "start": "99610",
    "end": "100480"
  },
  {
    "text": "And it's enabled\nlots of new things.",
    "start": "100480",
    "end": "103360"
  },
  {
    "text": "For us, the central\narchitecture will be BERT.",
    "start": "103360",
    "end": "105730"
  },
  {
    "text": "We'll have a separate\nscreencast on that.",
    "start": "105730",
    "end": "107570"
  },
  {
    "text": "And we're also going to have\na screencast on RoBERTa, which",
    "start": "107570",
    "end": "110028"
  },
  {
    "text": "is Robustly Optimized BERT.",
    "start": "110028",
    "end": "111430"
  },
  {
    "text": "I think it's an interesting\nperspective in the sense",
    "start": "111430",
    "end": "113680"
  },
  {
    "text": "that they explored more deeply\nsome of the open questions",
    "start": "113680",
    "end": "117160"
  },
  {
    "text": "from the original BERT paper.",
    "start": "117160",
    "end": "118930"
  },
  {
    "text": "And they also released very\npowerful pretrained parameters",
    "start": "118930",
    "end": "122080"
  },
  {
    "text": "that you could, again,\nuse in the context",
    "start": "122080",
    "end": "123850"
  },
  {
    "text": "of your own fine-tuning.",
    "start": "123850",
    "end": "126260"
  },
  {
    "text": "And then for a slightly\ndifferent perspective",
    "start": "126260",
    "end": "128192"
  },
  {
    "text": "on these transformers,\nwe're going",
    "start": "128193",
    "end": "129610"
  },
  {
    "text": "to look at the ELECTRA\narchitecture, which",
    "start": "129610",
    "end": "131470"
  },
  {
    "text": "came from Kevin\nClark and colleagues",
    "start": "131470",
    "end": "133210"
  },
  {
    "text": "at Stanford and Google.",
    "start": "133210",
    "end": "134830"
  },
  {
    "text": "I really like this\nas a new perspective.",
    "start": "134830",
    "end": "136600"
  },
  {
    "text": "There are, of course,\nmany different modes",
    "start": "136600",
    "end": "138490"
  },
  {
    "text": "of using the transformer\nat this point.",
    "start": "138490",
    "end": "140200"
  },
  {
    "text": "I'm going to mention a few\nat the end of the screencast.",
    "start": "140200",
    "end": "142942"
  },
  {
    "text": "And just for the\nsake of time, I've",
    "start": "142942",
    "end": "144400"
  },
  {
    "text": "decided to focus on ELECTRA.",
    "start": "144400",
    "end": "146439"
  },
  {
    "text": "And you can explore the\nothers in your own research.",
    "start": "146440",
    "end": "150460"
  },
  {
    "start": "150000",
    "end": "351000"
  },
  {
    "text": "Let's begin with\nsome intuitions.",
    "start": "150460",
    "end": "151840"
  },
  {
    "text": "And I'd like to begin with a\nlinguistic intuition, which",
    "start": "151840",
    "end": "154180"
  },
  {
    "text": "has to do with word\nrepresentations",
    "start": "154180",
    "end": "156400"
  },
  {
    "text": "and how they should be\nshaped by the context.",
    "start": "156400",
    "end": "158530"
  },
  {
    "text": "Let's focus on the\nEnglish verb break.",
    "start": "158530",
    "end": "161200"
  },
  {
    "text": "We have a simple\nexample, the vase broke,",
    "start": "161200",
    "end": "163030"
  },
  {
    "text": "which means it\nshattered to pieces.",
    "start": "163030",
    "end": "165650"
  },
  {
    "text": "Here's a superficially\nsimilar sentence, dawn broke.",
    "start": "165650",
    "end": "168700"
  },
  {
    "text": "Now the sense of break is\nsomething more like begin.",
    "start": "168700",
    "end": "172690"
  },
  {
    "text": "The news broke.",
    "start": "172690",
    "end": "173710"
  },
  {
    "text": "Again, a simple\nintransitive sentence.",
    "start": "173710",
    "end": "175660"
  },
  {
    "text": "But now the verb\nbreak means something",
    "start": "175660",
    "end": "177940"
  },
  {
    "text": "more like publish, or up\nhere, or become known.",
    "start": "177940",
    "end": "181770"
  },
  {
    "text": "Sandy broke the world record.",
    "start": "181770",
    "end": "183180"
  },
  {
    "text": "This is a transitive\nuse of the verb break.",
    "start": "183180",
    "end": "185250"
  },
  {
    "text": "And it means, surpass\nthe previous level.",
    "start": "185250",
    "end": "188760"
  },
  {
    "text": "Sandy broke the law is\nanother transitive use.",
    "start": "188760",
    "end": "190769"
  },
  {
    "text": "But now it means,\nSandy transgressed.",
    "start": "190770",
    "end": "194070"
  },
  {
    "text": "The burglar broke into the\nhouse is a physical act",
    "start": "194070",
    "end": "197010"
  },
  {
    "text": "of transgression on its face.",
    "start": "197010",
    "end": "198870"
  },
  {
    "text": "The newscaster broke\ninto the movie broadcast",
    "start": "198870",
    "end": "201390"
  },
  {
    "text": "is a sense that it's\nmore like interrupt.",
    "start": "201390",
    "end": "204610"
  },
  {
    "text": "And we have idioms\nlike break even, which",
    "start": "204610",
    "end": "206770"
  },
  {
    "text": "means we neither\ngained nor lost money.",
    "start": "206770",
    "end": "209380"
  },
  {
    "text": "And this is just a\nfew of the many ways",
    "start": "209380",
    "end": "211810"
  },
  {
    "text": "that the verb break\ncan be used in English.",
    "start": "211810",
    "end": "213940"
  },
  {
    "text": "How many senses\nare at work here?",
    "start": "213940",
    "end": "215590"
  },
  {
    "text": "It's very hard to say.",
    "start": "215590",
    "end": "216513"
  },
  {
    "text": "It could be one.",
    "start": "216513",
    "end": "217180"
  },
  {
    "text": "It could be two.",
    "start": "217180",
    "end": "217847"
  },
  {
    "text": "It could be ten.",
    "start": "217847",
    "end": "219230"
  },
  {
    "text": "It's very hard to\ndelimit word senses.",
    "start": "219230",
    "end": "220959"
  },
  {
    "text": "But it is very\nclear from this data",
    "start": "220960",
    "end": "223300"
  },
  {
    "text": "that our sense\nfor the verb break",
    "start": "223300",
    "end": "225460"
  },
  {
    "text": "is being shaped by the\nimmediate linguistic context.",
    "start": "225460",
    "end": "229000"
  },
  {
    "text": "Here are a few\nadditional examples,",
    "start": "229000",
    "end": "231280"
  },
  {
    "text": "things like flat tire, flat\nbeer, flat note, flat surface.",
    "start": "231280",
    "end": "234700"
  },
  {
    "text": "It's clear that there is\na conceptual core running",
    "start": "234700",
    "end": "238000"
  },
  {
    "text": "through all of these uses.",
    "start": "238000",
    "end": "239980"
  },
  {
    "text": "But it's also true\nthat a flat tire",
    "start": "239980",
    "end": "242260"
  },
  {
    "text": "is a very different\nsense for flat",
    "start": "242260",
    "end": "244120"
  },
  {
    "text": "than we get from flat\nnote or flat surface.",
    "start": "244120",
    "end": "247360"
  },
  {
    "text": "I have something similar for\nthrow a party, throw a fight,",
    "start": "247360",
    "end": "249880"
  },
  {
    "text": "throw a ball, throw a fit.",
    "start": "249880",
    "end": "251590"
  },
  {
    "text": "We have a mixture of\nwhat you might call",
    "start": "251590",
    "end": "253269"
  },
  {
    "text": "literal and metaphorical here.",
    "start": "253270",
    "end": "254860"
  },
  {
    "text": "But again, a kind of common\ncore that we're drawing on.",
    "start": "254860",
    "end": "258049"
  },
  {
    "text": "But the bottom line is that the\nsense for throw, in this case,",
    "start": "258050",
    "end": "261310"
  },
  {
    "text": "is very different depending on\nwhat kind of linguistic context",
    "start": "261310",
    "end": "264550"
  },
  {
    "text": "it's in.",
    "start": "264550",
    "end": "266360"
  },
  {
    "text": "And we can extend this to\nthings that seem to turn more",
    "start": "266360",
    "end": "269300"
  },
  {
    "text": "on world knowledge.",
    "start": "269300",
    "end": "270360"
  },
  {
    "text": "So if you have something\nlike, a crane caught a fish,",
    "start": "270360",
    "end": "272990"
  },
  {
    "text": "we have a sense that the\ncrane here is a bird.",
    "start": "272990",
    "end": "275930"
  },
  {
    "text": "Whereas, if we have a crane\npicked up the steel beam,",
    "start": "275930",
    "end": "278300"
  },
  {
    "text": "we have a sense that it's\na piece of equipment.",
    "start": "278300",
    "end": "281115"
  },
  {
    "text": "This seems like\nsomething that's guided",
    "start": "281115",
    "end": "282740"
  },
  {
    "text": "by our understanding of birds,\nand equipment, and fish,",
    "start": "282740",
    "end": "286009"
  },
  {
    "text": "and beams.",
    "start": "286010",
    "end": "287090"
  },
  {
    "text": "And when we have relatively\nunbiased sentences like,",
    "start": "287090",
    "end": "289580"
  },
  {
    "text": "I saw a crane,\nwe're kind of left",
    "start": "289580",
    "end": "291470"
  },
  {
    "text": "guessing about which\nobject is involved,",
    "start": "291470",
    "end": "293450"
  },
  {
    "text": "the bird or the machine.",
    "start": "293450",
    "end": "295468"
  },
  {
    "text": "And we can extend this\npast world knowledge",
    "start": "295468",
    "end": "297259"
  },
  {
    "text": "into things that are more\nlike discourse understanding.",
    "start": "297260",
    "end": "299760"
  },
  {
    "text": "So if you have a sentence\nlike, \"are there typos?",
    "start": "299760",
    "end": "302240"
  },
  {
    "text": "I didn't see any.\"",
    "start": "302240",
    "end": "303690"
  },
  {
    "text": "The sense of \"any\"\nhere, we have a feeling",
    "start": "303690",
    "end": "305720"
  },
  {
    "text": "that something is elided but\nprobably localized on \"any.\"",
    "start": "305720",
    "end": "308590"
  },
  {
    "text": "And \"any\" here means any typos\nas a result of the preceding",
    "start": "308590",
    "end": "312380"
  },
  {
    "text": "linguistic context.",
    "start": "312380",
    "end": "314580"
  },
  {
    "text": "\"Are there any\nbookstores downtown?",
    "start": "314580",
    "end": "316039"
  },
  {
    "text": "I didn't see any.\"",
    "start": "316040",
    "end": "316790"
  },
  {
    "text": "Same second sentence.",
    "start": "316790",
    "end": "318140"
  },
  {
    "text": "But now the sense\nof \"any\" is probably",
    "start": "318140",
    "end": "320240"
  },
  {
    "text": "going to be something\nmore like bookstores,",
    "start": "320240",
    "end": "322099"
  },
  {
    "text": "as a result of the discourse\ncontext that it appears in.",
    "start": "322100",
    "end": "326120"
  },
  {
    "text": "So all of this is\njust showing how much",
    "start": "326120",
    "end": "328370"
  },
  {
    "text": "individual linguistic units\ncan be shaped by context.",
    "start": "328370",
    "end": "331850"
  },
  {
    "text": "Linguists know this deeply.",
    "start": "331850",
    "end": "333210"
  },
  {
    "text": "This is a primary thing that\nlinguists try to get a grip on.",
    "start": "333210",
    "end": "336169"
  },
  {
    "text": "And I think it's a wonderful\npoint of connection",
    "start": "336170",
    "end": "338360"
  },
  {
    "text": "between what linguists\ndo and the way we're",
    "start": "338360",
    "end": "340610"
  },
  {
    "text": "representing examples in\nNLP using contextual models.",
    "start": "340610",
    "end": "344389"
  },
  {
    "text": "This is a very\nexciting development",
    "start": "344390",
    "end": "346040"
  },
  {
    "text": "for me as a linguist,\nas well as an NLPer.",
    "start": "346040",
    "end": "348380"
  },
  {
    "start": "348380",
    "end": "351850"
  },
  {
    "start": "351000",
    "end": "507000"
  },
  {
    "text": "Here's another set of intuitions\nthat's related more to things",
    "start": "351850",
    "end": "354820"
  },
  {
    "text": "like model architecture and what\nyou might call inductive biases",
    "start": "354820",
    "end": "358390"
  },
  {
    "text": "for different model designs.",
    "start": "358390",
    "end": "360195"
  },
  {
    "text": "Let's start off\nhere in the left.",
    "start": "360195",
    "end": "361570"
  },
  {
    "text": "This is a high-bias\nmodel in the sense",
    "start": "361570",
    "end": "363610"
  },
  {
    "text": "that it makes a lot of a\npriori decisions about how",
    "start": "363610",
    "end": "366759"
  },
  {
    "text": "we will represent our examples.",
    "start": "366760",
    "end": "368470"
  },
  {
    "text": "The idea is that we have\nthree tokens, which we look up",
    "start": "368470",
    "end": "371230"
  },
  {
    "text": "in a fixed embedding space.",
    "start": "371230",
    "end": "373360"
  },
  {
    "text": "And then we have decided to\nsummarize those embeddings",
    "start": "373360",
    "end": "375610"
  },
  {
    "text": "by simply summing them together\nto get a representation",
    "start": "375610",
    "end": "378219"
  },
  {
    "text": "for the entire example.",
    "start": "378220",
    "end": "379870"
  },
  {
    "text": "Very few of these components are\nlearned as part of our problem.",
    "start": "379870",
    "end": "382870"
  },
  {
    "text": "We've made most of the\ndecisions ahead of time.",
    "start": "382870",
    "end": "385990"
  },
  {
    "text": "As we've seen, as we move to\na recurrent neural network,",
    "start": "385990",
    "end": "388539"
  },
  {
    "text": "we relax some of\nthose assumptions.",
    "start": "388540",
    "end": "390880"
  },
  {
    "text": "We're still going to look\nupwards in a fixed embedding",
    "start": "390880",
    "end": "393130"
  },
  {
    "text": "space.",
    "start": "393130",
    "end": "393630"
  },
  {
    "text": "But now instead of deciding\nthat we know the proper way",
    "start": "393630",
    "end": "396010"
  },
  {
    "text": "to combine them\nis with summation,",
    "start": "396010",
    "end": "397870"
  },
  {
    "text": "we're going to\nlearn from our data",
    "start": "397870",
    "end": "399400"
  },
  {
    "text": "a very complicated function\nfor combining them.",
    "start": "399400",
    "end": "402555"
  },
  {
    "text": "And that will\npresumably allow us",
    "start": "402555",
    "end": "403930"
  },
  {
    "text": "to be more responsive, that is,\nless biased about what the data",
    "start": "403930",
    "end": "407889"
  },
  {
    "text": "are likely to look like.",
    "start": "407890",
    "end": "410020"
  },
  {
    "text": "The tree structured\narchitecture down here",
    "start": "410020",
    "end": "411819"
  },
  {
    "text": "is an interesting\nmixture of these ideas.",
    "start": "411820",
    "end": "413950"
  },
  {
    "text": "It's like the recurrent\nneural network.",
    "start": "413950",
    "end": "415660"
  },
  {
    "text": "Instead of assuming that I can\nprocess the data left-to-right,",
    "start": "415660",
    "end": "419200"
  },
  {
    "text": "the data get processed\ninto constituents,",
    "start": "419200",
    "end": "421720"
  },
  {
    "text": "like \"the Rock\" is a\nconstituent, as excluded",
    "start": "421720",
    "end": "424150"
  },
  {
    "text": "from \"rules\" over here.",
    "start": "424150",
    "end": "426520"
  },
  {
    "text": "Now this is probably\ngoing to be very powerful",
    "start": "426520",
    "end": "428740"
  },
  {
    "text": "if we're correct that the\nlanguage data are structured",
    "start": "428740",
    "end": "431289"
  },
  {
    "text": "according to these constituents.",
    "start": "431290",
    "end": "433150"
  },
  {
    "text": "Because it will give us a\nboost in terms of learning.",
    "start": "433150",
    "end": "436113"
  },
  {
    "text": "It could be counterproductive,\nthough, to the extent",
    "start": "436113",
    "end": "438280"
  },
  {
    "text": "that constituent\nstructure is wrong.",
    "start": "438280",
    "end": "440139"
  },
  {
    "text": "And I think that's\nshowing that biases",
    "start": "440140",
    "end": "442270"
  },
  {
    "text": "that we impose at the\nlevel of our architectures",
    "start": "442270",
    "end": "444940"
  },
  {
    "text": "can be helpful, as well\nas a hindrance, depending",
    "start": "444940",
    "end": "447580"
  },
  {
    "text": "on how they align with\nthe data-driven problem",
    "start": "447580",
    "end": "450129"
  },
  {
    "text": "that we're trying to solve.",
    "start": "450130",
    "end": "452570"
  },
  {
    "text": "In the bottom right here, I\nhave the least biased model",
    "start": "452570",
    "end": "455570"
  },
  {
    "text": "in all these sentences, of\nall the ones depicted here.",
    "start": "455570",
    "end": "458647"
  },
  {
    "text": "I've got a recurrent neural\nnetwork like this one,",
    "start": "458647",
    "end": "460730"
  },
  {
    "text": "except now I'm assuming\nthat information",
    "start": "460730",
    "end": "462520"
  },
  {
    "text": "can flow bidirectionally,\nso no longer",
    "start": "462520",
    "end": "464539"
  },
  {
    "text": "a presumption of left-to-right.",
    "start": "464540",
    "end": "466550"
  },
  {
    "text": "And in addition, I've added\nthese attention mechanisms,",
    "start": "466550",
    "end": "469560"
  },
  {
    "text": "which we'll talk a lot\nabout in this unit.",
    "start": "469560",
    "end": "471537"
  },
  {
    "text": "But essentially,\nthink of them as ways",
    "start": "471537",
    "end": "473120"
  },
  {
    "text": "of creating special connections\nbetween all of the hidden",
    "start": "473120",
    "end": "476630"
  },
  {
    "text": "units.",
    "start": "476630",
    "end": "477470"
  },
  {
    "text": "And the idea here is that we\nwould let the data tell us",
    "start": "477470",
    "end": "480620"
  },
  {
    "text": "how to weight all of\nthese various connections",
    "start": "480620",
    "end": "482600"
  },
  {
    "text": "and, in turn,\nrepresent our examples.",
    "start": "482600",
    "end": "485280"
  },
  {
    "text": "We're making very few\ndecisions ahead of time",
    "start": "485280",
    "end": "487880"
  },
  {
    "text": "about what connections\ncould be made,",
    "start": "487880",
    "end": "489740"
  },
  {
    "text": "and instead of just listening\nto the data and the learning",
    "start": "489740",
    "end": "492530"
  },
  {
    "text": "process.",
    "start": "492530",
    "end": "493700"
  },
  {
    "text": "And we are, at this\npoint, on the road",
    "start": "493700",
    "end": "496038"
  },
  {
    "text": "toward the Transformer,\nwhich is kind",
    "start": "496038",
    "end": "497580"
  },
  {
    "text": "of an extreme case of connecting\neverything with everything else",
    "start": "497580",
    "end": "501150"
  },
  {
    "text": "and then allowing the data\nto tell us how to weight all",
    "start": "501150",
    "end": "503660"
  },
  {
    "text": "of those various connections.",
    "start": "503660",
    "end": "506925"
  },
  {
    "text": "And that does bring us to this\nnotion of attention, which",
    "start": "506925",
    "end": "509300"
  },
  {
    "start": "507000",
    "end": "615000"
  },
  {
    "text": "we've not discussed before.",
    "start": "509300",
    "end": "510530"
  },
  {
    "text": "But I think I can introduce the\nconcepts and a bit of the math.",
    "start": "510530",
    "end": "513500"
  },
  {
    "text": "And then we'll see them\nagain throughout this unit.",
    "start": "513500",
    "end": "515940"
  },
  {
    "text": "So let's start with the\nsimple sentiment example",
    "start": "515940",
    "end": "517940"
  },
  {
    "text": "and imagine we're dealing with\na recurrent neural network",
    "start": "517940",
    "end": "520315"
  },
  {
    "text": "classifier.",
    "start": "520315",
    "end": "521390"
  },
  {
    "text": "Our example is\nreally not so good.",
    "start": "521390",
    "end": "523099"
  },
  {
    "text": "And we're going to fit the\nclassifier, traditionally,",
    "start": "523100",
    "end": "525620"
  },
  {
    "text": "on top of this final\nhidden state over here.",
    "start": "525620",
    "end": "528680"
  },
  {
    "text": "But we might worry\nabout doing that.",
    "start": "528680",
    "end": "531240"
  },
  {
    "text": "That by the time we've\ngotten to this final state,",
    "start": "531240",
    "end": "533570"
  },
  {
    "text": "the contribution of\nthese earlier words,",
    "start": "533570",
    "end": "535520"
  },
  {
    "text": "which are clearly\nimportant linguistically,",
    "start": "535520",
    "end": "537500"
  },
  {
    "text": "might be sort of forgotten\nor overly diffuse.",
    "start": "537500",
    "end": "540740"
  },
  {
    "text": "So attention mechanisms\nwould be a way for us",
    "start": "540740",
    "end": "543260"
  },
  {
    "text": "to bring that information\nback in and infuse",
    "start": "543260",
    "end": "546290"
  },
  {
    "text": "this representation,\nhc, with some",
    "start": "546290",
    "end": "548660"
  },
  {
    "text": "of those previous\nimportant connections.",
    "start": "548660",
    "end": "552248"
  },
  {
    "text": "So here's how we do that.",
    "start": "552248",
    "end": "553290"
  },
  {
    "text": "We're going to first have\nsome attention scores, which",
    "start": "553290",
    "end": "555540"
  },
  {
    "text": "are simply dot products of\nour target vector with all",
    "start": "555540",
    "end": "558110"
  },
  {
    "text": "the preceding hidden states.",
    "start": "558110",
    "end": "559821"
  },
  {
    "text": "So that gives us a vector of\nscores which are traditionally",
    "start": "559822",
    "end": "562280"
  },
  {
    "text": "softmax normalized.",
    "start": "562280",
    "end": "564440"
  },
  {
    "text": "And then what we do\nis create a context",
    "start": "564440",
    "end": "566150"
  },
  {
    "text": "vector by weighting each of\nthe previous hidden states",
    "start": "566150",
    "end": "569900"
  },
  {
    "text": "by its attention\nweight, and then taking",
    "start": "569900",
    "end": "572450"
  },
  {
    "text": "the average of those to give\nus the context vector K.",
    "start": "572450",
    "end": "576082"
  },
  {
    "text": "And then we're going to have\nthis special layer here, which",
    "start": "576082",
    "end": "578540"
  },
  {
    "text": "concatenates K with our\nprevious final hidden state,",
    "start": "578540",
    "end": "582230"
  },
  {
    "text": "and feeds that through this\nlayer of learned parameters",
    "start": "582230",
    "end": "585139"
  },
  {
    "text": "and a nonlinearity to give us\nthis new hidden representation,",
    "start": "585140",
    "end": "589100"
  },
  {
    "text": "h tilde.",
    "start": "589100",
    "end": "590540"
  },
  {
    "text": "And it is h tilde\nthat is finally",
    "start": "590540",
    "end": "592699"
  },
  {
    "text": "the input to our\nsoftmax classifier.",
    "start": "592700",
    "end": "595160"
  },
  {
    "text": "Whereas before, we would\nhave simply directly input hc",
    "start": "595160",
    "end": "598129"
  },
  {
    "text": "up here, we now input\nthis more refined version",
    "start": "598130",
    "end": "601640"
  },
  {
    "text": "that is drawing on all of\nthese attention connections",
    "start": "601640",
    "end": "603950"
  },
  {
    "text": "that we created with\nthese mechanisms.",
    "start": "603950",
    "end": "606230"
  },
  {
    "text": "And again, as you'll\nsee, the transformer",
    "start": "606230",
    "end": "608360"
  },
  {
    "text": "does this all over the place\nwith all of its representations",
    "start": "608360",
    "end": "611450"
  },
  {
    "text": "at various points\nin its computations.",
    "start": "611450",
    "end": "615570"
  },
  {
    "start": "615000",
    "end": "684000"
  },
  {
    "text": "Here's another guiding\nidea that really",
    "start": "615570",
    "end": "617212"
  },
  {
    "text": "shapes how these models work.",
    "start": "617212",
    "end": "618420"
  },
  {
    "text": "I've called this word pieces.",
    "start": "618420",
    "end": "620024"
  },
  {
    "text": "And we've seen this before.",
    "start": "620025",
    "end": "621150"
  },
  {
    "text": "These models typically do\nnot tokenize data in the way",
    "start": "621150",
    "end": "624420"
  },
  {
    "text": "that we might expect.",
    "start": "624420",
    "end": "625680"
  },
  {
    "text": "I've loaded in a BERT tokenizer.",
    "start": "625680",
    "end": "627930"
  },
  {
    "text": "And you can see\nthat for a sentence,",
    "start": "627930",
    "end": "629430"
  },
  {
    "text": "like \"this isn't too surprising\"\nthe result is some pretty",
    "start": "629430",
    "end": "631847"
  },
  {
    "text": "familiar tokens, by and large.",
    "start": "631847",
    "end": "634180"
  },
  {
    "text": "But when I feed in\nsomething like \"encode me\"",
    "start": "634180",
    "end": "636180"
  },
  {
    "text": "the intuitive word\n\"encode\" is split apart",
    "start": "636180",
    "end": "638339"
  },
  {
    "text": "into two-word pieces.",
    "start": "638340",
    "end": "639990"
  },
  {
    "text": "And clearly, we're\nimplicitly assuming",
    "start": "639990",
    "end": "642180"
  },
  {
    "text": "that the model, because\nit's contextual,",
    "start": "642180",
    "end": "644430"
  },
  {
    "text": "can figure out that these pieces\nare in some conceptual sense",
    "start": "644430",
    "end": "647610"
  },
  {
    "text": "one word.",
    "start": "647610",
    "end": "648899"
  },
  {
    "text": "And you might extend that up to\nidioms like \"out of this world\"",
    "start": "648900",
    "end": "651930"
  },
  {
    "text": "where we treat them as a\nbunch of distinct tokens.",
    "start": "651930",
    "end": "654750"
  },
  {
    "text": "But we might hope\nthe model can learn",
    "start": "654750",
    "end": "656430"
  },
  {
    "text": "that there's an idiomatic\nunity to that phrase.",
    "start": "656430",
    "end": "659910"
  },
  {
    "text": "And this also has\nthe side advantage",
    "start": "659910",
    "end": "661410"
  },
  {
    "text": "that for unknown tokens\nlike \"Snuffleupagus\"",
    "start": "661410",
    "end": "663899"
  },
  {
    "text": "it can break them apart\ninto familiar pieces.",
    "start": "663900",
    "end": "666180"
  },
  {
    "text": "And we at least have\na hope of getting",
    "start": "666180",
    "end": "668190"
  },
  {
    "text": "a sensible representation for\nthat out-of-vocabulary item.",
    "start": "668190",
    "end": "672127"
  },
  {
    "text": "The result of all this is\nthat these models can get away",
    "start": "672127",
    "end": "674460"
  },
  {
    "text": "with having very\nsmall vocabularies,",
    "start": "674460",
    "end": "676590"
  },
  {
    "text": "precisely because we are\nrelying on them implicitly",
    "start": "676590",
    "end": "680370"
  },
  {
    "text": "to be truly contextual.",
    "start": "680370",
    "end": "684290"
  },
  {
    "start": "684000",
    "end": "745000"
  },
  {
    "text": "Here's another inspiring\nidea that we've not",
    "start": "684290",
    "end": "686347"
  },
  {
    "text": "encountered before.",
    "start": "686348",
    "end": "687140"
  },
  {
    "text": "This is called\npositional encoding.",
    "start": "687140",
    "end": "689570"
  },
  {
    "text": "And it's another way in which\nwe can capture sensitivity",
    "start": "689570",
    "end": "692480"
  },
  {
    "text": "of words to their contexts.",
    "start": "692480",
    "end": "694589"
  },
  {
    "text": "So as you'll see, when\nyou go all the way down",
    "start": "694590",
    "end": "696530"
  },
  {
    "text": "inside the transformer\narchitecture,",
    "start": "696530",
    "end": "698270"
  },
  {
    "text": "you do have a traditional\nstatic embedding",
    "start": "698270",
    "end": "700790"
  },
  {
    "text": "of the sort we discussed in\nthe first unit for this course.",
    "start": "700790",
    "end": "703800"
  },
  {
    "text": "Those are in light gray\nhere, fixed representations",
    "start": "703800",
    "end": "706310"
  },
  {
    "text": "for the words.",
    "start": "706310",
    "end": "707540"
  },
  {
    "text": "However, in the context\nof a model like BERT,",
    "start": "707540",
    "end": "710420"
  },
  {
    "text": "what we traditionally think of\nas its embedding representation",
    "start": "710420",
    "end": "714230"
  },
  {
    "text": "is actually a combination\nof that fixed embedding",
    "start": "714230",
    "end": "717589"
  },
  {
    "text": "and a separate\nembedding space called",
    "start": "717590",
    "end": "719330"
  },
  {
    "text": "the positional embedding, where\nwe have learned representations",
    "start": "719330",
    "end": "723080"
  },
  {
    "text": "for each position in a sequence.",
    "start": "723080",
    "end": "725870"
  },
  {
    "text": "This has the intriguing property\nthat one and the same word,",
    "start": "725870",
    "end": "728540"
  },
  {
    "text": "like the, will have\na different embedding",
    "start": "728540",
    "end": "731029"
  },
  {
    "text": "in the sense of the green\nrepresentation here,",
    "start": "731030",
    "end": "733370"
  },
  {
    "text": "depending on where it\nappears in your sequence.",
    "start": "733370",
    "end": "735930"
  },
  {
    "text": "So right from the\nget-go, we have a notion",
    "start": "735930",
    "end": "738500"
  },
  {
    "text": "of context sensitivity\neven before we've",
    "start": "738500",
    "end": "740960"
  },
  {
    "text": "started to connect things in\nall sorts of interesting ways.",
    "start": "740960",
    "end": "745720"
  },
  {
    "start": "745000",
    "end": "933000"
  },
  {
    "text": "Now let's move to some\ncurrent issues and efforts,",
    "start": "745720",
    "end": "747939"
  },
  {
    "text": "some high-level things\nthat you might think about",
    "start": "747940",
    "end": "749982"
  },
  {
    "text": "as you work through this unit.",
    "start": "749982",
    "end": "751570"
  },
  {
    "text": "This is a really nice\ngraph from the ELECTRA",
    "start": "751570",
    "end": "753970"
  },
  {
    "text": "paper from Clark, et al.",
    "start": "753970",
    "end": "755949"
  },
  {
    "text": "Along the x-axis here, we have\nfloating point operations,",
    "start": "755950",
    "end": "758770"
  },
  {
    "text": "which you could think of\nas a kind of basic measure",
    "start": "758770",
    "end": "761290"
  },
  {
    "text": "of compute resources needed to\ncreate these representations.",
    "start": "761290",
    "end": "764860"
  },
  {
    "text": "And along the y-axis,\nwe have GLUE score.",
    "start": "764860",
    "end": "767019"
  },
  {
    "text": "So that's like a\nstandard NLU benchmark.",
    "start": "767020",
    "end": "770110"
  },
  {
    "text": "And the point of\nthis plot here is",
    "start": "770110",
    "end": "771730"
  },
  {
    "text": "that we're reaching kind\nof diminishing returns.",
    "start": "771730",
    "end": "773970"
  },
  {
    "text": "So we had rapid increases\nfrom GloVE, GPT,",
    "start": "773970",
    "end": "777160"
  },
  {
    "text": "and up through to BERT,\nwhere we're really",
    "start": "777160",
    "end": "779920"
  },
  {
    "text": "doing much better on\nthese GLUE scores.",
    "start": "779920",
    "end": "782732"
  },
  {
    "text": "We're increasing the\nfloating point operations.",
    "start": "782732",
    "end": "784690"
  },
  {
    "text": "But it seems to be\ncommensurate with how",
    "start": "784690",
    "end": "786400"
  },
  {
    "text": "we're doing on the benchmark.",
    "start": "786400",
    "end": "788110"
  },
  {
    "text": "But now with these larger\nmodels like XLNet and RoBERTa,",
    "start": "788110",
    "end": "791230"
  },
  {
    "text": "it's arguably the\ncase that we're",
    "start": "791230",
    "end": "793000"
  },
  {
    "text": "reaching diminishing returns.",
    "start": "793000",
    "end": "795010"
  },
  {
    "text": "RoBERTa involves more than\n3,000 times the floating point",
    "start": "795010",
    "end": "798700"
  },
  {
    "text": "operations of GloVe.",
    "start": "798700",
    "end": "801010"
  },
  {
    "text": "But it's not that much\nbetter along this y-axis",
    "start": "801010",
    "end": "804940"
  },
  {
    "text": "than some of its simpler\nvariants like BERT-Base.",
    "start": "804940",
    "end": "807340"
  },
  {
    "text": "And so this is something\nwe should think about",
    "start": "807340",
    "end": "809257"
  },
  {
    "text": "when we think about the\ncosts in terms of money,",
    "start": "809257",
    "end": "811960"
  },
  {
    "text": "and in the environments,\nand energy,",
    "start": "811960",
    "end": "814120"
  },
  {
    "text": "and so forth when we think about\ndeveloping these large models.",
    "start": "814120",
    "end": "818339"
  },
  {
    "text": "And here's a really\nextreme case.",
    "start": "818340",
    "end": "820030"
  },
  {
    "text": "Who knows how long we\ncan train these things",
    "start": "820030",
    "end": "822400"
  },
  {
    "text": "or how much benefit\nwe'll get when we do so.",
    "start": "822400",
    "end": "825020"
  },
  {
    "text": "But at a certain point,\nwe're likely to incur",
    "start": "825020",
    "end": "827080"
  },
  {
    "text": "costs that are\nlarger than any gains",
    "start": "827080",
    "end": "829570"
  },
  {
    "text": "that we can justify on the\nproblems we're trying to solve.",
    "start": "829570",
    "end": "832780"
  },
  {
    "text": "And that goes and leads\nus to this lovely paper,",
    "start": "832780",
    "end": "834850"
  },
  {
    "text": "which talks about the\nenvironmental footprint",
    "start": "834850",
    "end": "837310"
  },
  {
    "text": "of training these\nreally big models.",
    "start": "837310",
    "end": "838990"
  },
  {
    "text": "And it just shows that by\ntraining a big transformer",
    "start": "838990",
    "end": "841720"
  },
  {
    "text": "from scratch, it really incurs\na large environmental cost.",
    "start": "841720",
    "end": "845560"
  },
  {
    "text": "That's certainly\nsomething we should",
    "start": "845560",
    "end": "847060"
  },
  {
    "text": "have in mind as we think\nabout using these models.",
    "start": "847060",
    "end": "849250"
  },
  {
    "text": "For me, it's a complicated\nquestion though.",
    "start": "849250",
    "end": "851360"
  },
  {
    "text": "Because it's offset by the fact\nthat, by and large, all of us",
    "start": "851360",
    "end": "854709"
  },
  {
    "text": "aren't training these from\nscratch, but rather, benefiting",
    "start": "854710",
    "end": "857860"
  },
  {
    "text": "from publicly available\npretrained representations.",
    "start": "857860",
    "end": "861579"
  },
  {
    "text": "So while the pretraining\nfor that one version",
    "start": "861580",
    "end": "864130"
  },
  {
    "text": "had a large\nenvironmental cost, it",
    "start": "864130",
    "end": "866320"
  },
  {
    "text": "feels like it's kind of offset\nby the fact that a lot of us",
    "start": "866320",
    "end": "869020"
  },
  {
    "text": "are benefiting from it.",
    "start": "869020",
    "end": "870080"
  },
  {
    "text": "And it might be that\nin aggregate, this",
    "start": "870080",
    "end": "872350"
  },
  {
    "text": "is less environmentally\ncostly than the old days",
    "start": "872350",
    "end": "875589"
  },
  {
    "text": "when all of us always\ntrained all of our models",
    "start": "875590",
    "end": "878080"
  },
  {
    "text": "literally from scratch.",
    "start": "878080",
    "end": "879620"
  },
  {
    "text": "I just don't know how to\ndo the calculations here.",
    "start": "879620",
    "end": "881710"
  },
  {
    "text": "But I do know that increased\naccess has been empowering",
    "start": "881710",
    "end": "885190"
  },
  {
    "text": "and is likely offsetting\nsome of the costs.",
    "start": "885190",
    "end": "887170"
  },
  {
    "text": "And a lot of that is due to the\ncontributions of the Hugging",
    "start": "887170",
    "end": "890018"
  },
  {
    "text": "Face library.",
    "start": "890018",
    "end": "890560"
  },
  {
    "start": "890560",
    "end": "893028"
  },
  {
    "text": "There are a lot of efforts\nalong these same lines",
    "start": "893028",
    "end": "895070"
  },
  {
    "text": "to make BERT smaller\nby compressing it",
    "start": "895070",
    "end": "897260"
  },
  {
    "text": "literally fewer dimensions than\nother kinds of simplifications",
    "start": "897260",
    "end": "900590"
  },
  {
    "text": "of the training process,\nand BERT distillation,",
    "start": "900590",
    "end": "902930"
  },
  {
    "text": "and so forth.",
    "start": "902930",
    "end": "904080"
  },
  {
    "text": "Here are two outstanding\ncontributions,",
    "start": "904080",
    "end": "906230"
  },
  {
    "text": "kind of compendiums of lots of\ndifferent ideas in this space.",
    "start": "906230",
    "end": "909440"
  },
  {
    "text": "And I also highly\nrecommend this lovely paper",
    "start": "909440",
    "end": "911840"
  },
  {
    "text": "called \"A Primer in\nBERTology,\" which",
    "start": "911840",
    "end": "914210"
  },
  {
    "text": "explores a lot of\ndifferent aspects of what",
    "start": "914210",
    "end": "916580"
  },
  {
    "text": "we know about BERT and how\nit works, various variations",
    "start": "916580",
    "end": "920510"
  },
  {
    "text": "people have tried,\nvarious things",
    "start": "920510",
    "end": "922187"
  },
  {
    "text": "that people have done\nto probe these models,",
    "start": "922187",
    "end": "924020"
  },
  {
    "text": "and understand their\nlearning dynamics,",
    "start": "924020",
    "end": "925670"
  },
  {
    "text": "and so forth and so on.",
    "start": "925670",
    "end": "926779"
  },
  {
    "text": "It's a very rich\ncontribution, certainly can",
    "start": "926780",
    "end": "929750"
  },
  {
    "text": "be a resource for you as you\nthink about these models.",
    "start": "929750",
    "end": "934330"
  },
  {
    "start": "933000",
    "end": "1040000"
  },
  {
    "text": "And just because we don't\nhave time to cover them all,",
    "start": "934330",
    "end": "938152"
  },
  {
    "text": "there are a bunch of\ninteresting Transformer variants",
    "start": "938152",
    "end": "940360"
  },
  {
    "text": "that we will not be able\nto discuss in detail.",
    "start": "940360",
    "end": "942717"
  },
  {
    "text": "I thought I'd mention them here.",
    "start": "942717",
    "end": "944050"
  },
  {
    "text": "SBERT is an attempt to develop\nsentence-level representations",
    "start": "944050",
    "end": "947200"
  },
  {
    "text": "from BERT that are particularly\ngood at finding which sentences",
    "start": "947200",
    "end": "950620"
  },
  {
    "text": "are similar to which\nother sentences according",
    "start": "950620",
    "end": "952810"
  },
  {
    "text": "to cosine similarity.",
    "start": "952810",
    "end": "954100"
  },
  {
    "text": "And I think that could\nbe a powerful mode",
    "start": "954100",
    "end": "955899"
  },
  {
    "text": "of thinking about\nthese representations",
    "start": "955900",
    "end": "958120"
  },
  {
    "text": "and also of practical\nutility if you",
    "start": "958120",
    "end": "959920"
  },
  {
    "text": "need to find which sentences\nare similar to which others.",
    "start": "959920",
    "end": "963940"
  },
  {
    "text": "You have probably heard of\nGPT, the Generative Pre-trained",
    "start": "963940",
    "end": "967330"
  },
  {
    "text": "Transformer, in\nvarious of its forms.",
    "start": "967330",
    "end": "970130"
  },
  {
    "text": "You can get GPT-2\nfrom Hugging Space--",
    "start": "970130",
    "end": "972652"
  },
  {
    "text": "and Hugging Face.",
    "start": "972652",
    "end": "973360"
  },
  {
    "text": "And you have unfettered\naccess to it.",
    "start": "973360",
    "end": "975279"
  },
  {
    "text": "And of course, there's\nmore restrictive access",
    "start": "975280",
    "end": "977320"
  },
  {
    "text": "at this point to GPT-3.",
    "start": "977320",
    "end": "979360"
  },
  {
    "text": "These are conditional\nlanguage models,",
    "start": "979360",
    "end": "981130"
  },
  {
    "text": "so quite different from BERT.",
    "start": "981130",
    "end": "982600"
  },
  {
    "text": "And they might be\nbetter than BERT",
    "start": "982600",
    "end": "984069"
  },
  {
    "text": "for things like truly\nconditional language",
    "start": "984070",
    "end": "986830"
  },
  {
    "text": "generation.",
    "start": "986830",
    "end": "988770"
  },
  {
    "text": "XLNet is an attempt to\nbring in much more context",
    "start": "988770",
    "end": "991710"
  },
  {
    "text": "into these models.",
    "start": "991710",
    "end": "993270"
  },
  {
    "text": "It stands for Xtra\nLong Transformer.",
    "start": "993270",
    "end": "995480"
  },
  {
    "text": "So if you need to\nprocess long sequences,",
    "start": "995480",
    "end": "997350"
  },
  {
    "text": "this might be a good choice.",
    "start": "997350",
    "end": "998670"
  },
  {
    "text": "And this is also\nan attempt to bring",
    "start": "998670",
    "end": "1000170"
  },
  {
    "text": "in some of the benefits of\nconditional language models",
    "start": "1000170",
    "end": "1003110"
  },
  {
    "text": "into a mode that is more\nbidirectional, the way BERT is.",
    "start": "1003110",
    "end": "1007071"
  },
  {
    "text": "T5 is another conditional\nlanguage mode, as is BART.",
    "start": "1007072",
    "end": "1010660"
  },
  {
    "text": "These models might be\nbetter choices for you",
    "start": "1010660",
    "end": "1012550"
  },
  {
    "text": "if you need to actually\ngenerate language.",
    "start": "1012550",
    "end": "1014300"
  },
  {
    "text": "What I think the\nstandard wisdom is",
    "start": "1014300",
    "end": "1016240"
  },
  {
    "text": "that models like\nBERT and RoBERTa",
    "start": "1016240",
    "end": "1018279"
  },
  {
    "text": "are better if you simply\nneed good representations",
    "start": "1018280",
    "end": "1021060"
  },
  {
    "text": "for fine-tuning on a\nclassification problem,",
    "start": "1021060",
    "end": "1023320"
  },
  {
    "text": "for example.",
    "start": "1023320",
    "end": "1025270"
  },
  {
    "text": "More models will\nappear every day.",
    "start": "1025270",
    "end": "1027189"
  },
  {
    "text": "And I think it's worth\ntrying to stay up",
    "start": "1027190",
    "end": "1029079"
  },
  {
    "text": "to speed on the various\ndevelopments in this space.",
    "start": "1029079",
    "end": "1031689"
  },
  {
    "text": "Because this is probably just\nthe tip of the iceberg here.",
    "start": "1031690",
    "end": "1035939"
  },
  {
    "start": "1035940",
    "end": "1040000"
  }
]