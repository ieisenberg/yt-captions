[
  {
    "start": "0",
    "end": "5700"
  },
  {
    "text": "So last time we talked about\nthe generalization bounds. And today we are going\nto talk about some better",
    "start": "5700",
    "end": "11940"
  },
  {
    "text": "generalization bound\nfor deep networks. So recall that last\ntime what we did was that we show something\nlike the Rademacher complexity",
    "start": "11940",
    "end": "20760"
  },
  {
    "text": "is bonded by\nsomething like this,",
    "start": "20760",
    "end": "34350"
  },
  {
    "text": "times some polynomial of the\nnorms of the width, right?",
    "start": "34350",
    "end": "41010"
  },
  {
    "start": "41010",
    "end": "47500"
  },
  {
    "text": "And we said that this come from\na kind of a worst case bound for the Lipschitz-ness\nof the model.",
    "start": "47500",
    "end": "61235"
  },
  {
    "start": "61235",
    "end": "67230"
  },
  {
    "text": "So and actually this is the\nworst case Lipschitz-ness with respect to input\nover the entire--",
    "start": "67230",
    "end": "79350"
  },
  {
    "text": "worst case over the\nentire input space. ",
    "start": "79350",
    "end": "84645"
  },
  {
    "text": "And this is because when\nwe do the covering number, we have to use this Lipschitz\ndecomposition-- this Lipschitz compensation lemma.",
    "start": "84645",
    "end": "90610"
  },
  {
    "text": "And there you have to\nuse the Lipschitz-ness",
    "start": "90610",
    "end": "96880"
  },
  {
    "text": "for the entire set.  Sorry, this is a little bit\ndistracting in the light,",
    "start": "96880",
    "end": "105170"
  },
  {
    "text": "just because I'm sharing the\nscreen using my laptop so that I can charge my iPad. OK.",
    "start": "105170",
    "end": "111450"
  },
  {
    "text": "So and we have discussed\na few motivations for us to improve upon this theorem.",
    "start": "111450",
    "end": "117719"
  },
  {
    "text": "So I guess we\ndiscussed four of them. And one of them I, guess I'll\njust briefly mention them. And what is one of them is\nthat this boundless exponential",
    "start": "117720",
    "end": "130119"
  },
  {
    "text": "in depth, which is bad. Because typically you\nhave a lot of players.",
    "start": "130120",
    "end": "135890"
  },
  {
    "text": "And another thing is that this\nis worst case Lipschitz-ness. And another thing is\nthat typically you",
    "start": "135890",
    "end": "142730"
  },
  {
    "text": "want to have something like\nSGD prefers Lipschitz models.",
    "start": "142730",
    "end": "150500"
  },
  {
    "text": " That's good. But this is Lipschitz models.",
    "start": "150500",
    "end": "156730"
  },
  {
    "text": "And where Lipschitz is\non the empirical data.",
    "start": "156730",
    "end": "161970"
  },
  {
    "text": "Because if you think\nabout an algorithm, and an algorithm can\nonly do something",
    "start": "161970",
    "end": "167902"
  },
  {
    "text": "on the empirical data, right? So we'll show this more\nlater in the course.",
    "start": "167902",
    "end": "173250"
  },
  {
    "text": "But even if think about it,\nso on a high level, right? So the algorithm can only prefer\nsomething about empirical data,",
    "start": "173250",
    "end": "179790"
  },
  {
    "text": "but not about the\nentire space, right? And also, we said\nthat for tighter bond,",
    "start": "179790",
    "end": "186720"
  },
  {
    "text": "we are going to have\nsomething of data dependent, something that depends\non the Lipschitz-ness on the empirical data.",
    "start": "186720",
    "end": "192400"
  },
  {
    "text": "So concretely, I\nguess today we're going to do is that we are\ngoing to show something like. This the generalization\nof parameter theta",
    "start": "192400",
    "end": "200730"
  },
  {
    "text": "is a function of\nLipschitz-ness of f theta",
    "start": "200730",
    "end": "210450"
  },
  {
    "text": "on the empirical data x1 up to\nxn, and also the norm of theta.",
    "start": "210450",
    "end": "219720"
  },
  {
    "text": "And its function\nis a polynomial. So that there's no\nexponential dependency.",
    "start": "219720",
    "end": "225420"
  },
  {
    "text": "There is no [INAUDIBLE] So that's the goal\nof this lecture.",
    "start": "225420",
    "end": "232990"
  },
  {
    "text": "And we will call it-- so and we\nhave to define some kind of--",
    "start": "232990",
    "end": "239850"
  },
  {
    "text": "or introduce some new\nmachinery to achieve these kind of things. And the reason is that this\nis a different type of bond",
    "start": "239850",
    "end": "245160"
  },
  {
    "text": "than what we have done before. Because you can see that\non the right-hand side, you have a function\nof the training data.",
    "start": "245160",
    "end": "252390"
  },
  {
    "text": "So typically on the\nright-hand side, right, so the so-called classical\nuniform convergence,",
    "start": "252390",
    "end": "260260"
  },
  {
    "text": "I guess what really uniform\nconvergence really mean? That's slightly\nkind of debatable. Because it depends\non how you scope it.",
    "start": "260260",
    "end": "266620"
  },
  {
    "text": "But at least what we have\ndiscussed in this lecture, all the bounds are doing\nsomething like this, right? So the bounds before\nwere all looked",
    "start": "266620",
    "end": "274300"
  },
  {
    "text": "like for every f in\nsome hypothesis class f,",
    "start": "274300",
    "end": "280650"
  },
  {
    "text": "the empirical loss is less than\nsomething like a complexity measure of capital F\nover square root of n,",
    "start": "280650",
    "end": "288680"
  },
  {
    "text": "something like this, right? So or maybe with\nhigh probability.",
    "start": "288680",
    "end": "295260"
  },
  {
    "text": "Something like this. And or, alternatively,\nwe can also achieve these kind of things.",
    "start": "295260",
    "end": "300580"
  },
  {
    "text": "I think we-- implicitly\nwe discussed this, right? So for every f, L of f is\nless than a complexity measure",
    "start": "300580",
    "end": "308040"
  },
  {
    "text": "of little f over\nsquare root of n. So here this is capital F.",
    "start": "308040",
    "end": "316730"
  },
  {
    "text": "So the first type is what we\ndo exactly, what we got exactly from Rademacher complexity.",
    "start": "316730",
    "end": "321950"
  },
  {
    "text": "Because you just apply\nRademacher complexity on it. And this is in some sense\nthe Rademacher complexity.",
    "start": "321950",
    "end": "328400"
  },
  {
    "text": "And the second\ntype, you can also get it by doing a little bit\nthings from the first type. So you can get the second\ntype by something like--",
    "start": "328400",
    "end": "341530"
  },
  {
    "text": "I guess this is a remark-- by considering F to be\nsomething like all the functions",
    "start": "341530",
    "end": "348250"
  },
  {
    "text": "where the complexity of little\nf is less than capital C. Think of the\ncomplexity as focused",
    "start": "348250",
    "end": "354400"
  },
  {
    "text": "on the norm of the width. You first define\na hypothesis class where the norm of the weights\nis less than capital C.",
    "start": "354400",
    "end": "360640"
  },
  {
    "text": "And then you apply 1\non all, on capital F",
    "start": "360640",
    "end": "369260"
  },
  {
    "text": "on this hypothesis class. And then you do a\nunion bound, and then take a union bound\non all C, right?",
    "start": "369260",
    "end": "383120"
  },
  {
    "text": "So for every capital C, it\ndefines a hypothesis class. And you probably can write\nit as capital F sub C. So",
    "start": "383120",
    "end": "389900"
  },
  {
    "text": "and for this\ncapital F sub C, you can do the standard\nRademacher complexity. And then you can say, I'm\ngoing to enumerate over",
    "start": "389900",
    "end": "396680"
  },
  {
    "text": "all possible capital C and\nthen do another layer of union bound on top of it. We never do this formally.",
    "start": "396680",
    "end": "401960"
  },
  {
    "text": "But this is just one\nparameter you can just discretize whatever you want. So in some sense, this is how\nyou get the type I type II",
    "start": "401960",
    "end": "408919"
  },
  {
    "text": "bond. But the thing is that\neither of this bound, on the right-hand side the bound\ndepends on the empirical data.",
    "start": "408920",
    "end": "417210"
  },
  {
    "text": "It's always a property\neither of the model or of the function class. So the question is, how do you--",
    "start": "417210",
    "end": "424580"
  },
  {
    "text": "if you want to get something\nlike this, like our goal today, you have to\ndo something more--",
    "start": "424580",
    "end": "431629"
  },
  {
    "text": "you have to introduce some\nnew techniques, right? So our goal is to\nget something like--",
    "start": "431630",
    "end": "437360"
  },
  {
    "text": "I think maybe let's call this-- I think we call this\ndata-dependent bound, generalization bound. ",
    "start": "437360",
    "end": "445210"
  },
  {
    "text": "This term might be a little\nbit overused in certain cases. But what I mean here\nis that you want",
    "start": "445210",
    "end": "451750"
  },
  {
    "text": "to have a bound that with\nhigh probability for every f, your population loss is less\nthan some maybe complexity of f",
    "start": "451750",
    "end": "462480"
  },
  {
    "text": "and the empirical data. ",
    "start": "462480",
    "end": "468742"
  },
  {
    "text": "So the right-hand side is\nalso a random variable that depends on the empirical data. Of course, you're asking\nthis for high probability",
    "start": "468742",
    "end": "476350"
  },
  {
    "text": "anyway, right? So you're asking that for all-- with high probability over the\nchoice of the empirical data,",
    "start": "476350",
    "end": "482830"
  },
  {
    "text": "this inequality is true.  And this is still\nuseful in the sense",
    "start": "482830",
    "end": "491590"
  },
  {
    "text": "that you can regularize\nthe right-hand side. You can add the RHS\nas a regularizer.",
    "start": "491590",
    "end": "503960"
  },
  {
    "text": " So not only this is an\nexplanation in some sense,",
    "start": "503960",
    "end": "509980"
  },
  {
    "text": "but also it can be used\nactively as a regularizer. Because the right-hand side\nis something you can optimize,",
    "start": "509980",
    "end": "515630"
  },
  {
    "text": "right?  So this is the goal that\nwe are trying to achieve.",
    "start": "515630",
    "end": "523418"
  },
  {
    "text": "So and in some\nsense, I think I used to have a little argument\nabout why this is actually",
    "start": "523419",
    "end": "529390"
  },
  {
    "text": "the right thing to do. It's kind of tricky,\nbecause these days, still there is no\nconsensus on what exactly",
    "start": "529390",
    "end": "535509"
  },
  {
    "text": "kind of generalization\nbound you are looking for. I believe that this is one\nthing that is good to have. But there could be other forms\nof generalization bounds.",
    "start": "535510",
    "end": "544420"
  },
  {
    "text": "In some sense, you can argue\nthat this is the best you can achieve in the sense that you\ncannot have a stronger one",
    "start": "544420",
    "end": "552312"
  },
  {
    "text": "on the right-hand side. Because, for example, you cannot\nreplace this empirical data",
    "start": "552312",
    "end": "558100"
  },
  {
    "text": "by population\ndistribution, right? If you replace that, then\nyou can just choose-- suppose you allow\nthe complex measure",
    "start": "558100",
    "end": "564190"
  },
  {
    "text": "to depend on a\npopulation distribution. Suppose you allow\nthat I can have complexity of f and the\npopulation distribution p.",
    "start": "564190",
    "end": "572690"
  },
  {
    "text": "Then why not just define\nthis to be lp of f? Sorry. Why not define this to\nbe the population risk?",
    "start": "572690",
    "end": "580550"
  },
  {
    "text": "What if you allow this,\nwhy not just define it to be something\nlike x from p fx?",
    "start": "580550",
    "end": "588810"
  },
  {
    "text": "So the population risk would\nbe a good complex measure. Then in some sense, you lose\nthe gist here in some sense.",
    "start": "588810",
    "end": "597029"
  },
  {
    "text": "It becomes too trivial. And in some sense,\nthat suggests that you are cheating in some sense by\nallowing the complex measure",
    "start": "597030",
    "end": "602519"
  },
  {
    "text": "to depend on p. So in some sense, the\nfundamental question we are facing about this\nin the generalization bound",
    "start": "602520",
    "end": "609930"
  },
  {
    "text": "is that you don't have access\nto the population distribution. You want to have an empirical\nmeasure for complexity.",
    "start": "609930",
    "end": "616779"
  },
  {
    "text": "So that you can use\nthat for regularization. Anyway, but this argument\nis kind of anyway debatable.",
    "start": "616780",
    "end": "624630"
  },
  {
    "text": "So for now, we're just\nsaying that this is one of the reasonable goals, right? So and why doing\nthis is challenging?",
    "start": "624630",
    "end": "634720"
  },
  {
    "text": "I think the first thing is that\nthis is challenging because you cannot do the simple reduction\nas we have done before.",
    "start": "634720",
    "end": "642540"
  },
  {
    "text": "So the reduction between I and\nII, type I and type II bounds",
    "start": "642540",
    "end": "653269"
  },
  {
    "text": "doesn't work anymore. ",
    "start": "653270",
    "end": "659389"
  },
  {
    "text": "And, why? This is because\nlet's give a try. For example, let's\ndefine capital F to be all the little f such\nthat the complexity of f x.",
    "start": "659390",
    "end": "673204"
  },
  {
    "start": "673205",
    "end": "679110"
  },
  {
    "text": "Suppose you say\nthis is less than c. ",
    "start": "679110",
    "end": "684420"
  },
  {
    "text": "Suppose you define this, right? This is your hypothesis class. And let's say, suppose\nwe attempt to use--",
    "start": "684420",
    "end": "691399"
  },
  {
    "text": "attempt is that you use f\nwith Rademacher complexity",
    "start": "691400",
    "end": "697390"
  },
  {
    "text": "for capital F. What's the issue? Why we cannot do this? ",
    "start": "697390",
    "end": "705420"
  },
  {
    "text": "The reason is that if your\ncomplex nature depends on the data, then your\nrandom hypothesis class also",
    "start": "705420",
    "end": "711890"
  },
  {
    "text": "depends on data. Before your complex\nmight depend on data, your hypothesis class is just\na fixed hypothesis class.",
    "start": "711890",
    "end": "718590"
  },
  {
    "text": "So now, it's a hypothesis\nclass that depends on data. So f is also a random\nvariable depending on data.",
    "start": "718590",
    "end": "728555"
  },
  {
    "text": " Data means empirical data.",
    "start": "728555",
    "end": "734259"
  },
  {
    "text": "And then you can use the\nRademacher complexity. The theorem for\nRademacher complexity,",
    "start": "734260",
    "end": "740050"
  },
  {
    "text": "for why the Rademacher\ncomplexity bonds the generalization\nerror, that theorem requires the capital F\nto be a fixed hypothesis",
    "start": "740050",
    "end": "747579"
  },
  {
    "text": "class that is fixed before\nyou draw the random data.",
    "start": "747580",
    "end": "753190"
  },
  {
    "text": "So that's the challenge. OK? ",
    "start": "753190",
    "end": "760580"
  },
  {
    "text": "And how do we address this? So in some sense, the way\nto-- the high level way",
    "start": "760580",
    "end": "767510"
  },
  {
    "text": "to address it is\nto redefine, or you have to have a\nrefined way to think",
    "start": "767510",
    "end": "773960"
  },
  {
    "text": "about uniform convergence. So some refined\nuniform convergence.",
    "start": "773960",
    "end": "783300"
  },
  {
    "text": "This is not going to be\nexactly what we do eventually. Because what we do eventually\nwill be something very clean",
    "start": "783300",
    "end": "790339"
  },
  {
    "text": "and doesn't have any\nkind of a subtlety. But this is the rough thinking\nhow do you think about it.",
    "start": "790340",
    "end": "799079"
  },
  {
    "text": "So maybe let's make assumption. This assumption, suppose\nthe complexity measure",
    "start": "799080",
    "end": "808750"
  },
  {
    "text": "is separable in the sense\nthat this complexity of f",
    "start": "808750",
    "end": "819520"
  },
  {
    "text": "on the empirical example is of\nsome form like g of fxi, right?",
    "start": "819520",
    "end": "830420"
  },
  {
    "text": "It's really some\nfunction of f and xi and you take the sum of them. So suppose in this special\ncase, then you can think of--",
    "start": "830420",
    "end": "838310"
  },
  {
    "text": "essentially what we are doing\nis that we are considering-- then we can consider\nthe augmented loss.",
    "start": "838310",
    "end": "848980"
  },
  {
    "text": " So you can define\nsomething like l",
    "start": "848980",
    "end": "855250"
  },
  {
    "text": "tilde f is equal to something\nlike lf times the indicator that this complexity\nis less than c.",
    "start": "855250",
    "end": "865002"
  },
  {
    "text": "So in some sense, what\nyou are doing here is that you are changing the\nloss function in some way",
    "start": "865002",
    "end": "870520"
  },
  {
    "text": "so that it's easier for you\nto use the existing bound. So before, for\nexample, let's say,",
    "start": "870520",
    "end": "876580"
  },
  {
    "text": "the mental picture\nI have in mind is something like you have\na loss function, which",
    "start": "876580",
    "end": "883959"
  },
  {
    "text": "is something like this. Maybe let's say this\nis the empirical loss.",
    "start": "883960",
    "end": "890210"
  },
  {
    "text": "And have some region. And this is the region where\nyou have low complexity.",
    "start": "890210",
    "end": "896480"
  },
  {
    "text": " So but this region\nis a random region.",
    "start": "896480",
    "end": "903230"
  },
  {
    "text": "Because the low complexity--\nthe definition of low complexity depends on data. So this is random.",
    "start": "903230",
    "end": "910365"
  },
  {
    "text": "So that's why you cannot use\nthe uniform convergence only on this low complexity\nregion, right? So you cannot say that I'm\nonly going to apply my uniform",
    "start": "910365",
    "end": "918050"
  },
  {
    "text": "convergence for this region. Even though that's your\ngoal, but you cannot apply the Rademacher\ncomplexity theory.",
    "start": "918050",
    "end": "924019"
  },
  {
    "text": "So kind of this augmented\nloss, what is fundamental",
    "start": "924020",
    "end": "929510"
  },
  {
    "text": "is doing something like\nit change the geometry outside the low complex region.",
    "start": "929510",
    "end": "936280"
  },
  {
    "text": "So you, for example, you just\ndefined a new loss function to be 0 here. And then, the same thing as it\nwas in the low complex region.",
    "start": "936280",
    "end": "948740"
  },
  {
    "text": "So now we have a globally\ndefined loss function. And so, basically,\nthe region that you",
    "start": "948740",
    "end": "954700"
  },
  {
    "text": "are taking union bond over,\nright, the hypothesis class is still the same. But you change\nthe loss function.",
    "start": "954700",
    "end": "962100"
  },
  {
    "text": "So if you do this,\nthen you can hope to-- so can hope to apply\nexisting tools on l tilde of f.",
    "start": "962100",
    "end": "975700"
  },
  {
    "text": "And l tilde of f\nis sometimes kind of like a filtering thing that\nfilters the low complexity.",
    "start": "975700",
    "end": "985240"
  },
  {
    "text": "But you don't do it\nin a technical way. Technically, you are just\nchanging the loss function. That's the only thing we do.",
    "start": "985240",
    "end": "990636"
  },
  {
    "text": "But the effect of it\nis the same as you change the hypothesis class.",
    "start": "990637",
    "end": "995670"
  },
  {
    "text": "So I think this is\nthe first thing-- this is the first\nattempt that we have done in all of our paper\nand we try to address this.",
    "start": "995670",
    "end": "1001660"
  },
  {
    "text": "And this is actually the\nfundamental idea in some sense.",
    "start": "1001660",
    "end": "1007927"
  },
  {
    "text": "So you change your loss\nfunction so that you can deal with different\ntype of quantities or different regions\nof the hypothesis.",
    "start": "1007927",
    "end": "1014260"
  },
  {
    "text": "And then later, so this\nis one of the paper we had in, I think, 2019.",
    "start": "1014260",
    "end": "1022260"
  },
  {
    "text": "And we got some results. And if you exactly do\nthis indicator thing where you change\nthe loss like this,",
    "start": "1022260",
    "end": "1027400"
  },
  {
    "text": "you can already get something. But the results are messy. So then we kind of in some\nsense think a bit more broadly,",
    "start": "1027400",
    "end": "1035959"
  },
  {
    "text": "right? So in some sense,\nall this is doing is change the loss\nfunction, right? So you are trying to\nhave a surrogate loss.",
    "start": "1035960",
    "end": "1043180"
  },
  {
    "text": " And the surrogate loss,\nwe are not actually unfamiliar with it.",
    "start": "1043180",
    "end": "1048790"
  },
  {
    "text": "Where we have used the surrogate\nloss in the margin case. But it's just the surrogate\nloss there is the simplest way.",
    "start": "1048790",
    "end": "1054700"
  },
  {
    "text": "The simplest is surrogate loss. So basically, what I'm going\nto talk about today in the main",
    "start": "1054700",
    "end": "1062200"
  },
  {
    "text": "part is this so-called\norder margin, which is a different way of--",
    "start": "1062200",
    "end": "1070015"
  },
  {
    "text": "it's kind of like\na surrogate margin. And once you have this\nkind of a fake margin,",
    "start": "1070015",
    "end": "1077850"
  },
  {
    "text": "this is kind of, in some\nsense defining a new loss function for you. And once you have this\nnew loss function, you can do everything\nin your super clean way.",
    "start": "1077850",
    "end": "1085440"
  },
  {
    "text": "And then you can apply the\nexisting tools in some sense.",
    "start": "1085440",
    "end": "1092600"
  },
  {
    "text": " So this is a sketchy,\na vague introduction.",
    "start": "1092600",
    "end": "1100690"
  },
  {
    "text": "I'm not sure whether there\nare any questions so far? What do you mean\nby all layering?",
    "start": "1100690",
    "end": "1106580"
  },
  {
    "text": "Oh, sorry. This is a-- yeah. This is the name of the thing\nwe are going to introduce. But we are going to\nintroduce a new margin, which",
    "start": "1106580",
    "end": "1112692"
  },
  {
    "text": "we call it all-layer margin. Yeah. I probably should\ndefine that formally.",
    "start": "1112692",
    "end": "1118550"
  },
  {
    "text": "So we [INAUDIBLE]? So basically, the\nmidpoint I'm doing--",
    "start": "1118550",
    "end": "1123820"
  },
  {
    "text": "I'm saying here is\nthat we are going to define a surrogate loss. And using the surrogate loss--",
    "start": "1123820",
    "end": "1129153"
  },
  {
    "text": "the point of the\nsurrogate loss is to change our original\nloss so that you can focus on an important\npart of the space.",
    "start": "1129153",
    "end": "1134350"
  },
  {
    "text": "And the surrogate loss\nwill be basically boring for this high complexity part.",
    "start": "1134350",
    "end": "1140630"
  },
  {
    "text": "But they are just-- they\nare not doing anything. They are basically zero\none loss in some sense.",
    "start": "1140630",
    "end": "1145820"
  },
  {
    "text": "And so, that's the\ngeneral intuition. OK. So now let's see how\ndo we do that exactly.",
    "start": "1145820",
    "end": "1152409"
  },
  {
    "text": "So we're going to start with\na generalization of margin.",
    "start": "1152410",
    "end": "1161860"
  },
  {
    "text": "So let f-- so this is\na classification model.",
    "start": "1161860",
    "end": "1171429"
  },
  {
    "start": "1171430",
    "end": "1177130"
  },
  {
    "text": "So typically, you just\nthreshold f and get 0, 1. And your margin is\njust f itself, right?",
    "start": "1177130",
    "end": "1182260"
  },
  {
    "text": "So the typical\nmargin, the classic-- the standard margin\nis just defined--",
    "start": "1182260",
    "end": "1189990"
  },
  {
    "text": "the standard margin is\njust equals to y times fx.",
    "start": "1189990",
    "end": "1196710"
  },
  {
    "text": "y is between plus or minus 1. That's what we used before. And now, I'm going to define a\nso-called generalized margin.",
    "start": "1196710",
    "end": "1204720"
  },
  {
    "text": " We say gf, xy is a\ngeneralized margin",
    "start": "1204720",
    "end": "1222270"
  },
  {
    "text": "if satisfies the\nfollowing two property.",
    "start": "1222270",
    "end": "1229510"
  },
  {
    "text": "So the first property\nis that gf, xy is 0 if you classify correctly.",
    "start": "1229510",
    "end": "1238470"
  },
  {
    "start": "1238470",
    "end": "1246395"
  },
  {
    "text": "I think I have a typo here. Let me think. ",
    "start": "1246395",
    "end": "1251490"
  },
  {
    "text": "Sorry. I think if you classify wrongly,\nand this will be larger than 0",
    "start": "1251490",
    "end": "1259160"
  },
  {
    "text": "if fxy is classified correctly.",
    "start": "1259160",
    "end": "1265750"
  },
  {
    "text": "So let me mark this. This important typo. ",
    "start": "1265750",
    "end": "1279770"
  },
  {
    "text": "So and you can see\nthat this is trying to imitate the standard margin. For the standard\nmargin is bigger than 0",
    "start": "1279770",
    "end": "1285830"
  },
  {
    "text": "if you classify correctly. And otherwise, you\nsay you zero it out. So that's, in\nexternal margin, also",
    "start": "1285830",
    "end": "1293150"
  },
  {
    "text": "this is only defined for\ncorrect classification.",
    "start": "1293150",
    "end": "1301090"
  },
  {
    "text": "So in some sense, you can extend\nto incorrect classification just by extending it to 0.",
    "start": "1301090",
    "end": "1306330"
  },
  {
    "text": "And so, and we say that-- and there's another small thing.",
    "start": "1306330",
    "end": "1313510"
  },
  {
    "text": "Which is that we have to define\nthe so-called infinite covering",
    "start": "1313510",
    "end": "1319240"
  },
  {
    "text": "number. ",
    "start": "1319240",
    "end": "1324520"
  },
  {
    "text": "So this is defined to be l\ninfinity epsilon f is the--",
    "start": "1324520",
    "end": "1333580"
  },
  {
    "text": "this is a small\ntechnical extension of the l2 covering number. It's not that important\nin most of the cases.",
    "start": "1333580",
    "end": "1339950"
  },
  {
    "text": "It just makes in some sense-- in some cases, it makes\nthe definition cleaner.",
    "start": "1339950",
    "end": "1346070"
  },
  {
    "text": "And in some cases, it makes\nthe proof a little bit easier. So l infinity covering number\nis the minimum covering size",
    "start": "1346070",
    "end": "1359330"
  },
  {
    "text": "with respect to the matrix\nrho, where rho is defined",
    "start": "1359330",
    "end": "1366070"
  },
  {
    "text": "to be this l infinity norm. So basically, you say that--\nyou look at the entire space",
    "start": "1366070",
    "end": "1373510"
  },
  {
    "text": "of the input of f. And you look at a difference\nbetween fx and f prime x,",
    "start": "1373510",
    "end": "1378910"
  },
  {
    "text": "and you take the sup. So basically, this is the f\nminus f prime infinity node.",
    "start": "1378910",
    "end": "1387140"
  },
  {
    "text": "So given these two,\nwhat we will say is",
    "start": "1387140",
    "end": "1392540"
  },
  {
    "text": "that our lemma will be that--",
    "start": "1392540",
    "end": "1397840"
  },
  {
    "text": "with the, you can have\na analogous theory,",
    "start": "1397840",
    "end": "1403029"
  },
  {
    "text": "analogous to the\nmodern theory, where you use this generalized margin\nand also the infinite covering",
    "start": "1403030",
    "end": "1408700"
  },
  {
    "text": "number. Actually, you can\neven do it with l2, like the standard\ncovering number. It's just easier to state with\nthe infinite covering number.",
    "start": "1408700",
    "end": "1416049"
  },
  {
    "text": "And also, maybe\nbefore doing that, let me also have\nanother remark, which is that this infinite\ncovering number is larger",
    "start": "1416050",
    "end": "1425900"
  },
  {
    "text": "than the standard l2 covering.",
    "start": "1425900",
    "end": "1431770"
  },
  {
    "text": "This is just because this is\nthe more demanding notion. Because you are demanding\nthat f and f prime",
    "start": "1431770",
    "end": "1438130"
  },
  {
    "text": "are closed at every\npossible input. And before, you are demanding\nthat f and f prime are",
    "start": "1438130",
    "end": "1443500"
  },
  {
    "text": "closed on the empirical data. So this is because\nthe matrix that we",
    "start": "1443500",
    "end": "1450200"
  },
  {
    "text": "used before was the matrix\nthat is smaller then the matrix",
    "start": "1450200",
    "end": "1457230"
  },
  {
    "text": "used in the infinite case. ",
    "start": "1457230",
    "end": "1476789"
  },
  {
    "text": "So with this small extension,\nwhat we're going to do is that we're going\nto say, actually, you",
    "start": "1476790",
    "end": "1482820"
  },
  {
    "text": "can have analogous margins here\nwith the generalized margin. So the lemma is that, so suppose\ngf is a generalized margin.",
    "start": "1482820",
    "end": "1493470"
  },
  {
    "start": "1493470",
    "end": "1498520"
  },
  {
    "text": "And let this capital G\nto be the family of gf,",
    "start": "1498520",
    "end": "1507478"
  },
  {
    "text": "where f is ranging\nover the capital F. And suppose, recall that\nthis is like what we are--",
    "start": "1507478",
    "end": "1518010"
  },
  {
    "text": "this is in some sense just a\nslightly more complex version of your model hypothesis, right?",
    "start": "1518010",
    "end": "1523410"
  },
  {
    "text": "If you just use yfx,\nthen this will just be y times fx as the hypothesis\nclass, as the class G.",
    "start": "1523410",
    "end": "1529830"
  },
  {
    "text": "And this is a little\nmore general than that. And suppose for some\nR, the covering number,",
    "start": "1529830",
    "end": "1541720"
  },
  {
    "text": "the infinite\ncovering number of G is less than R square\nover epsilon square",
    "start": "1541720",
    "end": "1550720"
  },
  {
    "text": "for epsilon and 0, for\nany epsilon larger than 0. ",
    "start": "1550720",
    "end": "1560880"
  },
  {
    "text": "Suppose you have this 1\nover epsilon square decay in the low covering number. Recall that this is one of\nthe regime that is good.",
    "start": "1560880",
    "end": "1568290"
  },
  {
    "text": "So this is actually\nthe worst regime we can tolerate when we do\nthe Rademacher complex theory.",
    "start": "1568290",
    "end": "1574170"
  },
  {
    "text": "So and suppose you have this. Then with probability\nlarger than 1 minus delta,",
    "start": "1574170",
    "end": "1582665"
  },
  {
    "text": "delta is the\nfailure probability, which will be hidden in the\nlogarithmic over the randomness",
    "start": "1582665",
    "end": "1592760"
  },
  {
    "text": "training data for\nevery f in capital",
    "start": "1592760",
    "end": "1598820"
  },
  {
    "text": "F that correctly predicts\nour training example, right?",
    "start": "1598820",
    "end": "1608784"
  },
  {
    "text": " So for margin, we always--",
    "start": "1608785",
    "end": "1614289"
  },
  {
    "text": "in the margin theory,\nwe always consider functions that can correct it. But if other examples,\nthen you have the 0,",
    "start": "1614290",
    "end": "1621460"
  },
  {
    "text": "1 error is less than of tilde of\n1 over square root of n times 1",
    "start": "1621460",
    "end": "1630840"
  },
  {
    "text": "over the minimum\ngeneralized margin,",
    "start": "1630840",
    "end": "1640220"
  },
  {
    "text": "plus O tilde of 1\nover square root. So recall that basically\nbefore, what we",
    "start": "1640220",
    "end": "1646549"
  },
  {
    "text": "had was-- oh, there's\nan R here, sorry. So before what we\nhad was that here",
    "start": "1646550",
    "end": "1652390"
  },
  {
    "text": "you have the standard\nmargin, the minimum margin",
    "start": "1652390",
    "end": "1657620"
  },
  {
    "text": "over the entire data set. And here, R is the complexity\nof the model hypothesis class,",
    "start": "1657620",
    "end": "1663672"
  },
  {
    "text": "right? And all the other\nthings are the same. Now, the change is that\nnow here you replace it",
    "start": "1663672",
    "end": "1670643"
  },
  {
    "text": "by generalized margin. And R becomes the hypothesis-- the complexity of\nthe hypothesis class",
    "start": "1670643",
    "end": "1676690"
  },
  {
    "text": "of this generalized\nmargin Gf, right? And the complexity is\nmeasured slightly differently. We are using the\ncovering number.",
    "start": "1676690",
    "end": "1682840"
  },
  {
    "text": "But actually, you can also use\nRademacher complexity here. It's the same. I'm just stating it so that\nit's easier for the future part.",
    "start": "1682840",
    "end": "1690490"
  },
  {
    "start": "1690490",
    "end": "1699162"
  },
  {
    "text": "And this bond is\nactually not very tight. You can actually improve\nthis bond in some ways. But this is the\nsimplest version.",
    "start": "1699162",
    "end": "1707970"
  },
  {
    "text": "And the proof of\nthis is basically, it's just we just\nbasically reuse all what we have done\nwith margin theory.",
    "start": "1707970",
    "end": "1714990"
  },
  {
    "text": "It's just everything seems\nto just transfer exactly. So just to replace--\nin some sense,",
    "start": "1714990",
    "end": "1721740"
  },
  {
    "text": "the proof is just replace the\nF by G in the margin theory.",
    "start": "1721740",
    "end": "1731010"
  },
  {
    "text": "I will do this step by step. But this is a short version.",
    "start": "1731010",
    "end": "1736080"
  },
  {
    "text": "So technically what you do\nis still use the ramp loss.",
    "start": "1736080",
    "end": "1741929"
  },
  {
    "text": " Recall that the ramp\nloss was the loss",
    "start": "1741930",
    "end": "1747130"
  },
  {
    "text": "function that looks like this. This is a gamma,\nthis part is gamma,",
    "start": "1747130",
    "end": "1755710"
  },
  {
    "text": "this part is 1,\nsomething like this. And recall that before,\nafter we have this ramp loss,",
    "start": "1755710",
    "end": "1761350"
  },
  {
    "text": "we define this surrogate loss. So we define a surrogate loss\nl hat gamma theta to be--",
    "start": "1761350",
    "end": "1768460"
  },
  {
    "start": "1768460",
    "end": "1775130"
  },
  {
    "text": "before we just\napplied the model. But now we use the\ngeneralized margin. ",
    "start": "1775130",
    "end": "1783820"
  },
  {
    "text": "Before here this\nwas just f theta. But now it becomes\nlike G of f theta-- G sub f theta.",
    "start": "1783820",
    "end": "1788970"
  },
  {
    "text": "And we can also define\nthe surrogate population loss, which is just\nthe expectation",
    "start": "1788970",
    "end": "1794100"
  },
  {
    "text": "of the empirical loss. ",
    "start": "1794100",
    "end": "1805890"
  },
  {
    "text": "So and before what we did is\nthat we use the Rademacher complexity to control the\ndifferences of this true loss",
    "start": "1805890",
    "end": "1812070"
  },
  {
    "text": "function. ",
    "start": "1812070",
    "end": "1822029"
  },
  {
    "text": "We said that you take l gamma\ntheta is minus l hat gamma",
    "start": "1822030",
    "end": "1828240"
  },
  {
    "text": "theta, is less than the\nempirical Rademacher complexity of f. That's what we did before.",
    "start": "1828240",
    "end": "1833883"
  },
  {
    "text": "But now it's the-- sorry. Before we did the empirical\nRademacher complexity of l gamma composed with f.",
    "start": "1833883",
    "end": "1839549"
  },
  {
    "text": "And now it's l gamma\ncomposed with g because the function class-- the function is different.",
    "start": "1839550",
    "end": "1846020"
  },
  {
    "text": "Plus O to the 1 over\nsquare root of 1. [INAUDIBLE]",
    "start": "1846020",
    "end": "1851750"
  },
  {
    "text": "Sorry? [INAUDIBLE] Oh, thank you so much. Yeah, that's a-- thanks.",
    "start": "1851750",
    "end": "1858250"
  },
  {
    "text": "So I would just switch to this. I only have one charger. But yeah.",
    "start": "1858250",
    "end": "1864320"
  },
  {
    "text": " You need another charger?",
    "start": "1864320",
    "end": "1870790"
  },
  {
    "text": "I have one-- No, I think it's the problem\nis that when you use this, I cannot charge. Oh.",
    "start": "1870790",
    "end": "1877810"
  },
  {
    "text": "Right? Oh, but I can-- it doesn't matter how it-- yeah. It's not a charger,\nit's the plug, the hole.",
    "start": "1877810",
    "end": "1885279"
  },
  {
    "text": "Yeah. OK, so now it works? OK, good. Thanks.",
    "start": "1885280",
    "end": "1891270"
  },
  {
    "text": "OK, cool. So now we have to use the\nRademacher complexity.",
    "start": "1891270",
    "end": "1900350"
  },
  {
    "text": "And then, the Rademacher\nmore complexity is less than the\ncovering number, right?",
    "start": "1900350",
    "end": "1908090"
  },
  {
    "text": "So I guess, maybe let's\nstill do the covering number. So covering number, let's\ndo some preparation.",
    "start": "1908090",
    "end": "1916500"
  },
  {
    "text": "So we assume the\ninfinite covering number. But actually, it's, I\nguess, let's say, so",
    "start": "1916500",
    "end": "1925350"
  },
  {
    "text": "the covering number, the\nstandard covering number l gamma composed with\nG, the l2 pn this",
    "start": "1925350",
    "end": "1938200"
  },
  {
    "text": "is less than the\nstandard covering number",
    "start": "1938200",
    "end": "1943500"
  },
  {
    "text": "where you use the-- ",
    "start": "1943500",
    "end": "1948978"
  },
  {
    "text": "by removing the l gamma. So l gamma-- so l2 pn.",
    "start": "1948978",
    "end": "1956380"
  },
  {
    "text": "Because this step is using\nthe Lipschitz-ness of l gamma.",
    "start": "1956380",
    "end": "1965850"
  },
  {
    "text": "So it's actually 1 over\ngamma Lipschitz-ness. So this is using the\nLipschitz-ness of the covering",
    "start": "1965850",
    "end": "1972390"
  },
  {
    "text": "number. And now, next you say\nit is also bounded by the infinitive version.",
    "start": "1972390",
    "end": "1977925"
  },
  {
    "text": " And then, the infinitive\nversion we have an assumption.",
    "start": "1977925",
    "end": "1984080"
  },
  {
    "text": "The assumption was\nthat for every epsilon, this is less than R squared over\nepsilon square, gamma square.",
    "start": "1984080",
    "end": "1992750"
  },
  {
    "text": "The last type is\nthat assumption. So you can see\nthat actually, even",
    "start": "1992750",
    "end": "1998730"
  },
  {
    "text": "suppose you assume something\nabout this, then is also fine. If you assume something about-- ",
    "start": "1998730",
    "end": "2005560"
  },
  {
    "text": "so you don't have to literally\nuse the infinite node. So and then, because\nthis low covering number",
    "start": "2005560",
    "end": "2016040"
  },
  {
    "text": "is less than this, and we\nhave this kind of translation, right, so that if you\ntranslate a log covering number",
    "start": "2016040",
    "end": "2023750"
  },
  {
    "text": "to Rademacher complexity, you\ngot Rs l gamma composed with G.",
    "start": "2023750",
    "end": "2029060"
  },
  {
    "text": "It's less than O tilde of R over\ngamma square root over gamma",
    "start": "2029060",
    "end": "2035205"
  },
  {
    "text": "square root n, right? This is by chaining\nthe [INAUDIBLE]",
    "start": "2035205",
    "end": "2040940"
  },
  {
    "text": "theorem and its consequences. Because we have discussed\nwhat kind of covering numbers",
    "start": "2040940",
    "end": "2046460"
  },
  {
    "text": "to implies, what kind of\nRademacher complexity, right? ",
    "start": "2046460",
    "end": "2051719"
  },
  {
    "text": "So and then, the\nsame thing, I guess,",
    "start": "2051719",
    "end": "2057750"
  },
  {
    "text": "take gamma to be gamma min,\nwhich is the min over i,",
    "start": "2057750",
    "end": "2064750"
  },
  {
    "text": "G i f, and yi, right? So and then, this\nstep is not formed.",
    "start": "2064750",
    "end": "2075699"
  },
  {
    "text": "There are some caveat here. Because gamma is\na random variable, you have to do union\nbound eventually.",
    "start": "2075699",
    "end": "2082060"
  },
  {
    "start": "2082060",
    "end": "2090138"
  },
  {
    "text": "But let me not get into it. I guess we had this\nissue before as well. But it's only one number\nyou can discretize and do",
    "start": "2090139",
    "end": "2096940"
  },
  {
    "text": "union bound over gamma. But suppose, let's say we just\ntake gamma to be gamma min. And then, L hat gamma min is 0.",
    "start": "2096940",
    "end": "2105880"
  },
  {
    "text": "So then you got\nL01 theta, then 0 plus O tilde of R over square\nroot n times gamma min,",
    "start": "2105880",
    "end": "2115710"
  },
  {
    "text": "plus O tilde of 1\nover square root. OK. ",
    "start": "2115710",
    "end": "2122770"
  },
  {
    "text": "So this proof is not 100% formal\njust because the technical-- I am not allowed to take\ngamma to be anything",
    "start": "2122770",
    "end": "2128500"
  },
  {
    "text": "that depends on the data. So I have to really\nshow it for every gamma. And that requires another\nunion bound over gamma.",
    "start": "2128500",
    "end": "2135647"
  },
  {
    "text": "OK. ",
    "start": "2135647",
    "end": "2140820"
  },
  {
    "text": "Any questions?  So maybe, let's see what we\nhave achieved with this lemma.",
    "start": "2140820",
    "end": "2147310"
  },
  {
    "text": "What we achieve\nwith this lemma is that now if you define your-- basically you can put everything\nin this generalized margin.",
    "start": "2147310",
    "end": "2155218"
  },
  {
    "text": "This generalized\nmargin in some sense is a way to twist\nyour model output.",
    "start": "2155218",
    "end": "2160940"
  },
  {
    "text": "So you can stretch the\nmodel output for certain f. And you can squeeze it\nfor certain other f. So in some sense, this is what\nwe actually will do, right?",
    "start": "2160940",
    "end": "2168340"
  },
  {
    "text": "So we, in some sense,\nstretched the function for those places where--",
    "start": "2168340",
    "end": "2173620"
  },
  {
    "text": "you see how do it. You stretch the function\naccording to where you are at.",
    "start": "2173620",
    "end": "2179420"
  },
  {
    "text": "And so, basically\neverything is folded",
    "start": "2179420",
    "end": "2184790"
  },
  {
    "text": "into this generalized margin. And the question is, so the\nquestion now is that question.",
    "start": "2184790",
    "end": "2193420"
  },
  {
    "text": "So for what gf you can bound\nthe generalized error--",
    "start": "2193420",
    "end": "2200069"
  },
  {
    "text": "you can bound the covering\nnumber of g, right?",
    "start": "2200070",
    "end": "2207232"
  },
  {
    "text": "And also, you want this gf\nto be somewhat meaningful, so on and so forth. So and suppose if you just take\ngf to be the standard one, yfx,",
    "start": "2207232",
    "end": "2217839"
  },
  {
    "text": "then the covering\nnumber of this gf will be the same as\ncovering number of f. And it will be-- then the\nRademacher complexity will",
    "start": "2217840",
    "end": "2225610"
  },
  {
    "text": "be something like then the\ncovering number depends",
    "start": "2225610",
    "end": "2232760"
  },
  {
    "text": "on the product. ",
    "start": "2232760",
    "end": "2238320"
  },
  {
    "text": "So but we are trying\nto do better than this. OK. So how do we do this?",
    "start": "2238320",
    "end": "2244510"
  },
  {
    "text": "So now we define this\nso-called all-layer margin. This is a special\ninstance of this gf.",
    "start": "2244510",
    "end": "2250440"
  },
  {
    "text": "This is a concrete\ndefinition of gf for which we can bond the\nRademacher or the covering",
    "start": "2250440",
    "end": "2255960"
  },
  {
    "text": "number complexity. So to define this\nall-layer margin,",
    "start": "2255960",
    "end": "2262020"
  },
  {
    "text": "this generalized margin, so\nwe have to actually introduce some notations. So we are considering\nsome perturbed model.",
    "start": "2262020",
    "end": "2273849"
  },
  {
    "text": "So I guess, I think-- sorry, one moment. Maybe I think actually it's\nuseful to have some motivations",
    "start": "2273850",
    "end": "2281269"
  },
  {
    "text": "before I defined. I though I'd try this. So our motivation\nis the following.",
    "start": "2281270",
    "end": "2287130"
  },
  {
    "text": "So if you think about\nthe linear model,",
    "start": "2287130",
    "end": "2293660"
  },
  {
    "text": "and the margin is\ndefined to be-- the margin, the standard margin,\nso the normalized margin,",
    "start": "2293660",
    "end": "2307615"
  },
  {
    "text": "so normalized margin is defined\nto be something like y times fx",
    "start": "2307615",
    "end": "2315990"
  },
  {
    "text": "over the norm of maybe\nsetting your model is double transpose x. So your margin is defined to\nbe y times the model output",
    "start": "2315990",
    "end": "2324589"
  },
  {
    "text": "over the 2 norm of w, right? So this is the\nnormalized margin, which is something that governs\nthe generalization performance.",
    "start": "2324590",
    "end": "2334008"
  },
  {
    "text": "And the question is, how\ndo you normalize, right? So if you have deep\nmodel, then you can try to normalize\nby something, right?",
    "start": "2334008",
    "end": "2340310"
  },
  {
    "text": "So if you have a deep\nmodel, so one attempt",
    "start": "2340310",
    "end": "2346160"
  },
  {
    "text": "is that you can try to\nnormalize by some quantity.",
    "start": "2346160",
    "end": "2353619"
  },
  {
    "text": "Maybe this could be the product\nof the Lipschitz-ness or maybe",
    "start": "2353620",
    "end": "2358800"
  },
  {
    "text": "something else. So that's the natural attempt. And in some sense,\nall the previous work",
    "start": "2358800",
    "end": "2363827"
  },
  {
    "text": "is in some sense doing this. You are normalizing the\nmargin based on the worst case Lipschitz-ness. So and what we are\ndoing is that we",
    "start": "2363827",
    "end": "2371370"
  },
  {
    "text": "don't know-- we don't\nwant to normalize by only a constant that depends\nonly on the function class.",
    "start": "2371370",
    "end": "2377250"
  },
  {
    "text": "So we take a different approach. What we do is we say, we\ninterpret the standard margin",
    "start": "2377250",
    "end": "2383190"
  },
  {
    "text": "by something else. So we have another\ninterpretation. ",
    "start": "2383190",
    "end": "2391830"
  },
  {
    "text": "So our interpretation is that\nyou can view this as something like minimum delta such\nthat w plus delta--",
    "start": "2391830",
    "end": "2403990"
  },
  {
    "start": "2403990",
    "end": "2410460"
  },
  {
    "text": "sorry w times x plus\ndelta y is less than 0.",
    "start": "2410460",
    "end": "2422990"
  },
  {
    "text": "So you are trying to find\nthe minimum perturbation of your data point such\nthat after perturbed it,",
    "start": "2422990",
    "end": "2429800"
  },
  {
    "text": "you can cross the boundary. So intuitively, this\nis also kind of right.",
    "start": "2429800",
    "end": "2435680"
  },
  {
    "text": "Because the margin is the\ndistance to the boundary. So it's also the same\nthing as how much",
    "start": "2435680",
    "end": "2441050"
  },
  {
    "text": "you can perturb it so that\nyou can cross the boundary. So this is the\nkind of perspective",
    "start": "2441050",
    "end": "2447130"
  },
  {
    "text": "we take to generalize the margin\nfor all-- for deep models. So if you take this, there\nis some kind of a small--",
    "start": "2447130",
    "end": "2455110"
  },
  {
    "text": "if you do the exact\nmath maybe something doesn't match exactly. But this is kind of like the\nrough intuition about it.",
    "start": "2455110",
    "end": "2463780"
  },
  {
    "text": "And how do we do this exactly? So for deep models,\nwe are still trying",
    "start": "2463780",
    "end": "2470290"
  },
  {
    "text": "to take this\nperturbation-based perspective. But we have to perturb-- it turns out we have to perturb\nall the layers, not only",
    "start": "2470290",
    "end": "2477670"
  },
  {
    "text": "the input. So the first attempt we\ntried is that you just perturb the input.",
    "start": "2477670",
    "end": "2483069"
  },
  {
    "text": "You try to see what\nis the smallest perturbation of the\ninput so that you can change the decision\nof your model, right?",
    "start": "2483070",
    "end": "2490600"
  },
  {
    "text": "But that just\ntechnically doesn't work. It doesn't seem to capture\nthe fundamental complexity.",
    "start": "2490600",
    "end": "2495920"
  },
  {
    "text": "So we have to consider\nthis perturbed model that",
    "start": "2495920",
    "end": "2502619"
  },
  {
    "text": "perturbs all the layers.  So what we do is we have\na perturbation delta which",
    "start": "2502620",
    "end": "2510089"
  },
  {
    "text": "is a consequence of perturbation\ndelta 1 after delta R. And each delta i is a vector.",
    "start": "2510090",
    "end": "2518381"
  },
  {
    "text": "And the way you perturb\nis the following. You also have to work out the\nnormalization in the right way.",
    "start": "2518382",
    "end": "2523960"
  },
  {
    "text": "So you first perturb\nthe first layer. So the first layer used to\nbe w1 transpose x, w1 times",
    "start": "2523960",
    "end": "2529240"
  },
  {
    "text": "x in a deep net. And you perturb that\nby adding delta 1 which is a vector, times norm\nof x, or 2 norm of x.",
    "start": "2529240",
    "end": "2540220"
  },
  {
    "text": "And then you perturb\nthe second layer. ",
    "start": "2540220",
    "end": "2546245"
  },
  {
    "text": "So how do you perturb\nthe second layer? You first apply w2\non the first layer,",
    "start": "2546245",
    "end": "2553000"
  },
  {
    "text": "on the perturbed version\nof the first layer. And then you perturb it\nfurthermore with delta 2.",
    "start": "2553000",
    "end": "2559120"
  },
  {
    "text": "And how much you\nperturb with what's the scaling in front of delta 2? Delta 2 is a vector.",
    "start": "2559120",
    "end": "2565200"
  },
  {
    "text": "The scaling is the norm\nof the first layer. ",
    "start": "2565200",
    "end": "2575040"
  },
  {
    "text": "So how do you exactly\ndesign this perturbation is a little bit tricky, right? So we tried various\nversions in our research.",
    "start": "2575040",
    "end": "2582130"
  },
  {
    "text": "And it turns out this is\nactually make everything fit nicely. So you can do this\nfor multiple layers.",
    "start": "2582130",
    "end": "2589000"
  },
  {
    "text": "And then, eventually\nyou have this hR, the R'th layer perturbed\nlayer is equals to you",
    "start": "2589000",
    "end": "2596550"
  },
  {
    "text": "first apply the\nmatrix multiplication and nonlinearity on your\nprevious perturbed layer.",
    "start": "2596550",
    "end": "2603890"
  },
  {
    "text": "And then you perturb\nit by vector delta R scaled by the norm of\nthe previous layer.",
    "start": "2603890",
    "end": "2609245"
  },
  {
    "text": " And after you find\nthis perturbation,",
    "start": "2609245",
    "end": "2614440"
  },
  {
    "text": "you can ask, what's the\nsmallest perturbation that changed my decision? So you can-- and\nthat's the definition",
    "start": "2614440",
    "end": "2620890"
  },
  {
    "text": "of the all-layer margin,\nwhich we call mf, xy. This is the all-layer margin.",
    "start": "2620890",
    "end": "2626820"
  },
  {
    "text": "It's defined to be the\nminimum perturbation. And how do you measure the\nsize of the perturbation?",
    "start": "2626820",
    "end": "2634080"
  },
  {
    "text": "You measure it by the sum of\nthe 2 norm of the perturbation",
    "start": "2634080",
    "end": "2640060"
  },
  {
    "text": "of every layer. And your constraint is\nthat after perturb, I",
    "start": "2640060",
    "end": "2645290"
  },
  {
    "text": "guess you call this fx delta. This is the perturbation\nof the whole model. fx delta auto after perturbation\ntimes y it becomes inactive.",
    "start": "2645290",
    "end": "2655880"
  },
  {
    "text": "So incorrect perturbation. You can also do this\nformat here on labels.",
    "start": "2655880",
    "end": "2662330"
  },
  {
    "text": "But it's essentially the same. So I'm doing binary labels. ",
    "start": "2662330",
    "end": "2668970"
  },
  {
    "text": "So this is the definition\nof the all-layer margin. So you can see that the\ndefinition becomes much more",
    "start": "2668970",
    "end": "2674580"
  },
  {
    "text": "complicated. But then the proof\nof it will be easy. ",
    "start": "2674580",
    "end": "2681100"
  },
  {
    "text": "And I guess you can also\nintuitively interpret this. So mf xy, so in some sense this\nis big if it's hard to perturb.",
    "start": "2681100",
    "end": "2697550"
  },
  {
    "text": "So if it's hard to perturb,\nit's hard to change. It's hard to change\ndecisions of the network.",
    "start": "2697550",
    "end": "2705560"
  },
  {
    "text": "And how could it be hard? I think they are the two ways\nto make it hard to perturb.",
    "start": "2705560",
    "end": "2710690"
  },
  {
    "text": "So one thing is that the\nmodel f is Lipschitz.",
    "start": "2710690",
    "end": "2716730"
  },
  {
    "text": "And this means that\nit is very Lipschitz. So this means that you\nhave to perturb a lot to make a big difference or\nfeel a big change of your model",
    "start": "2716730",
    "end": "2724500"
  },
  {
    "text": "output, right? So and another possibility is\nthat your fx just is large.",
    "start": "2724500",
    "end": "2730830"
  },
  {
    "text": "And sometimes your\nstandard margin is large. If a standard margin is large,\nyou have to change a lot, right?",
    "start": "2730830",
    "end": "2736110"
  },
  {
    "text": "You also have to change a lot. Because before you're outputting\nsomething like positive, fx is very big.",
    "start": "2736110",
    "end": "2742050"
  },
  {
    "text": "And now you have to change it\nto another side of the boundary. So then you have to\nperturb a lot, right?",
    "start": "2742050",
    "end": "2748880"
  },
  {
    "text": "So or maybe I could say\nyfx, y times fx is large.",
    "start": "2748880",
    "end": "2756259"
  },
  {
    "text": "So typically, also I\nalways talk about y is 1. So positive means\nthat you are very",
    "start": "2756260",
    "end": "2763777"
  },
  {
    "text": "confident about your prediction. And if you are very\nconfident, then it means you have to\nchange-- perturb a lot",
    "start": "2763777",
    "end": "2769490"
  },
  {
    "text": "so that you can\nchange your mind, right, so that the model\ncan change its mind. ",
    "start": "2769490",
    "end": "2778880"
  },
  {
    "text": "So and here,\nLipschitz, technically, this is Lipschitz in the\nintermediate variables--",
    "start": "2778880",
    "end": "2785540"
  },
  {
    "text": "intermediate layers. Because you are measuring how\nrobust it is to perturbation.",
    "start": "2785540",
    "end": "2791770"
  },
  {
    "text": "But the perturbation is done\non the intermediate layers. But the Lipschitz in\nthe intermediate layers,",
    "start": "2791770",
    "end": "2798609"
  },
  {
    "text": "it turns out that it's actually\nclose to Lipschitz-ness with respect to parameters. I'll discuss that in a moment.",
    "start": "2798610",
    "end": "2804805"
  },
  {
    "start": "2804805",
    "end": "2810500"
  },
  {
    "text": "So and once you\nhave all of this. So then you can have\nthe following theorem.",
    "start": "2810500",
    "end": "2819560"
  },
  {
    "text": "So this is saying that with\nhigh probability of l 0",
    "start": "2819560",
    "end": "2825160"
  },
  {
    "text": "1f is 0 I error of f is less\nthan O tilde of the following.",
    "start": "2825160",
    "end": "2830990"
  },
  {
    "text": "We have 1 over\nsquare root 10 first. And then you have sum of-- ",
    "start": "2830990",
    "end": "2838240"
  },
  {
    "text": "this is the so-called\n1-1 norm of w, which I'm going to define in a moment. And also minimum i, mf,\nxi, yi, plus O tilde",
    "start": "2838240",
    "end": "2854316"
  },
  {
    "text": "of R over square root, where\njust sum of absolute values",
    "start": "2854316",
    "end": "2875540"
  },
  {
    "text": "matches w. I guess, in some sense\nwe are in the mindset",
    "start": "2875540",
    "end": "2883110"
  },
  {
    "text": "that anything polynomial in\nthe norm doesn't really matter, so doesn't matter that much. So this is in some sense you\njust consider as polynomial.",
    "start": "2883110",
    "end": "2892230"
  },
  {
    "text": "But of course, you can also\ntalk about whether it's 1 to 1, 1 norm is the right\nchoice of the norm.",
    "start": "2892230",
    "end": "2898200"
  },
  {
    "text": "In some sense, this is not\nthe best norm we can hope for. So there is still some\nroom for improvement here.",
    "start": "2898200",
    "end": "2907080"
  },
  {
    "text": "But I guess, suppose you ignore\nthe anything polynomial norm. So what's important here is\nthis all-layer margin here.",
    "start": "2907080",
    "end": "2914860"
  },
  {
    "text": "So basically, this is saying\nthat if the all-layer margin is always big.",
    "start": "2914860",
    "end": "2920940"
  },
  {
    "text": "The utilization is good. If the all-layer\nmargin is small, then your generalization is bad. And what's all-layer margin?",
    "start": "2920940",
    "end": "2926190"
  },
  {
    "text": "All-layer margin is about the\nperturbation robustness so the intermediate layers. So this is saying that if you\nare robust to perturbations",
    "start": "2926190",
    "end": "2939476"
  },
  {
    "text": "in intermediate layers,\nand that implies that you",
    "start": "2939476",
    "end": "2947050"
  },
  {
    "text": "have good generalization. ",
    "start": "2947050",
    "end": "2957660"
  },
  {
    "text": "And you can also compare\nthis with the bond that we got before.",
    "start": "2957660",
    "end": "2963142"
  },
  {
    "text": "You can pretty much argue\nthat this is strictly better than before. So basically, so compare--",
    "start": "2963142",
    "end": "2972180"
  },
  {
    "text": "is this the right place\nfor us to discuss this? ",
    "start": "2972180",
    "end": "2977600"
  },
  {
    "text": "I guess, let me\ndiscuss this comparing with the previous\nbonds later when",
    "start": "2977600",
    "end": "2982760"
  },
  {
    "text": "I'm doing all the kind of\nremarks about this theorem. But you can show that this is\nbetter than the previous one,",
    "start": "2982760",
    "end": "2990109"
  },
  {
    "text": "mostly just because of this mf-- in some sense with mf, xy\nit's kind of roughly speaking,",
    "start": "2990110",
    "end": "2998849"
  },
  {
    "text": "you can think of this as-- so in the worst case,\nI think this is small.",
    "start": "2998850",
    "end": "3008011"
  },
  {
    "text": "This may be the smaller than\nover fx, something like this.",
    "start": "3008011",
    "end": "3021347"
  },
  {
    "text": "So because this is a\nLipschitz-ness and this is how much you have\nto change your output. You have to change your\noutlook from fx to 0, right?",
    "start": "3021347",
    "end": "3028530"
  },
  {
    "text": "So and this is a Lipschitz-ness. So that's why you\nhave to change-- you have to make a big\nmovement to change it,",
    "start": "3028530",
    "end": "3036530"
  },
  {
    "text": "to change fx from something\nlike positive or negative to 0.",
    "start": "3036530",
    "end": "3041880"
  },
  {
    "text": "And wait, my bad. I think I-- sorry about that.",
    "start": "3041880",
    "end": "3047710"
  },
  {
    "text": "I think I'm doing a-- should be this. ",
    "start": "3047710",
    "end": "3056910"
  },
  {
    "text": "And I said, that's why this is\nbetter than the previous bond, because the previous\nbond didn't consider",
    "start": "3056910",
    "end": "3063150"
  },
  {
    "text": "the different Lipschitz-ness\nat different data point. But here, you are\nreally talking about",
    "start": "3063150",
    "end": "3068760"
  },
  {
    "text": "if your Lipschitz-ness at\nthe data point you have seen, then you can generalize well.",
    "start": "3068760",
    "end": "3074415"
  },
  {
    "text": " But maybe let me\ndiscuss this more. I think let me have a\nmore thorough discussion",
    "start": "3074415",
    "end": "3084260"
  },
  {
    "text": "about this later. I just don't want to-- I want to have a-- show\na little bit of this just so that you don't feel\nlike this is a useless bound.",
    "start": "3084260",
    "end": "3091710"
  },
  {
    "text": "But maybe bear with me and\njust assume this is useful. And then we can discuss\nall the interpretations.",
    "start": "3091710",
    "end": "3099329"
  },
  {
    "text": "Any question so far? ",
    "start": "3099330",
    "end": "3110460"
  },
  {
    "text": "How well I'm doing on time? ",
    "start": "3110460",
    "end": "3118190"
  },
  {
    "text": "OK I guess I only\nhave 30 minutes. So let's just dive\ninto the proof.",
    "start": "3118190",
    "end": "3125750"
  },
  {
    "text": "So I guess the proof\nrequires a few steps. But a few small steps.",
    "start": "3125750",
    "end": "3132150"
  },
  {
    "text": "So first of all, it suffices to\nbound this N infinite epsilon",
    "start": "3132150",
    "end": "3142279"
  },
  {
    "text": "g by O of this.",
    "start": "3142280",
    "end": "3147510"
  },
  {
    "text": " But I think I have some--",
    "start": "3147510",
    "end": "3155310"
  },
  {
    "text": "sorry. I have some typos here. Don't-- I think\nthis should be this.",
    "start": "3155310",
    "end": "3170330"
  },
  {
    "text": "Yeah. This should be this. I'll double-check later.",
    "start": "3170330",
    "end": "3175675"
  },
  {
    "text": "Because it's always\na polynomial, so I didn't really pay\ntoo much attention. But I think this is a typo.",
    "start": "3175675",
    "end": "3180830"
  },
  {
    "text": "So I think you only\nhave to show this. ",
    "start": "3180830",
    "end": "3192890"
  },
  {
    "text": "Sorry, I don't know. I don't really what this-- I will send a square note\ntaker clarification about this.",
    "start": "3192890",
    "end": "3199875"
  },
  {
    "text": "I think, I don't know\nexactly why that square is applied inside or outside. But either way, you have to\nshow some bound like this.",
    "start": "3199875",
    "end": "3206850"
  },
  {
    "text": "So let's assume this\nis the correct bond, and then you basically have\nto show something like this. Because if you have this, then\nyou can use the lemma before.",
    "start": "3206850",
    "end": "3214110"
  },
  {
    "text": "And then on the\ngeneralized margin, you get this Rademacher--\nthis generalized bound, right?",
    "start": "3214110",
    "end": "3219660"
  },
  {
    "text": "So essentially, we just have to\nbound the covering number of g. And it turns out that\nthe covering number of g,",
    "start": "3219660",
    "end": "3225210"
  },
  {
    "text": "you have this very nice\ndecomposition lemma. So let's say that fi define\neach layer, the hypothesis class",
    "start": "3225210",
    "end": "3235840"
  },
  {
    "text": "for every layer. And we also constrain that wi1,\n1 norm is less than beta i, OK?",
    "start": "3235840",
    "end": "3246170"
  },
  {
    "text": "So then, your f is\nreally fr composed with fr minus 1 up to f1.",
    "start": "3246170",
    "end": "3253423"
  },
  {
    "text": "This is the notation\nwe have used. And we recall that we had a\nkind of a decomposition lemma before, which was kind\nof complicated, right?",
    "start": "3253423",
    "end": "3260030"
  },
  {
    "text": "So you have all of\nthese dependencies and how the arrow propagates. But now the lemma\nis pretty simple. ",
    "start": "3260030",
    "end": "3272450"
  },
  {
    "text": "So let m composed\nwith f denote it's",
    "start": "3272450",
    "end": "3280589"
  },
  {
    "text": "family of the all-layer margin. ",
    "start": "3280590",
    "end": "3286039"
  },
  {
    "text": "And then consider\nthen you have that,",
    "start": "3286040",
    "end": "3291450"
  },
  {
    "text": "the log of the infinity\ncovering number,",
    "start": "3291450",
    "end": "3297290"
  },
  {
    "text": "where the radius is\njust simply the sum of-- the average, in some\nsense a quadratic average",
    "start": "3297290",
    "end": "3302809"
  },
  {
    "text": "of the radius on each layer. And you care about the\ngeneralized margin.",
    "start": "3302810",
    "end": "3309060"
  },
  {
    "text": "This is less than the sum\nof the log infinity norm",
    "start": "3309060",
    "end": "3315630"
  },
  {
    "text": "covering number of\nepsilon i f to fi.",
    "start": "3315630",
    "end": "3320799"
  },
  {
    "text": "So in some sense, this\nis saying that you only have to deal with the covering\nnumber for every layer.",
    "start": "3320800",
    "end": "3326537"
  },
  {
    "text": "And then you've got\na covering number for the composed function class. But you don't get the covering\nnumber for the compose function",
    "start": "3326538",
    "end": "3333570"
  },
  {
    "text": "class exactly. You get the covering number\nof the all-layer margin of the composed function class.",
    "start": "3333570",
    "end": "3340280"
  },
  {
    "text": "So and here, is n\ninfinity epsilon i fi",
    "start": "3340280",
    "end": "3346140"
  },
  {
    "text": "is defined with\nrespect to the input.",
    "start": "3346140",
    "end": "3351309"
  },
  {
    "text": "So there is an input domain to\ndefine this covering number, right? So the input domain, which\nis the one-- the 2 norm bond",
    "start": "3351310",
    "end": "3362790"
  },
  {
    "text": "is less than 1. ",
    "start": "3362790",
    "end": "3368130"
  },
  {
    "text": "And so, I guess the\nmost important thing is that this is not a-- this is n, this is the\nearlier margin, OK?",
    "start": "3368130",
    "end": "3375825"
  },
  {
    "start": "3375825",
    "end": "3382570"
  },
  {
    "text": "So and the corollary\nis that if each",
    "start": "3382570",
    "end": "3400950"
  },
  {
    "text": "of the layer you can bound the\ncovering number by something",
    "start": "3400950",
    "end": "3406260"
  },
  {
    "text": "like ci square over\nepsilon i square, suppose you can bound this.",
    "start": "3406260",
    "end": "3411950"
  },
  {
    "text": "Let's use little c here. So then, take epsilon to be\nepsilon times ci over sum",
    "start": "3411950",
    "end": "3424430"
  },
  {
    "text": "of square root, ci square. So i is equal to this.",
    "start": "3424430",
    "end": "3430240"
  },
  {
    "text": "Then we have log epsilon--",
    "start": "3430240",
    "end": "3436990"
  },
  {
    "text": "you have the carbon number\nof the compose model is less than sum of ci\nsquare over epsilon squared.",
    "start": "3436990",
    "end": "3445205"
  },
  {
    "text": "So which means that\nsuppose you believe ci is a complex measure\nfor each of the layer. Then you can get the complexity\nfor the composed model,",
    "start": "3445205",
    "end": "3452799"
  },
  {
    "text": "the all-layer margin\nof the composed model, the complexity will be\njust the sum of ci squared.",
    "start": "3452800",
    "end": "3458560"
  },
  {
    "text": " Yeah. I think.",
    "start": "3458560",
    "end": "3464349"
  },
  {
    "text": "I know-- I didn't\nhave error here. I think this is indeed\nsomething like this.",
    "start": "3464350",
    "end": "3470010"
  },
  {
    "text": " Yeah, this was correct.",
    "start": "3470010",
    "end": "3477872"
  },
  {
    "text": "Sorry. OK. Cool. So and the ci will be\nsomething-- like ci",
    "start": "3477872",
    "end": "3485140"
  },
  {
    "text": "will be something like\nthe wi1 comma 1 norm. And that's how you go through\nall these things, right?",
    "start": "3485140",
    "end": "3492715"
  },
  {
    "start": "3492715",
    "end": "3498210"
  },
  {
    "text": "So basically, we can show-- ",
    "start": "3498210",
    "end": "3504270"
  },
  {
    "text": "I think I will improve this. But this is not-- this is because this is\nfor one layer, right? So we assume you\ncan basically you",
    "start": "3504270",
    "end": "3512363"
  },
  {
    "text": "can believe that\nyou can basically invoke a theorem for a\nlinear model to get this so indeed it's true.",
    "start": "3512363",
    "end": "3518080"
  },
  {
    "text": "So for linear models you\nget something like this. ",
    "start": "3518080",
    "end": "3524249"
  },
  {
    "text": "And beta i was the\nbound, on the, sorry, call it beta i was the bound\non the 1, 1 norm of wi.",
    "start": "3524249",
    "end": "3532360"
  },
  {
    "text": "And this will imply\nthat min theorem. OK. ",
    "start": "3532360",
    "end": "3540370"
  },
  {
    "text": "So I think I hope\nI convinced you that basically as long as you\nprove this decomposition lemma,",
    "start": "3540370",
    "end": "3545410"
  },
  {
    "text": "then you are done. Because you, for\nthe right-hand side you invoke something\nabout linear model.",
    "start": "3545410",
    "end": "3551083"
  },
  {
    "text": "And then you plug in this lemma,\nand you get the covering number bonds for the all-layer margin. And then you get\nthe original theorem",
    "start": "3551083",
    "end": "3558850"
  },
  {
    "text": "using the lemma I\nhave shown before for the generalized margin. OK? ",
    "start": "3558850",
    "end": "3565410"
  },
  {
    "text": "So any questions so far?",
    "start": "3565410",
    "end": "3572018"
  },
  {
    "start": "3572019",
    "end": "3582525"
  },
  {
    "text": "OK. So now let's prove the lemma,\nthe decomposition lemma. ",
    "start": "3582525",
    "end": "3595250"
  },
  {
    "text": "So we always-- so I\nguess I only stated lemma for the concrete fi, right? Which is the z map to sigma yiz.",
    "start": "3595250",
    "end": "3602705"
  },
  {
    "text": " You can also have--\nyou can state the lemma in a more general form.",
    "start": "3602705",
    "end": "3608530"
  },
  {
    "text": "And also you can prove it\nin a more general form. But I'm only going to prove\nit for this particular family",
    "start": "3608530",
    "end": "3613609"
  },
  {
    "text": "of fi. And so, the first step is that-- so we'll show-- so\nthere are two steps.",
    "start": "3613610",
    "end": "3622320"
  },
  {
    "text": "Step one, we show that mf,\nmx is 1 Lipschitz in f.",
    "start": "3622320",
    "end": "3635860"
  },
  {
    "text": "And what 1 Lipschitz in\nf means is the following. So for every f and f prime,\nmf xy minus f, f prime, xy",
    "start": "3635860",
    "end": "3649520"
  },
  {
    "text": "it's less than in some sense the\nLipschitz-ness of every layer.",
    "start": "3649520",
    "end": "3657600"
  },
  {
    "text": "So you have all-layers. So this is sum from\n1 to R. And you",
    "start": "3657600",
    "end": "3667040"
  },
  {
    "text": "take the max of fix\nminus fi prime x to node.",
    "start": "3667040",
    "end": "3677190"
  },
  {
    "text": "X [INAUDIBLE]. So here, f is equals\nto fr, composed",
    "start": "3677190",
    "end": "3684020"
  },
  {
    "text": "with fr minus 1 up to f1. And f2 prime is fr prime\ncomposed with fr minus 1,",
    "start": "3684020",
    "end": "3692559"
  },
  {
    "text": "f1 prime. So basically, the Lipschitz-ness\nof this all-layer margin",
    "start": "3692560",
    "end": "3697690"
  },
  {
    "text": "is something that doesn't have\nactual scale in some sense because you are looking at\nthis ball with no scale.",
    "start": "3697690",
    "end": "3707740"
  },
  {
    "text": "And also, it only depends-- basically on sum of\nthe Lipschitz-ness,",
    "start": "3707740",
    "end": "3712795"
  },
  {
    "text": "or sum of the differences\nbetween f1 and fy prime.",
    "start": "3712795",
    "end": "3718240"
  },
  {
    "text": "So there's no multiplier here. You are not multiplying on\nthe Lipschitz-ness of f. So it's really literally a sum.",
    "start": "3718240",
    "end": "3724430"
  },
  {
    "text": "It's very clean. And let's prove this\nstep one in a moment.",
    "start": "3724430",
    "end": "3730330"
  },
  {
    "text": "But suppose you have step one. Then what you can get is\nthat you can use step two. You can use the step one just\nto get the theorem relatively",
    "start": "3730330",
    "end": "3738530"
  },
  {
    "text": "easily. What you do is you say,\nnow you construct a cover. Construct a cover. ",
    "start": "3738530",
    "end": "3746150"
  },
  {
    "text": "And how do you do that? The cover is-- the construction\nis also kind of trivial. So what you do is you let U1\nup to UR are be epsilon 1,",
    "start": "3746150",
    "end": "3758789"
  },
  {
    "text": "epsilon R covered of f1\nup to fr respectively.",
    "start": "3758790",
    "end": "3768150"
  },
  {
    "text": "And recall that if you still\nremember what we did last time, the covering was\nvery complicated.",
    "start": "3768150",
    "end": "3773230"
  },
  {
    "text": "So you iteratively\nconstruct covers and make it very complicated. But now we just individually\nconstruct covers for every fi.",
    "start": "3773230",
    "end": "3780970"
  },
  {
    "text": "And then, and we\nsay ui such that ui is equal to this the\ninfinite norm covering",
    "start": "3780970",
    "end": "3788470"
  },
  {
    "text": "number, epsilon f, fi.",
    "start": "3788470",
    "end": "3794250"
  },
  {
    "text": "And then, so this means\nthat by definition,",
    "start": "3794250",
    "end": "3800370"
  },
  {
    "text": "so we got that for\nevery fi and capital Fi,",
    "start": "3800370",
    "end": "3809480"
  },
  {
    "text": "there exists some function ui\nin capital Ui, such that fi",
    "start": "3809480",
    "end": "3816359"
  },
  {
    "text": "minus ui it's smal, right? So and I guess we are\nusing this matrix,",
    "start": "3816360",
    "end": "3822460"
  },
  {
    "text": "this infinity norm matrix,\nf ix minus uix 2 norm",
    "start": "3822460",
    "end": "3832050"
  },
  {
    "text": "is more than epsilon i. This is we know by definition. And now, we're\ngoing to turn this",
    "start": "3832050",
    "end": "3837150"
  },
  {
    "text": "into a cover for the\ncompose the family. So and the cover\nis just, so we just",
    "start": "3837150",
    "end": "3843269"
  },
  {
    "text": "take the U to be the family\nof just the composition of all",
    "start": "3843270",
    "end": "3850640"
  },
  {
    "text": "of this, the composition\nof UR composed with UR minus 1, composed\nwith U1, which is--",
    "start": "3850640",
    "end": "3857300"
  },
  {
    "start": "3857300",
    "end": "3864100"
  },
  {
    "text": "and this will be our cover. And we'll show this-- we'll be showing will be our\ncover for m composed with f.",
    "start": "3864100",
    "end": "3878010"
  },
  {
    "text": "And why that's the case? This is because\nsuppose we are given",
    "start": "3878010",
    "end": "3883750"
  },
  {
    "text": "f is equal to fr composed up to\nF1 in capital F. Then let u1 up",
    "start": "3883750",
    "end": "3893110"
  },
  {
    "text": "to ur up to u1 be the nearest\nneighbor of fr up to f1.",
    "start": "3893110",
    "end": "3907020"
  },
  {
    "text": "So then, as you can see that\nusing the Lipschitz-ness,",
    "start": "3907020",
    "end": "3913510"
  },
  {
    "text": "this minus n, let's\nsay u is equals to ur composed with u ms1 up to u1.",
    "start": "3913510",
    "end": "3922500"
  },
  {
    "text": "so you get this. So suppose you do this. This is less than\nusing our step one.",
    "start": "3922500",
    "end": "3931250"
  },
  {
    "text": "Sum of the difference\nbetween fi and ui,",
    "start": "3931250",
    "end": "3940310"
  },
  {
    "text": "the worst case difference\nbetween them over the norm bar 1.",
    "start": "3940310",
    "end": "3946130"
  },
  {
    "text": "And because f and\nu are close, that's how we constructed the cover. So we get square root sum of\nepsilon i square from 1 to r.",
    "start": "3946130",
    "end": "3954270"
  },
  {
    "text": "OK. ",
    "start": "3954270",
    "end": "3964810"
  },
  {
    "text": "So basically, once you have such\na nice Lipschitz-ness property,",
    "start": "3964810",
    "end": "3973990"
  },
  {
    "text": "then you can just cover\neverything individually. And you don't have to think\ntoo much about the composition. The composition is trivial,\nbecause this deal with this,",
    "start": "3973990",
    "end": "3981310"
  },
  {
    "text": "is dealt with by this. So now, why the\nLipschitz-ness holds?",
    "start": "3981310",
    "end": "3986530"
  },
  {
    "text": "So let's prove step one. ",
    "start": "3986530",
    "end": "3992320"
  },
  {
    "text": "So and so we only\nproved the upper bounds. So we only prove\nthis by symmetry.",
    "start": "3992320",
    "end": "4002670"
  },
  {
    "text": "But because f and f prime have\nthe same row, so you can-- you only have to prove one side. And you can flip them\nto the other side.",
    "start": "4002670",
    "end": "4010240"
  },
  {
    "text": "And it's sometimes the way\nto prove it is just really-- each of them is defined by some\noptimization program, right?",
    "start": "4010240",
    "end": "4018680"
  },
  {
    "text": "And they are the solutions. The optimal value of the\noptimization programs, right? Basically, you\nare trying to show that two optimization programs\nare doing similar stuff.",
    "start": "4018680",
    "end": "4027160"
  },
  {
    "text": "And how do you do that? Typically, you construct optimal\nsolution of one optimization program into a feasible\nsolution to another optimization",
    "start": "4027160",
    "end": "4035100"
  },
  {
    "text": "program. That's how you relate to\noptimization programs. So let delta 1 star\nup to delta R star",
    "start": "4035100",
    "end": "4044720"
  },
  {
    "text": "be the optimal choice of\ndelta in defining mf, x of f.",
    "start": "4044720",
    "end": "4060530"
  },
  {
    "text": "So and we want to turn--\nso our goal is to turn this into delta 1 hat, that R\ngamma of feasible solution",
    "start": "4060530",
    "end": "4076730"
  },
  {
    "text": "of mf prime xy. So if it's a feasible\nsolution, you",
    "start": "4076730",
    "end": "4082910"
  },
  {
    "text": "get mf prime xy is less than\nsum of delta i hat square.",
    "start": "4082910",
    "end": "4090250"
  },
  {
    "text": "And then you can relate this\nto some of the other i using",
    "start": "4090250",
    "end": "4096499"
  },
  {
    "text": "your construction, all right? OK. So that's the rough idea.",
    "start": "4096499",
    "end": "4102297"
  },
  {
    "text": "And how do we construct\nthis the other one hat up to get an r hat? So we wanted it to be\nfeasible so that we",
    "start": "4102298",
    "end": "4108448"
  },
  {
    "text": "can have this in an inequality. This part is going to\nbe the feasibility part. So basically, the way we do it\nis that we want to construct--",
    "start": "4108448",
    "end": "4116859"
  },
  {
    "text": "so we want to make\nf prime with delta",
    "start": "4116859",
    "end": "4125710"
  },
  {
    "text": "1 hat up to delta r hat\ndoing the same thing",
    "start": "4125710",
    "end": "4133028"
  },
  {
    "text": "as f and with delta 1\nstar up to delta r star. So basically you\nwant the perturbation",
    "start": "4133029",
    "end": "4139414"
  },
  {
    "text": "on f prime to do the same\nthing as perturbation-- the perturbation of\ndelta i star on f.",
    "start": "4139414",
    "end": "4146839"
  },
  {
    "text": "So that, then you know that this\nwill be a feasible solution. Because what's the feasibility?",
    "start": "4146840",
    "end": "4152000"
  },
  {
    "text": "The feasibility is about whether\nyou perturbed the prediction to the other side. So if this one can perturb\nthe prediction, the change",
    "start": "4152000",
    "end": "4160134"
  },
  {
    "text": "the prediction to\nthe other side, then the other one can\nalso change the prediction because they're\ndoing the same thing.",
    "start": "4160135",
    "end": "4166699"
  },
  {
    "text": "That's the principal and how\ndo you do that is just pretty much just algebra.",
    "start": "4166700",
    "end": "4172220"
  },
  {
    "text": "So f has parameter w1 of wr.",
    "start": "4172220",
    "end": "4180420"
  },
  {
    "text": "And f prime has parameter\nw1 prime up to wr prime.",
    "start": "4180420",
    "end": "4185759"
  },
  {
    "text": "And let's consider\nthe computation. So I guess the\ncomputation is this.",
    "start": "4185760",
    "end": "4194580"
  },
  {
    "text": "So h1 1 is equal to w1x\nplus delta 1 star x.",
    "start": "4194580",
    "end": "4200490"
  },
  {
    "text": "h2 is equal to sigma w2h1\nplus delta 2 star h1.",
    "start": "4200490",
    "end": "4208700"
  },
  {
    "text": "So on and so forth. hr is equal to sigma wr hr minus\n1 plus delta R star hr minus 1.",
    "start": "4208700",
    "end": "4217400"
  },
  {
    "text": "So this is the computation\nyou did for mf, xy, right?",
    "start": "4217400",
    "end": "4228340"
  },
  {
    "text": "So and I want to imitate this\ncomputation by perturbing f prime in some way.",
    "start": "4228340",
    "end": "4233680"
  },
  {
    "text": "And how do we imitate that? So the imitation\nis kind of trivial. So imitate this.",
    "start": "4233680",
    "end": "4241920"
  },
  {
    "text": "So what you do is\nyou say, you take-- so for f prime,\nwhat happens? h1 is",
    "start": "4241920",
    "end": "4247070"
  },
  {
    "text": "equal to w1 prime\nx plus something. Plus and how do you-- so and you\nsuppose you predict the delta 1",
    "start": "4247070",
    "end": "4254840"
  },
  {
    "text": "star x, they wouldn't have. Because this w1 prime is\ndifferent from w1, right?",
    "start": "4254840",
    "end": "4261487"
  },
  {
    "text": "So you have to predict\nsomething in addition to make this computation\nthe same as before. And how do you do that is\nyou perturb in addition",
    "start": "4261487",
    "end": "4268719"
  },
  {
    "text": "w1 minus w1 prime x. And then, these two are\nliterally exactly the same.",
    "start": "4268720",
    "end": "4275970"
  },
  {
    "text": "So basically, you declare\nthis as a new perturbation. You declare this has to be\ndelta 1 hat times x 2 norm.",
    "start": "4275970",
    "end": "4286610"
  },
  {
    "text": "So basically, you compensate the\ndifference between w1, w1 prime by adding this\nadditional perturbation.",
    "start": "4286610",
    "end": "4293840"
  },
  {
    "text": "So that means that\ndelta 1 hat is equal to delta 1\nstar plus w1 minus w1",
    "start": "4293840",
    "end": "4299929"
  },
  {
    "text": "prime x over 2 norm of x. And you do the same thing\nbasically for every layer.",
    "start": "4299930",
    "end": "4307050"
  },
  {
    "text": "So now h2, you want h2 to be\nequal to the same h2 above. But your first\nstep is you're only",
    "start": "4307050",
    "end": "4315350"
  },
  {
    "text": "perturbing based on w1 prime. You are not perturbing\nbased on w2. Sorry, you are only\nperturbing based on w2 prime,",
    "start": "4315350",
    "end": "4320810"
  },
  {
    "text": "but not based on w2. So what you do is you first\nperturb the original one,",
    "start": "4320810",
    "end": "4326360"
  },
  {
    "text": "the other 2 star h1. And then you compensate the\ndif by perturbing even more.",
    "start": "4326360",
    "end": "4332600"
  },
  {
    "text": " Something like this. And you declare this entire\nthing will be defined",
    "start": "4332600",
    "end": "4340130"
  },
  {
    "text": "to be the other two, h1 2 norm. And that means that the other\n2, delta 2 hat, the 2 hat",
    "start": "4340130",
    "end": "4347930"
  },
  {
    "text": "will be delta 1 2 star plus\nsomething like h1 2 norm.",
    "start": "4347930",
    "end": "4355070"
  },
  {
    "text": "And the denominator\nwill be sigma w2h1 minus sigma 2w2 primeh1.",
    "start": "4355070",
    "end": "4360260"
  },
  {
    "text": " I guess you do the same\nthing for every layer.",
    "start": "4360260",
    "end": "4366220"
  },
  {
    "text": "And in general, you just\ntake the other i hat",
    "start": "4366220",
    "end": "4371920"
  },
  {
    "text": "to be delta i star plus sigma\nw1 h i minus 1, minus sigma wi",
    "start": "4371920",
    "end": "4379840"
  },
  {
    "text": "prime h i minus 1, over\nh i minus 1 2 norm.",
    "start": "4379840",
    "end": "4386750"
  },
  {
    "text": "And now, we got our goal. So basically, delta 1\nhat on f, on f prime,",
    "start": "4386750",
    "end": "4396420"
  },
  {
    "text": "is the same doing the same thing\nas delta 1 up to delta R on f.",
    "start": "4396420",
    "end": "4401940"
  },
  {
    "text": "I'm using these shorthand just\nto save some time in writing. So I'm saying\nthat, basically I'm",
    "start": "4401940",
    "end": "4407370"
  },
  {
    "text": "saying you perturb the\ndelta 1 hat up to delta r hat from f prime\nit's doing exactly",
    "start": "4407370",
    "end": "4413070"
  },
  {
    "text": "the same functionality, the same\nprediction, as the other one. So that means that this\nis a feasible solution.",
    "start": "4413070",
    "end": "4420120"
  },
  {
    "text": "This is a feasible\nsolution for mf prime. So that's why mf prime xy is\nless than the sum of delta i",
    "start": "4420120",
    "end": "4429890"
  },
  {
    "text": "hat 2 norm square, square root. And now, this is\nequals to I guess",
    "start": "4429890",
    "end": "4439020"
  },
  {
    "text": "I'm going to bond this by\nsquare root sum of delta 1 star",
    "start": "4439020",
    "end": "4444320"
  },
  {
    "text": "2 norm squared plus square\nroot sum of the differences",
    "start": "4444320",
    "end": "4449420"
  },
  {
    "text": "between them. ",
    "start": "4449420",
    "end": "4457730"
  },
  {
    "text": "And this is using the\nso-called Minkowski--",
    "start": "4457730",
    "end": "4466230"
  },
  {
    "text": "I guess, I always think\nof it as Cauchy-Schwarz but I think there's a\ntechnical name, which",
    "start": "4466230",
    "end": "4471570"
  },
  {
    "text": "is called Minkowski inequality. ",
    "start": "4471570",
    "end": "4478060"
  },
  {
    "text": "So what it's doing is that\nthe Minkowski inequality is saying the following.",
    "start": "4478060",
    "end": "4484160"
  },
  {
    "text": "So if you look at square root\nof the sum of ai plus bi 2 norm",
    "start": "4484160",
    "end": "4490780"
  },
  {
    "text": "square, this is less than\nsquare root sum of ai square,",
    "start": "4490780",
    "end": "4497020"
  },
  {
    "text": "and square root sum\nof bi 2 norm square. So this is the\nMinkowski inequality.",
    "start": "4497020",
    "end": "4504460"
  },
  {
    "text": "And actually, you can prove this\ninequality by Cauchy-Schwarz by just x-- taking the square on both sides. And cancel a bunch of terms,\nand it becomes Cauchy-Schwarz.",
    "start": "4504460",
    "end": "4511390"
  },
  {
    "text": "All right.  So we apply this when--\nwhere ai is delta i star,",
    "start": "4511390",
    "end": "4520270"
  },
  {
    "text": "and bi is this thing, is\nthe difference between them.",
    "start": "4520270",
    "end": "4525870"
  },
  {
    "text": "I think 5% is enough for me.  Yeah.",
    "start": "4525870",
    "end": "4532240"
  },
  {
    "text": "Cool. So yeah. One minute per percentage. OK.",
    "start": "4532240",
    "end": "4537360"
  },
  {
    "text": " So now, let's see. So and this one is the mf of xy.",
    "start": "4537360",
    "end": "4547460"
  },
  {
    "text": "And the other one,\nyou can bound it by-- ",
    "start": "4547460",
    "end": "4553369"
  },
  {
    "text": "this is, I guess\nthis is literally",
    "start": "4553370",
    "end": "4558450"
  },
  {
    "text": "you can bound this by\nsquare root sum over i from 1 to r, the\nmax over x in 1 norm",
    "start": "4558450",
    "end": "4568350"
  },
  {
    "text": "and sigma wx minus\nsigma a prime x square.",
    "start": "4568350",
    "end": "4576460"
  },
  {
    "text": "Just because this whole\nthing is homogeneous. So dividing by the\n2 norm is the same as restricting 2\nnorm to be 1, right?",
    "start": "4576460",
    "end": "4584260"
  },
  {
    "text": "So and then this is equal\nto mf xy plus square root",
    "start": "4584260",
    "end": "4590619"
  },
  {
    "text": "sum over r max x 2\nnorm less than 1,",
    "start": "4590620",
    "end": "4595980"
  },
  {
    "text": "f ix minus fi prime x squared.",
    "start": "4595980",
    "end": "4601480"
  },
  {
    "text": "And this is what we\nwanted for step one. ",
    "start": "4601480",
    "end": "4620650"
  },
  {
    "text": "Any questions? [INAUDIBLE]",
    "start": "4620650",
    "end": "4627160"
  },
  {
    "text": "w and w prime? Yeah. So w and w prime-- w\nis the parameter for f, and w prime is the\nparameter for f prime.",
    "start": "4627160",
    "end": "4634420"
  },
  {
    "text": "And they don't have-- at least in this context, they\ndon't have any relationship. Because I'm just trying\nto show this step one.",
    "start": "4634420",
    "end": "4644260"
  },
  {
    "text": "So I'm taking two\narbitrary f and f prime. And I want to say that\nthe all-layer margin",
    "start": "4644260",
    "end": "4649560"
  },
  {
    "text": "difference, difference\nin all-layer margin is bonded by the difference\nin each of the layers.",
    "start": "4649560",
    "end": "4655312"
  },
  {
    "text": "So it doesn't matter\nwhat they are. [INAUDIBLE] ",
    "start": "4655312",
    "end": "4663040"
  },
  {
    "text": "Yes. So f prime involves\nall the wi primes, and f involves all the wi's.",
    "start": "4663040",
    "end": "4668450"
  },
  {
    "start": "4668450",
    "end": "4675300"
  },
  {
    "text": "Cool. Any other questions? It feels like this proposition\nthat we just proved [INAUDIBLE]",
    "start": "4675300",
    "end": "4684960"
  },
  {
    "text": "Can you say again? [INAUDIBLE]",
    "start": "4684960",
    "end": "4693970"
  },
  {
    "text": "Yeah. So this-- if I'm\nguessing the question, I think all of this depends\non the definition of mf, yes,",
    "start": "4693970",
    "end": "4701489"
  },
  {
    "text": "of course. And it's actually, when\nwe do the research, I think it's that we are\ntrying to meet in the middle.",
    "start": "4701490",
    "end": "4707850"
  },
  {
    "text": "You have to change the\ndefinition in a way so that the analysis is OK.",
    "start": "4707850",
    "end": "4713330"
  },
  {
    "text": "But in some sense,\nthis is-- in some sense because the proof\nis simple and clean, so somehow I feel good about\nthe definition to some extent.",
    "start": "4713330",
    "end": "4722320"
  },
  {
    "text": "Yeah. So I guess I'll use the next few\nminutes and the 4% of battery",
    "start": "4722320",
    "end": "4729150"
  },
  {
    "text": "to talk about some of the\ncomparisons, interpretations,",
    "start": "4729150",
    "end": "4734909"
  },
  {
    "text": "and our next\npossible extensions. So I think, I guess\ninterpretation,",
    "start": "4734910",
    "end": "4744890"
  },
  {
    "text": "I kind of in some sense I've\ndiscussed this a little bit. So the most important thing\nis the all-layer margin",
    "start": "4744890",
    "end": "4752030"
  },
  {
    "text": "part, at least that's our side. And we don't even\ncare about the norm.",
    "start": "4752030",
    "end": "4757110"
  },
  {
    "text": "So then you can compare\nwith Bartlett at all",
    "start": "4757110",
    "end": "4769520"
  },
  {
    "text": "'17 is the paper that\nwe discussed last time. So you can formally do\nthis, or you can formally",
    "start": "4769520",
    "end": "4775310"
  },
  {
    "text": "say that the perturbed\nmodel, if you look at the difference\nbetween the perturbed model and the original\nmodel, the difference",
    "start": "4775310",
    "end": "4782840"
  },
  {
    "text": "is something like if you do\nsome kind of telescoping thing.",
    "start": "4782840",
    "end": "4789989"
  },
  {
    "text": "This is supposed to\nbe not super hard. So you can basically\nimagine that for every layer",
    "start": "4789990",
    "end": "4796680"
  },
  {
    "text": "you perturb something, so\nyou pay something like that. And then you also have to pay\nthe blowing up factor because",
    "start": "4796680",
    "end": "4803309"
  },
  {
    "text": "of the other things. So you can prove this. ",
    "start": "4803310",
    "end": "4827780"
  },
  {
    "text": "So if you ignoring\nsome minor details, which allows me to have\na cleaner exposition.",
    "start": "4827780",
    "end": "4838989"
  },
  {
    "text": "So for example, you ignore\nthe dependency on r,",
    "start": "4838990",
    "end": "4846200"
  },
  {
    "text": "then you can basically\nsay that if-- maybe let's also suppose\ny is bigger than 0,",
    "start": "4846200",
    "end": "4853940"
  },
  {
    "text": "just for simplicity. Say y is 1. ",
    "start": "4853940",
    "end": "4859270"
  },
  {
    "text": "Then basically, if you want\nfx to be bigger than 0, and fx plus delta\nto be less than 0,",
    "start": "4859270",
    "end": "4866860"
  },
  {
    "text": "that's kind of what\nthe situation would be. You perturb the model to\npredict the wrong thing.",
    "start": "4866860",
    "end": "4873820"
  },
  {
    "text": "And then, this means\nthat your delta needs to be basically\nsomething larger than product",
    "start": "4873820",
    "end": "4883390"
  },
  {
    "text": "of the spectral norm. Because at least,\nbecause that's how",
    "start": "4883390",
    "end": "4891139"
  },
  {
    "text": "you can make enough difference. So if your delta is too small,\nthen the right-hand side",
    "start": "4891140",
    "end": "4896240"
  },
  {
    "text": "will be too small so\nthat you don't really make a big enough difference. So times fx.",
    "start": "4896240",
    "end": "4903224"
  },
  {
    "text": " So that's just saying that\nbasically in some sense",
    "start": "4903224",
    "end": "4910850"
  },
  {
    "text": "this is saying that mf xy over\ny times fx, the new margin",
    "start": "4910850",
    "end": "4918370"
  },
  {
    "text": "versus the old margin, the\nratio is something like this. ",
    "start": "4918370",
    "end": "4925510"
  },
  {
    "text": "I'm writing this in\nsomewhat informal way. So I'm ignoring constant\nor even ignoring",
    "start": "4925510",
    "end": "4930639"
  },
  {
    "text": "some small minor details. I think this product\nprobably shouldn't probably range from 1 to r, it should\nmiss some terms in the middle.",
    "start": "4930640",
    "end": "4938260"
  },
  {
    "text": "But those are not\nsuper important. And this is basically\nsaying that if you",
    "start": "4938260",
    "end": "4943750"
  },
  {
    "text": "look at the inverse margin,\nthis is kind of like f, fx times the product of\nthe spectral norm.",
    "start": "4943750",
    "end": "4950230"
  },
  {
    "text": " So this is indeed a\nbetter bound than before. Because our new bound\ndepends on this and old bond",
    "start": "4950230",
    "end": "4958380"
  },
  {
    "text": "depends on this, on\nthe right-hand side times the spectral norm. ",
    "start": "4958380",
    "end": "4964380"
  },
  {
    "text": "So this is a better bound.  At least in this aspect.",
    "start": "4964380",
    "end": "4970816"
  },
  {
    "text": "And another thing\nis that later--",
    "start": "4970816",
    "end": "4976655"
  },
  {
    "text": " but why this is, how\nmuch better it is right,",
    "start": "4976655",
    "end": "4984570"
  },
  {
    "text": "compared to the previous one? That's a question mark. So is it true that\nyour all-layer marginal",
    "start": "4984570",
    "end": "4990300"
  },
  {
    "text": "becomes polynomial\ninstead of exponential? There are some\nindicators that this",
    "start": "4990300",
    "end": "4996480"
  },
  {
    "text": "is a much better bound\nempirically, or conceptually. Empirically, we did verify\nit seems to be much better.",
    "start": "4996480",
    "end": "5003890"
  },
  {
    "text": "The number, it becomes smaller. Just because empirically\nyour Lipschitz",
    "start": "5003890",
    "end": "5009260"
  },
  {
    "text": "is better than the\nworst case bond. And another reason why\nyou can somewhat hope that the empirical-- this is better\nis because later we will show--",
    "start": "5009260",
    "end": "5018820"
  },
  {
    "text": "I think I've said\nthis once before. But let me write it down again. So SGD prefers Lipschitz\nsolutions, and in some sense--",
    "start": "5018820",
    "end": "5032429"
  },
  {
    "text": "on the data points, and\nLipschitz on the data points.",
    "start": "5032430",
    "end": "5037540"
  },
  {
    "text": "In some sense this is\nsaying that your algorithm in some sense is minimizing\nthe Lipschitz-ness on the data point.",
    "start": "5037540",
    "end": "5042840"
  },
  {
    "text": "So that's why your\nLipschitz-ness on the data point is probably better than\nthe worst case Lipschitz-ness",
    "start": "5042840",
    "end": "5048340"
  },
  {
    "text": "over the entire domain. And that's probably why the\ngap between these two bonds are significant.",
    "start": "5048340",
    "end": "5055270"
  },
  {
    "text": "So in some sense, this is saying\nyou are implicitly minimizing--",
    "start": "5055270",
    "end": "5062627"
  },
  {
    "text": "maximizing the all-layer margin. ",
    "start": "5062627",
    "end": "5073800"
  },
  {
    "text": "So but of course,\nthis is approximately. Because all of this what SGD\nprefers-- in terms of the form,",
    "start": "5073800",
    "end": "5084099"
  },
  {
    "text": "we will see it's similar. But they won't be exactly\nmatching the same form. So we haven't got a kind of\nfully coherent theory yet.",
    "start": "5084100",
    "end": "5095250"
  },
  {
    "text": "But conceptually, it all\nseems to roughly match. And another thing is\nthat there is something",
    "start": "5095250",
    "end": "5101580"
  },
  {
    "text": "which actually people\nactually use in practice, which is called SAM. This is called sharpness\naware regularization.",
    "start": "5101580",
    "end": "5112603"
  },
  {
    "text": "This is something\nthat can let you two get better performance\nempirically on many data sets. And what they are doing is that\nthey are doing perturbation.",
    "start": "5112603",
    "end": "5122068"
  },
  {
    "text": "So we are doing a perturbation. But they are also\ndoing a perturbation, but they are perturbing\nthe parameter theta.",
    "start": "5122068",
    "end": "5129449"
  },
  {
    "text": "So they are trying to make\nthis model more Lipschitz in the parameter theta instead\nof more Lipschitz in the hidden",
    "start": "5129450",
    "end": "5136920"
  },
  {
    "text": "variable, the intermediate\nvariable h i's. But actually, these\ntwo are very related.",
    "start": "5136920",
    "end": "5143290"
  },
  {
    "text": "So here is a fact. If you look at the loss, the\ngradient of the loss respect to the parameter wi, this\nis equals to the gradient",
    "start": "5143290",
    "end": "5155030"
  },
  {
    "text": "of loss respect to-- I'm always-- this is\non a single example, always on a single example.",
    "start": "5155030",
    "end": "5162300"
  },
  {
    "text": "This is the equals to\nthe gradient of the loss respects to the hidden\nvariables in the layer above",
    "start": "5162300",
    "end": "5174060"
  },
  {
    "text": "and times the hidden\nvariable transposed. So this is just by derivation.",
    "start": "5174060",
    "end": "5181770"
  },
  {
    "text": "Actually, this is\nused to called-- wait, I'm blanking on the name.",
    "start": "5181770",
    "end": "5188790"
  },
  {
    "text": "In neuroscience, there's\nactually a term for this thing. But this is just\nliterally, just you",
    "start": "5188790",
    "end": "5194000"
  },
  {
    "text": "compute a gradient of wi, how\nyou do it, you use change 1, you get this. So here, this is the\ngradient respect r1,",
    "start": "5194000",
    "end": "5201239"
  },
  {
    "text": "this is the size\nof the environment. So if you look-- so\nthat's why if you look at the norm of the gradient\nin respect to the perimeter,",
    "start": "5201240",
    "end": "5209330"
  },
  {
    "text": "then it's quite related to\nthe norm of the gradient with respect to hidden variable. ",
    "start": "5209330",
    "end": "5217880"
  },
  {
    "text": "This is a vector, this is a\nvector, and this is a matrix. So that's why this is true.",
    "start": "5217880",
    "end": "5223980"
  },
  {
    "text": "So Lipschitz-ness in parameter\nis similar to Lipschitz-ness in hidden variable. ",
    "start": "5223980",
    "end": "5233283"
  },
  {
    "text": "It's somewhat related. ",
    "start": "5233283",
    "end": "5242810"
  },
  {
    "text": "So the last thing-- I guess I'm running out of time. Sorry. So this is a more\ngeneral version,",
    "start": "5242810",
    "end": "5250340"
  },
  {
    "text": "where you don't have to care\nabout the minimum margin over the entire data set. You can prove something like\ntest error is less than 1",
    "start": "5250340",
    "end": "5259929"
  },
  {
    "text": "over square root 1 times-- instead of the average,\nthis is average margin",
    "start": "5259930",
    "end": "5265680"
  },
  {
    "text": "instead of the\nworst case margin-- the minimum margin\nover the data set. So you look at average inverse\nmargin of this form square.",
    "start": "5265680",
    "end": "5281449"
  },
  {
    "text": "And then times the sum of\ncomplexes of each layer.",
    "start": "5281450",
    "end": "5289835"
  },
  {
    "text": " So and plus low order term.",
    "start": "5289835",
    "end": "5296644"
  },
  {
    "start": "5296644",
    "end": "5303280"
  },
  {
    "text": "Oh, really? Oh, of course. It can really-- OK, 5% is not enough.",
    "start": "5303280",
    "end": "5309080"
  },
  {
    "text": "But this is literally the\nlast thing I want to say. Yeah.",
    "start": "5309080",
    "end": "5315310"
  },
  {
    "text": "But maybe let me-- are there any questions? ",
    "start": "5315310",
    "end": "5323630"
  },
  {
    "text": "So basically, the last\nthing I want to say is that instead of having the\nminimum, like all-layer margin",
    "start": "5323630",
    "end": "5329130"
  },
  {
    "text": "there, you can have the\naverage all-layer margin. Cool. ",
    "start": "5329130",
    "end": "5336200"
  },
  {
    "text": "OK. Any questions? ",
    "start": "5336200",
    "end": "5347250"
  },
  {
    "text": "OK. I guess then see you next\nMonday, oh, Wednesday, in two",
    "start": "5347250",
    "end": "5353086"
  },
  {
    "text": "days. Wait, today's Monday. Right. Yeah, OK. See you. Bye. Thanks.",
    "start": "5353086",
    "end": "5358960"
  },
  {
    "start": "5358960",
    "end": "5364000"
  }
]