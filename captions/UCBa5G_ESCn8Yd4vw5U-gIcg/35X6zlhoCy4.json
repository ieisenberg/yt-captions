[
  {
    "start": "0",
    "end": "5570"
  },
  {
    "text": "Good evening, people. How are you guys doing?",
    "start": "5570",
    "end": "11120"
  },
  {
    "text": "All right. My name is Richard Sherman. I'm a PhD student at Stanford,\nand I'm very, very excited",
    "start": "11120",
    "end": "16189"
  },
  {
    "text": "to talk about\nposttraining generally speaking for large\nlanguage models, and I hope you guys are ready\nto learn some stuff because this",
    "start": "16190",
    "end": "23930"
  },
  {
    "text": "has been one of\nthe last few years in machine learning\nhave been very, very exciting with the advent of\nlarge language models, ChatGPT,",
    "start": "23930",
    "end": "30770"
  },
  {
    "text": "and everything to that extent. And, hopefully, after\ntoday's lecture, you will be more\ncomfortable understanding",
    "start": "30770",
    "end": "36860"
  },
  {
    "text": "how we go from pretrained\nmodels to models like ChatGPT. And we'll take a whole journey\nthrough prompting instruction,",
    "start": "36860",
    "end": "43710"
  },
  {
    "text": "fine-tuning, and deep RLHF. So let's get started. ",
    "start": "43710",
    "end": "52399"
  },
  {
    "text": "All right. So something that has been very\nfundamental to our entire field",
    "start": "52400",
    "end": "61340"
  },
  {
    "text": "is this idea of scaling laws,\nand models are increasingly becoming larger and larger,\nand they're expanding",
    "start": "61340",
    "end": "67460"
  },
  {
    "text": "more and more compute. So this is a graph of models\nstarting all the way back in 1950s to somewhere around--",
    "start": "67460",
    "end": "74190"
  },
  {
    "text": "these are still-- this\nis an outdated graph. So this shows up to\n10 to power 24 flops or floating point\noperations that",
    "start": "74190",
    "end": "80450"
  },
  {
    "text": "go into pretraining\nthese models, but the number is well\nabove 10 to power 26 now. But you can see the graph\nand the way it's trending.",
    "start": "80450",
    "end": "88670"
  },
  {
    "text": "And more and more compute\nrequires more and more data because you need to train\non something meaningful.",
    "start": "88670",
    "end": "93870"
  },
  {
    "text": "And this is roughly the trend\non the amount of language tokens that are going into the\nlanguage models and pretraining.",
    "start": "93870",
    "end": "99810"
  },
  {
    "text": "And, again, this\nplot is outdated. Does anybody want to guess\nlike we're in 2024, 2022",
    "start": "99810",
    "end": "106290"
  },
  {
    "text": "we were at $1.4 trillion tokens\nor words, roughly speaking, in language model pretraining?",
    "start": "106290",
    "end": "112350"
  },
  {
    "text": "Does anyone want to guess\nwhere we are in 2024?",
    "start": "112350",
    "end": "117488"
  },
  {
    "text": "[INAUDIBLE] That's a pretty good guess. Yeah, so we're close\nto 15 trillion tokens.",
    "start": "117488",
    "end": "124660"
  },
  {
    "text": "Recent Llama 3\nmodels were roughly trained on 15 trillion tokens. So, yeah, just for\na second, appreciate",
    "start": "124660",
    "end": "131860"
  },
  {
    "text": "that these are a lot of words. And this is not-- yeah, I don't\nthink anybody of us",
    "start": "131860",
    "end": "137560"
  },
  {
    "text": "listens to trillions of\ntokens in our lifetime. So this is where\nwe are right now.",
    "start": "137560",
    "end": "142850"
  },
  {
    "text": "And I hope you guys were here\nfor the pretraining lectures. Cool.",
    "start": "142850",
    "end": "148780"
  },
  {
    "text": "So what do we do? So, I mean, broadly\nspeaking, we are really just learning to predict text\ntokens or language tokens.",
    "start": "148780",
    "end": "156440"
  },
  {
    "text": "But what do we learn in\nthe process of pretraining? Why are people spending so\nmuch money, so much compute--",
    "start": "156440",
    "end": "163130"
  },
  {
    "text": "because these compute and\ntokens take dollars to do, and we're on the order of\nspending hundreds of millions",
    "start": "163130",
    "end": "168970"
  },
  {
    "text": "of dollars on these runs,\nso why are we doing this? And this is basically a recall\nfrom whatever you've probably",
    "start": "168970",
    "end": "175780"
  },
  {
    "text": "learned till now, but we are\nlearning things like, oh, we are learning knowledge. Stanford University is located\nin Santa Clara or California",
    "start": "175780",
    "end": "183280"
  },
  {
    "text": "or wherever you want to,\nsay, you're learning syntax, you're learning semantics\nof the sentences.",
    "start": "183280",
    "end": "188597"
  },
  {
    "text": "These are things\nthat you would expect to learn when you're\ntraining on language data. Broadly, you're\nprobably learning",
    "start": "188597",
    "end": "195070"
  },
  {
    "text": "a lot about different\nlanguages as well, so depending on your\ntext data distribution, you're learning a lot of things.",
    "start": "195070",
    "end": "200360"
  },
  {
    "text": "But the models we interact\nwith are very intelligent, so where is that coming from?",
    "start": "200360",
    "end": "205490"
  },
  {
    "text": "I mean, just simply learning\nabout very factual things. And it's a very simple loss\nfunction we're optimizing,",
    "start": "205490",
    "end": "212930"
  },
  {
    "text": "and where is that\nintelligence coming from? And this perhaps is\nthe interesting bit.",
    "start": "212930",
    "end": "220330"
  },
  {
    "text": "Recently, people have started\naccumulating evidence for that. When you optimize the next\ntoken prediction losses,",
    "start": "220330",
    "end": "228320"
  },
  {
    "text": "you're not just\nlearning about syntax, you're not just\nlearning knowledge, but you're starting to form\nmodels of agents, beliefs,",
    "start": "228320",
    "end": "235220"
  },
  {
    "text": "and actions, as well. So how do we know this? Again, a lot of this is\nspeculative evidence,",
    "start": "235220",
    "end": "240650"
  },
  {
    "text": "but it's maybe to\nform an understanding that the losses we're optimizing\nare not just about the data fitting the data, but you start\nlearning something maybe more",
    "start": "240650",
    "end": "247540"
  },
  {
    "text": "meaningful as well. For example, I mean,\nin this specific case,",
    "start": "247540",
    "end": "253765"
  },
  {
    "text": "we changed the last\nsentence, and the prediction of the text or the next\ntext that is predicted",
    "start": "253765",
    "end": "259479"
  },
  {
    "text": "changes as well. So here it starts with Pat which\nis a demonstration of a bowling ball, and the leaf being\ndropped at the same time.",
    "start": "259480",
    "end": "266330"
  },
  {
    "text": "Pad, who is a\nphysicist, predicts that the bowling ball and the\nleaf will land at the same rate. We all know gravity\nthe way it works,",
    "start": "266330",
    "end": "273380"
  },
  {
    "text": "but when you change the last\nsentence to Pat, who has never seen this demonstration\nbefore, Pat",
    "start": "273380",
    "end": "279370"
  },
  {
    "text": "predicts that bowling ball\nwill fall to the ground first. Maybe somebody who's\nnever seen this experiment before might intuitively\nbelieve that.",
    "start": "279370",
    "end": "285820"
  },
  {
    "text": "Correct. So, I mean, the language model\nwas able to predict this. So how do you predict this?",
    "start": "285820",
    "end": "291110"
  },
  {
    "text": "You have to have some\nnotion of understanding of how humans work to even\nbe able to predict this.",
    "start": "291110",
    "end": "297230"
  },
  {
    "text": "And that's maybe something\nthat is not obvious when you're simply optimizing\nto predict the text.",
    "start": "297230",
    "end": "304000"
  },
  {
    "text": "Similarly, I mean,\nthese kind of examples, we're going to run\nthrough some examples to communicate that when\nyou're pretraining these models",
    "start": "304000",
    "end": "309940"
  },
  {
    "text": "you're learning much more\nthan just language tokens and so on, you're also\nlearning about math.",
    "start": "309940",
    "end": "315220"
  },
  {
    "text": "You're able to understand\nwhat a graph of a circle means and what the center is, and\nhow to understand equations.",
    "start": "315220",
    "end": "322750"
  },
  {
    "text": "Probably, my favorite\nexample, something I use pretty much\nevery day, is you're learning how to write code.",
    "start": "322750",
    "end": "328310"
  },
  {
    "text": "So I don't know how many of you\nhave interacted with Copilot before, but if you\nhave, you probably",
    "start": "328310",
    "end": "334240"
  },
  {
    "text": "know if you write down a few\ncommands, write down a function template, it will automatically\ncomplete code for you.",
    "start": "334240",
    "end": "339860"
  },
  {
    "text": "So again, it's not\nperfect, but it has to have some deeper\nunderstanding of what",
    "start": "339860",
    "end": "345070"
  },
  {
    "text": "your intent is for something\nlike that to emerge. And similarly, we have\nexamples from medicine as well.",
    "start": "345070",
    "end": "351470"
  },
  {
    "text": "I don't know about you guys,\nbut whenever I have some issue, I probably go to\nChatGPT or Claude or something to that\neffect and ask them",
    "start": "351470",
    "end": "357669"
  },
  {
    "text": "for a diagnosis for\nthose things as well. I don't recommend that.",
    "start": "357670",
    "end": "363370"
  },
  {
    "text": "Please don't take medical\nadvice from me, but, yeah. So broadly, the way we're seeing\nlanguage models at this point",
    "start": "363370",
    "end": "371289"
  },
  {
    "text": "is that they're sort of emerging\nas this general purpose, multi-task assistance. And it's very strange.",
    "start": "371290",
    "end": "376987"
  },
  {
    "text": "I mean, we started off\nwith text token prediction, and we're reaching\nthe stage where we can rely on them to do\nmany, many different things.",
    "start": "376987",
    "end": "383840"
  },
  {
    "text": "So how are we getting there? I'm sure you all are aware of\nwhat these models are, so yeah.",
    "start": "383840",
    "end": "389740"
  },
  {
    "text": "So today's lecture\nis largely going to be about, how do\nwe go from something Stanford University is located.",
    "start": "389740",
    "end": "396400"
  },
  {
    "text": "This very simple pretraining\ntask, a very simple procedure. Well, it's more complicated,\nbut in abstract terms,",
    "start": "396400",
    "end": "401949"
  },
  {
    "text": "it's not very complicated to\ndo like something as powerful as ChatGPT. Cool.",
    "start": "401950",
    "end": "407110"
  },
  {
    "text": "So I recommend you guys stopping\nme asking me a lot of questions",
    "start": "407110",
    "end": "412169"
  },
  {
    "text": "because there's a\nlot of fun examples and a lot of fun techniques,\nso I want you guys to learn everything about here.",
    "start": "412170",
    "end": "419139"
  },
  {
    "text": "So the overall\nplan is we're going to talk about zero shot and\nfew shot in context learning.",
    "start": "419140",
    "end": "424770"
  },
  {
    "text": "Next, we're going to follow up\nwith instruction fine-tuning, and then we're going to\ntalk about optimizing for preferences, and this is\nwhere roughly things are right",
    "start": "424770",
    "end": "432600"
  },
  {
    "text": "now in the industry,\nand then we're going to talk about what's\nnext, what the limitations are, and how do we move from here?",
    "start": "432600",
    "end": "438650"
  },
  {
    "text": " Cool. So we're going to start off\nwith zero shot and few shot",
    "start": "438650",
    "end": "443970"
  },
  {
    "text": "in context learning.  Broadly, we're going to take an\nexample of GPT or the Generative",
    "start": "443970",
    "end": "451230"
  },
  {
    "text": "Pretrained Transformer, and\nthis is a whole series of models that started off in roughly\n2018, and like up to 2020,",
    "start": "451230",
    "end": "456940"
  },
  {
    "text": "they were building\nGPT, GPT 2, GPT 3. So we're going to start\noff with this example, and yes, so it's a\ndecoder-only model that",
    "start": "456940",
    "end": "465360"
  },
  {
    "text": "is trained on roughly\n4.6 gigabytes of text, and it has 12 layers\nof transformers layers,",
    "start": "465360",
    "end": "471750"
  },
  {
    "text": "and it's trained with the\nnext token prediction loss. And the first model, obviously,\nwas not extremely good,",
    "start": "471750",
    "end": "478440"
  },
  {
    "text": "but it started\nshowing that, hey, this technique for\npretraining can be very effective for\ngeneral purpose tasks,",
    "start": "478440",
    "end": "485069"
  },
  {
    "text": "and we're going to\nsee some examples. For example, I mean,\nhere it's able to do",
    "start": "485070",
    "end": "490490"
  },
  {
    "text": "the task for entertainment,\nand GPT-1 itself was not",
    "start": "490490",
    "end": "501169"
  },
  {
    "text": "very strong as a model, but\nthey took the same recipe and, I mean, tried to\nincrease the model size.",
    "start": "501170",
    "end": "506850"
  },
  {
    "text": "So they went from 117\nmillion parameters to about 1.5 billion parameters. And we're now scaling up\nthe data alongside as well.",
    "start": "506850",
    "end": "514292"
  },
  {
    "text": "So we went from 4\ngigabytes of data to approximately 40\ngigabytes of data. And pretraining is a\nwhole different melting",
    "start": "514292",
    "end": "521027"
  },
  {
    "text": "pot of techniques, and there's\na lot that goes into it, but roughly, for\nexample, here, they filter data by the number of\nupvotes on the Reddit data.",
    "start": "521028",
    "end": "528920"
  },
  {
    "text": "And, yeah, so this is\nroughly where we are. And I think one of the things\nthat started emerging with GPT-2",
    "start": "528920",
    "end": "536089"
  },
  {
    "text": "is zero shot learning. And what do we mean\nby zero shot learning?",
    "start": "536090",
    "end": "542990"
  },
  {
    "text": "Conventionally, in the field,\nwhen we pretrained models, there was the idea that\nyou take a few examples,",
    "start": "542990",
    "end": "548280"
  },
  {
    "text": "you update the\nmodel, and then you are able to adapt\nto a specific task. But as you pretrain on more\nand more data and more and more",
    "start": "548280",
    "end": "556040"
  },
  {
    "text": "tasks, you start\nseeing this phenomenon where they're able to do the\ntask basically zero shot.",
    "start": "556040",
    "end": "561420"
  },
  {
    "text": "They're shown no examples\nof how to do the task, and you can start thinking about\nhow you can do it summarization,",
    "start": "561420",
    "end": "566630"
  },
  {
    "text": "you can follow\nsome instructions, you can do maybe a little\nbit of math as well. So this is where the idea\nof zero shot learning",
    "start": "566630",
    "end": "573410"
  },
  {
    "text": "started to emerge. ",
    "start": "573410",
    "end": "578660"
  },
  {
    "text": "Yeah, so how do we\ndo zero shot learning or task specific learning\nfrom these pretrained models?",
    "start": "578660",
    "end": "584310"
  },
  {
    "text": "Really, the idea is like we\nhave to be creative here. We know that these are\ntext prediction models. If you put in a text, they\nwill complete whatever follows.",
    "start": "584310",
    "end": "592290"
  },
  {
    "text": "So if we can coerce these\nmodels into completing the task we care about, maybe\nit's question-answering,",
    "start": "592290",
    "end": "598079"
  },
  {
    "text": "we can start getting\nthem to solve tasks here. So, for example, if you want to\nask questions about Tom Brady,",
    "start": "598080",
    "end": "606030"
  },
  {
    "text": "you set it up. You put information\nabout Tom Brady, and then you put a question\nthat you wanted to answer,",
    "start": "606030",
    "end": "611790"
  },
  {
    "text": "and then it will\nautocomplete in some sense. So this is one early\nperspective on these models. These are very advanced\nautocomplete models.",
    "start": "611790",
    "end": "619670"
  },
  {
    "text": "And, similarly, if you want to\nfigure out which answer is true or which is not, something\nthat is very useful to measure",
    "start": "619670",
    "end": "626990"
  },
  {
    "text": "is log probabilities. So, for example, we\nwant to figure out",
    "start": "626990",
    "end": "632300"
  },
  {
    "text": "what is the word it referring\nto here in this sentence. The cat couldn't fit into the\nhat because it was too big.",
    "start": "632300",
    "end": "639350"
  },
  {
    "text": "What we can do is we\ncan take the sentence, replace it with either\nthe cat or either the hat,",
    "start": "639350",
    "end": "645080"
  },
  {
    "text": "and then you can measure\nthe probability of which one does the model think\nis higher, and you",
    "start": "645080",
    "end": "650950"
  },
  {
    "text": "can get the idea of what\nthe reference here is. So none of this is like\nin the training data.",
    "start": "650950",
    "end": "657440"
  },
  {
    "text": "It's simply learning\nto predict text. But you can start\nseeing how we can leverage these models\nto do other tasks as",
    "start": "657440",
    "end": "663370"
  },
  {
    "text": "well besides prediction. So this is just more\nevidence about how GPT-2,",
    "start": "663370",
    "end": "671290"
  },
  {
    "text": "no task-specific fine-tuning,\nno task-specific training. It simply is learning\nto predict text,",
    "start": "671290",
    "end": "676880"
  },
  {
    "text": "and it's established\nas the state of the art on many,\nmany different tasks simply by scaling up\nthe model parameters",
    "start": "676880",
    "end": "683560"
  },
  {
    "text": "and scaling up the amount\nof data it's trained on. ",
    "start": "683560",
    "end": "688930"
  },
  {
    "text": "So this is a fun example. So if you want to do\nsummarization for data",
    "start": "688930",
    "end": "695230"
  },
  {
    "text": "or you have a news article\nthat you want to summarize, so how do you get a\nzero-shot model to do it?",
    "start": "695230",
    "end": "701150"
  },
  {
    "text": "The answer is you put the\ndocument into the context, and you simply put\nTLDR in front of it.",
    "start": "701150",
    "end": "707890"
  },
  {
    "text": "Now, I mean, most of the data in\ninternet, whenever you see TLDR, you'll naturally\nsummarize it, so yeah, you",
    "start": "707890",
    "end": "714735"
  },
  {
    "text": "can get 0 shot performance on\nsummarization here as well. And, again, this is not\ntrained to do summarization in any specific\nway, and it's still",
    "start": "714735",
    "end": "721840"
  },
  {
    "text": "doing really well simply because\nof it's pretraining data. ",
    "start": "721840",
    "end": "727959"
  },
  {
    "text": "So, yeah, I think GPT to\nTLDR is somewhere there, and some of the very\ntask-specific trained models",
    "start": "727960",
    "end": "734110"
  },
  {
    "text": "are. And I think you will\nsee the trend with-- again, if you were Alec\nRadford or somebody,",
    "start": "734110",
    "end": "741220"
  },
  {
    "text": "I mean, you see these\ncool things emerging, your next step\nwould obviously be, I'm going to scale\nthis up a little more.",
    "start": "741220",
    "end": "746955"
  },
  {
    "text": "I'm going to make an\neven bigger model. I'm going to train\nit on even more data, and we'll see how things go.",
    "start": "746955",
    "end": "752839"
  },
  {
    "text": "So that's how we got GPT-3. We went from 1.5\nbillion parameters to 175 billion parameters.",
    "start": "752840",
    "end": "758780"
  },
  {
    "text": "We are well over 40 gigabytes of\ndata to 600 gigabytes of data. Of course, now we're in\nlike terabytes of data,",
    "start": "758780",
    "end": "766370"
  },
  {
    "text": "and text is a very\ncompressed representation, so terabytes of data is a lot.",
    "start": "766370",
    "end": "771970"
  },
  {
    "text": "And we talked about\nzero shot learning. The cool thing that\nemerged in GPT-3 is--",
    "start": "771970",
    "end": "778180"
  },
  {
    "text": "go ahead. So, typically, [INAUDIBLE]\nbefore the passage [INAUDIBLE]. No, you typically\nput the passage--",
    "start": "778180",
    "end": "784540"
  },
  {
    "text": "if you have interacted with\nReddit or something like that, typically, somebody will\nwrite an entire post and then end with TLDR.",
    "start": "784540",
    "end": "791680"
  },
  {
    "text": "Here's a summary of the thing. Too long didn't read,\nor if you have used-- What about something like\nopposite data comes first.",
    "start": "791680",
    "end": "799394"
  },
  {
    "text": "Oh, yeah, there are situations\nwhere it also comes first, but one reason is that these\nare decoder-only models,",
    "start": "799395",
    "end": "805850"
  },
  {
    "text": "so they are-- often these\nare causal attention models, so they typically need to\nsee the context before.",
    "start": "805850",
    "end": "810940"
  },
  {
    "text": "Yeah, that part I understand. I'm just curious from my\nexperience at TLDR comes first,",
    "start": "810940",
    "end": "816560"
  },
  {
    "text": "and how is it even [INAUDIBLE]? ",
    "start": "816560",
    "end": "822390"
  },
  {
    "text": "There's probably a lot of data\nwhere the TLDR comes first, but there's probably\na lot of data where TLDR comes after as well.",
    "start": "822390",
    "end": "828720"
  },
  {
    "text": "Cool so we saw zero shot\nlearning emerging in GPT-2. Few shot learning maybe\nseems slightly easier,",
    "start": "828720",
    "end": "835312"
  },
  {
    "text": "but this is where things\nstarted getting really funny, is that you're starting\nto beat state of the art simply by just putting\nexamples in context.",
    "start": "835312",
    "end": "842920"
  },
  {
    "text": "So, yeah, what does few\nshot learning mean here? What are we talking about?",
    "start": "842920",
    "end": "849210"
  },
  {
    "text": "As I mentioned, the\ntypical idea here is that you want to\nsolve translation.",
    "start": "849210",
    "end": "855250"
  },
  {
    "text": "So you would put some examples\nof translation into context. And this is a\ncorrection task here",
    "start": "855250",
    "end": "861917"
  },
  {
    "text": "or maybe you want to be\ninterested in translation, and no gradient\nupdates, no learning",
    "start": "861917",
    "end": "867480"
  },
  {
    "text": "in any conventional\nsense whatsoever. You put a few examples\nin, and that's it.",
    "start": "867480",
    "end": "872560"
  },
  {
    "text": "I mean, you know how\nto solve the task. Isn't that crazy? You guys did the\nassignment on translation,",
    "start": "872560",
    "end": "880580"
  },
  {
    "text": "but this is what the\nmodern NLP looks like. So, yeah.",
    "start": "880580",
    "end": "886710"
  },
  {
    "text": "You put in some examples and\nyou have the entire system, and this is where things\ngot really interesting,",
    "start": "886710",
    "end": "892120"
  },
  {
    "text": "is that all these task\nspecific models that were created to be really,\nreally good at translation or really good at summarization,\nyou can just put--",
    "start": "892120",
    "end": "900089"
  },
  {
    "text": "let's look at this graph. So we start with the\nzero shot performance of this in a similar fashion\nthat I described earlier,",
    "start": "900090",
    "end": "905890"
  },
  {
    "text": "And you start somewhere there. You put one example in of\ntranslation from English to French, you get to\nsomewhere like already",
    "start": "905890",
    "end": "911760"
  },
  {
    "text": "at a fine-tuned level. A few examples\nin, you're already starting to be close to the\nstate of the art models.",
    "start": "911760",
    "end": "919105"
  },
  {
    "text": "But in that graph,\nthe state of the art is really high, isn't it? The fine-tuned [INAUDIBLE]. Fine-tuned BERT\nof-- the BERT+ here,",
    "start": "919105",
    "end": "926350"
  },
  {
    "text": "I think is the one\nI'm referring to. Find your state-of-the-art--\nwe're just trained exclusively on a lot of--",
    "start": "926350",
    "end": "931920"
  },
  {
    "text": "translation data might\nbe slightly better, yes. And I think that's the\nrelevant comparison",
    "start": "931920",
    "end": "938628"
  },
  {
    "text": "here is just the\nin-context learning starts to emerge at scale. ",
    "start": "938628",
    "end": "944180"
  },
  {
    "text": "And this is, I think, the key\npoint is that some of this is contested, just\nto be very upfront,",
    "start": "944180",
    "end": "950480"
  },
  {
    "text": "but there's this idea of\nemergence of this property as you train on more\ncompute and more scale.",
    "start": "950480",
    "end": "955712"
  },
  {
    "text": "There's more recent\nresearch which suggests that if you plot\nthe x-axis correctly, it feels less emergent,\nbut the general idea",
    "start": "955713",
    "end": "962530"
  },
  {
    "text": "is as you increase the\nnumber of parameters and increase the\nnumber of compute that is going into the\nmodels, the ability",
    "start": "962530",
    "end": "968620"
  },
  {
    "text": "to just go from a few examples\nto really strong performance is very compelling. ",
    "start": "968620",
    "end": "975790"
  },
  {
    "text": "Cool. And, yeah, I think\nas I explained earlier, the general\nidea is that this",
    "start": "975790",
    "end": "982092"
  },
  {
    "text": "is very different from the\nconventional idea of fine-tuning that we typically go for. Instead of iterating\nover examples,",
    "start": "982092",
    "end": "987890"
  },
  {
    "text": "putting it into context,\nand doing gradient updates, we are actually just going\nfor a few shot prompting.",
    "start": "987890",
    "end": "993000"
  },
  {
    "text": "We're going to put\nin a few examples and that's going to\ngive us the system. Go ahead. To clarify your\nprepared [INAUDIBLE]",
    "start": "993000",
    "end": "1000550"
  },
  {
    "text": "examples [INAUDIBLE]. Yes, I mean, the\nexact details roughly",
    "start": "1000550",
    "end": "1006009"
  },
  {
    "text": "can depend on the prompt\ntemplate that you use, but typically you would\njust put examples. So like sea otter and\nput these examples,",
    "start": "1006010",
    "end": "1013760"
  },
  {
    "text": "and then whatever your\ntask is, you can just let the model complete\nfrom there because it can infer the task based on\nthe examples you've given.",
    "start": "1013760",
    "end": "1023320"
  },
  {
    "text": "Any other questions? Cool. ",
    "start": "1023320",
    "end": "1028959"
  },
  {
    "text": "So, I mean, we have gotten\nfrom zero shot prompting, and we've seen we're seeing that\nfew shot prompting is becoming",
    "start": "1028960",
    "end": "1036310"
  },
  {
    "text": "really competitive\nwith good models, but there's still\nlimitations to this. I mean, you cannot solve\nevery task that you see here,",
    "start": "1036310",
    "end": "1042380"
  },
  {
    "text": "and particularly things that\ninvolve richer multi-step reasoning is something\nthat actually can be pretty",
    "start": "1042380",
    "end": "1047470"
  },
  {
    "text": "challenging. And just to be fair, humans\nstruggle at these tasks as well. So things like\naddition and so on,",
    "start": "1047470",
    "end": "1055495"
  },
  {
    "text": "these are probably\nstill hard to do when you keep increasing\nthe number of digits. But, one thing that you have\nto start being creative with,",
    "start": "1055495",
    "end": "1064400"
  },
  {
    "text": "I alluded to this\nearlier, is that you can get these models to\ndo the task if you're creative in how you\nprompt the model,",
    "start": "1064400",
    "end": "1071270"
  },
  {
    "text": "and this is what we're\ngoing to see next. So this technique called\nchain of thought prompting",
    "start": "1071270",
    "end": "1078160"
  },
  {
    "text": "emerged here, and the idea\nthat we have explored thus far is that we put in examples\nof the tasks we want to do,",
    "start": "1078160",
    "end": "1086740"
  },
  {
    "text": "and we expect the\nmodel to zero-shot learn what the task\nis and go from there.",
    "start": "1086740",
    "end": "1092380"
  },
  {
    "text": "The idea is that instead of\njust showing what the task is, you show them examples where\nthey reason through the task.",
    "start": "1092380",
    "end": "1098720"
  },
  {
    "text": "So they're not just\nlearning to do the task, but also learning how\nthe reasoning is working. So in this example,\ninitially, we",
    "start": "1098720",
    "end": "1104289"
  },
  {
    "text": "started with we have to\nsolve a simple math problem, and we are just shown\nexactly the answer directly.",
    "start": "1104290",
    "end": "1109780"
  },
  {
    "text": "Instead of doing that, and\nif you do that directly, you'll observe that the\nmodel gets the answer wrong.",
    "start": "1109780",
    "end": "1115070"
  },
  {
    "text": "Instead of that, what\nif you show model how to reason about the task,\nshow it a chain of thought,",
    "start": "1115070",
    "end": "1120340"
  },
  {
    "text": "and include that in\nthe prompt as well? And then you ask\nit a new question.",
    "start": "1120340",
    "end": "1125810"
  },
  {
    "text": "The idea is that now\nthe model is not just going to output an answer, it's\ngoing to reason about the task,",
    "start": "1125810",
    "end": "1131960"
  },
  {
    "text": "and it's going to do\nactually a lot better. And this has been shown\nto be very effective.",
    "start": "1131960",
    "end": "1137980"
  },
  {
    "text": "Chain of thought is also,\nas you can see, I mean, it's again something that\nimproves a lot with model scale,",
    "start": "1137980",
    "end": "1145190"
  },
  {
    "text": "it's not just--  but what you can\nprobably start seeing",
    "start": "1145190",
    "end": "1150370"
  },
  {
    "text": "is it's nearly better than\nsupervised best models here. So palm models roughly were\nabout 540 billion parameters,",
    "start": "1150370",
    "end": "1159470"
  },
  {
    "text": "and simply with this\nchain of thought skill, you're already beating\nstate-of-the-art.",
    "start": "1159470",
    "end": "1165785"
  },
  {
    "text": "Cool. ",
    "start": "1165785",
    "end": "1171010"
  },
  {
    "text": "So I showed you examples of\nchain of thought reasoning up to this point where you\ngo through a reasoning chain,",
    "start": "1171010",
    "end": "1176410"
  },
  {
    "text": "but you can be even\nslightly smarter than that. You might not even need\nto show them any examples. You just need to trick them into\nthinking about what to do next.",
    "start": "1176410",
    "end": "1185910"
  },
  {
    "text": "So, yeah, this idea emerged in\nthis paper called Let's think",
    "start": "1185910",
    "end": "1192090"
  },
  {
    "text": "step by step, where instead\nof even showing an example, you just start your answer\nwith let's think step by step.",
    "start": "1192090",
    "end": "1199870"
  },
  {
    "text": "And that's it. I mean, the model\nwill start reasoning about the answer\nitself instead of just",
    "start": "1199870",
    "end": "1206400"
  },
  {
    "text": "autocompleting to an answer,\nand you get something like this.",
    "start": "1206400",
    "end": "1211500"
  },
  {
    "text": "So maybe you don't even\nneed to show any examples. You can probably induce\nthe reasoning behavior",
    "start": "1211500",
    "end": "1218010"
  },
  {
    "text": "through zero shot\nbehavior as well. And, again, what\nthe final numbers",
    "start": "1218010",
    "end": "1223350"
  },
  {
    "text": "look like is compared to zero\nshot performance that we got from essentially autocompleting,\nthis zero shot chain of thought",
    "start": "1223350",
    "end": "1230880"
  },
  {
    "text": "substantially improves\nthe performance. So you go from 17.7 to 78.7. It's still worse than\nstill putting examples",
    "start": "1230880",
    "end": "1238260"
  },
  {
    "text": "of reasoning and multi-shot, few\nshot chain of thought as well. But you can see how much it\nimproves the performance simply",
    "start": "1238260",
    "end": "1244919"
  },
  {
    "text": "by asking it to let's think\nabout it step by step. And maybe this is a lesson that\ninteracting with these models",
    "start": "1244920",
    "end": "1251160"
  },
  {
    "text": "is when you interact\nwith these models, you might not get the\nexact desired behavior",
    "start": "1251160",
    "end": "1256740"
  },
  {
    "text": "from these models up front,\nbut often, these models are capable of doing the\nbehavior that you might want.",
    "start": "1256740",
    "end": "1263860"
  },
  {
    "text": "And often you have to\nthink about how to induce that behavior such that-- and the right way\nto think perhaps",
    "start": "1263860",
    "end": "1269730"
  },
  {
    "text": "is, what is the\npretraining data? What is some data on the\ninternet it might have seen, which induces a similar\nbehavior to the kind I want?",
    "start": "1269730",
    "end": "1276390"
  },
  {
    "text": "And you probably want\nto think about that and then induce these kinds of\nbehaviors from those models.",
    "start": "1276390",
    "end": "1282575"
  },
  {
    "text": " And, yeah, I mean, we had\ndesigned some of these prompts.",
    "start": "1282575",
    "end": "1290860"
  },
  {
    "text": "You can also get an LLM to\ndesign these prompts as well. There's recursive self-improving\nideas here that happen,",
    "start": "1290860",
    "end": "1297755"
  },
  {
    "text": "and you can bump up the\nperformance a little bit more. ",
    "start": "1297755",
    "end": "1302770"
  },
  {
    "text": "Cool. So what we have seen so far\nis that as models get stronger",
    "start": "1302770",
    "end": "1307950"
  },
  {
    "text": "and stronger, you\ncan start forcing them to do your task zero shot\nor with few shot examples,",
    "start": "1307950",
    "end": "1314080"
  },
  {
    "text": "and you can trick them\ninto thinking what task you want them to solve. But the downside is\nthat there's only so",
    "start": "1314080",
    "end": "1321960"
  },
  {
    "text": "much you can fit into context. This might not be\nvery true anymore. Models are becoming\nincreasingly larger context,",
    "start": "1321960",
    "end": "1328780"
  },
  {
    "text": "but it's still\nsomewhat unsatisfactory to think you have to trick the\nmodel into doing your task,",
    "start": "1328780",
    "end": "1334120"
  },
  {
    "text": "rather than it's just doing\nthe task you wanted to do. And, potentially, I\nmean, going forward,",
    "start": "1334120",
    "end": "1340860"
  },
  {
    "text": "you probably still want\nto fine-tune these models for more and more\ncomplex tasks, and that's where we're going to\ngo forward in this.",
    "start": "1340860",
    "end": "1349213"
  },
  {
    "text": "Next section, we're\ngoing to cover is instruction fine-tuning. And the general idea\nwe have right now",
    "start": "1349213",
    "end": "1355790"
  },
  {
    "text": "is that, as we talked\nabout, pretraining is not about assisting users.",
    "start": "1355790",
    "end": "1360870"
  },
  {
    "text": "It is about predicting\nthe next token. Now you can trick it into\nassisting users and following",
    "start": "1360870",
    "end": "1366920"
  },
  {
    "text": "the instruction you wanted\nto, but, in general, that's not what we trained it for. And this is an\nexample where if you",
    "start": "1366920",
    "end": "1373010"
  },
  {
    "text": "ask GPT 3, pretty strong\nmodel, to explain moon landing to a six-year-old\nin a few sentences,",
    "start": "1373010",
    "end": "1378330"
  },
  {
    "text": "and it will follow up with\nmore questions about what a six-year-old might want. This is not what you\nwanted the model to do.",
    "start": "1378330",
    "end": "1384620"
  },
  {
    "text": "So the general term that\npeople use these days is that they are not\naligned with user intent.",
    "start": "1384620",
    "end": "1390990"
  },
  {
    "text": "And the next section\nthat we're going to cover are going to talk about how\nto align it with user intent, so that they don't have to\ntrick the model into whatever",
    "start": "1390990",
    "end": "1398509"
  },
  {
    "text": "we want it to do. And this is a\ndesired completion we",
    "start": "1398510",
    "end": "1404030"
  },
  {
    "text": "want at the end of\ninstruction-tuning. And, yeah, how do we get\nfrom those pretrained models",
    "start": "1404030",
    "end": "1412310"
  },
  {
    "text": "to models which can\nrespond to user intent? Again, I hope this\nwas covered somewhere",
    "start": "1412310",
    "end": "1419180"
  },
  {
    "text": "in the class, the general idea\nof pretraining and fine-tuning, but what you have\nprobably seen thus far",
    "start": "1419180",
    "end": "1425090"
  },
  {
    "text": "is that you pretrained on a\nlot of different language tasks on data, but then you fine-tune\non your specific task.",
    "start": "1425090",
    "end": "1432180"
  },
  {
    "text": "So you're taking the\nsame decoder-only models, and you're fine-tuning to some\ntask with very little amount",
    "start": "1432180",
    "end": "1438680"
  },
  {
    "text": "of data. The thing that is\ngoing to be different now is not that we're no longer\nfine-tuning on a little amount",
    "start": "1438680",
    "end": "1443754"
  },
  {
    "text": "of data, we're\ngoing to fine-tune on many, many different\ntasks, and we're going to just try to put\nthem into a single usable",
    "start": "1443755",
    "end": "1451190"
  },
  {
    "text": "UX for our users. And this is where fine-tuning\nor instruction fine-tuning comes in.",
    "start": "1451190",
    "end": "1456730"
  },
  {
    "text": " Cool. So, again, the recipe is not\nvery, very complicated here.",
    "start": "1456730",
    "end": "1465520"
  },
  {
    "text": "We're going to collect a lot\nof examples of instruction and output pairs,\nand the instructions are going to range over\nseveral tasks, different forms.",
    "start": "1465520",
    "end": "1472875"
  },
  {
    "text": "There's going to be\nquestion-answering, there are going to be\nsummarization, translation, cohort reasoning, and so on. And we're going to\ncollect a lot of examples",
    "start": "1472875",
    "end": "1480760"
  },
  {
    "text": "related to all those tasks. And the idea is like, I mean, we\ntrain on instruction and output",
    "start": "1480760",
    "end": "1486340"
  },
  {
    "text": "pairs exactly with\nthem, and then we're going to evaluate on\nsome unseen tasks as well.",
    "start": "1486340",
    "end": "1491740"
  },
  {
    "text": "So this is a general paradigm\nof instruction fine-tuning, and again, it's\nthe same idea which",
    "start": "1491740",
    "end": "1499330"
  },
  {
    "text": "we explored in pretraining\nis that data plus scale is really important. And these days, I mean, you\nstart off with one task,",
    "start": "1499330",
    "end": "1506929"
  },
  {
    "text": "you're now extending it over\nthousands and thousands of tasks with 3 million plus examples.",
    "start": "1506930",
    "end": "1512090"
  },
  {
    "text": "And this is, generally,\na broad range of tasks that you might see in\ninstruction fine-tuning data sets.",
    "start": "1512090",
    "end": "1517330"
  },
  {
    "text": "And, yeah, you\nmight even think of, why are we calling it\nfine-tuning anymore? It's almost starting to look\nlike pretraining, but yeah,",
    "start": "1517330",
    "end": "1525679"
  },
  {
    "text": "we can-- these are just terms, so\nyou can decide whatever you are comfortable with.",
    "start": "1525680",
    "end": "1532450"
  },
  {
    "text": "So, yeah, we get this\nhuge instruction data set. We fine-tune our model. The next question is like how\ndo we evaluate these data sets.",
    "start": "1532450",
    "end": "1541540"
  },
  {
    "text": "Now I think you guys will see\nanother lecture on evaluation, so I don't want to dive\ntoo deep into this,",
    "start": "1541540",
    "end": "1547700"
  },
  {
    "text": "but generally evaluation\nof these language models is an extremely tricky topic.",
    "start": "1547700",
    "end": "1552774"
  },
  {
    "text": "There's a lot of biases\nthat you need to deal with, and a lot of this\nwill be covered, but some more recent\nprogress on this",
    "start": "1552775",
    "end": "1559000"
  },
  {
    "text": "is we are starting to curate\nthese really large benchmarks like MMLU where the\nmodels are tested",
    "start": "1559000",
    "end": "1565870"
  },
  {
    "text": "on a broad range of\ndiverse knowledge. And this is just one\nexample, which is--",
    "start": "1565870",
    "end": "1570879"
  },
  {
    "text": "and these are the topics\nthat you will see. And just to give some\nintuition of what the examples in these\nevaluation look like,",
    "start": "1570880",
    "end": "1578020"
  },
  {
    "text": "under astronomy,\nyou might be asked what is true for\ntype 1 as supernova, or you might be asked some\nquestions about biology.",
    "start": "1578020",
    "end": "1585170"
  },
  {
    "text": "And there's a huge\nhost of tasks for this, and you can typically-- these\nare multi choice questions,",
    "start": "1585170",
    "end": "1591320"
  },
  {
    "text": "and you can ask\nthe model to answer the question if their\ninstruction fine-tuned already. Hopefully, they can simply\nanswer the question,",
    "start": "1591320",
    "end": "1597290"
  },
  {
    "text": "but you can also\nchain-of-thought prompt these questions or few\nshot-prompt these questions too. And, recently, there's been\na huge amount of progress",
    "start": "1597290",
    "end": "1605830"
  },
  {
    "text": "on this benchmark. What people have\nobserved is more and more pretraining on\nmore and more data",
    "start": "1605830",
    "end": "1610990"
  },
  {
    "text": "and larger models is simply just\nclimbing up the number on this. So 90% is often seen\nas a benchmark number",
    "start": "1610990",
    "end": "1618640"
  },
  {
    "text": "that these models\nwanted to cross because it's roughly human-level\nknowledge or understanding. And recently, the Gemini models\npurposely crossed this number.",
    "start": "1618640",
    "end": "1629870"
  },
  {
    "text": "So yeah, go ahead. Isn't this the entire\nthing all over again,",
    "start": "1629870",
    "end": "1636640"
  },
  {
    "text": "like, ImageNet, at\nsome point you're like, OK, maybe my methods are\ntoo fine-tuned implicitly",
    "start": "1636640",
    "end": "1643110"
  },
  {
    "text": "on the ImageNet biases. And isn't something like\nthat happening here as well?",
    "start": "1643110",
    "end": "1648990"
  },
  {
    "text": "Yes, I think this\nis a tricky topic because a lot of the models-- often there's this idea about\nwhether your test sets are",
    "start": "1648990",
    "end": "1656250"
  },
  {
    "text": "leaking into your\ntraining data set, and there's our huge\nconcerns about that. It's a perfectly\nvalid question to ask.",
    "start": "1656250",
    "end": "1663059"
  },
  {
    "text": "How do we even evaluate? This is why evaluation\nis actually very tricky. But one general thing to be\ncareful about is at some point,",
    "start": "1663060",
    "end": "1670060"
  },
  {
    "text": "it doesn't matter what your\ntrain test is if the models are generally useful. If their models are doing useful\nstuff, does it matter how--",
    "start": "1670060",
    "end": "1677730"
  },
  {
    "text": "well, if your tests-- if you train on\neverything you care about and it does well on\nit, does it matter?",
    "start": "1677730",
    "end": "1683415"
  },
  {
    "text": " Again, we still need better\nways to evaluate the models",
    "start": "1683415",
    "end": "1690360"
  },
  {
    "text": "and to understand\nwhat methods are doing and how they are if\nthey're improving the model or not, but at some\npoint, those boundaries",
    "start": "1690360",
    "end": "1697290"
  },
  {
    "text": "start to be less important. ",
    "start": "1697290",
    "end": "1702940"
  },
  {
    "text": "Cool. So massive progress on this\nbenchmark starting with GPT-2, and we're roughly\nat 90% to the point",
    "start": "1702940",
    "end": "1709470"
  },
  {
    "text": "where these benchmarks are\nstarting to become unclear if improvements on these are\nactually meaningful or not.",
    "start": "1709470",
    "end": "1716160"
  },
  {
    "text": "In fact, like most of the times\nwhen the models are wrong, you might often find\nthat the question itself",
    "start": "1716160",
    "end": "1721710"
  },
  {
    "text": "was unclear or ambiguous. So all evaluation benchmarks\nhave a certain limited utility",
    "start": "1721710",
    "end": "1727049"
  },
  {
    "text": "to them. So Yeah, going to go\nover another evaluation",
    "start": "1727050",
    "end": "1733710"
  },
  {
    "text": "example of how this\nrecipe changes things. So T5 models were\ninstruction fine-tuned",
    "start": "1733710",
    "end": "1739260"
  },
  {
    "text": "on a huge number of tasks. And another trend\nwhich I think will be the theme across\nthis lecture is",
    "start": "1739260",
    "end": "1745410"
  },
  {
    "text": "that as your models\nbecome larger, as they're trained\non more data, they become more and more responsive\nto your task information",
    "start": "1745410",
    "end": "1751440"
  },
  {
    "text": "as well. So what you will observe\nhere is that as the number of parameters increase, we\nhave T5 small, flan-T5 small,",
    "start": "1751440",
    "end": "1758680"
  },
  {
    "text": "and we go up to 11 billion\nparameters where we have T5 XXL. You'll see that the\nimprovement actually improves.",
    "start": "1758680",
    "end": "1767100"
  },
  {
    "text": "Going from a pretrained\nto an instruction model, the instruction model\nis all the more better at following instructions.",
    "start": "1767100",
    "end": "1773200"
  },
  {
    "text": "So the difference is plus\n6.1 and goes to plus 26.6 as the models become larger.",
    "start": "1773200",
    "end": "1778720"
  },
  {
    "text": "So this is another\nvery encouraging trend that you probably should\ntrain on a lot of data",
    "start": "1778720",
    "end": "1784260"
  },
  {
    "text": "with a lot of compute,\nand pretraining just keeps on giving. ",
    "start": "1784260",
    "end": "1795120"
  },
  {
    "text": "I hope you guys get a chance to\nplay with a lot of these models. I think you already\nhopefully are.",
    "start": "1795120",
    "end": "1800190"
  },
  {
    "text": "But, yeah, before\ninstruction fine-tuning, something when you're\nasked a question related",
    "start": "1800190",
    "end": "1805620"
  },
  {
    "text": "to disambiguation QA, you\nget something like this, and it doesn't actually\nfollow the Let's think step",
    "start": "1805620",
    "end": "1812330"
  },
  {
    "text": "by step instruction\nvery clearly, but after instruction\nfine-tuning, it is able to answer\nthe question here.",
    "start": "1812330",
    "end": "1820370"
  },
  {
    "text": "And, yeah, like more\nrecently, people have been researching into what\nthe instruction-tuning data",
    "start": "1820370",
    "end": "1825410"
  },
  {
    "text": "sets should look like. There's a huge plethora of\ninstruction-tuning data sets now available. This is just a\nrepresentative diagram,",
    "start": "1825410",
    "end": "1831539"
  },
  {
    "text": "and there's a huge open\nsource community developing around these as well.",
    "start": "1831540",
    "end": "1836809"
  },
  {
    "text": "Some high level lessons that\nwe have learned from this is--",
    "start": "1836810",
    "end": "1842390"
  },
  {
    "text": "one lesson that I think\nmight be interesting is that we can actually use\nreally large, strong models to generate some of the\ninstruction-tuning data",
    "start": "1842390",
    "end": "1849470"
  },
  {
    "text": "to train some of\nour smaller models. So take your\nfavorite model right now, GPT-4 maybe or\nmaybe Claude or whatever,",
    "start": "1849470",
    "end": "1856020"
  },
  {
    "text": "and you can get it to\nanswer some questions and generate instruction\noutput pairs for training",
    "start": "1856020",
    "end": "1861740"
  },
  {
    "text": "your open source\nor smaller model. And that actually is a\nvery successful recipe. So instead of getting a human\nto collect all the instruction",
    "start": "1861740",
    "end": "1868970"
  },
  {
    "text": "output pairs or getting humans\nto generate the answers, you can get bigger models to\ngenerate the answers as well.",
    "start": "1868970",
    "end": "1874380"
  },
  {
    "text": "So that's number one thing\nthat has recently emerged. Another thing that is being\nemerged or is being discussed",
    "start": "1874380",
    "end": "1881450"
  },
  {
    "text": "is, how much data do we need? I talked about\nmillions of examples, but people have often found that\nif you have really high quality",
    "start": "1881450",
    "end": "1887900"
  },
  {
    "text": "examples, you can get away\nwith 1,000 examples as well. So this is the paper that\nsays more for alignment,",
    "start": "1887900",
    "end": "1893039"
  },
  {
    "text": "and this is still an\nactive area of research and how data scaling and\ninstruction-tuning affects the final model performance.",
    "start": "1893040",
    "end": "1899840"
  },
  {
    "text": "And, yeah, crowdsourcing\nthese models can be effective\nas well, so there",
    "start": "1899840",
    "end": "1905240"
  },
  {
    "text": "are very cool\nbenchmarks that are emerging like Open Assistant. A lot of activity in the field\nand, hopefully, like a lot",
    "start": "1905240",
    "end": "1913250"
  },
  {
    "text": "more progress as we go on. Yes. A question.",
    "start": "1913250",
    "end": "1918289"
  },
  {
    "text": "In the spirit of this Llima\npaper, doesn't code or--",
    "start": "1918290",
    "end": "1924210"
  },
  {
    "text": "I don't know, math-word problems\nhave this desired structure? So shouldn't we just\nbe training code models",
    "start": "1924210",
    "end": "1931520"
  },
  {
    "text": "and doing like\nsome English stuff and then just be like, OK,\nthis is the best reasoning we can get at some point?",
    "start": "1931520",
    "end": "1938330"
  },
  {
    "text": "Because code has this structure\nwhere you're going step by step,",
    "start": "1938330",
    "end": "1943700"
  },
  {
    "text": "and you're thinking in some way\nbreaking down a high concept",
    "start": "1943700",
    "end": "1949630"
  },
  {
    "text": "model? So you can consider code and\nhave very high value tokens. So maybe just doing--",
    "start": "1949630",
    "end": "1955130"
  },
  {
    "text": "So I think there's-- again, pretraining\nis a whole dark art that I am not completely\nfamiliar with,",
    "start": "1955130",
    "end": "1961289"
  },
  {
    "text": "but code actually\nends up being really useful in pretraining mixtures. And people do upweight\ncode data quite a lot.",
    "start": "1961290",
    "end": "1970309"
  },
  {
    "text": "Similarly, I mean,\nbut it depends upon what the users are\ngoing to use the models for.",
    "start": "1970310",
    "end": "1976190"
  },
  {
    "text": "Some people might\nuse it for code, some people might\nuse it for reasoning, but that's not the only\ntask we care about. As you might see later\non in the next step",
    "start": "1976190",
    "end": "1983029"
  },
  {
    "text": "we'll discuss this as\nwell, is that people often use these models\nfor creative tasks. They wanted to\nwrite a story, they",
    "start": "1983030",
    "end": "1989380"
  },
  {
    "text": "wanted to generate a\nmovie script or so on. And I don't know if necessarily\ntraining on reasoning only tasks",
    "start": "1989380",
    "end": "1995679"
  },
  {
    "text": "would help with that. So go ahead. Yeah, would you\nexpect there exists",
    "start": "1995680",
    "end": "2001020"
  },
  {
    "text": "some data distribution which is\nhigh value for creative tasks?",
    "start": "2001020",
    "end": "2007650"
  },
  {
    "text": "Yes, I mean, it\nseems a lot of people",
    "start": "2007650",
    "end": "2013620"
  },
  {
    "text": "write about stories and\neverything on the internet all the time which is not code. And sometimes there's this\nidea of hallucinations",
    "start": "2013620",
    "end": "2020655"
  },
  {
    "text": "as well in this like\nfield, but you can often think, hey, creativity might be\na byproduct of hallucinations",
    "start": "2020655",
    "end": "2027720"
  },
  {
    "text": "as well. So I don't know what\nexact data would lead to, like more creative models,\nbut generally, there's",
    "start": "2027720",
    "end": "2034020"
  },
  {
    "text": "a lot of data or\na lot of stories that are written on the\ninternet, which allows the model to be creative.",
    "start": "2034020",
    "end": "2039977"
  },
  {
    "text": "Yeah, but I don't know if\nI have a specific answer to the question. Cool so we discussed instruction\nfine-tuning, very simple",
    "start": "2039977",
    "end": "2048528"
  },
  {
    "text": "and very straightforward. There's like no complicated\nalgorithms here, just collect a lot\nof data and then you can start leveraging the\nperformance at scale as well.",
    "start": "2048528",
    "end": "2057330"
  },
  {
    "text": "as models become better,\nthese models also become more easily specifiable\nand they become more responsive",
    "start": "2057330",
    "end": "2063810"
  },
  {
    "text": "to tasks as well. We're going to discuss\nsome limitations, and I this is really\nimportant to understand",
    "start": "2063810",
    "end": "2068879"
  },
  {
    "text": "why we are going to optimize\nfor human preferences. Cool. So we talked a bit about this.",
    "start": "2068880",
    "end": "2076919"
  },
  {
    "text": "Instruction fine-tuning is\nnecessarily contingent on humans labeling the data.",
    "start": "2076920",
    "end": "2082020"
  },
  {
    "text": "Now humans, it's expensive\nto collect this data, especially as the questions\nbecome more and more complex,",
    "start": "2082020",
    "end": "2089469"
  },
  {
    "text": "you want to answer\nquestions which may be at physics-PhD level\nor things to that effect.",
    "start": "2089469",
    "end": "2095560"
  },
  {
    "text": "These things become increasingly\nexpensive to collect. So, yeah, this is maybe perhaps\nobvious collecting data.",
    "start": "2095560",
    "end": "2102970"
  },
  {
    "text": "Pretraining does not\nrequire any specific data. You scrape data off the web,\nbut for instruction fine-tuning, you probably need to recruit\nsome people to write down answer",
    "start": "2102970",
    "end": "2110282"
  },
  {
    "text": "to your instructions. So this can become very\nexpensive very quickly. But there's more\nlimitations to this as well.",
    "start": "2110282",
    "end": "2116380"
  },
  {
    "text": "And we were just\ndiscussing this. There are open-ended tasks\nrelated to creativity that don't really have an exact\ncorrect answer to begin with.",
    "start": "2116380",
    "end": "2125080"
  },
  {
    "text": "So how do you generate\nthe right answer to this kind of a question.",
    "start": "2125080",
    "end": "2130740"
  },
  {
    "text": "And, yeah, language\nmodeling inherently penalizes all token\nlevel mistakes equally.",
    "start": "2130740",
    "end": "2138510"
  },
  {
    "text": "This is what supervised\nfine-tuning does as well, but often, not all\nmistakes are the same.",
    "start": "2138510",
    "end": "2144130"
  },
  {
    "text": "So this is an example\nwhere you're trying to do this prediction task. Avatar is a fantasy TV show,\nand perhaps you can see--",
    "start": "2144130",
    "end": "2151350"
  },
  {
    "text": "I mean, calling it an adventure\nTV show is perhaps OK, but calling it a musical\nmay be a much worse mistake,",
    "start": "2151350",
    "end": "2160020"
  },
  {
    "text": "but both these mistakes\nare penalized equally. And I think one general aspect\nwhich is becoming increasingly",
    "start": "2160020",
    "end": "2169849"
  },
  {
    "text": "relevant is that the\nhumans that you might ask, might not generate the right\nor the highest quality answer.",
    "start": "2169850",
    "end": "2175349"
  },
  {
    "text": "Your models are becoming\nincreasingly competitive, and you want-- in some sense,\nyou're going to be",
    "start": "2175350",
    "end": "2180590"
  },
  {
    "text": "limited by how high quality\nthe answer humans can generate.",
    "start": "2180590",
    "end": "2185640"
  },
  {
    "text": "But often I find that the models\nare generating better and better answers, so do we\nreally want to keep",
    "start": "2185640",
    "end": "2191157"
  },
  {
    "text": "relying on humans to\nwrite down the answers, or do we want to\nsomehow go over that.",
    "start": "2191157",
    "end": "2197480"
  },
  {
    "text": "So these are the\nthree problems we have talked about with\ninstruction fine-tuning.",
    "start": "2197480",
    "end": "2204360"
  },
  {
    "text": "And we made a lot of\nprogress with this, but this is not\nhow we got ChatGPT.",
    "start": "2204360",
    "end": "2210590"
  },
  {
    "text": "And one high-level problem\nhere is that even when we are instruction fine-tuning,\nthere is still a huge mismatch",
    "start": "2210590",
    "end": "2218600"
  },
  {
    "text": "between-- the end goal is to optimize\nfor human preferences, generate an output that\na human might like.",
    "start": "2218600",
    "end": "2226650"
  },
  {
    "text": "And we're still doing\nprediction tasks where we're predicting\nthe next token, but now in a more\ncurated data set.",
    "start": "2226650",
    "end": "2232980"
  },
  {
    "text": "So there's still a bit of\na mismatch going on here, and it's not exactly\nwhat we want to do.",
    "start": "2232980",
    "end": "2238972"
  },
  {
    "text": "Hopefully, I mean, I'm going\nto take a second here to pause because this is important to\nunderstand the next section.",
    "start": "2238972",
    "end": "2244170"
  },
  {
    "text": "And if there's any\nquestions, feel free to ask. So is this step still taken as\na first step or [INAUDIBLE].",
    "start": "2244170",
    "end": "2255475"
  },
  {
    "text": "It's a good question. So I think this is still one\nof the more important steps",
    "start": "2255475",
    "end": "2260510"
  },
  {
    "text": "that you take before\ntaking the next step, but people are trying to remove\nthe step altogether and jump",
    "start": "2260510",
    "end": "2266607"
  },
  {
    "text": "directly to the next step. So there's work emerging\non that, but yeah, and this is still a\nvery important step",
    "start": "2266607",
    "end": "2272510"
  },
  {
    "text": "before we do the next step. ",
    "start": "2272510",
    "end": "2278465"
  },
  {
    "text": "Is problem two also\npresent in pretraining? And so how do you avoid that\njust by having a lot of data.",
    "start": "2278465",
    "end": "2287329"
  },
  {
    "text": "Yeah, that's a great question. There's one difference. One major difference\nfrom pretraining.",
    "start": "2287330",
    "end": "2293090"
  },
  {
    "text": "Pretraining covers\na lot more text. So just for context, I\nmean, as we talked about,",
    "start": "2293090",
    "end": "2299540"
  },
  {
    "text": "pretraining is roughly\n15 trillion tokens, whereas supervised\ninstruction fine-tuning",
    "start": "2299540",
    "end": "2304760"
  },
  {
    "text": "might be somewhere on\nthe order of millions to billions of tokens. So it's like few orders\nof magnitude lower. Typically, you'd\nonly see one answer",
    "start": "2304760",
    "end": "2311630"
  },
  {
    "text": "for a specific instruction,\nbut during pretraining, you'll see multiple texts and\nmultiple completions",
    "start": "2311630",
    "end": "2316790"
  },
  {
    "text": "for the same kind of a prompt. Now that's good, because\nwhen you see multiple answers",
    "start": "2316790",
    "end": "2321923"
  },
  {
    "text": "or completions\nduring pretraining, you start to weigh\ndifferent answers. You start to put probability\nmass on different answers",
    "start": "2321923",
    "end": "2328490"
  },
  {
    "text": "or completions. But instruction\nfine-tuning might force you to put on\nweight on only one answer.",
    "start": "2328490",
    "end": "2334660"
  },
  {
    "text": "Does that-- OK. But generally, yeah. I mean, this is a problem\nwith both the stages. You're right.",
    "start": "2334660",
    "end": "2342400"
  },
  {
    "text": "Anything else? Cool.",
    "start": "2342400",
    "end": "2347859"
  },
  {
    "text": "So as this whole\nthing alludes to, we're going to start\nto attempt to satisfy",
    "start": "2347860",
    "end": "2354430"
  },
  {
    "text": "human preferences directly. We're no longer going to try to\nget humans to generate some data and try to do some kind of a\ntoken level prediction loss.",
    "start": "2354430",
    "end": "2362509"
  },
  {
    "text": "We're going to try to optimize\nfor human preferences directly, and that is the\ngeneral field of RLHF,",
    "start": "2362510",
    "end": "2369340"
  },
  {
    "text": "and that's the final\nstep in typically getting a model like ChatGPT.",
    "start": "2369340",
    "end": "2374410"
  },
  {
    "text": "So we talked about how\ncollecting demonstration is expensive, and there's still a\nbroad mismatch between the LLM",
    "start": "2374410",
    "end": "2379600"
  },
  {
    "text": "objective and human\npreferences, and now we're going to try and optimize for\nhuman preferences directly.",
    "start": "2379600",
    "end": "2385150"
  },
  {
    "text": "So what is optimizing for\nhuman preferences even mean? To concretely establish that.",
    "start": "2385150",
    "end": "2391670"
  },
  {
    "text": "Let's go through a\nspecific example in mind, which is summarization.",
    "start": "2391670",
    "end": "2396880"
  },
  {
    "text": "We want to train a model to\nbe better at summarization, and we want to satisfy\nhuman preferences,",
    "start": "2396880",
    "end": "2402393"
  },
  {
    "text": "so let's imagine\nthat the human is able to prescribe a reward\nfor a specific summary. Let's just pretend there is a\nreward function you and I can",
    "start": "2402393",
    "end": "2409420"
  },
  {
    "text": "assign, say like reward. This is plus 1, this is minus\n1 or something to that effect. ",
    "start": "2409420",
    "end": "2419970"
  },
  {
    "text": "So in this specific\ncase, we have this input x which is about an\nearthquake in San Francisco.",
    "start": "2419970",
    "end": "2427003"
  },
  {
    "text": "So this is a news article\nthat we want to summarize. And let's pretend that\nwe get these rewards,",
    "start": "2427003",
    "end": "2433780"
  },
  {
    "text": "and we want to optimize this. So we get one summary\nY1 which gives us",
    "start": "2433780",
    "end": "2439270"
  },
  {
    "text": "an earthquake hit and so on,\nand we assign a reward of 8.0, and another summary which\ngives us a reward of 1.2.",
    "start": "2439270",
    "end": "2445990"
  },
  {
    "text": "Generally speaking, like the\nobjective that we want to set up is something of the\nfollowing form, where",
    "start": "2445990",
    "end": "2451720"
  },
  {
    "text": "we want to take\nour language model P theta, which generates a\ncompletion y given an input x,",
    "start": "2451720",
    "end": "2458980"
  },
  {
    "text": "and we want to maximize\nthe reward of r x y where x is the input\nand y is the output",
    "start": "2458980",
    "end": "2464770"
  },
  {
    "text": "summary in the specific task. And maybe just to really\nconcretely point out something",
    "start": "2464770",
    "end": "2472840"
  },
  {
    "text": "here. This is different\nfrom everything that we have done in\none very specific way.",
    "start": "2472840",
    "end": "2478210"
  },
  {
    "text": "We are sampling from the model\nitself in the bottom term. If you see we're\nusing y from p theta,",
    "start": "2478210",
    "end": "2485210"
  },
  {
    "text": "everything we've\nseen so far, the data is sampled from\nsome other source either during\npretraining, either in supervised\nfine-tuning, and we're",
    "start": "2485210",
    "end": "2491860"
  },
  {
    "text": "maximizing the log\nlikelihood of those tokens. But now we're explicitly\nsampling from our model",
    "start": "2491860",
    "end": "2496900"
  },
  {
    "text": "and optimizing potentially a\nnon-differentiable objective.",
    "start": "2496900",
    "end": "2502059"
  },
  {
    "text": "Cool. So broadly, the RLHF pipeline\nlooks something like this,",
    "start": "2502060",
    "end": "2507490"
  },
  {
    "text": "and first step is still\ninstruction-tuning, something we have seen\nup until now where we take our pretrained\nmodel, we instruction-tune",
    "start": "2507490",
    "end": "2514560"
  },
  {
    "text": "on a large collection\nof tasks, and we get something which starts\nresponding to our desired intent",
    "start": "2514560",
    "end": "2520230"
  },
  {
    "text": "or not. But there are two\nmore steps after this, which are typically followed\nin creating something",
    "start": "2520230",
    "end": "2525750"
  },
  {
    "text": "like InstructGPT. The first step is\nestimating some kind of a reward model,\nsomething which tells us,",
    "start": "2525750",
    "end": "2531580"
  },
  {
    "text": "given an instruction, how much\nwould a human like this answer or how much would a\nhuman hate this answer.",
    "start": "2531580",
    "end": "2536860"
  },
  {
    "text": "So we looked at something\nlike this earlier, but I didn't talk about how do\nwe even get something like that. That's the second step.",
    "start": "2536860",
    "end": "2543039"
  },
  {
    "text": "And then we take\nthis reward model, and we optimize it\nthrough the optimization that I suggested earlier.",
    "start": "2543040",
    "end": "2548589"
  },
  {
    "text": "So maximizing the\nexpected reward under your language model. And we're going to\ngo over a lot over",
    "start": "2548590",
    "end": "2555420"
  },
  {
    "text": "in the second and third steps. So the first question\nwe want to answer is, how do we even get a\nreward model for about what",
    "start": "2555420",
    "end": "2562920"
  },
  {
    "text": "humans are going to like? This is a very ill-defined\nproblem generally speaking.",
    "start": "2562920",
    "end": "2568780"
  },
  {
    "text": "So there's these\ntwo problems here that we're going to address. First is the human in\nthe loop is expensive.",
    "start": "2568780",
    "end": "2575259"
  },
  {
    "text": "So let's say if I ask a\nmodel to generate an answer and then I get a human to label\nwith some kind of a score,",
    "start": "2575260",
    "end": "2582220"
  },
  {
    "text": "I'm doing this over\nmillions of completions. That is not very scalable. I don't want to sit around and\nlabel millions of examples.",
    "start": "2582220",
    "end": "2589840"
  },
  {
    "text": "So this is very easy. We're in a machine\nlearning class, so what are we going to do?",
    "start": "2589840",
    "end": "2595548"
  },
  {
    "text": "What we're going to\ndo is we're going to train something which\npredicts what a human would like or what a human might not like.",
    "start": "2595548",
    "end": "2601060"
  },
  {
    "text": "And this is roughly-- this is essentially a\nmachine learning problem where we take these\nreward scores,",
    "start": "2601060",
    "end": "2607470"
  },
  {
    "text": "and we try to train a\nreward model to predict, given an input and output\nwhat the reward scores would",
    "start": "2607470",
    "end": "2613230"
  },
  {
    "text": "look like.  Simple machine learning\nregression style problem,",
    "start": "2613230",
    "end": "2618660"
  },
  {
    "text": "you might have\nseen this earlier. Cool. ",
    "start": "2618660",
    "end": "2624630"
  },
  {
    "text": "Now there's a bigger\nproblem here, and-- sorry. Go ahead. So do we use, I don't know, just\nembeddings with a classifier,",
    "start": "2624630",
    "end": "2633369"
  },
  {
    "text": "or do we use a real\nlanguage model to do that? That's a good question. Generally, what we do is\nwe still typically need",
    "start": "2633370",
    "end": "2641310"
  },
  {
    "text": "reward models where they\nneed to be able to understand the text really well,\nso like bigger models, and they're typically\ninitialized from the language",
    "start": "2641310",
    "end": "2648330"
  },
  {
    "text": "model that you trained\npretrained as well. So you typically start with\na pretrained language model and do some kind of a prediction\nthat we'll talk about,",
    "start": "2648330",
    "end": "2655750"
  },
  {
    "text": "and they'll give you a score. How do you-- if\nyou're doing that, how do you separate x and y.",
    "start": "2655750",
    "end": "2661407"
  },
  {
    "text": "Like how does the language model\nknow which parts [INAUDIBLE]? It doesn't need to. It can put the x and y.",
    "start": "2661407",
    "end": "2667319"
  },
  {
    "text": "It only sees x\nand y as an input, so it doesn't need to\ntypically see it separate. It's just going to predict\na score at the end.",
    "start": "2667320",
    "end": "2673960"
  },
  {
    "text": "OK. Yeah. The x and y is more for\nnotational convenience here because for us x\nand y are different.",
    "start": "2673960",
    "end": "2680640"
  },
  {
    "text": "X is a question user\nasks, and why is something the model generated. And you shut the\nwhole thing into it. --shut the whole thing into it.",
    "start": "2680640",
    "end": "2686990"
  },
  {
    "text": "Yes. Cool. Now this is the\nbigger problem here, and human judgments\nare very noisy.",
    "start": "2686990",
    "end": "2694110"
  },
  {
    "text": "We've talked about we want to\nassign a score to a completion. This is something that's\nextremely non-trivial to do.",
    "start": "2694110",
    "end": "2700619"
  },
  {
    "text": "So if I give you a\nsummary like this, what score are you going\nto assign on a scale of 10?",
    "start": "2700620",
    "end": "2705740"
  },
  {
    "text": "If you ask me on\ndifferent days, I'll give a different answer first of\nall, but across humans itself,",
    "start": "2705740",
    "end": "2712302"
  },
  {
    "text": "this number is not calibrated\nin any meaningful way. So you could assign\na number of 4.1, 6.6,",
    "start": "2712302",
    "end": "2718500"
  },
  {
    "text": "and different humans would just\nsimply assign different scores. And there are ways\nto address this. You can calibrate humans, you\ncan give them a specific rubric,",
    "start": "2718500",
    "end": "2726330"
  },
  {
    "text": "you can talk to them, but it's\na very complicated process, and still there's a lot\nof room for judgment,",
    "start": "2726330",
    "end": "2732570"
  },
  {
    "text": "which is not typically very nice\nfor training a model like this. If your labels can vary a lot,\nit's just hard to predict.",
    "start": "2732570",
    "end": "2741260"
  },
  {
    "text": "So the way this is\naddressed is that instead of trying to predict the\nreward label directly,",
    "start": "2741260",
    "end": "2747085"
  },
  {
    "text": "you actually want to set\nup a problem in a slightly different way. What is something much\neasier for humans to do is give them two answers,\nor maybe many answers",
    "start": "2747085",
    "end": "2755450"
  },
  {
    "text": "and tell them-- ask them\nwhich one is better. So this is where the idea of\nasking humans to rank answers",
    "start": "2755450",
    "end": "2762830"
  },
  {
    "text": "comes in. So if I give you a\nwhole news article and ask you which\nsummary is better,",
    "start": "2762830",
    "end": "2768147"
  },
  {
    "text": "you might be able\nto give me a ranking that, oh, this second\nsummary is the worst, but the first one is\nbetter and the third one",
    "start": "2768147",
    "end": "2773420"
  },
  {
    "text": "is somewhere in the\nmiddle between those two. So you get a ranking,\nwhich gives you a preference over summaries.",
    "start": "2773420",
    "end": "2780230"
  },
  {
    "text": "And, hopefully, I mean,\nyou can see the idea that is important here\nis that even when",
    "start": "2780230",
    "end": "2786770"
  },
  {
    "text": "we have some kind of a\nconsistent utility function, even when I have-- it's\nmuch easier to compare it",
    "start": "2786770",
    "end": "2791832"
  },
  {
    "text": "to something and know that\nwhich is better than this, rather than ascribing it an\narbitrary number on a scale.",
    "start": "2791832",
    "end": "2797303"
  },
  {
    "text": "And that's where the signal\nfrom something like this is a lot better. Now, how do we get--",
    "start": "2797303",
    "end": "2804240"
  },
  {
    "text": "we talked about we need-- we get this kind of\na preference data, and now we need some kind of\na reward score out of this.",
    "start": "2804240",
    "end": "2810099"
  },
  {
    "text": "And we shove in our input, we\nshove in a summary as well, and we still need to\nget a score out of this,",
    "start": "2810100",
    "end": "2815770"
  },
  {
    "text": "but it's not clearly\nobvious to me like, how do we take this data\nand convert it into that score? ",
    "start": "2815770",
    "end": "2822900"
  },
  {
    "text": "In comes our pretty good\nfriends named Bradley-Terry,",
    "start": "2822900",
    "end": "2828119"
  },
  {
    "text": "and, essentially,\nthere's a lot of studying like many in economics\nand psychology,",
    "start": "2828120",
    "end": "2834010"
  },
  {
    "text": "which basically tries to model\nhow humans make decisions in specific case.",
    "start": "2834010",
    "end": "2839760"
  },
  {
    "text": "Like this Bradley-Terry\nmodel essentially says that a probability that a\nhuman chooses answer y1 over y2",
    "start": "2839760",
    "end": "2847650"
  },
  {
    "text": "is based on the difference\nbetween the rewards that humans assign\ninternally, and then you",
    "start": "2847650",
    "end": "2854490"
  },
  {
    "text": "take a sigmoid around it. So if you've looked at\nbinary classification before,",
    "start": "2854490",
    "end": "2859930"
  },
  {
    "text": "the logit is simply\nthe difference between the reward of some y1\nminus y2, or the difference",
    "start": "2859930",
    "end": "2865630"
  },
  {
    "text": "between the winning completion\nand the losing completion.",
    "start": "2865630",
    "end": "2872019"
  },
  {
    "text": "Is everybody with\nme till this point? ",
    "start": "2872020",
    "end": "2877299"
  },
  {
    "text": "So the idea is that if\nyou have a data set where given y1 and y2 where y1\nis a winning completion,",
    "start": "2877300",
    "end": "2884630"
  },
  {
    "text": "and we have a winning completion\ny and a losing completion yl, the winning completion\nshould score higher",
    "start": "2884630",
    "end": "2891369"
  },
  {
    "text": "than the losing completion. Go ahead. Sorry what is j? Is that a log or--",
    "start": "2891370",
    "end": "2897940"
  },
  {
    "text": "Sorry, what? What is the type of j,\nthis number here that we're getting as the expectation?",
    "start": "2897940",
    "end": "2903650"
  },
  {
    "text": "Is it a log prob or what is it? It's an expected log prob. So it will be a\nscalar at the end.",
    "start": "2903650",
    "end": "2909280"
  },
  {
    "text": "Sigmoid is-- so\nyou're taking the-- let's say you have a reward\nmodel which gives a score r1",
    "start": "2909280",
    "end": "2916330"
  },
  {
    "text": "to yw and r2 to yl, you\nsubtract that number, you get another number,\nyou put it into sigmoid,",
    "start": "2916330",
    "end": "2922580"
  },
  {
    "text": "and you get a probability\nbecause sigmoid will convert a logit into\nprobability, and then you take a logarithm\nof that, and you",
    "start": "2922580",
    "end": "2930340"
  },
  {
    "text": "take the expectation\nof everything and you get this\nfinal number which tells you how good\nyour reward model is doing on the entire data set.",
    "start": "2930340",
    "end": "2937839"
  },
  {
    "text": "So a good model of humans\nshould behave like this. A good model of humans\nwould score very low here.",
    "start": "2937840",
    "end": "2944990"
  },
  {
    "text": "So it would generally assign\na higher reward to the winning completion, and generally\nassign a lower reward",
    "start": "2944990",
    "end": "2950410"
  },
  {
    "text": "to the losing completion. Cool. The math is just beginning,\nso hold on to your seats.",
    "start": "2950410",
    "end": "2959145"
  },
  {
    "text": " Cool. So now let's see where we are.",
    "start": "2959145",
    "end": "2964460"
  },
  {
    "text": "We have a pretrained\nmodel ppt y given x, and we got this\nfancy reward model",
    "start": "2964460",
    "end": "2970279"
  },
  {
    "text": "which tells us that, hey,\nwe have a model of humans and it can tell us\nwhich answer they like,",
    "start": "2970280",
    "end": "2976060"
  },
  {
    "text": "and which answer did not like. Now to do RLHF,\ngenerally, I mean,",
    "start": "2976060",
    "end": "2982310"
  },
  {
    "text": "we've discussed what\nthis will look like. We will copy our\npretrained model or our instruction-tune\nmodel, and we",
    "start": "2982310",
    "end": "2988569"
  },
  {
    "text": "will optimize the\nparameters for those models. And I suggested that objective\nthat we want to optimize",
    "start": "2988570",
    "end": "2994900"
  },
  {
    "text": "is the expected reward when we\nsample completions from p theta.",
    "start": "2994900",
    "end": "3000420"
  },
  {
    "text": "And we're going to optimize our\nlearned reward model instead of the true reward\nmodel, which humans would have typically assigned.",
    "start": "3000420",
    "end": "3006990"
  },
  {
    "text": "Do you guys see any\nproblem with this? ",
    "start": "3006990",
    "end": "3012210"
  },
  {
    "text": "Is there something\nthat's wrong here or that might go wrong if we\ndo something along these lines?",
    "start": "3012210",
    "end": "3017675"
  },
  {
    "start": "3017675",
    "end": "3023610"
  },
  {
    "text": "Go for it. The model will collapse.  It might collapse, yes.",
    "start": "3023610",
    "end": "3030210"
  },
  {
    "text": "But generally, at least\nfrom my intuition, like if you're ever\ndoing something and you have-- you're\noptimizing some learned metric,",
    "start": "3030210",
    "end": "3037020"
  },
  {
    "text": "I'd be very careful\nbecause, typically, our loss functions are very\nclearly defined, but here my reward\nmodel is learned.",
    "start": "3037020",
    "end": "3043640"
  },
  {
    "text": "When it's learned, it\nmeans it will have errors. [INAUDIBLE] Yes. So it's going to be trained\non some distribution.",
    "start": "3043640",
    "end": "3050850"
  },
  {
    "text": "It will generalize as well,\nbut it will have errors, and when you're optimizing\nagainst a learned model,",
    "start": "3050850",
    "end": "3057200"
  },
  {
    "text": "it will tend to hack\nthe reward model. So it might exploit-- the reward model\nmight erroneously",
    "start": "3057200",
    "end": "3063020"
  },
  {
    "text": "assign a really high score\nto a really bad completion. If your policy learns,\nor if your language model",
    "start": "3063020",
    "end": "3068390"
  },
  {
    "text": "learns to do that,\nit will completely hack that and start generating\nthose gibberish completions.",
    "start": "3068390",
    "end": "3073730"
  },
  {
    "text": "So just as a general machine\nlearning tip as well, if you're optimizing\na learned metric,",
    "start": "3073730",
    "end": "3079860"
  },
  {
    "text": "be careful about what\nyou're optimizing, and make sure that\nit's actually reliable. And the way-- and this is\nobviously not desirable.",
    "start": "3079860",
    "end": "3088282"
  },
  {
    "text": "I mean, if you start\noptimizing this objective, you're going to converge to\ngibberish language models very, very quickly. So typically what\npeople do is that you",
    "start": "3088282",
    "end": "3095692"
  },
  {
    "text": "want to add some\nkind of a penalty that avoids it drifting too\nfar from its initialization.",
    "start": "3095693",
    "end": "3100980"
  },
  {
    "text": "And why do we want to do that? If it cannot drift too far\nfrom its initialization, we know the initialization of\nthe model is a decent language",
    "start": "3100980",
    "end": "3108300"
  },
  {
    "text": "model, and we know it is not\nnecessarily satisfying this reward model too much, and we\nalso know that the reward model",
    "start": "3108300",
    "end": "3115500"
  },
  {
    "text": "is trained on a distribution of\ncompletions where the initial model is.",
    "start": "3115500",
    "end": "3120970"
  },
  {
    "text": "So typically, when we talk about\ntraining this reward model, we have trained it on certain\ncompletions, which are sampled",
    "start": "3120970",
    "end": "3126540"
  },
  {
    "text": "from this initial distribution. So we know the reward\nmodel will be somewhat reliable in that distribution.",
    "start": "3126540",
    "end": "3131650"
  },
  {
    "text": "So we're just\ngoing to simply add a penalty, which tells\nus that you should not drift too far away from\nthe initial distribution.",
    "start": "3131650",
    "end": "3139480"
  },
  {
    "text": "And just to go\nover this, we want to maximize the objective\nwhere we have our RL phi, which",
    "start": "3139480",
    "end": "3145560"
  },
  {
    "text": "is our learned reward\nmodel, but we're going to add this\nterm beta log ratio,",
    "start": "3145560",
    "end": "3150870"
  },
  {
    "text": "and the ratio is r, the model\nwe're optimizing p theta, and our initial model, ppt.",
    "start": "3150870",
    "end": "3156680"
  },
  {
    "text": "And what this says is that if we\nassign a much higher probability to certain completion\nas compared",
    "start": "3156680",
    "end": "3162487"
  },
  {
    "text": "to our pretrained\nmodel, you're going to add an increasingly\nlarge penalty to it. And, simply, you're\npaying a price",
    "start": "3162488",
    "end": "3169220"
  },
  {
    "text": "for drifting too far from\nyour initial distribution. If you guys have taken\nmachine learning,",
    "start": "3169220",
    "end": "3174329"
  },
  {
    "text": "the expectation of this\nquantity is exactly the Kullback-Leibler\ndivergence or KL divergence",
    "start": "3174330",
    "end": "3180650"
  },
  {
    "text": "between p theta and ppt. So you're penalizing drifting\nbetween two distributions.",
    "start": "3180650",
    "end": "3186180"
  },
  {
    "text": "Go for it. Just a quick question. Shouldn't you also do\nthis, add a penalty in the previous version\nwhere you had to fine-tuning,",
    "start": "3186180",
    "end": "3194340"
  },
  {
    "text": "or is this only\nrelevant for the RL HF? That's a good question. So I think people do add\nsome kinds of regularization",
    "start": "3194340",
    "end": "3203480"
  },
  {
    "text": "in fine-tuning. It's not nearly not as critical\nwhen you're doing this with RL,",
    "start": "3203480",
    "end": "3208869"
  },
  {
    "text": "the incentive is to exploit this\nreward model as well as much as possible. And we'll see examples where\nthe learned reward predicts",
    "start": "3208870",
    "end": "3217070"
  },
  {
    "text": "it's doing really well,\nbut the true reward models are completely garbage. So it's much more important\nin this optimization.",
    "start": "3217070",
    "end": "3223756"
  },
  {
    "text": " Cool. So now this assume does--\nthis course does not",
    "start": "3223756",
    "end": "3233267"
  },
  {
    "text": "assume background on\nreinforcement learning, So we're not going to go\ninto reinforcement learning, but I just want to give a very\nhigh-level intuition about how",
    "start": "3233267",
    "end": "3239442"
  },
  {
    "text": "this works. And reinforcement\nlearning is not typically just used for language models. It's been applied to\nseveral domains of interest,",
    "start": "3239442",
    "end": "3246789"
  },
  {
    "text": "game-playing agents, robotics,\ndeveloping chip designs, and so",
    "start": "3246790",
    "end": "3252640"
  },
  {
    "text": "on. And the interest\nbetween RL and models, it also dates back to\nroughly 2016 as well,",
    "start": "3252640",
    "end": "3260960"
  },
  {
    "text": "but it's been really successful\nrecently and especially with the success of RLHF.",
    "start": "3260960",
    "end": "3267220"
  },
  {
    "text": "The general idea\nis that we're going to use our model\nthat we're optimizing to generate several\ncompletions for an instruction.",
    "start": "3267220",
    "end": "3273970"
  },
  {
    "text": "We're going to compute the\nreward under our learned reward model, and then we're going\nto simply try and update",
    "start": "3273970",
    "end": "3281800"
  },
  {
    "text": "our model to increase the\nprobability on the high reward completions. So when we sample a model,\nwe'll see completions",
    "start": "3281800",
    "end": "3287350"
  },
  {
    "text": "of varying quality,\nand we'll see some good completions, good\nsummaries for our tasks, some bad summaries for\nour task, and we'll",
    "start": "3287350",
    "end": "3293680"
  },
  {
    "text": "try to update our log\nprobabilities in a way such that the reward for--",
    "start": "3293680",
    "end": "3298930"
  },
  {
    "text": "when you use the updated\nmodel, you're typically in the high reward region.",
    "start": "3298930",
    "end": "3304900"
  },
  {
    "text": "Does the high level\nsummary make sense? Cool.",
    "start": "3304900",
    "end": "3310690"
  },
  {
    "text": "And RLHF is\nincredibly successful. I think this is a\nvery good example of-- This.",
    "start": "3310690",
    "end": "3315760"
  },
  {
    "text": "Is the same\nsummarization example, and I think the\nkey point here is",
    "start": "3315760",
    "end": "3321430"
  },
  {
    "text": "that the performance improves\nby increasing the model size for sure. We have seen this in\nmany different examples,",
    "start": "3321430",
    "end": "3328670"
  },
  {
    "text": "but what you can actually see is\nthat even very small models can outperform human completions\nif you train it with RLHF,",
    "start": "3328670",
    "end": "3335860"
  },
  {
    "text": "and this is exactly the\nresult you see here. The reference summaries\nare human generated, and when you evaluate--",
    "start": "3335860",
    "end": "3341710"
  },
  {
    "text": "when you ask humans,\nwhich ones they prefer, they often prefer the\nmodel generated summary over the human\ngenerated summary,",
    "start": "3341710",
    "end": "3347390"
  },
  {
    "text": "and this is something you\nonly observe with RLHF. Even at small scales-- and,\nagain, the same scaling phenomenon still holds here.",
    "start": "3347390",
    "end": "3353510"
  },
  {
    "text": "Bigger models do become more\nresponsive, but RLHF itself is very impactful here. ",
    "start": "3353510",
    "end": "3360880"
  },
  {
    "text": "Cool. The problem with RLHF is that\nit's just incredibly complex.",
    "start": "3360880",
    "end": "3367190"
  },
  {
    "text": "Like I gave you a very\nhigh-level summary that's, like, doesn't-- there's whole courses on this\nfor a reason, so it just--",
    "start": "3367190",
    "end": "3375670"
  },
  {
    "text": "and this image is not\nfor you to understand, it's just completely\nto intimidate you.",
    "start": "3375670",
    "end": "3380730"
  },
  {
    "text": "So you want to fit a value\nfunction to something? You have to sample\nthe model a lot.",
    "start": "3380730",
    "end": "3387040"
  },
  {
    "text": "It can be sensitive to a\nlot of hyperparameters. So there's a lot\nthat goes on here. And, yeah, if you start\nimplementing an RLHF pipeline,",
    "start": "3387040",
    "end": "3396869"
  },
  {
    "text": "it can be very hard. And this is the reason\nwhy a lot of RLHF was restricted to very, very\nlike compute, high resource",
    "start": "3396870",
    "end": "3403770"
  },
  {
    "text": "places, and it was\nnot very accessible. So what we're going to talk\nabout and cover in this course",
    "start": "3403770",
    "end": "3409320"
  },
  {
    "text": "is something called direct\npreference optimization, which is a much simpler\nalternative to RLHF, and, hopefully, that's\nmuch more accessible.",
    "start": "3409320",
    "end": "3415900"
  },
  {
    "text": "But please bear with me, there\nwill be a lot of math on here, but the end goal of\nthe math is to come up",
    "start": "3415900",
    "end": "3421650"
  },
  {
    "text": "with a very simple algorithm. So, hopefully, it's-- and feel\nfree to stop me and ask me",
    "start": "3421650",
    "end": "3427140"
  },
  {
    "text": "questions. A couple questions,\nthe amount of examples you need from humans,\nin terms of GPT 4",
    "start": "3427140",
    "end": "3433770"
  },
  {
    "text": "versus 3, how much is the number\nof parameters in the base model help with needing to reduce\nthe number of parameters,",
    "start": "3433770",
    "end": "3440800"
  },
  {
    "text": "or in order to reduce the number\nof examples from humans for RLHF [INAUDIBLE].",
    "start": "3440800",
    "end": "3446529"
  },
  {
    "text": "Well, yeah, that's a\nreally good question. So generally speaking,\nas the-- if you hold the data set size\nconstant and simply increase",
    "start": "3446530",
    "end": "3453600"
  },
  {
    "text": "the model size, it will\nimprove quite a lot. But the nice thing is that\nyou can reuse the data",
    "start": "3453600",
    "end": "3459510"
  },
  {
    "text": "and you can keep adding data as\nyou keep like scaling models up. So, typically, nobody\ntries to reduce",
    "start": "3459510",
    "end": "3465839"
  },
  {
    "text": "the amount of data collection. You just keep increasing\nboth the things up. Yeah.",
    "start": "3465840",
    "end": "3471930"
  },
  {
    "text": "Cool. So we talked about RLHF\nand the current pipeline is something like--",
    "start": "3471930",
    "end": "3478200"
  },
  {
    "text": "we train a reward model\non the comparison data that we have seen so far,\nand we're going to optimize. We're going to start with our\npretrained or instruction-tuned",
    "start": "3478200",
    "end": "3484740"
  },
  {
    "text": "model and convert\nit into an RLHF model using the reinforcement\nlearning techniques.",
    "start": "3484740",
    "end": "3490080"
  },
  {
    "text": "Now really the key idea in\ndirect preference optimization is, what if we could just simply\nwrite a reward model in terms",
    "start": "3490080",
    "end": "3497010"
  },
  {
    "text": "of our language model itself? Now, to intuitively understand\nthat what is going on,",
    "start": "3497010",
    "end": "3502240"
  },
  {
    "text": "a language model is\nassigning probabilities to whatever is the most\nplausible completion next,",
    "start": "3502240",
    "end": "3507460"
  },
  {
    "text": "but those plausible completions\nmight not be what we intended. But you could restrict\nthe probability simply to the completions\nthat a human might like,",
    "start": "3507460",
    "end": "3515650"
  },
  {
    "text": "and then the log\nprobabilities of your model might represent something\nwhich the humans might like, and not just some arbitrary\ncompletion on the internet.",
    "start": "3515650",
    "end": "3522880"
  },
  {
    "text": "So there is a direct\ncorrespondence between the log probability that a\nlanguage model assigns,",
    "start": "3522880",
    "end": "3528760"
  },
  {
    "text": "and how much a human\nmight like the answer. They can have a direct\ncorrespondence in them.",
    "start": "3528760",
    "end": "3534040"
  },
  {
    "text": "And this is not some\narbitrary intuition that I'm trying to come up\nwith, we will derive this mathematically.",
    "start": "3534040",
    "end": "3541320"
  },
  {
    "text": "So the general idea with\ndirect preference optimization is going to be we're going to\nwrite down reward model in terms",
    "start": "3541320",
    "end": "3547319"
  },
  {
    "text": "of our language model, and now\nthat we can write our reward model in terms of our language\nmodel, we can simply solve--",
    "start": "3547320",
    "end": "3553859"
  },
  {
    "text": "directly fit our reward model\nto the preference data we have, and we don't need to\ndo the RL step at all.",
    "start": "3553860",
    "end": "3561327"
  },
  {
    "text": "So we started off with\nsome preference data, and we simply fit\nour reward model to it, which directly optimizes\nthe language parameters.",
    "start": "3561327",
    "end": "3568940"
  },
  {
    "text": "And maybe at a higher level,\nwhy is this even possible? We did this really\ncumbersome process",
    "start": "3568940",
    "end": "3574369"
  },
  {
    "text": "with fitting a reward\nmodel and optimizing it, but in the whole process,\nthe only external information",
    "start": "3574370",
    "end": "3580490"
  },
  {
    "text": "that was being\nadded to the system was human labels, labels\non the preference data. When we optimize a\nlearned reward model,",
    "start": "3580490",
    "end": "3587370"
  },
  {
    "text": "there is no new information\nbeing added into the system. So this is why something\nlike this is even possible.",
    "start": "3587370",
    "end": "3593480"
  },
  {
    "text": "For quite a few years,\nthis was not obvious, but as you will see,\nsome of these results",
    "start": "3593480",
    "end": "3598940"
  },
  {
    "text": "start to make sense. So we're going to derive\ndirect preference optimization.",
    "start": "3598940",
    "end": "3604862"
  },
  {
    "text": "I'll be here after the class\nas well if you have questions, but I'll hopefully\nthis is clear. So yes, we discussed\nthat we wanted",
    "start": "3604862",
    "end": "3614700"
  },
  {
    "text": "to solve this expected\nreward problem where we want to maximize\nthe expected reward, but we subtract this term, which\nis the beta log ratio, which",
    "start": "3614700",
    "end": "3622349"
  },
  {
    "text": "essentially penalizes the\ndistance between where our current model is and\nwhere we started off. So we don't want to drift too\nfar away from where we started.",
    "start": "3622350",
    "end": "3634080"
  },
  {
    "text": "Now it turns out that this\nspecific problem, instead of doing an iterative routine,\nthere's actually a closed form",
    "start": "3634080",
    "end": "3642120"
  },
  {
    "text": "solution to this problem. So the closed form solution\nlooks something like this.",
    "start": "3642120",
    "end": "3649410"
  },
  {
    "text": "Again, if you have seen\nthe Boltzmann distribution or something to\nthat effect before, this is basically the\nsame idea, but the idea",
    "start": "3649410",
    "end": "3655892"
  },
  {
    "text": "is this, that we're going to\ntake a pretrained distribution ppt y given x, and we're going\nto reweight the distribution",
    "start": "3655892",
    "end": "3662220"
  },
  {
    "text": "by the expected reward. So if a completion has\na very high reward,",
    "start": "3662220",
    "end": "3668530"
  },
  {
    "text": "it's going to have a\nhigher probability mass, and if it has a\nlower reward, it's going to have a lower\nprobability mass, and it's determined by\nthe expected reward.",
    "start": "3668530",
    "end": "3675420"
  },
  {
    "text": "And beta is a hyperparameter\nwhich essentially governs what is the trade off\nbetween the reward",
    "start": "3675420",
    "end": "3681109"
  },
  {
    "text": "model and the constraint. And as beta becomes\nlower and lower, you're going to start paying\nmore and more attention",
    "start": "3681110",
    "end": "3687890"
  },
  {
    "text": "to the reward model. So the probabilities\nlook something like this.",
    "start": "3687890",
    "end": "3693329"
  },
  {
    "text": "And there's this really\nannoying term the zx, and the reason why it exists\nis that the numerator by itself",
    "start": "3693330",
    "end": "3700670"
  },
  {
    "text": "is not normalized. It's not a probability\ndistribution. So to construct an actual\nprobability distribution,",
    "start": "3700670",
    "end": "3706730"
  },
  {
    "text": "you have to normalize\nit, and z is simply just this normalization. So real quick.",
    "start": "3706730",
    "end": "3712040"
  },
  {
    "text": "If we write z as the sum\nof all y, [INAUDIBLE]. Yeah, and that's exactly like--\nit's sum over all ys for a given",
    "start": "3712040",
    "end": "3719150"
  },
  {
    "text": "instruction, and that's\nexactly what this is very pesky is like it's intractable. If I take an instruction and\ntry to sum over every possible",
    "start": "3719150",
    "end": "3725990"
  },
  {
    "text": "completion and not just\nsyntactically correct ones, every single possible, we have\n50,000 tokens, maybe even more,",
    "start": "3725990",
    "end": "3733220"
  },
  {
    "text": "and the completions can\ngo arbitrarily long. So this space is\ncompletely intractable. This quantity is not\neasy to approximate even.",
    "start": "3733220",
    "end": "3739350"
  },
  {
    "text": " So the main point here is that\nif you're given a reward model,",
    "start": "3739350",
    "end": "3746750"
  },
  {
    "text": "you can actually-- there does\nexist at least a closed form solution which tells us\nwhat the optimal policy will look like or optimal language\nmodel will look like.",
    "start": "3746750",
    "end": "3754240"
  },
  {
    "text": "But if you do a\nlittle bit of algebra, just move some terms around,\ntake a logarithm here or there. I promise this is\nnot very complicated,",
    "start": "3754240",
    "end": "3760850"
  },
  {
    "text": "you can actually\nexpress the reward model in terms of the\nlanguage model itself.",
    "start": "3760850",
    "end": "3765970"
  },
  {
    "text": "And I think this term is\nreasonably intuitive as well. What it says is\nthat a completion y",
    "start": "3765970",
    "end": "3772660"
  },
  {
    "text": "hat has a high reward. If the model, my optimal policy,\nassigns a higher probability",
    "start": "3772660",
    "end": "3778630"
  },
  {
    "text": "to it relative to my\ninitialized model. And this is scaled by\nbeta, so the beta log ratio",
    "start": "3778630",
    "end": "3784600"
  },
  {
    "text": "is what we're looking at here. And the partition function,\nlet's just ignore it for now,",
    "start": "3784600",
    "end": "3790700"
  },
  {
    "text": "but it's intractable,\nbut the beta log ratio is the key part here.",
    "start": "3790700",
    "end": "3795880"
  },
  {
    "text": "Is everyone following along? Awesome. OK.",
    "start": "3795880",
    "end": "3801310"
  },
  {
    "text": "So right now I'm talking\nabout optimal policies. But really every\npolicy is probably",
    "start": "3801310",
    "end": "3808210"
  },
  {
    "text": "optimal for some\nkind of a reward. This is mathematically\ntrue as well. So the important bit here is\nthat you can actually express--",
    "start": "3808210",
    "end": "3816099"
  },
  {
    "text": "you take a current policy,\ntake your initialized model, and you can get some kind\nof a reward model out of it.",
    "start": "3816100",
    "end": "3822500"
  },
  {
    "text": "And this is the exact\nidentity, which leads to this. So reward model can be expressed\nin terms of your language model,",
    "start": "3822500",
    "end": "3829180"
  },
  {
    "text": "barring the log\npartition term, which we'll see what happens to it.",
    "start": "3829180",
    "end": "3835050"
  },
  {
    "text": "Go for it. I'm sorry. I don't know how you got there. Why is it that we can\nswap-- because there is a thing that we're trying to\noptimize and how to [INAUDIBLE]",
    "start": "3835050",
    "end": "3841795"
  },
  {
    "text": "to PRL. Yeah, for now, we're not\noptimizing any reward model.",
    "start": "3841795",
    "end": "3847000"
  },
  {
    "text": "All I'm saying is that if I\ntake my current language model, it probably represents\nsome kind of a reward model",
    "start": "3847000",
    "end": "3853590"
  },
  {
    "text": "implicitly because of this\nrelationship-- because this holds for every P star and every\nreward model, what I'm saying",
    "start": "3853590",
    "end": "3861930"
  },
  {
    "text": "is that if I plug in my\ncurrent language model, it also represents some\nkind of a reward model.",
    "start": "3861930",
    "end": "3867520"
  },
  {
    "text": "I'm not saying it's optimal. But I want to say because at\nthe beginning, PRL is ppt.",
    "start": "3867520",
    "end": "3875130"
  },
  {
    "text": "Yes. And so we just get that\nthe reward is basically-- 0. [INAUDIBLE] 0, and\nso what do we do?",
    "start": "3875130",
    "end": "3880750"
  },
  {
    "text": "Initially it's 0, but we\ncan optimize the parameters. Yeah, but that's\na good observation",
    "start": "3880750",
    "end": "3886410"
  },
  {
    "text": "that it's basically\n0 in the beginning. But how do we start\noptimizing if-- I'll get to that.",
    "start": "3886410",
    "end": "3891690"
  },
  {
    "text": "OK. Any other questions? So the idea is that\ngiven the language model",
    "start": "3891690",
    "end": "3898110"
  },
  {
    "text": "you have no reward model such\nthat makes the language model",
    "start": "3898110",
    "end": "3903270"
  },
  {
    "text": "optimal [INAUDIBLE]. That's the next step.",
    "start": "3903270",
    "end": "3909570"
  },
  {
    "text": "But the key idea is that my\nlanguage model, the probability is already implicitly\ndefine a reward model.",
    "start": "3909570",
    "end": "3916710"
  },
  {
    "text": "I think that's really\nthe main point here, and this mathematical\nrelationship is exact.",
    "start": "3916710",
    "end": "3922500"
  },
  {
    "text": "Cool. Now, I mean, I'm\nobviously ignoring the elephant in the room here,\nwhich is the partition function.",
    "start": "3922500",
    "end": "3929940"
  },
  {
    "text": "It's not going to\nmagically vanish away, so if this was just the\nbeta log ratio, that would be really nice.",
    "start": "3929940",
    "end": "3935079"
  },
  {
    "text": "I can compute all\nthese quantities. I know how to compute the log\nprobability under my language model. I know how to compute\nthe log probability",
    "start": "3935080",
    "end": "3941378"
  },
  {
    "text": "under my pretrained model, and\nI can compute the reward score, and I can optimize this. But I don't know what to\ndo about the log partition",
    "start": "3941378",
    "end": "3948930"
  },
  {
    "text": "function. This is where\nsomething fun happens. So recall what the reward\nmodeling objective was.",
    "start": "3948930",
    "end": "3958320"
  },
  {
    "text": "When we started\noff-- we started off with our friends\nBradley-Terry again, and what we really\nwanted to optimize",
    "start": "3958320",
    "end": "3964417"
  },
  {
    "text": "was the reward difference\nbetween the winning completion and the losing completion.",
    "start": "3964417",
    "end": "3969690"
  },
  {
    "text": "And really, I mean,\nthat's all we care about. We don't care about\nthe exact reward itself, what we care\nabout is maximizing",
    "start": "3969690",
    "end": "3978490"
  },
  {
    "text": "the difference between\nwinning and losing completion. And that's actually\nreally key here because if you plug-in the\ndefinition of the RM theta",
    "start": "3978490",
    "end": "3987480"
  },
  {
    "text": "there, what you'll observe is\nthat the partition function actually just cancels out.",
    "start": "3987480",
    "end": "3993070"
  },
  {
    "text": "Now why does it cancel out. The input is exactly the same.",
    "start": "3993070",
    "end": "3999190"
  },
  {
    "text": "The x is actually exactly\nthe same in the difference, so the partition function\nzx will just cancel out.",
    "start": "3999190",
    "end": "4004730"
  },
  {
    "text": "It's the same in both the terms. So what you get is that\nthe reward difference between the winning\nand losing completion is the differences between the\nbeta log ratio for the winning",
    "start": "4004730",
    "end": "4012349"
  },
  {
    "text": "and losing completion. You can plug-in the terms,\nyou can work it out. It's fairly simple.",
    "start": "4012350",
    "end": "4018090"
  },
  {
    "text": "So the partition\nfunction, which was our-- which was something\nwe could not address,",
    "start": "4018090",
    "end": "4023960"
  },
  {
    "text": "we could not compute, actually,\njust simply vanished away. I'm so sorry. Z doesn't appear in the\nBradley-Terry model.",
    "start": "4023960",
    "end": "4030490"
  },
  {
    "text": "But it appears here\nin this equation. So how does plugging in violate\nyour model transparency?",
    "start": "4030490",
    "end": "4037750"
  },
  {
    "text": "So we're going to take this\nequation, the last line that you see, and we're going to\nplug in place of R and phi,",
    "start": "4037750",
    "end": "4049015"
  },
  {
    "text": "and in this the\nfirst loss equation. Oh, I see. Got it. Yeah. So the first loss equation is\nthe Bradley-Terry loss model.",
    "start": "4049015",
    "end": "4056260"
  },
  {
    "text": "Cool. So this really is it. I mean, the key observation\nis we could express our reward",
    "start": "4056260",
    "end": "4062635"
  },
  {
    "text": "model in terms of\nlanguage model, and our problems with\nthe partition function actually go away\nbecause we're optimizing the Bradley-Terry model.",
    "start": "4062635",
    "end": "4068690"
  },
  {
    "text": "And what you get is\nsomething like this, is that we're going to express\nthe loss function directly",
    "start": "4068690",
    "end": "4077140"
  },
  {
    "text": "in terms of our language\nmodel parameters theta, and we're going to be able to\ndirectly optimize on our data",
    "start": "4077140",
    "end": "4085180"
  },
  {
    "text": "without doing any\nRL steps or not, and this is simply a binary\nclassification problem. So we're really just\ntrying to classify",
    "start": "4085180",
    "end": "4091900"
  },
  {
    "text": "whether an answer\nis good or bad, and that's really\nwhat we're doing.",
    "start": "4091900",
    "end": "4097600"
  },
  {
    "text": "Before I go on like people\nwant to absorb this in, I mean, feel they're OK with it?",
    "start": "4097600",
    "end": "4103005"
  },
  {
    "text": " I don't get why good-- and why\nwin and why lose come from.",
    "start": "4103005",
    "end": "4110097"
  },
  {
    "text": "Are they human input or\nare they just [INAUDIBLE]. Good question. It's the same data set\nwe started with in RLHF",
    "start": "4110097",
    "end": "4116259"
  },
  {
    "text": "as well, but the way the\nprocess works is that you take a set of instructions\nand get the model",
    "start": "4116260",
    "end": "4121450"
  },
  {
    "text": "to generate some\nanswers, and then you get humans to label\nwhich answer they prefer. So they're model-generated.",
    "start": "4121450",
    "end": "4127449"
  },
  {
    "text": "Typically, they can be\nhuman generated as well, but they typically\nmodel generated. And then you get some\npreference labels.",
    "start": "4127450",
    "end": "4134285"
  },
  {
    "text": "All you need is a label saying,\nwhich is a better answer.",
    "start": "4134285",
    "end": "4139450"
  },
  {
    "text": "What do you lose\nhere [INAUDIBLE]. You must be losing some\ninformation because of the lack",
    "start": "4139450",
    "end": "4145479"
  },
  {
    "text": "of information about other-- you're canceling out your--",
    "start": "4145479",
    "end": "4151366"
  },
  {
    "text": "because of the lack of any\ninformation about the partition function, you are bound\nto lose information",
    "start": "4151366",
    "end": "4158229"
  },
  {
    "text": "about other possible\ncompletions, which you would have taken into\naccount in standard RLHF, right?",
    "start": "4158229",
    "end": "4166203"
  },
  {
    "text": "That's a really good question. I don't think I'll be\nable to completely answer this question in time,\nbut partition function",
    "start": "4166204",
    "end": "4172420"
  },
  {
    "text": "is almost kind of\na free variable. So I think the problem here\nis that the reward model--",
    "start": "4172420",
    "end": "4177759"
  },
  {
    "text": "think of when you-- there's\nmany reward models that satisfy this optimization. So there's a free variable here\nthat you can actually completely",
    "start": "4177760",
    "end": "4186220"
  },
  {
    "text": "remove, and that's what this\noptimization benefits from. So think of it this way-- if I assign something\na reward of plus 1",
    "start": "4186220",
    "end": "4191770"
  },
  {
    "text": "and assign something\na reward of minus 1, that's basically\nthe same as saying it's a reward of plus 199, and\nit will give you the same loss.",
    "start": "4191770",
    "end": "4201888"
  },
  {
    "text": "So that scale doesn't-- it's shift invariant in a ways.",
    "start": "4201888",
    "end": "4208560"
  },
  {
    "text": "Isn't that somehow not\nwhat we want, though? ",
    "start": "4208560",
    "end": "4215055"
  },
  {
    "text": "If you're actually\ntraining a reward model, 199 is a much-- you should\npay much less attention",
    "start": "4215055",
    "end": "4221850"
  },
  {
    "text": "to that as compared to minus\n1 [INAUDIBLE] 0 or something.",
    "start": "4221850",
    "end": "4227110"
  },
  {
    "text": "What we are assuming\nis our choice model here is if a human prefers\nsomething over the other,",
    "start": "4227110",
    "end": "4232710"
  },
  {
    "text": "the probability is governed\nonly by the difference between the rewards. So that's an assumption\nthat every RLHF also makes",
    "start": "4232710",
    "end": "4239890"
  },
  {
    "text": "and DP also makes. Now is that assumption true? Not completely true, but it\nholds to a fairly large degree.",
    "start": "4239890",
    "end": "4250350"
  },
  {
    "text": "But that's a good question. Cool. I'll move on interest of time.",
    "start": "4250350",
    "end": "4257040"
  },
  {
    "text": "And really like, I mean, the\ngoal of this plot is to-- we actually get fairly\nperformant models when",
    "start": "4257040",
    "end": "4262590"
  },
  {
    "text": "we optimize things with DPO. So in this plot, I think the\nmain thing that you should look",
    "start": "4262590",
    "end": "4267840"
  },
  {
    "text": "at is PPO, which is the\ntypical RLHF pipeline, and we are evaluating the\nmodels for summarization, and we're comparing\ntwo human summaries.",
    "start": "4267840",
    "end": "4275170"
  },
  {
    "text": "And what we find is that\nDPO and people do similarly, but you're really not\nlosing much by just",
    "start": "4275170",
    "end": "4280410"
  },
  {
    "text": "doing the DPO procedure\ninstead of RLHF. And that's really compelling\nbecause DPO is simply a classification loss instead of\na whole reinforcement learning",
    "start": "4280410",
    "end": "4287850"
  },
  {
    "text": "procedure. So I'm going to\nquickly summarize.",
    "start": "4287850",
    "end": "4293010"
  },
  {
    "text": "What we have seen\nthus far is that we want to optimize for\nhuman preferences,",
    "start": "4293010",
    "end": "4298960"
  },
  {
    "text": "and the way we do this is\nlike instead of relying on uncalibrated scores,\nwe're getting comparison data and feedback on that.",
    "start": "4298960",
    "end": "4304540"
  },
  {
    "text": "And we use this ranking\ndata to either do something like RLHF, where we\nfirst fit a reward model",
    "start": "4304540",
    "end": "4310619"
  },
  {
    "text": "and optimize using\nreinforcement learning, or we do something like\ndirect preference optimization",
    "start": "4310620",
    "end": "4316205"
  },
  {
    "text": "where we simply take the data\nset and do a classification loss on that.",
    "start": "4316205",
    "end": "4322020"
  },
  {
    "text": "And, yeah, there's trade\noffs in these algorithms. People, when they have a\nlot of computational budget,",
    "start": "4322020",
    "end": "4327370"
  },
  {
    "text": "they typically maybe\ngo for research over some routine like that. But if you're really looking\nto get the bang for your buck,",
    "start": "4327370",
    "end": "4333330"
  },
  {
    "text": "I mean, you might\nwant to go for DPO, and that's probably going\nto work out of the box.",
    "start": "4333330",
    "end": "4339000"
  },
  {
    "text": "It's still an active\narea of research. People are still trying to\nunderstand how to best work with these algorithms, so I'm\nnot making any strong claims",
    "start": "4339000",
    "end": "4345960"
  },
  {
    "text": "here, but both of these\nalgorithms are very effective. DPO is just much\nsimpler to work with.",
    "start": "4345960",
    "end": "4351390"
  },
  {
    "text": "Cool. So, yeah, I mean,\nlet's see we went through all this\ninstruction-tuning, RLHF,",
    "start": "4351390",
    "end": "4358050"
  },
  {
    "text": "what do we get? InstructGPT is the first model\nwhich followed this pipeline.",
    "start": "4358050",
    "end": "4365110"
  },
  {
    "text": "It defined this pipeline. So we got models which\ndid 30,000 or so tasks. Remember when we were doing like\nonly one task and now we have",
    "start": "4365110",
    "end": "4372500"
  },
  {
    "text": "scaled it up from 1,000 tasks\nto 30,000 different tasks with many, many different examples,\nso that's where we are with",
    "start": "4372500",
    "end": "4379159"
  },
  {
    "text": "InstructGPT, and it follows this\npipeline that we just described. In this case, they're following\na specific RLHF pipeline",
    "start": "4379160",
    "end": "4385010"
  },
  {
    "text": "where we explicitly\nfit a reward model, and then do some kind of\na reinforcement learning routine on top of it.",
    "start": "4385010",
    "end": "4393080"
  },
  {
    "text": "And, yeah, the task\ncollected from labelers looks something like this. I'll leave it to\nyour imagination",
    "start": "4393080",
    "end": "4398910"
  },
  {
    "text": "or you can look at the details. But how we started off with\nthis model was something like completions we see from\nGPT-3, which explained the moon",
    "start": "4398910",
    "end": "4408560"
  },
  {
    "text": "landing to a six-year and\nit does not really following the instructions, but\nInstructGPT will give you something which is meaningful.",
    "start": "4408560",
    "end": "4414840"
  },
  {
    "text": "So it's inferring\nwhat a user wanted from the specific\ninstruction, and it's converting to a realistic\nanswer that a user might like.",
    "start": "4414840",
    "end": "4424070"
  },
  {
    "text": "And, yeah, these are\njust more examples of what an\nInstructGPT like model would do, whereas your\nbase model might not",
    "start": "4424070",
    "end": "4431390"
  },
  {
    "text": "follow the instructions to\nyour desired intentions. And yeah, we went from\nInstructGPT to ChatGPT,",
    "start": "4431390",
    "end": "4439760"
  },
  {
    "text": "and it was essentially\nthis pipeline. The key difference here\nis that it is still",
    "start": "4439760",
    "end": "4445340"
  },
  {
    "text": "doing the\ninstruction-tuning, but it is more optimized for\ndialogue, more optimized for interacting with users.",
    "start": "4445340",
    "end": "4451800"
  },
  {
    "text": "So the core\nalgorithmic techniques that we discussed today\nare what give us ChatGPT,",
    "start": "4451800",
    "end": "4457148"
  },
  {
    "text": "but you have to be really\ncareful about the data you're training on. And that's really\nthe whole game,",
    "start": "4457148",
    "end": "4462440"
  },
  {
    "text": "but this is the\nfoundation for ChatGPT. And yeah, it follows the\nsame pipeline as well,",
    "start": "4462440",
    "end": "4470449"
  },
  {
    "text": "and you might look at-- you\nmight interact with ChatGPT. I'm sure you all have interacted\nwith it in some form or not,",
    "start": "4470450",
    "end": "4476010"
  },
  {
    "text": "but this is an example of what\na ChatGPT interaction might look like. ",
    "start": "4476010",
    "end": "4482269"
  },
  {
    "text": "You want to make a\nGen Z, so, I mean, the idea here is that it's\nvery good at responding",
    "start": "4482270",
    "end": "4487640"
  },
  {
    "text": "to instructions and intent. This is not something\nthat we could even few shot in very easily.",
    "start": "4487640",
    "end": "4493890"
  },
  {
    "text": "These are instructions are\nhard to come examples for, but this is probably\nnot something you're trained on either, but\nit's able to infer the intent",
    "start": "4493890",
    "end": "4500970"
  },
  {
    "text": "and generalize\nvery, very nicely. And that's something I find\npersonally very remarkable.",
    "start": "4500970",
    "end": "4507990"
  },
  {
    "text": "Cool. And there's been\na lot of progress on the open source\nfront as well. So DPO is much simpler\nand much more efficient,",
    "start": "4507990",
    "end": "4514860"
  },
  {
    "text": "and essentially, all the\nopen source models these days are using DPO, so\nthis is a leaderboard",
    "start": "4514860",
    "end": "4520889"
  },
  {
    "text": "that is maintained\nby hugging face here. So, I mean, 9 out of 10 models\nhere are trained with DPO.",
    "start": "4520890",
    "end": "4526780"
  },
  {
    "text": "So that's been something that's\nbeen-- enabled the open source community to instruction-tune\ntheir model betters as well.",
    "start": "4526780",
    "end": "4533019"
  },
  {
    "text": "And same is being used in many\nproduction models now as well. Mistral is using DPO. Llama three use DPO.",
    "start": "4533020",
    "end": "4539440"
  },
  {
    "text": "So these are very,\nvery strong models which are nearly GPT-4\nlevel, and they're also starting to use these\nalgorithms as well.",
    "start": "4539440",
    "end": "4547970"
  },
  {
    "text": "And something that's\nvery cool to see is we went through\nall this optimization and, I mean, math and stuff,\nbut what is really fundamentally",
    "start": "4547970",
    "end": "4555500"
  },
  {
    "text": "changing in the behavior. And I think this is a really\ngood example, is that if you",
    "start": "4555500",
    "end": "4560510"
  },
  {
    "text": "simply ask an instruction for-- and ask for an SFT output from\nan instruction-tuned model,",
    "start": "4560510",
    "end": "4565948"
  },
  {
    "text": "you'll get something like this. But when you RLHF the\nmodel, you actually get a lot more details\nin your answer,",
    "start": "4565948",
    "end": "4571423"
  },
  {
    "text": "and they'll probably organize\nthe answers a little better. And this is something that\nthey maybe humans prefer.",
    "start": "4571423",
    "end": "4577110"
  },
  {
    "text": "That's why it's a property that\nis emerging in these model. But it's something that's a very\nclear difference between simply",
    "start": "4577110",
    "end": "4584150"
  },
  {
    "text": "instruction-tuned models, and\nsome models which are RLHF. ",
    "start": "4584150",
    "end": "4590750"
  },
  {
    "text": "So, yeah, we discussed\nthis whole RLHF routine",
    "start": "4590750",
    "end": "4595790"
  },
  {
    "text": "where we are directly\nmodeling the preferences, and we are generalizing\nbeyond label data,",
    "start": "4595790",
    "end": "4601010"
  },
  {
    "text": "and we also discuss\nRL can be very tricky to correctly implement. The DPO sort of implements this\nor l avoid some of these issues,",
    "start": "4601010",
    "end": "4608840"
  },
  {
    "text": "and we briefly also touched\nupon the idea of reward model and reward hacking.",
    "start": "4608840",
    "end": "4615039"
  },
  {
    "text": "And when you're optimizing\nfor learned reward models, you will often see\nthis example is that there is a way for it to\njust simply crash into the--",
    "start": "4615040",
    "end": "4625760"
  },
  {
    "text": "some keep repetitively\ncrashing the boat to get more and more points. That wasn't the\ngoal of this game.",
    "start": "4625760",
    "end": "4632150"
  },
  {
    "text": "So this is a very\ncommon example that is shown for reward hacking.",
    "start": "4632150",
    "end": "4637640"
  },
  {
    "text": "If you do not specify\na rewards well, the models can learn\nweird behaviors which",
    "start": "4637640",
    "end": "4642670"
  },
  {
    "text": "are not your desired intent. And this is something a lot\nof people worry about as well. Part of the reason is\nreinforcement learning",
    "start": "4642670",
    "end": "4648820"
  },
  {
    "text": "is a very strong\noptimization algorithm. It's at the heart of\nAlphaGo, AlphaZero, which",
    "start": "4648820",
    "end": "4654580"
  },
  {
    "text": "results in superhuman models. So you have to be careful\nabout how you specify things. And the other thing is\nlike even optimizing",
    "start": "4654580",
    "end": "4660954"
  },
  {
    "text": "for human preferences is\noften not the right thing, because humans do not\nalways like things which",
    "start": "4660955",
    "end": "4666070"
  },
  {
    "text": "are in their best interest. So something that\nemerges is that they like authoritative\nand helpful answers,",
    "start": "4666070",
    "end": "4671389"
  },
  {
    "text": "but they often don't necessarily\nlike truthful answers. So one property that\nhappens is that they",
    "start": "4671390",
    "end": "4679030"
  },
  {
    "text": "prefer authoritativeness\nmore than correctness, which is maybe not something nice. Please go ahead.",
    "start": "4679030",
    "end": "4684770"
  },
  {
    "text": "[INAUDIBLE] besides, I'm curious\nif maybe like ChatGPT being so now widely used by\nthe public will maybe",
    "start": "4684770",
    "end": "4690850"
  },
  {
    "text": "change how people really\nraise their voice because I, at least, feel now\nwhen I go to ChatGPT",
    "start": "4690850",
    "end": "4696280"
  },
  {
    "text": "and I have something\nthat gives me five detailed paragraphs\nof information, sometimes I'm just\nannoyed by that. That's not what I wanted.",
    "start": "4696280",
    "end": "4701772"
  },
  {
    "text": "But maybe in the\noriginal reward function in the original\nratings, people actually preferred that and\nnow prefer less.",
    "start": "4701772",
    "end": "4707540"
  },
  {
    "text": "Yeah, that's a great point\nbecause as these models integrate more and\nmore into our systems,",
    "start": "4707540",
    "end": "4713047"
  },
  {
    "text": "they're going to collect\nmore and more data, and they will pick up on\nthings, maybe undesirable things",
    "start": "4713047",
    "end": "4719110"
  },
  {
    "text": "as well. As far as I understand,\nChatGPT is really cutting down on the verbosity, which\nis like a huge issue",
    "start": "4719110",
    "end": "4725190"
  },
  {
    "text": "that all of these models\nare trying to cut down on, and they are dealing with that. Part of the reason\nwhy that emerges",
    "start": "4725190",
    "end": "4731070"
  },
  {
    "text": "is that when you collect\npreference data at scale, people are not necessarily\nreading the answers. The turkers might just simply\nchoose the longer answer,",
    "start": "4731070",
    "end": "4738550"
  },
  {
    "text": "and that's a property that\nactually goes into these models, but hopefully these things\nwill improve over time",
    "start": "4738550",
    "end": "4744000"
  },
  {
    "text": "as they get more feedback. And, yeah, hallucinations\nis not a problem that is going to\ngo away with RL,",
    "start": "4744000",
    "end": "4749173"
  },
  {
    "text": "and we talked a bit\nabout reward hacking as well, biases from\nthings and so on.",
    "start": "4749173",
    "end": "4756130"
  },
  {
    "text": "But, hopefully, I mean,\nwhat I want to conclude at is we started with\npretrained models.",
    "start": "4756130",
    "end": "4761594"
  },
  {
    "text": "We had these things which could\npredict text and we got ChatGPT, and hopefully, it's\na little more clearer",
    "start": "4761595",
    "end": "4766830"
  },
  {
    "text": "how we go from something\nlike that to ChatGPT. And I'll end here.",
    "start": "4766830",
    "end": "4774670"
  },
  {
    "text": "Thanks. [APPLAUSE] ",
    "start": "4774670",
    "end": "4782000"
  }
]