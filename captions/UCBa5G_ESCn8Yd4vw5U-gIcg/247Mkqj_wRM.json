[
  {
    "text": "So first, let's discuss about how do we define a single layer of a graph neural network, right?",
    "start": "4100",
    "end": "11910"
  },
  {
    "text": "So what- what goes into a single layer? A single layer has two components.",
    "start": "11910",
    "end": "17685"
  },
  {
    "text": "It has a component of a message transformation. And it has a component of message aggregation,",
    "start": "17685",
    "end": "24465"
  },
  {
    "text": "and as I mentioned, different graph neural network architectures basically differ in how these operations are being done,",
    "start": "24465",
    "end": "33195"
  },
  {
    "text": "among other kinds of things that differ between them. So what is the idea of a single GNN layer?",
    "start": "33195",
    "end": "39985"
  },
  {
    "text": "The idea is that we want to compress a set of messages, a set of vectors coming from the children,",
    "start": "39985",
    "end": "46025"
  },
  {
    "text": "from the- from the bottom layer of the neural network. In some sense compress them by aggregating them.",
    "start": "46025",
    "end": "53614"
  },
  {
    "text": "And we are going to- to do this as a two-step process, as a message transformation and message aggregation, right?",
    "start": "53615",
    "end": "59990"
  },
  {
    "text": "So if we think about this, we are getting a set of children at the- at the bottom, a set of inputs.",
    "start": "59990",
    "end": "65570"
  },
  {
    "text": "We have- we have an output. What do we have to do is take the message from each of the child and transform it.",
    "start": "65570",
    "end": "72229"
  },
  {
    "text": "Then we have to aggregate these messages into a single message and pass it on. So the way you can think of this is that we get",
    "start": "72230",
    "end": "78980"
  },
  {
    "text": "messages, denoted as circles here, from the three neighbors, from the previous layer.",
    "start": "78980",
    "end": "84365"
  },
  {
    "text": "We also have our own message, right? Message of the node v from the previous layer.",
    "start": "84365",
    "end": "89555"
  },
  {
    "text": "Somehow we want to combine this information to create the next level embedding or to the next level message for this node of interest.",
    "start": "89555",
    "end": "98720"
  },
  {
    "text": "What is important here to note is that this is a set. So the ordering in which we are aggregating",
    "start": "98720",
    "end": "104718"
  },
  {
    "text": "these messages from the children is not important. What is arbitrary?",
    "start": "104719",
    "end": "109905"
  },
  {
    "text": "And for this reason, these aggregation functions that aggregate, that summarize,",
    "start": "109905",
    "end": "115219"
  },
  {
    "text": "that compress in some sense, the messages coming from the children have to be order invariant because they shouldn't depend,",
    "start": "115220",
    "end": "122920"
  },
  {
    "text": "in which ordering, am I considering the neighbors? Because there is no special ordering to the neighbors,",
    "start": "122920",
    "end": "128914"
  },
  {
    "text": "to the lower level, to the children in the network. That's an important detail. Of course,",
    "start": "128915",
    "end": "135905"
  },
  {
    "text": "another important detail is that we want to combine information coming from the neighbor- from the neighbors together with a node's own information from the previous layer as denoted here.",
    "start": "135905",
    "end": "146160"
  },
  {
    "text": "So I'm connecting information from level l minus 1 to create a message at level l. And I'm collecting information from the neighbors,",
    "start": "146160",
    "end": "154130"
  },
  {
    "text": "from the previous layer, as well as from the representation of that node itself at  the previous layer.",
    "start": "154130",
    "end": "161620"
  },
  {
    "text": "So let me now make things a bit more precise. So we will talk about message computation as the first operation that we need to decide.",
    "start": "161620",
    "end": "171635"
  },
  {
    "text": "And basically, the message computation takes the representation of the node at the previous layer and",
    "start": "171635",
    "end": "176960"
  },
  {
    "text": "somehow transforms it into this message information.",
    "start": "176960",
    "end": "182390"
  },
  {
    "text": "So each node creates a message which will be sent to other nodes in the next layer.",
    "start": "182390",
    "end": "188420"
  },
  {
    "text": "An example of a simple message transformation is that you take the previous layer embedding of a node and multiply",
    "start": "188420",
    "end": "195540"
  },
  {
    "text": "it with the matrix W. So this is a simple linear layer, linear transformation, and this is what we talked about in the previous lecture.",
    "start": "195540",
    "end": "205025"
  },
  {
    "text": "All right? So that's the first part. Then we need to decide this message function. In this case, it's simply this matrix multiply.",
    "start": "205025",
    "end": "212875"
  },
  {
    "text": "The second question is about aggregation. The intuition here is that each node will aggregate the messages from its neighbors.",
    "start": "212875",
    "end": "221105"
  },
  {
    "text": "So the idea is that I take now these transformed messages m that we just defined on the previous slide, right?",
    "start": "221105",
    "end": "227579"
  },
  {
    "text": "So I take these transformed messages coming from nodes u, from the previous level that I transformed,",
    "start": "227580",
    "end": "233729"
  },
  {
    "text": "let's say in this case with W, and I want to aggregate them into a single message.",
    "start": "233729",
    "end": "239545"
  },
  {
    "text": "All right, I want to take this thing and kind of compress it, aggregate it. What are some examples of aggregation functions?",
    "start": "239545",
    "end": "245870"
  },
  {
    "text": "A summation is a simple aggregation function, an average is an order invariant aggregation function as well as for example, Max,",
    "start": "245870",
    "end": "255260"
  },
  {
    "text": "we take the maximum message or to the maximum coordinate-wise value, and that's how we aggregate.",
    "start": "255260",
    "end": "261229"
  },
  {
    "text": "And again, all these- all these aggregation functions are order invariant.",
    "start": "261230",
    "end": "266545"
  },
  {
    "text": "So for example, one concrete way how you could do this is to say, uh-huh the level l embedding for node v is simply a summation of",
    "start": "266545",
    "end": "276230"
  },
  {
    "text": "the transformed messages coming from the neighbors u of that node of interest,",
    "start": "276230",
    "end": "284020"
  },
  {
    "text": "v. And this is where the messages from previous layer got transformed and now we simply sum them up to have",
    "start": "284020",
    "end": "290750"
  },
  {
    "text": "the embedding for the node at level l. And of course, now this node at level l,",
    "start": "290750",
    "end": "296150"
  },
  {
    "text": "to send to- to create a message for level l plus 1. It will take now W l plus 1,",
    "start": "296150",
    "end": "301925"
  },
  {
    "text": "multiply it with h, and send it to whoever is above them in the graph neural network structure.",
    "start": "301925",
    "end": "309185"
  },
  {
    "text": "So this was now a message operation, message transformation, and message aggregation.",
    "start": "309185",
    "end": "316884"
  },
  {
    "text": "One important issue is, if you do it the way I defined it so far is that information",
    "start": "316884",
    "end": "322430"
  },
  {
    "text": "from the node itself could get lost, right? Basically, that computation of message for node v for level l does not directly",
    "start": "322430",
    "end": "332530"
  },
  {
    "text": "depend on what we have already computed for that node- same node v from the previous level, right?",
    "start": "332530",
    "end": "339510"
  },
  {
    "text": "So for example, if I do it simply as I show here, we are simply aggregating information about neighbors,",
    "start": "339510",
    "end": "345515"
  },
  {
    "text": "but we don't really say, okay, but who is this node v? What do we know about node v before? So an opportunity here is to actually,",
    "start": "345515",
    "end": "353790"
  },
  {
    "text": "to include the previous level embedding of node v. Then we are computing the embedding of v for the next level.",
    "start": "353790",
    "end": "361185"
  },
  {
    "text": "So usually basically a different message computation will be performed, right?",
    "start": "361185",
    "end": "366230"
  },
  {
    "text": "What do I mean by this is, for example, the message transformation matrix W will be applied to the neighbors u.",
    "start": "366230",
    "end": "372955"
  },
  {
    "text": "While there will be a different message aggregation function B that will be applied to the embedding of node v itself.",
    "start": "372955",
    "end": "381560"
  },
  {
    "text": "So that's the first difference, right? So that the message from the node itself from previous layer will be multiplied by B,",
    "start": "381560",
    "end": "389690"
  },
  {
    "text": "while messages from neighbors from previous layer are going to be multiplied by W. And then the second difference is that after aggregating from neighbors,",
    "start": "389690",
    "end": "399595"
  },
  {
    "text": "we can aggregate messages from v itself as well. And usually, this is done via a concatenation or a summation.",
    "start": "399595",
    "end": "407185"
  },
  {
    "text": "So to show you an example, the way we can, we can do this is to say, ah-ha, I'm taking my messages from neighbors and I'm aggregating them.",
    "start": "407185",
    "end": "415960"
  },
  {
    "text": "Let's say with a- with a summation operator. I'm taking the message from v itself, right,",
    "start": "415960",
    "end": "422509"
  },
  {
    "text": "like I defined it up here. And then I'm going to concatenate these two messages",
    "start": "422750",
    "end": "427960"
  },
  {
    "text": "simply like concatenate them one next to each other. And that's my next layer embedding for node v. So simply I'm saying,",
    "start": "427960",
    "end": "435604"
  },
  {
    "text": "I'm aggregating information from neighbors, plus retaining the information about the node that I already had.",
    "start": "435605",
    "end": "442595"
  },
  {
    "text": "And this is now a way how to keep track of the information that the node has",
    "start": "442595",
    "end": "448520"
  },
  {
    "text": "already computed about itself so that it doesn't get lost through the layers of propagation,",
    "start": "448520",
    "end": "454369"
  },
  {
    "text": "let's say by concatenation here, or by summation. That's another popular choice.",
    "start": "454369",
    "end": "460090"
  },
  {
    "text": "So putting all this together, what did we learn? We learned that we have this message where",
    "start": "460090",
    "end": "466940"
  },
  {
    "text": "each node from the previous layer takes its own embedding, its own information, transforms it,",
    "start": "466940",
    "end": "473015"
  },
  {
    "text": "and sends it up to the parent. This is denoted here through this message transformation function.",
    "start": "473015",
    "end": "479540"
  },
  {
    "text": "Usually, this is simply a linear function like a matrix multiply. And then we have this message aggregation step",
    "start": "479540",
    "end": "486740"
  },
  {
    "text": "where we aggregate transformed messages from the neighbors, right? So we take these messages m that we have computed here and we aggregated them.",
    "start": "486740",
    "end": "495155"
  },
  {
    "text": "We aggregate them with an average, with a summation, or with a maximum pooling type approach.",
    "start": "495155",
    "end": "502155"
  },
  {
    "text": "And then what we can also do is, another extension here is to also add a self message and concatenate it.",
    "start": "502155",
    "end": "510875"
  },
  {
    "text": "And then after we have done all this, we pass this through a non-linearity, through a non-linear activation function.",
    "start": "510875",
    "end": "517594"
  },
  {
    "text": "And this last step is important because it adds expressiveness.",
    "start": "517595",
    "end": "522735"
  },
  {
    "text": "Often, you know, this non-linearity is written as sigma. In reality, this can be a rectified linear unit or a sigmoid,",
    "start": "522735",
    "end": "531995"
  },
  {
    "text": "or any other specific type of non-linear activation function, popularly other types of neural networks as well.",
    "start": "531995",
    "end": "540215"
  },
  {
    "text": "But that's essentially how a single layer of graph neural network looks like.",
    "start": "540215",
    "end": "546445"
  },
  {
    "text": "So now that we have seen this in abstract, I want to mention some of",
    "start": "546445",
    "end": "551630"
  },
  {
    "text": "the seminal graph neural network architectures that have been developed and kind of interpret them in",
    "start": "551630",
    "end": "558410"
  },
  {
    "text": "this unified message transformation, message aggregation framework.",
    "start": "558410",
    "end": "563425"
  },
  {
    "text": "So, last lecture, we talked about graph convolutional neural network or a GCN.",
    "start": "563425",
    "end": "569375"
  },
  {
    "text": "And I've wrote this equation, I said, ah-ha, the embedding of node v at layer l is simply an average of the embeddings of",
    "start": "569375",
    "end": "581839"
  },
  {
    "text": "nodes u that are neighbors of we normalized the by the- by the N-degree of",
    "start": "581840",
    "end": "588275"
  },
  {
    "text": "node v and transformed with matrix W and sent through a non-linearity.",
    "start": "588275",
    "end": "594890"
  },
  {
    "text": "So now the question is, how can I take this equation that I've written here and write it in this message transformation plus aggregation function.",
    "start": "594890",
    "end": "603515"
  },
  {
    "text": "And the way you can- you can do this is simply take this W and distribute it inside.",
    "start": "603515",
    "end": "608540"
  },
  {
    "text": "So basically now W times h divided by number of neighbors is the message transformation function.",
    "start": "608540",
    "end": "615950"
  },
  {
    "text": "And then the message aggregation function is simply a summation. And then we have a non-linearity here.",
    "start": "615950",
    "end": "621760"
  },
  {
    "text": "So this is what a graph convolutional neural network is, in terms of message aggregation and message transformation.",
    "start": "621760",
    "end": "630420"
  },
  {
    "text": "Um, so to write it even more explicitly, each neighbor transforms the message by saying,",
    "start": "630420",
    "end": "636955"
  },
  {
    "text": "I take my, uh, previous layer embedding, multiply it with w, and divide it by the,",
    "start": "636955",
    "end": "642310"
  },
  {
    "text": "uh node degree of v, so, uh, this is normal- normalization by node degree and then the aggregation is a summation",
    "start": "642310",
    "end": "649180"
  },
  {
    "text": "over the neighbors, uh, of node v and then applying, uh, a nonlinearity activation function here,",
    "start": "649180",
    "end": "656410"
  },
  {
    "text": "uh, denoted as sigma. So this is now a G- GCN written as",
    "start": "656410",
    "end": "661630"
  },
  {
    "text": "a message transformation and a message aggregation, uh, type operation.",
    "start": "661630",
    "end": "666940"
  },
  {
    "text": "So that's, um, number, uh, the first classical architecture.",
    "start": "666940",
    "end": "671980"
  },
  {
    "text": "Uh, the second architecture I want to mention is called GraphSAGE, and GraphSAGE builds- builds upon the GCN,",
    "start": "671980",
    "end": "679855"
  },
  {
    "text": "but extends it in, uh, several important, uh, aspects. Uh, the first aspect is that it realizes that",
    "start": "679855",
    "end": "686680"
  },
  {
    "text": "this aggregation function is an arbitrary, uh, aggregation function, so it allows for multiple different choices",
    "start": "686680",
    "end": "693370"
  },
  {
    "text": "of aggregation functions, not only averaging. And the second thing is, it- it talks about, uh,",
    "start": "693370",
    "end": "700315"
  },
  {
    "text": "taking the message from the node itself, uh, transforming it, and then concatenating it with the aggregating- aggregated messages,",
    "start": "700315",
    "end": "707935"
  },
  {
    "text": "which adds, uh, a lot of expressive, uh, power. So now let's write the GraphSAGE equation.",
    "start": "707935",
    "end": "714495"
  },
  {
    "text": "In this message plus, uh, aggregation type operations, right? Messages computed through the,",
    "start": "714495",
    "end": "721139"
  },
  {
    "text": "uh, uh, aggregation operator AGG here. Um, and the way we can think of this is that this is kind of a two-stage approach.",
    "start": "721140",
    "end": "727464"
  },
  {
    "text": "First is that, um, uh, we, um, we take the individual messages,",
    "start": "727465",
    "end": "733058"
  },
  {
    "text": "um, and transform them, let's say through, uh, linear operations, then we apply the aggregation operator that basically",
    "start": "733059",
    "end": "739930"
  },
  {
    "text": "gives me now a summary of the messages coming from the neighbors. And then, uh, the second important step now is that I take the messages coming",
    "start": "739930",
    "end": "748000"
  },
  {
    "text": "from the neighbors already aggregated, I concatenate it with v's own,",
    "start": "748000",
    "end": "753310"
  },
  {
    "text": "um, um, message or embedding, uh, from the previous layer, concatenate these two together,",
    "start": "753310",
    "end": "759145"
  },
  {
    "text": "multiply them with a- with a transformation matrix and pass through a non-linearity, right?",
    "start": "759145",
    "end": "764820"
  },
  {
    "text": "So the differences between GCN is here and another more important difference is here that we are concatenating and taking our own,",
    "start": "764820",
    "end": "772110"
  },
  {
    "text": "uh, embedding, uh, as well. So, um, to, now to say what kind of aggregation functions can be used?",
    "start": "772110",
    "end": "779015"
  },
  {
    "text": "We can simply take, for example, weighted average of neighbors, which is what our GCN is doing.",
    "start": "779015",
    "end": "786520"
  },
  {
    "text": "We can, for example, take any kind of pooling, which is, uh, you know, take the, uh, uh,",
    "start": "786520",
    "end": "791980"
  },
  {
    "text": "take the- transform the neighbor vectors and apply a symmetric vector function like a min or a max.",
    "start": "791980",
    "end": "798610"
  },
  {
    "text": "So you could even have, for example, here as a transformation, you don't have to have a linear transformation. You could have a multilayer perceptron as",
    "start": "798610",
    "end": "806140"
  },
  {
    "text": "a message transformation function and then an aggregation. Um, you can also not take the average of the messages,",
    "start": "806140",
    "end": "813190"
  },
  {
    "text": "but your sum up the messages. And, uh, these different, um, uh,",
    "start": "813190",
    "end": "818920"
  },
  {
    "text": "aggregation functions have different theoretical properties. And we are going to actually talk about the theoretical properties and consequences, uh,",
    "start": "818920",
    "end": "828565"
  },
  {
    "text": "of the choice of the aggregation function on the expressive power, uh, of the model,",
    "start": "828565",
    "end": "834040"
  },
  {
    "text": "um, in one of the future lectures. And what you could even do, um, if you like, is you could apply an LSTM.",
    "start": "834040",
    "end": "840490"
  },
  {
    "text": "So basically you could apply a sequence model, uh, to the- to the messages coming from the neighbors.",
    "start": "840490",
    "end": "846040"
  },
  {
    "text": "Um, and here the important thing is that a sequence model is not order invariant. So when you train it,",
    "start": "846040",
    "end": "852160"
  },
  {
    "text": "you wanna permute the orderings so that the- you teach the- the sequence model, not to keep, uh,",
    "start": "852160",
    "end": "858415"
  },
  {
    "text": "kind of to ignore, uh, the ordering of the messages that it receives. But you could use something like this, um, as a,",
    "start": "858415",
    "end": "865690"
  },
  {
    "text": "uh, as an aggregation, uh, uh, operator. So a lot of freedom, uh, to choose here.",
    "start": "865690",
    "end": "872335"
  },
  {
    "text": "And then the last thing to mention about GraphSAGE is that adds this notion of l2 normalization.",
    "start": "872335",
    "end": "879715"
  },
  {
    "text": "So the idea is that we want to apply l2 normalization to the embeddings at every layer.",
    "start": "879715",
    "end": "885595"
  },
  {
    "text": "And when I say l2 normalization, all I mean by that is we wanna measure the distance,",
    "start": "885595",
    "end": "891534"
  },
  {
    "text": "some of the squared values, uh, of the entries of the embedding of a given- of a given node,",
    "start": "891534",
    "end": "897505"
  },
  {
    "text": "take the square root of that, and then divide by the distance. So basically this means that the Euclidean length of",
    "start": "897505",
    "end": "902769"
  },
  {
    "text": "this embedding vector will always be equal to 1. And sometimes this is quite important and,",
    "start": "902770",
    "end": "909040"
  },
  {
    "text": "uh, leads to big, uh, performance improvement because without l2 normalization, the embedding, uh,",
    "start": "909040",
    "end": "915520"
  },
  {
    "text": "vectors of nodes can have different scales, different lengths, um, and in some cases,",
    "start": "915520",
    "end": "921095"
  },
  {
    "text": "normalization of the embedding results in performance improvement. So after l2 normalization step,",
    "start": "921095",
    "end": "927790"
  },
  {
    "text": "as I introduced it here, all vectors will have the same, uh, l2 norm. They'll have the same length,",
    "start": "927790",
    "end": "933805"
  },
  {
    "text": "which is the length of, uh, 1. Uh, this is how, uh, this is defined, um,",
    "start": "933805",
    "end": "939295"
  },
  {
    "text": "if- if you wanna, uh, really see it. So l2 normalization is also an important component, um,",
    "start": "939295",
    "end": "946165"
  },
  {
    "text": "when deciding on the design decisions on the specific architecture of the,",
    "start": "946165",
    "end": "952600"
  },
  {
    "text": "uh, graph, uh, neural network. And then the last, uh,",
    "start": "952600",
    "end": "958015"
  },
  {
    "text": "classical architecture that I wanna talk about is called graph attention network.",
    "start": "958015",
    "end": "963385"
  },
  {
    "text": "And here we are going to learn, uh, this concept of an attention. So let me first tell you what a graph attention network is,",
    "start": "963385",
    "end": "970825"
  },
  {
    "text": "and then I will define the notion of attention and, uh, how do we learn it and what- what it intuitively means.",
    "start": "970825",
    "end": "978100"
  },
  {
    "text": "So, uh, the motivation is the following. Writing graph attention network when we are aggregating messages from the neighbors,",
    "start": "978100",
    "end": "986455"
  },
  {
    "text": "we have a weight, um, associated with every neighbor, right?",
    "start": "986455",
    "end": "992440"
  },
  {
    "text": "So for every neighbor u, of node v, we have a weight, alpha. And this weight we call attention weight.",
    "start": "992440",
    "end": "999430"
  },
  {
    "text": "[NOISE] And the idea is that this weight can now tell me how important of a neighbor, um,",
    "start": "999430",
    "end": "1006089"
  },
  {
    "text": "is a given- is a given node, or in some sense, how much attention to pay to a given, to a message from a given node u?",
    "start": "1006090",
    "end": "1014100"
  },
  {
    "text": "Because if these weights are different, then messages from different nodes will have different weight,",
    "start": "1014100",
    "end": "1019560"
  },
  {
    "text": "uh, in this summation. That's the idea. So now let's make a step back. Explain why- why- how- how this is motivated,",
    "start": "1019560",
    "end": "1028064"
  },
  {
    "text": "why it's a good idea, and how to, uh, learn these weights. So if you think about the two architectures we talked about so far,",
    "start": "1028065",
    "end": "1035655"
  },
  {
    "text": "so the GCN and GraphSAGE. There is already an implicit notion of this alpha.",
    "start": "1035655",
    "end": "1042539"
  },
  {
    "text": "So alpha_uv is simply 1 over the degree of node v. So basically this is",
    "start": "1042540",
    "end": "1048630"
  },
  {
    "text": "a weight factor or an importance of a message coming from u for the node v. And,",
    "start": "1048630",
    "end": "1055020"
  },
  {
    "text": "uh, so far, you know, this alpha was defined implicitly. Um, and we can actually define it,",
    "start": "1055020",
    "end": "1062460"
  },
  {
    "text": "um, more explicitly or we can actually learn it. Uh, in our case, alpha was actually for all the,",
    "start": "1062460",
    "end": "1069059"
  },
  {
    "text": "uh, incoming no- uh, nodes u, uh, alpha was the same. It only depended on the degree of node v but didn't really depend on the u itself.",
    "start": "1069060",
    "end": "1078179"
  },
  {
    "text": "So it does a very limiting kind of notion of attention, right? So in- in GraphSAGE or GCN,",
    "start": "1078180",
    "end": "1084000"
  },
  {
    "text": "all neighbors are equally important to node v, when aggregating messages and the question is,",
    "start": "1084000",
    "end": "1090389"
  },
  {
    "text": "can we kind of, er, uh, generalize this? Can we generalize this so that we learn how important is a message from a given node,",
    "start": "1090390",
    "end": "1098115"
  },
  {
    "text": "uh, to the node that is aggregating messages, right? So we wanna learn message importances.",
    "start": "1098115",
    "end": "1103380"
  },
  {
    "text": "And this notion of an importance is called attention, and attention- the word attention is kind of inspired",
    "start": "1103380",
    "end": "1110880"
  },
  {
    "text": "by the- by- in a sense with cognitive attention, right? So attention alpha focuses on the important parts of",
    "start": "1110880",
    "end": "1118799"
  },
  {
    "text": "the input data and kind of fades out or ignores, uh, the rest. So the idea is that the neural network should devote more computing power,",
    "start": "1118800",
    "end": "1128685"
  },
  {
    "text": "more attention to that small important part of the input, um, and perhaps, you know,",
    "start": "1128685",
    "end": "1135240"
  },
  {
    "text": "ignore- choose to ignore the rest. Um, and which part of the data is more important depends on the context.",
    "start": "1135240",
    "end": "1142665"
  },
  {
    "text": "And the idea is that we are going to learn, uh, what part of the data is important through the model training process.",
    "start": "1142665",
    "end": "1149835"
  },
  {
    "text": "So we allow the model to learn importance of different, uh, of different pieces of input that it is, uh, receiving.",
    "start": "1149835",
    "end": "1157800"
  },
  {
    "text": "So in our case, we would want to learn this attention weight that will tell us how important is a message coming from node u, uh,",
    "start": "1157800",
    "end": "1165375"
  },
  {
    "text": "to this, uh, node, uh, v. And we want this attention, of course, depend on the u as well as on,",
    "start": "1165375",
    "end": "1172740"
  },
  {
    "text": "uh, v as well. So the question is, right, how do we learn, uh,",
    "start": "1172740",
    "end": "1179025"
  },
  {
    "text": "these, uh, attention weights, these weighting factors, uh, alpha. So the goal is to specify an arbitrary importance, uh,",
    "start": "1179025",
    "end": "1188135"
  },
  {
    "text": "between, um, between neighbors, um, when we're doing message aggregation. And the idea is that we can compute the embedding, uh,",
    "start": "1188135",
    "end": "1195635"
  },
  {
    "text": "of each node in the graph following these attention strategies where nodes attend over the messages coming from the neighborhood,",
    "start": "1195635",
    "end": "1204330"
  },
  {
    "text": "by attend, I mean give different importances to them. And then, uh, we are going to, um,",
    "start": "1204330",
    "end": "1209550"
  },
  {
    "text": "implicitly specify different weights to different nodes, uh, in the neighborhood.",
    "start": "1209550",
    "end": "1214620"
  },
  {
    "text": "And we are going to learn these weights, these importances. The way we are going to do this is,",
    "start": "1214620",
    "end": "1221505"
  },
  {
    "text": "uh, to compute this as a, uh, byproduct of the attention mechanism, where we are going to define this notion of attention mechanism",
    "start": "1221505",
    "end": "1229095"
  },
  {
    "text": "that is going to give us these attention scores or attention weights. So let- let us think about this attention, uh, mechanism, a,",
    "start": "1229095",
    "end": "1237045"
  },
  {
    "text": "by first computing attention coefficients, e_vu across pairs of nodes,",
    "start": "1237045",
    "end": "1243300"
  },
  {
    "text": "uh, u and v based on their messages. So the idea would be that I wanna define some function a,",
    "start": "1243300",
    "end": "1248895"
  },
  {
    "text": "that will take the embedding of node u at previous layer, embedding of node v at previous layer,",
    "start": "1248895",
    "end": "1255420"
  },
  {
    "text": "perhaps transform these two embeddings, and then take these as input and prod- give me a weight, right?",
    "start": "1255420",
    "end": "1261795"
  },
  {
    "text": "And this weight will tell me the importance of, uh, u's message on the, uh,",
    "start": "1261795",
    "end": "1267825"
  },
  {
    "text": "on the node, uh, v. So for example, just to be concrete, right? If I wanna say, what is the attention coefficients e_AB?",
    "start": "1267825",
    "end": "1276600"
  },
  {
    "text": "It's simply some function a, of the embedding of node A at the previous step, uh,",
    "start": "1276600",
    "end": "1282465"
  },
  {
    "text": "and the embedding of node B at the previous step, at the previous layer of the graph neural network,",
    "start": "1282465",
    "end": "1288345"
  },
  {
    "text": "and this will give me now the weight, uh, of this, uh, or importance of this particular, uh, edge.",
    "start": "1288345",
    "end": "1294690"
  },
  {
    "text": "So now that I have these, uh, coefficients, I wanna normalize them to- to get the final attention weight.",
    "start": "1294690",
    "end": "1301950"
  },
  {
    "text": "So what do I mean by, for example, uh, normalize is that we can apply a softmax function,",
    "start": "1301950",
    "end": "1308415"
  },
  {
    "text": "uh, to them so that these attention weights are going to sum to 1. So I take the coefficients e that we have just defined,",
    "start": "1308415",
    "end": "1315450"
  },
  {
    "text": "I, uh, exponentiate them, and then, you know, divide by the s- exponentiated sum of them so that, uh,",
    "start": "1315450",
    "end": "1322570"
  },
  {
    "text": "these, uh, attention weights, uh, alpha now are going to sum to 1. And then, right when I'm doing message aggregation,",
    "start": "1322570",
    "end": "1330150"
  },
  {
    "text": "I can now do a weighted sum based on the attention weights, uh, alpha.",
    "start": "1330150",
    "end": "1335265"
  },
  {
    "text": "So here are the alphas. These are these alphas that depend on e, and e is the,",
    "start": "1335265",
    "end": "1340980"
  },
  {
    "text": "uh, is the, um, is- depends on the previous layer embeddings of nodes, uh,",
    "start": "1340980",
    "end": "1346304"
  },
  {
    "text": "u and v. So, for example, if I now say, how would aggregation for node A look like?",
    "start": "1346305",
    "end": "1352149"
  },
  {
    "text": "The way I would do this is I would compute these attention weights, uh- uh, Alpha_AB, Alpha_AC, and Alpha_AD because B,",
    "start": "1352150",
    "end": "1359540"
  },
  {
    "text": "C, and D are its neighbors. Uh, these alphas will be computed as I- as I show up here, and they will be computed by previous layer embeddings,",
    "start": "1359540",
    "end": "1368040"
  },
  {
    "text": "uh, of these, uh, nodes on the endpoints of the edge. And then my aggregation function is simply",
    "start": "1368040",
    "end": "1373980"
  },
  {
    "text": "a weighted average of the messages coming from the neighbors, where message is, uh- uh- uh,",
    "start": "1373980",
    "end": "1380700"
  },
  {
    "text": "multiplied by the weight Alpha that we have, uh, computed and defined up here.",
    "start": "1380700",
    "end": "1388350"
  },
  {
    "text": "So that's, um, [NOISE] basically the idea of the attention mechanism.",
    "start": "1388350",
    "end": "1393570"
  },
  {
    "text": "Um, now, what is the form of this attention mechanism a? We still haven't decided how embedding of one node",
    "start": "1393570",
    "end": "1400800"
  },
  {
    "text": "and embedding of the other node get- get combined, computed into this, uh- uh,",
    "start": "1400800",
    "end": "1405930"
  },
  {
    "text": "weight, uh, e. Uh, the way it is usually done is, uh- um, you- you have many different choices.",
    "start": "1405930",
    "end": "1412515"
  },
  {
    "text": "Like you could use a simple, uh, linear layer, uh, one layer neural network to do this, um, or, uh,",
    "start": "1412515",
    "end": "1419370"
  },
  {
    "text": "have alpha, uh, this, um, function a have trainable parameters. Uh, so for example a p- uh,",
    "start": "1419370",
    "end": "1425940"
  },
  {
    "text": "a popular choice is to simply to say: let me take the embeddings of nodes A and B at the previous layer,",
    "start": "1425940",
    "end": "1432720"
  },
  {
    "text": "perhaps let me transform them, let me concatenate them, and then apply a linear layer to them,",
    "start": "1432720",
    "end": "1438660"
  },
  {
    "text": "and that will give me this weight, uh, e_AB, to which then I can apply softmax,",
    "start": "1438660",
    "end": "1443924"
  },
  {
    "text": "um, and then based on that, ah, softmax transformed weight, I use that weight as,",
    "start": "1443925",
    "end": "1448950"
  },
  {
    "text": "uh- uh, in the aggregation function. And the important point is that these parameters of fu- of, uh, function a,",
    "start": "1448950",
    "end": "1455940"
  },
  {
    "text": "this attention mechanism a, uh, the- basically parameters of these functions are trained jointly.",
    "start": "1455940",
    "end": "1461970"
  },
  {
    "text": "So we learn the parameters of the attention mechanism together with the weight matrices,",
    "start": "1461970",
    "end": "1468809"
  },
  {
    "text": "so message transformation matrices, um, in the message aggregation step. So we do all this training in an end-to-end, uh, fashion.",
    "start": "1468810",
    "end": "1477029"
  },
  {
    "text": "What this means in practice is that working with this type of attention mechanisms,",
    "start": "1477030",
    "end": "1483110"
  },
  {
    "text": "uh, can be tricky because, uh, this can be quite finicky. Uh, in a sense, may- it's- sometimes it's hard to learn,",
    "start": "1483110",
    "end": "1489710"
  },
  {
    "text": "hard to make it converge, so what we can also do is, uh, to, uh,",
    "start": "1489710",
    "end": "1495470"
  },
  {
    "text": "expand this notion of attention to what is called a multi-head attention.",
    "start": "1495470",
    "end": "1500570"
  },
  {
    "text": "And multi-head attention is a way to stabilize learning process of the attention mechanism,",
    "start": "1500570",
    "end": "1506700"
  },
  {
    "text": "and the idea is quite simple. The idea is that we'll have multiple attention scores.",
    "start": "1506700",
    "end": "1512114"
  },
  {
    "text": "So we are going to have multiple attention mechanisms a, um, and each one- and we are going to train- learn all of them, uh, simultaneously.",
    "start": "1512114",
    "end": "1521519"
  },
  {
    "text": "So the idea is that we would have different functions a, for example, in this case, we would have three different functions a,",
    "start": "1521520",
    "end": "1528780"
  },
  {
    "text": "which means I would- we would get three different, uh, attention coefficients, attention weights for a given edge vu.",
    "start": "1528780",
    "end": "1537015"
  },
  {
    "text": "And then we will do the aggregation, uh, three times, uh,",
    "start": "1537015",
    "end": "1542325"
  },
  {
    "text": "get the aggregated messages from the neighbors, and now we can further aggregate, uh,",
    "start": "1542325",
    "end": "1547995"
  },
  {
    "text": "these messages into a single, uh, aggregated message. And the point here is now that we- when we learn these functions a^1, a^2, a^3,",
    "start": "1547995",
    "end": "1557265"
  },
  {
    "text": "we are going to randomly initialize parameters of each one of them, and through the learning process,",
    "start": "1557265",
    "end": "1562830"
  },
  {
    "text": "each one of them is kind of going to converge to some local minima. But because we are using multiple of them,",
    "start": "1562830",
    "end": "1570195"
  },
  {
    "text": "and we are averaging their transformations together, this will basically allow our model to- to be- to be more robust,",
    "start": "1570195",
    "end": "1578684"
  },
  {
    "text": "it will allow our learning process not to get stuck in some weird part of the optimization space,",
    "start": "1578685",
    "end": "1585525"
  },
  {
    "text": "um, and, kind of, it will work, uh, better, uh, on the average. So the idea of this multi-head attention is- is simple.",
    "start": "1585525",
    "end": "1593775"
  },
  {
    "text": "To summarize, is that we are going to have multiple attention weights on the- on the same edge,",
    "start": "1593775",
    "end": "1601215"
  },
  {
    "text": "and we are going to use them, uh, separately in message aggregation, and then the final message that we get",
    "start": "1601215",
    "end": "1608804"
  },
  {
    "text": "for a- for a node will be simply the aggregation, like the average, of these individual attention-based, uh, aggregations.",
    "start": "1608805",
    "end": "1616950"
  },
  {
    "text": "One important detail here is that each of these different, uh, Alphas has to be predicted with a different function a,",
    "start": "1616950",
    "end": "1625215"
  },
  {
    "text": "and each of these functions a, has to be initialized with a random, uh, different set of starting parameters so that each one gets a chance to,",
    "start": "1625215",
    "end": "1634380"
  },
  {
    "text": "kind of, converge to some, uh, local, uh, minima. Uh, that's the idea and that adds to the robustness and stabilizes,",
    "start": "1634380",
    "end": "1642554"
  },
  {
    "text": "uh, the learning process. So this is what I wanted to say about the attention mechanism,",
    "start": "1642555",
    "end": "1648825"
  },
  {
    "text": "and how do we define it? So next, let me defi- let me discuss a bit the benefits of the attention mechanism.",
    "start": "1648825",
    "end": "1657330"
  },
  {
    "text": "And the key benefit is that this allows implicitly for specifying different importance values to different neighbors.",
    "start": "1657330",
    "end": "1666045"
  },
  {
    "text": "Um, it is computationally efficient in a sense that computation of attention,",
    "start": "1666045",
    "end": "1671190"
  },
  {
    "text": "uh, coefficient can be parallelized across all the incoming messages, right? For every incoming message,",
    "start": "1671190",
    "end": "1676995"
  },
  {
    "text": "I compute the attention weight, uh, by applying function a that only depends on",
    "start": "1676995",
    "end": "1682320"
  },
  {
    "text": "the embedding of one node and the embedding of the other node, uh, in the previous layer, um, so this is good.",
    "start": "1682320",
    "end": "1688665"
  },
  {
    "text": "It is, uh, in some sense storage-efficient because, um, sparse matrix operators do not require,",
    "start": "1688665",
    "end": "1695940"
  },
  {
    "text": "um- um, too many non-zero elements. Basically, I need one entry per node and one entry per edge,",
    "start": "1695940",
    "end": "1702870"
  },
  {
    "text": "so, uh, you know, that's cheap, that's linear in the amount of data we have, um, and it has a fixed number of parameters,",
    "start": "1702870",
    "end": "1709649"
  },
  {
    "text": "meaning the, uh, mesh, the- the attention mechanism function a has a fixed number of parameters that is,",
    "start": "1709649",
    "end": "1716909"
  },
  {
    "text": "uh, independent of the graph size. Another important aspect is that,",
    "start": "1716910",
    "end": "1723210"
  },
  {
    "text": "uh, attention weights are localized. They attend to local network neighborhoods, so basically tell you what part of the neighborhood to focus on,",
    "start": "1723210",
    "end": "1731835"
  },
  {
    "text": "um, and they generalize, meaning that they- they give me this, uh, inductive capability, which means that, um,",
    "start": "1731835",
    "end": "1738780"
  },
  {
    "text": "this is a shared edge- edgewise mechanism and does not depend on the graph structure- so- on the global graph structure.",
    "start": "1738780",
    "end": "1746580"
  },
  {
    "text": "So it means can I can transfer it across the graphs, so this function A is transferable between graphs or from one part of the graph,",
    "start": "1746580",
    "end": "1754470"
  },
  {
    "text": "uh, to the next part of the graph. So, um, these are the benefits and kind of the discussion,",
    "start": "1754470",
    "end": "1759570"
  },
  {
    "text": "uh, of the attention mechanism. To give you an example,",
    "start": "1759570",
    "end": "1764670"
  },
  {
    "text": "um, here is, um, uh, an example of a, um, uh, of a network, uh, called Quora.",
    "start": "1764670",
    "end": "1772455"
  },
  {
    "text": "This is a citation network of different papers coming from different disciplines. And different disciplines,",
    "start": "1772455",
    "end": "1778890"
  },
  {
    "text": "different publication classes here are colored in different, uh, colors. And, uh, what we tried to show here is with",
    "start": "1778890",
    "end": "1786495"
  },
  {
    "text": "different edge thickness is the attention score um, between a pair of nodes uh, i and j.",
    "start": "1786495",
    "end": "1793620"
  },
  {
    "text": "So it's simply a normalized attention score by basically saying, what is the attention of i to j, and what's attention of uh,",
    "start": "1793620",
    "end": "1800100"
  },
  {
    "text": "j to i across different uh, layers uh, k. Notice that attentions,",
    "start": "1800100",
    "end": "1806250"
  },
  {
    "text": "um, can be asymmetric, right? One mes- the, uh, message from you to me might be very important,",
    "start": "1806250",
    "end": "1812745"
  },
  {
    "text": "while message from me to you [LAUGHTER], for example, might be less important, so do- it doesn't have to be symmetric.",
    "start": "1812745",
    "end": "1819645"
  },
  {
    "text": "And, um, if you look at, you know, in terms of improvements, for example, the graph attention networks can",
    "start": "1819645",
    "end": "1826559"
  },
  {
    "text": "give you quite a bit of improvement over, let's say, the graph convolutional neural network, uh, because, uh,",
    "start": "1826560",
    "end": "1833145"
  },
  {
    "text": "because of this attention mechanism and allowing you to learn what to focus- what to focus on and which sub-parts of the network, uh, to learn from.",
    "start": "1833145",
    "end": "1842804"
  },
  {
    "text": "So this is an example of graph attention network, how it learns attentions, and of course there has been many, uh, upgrades, iterations,",
    "start": "1842805",
    "end": "1851565"
  },
  {
    "text": "of this idea of attention on graph neural networks, but the graph attention network,",
    "start": "1851565",
    "end": "1856800"
  },
  {
    "text": "you know, two years ago was, uh, was the one that first developed and proposed depth.",
    "start": "1856800",
    "end": "1863340"
  },
  {
    "text": "So, um, this is, uh, quite exciting. So, uh, to summarize what have we learned so far.",
    "start": "1863340",
    "end": "1870060"
  },
  {
    "text": "We have learned that, uh, about classical graph neural network layers and how",
    "start": "1870060",
    "end": "1876180"
  },
  {
    "text": "they are defined and what kind of components do they include. Um, and they can often, you know,",
    "start": "1876180",
    "end": "1882270"
  },
  {
    "text": "we can often get better performance by- by, uh, combining different aspects, uh,",
    "start": "1882270",
    "end": "1887805"
  },
  {
    "text": "in terms of design, um, and we can also include, as I will talk about later,",
    "start": "1887805",
    "end": "1892875"
  },
  {
    "text": "other modern deep learning modules into graph neural network design, right?",
    "start": "1892875",
    "end": "1897990"
  },
  {
    "text": "Like for example, we'll- I'm going to talk more about batch normalization, dropout, you can choose different activation function,",
    "start": "1897990",
    "end": "1904530"
  },
  {
    "text": "attention, as well as aggregation. So these are kind of transformations and components you can choose to pick,",
    "start": "1904530",
    "end": "1912960"
  },
  {
    "text": "um, in order to design an effective architecture for your- for your problem.",
    "start": "1912960",
    "end": "1919605"
  },
  {
    "text": "So as I mentioned, many kind of modern deep learning modules or techniques can",
    "start": "1919605",
    "end": "1925500"
  },
  {
    "text": "be incorporated or generalized to graph neural networks. Um, there is- like for example,",
    "start": "1925500",
    "end": "1931215"
  },
  {
    "text": "I'm going to talk about- next about batch normalization, which stabilizes neural network training.",
    "start": "1931215",
    "end": "1937590"
  },
  {
    "text": "I'm going to talk about dropout that allows us to prevent over-fitting. Um, we talked about attention to- attention mechanism that",
    "start": "1937590",
    "end": "1946800"
  },
  {
    "text": "controls the importance of messages coming from different parts of the neighborhood, um, and we can also talk about skip connections, uh, and so on.",
    "start": "1946800",
    "end": "1955335"
  },
  {
    "text": "So, um, let me talk about some of these concepts, uh, in a bit more, uh, detail.",
    "start": "1955335",
    "end": "1961755"
  },
  {
    "text": "So first, I wanna talk about the notion of batch normalization and",
    "start": "1961755",
    "end": "1967170"
  },
  {
    "text": "the goal of batch normalization is to stabilize training of graph neural networks.",
    "start": "1967170",
    "end": "1972255"
  },
  {
    "text": "And the idea is that given a batch of inputs, given a batch of data points, in our case, a batch of node embeddings,",
    "start": "1972255",
    "end": "1978600"
  },
  {
    "text": "we wanna re-center node embeddings to zero mean and scale them to have unit variance.",
    "start": "1978600",
    "end": "1985020"
  },
  {
    "text": "So to- to be very precise what we mean by this, I'm given a set of inputs, in our case this would be vectors of, uh, node embeddings.",
    "start": "1985020",
    "end": "1992945"
  },
  {
    "text": "I then can compute what for- for every coordinate, what is the mean value of that coordinate and what is",
    "start": "1992945",
    "end": "1999860"
  },
  {
    "text": "the variance along that coordinate, uh, of the vector, uh, acro- across this n, uh,",
    "start": "1999860",
    "end": "2006250"
  },
  {
    "text": "input points, X, that are part of a- of a mini-batch. And then, um, I can also have, um, in this case,",
    "start": "2006250",
    "end": "2015184"
  },
  {
    "text": "I can have, um, uh, um, uh, then, uh, you know, there is the input,",
    "start": "2015185",
    "end": "2020825"
  },
  {
    "text": "there is the output that are two trainable parameters, gamma, uh, and beta, and then I can,",
    "start": "2020825",
    "end": "2027919"
  },
  {
    "text": "uh, come up with the output that is simply, um, I take these inputs x,",
    "start": "2027920",
    "end": "2033110"
  },
  {
    "text": "I standardize them in a sense that I subtract the mean and divide by the variance along that dimension so now these X's have, uh,",
    "start": "2033110",
    "end": "2042260"
  },
  {
    "text": "0 mean and unit variance and then I can further learn how to transform them by basically linearly transforming them by multiplying with gamma and,",
    "start": "2042260",
    "end": "2051800"
  },
  {
    "text": "uh, adding a bias factor, uh- uh, bias term, beta.",
    "start": "2051800",
    "end": "2057619"
  },
  {
    "text": "And I do this independently for every coordinate, for every dimension of, uh, every data point of every embedding,",
    "start": "2057620",
    "end": "2065750"
  },
  {
    "text": "i, that is in the mini batch. So to summarize, batch normalization stabilizes training,",
    "start": "2065750",
    "end": "2072800"
  },
  {
    "text": "it first standardizes the data. So stand- to standardize means subtract the mean,",
    "start": "2072800",
    "end": "2079129"
  },
  {
    "text": "divide by the, uh, by the standard deviation. So this means now X has 0 mean and variance of 1,",
    "start": "2079130",
    "end": "2086929"
  },
  {
    "text": "so unit variance, and then I can also learn, uh, these parameters, beta and gamma, that now linearly transform X",
    "start": "2086930",
    "end": "2095989"
  },
  {
    "text": "along each dimension and this is now the output of the, uh, batch normalization.",
    "start": "2095989",
    "end": "2102330"
  },
  {
    "text": "Um, the second technique I wanna discuss is called, uh, dropout.",
    "start": "2103120",
    "end": "2108635"
  },
  {
    "text": "And the idea- and what this allows us to do in neural networks is prevent over-fitting.",
    "start": "2108635",
    "end": "2114725"
  },
  {
    "text": "Um, and the idea is that during training, with some small probability P,",
    "start": "2114725",
    "end": "2119795"
  },
  {
    "text": "a random set of neurons will be set to 0. Um, and, uh, during testing,",
    "start": "2119795",
    "end": "2125960"
  },
  {
    "text": "we are going to use all the neurons, uh, of the network. So the idea is if you have a, let's say, feed-forward neural network,",
    "start": "2125960",
    "end": "2132950"
  },
  {
    "text": "the idea is that some of the neurons you set to 0 so that information now flows only between the neutrons that are not,",
    "start": "2132950",
    "end": "2140690"
  },
  {
    "text": "uh, set to 0. And the idea here is that this forces the neural network to be",
    "start": "2140690",
    "end": "2146630"
  },
  {
    "text": "more robust to corrupted data or to corrupted inputs. That's, uh, the idea and because the neural network is now more robust,",
    "start": "2146630",
    "end": "2154340"
  },
  {
    "text": "you- it prevents neural network, uh, from over-fitting. In a graph neural network,",
    "start": "2154340",
    "end": "2161360"
  },
  {
    "text": "dropout is applied to the linear layer in the message function. So the idea is that when we take the message from, uh,",
    "start": "2161360",
    "end": "2170299"
  },
  {
    "text": "node u from the previous layer and we were multiplying it with this matrix W here,",
    "start": "2170300",
    "end": "2175940"
  },
  {
    "text": "uh, to this linear layer, to this W, we can now apply dropout, right?",
    "start": "2175940",
    "end": "2181850"
  },
  {
    "text": "Rather than saying here are the inputs multiplied with W to get the outputs m,",
    "start": "2181850",
    "end": "2187250"
  },
  {
    "text": "you can now basically set some of the parts, uh, of the input, uh, um, as well as the output, uh,",
    "start": "2187250",
    "end": "2194390"
  },
  {
    "text": "to 0, and this way, mimic the dropout. That's the- that's the idea,",
    "start": "2194390",
    "end": "2201260"
  },
  {
    "text": "and, uh, as I said, in terms of dropout, what it does is it helps with, um,",
    "start": "2201260",
    "end": "2207755"
  },
  {
    "text": "preventing over-fitting of, uh, neural networks. The next component of our graph",
    "start": "2207755",
    "end": "2216890"
  },
  {
    "text": "neural network layer is in terms of non-linear activation function. Right, and the idea is that we apply this, uh,",
    "start": "2216890",
    "end": "2224180"
  },
  {
    "text": "activation to each dimension of the, uh, of the embedding X.",
    "start": "2224180",
    "end": "2230210"
  },
  {
    "text": "What we can do is apply a rectified linear unit, which is defined simply as the maximum of x and 0,",
    "start": "2230210",
    "end": "2237800"
  },
  {
    "text": "so the way you can think of it based on the inputs- input x, the red line gives you the output.",
    "start": "2237800",
    "end": "2242960"
  },
  {
    "text": "So if the x is negative, the output is 0, and if x is positive, the output is,",
    "start": "2242960",
    "end": "2248080"
  },
  {
    "text": "uh, x itself, uh, and this is most commonly used, um, activation function.",
    "start": "2248080",
    "end": "2253780"
  },
  {
    "text": "And we can also apply a sigmoid activation function. A sigmoid is defined here,",
    "start": "2253780",
    "end": "2260290"
  },
  {
    "text": "here is its shape. So as a function of the input, the output will be- will be on value 0 to",
    "start": "2260290",
    "end": "2266710"
  },
  {
    "text": "1 and this basically means that you can take a x that- that has a domain from minus infinity to plus infinity and kind",
    "start": "2266710",
    "end": "2274279"
  },
  {
    "text": "of transform it into something- to- to the bounded output from 0 to 1.",
    "start": "2274280",
    "end": "2280235"
  },
  {
    "text": "And this is used when you wanna restrict the range of your embeddings when you wanna restrict the range of your output.",
    "start": "2280235",
    "end": "2287060"
  },
  {
    "text": "And then what empirically works best is called a parametric ReLU, uh,",
    "start": "2287060",
    "end": "2293270"
  },
  {
    "text": "and parametric ReLU is defined as the maximum of x and 0 plus some trainable parameter alpha,",
    "start": "2293270",
    "end": "2300470"
  },
  {
    "text": "minimum of x and 0. So this basically means that empirically,",
    "start": "2300470",
    "end": "2306635"
  },
  {
    "text": "uh, if x is greater than 0, uh, you output x itself and if it's less than 0,",
    "start": "2306635",
    "end": "2312395"
  },
  {
    "text": "you- you output some x multiplied by some coefficient, uh, a, in this case.",
    "start": "2312395",
    "end": "2318350"
  },
  {
    "text": "This is not an attention weight, this is a different coefficient, uh, we trained, and now the shape of the parametric ReLU looks like I show",
    "start": "2318350",
    "end": "2327800"
  },
  {
    "text": "here and empirically this works better than ReLU because you can train this,",
    "start": "2327800",
    "end": "2333515"
  },
  {
    "text": "uh, parameter, uh, A. So, uh, to summarize what we have discussed and what we have learned so far,",
    "start": "2333515",
    "end": "2340955"
  },
  {
    "text": "we talked about, uh, modern deep learning modules that can be included into graph neural network layers to achieve even better performance,",
    "start": "2340955",
    "end": "2349895"
  },
  {
    "text": "we discussed about linear transformations, batch normalization, dropout, different activation functions,",
    "start": "2349895",
    "end": "2357545"
  },
  {
    "text": "and we also discussed the, uh, attention mechanism as well as different ways,",
    "start": "2357545",
    "end": "2362585"
  },
  {
    "text": "uh, then to aggregate, uh, the messages. Um, if you wanna play with these different,",
    "start": "2362585",
    "end": "2368695"
  },
  {
    "text": "um, architectural choices in an easy way, we have actually developed a package called GraphGym that basically allows you to,",
    "start": "2368695",
    "end": "2377330"
  },
  {
    "text": "very quickly and easily try out, uh, and test out different design choices to find the one that works best,",
    "start": "2377330",
    "end": "2385835"
  },
  {
    "text": "uh, on your, uh, individual, uh, problem. So, uh, if you click this, this is a link, it will lead you to GitHub, um,",
    "start": "2385835",
    "end": "2392900"
  },
  {
    "text": "and you can play with this code to see how different design choices make a practical difference,",
    "start": "2392900",
    "end": "2399815"
  },
  {
    "text": "uh, to your own, uh, application or use case.",
    "start": "2399815",
    "end": "2403980"
  }
]