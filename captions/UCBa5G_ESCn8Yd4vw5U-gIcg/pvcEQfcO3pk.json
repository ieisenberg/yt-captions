[
  {
    "text": "OK so we've seen a variety\nof different models",
    "start": "0",
    "end": "4059"
  },
  {
    "text": "from linear models which are\nrather simple and easy to work",
    "start": "4059",
    "end": "8020"
  },
  {
    "text": "with and interpret to\nmore complex models",
    "start": "8020",
    "end": "10390"
  },
  {
    "text": "like nearest neighbor averaging\nand thin plate splines.",
    "start": "10390",
    "end": "14140"
  },
  {
    "text": "And we need to know how to\ndecide amongst these models.",
    "start": "14140",
    "end": "19400"
  },
  {
    "text": "And so we need a way of\nassessing model accuracy,",
    "start": "19400",
    "end": "23560"
  },
  {
    "text": "and when is a model adequate\nand when may we improve it.",
    "start": "23560",
    "end": "28689"
  },
  {
    "text": "OK, so suppose we\nhave a model f hat",
    "start": "28690",
    "end": "30640"
  },
  {
    "text": "of x that's been fit\nto some training data--",
    "start": "30640",
    "end": "32829"
  },
  {
    "text": "and we will denote the\ntraining data by Tr.",
    "start": "32830",
    "end": "35830"
  },
  {
    "text": "And that consists of\nN data pairs xi, yi.",
    "start": "35830",
    "end": "39790"
  },
  {
    "text": "And remember xi-- the notation\nxi means the i-th observation.",
    "start": "39790",
    "end": "47120"
  },
  {
    "text": "And x may be a vector.",
    "start": "47120",
    "end": "48620"
  },
  {
    "text": "So it may have a bunch of\ncomponents. yi is typically",
    "start": "48620",
    "end": "52730"
  },
  {
    "text": "a single y, a scalar.",
    "start": "52730",
    "end": "55380"
  },
  {
    "text": "And we want to see how\nwell this model performs.",
    "start": "55380",
    "end": "57840"
  },
  {
    "text": "Well, we could compute the\naverage squared prediction",
    "start": "57840",
    "end": "60870"
  },
  {
    "text": "error over the training data.",
    "start": "60870",
    "end": "63149"
  },
  {
    "text": "So that means we take\nour y, the observed y,",
    "start": "63150",
    "end": "67050"
  },
  {
    "text": "we subtract from it f hat of x.",
    "start": "67050",
    "end": "71190"
  },
  {
    "text": "We square the difference\nto get rid of the sign,",
    "start": "71190",
    "end": "74310"
  },
  {
    "text": "and we just average those\nover all the training data.",
    "start": "74310",
    "end": "78259"
  },
  {
    "text": "Well, as you may imagine,\nthis may be biased",
    "start": "78260",
    "end": "80390"
  },
  {
    "text": "towards more overfit models.",
    "start": "80390",
    "end": "82783"
  },
  {
    "text": "We saw with that\nthin plate spline",
    "start": "82783",
    "end": "84200"
  },
  {
    "text": "we could fit the\ntraining data exactly.",
    "start": "84200",
    "end": "85825"
  },
  {
    "text": "We could make this mean\nsquared error sub train.",
    "start": "85825",
    "end": "89070"
  },
  {
    "text": "We could make it zero.",
    "start": "89070",
    "end": "90860"
  },
  {
    "text": "Instead, we should, if possible,\ncompute it using a fresh test",
    "start": "90860",
    "end": "94400"
  },
  {
    "text": "data set, which we'll call Te.",
    "start": "94400",
    "end": "96620"
  },
  {
    "text": "So that's an\nadditional say M data",
    "start": "96620",
    "end": "99320"
  },
  {
    "text": "pairs xi, yi different\nfrom the training set.",
    "start": "99320",
    "end": "102590"
  },
  {
    "text": "And then we compute the\nsimilar quantity, which we'll",
    "start": "102590",
    "end": "105170"
  },
  {
    "text": "call mean squared error sub Te.",
    "start": "105170",
    "end": "109170"
  },
  {
    "text": "And that may be a better\nreflection of the performance",
    "start": "109170",
    "end": "112909"
  },
  {
    "text": "of our model.",
    "start": "112910",
    "end": "113530"
  },
  {
    "start": "113530",
    "end": "116440"
  },
  {
    "text": "So now I'm going to\nshow you some examples.",
    "start": "116440",
    "end": "121310"
  },
  {
    "text": "We've got back to\none-dimensional function",
    "start": "121310",
    "end": "123250"
  },
  {
    "text": "fitting.",
    "start": "123250",
    "end": "124140"
  },
  {
    "text": "In the left-hand panel,\nwe see the black curve,",
    "start": "124140",
    "end": "127420"
  },
  {
    "text": "which is actually a simulated.",
    "start": "127420",
    "end": "129649"
  },
  {
    "text": "So it's a generating curve.",
    "start": "129650",
    "end": "131060"
  },
  {
    "text": "That's the true function\nthat we want to estimate.",
    "start": "131060",
    "end": "133780"
  },
  {
    "text": "The points or data points\ngenerated from that curve",
    "start": "133780",
    "end": "136660"
  },
  {
    "text": "with error.",
    "start": "136660",
    "end": "137710"
  },
  {
    "text": "And then we actually see--",
    "start": "137710",
    "end": "139095"
  },
  {
    "text": "you have to look\ncarefully in the plot.",
    "start": "139095",
    "end": "140720"
  },
  {
    "text": "We see three different\nmodels fit to these data.",
    "start": "140720",
    "end": "144050"
  },
  {
    "text": "There's the orange model, the\nblue model and the green model.",
    "start": "144050",
    "end": "148690"
  },
  {
    "text": "And they ordered in complexity.",
    "start": "148690",
    "end": "150980"
  },
  {
    "text": "The orange model\nis a linear model.",
    "start": "150980",
    "end": "154030"
  },
  {
    "text": "The blue model is a more\nflexible model, maybe",
    "start": "154030",
    "end": "157150"
  },
  {
    "text": "some kind of spline--",
    "start": "157150",
    "end": "159970"
  },
  {
    "text": "one-dimensional version\nof the thin plate spline.",
    "start": "159970",
    "end": "162970"
  },
  {
    "text": "And then the green one is a much\nmore flexible version of that.",
    "start": "162970",
    "end": "166720"
  },
  {
    "text": "You can see.",
    "start": "166720",
    "end": "167300"
  },
  {
    "text": "It gets closer to the data.",
    "start": "167300",
    "end": "169970"
  },
  {
    "text": "Now since this is a.",
    "start": "169970",
    "end": "172110"
  },
  {
    "text": "simulated example, we can\ncompute the mean squared error",
    "start": "172110",
    "end": "176010"
  },
  {
    "text": "on a very large\npopulation of test data.",
    "start": "176010",
    "end": "179640"
  },
  {
    "text": "And so in the right-hand plot,\nwe plot the mean squared error",
    "start": "179640",
    "end": "183900"
  },
  {
    "text": "for this large\npopulation of test data--",
    "start": "183900",
    "end": "186480"
  },
  {
    "text": "and that's the red curve.",
    "start": "186480",
    "end": "189450"
  },
  {
    "text": "And you'll notice that\nit starts off high",
    "start": "189450",
    "end": "195590"
  },
  {
    "text": "for the very rigid model.",
    "start": "195590",
    "end": "197660"
  },
  {
    "text": "It drops down and becomes\nquite low for in between model.",
    "start": "197660",
    "end": "202830"
  },
  {
    "text": "But then for the\nmore flexible model,",
    "start": "202830",
    "end": "205350"
  },
  {
    "text": "it starts increasing again.",
    "start": "205350",
    "end": "208425"
  },
  {
    "text": "Of course, the\nmean squared error",
    "start": "208425",
    "end": "209800"
  },
  {
    "text": "on the training data--\nthat's the gray curve just",
    "start": "209800",
    "end": "212530"
  },
  {
    "text": "keeps on decreasing because\nthe more flexible the model,",
    "start": "212530",
    "end": "215350"
  },
  {
    "text": "the closer it gets\nto the data point.",
    "start": "215350",
    "end": "218780"
  },
  {
    "text": "But for the mean squared\nerror on the test data,",
    "start": "218780",
    "end": "222170"
  },
  {
    "text": "we can see there's kind\nof a magic point, which",
    "start": "222170",
    "end": "225530"
  },
  {
    "text": "is the point at\nwhich it minimizes",
    "start": "225530",
    "end": "227180"
  },
  {
    "text": "the mean squared error.",
    "start": "227180",
    "end": "229140"
  },
  {
    "text": "And in this case, that's\nthis point over here.",
    "start": "229140",
    "end": "231710"
  },
  {
    "text": "And it turns out\nit's pretty much spot",
    "start": "231710",
    "end": "234030"
  },
  {
    "text": "on for our middle, the medium\nflexible model in this figure.",
    "start": "234030",
    "end": "238800"
  },
  {
    "text": "And if you look\nclosely at the plot,",
    "start": "238800",
    "end": "240870"
  },
  {
    "text": "you'll see that the\nblue curve actually",
    "start": "240870",
    "end": "244019"
  },
  {
    "text": "gets fairly close\nto the black curve.",
    "start": "244020",
    "end": "248880"
  },
  {
    "text": "Again, because this\nis a simulation model,",
    "start": "248880",
    "end": "251160"
  },
  {
    "text": "the horizontal dotted line\nis the mean squared error",
    "start": "251160",
    "end": "255000"
  },
  {
    "text": "that the true function makes\nfor data from this population.",
    "start": "255000",
    "end": "260130"
  },
  {
    "text": "And, of course, that is\nthe irreducible error,",
    "start": "260130",
    "end": "263220"
  },
  {
    "text": "which we call the\nvariance of epsilon.",
    "start": "263220",
    "end": "265405"
  },
  {
    "start": "265405",
    "end": "269050"
  },
  {
    "text": "Here's another example\nof the same kind.",
    "start": "269050",
    "end": "271740"
  },
  {
    "text": "But here the true functions\nare actually very smooth--",
    "start": "271740",
    "end": "274900"
  },
  {
    "text": "same setup.",
    "start": "274900",
    "end": "276430"
  },
  {
    "text": "Well, now we see that\nthe mean squared error,",
    "start": "276430",
    "end": "278860"
  },
  {
    "text": "the linear model\ndoes pretty well.",
    "start": "278860",
    "end": "280919"
  },
  {
    "text": "The best model is not much\ndifferent from the linear model,",
    "start": "280920",
    "end": "283720"
  },
  {
    "text": "and the wiggly one, of\ncourse, is overfitting again.",
    "start": "283720",
    "end": "287020"
  },
  {
    "text": "And so it's making\nbig prediction errors.",
    "start": "287020",
    "end": "289620"
  },
  {
    "start": "289620",
    "end": "292330"
  },
  {
    "text": "The training error again\nkeeps on going down.",
    "start": "292330",
    "end": "296169"
  },
  {
    "text": "And finally, here's quite\na wiggly true function",
    "start": "296170",
    "end": "299800"
  },
  {
    "text": "on the left.",
    "start": "299800",
    "end": "301330"
  },
  {
    "text": "The linear model does\na really lousy job.",
    "start": "301330",
    "end": "304939"
  },
  {
    "text": "The most flexible model\ndoes about the best.",
    "start": "304940",
    "end": "309750"
  },
  {
    "text": "The blue model and the green\nmodel are pretty good--",
    "start": "309750",
    "end": "313100"
  },
  {
    "text": "pretty close together\nin terms of the mean",
    "start": "313100",
    "end": "315320"
  },
  {
    "text": "squared error on the test data.",
    "start": "315320",
    "end": "317930"
  },
  {
    "text": "So I think this\ndrums home the point.",
    "start": "317930",
    "end": "321860"
  },
  {
    "text": "Again, the training mean squared\nerror just keeps on going down.",
    "start": "321860",
    "end": "325629"
  },
  {
    "text": "So this drums home\nthe point that if we",
    "start": "325630",
    "end": "327490"
  },
  {
    "text": "want to have a model that\nhas good prediction error--",
    "start": "327490",
    "end": "331750"
  },
  {
    "text": "and that's measured here in\nterms of mean squared prediction",
    "start": "331750",
    "end": "335960"
  },
  {
    "text": "error on the test data.",
    "start": "335960",
    "end": "337490"
  },
  {
    "text": "We'd like to be able\nto estimate this curve.",
    "start": "337490",
    "end": "340289"
  },
  {
    "text": "And one way you can do\nthat, the red curve.",
    "start": "340290",
    "end": "342540"
  },
  {
    "text": "You can do that is to have a\nheld out test data set that you",
    "start": "342540",
    "end": "345830"
  },
  {
    "text": "can value the performance\nof your different models",
    "start": "345830",
    "end": "348710"
  },
  {
    "text": "on the test data set.",
    "start": "348710",
    "end": "350600"
  },
  {
    "text": "And we're going to talk\nabout ways of doing this",
    "start": "350600",
    "end": "354230"
  },
  {
    "text": "later on in the course.",
    "start": "354230",
    "end": "355780"
  },
  {
    "start": "355780",
    "end": "359440"
  },
  {
    "text": "I want to tell you about one\naspect of this, which is called",
    "start": "359440",
    "end": "363250"
  },
  {
    "text": "a bias variance trade-off.",
    "start": "363250",
    "end": "365290"
  },
  {
    "text": "So, again, we've got f hat of\nx, which is fit to the training",
    "start": "365290",
    "end": "368560"
  },
  {
    "text": "data.",
    "start": "368560",
    "end": "369460"
  },
  {
    "text": "And let's say x0, y0\nis a test observation",
    "start": "369460",
    "end": "373930"
  },
  {
    "text": "drawn from the population.",
    "start": "373930",
    "end": "375289"
  },
  {
    "text": "And we're going to evaluate\nthe model at this test--",
    "start": "375290",
    "end": "378460"
  },
  {
    "text": "the single test observation.",
    "start": "378460",
    "end": "382120"
  },
  {
    "text": "And let's suppose the true\nmodel is given by the function f",
    "start": "382120",
    "end": "386949"
  },
  {
    "text": "again, where f is the\nregression function",
    "start": "386950",
    "end": "389230"
  },
  {
    "text": "or the conditional\nexpectation in the population.",
    "start": "389230",
    "end": "393190"
  },
  {
    "text": "So let's look at the expected\nprediction error between f hat",
    "start": "393190",
    "end": "396970"
  },
  {
    "text": "at x0.",
    "start": "396970",
    "end": "398440"
  },
  {
    "text": "So that's the predicted model--",
    "start": "398440",
    "end": "400990"
  },
  {
    "text": "the fitted model on\nthe training data",
    "start": "400990",
    "end": "403090"
  },
  {
    "text": "evaluated at the new\npoint x0, and see",
    "start": "403090",
    "end": "405910"
  },
  {
    "text": "what the expected distance\nis from the test point y0.",
    "start": "405910",
    "end": "410860"
  },
  {
    "text": "So this expected-- this\nexpectation averages",
    "start": "410860",
    "end": "414430"
  },
  {
    "text": "over the variability\nof the new y0,",
    "start": "414430",
    "end": "416979"
  },
  {
    "text": "as well as the variability\nthat went into the training",
    "start": "416980",
    "end": "420020"
  },
  {
    "text": "set used to build f hat.",
    "start": "420020",
    "end": "423860"
  },
  {
    "text": "So it turns out that\nwe can break this.",
    "start": "423860",
    "end": "426919"
  },
  {
    "text": "We can break up this expression\ninto three pieces exactly.",
    "start": "426920",
    "end": "431570"
  },
  {
    "text": "The one piece is, again,\nthe irreducible error",
    "start": "431570",
    "end": "434240"
  },
  {
    "text": "that comes from the random\nvariation in the new test point",
    "start": "434240",
    "end": "437690"
  },
  {
    "text": "y0 about the true function f.",
    "start": "437690",
    "end": "441410"
  },
  {
    "text": "But these other two pieces\nbreak up the reducible part",
    "start": "441410",
    "end": "446730"
  },
  {
    "text": "of the error, what we call the\nreducible part before into two",
    "start": "446730",
    "end": "449670"
  },
  {
    "text": "components.",
    "start": "449670",
    "end": "450790"
  },
  {
    "text": "One is called the\nvariance of f hat.",
    "start": "450790",
    "end": "454380"
  },
  {
    "text": "And that's the variance\nthat comes from having",
    "start": "454380",
    "end": "457350"
  },
  {
    "text": "different training sets.",
    "start": "457350",
    "end": "458430"
  },
  {
    "text": "If I go to a new training\nset and I fit my model again,",
    "start": "458430",
    "end": "461070"
  },
  {
    "text": "I'd have a different\nfunction f hat.",
    "start": "461070",
    "end": "463620"
  },
  {
    "text": "And so, as I--",
    "start": "463620",
    "end": "465650"
  },
  {
    "text": "well, if I were to look at many,\nmany different training sets,",
    "start": "465650",
    "end": "468380"
  },
  {
    "text": "there would be variability\nin my prediction at x0.",
    "start": "468380",
    "end": "471950"
  },
  {
    "text": "And then a quantity\ncalled the bias of f hat.",
    "start": "471950",
    "end": "475470"
  },
  {
    "text": "And what the bias\nis, is the difference",
    "start": "475470",
    "end": "478070"
  },
  {
    "text": "between the average\nprediction at x0",
    "start": "478070",
    "end": "482140"
  },
  {
    "text": "averaged over all these\ndifferent training",
    "start": "482140",
    "end": "483890"
  },
  {
    "text": "sets and the true f of x0.",
    "start": "483890",
    "end": "487400"
  },
  {
    "text": "And what you have is typically\nas the flexibility of f hat",
    "start": "487400",
    "end": "490850"
  },
  {
    "text": "increases, its\nvariance increases",
    "start": "490850",
    "end": "493280"
  },
  {
    "text": "because it's going after\nthe individual training set",
    "start": "493280",
    "end": "496430"
  },
  {
    "text": "that you've provided,\nwhich will, of course, be",
    "start": "496430",
    "end": "499009"
  },
  {
    "text": "different from the\nnext training set.",
    "start": "499010",
    "end": "501140"
  },
  {
    "text": "But its bias decreases.",
    "start": "501140",
    "end": "503420"
  },
  {
    "text": "So choosing the flexibility\nbased on average test error",
    "start": "503420",
    "end": "506960"
  },
  {
    "text": "amounts to what we call a\nbias variance trade-off.",
    "start": "506960",
    "end": "511569"
  },
  {
    "text": "This will come up a lot in\nfuture parts of the course.",
    "start": "511570",
    "end": "517820"
  },
  {
    "text": "For those three examples, we\nsee the bias variance trade off.",
    "start": "517820",
    "end": "522320"
  },
  {
    "text": "Again, in this\nplot, the red curve",
    "start": "522320",
    "end": "524540"
  },
  {
    "text": "is the mean squared\nerror on the test data.",
    "start": "524540",
    "end": "527149"
  },
  {
    "text": "And then below it, we\nhave the two components",
    "start": "527150",
    "end": "529730"
  },
  {
    "text": "of that mean squared error.",
    "start": "529730",
    "end": "531082"
  },
  {
    "text": "There are two important\ncomponents, which",
    "start": "531082",
    "end": "532790"
  },
  {
    "text": "are the bias and the variance.",
    "start": "532790",
    "end": "535459"
  },
  {
    "text": "And in the left plot, we've got\nthe bias decreasing and then",
    "start": "535460",
    "end": "543430"
  },
  {
    "text": "flattening off as\nwe get more flexible",
    "start": "543430",
    "end": "546250"
  },
  {
    "text": "and the variance increasing.",
    "start": "546250",
    "end": "548360"
  },
  {
    "text": "And when you add\nthose two components,",
    "start": "548360",
    "end": "550000"
  },
  {
    "text": "you get the u-shaped curve.",
    "start": "550000",
    "end": "553420"
  },
  {
    "text": "And in the middle\nand last plots that",
    "start": "553420",
    "end": "555579"
  },
  {
    "text": "correspond to the\nother two examples,",
    "start": "555580",
    "end": "560350"
  },
  {
    "text": "the same decomposition is given.",
    "start": "560350",
    "end": "562459"
  },
  {
    "text": "And because the nature\nof that problem changed,",
    "start": "562460",
    "end": "564910"
  },
  {
    "text": "the trade-off is changing.",
    "start": "564910",
    "end": "568259"
  },
  {
    "text": "OK, so we've seen\nnow that choosing",
    "start": "568260",
    "end": "570840"
  },
  {
    "text": "the amount of\nflexibility of a model",
    "start": "570840",
    "end": "573270"
  },
  {
    "text": "amounts to a bias\nvariance trade-off.",
    "start": "573270",
    "end": "575580"
  },
  {
    "text": "And depending on the\nproblem, we might",
    "start": "575580",
    "end": "578190"
  },
  {
    "text": "want to make that trade-off\nin a different place.",
    "start": "578190",
    "end": "580900"
  },
  {
    "text": "And we can use a validation\nset or left out data",
    "start": "580900",
    "end": "587760"
  },
  {
    "text": "to help us make that choice.",
    "start": "587760",
    "end": "589240"
  },
  {
    "text": "But that's a-- that's\nthe choice that needs",
    "start": "589240",
    "end": "591060"
  },
  {
    "text": "to be made to select the model.",
    "start": "591060",
    "end": "594150"
  },
  {
    "text": "Now, we've been addressing this\nin terms of regression problems.",
    "start": "594150",
    "end": "599200"
  },
  {
    "text": "In the next segment, we're\ngoing to see how all this works",
    "start": "599200",
    "end": "602070"
  },
  {
    "text": "for classification problems.",
    "start": "602070",
    "end": "604500"
  },
  {
    "start": "604500",
    "end": "605000"
  }
]