[
  {
    "start": "0",
    "end": "4329"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4329",
    "end": "6050"
  },
  {
    "text": "This is part 8 in our series\non Contextual Representations.",
    "start": "6050",
    "end": "9340"
  },
  {
    "text": "We're going to talk\nbriefly about sequence",
    "start": "9340",
    "end": "11770"
  },
  {
    "text": "to sequence architectures.",
    "start": "11770",
    "end": "13720"
  },
  {
    "text": "To kick it off, I thought\nI would begin with tasks.",
    "start": "13720",
    "end": "17030"
  },
  {
    "text": "These are going to be tasks\nthat have natural sequence",
    "start": "17030",
    "end": "19870"
  },
  {
    "text": "to sequence structure.",
    "start": "19870",
    "end": "21220"
  },
  {
    "text": "And I'm trying to leave open for\nnow whether we would actually",
    "start": "21220",
    "end": "24130"
  },
  {
    "text": "model them with sequence\nto sequence architectures.",
    "start": "24130",
    "end": "26650"
  },
  {
    "text": "That's a separate question.",
    "start": "26650",
    "end": "28510"
  },
  {
    "text": "Seq2seq tasks include\nmachine translation.",
    "start": "28510",
    "end": "31360"
  },
  {
    "text": "Of course, this is a classic\none where a text in one language",
    "start": "31360",
    "end": "34430"
  },
  {
    "text": "comes in.",
    "start": "34430",
    "end": "34930"
  },
  {
    "text": "And we would like\nto produce text",
    "start": "34930",
    "end": "36400"
  },
  {
    "text": "in another language\nas the output.",
    "start": "36400",
    "end": "38710"
  },
  {
    "text": "Summarization, also a\nclassic seq2seq problem.",
    "start": "38710",
    "end": "42280"
  },
  {
    "text": "A long text comes in.",
    "start": "42280",
    "end": "43629"
  },
  {
    "text": "And a presumably\nshorter one comes out",
    "start": "43630",
    "end": "45670"
  },
  {
    "text": "summarizing the input.",
    "start": "45670",
    "end": "47929"
  },
  {
    "text": "Free-form question\nanswering, where we're",
    "start": "47930",
    "end": "49820"
  },
  {
    "text": "trying to generate answers.",
    "start": "49820",
    "end": "51230"
  },
  {
    "text": "This could also be\na seq2seq problem",
    "start": "51230",
    "end": "53239"
  },
  {
    "text": "where a question may be with\nsome contextual information",
    "start": "53240",
    "end": "55730"
  },
  {
    "text": "comes in.",
    "start": "55730",
    "end": "56510"
  },
  {
    "text": "And the task in decoding\nis to generate an answer.",
    "start": "56510",
    "end": "60500"
  },
  {
    "text": "Dialogue, of course,\nclassic seq2seq problem,",
    "start": "60500",
    "end": "63350"
  },
  {
    "text": "utterances to utterances.",
    "start": "63350",
    "end": "65449"
  },
  {
    "text": "Semantic parsing could also\nbe thought of as a seq2seq.",
    "start": "65450",
    "end": "68750"
  },
  {
    "text": "Here, natural language\nsentences come in.",
    "start": "68750",
    "end": "71330"
  },
  {
    "text": "And we try to map them to\ntheir logical forms capturing",
    "start": "71330",
    "end": "74660"
  },
  {
    "text": "aspects of their meaning.",
    "start": "74660",
    "end": "76160"
  },
  {
    "text": "Related task would\nbe code generation.",
    "start": "76160",
    "end": "78230"
  },
  {
    "text": "Here, a natural language\nsentence comes in.",
    "start": "78230",
    "end": "80240"
  },
  {
    "text": "And we try to produce a\nprogram that the sentence is",
    "start": "80240",
    "end": "83000"
  },
  {
    "text": "describing.",
    "start": "83000",
    "end": "84230"
  },
  {
    "text": "And that is just a small\nsample of the many things",
    "start": "84230",
    "end": "87350"
  },
  {
    "text": "that we could call\nseq2seq tasks.",
    "start": "87350",
    "end": "90020"
  },
  {
    "text": "And even these are just special\ncases of the more general class",
    "start": "90020",
    "end": "93859"
  },
  {
    "text": "of things that we might call\nencoder-decoder problems, which",
    "start": "93860",
    "end": "97430"
  },
  {
    "text": "would be agnostic about\nwhether the encoding",
    "start": "97430",
    "end": "99740"
  },
  {
    "text": "and decoding involve sequences.",
    "start": "99740",
    "end": "101360"
  },
  {
    "text": "They could involve images,\nvideo, speech, and so forth.",
    "start": "101360",
    "end": "107220"
  },
  {
    "text": "I've been offering historical\nnotes throughout this series",
    "start": "107220",
    "end": "110220"
  },
  {
    "text": "of lectures.",
    "start": "110220",
    "end": "110880"
  },
  {
    "text": "And I think this is a nice\npoint to emphasize that the RNN",
    "start": "110880",
    "end": "114000"
  },
  {
    "text": "era really primed us to\nthink about seq2seq problems",
    "start": "114000",
    "end": "117750"
  },
  {
    "text": "in the context of transformers.",
    "start": "117750",
    "end": "120090"
  },
  {
    "text": "On the left here, I have\na classic RNN formulation",
    "start": "120090",
    "end": "123570"
  },
  {
    "text": "of a seq2seq problem where we\nhave the input sequence ABCD.",
    "start": "123570",
    "end": "127380"
  },
  {
    "text": "And then we begin decoding with\nthis special symbol, decode",
    "start": "127380",
    "end": "130860"
  },
  {
    "text": "XYZ.",
    "start": "130860",
    "end": "131550"
  },
  {
    "text": "And then we produce our n token.",
    "start": "131550",
    "end": "133800"
  },
  {
    "text": "And that is the job of decoding.",
    "start": "133800",
    "end": "136260"
  },
  {
    "text": "The historical note\nhere is that those tasks",
    "start": "136260",
    "end": "138930"
  },
  {
    "text": "evolved from standard\nRNNs into RNNs with lots",
    "start": "138930",
    "end": "142859"
  },
  {
    "text": "of attention mechanisms.",
    "start": "142860",
    "end": "144180"
  },
  {
    "text": "On the top here\ndesigned specifically",
    "start": "144180",
    "end": "147209"
  },
  {
    "text": "to help the decoding steps.",
    "start": "147210",
    "end": "149040"
  },
  {
    "text": "Remember what was in\nthe encoding part--",
    "start": "149040",
    "end": "151590"
  },
  {
    "text": "by offering all of these\nattention mechanisms",
    "start": "151590",
    "end": "154319"
  },
  {
    "text": "back into that encoding phase.",
    "start": "154320",
    "end": "156510"
  },
  {
    "text": "And what we see, again,\nin the transformer paper",
    "start": "156510",
    "end": "159989"
  },
  {
    "text": "is a full embrace of attention\nas the primary mechanism",
    "start": "159990",
    "end": "163440"
  },
  {
    "text": "and a dropping away of all\nof these recurrent mechanisms",
    "start": "163440",
    "end": "166750"
  },
  {
    "text": "here.",
    "start": "166750",
    "end": "169070"
  },
  {
    "text": "In the context of transformers,\nwe have a variety of ways",
    "start": "169070",
    "end": "173450"
  },
  {
    "text": "that we could think about\nseq2seq problems, one",
    "start": "173450",
    "end": "177050"
  },
  {
    "text": "of them being encoder-decoder.",
    "start": "177050",
    "end": "179090"
  },
  {
    "text": "But other options\npresent themselves.",
    "start": "179090",
    "end": "181129"
  },
  {
    "text": "This is a nice figure\nfrom the T5 paper, which",
    "start": "181130",
    "end": "183590"
  },
  {
    "text": "we'll talk about in a second.",
    "start": "183590",
    "end": "185420"
  },
  {
    "text": "On the left, you\nhave encoder-decoder,",
    "start": "185420",
    "end": "187310"
  },
  {
    "text": "as I said, where we fully encode\nthe input in the encoder side",
    "start": "187310",
    "end": "191120"
  },
  {
    "text": "with some set of parameters.",
    "start": "191120",
    "end": "192739"
  },
  {
    "text": "And then possibly\ndifferent parameters",
    "start": "192740",
    "end": "194870"
  },
  {
    "text": "do decoding, where in\nthe decoding steps,",
    "start": "194870",
    "end": "197420"
  },
  {
    "text": "we attend fully back to all\nthe steps from the encoder.",
    "start": "197420",
    "end": "201569"
  },
  {
    "text": "But we needn't have this\nencoder-decoder structure.",
    "start": "201570",
    "end": "205350"
  },
  {
    "text": "Another option,\nfor example, would",
    "start": "205350",
    "end": "207210"
  },
  {
    "text": "be to simply process\nthese sequences",
    "start": "207210",
    "end": "209610"
  },
  {
    "text": "with a standard language model.",
    "start": "209610",
    "end": "211420"
  },
  {
    "text": "So in the middle here, you have\na transformer-based language",
    "start": "211420",
    "end": "215030"
  },
  {
    "text": "model.",
    "start": "215030",
    "end": "215530"
  },
  {
    "text": "And you can see that\ncharacteristic attention",
    "start": "215530",
    "end": "217590"
  },
  {
    "text": "mask, where we don't get\nto look into the future",
    "start": "217590",
    "end": "220030"
  },
  {
    "text": "but rather can only attend to\nthe past, even for the part",
    "start": "220030",
    "end": "223770"
  },
  {
    "text": "that we're thinking of\nas the encoded part.",
    "start": "223770",
    "end": "227110"
  },
  {
    "text": "An obvious variation\nof that would",
    "start": "227110",
    "end": "228970"
  },
  {
    "text": "be to take our language model.",
    "start": "228970",
    "end": "230720"
  },
  {
    "text": "And when we do encoding, do\na full attention connection",
    "start": "230720",
    "end": "234790"
  },
  {
    "text": "set across all the things\nthat we're doing encoding.",
    "start": "234790",
    "end": "237730"
  },
  {
    "text": "That's what you can\nsee reflected here",
    "start": "237730",
    "end": "239890"
  },
  {
    "text": "where when we're\ndoing encoding just",
    "start": "239890",
    "end": "241510"
  },
  {
    "text": "as in the encoder-decoder\nstructure,",
    "start": "241510",
    "end": "243220"
  },
  {
    "text": "we can have every element\nattend to every other element.",
    "start": "243220",
    "end": "246880"
  },
  {
    "text": "And then here, when we\nstart to do decoding, that's",
    "start": "246880",
    "end": "249910"
  },
  {
    "text": "where the mask can only\nlook into the past and not",
    "start": "249910",
    "end": "253120"
  },
  {
    "text": "the future.",
    "start": "253120",
    "end": "254599"
  },
  {
    "text": "So that's a nice framework\nfor thinking about this.",
    "start": "254600",
    "end": "256898"
  },
  {
    "text": "And the middle and\nright-hand options",
    "start": "256899",
    "end": "259750"
  },
  {
    "text": "have become\nincreasingly prominent",
    "start": "259750",
    "end": "261700"
  },
  {
    "text": "as people have explored ever\nlarger variants of the GPT",
    "start": "261700",
    "end": "265360"
  },
  {
    "text": "architecture, which is a\nstandard language model.",
    "start": "265360",
    "end": "269080"
  },
  {
    "text": "But I'm going to focus now on\ntwo encoder-decoder releases",
    "start": "269080",
    "end": "272919"
  },
  {
    "text": "that I think are very\npowerful, beginning",
    "start": "272920",
    "end": "275110"
  },
  {
    "text": "with T5, which was the source\nof that nice previous framework",
    "start": "275110",
    "end": "278710"
  },
  {
    "text": "there.",
    "start": "278710",
    "end": "279490"
  },
  {
    "text": "T5 is an encoder-decoder\nvariant that",
    "start": "279490",
    "end": "282280"
  },
  {
    "text": "had extensive, multitask,\nsupervised and unsupervised",
    "start": "282280",
    "end": "286630"
  },
  {
    "text": "training across lots\nof different tasks.",
    "start": "286630",
    "end": "289750"
  },
  {
    "text": "And then one very\ninnovative thing",
    "start": "289750",
    "end": "291640"
  },
  {
    "text": "that they did in the T5\npaper, which really gives us",
    "start": "291640",
    "end": "294280"
  },
  {
    "text": "a glimpse of what\nwas about to happen",
    "start": "294280",
    "end": "296170"
  },
  {
    "text": "with in-context\nlearning, is that they",
    "start": "296170",
    "end": "298840"
  },
  {
    "text": "offered task prefixes\nlike, translate English",
    "start": "298840",
    "end": "302169"
  },
  {
    "text": "to German colon.",
    "start": "302170",
    "end": "303550"
  },
  {
    "text": "And then you got the true input.",
    "start": "303550",
    "end": "305379"
  },
  {
    "text": "And so that\ninstruction on the left",
    "start": "305380",
    "end": "307600"
  },
  {
    "text": "is telling the model what\nwe want to do in decoding.",
    "start": "307600",
    "end": "310930"
  },
  {
    "text": "And it guides the model, in,\nthis case, to do translation.",
    "start": "310930",
    "end": "313780"
  },
  {
    "text": "But the same part\nafter the colon",
    "start": "313780",
    "end": "316210"
  },
  {
    "text": "could be performing\na sentiment task",
    "start": "316210",
    "end": "318850"
  },
  {
    "text": "given a different description\nof the task before the colon.",
    "start": "318850",
    "end": "322960"
  },
  {
    "text": "Wonderfully\ninsightful thing where",
    "start": "322960",
    "end": "324880"
  },
  {
    "text": "we express all these tasks\nas natural language, which",
    "start": "324880",
    "end": "327400"
  },
  {
    "text": "we simply encode, and that\nguides the model's behavior,",
    "start": "327400",
    "end": "331460"
  },
  {
    "text": "essentially, as though\nthose task instructions were",
    "start": "331460",
    "end": "334310"
  },
  {
    "text": "themselves structured\ninformation as inputs",
    "start": "334310",
    "end": "336800"
  },
  {
    "text": "to the model.",
    "start": "336800",
    "end": "338979"
  },
  {
    "text": "For T5, we have lots\nof model releases",
    "start": "338980",
    "end": "341470"
  },
  {
    "text": "as well, which has been\ntremendously empowering.",
    "start": "341470",
    "end": "343510"
  },
  {
    "text": "This is a sample\nof the models that",
    "start": "343510",
    "end": "345130"
  },
  {
    "text": "are available on Hugging Face.",
    "start": "345130",
    "end": "346840"
  },
  {
    "text": "And you can see that they\nrange from very manageable",
    "start": "346840",
    "end": "349480"
  },
  {
    "text": "60 million parameter models on\nup to really large 11 billion",
    "start": "349480",
    "end": "353860"
  },
  {
    "text": "parameter releases.",
    "start": "353860",
    "end": "355750"
  },
  {
    "text": "Relatedly, the\nFLAN-T5 models are",
    "start": "355750",
    "end": "359260"
  },
  {
    "text": "variants of the T5 architecture\nthat were specifically",
    "start": "359260",
    "end": "362680"
  },
  {
    "text": "instruction tuned.",
    "start": "362680",
    "end": "364007"
  },
  {
    "text": "And that's a set of\nmethods that we'll",
    "start": "364007",
    "end": "365590"
  },
  {
    "text": "talk about in the next\nunit of the course.",
    "start": "365590",
    "end": "368660"
  },
  {
    "text": "Those are also very powerful.",
    "start": "368660",
    "end": "371150"
  },
  {
    "text": "So that's T5.",
    "start": "371150",
    "end": "372160"
  },
  {
    "text": "The other architecture that I\nthought I would highlight here",
    "start": "372160",
    "end": "375010"
  },
  {
    "text": "is BART.",
    "start": "375010",
    "end": "376300"
  },
  {
    "text": "BART has some similarities and\nsome real differences with T5.",
    "start": "376300",
    "end": "380349"
  },
  {
    "text": "The essence of BART is\nthat on the encoding side,",
    "start": "380350",
    "end": "383720"
  },
  {
    "text": "we're going to have a standard\nBERT-like architecture.",
    "start": "383720",
    "end": "387100"
  },
  {
    "text": "And on the decoding\nside, we're going",
    "start": "387100",
    "end": "388840"
  },
  {
    "text": "to have a standard\nGPT-like architecture.",
    "start": "388840",
    "end": "392199"
  },
  {
    "text": "That's fairly straightforward.",
    "start": "392200",
    "end": "393940"
  },
  {
    "text": "What's interesting about BART\nis the way pretraining happens.",
    "start": "393940",
    "end": "397510"
  },
  {
    "text": "This is essentially\noriented around taking",
    "start": "397510",
    "end": "401680"
  },
  {
    "text": "corrupted sequences as\ninput and figuring out",
    "start": "401680",
    "end": "404530"
  },
  {
    "text": "how to uncorrupt them.",
    "start": "404530",
    "end": "405889"
  },
  {
    "text": "So what they did on\nthe corrupting side",
    "start": "405890",
    "end": "408160"
  },
  {
    "text": "is, for example, text infilling\nwhere whole parts of the input",
    "start": "408160",
    "end": "412090"
  },
  {
    "text": "are masked out or removed,\nsentence shuffling where",
    "start": "412090",
    "end": "415540"
  },
  {
    "text": "we reorganize parts of\nthe input, token masking,",
    "start": "415540",
    "end": "419620"
  },
  {
    "text": "token deletion, and\ndocument rotation.",
    "start": "419620",
    "end": "423340"
  },
  {
    "text": "And what they found is that\nthe most effective pretraining",
    "start": "423340",
    "end": "426940"
  },
  {
    "text": "regime was a combination\nof that text infilling",
    "start": "426940",
    "end": "429670"
  },
  {
    "text": "step and the sentence\nshuffling step.",
    "start": "429670",
    "end": "431920"
  },
  {
    "text": "And remember, the idea here\nis that in pretraining,",
    "start": "431920",
    "end": "435220"
  },
  {
    "text": "we're feeding in these corrupted\nsequences with these two",
    "start": "435220",
    "end": "437860"
  },
  {
    "text": "techniques by and large\nand having the model learn",
    "start": "437860",
    "end": "440620"
  },
  {
    "text": "to uncorrupt those sequences.",
    "start": "440620",
    "end": "442210"
  },
  {
    "text": "And the idea there, which\nis similar to the insight",
    "start": "442210",
    "end": "444669"
  },
  {
    "text": "that we had from Electra,\nis that that kind of task",
    "start": "444670",
    "end": "447730"
  },
  {
    "text": "can lead the model to understand\nwhat good sequences look like.",
    "start": "447730",
    "end": "452060"
  },
  {
    "text": "So that's the pretraining phase.",
    "start": "452060",
    "end": "453700"
  },
  {
    "text": "If you download parameters\nfrom Hugging Face,",
    "start": "453700",
    "end": "456100"
  },
  {
    "text": "they're likely to be\npretrained in this kind",
    "start": "456100",
    "end": "458620"
  },
  {
    "text": "of uncorrupting fashion.",
    "start": "458620",
    "end": "461270"
  },
  {
    "text": "For fine-tuning, the protocol\nis a little bit different.",
    "start": "461270",
    "end": "464720"
  },
  {
    "text": "If we're doing\nclassification tasks,",
    "start": "464720",
    "end": "467000"
  },
  {
    "text": "we feed uncorrupted\ncopies of the input",
    "start": "467000",
    "end": "469580"
  },
  {
    "text": "into the encoder\nand the decoder.",
    "start": "469580",
    "end": "471810"
  },
  {
    "text": "And then maybe we fine-tune\nthe final decoder state",
    "start": "471810",
    "end": "475100"
  },
  {
    "text": "as we would with GPT against\nour classification task.",
    "start": "475100",
    "end": "479360"
  },
  {
    "text": "And for standard\nseq2seq problems,",
    "start": "479360",
    "end": "481849"
  },
  {
    "text": "we simply feed in the input\nand the output to the model",
    "start": "481850",
    "end": "485690"
  },
  {
    "text": "and then fine-tune it on that\nbasis with no corruption.",
    "start": "485690",
    "end": "488630"
  },
  {
    "text": "The corruption is by\nand large confined",
    "start": "488630",
    "end": "491210"
  },
  {
    "text": "to the pretraining phase.",
    "start": "491210",
    "end": "492710"
  },
  {
    "text": "And the evidence that\nis offered in the paper",
    "start": "492710",
    "end": "495229"
  },
  {
    "text": "is that that kind\nof objective puts",
    "start": "495230",
    "end": "498240"
  },
  {
    "text": "models in a good pretrained\nstate, where they're",
    "start": "498240",
    "end": "500240"
  },
  {
    "text": "really good at these\nfine-tuning tasks",
    "start": "500240",
    "end": "502490"
  },
  {
    "text": "across a lot of different tasks.",
    "start": "502490",
    "end": "504930"
  },
  {
    "text": "So that's T5 and BART.",
    "start": "504930",
    "end": "506419"
  },
  {
    "text": "That's just two samples\nfrom the wide range",
    "start": "506420",
    "end": "509060"
  },
  {
    "text": "of different seq2seq\narchitectures",
    "start": "509060",
    "end": "510710"
  },
  {
    "text": "that are out there.",
    "start": "510710",
    "end": "511580"
  },
  {
    "text": "But I think they're\nboth very powerful",
    "start": "511580",
    "end": "513529"
  },
  {
    "text": "as pretrained artifacts that\nyou can make use of and also",
    "start": "513530",
    "end": "516979"
  },
  {
    "text": "highlight some of\nthe innovation that",
    "start": "516980",
    "end": "518690"
  },
  {
    "text": "is happening with transformers\nin the seq2seq space.",
    "start": "518690",
    "end": "523479"
  },
  {
    "start": "523479",
    "end": "528000"
  }
]