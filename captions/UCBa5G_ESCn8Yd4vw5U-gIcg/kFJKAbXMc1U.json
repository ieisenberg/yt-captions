[
  {
    "text": "uh good afternoon welcome to ee 380 this is a stanford university's",
    "start": "11280",
    "end": "17440"
  },
  {
    "text": "symposium on computer systems our speaker today is peter mcmahon he's",
    "start": "17440",
    "end": "23840"
  },
  {
    "text": "going to speak about computing with physical systems he's a professor at cornell university and",
    "start": "23840",
    "end": "31039"
  },
  {
    "text": "spent his graduate years at stanford uh in any case uh",
    "start": "31039",
    "end": "36880"
  },
  {
    "text": "peter it's all yours great well thank you very much for the invitation to speak uh and thanks for",
    "start": "36880",
    "end": "43680"
  },
  {
    "text": "the the kind introduction uh ee 380 was actually the very first class",
    "start": "43680",
    "end": "49520"
  },
  {
    "text": "i took at stanford it was in fall of 2008 and i remember sitting in there in the room and gates where the",
    "start": "49520",
    "end": "55600"
  },
  {
    "text": "seminars took place and filling in the uh the weekly homework of summarizing what the speaker said um so it's very",
    "start": "55600",
    "end": "62000"
  },
  {
    "text": "fun fun to be back and uh even though it's only virtual i hope that i i get to come back in person at some point in the",
    "start": "62000",
    "end": "67920"
  },
  {
    "text": "not too distant future i'm sure i will but nevertheless it's great to have the opportunity to share this work with the",
    "start": "67920",
    "end": "74240"
  },
  {
    "text": "with everyone here virtually today um and uh the as the title of the slide says i'm",
    "start": "74240",
    "end": "81280"
  },
  {
    "text": "going to talk about computing with physical systems and uh maybe slightly",
    "start": "81280",
    "end": "87600"
  },
  {
    "text": "uh to give a slight more hint of kind of what this is going to entail is i'm going to talk about how we can try to",
    "start": "87600",
    "end": "94240"
  },
  {
    "text": "harness physical systems that have complex dynamics and use the intrinsic computing",
    "start": "94240",
    "end": "101040"
  },
  {
    "text": "that those systems naturally do to do useful things for us and in particular we're going to",
    "start": "101040",
    "end": "106079"
  },
  {
    "text": "try to train them to act as neural networks before i get ahead of myself though i'm",
    "start": "106079",
    "end": "112720"
  },
  {
    "text": "really just here to do the advertising the the actual work is of course done by the members of my group",
    "start": "112720",
    "end": "118320"
  },
  {
    "text": "uh and i've listed at least most of the members here and uh",
    "start": "118320",
    "end": "123520"
  },
  {
    "text": "the funding for this work primarily comes from ntt and the nsf but",
    "start": "123520",
    "end": "129280"
  },
  {
    "text": "later on in the talk i will highlight a couple of members whose work that i'm really focusing on today",
    "start": "129280",
    "end": "136640"
  },
  {
    "text": "so the the grand plan is first of all i think for this audience i probably don't need to go into great detail about",
    "start": "136640",
    "end": "141760"
  },
  {
    "text": "exactly what neural networks are but i wanted to at least set the nomenclature as that we're all on the same page about",
    "start": "141760",
    "end": "147920"
  },
  {
    "text": "what particular terms mean so i'll quickly go through about what neural networks are",
    "start": "147920",
    "end": "153040"
  },
  {
    "text": "and in particular why and how we might want to improve their energy efficiency for inference",
    "start": "153040",
    "end": "159120"
  },
  {
    "text": "then the meat of the talk is really going to be this middle part here of this discussion of physical neural networks of how we",
    "start": "159120",
    "end": "165200"
  },
  {
    "text": "can build neural networks out of essentially arbitrary physical systems there will be some caveats on the",
    "start": "165200",
    "end": "171360"
  },
  {
    "text": "arbitrary but to first order how you can use arbitrary physical",
    "start": "171360",
    "end": "176560"
  },
  {
    "text": "systems to do neural network computing for us and then once i've shown you how we've",
    "start": "176560",
    "end": "181680"
  },
  {
    "text": "done this and the demonstrations that we've performed so far i'll talk a little bit about future directions that",
    "start": "181680",
    "end": "186959"
  },
  {
    "text": "hopefully some of them appeal to a computer systems audience so maybe one of the main future directions that one",
    "start": "186959",
    "end": "193920"
  },
  {
    "text": "can i can think of building off the narrative that we will set up the the first parts of the talk is how",
    "start": "193920",
    "end": "200000"
  },
  {
    "text": "we can potentially build machine learning accelerators and this is something that i think connects very well with several previous",
    "start": "200000",
    "end": "206400"
  },
  {
    "text": "speakers uh already at least in calendar 2022 i was watching some of the ee 380",
    "start": "206400",
    "end": "212879"
  },
  {
    "text": "youtube recordings last night and saw some nice talks from grok and others on work that's happening with cmos",
    "start": "212879",
    "end": "220159"
  },
  {
    "text": "devices that are machine learning accelerators and i'll talk a little bit about the prospects for trying to build",
    "start": "220159",
    "end": "225599"
  },
  {
    "text": "physical systems that are much more exotic than cmos that might be able to compete i'll also talk about smart sensors which",
    "start": "225599",
    "end": "232480"
  },
  {
    "text": "i think is a perhaps much more near-term direction of how you can do machine learning within a",
    "start": "232480",
    "end": "238239"
  },
  {
    "text": "sensing device uh this is maybe a little abstract but i'll i'll go into some detail that will hopefully make it clear",
    "start": "238239",
    "end": "244640"
  },
  {
    "text": "i'll talk a bit about an extension of this work to a potential extension of this work to quantum systems",
    "start": "244640",
    "end": "251040"
  },
  {
    "text": "and also talk about something a little more general if we figured out how to make neural networks out of arbitrary",
    "start": "251040",
    "end": "256479"
  },
  {
    "text": "physical systems but it turns out that we've essentially figured out a way to make arbitrary physical systems perform",
    "start": "256479",
    "end": "261840"
  },
  {
    "text": "functions for us and this might be useful for other things besides neural networks so i'll also talk about that",
    "start": "261840",
    "end": "269120"
  },
  {
    "text": "so let's uh let's dive in so first of all i wanted to really briefly emphasize the difference between inference and",
    "start": "269120",
    "end": "274800"
  },
  {
    "text": "training in neural networks and the reason for this is that we're going to be focusing on inference in this talk it's not because i don't",
    "start": "274800",
    "end": "280880"
  },
  {
    "text": "believe training is important or because we don't have work trying to improve training but",
    "start": "280880",
    "end": "287280"
  },
  {
    "text": "the focus of the work that i'm going to tell you about today is really on inference so just as a quick reminder",
    "start": "287280",
    "end": "292479"
  },
  {
    "text": "inference is essentially this task of given some trained machine learning model and for example an image of a",
    "start": "292479",
    "end": "297759"
  },
  {
    "text": "handwritten digit uh make a prediction of what that image is you get given this image and it makes a prediction that",
    "start": "297759",
    "end": "303520"
  },
  {
    "text": "that was an eight this is as opposed to training where in training you are given some",
    "start": "303520",
    "end": "309360"
  },
  {
    "text": "labeled data set uh for example we told these are all examples of zeros and these are all",
    "start": "309360",
    "end": "314720"
  },
  {
    "text": "examples of ones etc and given this label data set of handwritten images produce a trained model",
    "start": "314720",
    "end": "320800"
  },
  {
    "text": "and then that trained model is the thing that gets used during inference so we're going to focus on inference and",
    "start": "320800",
    "end": "327199"
  },
  {
    "text": "the reason that this is sensible is that in these cloud settings inference costs can dominate",
    "start": "327199",
    "end": "334000"
  },
  {
    "text": "it could be 80 to 90 percent of the total dollar or energy cost of performing machine learning and",
    "start": "334000",
    "end": "339360"
  },
  {
    "text": "intuitively this is because you can imagine some small engineering team maybe uses a compute cluster of",
    "start": "339360",
    "end": "344960"
  },
  {
    "text": "reasonable size to train some model but then once they finish training it they deploy it to the cloud and it can get used by",
    "start": "344960",
    "end": "351120"
  },
  {
    "text": "tens or even hundreds of millions of people if you think of something like google translate for example",
    "start": "351120",
    "end": "356400"
  },
  {
    "text": "and uh this is not a number i just pulled from nowhere this number of 80 to 90 comes from a number of sources",
    "start": "356400",
    "end": "362639"
  },
  {
    "text": "including amazon and their introduction of their inference targeted",
    "start": "362639",
    "end": "368240"
  },
  {
    "text": "chips a couple of years ago also from nvidia also from mckenzie etc",
    "start": "368240",
    "end": "375039"
  },
  {
    "text": "also just to sort of set the set the nomenclature not because i expect the so this audience",
    "start": "375840",
    "end": "381600"
  },
  {
    "text": "doesn't know about neural networks but just to make sure we're all on the same page with kind of what term what terms i'm using",
    "start": "381600",
    "end": "387360"
  },
  {
    "text": "uh a quick review of neural networks in particular so again if we're trying to do handwritten",
    "start": "387360",
    "end": "393600"
  },
  {
    "text": "handwritten digit inference we're trying to go from this input image to a prediction that this was an eight and",
    "start": "393600",
    "end": "398880"
  },
  {
    "text": "the most canonical straightforward diagram of a neural network doing this would be a multi-layer perceptron that's",
    "start": "398880",
    "end": "404639"
  },
  {
    "text": "depicted here where you have multiple layers in the neural network that are each depicted by the set of circles",
    "start": "404639",
    "end": "410560"
  },
  {
    "text": "which are the neurons and you can think about the information propagating through this neural network",
    "start": "410560",
    "end": "415759"
  },
  {
    "text": "as being realized by matrix vector multiplications where you have a vector representing each layer and then a",
    "start": "415759",
    "end": "422639"
  },
  {
    "text": "matrix vector multiplication causes the information to propagate from one layer to the next and after this",
    "start": "422639",
    "end": "429360"
  },
  {
    "text": "linear operation of matrix vector multiplication you apply element-wise non-linearity at each neuron this is the",
    "start": "429360",
    "end": "435199"
  },
  {
    "text": "most canonical form of a neural network which would be good enough for our purposes",
    "start": "435199",
    "end": "441039"
  },
  {
    "text": "so now there are many different academic and commercial groups trying to build accelerators for",
    "start": "441039",
    "end": "447280"
  },
  {
    "text": "neural networks and accelerators is sometimes a misnomer because often they're not trying to make the inference",
    "start": "447280",
    "end": "453680"
  },
  {
    "text": "task happen faster but they're trying to make it happen more energy efficiently although",
    "start": "453680",
    "end": "459120"
  },
  {
    "text": "sometimes it's also faster but in general the main goal is typically to try and make inference more energy",
    "start": "459120",
    "end": "465280"
  },
  {
    "text": "efficient and there are essentially two philosophies about how you can go about",
    "start": "465280",
    "end": "470960"
  },
  {
    "text": "trying to make it a hardware accelerator for for neural networks you can either go for an exact mathematical equivalence",
    "start": "470960",
    "end": "477280"
  },
  {
    "text": "with the mathematical description of a neural network i just gave or you can go for what i might call an approximate",
    "start": "477280",
    "end": "483840"
  },
  {
    "text": "mathematical equivalence there's an asterisk here that i'll explain shortly so",
    "start": "483840",
    "end": "489120"
  },
  {
    "text": "concretely a little bit more concrete what i really mean by this well again this is this description of the mathematical description of a canonical",
    "start": "489120",
    "end": "495759"
  },
  {
    "text": "multi-teptron where the workhorse here is a matrix vector multiplication and",
    "start": "495759",
    "end": "501759"
  },
  {
    "text": "this even in more complicated neural network architectures typically the matrix vector multiplications or vector",
    "start": "501759",
    "end": "507199"
  },
  {
    "text": "vector dot products are the the most uh compute intensive part of the the neural",
    "start": "507199",
    "end": "512560"
  },
  {
    "text": "network inference so an accelerator that has an exact equivalence the goal is to try construct",
    "start": "512560",
    "end": "518159"
  },
  {
    "text": "a piece of hardware that will exactly implement matrix vector multiplication where you give it a matrix you give it a",
    "start": "518159",
    "end": "524000"
  },
  {
    "text": "vector and it does that and the benefit of building such an accelerator is that",
    "start": "524000",
    "end": "530240"
  },
  {
    "text": "now you can use this hardware as essentially a drop-in replacement for a model that was trained on a cpu or a gpu",
    "start": "530240",
    "end": "536080"
  },
  {
    "text": "or a tpu or any other kind of asic that were for neumann computing device that was",
    "start": "536080",
    "end": "544080"
  },
  {
    "text": "trained under the assumption that you had matrix vector multiplications to propagate your information",
    "start": "544480",
    "end": "549600"
  },
  {
    "text": "the disadvantage is that making a system that performs matrix vector multiplication is",
    "start": "549600",
    "end": "555120"
  },
  {
    "text": "it's a very specific type of operation that you're asking it to do and so a physical system doesn't naturally just",
    "start": "555120",
    "end": "561120"
  },
  {
    "text": "do matrix-vector multiplications you have to really carefully engineer a device to do this and there are people",
    "start": "561120",
    "end": "566320"
  },
  {
    "text": "building all sorts of interesting devices including photonic neural networks and memories to crossbar arrays",
    "start": "566320",
    "end": "572399"
  },
  {
    "text": "as well as kind of more conventional cmos devices but all of them have this con that",
    "start": "572399",
    "end": "578880"
  },
  {
    "text": "because you're insisting that it exactly does a matrix vector multiplication you have to leave some energy",
    "start": "578880",
    "end": "585279"
  },
  {
    "text": "you have to use some extra energy and some extra engineering effort to achieve that and a tall alternative is let's",
    "start": "585279",
    "end": "591600"
  },
  {
    "text": "make some hardware that doesn't exactly do this it does something like this but not exactly",
    "start": "591600",
    "end": "598160"
  },
  {
    "text": "and the big pro of making something that has this sort of approximate equivalence for the mathematical operation shown here is",
    "start": "598160",
    "end": "604560"
  },
  {
    "text": "that now you can make the hardware maybe a little more sloppy and it can be more energy efficient",
    "start": "604560",
    "end": "610800"
  },
  {
    "text": "because you are forcing it to do something that didn't exactly want to do to begin with",
    "start": "610800",
    "end": "616079"
  },
  {
    "text": "the big disadvantage of uh building some accelerator that only has an approximate mathematical equivalence though is that",
    "start": "616079",
    "end": "621680"
  },
  {
    "text": "now you need to retrain your machining learning model every time uh you want to use this hardware because",
    "start": "621680",
    "end": "627760"
  },
  {
    "text": "you can't just take a model that was trained assuming this operation is the thing it does and actually does something different",
    "start": "627760",
    "end": "634480"
  },
  {
    "text": "so there's a trade-off but uh we're interested in well we're my group is interested in both parts of this trade-off but the work i'll tell you",
    "start": "634480",
    "end": "640399"
  },
  {
    "text": "about today is more in the sort of second category here and the catch or the the asterix is",
    "start": "640399",
    "end": "646320"
  },
  {
    "text": "really about i set up this dichotomy between exact and approximate equivalence um",
    "start": "646320",
    "end": "652320"
  },
  {
    "text": "but really it's even more extreme than that we don't have to even go for an approximate equivalence we it turns out",
    "start": "652320",
    "end": "657519"
  },
  {
    "text": "that we can really even just make hardware accelerators that do an operation that's kind of vaguely",
    "start": "657519",
    "end": "662959"
  },
  {
    "text": "inspired by what happens in this canonical form but again i'll i'll dive into",
    "start": "662959",
    "end": "669040"
  },
  {
    "text": "this much more deeply in the next section and show you some concrete examples of this but this is what we're",
    "start": "669040",
    "end": "674800"
  },
  {
    "text": "about we're trying to build hardware accelerators for machine learning or at least one way of framing",
    "start": "674800",
    "end": "680320"
  },
  {
    "text": "our work as or targets of our work is to try build hardware accelerators for machine",
    "start": "680320",
    "end": "685440"
  },
  {
    "text": "learning where we have only an approximate or sort of very vaguely inspired connection with the canonical",
    "start": "685440",
    "end": "691680"
  },
  {
    "text": "mathematics but we still want to try and see if this works so this uh this work we call physical",
    "start": "691680",
    "end": "697680"
  },
  {
    "text": "neural networks because we're going to try to build neural networks out of physical systems and this work was led by two postdocs in my group logan wright",
    "start": "697680",
    "end": "704160"
  },
  {
    "text": "and tatsuhiro onodera and they were ably assisted by phd student martin stein",
    "start": "704160",
    "end": "710560"
  },
  {
    "text": "so again the goal is to try build some hardware that will perform the inference task in neural networks",
    "start": "710720",
    "end": "716880"
  },
  {
    "text": "very energy efficiently and the intuition and the promise of trying",
    "start": "716880",
    "end": "722240"
  },
  {
    "text": "to build neural networks out of physical systems is that many physical systems are very expensive in energy and in time",
    "start": "722240",
    "end": "729120"
  },
  {
    "text": "to try a simulator on a digital computer this is because they exhibit some complex behavior so they're",
    "start": "729120",
    "end": "734320"
  },
  {
    "text": "intrinsically doing a lot of computing and a very canonical example of this would be",
    "start": "734320",
    "end": "739519"
  },
  {
    "text": "a wind tunnel if you have some car or some airplane design",
    "start": "739519",
    "end": "744639"
  },
  {
    "text": "and you want to check how well it works you could do a simulation on a cluster of gpus that takes",
    "start": "744639",
    "end": "751279"
  },
  {
    "text": "minutes or hours or days depending on how high the resolution or accuracy you want is",
    "start": "751279",
    "end": "756399"
  },
  {
    "text": "or you could take that bottle car or airplane design and put it into a wind",
    "start": "756399",
    "end": "761680"
  },
  {
    "text": "tunnel turn the wind on and one second later you will have the full distribution of pressures and velocity velocities",
    "start": "761680",
    "end": "767839"
  },
  {
    "text": "everywhere so there's a strong sense in which a wind tunnel is computing the navy stokes",
    "start": "767839",
    "end": "774079"
  },
  {
    "text": "equations very very energy efficiently far more so than a digital computer could",
    "start": "774079",
    "end": "780639"
  },
  {
    "text": "and so the promise or the thought that we had from this is well can you try to turn the",
    "start": "780639",
    "end": "785839"
  },
  {
    "text": "situation around say well if a wind tunnel computes very is this this very complicated thing very energy",
    "start": "785839",
    "end": "791360"
  },
  {
    "text": "efficiently can we somehow harness that to use and use that physical system to",
    "start": "791360",
    "end": "796560"
  },
  {
    "text": "perform expensive computations for us besides just acting as a wind tunnel it's clear that it can act well as a",
    "start": "796560",
    "end": "802560"
  },
  {
    "text": "wind tunnel but cannot do some other type of computation for us that's essentially the uh",
    "start": "802560",
    "end": "807680"
  },
  {
    "text": "the intuition and this is very much uh kind of copying feynman's intuition or",
    "start": "807680",
    "end": "815760"
  },
  {
    "text": "building on it for uh or adapting it to classical systems where feynman motivated the development of quantum",
    "start": "815760",
    "end": "821279"
  },
  {
    "text": "computers by saying the quantum systems are very difficult to simulate on classical computers so why don't we",
    "start": "821279",
    "end": "826560"
  },
  {
    "text": "build a computer out of quantum systems and then it should be more powerful than what we have available classically",
    "start": "826560",
    "end": "831760"
  },
  {
    "text": "uh and we are making essentially the exact same argument except we're saying well it doesn't even have to be a quantum system it's",
    "start": "831760",
    "end": "837760"
  },
  {
    "text": "difficult to simulate with a digital computer many classical physical systems are also difficult",
    "start": "837760",
    "end": "843680"
  },
  {
    "text": "the big challenge then is how do we take some arbitrary physical system that has some complex dynamics and make it behave",
    "start": "843680",
    "end": "849680"
  },
  {
    "text": "as a neural network when it is some physical system that could be pretty arbitrary like a wind tunnel it doesn't naturally look like a a neural",
    "start": "849680",
    "end": "856480"
  },
  {
    "text": "network so that's really the the challenge we set for ourselves so we're not going to try to",
    "start": "856480",
    "end": "862000"
  },
  {
    "text": "arbitrary computations uh we're not going to try kind of make microsoft excel or whatever go faster we're really",
    "start": "862000",
    "end": "868000"
  },
  {
    "text": "going to specialize in neural networks but still it's less than obvious kind of how one would really go about doing this",
    "start": "868000",
    "end": "875600"
  },
  {
    "text": "so the way that we approach this is the following so here at the bottom is a",
    "start": "875760",
    "end": "881600"
  },
  {
    "text": "diagram that is a slightly different drawing of something that's very standard in machine learning this is a",
    "start": "881600",
    "end": "888800"
  },
  {
    "text": "depiction of a several layer deep neural network for classifying a handwritten nate",
    "start": "888800",
    "end": "895519"
  },
  {
    "text": "and the the diagram of the the neural network is just a little different than we used to because now instead of",
    "start": "895519",
    "end": "901680"
  },
  {
    "text": "drawing circles for neurons at each layer we're just going to have these gray boxes and each gray box represents",
    "start": "901680",
    "end": "908160"
  },
  {
    "text": "a layer in the deep neural network and into each box we have an input of data",
    "start": "908160",
    "end": "915440"
  },
  {
    "text": "and some trainable parameters and in the case of a canonical",
    "start": "915440",
    "end": "920480"
  },
  {
    "text": "multi-layer perceptron this gray box would be a matrix vector multiplication",
    "start": "920480",
    "end": "926000"
  },
  {
    "text": "using the parameters the parameters would be the elements of the weight matrix and then",
    "start": "926000",
    "end": "931839"
  },
  {
    "text": "that would be followed by an element wise non-linearity so multi-layer perceptron in this diagram's way of showing things would just be a matrix",
    "start": "931839",
    "end": "938560"
  },
  {
    "text": "vector multiplication and element-wise non-linearity happening in each of these gray boxes",
    "start": "938560",
    "end": "944480"
  },
  {
    "text": "and then eventually at the output of this network the goal is well if you put in an 8 you wanted to say well that was",
    "start": "944480",
    "end": "949759"
  },
  {
    "text": "an 8 with high probability so the the twist that we",
    "start": "949759",
    "end": "956000"
  },
  {
    "text": "decided to test was nobody said that these gray boxes had to be matrix vector multiplications and",
    "start": "956000",
    "end": "962480"
  },
  {
    "text": "element wise non-linearities that's a it's motivated for reasonably good reason but it's in some sense a",
    "start": "962480",
    "end": "968639"
  },
  {
    "text": "historical accident of how artificial neural networks were developed so that they didn't have to have that",
    "start": "968639",
    "end": "974720"
  },
  {
    "text": "functional form so what happens if we do something a little bit crazy and in the in the gray box",
    "start": "974720",
    "end": "982240"
  },
  {
    "text": "we put a physical system that into the physical system we feed some data and some parameters",
    "start": "982240",
    "end": "989040"
  },
  {
    "text": "and we let the physical system evolve for some amount of time and then we read out the state of the system or some part of the state of the",
    "start": "989040",
    "end": "995440"
  },
  {
    "text": "system and so we can in this way treat the physical system as performing an input output map or an implementation of a",
    "start": "995440",
    "end": "1001839"
  },
  {
    "text": "function it takes these parameters and this input data as inputs and then it after some amount of time",
    "start": "1001839",
    "end": "1008160"
  },
  {
    "text": "produces some set of outputs so in an abstract sense this physical",
    "start": "1008160",
    "end": "1013920"
  },
  {
    "text": "dynamical system can produce a it can perform a function for you that's some just some input output map",
    "start": "1013920",
    "end": "1020000"
  },
  {
    "text": "and we could use that function inside these gray boxes so just for this grey box we can replace",
    "start": "1020000",
    "end": "1026640"
  },
  {
    "text": "it with a physical system that physical system could be anything it could be some optical setup where we",
    "start": "1026640",
    "end": "1032160"
  },
  {
    "text": "have for example a non-linear optical crystal and we shine laser pulses into it and it converts the colors of them it",
    "start": "1032160",
    "end": "1038480"
  },
  {
    "text": "could be some mechanical system where what's depicted here is a piece of metal being shaken on a rod",
    "start": "1038480",
    "end": "1044160"
  },
  {
    "text": "it could be some electronic system but that's not a digital cmos device but some complex analog",
    "start": "1044160",
    "end": "1050960"
  },
  {
    "text": "non-linear dynamical system or many other different kinds of physical system anything in principle is possible",
    "start": "1050960",
    "end": "1059760"
  },
  {
    "text": "so our kind of concept was to replace these block these layers and neural network with physical systems and then",
    "start": "1059760",
    "end": "1066080"
  },
  {
    "text": "figure out a way to train the parameters of the physical systems to see if they could be trained to perform machine",
    "start": "1066080",
    "end": "1072000"
  },
  {
    "text": "learning tasks so the the way we went about training is",
    "start": "1072000",
    "end": "1077679"
  },
  {
    "text": "that we had two key ideas the first is that we really want to be able to use back propagation and perform gradient",
    "start": "1077679",
    "end": "1084240"
  },
  {
    "text": "descent because this has proved to be enormously successful in the conventional deep learning community so",
    "start": "1084240",
    "end": "1089760"
  },
  {
    "text": "we wanted to be able to to leverage that and in order to use back propagation",
    "start": "1089760",
    "end": "1096799"
  },
  {
    "text": "uh the at least a natural way to do this is to have a digital model of your physical",
    "start": "1096799",
    "end": "1103440"
  },
  {
    "text": "system so that you can have a computer compute the gradients with respect to the",
    "start": "1103440",
    "end": "1108799"
  },
  {
    "text": "function so the first key idea is we're going to use a digital model as part of our training",
    "start": "1108799",
    "end": "1115200"
  },
  {
    "text": "and this is why i emphasize at the start of the talk our goal is going to be to try speed up the inference we need to be able to train",
    "start": "1115200",
    "end": "1121600"
  },
  {
    "text": "the system but the training will not necessarily receive any benefit in the way we're currently doing things here because",
    "start": "1121600",
    "end": "1127919"
  },
  {
    "text": "we're going to involve a digital computer then the second key idea",
    "start": "1127919",
    "end": "1132960"
  },
  {
    "text": "is that if we try to train everything on a computer with the digital model",
    "start": "1132960",
    "end": "1138080"
  },
  {
    "text": "we'll find that you need to have an extremely accurate digital model which isn't really realistic for an analog",
    "start": "1138080",
    "end": "1144000"
  },
  {
    "text": "complicated physical system and so we introduce a procedure that's essentially a",
    "start": "1144000",
    "end": "1149600"
  },
  {
    "text": "it's named in homage to quantization aware training uh for those of you from the neural network community that will",
    "start": "1149600",
    "end": "1155440"
  },
  {
    "text": "be familiar uh it's also has come up in various more specialized guises before for",
    "start": "1155440",
    "end": "1161760"
  },
  {
    "text": "example chip in the loop training and so on which we call physics away training where essentially the key idea is that",
    "start": "1161760",
    "end": "1168240"
  },
  {
    "text": "you do the training not just with the digital model you do a combination where you do forward pass and hardware",
    "start": "1168240",
    "end": "1174720"
  },
  {
    "text": "and then you do the backward pass in the digital model and this makes the system",
    "start": "1174720",
    "end": "1179919"
  },
  {
    "text": "robust to noise and imperfections in the model in a way that i'll show now",
    "start": "1179919",
    "end": "1184960"
  },
  {
    "text": "so to give a single layer example of how this works we have",
    "start": "1184960",
    "end": "1190000"
  },
  {
    "text": "some piece of training data from the training set and it's shown in red we have some parameter choices the",
    "start": "1190000",
    "end": "1195200"
  },
  {
    "text": "parameters shown in orange this is fed into a physical system the physical system performs a function on this and",
    "start": "1195200",
    "end": "1200880"
  },
  {
    "text": "it gives some output we can now feed that output into a laptop and your laptop can identify",
    "start": "1200880",
    "end": "1207200"
  },
  {
    "text": "how wrong this output was knows what the target was let's say if we're trying to classify the handwritten digit as an",
    "start": "1207200",
    "end": "1212240"
  },
  {
    "text": "eight it should be high at eight and lower everywhere else and it wasn't quite there so it can say",
    "start": "1212240",
    "end": "1217919"
  },
  {
    "text": "okay well this part was overlapping with eight but it's supposed to be higher and everything else is supposed to be lower",
    "start": "1217919",
    "end": "1223440"
  },
  {
    "text": "so for that you can it can do a subtraction and perform a compute an error vector",
    "start": "1223440",
    "end": "1228640"
  },
  {
    "text": "and then this error vector can be propagated backwards through the differentiable digital model sitting on the laptop",
    "start": "1228640",
    "end": "1234400"
  },
  {
    "text": "and and that and this can be done with standard pi torch automatic differentiation tools",
    "start": "1234400",
    "end": "1241200"
  },
  {
    "text": "and that will produce gradients so it'll tell you how you should move these parameters to try and reduce this error",
    "start": "1241200",
    "end": "1248320"
  },
  {
    "text": "and then you can update the parameters and then go through the loop again send through the physical system it'll hopefully have a smaller error from the",
    "start": "1248320",
    "end": "1255120"
  },
  {
    "text": "target and kind of keep going and going and going and so you trade in the loop in the standard way",
    "start": "1255120",
    "end": "1262320"
  },
  {
    "text": "and it turns out that this both works and also isn't it and also is necessary uh",
    "start": "1262320",
    "end": "1270000"
  },
  {
    "text": "so this uh plot here shows accuracy as a function of the training epoch",
    "start": "1270000",
    "end": "1276799"
  },
  {
    "text": "and this was performed with a an experimental setup that used optics",
    "start": "1276799",
    "end": "1282080"
  },
  {
    "text": "and was training to perform vowel spoken vowel classification so some relatively simple machine learning",
    "start": "1282080",
    "end": "1288240"
  },
  {
    "text": "benchmark task what we see is that in blue as we go through the the training data",
    "start": "1288240",
    "end": "1294240"
  },
  {
    "text": "set and we go through training the system we get better and better accuracy and this is using our procedure that i",
    "start": "1294240",
    "end": "1301520"
  },
  {
    "text": "described here we do forward pass positive physical system and backward pass through the digital model",
    "start": "1301520",
    "end": "1307360"
  },
  {
    "text": "in contrast if you try to use the laptop alone to do the training so you do the",
    "start": "1307360",
    "end": "1313120"
  },
  {
    "text": "forward pass through the digital model and the backward pass through the digital model which we call in silico",
    "start": "1313120",
    "end": "1318400"
  },
  {
    "text": "training then you what you see is that when you run on the actual hardware you get an",
    "start": "1318400",
    "end": "1324240"
  },
  {
    "text": "answer that's no better than random guessing so this showed that this procedure really was necessary",
    "start": "1324240",
    "end": "1330240"
  },
  {
    "text": "so both works and is it's important to do it this way that's because building physical models that exactly capture",
    "start": "1330240",
    "end": "1336960"
  },
  {
    "text": "analog dynamics even for relatively simple systems turns out to be pretty hard",
    "start": "1336960",
    "end": "1342799"
  },
  {
    "text": "so let's now show a few more examples uh with a slightly more complicated task",
    "start": "1343120",
    "end": "1350080"
  },
  {
    "text": "so in this case we're doing we're going to do handwritten digit recognition",
    "start": "1350080",
    "end": "1355840"
  },
  {
    "text": "and the first example that i'm showing here is with a mechanical system that's going to be implemented with three",
    "start": "1355840",
    "end": "1361600"
  },
  {
    "text": "layers and the mechanical system that exists within each of the gray boxes",
    "start": "1361600",
    "end": "1367200"
  },
  {
    "text": "is a piece of metal that's shown here that's this gray square it's a piece of titanium",
    "start": "1367200",
    "end": "1372640"
  },
  {
    "text": "that gets shaken by an audio speaker and so the input to this layer is some",
    "start": "1372640",
    "end": "1379600"
  },
  {
    "text": "time domain voltage or current signal being fed to the speaker and then the output is a recording of a microphone",
    "start": "1379600",
    "end": "1386000"
  },
  {
    "text": "sitting above that metal plate so this physical system",
    "start": "1386000",
    "end": "1392480"
  },
  {
    "text": "i hope it is clear it was not a priori design to perform handwritten digit classification",
    "start": "1392480",
    "end": "1399120"
  },
  {
    "text": "it seemingly has absolutely nothing to do with image classification of any type",
    "start": "1399120",
    "end": "1404240"
  },
  {
    "text": "let alone human handwritten classification it's literally just a piece of metal on an audio speaker",
    "start": "1404240",
    "end": "1411840"
  },
  {
    "text": "and we constructed this three-lane neural network can use the p-80 training procedure to try to",
    "start": "1411840",
    "end": "1419200"
  },
  {
    "text": "convince the system to classify handwritten digits accurately and what's shown at the bottom are the",
    "start": "1419200",
    "end": "1424640"
  },
  {
    "text": "results which is the confusion matrix of what digit did we predict versus what",
    "start": "1424640",
    "end": "1429679"
  },
  {
    "text": "was the correct answer and if this was performing with 100 accuracy you would see every element along the diagonal",
    "start": "1429679",
    "end": "1436480"
  },
  {
    "text": "would be 100 and everything off diagonal would be zero so you see that it doesn't perform perfectly",
    "start": "1436480",
    "end": "1443760"
  },
  {
    "text": "but it's getting many of the digits very well some of them a little bit less well like 73 percent here and overall the",
    "start": "1443760",
    "end": "1449200"
  },
  {
    "text": "accuracy was about 87 so certainly by kind of standard machine learning",
    "start": "1449200",
    "end": "1454480"
  },
  {
    "text": "standards uh even from 20 plus years ago this is a very bad mnist handwritten",
    "start": "1454480",
    "end": "1461440"
  },
  {
    "text": "digit classifier still it was a shaken piece of metal that did",
    "start": "1461440",
    "end": "1466559"
  },
  {
    "text": "this that got 87 accuracy uh so that was this",
    "start": "1466559",
    "end": "1472559"
  },
  {
    "text": "is not intended as a well we're going to replace gpus with shaken pieces of metal uh i'll get to kind of how we how we",
    "start": "1472559",
    "end": "1480000"
  },
  {
    "text": "might think about trying to to leverage this to to compete against standard cmos",
    "start": "1480000",
    "end": "1485279"
  },
  {
    "text": "and neural network accelerators this was meant more as a demonstration that a really obscure piece of physical",
    "start": "1485279",
    "end": "1491279"
  },
  {
    "text": "hardware can actually perform a pretty standard machine learning task",
    "start": "1491279",
    "end": "1497600"
  },
  {
    "text": "we did the same thing with an electronic system but not a cpu or a gpu or even a cpu from 30 or",
    "start": "1497600",
    "end": "1504480"
  },
  {
    "text": "40 or 50 years ago an extremely simple circuit so this circuit is shown here and the breadboard the circuit diagram",
    "start": "1504480",
    "end": "1511840"
  },
  {
    "text": "is here it's an rlc circuit so an rlc oscillator with one transistor added in",
    "start": "1511840",
    "end": "1518159"
  },
  {
    "text": "and the purpose of the added transistor is to provide non-linearity because an rlc circuit is at least an idealized",
    "start": "1518159",
    "end": "1524159"
  },
  {
    "text": "case linear and we know from the standard machine learning literature that non-linearity is extremely important in",
    "start": "1524159",
    "end": "1530799"
  },
  {
    "text": "neural networks but this is a very simple circuit by",
    "start": "1530799",
    "end": "1536080"
  },
  {
    "text": "computer standards and you can see that from the photograph this was constructed in my postdoc logan's apartment during",
    "start": "1536080",
    "end": "1542880"
  },
  {
    "text": "the pandemic lockdown this is uh i don't think he will be offended if i say",
    "start": "1542880",
    "end": "1548159"
  },
  {
    "text": "it looks kind of like a middle schooler's first attempt at a circuit and it was purposefully so we did not carefully engineer the circuit to do",
    "start": "1548159",
    "end": "1555200"
  },
  {
    "text": "machine learning we just built a very simple circuit that was supposed to have",
    "start": "1555200",
    "end": "1560480"
  },
  {
    "text": "relatively complex time dynamics and then we wanted to see if we could use it to perform machine learning and",
    "start": "1560480",
    "end": "1565760"
  },
  {
    "text": "in this case it was again a three layer neural network that was implemented uh this time instead of just one copy of",
    "start": "1565760",
    "end": "1572080"
  },
  {
    "text": "the neural network logan made seven copies of it and then averaged the results",
    "start": "1572080",
    "end": "1577440"
  },
  {
    "text": "um and this uh this circuit has complex enough dynamics this actually worked it's",
    "start": "1577440",
    "end": "1582720"
  },
  {
    "text": "achieved accuracy of a little bit over 90 percent",
    "start": "1582720",
    "end": "1587919"
  },
  {
    "text": "so it's actually surprisingly good considering it's just got one transistor",
    "start": "1587919",
    "end": "1593440"
  },
  {
    "text": "and then a final example is we use the nonlinear optical system where it was a nonlinear optical crystal",
    "start": "1593440",
    "end": "1600480"
  },
  {
    "text": "that we shine pulses of laser light through where the pulses are shortened time so they're broad in frequency and",
    "start": "1600480",
    "end": "1606720"
  },
  {
    "text": "we encoded the parameters in the data in the frequency domain or the wavelength domain of the pulse",
    "start": "1606720",
    "end": "1612880"
  },
  {
    "text": "and then this crystal convert takes uh pairs of photons and you can think about",
    "start": "1612880",
    "end": "1618640"
  },
  {
    "text": "them kind of mushing them together and producing one single photon that is at the sum of the energies of the two input",
    "start": "1618640",
    "end": "1624880"
  },
  {
    "text": "photons and so it performs a relatively complex operation of uh of taking all",
    "start": "1624880",
    "end": "1630400"
  },
  {
    "text": "these different combinations of pairs of photons and summing them together and then producing a new",
    "start": "1630400",
    "end": "1636320"
  },
  {
    "text": "spectrum of color out so we have a spectrum of light going in and a spectrum of light that we can measure as",
    "start": "1636320",
    "end": "1641679"
  },
  {
    "text": "the output and in this case we performed two physical layers with this as well as",
    "start": "1641679",
    "end": "1647520"
  },
  {
    "text": "a digital layer on a laptop and in this case we're able to achieve 97 accuracy on this endless handwritten",
    "start": "1647520",
    "end": "1654320"
  },
  {
    "text": "digit classification task i'm going to pause briefly there so dennis's video is turned on and i'm",
    "start": "1654320",
    "end": "1660080"
  },
  {
    "text": "curious if he has a question no i'm just uh playing",
    "start": "1660080",
    "end": "1665440"
  },
  {
    "text": "very good um incidentally if anyone has any questions",
    "start": "1665440",
    "end": "1670880"
  },
  {
    "text": "at any time please do feel free to interrupt so these three demonstrations are",
    "start": "1670880",
    "end": "1678080"
  },
  {
    "text": "demonstrations of the basic concept that we can take very not carefully designed for machine",
    "start": "1678080",
    "end": "1684399"
  },
  {
    "text": "learning physical systems and convert them into things that actually perform machine learning for us",
    "start": "1684399",
    "end": "1689760"
  },
  {
    "text": "uh how you feed the pictures actually pixels here in that",
    "start": "1689760",
    "end": "1694840"
  },
  {
    "text": "models right yeah so it's a great question so how do we encode the image data into",
    "start": "1694840",
    "end": "1702080"
  },
  {
    "text": "these physical systems and i guess simultaneously i can also answer how do we put the parameters in",
    "start": "1702080",
    "end": "1707840"
  },
  {
    "text": "so the input data is you can think about it as a 2d image",
    "start": "1707840",
    "end": "1713600"
  },
  {
    "text": "it's just you can flatten it and it becomes a 1d vector of numbers",
    "start": "1713600",
    "end": "1718640"
  },
  {
    "text": "and in the case of the optical system we fed the image in",
    "start": "1718640",
    "end": "1724559"
  },
  {
    "text": "conceptually as uh a part of the frequency spectrum the frequency spectrum is well the x-axis is",
    "start": "1724559",
    "end": "1731440"
  },
  {
    "text": "wavelength of frequency so it's a one-dimensional thing and we can just say well part of that spectrum uh is",
    "start": "1731440",
    "end": "1736640"
  },
  {
    "text": "where we're gonna take the flattened eight uh image and encode that into that part",
    "start": "1736640",
    "end": "1742000"
  },
  {
    "text": "of the spectrum uh in the case of the electronics and the mechanics uh both of them were done very similarly is that you have some",
    "start": "1742000",
    "end": "1748720"
  },
  {
    "text": "time varying voltage or current waveform and you can again take your flattened image vector",
    "start": "1748720",
    "end": "1754880"
  },
  {
    "text": "you've made it into a 1d vector and then you can feed it in as a voltage as a function of time",
    "start": "1754880",
    "end": "1761440"
  },
  {
    "text": "so one more question about uh the encoding so instead of flattening just do it just down like pixel by pixel and",
    "start": "1765039",
    "end": "1772159"
  },
  {
    "text": "line by line or is it think uh or is it using something like locality preserving order or something more sophistic like",
    "start": "1772159",
    "end": "1779039"
  },
  {
    "text": "something more more even more complicated we tried a bunch of different things i",
    "start": "1779039",
    "end": "1785520"
  },
  {
    "text": "think in the end um the the way things were done was with patches",
    "start": "1785520",
    "end": "1791360"
  },
  {
    "text": "so there's some notion of locality preserving maybe in your nomenclature if we didn't just",
    "start": "1791360",
    "end": "1796799"
  },
  {
    "text": "go row by row um because from kind of reasonable intuition",
    "start": "1796799",
    "end": "1803440"
  },
  {
    "text": "it's more important to have the sort of things that are spatially nearby within 2d then the stuff that's far apart in",
    "start": "1803440",
    "end": "1808799"
  },
  {
    "text": "one dimension yeah thank you that makes sense i think i missed something basic which",
    "start": "1808799",
    "end": "1815120"
  },
  {
    "text": "is how do you adjust adjust the parameters given the error vector how do you know which parameters to adjust and",
    "start": "1815120",
    "end": "1822320"
  },
  {
    "text": "how sure so the way that this works is through this training procedure where we send",
    "start": "1822320",
    "end": "1829919"
  },
  {
    "text": "some data with some choice of parameters through the physical system it computes you now compute an error vector on a",
    "start": "1829919",
    "end": "1836000"
  },
  {
    "text": "laptop and then this error vector gets sent through a differentiable digital",
    "start": "1836000",
    "end": "1842000"
  },
  {
    "text": "model so we literally use the standard machine learning software pytorch to do this",
    "start": "1842000",
    "end": "1849520"
  },
  {
    "text": "and this uh the pi torch uh different order differentiation tool will compute the",
    "start": "1849520",
    "end": "1856559"
  },
  {
    "text": "gradients for us so we give it a model of the system uh that is differentiable we give it the",
    "start": "1856559",
    "end": "1861760"
  },
  {
    "text": "error vector and it will tell us the gradient and then we can say okay let's take the parameters we had and add the",
    "start": "1861760",
    "end": "1868000"
  },
  {
    "text": "gradient to it so that we it's adjusted and then we feed it back into the system again",
    "start": "1868000",
    "end": "1873519"
  },
  {
    "text": "what do you do in case of multi-layer multiple layers right so this uh this",
    "start": "1873519",
    "end": "1879440"
  },
  {
    "text": "example here you're showing is just a single layer for essentially ease of illustration",
    "start": "1879440",
    "end": "1884640"
  },
  {
    "text": "purposes this naturally generalizes to multiple layers though in",
    "start": "1884640",
    "end": "1889679"
  },
  {
    "text": "much the same way that automatic differentiation training or backprop training",
    "start": "1889679",
    "end": "1896880"
  },
  {
    "text": "extends to multiple layers in the case of conventional artificial neural networks",
    "start": "1896880",
    "end": "1904158"
  },
  {
    "text": "yes thank you great thanks",
    "start": "1904399",
    "end": "1909600"
  },
  {
    "text": "so a natural question one might ask after seeing this is like well we shook a",
    "start": "1911440",
    "end": "1917440"
  },
  {
    "text": "metal plate and it banished we manage to train it to classify 108 digits is why does this work why does",
    "start": "1917440",
    "end": "1925039"
  },
  {
    "text": "using a continuous time dynamical system as the neural network work",
    "start": "1925039",
    "end": "1930720"
  },
  {
    "text": "why does replacing these grey block boxes with physical systems function and essentially i can give some",
    "start": "1930720",
    "end": "1936320"
  },
  {
    "text": "intuition that is inspiration from the so-called neural ode literature which is actually something that was",
    "start": "1936320",
    "end": "1942080"
  },
  {
    "text": "inspirational in the development of this project in the first place so it's not a explanation we came up with after the",
    "start": "1942080",
    "end": "1947279"
  },
  {
    "text": "fact that something that inspired us to try to begin with and there's a very very interesting",
    "start": "1947279",
    "end": "1953679"
  },
  {
    "text": "uh essential state of results at this point about deep deep artificial neural networks",
    "start": "1953679",
    "end": "1960880"
  },
  {
    "text": "where for example you have something like resnet from 2015 where it achieved state-of-the-art",
    "start": "1960880",
    "end": "1966720"
  },
  {
    "text": "performance on imagenet but it did so with a very deep network",
    "start": "1966720",
    "end": "1972480"
  },
  {
    "text": "and a bunch of people got thinking about this one of them was wine and e",
    "start": "1972480",
    "end": "1978080"
  },
  {
    "text": "uh and i think the the thought process was it's kind of interesting that",
    "start": "1978080",
    "end": "1985760"
  },
  {
    "text": "if you take a d a a few layer neural network and you make it deeper it gets better and then you",
    "start": "1985760",
    "end": "1991120"
  },
  {
    "text": "make it deeper it gets better and then you make it even deeper and it gets even better and then there's some catches of you",
    "start": "1991120",
    "end": "1997840"
  },
  {
    "text": "have to put skip connections in to deal with vanishing gradients and so on but to first order the message of deep",
    "start": "1997840",
    "end": "2004159"
  },
  {
    "text": "learning is that deeper is better uh they looked at this",
    "start": "2004159",
    "end": "2009200"
  },
  {
    "text": "and said this is kind of interesting that as you go deeper and you add more and more",
    "start": "2009200",
    "end": "2015039"
  },
  {
    "text": "discreet steps uh that things get better and better and you know what that reminds one of that",
    "start": "2015039",
    "end": "2020240"
  },
  {
    "text": "reminds one of uh dynamical systems and numerical solving of odes or pdes where if you",
    "start": "2020240",
    "end": "2027200"
  },
  {
    "text": "discretize with some big time step and you only have a few time steps you don't get a very accurate answer but if you",
    "start": "2027200",
    "end": "2033200"
  },
  {
    "text": "make the time step smaller and smaller and smaller you get more and more accurate answers",
    "start": "2033200",
    "end": "2038320"
  },
  {
    "text": "and the wine and e and others manage to show that actually resnet you can model this",
    "start": "2038320",
    "end": "2044720"
  },
  {
    "text": "as a discretization of a continuous time dynamical system",
    "start": "2044720",
    "end": "2050638"
  },
  {
    "text": "so there's a sense in which resonate is really a discretization of something that's underlying is actually continuous",
    "start": "2050639",
    "end": "2058240"
  },
  {
    "text": "and so this was very inspirational to us because we said well if if that's true then why instead of making a digital",
    "start": "2058240",
    "end": "2063679"
  },
  {
    "text": "system that discretizes it discretizes everything in time why don't you build a physical system that is continuous time",
    "start": "2063679",
    "end": "2069679"
  },
  {
    "text": "in the first place um and we didn't end up doing exactly that so this connection",
    "start": "2069679",
    "end": "2076000"
  },
  {
    "text": "between discretization of dynamical systems being what",
    "start": "2076000",
    "end": "2081919"
  },
  {
    "text": "some artificial neural networks at least like resonate are and the actual dynamical systems we use is just",
    "start": "2081919",
    "end": "2087280"
  },
  {
    "text": "intuition or inspiration because we didn't actually make a dynamical system that actually had",
    "start": "2087280",
    "end": "2092878"
  },
  {
    "text": "exactly this ode but it shows an example at least of why",
    "start": "2092879",
    "end": "2098400"
  },
  {
    "text": "a continuous time dynamical system or a way in which it can actually be in some sense the same as a",
    "start": "2098400",
    "end": "2104960"
  },
  {
    "text": "as a conventional state-of-the-art deep neural network so i think this is a really",
    "start": "2104960",
    "end": "2112160"
  },
  {
    "text": "nice connection that uh that has both motivated us but it's also nice as a make as some intuition for why this is",
    "start": "2112160",
    "end": "2119359"
  },
  {
    "text": "even not a totally crazy thing to have have had happened that we can do it",
    "start": "2119359",
    "end": "2125680"
  },
  {
    "text": "endlessly these three totally different dynamical continuous time dynamical systems and it actually works",
    "start": "2125680",
    "end": "2133200"
  },
  {
    "text": "so another natural thing to wonder is okay these three physical systems each of them were pretty rudimentary the optics won less",
    "start": "2133680",
    "end": "2140240"
  },
  {
    "text": "so than the mechanics and the electronics but what else might you try to do this with and so we're very",
    "start": "2140240",
    "end": "2146079"
  },
  {
    "text": "interested in figuring out what are good physical systems to do this with um and you there are many people working on",
    "start": "2146079",
    "end": "2151760"
  },
  {
    "text": "neuromorphic computing with spintronic nano oscillators for example and i think this could be a very interesting",
    "start": "2151760",
    "end": "2156960"
  },
  {
    "text": "platform to try this with um similarly there are many people thinking about superconducting circuits in the",
    "start": "2156960",
    "end": "2163040"
  },
  {
    "text": "classical domain and using those for neuromorphic computing and we can imagine trying to",
    "start": "2163040",
    "end": "2168560"
  },
  {
    "text": "build physical neural networks out of these you could also imagine somewhat more exotic things if you take some material system for example some 2d",
    "start": "2168560",
    "end": "2175359"
  },
  {
    "text": "material heterostructures and you have you can inject photons into them and make or you",
    "start": "2175359",
    "end": "2181520"
  },
  {
    "text": "can shine photons on them and generate excitons and these exotics can have some complex dynamics and you can imagine",
    "start": "2181520",
    "end": "2187599"
  },
  {
    "text": "trying to harness these complex dynamics in a in a quantum material to to perform machine learning for you and",
    "start": "2187599",
    "end": "2194079"
  },
  {
    "text": "it could really be any physical system so i think when i give this talk to applied physics",
    "start": "2194079",
    "end": "2199200"
  },
  {
    "text": "so physics audiences i really emphasize if you have a physical system that's hard to simulate on your laptop and you",
    "start": "2199200",
    "end": "2204880"
  },
  {
    "text": "would like to try turning it into a neural network feel free to give it a try we put our code on github or you're welcome to contact us",
    "start": "2204880",
    "end": "2211280"
  },
  {
    "text": "so if it happens to be anybody who in the audience today who's a physicist or applied physicist who works with some",
    "start": "2211280",
    "end": "2217599"
  },
  {
    "text": "hard to simulate physical system and you want to give this a go please feel free to reach out but also you can also just try it with the",
    "start": "2217599",
    "end": "2224160"
  },
  {
    "text": "open source code we put out so that is a",
    "start": "2224160",
    "end": "2230800"
  },
  {
    "text": "a kind of natural seg into well what are some future directions besides well let's try it with other different physical systems and as physicists we",
    "start": "2230800",
    "end": "2237599"
  },
  {
    "text": "can certainly have fun with that but what are some application directions for this so now i'm going to talk about a few in",
    "start": "2237599",
    "end": "2243119"
  },
  {
    "text": "the last sort of ten or so minutes of uh what are the ways or at least what",
    "start": "2243119",
    "end": "2249280"
  },
  {
    "text": "are the one way we can imagine building uh large-scale optical accelerators or accelerators using optics that might",
    "start": "2249280",
    "end": "2255280"
  },
  {
    "text": "eventually be competitors to cpus or gpus or tpus as neural network inference",
    "start": "2255280",
    "end": "2260560"
  },
  {
    "text": "engines i'll describe smart sensors which is a catch-all phrase for devices where",
    "start": "2260560",
    "end": "2267599"
  },
  {
    "text": "you're trying to sense something about the environment and you want to you do some machine learning within the hardware itself before you send it off",
    "start": "2267599",
    "end": "2274000"
  },
  {
    "text": "to a computer i think it's a very interesting extension of this work to quantum systems",
    "start": "2274000",
    "end": "2280400"
  },
  {
    "text": "and then there's something that's really not connected with machine learning at all but that we can try to leverage our",
    "start": "2280400",
    "end": "2286079"
  },
  {
    "text": "physics aware training procedure to let physical devices be trained to perform new functions",
    "start": "2286079",
    "end": "2292480"
  },
  {
    "text": "let's start off with the photonic neural networks or optical neural networks and uh there's a i'm personally very",
    "start": "2292480",
    "end": "2298960"
  },
  {
    "text": "interested in the prospect for optics to be used to construct machines that can outperform cmos electronics",
    "start": "2298960",
    "end": "2306720"
  },
  {
    "text": "at neural network inference i think it's an extremely difficult goal because cmos has had so many years of development and",
    "start": "2306720",
    "end": "2312480"
  },
  {
    "text": "so much money put into the development but from a physics basis there is a reason to believe that one could achieve",
    "start": "2312480",
    "end": "2317760"
  },
  {
    "text": "this and i think it's a very interesting research topic so in my own lab we've been working on",
    "start": "2317760",
    "end": "2324400"
  },
  {
    "text": "optical neural networks where we are working in the spatial domain so we encode vectors into beams of light that",
    "start": "2324400",
    "end": "2330640"
  },
  {
    "text": "we can fan out and make copies of we then uh perform element-wise multiplications using",
    "start": "2330640",
    "end": "2336720"
  },
  {
    "text": "spatial light modulators and then uh can perform",
    "start": "2336720",
    "end": "2342079"
  },
  {
    "text": "essentially perform vector vector dot products where uh the the sum and the vector vector dot product happens by",
    "start": "2342079",
    "end": "2347839"
  },
  {
    "text": "just multiple beams of light being collapsed onto a single detector and this is just one example of a",
    "start": "2347839",
    "end": "2353839"
  },
  {
    "text": "photonic neural network there are many other approaches people are uh are pursuing at the moment including both",
    "start": "2353839",
    "end": "2359920"
  },
  {
    "text": "commercially and in academia uh but in our case one of the things that we've recently worked on that i",
    "start": "2359920",
    "end": "2365920"
  },
  {
    "text": "think is really suggestive of the potential for optical neural networks is that we managed to show mnist",
    "start": "2365920",
    "end": "2370960"
  },
  {
    "text": "handwritten digital classification using less than a single photon per scalar multiplication",
    "start": "2370960",
    "end": "2377280"
  },
  {
    "text": "so that's this red highlighted box here is really the highlight of this figure you're showing the accuracy of imminent",
    "start": "2377280",
    "end": "2383440"
  },
  {
    "text": "digit classification as a function of how many photons were used in the processor",
    "start": "2383440",
    "end": "2388640"
  },
  {
    "text": "and at first glance this might seem a little crazy if how can you perform a",
    "start": "2388640",
    "end": "2393920"
  },
  {
    "text": "multiplication using less than a photon how do you even encode the information less than a photon and essentially the",
    "start": "2393920",
    "end": "2400000"
  },
  {
    "text": "the trick is that you're performing vector vector of products where you're summing over many scalar multiplications and so you have a",
    "start": "2400000",
    "end": "2406880"
  },
  {
    "text": "lot of noise when you use this few photons per multiplication but when you sum over many of them the",
    "start": "2406880",
    "end": "2412240"
  },
  {
    "text": "noise gets averaged down and so overall this ends up working",
    "start": "2412240",
    "end": "2418000"
  },
  {
    "text": "and if we look towards the future we and others have been doing projections of uh",
    "start": "2418240",
    "end": "2424000"
  },
  {
    "text": "how big would you have to make an optical neural network to actually out compete a gpu or a tpu or some other",
    "start": "2424000",
    "end": "2430640"
  },
  {
    "text": "cmos device and it turns out that in the case of for example transformer models",
    "start": "2430640",
    "end": "2435920"
  },
  {
    "text": "for language natural language processing there is a crossover point and",
    "start": "2435920",
    "end": "2442800"
  },
  {
    "text": "there's certainly sort of reasonable size models where you can imagine getting several",
    "start": "2442800",
    "end": "2447920"
  },
  {
    "text": "orders of magnitude energy efficiency improvements even when you bake in the costs of not just the",
    "start": "2447920",
    "end": "2454240"
  },
  {
    "text": "photons but all the electronic infrastructure around the optical neural network itself like you have detectors",
    "start": "2454240",
    "end": "2460160"
  },
  {
    "text": "that are electronic they have to have adcs and there's dax and so on but even when you account for all of",
    "start": "2460160",
    "end": "2465680"
  },
  {
    "text": "that if you make the system big enough uh you can really uh foresee a future where you get an",
    "start": "2465680",
    "end": "2472480"
  },
  {
    "text": "energy advantage and the reason for this is maybe a little bit glibly that",
    "start": "2472480",
    "end": "2478079"
  },
  {
    "text": "optical neural networks have the potential to do n squared operations uh in optics where you only have to pay an",
    "start": "2478079",
    "end": "2484720"
  },
  {
    "text": "order in energy cost to get the data in and out and so if you make you have a lot of constant factors but",
    "start": "2484720",
    "end": "2490960"
  },
  {
    "text": "if you make things big enough the n the n squared in electronics ends up dominating and the n and optics-based",
    "start": "2490960",
    "end": "2497839"
  },
  {
    "text": "systems can uh can end up if even in kind of absolute terms end up winning in energy efficiency so i think this is a",
    "start": "2497839",
    "end": "2504640"
  },
  {
    "text": "very interesting direction um and uh there's a lot of people working on this and this uh general approach of",
    "start": "2504640",
    "end": "2511680"
  },
  {
    "text": "trying to take a complex optical system and and get it to do machine learning for you i think is",
    "start": "2511680",
    "end": "2516880"
  },
  {
    "text": "is going to be something that we we in my group play with a lot over the next couple of years",
    "start": "2516880",
    "end": "2522720"
  },
  {
    "text": "another direction that is somewhat related the previous slide but",
    "start": "2522720",
    "end": "2527839"
  },
  {
    "text": "is more general is the notion of smart senses and so an example of this is in",
    "start": "2527839",
    "end": "2532880"
  },
  {
    "text": "it's inspired by kind of human systems is that if you",
    "start": "2532880",
    "end": "2538079"
  },
  {
    "text": "think about the optical bandwidth going into like the data rate going into the human eye",
    "start": "2538079",
    "end": "2544160"
  },
  {
    "text": "at least i didn't calculate this somebody calculated that this is apparently about 10 megabits per second of data um but the human brain is",
    "start": "2544160",
    "end": "2552160"
  },
  {
    "text": "actually only processing at about 100 bits per second so what's what's going on well it turns",
    "start": "2552160",
    "end": "2558160"
  },
  {
    "text": "out that there's some very serious kind of compression stage that's going on at the beginning before you get to the",
    "start": "2558160",
    "end": "2564000"
  },
  {
    "text": "neural network stage if you will [Music] and this has inspired",
    "start": "2564000",
    "end": "2570880"
  },
  {
    "text": "a thought from our side which is if you have some physical system that you can train",
    "start": "2570880",
    "end": "2576960"
  },
  {
    "text": "you can send some physical object or signal into this physical system and train that physical system to perform",
    "start": "2576960",
    "end": "2583040"
  },
  {
    "text": "some transformation for you that effectively does this kind of compression from 8.75 megabits per",
    "start": "2583040",
    "end": "2588560"
  },
  {
    "text": "second down to 100 bits per second and then you perform some transduction and adc and digital processing afterwards",
    "start": "2588560",
    "end": "2595440"
  },
  {
    "text": "so you can use a physical system to basically do compression for you in some sense but not a general purpose",
    "start": "2595440",
    "end": "2601760"
  },
  {
    "text": "compression like a gzip but neural network compression dedicated specifically to the task you're trying",
    "start": "2601760",
    "end": "2608160"
  },
  {
    "text": "to do and so this physical object or data could be photons like in the case of human eye but this could also be",
    "start": "2608160",
    "end": "2615680"
  },
  {
    "text": "fluid flow it could be literal molecules it could be acoustic waves it could be mechanical forces and this processing of",
    "start": "2615680",
    "end": "2621520"
  },
  {
    "text": "this physical system we're imagining that this is in the same domain so if we're talking about photons going in",
    "start": "2621520",
    "end": "2626560"
  },
  {
    "text": "then this physical system should be some optical system if it's some fluid flow then this might be some literally some",
    "start": "2626560",
    "end": "2631839"
  },
  {
    "text": "fluid liquid system if it's acoustic waves this could be something like our mechanical plate on a speaker again",
    "start": "2631839",
    "end": "2638960"
  },
  {
    "text": "and as a concrete example of this we've recently been working on doing real scene imaging where we have",
    "start": "2638960",
    "end": "2645440"
  },
  {
    "text": "some 3d printed car scene with stop signs and so on where we send this through a multiple",
    "start": "2645440",
    "end": "2650800"
  },
  {
    "text": "layers of optical transformations that we then train where the goal is to try get the whole",
    "start": "2650800",
    "end": "2658480"
  },
  {
    "text": "visual scene down into just a few pixels on a camera and you've really forced a strong",
    "start": "2658480",
    "end": "2665359"
  },
  {
    "text": "bottleneck there and the big benefit of this in principle is that you can now have the camera go",
    "start": "2665359",
    "end": "2671760"
  },
  {
    "text": "at much much higher frame rates than where possible otherwise because if you insisted on taking a 10 megapixel image",
    "start": "2671760",
    "end": "2678079"
  },
  {
    "text": "uh you you're going to be stuck to some number of tens of hertz frame rates",
    "start": "2678079",
    "end": "2683599"
  },
  {
    "text": "uh maybe a bit higher but you're not going to get be able to get the gigahertz but if you are able to perform optical",
    "start": "2683599",
    "end": "2690000"
  },
  {
    "text": "pre-processing to get to a point where you now have only uh let's say four pixels here now you can run these four",
    "start": "2690000",
    "end": "2695359"
  },
  {
    "text": "pixels at a gigahertz uh so this is i think it's a a very nice example of how kind of doing processing",
    "start": "2695359",
    "end": "2702079"
  },
  {
    "text": "in the physical domain of the data that you you got in the first place could actually give you a practical benefit",
    "start": "2702079",
    "end": "2708560"
  },
  {
    "text": "and it's something that's not really comparable to a gpu or a tpu it couldn't have done this because you can't just put a gpu in",
    "start": "2708560",
    "end": "2715359"
  },
  {
    "text": "here before you have your camera the gpu can only come afterwards",
    "start": "2715359",
    "end": "2721280"
  },
  {
    "text": "so another uh future direction that i think is really interesting and ties a lot with my background of in in quantum computing",
    "start": "2721280",
    "end": "2728480"
  },
  {
    "text": "is you know quantum physical neural networks so there are many people interested in quantum neural networks and you can see this goes back to at",
    "start": "2728480",
    "end": "2735280"
  },
  {
    "text": "least the early 90s of people proposing quantum versions of neural networks but there's been a renaissance of this over",
    "start": "2735280",
    "end": "2740800"
  },
  {
    "text": "the last five or so years this is just paper from 2018 is just one of many",
    "start": "2740800",
    "end": "2745839"
  },
  {
    "text": "examples of neural network papers that people have proposed on a quantum computer running a series of quantum",
    "start": "2745839",
    "end": "2751119"
  },
  {
    "text": "gates where the gates are parameterized by some angles for example and these are",
    "start": "2751119",
    "end": "2756160"
  },
  {
    "text": "the tunable parameters of your neural network and you fiddle with the angles and the quantum circuits such that you",
    "start": "2756160",
    "end": "2761200"
  },
  {
    "text": "train the circuit to give an output that it says an eight when you put in an eight an image of an eight and so on",
    "start": "2761200",
    "end": "2767760"
  },
  {
    "text": "uh and this has been generalized to uh sort of quantum circuit learning where you can imagine generic circuits",
    "start": "2767760",
    "end": "2775280"
  },
  {
    "text": "of quantum circuit where you have some input encoding and then some",
    "start": "2775280",
    "end": "2780400"
  },
  {
    "text": "programmable unitary that has programmable parameters that you train to to perform whatever task you want",
    "start": "2780400",
    "end": "2787440"
  },
  {
    "text": "and the the kind of key thing of all these things that have come up in the last or almost all of the literature in the space that's come up over the last",
    "start": "2787440",
    "end": "2793440"
  },
  {
    "text": "few years people want to use to do this on standard circuit model quantum computers like google and",
    "start": "2793440",
    "end": "2799920"
  },
  {
    "text": "ibm and others are building but our thought is to follow is what if we adapt",
    "start": "2799920",
    "end": "2805359"
  },
  {
    "text": "our procedure that uh that i showed earlier for classical systems and instead now of a physical",
    "start": "2805359",
    "end": "2811200"
  },
  {
    "text": "system that is some very weird classical system like a shaken piece of metal what if we now instead of",
    "start": "2811200",
    "end": "2818240"
  },
  {
    "text": "putting a instead of using a quantum computer we take some quantum system that is not well calibrated it's",
    "start": "2818240",
    "end": "2823760"
  },
  {
    "text": "dissipative potentially um it's definitely not on its own a quantum computer but it's still a system",
    "start": "2823760",
    "end": "2829599"
  },
  {
    "text": "that behaves according to the laws of quantum mechanics and is not trivially classical can we somehow turn that into a useful",
    "start": "2829599",
    "end": "2835920"
  },
  {
    "text": "quantum neural network and i think if we use this procedure we should be able to and in this case well what would the",
    "start": "2835920",
    "end": "2841440"
  },
  {
    "text": "differentiable digital model be how would we do that well if the system's big enough you'll probably need to actually use a standard",
    "start": "2841440",
    "end": "2847359"
  },
  {
    "text": "circuit model quantum computer to do this um so we think this is an interesting direction as well can we turn badly",
    "start": "2847359",
    "end": "2854800"
  },
  {
    "text": "designed quantum systems into useful neural networks potentially using standard quantum computers for doing the",
    "start": "2854800",
    "end": "2860319"
  },
  {
    "text": "training and then a final direction that i wanted to mention was",
    "start": "2860319",
    "end": "2865839"
  },
  {
    "text": "beyond machine learning what could this be good for in some strong sense physics aware training is letting us",
    "start": "2865839",
    "end": "2871760"
  },
  {
    "text": "train a physical piece of hardware to perform a particular function and the function we're choosing is essentially",
    "start": "2871760",
    "end": "2878480"
  },
  {
    "text": "the the feed-forward function of a neural network but it doesn't have to be that so",
    "start": "2878480",
    "end": "2883920"
  },
  {
    "text": "as an example imagine we want to perform we want to make a filter and we want to",
    "start": "2883920",
    "end": "2889280"
  },
  {
    "text": "perform some filter function and the conventional approach to doing",
    "start": "2889280",
    "end": "2895599"
  },
  {
    "text": "this would be you define your filter and you say well this is this is the cutoff frequency i want and this is the 3db bandwidth and so on this",
    "start": "2895599",
    "end": "2903200"
  },
  {
    "text": "is the center frequency and a circuit designer goes off and uses",
    "start": "2903200",
    "end": "2908640"
  },
  {
    "text": "the known building blocks and filter design theory and simulations and so on to design some cmos circuit that will do",
    "start": "2908640",
    "end": "2914319"
  },
  {
    "text": "this but using the physical neural networks and physics aware training approach you",
    "start": "2914319",
    "end": "2920720"
  },
  {
    "text": "can imagine doing this in a completely different way which is that you can take some material or device that has tunable",
    "start": "2920720",
    "end": "2926160"
  },
  {
    "text": "parameters and you can train those parameters such that the material or device performs this input output map",
    "start": "2926160",
    "end": "2932160"
  },
  {
    "text": "that you you wanted from whatever you designed which could have been a filter and so what are some materials or",
    "start": "2932160",
    "end": "2938960"
  },
  {
    "text": "devices you might do this with one of the things that we're actually playing with is networks of oscillators you if you",
    "start": "2938960",
    "end": "2944640"
  },
  {
    "text": "imagine having a bunch of oscillators that are coupled in some tunable way uh and these are non-linear oscillators",
    "start": "2944640",
    "end": "2950559"
  },
  {
    "text": "and you can you can program how strongly they're coupled to one another you can imagine tweaking these parameters such that",
    "start": "2950559",
    "end": "2957119"
  },
  {
    "text": "given whatever rf input signal you put into one of the oscillators you can make the rf output signal",
    "start": "2957119",
    "end": "2963280"
  },
  {
    "text": "maybe not anything you want but some large set of possible functions can be applied to it",
    "start": "2963280",
    "end": "2968960"
  },
  {
    "text": "and this could also be some some more exotic thing like a quantum material that you could maybe",
    "start": "2968960",
    "end": "2975280"
  },
  {
    "text": "have source and gate and drain voltages on and that you could fiddle fiddle with the parameters of voltages",
    "start": "2975280",
    "end": "2981359"
  },
  {
    "text": "applied to the material and see what input output relations is able to represent",
    "start": "2981359",
    "end": "2987760"
  },
  {
    "text": "so with that i will put up a summary so basically i've shown how we can use",
    "start": "2988960",
    "end": "2995400"
  },
  {
    "text": "a training procedure that we've designed physics away training to harness the computation the physical systems",
    "start": "2995400",
    "end": "3001839"
  },
  {
    "text": "naturally do to perform machine learning for us and we've demonstrated this procedure with three different types of",
    "start": "3001839",
    "end": "3007280"
  },
  {
    "text": "system a mechanical one an electrical one and an optical one and we've shown it on a relatively",
    "start": "3007280",
    "end": "3012640"
  },
  {
    "text": "simple task of amnesty classification but hopefully that proof of principle is inspirational about what else we could",
    "start": "3012640",
    "end": "3018720"
  },
  {
    "text": "do with this and i think there are many interesting future directions to take this line of work including trying to figure out how",
    "start": "3018720",
    "end": "3025440"
  },
  {
    "text": "to scale to large large scale devices that could potentially actually beat cmos processors but as well as things",
    "start": "3025440",
    "end": "3032079"
  },
  {
    "text": "where we're not directly competing with gpus for example in smart sensors or building quantum physical neural",
    "start": "3032079",
    "end": "3037359"
  },
  {
    "text": "networks out of systems that are not well designed quantum computers as well as trying to endow",
    "start": "3037359",
    "end": "3043119"
  },
  {
    "text": "uh materials and devices with new functionality that they didn't otherwise have",
    "start": "3043119",
    "end": "3048559"
  },
  {
    "text": "uh so with that i'll leave it there and i'm very happy to to stick around with questions but thank you very much for",
    "start": "3048559",
    "end": "3054960"
  },
  {
    "text": "your attention and for the question so far and again the invitation to speak",
    "start": "3054960",
    "end": "3060558"
  },
  {
    "text": "uh did you try to use an optical device with some kind of controllable shadow",
    "start": "3062640",
    "end": "3070240"
  },
  {
    "text": "on the picture the reason for that because i'm surprised your first experiments about",
    "start": "3070240",
    "end": "3076880"
  },
  {
    "text": "when you feed the picture through linear row it get some uh positive results with",
    "start": "3076880",
    "end": "3083920"
  },
  {
    "text": "very good accuracy uh usually uh neural networks actually has",
    "start": "3083920",
    "end": "3089359"
  },
  {
    "text": "a need to have a close relationship adjusting pixels to make a good recognition because when",
    "start": "3089359",
    "end": "3097200"
  },
  {
    "text": "pixels are located far away it's where influence through system need to you",
    "start": "3097200",
    "end": "3102800"
  },
  {
    "text": "need to have many many layers to get that influence right so in case of optical it's much easier",
    "start": "3102800",
    "end": "3110160"
  },
  {
    "text": "because you have a regular optic or regular picture and you may try to make a matrix of",
    "start": "3110160",
    "end": "3119599"
  },
  {
    "text": "controllable shadow as a as a vectors for collection",
    "start": "3119599",
    "end": "3126160"
  },
  {
    "text": "for training right so in the examples that i showed",
    "start": "3126160",
    "end": "3132640"
  },
  {
    "text": "over here i think in the end for each of these",
    "start": "3132640",
    "end": "3138640"
  },
  {
    "text": "ones we ended up doing things with patches because like you said if you just",
    "start": "3138640",
    "end": "3144000"
  },
  {
    "text": "flatten the neural network into 1d um [Music] it will work but it won't work very well",
    "start": "3144000",
    "end": "3150319"
  },
  {
    "text": "i think to get the results that we showed here we did the slightly more sophisticated thing of of sort of flattening into patches so that you",
    "start": "3150319",
    "end": "3156720"
  },
  {
    "text": "preserve some of the locality um in the case and in in all three examples",
    "start": "3156720",
    "end": "3162319"
  },
  {
    "text": "including the optical case here we were going from 2d into 1d because the encoding in the optics case was in the",
    "start": "3162319",
    "end": "3168640"
  },
  {
    "text": "frequency domain which only has one dimension but in the case of this other work that",
    "start": "3168640",
    "end": "3174640"
  },
  {
    "text": "we've done on optical neural networks you literally do feed just the image into the system",
    "start": "3174640",
    "end": "3180800"
  },
  {
    "text": "and it makes multiple copies of it and then you could perform these uh the weight multiplications in element element wise",
    "start": "3180800",
    "end": "3187040"
  },
  {
    "text": "fashion but here we're not flattening into 1d and this is indeed a place where you could imagine putting a mask in",
    "start": "3187040",
    "end": "3193599"
  },
  {
    "text": "front of the image over here for example to essentially i guess it identifies features in some sense",
    "start": "3193599",
    "end": "3200000"
  },
  {
    "text": "um that's not something we've done but uh i agree it's something that a principal one could do",
    "start": "3200000",
    "end": "3205280"
  },
  {
    "text": "what we've done is uh is do this in multiple layers we achieved these results this was with",
    "start": "3205280",
    "end": "3210720"
  },
  {
    "text": "mnist we did three layers of the neural network and it was possible to achieve reasonable accuracy um with three layers",
    "start": "3210720",
    "end": "3217760"
  },
  {
    "text": "so we didn't need to play any extra tricks but indeed we did have to have multiple layers to get it to work because with just one layer it wouldn't",
    "start": "3217760",
    "end": "3224800"
  },
  {
    "text": "it wouldn't be able to achieve this accuracy at least you probably need to also to think about",
    "start": "3224800",
    "end": "3229920"
  },
  {
    "text": "loops in regular neural networks models right now it's very popular to have",
    "start": "3229920",
    "end": "3236480"
  },
  {
    "text": "uh some uh bypass of information and that's a reason to get a influence",
    "start": "3236480",
    "end": "3243359"
  },
  {
    "text": "uh mixing mixing information from first layer to",
    "start": "3243359",
    "end": "3248640"
  },
  {
    "text": "later layers that's right these skip connections are",
    "start": "3248640",
    "end": "3254400"
  },
  {
    "text": "very common in a number of different architectures so i guess the two things i can say about that one is that",
    "start": "3254400",
    "end": "3260000"
  },
  {
    "text": "there's some sense in which the continuous time dynamical systems are intrinsically doing this this is this really nice proof from wine and e and",
    "start": "3260000",
    "end": "3266559"
  },
  {
    "text": "others uh showing that a continuous time system at least some particular continuous time system you can really",
    "start": "3266559",
    "end": "3272960"
  },
  {
    "text": "think about it as doing this this discrete neural network that has the skips um that's maybe a little bit too",
    "start": "3272960",
    "end": "3279200"
  },
  {
    "text": "glib though i think we have we haven't in this work demonstrated ones where we put",
    "start": "3279200",
    "end": "3285440"
  },
  {
    "text": "skip connections in but in principle we could um there's nothing stopping us from from programming and skip connections either",
    "start": "3285440",
    "end": "3291839"
  },
  {
    "text": "physically or in a laptop the other thing i could say is that we'd be very interested in transformers",
    "start": "3291839",
    "end": "3297680"
  },
  {
    "text": "and the transformer models are um where is it over here ones where there's less of the skip",
    "start": "3297680",
    "end": "3304240"
  },
  {
    "text": "connection business which is maybe a little bit more natural for optics where you really want to",
    "start": "3304240",
    "end": "3309920"
  },
  {
    "text": "at least if you're trying to do everything as up with as much optics and as little electronics as possible it's it's helpful to avoid the skips but uh",
    "start": "3309920",
    "end": "3318559"
  },
  {
    "text": "it's it's still possible to do it as long as if you're going into electronics afterwards uh before the next layer you",
    "start": "3318559",
    "end": "3324240"
  },
  {
    "text": "can store things in a little bit of electronic memory and and be able to kind of yeah",
    "start": "3324240",
    "end": "3329599"
  },
  {
    "text": "introduce the skip in the in the laptop thank you",
    "start": "3329599",
    "end": "3335838"
  },
  {
    "text": "yeah uh so uh one more question about the e-coding i'm very interested in that sorry yeah uh",
    "start": "3337040",
    "end": "3344480"
  },
  {
    "text": "so uh how many different just curious how many different kind of encodings just right because i wonder if",
    "start": "3344480",
    "end": "3351520"
  },
  {
    "text": "you have ever tried a space building curve like a fractal and i i wonder if the if that would do anything to",
    "start": "3351520",
    "end": "3358960"
  },
  {
    "text": "to the performance that's an interesting question i don't",
    "start": "3358960",
    "end": "3364000"
  },
  {
    "text": "think we've tried any any fractal one it's an interesting thought though",
    "start": "3364000",
    "end": "3369040"
  },
  {
    "text": "uh yeah it it might be better we should give it a go yeah because it sounds like a fractal preservative or locality kind",
    "start": "3369040",
    "end": "3375920"
  },
  {
    "text": "of like different different scales right because you have that curious furious",
    "start": "3375920",
    "end": "3381440"
  },
  {
    "text": "characteristic right yeah",
    "start": "3381440",
    "end": "3387040"
  },
  {
    "text": "okay yeah so that's an interesting suggestion i i i don't really have much to say other than we should probably give it a try",
    "start": "3387040",
    "end": "3393680"
  },
  {
    "text": "because i it's possible that like like i said at the start i'm not the one who personally does the work but i don't",
    "start": "3393680",
    "end": "3399359"
  },
  {
    "text": "remember the postdocs of students telling me about them trying that so probably we haven't either that or my memory is bad or both",
    "start": "3399359",
    "end": "3406880"
  },
  {
    "text": "okay thank you yeah and uh i have another question so uh do you think is it it's possible to so",
    "start": "3406880",
    "end": "3414559"
  },
  {
    "text": "so the example you are currently giving you are computing like either over a fixed size vector or you are just",
    "start": "3414559",
    "end": "3420640"
  },
  {
    "text": "computing a stream of like a stream of scalar value or or like again like a vector value over",
    "start": "3420640",
    "end": "3427680"
  },
  {
    "text": "time it i wonder if is it possible to compute on",
    "start": "3427680",
    "end": "3433040"
  },
  {
    "text": "the normal of the data structure in the normal sense in computer science which which are maybe graph or trees and they",
    "start": "3433040",
    "end": "3439359"
  },
  {
    "text": "are highly irregular and they might be they and they might have a like much bigger",
    "start": "3439359",
    "end": "3444880"
  },
  {
    "text": "size than the than the internal state vector of your physical system because",
    "start": "3444880",
    "end": "3451680"
  },
  {
    "text": "if i'm going to uh so if you think about a analogy",
    "start": "3451680",
    "end": "3458240"
  },
  {
    "text": "currently is it at least the examples i'm seeing it seems that it is like computing with a registered machine and",
    "start": "3458240",
    "end": "3464559"
  },
  {
    "text": "with without only a fixed number of registers with that and without and like an external memory and i wonder",
    "start": "3464559",
    "end": "3472160"
  },
  {
    "text": "if if there's an analog of a of a random access machine",
    "start": "3472160",
    "end": "3478559"
  },
  {
    "text": "to to this like physical computing system",
    "start": "3478559",
    "end": "3483119"
  },
  {
    "text": "right yeah that's a very interesting set of points um so the first thing i should say is in",
    "start": "3483920",
    "end": "3490160"
  },
  {
    "text": "all three three examples we're using fixed size essentially arrays or vectors that we're feeding into the system",
    "start": "3490160",
    "end": "3496720"
  },
  {
    "text": "um and in principle you can make something that actually works on streaming data and we've thought a",
    "start": "3496720",
    "end": "3502079"
  },
  {
    "text": "little bit about this and we haven't yet published any experiments on it but we've we've certainly done some simulations and thought about it of",
    "start": "3502079",
    "end": "3509119"
  },
  {
    "text": "what about designing some dynamical system where you continuously feed time series into it and it just it",
    "start": "3509119",
    "end": "3514960"
  },
  {
    "text": "never ends and it keeps continually performing some tasks on that and that's also possible",
    "start": "3514960",
    "end": "3521200"
  },
  {
    "text": "um the point about memory is very astute",
    "start": "3521200",
    "end": "3526240"
  },
  {
    "text": "this these physical systems currently we're kind of limited in how much data we can put into them both data data and",
    "start": "3526240",
    "end": "3532880"
  },
  {
    "text": "parameters um because they don't have directly connected to them some memory",
    "start": "3532880",
    "end": "3538720"
  },
  {
    "text": "we just have a computer with some memory and then we're feeding it in via essentially",
    "start": "3538720",
    "end": "3543760"
  },
  {
    "text": "some transducer and that is a pretty severe limitation on",
    "start": "3543760",
    "end": "3549760"
  },
  {
    "text": "scaling these to really large sizes so one of the things we're really trying to push towards when you go to for example",
    "start": "3549760",
    "end": "3555520"
  },
  {
    "text": "an optical neural network is thinking how can you make this thing as big as possible um and in optics it turns out that it",
    "start": "3555520",
    "end": "3562880"
  },
  {
    "text": "seems pretty reasonable to go pretty big why because we can have a",
    "start": "3562880",
    "end": "3568400"
  },
  {
    "text": "like a 10 10 megapixel image that we project onto some crystal so we're going to get at least",
    "start": "3568400",
    "end": "3574480"
  },
  {
    "text": "10 million data points in every time we can flash that image on there's still",
    "start": "3574480",
    "end": "3580319"
  },
  {
    "text": "a bunch of tricky business though because well you can do that at maybe 10 hertz but what",
    "start": "3580319",
    "end": "3586960"
  },
  {
    "text": "about if you want to go to gigahertz so you need to try to be",
    "start": "3586960",
    "end": "3592640"
  },
  {
    "text": "a little careful with things that it's very analogous to things that are happening in the cmos neural network",
    "start": "3592640",
    "end": "3599440"
  },
  {
    "text": "accelerator community because they have a very similar problem of neural networks work better and better",
    "start": "3599440",
    "end": "3606000"
  },
  {
    "text": "if you have more and more parameters you want to have as many parameters as possible they can't necessarily all stay",
    "start": "3606000",
    "end": "3611760"
  },
  {
    "text": "within the on-chip memory and it's very expensive to move things on and off the chip so what do you do well you try",
    "start": "3611760",
    "end": "3619280"
  },
  {
    "text": "to design things where you reuse the weights as much as possible and in the case of optics you try to fan out",
    "start": "3619280",
    "end": "3625920"
  },
  {
    "text": "the data optically which is free in optics but not in electronics but it's a pretty difficult systems",
    "start": "3625920",
    "end": "3631280"
  },
  {
    "text": "engineering challenge because you at least in a physical system you don't naturally have a memory so you you have",
    "start": "3631280",
    "end": "3637520"
  },
  {
    "text": "to somehow find a way to take advantage of things either being static or taking advantage of memories that are outside the system",
    "start": "3637520",
    "end": "3644319"
  },
  {
    "text": "and another final thought on that is there's some physical systems where you can imagine the physical system itself",
    "start": "3644319",
    "end": "3649839"
  },
  {
    "text": "having memory like you shine light on a crystal and it heats part of it up so it sort of remembers what happened before and i think it's a",
    "start": "3649839",
    "end": "3656720"
  },
  {
    "text": "very interesting direction to try and figure out how to harness that we don't exactly know how to use that to our",
    "start": "3656720",
    "end": "3662480"
  },
  {
    "text": "benefit yet but it seems in principle like this kind of thing where physical systems have a memory of what happened",
    "start": "3662480",
    "end": "3667599"
  },
  {
    "text": "to them could be useful beyond just building a recurring neural network you want memory",
    "start": "3667599",
    "end": "3672640"
  },
  {
    "text": "because we know how to do that but to do it in the sense you're talking about uh probably requires more work",
    "start": "3672640",
    "end": "3679280"
  },
  {
    "text": "i see thank you yeah very inspiring this is a great great question so thank you uh i have a question uh i wonder how uh",
    "start": "3679280",
    "end": "3687839"
  },
  {
    "text": "how's the system um behave when the in the presence of maybe noise or device",
    "start": "3687839",
    "end": "3692880"
  },
  {
    "text": "mismatches because i i'm i think there must be some like you know",
    "start": "3692880",
    "end": "3697920"
  },
  {
    "text": "uh non-ideality happen inside those mechanical systems or optic systems or",
    "start": "3697920",
    "end": "3703200"
  },
  {
    "text": "or either electronic system i wonder like how large does it affect the training accuracy maybe like does the",
    "start": "3703200",
    "end": "3709920"
  },
  {
    "text": "training accuracy the same every time you run the with the same parameters",
    "start": "3709920",
    "end": "3715760"
  },
  {
    "text": "or or does the noise like affect the the results yeah that's a great question so",
    "start": "3715760",
    "end": "3721839"
  },
  {
    "text": "essentially the biggest challenge we had in this project was trying to deal with making a training procedure that was",
    "start": "3721839",
    "end": "3727359"
  },
  {
    "text": "resilient to the noise because the model inaccuracy slash the noise you",
    "start": "3727359",
    "end": "3732559"
  },
  {
    "text": "have really destroys things if you only train on the laptop which is what was shown in",
    "start": "3732559",
    "end": "3738240"
  },
  {
    "text": "the black here you trade on the laptop the laptop tells yeah trained it great you should get 90 accuracy and then you",
    "start": "3738240",
    "end": "3743680"
  },
  {
    "text": "run it on the actual physical hardware and you get no answers no better than random guessing and this is because the",
    "start": "3743680",
    "end": "3749359"
  },
  {
    "text": "model in the laptop was different than the physical thing and so it went the model and laptop went off into some weird part",
    "start": "3749359",
    "end": "3755760"
  },
  {
    "text": "of parameter space where it said this should be giving you a good answer and you run a little harder and it's actually different",
    "start": "3755760",
    "end": "3760960"
  },
  {
    "text": "um and so that was really what motivated us to do this loop where we have the physical hardware in the loop and then",
    "start": "3760960",
    "end": "3767359"
  },
  {
    "text": "it turns out to be pretty good and you can wonder then why is this succeeding why is this resilient to noise and it's",
    "start": "3767359",
    "end": "3774559"
  },
  {
    "text": "one of the intuitions is that because during the training procedure it's subject to the noise",
    "start": "3774559",
    "end": "3779599"
  },
  {
    "text": "it more or less learns to not go into parts of parameter space that are very sensitive to the noise because if you do",
    "start": "3779599",
    "end": "3785680"
  },
  {
    "text": "that then you'll get bad you'll get bad errors so the this procedure is robust there is",
    "start": "3785680",
    "end": "3792799"
  },
  {
    "text": "a catch which is once you've done the training and you now have your parameters if your physical system changes",
    "start": "3792799",
    "end": "3800079"
  },
  {
    "text": "and you then try run again you're not going to get good results so these results some of these took a few days to",
    "start": "3800079",
    "end": "3805680"
  },
  {
    "text": "collect so these physical systems were like the training procedure was robust to the",
    "start": "3805680",
    "end": "3811359"
  },
  {
    "text": "variation you have over days now if we try do it again",
    "start": "3811359",
    "end": "3817280"
  },
  {
    "text": "on some of these uh especially the the optics one i think like a year later it's probably not going to be very good",
    "start": "3817280",
    "end": "3823520"
  },
  {
    "text": "the mechanics one actually this can be can be revived pretty easily so this one",
    "start": "3823520",
    "end": "3829599"
  },
  {
    "text": "actually stays stays the same for",
    "start": "3829599",
    "end": "3834559"
  },
  {
    "text": "at least months but uh it depends on the physical system because you do need you do require that",
    "start": "3834799",
    "end": "3840319"
  },
  {
    "text": "like the the physical system perform roughly the same function between when you trained",
    "start": "3840319",
    "end": "3846319"
  },
  {
    "text": "it and when you want to use it i see yeah that makes sense like yeah in",
    "start": "3846319",
    "end": "3852240"
  },
  {
    "text": "the training kind of explicitly consider the noise in the learning process right",
    "start": "3852240",
    "end": "3857359"
  },
  {
    "text": "and another cool interpretation of what's going on here is why does this work when the model isn't exactly right",
    "start": "3857359",
    "end": "3863760"
  },
  {
    "text": "is that the gradient only has to be in the right direction if the gradient is in the",
    "start": "3863760",
    "end": "3869599"
  },
  {
    "text": "correct direction for each value in the parameters then it will make progress",
    "start": "3869599",
    "end": "3875280"
  },
  {
    "text": "if the if the model was so inaccurate versus the physical system itself that the gradients were coming out to be the",
    "start": "3875280",
    "end": "3880960"
  },
  {
    "text": "wrong direction then you would not be able to train because it would never go it would never it would never move in",
    "start": "3880960",
    "end": "3886160"
  },
  {
    "text": "the right direction but so long as the model is close enough to the physical system that the gradients or to have the",
    "start": "3886160",
    "end": "3891200"
  },
  {
    "text": "right sign then it makes progress and we quantified how inaccurate the model can",
    "start": "3891200",
    "end": "3896480"
  },
  {
    "text": "be and have those things still work and it turns out that you can have model misspecification mismatch of",
    "start": "3896480",
    "end": "3903599"
  },
  {
    "text": "tens of percent and it's still it still is okay",
    "start": "3903599",
    "end": "3908960"
  },
  {
    "text": "i see then we expect that if if if we're going to move this uh methodology to",
    "start": "3908960",
    "end": "3914160"
  },
  {
    "text": "sensor smart sensor well do you think it will like become worse because i would imagine for example for optic systems",
    "start": "3914160",
    "end": "3920559"
  },
  {
    "text": "like in in your normal training scheme you're going to provide like lasers but if you",
    "start": "3920559",
    "end": "3925680"
  },
  {
    "text": "put it on the sensors those lights are become more like less like the noise right inside of those uh",
    "start": "3925680",
    "end": "3932799"
  },
  {
    "text": "uh lights become like a larger portion of that i would right",
    "start": "3932799",
    "end": "3939359"
  },
  {
    "text": "yes i think always with these going to be a trade-off of how much robustness you want to the system and how long you want",
    "start": "3939359",
    "end": "3945440"
  },
  {
    "text": "it to last and so on versus how much energy efficiency you want it to have so in our work on smart sensors",
    "start": "3945440",
    "end": "3951119"
  },
  {
    "text": "for imaging uh we've just we've we haven't taken some completely random optical system and tried to use that we",
    "start": "3951119",
    "end": "3957680"
  },
  {
    "text": "tried to engineer an optical system that would be relatively stable and not use it with light that was super",
    "start": "3957680",
    "end": "3964000"
  },
  {
    "text": "sensitive to a lot of parameters but we paid a a price an engineering price in that of now our system won't be as",
    "start": "3964000",
    "end": "3970480"
  },
  {
    "text": "energy efficient uh or as low cost as it could it could have been so i think there's always",
    "start": "3970480",
    "end": "3975920"
  },
  {
    "text": "going to be with these a trade-off if you take literally a shaken piece of metal it's sort of like the minimum cost but",
    "start": "3975920",
    "end": "3981920"
  },
  {
    "text": "it's maybe not as robust as if you carefully design it and there's always you need to pick where you want to be in",
    "start": "3981920",
    "end": "3987200"
  },
  {
    "text": "that continuum like another example would be sort of a cmos layout versus",
    "start": "3987200",
    "end": "3993760"
  },
  {
    "text": "randomly scattered transistors um those are the two extremes but you can",
    "start": "3993760",
    "end": "3999119"
  },
  {
    "text": "imagine kind of things in the middle where you're uh where you're getting more benefit but now you're",
    "start": "3999119",
    "end": "4005119"
  },
  {
    "text": "insisting on uh on more or if you want more robustness you have",
    "start": "4005119",
    "end": "4010319"
  },
  {
    "text": "to lose a little bit in the energy efficiency i see yeah that that makes sense yeah we",
    "start": "4010319",
    "end": "4015920"
  },
  {
    "text": "always need to do some trade-offs like for energy and accuracy yeah thanks",
    "start": "4015920",
    "end": "4021359"
  },
  {
    "text": "sure another concrete example of that is like in in this work here we were doing pixel",
    "start": "4021359",
    "end": "4027520"
  },
  {
    "text": "the pixel mapping where literally one pixel is going through one pixel going to one pixel tnu did an amazing job",
    "start": "4027520",
    "end": "4033920"
  },
  {
    "text": "aligning more than 500 000 pixels to be exactly matched with each other but it turns out that",
    "start": "4033920",
    "end": "4039920"
  },
  {
    "text": "yeah that's pretty hard to do but you can imagine making a system that's more dense and more energy efficient if you",
    "start": "4039920",
    "end": "4045200"
  },
  {
    "text": "don't insist on doing that now you try kind of compensate for the aberrations that happened uh in principle you could use the pit",
    "start": "4045200",
    "end": "4051839"
  },
  {
    "text": "strategy to try compensate for things and but the benefits you get is now the system can be smaller",
    "start": "4051839",
    "end": "4059280"
  },
  {
    "text": "i see yeah thanks thanks for for the explanation",
    "start": "4060240",
    "end": "4065920"
  },
  {
    "text": "well if enough further questions uh i'd like to thank everyone again for well all the questions i do received so far",
    "start": "4066240",
    "end": "4071839"
  },
  {
    "text": "and uh and for your attention throughout the talk thank you very much",
    "start": "4071839",
    "end": "4077119"
  },
  {
    "text": "well thank you very much it's been a great talk awesome thanks very much appreciate the invitation really great uh",
    "start": "4077119",
    "end": "4084480"
  },
  {
    "text": "piece of work and uh it's gonna see practical application all over everywhere i think",
    "start": "4084480",
    "end": "4090160"
  },
  {
    "text": "i'm hopeful we'll we'll see at least academic application and if we're really lucky practical",
    "start": "4090160",
    "end": "4095440"
  },
  {
    "text": "application i'll be very excited if it makes it into the real world okay any case thank you very much for the",
    "start": "4095440",
    "end": "4101600"
  },
  {
    "text": "talk and we'll be in touch awesome thanks so much dennis appreciate it thanks everyone bye",
    "start": "4101600",
    "end": "4109000"
  }
]