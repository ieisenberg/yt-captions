[
  {
    "start": "0",
    "end": "5178"
  },
  {
    "text": "Today, we'll be talking about\nthe meta-learning problem and black-box meta-learning. ",
    "start": "5178",
    "end": "12400"
  },
  {
    "text": "This is like the first\ntime we'll actually see some of the kind of core\nmeta-learning topics, which I think is pretty exciting.",
    "start": "12400",
    "end": "17994"
  },
  {
    "text": "They're actually\npretty cool when you look into how they work. First, some less\ninteresting logistics.",
    "start": "17995",
    "end": "24080"
  },
  {
    "text": "So the optional homework 0\nis due tonight at midnight.",
    "start": "24080",
    "end": "29420"
  },
  {
    "text": "We're also going to be\nposting homework 1 today and it will be due on\nWednesday next week.",
    "start": "29420",
    "end": "35370"
  },
  {
    "text": "We're also going to post the\nAzure guide today as well. Basically homework\n1 will be using GPUs",
    "start": "35370",
    "end": "41150"
  },
  {
    "text": "and you'll be able to use\nAzure if you don't have access to a GPU, and we have a guide\nfor getting that all set up",
    "start": "41150",
    "end": "49280"
  },
  {
    "text": "and such and so forth. We'd encourage\nyou to try to kind of to start getting\nset up on Azure earlier",
    "start": "49280",
    "end": "54620"
  },
  {
    "text": "rather than later, in case\nyou run into any catches or anything like that. Great. So the plan for today.",
    "start": "54620",
    "end": "60695"
  },
  {
    "text": "First, we're going to be talking\nabout transfer learning, which we didn't get to last time. We'll talk about the problem\nformulation and fine-tuning,",
    "start": "60695",
    "end": "68430"
  },
  {
    "text": "which is the kind of predominant\napproach for transfer learning. And then we'll talk about\nmeta-learning, including",
    "start": "68430",
    "end": "73729"
  },
  {
    "text": "the problem formulation,\na fairly general recipe of meta-learning\nalgorithms, as well as",
    "start": "73730",
    "end": "79807"
  },
  {
    "text": "a certain class of\nmeta-learning approaches called black-box approaches. And if we have some time,\nwe'll also go through GPT-3,",
    "start": "79807",
    "end": "86270"
  },
  {
    "text": "although I suspect we probably\nwon't have time for that and we'll probably go over\nit in a future lecture.",
    "start": "86270",
    "end": "91910"
  },
  {
    "text": "And also, these kind of\nfirst three bullet points are the topic of homework 1.",
    "start": "91910",
    "end": "97190"
  },
  {
    "text": "You'll be implementing black-box\nmeta-learning algorithms-- yeah. So this lecture will\nbe helpful for that.",
    "start": "97190",
    "end": "107450"
  },
  {
    "text": "Cool. So the goals by the\nend of the lecture are to understand\nthe differences between multi-task\nlearning, transfer learning,",
    "start": "107450",
    "end": "113240"
  },
  {
    "text": "and meta-learning\nproblems; understand some of the basics\nof transfer learning; and then also get into the\ndetails of meta-learning,",
    "start": "113240",
    "end": "120180"
  },
  {
    "text": "including the overall\ntraining setup and how to implement\ntechniques, like black-box meta-learning\ntechniques.",
    "start": "120180",
    "end": "126173"
  },
  {
    "text": " OK, so let's get started\nwith transfer learning. So last week, we talked\nabout multi-task learning,",
    "start": "126174",
    "end": "134530"
  },
  {
    "text": "where our goal was to\nsolve multiple tasks all at the same time. And we looked at this\nobjective function",
    "start": "134530",
    "end": "139870"
  },
  {
    "text": "where we had a loss\nfunction for each task with the corresponding\ndataset for each task and we were trying to\noptimize a network that",
    "start": "139870",
    "end": "146470"
  },
  {
    "text": "can solve all of the tasks\nsimultaneously and leverage the data from different\ntasks in order to try to improve performance\ncompared to training",
    "start": "146470",
    "end": "153310"
  },
  {
    "text": "a task independently. Now, in transfer learning, the\ngoal is a little bit different.",
    "start": "153310",
    "end": "159340"
  },
  {
    "text": "The overall goal,\nyou can think of it as trying to solve a particular\ntarget task after having solved",
    "start": "159340",
    "end": "166870"
  },
  {
    "text": "some source task, and the\nway that you try to do this is essentially\ntrying to transfer knowledge or information\nfrom the source task",
    "start": "166870",
    "end": "173769"
  },
  {
    "text": "to the target task. And usually in\nthis sort of setup, you assume that the source\ntask is fairly rich--",
    "start": "173770",
    "end": "182205"
  },
  {
    "text": "for example,\ntraining on ImageNet is a very common source task-- whereas the target\ntask might be something",
    "start": "182205",
    "end": "187660"
  },
  {
    "text": "that is more narrow\nor more specialized and you want to be able\nto kind of leverage",
    "start": "187660",
    "end": "192970"
  },
  {
    "text": "the rich and diverse\ninformation from the source task when trying to solve\nthe target task.",
    "start": "192970",
    "end": "198443"
  },
  {
    "text": "Now, one key assumption in\ntransfer learning approaches is that you typically can't\ndirectly access the source data",
    "start": "198443",
    "end": "203980"
  },
  {
    "text": "during the transfer\nprocess itself, and this is unlike the\nmulti-task learning setup,",
    "start": "203980",
    "end": "210082"
  },
  {
    "text": "whereas in multi-task learning\nyou typically have all the tasks at once, whereas\nin transfer learning you previously learned task\nA and you have some,",
    "start": "210082",
    "end": "217060"
  },
  {
    "text": "maybe some pretrained weights or\nsomething like that about what you learned about task A,\nbut you can't directly access",
    "start": "217060",
    "end": "222459"
  },
  {
    "text": "the original dataset-- for example, if you're trying\nto transfer from ImageNet to a medical imaging\nproblem, for example,",
    "start": "222460",
    "end": "230019"
  },
  {
    "text": "storing all of\nImageNet and using that might be quite expensive\nwhen you're trying to solve the target task.",
    "start": "230020",
    "end": "235900"
  },
  {
    "text": " It's worth mentioning\nthat transfer learning is a valid solution to\nmulti-task learning.",
    "start": "235900",
    "end": "243130"
  },
  {
    "text": "You can-- if you want\nto-- if you care about one of your tasks in\nparticular, you can try to solve a previous task\nand then transfer to that target",
    "start": "243130",
    "end": "249790"
  },
  {
    "text": "task, but not vice versa. So in transfer\nlearning, you really",
    "start": "249790",
    "end": "256182"
  },
  {
    "text": "care about the target task,\nwhereas in multi-task learning, ultimately you are\ntrying to optimize the loss for multiple tasks.",
    "start": "256182",
    "end": "261819"
  },
  {
    "text": " OK, so a question for you.",
    "start": "261819",
    "end": "267490"
  },
  {
    "text": "What are some problems\nor applications where you think that\ntransfer learning might make a lot of sense, relative\nto multi-task learning?",
    "start": "267490",
    "end": "276870"
  },
  {
    "text": "Any thoughts? Yeah? In image classification,\nmaybe you want to classify images\nin a very specific way.",
    "start": "276870",
    "end": "283116"
  },
  {
    "text": "[INAUDIBLE] classify\nmodels [INAUDIBLE].. Yeah. So the example was\nimage classification,",
    "start": "283116",
    "end": "289080"
  },
  {
    "text": "where maybe you have some\nexisting model for image classification and then\nyou want to transfer that to a different image\nclassification problem.",
    "start": "289080",
    "end": "295020"
  },
  {
    "text": "Yeah? Is limited data in class B? Yeah. So if you have limited\ndata in task B, you want to leverage kind of\nthe information from task A,",
    "start": "295020",
    "end": "302819"
  },
  {
    "text": "and that's a scenario where\nprobably transfer learning would make a lot of sense. Multi-task learning may\nalso make a lot of sense if you have the information\nat task A available.",
    "start": "302820",
    "end": "311880"
  },
  {
    "text": "Yeah. When you have a lot\nof unsupervised data, could it then be used to\nfind [INAUDIBLE] data?",
    "start": "311880",
    "end": "317840"
  },
  {
    "text": "Yeah. So maybe you have a lot\nof unsupervised data and that's your\nsource task, and you want to leverage the information\nfrom that unlabeled data",
    "start": "317840",
    "end": "324960"
  },
  {
    "text": "when solving a target task. Any other thoughts?",
    "start": "324960",
    "end": "331139"
  },
  {
    "text": "Yeah? If the source dataset\nis, like, really large, where do you transfer it from?",
    "start": "331140",
    "end": "337449"
  },
  {
    "text": "Yeah. So if the source\ndataset is really large and you don't want to\nhave to actually train on this really huge dataset\nwhen solving your target task,",
    "start": "337450",
    "end": "345039"
  },
  {
    "text": "it might be a lot\ncheaper computationally, and more practical\ncomputationally, to instead try to\ntransfer kind of a more",
    "start": "345040",
    "end": "352980"
  },
  {
    "text": "compact representation of\nthe knowledge in task A.",
    "start": "352980",
    "end": "359250"
  },
  {
    "text": "So this is actually\none of the examples that I put up on the slides,\nis if your source dataset",
    "start": "359250",
    "end": "364470"
  },
  {
    "text": "is very large, you\ndon't want to have to retain and retrain\non the source dataset every single time you want to\nsolve a downstream target task.",
    "start": "364470",
    "end": "371565"
  },
  {
    "text": " And then another\nscenario where it might make sense is\nmaybe you don't actually",
    "start": "371565",
    "end": "377523"
  },
  {
    "text": "care about solving both\nA and B-- for example, maybe A is self-supervised or\nmaybe it's some previous image classification tasks that\nyou no longer care, about",
    "start": "377523",
    "end": "384540"
  },
  {
    "text": "and you really want to\nfocus on this target task B, then this sort of\ntransfer learning scenario can make sense because it\nallows you to kind of try",
    "start": "384540",
    "end": "391710"
  },
  {
    "text": "to leverage the\nknowledge from task A without necessarily having\nto solve task A. Yeah?",
    "start": "391710",
    "end": "397500"
  },
  {
    "text": "[INAUDIBLE] task B [INAUDIBLE]?",
    "start": "397500",
    "end": "402678"
  },
  {
    "text": " Yeah. So the point is that--",
    "start": "402678",
    "end": "408840"
  },
  {
    "text": "the point that was brought\nup is that you may also have a fairly large\ndataset for task B, and maybe it takes a lot\nof time to learn that",
    "start": "408840",
    "end": "414870"
  },
  {
    "text": "and you want to jump\nstart that optimization process by leveraging a\nmodel that you already trained on a previous task.",
    "start": "414870",
    "end": "420630"
  },
  {
    "start": "420630",
    "end": "426230"
  },
  {
    "text": "OK, and so I'll\ncover one approach for transfer learning, which\nis referred to as fine-tuning.",
    "start": "426230",
    "end": "432229"
  },
  {
    "text": "Many of you, I would guess,\nhave encountered fine-tuning in other courses or\nother experiences,",
    "start": "432230",
    "end": "438172"
  },
  {
    "text": "and the way that\nfine-tuning works is you have some pretrained parameters\nthat were trained just on your source task, and\nthen you run gradient descent",
    "start": "438172",
    "end": "446540"
  },
  {
    "text": "on the training data\nfor the new task. So you initialize the\nweights with something",
    "start": "446540",
    "end": "452882"
  },
  {
    "text": "that was trained\non task A and then you run gradient descent--\nthis is just showing one step of gradient descent. In reality, you\nwould run probably",
    "start": "452882",
    "end": "458990"
  },
  {
    "text": "many steps of gradient descent\non the data for your target task. ",
    "start": "458990",
    "end": "468070"
  },
  {
    "text": "And so if you actually look\nat transfer learning compared to training from\nscratch, you can see pretty large performance\ngains in a number of scenarios.",
    "start": "468070",
    "end": "476090"
  },
  {
    "text": "So for example, if you\npretrain on ImageNet versus if you start from scratch\nwith a random initialization,",
    "start": "476090",
    "end": "484210"
  },
  {
    "text": "you can see an improvement\nfrom 41% to 58% on the Pascal dataset, and\n35% to 52% on the Sun dataset.",
    "start": "484210",
    "end": "493573"
  },
  {
    "text": "And so in this case, the\npretrained parameters came from ImageNet,\nbut in general, the pretrained parameters may\ncome from a variety of sources.",
    "start": "493573",
    "end": "500890"
  },
  {
    "text": "They can come from ImageNet,\nthey can come from models trained on a large language\ncorpora using some sort",
    "start": "500890",
    "end": "507310"
  },
  {
    "text": "of unsupervised or\nself-supervised technique-- really, whatever large\nand diverse dataset",
    "start": "507310",
    "end": "513010"
  },
  {
    "text": "you might have access\nto, or whatever large pretrained models you\nmight have access to that you",
    "start": "513010",
    "end": "518558"
  },
  {
    "text": "think might be related and\nuseful for the target task you're trying to solve. One of the things that's\nreally nice about this sort",
    "start": "518559",
    "end": "525339"
  },
  {
    "text": "of technique is you don't\neven have to actually store the source dataset anywhere. In many cases, you\ncan actually just",
    "start": "525340",
    "end": "530710"
  },
  {
    "text": "go and download a pretrained\nmodel from the internet and run fine-tuning from there.",
    "start": "530710",
    "end": "536150"
  },
  {
    "text": "So these kinds of\npretrained models are often available online\nand they're reusable for many different target tasks.",
    "start": "536150",
    "end": "542380"
  },
  {
    "text": " OK, now all of a sudden,\nfine-tuning is a really great",
    "start": "542380",
    "end": "549149"
  },
  {
    "text": "technique, but there are some\nkind of different practices to try to get fine-tuning\nto work better,",
    "start": "549150",
    "end": "556980"
  },
  {
    "text": "and there isn't-- this is really\nmore of an art than a science. So people have considered a\nnumber of different practices,",
    "start": "556980",
    "end": "562470"
  },
  {
    "text": "like trying to fine-tune\nwith a smaller learning rate than your original learning\nrate, and the goal of this",
    "start": "562470",
    "end": "568019"
  },
  {
    "text": "is to try not to\nerase the knowledge in the pretrained model. Sometimes it makes sense to\nuse a smaller learning rate",
    "start": "568020",
    "end": "575490"
  },
  {
    "text": "for earlier layers\nbecause often the earlier layers of the network\nhave information that is more general about the inputs.",
    "start": "575490",
    "end": "583589"
  },
  {
    "text": "Also, some models just\nactually just completely freeze the earlier layers and only\nfine-tune the later layers,",
    "start": "583590",
    "end": "590340"
  },
  {
    "text": "or gradually unfreeze them\nas it has learned the higher level representations.",
    "start": "590340",
    "end": "596250"
  },
  {
    "text": "Another approach is to simply\nre-initialize the last layer, or put a head on\ntop of the network that you're going to train,\nbut keep everything else fixed.",
    "start": "596250",
    "end": "603435"
  },
  {
    "text": " And then in general, there's all\nthese kind of hyperparameters",
    "start": "603435",
    "end": "608850"
  },
  {
    "text": "or things that you can\ntry with fine-tuning, and oftentimes\nwhat you can do is you can just hold out a\nsubset of your target task",
    "start": "608850",
    "end": "616680"
  },
  {
    "text": "and use cross\nvalidation to figure out which kind of technique works\nbest for the fine-tuning",
    "start": "616680",
    "end": "622750"
  },
  {
    "text": "process.  OK, and it's also worth\nmentioning that architecture",
    "start": "622750",
    "end": "628890"
  },
  {
    "text": "choices here also matter a lot. So for example,\nresidual networks have been found to work\nquite well for fine-tuning",
    "start": "628890",
    "end": "635238"
  },
  {
    "text": "because they lead to gradients\nthat are better behaved.  Great.",
    "start": "635238",
    "end": "640560"
  },
  {
    "text": "Any questions on fine-tuning? ",
    "start": "640560",
    "end": "649970"
  },
  {
    "text": "OK, and so here's an example\nof fine-tuning on a text classification problem\nwhere it was pretraining",
    "start": "649970",
    "end": "656620"
  },
  {
    "text": "using a language\nmodel and fine-tuning using three different text\nclassification problems",
    "start": "656620",
    "end": "663640"
  },
  {
    "text": "from IMDb, TREC-6 and AG. And what you see here is each\nof these plots is showing the--",
    "start": "663640",
    "end": "669280"
  },
  {
    "text": "x-axis is showing the number of\ntraining examples in the target task, and the y-axis is showing\nthe validation error rate",
    "start": "669280",
    "end": "675280"
  },
  {
    "text": "on that target task. And so what we see is\noverall, of course, as you have more examples, you\ndo better on the target task,",
    "start": "675280",
    "end": "681970"
  },
  {
    "text": "but we also see a big\nimprovement between learning from scratch-- sorry-- big improvement from\nusing the pretrained model",
    "start": "681970",
    "end": "689829"
  },
  {
    "text": "compared to learning\nfrom scratch. The other thing that\nyou might notice here is that as you move towards\nfewer and fewer training",
    "start": "689830",
    "end": "697540"
  },
  {
    "text": "examples, fine-tuning still\nhelps a lot over learning from scratch, but\nit doesn't work",
    "start": "697540",
    "end": "703120"
  },
  {
    "text": "as well as when you\nhave a lot of examples. And so, for example, when\nyou have only 100 training examples, or even fewer\nthan 100 training examples,",
    "start": "703120",
    "end": "710500"
  },
  {
    "text": "you might not expect\nfine-tuning to work very well. Yeah?",
    "start": "710500",
    "end": "715730"
  },
  {
    "text": "Why is-- this\nmight be unrelated, but why is the semi-supervised\nlearning better than the supervised?",
    "start": "715730",
    "end": "721850"
  },
  {
    "text": "Yeah. So in this case,\nwhat the supervised versus semi-supervised means is\nthat the supervised example is",
    "start": "721850",
    "end": "727509"
  },
  {
    "text": "using label data\nfrom the target task and the semi-supervised\nexample is using that labeled data\nas well as unlabeled data",
    "start": "727510",
    "end": "734290"
  },
  {
    "text": "that's also available, and\nso it's using additional data in this case and it's\nrunning supervised learning",
    "start": "734290",
    "end": "740140"
  },
  {
    "text": "on the target objective, the\nclassification objective, and it's using the unlabeled\ndata with a language modeling",
    "start": "740140",
    "end": "745690"
  },
  {
    "text": "objective. And so that's why it's\nreferred to as semi-supervised. Yeah? Could you just explain\nwhy it doesn't work well",
    "start": "745690",
    "end": "753670"
  },
  {
    "text": "with small [INAUDIBLE]\nsmall property datasets?",
    "start": "753670",
    "end": "762250"
  },
  {
    "text": "Yeah, absolutely. So when the dataset is\nsmall, essentially what happens is that either--",
    "start": "762250",
    "end": "768389"
  },
  {
    "text": "it either still overfits to\nthat because you are still training on a relatively\nsmall dataset,",
    "start": "768390",
    "end": "775347"
  },
  {
    "text": "or maybe you tune\nthe hyperparameters so that it doesn't overfit but\nit doesn't change the source model enough to actually\nsufficiently solve the task.",
    "start": "775348",
    "end": "781345"
  },
  {
    "text": "OK. ",
    "start": "781345",
    "end": "787550"
  },
  {
    "text": "Yeah? Is that also the\nreason why they all start with [INAUDIBLE]\nfor example [INAUDIBLE]?? Yeah, exactly. So that's why this\nx-axis starts at 100.",
    "start": "787550",
    "end": "794370"
  },
  {
    "text": "If you move even further to\nthe left, it gets even worse. Yeah? Are there other ways\nof including knowledge",
    "start": "794370",
    "end": "799860"
  },
  {
    "text": "from the training\ntask or training data other than just\ntransferring the weights over to the new set?",
    "start": "799860",
    "end": "807129"
  },
  {
    "text": "Yeah. So transferring the\nweights is certainly the most popular approach. There are other approaches\nin the domain adaptation",
    "start": "807130",
    "end": "814360"
  },
  {
    "text": "literature. You can also use things\nlike multi-task learning. Most of the other approaches\nfor transferring information,",
    "start": "814360",
    "end": "821700"
  },
  {
    "text": "either just transfer\nthe weights typically or they assume that you\nhave access to the data-- all the data from\nthe source task--",
    "start": "821700",
    "end": "828150"
  },
  {
    "text": "and try to use that data\ninstead of using the weights, or in addition to\nusing the weights.",
    "start": "828150",
    "end": "834000"
  },
  {
    "text": "And then, of course, another\napproach that we'll talk about right after this is how\nmeta-learning can actually be",
    "start": "834000",
    "end": "839250"
  },
  {
    "text": "useful for leveraging-- for leveraging the\nsource data in a way",
    "start": "839250",
    "end": "844260"
  },
  {
    "text": "that can actually work well\nwith very small amounts of data of the target task.",
    "start": "844260",
    "end": "849269"
  },
  {
    "text": " Yeah? I have a question. Can you transfer learn\nacross different networks?",
    "start": "849270",
    "end": "857640"
  },
  {
    "text": "Right. So the question is,\ncan you transfer learn across different\nnetwork architectures?",
    "start": "857640",
    "end": "863240"
  },
  {
    "text": "In general with\nfine-tuning, you do assume that the architecture for\nthe source task and the target",
    "start": "863240",
    "end": "869930"
  },
  {
    "text": "task is the same. You might add a couple\nof layers on top, or you might chop some layers\noff of the source architecture,",
    "start": "869930",
    "end": "877530"
  },
  {
    "text": "but in general, it's going to be\na fairly similar architecture. At least the first few\nlayers of the network",
    "start": "877530",
    "end": "882589"
  },
  {
    "text": "have to have to be the same. ",
    "start": "882590",
    "end": "889320"
  },
  {
    "text": "OK, so now let's talk a\nbit about meta-learning and how we can actually\nlearn target tasks",
    "start": "889320",
    "end": "895010"
  },
  {
    "text": "with very small amounts\nof data by essentially learning how to learn\ndifferent source tasks.",
    "start": "895010",
    "end": "902270"
  },
  {
    "text": "And so to start, let's\nactually just cover the problem statement\nof meta-learning, and this is the\nproblem statement",
    "start": "902270",
    "end": "907460"
  },
  {
    "text": "that we'll consider\nin this class. There are other ways to think\nabout the meta-learning problem statement as well, and happy\nto chat about those separately.",
    "start": "907460",
    "end": "917937"
  },
  {
    "text": "OK, so there's really\ntwo ways to view meta-learning algorithms and\nwe'll discuss both of them",
    "start": "917937",
    "end": "923120"
  },
  {
    "text": "in the course. One we'll refer to as more\nof a mechanistic view, which is thinking about how\nto implement algorithms",
    "start": "923120",
    "end": "930259"
  },
  {
    "text": "and how they actually work\nin terms of the underlying mechanisms, and the\nsecond view is more",
    "start": "930260",
    "end": "935930"
  },
  {
    "text": "of a probabilistic\nview, which ties into nice intuition with regard\nto probabilistic graphical",
    "start": "935930",
    "end": "941540"
  },
  {
    "text": "models and thinking\nabout things in terms of priors and posteriors.",
    "start": "941540",
    "end": "946790"
  },
  {
    "text": "And so from the\nmechanistic standpoint, the way that meta-learning\nalgorithms work is they essentially have a model that\nreads in an entire dataset",
    "start": "946790",
    "end": "956120"
  },
  {
    "text": "and makes predictions\nfor new data points. And then you train this network\nitself with a metadata set,",
    "start": "956120",
    "end": "964699"
  },
  {
    "text": "or a dataset of\ndatasets, such that it's kind of trained to\nread in datasets",
    "start": "964700",
    "end": "971060"
  },
  {
    "text": "and make predictions and\ngeneralize from datasets across a set of datasets-- so essentially,\nit's learning how",
    "start": "971060",
    "end": "977150"
  },
  {
    "text": "to learn from a set\nof datasets, and this is why it's referred\nto as meta-learning.",
    "start": "977150",
    "end": "983300"
  },
  {
    "text": "So it's explicitly\noptimizing for the ability to learn from datasets.",
    "start": "983300",
    "end": "989029"
  },
  {
    "text": "And then more probabilistic\nview is, instead of thinking about it in\nterms of reading in datasets and making predictions, instead\nuse the meta-learning processes",
    "start": "989030",
    "end": "996770"
  },
  {
    "text": "trying to extract prior\nknowledge from a set of tasks, and then using that\nprior knowledge",
    "start": "996770",
    "end": "1003010"
  },
  {
    "text": "to quickly solve new tasks. And then from this\nstandpoint, you can essentially\nview it as learning",
    "start": "1003010",
    "end": "1009610"
  },
  {
    "text": "a task that uses a prior\nand a small training set to infer a posterior over the\nparameters for that new task.",
    "start": "1009610",
    "end": "1017215"
  },
  {
    "text": " So we're going to be talking\nabout both of these views. For now, we're going\nto focus primarily",
    "start": "1017215",
    "end": "1023535"
  },
  {
    "text": "on the mechanistic view. But when we talk about\nBayesian meta-learning, we're going to talk about\nthe probabilistic view,",
    "start": "1023535",
    "end": "1029079"
  },
  {
    "text": "and the probabilistic\nview does-- does end up being quite\nnice, in terms of intuition",
    "start": "1029079",
    "end": "1035079"
  },
  {
    "text": "and thinking about\nthe problem, but I think that the\nmechanistic standpoint is a little bit easier\nfor just understanding how",
    "start": "1035079",
    "end": "1040751"
  },
  {
    "text": "things work from the beginning. So we'll have Bayes\ncome back later. ",
    "start": "1040752",
    "end": "1047650"
  },
  {
    "text": "So how does meta-learning work? We're going to run through an\nexample of kind of the problem",
    "start": "1047650",
    "end": "1054490"
  },
  {
    "text": "setup and the training\nsetup, and then we'll talk about how algorithms go\nabout solving this training setup.",
    "start": "1054490",
    "end": "1060860"
  },
  {
    "text": "So as we saw in\nthe first lecture,",
    "start": "1060860",
    "end": "1066950"
  },
  {
    "text": "we are going to\nlook at an example where we want to be able to\nperform image classification and we're given a really\ntiny training dataset.",
    "start": "1066950",
    "end": "1074049"
  },
  {
    "text": "This training dataset\nhas five examples-- one example of five\ndifferent classes--",
    "start": "1074050",
    "end": "1079210"
  },
  {
    "text": "and we want to be able\nto use this tiny training dataset in order to\nclassify new examples.",
    "start": "1079210",
    "end": "1084710"
  },
  {
    "text": "So this is essentially\nour test set. And of course, like we\nsaw in the first lecture,",
    "start": "1084710",
    "end": "1090370"
  },
  {
    "text": "humans are pretty good\nat solving problems with very small amounts of\ndata, including in scenarios",
    "start": "1090370",
    "end": "1096700"
  },
  {
    "text": "where you haven't seen\nthese classes before, but if you train from\nscratch, or even if you fine tuned on this tiny\namount of data,",
    "start": "1096700",
    "end": "1102850"
  },
  {
    "text": "you wouldn't be able to\ngeneralize to new examples very well.",
    "start": "1102850",
    "end": "1108080"
  },
  {
    "text": "So this is what we want to\nbe able to do essentially at test time. We want the model to\nbe able to perform",
    "start": "1108080",
    "end": "1113620"
  },
  {
    "text": "this few-shot\nclassification problem. And so the way that\nmeta-learning algorithms work",
    "start": "1113620",
    "end": "1118809"
  },
  {
    "text": "is they try to create\na training setup that prepares the model to solve\nthese kinds of problems.",
    "start": "1118810",
    "end": "1126100"
  },
  {
    "text": "And the way that\nyou can do that, in the same part of\nmachine learning, is if this is what you\nwant to do at test time,",
    "start": "1126100",
    "end": "1132130"
  },
  {
    "text": "then at training time you\nshould give it examples of solving problems like this. So what we can do is we can\ngive it kind of mini train sets",
    "start": "1132130",
    "end": "1139930"
  },
  {
    "text": "and mini test sets\nand train the model to solve these kinds of problems\nso that it can generalize",
    "start": "1139930",
    "end": "1146500"
  },
  {
    "text": "two test examples\nfrom training sets so that after it's seen kind of\nall of these different training",
    "start": "1146500",
    "end": "1152710"
  },
  {
    "text": "tasks, it can eventually solve\nthe problem at the bottom.",
    "start": "1152710",
    "end": "1158360"
  },
  {
    "text": "So each of these rows will be\njust different classification tasks. These are constructed using\na set of training classes.",
    "start": "1158360",
    "end": "1166210"
  },
  {
    "text": "And then-- so this is kind of\nthe meta-training process-- and then at, what we'll\ncall, meta-test time,",
    "start": "1166210",
    "end": "1173800"
  },
  {
    "text": "we want to be able to\nlearn a new task using the tiny training dataset. And this is kind of\na held out test task,",
    "start": "1173800",
    "end": "1182035"
  },
  {
    "text": "and it has a training\ndataset and a test set, and this training set\nand test set exactly mimics the kinds of\ntraining sets and test sets",
    "start": "1182035",
    "end": "1187608"
  },
  {
    "text": "we'll see during the\nmeta-training process. Yeah? [INAUDIBLE] These are\nnot technically held out",
    "start": "1187608",
    "end": "1198000"
  },
  {
    "text": "[INAUDIBLE]. Yeah, exactly. So the examples on the top right\nin the blue rectangles, these",
    "start": "1198000",
    "end": "1205640"
  },
  {
    "text": "are not actually held out from\nthe meta-training process. They are being used to test\nthe model's generalization",
    "start": "1205640",
    "end": "1213140"
  },
  {
    "text": "on that particular task, but\nthese are kind of the training classes, and then\nthese are the examples",
    "start": "1213140",
    "end": "1218270"
  },
  {
    "text": "that are completely held out-- these examples down here-- and these will be kind of--",
    "start": "1218270",
    "end": "1225065"
  },
  {
    "text": "yeah, classes that are held\nout from the training classes used for meta-training.",
    "start": "1225065",
    "end": "1232070"
  },
  {
    "text": "Yeah? Is transfer learning\nsuperior to meta-learning when there's a domain shift?",
    "start": "1232070",
    "end": "1238160"
  },
  {
    "text": "Yeah. So the question is,\nis transfer learning superior to meta-learning\nwhen there's a domain shift?",
    "start": "1238160",
    "end": "1243400"
  },
  {
    "text": "And in particular, one\nthing that you might notice is that we're\ntrying to prepare-- we're trying to make it so that\ntraining and test time match",
    "start": "1243400",
    "end": "1251140"
  },
  {
    "text": "so that we're\nessentially preparing it for solving this sort of\nclassification problem. And if you do have a task\nthat is out of distribution",
    "start": "1251140",
    "end": "1258970"
  },
  {
    "text": "from what you're\ntraining it on, then it may not work well because that's\nnot what you prepared it for, and in those kinds of\nscenarios, transfer learning",
    "start": "1258970",
    "end": "1266140"
  },
  {
    "text": "can sometimes perform\nbetter meta-learning kinds of approaches.",
    "start": "1266140",
    "end": "1271540"
  },
  {
    "text": " Now, each of these\nrows corresponds",
    "start": "1271540",
    "end": "1277180"
  },
  {
    "text": "to a different image\nclassification task, but in reality you can really\nreplace image classification",
    "start": "1277180",
    "end": "1283090"
  },
  {
    "text": "with really any sort of\nmachine learning task. You can replace it with\ndifferent regression problems, you can replace it with\ndifferent language generation",
    "start": "1283090",
    "end": "1290020"
  },
  {
    "text": "problems, different\nskill learning problems, and so forth.",
    "start": "1290020",
    "end": "1295190"
  },
  {
    "text": "You can also\npotentially combine it with really a variety of\nmachine learning problems. The most important thing is\nthat the task that you're",
    "start": "1295190",
    "end": "1302200"
  },
  {
    "text": "seeing here is within\nthe distribution of tasks that it sees during\nmeta-training.",
    "start": "1302200",
    "end": "1308510"
  },
  {
    "text": "So we're essentially\nraising machine learning up to the level of tasks. ",
    "start": "1308510",
    "end": "1315768"
  },
  {
    "text": "OK, so you can think of\nthe meta-learning problem as given data from\na number of tasks, quickly learn a new test task.",
    "start": "1315768",
    "end": "1322970"
  },
  {
    "text": "And one key assumption is\nthat these meta-training and meta-test tasks are sampled\ni.i.d from the same task",
    "start": "1322970",
    "end": "1329030"
  },
  {
    "text": "distribution. And so we can think\nof it as having some distribution over\ntasks and then sampling them",
    "start": "1329030",
    "end": "1337280"
  },
  {
    "text": "from that same distribution. And like before, like\nin multi-task learning and transfer learning,\nyou also want these tasks",
    "start": "1337280",
    "end": "1344110"
  },
  {
    "text": "to share some structure. If they are completely\nrandom tasks, then even if they're\ndrawn i.i.d.,",
    "start": "1344110",
    "end": "1349632"
  },
  {
    "text": "you won't be able to do\nbetter at the test task than learning from scratch. ",
    "start": "1349632",
    "end": "1356700"
  },
  {
    "text": "OK, so what did the task\nactually correspond to? I gave an image classification\nexample on the previous slide.",
    "start": "1356700",
    "end": "1362087"
  },
  {
    "text": "It could also be something like\nrecognizing handwriting digits from different languages,\nand this is the example",
    "start": "1362087",
    "end": "1367310"
  },
  {
    "text": "that we'll see in homework 1. It can also correspond\nto giving feedback to students on different\nexams, where different exam",
    "start": "1367310",
    "end": "1375740"
  },
  {
    "text": "problems correspond\nto different tasks that you need to be able\nto get feedback on--",
    "start": "1375740",
    "end": "1381410"
  },
  {
    "text": "we'll see an example of this\nin two lectures, I think. It could also correspond\nto classifying species",
    "start": "1381410",
    "end": "1388250"
  },
  {
    "text": "in different regions of the\nworld, where different regions correspond to different tasks. It could also correspond\nto a robot performing",
    "start": "1388250",
    "end": "1395065"
  },
  {
    "text": "different tasks\nas well, where it wants to learn a new\nskill or a new task by leveraging experience\nsolving previous tasks.",
    "start": "1395065",
    "end": "1401570"
  },
  {
    "text": " Now another natural question\nis, how many tasks do you",
    "start": "1401570",
    "end": "1407669"
  },
  {
    "text": "need for this whole\nmeta-training process? And in general, it's really a\ncase of the more, the better.",
    "start": "1407670",
    "end": "1417370"
  },
  {
    "text": "Essentially, meta-training-- or,\nmeta-learning algorithms treat tasks as data points,\nand analogously,",
    "start": "1417370",
    "end": "1424823"
  },
  {
    "text": "in machine learning,\nthe more data you have, the better,\nand so in meta-learning, the more tasks you\nhave, the better.",
    "start": "1424823",
    "end": "1431870"
  },
  {
    "text": "Yeah? Are you trying to gain\nsome intuition for what does task distribution mean?",
    "start": "1431870",
    "end": "1437060"
  },
  {
    "text": "What does it mean for two\ntasks focusing on distribution?",
    "start": "1437060",
    "end": "1442415"
  },
  {
    "text": "Yeah, that's a great question.  There's different ways\nthat you can look at the--",
    "start": "1442415",
    "end": "1452177"
  },
  {
    "text": "there's different\nways of representing different kinds of tasks. ",
    "start": "1452177",
    "end": "1457700"
  },
  {
    "text": "Let me think of a good example. So I mean, as one simple\nexample, you could say that--",
    "start": "1457700",
    "end": "1466330"
  },
  {
    "text": "say that you want\ndifferent tasks to correspond to a robot\nrunning at different velocities.",
    "start": "1466330",
    "end": "1471850"
  },
  {
    "text": "One task is to run\nas fast as possible, one task is to do a very slow\ntrot, something like that--",
    "start": "1471850",
    "end": "1477610"
  },
  {
    "text": "then, your task distribution\nwould correspond to those-- correspond to, essentially,\njust different velocities,",
    "start": "1477610",
    "end": "1484270"
  },
  {
    "text": "and you would assume that\nyou have some distribution of velocities--",
    "start": "1484270",
    "end": "1489893"
  },
  {
    "text": "you have some\ndistribution of velocities and the training tasks are\nsampled from that distribution of velocities and\nthe test tasks are",
    "start": "1489893",
    "end": "1495160"
  },
  {
    "text": "sampled from that\ndistribution of velocities. Now that's one example. There are also going to be\nexamples where you might not",
    "start": "1495160",
    "end": "1502240"
  },
  {
    "text": "have this nice parametric\nway to specify tasks, and in those cases, it gets\na little bit murkier per se.",
    "start": "1502240",
    "end": "1509500"
  },
  {
    "text": " You can also think of when we\ndefined tasks two lectures ago,",
    "start": "1509500",
    "end": "1516950"
  },
  {
    "text": "we also-- we defined it as\nkind of a distribution over x and a distribution over\ny given x, at least",
    "start": "1516950",
    "end": "1523480"
  },
  {
    "text": "for supervised\nlearning problems. And in this case, if you can\nessentially somehow parameteriz",
    "start": "1523480",
    "end": "1529780"
  },
  {
    "text": "these distributions,\nthen you can think of the\ndistribution over tasks as being a distribution\nover those parameters.",
    "start": "1529780",
    "end": "1538789"
  },
  {
    "text": "So if these are parameterized\nby 5 for example, then you would assume that you\nhave some distribution over 5.",
    "start": "1538790",
    "end": "1546997"
  },
  {
    "text": "In practice, you may not always\nhave this nice parametric form for your tasks.",
    "start": "1546997",
    "end": "1552210"
  },
  {
    "text": "Yeah? Just a thought for\nthe robot example.",
    "start": "1552210",
    "end": "1557230"
  },
  {
    "text": "Sorry if I missed\nthis, but what's the difference between\nhaving these different tasks",
    "start": "1557230",
    "end": "1564460"
  },
  {
    "text": "and different velocities versus\njust having one big dataset? Does that make sense? ",
    "start": "1564460",
    "end": "1572310"
  },
  {
    "text": "You're asking, what's the\ndifference between having-- like, breaking it down\ninto different tasks",
    "start": "1572310",
    "end": "1578540"
  },
  {
    "text": "comparing to have\na single dataset with all the tasks in it? Yeah. Yeah, Yeah. I don't know if there's\n[INAUDIBLE] just in general.",
    "start": "1578540",
    "end": "1583970"
  },
  {
    "text": "Yeah. So essentially,\nthe important thing here is we want to be\nable to learn a new task.",
    "start": "1583970",
    "end": "1589500"
  },
  {
    "text": "We want-- like, maybe\nthe robot has acquired a bunch of previous\nskills and now it wants to learn how to ride a\nbike or skateboard or something",
    "start": "1589500",
    "end": "1597140"
  },
  {
    "text": "like that. One of the things that's pretty\nimportant in the meta-learning",
    "start": "1597140",
    "end": "1602240"
  },
  {
    "text": "problem statement-- and\nwe'll see why this is later-- is to actually break down\nthe previous experience into these different\ntasks, and that",
    "start": "1602240",
    "end": "1610159"
  },
  {
    "text": "allows you to try to\nextract the shared structure between those\ntasks and ultimately learn a new task.",
    "start": "1610160",
    "end": "1615559"
  },
  {
    "text": "And if you don't\nhave it broken down into those different\ntasks, then it's much more difficult\nto try to quickly--",
    "start": "1615560",
    "end": "1621620"
  },
  {
    "text": "very quickly learn a new\ntask like skateboarding. Yeah? [INAUDIBLE],,\nespecially yesterday,",
    "start": "1621620",
    "end": "1627968"
  },
  {
    "text": "in class what we learned, we\nclassify both datasets as loss functions. Is the distribution here--",
    "start": "1627968",
    "end": "1633980"
  },
  {
    "text": "it seems like your\ndistribution should-- you have the same different set\nof loss functions [INAUDIBLE] loss functions,\n[INAUDIBLE] distribution.",
    "start": "1633980",
    "end": "1642350"
  },
  {
    "text": "Is there-- is that true? And also, is there one\nthat is more common? Yeah. So I guess I should\nhave also said",
    "start": "1642350",
    "end": "1648230"
  },
  {
    "text": "you could also have-- you have\na loss function for each task as well, and that could\nalso be potentially parameterized or something.",
    "start": "1648230",
    "end": "1654169"
  },
  {
    "text": "The-- and-- yeah. So different tasks can vary\nin these different ways.",
    "start": "1654170",
    "end": "1659390"
  },
  {
    "text": "They could only-- they\ncould choose to only vary in this way, they could choose\nto only vary in this way, and so forth. I think that it's\nprobably most common",
    "start": "1659390",
    "end": "1666169"
  },
  {
    "text": "for different meta-learning\ntasks to vary in the data",
    "start": "1666170",
    "end": "1671930"
  },
  {
    "text": "and not in the loss function. But in principle, you can\nvary any aspect of it,",
    "start": "1671930",
    "end": "1680330"
  },
  {
    "text": "or choose to not vary\nany aspect of it as well. Yeah? Is the loss function\nthe same across tasks?",
    "start": "1680330",
    "end": "1687332"
  },
  {
    "text": "Say some kind of\nparametric analysis one can do to figure out if the\nproblem would be best solved",
    "start": "1687332",
    "end": "1693559"
  },
  {
    "text": "through meta-learning or\njust long before one dataset is created [INAUDIBLE]? Is there a way to kind\nof govern beforehand",
    "start": "1693560",
    "end": "1701250"
  },
  {
    "text": "which approach to take? Yeah. So the question\nis, is there a way to determine ahead of\ntime which approach",
    "start": "1701250",
    "end": "1706790"
  },
  {
    "text": "to take-- whether to take\na meta-learning approach or a transfer learning approach? In general, there isn't any\nformula that will tell you",
    "start": "1706790",
    "end": "1713870"
  },
  {
    "text": "which approach to use,\nalthough hopefully over kind of the course of the\nnext couple of weeks",
    "start": "1713870",
    "end": "1720320"
  },
  {
    "text": "you'll be able to\nget some intuition about which approaches\nmight be more suitable. Yeah?",
    "start": "1720320",
    "end": "1726170"
  },
  {
    "text": "So that's my problem. Is there something to give\nthe order of the tasks at training time so that it\nwould make the taks easier",
    "start": "1726170",
    "end": "1733620"
  },
  {
    "text": "to solve at test time? Yeah. The question is,\nis there a way to-- is there something\nthat tells us how",
    "start": "1733620",
    "end": "1738680"
  },
  {
    "text": "to order the tasks in a way\nthat will help make it easier to solve the test task?",
    "start": "1738680",
    "end": "1744080"
  },
  {
    "text": "I guess the short answer\nis that's an open problem, I think. Also, one interesting\nproblem beyond that",
    "start": "1744080",
    "end": "1750140"
  },
  {
    "text": "is not even ordering, but can we\npick the set of training tasks that will be most useful for\nlearning the target task rather",
    "start": "1750140",
    "end": "1756650"
  },
  {
    "text": "than-- and then from there also picking\nthe ordering and so forth? As we'll see later,\nwe do actually",
    "start": "1756650",
    "end": "1762950"
  },
  {
    "text": "train on all the tasks\nat once essentially, rather than training\non them one at a time.",
    "start": "1762950",
    "end": "1768205"
  },
  {
    "text": "And there is a\nlittle bit of work on setting curriculum\nlearning, but in general, it's an open problem.",
    "start": "1768205",
    "end": "1774380"
  },
  {
    "text": "Yeah? [INAUDIBLE],, can you just\nlike-- the velocity of the robot",
    "start": "1774380",
    "end": "1779530"
  },
  {
    "text": "working, is that continuous\nor noncontinuous? Yeah. So the question is, what if the\ntask is somewhat continuous,",
    "start": "1779530",
    "end": "1785960"
  },
  {
    "text": "like the velocity of a robot? In that case, we will still be\nsampling tasks for training,",
    "start": "1785960",
    "end": "1792649"
  },
  {
    "text": "and so we'll be sampling\nnumbers from a distribution over velocities. You can choose to sample every\nsingle iteration of training,",
    "start": "1792650",
    "end": "1801445"
  },
  {
    "text": "and so you don't\nnecessarily need to have a finite\nnumber of tasks. If you have infinite, that's--\nthe more, the better--",
    "start": "1801445",
    "end": "1807080"
  },
  {
    "text": "and that's great if you\nhave access to that. Yeah? How do you ensure\nthat tasks are drawn i.i.d. from the distribution?",
    "start": "1807080",
    "end": "1814179"
  },
  {
    "text": "Yeah. So the question is, how do you\nensure that tasks are drawn i.i.d. from the distribution? ",
    "start": "1814180",
    "end": "1820920"
  },
  {
    "text": "It really depends\non the problem. You want to-- the\nmost important thing",
    "start": "1820920",
    "end": "1826280"
  },
  {
    "text": "is to try to have the\ntest task be drawn from a similar\ndistribution and not",
    "start": "1826280",
    "end": "1832183"
  },
  {
    "text": "be completely out\nof distribution, or significantly harder\nor significantly easier than the task that you\nsaw during training.",
    "start": "1832183",
    "end": "1837530"
  },
  {
    "text": " OK, one more question. One more question.",
    "start": "1837530",
    "end": "1843409"
  },
  {
    "text": "So what are some strategies\nfor overcoming domain shift in meta-learning? Yeah. So the question is,\nwhat are some strategies",
    "start": "1843410",
    "end": "1849650"
  },
  {
    "text": "for overcoming domain shift? We'll talk about this\nnext lecture a little bit.",
    "start": "1849650",
    "end": "1856940"
  },
  {
    "text": "OK, and then we can\nanswer more questions a little bit later, or\nmaybe some of the slides will answer them.",
    "start": "1856940",
    "end": "1863759"
  },
  {
    "text": "OK, so I want to go\nover some terminology. So in the example\nthat I showed before, we have these meta-training\ntasks and meta-test tasks.",
    "start": "1863760",
    "end": "1873590"
  },
  {
    "text": "Each dataset-- each\ntraining dataset for a task",
    "start": "1873590",
    "end": "1878720"
  },
  {
    "text": "can be referred to as\nthe task training set. And so if a task\nis indexed with i, then I'll use D train\ni to refer to that.",
    "start": "1878720",
    "end": "1887180"
  },
  {
    "text": "And then each task also\nhas a corresponding test set for that task.",
    "start": "1887180",
    "end": "1892525"
  },
  {
    "text": "Sometimes these are also\nreferred to as the support set and the query set. I'll also be using\nthis terminology.",
    "start": "1892525",
    "end": "1898580"
  },
  {
    "text": "It's helpful to use\nsupport set and query set for these\ntraining and test sets because it distinguishes it from\ntraining tasks and test tasks,",
    "start": "1898580",
    "end": "1905750"
  },
  {
    "text": "or meta-training tasks\nand meta-testing tasks. ",
    "start": "1905750",
    "end": "1911279"
  },
  {
    "text": "And then in general,\nif we want to learn with a small number\nof examples, we refer to that as\nk-shot learning, where",
    "start": "1911280",
    "end": "1918150"
  },
  {
    "text": "we assume that we have\nk examples per class, or if you're in a\nregression scenario,",
    "start": "1918150",
    "end": "1924570"
  },
  {
    "text": "you just have k examples total. So for example, in\none-shot learning, we'll have one\nexample per class.",
    "start": "1924570",
    "end": "1932400"
  },
  {
    "text": "And then in N-way\nclassification, essentially the model\nwill be choosing between N different classes.",
    "start": "1932400",
    "end": "1938220"
  },
  {
    "text": " So a question for you.",
    "start": "1938220",
    "end": "1943720"
  },
  {
    "text": "What is k and N for\nthis example here? ",
    "start": "1943720",
    "end": "1951600"
  },
  {
    "text": "Yeah? k is 1 in this one. Yes. k is 1 and N\nis 5 because we're doing five-way classification,\nand each of our support sets",
    "start": "1951600",
    "end": "1959800"
  },
  {
    "text": "has one example for each\nof the five classes. ",
    "start": "1959800",
    "end": "1966975"
  },
  {
    "text": "OK, so this is kind of the\nnotation and terminology that we'll be using throughout\nthe next several lectures, including in the homework.",
    "start": "1966975",
    "end": "1972910"
  },
  {
    "text": "Yeah? So what the test\ndataset is exactly",
    "start": "1972910",
    "end": "1978260"
  },
  {
    "text": "updating the model to do? Like, is there two\nimages that it's trying to classify together?",
    "start": "1978260",
    "end": "1983720"
  },
  {
    "text": "Yeah. So essentially, what we're\ngoing to be trying to do is learn from the\ntraining datasets,",
    "start": "1983720",
    "end": "1988990"
  },
  {
    "text": "and we want to be able to learn\nfrom the training datasets in a way that generalizes. We don't just want to be able to\nmemorize the training examples.",
    "start": "1988990",
    "end": "1995770"
  },
  {
    "text": "And so these test sets\nshown in the red boxes are essentially going to be\ntesting the model's ability",
    "start": "1995770",
    "end": "2001170"
  },
  {
    "text": "to generalize. I'm showing just\ntwo examples here for conciseness on the slides.",
    "start": "2001170",
    "end": "2007560"
  },
  {
    "text": "It, in reality, this\ncould be different sizes-- it could be one\nexample per class, it could be larger than that,\nit could be smaller than that.",
    "start": "2007560",
    "end": "2013260"
  },
  {
    "text": "It should be at least one\nto measure generalization, but it could be different sizes. Yeah? Does the N include\nthe classifying makeup",
    "start": "2013260",
    "end": "2021570"
  },
  {
    "text": "testing as well, or\njust the [INAUDIBLE]?? Right. So N includes\nessentially the number",
    "start": "2021570",
    "end": "2028830"
  },
  {
    "text": "of the classification problem\nthat each of these tasks involves.",
    "start": "2028830",
    "end": "2035070"
  },
  {
    "text": "In reality, you may\nbe doing meta-training with many more than five\nclasses, or many more than N",
    "start": "2035070",
    "end": "2041160"
  },
  {
    "text": "classes. So for example, you\nmight notice here that we see classes like\nbird, mushroom, singer,",
    "start": "2041160",
    "end": "2047410"
  },
  {
    "text": "or piano in the first\nexample, and we see classes like landscape, tank,\nbarrel, that sort of thing--",
    "start": "2047410",
    "end": "2053495"
  },
  {
    "text": "and so we actually have\nmore than five classes that are being used in this\nmeta-training process, but the classification\nproblem that's",
    "start": "2053495",
    "end": "2059340"
  },
  {
    "text": "being solved at meta-test time\nis a five-way classification. Oh, I see. So the the N is per task?",
    "start": "2059340",
    "end": "2065911"
  },
  {
    "text": "Per task. Exactly. Yeah. Yeah? Does the terminology-- is\nthere a term for [INAUDIBLE]",
    "start": "2065912",
    "end": "2073304"
  },
  {
    "text": "more or less classes? Yeah. So the question is,\nis it easier or harder to learn with more\nor less classes?",
    "start": "2073305",
    "end": "2079949"
  },
  {
    "text": "If N is larger, that's\nharder because it has to distinguish more\nclasses from one another.",
    "start": "2079949",
    "end": "2087120"
  },
  {
    "text": "Although, if you-- so\nlarger N means it's harder. If you have a lot\nof classes to choose",
    "start": "2087120",
    "end": "2092369"
  },
  {
    "text": "from in the meta-training\nprocess but N is held constant, that generally makes\nit easier because you have more data to learn from,\nand more-- essentially more",
    "start": "2092370",
    "end": "2099364"
  },
  {
    "text": "tasks. Yeah? To clarify, does it\nneed to be-- does",
    "start": "2099364",
    "end": "2104460"
  },
  {
    "text": "k and N need to be homogeneous\nacross all training? And if so, why? Yeah, that's a great question.",
    "start": "2104460",
    "end": "2109810"
  },
  {
    "text": "So does k and N need to be\nthe same across all the tasks? It doesn't. It can be different\nfor different tasks.",
    "start": "2109810",
    "end": "2116250"
  },
  {
    "text": "You want to make sure that you\ndon't have a harder meta-test task than at\nmeta-training, and so you",
    "start": "2116250",
    "end": "2121290"
  },
  {
    "text": "shouldn't set N to be larger\nthan any of the training tasks. And for simplicity,\nit's all the same here,",
    "start": "2121290",
    "end": "2126450"
  },
  {
    "text": "but it can be different\nfor different tasks. Yeah? [INAUDIBLE] which task,\nfor example, [INAUDIBLE]",
    "start": "2126450",
    "end": "2137369"
  },
  {
    "text": "as the same class because\nwe are all animals. However, in that\ncase it can be 4,",
    "start": "2137370",
    "end": "2143460"
  },
  {
    "text": "but I can also think\nthis one, dog and lion is two separate classes.",
    "start": "2143460",
    "end": "2149730"
  },
  {
    "text": "So in that case,\nthe k would be 5. So should k be\nfixed for each task?",
    "start": "2149730",
    "end": "2156430"
  },
  {
    "text": "Yeah. So k can be different\nfor different tasks. You don't-- yeah, so\nit can be different.",
    "start": "2156430",
    "end": "2162360"
  },
  {
    "text": "In this case, the-- is considering, like, dogs and\nlions to be different classes,",
    "start": "2162360",
    "end": "2168720"
  },
  {
    "text": "it'd be a more fine grained\nclassification problem. But in general, k can\nvary across the tasks. ",
    "start": "2168720",
    "end": "2178170"
  },
  {
    "text": "So to recap the\ndifferent problems, we talked about\nmulti-task learning, where we want to learn a set\nof tasks, transfer learning,",
    "start": "2178170",
    "end": "2185190"
  },
  {
    "text": "where you want to transfer\nfrom one task to another task, and in the\nmeta-learning problem, we assume that we have\na set of previous tasks",
    "start": "2185190",
    "end": "2193100"
  },
  {
    "text": "and we want to quickly\nlearn a new task. And so this is somewhat similar\nto multi-task learning in that we have multiple tasks.",
    "start": "2193100",
    "end": "2200030"
  },
  {
    "text": "However, unlike\nmulti-task learning, we're trying to\nlearn a new task, and we're trying to learn\na new task efficiently.",
    "start": "2200030",
    "end": "2206016"
  },
  {
    "text": "And it's related to transfer\nlearning in the sense that we want to\nlearn a new task, except that we\nactually assume that we",
    "start": "2206017",
    "end": "2212415"
  },
  {
    "text": "have multiple previous\ntasks that we're going to learn from during\nthe training process.",
    "start": "2212415",
    "end": "2218340"
  },
  {
    "text": "Yeah? In terms of\nconceptualization, is it usually the case that\nwe assume that all--",
    "start": "2218340",
    "end": "2225880"
  },
  {
    "text": "that the meta-training phase\nis done conceptually blindly according to the new task?",
    "start": "2225880",
    "end": "2232820"
  },
  {
    "text": "Yeah, exactly. So we typically assume\nthat you don't have access to these training tasks--",
    "start": "2232820",
    "end": "2238247"
  },
  {
    "text": "like, you don't have\naccess to the test task and the training task\nat the same time. If you did have access\nto them at the same time,",
    "start": "2238247",
    "end": "2243670"
  },
  {
    "text": "then you could use\nmulti-task learning. And instead, we're going\nto be in a scenario where we kind of meta-train some\nsort of prior from the tasks",
    "start": "2243670",
    "end": "2250940"
  },
  {
    "text": "and use that to quickly\nsolve a new task. Yeah? Can you go back one slide?",
    "start": "2250940",
    "end": "2256950"
  },
  {
    "text": "Yeah. Sorry. Not that slide. This slide.",
    "start": "2256950",
    "end": "2262600"
  },
  {
    "text": "Yeah. So would it be possible--\nbecause at the end of the day, it's trying to do an N-way\nclassification for all of these",
    "start": "2262600",
    "end": "2270980"
  },
  {
    "text": "set-- like, datasets, right? So do we have any shots? Can we just do a zero-shot\nbecause the end of day,",
    "start": "2270980",
    "end": "2277970"
  },
  {
    "text": "like, overlaying\n[INAUDIBLE] all the same? Like, they give you two\nimages, and five images,",
    "start": "2277970",
    "end": "2283805"
  },
  {
    "text": "and tell me which\nprocess there are? Can I just throw it out\nthere and see what happens?",
    "start": "2283805",
    "end": "2289080"
  },
  {
    "text": "Does it need a shot at all? Yeah. So I think the question\nis that, can we just kind of combine all the\nclasses together and just use",
    "start": "2289080",
    "end": "2295160"
  },
  {
    "text": "kind of standard classification?  Basically a zero-shot learning.",
    "start": "2295160",
    "end": "2301888"
  },
  {
    "text": "Zero-shot learning. OK. Great. So yeah, so the question is\ncan you do zero-shot learning? ",
    "start": "2301888",
    "end": "2309722"
  },
  {
    "text": "In general-- so if you've never\nseen these classes before--",
    "start": "2309722",
    "end": "2315589"
  },
  {
    "text": "if you've never seen ice\ncream and dogs and so forth-- you won't\nnecessarily know what the corresponding\nclass label is for that.",
    "start": "2315590",
    "end": "2322520"
  },
  {
    "text": "We are going to see approaches\nnext week that look a little bit more like zero-shot learning\nin some way, but in general,",
    "start": "2322520",
    "end": "2330960"
  },
  {
    "text": "unless-- typically for zero-shot\nlearning approaches, you need some description or\nsome meta-data or something",
    "start": "2330960",
    "end": "2336650"
  },
  {
    "text": "to tell you about\nwhat the task is. Otherwise, you won't-- you\nsimply just won't know what",
    "start": "2336650",
    "end": "2344350"
  },
  {
    "text": "to output for lion, for example,\nor what to output for bull, and so forth-- what the\nmodel should be outputting, what the class label\ncorresponds to.",
    "start": "2344350",
    "end": "2350780"
  },
  {
    "text": " Yeah? [INAUDIBLE] framework,\nfor example.",
    "start": "2350780",
    "end": "2359006"
  },
  {
    "text": "You say you've given these\nsay for each test image. Which [INAUDIBLE]?",
    "start": "2359007",
    "end": "2364874"
  },
  {
    "text": " Yeah, exactly. So next week we'll cover\nmetric learning approaches",
    "start": "2364874",
    "end": "2370240"
  },
  {
    "text": "that are going to\nlook a little bit more like zero-shot learning. That will be on, I\nthink, Monday next week,",
    "start": "2370240",
    "end": "2375849"
  },
  {
    "text": "and those will look a lot\nmore like zero-shot learning approaches. Yeah? It seems like the [INAUDIBLE]\nyou had with meta-learning",
    "start": "2375850",
    "end": "2383073"
  },
  {
    "text": "is that you can work\nwith your task quickly. Are there examples where\nyou can actually work [INAUDIBLE] task [INAUDIBLE]?",
    "start": "2383073",
    "end": "2390770"
  },
  {
    "text": " Yeah. So the question is,\nare there examples",
    "start": "2390770",
    "end": "2396760"
  },
  {
    "text": "of actually being able to\nlearn new tasks quickly using meta-learning, and I guess\nthe short answer is yes.",
    "start": "2396760",
    "end": "2404740"
  },
  {
    "text": "And the-- I mean,\nyou've seen us all try to cover some different\napplications of meta-learning",
    "start": "2404740",
    "end": "2410830"
  },
  {
    "text": "in the next few lectures. but one example that we'll\nsee, we applied meta-learning",
    "start": "2410830",
    "end": "2417220"
  },
  {
    "text": "to education, and we were\nable to essentially give feedback on an entirely new exam\nwhere instructors of the course",
    "start": "2417220",
    "end": "2426340"
  },
  {
    "text": "simply gave feedback on a\nsmall number of examples and then the system was\nable to give feedback on the remainder of\nthe student's exams.",
    "start": "2426340",
    "end": "2432820"
  },
  {
    "start": "2432820",
    "end": "2438620"
  },
  {
    "text": "So going back to\nthe problem setting, in transfer learning\nand meta-learning",
    "start": "2438620",
    "end": "2444890"
  },
  {
    "text": "is generally assumed to\nbe impractical to access the previous tasks because\nmaybe we have a lot of data",
    "start": "2444890",
    "end": "2450112"
  },
  {
    "text": "and we don't want to\ncontinuously retrain on all of those\nprevious tasks and data. We want to be able to just\nquickly, both computationally",
    "start": "2450112",
    "end": "2457010"
  },
  {
    "text": "and data efficiently,\nsolve a new task. ",
    "start": "2457010",
    "end": "2462680"
  },
  {
    "text": "In all of these\nsettings, we want the task to be able to\nshare some structure. ",
    "start": "2462680",
    "end": "2469890"
  },
  {
    "text": "OK, so we've talked\nabout the problem. What is the approach\nfor actually going about solving this problem?",
    "start": "2469890",
    "end": "2476370"
  },
  {
    "text": "So one way to view this\nproblem is kind of in contrast",
    "start": "2476370",
    "end": "2482312"
  },
  {
    "text": "to supervised learning. So in supervised learning, we\nhave inputs, x, and outputs, y. We learn a mapping from\nx to y using a dataset",
    "start": "2482312",
    "end": "2489050"
  },
  {
    "text": "of input/output pairs. And in meta-learning, or kind\nof the supervised learning",
    "start": "2489050",
    "end": "2497600"
  },
  {
    "text": "variant of meta-learning,\nnow instead of having kind of\ninput x, output y,",
    "start": "2497600",
    "end": "2502940"
  },
  {
    "text": "it's a little bit\nmore complicated. So we have a training\ndataset, the training dataset has k examples,\nand we want to be",
    "start": "2502940",
    "end": "2509643"
  },
  {
    "text": "able to learn from\nthis training dataset in a way that allows us to\nclassify new examples, x test.",
    "start": "2509643",
    "end": "2515910"
  },
  {
    "text": "And so if we want to\nclassify new example x test, then the\noutput will be y test.",
    "start": "2515910",
    "end": "2522990"
  },
  {
    "text": "And so in some ways, you can\nview the meta-learning process as trying to learn a mapping\nfrom a dataset and a test",
    "start": "2522990",
    "end": "2528410"
  },
  {
    "text": "example to the corresponding\nlabel for that test example.",
    "start": "2528410",
    "end": "2535160"
  },
  {
    "text": "And the way that we can go\nabout learning this mapping is with a dataset of datasets,\nwhere each of these datasets",
    "start": "2535160",
    "end": "2542510"
  },
  {
    "text": "is going to have more than k\nexamples-- so you can sample k for the training\ndataset, and at least one",
    "start": "2542510",
    "end": "2548330"
  },
  {
    "text": "to be used as the test example.  And this view is\npretty nice because it",
    "start": "2548330",
    "end": "2556500"
  },
  {
    "text": "means that if we view\nthe process of learning from small amounts\nof data in this way,",
    "start": "2556500",
    "end": "2562270"
  },
  {
    "text": "then we simply just need\nto design this function f that can take as input\na dataset and generalize from that dataset.",
    "start": "2562270",
    "end": "2569448"
  },
  {
    "text": "And once we design\nthis, then we just need to optimize it\nusing a set of datasets. ",
    "start": "2569448",
    "end": "2576550"
  },
  {
    "text": "So f is essentially\nrepresenting some form of learning procedure,\nand we're going to be optimizing this\nlearning procedure in order",
    "start": "2576550",
    "end": "2583690"
  },
  {
    "text": "to learn from data-- learn\nfrom small amounts of data. ",
    "start": "2583690",
    "end": "2590589"
  },
  {
    "text": "And so from this\nstandpoint, we can-- when designing a\nmeta-learning algorithm, we can essentially choose a\nform of this learning procedure,",
    "start": "2590590",
    "end": "2599430"
  },
  {
    "text": "parameterize that learning\nprocedure in some way, and then choose how to optimize\nthe parameters of that learning",
    "start": "2599430",
    "end": "2606120"
  },
  {
    "text": "procedure using some form of\nmaximum likelihood objective on the meta-training dataset.",
    "start": "2606120",
    "end": "2612210"
  },
  {
    "text": " And I'll refer to theta\nas the meta parameters.",
    "start": "2612210",
    "end": "2618440"
  },
  {
    "text": "These are the parameters\nthat kind of parameterizes learning procedure so that\nyou can learn from data.",
    "start": "2618440",
    "end": "2623950"
  },
  {
    "start": "2623950",
    "end": "2629880"
  },
  {
    "text": "OK, so this is the\ngeneral recipe. Now let's actually\ntalk about one way to instantiate this recipe.",
    "start": "2629880",
    "end": "2636165"
  },
  {
    "text": "And in particular,\nwe're going to cover three different\npossible instantiations. One will be referred to\nas black-box approaches,",
    "start": "2636165",
    "end": "2641867"
  },
  {
    "text": "another will be referred to as\noptimization-based approaches-- which we'll cover on\nWednesday-- and the last will be metric\nlearning approaches,",
    "start": "2641867",
    "end": "2647990"
  },
  {
    "text": "or these more non-parametric\nlearning methods. ",
    "start": "2647990",
    "end": "2653940"
  },
  {
    "text": "OK, so the key idea behind\nthese black-box approaches is to essentially parameterize\nthe learning procedure",
    "start": "2653940",
    "end": "2661800"
  },
  {
    "text": "as a neural network. And in particular, you can\njust have a neural network",
    "start": "2661800",
    "end": "2668220"
  },
  {
    "text": "take as input a dataset and\noutput parameters of a model from that dataset.",
    "start": "2668220",
    "end": "2674840"
  },
  {
    "text": "And then once you\noutput these parameters, then you can just try to\npredict the label for new data",
    "start": "2674840",
    "end": "2680250"
  },
  {
    "text": "points using a\nnetwork parameterized by those parameters phi. ",
    "start": "2680250",
    "end": "2687230"
  },
  {
    "text": "And so essentially, what\nthis might look like is we have some neural\nnetwork here-- it",
    "start": "2687230",
    "end": "2692265"
  },
  {
    "text": "might be a recurrent neural\nnetwork that takes its input, the data points from\nyour training dataset, and then uses that to output\na set of parameters, phi,",
    "start": "2692265",
    "end": "2700790"
  },
  {
    "text": "and then you have another\nneural network that uses those parameters\nphi to make predictions",
    "start": "2700790",
    "end": "2706250"
  },
  {
    "text": "for new examples. And so this is essentially\nD train and D test, or your support set\nand your query set.",
    "start": "2706250",
    "end": "2713600"
  },
  {
    "text": "And then you can train\nthese neural networks, a standard supervised\nlearning across all",
    "start": "2713600",
    "end": "2718760"
  },
  {
    "text": "of the tasks in your\nmeta-training set. So essentially, sum over all of\nyour possible tasks and measure",
    "start": "2718760",
    "end": "2729380"
  },
  {
    "text": "how well the model\nwith parameters phi is able to classify\nnew examples.",
    "start": "2729380",
    "end": "2734495"
  },
  {
    "start": "2734495",
    "end": "2741173"
  },
  {
    "text": "OK, and then if we refer\nto this as kind of the loss function of the parameters phi\non your held out data points",
    "start": "2741173",
    "end": "2746850"
  },
  {
    "text": "D test, then we can also write\nthis loss function like this.",
    "start": "2746850",
    "end": "2751900"
  },
  {
    "text": "So essentially, we're using f\nto predict the parameters phi, and then measuring how will\nthe parameters phi generalize",
    "start": "2751900",
    "end": "2758550"
  },
  {
    "text": "to test examples. Yeah? Does is make sense to\ncondition phi i on x ts--",
    "start": "2758550",
    "end": "2768560"
  },
  {
    "text": "like, actually have\nthe test input-- like, do do the parameters\ndepend on the test input?",
    "start": "2768560",
    "end": "2774450"
  },
  {
    "text": "Yeah. So essentially,\ndoes it make sense to have the learning procedure\ntake as input the test input and use that in the learning\nprocess in some way?",
    "start": "2774450",
    "end": "2782779"
  },
  {
    "text": "I haven't seen that\ndone before, but I could imagine some\napplications where maybe knowing what you're going\nto be asked to do at test time",
    "start": "2782780",
    "end": "2789980"
  },
  {
    "text": "is informative for\nthe learning process. Yeah? [INAUDIBLE] parameterize.",
    "start": "2789980",
    "end": "2797190"
  },
  {
    "text": "So does it mean you have\nto individually train the parameter phi for each task?",
    "start": "2797190",
    "end": "2803390"
  },
  {
    "text": "Or you can do it\nlike another way? Yeah, that's a great question.",
    "start": "2803390",
    "end": "2808650"
  },
  {
    "text": "So i here is indexing the\ndifferent tasks, and phi i,",
    "start": "2808650",
    "end": "2814069"
  },
  {
    "text": "you could actually think\nof more as an activation of this network, rather\nthan actually as a parameter",
    "start": "2814070",
    "end": "2820640"
  },
  {
    "text": "or as weights-- essentially something that\nthis network is producing, the neural networks\noutputting weights",
    "start": "2820640",
    "end": "2826280"
  },
  {
    "text": "of another neural network,\nand then the output of that is using-- is being used as\nweights of this network g here.",
    "start": "2826280",
    "end": "2835320"
  },
  {
    "text": "And so it's not something--\nphi i isn't something that you actually\nrepresent separately, it's something that this\nneural network is producing.",
    "start": "2835320",
    "end": "2843770"
  },
  {
    "text": "Yeah? So are you going to directly\noptimize this data here? Yeah, exactly. So this-- as you can\nsee in the optimization,",
    "start": "2843770",
    "end": "2851359"
  },
  {
    "text": "this is optimizing the\nweights of this network here, and you're not actually\noptimizing phi.",
    "start": "2851360",
    "end": "2857250"
  },
  {
    "text": "Yeah? So we are maximizing the\nloss, but the log likelihood [INAUDIBLE]?",
    "start": "2857250",
    "end": "2862300"
  },
  {
    "text": " Yeah. So this is showing maximizing\nthe log likelihood.",
    "start": "2862300",
    "end": "2868040"
  },
  {
    "text": "I guess I should also have\na negative sign there. This could correspond to\nthe negative loss function",
    "start": "2868040",
    "end": "2873800"
  },
  {
    "text": "where you're minimizing the\nnegative log likelihood. And yeah, I should\nessentially put a--",
    "start": "2873800",
    "end": "2881210"
  },
  {
    "text": "I don't think I have\na way to edit this-- but I should put a negative\nsign in the second two rows",
    "start": "2881210",
    "end": "2888680"
  },
  {
    "text": "on the bottom right. Yeah? So both phi depend--\nphi [INAUDIBLE]----",
    "start": "2888680",
    "end": "2894156"
  },
  {
    "text": "doesn't that mean that you're\ngoing to create both networks [INAUDIBLE]? ",
    "start": "2894156",
    "end": "2903690"
  },
  {
    "text": "So you can think of phi, again,\nas a bit of an activation. And so you're not actually\ntraining phi in any way,",
    "start": "2903690",
    "end": "2909530"
  },
  {
    "text": "you're only training\nthe parameters of this neural network,\nand you're training it to output parameters--",
    "start": "2909530",
    "end": "2915710"
  },
  {
    "text": "output weights for\nanother neural network so that other neural\nnetwork works well. And so phi isn't actually\nbeing optimized here per se,",
    "start": "2915710",
    "end": "2923000"
  },
  {
    "text": "it's only theta that's\nbeing optimized. ",
    "start": "2923000",
    "end": "2929375"
  },
  {
    "text": "So now, let's look at an\nexample of how we could actually go about training\nsomething like this.",
    "start": "2929375",
    "end": "2934580"
  },
  {
    "text": "So what we'll do is\nwe can first sample a task from our set of\ntraining tasks, then we--",
    "start": "2934580",
    "end": "2942010"
  },
  {
    "text": "or a mini batch of tasks-- then we'll sample a support\nset and a query set,",
    "start": "2942010",
    "end": "2947260"
  },
  {
    "text": "or a train set and a test set,\nfrom the dataset for that task.",
    "start": "2947260",
    "end": "2953960"
  },
  {
    "text": "So say, for example,\nour task is to classify this kind of dog, landscapes,\npianos, and barrels.",
    "start": "2953960",
    "end": "2961450"
  },
  {
    "text": "Then what we'll do is we'll\nsample a set of images for that particular task--",
    "start": "2961450",
    "end": "2968680"
  },
  {
    "text": "that N-way classification task,\nthen we will randomly partition that into a training\nset and a test set--",
    "start": "2968680",
    "end": "2975260"
  },
  {
    "text": "so we'll randomly\nchoose some images to be used as a support\nset and some images to be used as the query set.",
    "start": "2975260",
    "end": "2982270"
  },
  {
    "text": "Then once we have a support\nset and a query set, we'll compute-- we'll run f forward to\nget a set of weights",
    "start": "2982270",
    "end": "2990370"
  },
  {
    "text": "for this particular\ntask, and then we will evaluate how\nwell the network g",
    "start": "2990370",
    "end": "2996560"
  },
  {
    "text": "phi does on held out examples\nand back propagate our loss to update theta so that it\neffectively learned-- training",
    "start": "2996560",
    "end": "3005452"
  },
  {
    "text": "to effectively learn from\nthis training dataset in order to classify\nnew examples. ",
    "start": "3005452",
    "end": "3013799"
  },
  {
    "text": "And then we can\nrepeat this process. Again, sampling a\nnew set of tasks and updating the learner so that\nit generalizes to new examples.",
    "start": "3013800",
    "end": "3022505"
  },
  {
    "text": "Yeah? How would the\noutput loss function",
    "start": "3022505",
    "end": "3027990"
  },
  {
    "text": "work to produce the output,\nthe network be included? Because say you're doing a\nquick project or something.",
    "start": "3027990",
    "end": "3036329"
  },
  {
    "text": "Wouldn't the classes that\ncorrespond to [INAUDIBLE]?? ",
    "start": "3036330",
    "end": "3045490"
  },
  {
    "text": "So you're asking how do you-- how do you\nparameterize phi i, or? I guess the y ts.",
    "start": "3045490",
    "end": "3052089"
  },
  {
    "text": "How would you-- how\nwould that look? Yeah, that's a good question. So in this particular case\nof image classification",
    "start": "3052090",
    "end": "3059850"
  },
  {
    "text": "where you're sampling\nessentially a set of tasks, you also need a\nsample-- you also need to choose the label for\neach of these image classes.",
    "start": "3059850",
    "end": "3067410"
  },
  {
    "text": "And what's typically done\nin this sort of image classification example is to-- when you sample-- in the\nfive-way classification",
    "start": "3067410",
    "end": "3074610"
  },
  {
    "text": "problem, when you\nsample five images, you just assign a one-hot ID\nfor each of those five classes",
    "start": "3074610",
    "end": "3080820"
  },
  {
    "text": "at random. And so you'll assign a\nlabel of 0 for barrel, a label of 1 for\nthis kind of dog, a label of 2 for\nlandscape, and so forth.",
    "start": "3080820",
    "end": "3089470"
  },
  {
    "text": "And this does mean that across\ndifferent tasks, the label of 0 will mean different\nthings, and a barrel might",
    "start": "3089470",
    "end": "3095820"
  },
  {
    "text": "be assigned to\nlabel 0 in one task, and assigned to a different\nlabel in another task.",
    "start": "3095820",
    "end": "3101010"
  },
  {
    "text": "And that sort of\napproach is actually somewhat beneficial in that\nit prevents the network from essentially memorizing\nthe classes of the labels",
    "start": "3101010",
    "end": "3109860"
  },
  {
    "text": "of certain classes and\nallows it to generalize to new image classes which might\nhave different labels assigned.",
    "start": "3109860",
    "end": "3116549"
  },
  {
    "text": "Yeah? Does this usually remain\nfully differentiable? Like, can we just take\nthe gradient with respect to theta or something like that?",
    "start": "3116550",
    "end": "3125365"
  },
  {
    "text": "So the question is, is this\nkind of fully differentiable, and can we take the\ngradient of theta directly with respect to the\nperformance on the test data",
    "start": "3125365",
    "end": "3132270"
  },
  {
    "text": "points? The answer here is yes. If you-- well, in\ngeneral, it should be yes.",
    "start": "3132270",
    "end": "3140100"
  },
  {
    "text": "There are scenarios\nwhere maybe you parameterize g in a way that\nis isn't differentiable,",
    "start": "3140100",
    "end": "3145710"
  },
  {
    "text": "but in that case, you wouldn't\nbe able to optimize it as a normal neural network. In general, neural networks\nare fully differentiable,",
    "start": "3145710",
    "end": "3155220"
  },
  {
    "text": "and so you can essentially\njust differentiate directly through phi into theta so\nthat you don't actually",
    "start": "3155220",
    "end": "3160920"
  },
  {
    "text": "have to optimize phi. Yeah? Would we generally parameterize\nall weights in g phi,",
    "start": "3160920",
    "end": "3168470"
  },
  {
    "text": "or would we already\nhave some of the weight functions [INAUDIBLE]? Yeah. So that's a great\nquestion, and we'll",
    "start": "3168470",
    "end": "3174910"
  },
  {
    "text": "talk about that\non the next slide. So one thing that\nyou might notice is that if g phi is a\nhuge neural network,",
    "start": "3174910",
    "end": "3183280"
  },
  {
    "text": "then outputting all\nof the parameters for a huge neural network\nseems a bit impractical.",
    "start": "3183280",
    "end": "3188950"
  },
  {
    "text": "It doesn't seem very scalable. And so you don't actually need\nto output all the parameters",
    "start": "3188950",
    "end": "3194109"
  },
  {
    "text": "of a neural network. What you can do is essentially\njust output something,",
    "start": "3194110",
    "end": "3199497"
  },
  {
    "text": "like just the submission\nstatistics or just output the parameters of the\nlast layer of the network, and use other parameters\nin g that are kind of",
    "start": "3199497",
    "end": "3207369"
  },
  {
    "text": "fixed across all of the tasks. And so one way that\nyou could look at this is we can take the\ndiagram that we had before",
    "start": "3207370",
    "end": "3214660"
  },
  {
    "text": "and replace phi with\nessentially just the kind of,",
    "start": "3214660",
    "end": "3219940"
  },
  {
    "text": "as some sort of hidden vector\nlike a recurrent neural network, and so we'll have\nthis low dimensional vector hi,",
    "start": "3219940",
    "end": "3228910"
  },
  {
    "text": "and this can represent\nessentially contextual task information, and you could have\nother parts of g that have--",
    "start": "3228910",
    "end": "3236770"
  },
  {
    "text": "instead of-- essentially share\nsome parameters between theta and other parts of g so\nit's using both theta",
    "start": "3236770",
    "end": "3242320"
  },
  {
    "text": "and hi in order\nto solve the task. ",
    "start": "3242320",
    "end": "3247400"
  },
  {
    "text": "And so in this case, phi i\nwill correspond to the hidden parameter-- the hidden\nrepresentation h,",
    "start": "3247400",
    "end": "3253510"
  },
  {
    "text": "as well as some other\nparameters of theta, which I'll refer to as\ntheta sub g, which will be--",
    "start": "3253510",
    "end": "3260860"
  },
  {
    "text": "which could be some\nseparate parameters that are specific to g. In the case of an\nRNN, it could also be things that are\nshared with f as well.",
    "start": "3260860",
    "end": "3267640"
  },
  {
    "text": " Now, one thing that you--",
    "start": "3267640",
    "end": "3273474"
  },
  {
    "text": "one thing that could\nbe useful in terms of thinking about\nthis sort of approach is if we recall the\nmulti-task learning approach,",
    "start": "3273475",
    "end": "3279583"
  },
  {
    "text": "where you have a network that\ntakes as input a description of the task, you\ncould essentially",
    "start": "3279583",
    "end": "3285270"
  },
  {
    "text": "think of the description\nof the task as this hi, or in some more general\nform, you could essentially",
    "start": "3285270",
    "end": "3291210"
  },
  {
    "text": "view the description\nof the task zi as just corresponding to\na dataset for that task.",
    "start": "3291210",
    "end": "3299605"
  },
  {
    "text": "So essentially,\ninstead of conditioning on a task description,\nyou're conditioning the network on a dataset. ",
    "start": "3299605",
    "end": "3310800"
  },
  {
    "text": "Cool. Any questions on how this\nworks before we move on? So this is sort of\nexcluding the change",
    "start": "3310800",
    "end": "3318415"
  },
  {
    "text": "in velocity of the [INAUDIBLE]?",
    "start": "3318415",
    "end": "3323560"
  },
  {
    "text": "So the question is, is\nthis excluding-- like, does this mean that you\nhave to have the same loss function for all of the tasks?",
    "start": "3323560",
    "end": "3328829"
  },
  {
    "text": "Yeah. But [INAUDIBLE]. ",
    "start": "3328830",
    "end": "3334460"
  },
  {
    "text": "Yeah. This is a good question. So actually-- I mean, this-- the way that I presented\nthis actually means like-- in general, I think\nthat people typically",
    "start": "3334460",
    "end": "3340770"
  },
  {
    "text": "assume that you have the\nsame loss function across all of the tasks. In principle, instead of\nonly passing in the dataset,",
    "start": "3340770",
    "end": "3346050"
  },
  {
    "text": "you could also pass\nsome information about the loss function\nif the loss function",
    "start": "3346050",
    "end": "3351370"
  },
  {
    "text": "is training across tasks. Yeah? So just [INAUDIBLE],, consider\ng being parameterized",
    "start": "3351370",
    "end": "3358871"
  },
  {
    "text": "on the network\nparameters outputted by f sub theta [INAUDIBLE]? ",
    "start": "3358871",
    "end": "3370440"
  },
  {
    "text": "Exactly. Yeah. What exactly is gw\ncompared to [INAUDIBLE]??",
    "start": "3370440",
    "end": "3376750"
  },
  {
    "text": "Yeah. So the-- there's a\nfew different ways that you can think of this. Theta g might be something\nthat's completely separate,",
    "start": "3376750",
    "end": "3383340"
  },
  {
    "text": "that's optimized alongside\nthe parameters of f. It could also be\nthings that are-- something that's shared with f.",
    "start": "3383340",
    "end": "3388890"
  },
  {
    "text": "So if you essentially\njust kind of collapse these two things\ntogether, then you can view it as just a\nsingle recurrent neural",
    "start": "3388890",
    "end": "3394080"
  },
  {
    "text": "network that has-- that kind of shares all\nof its parameters, and hi",
    "start": "3394080",
    "end": "3399870"
  },
  {
    "text": "just corresponds to the\nrecurrent state of that RNN.",
    "start": "3399870",
    "end": "3407040"
  },
  {
    "text": "Thank you.  Now speaking of RNNs, one\nnatural question is like,",
    "start": "3407040",
    "end": "3415970"
  },
  {
    "text": "this is kind of one\nway to represent it, we have a black-box neural\nnetwork that takes as input a dataset and a new test example\nand outputs predictions--",
    "start": "3415970",
    "end": "3423740"
  },
  {
    "text": "now what architecture should\nwe actually use for f theta? ",
    "start": "3423740",
    "end": "3429770"
  },
  {
    "text": "There are a variety\nof choices here. One example, or one of the\nkind of initial examples",
    "start": "3429770",
    "end": "3434809"
  },
  {
    "text": "that looked at this\nkind of problem, used either LSTMs or what's\nreferred to as a Neural Turing",
    "start": "3434810",
    "end": "3440420"
  },
  {
    "text": "machine, which has this kind\nof form of external memory that it's using to kind of\nin the learning process.",
    "start": "3440420",
    "end": "3446870"
  },
  {
    "text": " Now, a more recent architecture\nthat I think is quite nice",
    "start": "3446870",
    "end": "3453410"
  },
  {
    "text": "is something that,\ninstead of using some sort of recurrent neural\nnetwork or an external memory",
    "start": "3453410",
    "end": "3458450"
  },
  {
    "text": "mechanism, is what it\ndoes is it just takes all the inputs from\nyour training examples and runs a feedforward\nneural network on each",
    "start": "3458450",
    "end": "3467420"
  },
  {
    "text": "of those training examples, and\nthen averages the predictions-- averages the embeddings of\nthat feedforward network",
    "start": "3467420",
    "end": "3473960"
  },
  {
    "text": "into a single context, and\nthen that single context is",
    "start": "3473960",
    "end": "3479089"
  },
  {
    "text": "used by the network g in\norder to make predictions. ",
    "start": "3479090",
    "end": "3485110"
  },
  {
    "text": "So does anyone have\nany thoughts on why something like a feedforward\nand then averaging architecture",
    "start": "3485110",
    "end": "3490410"
  },
  {
    "text": "for computing the context\nmight be better than something like a recurrent neural network?",
    "start": "3490410",
    "end": "3496190"
  },
  {
    "text": "Yeah? First, the [INAUDIBLE] to see\nthat there is one [INAUDIBLE]",
    "start": "3496190",
    "end": "3501230"
  },
  {
    "text": "data, which is not this case. And so the average [INAUDIBLE].",
    "start": "3501230",
    "end": "3508080"
  },
  {
    "text": "Yeah, exactly. So a recurrent\narchitecture is going to assume that there is\na particular ordering to each of the datasets.",
    "start": "3508080",
    "end": "3513690"
  },
  {
    "text": "It would-- in reality, a\ndataset is usually a set, not a sequence-- and a\nfeedforward and averaging",
    "start": "3513690",
    "end": "3519180"
  },
  {
    "text": "architecture will\nessentially encode the fact that data points are\npermutation invariant and it doesn't matter the order\nof the particular data points.",
    "start": "3519180",
    "end": "3527642"
  },
  {
    "text": "And so this is kind\nof the main advantage of an architecture like this. One additional architecture--\none additional advantage",
    "start": "3527642",
    "end": "3533520"
  },
  {
    "text": "is that it is often\ncomputationally cheaper to run feedforward in averaging\nbecause you can completely",
    "start": "3533520",
    "end": "3538560"
  },
  {
    "text": "parallelize things, whereas\nin recurrent neural networks, you can't parallelize\neach step as easily. ",
    "start": "3538560",
    "end": "3545670"
  },
  {
    "text": "But the permutation and variance\nis really the main benefit.",
    "start": "3545670",
    "end": "3551260"
  },
  {
    "text": "Yeah? Could one also be that\nif you have [INAUDIBLE]",
    "start": "3551260",
    "end": "3557380"
  },
  {
    "text": "augment the data [INAUDIBLE]? Yeah, definitely. So if you want a really\nlarge support set,",
    "start": "3557380",
    "end": "3564369"
  },
  {
    "text": "an RNN can certainly\nforget things over time, whereas a sort of\nfeedforward and average,",
    "start": "3564370",
    "end": "3569790"
  },
  {
    "text": "it can sort of also\nforget things in some way, but it's not going\nto be forgetting in a sequence dependent way.",
    "start": "3569790",
    "end": "3575940"
  },
  {
    "text": " OK, there's also some\nother architectures",
    "start": "3575940",
    "end": "3582610"
  },
  {
    "text": "that people have used as well,\nother kinds of external memory mechanisms. People have also used things\nlike attention as well,",
    "start": "3582610",
    "end": "3590410"
  },
  {
    "text": "and temporal convolutions. Things like attention are\nalso nice in the sense that they are also permutation\ninvariant, depending on some",
    "start": "3590410",
    "end": "3598420"
  },
  {
    "text": "of the implementation details. ",
    "start": "3598420",
    "end": "3603950"
  },
  {
    "text": "OK. And then if you actually\nrun the architecture",
    "start": "3603950",
    "end": "3609050"
  },
  {
    "text": "on the far right on a few shot\nimage classification problem, like the Omniglot dataset,\nwhich is a character recognition",
    "start": "3609050",
    "end": "3616220"
  },
  {
    "text": "dataset, you can see\nthat you can perform one-shot five-way\nimage classification",
    "start": "3616220",
    "end": "3623450"
  },
  {
    "text": "with around 99% accuracy\nfor new characters that it's never seen before.",
    "start": "3623450",
    "end": "3629040"
  },
  {
    "text": "So essentially, learn\nto recognize, yeah, new characters with just a\nsingle example per character,",
    "start": "3629040",
    "end": "3636300"
  },
  {
    "text": "which is pretty cool.  And then beyond the\ncharacter recognition task,",
    "start": "3636300",
    "end": "3642760"
  },
  {
    "text": "we can actually also look\nat the mini ImageNet task-- this is actually the exact\nexample that I was showing on the previous slides--",
    "start": "3642760",
    "end": "3649000"
  },
  {
    "text": "and architecture like\nthe one on the far right gets around a 55%\nor 68% accuracy",
    "start": "3649000",
    "end": "3656020"
  },
  {
    "text": "on a five-way\nclassification problem when you're either given\none example per class or five examples per class.",
    "start": "3656020",
    "end": "3661840"
  },
  {
    "text": " Cool.",
    "start": "3661840",
    "end": "3667210"
  },
  {
    "text": "And so in homework\n1, you're going to be implementing kind of\nthe data processing training",
    "start": "3667210",
    "end": "3673900"
  },
  {
    "text": "setup essentially for these\nkinds of few-shot image classification problems,\nyou'll implement a very simple",
    "start": "3673900",
    "end": "3679240"
  },
  {
    "text": "black-box meta-learner,\nand then also just train a few-shot Omniglot\nclassifier and hopefully get",
    "start": "3679240",
    "end": "3686920"
  },
  {
    "text": "numbers that aren't too\nfar off from the numbers that we saw there. ",
    "start": "3686920",
    "end": "3694230"
  },
  {
    "text": "Great. Any more questions? Or I'm going to-- I'm thinking about running\nthrough an example.",
    "start": "3694230",
    "end": "3700167"
  },
  {
    "text": " So another example, and in\nparticular, in your homework",
    "start": "3700167",
    "end": "3709830"
  },
  {
    "text": "you'll be using the\nOmniglot dataset. This is a dataset\nthat has 1,600-- 1,623 characters from 50\ndifferent alphabets, and so",
    "start": "3709830",
    "end": "3719730"
  },
  {
    "text": "an example of some of the\nalphabets are shown here. And each character has\n20 different instances--",
    "start": "3719730",
    "end": "3727750"
  },
  {
    "text": "so essentially, they\nhad 20 different people write that character and for all\nthe characters in the dataset.",
    "start": "3727750",
    "end": "3736069"
  },
  {
    "text": "And one of the things that's\nkind of cool about this dataset is it has many different\ncharacter classes and only",
    "start": "3736070",
    "end": "3742290"
  },
  {
    "text": "a few examples per\ncharacter class. And in some ways, this is\nkind of like the transpose",
    "start": "3742290",
    "end": "3747480"
  },
  {
    "text": "of a lot of common datasets. It's kind of like the\ntranspose of MNIST because in MNIST\nyou have 10 classes and you have thousands\nof examples per class.",
    "start": "3747480",
    "end": "3755559"
  },
  {
    "text": "And I also think\nthat the statistics of a dataset like\nthis are actually much more reflective of\nthe real world, where",
    "start": "3755560",
    "end": "3761175"
  },
  {
    "text": "you have kind of very broad\ndata and a very small amount of examples per class.",
    "start": "3761175",
    "end": "3766860"
  },
  {
    "text": "For example, kind of\nprobably over your lifetime, for many objects,\nyou probably haven't",
    "start": "3766860",
    "end": "3772380"
  },
  {
    "text": "seen like thousands of\ndifferent instances of that object before. You've probably only seen--",
    "start": "3772380",
    "end": "3777881"
  },
  {
    "text": "well, depending on\nhow old you are, maybe you've seen\nonly a few instances, maybe you've seen maybe 100\ninstances, but probably not",
    "start": "3777882",
    "end": "3784950"
  },
  {
    "text": "a ton.  OK< so I want to run through\nan example from this problem,",
    "start": "3784950",
    "end": "3791640"
  },
  {
    "text": "and this, I think, will\nshould be quite helpful for your homework. ",
    "start": "3791640",
    "end": "3798520"
  },
  {
    "text": "So we'll see that we're doing-- let's say that we're doing\nkind of three-way one-shot",
    "start": "3798520",
    "end": "3805500"
  },
  {
    "text": "classification, and we want to\nbe able to do this on Omniglot. And so at test time, or at\nmeta-test time we're given",
    "start": "3805500",
    "end": "3817170"
  },
  {
    "text": "a new task and we have a\ntraining dataset for that task,",
    "start": "3817170",
    "end": "3822630"
  },
  {
    "text": "which will be three\ndifferent images-- maybe a, b, and c from\nthe English alphabet--",
    "start": "3822630",
    "end": "3830369"
  },
  {
    "text": "and we'll assign a label\nfor each of these images-- maybe y equals 0, y\nequals 1, and y equals 2.",
    "start": "3830370",
    "end": "3839370"
  },
  {
    "text": "And then we also\nhave a test example, which will be an image for\nmaybe a written in a slightly",
    "start": "3839370",
    "end": "3848550"
  },
  {
    "text": "different way, and\nyou want your goal is to be able to predict\nthe correct label for this, which is y equals 0.",
    "start": "3848550",
    "end": "3855660"
  },
  {
    "text": "So this is what we\nhave at meta-test time. ",
    "start": "3855660",
    "end": "3860730"
  },
  {
    "text": "And then-- we'll do\nall meta-training on this whiteboard. So kind of during meta-training,\nwhat should happen?",
    "start": "3860730",
    "end": "3871420"
  },
  {
    "text": "So maybe the English\nalphabet is held out-- or at least these characters\nfrom the English alphabet",
    "start": "3871420",
    "end": "3876450"
  },
  {
    "text": "are completely held\nout, and so what",
    "start": "3876450",
    "end": "3883349"
  },
  {
    "text": "is maybe kind of the\nfirst thing that we should do during meta-training?",
    "start": "3883350",
    "end": "3888930"
  },
  {
    "text": "If we're-- or, if we kind of\nwant to compute a gradient step on our network, what's the\nfirst thing that we should do?",
    "start": "3888930",
    "end": "3895434"
  },
  {
    "text": "Sample an alphabet. Sample-- sorry? Sample an alphabet. Yeah, so simple an alphabet.",
    "start": "3895434",
    "end": "3901680"
  },
  {
    "text": "I'm going to do something\na little bit more crude and simply, we're going to\nkind of put all the alphabets together, and we're\ngoing to sample",
    "start": "3901680",
    "end": "3907638"
  },
  {
    "text": "three different characters. They could be from\nthe same alphabet, they could be from\ndifferent alphabets,",
    "start": "3907638",
    "end": "3916770"
  },
  {
    "text": "and we're going to\nbe in the setting where we're just\ngoing to kind of train for the three-way one-shot\nclassification problem,",
    "start": "3916770",
    "end": "3922060"
  },
  {
    "text": "so that's why we're going to\nbe sampling three characters. And then we're going to be\nsampling multiple instances",
    "start": "3922060",
    "end": "3931470"
  },
  {
    "text": "per character. And because we're in\nthe one-shot setting, we want to sample at least one\ninstance for the support set,",
    "start": "3931470",
    "end": "3938210"
  },
  {
    "text": "and we also need to be able\nto sample some instances for the query set. And so we're going to\nsample kind of two--",
    "start": "3938210",
    "end": "3949530"
  },
  {
    "text": "I guess two images\nkind of per character.",
    "start": "3949530",
    "end": "3959617"
  },
  {
    "text": "Is this-- can everyone\nsee this, by the way? Is this big enough? Cool. ",
    "start": "3959617",
    "end": "3966870"
  },
  {
    "text": "OK, and then we can just\nsample these at random, and then we need to--",
    "start": "3966870",
    "end": "3972359"
  },
  {
    "text": "maybe I can draw this out. Maybe we have alpha--",
    "start": "3972360",
    "end": "3978660"
  },
  {
    "text": "I guess these actually are\nfrom the same alphabet-- alpha, beta, gamma, and then\nwe'll also have--",
    "start": "3978660",
    "end": "3984299"
  },
  {
    "text": "we'll sample other\ninstances that are maybe drawn slightly differently.",
    "start": "3984300",
    "end": "3992592"
  },
  {
    "text": "Then once we have\nthese samples, we need to use some for the support\nset and some for the query set. So we will kind of assign these\nto be all for the support set",
    "start": "3992592",
    "end": "4003160"
  },
  {
    "text": "and all of these\nfor the query set, and then we'll also\nneed to assign labels for each of these. So maybe we'll give this a\nlabel of 2, a label of 1,",
    "start": "4003160",
    "end": "4011708"
  },
  {
    "text": "and a label of 0-- and likewise,\nthis will have a label of 2, a label of 1, and a label of 0. ",
    "start": "4011708",
    "end": "4020742"
  },
  {
    "text": "OK, and then from\nhere, we're essentially ready to actually pass\nthis data into the network. So this is kind of the\ndata processing step.",
    "start": "4020742",
    "end": "4029895"
  },
  {
    "text": "And then once we\nhave this, we'll essentially kind of pass D\ntrain into our neural network.",
    "start": "4029895",
    "end": "4038170"
  },
  {
    "text": "We'll also sample one of\nthe examples from D test.",
    "start": "4038170",
    "end": "4043210"
  },
  {
    "text": "So maybe it's-- this will\nbe this image right here.",
    "start": "4043210",
    "end": "4049997"
  },
  {
    "text": "And note that when\nwe pass in D train, we're going to be passing in\nboth the images and the labels, whereas here we don't\nwant to pass on the label",
    "start": "4049997",
    "end": "4056830"
  },
  {
    "text": "because we don't want to\ngive it the answer, right? So we'll pass this into\nthe network as well,",
    "start": "4056830",
    "end": "4064220"
  },
  {
    "text": "and then it's going\nto predict y hat,",
    "start": "4064220",
    "end": "4070080"
  },
  {
    "text": "and then we're going to\ncompute how good y hat is. So we'll have some loss function\nthat compares y hat to the true",
    "start": "4070080",
    "end": "4077700"
  },
  {
    "text": "label, which is 2, and then we\ncan take the gradient of this with respect to theta-- so maybe\nthere are weights theta here--",
    "start": "4077700",
    "end": "4086310"
  },
  {
    "text": "and we'll use this to\nupdate the network. ",
    "start": "4086310",
    "end": "4092800"
  },
  {
    "text": "Does that example make sense? Do people have any questions? Yeah? Why do you normally--\nso when you're",
    "start": "4092800",
    "end": "4100223"
  },
  {
    "text": "sampling three characters,\nwhy [INAUDIBLE] different alphabets on? Isn't each task related\nto a specific alphabet?",
    "start": "4100224",
    "end": "4107778"
  },
  {
    "text": "Yeah. So the question is\nwhy are we allowing it to necessarily--\npotentially sample classes across alphabets, and each\ntask be a different alphabet?",
    "start": "4107779",
    "end": "4116680"
  },
  {
    "text": "There are different ways\nto design the problem, and what-- it is very\nvalid to essentially",
    "start": "4116680",
    "end": "4121810"
  },
  {
    "text": "kind of keep tasks constrained\nto individual alphabets. And when I wrote\nthese, it was actually",
    "start": "4121810",
    "end": "4128080"
  },
  {
    "text": "constrained to the\nalphabets, but one thing that can be helpful is\nif you don't constrain it",
    "start": "4128080",
    "end": "4133600"
  },
  {
    "text": "to an alphabet, then\nyou can essentially get more data,\nmore tasks, and so it's essentially kind\nof a form of task",
    "start": "4133600",
    "end": "4140318"
  },
  {
    "text": "augmentation in the sense,\nwhich is that instead of only considering tasks\nwithin an alphabet, you could also consider tasks\nthat are classifying examples",
    "start": "4140319",
    "end": "4147333"
  },
  {
    "text": "kind of across different\nalphabets as well,",
    "start": "4147333",
    "end": "4153333"
  },
  {
    "text": "and this will\nessentially generate more tasks for meta training.",
    "start": "4153333",
    "end": "4158450"
  },
  {
    "text": "And then in the homework,\nwe will also be-- we won't actually be holding\nout an entire alphabet,",
    "start": "4158450",
    "end": "4164350"
  },
  {
    "text": "we'll just be holding\nout characters from across different alphabets. In a very realistic\nscenario, maybe it",
    "start": "4164350",
    "end": "4170920"
  },
  {
    "text": "is actually an\nentirely new alphabet that you want to\nbe able to test on. And in that case,\nyou would actually--",
    "start": "4170920",
    "end": "4175930"
  },
  {
    "text": "if you want to prepare for\nthat scenario, you do actually want to hold out an\nentirely distinct alphabet",
    "start": "4175930",
    "end": "4182200"
  },
  {
    "text": "during the meta-training process\nto test whether the method is actually good at that.",
    "start": "4182200",
    "end": "4188525"
  },
  {
    "text": "Sorry, the reason\nI was asking that is because along\nwith augmentation,",
    "start": "4188525",
    "end": "4195072"
  },
  {
    "text": "I don't see the benefit\nof taking meta-learning [INAUDIBLE] and just putting\neverything in one dataset",
    "start": "4195072",
    "end": "4200821"
  },
  {
    "text": "and just training\nsomething there. Yeah. So with the form of\ntask augmentation, you're still\nessentially preparing",
    "start": "4200821",
    "end": "4207830"
  },
  {
    "text": "the method to be able\nto quickly classify examples for a new alphabet. And so maybe during this\nmeta-training process,",
    "start": "4207830",
    "end": "4214388"
  },
  {
    "text": "you don't actually have data\nfor the test alphabet available, and you want it to be\nable to make predictions",
    "start": "4214388",
    "end": "4220340"
  },
  {
    "text": "for a new alphabet\nkind of on the fly. If you do have all\nof the alphabets",
    "start": "4220340",
    "end": "4226550"
  },
  {
    "text": "available ahead\nof time, then you can also use a multi-task\nlearning approach rather than a meta-learning approach.",
    "start": "4226550",
    "end": "4233730"
  },
  {
    "text": "Yeah? So for example, in this case,\nwe have multiple instances",
    "start": "4233730",
    "end": "4239160"
  },
  {
    "text": "of each character. So is it, like--",
    "start": "4239160",
    "end": "4244530"
  },
  {
    "text": "is it standard to\nlike, for example, if our target task is\none-shot learning--",
    "start": "4244530",
    "end": "4249870"
  },
  {
    "text": "one-shot classification, is it-- like, are meta-training\n[INAUDIBLE] usually also going",
    "start": "4249870",
    "end": "4257960"
  },
  {
    "text": "to be one-shot learning? Yeah. So typically, if you\nwant to prepare it",
    "start": "4257960",
    "end": "4263699"
  },
  {
    "text": "for one-shot learning, it's good\nto give it one-shot learning tasks at meta-training time\nbecause that will kind of",
    "start": "4263700",
    "end": "4269220"
  },
  {
    "text": "prepare it for exactly what\nit will see at meta-test time. You can also train it\non variable shots--",
    "start": "4269220",
    "end": "4274403"
  },
  {
    "text": "like some tasks\nthat are one-shot, some tasks that are\nfive-shots, some tasks that are three-shot-- and that should, in\nprinciple also prepare it",
    "start": "4274403",
    "end": "4280410"
  },
  {
    "text": "for different kinds of tasks,\nincluding one-shot tasks. One thing that you\nwant to avoid is",
    "start": "4280410",
    "end": "4285929"
  },
  {
    "text": "if you train it\nfor five-shot tasks and then test it\non a one-shot task, it probably won't\ndo well in that case",
    "start": "4285930",
    "end": "4291513"
  },
  {
    "text": "because you're training\nit to be able to learn from large datasets\nand then asking it to be able to perform\na task where you only",
    "start": "4291513",
    "end": "4298143"
  },
  {
    "text": "have a small amount of\ndata, and so you're not preparing it for what it\nwill see at test time.",
    "start": "4298143",
    "end": "4303380"
  },
  {
    "text": "Yeah? In that case, isn't\nit always better to train it on one-shot tasks\nbecause it's the hardest thing",
    "start": "4303380",
    "end": "4311570"
  },
  {
    "text": "that you can ask it to do? Yeah, that's a great question. So actually, people\noften do exactly",
    "start": "4311570",
    "end": "4316775"
  },
  {
    "text": "that, which is that you\ntrain it for one-shot and then you test it for maybe\none-shot or more than one-shot.",
    "start": "4316775",
    "end": "4321890"
  },
  {
    "text": "And one-shot is the\nhardest setting. It often actually prepares\nit for the one-shot setting and also for settings that\nare a bit easier than that.",
    "start": "4321890",
    "end": "4329520"
  },
  {
    "text": "Yeah? Also, you would not be simply\n[INAUDIBLE] multiple control",
    "start": "4329520",
    "end": "4337369"
  },
  {
    "text": "runs, and then [INAUDIBLE]? Yeah.",
    "start": "4337370",
    "end": "4342560"
  },
  {
    "text": "So if you train it\non, like, five-shot and then test it\non one-shot, you're breaking the i.d.d.\nassumption and that's exactly why it performs poorly.",
    "start": "4342560",
    "end": "4349370"
  },
  {
    "text": "It is valid to still\ntrain it on variable shot, as long as the thing that\nyou're going to be testing it on",
    "start": "4349370",
    "end": "4354530"
  },
  {
    "text": "is a part of that and is\nsufficiently represented in the kinds of tasks\nyou're training on.",
    "start": "4354530",
    "end": "4360025"
  },
  {
    "text": "I think [INAUDIBLE].  So the question is, how\nis-- what I said before,",
    "start": "4360025",
    "end": "4367310"
  },
  {
    "text": "breaking the i.i.d. assumption. So if all of your tasks are\nfive-shot learning tasks, and at test time you're given\na one-shot learning task,",
    "start": "4367310",
    "end": "4374420"
  },
  {
    "text": "then the one-shot learning task\nor problem isn't actually--",
    "start": "4374420",
    "end": "4381620"
  },
  {
    "text": "isn't represented in the\ndistribution of tasks that you see during training. In that sense, one\nof the things that's",
    "start": "4381620",
    "end": "4388160"
  },
  {
    "text": "kind of interesting\nabout task distributions is it's not just like what's in\nthe data and the loss function,",
    "start": "4388160",
    "end": "4393690"
  },
  {
    "text": "it's also the size of\nthe dataset as well. So why is that a problem in\nthat direction but not a problem in like-",
    "start": "4393690",
    "end": "4401090"
  },
  {
    "text": "Yeah. Why is that\njustification [INAUDIBLE] instead of some other\ndistribution [INAUDIBLE]??",
    "start": "4401090",
    "end": "4408540"
  },
  {
    "text": "Yeah. So in practice,\ntraining on one-shot and testing on\nfive-shot actually still works pretty well.",
    "start": "4408540",
    "end": "4414320"
  },
  {
    "text": "And completely-- in theory,\nit shouldn't work well in some sense because\nyou're training it",
    "start": "4414320",
    "end": "4420110"
  },
  {
    "text": "for something that you're\nnot testing it on something. The reason why in\npractice it works well is you're\nkind of testing it",
    "start": "4420110",
    "end": "4426170"
  },
  {
    "text": "on a setting that's easier\nthan the training setting. I guess that one thing\nthat I should also mention",
    "start": "4426170",
    "end": "4433280"
  },
  {
    "text": "is if you're using an\nRNN, like, using an RNN",
    "start": "4433280",
    "end": "4439159"
  },
  {
    "text": "and you train it for\none-shot and then you test it on five-shot,\nif you're rolling out",
    "start": "4439160",
    "end": "4444500"
  },
  {
    "text": "the RNN for longer than\nyou trained it for, it will actually\nperform very poorly. And so for RNN type\nmethods, you don't",
    "start": "4444500",
    "end": "4450349"
  },
  {
    "text": "want to go from\none-shot to five-shot. But for a lot of other\nmeta-learning methods, they usually work\nwell when you train",
    "start": "4450350",
    "end": "4455552"
  },
  {
    "text": "on one-shot and test on\nfive-shot because of some-- because of how they add\nstructure into the algorithm.",
    "start": "4455552",
    "end": "4461969"
  },
  {
    "text": "Yeah? [INAUDIBLE]\napplication [INAUDIBLE] because it seems like it would\nbe [INAUDIBLE] like a one-shot,",
    "start": "4461970",
    "end": "4474990"
  },
  {
    "text": "but it's like given\nthose [INAUDIBLE] as opposed to given these five\ncharacters, what [INAUDIBLE]..",
    "start": "4474990",
    "end": "4483606"
  },
  {
    "text": "Does that make sense? I'm not sure I understand. Could you repeat? So basically my question\nis, why this type of example",
    "start": "4483606",
    "end": "4494550"
  },
  {
    "text": "would be used in combination,\nbecause from what I'm",
    "start": "4494550",
    "end": "4500320"
  },
  {
    "text": "thinking of is, if I take\na picture of a character, I'd want to know what\nthat character is [INAUDIBLE] to an actual\nplan, like [INAUDIBLE]..",
    "start": "4500320",
    "end": "4511476"
  },
  {
    "text": "So I'm a little confused\nhow [INAUDIBLE].. ",
    "start": "4511476",
    "end": "4519219"
  },
  {
    "text": "So the question is kind of\ntell me a little bit more about a real world use\ncase of this and so forth.",
    "start": "4519220",
    "end": "4525750"
  },
  {
    "text": "I guess the typical-- the use case of this is\none where maybe you have-- you take a picture of images--",
    "start": "4525750",
    "end": "4532470"
  },
  {
    "text": "it is-- it's not a\nzero-shot setting. Like, if you see\na character that wasn't seen in the\ntraining dataset and you take a\npicture of it, it's",
    "start": "4532470",
    "end": "4538380"
  },
  {
    "text": "not going to be able to classify\nit because it's just simply not enough information. And so the application\nis something",
    "start": "4538380",
    "end": "4544110"
  },
  {
    "text": "where maybe you have-- maybe you want to deploy\nsomething in a new country",
    "start": "4544110",
    "end": "4549960"
  },
  {
    "text": "and you collect a small\namount of data for characters from that country, maybe to\nhelp with like the Postal Service or something\nlike that, and you",
    "start": "4549960",
    "end": "4557488"
  },
  {
    "text": "don't want to collect a\nmassive dataset for that, you want to just collect\na very small dataset because that will be very\ncheap, and so you take images",
    "start": "4557488",
    "end": "4563610"
  },
  {
    "text": "of characters from\nthat new alphabet and then use that to train\na classifier so that you",
    "start": "4563610",
    "end": "4569070"
  },
  {
    "text": "can deploy that\nclassifier on all of the mail from that\ncountry or something. Does that make sense?",
    "start": "4569070",
    "end": "4574530"
  },
  {
    "text": "I think so, yes. Great. Yeah? Most images from [INAUDIBLE]?",
    "start": "4574530",
    "end": "4581004"
  },
  {
    "start": "4581004",
    "end": "4587920"
  },
  {
    "text": "Yeah. So the question is, what\nare the implications of using a neural network\nto output the weights of another neural network?",
    "start": "4587920",
    "end": "4592950"
  },
  {
    "text": "Are we kind of contradicting\nthe inductive bias of neural networks? I think that the--",
    "start": "4592950",
    "end": "4599010"
  },
  {
    "text": "it's possible-- I guess-- in general, this sort of\nidea of outputting weights",
    "start": "4599010",
    "end": "4604890"
  },
  {
    "text": "of another neural network is\nreferred to as hyper networks sometimes, and it's\npossible that this will not",
    "start": "4604890",
    "end": "4610560"
  },
  {
    "text": "retain some of the inductive\nbiases of neural networks when the weights come from\nanother neural network.",
    "start": "4610560",
    "end": "4617760"
  },
  {
    "text": "And we will actually\nsee in the kind of coming lectures approaches\nthat have better inductive biases, including metric\nlearning approaches and more",
    "start": "4617760",
    "end": "4626130"
  },
  {
    "text": "optimization-based approaches\nwhere the weights aren't coming from another network.",
    "start": "4626130",
    "end": "4632348"
  },
  {
    "text": "How many tasks do we need? [INAUDIBLE] ",
    "start": "4632348",
    "end": "4641030"
  },
  {
    "text": "Yes. How many tasks do we need? In general, like I said\nbefore, the more, the better.",
    "start": "4641030",
    "end": "4646280"
  },
  {
    "text": "And it really-- it ends up\nbeing somewhat problem specific and domain specific. If the tasks are\nrelatively simple,",
    "start": "4646280",
    "end": "4651590"
  },
  {
    "text": "you might not need that many. But if you have a very\nbroad distribution of tasks, then you're going\nto need more tasks",
    "start": "4651590",
    "end": "4657290"
  },
  {
    "text": "to cover that\nbroad distribution. There is some work that tries\nto theoretically characterize how the number of tasks\nimpacts performance.",
    "start": "4657290",
    "end": "4664385"
  },
  {
    "text": "I think that there's\nsome work by Rich Zemel, for example, at Toronto\nthat studies that, and they do look at\nsome bounds, but that's",
    "start": "4664385",
    "end": "4671870"
  },
  {
    "text": "kind of beyond what we'll\nlook at in this course. ",
    "start": "4671870",
    "end": "4678288"
  },
  {
    "text": "And then a couple of more\nthings worth mentioning here. This was an example\nof an Omniglot. There are also a number of other\nimage recognition datasets,",
    "start": "4678288",
    "end": "4684690"
  },
  {
    "text": "like CIFAR, CUB,\nCelebA and ORBIT. ORBIT is actually a\npretty nice example. It's coming from the standpoint\nof trying to build classifiers",
    "start": "4684690",
    "end": "4692460"
  },
  {
    "text": "for blind or visually impaired\nusers, where the user will take pictures of their items--",
    "start": "4692460",
    "end": "4697470"
  },
  {
    "text": "take only a few\npictures naturally-- and then you want\nto be able to allow them to build a\nclassifier for their items",
    "start": "4697470",
    "end": "4703680"
  },
  {
    "text": "using those pictures. There's also benchmarks that\nare outside of the image",
    "start": "4703680",
    "end": "4708810"
  },
  {
    "text": "recognition domain. Here are a few examples. And if you're\ninterested in looking at datasets for your\nprojects, these benchmarks",
    "start": "4708810",
    "end": "4714338"
  },
  {
    "text": "could be interesting.  And then to summarize kind\nof the black-box approach",
    "start": "4714338",
    "end": "4721410"
  },
  {
    "text": "and those pros and cons. These sorts of networks are\nvery expressive because they're essentially\nparameterizing a learning",
    "start": "4721410",
    "end": "4728040"
  },
  {
    "text": "procedure using a potentially\nvery large neural network. It's also very easy to\ncombine this kind of approach",
    "start": "4728040",
    "end": "4735210"
  },
  {
    "text": "with a variety of\ndifferent problems, including supervised learning\nand reinforcement learning.",
    "start": "4735210",
    "end": "4741960"
  },
  {
    "text": "There are some downsides. So in general, you're\ntrying to learn how to learn from scratch,\nessentially-- you're",
    "start": "4741960",
    "end": "4747840"
  },
  {
    "text": "trying to learn this\nlearning procedure from completely randomly\ninitialized weights of a neural network, and this\nleads to a fairly challenging",
    "start": "4747840",
    "end": "4754530"
  },
  {
    "text": "optimization procedure. And as a result,\nit can sometimes be data inefficient\nin terms of the tasks",
    "start": "4754530",
    "end": "4760590"
  },
  {
    "text": "that you need for meta-training. And so there are other ways that\nwe could essentially represent",
    "start": "4760590",
    "end": "4767430"
  },
  {
    "text": "these learning procedures. And next time on\nWednesday, we're going to be looking at how\nwe can essentially treat it",
    "start": "4767430",
    "end": "4773370"
  },
  {
    "text": "as an optimization procedure\nand parameterize an optimization procedure as part of\nthe learning process,",
    "start": "4773370",
    "end": "4780480"
  },
  {
    "text": "rather than using a neural\nnetwork to represent the entire learning procedure.",
    "start": "4780480",
    "end": "4786530"
  },
  {
    "start": "4786530",
    "end": "4791000"
  }
]