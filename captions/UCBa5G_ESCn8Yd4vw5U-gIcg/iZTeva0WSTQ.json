[
  {
    "text": "Couple of announcements, uh, before we get started. So, uh, first of all, PS1 is out.",
    "start": "3500",
    "end": "10005"
  },
  {
    "text": "Uh, problem set 1, um, it is due on the 17th.",
    "start": "10005",
    "end": "16049"
  },
  {
    "text": "That's two weeks from today. You have, um, exactly two weeks to work on it. You can take up to,",
    "start": "16050",
    "end": "22090"
  },
  {
    "text": "um, two or three late days. I think you can take up to, uh, three late days, um.",
    "start": "22090",
    "end": "27100"
  },
  {
    "text": "There is, uh, there's a good amount of programming and a good amount of math you, uh, you need to do.",
    "start": "27100",
    "end": "34980"
  },
  {
    "text": "So PS1 needs to be uploaded. Uh, the solutions need to be uploaded to Gradescope. Um, you'll have to make two submissions.",
    "start": "34980",
    "end": "42684"
  },
  {
    "text": "One submission will be a PDF file, uh, which you can either, uh,",
    "start": "42685",
    "end": "48020"
  },
  {
    "text": "which you can either use a LaTeX template that we provide or you can handwrite it as well but you're strongly encouraged to use the- the LaTeX template.",
    "start": "48020",
    "end": "56885"
  },
  {
    "text": "Um, and there is a separate coding assignment, uh, for which you'll have to submit code as a separate,",
    "start": "56885",
    "end": "63440"
  },
  {
    "text": "uh, Gradescope assignment. So they're gonna- you're gonna see two assignments in Gradescope. One is for the written part.",
    "start": "63440",
    "end": "68570"
  },
  {
    "text": "The other is for the, uh, is for the programming part. Uh, with that, let's- let's jump right into today's topics.",
    "start": "68570",
    "end": "76710"
  },
  {
    "text": "So, uh, today, we're gonna cover, uh- briefly we're gonna cover, uh, the perceptron, uh, algorithm.",
    "start": "76710",
    "end": "83055"
  },
  {
    "text": "Um, and then, you know, a good chunk of today is gonna be exponential family and,",
    "start": "83055",
    "end": "88125"
  },
  {
    "text": "uh, generalized linear models. And, uh, we'll- we'll end it with, uh, softmax regression for multi-class classification.",
    "start": "88125",
    "end": "95884"
  },
  {
    "text": "So, uh, perceptron, um, we saw in logistic regression, um.",
    "start": "95885",
    "end": "102270"
  },
  {
    "text": "So first of all, the perceptron algorithm, um, I should mention is not something that is widely used in practice.",
    "start": "102270",
    "end": "108770"
  },
  {
    "text": "Uh, we study it mostly for, um, historical reasons. And also because it is- it's nice and simple and, you know,",
    "start": "108770",
    "end": "116580"
  },
  {
    "text": "it's easy to analyze and, uh, we also have homework questions on it. So, uh, logistic regression.",
    "start": "116580",
    "end": "124100"
  },
  {
    "text": "Uh, we saw logistic regression uses, uh, the sigmoid function.",
    "start": "124100",
    "end": "130599"
  },
  {
    "text": "Right. So, uh, the logistic regression, uh, using the sigmoid function which, uh,",
    "start": "153500",
    "end": "159690"
  },
  {
    "text": "which essentially squeezes the entire real line from minus infinity to infinity between 0 and 1.",
    "start": "159690",
    "end": "165760"
  },
  {
    "text": "Um, and - and the 0 and 1 kind of represents, uh, the probability right?",
    "start": "165760",
    "end": "170875"
  },
  {
    "text": "Um, you could also think of, uh, a variant of that, uh, which will be, um,",
    "start": "170875",
    "end": "177375"
  },
  {
    "text": "like the perceptron where, um. So in- in- in the sigmoid function at, um,",
    "start": "177375",
    "end": "183140"
  },
  {
    "text": "at z equals 0- at z equals 0- g of z is a half.",
    "start": "183140",
    "end": "192950"
  },
  {
    "text": "And as z tends to minus infinity, g tends to 0 and as z tends to plus infinity,",
    "start": "192950",
    "end": "198845"
  },
  {
    "text": "g tends to 1. The perceptron, um, algorithm uses,",
    "start": "198845",
    "end": "206370"
  },
  {
    "text": "uh, uh, a somewhat similar but, uh, different, uh, function which,",
    "start": "206370",
    "end": "218655"
  },
  {
    "text": "uh, let's say this is z.",
    "start": "218655",
    "end": "220959"
  },
  {
    "text": "Right. So, uh, g of z in this case",
    "start": "228950",
    "end": "235635"
  },
  {
    "text": "is 1 if z is greater than or equal to 0 and 0 if z is less than 0, right?",
    "start": "235635",
    "end": "245025"
  },
  {
    "text": "So you ca- you can think of this as the hard version of- of- of the sigmoid function, right?",
    "start": "245025",
    "end": "250349"
  },
  {
    "text": "And this leads to, um, um, this leads to the hypothesis function, uh,",
    "start": "250350",
    "end": "258690"
  },
  {
    "text": "here being, uh, h Theta of x is equal to,",
    "start": "258690",
    "end": "264705"
  },
  {
    "text": "um, g of Theta transpose x.",
    "start": "264705",
    "end": "270129"
  },
  {
    "text": "So, uh, Theta transpose x, um, your Theta has the parameter,",
    "start": "270350",
    "end": "275445"
  },
  {
    "text": "x is the, um, x is the input and h Theta of x will be 0 or 1,",
    "start": "275445",
    "end": "281544"
  },
  {
    "text": "depending on whether Theta transpose x was less than 0 or- or, uh, greater than 0.",
    "start": "281545",
    "end": "287205"
  },
  {
    "text": "And you tol- and, um, similarly in, uh, logistic regression we had a state of x is equal to,",
    "start": "287205",
    "end": "296020"
  },
  {
    "text": "um, 1 over 1 plus e to the minus Theta transpose x. Yeah.",
    "start": "296020",
    "end": "301500"
  },
  {
    "text": "That's essentially, uh, g of- g of z where g is s, uh, the sigma- sigmoid function.",
    "start": "301500",
    "end": "307870"
  },
  {
    "text": "Um, both of them have a common update rule, uh, which, you know,",
    "start": "307870",
    "end": "313789"
  },
  {
    "text": "on the surface looks similar. So Theta j equal to Theta j plus Alpha times y_i",
    "start": "313790",
    "end": "325895"
  },
  {
    "text": "minus h Theta of- of",
    "start": "325895",
    "end": "331460"
  },
  {
    "text": "x_i times x_ij, right?",
    "start": "331460",
    "end": "338205"
  },
  {
    "text": "So the update rules for, um, the perceptron and logistic regression, they look the same except h Theta of x means",
    "start": "338205",
    "end": "345285"
  },
  {
    "text": "different things in- in- in- in the two different, um, uh, scenarios. We also saw that it was similar for linear regression as well.",
    "start": "345285",
    "end": "353525"
  },
  {
    "text": "And we're gonna see why this- this is, um, you know, that this is actually a- a more common- common theme.",
    "start": "353525",
    "end": "359914"
  },
  {
    "text": "So, uh, what's happening here? So, uh, if you inspect this equation, um,",
    "start": "359915",
    "end": "366925"
  },
  {
    "text": "to get a better sense of what's happening in- in the perceptron algorithm, this quantity over here is a scalar, right?",
    "start": "366925",
    "end": "376740"
  },
  {
    "text": "It's the difference between y_i which can be either 0 and 1 and h Theta of x_i which can either be 0 or 1, right?",
    "start": "376740",
    "end": "384090"
  },
  {
    "text": "So when the algorithm makes a prediction of h Theta of- h Theta of x_i for a given x_i,",
    "start": "384090",
    "end": "392910"
  },
  {
    "text": "this quantity will either be zero if- if, uh,",
    "start": "392910",
    "end": "401340"
  },
  {
    "text": "the algorithm got it right already, right?",
    "start": "401340",
    "end": "409780"
  },
  {
    "text": "And it would be either plus 1 or minus 1 if- if y_i- if- if the actual, uh,",
    "start": "409780",
    "end": "419055"
  },
  {
    "text": "if the ground truth was plus 1 and the algorithm predicted 0, then it, uh, uh,",
    "start": "419055",
    "end": "424699"
  },
  {
    "text": "this will evaluate to 1 if wrong and y_i equals 1 and similarly it is,",
    "start": "424700",
    "end": "435180"
  },
  {
    "text": "uh, minus 1 if",
    "start": "435180",
    "end": "441930"
  },
  {
    "text": "wrong and y_i is 0.",
    "start": "441930",
    "end": "446470"
  },
  {
    "text": "So what's happening here? Um, to see what's- what's- what's happening, uh, it's useful to see this picture, right?",
    "start": "447510",
    "end": "456280"
  },
  {
    "text": "So this is the input space, right? And, uh, let's imagine there are two, uh,",
    "start": "458430",
    "end": "465895"
  },
  {
    "text": "two classes, boxes and,",
    "start": "465895",
    "end": "472315"
  },
  {
    "text": "let's say, circles, right? And you want to learn, I wanna learn an algorithm that can separate these two classes, right?",
    "start": "472315",
    "end": "480835"
  },
  {
    "text": "And, uh, if you imagine that the, uh, uh,",
    "start": "480835",
    "end": "486425"
  },
  {
    "text": "what- what the algorithm has learned so far is a Theta that represents this decision boundary, right?",
    "start": "486425",
    "end": "494685"
  },
  {
    "text": "So this represents, uh, Theta transpose x equals 0.",
    "start": "494685",
    "end": "499860"
  },
  {
    "text": "And, uh, anything above is Theta transpose, uh, x is greater than 0.",
    "start": "499860",
    "end": "506695"
  },
  {
    "text": "And anything below is Theta transpose x less than 0, all right?",
    "start": "506695",
    "end": "512935"
  },
  {
    "text": "And let's say, um, the algorithm is learning one example at a time, and a new example comes in.",
    "start": "512935",
    "end": "518380"
  },
  {
    "text": "Uh, and this time it happens to be- the new example happens to be a square, uh, or a box.",
    "start": "518380",
    "end": "527400"
  },
  {
    "text": "And, uh, but the algorithm has mis- misclassified it, right? Now, um, this line, the separating boundary,",
    "start": "527400",
    "end": "536639"
  },
  {
    "text": "um, if- if- if the vector equivalent of that would be a vector that's normal to the line.",
    "start": "536640",
    "end": "543415"
  },
  {
    "text": "So, uh, this was- would be Theta, all right? And this is our new x, right? This is the new x.",
    "start": "543415",
    "end": "553490"
  },
  {
    "text": "So this got misclassified, this, uh, uh, this is lying to, you know,",
    "start": "553740",
    "end": "559075"
  },
  {
    "text": "lying on the bottom of the decision boundary. So what- what- what's gonna happen here? Um, y_i, let's call this the one class and this is- this is the zero class, right?",
    "start": "559075",
    "end": "569035"
  },
  {
    "text": "So y_i minus- h state of i will be plus 1, right?",
    "start": "569035",
    "end": "574509"
  },
  {
    "text": "And what the algorithm is doing is, uh, it sets Theta to be Theta plus Alpha times x, right?",
    "start": "574510",
    "end": "582280"
  },
  {
    "text": "So this is the old Theta, this is x. Alpha is some small learning rate.",
    "start": "582280",
    "end": "587860"
  },
  {
    "text": "So it adds- let me use a different color here. It adds, right, Alpha times x to",
    "start": "587860",
    "end": "597180"
  },
  {
    "text": "Theta and now say this is- let's call it Theta prime, is the new vector.",
    "start": "597180",
    "end": "604464"
  },
  {
    "text": "That's- that's the updated value, right? And the- and the separating, um, uh, hyperplane corresponding to this is something that is normal to it, right?",
    "start": "604465",
    "end": "614725"
  },
  {
    "text": "Yeah. So- so it updated the, um, decision boundary such that x is now included in the positive class, right?",
    "start": "614725",
    "end": "622060"
  },
  {
    "text": "The- the, um, idea here- here is that, um, Theta,",
    "start": "622060",
    "end": "629455"
  },
  {
    "text": "we want Theta to be similar to x in general, where such- where y is 1.",
    "start": "629455",
    "end": "638335"
  },
  {
    "text": "And we want Theta to be not similar to x when y equals 0.",
    "start": "638335",
    "end": "644950"
  },
  {
    "text": "The reason is, uh, when two vectors are similar, the dot product is positive and they are not similar,",
    "start": "644950",
    "end": "650230"
  },
  {
    "text": "the dot product is negative. Uh, what does that mean? If, uh, let's say this is, um, x and let's say you have Theta.",
    "start": "650230",
    "end": "657655"
  },
  {
    "text": "If they're kind of, um, pointed outwards, their dot product would be, um, negative.",
    "start": "657655",
    "end": "663730"
  },
  {
    "text": "And when- and if you have a Theta that looks like this, we call it Theta prime, then the dot product will be",
    "start": "663730",
    "end": "669670"
  },
  {
    "text": "positive if the angle is- is less than r. So, um, this essentially means that as Theta is rotating,",
    "start": "669670",
    "end": "675220"
  },
  {
    "text": "the, um, decision boundary is kind of perpendicular to Theta. And you wanna get all the positive x's on one side of the decision boundary.",
    "start": "675220",
    "end": "683815"
  },
  {
    "text": "And what's the- what's the, uh, most naive way of- of- of taking Theta and given x,",
    "start": "683815",
    "end": "690579"
  },
  {
    "text": "try to make Theta more kind of closer to x? A simple thing is to just add a component of x in that direction.",
    "start": "690580",
    "end": "697705"
  },
  {
    "text": "You know, add it here and kind of make Theta. And so this- this is a very common technique used in lots of",
    "start": "697705",
    "end": "702820"
  },
  {
    "text": "algorithms where if you add a vector to another vector, you make the second one kind of closer to the first one, essentially.",
    "start": "702820",
    "end": "709810"
  },
  {
    "text": "So this is- this is, uh, the perceptron algorithm. Um, you go example by example in an online manner,",
    "start": "709810",
    "end": "716665"
  },
  {
    "text": "and if the al- if the, um, example is already classified, you do nothing. You get a 0 over here.",
    "start": "716665",
    "end": "722395"
  },
  {
    "text": "If it is misclassified, you either add the- add a small component of, uh,",
    "start": "722395",
    "end": "728320"
  },
  {
    "text": "as, uh, you add the vector itself, the example itself to your Theta or you subtract it, depending on the class of the vector.",
    "start": "728320",
    "end": "735205"
  },
  {
    "text": "This is about it. Any- any- any questions about the perceptron?",
    "start": "735205",
    "end": "739130"
  },
  {
    "text": "Cool. So let's move on to the next topic, um, exponential families.",
    "start": "741120",
    "end": "749335"
  },
  {
    "text": "Um, so, um, exponential family is essentially a class of- yeah.",
    "start": "749335",
    "end": "759690"
  },
  {
    "text": "Why don't we use them in practice? Um, it's, um, not used in practice because,",
    "start": "759690",
    "end": "766510"
  },
  {
    "text": "um, it- it does not have a probabilistic interpretation of what's- what's happening.",
    "start": "766510",
    "end": "771790"
  },
  {
    "text": "You kinda have a geometrical feel of what's happening with- with the hyperplane but it- it doesn't have a probabilistic interpretation.",
    "start": "771790",
    "end": "777430"
  },
  {
    "text": "Also, um, it's, um, it- it was- and I think the perceptron was,",
    "start": "777430",
    "end": "784390"
  },
  {
    "text": "uh, pretty famous in, I think, the 1950s or the '60s where people thought this is a good model of how the brain works.",
    "start": "784390",
    "end": "790645"
  },
  {
    "text": "And, uh, I think it was, uh, Marvin Minsky who wrote a paper saying, you know, the perceptron is- is kind of limited because it- it could never classify,",
    "start": "790645",
    "end": "800350"
  },
  {
    "text": "uh, points like this. And there is no possible separating boundary that can,",
    "start": "800350",
    "end": "805855"
  },
  {
    "text": "you know, do- do something as simple as this. And kind of people lost interest in it, but, um, yeah.",
    "start": "805855",
    "end": "811255"
  },
  {
    "text": "And in fact, what- what we see is- is, uh, in logistic regression, it's like a software version of,",
    "start": "811255",
    "end": "816880"
  },
  {
    "text": "uh, the perceptron itself in a way. Yeah. [inaudible]",
    "start": "816880",
    "end": "828100"
  },
  {
    "text": "It's- it's, uh, it's up to, you know, it's- it's a design choice that you make. What you could do is you can- you can kind of,",
    "start": "828100",
    "end": "835150"
  },
  {
    "text": "um, anneal your learning rate with every step, every time, uh, you see a new example decrease your learning rate until something,",
    "start": "835150",
    "end": "842050"
  },
  {
    "text": "um, um, until you stop changing, uh, Theta by a lot. You can- you're not guaranteed that you'll- you'll be able to get every example right.",
    "start": "842050",
    "end": "850089"
  },
  {
    "text": "For example here, no matter how long you learn you're- you're never gonna, you know, um, uh, find, uh, a learning boundary.",
    "start": "850090",
    "end": "856000"
  },
  {
    "text": "So it's- it's up to you when you wanna stop training. Uh, a common thing is to just decrease the learning rate,",
    "start": "856000",
    "end": "861850"
  },
  {
    "text": "uh, with every time step until you stop making changes. All right.",
    "start": "861850",
    "end": "869079"
  },
  {
    "text": "Let's move on to exponential families. So, uh, exponential families is, uh, is a class of probability distributions,",
    "start": "869080",
    "end": "876894"
  },
  {
    "text": "which are somewhat nice mathematically, right? Um, they're also very closely related to GLMs,",
    "start": "876895",
    "end": "883855"
  },
  {
    "text": "which we will be going over next, right? But first we kind of take a deeper look at, uh,",
    "start": "883855",
    "end": "889765"
  },
  {
    "text": "exponential families and, uh, and- and what they're about. So, uh, an exponential family is one, um, whose PDF,",
    "start": "889765",
    "end": "901459"
  },
  {
    "text": "right? So whose PDF can be written in the form- by PDF I mean probability density function,",
    "start": "910230",
    "end": "917800"
  },
  {
    "text": "but for a discrete, uh, distribution, then it would be the probability mass function, right?",
    "start": "917800",
    "end": "923600"
  },
  {
    "text": "Whose PDF can be written in the form, um. All right. This looks pretty scary.",
    "start": "923600",
    "end": "950245"
  },
  {
    "text": "Let's- let's- let's kind of, uh, break it down into, you know, what- what- what they actually mean. So y over here is the data, right?",
    "start": "950245",
    "end": "960785"
  },
  {
    "text": "And there's a reason why we call it y because- yeah. Can you write a bit larger.",
    "start": "960785",
    "end": "966200"
  },
  {
    "text": "A bit larger, sure.",
    "start": "966200",
    "end": "967860"
  },
  {
    "text": "Is this better?",
    "start": "976920",
    "end": "990535"
  },
  {
    "text": "Yeah. So y is the data. And the reason- there's a reason why we call it y and not x. And that- and that's because we're gonna use exponential families",
    "start": "990535",
    "end": "998150"
  },
  {
    "text": "to model the output of your- of- of your data, you know, in a, uh, in a supervised learning setting. Um, and- and you're gonna see x when we move on to GLMs.",
    "start": "998150",
    "end": "1006820"
  },
  {
    "text": "Until, you know, until then we're just gonna deal with y's for now. Uh, so y is the data. Um, Eta is- is called the natural parameter.",
    "start": "1006820",
    "end": "1017150"
  },
  {
    "text": "T of y is called a sufficient statistic.",
    "start": "1023030",
    "end": "1029080"
  },
  {
    "text": "If you have a statistics background and you've learn- if you come across the word sufficient statistic before, it's the exact same thing.",
    "start": "1029990",
    "end": "1036944"
  },
  {
    "text": "But you don't need to know much about this because for all the distributions that we're gonna be seeing today,",
    "start": "1036945",
    "end": "1044699"
  },
  {
    "text": "uh, or in this class, t of y will be equal to just y. So you can, you can just replace t of y with y for,",
    "start": "1044700",
    "end": "1052875"
  },
  {
    "text": "um, for all the examples today and in the rest of the calcu- of the class.",
    "start": "1052875",
    "end": "1058080"
  },
  {
    "text": "Uh, b of y, is called a base measure.",
    "start": "1058080",
    "end": "1063910"
  },
  {
    "text": "Right, and finally a of Eta,",
    "start": "1067820",
    "end": "1072914"
  },
  {
    "text": "is called the log-partition function. And we're gonna be seeing a lot of this function, log-partition function.",
    "start": "1072915",
    "end": "1082395"
  },
  {
    "text": "Right, so, um, again, y is the data that, uh, this probability distribution is trying to model.",
    "start": "1082395",
    "end": "1090075"
  },
  {
    "text": "Eta is the parameter of the distribution. Um, t of y,",
    "start": "1090075",
    "end": "1096179"
  },
  {
    "text": "which will mostly be just y, um, but technically you know, t of y is more, more correct.",
    "start": "1096180",
    "end": "1101490"
  },
  {
    "text": "Um, um, b of y, which means it is a function of only y.",
    "start": "1101490",
    "end": "1107565"
  },
  {
    "text": "This function cannot involve Eta. All right. And similarly t of y cannot involve Eta. It should be purely a function of y. Um,",
    "start": "1107565",
    "end": "1115455"
  },
  {
    "text": "b of y is called the base measure, and a of Eta, which has to be a function of only Eta and, and constants.",
    "start": "1115455",
    "end": "1122280"
  },
  {
    "text": "No, no y can, can, uh, can be part of a of, uh, Eta. This is called the log-partition function.",
    "start": "1122280",
    "end": "1128625"
  },
  {
    "text": "Right. And, uh, the reason why this is called the log-partition function",
    "start": "1128625",
    "end": "1133710"
  },
  {
    "text": "is pretty easy to see because this can be written as b of y,",
    "start": "1133710",
    "end": "1140520"
  },
  {
    "text": "ex of Eta, times t of y over.",
    "start": "1140520",
    "end": "1151845"
  },
  {
    "text": "So these two are exactly the same. Um, just take this out and, um, um.",
    "start": "1151845",
    "end": "1159510"
  },
  {
    "text": "Sorry, this should be the log.",
    "start": "1159510",
    "end": "1162520"
  },
  {
    "text": "I think it's fine. These two are exactly the same.",
    "start": "1171680",
    "end": "1178860"
  },
  {
    "text": "And, uh. It should be the [inaudible] and that should be positive. Oh, yeah, you're right. This should be positive, um. Thank you.",
    "start": "1178860",
    "end": "1190155"
  },
  {
    "text": "So, uh, this is, um, you can think of this as a normalizing constant of the distribution such that the,",
    "start": "1190155",
    "end": "1197610"
  },
  {
    "text": "um, the whole thing integrates to 1, right? Um, and, uh, therefore the log of this will be a of Eta,",
    "start": "1197610",
    "end": "1204540"
  },
  {
    "text": "that's why it's just called the log of the partition function. So the partition function is a technical term to indicate the normalizing constant of, uh, probability distributions.",
    "start": "1204540",
    "end": "1212700"
  },
  {
    "text": "Now, um, you can plug-in any definition of b,",
    "start": "1212700",
    "end": "1220844"
  },
  {
    "text": "a, and t. Yeah.",
    "start": "1220844",
    "end": "1226710"
  },
  {
    "text": "Sure. So why is your y, and for most of, uh, most of our example is going to be a scalar.",
    "start": "1226710",
    "end": "1234750"
  },
  {
    "text": "Eta can be a vector. But we will also be focusing, uh,",
    "start": "1234750",
    "end": "1241005"
  },
  {
    "text": "except maybe in Softmax, um, this would be, uh, a scalar. T of y has to match,",
    "start": "1241005",
    "end": "1248010"
  },
  {
    "text": "so these- the dimension of these two has to match [NOISE].",
    "start": "1248010",
    "end": "1255630"
  },
  {
    "text": "And these are scalars, right? So for any choice of a,",
    "start": "1255630",
    "end": "1264390"
  },
  {
    "text": "b and t, that you've- that, that, that can be your choice completely.",
    "start": "1264390",
    "end": "1269414"
  },
  {
    "text": "As long as the expression integrates to 1, you have a family in the exponential family, right?",
    "start": "1269415",
    "end": "1277680"
  },
  {
    "text": "Uh, what does that mean? For a specific choice of, say, for, for, for some choice of a,",
    "start": "1277680",
    "end": "1283230"
  },
  {
    "text": "b, and t. This can actually- this will be equal to say the, uh, PDF of the Gaussian, in which case you,",
    "start": "1283230",
    "end": "1290399"
  },
  {
    "text": "you got for that choice of t, a, and, and b, you got the Gaussian distribution.",
    "start": "1290400",
    "end": "1296325"
  },
  {
    "text": "A family of Gaussian distribution such that for any value of the parameter, you get a member of the Gaussian family. All right.",
    "start": "1296325",
    "end": "1304695"
  },
  {
    "text": "And this is mostly, uh, to show that, uh, a distribution is in the exponential family.",
    "start": "1304695",
    "end": "1311430"
  },
  {
    "text": "Um, the most straightforward way to do it is to write out the PDF of the distribution in a form that you know,",
    "start": "1311430",
    "end": "1318640"
  },
  {
    "text": "and just do some algebraic massaging to bring it into this form, right? And then you do a pattern match to, to and,",
    "start": "1318640",
    "end": "1326015"
  },
  {
    "text": "and, you know, conclude that it's a member of the exponential family. So let's do it for a couple of examples.",
    "start": "1326015",
    "end": "1332730"
  },
  {
    "text": "So, uh, we have [NOISE].",
    "start": "1335450",
    "end": "1351960"
  },
  {
    "text": "So, uh, a Bernoulli distribution is one you use to, uh, model binary data.",
    "start": "1351960",
    "end": "1358900"
  },
  {
    "text": "Right. And it has a parameter, uh, let's call it Phi, which is,",
    "start": "1361310",
    "end": "1367620"
  },
  {
    "text": "you know, the probability of the event happening or not. Right, right. Now, the,",
    "start": "1367620",
    "end": "1380385"
  },
  {
    "text": "uh, what's the PDF of a Bernoulli distribution?",
    "start": "1380385",
    "end": "1387000"
  },
  {
    "text": "One way to, um, write this is Phi of y,",
    "start": "1387000",
    "end": "1393675"
  },
  {
    "text": "times 1 minus Phi, 1 minus y. I think this makes sense.",
    "start": "1393675",
    "end": "1400815"
  },
  {
    "text": "This, this pattern is like, uh, uh, a way of writing a programming- programmatic if else in,",
    "start": "1400815",
    "end": "1408105"
  },
  {
    "text": "in, in math. All right. So whenever y is 1, this term cancels out, so the answer would be Phi.",
    "start": "1408105",
    "end": "1415110"
  },
  {
    "text": "And whenever y is 0 this term cancels out and the answer is 1 minus Phi. So this is just a mathematical way to,",
    "start": "1415110",
    "end": "1421965"
  },
  {
    "text": "to represent an if else that you would do in programming, right. So this is the PDF of, um, a Bernoulli.",
    "start": "1421965",
    "end": "1429225"
  },
  {
    "text": "And our goal is to take this form and massage it into that form, right,",
    "start": "1429225",
    "end": "1435855"
  },
  {
    "text": "and see what, what the individual t, b, and a turn out to be, right.",
    "start": "1435855",
    "end": "1440895"
  },
  {
    "text": "So, uh, whenever you, you, uh, see your distribution in this form, a common, um,",
    "start": "1440895",
    "end": "1448455"
  },
  {
    "text": "technique is to wrap this with a log and then Exp.",
    "start": "1448455",
    "end": "1460110"
  },
  {
    "text": "Right, um, because these two cancel out so, uh, this is actually exactly equal to this [NOISE].",
    "start": "1460110",
    "end": "1472890"
  },
  {
    "text": "And, uh, if you, uh, do some more algebra and this, uh, we will see that,",
    "start": "1472890",
    "end": "1479520"
  },
  {
    "text": "this turns out to be Exp of log Phi over 1 minus Phi times y,",
    "start": "1479520",
    "end": "1490810"
  },
  {
    "text": "plus log of 1 minus Phi, right?",
    "start": "1491030",
    "end": "1499470"
  },
  {
    "text": "It's pretty straightforward to go from here to here. Um, I'll, I'll let you guys,uh, uh, verify it yourself.",
    "start": "1499470",
    "end": "1505515"
  },
  {
    "text": "But once we have it in this form, um, it's easy to kind of start doing some pattern matching,",
    "start": "1505515",
    "end": "1510600"
  },
  {
    "text": "from this expression to, uh, that expression. So what, what we see, um,",
    "start": "1510600",
    "end": "1515804"
  },
  {
    "text": "here is, uh, the base measure b of y is equal to.",
    "start": "1515805",
    "end": "1521020"
  },
  {
    "text": "If you match this with that, b of y will be just 1.",
    "start": "1521180",
    "end": "1526545"
  },
  {
    "text": "Uh, because there's no b of y term here. All right. And, um, so this would be b of y.",
    "start": "1526545",
    "end": "1535270"
  },
  {
    "text": "This would be Eta. This would be t of y.",
    "start": "1535880",
    "end": "1542730"
  },
  {
    "text": "This would be a of Eta, right? So that could be, uh, um,",
    "start": "1542730",
    "end": "1550680"
  },
  {
    "text": "you can see that the kind of matching pattern. So b of y would be 1.",
    "start": "1550680",
    "end": "1557070"
  },
  {
    "text": "T of y is just y, as, um, as expected.",
    "start": "1557070",
    "end": "1563410"
  },
  {
    "text": "Um, so Eta is equal to log Phi over 1 minus Phi.",
    "start": "1563900",
    "end": "1574510"
  },
  {
    "text": "And, uh, this is an equivalent statement is to invert this operation and say",
    "start": "1574600",
    "end": "1583575"
  },
  {
    "text": "Phi is equal to 1 over 1 plus e to the minus Eta.",
    "start": "1583575",
    "end": "1593200"
  },
  {
    "text": "I'm just flipping the operation from, uh, this went from Phi to Eta here.",
    "start": "1593330",
    "end": "1598695"
  },
  {
    "text": "It's, it's, it's the equivalent. Now, here it goes from Eta to Phi, right? And a of Eta is going to be, um,",
    "start": "1598695",
    "end": "1611505"
  },
  {
    "text": "so here we have it as a function of Phi, but we got an expression for Phi in terms of eta,",
    "start": "1611505",
    "end": "1618929"
  },
  {
    "text": "so you can plug this expression in here, and that, uh, change of minus sign.",
    "start": "1618930",
    "end": "1626520"
  },
  {
    "text": "So, so, let, let me work out this, minus log of 1 minus Phi.",
    "start": "1626520",
    "end": "1632895"
  },
  {
    "text": "This is, uh, just, uh, the pattern matching there. And minus log 1 minus,",
    "start": "1632895",
    "end": "1642240"
  },
  {
    "text": "this thing over, 1 over 1 plus Eta to the minus Eta. The reason is because we want an expression in terms of Eta.",
    "start": "1642240",
    "end": "1650025"
  },
  {
    "text": "Here we got it in terms of Phi, but we need to, uh, plug in, um, plug in Eta over here.",
    "start": "1650025",
    "end": "1655845"
  },
  {
    "text": "Uh, Eta, and this will just be, uh, log of 1 plus e to the Eta.",
    "start": "1655845",
    "end": "1663790"
  },
  {
    "text": "Right. So there you go. So this, this kind of, uh,",
    "start": "1664130",
    "end": "1669195"
  },
  {
    "text": "verifies that the Bernoulli distribution is a member of the exponential family. Any questions here? So note that this may look familiar.",
    "start": "1669195",
    "end": "1681105"
  },
  {
    "text": "It looks like the, uh, sigmoid function, somewhat like the sigmoid function,",
    "start": "1681105",
    "end": "1686160"
  },
  {
    "text": "and there's actually no accident. We'll see, uh, why, why it's, uh, actually the sigmoid- how,",
    "start": "1686160",
    "end": "1692909"
  },
  {
    "text": "how it kind of relates to, uh, logistic regression in a minute. So another example, um",
    "start": "1692910",
    "end": "1698640"
  },
  {
    "text": "[NOISE].",
    "start": "1698640",
    "end": "1708195"
  },
  {
    "text": "So, uh, a Gaussian with fixed variance.",
    "start": "1708195",
    "end": "1718870"
  },
  {
    "text": "Right, so, um, a Gaussian distribution, um, has two parameters the mean and the variance, uh,",
    "start": "1721100",
    "end": "1729179"
  },
  {
    "text": "for our purposes we're gonna assume a constant variance, um, you-you can, uh, have,",
    "start": "1729180",
    "end": "1735690"
  },
  {
    "text": "um, you can also consider Gaussians with, with where the variance is also a variable,",
    "start": "1735690",
    "end": "1741225"
  },
  {
    "text": "but for-for, uh, our course we are go- we are only interested in, um, Gaussians with fixed variance and we are going to assume,",
    "start": "1741225",
    "end": "1749770"
  },
  {
    "text": "assume that variance is equal to 1.",
    "start": "1751190",
    "end": "1756240"
  },
  {
    "text": "So, this gives the PDF of a Gaussian to look like this, p of y parameterized as mu. So note here,",
    "start": "1756240",
    "end": "1766679"
  },
  {
    "text": "when we start writing out, we start with the, uh, parameters that we are, um,",
    "start": "1766680",
    "end": "1772740"
  },
  {
    "text": "commonly used to, and we- they are also called like the canonical parameters. And then we set up a link between the canonical parameters and the natural parameters,",
    "start": "1772740",
    "end": "1781080"
  },
  {
    "text": "that's part of the massaging exercise that we do. So we're going to start with the canonical parameters, um,",
    "start": "1781080",
    "end": "1787780"
  },
  {
    "text": "is equal to 1 over root 2 pi, minus over 2.",
    "start": "1787780",
    "end": "1802680"
  },
  {
    "text": "So this is the Gaussian PDF with, um, with- with a variance equal to 1, right,",
    "start": "1802680",
    "end": "1810105"
  },
  {
    "text": "and this can be rewritten as- again,",
    "start": "1810105",
    "end": "1815340"
  },
  {
    "text": "I'm skipping a few algebra steps, you know, straightforward no tricks there,",
    "start": "1815340",
    "end": "1821020"
  },
  {
    "text": "uh, any question? Yep? [BACKGROUND].",
    "start": "1821300",
    "end": "1826530"
  },
  {
    "text": "Fixed variance. E to the minus y squared over 2, times EX.",
    "start": "1826530",
    "end": "1836470"
  },
  {
    "text": "Again, we go to the same exercise, you know, pattern match, this is b of y,",
    "start": "1843770",
    "end": "1852070"
  },
  {
    "text": "this is eta, this is t of y,",
    "start": "1852410",
    "end": "1859230"
  },
  {
    "text": "and this would be a of eta, right?",
    "start": "1859230",
    "end": "1865309"
  },
  {
    "text": "So, uh, we have, uh, b of y equals 1 over root 2",
    "start": "1865310",
    "end": "1871685"
  },
  {
    "text": "pi minus y squared by 2.",
    "start": "1871685",
    "end": "1877470"
  },
  {
    "text": "Note that this is a function of only y, there's no eta here, um, t of y is just y, and in this case,",
    "start": "1877470",
    "end": "1886110"
  },
  {
    "text": "the natural parameter is-is mu, eta is mu, and the log partition function is equal to mu square by 2,",
    "start": "1886110",
    "end": "1897750"
  },
  {
    "text": "and when we-and we repeat the same exercise we did here,",
    "start": "1897750",
    "end": "1903300"
  },
  {
    "text": "we start with a log partition function that is parameterized by the canonical parameters,",
    "start": "1903300",
    "end": "1909915"
  },
  {
    "text": "and we use the, the link between the canonical and, and, uh, the natural parameters, invert it and,",
    "start": "1909915",
    "end": "1917640"
  },
  {
    "text": "um, um, so in this case it's- it's the- it's the same sets, eta over 2.",
    "start": "1917640",
    "end": "1924615"
  },
  {
    "text": "So, a of eta is a function of only eta, again here a of eta was a function of only eta,",
    "start": "1924615",
    "end": "1929955"
  },
  {
    "text": "and, um, p of y is a function of only y, and b of y is a function of only,",
    "start": "1929955",
    "end": "1934980"
  },
  {
    "text": "um, y as well. Any questions on this? Yeah.",
    "start": "1934980",
    "end": "1944010"
  },
  {
    "text": "If the variance is unknown [inaudible]. Yeah, you- if, if the variance is unknown you can write it as",
    "start": "1944010",
    "end": "1951120"
  },
  {
    "text": "an exponential family in which case eta will now be a vector, it won't be a scalar anymore, it'll be- it'll have two, uh,",
    "start": "1951120",
    "end": "1957090"
  },
  {
    "text": "like eta1 and eta2, and you will also have, um,",
    "start": "1957090",
    "end": "1962190"
  },
  {
    "text": "you will have a mapping between each of the canonical parameters and each of the natural parameters,",
    "start": "1962190",
    "end": "1968339"
  },
  {
    "text": "you, you can do it, uh, you know, it's pretty straightforward. Right, so this is- this is exponential- these are exponential families, right?",
    "start": "1968339",
    "end": "1978495"
  },
  {
    "text": "Uh, the reason why we are, uh, why we use exponential families is because it has some nice mathematical properties, right?",
    "start": "1978495",
    "end": "1990900"
  },
  {
    "text": "So, uh, so one property is now,",
    "start": "1990900",
    "end": "1996570"
  },
  {
    "text": "uh, if we perform maximum likelihood on, um, on the exponential family,",
    "start": "1996570",
    "end": "2002690"
  },
  {
    "text": "um, as, as, uh, when, when the exponential family is parameterized in the natural parameters,",
    "start": "2002690",
    "end": "2009979"
  },
  {
    "text": "then, uh, the optimization problem is concave. So MLE with respect to eta is concave.",
    "start": "2009979",
    "end": "2023955"
  },
  {
    "text": "Similarly, if you, uh, flip this sign and use the, the, uh, what's called the negative log-likelihood,",
    "start": "2023955",
    "end": "2030408"
  },
  {
    "text": "so you take the log of the expression negate it and in this case, the negative log-likelihood is like",
    "start": "2030409",
    "end": "2035779"
  },
  {
    "text": "the cost function equivalent of doing maximum likelihood, so you're just flipping the sign, instead of maximizing, you minimize the negative log likelihood,",
    "start": "2035780",
    "end": "2042230"
  },
  {
    "text": "so-and, and you know, uh, the NLL is therefore convex, okay.",
    "start": "2042230",
    "end": "2050284"
  },
  {
    "text": "Um, the expectation of y.",
    "start": "2050285",
    "end": "2058349"
  },
  {
    "text": "What does this mean? Um, each of the distribution,",
    "start": "2065710",
    "end": "2071915"
  },
  {
    "text": "uh, we start with, uh, a of eta, differentiate this with respect to eta,",
    "start": "2071915",
    "end": "2077495"
  },
  {
    "text": "the log partition function with respect to eta, and you get another function with respect to eta,",
    "start": "2077495",
    "end": "2083179"
  },
  {
    "text": "and that function will- is, is the mean of the distribution as parameterized by eta,",
    "start": "2083180",
    "end": "2088730"
  },
  {
    "text": "and similarly the variance of y parameterized by eta,",
    "start": "2088730",
    "end": "2098180"
  },
  {
    "text": "is just the second derivative, this was the first derivative, this is the second derivative, this is eta.",
    "start": "2098180",
    "end": "2108110"
  },
  {
    "text": "So, um, the reason why",
    "start": "2108110",
    "end": "2113120"
  },
  {
    "text": "this is nice is because in general for probability distributions to calculate the mean and the variance,",
    "start": "2113120",
    "end": "2118235"
  },
  {
    "text": "you generally need to integrate something, but over here you just need to differentiate, which is a lot easier operation, all right?",
    "start": "2118235",
    "end": "2123859"
  },
  {
    "text": "And, um, and you",
    "start": "2123860",
    "end": "2131900"
  },
  {
    "text": "will be proving these properties in your first homework.",
    "start": "2131900",
    "end": "2135809"
  },
  {
    "text": "You're provided hints so it should be [LAUGHTER]. All right, so, um,",
    "start": "2138910",
    "end": "2145954"
  },
  {
    "text": "now we're going to move on to, uh, generalized linear models, uh, this- this is all we wanna talk about exponential families, any questions? Yep.",
    "start": "2145955",
    "end": "2159590"
  },
  {
    "text": "[inaudible].",
    "start": "2159590",
    "end": "2166130"
  },
  {
    "text": "Exactly, so, ah, if you're-if you're, um, if you're- if it's a multi-variate Gaussian,",
    "start": "2166130",
    "end": "2171529"
  },
  {
    "text": "then this eta would be a vector, and this would be the Hessian.",
    "start": "2171530",
    "end": "2176549"
  },
  {
    "text": "All right, let's move on to, uh, GLM's.",
    "start": "2182440",
    "end": "2186720"
  },
  {
    "text": "So the GLM is, is, um, somewhat like a natural extension of the exponential families to include,",
    "start": "2195210",
    "end": "2204040"
  },
  {
    "text": "um, include covariates or include your input features in some way, right.",
    "start": "2204040",
    "end": "2209620"
  },
  {
    "text": "So over here, uh, we are only dealing with, uh, in, in the exponential families, you're only dealing with like the y, uh, which in,",
    "start": "2209620",
    "end": "2216985"
  },
  {
    "text": "in our case, it- it'll kind of map to the outputs, um. But, um, we can actually build a lot of many powerful models by,",
    "start": "2216985",
    "end": "2228580"
  },
  {
    "text": "by choosing, uh, an appropriate, um, um, family in the exponential family and kind of plugging it onto a, a linear model.",
    "start": "2228580",
    "end": "2239410"
  },
  {
    "text": "So, so the, uh, assumptions we are going to make for GLM is that one, um,",
    "start": "2239410",
    "end": "2246295"
  },
  {
    "text": "so these are the assumptions or",
    "start": "2246295",
    "end": "2252880"
  },
  {
    "text": "design choices that are gonna take us from exponential families to,",
    "start": "2252880",
    "end": "2260769"
  },
  {
    "text": "uh, generalized linear models. So the most important assumption is that, uh, well, yeah.",
    "start": "2260770",
    "end": "2266430"
  },
  {
    "text": "Assumption is that y given x parameterized",
    "start": "2266430",
    "end": "2271950"
  },
  {
    "text": "by Theta is a member of an exponential family.",
    "start": "2271950",
    "end": "2278320"
  },
  {
    "text": "Right. By exponential family of Theta, I mean that form.",
    "start": "2286410",
    "end": "2291595"
  },
  {
    "text": "It could, it could, uh, in, in the particular, uh, uh, uh, scenario that you have, it could take on any one of these, um, uh, distributions.",
    "start": "2291595",
    "end": "2301195"
  },
  {
    "text": "Um, we only, we only, uh, talked about the Bernoullian Gaussian.",
    "start": "2301195",
    "end": "2306295"
  },
  {
    "text": "There are also, um, other distributions that are- those are part of the, uh, exponential family.",
    "start": "2306295",
    "end": "2312940"
  },
  {
    "text": "For example, um, I forgot to mention this. So if you have, uh,",
    "start": "2312940",
    "end": "2318640"
  },
  {
    "text": "real value data, you use a Gaussian.",
    "start": "2318640",
    "end": "2323420"
  },
  {
    "text": "If you have binary, a Bernoulli.",
    "start": "2323850",
    "end": "2330260"
  },
  {
    "text": "If you have count, uh, like, counts here.",
    "start": "2331440",
    "end": "2336595"
  },
  {
    "text": "And so this is a real value. It can take any value between zero and infinity by count. That means just non-negative integers,",
    "start": "2336595",
    "end": "2343735"
  },
  {
    "text": "uh, but not anything between it. So if you have counts, you can use a Poisson. If you have uh, positive real value integers like say,",
    "start": "2343735",
    "end": "2354865"
  },
  {
    "text": "the volume of some object or a time to an event which, you know, um, that you are only predicting into the future.",
    "start": "2354865",
    "end": "2360700"
  },
  {
    "text": "So here, you can use, uh, like Gamma or exponential.",
    "start": "2360700",
    "end": "2367670"
  },
  {
    "text": "So, um, so there is the exponential family, and there is also a distribution called the exponential distribution,",
    "start": "2369300",
    "end": "2375984"
  },
  {
    "text": "which are, you know, two distinct things. The exponential distribution happens to be a member of the exponential family as well,",
    "start": "2375985",
    "end": "2381775"
  },
  {
    "text": "but no, they're not the same thing. Um, the exponential and, um, yeah,",
    "start": "2381775",
    "end": "2387460"
  },
  {
    "text": "and you can also have, um, you can also have probability distributions over probability distributions.",
    "start": "2387460",
    "end": "2394190"
  },
  {
    "text": "Like, uh, the Beta, the Dirichlet.",
    "start": "2394920",
    "end": "2400730"
  },
  {
    "text": "These mostly show up in Bayesian machine learning or Bayesian statistics.",
    "start": "2401310",
    "end": "2406345"
  },
  {
    "text": "Right. So depending on the kind of data that you have,",
    "start": "2406345",
    "end": "2415015"
  },
  {
    "text": "if your y-variable is, is, is if you're trying to do a regression, then your y is going to be say, say a Gaussian.",
    "start": "2415015",
    "end": "2421420"
  },
  {
    "text": "If you're trying to do a classification, then your y is, and if it's a binary classification, then the exponential family would be Bernoulli.",
    "start": "2421420",
    "end": "2428290"
  },
  {
    "text": "So depending on the problem that you have, you can choose any member of the exponential family,",
    "start": "2428290",
    "end": "2433315"
  },
  {
    "text": "um, as, as parameterized by Eta.",
    "start": "2433315",
    "end": "2438400"
  },
  {
    "text": "And so that's the first assumption. That y conditioned on y given x is a member of the exponential family.",
    "start": "2438400",
    "end": "2446724"
  },
  {
    "text": "And the, uh, second, the design choice that we are making here is that Eta is equal to Theta transpose x.",
    "start": "2446725",
    "end": "2456204"
  },
  {
    "text": "So this is where your x now comes into the picture. Right. So Theta is, um,",
    "start": "2456205",
    "end": "2463240"
  },
  {
    "text": "is in Rn, and x is also in Rn.",
    "start": "2463850",
    "end": "2471120"
  },
  {
    "text": "Now, this n has nothing to do with anything in the exponential family. It's purely, um, a dimensions of your of,",
    "start": "2471120",
    "end": "2479365"
  },
  {
    "text": "of your data that you have, of the x's of your inputs, and,  and this does not show up anywhere else. And that, that- that's, um.",
    "start": "2479365",
    "end": "2486740"
  },
  {
    "text": "And, and, uh, Eta is, is, uh, we,",
    "start": "2487020",
    "end": "2492610"
  },
  {
    "text": "we make a design choice that Eta will be Theta transpose- transpose x. Um, and",
    "start": "2492610",
    "end": "2499840"
  },
  {
    "text": "another kind of assumption is that at test time, um, right.",
    "start": "2499840",
    "end": "2509290"
  },
  {
    "text": "When we want an output for a new x, given a new x, we want to make an output, right. So the output will be, right.",
    "start": "2509290",
    "end": "2524550"
  },
  {
    "text": "So given an x and, um, given an x, we get, uh, an exponential family distribution, right.",
    "start": "2524550",
    "end": "2531444"
  },
  {
    "text": "And the mean of that distribution will be the prediction that we make for a given, for a given x. Um, this may sound a little abstract, but, you know,",
    "start": "2531445",
    "end": "2539935"
  },
  {
    "text": "uh, we're going to make this, uh, uh, more clear. So this- what this essentially means is that the hypothesis function",
    "start": "2539935",
    "end": "2546320"
  },
  {
    "text": "is actually just, right.",
    "start": "2546330",
    "end": "2553180"
  },
  {
    "text": "This is our hypothesis function. And we will see that, you know, what we do over here, if you plug in the,",
    "start": "2553180",
    "end": "2558595"
  },
  {
    "text": "uh, um, exponential family, uh, as, as Gaussian, then the hypothesis will be the same, you know,",
    "start": "2558595",
    "end": "2564115"
  },
  {
    "text": "Gaussian hypothesis that we saw in linear regression. If we plug in a Bernoulli, then this will turn out to be the same hypothesis that we saw in logistic regression,",
    "start": "2564115",
    "end": "2572800"
  },
  {
    "text": "and so on, right. So, uh, one way to kind of, um, visualize this is,",
    "start": "2572800",
    "end": "2580490"
  },
  {
    "text": "right. So one way to think of is, of- if this is, there is a model and there is a distribution, right.",
    "start": "2620370",
    "end": "2627369"
  },
  {
    "text": "So the model we are assuming it to be a linear model, right. Given x, there is a learnable parameter Theta,",
    "start": "2627370",
    "end": "2633370"
  },
  {
    "text": "and Theta transpose x will give you a parameter, right. This is the model, and here is the distribution.",
    "start": "2633370",
    "end": "2639880"
  },
  {
    "text": "Now, the distribution, um, is a member of the exponential family. And the parameter for this distribution is the output of the linear model, right.",
    "start": "2639880",
    "end": "2650110"
  },
  {
    "text": "This, this is the picture you want to have in your mind. And the exponential family, we make, uh, depending on the data that we have.",
    "start": "2650110",
    "end": "2657430"
  },
  {
    "text": "Whether it's a, you know, whether it's, uh, a classification problem or a regression problem or a time to vent problem, you would choose an appropriate b,",
    "start": "2657430",
    "end": "2665785"
  },
  {
    "text": "a and t, uh, based on the distribution of your choice, right.",
    "start": "2665785",
    "end": "2671455"
  },
  {
    "text": "So this entire thing, uh, a-and from this, you can say, uh,",
    "start": "2671455",
    "end": "2677335"
  },
  {
    "text": "get the, uh, expectation of y given Eta.",
    "start": "2677335",
    "end": "2685330"
  },
  {
    "text": "And this is the same as expectation of y given Theta transpose x, right.",
    "start": "2685330",
    "end": "2694120"
  },
  {
    "text": "And this is essentially our hypothesis function.",
    "start": "2694120",
    "end": "2698540"
  },
  {
    "text": "Right.",
    "start": "2699390",
    "end": "2709450"
  },
  {
    "text": "Yep. [BACKGROUND] That's exactly right. Uh, so, uh, so the question is, um, are we training Theta to, uh, uh, um,",
    "start": "2712560",
    "end": "2721809"
  },
  {
    "text": "to predict the parameter of the, um, exponential family distribution whose mean is,",
    "start": "2721810",
    "end": "2728320"
  },
  {
    "text": "um, the, uh, uh, uh, prediction that we're gonna make for y. That's, that's correct, right.",
    "start": "2728320",
    "end": "2734320"
  },
  {
    "text": "And, um, so this is what we do at test time, right.",
    "start": "2734320",
    "end": "2740275"
  },
  {
    "text": "And during training time, how do we train this model?",
    "start": "2740275",
    "end": "2746620"
  },
  {
    "text": "So in this model, the parameter that we are learning by doing gradient descent, are these parameters, right.",
    "start": "2746620",
    "end": "2752890"
  },
  {
    "text": "So you're not learning any the parameters in the, uh, in the, uh, uh, exponential family.",
    "start": "2752890",
    "end": "2759130"
  },
  {
    "text": "We're not learning Mu or Sigma square or, or Eta. We are not learning those. We're learning Theta that's part of the model,",
    "start": "2759130",
    "end": "2765700"
  },
  {
    "text": "and not part of, uh, the distribution. And the output of this will become the, um, the distributions parameter.",
    "start": "2765700",
    "end": "2772480"
  },
  {
    "text": "It's unfortunate that we use the word parameter for this and that, but, uh, there,",
    "start": "2772480",
    "end": "2778180"
  },
  {
    "text": "there are- it's important to understand what, what is being learned during training phase and, and, and what's not.",
    "start": "2778180",
    "end": "2785695"
  },
  {
    "text": "So this parameter is the output of a function. It's not, it's not a variable that we,",
    "start": "2785695",
    "end": "2791635"
  },
  {
    "text": "that we, uh, do gradient descent on. So during learning, what we do is maximum likelihood.",
    "start": "2791635",
    "end": "2799255"
  },
  {
    "text": "Maximize with respect to Theta of P of",
    "start": "2799255",
    "end": "2806200"
  },
  {
    "text": "y i given, right.",
    "start": "2806200",
    "end": "2814224"
  },
  {
    "text": "So you're doing gradient ascent on the log probability of,",
    "start": "2814225",
    "end": "2820720"
  },
  {
    "text": "of y where, um, the, the, um, natural parameter was re-parameterized, uh,",
    "start": "2820720",
    "end": "2827920"
  },
  {
    "text": "with the linear model, right. And we are doing gradient ascent by taking gradients on Theta, right.",
    "start": "2827920",
    "end": "2835075"
  },
  {
    "text": "Thi-this is like the big picture of what's happening with GLMs, and how they kind of, yeah, are an extension of exponential families.",
    "start": "2835075",
    "end": "2841540"
  },
  {
    "text": "You re-parameterize the parameters with the linear model, and you get a GLM. [NOISE].",
    "start": "2841540",
    "end": "2860230"
  },
  {
    "text": "So let's, let's look at, uh, some more detail on what happens at train time. [NOISE]",
    "start": "2861050",
    "end": "2900540"
  },
  {
    "text": "So another, um, kind of incidental benefit of using, uh, uh,",
    "start": "2900540",
    "end": "2905790"
  },
  {
    "text": "GLMs is",
    "start": "2905790",
    "end": "2913890"
  },
  {
    "text": "that at train time, we saw that you wanna do, um,",
    "start": "2913890",
    "end": "2920700"
  },
  {
    "text": "maximum likelihood on the log prob- using the log probability with respect to Thetas, right?",
    "start": "2920700",
    "end": "2926924"
  },
  {
    "text": "Now, um, at first it may appear that, you know, we need to do some more algebra, uh,",
    "start": "2926925",
    "end": "2933750"
  },
  {
    "text": "figure out what the expression for, you know, P is, um, represented in the- in-",
    "start": "2933750",
    "end": "2939555"
  },
  {
    "text": "in- as a function of Theta transpose x and take the derivatives and, you know, come up with a gradient update rule and so on.",
    "start": "2939555",
    "end": "2946695"
  },
  {
    "text": "But it turns out that, uh, no matter which- uh,",
    "start": "2946695",
    "end": "2953070"
  },
  {
    "text": "what kind of GLM you're doing, no matter which choice of distribution that you make, the learning update rule is the same.",
    "start": "2953070",
    "end": "2961890"
  },
  {
    "text": "[NOISE] The learning update rule is Theta equals",
    "start": "2961890",
    "end": "2972734"
  },
  {
    "text": "Theta j plus Alpha times y_i",
    "start": "2972735",
    "end": "2978855"
  },
  {
    "text": "minus h Theta of x_i.",
    "start": "2978855",
    "end": "2986619"
  },
  {
    "text": "You guys have seen this so many times by now. So this is- you can,",
    "start": "2987200",
    "end": "2992685"
  },
  {
    "text": "you can straight away just apply this learning rule without ever having to,",
    "start": "2992685",
    "end": "2998700"
  },
  {
    "text": "um, do any more algebra to figure out what the gradients are or what the- what, what the loss is.",
    "start": "2998700",
    "end": "3005299"
  },
  {
    "text": "You can go straight to the update rule and do your learning. You plug in the appropriate h Theta of x,",
    "start": "3005300",
    "end": "3012020"
  },
  {
    "text": "you plug in the appropriate h Theta of x, uh, depending on the choice of distribution that you make and you can start learning.",
    "start": "3013000",
    "end": "3020015"
  },
  {
    "text": "Initialize your Theta to some random values and, and, and you can start learning. So um, any question on this? Yeah.",
    "start": "3020015",
    "end": "3030170"
  },
  {
    "text": "[inaudible] You can do, uh- if you wanna do it for batch gradient descent,",
    "start": "3030170",
    "end": "3039470"
  },
  {
    "text": "then you just, um, sum over all your examples. [inaudible]",
    "start": "3039470",
    "end": "3050750"
  },
  {
    "text": "Yeah. So, um, the uh, Newton method is, is, uh, is probably the most common you would use with GLMs, uh,",
    "start": "3050750",
    "end": "3058235"
  },
  {
    "text": "and that again comes with the assumption that you're- the dimensionality of your data is not extremely high.",
    "start": "3058235",
    "end": "3063964"
  },
  {
    "text": "As long as the number of features is less than a few thousand, then you can do Newton's method.",
    "start": "3063965",
    "end": "3070530"
  },
  {
    "text": "Any other questions? Good. So, um,",
    "start": "3072610",
    "end": "3081090"
  },
  {
    "text": "so this is the same update rule for any, any, um, any specific type of GLM based on the choice of distribution that you have.",
    "start": "3083590",
    "end": "3092135"
  },
  {
    "text": "Whether you are modeling, uh, you know, um, you're doing classification, whether you're doing regression, whether you're doing- you know,",
    "start": "3092135",
    "end": "3098390"
  },
  {
    "text": "a Poisson regression, the update rule is the same. You just plug in a different h Theta of x and you get your learning rule.",
    "start": "3098390",
    "end": "3105360"
  },
  {
    "text": "Another, um, some more terminology.",
    "start": "3106720",
    "end": "3112859"
  },
  {
    "text": "So Eta is what we call the natural parameter.",
    "start": "3118660",
    "end": "3123980"
  },
  {
    "text": "[NOISE] So Eta is",
    "start": "3123980",
    "end": "3132859"
  },
  {
    "text": "the natural parameter and the function that links the natural parameter",
    "start": "3132860",
    "end": "3147369"
  },
  {
    "text": "to the mean of the distribution and this has a name, it's called the canonical response function.",
    "start": "3147370",
    "end": "3153560"
  },
  {
    "text": "Right. And, um, similarly, you can also- let's call it Mu.",
    "start": "3162060",
    "end": "3168339"
  },
  {
    "text": "It's like the mean of the distribution. Uh, similarly you can go from Mu back to Eta with the inverse of this,",
    "start": "3168340",
    "end": "3177995"
  },
  {
    "text": "and this is also called the canonical link function.",
    "start": "3177995",
    "end": "3183420"
  },
  {
    "text": "There's some, uh, terminology. We also already saw that g of Eta is also equal to the,",
    "start": "3186220",
    "end": "3198635"
  },
  {
    "text": "the, the gradient of the log partition function with respect to Eta. So a side-note g is equal to- [NOISE]",
    "start": "3198635",
    "end": "3220520"
  },
  {
    "text": "right. And it's also helpful to make- explicit the distinction between",
    "start": "3220520",
    "end": "3226910"
  },
  {
    "text": "the three different kinds of parameterizations we have. So we have three parameterizations.",
    "start": "3226910",
    "end": "3232680"
  },
  {
    "text": "So we have the model parameters, that's Theta,",
    "start": "3236560",
    "end": "3246350"
  },
  {
    "text": "the natural parameters, that's Eta,",
    "start": "3246350",
    "end": "3254390"
  },
  {
    "text": "and we have the canonical parameters.",
    "start": "3254390",
    "end": "3257910"
  },
  {
    "text": "And this is a Phi for Bernoulli, Mu and Sigma square for Gaussian, Lambda for Poisson.",
    "start": "3259960",
    "end": "3271440"
  },
  {
    "text": "Right. So these are three different ways we are- we can parameterize,",
    "start": "3271900",
    "end": "3277145"
  },
  {
    "text": "um, either the exponential family or, or, or the G- uh, GLM. And whenever we are learning a GLM,",
    "start": "3277145",
    "end": "3285605"
  },
  {
    "text": "it is only this thing that we learn. Right. That is the Theta in the linear model.",
    "start": "3285605",
    "end": "3293000"
  },
  {
    "text": "This is the Theta that is, that is learned. Right. And, uh, the connection between these two is, is linear.",
    "start": "3293000",
    "end": "3300950"
  },
  {
    "text": "So Theta transpose x will give you a natural parameter. Uh, and this is the design choice that we're making.",
    "start": "3300950",
    "end": "3309330"
  },
  {
    "text": "Right. We choose to reparameterize Eta by a linear model,",
    "start": "3311470",
    "end": "3316955"
  },
  {
    "text": "uh, a linear of- linear in your data. And, um, between these two,",
    "start": "3316955",
    "end": "3322250"
  },
  {
    "text": "you have g to go this way and g inverse to come back this way where g is also the,",
    "start": "3322250",
    "end": "3330860"
  },
  {
    "text": "uh, uh, uh, derivative of the log partition. So yeah. So it's important to,",
    "start": "3330860",
    "end": "3336665"
  },
  {
    "text": "to kind of realize. It can get pretty confusing when you're seeing this for the first time because you have so many parameters that are being swapped around and,",
    "start": "3336665",
    "end": "3344180"
  },
  {
    "text": "you know, getting reparameterized. There are three kind of spaces in which- three different ways in which we are parameterizing,",
    "start": "3344180",
    "end": "3351755"
  },
  {
    "text": "uh, uh, generalized linear models. Uh, the model parameters, the ones that we learn and the output of this is",
    "start": "3351755",
    "end": "3358490"
  },
  {
    "text": "the natural parameter for the exponential family and you can, you know, do some algebraic manipulations and get the canonical parameters for, uh,",
    "start": "3358490",
    "end": "3367280"
  },
  {
    "text": "the distribution, uh, that we are choosing, uh, depending on the task where there's classification or regression.",
    "start": "3367280",
    "end": "3373460"
  },
  {
    "text": "[NOISE]",
    "start": "3373460",
    "end": "3381605"
  },
  {
    "text": "Any questions on this? [NOISE]",
    "start": "3381605",
    "end": "3393230"
  },
  {
    "text": "So no- now it's actually pretty, you know, um, you can- you can see that, you know,",
    "start": "3393230",
    "end": "3398570"
  },
  {
    "text": "when you are doing logistic regression, [NOISE] right?",
    "start": "3398570",
    "end": "3406430"
  },
  {
    "text": "So h theta of X, um, so h theta of X, um,",
    "start": "3406430",
    "end": "3413375"
  },
  {
    "text": "is the expected value of- of,",
    "start": "3413375",
    "end": "3420000"
  },
  {
    "text": "um, of Y, uh, conditioned on X theta,",
    "start": "3422500",
    "end": "3427550"
  },
  {
    "text": "[NOISE] and this is equal to phi, right?",
    "start": "3427550",
    "end": "3435305"
  },
  {
    "text": "Because, um, here the choice of distribution is a Bernoulli. And the mean of a Bernoulli distribution is",
    "start": "3435305",
    "end": "3441650"
  },
  {
    "text": "just phi the- in- in the canonical parameter space. And if we, um,",
    "start": "3441650",
    "end": "3448505"
  },
  {
    "text": "write that as, um, in terms of the, um, h minus eta and this is",
    "start": "3448505",
    "end": "3457670"
  },
  {
    "text": "equal to 1 over minus theta transpose X, right?",
    "start": "3457670",
    "end": "3463234"
  },
  {
    "text": "So, ah, the logistic function which when we introduced, ah, linear reg-, uh,",
    "start": "3463235",
    "end": "3468245"
  },
  {
    "text": "logistic regression we just, you know, pulled out the logistic function out of thin air, and said, hey,",
    "start": "3468245",
    "end": "3474470"
  },
  {
    "text": "this is something that can squash minus infinity to infinity, between 0 and 1, seems like a good choice.",
    "start": "3474470",
    "end": "3480095"
  },
  {
    "text": "Bu-but now we see that it is- it is a natural outcome. It just pops out from",
    "start": "3480095",
    "end": "3486110"
  },
  {
    "text": "this more elegant generalized linear model where if you choose Bernoulli to be, uh,",
    "start": "3486110",
    "end": "3493100"
  },
  {
    "text": "uh, to be the distribution of your, uh, output, then, you know, the logistic regression just- just pops out naturally.",
    "start": "3493100",
    "end": "3501980"
  },
  {
    "text": "[NOISE] So,um, [NOISE] any questions? Yeah.",
    "start": "3501980",
    "end": "3514340"
  },
  {
    "text": "Maybe you speak a little bit more about choosing a distribution to be the output.",
    "start": "3514340",
    "end": "3520845"
  },
  {
    "text": "Yeah. So the, uh, the choice of what distribution you are going to",
    "start": "3520845",
    "end": "3525910"
  },
  {
    "text": "choose is really dependent on the task that you have. So if your task is regression,",
    "start": "3525910",
    "end": "3531624"
  },
  {
    "text": "where you want to output real valued numbers like, you know, price of the house, or- or something, uh, then you choose a distribution over the real va- real- real numbers like a Gaussian.",
    "start": "3531624",
    "end": "3542825"
  },
  {
    "text": "If your task is classification, where your output is binary 0, or 1,",
    "start": "3542825",
    "end": "3548000"
  },
  {
    "text": "you choose a distribution that models binary data. Right? So the task in a way influences you to pick the distribution.",
    "start": "3548000",
    "end": "3558680"
  },
  {
    "text": "And, you know, uh, most of the times that choice is pretty obvious. [NOISE] If you want to model the number of visitors to a website which is like a count,",
    "start": "3558680",
    "end": "3566075"
  },
  {
    "text": "you know, you want to use a Poisson distribution, because Poisson distribution is a distribution over integers. So the task deci-,",
    "start": "3566075",
    "end": "3572224"
  },
  {
    "text": "you know, pretty much tells you what distribution you want to choose, and then you- you do the- you know, uh,",
    "start": "3572225",
    "end": "3578090"
  },
  {
    "text": "um, you do this, you know, all- you- you go through this machinery of- of- of figuring out what are the, uh,",
    "start": "3578090",
    "end": "3586175"
  },
  {
    "text": "what h state of X is, and you plug in h state of X over there and you have your learning rule.",
    "start": "3586175",
    "end": "3592740"
  },
  {
    "text": "Any more questions? So, uh, it-,",
    "start": "3593350",
    "end": "3598670"
  },
  {
    "text": "so we made some assumptions. Uh, these assumptions.",
    "start": "3598670",
    "end": "3603860"
  },
  {
    "text": "Now it- it- it's also helpful to kind of get,",
    "start": "3603860",
    "end": "3609395"
  },
  {
    "text": "uh, a visualization of what these assumptions actually mean, right? [NOISE]",
    "start": "3609395",
    "end": "3636680"
  },
  {
    "text": "So to expand upon your point, um, um. You know if you think of the question,",
    "start": "3636680",
    "end": "3642200"
  },
  {
    "text": "\"Are GLMs used for classification, or are they used for regression, or are they used for, you know, um, something else?\"",
    "start": "3642200",
    "end": "3648214"
  },
  {
    "text": "The answer really depends on what is the choice of distribution that you're gonna choose, you know.",
    "start": "3648215",
    "end": "3653315"
  },
  {
    "text": "GLMs are just a general way to model data, and that data could be, you know, um, binary, it could be real value.",
    "start": "3653315",
    "end": "3659359"
  },
  {
    "text": "And- and, uh, as long as you have a distribution that can model, ah, that kind of data,",
    "start": "3659360",
    "end": "3664369"
  },
  {
    "text": "and falls in the exponential family, it can be just plugged into a GLM and everything just, uh, uh, uh works out nicely.",
    "start": "3664370",
    "end": "3671540"
  },
  {
    "text": "Right. So, uh, [NOISE] so the assumptions that we made.",
    "start": "3671540",
    "end": "3680945"
  },
  {
    "text": "Let, uh, let's start with regression, [NOISE] right?",
    "start": "3680945",
    "end": "3686135"
  },
  {
    "text": "So for regression, we assume there is some X. Uh, to simplify I'm, um,",
    "start": "3686135",
    "end": "3692450"
  },
  {
    "text": "I'm drawing X as one dimension but, you know, X could be multi-dimensional.",
    "start": "3692450",
    "end": "3697475"
  },
  {
    "text": "And there exists a theta, right? And theta transpose X would- would be some linear, um,",
    "start": "3697475",
    "end": "3706359"
  },
  {
    "text": "um, some linear, uh, uh, uh, hyperplane.",
    "start": "3706359",
    "end": "3713255"
  },
  {
    "text": "And this, we assume is Eta, right?",
    "start": "3713255",
    "end": "3723869"
  },
  {
    "text": "And in case of regression Eta was also Mu.",
    "start": "3723880",
    "end": "3728730"
  },
  {
    "text": "So Eta was also Mu, right? Um, and then we are assuming that the Y,",
    "start": "3729580",
    "end": "3735695"
  },
  {
    "text": "for any given X, is distributed as a Gaussian with Mu as the mean.",
    "start": "3735695",
    "end": "3741815"
  },
  {
    "text": "So which means, for every X, every possible X, you have the appropriate, uh, um, um, Eta.",
    "start": "3741815",
    "end": "3749795"
  },
  {
    "text": "And with this as the mean, let's- let's think of this as Y. So that is, uh, a Gaussian distribution at",
    "start": "3749795",
    "end": "3757895"
  },
  {
    "text": "every possible- we assume a variance of 1.",
    "start": "3757895",
    "end": "3765140"
  },
  {
    "text": "So this is like, uh, a Gaussian with standard deviation or variance equal to 1, right? So for every possible X,",
    "start": "3765140",
    "end": "3770870"
  },
  {
    "text": "there is a Y given X, um, which is parameterized by- by- by theta transpose X as- as the mean, right?",
    "start": "3770870",
    "end": "3780200"
  },
  {
    "text": "And you assume that your data is generated from this process, right?",
    "start": "3780200",
    "end": "3787339"
  },
  {
    "text": "So what does it mean? It means, um, you're given X,",
    "start": "3787340",
    "end": "3792830"
  },
  {
    "text": "and let's- let's say this is Y. So you would have examples in your training set that- that may look like this, right?",
    "start": "3792830",
    "end": "3804380"
  },
  {
    "text": "The assumption here is that, for every X there is, um, um- let's say for this particular value of X,",
    "start": "3804380",
    "end": "3812869"
  },
  {
    "text": "um, there was a Gaussian distribution that started from the mean over here.",
    "start": "3812870",
    "end": "3817985"
  },
  {
    "text": "And from this Gaussian distribution this value was sampled, right?",
    "start": "3817985",
    "end": "3824060"
  },
  {
    "text": "You're - you're- you're- you're just sampling it from- from the distribution. Now, the, um- this is how your data is generated.",
    "start": "3824060",
    "end": "3831244"
  },
  {
    "text": "Again, this is our assumption, [NOISE] right?",
    "start": "3831245",
    "end": "3836825"
  },
  {
    "text": "Now that- now based on these assumptions what we are doing with the GLM is we start with the data.",
    "start": "3836825",
    "end": "3844220"
  },
  {
    "text": "We don't know anything else. We make an assumption that there is some linear model from which the data was-was- was- was generated in this format.",
    "start": "3844220",
    "end": "3852335"
  },
  {
    "text": "And we want to work backwards, right, to find theta that will give us this line, right?",
    "start": "3852335",
    "end": "3860480"
  },
  {
    "text": "So for different choice of theta we get a different line, right? We assume that, you know,",
    "start": "3860480",
    "end": "3866510"
  },
  {
    "text": "if -if that line represents the- the Mu's, or the means of the Y's for that particular X, uh,",
    "start": "3866510",
    "end": "3872359"
  },
  {
    "text": "from which it's sampled from, we are trying to find a line, [NOISE] ah,",
    "start": "3872360",
    "end": "3879470"
  },
  {
    "text": "which is- which will be like your theta transpose X from which these Y's are most likely to have sampled.",
    "start": "3879470",
    "end": "3885680"
  },
  {
    "text": "That's- that's essentially what's happening when you do maximum likelihood with- with -with the GLM, right? Ah, similarly, um, [NOISE]",
    "start": "3885680",
    "end": "3906910"
  },
  {
    "text": "Similarly for, um, classification, again let's assume there's an x, right?",
    "start": "3906910",
    "end": "3912835"
  },
  {
    "text": "And there are some Theta transpose x, right?",
    "start": "3912835",
    "end": "3917964"
  },
  {
    "text": "And, uh, and this Theta transpose x is equal- is Eta.",
    "start": "3917965",
    "end": "3923995"
  },
  {
    "text": "We assign this to be Eta, right? And this Eta is, uh, from this Eta,",
    "start": "3923995",
    "end": "3930924"
  },
  {
    "text": "we- we run this through the sigmoid function, uh, 1 over 1 plus e to the minus Eta to get Phi, right?",
    "start": "3930925",
    "end": "3941380"
  },
  {
    "text": "So if these are the Etas for each, um, for each Eta we run it through the sigmoid and we get something like this, right?",
    "start": "3941380",
    "end": "3953050"
  },
  {
    "text": "So this tends to, uh, 1. This tends to 0. And, um, when- at this point when Eta is 0,",
    "start": "3953050",
    "end": "3961570"
  },
  {
    "text": "the sigmoid is- is 0.5. This is 0.5, right?",
    "start": "3961570",
    "end": "3968290"
  },
  {
    "text": "And now, um, at each point- at- at- at any given choice of x,",
    "start": "3968290",
    "end": "3975460"
  },
  {
    "text": "we have a probability distribution. In this case, it's- it's a- it's a binary.",
    "start": "3975460",
    "end": "3983335"
  },
  {
    "text": "So let's assume probability of y is the height to the sigmoid line and here it is low.",
    "start": "3983335",
    "end": "3990460"
  },
  {
    "text": "Um, right. Every x we have a different, uh, Bernoulli distribution essentially, um, that's obtained where,",
    "start": "3990460",
    "end": "3996895"
  },
  {
    "text": "you know, the- the probability of y is- is the height to the, uh, uh, sigmoid through the natural parameter.",
    "start": "3996895",
    "end": "4003180"
  },
  {
    "text": "And from this, you have a data generating distribution that would look like this.",
    "start": "4003180",
    "end": "4008415"
  },
  {
    "text": "So x and, uh, you have a few xs in your training set. And for those xs, you calc- you- you figure out what your,",
    "start": "4008415",
    "end": "4017579"
  },
  {
    "text": "you know, y distribution is and sample from it. So let's say- right.",
    "start": "4017580",
    "end": "4031065"
  },
  {
    "text": "And now, um, again our goal is to stop- given- given this data,",
    "start": "4031065",
    "end": "4037005"
  },
  {
    "text": "so- so over here this is the x and this is y. So this is- these are points for which y is 0.",
    "start": "4037005",
    "end": "4042830"
  },
  {
    "text": "These are points for which y is 1. And so given- given this data, we wanna work backwards to find out,",
    "start": "4042830",
    "end": "4049135"
  },
  {
    "text": "uh, what Theta was. What's the Theta that would have resulted in a sigmoid like",
    "start": "4049135",
    "end": "4056660"
  },
  {
    "text": "curve from which these- these y's were most likely to have been sampled?",
    "start": "4056660",
    "end": "4061775"
  },
  {
    "text": "That's- and- and figuring out that y is- is- is essentially doing logistic regression.",
    "start": "4061775",
    "end": "4066944"
  },
  {
    "text": "Any questions?",
    "start": "4066945",
    "end": "4069370"
  },
  {
    "text": "All right. So in the last 10 minutes or so, we will, uh, go over softmax regression.",
    "start": "4077150",
    "end": "4086380"
  },
  {
    "text": "So softmax regression is, um, so in the lecture notes,",
    "start": "4110660",
    "end": "4116190"
  },
  {
    "text": "softmax regression is, uh, explained as, uh, as yet another member of the GLM family.",
    "start": "4116190",
    "end": "4123194"
  },
  {
    "text": "Uh, however, in- in- in today's lecture we'll be taking a non-GLM approach and kind of, um,",
    "start": "4123195",
    "end": "4128924"
  },
  {
    "text": "seeing- and- and see how softmax is- is essentially doing, uh, what's also called as cross entropy minimization.",
    "start": "4128925",
    "end": "4136660"
  },
  {
    "text": "We'll end up with the same- same formulas and equations. You can- you can go through the GLM interpretation in the notes.",
    "start": "4137720",
    "end": "4143835"
  },
  {
    "text": "It's a little messy to kind of do it on the whiteboard. So, um, whereas this has- has- has a nicer, um, um, interpretation.",
    "start": "4143835",
    "end": "4151065"
  },
  {
    "text": "Um, and it's good to kind of get this cross entropy interpretation as well. So, uh, let's assume- so here we are talking about multiclass classification.",
    "start": "4151065",
    "end": "4160574"
  },
  {
    "text": "So let's assume we have three cat- three, uh, classes of data.",
    "start": "4160575",
    "end": "4165690"
  },
  {
    "text": "Let's call them circles, um, squares, and say triangles.",
    "start": "4165690",
    "end": "4177910"
  },
  {
    "text": "Now, uh, if- here and this is x1 and x2.",
    "start": "4180590",
    "end": "4186330"
  },
  {
    "text": "We're just- we're just visualizing your input space and the output space, y is kind of implicit in the shape of this, so, um, um.",
    "start": "4186330",
    "end": "4193920"
  },
  {
    "text": "So, um, in- in, um, in multicl- multiclass classification,",
    "start": "4193920",
    "end": "4200430"
  },
  {
    "text": "our goal is to start from this data and learn a model that can,",
    "start": "4200430",
    "end": "4206385"
  },
  {
    "text": "given a new data point, you know, make a prediction of whether this point is a circle,",
    "start": "4206385",
    "end": "4213030"
  },
  {
    "text": "square or a triangle, right? Uh, you're just looking at three because it's",
    "start": "4213030",
    "end": "4218190"
  },
  {
    "text": "easy to visualize but this can work over thousands of classes. And, um, so what we have is",
    "start": "4218190",
    "end": "4228360"
  },
  {
    "text": "so you have x_is in R_n.",
    "start": "4228360",
    "end": "4234300"
  },
  {
    "text": "All right. So the label y",
    "start": "4234300",
    "end": "4239590"
  },
  {
    "text": "is, uh, is 0, 1_k.",
    "start": "4239590",
    "end": "4245744"
  },
  {
    "text": "So k is the number of classes, right?",
    "start": "4245745",
    "end": "4252370"
  },
  {
    "text": "So the labels y is- is- is a one-hot vector. What would you call it as a one-hot vector?",
    "start": "4257990",
    "end": "4264330"
  },
  {
    "text": "Where it's a vector which indicates which class the,",
    "start": "4264330",
    "end": "4269520"
  },
  {
    "text": "uh, x corresponds to. So each- each element in the vector, uh, corresponds to one of the classes.",
    "start": "4269520",
    "end": "4276375"
  },
  {
    "text": "So this may correspond to the triangle class, circle class, square class or maybe something else.",
    "start": "4276375",
    "end": "4282135"
  },
  {
    "text": "Uh, so the labels are, uh, in this one-hot vector where we have a vector that's filled with 0s",
    "start": "4282135",
    "end": "4290204"
  },
  {
    "text": "except with a 1 in one of the places, right? And- and- and- and the way we're gonna- the way we're gonna,",
    "start": "4290205",
    "end": "4304139"
  },
  {
    "text": "uh, um, think of softmax regression is that each class has its- its own set of parameters.",
    "start": "4304140",
    "end": "4312974"
  },
  {
    "text": "So we have, uh, Theta class, right, in R_n.",
    "start": "4312975",
    "end": "4321735"
  },
  {
    "text": "And there are k such things where class is in here,",
    "start": "4321735",
    "end": "4331590"
  },
  {
    "text": "triangle, circle, square, etc, right? So in logistic regression,",
    "start": "4331590",
    "end": "4337485"
  },
  {
    "text": "we had just one Theta, which would do a binary, you know, yes versus no. Uh, in softmax, we have one such vector of Theta per class, right?",
    "start": "4337485",
    "end": "4349170"
  },
  {
    "text": "So you could also optionally represent them as a matrix. There's an n by k matrix where, you know,",
    "start": "4349170",
    "end": "4356925"
  },
  {
    "text": "you have a Theta class- Theta class, right? Uh, so in softmax, uh, regression, um,",
    "start": "4356925",
    "end": "4364725"
  },
  {
    "text": "it's- it's- it's a generalization of logistic regression where you have, um, a set of parameters per class, right?",
    "start": "4364725",
    "end": "4373409"
  },
  {
    "text": "And we're gonna do something, um,",
    "start": "4373410",
    "end": "4377770"
  },
  {
    "text": "something similar to, uh, so, uh,",
    "start": "4381980",
    "end": "4398610"
  },
  {
    "text": "[NOISE] so corresponding",
    "start": "4398610",
    "end": "4409619"
  },
  {
    "text": "to each- each class, uh, uh, of- of, uh, parameters that exists, um [NOISE]",
    "start": "4409620",
    "end": "4421815"
  },
  {
    "text": "So there's- there exists this line which represents say, Theta triangle transpose x equals 0,",
    "start": "4421815",
    "end": "4428100"
  },
  {
    "text": "and anything to the left, will be Theta triangle transpose x is greater than 0,",
    "start": "4428100",
    "end": "4433425"
  },
  {
    "text": "and over here it'll be less than 0, right? So if, if- for, for- uh, uh, the- Theta triangle class,",
    "start": "4433425",
    "end": "4439050"
  },
  {
    "text": "um, there is- uh, there is this line, um, which- which corresponds to,",
    "start": "4439050",
    "end": "4445065"
  },
  {
    "text": "uh, uh, Theta transpose x equals 0. Anything to the left, uh will give you a value greater than on- zero, anything to the right.",
    "start": "4445065",
    "end": "4452190"
  },
  {
    "text": "Similarly, there is also-. Uh, so this corresponds to Theta,",
    "start": "4452190",
    "end": "4458489"
  },
  {
    "text": "uh, square transpose x equals 0. Anything below will be greater than 0,",
    "start": "4458490",
    "end": "4466080"
  },
  {
    "text": "anything above will be less than 0. Similarly you have another one for,",
    "start": "4466080",
    "end": "4473805"
  },
  {
    "text": "um, this corresponds to Theta circle transpose x equals 0.",
    "start": "4473805",
    "end": "4480720"
  },
  {
    "text": "And, and, and, and this half plane, we have, uh, to be greater than 0,",
    "start": "4480720",
    "end": "4485835"
  },
  {
    "text": "and to the left, it is less than 0, right? So we have, um,",
    "start": "4485835",
    "end": "4490865"
  },
  {
    "text": "a different set of parameters per class which, um, which, which, which hopefully satisfies this property, um,",
    "start": "4490865",
    "end": "4500025"
  },
  {
    "text": "and now, um, our goal is to take these parameters and let's see what happens when,",
    "start": "4500025",
    "end": "4510719"
  },
  {
    "text": "when we field a new example. So given an example x, we get a set of- given x,",
    "start": "4510720",
    "end": "4524699"
  },
  {
    "text": "um, and over here we have classes, right? So we have the circle class,",
    "start": "4524700",
    "end": "4530940"
  },
  {
    "text": "the triangle class, the square class, right? So, um, over here, we plot Theta class transpose x.",
    "start": "4530940",
    "end": "4539685"
  },
  {
    "text": "So we may get something that looks like this.",
    "start": "4539685",
    "end": "4543250"
  },
  {
    "text": "So let's say for a new point x over here,",
    "start": "4545180",
    "end": "4550515"
  },
  {
    "text": "uh, if that's our new x, we would have Theta transpose, um, Theta trans- Theta square transpose x to be positive.",
    "start": "4550515",
    "end": "4560369"
  },
  {
    "text": "So we- all right. And maybe for, um, for the others,",
    "start": "4560370",
    "end": "4566040"
  },
  {
    "text": "we may have some negative and maybe something like this for this, right? So- th- this space is,",
    "start": "4566040",
    "end": "4573800"
  },
  {
    "text": "is also called the logic space, right? So these are real numbers, right? Thi- this will, this will, uh,",
    "start": "4573800",
    "end": "4579965"
  },
  {
    "text": "this is not a value between 0 and 1, this is between plus infinity and minus infinity, right?",
    "start": "4579965",
    "end": "4584989"
  },
  {
    "text": "And, and our goal is to get, uh, a probability distribution over the classes.",
    "start": "4584990",
    "end": "4593025"
  },
  {
    "text": "Uh, and in order to do that, we perform a few steps. So we exponentiate the logics which would give us- so now it is x above Theta class",
    "start": "4593025",
    "end": "4605639"
  },
  {
    "text": "transpose x and this will make everything positive so it should be a small one.",
    "start": "4605640",
    "end": "4617100"
  },
  {
    "text": "Squares, triangles and circles, right? Now we've got a set of positive numbers.",
    "start": "4617100",
    "end": "4623639"
  },
  {
    "text": "And next, we normalize this.",
    "start": "4623640",
    "end": "4626650"
  },
  {
    "text": "By normalize, I mean, um, divide everything by the sum of all of them.",
    "start": "4630560",
    "end": "4637770"
  },
  {
    "text": "So here we have Theta e to the Theta class transpose x over the sum of i in triangle,",
    "start": "4637770",
    "end": "4650340"
  },
  {
    "text": "square, circle, e to the Theta i transpose x.",
    "start": "4650340",
    "end": "4657645"
  },
  {
    "text": "So n- once we do this operation, we now get a probability distribution",
    "start": "4657645",
    "end": "4663340"
  },
  {
    "text": "where the sum of the heights will add up to 1, right? So, uh- so given- so- if, if,",
    "start": "4673040",
    "end": "4681030"
  },
  {
    "text": "if- given a new point x and we run through this pipeline, we get a probability output over the classes for which",
    "start": "4681030",
    "end": "4689250"
  },
  {
    "text": "class that example is most likely to belong to, right?",
    "start": "4689250",
    "end": "4694560"
  },
  {
    "text": "And this whole process, so let's call this p hat of,",
    "start": "4694560",
    "end": "4700474"
  },
  {
    "text": "of, of, of y for the given x, right? So this is like our hypothesis.",
    "start": "4700475",
    "end": "4706170"
  },
  {
    "text": "The output of the hypothesis function will output this probability distribution. In the other cases, the output of the hypothesis function,",
    "start": "4706170",
    "end": "4712620"
  },
  {
    "text": "generally, output a scalar or a probability. In this case, it's outputting a probability di- distribution over all the classes.",
    "start": "4712620",
    "end": "4719565"
  },
  {
    "text": "And now, the true y would look something like this, right?",
    "start": "4719565",
    "end": "4725055"
  },
  {
    "text": "Let's say, the point over there was- le- let's say it was a triangle,",
    "start": "4725055",
    "end": "4730500"
  },
  {
    "text": "for, for whatever reason, right? If that was the triangle, then the p of y which is also called the label,",
    "start": "4730500",
    "end": "4739300"
  },
  {
    "text": "you can think of that as a probability distribution which is 1 over",
    "start": "4739490",
    "end": "4744795"
  },
  {
    "text": "the correct class and 0 elsewhere, right?",
    "start": "4744795",
    "end": "4749820"
  },
  {
    "text": "So p of y. This is essentially representing the one-hot representation as a probability distribution, right?",
    "start": "4749820",
    "end": "4755985"
  },
  {
    "text": "Now the goal or, or, um, the learning approach that we're going to do is in a way minimize",
    "start": "4755985",
    "end": "4763215"
  },
  {
    "text": "the distance between these two distributions, right?",
    "start": "4763215",
    "end": "4768300"
  },
  {
    "text": "This is one distribution, this is another distribution. We want to change this distribution to look like that distribution, right?",
    "start": "4768300",
    "end": "4774930"
  },
  {
    "text": "Uh, and, and, uh, technically, that- the term for that is minimize the cross entropy between the two distributions.",
    "start": "4774930",
    "end": "4783190"
  },
  {
    "text": "So the cross entropy",
    "start": "4795380",
    "end": "4798760"
  },
  {
    "text": "between p and p hat is equal to,",
    "start": "4800840",
    "end": "4807639"
  },
  {
    "text": "for y in circle, triangle, square,",
    "start": "4812060",
    "end": "4819490"
  },
  {
    "text": "p of y times log p hat of",
    "start": "4819500",
    "end": "4825210"
  },
  {
    "text": "y. I don't think we'll have time to go over the interpretation of cross entropy but you can look that up.",
    "start": "4825210",
    "end": "4832695"
  },
  {
    "text": "So here we see that p of y will be one for just one of the classes and zero for the others.",
    "start": "4832695",
    "end": "4837735"
  },
  {
    "text": "So let's say in this, this example, p of- so y was say a triangle. So this will essentially boil down to- there's a little min-",
    "start": "4837735",
    "end": "4846810"
  },
  {
    "text": "minus log p hat of y triangle, right?",
    "start": "4846810",
    "end": "4856080"
  },
  {
    "text": "And what we saw that this- the hypothesis is essentially that expression.",
    "start": "4856080",
    "end": "4862065"
  },
  {
    "text": "So that's equal to minus log x e x of",
    "start": "4862065",
    "end": "4868035"
  },
  {
    "text": "Theta triangle transpose x over sum of class in triangle,",
    "start": "4868035",
    "end": "4875700"
  },
  {
    "text": "square, circle, e to the triangle.",
    "start": "4875700",
    "end": "4883155"
  },
  {
    "text": "Right. And on this, you, you, you, you treat this as the loss and do gradient descent.",
    "start": "4883155",
    "end": "4890860"
  },
  {
    "text": "Gradient descent with respect to the parameters. Right, um, yeah.",
    "start": "4894140",
    "end": "4903165"
  },
  {
    "text": "With, with, with that I think, uh, uh, any, any questions on softmax?",
    "start": "4903165",
    "end": "4908410"
  },
  {
    "text": "Okay. So we'll, we'll break for today in that case. Thanks.",
    "start": "4913280",
    "end": "4918099"
  }
]