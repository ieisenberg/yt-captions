[
  {
    "start": "0",
    "end": "5530"
  },
  {
    "text": "So today I'm going to be\ntalking about some recent work that we've been\ndoing at DeepMind, developing this line\nof architectures",
    "start": "5530",
    "end": "11469"
  },
  {
    "text": "that we're calling perceivers. And I'll be motivating this in\nterms of a goal that we have,",
    "start": "11470",
    "end": "17060"
  },
  {
    "text": "which is to develop a general\npurpose architectures. And so just right\noff the bat, I want",
    "start": "17060",
    "end": "22330"
  },
  {
    "text": "to motivate why we care about\ngeneral purpose architectures. And so the first--",
    "start": "22330",
    "end": "27700"
  },
  {
    "text": "both of the reasons\nare fairly pragmatic. But basically the\nidea is if we're thinking about all of the\ndata that we could possibly",
    "start": "27700",
    "end": "34750"
  },
  {
    "text": "imagine collecting in\nthe world, a lot of it is basically involved\nin what we think of as sort of traditional\nsense modalities.",
    "start": "34750",
    "end": "42560"
  },
  {
    "text": "And these things range from\ntouch, and proprioception, to echolocation, to\nthe kind of perception",
    "start": "42560",
    "end": "47650"
  },
  {
    "text": "you need to ingest texts-- however you want\nto format that-- just to more exotic things\nlike event-based cameras,",
    "start": "47650",
    "end": "56290"
  },
  {
    "text": "touching with your senses--\nthings like smell and depth-- and all the way up to the\nkinds of sense modalities",
    "start": "56290",
    "end": "62710"
  },
  {
    "text": "that we really think\nabout when we're thinking about scientific perception. And so basically if we think\nabout the full set of data",
    "start": "62710",
    "end": "70270"
  },
  {
    "text": "and what it would\ntake to actually model each of these\ndifferent modalities, it basically looks\neffectively intractable to try",
    "start": "70270",
    "end": "77342"
  },
  {
    "text": "to the engineer inductive\nbiases that will work for every single one of these. So we don't want to\nengineer them one by one.",
    "start": "77342",
    "end": "84003"
  },
  {
    "text": "This is an approach\nthat's worked. And in some ways it's maybe\na reasonable description of how we think about\ndeveloping new architectures",
    "start": "84003",
    "end": "90159"
  },
  {
    "text": "for different problems. But it's just not\ngoing to scale. We can't afford as a\ncommunity to hand design",
    "start": "90160",
    "end": "96713"
  },
  {
    "text": "inductive biases that\nwill work for each and every one of these. And so rather than\ndoing that, we want to sort of\nbuild architectures",
    "start": "96713",
    "end": "102039"
  },
  {
    "text": "that at least a first pass will\nallow us to handle everything. There's another\npractical argument",
    "start": "102040",
    "end": "107150"
  },
  {
    "text": "for why we should work on\ngeneral purpose architectures. And that's because it will\nallow us to build simpler",
    "start": "107150",
    "end": "112460"
  },
  {
    "text": "and more unified systems. So if you look at\nhow, in particular, complex multimodal data\nstreams are typically",
    "start": "112460",
    "end": "121310"
  },
  {
    "text": "approached in the sensory\ncomputer vision or pattern",
    "start": "121310",
    "end": "126470"
  },
  {
    "text": "recognition literatures. Effectively, the\ntypical way this is done is by using inductive\nbiases that we now",
    "start": "126470",
    "end": "132230"
  },
  {
    "text": "hold for the\nindividual modalities, and then engineer\nways of combining those different subsystems.",
    "start": "132230",
    "end": "138140"
  },
  {
    "text": "So this can mean building\nspecific heads, specific input modules for each\nof these things,",
    "start": "138140",
    "end": "144290"
  },
  {
    "text": "and then trying out the\nvarious different ways of combining them. So this can work,\nbut it gives us",
    "start": "144290",
    "end": "149690"
  },
  {
    "text": "systems that in principle\nreally will only work on one or a small number of domains. And it gives us systems that\nare very hard to maintain,",
    "start": "149690",
    "end": "156409"
  },
  {
    "text": "tend to be fragile,\ntend to depend on specific processing\nassumptions about the input modalities.",
    "start": "156410",
    "end": "161730"
  },
  {
    "text": "So rather than do\nthat, we sort of want to move in a\ndirection of having unified black-box architectures\nthat kind of just work.",
    "start": "161730",
    "end": "168800"
  },
  {
    "text": "And the idea here is that\nif we can get to that point, we can abstract the architecture\nconstruction process",
    "start": "168800",
    "end": "174470"
  },
  {
    "text": "and really focus on other\nmore high-level problems. So this is sort\nof the motivation for this line of work.",
    "start": "174470",
    "end": "181269"
  },
  {
    "text": "And the way that we're\ngoing to be doing this is of course by working on\nthe most general purpose",
    "start": "181270",
    "end": "186910"
  },
  {
    "text": "architecture that\nwe have so far, which is basically\na transformer. And you'll all be very familiar\nwith the basic building",
    "start": "186910",
    "end": "192580"
  },
  {
    "text": "blocks of a transformer. But just at a very\nhigh level, we can think about\nwhat they do right,",
    "start": "192580",
    "end": "197860"
  },
  {
    "text": "which is they use a general\npurpose inductive bias. So they're non-local,\nwhich means they're not making domain\nspecific assumptions",
    "start": "197860",
    "end": "204362"
  },
  {
    "text": "about which point should\nbe compared to each other, rather they tend to be global in\nterms of the attentional focus",
    "start": "204362",
    "end": "209770"
  },
  {
    "text": "that they have. They use a position as a feature\nrather than a hard constraint",
    "start": "209770",
    "end": "215270"
  },
  {
    "text": "of the architecture. And this is in contrast to\nsort of NLP-based architectures",
    "start": "215270",
    "end": "220280"
  },
  {
    "text": "or ConvNets in the way\nthat they typically work, which used position as an\narchitectural component",
    "start": "220280",
    "end": "226610"
  },
  {
    "text": "to constrain how\ncompute is happening. And then of course\nfinally, there's",
    "start": "226610",
    "end": "232489"
  },
  {
    "text": "extensive weight sharing in the\nway that that they're designed. And because they\nfocus on Matmuls,",
    "start": "232490",
    "end": "238250"
  },
  {
    "text": "they tend to be TPU\nand GPU friendly. So these are all very\nnice things about the way transformers work.",
    "start": "238250",
    "end": "243475"
  },
  {
    "text": "Of course on the\nother hand, they have very poor compute\nmemory scaling. And there are two\ncomponents to this.",
    "start": "243475",
    "end": "249030"
  },
  {
    "text": "So attention itself\nscales quadratically. So there's this O of\nM squared L complexity",
    "start": "249030",
    "end": "257060"
  },
  {
    "text": "at the heart of transformers,\nand I like writing it this way, because it really emphasizes\nthat this is a property of--",
    "start": "257060",
    "end": "263240"
  },
  {
    "text": "basically as you make\nyour models bigger, either at the input size\nor as you make them deeper, this problem is just\ngoing to get worse.",
    "start": "263240",
    "end": "270800"
  },
  {
    "text": "And because you have this\nscaling in depth as well, there's another sort\nof practical thing",
    "start": "270800",
    "end": "275870"
  },
  {
    "text": "that happens here because\nthe amount of compute that we're doing is\nproportional to the input size.",
    "start": "275870",
    "end": "281250"
  },
  {
    "text": "So there's no\nbottleneck in the way that standard transformers work. Even the linear scaling\nbecomes a problem.",
    "start": "281250",
    "end": "288570"
  },
  {
    "text": "And so in practice for very,\nvery large transformers, this can often be the\nbottleneck that really matters.",
    "start": "288570",
    "end": "294240"
  },
  {
    "text": "But they're both at play here. And so we really want to sort\nof tamp down both of these. And so the sort of\nperspective here",
    "start": "294240",
    "end": "300199"
  },
  {
    "text": "is that to have really\ngeneral purpose architectures, we can't have ones that are\njust in principle general. We have to have ones\nthat you can actually",
    "start": "300200",
    "end": "306950"
  },
  {
    "text": "use on the scales and the kinds\nof data that we care about. ",
    "start": "306950",
    "end": "313250"
  },
  {
    "text": "And so just to-- this will all be old\nhat for all of you, but just the way that\nstandard QKV attention",
    "start": "313250",
    "end": "319220"
  },
  {
    "text": "works is basically like this. So it's all about\nmatrix multiplications. So we have some input.",
    "start": "319220",
    "end": "324319"
  },
  {
    "text": "We compute the query\nkeys and values by having a 1d convolution,\na 1 by 1 convolution",
    "start": "324320",
    "end": "329330"
  },
  {
    "text": "that we run over the input. We then compute the\nattention scores.",
    "start": "329330",
    "end": "334800"
  },
  {
    "text": "This is a matrix multiply\nthat has the following, these sorts of shapes. We then use the\noutput here to compute",
    "start": "334800",
    "end": "342470"
  },
  {
    "text": "the weights to compute the\nactual output of the attention module itself. And then finally, we run this\nthrough an additional MLP,",
    "start": "342470",
    "end": "349760"
  },
  {
    "text": "which is applied convolutionally\nto get the outputs. So this is the starting point\nof what we're working on here.",
    "start": "349760",
    "end": "357930"
  },
  {
    "text": "And let me just\nbriefly just reiterate why we would want the\nsort of advantages that we have with these sort\nof standard transformers.",
    "start": "357930",
    "end": "364910"
  },
  {
    "text": "So non-locality is one of the\ntwo inductive bias principles that we have here. It's useful, I think,\nto contrast this",
    "start": "364910",
    "end": "371060"
  },
  {
    "text": "to way that the\neffective locality that you get in convnets and\nwhat this actually means.",
    "start": "371060",
    "end": "376110"
  },
  {
    "text": "So if we look at basically as a\nfunction of depth, which inputs can see which other\nones, which means",
    "start": "376110",
    "end": "382220"
  },
  {
    "text": "like how easily it is to express\na function of two input points. Let's say we look at these\nthis yellow and purple point",
    "start": "382220",
    "end": "388789"
  },
  {
    "text": "here at the input. Now I set them sort of\nas far apart as possible. But we might ask how deep\nwould the effect of computation",
    "start": "388790",
    "end": "396680"
  },
  {
    "text": "have to be before you\nactually process these two? And if you look at a 3 by 3\nconvolution, you're talking--",
    "start": "396680",
    "end": "403667"
  },
  {
    "text": "you're having to look\nbasically until the very end of the network until\nyou're processing",
    "start": "403667",
    "end": "408840"
  },
  {
    "text": "these things together. And what this means\nis that the functions that you can express\nthat actually",
    "start": "408840",
    "end": "415327"
  },
  {
    "text": "look at both of these points\nend up being quite shallow, because they have\nto be built on top of this very, very deep\nstack that just gives you",
    "start": "415327",
    "end": "421232"
  },
  {
    "text": "the locality. And so in point of\nfact, if you look at, for example, the\nway ResNets work,",
    "start": "421232",
    "end": "427110"
  },
  {
    "text": "so you have an initial block\nwhich has a 7 by 7 convolution. And then afterwards it's 3\nby 3 columns all the way up.",
    "start": "427110",
    "end": "432449"
  },
  {
    "text": "You need 28, 3 by 3 convs\nwith that standard processing stack before all of the 224\nby 224 pixels in an image",
    "start": "432450",
    "end": "439560"
  },
  {
    "text": "you're looking at each other. And what this means is\nthat in a ResNet 50, the points on the very\nedge of the pixels",
    "start": "439560",
    "end": "445830"
  },
  {
    "text": "actually never see each other. And it's a little bit-- I found this a little\nbit counterintuitive.",
    "start": "445830",
    "end": "451229"
  },
  {
    "text": "But it suggests that we\nreally are constraining quite a lot, the functions\nthat are easy to express with these models.",
    "start": "451230",
    "end": "456580"
  },
  {
    "text": "And so there are some\nfunctions of images you just can't capture\nwith a ResNet 50.",
    "start": "456580",
    "end": "462699"
  },
  {
    "text": "On the other hand, if you\nlook at an architecture that has global attention\nover the full input, so",
    "start": "462700",
    "end": "468280"
  },
  {
    "text": "the transformers, you\ncould scale it that way or in a perceiver, we're\ngoing to be talking about all",
    "start": "468280",
    "end": "473560"
  },
  {
    "text": "of the pixels can interact. So the model can basically\ncapture these things and express these\nfunctions much more",
    "start": "473560",
    "end": "479440"
  },
  {
    "text": "easily than can be expressed in\nthings that put locality first. ",
    "start": "479440",
    "end": "486229"
  },
  {
    "text": "The other sort of\ninteresting property of these sorts of architectures\nis that position is featurized.",
    "start": "486230",
    "end": "492020"
  },
  {
    "text": "And this basically\nmeans that we're no longer sort of encoding\nthe architectural location of something to figure\nout where it's located",
    "start": "492020",
    "end": "499820"
  },
  {
    "text": "with respect to the other ones. And this allows the\nnetwork to basically use any positional information\nthat it wants but can also",
    "start": "499820",
    "end": "507680"
  },
  {
    "text": "discard it as it prefers. And so this is the\nstandard way it's done, of course, in the\ncontext of architectures",
    "start": "507680",
    "end": "514400"
  },
  {
    "text": "that use Fourier or\nsinusoidal-like features. But there's a lot\nof flexibility here.",
    "start": "514400",
    "end": "520365"
  },
  {
    "text": "OK, so now just\nthinking in terms of how ConvNets relate\nto transformers sort of at the opposite\nend, it may look",
    "start": "520365",
    "end": "526629"
  },
  {
    "text": "like that we have\nsort of a scalability versus generality trade off. And so if we look at ConvNets\nthe way that they're applied,",
    "start": "526630",
    "end": "535459"
  },
  {
    "text": "so typically we can\nthink about using them on grid structure data. There are, of course,\ngeneralizations of convolutions",
    "start": "535460",
    "end": "541750"
  },
  {
    "text": "that work on data sets with\nmore interesting topology. But typically we\ncan think of them",
    "start": "541750",
    "end": "547000"
  },
  {
    "text": "as operating on grids\nin some sort of space. Whereas, transformers\napply to generic sets.",
    "start": "547000",
    "end": "552850"
  },
  {
    "text": "So transformers are more\ngeneral from this point of view. On the other hand, they\nscale much, much worse.",
    "start": "552850",
    "end": "558560"
  },
  {
    "text": "So ConvNets are linear both in\nthe input points, the filter size, and the number of\nlayers of that architecture,",
    "start": "558560",
    "end": "565360"
  },
  {
    "text": "whereas transformers have\nthis quadratic scaling and they're still\nlinear in the depth.",
    "start": "565360",
    "end": "571180"
  },
  {
    "text": "So from this point\nof view, what we're interested in doing in\nthe perceiver line of work was to scale transformers but to\nkeep the generality properties.",
    "start": "571180",
    "end": "579050"
  },
  {
    "text": "So we want something that lives\nin between these two extremes. And the way that we\ndo this is by looking",
    "start": "579050",
    "end": "586389"
  },
  {
    "text": "at self-attention and sort\nof modifying it in a way that allows us to scale better.",
    "start": "586390",
    "end": "591610"
  },
  {
    "text": "So to walk through what\nself-attention actually does in sort of\nstandard transformers, we take our input array, which\nhere is written as the number--",
    "start": "591610",
    "end": "599860"
  },
  {
    "text": "as the indices, which\nis the number of tokens or the number of\npixels, basically the number of input units\ndepending what you're",
    "start": "599860",
    "end": "605650"
  },
  {
    "text": "looking at, and the channels. We have a 1D convolution. So this is O of M\nwith respect to the Q,",
    "start": "605650",
    "end": "612250"
  },
  {
    "text": "K, and V. We then compute\nthe attention maps using the output of this operation.",
    "start": "612250",
    "end": "617290"
  },
  {
    "text": "This gives us a\nmatrix multiply, which is the source of the\nquadratic scaling. And then finally, we\ncompute output features",
    "start": "617290",
    "end": "623800"
  },
  {
    "text": "with another matrix multiply. ",
    "start": "623800",
    "end": "629090"
  },
  {
    "text": "We're already rate limited\nhere, because for even standard resolution\nimages, M is quite large.",
    "start": "629090",
    "end": "634110"
  },
  {
    "text": "So it's around 50,000 for\nstandard ImageNet images, which again are very small. So this is something\nthat just isn't",
    "start": "634110",
    "end": "640040"
  },
  {
    "text": "going to work if we\nwant deep architectures. So what we do is we replace, at\nthe input to the architecture",
    "start": "640040",
    "end": "646790"
  },
  {
    "text": "we replace the self-attention\nwith a cross-attention layer. And we do this using\nbasically a learned query.",
    "start": "646790",
    "end": "653420"
  },
  {
    "text": "And so we're replacing only\nthe query from the input here with a learned component.",
    "start": "653420",
    "end": "658880"
  },
  {
    "text": "And so these indices\nand channels, you can just think\nof these as basically working like the learned\ninitial state for an RNN.",
    "start": "658880",
    "end": "665030"
  },
  {
    "text": "there's a variety of\nnames that this idea goes under in the literature. We refer to them as latents.",
    "start": "665030",
    "end": "673009"
  },
  {
    "text": "But they're sometimes\ncalled inducing points or other things. So the basic idea is we're\nlearning the input to the query",
    "start": "673010",
    "end": "679880"
  },
  {
    "text": "and keeping the key\nvalue input the same. The downside or the\nsort of upside of this is that when we compute the\nattention map after this, now",
    "start": "679880",
    "end": "688660"
  },
  {
    "text": "we basically turned this\nfrom a square matrix to a rectangular matrix--",
    "start": "688660",
    "end": "694060"
  },
  {
    "text": "and reduces the complexity\nof the matrix multiplied to O of MN, so now it's\nlinear in the input size.",
    "start": "694060",
    "end": "700240"
  },
  {
    "text": "And the second matrix multiply\nhas the exact same property. So it from quadratic,\nit becomes linear.",
    "start": "700240",
    "end": "707500"
  },
  {
    "text": "And the quite cool\nthing about this is that, OK, so\nthe cross-attention is linear in complexity, but\nthe output is actually smaller.",
    "start": "707500",
    "end": "714525"
  },
  {
    "text": "And so this, I\nthink, is actually the more important point here. It's that this allows us\nto map something which is quite large into something\nthat has size that's",
    "start": "714525",
    "end": "721510"
  },
  {
    "text": "independent of the input. So we have full control over\nthis as a hyperparameter. And this allows us to build deep\nnetworks on top of this latent.",
    "start": "721510",
    "end": "730160"
  },
  {
    "text": "So because this is of a small\nsize that we can control, we can afford to have quadratic\ncomplexity on top of this.",
    "start": "730160",
    "end": "735740"
  },
  {
    "text": "And so we use this-- yep? I'm sorry. I'm still a little\nbit confused as to how",
    "start": "735740",
    "end": "741250"
  },
  {
    "text": "you guys are able to turn\nthis square into a rectangle in the second step. Is it because you\nreplaced the query",
    "start": "741250",
    "end": "748270"
  },
  {
    "text": "with the learned something\nthat is significantly smaller compared to the\ninput size in the first step?",
    "start": "748270",
    "end": "753910"
  },
  {
    "text": "Yeah, that's exactly right. So if you look at the-- so the underlying\nmatrix multiply here,",
    "start": "753910",
    "end": "760250"
  },
  {
    "text": "which is written as\nthe QK transpose, so the outer dimension\nhere has shape N, which",
    "start": "760250",
    "end": "767319"
  },
  {
    "text": "is determined by the query. And so by shrinking\nthat query, we're just changing the output\nof the matrix multiply.",
    "start": "767320",
    "end": "773699"
  },
  {
    "text": "OK. Thank you. Yeah, so I guess-- sorry, go ahead please.",
    "start": "773700",
    "end": "779820"
  },
  {
    "text": "OK, cool. So basically you only do\nthat for the query, right? So key and value remain\nthe original size",
    "start": "779820",
    "end": "786900"
  },
  {
    "text": "matrices, correct? That's right, yeah. OK. But so basically-- so I\ndon't know what I'm not",
    "start": "786900",
    "end": "796110"
  },
  {
    "text": "understanding basically. So the problem for me is that\nfor a query, now in my head",
    "start": "796110",
    "end": "801579"
  },
  {
    "text": "I'm looking for let's\nsay, I have the ith token, now there is no\nith query anymore.",
    "start": "801580",
    "end": "808750"
  },
  {
    "text": "Doesn't that cause a problem\nwhen I'm trying to use it and to compute scores?",
    "start": "808750",
    "end": "815980"
  },
  {
    "text": "Yeah. So what's happening\nhere is you're comparing-- you'll have a\nsmaller subset of query. So if you think about this\nand not in terms of the matrix",
    "start": "815980",
    "end": "822579"
  },
  {
    "text": "multiplies but in\nterms of comparing each query to each key, so\nin normal self-attention",
    "start": "822580",
    "end": "828250"
  },
  {
    "text": "we have one query for\neach key, so every point compares to every other point. Right? So here what we've done\nis instead of comparing",
    "start": "828250",
    "end": "834190"
  },
  {
    "text": "every point to\nevery other point, we have a set of sort\nof cluster centers-- you may be able to\nthink about them as.",
    "start": "834190",
    "end": "839410"
  },
  {
    "text": "So that's a smaller number. And we compare each of those\nto each of the input points.",
    "start": "839410",
    "end": "844480"
  },
  {
    "text": "But we don't know which\ntokens technically belong to which clusters, right?",
    "start": "844480",
    "end": "850837"
  },
  {
    "text": "That's right. So it has to be learned. Through Matmuls. Yeah, exactly. So one way to think about this,\nabout what's happening here",
    "start": "850838",
    "end": "858790"
  },
  {
    "text": "is that instead of-- so\nin a normal self-attention",
    "start": "858790",
    "end": "864339"
  },
  {
    "text": "transformer, by comparing all\nto all we're sort of saying, OK, I know what the\nfeature is at this point",
    "start": "864340",
    "end": "869530"
  },
  {
    "text": "and I want it to attend\nto similar features. Here what we're saying\nis we're learning",
    "start": "869530",
    "end": "874840"
  },
  {
    "text": "a bunch of supplementary\npoints that should be sort of\nmaximally similar",
    "start": "874840",
    "end": "880029"
  },
  {
    "text": "to some subset of the inputs,. So correct me if I'm wrong\nbut this is essentially",
    "start": "880030",
    "end": "885290"
  },
  {
    "text": "using a hard engine, right? We're using a-- it's about\nlike declaring all the points. It's like some points, which\nwe think are very similar,",
    "start": "885290",
    "end": "893240"
  },
  {
    "text": "and only were self-attention\nwas so hardened like this points you're sharing, right?",
    "start": "893240",
    "end": "898800"
  },
  {
    "text": "Yeah. So they're related. That would be one way\nto think about it. The slight modifier\nto that idea,",
    "start": "898800",
    "end": "906240"
  },
  {
    "text": "though, is that they basically\nlive in an abstract space. So they're not\nassigned sort of one to one, to one of\nthe input queries",
    "start": "906240",
    "end": "913190"
  },
  {
    "text": "or to one of the input points. They're sort of learned, so they\ncan be somewhere in the middle. But I think that's a good\nway to think about it,",
    "start": "913190",
    "end": "919690"
  },
  {
    "text": "that's a good intuition. But I guess one of the places\nwhere I'm a little confused here is you have here indices\nand indices for the twos,",
    "start": "919690",
    "end": "926850"
  },
  {
    "text": "likessssssssssss\nsssssssssssssssssssssssssssssss the purple and green\nmatrices of the far left. But those indices\nare not necessarily",
    "start": "926850",
    "end": "932899"
  },
  {
    "text": "corresponding to inputs. Like in the NLP space,\nthose would not necessarily be tokens, right? These are just sort of--",
    "start": "932900",
    "end": "938660"
  },
  {
    "text": "Exactly. But the matrix in this\ncase is the result of some kind of mapping from\nthe input tokens to an N",
    "start": "938660",
    "end": "946410"
  },
  {
    "text": "by D matrix, is that right? No, it's actually--\nso it basically acts like it's a\nlearned set of weights.",
    "start": "946410",
    "end": "952070"
  },
  {
    "text": "It's one way to think about it. So they function exactly\nthe same way that learned position encodings do. So it's basically just\na learned embedding.",
    "start": "952070",
    "end": "959390"
  },
  {
    "text": "But it's not\nconditioned on anything. ",
    "start": "959390",
    "end": "965150"
  },
  {
    "text": "It's just sort of\na set of weights. Oh, OK. That makes more sense. Thank you. Mm-hmm. ",
    "start": "965150",
    "end": "974160"
  },
  {
    "text": "OK, so if there are\nno more questions, I'm going to keep going. But of course, feel\nfree to interrupt me.",
    "start": "974160",
    "end": "980350"
  },
  {
    "text": "So the way that given\nthis idea, so we have this learned latent\narray, which again it",
    "start": "980350",
    "end": "985988"
  },
  {
    "text": "functions sort of like an\nRNN in an initial state or it's a set of weights. We basically randomly\ninitialize that.",
    "start": "985988",
    "end": "992530"
  },
  {
    "text": "And then we use this to attend\nonto the input byte array. And so the byte array here is\nthe flattened set of pixels",
    "start": "992530",
    "end": "999160"
  },
  {
    "text": "for example for ImageNet. And the output of this is\ngoing to live in the same space as-- so the same index space\nas the latent array does.",
    "start": "999160",
    "end": "1007860"
  },
  {
    "text": "And there's residual\nconnections in the way that you would normally do in\nan attention layer as well.",
    "start": "1007860",
    "end": "1015959"
  },
  {
    "text": "So once we're in the space, we\ncan then build an architecture by using a standard transformer\nbut phrased in the latent space",
    "start": "1015960",
    "end": "1023930"
  },
  {
    "text": "rather than in the input space. And this is going to allow\nus to basically end up,",
    "start": "1023930",
    "end": "1029307"
  },
  {
    "text": "because we sort of\ndistilled the input down to the smaller space. We can still flexibly allow all\nof these points to interact.",
    "start": "1029307",
    "end": "1035189"
  },
  {
    "text": "So this should be still\nas nearly as expressive as a normal transformer is.",
    "start": "1035190",
    "end": "1040748"
  },
  {
    "text": "And then each of\nthe modules here now is quadratic in the latent size\nrather than the input size. So this is something that\nwe can control quite a lot.",
    "start": "1040748",
    "end": "1049880"
  },
  {
    "text": "So in the original\nversion of the procedure, we found it was very helpful to\nhave additional cross attends.",
    "start": "1049880",
    "end": "1056890"
  },
  {
    "text": "So this is certainly\nsomething that you could do. And the reason-- the\nintuition behind this",
    "start": "1056890",
    "end": "1062270"
  },
  {
    "text": "is that this bottleneck\nis quite severe. We can't maintain all of the\ninformation from the input,",
    "start": "1062270",
    "end": "1068750"
  },
  {
    "text": "and so we want\nthese queries, which are now sort of\nconditioned on the past to be able to look back\nat the input point.",
    "start": "1068750",
    "end": "1074850"
  },
  {
    "text": "And so this is something that we\nfound to be quite helpful when tuning for the first paper.",
    "start": "1074850",
    "end": "1080570"
  },
  {
    "text": "But the caveat, I will say,\nis that we're no longer recommending this\nas best practice, because these cross attentions\nend up being quite heavy.",
    "start": "1080570",
    "end": "1087133"
  },
  {
    "text": "But this is something that\nyou can explore certainly if you want sort of more\nconditional queries, or if you want to be able to\ncross attend to new inputs that",
    "start": "1087133",
    "end": "1093680"
  },
  {
    "text": "are coming in. The other thing that\nwe found quite helpful in the context of data sets that\nhave a limited amount of data,",
    "start": "1093680",
    "end": "1101809"
  },
  {
    "text": "which for these architectures\nincludes ImageNet, is to allow weight\nsharing in depth.",
    "start": "1101810",
    "end": "1106909"
  },
  {
    "text": "And so this basically\njust amounts to tying the weights\nfor the different cross attention and different\nself-attention layers",
    "start": "1106910",
    "end": "1112730"
  },
  {
    "text": "as they're repeated. So this ends up looking like an\nRNN that's unrolled in depth. ",
    "start": "1112730",
    "end": "1120919"
  },
  {
    "text": "At a high level, this\ngives us an architecture that we can apply to images. But it doesn't make\nany assumptions about image structure.",
    "start": "1120920",
    "end": "1126679"
  },
  {
    "text": "So it's one that you\ncan use elsewhere. And we basically, we give\ninformation about the input",
    "start": "1126680",
    "end": "1134700"
  },
  {
    "text": "sort of spatial structure by\nhaving positional coatings and here we use a 2D Fourier\nfeature position encoding.",
    "start": "1134700",
    "end": "1141110"
  },
  {
    "text": "And just to show you what that\nlooks like here to give you a sense, so each of the\ninput points is assigned.",
    "start": "1141110",
    "end": "1148100"
  },
  {
    "text": "Basically, so you'll\nbe some position here. And we have sinusoidal and\ncosinusoidal features in 2D.",
    "start": "1148100",
    "end": "1154490"
  },
  {
    "text": "So this is basically a\nFourier decomposition of the position of the 2D input.",
    "start": "1154490",
    "end": "1159710"
  },
  {
    "text": "And a couple of things that we\nfound with that, if we sampled the frequency, that's the\nmaximum frequency that's",
    "start": "1159710",
    "end": "1165680"
  },
  {
    "text": "used up to the Nyquist\nfrequency of the signal, we end up doing\nbetter than if you",
    "start": "1165680",
    "end": "1171049"
  },
  {
    "text": "use a lower version of this. And this basically\nis because this will allow every\nother point to be aware of every distinct\npoint in the image.",
    "start": "1171050",
    "end": "1178857"
  },
  {
    "text": "Whereas if you sample\nat a lower frequency, you're going to end\nup with aliasing. And so not all points\nwill be sort of legible.",
    "start": "1178858",
    "end": "1185390"
  },
  {
    "text": "We also found that sampling\nthe spectrum relatively densely tends to help and\nthe contrast here",
    "start": "1185390",
    "end": "1190940"
  },
  {
    "text": "at the time we were developing\nwas with respect to NeRF. So NeRF, at least in\nearlier implementations,",
    "start": "1190940",
    "end": "1196250"
  },
  {
    "text": "used quite a small number\nof frequency bands. We found that the more we\nadded the better we did.",
    "start": "1196250",
    "end": "1201740"
  },
  {
    "text": "So in general, this is\nsomething to be attentive to. An then, finally, as opposed\nto language where you typically",
    "start": "1201740",
    "end": "1209090"
  },
  {
    "text": "have addition of\nwhatever you're embedding is with the sinusoidal or\nposition encoding that you use.",
    "start": "1209090",
    "end": "1215269"
  },
  {
    "text": "Here we found that\nconcatenating them performed consistently better.",
    "start": "1215270",
    "end": "1220318"
  },
  {
    "text": "And so this may be\nbecause the positioning, the content embedding is not\nas sparse as it is in language.",
    "start": "1220318",
    "end": "1225382"
  },
  {
    "text": "We're not totally sure. But this is something that\nI observed consistently. And before I move on to results.",
    "start": "1225382",
    "end": "1231450"
  },
  {
    "text": "I just want to contrast this\nto some other approaches for using transformers\nin the image context.",
    "start": "1231450",
    "end": "1238520"
  },
  {
    "text": "So the obvious precedent\nhere is visual transformers. And I think this line of\nwork is great, especially",
    "start": "1238520",
    "end": "1247130"
  },
  {
    "text": "in the image context, but\nthere are some caveats about it that make it less suitable for\nsort of more general purpose",
    "start": "1247130",
    "end": "1253400"
  },
  {
    "text": "use. So one is that-- so\nvision transformers do use an input 2D convolution.",
    "start": "1253400",
    "end": "1258800"
  },
  {
    "text": "So this is often\nphrased in terms of patches, input patches,\nthe special case of a 2D",
    "start": "1258800",
    "end": "1264440"
  },
  {
    "text": "transformer. So it does restrict the class\nof inputs you can use it for.",
    "start": "1264440",
    "end": "1270380"
  },
  {
    "text": "And because we're basically\nbuilding this patching or convolution\ninto it, this means",
    "start": "1270380",
    "end": "1276260"
  },
  {
    "text": "that this has an approach\nreally isn't sufficient to get it to work on non-grid data. There are other ways\nyou could adapt it,",
    "start": "1276260",
    "end": "1281660"
  },
  {
    "text": "but this is something\nthat you will have to special case for every\ndomain you're looking at. And then finally, because we\nhave this input where we're",
    "start": "1281660",
    "end": "1289732"
  },
  {
    "text": "telling the architecture\nwhat it should look at first in the initial grouping,\nthis does amount to getting rid of the\nnon-locality assumption.",
    "start": "1289732",
    "end": "1296410"
  },
  {
    "text": "It's not super clear doing\nmuch doing this just once will make a difference. But this is something to be\naware of when you're thinking",
    "start": "1296410",
    "end": "1302905"
  },
  {
    "text": "about this architecture. And then finally cross-attention\nitself is used quite broadly",
    "start": "1302905",
    "end": "1308600"
  },
  {
    "text": "in the vision literature. So to highlight a couple\nof examples, DETR,",
    "start": "1308600",
    "end": "1314540"
  },
  {
    "text": "which is an object detection\nmethod from Facebook, basically, has a\nconvolutional backbone",
    "start": "1314540",
    "end": "1320450"
  },
  {
    "text": "that's then used to give\nan output feature map. This is then passed into a\ntransformer encoder-decoder.",
    "start": "1320450",
    "end": "1325788"
  },
  {
    "text": "And of course, whenever\nyou hear encoder-decoder, you can cross-attention because\nfrom the encoder to the decoder",
    "start": "1325788",
    "end": "1330890"
  },
  {
    "text": "there's a cross-attention step. And so they're using\nbasically the cross-attention to go from some feature\nmap representation",
    "start": "1330890",
    "end": "1336560"
  },
  {
    "text": "to something that looks more\nlike the object bounding boxes. There's also quite\nnice work on learning",
    "start": "1336560",
    "end": "1345290"
  },
  {
    "text": "self-supervised or unsupervised\nobject segmentation models. And in this work, they're\ndoing something very similar,",
    "start": "1345290",
    "end": "1352432"
  },
  {
    "text": "where they have a\nconvolutional backbone. They then use something like\nthese latent the latency",
    "start": "1352432",
    "end": "1358160"
  },
  {
    "text": "that we introduce here to do to\nwhat they call them slots here. But basically, to assign\nsome of the output",
    "start": "1358160",
    "end": "1365060"
  },
  {
    "text": "pixels to different slots,\nso that they sort of have independent complementary\ndecoding of the slots",
    "start": "1365060",
    "end": "1371120"
  },
  {
    "text": "in the segmentation model here.  And there's a lot\nof other things, OK.",
    "start": "1371120",
    "end": "1378820"
  },
  {
    "text": "So now I'm going to walk you\nthrough results of this model.",
    "start": "1378820",
    "end": "1383919"
  },
  {
    "text": "Hi, can I-- oh, go ahead. I'll go after you. Go for it.",
    "start": "1383920",
    "end": "1388929"
  },
  {
    "text": "OK, cool, sorry for that. Can you go back a\ncouple of slides, where you had how the inputs\nflow into I think one--",
    "start": "1388930",
    "end": "1400900"
  },
  {
    "text": "yeah, that one. OK, so I have two questions. The latent transformer\nis basically like a self-attention.",
    "start": "1400900",
    "end": "1406960"
  },
  {
    "text": "Is that correct? Yeah, so the latent\ntransformer is a fully self-attentional transformer.",
    "start": "1406960",
    "end": "1414090"
  },
  {
    "text": "Got it. Why-- see, for\nthe key and value,",
    "start": "1414090",
    "end": "1419279"
  },
  {
    "text": "they flow directly into\nyour cross-attention. And there is the query\nalso flowing into it. But the latent array is flowing\ninto the cross-attention",
    "start": "1419280",
    "end": "1428070"
  },
  {
    "text": "in parallel to the query. Can you explain that? Yeah, so this is just--",
    "start": "1428070",
    "end": "1433200"
  },
  {
    "text": "[INTERPOSING VOICES]  Yeah, it's meant to depict\nthe residual connection. So the cross-attention--\nthis is sort",
    "start": "1433200",
    "end": "1441060"
  },
  {
    "text": "of a cross-attention depicted\nas a cross-attention module. And so the cross-attention\nitself has the attention,",
    "start": "1441060",
    "end": "1446490"
  },
  {
    "text": "it has a residual connection,\nand then there's an MLP. So that's what that's\nmeant to indicate. OK.",
    "start": "1446490",
    "end": "1451920"
  },
  {
    "text": "But it's basically--\nthe QKV is standard. Got it. Thanks.",
    "start": "1451920",
    "end": "1458890"
  },
  {
    "text": "Hi, I had a question that\nis slightly related to this. So you can just stay on\nthis slide, actually.",
    "start": "1458890",
    "end": "1464050"
  },
  {
    "text": "So I think one thing that's\ninteresting about this-- ",
    "start": "1464050",
    "end": "1470050"
  },
  {
    "text": "I lost you. You're cutting off. It's mostly consisting\nof attention layers,",
    "start": "1470050",
    "end": "1477620"
  },
  {
    "text": "whether it's attention\nor image transformers. Can you hear me? Is that coming through? No, it's cutting off.",
    "start": "1477620",
    "end": "1483130"
  },
  {
    "text": "I think-- But I think some-- Oh, OK. Should I type it?",
    "start": "1483130",
    "end": "1488710"
  },
  {
    "text": "Is that-- should I type it? Yeah, that's a good idea. I'll type it. Thanks, sorry.",
    "start": "1488710",
    "end": "1494270"
  },
  {
    "text": "No worries.  Feel free to go ahead.",
    "start": "1494270",
    "end": "1499620"
  },
  {
    "text": "I'll type it slowly. Sounds good. Sounds good to me. Actually, can I chime in?",
    "start": "1499620",
    "end": "1505440"
  },
  {
    "text": "Drew, while you're on that\nprevious slide on the flow, so these residual\nconnections, I actually",
    "start": "1505440",
    "end": "1511088"
  },
  {
    "text": "didn't know that\ncross-attention used them. How reliant are these sequential\ncross-attention layers on the residual connections?",
    "start": "1511088",
    "end": "1519250"
  },
  {
    "text": "Yeah, so here, in the initial--",
    "start": "1519250",
    "end": "1525320"
  },
  {
    "text": "so the two things\nI will say is that in the initial cross-attention,\nit doesn't really",
    "start": "1525320",
    "end": "1531140"
  },
  {
    "text": "make a difference, if it's\nsomething we've ablated. When we get to the Perceiver\nIO version of this,",
    "start": "1531140",
    "end": "1536539"
  },
  {
    "text": "we also did the same thing in\nthe decoder cross-attention. And it can make some of-- it can make a difference there,\ndepending on what you're doing.",
    "start": "1536540",
    "end": "1543125"
  },
  {
    "text": "I think it's actually\nessential in the content-- when you're using repeated\ncross-attention this way, so when you have this sort\nof iterative structure.",
    "start": "1543125",
    "end": "1549510"
  },
  {
    "text": "And the reason for\nthis is that the thing that's actually used\nto condition the query is basically all--",
    "start": "1549510",
    "end": "1555799"
  },
  {
    "text": "that's your full representation\nof this sort of the state of the architecture so far. And so the skip\nconnection is from--",
    "start": "1555800",
    "end": "1562559"
  },
  {
    "text": "it's in, basically\nthe query channel. It's in the latent space. And so this is\nbasically what allows",
    "start": "1562560",
    "end": "1567800"
  },
  {
    "text": "you to end up with this sort of\ndense and stable architecture.",
    "start": "1567800",
    "end": "1573020"
  },
  {
    "text": "Thank you. Mm-hmm. ",
    "start": "1573020",
    "end": "1581650"
  },
  {
    "text": "So to ImageNet. OK, so in standard\nImageNet processing, basically, we compare\nagainst a few--",
    "start": "1581650",
    "end": "1589677"
  },
  {
    "text": "so this is a little bit\nout of date at this point, but against a few just sort of\nsanity check baselines here.",
    "start": "1589677",
    "end": "1595900"
  },
  {
    "text": "So comparing against ResNet50,\nand then at the time, the best vision\ntransformer model that was purely on ImageNet.",
    "start": "1595900",
    "end": "1601870"
  },
  {
    "text": "And we're definitely\nin the ballpark. This isn't-- these aren't\nanywhere near state of the art results.",
    "start": "1601870",
    "end": "1607299"
  },
  {
    "text": "But this is an architecture\nthat, again, it's not using any 2D convolutions. And so the fact that it was\nable to do this well was,",
    "start": "1607300",
    "end": "1613618"
  },
  {
    "text": "we found, very, very\nsurprising at the time. One of the quite cool\nthings about this is that because this\narchitecture is not",
    "start": "1613618",
    "end": "1620169"
  },
  {
    "text": "making any assumptions,\nthe architecture itself isn't making any assumptions\nabout the spatial structure",
    "start": "1620170",
    "end": "1626230"
  },
  {
    "text": "of the input images. We can look at\npermuted ImageNet. And in the first version of\nthis, what we do is basically",
    "start": "1626230",
    "end": "1633490"
  },
  {
    "text": "we compute the features\nusing the 2D position. So the 2D position is sort\nof fixed to a position--",
    "start": "1633490",
    "end": "1638560"
  },
  {
    "text": "to the pixel. And then we just shuffle it all. And so this is\nbasically-- will give you a sense of how dependent\nthe baselines are",
    "start": "1638560",
    "end": "1645460"
  },
  {
    "text": "on the input image structure. And so if we look at the\ntransformer perceiver,",
    "start": "1645460",
    "end": "1651250"
  },
  {
    "text": "by construction,\nthey don't change. So this is not an\nempirical finding. This is a property\nof the models.",
    "start": "1651250",
    "end": "1656649"
  },
  {
    "text": "But we find that\nResNet50 falls by-- the performance\nfalls by about half. And ViT, which again,\nonly has one layer where",
    "start": "1656650",
    "end": "1664270"
  },
  {
    "text": "it's relying on the\nspatial structure also has about a 15 point drop. And so this suggests\nthat it's relying",
    "start": "1664270",
    "end": "1669670"
  },
  {
    "text": "quite a lot on\nthat very first one to give it some information\nabout the structure.",
    "start": "1669670",
    "end": "1675400"
  },
  {
    "text": "We can push this\na little bit by, instead of relying on\n2D Fourier features, learning completely learned\npositional encodings.",
    "start": "1675400",
    "end": "1683400"
  },
  {
    "text": "And this basically-- this\nis an architecture now-- this is a model that has\nabsolutely no information about the input structure.",
    "start": "1683400",
    "end": "1690010"
  },
  {
    "text": "And so shuffling them\nand learning them again is absolutely equivalent. And we find that this\narchitecture also",
    "start": "1690010",
    "end": "1695080"
  },
  {
    "text": "can be pushed above 70%. And we've gotten slightly\nbetter numbers here. In general, this\nseems to work worse.",
    "start": "1695080",
    "end": "1701710"
  },
  {
    "text": "So the 2D information is useful. But it's quite cool\nthat you can get what would have been\nnumbers comparable to state",
    "start": "1701710",
    "end": "1709150"
  },
  {
    "text": "of the art about five\nor six years ago. So this is quite cool. Sorry, I'm a little thick here.",
    "start": "1709150",
    "end": "1715570"
  },
  {
    "text": "You're saying the difference\nbetween the last two rows is that the second-to-last row\nhas a two-dimensional position",
    "start": "1715570",
    "end": "1722992"
  },
  {
    "text": "embedding, and the last one\nhas a one-dimensional position embedding, essentially,\nis that right? So it's learned.",
    "start": "1722993",
    "end": "1728740"
  },
  {
    "text": "So it's basically. It'll be-- it's a, I believe,\na 256 dimensional vector",
    "start": "1728740",
    "end": "1734890"
  },
  {
    "text": "that's learned. But it doesn't-- it basically-- it means that the\nmodel itself has",
    "start": "1734890",
    "end": "1740470"
  },
  {
    "text": "no information about the\ninput spatial structure. So the 2D positional\nencodings that we're",
    "start": "1740470",
    "end": "1746920"
  },
  {
    "text": "using end up having about 200-- it's 200 some\nfeatures, depending on what you're looking at. But they're always-- they give\nyou very detailed information",
    "start": "1746920",
    "end": "1754270"
  },
  {
    "text": "about the 2D structure\nof the input, because they're based on\na Fourier decomposition of the input space.",
    "start": "1754270",
    "end": "1759760"
  },
  {
    "text": "OK, that makes sense. Thank you. Hi, Drew. Can I ask a question\nabout frequency",
    "start": "1759760",
    "end": "1765985"
  },
  {
    "text": "that you used to generate those\n[INAUDIBLE] a couple of slides",
    "start": "1765985",
    "end": "1772429"
  },
  {
    "text": "before. ",
    "start": "1772430",
    "end": "1778890"
  },
  {
    "text": "Yeah, this slide. So basically, I have taken some\nlectures in signal processing.",
    "start": "1778890",
    "end": "1786810"
  },
  {
    "text": "And I know if I want\nto avoid aliasing, I need to sample with at\nleast Nyquist frequency.",
    "start": "1786810",
    "end": "1794350"
  },
  {
    "text": "So I'm curious to know why do\nyou use frequencies starting from one to the\nNyquist frequency",
    "start": "1794350",
    "end": "1801060"
  },
  {
    "text": "instead of starting\nfrom Nyquist frequency to some very high frequency?",
    "start": "1801060",
    "end": "1806810"
  },
  {
    "text": "I see, so basically, so the\nmaximum frequency that's used",
    "start": "1806810",
    "end": "1812390"
  },
  {
    "text": "is always Nyquist. So anything above Nyquist\nis going to be aliased. So you're not actually going\nto be able to resolve it.",
    "start": "1812390",
    "end": "1818960"
  },
  {
    "text": "Because it's in\npixel space, right? So we sample-- 1 is basically just\ngiving you an oscillation",
    "start": "1818960",
    "end": "1824420"
  },
  {
    "text": "that covers the entire image. And so this is basically just\nto sample of the full range",
    "start": "1824420",
    "end": "1829580"
  },
  {
    "text": "of non-aliased frequencies. Oh, OK, cool, thank you, got it. ",
    "start": "1829580",
    "end": "1842950"
  },
  {
    "text": "OK, so after the\nImageNet results, we wanted to try it\non other domains.",
    "start": "1842950",
    "end": "1848287"
  },
  {
    "text": "And in particular, we\nwere interested in how this could be used to work on\nsort of multimodal domains, so ones combining\nvarious different types",
    "start": "1848287",
    "end": "1855700"
  },
  {
    "text": "of input features. And one challenge or\none sort of problem that you encounter in\nthese sorts of spaces",
    "start": "1855700",
    "end": "1862330"
  },
  {
    "text": "is that the data from\ndifferent modalities end up having\ndifferent features. And they always have\ndifferent semantics.",
    "start": "1862330",
    "end": "1868430"
  },
  {
    "text": "So if you take the\npositional encoding plus the RGB for\nvideo, you end up with some number of channels.",
    "start": "1868430",
    "end": "1874299"
  },
  {
    "text": "And then if you have\naudio, that corresponds-- the data may be paired, but it\ntends to have fewer features. And it only has a 1D\npositional encoding.",
    "start": "1874300",
    "end": "1881688"
  },
  {
    "text": "So the way that we\nhandle this is basically by learning modality\nspecific position encodings.",
    "start": "1881688",
    "end": "1887720"
  },
  {
    "text": "And so these are\nbasically embeddings that are special and learned\nfor each of the modalities.",
    "start": "1887720",
    "end": "1892810"
  },
  {
    "text": "And what this does is\nbasically tags-- ends up tagging the features that\ncome from audio or video",
    "start": "1892810",
    "end": "1898180"
  },
  {
    "text": "with some information\nthat the network can learn that allows it to\ndistinguish which one's which. But given these padded, these\nsort of learned, padded feature",
    "start": "1898180",
    "end": "1907060"
  },
  {
    "text": "vectors, we then\nconcatenate them all. And that's how we\nprocess multimodal data. So basically, the input\nto the architecture",
    "start": "1907060",
    "end": "1913120"
  },
  {
    "text": "still looks like\njust one big array. It's just that when\nconstructing this, we know that some\nof those features, some of the rows in that\narray, come from video",
    "start": "1913120",
    "end": "1920050"
  },
  {
    "text": "and some come from audio. But the model itself isn't\ngiven information about that, other than what it learns.",
    "start": "1920050",
    "end": "1925490"
  },
  {
    "text": "We also have some questions. [INAUDIBLE] can go first. ",
    "start": "1925490",
    "end": "1933130"
  },
  {
    "text": "You're muted. Yeah, sorry. I thought I was muted now.",
    "start": "1933130",
    "end": "1938450"
  },
  {
    "text": "Yeah, sure. If you can you hear me,\nthis is a super easy one.",
    "start": "1938450",
    "end": "1943631"
  },
  {
    "text": "I haven't been studying a lot\ntransformer stuff normally, so I just didn't know what\npositional embedding was.",
    "start": "1943631",
    "end": "1950870"
  },
  {
    "text": "Oh, so what is\npositional embedding? Yes, so basically a\npositional embedding is--",
    "start": "1950870",
    "end": "1957770"
  },
  {
    "text": "it's a feature that says this-- so the simplest way to\nthink about it is in text.",
    "start": "1957770",
    "end": "1962810"
  },
  {
    "text": "So text, the input is 1D. So things live in\nsome 1D sequence. And for each point there,\nyou featurize where",
    "start": "1962810",
    "end": "1969169"
  },
  {
    "text": "it's located in that sequence. So the simplest\nthing to do would be if you have negative\n1 to 1 is the full range,",
    "start": "1969170",
    "end": "1974910"
  },
  {
    "text": "it just denotes actually where\nit's located in that sequence. But we typically\nwill add sort of--",
    "start": "1974910",
    "end": "1980690"
  },
  {
    "text": "we'll want to\nfeaturize allows this to have more dimensions\nthan just a single one. And so the Fourier transpose--\nthe Fourier decomposition",
    "start": "1980690",
    "end": "1987799"
  },
  {
    "text": "is one way to do this\nto sort of give it privileged information about\nthe high frequency structure. But we can also just use\nthe position to index",
    "start": "1987800",
    "end": "1996080"
  },
  {
    "text": "on to some embedding array,\nwhich is how we do it when we're learning things. So basically, it's\njust a set of weights",
    "start": "1996080",
    "end": "2002110"
  },
  {
    "text": "that are added to the\nfeature for that point that give the network information\nabout where it's located in the ground truth sequence.",
    "start": "2002110",
    "end": "2007488"
  },
  {
    "text": " OK, you want to go next?",
    "start": "2007488",
    "end": "2013000"
  },
  {
    "start": "2013000",
    "end": "2018290"
  },
  {
    "text": "Sorry, I had to find a\nmute button, unmute button. OK, so I actually\nhave two questions",
    "start": "2018290",
    "end": "2023659"
  },
  {
    "text": "regarding to the\nFourier features. The I think, do you guys sample\nthem uniformly or are they--",
    "start": "2023660",
    "end": "2036320"
  },
  {
    "text": "do you like to learn these? Yeah, so basically, we\nsample them linearly.",
    "start": "2036320",
    "end": "2042890"
  },
  {
    "text": "So basically, we\ntake the full space and we sample them linearly\nwith whatever the budget is. There are-- so in\nvarious settings,",
    "start": "2042890",
    "end": "2050547"
  },
  {
    "text": "we have actually\ntried learning these. So you could actually\ninitialize an array with them and then learn them. And that does help\nsometimes, actually.",
    "start": "2050547",
    "end": "2058190"
  },
  {
    "text": "And you could\npotentially learn-- you could try a more\nsophisticated strategy on this, too. OK, cool.",
    "start": "2058190",
    "end": "2063620"
  },
  {
    "text": "My follow-up question\nis that basically, I feel like the selling\npoint of your research,",
    "start": "2063620",
    "end": "2069469"
  },
  {
    "text": "right, is that you don't make\nany structural assumptions, right? You can take any type of format.",
    "start": "2069469",
    "end": "2076110"
  },
  {
    "text": "However, for the encoding,\nwouldn't the dimensionality, so for example, if\nit's text, it's 1D,",
    "start": "2076110",
    "end": "2082826"
  },
  {
    "text": "right, if it's an image, it will\nbe 2D, and if it's like a video it would be 3D.",
    "start": "2082827",
    "end": "2089780"
  },
  {
    "text": "You have more-- the positional\nencoding will have more points,",
    "start": "2089780",
    "end": "2095240"
  },
  {
    "text": "right? Wouldn't that inherently give\naway the nature of the input?",
    "start": "2095240",
    "end": "2104140"
  },
  {
    "text": "Yeah, so it does. So I completely agree with this. You're totally right. The version of this where we\nhave learned position encodings",
    "start": "2104140",
    "end": "2110770"
  },
  {
    "text": "is the most pure from\nthat point of view. So it's one that gives it\nbasically no information about the ground truth\nspatial structure.",
    "start": "2110770",
    "end": "2118090"
  },
  {
    "text": "What it does give\nthe model, so when you do the learned\nposition encoding, it will say that,\nfor example, there",
    "start": "2118090",
    "end": "2123820"
  },
  {
    "text": "is a correspondence between\npoint k on image one and point k on image two.",
    "start": "2123820",
    "end": "2129789"
  },
  {
    "text": "So that's basically the\nleast amount of information you can give it\nwhile still allowing it to sort of figure out what\nthe structural relationship",
    "start": "2129790",
    "end": "2137110"
  },
  {
    "text": "between the input points is. So this is the direction that\nwe've been trying to push in. In general, giving the\narchitecture access",
    "start": "2137110",
    "end": "2144880"
  },
  {
    "text": "to sort of ground truth\nstructural information like this lives on this\npoint in 2D is helpful.",
    "start": "2144880",
    "end": "2150190"
  },
  {
    "text": "So there's a couple\nof things here. There's sort of from a\npractical point of view,",
    "start": "2150190",
    "end": "2155660"
  },
  {
    "text": "if you want good results, you\nneed to exploit these things. Or it's helpful to\nexploit these things. But we do want to\nmove in the direction",
    "start": "2155660",
    "end": "2161950"
  },
  {
    "text": "where we're relying\non these things less. And so this is\nbasically something we're actively looking into.",
    "start": "2161950",
    "end": "2168490"
  },
  {
    "text": "OK, makes sense. Thank you. So I think [AUDIO OUT]\nhas posted her question",
    "start": "2168490",
    "end": "2174820"
  },
  {
    "text": "on the chat. I also see you have\nyour hand raised. So if you want, you\ncan give it a try.",
    "start": "2174820",
    "end": "2181460"
  },
  {
    "text": "If not, I'll read\nout the question. OK, I'll try. Just let me know if it's choppy.",
    "start": "2181460",
    "end": "2187640"
  },
  {
    "text": "Yeah, so is it good right now? So far so good. Oh good, OK cool. So I'm just curious, looking\nat the perceiver diagram",
    "start": "2187640",
    "end": "2194170"
  },
  {
    "text": "you had, it's a bunch\nof attention layers, the cross-attention\nand self-attention. And I think there's\nbeen this small trend",
    "start": "2194170",
    "end": "2201849"
  },
  {
    "text": "in recent work in\nvision transformers to try to sort of replace\nthe last few layers instead",
    "start": "2201850",
    "end": "2206932"
  },
  {
    "text": "of having attention,\nlike make them be convolutions to address\nthis attention scaling problem, right, in a different manner.",
    "start": "2206932",
    "end": "2213470"
  },
  {
    "text": "And so here, the\nperceiver architecture is trying to make\nself-attention less expensive. And there, they're just\ntrying to replace it.",
    "start": "2213470",
    "end": "2221020"
  },
  {
    "text": "And they kind of just\navoid the problem. And so I'm curious-- and so I've seen\npapers both ways,",
    "start": "2221020",
    "end": "2226493"
  },
  {
    "text": "some that try to do\nthings like the ones you cited, and then some they're\ntrying to do this as well. And in my mind, everyone always\nhas the good results and stuff.",
    "start": "2226493",
    "end": "2234067"
  },
  {
    "text": "So I'm curious if you\nthink there's a reason to do one or the\nother, or if you think this alternative\napproach is also promising,",
    "start": "2234067",
    "end": "2241300"
  },
  {
    "text": "or is there reason\nresearch should go in one direction or the other?",
    "start": "2241300",
    "end": "2246410"
  },
  {
    "text": "Yeah, so to my mind,\nthe big trade-off is one between-- so the\nvision literature, I think,",
    "start": "2246410",
    "end": "2251569"
  },
  {
    "text": "it's just exploded in terms\nof these sort of hybrids. And people trying to find\nthe right-- the exact right",
    "start": "2251570",
    "end": "2256588"
  },
  {
    "text": "place on the Pareto\ncurve for the trade-off of speed and performance. But they're basically\nlooking primarily",
    "start": "2256588",
    "end": "2263119"
  },
  {
    "text": "on vision specific problems. So something-- the\ncomputer vision community itself typically doesn't\nregularize itself away",
    "start": "2263120",
    "end": "2270110"
  },
  {
    "text": "from things that don't work\non things that aren't vision. So you end up with things\nthat are very, very efficient",
    "start": "2270110",
    "end": "2276170"
  },
  {
    "text": "and very performant\non vision problems. And so I think from\nthat point of view, it's an incredibly\nimportant line of work.",
    "start": "2276170",
    "end": "2282750"
  },
  {
    "text": "And that's probably the\nright way of doing things. But what we're sort of\naiming for is something--",
    "start": "2282750",
    "end": "2288920"
  },
  {
    "text": "is the things that are as\ngeneral as possible while still kind of being performant.",
    "start": "2288920",
    "end": "2294570"
  },
  {
    "text": "Got it. So this kind of\nthing is critical-- Oh sorry to cut you off. Go ahead. No, no, please go ahead.",
    "start": "2294570",
    "end": "2299870"
  },
  {
    "text": "I was going to say-- so\nthis kind of thing is-- it's important. Just to summarize,\nso you feel like it's important to focus on\nattention, because that's",
    "start": "2299870",
    "end": "2306388"
  },
  {
    "text": "kind of critical for NLP. You can't just sort put in\na convolution at the end and sort of fix the problem. But in vision maybe\nyou can, and it's fine.",
    "start": "2306388",
    "end": "2313099"
  },
  {
    "text": "Is that a right way\nof understanding it? That's part of it. Vision and NLP aren't\nthe only two domains.",
    "start": "2313100",
    "end": "2319289"
  },
  {
    "text": "And so the thing that\nwe're really basically-- so the kinds of problems that we're\ninterested in doing with this",
    "start": "2319290",
    "end": "2326450"
  },
  {
    "text": "include things like event-based\ncameras, cell biology, sort of proteins, all of\nthese sorts of things,",
    "start": "2326450",
    "end": "2333290"
  },
  {
    "text": "where we may or may not have the\nright convolutional inductive biases to even know how to\nbuild those sorts of things.",
    "start": "2333290",
    "end": "2340670"
  },
  {
    "text": "They end up being whole\nresearch programs, like the mesh-based\nconvolution work. Oh, cool, thank you.",
    "start": "2340670",
    "end": "2345920"
  },
  {
    "text": "I also have one more question\nabout the architecture. So I saw that-- and I'm\nsorry if you said this",
    "start": "2345920",
    "end": "2351470"
  },
  {
    "text": "and I just missed it. But you had cross-attention and\nthen the latent transformer, and then cross-attention.",
    "start": "2351470",
    "end": "2357799"
  },
  {
    "text": "I'm curious what\nhappens if you replace the self-attention in those\nlayers with cross attention. Does it affect your accuracy?",
    "start": "2357800",
    "end": "2364160"
  },
  {
    "text": "Is that even feasible? Is that a valid question? Yeah, so the sort of\nthing that you could do",
    "start": "2364160",
    "end": "2370119"
  },
  {
    "text": "is you could modify this to\nmake it sort of hierarchical. So that there are multiple\nstages of cross-attention. We haven't gotten\nthis working yet.",
    "start": "2370120",
    "end": "2377350"
  },
  {
    "text": "But it doesn't mean\nit's not a good idea. So there might be-- there\nmight be a right way",
    "start": "2377350",
    "end": "2382630"
  },
  {
    "text": "to do this that we haven't\nfigured out, right? But it's something we\nhave tried a little bit. Oh, cool, OK, thank you so much. I appreciate it.",
    "start": "2382630",
    "end": "2388630"
  },
  {
    "text": "Yeah, no problem. OK, let me--",
    "start": "2388630",
    "end": "2394180"
  },
  {
    "text": "I think we're running\nshort on time. So maybe I'll skip ahead. ",
    "start": "2394180",
    "end": "2401490"
  },
  {
    "text": "So before we run out\nof too much time, I want to at least talk\nabout the modifications",
    "start": "2401490",
    "end": "2407820"
  },
  {
    "text": "to this architecture that\nwe've made to make it work sort of even more generally. So one of the problems with\nsort of the first architecture",
    "start": "2407820",
    "end": "2415740"
  },
  {
    "text": "that we looked at here,\nthe basic procedure, is that it works basically\nfor arbitrary inputs.",
    "start": "2415740",
    "end": "2421740"
  },
  {
    "text": "But it's designed to work\nonly on classification and regression\ntasks as an output.",
    "start": "2421740",
    "end": "2428230"
  },
  {
    "text": "And so basically,\nwe wanted to see if we could use the same\ncross-attention strategy for decoding. And it turns out, you can.",
    "start": "2428230",
    "end": "2434250"
  },
  {
    "text": "It's something that works\npretty well just kind of out of the box. So the idea is that\nwe have-- if we",
    "start": "2434250",
    "end": "2440160"
  },
  {
    "text": "have our cross-attention\ninput and self-attention sort of to do the\nprocessing, we can introduce",
    "start": "2440160",
    "end": "2445530"
  },
  {
    "text": "a set of additional queries. And these are\nbasically queries that give the semantics\nof each of the points",
    "start": "2445530",
    "end": "2451590"
  },
  {
    "text": "that you're trying to decode. And we pass these as input to\nanother cross-attention layer,",
    "start": "2451590",
    "end": "2457270"
  },
  {
    "text": "which is configured in\nbasically the opposite way that the encoder\ncross-attention is configured. So now, the queries are\ngoing to be something",
    "start": "2457270",
    "end": "2463710"
  },
  {
    "text": "that's potentially large. And the keys and values are\ncoming from this latent. And so what this allows\nus to do basically",
    "start": "2463710",
    "end": "2469559"
  },
  {
    "text": "is to keep all of\nthe nice advantages of the original perceiver. So we have an encoder\nthat scales linearly.",
    "start": "2469560",
    "end": "2475080"
  },
  {
    "text": "We have a processor stage, this\nsort of latent self-attention that scales independently\nof the input size.",
    "start": "2475080",
    "end": "2480300"
  },
  {
    "text": "And we now have a decoder\nthat keeps the decoupling, but gives us linear scaling\nwith respect to output size.",
    "start": "2480300",
    "end": "2486820"
  },
  {
    "text": "And so by doing this, we\ncan now basically apply the same approach to\nbasically dense output tasks.",
    "start": "2486820",
    "end": "2495107"
  },
  {
    "text": "And so to give you a sense\nof how this works, just sort of intuitively, if\nwe're doing autoencoding",
    "start": "2495107",
    "end": "2500400"
  },
  {
    "text": "on this image of\npuppies, basically what we do is we encode,\nprocess, and then to decode,",
    "start": "2500400",
    "end": "2506910"
  },
  {
    "text": "we take a query that corresponds\nto each of the points. And then we pass it\ninto this decoder.",
    "start": "2506910",
    "end": "2512070"
  },
  {
    "text": "So we can query\none of the points, we get one pixel, query another\none, we get another one. And all the way up, until\nwe get all 10,000 points.",
    "start": "2512070",
    "end": "2520260"
  },
  {
    "text": "And that's how we can do\nreconstruction with this. And the cool thing about\nthis is that it opens up a bunch of new applications.",
    "start": "2520260",
    "end": "2528599"
  },
  {
    "text": "And we can get different\nkinds of outputs just by changing how\nthe queries work. So if we want to do something\nlike multimodal autoencoding,",
    "start": "2528600",
    "end": "2535020"
  },
  {
    "text": "where we have some of\nthe outputs or videos, we use the same construction\ntrick to get positions that--",
    "start": "2535020",
    "end": "2541529"
  },
  {
    "text": "to get queries that have\nthe relevant semantics for each of the points\nthat we're decoding. And we can do this,\neven though, basically,",
    "start": "2541530",
    "end": "2548813"
  },
  {
    "text": "the sizes of these\ndifferent data, so the number of points\nthey have is quite diverse. So in the multimodal\nautoencoding experiments",
    "start": "2548813",
    "end": "2555000"
  },
  {
    "text": "that we have in this paper,\nwe do this for video, audio, and labels at the same time. So that all of them\nare just passed",
    "start": "2555000",
    "end": "2560685"
  },
  {
    "text": "into their uniform network\nand then decoded one by one in this way. But we can also do masked\nlanguage modeling now",
    "start": "2560685",
    "end": "2567780"
  },
  {
    "text": "by conditioning on the\nposition in a sequence. We can do multitask\nclassification",
    "start": "2567780",
    "end": "2573180"
  },
  {
    "text": "by having basically an index\nthat gives which task you're querying from the network.",
    "start": "2573180",
    "end": "2578610"
  },
  {
    "text": "And we can do things\nlike optical flow by passing in input features\nas well as the positions.",
    "start": "2578610",
    "end": "2583859"
  },
  {
    "text": "And so I'm just going\nto just quickly skip to a couple of the different--",
    "start": "2583860",
    "end": "2589530"
  },
  {
    "text": "I can share these slides with\nyou, with you all afterwards, to look through them. Some of these things\nare quite cool.",
    "start": "2589530",
    "end": "2596380"
  },
  {
    "text": "But just quickly, I want\nto talk about language and then optical flow.",
    "start": "2596380",
    "end": "2601800"
  },
  {
    "text": "So for language,\nbasically, what we wanted to do with this\nwas to see if we could use",
    "start": "2601800",
    "end": "2607290"
  },
  {
    "text": "this to replace tokenization. And why might we care about\ngetting rid of tokenization?",
    "start": "2607290",
    "end": "2613500"
  },
  {
    "text": "So we use tokenization primarily\nbecause transformers scale poorly with sequence length.",
    "start": "2613500",
    "end": "2618570"
  },
  {
    "text": "And tokenizing cuts sequence\nlength by about a factor of 4. ",
    "start": "2618570",
    "end": "2624390"
  },
  {
    "text": "But there are various\nproblems that arise with this. And so why might we care\nabout removing tokenizers?",
    "start": "2624390",
    "end": "2632290"
  },
  {
    "text": "So for one, tokenizers perform\nless well on rare words.",
    "start": "2632290",
    "end": "2638120"
  },
  {
    "text": "So if you compare the sort of\nthe byte based decomposition, the UTF based-- UTF-8 encoding of an input--\nof an input sequence like this,",
    "start": "2638120",
    "end": "2645730"
  },
  {
    "text": "you can see that\nthere's basically a uniform allocation\nof points in memory to each of the input characters.",
    "start": "2645730",
    "end": "2652120"
  },
  {
    "text": "The exception are\ndiacritics, which end up splitting it into two. But if you look at the\nSentencePiece tokenization,",
    "start": "2652120",
    "end": "2658210"
  },
  {
    "text": "so it's learned that\npepper is one token. But jalapeno gets split\ninto five in this case.",
    "start": "2658210",
    "end": "2665320"
  },
  {
    "text": "So this basically says it's-- the amount of capacity\nthat you allocate depends on how rare\nthe word is, which can",
    "start": "2665320",
    "end": "2672400"
  },
  {
    "text": "lead to suboptimal encodings. They're also brittle to\nsubtle perturbations.",
    "start": "2672400",
    "end": "2677780"
  },
  {
    "text": "A famous example of this\nis that if you enter-- so if you've ever played\naround with GPT-3,",
    "start": "2677780",
    "end": "2683960"
  },
  {
    "text": "you'll notice that\nthe output can be quite sensitive\nif you add a space or omit a space at the end.",
    "start": "2683960",
    "end": "2688970"
  },
  {
    "text": "And this basically\nis because the space can end up being factorized\ninto different parts of the tokenization. There are other\nproblems-- there are",
    "start": "2688970",
    "end": "2694250"
  },
  {
    "text": "other things that can\nhappen there, too, but this is one cause of that. And finally, tokens don't\ntransfer across languages.",
    "start": "2694250",
    "end": "2701550"
  },
  {
    "text": "So if you wanted to have a\nmodel that, without any tuning, could be used on many different\nlanguages at the same time,",
    "start": "2701550",
    "end": "2706730"
  },
  {
    "text": "tokenizers are a\nblocker for this. So if we can get rid of them,\nit'll simplify the pipeline. It'll also make\nthings less brittle",
    "start": "2706730",
    "end": "2713240"
  },
  {
    "text": "and then hopefully lead\nto more general models. So the way that we do\nmasked language modeling",
    "start": "2713240",
    "end": "2718590"
  },
  {
    "text": "is the same as the\nway that I showed in that schematic\nautoencoding experiment. So we mask some fraction\nof our inputs, about 15%",
    "start": "2718590",
    "end": "2726300"
  },
  {
    "text": "is sort of the\nstandard magic number. We then decode at each of the\npositions that are masked.",
    "start": "2726300",
    "end": "2731940"
  },
  {
    "text": "And we task the model with\ndecoding whatever characters were masked at those locations.",
    "start": "2731940",
    "end": "2738225"
  },
  {
    "text": "And then once we\nhave this model, so this is what we\ndo for pre-training, we can then fine tune it\nby replacing the decoder",
    "start": "2738225",
    "end": "2744510"
  },
  {
    "text": "with a multitask decoder\nthat takes in the tasks that we're using on the\ndownstream evaluation",
    "start": "2744510",
    "end": "2750600"
  },
  {
    "text": "setting and training the model\nto reconstruct the logits on a per task basis. ",
    "start": "2750600",
    "end": "2758329"
  },
  {
    "text": "OK, so to look at how\nthis model performs, we basically first\ncompare it to BERT Base.",
    "start": "2758330",
    "end": "2764240"
  },
  {
    "text": "So this is just\na solid benchmark that we understand very well. And first, by looking\nat sort of matched",
    "start": "2764240",
    "end": "2771380"
  },
  {
    "text": "two models that\nhave matched FLOPs, we can see that Perceiver IO\nand BERT Base work on par.",
    "start": "2771380",
    "end": "2778238"
  },
  {
    "text": "You see there's a\ndifferent trade-off here. So to get the same\nnumber of FLOPs, basically, we make\nPerceiver IO deeper.",
    "start": "2778238",
    "end": "2783980"
  },
  {
    "text": "And this ends up giving\nit more parameters. But on a per FLOPs\nbasis, it ends up performing about the same.",
    "start": "2783980",
    "end": "2791940"
  },
  {
    "text": "On the other hand, if we\nremove the tokenizer from BERT and keep the FLOPs\nthe same, we see that the number of\nparameters-- the number",
    "start": "2791940",
    "end": "2798470"
  },
  {
    "text": "of parameters and the depth\njust drastically fall down. And this is because\nBERT scales quite poorly",
    "start": "2798470",
    "end": "2803960"
  },
  {
    "text": "with sequence length, because\nit uses a normal transformer. But if we use a perceiver\nwithout the tokenization,",
    "start": "2803960",
    "end": "2810980"
  },
  {
    "text": "we can see that we only\nget a slight reduction in the number of parameters\nat the FLOPs count.",
    "start": "2810980",
    "end": "2817370"
  },
  {
    "text": "But the performance performs\nalmost exactly the same. So this means that the\nperceiver in this setting",
    "start": "2817370",
    "end": "2822590"
  },
  {
    "text": "is performing basically\nthe same with and without the tokenization. It's learning a\ndifferent strategy. It's using different parameters.",
    "start": "2822590",
    "end": "2827700"
  },
  {
    "text": "But it basically can be brought\nto the same performance. We can then scale\nthis more by sort",
    "start": "2827700",
    "end": "2832819"
  },
  {
    "text": "of leaning into what happens\nin the tokenizer free setting. And we see that we can\nget a moderate performance",
    "start": "2832820",
    "end": "2837859"
  },
  {
    "text": "boost as well. I think it's also-- in\nthe language setting, it's also useful to look\nat what the attention--",
    "start": "2837860",
    "end": "2844377"
  },
  {
    "text": "the attention maps that\nare sort of learned. And what's being visualized\nhere are basically, for each of the latents, for\nsome subset of the latents,",
    "start": "2844377",
    "end": "2851130"
  },
  {
    "text": "we're looking at where\nit's attending to in the input sequence. And some of these\nend up being local,",
    "start": "2851130",
    "end": "2856950"
  },
  {
    "text": "so looking at specific\npoints in the sentence. Some of them are\nperiodic, so they",
    "start": "2856950",
    "end": "2862170"
  },
  {
    "text": "look sort of at recurring\npoints over the sequence. And some of them also\nlook like they pick out",
    "start": "2862170",
    "end": "2867630"
  },
  {
    "text": "syntactic features,\nwhich is quite nice. So they pick out,\nbasically, exclamation",
    "start": "2867630",
    "end": "2872880"
  },
  {
    "text": "points, or capital letters,\nor other punctuation that's quite useful and\ndecodable from right",
    "start": "2872880",
    "end": "2878010"
  },
  {
    "text": "at the beginning\nof the sequence.  We can also basically use\nthis exact same architecture",
    "start": "2878010",
    "end": "2885660"
  },
  {
    "text": "on optical flow. And optical flow is basically\nan important classical problem",
    "start": "2885660",
    "end": "2891210"
  },
  {
    "text": "in computer vision, where given\na pair of frames in a video, we want to basically\ntrack all of the points,",
    "start": "2891210",
    "end": "2897520"
  },
  {
    "text": "so figure out the motion from\nevery point from one frame to the other. And so optical flow\nis usually visualized",
    "start": "2897520",
    "end": "2903330"
  },
  {
    "text": "using this sort of\ncolorized images that are shown in the bottom. And what this\ngives you basically",
    "start": "2903330",
    "end": "2908640"
  },
  {
    "text": "is a per pixel indication of the\nvelocity at every single point. And so you can see that--",
    "start": "2908640",
    "end": "2915690"
  },
  {
    "text": "so the blade the\ncharacter here is holding is moving to the right.",
    "start": "2915690",
    "end": "2920830"
  },
  {
    "text": "Whereas, this\ncreature behind her is sort of moving\nas moving downwards.",
    "start": "2920830",
    "end": "2926770"
  },
  {
    "text": "So there are a couple of\nproblems with optical flow that make it interesting\nto sort of approach.",
    "start": "2926770",
    "end": "2932900"
  },
  {
    "text": "So one, it's a dense task. And it basically involves\nlong range correspondences.",
    "start": "2932900",
    "end": "2938170"
  },
  {
    "text": "But the standard training\nprotocol, there's basically no large scale\nrealistic training data, just because it's\nincredibly hard",
    "start": "2938170",
    "end": "2943925"
  },
  {
    "text": "to sort of label all of the\npixels in a real world scene and figure out where they go to. So the typical way to do this is\nto train on some synthetic data",
    "start": "2943925",
    "end": "2951130"
  },
  {
    "text": "and then evaluate on\nmore realistic scenes. And optical flow is\nalso interesting,",
    "start": "2951130",
    "end": "2957880"
  },
  {
    "text": "because it's basically\nthe locus of some of the most complicated\nvisual architectures",
    "start": "2957880",
    "end": "2963460"
  },
  {
    "text": "in the literature. So the previous state-of-the-art\nresult here is this method called RAFT, which won the best\npaper award at ECCV last year.",
    "start": "2963460",
    "end": "2971070"
  },
  {
    "text": "And I'm just highlighting\nthis to give you a sense of how much work\npeople do into sort of hand engineering these architectures.",
    "start": "2971070",
    "end": "2977458"
  },
  {
    "text": "So this is a very, very\ncleverly designed architecture. And basically it incorporates\nthings like global correlation",
    "start": "2977458",
    "end": "2983080"
  },
  {
    "text": "volumes that are explicitly\ncomputed at different offsets to basically allow the\nmodel to reason about how",
    "start": "2983080",
    "end": "2989140"
  },
  {
    "text": "things at different\nscales are moving with respect to each other. It also has local neighborhood\ngather operations, as well as",
    "start": "2989140",
    "end": "2997648"
  },
  {
    "text": "update blocks to\nkeep track of what's happening within each\nspecific correlation block. And then finally, there's\na flow specific upsampling",
    "start": "2997648",
    "end": "3005190"
  },
  {
    "text": "operators that were developed. So in contrast to\nthis, we're basically--",
    "start": "3005190",
    "end": "3010550"
  },
  {
    "text": "we wanted to see how well\nPerceiver IO would do here. And just to give\nyou a sense of sort of what we were expecting\ncoming into this,",
    "start": "3010550",
    "end": "3017150"
  },
  {
    "text": "we thought, well, maybe-- so Perceiver IO is throwing\na lot of the structure away. So we were hoping that we\nwould get some good results.",
    "start": "3017150",
    "end": "3023060"
  },
  {
    "text": "But it would probably overfit. And there's this sort of\nproblem of the domain transfer that's happening here. But on the other\nhand, self-attention",
    "start": "3023060",
    "end": "3029060"
  },
  {
    "text": "seems to be a reasonable\nway to match this sort of correspondence thing. What we actually found was\nthat just by doing the very,",
    "start": "3029060",
    "end": "3037140"
  },
  {
    "text": "very simple preprocessing here,\nso extracting basically a patch around each pixel and then\nusing the standard Perceiver IO",
    "start": "3037140",
    "end": "3044869"
  },
  {
    "text": "architecture, we were able to\nget state-of-the-art results here. And so this is\nbasically was validation",
    "start": "3044870",
    "end": "3053030"
  },
  {
    "text": "of this general\napproach of trying to have general purpose\narchitectures that would transfer over.",
    "start": "3053030",
    "end": "3058370"
  },
  {
    "text": "And so basically\nwith minimal tuning, we are able to get\nresults that are",
    "start": "3058370",
    "end": "3064100"
  },
  {
    "text": "both of the sort of\ncompelling benchmarks on both of the Sintel, the\nSintel evaluation methods,",
    "start": "3064100",
    "end": "3071360"
  },
  {
    "text": "and to get comparable\nresults on KITTI. So these are the standard ones. And we can also\nsort of visualize",
    "start": "3071360",
    "end": "3076970"
  },
  {
    "text": "what happens when we apply\nthis on real world data. So there's no ground truth here. So you-- we can't\nreally compare it.",
    "start": "3076970",
    "end": "3083810"
  },
  {
    "text": "But it's still useful to sort\nof see how it moves around. And we can see that\nqualitatively, it's able to capture a lot\nof the fine structure",
    "start": "3083810",
    "end": "3091310"
  },
  {
    "text": "and to sort of get the right\nmotion for the things that are very clearly moving\nin the specific direction.",
    "start": "3091310",
    "end": "3099450"
  },
  {
    "text": "We can also sort of--\nit's also, I think, informative to look\nat what happens-- how it managed to--",
    "start": "3099450",
    "end": "3104640"
  },
  {
    "text": "how it manages to represent\nsort of small structure. Is this video playing?",
    "start": "3104640",
    "end": "3110270"
  },
  {
    "text": "Yeah, we can see it. OK, cool. So the thing to look at here\nis the fine water droplets",
    "start": "3110270",
    "end": "3115820"
  },
  {
    "text": "that are sort of flying through\nthe air as that bird flies by. And because we're decoding\nat every single output point,",
    "start": "3115820",
    "end": "3122839"
  },
  {
    "text": "the architecture is\nable to represent those. So it's able to capture very,\nvery fine scale segmentation",
    "start": "3122840",
    "end": "3128142"
  },
  {
    "text": "that would be\ndifficult to capture if you had, for example, a\nconvolutional upsampler here. ",
    "start": "3128143",
    "end": "3135030"
  },
  {
    "text": "OK, so I'm just going\nto-- also, the light might've just gone\noff in this room. ",
    "start": "3135030",
    "end": "3141075"
  },
  {
    "text": "I'm just going to-- Also try a task\nlike gap estimation, because Perceiver IO looks\nlike it can also work",
    "start": "3141075",
    "end": "3147570"
  },
  {
    "text": "very well on that modalities. Yeah, so we haven't\npublished anything.",
    "start": "3147570",
    "end": "3153450"
  },
  {
    "text": "But some internal\nresults suggest that it works just fine. There basically--\nthere don't seem",
    "start": "3153450",
    "end": "3159450"
  },
  {
    "text": "to be-- one of the\nsurprising things, the things that we were a little\nbit unsure about was how much information was going\nto be contained in this latent.",
    "start": "3159450",
    "end": "3166230"
  },
  {
    "text": "Because basically, you're\nabstracting quite a lot. And you're not-- it doesn't have\nany 2D structure intrinsically.",
    "start": "3166230",
    "end": "3171720"
  },
  {
    "text": "But it does seem\nlike that it seems to be able to represent\nthings quite well. And these sorts of\ndecoding mechanisms",
    "start": "3171720",
    "end": "3179130"
  },
  {
    "text": "do seem to be able to do that. Got it, OK. ",
    "start": "3179130",
    "end": "3184193"
  },
  {
    "text": "So I'm just going to-- in the interest of time,\nI'm going to skip ahead to the conclusion. Drew, I had one\nquestion with respect",
    "start": "3184193",
    "end": "3191260"
  },
  {
    "text": "to the metrics that you've\nshared for the optical flow, the number. So in the table, it was like\nSinel.final, clean, and KITTI",
    "start": "3191260",
    "end": "3200410"
  },
  {
    "text": "were these different data\nsets or different metrics, same metric for\ndifferent sets or these",
    "start": "3200410",
    "end": "3206020"
  },
  {
    "text": "are three different metrics? Yeah, so these are three\ndifferent data sets. So Sintel.clean and\nSintel.final are basically two--",
    "start": "3206020",
    "end": "3213430"
  },
  {
    "text": "they're two ways of doing the\nfinal rendering for Sintel. In all cases, these\nmethods are trained just",
    "start": "3213430",
    "end": "3219790"
  },
  {
    "text": "on the autoflow data set. So they're trained on this\nsort of general purpose kind of wacky synthetic\nmotion data set.",
    "start": "3219790",
    "end": "3227715"
  },
  {
    "text": "And then we're\nevaluation-- we're evaluating on these different\ndomains without fine tuning. OK. ",
    "start": "3227715",
    "end": "3235710"
  },
  {
    "text": "Yeah, the flow has quite--\nthe data sets are quite small. So it's generally even\nproblematic to fine tune.",
    "start": "3235710",
    "end": "3241480"
  },
  {
    "text": "Thank you.  OK, so just to summarize--",
    "start": "3241480",
    "end": "3248070"
  },
  {
    "text": "What is the ground truth\nto find the point error?",
    "start": "3248070",
    "end": "3253590"
  },
  {
    "text": "Yeah, so the way\nthis works is Sintel is a computer-- it's basically\na relatively high quality CGI",
    "start": "3253590",
    "end": "3261329"
  },
  {
    "text": "movie that was\nbasically open-source. And so they actually\nhave the ground truth. So if you know the ground\ntruth and the state,",
    "start": "3261330",
    "end": "3268320"
  },
  {
    "text": "you can compute the pixel\ncorrespondence from frame to frame. So that's what's used on Sintel.",
    "start": "3268320",
    "end": "3273690"
  },
  {
    "text": "And then KITTI, they\nbasically have-- they have a LIDAR sensor\nthat's used to figure out",
    "start": "3273690",
    "end": "3279410"
  },
  {
    "text": "the depth of all points. And then they compute\nthe correspondences. So the ground truth is actually\nthe ground truth optical flow.",
    "start": "3279410",
    "end": "3286260"
  },
  {
    "text": "But in general, it's hard\nto get dense optical flow. It's very expensive\nto collect it.",
    "start": "3286260",
    "end": "3291990"
  },
  {
    "text": "Great, thanks.  OK, so the-- so basically,\njust to summarize,",
    "start": "3291990",
    "end": "3299280"
  },
  {
    "text": "so the perceivers are\nattention-based architectures that scale linearly and\nwork as drop-in replacements",
    "start": "3299280",
    "end": "3305490"
  },
  {
    "text": "for transformers on a\nvariety of settings. They also seem to be able\nto achieve results that",
    "start": "3305490",
    "end": "3310950"
  },
  {
    "text": "are comparable, at\nleast, in performance with models that rely\non 2D convolutions. But of course, there\nis a trade-off here.",
    "start": "3310950",
    "end": "3317190"
  },
  {
    "text": "And so it's good to\nbe very aware of this, of generality versus\nspeed in specific domains.",
    "start": "3317190",
    "end": "3322540"
  },
  {
    "text": "And so as was pointed\nout in settings where you can use\n2D convolutions. It's certainly good to\nhave them in the loop.",
    "start": "3322540",
    "end": "3330300"
  },
  {
    "text": "It's basically a\nunified architecture that allows joint modeling\nof different modalities",
    "start": "3330300",
    "end": "3336420"
  },
  {
    "text": "of different sizes. And basically, overall, it\nseems to be a quite flexible",
    "start": "3336420",
    "end": "3342210"
  },
  {
    "text": "architecture that's able to\nproduce a state-of-the-art or near state-of-the-art results\non a variety of different domains.",
    "start": "3342210",
    "end": "3348030"
  },
  {
    "text": "And there's-- in the two papers,\nwe look at a number of other domains that I\ndidn't talk about,",
    "start": "3348030",
    "end": "3353100"
  },
  {
    "text": "including 3D point\ncloud modeling, replacing the transformer\nthat's used in the StarCraft,",
    "start": "3353100",
    "end": "3359100"
  },
  {
    "text": "in the sort of StarCraft\nbehavioral cloning agent and a couple of others. So this does-- we\nhave a lot of evidence",
    "start": "3359100",
    "end": "3365160"
  },
  {
    "text": "that this general approach\nseems to work broadly. And there's a lot of things\nwe still haven't tried. So we're very interested\nin pushing this and always,",
    "start": "3365160",
    "end": "3372750"
  },
  {
    "text": "always open for\nsuggestions and so forth. So we're relying on a\nlarge body of related work,",
    "start": "3372750",
    "end": "3379141"
  },
  {
    "text": "because we're sort of drawing\nfrom a lot of different areas here. So here are some highlights. And then I just want to thank\nmy coauthors on this work.",
    "start": "3379142",
    "end": "3389290"
  },
  {
    "text": "And of course, I'm\nhappy to talk more.  Thanks, Drew.",
    "start": "3389290",
    "end": "3395758"
  },
  {
    "text": "Thanks a lot. Thanks, Drew. So one question I had\nis so what do you think",
    "start": "3395758",
    "end": "3401790"
  },
  {
    "text": "is the future of similar models. Do you think these\nare going to be",
    "start": "3401790",
    "end": "3407819"
  },
  {
    "text": "used more in the sort\nof transformer community to replace ConvNet\nand other stuff?",
    "start": "3407820",
    "end": "3414690"
  },
  {
    "text": "Yeah, so the-- I think, broadly\nspeaking, I think it's-- I think of perceivers\nnow as sort of--",
    "start": "3414690",
    "end": "3421080"
  },
  {
    "text": "because we know how to\nadapt them pretty well to sort of domains\nwhere we don't have a great idea of the right way\nto structure an architecture,",
    "start": "3421080",
    "end": "3428490"
  },
  {
    "text": "an inductive bias. So I think that's one of the\nreally strong cases for it,",
    "start": "3428490",
    "end": "3433800"
  },
  {
    "text": "so settings in which\nyou don't really know what the right way\nto structure a problem is. I also think these\nkinds of approaches",
    "start": "3433800",
    "end": "3442200"
  },
  {
    "text": "can be used in\nconjunction with ConvNets for sort of things that are\nas domain agnostic as needed.",
    "start": "3442200",
    "end": "3448900"
  },
  {
    "text": "But I think multimodal and\nnew domains is really-- that's where these\nare obvious choices.",
    "start": "3448900",
    "end": "3456870"
  },
  {
    "text": "Also, what do you think are the\ncurrent bottlenecks with this? And if you don't mind,\nif you can disclose,",
    "start": "3456870",
    "end": "3462210"
  },
  {
    "text": "what are you working on\ntowards next with this stuff? So I can't-- I can't talk about\ntoo many details about that.",
    "start": "3462210",
    "end": "3469410"
  },
  {
    "text": "But a couple of domains-- so one, we don't really\nhave a great handle",
    "start": "3469410",
    "end": "3475050"
  },
  {
    "text": "on how to use them on\nsort of small scale data, so data where you don't have\nthe data to sort of recover",
    "start": "3475050",
    "end": "3484410"
  },
  {
    "text": "the inductive bias. So this is, I think, a\nreally important area. The other thing that we haven't\nsort of talked about here,",
    "start": "3484410",
    "end": "3490603"
  },
  {
    "text": "but you could probably imagine\nthat we'd be thinking about would be how to train on\nmultiple modalities or sort of multiple things\nat the same time.",
    "start": "3490603",
    "end": "3497140"
  },
  {
    "text": "So right now, all of\nthese architectures are sort of trained\nin isolation. But there are a lot\nof opportunities",
    "start": "3497140",
    "end": "3504329"
  },
  {
    "text": "for sort of figuring out how to\npose problems together and use a single architecture\non all of them.",
    "start": "3504330",
    "end": "3509500"
  },
  {
    "text": "Got it. Also, I'm sure if you've\ntried, but can you also use this for tabular data?",
    "start": "3509500",
    "end": "3514910"
  },
  {
    "text": "Yeah, so effectively,\nthe architecture treats any input\ndata as tabular data.",
    "start": "3514910",
    "end": "3521220"
  },
  {
    "text": "So I think that's exactly the\nright way to think about it. Sounds good.",
    "start": "3521220",
    "end": "3526447"
  },
  {
    "text": "Thanks for the talk. I will open mic general\nquestions for the students. So stop the recording.",
    "start": "3526447",
    "end": "3533150"
  },
  {
    "start": "3533150",
    "end": "3538000"
  }
]