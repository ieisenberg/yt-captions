[
  {
    "start": "0",
    "end": "42000"
  },
  {
    "text": "Welcome everyone. So, uh, today, this is Lecture 21 of, uh,",
    "start": "4040",
    "end": "9855"
  },
  {
    "text": "CS229 and the topic for today was, uh, Evaluation Metrics and also some-",
    "start": "9855",
    "end": "16560"
  },
  {
    "text": "some general advice on applying machine learning in- in practice, you know, in deployment or in production.",
    "start": "16560",
    "end": "23580"
  },
  {
    "text": "So we're gonna look at a few evaluation metrics and next, uh, once we're through with this presentation will,",
    "start": "23580",
    "end": "30250"
  },
  {
    "text": "you know, quickly go through some- some, uh, ways in which these evaluation metrics can actually be useful in- in production.",
    "start": "30250",
    "end": "37510"
  },
  {
    "text": "So let's get started. So, uh, this is the, uh,",
    "start": "37510",
    "end": "42899"
  },
  {
    "start": "42000",
    "end": "76000"
  },
  {
    "text": "quick overview of what we're gonna, uh, be covering in this presentation. You know, first, you know, why evaluation metrics then, uh,",
    "start": "42900",
    "end": "49220"
  },
  {
    "text": "we'll focus on binary classifiers that will be, uh, that will be the- the,",
    "start": "49220",
    "end": "55215"
  },
  {
    "text": "uh, problem setting that we'll spend most time on. Uh, different kinds of metrics and you know, some general, uh,",
    "start": "55215",
    "end": "63035"
  },
  {
    "text": "tips on how to choose evaluation metrics and what to do when, you know, in certain scenarios,",
    "start": "63035",
    "end": "68690"
  },
  {
    "text": "when there's class imbalance and how certain metrics kind of break down in, uh, in- in different settings.",
    "start": "68690",
    "end": "75450"
  },
  {
    "start": "76000",
    "end": "323000"
  },
  {
    "text": "So first of all, uh, why are, uh, evaluation metrics important?",
    "start": "76040",
    "end": "81390"
  },
  {
    "text": "In what we've seen in, uh, in all the, uh,",
    "start": "81390",
    "end": "86579"
  },
  {
    "text": "algorithms in this class, we defined some kind of a loss function and",
    "start": "86580",
    "end": "91720"
  },
  {
    "text": "minimize the loss function with respect to you know, the parameters, uh, to- to minimize the loss on the, uh, training data.",
    "start": "91720",
    "end": "99455"
  },
  {
    "text": "However, these, uh, loss functions, uh, it's not always clear whether these loss functions are meaningful,",
    "start": "99455",
    "end": "107800"
  },
  {
    "text": "uh, in terms of the real world use, right? Uh, for example, the loss function may not, uh,",
    "start": "107800",
    "end": "113790"
  },
  {
    "text": "may or may not capture some kind of a business goal you have, right? Uh, so you know, if- if your business goal is to improve your revenue or your profit,",
    "start": "113790",
    "end": "123079"
  },
  {
    "text": "like your loss function may or may not always reflect that. Right? So it's always important. You know, that- that's why evaluation metrics,",
    "start": "123080",
    "end": "130160"
  },
  {
    "text": "which are like this- this, uh, secondary, uh, measure that you have on your model performance become extremely important, right?",
    "start": "130160",
    "end": "137600"
  },
  {
    "text": "It also- evaluation metrics also helps organize your team effort, uh, towards some kind of a business target.",
    "start": "137600",
    "end": "144125"
  },
  {
    "text": "Alright? So usually it's- it's common practice to define some kind of a dev set and the team which is working on improving the model performance,",
    "start": "144125",
    "end": "153755"
  },
  {
    "text": "strive to improve the evaluation metric on the dev set, right? Uh, you- you know, uh,",
    "start": "153755",
    "end": "160270"
  },
  {
    "text": "you want- you want to start thinking about the evaluation metric as something distinct from the loss itself, right?",
    "start": "160270",
    "end": "168150"
  },
  {
    "text": "It- It's also useful to kind of quantify the gap between,",
    "start": "168150",
    "end": "173510"
  },
  {
    "text": "uh, the desired performance and the baseline, right? So it's, uh, easy for, say,",
    "start": "173510",
    "end": "179840"
  },
  {
    "text": "a product manager to define what the evaluation metric should evaluate to on your dev set,",
    "start": "179840",
    "end": "185090"
  },
  {
    "text": "but it's hard for someone to- to say what the loss value should be on a dev set, right?",
    "start": "185090",
    "end": "190190"
  },
  {
    "text": "So, uh, so evaluation metrics are useful to, you know, measure how well your model is doing in terms of your desired performance level, right?",
    "start": "190190",
    "end": "198830"
  },
  {
    "text": "So it's- it's useful to, uh, quanti- uh, to quantify the gap. And, uh, it also, uh, you know,",
    "start": "198830",
    "end": "206460"
  },
  {
    "text": "you can use it to- to, uh, see the gap between the desired performance and the baseline.",
    "start": "206460",
    "end": "211970"
  },
  {
    "text": "Uh, baseline is generally the first attempt that you come up with some kind of a simple model that gives you a sense of,",
    "start": "211970",
    "end": "217980"
  },
  {
    "text": "uh, difficulty of the overall project that you're starting with. If the gap between the desired performance and the baseline is a lot then,",
    "start": "217980",
    "end": "224450"
  },
  {
    "text": "you know, probably it's- it's, uh, a challenging task. And similarly, you can also measure the gap between the desired performance and the current performance,",
    "start": "224450",
    "end": "231635"
  },
  {
    "text": "which kind of gives you a sense of how much more progress is left to be made, right? Uh, and it- it's also useful to kind of,",
    "start": "231635",
    "end": "239584"
  },
  {
    "text": "uh, keep- keep track of how our performance is improving over time. Um, you know, when we started off, we were at, you know,",
    "start": "239585",
    "end": "247970"
  },
  {
    "text": "60% accuracy three months in we are at 75% accuracy. Our ideal, you know, our target is, you know,",
    "start": "247970",
    "end": "253545"
  },
  {
    "text": "85% accuracy so that, you know, it gives you a sense of how much progress you've made towards the end goal.",
    "start": "253545",
    "end": "259310"
  },
  {
    "text": "Um, It's also useful for lower level tasks such as debugging, right?",
    "start": "259310",
    "end": "264410"
  },
  {
    "text": "You wanna do some bias-variance analysis on your model to improve your model performance and evaluation metrics are useful there as well.",
    "start": "264410",
    "end": "270685"
  },
  {
    "text": "Uh, so ideally your training objective, your loss function, should reflect your business goal, right?",
    "start": "270685",
    "end": "278070"
  },
  {
    "text": "But that's not always possible. For example, if you care about accuracy, it's very hard to train a model by using accuracy as the loss function, right?",
    "start": "278070",
    "end": "287210"
  },
  {
    "text": "So accuracy is- is, uh, is not even differentiable. It's uh, it- it ends up being a very hard combinatorial problem and it's,",
    "start": "287210",
    "end": "295205"
  },
  {
    "text": "um, using accuracy as your loss, uh, value- as your loss function, you know, generally just does not work.",
    "start": "295205",
    "end": "301625"
  },
  {
    "text": "You can't get gradients out with them, etc. So, um, ideally we would want, um, your metric to itself be the loss function,",
    "start": "301625",
    "end": "308990"
  },
  {
    "text": "but that's- that's not always feasible. So you know, that kind of makes it necessary to- to do this, uh,",
    "start": "308990",
    "end": "314720"
  },
  {
    "text": "measuring this evaluation metric in parallel as we are developing the model be- beyond just the loss function.",
    "start": "314720",
    "end": "320850"
  },
  {
    "text": "So, uh, we're gonna limit, uh, for most of this, uh, presentation,",
    "start": "322690",
    "end": "327960"
  },
  {
    "start": "323000",
    "end": "715000"
  },
  {
    "text": "we're gonna, uh, stick to binary classification, uh, in a supervised setting. So think of X as an input.",
    "start": "327960",
    "end": "334635"
  },
  {
    "text": "X could be an image, it could be an email. Uh, Y is a binary output. You know [NOISE] If it's an image, you know,",
    "start": "334635",
    "end": "340745"
  },
  {
    "text": "you want to tell whether there's- whether there's a pedestrian in that picture or not, uh, if you're, you know, in- in the context of, say,",
    "start": "340745",
    "end": "346940"
  },
  {
    "text": "a self-driving car, um, it could be whether a given text is spam or not. Uh, so Y is- is, uh,",
    "start": "346940",
    "end": "354210"
  },
  {
    "text": "think of Y as binary, the model output is y hat, right? And this y hat can be of- of different types.",
    "start": "354210",
    "end": "361900"
  },
  {
    "text": "For certain kinds of algorithms, uh, for example, uh, we did not cover these in- in the course itself, but you know,",
    "start": "361900",
    "end": "368350"
  },
  {
    "text": "algorithms like K nearest neighbor or decision trees, the output of the algorithm is directly the class prediction itself.",
    "start": "368350",
    "end": "375005"
  },
  {
    "text": "The output is the class directly, right? And there are other kinds of algorithms whose output is some kind of a real-valued score,",
    "start": "375005",
    "end": "381920"
  },
  {
    "text": "like, you know, support vector machines or logistic regression. You know logistic regression, the output was the probability of Y being 1, right?",
    "start": "381920",
    "end": "389479"
  },
  {
    "text": "It's a real value, it's not a class directly, right? And, uh, with- with, uh, support vector machines, uh,",
    "start": "389480",
    "end": "395590"
  },
  {
    "text": "you will output the margin, right? How far from the margin separating hyperplane is the,",
    "start": "395590",
    "end": "400790"
  },
  {
    "text": "uh, example that we're looking at. Is it to the- to the left of the margin or to the right of the margin, right?",
    "start": "400790",
    "end": "405979"
  },
  {
    "text": "So those are- those are real value. And in these, uh, these- these, uh, score based, uh,",
    "start": "405980",
    "end": "413600"
  },
  {
    "text": "algorithms, where we output some kind of a real value, it becomes necessary to choose some kind of a threshold.",
    "start": "413600",
    "end": "419870"
  },
  {
    "text": "And once you pick that threshold, you can convert your- your model into a classifier, right?",
    "start": "419870",
    "end": "426435"
  },
  {
    "text": "And thi- this is, uh, the score based, uh, models are the one we'll be, uh, we'll be looking at in- in this presentation, right?",
    "start": "426435",
    "end": "434805"
  },
  {
    "text": "So think of, uh, this is kind of the, uh, mental picture you want to have. So, uh, the line here from, uh, 0-1,",
    "start": "434805",
    "end": "442610"
  },
  {
    "text": "think of it as, uh, probably the range of probabilities that can be output by the model, right?",
    "start": "442610",
    "end": "448160"
  },
  {
    "text": "0 indicates pro- the predicted probability 0 and 1 indicates predicted probability is 1.",
    "start": "448160",
    "end": "453305"
  },
  {
    "text": "So the- the green dots, uh, represent, uh, positive examples.",
    "start": "453305",
    "end": "460360"
  },
  {
    "text": "And the position of the dot is the probability assigned by the model for that example, right?",
    "start": "460360",
    "end": "466370"
  },
  {
    "text": "So the green dots are- are the positive examples, gray dots are the, um, negative examples and, uh,",
    "start": "466370",
    "end": "473470"
  },
  {
    "text": "we- we, uh, this- this by, uh, placing them on a line like this, we are kind of defining, uh,",
    "start": "473470",
    "end": "479870"
  },
  {
    "text": "an actual order on the predictions, right? And it's, uh, it's- it's always good to first have a look at, uh,",
    "start": "479870",
    "end": "489320"
  },
  {
    "text": "you know, have a look at this kind of rank order of your, uh, of your examples in your dev set before you start debugging your model, right?",
    "start": "489320",
    "end": "497720"
  },
  {
    "text": "This gives you, you know, uh, a good sense of- of, uh, you know, what- what your model is- is kind of doing, uh, in- in a big picture.",
    "start": "497720",
    "end": "505300"
  },
  {
    "text": "A few more, uh, you know, so- some metrics that are, or some terminology that are- are useful.",
    "start": "505300",
    "end": "511520"
  },
  {
    "text": "So prevalence is a term that refers to the fraction of examples that are positives, right?",
    "start": "511520",
    "end": "517219"
  },
  {
    "text": "So that's- that's a standard terminology. So if you're in a situation where- where, uh,",
    "start": "517220",
    "end": "522370"
  },
  {
    "text": "you have an equal number of positive and negative examples in your data, then the prevalence is 50%, right?",
    "start": "522370",
    "end": "528255"
  },
  {
    "text": "If you have, say, um, 10 positive examples and, uh, and 90 negative examples then your prevalence is 10%,",
    "start": "528255",
    "end": "536810"
  },
  {
    "text": "10 over 10 plus 90. Alright.",
    "start": "536810",
    "end": "543090"
  },
  {
    "text": "This prevalence [inaudible] Good question. So prevalence- the question is",
    "start": "544480",
    "end": "549930"
  },
  {
    "text": "prevalence a property of the data or is it a property of the, uh, of the model? Prevalence is totally a property of the data that you have.",
    "start": "549930",
    "end": "557584"
  },
  {
    "text": "Um, you know, your- your, uh, yeah, it's- it's just a fraction of positives that you- So here,",
    "start": "557585",
    "end": "563845"
  },
  {
    "text": "a good- good, uh, good point is that here when we are counting positives and negatives, we are counting the- the true ground truths,",
    "start": "563845",
    "end": "570770"
  },
  {
    "text": "not the predicted probabilities, right?. Any- any questions on this.",
    "start": "570770",
    "end": "577139"
  },
  {
    "text": "An- and it's this prev- this prevalence term that- that allows us to,",
    "start": "577660",
    "end": "583470"
  },
  {
    "text": "uh, kind of, uh, decide whether we have something like a class imbalance problem. So class imbalance problem is when,",
    "start": "583470",
    "end": "590075"
  },
  {
    "text": "you know, one of the, uh, classes is either too many or- or too little compared relative to the other class.",
    "start": "590075",
    "end": "596644"
  },
  {
    "text": "If- if out of 100 examples in your dataset, if 2 are positive and 98 are negative,",
    "start": "596645",
    "end": "602885"
  },
  {
    "text": "then, you know, you generally call it a class imbalance problem. There's no well, uh,",
    "start": "602885",
    "end": "608605"
  },
  {
    "text": "well a- agreed upon threshold to decide whether you have a class imbalance problem or not.",
    "start": "608605",
    "end": "615245"
  },
  {
    "text": "But, uh, as a thumb rule, if- if, you know, one of the classes, if the prevalence is, you know,",
    "start": "615245",
    "end": "620360"
  },
  {
    "text": "less than 5%  or 10% or more than 90% or 95%, then, you know, it's- it's reasonable to think",
    "start": "620360",
    "end": "627199"
  },
  {
    "text": "of- think of the problem as having a low prevalence. And, er, sometimes for example,",
    "start": "627200",
    "end": "632389"
  },
  {
    "text": "if, uh, you're in- you're in, uh, the problem of, say, detecting, you know, credit card fraud, right?",
    "start": "632390",
    "end": "640100"
  },
  {
    "text": "The volume of non-fraud transactions is- is, you know, is probably so high that, you know,",
    "start": "640100",
    "end": "646640"
  },
  {
    "text": "in those kind of problems prevalence would be really- really small, like you know, much smaller than 1% [NOISE].",
    "start": "646640",
    "end": "654209"
  },
  {
    "text": "Okay. So we started with, uh- uh, you know, the score based view. I'm going to go back to the slide to compare again.",
    "start": "654210",
    "end": "661245"
  },
  {
    "text": "So we started with this line where, you know, we've- we've placed all the examples,",
    "start": "661245",
    "end": "666524"
  },
  {
    "text": "uh- uh, based on- by ranking them based on the predicted probability of the model, right?",
    "start": "666525",
    "end": "672120"
  },
  {
    "text": "So the green and white are property of the data. Their positions is the property of the model, the model assigned probabilities for each example,",
    "start": "672120",
    "end": "679830"
  },
  {
    "text": "and, uh, that's how we started. Uh This by itself is not a classifier, but once we decide a threshold,",
    "start": "679830",
    "end": "686774"
  },
  {
    "text": "right, once we have a threshold, let's say we set the threshold equals 0.5 now we have a classifier.",
    "start": "686775",
    "end": "693254"
  },
  {
    "text": "Every- every example that's above the threshold line is classified as positive,",
    "start": "693254",
    "end": "698370"
  },
  {
    "text": "whether it was positive or not, every- every example below the threshold is classified as negative, right?",
    "start": "698370",
    "end": "704430"
  },
  {
    "text": "So the- the, uh, vertical axis is um, is- is our prediction.",
    "start": "704430",
    "end": "711105"
  },
  {
    "text": "Anything above that, uh, the- the 0.5 threshold is predicted to be positive.",
    "start": "711105",
    "end": "716130"
  },
  {
    "start": "715000",
    "end": "820000"
  },
  {
    "text": "Anything below is predicted to be negative. And the true label is on this,",
    "start": "716130",
    "end": "722235"
  },
  {
    "text": "um, you know uh, we- we kind of separate them horizontally. So to the left is those examples that truly",
    "start": "722235",
    "end": "728820"
  },
  {
    "text": "are positive and to the right are examples that truly are negative, and here we've chosen a- a threshold to be, you know,",
    "start": "728820",
    "end": "736590"
  },
  {
    "text": "arbitrarily 0.5 and only once we choose a threshold does a- a- a score based model become a classifier,",
    "start": "736590",
    "end": "746984"
  },
  {
    "text": "until we decide a threshold it's not a classifier, right, it's just assigning scores or probabilities. After we choose a threshold, we get a classifier, right?",
    "start": "746984",
    "end": "754980"
  },
  {
    "text": "So, uh, what did we do here? We just summed up the number of examples in each block, right?",
    "start": "754980",
    "end": "762945"
  },
  {
    "text": "On the- on the, uh, top-left is the examples that were actually positive,",
    "start": "762945",
    "end": "769500"
  },
  {
    "text": "and we also predicted positive. And similarly, on the- uh, on the right- right top, we have examples that were predicted positive but were actually negative, right?",
    "start": "769500",
    "end": "779010"
  },
  {
    "text": "Uh, this kind of a matrix is called a confusion matrix. That's just a standard terminology.",
    "start": "779010",
    "end": "785490"
  },
  {
    "text": "So A few properties of, um, of- of- of the confusion matrix,",
    "start": "785490",
    "end": "791024"
  },
  {
    "text": "the first is that the total sum of all the- all the four squares is fixed,",
    "start": "791025",
    "end": "797265"
  },
  {
    "text": "and because the number of examples we have is fixed. Uh, also the column sums are also fixed, right,",
    "start": "797265",
    "end": "806580"
  },
  {
    "text": "the sum are 9 plus 1, and 2 plus 8 in this case, will always be fixed because they- they are, you know,",
    "start": "806580",
    "end": "813555"
  },
  {
    "text": "the number of positive examples we have and number of negative examples we have. What can change in a confusion matrix is the, uh,",
    "start": "813555",
    "end": "821774"
  },
  {
    "start": "820000",
    "end": "905000"
  },
  {
    "text": "based on what threshold we choose and the way in which the examples have been- have been,",
    "start": "821775",
    "end": "828165"
  },
  {
    "text": "um, ordered by our model, the- the fraction of the left column that",
    "start": "828165",
    "end": "833970"
  },
  {
    "text": "goes up and below and fraction of the right column that gets split up and below, that can change, so the total sum is fixed,",
    "start": "833970",
    "end": "840300"
  },
  {
    "text": "the- the column sums are also fixed. Basically, um, by looking at a confusion matrix,",
    "start": "840300",
    "end": "848760"
  },
  {
    "text": "you kind of judge the quality of your model depending on how heavy the diagonals are and how light the off diagonals are,",
    "start": "848760",
    "end": "857775"
  },
  {
    "text": "so you want the diagonals to be heavy, which means you want the- the, uh, examples that were positive to be predicted positive and that were",
    "start": "857775",
    "end": "865740"
  },
  {
    "text": "negative to be predicted negative and you want the errors, you know- you know, examples that were positive but predicted negative to be low and,",
    "start": "865740",
    "end": "873975"
  },
  {
    "text": "you know, similarly that were negative and predicted positive to be low. So um, another, uh, observation to make",
    "start": "873975",
    "end": "881820"
  },
  {
    "text": "here is that a confusion matrix does not give you a scalar value, like it's a set of four numbers,",
    "start": "881820",
    "end": "887085"
  },
  {
    "text": "given two, um, uh, different confusion matrix- matrices for the same model in the same data set,",
    "start": "887085",
    "end": "894165"
  },
  {
    "text": "it's hard to- it's hard to compare them and say which confusion matrix is better. They're not scalar, they're like sets of four numbers,",
    "start": "894165",
    "end": "900375"
  },
  {
    "text": "you- you can't compare them, um, uh, unambiguously. Okay? And- and so we start,",
    "start": "900375",
    "end": "907485"
  },
  {
    "start": "905000",
    "end": "956000"
  },
  {
    "text": "um, extracting comparable, uh, metrics out of this. The, uh, top left,",
    "start": "907485",
    "end": "914459"
  },
  {
    "text": "that- the- the- the number in the- in the top left quadrant is called true positives so they are,",
    "start": "914460",
    "end": "920235"
  },
  {
    "text": "uh, true positives, uh, is a confusing term. So you- you might want to- you know, uh, it's- it's,",
    "start": "920235",
    "end": "927855"
  },
  {
    "text": "um, true positive sounds like what- what is the ground truth. But- but unfortunately, the term true positive refers to",
    "start": "927855",
    "end": "934415"
  },
  {
    "text": "examples that were actually positive and were also predicted to be positive.",
    "start": "934415",
    "end": "939725"
  },
  {
    "text": "Um, so in this case, true positive is, um, is- is nine,",
    "start": "939725",
    "end": "945840"
  },
  {
    "text": "and so on the- on the- on the, uh, right-hand side, we're going to start extracting, um,",
    "start": "945840",
    "end": "951600"
  },
  {
    "text": "these metrics or statistics out of, uh, this confusion matrix. Next, um, true negatives.",
    "start": "951600",
    "end": "958649"
  },
  {
    "start": "956000",
    "end": "988000"
  },
  {
    "text": "You know, again, true negatives, uh, is the number of examples that were actually negative but also predicted to be, uh, negative.",
    "start": "958650",
    "end": "965130"
  },
  {
    "text": "Uh, here trueness refers to, you know, um, you know, that our prediction was true.",
    "start": "965130",
    "end": "971685"
  },
  {
    "text": "That's- that's what it basically means. So true positive means it was predicted positive and we got it right. True negative means it was actually negative and we got it right.",
    "start": "971685",
    "end": "979470"
  },
  {
    "text": "So- so trueness kind of lies on the diagonal. We want the true positives and true negatives to be high.",
    "start": "979470",
    "end": "985050"
  },
  {
    "text": "[NOISE] False positive is, uh,",
    "start": "985050",
    "end": "992535"
  },
  {
    "start": "988000",
    "end": "1009000"
  },
  {
    "text": "examples that were predicted to be positive,",
    "start": "992535",
    "end": "998084"
  },
  {
    "text": "but our prediction was wrong, a false prediction. So false positives are examples that were actually negative,",
    "start": "998085",
    "end": "1004850"
  },
  {
    "text": "but we predicted them as positive. Uh, similarly false negative are, um,",
    "start": "1004850",
    "end": "1012800"
  },
  {
    "start": "1009000",
    "end": "1063000"
  },
  {
    "text": "examples that were, um, um, actually positive, but we predicted them as, uh, negative.",
    "start": "1012800",
    "end": "1019865"
  },
  {
    "text": "And these two are- are, uh, false positives and false negatives are- are kind of,",
    "start": "1019865",
    "end": "1025189"
  },
  {
    "text": "you know, two very different kinds of errors, and depending on the actual application that you are, uh,",
    "start": "1025190",
    "end": "1031444"
  },
  {
    "text": "working on, uh, false positives and false negatives may have very different kinds of impacts.",
    "start": "1031445",
    "end": "1037130"
  },
  {
    "text": "It's- it's a very common that the- the, uh, the kind of weight that you want to assign to",
    "start": "1037130",
    "end": "1044150"
  },
  {
    "text": "false positives and false negatives will generally be asymmetric. Um, so- um, so we have, you know,",
    "start": "1044150",
    "end": "1052265"
  },
  {
    "text": "basically extracted four metrics out of this confusion matrix so far. This false positive and false negative are- um,",
    "start": "1052265",
    "end": "1059929"
  },
  {
    "text": "they also have, uh, some other terminology, for example, they're also called type 1 and type 2 errors, right,",
    "start": "1059930",
    "end": "1065690"
  },
  {
    "start": "1063000",
    "end": "1178000"
  },
  {
    "text": "so type 1 error is when, um, is a false positive, when- you know, you don't have a certain condition,",
    "start": "1065690",
    "end": "1073519"
  },
  {
    "text": "but you predict that you have the condition. You don't have, you know, the- the- the y label is false,",
    "start": "1073520",
    "end": "1079385"
  },
  {
    "text": "but you predicted y to be true, right? Similarly, the type-2 error uh, is- is commonly called um, uh,",
    "start": "1079385",
    "end": "1085745"
  },
  {
    "text": "is another name for false negatives, where the- the true uh, uh, uh, condition is- is -is true- is- um,",
    "start": "1085745",
    "end": "1094610"
  },
  {
    "text": "is 1, you know, y equals 1, but you predict y equals 0, right? Uh, this is, um, um, you know,",
    "start": "1094610",
    "end": "1100580"
  },
  {
    "text": "I found this on- on -on Twitter on- um, on- on someone's, uh, Twitter page, but I could not find the source of, um,",
    "start": "1100580",
    "end": "1108830"
  },
  {
    "text": "source of the image to- to cite it but, you know, it's- it's- um, it's a funny image, right?",
    "start": "1108830",
    "end": "1114300"
  },
  {
    "text": "Um, so- so, uh, back to- back to the slide, the larger,",
    "start": "1116470",
    "end": "1122164"
  },
  {
    "text": "uh, takeaway is that, uh, depending on the kind of errors you make, the- the kind of,",
    "start": "1122165",
    "end": "1127895"
  },
  {
    "text": "um, impact that false positives and false negatives have can be very, very different, right?",
    "start": "1127895",
    "end": "1133430"
  },
  {
    "text": "So if you were to, for example, prescribe some kind of a medication, uh, then,",
    "start": "1133430",
    "end": "1139595"
  },
  {
    "text": "you know, uh, false positives and false negatives can have different kinds of effects, you know, depending on whether the, um, um, so for example,",
    "start": "1139595",
    "end": "1146810"
  },
  {
    "text": "if you want to prescribe a medication based on whether a person has a disease or not and if that, uh, uh, medication has no adverse side effects when given to an ordinary person,",
    "start": "1146810",
    "end": "1156425"
  },
  {
    "text": "then you are, uh, much more likely to worry about false um, um, uh,",
    "start": "1156425",
    "end": "1162350"
  },
  {
    "text": "false negatives because you don't want to lose out on, uh, people from getting that medication.",
    "start": "1162350",
    "end": "1167630"
  },
  {
    "text": "So you want, uh, all the people who have the patients who have the con- condition to be predicted so that you can give them medication, um.",
    "start": "1167630",
    "end": "1175210"
  },
  {
    "text": "[NOISE] Um, another metric is accuracy, right?",
    "start": "1175210",
    "end": "1183845"
  },
  {
    "start": "1178000",
    "end": "1229000"
  },
  {
    "text": "So accuracy is basically what is the fraction of all the examples, no matter what class they were that we predicted right,",
    "start": "1183845",
    "end": "1190850"
  },
  {
    "text": "that you got them right, right? So accuracy is generally, you know, kind of the trace of the matrix divided by,",
    "start": "1190850",
    "end": "1196615"
  },
  {
    "text": "you know, the sum of all the elements. And, uh, if you want to, uh, optimize your model for accuracy, then, uh,",
    "start": "1196615",
    "end": "1204875"
  },
  {
    "text": "it corresponds to actually using something that's called the 0, 1 loss, right, 0, 1 loss, uh, refers to, you know, um,",
    "start": "1204875",
    "end": "1211804"
  },
  {
    "text": "you have a loss of 1 if you got the answer, er, correct and, um,",
    "start": "1211805",
    "end": "1217640"
  },
  {
    "text": "you get- you get a 0 if you got it wrong, right? So optimizing for 0, 1 loss, which is very hard to do in practice because it is not a differentiable loss,",
    "start": "1217640",
    "end": "1224900"
  },
  {
    "text": "um, is- is, um, that's accuracy. Now we- um, there is,",
    "start": "1224900",
    "end": "1232024"
  },
  {
    "start": "1229000",
    "end": "1395000"
  },
  {
    "text": "um, another metric that's very important called precision. Accuracy and precision in our day to day language,",
    "start": "1232025",
    "end": "1239510"
  },
  {
    "text": "we- we might kind of use them interchangeably, uh, whether we are accurate or precise. But, um, in- in a- in a statistical setting or in a machine learning setting,",
    "start": "1239510",
    "end": "1247610"
  },
  {
    "text": "the two have very, very different meanings, right. So accuracy is what fraction of all the examples that we got right,",
    "start": "1247610",
    "end": "1253385"
  },
  {
    "text": "whether they were- uh, whether they were- they were, you know, uh, positives or negatives doesn't matter whether we predicted them right or not,",
    "start": "1253385",
    "end": "1260480"
  },
  {
    "text": "that's what accuracy measures. However, precision means among the examples that we predicted to be positive,",
    "start": "1260480",
    "end": "1267535"
  },
  {
    "text": "what fraction of them were actually positive, right? So in precision, we are kind of limited to the top half of the confusion matrix, right?",
    "start": "1267535",
    "end": "1276424"
  },
  {
    "text": "So the top half of the confusion matrix is categorizing all the examples that were predicted to be positive,",
    "start": "1276425",
    "end": "1283130"
  },
  {
    "text": "and now, uh, precision measures what fraction of the predicted examples,",
    "start": "1283130",
    "end": "1288560"
  },
  {
    "text": "uh, were actually positive. And there's another name for precision,",
    "start": "1288560",
    "end": "1293965"
  },
  {
    "text": "it's also called PPV. So PPV stands for [BACKGROUND] positive predictive value.",
    "start": "1293965",
    "end": "1300110"
  },
  {
    "text": "So precision is also called PPV,",
    "start": "1301050",
    "end": "1305840"
  },
  {
    "text": "positive predictive value.",
    "start": "1306510",
    "end": "1315650"
  },
  {
    "text": "Yes. Question. [inaudible]",
    "start": "1317310",
    "end": "1331865"
  },
  {
    "text": "Yeah. So the question is, uh, if you- if you worry a lot about false positives, you know, how do you kind of quantify that into you- into your loss function?",
    "start": "1331865",
    "end": "1339500"
  },
  {
    "text": "Because your loss function is just a loss function. Uh, we'll touch upon that in- in some of the future slides,",
    "start": "1339500",
    "end": "1345260"
  },
  {
    "text": "um, uh, but- but that's- that's a good question, that's- that's the kind of questions you should be thinking about because",
    "start": "1345260",
    "end": "1350434"
  },
  {
    "text": "a loss function is- is- you know, um, is, uh, is our loss function kind of asymmetrically treating, uh,",
    "start": "1350435",
    "end": "1357049"
  },
  {
    "text": "false positive and false negatives, uh, if not, shoot it, how do we do it? You know, uh, that's- that's definitely,",
    "start": "1357050",
    "end": "1362285"
  },
  {
    "text": "uh, a very good question. So, uh, that's precision. So precision is among the examples that we predicted to be positive,",
    "start": "1362285",
    "end": "1370160"
  },
  {
    "text": "you know, what- what fraction of them were actually positive? And, um, you know, um, you can- you can see that, you know, the- the,",
    "start": "1370160",
    "end": "1377360"
  },
  {
    "text": "uh, precision is, um, all these metrics that we've seen so far, you know, true positive,",
    "start": "1377360",
    "end": "1383510"
  },
  {
    "text": "true negative, false-positive, false-negative, accuracy, precision, all- these are all metrics that actually depend on the threshold that we choose, right?",
    "start": "1383510",
    "end": "1390769"
  },
  {
    "text": "Um, so these are all threshold, um, um, uh, sensitive metrics [NOISE].",
    "start": "1390770",
    "end": "1395990"
  },
  {
    "start": "1395000",
    "end": "1702000"
  },
  {
    "text": "And similarly, there is, uh, something called sensitivity. Sensitivity is also called recall. All right? So precis- recall is also called sensitivity.",
    "start": "1395990",
    "end": "1407670"
  },
  {
    "text": "So in recall, the, um, what recall measures is,",
    "start": "1410670",
    "end": "1416304"
  },
  {
    "text": "if we were to use this classifier in para- in- in, uh, in deployment, what fraction of all the actual positives are we going to recover?",
    "start": "1416305",
    "end": "1426890"
  },
  {
    "text": "All right. So if- if- if, uh, um, we- we, uh, let say there are, you know, um,",
    "start": "1426890",
    "end": "1433054"
  },
  {
    "text": "uh, 100 patients who are, uh, out there and some fraction of them, say half of them have a certain condition.",
    "start": "1433055",
    "end": "1439850"
  },
  {
    "text": "We run all the patients through our model and our model is going to predict, you know, true or fal- you know, predict positive or negative for each example or each patient.",
    "start": "1439850",
    "end": "1448385"
  },
  {
    "text": "And finally if we, um, if- if we, um, um, look at all the patients who actually had the condition,",
    "start": "1448385",
    "end": "1456095"
  },
  {
    "text": "what fraction of those patients did our model predict as- as, um, positive, right? And this is called sensitivity.",
    "start": "1456095",
    "end": "1462200"
  },
  {
    "text": "So this- the- the term sensitivity kind of, um, uh, comes from, I think it comes from epidemiology where, you know,",
    "start": "1462200",
    "end": "1468890"
  },
  {
    "text": "how sensitive is this test for detecting a certain condition, right? That's- that's exactly what this is measuring.",
    "start": "1468890",
    "end": "1474545"
  },
  {
    "text": "So, um, in this case, say, uh, there were a total of ten,",
    "start": "1474545",
    "end": "1479570"
  },
  {
    "text": "uh, positive exa- positive examples. I'm- I'm counting, uh, the, um, the examples on the left half of the confusion matrix and sensitivity is measuring,",
    "start": "1479570",
    "end": "1490010"
  },
  {
    "text": "you know, the top number, 9, divided by nine plus one. So like, you know, the sensitivity here is- is",
    "start": "1490010",
    "end": "1495935"
  },
  {
    "text": "90% because we were able to correctly, um, or we were able to recover nine out of the,",
    "start": "1495935",
    "end": "1502820"
  },
  {
    "text": "uh, uh, ten positive, uh, uh, positive examples. So one thing to, um,",
    "start": "1502820",
    "end": "1509000"
  },
  {
    "text": "to note here is that sensitivity is kind of staying agnostic to everything on the right, right?",
    "start": "1509000",
    "end": "1515690"
  },
  {
    "text": "So we're only focused on what fraction of patients were,",
    "start": "1515690",
    "end": "1521524"
  },
  {
    "text": "uh, or- or what fraction of examples that were actually positive. So we're conditioning on just the positive examples and checking,",
    "start": "1521525",
    "end": "1529520"
  },
  {
    "text": "you know, what fraction of them we were able to recover, right? Um, this is- this is completely agnostic to what",
    "start": "1529520",
    "end": "1535400"
  },
  {
    "text": "happened to the examples that were- that had a ground truth of negative. Yet we're- we're just ignoring that- that- that half of,",
    "start": "1535400",
    "end": "1542420"
  },
  {
    "text": "uh, our data-set completely, right? So, um, the- the- the,",
    "start": "1542420",
    "end": "1549784"
  },
  {
    "text": "um, one- one- one way to- another, uh, uh, way to think of this is, if you want to have perfect recall, right?",
    "start": "1549785",
    "end": "1559790"
  },
  {
    "text": "If you want to have 100% recall, then no matter how good or bad your model is,",
    "start": "1559790",
    "end": "1566315"
  },
  {
    "text": "just choose a threshold of- of 0, okay? If- if you choose a threshold of 0,",
    "start": "1566315",
    "end": "1571610"
  },
  {
    "text": "no matter how good or bad your model is, you will get 100% recall, right? And similarly, if you want to- in the- in the previous example over here,",
    "start": "1571610",
    "end": "1579335"
  },
  {
    "text": "um, this was about precision. If you want to have a model that is 100% precise, right?",
    "start": "1579335",
    "end": "1585695"
  },
  {
    "text": "Then all that you want to do is, uh, choose the threshold such that, um,",
    "start": "1585695",
    "end": "1591440"
  },
  {
    "text": "choose a threshold such that the threshold just classifies the top one example to be positive and you will get 100% recall, right?",
    "start": "1591440",
    "end": "1601235"
  },
  {
    "text": "So viewing these metrics in isolation generally does not, um,",
    "start": "1601235",
    "end": "1606320"
  },
  {
    "text": "kind of capture the- the, uh, true performance of a model [NOISE].",
    "start": "1606320",
    "end": "1611630"
  },
  {
    "text": "Right? So, uh, so- so, uh, uh, to- to, um,",
    "start": "1611630",
    "end": "1618350"
  },
  {
    "text": "so what we'll- we will see here shortly that most of the times we are trying to balance off, you know, uh,",
    "start": "1618350",
    "end": "1624035"
  },
  {
    "text": "precision and recall in some way, uh, because we don't want- we don't want our metric to be, you know, blindly placed all the way at the bottom to maximize a recall.",
    "start": "1624035",
    "end": "1632495"
  },
  {
    "text": "Similarly, we don't want, uh, uh, our threshold to be blindly placed all the way at the top to just get good, uh, uh, precision numbers. Yes, question.",
    "start": "1632495",
    "end": "1639020"
  },
  {
    "text": "[inaudible]",
    "start": "1639020",
    "end": "1651080"
  },
  {
    "text": "So the- so the question is, you know, why are we looking at the top half? Uh, we're interested in the left half.",
    "start": "1651080",
    "end": "1656510"
  },
  {
    "text": "Yeah. So the left half of this matrix are, you know, the- the examples that have a ground truth to be positive, right?",
    "start": "1656510",
    "end": "1663770"
  },
  {
    "text": "But our model, when it's making a prediction, it does not know, you know, what- what, uh, uh, uh, what the ground truth is and it's going to assign some kind of a probability.",
    "start": "1663770",
    "end": "1671705"
  },
  {
    "text": "And depending on what threshold we choose, right? So in- in this case threshold was 0.5, you know,",
    "start": "1671705",
    "end": "1677390"
  },
  {
    "text": "we classify everything that the model classified, uh, you know, uh, as above- above, um,",
    "start": "1677390",
    "end": "1682640"
  },
  {
    "text": "0.5 to be- we predict them to be positive, right? We don't know what the ground truth is. Yes, we are interested in the left half,",
    "start": "1682640",
    "end": "1689045"
  },
  {
    "text": "but the only way we can classify them is as, you know, top half and bottom half.",
    "start": "1689045",
    "end": "1694380"
  },
  {
    "text": "Good question. So that's- that's, um, recall [NOISE].",
    "start": "1694630",
    "end": "1703160"
  },
  {
    "start": "1702000",
    "end": "2028000"
  },
  {
    "text": "Similarly, there's something called negative recall. So negative recall is, um, basically doing the same, um, uh,",
    "start": "1703160",
    "end": "1709684"
  },
  {
    "text": "same exercise as- as a recall, but we do it on the- on the right-hand side of- of this- of the confusion matrix.",
    "start": "1709685",
    "end": "1716255"
  },
  {
    "text": "Which means, uh, so negative recall is- is, um, is- is- is calculating of all the examples that had, uh,",
    "start": "1716255",
    "end": "1724430"
  },
  {
    "text": "a true or- or the actual label to be negative, what fraction of that sub-population did",
    "start": "1724430",
    "end": "1730549"
  },
  {
    "text": "the model actually classi- correctly classify as negative, right? And in negative recall,",
    "start": "1730550",
    "end": "1736670"
  },
  {
    "text": "um, just like, uh, uh, you know, positive recall, we kind of stay agnostic to the left half of this picture, right?",
    "start": "1736670",
    "end": "1743645"
  },
  {
    "text": "We don't- we don't, um, we don't- we don't care how correctly or incorrectly the model performed on the,",
    "start": "1743645",
    "end": "1751010"
  },
  {
    "text": "uh, uh, on the ground truth, uh, uh, equals positive. We just look at- we considered the examples on",
    "start": "1751010",
    "end": "1756830"
  },
  {
    "text": "the right-hand- right-hand side and- and see what fraction of the, um, actual examples, uh,",
    "start": "1756830",
    "end": "1762935"
  },
  {
    "text": "actual negative examples to the model correc- uh, correctly classify as negative. And negative recall has another term,",
    "start": "1762935",
    "end": "1770705"
  },
  {
    "text": "specificity [NOISE]. Okay, um.",
    "start": "1770705",
    "end": "1773640"
  },
  {
    "text": "Right? So negative recall is a specificity.",
    "start": "1783730",
    "end": "1788549"
  },
  {
    "text": "And you can start, um, uh, kind of combining these individual scores, uh,",
    "start": "1792820",
    "end": "1798875"
  },
  {
    "text": "into, you know, um, in- in certain ways, for example, there's something called the F score or the F1- F1-score.",
    "start": "1798875",
    "end": "1804320"
  },
  {
    "text": "So F1 score is, uh, you can- you can, uh, so F1-score is basically the,",
    "start": "1804320",
    "end": "1810480"
  },
  {
    "text": "um, um, F1-score is the harmonic mean of- of, uh, precision and recall.",
    "start": "1810700",
    "end": "1816635"
  },
  {
    "text": "So 1 over F1 is 1 over precision plus 1 over recall.",
    "start": "1816635",
    "end": "1824600"
  },
  {
    "text": "Right? So you can combine your precision and recall into a single number called, uh, the F1-score.",
    "start": "1824600",
    "end": "1832220"
  },
  {
    "text": "Similarly, there is, um, uh, something called, uh, just like the F-score, there is something called the G-score,",
    "start": "1832220",
    "end": "1838865"
  },
  {
    "text": "which is the geometric mean of- of, uh, precision and recall. So, uh, so the G-score you have,",
    "start": "1838865",
    "end": "1850445"
  },
  {
    "text": "uh, so you have- the geometric mean, so it is in a logG equals logPrecision plus logRecall.",
    "start": "1850445",
    "end": "1860360"
  },
  {
    "text": "So it's- it's, uh, uh, you know, G is square root of precision [NOISE] times recall and, uh,",
    "start": "1860360",
    "end": "1868820"
  },
  {
    "text": "F-score- F- F-score is therefore, you know, precision times recall times precision plus recall.",
    "start": "1868820",
    "end": "1876620"
  },
  {
    "text": "[inaudible]",
    "start": "1876620",
    "end": "1881925"
  },
  {
    "text": "Oh sorry. No. It's not quite obvious. Sorry.",
    "start": "1881925",
    "end": "1887430"
  },
  {
    "text": "So- so G- so G",
    "start": "1887430",
    "end": "1897600"
  },
  {
    "text": "is- so ignore this G is actually- so this is the correct.",
    "start": "1897600",
    "end": "1902730"
  },
  {
    "text": "It is the geometric mean. So I guess I missed a half over there.",
    "start": "1902730",
    "end": "1907800"
  },
  {
    "text": "Yeah. Next question. [inaudible]",
    "start": "1907800",
    "end": "1916260"
  },
  {
    "text": "Yeah. So you know what- what do these F scores and G scores kind of represent? So as we saw before,",
    "start": "1916260",
    "end": "1923230"
  },
  {
    "text": "just looking at precision or just looking at recall is- is insufficient because you can",
    "start": "1923300",
    "end": "1929040"
  },
  {
    "text": "just fool the model by having 100% recall by setting the threshold all the way to 0, right? But if you were to do that,",
    "start": "1929040",
    "end": "1935220"
  },
  {
    "text": "you will take a hit on precision. Right? And similarly, if you- if you- if you maximize your precision to 100% by setting your threshold all the way at",
    "start": "1935220",
    "end": "1942600"
  },
  {
    "text": "the top so that you just classify that one correct example as positive and- and just ignore everything else.",
    "start": "1942600",
    "end": "1949215"
  },
  {
    "text": "Your precision will be 100% but your recall will be really low because you've recalled only one out of all the positive examples that you have, right?",
    "start": "1949215",
    "end": "1957420"
  },
  {
    "text": "So it is some way to kind of balance the two in some way. So these- the geometric mean and- and harmonic mean",
    "start": "1957420",
    "end": "1965010"
  },
  {
    "text": "are- are- are- are you know kinds of means where if any one of them low- is low,",
    "start": "1965010",
    "end": "1972390"
  },
  {
    "text": "then the entire mean is low, which is not the case with arithmetic mean. So arithmetic mean will just- is- is pretty robust to stay high,",
    "start": "1972390",
    "end": "1980730"
  },
  {
    "text": "but geometric mean will- will just bring down your value to be really low if either one of them is- is low.",
    "start": "1980730",
    "end": "1987120"
  },
  {
    "text": "So these are just two different- you know, two different ways of kind of combining the two- the two metrics in some way.",
    "start": "1987120",
    "end": "1994530"
  },
  {
    "text": "Yeah, it's another way to- to- to say, you know, what is the min of my precision and recall, right?",
    "start": "1994530",
    "end": "2001190"
  },
  {
    "text": "That's what the geometric mean will- is kind of doing. And of course, next question. So why can't we just [inaudible]",
    "start": "2001190",
    "end": "2012830"
  },
  {
    "text": "So the question is why are- why not just focus on accuracy? Why are we doing all the- all these other things, we'll come to that.",
    "start": "2012830",
    "end": "2017990"
  },
  {
    "text": "[inaudible]",
    "start": "2017990",
    "end": "2023090"
  },
  {
    "text": "So- so- so- so the question is, does accuracy capture precision and recall in some way?",
    "start": "2023090",
    "end": "2029130"
  },
  {
    "start": "2028000",
    "end": "2140000"
  },
  {
    "text": "Maybe, maybe not. We'll see- we'll see more- more. Why accuracy is not always the right thing to- to look for.",
    "start": "2029760",
    "end": "2038445"
  },
  {
    "text": "Right? So in- in examples, when- in situations where your data is well-balanced,",
    "start": "2038445",
    "end": "2044600"
  },
  {
    "text": "you have you know an equal number of positives and negatives, then you know accuracy and precision and recall are being more or less kind of similar.",
    "start": "2044600",
    "end": "2051679"
  },
  {
    "text": "But if- if you have class imbalance, let's say you know just 1% of all your examples are positive,",
    "start": "2051680",
    "end": "2058940"
  },
  {
    "text": "right, and 99% of your examples are negative. Then if you have a model that just says no to everything,",
    "start": "2058940",
    "end": "2066200"
  },
  {
    "text": "it'll get 99% accuracy, right? So- so in class imbalance situations",
    "start": "2066200",
    "end": "2072994"
  },
  {
    "text": "this precision and recall kind of metrics are- will be much more robust from,",
    "start": "2072995",
    "end": "2078050"
  },
  {
    "text": "you know, from- from getting fooled whereas accuracy can be easily fooled in low- low- low prevalence scenarios.",
    "start": "2078050",
    "end": "2086190"
  },
  {
    "text": "Right? So that's- that's F-score. And now, so everything that we've seen so far are",
    "start": "2087100",
    "end": "2094339"
  },
  {
    "text": "specific to this chosen threshold, right? So our model did the ranking and then we choose a threshold.",
    "start": "2094340",
    "end": "2102935"
  },
  {
    "text": "And out of this- once we- once we chose this threshold, we got this confusion matrix.",
    "start": "2102935",
    "end": "2108109"
  },
  {
    "text": "And from the confusion matrix we extracted all these threshold-specific matrix.",
    "start": "2108110",
    "end": "2115070"
  },
  {
    "text": "Now, what happens if we change the threshold from 0.5? Let's say we move it up to 0.6.",
    "start": "2115070",
    "end": "2122165"
  },
  {
    "text": "If you were to choose a different threshold, then you know, some of these numbers changed.",
    "start": "2122165",
    "end": "2127790"
  },
  {
    "text": "So the- the- the metrics that changed are highlighted in red. So I'm going to go back one slide for you to compare again, right?",
    "start": "2127790",
    "end": "2135725"
  },
  {
    "text": "So this was with- with threshold equals 0.5. With 0.6, we see that the true positive changed,",
    "start": "2135725",
    "end": "2142325"
  },
  {
    "start": "2140000",
    "end": "2765000"
  },
  {
    "text": "the true negative did not change, similarly the false-negative changed, the accuracy changed, the specificity did not change.",
    "start": "2142325",
    "end": "2148055"
  },
  {
    "text": "So some of them changed, some of them did not change, right? And now- now you know similarly we can- we",
    "start": "2148055",
    "end": "2158150"
  },
  {
    "text": "can repeat this exercise by trying- you know trying as many different thresholds as possible.",
    "start": "2158150",
    "end": "2165650"
  },
  {
    "text": "So first of all, how many effective thresholds are in this- are there in this situation,",
    "start": "2165650",
    "end": "2172265"
  },
  {
    "text": "for example, you know choosing the threshold here versus slightly below changes nothing, right?",
    "start": "2172265",
    "end": "2179825"
  },
  {
    "text": "No example was- was classification of- no example changed. So in- in this kind of situation,",
    "start": "2179825",
    "end": "2188030"
  },
  {
    "text": "the number of effective thresholds we have is 1 plus the number of examples.",
    "start": "2188030",
    "end": "2193400"
  },
  {
    "text": "Right? So that gives us all the- all the possible different thresholds that are somewhat meaningful.",
    "start": "2193400",
    "end": "2201605"
  },
  {
    "text": "And so what we can do is basically repeat this exercise of not just checking it at 0.5 and 0.6,",
    "start": "2201605",
    "end": "2209660"
  },
  {
    "text": "but check it with all possible thresholds. Right? So this is the same ranking.",
    "start": "2209660",
    "end": "2215885"
  },
  {
    "text": "The ranking has not changed. Ranking is a property of the model, but the threshold is something that we choose to apply after the fact, right?",
    "start": "2215885",
    "end": "2225049"
  },
  {
    "text": "And for each particular value of the threshold corresponds to each row in this table on the right side, right?",
    "start": "2225050",
    "end": "2233494"
  },
  {
    "text": "And as we- as we change the threshold, we get different values of you know true positive true negative accuracy,",
    "start": "2233495",
    "end": "2239450"
  },
  {
    "text": "precision, recall, and so on. Right? Now, as I said, each row corresponds to each effective threshold,",
    "start": "2239450",
    "end": "2247970"
  },
  {
    "text": "and the table itself is a property of our model. So if our model was- were- if we were to take",
    "start": "2247970",
    "end": "2254300"
  },
  {
    "text": "a different model and you know take the predictions from the other model, the ranking would probably be very different.",
    "start": "2254300",
    "end": "2261065"
  },
  {
    "text": "Right? And- and so this- this entire table is kind of specific to the model that made the predictions.",
    "start": "2261065",
    "end": "2269075"
  },
  {
    "text": "But each rows within this table depends on what threshold that we chose.",
    "start": "2269075",
    "end": "2274230"
  },
  {
    "text": "And you can make a few more observations in- in- in this table.",
    "start": "2274300",
    "end": "2279395"
  },
  {
    "text": "So first of all, many of these columns are monotonic. So for example, true positives starts at 0 and monotonically increases to 10,",
    "start": "2279395",
    "end": "2289565"
  },
  {
    "text": "true negative similarly monotonically decreases. Similarly, false-positive monotonically increases,",
    "start": "2289565",
    "end": "2296000"
  },
  {
    "text": "false-negative monotonically decreases. Recall, monotonically increases, starts at 0and ends at 1.",
    "start": "2296000",
    "end": "2304490"
  },
  {
    "text": "And specificity monotonically decreases,  starts at 1 and ends at 0.",
    "start": "2304490",
    "end": "2311119"
  },
  {
    "text": "However, there are these other metrics, for example, F1 score, right?",
    "start": "2311120",
    "end": "2317825"
  },
  {
    "text": "There is- there is- there is no strict ordering. It kind of reaches a maximum somewhere at around threshold of 0.4 or something.",
    "start": "2317825",
    "end": "2327200"
  },
  {
    "text": "Similarly, accuracy, accuracy also kind of reaches a maximum at 0.45. And it- it- it- it reduces as we go to the extremes, right?",
    "start": "2327200",
    "end": "2336154"
  },
  {
    "text": "So some of them have this- this kind of monotonic relation where you know,",
    "start": "2336155",
    "end": "2341735"
  },
  {
    "text": "we- we increase in one direction or decrease in the other direction. And precision is kind of interesting because in- in precision,",
    "start": "2341735",
    "end": "2353539"
  },
  {
    "text": "we kind of keep going up and down, right? So at precision, we start at 1, come down to 0.6, go up to 0.75.",
    "start": "2353540",
    "end": "2360710"
  },
  {
    "text": "0.8, 0.83 and then come down to 0.71. So it's kind of- just kind of going up and down.",
    "start": "2360710",
    "end": "2367475"
  },
  {
    "text": "And so what we want to do is try to capture the trade-off between these columns in some way, right?",
    "start": "2367475",
    "end": "2377630"
  },
  {
    "text": "So as we saw before, if you want to just maximize recall, just- just set your threshold to 0.",
    "start": "2377630",
    "end": "2384755"
  },
  {
    "text": "If you want to maximize your precision, maybe just set it at- at- at 1 or just below 1.",
    "start": "2384755",
    "end": "2391205"
  },
  {
    "text": "But instead what we want do is not just look at how the model performs at some specified threshold,",
    "start": "2391205",
    "end": "2398900"
  },
  {
    "text": "but summarize these- these- these entire columns in some way.",
    "start": "2398900",
    "end": "2403970"
  },
  {
    "text": "Right? And- so that's- that's the question. How do we summarize the trade-off between say,",
    "start": "2403970",
    "end": "2409115"
  },
  {
    "text": "precision and recall or specificity and sensitivity. Right? And that's where things like the ROC curves come into picture, right?",
    "start": "2409115",
    "end": "2417290"
  },
  {
    "text": "And the ROC curve, it's called the Receiver Operator Characteristic curve. It is a trade-off between specificity and sensitivity, right?",
    "start": "2417290",
    "end": "2426020"
  },
  {
    "text": "So sometimes you may see the ROC curve to be flipped in some books or some- some articles.",
    "start": "2426020",
    "end": "2433340"
  },
  {
    "text": "You'll see it flipped where on the- on the x-axis you have 1 minus sensitivity but it's- it's basically the same, right?",
    "start": "2433340",
    "end": "2442355"
  },
  {
    "text": "And so the ROC curve is this- is the plot that you get by scanning the threshold.",
    "start": "2442355",
    "end": "2449585"
  },
  {
    "text": "So I'm going to go back one slide. So- two slides rather. So as we scan the threshold from 0-1.",
    "start": "2449585",
    "end": "2458194"
  },
  {
    "text": "And for each threshold, we read off the sensitivity and specificity and plot",
    "start": "2458195",
    "end": "2464270"
  },
  {
    "text": "that point on a graph of sensitivity versus specificity. And as we plot the different points that we get",
    "start": "2464270",
    "end": "2471550"
  },
  {
    "text": "by scanning different thresholds you know connect those points as well. And you get this ROC curve.",
    "start": "2471550",
    "end": "2477780"
  },
  {
    "text": "Right? So the ROC curve is- is giving you the trade-off between sensitivity and specificity as we scan the threshold.",
    "start": "2477780",
    "end": "2484160"
  },
  {
    "text": "So for example, when the threshold is- is- is- set at 1.0.",
    "start": "2484160",
    "end": "2490930"
  },
  {
    "text": "So you know, you set the threshold all the way at the top, right, your recall will be 0, right? Your sensitivity will be- your sensitive- you",
    "start": "2490930",
    "end": "2500920"
  },
  {
    "text": "specify- when- when you- when you set your threshold all the way at the top,",
    "start": "2500920",
    "end": "2507619"
  },
  {
    "text": "at 1 that corresponds to the top left- top-left corner, your sensitivity will be 0,",
    "start": "2507620",
    "end": "2514174"
  },
  {
    "text": "but specificity will be, you know, will be maximum. And as you keep bri- you know,",
    "start": "2514175",
    "end": "2520645"
  },
  {
    "text": "bring- as you keep scanning your threshold down towards 0,",
    "start": "2520645",
    "end": "2525955"
  },
  {
    "text": "you start exploring these- all these other points. Right? So one way to think of the- the ROC curve is- so another way",
    "start": "2525955",
    "end": "2537260"
  },
  {
    "text": "to- to think of it is- so create a grid.",
    "start": "2537260",
    "end": "2546300"
  },
  {
    "text": "Create a grid well, uh, you have- you- you chop up the horizontal axis into equal-sized,",
    "start": "2549250",
    "end": "2558530"
  },
  {
    "text": "uh, ranges, into as many number of examples you have. So if you have, say, a 1, 2, 3,",
    "start": "2558530",
    "end": "2565460"
  },
  {
    "text": "4, 5, 6, 7, 8, 9, 10, 11, 12, so let's say you have 12 positive examples, right?",
    "start": "2565460",
    "end": "2573830"
  },
  {
    "text": "And chop up the vertical axis into a number of negative examples that you have, so 1, 2, 3, 4, 5, 6,",
    "start": "2573830",
    "end": "2580190"
  },
  {
    "text": "7, 8, 9, 10, 11, 12, 13. So 13 negative examples. Examples, right?",
    "start": "2580190",
    "end": "2589325"
  },
  {
    "text": "And now your model gives you- you know, some kind of an ordering.",
    "start": "2589325",
    "end": "2594380"
  },
  {
    "text": "Right? So your ordering looks something like this, right? So positive, positive, negative,",
    "start": "2594380",
    "end": "2604430"
  },
  {
    "text": "positive, positive and so on. So it's going to give you some, some kind of an ordering. Now, one way to think of this ROC curve is start from the top left, right?",
    "start": "2604430",
    "end": "2617060"
  },
  {
    "text": "And inspect one example at a time, right? If you encounter, uh,",
    "start": "2617060",
    "end": "2623450"
  },
  {
    "text": "positive examples, move right, if you encounter a negative example, move down.",
    "start": "2623450",
    "end": "2629580"
  },
  {
    "text": "Right? So first is a positive, we move right, another is a positive we move right,",
    "start": "2629580",
    "end": "2635065"
  },
  {
    "text": "next we've got an neg- a negative example, we move down, then we got a positive, move right, positive, move right, et cetera.",
    "start": "2635065",
    "end": "2641250"
  },
  {
    "text": "And because we have, uh- uh, we can only make, you know, 12 number of- you know,",
    "start": "2641250",
    "end": "2647359"
  },
  {
    "text": "exactly 12 number of right moves, and 13 number of down moves, no matter what order you are, you always start here, and you always end here.",
    "start": "2647360",
    "end": "2654635"
  },
  {
    "text": "Right? And if all your positive examples are at the top, then your curve will- will look like this.",
    "start": "2654635",
    "end": "2662585"
  },
  {
    "text": "So it will- it will turn towards the top right corner if most of the positive examples are at the top.",
    "start": "2662585",
    "end": "2668300"
  },
  {
    "text": "Right? And so, uh, this intuition gives rise to this derived metric from a ROC curve called the area under the ROC curve.",
    "start": "2668300",
    "end": "2676070"
  },
  {
    "text": "Right? So the area under the ROC curve, is basically this area, and this area under the ROC curve will be high,",
    "start": "2676070",
    "end": "2683674"
  },
  {
    "text": "if most of the positive examples or if all of the positive examples are the ones that you encounter first as you start scanning down.",
    "start": "2683675",
    "end": "2690755"
  },
  {
    "text": "Right? Because, you know, positive example will just keep you moving to the right,",
    "start": "2690755",
    "end": "2695900"
  },
  {
    "text": "and another observation you can- you can- you can, uh, make is that ROC curves always have this- you know,",
    "start": "2695900",
    "end": "2703055"
  },
  {
    "text": "can always be broken down into these horiz- horizontal, and vertical um- um- um,",
    "start": "2703055",
    "end": "2708770"
  },
  {
    "text": "segments or chunks, and each- each chunk corresponds to",
    "start": "2708770",
    "end": "2714230"
  },
  {
    "text": "no- one segment of consecutively occurring positive group or a negative group.",
    "start": "2714230",
    "end": "2720920"
  },
  {
    "text": "Right? So you keep moving to the right so you get- you get one long horizontal chunk if you have lot- if you encounter lots of positive examples,",
    "start": "2720920",
    "end": "2729125"
  },
  {
    "text": "and then once you- once you switch over to- you know, some negative examples, you kind of get a kink here,",
    "start": "2729125",
    "end": "2734150"
  },
  {
    "text": "and you start moving down, and so on, okay? So this is- this is, uh, the ROC curve, and the area under the ROC curve is generally a useful metric to- to,",
    "start": "2734150",
    "end": "2744410"
  },
  {
    "text": "uh, to check how well our model is- is kind of classifying. If- if- if it does a good job of placing all the positives up, and negatives down,",
    "start": "2744410",
    "end": "2752420"
  },
  {
    "text": "then the area under the ROC curve will be high, right? Any questions on this?",
    "start": "2752420",
    "end": "2758370"
  },
  {
    "text": "Good. So a few um, a few more observations on ROC curves.",
    "start": "2758620",
    "end": "2765410"
  },
  {
    "start": "2765000",
    "end": "3242000"
  },
  {
    "text": "So, ah, if you have a model that randomly assigns probabilities to your examples,",
    "start": "2765410",
    "end": "2774545"
  },
  {
    "text": "then the ROC curve will fall along this diagonal. Right? So if you, if,",
    "start": "2774545",
    "end": "2780230"
  },
  {
    "text": "if- let's, let's say you- you, uh, you have a model, you get an example,",
    "start": "2780230",
    "end": "2785345"
  },
  {
    "text": "and your- your, uh, your model, instead of predicting some kind of a probability, it just samples a probability between zero, and one,",
    "start": "2785345",
    "end": "2793565"
  },
  {
    "text": "and a uniform distribution zero and one [inaudible] our probability, then the ROC curve will- will- will exactly coincide with this diagonal line.",
    "start": "2793565",
    "end": "2802775"
  },
  {
    "text": "Of course, uh, you know, this diagonal line looks smooth, you know, it assumes you have lots,",
    "start": "2802775",
    "end": "2807920"
  },
  {
    "text": "and lots of examples, otherwise it's gonna be, you know, uh, a diagonal line that, you know, looks like this, right?",
    "start": "2807920",
    "end": "2816650"
  },
  {
    "text": "So your- your, uh- uh, random guesser will have an ROC of 0.5, right?",
    "start": "2816650",
    "end": "2822635"
  },
  {
    "text": "So- so that's like a low bar for what you should be aiming for with ROC.",
    "start": "2822635",
    "end": "2828500"
  },
  {
    "text": "Right? If your- if your, uh, model has an ROC less than 0.5, then your model is basically doing worse than random guessing, right? That's bad.",
    "start": "2828500",
    "end": "2837665"
  },
  {
    "text": "However, you know, there is- there is, um, um - if it is- if it is doing like really low, like if- if your, uh,",
    "start": "2837665",
    "end": "2845240"
  },
  {
    "text": "model has an ROC of 0.1, that is somewhat good news because you can just flip the labels,",
    "start": "2845240",
    "end": "2851045"
  },
  {
    "text": "and you can get a really high ROC, right? So um- um, staying- staying at 0.5,",
    "start": "2851045",
    "end": "2857710"
  },
  {
    "text": "if your ROC is- is 0.5, that's bad, Uh, you want it to be as high as possible, but if it's really low,",
    "start": "2857710",
    "end": "2863530"
  },
  {
    "text": "you can always just flip the labels, and you know, it will perform- perform pretty well.",
    "start": "2863530",
    "end": "2868599"
  },
  {
    "text": "Uh, but of course that is still disturbing you should- you know, definitely investigate why it is predicting low, maybe there's a bug somewhere.",
    "start": "2868600",
    "end": "2875030"
  },
  {
    "text": "But, you know, uh, yeah, so- so- so 0.5 is- is- is- uh, it should be the low bar when you're looking at ROC values.",
    "start": "2875030",
    "end": "2882935"
  },
  {
    "text": "So if you see, um- um, if you see and read some paper and say the ROC was,",
    "start": "2882935",
    "end": "2888215"
  },
  {
    "text": "you know, 0.65, you know, you should not think of the model could have scored 0-1, and achieved 0.65,",
    "start": "2888215",
    "end": "2896660"
  },
  {
    "text": "you should think of know, between, you know six point- 0.65 as a ratio starting from 0.5-1,",
    "start": "2896660",
    "end": "2904490"
  },
  {
    "text": "not from starting from 0-1. Right? So something with an ROC of 0.6 is quite close to random guessing.",
    "start": "2904490",
    "end": "2914310"
  },
  {
    "text": "There's another interpretation of the ROC- the area under the ROC curve. So the other interpretation of the area under the ROC curve is, um, supposing,",
    "start": "2917380",
    "end": "2929000"
  },
  {
    "text": "um, you choose a random example- a random positive example,",
    "start": "2929000",
    "end": "2935330"
  },
  {
    "text": "and then separately you choose a random negative example. Now, what is the probability that this- in this random pair of positive,",
    "start": "2935330",
    "end": "2945170"
  },
  {
    "text": "and negative examples, the positive examples will be ranked higher than the negative example?",
    "start": "2945170",
    "end": "2950660"
  },
  {
    "text": "That probability is exactly equal to the area under the ROC curve, okay?",
    "start": "2950660",
    "end": "2956734"
  },
  {
    "text": "So that's- that's another way to think of the ROC curve. What's the- what's the probability that a random positive example is ranked higher,",
    "start": "2956735",
    "end": "2963230"
  },
  {
    "text": "than a random negative example? Rank means, uh, in this- you know,",
    "start": "2963230",
    "end": "2971735"
  },
  {
    "text": "uh, pick one random positive example, say it was this green dot,",
    "start": "2971735",
    "end": "2977600"
  },
  {
    "text": "pick another random negative example, say it was this gray dot, right? So those random positive,",
    "start": "2977600",
    "end": "2984230"
  },
  {
    "text": "and negative examples get some kind of a score from the model. Were those scores ordered in the correct way?",
    "start": "2984230",
    "end": "2990875"
  },
  {
    "text": "That's what ROC measures. Any- any- any questions?",
    "start": "2990875",
    "end": "2996930"
  },
  {
    "text": "So uh, another thing to note is that the ROC curve",
    "start": "2998320",
    "end": "3003340"
  },
  {
    "text": "is agnostic to the prevalence of your data-set, right? So what that means is if uh,",
    "start": "3003340",
    "end": "3011250"
  },
  {
    "text": "instead of 13 examples, you had 130 examples, right?",
    "start": "3011250",
    "end": "3017110"
  },
  {
    "text": "All it means is the granularity of each of the vertical drop would be smaller, right?",
    "start": "3017110",
    "end": "3025390"
  },
  {
    "text": "But roughly speaking, you will still get a similar area under the curve, right?",
    "start": "3025390",
    "end": "3031119"
  },
  {
    "text": "It- it- it would just be a little more fine-grained, but the area under the curve will be pretty much the same.",
    "start": "3031120",
    "end": "3037330"
  },
  {
    "text": "So the AU ROC is agnostic to the prevalence of- of the data-set.",
    "start": "3037330",
    "end": "3043420"
  },
  {
    "text": "That's- that's- uh, that's something to, you know, keep in mind when you're evaluating models uh,",
    "start": "3043420",
    "end": "3048835"
  },
  {
    "text": "because that's both a strength and weakness of the ROC, uh, ROC curve. That the ROC curve um,",
    "start": "3048835",
    "end": "3055720"
  },
  {
    "text": "if- if- if you measure your um, if the ROC curve that is reported to you was",
    "start": "3055720",
    "end": "3060880"
  },
  {
    "text": "measured on a data-set that had equal prevalence, but uh, let's say in the real world the um,",
    "start": "3060880",
    "end": "3066970"
  },
  {
    "text": "the- the- the true prevalence is- is very different um, then you know that the ROC- the ROC performance in the real world will still be the same.",
    "start": "3066970",
    "end": "3078640"
  },
  {
    "text": "So that's the good part of it. But the bad part of it is probably the ROC curve is measuring the wrong thing, right?",
    "start": "3078640",
    "end": "3086050"
  },
  {
    "text": "If- if, if it is- if it is- um, if it's-if the- if what it reports is agnostic to the um,",
    "start": "3086050",
    "end": "3092680"
  },
  {
    "text": "- to the prevalence, then maybe what- what er, ROC is measuring is- is not the right metric.",
    "start": "3092680",
    "end": "3098035"
  },
  {
    "text": "Okay? And when- we'll see some of that er, soon. All right? So uh, that's ROC curves.",
    "start": "3098035",
    "end": "3106075"
  },
  {
    "text": "So there's this um- so the ROC curve was uh, - was the trade off between um, sensitivity and specificity.",
    "start": "3106075",
    "end": "3114295"
  },
  {
    "text": "The precision-recall curve is um, you know, is- is the trade-off between um,",
    "start": "3114295",
    "end": "3120730"
  },
  {
    "text": "precision and recall, right? Uh, it's- it's uh, plotted in the same-, uh,",
    "start": "3120730",
    "end": "3126430"
  },
  {
    "text": "in a similar way as the ROC curve, right? You start with the threshold at 0.1. Read off the uh, precision and the recall values.",
    "start": "3126430",
    "end": "3135864"
  },
  {
    "text": "You get- you get a point and then, you know, scan down the threshold a little bit. Read the new precision and recall values.",
    "start": "3135864",
    "end": "3142615"
  },
  {
    "text": "That's the next point, you know, connected by a line. Next, bring down the threshold some more. You get a new precision and recall value plotted and connected by a line.",
    "start": "3142615",
    "end": "3150055"
  },
  {
    "text": "And finally you get a- a- a full curve that, uh, looks like this, right? And this is called the precision recall curve.",
    "start": "3150055",
    "end": "3156684"
  },
  {
    "text": "A few things to note here again, the uh- the ROC curve was kind of",
    "start": "3156685",
    "end": "3162579"
  },
  {
    "text": "monotonic in the sense it could never go back up, right? You would- it would either go to the right or come down to the right or come down.",
    "start": "3162580",
    "end": "3171490"
  },
  {
    "text": "It could never go back up whereas the precision recall curve can sometimes go back up.",
    "start": "3171490",
    "end": "3177369"
  },
  {
    "text": "And the reason for that is in um, what we're measuring is uh,",
    "start": "3177370",
    "end": "3182860"
  },
  {
    "text": "a trade-off between precision and recall. So let's imagine what happens if we were to start over here,",
    "start": "3182860",
    "end": "3188470"
  },
  {
    "text": "right? Bring the threshold down. We got one example, and that one example was positive so the precision is 100%,",
    "start": "3188470",
    "end": "3197710"
  },
  {
    "text": "but the recall was, you know, 0.1. So by- by, uh, we come to this point basically,",
    "start": "3197710",
    "end": "3203470"
  },
  {
    "text": "where the precision is, you know, 100% but the recall is 0.1. Then let's say we bring the threshold down one more level.",
    "start": "3203470",
    "end": "3211900"
  },
  {
    "text": "The thre- the precision is still at 100%. The recall has increased to- to you know, um,",
    "start": "3211900",
    "end": "3218740"
  },
  {
    "text": "0.2 because we- we recovered 2 out of the 10 examples. And then let's say we encountered a third example,",
    "start": "3218740",
    "end": "3226315"
  },
  {
    "text": "a third that was negative, right? So recall has not improved at all because you've still recovered only 2 out of the 10 examples.",
    "start": "3226315",
    "end": "3232990"
  },
  {
    "text": "But the precision drops from 2 out of 2 to 2 out of 3, right? So the drops are always vertical because we come down in precision.",
    "start": "3232990",
    "end": "3242859"
  },
  {
    "start": "3242000",
    "end": "3545000"
  },
  {
    "text": "Whenever we encounter a new example, the precision will come down,",
    "start": "3242860",
    "end": "3248290"
  },
  {
    "text": "but the recall will stay the same because the number of positive examples that we've recovered does- you know,",
    "start": "3248290",
    "end": "3253420"
  },
  {
    "text": "does not- does not change if you encounter a new negative example. So the precision recall curve has these vertical drops that",
    "start": "3253420",
    "end": "3261040"
  },
  {
    "text": "correspond to encountering negative examples in our scan, right? But once we are here,",
    "start": "3261040",
    "end": "3267624"
  },
  {
    "text": "let's- let's say we start encountering positive examples again. Then- then what we see is um- so we go from 2 out of 3,",
    "start": "3267624",
    "end": "3277915"
  },
  {
    "text": "to 3 out of 4, to 4 out of 5. So this is like a gradual increase as we encounter more positive examples.",
    "start": "3277915",
    "end": "3285579"
  },
  {
    "text": "So the precision recall curve will um- will have these, you know pretty,",
    "start": "3285580",
    "end": "3291175"
  },
  {
    "text": "uh- uh- uh pretty standard pattern of having vertical drops and slow climbs, vertical drops and slow climbs.",
    "start": "3291175",
    "end": "3297385"
  },
  {
    "text": "Whereas the ROC curve will have horizontal, vertical, horizontal, vertical, uh,-uh, movements.",
    "start": "3297385",
    "end": "3304765"
  },
  {
    "text": "Okay? And just like the ROC curve, you can um- you can also measure the area under the precision-recall curve, right?",
    "start": "3304765",
    "end": "3312700"
  },
  {
    "text": "And one- one kind of uh- uh - another observation that you can make is that while the ROC curve always started at",
    "start": "3312700",
    "end": "3320020"
  },
  {
    "text": "the top-left corner and ended at the bottom right corner, the precision recall curve,",
    "start": "3320020",
    "end": "3325285"
  },
  {
    "text": "when it reaches 100% recall, right? When you- when you take the - the threshold all the way to 0, your precision,",
    "start": "3325285",
    "end": "3334420"
  },
  {
    "text": "when you- you effectively, you know, uh- uh predicted everything to be positive,",
    "start": "3334420",
    "end": "3340315"
  },
  {
    "text": "your precision will be exactly equal to your prevalence, right? So the precision recall curve will terminate at- at the- at- at the right-hand side,",
    "start": "3340315",
    "end": "3350935"
  },
  {
    "text": "at the level equal to the prevalence. So in this case, prevalence was 0.5 ish or yeah, somewhere- somewhere there.",
    "start": "3350935",
    "end": "3358195"
  },
  {
    "text": "So it ends here at prevalence equals to 0.5. That also means that,",
    "start": "3358195",
    "end": "3364435"
  },
  {
    "text": "you know, there is this, uh, [NOISE] the precision-recall curve, you know, might look something like- like this.",
    "start": "3364435",
    "end": "3372369"
  },
  {
    "text": "[NOISE] So this is always the prevalence level where the precision recall curve will end.",
    "start": "3372370",
    "end": "3379120"
  },
  {
    "text": "It also means that there is this kind of area in a precision recall curve,",
    "start": "3379120",
    "end": "3387655"
  },
  {
    "text": "a certain area in the curve where the precision recall curve just cannot enter, right?",
    "start": "3387655",
    "end": "3395320"
  },
  {
    "text": "No matter how horribly you- you- your modeled ranks your, uh, examples,",
    "start": "3395320",
    "end": "3400975"
  },
  {
    "text": "in the worst-case, let's say your model places all the negatives at the top and all the positives at the bottom.",
    "start": "3400975",
    "end": "3409945"
  },
  {
    "text": "Like the worst possible situation, right? In the worst possible situation,",
    "start": "3409945",
    "end": "3415540"
  },
  {
    "text": "you're gonna keep encountering negatives that gonna, um, you know, kill our precision with zero recall, right?",
    "start": "3415540",
    "end": "3423190"
  },
  {
    "text": "And you're gonna start off climbing the ROC- the- the precision-recall curve starting from the first positive example.",
    "start": "3423190",
    "end": "3430690"
  },
  {
    "text": "And we're going to basically, uh, start at, uh, you know, once the threshold comes here,",
    "start": "3430690",
    "end": "3435700"
  },
  {
    "text": "we're gonna start at 1 over the sum of you know, all the negative examples plus 1.",
    "start": "3435700",
    "end": "3442165"
  },
  {
    "text": "From that, you're gonna um, move on to 2 over sum of all the negative examples plus 2 and so on until we reach you know,",
    "start": "3442165",
    "end": "3451779"
  },
  {
    "text": "a number of positive over number of negative plus number of positive.",
    "start": "3451780",
    "end": "3460030"
  },
  {
    "text": "And it's gonna be the slow climb until the prevalence level. So, no matter how bad your model is, you know,",
    "start": "3460030",
    "end": "3467860"
  },
  {
    "text": "you- you kind of get this amount of area for free, in the area under the precision- recall curve.",
    "start": "3467860",
    "end": "3473125"
  },
  {
    "text": "All right? So um, there is- there is- there are variants of the area under the precision curve that account for this,",
    "start": "3473125",
    "end": "3480145"
  },
  {
    "text": "that kind of, you know, discounts this area and only measures, you know, kind of the area that your model has truly earned.",
    "start": "3480145",
    "end": "3487690"
  },
  {
    "text": "And regarding- disregarding the part you get for free. Um, and that's something uh,",
    "start": "3487690",
    "end": "3493930"
  },
  {
    "text": "you might be- might be kind of, um, that's something worth knowing, right? If your prevalence is very high,",
    "start": "3493930",
    "end": "3499750"
  },
  {
    "text": "then your area under precision recall curve might look very good just because you get a lot of area just for free.",
    "start": "3499750",
    "end": "3508069"
  },
  {
    "text": "All right, moving on. Okay. So let's- let's look at these two,",
    "start": "3509130",
    "end": "3516039"
  },
  {
    "text": "um-, these two models, okay? So um- we, uh,",
    "start": "3516040",
    "end": "3521230"
  },
  {
    "text": "let's- let's suppose there is some data-set and we have two models, model A and model B, right? And the models, uh,",
    "start": "3521230",
    "end": "3527110"
  },
  {
    "text": "models A and B make predictions or assign probability values to every example. And let's say we, you know,",
    "start": "3527110",
    "end": "3533319"
  },
  {
    "text": "um, this is the plot. Um, so model A assigned you know, some negative example, this probability value and this probability value.",
    "start": "3533320",
    "end": "3541674"
  },
  {
    "text": "And these are just the probability values that were assigned by model A and B, these are the um,",
    "start": "3541675",
    "end": "3547224"
  },
  {
    "start": "3545000",
    "end": "6417000"
  },
  {
    "text": "probability values- values that were assigned by model B. Now, by just looking at this,",
    "start": "3547225",
    "end": "3553224"
  },
  {
    "text": "is there a reason to prefer one model over the other? Is one of them better than the other? Or does one of them looked better than the other?",
    "start": "3553225",
    "end": "3559990"
  },
  {
    "text": "B looks better, okay? Anybody thinks A looks better?",
    "start": "3559990",
    "end": "3565945"
  },
  {
    "text": "Why does B look better?",
    "start": "3565945",
    "end": "3568850"
  },
  {
    "text": "So- [NOISE] so the- the answer was-B was confusing er, for a few points,",
    "start": "3575370",
    "end": "3581410"
  },
  {
    "text": "but um, it did well on this region and did well on this region, right?",
    "start": "3581410",
    "end": "3586539"
  },
  {
    "text": "Yeah. So yeah, it might be reasonable to-to, uh, think B does- does better and we should- we should choose B.",
    "start": "3586540",
    "end": "3594325"
  },
  {
    "text": "Now of all the metrics that we have looked so far, which metric would rank B higher than A?",
    "start": "3594325",
    "end": "3599650"
  },
  {
    "text": "None?",
    "start": "3599650",
    "end": "3604010"
  },
  {
    "text": "Any- any other guesses? Right. That's- but- that's the correct answer.",
    "start": "3616910",
    "end": "3622950"
  },
  {
    "text": "None of them will ra- r- rank, uh, uh, um, B higher than A because the ranking of all the points,",
    "start": "3622950",
    "end": "3630735"
  },
  {
    "text": "were the same, right. All the points over here, though they were clustered together,",
    "start": "3630735",
    "end": "3637605"
  },
  {
    "text": "the- if you- if you just look at the ranking position of all the examples, we encounter positives and negatives in the same order.",
    "start": "3637605",
    "end": "3645285"
  },
  {
    "text": "And all the matrix that we've seen so far only cared about the ranking, right.",
    "start": "3645285",
    "end": "3650490"
  },
  {
    "text": "For example, for the area under the precision-recall curve we were scanning the thresholds, but we were only, uh, you know,",
    "start": "3650490",
    "end": "3656910"
  },
  {
    "text": "making moves when we encounter the next positive or negative example, right. We didn't- we didn't care what the threshold value was.",
    "start": "3656910",
    "end": "3664065"
  },
  {
    "text": "We just looked at the order in which the- the positives and negatives were encountered, right.",
    "start": "3664065",
    "end": "3669195"
  },
  {
    "text": "So all the- all the metrics that we've seen so far are ranking based metrics, right.",
    "start": "3669195",
    "end": "3675750"
  },
  {
    "text": "And, uh, as we can see here, here are two situations which qualitatively look very different,",
    "start": "3675750",
    "end": "3682529"
  },
  {
    "text": "but from the point of view of all the metrics we've seen so far, they're exactly the same.",
    "start": "3682530",
    "end": "3687630"
  },
  {
    "text": "Right. And that's- that's, um, um, kind of a motivation for something that's called the log-loss.",
    "start": "3687630",
    "end": "3694590"
  },
  {
    "text": "Okay. The log-loss is, um, it's- it's in fact the- the loss value that we use in logistic regression, right.",
    "start": "3694590",
    "end": "3704700"
  },
  {
    "text": "So the log-loss is, um, think of the log-loss as- [NOISE]",
    "start": "3704700",
    "end": "3721619"
  },
  {
    "text": "so if you remember in logistic regression, if the true label was y and our prediction was y hat, right.",
    "start": "3721620",
    "end": "3728550"
  },
  {
    "text": "The loss that we used was y log y hat",
    "start": "3728550",
    "end": "3735600"
  },
  {
    "text": "plus 1 minus y log 1 minus y hat, right.",
    "start": "3735600",
    "end": "3743220"
  },
  {
    "text": "This value itself can be used as a metric, right.",
    "start": "3743220",
    "end": "3748740"
  },
  {
    "text": "So it's basically just the- the- the- the ah, the log-likelihood of our prediction. That's also called the log-loss, right.",
    "start": "3748740",
    "end": "3756390"
  },
  {
    "text": "So the log-loss is, um, is- is kind of different from the two that we've seen so far,",
    "start": "3756390",
    "end": "3763650"
  },
  {
    "text": "uh, is kind of different from the ones, uh, that we've seen- we've seen so far in that. It- it takes into account the value of- of the,",
    "start": "3763650",
    "end": "3773865"
  },
  {
    "text": "uh, predicted probability itself. It's not just looking at the- at the, uh, um, relative ah, thresholds, right.",
    "start": "3773865",
    "end": "3780390"
  },
  {
    "text": "So if we were to, kind of, sum of our I equals 1-n, you know, yI, yI,",
    "start": "3780390",
    "end": "3787049"
  },
  {
    "text": "yI, y hat I, right. So this- this loss value takes into account",
    "start": "3787050",
    "end": "3794430"
  },
  {
    "text": "the predicted value o- of the probability as well, okay. And it basically, uh,",
    "start": "3794430",
    "end": "3802110"
  },
  {
    "text": "rewards confident correct answers, which means it's not enough that you, you know, um,",
    "start": "3802110",
    "end": "3808620"
  },
  {
    "text": "when- when you're- when you're, um, when you predict, um, a correct answer with a relatively high probability,",
    "start": "3808620",
    "end": "3816405"
  },
  {
    "text": "then it rewards you to take that even higher, but at the same time,",
    "start": "3816405",
    "end": "3821790"
  },
  {
    "text": "it very heavily penalizes you when you get it wrong and are very confident about it. [NOISE] Right.",
    "start": "3821790",
    "end": "3828420"
  },
  {
    "text": "So it- it's- it's kind of asymmetric in a- in a way that it rewards you for having confident correct answers,",
    "start": "3828420",
    "end": "3834990"
  },
  {
    "text": "but pretty much kills you if you make a confident wrong answer, right. And, um, it is- it is at this- this property that [NOISE]",
    "start": "3834990",
    "end": "3845549"
  },
  {
    "text": "makes the model kind of choose the actual probability value more carefully, right.",
    "start": "3845550",
    "end": "3851550"
  },
  {
    "text": "Um, this- this- this, uh, objective does not just,",
    "start": "3851550",
    "end": "3856560"
  },
  {
    "text": "uh, does not just look at whether, you know, the positives were higher and negatives were low, it also looks into how much, uh, you know,",
    "start": "3856560",
    "end": "3864960"
  },
  {
    "text": "what level of confidence did the model, uh, assigning each of these, uh, predictions, right. So another way to think of the, uh,",
    "start": "3864960",
    "end": "3871920"
  },
  {
    "text": "log-loss is, you can think of a betting game, right. Let's say, um, you need to make a prediction of,",
    "start": "3871920",
    "end": "3880369"
  },
  {
    "text": "um, of- of- of n examples. Let's say there are n examples, they can be positive or negative,",
    "start": "3880370",
    "end": "3885845"
  },
  {
    "text": "and you're gonna be presented one example at a time, and you need to make a prediction of,",
    "start": "3885845",
    "end": "3891089"
  },
  {
    "text": "you know, true or false. And the way you are asked to make this prediction is not just,",
    "start": "3891090",
    "end": "3896220"
  },
  {
    "text": "you know, make a prediction, but you're asked to make a bet, right. You need to make a bet of some dollar amount between 0 and 1,",
    "start": "3896220",
    "end": "3903495"
  },
  {
    "text": "let me say, you know, $0.25, or $0.99, or whatever. You need to make a bet of what the correct answer is for a given example.",
    "start": "3903495",
    "end": "3911714"
  },
  {
    "text": "Now, if you make the bet, you- you, um, um, you win the- the bet amount,",
    "start": "3911715",
    "end": "3918480"
  },
  {
    "text": "but if you lose the bet, you win 1 minus the betted amount, right.",
    "start": "3918480",
    "end": "3924585"
  },
  {
    "text": "So, um, which means now if, um, um, [NOISE] so let's say, you know, uh,",
    "start": "3924585",
    "end": "3930390"
  },
  {
    "text": "you have example I- xI, you need to make a bet of, you know, what is the- wha- uh,",
    "start": "3930390",
    "end": "3936270"
  },
  {
    "text": "you need to make a bit of what, uh, uh, uh, of- of- of how confident you are about the,",
    "start": "3936270",
    "end": "3942885"
  },
  {
    "text": "you know, um, um, prediction that you made. Let's say you predict it to be, you know, um, y equals 1.",
    "start": "3942885",
    "end": "3948690"
  },
  {
    "text": "You know, you make some prediction and you- you also place a bet, we'll call it p, right.",
    "start": "3948690",
    "end": "3954285"
  },
  {
    "text": "Then for x, um, um, x2, [NOISE] let's say you predict y equals whatever,",
    "start": "3954285",
    "end": "3960510"
  },
  {
    "text": "0, and some bet amount p. Let's call this p1, p2.",
    "start": "3960510",
    "end": "3966840"
  },
  {
    "text": "Now, if you- if you win, you get p, if you lose, you get 1-p. [NOISE] Right.",
    "start": "3966840",
    "end": "3978300"
  },
  {
    "text": "And then finally, you know, you go through all the example, and in the very end, what you get is not the sum of them,",
    "start": "3978300",
    "end": "3985170"
  },
  {
    "text": "but the product of them, right. So for- for- for one example,",
    "start": "3985170",
    "end": "3991335"
  },
  {
    "text": "maybe you got- you got the correct answer and you earn this value, and for another example, maybe you got it wrong and you earned this value, right.",
    "start": "3991335",
    "end": "3998535"
  },
  {
    "text": "And the final, uh, take-home amount that you're gonna, uh, uh, uh, um, uh that you're gonna, uh,",
    "start": "3998535",
    "end": "4004505"
  },
  {
    "text": "walk away with at the end of it is gonna be the product of all these values, right. Which means if for any one of the examples you predicted, um, uh,",
    "start": "4004505",
    "end": "4014825"
  },
  {
    "text": "you assign a confidence of 1 or 0.99999,",
    "start": "4014825",
    "end": "4020015"
  },
  {
    "text": "and you happen to be wrong, the entire product will be 0or very close to 0, right.",
    "start": "4020015",
    "end": "4028940"
  },
  {
    "text": "So- so in this exercise, um, in fact, there are- there's,",
    "start": "4028940",
    "end": "4034235"
  },
  {
    "text": "uh, kind of, uh, studies that show that, you know, in- with- with this kind of an exercise, you can extract the- the true level of confidence that,",
    "start": "4034235",
    "end": "4043490"
  },
  {
    "text": "uh, a person has in making a bet, right. If you were to earn the sum of these things, then you would be a lot more- you'd- you-",
    "start": "4043490",
    "end": "4050555"
  },
  {
    "text": "your- your betting strategy would be very different. But if you're going to walk away with the product of all these values,",
    "start": "4050555",
    "end": "4056330"
  },
  {
    "text": "then you know that if you make one confident wrong prediction, you assign a probability of 1 and the answer was wrong,",
    "start": "4056330",
    "end": "4064610"
  },
  {
    "text": "then you get 0 and the product of 0 with eve- anything else is 0, right.",
    "start": "4064610",
    "end": "4070370"
  },
  {
    "text": "And, um, so that's- that's basically how the, um, yeah. A question?",
    "start": "4070370",
    "end": "4075500"
  },
  {
    "text": "[inaudible]",
    "start": "4075500",
    "end": "4082130"
  },
  {
    "text": "So the question is, can we use this in, uh, when, uh, on models that make di- you know, class direct- class predictions rather than scores of probabilities.",
    "start": "4082130",
    "end": "4090605"
  },
  {
    "text": "Now, so the log- log-loss is for probabilistic models only. [inaudible].",
    "start": "4090605",
    "end": "4110299"
  },
  {
    "text": "So the question is, you know, um, [NOISE] uh, perhaps rightly you're identifying that, you know,",
    "start": "4110300",
    "end": "4115940"
  },
  {
    "text": "wouldn't this encourage the models to be more conservative, you know, rather than making extreme values? Yeah, the log-loss can make your models more conservative,",
    "start": "4115940",
    "end": "4124115"
  },
  {
    "text": "but, uh, uh, the- the, um, the interesting thing out of this is that,",
    "start": "4124115",
    "end": "4130910"
  },
  {
    "text": "this process, you know, this kind of a loss, is of proper scoring rule. Which means- [NOISE] which means the predicted probabilities,",
    "start": "4130910",
    "end": "4143525"
  },
  {
    "text": "you know, from- from this kind of a loss will always result in, you know, uh, calibrated probabilities.",
    "start": "4143525",
    "end": "4149315"
  },
  {
    "text": "Then let's say you want to do like apply modern in the financial market Yeah. You're never going to prove in this because",
    "start": "4149315",
    "end": "4155750"
  },
  {
    "text": "in the financial market you don't get products, you get services. And if you make a words in place [inaudible] any investments",
    "start": "4155750",
    "end": "4161088"
  },
  {
    "text": "and it makes one loss but the [inaudible]",
    "start": "4161089",
    "end": "4166139"
  },
  {
    "text": "[inaudible] Yeah, so the question is, uh, would this be, uh, a good- good metric or good loss to use?",
    "start": "4166140",
    "end": "4172365"
  },
  {
    "text": "So in a financial setting where you generally take the sum of, you know, rewards rather than- yeah, so there are- there are, um, um,",
    "start": "4172365",
    "end": "4178559"
  },
  {
    "text": "you know, the- if- if you're interested, you know, the- the- the thing to look for are robust scoring rules.",
    "start": "4178560",
    "end": "4184889"
  },
  {
    "text": "[NOISE] And we're gonna look at one,",
    "start": "4184890",
    "end": "4192180"
  },
  {
    "text": "uh, in- in a few slides. So there are other kind of scoring rules which still give, you know, p- uh, a calibrated probabilities.",
    "start": "4192180",
    "end": "4199395"
  },
  {
    "text": "Other scoring rules that give you calibrated probabilities, but are much more risk tolerant than,",
    "start": "4199395",
    "end": "4204659"
  },
  {
    "text": "um, uh, the log-loss, right? And- and you can use those, uh, in, uh, those contexts, right?",
    "start": "4204660",
    "end": "4211815"
  },
  {
    "text": "So, uh, so the log-loss is this, uh, is this kind of a loss that not only evaluates the quali- not only evaluates whether you,",
    "start": "4211815",
    "end": "4222105"
  },
  {
    "text": "you know, got the answer right or wrong, but also kind of elicits the true level of",
    "start": "4222105",
    "end": "4227820"
  },
  {
    "text": "confidence [NOISE] or- or the true kind of belief you have, probabilistic belief, you have in making- making a certain, uh, uh, prediction.",
    "start": "4227820",
    "end": "4236710"
  },
  {
    "text": "So this is- this is an example of a proper scoring rule and we- we saw kind of proper scoring rules in- in,",
    "start": "4236710",
    "end": "4242870"
  },
  {
    "text": "uh, in the- in the maximum entropy lecture as well. So that's, uh, log-loss.",
    "start": "4242870",
    "end": "4249320"
  },
  {
    "text": "[NOISE] Which brings us to, uh, this concept of calibration, right? So calibration is when",
    "start": "4249320",
    "end": "4254910"
  },
  {
    "text": "the predicted probabilities match the real-world occurrence or frequencies, right?",
    "start": "4254910",
    "end": "4262470"
  },
  {
    "text": "So in this case, uh, we have two models on the right, over here, right?",
    "start": "4262470",
    "end": "4267960"
  },
  {
    "text": "So the blue line corresponds to the cali- is the calibration curve of, you know,",
    "start": "4267960",
    "end": "4273375"
  },
  {
    "text": "a logistic regression model and the orange line corresponds to the calibration curve of a support vector classifier,",
    "start": "4273375",
    "end": "4279945"
  },
  {
    "text": "support vector machine, right? What- the way to read this plot is on the x-axis.",
    "start": "4279945",
    "end": "4288495"
  },
  {
    "text": "The x-axis corresponds to predicted probabilities and the y-axis is",
    "start": "4288495",
    "end": "4294225"
  },
  {
    "text": "the fraction of positives in the region around this predicted probability, right?",
    "start": "4294225",
    "end": "4300675"
  },
  {
    "text": "And the bottom plot over here is just a histogram of how were the different predicted probabilities,",
    "start": "4300675",
    "end": "4308489"
  },
  {
    "text": "uh, how are they distributed, right? And you always want to look at, uh,",
    "start": "4308490",
    "end": "4314969"
  },
  {
    "text": "a calibration plot and the histogram of predicted probabilities in conjunction all the time, right?",
    "start": "4314970",
    "end": "4321990"
  },
  {
    "text": "So, um, if the calibration curve goes close to the 45-degree line,",
    "start": "4321990",
    "end": "4329670"
  },
  {
    "text": "it means when the predicted probability was 0.8, for example, then the actual fraction of examples was also,",
    "start": "4329670",
    "end": "4339480"
  },
  {
    "text": "you know, roughly 80%. Similarly, if the, uh, of all the predictions that had a prediction of 0.2,",
    "start": "4339480",
    "end": "4345824"
  },
  {
    "text": "roughly 20% of them were, you know, uh, the, uh, positives, right?",
    "start": "4345825",
    "end": "4351255"
  },
  {
    "text": "However, the orange curve looks pretty different, right? So, you know, for the orange curve,",
    "start": "4351255",
    "end": "4357465"
  },
  {
    "text": "even when it predicted, you know, a 0.6 probability, most of them were actually,",
    "start": "4357465",
    "end": "4365115"
  },
  {
    "text": "you know, uh, uh, uh, mo- most of them were- were actually, you know, all positives.",
    "start": "4365115",
    "end": "4370125"
  },
  {
    "text": "This means that the orange curve, you can think of it as being under confident in some way.",
    "start": "4370125",
    "end": "4375330"
  },
  {
    "text": "And even though the examples were mostly positive, it kind of hesitated to assign high probabilities to them, right?",
    "start": "4375330",
    "end": "4382770"
  },
  {
    "text": "So even- even in the region where, you know, the predictions are still 0.6 and 0.7,",
    "start": "4382770",
    "end": "4387929"
  },
  {
    "text": "all of them are actually, you know, positive examples, right? Uh, so a ca- a calibration curve that looks like this.",
    "start": "4387930",
    "end": "4396090"
  },
  {
    "text": "So a perfectly calibrated curve would be like a straight line, right? An underconfident calibration curve,",
    "start": "4396090",
    "end": "4403620"
  },
  {
    "text": "will have this kind of an inverted S-shape, right? Similarly, an overconfident, uh,",
    "start": "4403620",
    "end": "4410460"
  },
  {
    "text": "model will have a calibration curve that looks like this, right? So this would correspond to overconfident.",
    "start": "4410460",
    "end": "4418480"
  },
  {
    "text": "This is underconfident, right?",
    "start": "4420890",
    "end": "4428895"
  },
  {
    "text": "If it's correctly here, this is, you know, well-calibrated.",
    "start": "4428895",
    "end": "4432910"
  },
  {
    "text": "So, um, the- the, um, um,",
    "start": "4439730",
    "end": "4445500"
  },
  {
    "text": "few take- takeaways from this slide is that it's- it's always,",
    "start": "4445500",
    "end": "4450645"
  },
  {
    "text": "you know, uh, important to kind of see these two plots in conjunction. And the way you wanna read them is, first,",
    "start": "4450645",
    "end": "4458549"
  },
  {
    "text": "make sure that your model is well calibrated, right? If your model is not even calibrated, then the probabilities that it is,",
    "start": "4458549",
    "end": "4465630"
  },
  {
    "text": "uh, uh, uh, uh, uh, predicting are not even valid probabilities, right? And then once you- once you know that a model is well calibrated,",
    "start": "4465630",
    "end": "4474600"
  },
  {
    "text": "then look at the histogram to see if you have these nice two peaks at the extremes, right?",
    "start": "4474600",
    "end": "4481725"
  },
  {
    "text": "So that- that's like the perfect scenario where the calibration curve is- is exactly along the 45 degree lines and the- and the histogram,",
    "start": "4481725",
    "end": "4490125"
  },
  {
    "text": "it has these two perfect, uh, uh, uh, you know, two peaks all the way at the left and another peak all the way to the right.",
    "start": "4490125",
    "end": "4497040"
  },
  {
    "text": "So that's- that's the, uh, ideal scenario. However, if your model is not well calibrated, right?",
    "start": "4497040",
    "end": "4505695"
  },
  {
    "text": "In this case, in the orange case, if the model is not well calibrated, then this histogram says nothing at all.",
    "start": "4505695",
    "end": "4512160"
  },
  {
    "text": "It does not tell you whether the model is doing well or bad, right? In this example, if you see, you know, uh,",
    "start": "4512160",
    "end": "4518085"
  },
  {
    "text": "both these models have, uh, you know, for a chosen threshold of 0.5, the logistic regression and support vector machine have, you know,",
    "start": "4518085",
    "end": "4525690"
  },
  {
    "text": "0.87 precision, 0.85 recall, 0.86 F1 score. They're all exactly the same, right?",
    "start": "4525690",
    "end": "4532920"
  },
  {
    "text": "But the calibration of the support vector machine is completely bonkers.",
    "start": "4532920",
    "end": "4538545"
  },
  {
    "text": "You know, it's not well calibrated at all and so the histogram does not reflect the quality of classification.",
    "start": "4538545",
    "end": "4546614"
  },
  {
    "text": "But if the model is well calibrated and the histogram gives you a good sense of se- of- of how well the model is- is,",
    "start": "4546615",
    "end": "4553724"
  },
  {
    "text": "um, um, uh, separating them. So just like the log-loss that we saw over here,",
    "start": "4553725",
    "end": "4559305"
  },
  {
    "text": "there is this other score that you can use called the Brier score. So the Brier score is [NOISE] very similar to the mean squared error.",
    "start": "4559305",
    "end": "4569790"
  },
  {
    "text": "[NOISE]",
    "start": "4569790",
    "end": "4581070"
  },
  {
    "text": "So the Brier score, we can think of this as, you know,",
    "start": "4581070",
    "end": "4588570"
  },
  {
    "text": "sum over i equals 1 to n of all your, uh, n examples, p-hat, let- let,",
    "start": "4588570",
    "end": "4595469"
  },
  {
    "text": "you know, let- this is the predicate probability, right? So p-hat^i minus y^i [NOISE] squared.",
    "start": "4595470",
    "end": "4602790"
  },
  {
    "text": "[NOISE] It's just the- the mean of the squared error between the label and the predicted probability. Yes, question.",
    "start": "4602790",
    "end": "4612010"
  },
  {
    "text": "So it's- if, uh, if your model is, uh, overconfident,",
    "start": "4615710",
    "end": "4620820"
  },
  {
    "text": "then your histogram will generally look like the blue one because it's- it's assigning like really high- high,",
    "start": "4620820",
    "end": "4626909"
  },
  {
    "text": "uh, uh, uh, probabilities. So, you know, the probabilities are more at the- at- at the extremes it's- it's more confident,",
    "start": "4626910",
    "end": "4632985"
  },
  {
    "text": "but they are wrong, which means they've got the orderings wrong. So over here in the histogram that, you know,",
    "start": "4632985",
    "end": "4639030"
  },
  {
    "text": "the labels are not visible here, right? So this means most of the probabilities at- most of the examples got this probably,",
    "start": "4639030",
    "end": "4646710"
  },
  {
    "text": "but it does not tell you what fraction of them were correct or wrong, right? Whereas the- the, um, uh,",
    "start": "4646710",
    "end": "4652395"
  },
  {
    "text": "calibration curve tells you what fraction of them were correct or wrong. [inaudible]",
    "start": "4652395",
    "end": "4660480"
  },
  {
    "text": "It could look similar to- so yes, so the- the histogram could look similar to the well-calibrated one,",
    "start": "4660480",
    "end": "4665730"
  },
  {
    "text": "it could not- there's- there's- generally underconfident, um, underconfident models,",
    "start": "4665730",
    "end": "4671715"
  },
  {
    "text": "will have more probabilities assigned near 0.5. So you'll ha- you'll see a bulge over here if they are underconfident and, you know,",
    "start": "4671715",
    "end": "4678645"
  },
  {
    "text": "confident ones will have, uh, more- more assignments at the extremes, right?",
    "start": "4678645",
    "end": "4683895"
  },
  {
    "text": "What you wa- ideally want are confident models that are also calibrated. Yes, question.",
    "start": "4683895",
    "end": "4690390"
  },
  {
    "text": "[inaudible]",
    "start": "4690390",
    "end": "4698580"
  },
  {
    "text": "Yes. So the question is, uh, why not just look at the calibration plot and just ignore,",
    "start": "4698580",
    "end": "4704774"
  },
  {
    "text": "ah, ah, ah, the - the uh, histogram completely. And, uh, the - the answer for that is,",
    "start": "4704774",
    "end": "4710864"
  },
  {
    "text": "back in the calibration discussion, we saw how, you know, uh, calibration does not necessarily mean accuracy at all, right?",
    "start": "4710865",
    "end": "4718140"
  },
  {
    "text": "So you can have a perfectly calibrated plot. But most of your assigned probabilities might be, you know, just 0.5.",
    "start": "4718140",
    "end": "4726435"
  },
  {
    "text": "And you have a few of them that are, you know, well calibrated on the other side, but for most of them you're just predicting 0.5. And that's - that's kind of bad.",
    "start": "4726435",
    "end": "4732719"
  },
  {
    "text": "So ideally you want a plot that is perfectly calibrated and also well separated and have these two peaks.",
    "start": "4732720",
    "end": "4739405"
  },
  {
    "text": "So that's - that's like a good sign, right? So the Brier score is basically the - the mean",
    "start": "4739405",
    "end": "4746610"
  },
  {
    "text": "squared error between the predicted probability, right? So this y is either 0 or 1.",
    "start": "4746610",
    "end": "4753824"
  },
  {
    "text": "And this is between 0 and 1, okay?",
    "start": "4753825",
    "end": "4759360"
  },
  {
    "text": "So we see that, uh, even though the - the usual ranking based metrics are exactly the same,",
    "start": "4759360",
    "end": "4767730"
  },
  {
    "text": "the Brier score between them is quite different, right? And the Brier score is also",
    "start": "4767730",
    "end": "4776339"
  },
  {
    "text": "a proper scoring rule [NOISE].",
    "start": "4776339",
    "end": "4783270"
  },
  {
    "text": "So in the - in the - in the example one of you asked before about, you know, what if you're in a - in a - in a kind of a finance kind of a scenario where you want to,",
    "start": "4783270",
    "end": "4791745"
  },
  {
    "text": "uh, where you want to - to, um, uh be more risky in some way.",
    "start": "4791745",
    "end": "4797295"
  },
  {
    "text": "For example, uh. You know, you can use, you know, the Brier scoring rule in those cases. You know, the Brier scoring rule also - um,",
    "start": "4797295",
    "end": "4804720"
  },
  {
    "text": "is also a - a proper scoring rule, which means it will value calibrated, uh, uh, probabilities more.",
    "start": "4804720",
    "end": "4811380"
  },
  {
    "text": "But it is more risk tolerant. It has other trade-offs compared to the log loss.",
    "start": "4811380",
    "end": "4819070"
  },
  {
    "text": "Right, any questions on this? So - so - so basically the takeaway is that,",
    "start": "4821240",
    "end": "4828060"
  },
  {
    "text": "you know, always look at the calibration and the histogram in conjunction. Make sure - first make sure that the model is well-calibrated.",
    "start": "4828060",
    "end": "4836835"
  },
  {
    "text": "Otherwise your predictions are not even - they're not even valid probabilities.",
    "start": "4836835",
    "end": "4842085"
  },
  {
    "text": "And once it's calibrated, look at, you know, look at - look at ho - how well separated they are.",
    "start": "4842085",
    "end": "4848280"
  },
  {
    "text": "So the - the - the - you may also see uh, the terminologies like calibration",
    "start": "4848280",
    "end": "4854310"
  },
  {
    "text": "versus discrimination [NOISE] right?",
    "start": "4854310",
    "end": "4866025"
  },
  {
    "text": "So discrimination generally refers to how well your model can separate positives and negatives in a ranking sense,",
    "start": "4866025",
    "end": "4874230"
  },
  {
    "text": "and calibration tells you how meaningful are the assigned probability values, right?",
    "start": "4874230",
    "end": "4880725"
  },
  {
    "text": "And - and you - you generally want your model to be well-calibrated and have high dis - high discrimination.",
    "start": "4880725",
    "end": "4887770"
  },
  {
    "text": "Right, moving on. Right. So, ah,",
    "start": "4889130",
    "end": "4894764"
  },
  {
    "text": "the - the log probability of the log loss can also be used in unsupervised learning.",
    "start": "4894765",
    "end": "4900855"
  },
  {
    "text": "So supposing you have a Gaussian mixture model or factor analysis, right? You can use log P of x to measure your, um,",
    "start": "4900855",
    "end": "4908159"
  },
  {
    "text": "uh, whether your model has under-fit or lower - or over-fit. Log p of x by itself,",
    "start": "4908160",
    "end": "4913320"
  },
  {
    "text": "the raw value is hard to interpret. You know, it's - it's hard to just look at the log likelihood and tell whether the model is good or bad.",
    "start": "4913320",
    "end": "4921915"
  },
  {
    "text": "But you can always measure the log likelihood or log p of x on your training set and on your test set and measure the difference between them, right?",
    "start": "4921915",
    "end": "4931304"
  },
  {
    "text": "If - if the, ah, if the gap between them is very large, if you - if you have a high log likelihood on your training set,",
    "start": "4931305",
    "end": "4937590"
  },
  {
    "text": "but low likelihood on your test set, then your unsupervised learning model has over-fit on your training data.",
    "start": "4937590",
    "end": "4944745"
  },
  {
    "text": "So you want - you want to, you know, ah - so - so under-fitting, over-fitting, uh,",
    "start": "4944745",
    "end": "4950250"
  },
  {
    "text": "even though it's commonly used in a supervised learning setting uh, to - to uh, measure accuracy or whatever,",
    "start": "4950250",
    "end": "4957270"
  },
  {
    "text": "is not limited to supervised learning alone, right? Under-fitting or overfitting concepts apply just as well to uh,",
    "start": "4957270",
    "end": "4964650"
  },
  {
    "text": "unsupervised uh, settings as well. Yes. Question? [inaudible]. Between uh - so - so the question is - ah -",
    "start": "4964650",
    "end": "4976409"
  },
  {
    "text": "Take the log likelihoods for like his training versus validation set? So - so - so what you want - so - s",
    "start": "4976410",
    "end": "4986880"
  },
  {
    "text": "- the way you want to - so - so the question is,",
    "start": "4986880",
    "end": "4992610"
  },
  {
    "text": "um, you have a t - a particular training value and you have a particular test value. How do you kind of find a balance between them?",
    "start": "4992610",
    "end": "4999435"
  },
  {
    "text": "The - the onset is very similar to how you would do in - in a supervised setting. Let's say you have, you know, uh, uh,",
    "start": "4999435",
    "end": "5005180"
  },
  {
    "text": "85% accuracy on your training set, on the supervised classification.",
    "start": "5005180",
    "end": "5011675"
  },
  {
    "text": "Let's say you have, you know, 23% on your test set, right? So what do you do, you go and, you know,",
    "start": "5011675",
    "end": "5018260"
  },
  {
    "text": "address bias or adr - address variance to bring the gap, uh, closer together. Similarly, you know, you fit a Gaussian mixture model that",
    "start": "5018260",
    "end": "5024889"
  },
  {
    "text": "gives you a log likelihood on your training data to be, you know, generally there are like, you know - you know, like minus 2,000.",
    "start": "5024890",
    "end": "5032600"
  },
  {
    "text": "[NOISE] This is like a reasonable value for, you know, a log likelihood of a - of a Gaussian mixture model.",
    "start": "5032600",
    "end": "5038300"
  },
  {
    "text": "Let's say you get minus 2,000 on your uh, training set and on your test set, you get minus 18,000 [NOISE], right?",
    "start": "5038300",
    "end": "5046640"
  },
  {
    "text": "Then - then you know uh, it probably means you have assigned too many number of clusters.",
    "start": "5046640",
    "end": "5052520"
  },
  {
    "text": "You want to reduce your K value, for example, and kind of bring the two together, um, or so - something - something along those lines.",
    "start": "5052520",
    "end": "5060420"
  },
  {
    "text": "So, yeah, so log P of x is - is - is - can be used as",
    "start": "5061090",
    "end": "5067190"
  },
  {
    "text": "an evaluation metric to measure the gap between your training likelihood and the test likelihood,",
    "start": "5067190",
    "end": "5072530"
  },
  {
    "text": "to see if your unsupervised model has over-fit or not. Okay? Um, however, you know,",
    "start": "5072530",
    "end": "5078290"
  },
  {
    "text": "k-means is a little tricky. You know, k-means does not have, uh, a good probabilistic interpretation, and it also uses, you know, fixed co-variances.",
    "start": "5078290",
    "end": "5085340"
  },
  {
    "text": "So with k-means, it's - it's a little harder to measure, uh, whether it is,",
    "start": "5085340",
    "end": "5091505"
  },
  {
    "text": "uh, under-fit or over-fit. Howe - uh, so, uh, however with Gaussian mixture models it uh - uh,",
    "start": "5091505",
    "end": "5097580"
  },
  {
    "text": "or probabilistic models in general like it uh, works pretty well [NOISE]. Yes. Question.",
    "start": "5097580",
    "end": "5105500"
  },
  {
    "text": "[inaudible].",
    "start": "5105500",
    "end": "5111950"
  },
  {
    "text": "Yeah. So - so, you know, can we take the mean squared between uh, you know uh, the centroids and, um, the examples.",
    "start": "5111950",
    "end": "5119585"
  },
  {
    "text": "So we just basically the distortion function and measure the distortion function on the test set. Uh, you could do that,",
    "start": "5119585",
    "end": "5124940"
  },
  {
    "text": "but that does not always work well. So even the distortion function can - can um - um - so for example, uh, let's say,",
    "start": "5124940",
    "end": "5132185"
  },
  {
    "text": "you know you have [NOISE] some - some clusters like this.",
    "start": "5132185",
    "end": "5140240"
  },
  {
    "text": "And let's say you assign, um, uh - let's say you - you - you uh,",
    "start": "5140240",
    "end": "5145580"
  },
  {
    "text": "assign six uh - uh clusters cent - centroids. So you get one - let me use a different color.",
    "start": "5145580",
    "end": "5152480"
  },
  {
    "text": "[NOISE] So you get this centroid, this centroid, centroid, centroid, centroid, centroid.",
    "start": "5152480",
    "end": "5161870"
  },
  {
    "text": "And I mean - I mean, looking at this qualitatively, you can tell you kind of over-fit that will actually just three clusters,",
    "start": "5161870",
    "end": "5167510"
  },
  {
    "text": "but you've assigned six. Um, however, if you were to measure the distortion function,",
    "start": "5167510",
    "end": "5173255"
  },
  {
    "text": "if you were to hold a fraction of them as a hold-out set and measure the, um, measure the distortion function on the test set,",
    "start": "5173255",
    "end": "5180410"
  },
  {
    "text": "they will look kind of pretty similar. So the distortion function um, you know, because it does not uh,",
    "start": "5180410",
    "end": "5186980"
  },
  {
    "text": "have this flexible covariance per cluster, right? It's, uh, it's - it's hard to use",
    "start": "5186980",
    "end": "5193940"
  },
  {
    "text": "the distortion function as - as a way to check whether you under-fit or over-fit.",
    "start": "5193940",
    "end": "5199070"
  },
  {
    "text": "[inaudible].",
    "start": "5199070",
    "end": "5205880"
  },
  {
    "text": "Yeah. So you - it's - it's - of - of the k-means it's - it's usually, you - you end up doing some kind of a visualization and it's more, you know, like,",
    "start": "5205880",
    "end": "5214114"
  },
  {
    "text": "you know I see and do some spot checks to see if, you know, a few examples that were in the same cluster are meaningful or not.",
    "start": "5214115",
    "end": "5220280"
  },
  {
    "text": "Or you know, in a similar examples ended up in different clusters. You know, it's - there is - there is no,",
    "start": "5220280",
    "end": "5225890"
  },
  {
    "text": "it - it's not as cleanly, um, um, defined as the uh,",
    "start": "5225890",
    "end": "5230920"
  },
  {
    "text": "log likelihood that you get from uh, Gaussian mixture models. Yes question.",
    "start": "5230920",
    "end": "5236830"
  },
  {
    "text": "[inaudible] [NOISE]",
    "start": "5236830",
    "end": "5248890"
  },
  {
    "text": "Yeah- yeah, so- so- so the comment there was you can- you can have this, you know, distortion versus K and",
    "start": "5248890",
    "end": "5257665"
  },
  {
    "text": "eventually it's gonna- the distortion is gonna flatten out and maybe,",
    "start": "5257665",
    "end": "5262705"
  },
  {
    "text": "you know, just look at it and- and- and choose some value, right? However, this requires, as I said, you know,",
    "start": "5262705",
    "end": "5268870"
  },
  {
    "text": "this is like a heuristic you are to look at it and- and decide, you can't compare one scalar value with another scalar value,",
    "start": "5268870",
    "end": "5275275"
  },
  {
    "text": "one on a test set and one on training set and- and declare that it will all fit, right? So yeah- so measuring underfitting and overfitting",
    "start": "5275275",
    "end": "5282610"
  },
  {
    "text": "is- is- is not as clean with k-means as it is with Gaussian mixture models.",
    "start": "5282610",
    "end": "5287725"
  },
  {
    "text": "Yeah, but this- this- this is a commonly used approach yeah.",
    "start": "5287725",
    "end": "5291620"
  },
  {
    "text": "Right, moving on to class imbalance problems. So generally if you have examples",
    "start": "5294960",
    "end": "5302710"
  },
  {
    "text": "where one classes is- is a much smaller fraction than the other, or if one class, kind of,",
    "start": "5302710",
    "end": "5308605"
  },
  {
    "text": "dominates the other class, then, you know, you kind of call it- you say that you are in a class imbalanced scenario.",
    "start": "5308605",
    "end": "5316179"
  },
  {
    "text": "And a lot of the metrics that we've, ah, kind of seen, um- um, ah- ah,",
    "start": "5316180",
    "end": "5321655"
  },
  {
    "text": "today can end up, ah, breaking down in under class imbalance scenarios, right?",
    "start": "5321655",
    "end": "5328090"
  },
  {
    "text": "So, uh, so for example, accuracy, right?",
    "start": "5328090",
    "end": "5333520"
  },
  {
    "text": "So if you're- if you're in- in- in a class imbalanced scenario, and lets say you have just 1% of all your examples to be, you know,",
    "start": "5333520",
    "end": "5340525"
  },
  {
    "text": "positive, um, and you just- you have this- this blind classifier that predicts no all the time,",
    "start": "5340525",
    "end": "5347485"
  },
  {
    "text": "it gets 99% accuracy, right? So having a high accuracy, does not mean much, ah,",
    "start": "5347485",
    "end": "5353350"
  },
  {
    "text": "in- in a- in a class, uh, imbalance scenario. Similarly, the log loss, right?",
    "start": "5353350",
    "end": "5358810"
  },
  {
    "text": "In the log loss, the majority class can easily dominate the assigned probabilities.",
    "start": "5358810",
    "end": "5363955"
  },
  {
    "text": "So if you have, um, if you- if you- if you- log loss can be easily,",
    "start": "5363955",
    "end": "5374335"
  },
  {
    "text": "um- um- so imagine a- a logistic regression",
    "start": "5374335",
    "end": "5382900"
  },
  {
    "text": "where [NOISE] you have some,",
    "start": "5382900",
    "end": "5391435"
  },
  {
    "text": "you know, some- some number of positive examples over here, um, and you have just one negative example over here.",
    "start": "5391435",
    "end": "5400130"
  },
  {
    "text": "Logistic regression, will place the separating hyperplane, that is the line at which the- the,",
    "start": "5400250",
    "end": "5406560"
  },
  {
    "text": "ah- ah, y hat is 0.5, probably somewhere over here, right?",
    "start": "5406560",
    "end": "5413290"
  },
  {
    "text": "And- and, um- um- so- so the- the majority class can easily dominate the log loss and make the,",
    "start": "5413290",
    "end": "5421600"
  },
  {
    "text": "um- um- make the, uh- uh, separating hyperplane be placed at some, kind of an unreasonable uh- uh,",
    "start": "5421600",
    "end": "5427825"
  },
  {
    "text": "unreasonable, uh- uh, location, right. So the log loss is also vulnerable to class imbalance.",
    "start": "5427825",
    "end": "5434409"
  },
  {
    "text": "AUROC, right. So imagine, uh, let- let- let's see how the AUROC can break down. All right.",
    "start": "5434410",
    "end": "5443059"
  },
  {
    "text": "So let's say you have, um, examples that- that are ranked like this [NOISE], right.",
    "start": "5443280",
    "end": "5455335"
  },
  {
    "text": "So suppose you are- you are, um- suppose you are, um- um,",
    "start": "5455335",
    "end": "5462055"
  },
  {
    "text": "in a- in a fraud detection scenario, credit card fraud detection, right. You want to build a classifier given a credit card transaction",
    "start": "5462055",
    "end": "5469989"
  },
  {
    "text": "classified- classify it as fraudulent or not, right. In this scenario, let's say you decide to use AUROC as your metric, right.",
    "start": "5469990",
    "end": "5479470"
  },
  {
    "text": "AUROC, uh, and- and- let- let's say the- the- so typically in- in these,",
    "start": "5479470",
    "end": "5485620"
  },
  {
    "text": "kind of scenarios where the prevalence is very low, most of the times you are in this,",
    "start": "5485620",
    "end": "5491830"
  },
  {
    "text": "kind of retrieval scenario or information retrieval, kind of a scenario where you want to, you know,",
    "start": "5491830",
    "end": "5498025"
  },
  {
    "text": "in this- given this pool of- of all credit card transactions, you just want to surface the fraudulent wants to the top,",
    "start": "5498025",
    "end": "5505825"
  },
  {
    "text": "and, you know, act on them somehow, right. So in- in this, kind of a scenario,",
    "start": "5505825",
    "end": "5511435"
  },
  {
    "text": "let's say you- you have a model that- let's say you have 10,000 transactions,",
    "start": "5511435",
    "end": "5519205"
  },
  {
    "text": "and in this 10,000 transactions, let's say 10 of them are fraudulent, okay?",
    "start": "5519205",
    "end": "5528810"
  },
  {
    "text": "And let's say 1, 2, 3, 4, 5, 6, 7, 8, 9,",
    "start": "5528810",
    "end": "5535605"
  },
  {
    "text": "10 so the first 10 were all good transactions, and the next 10 were all",
    "start": "5535605",
    "end": "5542139"
  },
  {
    "text": "fraudulent, right, or we assume there are 10 here,",
    "start": "5542140",
    "end": "5551275"
  },
  {
    "text": "and then you have all the good examples. So this is the first 10, the next 10, and the remaining 9,980, right.",
    "start": "5551275",
    "end": "5563265"
  },
  {
    "text": "Let's say your- your model ranks them in this way. Okay. From, um- um,",
    "start": "5563265",
    "end": "5569929"
  },
  {
    "text": "from- from the user of this model, this is a useless model,",
    "start": "5569930",
    "end": "5574950"
  },
  {
    "text": "because you want to act on the top most likely fraudulent, um- um- the- the top 10 most likely transactions are to be fraudulent,",
    "start": "5574950",
    "end": "5584010"
  },
  {
    "text": "but all of them over here were valid ones right, and then comes the fraudulent, you know,",
    "start": "5584010",
    "end": "5590215"
  },
  {
    "text": "after you have an equal number of, um- um- um, you know, good transactions and then you have the rest, right?",
    "start": "5590215",
    "end": "5598225"
  },
  {
    "text": "Now, what's gonna be the AUC of this model? All right. Let's- let's do a quick sanity check [NOISE].",
    "start": "5598225",
    "end": "5605500"
  },
  {
    "text": "So- so we have our 10 positives. So chop this into 10,",
    "start": "5605500",
    "end": "5611935"
  },
  {
    "text": "ah- ah- ah, 10, intervals the sensitivity side, and the specificity side chop it into 9,990, right.",
    "start": "5611935",
    "end": "5622240"
  },
  {
    "text": "You have 9,990 pieces here. So the first 10 were all negatives,",
    "start": "5622240",
    "end": "5628928"
  },
  {
    "text": "which means for the first 10, you get a steep drop,",
    "start": "5628929",
    "end": "5634120"
  },
  {
    "text": "and then we have 10- all the 10 positive examples. So, you know 1, 2, 3, 4,",
    "start": "5634120",
    "end": "5640105"
  },
  {
    "text": "5, 6, 7, 8, 9, 10, and then you get the negative, and this will give you an area- area under the curve of 0.999 or 0.99.",
    "start": "5640105",
    "end": "5650575"
  },
  {
    "text": "Maybe I missed the 0.999 or 0.99. Anyway it doesn't matter. It gives you a really high AUC.",
    "start": "5650575",
    "end": "5656260"
  },
  {
    "text": "Even though, you know from- from, um, the usefulness of this model, you got only good transactions flagged as- as fraudulent at the top, right.",
    "start": "5656260",
    "end": "5666550"
  },
  {
    "text": "So in class imbalanced scenarios, AUC can also be pretty bad. Accuracy is the worst, you know,",
    "start": "5666550",
    "end": "5672790"
  },
  {
    "text": "you probably never should use accuracy when you have class imbalance. But AUC can also be- can also be fooled, right?",
    "start": "5672790",
    "end": "5680830"
  },
  {
    "text": "And so AUPRC is somewhat more robust, um, in precision- in- in- in these kind of scenarios,",
    "start": "5680830",
    "end": "5687849"
  },
  {
    "text": "when you're interested in, you know, um, the quality at the top, where your focus is on the top of- of what you",
    "start": "5687850",
    "end": "5695800"
  },
  {
    "text": "do because you're generally not going to act on the rest at all. You're just gonna, um- um, do something with the- the top predicted examples.",
    "start": "5695800",
    "end": "5704020"
  },
  {
    "text": "In those kinds of scenarios, you probably want to use AUPRC rather than AUROC.",
    "start": "5704020",
    "end": "5710150"
  },
  {
    "text": "So, um- so- so the, uh- uh, summary is that we're in- in,",
    "start": "5711750",
    "end": "5717010"
  },
  {
    "text": "um, class imbalanced scenarios, your least preference should be accuracy as the choice of metric.",
    "start": "5717010",
    "end": "5722905"
  },
  {
    "text": "AUROC might be better, might not be better. Because AUROC can also be fooled.",
    "start": "5722905",
    "end": "5728125"
  },
  {
    "text": "AUPRC is generally more robust, uh. But with AUPRC there are, you know, uh, other challenges that come with it.",
    "start": "5728125",
    "end": "5735085"
  },
  {
    "text": "For example, you know, if your AUPRC curve, you know, looks jagged like this.",
    "start": "5735085",
    "end": "5740395"
  },
  {
    "text": "This is pretty sensitive to, um- um, you know the ordering over here.",
    "start": "5740395",
    "end": "5746200"
  },
  {
    "text": "If your ordering is- is different, it might look very different, right. So the AUROC can sometimes seem pre- pretty sensitive to small changes. Yes question.",
    "start": "5746200",
    "end": "5756820"
  },
  {
    "text": "[inaudible]",
    "start": "5756820",
    "end": "5772474"
  },
  {
    "text": "Yes. So the- so the question is, uh, you know, uh, can you- can you kind of adjust the class balance by upsampling or downsampling,",
    "start": "5772475",
    "end": "5783155"
  },
  {
    "text": "uh, the classes, uh, to fit your model? You can- you can do such things. So that's- that's actually a very good point.",
    "start": "5783155",
    "end": "5789290"
  },
  {
    "text": "If you- in order to make your model perform better, you can do all kinds of things like adjusting the class balance,",
    "start": "5789290",
    "end": "5796489"
  },
  {
    "text": "upsampling the, you know, the minority class, downsampling the majority class and so on.",
    "start": "5796490",
    "end": "5801980"
  },
  {
    "text": "However, when you are measuring the quality of your model on a test set or validation set,",
    "start": "5801980",
    "end": "5809565"
  },
  {
    "text": "the prevalence in the test set or the validation sets should match the true prevalence, right? Do the adjustment upsampling,",
    "start": "5809565",
    "end": "5815409"
  },
  {
    "text": "downsampling only on the training set and not on the- on the test set. [inaudible]",
    "start": "5815410",
    "end": "5821300"
  },
  {
    "text": "It could help. It's, you know, the question of whether, uh, you know, upsampling and downsampling, will it help or not?",
    "start": "5821300",
    "end": "5826505"
  },
  {
    "text": "The answer is always cross-validate and check, right? Have- have a validation set which, you know, which is- which is kinda of,",
    "start": "5826505",
    "end": "5832880"
  },
  {
    "text": "you know, pure in the sense that it- it has the same prevalence ratios and so on. Um, do whatever you want with the- with",
    "start": "5832880",
    "end": "5839540"
  },
  {
    "text": "the training set upsampling, downsampling, whatever, but then measure it on this, you know, pure validation set which has",
    "start": "5839540",
    "end": "5845030"
  },
  {
    "text": "the right or the true prevalence ratio and see if it helped or not. And in terms of [inaudible]",
    "start": "5845030",
    "end": "5852050"
  },
  {
    "text": "Yeah, generally, AUPRC is- is, uh, that there are a few more metrics, uh, I think in the next slide.",
    "start": "5852050",
    "end": "5858840"
  },
  {
    "text": "Uh, the next slide, so probably the slide after we'll, uh, uh, will look at them, right?",
    "start": "5858910",
    "end": "5864695"
  },
  {
    "text": "So you can extend all of these to a multi-class, uh, classification setting as well. So the confusion matrix in a multiclass will be, uh,",
    "start": "5864695",
    "end": "5872150"
  },
  {
    "text": "you know, uh, k by k or n by n, depending on the number of classes you have. So in binary, it was just, you know, um,",
    "start": "5872150",
    "end": "5881090"
  },
  {
    "text": "the confusion matrix had just four where, you know, you had two classes, two classes.",
    "start": "5881090",
    "end": "5886400"
  },
  {
    "text": "But if you're in a- in a multiclass setting, you can have one row per",
    "start": "5886400",
    "end": "5894230"
  },
  {
    "text": "predicted class and one column",
    "start": "5894230",
    "end": "5900290"
  },
  {
    "text": "per actual class, right? And then you just count, you know,",
    "start": "5900290",
    "end": "5907175"
  },
  {
    "text": "what- what- what number of class C did, you predict a C, and so on.",
    "start": "5907175",
    "end": "5912290"
  },
  {
    "text": "And here, you want your diagonals to be heavy, right?",
    "start": "5912290",
    "end": "5919580"
  },
  {
    "text": "And the reason why, uh, you know, the name confusion matrix is used is probably more, uh, illuminating here.",
    "start": "5919580",
    "end": "5927199"
  },
  {
    "text": "Because if you have high, you know, high- high values over here,",
    "start": "5927200",
    "end": "5933005"
  },
  {
    "text": "then it means the model is generally confusing, you know, Class B to Class B, right?",
    "start": "5933005",
    "end": "5938480"
  },
  {
    "text": "So, you know, the model is confused between those two classes. You know that, that's an interpretation you can, uh,",
    "start": "5938480",
    "end": "5943565"
  },
  {
    "text": "you can use, right? So, uh, some of the other metrics like accuracy can be,",
    "start": "5943565",
    "end": "5951155"
  },
  {
    "text": "um, except, um, um, accuracy can be analyzed as one versus many.",
    "start": "5951155",
    "end": "5956600"
  },
  {
    "text": "Which means you can- you can plot AUPRC curve for one class, right?",
    "start": "5956600",
    "end": "5962600"
  },
  {
    "text": "Um, you, you know, take all the- all the, uh, the predicted probabilities, see what probability was assigned to this class versus others.",
    "start": "5962600",
    "end": "5969755"
  },
  {
    "text": "You know, just- just condense your classes into just two. You can do that. Um, accuracy, you can, however, you know, get- get.",
    "start": "5969755",
    "end": "5976865"
  },
  {
    "text": "It's basically the sum of all the diagonals divided by the sum of all the- all the elements.",
    "start": "5976865",
    "end": "5983030"
  },
  {
    "text": "There are variants of the, uh, uh, ROC and the PRC curve. Uh, where there's a minor retake,",
    "start": "5983030",
    "end": "5990440"
  },
  {
    "text": "or micro averaging and macro averaging and you can extend the ROC curves in the precision-recall curve to, um, multiclass setting.",
    "start": "5990440",
    "end": "5999329"
  },
  {
    "text": "Uh, and you can also, uh, uh, there's something called a cost-sensitive learning techniques,",
    "start": "5999910",
    "end": "6006205"
  },
  {
    "text": "where you can assign some kind of a dollar value to each, um, uh, on each, um, cell in the confusion matrix.",
    "start": "6006205",
    "end": "6014679"
  },
  {
    "text": "And you can construct a loss function by, um, um,",
    "start": "6014680",
    "end": "6020035"
  },
  {
    "text": "uh- So you can- you can construct loss functions by- by, uh, absorbing these dollar values or- or any kind",
    "start": "6020035",
    "end": "6027460"
  },
  {
    "text": "of a weight into the loss function and- and train on them. So you can- you can look up cost sensitive learning techniques",
    "start": "6027460",
    "end": "6035320"
  },
  {
    "text": "to do that, right?",
    "start": "6035320",
    "end": "6042175"
  },
  {
    "text": "If- if- if the cost of, um, a false positive is a lot more than a cost negative,",
    "start": "6042175",
    "end": "6047275"
  },
  {
    "text": "then you can incorporate those kind of prior information into your loss function, right?",
    "start": "6047275",
    "end": "6054040"
  },
  {
    "text": "And finally, some, uh, some- some tips on choosing a matri-, uh, evaluation matrix.",
    "start": "6054040",
    "end": "6059980"
  },
  {
    "text": "So you will see that commonly-, you know, the problem that you're trying to solve generally fits into a few templates or few patterns, right?",
    "start": "6059980",
    "end": "6067440"
  },
  {
    "text": "So there are going to be some problems where high-precision is a hard constraint and you want to maximize",
    "start": "6067440",
    "end": "6073500"
  },
  {
    "text": "the recall as much as possible subject to this high-precision. So one way to, uh, one, uh,",
    "start": "6073500",
    "end": "6079800"
  },
  {
    "text": "one example for that would be if, um, the action that you are going to take on positive, um, examples,",
    "start": "6079800",
    "end": "6087790"
  },
  {
    "text": "um, or examples that are predicted to be a positive can have",
    "start": "6087790",
    "end": "6092815"
  },
  {
    "text": "a severe negative effect or side effect on- on if the example happened to be negative.",
    "start": "6092815",
    "end": "6100239"
  },
  {
    "text": "Then you want to have high-precision as a hard constraint, which means you want to, your model when it says positive,",
    "start": "6100240",
    "end": "6105790"
  },
  {
    "text": "it has to be positive. And subject to that hard constraint, you want to improve your recall as much as possible, right?",
    "start": "6105790",
    "end": "6112675"
  },
  {
    "text": "And, um, for example, you know, um, uh, search engine results, right? When you- when you search for, um,",
    "start": "6112675",
    "end": "6118990"
  },
  {
    "text": "something on, let's say Google, right? What's going to show 1up on the top? It better be relative, it better be, you know, correct.",
    "start": "6118990",
    "end": "6125845"
  },
  {
    "text": "You may miss out a few search results, but showing irrelevant- irrelevant results on the top is- is really,",
    "start": "6125845",
    "end": "6132130"
  },
  {
    "text": "really bad, as opposed to missing out some relevant results. In those cases, high-precision is- is- is- is- is a hard constraint.",
    "start": "6132130",
    "end": "6140335"
  },
  {
    "text": "And you want to, um, uh, recall as, you want to improve your recall subject to this constraint.",
    "start": "6140335",
    "end": "6145645"
  },
  {
    "text": "Similarly, if you're writing a grammar correction software, then, you know, if you miss out a few corrections, that is fine,",
    "start": "6145645",
    "end": "6151540"
  },
  {
    "text": "but making a bad suggestion is a lot, lot worse, right? And in those cases, you want- the metric that you want to",
    "start": "6151540",
    "end": "6159310"
  },
  {
    "text": "use might not be any one of the metric, but, you know, um, the metric could be recall at a certain precision,",
    "start": "6159310",
    "end": "6166555"
  },
  {
    "text": "which means you have, you know, a precision-recall curve that looks like this.",
    "start": "6166555",
    "end": "6171595"
  },
  {
    "text": "You fix your- your, uh, um,",
    "start": "6171595",
    "end": "6177290"
  },
  {
    "text": "you fix your precision level to be some acceptable threshold and",
    "start": "6177930",
    "end": "6184540"
  },
  {
    "text": "you measure what the recall was at that fixed precision level.",
    "start": "6184540",
    "end": "6190480"
  },
  {
    "text": "And then you want to optimize your recall, right? So this length will now be your metric, right?",
    "start": "6190480",
    "end": "6196000"
  },
  {
    "text": "The recall at some fixed precision. On the other hand, you can have, um, other kinds of problems.",
    "start": "6196000",
    "end": "6202090"
  },
  {
    "text": "For example, medical diagnostics, where you want to- to, um, let's say you have some kind of a screening test",
    "start": "6202090",
    "end": "6208525"
  },
  {
    "text": "where let's say it's a blood test and if it's, you know, if it's positive, then you want to send them to a- a more expensive,",
    "start": "6208525",
    "end": "6214600"
  },
  {
    "text": "more accurate test, you know, and so let's say you are- you need to build some kind of a- a- a simple- simple screening test.",
    "start": "6214600",
    "end": "6221710"
  },
  {
    "text": "And in those cases, it would be very bad if you missed patients who actually had a condition.",
    "start": "6221710",
    "end": "6229375"
  },
  {
    "text": "In which case false negatives are a lot worse. But it's okay to have a few false positives because you're going to",
    "start": "6229375",
    "end": "6235345"
  },
  {
    "text": "conduct this more expensive test anyways on those who- whom you flag. In those kind of scenarios,",
    "start": "6235345",
    "end": "6240535"
  },
  {
    "text": "you want to maximize your precision subject to a high recall, right? You don't want to lose out,",
    "start": "6240535",
    "end": "6246130"
  },
  {
    "text": "um, uh, patients who have a condition in the screening test, right? But at the same time, you know, um,",
    "start": "6246130",
    "end": "6252355"
  },
  {
    "text": "subject to this, you know, 100% recall constraint. You still want to improve your precision because you do want, I mean,",
    "start": "6252355",
    "end": "6259179"
  },
  {
    "text": "you do want to waste the more expensive test on people who may not have the condition. So in those cases,",
    "start": "6259180",
    "end": "6264535"
  },
  {
    "text": "you may want to have a fixed recall. Okay. Fix the recall, and maximize the precision, right?",
    "start": "6264535",
    "end": "6272230"
  },
  {
    "text": "In that case, this height will be the metric that you care about, right? So you can- you can- you can construct these derived metrics from",
    "start": "6272230",
    "end": "6280510"
  },
  {
    "text": "the precision-recall curve depending on what's- what's important for your application. And sometimes they may be capacity constraint, right?",
    "start": "6280510",
    "end": "6287760"
  },
  {
    "text": "Let's say you want to- you want to, um, um, you want among- among the patients that are admitted today in the hospital,",
    "start": "6287760",
    "end": "6295270"
  },
  {
    "text": "let's say you wanna take some kind of, um, you know, a different action on, say,",
    "start": "6295270",
    "end": "6301140"
  },
  {
    "text": "five of them because you have only five doctors available to say test this new procedure,",
    "start": "6301140",
    "end": "6306900"
  },
  {
    "text": "for example, in those case, K is a hard constraint, your capacity is a hard constraint, right? In- in- in those cases,",
    "start": "6306900",
    "end": "6313210"
  },
  {
    "text": "all what matters is the quality of the top-K results, right? You want- you want high-",
    "start": "6313210",
    "end": "6319375"
  },
  {
    "text": "high-precision in the top-K. You don't really care what- what happened here, right? Because you have this hard capacity constraint.",
    "start": "6319375",
    "end": "6324969"
  },
  {
    "text": "Even if you're losing out other positives, you don't- you don't care because anybody who did not have the capacity to do anything about them, right?",
    "start": "6324970",
    "end": "6332235"
  },
  {
    "text": "In those cases, just look at the precision in the top-K. So rank your examples.",
    "start": "6332235",
    "end": "6337930"
  },
  {
    "text": "Just look at the top-K ranks and see what the precision was in- in- in that fraction. Okay. And there are many,",
    "start": "6337930",
    "end": "6343840"
  },
  {
    "text": "you know, similar to this, there are many other derived- derived metrics that's something that you should think about and decide what meaningful metric,",
    "start": "6343840",
    "end": "6352030"
  },
  {
    "text": "what- what a metric is meaningful for the application that you're doing, right?",
    "start": "6352030",
    "end": "6357400"
  },
  {
    "text": "And similarly, um, you know, to- to- to choose- choose a threshold,",
    "start": "6357400",
    "end": "6362710"
  },
  {
    "text": "um, you're going to use, you know, the threshold that you choose for classification would also be based on this.",
    "start": "6362710",
    "end": "6368770"
  },
  {
    "text": "Supposing you have a fixed, um, um, a fixed recall, you- you want a- a fixed recall of certain level, you know,",
    "start": "6368770",
    "end": "6376090"
  },
  {
    "text": "go up to the curve and- and work backwards and see what the threshold was that got you this point.",
    "start": "6376090",
    "end": "6383270"
  },
  {
    "text": "All right, that- that's about it from, uh, for the evaluation metrics.",
    "start": "6387120",
    "end": "6392500"
  },
  {
    "text": "Uh, we'll wrap up with that. Any other questions with what we've seen so far?",
    "start": "6392500",
    "end": "6399410"
  },
  {
    "text": "If not, I'll- I'll- I'll be around here, you know, walk up and, you know, ask any questions you have, and- and, uh,",
    "start": "6401340",
    "end": "6406825"
  },
  {
    "text": "that's it for today. We'll break. [NOISE]",
    "start": "6406825",
    "end": "6418000"
  }
]