[
  {
    "start": "0",
    "end": "5290"
  },
  {
    "text": "Welcome, everyone. This is Natural\nLanguage Understanding. It is a weird, and\nwonderful, and maybe worrying",
    "start": "5290",
    "end": "13690"
  },
  {
    "text": "moment to be doing Natural\nLanguage Understanding. My goal for today is just\nto immerse us in this moment",
    "start": "13690",
    "end": "20930"
  },
  {
    "text": "and think about how we\ngot here and what it's like to be doing research now. And I think that\nwill set us up well",
    "start": "20930",
    "end": "27340"
  },
  {
    "text": "to think about what we're\ngoing to do in the course and how that's\ngoing to set you up to participate in\nthis moment in AI",
    "start": "27340",
    "end": "35620"
  },
  {
    "text": "in many ways in whichever\nways you choose. And it's an especially impactful\nmoment to be doing that,",
    "start": "35620",
    "end": "40870"
  },
  {
    "text": "and this is a\nproject-oriented course, and I feel like we can get\nyou all to the point where you are doing meaningful\nthings that contribute",
    "start": "40870",
    "end": "48460"
  },
  {
    "text": "to this ongoing moment\nin ways that are going to be exciting and impactful. That is the fundamental\ngoal of the course.",
    "start": "48460",
    "end": "55872"
  },
  {
    "text": "Let's now think about\nthe current moment. This is always a moment\nof reflection for me. I started teaching this\ncourse in 2012, which I guess",
    "start": "55872",
    "end": "65360"
  },
  {
    "text": "is ages ago now. It feels recent in\nmy lived experience, but it does feel like ages\nago in terms of the content.",
    "start": "65360",
    "end": "71510"
  },
  {
    "text": "In 2012, on the first day, I had\na slide that looked like this. I said it was an exciting time\nto be doing natural language",
    "start": "71510",
    "end": "78829"
  },
  {
    "text": "understanding research. I noted that there was\na resurgence of interest in the area after a long\nperiod of people mainly focused",
    "start": "78830",
    "end": "86329"
  },
  {
    "text": "on syntax and things like that. But there was a\nwidespread perception that NLU was poised for a\nbreakthrough and to have",
    "start": "86330",
    "end": "94100"
  },
  {
    "text": "huge impact. That was relating\nto business things and that there was a white-hot\njob market for Stanford grads.",
    "start": "94100",
    "end": "100250"
  },
  {
    "text": "A lot of this language\nis coming from the fact that we were in this moment\nwhen Siri had just launched,",
    "start": "100250",
    "end": "105530"
  },
  {
    "text": "Watson had just won\non Jeopardy, and we had all of these in-home\ndevices and all the tech giants",
    "start": "105530",
    "end": "111409"
  },
  {
    "text": "kind of competing\non what was emerging as the field of natural\nlanguage understanding.",
    "start": "111410",
    "end": "117080"
  },
  {
    "text": "Let's fast forward to 2022. I did feel like I should\nupdate that in 2022 by saying this is the most\nexciting moment ever as opposed",
    "start": "117080",
    "end": "124790"
  },
  {
    "text": "to just being an exciting time. But I emphasize the\nsame things, right? We were on in this\nfeeling that we",
    "start": "124790",
    "end": "131060"
  },
  {
    "text": "had experienced a resurgence\nof interest in the area, although now it was\nhyper intensified.",
    "start": "131060",
    "end": "137180"
  },
  {
    "text": "Same thing with industry. The industry interest\nat this point makes the stuff from 2012\nlook like small potatoes.",
    "start": "137180",
    "end": "144740"
  },
  {
    "text": "Systems were getting\nvery impressive, but and I maintain this here,\nthey show their weaknesses",
    "start": "144740",
    "end": "150890"
  },
  {
    "text": "very quickly. And the core things about\nNLU remain far from solved. So the big breakthroughs\nlie in the future.",
    "start": "150890",
    "end": "158000"
  },
  {
    "text": "I will say that\neven since 2022 it has felt like there has\nbeen an acceleration, and some problems that\nwe used to focus on",
    "start": "158000",
    "end": "165860"
  },
  {
    "text": "feel like they're less pressing. I won't say solved,\nbut they feel like we've made\na lot of progress",
    "start": "165860",
    "end": "171680"
  },
  {
    "text": "on them as a result of\nmodels getting better. But all that means\nfor me is that there",
    "start": "171680",
    "end": "177590"
  },
  {
    "text": "are more exciting\nthings in the future that we can tackle even\nmore ambitious things. And you'll see that I've\ntried to overhaul the course",
    "start": "177590",
    "end": "184280"
  },
  {
    "text": "to be ever more ambitious\nabout the kind of problems that we might take on.",
    "start": "184280",
    "end": "190269"
  },
  {
    "text": "But we do live in a golden\nage for all of this stuff. And even in 2022,\nI'm not sure what",
    "start": "190270",
    "end": "195280"
  },
  {
    "text": "I would have predicted\nto say nothing of 2012 that we would have these\nincredible models like DALL-E 2",
    "start": "195280",
    "end": "201040"
  },
  {
    "text": "which can take you from text\ninto these incredible images. Language models which\nwill more or less be",
    "start": "201040",
    "end": "206980"
  },
  {
    "text": "the star of the quarter for us. But also models\nthat can take you from natural language\nto code, and of course,",
    "start": "206980",
    "end": "214010"
  },
  {
    "text": "we are all seeing\nright now as we speak that the entire\nindustry related to web search",
    "start": "214010",
    "end": "219310"
  },
  {
    "text": "is being reshaped\naround NLU technologies. So whereas this felt\nlike a niche area of NLP",
    "start": "219310",
    "end": "228520"
  },
  {
    "text": "when we started this\ncourse in 2012, now it feels like the\nentire field of NLP.",
    "start": "228520",
    "end": "234010"
  },
  {
    "text": "Certainly in some\naspects all of AI is focused on these questions of\nnatural language understanding,",
    "start": "234010",
    "end": "240280"
  },
  {
    "text": "which is exciting for us. One more moment of\nreflection here.",
    "start": "240280",
    "end": "245739"
  },
  {
    "text": "In this course,\nthroughout the years, we have used simple examples\nto kind of highlight the weaknesses of\ncurrent models.",
    "start": "245740",
    "end": "252620"
  },
  {
    "text": "And so a classic one for us\nwas simply this question, which US states border no US states?",
    "start": "252620",
    "end": "259489"
  },
  {
    "text": "And the idea here is that\nit's a simple question, but it can be hard for\nour language technologies",
    "start": "259490",
    "end": "265070"
  },
  {
    "text": "because of that\nnegation, the \"no\" there. In 1980, there was a famous\nsystem called Chat-80.",
    "start": "265070",
    "end": "273229"
  },
  {
    "text": "It was a symbolic\nsystem representing the first major phase\nof research in NLP.",
    "start": "273230",
    "end": "278840"
  },
  {
    "text": "You can see the fragment\nof the system here. And Chat-80 was an\nincredible system in that it could\nanswer questions like,",
    "start": "278840",
    "end": "285650"
  },
  {
    "text": "which country bordering\nthe Mediterranean borders a country that is bordered\nby a country whose population exceeds the\npopulation of India?",
    "start": "285650",
    "end": "292880"
  },
  {
    "text": "I've given you the answer here,\nTurkey, at least according to 1980s geography.",
    "start": "292880",
    "end": "299000"
  },
  {
    "text": "But if you asked Chat-80\na simple question like, which US states\nborder no US states?",
    "start": "299000",
    "end": "304040"
  },
  {
    "text": "It would just say,\nI don't understand. It was an incredibly\nexpressive system but rigid.",
    "start": "304040",
    "end": "310880"
  },
  {
    "text": "It could do some things\nvery deeply as you see from the first question. But things that fell\noutside of its capacity,",
    "start": "310880",
    "end": "317479"
  },
  {
    "text": "it would just fall down flat. That was the 1980s. Let's fast forward. 2009, around the\ntime this course",
    "start": "317480",
    "end": "324590"
  },
  {
    "text": "launched, Wolfram\nAlpha hit the scene. And this was meant to be\na revolutionary language",
    "start": "324590",
    "end": "330050"
  },
  {
    "text": "technology. The website is still\nup and to my amazement it still gives the\nfollowing behavior.",
    "start": "330050",
    "end": "336440"
  },
  {
    "text": "If you search for which US\nstates border no US states, it kind of just gives you a\nlist of the US states revealing,",
    "start": "336440",
    "end": "344360"
  },
  {
    "text": "I would say, that it has\nno capacity to understand the question posed.",
    "start": "344360",
    "end": "349550"
  },
  {
    "text": "That was 2009. So we've gone from 1980 to 2009. OK. Let's go to 2020.",
    "start": "349550",
    "end": "355490"
  },
  {
    "text": "This is the first of\nthe OpenAI models, Ada. Which US states\nborder no US states?",
    "start": "355490",
    "end": "362070"
  },
  {
    "text": "The answer is no. And then it sort of starts to\nbabble the US border is not a state border.",
    "start": "362070",
    "end": "367610"
  },
  {
    "text": "It did that for\na very long time. What about Babbage? This is still 2020.",
    "start": "367610",
    "end": "373640"
  },
  {
    "text": "The US states\nborder no US states. What is the name\nof the US state? And then it really went off\nthe deep end from there, again,",
    "start": "373640",
    "end": "380370"
  },
  {
    "text": "for a very long time. That was Babbage. If you had seen this output,\nwell, at least for me,",
    "start": "380370",
    "end": "385770"
  },
  {
    "text": "it might have shaken\nmy faith that this was a viable approach, right? But the team persisted, I guess.",
    "start": "385770",
    "end": "392129"
  },
  {
    "text": "2021, this is the Curie model. Which US states\nborder no US states? It had a problem that it started\nlisting things, but it did",
    "start": "392130",
    "end": "400110"
  },
  {
    "text": "say Alaska, Hawaii,\nand Puerto Rico, which is an interestingly\nmore impressive answer",
    "start": "400110",
    "end": "405930"
  },
  {
    "text": "than the first answer, right? It still has some\nproblem understanding what it means to\nrespond, but it's looking",
    "start": "405930",
    "end": "411480"
  },
  {
    "text": "like we're seeing some signal. Davinci-instruct-beta. This is 2022.",
    "start": "411480",
    "end": "416940"
  },
  {
    "text": "It's important, I\nthink, that this is the first of the models\nthat have instruct in the name. We'll talk about\nthat in a minute.",
    "start": "416940",
    "end": "422730"
  },
  {
    "text": "Which US states\nborder no US states? Alaska and Hawaii. From 2020 to 2022, we have\nseen this astounding leap",
    "start": "422730",
    "end": "430860"
  },
  {
    "text": "forward making\neverything before then sort of pale in comparison. And then finally,\ntext-davinci-001.",
    "start": "430860",
    "end": "437610"
  },
  {
    "text": "One of the new best\nin-class models at least until two months ago. Which US states\nborder no US states?",
    "start": "437610",
    "end": "443280"
  },
  {
    "text": "Alaska and Hawaii are\nthe only US states that border no other US states. A very impressive answer indeed.",
    "start": "443280",
    "end": "449520"
  },
  {
    "text": "And if you just think about\nthe little history I've given, a microcosm of what is happening\nin the field a lot of time",
    "start": "449520",
    "end": "458460"
  },
  {
    "text": "without much progress\nwith some hype attached. And now in the last few years\nthis rapid progress forward.",
    "start": "458460",
    "end": "467730"
  },
  {
    "text": "And that's just one example,\nbut these examples multiply, and we can quantify this. Here's another impressive case.",
    "start": "467730",
    "end": "474030"
  },
  {
    "text": "I asked the Davinci-2\nmodel, in which year was Stanford University\nfounded, when did it",
    "start": "474030",
    "end": "479520"
  },
  {
    "text": "enroll its first students,\nwho is its current president, and what is its mascot? A complicated question\nindeed, and it",
    "start": "479520",
    "end": "486090"
  },
  {
    "text": "gave a fluent and factually\ncorrect answer on all counts.",
    "start": "486090",
    "end": "491370"
  },
  {
    "text": "This is the\nDavinci-3 model which was best in class\nuntil a few weeks ago, and it gave exactly\nthe same answer.",
    "start": "491370",
    "end": "498420"
  },
  {
    "text": "Very impressive. Now, in this course, you'll\nsee at the website one",
    "start": "498420",
    "end": "504000"
  },
  {
    "text": "of the readings we've suggested\nfor the start of the course is this classic paper by Hector\nLevesque called \"On Our Best",
    "start": "504000",
    "end": "510420"
  },
  {
    "text": "Behavior\". And the thrust of this\narticle essentially channeling Terry Winograd and\nTerry Winograd's schema.",
    "start": "510420",
    "end": "517799"
  },
  {
    "text": "The idea is that\nwe should come up with examples that will\ntest whether models deeply",
    "start": "517799",
    "end": "522959"
  },
  {
    "text": "understand, and\nin particular, get past the simple\nmemorization of statistics",
    "start": "522960",
    "end": "528490"
  },
  {
    "text": "and other things about the\ndata they're trained on and really probe to see\nwhether they understand what the world is like.",
    "start": "528490",
    "end": "534100"
  },
  {
    "text": "And Levesque and Winograd's\ntechnique for doing this is to pose very unlikely\nquestions where humans",
    "start": "534100",
    "end": "542110"
  },
  {
    "text": "have very natural answers. Like one of the ones\nLevesque poses is, could a crocodile\nrun the steeplechase?",
    "start": "542110",
    "end": "548890"
  },
  {
    "text": "Maybe it's a question you've\nnever thought about before, but you probably have a\npretty consistent answer",
    "start": "548890",
    "end": "554020"
  },
  {
    "text": "across this group. Could a crocodile\nrun the steeplechase? Here I asked another one\nof Leveque's questions,",
    "start": "554020",
    "end": "559690"
  },
  {
    "text": "are professional baseball\nplayers allowed to glue small wings onto their caps? You could think about\nthat for a second.",
    "start": "559690",
    "end": "566270"
  },
  {
    "text": "The Davinci-2 model said\nthere is no rule against it, but it is not common. And that seemed like a very\ngood answer to me at the time.",
    "start": "566270",
    "end": "575020"
  },
  {
    "text": "When the Davinci-3\nengine came out, though, this\nstarted to worry me. No, professional\nbaseball players",
    "start": "575020",
    "end": "581079"
  },
  {
    "text": "are not allowed to glue\nsmall wings onto their caps. Major League Baseball\nhas strict rules about the appearance of\nplayers' uniforms and caps,",
    "start": "581080",
    "end": "588760"
  },
  {
    "text": "and any modifications to\nthe caps are not allowed. OK. I thought I was feeling\ngood about this,",
    "start": "588760",
    "end": "595210"
  },
  {
    "text": "but now I don't even myself\nknow what the answer is. Are professional\nbaseball players allowed to glue small\nwings onto their caps?",
    "start": "595210",
    "end": "601570"
  },
  {
    "text": "We have two confident\nanswers that are contradictory\nacross two models that",
    "start": "601570",
    "end": "607149"
  },
  {
    "text": "are very closely related. Starting to worry\nus a little bit I hope but still\nit's impressive.",
    "start": "607150",
    "end": "613610"
  },
  {
    "text": "[INAUDIBLE] What's that? You want me to ask Bard? Nah, I'm just kidding. You could check. Yes, I have a few\ncases, and this",
    "start": "613610",
    "end": "619570"
  },
  {
    "text": "is an interesting experiment\nfor us to run for sure. Let me show you the\nresponses I got a bit later.",
    "start": "619570",
    "end": "626330"
  },
  {
    "text": "The point, though,\nI guess, if you've seen the movie\nBlade Runner, this is starting to feel\nlike to figure out",
    "start": "626330",
    "end": "631629"
  },
  {
    "text": "whether an agent we were\ninteracting with was human or AI. We would need to get very\nsophisticated interview",
    "start": "631630",
    "end": "639820"
  },
  {
    "text": "techniques indeed. The Turing test\nlong forgotten here. Now we're into the mode of\ntrying to figure out exactly",
    "start": "639820",
    "end": "647410"
  },
  {
    "text": "what kind of agents we're\ninteracting with by having to be extremely clever\nabout the kinds of things",
    "start": "647410",
    "end": "653170"
  },
  {
    "text": "that we do with them. Now, that's kind of\nanecdotal evidence,",
    "start": "653170",
    "end": "658420"
  },
  {
    "text": "but I think that the\npicture of progress is also supported by what's\nhappening in the field.",
    "start": "658420",
    "end": "663820"
  },
  {
    "text": "Let me start this story\nwith our benchmarks. And the headline here is that\nour benchmarks, the tasks,",
    "start": "663820",
    "end": "670029"
  },
  {
    "text": "the data sets we use\nto probe our models are saturating faster\nthan ever before,",
    "start": "670030",
    "end": "675279"
  },
  {
    "text": "and I'll articulate\nwhat I mean by saturate. So we have a little framework. Along the x-axis I have time\nstretching back into the 1990s.",
    "start": "675280",
    "end": "684519"
  },
  {
    "text": "And along the y-axis I have a\nnormalized measure of distance from what we call\nhuman performance.",
    "start": "684520",
    "end": "691370"
  },
  {
    "text": "That's the red line set at 0. Each one of these benchmarks\nhas in its own particular way",
    "start": "691370",
    "end": "697160"
  },
  {
    "text": "set a so-called estimate\nof human performance. I think we should be cynical\nabout that but nonetheless this",
    "start": "697160",
    "end": "703339"
  },
  {
    "text": "will be a kind of marker\nof progress for us. First data set, MNIST.",
    "start": "703340",
    "end": "708620"
  },
  {
    "text": "This is like digit recognition. Famous task in AI. It was launched in the 1990s,\nand it took about 20 years",
    "start": "708620",
    "end": "715580"
  },
  {
    "text": "for us to see a system that\nsurpassed human performance in this very loose sense.",
    "start": "715580",
    "end": "721610"
  },
  {
    "text": "The Switchboard Corpus this\nis going from speech to text. It's a very similar story.",
    "start": "721610",
    "end": "726980"
  },
  {
    "text": "Launched in the '90s, and it\ntook about 20 years for us to see a superhuman system.",
    "start": "726980",
    "end": "733220"
  },
  {
    "text": "ImageNet-- this was\nlaunched I believe in 2009, and it took less than 10 years\nfor us to see a system that",
    "start": "733220",
    "end": "740540"
  },
  {
    "text": "surpassed that red line. And now progress is going\nto pick up really fast. SQuAD 1.1, the Stanford\nQuestion Answering Dataset",
    "start": "740540",
    "end": "748970"
  },
  {
    "text": "was launched in 2016, and it\ntook about three years for it to be saturated in this sense.",
    "start": "748970",
    "end": "755030"
  },
  {
    "text": "SQuAD 2.0 was the team's attempt\nto pose an even harder problem,",
    "start": "755030",
    "end": "760160"
  },
  {
    "text": "one where there were\nunanswerable questions, but it took even\nless time for systems",
    "start": "760160",
    "end": "765200"
  },
  {
    "text": "to get past that red line. Then we get the GLUE benchmark. This is a famous benchmark in\nnatural language understanding,",
    "start": "765200",
    "end": "772880"
  },
  {
    "text": "a multitask benchmark. When this was\nlaunched, a lot of us thought that GLUE would be\ntoo difficult for present day",
    "start": "772880",
    "end": "781080"
  },
  {
    "text": "systems. It looked like this\nmight be a challenge that would stand for\na very long time, but it took like less\nthan a year for systems",
    "start": "781080",
    "end": "788450"
  },
  {
    "text": "to pass human performance. The response was SuperGLUE, but\nit was saturated, if anything,",
    "start": "788450",
    "end": "795030"
  },
  {
    "text": "even more quickly. Now, we can be as\ncynical as we want about this notion of\nhuman performance,",
    "start": "795030",
    "end": "800899"
  },
  {
    "text": "and I think we should\ndwell on whether or not it's fair to call it that. But even setting\nthat aside, this",
    "start": "800900",
    "end": "806990"
  },
  {
    "text": "looks like undeniably\na story of progress. The systems that we had\nin 2012 would not even",
    "start": "806990",
    "end": "814700"
  },
  {
    "text": "have been able to enter the\nGLUE benchmark to say nothing of achieving scores like this.",
    "start": "814700",
    "end": "819770"
  },
  {
    "text": "So something meaningful\nhas happened. Now, you might think,\nby the standards of AI these data sets are kind of old.",
    "start": "819770",
    "end": "826340"
  },
  {
    "text": "Here's a post from Jason\nWei where he evaluated our latest and greatest\nlarge language models",
    "start": "826340",
    "end": "831620"
  },
  {
    "text": "on a bunch of mostly new\ntasks that were actually designed to stress test this\nnew class of very large language",
    "start": "831620",
    "end": "838970"
  },
  {
    "text": "models. And Jason's observation is\nthat we see emergent abilities across more than 100\ntasks for these models,",
    "start": "838970",
    "end": "846030"
  },
  {
    "text": "especially for our\nlargest models. The point though,\nis that we again thought these tasks would\nstand for a very long time,",
    "start": "846030",
    "end": "853260"
  },
  {
    "text": "and what we're seeing instead\nis that one by one systems are certainly getting\ntraction and in some cases",
    "start": "853260",
    "end": "859130"
  },
  {
    "text": "performing at the standard\nwe had set for humans. Again, an incredible\nstory of progress there.",
    "start": "859130",
    "end": "868459"
  },
  {
    "text": "So I hope that is energizing,\nmaybe a little intimidating, but I hope fundamentally\nenergizing for you all.",
    "start": "868460",
    "end": "875709"
  },
  {
    "text": "The next question that I want\nto ask for you is just what is going on? What is driving all of\nthis sudden progress?",
    "start": "875710",
    "end": "883480"
  },
  {
    "text": "Let's get a feel for\nthat, and that'll kind of serve as the foundation\nfor the course itself.",
    "start": "883480",
    "end": "888790"
  },
  {
    "text": "Before I do that though, are\nthere questions or comments? Things I could\nresolve or things I",
    "start": "888790",
    "end": "894370"
  },
  {
    "text": "left out about the\ncurrent moment? ",
    "start": "894370",
    "end": "900308"
  },
  {
    "text": "Right on par. You did very well. We should reflect though\nmaybe as a group about what",
    "start": "900308",
    "end": "906700"
  },
  {
    "text": "it means to do very well. My question for you when\nyou say it did well, what is the Major League\nBaseball rule about players",
    "start": "906700",
    "end": "914020"
  },
  {
    "text": "gluing things onto their caps? Rule 3.06. You found the actual rule. No, this is what\nBard-- well, I don't--",
    "start": "914020",
    "end": "921593"
  },
  {
    "text": "Did you find the rule? I didn't find the rule. Bard found that rule\nand gave me that number. OK. Is it accurate? Yes, that is going to\nbe the question for us.",
    "start": "921593",
    "end": "928690"
  },
  {
    "text": "I can get-- It's a direct quote too, which\nis ripe for hallucination. Well, I'm going to show you\nthe OpenAI models will offer me",
    "start": "928690",
    "end": "934990"
  },
  {
    "text": "links, but the links go nowhere. [LAUGHTER] What you're pointing\nout, I think,",
    "start": "934990",
    "end": "941740"
  },
  {
    "text": "is an increasing\nsocietal problem. These models are offering\nus what looks like evidence,",
    "start": "941740",
    "end": "947079"
  },
  {
    "text": "but a lot of the evidence\nis just fabricated. And this is worse than\noffering no evidence at all.",
    "start": "947080",
    "end": "952480"
  },
  {
    "text": "What I really need\nis someone who knows Major League\nBaseball to tell me what is the rule about\nplayers and their caps.",
    "start": "952480",
    "end": "959750"
  },
  {
    "text": "I want it from an expert human,\nnot an expert language model. Can we google?",
    "start": "959750",
    "end": "966000"
  },
  {
    "text": "What's that? Can we google? Be careful how\nyou google though. I guess that's the\nlesson of 2023.",
    "start": "966000",
    "end": "971130"
  },
  {
    "text": "[LAUGHTER]  All right, what's going on?",
    "start": "971130",
    "end": "976649"
  },
  {
    "text": "Let's start to make\nsome progress on this. Again, first, a little\nbit of historical context.",
    "start": "976650",
    "end": "981950"
  },
  {
    "text": "I've got a timeline going back\nto the 1960s along the x-axis. This is more or less the\nstart of the field itself.",
    "start": "981950",
    "end": "989010"
  },
  {
    "text": "And in that early\nera, essentially all of the approaches were\nbased in symbolic algorithms",
    "start": "989010",
    "end": "995990"
  },
  {
    "text": "like the Chat-80 one\nthat I showed you. In fact, that was\npioneered here at Stanford by people who were pioneering\nthe very field of AI.",
    "start": "995990",
    "end": "1003880"
  },
  {
    "text": "And that paradigm of essentially\nprogramming these systems lasted well into the 1980s.",
    "start": "1003880",
    "end": "1011800"
  },
  {
    "text": "In the '90s, early 2000s, we\nget the statistical revolution",
    "start": "1011800",
    "end": "1017080"
  },
  {
    "text": "throughout artificial\nintelligence and then in turn in natural\nlanguage processing. And the big change there is that\ninstead of programming systems",
    "start": "1017080",
    "end": "1025060"
  },
  {
    "text": "with all these\nrules, we're going to design machine learning\nsystems that are going to try to learn from data.",
    "start": "1025060",
    "end": "1030280"
  },
  {
    "text": "Under the hood there was still\na lot of programming involved because we would write a\nlot of feature functions",
    "start": "1030280",
    "end": "1035650"
  },
  {
    "text": "that were little programs that\nwould help us detect things about data, and we would hope\nthat our machine learning",
    "start": "1035650",
    "end": "1040929"
  },
  {
    "text": "systems could learn from\nthe output of those feature functions. But in the end, this was the\nrise of the fully data-driven",
    "start": "1040930",
    "end": "1048730"
  },
  {
    "text": "learning systems. And we just hope that some\nprocess of optimization leads us to new capabilities.",
    "start": "1048730",
    "end": "1055910"
  },
  {
    "text": "The next big phase of this was\nthe deep learning revolution. This happened starting\naround 2009, 2010.",
    "start": "1055910",
    "end": "1062530"
  },
  {
    "text": "Again, Stanford was at the\nforefront of this to be sure. It felt like a big\nchange at the time,",
    "start": "1062530",
    "end": "1068230"
  },
  {
    "text": "but in retrospect,\nthis is kind of not so different from\nthis mode here. It's just that we now\nreplace that simple model",
    "start": "1068230",
    "end": "1076150"
  },
  {
    "text": "with really big models,\nreally deep models that have a tremendous capacity\nto learn things from data.",
    "start": "1076150",
    "end": "1083170"
  },
  {
    "text": "We started also to see\na shift even further away from those\nfeature functions",
    "start": "1083170",
    "end": "1088600"
  },
  {
    "text": "from writing little programs\nand more toward a mode where we would just hope that the data\nand the optimization process",
    "start": "1088600",
    "end": "1096100"
  },
  {
    "text": "could do all the work for us. Then the next big\nthing that happened,",
    "start": "1096100",
    "end": "1101450"
  },
  {
    "text": "which could take us, I\nsuppose, until about 2018, would be this mode where we have\na lot of pretrained parameters.",
    "start": "1101450",
    "end": "1108260"
  },
  {
    "text": "These are pictures of maybe\nbig language models or computer vision models or something. And when we build\nsystems, we build",
    "start": "1108260",
    "end": "1115250"
  },
  {
    "text": "on those pretrained components\nand stitch them together with these task-specific\nparameters.",
    "start": "1115250",
    "end": "1120890"
  },
  {
    "text": "And we hope that when\nthey're all combined and we do some learning on\nsome task-specific data,",
    "start": "1120890",
    "end": "1126140"
  },
  {
    "text": "we have something that's\nbenefiting from all these pretrained components. And then the mode that we\nseem to be in now that I want",
    "start": "1126140",
    "end": "1134390"
  },
  {
    "text": "us to reflect critically on\nis this mode where we're going to replace everything\nwith maybe one ginormous",
    "start": "1134390",
    "end": "1141710"
  },
  {
    "text": "language model of some\nkind and hope that thing, that enormous black box\nwill do all the work for us.",
    "start": "1141710",
    "end": "1148700"
  },
  {
    "text": "We should think critically\nabout whether that's really the path forward,\nbut it certainly feels like the\nzeitgeist to be sure.",
    "start": "1148700",
    "end": "1155960"
  },
  {
    "text": "Question, yeah. If you think it's\nworth it, could you go back to the last\nslide and maybe",
    "start": "1155960",
    "end": "1162090"
  },
  {
    "text": "explain a little bit about\nmore grounded example of what that all means? I couldn't quite follow. Let's do that later.",
    "start": "1162090",
    "end": "1168580"
  },
  {
    "text": "The point for now though is\nreally this shift from here where we're mostly learning\nfrom scratch for our task, here,",
    "start": "1168580",
    "end": "1177669"
  },
  {
    "text": "we've got things\nlike BERT in the mix. We've got pretrained components. Models that we hope\nbegin in a state",
    "start": "1177670",
    "end": "1184679"
  },
  {
    "text": "that gives us a leg up on the\nproblem we're trying to solve. That's the big\nthing that happened. And you get this\nemphasis on people",
    "start": "1184680",
    "end": "1191640"
  },
  {
    "text": "releasing model parameters. In this earlier phase\nlike here, there",
    "start": "1191640",
    "end": "1196650"
  },
  {
    "text": "was no talk of releasing\nmodel parameters because mostly the models\npeople trained were",
    "start": "1196650",
    "end": "1201750"
  },
  {
    "text": "just good for the task\nthat they had set. As we move into this era\nand then certainly this one,",
    "start": "1201750",
    "end": "1207570"
  },
  {
    "text": "these things are meant to be\nlike general purpose language capabilities, or maybe general\npurpose computer vision",
    "start": "1207570",
    "end": "1214260"
  },
  {
    "text": "capabilities that\nwe stitch together into a system that can do more\nthan any previous system could",
    "start": "1214260",
    "end": "1219924"
  },
  {
    "text": "do.  Right. So then we have\nthis big thing here.",
    "start": "1219925",
    "end": "1226550"
  },
  {
    "text": "So that's the feeling now. Behind all of this, certainly\nbeginning in this final phase",
    "start": "1226550",
    "end": "1232330"
  },
  {
    "text": "here is the transformer\narchitecture. Just may take the\ntemperature of the room. How many people have encountered\nthe transformer before?",
    "start": "1232330",
    "end": "1240530"
  },
  {
    "text": "Right. Yeah, it's sort of unavoidable\nif you're doing this research. Here's a diagram\nof it, but I am not",
    "start": "1240530",
    "end": "1245950"
  },
  {
    "text": "going to go through this\ndiagram now because starting on Wednesday we\nare going to have",
    "start": "1245950",
    "end": "1251019"
  },
  {
    "text": "an entire lecture essentially\ndevoted to unpacking this thing and understanding it. All I can say for\nyou now is that I",
    "start": "1251020",
    "end": "1257890"
  },
  {
    "text": "expect you to go on the\nfollowing journey, which all of us go on. How on earth does\nthe transformer work?",
    "start": "1257890",
    "end": "1264820"
  },
  {
    "text": "It looks very, very complicated. I hope I can get\nyou to the point where you feel, oh,\nthis is actually",
    "start": "1264820",
    "end": "1271630"
  },
  {
    "text": "pretty simple components that\nhave been combined in a pretty straightforward way. That's your second\nstep on the journey.",
    "start": "1271630",
    "end": "1277540"
  },
  {
    "text": "The one-- the true\nenlightenment comes from, wait a second, why\ndoes this work at all?",
    "start": "1277540",
    "end": "1283190"
  },
  {
    "text": "And then you're with\nthe entire field trying to understand why\nthese simple things are",
    "start": "1283190",
    "end": "1288290"
  },
  {
    "text": "brought together in this\nway have proved so powerful. The other major\nthing that happened,",
    "start": "1288290",
    "end": "1295590"
  },
  {
    "text": "which is latent going all the\nway back to the start of AI, especially as it\nrelates to linguistics,",
    "start": "1295590",
    "end": "1301940"
  },
  {
    "text": "is this notion of\nself-supervision of distributional learning. Because this is going\nto unlock the door",
    "start": "1301940",
    "end": "1308360"
  },
  {
    "text": "to us just learning\nfrom the world in the most general sense. In self-supervision,\nyour model's only goal",
    "start": "1308360",
    "end": "1316640"
  },
  {
    "text": "is to learn from co-occurrence\npatterns in the sequences that it's trained on. And the sequences\ncan be language,",
    "start": "1316640",
    "end": "1323180"
  },
  {
    "text": "but they could be language plus\nsensor readings, computer code, maybe even images that you embed\nin this space, just symbols.",
    "start": "1323180",
    "end": "1331130"
  },
  {
    "text": "And the model's only\ngoal is to learn from the distributional\npatterns that they contain.",
    "start": "1331130",
    "end": "1336410"
  },
  {
    "text": "Or for many of these models,\nto assign high probability to the attested sequences in\nwhatever data that you pour in.",
    "start": "1336410",
    "end": "1343860"
  },
  {
    "text": "For this kind of learning we\ndon't need to do any labeling. All we need to do is have lots\nand lots of symbol streams.",
    "start": "1343860",
    "end": "1353490"
  },
  {
    "text": "And then when we generate\nfrom these models, we're sampling from them. And that's what we all think\nof when we think of prompting",
    "start": "1353490",
    "end": "1358988"
  },
  {
    "text": "and getting a response back. But the underlying mechanism\nis, at least in part, this notion of self-supervision.",
    "start": "1358988",
    "end": "1365273"
  },
  {
    "text": "And I'll emphasize\nagain because I think this is really important\nfor why these models are so powerful, the symbols do\nnot need to be just language.",
    "start": "1365273",
    "end": "1372600"
  },
  {
    "text": "They can include\nlots of other things that might help a\nmodel piece together",
    "start": "1372600",
    "end": "1377789"
  },
  {
    "text": "a full picture of the\nworld we live in, and also the connections between\nlanguage and those pieces",
    "start": "1377790",
    "end": "1383340"
  },
  {
    "text": "of the world, just from this\ndistributional learning. The result of this\nproving so powerful",
    "start": "1383340",
    "end": "1390570"
  },
  {
    "text": "is the advent of large scale\npretraining because now we're not held back anymore by\nthe need for labeled data,",
    "start": "1390570",
    "end": "1398230"
  },
  {
    "text": "all we need is lots of data\nin unstructured format. This really begins in the era\nof static word representations",
    "start": "1398230",
    "end": "1406140"
  },
  {
    "text": "like Word2Vec and GloVe. And in fact, those\nteams and I would say especially the\nGloVe team they",
    "start": "1406140",
    "end": "1412080"
  },
  {
    "text": "were really visionary\nin the sense that they not only\nreleased a paper and code",
    "start": "1412080",
    "end": "1419320"
  },
  {
    "text": "but pretrained parameters. This was really brand\nnew for the field. This idea that you would empower\npeople with model artifacts,",
    "start": "1419320",
    "end": "1428710"
  },
  {
    "text": "and people started\nusing them as the inputs to recurrent neural\nnetworks and other things.",
    "start": "1428710",
    "end": "1434529"
  },
  {
    "text": "And you started\nto see pretraining as an important\ncomponent to doing really",
    "start": "1434530",
    "end": "1439809"
  },
  {
    "text": "well at hard things. There were some predecessors\nthat I'll talk about next time,",
    "start": "1439810",
    "end": "1446270"
  },
  {
    "text": "but the really big moment for\ncontextual representations is the ELMo model.",
    "start": "1446270",
    "end": "1451390"
  },
  {
    "text": "This is the paper, \"Deep\nContextualized Word Representations\". I can remember being at\nthe North American ACL",
    "start": "1451390",
    "end": "1457990"
  },
  {
    "text": "meeting in New Orleans in 2018\nat the Best Paper session. They had not announced\nwhich of the best papers",
    "start": "1457990",
    "end": "1464830"
  },
  {
    "text": "was going to win the\nOutstanding Paper Award, but we all knew it was\ngoing to be the ELMo",
    "start": "1464830",
    "end": "1469900"
  },
  {
    "text": "paper because the gains that\nthey had reported from fine tuning their ELMo parameters\non hard tasks for the field",
    "start": "1469900",
    "end": "1477250"
  },
  {
    "text": "were just mind blowing. The sort of thing that\nyou really only see once in a generation\nof this research",
    "start": "1477250",
    "end": "1483580"
  },
  {
    "text": "or so we thought because\nthe next year BERT came out.",
    "start": "1483580",
    "end": "1488929"
  },
  {
    "text": "Same thing. I think same best\npaper award thing. The paper already\nhad huge impact",
    "start": "1488930",
    "end": "1494530"
  },
  {
    "text": "by the time it was\neven published, and they too released\ntheir model parameters.",
    "start": "1494530",
    "end": "1500230"
  },
  {
    "text": "ELMo is not transformer based. BERT is the first of the\nsequence of things that's based in the transformer, and\nagain, lifting all boats even",
    "start": "1500230",
    "end": "1508210"
  },
  {
    "text": "above where ELMo had brought us. Then we get GPT. This is the first GPT paper.",
    "start": "1508210",
    "end": "1514120"
  },
  {
    "text": "And then fast forward a\nlittle bit we get GPT-3, and that was pretraining\nat a scale that",
    "start": "1514120",
    "end": "1521590"
  },
  {
    "text": "was previously unimaginable. Because this now we're talking\nabout for the BERT model",
    "start": "1521590",
    "end": "1527470"
  },
  {
    "text": "100 million parameters\nand for GPT-3 well north of 100 billion.",
    "start": "1527470",
    "end": "1532970"
  },
  {
    "text": "Different order of magnitude. And what we started to see\nis emergent capabilities.",
    "start": "1532970",
    "end": "1539260"
  },
  {
    "text": "That model size\nthing is important. Again, this is a feeling\nof progress and maybe also despair.",
    "start": "1539260",
    "end": "1545380"
  },
  {
    "text": "I think I can lift your\nspirits a little bit, but we should think\nabout model size. So I have years along\nthe x-axis again,",
    "start": "1545380",
    "end": "1553090"
  },
  {
    "text": "and I have model size going from\n100 million to 1 trillion here on a logarithmic scale.",
    "start": "1553090",
    "end": "1559300"
  },
  {
    "text": "So 2018, GPT that's\nlike 100 million. BERT I think it's 300\nmillion for the large one.",
    "start": "1559300",
    "end": "1566059"
  },
  {
    "text": "OK. GPT-2 even larger. Megatron, 8.3 billion. I remember when this came out.",
    "start": "1566060",
    "end": "1572320"
  },
  {
    "text": "I probably laughed. Maybe I thought it was a joke. I certainly thought\nit was some typo because I couldn't imagine\nthat it was actually",
    "start": "1572320",
    "end": "1579730"
  },
  {
    "text": "billion like with a B there. But now we take\nthat for granted.",
    "start": "1579730",
    "end": "1586180"
  },
  {
    "text": "Megatron, 11 billion. This is 2021 or so. Then we get GPT-3 reportedly\n175 billion parameters.",
    "start": "1586180",
    "end": "1594730"
  },
  {
    "text": "And then we get this\nthing where it seems like we're doing typos again. Megatron-Turing NLG\nwas like 500 billion,",
    "start": "1594730",
    "end": "1601580"
  },
  {
    "text": "and then PaLM is 540\nbillion parameters. And I guess there are rumors\nthat we have gone up all",
    "start": "1601580",
    "end": "1608780"
  },
  {
    "text": "the way to a trillion, right? There's an undeniable\ntrend here.",
    "start": "1608780",
    "end": "1614490"
  },
  {
    "text": "I think there is\nsomething to this trend, but we should reflect\non it a little bit. One thing I want\nto say is there's",
    "start": "1614490",
    "end": "1621680"
  },
  {
    "text": "a noteworthy pattern\nof very few entities have participated\nin this very large--",
    "start": "1621680",
    "end": "1627770"
  },
  {
    "text": "in this race for\nvery large models. We've got like Google, Nvidia,\nMeta, and OpenAI, right?",
    "start": "1627770",
    "end": "1634340"
  },
  {
    "text": "And that was actually a\nreal cause for concern. I remember being at a workshop\nbetween Stanford and OpenAI",
    "start": "1634340",
    "end": "1640730"
  },
  {
    "text": "where the number one\nsource of consternation was really that only\nOpenAI at that point",
    "start": "1640730",
    "end": "1646970"
  },
  {
    "text": "had trained these\nreally large models. And after that, predictably\nthese other large tech companies kind of caught\nup, but it was still",
    "start": "1646970",
    "end": "1654679"
  },
  {
    "text": "for a while looking like a\nstory of real centralization of power. That might still\nbe happening, but I",
    "start": "1654680",
    "end": "1661030"
  },
  {
    "text": "think there's reason\nto be optimistic. So here at Stanford,\nthe HELM group which is part of the Center for\nResearch on Foundation Models",
    "start": "1661030",
    "end": "1667840"
  },
  {
    "text": "led this incredibly\nambitious project of evaluating lots\nof language models. And one thing that\nemerges from that",
    "start": "1667840",
    "end": "1674410"
  },
  {
    "text": "is that we have a more\nhealthy ecosystem now. So we have these\nloose collectives BigScience and Eleuther\nare both fully open source",
    "start": "1674410",
    "end": "1682210"
  },
  {
    "text": "groups of researchers. We've got well, one academic\ninstitution represented.",
    "start": "1682210",
    "end": "1687442"
  },
  {
    "text": "This could be a little bit\nembarrassing for Stanford. Maybe we'll correct that. And then maybe the\nmore important thing",
    "start": "1687442",
    "end": "1692680"
  },
  {
    "text": "is that we have lots of\nstartups represented. So these are well-funded\nbut relatively small",
    "start": "1692680",
    "end": "1697900"
  },
  {
    "text": "outfits that are producing\noutstanding language models. And so the result, I think we're\ngoing to see much more of this,",
    "start": "1697900",
    "end": "1704750"
  },
  {
    "text": "and then we'll worry less\nabout centralization of power. There's plenty of other\nthings to worry about",
    "start": "1704750",
    "end": "1710420"
  },
  {
    "text": "so we shouldn't get\nsanguine about this, but this particular\npoint I think is being alleviated\nby current trends.",
    "start": "1710420",
    "end": "1716530"
  },
  {
    "text": "And there's another\naspect of this too which is we have this\nscary rise in model size",
    "start": "1716530",
    "end": "1722090"
  },
  {
    "text": "but what is happening\nright now as we speak in a very quick way is we're\nseeing a push towards smaller",
    "start": "1722090",
    "end": "1728960"
  },
  {
    "text": "models. And in particular,\nwe're seeing that models that are in the range of\nlike 10 billion parameters",
    "start": "1728960",
    "end": "1735080"
  },
  {
    "text": "can be highly performant. So we have the FLAN\nmodels, we have LLaMA,",
    "start": "1735080",
    "end": "1740570"
  },
  {
    "text": "and then here at Stanford they\nreleased the Alpaca thing, and then Databricks released\nthe Hello Dolly model.",
    "start": "1740570",
    "end": "1746700"
  },
  {
    "text": "These are all models\nthat are like 8 to 10 billion parameters, which\nI know this sounds funny because I laughed a few years\nago when the Megatron model had",
    "start": "1746700",
    "end": "1754400"
  },
  {
    "text": "8.3 billion. And now what I'm\nsaying to you is that this is relatively\nsmall but so it goes.",
    "start": "1754400",
    "end": "1760160"
  },
  {
    "text": "And the point is\nthat a 10 billion parameter model\nis one that could be run on regular old\ncommercial hardware.",
    "start": "1760160",
    "end": "1767540"
  },
  {
    "text": "Whereas these monsters\nup here, really you have lots of pressures toward\ncentralization of power",
    "start": "1767540",
    "end": "1772670"
  },
  {
    "text": "there because almost no\none can work with them. But anyone essentially\ncan work with Alpaca,",
    "start": "1772670",
    "end": "1778100"
  },
  {
    "text": "and it won't be\nlong before we've got the ability to work\nwith it on small devices and things like that.",
    "start": "1778100",
    "end": "1783860"
  },
  {
    "text": "And that too is\nreally going to open the door to lots of innovation. I think that will\nbring some good,",
    "start": "1783860",
    "end": "1790040"
  },
  {
    "text": "and I think it will\nbring some bad, but it is certainly a meaningful\nchange from this scary trend that we were seeing\nuntil four months ago.",
    "start": "1790040",
    "end": "1797840"
  },
  {
    "text": " As a result of these\nmodels being so powerful,",
    "start": "1797840",
    "end": "1805400"
  },
  {
    "text": "people started to\nrealize that you can get a lot of mileage out of\nthem simply by prompting them.",
    "start": "1805400",
    "end": "1812120"
  },
  {
    "text": "When you prompt one of\nthese very large models, you put it in a temporary\nstate by inputting some text,",
    "start": "1812120",
    "end": "1817940"
  },
  {
    "text": "and then you generate a\nsample from the model using some technique and you\nsee what comes out, right? So if you type into one of\nthese models, better late than--",
    "start": "1817940",
    "end": "1826130"
  },
  {
    "text": "it's going to probably\nspit out never. If you put in, every day,\nI eat breakfast, lunch,",
    "start": "1826130",
    "end": "1832009"
  },
  {
    "text": "and-- it will\nprobably say dinner. And you might have an intuition\nthat the reasons, the causes",
    "start": "1832010",
    "end": "1837078"
  },
  {
    "text": "for that are kind of different. The first one is a idiom. So that it could just learn from\nco-occurrence patterns in text",
    "start": "1837078",
    "end": "1843250"
  },
  {
    "text": "transparently. For the second one,\nwe interpret it as humans as reflecting\nsomething about routines.",
    "start": "1843250",
    "end": "1850420"
  },
  {
    "text": "But you should remind\nyourself that the mechanism is the same as in the first case.",
    "start": "1850420",
    "end": "1856070"
  },
  {
    "text": "This was just a bunch of\nco-occurrence patterns. A lot of people described\ntheir routines in text, and the model picked up on that.",
    "start": "1856070",
    "end": "1862960"
  },
  {
    "text": "And carry that thought forward\nas you think about things like, the president\nof the US is-- when it fills that in\nwith Biden or whoever,",
    "start": "1862960",
    "end": "1871420"
  },
  {
    "text": "it might look like it is\noffering us factual knowledge and maybe in some sense it is. But it's the same mechanism as\nfor those first two examples.",
    "start": "1871420",
    "end": "1879820"
  },
  {
    "text": "It is just learning from the\nfact that a lot of people have expressed a\nlot of texts that look like the president\nof the US is Joe Biden,",
    "start": "1879820",
    "end": "1887500"
  },
  {
    "text": "and it is repeating\nthat back to us. And so definitely if you\nask a model something like,",
    "start": "1887500",
    "end": "1893270"
  },
  {
    "text": "the key to happiness is-- you should remember\nthat this is just the aggregate of a lot of\ndata that it was trained on.",
    "start": "1893270",
    "end": "1900230"
  },
  {
    "text": "It has no particular wisdom\nto offer you necessarily beyond what was encoded latently\nin that giant sea of mostly",
    "start": "1900230",
    "end": "1911030"
  },
  {
    "text": "unaudited, unstructured text. Yeah, question. Yeah. So I guess, it would be hard\nto get something like this.",
    "start": "1911030",
    "end": "1919180"
  },
  {
    "text": "But if we have a corpus of\njust like all the languages but literally all of\nthe facts were wrong.",
    "start": "1919180",
    "end": "1925170"
  },
  {
    "text": "Like we just imagine a very\nfactually incorrect corpus. I guess I'm getting at\nhow do we inject truth",
    "start": "1925170",
    "end": "1933570"
  },
  {
    "text": "into these corpuses? It's a question that\nbears repeating,",
    "start": "1933570",
    "end": "1939050"
  },
  {
    "text": "how do we inject truth? It's a question you\nall can think about. What is truth, of course?",
    "start": "1939050",
    "end": "1944809"
  },
  {
    "text": "But also what would that mean\nand how would we achieve it? And even if we did\nback off to something",
    "start": "1944810",
    "end": "1951020"
  },
  {
    "text": "like how would we ensure\nself-consistency for a model or at the level of a\nworldview or a set of facts?",
    "start": "1951020",
    "end": "1957680"
  },
  {
    "text": "Even those questions\nwhich seem easier to pose are incredibly\ndifficult questions",
    "start": "1957680",
    "end": "1962990"
  },
  {
    "text": "in the current moment where our\nonly mechanisms are basically that self-supervision\nthing that I described",
    "start": "1962990",
    "end": "1968840"
  },
  {
    "text": "and then a little bit of\nwhat I'll talk about next. But none of the\nstructure that we",
    "start": "1968840",
    "end": "1974210"
  },
  {
    "text": "used to have where we would\nhave a database of knowledge and things like that. That is posing problems.",
    "start": "1974210",
    "end": "1979490"
  },
  {
    "text": "[CHUCKLES] ",
    "start": "1979490",
    "end": "1985130"
  },
  {
    "text": "The prompting thing, we take\nthis a step forward, right? So the GPT-3 paper, remember\nthat's that 175 billion",
    "start": "1985130",
    "end": "1991760"
  },
  {
    "text": "parameter monster. The eye opening thing\nabout that is what we now call in-context\nlearning which was just",
    "start": "1991760",
    "end": "1998870"
  },
  {
    "text": "the notion that for these very\nlarge, very capable models you could input a bunch of text.",
    "start": "1998870",
    "end": "2004300"
  },
  {
    "text": "Like here's a passage and\nmaybe an example of the kind of behavior that you wanted. And then your actual question.",
    "start": "2004300",
    "end": "2011200"
  },
  {
    "text": "And the model would\ndo a pretty good job at answering the question. And what you're doing here\nis with your context passage",
    "start": "2011200",
    "end": "2018220"
  },
  {
    "text": "and your demonstration\npushing the model to be extractive, to find\nan answer to its question",
    "start": "2018220",
    "end": "2023680"
  },
  {
    "text": "in the context passage. And then the observation\nof this paper is that they do\na pretty good job",
    "start": "2023680",
    "end": "2030039"
  },
  {
    "text": "at following that same\nbehavior for the actual target question at the bottom here. Remember, this is\nall just prompting.",
    "start": "2030040",
    "end": "2037330"
  },
  {
    "text": "Putting the model\nin a temporary state and seeing what comes out. You don't change the\nmodel, you just prompt it.",
    "start": "2037330",
    "end": "2045740"
  },
  {
    "text": "In 2012 if you had\nasked me whether this was a viable path forward\nfor a class project, I want to prompt an\nRNN or something,",
    "start": "2045740",
    "end": "2053190"
  },
  {
    "text": "I would have advised\nyou as best I could to choose some other\ntopic because I never would have guessed that this would work.",
    "start": "2053190",
    "end": "2059940"
  },
  {
    "text": "So the mind blowing thing\nabout this paper and everything that's followed is that we\nmight be nearing the point where",
    "start": "2059940",
    "end": "2067250"
  },
  {
    "text": "we can design entire\nAI systems on the basis of this simple in-context\nlearning mechanism,",
    "start": "2067250",
    "end": "2073730"
  },
  {
    "text": "transformatively different from\nanything that we saw before. In fact, let me just\nemphasize this a little bit.",
    "start": "2073730",
    "end": "2081020"
  },
  {
    "text": "It is worth dwelling\non how strange this is. For those of you who have been\nin the field a little while,",
    "start": "2081020",
    "end": "2087739"
  },
  {
    "text": "just contrast what I\ndescribed in context learning with the standard\nmode of supervision.",
    "start": "2087739",
    "end": "2094760"
  },
  {
    "text": "Let's imagine for\na case here that we want to train a model to\ndetect nervous anticipation.",
    "start": "2094760",
    "end": "2100850"
  },
  {
    "text": "And I have picked this because\nthis is a very particular human emotion. And in the old mode, we would\nneed an entire dedicated model",
    "start": "2100850",
    "end": "2108710"
  },
  {
    "text": "to this, right? We would collect\na little data set of positive and\nnegative instances",
    "start": "2108710",
    "end": "2113960"
  },
  {
    "text": "of nervous anticipation,\nand we would train a supervised classifier\non feature representations",
    "start": "2113960",
    "end": "2120590"
  },
  {
    "text": "of these examples\nover here learning from this binary distinction. We would need custom\ndata and a custom model",
    "start": "2120590",
    "end": "2128270"
  },
  {
    "text": "for this particular\ntask in all likelihood. In this new mode, few-shot\nin-context learning,",
    "start": "2128270",
    "end": "2135430"
  },
  {
    "text": "we essentially just\nprompt the model. Hey, model, here's an example\nof nervous anticipation.",
    "start": "2135430",
    "end": "2140560"
  },
  {
    "text": "My palms started to sweat as\nthe lotto numbers were read off. Hey, model, here's an example\nwithout nervous anticipation",
    "start": "2140560",
    "end": "2146890"
  },
  {
    "text": "and so forth. And it learns from\nall those symbols",
    "start": "2146890",
    "end": "2151990"
  },
  {
    "text": "that you put in and their\nco-occurrences something about nervous anticipation, on\nthe left, for this model here,",
    "start": "2151990",
    "end": "2159910"
  },
  {
    "text": "I've written out nervous\nanticipation but remember, that has no special status. I've structured the model around\nthe binary distinction, the 1",
    "start": "2159910",
    "end": "2167620"
  },
  {
    "text": "and the 0. And everything\nabout the model is geared toward my learning goal. On the right,\nnervous anticipation",
    "start": "2167620",
    "end": "2174640"
  },
  {
    "text": "is just more of the symbols\nthat I've put into the model. And the eye opening thing\nagain about the GPT-3 paper",
    "start": "2174640",
    "end": "2182859"
  },
  {
    "text": "and what's followed\nis that models can learn, be put\nin a temporary state",
    "start": "2182860",
    "end": "2188080"
  },
  {
    "text": "and do well at tasks like this. Now, I talked about\nself-supervision before,",
    "start": "2188080",
    "end": "2195900"
  },
  {
    "text": "and I think that is\na major component to the success of these models. But it is increasingly\nclear that it is not",
    "start": "2195900",
    "end": "2202609"
  },
  {
    "text": "the only thing that is\ndriving learning in the best models in this class.",
    "start": "2202610",
    "end": "2207720"
  },
  {
    "text": "The other thing that\nwe should think about is what's called reinforcement\nlearning with human feedback.",
    "start": "2207720",
    "end": "2214560"
  },
  {
    "text": "This is a diagram from\nthe ChatGPT blog post. There are a lot of details\nhere but really two of them",
    "start": "2214560",
    "end": "2221059"
  },
  {
    "text": "are important for\nus for right now. The first is that in a phase\nof training these models,",
    "start": "2221060",
    "end": "2227900"
  },
  {
    "text": "people are given inputs\nand asked themselves to produce good outputs\nfor those inputs.",
    "start": "2227900",
    "end": "2235260"
  },
  {
    "text": "So you might be asked to\ndo a little Python program, and you yourself as an annotator\nmight write that Python",
    "start": "2235260",
    "end": "2240529"
  },
  {
    "text": "program, for example. So that's highly skilled\nwork that depends on a lot of human intelligence.",
    "start": "2240530",
    "end": "2246440"
  },
  {
    "text": "And those examples,\nthose pairs, are part of how the model is trained. And that is so important\nbecause that takes us",
    "start": "2246440",
    "end": "2253650"
  },
  {
    "text": "way beyond just learning\nfrom co-occurrence patterns of symbols in text. It is now back to a very\nfamiliar story from all of AI",
    "start": "2253650",
    "end": "2263070"
  },
  {
    "text": "which is that it's not magic. What is happening is that\na lot of human intelligence is driving the behavior\nof these systems.",
    "start": "2263070",
    "end": "2271840"
  },
  {
    "text": "And that happens again\nat step two here. So now the model produces\ndifferent outputs and humans",
    "start": "2271840",
    "end": "2277440"
  },
  {
    "text": "come in and rank\nthose outputs, again, expressing direct human\npreferences that take us",
    "start": "2277440",
    "end": "2283320"
  },
  {
    "text": "well beyond self-supervision. So we should remember,\nwe had that brief moment where it looked like it was all\nunstructured, unlabeled data,",
    "start": "2283320",
    "end": "2291210"
  },
  {
    "text": "and that was important to\nunlocking these capacities. But now we are back at a very\nlabor intensive human capacity",
    "start": "2291210",
    "end": "2298710"
  },
  {
    "text": "here driving what look like\nthe really important behaviors for these models. ",
    "start": "2298710",
    "end": "2307010"
  },
  {
    "text": "Final step, which\nI think actually intimately relates\nto that instruct tuning that I just described. That's a way of summarizing\nthis reinforcement learning",
    "start": "2307010",
    "end": "2314660"
  },
  {
    "text": "with human feedback. And this is what's\ncalled step-by-step or chain-of-thought reasoning.",
    "start": "2314660",
    "end": "2319670"
  },
  {
    "text": "Now we're thinking\nabout the prompts that we use for these models. So suppose we asked\nourselves a question like,",
    "start": "2319670",
    "end": "2325430"
  },
  {
    "text": "can models reason\nabout negation? To give an example,\ndoes the model know that if the customer\ndoesn't have any auto",
    "start": "2325430",
    "end": "2332450"
  },
  {
    "text": "loan-- sorry-- doesn't\nhave any loans, then the customer doesn't\nhave any auto loans? It's a simple example.",
    "start": "2332450",
    "end": "2338101"
  },
  {
    "text": "It's the sort of\nreasoning that you might have to do if you're\nthinking about a contract or something like that whether\na rule has been followed,",
    "start": "2338102",
    "end": "2345020"
  },
  {
    "text": "and it just involves\nnegation, our old friend from the start of the lecture.",
    "start": "2345020",
    "end": "2350670"
  },
  {
    "text": "Now, in the old school\nprompting style, all the way back in 2021,\nwe would naively just input,",
    "start": "2350670",
    "end": "2358190"
  },
  {
    "text": "is it true that if the customer\ndoesn't have any loans then the customer doesn't\nhave any auto loans? into one of these models, and\nwe would see what came back.",
    "start": "2358190",
    "end": "2366050"
  },
  {
    "text": "And here it says no, this\nis not necessarily true. A customer can have auto\nloans without having any other loans, which is\nthe reverse of the question",
    "start": "2366050",
    "end": "2373620"
  },
  {
    "text": "that I asked. Again, kind of showing\nit doesn't deeply understand what we put in here.",
    "start": "2373620",
    "end": "2378990"
  },
  {
    "text": "It just kind of does an\nact that looks like it did, and that's worrisome.",
    "start": "2378990",
    "end": "2384090"
  },
  {
    "text": "But we're learning how to\ncommunicate with these very alien creatures. Now we do what's called\nstep-by-step prompting.",
    "start": "2384090",
    "end": "2390210"
  },
  {
    "text": "This is the cutting edge thing. You would just tell\nthe model that it was in some logical or\ncommon sense reasoning exam.",
    "start": "2390210",
    "end": "2396450"
  },
  {
    "text": "That matters to the model. Then you could give\nsome instructions, and then you could give\nan example in your prompt",
    "start": "2396450",
    "end": "2403080"
  },
  {
    "text": "of the kind of thing\nit was going to see. And then, finally,\nyou could prompt it with your premise and\nthen your question.",
    "start": "2403080",
    "end": "2409620"
  },
  {
    "text": "And the model would\nspit out something that looked really good. Here, I won't bother\ngoing through the details.",
    "start": "2409620",
    "end": "2415390"
  },
  {
    "text": "But with that kind of\nprompt the model now not only answers and\nreasons correctly",
    "start": "2415390",
    "end": "2421470"
  },
  {
    "text": "but also offers a\nreally nice explanation of its own reasoning. The capacity was there.",
    "start": "2421470",
    "end": "2427500"
  },
  {
    "text": "It was latent, and\nwe didn't see it in the simple prompting mode,\nbut the more sophisticated",
    "start": "2427500",
    "end": "2432510"
  },
  {
    "text": "prompting mode elicited it. And I think this\nis in large part",
    "start": "2432510",
    "end": "2437730"
  },
  {
    "text": "the result of the fact that\nthis model was instruct tuned, and so people actually taught\nit about how that markup is",
    "start": "2437730",
    "end": "2444360"
  },
  {
    "text": "supposed to work and how it's\nsupposed to think about prompts like this. So the combination of all\nthat human intelligence",
    "start": "2444360",
    "end": "2450270"
  },
  {
    "text": "and the capacity\nof the model led to this really interesting\nand much better behavior. ",
    "start": "2450270",
    "end": "2459370"
  },
  {
    "text": "That is a glimpse of the\nfoundations of all of this, I would say. Of course, we're going to\nunpack all of that stuff",
    "start": "2459370",
    "end": "2466120"
  },
  {
    "text": "as we go through the quarter. But I hope you're\ngetting a sense for it. Are there questions I\ncan answer about it,",
    "start": "2466120",
    "end": "2471850"
  },
  {
    "text": "things I could circle back on? Yes. The human brain has\nabout 100 billion neurons",
    "start": "2471850",
    "end": "2477880"
  },
  {
    "text": "is my understanding. And I'm not sure how many\nparameters that may be. Maybe like 10\ntrillion parameters",
    "start": "2477880",
    "end": "2484390"
  },
  {
    "text": "or something like that. Are we approaching a\npoint where these machines can start emulating\nthe human brain",
    "start": "2484390",
    "end": "2490180"
  },
  {
    "text": "or is there something\nto the language instinct or instincts of\nall kinds that may",
    "start": "2490180",
    "end": "2495550"
  },
  {
    "text": "be baked into the human brains? Oh, it's nothing but\nbig questions today.",
    "start": "2495550",
    "end": "2501150"
  },
  {
    "text": "Right. So the question is\nkind of like, what is the relationship between\nthe models we're talking about",
    "start": "2501150",
    "end": "2506760"
  },
  {
    "text": "and the human brain? And you raise that\nin terms of the size and, I guess, the upshot\nof your description",
    "start": "2506760",
    "end": "2512339"
  },
  {
    "text": "was that these models remain\nsmaller than the human brain. Yeah, I think that's reasonable.",
    "start": "2512340",
    "end": "2517890"
  },
  {
    "text": "It's tricky though. On the one hand, they obviously\nhave superhuman capabilities.",
    "start": "2517890",
    "end": "2523570"
  },
  {
    "text": "On the other hand, they fall\ndown in ways that humans don't. It's very interesting to ask\nwhy that difference exists.",
    "start": "2523570",
    "end": "2530880"
  },
  {
    "text": "And maybe that would tell us\nsomething about the limitations of learning from scratch versus\nbeing initialized by evolution",
    "start": "2530880",
    "end": "2539160"
  },
  {
    "text": "the way all of us were. I don't know but I would\nsay that underlying",
    "start": "2539160",
    "end": "2544560"
  },
  {
    "text": "your whole line of\nquestioning is the question, can we use these\nmodels to illuminate questions of neuroscience\nand cognitive science?",
    "start": "2544560",
    "end": "2552480"
  },
  {
    "text": "And I think we should be\ncareful but that the answer is absolutely yes. And in fact, the\nincreased ability",
    "start": "2552480",
    "end": "2559890"
  },
  {
    "text": "of these models\nto learn from data has been really illuminating\nabout certain kind",
    "start": "2559890",
    "end": "2564940"
  },
  {
    "text": "of recalcitrant questions\nfrom cognitive science in particular. You have to be careful\nbecause they're so",
    "start": "2564940",
    "end": "2571350"
  },
  {
    "text": "different from us these models. On the other hand,\nI think they are helping us understand\nhow to differentiate",
    "start": "2571350",
    "end": "2578099"
  },
  {
    "text": "different theories of cognition. And ultimately, I\nthink they will help us understand cognition itself.",
    "start": "2578100",
    "end": "2583240"
  },
  {
    "text": "Yeah. And I would, of course,\nwelcome projects that were focused on those\ncognitive questions in here.",
    "start": "2583240",
    "end": "2590059"
  },
  {
    "text": "This is a wonderful\nspace in which to explore this kind of more\nspeculative angle connecting AI",
    "start": "2590060",
    "end": "2597220"
  },
  {
    "text": "to the cognitive sciences. ",
    "start": "2597220",
    "end": "2603560"
  },
  {
    "text": "Other questions, comments? Yes, in the back. I would be curious to\nunderstand whether--",
    "start": "2603560",
    "end": "2608810"
  },
  {
    "text": "I mean, harshly following\nup on the brain thing just to use a\nmetaphor of our brain not being just one\nhuge lump of neurons",
    "start": "2608810",
    "end": "2615560"
  },
  {
    "text": "but being separated\ninto different areas. And then also thinking\nabout the previous phase",
    "start": "2615560",
    "end": "2620990"
  },
  {
    "text": "that you talked about\nbreaking up the models and potentially having a\nmodel in the front that",
    "start": "2620990",
    "end": "2626630"
  },
  {
    "text": "decides which domain\nour question falls into and then having\ndifferent sub models.",
    "start": "2626630",
    "end": "2632460"
  },
  {
    "text": "And I'm wondering\nwhether that's arising, whether we're going to touch\non an architecture like that?",
    "start": "2632460",
    "end": "2637640"
  },
  {
    "text": "Because it just\nseems natural to me because prompting a\nhuge model is just very expensive computationally.",
    "start": "2637640",
    "end": "2645140"
  },
  {
    "text": "It feels like combining\nbig models and logic trees could be a cool approach.",
    "start": "2645140",
    "end": "2650750"
  },
  {
    "text": "I love it. Yeah, one quick summary\nof what you said would relate directly\nto your question, the modularity of mind is\nan important old question",
    "start": "2650750",
    "end": "2659360"
  },
  {
    "text": "about human cognition. To what extent are our abilities\nmodularized in the mind brain?",
    "start": "2659360",
    "end": "2667470"
  },
  {
    "text": "With these current models\nwhich have a capacity to do lots of different\nthings, if they have the right pretraining\nand the right structure,",
    "start": "2667470",
    "end": "2674155"
  },
  {
    "text": "we could ask does modularity\nemerge naturally or do they learn nonmodular solutions?",
    "start": "2674155",
    "end": "2680309"
  },
  {
    "text": "Both of those seem like they\ncould be indirect evidence for how people work. Again, we have to be careful\nbecause these models are",
    "start": "2680310",
    "end": "2686940"
  },
  {
    "text": "so different from us. But as a existence proof,\nfor example, that modularity was emergent from otherwise\nunstructured learning,",
    "start": "2686940",
    "end": "2694110"
  },
  {
    "text": "that would be certainly\neye-opening, right? Is it? I have no idea. [LAUGHTER] Yeah, I don't know whether\nthere are results for that.",
    "start": "2694110",
    "end": "2700680"
  },
  {
    "text": "Are there results? No, just kind of a follow-up\nquestion on that as well.",
    "start": "2700680",
    "end": "2706210"
  },
  {
    "text": "So given how closed all\nthese big models are, how could we interact with\nthe model in such a way that",
    "start": "2706210",
    "end": "2713400"
  },
  {
    "text": "helps us learn if there is\nmodular because we literally can only interact\nwith [INAUDIBLE]",
    "start": "2713400",
    "end": "2719430"
  },
  {
    "text": "so how do we go\nabout studying that? Right. So question is the closed-off\nnature of a lot of these models",
    "start": "2719430",
    "end": "2726390"
  },
  {
    "text": "has been a problem. We can access the OpenAI\nmodels but only through an API. We don't get to look at their\ninternal representations,",
    "start": "2726390",
    "end": "2733170"
  },
  {
    "text": "and that has been a blocker. But I mentioned the rise of\nthese 10 billion parameter",
    "start": "2733170",
    "end": "2739020"
  },
  {
    "text": "models as being performant\nand interesting, and those are models that\nwith the right hardware",
    "start": "2739020",
    "end": "2744359"
  },
  {
    "text": "you can dissect a little bit. And I think that's just going\nto get better and better, and so we'll be able to\npeer inside them in ways",
    "start": "2744360",
    "end": "2751140"
  },
  {
    "text": "that we haven't been\nable to until recently. Yeah. And in fact, we're going to\ntalk a lot about explainability.",
    "start": "2751140",
    "end": "2759020"
  },
  {
    "text": "That's a major unit\nof this course. And I think it's an\nincreasingly important area of the whole field that we have\ntechniques for understanding",
    "start": "2759020",
    "end": "2766640"
  },
  {
    "text": "these models so that we\nknow how they're going to behave when we deploy them. And it would be\nwonderfully exciting",
    "start": "2766640",
    "end": "2772130"
  },
  {
    "text": "if you all wanted to try to\nscale the methods we talk about to a model that was as big\nas 8 or 10 billion parameters.",
    "start": "2772130",
    "end": "2778640"
  },
  {
    "text": "Ambitious just to do\nthat but then maybe a meaningful step forward. Yeah.",
    "start": "2778640",
    "end": "2784670"
  },
  {
    "text": "I have a question back\nto this baseball cap prompt that we were discussing. So I suppose a part\nof the way that we",
    "start": "2784670",
    "end": "2791510"
  },
  {
    "text": "discuss rules is\nthere is a little bit of ambiguity for\nhuman interpretation, for example, in the honor code\nand the fundamental standard",
    "start": "2791510",
    "end": "2798740"
  },
  {
    "text": "like it's intentionally\nambiguous so that it's context dependent. And so the idea is that there's\nthis inherent underlying value",
    "start": "2798740",
    "end": "2806450"
  },
  {
    "text": "system that affords whatever the\nrules that are written out are. And so that's the primary\nform of evaluation.",
    "start": "2806450",
    "end": "2814620"
  },
  {
    "text": "And so I guess how does\nthat play into then how these language models\nare understanding? Is there some form of\nencoded or understood",
    "start": "2814620",
    "end": "2822330"
  },
  {
    "text": "deeper value system\nthat's encoded into them? You could certainly ask. I mean, the essence\nof your question",
    "start": "2822330",
    "end": "2828750"
  },
  {
    "text": "is, could we with\nanalysis techniques, say, find out that a model had\na particular belief system that",
    "start": "2828750",
    "end": "2835230"
  },
  {
    "text": "was guiding its behavior? I think we can ask\nthat question now. It sounds\nfantastically difficult",
    "start": "2835230",
    "end": "2840990"
  },
  {
    "text": "but maybe piecemeal we could\nmake some progress on it for sure. Yeah, I want to\nreturn to the MLB one",
    "start": "2840990",
    "end": "2846510"
  },
  {
    "text": "though because\nwell, as you'll see and as I think we already\nsaw, these models purport to offer evidence\nfrom a rule book,",
    "start": "2846510",
    "end": "2853380"
  },
  {
    "text": "and that's where I feel stuck. [CHUCKLES] You're keeping\nscore at home close",
    "start": "2853380",
    "end": "2859670"
  },
  {
    "text": "to the answer and some other\nstuff in the class discussion. Wonderful.",
    "start": "2859670",
    "end": "2864770"
  },
  {
    "text": "Thank you.  Yes.",
    "start": "2864770",
    "end": "2870500"
  },
  {
    "text": "OK. Can we just hook up these\nmodels to a large database of factual provided information\nlike an encyclopedia",
    "start": "2870500",
    "end": "2877329"
  },
  {
    "text": "and allow it to look stuff up? Oh, well, kind of yes.",
    "start": "2877330",
    "end": "2883877"
  },
  {
    "text": "Actually, this is\nthe sort of solution that I want to advocate for. I'm going to do\nthis in a minute. Yeah. [CHUCKLES]",
    "start": "2883877",
    "end": "2890980"
  },
  {
    "text": "Here let's-- so we'll\ndo this overview. I want to give\nyou a feel for how the course will\nwork, and then dive",
    "start": "2890980",
    "end": "2896470"
  },
  {
    "text": "into some of our major themes. So high-level overview\nwe've got these topics, contextual representations--\ntransformers and stuff.",
    "start": "2896470",
    "end": "2903460"
  },
  {
    "text": "Multi-domain sentiment\nanalysis-- that will be the topic of\nthe first homework, and it's going to build\non the first unit there.",
    "start": "2903460",
    "end": "2910299"
  },
  {
    "text": "Retrieval augmented\nin-context learning, this is where we might\nhook up to a database and get some guarantees about\nhow these models will behave.",
    "start": "2910300",
    "end": "2918040"
  },
  {
    "text": "Compositional generalization--\nin case you were worried that all the tasks were solved,\nI'm going to confront you with",
    "start": "2918040",
    "end": "2924250"
  },
  {
    "text": "a seemingly simple task about\nsemantic interpretation that you will-- well, I think it\nwill not be solved.",
    "start": "2924250",
    "end": "2930183"
  },
  {
    "text": "I mean, those could\nbe famous last words because who knows what\nyou all are capable of. But it's a very hard\ntask that we will pose.",
    "start": "2930183",
    "end": "2936829"
  },
  {
    "text": "We'll talk about\nbenchmarking and adversarial training and testing. Increasingly important topics\nas we move into this mode",
    "start": "2936830",
    "end": "2942908"
  },
  {
    "text": "where everyone is interacting\nwith these large language models and feeling\nimpressed by their behavior,",
    "start": "2942908",
    "end": "2948050"
  },
  {
    "text": "we need to take a step\nback and rigorously assess whether they actually\nare behaving in good ways",
    "start": "2948050",
    "end": "2953270"
  },
  {
    "text": "or whether we're just\nbiased toward remembering the good things and\nforgetting the bad ones.",
    "start": "2953270",
    "end": "2958369"
  },
  {
    "text": "We'll do model introspection. That's the explainability\nstuff that I mentioned. And finally,\nmethods and metrics. And as you can see\nfor the 5, 6, and 7,",
    "start": "2958370",
    "end": "2966650"
  },
  {
    "text": "that's going to be in\nthe phase of the course where you're focused\non final projects, and I'm hoping that\ngives you tools",
    "start": "2966650",
    "end": "2973099"
  },
  {
    "text": "to write really rich\nfinal papers that have great analysis in them and\nreally excellent assessments.",
    "start": "2973100",
    "end": "2980270"
  },
  {
    "text": "And then for the\nwork that you'll do. We're going to have\nthree assignments, and each one of\nthe assignments is",
    "start": "2980270",
    "end": "2986030"
  },
  {
    "text": "paired with what we\ncall a bakeoff, which is an informal competition\naround data and modeling.",
    "start": "2986030",
    "end": "2991130"
  },
  {
    "text": "Essentially the\nhomework problems ask you to set up\nsome baseline systems and get a feel for a problem.",
    "start": "2991130",
    "end": "2997339"
  },
  {
    "text": "And then you write your\nown original system, and you enter that\ninto the bakeoff. And we have a leaderboard\non Gradescope,",
    "start": "2997340",
    "end": "3003820"
  },
  {
    "text": "and the team is going to\nlook at all your submissions and give out some prizes\nfor top-performing systems",
    "start": "3003820",
    "end": "3009940"
  },
  {
    "text": "but also systems that are really\ncreative or interesting or ambitious or\nsomething like that.",
    "start": "3009940",
    "end": "3015250"
  },
  {
    "text": "And that has always\nbeen a lot of fun and also really\nilluminating because it's like crowdsourcing a whole\nlot of different approaches",
    "start": "3015250",
    "end": "3022779"
  },
  {
    "text": "to a problem. And then as a group we\ncan reflect on what worked and what didn't and look at\nthe really ambitious things",
    "start": "3022780",
    "end": "3029260"
  },
  {
    "text": "that you all try. So that's my favorite part. We have three offline quizzes.",
    "start": "3029260",
    "end": "3034570"
  },
  {
    "text": "And this is just\nas a way to make sure you have incentives\nto really immerse yourself",
    "start": "3034570",
    "end": "3040210"
  },
  {
    "text": "in the course material. And those are done on Canvas. There's actually a\nfourth quiz which",
    "start": "3040210",
    "end": "3045940"
  },
  {
    "text": "I'll talk a little bit about\nprobably next time that is just making sure you\nunderstand the course policies.",
    "start": "3045940",
    "end": "3051940"
  },
  {
    "text": "That's quiz 0. You can take it as\nmany times as you want, but the idea is that you\nwill have some incentive",
    "start": "3051940",
    "end": "3058480"
  },
  {
    "text": "to learn about policies\nlike due dates and so forth. And then the real action\nis in the final project,",
    "start": "3058480",
    "end": "3064599"
  },
  {
    "text": "and that will have a lit review\nphase, an experiment protocol, and a final paper,\nthose three components.",
    "start": "3064600",
    "end": "3070539"
  },
  {
    "text": "You'll probably\ndo those in teams. And throughout all\nof that work you'll be mentored by someone\nfrom the teaching team.",
    "start": "3070540",
    "end": "3076790"
  },
  {
    "text": "And as I said before, we have\nthis incredibly expert teaching team, lots of varied\nexpertise, a lot",
    "start": "3076790",
    "end": "3083619"
  },
  {
    "text": "of experience in the field. And so we hope to align\nyou with the person-- with someone who's\nreally aligned",
    "start": "3083620",
    "end": "3089950"
  },
  {
    "text": "with your project\ngoals, and then I think you can go\nreally, really far. Yeah.",
    "start": "3089950",
    "end": "3095730"
  },
  {
    "text": "It looks like\n[INAUDIBLE] quarter or are you looking\nforward to bakeoff. I know Stanford kids get\nobsessed about this stuff.",
    "start": "3095730",
    "end": "3102670"
  },
  {
    "text": "On the final project, is this\nmore of an academic paper",
    "start": "3102670",
    "end": "3107829"
  },
  {
    "text": "or rather about\nbuilding working code and showing off\nstate of the art?",
    "start": "3107830",
    "end": "3114609"
  },
  {
    "text": "Great question. For the first one,\nthe bakeoffs, yes, it is easy to get obsessed\nwith your bakeoff entry.",
    "start": "3114610",
    "end": "3120010"
  },
  {
    "text": "I would say that\nif you get obsessed and you do really\nwell, just make that into your final project.",
    "start": "3120010",
    "end": "3127270"
  },
  {
    "text": "All three of them are\nreally important problems. They are not idle work. I mean, one of them\nis on retrieval",
    "start": "3127270",
    "end": "3133150"
  },
  {
    "text": "augmented in-context\nlearning, which is one of my core research\nfocuses right now. So is compositional\ngeneralization.",
    "start": "3133150",
    "end": "3138700"
  },
  {
    "text": "If you do something really\ninteresting for a bakeoff, make it your final paper,\nand then go on to publish it.",
    "start": "3138700",
    "end": "3144358"
  },
  {
    "text": "For the second part\nof your question, I would say that\nthe core goal is to get you to produce something\nthat could be a research",
    "start": "3144358",
    "end": "3150730"
  },
  {
    "text": "contribution in the field. And we have lots\nof success stories. I've got links at\nthe website to people",
    "start": "3150730",
    "end": "3156640"
  },
  {
    "text": "who have gone on to publish\ntheir final paper as an NLP paper. I'm careful the way I say that.",
    "start": "3156640",
    "end": "3162940"
  },
  {
    "text": "They didn't literally\npublish the final paper because in 10 weeks\nalmost no one can produce a publishable paper.",
    "start": "3162940",
    "end": "3168720"
  },
  {
    "text": "It's just not enough time. But you could form the basis for\nthen working a little bit more",
    "start": "3168720",
    "end": "3173760"
  },
  {
    "text": "or a lot more and then getting\na really outstanding publication out of it. And I would say that\nthat's the default goal.",
    "start": "3173760",
    "end": "3179520"
  },
  {
    "text": "The nature of the contribution\nthough is highly varied. We have one requirement which\nis that the final paper have",
    "start": "3179520",
    "end": "3184950"
  },
  {
    "text": "some quantitative\nevaluation in it, but there are a lot of ways\nto satisfy that requirement",
    "start": "3184950",
    "end": "3190420"
  },
  {
    "text": "and then you could be serving\nmany different questions in the field for some expansive\nnotion of the field as well.",
    "start": "3190420",
    "end": "3197340"
  },
  {
    "start": "3197340",
    "end": "3204530"
  },
  {
    "text": "Background materials-- so I\nshould say that officially we are presupposing CS 224n\nor CS224s as prerequisites",
    "start": "3204530",
    "end": "3214010"
  },
  {
    "text": "for the course. And what that means\nis that I'm going to skip a lot of\nthe fundamentals that we have covered\nin past years.",
    "start": "3214010",
    "end": "3221450"
  },
  {
    "text": "If you need a refresher,\ncheck out the background page of the course site. It covers fundamentals\nof scientific computing,",
    "start": "3221450",
    "end": "3229220"
  },
  {
    "text": "static vector\nrepresentations like Word2Vec and GloVe and\nsupervised learning.",
    "start": "3229220",
    "end": "3234800"
  },
  {
    "text": "And I'm hoping that that's\nenough of a refresher. If you look at that material\nand find that it too",
    "start": "3234800",
    "end": "3240890"
  },
  {
    "text": "is kind of beyond where\nyou're at right now, then contact us on\nthe teaching team and we can think about\nhow to manage that.",
    "start": "3240890",
    "end": "3248819"
  },
  {
    "text": "But officially, this is a\ncourse that presupposes CS224n. ",
    "start": "3248820",
    "end": "3256100"
  },
  {
    "text": "And then the core\ngoals-- this relates to that previous question--\nhands-on experience with a wide range of problems.",
    "start": "3256100",
    "end": "3262069"
  },
  {
    "text": "Mentorship from\nthe teaching team to guide you through\nprojects and assignments.",
    "start": "3262070",
    "end": "3267560"
  },
  {
    "text": "And then really the central goal\nhere is to make you the best, that is, most insightful, most\nresponsible, most flexible NLU",
    "start": "3267560",
    "end": "3275000"
  },
  {
    "text": "researcher and\npractitioner that you can be for whatever\nyou decide to do next, and we're assuming that you\nhave lots of diverse goals",
    "start": "3275000",
    "end": "3282470"
  },
  {
    "text": "that somehow connect with NLU. ",
    "start": "3282470",
    "end": "3289820"
  },
  {
    "text": "All right. Let's do some course themes,\nunless there are questions. I have a whole final section\nof this slideshow that's",
    "start": "3289820",
    "end": "3297140"
  },
  {
    "text": "about the course materials\nand requirements, and stuff.",
    "start": "3297140",
    "end": "3302250"
  },
  {
    "text": "Save that for next time\nand you can check it out at the website, and\nyou'll be forced to engage with it for quiz 0.",
    "start": "3302250",
    "end": "3309020"
  },
  {
    "text": "But I thought instead\nI would dive back into the content part\nof this, unless there are questions or comments.",
    "start": "3309020",
    "end": "3314030"
  },
  {
    "text": " All right.",
    "start": "3314030",
    "end": "3319569"
  },
  {
    "text": "First course theme,\ntransformer-based pretraining. So starting with\nthe transformer,",
    "start": "3319570",
    "end": "3326000"
  },
  {
    "text": "we want to talk about\ncore concepts and goals. Give you a sense for\nwhat these models are like, why they work,\nwhat they're supposed to do,",
    "start": "3326000",
    "end": "3333250"
  },
  {
    "text": "all of that stuff. We'll talk about a bunch\nof different architectures. There are dozens\nand dozens of them,",
    "start": "3333250",
    "end": "3339490"
  },
  {
    "text": "but I hope that I have\npicked enough of them with the right selection\nof them to give you a feel for how\npeople are thinking",
    "start": "3339490",
    "end": "3345880"
  },
  {
    "text": "about these models and the kind\nof innovations they've brought in that have led to real\nmeaningful advancement",
    "start": "3345880",
    "end": "3351910"
  },
  {
    "text": "just at the level\nof architectures. We'll also talk about\npositional encoding, which I think maybe a lot of us have\nbeen surprised to see just how",
    "start": "3351910",
    "end": "3359020"
  },
  {
    "text": "important that is\nas a differentiator for different approaches\nin this space.",
    "start": "3359020",
    "end": "3364070"
  },
  {
    "text": "We'll talk about distillation-- taking really large models\nand making them smaller.",
    "start": "3364070",
    "end": "3369400"
  },
  {
    "text": "It's an important goal\nfor lots of reasons and an exciting\narea of research. And then as I\nmentioned, we're just",
    "start": "3369400",
    "end": "3375730"
  },
  {
    "text": "going to do a little\nlecture for us on diffusion objectives\nfor these models, and then just going to talk\nabout practical pretraining",
    "start": "3375730",
    "end": "3382970"
  },
  {
    "text": "and fine tuning. I'm going to enlist\nthe entire teaching team to do guest lectures,\nand these are the two",
    "start": "3382970",
    "end": "3389000"
  },
  {
    "text": "that I've lined up so far. And that will kind\nof culminate or be aligned with this first\nhomework and bakeoff which",
    "start": "3389000",
    "end": "3396050"
  },
  {
    "text": "is on multi-domain sentiment. I'm going to give you a bunch\nof different sentiment data sets and you're going to have\nto design one system that",
    "start": "3396050",
    "end": "3402890"
  },
  {
    "text": "can succeed on all of them. And then for the bakeoff,\nwe have an unlabeled data set for you.",
    "start": "3402890",
    "end": "3408620"
  },
  {
    "text": "We have the labels\nbut you won't. And that has data that's like\nwhat you developed on and then",
    "start": "3408620",
    "end": "3414590"
  },
  {
    "text": "some mystery examples\nthat you will not really be able to anticipate. And we're going to\nsee how well you",
    "start": "3414590",
    "end": "3419990"
  },
  {
    "text": "do at handling all of\nthese different domains with one system. And this is by way\nof, again, a refresher",
    "start": "3419990",
    "end": "3428960"
  },
  {
    "text": "on core concepts in supervised\nlearning and really getting you to think about transformers,\nalthough, we're not",
    "start": "3428960",
    "end": "3434444"
  },
  {
    "text": "going to constrain\nthe kind of solution that you offer for\nyour original system. ",
    "start": "3434445",
    "end": "3442540"
  },
  {
    "text": "Our second major theme will be\nretrieval augmented in-context learning. A topic that I would not even\nhave dreamed of five years ago",
    "start": "3442540",
    "end": "3453420"
  },
  {
    "text": "and seemed infeasible\nthree years ago and that we first\ndid two years-- one year ago, Oh, goodness.",
    "start": "3453420",
    "end": "3460128"
  },
  {
    "text": "I think this is only\nthe second time, but I had to redo it\nentirely because things have changed so much.",
    "start": "3460128",
    "end": "3467190"
  },
  {
    "text": "Here's the idea. We have two characters\nso far in our kind of emerging narrative for NLU.",
    "start": "3467190",
    "end": "3473550"
  },
  {
    "text": "On the one hand, we\nhave this approach that I'm going to call LLMs\nfor everything, large language",
    "start": "3473550",
    "end": "3478650"
  },
  {
    "text": "models for everything. You input some kind of question. Here I've chosen a very\ncomplicated question,",
    "start": "3478650",
    "end": "3484630"
  },
  {
    "text": "which MVP of a game\nRed Flaherty umpired was elected to the\nBaseball Hall of Fame?",
    "start": "3484630",
    "end": "3490470"
  },
  {
    "text": "And hats off to if you know\nthat the answer is Sandy Koufax.",
    "start": "3490470",
    "end": "3495780"
  },
  {
    "text": "The LLMs for\neverything approach is that you just type\nthat question in",
    "start": "3495780",
    "end": "3500790"
  },
  {
    "text": "and the model gives\nyou an answer. And hopefully, you're\nhappy with the answer.",
    "start": "3500790",
    "end": "3506490"
  },
  {
    "text": "The other character that\nI'm going to introduce here is what I'm going to\ncall retrieval-augmented.",
    "start": "3506490",
    "end": "3511829"
  },
  {
    "text": "So I have the same\nquestion at the top here except now this is\ngoing to proceed differently. The first thing that we will\ndo is take some large language",
    "start": "3511830",
    "end": "3519350"
  },
  {
    "text": "model and encode that query into\nsome numerical representation.",
    "start": "3519350",
    "end": "3525160"
  },
  {
    "text": "That's sort of familiar. The new piece is that we're\ngoing to also have a knowledge store, which you could think\nof as an old fashioned web",
    "start": "3525160",
    "end": "3534190"
  },
  {
    "text": "index, right? Just a knowledge store of\ndocuments with the modern twist that now all\nof the documents",
    "start": "3534190",
    "end": "3541240"
  },
  {
    "text": "are also represented by\nlarge language models. But fundamentally,\nthis is an index",
    "start": "3541240",
    "end": "3546490"
  },
  {
    "text": "of a sort that drives\nall web search right now. We can score\ndocuments with respect",
    "start": "3546490",
    "end": "3551890"
  },
  {
    "text": "to queries on the basis of\nthese numerical representations. And if we want to, we can\nreproduce the classic search",
    "start": "3551890",
    "end": "3558460"
  },
  {
    "text": "experience. Here I've got a ranked list\nof documents that came back from my query just like when you\ndo Google as of the last time",
    "start": "3558460",
    "end": "3567190"
  },
  {
    "text": "I googled. But in this mode we\ncan continue, right? We could have another\nlanguage model slurp up",
    "start": "3567190",
    "end": "3573040"
  },
  {
    "text": "those retrieved documents and\nsynthesize them into an answer. And so here at the bottom\nI've got-- it's small,",
    "start": "3573040",
    "end": "3579700"
  },
  {
    "text": "but it's the same\nanswer over here, although, notably\nthis answer is now decorated with links that\nwould allow you, the user,",
    "start": "3579700",
    "end": "3587240"
  },
  {
    "text": "to track back to what\ndocuments actually provided that evidence. Whereas on the left\nwho knows where",
    "start": "3587240",
    "end": "3594910"
  },
  {
    "text": "that information came from? And that's what we were\nalready grappling with.",
    "start": "3594910",
    "end": "3600490"
  },
  {
    "text": "This is an important\nsocietal need because this is taking\nover web search. What are our goals for\nthis kind of model here?",
    "start": "3600490",
    "end": "3606740"
  },
  {
    "text": "So first we want\nsynthesis/fluency, right? We want to be able\nto take information",
    "start": "3606740",
    "end": "3611920"
  },
  {
    "text": "from multiple documents\nand synthesize it down into a single answer. And I think both\nof the approaches",
    "start": "3611920",
    "end": "3618012"
  },
  {
    "text": "that I just showed you are\ngoing to do really well on that. We also need these\nmodels to be efficient.",
    "start": "3618012",
    "end": "3623050"
  },
  {
    "text": "To be updatable because the\nworld is changing all the time. We need it to track\nprovenance and maybe",
    "start": "3623050",
    "end": "3630190"
  },
  {
    "text": "invoke something like factuality\nbut certainly provenance. We need to know where the\ninformation came from.",
    "start": "3630190",
    "end": "3635830"
  },
  {
    "text": "And we need some\nsafety and security. We need to know that\nthe model won't produce private information,\nand we might",
    "start": "3635830",
    "end": "3641410"
  },
  {
    "text": "need to restrict access\nto parts of the model's knowledge to different groups,\nlike different customers",
    "start": "3641410",
    "end": "3646790"
  },
  {
    "text": "or different people with\ndifferent privileges and so forth. That's what we're going\nto need if we're really going to deploy these\nmodels out into the world.",
    "start": "3646790",
    "end": "3655460"
  },
  {
    "text": "As I said, I think\nboth of the approaches that I sketched do well\non the synthesis part because they both\nuse a language model",
    "start": "3655460",
    "end": "3661040"
  },
  {
    "text": "and those are really good. They all have the gift\nof gab, so to speak. What about efficiency, right?",
    "start": "3661040",
    "end": "3666770"
  },
  {
    "text": "On the LLM for\neverything approach, we had this undeniable\nrise in model size.",
    "start": "3666770",
    "end": "3673040"
  },
  {
    "text": "And I pointed out models\nlike Alpaca that are smaller. But I strongly\nsuspect that if we",
    "start": "3673040",
    "end": "3679400"
  },
  {
    "text": "are going to continue to\nask these models to be both a knowledge store and\na language capability,",
    "start": "3679400",
    "end": "3686700"
  },
  {
    "text": "we're going to be dealing with\nthese really large models. The hope of the\nretrieval-augmented approach",
    "start": "3686700",
    "end": "3693920"
  },
  {
    "text": "is that we could get by\nwith the smaller models. And the reason we\ncould do that is that we're going to\nfactor out the knowledge",
    "start": "3693920",
    "end": "3700880"
  },
  {
    "text": "store into that index and the\nlanguage capability, which is going to be the language model.",
    "start": "3700880",
    "end": "3706110"
  },
  {
    "text": "The only thing we're going to\nbe asking the language model is to be good at that\nin-context learning.",
    "start": "3706110",
    "end": "3711920"
  },
  {
    "text": "It doesn't need to also store\na full model of the world. And I think that means that\nthese models could be smaller.",
    "start": "3711920",
    "end": "3718760"
  },
  {
    "text": "So overall, a big gain\nin efficiency if we go retrieval-augmented. People will make progress, but\nI think it's going to be tense.",
    "start": "3718760",
    "end": "3727940"
  },
  {
    "text": "What about updateability? Again, this is a\nproblem that people are working on very concertedly\nfor the LLMs for everything",
    "start": "3727940",
    "end": "3733970"
  },
  {
    "text": "approach, but these\nmodels persist in giving outdated\nanswers to questions.",
    "start": "3733970",
    "end": "3739700"
  },
  {
    "text": "And one pattern you see is that\nthere's a lot of progress where you could edit a\nmodel so that it gives the correct answer to\nwho is the president of the US.",
    "start": "3739700",
    "end": "3747319"
  },
  {
    "text": "But then you ask\nit about something related to the family\nof the president,",
    "start": "3747320",
    "end": "3752839"
  },
  {
    "text": "and it reveals that it has\noutdated information stored in its parameters. And that's because all of this\ninformation is interconnected,",
    "start": "3752840",
    "end": "3760280"
  },
  {
    "text": "and we don't at\nthe present moment know how to reliably do that\nkind of systematic editing.",
    "start": "3760280",
    "end": "3767240"
  },
  {
    "text": "OK. On the retrieval-augmented\napproach we just reindex our data, right?",
    "start": "3767240",
    "end": "3772520"
  },
  {
    "text": "If the world changes, we\nassume that the knowledge store changed like somebody\nupdated a Wikipedia page.",
    "start": "3772520",
    "end": "3778220"
  },
  {
    "text": "So we represent all the\ndocuments again or at least just the ones that changed. And now we have a\nlot of guarantees",
    "start": "3778220",
    "end": "3784640"
  },
  {
    "text": "that as that propagates forward\ninto the retrieved results which are consumed by\nthe language model,",
    "start": "3784640",
    "end": "3789830"
  },
  {
    "text": "it will reflect the changes we\nmade to the underlying database in exactly the same\nway that a web search",
    "start": "3789830",
    "end": "3796730"
  },
  {
    "text": "index is updated now. One forward pass of the\nlarge language model compared",
    "start": "3796730",
    "end": "3803550"
  },
  {
    "text": "to maybe training\nfrom scratch over here on new data to get\nan absolute guarantee",
    "start": "3803550",
    "end": "3808980"
  },
  {
    "text": "that the change will propagate. What about providence? OK. We have seen this already\nthis problem here,",
    "start": "3808980",
    "end": "3816520"
  },
  {
    "text": "LLMs for everything. I asked GPT-3, the\nDavinci-3 model my question,",
    "start": "3816520",
    "end": "3822878"
  },
  {
    "text": "are professional baseball\nplayers allowed to glue small wings onto their caps? But I kind of cut it off.",
    "start": "3822878",
    "end": "3827990"
  },
  {
    "text": "But at the top there, I\nsaid provide me some links to the evidence.",
    "start": "3827990",
    "end": "3833230"
  },
  {
    "text": "And it dutifully provided the\nlinks but none of the links are real. If you copy them out and follow\nthem, they all go to 404 pages.",
    "start": "3833230",
    "end": "3842530"
  },
  {
    "text": "And I think that this is worse\nthan providing no links at all because I'm attuned, as a\nhuman in the current moment,",
    "start": "3842530",
    "end": "3850599"
  },
  {
    "text": "to see links and think\nthey're probably evidence. And I don't follow\nall the links.",
    "start": "3850600",
    "end": "3855640"
  },
  {
    "text": "And here you might\nlook and say, oh, yeah, I see it found\nthe relevant MLB pages and that's it, right?",
    "start": "3855640",
    "end": "3862119"
  },
  {
    "text": "Over here, the point of\nthis is that we are first doing a search phase where\nwe're actually linked back",
    "start": "3862120",
    "end": "3869530"
  },
  {
    "text": "to documents, and\nthen we just need to solve the interesting\nnontrivial question of how to link those documents\ninto the synthesized answer.",
    "start": "3869530",
    "end": "3877550"
  },
  {
    "text": "But all of the\ninformation we need is right there on\nthe screen for us, and so this feels like a\nrelatively tractable problem",
    "start": "3877550",
    "end": "3884359"
  },
  {
    "text": "compared to what we were\nfaced with on the left. I will say I've been just\namazed at the rollout,",
    "start": "3884360",
    "end": "3892519"
  },
  {
    "text": "especially of the Bing search\nengine which now incorporates OpenAI models at some level.",
    "start": "3892520",
    "end": "3898070"
  },
  {
    "text": "Because it is clear that it is\ndoing web search because it's got information that\ncomes from documents",
    "start": "3898070",
    "end": "3904549"
  },
  {
    "text": "that only appeared on the\nweb days before your query. But what it's doing\nwith that information",
    "start": "3904550",
    "end": "3911000"
  },
  {
    "text": "seems completely\nchaotic to me so that it's kind of\njust getting mushed in with whatever else\nthe model is doing",
    "start": "3911000",
    "end": "3917580"
  },
  {
    "text": "and you get this unpredictable\ncombination of things that",
    "start": "3917580",
    "end": "3922790"
  },
  {
    "text": "are grounded in documents and\nthings that are completely fabricated. And again, I maintain\nthis is worse than just",
    "start": "3922790",
    "end": "3928730"
  },
  {
    "text": "giving an answer with no\nevidence attached to it.",
    "start": "3928730",
    "end": "3933800"
  },
  {
    "text": "I don't know why these\ncompanies are not simply doing the\nretrieval-augmented thing, but I'm sure they\nare going to wise up",
    "start": "3933800",
    "end": "3939589"
  },
  {
    "text": "and maybe your\nresearch could help them wise up a little\nbit about this. Finally, safety and security.",
    "start": "3939590",
    "end": "3945752"
  },
  {
    "text": "This is relatively\nstraightforward. On the LLMs for\neverything approach we have a pressing problem,\nprivacy challenges.",
    "start": "3945752",
    "end": "3951960"
  },
  {
    "text": "We know that those models\ncan memorize long strings in their training data\nand that could include some very particular\ninformation about one of us,",
    "start": "3951960",
    "end": "3959510"
  },
  {
    "text": "and that should be worrying us. We have no known way\nwith a language model to compartmentalize LLM\ncapabilities and say you can",
    "start": "3959510",
    "end": "3967190"
  },
  {
    "text": "see this kind of\nresult and you cannot. And similarly, we\nhave no known way to restrict access to part\nof an LLMs capabilities.",
    "start": "3967190",
    "end": "3975740"
  },
  {
    "text": "They just produce things\nbased on their prompts. And you could try to have\nsome prompt tuning that would tell them for this\nkind of person or setting,",
    "start": "3975740",
    "end": "3982670"
  },
  {
    "text": "do this and not that. But nobody could guarantee\nthat that would succeed. Whereas for the retrieval\naugmented approach again,",
    "start": "3982670",
    "end": "3991170"
  },
  {
    "text": "we're thinking about accessing\ninformation from an index, and access restrictions on an\nindex is an old problem by now.",
    "start": "3991170",
    "end": "3999860"
  },
  {
    "text": "Again, I don't want to\nsay solved but something that a lot of people have\ntackled for decades now,",
    "start": "3999860",
    "end": "4005730"
  },
  {
    "text": "and so we can offer\nsomething like guarantees just from the fact that we have\na separated knowledge store.",
    "start": "4005730",
    "end": "4011540"
  },
  {
    "text": " Again, my smiley face. You can see where my feelings\nare for the LLMs for everything",
    "start": "4011540",
    "end": "4019200"
  },
  {
    "text": "approach. People are working\non these problems and it's very exciting. And if you want a\nchallenge, take up",
    "start": "4019200",
    "end": "4025859"
  },
  {
    "text": "one of these challenges\nhere but over here on the retrieval-augmented side. I think we have lots of reasons\nto think it's not that they're",
    "start": "4025860",
    "end": "4032640"
  },
  {
    "text": "completely solved,\nit's just that we can see the path to solving them. And this feels very urgent\nto me because of how suddenly",
    "start": "4032640",
    "end": "4040590"
  },
  {
    "text": "this kind of technology is being\ndeployed in a very user facing way for one of the\ncore things we do",
    "start": "4040590",
    "end": "4045810"
  },
  {
    "text": "in society which is web search. So it's an urgent thing\nthat we get good at this.",
    "start": "4045810",
    "end": "4053520"
  },
  {
    "text": "Final things I want\nto say about this. So until recently,\nthe way you would",
    "start": "4053520",
    "end": "4058530"
  },
  {
    "text": "do even the\nretrieval-augmented thing would be that you\nwould have your index,",
    "start": "4058530",
    "end": "4063600"
  },
  {
    "text": "and then you might train\na custom purpose model to do the\nquestion-answering part. And it could extract things\nfrom the text that you produced",
    "start": "4063600",
    "end": "4070740"
  },
  {
    "text": "or maybe even generate some\nnew things from the text that you produced. And that's kind of the mode that\nI mentioned before where you'd",
    "start": "4070740",
    "end": "4077340"
  },
  {
    "text": "have some language models,\nmaybe a few of them, and you'd have an index. And you would\nstitch them together",
    "start": "4077340",
    "end": "4082500"
  },
  {
    "text": "into a question answering system\nthat you would probably train on question answering data.",
    "start": "4082500",
    "end": "4087857"
  },
  {
    "text": "And you would hope that\nthis whole big monster may be fine tuned on SQuAD,\nor natural questions,",
    "start": "4087857",
    "end": "4093150"
  },
  {
    "text": "or one of those data sets\ngave you a general purpose question answering capability.",
    "start": "4093150",
    "end": "4099990"
  },
  {
    "text": "That's the present, but\nI think it might actually be the recent past. And in fact, the way that\nyou all will probably",
    "start": "4099990",
    "end": "4106560"
  },
  {
    "text": "work when we do this unit and\ncertainly for the homework is that we will just\nhave frozen components.",
    "start": "4106560",
    "end": "4113850"
  },
  {
    "text": "And this starts\nfrom the observation that the retriever\nmodel is really just a model that takes in text\nand produces text with scores.",
    "start": "4113850",
    "end": "4122909"
  },
  {
    "text": "And a language model is also\na device for taking in text and producing text with scores.",
    "start": "4122910",
    "end": "4129860"
  },
  {
    "text": "And these are when these\nare frozen components. You can think of them as\njust black box devices that do this input/output thing.",
    "start": "4129861",
    "end": "4136028"
  },
  {
    "text": "And then you get into\nthe intriguing mode of asking, well, what if we had\nthem just talk to each other?",
    "start": "4136029",
    "end": "4141890"
  },
  {
    "text": "And that is what you will do\nfor the homework in bakeoff. You will have frozen retriever\nand a frozen large language",
    "start": "4141890",
    "end": "4148000"
  },
  {
    "text": "model. And you will get\nthem to work together to solve a very\ndifficult open domain",
    "start": "4148000",
    "end": "4154180"
  },
  {
    "text": "question answering problem. And that's pushing\nus into a new mode for even thinking about how we\ndesign AI systems where it's",
    "start": "4154180",
    "end": "4162130"
  },
  {
    "text": "not so much about\nfine tuning, it's much more about getting\nthem to communicate with each other effectively\nto design a system",
    "start": "4162130",
    "end": "4169719"
  },
  {
    "text": "from frozen components. Again, unanticipated at\nleast by me as of a few years",
    "start": "4169720",
    "end": "4175809"
  },
  {
    "text": "ago and now an\nexciting new direction. So just to wrap up,\nI think what I'll",
    "start": "4175810",
    "end": "4182450"
  },
  {
    "text": "do since we're near\nthe end of class here, I'll just finish\nup this one unit, and then we'll use\nsome of our time",
    "start": "4182450",
    "end": "4188180"
  },
  {
    "text": "next time to introduce a few\nother of these course themes, and that'll set us up well\nfor diving into transformers.",
    "start": "4188180",
    "end": "4195500"
  },
  {
    "text": "Final piece here\njust to inspire you, few-shot openQA is the task that\nyou will tackle for homework 2.",
    "start": "4195500",
    "end": "4202640"
  },
  {
    "text": "And here's how you could\nthink about this, imagine that the question has come in,\nwhat is the course to take?",
    "start": "4202640",
    "end": "4208840"
  },
  {
    "text": "The most standard\nthing we could do is just prompt\nthe language model with that question,\nwhat is the course",
    "start": "4208840",
    "end": "4214030"
  },
  {
    "text": "to take down here and see\nwhat answer it gave back. But the\nretrieval-augmented insight",
    "start": "4214030",
    "end": "4219969"
  },
  {
    "text": "is that we might also\nretrieve some kind of passage from a knowledge store here. I have a very short passage.",
    "start": "4219970",
    "end": "4225400"
  },
  {
    "text": "The course to take is Natural\nLanguage Understanding and that could be done\nwith a retrieval mechanism.",
    "start": "4225400",
    "end": "4231520"
  },
  {
    "text": "But why stop there? It might help the\nmodel as we saw going back to the\nGPT-3 paper to have",
    "start": "4231520",
    "end": "4237670"
  },
  {
    "text": "some examples of\nthe kind of behavior that I'm hoping to\nget from the model. And so here I have\nretrieved from some data set",
    "start": "4237670",
    "end": "4244810"
  },
  {
    "text": "question-answer pairs\nthat will kind of give it a sense for what I want\nit to do in the end. But again, why stop there?",
    "start": "4244810",
    "end": "4251450"
  },
  {
    "text": "We could also pick\nquestions that were based very\nclosely on the question",
    "start": "4251450",
    "end": "4256810"
  },
  {
    "text": "that we posed that\nwould be like K nearest neighbors approach\nwhere we use our retrieval mechanism to find similar\nquestions to the one",
    "start": "4256810",
    "end": "4264519"
  },
  {
    "text": "that we care about. I could also add in\nsome context passages, and I could do\nthat by retrieval.",
    "start": "4264520",
    "end": "4271160"
  },
  {
    "text": "So now we've used the retrieval\nmodel twice potentially. Once to get good demonstrations\nand once to provide context",
    "start": "4271160",
    "end": "4277880"
  },
  {
    "text": "for each one of them. But I could also use\nmy retrieval mechanism with the questions and\nanswers from the demonstration",
    "start": "4277880",
    "end": "4284630"
  },
  {
    "text": "to get even richer connections\nbetween my demonstrations and the passages. I could even use\na language model",
    "start": "4284630",
    "end": "4291560"
  },
  {
    "text": "to rewrite aspects of\nthose demonstrations to put them in a format\nthat might help me",
    "start": "4291560",
    "end": "4296780"
  },
  {
    "text": "with the final question\nthat I want to pose. So now I have an interwoven\nuse of the retrieval mechanism",
    "start": "4296780",
    "end": "4303590"
  },
  {
    "text": "and the large language model\nto build up this prompt, right? Down at the retrieval thing\nI could do the same thing.",
    "start": "4303590",
    "end": "4310949"
  },
  {
    "text": "And then when you think\nabout the model generation, again, we could just take the\ntop response from the model,",
    "start": "4310950",
    "end": "4317030"
  },
  {
    "text": "but we can do very\nsophisticated things on up to this full retrieval-augmented\ngeneration model, which",
    "start": "4317030",
    "end": "4324409"
  },
  {
    "text": "essentially marginalizes\nout the evidence passage and gives us a really\npowerful look at a good answer",
    "start": "4324410",
    "end": "4330989"
  },
  {
    "text": "conditional on that\nvery complicated prompt that we constructed. I think what you're\nseeing on the left here",
    "start": "4330990",
    "end": "4338579"
  },
  {
    "text": "is that we are going\nto move from an era where we just type in\nprompts into these models and hope for the best\ninto an era where",
    "start": "4338580",
    "end": "4346679"
  },
  {
    "text": "prompt construction is a\nkind of new programming mode, where you're writing\ndown computer code,",
    "start": "4346680",
    "end": "4353760"
  },
  {
    "text": "could be Python code, that is\ndoing traditional computing things but also drawing on very\npowerful pretrained components",
    "start": "4353760",
    "end": "4362280"
  },
  {
    "text": "to assemble this instruction kit\nfor your large language model",
    "start": "4362280",
    "end": "4367349"
  },
  {
    "text": "to do whatever task\nyou have set for it. And so instead of\ndesigning these AI systems",
    "start": "4367350",
    "end": "4373140"
  },
  {
    "text": "with all that fine tuning\nI described before, we might actually be moving\nback into a mode that's",
    "start": "4373140",
    "end": "4378720"
  },
  {
    "text": "like that symbolic mode\nfrom the '80s where you type in a computer program. It's just that now the\nprogram that you type in",
    "start": "4378720",
    "end": "4386579"
  },
  {
    "text": "is connected to these very\npowerful modern AI components. And we're seeing right now\nthat that is opening doors",
    "start": "4386580",
    "end": "4395550"
  },
  {
    "text": "to all kinds of new\ncapabilities for these systems, and this first\nhomework in bakeoff",
    "start": "4395550",
    "end": "4401100"
  },
  {
    "text": "is going to give you\na glimpse of that. And you're going to use\na programming model we've developed called\nDemonstrate-Search-Predict",
    "start": "4401100",
    "end": "4408060"
  },
  {
    "text": "that I hope will give\nyou a glimpse of just how powerful this can be. ",
    "start": "4408060",
    "end": "4414430"
  },
  {
    "text": "All right. We are out of time. Right? 4:20?",
    "start": "4414430",
    "end": "4420470"
  },
  {
    "text": "So next time I'll show you a\nfew more units from the course, and then we'll dive\ninto transformers.",
    "start": "4420470",
    "end": "4426849"
  },
  {
    "start": "4426850",
    "end": "4432000"
  }
]