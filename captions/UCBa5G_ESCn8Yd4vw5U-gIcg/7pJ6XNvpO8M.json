[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "CS229, Lecture 18. The topic for today will be continuing our study of, er, unsupervised learning.",
    "start": "4040",
    "end": "11965"
  },
  {
    "text": "So the topics today is, we're going to wrap up factor analysis. We mostly finished factor analysis last time, uh,",
    "start": "11965",
    "end": "19315"
  },
  {
    "text": "except we just stopped at the end where we finish the derivation and didn't really have- have time to kind of,",
    "start": "19315",
    "end": "25125"
  },
  {
    "text": "er, answer some of the pending questions and really get a good feel for factor analysis. It almost felt like, you know,",
    "start": "25125",
    "end": "31650"
  },
  {
    "text": "monstrous looking expressions and that's it. Er, but let's- let's, er, kind of quickly revisit factor analysis to- to",
    "start": "31650",
    "end": "38410"
  },
  {
    "text": "get some better understanding of what's happening under the covers. It's a lot easier than it looks. Uh, and then the plan is to, uh,",
    "start": "38410",
    "end": "46140"
  },
  {
    "text": "finish up Principal Components Analysis and maybe start ICA: Independent Components Analysis.",
    "start": "46140",
    "end": "52010"
  },
  {
    "text": "I'm not sure, uh, how much time we'll have for this but hopefully, we'll at least start this and if possible finish it, all right?",
    "start": "52010",
    "end": "58200"
  },
  {
    "text": "So a quick recap of, er, the last lecture. The last lecture, er, one- one big proof that we covered was",
    "start": "58200",
    "end": "65840"
  },
  {
    "start": "60000",
    "end": "240000"
  },
  {
    "text": "the convergence of the expectation maximization algorithm and the proof is pretty simple.",
    "start": "65840",
    "end": "70865"
  },
  {
    "text": "It was, er, we- we wanted to show that, er, the EM algorithm is guaranteed to increase the likelihood of our,",
    "start": "70865",
    "end": "80030"
  },
  {
    "text": "er, of- of- of the observed data in every step, in every iteration. All right, and the way we went about doing that is to show that",
    "start": "80030",
    "end": "88085"
  },
  {
    "text": "the likelihood at theta of t plus 1. We want to show that it is greater than, er,",
    "start": "88085",
    "end": "94085"
  },
  {
    "text": "the likelihood at the previous, er, er, theta of t. And the way we went about doing that",
    "start": "94085",
    "end": "99500"
  },
  {
    "text": "is to- is to show that the likelihood is greater than the ELBO. You know, Jensen's inequality tells us that the ELBO,",
    "start": "99500",
    "end": "105890"
  },
  {
    "text": "er, for the same- at- at the same parameter value, er, the ELBO for any value of t- any value of q will always be less than or equal 2,",
    "start": "105890",
    "end": "115700"
  },
  {
    "text": "that's Jensen's inequality and then we- um, we saw what happened when we switched from theta t plus 1 to theta t and we- and this",
    "start": "115700",
    "end": "124130"
  },
  {
    "text": "basically was the M-step at time step t. So at the M step of time-step t,",
    "start": "124130",
    "end": "129530"
  },
  {
    "text": "we optimize theta to obtain theta t plus 1,  such that the ELBO was maximized.",
    "start": "129530",
    "end": "134735"
  },
  {
    "text": "So theta t plus 1 is the maximizer of this- of- of- of the ELBO at qt.",
    "start": "134735",
    "end": "140210"
  },
  {
    "text": "So therefore, at Theta t, the ELBO value will always be, er, lower and then we saw from the corollary of Jensen's inequality that,",
    "start": "140210",
    "end": "148265"
  },
  {
    "text": "er, that the ELBO at- at theta t and qt is equal to,",
    "start": "148265",
    "end": "153990"
  },
  {
    "text": "er, the likelihood, um, at theta t, right? So it was three simple steps and we- we also kind of saw the,",
    "start": "153990",
    "end": "164460"
  },
  {
    "text": "er, pictorial intuition- intuition for this. So at Theta t, let's call this -this pen is not good [NOISE].",
    "start": "164460",
    "end": "178380"
  },
  {
    "text": "Yeah, so this is ELBO of qt theta t, right?",
    "start": "178380",
    "end": "189700"
  },
  {
    "text": "This is theta t where it is tight at, er, the likelihood. So this is just L theta and this is the ELBO where at",
    "start": "190100",
    "end": "199110"
  },
  {
    "text": "theta t it is tight and then we maximize it and we get theta t plus 1 and we construct a new ELBO at this tighter theta lt plus 1.",
    "start": "199110",
    "end": "213375"
  },
  {
    "text": "So this ELBO is this, and this ELBO is this, right?",
    "start": "213375",
    "end": "220275"
  },
  {
    "text": "And- and - and so we got that L of theta t plus 1 was- was less than the- the- um,",
    "start": "220275",
    "end": "231410"
  },
  {
    "text": "um- l- l- likelihood at theta t is less than likelihood of theta t plus 1, right?",
    "start": "231410",
    "end": "238760"
  },
  {
    "text": "So that was, er, the, er, EM proof and then we moved on to factor analysis. Where in factor analysis,",
    "start": "238760",
    "end": "245485"
  },
  {
    "start": "240000",
    "end": "2070000"
  },
  {
    "text": "we- we first- we were interested in the case where we want to model Xs that live in a d-dimensional space but we have n number of x's,",
    "start": "245485",
    "end": "255754"
  },
  {
    "text": "where n is much smaller than d, right? And in these cases, er, we saw that, um,",
    "start": "255755",
    "end": "261894"
  },
  {
    "text": "performing- modeling it as a simple Gaussian will give us singular covariance matrices and therefore,",
    "start": "261895",
    "end": "268880"
  },
  {
    "text": "we- instead what we, er, want to do is to have a latent variable corresponding to z",
    "start": "268880",
    "end": "275180"
  },
  {
    "text": "that lives in a low-dimensional space and have a- a matrix l from which the low-dimensional space is mapped up to",
    "start": "275180",
    "end": "284270"
  },
  {
    "text": "a high-dimensional, a d-dimensional space and there is some extra noise added,",
    "start": "284270",
    "end": "289440"
  },
  {
    "text": "er, in the d-dimensional space, right? And so the- so the model was defined like this.",
    "start": "289440",
    "end": "294725"
  },
  {
    "text": "Z comes from a lowers- a low dimensional Gaussian which has these k dimensions.",
    "start": "294725",
    "end": "300319"
  },
  {
    "text": "So in- in this problem, k is less than, um, d, but k is less than- is greater than n, right?",
    "start": "300319",
    "end": "310715"
  },
  {
    "text": "So this is the assumption we are operating where n is much smaller than d, er,",
    "start": "310715",
    "end": "315949"
  },
  {
    "text": "but k is also smaller than d but but k is bigger than n. That's- that's the, er, setting we're, er, er, working in.",
    "start": "315949",
    "end": "322990"
  },
  {
    "text": "Or did I flip it? So we want- k should be less than d, but, er,",
    "start": "322990",
    "end": "330390"
  },
  {
    "text": "d is much bigger than n but we want n to be greater than k. Yeah, that's better, right?",
    "start": "330390",
    "end": "340230"
  },
  {
    "text": "So we want, um- so z lives in a k-dimensional, er, um, subspace, er,",
    "start": "340230",
    "end": "345569"
  },
  {
    "text": "and it's- it's normally distributed with mean 0 and identity covariance and then x given z,",
    "start": "345570",
    "end": "351430"
  },
  {
    "text": "we assume is generated in this way where mu, L and psi are parameters, right?",
    "start": "351430",
    "end": "357890"
  },
  {
    "text": "So mu is- is like an offset for- in the x-dimensional space.",
    "start": "357890",
    "end": "363320"
  },
  {
    "text": "So once we map- map z to high-dimensional space, it will still be mean 0, right?",
    "start": "363320",
    "end": "370025"
  },
  {
    "text": "So this is like an offset that we add. And, um, psi is some kind of an extra covariance, um,",
    "start": "370025",
    "end": "376560"
  },
  {
    "text": "an extra diagonal matrix where each dimension in the d-dimensional space has an independent noise that's added, right?",
    "start": "376560",
    "end": "385365"
  },
  {
    "text": "So at first, this- these might seem pretty, er, arbitrary choices. However, if we kind of,",
    "start": "385365",
    "end": "391935"
  },
  {
    "text": "er, visualize what's happening, er, basically what we have is, um,",
    "start": "391935",
    "end": "397125"
  },
  {
    "text": "this- this process can be visualized like this, right? So, um, from this,",
    "start": "397125",
    "end": "402545"
  },
  {
    "text": "we see that x minus Mu is distributed according to, um, um, mean lz and some, er, noise psi.",
    "start": "402545",
    "end": "411495"
  },
  {
    "text": "So we can think of the set of Zs, the set of latent- latent variables that we have.",
    "start": "411495",
    "end": "418025"
  },
  {
    "text": "Imagine we have, you know, a design matrix of these z's, right? Just like in linear regression.",
    "start": "418025",
    "end": "423575"
  },
  {
    "text": "So each z_i is one example. We have n such examples which of- of k dimension and then suppose we have,",
    "start": "423575",
    "end": "431960"
  },
  {
    "text": "you know, L1, right? Consider L1 to be like theta, right?",
    "start": "431960",
    "end": "439755"
  },
  {
    "text": "So we have this- this theta, um, which is like, you know, a parameter and when we- when we multiply this- this design matrix by theta,",
    "start": "439755",
    "end": "449830"
  },
  {
    "text": "we get one sort of column, one column over here, right?",
    "start": "449830",
    "end": "454955"
  },
  {
    "text": "So you can think of this as d independent linear regression problems.",
    "start": "454955",
    "end": "460235"
  },
  {
    "text": "All right, so L is d by k and k and d, um, um, k- k matches the,",
    "start": "460235",
    "end": "466290"
  },
  {
    "text": "er, dimension of the design matrix. So think of this as the design matrix, design matrix and think of this as d theta vectors and think of this",
    "start": "466290",
    "end": "479160"
  },
  {
    "text": "as d label vectors, right?",
    "start": "479160",
    "end": "488775"
  },
  {
    "text": "So what's- what's happening is for each dimension in which x lives,",
    "start": "488775",
    "end": "495220"
  },
  {
    "text": "we associate a parameter vector. That's one way to think of it. We associate a- a parameter vector and we get a noisy observation,",
    "start": "495220",
    "end": "504115"
  },
  {
    "text": "which is x minus- x- one column of x minus mu, right?",
    "start": "504115",
    "end": "510960"
  },
  {
    "text": "And- and we have- so- so this- this, um, um, in linear regression, we had a design matrix,",
    "start": "510960",
    "end": "518020"
  },
  {
    "text": "one parameter vector, which we tried to fit to one column vector of Ys.",
    "start": "518020",
    "end": "524255"
  },
  {
    "text": "So this was y this was theta and this was x but here the terminology is different,",
    "start": "524255",
    "end": "529745"
  },
  {
    "text": "we have a z design matrix and d different parameter vectors,",
    "start": "529745",
    "end": "535069"
  },
  {
    "text": "d different and for each vector we have, you know, a different, different label, right?",
    "start": "535069",
    "end": "541139"
  },
  {
    "text": "So you can think of this as d different linear regressions happening simultaneously.",
    "start": "541140",
    "end": "545910"
  },
  {
    "text": "Any questions? Any questions here? Right. Yes, question.",
    "start": "546320",
    "end": "551339"
  },
  {
    "text": "[inaudible]",
    "start": "551340",
    "end": "558560"
  },
  {
    "text": "Yeah, so every single, exactly, so every single dimension in the d dimensional space you can think of um, uh,",
    "start": "558560",
    "end": "565115"
  },
  {
    "text": "you can think of the column corresponding to, uh, every single dimension in the largest space -corre- is",
    "start": "565115",
    "end": "571930"
  },
  {
    "text": "like a label vector for that linear regression. So if you do not want to start seeing the prediction move",
    "start": "571930",
    "end": "577690"
  },
  {
    "text": "to a certain dimension. We're gonna come to prediction soon. Yes, so for now, this is the setting we are operating and we subtra- subtract x minus mu with,",
    "start": "577690",
    "end": "586300"
  },
  {
    "text": "you can think of mu as being the bias term, right? So bias term, you can add, uh, uh, you know,",
    "start": "586300",
    "end": "593964"
  },
  {
    "text": "each component of mu, you can think of it as a bias term you're adding to each of the d problems.",
    "start": "593965",
    "end": "600699"
  },
  {
    "text": "And so if you subtract the mean from x is, you get this kind of um, um,",
    "start": "600700",
    "end": "605920"
  },
  {
    "text": "uh, a setting where this is, this is, this is like a linear regression problem.",
    "start": "605920",
    "end": "611665"
  },
  {
    "text": "And in linear regression problem, we also assume that y is equal to theta transpose _ x plus epsilon,",
    "start": "611665",
    "end": "620305"
  },
  {
    "text": "where epsilon was independent noise. Whereas over here, we are doing x minus mu is equal to,",
    "start": "620305",
    "end": "629360"
  },
  {
    "text": "is equal to z l plus psi_ii.",
    "start": "629670",
    "end": "638785"
  },
  {
    "text": "So this is one, you can think of this as z_i and this is the first, the first row.",
    "start": "638785",
    "end": "646795"
  },
  {
    "text": "So this is like a z_i transpose l_1 plus psi_ii,",
    "start": "646795",
    "end": "652029"
  },
  {
    "text": "where this is the diagonal matrix that we are adding, right? So this is, this model is basically trying to",
    "start": "652030",
    "end": "659320"
  },
  {
    "text": "represent the different linear regression models in, in, in a vector- vectorized notation where all of",
    "start": "659320",
    "end": "666220"
  },
  {
    "text": "them are written in this kind of matrix, matrix, matrix notation. Whereas previously we also had this matrix vector, vector notation.",
    "start": "666220",
    "end": "673420"
  },
  {
    "text": "All right, that's all that's happening here. And from this we, ah, constructed the E step and the M step, right?",
    "start": "673420",
    "end": "680050"
  },
  {
    "text": "So the E step was pretty straight forward. So q_i was this, so q_i we set,",
    "start": "680050",
    "end": "685450"
  },
  {
    "text": "we set it with the posterior distribution at the current values of uh,",
    "start": "685450",
    "end": "691225"
  },
  {
    "text": "mu, l, and psi. And this notation is pretty straight forward. It is just the conditional of the Gaussian.",
    "start": "691225",
    "end": "698320"
  },
  {
    "text": "We've seen this kind of notation in Gaussian process as well. So for every i for every example,",
    "start": "698320",
    "end": "705220"
  },
  {
    "text": "we set q_i to be a normal distribution that has this mean and this covariance.",
    "start": "705220",
    "end": "710245"
  },
  {
    "text": "And these are just, if you remember this just comes from the Schur complement and this, ah, follows the, the,",
    "start": "710245",
    "end": "717100"
  },
  {
    "text": "ah, notation where, you know, ah, we consider so much to be like the z value of,",
    "start": "717100",
    "end": "723204"
  },
  {
    "text": "of the x variable, where you subtract the mean divided by the variance and then you,",
    "start": "723205",
    "end": "730209"
  },
  {
    "text": "this also has like the correlation coefficient embedded in this, and then you multiply it by a- a,",
    "start": "730210",
    "end": "737785"
  },
  {
    "text": "multiplied by l_transpose, right? So this is just like transforming, uh,",
    "start": "737785",
    "end": "743140"
  },
  {
    "text": "uh, in the case of, in the case of multivariate Gaussians. And this is the Schur compliment from which we get the covariance of z given i.",
    "start": "743140",
    "end": "751329"
  },
  {
    "text": "So the goal here was we calculated z_i given x_i and that's,",
    "start": "751330",
    "end": "757825"
  },
  {
    "text": "that's, that's, that's q_i, right. And from this, we come up with the M step.",
    "start": "757825",
    "end": "763810"
  },
  {
    "text": "So in the E step we always estimate the latent variables and in the M step we estimate the parameters, right? That's, that's going to be common for any model, including,",
    "start": "763810",
    "end": "771505"
  },
  {
    "text": "including the Gaussian, Gaussian mixture model and factor analysis. So in the E step, we estimated the latent variables z_i, and in the M step,",
    "start": "771505",
    "end": "779125"
  },
  {
    "text": "we re-estimate the parameters where l, mu, and psi are the parameters.",
    "start": "779125",
    "end": "785275"
  },
  {
    "text": "So l, we had this monstrous looking expression,",
    "start": "785275",
    "end": "790780"
  },
  {
    "text": "mu was just the mean of the exercise that should be intuitive, right? Because we are just,",
    "start": "790780",
    "end": "796855"
  },
  {
    "text": "what we are doing is, is, is. uh, transforming z via the l and adding some mu.",
    "start": "796855",
    "end": "802330"
  },
  {
    "text": "So l z has mean 0. So uh, obviously the,",
    "start": "802330",
    "end": "807714"
  },
  {
    "text": "the, the maximum likelihood estimate of mu will just be the mean of x's. And in fact, that will not change from iteration to iteration, right?",
    "start": "807715",
    "end": "815980"
  },
  {
    "text": "So that's just gonna be mu. So you calculate it once and in the next M step you can just reuse it.",
    "start": "815980",
    "end": "821529"
  },
  {
    "text": "There's, there's nothing that changes here. And this psi_ii was this, you know,",
    "start": "821530",
    "end": "826870"
  },
  {
    "text": "even horrible looking expression worse than the l, ah, ah, expression for the l. And,",
    "start": "826870",
    "end": "833905"
  },
  {
    "text": "and, uh, and that's where we stopped last lecture.",
    "start": "833905",
    "end": "838945"
  },
  {
    "text": "However, even though these look pretty scary, we can actually kind of dissect this a little more and,",
    "start": "838945",
    "end": "845545"
  },
  {
    "text": "and get a better understanding of what's happening, right? So first of all, let's start with the l,",
    "start": "845545",
    "end": "853100"
  },
  {
    "text": "right? So to, to, uh, to make this understanding easier,",
    "start": "862980",
    "end": "868555"
  },
  {
    "text": "I'll introduce some new notation, right? So mu of mu z_i given x_i right?",
    "start": "868555",
    "end": "880449"
  },
  {
    "text": "So mu of z_i given x_i is the mean of the posterior distribution that you obtain in the,",
    "start": "880450",
    "end": "885550"
  },
  {
    "text": "in the E step, right? It's a Gaussian that is concentrated at mu. So you can think of this as loosely speaking let's,",
    "start": "885550",
    "end": "892525"
  },
  {
    "text": "let's call this kind of z hat_i, right? So it's, it's our best,",
    "start": "892525",
    "end": "898149"
  },
  {
    "text": "best guess of what z is, given our knowledge about x's, right?",
    "start": "898150",
    "end": "903790"
  },
  {
    "text": "So, so this is our best estimate of, of z_i, right? Does it make sense? This just our best, best,",
    "start": "903790",
    "end": "910329"
  },
  {
    "text": "best guess or best estimate about z_i, given what we know about x. So that's one notation.",
    "start": "910330",
    "end": "917095"
  },
  {
    "text": "And now, um, x_i minus mu, alright let's, let's,",
    "start": "917095",
    "end": "927069"
  },
  {
    "text": "um, and we also have this term,",
    "start": "927070",
    "end": "932440"
  },
  {
    "text": "l mu_z_i given x_i.",
    "start": "932440",
    "end": "938020"
  },
  {
    "text": "So now, l mu, l times mu_z_i given x_i is equal to basically l times z hat_i, right?",
    "start": "938020",
    "end": "950245"
  },
  {
    "text": "We just defined ah, we saw that mu of z_i given x_i we, we can think of this as our best estimate of z.",
    "start": "950245",
    "end": "956420"
  },
  {
    "text": "And l times z_i, we see that, you know,",
    "start": "957000",
    "end": "962140"
  },
  {
    "text": "l times z_i is our best estimate of x minus mu, right?",
    "start": "962140",
    "end": "969280"
  },
  {
    "text": "So l times z_i, think of this as our best estimate of,",
    "start": "969280",
    "end": "979640"
  },
  {
    "text": "of x_i minus mu.",
    "start": "980370",
    "end": "990940"
  },
  {
    "text": "And therefore, we will call this x hat_i minus mu.",
    "start": "990940",
    "end": "997700"
  },
  {
    "text": "So, l of mu z_i given x_i is just l of z, z hat_i.",
    "start": "997980",
    "end": "1004260"
  },
  {
    "text": "And that's the same as x hat_i minus mu, right?",
    "start": "1004260",
    "end": "1010545"
  },
  {
    "text": "Now, once we, we, ah, uh, so what, what,",
    "start": "1010545",
    "end": "1016380"
  },
  {
    "text": "what do we, ah, ah, what can we, ah, think of this as? So think of l z hat_i to be the reconstruction of x.",
    "start": "1016380",
    "end": "1026069"
  },
  {
    "text": "So z hat_i is our best guess of what z is given x's.",
    "start": "1026070",
    "end": "1031694"
  },
  {
    "text": "And then l times z hat, uh, x_i is what, you know, is, is now we are going from z back to x.",
    "start": "1031695",
    "end": "1037980"
  },
  {
    "text": "So you can think of l z hat_i plus mu to be the reconstruction of x, right?",
    "start": "1037980",
    "end": "1047040"
  },
  {
    "text": "So over here, plug in z hat x_i here, multiply it by l,",
    "start": "1047040",
    "end": "1052140"
  },
  {
    "text": "and we get x hat minus mu, right? So this is like the reconstruction of x_i.",
    "start": "1052140",
    "end": "1058785"
  },
  {
    "text": "So that expression can now be written as l is equal",
    "start": "1058785",
    "end": "1066330"
  },
  {
    "text": "to sum over i equals 1 to n, x_i minus mu.",
    "start": "1066330",
    "end": "1073485"
  },
  {
    "text": "So x_i minus mu let's, let's, ah, you know, um, think of it as,",
    "start": "1073485",
    "end": "1078510"
  },
  {
    "text": "as y_i, you know, following the linear regression example.",
    "start": "1078510",
    "end": "1085669"
  },
  {
    "text": "And, mu, uh, uh, mu of, of, z_i,",
    "start": "1085670",
    "end": "1099539"
  },
  {
    "text": "so mu of z_i, let's, let's call this, so mu of z_i is basically this entire vector z so mu of z_i is our estimate of,",
    "start": "1099540",
    "end": "1109845"
  },
  {
    "text": "of, ah, the z vector, so let's just, so I'm just gonna call it capital z_i transpose, right?",
    "start": "1109845",
    "end": "1124395"
  },
  {
    "text": "And here again, this- this basically is- is, can think of it as zz,",
    "start": "1124395",
    "end": "1130559"
  },
  {
    "text": "z transpose z [NOISE] plus,",
    "start": "1130560",
    "end": "1136275"
  },
  {
    "text": "you know, some- some, uh, some covariance inverse. Or if- if we,",
    "start": "1136275",
    "end": "1143370"
  },
  {
    "text": "uh, um, we take the transpose, [NOISE] we get [NOISE] We don't need that much so this is basically like,",
    "start": "1143370",
    "end": "1155265"
  },
  {
    "text": "um, um, you can think of this as, you know, L transpose is equal to and this comes here.",
    "start": "1155265",
    "end": "1163365"
  },
  {
    "text": "Z transpose, Z plus, you know, something inverse Z transpose y, right?",
    "start": "1163365",
    "end": "1173925"
  },
  {
    "text": "So basically we are- we are doing some kind of a regularized linear regression, right? That's what's happening in the L expression over there, right?",
    "start": "1173925",
    "end": "1181530"
  },
  {
    "text": "So we have- we- we make our best guesses about Z, we have x's that are given to us.",
    "start": "1181530",
    "end": "1188865"
  },
  {
    "text": "And using those best- best guesses for Z, we may- update our L and for that we're just solving",
    "start": "1188865",
    "end": "1195735"
  },
  {
    "text": "d different linear regressions in parallel, right?",
    "start": "1195735",
    "end": "1200880"
  },
  {
    "text": "So that's- that's, um, uh, that's L. Now, what about Psi, right?",
    "start": "1200880",
    "end": "1206700"
  },
  {
    "text": "So Psi is also this, you know, scary-looking expression and we're gonna use,",
    "start": "1206700",
    "end": "1212370"
  },
  {
    "text": "you know, the same terminologies, uh, in there as well. So, um, [NOISE] what we end up getting there is, um,",
    "start": "1212370",
    "end": "1220980"
  },
  {
    "text": "something like you can- you can think of this as, you know,",
    "start": "1220980",
    "end": "1228419"
  },
  {
    "text": "i equals one to n x^i, x^i transpose.",
    "start": "1228420",
    "end": "1235875"
  },
  {
    "text": "And now in place of- of Mu of, uh, uh, z^i, um, so over here, uh,",
    "start": "1235875",
    "end": "1244290"
  },
  {
    "text": "let's call L Mu to be x hat so that's- that is minus x^i,",
    "start": "1244290",
    "end": "1251760"
  },
  {
    "text": "you know, x hat i transpose. And then L Mu is again x hat i minus x hat i x^i transpose minus,",
    "start": "1251760",
    "end": "1266220"
  },
  {
    "text": "if we expand the- the- uh, the matrix on the other side, we get plus L Mu.",
    "start": "1266220",
    "end": "1273120"
  },
  {
    "text": "You know, think of it as x hat i,",
    "start": "1273120",
    "end": "1279760"
  },
  {
    "text": "x hat i and the L comes from the other side,",
    "start": "1279920",
    "end": "1285875"
  },
  {
    "text": "x hat i transpose plus L",
    "start": "1285875",
    "end": "1292160"
  },
  {
    "text": "Sigma z^i given x^i L transpose, right?",
    "start": "1292160",
    "end": "1299700"
  },
  {
    "text": "And now this can be written as, [NOISE] i equals 1 to n x^i minus x hat i,",
    "start": "1299700",
    "end": "1310200"
  },
  {
    "text": "[NOISE] x^i minus x hat i transpose",
    "start": "1310200",
    "end": "1317519"
  },
  {
    "text": "plus L times z^i given x^i L transpose, right?",
    "start": "1317520",
    "end": "1326985"
  },
  {
    "text": "So this, if you- if you- if, uh, if you recognize what we're doing is we're taking the-",
    "start": "1326985",
    "end": "1333450"
  },
  {
    "text": "the correct label minus our estimate of the correct label, right?",
    "start": "1333450",
    "end": "1339225"
  },
  {
    "text": "And- and, uh, uh, this is- is- is- is therefore similar to",
    "start": "1339225",
    "end": "1344520"
  },
  {
    "text": "estimating the noise in linear regression, right? So in- in linear regression, if y equals Theta transpose x plus Epsilon,",
    "start": "1344520",
    "end": "1352740"
  },
  {
    "text": "the way to estimate Epsilon from your data is to first find out Theta hat and then do 1 over n,",
    "start": "1352740",
    "end": "1359910"
  },
  {
    "text": "i equals one to n, y^i minus Theta hat transpose x^i y squared, right?",
    "start": "1359910",
    "end": "1370260"
  },
  {
    "text": "So this is- [NOISE] this is, uh, this the way we estimated the estimate Epsilon in- in linear regression if you need to.",
    "start": "1370260",
    "end": "1376950"
  },
  {
    "text": "And similarly, you know, we are- we are estimating the Psi, which is the noise that's getting added to- to x^i's, right?",
    "start": "1376950",
    "end": "1385230"
  },
  {
    "text": "And this, because of the notation, turns out to be a full matrix however,",
    "start": "1385230",
    "end": "1390554"
  },
  {
    "text": "we are- we are interested in each of the d examples independently. And therefore, this- this matrix that you obtain,",
    "start": "1390554",
    "end": "1399570"
  },
  {
    "text": "we're just going to take the i- ith element to be [NOISE] ith element of this.",
    "start": "1399570",
    "end": "1405139"
  },
  {
    "text": "This is basically like saying we don't care about the covariance of the noise between one linear regression problem",
    "start": "1405140",
    "end": "1412070"
  },
  {
    "text": "and another regression- linear regression problem, right? So we're just ignoring the off-diagonal elements here.",
    "start": "1412070",
    "end": "1417365"
  },
  {
    "text": "Okay, does that make sense, any questions? How did you get the L matrix [inaudible]? [NOISE]",
    "start": "1417365",
    "end": "1425210"
  },
  {
    "text": "So the L matrix is what we- what we have in the- in the update. That- that's or was it a different question.",
    "start": "1425210",
    "end": "1433010"
  },
  {
    "text": "[inaudible]",
    "start": "1433010",
    "end": "1439170"
  },
  {
    "text": "So x hat, you can think of x hat as- so if you- so can think of, you know, um,",
    "start": "1439170",
    "end": "1446280"
  },
  {
    "text": "you can think of- of, um, factor analysis as this, you're given the y labels for different problems, right?",
    "start": "1446280",
    "end": "1453855"
  },
  {
    "text": "And by just given, by just having the y labels, you want to construct both the covariates as well as the parameters, right?",
    "start": "1453855",
    "end": "1462780"
  },
  {
    "text": "It's a really hard problem, right? You're given x's and you want to estimate both z's and L's. L's are the parameters that we- that- that's part of the model, right?",
    "start": "1462780",
    "end": "1472980"
  },
  {
    "text": "And z^i's are the latent variables that's specific to ea- each example, right? So our goal is to not only estimate what our design matrix was,",
    "start": "1472980",
    "end": "1481170"
  },
  {
    "text": "but also estimate what the parameters was by just having the labels, right?",
    "start": "1481170",
    "end": "1486180"
  },
  {
    "text": "Which- which- which sounds, you know, kind of impossible but given these- these, uh,",
    "start": "1486180",
    "end": "1492270"
  },
  {
    "text": "modeling assumptions that z's come from- from, uh, uh, normally come from a normal distribution",
    "start": "1492270",
    "end": "1498585"
  },
  {
    "text": "and that they follow this kind of a linear relation. You can apply EM algorithm to this and you can estimate it.",
    "start": "1498585",
    "end": "1503789"
  },
  {
    "text": "[NOISE] Was- there was another question?",
    "start": "1503790",
    "end": "1509190"
  },
  {
    "text": "Okay. So you can think of- think of, uh, uh,",
    "start": "1509190",
    "end": "1515370"
  },
  {
    "text": "factor analysis as this problem where you're given just the labels for d different problems, right?",
    "start": "1515370",
    "end": "1523275"
  },
  {
    "text": "And you're assuming, a linear modeling of some hidden covariates we- we don't know the design matrix.",
    "start": "1523275",
    "end": "1531870"
  },
  {
    "text": "We don't know the parameters, and we're estimating both the design matrix and the set of",
    "start": "1531870",
    "end": "1537870"
  },
  {
    "text": "parameters given just the labels, right? It sounds of it- it's kind of magical here you're just given labels and you are estimating",
    "start": "1537870",
    "end": "1544770"
  },
  {
    "text": "both what your x's were as well as the parameters with only the assumption that your- your- your,",
    "start": "1544770",
    "end": "1551655"
  },
  {
    "text": "um, the only assumption that the inputs are distributed according to some kind of a normal distribution with Mu, right?",
    "start": "1551655",
    "end": "1559170"
  },
  {
    "text": "And Mu here just ends up being the- the bias term, you know, one per- one per- uh,",
    "start": "1559170",
    "end": "1564570"
  },
  {
    "text": "one per linear regression problem, [NOISE] right? Yeah, so this looks kind of monstrous,",
    "start": "1564570",
    "end": "1571199"
  },
  {
    "text": "but if you- if you think of each of these L Mu [NOISE] to be like x hat, right?",
    "start": "1571199",
    "end": "1579870"
  },
  {
    "text": "Then you can visualize this as, you know, uh, estimating just the- the reconstruction noise between x and x hat.",
    "start": "1579870",
    "end": "1587625"
  },
  {
    "text": "Um, and, and we're only interested in the diagonal elements here. [NOISE] All right so that wraps up our coverage of factor analysis oh, and, uh,",
    "start": "1587625",
    "end": "1598890"
  },
  {
    "text": "[NOISE] I think there was also one question which a- a student asked by doing all of this,",
    "start": "1598890",
    "end": "1604320"
  },
  {
    "text": "how did we actually go about solving the- the singularity problem where, you know [NOISE] so if you remember, um.",
    "start": "1604320",
    "end": "1612820"
  },
  {
    "text": "We briefly wrote this notation, [NOISE] that for factor analysis, you know,",
    "start": "1613040",
    "end": "1624180"
  },
  {
    "text": "log p of x, of the marginal likelihood actually had this closed-form expression where it turns out to be, um,",
    "start": "1624180",
    "end": "1632730"
  },
  {
    "text": "our x happens to be a normal with",
    "start": "1632730",
    "end": "1639705"
  },
  {
    "text": "mean Mu and covariance LL transpose plus Psi, right?",
    "start": "1639705",
    "end": "1649215"
  },
  {
    "text": "So the covariance matrix estimated for the marginal of x is therefore this LL transpose,",
    "start": "1649215",
    "end": "1656970"
  },
  {
    "text": "which is a low-rank matrix plus a diagonal matrix. And because we are adding a diagonal matrix to this low-rank matrix,",
    "start": "1656970",
    "end": "1663855"
  },
  {
    "text": "we are guaranteed that this is no longer singular, [NOISE] right?",
    "start": "1663855",
    "end": "1673210"
  },
  {
    "text": "All right, so let's move on uh, so this- this basically wraps up factor analysis,",
    "start": "1673520",
    "end": "1679770"
  },
  {
    "text": "and now we will start [NOISE] Principal Components Analysis. PCA.",
    "start": "1679770",
    "end": "1688200"
  },
  {
    "text": "[NOISE] PCA. [NOISE]",
    "start": "1688200",
    "end": "1724125"
  },
  {
    "text": "So in factor analysis, our goal was to- to model",
    "start": "1724125",
    "end": "1730035"
  },
  {
    "text": "X's as something that approximately lies in a low-dimensional space, right? So for every X we had a corresponding Z,",
    "start": "1730035",
    "end": "1737669"
  },
  {
    "text": "and Z lived in a low-dimensional space, and the relation between X and Z was- was,",
    "start": "1737670",
    "end": "1743970"
  },
  {
    "text": "uh, what you call as affine. Affine is another word for linear plus some offset, right?",
    "start": "1743970",
    "end": "1749340"
  },
  {
    "text": "So, um, L defined, um, L gives us from Z to the centered X,",
    "start": "1749340",
    "end": "1754815"
  },
  {
    "text": "and then you add some kind of an offset, so X and Z are related by what's also called an affine relationship, right?",
    "start": "1754815",
    "end": "1761100"
  },
  {
    "text": "So, um, and we had to go through this EM algorithm because we were,",
    "start": "1761100",
    "end": "1769125"
  },
  {
    "text": "um, um, we were modeling it with this- with the assumption that d was much larger than n, right?",
    "start": "1769125",
    "end": "1776220"
  },
  {
    "text": "PCA is another approach to, uh,",
    "start": "1776220",
    "end": "1782190"
  },
  {
    "text": "where you are given X_i [NOISE] i equals 1-n,",
    "start": "1782190",
    "end": "1787409"
  },
  {
    "text": "and X_i [NOISE] is, uh,",
    "start": "1787410",
    "end": "1793215"
  },
  {
    "text": "in d. And now here we no longer, um, um,",
    "start": "1793215",
    "end": "1800534"
  },
  {
    "text": "we no longer assume that d is much larger than n. We- we are back into the usual regime where we have,",
    "start": "1800535",
    "end": "1807885"
  },
  {
    "text": "you know, generally have more number of- much more number of data points, than the dimensions of the space.",
    "start": "1807885",
    "end": "1814784"
  },
  {
    "text": "And now we want to- [NOISE] we want to",
    "start": "1814785",
    "end": "1820500"
  },
  {
    "text": "find out if our data actually lives in a low-dimensional space, right?",
    "start": "1820500",
    "end": "1825765"
  },
  {
    "text": "So, um, for example, if- you know, here's one example.",
    "start": "1825765",
    "end": "1831130"
  },
  {
    "text": "So if this is, uh, a dataset where we are- we are plotting,",
    "start": "1835490",
    "end": "1840660"
  },
  {
    "text": "so this is X_1 and X_2. So if you have a dataset where you are plotting- [NOISE] the example in the notes is, uh,",
    "start": "1840660",
    "end": "1849990"
  },
  {
    "text": "supposing you are, uh, uh, [NOISE] you fly helicopters or you fly,",
    "start": "1849990",
    "end": "1855350"
  },
  {
    "text": "you know, toy helicopters, whatever. And, um, for every- for every such, you know,",
    "start": "1855350",
    "end": "1861855"
  },
  {
    "text": "pilot, you have a skill level, and the level they enjoy flying the helicopter, right? And if you are trying to- to, um,",
    "start": "1861855",
    "end": "1870090"
  },
  {
    "text": "if- if you look at the scatter plot of skill versus enjoyment,",
    "start": "1870090",
    "end": "1876340"
  },
  {
    "text": "you know, you wouldn't be surprised if you see some kind of a linear relationship between them.",
    "start": "1877040",
    "end": "1882465"
  },
  {
    "text": "It's more or less because, you know, people who enjoy flying helicopters are likely to be,",
    "start": "1882465",
    "end": "1889205"
  },
  {
    "text": "you know, good at it and vice versa, right? So, um, even though you have two different variables, they are kind of, you know,",
    "start": "1889205",
    "end": "1896255"
  },
  {
    "text": "reading off this underlying, you know, common, um, um, you know,",
    "start": "1896255",
    "end": "1903690"
  },
  {
    "text": "common, uh, trait of the pilot, you can call it, you know, piloting karma or whatever. And, you know, and- and therefore these two are, are pretty correlated.",
    "start": "1903690",
    "end": "1912255"
  },
  {
    "text": "And now, um, your data may therefore have, um,",
    "start": "1912255",
    "end": "1918090"
  },
  {
    "text": "if your X is, in general so, you know,",
    "start": "1918090",
    "end": "1923475"
  },
  {
    "text": "if you have any examples in d features, right?",
    "start": "1923475",
    "end": "1928860"
  },
  {
    "text": "So some of your, um, uh, features may be kind of correlated.",
    "start": "1928860",
    "end": "1934215"
  },
  {
    "text": "So if you- if you have, um, um, if this is a dataset about pilots and one of the features is, you know,",
    "start": "1934215",
    "end": "1941669"
  },
  {
    "text": "their skill level, and another feature is, you know, the level of enjoyment they report flying helicopters, right?",
    "start": "1941670",
    "end": "1950520"
  },
  {
    "text": "Uh, even though this looks as a d-dimensional space, because, you know,",
    "start": "1950520",
    "end": "1956790"
  },
  {
    "text": "some or many of your features may kind of actually live in a lower-dimensional space,",
    "start": "1956790",
    "end": "1963255"
  },
  {
    "text": "your entire d-dimensional space can probably be represented in a lower-dimensional space,",
    "start": "1963255",
    "end": "1971290"
  },
  {
    "text": "which is k, right? By, uh, by this,",
    "start": "1972320",
    "end": "1977655"
  },
  {
    "text": "I don't mean element-wise similar. I- I jus, uh, you're just thinking of representing",
    "start": "1977655",
    "end": "1983009"
  },
  {
    "text": "this high-demens- high-dimensional data in a low-dimensional space. Yes, question. [inaudible] [OVERLAPPING]",
    "start": "1983009",
    "end": "1994320"
  },
  {
    "text": "Yes. So the question is, uh, why isn't this supervised learning? Well, you know, you can think of one of them, uh,",
    "start": "1994320",
    "end": "2000260"
  },
  {
    "text": "you can take, uh, one of them to be y and, and kind of regress it from the others. You can't do it as a supervised learning problem, but the,",
    "start": "2000260",
    "end": "2008480"
  },
  {
    "text": "the problem is in general quite hard in the sense, uh, you're- you don't know upfront which label-",
    "start": "2008480",
    "end": "2016175"
  },
  {
    "text": "which- which of these features you wanna choose as your y label. You can try it with all of them, you  can try, you know, a- a- a subset of them.",
    "start": "2016175",
    "end": "2022265"
  },
  {
    "text": "It's not clear, you know, how to go about actually, uh, solving this supervised learning problem. What if- if d is 10,000, right?",
    "start": "2022265",
    "end": "2029915"
  },
  {
    "text": "Are you gonna perform 10,000 linear, you know, regressions? You could do that, but, you know, PCA is- is much more efficient.",
    "start": "2029915",
    "end": "2036140"
  },
  {
    "text": "[inaudible].",
    "start": "2036140",
    "end": "2044240"
  },
  {
    "text": "You- you, uh, you could, uh, uh, uh, do something like that. Though this is, uh, PCA,",
    "start": "2044240",
    "end": "2049310"
  },
  {
    "text": "as you will see, is much more straightforward. You don't have to, you know, um, um, pose it as a supervised learning and- and learn it etc.",
    "start": "2049310",
    "end": "2055429"
  },
  {
    "text": "So it's uh, PCA is more straightforward, right? So in- in PCA, our goal is to therefore, uh,",
    "start": "2055430",
    "end": "2062315"
  },
  {
    "text": "find subspaces of our data, uh, s, uh, find a subspace in our input space where the data kind of lives.",
    "start": "2062315",
    "end": "2068120"
  },
  {
    "text": "[NOISE] Okay? So in PCA,",
    "start": "2068120",
    "end": "2073850"
  },
  {
    "start": "2070000",
    "end": "2220000"
  },
  {
    "text": "the first thing we do is to sta- center our data and scale it to have variance 1.",
    "start": "2073850",
    "end": "2084095"
  },
  {
    "text": "What we mean is, the first step in PCA is to [NOISE], you know, standardize your dataset.",
    "start": "2084095",
    "end": "2092060"
  },
  {
    "text": "[NOISE] Okay? Which means we wanna set",
    "start": "2092060",
    "end": "2099660"
  },
  {
    "text": "X_j_i to be X_j_i",
    "start": "2100030",
    "end": "2106925"
  },
  {
    "text": "minus Mu j divided by Sigma j,",
    "start": "2106925",
    "end": "2112790"
  },
  {
    "text": "where Mu j is the mean of the jth column and Sigma j is the- is the standard deviation of the jth column, right?",
    "start": "2112790",
    "end": "2120635"
  },
  {
    "text": "So first thing is independently kind of center each column and scale each column so that the column has variance one, okay?",
    "start": "2120635",
    "end": "2131720"
  },
  {
    "text": "So that's the first thing. Where Mu j is- [NOISE] is one over n,",
    "start": "2131720",
    "end": "2139205"
  },
  {
    "text": "i equals 1-n X_j_i,",
    "start": "2139205",
    "end": "2146540"
  },
  {
    "text": "and Sigma j squared is 1 over n,",
    "start": "2146540",
    "end": "2151770"
  },
  {
    "text": "i equals 1-n X_j_i minus Mu j squared, right?",
    "start": "2153070",
    "end": "2161405"
  },
  {
    "text": "The first thing is we want to standardize our dataset to have mean 0 and standard deviation 1.",
    "start": "2161405",
    "end": "2167974"
  },
  {
    "text": "And the reason we do- we wanna do this is, um, it may so happen that the units in which your- your,",
    "start": "2167975",
    "end": "2177470"
  },
  {
    "text": "uh, data is represented can be very different. So, uh, for example, uh, if you're, uh, representing,",
    "start": "2177470",
    "end": "2183889"
  },
  {
    "text": "for example, the height and weight of a person in a dataset where each row corresponds to a person, the, you know, height may be,",
    "start": "2183889",
    "end": "2190835"
  },
  {
    "text": "you know, you never know, the weight may be represented in, in kilograms, but height may be represented in millimeters, right?",
    "start": "2190835",
    "end": "2198140"
  },
  {
    "text": "So the- the values in the height column will generally be- be much, much bigger than values in the- in the weight column, for example, right?",
    "start": "2198140",
    "end": "2205610"
  },
  {
    "text": "So, um, the first thing you wanna do is to kind of become agnostic to the units in which your data is represented,",
    "start": "2205610",
    "end": "2213665"
  },
  {
    "text": "and for that, we just standardize each of the column independently, [NOISE] right?",
    "start": "2213665",
    "end": "2219200"
  },
  {
    "text": "And once we do that, the way we want to find the- [NOISE] the,",
    "start": "2219200",
    "end": "2225230"
  },
  {
    "text": "um, underlying subspace is like this.",
    "start": "2225230",
    "end": "2231080"
  },
  {
    "text": "[NOISE] So, right? So let's assume this is X_1, Xd, okay?",
    "start": "2231080",
    "end": "2241954"
  },
  {
    "text": "And if your- if- if your data is- [NOISE]",
    "start": "2241955",
    "end": "2254635"
  },
  {
    "text": "So we could try to find [NOISE] a subspace here and project all our points onto the subspace.",
    "start": "2254635",
    "end": "2265405"
  },
  {
    "text": "So this is one possible projection.",
    "start": "2265405",
    "end": "2268850"
  },
  {
    "text": "So each of the, uh, each of the crosses were the x's that lived in the high dimensional space.",
    "start": "2273330",
    "end": "2282320"
  },
  {
    "text": "Right? So this is one way to, uh, uh, this is one possible low dimensional subspace onto which we are",
    "start": "2285240",
    "end": "2292810"
  },
  {
    "text": "projecting all the individual data elements onto the low dimensional subspace. Right? And another possible subspace is x1,",
    "start": "2292810",
    "end": "2306070"
  },
  {
    "text": "xd and trying to draw the same, um, uh, things here.",
    "start": "2307920",
    "end": "2317755"
  },
  {
    "text": "Now, what if instead we chose this to be our subspace? Right? If, uh, that- that's one technical error in what I did,",
    "start": "2317755",
    "end": "2328960"
  },
  {
    "text": "the subspace must span through the origin. So instead let me just redraw the axes so that it,",
    "start": "2328960",
    "end": "2335575"
  },
  {
    "text": "yeah, let's assume this is xd. Right? And it's going through the origin.",
    "start": "2335575",
    "end": "2341710"
  },
  {
    "text": "So instead we could- we could also have this to be the subspace. And if we can project all the points onto our subspace.",
    "start": "2341710",
    "end": "2354099"
  },
  {
    "text": "Right? If you remember, projection means to find the closest point on",
    "start": "2354100",
    "end": "2360190"
  },
  {
    "text": "the subspace to our- the point which you are trying to project. All right? So, and- and the,",
    "start": "2360190",
    "end": "2367960"
  },
  {
    "text": "uh, a natural consequence of that is that the- the, uh, the line connecting",
    "start": "2367960",
    "end": "2373960"
  },
  {
    "text": "the projection to the point will always be perpendicular to the subspace itself. Right? So we could have, you know,",
    "start": "2373960",
    "end": "2381610"
  },
  {
    "text": "um, there are an infinite number of subspaces that we can come up with. Right? And we could either use, you know,",
    "start": "2381610",
    "end": "2388135"
  },
  {
    "text": "this is one possible subspace onto which you are projecting, this is another possible subspace onto which you are projecting,",
    "start": "2388135",
    "end": "2394150"
  },
  {
    "text": "but PCA tells us that the subspace onto which you want to project your data",
    "start": "2394150",
    "end": "2399309"
  },
  {
    "text": "should be the one where the- the variance of the projected points is maximized.",
    "start": "2399310",
    "end": "2405975"
  },
  {
    "text": "What do we mean by that? It means, so if you look at the projected points in this example, you know,",
    "start": "2405975",
    "end": "2413940"
  },
  {
    "text": "there are kind of clustered- [NOISE] clustered within- within, uh, uh, within this space.",
    "start": "2413940",
    "end": "2421255"
  },
  {
    "text": "And if you calculate the sample covariance it would be, you know, some- some, uh, uh,",
    "start": "2421255",
    "end": "2428620"
  },
  {
    "text": "uh, it's- it's have some value. All right? However, with this we see that the projected points are much more spread out.",
    "start": "2428620",
    "end": "2435205"
  },
  {
    "text": "Right? This is much bigger than this. Right? It's the same points, but different choice of subspace,",
    "start": "2435205",
    "end": "2441369"
  },
  {
    "text": "but the- the variance of the projected points, the, uh, projected points in this case are much more spread out compared to the,",
    "start": "2441370",
    "end": "2449005"
  },
  {
    "text": "uh, uh, projected points here. And PCA tells us, what PCA, uh, uh,",
    "start": "2449005",
    "end": "2454615"
  },
  {
    "text": "does is to find this low dimensional subspace, this k-dimensional subspace of",
    "start": "2454615",
    "end": "2461290"
  },
  {
    "text": "this d-dimensional space where the projected points have maximum variance.",
    "start": "2461290",
    "end": "2466495"
  },
  {
    "text": "Right? So it's- it's a variance maximizing procedure where we find a subspace such that the projected point on the subspace have highest variance.",
    "start": "2466495",
    "end": "2476395"
  },
  {
    "text": "Okay? And the way we represent the subspace is through these unit length vectors.",
    "start": "2476395",
    "end": "2486040"
  },
  {
    "text": "[NOISE]",
    "start": "2486040",
    "end": "2502060"
  },
  {
    "text": "So the idea behind, you know, doing something like this is that we want to- we want to",
    "start": "2502060",
    "end": "2507250"
  },
  {
    "text": "retain as much of variance there is in the data to the extent possible, even after projecting it down to a low dimensional space.",
    "start": "2507250",
    "end": "2514480"
  },
  {
    "text": "We don't want to lose the variance, we just want to lose the number of dimensions where the data lives,",
    "start": "2514480",
    "end": "2519850"
  },
  {
    "text": "we want to project onto a low dimensional- a lower dimensional space but we don't want to lose the variance in our data.",
    "start": "2519850",
    "end": "2524980"
  },
  {
    "text": "We want to retain the variance in the data but lose just the dimensions. That's the idea with PCA.",
    "start": "2524980",
    "end": "2530695"
  },
  {
    "text": "Right? So- [NOISE] so",
    "start": "2530695",
    "end": "2541765"
  },
  {
    "text": "suppose we are- we are, um, suppose we represent the low dimensional subspace with unit vectors u.",
    "start": "2541765",
    "end": "2552715"
  },
  {
    "text": "So if- u, you can think of u in R_d to be unit length.",
    "start": "2552715",
    "end": "2563060"
  },
  {
    "text": "Right? So if- if u is- represents, you know, um,",
    "start": "2563430",
    "end": "2568974"
  },
  {
    "text": "a basis of the subspace of unit length onto which we want to project our data,",
    "start": "2568975",
    "end": "2575165"
  },
  {
    "text": "you might remember from one of our audio lectures, um, if u is- is a basis vector,",
    "start": "2575165",
    "end": "2583315"
  },
  {
    "text": "the projection of x onto the space spanned by u,",
    "start": "2583315",
    "end": "2588414"
  },
  {
    "text": "will be the projection matrix of u- [NOISE] projection matrix of u times x.",
    "start": "2588414",
    "end": "2602650"
  },
  {
    "text": "Where x is, you know, the- the point that we want to maximize. And so, um, so that would be the,",
    "start": "2602650",
    "end": "2613420"
  },
  {
    "text": "um, um, that would be the projected point. And this, you know, the projection matrix of u is basically u",
    "start": "2613420",
    "end": "2620890"
  },
  {
    "text": "u transpose over u transpose u of x.",
    "start": "2620890",
    "end": "2627625"
  },
  {
    "text": "That's just the projection matrix of- of, um, projection y.",
    "start": "2627625",
    "end": "2634645"
  },
  {
    "text": "And we see that, um, u transpose u is just 1.",
    "start": "2634645",
    "end": "2641335"
  },
  {
    "text": "And so the projected point will be just x_i transpose u.",
    "start": "2641335",
    "end": "2650755"
  },
  {
    "text": "Right? So this should be x_i- x_i transpose u,",
    "start": "2650755",
    "end": "2656529"
  },
  {
    "text": "this is a scalar, times u. Now, the, uh, what we- what we want to do is to find u such that",
    "start": "2656530",
    "end": "2667360"
  },
  {
    "text": "the- the variance of across these projected points is maximum.",
    "start": "2667360",
    "end": "2673900"
  },
  {
    "text": "Right? So, um, if we have n examples, what we want to maximize is the- for all in examples,",
    "start": "2673900",
    "end": "2681655"
  },
  {
    "text": "the norm of this- this, uh, projected point. So the norm of the projected point is",
    "start": "2681655",
    "end": "2686740"
  },
  {
    "text": "basically the length from here to the projected point. Right. The, uh, if we max- if we find u such",
    "start": "2686740",
    "end": "2695440"
  },
  {
    "text": "that the sum of the squares of all the projected lengths is maximum,",
    "start": "2695440",
    "end": "2701530"
  },
  {
    "text": "then effectively we would have performed PCA. Okay? So what we want to do is, um,",
    "start": "2701530",
    "end": "2709165"
  },
  {
    "text": "find u such that 1 over n,",
    "start": "2709165",
    "end": "2719650"
  },
  {
    "text": "sum i equals 1 to n, the norm of the projection of u times x squared is maximized.",
    "start": "2719650",
    "end": "2732355"
  },
  {
    "text": "Right. And this is the same as 1 over n, i equals 1 to n x- x_i transpose u,",
    "start": "2732355",
    "end": "2750925"
  },
  {
    "text": "so this is just a scalar, times u, norm is maximized.",
    "start": "2750925",
    "end": "2757135"
  },
  {
    "text": "So we want to maximize this- maximize this.",
    "start": "2757135",
    "end": "2763520"
  },
  {
    "text": "All over u is equal to arg max of this. Right? And this, we see that it is some scalar times",
    "start": "2765150",
    "end": "2773275"
  },
  {
    "text": "a unit length vector so the norm of this whole thing is just 1 over n,",
    "start": "2773275",
    "end": "2780085"
  },
  {
    "text": "i equals 1 to n, x_i transpose u squared.",
    "start": "2780085",
    "end": "2792280"
  },
  {
    "text": "Right? So the norm of, uh, a scalar times a unit vector, so the norm of that is just the square of the- the scalar itself.",
    "start": "2792280",
    "end": "2801260"
  },
  {
    "text": "Right? And this, we can now, uh, write this as- so continue here.",
    "start": "2801260",
    "end": "2807045"
  },
  {
    "text": "Right? And that is equal to u transpose 1 over n,",
    "start": "2807045",
    "end": "2816790"
  },
  {
    "text": "i equals 1 to n, x_i- x_i transpose u.",
    "start": "2817670",
    "end": "2827210"
  },
  {
    "text": "Right? And because our x's were center,",
    "start": "2827210",
    "end": "2834805"
  },
  {
    "text": "this thing has mean 0, and therefore this entity over here is the sample covariance matrix.",
    "start": "2834805",
    "end": "2842125"
  },
  {
    "text": "Right? [NOISE]",
    "start": "2842125",
    "end": "2852035"
  },
  {
    "text": "All right? So you might remember this from some optimization class you took.",
    "start": "2852035",
    "end": "2859400"
  },
  {
    "text": "This u equals Arg max",
    "start": "2859400",
    "end": "2864859"
  },
  {
    "text": "U of u transpose some matrix u.",
    "start": "2864860",
    "end": "2871865"
  },
  {
    "text": "In this case, this happens to be 1 over n, I equals 1 to n,",
    "start": "2871865",
    "end": "2878045"
  },
  {
    "text": "xi, xi transpose, right?",
    "start": "2878045",
    "end": "2885875"
  },
  {
    "text": "Does anybody know what the solution of- for u is for this problem? Where u is a vector.",
    "start": "2885875",
    "end": "2893340"
  },
  {
    "text": "So u, in this case, is identifying so- so if- if you solve this,",
    "start": "2893770",
    "end": "2899180"
  },
  {
    "text": "the solution u is basically the eigenvector corresponding to the largest eigenvalue of this matrix.",
    "start": "2899180",
    "end": "2906065"
  },
  {
    "text": "Right? And-and that's, ah, that's- that's, ah that's because arg max of u,",
    "start": "2906065",
    "end": "2918470"
  },
  {
    "text": "of u transpose AU will give you eigenvector of largest eigenvalue of A, right?",
    "start": "2918470",
    "end": "2934080"
  },
  {
    "text": "And this analysis holds where instead of u, if you have um- um,",
    "start": "2938740",
    "end": "2945845"
  },
  {
    "text": "where here- we- we just considered one u, one vector, but if you consider the whole basis of- of,",
    "start": "2945845",
    "end": "2953255"
  },
  {
    "text": "uh, um, basis vectors, then nothing in this argument changes. By performing this maximization,",
    "start": "2953255",
    "end": "2959345"
  },
  {
    "text": "you would have recovered um, where a single u if you had KU vectors,",
    "start": "2959345",
    "end": "2966620"
  },
  {
    "text": "then by performing this maximization, where instead of one vector you have KU vectors,",
    "start": "2966620",
    "end": "2973548"
  },
  {
    "text": "you will have recovered the top K eigenvectors corresponding to the largest eigenvalues. Yes, question?",
    "start": "2973549",
    "end": "2979505"
  },
  {
    "text": "Can you explain from [inaudible] So, how we went from here to here?",
    "start": "2979505",
    "end": "2987005"
  },
  {
    "text": "Yeah. So, ah, [NOISE] this step, I'm gonna write it here.",
    "start": "2987005",
    "end": "3002900"
  },
  {
    "text": "1 over N, I equals 1 to N,",
    "start": "3003870",
    "end": "3011960"
  },
  {
    "text": "xi transpose U squared.",
    "start": "3013290",
    "end": "3020395"
  },
  {
    "text": "Right? And this is the same as 1 over n. So I'm, ah,",
    "start": "3020395",
    "end": "3026140"
  },
  {
    "text": "equals 1 to n, xi transpose U-xi transpose U is just the square.",
    "start": "3026140",
    "end": "3037285"
  },
  {
    "text": "And this, we can write it as one over n, i equals 1 to n,",
    "start": "3037285",
    "end": "3042520"
  },
  {
    "text": "xi transpose U, and this dot product, we just write it the other way and then we get U transpose xi.",
    "start": "3042520",
    "end": "3051685"
  },
  {
    "text": "Rather I should have done it the other way. So instead, if you had swapped this, you will get i equals 1 to n,",
    "start": "3051685",
    "end": "3058570"
  },
  {
    "text": "u transpose xi, xi transpose U.",
    "start": "3058570",
    "end": "3064720"
  },
  {
    "text": "Right? And then you can take the U common out of the summation. And that gives us U transpose,",
    "start": "3064720",
    "end": "3071665"
  },
  {
    "text": "i, 1 over n x1. I equals 1 to n, xi transpose U.",
    "start": "3071665",
    "end": "3084230"
  },
  {
    "text": "So basically, ah, by- by following this variance maximizing argument,",
    "start": "3085620",
    "end": "3094915"
  },
  {
    "text": "where we want to maximize the sample variance or the- the, ah,",
    "start": "3094915",
    "end": "3100120"
  },
  {
    "text": "projected points onto a subspace must be as far as possible from the origin, right?",
    "start": "3100120",
    "end": "3105715"
  },
  {
    "text": "By following this variance maximizing principle, if we find the basis U,",
    "start": "3105715",
    "end": "3111835"
  },
  {
    "text": "such that the- the projected, ah, variance is maximized, we will see that we will end up with an eigenvalue problem where we want to, ah,",
    "start": "3111835",
    "end": "3121030"
  },
  {
    "text": "calculate the eigenvectors of the sample covariance val- ah, ah, matrix of x's.",
    "start": "3121030",
    "end": "3128350"
  },
  {
    "text": "Right? So, if x is our data- data matrix.",
    "start": "3128350",
    "end": "3134785"
  },
  {
    "text": "PCA tells us that first calculate x transpose x, right?",
    "start": "3134785",
    "end": "3140680"
  },
  {
    "text": "x transpose x is basically this written in matrix notation.",
    "start": "3140680",
    "end": "3145795"
  },
  {
    "text": "Take x transpose x and calculate the eigenvectors and eigenvalues,",
    "start": "3145795",
    "end": "3152470"
  },
  {
    "text": "or calculate the spectrum of- of the ah, x transpose x. And depending on the number of subspaces that we want to retain.",
    "start": "3152470",
    "end": "3164540"
  },
  {
    "text": "So x transpose x will give us a collection of Lambda i, ui.",
    "start": "3165240",
    "end": "3172945"
  },
  {
    "text": "So say Lambda 1 u_1, Lambda 2 u_2 until Lambda d UD, right?",
    "start": "3172945",
    "end": "3184285"
  },
  {
    "text": "These are the D eigenvectors, ah, eigenvalues and u's are the D, and u's are the D, ah, eigenvectors.",
    "start": "3184285",
    "end": "3193945"
  },
  {
    "text": "And now, what we wanna do is- is, ah, if, depending on the amount of variance that we want to retain,",
    "start": "3193945",
    "end": "3204505"
  },
  {
    "text": "a common practice is to then choose as many lambdas, are as many pairs of- of ah, eigenvectors,",
    "start": "3204505",
    "end": "3213385"
  },
  {
    "text": "such that the- a more precise way to say it is,",
    "start": "3213385",
    "end": "3219535"
  },
  {
    "text": "find k such that sum",
    "start": "3219535",
    "end": "3226135"
  },
  {
    "text": "equals 1-k. Lambda k over sum equals 1-d.",
    "start": "3226135",
    "end": "3233510"
  },
  {
    "text": "Sorry, Lambda i- Lambda i.",
    "start": "3233790",
    "end": "3239920"
  },
  {
    "text": "This is equal to, you know, some- some kind of a percent, say 95% or something, right?",
    "start": "3239920",
    "end": "3247495"
  },
  {
    "text": "And it's- it's common to choose k in PCA by first performing the eigendecomposition of x transpose x, right?",
    "start": "3247495",
    "end": "3256089"
  },
  {
    "text": "And then by-sort eig- eigenvalues in- in some-some kind of ah,",
    "start": "3256090",
    "end": "3261640"
  },
  {
    "text": "ah, decreasing order- in a decreasing order. And then find k such that the sum of",
    "start": "3261640",
    "end": "3268630"
  },
  {
    "text": "the first k- top k eigenvalues divided by the sum of all the eigenvalues is some,",
    "start": "3268630",
    "end": "3275305"
  },
  {
    "text": "you know, uh, per- percentage of your choice. And by choosing k to be, you know, 95%,",
    "start": "3275305",
    "end": "3281290"
  },
  {
    "text": "you can effectively say that you've retained 95% of the variance in your data, even though you went down from d dimensions down to k dimensions. Yes,  question?",
    "start": "3281290",
    "end": "3291430"
  },
  {
    "text": "[inaudible] Yeah.",
    "start": "3291430",
    "end": "3298870"
  },
  {
    "text": "Yeah. So- so, uh, um, uh, uh, uh, I'm gonna come to that, uh, right after this. All right.",
    "start": "3298870",
    "end": "3305490"
  },
  {
    "text": "So, um, any questions on this? The way we go about doing it is to take x transpose x,",
    "start": "3305490",
    "end": "3312505"
  },
  {
    "text": "take the Eigendecomposition of X transpose X, obtain the- the- the, uh, eigenvectors of X transpose X,",
    "start": "3312505",
    "end": "3320395"
  },
  {
    "text": "and then choose k such that the sum of the top k eigenvalues divided by the sum of",
    "start": "3320395",
    "end": "3327609"
  },
  {
    "text": "all the eigenvalues is the fraction of variance that you want to retain. So if you were to plot, um, um, um,",
    "start": "3327610",
    "end": "3335810"
  },
  {
    "text": "let's say this is k and this",
    "start": "3336780",
    "end": "3342175"
  },
  {
    "text": "is fraction of variance,",
    "start": "3342175",
    "end": "3348070"
  },
  {
    "text": "you will generally see that, um, if this is d, then you would generally see that this goes down something like this.",
    "start": "3348070",
    "end": "3356635"
  },
  {
    "text": "Where- or rather this is d minus k which means, um,",
    "start": "3356635",
    "end": "3363655"
  },
  {
    "text": "if- if you were to prune out all the d eigenvectors, then you- you will be having 0 variance remaining.",
    "start": "3363655",
    "end": "3372119"
  },
  {
    "text": "But if you prune 0, you will, uh, you will be having all the variants remaining. And generally, you know,",
    "start": "3372120",
    "end": "3378135"
  },
  {
    "text": "this remains flat for quite a long time, which means even if you prune a big fraction of your feature space,",
    "start": "3378135",
    "end": "3385725"
  },
  {
    "text": "you will still be having, you know, like 95% of your variance Yes,  question.",
    "start": "3385725",
    "end": "3390880"
  },
  {
    "text": "Professor. So u is a matrix of Eigenvectors you want to [inaudible].",
    "start": "3390880",
    "end": "3396579"
  },
  {
    "text": "Yes. So u, in this case, we showed it- we showed the- the, uh, showed the intuition using a single- a single Eigenvector.",
    "start": "3396580",
    "end": "3404725"
  },
  {
    "text": "But, uh, effectively, we- we just calculated the largest Eigen- you know,",
    "start": "3404725",
    "end": "3409870"
  },
  {
    "text": "showed that the largest Eigen- Eigenvector is- is the one that minimizes- maximizes the variance if you projected it into one-dimension.",
    "start": "3409870",
    "end": "3416994"
  },
  {
    "text": "Um, but if you want u to be larger then or rather if you want k to be larger- this was k equals 1,",
    "start": "3416995",
    "end": "3425185"
  },
  {
    "text": "if you want k to be larger, then just do the, uh, the- the full Eigendecomposition rather than just this optimization,",
    "start": "3425185",
    "end": "3433525"
  },
  {
    "text": "because this just gives you the top Eigenvector. So instead do the full, um, Eigen decomposition and hold on to the, you know,",
    "start": "3433525",
    "end": "3440275"
  },
  {
    "text": "top k, um, Eigenvalue Eigenvector pairs. [NOISE] Okay?",
    "start": "3440275",
    "end": "3447190"
  },
  {
    "text": "Any questions in, um, in this so far? So for those of you who may be familiar with singular value decomposition,",
    "start": "3447190",
    "end": "3455575"
  },
  {
    "text": "what we performed here is also called singular value decomposition so SVD.",
    "start": "3455575",
    "end": "3461540"
  },
  {
    "text": "Let me write it here. Okay. Okay? Maybe we",
    "start": "3464340",
    "end": "3472690"
  },
  {
    "text": "covered this in one of the earlier lectures but if you have a matrix X, where X is square and symmetric,",
    "start": "3472690",
    "end": "3486530"
  },
  {
    "text": "then X has orthogonal Eigen basis,",
    "start": "3487560",
    "end": "3494840"
  },
  {
    "text": "Eigenvectors and real",
    "start": "3498420",
    "end": "3504579"
  },
  {
    "text": "Eigenvalues, right?",
    "start": "3504580",
    "end": "3510010"
  },
  {
    "text": "And if X is positive semi-definite,",
    "start": "3510010",
    "end": "3515650"
  },
  {
    "text": "then- then it is obviously square and symmetric.",
    "start": "3515650",
    "end": "3520750"
  },
  {
    "text": "And if it's also PSD, then Eigenvalues are positive- all the Eigenvalues are positive, right?",
    "start": "3520750",
    "end": "3531325"
  },
  {
    "text": "Now, in this case, um, we are using X transpose X to be,",
    "start": "3531325",
    "end": "3539350"
  },
  {
    "text": "uh, to be the matrix on which we are- we are performing, uh, uh, Eigendecomposition. So it is X transpose X is guaranteed to be square,",
    "start": "3539350",
    "end": "3549670"
  },
  {
    "text": "symmetric, and positive semi-definite, right? Now, performing Eigendecomposition on",
    "start": "3549670",
    "end": "3562255"
  },
  {
    "text": "X transpose X is the same as performing singular value decomposition on X, right?",
    "start": "3562255",
    "end": "3573130"
  },
  {
    "text": "The way you'd go about doing singular value decomposition on any matrix X, in this case,",
    "start": "3573130",
    "end": "3578244"
  },
  {
    "text": "for- to do singular value decomposition, your matrix need not be PSD, it need not be symmetric,",
    "start": "3578245",
    "end": "3583390"
  },
  {
    "text": "it need not even be square. You're given any matrix whatsoever, the way you perform singular value decomposition on",
    "start": "3583390",
    "end": "3590654"
  },
  {
    "text": "any matrix X is to take X transpose X, take the Eigenvectors of X transpose X,",
    "start": "3590655",
    "end": "3596685"
  },
  {
    "text": "and Eigenvectors of X transpose X, um, um, and the Eigenvalues of X transpose X will,",
    "start": "3596685",
    "end": "3605725"
  },
  {
    "text": "or the square root of the Eigenvalues will give you the singular values of X. So, um, this is the same as performing singular value decomposition on- on,",
    "start": "3605725",
    "end": "3616420"
  },
  {
    "text": "um, on the X matrix, which is, you know, another way to think of the same thing is to construct",
    "start": "3616420",
    "end": "3622870"
  },
  {
    "text": "X transpose X and just do Eigen- Eigen decomposition on X transpose X.",
    "start": "3622870",
    "end": "3627830"
  },
  {
    "text": "Any questions on- on- on, uh, PCA? In homework we uh,",
    "start": "3627930",
    "end": "3633970"
  },
  {
    "text": "you will see- so here we- we saw the variance maximizing argument where if,",
    "start": "3633970",
    "end": "3642950"
  },
  {
    "text": "where, uh, you know, we want to find a hyperplane,",
    "start": "3646710",
    "end": "3651760"
  },
  {
    "text": "where if you take the projections,",
    "start": "3651760",
    "end": "3658885"
  },
  {
    "text": "then the variance of the projections- so this distance, you know, this distance, this distance, you know.",
    "start": "3658885",
    "end": "3664390"
  },
  {
    "text": "This is basically the, you know, the- the- the spread of the projected points is maximized.",
    "start": "3664390",
    "end": "3671784"
  },
  {
    "text": "In homework, you shou- you will show that this method is equivalent to, um,",
    "start": "3671784",
    "end": "3678369"
  },
  {
    "text": "or- or- or you will see another motivation where if you were- if you- if you rephrase the problem as finding",
    "start": "3678370",
    "end": "3685629"
  },
  {
    "text": "a subspace where the- the projected points",
    "start": "3685629",
    "end": "3692200"
  },
  {
    "text": "are as close as possible or as near as possible to the original points,",
    "start": "3692200",
    "end": "3697270"
  },
  {
    "text": "that is to minimizing this- this distance. All right? So maximizing the- the projected variance and",
    "start": "3697270",
    "end": "3705760"
  },
  {
    "text": "minimizing the- the residuals are equivalent. And in- in -in your homework, you will show that if you phrase it as a problem of minimizing the-the- the, um, um,",
    "start": "3705760",
    "end": "3715840"
  },
  {
    "text": "residuals or minimizing or- or improving the approximation of the projected points or your original points,",
    "start": "3715840",
    "end": "3721600"
  },
  {
    "text": "you will recover the same- same, um, solution as maximizing the variance. There are two different- two different ways to think of- of PCA.",
    "start": "3721600",
    "end": "3728890"
  },
  {
    "text": "Find- find a subspace such that the projections are as close as possible to the original points.",
    "start": "3728890",
    "end": "3734440"
  },
  {
    "text": "That's, you know, interpretation two that you'll do in the homework. The interpretation one that we saw in the lecture is find",
    "start": "3734440",
    "end": "3740170"
  },
  {
    "text": "a subspace such that the projected points have maximum variance.",
    "start": "3740170",
    "end": "3745040"
  },
  {
    "text": "Any questions on PCA? Okay, great.",
    "start": "3746700",
    "end": "3752770"
  },
  {
    "text": "So let's move on to ICA, so it looks like we're making good progress, so we might be able to finish ICA today.",
    "start": "3752770",
    "end": "3759700"
  },
  {
    "text": "[NOISE] So in ICA,",
    "start": "3759700",
    "end": "3765470"
  },
  {
    "text": "it's, uh, uh, somewhat different problem from- from,",
    "start": "3765540",
    "end": "3775015"
  },
  {
    "text": "um- um, the problems we've seen so far. And to- perhaps to understand kind of the- the- the, um,",
    "start": "3775015",
    "end": "3783625"
  },
  {
    "text": "the difference of ICA, it might be useful to kind of- kind of,",
    "start": "3783625",
    "end": "3789295"
  },
  {
    "text": "uh, uh, place together the algorithms we've seen so far. So- so far we have seen- let's see,",
    "start": "3789295",
    "end": "3794710"
  },
  {
    "text": "I can erase this one. [NOISE]",
    "start": "3794710",
    "end": "3807160"
  },
  {
    "text": "So so far in unsupervised learning, we've seen four algorithms, right?",
    "start": "3807160",
    "end": "3813460"
  },
  {
    "text": "So the first one we saw was k-means. We started off with k-means, [NOISE] right?",
    "start": "3813460",
    "end": "3824185"
  },
  {
    "text": "And then after k-means, we studied Gaussian mixture models.",
    "start": "3824185",
    "end": "3829340"
  },
  {
    "text": "Okay. And after Gaussian mixture models, we studied, uh, factor analysis, right?",
    "start": "3829770",
    "end": "3841045"
  },
  {
    "text": "And- and today, ah, just now we study PCA, right?",
    "start": "3841045",
    "end": "3846805"
  },
  {
    "text": "So we've studied four algorithms so far. And you can kind of classify them in two different ways.",
    "start": "3846805",
    "end": "3852520"
  },
  {
    "text": "Okay. So these you can call clustering algorithms.",
    "start": "3852520",
    "end": "3858980"
  },
  {
    "text": "And these you can call them, you know, subspace algorithms.",
    "start": "3859380",
    "end": "3866330"
  },
  {
    "text": "These were probabilistic, and these were non-probabilistic.",
    "start": "3866400",
    "end": "3875680"
  },
  {
    "text": "[NOISE] Okay.",
    "start": "3875680",
    "end": "3882145"
  },
  {
    "text": "So the probabilistic algorithms, we solved them using EM, expectation-maximization.",
    "start": "3882145",
    "end": "3891030"
  },
  {
    "text": "Whereas the non-probabilistic, uh, uh, problems, uh, in the non-probabilistic approach,",
    "start": "3891030",
    "end": "3896369"
  },
  {
    "text": "there was no- there was no, uh, um- um, we didn't have to use EM, right? And loosely speaking, um, again,",
    "start": "3896370",
    "end": "3903744"
  },
  {
    "text": "this is a- a very loose, uh, um- um, analogy. You can think of clustering problems to be",
    "start": "3903745",
    "end": "3911950"
  },
  {
    "text": "the counterpart of classification problems from supervised learning. [NOISE] Right?",
    "start": "3911950",
    "end": "3919510"
  },
  {
    "text": "You can think of them as- as a, um, a classification problem if you have not- where you're not given the- the, uh, the labels.",
    "start": "3919510",
    "end": "3926080"
  },
  {
    "text": "And you can think of these subspace finding problems as the- the counterpart of regression problems on supervised learning.",
    "start": "3926080",
    "end": "3932260"
  },
  {
    "text": "[NOISE] And they, you know,",
    "start": "3932260",
    "end": "3942160"
  },
  {
    "text": "uh, both PCA and factor analysis. In- in both of these,",
    "start": "3942160",
    "end": "3948205"
  },
  {
    "text": "we -we recovered a low dimensional subspace, wherein factor analysis z's were the subspace.",
    "start": "3948205",
    "end": "3956560"
  },
  {
    "text": "z was the subspace. [NOISE] And in PCA,",
    "start": "3956560",
    "end": "3964945"
  },
  {
    "text": "the way we recovered the subspace is to construct the- the, um, the matrix u,",
    "start": "3964945",
    "end": "3972460"
  },
  {
    "text": "where u is the, um- um,",
    "start": "3972460",
    "end": "3978790"
  },
  {
    "text": "there are k different columns,",
    "start": "3978790",
    "end": "3984310"
  },
  {
    "text": "each of dimension d. And these are the- the, uh,",
    "start": "3984310",
    "end": "3989530"
  },
  {
    "text": "the top k eigenvectors of x transpose x, right?",
    "start": "3989530",
    "end": "3995875"
  },
  {
    "text": "And then using this U matrix, which is, uh, we can multiply UX transpose,",
    "start": "3995875",
    "end": "4006225"
  },
  {
    "text": "so UX transpose will give us, um,",
    "start": "4006225",
    "end": "4011700"
  },
  {
    "text": "UX transpose will give us k such- or it- it will be easier to just do it as U transpose.",
    "start": "4011700",
    "end": "4025484"
  },
  {
    "text": "XU transpose will give us the- the, uh, projected points of X onto the lower-dimensional space, right?",
    "start": "4025485",
    "end": "4032849"
  },
  {
    "text": "You take the eigenbasis and take each example and multiply it, um, uh, and just, uh, multiply it on to U.",
    "start": "4032850",
    "end": "4038955"
  },
  {
    "text": "So, that's like projecting it on to U because they have unit, um- um, unit length and, uh,",
    "start": "4038955",
    "end": "4044355"
  },
  {
    "text": "XU will give us the, uh- uh, subspace. Now, you might- you might ask what's,",
    "start": "4044355",
    "end": "4051990"
  },
  {
    "text": "you know, what- what's- what else is different between factor analysis and PCA. The other difference, oh, in factor analysis,",
    "start": "4051990",
    "end": "4058170"
  },
  {
    "text": "we assume d was much bigger than n. [NOISE] But here,",
    "start": "4058170",
    "end": "4064319"
  },
  {
    "text": "we made no such assumption. Here, in fact, we assume that n will be bigger than d. If we don't assume, um,",
    "start": "4064320",
    "end": "4071010"
  },
  {
    "text": "n is bigger than- than d, then again, you know, this would- it might work.",
    "start": "4071010",
    "end": "4077220"
  },
  {
    "text": "Uh, if this would be the- this would be singular, but yes. So, in fact, actually in- in PCA you probably don't- don't- don't need to make,",
    "start": "4077220",
    "end": "4086415"
  },
  {
    "text": "um- um- um, n to b because nd, but- but it's commonly the case that n is indeed bigger than d. And the other, uh,",
    "start": "4086415",
    "end": "4096540"
  },
  {
    "text": "difference between the two was in- in factor analysis, we recovered the matrix L. All right.",
    "start": "4096540",
    "end": "4104654"
  },
  {
    "text": "And L, the- the parameter that we recovered, took us from z to x- took us from z to x.",
    "start": "4104655",
    "end": "4114060"
  },
  {
    "text": "[NOISE] Whereas in- in PCA, we recover U. U takes us from x to z,",
    "start": "4114060",
    "end": "4121770"
  },
  {
    "text": "if you call z as the, uh, projected, uh, projected points. All right. So- so the- the,",
    "start": "4121770",
    "end": "4127890"
  },
  {
    "text": "uh, the recovered parameters, kind of, go in opposite direction between, uh- uh, factor analysis and- and PCA.",
    "start": "4127890",
    "end": "4135914"
  },
  {
    "text": "The other difference between PCA and factor analysis was in PCA, you can first perform PCA,",
    "start": "4135915",
    "end": "4143505"
  },
  {
    "text": "get the set of all, uh, eigenvectors and eigenvalues, and then decide after the fact,",
    "start": "4143505",
    "end": "4150674"
  },
  {
    "text": "what is the appropriate care for your problem. You can decide k after you perform PCA.",
    "start": "4150675",
    "end": "4156900"
  },
  {
    "text": "Whereas in EM, you need to decide k first and then run EM. If you're not satisfied with it,",
    "start": "4156900",
    "end": "4162900"
  },
  {
    "text": "you need to change, uh- eh, k to a different value and rerun, uh- uh, factor analysis all the way again, right?",
    "start": "4162900",
    "end": "4168990"
  },
  {
    "text": "Whereas in PCA, you first perform Eigen, the, uh, Eigen decomposition, and then you look at what k will give you the best,",
    "start": "4168990",
    "end": "4176835"
  },
  {
    "text": "uh- uh, variance retaining factor. Yes, question. [inaudible] to find k and then go [inaudible]",
    "start": "4176835",
    "end": "4183599"
  },
  {
    "text": "Uh, you- so- so the question is,",
    "start": "4183600",
    "end": "4189075"
  },
  {
    "text": "uh, is it reasonable to first do PCA to find k and then go to factor analysis.",
    "start": "4189075",
    "end": "4194460"
  },
  {
    "text": "But then what's the point of going to factor analysis because you would have, you know, recovered a lower-dimensional subspace anyways.",
    "start": "4194460",
    "end": "4200655"
  },
  {
    "text": "Okay. So these are- these are, uh, um, some- some, um, some ways in which we can classify what we've covered so far.",
    "start": "4200655",
    "end": "4210120"
  },
  {
    "text": "And now we're going to start ICA, independent component analysis. All right.",
    "start": "4210120",
    "end": "4215130"
  },
  {
    "text": "And inde- independent component analysis is- is somewhat different in nature from what we've seen so far.",
    "start": "4215130",
    "end": "4223570"
  },
  {
    "text": "[inaudible] question. Yes. Question?",
    "start": "4229750",
    "end": "4233369"
  },
  {
    "text": "Is [inaudible] of doing factor analysis [inaudible]? So, ah, is- is there- is there a benefit of doing factor analysis?",
    "start": "4236290",
    "end": "4243679"
  },
  {
    "text": "Um, [NOISE] um, it- it, ah, so happens that,",
    "start": "4243679",
    "end": "4248960"
  },
  {
    "text": "ah- so factor analysis of- last say of- there are benefits to doing factor analysis.",
    "start": "4248960",
    "end": "4259114"
  },
  {
    "text": "For example, when d is- is, um, much larger than- than,",
    "start": "4259115",
    "end": "4264725"
  },
  {
    "text": "um- so thi- this would also- In PCA [inaudible]",
    "start": "4264725",
    "end": "4272570"
  },
  {
    "text": "Yeah, PC- PCA- PCA, ah, ah, it work, but the problem with- with, ah,",
    "start": "4272570",
    "end": "4278150"
  },
  {
    "text": "so- so- so- so- so here's the thing. In PCA, we're not explicitly modeling the noise in the data, right?",
    "start": "4278150",
    "end": "4285889"
  },
  {
    "text": "So there was no probabilistic assumptions being- being made. Whereas with- with a factor analysis, even though you choose,",
    "start": "4285890",
    "end": "4292130"
  },
  {
    "text": "you know, um, k to be some value, it- it generally works, ah, pretty well because it will end up adjusting your,",
    "start": "4292130",
    "end": "4300379"
  },
  {
    "text": "ah, the design matrix accordingly if- if- if, ah, k is different. So it's quite, um, um,",
    "start": "4300379",
    "end": "4307955"
  },
  {
    "text": "you- you can think of them as two different kind of equivalent, um, ah, approaches, you know.",
    "start": "4307955",
    "end": "4313000"
  },
  {
    "text": "The differences are what we discussed, um, you know. In- in- in some cases, it makes sense to use factor analysis,",
    "start": "4313000",
    "end": "4319270"
  },
  {
    "text": "in some cases, it makes sense to use PCA. But PCA happens to be, you know, more commonly used because it's easy to implement.",
    "start": "4319270",
    "end": "4325430"
  },
  {
    "text": "You- you just perform, um, um, eigendecomposition and- and you get it. Whereas a factor analysis the,",
    "start": "4325430",
    "end": "4331114"
  },
  {
    "text": "you know- it- it also provided us a good- good way, good example to apply EM.",
    "start": "4331115",
    "end": "4336515"
  },
  {
    "text": "So when we applied EM in one case, z's were discrete, but here we saw z's were continuous, right?",
    "start": "4336515",
    "end": "4342290"
  },
  {
    "text": "So it was, you know, um, as- as- as an example to understand EM better,",
    "start": "4342290",
    "end": "4349235"
  },
  {
    "text": "factor analysis is- is- is, um, helpful, like, you know, from an educational purpose. Ah, but PCA is probably used much more commonly in",
    "start": "4349235",
    "end": "4356990"
  },
  {
    "text": "practice than factor analysis. All right. Um, another, uh, thing about PCA that we kind of,",
    "start": "4356990",
    "end": "4367324"
  },
  {
    "text": "ah, skipped over was, um, we just said do eigendecomposition on x transpose x,",
    "start": "4367325",
    "end": "4373475"
  },
  {
    "text": "but sometimes x transpose x can be, you know, really huge in terms of, you know, the number of dimensions.",
    "start": "4373475",
    "end": "4379745"
  },
  {
    "text": "And, you know, ah, if you were to use NumPy or MATLAB and, you know, do,",
    "start": "4379745",
    "end": "4385055"
  },
  {
    "text": "you know, eigendecompose on a large matrix, then, you know, that may- that may just fail.",
    "start": "4385055",
    "end": "4390500"
  },
  {
    "text": "You may run out of memory or something. So in fact, when you- when you run PCA in practice, [NOISE] um,",
    "start": "4390500",
    "end": "4396230"
  },
  {
    "text": "a common- a common technique to perform eigendecomposition is to use something called power iteration [NOISE], right?",
    "start": "4396230",
    "end": "4407180"
  },
  {
    "text": "So power iteration is- is a commonly- uh, is one of the commonly used, ah, techniques to perform eigendecomposition on large matrices.",
    "start": "4407180",
    "end": "4415325"
  },
  {
    "text": "Um, so, ah, again, this is- this is, um- there's- there's not gonna be something you're gonna be- you're gonna be tested on,",
    "start": "4415325",
    "end": "4421925"
  },
  {
    "text": "but the idea of power iteration is to start with some u^0 and randomly initialize u^0,",
    "start": "4421925",
    "end": "4430025"
  },
  {
    "text": "anything that's non-zero, right? And from u^0, um, [NOISE] calculate x transpose x times u^0 equal, right?",
    "start": "4430025",
    "end": "4441695"
  },
  {
    "text": "So u^1 will be x transpose x times u^0",
    "start": "4441695",
    "end": "4449165"
  },
  {
    "text": "divided by the norm of x- x transpose x- x transpose x times, ah, u^0.",
    "start": "4449165",
    "end": "4458225"
  },
  {
    "text": "So basically, take some randomly initialized matrix- ah, randomly initialized vector, multiply it by",
    "start": "4458225",
    "end": "4464150"
  },
  {
    "text": "the matrix whose eigenvector you're interested to calculate, and you get, ah, u^1, then multiply, you know,",
    "start": "4464150",
    "end": "4471230"
  },
  {
    "text": "x transpo- x transpose x times u^1, it will give you u^2, right?",
    "start": "4471230",
    "end": "4478639"
  },
  {
    "text": "And if you keep multiplying, um, the vector over and over again by the same matrix,",
    "start": "4478640",
    "end": "4485480"
  },
  {
    "text": "and after every multiplication, if you re-scale it to have unit length [NOISE] ,",
    "start": "4485480",
    "end": "4492980"
  },
  {
    "text": "and if you repeat this over and over, you will end up- uh you will re- you will see that- you will- the- the, ah, the vector that you'll end up in,",
    "start": "4492980",
    "end": "4500074"
  },
  {
    "text": "the vector that you converge will be the largest eigenvector, right? The- the intuition there is, um,",
    "start": "4500075",
    "end": "4505789"
  },
  {
    "text": "um, suppose, um, um, you know, this is x_1 and x_d.",
    "start": "4505790",
    "end": "4513585"
  },
  {
    "text": "Now, if you remember, if you take a unit sphere in your input space and multiply it by a positive definite matrix,",
    "start": "4513585",
    "end": "4522985"
  },
  {
    "text": "then the outputs will be on- will be on some kind of an ellipse, right?",
    "start": "4522985",
    "end": "4529175"
  },
  {
    "text": "So if you were to, uh, take this as, uh, as the- as the input, and, you know,",
    "start": "4529175",
    "end": "4535445"
  },
  {
    "text": "if this is u-naught, multiply it through- through, ah, x transpose x,",
    "start": "4535445",
    "end": "4541700"
  },
  {
    "text": "then you'll- you may get this to be the point that got mapped to, so this would be,",
    "start": "4541700",
    "end": "4548060"
  },
  {
    "text": "ah, x transpose x times u-naught,",
    "start": "4548060",
    "end": "4553429"
  },
  {
    "text": "and then re-scale it back to unit length multiplied through x and you will get this,",
    "start": "4553429",
    "end": "4560060"
  },
  {
    "text": "and then re-scale it back, multiply it through x and you'll get this. And eventually, you will see that you, you know,",
    "start": "4560060",
    "end": "4565790"
  },
  {
    "text": "this will always converge to the principal axis of the ellipse, which is the eigenvector, right?",
    "start": "4565790",
    "end": "4571520"
  },
  {
    "text": "So multiplying the same matrix, uh, the- the matrix over and over, and then re-scaling the- the multiplied,",
    "start": "4571520",
    "end": "4577910"
  },
  {
    "text": "ah, ah, vector back to unit length, if you repeat that over and over, um, no matter where you start,",
    "start": "4577910",
    "end": "4584300"
  },
  {
    "text": "you will either, you know, converge along this direction, or if you start here,",
    "start": "4584300",
    "end": "4589655"
  },
  {
    "text": "you will end up converging along this direction, but that's gonna be your principal, ah, ah, eigenvector, right?",
    "start": "4589655",
    "end": "4596120"
  },
  {
    "text": "And then one, you know, you can then take, you know, x transpose x, subtract from it uu transpose,",
    "start": "4596120",
    "end": "4602630"
  },
  {
    "text": "you will get, um, um, ah, a matrix of one lower rank and then perform,",
    "start": "4602630",
    "end": "4607969"
  },
  {
    "text": "um, the same, um, um, ah, power iteration on this matrix and you'll recover the next largest eigenvector and so on.",
    "start": "4607969",
    "end": "4615755"
  },
  {
    "text": "So in practice, usually, you do something like this, ah, when the number of dimension is large. So this is called power iteration.",
    "start": "4615755",
    "end": "4621965"
  },
  {
    "text": "This is just a- a good detail to know if you come across the word power iteration. Basically, what's happening is, you know,",
    "start": "4621965",
    "end": "4628145"
  },
  {
    "text": "take a- take a vector and repeatedly keep multiplying it by the same matrix and re-scaling the- the,",
    "start": "4628145",
    "end": "4633560"
  },
  {
    "text": "ah, length back to- length back to 1. Anyways, ICA.",
    "start": "4633560",
    "end": "4641000"
  },
  {
    "text": "[NOISE] In ICA, um, the- the- in ICA,",
    "start": "4641000",
    "end": "4649880"
  },
  {
    "text": "the- the motivation is probably best seen through this, um, cocktail party problem, right?",
    "start": "4649880",
    "end": "4656989"
  },
  {
    "text": "In the cocktail party problem, um, assume there are d speakers,",
    "start": "4656990",
    "end": "4664955"
  },
  {
    "text": "you know, people who are speaking, and there are d microphones, right?",
    "start": "4664955",
    "end": "4674105"
  },
  {
    "text": "So there are d speakers and d microphones, and the d microphones are placed in different places of- of- of- of different lengths,",
    "start": "4674105",
    "end": "4682070"
  },
  {
    "text": "and they are recording the d speakers simultaneously, right? And we will call",
    "start": "4682070",
    "end": "4690320"
  },
  {
    "text": "s in R^d to be,",
    "start": "4690320",
    "end": "4699290"
  },
  {
    "text": "um, you know, think of s in R^d to be one, ah,",
    "start": "4699290",
    "end": "4704570"
  },
  {
    "text": "instantaneous recording of the- of the, ah, ah, speeches that are happening,",
    "start": "4704570",
    "end": "4710240"
  },
  {
    "text": "um, at the sources, right? And we also assume that the- the recordings in the d microphones,",
    "start": "4710240",
    "end": "4721189"
  },
  {
    "text": "we will call it x. It's also R^d. And we assume that x is equal to A times s,",
    "start": "4721189",
    "end": "4732364"
  },
  {
    "text": "where A is commonly called as the mixing matrix. [NOISE]",
    "start": "4732365",
    "end": "4741585"
  },
  {
    "text": "Right? So S^i_j is",
    "start": "4741585",
    "end": "4748140"
  },
  {
    "text": "the- is the thing that the ith speaker is pronouncing at the jth time.",
    "start": "4748140",
    "end": "4755775"
  },
  {
    "text": "Right? And similarly, x^i_ j is",
    "start": "4755775",
    "end": "4763590"
  },
  {
    "text": "the recording in the- in the jth microphone at ith time.",
    "start": "4763590",
    "end": "4771420"
  },
  {
    "text": "Did I say the other way? So j is the speaker identity.",
    "start": "4771420",
    "end": "4777429"
  },
  {
    "text": "Maybe I told you the other way first time and i is the time.",
    "start": "4779450",
    "end": "4784810"
  },
  {
    "text": "And similarly, j here is the microphone identity,",
    "start": "4784850",
    "end": "4790390"
  },
  {
    "text": "and i is the time, right?",
    "start": "4791360",
    "end": "4797085"
  },
  {
    "text": "So what's happening is we have d",
    "start": "4797085",
    "end": "4802770"
  },
  {
    "text": "different speakers saying whatever they are saying,",
    "start": "4802770",
    "end": "4811720"
  },
  {
    "text": "and b different microphones that's recording them.",
    "start": "4824540",
    "end": "4830250"
  },
  {
    "text": "So we have speaker one over here, speaker two over here, right?",
    "start": "4830250",
    "end": "4835710"
  },
  {
    "text": "Let's- let's just start with two, right? And we have microphone one over here, right?",
    "start": "4835710",
    "end": "4844620"
  },
  {
    "text": "And microphone two over here. That's recording what they're seeing.",
    "start": "4844620",
    "end": "4849930"
  },
  {
    "text": "So this is X_1, X_2, S_1, S_2.",
    "start": "4849930",
    "end": "4858090"
  },
  {
    "text": "Now these are kind of placed at you know arbitrary distances from the speakers.",
    "start": "4858090",
    "end": "4863385"
  },
  {
    "text": "You assume there is no lag in- in recording and let's say,",
    "start": "4863385",
    "end": "4869130"
  },
  {
    "text": "you know, speaker 1. Let's assume this is the waveform of what the speaker 1 is speaking,",
    "start": "4869130",
    "end": "4879344"
  },
  {
    "text": "right? And speaker 2.",
    "start": "4879345",
    "end": "4882580"
  },
  {
    "text": "And this is the waveform of what speaker two is speaking, right?",
    "start": "4884390",
    "end": "4889695"
  },
  {
    "text": "And this is time, right? Time is progressing this wave.",
    "start": "4889695",
    "end": "4895865"
  },
  {
    "text": "And if we take a point in time and record a snapshot,",
    "start": "4895865",
    "end": "4904545"
  },
  {
    "text": "right, this will give us S^i. So at time i,",
    "start": "4904545",
    "end": "4910620"
  },
  {
    "text": "S^i will be set S^i_1,",
    "start": "4910620",
    "end": "4917560"
  },
  {
    "text": "S^i_2.",
    "start": "4919730",
    "end": "4924340"
  },
  {
    "text": "And this is in ER_d here, d equals 2, right?",
    "start": "4926630",
    "end": "4933720"
  },
  {
    "text": "So the- the S^i_1 is this height,",
    "start": "4933720",
    "end": "4939090"
  },
  {
    "text": "S^i_2 is this height, right? And- and this is the time. So you can think of a snapshot in time to be one example.",
    "start": "4939090",
    "end": "4947805"
  },
  {
    "text": "And the- the kind of the amplitude of what the speaker is speaking at that time to be the coordinate value at that example.",
    "start": "4947805",
    "end": "4955590"
  },
  {
    "text": "Right? And similarly, we have you know, the two microphones, right?",
    "start": "4955590",
    "end": "4964260"
  },
  {
    "text": "So this is microphone one, X_1, X_2.",
    "start": "4964260",
    "end": "4969300"
  },
  {
    "text": "And this microphone is recording some linear combination of the two, right?",
    "start": "4969300",
    "end": "4975040"
  },
  {
    "text": "This is X_1 and this is X_2.",
    "start": "4976040",
    "end": "4982800"
  },
  {
    "text": "Right? Similarly, over here, again, time is moving to the right,",
    "start": "4982800",
    "end": "4989325"
  },
  {
    "text": "and at time i, right,",
    "start": "4989325",
    "end": "4998160"
  },
  {
    "text": "X_i is equal to X^i_1, X^i_2.",
    "start": "4998160",
    "end": "5007220"
  },
  {
    "text": "But here these are the values recorded by the microphone at time i.",
    "start": "5007220",
    "end": "5012000"
  },
  {
    "text": "We will assume that we have the same number of microphones as the number of speakers.",
    "start": "5012400",
    "end": "5022855"
  },
  {
    "text": "And we are only observing these X's.",
    "start": "5022855",
    "end": "5028470"
  },
  {
    "text": "And the problem now is, we are only given these X's.",
    "start": "5028470",
    "end": "5033785"
  },
  {
    "text": "We are told the number of, you know, we obviously know the number of microphones. We assume that there are the same number of speakers.",
    "start": "5033785",
    "end": "5041945"
  },
  {
    "text": "And by only having these d different microphone recordings, we want to recover the original- original speech signal spoken by each speaker, right?",
    "start": "5041945",
    "end": "5053855"
  },
  {
    "text": "That's- that's the- that's the setting in which we want to- in which we are operating.",
    "start": "5053855",
    "end": "5059765"
  },
  {
    "text": "And the- and what that translates to essentially is we",
    "start": "5059765",
    "end": "5066550"
  },
  {
    "text": "have- we make the assumption that X equals AS,",
    "start": "5066550",
    "end": "5074089"
  },
  {
    "text": "which means X_i equals A of S^i, okay, A is a square matrix",
    "start": "5074089",
    "end": "5081590"
  },
  {
    "text": "because we have the same number of speakers as the number of microphones. And what we wanna do is to calculate W is equal to A inverse,",
    "start": "5081590",
    "end": "5092939"
  },
  {
    "text": "which is called the unmixing matrix.",
    "start": "5093880",
    "end": "5098370"
  },
  {
    "text": "Right? W equals A inverse is the unmi- unmixing matrix where we are- where W can take X,",
    "start": "5102430",
    "end": "5111455"
  },
  {
    "text": "right, and return S. Okay? So the W matrix can separate the speaker.",
    "start": "5111455",
    "end": "5118910"
  },
  {
    "text": "So this is also called the source separation problem, where we're given X's, which are different- different mixtures of the sources.",
    "start": "5118910",
    "end": "5127835"
  },
  {
    "text": "And using just these mixed sources, we want to construct this W matrix.",
    "start": "5127835",
    "end": "5133714"
  },
  {
    "text": "It's called the unmixing matrix, which will recover back the original speech. Right? And the- the- the assumption that we are going to make,",
    "start": "5133714",
    "end": "5142895"
  },
  {
    "text": "we're going to make only one assumption. The one assumption that we're going to make is that S_i's,",
    "start": "5142895",
    "end": "5150030"
  },
  {
    "text": "are distributed- are independent",
    "start": "5150700",
    "end": "5156650"
  },
  {
    "text": "in the sense each S- S- SJ is independent of the other SJ's,",
    "start": "5156650",
    "end": "5161900"
  },
  {
    "text": "so the speaker-the assumptions we are going to make are.",
    "start": "5161900",
    "end": "5165690"
  },
  {
    "text": "So what are the assumptions we've made so far? So the first assumption we want is the number of speakers equals number of microphones.",
    "start": "5168940",
    "end": "5177305"
  },
  {
    "text": "Right? And the second assumption we made was that these two are- are associated with a linear relation,",
    "start": "5177305",
    "end": "5186110"
  },
  {
    "text": "so S is equal to WX. Right? We made a linearity assumption.",
    "start": "5186110",
    "end": "5191495"
  },
  {
    "text": "And the third assumption that we're going to make, which- you know, with just these two it's impossible to recover, right?",
    "start": "5191495",
    "end": "5199205"
  },
  {
    "text": "We want- we need to make one more- one more- some more assumptions. The - the assumption that- that's going to help us recover is",
    "start": "5199205",
    "end": "5206720"
  },
  {
    "text": "that we're going to assume SJ is independent of SK,",
    "start": "5206720",
    "end": "5212675"
  },
  {
    "text": "where J is not equal to K. What that means is SJ has some kind of a probability distribution,",
    "start": "5212675",
    "end": "5220295"
  },
  {
    "text": "and SK has- you know it belongs to another probability distribution. And these two random variables are independent.",
    "start": "5220295",
    "end": "5228210"
  },
  {
    "text": "Right? What that essentially means is we are assuming that what one speaker is saying is independent of what the other speaker is saying,",
    "start": "5228820",
    "end": "5237830"
  },
  {
    "text": "which may not be a valid assumption in reality, if two people are having a conversation,",
    "start": "5237830",
    "end": "5243290"
  },
  {
    "text": "then most likely only one of them is speaking at a time and et cetera. But for- for- you know in- in-",
    "start": "5243290",
    "end": "5251060"
  },
  {
    "text": "in- it generally turns out that this is- this still happens to be a reasonable assumption to make.",
    "start": "5251060",
    "end": "5257600"
  },
  {
    "text": "Right? So independence assumption.",
    "start": "5257600",
    "end": "5261570"
  },
  {
    "text": "And we need to make one more assumption that will allow us to solve this problem. The last assumption that we're going to make is that SJ",
    "start": "5262930",
    "end": "5271970"
  },
  {
    "text": "is not Gaussian, right?",
    "start": "5271970",
    "end": "5279545"
  },
  {
    "text": "So this is a non-Gaussian assumption.",
    "start": "5279545",
    "end": "5282780"
  },
  {
    "text": "Right? Why- why- why do we need to make this non-Gaussian assumption?",
    "start": "5284740",
    "end": "5290885"
  },
  {
    "text": "Let's- let's see a few pictures and hopefully, that will give some intuition.",
    "start": "5290885",
    "end": "5299300"
  },
  {
    "text": "[NOISE]",
    "start": "5299300",
    "end": "5358250"
  },
  {
    "text": "All right, so these are basically the- the slides we saw in the intro lecture just on- well,",
    "start": "5358250",
    "end": "5366530"
  },
  {
    "text": "let's hope the- the audio works. So- so here we have- we are considering the case of d equals 2.",
    "start": "5366530",
    "end": "5376985"
  },
  {
    "text": "And these are the versions that are recorded in - in the microphones, the two microphones.",
    "start": "5376985",
    "end": "5383270"
  },
  {
    "text": "And these are what the original speaker said. Right? So.",
    "start": "5383270",
    "end": "5388760"
  },
  {
    "text": "[OVERLAPPING] [FOREIGN].",
    "start": "5388760",
    "end": "5398150"
  },
  {
    "text": "It's basically two speakers are counting 1-10, one in English and the other in Spanish. And this is a recording from another microphone.",
    "start": "5398150",
    "end": "5405800"
  },
  {
    "text": "[OVERLAPPING] [FOREIGN].",
    "start": "5405800",
    "end": "5414349"
  },
  {
    "text": "Audio is a little low. And once it's separated, it sounds like this.",
    "start": "5414350",
    "end": "5420410"
  },
  {
    "text": "Can people at the back hear it? Okay. 1, 2 , 3, 4,",
    "start": "5420410",
    "end": "5427670"
  },
  {
    "text": "5, 6, 7, 8, 9, 10. And the, uh, other one once it's separated.",
    "start": "5427670",
    "end": "5435699"
  },
  {
    "text": "[FOREIGN] Right?",
    "start": "5435700",
    "end": "5443710"
  },
  {
    "text": "So all that was fed to this, uh, algorithm was these two, uh, mixed clips and nothing else.",
    "start": "5443710",
    "end": "5451150"
  },
  {
    "text": "And by making certain independent assumptions and linearity assumptions, the algorithm was able to recover these separated versions of the two audio clips, right?",
    "start": "5451150",
    "end": "5462805"
  },
  {
    "text": "And to, um, to understand, um, these- these assumptions,",
    "start": "5462805",
    "end": "5472045"
  },
  {
    "text": "I made a few slides that might, yeah, so let me open it.",
    "start": "5472045",
    "end": "5488119"
  },
  {
    "text": "All right, so what do we have here? So, um, we assume that SI and SJ must be independent.",
    "start": "5502160",
    "end": "5510630"
  },
  {
    "text": "And we also assume that each of the SJs must not be Gaussian. So the- the first row- all the three rows correspond to Gaussian distribution.",
    "start": "5510630",
    "end": "5521125"
  },
  {
    "text": "The second row, all the three correspond to Laplace distribution and a third of the three correspond to logistic distribution.",
    "start": "5521125",
    "end": "5527739"
  },
  {
    "text": "So if in the case of Gaussians, here x1, the x-axis and the y-axis represents two independent Gaussians.",
    "start": "5527740",
    "end": "5537219"
  },
  {
    "text": "And each point in this cluster of points are points that are sampled",
    "start": "5537220",
    "end": "5542905"
  },
  {
    "text": "from a joint Gaussian distribution where the- the- the- the two components are independent.",
    "start": "5542905",
    "end": "5548875"
  },
  {
    "text": "So if we sample from two independent Gaussian distributions, uh, assume both are, have mean 0, variance 1.",
    "start": "5548875",
    "end": "5556810"
  },
  {
    "text": "Then you will see that the points are generally, um, the clustered points have this circular formation, right?",
    "start": "5556810",
    "end": "5564670"
  },
  {
    "text": "And then we apply, uh, some kind of a linear transform. So the first column corresponds to, you know,",
    "start": "5564670",
    "end": "5572470"
  },
  {
    "text": "one linear transform, the second column corresponds to another linear transform. The linear transform is applied to the points that were sampled here, right?",
    "start": "5572470",
    "end": "5582580"
  },
  {
    "text": "And we see that if, uh, so, the- the- the first column corresponds to a linear transform that was more or less just a rotation.",
    "start": "5582580",
    "end": "5590680"
  },
  {
    "text": "We see that the rotated version looks exactly like the original version if it's Gaussian, right?",
    "start": "5590680",
    "end": "5597835"
  },
  {
    "text": "If it is some other, um, uh, distributed- some other kind of a linear transform,",
    "start": "5597835",
    "end": "5603219"
  },
  {
    "text": "then it gets some kind of an ellipse shape, right? Now, instead of- instead of Gaussians,",
    "start": "5603220",
    "end": "5610855"
  },
  {
    "text": "let's assume we have- we start with two independent Laplace distributions, right? Where the Source 1 and Source 2 are",
    "start": "5610855",
    "end": "5618070"
  },
  {
    "text": "sampled according to independent Laplace distributions. We see that the shape taken by the product of two independent Laplace has this kind of,",
    "start": "5618070",
    "end": "5627955"
  },
  {
    "text": "uh, you know, um, it has this diamond shape, right?",
    "start": "5627955",
    "end": "5635545"
  },
  {
    "text": "And then if we, you know, apply linear transformations, we get, you know, shapes that look like this.",
    "start": "5635545",
    "end": "5642460"
  },
  {
    "text": "And similarly, if you start with a logistic- two independent logistic distributions, um, and they're independent, they look like this.",
    "start": "5642460",
    "end": "5648864"
  },
  {
    "text": "And as you- you know, apply transformations, they take different shapes. In ICA, we are starting with data that looks like this, right?",
    "start": "5648865",
    "end": "5661330"
  },
  {
    "text": "The recorded- the microphone recordings. If you were to plot them, they look something like this.",
    "start": "5661330",
    "end": "5667315"
  },
  {
    "text": "And given these, uh, when we start- if- if we were to start with, uh,",
    "start": "5667315",
    "end": "5673494"
  },
  {
    "text": "data that looks like this, and we want to recover the original untransformed data.",
    "start": "5673495",
    "end": "5680155"
  },
  {
    "text": "With Laplace and logistic, we see that we can make a good guess.",
    "start": "5680155",
    "end": "5685210"
  },
  {
    "text": "You know, the corners corresponding- correspond to corners, et cetera. But in case of Gaussian,",
    "start": "5685210",
    "end": "5690815"
  },
  {
    "text": "it's hard to tell whether the, you know, in fact, it's impossible to tell whether this transform data came",
    "start": "5690815",
    "end": "5697950"
  },
  {
    "text": "from this version as the source or this version as the source, right?",
    "start": "5697950",
    "end": "5703150"
  },
  {
    "text": "Because in- in- in Gaussians, you have the smooth ellipsis.",
    "start": "5703150",
    "end": "5708429"
  },
  {
    "text": "And there is this rotational ambiguity of not being able to determine what the original shape was.",
    "start": "5708430",
    "end": "5716184"
  },
  {
    "text": "Whereas with these other distributions, they do not have this, you know, smooth ellipse shape. They have these, you know,",
    "start": "5716185",
    "end": "5722530"
  },
  {
    "text": "the- the intuition to have is that you have these corners, right? And these corners can correspond to one of these corners.",
    "start": "5722530",
    "end": "5729220"
  },
  {
    "text": "These non-Gaussian distributions still do have the ambiguity that the- the axes can be permuted, right?",
    "start": "5729220",
    "end": "5738460"
  },
  {
    "text": "You can construct- you can construct unmixing matrix- matrices that start from here and map you back to the,",
    "start": "5738460",
    "end": "5748570"
  },
  {
    "text": "uh, the- the- the unmixed versions where the axes are permuted.",
    "start": "5748570",
    "end": "5753790"
  },
  {
    "text": "Because one corner can get mapped to a different corner, right? And you can also have ambiguity that the axes are flipped in their, uh, in their sign.",
    "start": "5753790",
    "end": "5764635"
  },
  {
    "text": "However, though in- in problems where those ambiguities are tolerable,",
    "start": "5764635",
    "end": "5771130"
  },
  {
    "text": "ICA works very well. Which means, for example, if you are applying ICA to,",
    "start": "5771130",
    "end": "5776575"
  },
  {
    "text": "you know, audio source separation, where once we unmixed the- unmix the, uh,",
    "start": "5776575",
    "end": "5782410"
  },
  {
    "text": "mixed audio clips, we recover the original audio clips. But the- the Speaker 1 and Speaker 2,",
    "start": "5782410",
    "end": "5790180"
  },
  {
    "text": "identities may have shifted, but that's okay because we never knew what the original identities were, right?",
    "start": "5790180",
    "end": "5795835"
  },
  {
    "text": "In- in case of ICA, once we- once we run ICA and recover the unmixed versions,",
    "start": "5795835",
    "end": "5801010"
  },
  {
    "text": "there is no guarantee that the first column of- or- or- or the identity of the first source will always be the same, right?",
    "start": "5801010",
    "end": "5809094"
  },
  {
    "text": "There's- there's ambiguity about- about what the original sources are.",
    "start": "5809095",
    "end": "5814300"
  },
  {
    "text": "And there's also ambiguity that the signs flip and an audio generally that's okay too. The sign of your- of your- of",
    "start": "5814300",
    "end": "5820330"
  },
  {
    "text": "your wave file can- can- can flip and you'll still get the same audio, right? So this gives us some intuition of",
    "start": "5820330",
    "end": "5826929"
  },
  {
    "text": "the ambiguity that- that arises only in Gaussians, right? This- this spherical shape is characteristic of the Gaussian distribution, right?",
    "start": "5826930",
    "end": "5835059"
  },
  {
    "text": "In no other distribution, if you multiply the- the, uh, uh, uh, the, if you multiply the- the PDFs of two independent distribution,",
    "start": "5835060",
    "end": "5844150"
  },
  {
    "text": "no other will give you a contour map that is spherical. The spherical shape comes only from a Gaussian. So as long as the, uh,",
    "start": "5844150",
    "end": "5852025"
  },
  {
    "text": "the distribution of our sources is assumed to be non-Gaussian,",
    "start": "5852025",
    "end": "5857440"
  },
  {
    "text": "we have some hope of recovering the- the original signal. Yes, there was a question? In theory, we believe to care about this last assumption.",
    "start": "5857440",
    "end": "5863619"
  },
  {
    "text": "But in practice, no one actually speaks according to the distribution. So does it ever matter what our last assumption is?",
    "start": "5863620",
    "end": "5870370"
  },
  {
    "text": "Yeah, so- so the question is, in practice, you know, nobody actually speaks according to a Gaussian or a Laplace, right?",
    "start": "5870370",
    "end": "5875905"
  },
  {
    "text": "What's- what's- what's the, uh, you know, does it actually matter? Uh, it actually does matter a lot.",
    "start": "5875905",
    "end": "5881380"
  },
  {
    "text": "In fact, that's the only thing that matters because even though your data may not be- may not be,",
    "start": "5881380",
    "end": "5887364"
  },
  {
    "text": "uh, um, distributed according to a Gaussian. What I'm about to show in the next slide here is that",
    "start": "5887365",
    "end": "5894070"
  },
  {
    "text": "these- these- these two clusters that we see are in fact- uh, so I just took the waveforms on the two audio clips that I, uh, uh,",
    "start": "5894070",
    "end": "5901615"
  },
  {
    "text": "that I played short a- a few minutes ago and just plotted a scatter plot of the corresponding,",
    "start": "5901615",
    "end": "5907765"
  },
  {
    "text": "um, uh, you know, audio amplitudes from the two sound clips. We see that the- in the- in the separated version,",
    "start": "5907765",
    "end": "5917244"
  },
  {
    "text": "even though they are not Gaussian, it is still not spherical, right? So there- there is some amount of,",
    "start": "5917245",
    "end": "5923530"
  },
  {
    "text": "you know, you- you can kind of see that, you know, there are some elongation along this axis and there are some elongation on- on- on this axis and it's not perfectly spherical.",
    "start": "5923530",
    "end": "5931270"
  },
  {
    "text": "So when we start with the mixed version, so this is basically th- the scatter plot of the mixed audio clips, right?",
    "start": "5931270",
    "end": "5939010"
  },
  {
    "text": "We see that the algorithm is able to recover, you know, the- the- the- the two independent components even though the- the, uh,",
    "start": "5939010",
    "end": "5946990"
  },
  {
    "text": "even though the data was not actually, uh, uh, distributed according- exactly according to a Laplace or- or- or logistic, right?",
    "start": "5946990",
    "end": "5956829"
  },
  {
    "text": "So, uh, back some intuition to- to, uh, show why non-Gaussian assumption is- is important.",
    "start": "5956830",
    "end": "5963490"
  },
  {
    "text": "[NOISE] So with these two assumptions-",
    "start": "5963490",
    "end": "5974260"
  },
  {
    "text": "so with, uh, these two assumptions, we can now formulate ICA.",
    "start": "5985950",
    "end": "5992510"
  },
  {
    "text": "And we need one final thing to take care off, which is the- the linearity assumption.",
    "start": "5995760",
    "end": "6004934"
  },
  {
    "text": "So, uh, supposing we have a random variable, let's assume it is uniformly distributed between 0 and 1.",
    "start": "6004935",
    "end": "6012344"
  },
  {
    "text": "It's a uniform random distribution that lies between 0 and 1. So, um, x is uniform. Let me take a different pen.",
    "start": "6012345",
    "end": "6026260"
  },
  {
    "text": "Uniform between 0 and 1.",
    "start": "6027710",
    "end": "6031480"
  },
  {
    "text": "Yeah, this pen is better, right?",
    "start": "6034670",
    "end": "6043350"
  },
  {
    "text": "And the- so which means x looks like this,",
    "start": "6043350",
    "end": "6048190"
  },
  {
    "text": "0 and 1 and the PDF of x looks like this.",
    "start": "6048770",
    "end": "6053920"
  },
  {
    "text": "So this is x. This is p of x.",
    "start": "6054680",
    "end": "6059910"
  },
  {
    "text": "So it has density equal to 1 between 0 and 1, and zero everywhere else, right?",
    "start": "6059910",
    "end": "6067065"
  },
  {
    "text": "So this is just a uniform random, uh, variable over 0 and 1. Now what about 2x, right?",
    "start": "6067065",
    "end": "6075045"
  },
  {
    "text": "What is- let's consider y equals 2x. What is p of y?",
    "start": "6075045",
    "end": "6081340"
  },
  {
    "text": "So- so what's- what's- what's p of- so when we- when we, uh,",
    "start": "6083140",
    "end": "6088625"
  },
  {
    "text": "when we do y equals 2x now y will be distributed between 0 and 2, right?",
    "start": "6088625",
    "end": "6096255"
  },
  {
    "text": "It has density 0 outside this region. What is p of y between 0 and 2? Half, right?",
    "start": "6096255",
    "end": "6106840"
  },
  {
    "text": "This is half. Why is this half? [BACKGROUND] Right.",
    "start": "6108900",
    "end": "6119349"
  },
  {
    "text": "So- exactly. So we- we- the probability distributions require that the area under",
    "start": "6119350",
    "end": "6124600"
  },
  {
    "text": "the PDF must integrate to 1, right? So if we stretch the x-axis,",
    "start": "6124600",
    "end": "6130810"
  },
  {
    "text": "we have to shrink the y-axis by a corresponding amount and that's because the area under the PDF must integrate to 1, right?",
    "start": "6130810",
    "end": "6138940"
  },
  {
    "text": "Now, what happens in a higher dimension? If x is, you know,",
    "start": "6138940",
    "end": "6144205"
  },
  {
    "text": "in general x inste- instead of 1x, let's assume this is some script x, right?",
    "start": "6144205",
    "end": "6149980"
  },
  {
    "text": "And instead of multiplying it by one scalar, we do y equals, say,",
    "start": "6149980",
    "end": "6156040"
  },
  {
    "text": "some, um, um, y equals wx,",
    "start": "6156040",
    "end": "6161320"
  },
  {
    "text": "where w is some matrix. Now, x is- x is- is- is in RD.",
    "start": "6161320",
    "end": "6169945"
  },
  {
    "text": "This is R^ d by d, and y is again in R^d, right?",
    "start": "6169945",
    "end": "6177370"
  },
  {
    "text": "And we have P- P of x to be some x- to be some value.",
    "start": "6177370",
    "end": "6185185"
  },
  {
    "text": "So p of y will be,",
    "start": "6185185",
    "end": "6190280"
  },
  {
    "text": "what can we say about P of y? P of y will be? [BACKGROUND] P of y is wx, right?",
    "start": "6190980",
    "end": "6200994"
  },
  {
    "text": "So let's assume this is the density of y, and this is the density of x.",
    "start": "6200995",
    "end": "6206665"
  },
  {
    "text": "This is correct? This is not correct, right? And it's- it's- it's not correct because of- of the same reason",
    "start": "6206665",
    "end": "6212770"
  },
  {
    "text": "where P of y had to be- so P_y of y had to be P_x of 2x times 1 over 2, right?",
    "start": "6212770",
    "end": "6224065"
  },
  {
    "text": "So similarly, here, we need to divide it by the determinant of w, right?",
    "start": "6224065",
    "end": "6231670"
  },
  {
    "text": "This is - this determinant of w- you - you might have seen this from your probability theory course.",
    "start": "6231670",
    "end": "6239275"
  },
  {
    "text": "This is also called the Jacobian. So Jacobian, when- whenever you perform a change of variable of your random variable,",
    "start": "6239275",
    "end": "6248275"
  },
  {
    "text": "the density will be transformed according to, you know, this extra Jacobian, right? Yes, question.",
    "start": "6248275",
    "end": "6255010"
  },
  {
    "text": "[inaudible]",
    "start": "6255010",
    "end": "6265929"
  },
  {
    "text": "Sorry, this should be py. Yeah, because py is- is- is,",
    "start": "6265930",
    "end": "6272500"
  },
  {
    "text": "oh, so- sorry, sorry, sorry. So py is p- px",
    "start": "6272500",
    "end": "6282860"
  },
  {
    "text": "of x is y by 2, yeah, y by 2 times p, times half.",
    "start": "6283350",
    "end": "6289870"
  },
  {
    "text": "That make sense? [BACKGROUND] And- and what did I write here?",
    "start": "6289870",
    "end": "6296635"
  },
  {
    "text": "I think here also I- this - if - similarly, if py of y will be w inverse of",
    "start": "6296635",
    "end": "6304570"
  },
  {
    "text": "y- w inverse of y times 1 order, right?",
    "start": "6304570",
    "end": "6312409"
  },
  {
    "text": "So whenever- whenever we- we transform a variable, some linear transform,",
    "start": "6313350",
    "end": "6319750"
  },
  {
    "text": "the density will get this extra determinant factor with just- with with these- with these pieces.",
    "start": "6319750",
    "end": "6328630"
  },
  {
    "text": "In fact, we are ready to finish ICA. So in ICA, we define P of x to",
    "start": "6328630",
    "end": "6343380"
  },
  {
    "text": "be product from j equals 1 to d ps",
    "start": "6343380",
    "end": "6352045"
  },
  {
    "text": "of w_j transpose",
    "start": "6352045",
    "end": "6357955"
  },
  {
    "text": "x times w. Over here,",
    "start": "6357955",
    "end": "6367555"
  },
  {
    "text": "w is the on mixing matrix,",
    "start": "6367555",
    "end": "6373240"
  },
  {
    "text": "where wj times sum x",
    "start": "6373240",
    "end": "6388690"
  },
  {
    "text": "will give us back the jth source, right?",
    "start": "6388690",
    "end": "6395050"
  },
  {
    "text": "So w is - w, j is the unmixing matrix. You give it this mixed x multiply each w j times x will give us sj,",
    "start": "6395050",
    "end": "6402565"
  },
  {
    "text": "sj, so sj equals wsj times x. So w_j times x and we define the - the probability of x",
    "start": "6402565",
    "end": "6414960"
  },
  {
    "text": "as- in term of p of s and we make the assumption that p of s is distributed according to the logistic distribution.",
    "start": "6414960",
    "end": "6425679"
  },
  {
    "text": "So the logistic distribution is the distribution whose CDF is the logistic function.",
    "start": "6428910",
    "end": "6434350"
  },
  {
    "text": "Okay? So if you remember the logistic function is- is p,",
    "start": "6434350",
    "end": "6445690"
  },
  {
    "text": "so the CDF- the CDF of x is equal to 1 over 1 plus e to the minus x.",
    "start": "6445690",
    "end": "6455020"
  },
  {
    "text": "So this is the CDF of logistic.",
    "start": "6455020",
    "end": "6460240"
  },
  {
    "text": "You can call this sigma of x and so the PDF - the probability density of the logistic f of x is the derivative of this.",
    "start": "6460240",
    "end": "6472360"
  },
  {
    "text": "That's basically sigma of x times 1 minus sigma x by sigma is the- is the sigmoid of the logistic function, right?",
    "start": "6472360",
    "end": "6481045"
  },
  {
    "text": "And using this, so, so this sigma- sigmoid x times 1 minus sigmoid x will be ps of x.",
    "start": "6481045",
    "end": "6491605"
  },
  {
    "text": "So we assume that the sources are distributed according to the sigmoid distribution which gives us,",
    "start": "6491605",
    "end": "6497920"
  },
  {
    "text": "we can write the l of w, where the parameter here that we are training is w is sum over I equals 1 to",
    "start": "6497920",
    "end": "6508630"
  },
  {
    "text": "n. Sum over j",
    "start": "6508630",
    "end": "6514719"
  },
  {
    "text": "equals 1 to d log of the PDF,",
    "start": "6514720",
    "end": "6521605"
  },
  {
    "text": "which is sigmoid of x times 1 minus sigmoid times 1 minus sigmoid of",
    "start": "6521605",
    "end": "6531205"
  },
  {
    "text": "x I plus log determinant of w, right?",
    "start": "6531205",
    "end": "6555970"
  },
  {
    "text": "So the parameter that we are interested here is just w. Previously, the parameters that we were trying to recover happened to be the parameters",
    "start": "6555970",
    "end": "6564385"
  },
  {
    "text": "of the - of- of the probability distribution itself. For example, we would define the likelihood over",
    "start": "6564385",
    "end": "6571135"
  },
  {
    "text": "mu or sigma or some such thing like that here, the parameter that we're trying to learn is this transformation",
    "start": "6571135",
    "end": "6578590"
  },
  {
    "text": "w that's going to take us back into something that is distributed according to a sigmoid.",
    "start": "6578590",
    "end": "6583989"
  },
  {
    "text": "And using the- the- the linear transformation,",
    "start": "6583990",
    "end": "6589570"
  },
  {
    "text": "we- we- we get this extra log determinant term due to the extra determinant that came due to the Jacobian, right?",
    "start": "6589570",
    "end": "6598900"
  },
  {
    "text": "And that's all there is to it. You take this objective, take the derivative,",
    "start": "6598900",
    "end": "6604930"
  },
  {
    "text": "perform gradient descent and run gradient descent until you converge, right, and the corresponding update step,",
    "start": "6604930",
    "end": "6611350"
  },
  {
    "text": "so you take the derivative of this and you get the update step that looks like this, w equals w plus alpha.",
    "start": "6611350",
    "end": "6619030"
  },
  {
    "text": "Some learning rate times 1 minus 2g,",
    "start": "6619030",
    "end": "6627550"
  },
  {
    "text": "where g is just the sigmoid, 1 minus 2 sigmoid of w1 transpose xi,",
    "start": "6627550",
    "end": "6638020"
  },
  {
    "text": "1 minus sigmoid w2 transpose xi.",
    "start": "6638970",
    "end": "6648490"
  },
  {
    "text": "And so 1 times x, right - transpose plus w transpose inverse, right?",
    "start": "6648490",
    "end": "6658224"
  },
  {
    "text": "So this is the update rule and you can perform stochastic gradient ascent and once this converges,",
    "start": "6658225",
    "end": "6664420"
  },
  {
    "text": "take the converged w, multiply your x's with w, and you get Ss.",
    "start": "6664420",
    "end": "6674594"
  },
  {
    "text": "And if you play these Ss, you write it into a wav file and you play these Ss, they'll be separated, okay? It's almost magical, it works.",
    "start": "6674595",
    "end": "6681554"
  },
  {
    "text": "And in your homework. So here we have the update rules for assuming that PSs were logistic and in your homework you will do it.",
    "start": "6681555",
    "end": "6689115"
  },
  {
    "text": "You make a small- you will try it with a different distribution, you assume it's Laplace distributed and",
    "start": "6689115",
    "end": "6694860"
  },
  {
    "text": "in- in your homework you have five different audio clips instead of two audio clips and run it until it converges.",
    "start": "6694860",
    "end": "6702220"
  },
  {
    "text": "Take the resulting unmixing matrix, the converged unmixing matrix, and multiply your mixed audio clips through it and you'll recover",
    "start": "6702220",
    "end": "6710650"
  },
  {
    "text": "a new audio clip and you play it and they should- they should sound well separated.",
    "start": "6710650",
    "end": "6716199"
  },
  {
    "text": "And one final tip for your homework, in the logistic distribution you will encounter",
    "start": "6716200",
    "end": "6722095"
  },
  {
    "text": "the absolute value of x and you'll have to differentiate it, right? And the, what's d by dx of the absolute value of x.",
    "start": "6722095",
    "end": "6730720"
  },
  {
    "text": "[BACKGROUND] Yeah, it is split. So absolute value of x looks like this, right?",
    "start": "6730720",
    "end": "6736480"
  },
  {
    "text": "So it is- it has slope minus 1 when it's negative and slope plus 1 when it's positive.",
    "start": "6736480",
    "end": "6741730"
  },
  {
    "text": "So d by dx of the absolute value of x is just the sine of x, right?",
    "start": "6741730",
    "end": "6746830"
  },
  {
    "text": "So you'll- you'll need that for your homework. You can probably- you would have probably figured it out but in case you're struggling,",
    "start": "6746830",
    "end": "6753805"
  },
  {
    "text": "the derivative of the absolute value of x, this is the sine of x and with this you can you know replace- replace the sigmoid PDF with the Laplace PDF.",
    "start": "6753805",
    "end": "6764140"
  },
  {
    "text": "Take the derivatives, derive a different update rule and run it on the audio clips and you'll be able to recover them and yeah,",
    "start": "6764140",
    "end": "6772539"
  },
  {
    "text": "so that- that basically covers PCA and ICF. Any other questions? If you have questions, you know,",
    "start": "6772540",
    "end": "6778810"
  },
  {
    "text": "walk up- walk up to the stage here. I'm happy to take questions.",
    "start": "6778810",
    "end": "6783230"
  }
]