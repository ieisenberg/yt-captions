[
  {
    "start": "0",
    "end": "5660"
  },
  {
    "text": "OK, I should try\nand get started. So what we're going\nto do today is",
    "start": "5660",
    "end": "12350"
  },
  {
    "text": "we're going to try and do\neverything else that you need to know about word\nvectors and start to learn",
    "start": "12350",
    "end": "19820"
  },
  {
    "text": "a teeny bit about neural nets. And then we'll get much further\ninto doing more with the math",
    "start": "19820",
    "end": "26270"
  },
  {
    "text": "of neural nets next week. So this is the general plan. So I'm going to\nfinish up from where",
    "start": "26270",
    "end": "34310"
  },
  {
    "text": "I was last time with\noptimization basics, then look a little bit more\nabout word2vec and word vectors,",
    "start": "34310",
    "end": "41730"
  },
  {
    "text": "and then some of the\nvariants of word2vec. And then I'm going to briefly\nconsider alternatives,",
    "start": "41730",
    "end": "48180"
  },
  {
    "text": "sort of like what can you\nget from just counting words in different ways. Then we're going to go\non and talk a little bit",
    "start": "48180",
    "end": "54860"
  },
  {
    "text": "about the evaluation of word\nvectors, the topic of word senses that already came up a\ncouple of times last time when",
    "start": "54860",
    "end": "64089"
  },
  {
    "text": "people were asking questions,\nand then towards the end, start to introduce the\nidea of classification,",
    "start": "64090",
    "end": "71420"
  },
  {
    "text": "doing neural classification, and\nwhat neural networks are about,",
    "start": "71420",
    "end": "76430"
  },
  {
    "text": "which is something\nthat we'll then expand on more in the second week.",
    "start": "76430",
    "end": "82000"
  },
  {
    "text": "Before I get into that, just\nnotes on course organization. So remember, the first\nassignment is already out",
    "start": "82000",
    "end": "89500"
  },
  {
    "text": "and it's due before\nclass next Tuesday. So then our Python review\nsession is going to be taught",
    "start": "89500",
    "end": "98799"
  },
  {
    "text": "this Friday 3:30 to 4:20. It's not going to\nbe taught here. It's going to be taught in\ngates B01, the gates basement.",
    "start": "98800",
    "end": "107170"
  },
  {
    "text": "I encourage everyone again\nto come to office hours and help sessions. They've already started.",
    "start": "107170",
    "end": "113810"
  },
  {
    "text": "They're listed on the website. We're having these\nsort of offers our help",
    "start": "113810",
    "end": "119750"
  },
  {
    "text": "sessions in classrooms\nwith multiple TAs. So just turn up if you're on\ncampus and you can be helped.",
    "start": "119750",
    "end": "126900"
  },
  {
    "text": "And if you are on campus,\nwe'd like you to just turn up. Though we do also\nhave a Zoom option",
    "start": "126900",
    "end": "132500"
  },
  {
    "text": "for Stanford Online students. ",
    "start": "132500",
    "end": "137510"
  },
  {
    "text": "And finally, I\nhave office hours, which I have not yet opened, but\nI will open sometime tonight.",
    "start": "137510",
    "end": "144109"
  },
  {
    "text": "They're going to be\non Monday afternoons. Now obviously, given\nthe number of people, not everyone can make\nit into my office hours.",
    "start": "144110",
    "end": "151370"
  },
  {
    "text": "And I'm going to do\nthese by appointment. So they're by 15 minute\nappointments on Calendly.",
    "start": "151370",
    "end": "157160"
  },
  {
    "text": "But I'm very happy to\ntalk to some people. And I put this little\nnote at the end saying,",
    "start": "157160",
    "end": "165180"
  },
  {
    "text": "don't hog the slots. Some people think it would be a\nreally good idea if they really",
    "start": "165180",
    "end": "170390"
  },
  {
    "text": "work out how to sign up every\nweek for an office hour session with me. And that's of a little\nbit anti-social.",
    "start": "170390",
    "end": "178060"
  },
  {
    "text": "So think about that. So at the end of last\ntime, I did a sort",
    "start": "178060",
    "end": "185980"
  },
  {
    "text": "of bad job of trying to write\non slides of working out the derivatives of word2vec.",
    "start": "185980",
    "end": "194080"
  },
  {
    "text": "And hopefully, you could\nread it much more clearly in the version that appears on\nthe website where I was doing it",
    "start": "194080",
    "end": "200110"
  },
  {
    "text": "at home more carefully. So that was saying that\nwe had this loss function,",
    "start": "200110",
    "end": "206030"
  },
  {
    "text": "and our job was to work\nout its derivatives, which would tell us which direction\nto go to walk downhill.",
    "start": "206030",
    "end": "215200"
  },
  {
    "text": "And so I didn't really\nquite finish the loop here. So we have some cost function\nthat we want to minimize.",
    "start": "215200",
    "end": "222890"
  },
  {
    "text": "And then we work out the\ngradient of that function to work out which\ndirection is downhill.",
    "start": "222890",
    "end": "230240"
  },
  {
    "text": "And then the simplest\nalgorithm is then that we work out the\ndirection downhill,",
    "start": "230240",
    "end": "237960"
  },
  {
    "text": "we walk a little bit in that\ndirection, and then we repeat. We work out the\ngradient at this point,",
    "start": "237960",
    "end": "244860"
  },
  {
    "text": "we walk downhill a little\nbit, and we keep on going, and we'll get to the minimum.",
    "start": "244860",
    "end": "249930"
  },
  {
    "text": "And with a sort of a one\ndimensional function like this, it's very simple. We're just walking downhill.",
    "start": "249930",
    "end": "256440"
  },
  {
    "text": "But when we have a function\nof many, many dimensions, when we calculate the\ngradient at different points,",
    "start": "256440",
    "end": "262049"
  },
  {
    "text": "we might be starting to walk\nin different directions. And so that's why we need to\ndo calculus and have gradients.",
    "start": "262050",
    "end": "269910"
  },
  {
    "text": "And so this gives us the basic\nalgorithm of gradient descent.",
    "start": "269910",
    "end": "275330"
  },
  {
    "text": "And so under the\ngradient descent algorithm, what we're\ndoing is that we've",
    "start": "275330",
    "end": "282020"
  },
  {
    "text": "got our loss function J. We're\nworking out its gradient.",
    "start": "282020",
    "end": "289310"
  },
  {
    "text": "And then we're taking a little\nmultiplier of the gradient. So that alpha is our step\nsize or learning rate.",
    "start": "289310",
    "end": "297310"
  },
  {
    "text": "And that's normally alpha\nis a very small number, something like 10 to the\nminus 3 or 10 to the minus 4",
    "start": "297310",
    "end": "302960"
  },
  {
    "text": "or maybe even 10 to the minus 5. So we're taking a really\nlittle bit of the gradient, and then we're subtracting\nit from our parameters",
    "start": "302960",
    "end": "312580"
  },
  {
    "text": "to get new parameters. And as we do that, we\nwill walk downhill.",
    "start": "312580",
    "end": "317599"
  },
  {
    "text": "And the reason why we want\nto have a small learning rate is we don't want\nto walk too far.",
    "start": "317600",
    "end": "322610"
  },
  {
    "text": "So if from here, we worked\nout the gradient and said, it's in this direction, and\nwe just kept on walking.",
    "start": "322610",
    "end": "329390"
  },
  {
    "text": "We might end up way over here. Or if we had a\nreally big step size, we might even end up at a worse\npoint than we started with.",
    "start": "329390",
    "end": "337400"
  },
  {
    "text": "So we want to take little\nsteps to walk downhill. And so that's the very basic\ngradient descent algorithm.",
    "start": "337400",
    "end": "344690"
  },
  {
    "text": "Now the very basic gradient\ndescent algorithm we never use.",
    "start": "344690",
    "end": "350030"
  },
  {
    "text": "What we actually use\nis the next thing up, which is called stochastic\ngradient descent.",
    "start": "350030",
    "end": "356470"
  },
  {
    "text": "So the problem is for the basic\ngradient descent algorithm, we've worked out,\nfor an entire set",
    "start": "356470",
    "end": "365729"
  },
  {
    "text": "of data, what the\nobjective function is",
    "start": "365730",
    "end": "370890"
  },
  {
    "text": "and what the slope at the\npoint of evaluation is. And in general, we've got a\nlot of data in which we're",
    "start": "370890",
    "end": "381719"
  },
  {
    "text": "computing models. So simply trying to calculate\nour objective function over all",
    "start": "381720",
    "end": "388350"
  },
  {
    "text": "of our data for our model, the\ntraining data for the model, would take us a\nvery, very long time.",
    "start": "388350",
    "end": "395280"
  },
  {
    "text": "And so that's very\nexpensive to compute. And so we'd wait\na very long time",
    "start": "395280",
    "end": "400919"
  },
  {
    "text": "before we make even a single\nstep of gradient update. So for neural nets,\nwhat you're always doing",
    "start": "400920",
    "end": "407430"
  },
  {
    "text": "is using this variant that's\ncalled stochastic gradient descent. And so for stochastic\ngradient descent,",
    "start": "407430",
    "end": "413300"
  },
  {
    "text": "what that means is we pick a\nvery small subset of our data, like maybe we pick\n16 or 32 data items,",
    "start": "413300",
    "end": "421280"
  },
  {
    "text": "and we pretend that's\nall of our data, and we evaluate the function\nJ based on that small subset",
    "start": "421280",
    "end": "428500"
  },
  {
    "text": "and work out the gradient\nbased on that small subset. So it's a noisy, inaccurate\nestimate of the gradient.",
    "start": "428500",
    "end": "434990"
  },
  {
    "text": "And we use that to be the\ndirection in which we walk.",
    "start": "434990",
    "end": "440949"
  },
  {
    "text": "So that's normally\nreferred to also as having mini batches or\nmini batch gradient descent.",
    "start": "440950",
    "end": "448240"
  },
  {
    "text": "And in theory, working\nout the gradient based on this small subset\nis an approximation.",
    "start": "448240",
    "end": "458030"
  },
  {
    "text": "But one of the\ninteresting things in the way things have\nemerged in neural network land",
    "start": "458030",
    "end": "463510"
  },
  {
    "text": "is it turns out that neural\nnetworks actually often work better when you throw\nsome noise into the system",
    "start": "463510",
    "end": "469900"
  },
  {
    "text": "that having this\nnoise in the system gives you jiggle and\nmoves things around.",
    "start": "469900",
    "end": "474940"
  },
  {
    "text": "And so, actually,\nstochastic gradient descent not only is way,\nway, way faster,",
    "start": "474940",
    "end": "480880"
  },
  {
    "text": "but actually works better\nas a system for optimization of neural networks. ",
    "start": "480880",
    "end": "488180"
  },
  {
    "text": "So if you remember from\nlast time for word2vec, the idea was we\nstarted by just saying",
    "start": "488180",
    "end": "495540"
  },
  {
    "text": "each word has a random\nvector representing it. So we will literally sort of\njust get random small numbers",
    "start": "495540",
    "end": "504330"
  },
  {
    "text": "and fill up the vectors with\nthose random small numbers. There's an important\npoint there, which is you do have to\ninitialize your vectors",
    "start": "504330",
    "end": "512459"
  },
  {
    "text": "with random small numbers. If you just leave\nall the vectors as 0, then nothing works.",
    "start": "512460",
    "end": "519240"
  },
  {
    "text": "And that's because if\neverything starts off the same, you get these false\nsymmetries, which",
    "start": "519240",
    "end": "526890"
  },
  {
    "text": "means that you can't learn. So you always do want to be\ninitializing your vectors",
    "start": "526890",
    "end": "532240"
  },
  {
    "text": "with random numbers. And then we're going to\ngo through each position in the corpus. Using our estimates, we're\ngoing to try and predict",
    "start": "532240",
    "end": "540339"
  },
  {
    "text": "the probability of\nwords in the context as we talked about last time.",
    "start": "540340",
    "end": "545540"
  },
  {
    "text": "So that gives us an objective\nfunction from which we can then look at our errors,\nlook at our gradient,",
    "start": "545540",
    "end": "553240"
  },
  {
    "text": "and update the vectors so\nthat they learn to predict surrounding words better.",
    "start": "553240",
    "end": "558260"
  },
  {
    "text": "And so the incredible\nthing is that we can do no more than\nthat, and we end up",
    "start": "558260",
    "end": "564850"
  },
  {
    "text": "learning word vectors,\nwhich actually capture quite a lot of the semantics,\nthe meaning and relationships",
    "start": "564850",
    "end": "572680"
  },
  {
    "text": "between different words. So, when this was first\ndiscovered for these algorithms,",
    "start": "572680",
    "end": "581870"
  },
  {
    "text": "I mean, it really\nfeels like magic that you can just do this\nsimple math over a lot of text",
    "start": "581870",
    "end": "589890"
  },
  {
    "text": "and actually learn about\nthe meanings of words that it's just so surprising\nthat something so simple",
    "start": "589890",
    "end": "597000"
  },
  {
    "text": "could work. But as time has gone\non, this same recipe has then been for\nall kinds of learning",
    "start": "597000",
    "end": "603000"
  },
  {
    "text": "about the behavior of\nlanguage from neural networks.",
    "start": "603000",
    "end": "609060"
  },
  {
    "text": "So let's just go through\na sense of how that is. But before we do that,\nso let me just mention--",
    "start": "609060",
    "end": "618480"
  },
  {
    "text": "so for our word2vec algorithms,\nthe only parameters of the model",
    "start": "618480",
    "end": "623579"
  },
  {
    "text": "are these word vectors. They're the outside word vectors\nand the center word vectors,",
    "start": "623580",
    "end": "629020"
  },
  {
    "text": "which we actually\ntreat as disjoint, as I mentioned last time. And when we do the\ncomputations, we're",
    "start": "629020",
    "end": "635910"
  },
  {
    "text": "considering the dot product\nbetween the various possible outside words with\nour center word,",
    "start": "635910",
    "end": "644680"
  },
  {
    "text": "and we're using those to get\na probability distribution over how likely the model thinks\nthat different outside words",
    "start": "644680",
    "end": "652399"
  },
  {
    "text": "were. And then we're comparing that\nto the actual outside word in the context.",
    "start": "652400",
    "end": "657690"
  },
  {
    "text": "And that gives us\nour source of error. So as such, this\nis what's referred to in NLP as a bag\nof words model,",
    "start": "657690",
    "end": "665190"
  },
  {
    "text": "that it doesn't actually know\nabout the structure of sentences or even what's to the left\nand what's to the right.",
    "start": "665190",
    "end": "671550"
  },
  {
    "text": "It's predicting exactly the same\nprobabilities at each position to the left or right,\nbut it's wanting",
    "start": "671550",
    "end": "678200"
  },
  {
    "text": "to know about what\nkind of words appear in the context of\nthe center word.",
    "start": "678200",
    "end": "684440"
  },
  {
    "text": "So I just wanted to stop\nthis for a minute and--",
    "start": "684440",
    "end": "690410"
  },
  {
    "text": "let's see, not that one. ",
    "start": "690410",
    "end": "699620"
  },
  {
    "text": "So let's give you\nsome kind of a sense that this really does work.",
    "start": "699620",
    "end": "704760"
  },
  {
    "text": "So this is a little Jupyter\nNotebook that I've got for this. ",
    "start": "704760",
    "end": "714220"
  },
  {
    "text": "And here, I'm using a package\ngensim which we don't continue to use after that really.",
    "start": "714220",
    "end": "722040"
  },
  {
    "text": "But it's one package\nthat lets you load and play with word vectors.",
    "start": "722040",
    "end": "727210"
  },
  {
    "text": "And the word vectors\nI'm going to use here are glove word vectors.",
    "start": "727210",
    "end": "733270"
  },
  {
    "text": "And actually, glove was a\nmodel we built at Stanford. And I'm going to actually talk\nabout it a little bit later.",
    "start": "733270",
    "end": "740610"
  },
  {
    "text": "So strictly speaking,\nthese aren't exactly word for word vectors, but they\nbehave in exactly the same way.",
    "start": "740610",
    "end": "748370"
  },
  {
    "text": " So now it's loaded\nup my word vectors because the word vectors\nare a big data file.",
    "start": "748370",
    "end": "755860"
  },
  {
    "text": "And so as we've discussed, for\na word that the representation",
    "start": "755860",
    "end": "762120"
  },
  {
    "text": "of any word-- here's bread is just a\nvector of real numbers.",
    "start": "762120",
    "end": "768290"
  },
  {
    "text": "So I'm using 100\ndimensional word vectors to keep things quicker\nfor my class demo.",
    "start": "768290",
    "end": "775980"
  },
  {
    "text": "So this is the word bread. And then I can say, well,\nwhat's the representation",
    "start": "775980",
    "end": "781640"
  },
  {
    "text": "for croissant. And this is croissant.",
    "start": "781640",
    "end": "788220"
  },
  {
    "text": "And we can get a visual sense\nof they're at least a little bit similar. So the first components\nare both negative,",
    "start": "788220",
    "end": "796470"
  },
  {
    "text": "the second components\nare both positive, the third components are\nboth negative and large,",
    "start": "796470",
    "end": "802110"
  },
  {
    "text": "the fourth components\nare both positive. They seem like they're\nkind of similar vectors.",
    "start": "802110",
    "end": "807990"
  },
  {
    "text": "So that seems kind of\nhopeful because that means that it knows that\nbread and croissant are",
    "start": "807990",
    "end": "813950"
  },
  {
    "text": "a bit similar to each other. This package has a\nnice simple function",
    "start": "813950",
    "end": "820400"
  },
  {
    "text": "where rather than\ndoing that by hand, you can just ask it about\nall the word vectors",
    "start": "820400",
    "end": "825630"
  },
  {
    "text": "and say which ones\nare most similar. So I can ask it what\nwords in its vocabulary",
    "start": "825630",
    "end": "832440"
  },
  {
    "text": "are most similar to USA. And in this model,\neverything's been lowercased, I should mention.",
    "start": "832440",
    "end": "838650"
  },
  {
    "text": "And so if I do that, it\nhas Canada, America, U.S.A, then United States, Australia.",
    "start": "838650",
    "end": "845710"
  },
  {
    "text": "Well, those seem a\nfairly reasonable list of most similar\nwords, though you might think it's a little\nstrange that Canada wins out",
    "start": "845710",
    "end": "853170"
  },
  {
    "text": "over the USA with dots over it.  Similarly, I can ask it\nwhat's most similar to banana.",
    "start": "853170",
    "end": "860769"
  },
  {
    "text": "And I get coconut, mango,\nbananas, potato, pineapple, fruit, et cetera. Again, pretty sensible.",
    "start": "860770",
    "end": "866910"
  },
  {
    "text": "A little bit of a bias\nto more tropical fruits. Or I can go to croissant and\nask what's the most similar to",
    "start": "866910",
    "end": "873600"
  },
  {
    "text": "croissant. The most similar things\nto croissant isn't bread, but it's things like\nbrioche, baguette, focaccia,",
    "start": "873600",
    "end": "880020"
  },
  {
    "text": "which sort of basically makes\nsense though he is putting here.",
    "start": "880020",
    "end": "885140"
  },
  {
    "text": "And I've got-- wait,\nI'd already done-- oh, sorry. Yeah, I remember what this is.",
    "start": "885140",
    "end": "890160"
  },
  {
    "text": "So with this most similar,\nyou've got a positive word vector and you're saying,\nwhat other words are most",
    "start": "890160",
    "end": "897650"
  },
  {
    "text": "similar in position to that. There's something\nelse you can do, which you can say\nis this is let me",
    "start": "897650",
    "end": "904250"
  },
  {
    "text": "take the negative\nof that word vector and say what's most similar\nto the negative of it.",
    "start": "904250",
    "end": "910260"
  },
  {
    "text": "And you could\npossibly think, oh, that might be useful to find\nantonyms or something like that.",
    "start": "910260",
    "end": "916259"
  },
  {
    "text": "I mean, the truth is, it isn't. If you ask for the\nthings that are most similar to the negative\nof the banana vector--",
    "start": "916260",
    "end": "923990"
  },
  {
    "text": "and in most other\nvectors, it's the same-- you get off out\nthese weirdo things",
    "start": "923990",
    "end": "929240"
  },
  {
    "text": "that you're not really sure\nif they're words at all, or maybe they are in\nsome other language. Or some of them are names--",
    "start": "929240",
    "end": "935300"
  },
  {
    "text": "like Shunichi is\na Japanese name-- but not very useful stuff.",
    "start": "935300",
    "end": "940769"
  },
  {
    "text": "They don't really feel\nlike a negative of banana. But it turns out that\nfrom there, we get",
    "start": "940770",
    "end": "948180"
  },
  {
    "text": "to this powerful ability of-- there was observed\nfor word2vec, which",
    "start": "948180",
    "end": "954390"
  },
  {
    "text": "is that we could isolate\nsemantic components and then put them together\nin interesting ways.",
    "start": "954390",
    "end": "962710"
  },
  {
    "text": "So looking at this\npicture, what we could do is start with a positive\nvector for King,",
    "start": "962710",
    "end": "969780"
  },
  {
    "text": "from the origin\nup the King, then we could use the negation to say\nsubtract out the vector for man.",
    "start": "969780",
    "end": "978070"
  },
  {
    "text": "And then we could have\nanother positive vector of add on the vector for woman.",
    "start": "978070",
    "end": "983649"
  },
  {
    "text": "And then we can ask\nthe model is if you're over here in the space,\nwhat is the nearest",
    "start": "983650",
    "end": "989880"
  },
  {
    "text": "word to you over there. And so that's what\nthis next thing does.",
    "start": "989880",
    "end": "996730"
  },
  {
    "text": "It sort of says, positive vector\nfor King, negative for man, also positive for Queen,\nwhere does that get you to.",
    "start": "996730",
    "end": "1004910"
  },
  {
    "text": "And that gets you to Queen. Yay! And so this was the\nmost celebrated property",
    "start": "1004910",
    "end": "1012280"
  },
  {
    "text": "that was discovered\nwith these word vectors that they weren't only good\nfor meaning similarity,",
    "start": "1012280",
    "end": "1018470"
  },
  {
    "text": "but that they were good for\ndoing these kind of meaning",
    "start": "1018470",
    "end": "1023680"
  },
  {
    "text": "components. And these got referred\nto as analogies because you can think of them\nas A is B is C is to what.",
    "start": "1023680",
    "end": "1032030"
  },
  {
    "text": "So it's sort of like\nwoman is to King-- no, sorry. Man is to King or\nKing is to man as--",
    "start": "1032030",
    "end": "1041338"
  },
  {
    "text": "I'm saying this the\nwrong way around. Man is to King as woman is\nto what in the analogies.",
    "start": "1041338",
    "end": "1047569"
  },
  {
    "text": "And so here, I've\ndefined a little function that is now saying this little\nfunction just automates that",
    "start": "1047569",
    "end": "1054250"
  },
  {
    "text": "and will compute analogies. And so now, I can ask it in\njust this analogy format,",
    "start": "1054250",
    "end": "1061800"
  },
  {
    "text": "man is to King as\nwoman is to Queen. And that one was the\ncanonical example.",
    "start": "1061800",
    "end": "1069490"
  },
  {
    "text": "But you can actually sort\nof have fun with this. And I mean, this is pretty\nold fashioned stuff.",
    "start": "1069490",
    "end": "1079662"
  },
  {
    "text": "I feel like I'm, maybe like\nnow at this point, an old guy talking about how\nmuch fun we used",
    "start": "1079662",
    "end": "1086100"
  },
  {
    "text": "to have sitting around the\nradio, listening to radio plays because basically no\none uses this stuff anymore.",
    "start": "1086100",
    "end": "1093880"
  },
  {
    "text": "And there are much, much better\nand fancier things like ChatGPT. But back in the day,\nwhen I was younger,",
    "start": "1093880",
    "end": "1102840"
  },
  {
    "text": "it was really stunning\nalready just how this very simple model\nbuilt on very simple data",
    "start": "1102840",
    "end": "1110340"
  },
  {
    "text": "could just have quite good\nsemantic understanding and do quite good analogies.",
    "start": "1110340",
    "end": "1117309"
  },
  {
    "text": "So you can actually play\nwith this quite a bit and have a bit of fun. So you can do something like\nanalogy, Australia comma beer",
    "start": "1117310",
    "end": "1129046"
  },
  {
    "text": "France.  What do people think\nthe answer will be?",
    "start": "1129046",
    "end": "1134320"
  },
  {
    "text": "Wine. Close. The answer gives us\nchampagne, but that seems a pretty good answer.",
    "start": "1134320",
    "end": "1140590"
  },
  {
    "text": "I could then put in Russia. What do people think-- Vodka!",
    "start": "1140590",
    "end": "1145840"
  },
  {
    "text": "Yeah, it'll see. It can get back vodka. This is actually works\nkind of interestingly.",
    "start": "1145840",
    "end": "1154140"
  },
  {
    "text": "I could do a different one. I can test something different. I can do something\nlike pencil is",
    "start": "1154140",
    "end": "1161770"
  },
  {
    "text": "to sketching as camera is to--",
    "start": "1161770",
    "end": "1169053"
  },
  {
    "text": " photographing. Yeah, that works quite well.",
    "start": "1169053",
    "end": "1175380"
  },
  {
    "text": "So we built this model in 2014.",
    "start": "1175380",
    "end": "1180640"
  },
  {
    "text": "So it's a little bit\nout of date in politics. So we can't do the last\ndecade of politics,",
    "start": "1180640",
    "end": "1188230"
  },
  {
    "text": "which is maybe unfortunate. But we could try out\nolder politics questions.",
    "start": "1188230",
    "end": "1193660"
  },
  {
    "text": "So we could try Obama is to\nClinton as Reagan is to--",
    "start": "1193660",
    "end": "1207540"
  },
  {
    "text": "if you remember your\nUS World history class, any guesses what\nit's going to say?",
    "start": "1207540",
    "end": "1214080"
  },
  {
    "text": "There's a Bush one. Any other ideas? Some people have different\nopinions of Bill Clinton?",
    "start": "1214080",
    "end": "1220965"
  },
  {
    "text": "Any--  What it answers\nis Nixon, which I",
    "start": "1220965",
    "end": "1226980"
  },
  {
    "text": "think is actually kind of fair. ",
    "start": "1226980",
    "end": "1232930"
  },
  {
    "text": "You can also get it to do some\njust sort of language syntactic",
    "start": "1232930",
    "end": "1238630"
  },
  {
    "text": "facts. So you can do something like\ntall is to tallest as long--",
    "start": "1238630",
    "end": "1247300"
  },
  {
    "text": "oops-- as long is\nto-- this one's easy.",
    "start": "1247300",
    "end": "1255190"
  },
  {
    "text": "Yeah, so with this simple\nmethod of learning, with this simple\nbag of words model,",
    "start": "1255190",
    "end": "1262880"
  },
  {
    "text": "it's enough to learn a lot about\nthe semantics of words and stuff",
    "start": "1262880",
    "end": "1269650"
  },
  {
    "text": "that's beyond\nconventional semantics like our examples\nwith Australia is",
    "start": "1269650",
    "end": "1274930"
  },
  {
    "text": "to beer as Russia is to vodka. I mean, that's sort of\ncultural world knowledge, which goes a little bit beyond\nwhat people normally",
    "start": "1274930",
    "end": "1282820"
  },
  {
    "text": "think of as word\nmeaning semantics, but it's also in there. Yes. If you perhaps subtract\nthe distance from,",
    "start": "1282820",
    "end": "1289789"
  },
  {
    "text": "let's say like man and\nKing, does that also capture a concept of\nrelationship between two words? Would that give you\nback ruler or something",
    "start": "1289790",
    "end": "1297060"
  },
  {
    "text": "like that, where taking the\ndistance-- the difference between two vectors does\ncapture some concept [INAUDIBLE]",
    "start": "1297060",
    "end": "1303990"
  },
  {
    "text": "The distance between man-- so man compared to King\nshould be a ruler concept.",
    "start": "1303990",
    "end": "1311590"
  },
  {
    "text": "But isn't that what I'm using? Because then I'm taking\nthe distance between man",
    "start": "1311590",
    "end": "1318809"
  },
  {
    "text": "and King is what I'm adding on\nto woman to get to Queen, right?",
    "start": "1318810",
    "end": "1325030"
  },
  {
    "text": "[INAUDIBLE] Yeah. So if you think of\nthese words, depending",
    "start": "1325030",
    "end": "1331200"
  },
  {
    "text": "on which thing you\nthink of as the analogy, you can think of it-- you've\nboth got a vector, a difference",
    "start": "1331200",
    "end": "1341100"
  },
  {
    "text": "vector between words that gives\nyou a gender analogy and one that gives you a ruler analogy.",
    "start": "1341100",
    "end": "1347740"
  },
  {
    "text": "Yeah, absolutely. Any other questions?",
    "start": "1347740",
    "end": "1352760"
  },
  {
    "text": "Yeah. In the word2vec algorithm,\nwe get two vectors",
    "start": "1352760",
    "end": "1358700"
  },
  {
    "text": "for each word, a U and a V. But\nhere, you only have one vector. So how do you go\nfrom two to one.",
    "start": "1358700",
    "end": "1366630"
  },
  {
    "text": "Yeah, good question. I mean, the commonest\nway in practice was you just average\nthe two of them.",
    "start": "1366630",
    "end": "1372049"
  },
  {
    "text": "And really, you find out\nthat they end up very close,",
    "start": "1372050",
    "end": "1380673"
  },
  {
    "text": "because if you think\nof it, since you're going along every\nposition of the text, you're both going to be the case\nwhere if the text is the octopus",
    "start": "1380673",
    "end": "1389780"
  },
  {
    "text": "has legs-- you're going to have octopus\nin the center with legs in the context, and a\ncouple of time steps later,",
    "start": "1389780",
    "end": "1395910"
  },
  {
    "text": "it's going to be legs in\nthe center with octopus in the context. So although they vary a\nbit for all the regions",
    "start": "1395910",
    "end": "1402230"
  },
  {
    "text": "of the neural nets\nvary, basically they end up very similar and people\nnormally just average them. Yeah.",
    "start": "1402230",
    "end": "1407810"
  },
  {
    "text": "Can you nest this process-- So use this-- the\nanswer of one to then",
    "start": "1407810",
    "end": "1413700"
  },
  {
    "text": "be placed into the analogy\nfunction of another, and see how far away you can go\nbefore it starts to break down?",
    "start": "1413700",
    "end": "1421200"
  },
  {
    "text": "I think you can. So you're wanting to--",
    "start": "1421200",
    "end": "1426299"
  },
  {
    "text": "How distant a relation\nbetween two words can you do before\nit starts providing incorrect relationships\nbetween words?",
    "start": "1426300",
    "end": "1433990"
  },
  {
    "text": " Are you wanting to make two\nsteps from somewhere or--",
    "start": "1433990",
    "end": "1440430"
  },
  {
    "text": "Yeah, two steps or three steps\nor how far-- how many steps can you go away from King\nbefore it starts failing?",
    "start": "1440430",
    "end": "1450360"
  },
  {
    "text": "So it doesn't always work. I mean, there are certainly\nexamples that are fail. I'm sort of shy to try\nthat now because I don't",
    "start": "1450360",
    "end": "1458280"
  },
  {
    "text": "have a predefined\nfunction that did it, and that might take me too long. But you could play\nwith it at home",
    "start": "1458280",
    "end": "1463980"
  },
  {
    "text": "and see how it works for you.  Curious, as the\nqualification, why",
    "start": "1463980",
    "end": "1471650"
  },
  {
    "text": "is it that we use two separate\nsets of vectors for word2vec? Is it just to get\nmore parameters, or is there [INAUDIBLE]?",
    "start": "1471650",
    "end": "1478220"
  },
  {
    "text": "I'll get back to that. Maybe I should go\non at this point. Let me move on and kind just\nget through some more details",
    "start": "1478220",
    "end": "1487070"
  },
  {
    "text": "of the word2vec algorithm. So just a technical\npoint on this class",
    "start": "1487070",
    "end": "1494340"
  },
  {
    "text": "so you don't make any big\nmistakes and waste your weekend. I mean, for most instances of\n224N we've actually had people",
    "start": "1494340",
    "end": "1502910"
  },
  {
    "text": "implement from scratch\nword2vec as assignment 2. But for this quarter, doing\nit in spring quarter--",
    "start": "1502910",
    "end": "1512130"
  },
  {
    "text": "as you probably know,\nspring quarter is actually a little shorter than\nthe other two quarters-- we decided to skip having\npeople implement word2vec.",
    "start": "1512130",
    "end": "1521000"
  },
  {
    "text": "So don't look at\nthe old assignment 2 that says, implement\nword2vec, or else you'll be spending your time.",
    "start": "1521000",
    "end": "1526980"
  },
  {
    "text": "Wait for the newest\nassignment 2 to come out. But despite that, let me\njust say a little bit more",
    "start": "1526980",
    "end": "1535500"
  },
  {
    "text": "about some of the details. So why two vectors? So the two vectors\nis it just makes",
    "start": "1535500",
    "end": "1543659"
  },
  {
    "text": "the math a little bit easier. So if you think about the math,\nif you have the same vectors",
    "start": "1543660",
    "end": "1551910"
  },
  {
    "text": "for the center word and\nfor the outside words, well, for whatever\nthe center word is,",
    "start": "1551910",
    "end": "1558070"
  },
  {
    "text": "let's say it's octopus, that\nwhen you're going through the trying out every\npossible context",
    "start": "1558070",
    "end": "1565140"
  },
  {
    "text": "word for the normalization,\nat some point, you'll hit octopus again. And so at that point, you'll\nhave a quadratic term.",
    "start": "1565140",
    "end": "1573300"
  },
  {
    "text": "You'll have the x squared\nof the octopus vector. And that kind of messes up--",
    "start": "1573300",
    "end": "1578430"
  },
  {
    "text": "I mean you're clever people\ncould work out the math of it, but it makes the\nmath more of a mess,",
    "start": "1578430",
    "end": "1584763"
  },
  {
    "text": "because every other\nterm, it's something different and it's just like ax. And then at one position,\nyou've got an x squared.",
    "start": "1584763",
    "end": "1591920"
  },
  {
    "text": "So it just makes\nthe math messier. And so they kept it really\nsimple by just having",
    "start": "1591920",
    "end": "1597860"
  },
  {
    "text": "them be disjoint vectors. But it doesn't make it better. I mean, it actually turns out\nit works a fraction better",
    "start": "1597860",
    "end": "1604970"
  },
  {
    "text": "if you do it right. But in practice, people\nhave usually just estimated",
    "start": "1604970",
    "end": "1611450"
  },
  {
    "text": "them separately and then\naverage them at the end. If you actually look at the\npaper, here's Mikolov et Al.",
    "start": "1611450",
    "end": "1618810"
  },
  {
    "text": "You can find the 2013 paper. There's actually sort\nof a family of methods",
    "start": "1618810",
    "end": "1624980"
  },
  {
    "text": "that they describe. So they describe\ntwo methods, one of which was that you have an\ninside word that's predicting",
    "start": "1624980",
    "end": "1634400"
  },
  {
    "text": "the words around it, and\nthen the other one tried to predict the\ncenter word from all",
    "start": "1634400",
    "end": "1639530"
  },
  {
    "text": "the words in the\ncontext, which was called continuous bag\nof words in their paper. The one that I've\ndescribed as skip-grams,",
    "start": "1639530",
    "end": "1646640"
  },
  {
    "text": "which is simpler and\nworks just great. But then the other part\nof it is for working out",
    "start": "1646640",
    "end": "1656680"
  },
  {
    "text": "what loss function to\nbe used for training. And what I've presented\nso far is naive softmax,",
    "start": "1656680",
    "end": "1666170"
  },
  {
    "text": "where we just consider every\npossible choice of a context word and just run all the math.",
    "start": "1666170",
    "end": "1672760"
  },
  {
    "text": "That's totally doable. And with our modern\nsuper fast computers, it's not even that\nunreasonable to do.",
    "start": "1672760",
    "end": "1679340"
  },
  {
    "text": "We do things like\nthis all the time. But at least at the time\nthat they wrote their paper,",
    "start": "1679340",
    "end": "1685070"
  },
  {
    "text": "this seemed kind of expensive. And they considered\nother alternatives like a hierarchical\nsoftmax, which I'm not",
    "start": "1685070",
    "end": "1691780"
  },
  {
    "text": "going to explain right\nnow, but I do just want to explain\nnegative sampling.",
    "start": "1691780",
    "end": "1698110"
  },
  {
    "text": "So this is just to see a bit of\na different way of doing things. So for what we did\nlast time, we had",
    "start": "1698110",
    "end": "1705780"
  },
  {
    "text": "this sort of straightforward\nsoftmax equation. And so in the\ndenominator, you're",
    "start": "1705780",
    "end": "1711540"
  },
  {
    "text": "summing over every\nword in the vocabulary. And so if you might have 400,000\nwords in your vocabulary--",
    "start": "1711540",
    "end": "1719020"
  },
  {
    "text": "a lot of words in\nhuman languages-- that's kind of a big\nsum, especially when",
    "start": "1719020",
    "end": "1725010"
  },
  {
    "text": "for each element\nof the sum, you're taking a dot product between\n100 dimensional or 300",
    "start": "1725010",
    "end": "1730950"
  },
  {
    "text": "dimensional vectors and\nthen exponentiating it. A lot of math going\non somewhere in there.",
    "start": "1730950",
    "end": "1737130"
  },
  {
    "text": "So maybe we could\nshort circuit that. And so the idea of\nthe negative sampling",
    "start": "1737130",
    "end": "1744120"
  },
  {
    "text": "was to say, well, rather\nthan evaluating it for every single possible word,\nmaybe we could just sort of",
    "start": "1744120",
    "end": "1752279"
  },
  {
    "text": "train some simple\nlogistic regressions where they're going to say,\nyou should like the true word",
    "start": "1752280",
    "end": "1760270"
  },
  {
    "text": "that's in the context. And if we randomly\npick a few other words, you shouldn't like\nthem very much.",
    "start": "1760270",
    "end": "1767230"
  },
  {
    "text": "And that's skip-gram\nnegative sampling. So that's what this looks\nlike as an equation.",
    "start": "1767230",
    "end": "1773529"
  },
  {
    "text": "So we've got our center word\nand our actual context word.",
    "start": "1773530",
    "end": "1779120"
  },
  {
    "text": "And we're saying,\nwell, let's work out the term for the\nactual center word.",
    "start": "1779120",
    "end": "1786410"
  },
  {
    "text": "We'd like this to\nbe high probability.",
    "start": "1786410",
    "end": "1792070"
  },
  {
    "text": "So since we're\nminimizing, we're going to negate that and\nhave it go down. And then we're going to\nsample some other words,",
    "start": "1792070",
    "end": "1798919"
  },
  {
    "text": "and we'd like this\nto be the opposite. But the other thing\nthat we've changed here",
    "start": "1798920",
    "end": "1803950"
  },
  {
    "text": "is now we're not using\nthe softmax anymore. We're using this\nsigma which stands",
    "start": "1803950",
    "end": "1810640"
  },
  {
    "text": "for the logistic function, which\nis often called the sigmoid. Sigmoid just means s-shaped.",
    "start": "1810640",
    "end": "1817010"
  },
  {
    "text": "But you could actually have an\ninfinity of s-shaped functions. And the one that we actually\nuse is the logistic function.",
    "start": "1817010",
    "end": "1824759"
  },
  {
    "text": "So the logistic function\nhas this form and maps from any real number to a\nprobability between 0 and 1.",
    "start": "1824760",
    "end": "1835550"
  },
  {
    "text": "So what we're wanting\nto say at that point is for the real\noutside world, we're",
    "start": "1835550",
    "end": "1842990"
  },
  {
    "text": "hoping that this\ndot product is large so its probability is near 1.",
    "start": "1842990",
    "end": "1848690"
  },
  {
    "text": "And so that will then help\nwith the minimization. And for the other words,\nwe'd like their probability",
    "start": "1848690",
    "end": "1856520"
  },
  {
    "text": "to be small. So we'd like them to\nappear sort of over here.",
    "start": "1856520",
    "end": "1863840"
  },
  {
    "text": "And that's what\nthis is calculating. But as written, it's sort\nof sticking the minus sign on the inside there, which\nworks because this is symmetric.",
    "start": "1863840",
    "end": "1873390"
  },
  {
    "text": "So you're wanting\nto be over here, which means that\nif you negate it,",
    "start": "1873390",
    "end": "1878450"
  },
  {
    "text": "you'll be on this side,\nwhich will be large. ",
    "start": "1878450",
    "end": "1888160"
  },
  {
    "text": "And so then the final bit of\nthis, which is the asterisk, is-- so we're going to\npick a few words.",
    "start": "1888160",
    "end": "1894789"
  },
  {
    "text": "It might only be five or 10\nthat are our negative samples. But for picking those words,\nwhat works well is not just",
    "start": "1894790",
    "end": "1904990"
  },
  {
    "text": "to pick sort of randomly\nuniformly from all the 400,000",
    "start": "1904990",
    "end": "1910360"
  },
  {
    "text": "words in our vocab. What you basically want to\ndo is be paying attention",
    "start": "1910360",
    "end": "1915910"
  },
  {
    "text": "to how common the words are. So something like that\nis a really common word. And so we refer to that as\nthe unigram distribution.",
    "start": "1915910",
    "end": "1924028"
  },
  {
    "text": "That means you're\nsort of just taking individual words independently\nhow commonly they are.",
    "start": "1924028",
    "end": "1929149"
  },
  {
    "text": "So about 10% of the time,\nyou'd be choosing the. But so that's sort\nof roughly what",
    "start": "1929150",
    "end": "1935410"
  },
  {
    "text": "you want to do for\nsampling, but people have found that you can actually\ndo even a bit better than that.",
    "start": "1935410",
    "end": "1941200"
  },
  {
    "text": "So the standard thing that\nthey presented for word2vec is you're taking the unigram\nprobability of the word",
    "start": "1941200",
    "end": "1948780"
  },
  {
    "text": "and raising it to the power 3/4. What does that end up doing--",
    "start": "1948780",
    "end": "1956669"
  },
  {
    "text": "question for the audience-- if I take probabilities\nand raise them to the 3/4?",
    "start": "1956670",
    "end": "1963600"
  },
  {
    "text": "[INAUDIBLE] less frequent words\njust become sampling more.",
    "start": "1963600",
    "end": "1969000"
  },
  {
    "text": "Correct. Yeah, so raising it to the\n3/4 means that you're sort",
    "start": "1969000",
    "end": "1975540"
  },
  {
    "text": "of somewhat upping the\nprobability of the less frequent",
    "start": "1975540",
    "end": "1980730"
  },
  {
    "text": "word. So you're sort of in between\nhaving every word uniform",
    "start": "1980730",
    "end": "1986970"
  },
  {
    "text": "and exactly using their relative\nfrequencies in the text. You're sort of moving a little\nbit in the direction of uniform.",
    "start": "1986970",
    "end": "1994210"
  },
  {
    "text": "And so you get better results by\ngoing somewhat in the distance of sampling more uniformly, but\nyou don't want to go all the way",
    "start": "1994210",
    "end": "2002670"
  },
  {
    "text": "there, which would\ncorrespond to, I guess, putting a 0 in there\nrather than a 3/4.",
    "start": "2002670",
    "end": "2008673"
  },
  {
    "start": "2008673",
    "end": "2016140"
  },
  {
    "text": "Let's see. I had an aside here,\nbut time rushes along. So let's not bother\nwith this side.",
    "start": "2016140",
    "end": "2022150"
  },
  {
    "text": "It's not that important. So that's the word2vec\nalgorithm that we've seen",
    "start": "2022150",
    "end": "2028950"
  },
  {
    "text": "all of in its different forms.",
    "start": "2028950",
    "end": "2034380"
  },
  {
    "text": "A reasonable wonder that\nyou could have at this point is, this seems a kind\nof a weird way of doing",
    "start": "2034380",
    "end": "2045330"
  },
  {
    "text": "what we're wanting to do. The idea is, look, we have\nthis text, we have words,",
    "start": "2045330",
    "end": "2052419"
  },
  {
    "text": "and we have words in\nthe context of words. It sort of seems like an obvious\nthing to do would be to say,",
    "start": "2052420",
    "end": "2059460"
  },
  {
    "text": "let's just count\nsome statistics. We have words and\nthere are other words that occur in their context.",
    "start": "2059460",
    "end": "2065190"
  },
  {
    "text": "So let's just see how\noften the word swim occurs next to octopus, and how\noften the word fish occurs next",
    "start": "2065190",
    "end": "2072739"
  },
  {
    "text": "to octopus. Let's get some counts and\nsee how often words occur",
    "start": "2072739",
    "end": "2078199"
  },
  {
    "text": "in the context of other words. And maybe we could\nuse that to calculate",
    "start": "2078199",
    "end": "2083719"
  },
  {
    "text": "some form of word vectors. And so that's something\nthat people have already",
    "start": "2083719",
    "end": "2089719"
  },
  {
    "text": "also considered. So if we use the same kind\nof idea of a context window,",
    "start": "2089719",
    "end": "2095040"
  },
  {
    "text": "we could just make a matrix\nof how often words occur in the context of other words.",
    "start": "2095040",
    "end": "2100500"
  },
  {
    "text": "And so here's a baby example. My corpus is I like deep\nlearning, I like NLP,",
    "start": "2100500",
    "end": "2106530"
  },
  {
    "text": "I enjoy flying. And my context window I'm\nusing is just one word",
    "start": "2106530",
    "end": "2111900"
  },
  {
    "text": "to the left and the right. And then I can make this kind\nof co-occurrence count matrix",
    "start": "2111900",
    "end": "2118920"
  },
  {
    "text": "where I'm putting in the\ncounts of different words in every context. And because my corpus is so\nsmall, everything in the matrix",
    "start": "2118920",
    "end": "2129210"
  },
  {
    "text": "is a 0 or 1, except for right\nhere where I've got the twos, because I have I like twice.",
    "start": "2129210",
    "end": "2134680"
  },
  {
    "text": "But in principle,\nI've got a matrix of counts for all the\ndifferent counts here.",
    "start": "2134680",
    "end": "2140700"
  },
  {
    "text": "So maybe this gives\nme a word vector.",
    "start": "2140700",
    "end": "2146160"
  },
  {
    "text": "Here's a word vector for deep. It's this long vector here. And I could just say\nthat is my word vector.",
    "start": "2146160",
    "end": "2153819"
  },
  {
    "text": "And indeed, sometimes\npeople have done that, but they're kind of ungainly\nword vectors because if we have",
    "start": "2153820",
    "end": "2161790"
  },
  {
    "text": "400,000 words in our vocabulary,\nthe size of this matrix is 400,000 by 400,000, which is a\nlot worse than our word vectors,",
    "start": "2161790",
    "end": "2172540"
  },
  {
    "text": "because if we're making\nthem only 100 dimensional and we've only got\n400,000 by 100,",
    "start": "2172540",
    "end": "2178720"
  },
  {
    "text": "which is still a big number, but\nit's a lot smaller than 400,000 times 400,000.",
    "start": "2178720",
    "end": "2183940"
  },
  {
    "text": "So that's inconvenient. So when people have\nstarted with these kind",
    "start": "2183940",
    "end": "2188970"
  },
  {
    "text": "of co-occurrence matrix,\nthe general thing that people have\ndone is to say, well,",
    "start": "2188970",
    "end": "2195430"
  },
  {
    "text": "somehow we want to reduce the\ndimensionality of that matrix so that we have a smaller\nmatrix to deal with.",
    "start": "2195430",
    "end": "2204690"
  },
  {
    "text": "And so then how can we\nreduce the dimensionality of the matrix? And at this point, if you\nremember your linear algebra",
    "start": "2204690",
    "end": "2213150"
  },
  {
    "text": "and stuff like that, you should\nbe thinking of things like PCA. And in particular,\nif you want it",
    "start": "2213150",
    "end": "2218520"
  },
  {
    "text": "to work for any\nmatrix of any shape, there's the singular\nvalue decomposition.",
    "start": "2218520",
    "end": "2224440"
  },
  {
    "text": "So there's the classic\nsingular value decomposition for any matrix,\nyou can rewrite it",
    "start": "2224440",
    "end": "2230860"
  },
  {
    "text": "as a product of three\nmatrices, a U and a V which are both\northonormal, which",
    "start": "2230860",
    "end": "2237850"
  },
  {
    "text": "means that you get these\nindependent vectors,",
    "start": "2237850",
    "end": "2246550"
  },
  {
    "text": "they're orthogonal\nto each other. And then in the middle, we\nhave the singular vectors,",
    "start": "2246550",
    "end": "2254150"
  },
  {
    "text": "which are ordered in size. This is the most\nimportant singular vector. And these are sort\nof waiting terms",
    "start": "2254150",
    "end": "2260350"
  },
  {
    "text": "on the different number of\nthe different dimensions. And so this is of the\nfull SVD decomposition.",
    "start": "2260350",
    "end": "2269349"
  },
  {
    "text": "But part of it is\nirrelevant because if I've got this picture, nothing is\nhappening in the part that's",
    "start": "2269350",
    "end": "2276580"
  },
  {
    "text": "shown in yellow there. But if you want-- at the moment, this is\njust a full decomposition.",
    "start": "2276580",
    "end": "2287480"
  },
  {
    "text": "But if we're wanting to have\nsmaller low dimensional vectors, well, the next\ntrick we pull is we",
    "start": "2287480",
    "end": "2293720"
  },
  {
    "text": "say, well, we know where the\nsmallest singular vectors are. So we could just\nset them to zero.",
    "start": "2293720",
    "end": "2300000"
  },
  {
    "text": "And if we did that, then\nmore of this goes away, and we end up with two\ndimensional representations",
    "start": "2300000",
    "end": "2307880"
  },
  {
    "text": "of our words. And so that gives us another way\nof forming low dimensional word",
    "start": "2307880",
    "end": "2315829"
  },
  {
    "text": "representations. And this had actually\nbeen explored before modern\nneural word vectors",
    "start": "2315830",
    "end": "2323250"
  },
  {
    "text": "and using algorithms such\nas latent semantic analysis. And it is sort of half worked,\nbut it never worked very well.",
    "start": "2323250",
    "end": "2334590"
  },
  {
    "text": "But some people,\nespecially in psychology, had kept on working on it.",
    "start": "2334590",
    "end": "2339830"
  },
  {
    "text": "And among other people\nin the early 2000, there was this grad\nstudent, Doug Rohde,",
    "start": "2339830",
    "end": "2346760"
  },
  {
    "text": "who kept on working on it, and\nhe came up with an algorithm that he called COALS.",
    "start": "2346760",
    "end": "2354170"
  },
  {
    "text": "And he'd known as\nother people before him had known that just sort of\ndoing an SVD on raw count",
    "start": "2354170",
    "end": "2362500"
  },
  {
    "text": "didn't seem to give you word\nvectors that worked very well. But he had some ideas\nto do better than that.",
    "start": "2362500",
    "end": "2369160"
  },
  {
    "text": "So one thing that helps a lot\nis if you log the frequencies. So you can put log\nfrequencies in the cells.",
    "start": "2369160",
    "end": "2377500"
  },
  {
    "text": "But then he sort of used some\nother ideas, some of which were also picked up in\nword2vec, one of which",
    "start": "2377500",
    "end": "2385030"
  },
  {
    "text": "is ramping the windows so that\nyou count closer words more than further away words.",
    "start": "2385030",
    "end": "2391240"
  },
  {
    "text": "He used Pearson correlations\ninstead of counts, et cetera, but he ended up coming up with a\nlow dimensional version of word",
    "start": "2391240",
    "end": "2399520"
  },
  {
    "text": "vectors that are\nsort of ultimately still based on an SVD.",
    "start": "2399520",
    "end": "2404560"
  },
  {
    "text": "And he got out\nthese word vectors. And interestingly, sort of no\none really noticed at the time.",
    "start": "2404560",
    "end": "2411930"
  },
  {
    "text": "But Doug Rohde in\nhis dissertation effectively discovered\nthis same property",
    "start": "2411930",
    "end": "2417890"
  },
  {
    "text": "of having linear\nsemantic components. So look, here we go. Here's one of them. So this is actually a picture\nfrom his dissertation.",
    "start": "2417890",
    "end": "2425880"
  },
  {
    "text": "And look here, we've got\nthis meaning component, which is doer of an event.",
    "start": "2425880",
    "end": "2430980"
  },
  {
    "text": "And he's essentially shown with\nthe way he's processed his word vectors, that the\ndoer of an event",
    "start": "2430980",
    "end": "2437420"
  },
  {
    "text": "is a linear meaning\ncomponent that you can use to move between a\nverb and the doer of the verb.",
    "start": "2437420",
    "end": "2444599"
  },
  {
    "text": "Kind of cool. But he didn't become\nfamous because no one was paying attention to\nwhat he had come up with.",
    "start": "2444600",
    "end": "2451920"
  },
  {
    "text": "So once word2vec\nbecame popular, that was something that I was\nkind of interested in.",
    "start": "2451920",
    "end": "2461010"
  },
  {
    "text": "And so working together with\na postdoc, Jeffrey Pennington,",
    "start": "2461010",
    "end": "2466480"
  },
  {
    "text": "we thought that\nthere was interest in this space of having doing\nthings with matrices of counts,",
    "start": "2466480",
    "end": "2476359"
  },
  {
    "text": "and how do you then\nget them to work well as word vectors in the same\nway that word2vec worked well",
    "start": "2476360",
    "end": "2482710"
  },
  {
    "text": "as word vectors. And so that's what led\ninto the GloVe algorithm.",
    "start": "2482710",
    "end": "2488089"
  },
  {
    "text": "That was what I was\nactually showing you. And so what we wanted\nwas to say, look,",
    "start": "2488090",
    "end": "2495500"
  },
  {
    "text": "we want a model in which linear\ncomponents adding or subtracting",
    "start": "2495500",
    "end": "2502090"
  },
  {
    "text": "a vector and a vector space\ncorrespond to a meaning difference, how can we do that.",
    "start": "2502090",
    "end": "2509859"
  },
  {
    "text": "And Geoffrey did good\nthinking and math",
    "start": "2509860",
    "end": "2515350"
  },
  {
    "text": "and thought about\nthat for a bit. And his solution\nwas to say, well,",
    "start": "2515350",
    "end": "2521450"
  },
  {
    "text": "if we think of the ratios of\nco-occurrence probabilities can",
    "start": "2521450",
    "end": "2527240"
  },
  {
    "text": "encode meaning components--\nso if we can make a ratio of co-occurrence\nprobabilities",
    "start": "2527240",
    "end": "2532849"
  },
  {
    "text": "into something linear\nin the vector space, we'll get the kind of result\nthat word2vec or Doug Rohde got.",
    "start": "2532850",
    "end": "2540710"
  },
  {
    "text": "So what does that mean? Well, so if you start\nthinking of words occurring",
    "start": "2540710",
    "end": "2545930"
  },
  {
    "text": "in the context of\nice, you might think that sort of solid and\nwater are likely to occur",
    "start": "2545930",
    "end": "2551329"
  },
  {
    "text": "near ice and gas or random\nword like random aren't",
    "start": "2551330",
    "end": "2556490"
  },
  {
    "text": "likely to occur near ice. And similarly for\nsteam, you'd expect",
    "start": "2556490",
    "end": "2563420"
  },
  {
    "text": "that gas and water are\nlikely to occur near steam, but probably not\nsolid or random.",
    "start": "2563420",
    "end": "2570539"
  },
  {
    "text": "And, well, if you're just\nlooking at one of these don't really get\nmeaning components because you get something\nthat's large here or large here.",
    "start": "2570540",
    "end": "2579750"
  },
  {
    "text": "But if you then look\nat the ratio of two of these co-occurrence\nprobabilities,",
    "start": "2579750",
    "end": "2585860"
  },
  {
    "text": "then what you get out\nis that for solid, it's going to be large, and for\ngas, it's going to be small.",
    "start": "2585860",
    "end": "2595760"
  },
  {
    "text": "And so you're\ngetting a direction in the space, which\nwill correspond to the solid, liquid,\ngas dimension of physics.",
    "start": "2595760",
    "end": "2605500"
  },
  {
    "text": "Whereas for the other\nwords, it will be about one. This is just the\nwave your hands.",
    "start": "2605500",
    "end": "2612680"
  },
  {
    "text": "This was the\nconception of the idea. But if you actually do the\ncounts, this actually works out. So using real data, this is\nwhat you get for co-occurrence.",
    "start": "2612680",
    "end": "2622220"
  },
  {
    "text": "And indeed, you kind of get\nthese sort of factors of 10 in both of these\ndirections of these two",
    "start": "2622220",
    "end": "2628990"
  },
  {
    "text": "and the numbers over there\nare approximately one. So Jeffrey's idea\nwas, well, we're",
    "start": "2628990",
    "end": "2636910"
  },
  {
    "text": "going to start with a\nco-occurrence count matrix, and we want to make this\nturn into a linear component.",
    "start": "2636910",
    "end": "2647490"
  },
  {
    "text": "Well, how do you do that? Well, first of all,\nit sort of makes sense immediately that you\nshould be putting a log in,",
    "start": "2647490",
    "end": "2653740"
  },
  {
    "text": "because once you put a\nlog in, this ratio will be being turned into\nsomething that's subtracted.",
    "start": "2653740",
    "end": "2660510"
  },
  {
    "text": "And so simply,\nall you have to do is have a log\nbilinear model where",
    "start": "2660510",
    "end": "2666299"
  },
  {
    "text": "the dot product of two\nword vectors models this conditional probability.",
    "start": "2666300",
    "end": "2672850"
  },
  {
    "text": "And then the difference\nbetween two vectors will be corresponding\nto this log",
    "start": "2672850",
    "end": "2678450"
  },
  {
    "text": "of the ratio of their\nco-occurrence probabilities. So that was basically\nthe glove model.",
    "start": "2678450",
    "end": "2686280"
  },
  {
    "text": "So you're wanting to model\nthis dot product such",
    "start": "2686280",
    "end": "2693390"
  },
  {
    "text": "that it's being close to\nthe log of the co-occurrence probability, but you do a\nlittle bit of extra work",
    "start": "2693390",
    "end": "2700690"
  },
  {
    "text": "to have some bias terms and\nsome frequency thresholds which aren't very important.",
    "start": "2700690",
    "end": "2707110"
  },
  {
    "text": "So I'm going to skip past them. But I think that basic intuition\nas to what's the important thing",
    "start": "2707110",
    "end": "2714010"
  },
  {
    "text": "to get linear meaning components\nis a good one to know about. ",
    "start": "2714010",
    "end": "2721270"
  },
  {
    "text": "Is everyone good to there? Cool. Yes.",
    "start": "2721270",
    "end": "2726700"
  },
  {
    "text": "I noticed the original\nx matrix you showed was like 3 by 5 or something.",
    "start": "2726700",
    "end": "2732020"
  },
  {
    "text": "Shouldn't it be square? So yeah, I mean,\nif you're doing-- sorry, yeah. I maybe should have just\nshown you a square one.",
    "start": "2732020",
    "end": "2739210"
  },
  {
    "text": "If you were just doing\nvocabulary to vocabulary, yes, it should be square. But there was a\nbit in the slides",
    "start": "2739210",
    "end": "2745153"
  },
  {
    "text": "that I didn't mention that there\nwas another way you could do it where you did it\nwords versus documents",
    "start": "2745153",
    "end": "2750410"
  },
  {
    "text": "and then it would be non-square. But yeah, you're right. So we can just consider\nthe square case.",
    "start": "2750410",
    "end": "2758405"
  },
  {
    "text": " Hey, I showed you that\ndemo of the glove vectors,",
    "start": "2758405",
    "end": "2765580"
  },
  {
    "text": "and they work\ngreat, didn't they? So these are good vectors.",
    "start": "2765580",
    "end": "2771000"
  },
  {
    "text": "But in general,\nin NLP, we'd like to have things that we\ncan evaluate and know",
    "start": "2771000",
    "end": "2777420"
  },
  {
    "text": "whether things are really good. And so everywhere\nthrough the course,",
    "start": "2777420",
    "end": "2783550"
  },
  {
    "text": "we're going to want to\nevaluate things and work out how good they are and what's\nbetter and what's worse.",
    "start": "2783550",
    "end": "2789810"
  },
  {
    "text": "And so one of the fundamental\nnotions of evaluation that will come up\nagain and again",
    "start": "2789810",
    "end": "2794970"
  },
  {
    "text": "is intrinsic and\nextrinsic evaluations. So an intrinsic\nvaluation is where",
    "start": "2794970",
    "end": "2801120"
  },
  {
    "text": "you are doing a very\nspecific internal subtask",
    "start": "2801120",
    "end": "2806380"
  },
  {
    "text": "and you just try and score\nwhether it's good or bad. So normally,\nintrinsic evaluations",
    "start": "2806380",
    "end": "2811859"
  },
  {
    "text": "are fast to compute, help\nyou understand the components component you're\nbuilding, but they",
    "start": "2811860",
    "end": "2817570"
  },
  {
    "text": "are sort of distant from\nyour downstream task and improving the numbers\ninternally may or may not",
    "start": "2817570",
    "end": "2823540"
  },
  {
    "text": "help you. And that's the contrast with\nan extrinsic evaluation, which",
    "start": "2823540",
    "end": "2830380"
  },
  {
    "text": "is that you've got\nsome real task you want to do question answering\nor document summarization",
    "start": "2830380",
    "end": "2836740"
  },
  {
    "text": "or machine translation. And you want to know\nwhether some clever bit",
    "start": "2836740",
    "end": "2842020"
  },
  {
    "text": "of internal modeling will\nhelp you on that task. So then you have to\nrun an entire system",
    "start": "2842020",
    "end": "2849550"
  },
  {
    "text": "and work out\ndownstream accuracies and find out whether it actually\nhelps you at the end of the day.",
    "start": "2849550",
    "end": "2856630"
  },
  {
    "text": "But that often\nmeans it's indirect. So hard to see exactly what's\nhappening in your task.",
    "start": "2856630",
    "end": "2863170"
  },
  {
    "text": "So for something like\nword vectors, if we just sort of measure, are they\nmodeling word similarity, well,",
    "start": "2863170",
    "end": "2870950"
  },
  {
    "text": "that's an intrinsic evaluation. But we'd probably like to know\nwhether a model words similarity",
    "start": "2870950",
    "end": "2880330"
  },
  {
    "text": "well for some downstream task,\nwhich might be doing web search.",
    "start": "2880330",
    "end": "2887130"
  },
  {
    "text": "When you say, cell\nphone or mobile phone that it comes out\nat about the same,",
    "start": "2887130",
    "end": "2892710"
  },
  {
    "text": "so that would then\nbe web search might be our extrinsic evaluation.",
    "start": "2892710",
    "end": "2897930"
  },
  {
    "text": "So for word vectors, two\nintrinsic evaluations",
    "start": "2897930",
    "end": "2905339"
  },
  {
    "text": "are the ones we've already seen. So there's the word\nvector analogies.",
    "start": "2905340",
    "end": "2910859"
  },
  {
    "text": "I cheated. When I showed you\nthe GloVe demo, I only showed you\nones that work.",
    "start": "2910860",
    "end": "2916660"
  },
  {
    "text": "But if you play for\nit yourself, you can find some that don't work. So what we can do is have\na set of word analogies",
    "start": "2916660",
    "end": "2927210"
  },
  {
    "text": "and find out which ones work. Now in general, GloVe does work. Here's a set of word\nvectors showing you",
    "start": "2927210",
    "end": "2936050"
  },
  {
    "text": "the sort of male\nfemale distinction. It's kind of good and linear. But in general, for different\nones, it's going to work",
    "start": "2936050",
    "end": "2943789"
  },
  {
    "text": "and it's not work,\nand you're going to be able to score what\npercentage of the time it works.",
    "start": "2943790",
    "end": "2949280"
  },
  {
    "text": "Or we can do word similarity. How we do word\nsimilarity is we actually",
    "start": "2949280",
    "end": "2955670"
  },
  {
    "text": "use human judgments\nof similarity. So psychologists ask undergrads\nand they say, here is the word",
    "start": "2955670",
    "end": "2964370"
  },
  {
    "text": "plane and car. How similar are they on a scale\nof 1 to 10-- or 0 to 10 maybe.",
    "start": "2964370",
    "end": "2970920"
  },
  {
    "text": "Actually, I think it's 0 to 10\nhere-- on a scale of 0 to 10. And the person says, 7.",
    "start": "2970920",
    "end": "2977850"
  },
  {
    "text": "And then they ask\nanother person, and they average what\nthe undergrads say,",
    "start": "2977850",
    "end": "2984600"
  },
  {
    "text": "and they come out\nwith these numbers. So tiger tiger gets 10, book and\npaper got an average of 7.46,",
    "start": "2984600",
    "end": "2993180"
  },
  {
    "text": "plane and car got 5.77,\nstock and phone got 1.62,",
    "start": "2993180",
    "end": "2999059"
  },
  {
    "text": "and stock and jaguar got 0.92. Noisy process, but\nyou roughly get",
    "start": "2999060",
    "end": "3004430"
  },
  {
    "text": "to see how similar\npeople think words are. And so then we ask our models\nto also score how similar they",
    "start": "3004430",
    "end": "3012260"
  },
  {
    "text": "think words are. And then we get models of\nhow well the scores are",
    "start": "3012260",
    "end": "3018440"
  },
  {
    "text": "correlated between human\njudgments and our models",
    "start": "3018440",
    "end": "3023810"
  },
  {
    "text": "judgments. And so here are sort of a big\ntable of numbers that we don't",
    "start": "3023810",
    "end": "3029990"
  },
  {
    "text": "need to go through all of. But it shows that a\nplain SVD works terribly.",
    "start": "3029990",
    "end": "3035880"
  },
  {
    "text": "Simply doing SVD over\nlog counts already starts to work reasonably.",
    "start": "3035880",
    "end": "3041540"
  },
  {
    "text": "And then here's the two word2vec\nalgorithms CBOW and skip-gram,",
    "start": "3041540",
    "end": "3047760"
  },
  {
    "text": "and here are numbers\nfrom our glove vectors. And so you get\nthese kind of scores that you can then\nscore different models",
    "start": "3047760",
    "end": "3054520"
  },
  {
    "text": "as to how good they are. And well then you\ncan also-- oh, sorry. Yeah, that's the only\nthing I have there.",
    "start": "3054520",
    "end": "3062089"
  },
  {
    "text": "But what can you do for\ndownstream evaluation? Well, then you want to\npick some downstream task.",
    "start": "3062090",
    "end": "3070340"
  },
  {
    "text": "And so a simple downstream task\nthat's been used a lot in NLP",
    "start": "3070340",
    "end": "3075670"
  },
  {
    "text": "is what's called named\nentity recognition. And so that's recognizing\nnames of things",
    "start": "3075670",
    "end": "3082420"
  },
  {
    "text": "and what type they are. So if the sentence is Chris\nManning lives in Palo Alto,",
    "start": "3082420",
    "end": "3087800"
  },
  {
    "text": "you want to say, Chris\nand Manning, that's the name of a person,\nand Palo and Alto, that's",
    "start": "3087800",
    "end": "3094210"
  },
  {
    "text": "the name of a place. So that can be the task. And, well, that's\nthe kind of task",
    "start": "3094210",
    "end": "3100270"
  },
  {
    "text": "which you might think word\nvectors would help you with. And it's indeed the case.",
    "start": "3100270",
    "end": "3106070"
  },
  {
    "text": "So what's label is discrete\nwas the baseline symbolic,",
    "start": "3106070",
    "end": "3111290"
  },
  {
    "text": "probabilistic, named\nentity recognition task. And by putting word\nvectors into it,",
    "start": "3111290",
    "end": "3118200"
  },
  {
    "text": "you could make them\nthe numbers go up. So these numbers for GloVe\nare higher than the ones",
    "start": "3118200",
    "end": "3123920"
  },
  {
    "text": "on the first line. And so I'm getting\nsubstantial improvements from adding word\nvectors to my system.",
    "start": "3123920",
    "end": "3131386"
  },
  {
    "text": "Yay. ",
    "start": "3131386",
    "end": "3138980"
  },
  {
    "text": "I'll pile ahead\ninto the next thing. This next one I\nthink is interesting and we should spend a minute on.",
    "start": "3138980",
    "end": "3145170"
  },
  {
    "text": "And it came up in your\nquestions last time-- words have lots of meanings.",
    "start": "3145170",
    "end": "3151110"
  },
  {
    "text": "Most words have a whole\nbunch of meanings.",
    "start": "3151110",
    "end": "3156300"
  },
  {
    "text": "Words that don't have a\nlot of different meanings are only some very\nspecialized scientific words.",
    "start": "3156300",
    "end": "3163109"
  },
  {
    "text": "So my example of word\nwith multiple meanings is probably not the first one\nyou think of all the time.",
    "start": "3163110",
    "end": "3169300"
  },
  {
    "text": "The most famous example of a\nword with a lot of meanings is bank, which already\ncame up last time.",
    "start": "3169300",
    "end": "3174319"
  },
  {
    "text": "And I use star,\nwhich is another one. Here's a word that you\nprobably don't use that often, but it still has\nlots of meanings.",
    "start": "3174320",
    "end": "3181250"
  },
  {
    "text": "So the word pike-- what are some things that\nthe word pike can mean?",
    "start": "3181250",
    "end": "3186400"
  },
  {
    "text": "A fish. Yes, it's a kind of fish. OK, we've got one. What else can a pike be?",
    "start": "3186400",
    "end": "3191600"
  },
  {
    "text": "Yeah. A spear. A spear, yeah. For the Dungeons\nand Dragons crowd. Yeah, it's a long arm.",
    "start": "3191600",
    "end": "3197990"
  },
  {
    "text": "Yep, that's another one. Yeah. A road, right. Yeah, so pike is\nused as a shorthand--",
    "start": "3197990",
    "end": "3205059"
  },
  {
    "text": "well, a shorthand\nfor a turnpike. Why it's called a\nturnpike whereas, yeah, originally you had\nthe scary looking thing",
    "start": "3205060",
    "end": "3212770"
  },
  {
    "text": "at the start of it that's\nsort of count people. We've got three. Other meanings for pike? Yes.",
    "start": "3212770",
    "end": "3218110"
  },
  {
    "text": "It's also a frat. Like a fraternity, like\nthe name of a fraternity.",
    "start": "3218110",
    "end": "3224260"
  },
  {
    "text": "I'll believe you. I can't say I know that one. ",
    "start": "3224260",
    "end": "3229470"
  },
  {
    "text": "Other pikes? Could be sharp like a needle.",
    "start": "3229470",
    "end": "3234810"
  },
  {
    "text": "Something sharp. Maybe. I mean, I think it's really\nthe sort of pike as the weapon.",
    "start": "3234810",
    "end": "3240323"
  },
  {
    "text": " Any other scratching your heads?",
    "start": "3240323",
    "end": "3246450"
  },
  {
    "text": "One that I think a\nlot of you will have seen in diving and swimming.",
    "start": "3246450",
    "end": "3251770"
  },
  {
    "text": "You can do a pike. Olympics-- if you see Olympic\ndiving, there are pikes.",
    "start": "3251770",
    "end": "3258430"
  },
  {
    "text": "Anyone seen those?  Trust me, there's a pike.",
    "start": "3258430",
    "end": "3264780"
  },
  {
    "text": "And so we've sort of\nbeen doing the noun uses.",
    "start": "3264780",
    "end": "3270130"
  },
  {
    "text": "But you can also\nuse pike as a verb. Once you've got your medieval\nweapon, you can pike somebody,",
    "start": "3270130",
    "end": "3279000"
  },
  {
    "text": "and that's a usage of pike. And you can do other ones.",
    "start": "3279000",
    "end": "3284530"
  },
  {
    "text": "So here we go. Here's ones I got\nfrom a dictionary.",
    "start": "3284530",
    "end": "3291530"
  },
  {
    "text": "We got most of those. There are sort of\nweirder usages, like coming down the pike. That's kind of a\nmetaphorical use that",
    "start": "3291530",
    "end": "3298210"
  },
  {
    "text": "comes from the road\nsense, but it sort of ends up meaning the future.",
    "start": "3298210",
    "end": "3304750"
  },
  {
    "text": "Yeah. In Australia, we also use\npike to mean sort of chicken",
    "start": "3304750",
    "end": "3310810"
  },
  {
    "text": "out of doing something, but I\ndon't think that usage is really used in the US. Anyway, words have\nlots of meanings.",
    "start": "3310810",
    "end": "3317390"
  },
  {
    "text": "So how can you deal with that? Well, one way you could\ndeal with it is to say, OK, words have several meanings.",
    "start": "3317390",
    "end": "3326810"
  },
  {
    "text": "And so we're just going to say\nwords have several meanings. And then we're going to take\ninstances of words in text.",
    "start": "3326810",
    "end": "3334850"
  },
  {
    "text": "We're going to\ncluster them based on their similarity\nof occurrence to decide which sense of the\nword to regard each token as.",
    "start": "3334850",
    "end": "3344559"
  },
  {
    "text": "And then we're going\nto learn word vectors for those token clusters\nwhich are our sensors.",
    "start": "3344560",
    "end": "3352090"
  },
  {
    "text": "And you can do that. We did it in 2012 before\nword2vec came out.",
    "start": "3352090",
    "end": "3358710"
  },
  {
    "text": "So you see here, we have bank 1. And somewhere over,\nhere we have bank 2.",
    "start": "3358710",
    "end": "3365680"
  },
  {
    "text": "And here we have jaguar 1,\njaguar 2, jaguar 3, jaguar 4.",
    "start": "3365680",
    "end": "3372550"
  },
  {
    "text": "And this really works\nout great right. So jaguar 1 picks out the\nsense of the kind of car.",
    "start": "3372550",
    "end": "3381880"
  },
  {
    "text": "And it's close to\nluxury and convertible. Jaguar 2 comes right close\nto software and Microsoft.",
    "start": "3381880",
    "end": "3389620"
  },
  {
    "text": "And this one's a bit\nof an historical one, but that's when most of\nyou were five or whatever.",
    "start": "3389620",
    "end": "3397690"
  },
  {
    "text": "You might remember, Apple\nused to use large cats for versions of Mac OS.",
    "start": "3397690",
    "end": "3406280"
  },
  {
    "text": "So sort of Mac OS\n10.3 or something like that a long time\nago was called Jaguar.",
    "start": "3406280",
    "end": "3413140"
  },
  {
    "text": "So it's software\nclose to Microsoft. Jaguar 3, string keyboard,\nsolo musical, drum bass.",
    "start": "3413140",
    "end": "3423589"
  },
  {
    "text": "That's because there's\na Jaguar keyboard. And then finally, what we\nthink of as the basic sense,",
    "start": "3423590",
    "end": "3432380"
  },
  {
    "text": "but turns up rather less\nin text corpora normally,",
    "start": "3432380",
    "end": "3437589"
  },
  {
    "text": "jaguar next to\nhunter is the animal. So it's done a good job at\nlearning the different senses.",
    "start": "3437590",
    "end": "3443359"
  },
  {
    "text": "But that's not what's actually\nusually done these days.",
    "start": "3443360",
    "end": "3448610"
  },
  {
    "text": "And instead, what's\nusually done is you do only have one vector for jaguar.",
    "start": "3448610",
    "end": "3455000"
  },
  {
    "text": "And when you do\nthat, or pike here, the one vector you learn is a\nweighted average of the vectors",
    "start": "3455000",
    "end": "3467400"
  },
  {
    "text": "that you would have\nlearnt for the sensors. It's often referred\nto as a superposition",
    "start": "3467400",
    "end": "3474329"
  },
  {
    "text": "because somehow,\nneural net math people like to use physics terms.",
    "start": "3474330",
    "end": "3479790"
  },
  {
    "text": "And so they call\nit a superposition. But it's a weighted average.",
    "start": "3479790",
    "end": "3485050"
  },
  {
    "text": "So you're taking the\nrelative frequency of the different\nsensors and multiplying the vectors you\nwould have learned",
    "start": "3485050",
    "end": "3491130"
  },
  {
    "text": "if you'd had sense vectors. And that's what you get as\nthe representation as a whole.",
    "start": "3491130",
    "end": "3498600"
  },
  {
    "text": "And I can make a sort\nof a linguistic argument",
    "start": "3498600",
    "end": "3504120"
  },
  {
    "text": "as to why you might\nwant to do that, which is although this model\nof words have sensors is",
    "start": "3504120",
    "end": "3512760"
  },
  {
    "text": "very long standing and common-- I mean, it's essentially the\nway dictionaries are built.",
    "start": "3512760",
    "end": "3519170"
  },
  {
    "text": "You look up a word\nin the dictionary and it says sense\n1, sense 2, sense 3, and you get them for\nthings like bank or jaguar",
    "start": "3519170",
    "end": "3526670"
  },
  {
    "text": "as we're talking about. I mean, it's really\na broken model",
    "start": "3526670",
    "end": "3531890"
  },
  {
    "text": "that word meanings\nhave a lot of nuance.",
    "start": "3531890",
    "end": "3537420"
  },
  {
    "text": "They're used in a lot\nof different contexts. They're extreme examples\nlike bank, wherever",
    "start": "3537420",
    "end": "3543740"
  },
  {
    "text": "it was, where we have finance\nbank and bank of a river bank over here where it seems\nlike the senses are this",
    "start": "3543740",
    "end": "3550670"
  },
  {
    "text": "far apart. But most words have\ndifferent meanings, but they're not\nactually that far apart.",
    "start": "3550670",
    "end": "3557400"
  },
  {
    "text": "And trying to cut\nthem into senses seems actually very artificial.",
    "start": "3557400",
    "end": "3562410"
  },
  {
    "text": "And if you look up five\ndifferent dictionaries and you say, how many\nsenses does this word have,",
    "start": "3562410",
    "end": "3570210"
  },
  {
    "text": "pretty much everyone will\ngive you a different answer. So the kind of situation you\nhave is a word like field.",
    "start": "3570210",
    "end": "3578260"
  },
  {
    "text": "Well, a field can be used for\na place where you grow a crop.",
    "start": "3578260",
    "end": "3583350"
  },
  {
    "text": "It can be used for\nsort of natural things like a rock field\nor an ice field.",
    "start": "3583350",
    "end": "3590590"
  },
  {
    "text": "It can be used for\na sporting field. There's the mathematical\nsense of field.",
    "start": "3590590",
    "end": "3596440"
  },
  {
    "text": "Now, all of these things have\nsomething to do with each other. I mean, the math\none is further away,",
    "start": "3596440",
    "end": "3601600"
  },
  {
    "text": "but the physical ones\nare sort of flat spaces. But the sense of it\nbeing a sporting field",
    "start": "3601600",
    "end": "3610380"
  },
  {
    "text": "is clearly kind of different\nfrom the sense of it being an ice field.",
    "start": "3610380",
    "end": "3615430"
  },
  {
    "text": "Is the ice field and the\nrock field different, or am I just modifying them,\nare they different senses?",
    "start": "3615430",
    "end": "3621940"
  },
  {
    "text": "So really have a kind of\nwhat a math person say",
    "start": "3621940",
    "end": "3628710"
  },
  {
    "text": "is some probability density\ndistribution over things that can be meant by\nthe meaning of a word.",
    "start": "3628710",
    "end": "3635049"
  },
  {
    "text": "So it's sort of\nmaybe makes sense to more use this model where\nyou're just actually saying,",
    "start": "3635050",
    "end": "3640080"
  },
  {
    "text": "we have a vector that's an\naverage of all the contexts. And we'll see more of that\nwhen we get to contextual",
    "start": "3640080",
    "end": "3646460"
  },
  {
    "text": "word vectors later on. But one more surprising\nresult on this",
    "start": "3646460",
    "end": "3652820"
  },
  {
    "text": "is since you have the vector\nfor pike overall being",
    "start": "3652820",
    "end": "3661400"
  },
  {
    "text": "the sum of these\ndifferent sense vectors,",
    "start": "3661400",
    "end": "3666990"
  },
  {
    "text": "standard math would tell\nyou that if you just have the single\nvector, there's no way",
    "start": "3666990",
    "end": "3673640"
  },
  {
    "text": "that you can recover the\nindividual sense vectors. But higher math tells you that\nactually, these vector spaces",
    "start": "3673640",
    "end": "3685640"
  },
  {
    "text": "are so high dimensional and\nsparse that you can use ideas",
    "start": "3685640",
    "end": "3691130"
  },
  {
    "text": "from sparse coding theory to\nreconstruct the sense vectors",
    "start": "3691130",
    "end": "3696420"
  },
  {
    "text": "out of the whole vector. And if you actually want\nto understand this, some",
    "start": "3696420",
    "end": "3702180"
  },
  {
    "text": "of the people in statistics-- David Donoho, I think,\nis one of them-- teach courses on\nsparse coding theory.",
    "start": "3702180",
    "end": "3708930"
  },
  {
    "text": "But I'm not going to\ntry and teach that. But here's an example from this\npaper, the Sanjeev Arora et",
    "start": "3708930",
    "end": "3716460"
  },
  {
    "text": "Al where one of the et Als\nis Tim [INAUDIBLE] who's now a faculty in\ncomputer science",
    "start": "3716460",
    "end": "3721980"
  },
  {
    "text": "here, where they are starting\noff with the word vector and using sparse coding\nto divide out sense",
    "start": "3721980",
    "end": "3731130"
  },
  {
    "text": "vectors from one word vector. And they work pretty well. So here's one sense of tie,\nwhich is a piece of clothing,",
    "start": "3731130",
    "end": "3739089"
  },
  {
    "text": "another sense of tie,\nwhich is tie in a game. This one is similar to\nthat one, I'll admit,",
    "start": "3739090",
    "end": "3746680"
  },
  {
    "text": "but this sense of tie\nhere is then a tie as you put on your\nelectrical cables.",
    "start": "3746680",
    "end": "3753500"
  },
  {
    "text": "Then you have the\nmusical sense of tie. At least four out of five. They've done a pretty good\njob of getting sensors out",
    "start": "3753500",
    "end": "3760670"
  },
  {
    "text": "of this single word\nvector by sparse coding. So sparse coding\nmust be cool if you",
    "start": "3760670",
    "end": "3766685"
  },
  {
    "text": "want to go off and\nlearn more about it. ",
    "start": "3766685",
    "end": "3774830"
  },
  {
    "text": "So that's everything I was\ngoing to say about word vectors and word senses.",
    "start": "3774830",
    "end": "3780770"
  },
  {
    "text": "Is everyone good till there? Any questions? ",
    "start": "3780770",
    "end": "3786470"
  },
  {
    "text": "I'll rush ahead for\nthe last two pieces. So I just wanted to start\nto introduce, in the last 15",
    "start": "3786470",
    "end": "3793730"
  },
  {
    "text": "minutes, the ideas of how we\ncan build neural classifiers",
    "start": "3793730",
    "end": "3801410"
  },
  {
    "text": "and how we start to build,\nin general, neural networks.",
    "start": "3801410",
    "end": "3806430"
  },
  {
    "text": "I mean, in a sense,\nwe've already built a very simple\nneural classifier",
    "start": "3806430",
    "end": "3812930"
  },
  {
    "text": "because our word2vec\nmodel is predicting what words are likely to occur\nin the context of another word,",
    "start": "3812930",
    "end": "3820869"
  },
  {
    "text": "and you can think of\nthat as a classifier. But let's look at\na simple classifier like our named entity recognizer\nthat I mentioned before.",
    "start": "3820870",
    "end": "3827930"
  },
  {
    "text": "So for the named\nentity recognizer, we want to label words\nwith their class. So we want to say, these\ntwo words are a person,",
    "start": "3827930",
    "end": "3836600"
  },
  {
    "text": "but the same words,\nParis and Hilton, are then locations in\nthis second sentence.",
    "start": "3836600",
    "end": "3843440"
  },
  {
    "text": "So words can be ambiguous\nas to what their class is. And the other state\nis that they're not",
    "start": "3843440",
    "end": "3851590"
  },
  {
    "text": "a named entity at all. They're just a word\nthat is some other word. And this is something that's\nused in lots of places",
    "start": "3851590",
    "end": "3860140"
  },
  {
    "text": "as a bit of understanding. So if you've seen any\nof those web pages",
    "start": "3860140",
    "end": "3865840"
  },
  {
    "text": "where they've tagged company\nnames with the stock ticker or there's links on a Wikipedia\npage to a Wikipedia page",
    "start": "3865840",
    "end": "3875119"
  },
  {
    "text": "or something like that,\nyou've got named entities where commonly, after\nfinding the named entities,",
    "start": "3875120",
    "end": "3882110"
  },
  {
    "text": "you're doing this second\nstage of entity linking, where you're then\nlinking the named entity to some canonical form\nof it, like a Wikipedia page.",
    "start": "3882110",
    "end": "3891390"
  },
  {
    "text": "But we're not going to talk\nabout the second part of it for the rest of the day.",
    "start": "3891390",
    "end": "3897020"
  },
  {
    "text": "And so we could say that\nbuilding with our word vectors,",
    "start": "3897020",
    "end": "3903450"
  },
  {
    "text": "we've got this simple task\nwhere what we're going to do is we're going to look\nat a word in context--",
    "start": "3903450",
    "end": "3909930"
  },
  {
    "text": "because sometimes Paris\nis a name of a person, and sometimes it's a location.",
    "start": "3909930",
    "end": "3914940"
  },
  {
    "text": "And so we're going to want to\nlook at this word in its context and say, this is the name of\na location in this instance.",
    "start": "3914940",
    "end": "3923880"
  },
  {
    "text": "And so the way that we're\ngoing to do it is we're going to form a\nwindow classifier.",
    "start": "3923880",
    "end": "3931880"
  },
  {
    "text": "So we're going to take a\nword with a couple of words of context on each side. And for the words in\nour context window,",
    "start": "3931880",
    "end": "3939590"
  },
  {
    "text": "we're going to use\nour word vectors because we want to show\nthey're useful for something. And then we want to\nfeed this into something",
    "start": "3939590",
    "end": "3946540"
  },
  {
    "text": "that is a classifier. And our classifier,\nit's actually going to be a really\nsimple classifier.",
    "start": "3946540",
    "end": "3954020"
  },
  {
    "text": "We're only here going to do\nlocation or not a location. So this one here, we're wanting\nto say for this window here,",
    "start": "3954020",
    "end": "3963470"
  },
  {
    "text": "yes, it's a location. Whereas if it had been I\nlove Paris Hilton greatly,",
    "start": "3963470",
    "end": "3973300"
  },
  {
    "text": "then we'd be saying\nno because Paris, the word in the middle of the\ncontext, then isn't a location.",
    "start": "3973300",
    "end": "3981500"
  },
  {
    "text": "So that's the idea of a\nclassification or classifier. We're assigning some set\nof classes to things.",
    "start": "3981500",
    "end": "3992180"
  },
  {
    "text": "So in general, for\nclassifiers, we do supervised\nlearning, which means",
    "start": "3992180",
    "end": "3997640"
  },
  {
    "text": "we have some labeled examples\nour training data set. So we have input items\nx I. And for each one,",
    "start": "3997640",
    "end": "4005619"
  },
  {
    "text": "we've got a class y I.\nSo I had, for my example,",
    "start": "4005620",
    "end": "4011030"
  },
  {
    "text": "training examples ones like\nI love Paris Hilton greatly. That was negative,\nnot a location.",
    "start": "4011030",
    "end": "4020000"
  },
  {
    "text": "And I visit Paris every spring. That's positive. That is a location\nwhere I'm actually",
    "start": "4020000",
    "end": "4025630"
  },
  {
    "text": "classifying the middle word. So inputs labels. And in general, we've got labels\nthat are a set of classes.",
    "start": "4025630",
    "end": "4034099"
  },
  {
    "text": "So my set here is simply\nlocation not a location. But I could get fancier\nand I could say,",
    "start": "4034100",
    "end": "4040900"
  },
  {
    "text": "I've got five classes. I've got location,\nperson, name--",
    "start": "4040900",
    "end": "4047220"
  },
  {
    "text": "whatever other ones there are-- company name, drug name. I could be assigning a bunch of\nor other not a name and a bunch",
    "start": "4047220",
    "end": "4056550"
  },
  {
    "text": "of different classes. But I'm going to be\ndoing it with only two because I'm using this example\non next Tuesday's lecture",
    "start": "4056550",
    "end": "4063809"
  },
  {
    "text": "as well. And I'm wanting\nto keep it simple. So that's what\nwe're going to do.",
    "start": "4063810",
    "end": "4069750"
  },
  {
    "text": "And so what we're going\nto be using in our class is neural classifiers.",
    "start": "4069750",
    "end": "4076720"
  },
  {
    "text": "And so I just wanted to just go\nthrough quickly just for food",
    "start": "4076720",
    "end": "4083700"
  },
  {
    "text": "for thought as we go into it. So for a typical stats\nmachine learning classifier,",
    "start": "4083700",
    "end": "4091210"
  },
  {
    "text": "you can build classifiers like\nlogistic regression or softmax classifiers or other ones like\nsupport vector machines or Naive",
    "start": "4091210",
    "end": "4100109"
  },
  {
    "text": "Bayes or whatever else\nyou might have seen. The vast majority\nof these classifiers",
    "start": "4100109",
    "end": "4107170"
  },
  {
    "text": "are linear classifiers, meaning\nthat they have a linear decision boundary. And when we're learning\nthese classifiers,",
    "start": "4107170",
    "end": "4114740"
  },
  {
    "text": "we're learning\nparameters here W. But our inputs are fixed, that\nour inputs are represented",
    "start": "4114740",
    "end": "4121870"
  },
  {
    "text": "by symbols or quantities. So we have fixed inputs.",
    "start": "4121870",
    "end": "4127040"
  },
  {
    "text": "We learn parameters\nas weights that are used to multiply the inputs.",
    "start": "4127040",
    "end": "4133250"
  },
  {
    "text": "And then we use a linear\ndecision boundary. So when we have our\nneural classifier,",
    "start": "4133250",
    "end": "4139210"
  },
  {
    "text": "we're kind of getting\nsome more power. So first of all, we're\nnot only learning",
    "start": "4139210",
    "end": "4145120"
  },
  {
    "text": "weights W for our\nclassifier, we're also learning distributed\nrepresentations for our words.",
    "start": "4145120",
    "end": "4151759"
  },
  {
    "text": "So our words can re-represent-- our word vectors represent\nthe actual words as symbols",
    "start": "4151760",
    "end": "4159850"
  },
  {
    "text": "and can move them\naround in the space so that in terms of\nthe original space,",
    "start": "4159850",
    "end": "4166359"
  },
  {
    "text": "we've got a nonlinear classifier\nthat can represent much more complex functions,\nbut we will then",
    "start": "4166359",
    "end": "4174000"
  },
  {
    "text": "use the word vectors to\nre-represent those words to do",
    "start": "4174000",
    "end": "4179609"
  },
  {
    "text": "a final classification. So at the end of our\ndeep network, which",
    "start": "4179609",
    "end": "4184710"
  },
  {
    "text": "we're about to build, we\nwill have a linear classifier in terms of our\nrerepresented vectors,",
    "start": "4184710",
    "end": "4191380"
  },
  {
    "text": "but not in terms of\nour original space. Let me try and be\nconcrete about that.",
    "start": "4191380",
    "end": "4196590"
  },
  {
    "text": "So here's what I'm going to use. And we'll use again next Tuesday\nas my little neural network.",
    "start": "4196590",
    "end": "4205230"
  },
  {
    "text": "And so I start with some words,\nMuseums and Paris are amazing.",
    "start": "4205230",
    "end": "4210676"
  },
  {
    "text": "I first of all come up\nwith the word embedding of those using my word vectors.",
    "start": "4210676",
    "end": "4217360"
  },
  {
    "text": "So now I've got this sort of\nhigh dimensional vector, which is just a concatenation\nof five word vectors.",
    "start": "4217360",
    "end": "4224680"
  },
  {
    "text": "So if I have 100\ndimensional word vectors, this is 500 dimensional. And then I'm going to put\nit through a neural network",
    "start": "4224680",
    "end": "4231600"
  },
  {
    "text": "layer, which is\nsimply multiplying that vector by a matrix and\nadding on a bias vector.",
    "start": "4231600",
    "end": "4239890"
  },
  {
    "text": "And then I'm going to put it\nthrough some non-linearity, which might be, for example,\nthe logistic function",
    "start": "4239890",
    "end": "4246090"
  },
  {
    "text": "that we've already seen. So that will give me\na new representation. And in particular, if\nthe W is, say, 8 by 500,",
    "start": "4246090",
    "end": "4257980"
  },
  {
    "text": "I'll be reducing it to a much-- yeah, 8 by 500, I'll be reducing\nit to a much smaller vector.",
    "start": "4257980",
    "end": "4265989"
  },
  {
    "text": "So then I can do, after\nthat, I can multiply my hidden representation in\nthe middle of my neural network",
    "start": "4265990",
    "end": "4274260"
  },
  {
    "text": "by another vector. And that will give me a score. And I'm going to put the score\ninto the logistic function",
    "start": "4274260",
    "end": "4280860"
  },
  {
    "text": "that we saw earlier to say,\nwhat's the probability this is the location. So at this point,\nmy classifier is",
    "start": "4280860",
    "end": "4289530"
  },
  {
    "text": "going to be a linear\nclassifier in terms of this internal representation\nthat's used right at the end.",
    "start": "4289530",
    "end": "4296680"
  },
  {
    "text": "But it's going to be a nonlinear\nclassifier in terms of my word vectors. ",
    "start": "4296680",
    "end": "4309600"
  },
  {
    "text": "Great. Here's one other thing. This is just sort of\na note for learn ahead",
    "start": "4309600",
    "end": "4318630"
  },
  {
    "text": "since you want to know\nthis when we start doing the next assignments. I mean, up until now,\nI've presented everything",
    "start": "4318630",
    "end": "4324940"
  },
  {
    "text": "as doing log likelihood\nand negative log likelihood",
    "start": "4324940",
    "end": "4330330"
  },
  {
    "text": "for building our models. Very soon now,\nassignment 2, we're",
    "start": "4330330",
    "end": "4335940"
  },
  {
    "text": "going to be starting to\ndo things with PyTorch. And when you start working\nout your losses with PyTorch,",
    "start": "4335940",
    "end": "4346099"
  },
  {
    "text": "what you're going to be wanting\nto use is cross entropy loss. And so just let me quickly say\nwhat cross entropy loss is.",
    "start": "4346100",
    "end": "4354659"
  },
  {
    "text": "So cross entropy is\nfrom information theory. So if you have a true\nprobability distribution p",
    "start": "4354660",
    "end": "4362270"
  },
  {
    "text": "and you're computing a\nprobability distribution q, your cross entropy\nloss is like this.",
    "start": "4362270",
    "end": "4369800"
  },
  {
    "text": "So it's the log of\nyour model probability,",
    "start": "4369800",
    "end": "4376489"
  },
  {
    "text": "the expectation of that\nunder your true probability distribution.",
    "start": "4376490",
    "end": "4381780"
  },
  {
    "text": "But there's sort\nof a special case whereas if you have ground truth\nor gold or target data where",
    "start": "4381780",
    "end": "4389990"
  },
  {
    "text": "things are labeled 1, 0-- so like for examples of\nI love Paris when warm,",
    "start": "4389990",
    "end": "4401250"
  },
  {
    "text": "I'm just labeling\nit 1 for location. Probability 1, it's a\nlocation, probability 0,",
    "start": "4401250",
    "end": "4407710"
  },
  {
    "text": "it's not a location. So if you're just labeling the\nright class as probability 1,",
    "start": "4407710",
    "end": "4413680"
  },
  {
    "text": "then in this summation,\nevery other term goes to 0.",
    "start": "4413680",
    "end": "4419140"
  },
  {
    "text": "And the only thing\nyou're left with is what probability\nis my model--",
    "start": "4419140",
    "end": "4424750"
  },
  {
    "text": "what log probability is my\nmodel giving to the right class.",
    "start": "4424750",
    "end": "4430150"
  },
  {
    "text": "And so that then is\nyour log likelihood which we can use for the\nnegative log likelihood.",
    "start": "4430150",
    "end": "4440550"
  },
  {
    "text": "A little bit of a\ncomplication here. Just remember that you'll\nwant to use cross entropy loss in PyTorch when\nbuilding the model.",
    "start": "4440550",
    "end": "4449700"
  },
  {
    "text": "Before we end today, here\nis my obligatory one picture",
    "start": "4449700",
    "end": "4454800"
  },
  {
    "text": "of human neurons. Don't miss it because I'm not\ngoing to show any more of these.",
    "start": "4454800",
    "end": "4459920"
  },
  {
    "text": "These are human neurons. Human neurons were the\ninspiration for neural networks.",
    "start": "4459920",
    "end": "4469740"
  },
  {
    "text": "So human neurons\nhave a single output, which comes down this axon.",
    "start": "4469740",
    "end": "4476719"
  },
  {
    "text": "And then when you\nhave these outputs,",
    "start": "4476720",
    "end": "4483350"
  },
  {
    "text": "they then feed\ninto other neurons. I guess I don't really\nhave an example here.",
    "start": "4483350",
    "end": "4490079"
  },
  {
    "text": "But in general,\none output can feed into multiple different neurons. You can see the different\nthings hanging into it.",
    "start": "4490080",
    "end": "4496469"
  },
  {
    "text": "So you have the output\nconnecting to the input. And sort of where you\nmake this connection,",
    "start": "4496470",
    "end": "4502500"
  },
  {
    "text": "that's the synapses\nthat people talk about. And so one neuron will normally\nhave many, many inputs where",
    "start": "4502500",
    "end": "4510590"
  },
  {
    "text": "it picks things up\nfrom other neurons, and they all go into\nthe nucleus of the cell,",
    "start": "4510590",
    "end": "4516920"
  },
  {
    "text": "and the nucleus combines\ntogether all those inputs. And kind of what\nhappens is, if there's",
    "start": "4516920",
    "end": "4523600"
  },
  {
    "text": "enough positive activation\nfrom all of these inputs,",
    "start": "4523600",
    "end": "4529670"
  },
  {
    "text": "it then sends signals\ndown its output. Now, strictly how neurons\nwork is that they send spikes.",
    "start": "4529670",
    "end": "4538880"
  },
  {
    "text": "So the level of activation of a\nneuron is its rate of spiking.",
    "start": "4538880",
    "end": "4544179"
  },
  {
    "text": "But that immediately got turned\nin artificial neural networks into just a real value as to\nwhat is its level of activation.",
    "start": "4544180",
    "end": "4553449"
  },
  {
    "text": "And so it does this. So this was kind of\nthe genuine inspiration of all of our neural networks.",
    "start": "4553450",
    "end": "4560390"
  },
  {
    "text": "So a binary logistic\nregression is kind of a bit similar to a neuron.",
    "start": "4560390",
    "end": "4566330"
  },
  {
    "text": "It has multiple inputs. You're working out your\ntotal level of excitation",
    "start": "4566330",
    "end": "4574310"
  },
  {
    "text": "where, in particular,\nyou can have inputs that are both exciting\npositive inputs and inputs that",
    "start": "4574310",
    "end": "4581420"
  },
  {
    "text": "are negative, which are\nthen inhibitory inputs.",
    "start": "4581420",
    "end": "4586469"
  },
  {
    "text": "You combine them\nall together and you get an output that's\nyour level of excitation.",
    "start": "4586470",
    "end": "4593220"
  },
  {
    "text": "And you're then converting that\nthrough some non-linearity. And so this was proposed\nas a very simple model",
    "start": "4593220",
    "end": "4601220"
  },
  {
    "text": "of human neurons. Now human neurons are way\nmore complex than this.",
    "start": "4601220",
    "end": "4606380"
  },
  {
    "text": "And some people,\nlike neuroscientists, think we maybe should\nbe doing a better model of actual human neurons.",
    "start": "4606380",
    "end": "4614070"
  },
  {
    "text": "But in terms of\nwhat's being done in the current neural networks\neat the world revolution,",
    "start": "4614070",
    "end": "4620760"
  },
  {
    "text": "everyone's forgotten about that. And it's just sticking with this\nvery, very simple model, which",
    "start": "4620760",
    "end": "4627530"
  },
  {
    "text": "conveniently turns into linear\nalgebra in a very simple way.",
    "start": "4627530",
    "end": "4632739"
  },
  {
    "text": "So this gives us sort\nof like a single neuron,",
    "start": "4632740",
    "end": "4637900"
  },
  {
    "text": "but then precise-- so this single neuron, if\nyou use the logistic function",
    "start": "4637900",
    "end": "4644890"
  },
  {
    "text": "is identical to\nlogistic regression, which you've probably seen in\nsome stats class or something.",
    "start": "4644890",
    "end": "4650570"
  },
  {
    "text": "But the difference is\nthat for neural networks, we don't just have one\nlogistic regression.",
    "start": "4650570",
    "end": "4656900"
  },
  {
    "text": "We have a bunch of logistic\nregressions at once.",
    "start": "4656900",
    "end": "4662120"
  },
  {
    "text": "And well, that'd\nbe tricky if we had to define what each of\nthese logistic regressions",
    "start": "4662120",
    "end": "4667360"
  },
  {
    "text": "was calculating. But what we do is\nwe just feed them",
    "start": "4667360",
    "end": "4673420"
  },
  {
    "text": "into another\nlogistic regression. And so we have some\neventual output",
    "start": "4673420",
    "end": "4680710"
  },
  {
    "text": "that we want to be something,\nlike we want it to say, this is or isn't a location.",
    "start": "4680710",
    "end": "4687050"
  },
  {
    "text": "But then what will\nhappen is, by our machine learning, these intermediate\nlogistic regressions will",
    "start": "4687050",
    "end": "4694850"
  },
  {
    "text": "figure out all by themselves\nsomething useful to do. That's the magic.",
    "start": "4694850",
    "end": "4700820"
  },
  {
    "text": "So you get this sort of\nself-learning property where the model has\na lot of parameters",
    "start": "4700820",
    "end": "4708140"
  },
  {
    "text": "and internally will work\nout useful things to do. So in general, we\ncan get more magic",
    "start": "4708140",
    "end": "4714080"
  },
  {
    "text": "by having more layers\nin the neural network, and that we will\nbuild up functions.",
    "start": "4714080",
    "end": "4720600"
  },
  {
    "text": "So effectively, these\nintermediate layers let us learn a model\nthat represents the input",
    "start": "4720600",
    "end": "4727730"
  },
  {
    "text": "data in ways that will\nmake it easier to classify or easier to interpret and\ndo things with downstream",
    "start": "4727730",
    "end": "4736250"
  },
  {
    "text": "in our neural network. And it's time. So I should stop there.",
    "start": "4736250",
    "end": "4742820"
  },
  {
    "text": "Thank you. ",
    "start": "4742820",
    "end": "4752000"
  }
]