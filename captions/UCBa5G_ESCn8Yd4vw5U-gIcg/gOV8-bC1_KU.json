[
  {
    "start": "0",
    "end": "158000"
  },
  {
    "text": "So, homework two is out now. I recognize that there's a really broad spectrum of",
    "start": "4610",
    "end": "10170"
  },
  {
    "text": "background in terms of whether people have seen deep learning, or not before, or, or taken a class, [NOISE] or used it extensively.",
    "start": "10170",
    "end": "16560"
  },
  {
    "text": "[NOISE] Um, just a quick humble, who, which of you have used TensorFlow or, um, PyTorch before?",
    "start": "16560",
    "end": "23640"
  },
  {
    "text": "Okay. A number of you, but not everybody. So, what we're gonna be doing this weekend sessions is, we gonna be having some more background on deep learning.",
    "start": "23640",
    "end": "30750"
  },
  {
    "text": "You're not expected to become, or, or to be a deep learning expert to be in this class, but we, you only need to have some basic skills in order to do homework two,",
    "start": "30750",
    "end": "38760"
  },
  {
    "text": "um, be able to use function approximation with a deep neural network. So, I encourage you to go to session this week if you don't have a background on that.",
    "start": "38760",
    "end": "45480"
  },
  {
    "text": "We're gonna today cover a little bit on deep learning but very, very, very small amount, um,",
    "start": "45480",
    "end": "50585"
  },
  {
    "text": "and focus more on deep reinforcement learning. [NOISE] Uh, but the sessions will be a good chance to catch up on that material.",
    "start": "50585",
    "end": "57275"
  },
  {
    "text": "Um, we're also gonna be reaching, uh, releasing by the end of tomorrow. Um, what the default projects will be, uh, for this class.",
    "start": "57275",
    "end": "64440"
  },
  {
    "text": "Er, and you guys will get to pick whether or not you wanna do your own construction project or, uh, the default project.",
    "start": "64440",
    "end": "70900"
  },
  {
    "text": "Um, and those proposals will be due, um, very soon, er, in a little over a week.",
    "start": "70900",
    "end": "76700"
  },
  {
    "text": "Are there any other questions that people have right now? Yeah. The assignments, [inaudible] are they limited to TensorFlow?",
    "start": "76700",
    "end": "84475"
  },
  {
    "text": "asked the question if, if the assignment is limited to TensorFlow. I'm, I'm pretty sure that everything relies that,",
    "start": "84475",
    "end": "90665"
  },
  {
    "text": "er, you're using TensorFlow. So, yeah. Just a-feel free to reach out on Piazza and double-check that, but I'm pretty sure any Oliver auto-graders is just set up for TensorFlow,",
    "start": "90665",
    "end": "98150"
  },
  {
    "text": "for, so for this one even if you use PyTorch, some way please use TensorFlow. Um, I'll believe you guys also should have access to the Azure credit.",
    "start": "98150",
    "end": "105409"
  },
  {
    "text": "Um, If you have any questions about getting setup without feel free to use the Piazza, uh, Piazza channel. We also released a tutorial for how to just sort of set up your machine last week.",
    "start": "105410",
    "end": "113690"
  },
  {
    "text": "So, if you're having any questions with that, that's a great place to get started. Um, you could look at the tutorial, you can look at the video, or you can reach out to us on Piazza.",
    "start": "113690",
    "end": "121400"
  },
  {
    "text": "Any other questions? All right. So, we're gonna go ahead and get started.",
    "start": "121400",
    "end": "127520"
  },
  {
    "text": "Um, uh, what we're gonna be covering today is sort of a very brief overview about Deep Learning, um, as well as Deep Q Learning.",
    "start": "127520",
    "end": "133959"
  },
  {
    "text": "Um So, in terms of where we are in the class, we've been, we have been discussing how to learn to make decisions in",
    "start": "133960",
    "end": "141140"
  },
  {
    "text": "the world when we don't know the dynamics model of the Reward Model in advance. Um, and last week, we were, we were discussing value function approximation,",
    "start": "141140",
    "end": "148250"
  },
  {
    "text": "particularly linear value function approximation. And today we're gonna start to talk about other forms of value function approximation in particular,",
    "start": "148250",
    "end": "154610"
  },
  {
    "text": "um, uh, using deep neural networks. So, the- why do we wanna do this at all?",
    "start": "154610",
    "end": "160560"
  },
  {
    "start": "158000",
    "end": "237000"
  },
  {
    "text": "Well, the reasons we wanted to start thinking about, uh, er, using function approximators is that if we wanna be able to use",
    "start": "160560",
    "end": "166340"
  },
  {
    "text": "reinforcement learning to tackle really complex carry problems. Um, we need to be able to deal with the fact that often we're gonna have",
    "start": "166340",
    "end": "172850"
  },
  {
    "text": "very high dimensional input signals or observations. Um, so, we wanna be able to deal with sort of pixel input,",
    "start": "172850",
    "end": "179090"
  },
  {
    "text": "like images, or we wanna be able to deal with really complex, um, information about customers, or patients,",
    "start": "179090",
    "end": "184220"
  },
  {
    "text": "or students, um, where we might have enormous state and our actions spaces. I'll note today that we're mostly not gonna talk so much about enormous action spaces,",
    "start": "184220",
    "end": "193069"
  },
  {
    "text": "but we are gonna think a lot about really large state spaces. And so, when we started talking about those,",
    "start": "193070",
    "end": "198620"
  },
  {
    "text": "I was arguing that we either need representations of models. Those mean that's sort of the dynamics, or the reward models.",
    "start": "198620",
    "end": "204290"
  },
  {
    "text": "T, T or R or a state-action values Q,",
    "start": "204290",
    "end": "210659"
  },
  {
    "text": "or V, or our policies, um, that can generalize across states and our actions.",
    "start": "210660",
    "end": "216080"
  },
  {
    "text": "With the idea being that we may in fact never encountered the exact same state again. You might never see the exact same image of the world again,",
    "start": "216080",
    "end": "222785"
  },
  {
    "text": "um, but we wanna be able to generalize from our past experience. And so, we thought about it, instead of having a table to represent our value functions, um,",
    "start": "222785",
    "end": "229910"
  },
  {
    "text": "we were gonna use this generic function approximator where we have a W now, which are some parameters.",
    "start": "229910",
    "end": "235370"
  },
  {
    "text": "[NOISE] And when we thought about doing this, we said what we're gonna focus on is we're gonna focus on",
    "start": "235370",
    "end": "242120"
  },
  {
    "start": "237000",
    "end": "303000"
  },
  {
    "text": "function approximations that are differentiable. Um, and the nice thing about differentiable rep-representations is that we can use our data,",
    "start": "242120",
    "end": "249500"
  },
  {
    "text": "and we can estimate our parameters, and then we can take use gradient descent to try to fit our function,",
    "start": "249500",
    "end": "254555"
  },
  {
    "text": "to try to write that into write, or represent our q function or a value function. So, I mentioned last time that most the time we're",
    "start": "254555",
    "end": "261590"
  },
  {
    "text": "gonna think about trying to quantify the fit of our function, compared to the true value as, um, a mean squared error.",
    "start": "261590",
    "end": "267574"
  },
  {
    "text": "So, we can define our loss j, and we can use gradient descent on that to try to find the parameters w that optimize.",
    "start": "267575",
    "end": "274655"
  },
  {
    "text": "And just as a reminder stochastic gradient descent was useful because when we could just slowly update our parameters as we get more information.",
    "start": "274655",
    "end": "282995"
  },
  {
    "text": "And that information now could be [NOISE] in the form of episodes or it [NOISE] could be individual tuples. [NOISE] When I say a tuple,",
    "start": "282995",
    "end": "288905"
  },
  {
    "text": "I generally mean a state-action reward next state tuple.",
    "start": "288905",
    "end": "295375"
  },
  {
    "text": "Um, and the nice thing is that the expected stochastic gradient descent is the same as the full gradient update.",
    "start": "295375",
    "end": "302789"
  },
  {
    "start": "303000",
    "end": "446000"
  },
  {
    "text": "So, just to remind ourselves, last time we were talking about linear value function approximations.",
    "start": "303670",
    "end": "309155"
  },
  {
    "text": "Um, and that meant that what we're gonna do is, we're gonna have a whole bunch of features to describe our world, um,",
    "start": "309155",
    "end": "314330"
  },
  {
    "text": "as so, you know, these features we will input our state, state as the real state of the world and we would output our features.",
    "start": "314330",
    "end": "321470"
  },
  {
    "text": "And so, this could be things like a laser range finder for our robot,",
    "start": "321470",
    "end": "326875"
  },
  {
    "text": "which told us how far away the walls were in all 180 deg- degree directions.",
    "start": "326875",
    "end": "332300"
  },
  {
    "text": "We talked about the fact that that was an aliased version of the world because, um, multiple hallways might, might look identical.",
    "start": "332300",
    "end": "338690"
  },
  {
    "text": "So, our value function now is a dot product between those features, that we've got out about the world, um, with the weights.",
    "start": "338690",
    "end": "345264"
  },
  {
    "text": "Our objective function is again the mean squared error. And then we could do this same weight update. And the key hard thing was that we don't know what this is.",
    "start": "345265",
    "end": "353920"
  },
  {
    "text": "So, this is the true value of a policy. And the problem is we don't know what the true value of a policy is,",
    "start": "353920",
    "end": "360910"
  },
  {
    "text": "otherwise we wouldn't have to be doing all of this learning. Um, and so we needed to have different ways to approximate it.",
    "start": "360910",
    "end": "366755"
  },
  {
    "text": "And so, the two ways we talked about last time was inspired by a work on Monte Carlo,",
    "start": "366755",
    "end": "372010"
  },
  {
    "text": "or on TD learning is we could either plug-in the return from the full episode.",
    "start": "372010",
    "end": "378490"
  },
  {
    "text": "This is the sum of her words.",
    "start": "378490",
    "end": "381710"
  },
  {
    "text": "Or we could put in a bootstrapped return. So, now we're doing bootstrapping.",
    "start": "384140",
    "end": "390500"
  },
  {
    "text": "Where we look at the reward, the next state, and the value of our next state.",
    "start": "393170",
    "end": "398425"
  },
  {
    "text": "And in this case we're using a linear value function approximators for everything, which gave us a really simple form of what the derivative is,",
    "start": "398425",
    "end": "405250"
  },
  {
    "text": "of this function with respect to W. Basically it's just our features times essentially this prediction error.",
    "start": "405250",
    "end": "413485"
  },
  {
    "text": "So, people sometimes call this is the prediction error. Cause it's the difference between the value,",
    "start": "413485",
    "end": "421259"
  },
  {
    "text": "or right now we're using GT as the true value. Of course in reality it's just a sample of the value but, well,",
    "start": "421260",
    "end": "427479"
  },
  {
    "text": "it's the difference between, um, the true value and our estimated value. I'm gonna shrink that difference.",
    "start": "427480",
    "end": "433960"
  },
  {
    "text": "So, in this case I've written, um, these equations of all for linear value function approximation,",
    "start": "433960",
    "end": "438980"
  },
  {
    "text": "but there are some limitations to use the linear value function approximation, even though this has been probably the most well-studied.",
    "start": "438980",
    "end": "445530"
  },
  {
    "start": "446000",
    "end": "686000"
  },
  {
    "text": "So, if you have the right set of features, and historically there was a lot of work on",
    "start": "446120",
    "end": "452150"
  },
  {
    "text": "figuring out what those rights set of features are. They often worked really well. And in fact when we get into,",
    "start": "452150",
    "end": "457425"
  },
  {
    "text": "I think I mentioned briefly before. When we start to talk about deep neural networks you can think of, a deep neural network is just a really complicated way to get out features,",
    "start": "457425",
    "end": "464900"
  },
  {
    "text": "plus the last layer being a linear combination of those features. For most of the time when we're talking about deep RL with,",
    "start": "464900",
    "end": "471770"
  },
  {
    "text": "um, a deep neural networks represent the Q function. That's the type of representation will be looking at. So, linear value function is often",
    "start": "471770",
    "end": "478205"
  },
  {
    "text": "really works very well if you're the right set of features, but is this challenge of what is the right set of features.",
    "start": "478205",
    "end": "483305"
  },
  {
    "text": "Um, and there are all sorts of implications about whether or not we're even gonna be able to write down the true p- um,",
    "start": "483305",
    "end": "489500"
  },
  {
    "text": "value function using our set of features, and how easy is it for us to converge to that.",
    "start": "489500",
    "end": "494845"
  },
  {
    "text": "So, one alternative that we didn't talk so much about last time is to use sort of a really, really rich function approximator class.",
    "start": "494845",
    "end": "501905"
  },
  {
    "text": "Um, where we don't have to, have to have a direct representation of the features. Er, and some of those are Kernel based approaches.",
    "start": "501905",
    "end": "509550"
  },
  {
    "text": "Um, has anybody seen like, ah, Kernel based approaches before? Or like k-nearest neighbor type approaches?",
    "start": "509550",
    "end": "516000"
  },
  {
    "text": "If you take a machine learning you've heard of k-nearest neighbors, those are sort of these non-parametric approaches, where your representation size tends to grow with the number of data points.",
    "start": "516000",
    "end": "524450"
  },
  {
    "text": "Um, and then they can be really nice and they have some actually really nice convergence properties for reinforcement learning.",
    "start": "524450",
    "end": "530704"
  },
  {
    "text": "The problem is, um, that the number of data points you need tends to scale with the dimension. So, um, if you have let's say those 180, um, features,",
    "start": "530705",
    "end": "541589"
  },
  {
    "text": "um, the number of points you need to tile that 180 degrees space, generally scales exponentially with the dimension.",
    "start": "541590",
    "end": "548350"
  },
  {
    "text": "So, that's not so appealing both in terms of computational complexity, memory requirements, and sample complexity.",
    "start": "548350",
    "end": "555580"
  },
  {
    "text": "So, these actually have a lot stronger convergence results compared to linear value function approximators.",
    "start": "555580",
    "end": "561440"
  },
  {
    "text": "Um, but they haven't far been used for in a very widespread way. Yeah. Um, and everyone just [inaudible] name first please to stop me.",
    "start": "561440",
    "end": "568875"
  },
  {
    "text": "Yes. Yeah. [LAUGHTER] So, can you repeat again why the exponential behavior happening?",
    "start": "568875",
    "end": "573955"
  },
  {
    "text": "Yeah, student's question was why does the exponential behavior happen and a lot of these sort of kernel based approximators or non-parametric.",
    "start": "573955",
    "end": "580165"
  },
  {
    "text": "The intuition is that if you want to have sort of an accurate representation of your value function,",
    "start": "580165",
    "end": "587350"
  },
  {
    "text": "um, and you're representing it by say, uh, local points around it. For example, like, with the k-nearest neighbor approach.",
    "start": "587350",
    "end": "593470"
  },
  {
    "text": "then the number of points you need to have everything be close like in an epsilon ball scales with the- the dimensionality.",
    "start": "593470",
    "end": "600460"
  },
  {
    "text": "So, basically you're just gridding the space. So, if you think of -- if",
    "start": "600460",
    "end": "607629"
  },
  {
    "text": "you think you have sort of- if you want to have any point on this line, be close, then you could put a point here and a point here in order to have",
    "start": "607630",
    "end": "616330"
  },
  {
    "text": "everything be sort of epsilon close for all points on that line to have, uh, a neighbor that's within epsilon distance.",
    "start": "616330",
    "end": "622945"
  },
  {
    "text": "If you want to have it in a square, you're gonna need four points so that everything can be somewhat close to one of the points.",
    "start": "622945",
    "end": "629680"
  },
  {
    "text": "Generally, the number of points you need this going to scale exponentially with the dimension. [NOISE] But they are really nice, um, uh,",
    "start": "629680",
    "end": "637870"
  },
  {
    "text": "because they can be guaranteed to be averagers which we talked about really briefly last time that views a linear value function approximator.",
    "start": "637870",
    "end": "645100"
  },
  {
    "text": "Um, when you do a bellman backup, it's not necessarily a contraction operator anymore which is why you can sometimes blow up as you do more and more backups.",
    "start": "645100",
    "end": "652960"
  },
  {
    "text": "A really cool thing about averagers is sort of by their name. Um, when you use this type of approximation,",
    "start": "652960",
    "end": "658329"
  },
  {
    "text": "you don't- they're guaranteed to be- to be a non-expansion, which means that when you combine them with a bellman backup it's guaranteed it'd still be a contraction which is really cool.",
    "start": "658329",
    "end": "666355"
  },
  {
    "text": "So, that means these sort of approximators are guaranteed to converge compared to a lot of other ones.",
    "start": "666355",
    "end": "671620"
  },
  {
    "text": "All right, but they're not gonna scale very well and in practice you don't tend to see them, though there's some really cool work by my colleague, Finale Doshi-Velez,",
    "start": "671620",
    "end": "678190"
  },
  {
    "text": "over at Harvard who's thinking about using these for things like, um, health care applications and how do you sort of generalized from related patients.",
    "start": "678190",
    "end": "684535"
  },
  {
    "text": "So, they can be useful but they generally don't scale so well. So, what we're gonna talk about today is thinking about deep neural networks which also",
    "start": "684535",
    "end": "691885"
  },
  {
    "start": "686000",
    "end": "935000"
  },
  {
    "text": "have very flexible representations but we hope we're gonna scale a lot better. Um, now, in general we're going to have",
    "start": "691885",
    "end": "698199"
  },
  {
    "text": "almost no theoretical guarantees for the rest of the day, um, and- but in practice they often work really really well.",
    "start": "698200",
    "end": "705235"
  },
  {
    "text": "So, they become an incredibly useful tool in reinforcement learning and everywhere else really in terms of machine learning.",
    "start": "705235",
    "end": "711010"
  },
  {
    "text": "So, what do we mean by deep neural networks? Well, a number of you guys are experts but, um, what it generally means in this case is we're just gonna think of com- making",
    "start": "711010",
    "end": "718510"
  },
  {
    "text": "a function approximator which is a composition of a number of functions. So, we're gonna have our input x and I'm gonna feed it into",
    "start": "718510",
    "end": "726160"
  },
  {
    "text": "some function which is gonna take in some weights.",
    "start": "726160",
    "end": "731319"
  },
  {
    "text": "So, in general, all of these things can be vectors. So, you're gonna take in some weights and combine them with your x and then you're going to push",
    "start": "731320",
    "end": "738550"
  },
  {
    "text": "them into some function and then you're gonna output something which is probably gonna be also a vector.",
    "start": "738550",
    "end": "743890"
  },
  {
    "text": "Then you're gonna push that into another function, and throw in some more weights.",
    "start": "743890",
    "end": "750040"
  },
  {
    "text": "I'm gonna do that a whole bunch of times, and then at the very end of that you can output",
    "start": "750040",
    "end": "756820"
  },
  {
    "text": "some y which you could think of as being like our Q. Then, we can output that to some loss function j.",
    "start": "756820",
    "end": "763285"
  },
  {
    "text": "So, what does that mean here? It means that y is equal to hn of hn",
    "start": "763285",
    "end": "770110"
  },
  {
    "text": "minus one dot dot dot dot dot h1",
    "start": "770110",
    "end": "775660"
  },
  {
    "text": "of x. I haven't written all the weights that are going in there but there's a whole bunch of weights too,",
    "start": "775660",
    "end": "782695"
  },
  {
    "text": "and then this is sort of loss function like before and this you can think of as like our Q for example.",
    "start": "782695",
    "end": "791770"
  },
  {
    "text": "But these are- happen a lot in unsupervised learning like predicting whether or not something is a cat or not or,",
    "start": "791770",
    "end": "797500"
  },
  {
    "text": "you know, an image, uh, of a particular object, um, or for regression. So, why do we want to do this?",
    "start": "797500",
    "end": "803650"
  },
  {
    "text": "Well, first of all it should be clear that as you compose lots of functions, um, you could represent really complicated functions",
    "start": "803650",
    "end": "809620"
  },
  {
    "text": "by adding and subtracting and taking polynomials and all sorts of things you could do by just composing functions together that this could be",
    "start": "809620",
    "end": "816070"
  },
  {
    "text": "a really powerful space of functions you could represent. But the nice reason to write it down like this",
    "start": "816070",
    "end": "821320"
  },
  {
    "text": "is that you can do the chain rule to try to do stochastic gradient descent. So, how does this work?",
    "start": "821320",
    "end": "827200"
  },
  {
    "text": "Well, that means that we can write down that dj. So, we really want, you know,",
    "start": "827200",
    "end": "833125"
  },
  {
    "text": "dj with respect to all these different parameters. So, what we can write down here is we can write down dj of hn and",
    "start": "833125",
    "end": "841170"
  },
  {
    "text": "dhn of dwn and we can do- do this kind of everywhere.",
    "start": "841170",
    "end": "846565"
  },
  {
    "text": "So, dj of h2, dh of h2, and dh2 of dw2.",
    "start": "846565",
    "end": "854665"
  },
  {
    "text": "So, you can use the chain rule to propagate all of this the- the gradient of, um, your loss function with respect to w,",
    "start": "854665",
    "end": "861760"
  },
  {
    "text": "all the way back down all of these different compositions by writing out the chain rule.",
    "start": "861760",
    "end": "866860"
  },
  {
    "text": "Um, so that's nice because it means that you can take our output signal and then propagate that back,",
    "start": "866860",
    "end": "872515"
  },
  {
    "text": "um, in terms of updating all of your weights. Now, I'm gonna date myself. So, when I first learned about deep neural networks, you have to do this by hand.",
    "start": "872515",
    "end": "880345"
  },
  {
    "text": "Um, and, uh, so as you might imagine, this was a less popular assignment and,",
    "start": "880345",
    "end": "886029"
  },
  {
    "text": "uh, it's called backpropagation. So, you can derive this by hand, um, and I'll talk in a second about what these type of functions are, you know,",
    "start": "886030",
    "end": "893980"
  },
  {
    "text": "you need differentiable functions for h. But I think one of the major major innovations that's happened over there, you know, roughly what?",
    "start": "893980",
    "end": "899529"
  },
  {
    "text": "Like last 5 to 8 years is that there's auto differentiation. So, that now, um,",
    "start": "899530",
    "end": "904899"
  },
  {
    "text": "you don't have to derive all of these, uh, gradients by hand instead, you can just write down your network parameter.",
    "start": "904900",
    "end": "910764"
  },
  {
    "text": "Um, and then your network of para- which includes a bunch of parameters and then you have software like,",
    "start": "910765",
    "end": "916840"
  },
  {
    "text": "um, TensorFlow to do all of the differentiation for you. So, I think these sort of tools have made it much much",
    "start": "916840",
    "end": "923080"
  },
  {
    "text": "more practical for people- lots and lots of people to use, um, deep neural networks because you don't- you can have",
    "start": "923080",
    "end": "928930"
  },
  {
    "text": "very very complicated networks so very very large number of layers and there's no sort of hand writing down of what the gradients are.",
    "start": "928930",
    "end": "934460"
  },
  {
    "start": "935000",
    "end": "1095000"
  },
  {
    "text": "So, what are these h functions? Generally, they combine, um, both linear and nonlinear transformations.",
    "start": "935610",
    "end": "941755"
  },
  {
    "text": "Um, basically they just- they have to be differentiable. So, you know, this- this h need to be",
    "start": "941755",
    "end": "947290"
  },
  {
    "text": "differentiable if we're gonna use gradient descent to fit them.",
    "start": "947290",
    "end": "952070"
  },
  {
    "text": "So, the common choices are either linear so you can think of hn is equal to whn minus",
    "start": "952980",
    "end": "960040"
  },
  {
    "text": "one or non-linear where we can think of hn is equal to some function hn minus one.",
    "start": "960040",
    "end": "967225"
  },
  {
    "text": "If it's nonlinear, we often call this an activation function.",
    "start": "967225",
    "end": "971870"
  },
  {
    "text": "Due to time, I'm not gonna talk in class much about the connections with neural networks which is",
    "start": "974370",
    "end": "981220"
  },
  {
    "text": "what inside of our brain which was what's inspiring, these sort of artificial neural networks. Um, but inside of the brain,",
    "start": "981220",
    "end": "987265"
  },
  {
    "text": "people think of there is being the sort of non-linear activation functions where if the signal passes a certain threshold then,",
    "start": "987265",
    "end": "992350"
  },
  {
    "text": "for example, the neuron would fire. So, these sort of non-linear activation functions can be things like",
    "start": "992350",
    "end": "997750"
  },
  {
    "text": "sigmoid functions or ReLU. ReLU's particularly popular right but- um.",
    "start": "997750",
    "end": "1007050"
  },
  {
    "text": "So- so, you can choose different combinations, uh, of linear functions or non-linear functions, um, and as usual we need a loss function at the end.",
    "start": "1007050",
    "end": "1013949"
  },
  {
    "text": "Typically, we use mean squared error. You could also use log likelihood but we need something that- that we can differentiate",
    "start": "1013950",
    "end": "1019154"
  },
  {
    "text": "how close we are achieving that target in order to update our weights. [NOISE] Yeah? Name first.",
    "start": "1019155",
    "end": "1026550"
  },
  {
    "text": "So, this ReLU function is not differentiable, right?",
    "start": "1026550",
    "end": "1031860"
  },
  {
    "text": "It is differentiable, like, you can- you- you can- you can take it to- the- the- differentiable and it's",
    "start": "1031860",
    "end": "1037020"
  },
  {
    "text": "ended up being a lot more popular than sigmoid recently, though I feel like it [OVERLAPPING]. It's not differentiable at one point? Yes.",
    "start": "1037020",
    "end": "1042390"
  },
  {
    "text": "But I don't see how gradient [inaudible] is gonna work on the part where it's flat. Well, if it's flat, it's zero.",
    "start": "1042390",
    "end": "1048615"
  },
  {
    "text": "So, that ends up just- your gradient is just zero. [OVERLAPPING] Yeah. The question is about how for- for ReLU,",
    "start": "1048615",
    "end": "1056595"
  },
  {
    "text": "there's a lot of it where it's flat. Um, and so if your gradient is zero then your gradients can vanish there. Um, in- in- in general actually,",
    "start": "1056595",
    "end": "1063555"
  },
  {
    "text": "we're not gonna talk about this at all in class but, uh, um, there's certainly a problem is you start having very deep neural networks.",
    "start": "1063555",
    "end": "1069615"
  },
  {
    "text": "Um, but because of some of these functions you can sometimes end up sort of having, um, almost no signal going back to the- the earlier layers.",
    "start": "1069615",
    "end": "1077370"
  },
  {
    "text": "But I- I'm not gonna talk about any of that. We'll talk- we'll talk some about that in sessions. Um, they're good to be aware of, um,",
    "start": "1077370",
    "end": "1083460"
  },
  {
    "text": "and we're also happy to give other pointers. But yeah, if it's flat, it's okay, you can still just have, uh, a zero derivative.",
    "start": "1083460",
    "end": "1092565"
  },
  {
    "text": "Okay. All right. So, why do we want to do this?",
    "start": "1092565",
    "end": "1098070"
  },
  {
    "start": "1095000",
    "end": "1184000"
  },
  {
    "text": "Well, it's nice if we can use this sort of like much more complicated representation. Um, another thing is that, um,",
    "start": "1098070",
    "end": "1103935"
  },
  {
    "text": "if you have at least one hidden layer, um, if you have a sufficient number of nodes.",
    "start": "1103935",
    "end": "1109065"
  },
  {
    "text": "Um, nodes you can think of as a- if you're not familiar with this is basically just sort of a sufficiently complicated,",
    "start": "1109065",
    "end": "1114525"
  },
  {
    "text": "uh, set of, uh, combination of features, um, and functions. Um, this is a universal function approximators which means that you",
    "start": "1114525",
    "end": "1121530"
  },
  {
    "text": "can represent any function with the deep neural network. So, that's really nice. We're not gonna have any capacity problems if we use",
    "start": "1121530",
    "end": "1126860"
  },
  {
    "text": "a sufficiently expressive function approximators. Um, and that's important because if you",
    "start": "1126860",
    "end": "1132335"
  },
  {
    "text": "think about what we're doing with linear value function approximators, it was clearly the case sometimes that you might have too limited features and you",
    "start": "1132335",
    "end": "1138559"
  },
  {
    "text": "just wouldn't be able to express the true value function for some states. What the universal function approximator, um,",
    "start": "1138560",
    "end": "1145410"
  },
  {
    "text": "property is stating is that that will not occur for, um, uh, deep neural network if it is, uh, sufficiently rich.",
    "start": "1145410",
    "end": "1153480"
  },
  {
    "text": "All right. Now, of course, you can always think of doing a linear value function approximator with very very rich features and then that becomes equivalent.",
    "start": "1153480",
    "end": "1160275"
  },
  {
    "text": "So, given that, you know, what's another benefit, um, another benefit is that potentially you can use exponentially less nodes or",
    "start": "1160275",
    "end": "1167070"
  },
  {
    "text": "parameters compared to using a shallow net which means not as many of those compositions, um, to represent the same function and that's pretty elegant and,",
    "start": "1167070",
    "end": "1175110"
  },
  {
    "text": "uh, I'm happy to talk about that offline or- or we can talk about on Piazza. Then the final thing is that you can learn the parameters using stochastic gradient descent.",
    "start": "1175110",
    "end": "1182860"
  },
  {
    "text": "All right. So, that's pretty much that, you know, deep neural networks in like five seconds. Um, we're now gonna talk a little bit about convolutional neural networks.",
    "start": "1184040",
    "end": "1192600"
  },
  {
    "text": "Um, and again this is all gonna be a pretty light introduction because you're not gonna need to know the details",
    "start": "1192600",
    "end": "1198300"
  },
  {
    "text": "in order to do the homework except for mostly the fact of understanding that these are sort of very,",
    "start": "1198300",
    "end": "1203879"
  },
  {
    "text": "um, expressive function approximators. So, why do we care about convolutional neural networks?",
    "start": "1203880",
    "end": "1210510"
  },
  {
    "start": "1207000",
    "end": "1228000"
  },
  {
    "text": "Um, well, they're used very extensively in computer vision, um, and if we're interested in having robots",
    "start": "1210510",
    "end": "1215580"
  },
  {
    "text": "and other sorts of agents that can interact in the real world, one of our primary sensory modalities is vision, um,",
    "start": "1215580",
    "end": "1221039"
  },
  {
    "text": "and it's very likely that we're going to want to be able to use similar sorts of input on our- our robots in our artificial agents.",
    "start": "1221040",
    "end": "1227290"
  },
  {
    "text": "So, if you think about this, um, think about there being an image, in this case, of Einstein.",
    "start": "1227420",
    "end": "1233595"
  },
  {
    "start": "1228000",
    "end": "1369000"
  },
  {
    "text": "Um, and there's a whole bunch of different pixels on Einstein, um, of this picture of Einstein.",
    "start": "1233595",
    "end": "1238995"
  },
  {
    "text": "Let's say it's 1,000 by 1,000. So, it's 1,000 by 1,000, you know, x and y. So, we have 10 to the 6 pixels.",
    "start": "1238995",
    "end": "1249420"
  },
  {
    "text": "So, this standard often called feedforward deep neural network. Um, you would have all of those pixels, um,",
    "start": "1249420",
    "end": "1258554"
  },
  {
    "text": "and then they would be going as input to another layer and, um, you might want to have a bunch of different nodes that are taking input from",
    "start": "1258555",
    "end": "1265350"
  },
  {
    "text": "all of those and so you can get a huge number of weights. So, you might have 10 to the 6 weights per- the st- the- often,",
    "start": "1265350",
    "end": "1274470"
  },
  {
    "text": "we think about sort of- I know I haven't given you enough details about this, but often we think of there as being sort of this deep neural network where we have many functions in parallel.",
    "start": "1274470",
    "end": "1283080"
  },
  {
    "text": "So, it's not just like a single line but we might have x going into, uh, h1,",
    "start": "1283080",
    "end": "1288945"
  },
  {
    "text": "h2, h3, h4 then all of those would then be going in some complicated way to some other functions.",
    "start": "1288945",
    "end": "1295649"
  },
  {
    "text": "So, you can have lots of sort of functions being computed in parallel. So, you can imagine your image goes and you've got",
    "start": "1295650",
    "end": "1302820"
  },
  {
    "text": "one function that computes some aspect of the image and another function that compute some other aspect of the image and then you're gonna combin- combine those in all sorts of complicated ways.",
    "start": "1302820",
    "end": "1310485"
  },
  {
    "text": "So, what this is saying is, well for that very first one there's maybe gonna be, you know, a whole bunch of n different functions were computing of the image.",
    "start": "1310485",
    "end": "1316995"
  },
  {
    "text": "There'd be 10 to the 6 parameters here. So, if we have these weight times x,",
    "start": "1316995",
    "end": "1323275"
  },
  {
    "text": "then that would be 10 to the 6 parameters to take in all of that x. That's a lot.",
    "start": "1323275",
    "end": "1328695"
  },
  {
    "text": "Um, and then if we want to do this for doing different types of weights all in parallel, then that's gonna be a very very large number of",
    "start": "1328695",
    "end": "1334515"
  },
  {
    "text": "parameters and we do have a lot of data off it now but that's still an enormous number of parameters to- to",
    "start": "1334515",
    "end": "1341350"
  },
  {
    "text": "represent and it also sort of misses some of the point of what we often think about with vision. So, if we think about doing this many times and having lots of hidden units,",
    "start": "1341350",
    "end": "1350460"
  },
  {
    "text": "we can get a really an enormous number of parameters. Um, so to avoid sort of",
    "start": "1350460",
    "end": "1357030"
  },
  {
    "text": "this space-time complexity and the fact that we're sort of ignoring the structure of images, convolutional neural networks try to have a particular form of",
    "start": "1357030",
    "end": "1364680"
  },
  {
    "text": "deep neural network that tries to think about the properties of images. So, in particular, images often have structure, um,",
    "start": "1364680",
    "end": "1373140"
  },
  {
    "start": "1369000",
    "end": "1427000"
  },
  {
    "text": "in the way that our- our brain promises images also has structure and this sort of distinctive features in space and frequency.",
    "start": "1373140",
    "end": "1379930"
  },
  {
    "text": "So, when you have a convolutional neural network, we think of there being particular types of operators.",
    "start": "1379970",
    "end": "1385635"
  },
  {
    "text": "Having so operators again here are like our functions, h1 and hn, which I said before could either be linear or nonlinear and then",
    "start": "1385635",
    "end": "1393840"
  },
  {
    "text": "convolutional neural network learn a particular structures for those, um, uh, for those functions to try to sort of",
    "start": "1393840",
    "end": "1400020"
  },
  {
    "text": "think about the properties that we might want to be extracting from images and kind of the key aspects here is that",
    "start": "1400020",
    "end": "1405375"
  },
  {
    "text": "we're gonna do a lot of weight sharing to do parameter reduction. So, instead of saying,",
    "start": "1405375",
    "end": "1411345"
  },
  {
    "text": "\"I'm going to have totally different parameters each taking in all of the pixels.\"",
    "start": "1411345",
    "end": "1416565"
  },
  {
    "text": "I'm gonna end up having sort of local parameters that are identical and then I apply them to different parts of the image",
    "start": "1416565",
    "end": "1422610"
  },
  {
    "text": "to try to extract, for example, features. Because ultimately, the point of doing this is gonna be trying to extracting",
    "start": "1422610",
    "end": "1430230"
  },
  {
    "start": "1427000",
    "end": "1470000"
  },
  {
    "text": "features that we think are gonna be useful for either predicting things like whether or not, you know, a face isn't an image or that are gonna",
    "start": "1430230",
    "end": "1436020"
  },
  {
    "text": "help us in terms of understanding what the Q function should be. So, the key idea- one of the key ideas is to say that we're gonna have",
    "start": "1436020",
    "end": "1443070"
  },
  {
    "text": "a filter or a receptive field which is that we're gonna have some hidden unit. Um, so it's gonna be a function that's applied to some previous input.",
    "start": "1443070",
    "end": "1451005"
  },
  {
    "text": "At the beginning, that's just gonna be a subset of our image and instead of, um, taking in the whole image,",
    "start": "1451005",
    "end": "1456030"
  },
  {
    "text": "we're just going to take in part. So, we're just gonna take in a patch. So, we're gonna take the upper corner and we're gonna take the middle.",
    "start": "1456030",
    "end": "1463650"
  },
  {
    "text": "So, it's like we're just gonna try to compute some properties of a particular patch of the image.",
    "start": "1463650",
    "end": "1469990"
  },
  {
    "start": "1470000",
    "end": "1597000"
  },
  {
    "text": "So, then we can imagine taking, it's often called a filter, that little, um,",
    "start": "1470510",
    "end": "1476085"
  },
  {
    "text": "those set of weights that we're applying to that patch and we could do that all over the image, um, and we often called the- there's a stride which means sort of how much you move,",
    "start": "1476085",
    "end": "1485100"
  },
  {
    "text": "um, that little patch at each time point. There's also this thing called zero-padding which is how many zeros to add on",
    "start": "1485100",
    "end": "1490860"
  },
  {
    "text": "each input layer and this determines sort of help, um, helps determine what your output is.",
    "start": "1490860",
    "end": "1496860"
  },
  {
    "text": "So in this case, if you have an input of 28 by 28 and you have a little five-by-five patch that you're going to slide over the entire image,",
    "start": "1496860",
    "end": "1503910"
  },
  {
    "text": "then you're gonna end up with a 24 by 24 layer next because basically you just take this and then you move it over a little bit.",
    "start": "1503910",
    "end": "1511559"
  },
  {
    "text": "You move it over, and each of those times you're gonna take those 25.",
    "start": "1511560",
    "end": "1518385"
  },
  {
    "text": "So, this is five-by-five so you're gonna have 25 input x's and you're gonna dot-product them with some weights and that's gonna give you an output.",
    "start": "1518385",
    "end": "1526860"
  },
  {
    "text": "So, here in this case that means we're gonna need 25 weights.",
    "start": "1526860",
    "end": "1531640"
  },
  {
    "text": "Okay. So, one thing is instead of having our full x input, we're just gonna take in- we're gonna direct different parts of the x",
    "start": "1531920",
    "end": "1539250"
  },
  {
    "text": "input to different neurons which you can think of just different functions. Um, but the other nice idea here is that",
    "start": "1539250",
    "end": "1545745"
  },
  {
    "text": "we're going to have the same weights for everything. So, when we took those weights we're going to have sort of,",
    "start": "1545745",
    "end": "1552840"
  },
  {
    "text": "um, you can think of them as trying to extract a feature from that sub patch of the image. For example, whether or not there's an edge.",
    "start": "1552840",
    "end": "1559320"
  },
  {
    "text": "So, you can imagine I'm trying to detect whether or not there's something that looks like a horizontal edge in that part of the image",
    "start": "1559320",
    "end": "1565215"
  },
  {
    "text": "and I try to- and that is determined by the weights I'm specifying and I just move that over my entire image to see whether or not it's present.",
    "start": "1565215",
    "end": "1572010"
  },
  {
    "text": "So, now the weights are identical, and you're just moving them over the entire image. So, instead of having,",
    "start": "1572010",
    "end": "1577500"
  },
  {
    "text": "um, you know, 10 to the 6 weights, I might only having 25 weights and I'm applying those to the same- uh,",
    "start": "1577500",
    "end": "1583815"
  },
  {
    "text": "just applying them to lots of different parts of the image. Okay. So, this is sort of what that would look like.",
    "start": "1583815",
    "end": "1590340"
  },
  {
    "text": "You sort of have this input, you go to the hi- um, the hidden layer and, yeah, you're sort of do- also down-sampling the image.",
    "start": "1590340",
    "end": "1595890"
  },
  {
    "text": "Um. Why would you want to do this?",
    "start": "1595890",
    "end": "1601860"
  },
  {
    "start": "1597000",
    "end": "1656000"
  },
  {
    "text": "Well, we think that often, the brain is doing this. It's trying to pick up different sort of features. In fact, a lot of computer vision before deep learning was,",
    "start": "1601860",
    "end": "1607980"
  },
  {
    "text": "um, trying to construct these special sorts of features, things like sift features or other features that",
    "start": "1607980",
    "end": "1613200"
  },
  {
    "text": "they really think captures sort of important properties of the image, but they're also may be invariant to things like translation.",
    "start": "1613200",
    "end": "1619665"
  },
  {
    "text": "Because we also think that, you know, whether I'm looking at, um, the world like this, or I move my head slightly, um,",
    "start": "1619665",
    "end": "1625335"
  },
  {
    "text": "that the features that I see are often gonna be identical, whether I moved to the left or right a little bit. There are particular salient aspects of the world that are gonna",
    "start": "1625335",
    "end": "1632640"
  },
  {
    "text": "be relevant for detecting whether or not there's a face, and relevant for deciding my value function. So, we want to sort of extract features that we think are gonna",
    "start": "1632640",
    "end": "1640020"
  },
  {
    "text": "represent this sort of translation in variance.",
    "start": "1640020",
    "end": "1643660"
  },
  {
    "text": "This means also that rather than just computing, uh, you- you can do this. You'll use the same weights all the way across the feature, er,",
    "start": "1645170",
    "end": "1651675"
  },
  {
    "text": "all across the image and then you can do this for multiple different types of features. So there's a really nice, um,",
    "start": "1651675",
    "end": "1659175"
  },
  {
    "start": "1656000",
    "end": "1693000"
  },
  {
    "text": "discussion of this that goes into more depth from 231-n, which some of you guys might've taken. Um, and there's a nice animation where they show, okay,",
    "start": "1659175",
    "end": "1666270"
  },
  {
    "text": "imagine you have your input, you can think of this as an image, and then you could apply these different filters on top of it,",
    "start": "1666270",
    "end": "1672425"
  },
  {
    "text": "which you can think of as trying to detect different features, and then you move them around your image, and see whether or not that feature is present anywhere.",
    "start": "1672425",
    "end": "1678665"
  },
  {
    "text": "So you can do that with multiple different fype- types of filters. You could think of this as trying to look for whether something's like that",
    "start": "1678665",
    "end": "1683795"
  },
  {
    "text": "or something's horizontal or vertical, different types of edges, um, and, uh, these give you different features essentially that are been extracted.",
    "start": "1683795",
    "end": "1692895"
  },
  {
    "text": "Um, the other really important thing in CNNs, is what are known as pooling layers. They are often used as a way to sort of down-sample the image.",
    "start": "1692895",
    "end": "1700575"
  },
  {
    "start": "1693000",
    "end": "1732000"
  },
  {
    "text": "So you can do things like max pooling to detect whether or not a particular feature is present, um,",
    "start": "1700575",
    "end": "1705794"
  },
  {
    "text": "or take averages or other ways to kind of just down, ah, and compress the, the information that you got it in.",
    "start": "1705795",
    "end": "1711915"
  },
  {
    "text": "So, just remember in this case and in many cases, we're gonna start with a really high dimensional input,",
    "start": "1711915",
    "end": "1716925"
  },
  {
    "text": "like x might be an image and output, a scalar, like, um, you know, the Q value.",
    "start": "1716925",
    "end": "1723405"
  },
  {
    "text": "So we're somehow gonna have to go for really high dimensional input and kind of average and slow down until we can get to,",
    "start": "1723405",
    "end": "1729075"
  },
  {
    "text": "um, a low dimensional output. So, the final layer is typically fully connected.",
    "start": "1729075",
    "end": "1735315"
  },
  {
    "start": "1732000",
    "end": "1775000"
  },
  {
    "text": "So we can again think about all of these previous processes as essentially computing some new feature representation,",
    "start": "1735315",
    "end": "1742320"
  },
  {
    "text": "so essentially from here to here. We're kind of computing this new feature representation of the image,",
    "start": "1742320",
    "end": "1748245"
  },
  {
    "text": "and at the very end, we can take some fully connected layer, where it's like doing linear regression,",
    "start": "1748245",
    "end": "1754155"
  },
  {
    "text": "and use that to output predictions or scalars. Again, I know either for some of you, guys,",
    "start": "1754155",
    "end": "1760410"
  },
  {
    "text": "this is sort of a quick shallow refresher. For others of you, this is clearly not in, ah, this is, ah, would be a whirlwind introduction, um,",
    "start": "1760410",
    "end": "1767625"
  },
  {
    "text": "but we won't be requiring you to know a lot of these details. And again, just go to a session, if you have some questions and feel free to reach out to us.",
    "start": "1767625",
    "end": "1774850"
  },
  {
    "text": "Okay. So these type of representations, both Deep Neural Networks and Convolutional Neural Networks,",
    "start": "1775190",
    "end": "1780809"
  },
  {
    "text": "are both used extensively in deep reinforcement learning. So it was around in 2014, um,",
    "start": "1780810",
    "end": "1787799"
  },
  {
    "text": "where I- the workshop where David Silver started talking about, um, how we could use these type of approximations for Atari.",
    "start": "1787800",
    "end": "1795225"
  },
  {
    "start": "1788000",
    "end": "2139000"
  },
  {
    "text": "So why was the surprising? I just sort of wandering back. So in around 1994, personally in 1994,",
    "start": "1795225",
    "end": "1802770"
  },
  {
    "text": "um, we had TD backgammon which used Deep Neural Networks.",
    "start": "1802770",
    "end": "1808680"
  },
  {
    "text": "Well, they used neural networks. I think there was someone that deep, and I think out like a world-class backgammon player out of that.",
    "start": "1808680",
    "end": "1816120"
  },
  {
    "text": "So, that was pretty early on. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that,",
    "start": "1816120",
    "end": "1825929"
  },
  {
    "text": "\"Function approximation plus offline off policy control,",
    "start": "1825930",
    "end": "1833680"
  },
  {
    "text": "plus bootstrapping can be bad,",
    "start": "1833960",
    "end": "1840100"
  },
  {
    "text": "can fail to converge.\" So, we talked about this a little last time.",
    "start": "1840530",
    "end": "1846720"
  },
  {
    "text": "That in general, as soon as we start doing this function approximation even with the linear function approximator,",
    "start": "1846720",
    "end": "1852645"
  },
  {
    "text": "um, that when you're combining off policy control, bootstrapping, which means we're doing like TD learning or Q learning,",
    "start": "1852645",
    "end": "1857654"
  },
  {
    "text": "um, and, uh, in functional approximator, then you can start to have this, uh, challenging triad, um, which often means that we're not guaranteed to converge.",
    "start": "1857655",
    "end": "1867090"
  },
  {
    "text": "And even if we're guaranteed to converge, the solution may not be a good one. So sort of there was this early encouraging success and then there were",
    "start": "1867090",
    "end": "1873690"
  },
  {
    "text": "these results in sort of the middle of the nineties that we're trying to better understand this, that indicated that things could be very bad,",
    "start": "1873690",
    "end": "1879750"
  },
  {
    "text": "and the risk was some of the- In addition to the theoretical results, there were sort of these simple test cases, that, you know, these simple cases that went wrong.",
    "start": "1879750",
    "end": "1887820"
  },
  {
    "text": "So, it wasn't just sort of in principle this could happen, uh, but there were cases which failed.",
    "start": "1887820",
    "end": "1895420"
  },
  {
    "text": "And so I think for a long time after that, the, the community was sort of backed away from Deep Neural Networks for a while.",
    "start": "1895490",
    "end": "1901440"
  },
  {
    "text": "People were quite cautious about using them because they were clearly, even simple cases where things started to go really badly with function approximation.",
    "start": "1901440",
    "end": "1907545"
  },
  {
    "text": "And theoretically, people could prove that it could go badly, and so there's less attention to it for quite a while.",
    "start": "1907545",
    "end": "1913559"
  },
  {
    "text": "And then, um, there was the rise of Deep Neural Networks in sort of, you know, the- the mid 2000s, like, to now.",
    "start": "1913560",
    "end": "1922924"
  },
  {
    "text": "So, uh, Deep Neural Networks became huge, and there was called a huge success in them for things like vision,",
    "start": "1922925",
    "end": "1930300"
  },
  {
    "text": "in other areas, there was a whole bunch of data, there's a whole bunch of compute. They we're getting really extraordinary results. And so, then, perhaps it was natural that, like,",
    "start": "1930300",
    "end": "1937140"
  },
  {
    "text": "around in like 2014, DeepMind, DeepMind combined them and had some really amazing successes with Atari.",
    "start": "1937140",
    "end": "1947505"
  },
  {
    "text": "And so I think it sort of really changed the story of how people are perceiving using, um, this sort of complicated function approximation, meters,",
    "start": "1947505",
    "end": "1954120"
  },
  {
    "text": "and RL, and that yes, it can fail to converge. Yes, things can go really badly,",
    "start": "1954120",
    "end": "1959130"
  },
  {
    "text": "and they do go really badly sometimes in practice, but it is also possible of them that despite that- you know,",
    "start": "1959130",
    "end": "1965235"
  },
  {
    "text": "the fact that we don't always fully understand why they always work, um, that often in practice, we can still get pretty good policies out.",
    "start": "1965235",
    "end": "1971310"
  },
  {
    "text": "Now, we often don't know if they're optimal. Often, we know they're not optimal because we know that people can play better, but that doesn't mean that they might not be pretty good,",
    "start": "1971310",
    "end": "1978090"
  },
  {
    "text": "and so we sort of saw this resurgence of interest in- into deep reinforcement learning. Yeah. [NOISE]",
    "start": "1978090",
    "end": "1984659"
  },
  {
    "text": "Um, I guess is there anything from your perspective that the, the deep learning has solved the problems that they had come up with in the mid '90s?",
    "start": "1984660",
    "end": "1993540"
  },
  {
    "text": "Or, is it just that kind of through increases in computational power and the ability to gather a lot of data,",
    "start": "1993540",
    "end": "2000305"
  },
  {
    "text": "that when it failed, it kinda doesn't matter, and we can try some different, like we- you know,",
    "start": "2000305",
    "end": "2005720"
  },
  {
    "text": "try it again and kinda put it together and just keep trying until it works? I guess my question is, did we actually overcome",
    "start": "2005720",
    "end": "2010880"
  },
  {
    "text": "any of the problems that arose in the late '90s, or is it just that we're just kinda powered through? The question is, you know, how we sort of",
    "start": "2010880",
    "end": "2017000"
  },
  {
    "text": "fundamentally resolve some of the issues of the late my '90s, or, um, we kind of brute forcing it. Um, I think that some of the issues that were coming",
    "start": "2017000",
    "end": "2024650"
  },
  {
    "text": "up in 1995 to 1998 in terms of convergence, there are some algorithms now that are more",
    "start": "2024650",
    "end": "2029810"
  },
  {
    "text": "true stochastic gradient algorithms that are covered in chapter 11, um, so that I- a- a- are guaranteed to converge.",
    "start": "2029810",
    "end": "2036530"
  },
  {
    "text": "They may not be guaranteed to converge to the optimal policy, um, so there's still lot of- there's still a ton of work, I think to be done to understand function approximator and off",
    "start": "2036530",
    "end": "2043789"
  },
  {
    "text": "policy control and bootstrapping. I think there's also a couple algorithmic ideas",
    "start": "2043790",
    "end": "2049460"
  },
  {
    "text": "that we're gonna see later in this lecture, that help the performance kind of avoid some of those convergence problems.",
    "start": "2049460",
    "end": "2055445"
  },
  {
    "text": "So I think people knew about this when they started going into 2013, 2014. And so they tried to think about, \"Well,",
    "start": "2055445",
    "end": "2060965"
  },
  {
    "text": "when might this issue happen, and how could we avoid some of that stuff?\" Like, what's causing that? And so at least algorithmically,",
    "start": "2060965",
    "end": "2066619"
  },
  {
    "text": "can we try to make things, that people often talk about stability, so can we try to make sure that the Deep Neural Network doesn't seem to start having weights that are",
    "start": "2066620",
    "end": "2073819"
  },
  {
    "text": "going off towards infinity and at least empirically have sort of more stable performance. Yes, [inaudible] for me.",
    "start": "2073820",
    "end": "2079730"
  },
  {
    "text": "So with the Atari case specifically, did you- did you avoid that problem? Well, sort of, that if you tried it by having on policy control? I just don't know.",
    "start": "2079730",
    "end": "2087500"
  },
  {
    "text": "[inaudible] there wasn't the case that in fact, that in your Deep Neural experiment, they updated the performance to match the Apple policies [inaudible].",
    "start": "2087500",
    "end": "2099530"
  },
  {
    "text": "The question is whether or not in this sort of, um, ah, if I understood correctly, in the Atari case like, you know, where they changed things to be more on policy or,",
    "start": "2099530",
    "end": "2106520"
  },
  {
    "text": "or which we know can be much more stable. Um, ah, they are doing deep learning in, in this case, Deep-Q Learning.",
    "start": "2106520",
    "end": "2112414"
  },
  {
    "text": "And so it can still be very unstable, but they're gonna do something about how they do with the frequency of updates to the networks,",
    "start": "2112415",
    "end": "2117830"
  },
  {
    "text": "to try to make it more stable. Um, and it's a great question for me. We'll see how it works here.",
    "start": "2117830",
    "end": "2122900"
  },
  {
    "text": "Anyone else? Okay, cool. So, um, we'll- we'll see an example for breakout shortly, um, of what they did.",
    "start": "2122900",
    "end": "2129440"
  },
  {
    "text": "Um, so again right now, we're gonna be talking about using Deep Neural Networks to represent the value function.",
    "start": "2129440",
    "end": "2134690"
  },
  {
    "text": "Um, we'll talk about using Deep Neural Networks to represent the policy pretty shortly, next week.",
    "start": "2134690",
    "end": "2140045"
  },
  {
    "start": "2139000",
    "end": "2148000"
  },
  {
    "text": "So what are we going to do? We're gonna, again, have our weights. We're gonna have our same approximators. Now, we're gonna be using Deep Neural Networks.",
    "start": "2140045",
    "end": "2147390"
  },
  {
    "text": "And in this case, again, just, uh, to be clear we're gonna be using a Q function,",
    "start": "2148360",
    "end": "2154475"
  },
  {
    "text": "um, because we're gonna wanna be able to be doing control. So, we're gonna be doing control, in this case.",
    "start": "2154475",
    "end": "2161119"
  },
  {
    "text": "Um, and so, we're gonna need to be learning the, the values of the actions. Just to be clear here, an Atari, it generally,",
    "start": "2161120",
    "end": "2166970"
  },
  {
    "text": "doesn't have a really high dimensional action space. It's discrete. It's normally somewhere between like four to 18, depends on the game.",
    "start": "2166970",
    "end": "2173059"
  },
  {
    "text": "Um, so, it's fairly low dimensional, fairly, um, uh, it's discreet and fairly small. So, though the state space is enormous because it's pixels it's images,",
    "start": "2173060",
    "end": "2181609"
  },
  {
    "text": "um, uh, the, the action space is pretty small. Okay. So, just as a reminder, for Q learning,",
    "start": "2181610",
    "end": "2188869"
  },
  {
    "start": "2185000",
    "end": "2232000"
  },
  {
    "text": "what we saw is Q learning looks like this for our weights. We have to have the derivative of our function.",
    "start": "2188870",
    "end": "2195920"
  },
  {
    "text": "This is not necessarily gonna be linear anymore, um, but the way we updated our weights, was we did this, um,",
    "start": "2195920",
    "end": "2202055"
  },
  {
    "text": "TD backup, where we have this target. Um, but we're now gonna be taking a max over a,",
    "start": "2202055",
    "end": "2208130"
  },
  {
    "text": "over our next state and our action and our weights. Now, notice in this equation,",
    "start": "2208130",
    "end": "2214280"
  },
  {
    "text": "all the W's you see are identical on the right-hand side. So, we're using the same weights to represent our current value and we're using",
    "start": "2214280",
    "end": "2222650"
  },
  {
    "text": "our same weights to plug in and get an estimate of our future value as well as in our derivative.",
    "start": "2222650",
    "end": "2227825"
  },
  {
    "text": "And whether we're gonna see is, is uh, an alternative to that. Okay. So, their idea was,",
    "start": "2227825",
    "end": "2234770"
  },
  {
    "start": "2232000",
    "end": "2469000"
  },
  {
    "text": "we'd really like to be able to use this type of function approximator. These deep function approximators to do Atari.",
    "start": "2234770",
    "end": "2240470"
  },
  {
    "text": "They picked Atari in part. Well, I think at least Dennis and I think, Dennis and David, both had sort of a joint startup on,",
    "start": "2240470",
    "end": "2247130"
  },
  {
    "text": "um, video games, a long time ago. I think it was before David went back to grad school if I remember correctly.",
    "start": "2247130",
    "end": "2252410"
  },
  {
    "text": "Um, so, they're both interest in this it's clearly, uh, games are often hard for people to learn.",
    "start": "2252410",
    "end": "2257720"
  },
  {
    "text": "I'm so, it's a nice sort of, uh, demonstration of intellect and they thought well, we can get access to this and,",
    "start": "2257720",
    "end": "2263465"
  },
  {
    "text": "and there was a paper published. I'm forgetting when, maybe 2011, 2013, talking about Atari games and emulators as being sort of interesting challenge for RL.",
    "start": "2263465",
    "end": "2272630"
  },
  {
    "text": "So, what happens in this case, well, the state is just gonna be the full image. The action is gonna be the equivalent of",
    "start": "2272630",
    "end": "2279260"
  },
  {
    "text": "actions of what you could normally do in the game. This is normally somewhere between four to 18, approximately four to 18 actions.",
    "start": "2279260",
    "end": "2286070"
  },
  {
    "text": "Um, and the reward can be, well, really whatever you want but you can use the score or some other aspect to,",
    "start": "2286070",
    "end": "2292295"
  },
  {
    "text": "um, uh, as proxy reward. Generally we're gonna think about score. So, what's gonna happen?",
    "start": "2292295",
    "end": "2299615"
  },
  {
    "text": "Well, they're gonna use a particular input state. We've talked before about whether or not,",
    "start": "2299615",
    "end": "2304625"
  },
  {
    "text": "um, a representation is Markov. In these games, you typically need to have velocity.",
    "start": "2304625",
    "end": "2311000"
  },
  {
    "text": "So, because you need velocity, you need more than just the current image. So what they chose to do is,",
    "start": "2311000",
    "end": "2317555"
  },
  {
    "text": "you'd need to use four previous frames. So this at least allows you to catch for a velocity and position,",
    "start": "2317555",
    "end": "2324454"
  },
  {
    "text": "observe the balls and things like that. It's not always sufficient.",
    "start": "2324455",
    "end": "2330360"
  },
  {
    "text": "Can anybody think of an example where maybe an Atari game, I don't know how many people played Atari.",
    "start": "2330400",
    "end": "2335465"
  },
  {
    "text": "Um, uh, that might not be sufficient for the last four images still might not be sufficient",
    "start": "2335465",
    "end": "2342140"
  },
  {
    "text": "or the type of game where it might not be sufficient. Yeah.",
    "start": "2342140",
    "end": "2349190"
  },
  {
    "text": "[inaudible].",
    "start": "2349190",
    "end": "2357260"
  },
  {
    "text": "Microbes exactly right. So like things like Montezuma's Revenge, things we often have to get like a key and then you have to grab that key and then, uh,",
    "start": "2357260",
    "end": "2363410"
  },
  {
    "text": "maybe it's visible on screen, it maybe sad, um, and, er, maybe it stored in inventory somewhere. So you have to sort of remember that you have it in order",
    "start": "2363410",
    "end": "2371210"
  },
  {
    "text": "to make the right decision much later or there might be some information you've seen early on. So there are a lot of games and a lot of tasks where, um,",
    "start": "2371210",
    "end": "2377240"
  },
  {
    "text": "the, even the last four frames will not give you the information you need. But it's not about approximation, it's much easier than representing the entire history.",
    "start": "2377240",
    "end": "2384290"
  },
  {
    "text": "So they started with that. Um, er, so, here in this case, there's 80 joystick button positions,",
    "start": "2384290",
    "end": "2390230"
  },
  {
    "text": "um, may or may not need to use all of them in particular game. And the reward can be the change in score.",
    "start": "2390230",
    "end": "2395734"
  },
  {
    "text": "Now notice that that can be very helpful or may not be it, depends on the game. So in some games it takes you a really,",
    "start": "2395735",
    "end": "2401960"
  },
  {
    "text": "really long time to get to anywhere where your score can possibly change. Um, so in that case, you might have a really sparse reward.",
    "start": "2401960",
    "end": "2407825"
  },
  {
    "text": "In other cases, you're gonna win a reward a lot. And so that's gonna be much easier to learn what to do. What are the important things that they, um, did in their paper,",
    "start": "2407825",
    "end": "2415160"
  },
  {
    "text": "this is a nature paper from 2015, is they use the same architecture and hyperparameters across all games.",
    "start": "2415160",
    "end": "2422615"
  },
  {
    "text": "Now just to be clear, they're gonna then learn different Q functions and different policies for each game.",
    "start": "2422615",
    "end": "2428360"
  },
  {
    "text": "But their point was that they didn't have to use totally different architectures, do totally different hyperparameter tuning for",
    "start": "2428360",
    "end": "2433790"
  },
  {
    "text": "every single game separately in order to get it to work. It really was the sort of general, um,",
    "start": "2433790",
    "end": "2439085"
  },
  {
    "text": "architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games.",
    "start": "2439085",
    "end": "2444860"
  },
  {
    "text": "I think it was another nice contribution to this paper is to say well we're going to try to get sort of a general algorithm and setup that is gonna go much",
    "start": "2444860",
    "end": "2451580"
  },
  {
    "text": "beyond the sort of normal three examples that we see in reinforcement learning papers. But just try and get to try to do well in all 50 games.",
    "start": "2451580",
    "end": "2457775"
  },
  {
    "text": "Again, each agent is gonna learn from scratch in each of the 50 games, um,",
    "start": "2457775",
    "end": "2462920"
  },
  {
    "text": "because it's gonna do so with the same basic parameters, same hyperparameters and same neural network so same function approximators act.",
    "start": "2462920",
    "end": "2470915"
  },
  {
    "start": "2469000",
    "end": "2580000"
  },
  {
    "text": "And the nice thing is that, I think this is actually required by nature. They, they released the source code as well. So you can play around with this. So how did they do it?",
    "start": "2470915",
    "end": "2479435"
  },
  {
    "text": "Well, they're gonna do value function approximators. So they're, they're representing the Q function. They're going to minimize the mean squared lost by stochastic gradient descent.",
    "start": "2479435",
    "end": "2487295"
  },
  {
    "text": "Uh, but we know that this can diverge with value function approximators. And what are the two of the problems for this?",
    "start": "2487295",
    "end": "2494015"
  },
  {
    "text": "Well one is that, uh, there is this or the correlation between samples which means that if you have s,",
    "start": "2494015",
    "end": "2503255"
  },
  {
    "text": "a, r, s prime, a prime, r prime, double prime.",
    "start": "2503255",
    "end": "2509510"
  },
  {
    "text": "If you think about what the value function or the return is",
    "start": "2509510",
    "end": "2515180"
  },
  {
    "text": "for us and the value function and the return for S prime, are they independent?",
    "start": "2515180",
    "end": "2521450"
  },
  {
    "text": "No, right. In fact, like you expect them to be highly correlated with their part, you know, I mean, it depends on the probability of S prime.",
    "start": "2521450",
    "end": "2527720"
  },
  {
    "text": "If this is a deterministic system, the only difference between them will be R. So, so these are highly correlated,",
    "start": "2527720",
    "end": "2532880"
  },
  {
    "text": "this is not IID samples when we're doing updates, there's a lot of correlations. Um, and also this issue with non-stationary targets. What does that mean?",
    "start": "2532880",
    "end": "2539990"
  },
  {
    "text": "It means that when you're trying to do your supervised learning and train your value function predictor, um, it's not like you always have",
    "start": "2539990",
    "end": "2546710"
  },
  {
    "text": "the same v pi oracle that's telling you what the true value is. That's changing over time because you are doing Q-learning to try to estimate what",
    "start": "2546710",
    "end": "2553550"
  },
  {
    "text": "that is and your policies changing and so it's huge amounts of non-stationarity. So you don't have a stationary target when you're even just trying to fit",
    "start": "2553550",
    "end": "2560030"
  },
  {
    "text": "your function because it could be constantly changing at each step. Um, so you change your po-, you change your weights,",
    "start": "2560030",
    "end": "2565490"
  },
  {
    "text": "then you change your policy and then now you're gonna change your weights again. And so, perhaps it's not surprising that things might be very hard in terms of convergence.",
    "start": "2565490",
    "end": "2572825"
  },
  {
    "text": "So the way that sort of, uh, DQN, deep Q-learning addresses these is by experienced replay and fixed Q-targets.",
    "start": "2572825",
    "end": "2580505"
  },
  {
    "text": "Experienced replay, prime number if you guys have heard about this, if you learned about DQN before is we're just gonna stroll data.",
    "start": "2580505",
    "end": "2587525"
  },
  {
    "text": "We've talked about a little bit before of how like TD learning in their standard approach, just uses a data point.",
    "start": "2587525",
    "end": "2593315"
  },
  {
    "text": "Now what I mean by a data point here is really one of these sar, S prime tuples. In the simplest way of TD Learning or Q-learning,",
    "start": "2593315",
    "end": "2600470"
  },
  {
    "text": "you use that once and you throw it away. That's great for data storage, um, it's not so good for performance.",
    "start": "2600470",
    "end": "2605975"
  },
  {
    "text": "So the idea is that we're just gonna store this. Uh, we're gonna keep around some finite buffer of",
    "start": "2605975",
    "end": "2611569"
  },
  {
    "text": "prior experience and we're gonna re-basically redo Q-learning updates.",
    "start": "2611570",
    "end": "2616955"
  },
  {
    "text": "Just remember a Q-learning update here would be looking like this. We would update our weights, that's considered one update to take a tuple and update the weight.",
    "start": "2616955",
    "end": "2624815"
  },
  {
    "text": "It's like one stochastic gradient descent update. And so you can just sample from your experience, um, ah, replay,",
    "start": "2624815",
    "end": "2631745"
  },
  {
    "text": "your replay buffer and compute the target value given your current Q function and then you do stochastic gradient descent.",
    "start": "2631745",
    "end": "2638465"
  },
  {
    "text": "Now notice here because your Q function will be changing over time.",
    "start": "2638465",
    "end": "2643820"
  },
  {
    "text": "Each time you do your update of the same tuple, you might have a different target value",
    "start": "2643820",
    "end": "2649490"
  },
  {
    "text": "because your Q function has changed for that point. So this is nice because basically it means that you reuse",
    "start": "2649490",
    "end": "2657500"
  },
  {
    "text": "your data instead of just using each data point once, you can reuse it and that can be helpful. And what we'll look at that more in a minute.",
    "start": "2657500",
    "end": "2664290"
  },
  {
    "text": "So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes and so, um,",
    "start": "2666700",
    "end": "2673625"
  },
  {
    "text": "you can sort of propagate this information and essentially the main idea, is just that we're gonna use data more than once,",
    "start": "2673625",
    "end": "2679160"
  },
  {
    "text": "um, and that's often very helpful. Yes, and, um, name first. Um, my question is is this equivalent to keeping more frames in our,",
    "start": "2679160",
    "end": "2689990"
  },
  {
    "text": "uh, representation or is this, uh [inaudible] It's a great question which is is this equivalent to",
    "start": "2689990",
    "end": "2696410"
  },
  {
    "text": "keeping more frames in our representation? It's not. Though that's a really interesting question. Um, more frames would be like keeping,",
    "start": "2696410",
    "end": "2701960"
  },
  {
    "text": "uh, a more complicated state representation. But you can still just use a state action or word next state tuple once and throw that data way.",
    "start": "2701960",
    "end": "2709060"
  },
  {
    "text": "This is like saying that periodically I- like let's say I went s1 a1 r1 s2 and then I keep going on and now I'm at like s3 a3 r3 s4.",
    "start": "2709060",
    "end": "2718089"
  },
  {
    "text": "So, that's really where I am in the world, I'm now in state four. It's like I suddenly pretend, oh wait,",
    "start": "2718090",
    "end": "2723095"
  },
  {
    "text": "I'm gonna pretend that I'm back in s1, took a1, got r1, and went to s2 and I'm gonna update my weights again,",
    "start": "2723095",
    "end": "2729035"
  },
  {
    "text": "and the reason that that update will be different than before is because I've now updated using my second update and my third update.",
    "start": "2729035",
    "end": "2736775"
  },
  {
    "text": "So, my Q function, in general, will be different than before. So, it'll cause a different weight update.",
    "start": "2736775",
    "end": "2741829"
  },
  {
    "text": "So, even though it's the same data point as before, it's gonna cause a different weight update. In general, one thing we talked about a long time ago is that if you, um, uh,",
    "start": "2741830",
    "end": "2749645"
  },
  {
    "text": "do TD learning to converge it, which means that you go over your data mu- like, um, an infinite amount of time.",
    "start": "2749645",
    "end": "2754970"
  },
  {
    "text": "Um, at least in the tabular case, that is equivalent to if you learned an MDP model.",
    "start": "2754970",
    "end": "2760355"
  },
  {
    "text": "You learned the transition dynamics in the reward model and you just did MDP planning with that. That's what TD learning converges to,",
    "start": "2760355",
    "end": "2767089"
  },
  {
    "text": "is that if you repeatedly go through your data in infinite amount of time, eventually it will converge to as if you'll learn to model,",
    "start": "2767090",
    "end": "2773165"
  },
  {
    "text": "a dynamics model, a word model, and then the planning for that which is pretty cool. So, this is getting us closer to that.",
    "start": "2773165",
    "end": "2780200"
  },
  {
    "text": "But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games.",
    "start": "2780200",
    "end": "2787040"
  },
  {
    "text": "Um, there's a direct trade-off between computation and getting more experience. It's actually a really interesting trade-off",
    "start": "2787040",
    "end": "2792470"
  },
  {
    "text": "because in these cases it's sort of like should you think more and plan more and use your old data or should you just gather more experience?",
    "start": "2792470",
    "end": "2797570"
  },
  {
    "text": "Um, but we can talk more about that later. Yeah, question and name first please. And so, the experienced replay buffer has like a fixed size.",
    "start": "2797570",
    "end": "2806255"
  },
  {
    "text": "Just for, like, clarification of understanding, are those samples, like, replaced by new samples after fixed amount of time?",
    "start": "2806255",
    "end": "2814010"
  },
  {
    "text": "Or is there, like, a specific way to choose what samples to store in the buffer? That's a great question which is, okay,",
    "start": "2814010",
    "end": "2820130"
  },
  {
    "text": "this is presumably gonna be a fixed size buffer. Um, and if it's a fixed size buffer, how do you pick what's in it?",
    "start": "2820130",
    "end": "2825349"
  },
  {
    "text": "Um, is it the most recent and- and how do you get things, uh, how do you remove items from it. It's a really interesting question.",
    "start": "2825350",
    "end": "2830975"
  },
  {
    "text": "Different people do different things. Normally, it's often the most recent buffer, um, can be for example the last one million samples,",
    "start": "2830975",
    "end": "2837320"
  },
  {
    "text": "which gives you a highlight of how many samples we are gonna be talking about. But you can make different choices and there's",
    "start": "2837320",
    "end": "2843470"
  },
  {
    "text": "interesting questions of what thing that you should kick out. Um, it also depends if your problem is really non-stationary or not,",
    "start": "2843470",
    "end": "2850430"
  },
  {
    "text": "and I want to mean there's, like, the real world is non-stationary, like your customer base is changing. Yeah?",
    "start": "2850430",
    "end": "2856220"
  },
  {
    "text": "Uh, I'm trying to strike the right balance between continuing experience like new data points versus re-flagging it.",
    "start": "2856220",
    "end": "2861890"
  },
  {
    "text": "Can we use something similar to like exploitation versus exploration. Um, essentially like with random probability just decide to re-flag [inaudible].",
    "start": "2861890",
    "end": "2869690"
  },
  {
    "text": "The question is about how would we choose between, like, what, um, you know, getting new data and how much to replay et cetera, um,",
    "start": "2869690",
    "end": "2876020"
  },
  {
    "text": "and could we do that sort of as an exploration-exploitation trade-off. I think this is generally understudied but there's lots of different heuristics people use.",
    "start": "2876020",
    "end": "2882230"
  },
  {
    "text": "Often people have some of- sort of a fixed ratio of how much they're updating based on the experience replay versus getting,",
    "start": "2882230",
    "end": "2889085"
  },
  {
    "text": "um, putting new samples into there. So, generally right now is really heuristic trade-off.",
    "start": "2889085",
    "end": "2894365"
  },
  {
    "text": "Could certainly imagine trying to optimally figure this out but that also requires computation. Um, this gets us into the really interesting question",
    "start": "2894365",
    "end": "2900619"
  },
  {
    "text": "of metacomputation and metacognition. Um, but if, you know, your agent thinking about how to prioritize its own computation which is a super cool problem.",
    "start": "2900620",
    "end": "2907475"
  },
  {
    "text": "Which is what we solve all the time. Okay. So, um, the second thing that DQN does is it first have- it first keeps route this old data.",
    "start": "2907475",
    "end": "2915365"
  },
  {
    "start": "2913000",
    "end": "3361000"
  },
  {
    "text": "The second thing that it does is it has fixed Q targets. So, what does that mean? Um, so to improve stability,",
    "start": "2915365",
    "end": "2921740"
  },
  {
    "text": "and what we mean by stability here is that we don't want our weights to explode and go to infinity which we saw could happen in linear value function.",
    "start": "2921740",
    "end": "2928090"
  },
  {
    "text": "Um, we're gonna fix the target weights that are used in the target calculation for multiple updates.",
    "start": "2928090",
    "end": "2933430"
  },
  {
    "text": "So, remember here what I mean by the target calculation here is that reward plus Gamma V of S prime.",
    "start": "2933430",
    "end": "2940740"
  },
  {
    "text": "So, this itself is a function of w and we're gonna",
    "start": "2940740",
    "end": "2945965"
  },
  {
    "text": "fix the w we use in that value of S prime for several rounds.",
    "start": "2945965",
    "end": "2951370"
  },
  {
    "text": "So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile and that's basically like making this more stable.",
    "start": "2951370",
    "end": "2959020"
  },
  {
    "text": "Because this, in general, is an approximation of the oracle of V star.",
    "start": "2959020",
    "end": "2968540"
  },
  {
    "text": "So, you'd really like an oracle to just give this to you every time you reach, you know, an S prime or you take an action [inaudible] and go to S prime,",
    "start": "2968800",
    "end": "2976280"
  },
  {
    "text": "you'd like an oracle to give you what the true value is. You don't have that, um, and it could change on every single step because you could be updating the weights.",
    "start": "2976280",
    "end": "2983105"
  },
  {
    "text": "What this is saying is, don't do that, keep the weights fixed that used to compute VS prime for a little while,",
    "start": "2983105",
    "end": "2990005"
  },
  {
    "text": "maybe for 10 steps, maybe for a 100 steps, um, and that just makes the target,",
    "start": "2990005",
    "end": "2995135"
  },
  {
    "text": "the sort of the thing that you're trying to minimize your loss with respect to, more stable. So, we're gonna have, um, we still have",
    "start": "2995135",
    "end": "3002724"
  },
  {
    "text": "our single network but we're just gonna maintain two different sets of weights for that network. Um, one is gonna be this weight minus.",
    "start": "3002725",
    "end": "3009849"
  },
  {
    "text": "I'll call it minus because, um, well there might be other conventions but in particular it's the older set of weights,",
    "start": "3009850",
    "end": "3015295"
  },
  {
    "text": "the ones we're not updating right now. Those are the ones that we're using them as target calculation. So, those are the ones we're gonna use when we want to figure out the value of S",
    "start": "3015295",
    "end": "3023230"
  },
  {
    "text": "prime and then we have some other W which is what we're using to update. So, when we compute our target value we, again,",
    "start": "3023230",
    "end": "3031960"
  },
  {
    "text": "can sample and experience tuple from the dataset from our experience replay buffer,",
    "start": "3031960",
    "end": "3037089"
  },
  {
    "text": "compute the target value using our w minus, and then we use stochastic gradient descent to update the network weights.",
    "start": "3037090",
    "end": "3044260"
  },
  {
    "text": "So, this is used with minus this is used with the current one. Yeah?",
    "start": "3044260",
    "end": "3050815"
  },
  {
    "text": "So, uh, I guess two questions like intuitively, why does this is help and, like,",
    "start": "3050815",
    "end": "3056590"
  },
  {
    "text": "why does it make it more stable and, like, secondly, like, are there any other benefits on the stability from doing this?",
    "start": "3056590",
    "end": "3062650"
  },
  {
    "text": "These are two questions, one is, intuitively, why does this help? Um, which is a great question and second of all, beyond the stability, is there any other benefits?",
    "start": "3062650",
    "end": "3069265"
  },
  {
    "text": "So, intuitively, why does this help, um, in terms of stability? In terms of stability, it helps because you're",
    "start": "3069265",
    "end": "3075490"
  },
  {
    "text": "basically reducing the noise in your target. If you think back to Monte Carlo, um,",
    "start": "3075490",
    "end": "3080530"
  },
  {
    "text": "there instead of using this target like this bootstrap target where we're using GT.",
    "start": "3080530",
    "end": "3086410"
  },
  {
    "text": "So, in Monte Carlo, we used GT and I told you the nice thing about that was that it was an unbiased estimator of V pie.",
    "start": "3086410",
    "end": "3092950"
  },
  {
    "text": "But the downside was that it was high variance because you're summing up the rewards till the end of the episode. Um, and so if things are high-variance when you're trying to regress on them,",
    "start": "3092950",
    "end": "3101260"
  },
  {
    "text": "it's gonna be more noisy, um, and you could [inaudible] gradients. Imagine that we do something- take the extreme of this, if we want to be,",
    "start": "3101260",
    "end": "3107545"
  },
  {
    "text": "um, for stability, you could always make your target equal to a constant. You could always make it equal to zero for example,",
    "start": "3107545",
    "end": "3114040"
  },
  {
    "text": "and if you kept your target fixed forever, you would learn the weights that- that minimize the error to a constant function and that would then be stable because you always",
    "start": "3114040",
    "end": "3122680"
  },
  {
    "text": "have the same target value that you're always trying to predict and eventually you'd learn that you should just set your w equal to zero and- and that would be fine.",
    "start": "3122680",
    "end": "3129400"
  },
  {
    "text": "So, this is just reducing the noise and the target that we're trying to sort of, um, if you think of this as a supervised learning problem,",
    "start": "3129400",
    "end": "3135790"
  },
  {
    "text": "we have an input x and output y. The challenge in RL is that our y is changing, if you make it that you're- so your y is not changing, it's much easier to fit.",
    "start": "3135790",
    "end": "3144655"
  },
  {
    "text": "Um, unless I convince whether or not there's, uh, any benefit beyond stability. I think mostly not, um,",
    "start": "3144655",
    "end": "3150295"
  },
  {
    "text": "I- the- this is also sort of reducing how quickly you propagate information because you're using",
    "start": "3150295",
    "end": "3156069"
  },
  {
    "text": "an- a stale set of weights to represent the value of a state. So, you might misestimate the value of a state because you haven't updated it",
    "start": "3156070",
    "end": "3162910"
  },
  {
    "text": "with holding permit- with new information. Yeah? Uh, assuming we want to do [inaudible] approximator.",
    "start": "3162910",
    "end": "3171460"
  },
  {
    "text": "Is there something that's specific to the deep neural networks?. That's a great questions which is, is this specific to deep neural networks or can we use",
    "start": "3171460",
    "end": "3177820"
  },
  {
    "text": "this with linear value function approximate or any value par-, you can use those to any value function approximators. Yeah, this is not specific.",
    "start": "3177820",
    "end": "3183520"
  },
  {
    "text": "This is really just about stability and that's- that's true for the experience replay too. Experience replay is just kinda propagate information more- more effectively and,",
    "start": "3183520",
    "end": "3190675"
  },
  {
    "text": "um, this is just gonna make it more stable. Uh, so these aren't sort of unique using deep neural network.",
    "start": "3190675",
    "end": "3195700"
  },
  {
    "text": "I think they were just more worried about the stability with these really complicated function approximators. Yeah, in the red.",
    "start": "3195700",
    "end": "3201309"
  },
  {
    "text": "Do you every update the- Minus at all, or is that [inaudible].",
    "start": "3201310",
    "end": "3206340"
  },
  {
    "text": "Great question. So, the- Di- Dell? Dian. Dian. Dian's question is whether or not, um, we ever update w minus, yes we do.",
    "start": "3206340",
    "end": "3212565"
  },
  {
    "text": "We pu- can periodically update w minus as well. So, in a fixed schedule, say every 50 or, you know,",
    "start": "3212565",
    "end": "3219000"
  },
  {
    "text": "every n episodes or every n steps, you, um, update sort of like every- and you would set w minus dw. Yeah?",
    "start": "3219000",
    "end": "3230810"
  },
  {
    "text": "I was thinking, like, given that we know that this is work for gradient descent and you're not using the same kind of structure as gradient descent,",
    "start": "3230810",
    "end": "3240720"
  },
  {
    "text": "you're using to create, you know, different option subtracting the [inaudible] value of- of the function.",
    "start": "3240720",
    "end": "3246960"
  },
  {
    "text": "How is this supposed to, like, not grade- grade- gradient descent, like, all those assumptions from [inaudible]",
    "start": "3246960",
    "end": "3252810"
  },
  {
    "text": "Your question is, okay this- does this really work in terms of gradient descent? This is not- I mean,",
    "start": "3252810",
    "end": "3257850"
  },
  {
    "text": "the- it's a great question, and these sort of Q learning are not true gradient descent methods. They're- they're are approximations to such,",
    "start": "3257850",
    "end": "3264540"
  },
  {
    "text": "they often do shockingly well given that. Some of the more recent ones which, um, uh, Chapter 11 has a nice discussion of this,",
    "start": "3264540",
    "end": "3271319"
  },
  {
    "text": "sort of the GTD's or gradient temporal difference learning are more true gradient descent algorithms. These are really just approximations and it's,",
    "start": "3271320",
    "end": "3277800"
  },
  {
    "text": "uh, this, um, uh, as to the point, this has no guarantees of convergence still. This is hopefully gonna help but we have no guarantees. Yeah?",
    "start": "3277800",
    "end": "3285645"
  },
  {
    "text": "Uh, George, uh, so in practice, do people have some cyclical pattern and how can they refresh",
    "start": "3285645",
    "end": "3291330"
  },
  {
    "text": "the- the gradient that's used to compute, uh, the gradients? Yeah, his question is, um, you know,",
    "start": "3291330",
    "end": "3297315"
  },
  {
    "text": "in practice are there some sort of cyclical pattern of how often you update w minus. Yes, yeah there's often particular patterns or- or hyper-",
    "start": "3297315",
    "end": "3304110"
  },
  {
    "text": "it's a hyperparameter choice of how quickly and how frequently you update this. Um, and it will trade-off between propagating information fester,",
    "start": "3304110",
    "end": "3312105"
  },
  {
    "text": "um, and possibly being less stable. So. If you make, um, you know, if n here is one that you're back to standard TD learning.",
    "start": "3312105",
    "end": "3319454"
  },
  {
    "text": "If n is infinity, that means you've never updated it. Um, so, there's a- there's a smooth continuum there. William?",
    "start": "3319455",
    "end": "3325260"
  },
  {
    "text": "Uh, we notice, like, for w, there are better initializations than just like zero,",
    "start": "3325260",
    "end": "3330839"
  },
  {
    "text": "uh, something, like, if you take into account, I guess like the mean and variance. Uh, would you initialize w minus just two",
    "start": "3330840",
    "end": "3337050"
  },
  {
    "text": "w or is there like an even better initialization for w minus? Yeah, his questions is about, you know,",
    "start": "3337050",
    "end": "3343140"
  },
  {
    "text": "the- the impact of how we, um, uh, initialize w ca- can matter. Um, uh, and is the- how do we initialize w minus.",
    "start": "3343140",
    "end": "3349890"
  },
  {
    "text": "Typically, we initialize w minus to be exactly the same as w at the beginning. Um, the choice of it will also affect, uh,",
    "start": "3349890",
    "end": "3356295"
  },
  {
    "text": "certainly the early performance. Those are great questions. Let me keep going because I wanna make sure we get to some of the extensions as well.",
    "start": "3356295",
    "end": "3362600"
  },
  {
    "text": "Um, so just to summarize how DQN works. Um, the main two innovations that data- it uses experienced replay and fixed Q targets.",
    "start": "3362600",
    "end": "3369695"
  },
  {
    "text": "It stores the transition in this sort of replay buffer, a replay memory, um, use sample random mini-batches from D. So,",
    "start": "3369695",
    "end": "3377039"
  },
  {
    "text": "normally sample in mini-batch instead of a single one. So, maybe a sample 1- or whatever other parameter.",
    "start": "3377040",
    "end": "3382440"
  },
  {
    "text": "You do your gradient descent given those. Um, you compute Q learning using these old targets and you",
    "start": "3382440",
    "end": "3388020"
  },
  {
    "text": "optimize the mean squared error between the Q network and Q learning targets, use stochastic gradient descent, and something I did not mention on here is that we're",
    "start": "3388020",
    "end": "3394380"
  },
  {
    "text": "typically doing E-greedy exploration. So, you need some schedule here too for how to do E-greedy.",
    "start": "3394380",
    "end": "3401700"
  },
  {
    "text": "So, they were not doing, um, sophisticated exploration in their original paper.",
    "start": "3401700",
    "end": "3406860"
  },
  {
    "text": "So, this is what it looks like. You sort of go in and you do multiple different convolutions. They have the images, um,",
    "start": "3406860",
    "end": "3412935"
  },
  {
    "text": "and they do some fully connected layers and then the output a Q value for each action.",
    "start": "3412935",
    "end": "3417820"
  },
  {
    "text": "Let me just bring it up. Um, for those of you who haven't seen it before. So, the nice thing is what they- so,",
    "start": "3418730",
    "end": "3426345"
  },
  {
    "text": "you're about to see breakout which is, um, an Atari game and what they do is they show you sort of the performance of what the agent is doing.",
    "start": "3426345",
    "end": "3433650"
  },
  {
    "text": "So, remember the agent's just learning from pixels here how to do this. So, it was pretty extraordinary when they showed this in about 2014.",
    "start": "3433650",
    "end": "3439559"
  },
  {
    "text": "Um, and the beginning of its learning sort of this policy. You can see it's not making- doing the right thing very much, um,",
    "start": "3439560",
    "end": "3445665"
  },
  {
    "text": "and that over time as it gets more episodes it starting to learn to make better decisions about how to do it.",
    "start": "3445665",
    "end": "3452684"
  },
  {
    "text": "Um, and one of the interesting things about it is that as you'd hope,",
    "start": "3452685",
    "end": "3458130"
  },
  {
    "text": "as it gets more and more data, it learns to make better decisions. But one of the things people like about this a lot is that,",
    "start": "3458130",
    "end": "3465305"
  },
  {
    "text": "uh, you can learn to exploit, um, the reward function. Uh, so in this case,",
    "start": "3465305",
    "end": "3473120"
  },
  {
    "text": "um, it figures out that if you really just want me to maximize the expected reward, what the best thing for me to do is to just kind of",
    "start": "3473120",
    "end": "3478940"
  },
  {
    "text": "get a hole through there and then as soon as I can start to just bounce around the top [inaudible].",
    "start": "3478940",
    "end": "3485359"
  },
  {
    "text": "Um, and so this is one of the things where, you know, if you ask the agent to maximize the reward,",
    "start": "3485360",
    "end": "3492060"
  },
  {
    "text": "it'll- it'll learn the right way to maximize the reward given enough data. Um, and so this is really cool that sort of it could discover things that maybe",
    "start": "3492060",
    "end": "3497940"
  },
  {
    "text": "are strategies that people take a little while to learn when they're first learning the game as well.",
    "start": "3497940",
    "end": "3502480"
  },
  {
    "text": "So, when they did this, they then showed, um, some pretty amazing performance on a lot of different games.",
    "start": "3506980",
    "end": "3512450"
  },
  {
    "text": "Many games they could do as well as humans. Now, to be precise here- oh yeah I'm sorry.",
    "start": "3512450",
    "end": "3518730"
  },
  {
    "text": "Uh, I'm just wondering why, um, it's playing, like, why was it [inaudible] around a lot, like, it wasn't sure of its movements like it moved",
    "start": "3518730",
    "end": "3525300"
  },
  {
    "text": "around places often, like [OVERLAPPING]. Yeah. Yeah, so, um, you might see, uh, I think she is talking- she is referring to the fact that the paddle was moving a lot.",
    "start": "3525300",
    "end": "3532829"
  },
  {
    "text": "As the agent is trying to learn, like, when we see that, you sort of go, \"Why would he jerk a lot.\" From the agent's perspective, particularly if there's a cost to moving,",
    "start": "3532830",
    "end": "3539985"
  },
  {
    "text": "then it may just be kind of babbling, uh, and doing exploration just to see what works and- and it- from our perspective that's clearly sort of an inexperienced player to do that.",
    "start": "3539985",
    "end": "3547890"
  },
  {
    "text": "That would be a strange thing but from the agent's perspective, that's completely reasonable. Um, and it does not give him positive or negative reward from that.",
    "start": "3547890",
    "end": "3553860"
  },
  {
    "text": "So, it can't distinguish between, you know, stay in stationary versus going left or right. If you put it in a cost for movement that could help. Yeah?",
    "start": "3553860",
    "end": "3561390"
  },
  {
    "text": "This might become a little bit of [inaudible] but is there a reason to introduce a pulling layer? Puling layer? There might be one in there.",
    "start": "3561390",
    "end": "3567150"
  },
  {
    "text": "I- the- the- I don't remember the complete arc- network architecture, um. The question is whether or not there's a pulling layer in there.",
    "start": "3567150",
    "end": "3572490"
  },
  {
    "text": "I think there prob- there might be inside. There has- they have to be going from images all the way up. But they have the complete architecture. It's a good question.",
    "start": "3572490",
    "end": "3579375"
  },
  {
    "text": "So, the next thing that you can see here is that, um, they got sort of human level performance on a number of different Atari games.",
    "start": "3579375",
    "end": "3586140"
  },
  {
    "text": "There's about 50 games up here. Um, just to be clear here. When they say human level performance that means asymptotically.",
    "start": "3586140",
    "end": "3591945"
  },
  {
    "text": "So, after they have trained their agent, this-, uh, they're not talking about how long it took them or",
    "start": "3591945",
    "end": "3597089"
  },
  {
    "text": "their agent to learn and as you guys will find out for homework two, it can be a lot of experience. Um, uh, a lot of time to learn how to make- do a good performance.",
    "start": "3597090",
    "end": "3605475"
  },
  {
    "text": "But nevertheless, there are a lot of cases where that might be reasonable in terms of games. So, they did very well on some domains.",
    "start": "3605475",
    "end": "3611070"
  },
  {
    "text": "Some domains, they did very poorly. Um, there's been a lot of interest in these sort of games on the bottom end of the tail which often known as those hard exploration games.",
    "start": "3611070",
    "end": "3619200"
  },
  {
    "text": "We'll probably talk- uh, we'll talk a lot more about exploration later on in the course. So, what was critical?",
    "start": "3619200",
    "end": "3625410"
  },
  {
    "text": "So, I- I like the, uh, there's a lot of really lovely things about this paper and one of the really nice things is that they did a nice ablation study,",
    "start": "3625410",
    "end": "3630780"
  },
  {
    "text": "um, uh, for us to sort of understand what were the important features and if you look at these numbers.",
    "start": "3630780",
    "end": "3635925"
  },
  {
    "text": "Um, I think that it's clear that the really important feature is replay. So, this is their performance using a linear network,",
    "start": "3635925",
    "end": "3642735"
  },
  {
    "text": "deeply network seemed to not help so much. Using that fixed Q. Um, fixed Q here means you seem like a fixed target.",
    "start": "3642735",
    "end": "3649515"
  },
  {
    "text": "Okay, that gets you a little bit three from ten. You do replay and suddenly you're at 241. Okay, so throwing away each data point what-",
    "start": "3649515",
    "end": "3656520"
  },
  {
    "text": "after you use it once is not a very good thing to do. You want to reuse that data. Um, and then if you combine replay and",
    "start": "3656520",
    "end": "3662880"
  },
  {
    "text": "fixed Q you do get an improvement over that but, uh, it's really that you get this huge increase,",
    "start": "3662880",
    "end": "3669075"
  },
  {
    "text": "um, uh, at least in break out in some of the other games by doing replay. Now, in some other ones, um,",
    "start": "3669075",
    "end": "3674730"
  },
  {
    "text": "you start to get a significant improvement as soon as you use a more complicated function approximators. But in general, replay is hugely important",
    "start": "3674730",
    "end": "3681839"
  },
  {
    "text": "and it just gives us a much better way to use the data. Yeah? Um, you know, because here in this table it seems like you'd want to use replay and",
    "start": "3681840",
    "end": "3688830"
  },
  {
    "text": "fixed Q with the linear model and that it might be a mistake to be using, uh, a deep model here.",
    "start": "3688830",
    "end": "3694650"
  },
  {
    "text": "Do you agree with that table reference table or? So, the question is like, \"Well, maybe we could use, like, linear-\" also I guess I should be clear.",
    "start": "3694650",
    "end": "3701220"
  },
  {
    "text": "So, this is all- everything on the- the next four were all deep. So, they don't have here linear plus replay.",
    "start": "3701220",
    "end": "3708480"
  },
  {
    "text": "But you could certainly imagine trying linear plus replay and it seems like you might do very well here, it might depend on which features you're using.",
    "start": "3708480",
    "end": "3715110"
  },
  {
    "text": "There's some cool work, um, uh, over the last few years looking also at, uh, whether you can combine these two.",
    "start": "3715110",
    "end": "3720270"
  },
  {
    "text": "So, we've done some work, um, using a Bayesian last layer, using like Bayesian linear regression which is useful for uncertainty.",
    "start": "3720270",
    "end": "3726180"
  },
  {
    "text": "Other people have just done linear regression where the idea is you- you sort of, um, uh, deep neural network up to a certain point and then you do,",
    "start": "3726180",
    "end": "3733785"
  },
  {
    "text": "um, kind of direct linear regression to fit exactly what the weights are at the final layer. So, that can be much more efficient,",
    "start": "3733785",
    "end": "3740234"
  },
  {
    "text": "um, but you still have a complicated representation. All right. So, since then,",
    "start": "3740235",
    "end": "3747480"
  },
  {
    "text": "there's been a huge number amount of interest in this area. Um, ah, so, again, dating myself reinforcement learning,",
    "start": "3747480",
    "end": "3753180"
  },
  {
    "text": "we used to go and give a talk about reinforcement learning, and like 40 people would show up, but most of them you knew, and, um, and then, uh,",
    "start": "3753180",
    "end": "3759780"
  },
  {
    "text": "and then it started really changing. I think I was maybe in 2016, when, um, er, ICML, I was in New York and like suddenly",
    "start": "3759780",
    "end": "3766770"
  },
  {
    "text": "there were 400 people in the room for reinforcement learning talks. Um, and then, this year at NLP's which is one of the major machine-learning conferences,",
    "start": "3766770",
    "end": "3773580"
  },
  {
    "text": "it's sold out in like eight minutes. Um, so, there were 8,000 people there, and there was a huge amount of interest in deep learning,",
    "start": "3773580",
    "end": "3779490"
  },
  {
    "text": "and for the deep learning workshop, you sort of have a 2,000 person auditorium. So, there's been a huge amount of excitement based on this work,",
    "start": "3779490",
    "end": "3786150"
  },
  {
    "text": "which I think really a huge credit to- to deep mind and to the work that David Silver and others have been doing, um, to sort of show that this was possible.",
    "start": "3786150",
    "end": "3792885"
  },
  {
    "text": "Uh, some of the immediate improvements that we're going to go through really quickly here is, um, Doubled DQN, prioritize replay, and dueling DQN.",
    "start": "3792885",
    "end": "3800745"
  },
  {
    "text": "Um, there's been way, way, way more papers in that, but these are some of the early really big improvements on top of DQN.",
    "start": "3800745",
    "end": "3808530"
  },
  {
    "text": "So, double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago.",
    "start": "3808530",
    "end": "3816390"
  },
  {
    "text": "The thing that we discussed there was this sort of maximization bias, is that, um, the max of estimated state action values can be a biased estimator of the true max.",
    "start": "3816390",
    "end": "3826260"
  },
  {
    "text": "So, we talked really briefly about double Q learning. Um, so, a double Q learning,",
    "start": "3826260",
    "end": "3832230"
  },
  {
    "text": "the idea was that we are going to maintain two different Q networks. Uh, we can select our action using,",
    "start": "3832230",
    "end": "3838770"
  },
  {
    "text": "like an E Greedy Policy where we average between those Q networks, and then we'll observe a reward in a state and we basically",
    "start": "3838770",
    "end": "3845849"
  },
  {
    "text": "use one of the Qs as the target for the other. So, if, you know,",
    "start": "3845850",
    "end": "3851175"
  },
  {
    "text": "with a 50 percent probability, we're going to update one network, and we're going to do that by using picking the action from the other network.",
    "start": "3851175",
    "end": "3862299"
  },
  {
    "text": "This is to try to separate how we pick our action",
    "start": "3862490",
    "end": "3868455"
  },
  {
    "text": "versus our estimate of the value of that action to deal with this sort of maximization bias issue.",
    "start": "3868455",
    "end": "3874905"
  },
  {
    "text": "Then with 50 percent other probability, we update Q2, and we pick the next action from the other network.",
    "start": "3874905",
    "end": "3883470"
  },
  {
    "text": "So, this is a pretty small change, it means you have to- you have to maintain two different networks or two different sets of weights,",
    "start": "3885980",
    "end": "3893295"
  },
  {
    "text": "um, and it can be pretty helpful. So, um, if you extend this idea to DQN,",
    "start": "3893295",
    "end": "3899554"
  },
  {
    "text": "you have sort of our current Q network, w select actions, and this older one to evaluate actions.",
    "start": "3899554",
    "end": "3905535"
  },
  {
    "text": "So, you can put this in there to do action selection, and then you can evaluate the value of it with your",
    "start": "3905535",
    "end": "3912180"
  },
  {
    "text": "other networks- other, other network weights. So, it's a fairly small change,",
    "start": "3912180",
    "end": "3917819"
  },
  {
    "text": "it's very similar to what we were doing already for the target network, network weights.",
    "start": "3917820",
    "end": "3923190"
  },
  {
    "text": "It turns out that it gives you a huge benefit in many, many cases for the Atari games. So, uh, this is something that's generally very useful to do, um,",
    "start": "3923190",
    "end": "3932835"
  },
  {
    "text": "and gives you sort- of sort of immediate significant boost in performance, sort of, you know, for the equivalent of like a small amount of coding.",
    "start": "3932835",
    "end": "3939780"
  },
  {
    "text": "That's one idea, and that's sort of a direct lift up from sort of, you know, double Q learning. The second thing is prioritized replay.",
    "start": "3939780",
    "end": "3946920"
  },
  {
    "text": "So, let's go back to the Mars Rover example. Um, er, so, in Mars Rover we had this really small domain,",
    "start": "3946920",
    "end": "3954330"
  },
  {
    "text": "we are talking about tabular setting through just seven states, um, and we're talking about a policy that just",
    "start": "3954330",
    "end": "3960540"
  },
  {
    "text": "always took action a1 which turned out to mostly go left. So, we had this trajectory, we started off in state s3,",
    "start": "3960540",
    "end": "3967455"
  },
  {
    "text": "we took action a1, we got rewarded zero, we went to s2, we stayed in s2 for one round when we did a1,",
    "start": "3967455",
    "end": "3975809"
  },
  {
    "text": "and then eventually went to s1, and then we terminated. So, it was this. And the first visit Monte Carlo estimate",
    "start": "3975810",
    "end": "3984749"
  },
  {
    "text": "of v for every state was 1110000, and the TD estimate with alpha equal one was this.",
    "start": "3984749",
    "end": "3993480"
  },
  {
    "text": "That was when we talked about the fact that TD only uses each data point once and it didn't propagate the information back.",
    "start": "3993480",
    "end": "3999795"
  },
  {
    "text": "So, the only update for TD learning was when we reached state s1, we took action one, we got a reward of one, and then we terminated.",
    "start": "3999795",
    "end": "4007370"
  },
  {
    "text": "So, we only updated the value of state one. So, now let's imagine that you get to do-",
    "start": "4007370",
    "end": "4013520"
  },
  {
    "text": "now let's think about what your- like your replay back up would be in this case. You'd have something like this, you'd have s3, a1,",
    "start": "4013520",
    "end": "4019190"
  },
  {
    "text": "0, s2, s2, a1, 0 s2, s2, a1, 0,",
    "start": "4019190",
    "end": "4025280"
  },
  {
    "text": "s1, s1, a1, 1, terminate. That's what your replay back up would look like.",
    "start": "4025280",
    "end": "4032700"
  },
  {
    "text": "So, let's say you get to choose two replay backups to do. So, you have four possible replay backups,",
    "start": "4032830",
    "end": "4039275"
  },
  {
    "text": "you can pick the same one twice, if you want to, and I'm going to ask you to pick to replay backups to",
    "start": "4039275",
    "end": "4044390"
  },
  {
    "text": "do to improve the value function, in some way. Um, and I'd like you to think for a",
    "start": "4044390",
    "end": "4051290"
  },
  {
    "text": "second or talk to your neighbor about which of the two you should pick, and why, and which order you do them in as well,",
    "start": "4051290",
    "end": "4058520"
  },
  {
    "text": "and whether it makes any difference. Maybe it doesn't matter if you can just pick any of these, you're going to get the same value function no matter what you do.",
    "start": "4058520",
    "end": "4064340"
  },
  {
    "text": "So, are there two- two updates that are particularly good, and if so, why and what order would you do them in?",
    "start": "4064340",
    "end": "4069860"
  },
  {
    "text": "[NOISE]",
    "start": "4069860",
    "end": "4138170"
  },
  {
    "text": "Hopefully you had a chance to think about that for a second. First of all, does it matter? So, I'm going to first ask you guys, uh, the question.",
    "start": "4138170",
    "end": "4146060"
  },
  {
    "text": "Vote if you think it matters which ones you pick, in terms of the value function you get out. That's right.",
    "start": "4146060",
    "end": "4151819"
  },
  {
    "text": "So, it absolutely matters which two you pick in terms of the resulting value function, you will not get the same value function no matter which two you pick.",
    "start": "4151820",
    "end": "4157790"
  },
  {
    "text": "Um, uh, now as for another voting. So, I will ask for which one we should do first? Should we update- should we do four first?",
    "start": "4157790",
    "end": "4164450"
  },
  {
    "text": "Four is the last one on our replay buffer. Should we do three first? Should we do two first? Okay. All right.",
    "start": "4164450",
    "end": "4173134"
  },
  {
    "text": "So, 3s have it, does somebody want to explain why? Yeah.",
    "start": "4173135",
    "end": "4178204"
  },
  {
    "text": "I think. You've got to back-propagate from the information you're already [NOISE] have on step one to step two.",
    "start": "4178205",
    "end": "4184560"
  },
  {
    "text": " Right. Yeah. So what the student said is right. So, if you pick, um, backup three, so what's backup three?",
    "start": "4184560",
    "end": "4191040"
  },
  {
    "text": "It is, S2, A1, 0, S1. So if you do the backup, that's, zero,",
    "start": "4191040",
    "end": "4196830"
  },
  {
    "text": "plus gamma V of S prime, S1. And this is one. So that means now you're gonna get to backup",
    "start": "4196830",
    "end": "4203595"
  },
  {
    "text": "and so now your V of S2 is gonna be equal to one. So you get to backup that information.",
    "start": "4203595",
    "end": "4209200"
  },
  {
    "text": "Yeah. So I, I wasn't extremely specific on what like, the right thing to do here is, that,",
    "start": "4210050",
    "end": "4216195"
  },
  {
    "text": "that the- um, that my main thing is that I wanted to emphasize that it makes the big difference and that,",
    "start": "4216195",
    "end": "4221460"
  },
  {
    "text": "and that, um, it's gonna matter in terms of order. What's the next one we should do? Should we do, raise your hand if we should do, three again.",
    "start": "4221460",
    "end": "4227370"
  },
  {
    "text": "Raise your hand if we should do two. Raise your hand if we should do one.",
    "start": "4227370",
    "end": "4232530"
  },
  {
    "text": "Yeah. The ones have it. I- someone want to explain why? Yeah, in the back.",
    "start": "4232530",
    "end": "4238770"
  },
  {
    "text": "And that's the same as the last time I [inaudible]",
    "start": "4238770",
    "end": "4245820"
  },
  {
    "text": "That's right. Yes. So, um, if you wanted to get all the way to the Monte Carlo estimate. What you would wanna do here,",
    "start": "4245820",
    "end": "4251010"
  },
  {
    "text": "is you'd wanna do S3, a1, 0, S2 which would allow your V of S3 to be updated to one.",
    "start": "4251010",
    "end": "4257745"
  },
  {
    "text": "And at this point your value function will be exactly the same as the Monte Carlo. So it definitely matters.",
    "start": "4257745",
    "end": "4264285"
  },
  {
    "text": "It matters the order in which you did, do it. If you had done S3, a1, 0, S2, your S3 wouldn't have changed.",
    "start": "4264285",
    "end": "4270179"
  },
  {
    "text": "Um, so ordering can make a big difference. Uh, so not only do we wanna think about like, what, um, was being brought up before but I think",
    "start": "4270180",
    "end": "4277034"
  },
  {
    "text": "to say like what should we be putting in our replay buffer, not only do we wanna think about what- should be in a replay buffer but also what order do",
    "start": "4277035",
    "end": "4282930"
  },
  {
    "text": "we sampled them can make a big difference in terms of convergence rates. Um, uh, and in particular,",
    "start": "4282930",
    "end": "4288705"
  },
  {
    "text": "there's some really cool work from a couple of years ago looking at this formally of like how, at what the ordering, matters. Um, so, there is this paper back in",
    "start": "4288705",
    "end": "4296190"
  },
  {
    "text": "2016 that tried to look at what the optimal order would be. So imagine that you had an oracle that could, um, exactly compute.",
    "start": "4296190",
    "end": "4303210"
  },
  {
    "text": "Now, this is gonna be computationally intractable, we're not gonna be able to do this in general, but imagine that the oracle could go through and pick",
    "start": "4303210",
    "end": "4308909"
  },
  {
    "text": "and figure out exactly what the right order is. Um, then what they found out in this case is that, for this or a small chain like example,",
    "start": "4308910",
    "end": "4315704"
  },
  {
    "text": "um, you'd get this exponential improvement in convergence, which is pretty awesome. So what does that mean? The number, of, um,",
    "start": "4315705",
    "end": "4321990"
  },
  {
    "text": "updates you need to do until your value function converges to the right thing. It can be exponentially smaller,",
    "start": "4321990",
    "end": "4327120"
  },
  {
    "text": "if you update carefully and you, you could have an oracle tells you exactly what tuple the sample. Which is super cool. Um, so you can be much much better.",
    "start": "4327120",
    "end": "4335065"
  },
  {
    "text": "But you can't do that. You're not gonna spend all this. It- it's very computationally, expensive or impossible in some cases to figure out exactly what that uh,",
    "start": "4335065",
    "end": "4341975"
  },
  {
    "text": "that oracle ordering should be. Um, but it does illustrate that we, we might wanna be careful about the order that we do it and- so, their,",
    "start": "4341975",
    "end": "4350474"
  },
  {
    "text": "intuition, for this, was, let's try to prioritize a tuple for replay according to its DQN error.",
    "start": "4350475",
    "end": "4358860"
  },
  {
    "text": "So, the DQN error, um, in this case is just our TD Learning error.",
    "start": "4358860",
    "end": "4363929"
  },
  {
    "text": "So it's gonna be the difference between, our current. This is basically our prediction error. So this is our prediction error [NOISE],",
    "start": "4363930",
    "end": "4372090"
  },
  {
    "text": "Almost our prediction error, I'll just call it TD, because it's not quite because we were doing the max. So this is like, sort of our predicted,",
    "start": "4372090",
    "end": "4380050"
  },
  {
    "text": "TD error minus our current. Let us say, if you have a really really big error,",
    "start": "4380750",
    "end": "4389340"
  },
  {
    "text": "that we're gonna prioritize, updating that more. And you update this quiet quantity at every update, you set it for new tuples to be zero and one method- they have two different methods for,",
    "start": "4389340",
    "end": "4398505"
  },
  {
    "text": "for trying to do this sort of prioritization. That one method basically takes these, um, priorities,",
    "start": "4398505",
    "end": "4405570"
  },
  {
    "text": "raises them to some power alpha, um, and then normalizes And then that's the probability,",
    "start": "4405570",
    "end": "4411915"
  },
  {
    "text": "of selecting that tuple. So you prioritize more things that are weights. Yeah. Doesn't freezing.",
    "start": "4411915",
    "end": "4417300"
  },
  {
    "text": "Name first, please. Oh, Sorry. Doesn't freezing in the old ways were a counter to propagating back the information there?",
    "start": "4417300",
    "end": "4424170"
  },
  {
    "text": "It's like, you first the old ways and uh, example we're just going through, after you like propagated the one back once,",
    "start": "4424170",
    "end": "4430980"
  },
  {
    "text": "you wouldn't be able to do anymore because your value's totally zero It's a great point, which is,",
    "start": "4430980",
    "end": "4436320"
  },
  {
    "text": "if you are fixing, um, uh, your w minus, then, if you were looking at our case that we had before,",
    "start": "4436320",
    "end": "4442980"
  },
  {
    "text": "then you wouldn't be able to continue propagating that back, because you wouldn't update yet, yet, that's exactly right. So there's gonna be this tension between,",
    "start": "4442980",
    "end": "4449175"
  },
  {
    "text": "when you fix things versus her propagating information back. Um, I, and, it's a tention that one has to sort of figure out,",
    "start": "4449175",
    "end": "4457230"
  },
  {
    "text": "there's not necessarily principled ways for, exactly what the right schedule is to do that, but it's a hyperparameter to do.",
    "start": "4457230",
    "end": "4462465"
  },
  {
    "text": "So why does it, what does ordering matter, that if you're fixing, and so you are not changing, uh, like, then it wouldn't matter what order we sampled those previous ones, right?",
    "start": "4462465",
    "end": "4471690"
  },
  {
    "text": "Uh, okay. So basically, ordering matter at all, in that case. It still matters because we're still gonna be doing replay, o- over,",
    "start": "4471690",
    "end": "4478139"
  },
  {
    "text": "uh, the weights will be changing during the time period of which will be replayed over that buffer. So that buffer could be like,",
    "start": "4478140",
    "end": "4483330"
  },
  {
    "text": "million and you might re-update your weights like every 50 steps or something like that. So there's still gonna be a whole bunch of data points in,",
    "start": "4483330",
    "end": "4489465"
  },
  {
    "text": "uh, in your replay buffer, that it's useful to think about, now that your weights have changed, what ordering do you wanna go through those? It's a great question.",
    "start": "4489465",
    "end": "4497380"
  },
  {
    "text": "Okay. So what method is this? Lemme just, just to clarify, if we set, um,",
    "start": "4497930",
    "end": "4503625"
  },
  {
    "text": "alpha equal to zero, what's the rule for selecting among the existing tuples?",
    "start": "4503625",
    "end": "4508330"
  },
  {
    "text": "So out- so Pi is our, uh, sort of basically our DQN error. If we set Alpha equal to zero, you know, it's right.",
    "start": "4509300",
    "end": "4519180"
  },
  {
    "text": "Yeah. So, so this sort of trades off between uniform, no prioritization to completely picking the one that,",
    "start": "4519180",
    "end": "4527310"
  },
  {
    "text": "um, like if alpha's infinity then that's gonna be picking the one with the highest DQN error. So it's a trade-off, it's a stochastic. All right.",
    "start": "4527310",
    "end": "4534105"
  },
  {
    "text": "So, um, then, they combine this with, sort of- the reason why I'm picking these three [inaudible] they are sort of layer on top of each other.",
    "start": "4534105",
    "end": "4540405"
  },
  {
    "text": "So prioritise replay versus, um, I think this is prioritise replay plus D, um, double DQN versus just double DQN.",
    "start": "4540405",
    "end": "4547020"
  },
  {
    "text": "Most of the time, um, this is zero would be, they're both the same underneath means flat, uh, vanilla DQ, double DQN is better.",
    "start": "4547020",
    "end": "4554415"
  },
  {
    "text": "Above means that prioritize replay is better. Most, the time prioritize replay is better and there's some hyper parameters here to",
    "start": "4554415",
    "end": "4561239"
  },
  {
    "text": "play with but most of the time it's, it's useful. And it's certainly useful to think about, you know, we're order might matter.",
    "start": "4561240",
    "end": "4566460"
  },
  {
    "text": "All right. We don't have very much time left so I'm just gonna do, short through this just so you're aware of it. Um, one of the best papers from ICML 2016 was dueling.",
    "start": "4566460",
    "end": "4574930"
  },
  {
    "text": "Um, the idea is that, if you want to, make decisions in the world,",
    "start": "4575420",
    "end": "4581579"
  },
  {
    "text": "they're working some states that are better or worse, um, and they're just gonna have higher value or lower value, but that- what you really wanna be able to do is,",
    "start": "4581580",
    "end": "4587340"
  },
  {
    "text": "figure out what the right action is to do, in a particular state. Um, and so that- what you want us to have understand is,",
    "start": "4587340",
    "end": "4592935"
  },
  {
    "text": "this, this advantage function. You wanna know, how much better or worse taking a particular action is versus following the current policy.",
    "start": "4592935",
    "end": "4600780"
  },
  {
    "text": "That really like I don't care about estimating the value of a state, i care about being able to understand which of the actions has the better value.",
    "start": "4600780",
    "end": "4608265"
  },
  {
    "text": "So I'm looking at this advantage function. So, what they do to do this is that in contrast to DQN,",
    "start": "4608265",
    "end": "4615390"
  },
  {
    "text": "where you output all of the Q's.They're gonna separate and they're gonna first estimate the value of a state,",
    "start": "4615390",
    "end": "4620579"
  },
  {
    "text": "and they're gonna estimate this advantage function, which is Q of s. One minus V of s,",
    "start": "4620580",
    "end": "4627615"
  },
  {
    "text": "Q of s, a2 minus V of s. [inaudible] just gonna separate it.",
    "start": "4627615",
    "end": "4632730"
  },
  {
    "text": "It's an architectural choice and learning a recombine these for the Q. And I get this is gonna help us refocus on the signal that we care about which is,",
    "start": "4632730",
    "end": "4640949"
  },
  {
    "text": "um, you know,after accurately estimate which action is better or worse.",
    "start": "4640950",
    "end": "4645100"
  },
  {
    "text": "Um, there's intruding questions about whether or not this is identifiable, I don't have enough time to go into these today.",
    "start": "4646100",
    "end": "4651480"
  },
  {
    "text": "It is not identifiable. I'm happy to talk about all of that off light, um, uh, the, the reason this is, uh, important is,",
    "start": "4651480",
    "end": "4658560"
  },
  {
    "text": "they just forces one to make some sort of default assumptions about, um, specifying the appendage functions.",
    "start": "4658560",
    "end": "4665770"
  },
  {
    "text": "Empirically, it's often super helpful. So, again compared to double DQN with prioritize replay,",
    "start": "4665960",
    "end": "4672570"
  },
  {
    "text": "which we just saw, which was already better than w- double DQN, which is also better than DQN. Um, this again gives you another performance gain, substantial one.",
    "start": "4672570",
    "end": "4680670"
  },
  {
    "text": "So basically these are sort of threes, three different ones that came up within the- for two years after DQN that started",
    "start": "4680670",
    "end": "4687240"
  },
  {
    "text": "making some really big big performance gains compared to destroy completely vanilla DQN. For homework two, you're gonna be implementing DQ and not the other ones,",
    "start": "4687240",
    "end": "4695820"
  },
  {
    "text": "they welcome to implement some of the other ones. They just good to be aware of- those are some and sort of the major initial improvements to giving it substantially better performance on ATARI.",
    "start": "4695820",
    "end": "4703949"
  },
  {
    "text": "Um, I'll leave this up. We're almost out of time. Uh, feel free to look at the last couple slides of this",
    "start": "4703950",
    "end": "4708989"
  },
  {
    "text": "for some practical [NOISE] tips that came from John Schulman, um John Schulman was a PhD student at Berkeley, that is now of the heads of open AI.",
    "start": "4708990",
    "end": "4715510"
  },
  {
    "text": "Um, I- just one thing that I will make sure to highlight, it could be super tempting to try to start,",
    "start": "4715510",
    "end": "4721550"
  },
  {
    "text": "by like implementing Q learning directly on the ATARI. Highly encourage you to first go through,",
    "start": "4721550",
    "end": "4726890"
  },
  {
    "text": "sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working, um, before you deploy on ATARI.",
    "start": "4726890",
    "end": "4733505"
  },
  {
    "text": "Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming.",
    "start": "4733505",
    "end": "4739410"
  },
  {
    "text": "Um,and so in terms of just understanding and deep again, it's way better to make sure that you know your Q Learning method is working,",
    "start": "4739410",
    "end": "4744930"
  },
  {
    "text": "before you wait, 12 hours to see whether or not, oh it didn't learn anything on Pogge. So, that, that- there's a reason for why we,",
    "start": "4744930",
    "end": "4751440"
  },
  {
    "text": "sort of build up, the way we do in the assignment. Um, another practical, to a few other practical tips, feel free to,",
    "start": "4751440",
    "end": "4757575"
  },
  {
    "text": "to look at those, um, and then we were on Thursday. Thanks.",
    "start": "4757575",
    "end": "4763600"
  }
]