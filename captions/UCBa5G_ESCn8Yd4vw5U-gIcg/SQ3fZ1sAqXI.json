[
  {
    "text": "welcome everyone um this is CS uh 336",
    "start": "5200",
    "end": "10400"
  },
  {
    "text": "language models from scratch and this is our the core staff so I'm Percy one of",
    "start": "10400",
    "end": "16400"
  },
  {
    "text": "your instructors um I'm really excited about this class because it really allows you to see the whole language",
    "start": "16400",
    "end": "22080"
  },
  {
    "text": "modeling building pipeline end to end including data systems and modeling um Tatsu I'll be co-eing with him so I'll",
    "start": "22080",
    "end": "29599"
  },
  {
    "text": "let everyone introduce themselves hi everyone i'm Tatsu i'm one of the the co-instructors i'll be uh giving lecture",
    "start": "29599",
    "end": "36160"
  },
  {
    "text": "in you know week or two probably a few weeks um I'm really excited about this class we Percy and I you know spent a",
    "start": "36160",
    "end": "42399"
  },
  {
    "text": "while being a little disgruntled thinking like what's the really deep technical stuff that we can teach our students today and I think one of the",
    "start": "42399",
    "end": "48800"
  },
  {
    "text": "things that is really you got to build it from scratch to understand it so I'm hoping that that's sort of the ethos that I take away from from that class",
    "start": "48800",
    "end": "56559"
  },
  {
    "text": "uh hey everyone i'm Roit um I actually failed this class when I took",
    "start": "56559",
    "end": "62840"
  },
  {
    "text": "it but now I'm your CA so when they say anything is possible",
    "start": "62840",
    "end": "70520"
  },
  {
    "text": "hey everyone I'm Neil i'm a third year student PhD student in the CS department i work with you um yeah mostly",
    "start": "71439",
    "end": "78799"
  },
  {
    "text": "interested in my research on synthetic data language models reasoning all that stuff so yeah should be up on the",
    "start": "78799",
    "end": "84560"
  },
  {
    "text": "quarter uh hey guys I'm Marcel i'm a second year PhD i work as good these",
    "start": "84560",
    "end": "90400"
  },
  {
    "text": "days I work on health and he was a topper of many leaderboards",
    "start": "90400",
    "end": "95840"
  },
  {
    "text": "from last year so he's the number to beat okay all right well thanks everyone",
    "start": "95840",
    "end": "101360"
  },
  {
    "text": "um so let's let's continue as Satu mentioned this is the second time we're teaching the class we've grown the class",
    "start": "101360",
    "end": "108560"
  },
  {
    "text": "uh by around 50% i have three TAs instead of two and one big thing is we're making all the lectures uh u on",
    "start": "108560",
    "end": "116560"
  },
  {
    "text": "YouTube so that um the world can learn how to build language models from scratch okay so why do we decide to make",
    "start": "116560",
    "end": "125600"
  },
  {
    "text": "this course and endure all the all the pain um so let's ask GPD4 so if you ask",
    "start": "125600",
    "end": "132800"
  },
  {
    "text": "it why teach a course on building language models from scratch um it the reply is teaching a course provides",
    "start": "132800",
    "end": "140319"
  },
  {
    "text": "foundational understanding of techniques fosters innovation um kind of the",
    "start": "140319",
    "end": "145520"
  },
  {
    "text": "typical kind of generic blathers okay so here's the real reason so we're in a bit",
    "start": "145520",
    "end": "151520"
  },
  {
    "text": "of a crisis I would say researchers are beingcoming more and more disconnected",
    "start": "151520",
    "end": "156720"
  },
  {
    "text": "from the underlying technology um eight years ago researchers would implement",
    "start": "156720",
    "end": "162319"
  },
  {
    "text": "and train their own models in AI even six years ago you at least uh take the",
    "start": "162319",
    "end": "167920"
  },
  {
    "text": "models uh like BERT and download them and fine-tune them and now many people",
    "start": "167920",
    "end": "174000"
  },
  {
    "text": "can just get away with prompting a proprietary model so this is not",
    "start": "174000",
    "end": "179200"
  },
  {
    "text": "necessarily bad right because as you introduce layers of abstraction we can",
    "start": "179200",
    "end": "184239"
  },
  {
    "text": "all do more and a lot of research has been unlocked by um be the simplicity of",
    "start": "184239",
    "end": "189920"
  },
  {
    "text": "being able to prompt the language model and I do a fair my share of uh prompting so so there's nothing wrong with that",
    "start": "189920",
    "end": "196879"
  },
  {
    "text": "but it's also remember that these abstractions are leaky so in contrast to",
    "start": "196879",
    "end": "202000"
  },
  {
    "text": "programming languages or operating systems um you don't really understand what the abstraction is it's a it's a",
    "start": "202000",
    "end": "208400"
  },
  {
    "text": "string in and string out I guess um and I would say that there's still a lot of fundamental research to be done that",
    "start": "208400",
    "end": "215519"
  },
  {
    "text": "required tearing up the stack and co-designing different aspects of the data and the uh systems and the model",
    "start": "215519",
    "end": "221680"
  },
  {
    "text": "and I think really that full understanding of this technology is necessary for fundamental research so",
    "start": "221680",
    "end": "228879"
  },
  {
    "text": "that's why this class exists we want to enable the fundamental research to continue and our philosophy is to",
    "start": "228879",
    "end": "236000"
  },
  {
    "text": "understand it you have to build it so there's one small problem here and",
    "start": "236000",
    "end": "242879"
  },
  {
    "text": "this is because of the industrialization of language models so GPD4 has rumored",
    "start": "242879",
    "end": "251040"
  },
  {
    "text": "to be 1.8 trillion parameters cost 100 million dollars to train um you have XAI",
    "start": "251040",
    "end": "258160"
  },
  {
    "text": "building the clusters with uh 200,000 H100s if you can imagine that um there's",
    "start": "258160",
    "end": "264240"
  },
  {
    "text": "an investment of over 500 billion you know supposedly um over over four years",
    "start": "264240",
    "end": "270479"
  },
  {
    "text": "so these are pretty large numbers right um and furthermore there's no public details on how these models are being",
    "start": "270479",
    "end": "277199"
  },
  {
    "text": "built um here from GPD4 this is even two years ago um they very honestly say that",
    "start": "277199",
    "end": "284240"
  },
  {
    "text": "due to the competitive landscape and simply safety limitations uh we're going to disclose no details okay so this is",
    "start": "284240",
    "end": "291360"
  },
  {
    "text": "the state of the of the world um right now and so in some sense frontier models",
    "start": "291360",
    "end": "297680"
  },
  {
    "text": "are out of reach for us so if you came into this class thinking you're each going to train your own GPD for um sorry",
    "start": "297680",
    "end": "306320"
  },
  {
    "text": "um so we're going to build small language models but the problem is that",
    "start": "306320",
    "end": "312400"
  },
  {
    "text": "these might not be representative and here's some of two examples uh to illustrate why so here's a kind of a",
    "start": "312400",
    "end": "319840"
  },
  {
    "text": "simple simple one um if you look at the fraction of flops spent in the in",
    "start": "319840",
    "end": "326160"
  },
  {
    "text": "attention layers of a transformer versus a MLP um this changes quite a bit so",
    "start": "326160",
    "end": "331440"
  },
  {
    "text": "this is a this is a tweet from Steven Fuller from quite a few years ago but it's it this is still true um if you",
    "start": "331440",
    "end": "339520"
  },
  {
    "text": "look at small models it looks like the number of flops in the attention versus the MLP layers are roughly comparable",
    "start": "339520",
    "end": "347120"
  },
  {
    "text": "but if you go up to 175 billion then the you know the MLPS really dominate right",
    "start": "347120",
    "end": "353360"
  },
  {
    "text": "so why does this matter well if you spend a lot of time at small scale and you're optimizing the attention you",
    "start": "353360",
    "end": "360639"
  },
  {
    "text": "might be optimizing the wrong thing because um at larger scale it's it",
    "start": "360639",
    "end": "366720"
  },
  {
    "text": "doesn't it just get gets gets washed out this is kind of a simple example because you can literally make this plot without",
    "start": "366720",
    "end": "373120"
  },
  {
    "text": "actually any compute you just like do it's napkin math um here's something",
    "start": "373120",
    "end": "378639"
  },
  {
    "text": "that's a little bit harder to grapple with is just emergent behavior so this is a paper from Jason Wave from 2022 and",
    "start": "378639",
    "end": "387680"
  },
  {
    "text": "um here this plot shows that as you increase the amount of training flops",
    "start": "387680",
    "end": "393600"
  },
  {
    "text": "and you look at accuracy a bunch on a bunch of tasks you'll see that for a while it looks like the accuracy nothing",
    "start": "393600",
    "end": "400479"
  },
  {
    "text": "is happening and all of the sudden you get these kind of uh you know emergent of various phenomena like in context",
    "start": "400479",
    "end": "407680"
  },
  {
    "text": "learning so if you were hanging around at this scale you would have be concluding that well these language",
    "start": "407680",
    "end": "412960"
  },
  {
    "text": "models really don't work when in fact you had to scale up to get that",
    "start": "412960",
    "end": "418520"
  },
  {
    "text": "behavior so so don't despair we can still learn something in this class and",
    "start": "418520",
    "end": "425280"
  },
  {
    "text": "but we have to be very precise about what we're learning so there's three types of knowledge there's the mechanics",
    "start": "425280",
    "end": "432400"
  },
  {
    "text": "of how things work this we can teach you we can teach you what a transformer is you can you'll implement a transformer",
    "start": "432400",
    "end": "438639"
  },
  {
    "text": "we can teach you how model parallelism leverages GPUs efficiently these are",
    "start": "438639",
    "end": "443680"
  },
  {
    "text": "just like kind of the raw ingredients the mechanics so that's fine we can also",
    "start": "443680",
    "end": "449919"
  },
  {
    "text": "teach you mindset so this is something a bit more subtle and seems like a little bit you know um fuzzy but uh this is",
    "start": "449919",
    "end": "459240"
  },
  {
    "text": "actually in some ways more important I would say because um the mindset that",
    "start": "459240",
    "end": "465440"
  },
  {
    "text": "we're going to take is that we want to squeeze as most out of the hardware as possible and take scaling seriously",
    "start": "465440",
    "end": "472000"
  },
  {
    "text": "right because in some sense the mechanics all those we'll see later that all of these ingredients have been",
    "start": "472000",
    "end": "477039"
  },
  {
    "text": "around for a while but it was really I think the scaling mindset that open AI",
    "start": "477039",
    "end": "482400"
  },
  {
    "text": "pioneered that led to this next generation of um AI models so mindset I",
    "start": "482400",
    "end": "488400"
  },
  {
    "text": "think hopefully we can you know bang into you that to think in a certain way and then thirdly is",
    "start": "488400",
    "end": "495000"
  },
  {
    "text": "intuitions and this is about which data um and modeling decisions lead to good",
    "start": "495000",
    "end": "501120"
  },
  {
    "text": "models this unfortunately we can only partially teach you and this is because",
    "start": "501120",
    "end": "506800"
  },
  {
    "text": "what architectures and what data sets work at no scales might not be the same",
    "start": "506800",
    "end": "512320"
  },
  {
    "text": "ones that work at um large scales and but you know that's just uh but",
    "start": "512320",
    "end": "518159"
  },
  {
    "text": "hopefully you got two and a half out of three so that's um pretty good bang for your buck um okay speaking of intuitions",
    "start": "518159",
    "end": "526160"
  },
  {
    "text": "there's this sort of I guess sad reality of things that you know you can tell a",
    "start": "526160",
    "end": "531200"
  },
  {
    "text": "lot of stories about why certain um things in the transformer are the way they are but sometimes it's just you",
    "start": "531200",
    "end": "537920"
  },
  {
    "text": "know come you do the experiments and the experiments speak um so for example there's this nom paper that introduced",
    "start": "537920",
    "end": "544800"
  },
  {
    "text": "the swiggloo which is something that we'll uh see a bit more in the in this class which is a type of nonlinearity",
    "start": "544800",
    "end": "551399"
  },
  {
    "text": "um and in the conclusion you know the results are quite good and this got",
    "start": "551399",
    "end": "557120"
  },
  {
    "text": "adopted but in the conclusion there's this honest statement that we offer no explanation except for this is divine",
    "start": "557120",
    "end": "564040"
  },
  {
    "text": "benevolence so there you go this is uh um the extent to our under of our",
    "start": "564040",
    "end": "571640"
  },
  {
    "text": "understanding okay so now let's talk about this bitter lesson that I'm sure people have you know heard about i think",
    "start": "571640",
    "end": "578560"
  },
  {
    "text": "there's a sort of a misconception that the bitter lesson means that scale is all that matters algorithms don't matter",
    "start": "578560",
    "end": "585279"
  },
  {
    "text": "all you do is pump more capital into building the model and you're good to go i think this couldn't be further from",
    "start": "585279",
    "end": "591440"
  },
  {
    "text": "the truth i think the right interpretation is that algorithms at scale is what matters and because at the",
    "start": "591440",
    "end": "598480"
  },
  {
    "text": "end of the day your accuracy of your model is really a product of your efficiency and the number of resources",
    "start": "598480",
    "end": "606160"
  },
  {
    "text": "you put in and actually efficiency if you think about it is way more important",
    "start": "606160",
    "end": "612080"
  },
  {
    "text": "at larger scale because if you're spending you know hundreds of millions of dollars you cannot afford to be",
    "start": "612080",
    "end": "617920"
  },
  {
    "text": "wasteful in the same way that if you're uh looking at running a job on your on",
    "start": "617920",
    "end": "624640"
  },
  {
    "text": "your local cluster you might run it again you fail you you debug it and if",
    "start": "624640",
    "end": "629760"
  },
  {
    "text": "you look at actually the utilization and the use I'm I'm sure open is way more",
    "start": "629760",
    "end": "635440"
  },
  {
    "text": "efficient than any of us um right now so efficiency really is important and",
    "start": "635440",
    "end": "642240"
  },
  {
    "text": "furthermore this I think is this point is maybe not as well appreciated in the sort of scaling rhetoric so to speak um",
    "start": "642240",
    "end": "650480"
  },
  {
    "text": "which is that if you look at efficiency which is combination of hardware and algorithms but if you just look at the",
    "start": "650480",
    "end": "656160"
  },
  {
    "text": "algorithm efficiency there's this nice open air paper from 2020 that showed uh",
    "start": "656160",
    "end": "662959"
  },
  {
    "text": "over the period of 2012 to 2019 there's a 44 for X if algorithmic efficiency",
    "start": "662959",
    "end": "670399"
  },
  {
    "text": "improvement in um the time that it took to train imageet to a certain level of",
    "start": "670399",
    "end": "675560"
  },
  {
    "text": "accuracy right so this is huge and I think if you I don't know if you could see the the abstract here um this is",
    "start": "675560",
    "end": "682880"
  },
  {
    "text": "faster than Morris law right so algorithms do matter if you didn't have this efficiency you would be",
    "start": "682880",
    "end": "690480"
  },
  {
    "text": "paying 44 times more cost this is for image models but uh there's some results",
    "start": "690480",
    "end": "696880"
  },
  {
    "text": "language as well okay so with all that I think the right framing or mindset to",
    "start": "696880",
    "end": "702320"
  },
  {
    "text": "have is what is the best model one can build given a certain compute and data budget okay and this question makes",
    "start": "702320",
    "end": "709600"
  },
  {
    "text": "sense no matter what scale you're at because um you're sort of like act it's",
    "start": "709600",
    "end": "714640"
  },
  {
    "text": "accuracy per resources and of course if you can raise the capital and get more resources you'll get better models but",
    "start": "714640",
    "end": "721519"
  },
  {
    "text": "as researchers our goal is to improve the efficiency of the algorithms okay so maximize efficiency",
    "start": "721519",
    "end": "729680"
  },
  {
    "text": "we're going to hear a lot of that okay so now let me talk a little",
    "start": "729680",
    "end": "735200"
  },
  {
    "text": "bit about the current uh landscape um and a little bit of I guess you know",
    "start": "735200",
    "end": "741120"
  },
  {
    "text": "obligatory history um so language models have been around for a while now um you",
    "start": "741120",
    "end": "747920"
  },
  {
    "text": "know go going back to Shannon um you know who looked at language models a way to estimate the entropy of um English um",
    "start": "747920",
    "end": "755920"
  },
  {
    "text": "I think in AI they really were prominent in NLP where they were a component of",
    "start": "755920",
    "end": "762160"
  },
  {
    "text": "larger systems like machine translation speech recognition and one thing that's maybe not as appreciated these days is",
    "start": "762160",
    "end": "768720"
  },
  {
    "text": "that if you look back in 2007 uh Google was training fairly large engram models",
    "start": "768720",
    "end": "775440"
  },
  {
    "text": "so five gram models over two trillion tokens which is a lot more tokens than GPT3 um and it was only rec I guess in",
    "start": "775440",
    "end": "783120"
  },
  {
    "text": "the last two years that um we've gotten to that in token count um but they were",
    "start": "783120",
    "end": "788800"
  },
  {
    "text": "engram models so they didn't really exhibit any of the interesting phenomena that we know of language models today",
    "start": "788800",
    "end": "795440"
  },
  {
    "text": "okay okay so in the 2010s I think a lot of the you can think about this a lot of the deep learning revolution happened",
    "start": "795440",
    "end": "802000"
  },
  {
    "text": "and a lot of the ingredients sort of kind of falling into place right so there was the first neural language",
    "start": "802000",
    "end": "808000"
  },
  {
    "text": "model from Joshra Benjel's group in back in uh 2003 there was seek to seek models",
    "start": "808000",
    "end": "814720"
  },
  {
    "text": "um this I think was a you know big deal for you know how do you basically model",
    "start": "814720",
    "end": "820200"
  },
  {
    "text": "sequences from Ilia and uh Google folks Um there's an atom optimizer which still",
    "start": "820200",
    "end": "828240"
  },
  {
    "text": "is used by the majority of uh people dating over a decade ago um there's attention mechanism which was um",
    "start": "828240",
    "end": "836480"
  },
  {
    "text": "developed in the context of machine translation um which then led up to the",
    "start": "836480",
    "end": "841839"
  },
  {
    "text": "famous attention all you need or the aka the transformer paper in 2017 people",
    "start": "841839",
    "end": "847680"
  },
  {
    "text": "were looking at how to scale mixture of experts there's a lot of work around late 20110s on how to essentially do",
    "start": "847680",
    "end": "857040"
  },
  {
    "text": "model parallelism and they were actually figuring out how you could train you know 100 billion parameter models they",
    "start": "857040",
    "end": "863360"
  },
  {
    "text": "didn't train it for very long because these are these were like more system work but the all the ingredients were",
    "start": "863360",
    "end": "869360"
  },
  {
    "text": "kind of in place um before in by the time the 2020 came around",
    "start": "869360",
    "end": "877160"
  },
  {
    "text": "um so I I think one you know other trend",
    "start": "877160",
    "end": "882240"
  },
  {
    "text": "which was starting in LPU was the idea of you know these foundation models that could be trained on a lot of text and",
    "start": "882240",
    "end": "888880"
  },
  {
    "text": "adapted to a wide range of downstream tasks so Elmo BERT um you know T5 these",
    "start": "888880",
    "end": "895839"
  },
  {
    "text": "were models that um were for their time very exciting we kind of maybe forget",
    "start": "895839",
    "end": "902959"
  },
  {
    "text": "how excited people were about you know things like bird but um it was a big deal and then I think a um I mean this",
    "start": "902959",
    "end": "910639"
  },
  {
    "text": "is abbreviated history but um I think one critical piece of the puzzle is you",
    "start": "910639",
    "end": "917279"
  },
  {
    "text": "know open AI this taking these ingredients you know they and applying",
    "start": "917279",
    "end": "922480"
  },
  {
    "text": "very nice engineering and um really kind of pushing on the kind of the scaling",
    "start": "922480",
    "end": "928639"
  },
  {
    "text": "laws embracing it as you know this is the kind of the minds set piece and that led to GPD2 and GPT3 um Google you know",
    "start": "928639",
    "end": "937680"
  },
  {
    "text": "obviously was in the game and trying to uh you know compete as well um but um",
    "start": "937680",
    "end": "944560"
  },
  {
    "text": "that sort of paved the way I think to another kind of line of work which is um",
    "start": "944560",
    "end": "950639"
  },
  {
    "text": "these were all closed models so models that weren't released and you can only access via API but they were although",
    "start": "950639",
    "end": "958160"
  },
  {
    "text": "open models starting with you know early work by you know Eluther right after GP3",
    "start": "958160",
    "end": "963440"
  },
  {
    "text": "came out Meta's early attempt um which uh didn't work maybe as quite as well um",
    "start": "963440",
    "end": "969920"
  },
  {
    "text": "Bloom um and then Meta Alibaba DeepS AI2 and there's a few others which I have",
    "start": "969920",
    "end": "976720"
  },
  {
    "text": "listed have been creating these uh open models where um the the weights are",
    "start": "976720",
    "end": "983399"
  },
  {
    "text": "released um one other piece of I think tibbit about openness I think is",
    "start": "983399",
    "end": "988720"
  },
  {
    "text": "important is that there's many levels of openness there's closed models like GPD4 there's open weight models where the",
    "start": "988720",
    "end": "996000"
  },
  {
    "text": "weights are available and there's actually a paper a very nice paper with lots of architectural details but no",
    "start": "996000",
    "end": "1002480"
  },
  {
    "text": "details about the data set and then there's uh open source models where all",
    "start": "1002480",
    "end": "1008399"
  },
  {
    "text": "the weights and data are available and the paper that where they're honestly trying to explain as much as they can um",
    "start": "1008399",
    "end": "1015839"
  },
  {
    "text": "you know but of course you can't really capture everything you know in a paper and there's no substitute for learning",
    "start": "1015839",
    "end": "1022800"
  },
  {
    "text": "how to build it except for kind of doing your yourself okay so that leads to kind of",
    "start": "1022800",
    "end": "1028240"
  },
  {
    "text": "the present day where um there's a whole host of you know frontier models from",
    "start": "1028240",
    "end": "1035038"
  },
  {
    "text": "open anthropic xi google meta deepseeek Alibaba tensen and probably a few others",
    "start": "1035039",
    "end": "1041038"
  },
  {
    "text": "um that are sort of dominate the the current you know landscape so we're kind of interested",
    "start": "1041039",
    "end": "1047438"
  },
  {
    "text": "interesting time where you know just to kind of reflect a lot of the ingredients",
    "start": "1047439",
    "end": "1052960"
  },
  {
    "text": "like I said were developed which is good because I think we're going to revisit some of those um ingredients and trace",
    "start": "1052960",
    "end": "1060720"
  },
  {
    "text": "how they these techniques work and then we're going to try to move as close as",
    "start": "1060720",
    "end": "1065760"
  },
  {
    "text": "we can to best practices on frontier models but you know using um information",
    "start": "1065760",
    "end": "1072640"
  },
  {
    "text": "from essentially the open you know community and reading between the lines",
    "start": "1072640",
    "end": "1078960"
  },
  {
    "text": "from what we know about the closed uh models okay so just as an interlude um",
    "start": "1078960",
    "end": "1087919"
  },
  {
    "text": "so what are you looking at here so um this is a executable lecture so it's a",
    "start": "1087919",
    "end": "1095600"
  },
  {
    "text": "program where I'm stepping through and it delivers the content of lecture so one thing that I think is interesting",
    "start": "1095600",
    "end": "1102000"
  },
  {
    "text": "here is that um you can embed code so if you um you can just step through code",
    "start": "1102000",
    "end": "1108880"
  },
  {
    "text": "and I think this is a smaller screen than I'm used to but uh you can look at the environment variables as you're",
    "start": "1108880",
    "end": "1114640"
  },
  {
    "text": "stepping through code so that's uh useful later when we start actually um",
    "start": "1114640",
    "end": "1120000"
  },
  {
    "text": "trying to drill down and giving code examples you can see the hierarchical structure of the lecture like we're in this module and you can see where it's",
    "start": "1120000",
    "end": "1126799"
  },
  {
    "text": "it was called from main um and you can jump to definitions um like supervised",
    "start": "1126799",
    "end": "1132400"
  },
  {
    "text": "fine-tuning which we'll uh talk about later okay and if you think this looks",
    "start": "1132400",
    "end": "1138400"
  },
  {
    "text": "like a Python program um well it is a Python program um but I've made it uh",
    "start": "1138400",
    "end": "1145919"
  },
  {
    "text": "you'll processed it so for your viewing pleasure okay so let's move on to the course uh",
    "start": "1145919",
    "end": "1155679"
  },
  {
    "text": "logistics now um actually maybe I'll pause for",
    "start": "1155679",
    "end": "1162039"
  },
  {
    "text": "questions any questions about um you know what we're learning in this",
    "start": "1162039",
    "end": "1168080"
  },
  {
    "text": "class would you",
    "start": "1168080",
    "end": "1173799"
  },
  {
    "text": "expect from this class to be able to lead a team to build a frontier model",
    "start": "1173799",
    "end": "1181000"
  },
  {
    "text": "so the question is would I expect a graduate from this class to be able to lead a team and build a frontier model",
    "start": "1181360",
    "end": "1187120"
  },
  {
    "text": "of course with you know like a billion dollars of capital yeah of course um I",
    "start": "1187120",
    "end": "1192480"
  },
  {
    "text": "would say that it's a good step but I there's definitely uh many pieces that",
    "start": "1192480",
    "end": "1198880"
  },
  {
    "text": "are missing and I think you know we thought about we should really teach like a series of classes that eventually",
    "start": "1198880",
    "end": "1205679"
  },
  {
    "text": "leads up to to as close as we can get but um I think this is maybe the first",
    "start": "1205679",
    "end": "1211039"
  },
  {
    "text": "step of the puzzle but there are a lot of things and happy to talk offline about that but I like the ambition",
    "start": "1211039",
    "end": "1217400"
  },
  {
    "text": "yeah that's what you should be doing taking the class so you can go lead teams and build frontier models",
    "start": "1217400",
    "end": "1224520"
  },
  {
    "text": "okay um okay let's talk a little bit about the course um so here's a website",
    "start": "1224520",
    "end": "1231679"
  },
  {
    "text": "everything's online this is a fiveunit class um but I I think that maybe",
    "start": "1231679",
    "end": "1238559"
  },
  {
    "text": "doesn't express the the level here um as well as this quote that I pulled out",
    "start": "1238559",
    "end": "1243919"
  },
  {
    "text": "from a course evaluation um the entire assignment was approximately the same amount of work as all five assignments",
    "start": "1243919",
    "end": "1249679"
  },
  {
    "text": "from the CSU24N plus the final project and that's the first homework assignment so not to all scare you off but just",
    "start": "1249679",
    "end": "1257760"
  },
  {
    "text": "just giving some data here um so why should you endure that um why should you",
    "start": "1257760",
    "end": "1265039"
  },
  {
    "text": "do it i I think you this class is really for people who have sort of this obsessive need to understand how things",
    "start": "1265039",
    "end": "1272000"
  },
  {
    "text": "work all the way down to the the atoms so to speak and I think if you you know",
    "start": "1272000",
    "end": "1278159"
  },
  {
    "text": "when you get through this class I think you will have really leveled up in terms of your research engineering and the",
    "start": "1278159",
    "end": "1283520"
  },
  {
    "text": "comfort level of comfort that you'll have in building ML systems at scale will just be I think um you know",
    "start": "1283520",
    "end": "1290280"
  },
  {
    "text": "something there's also a bunch of reasons that you shouldn't take the class for example example if you want to",
    "start": "1290280",
    "end": "1296159"
  },
  {
    "text": "get any research done um this quarter maybe this class isn't for you if you're",
    "start": "1296159",
    "end": "1301679"
  },
  {
    "text": "interested in learning just about the hottest new techniques um there are many other classes that can probably deliver",
    "start": "1301679",
    "end": "1308480"
  },
  {
    "text": "on that you know better um than for example you spending a lot of time debugging BPE um and this is really I",
    "start": "1308480",
    "end": "1317200"
  },
  {
    "text": "think about a class about you know the the primitives and learning things bottom up as opposed to um the the kind",
    "start": "1317200",
    "end": "1325120"
  },
  {
    "text": "of the latest um and also if you're interested in building language models",
    "start": "1325120",
    "end": "1330480"
  },
  {
    "text": "or you know 4X um this is probably not the first class",
    "start": "1330480",
    "end": "1336159"
  },
  {
    "text": "I you would take um I think practically speaking you know as much as I kind of",
    "start": "1336159",
    "end": "1342480"
  },
  {
    "text": "made fun of prompting prompting is great fine-tuning is great if you can do that and it works then I think that is",
    "start": "1342480",
    "end": "1349440"
  },
  {
    "text": "something you should absolutely start with so I don't want people taking this class and thinking like great any",
    "start": "1349440",
    "end": "1356000"
  },
  {
    "text": "problem the first step is to train a language model from scratch that is not the right way uh of thinking about it um",
    "start": "1356000",
    "end": "1364400"
  },
  {
    "text": "okay and I know that many of you um you know some of you were enrolled but we",
    "start": "1364400",
    "end": "1370880"
  },
  {
    "text": "didn't we did have a cap so we weren't able to enroll everyone and and also for the people online you can follow at at",
    "start": "1370880",
    "end": "1377520"
  },
  {
    "text": "home um all the lecture materials and assignments are online so you can look at them the lectures are also recorded",
    "start": "1377520",
    "end": "1385039"
  },
  {
    "text": "and will be put on YouTube although there will be um some number of week lag",
    "start": "1385039",
    "end": "1390159"
  },
  {
    "text": "u there um and also we'll uh offer this class next year so If you were not able",
    "start": "1390159",
    "end": "1395840"
  },
  {
    "text": "to take it this year um don't fret there will be next time okay so the class has five",
    "start": "1395840",
    "end": "1404400"
  },
  {
    "text": "assignments um and each of the assignments we don't provide scaffolding",
    "start": "1404400",
    "end": "1411120"
  },
  {
    "text": "code in the sense that the uh the you're literally give you a blank file and",
    "start": "1411120",
    "end": "1416159"
  },
  {
    "text": "you're supposed to you know build things up um and in the spirit of learning uh",
    "start": "1416159",
    "end": "1422159"
  },
  {
    "text": "building from scratch but we're not that mean um we do provide unit tests and",
    "start": "1422159",
    "end": "1427840"
  },
  {
    "text": "some adapter interfaces that allow you to check uh correctness of different uh",
    "start": "1427840",
    "end": "1433120"
  },
  {
    "text": "pieces and also the assignment write up if you walk through it does do it for sort of a gentle job of doing that but",
    "start": "1433120",
    "end": "1439919"
  },
  {
    "text": "you're kind of on your own for making um good software design decisions and",
    "start": "1439919",
    "end": "1445120"
  },
  {
    "text": "figuring out what you name your functions and how to you know organize your code which is a useful skill I",
    "start": "1445120",
    "end": "1450760"
  },
  {
    "text": "think um so one strategy I think for all",
    "start": "1450760",
    "end": "1455919"
  },
  {
    "text": "assignments is that there is a piece of assignment which is just implement the thing and make sure it's correct that",
    "start": "1455919",
    "end": "1463600"
  },
  {
    "text": "mostly you can do locally on your laptop you shouldn't need compute for that and then you should we have a cluster that",
    "start": "1463600",
    "end": "1471120"
  },
  {
    "text": "you can run um for benchmarking both accuracy and speed right so I I want",
    "start": "1471120",
    "end": "1476480"
  },
  {
    "text": "everyone to kind of embrace this idea of like you want to use as a as small data",
    "start": "1476480",
    "end": "1482080"
  },
  {
    "text": "set or as few resources as possible to you know prototype before running large jobs you shouldn't be debugging with one",
    "start": "1482080",
    "end": "1488720"
  },
  {
    "text": "billion parameter models on the cluster um if you can help it okay um there's",
    "start": "1488720",
    "end": "1496080"
  },
  {
    "text": "some assignments which will have a leaderboard um which usually is of the",
    "start": "1496080",
    "end": "1501159"
  },
  {
    "text": "form do things to make perplexity go down given a particular training budget",
    "start": "1501159",
    "end": "1506400"
  },
  {
    "text": "last year it was I think pretty um you know exciting for people to try to um",
    "start": "1506400",
    "end": "1512480"
  },
  {
    "text": "you know try different things that you either learn from the class or you read online",
    "start": "1512480",
    "end": "1517960"
  },
  {
    "text": "um and then finally I guess this year is you know this was less of a problem",
    "start": "1517960",
    "end": "1524640"
  },
  {
    "text": "last year because I guess Copilot wasn't as good but you know curs is pretty good um so I I think our general strategy is",
    "start": "1524640",
    "end": "1531600"
  },
  {
    "text": "that you know AI tools are you know can take away from learning because there",
    "start": "1531600",
    "end": "1536880"
  },
  {
    "text": "are cases where it can just solve the thing you you want it to do but you know",
    "start": "1536880",
    "end": "1542240"
  },
  {
    "text": "I think you can obviously use them judiciously so but use at your own risk you're kind of responsible for your own",
    "start": "1542240",
    "end": "1548720"
  },
  {
    "text": "learning experience here okay so uh we do have a cluster so",
    "start": "1548720",
    "end": "1554400"
  },
  {
    "text": "thank you Together AI for providing a bunch of H100s for us um there's a guide",
    "start": "1554400",
    "end": "1560080"
  },
  {
    "text": "to please read it carefully to learn how to use the cluster um and uh start your",
    "start": "1560080",
    "end": "1565919"
  },
  {
    "text": "assignments early because um the cluster will fill up towards the end of a deadline as everyone's uh trying to get",
    "start": "1565919",
    "end": "1573200"
  },
  {
    "text": "their large runs in okay um any questions about that you",
    "start": "1573200",
    "end": "1580960"
  },
  {
    "text": "mentioned it was a five unit were you able to sign up for it for like",
    "start": "1580960",
    "end": "1587120"
  },
  {
    "text": "right so the question is can you sign up for less than five units i think administratively",
    "start": "1587120",
    "end": "1592240"
  },
  {
    "text": "uh if you have to sign up for less that is possible but it's the same class and the same workload",
    "start": "1592240",
    "end": "1599799"
  },
  {
    "text": "yeah any other questions",
    "start": "1599799",
    "end": "1605000"
  },
  {
    "text": "okay so in this part I'm going to go through all the different components of",
    "start": "1607240",
    "end": "1612880"
  },
  {
    "text": "the course and just give a broad overview a preview of what you're going to experience um so remember it's all",
    "start": "1612880",
    "end": "1619440"
  },
  {
    "text": "about efficiency given hardware and data um how do you train the best model given",
    "start": "1619440",
    "end": "1625440"
  },
  {
    "text": "your resources so for example if I give you a common crawl dump a web dump and",
    "start": "1625440",
    "end": "1630559"
  },
  {
    "text": "32 H100s for two weeks what should you do there are a lot of different design",
    "start": "1630559",
    "end": "1636480"
  },
  {
    "text": "decisions um there's you know questions about the tokenizer the architecture systems optimizations you can do data",
    "start": "1636480",
    "end": "1643919"
  },
  {
    "text": "things you can do and we've organized the class into these five um units or",
    "start": "1643919",
    "end": "1650039"
  },
  {
    "text": "pillars so I'm going to go through each of them you know in turn um and talk",
    "start": "1650039",
    "end": "1656480"
  },
  {
    "text": "about what we'll cover what the assignment will involve and and then",
    "start": "1656480",
    "end": "1661840"
  },
  {
    "text": "I'll kind of wrap up okay so the goal of the basics unit is just get a basic",
    "start": "1661840",
    "end": "1667520"
  },
  {
    "text": "version of a full pipeline working so here you implement a tokenizer model",
    "start": "1667520",
    "end": "1673039"
  },
  {
    "text": "architecture and training so just say a bit more about what these components are so a",
    "start": "1673039",
    "end": "1678919"
  },
  {
    "text": "tokenizer is something that converts between strings and sequences of",
    "start": "1678919",
    "end": "1684799"
  },
  {
    "text": "integers intuitively you can think about the integers corresponding to breaking up the string into uh segments and",
    "start": "1684799",
    "end": "1693279"
  },
  {
    "text": "mapping each segment to an integer and the idea is that you just you your sequence of integers is what goes into",
    "start": "1693279",
    "end": "1699840"
  },
  {
    "text": "the actual model which has to be like a fixed uh dimension okay so in this course we'll",
    "start": "1699840",
    "end": "1706080"
  },
  {
    "text": "talk about the bip pair encoding BPE tokenizer which is um relatively simple",
    "start": "1706080",
    "end": "1713520"
  },
  {
    "text": "and um and still is is used um there",
    "start": "1713520",
    "end": "1719640"
  },
  {
    "text": "are I guess a promising set of um methods on tokenizer free approaches so",
    "start": "1719640",
    "end": "1725600"
  },
  {
    "text": "these are methods that just start with the the raw bytes and don't do",
    "start": "1725600",
    "end": "1730840"
  },
  {
    "text": "tokenization and develop a particular architecture that just takes the raw bytes um this work is is promising but",
    "start": "1730840",
    "end": "1739120"
  },
  {
    "text": "you know so far I haven't seen it been scaled to the frontier yet so we'll go with BP for now okay okay so once you've",
    "start": "1739120",
    "end": "1747200"
  },
  {
    "text": "tokenized your sequence or strings into a sequence of integers now we define a",
    "start": "1747200",
    "end": "1753440"
  },
  {
    "text": "model architecture over these sequences so the starting point here is original",
    "start": "1753440",
    "end": "1759840"
  },
  {
    "text": "transformer um that's what is the backbone of basically all um you know",
    "start": "1759840",
    "end": "1765840"
  },
  {
    "text": "frontier models um and here's architectural diagram um we won't go",
    "start": "1765840",
    "end": "1771600"
  },
  {
    "text": "into details here but uh it there's a attention you know piece and then there's a um MLP you know layer with",
    "start": "1771600",
    "end": "1779360"
  },
  {
    "text": "some you know normalization um so a lot has actually happened till",
    "start": "1779360",
    "end": "1786240"
  },
  {
    "text": "since 2017 right I think there's a sort of sense to which oh the transformer was",
    "start": "1786240",
    "end": "1791600"
  },
  {
    "text": "invented and then you know everyone's just using transformer and to first approximation that's true we're still using the same recipe but there have",
    "start": "1791600",
    "end": "1798399"
  },
  {
    "text": "been a bunch of the smaller uh improvements that do make a substantial",
    "start": "1798399",
    "end": "1803919"
  },
  {
    "text": "difference when you add them all up so for example there is um the activation",
    "start": "1803919",
    "end": "1809039"
  },
  {
    "text": "um you know nonlinear activation function the squiggly which we saw a little bit before positional embeddings",
    "start": "1809039",
    "end": "1815120"
  },
  {
    "text": "there's new positional embeddings um um these rotary positional embeddings which we'll uh talk about um normalization um",
    "start": "1815120",
    "end": "1825120"
  },
  {
    "text": "you know instead of using layer norm we're going to look at something called RMS norm which is similar but simpler um",
    "start": "1825120",
    "end": "1830960"
  },
  {
    "text": "there's a question where you place the normalization which has been changed from the original transformer um the MLP",
    "start": "1830960",
    "end": "1837679"
  },
  {
    "text": "use uh the canonical version is a dense MLP And you can replace that with mixture of experts um attention is",
    "start": "1837679",
    "end": "1845360"
  },
  {
    "text": "something that has actually been uh getting a lot of um attention I guess um",
    "start": "1845360",
    "end": "1850799"
  },
  {
    "text": "there's there's full attention and then there's you know sliding window attention and linear attention all of",
    "start": "1850799",
    "end": "1856559"
  },
  {
    "text": "these are trying to prevent the quadratic blow up there's also lower dimensional versions like you know GQA",
    "start": "1856559",
    "end": "1864080"
  },
  {
    "text": "and MLA which we'll get to in a second um or not in a second but in a future",
    "start": "1864080",
    "end": "1869600"
  },
  {
    "text": "lecture and then you know the most kind of maybe radical thing is other",
    "start": "1869600",
    "end": "1874640"
  },
  {
    "text": "alternatives to the um transformer like space models like hyena where they're",
    "start": "1874640",
    "end": "1880640"
  },
  {
    "text": "not doing you know attention but you know some other sort of operation and",
    "start": "1880640",
    "end": "1886559"
  },
  {
    "text": "sometimes you get best of both worlds by you know mixing making a hybrid model",
    "start": "1886559",
    "end": "1892000"
  },
  {
    "text": "that mixes these in with transformers um okay so once you define your",
    "start": "1892000",
    "end": "1897440"
  },
  {
    "text": "architecture you need a train so there's a you know design decisions include optimizer so atom w uh which is a",
    "start": "1897440",
    "end": "1905760"
  },
  {
    "text": "variant basically atom fixed up um is is still very prominent so we'll mostly",
    "start": "1905760",
    "end": "1912080"
  },
  {
    "text": "work with that but uh it is worth mentioning that there is more recent optimizers like muan and soap that have",
    "start": "1912080",
    "end": "1918480"
  },
  {
    "text": "shown promise um learning rate schedule um you know batch size you know whether",
    "start": "1918480",
    "end": "1924640"
  },
  {
    "text": "you do regularization or not hyperparameters there's a lot of details here and and I think this class is one",
    "start": "1924640",
    "end": "1932960"
  },
  {
    "text": "where the details do matter because you can easily have you know order of magnitude difference between a welltuned",
    "start": "1932960",
    "end": "1939840"
  },
  {
    "text": "you know architecture and something that's just like a vanilla transformer so in assignment one basically you'll",
    "start": "1939840",
    "end": "1947200"
  },
  {
    "text": "implement the BP tokenizer um I'll warn um you that this is actually the part",
    "start": "1947200",
    "end": "1954399"
  },
  {
    "text": "that seems to have been a lot of surprising maybe a lot of work for",
    "start": "1954399",
    "end": "1960320"
  },
  {
    "text": "people so um just you know you're warned and uh you also implement the",
    "start": "1960320",
    "end": "1966200"
  },
  {
    "text": "transformer crossmput loss atomw optimizer and training loop so again the",
    "start": "1966200",
    "end": "1971600"
  },
  {
    "text": "whole stack and you know we're not making you implement you know pietorch",
    "start": "1971600",
    "end": "1977039"
  },
  {
    "text": "uh from scratch so you can use pietorch but you can't use like you know the",
    "start": "1977039",
    "end": "1982399"
  },
  {
    "text": "transformer implementation for pietorch you there's a small list of um uh",
    "start": "1982399",
    "end": "1987840"
  },
  {
    "text": "functions that you can use and you can only use those okay so we're going to have some uh you know tiny stories and",
    "start": "1987840",
    "end": "1995279"
  },
  {
    "text": "open web text data sets that you'll train on and then there will be a leaderboard um to minimize the open web",
    "start": "1995279",
    "end": "2001760"
  },
  {
    "text": "text perplexity we'll give you 90 minutes on a a H100 and see what you can do so this is last year um so see we",
    "start": "2001760",
    "end": "2011279"
  },
  {
    "text": "have the top so this is the number to beat for this year okay all right so that's the basics now",
    "start": "2011279",
    "end": "2020240"
  },
  {
    "text": "after basics um I mean in some sense you're done right like you have ability to train a",
    "start": "2020240",
    "end": "2027679"
  },
  {
    "text": "transformer what what else do you need so the system part really goes into how",
    "start": "2027679",
    "end": "2035519"
  },
  {
    "text": "you can optimize this further so how do you get the most out of hardware and for this we need to take a closer look at",
    "start": "2035519",
    "end": "2042240"
  },
  {
    "text": "the hardware and how we can you know leverage it so there's kernels parallelism and inference are the three",
    "start": "2042240",
    "end": "2048878"
  },
  {
    "text": "components of this uh unit so okay so to first talk about kernels um let's talk a",
    "start": "2048879",
    "end": "2056720"
  },
  {
    "text": "little bit about what a GPU looks like okay so a GPU um which we'll get much",
    "start": "2056720",
    "end": "2062079"
  },
  {
    "text": "more into um is basically a huge array of these um you know little uh units",
    "start": "2062079",
    "end": "2070720"
  },
  {
    "text": "that do floatingoint operations um and maybe the one thing to note is that this",
    "start": "2070720",
    "end": "2077839"
  },
  {
    "text": "is the GPU chip and here is the um the memory that's actually offchip um and",
    "start": "2077839",
    "end": "2085280"
  },
  {
    "text": "then there's some other memory like L2 caches and L1 caches on chip and so the",
    "start": "2085280",
    "end": "2092480"
  },
  {
    "text": "basic idea is that compute has to happen here your data might be somewhere else",
    "start": "2092480",
    "end": "2098000"
  },
  {
    "text": "and how do you basically organize your compute so that um you can be most",
    "start": "2098000",
    "end": "2104000"
  },
  {
    "text": "efficient so one quick analogy is imagine that your your memory is and is",
    "start": "2104000",
    "end": "2111440"
  },
  {
    "text": "where you can store like your data and the model parameters is like a warehouse",
    "start": "2111440",
    "end": "2117119"
  },
  {
    "text": "and your compute is like the the the factory and what you what ends up being",
    "start": "2117119",
    "end": "2123440"
  },
  {
    "text": "a big bottleneck is just data movement costs right um so the thing that we have",
    "start": "2123440",
    "end": "2131760"
  },
  {
    "text": "to do is how do you organize the compute like even a matrix multiplication to maximize the utilization of the GPUs by",
    "start": "2131760",
    "end": "2140480"
  },
  {
    "text": "minimizing the data movement and there's a bunch of techniques like fusion and um",
    "start": "2140480",
    "end": "2146320"
  },
  {
    "text": "and tiling that allow you to do that so we'll get all into the details of that and to implement and leverage a kernel",
    "start": "2146320",
    "end": "2154800"
  },
  {
    "text": "uh we're going to look at Triton there's other things you can do with various levels of uh sophistication but we're",
    "start": "2154800",
    "end": "2160880"
  },
  {
    "text": "going to use Triton which is developed by OpenAI in a popular way to build kernels okay so we're going to write",
    "start": "2160880",
    "end": "2166480"
  },
  {
    "text": "some kernels that's for one GPU so now um in general you have these big runs",
    "start": "2166480",
    "end": "2173320"
  },
  {
    "text": "take you know ten thousands if not tens of thousands of GPUs and but even at 8",
    "start": "2173320",
    "end": "2180560"
  },
  {
    "text": "it kind of starts becoming interesting because um you have a lot of GPUs they're connected to some CPU nodes and",
    "start": "2180560",
    "end": "2187440"
  },
  {
    "text": "they also have are directly connected via N MV switch MVL link um and",
    "start": "2187440",
    "end": "2195079"
  },
  {
    "text": "the it's the same idea right now the only thing is that data movement between",
    "start": "2195079",
    "end": "2200160"
  },
  {
    "text": "GPUs is even slower right um and so we need to figure out how to put um model",
    "start": "2200160",
    "end": "2209280"
  },
  {
    "text": "you know parameters and activations and gradients and put them on the GPUs and do the computation and to minimize",
    "start": "2209280",
    "end": "2215839"
  },
  {
    "text": "amount of you know movement um and then so we're going to explore different type of techniques like data",
    "start": "2215839",
    "end": "2222520"
  },
  {
    "text": "parallelism and you know tensor parallelism and and so on so",
    "start": "2222520",
    "end": "2227960"
  },
  {
    "text": "um so that's all I'll say about that and finally inference um is something that",
    "start": "2227960",
    "end": "2234880"
  },
  {
    "text": "we didn't actually do last year in the class um although we had a guest lecture",
    "start": "2234880",
    "end": "2240720"
  },
  {
    "text": "um but this is important because um inference is how you actually use a",
    "start": "2240720",
    "end": "2246880"
  },
  {
    "text": "model right it's basically the task of generating tokens given a prompt given a trained model and it also turns out to",
    "start": "2246880",
    "end": "2255119"
  },
  {
    "text": "be really useful for a bunch of other things besides just chatting with your your favorite um um model you need it",
    "start": "2255119",
    "end": "2263040"
  },
  {
    "text": "for reinforcement learning test time compute which has been you know very popular lately and even evaluating",
    "start": "2263040",
    "end": "2269359"
  },
  {
    "text": "models you need uh to do inference so we're going to spend some time talking about inference um actually if you think",
    "start": "2269359",
    "end": "2275920"
  },
  {
    "text": "about the globally the cost that's dedic that's spent on inference is going it's",
    "start": "2275920",
    "end": "2283200"
  },
  {
    "text": "you know ex eclipsing the cost that it is used to train models because training",
    "start": "2283200",
    "end": "2288960"
  },
  {
    "text": "despite it being very intensive is ultimately a onetime cost and inference",
    "start": "2288960",
    "end": "2294400"
  },
  {
    "text": "is cost scales with every use and the more people use your your your model the",
    "start": "2294400",
    "end": "2300160"
  },
  {
    "text": "the more you'll need inference to be efficient okay so um in inference there's two",
    "start": "2300160",
    "end": "2308560"
  },
  {
    "text": "phases there's a prefill and a decode prefill is you take the prompt and you can run it through the model and get",
    "start": "2308560",
    "end": "2314880"
  },
  {
    "text": "some you know activations and then decode is you go autogressively one by",
    "start": "2314880",
    "end": "2320079"
  },
  {
    "text": "one and generate tokens so prefill all the tokens are",
    "start": "2320079",
    "end": "2325280"
  },
  {
    "text": "given so you can process everything at once so this is exactly what you see at training time and generally this is a",
    "start": "2325280",
    "end": "2331839"
  },
  {
    "text": "good setting to be in because um you can par it's naturally parallel and you're",
    "start": "2331839",
    "end": "2337119"
  },
  {
    "text": "mostly computebound what makes inference I think uh special and difficult is that this auto reggressive decoding you need",
    "start": "2337119",
    "end": "2343760"
  },
  {
    "text": "to generate one token at a time and ends you it's hard to actually saturate all your GPUs and it becomes you know memory",
    "start": "2343760",
    "end": "2349520"
  },
  {
    "text": "bound because you're constantly you know moving data around and we'll talk about a few ways to speed the models up um",
    "start": "2349520",
    "end": "2356880"
  },
  {
    "text": "just speed inference up you can use a cheaper model um you can use this uh",
    "start": "2356880",
    "end": "2362079"
  },
  {
    "text": "really cool technique called speculative decoding where you use a cheaper model to sort of scout ahead and generate",
    "start": "2362079",
    "end": "2368960"
  },
  {
    "text": "multiple tokens and then if these tokens happen to be good by some for some",
    "start": "2368960",
    "end": "2374079"
  },
  {
    "text": "definition good you can have the full model model just you know score in and",
    "start": "2374079",
    "end": "2379520"
  },
  {
    "text": "accept them all in in parallel um and then there's a bunch of systems optimizations that you can do as well",
    "start": "2379520",
    "end": "2387359"
  },
  {
    "text": "okay so after the systems oh okay assignment two so um you're going to",
    "start": "2387359",
    "end": "2393920"
  },
  {
    "text": "implement a kernel you're going to implement um some parallelism so data uh",
    "start": "2393920",
    "end": "2400560"
  },
  {
    "text": "parallel is is very natural and so we we'll do that",
    "start": "2400560",
    "end": "2406119"
  },
  {
    "text": "um some of the model parallelism like FSTP turns out to be a bit kind of uh",
    "start": "2406119",
    "end": "2413280"
  },
  {
    "text": "complicated to do from scratch so we'll do sort of a baby version of that um but",
    "start": "2413280",
    "end": "2418640"
  },
  {
    "text": "you know I encourage you to learn and you know about the full version um we'll go over the full version in class but um",
    "start": "2418640",
    "end": "2425680"
  },
  {
    "text": "implementing from scratch might be a bit you know too much um and then I think an",
    "start": "2425680",
    "end": "2431119"
  },
  {
    "text": "important thing is getting in the habit of always benchmarking profile i think that's actually probably the most",
    "start": "2431119",
    "end": "2436680"
  },
  {
    "text": "important thing is that you can implement things but unless you have a a",
    "start": "2436680",
    "end": "2442480"
  },
  {
    "text": "feedback on how well your implementation is going and where the bottlenecks are you're just going to be kind of flying",
    "start": "2442480",
    "end": "2449560"
  },
  {
    "text": "blind okay so unit three is uh scaling",
    "start": "2449560",
    "end": "2454680"
  },
  {
    "text": "laws um and here the goal is you want to do experiments at small scale and figure",
    "start": "2454680",
    "end": "2461280"
  },
  {
    "text": "things out and then um predict the hyperparameters and loss at large scale",
    "start": "2461280",
    "end": "2467960"
  },
  {
    "text": "so here's a fundamental question so um if I give you a flops budget you know",
    "start": "2467960",
    "end": "2475200"
  },
  {
    "text": "what model size should you use if you use a larger model that means you can train on less data and if you use a",
    "start": "2475200",
    "end": "2480880"
  },
  {
    "text": "smaller model you can train on more data so what's the right balance here and this has been quite ex studied quite",
    "start": "2480880",
    "end": "2487359"
  },
  {
    "text": "extensively and figured out by a series of paper from open air and and deep mind so if you hear the term chinchilla",
    "start": "2487359",
    "end": "2494160"
  },
  {
    "text": "optimal this is what this is referring to and the and the basic idea is that for every compute budget number of flops",
    "start": "2494160",
    "end": "2502880"
  },
  {
    "text": "you can vary the number of parameters of your model okay and that and then you",
    "start": "2502880",
    "end": "2508640"
  },
  {
    "text": "measure how good that model is so for every level of compute you can get the",
    "start": "2508640",
    "end": "2514160"
  },
  {
    "text": "optimal um you know parameter count and then what you do is you you can fit a a",
    "start": "2514160",
    "end": "2521160"
  },
  {
    "text": "curve to extrapolate and see if you had let's say you know one E22 flops you",
    "start": "2521160",
    "end": "2529200"
  },
  {
    "text": "know what would be the parameter size and it turns out these minimum when you plot them it's actually remarkably um",
    "start": "2529200",
    "end": "2535680"
  },
  {
    "text": "you know linear um which led leads to this like very actually simple but",
    "start": "2535680",
    "end": "2542000"
  },
  {
    "text": "useful um rule of thumb which is that if you have um a particular um model of size n if",
    "start": "2542000",
    "end": "2552480"
  },
  {
    "text": "you multiply by 20 that's the number of tokens you should train on essentially so that means if I say you know 1.4 four",
    "start": "2552480",
    "end": "2560560"
  },
  {
    "text": "billion parameter model should be trained on 28 billion you know tokens okay but you know this doesn't take into",
    "start": "2560560",
    "end": "2567520"
  },
  {
    "text": "account inference cost this is literally how can you train the best model regardless of how big that model is so",
    "start": "2567520",
    "end": "2574400"
  },
  {
    "text": "there's some limitations here but it's nonetheless been extremely useful for model development so in this assignment",
    "start": "2574400",
    "end": "2580960"
  },
  {
    "text": "this is kind of um you know fun because we define a quote unquote training API",
    "start": "2580960",
    "end": "2587200"
  },
  {
    "text": "which you can query with a particular set of hyperparameters you specify the architecture you know um and batch size",
    "start": "2587200",
    "end": "2594319"
  },
  {
    "text": "and so on and we return you a loss that you your decisions will get you okay so",
    "start": "2594319",
    "end": "2601599"
  },
  {
    "text": "your job is you have a flops budget and you're going to try to figure out how to",
    "start": "2601599",
    "end": "2607599"
  },
  {
    "text": "train a bunch of models and then gather the data you're going to fit a scaling law to the gather data and then you're",
    "start": "2607599",
    "end": "2615040"
  },
  {
    "text": "going to submit your prediction on you know what you would choose to be the",
    "start": "2615040",
    "end": "2620400"
  },
  {
    "text": "hyperparameters what model size and and so on um at a larger scale okay so this",
    "start": "2620400",
    "end": "2627920"
  },
  {
    "text": "is a case where you have to be really we want to put you in this position where uh there's some stakes i mean this is",
    "start": "2627920",
    "end": "2634640"
  },
  {
    "text": "not like burning real compute but you know once you run out of your flops budget that's that's it um so you have",
    "start": "2634640",
    "end": "2641520"
  },
  {
    "text": "to be very careful in terms of how you prioritize what experiments uh to run",
    "start": "2641520",
    "end": "2647200"
  },
  {
    "text": "which is something that the frontier labs have to do all the time and there will be a leaderboard uh for this which",
    "start": "2647200",
    "end": "2652800"
  },
  {
    "text": "is minimize flops minimize loss given your flops budget",
    "start": "2652800",
    "end": "2658920"
  },
  {
    "text": "question from point 24 yeah so if we're",
    "start": "2660359",
    "end": "2665440"
  },
  {
    "text": "working ahead should we expect assignments to change over time or are these going to be the final assignments",
    "start": "2665440",
    "end": "2671760"
  },
  {
    "text": "so the the question is that these links are from 2024 um the rough assignments",
    "start": "2671760",
    "end": "2677280"
  },
  {
    "text": "the the rough structure will be the same from 2025 there will be some modifications but if you look at these",
    "start": "2677280",
    "end": "2682960"
  },
  {
    "text": "you should have a pretty good idea of what to",
    "start": "2682960",
    "end": "2686559"
  },
  {
    "text": "expect okay so let's go into data now um",
    "start": "2688119",
    "end": "2693280"
  },
  {
    "text": "okay so up until now you've done you've have scaling laws you have systems you can you have your transformer",
    "start": "2693280",
    "end": "2700400"
  },
  {
    "text": "implementation everything you're really kind of good to go but data I would say is a really kind of key ingredient that",
    "start": "2700400",
    "end": "2708480"
  },
  {
    "text": "I think differentiates in some sense and the the question to ask here is what do",
    "start": "2708480",
    "end": "2714960"
  },
  {
    "text": "I want this model to do right because it's what I what the model does is completely deter I mean mostly",
    "start": "2714960",
    "end": "2721760"
  },
  {
    "text": "determined by uh the data if I put if I train a multilingual data it will have multilingual capabilities if I train on",
    "start": "2721760",
    "end": "2728000"
  },
  {
    "text": "code it'll have code capabilities it's not you know it's very natural and usually data sets are a conglomeration",
    "start": "2728000",
    "end": "2735680"
  },
  {
    "text": "of a lot of different pieces there's you know uh this is from a pile which is you know four years ago but the same idea I",
    "start": "2735680",
    "end": "2743040"
  },
  {
    "text": "think holds you know you have data from you know the web this is common crawl um",
    "start": "2743040",
    "end": "2748640"
  },
  {
    "text": "you have you know maybe sack exchange Wikipedia GitHub and different you know sources which are",
    "start": "2748640",
    "end": "2754119"
  },
  {
    "text": "curated and so in the data section we're going to start talking about evaluation",
    "start": "2754119",
    "end": "2759920"
  },
  {
    "text": "which is given a model how do you evaluate whether it's any good so we're going to talk about perplexity way",
    "start": "2759920",
    "end": "2766760"
  },
  {
    "text": "measures standard kind of standardized testing like MMLU",
    "start": "2766760",
    "end": "2771839"
  },
  {
    "text": "um if you have models that generate utterances for instruction following how do you evaluate that um there's you know",
    "start": "2771839",
    "end": "2779839"
  },
  {
    "text": "also decisions about if you can enso or do chain of thought at test time um you",
    "start": "2779839",
    "end": "2786000"
  },
  {
    "text": "know how does that affect your evaluation and then you know you can talk about entire systems um evaluation",
    "start": "2786000",
    "end": "2794400"
  },
  {
    "text": "of entire system not just a language model because language models often get these days plugged into some agentic",
    "start": "2794400",
    "end": "2800319"
  },
  {
    "text": "system or something um okay so now after",
    "start": "2800319",
    "end": "2806000"
  },
  {
    "text": "establishing evaluation um let's look at data curation so this is I I think an",
    "start": "2806000",
    "end": "2812240"
  },
  {
    "text": "important point that people don't realize i often hear people say oh we're training the the model on the internet",
    "start": "2812240",
    "end": "2820240"
  },
  {
    "text": "this just doesn't make sense right data doesn't just you know you know fall from",
    "start": "2820240",
    "end": "2825599"
  },
  {
    "text": "the sky and there's the internet that you can you know um pipe into your model",
    "start": "2825599",
    "end": "2830960"
  },
  {
    "text": "um you know data has to always be actively acquired uh somehow um",
    "start": "2830960",
    "end": "2838040"
  },
  {
    "text": "so even if you you know just just as an example of you know I always tell people",
    "start": "2838040",
    "end": "2844000"
  },
  {
    "text": "look at the data um and so let's look at some data so this is uh some common",
    "start": "2844000",
    "end": "2849960"
  },
  {
    "text": "crawl um you know data I'm going to take 10 documents and I think hopefully this",
    "start": "2849960",
    "end": "2857040"
  },
  {
    "text": "works okay I think the rendering is off but Um you can kind of see uh this is a",
    "start": "2857040",
    "end": "2864480"
  },
  {
    "text": "this is a sort of random sample of of common crawl",
    "start": "2864480",
    "end": "2870200"
  },
  {
    "text": "um and you can see that this is maybe",
    "start": "2870200",
    "end": "2877240"
  },
  {
    "text": "um not exactly the data oh here's some actually real text here okay that's cool",
    "start": "2877240",
    "end": "2882960"
  },
  {
    "text": "um but if you look at most of common crawl aside from this is a different language but you can also see this is",
    "start": "2882960",
    "end": "2888240"
  },
  {
    "text": "very spammy sites and you'll quickly realize that a lot of the web is just",
    "start": "2888240",
    "end": "2893920"
  },
  {
    "text": "you know trash and so well okay maybe that's not that's surprising but it's",
    "start": "2893920",
    "end": "2900319"
  },
  {
    "text": "more trash than you would actually expect i promise um so what what I'm saying is that there's",
    "start": "2900319",
    "end": "2907119"
  },
  {
    "text": "a lot of work that needs to happen in data so you can crawl the internet you can take books archives papers um GitHub",
    "start": "2907119",
    "end": "2915839"
  },
  {
    "text": "um and there's actually a lot of processing that needs to happen um you know there's also legal questions about",
    "start": "2915839",
    "end": "2922559"
  },
  {
    "text": "what data you can you know train on which we'll touch on um nowadays a lot",
    "start": "2922559",
    "end": "2927680"
  },
  {
    "text": "of frontier models have to actually buy data um because the data on the internet that's publicly uh accessible is",
    "start": "2927680",
    "end": "2935440"
  },
  {
    "text": "actually uh turns out to be you know a bit limited for that kind of the you",
    "start": "2935440",
    "end": "2940559"
  },
  {
    "text": "know the really frontier um performance and also I I think it's important to remember that this data that's scraped",
    "start": "2940559",
    "end": "2947280"
  },
  {
    "text": "it's not actually text right first of all it's HTML or it's PDFs or in the",
    "start": "2947280",
    "end": "2952559"
  },
  {
    "text": "case of code it's just directories so there has to be an explicit process that takes this data and turns it into text",
    "start": "2952559",
    "end": "2960880"
  },
  {
    "text": "okay so we're going to talk about the transformation from HTML to to text um",
    "start": "2960880",
    "end": "2967599"
  },
  {
    "text": "and this is going to be a lossy process um so the trick is how can you preserve",
    "start": "2967599",
    "end": "2974240"
  },
  {
    "text": "the content and some of the structure um without um you know basically just",
    "start": "2974240",
    "end": "2981359"
  },
  {
    "text": "having HTML um filtering as you could you know surmise is going to be very",
    "start": "2981359",
    "end": "2987599"
  },
  {
    "text": "important both for getting high quality data but also removing harmful content um generally people train classifiers to",
    "start": "2987599",
    "end": "2994240"
  },
  {
    "text": "do this the dduplication is also um an important step which we'll talk about",
    "start": "2994240",
    "end": "3000480"
  },
  {
    "text": "okay so assignment four is all about data we're going to give you the raw",
    "start": "3000480",
    "end": "3005520"
  },
  {
    "text": "common crawl you know dump so you can see just how bad it is and you're going",
    "start": "3005520",
    "end": "3011520"
  },
  {
    "text": "to train classifiers ddup and then there's going to be a leaderboard where you're going to try to um minimize",
    "start": "3011520",
    "end": "3018400"
  },
  {
    "text": "perplexity given your token budget so now let's now have the data",
    "start": "3018400",
    "end": "3024640"
  },
  {
    "text": "you've done this built all your fancy kernels you've trained now you can really train models but at this point",
    "start": "3024640",
    "end": "3031119"
  },
  {
    "text": "what you'll get is a model that can um complete the next token right and",
    "start": "3031119",
    "end": "3038400"
  },
  {
    "text": "this is called a a essentially a base model and I think about it as a model that has a lot of raw potential but it",
    "start": "3038400",
    "end": "3044319"
  },
  {
    "text": "needs to be aligned or modified some way and alignment is a process of making it",
    "start": "3044319",
    "end": "3049440"
  },
  {
    "text": "useful so in alignment captures a lot of different",
    "start": "3049440",
    "end": "3054800"
  },
  {
    "text": "things but three things I think it captures is that you want to get the language model to follow instructions",
    "start": "3054800",
    "end": "3062000"
  },
  {
    "text": "right completing the next token is not necessarily following the instruction it'll just complete the instruction or",
    "start": "3062000",
    "end": "3067680"
  },
  {
    "text": "whatever it thinks will follow the instruction um you get to here specify",
    "start": "3067680",
    "end": "3073680"
  },
  {
    "text": "the style of the generation whether you want to be a long or short whether you",
    "start": "3073680",
    "end": "3078880"
  },
  {
    "text": "want bullets whether whether you know you want it to be witty or have SAS or not um and you when you play with um you",
    "start": "3078880",
    "end": "3087200"
  },
  {
    "text": "know you you know chatbt versus grock you'll see that there's different alignment uh that has happened and then",
    "start": "3087200",
    "end": "3094240"
  },
  {
    "text": "also safety um one important thing is for these models to be able to refuse answers that can be you know harmful so",
    "start": "3094240",
    "end": "3101680"
  },
  {
    "text": "that's where alignment also kicks in so there's generally two phases of",
    "start": "3101680",
    "end": "3106800"
  },
  {
    "text": "alignment there's supervised fine-tuning and here the goal is I mean it's very",
    "start": "3106800",
    "end": "3112720"
  },
  {
    "text": "simple you basically gather a set of um user assistant pairs um so prompt",
    "start": "3112720",
    "end": "3120800"
  },
  {
    "text": "response pairs and then you do um supervised learning okay and the idea",
    "start": "3120800",
    "end": "3128079"
  },
  {
    "text": "here is that the base model already has the sort of the raw potential so just",
    "start": "3128079",
    "end": "3133119"
  },
  {
    "text": "fine-tuning it on um a few examples is uh sufficient of course the more",
    "start": "3133119",
    "end": "3138720"
  },
  {
    "text": "examples you have the better the the results but um there's papers like this one that shows even like a thousand uh",
    "start": "3138720",
    "end": "3145359"
  },
  {
    "text": "examples suffices to give you instruction following capabilities from a base good base model okay so this part",
    "start": "3145359",
    "end": "3152480"
  },
  {
    "text": "is actually very you know simple and it's not that different from um you know pre-training because it's just you're",
    "start": "3152480",
    "end": "3158559"
  },
  {
    "text": "given text and you just maximize the probability of the text um so the second part is a bit more",
    "start": "3158559",
    "end": "3166079"
  },
  {
    "text": "interesting from a algorithmic perspective so the idea here is that even with SFT phase you will have a",
    "start": "3166079",
    "end": "3173599"
  },
  {
    "text": "decent um model and now how do you improve it what you can get there more SFT data but that can be very expensive",
    "start": "3173599",
    "end": "3180559"
  },
  {
    "text": "because you have to you know annot someone sit down and annotate data so",
    "start": "3180559",
    "end": "3186440"
  },
  {
    "text": "there the goal of learning from feedback is that you can leverage lighter forms",
    "start": "3186440",
    "end": "3191839"
  },
  {
    "text": "of annotation um and have the algorithms do a bit more work okay so one type of",
    "start": "3191839",
    "end": "3198480"
  },
  {
    "text": "data you can learn from is preference data so this is where you generate multiple responses from a model to a",
    "start": "3198480",
    "end": "3205599"
  },
  {
    "text": "given prompt like A or B and the user rates whether A or B is better and so",
    "start": "3205599",
    "end": "3211680"
  },
  {
    "text": "the data might look like you know it generates uh you know what's the best way to train a language model use a",
    "start": "3211680",
    "end": "3217280"
  },
  {
    "text": "large data set or use a small data set and of course the answer should be a so that is a a unit of expressing",
    "start": "3217280",
    "end": "3225480"
  },
  {
    "text": "preferences another type of supervision you could have is using verifiers so for",
    "start": "3225480",
    "end": "3231040"
  },
  {
    "text": "some domains you're lucky enough to have a formal verifier like for math or code",
    "start": "3231040",
    "end": "3236240"
  },
  {
    "text": "or you can use learn verifiers where um you train an actual language model to um",
    "start": "3236240",
    "end": "3242480"
  },
  {
    "text": "to rate uh the the the the response and of course this relates",
    "start": "3242480",
    "end": "3248160"
  },
  {
    "text": "to evaluation again algorithms um this is you know we're in the realm of",
    "start": "3248160",
    "end": "3254319"
  },
  {
    "text": "reinforcement learning so uh one of the earliest algorithms uh that was",
    "start": "3254319",
    "end": "3259920"
  },
  {
    "text": "developed that was applied to instruction um tuning models uh were was",
    "start": "3259920",
    "end": "3265480"
  },
  {
    "text": "PO proximal policy optimization um it turns out that if you just have",
    "start": "3265480",
    "end": "3271359"
  },
  {
    "text": "preference data you there's a much simpler algorithm called DPO that works really well um but in general if you",
    "start": "3271359",
    "end": "3278880"
  },
  {
    "text": "want to learn uh from verifiers data you have to it's not preference data so you have to embrace RL fully and um you know",
    "start": "3278880",
    "end": "3287280"
  },
  {
    "text": "there's this um method which we'll uh do in this class which called group",
    "start": "3287280",
    "end": "3293599"
  },
  {
    "text": "relative preference optimization which simplifies po makes it more efficient by removing the value function developed by",
    "start": "3293599",
    "end": "3299520"
  },
  {
    "text": "deepseek which u seems to work pretty well okay so assignment five implements",
    "start": "3299520",
    "end": "3306640"
  },
  {
    "text": "supervised tuning DPO and GRPO and of course evaluate",
    "start": "3306640",
    "end": "3313960"
  },
  {
    "text": "question about assignment one do people have similar things to say about assignments two or Yeah the question is",
    "start": "3317559",
    "end": "3324800"
  },
  {
    "text": "um assignment one seems a bit uh daunting what about the other ones i would say that assignment one and two",
    "start": "3324800",
    "end": "3331359"
  },
  {
    "text": "are definitely the most heavy and hardest um assignment three is um a bit",
    "start": "3331359",
    "end": "3338640"
  },
  {
    "text": "more of a breather and assignment four and five at least last year were um I",
    "start": "3338640",
    "end": "3344160"
  },
  {
    "text": "would say a notch below assignment one or two um although I don't know depends on we haven't fully worked out the",
    "start": "3344160",
    "end": "3350880"
  },
  {
    "text": "details for this year yeah it does get",
    "start": "3350880",
    "end": "3357400"
  },
  {
    "text": "better okay so just to a recap of the different pieces here um you know",
    "start": "3357400",
    "end": "3365280"
  },
  {
    "text": "remember efficiency is this driving principle and there's a bunch of different design decisions and you can I",
    "start": "3365280",
    "end": "3373440"
  },
  {
    "text": "think if you view efficiency um everything through a lens of efficiency I think a lot of things kind",
    "start": "3373440",
    "end": "3379119"
  },
  {
    "text": "of make sense um and importantly I think you know we are it's worth pointing out",
    "start": "3379119",
    "end": "3386240"
  },
  {
    "text": "there we are currently um in this compute constraint regime at least this class and most people who are somewhat",
    "start": "3386240",
    "end": "3392960"
  },
  {
    "text": "GPU poor so we have a lot of data but we don't have that much compute and so these design decisions will reflect",
    "start": "3392960",
    "end": "3399920"
  },
  {
    "text": "squeezing the most out of the hardware so for example data processing we're filtering fairly aggressively because we",
    "start": "3399920",
    "end": "3405920"
  },
  {
    "text": "don't want to waste precious compute on bad or irrelevant data tokeniz ization",
    "start": "3405920",
    "end": "3411200"
  },
  {
    "text": "like it's it's nice to have a a model over bytes that's very elegant but it's",
    "start": "3411200",
    "end": "3416240"
  },
  {
    "text": "very compute inefficient with today's model architectures so we have to do tokenization to as an efficiency gain",
    "start": "3416240",
    "end": "3423760"
  },
  {
    "text": "model architecture there are a lot of design decisions there that are essentially motivated by you know",
    "start": "3423760",
    "end": "3429880"
  },
  {
    "text": "efficiency training i think the fact that we're most of what we're doing to do is just a single epoch this is",
    "start": "3429880",
    "end": "3436799"
  },
  {
    "text": "clearly we're in a hurry um we just need to see more data as opposed to spend a",
    "start": "3436799",
    "end": "3441839"
  },
  {
    "text": "lot of time on any given data point scaling laws is completely about efficiency we use less compute to figure",
    "start": "3441839",
    "end": "3448319"
  },
  {
    "text": "out the hyperparameters um and alignment is is is maybe a little bit different",
    "start": "3448319",
    "end": "3455920"
  },
  {
    "text": "but um the connection to efficiency is that if you can put resources into",
    "start": "3455920",
    "end": "3462359"
  },
  {
    "text": "alignment then you actually require less uh you know smaller base models okay so",
    "start": "3462359",
    "end": "3470160"
  },
  {
    "text": "there is a you know there's sort of two paths if your use case is fairly narrow",
    "start": "3470160",
    "end": "3475520"
  },
  {
    "text": "you can probably use a smaller model you align it or fine-tune it and you can do well but if you your use cases are very",
    "start": "3475520",
    "end": "3482799"
  },
  {
    "text": "broad then there might not be a substitute for training a a big model so",
    "start": "3482799",
    "end": "3488079"
  },
  {
    "text": "that's today so increasingly now um at least for Frontier Labs u they're",
    "start": "3488079",
    "end": "3494559"
  },
  {
    "text": "becoming data constrained which is interesting because I think that the design decisions will presumably",
    "start": "3494559",
    "end": "3501839"
  },
  {
    "text": "completely change well I mean compute will always be important but I think the design decisions will change for example",
    "start": "3501839",
    "end": "3508640"
  },
  {
    "text": "you know learning taking one epoch of your data I think doesn't really make",
    "start": "3508640",
    "end": "3514559"
  },
  {
    "text": "sense if you have more compute why wouldn't you take more epochs at least or do something uh smarter",
    "start": "3514559",
    "end": "3519839"
  },
  {
    "text": "or maybe there will be um different architectures for example um because a",
    "start": "3519839",
    "end": "3524960"
  },
  {
    "text": "transformer was really motivated by you know compute efficiency um so that's",
    "start": "3524960",
    "end": "3530799"
  },
  {
    "text": "something to kind of ponder still it's about efficiency but the design decisions reflect what regime you're",
    "start": "3530799",
    "end": "3538559"
  },
  {
    "text": "in okay so now I'm going to dive into the",
    "start": "3539160",
    "end": "3544960"
  },
  {
    "text": "first uh unit um you know before Not any questions",
    "start": "3544960",
    "end": "3552440"
  },
  {
    "text": "do you have a slack or uh the question is we have a slack or we",
    "start": "3555280",
    "end": "3560559"
  },
  {
    "text": "will have a slack we'll send out details um after this class",
    "start": "3560559",
    "end": "3566480"
  },
  {
    "text": "will students auditing the course also have access to the same materials uh the question is students auditing the class",
    "start": "3566480",
    "end": "3573040"
  },
  {
    "text": "will have access to all the um online uh",
    "start": "3573040",
    "end": "3578079"
  },
  {
    "text": "you know materials assignments and we'll give you access to uh Canvas so you can",
    "start": "3578079",
    "end": "3584160"
  },
  {
    "text": "watch the the uh lecture videos yeah what's the grading of the",
    "start": "3584160",
    "end": "3590640"
  },
  {
    "text": "assignments what's the grading of the assignments um good question so there",
    "start": "3590640",
    "end": "3596480"
  },
  {
    "text": "will be a set of unit tests that uh you will have to pass so part of the grading",
    "start": "3596480",
    "end": "3601680"
  },
  {
    "text": "is just did you implement this correctly um there will be also parts of the grade which will did you implement a model",
    "start": "3601680",
    "end": "3608480"
  },
  {
    "text": "that achieved a certain level of loss or is efficient enough um in the um",
    "start": "3608480",
    "end": "3614880"
  },
  {
    "text": "assignment every problem part has a number of points associated with it and",
    "start": "3614880",
    "end": "3619920"
  },
  {
    "text": "so that gives you a fairly granular level of what um grading looks like",
    "start": "3619920",
    "end": "3626799"
  },
  {
    "text": "okay let's jump into tokenization okay so um Andre Kapati has",
    "start": "3628880",
    "end": "3634640"
  },
  {
    "text": "this really nice video on tokenization and in general he makes a lot of these videos on um that uh actually inspired a",
    "start": "3634640",
    "end": "3642319"
  },
  {
    "text": "lot of this class how you can build things from from scratch um so you should go check out some of his videos",
    "start": "3642319",
    "end": "3649000"
  },
  {
    "text": "um so tokenization as we talked about it um is the process of taking raw text",
    "start": "3649000",
    "end": "3656000"
  },
  {
    "text": "which is generally represented as unic code strings and um turning it into a",
    "start": "3656000",
    "end": "3662079"
  },
  {
    "text": "set of integers essentially and where each integer is uh represents a token",
    "start": "3662079",
    "end": "3669119"
  },
  {
    "text": "okay so we need a procedure that encodes strings to tokens and decodes them back",
    "start": "3669119",
    "end": "3674400"
  },
  {
    "text": "into strings um and the vocabulary size is just the number of values that a a",
    "start": "3674400",
    "end": "3680400"
  },
  {
    "text": "token take on the number of the range of the integers okay so just to give you an",
    "start": "3680400",
    "end": "3687040"
  },
  {
    "text": "example of how tokenizers work let's uh play around with this really nice website which allows you to look at",
    "start": "3687040",
    "end": "3693680"
  },
  {
    "text": "different tokenizers and just type in something like you know hello uh you",
    "start": "3693680",
    "end": "3699599"
  },
  {
    "text": "know hello or or whatever um maybe I'll",
    "start": "3699599",
    "end": "3705960"
  },
  {
    "text": "um do this um and one thing it does is",
    "start": "3705960",
    "end": "3711119"
  },
  {
    "text": "it shows you the list of integers this is the output of tokenizer it also nicely maps out um the decomposition of",
    "start": "3711119",
    "end": "3718480"
  },
  {
    "text": "the the original string into a bunch of segments um and a few few things to kind",
    "start": "3718480",
    "end": "3723839"
  },
  {
    "text": "of note first of all the space is part of a token so unlike classical NLP where",
    "start": "3723839",
    "end": "3730559"
  },
  {
    "text": "the space just kind of disappears everything is accounted for these are meant to be kind of reversible",
    "start": "3730559",
    "end": "3736480"
  },
  {
    "text": "operations tokenization um and by convention it you know for whatever",
    "start": "3736480",
    "end": "3743359"
  },
  {
    "text": "reason the the space is usually preceding um the token um also notice",
    "start": "3743359",
    "end": "3748799"
  },
  {
    "text": "that you know hello is a completely different token than uh space hello",
    "start": "3748799",
    "end": "3754960"
  },
  {
    "text": "which um you might make you a little bit squeamish but you know seems and it can",
    "start": "3754960",
    "end": "3761200"
  },
  {
    "text": "cause problems but um that's just how it is question I was going to ask is the space theme leading instead of trailing",
    "start": "3761200",
    "end": "3768079"
  },
  {
    "text": "intentional or is it just an artifact of the BP process um so the question is is",
    "start": "3768079",
    "end": "3773200"
  },
  {
    "text": "the spacing before intentional or not um so in the BP process I will talk about",
    "start": "3773200",
    "end": "3782480"
  },
  {
    "text": "you actually pre-tokenize and then you um and then",
    "start": "3782480",
    "end": "3787760"
  },
  {
    "text": "you tokenize each part and I think the pre-tokenizer it does put the space in the front so it is built into the",
    "start": "3787760",
    "end": "3794079"
  },
  {
    "text": "algorithm you could put it at the end but I think it probably makes more sense to put in the beginning um but um",
    "start": "3794079",
    "end": "3803200"
  },
  {
    "text": "actually don't well it I guess it could go either way it's my sense um okay so",
    "start": "3803200",
    "end": "3810000"
  },
  {
    "text": "then if you look at numbers um you see that um the numbers are chopped down",
    "start": "3810000",
    "end": "3815440"
  },
  {
    "text": "into um different you know pieces um it's a little bit kind of interesting",
    "start": "3815440",
    "end": "3822079"
  },
  {
    "text": "that it's left to right so it's definitely not grouping by thousands or anything like semantic um but anyway I",
    "start": "3822079",
    "end": "3828160"
  },
  {
    "text": "encourage you to kind of play with it and get a sense of what these existing tokenizers look like um so this is a",
    "start": "3828160",
    "end": "3834960"
  },
  {
    "text": "tokenizer for GPT40 for example um so there's some observations",
    "start": "3834960",
    "end": "3841680"
  },
  {
    "text": "um that we made um so if you look at the GB22 tokenizer which will use this kind",
    "start": "3841680",
    "end": "3848880"
  },
  {
    "text": "of as a reference um okay let me see if I can",
    "start": "3848880",
    "end": "3854760"
  },
  {
    "text": "um okay hopefully this is let me know if this is too getting too small in the back um you could take a string um if",
    "start": "3854760",
    "end": "3862960"
  },
  {
    "text": "you apply the GPD2 tokenizer you get your indices so it maps uh strings to",
    "start": "3862960",
    "end": "3868960"
  },
  {
    "text": "indices and then you can decode to uh get back the string and this is just a",
    "start": "3868960",
    "end": "3874799"
  },
  {
    "text": "sanity check to make sure that um you actually it round trips um another thing",
    "start": "3874799",
    "end": "3881119"
  },
  {
    "text": "that's I guess interesting to look at is this compression ratio which is if you look at the number of bytes divided by",
    "start": "3881119",
    "end": "3888559"
  },
  {
    "text": "the number of tokens so how many bytes are represented by a token and the",
    "start": "3888559",
    "end": "3894000"
  },
  {
    "text": "answer here is 1.6 okay so every token represents 1.6 bytes",
    "start": "3894000",
    "end": "3900000"
  },
  {
    "text": "of data okay so that's just a GPT tok to tokenizer that open air trained um to",
    "start": "3900000",
    "end": "3907200"
  },
  {
    "text": "motivate kind of BPE I want to go through a sequence of attempts like",
    "start": "3907200",
    "end": "3912319"
  },
  {
    "text": "suppose you wanted to do tokenization what would be the sort of the the simplest thing the simplest thing is",
    "start": "3912319",
    "end": "3918319"
  },
  {
    "text": "probably character-based tokenization a unic code string is a sequence of unic code characters and each character can",
    "start": "3918319",
    "end": "3925760"
  },
  {
    "text": "be converted into an integer in called a code point okay so a maps to 97 um the",
    "start": "3925760",
    "end": "3932799"
  },
  {
    "text": "world emoji maps to 127,757 and you can see that it converts",
    "start": "3932799",
    "end": "3938960"
  },
  {
    "text": "back okay so you can define a tokenizer which simply um you know maps",
    "start": "3938960",
    "end": "3947039"
  },
  {
    "text": "uh each character into a code point",
    "start": "3947039",
    "end": "3952200"
  },
  {
    "text": "okay so what's one problem with this",
    "start": "3952200",
    "end": "3958440"
  },
  {
    "text": "yeahression ratio is one the compression ratio is one um so that's uh well",
    "start": "3958480",
    "end": "3964960"
  },
  {
    "text": "actually the compression ratio is not quite one because a character is not a bite um but it's it's maybe not as good",
    "start": "3964960",
    "end": "3971760"
  },
  {
    "text": "as you want one problem with that if you look at some code points they're actually really large right um so you're",
    "start": "3971760",
    "end": "3980359"
  },
  {
    "text": "basically allocating each like one slot in your vocabulary for every character",
    "start": "3980359",
    "end": "3986920"
  },
  {
    "text": "uniformly and some characters appear way more frequently than others so this is",
    "start": "3986920",
    "end": "3992400"
  },
  {
    "text": "um not a very effective use of your kind of budget okay um so the vocabulary size is",
    "start": "3992400",
    "end": "4000400"
  },
  {
    "text": "you huge i mean the vocabulary size being 127 is actually a big deal but um",
    "start": "4000400",
    "end": "4006079"
  },
  {
    "text": "the bigger problem is that some characters are rare and this is inefficient use of the vocab",
    "start": "4006079",
    "end": "4013760"
  },
  {
    "text": "um okay so the comparation ratio is um is 1.5 in this case because it's the",
    "start": "4013760",
    "end": "4020960"
  },
  {
    "text": "tokens uh sorry the number of bytes per token and um a character can be multiple",
    "start": "4020960",
    "end": "4028599"
  },
  {
    "text": "bytes okay so that was a very kind of naive approach um on the other hand you",
    "start": "4028599",
    "end": "4034880"
  },
  {
    "text": "can do bite based tokenization okay so unic code strings can be represented as",
    "start": "4034880",
    "end": "4040160"
  },
  {
    "text": "sequence of bytes um because um every string can just be you",
    "start": "4040160",
    "end": "4048640"
  },
  {
    "text": "know converted into bytes okay so some um you know a is already just kind of",
    "start": "4048640",
    "end": "4054400"
  },
  {
    "text": "one bite but some uh characters uh take up as many as four bytes and this is",
    "start": "4054400",
    "end": "4060720"
  },
  {
    "text": "using the UTF8 kind of encoding of unic code there's other encodings but this is the most common one that's dynamic",
    "start": "4060720",
    "end": "4069280"
  },
  {
    "text": "so let's just convert everything into bytes um and see what",
    "start": "4069280",
    "end": "4075319"
  },
  {
    "text": "happens so if you do it into bytes now all the indices are between 0 and 256",
    "start": "4075319",
    "end": "4081359"
  },
  {
    "text": "because there only 256 possible values for a bite by definition um so your",
    "start": "4081359",
    "end": "4086960"
  },
  {
    "text": "vocabulary is very you know small and each bite is I guess not all bytes are",
    "start": "4086960",
    "end": "4092400"
  },
  {
    "text": "equally used but you know it's not too you don't have that many sparsity you know problems um but what's the problem",
    "start": "4092400",
    "end": "4099199"
  },
  {
    "text": "with bite- based encoding long sequences yeah long sequences so",
    "start": "4099199",
    "end": "4107600"
  },
  {
    "text": "this is I mean in some ways I really wish by coding would work it's the most elegant thing but um but you have long",
    "start": "4107600",
    "end": "4115679"
  },
  {
    "text": "sequences your compression ratio is one one bite per token and this is just terrible a",
    "start": "4115679",
    "end": "4122640"
  },
  {
    "text": "compression ratio of one is terrible because your sequences will be really long attention is quadratic naively in",
    "start": "4122640",
    "end": "4129278"
  },
  {
    "text": "the sequence lane so this is you're just gonna have a bad time in terms of",
    "start": "4129279",
    "end": "4134440"
  },
  {
    "text": "efficiency okay so that wasn't really good um so now the thing that you might",
    "start": "4134440",
    "end": "4142960"
  },
  {
    "text": "think about is well maybe we kind of have to be adaptive here right like you",
    "start": "4142960",
    "end": "4148880"
  },
  {
    "text": "know we can't allocate a character or a bite per token but maybe some tokens can represent lots of bytes and some tokens",
    "start": "4148880",
    "end": "4154960"
  },
  {
    "text": "can represent few bytes so one way to do this is wordbased tokenization and this is something that was actually very",
    "start": "4154960",
    "end": "4161679"
  },
  {
    "text": "classic in in NLP right so here's a string and you can just uh you know",
    "start": "4161679",
    "end": "4168000"
  },
  {
    "text": "split it into let's a sequence of segments",
    "start": "4168000",
    "end": "4173278"
  },
  {
    "text": "okay and you can call each of these tokens so you just use a regular expression um here's a different regular",
    "start": "4173279",
    "end": "4179838"
  },
  {
    "text": "expression um that GPT2 uses to pre-tokenize um and it just splits um um",
    "start": "4179839",
    "end": "4187520"
  },
  {
    "text": "you know your string into a sequence of strings so um and then what you do with",
    "start": "4187520",
    "end": "4196080"
  },
  {
    "text": "each segment is that you assign each of these to an integer and then you're done okay so what's the problem with",
    "start": "4196080",
    "end": "4204000"
  },
  {
    "text": "this",
    "start": "4204000",
    "end": "4207000"
  },
  {
    "text": "yeah so the problem is that your vocabulary size is sort of unbounded well not maybe not quite unbounded but",
    "start": "4209360",
    "end": "4216239"
  },
  {
    "text": "um you don't know how big it is right because on a given new input you might",
    "start": "4216239",
    "end": "4221520"
  },
  {
    "text": "get a segment that's uh that just you've never seen before and that's actually",
    "start": "4221520",
    "end": "4226880"
  },
  {
    "text": "kind of a big problem this is actually wordbased is a really big pain in the butt because um you know some real words",
    "start": "4226880",
    "end": "4234159"
  },
  {
    "text": "are rare and um you know you actually it's it's really annoying because new",
    "start": "4234159",
    "end": "4240239"
  },
  {
    "text": "words have to receive this UNC token um and if you're not careful about how you",
    "start": "4240239",
    "end": "4246000"
  },
  {
    "text": "compute you know the perplexity um then you're just going to mess up um so you",
    "start": "4246000",
    "end": "4253440"
  },
  {
    "text": "know wordbased isn't I think it captures the right intuition of adapt activity um",
    "start": "4253440",
    "end": "4259040"
  },
  {
    "text": "but it's not exactly what we want here so here we're finally going to talk",
    "start": "4259040",
    "end": "4265040"
  },
  {
    "text": "about the BPE encoding or by pair encoding um so this was actually a very",
    "start": "4265040",
    "end": "4270320"
  },
  {
    "text": "old algorithm uh developed by Philip Gage in 94 for data compression um and",
    "start": "4270320",
    "end": "4277280"
  },
  {
    "text": "it was first introduced into NLP for neural machine translation so before",
    "start": "4277280",
    "end": "4284239"
  },
  {
    "text": "papers that did machine translation or any basically all NLP used wordbased",
    "start": "4284239",
    "end": "4289800"
  },
  {
    "text": "tokenization and again wordbased was a pain so um you know this paper pioneered",
    "start": "4289800",
    "end": "4296960"
  },
  {
    "text": "this idea well we can use this nice algorithm form 94 and we can just make",
    "start": "4296960",
    "end": "4303199"
  },
  {
    "text": "um the tokenization kind of roundtrip and we don't have to deal with anks or any of that stuff and then finally this",
    "start": "4303199",
    "end": "4310159"
  },
  {
    "text": "entered the kind of language modeling um era of through GPD2 which was uh trained",
    "start": "4310159",
    "end": "4318000"
  },
  {
    "text": "on using the BP tokenizer um okay so the basic idea is",
    "start": "4318000",
    "end": "4323760"
  },
  {
    "text": "instead of defining some sort of pre preconceived notion of how to split",
    "start": "4323760",
    "end": "4328960"
  },
  {
    "text": "up we're going to train the tokenizer on raw text that's a basic kind of insight if you will and so organically common",
    "start": "4328960",
    "end": "4337520"
  },
  {
    "text": "sequences um that um span multiple characters we're going to try to represent as one token and rare",
    "start": "4337520",
    "end": "4344080"
  },
  {
    "text": "sequences are going to be represented by multiple tokens um there's a sort of a slight detail",
    "start": "4344080",
    "end": "4351520"
  },
  {
    "text": "which is to for efficiency the GPD2 paper um uses warbased tokenizer as a sort of pre-processing to break it up",
    "start": "4351520",
    "end": "4357840"
  },
  {
    "text": "into segments and then runs BP on each of the segments which is what you're going to do in this class as well um the",
    "start": "4357840",
    "end": "4364239"
  },
  {
    "text": "algorithm BP is actually very simple so we first convert the string into a sequence of bytes which we already did",
    "start": "4364239",
    "end": "4370880"
  },
  {
    "text": "when we talked about bybased tokenization and now we're going to successfully merge the most common pair",
    "start": "4370880",
    "end": "4376800"
  },
  {
    "text": "of adjacent tokens over and over again so the intuition is that if a pair of tokens that shows up a lot then we're",
    "start": "4376800",
    "end": "4383199"
  },
  {
    "text": "going to compress it into one token we're going to dedicate space for that okay so let's walk through what this",
    "start": "4383199",
    "end": "4389760"
  },
  {
    "text": "algorithm looks like so we're going to use this cat and hat as an example and",
    "start": "4389760",
    "end": "4396040"
  },
  {
    "text": "um uh we're going to convert this into a sequence of um integers these are the",
    "start": "4396040",
    "end": "4401920"
  },
  {
    "text": "bytes um and then we're going to keep track of what we've merged so remember",
    "start": "4401920",
    "end": "4408159"
  },
  {
    "text": "merges is a map from two integers which can represent bytes or other you know ex",
    "start": "4408159",
    "end": "4415199"
  },
  {
    "text": "preexisting tokens and we're going to create a new token and um the vocab is just going to",
    "start": "4415199",
    "end": "4422159"
  },
  {
    "text": "kind of be a handy way to represent the index to to bytes um okay so we're going",
    "start": "4422159",
    "end": "4428640"
  },
  {
    "text": "to the BP algorithm i mean it's very simple so I'm just actually going to run through the code you're going to um do",
    "start": "4428640",
    "end": "4434480"
  },
  {
    "text": "this number of times so number is three in this case we're going to first count",
    "start": "4434480",
    "end": "4439840"
  },
  {
    "text": "up uh the number of occurrences of pairs of bytes so um hopefully this doesn't",
    "start": "4439840",
    "end": "4445600"
  },
  {
    "text": "become too small so we're going to just step through um this uh sequence and",
    "start": "4445600",
    "end": "4451600"
  },
  {
    "text": "we're going to see that okay so what's 116 104 we're going to increment that",
    "start": "4451600",
    "end": "4456800"
  },
  {
    "text": "count 104 101 increment that count we're go through the sequence and we're going to count up um you know the bytes okay",
    "start": "4456800",
    "end": "4466320"
  },
  {
    "text": "so now after we have these counts we're going to um find the pair that occurs",
    "start": "4466320",
    "end": "4474480"
  },
  {
    "text": "the most number of times um so I guess there's multiple ones but we're just going to break ties and say 116 and",
    "start": "4474480",
    "end": "4481800"
  },
  {
    "text": "104 okay so that occurred twice um so now we're going to merge that pair so",
    "start": "4481800",
    "end": "4487920"
  },
  {
    "text": "we're going to create a new slot in our vocab which is going to be",
    "start": "4487920",
    "end": "4494280"
  },
  {
    "text": "256 so so far it's 0 through 255 but now we're expanding the vocab to 256 and",
    "start": "4494280",
    "end": "4500800"
  },
  {
    "text": "we're going to say every time we see 116 and 104 we're going to replace it with",
    "start": "4500800",
    "end": "4507120"
  },
  {
    "text": "256 okay and then we're going to um just apply",
    "start": "4508120",
    "end": "4515280"
  },
  {
    "text": "that merge to our our training set so after we do that the the um 116 104",
    "start": "4515280",
    "end": "4523440"
  },
  {
    "text": "became 256 and this 256 remember occurred twice okay so now we're just",
    "start": "4523440",
    "end": "4530239"
  },
  {
    "text": "going to loop through this algorithm you know one more time the second time um it",
    "start": "4530239",
    "end": "4536159"
  },
  {
    "text": "decided to merge 256 and 101 um and now",
    "start": "4536159",
    "end": "4541440"
  },
  {
    "text": "I'm going to replace uh that in indices um and notice that the indices is going to shrink right because our compression",
    "start": "4541440",
    "end": "4548640"
  },
  {
    "text": "ratio is getting better as we make room for more vocabulary items and we have a",
    "start": "4548640",
    "end": "4554800"
  },
  {
    "text": "greater vocabulary to represent everything okay so let me do this one more time um and then the next merge is",
    "start": "4554800",
    "end": "4563480"
  },
  {
    "text": "2573 and this is shrinking one more time okay and then now we're",
    "start": "4563480",
    "end": "4570199"
  },
  {
    "text": "done okay so let's try out this tokenizer so we have the string the",
    "start": "4570199",
    "end": "4576000"
  },
  {
    "text": "quick brown fox um we're going to encode",
    "start": "4576000",
    "end": "4581040"
  },
  {
    "text": "into a sequence of indices and then we're going to use our BP tokenizer to decode let's actually",
    "start": "4581040",
    "end": "4588560"
  },
  {
    "text": "step through what that uh you know looks like um",
    "start": "4588560",
    "end": "4594719"
  },
  {
    "text": "uh this well actually maybe decoding isn't actually interesting sorry I should have gone through the encode um",
    "start": "4594719",
    "end": "4601040"
  },
  {
    "text": "let's go back to encode um so encode um you take a string you",
    "start": "4601040",
    "end": "4607360"
  },
  {
    "text": "convert to indices and you just replay the merges in and importantly in the",
    "start": "4607360",
    "end": "4612800"
  },
  {
    "text": "order that they occur so I'm going to replay um these merges and and then",
    "start": "4612800",
    "end": "4621159"
  },
  {
    "text": "um and then I'm going to get my indices okay and then verify that this uh works",
    "start": "4621159",
    "end": "4629440"
  },
  {
    "text": "okay so that was um it's pretty simple the you know it's because it's simple",
    "start": "4629440",
    "end": "4635840"
  },
  {
    "text": "it's it was also very inefficient for example encode loops over the merges you should only loops over the merges that",
    "start": "4635840",
    "end": "4641679"
  },
  {
    "text": "matter um and there's some other bells and whistles like there's special tokens",
    "start": "4641679",
    "end": "4647960"
  },
  {
    "text": "pre-tokenization and so in your assignment you're going to essentially take this as a starting point and or I",
    "start": "4647960",
    "end": "4655199"
  },
  {
    "text": "mean I guess you should implement your own from scratch um but your goal is to make the implementation you know fast um",
    "start": "4655199",
    "end": "4662080"
  },
  {
    "text": "and you can like paralyze it if you want um you can go have fun okay so summary of tokenization so",
    "start": "4662080",
    "end": "4669840"
  },
  {
    "text": "tokenizer maps between strings and sequences of integers um we looked at",
    "start": "4669840",
    "end": "4676520"
  },
  {
    "text": "characterbased bite-based wordbased they're highly suboptimal uh for various",
    "start": "4676520",
    "end": "4681600"
  },
  {
    "text": "reasons bpe is a very old algorithm from 94 that still proves to be effective",
    "start": "4681600",
    "end": "4688000"
  },
  {
    "text": "horistic and the important thing is that looks at your corpus statistics to make sensible decisions about how to best",
    "start": "4688000",
    "end": "4694960"
  },
  {
    "text": "adaptively allocate um vocabulary to represent sequences of characters um and you know I hope that",
    "start": "4694960",
    "end": "4704159"
  },
  {
    "text": "one day I won't have to give this lecture because we'll just have architectures that um map fromtes but",
    "start": "4704159",
    "end": "4710400"
  },
  {
    "text": "until then um we'll have to deal with tokenization okay so that's it for today",
    "start": "4710400",
    "end": "4716400"
  },
  {
    "text": "next time we're going to dive into the details of PyTorch um and give you the building blocks and pay attention to",
    "start": "4716400",
    "end": "4723440"
  },
  {
    "text": "resource accounting all of you have presumably implemented you know PyTorch programs but we're going to really look",
    "start": "4723440",
    "end": "4729520"
  },
  {
    "text": "at where all the flops are going okay see you next time",
    "start": "4729520",
    "end": "4735560"
  }
]