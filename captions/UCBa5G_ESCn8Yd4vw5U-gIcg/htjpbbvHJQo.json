[
  {
    "text": "OK so I think this\nwas a good discussion and hopefully this got you\nall thinking a little bit",
    "start": "5560",
    "end": "11050"
  },
  {
    "text": "about some of the pieces that\nwe are going to talk about next. So which is basically\nevaluating model interpretations",
    "start": "11050",
    "end": "18550"
  },
  {
    "text": "and explanations\nright OK so so let's",
    "start": "18550",
    "end": "23859"
  },
  {
    "text": "kind of think about this\nfirst at a very high level right so whenever we have\nsome model interpretations",
    "start": "23860",
    "end": "30369"
  },
  {
    "text": "or explanations so we want\nto evaluate two pieces. The first one is\nthe meaningfulness",
    "start": "30370",
    "end": "36520"
  },
  {
    "text": "or the correctness of\nexplanations or interpretations right and the second one\nis the interpretability",
    "start": "36520",
    "end": "44379"
  },
  {
    "text": "of these interpretations\nor explanations right so both are important.",
    "start": "44380",
    "end": "50060"
  },
  {
    "text": "So as I think several\nof you brought up if I give you an explanation\nthat's actually not",
    "start": "50060",
    "end": "56920"
  },
  {
    "text": "being used by the model\nin a very specific way or it's an incorrect description\nof what the model is actually",
    "start": "56920",
    "end": "64360"
  },
  {
    "text": "doing that's very bad and\nharmful actually because now we are telling people that this\nis what the model is doing.",
    "start": "64360",
    "end": "71180"
  },
  {
    "text": "And that's not what it is doing\nthat's probably worser than not giving them anything right and\nthe second thing is you can",
    "start": "71180",
    "end": "77590"
  },
  {
    "text": "give people something but they\nneed to be able to look at it and make sense of what that is\nright if it is too complicated",
    "start": "77590",
    "end": "84100"
  },
  {
    "text": "if I give you 200 million\nparameters that's not going to be useful even if it's a very\naccurate representation of what",
    "start": "84100",
    "end": "90820"
  },
  {
    "text": "the model is doing OK so\nlet's talk a little bit about evaluating interpretability so\nthese are two pieces evaluating",
    "start": "90820",
    "end": "99130"
  },
  {
    "text": "the correctness or\nmeaningfulness and then evaluating interpretability\nright so when it comes",
    "start": "99130",
    "end": "104770"
  },
  {
    "text": "to evaluating interpretability\nso here are three classes",
    "start": "104770",
    "end": "110020"
  },
  {
    "text": "of evaluation that was\nsuggested in one of the prior survey papers. So the first is\nfunctionally grounded",
    "start": "110020",
    "end": "117070"
  },
  {
    "text": "evaluation where you use some\nkinds of quantitative proxy metrics there are no real\nhumans doing anything with it",
    "start": "117070",
    "end": "125200"
  },
  {
    "text": "but you use some proxy\nmetrics to determine the interpretability of a given\ninterpretation or explanation",
    "start": "125200",
    "end": "132130"
  },
  {
    "text": "right the second is human ground\nevaluation where there are people but they're\ndoing simpler tasks",
    "start": "132130",
    "end": "139270"
  },
  {
    "text": "and not the exact tasks that\nare done in an application. And then there is application\ngrounded evaluation,",
    "start": "139270",
    "end": "145850"
  },
  {
    "text": "which basically is what\nyou guys were pointing out, which is just sort\nof do as close",
    "start": "145850",
    "end": "151360"
  },
  {
    "text": "to possible to\ndownstream tasks right so let users do either the exact\ndownstream task or something",
    "start": "151360",
    "end": "158020"
  },
  {
    "text": "very close to it and then see\nhow much they're able to do it. So all of these can help with\nevaluating the interpretability",
    "start": "158020",
    "end": "166540"
  },
  {
    "text": "or probably more broadly\nutility of these interpretations and explanations right so just\nto give some examples of each",
    "start": "166540",
    "end": "174970"
  },
  {
    "text": "of these functionally grounded\nevaluation the proxy metrics could be something like\nnumber of routes or number",
    "start": "174970",
    "end": "182290"
  },
  {
    "text": "of prototypes right\njust lower is better. So there is we are not\ndoing any user studies we are not doing any kind\nof human subject evaluations",
    "start": "182290",
    "end": "190269"
  },
  {
    "text": "here we are just saying\nless things are better right so that's just a rule\nof thumb by the way",
    "start": "190270",
    "end": "196269"
  },
  {
    "text": "and human grounding\nmeans you can think of like two to three different\nkinds of tasks here no recall",
    "start": "196270",
    "end": "202569"
  },
  {
    "text": "that this is\nbasically a simplified version of human\nevaluation right so",
    "start": "202570",
    "end": "207790"
  },
  {
    "text": "either we force people\nto make a choice. So I give you two\nexplanations and then I say which one is\nmore easier for you",
    "start": "207790",
    "end": "214540"
  },
  {
    "text": "to read right so we force\npeople to make a choice and through that evaluate\nthe interpretability",
    "start": "214540",
    "end": "220480"
  },
  {
    "text": "of these explanations\nor interpretations the second one is forward\nsimulation or prediction which",
    "start": "220480",
    "end": "226780"
  },
  {
    "text": "means I give you the input the\nexplanation and then the output or sorry the input\nand the explanation",
    "start": "226780",
    "end": "233470"
  },
  {
    "text": "and I ask you to predict the\noutput of the model right so you should look at the\ninput and the explanation",
    "start": "233470",
    "end": "239260"
  },
  {
    "text": "and say, Oh, I know that the model\nor at least I can infer that the model can predict\na label one for this instance",
    "start": "239260",
    "end": "245739"
  },
  {
    "text": "right and the last one is\ncounterfactual simulation in that case, I'll give you\nthe input data point and then",
    "start": "245740",
    "end": "252700"
  },
  {
    "text": "the explanation\nand people should be able to tell what features\nneed to be changed in order",
    "start": "252700",
    "end": "258700"
  },
  {
    "text": "for the prediction to change. Of course, this is probably more\nobvious with some explanations",
    "start": "258700",
    "end": "263860"
  },
  {
    "text": "than others but that's another\nsort of like a proxy task that is used to evaluate\ninterpretability right",
    "start": "263860",
    "end": "271420"
  },
  {
    "text": "so last one, of\ncourse, is either let domain experts do the\nexact application task",
    "start": "271420",
    "end": "277720"
  },
  {
    "text": "or let domain\nexperts do a simpler version of the application. But in either case, we are\ntesting with real users",
    "start": "277720",
    "end": "285100"
  },
  {
    "text": "to see what is going\non right OK all right",
    "start": "285100",
    "end": "292120"
  },
  {
    "text": "so let's talk a little bit first\nabout evaluating the inherently",
    "start": "292120",
    "end": "297760"
  },
  {
    "text": "interpretable models. So here just kind\nof instantiating it",
    "start": "297760",
    "end": "302800"
  },
  {
    "text": "for this setup, we\nwant to evaluate the accuracy of\nthe resulting model because these models are at the\nend of the day classifiers are",
    "start": "302800",
    "end": "311860"
  },
  {
    "text": "predictors right\nso when evaluating inherently\ninterpretable models you want to see what's the accuracy\nof the resulting model you also",
    "start": "311860",
    "end": "319900"
  },
  {
    "text": "want to evaluate\nthe interpretability of the resulting models\nlike for example prototypes.",
    "start": "319900",
    "end": "325370"
  },
  {
    "text": "So we get those\nas interpretations we want to see how\nunderstandable they are or even attention weights and\nthe last thing is in this case",
    "start": "325370",
    "end": "334569"
  },
  {
    "text": "and I think some of you have\nprobably answered this question partly or indirectly.",
    "start": "334570",
    "end": "339980"
  },
  {
    "text": "So in this case do we need\nto evaluate the correctness or meaningfulness of the\nresulting interpretations where",
    "start": "339980",
    "end": "348220"
  },
  {
    "text": "do you guys think so it's an\ninherently interpretable model",
    "start": "348220",
    "end": "353930"
  },
  {
    "text": "right so since it's a model for\nevaluating its accuracy since it gives you prototypes or\nweights you are saying how",
    "start": "353930",
    "end": "360080"
  },
  {
    "text": "interpretable are these\nbut do I need to evaluate the correctness of\nthose prototypes or say",
    "start": "360080",
    "end": "368690"
  },
  {
    "text": "the meaningfulness of those\nprototypes are attention weights or is it obvious\nlike since it's coming from",
    "start": "368690",
    "end": "374660"
  },
  {
    "text": "the model it must be right or\nare there some cases where it is obvious in some cases where\nit's not because there is",
    "start": "374660",
    "end": "381979"
  },
  {
    "text": "a whole gamut of inherently\ninterpretable models Yep if these are exactly to your",
    "start": "381980",
    "end": "389009"
  },
  {
    "text": "model for you make\nit in there then you need to actually\nbuy it right",
    "start": "389010",
    "end": "395360"
  },
  {
    "text": "but if you actually believe\nthat this is exactly right so if you're adding\nlayers you might want",
    "start": "395360",
    "end": "402259"
  },
  {
    "text": "to check the output of what\nthose layers are producing that's what you're seeing. So what do you think\nabout in the beginning",
    "start": "402260",
    "end": "409400"
  },
  {
    "text": "in the very beginning, we talked\nabout some rule lists and rule sets what do you\nthink about those",
    "start": "409400",
    "end": "414770"
  },
  {
    "text": "do those need some kind\nof correctness evaluation for the interpretation he's\npregnant is probably fine.",
    "start": "414770",
    "end": "424210"
  },
  {
    "text": "But like really I\nguess Yeah but what do you mean by correctness\nright so when I say correctness",
    "start": "424210",
    "end": "431190"
  },
  {
    "text": "for example, if I\ngive you a prototype output by the prototype\nlayer in the models",
    "start": "431190",
    "end": "437400"
  },
  {
    "text": "that we are considering\nin the units so should I just assume that\nOh I've added a layer.",
    "start": "437400",
    "end": "443230"
  },
  {
    "text": "The model is giving me\na prototype apparently it's basing its prediction\non this prototype",
    "start": "443230",
    "end": "449220"
  },
  {
    "text": "and should I just accept\nthat at face value or should I need to\nprobe that a bit more I",
    "start": "449220",
    "end": "457470"
  },
  {
    "text": "think you should drop that\nright you can also use right but",
    "start": "457470",
    "end": "462840"
  },
  {
    "text": "but would this answer\nchange if we were talking about, say, a rule\nlist maybe I'll be OK.",
    "start": "462840",
    "end": "476410"
  },
  {
    "text": "So when you do this you\nmean like a global or global Yeah Yeah just not even a\npost hoc explanation like just",
    "start": "476410",
    "end": "483560"
  },
  {
    "text": "the rule list is the\nclassifier I do not think it always requires its\nown permission to do this how",
    "start": "483560",
    "end": "492270"
  },
  {
    "text": "you doing Yeah it's all there\nYeah I think what I think about your model all the\nbuildings and models and all",
    "start": "492270",
    "end": "498750"
  },
  {
    "text": "that right Yeah right so\nin some sense for some of the very initial models\nwe talked about everything",
    "start": "498750",
    "end": "506760"
  },
  {
    "text": "is all there right like as you\nsuggesting so you essentially the five rules you have in\nyour hand are the model.",
    "start": "506760",
    "end": "513400"
  },
  {
    "text": "So that's what is used\nfor classification. So maybe in that setting this\nwhole notion of evaluating",
    "start": "513400",
    "end": "519030"
  },
  {
    "text": "the correctness of\nthe interpretation is not really as\nrelevant or important",
    "start": "519030",
    "end": "524880"
  },
  {
    "text": "but the moment we go to\nsomething like adding layers to the models you might\nwant to double check",
    "start": "524880",
    "end": "531029"
  },
  {
    "text": "what those layers are learning\nand if they're even learning something useful\nor meaningful right",
    "start": "531030",
    "end": "537570"
  },
  {
    "text": "and we'll talk a\nlittle bit about how some of the prior\nworks have taught about this particular\npart but hopefully",
    "start": "537570",
    "end": "544380"
  },
  {
    "text": "the rest too are\npretty obvious, which is accuracy of the entire\nmodel and interpretability",
    "start": "544380",
    "end": "549420"
  },
  {
    "text": "of these interpretations. So Yeah I think we did\nsome of this discussion already about this.",
    "start": "549420",
    "end": "555310"
  },
  {
    "text": "So essentially my plan was\nto go over some of these and then say, how do we\nthink about evaluating",
    "start": "555310",
    "end": "562350"
  },
  {
    "text": "the accuracy or the\ninterpretability of this. So maybe we can do\na couple of these",
    "start": "562350",
    "end": "568097"
  },
  {
    "text": "and then I think we have\ndiscussed the third point at length, which is correctness. So I think in this\ncase, the correctness",
    "start": "568097",
    "end": "574620"
  },
  {
    "text": "of the interpretation\nis not really something that we may question much\nbecause this is essentially the model right\nit is what it is.",
    "start": "574620",
    "end": "581500"
  },
  {
    "text": "Again, whether it's causal or not it's\nprobably not but we are not getting into that right so\naccuracy of this model I guess",
    "start": "581500",
    "end": "589140"
  },
  {
    "text": "you can use this to compute\nthe labels for your test points right and then get accuracy\nnumbers of this model",
    "start": "589140",
    "end": "595230"
  },
  {
    "text": "and accuracy is what it is right\nso I guess interpretability how",
    "start": "595230",
    "end": "600959"
  },
  {
    "text": "about evaluating\ninterpretability on this would like a number of rules and\nthe downstream applications",
    "start": "600960",
    "end": "609360"
  },
  {
    "text": "and some of those things still\nhold here like asking people to do predictions based on this\ndo all of those seem like valid",
    "start": "609360",
    "end": "618860"
  },
  {
    "text": "approaches to test the\ninterpretability Yeah that's right I think we\ncan argue something similar",
    "start": "618860",
    "end": "624890"
  },
  {
    "text": "for this also right so\nit's essentially a slightly different form of classification.",
    "start": "624890",
    "end": "630030"
  },
  {
    "text": "So that is that. So one thing I wanted to\nshow you here is basically",
    "start": "630030",
    "end": "636590"
  },
  {
    "text": "some kind of comparative\nanalysis between the rules and then the decision sets\nright so while these are almost",
    "start": "636590",
    "end": "643760"
  },
  {
    "text": "seeming like very similar models\nor at least in the constructs that they use they seem\nvery similar they can impact",
    "start": "643760",
    "end": "651740"
  },
  {
    "text": "certain tasks that we might ask\npeople to do right so one way to think about this\nis we can just compare",
    "start": "651740",
    "end": "657920"
  },
  {
    "text": "the number of rules and\npredicates in these models and say whichever is lower is\nbetter right the other thing",
    "start": "657920",
    "end": "663890"
  },
  {
    "text": "is as we were all discussing. Let's do some user studies. So one of the user\nstudies that was",
    "start": "663890",
    "end": "669440"
  },
  {
    "text": "done in what we did\nhere was basically get a bunch of\nusers and each user",
    "start": "669440",
    "end": "675500"
  },
  {
    "text": "will be randomly assigned\nto one of the two models either the rules or the\ndecision sets and each of them",
    "start": "675500",
    "end": "682100"
  },
  {
    "text": "will answer about 12 questions\nor 10 objective and then two descriptive questions.",
    "start": "682100",
    "end": "687120"
  },
  {
    "text": "So the object questions\nlook something like this. So essentially, we\nsay there is a patient",
    "start": "687120",
    "end": "693170"
  },
  {
    "text": "with the following\nmedical record so we have partial information\nabout this patient just",
    "start": "693170",
    "end": "698990"
  },
  {
    "text": "using this partial information\ncan you be absolutely sure that the model will\nsay that the patient suffers",
    "start": "698990",
    "end": "708170"
  },
  {
    "text": "from lung cancer right so\nquestions like these are the objective questions\nthat we were asking",
    "start": "708170",
    "end": "714649"
  },
  {
    "text": "so essentially give\npartial information. And see if people\ncan correctly ascribe",
    "start": "714650",
    "end": "720920"
  },
  {
    "text": "the prediction of that\nmodel to basically the point",
    "start": "720920",
    "end": "726560"
  },
  {
    "text": "that we are thinking\nabout and the next one is a descriptive question\nwhere we say so",
    "start": "726560",
    "end": "732590"
  },
  {
    "text": "write a short\nparagraph describing the characteristics\nof asthma patients based on these rules provided\nabout right so they have",
    "start": "732590",
    "end": "740120"
  },
  {
    "text": "to just describe\nin plain English what are the characteristics\nof asthma patient according to the rules given.",
    "start": "740120",
    "end": "746700"
  },
  {
    "text": "So what we observe\nis actually something interesting for these\ntwo specific tasks right so we have seen\nthat this kind of an rules",
    "start": "746700",
    "end": "756620"
  },
  {
    "text": "are actually helping\npeople give answers to those questions that we are\nasking much more accurately",
    "start": "756620",
    "end": "764270"
  },
  {
    "text": "and in a much more faster\nway both the objective as well as the\ndescriptive questions.",
    "start": "764270",
    "end": "769320"
  },
  {
    "text": "So in some sense\neven something as small as having an\nelusive structure which",
    "start": "769320",
    "end": "774770"
  },
  {
    "text": "requires you to unroll\nthe previous rules and then say Oh if\nthat does not apply.",
    "start": "774770",
    "end": "781290"
  },
  {
    "text": "And then I come here that\nlogic is all relatively already cognitively taxing to people\nin doing certain tasks right",
    "start": "781290",
    "end": "789530"
  },
  {
    "text": "like in the kinds of\nquestions that we were asking so that kind of shows\nthat even simple changes",
    "start": "789530",
    "end": "795380"
  },
  {
    "text": "to visualizations or constructs\ncan have a big impact when you think about downstream\ntasks or interpretability",
    "start": "795380",
    "end": "803570"
  },
  {
    "text": "all right so let's talk about\nthis prototype and attention layers I think which\nwe already kicked off",
    "start": "803570",
    "end": "809509"
  },
  {
    "text": "so I think several of you\nare thinking that there is no guarantee that these\nprototypes and attention",
    "start": "809510",
    "end": "815150"
  },
  {
    "text": "weights are going\nto be meaningful. So what do we do about it\nright so one of the prior works",
    "start": "815150",
    "end": "820490"
  },
  {
    "text": "by Jane and Wallace\nthey actually check if attention\nweights correlate",
    "start": "820490",
    "end": "825830"
  },
  {
    "text": "with other measures of\nfeature importance for example gradients and they\nalso check if we",
    "start": "825830",
    "end": "833660"
  },
  {
    "text": "were to give different\nattention weights would they yield in\ndifferent predictions.",
    "start": "833660",
    "end": "839339"
  },
  {
    "text": "So they do two kinds of checks\nto determine the correctness or meaningfulness of\nthese attention weights",
    "start": "839340",
    "end": "846589"
  },
  {
    "text": "and the answer turns out\nto be no in both cases. So neither are these\nattention weights",
    "start": "846590",
    "end": "853700"
  },
  {
    "text": "learn by the attention layers\nof the model correlating with the gradients of the model\nwith respect to input points",
    "start": "853700",
    "end": "861410"
  },
  {
    "text": "nor is changing the\nattention weights to different values changing\nprediction significantly.",
    "start": "861410",
    "end": "869040"
  },
  {
    "text": "So this paper basically\nraised a lot of questions about this whole notion\nof let's add some layers",
    "start": "869040",
    "end": "875329"
  },
  {
    "text": "and make models\ninherently interpretable and then everything\nwill get solved right",
    "start": "875330",
    "end": "880550"
  },
  {
    "text": "because the complexity\nof these models is such that I think the way\npeople are just combining",
    "start": "880550",
    "end": "886070"
  },
  {
    "text": "different things adding\na layer and saying this will take care of\neverything that's unfortunately",
    "start": "886070",
    "end": "891560"
  },
  {
    "text": "not turning out to be true right\nOK so let's talk a little bit",
    "start": "891560",
    "end": "897170"
  },
  {
    "text": "about evaluating post\nhoc explanations right and under this category.",
    "start": "897170",
    "end": "903690"
  },
  {
    "text": "So you could think of\nseveral aspects that can be quantitatively\nevaluated and some aspects that",
    "start": "903690",
    "end": "911960"
  },
  {
    "text": "have to be qualitatively\nevaluated are three user studies right so for it\nis, of course, evaluating",
    "start": "911960",
    "end": "918070"
  },
  {
    "text": "the faithfulness or correctness\nof post hoc explanations and we'll discuss\nthat in detail next",
    "start": "918070",
    "end": "924160"
  },
  {
    "text": "is evaluating the stability\nof post hoc explanations, which is when you make small\ninput perturbations how much do",
    "start": "924160",
    "end": "931660"
  },
  {
    "text": "the explanations change\nright third is evaluating the fairness of post hoc\nexplanations which means is",
    "start": "931660",
    "end": "938980"
  },
  {
    "text": "the accuracy of explanations\nkind of roughly similar for majority and minority groups\nright or are the explanations",
    "start": "938980",
    "end": "947680"
  },
  {
    "text": "for instances in a majority\ngroup much more accurate than the explanations for instances\nin minority group if so there",
    "start": "947680",
    "end": "954850"
  },
  {
    "text": "is some weird kind of unfairness\nor bias that is kicking in there right and lastly\nevaluating the interpretability",
    "start": "954850",
    "end": "961870"
  },
  {
    "text": "of the post hoc explanations\nso let's go piece by piece and this is actually like\nthis entire setup of all these",
    "start": "961870",
    "end": "971380"
  },
  {
    "text": "evaluations as part of one of\nour recent benchmark papers which I'll also discuss a little bit at the\nend of this note OK so",
    "start": "971380",
    "end": "981580"
  },
  {
    "text": "evaluating faithfulness of\npost hoc explanations again there are different\nways to do it",
    "start": "981580",
    "end": "987370"
  },
  {
    "text": "depending on the kind of\ninformation you have available. So for example, let's say that\nyou have some ground truth",
    "start": "987370",
    "end": "994660"
  },
  {
    "text": "information available and\nthat ground truth could come in the form of here are the\ntop features that the model is",
    "start": "994660",
    "end": "1003570"
  },
  {
    "text": "using when making these\nkinds of predictions and I know for a fact that those\nare the features the model is",
    "start": "1003570",
    "end": "1010350"
  },
  {
    "text": "using right if such a ground\ntruth exists then you could basically think of several\nsort of agreement metrics",
    "start": "1010350",
    "end": "1018870"
  },
  {
    "text": "in the top k features and\nalso generally if you are able to rank features in\nsome order of importance",
    "start": "1018870",
    "end": "1024839"
  },
  {
    "text": "you can use that ground\ntruth to sort of determine how accurate your different\nkinds of explanations",
    "start": "1024839",
    "end": "1030930"
  },
  {
    "text": "are right so for example, you can think of this\nas feature agreement,",
    "start": "1030930",
    "end": "1036069"
  },
  {
    "text": "which is the fraction of\nthe top k features output by your explanation how many\nof those actually appear",
    "start": "1036069",
    "end": "1043500"
  },
  {
    "text": "in the top k features of the\nground truth right or you can say rank agreement not only\nshould these features that are",
    "start": "1043500",
    "end": "1051330"
  },
  {
    "text": "appearing in the top tier\nof explanation appear in the ground truth but the\nrank ordering of these features",
    "start": "1051330",
    "end": "1057840"
  },
  {
    "text": "should also match the ground\ntruth then I consider it as where is the rank agreement\nright and you can make them",
    "start": "1057840",
    "end": "1063810"
  },
  {
    "text": "stricter like for example\nsign agreement could be that the sign whether it's a positive\ncontribution or a negative even",
    "start": "1063810",
    "end": "1071130"
  },
  {
    "text": "within the top five features\nor top k features that should match sign rank\nagreement basically the",
    "start": "1071130",
    "end": "1076650"
  },
  {
    "text": "sign should match rank should\nmatch the features should appear everything happens\nthen it's a perfect match",
    "start": "1076650",
    "end": "1082500"
  },
  {
    "text": "right so you could think of\nlike a bunch of these kinds of things or you could also\nthink of rank correlation",
    "start": "1082500",
    "end": "1088740"
  },
  {
    "text": "coefficients if you are\ngiven a ground truth ordering of features somehow if you are\ngiven your explanation ordering",
    "start": "1088740",
    "end": "1095490"
  },
  {
    "text": "of features, then you can compute rank\ncorrelation coefficients you could even compute\npairwise rank agreement which",
    "start": "1095490",
    "end": "1102720"
  },
  {
    "text": "is basically take two\nfeatures at a time and then see if the ordering\nof those two features",
    "start": "1102720",
    "end": "1108930"
  },
  {
    "text": "is maintained between\nthe explanation learned and a ground truth\nexplanation right so these are all the things\nthat you can do if there is",
    "start": "1108930",
    "end": "1116820"
  },
  {
    "text": "some notion of ground truth\nand some notion of ordering among the features in\nthat ground truth is there",
    "start": "1116820",
    "end": "1124320"
  },
  {
    "text": "a question. Yeah Yeah so this not\ninto the same model",
    "start": "1124320",
    "end": "1129929"
  },
  {
    "text": "that we discussed\nearlier that I'll be evaluating the correctness\nor I'll be evaluating",
    "start": "1129930",
    "end": "1136170"
  },
  {
    "text": "how good explanation\nis or are we testing how good our model was.",
    "start": "1136170",
    "end": "1141820"
  },
  {
    "text": "And then my essentially\nI could have had a bad model but a good\nexplanation on top of it",
    "start": "1141820",
    "end": "1149100"
  },
  {
    "text": "right that\nexplanation would fail these tests because the model\nitself did not right so let",
    "start": "1149100",
    "end": "1156659"
  },
  {
    "text": "me clarify the notion\nof ground truth here right so when I'm using\nthe term ground truth here",
    "start": "1156660",
    "end": "1162059"
  },
  {
    "text": "I don't mean it's a ground truth\nassociated with the data I mean it's the ground truth\nassociated with the model",
    "start": "1162060",
    "end": "1169289"
  },
  {
    "text": "right for example, if\nyour underlying model is a logistic regression,\nthe answer for what",
    "start": "1169290",
    "end": "1175620"
  },
  {
    "text": "are the top features of that\nmodel right so you take that.",
    "start": "1175620",
    "end": "1180830"
  },
  {
    "text": "And that is your ground\ntruth so just to Yeah so this ground truth should\nnot be confused with the ground",
    "start": "1180830",
    "end": "1186550"
  },
  {
    "text": "truth of the data. This is the ground\ntruth of the model if we know the ground\ntruth of the model",
    "start": "1186550",
    "end": "1192280"
  },
  {
    "text": "then we can apply these metrics,\nwe wasn't going to find out the ground to say how do I find\nout the ground truth for a more",
    "start": "1192280",
    "end": "1200950"
  },
  {
    "text": "complex model Yeah\nso this is basically the case where your\nmodels are simple",
    "start": "1200950",
    "end": "1207850"
  },
  {
    "text": "and you are able to do this. So we are going to\ncome to different. We are going to relax\nthat assumption right",
    "start": "1207850",
    "end": "1213280"
  },
  {
    "text": "so this is a very\nbasic case wherever you have ground truth\navailable of the model",
    "start": "1213280",
    "end": "1218620"
  },
  {
    "text": "we can potentially\nuse this in fact, in one of our papers I\ndon't think we have too much",
    "start": "1218620",
    "end": "1225190"
  },
  {
    "text": "discussion in this about it but\nwe can create synthetic data sets which sort of create these\nislands of data points that are",
    "start": "1225190",
    "end": "1234940"
  },
  {
    "text": "farther away and fit a linear\nmodel on each island so we know",
    "start": "1234940",
    "end": "1241059"
  },
  {
    "text": "for a fact that there is one\nlinear model operating on this island of data points and that\nwill give you the ground truth",
    "start": "1241060",
    "end": "1248920"
  },
  {
    "text": "feature importance is for that\nisland and we have some terms and conditions that need to\nbe satisfied and any accurate",
    "start": "1248920",
    "end": "1256570"
  },
  {
    "text": "model that you train on these\nsynthetic data sets we'll have to respect these feature\nimportance is across these",
    "start": "1256570",
    "end": "1262840"
  },
  {
    "text": "islands right but this\nis only possible because we are able to\ngenerate synthetic data sets all right so now we are\ngoing to the next condition",
    "start": "1262840",
    "end": "1274059"
  },
  {
    "text": "where we don't have ground truth\nbut our explanations themselves are models right so for example\nlime is basically fitting",
    "start": "1274060",
    "end": "1282850"
  },
  {
    "text": "a linear model on some\nlocal neighborhood. So if the explanation\nitself is a model",
    "start": "1282850",
    "end": "1288730"
  },
  {
    "text": "we can basically take\nthe explanation model and then get predictions\nof that model on instances",
    "start": "1288730",
    "end": "1295420"
  },
  {
    "text": "in the local\nneighborhood and we can compare how these predictions\nmatch with the predictions",
    "start": "1295420",
    "end": "1301120"
  },
  {
    "text": "of the underlying model. So you have one model\nwhich is your explanation model you have one model, which\nis the underlying model compare",
    "start": "1301120",
    "end": "1308440"
  },
  {
    "text": "what fraction of the\npredictions of these two models match in a given\nlocal neighborhood",
    "start": "1308440",
    "end": "1314110"
  },
  {
    "text": "and the higher the\nmatch basically the more accurate the explanation\nthat's one notion of thinking",
    "start": "1314110",
    "end": "1320230"
  },
  {
    "text": "about faithfulness when\nyour explanations themselves are models. So now the question is, what if\nwe don't have any ground truth",
    "start": "1320230",
    "end": "1328419"
  },
  {
    "text": "right and what if explanations\nthat we are looking at cannot be considered as models\nthat output predictions.",
    "start": "1328420",
    "end": "1335210"
  },
  {
    "text": "So for that we have,\nagain, some tricks. So one of the basic idea here\nis remove important features",
    "start": "1335210",
    "end": "1343690"
  },
  {
    "text": "as designated by the explanation\nand see what happens. So for example here is an\nimage and we have constructed",
    "start": "1343690",
    "end": "1352870"
  },
  {
    "text": "some explanations on this and\naccording to the explanations there are a bunch of\nimportant features",
    "start": "1352870",
    "end": "1358420"
  },
  {
    "text": "keep removing those\nfrom that image right so keep removing\nthe important features one",
    "start": "1358420",
    "end": "1364390"
  },
  {
    "text": "after the other from the image\nand see how the prediction probability drops\nand in some sense",
    "start": "1364390",
    "end": "1373390"
  },
  {
    "text": "if removing the top\nfive important features from the image as\ngiven by an explanation",
    "start": "1373390",
    "end": "1380170"
  },
  {
    "text": "causes a bigger drop in\nthe prediction probability than some other explanations\ntop five features.",
    "start": "1380170",
    "end": "1386090"
  },
  {
    "text": "The first one is better because\nthose important features are somehow capturing more\nof the predictive power",
    "start": "1386090",
    "end": "1392380"
  },
  {
    "text": "right so that's the\nidea essentially Yeah",
    "start": "1392380",
    "end": "1397540"
  },
  {
    "text": "what do you mean by like\nremoving so if you like I mean if you change the color Yeah\nit becomes a different picture",
    "start": "1397540",
    "end": "1404740"
  },
  {
    "text": "rather than like you don't\nreally remove Yeah Yeah so in some sense, this\nis where the notion of background distribution\ncomes into picture.",
    "start": "1404740",
    "end": "1412580"
  },
  {
    "text": "So often for each data\nset people basically have and this can be selected\neither through pursuing data",
    "start": "1412580",
    "end": "1420520"
  },
  {
    "text": "or like looking at data or using\nsome computational methods as well. But essentially\nyou kind of come up",
    "start": "1420520",
    "end": "1426700"
  },
  {
    "text": "with in this data set\ngiven the nature of images maybe replacing\nsomething by Black means",
    "start": "1426700",
    "end": "1433150"
  },
  {
    "text": "and removing that feature\nquote unquote right so that's one way to think\nabout it the other way",
    "start": "1433150",
    "end": "1438730"
  },
  {
    "text": "to think about it if such\na notion of background distribution is not\ncoming out naturally is part of the values\nof those pixels.",
    "start": "1438730",
    "end": "1446779"
  },
  {
    "text": "So you have some\nvalues RGB values just add a bunch of noise to them. And when you add that noise\nyour prediction should change",
    "start": "1446780",
    "end": "1454960"
  },
  {
    "text": "if those features are\nimportant right Yep do you think that's a\nway for you to delete",
    "start": "1454960",
    "end": "1461660"
  },
  {
    "text": "a delete pixel influence out of\nthis experiment it depends on.",
    "start": "1461660",
    "end": "1468110"
  },
  {
    "text": "So typically as I was saying. So there is think of\nit as for each data set",
    "start": "1468110",
    "end": "1473559"
  },
  {
    "text": "there could be some background\nimage which basically is like a no signal\nimage right so if we",
    "start": "1473560",
    "end": "1479950"
  },
  {
    "text": "are able to get to that then you\nare truly removing a feature. Otherwise you are merely\nchanging something",
    "start": "1479950",
    "end": "1486910"
  },
  {
    "text": "you have there to something else\nbut the idea is still the same. If a feature is important.",
    "start": "1486910",
    "end": "1493070"
  },
  {
    "text": "And if you mess with\nit the prediction should change drastically\nright so that's one piece of it",
    "start": "1493070",
    "end": "1503040"
  },
  {
    "text": "and then the other\npiece of it is insertion or like adding\nimportant features",
    "start": "1503040",
    "end": "1508380"
  },
  {
    "text": "and seeing what happens. So for example, if this\nis the background image. So then essentially you\nkeep adding features",
    "start": "1508380",
    "end": "1515070"
  },
  {
    "text": "and then seeing how much sort\nof the prediction probability changes right in\nsome sense I would",
    "start": "1515070",
    "end": "1521370"
  },
  {
    "text": "think of it as take there\nare two sides to it, right so take important\nfeatures perturb them.",
    "start": "1521370",
    "end": "1528639"
  },
  {
    "text": "And if you're perturbing\nimportant features really your predictions should\nchange the alternative",
    "start": "1528640",
    "end": "1534000"
  },
  {
    "text": "is take unimportant\nfeatures part of them if your prediction is changing\nthen that's bad because you're",
    "start": "1534000",
    "end": "1540330"
  },
  {
    "text": "an important features are\ncarrying signal right so those are roughly the two ways of\nthinking about how to evaluate",
    "start": "1540330",
    "end": "1547830"
  },
  {
    "text": "the faithfulness\nof explanations. But when doing this evaluation\nI think it's useful to think",
    "start": "1547830",
    "end": "1555330"
  },
  {
    "text": "of it as a comparative\none right so I don't know if it's absolutely good if I get\nsome value drop in probability",
    "start": "1555330",
    "end": "1563820"
  },
  {
    "text": "but if there are two\nexplanations where one has a higher drop in probability\nthan other when I'm removing",
    "start": "1563820",
    "end": "1569910"
  },
  {
    "text": "important features or perturbing\nthem then this explanation is better than the other right\nso it at least helps you place",
    "start": "1569910",
    "end": "1576899"
  },
  {
    "text": "explanations relatively in terms\nof their faithfulness all right",
    "start": "1576900",
    "end": "1582050"
  },
  {
    "text": "so the next aspect is stability\nso are post hoc explanations stable with respect to small\ninput perturbations right",
    "start": "1582050",
    "end": "1591289"
  },
  {
    "text": "so to this end this was one\nof the first metrics proposed and the idea what's going on here is that you are\ncomputing the maximum change",
    "start": "1591290",
    "end": "1600260"
  },
  {
    "text": "in explanation relative to\nthe change in the instances.",
    "start": "1600260",
    "end": "1605550"
  },
  {
    "text": "So if you take an instance\nz part of it a bit and do this like\nseveral times and then",
    "start": "1605550",
    "end": "1611720"
  },
  {
    "text": "you calculate the maximum change\nin the explanation divided by the change in the points or\nthe distance between the points",
    "start": "1611720",
    "end": "1619910"
  },
  {
    "text": "so then you will\nbasically get a value for what is the maximum\npossible change in explanation",
    "start": "1619910",
    "end": "1625010"
  },
  {
    "text": "in a given small epsilon\nball neighborhood. So the higher this\nvalue, the more unstable",
    "start": "1625010",
    "end": "1631490"
  },
  {
    "text": "the corresponding\nexplanation is considered OK so that's one part that's\na metric so now there are",
    "start": "1631490",
    "end": "1639830"
  },
  {
    "text": "a bunch of other questions\nthat sort of come out here right so what if\nthe underlying model itself is unstable in\nthe sense that if I make",
    "start": "1639830",
    "end": "1647840"
  },
  {
    "text": "a small perturbation\nto the point the prediction of the\nmodel itself changes then in that case, my explanation\nshould change right",
    "start": "1647840",
    "end": "1654920"
  },
  {
    "text": "like it's just reflecting\nmodel behavior. So to account for that there\nare a couple more metrics",
    "start": "1654920",
    "end": "1660080"
  },
  {
    "text": "in some of our recent work. So one is you also account in\nthe denominator for the change",
    "start": "1660080",
    "end": "1665840"
  },
  {
    "text": "in the output prediction\nprobabilities are you in fact account for the changes in the\nintermediate representations",
    "start": "1665840",
    "end": "1673670"
  },
  {
    "text": "of the model as you sort\nof perturb instances right and the change in your\nexplanation should be",
    "start": "1673670",
    "end": "1680360"
  },
  {
    "text": "proportionate to some of these\nchanges and that's when it is sort of considered stable\nbecause you're also accounting",
    "start": "1680360",
    "end": "1686660"
  },
  {
    "text": "for the models properties in\nthere all right so fairness of post hoc explanations so\nreally fairness of post hoc",
    "start": "1686660",
    "end": "1695570"
  },
  {
    "text": "explanations is about taking\nall your faithfulness metrics stability metrics and computing\nthe mean values of those",
    "start": "1695570",
    "end": "1703610"
  },
  {
    "text": "metrics for a majority group\nby averaging all those metrics or instances in the\nmajority group and",
    "start": "1703610",
    "end": "1710360"
  },
  {
    "text": "the minority group and\nseeing if the two means are different in a statistically\nsignificant way right",
    "start": "1710360",
    "end": "1717380"
  },
  {
    "text": "if there is a difference,\nthen there is potentially unfairness because for\nexample, think of a health care",
    "start": "1717380",
    "end": "1722810"
  },
  {
    "text": "situation where there are\nbunch of explanations for men and a bunch of\nexplanations for women",
    "start": "1722810",
    "end": "1728480"
  },
  {
    "text": "about how they are getting\ndiagnosed with a disease right if the women's explanations\nare looking correct",
    "start": "1728480",
    "end": "1734450"
  },
  {
    "text": "but are actually incorrect. They're not faithful the\ndoctor might just rely on them",
    "start": "1734450",
    "end": "1740210"
  },
  {
    "text": "and make decisions\naccording to those and have no idea that\nthose explanations are",
    "start": "1740210",
    "end": "1745970"
  },
  {
    "text": "in fact incorrect whereas\none group's explanations are correct. So it can cause all kinds\nof issues in practice",
    "start": "1745970",
    "end": "1752120"
  },
  {
    "text": "so this is also\nundesirable in some sense. So your accuracies across\ndifferent subgroups",
    "start": "1752120",
    "end": "1757640"
  },
  {
    "text": "that you care about with\nrespect to explanations should not be\nsignificantly different.",
    "start": "1757640",
    "end": "1763500"
  },
  {
    "text": "So why and when\ncan such unfairness occur or you don't\nhave to give all",
    "start": "1763500",
    "end": "1768559"
  },
  {
    "text": "the set of sufficient conditions\nbut any intuition about when such unfairness can occur.",
    "start": "1768560",
    "end": "1775170"
  },
  {
    "text": "So when can explanations be\nmore accurate for let's say, male subgroup and less\naccurate for female subgroup",
    "start": "1775170",
    "end": "1782960"
  },
  {
    "text": "when does that happen Yep\nthe data on the one subgroup",
    "start": "1782960",
    "end": "1793370"
  },
  {
    "text": "you mean wherever there AI think\nyou're sort of in the ballpark so lack of data\nabout one subgroup",
    "start": "1793370",
    "end": "1799399"
  },
  {
    "text": "any other thoughts OK\nso how about we think",
    "start": "1799400",
    "end": "1808730"
  },
  {
    "text": "of this scenario. So for example, let's say we\nare using line right so what is lime doing lime is trying\nto fit a local linear model",
    "start": "1808730",
    "end": "1817429"
  },
  {
    "text": "at each point, which is explaining its\nprediction right so now when is that local linear\nmodel more accurate for one",
    "start": "1817430",
    "end": "1826400"
  },
  {
    "text": "group than the other right so\nmaybe the underlying model is actually a linear model for\ninstances in one subgroup",
    "start": "1826400",
    "end": "1835310"
  },
  {
    "text": "and it's actually a\nnon-linear model for instances in the other subgroup right\nso potentially the differences",
    "start": "1835310",
    "end": "1842930"
  },
  {
    "text": "in the myeloid service and the\nability of your explanation methods to capture those\nmodel surfaces can cause",
    "start": "1842930",
    "end": "1851990"
  },
  {
    "text": "this unfairness\nright so it's really it's not a big deal in\nthere it's the fact that you",
    "start": "1851990",
    "end": "1858800"
  },
  {
    "text": "are trying to fit a linear model\nnow you're underlying model is linear in some parts\nof the feature space",
    "start": "1858800",
    "end": "1865040"
  },
  {
    "text": "and is not linear. In other parts so you're\njust getting a better fit in some parts than the other.",
    "start": "1865040",
    "end": "1870500"
  },
  {
    "text": "So that's when unfairness can\nhappen but it's important to be mindful of something like\nthis because it can affect how",
    "start": "1870500",
    "end": "1876799"
  },
  {
    "text": "correct your explanations OK all\nright so this is something that",
    "start": "1876800",
    "end": "1882750"
  },
  {
    "text": "we have just looked at earlier\njust kind of bringing that back into the foreground because\nwe're going to talk about interpretability and how\nto evaluate it for post-hoc",
    "start": "1882750",
    "end": "1891000"
  },
  {
    "text": "explanations so the first one is\nessentially forward simulation",
    "start": "1891000",
    "end": "1897540"
  },
  {
    "text": "so essentially you will show\na new data point to the user the user may also have\naccess to past predictions",
    "start": "1897540",
    "end": "1905010"
  },
  {
    "text": "and explanations but for this\nnew data point you basically show the explanation to the user\nand then you are trying to make",
    "start": "1905010",
    "end": "1912450"
  },
  {
    "text": "the user guess the prediction of the model based on this\ninstance and its explanation",
    "start": "1912450",
    "end": "1918960"
  },
  {
    "text": "and you will compare\nthe accuracy of the user or rather you compare the\nlabel assign to the instance",
    "start": "1918960",
    "end": "1925500"
  },
  {
    "text": "where the user with that of the\nlabel assigned by the model. So if those both matched\nthen the user is accurate",
    "start": "1925500",
    "end": "1931529"
  },
  {
    "text": "or they're able to\nsimulate the model. So this was something that\nwas done in one of the studies",
    "start": "1931530",
    "end": "1937260"
  },
  {
    "text": "by Microsoft Research\nso essentially they showed people instances their\ncorresponding explanations",
    "start": "1937260",
    "end": "1943770"
  },
  {
    "text": "and try to ask them\nquestions like what do you think this model will\npredict how confident are you that the model will\npredict this and so on.",
    "start": "1943770",
    "end": "1951340"
  },
  {
    "text": "So these are the kinds of\ntasks that people typically do in this setting right\nor there is even more sort",
    "start": "1951340",
    "end": "1958259"
  },
  {
    "text": "of like the next level of\ntasks in terms of complexity where you will check for the\neffectiveness of human error",
    "start": "1958260",
    "end": "1965010"
  },
  {
    "text": "collaboration. So again as some of\nyou are pointing out downstream applications\nare these explanations",
    "start": "1965010",
    "end": "1971220"
  },
  {
    "text": "useful for making decisions\nwhere algorithms are not",
    "start": "1971220",
    "end": "1976289"
  },
  {
    "text": "by themselves reliable\nso for example, there are some studies\nwhich basically do",
    "start": "1976290",
    "end": "1981930"
  },
  {
    "text": "this with tasks like\ndeception deception detection, which is identifying\nfake reviews online",
    "start": "1981930",
    "end": "1988710"
  },
  {
    "text": "so if you give explanations to\nhumans as they're identifying fake reviews online will\nit help right so that was",
    "start": "1988710",
    "end": "1997050"
  },
  {
    "text": "the kind of task that\nwas done and Yeah so the results in\nsome of these tests",
    "start": "1997050",
    "end": "2002690"
  },
  {
    "text": "are actually positive for this\nparticular task people did find or researchers did find that\nwith explanations humans",
    "start": "2002690",
    "end": "2009980"
  },
  {
    "text": "were able to identify fake\nreviews online much better than without so this is\none of our recent studies",
    "start": "2009980",
    "end": "2017780"
  },
  {
    "text": "and this has been in the\nmaking for a long time because it involves access\nto doctors and people who",
    "start": "2017780",
    "end": "2024020"
  },
  {
    "text": "are actually domain\nexperts in health care. So what we were doing\nwas we were trying to work with the setting\nof a prediction problem",
    "start": "2024020",
    "end": "2032270"
  },
  {
    "text": "where we ask a bunch of\ndoctors a question a prediction question, which is I'll give\nyou the patient's current health",
    "start": "2032270",
    "end": "2040250"
  },
  {
    "text": "record in terms of\ntheir current symptoms their previous medical history, family medical history\nall of that stuff and you",
    "start": "2040250",
    "end": "2049129"
  },
  {
    "text": "have to predict\nif this patient is likely to be diagnosed\nwith breast cancer within the next\ntwo years right so",
    "start": "2049130",
    "end": "2055790"
  },
  {
    "text": "there's not a diagnosis problem\nit's a forecasting problem right so we basically carried\nout studies with about",
    "start": "2055790",
    "end": "2063379"
  },
  {
    "text": "78 doctors who are residents\nin internal medicine in hospitals in Boston\nand each doctor so this is",
    "start": "2063380",
    "end": "2071030"
  },
  {
    "text": "an online user study\neach doctor looks at 10 patient records\nfrom historical data",
    "start": "2071030",
    "end": "2076250"
  },
  {
    "text": "and make predictions on them\nso none of this is live. We are just kind of mocking\nthe situation that they might",
    "start": "2076250",
    "end": "2083780"
  },
  {
    "text": "encounter in their practice. But we are doing this\nwith historical data.",
    "start": "2083780",
    "end": "2089460"
  },
  {
    "text": "So what we found was\nsomething interesting. So if we let just doctors make\nthis prediction on their own",
    "start": "2089460",
    "end": "2096138"
  },
  {
    "text": "with in fact no models\ntheir accuracy of predicting somebody's chances of getting\nbreast cancer accurately",
    "start": "2096139",
    "end": "2102560"
  },
  {
    "text": "is about 78% right\nif we give them the prediction of the model.",
    "start": "2102560",
    "end": "2107910"
  },
  {
    "text": "So instead of just the doctor\nwe say OK for that individual. It's a label of at risk\nwith a probability of 0.91",
    "start": "2107910",
    "end": "2115640"
  },
  {
    "text": "so if we give that much just\nthat part and the model's accuracy in this case is\nabout 89% what we found",
    "start": "2115640",
    "end": "2123740"
  },
  {
    "text": "was something very\ninteresting the combined accuracy of the doctor,\nalong with model predictions",
    "start": "2123740",
    "end": "2131000"
  },
  {
    "text": "is actually worser than\nthe model's accuracy Oh and talk a little\nbit about why but I",
    "start": "2131000",
    "end": "2137450"
  },
  {
    "text": "think that's not the central\npoint of this discussion. But then there is\na third condition",
    "start": "2137450",
    "end": "2142790"
  },
  {
    "text": "where there was a doctor\nwe showed the prediction the prediction probability\nand also the top four",
    "start": "2142790",
    "end": "2149329"
  },
  {
    "text": "important features that\nwent into the prediction where we evaluated the\nfaithfulness of these",
    "start": "2149330",
    "end": "2155720"
  },
  {
    "text": "and picked the method that\nwas giving the highest faithfulness\naccording to this pot",
    "start": "2155720",
    "end": "2160849"
  },
  {
    "text": "of important and\nunimportant features matrix so in that case, the\naccuracy shot up quite a bit.",
    "start": "2160850",
    "end": "2169480"
  },
  {
    "text": "So we are trying to do this\nstudy on a much larger scale but like essentially\nthis is with three",
    "start": "2169480",
    "end": "2174810"
  },
  {
    "text": "doctors but a task that is\nclose enough to their day to day decision making right so\nthat's one thing that I wanted",
    "start": "2174810",
    "end": "2182940"
  },
  {
    "text": "to point out and the reason\nwhy that could happen is potentially there are also\ninstances where people are",
    "start": "2182940",
    "end": "2189810"
  },
  {
    "text": "seeing that the important\nfeatures are things like those appointment\ntime the zip code some",
    "start": "2189810",
    "end": "2196410"
  },
  {
    "text": "of these spurious features\nand in those cases, doctors are able to say I\ndon't want to rely on the model",
    "start": "2196410",
    "end": "2201930"
  },
  {
    "text": "prediction make my own\ndecision right Yeah Yeah",
    "start": "2201930",
    "end": "2207660"
  },
  {
    "text": "so you can kind of have a\ncontrolled experiment where you can have your doctor\nyou have your doctor",
    "start": "2207660",
    "end": "2214050"
  },
  {
    "text": "and the neural network\noutput but not what the neural networks tell\nyou put a random number",
    "start": "2214050",
    "end": "2220380"
  },
  {
    "text": "and see what the\ndoctors would guess. Yeah so what we did\nwas we did not do that",
    "start": "2220380",
    "end": "2226440"
  },
  {
    "text": "with the models of predictions\nbut we did it with explanations so what if we could just\nrandomly pick top four features",
    "start": "2226440",
    "end": "2234630"
  },
  {
    "text": "and give it to the doctor\nwould that actually help improve their decision\nmaking like is it",
    "start": "2234630",
    "end": "2240240"
  },
  {
    "text": "just the concept of explanation\nor is it a correct explanation and turns out it is the\ncorrect explanation.",
    "start": "2240240",
    "end": "2246940"
  },
  {
    "text": "In fact, what we have\nseen is if I just pick randomly top four features\nand say here is explanation.",
    "start": "2246940",
    "end": "2253510"
  },
  {
    "text": "And if doctors look at\nit and make decisions based on that their performance\nis kind of much worser",
    "start": "2253510",
    "end": "2260819"
  },
  {
    "text": "than what they would sometimes\nI think in we are trying to replicate this across\ndifferent hospitals",
    "start": "2260820",
    "end": "2266280"
  },
  {
    "text": "but like at least\nin one hospital it's actually worser than\nwhat it would have been had",
    "start": "2266280",
    "end": "2271619"
  },
  {
    "text": "they just have access to 0\nat all so bad explanations are going to hurt very badly so\nthat's The moral of that story",
    "start": "2271620",
    "end": "2280450"
  },
  {
    "text": "Yeah you know I see\nlike the first two my gut feeling feelings like\nwhenever the doctors are unsure about something,",
    "start": "2280450",
    "end": "2286869"
  },
  {
    "text": "they tend to trust anybody so\nbasically they tend to trust the sign the complete sign right\nwell I think that should depend",
    "start": "2286870",
    "end": "2295500"
  },
  {
    "text": "on whether the model itself\nis sure about the prediction or what it is doing also right\nbut Yeah it's a good point Yep",
    "start": "2295500",
    "end": "2303790"
  },
  {
    "text": "in the last row you said that\nyou're giving an explanation about the important features but\nthey're not actually important",
    "start": "2303790",
    "end": "2310390"
  },
  {
    "text": "features not in this sense\nnone of these numbers are with the bad explanations or\nrandomly chosen explanations Oh",
    "start": "2310390",
    "end": "2318520"
  },
  {
    "text": "so the increase of 90% no it's\nnot it's only the accurate explanations Yeah OK so Yeah\nthese were just clarifications",
    "start": "2318520",
    "end": "2329050"
  },
  {
    "text": "of like how those numbers are\ncoming in OK so let's kind of",
    "start": "2329050",
    "end": "2334540"
  },
  {
    "text": "Zoom out a bit\nand several of you already mentioned\nchallenges of evaluating",
    "start": "2334540",
    "end": "2339940"
  },
  {
    "text": "these kinds of interpretations\nor explanations as challenging. So why because first of all this\nis actually an ongoing endeavor",
    "start": "2339940",
    "end": "2347500"
  },
  {
    "text": "and what are the challenges\nassociated with that. One thing that needs\nto be kept in mind is parameter settings\nor hyperparameters",
    "start": "2347500",
    "end": "2355750"
  },
  {
    "text": "influence the resulting\ninterpretations or explanations right so given\nthat we need to be",
    "start": "2355750",
    "end": "2362830"
  },
  {
    "text": "very careful about\nthose settings and what we are using\nbecause for example in line",
    "start": "2362830",
    "end": "2368530"
  },
  {
    "text": "while we do not look at\nthe objective per se. So the objective function\nhas two terms so one of it",
    "start": "2368530",
    "end": "2374200"
  },
  {
    "text": "is it's basically trying\nto fit a linear model that matches the predictions\nof the underlying model.",
    "start": "2374200",
    "end": "2380030"
  },
  {
    "text": "The second term is\na regular user term where it is trying to reduce\nthe number of features",
    "start": "2380030",
    "end": "2385480"
  },
  {
    "text": "for which the coefficient\nweights are non-zero right so essentially depending\non the trade off parameter",
    "start": "2385480",
    "end": "2393099"
  },
  {
    "text": "you set there you might\nget different answers or essentially,\neven if you change the number of\nperturbations you might get",
    "start": "2393100",
    "end": "2399370"
  },
  {
    "text": "different answers right so there\nare all these things that need to be kept in mind when thinking\nabout the explanations that",
    "start": "2399370",
    "end": "2407140"
  },
  {
    "text": "are being generated. So one approach can generate\ndifferent explanations depending on the parameter\nsettings you are producing",
    "start": "2407140",
    "end": "2414220"
  },
  {
    "text": "or providing right\nand the second thing is there is as we\nhave seen there is a whole lot of\ndiversity in terms",
    "start": "2414220",
    "end": "2421059"
  },
  {
    "text": "of what is a basic\nunit of interpretation. What is a method doing for\nexample there are a bunch",
    "start": "2421060",
    "end": "2427750"
  },
  {
    "text": "of methods all of which produce\nfeature importance like lime shape maple gradient based\nmethods all of those that are",
    "start": "2427750",
    "end": "2435279"
  },
  {
    "text": "giving you some numbers\nindicating the importance of features right but they're\nalso using seemingly diverse",
    "start": "2435280",
    "end": "2442540"
  },
  {
    "text": "approaches to do that like one\nis fitting a linear model one is trying to get to shapely\nvalues one is trying to compute",
    "start": "2442540",
    "end": "2448270"
  },
  {
    "text": "gradients that there is trying\nto smooth gradients so that are doing a lot of different kinds\nof things right so given this",
    "start": "2448270",
    "end": "2455980"
  },
  {
    "text": "diversity in approaches and the\nconstructs like how do we even think about comparisons so we\nhave to come up with diverse",
    "start": "2455980",
    "end": "2462880"
  },
  {
    "text": "metrics which is also non-trivial and user studies\ncan themselves be inconsistent",
    "start": "2462880",
    "end": "2469269"
  },
  {
    "text": "because they're often affected\nby the choice of UI phrasing visualization the\npopulation that you're",
    "start": "2469270",
    "end": "2476349"
  },
  {
    "text": "considering if you're giving\nany incentives to people to do these tasks. And so on.",
    "start": "2476350",
    "end": "2481750"
  },
  {
    "text": "So there are all\nthese things to be kept in mind, which is why\nsome of the results that we",
    "start": "2481750",
    "end": "2486910"
  },
  {
    "text": "discuss and I do talk about\nwhy we are getting mixed conclusions about the utility\nor interpretability of some",
    "start": "2486910",
    "end": "2493900"
  },
  {
    "text": "of these methods is because\nof this since it is still an ongoing active area\nof research and things",
    "start": "2493900",
    "end": "2500619"
  },
  {
    "text": "are still being figured out\nright OK so I want to just talk",
    "start": "2500620",
    "end": "2506280"
  },
  {
    "text": "a little bit about some of the\ncurrently available open source tool for quantitative\nevaluation so none of these",
    "start": "2506280",
    "end": "2513240"
  },
  {
    "text": "are for user studies. So these are just\nquantitative evaluations if you are thinking about\ninherently interpretable models",
    "start": "2513240",
    "end": "2520380"
  },
  {
    "text": "so there is this interpret\nmL of GitHub report and also online library which\ncan be useful with bunch",
    "start": "2520380",
    "end": "2528210"
  },
  {
    "text": "of inherently interpretable\nmodels including GAMS and lists and sets and all of that.",
    "start": "2528210",
    "end": "2533830"
  },
  {
    "text": "So then there are a\nbunch of Excel libraries for different things. For example Capcom\nfrom Facebook has",
    "start": "2533830",
    "end": "2540300"
  },
  {
    "text": "a lot of implementations of\ndifferent explanation post hoc explanation methods\nthen there are",
    "start": "2540300",
    "end": "2547650"
  },
  {
    "text": "some of the\nbenchmarks like eraser is a good benchmark for\nexplanations on NLP.",
    "start": "2547650",
    "end": "2553210"
  },
  {
    "text": "So we have this library called\nopen xxii which is basically like 22 metrics all the\nthings that we discussed state",
    "start": "2553210",
    "end": "2560850"
  },
  {
    "text": "of the art metrics\nwe are also having some public dashboards\nfor comparing explanation",
    "start": "2560850",
    "end": "2566640"
  },
  {
    "text": "methods on different kinds\nof data sets and so on. So it's pretty easy to use\nsome of these frameworks",
    "start": "2566640",
    "end": "2571920"
  },
  {
    "text": "actually both open\nExcel and Cantor's you can use with 11 lines of code\nyou'll be able to evaluate",
    "start": "2571920",
    "end": "2578640"
  },
  {
    "text": "different explanation\nmethods that are supported by these packages right so\nthings are getting there",
    "start": "2578640",
    "end": "2584070"
  },
  {
    "text": "but whether we have correct\nanswers conclusive insights and inferences that's another\nquestion I think that's going",
    "start": "2584070",
    "end": "2590730"
  },
  {
    "text": "to take some time OK all\nright so we have actually a bit more time.",
    "start": "2590730",
    "end": "2596120"
  },
  {
    "text": "So I'm going to jump\ninto the next part, which is trying to think\nabout the insights",
    "start": "2596120",
    "end": "2601750"
  },
  {
    "text": "that we have about the behavior\nof different inherently interpretable models\nand post hoc explanation",
    "start": "2601750",
    "end": "2607480"
  },
  {
    "text": "methods all right any questions\nso far sorry maybe I'll",
    "start": "2607480",
    "end": "2612670"
  },
  {
    "text": "pause here Yeah so let's\nsay we have an image or something if we use the\nexplanation myself very",
    "start": "2612670",
    "end": "2622270"
  },
  {
    "text": "often we get very\ndifferent results possibly great question. So your question is if we use\ndifferent explanation methods",
    "start": "2622270",
    "end": "2630520"
  },
  {
    "text": "if I get different results. What do I do right\nthat's a great question. We are going to touch upon\nthat quite a bit in this part",
    "start": "2630520",
    "end": "2638180"
  },
  {
    "text": "Yeah all right Yes but\nthat doesn't necessarily",
    "start": "2638180",
    "end": "2644800"
  },
  {
    "text": "mean that it's bad\nbut you feel like you Credit One particular\nessay line and we",
    "start": "2644800",
    "end": "2650170"
  },
  {
    "text": "should get different\nresults because we can have multiple explanations\nfor the same thing.",
    "start": "2650170",
    "end": "2657680"
  },
  {
    "text": "So you're saying different\nmethods are giving different answers that\ndoes not have to be bad",
    "start": "2657680",
    "end": "2662770"
  },
  {
    "text": "Yeah even from the same\nmethod say a few same method different say, for\ninstance, we use line",
    "start": "2662770",
    "end": "2669190"
  },
  {
    "text": "and we use different variations\nyou get different explanation Yeah that does not\nmean that line is bad",
    "start": "2669190",
    "end": "2675849"
  },
  {
    "text": "or anything because I mean we\ncan have multiple explanations right so your question is,\neven if I just use line",
    "start": "2675850",
    "end": "2683560"
  },
  {
    "text": "and if I just set the\nnumber of perturbations to different values\nif line gives different explanations why\nis that bad right so we'll",
    "start": "2683560",
    "end": "2691570"
  },
  {
    "text": "get to that in a bit. So there is something fuzzy\nabout this particular aspect",
    "start": "2691570",
    "end": "2697150"
  },
  {
    "text": "if I set the number\nof perturbations to different values if I'm\ngetting different explanations",
    "start": "2697150",
    "end": "2703060"
  },
  {
    "text": "the most likely answer is the\nexplanation method has not converged to an\nexpected explanation",
    "start": "2703060",
    "end": "2710500"
  },
  {
    "text": "so how do we think\nabout that of course if you change the\nparameters like for example,",
    "start": "2710500",
    "end": "2715670"
  },
  {
    "text": "the lambda which balances\nthe regularization term with the last term. And so on, then you can get\ndifferent explanations right",
    "start": "2715670",
    "end": "2722589"
  },
  {
    "text": "so one is having less non-zero\ncoefficient associated",
    "start": "2722590",
    "end": "2727780"
  },
  {
    "text": "with features that there\nis more accurate in terms of the model that is\nfitting and so on. But we'll get to\nthis discussion this",
    "start": "2727780",
    "end": "2733750"
  },
  {
    "text": "exactly what is\nthe next part all right so let's get into this.",
    "start": "2733750",
    "end": "2742220"
  },
  {
    "text": "So there is a lot of\nrecent focus on analyzing the behavior of post\nhoc explanation methods",
    "start": "2742220",
    "end": "2748599"
  },
  {
    "text": "and there are a bunch of\nempirical studies analyzing the faithfulness and\nstability fairness",
    "start": "2748600",
    "end": "2754450"
  },
  {
    "text": "adversarial vulnerabilities\nutility of post hoc explanation methods and a lot\nof these studies",
    "start": "2754450",
    "end": "2760599"
  },
  {
    "text": "also demonstrate the\nlimitations of existing post hoc explanation methods\nright so this",
    "start": "2760600",
    "end": "2765820"
  },
  {
    "text": "is a very active\narea of research personally this kind of\nempirical and theoretical",
    "start": "2765820",
    "end": "2771310"
  },
  {
    "text": "analysis is something\nthat I'm very keen on and constitutes a bunch\nof our current work",
    "start": "2771310",
    "end": "2776740"
  },
  {
    "text": "and we'll talk about some\nof the theoretical results and empirical results and so on. So this is actually a paper by\nJulius Adebayo at all in 2018",
    "start": "2776740",
    "end": "2787840"
  },
  {
    "text": "what they were trying\nto test is basically they were trying to see if\ngradient based explanations are",
    "start": "2787840",
    "end": "2793660"
  },
  {
    "text": "any good which is\nbasically input gradient input times gradient smooth\ngrammar integrated grid",
    "start": "2793660",
    "end": "2799210"
  },
  {
    "text": "all the things that\nwe were talking about. So they were checking if those\nexplanations are any good",
    "start": "2799210",
    "end": "2804250"
  },
  {
    "text": "and the test that they did was\nthat they started randomizing the parameters of the model of\ndeep neural networks starting",
    "start": "2804250",
    "end": "2813160"
  },
  {
    "text": "all the way from\nthe top layer that is closer to the softmax\nlayer and all the way to the bottom layer so as they\nrandomized each layer what they",
    "start": "2813160",
    "end": "2823540"
  },
  {
    "text": "realized is that the\nexplanations are not really changing with these methods,\nespecially a couple of them",
    "start": "2823540",
    "end": "2831490"
  },
  {
    "text": "I think well even gradient\ntimes input has that problem but it's more pronounced with\nsome of the other methods which",
    "start": "2831490",
    "end": "2839079"
  },
  {
    "text": "basically means no matter what\nthe parameters on your network. Are somehow these\nmethods are trying",
    "start": "2839080",
    "end": "2845110"
  },
  {
    "text": "to give you the same answer\nand that is particularly happening with some\nof these methods",
    "start": "2845110",
    "end": "2850720"
  },
  {
    "text": "because I think some\nof you were already pointing to your\ndiscussion there. So some of these methods\nare trying to guide or bias",
    "start": "2850720",
    "end": "2859240"
  },
  {
    "text": "the explanations\ngenerated to something that is likely to be\nintuitive to the users right",
    "start": "2859240",
    "end": "2865720"
  },
  {
    "text": "because a lot of times people\nare like Oh we should get an explanation that's less\nnoisy that's more meaningful all",
    "start": "2865720",
    "end": "2873550"
  },
  {
    "text": "that is amazing but it's only\namazing when your model is also doing that right but if\nyour model is not doing it",
    "start": "2873550",
    "end": "2879610"
  },
  {
    "text": "and you're sort of guiding\nyour explanations to do it then you will have beautiful\nexplanations for a model that's",
    "start": "2879610",
    "end": "2886570"
  },
  {
    "text": "bad right so that's what\nis kind of happening with some of these approaches.",
    "start": "2886570",
    "end": "2892580"
  },
  {
    "text": "So basically they're failing the\nmodel parameter randomization test and the next\nthing that they",
    "start": "2892580",
    "end": "2901310"
  },
  {
    "text": "tried is that they\nbasically randomize the class labels of instances. So they took their data\nset the randomize the class",
    "start": "2901310",
    "end": "2908270"
  },
  {
    "text": "labels of the instances\nand had a different model and even that really did\nnot impact the explanations.",
    "start": "2908270",
    "end": "2915690"
  },
  {
    "text": "So this is bad. So basically changes in\nmodel are not being reflected in the changes in explanation at\nall right for some methods more",
    "start": "2915690",
    "end": "2922490"
  },
  {
    "text": "than others right mostly those\nguided sort of methods that are being used OK so we talked a\nlittle bit about stability we",
    "start": "2922490",
    "end": "2931820"
  },
  {
    "text": "ask the question or at least we\nsaw this metric earlier and we were thinking if post hoc\nexplanations are unstable with",
    "start": "2931820",
    "end": "2938119"
  },
  {
    "text": "respect to small non adversarial\ninput perturbations turns out approaches like line can be\nunstable so these approaches",
    "start": "2938120",
    "end": "2946970"
  },
  {
    "text": "are relying on perturbation\nbased perturbations basically they create perturbations of an\ninstance and try to fit a model",
    "start": "2946970",
    "end": "2953750"
  },
  {
    "text": "on those so they are more\nlikely to be unstable than some of the other gradient based methods.",
    "start": "2953750",
    "end": "2959460"
  },
  {
    "text": "So that's one of the findings\nin one of the studies and this is true even when the\nunderlying model, whether it's",
    "start": "2959460",
    "end": "2966890"
  },
  {
    "text": "prediction or whether it is\nrepresentations those are not changing much so even when\nthey're not changing much",
    "start": "2966890",
    "end": "2973039"
  },
  {
    "text": "the explanation is changing\ndrastically with some of these methods for example.",
    "start": "2973040",
    "end": "2979680"
  },
  {
    "text": "So there is I think one of\nyou raised the question just now about your different\nexplanations of coming out",
    "start": "2979680",
    "end": "2988220"
  },
  {
    "text": "from the same method\nright so I just want to highlight this example\na bit in this one of our works",
    "start": "2988220",
    "end": "2995300"
  },
  {
    "text": "a couple of years back is that. So what we see in\nthis image maybe I'll",
    "start": "2995300",
    "end": "3000970"
  },
  {
    "text": "focus on the last two pictures\nso the underlying model is a nonlinear model and in figure\nC the number of perturbations",
    "start": "3000970",
    "end": "3010630"
  },
  {
    "text": "is high and in figure D the\nnumber of perturbations is low. So when the number of\nperturbations is high",
    "start": "3010630",
    "end": "3018970"
  },
  {
    "text": "and if I keep repeatedly\nrerunning line with the same number\nof perturbations",
    "start": "3018970",
    "end": "3025570"
  },
  {
    "text": "I get kind of similar answers\nin terms of explanation. But when the number of\nperturbations is low",
    "start": "3025570",
    "end": "3032860"
  },
  {
    "text": "and if I keep rerunning\nLyme on the same point with the same parameters I\nget varied answers in terms",
    "start": "3032860",
    "end": "3039820"
  },
  {
    "text": "of the explanations that\nLyme outputs right so indicating the potential that\nwith fewer samples or fewer",
    "start": "3039820",
    "end": "3047410"
  },
  {
    "text": "perturbations the method has\npotentially not converged so there is effect\non the perturbations",
    "start": "3047410",
    "end": "3054160"
  },
  {
    "text": "you pick on the explanation\nthat you get right whereas if you increase that\nnumber this chance reduces",
    "start": "3054160",
    "end": "3062200"
  },
  {
    "text": "and you are likely to get\nsimilar explanations so that's one possible way\nto think about it.",
    "start": "3062200",
    "end": "3067490"
  },
  {
    "text": "But if we I think the point\nthat I'm trying to make is that we need to be\nvery careful about setting",
    "start": "3067490",
    "end": "3073510"
  },
  {
    "text": "the parameters in these methods\nbecause these kinds of issues can happen OK so here what\nthis is sort of bringing up",
    "start": "3073510",
    "end": "3083770"
  },
  {
    "text": "is an important question,\nwhich is OK so maybe there's a problem with having\ntoo few perturbations",
    "start": "3083770",
    "end": "3089170"
  },
  {
    "text": "right so then the question is, what is the optimal\nnumber of perturbations",
    "start": "3089170",
    "end": "3094210"
  },
  {
    "text": "so that is some of the recent\nresearch which I'll touch upon as we are thinking about\nsome of the future directions",
    "start": "3094210",
    "end": "3101800"
  },
  {
    "text": "all right so some\nare findings right so some of the recent\nworks also identified",
    "start": "3101800",
    "end": "3107530"
  },
  {
    "text": "that post hoc explanations\ncan be easily manipulated so just to show you an example.",
    "start": "3107530",
    "end": "3114230"
  },
  {
    "text": "So here is the\noriginal image here is the explanation generated here\nis the manipulated image you",
    "start": "3114230",
    "end": "3120340"
  },
  {
    "text": "kind of changed it very slightly\nin an almost imperceptible way",
    "start": "3120340",
    "end": "3125380"
  },
  {
    "text": "for a human eye. And this is the\nexplanation right so",
    "start": "3125380",
    "end": "3130450"
  },
  {
    "text": "in some sense for\nthose of you familiar with adversarial examples\nliterature for generic machine",
    "start": "3130450",
    "end": "3135760"
  },
  {
    "text": "learning models those analogous\nto that in the explanation literature right so\nessentially this paper",
    "start": "3135760",
    "end": "3143980"
  },
  {
    "text": "and also the next paper they're\ntrying to do something very similar what\nthey're trying to do is find the smallest\nperturbation to the input",
    "start": "3143980",
    "end": "3151330"
  },
  {
    "text": "image that can help maximally\nchange the explanation and also kind of anchor it to\nsome misleading explanations",
    "start": "3151330",
    "end": "3158859"
  },
  {
    "text": "that an adversary\nwould want the approach to generate right\nso that's possible",
    "start": "3158860",
    "end": "3163990"
  },
  {
    "text": "that is doable of course, whether adversaries will have\nthat kind of access in practice",
    "start": "3163990",
    "end": "3169150"
  },
  {
    "text": "or not that's a\nseparate issue but you if you were an adversary\nwith a lot of control",
    "start": "3169150",
    "end": "3175930"
  },
  {
    "text": "and can run all these things\nyou could potentially do that. So another one of\nour recent works",
    "start": "3175930",
    "end": "3182589"
  },
  {
    "text": "was basically about trying\nto design scaffolding attacks to hide classifiers dependence\non certain bad features.",
    "start": "3182590",
    "end": "3191000"
  },
  {
    "text": "So essentially what you\nsee here in this picture is that there is a biased\nclassifier that we created,",
    "start": "3191000",
    "end": "3197930"
  },
  {
    "text": "which was essentially\ncompletely using gender to make its decisions on in\nsample data points on the data",
    "start": "3197930",
    "end": "3204880"
  },
  {
    "text": "distribution and then the attack\nI'll talk about in a bit what the attack is.",
    "start": "3204880",
    "end": "3210830"
  },
  {
    "text": "But once we do the attack that\nis the explanation generated by line and line\ncompletely thinks",
    "start": "3210830",
    "end": "3217690"
  },
  {
    "text": "that we are using some low rate\nfeature as the main feature so it's not even detecting\nthat like gender is being used",
    "start": "3217690",
    "end": "3224110"
  },
  {
    "text": "predominantly similar thing with\nsharp it's not able to detect that gender is being\nused predominantly",
    "start": "3224110",
    "end": "3230290"
  },
  {
    "text": "it's thinking some\nother innocent innocuous feature is being used. So what did we do here I'll\nexplain the attack just",
    "start": "3230290",
    "end": "3238510"
  },
  {
    "text": "in a bit. But essentially this attack\nis exploiting one intuition",
    "start": "3238510",
    "end": "3243580"
  },
  {
    "text": "about the behavior\nof Liam and sharp so while we talked about\nperturbations in line",
    "start": "3243580",
    "end": "3249610"
  },
  {
    "text": "the initial version\nof line essentially does binary\nperturbations what I mean by that is it kind of\ntransforms the input feature",
    "start": "3249610",
    "end": "3258640"
  },
  {
    "text": "space to something that is just\nbinary so every feature will just have a one or 0 value and\nwhen it does a perturbation it",
    "start": "3258640",
    "end": "3267100"
  },
  {
    "text": "basically flips once to zeros\nby choosing features randomly, which have 1 values.",
    "start": "3267100",
    "end": "3273740"
  },
  {
    "text": "So it just does the\nflipping of ones and zeros when it does\nperturbations now in doing",
    "start": "3273740",
    "end": "3279190"
  },
  {
    "text": "that what happens is if I\nwere to solve this actually the Compas data set\nwhat we did is took",
    "start": "3279190",
    "end": "3285430"
  },
  {
    "text": "the instances of\nthe Compas data set did a PCA projected them\non to these two dimensions.",
    "start": "3285430",
    "end": "3291320"
  },
  {
    "text": "First dimension of PC on\nthe x-axis second dimension on the y-axis. And you can see the\noriginal composite instances",
    "start": "3291320",
    "end": "3298960"
  },
  {
    "text": "in blue composite is a\ndata set that people often use in this community for\nthinking about criminal justice",
    "start": "3298960",
    "end": "3305289"
  },
  {
    "text": "tasks for example right it\nhas its fair share of issues but nevertheless it\nis a popular data set",
    "start": "3305290",
    "end": "3311589"
  },
  {
    "text": "so blue is the original\ninstances from the data and the red is the perturbations\ngenerated by Lyman sharp",
    "start": "3311590",
    "end": "3318910"
  },
  {
    "text": "so the insight from this is\na lot of the perturbed data points are actually\nout of distribution.",
    "start": "3318910",
    "end": "3326960"
  },
  {
    "text": "So that is happening. And I think in\nfact those resonate with the question or\ndiscussion some of you are also mentioning.",
    "start": "3326960",
    "end": "3333339"
  },
  {
    "text": "So why do I say this is\nbecause I can actually construct accurate classifiers\nthat's separate that can nicely",
    "start": "3333340",
    "end": "3340839"
  },
  {
    "text": "separate points that are\nperturbations from points that are in the data. So those are separable\nright so those",
    "start": "3340840",
    "end": "3347650"
  },
  {
    "text": "are in some sense distinctly\nsort of perceivable at least by models.",
    "start": "3347650",
    "end": "3353450"
  },
  {
    "text": "So because of that the intuition\nhere is that it was this can sort of exploit this notion\nor intuition and build",
    "start": "3353450",
    "end": "3361840"
  },
  {
    "text": "a classifier that is highly\nbiased on in sample data points",
    "start": "3361840",
    "end": "3366970"
  },
  {
    "text": "and is unbiased on out of\ndistribution samples right so if I just create a classifier\nwhich is using race or gender",
    "start": "3366970",
    "end": "3374890"
  },
  {
    "text": "through and through on the\nsample points and is using loan rate and income and other legit\nfeatures on the out of sample",
    "start": "3374890",
    "end": "3383680"
  },
  {
    "text": "perturbations then the fact\nthat these approaches rely so heavily on perturbations\nmeans they will not be able",
    "start": "3383680",
    "end": "3391210"
  },
  {
    "text": "to see what is happening in\nsample they'll just assume that the model is just doing whatever\nit is doing or out of sample",
    "start": "3391210",
    "end": "3397630"
  },
  {
    "text": "perturbations right\nso that is basically the attack or the\nintuition behind it",
    "start": "3397630",
    "end": "3403750"
  },
  {
    "text": "so essentially you can think\nof the setting as adversaries want to deploy some\nbiased classifier",
    "start": "3403750",
    "end": "3410750"
  },
  {
    "text": "in real world example. This classifier will just\nuse race to make decisions. So adversary must\nprovide Black box access",
    "start": "3410750",
    "end": "3418520"
  },
  {
    "text": "to customers, and regulators\nwho may use post hoc techniques like lime or shap\nas mandated by GDPR they need",
    "start": "3418520",
    "end": "3425810"
  },
  {
    "text": "to provide that query axis. So the goal of diversity is\nto fool post hoc explanation",
    "start": "3425810",
    "end": "3431720"
  },
  {
    "text": "techniques and\nhide the underlying biases of these models. So to that end\nessentially the diversity",
    "start": "3431720",
    "end": "3438260"
  },
  {
    "text": "can create this\nkind of a classifier where if the point is\nin the distribution",
    "start": "3438260",
    "end": "3444530"
  },
  {
    "text": "the adversarial\nclassifier will return the label of the\nbiased classifier",
    "start": "3444530",
    "end": "3450380"
  },
  {
    "text": "if the point is not\nin their distribution, which is out of distribution\nperturbation the adversarial",
    "start": "3450380",
    "end": "3456350"
  },
  {
    "text": "classifier will return the\noutput of a unbiased classifier which uses the legitimate\nfeatures right so that kind",
    "start": "3456350",
    "end": "3464480"
  },
  {
    "text": "of scaffolding attack can\nalready fool a lot of these kinds of algorithms all right so\nwe discussed this OK so another",
    "start": "3464480",
    "end": "3476930"
  },
  {
    "text": "thing that we slightly touched\nupon is there were also studies which kind of showed that\nexplanations can be highly",
    "start": "3476930",
    "end": "3483560"
  },
  {
    "text": "sensitive to random high\nhyperparameters including random seeds right so we went\nthrough some of the number",
    "start": "3483560",
    "end": "3489560"
  },
  {
    "text": "of perturbations and so on but\nrandom seeds and even that kind of patch sizes and so on those\nhyperparameters are also going",
    "start": "3489560",
    "end": "3497960"
  },
  {
    "text": "to influence the kinds of\nexplanations you generate and it's important to keep that in mind as\nwe think about this.",
    "start": "3497960",
    "end": "3504360"
  },
  {
    "text": "But by the way just\nto also not make it super scandalous\nthese kinds of things",
    "start": "3504360",
    "end": "3509480"
  },
  {
    "text": "are probably true\nwith a lot of things we do in machine learning\nthat hasn't really stopped us",
    "start": "3509480",
    "end": "3515090"
  },
  {
    "text": "from doing that anyway so\nthere is no villain here in post hoc explanations\nI think this",
    "start": "3515090",
    "end": "3521750"
  },
  {
    "text": "is just how the field is going. And moving forward whether\nit's perfect not really",
    "start": "3521750",
    "end": "3528380"
  },
  {
    "text": "but that's the status quo it's\nnot specific to explainability I just want to say that.",
    "start": "3528380",
    "end": "3534809"
  },
  {
    "text": "So this is one of\nour other studies that's actually hinting\nat what some of you",
    "start": "3534810",
    "end": "3539880"
  },
  {
    "text": "were pointing out which is\nso there is a way in which we",
    "start": "3539880",
    "end": "3545359"
  },
  {
    "text": "as machine learning\nresearchers think of what an explanation means\nand what a domain expert let's",
    "start": "3545360",
    "end": "3551420"
  },
  {
    "text": "say in health care or law\ncan think of explanations to mean right so for example,",
    "start": "3551420",
    "end": "3557610"
  },
  {
    "text": "what we did was like this\nkind of a simple online user study with a bunch of law\nschool students at Harvard",
    "start": "3557610",
    "end": "3564500"
  },
  {
    "text": "the task was sort\nof bail adjudication which is there is a classifier\nthat basically is predicting",
    "start": "3564500",
    "end": "3572420"
  },
  {
    "text": "if somebody is risky enough\nto be released on bail so whether they're risky\nor not and what we did",
    "start": "3572420",
    "end": "3578480"
  },
  {
    "text": "was so this is the classifier. So this is the true\nclassifier it relies on race.",
    "start": "3578480",
    "end": "3584010"
  },
  {
    "text": "And it's clearly discriminating\nthis is a very bad classifier and we constructed a\nhigh fidelity misleading",
    "start": "3584010",
    "end": "3592640"
  },
  {
    "text": "global explanation for this. So essentially what\nwe did was ensure that all the features\nlike race and gender",
    "start": "3592640",
    "end": "3600170"
  },
  {
    "text": "be removed when we\nconstruct that explanation. So we are so if you guys recall.",
    "start": "3600170",
    "end": "3606300"
  },
  {
    "text": "So we were constructing\nglobal explanations using rules earlier and that's\nan explanation constructed",
    "start": "3606300",
    "end": "3612620"
  },
  {
    "text": "using one of those approaches\nit will approximate this very accurately. So if you take the prediction\nmatch whatever predictions that",
    "start": "3612620",
    "end": "3621170"
  },
  {
    "text": "makes accurately match with\nthe predictions made by this but if you see it as relying\non different features right",
    "start": "3621170",
    "end": "3628040"
  },
  {
    "text": "so what we did was essentially\ntake a bunch of law school students assign some of them to\nsee this true classifier assign",
    "start": "3628040",
    "end": "3636590"
  },
  {
    "text": "some of them to see that\nexplanation and then ask them the question of if this is what\nyour classifier is doing or I",
    "start": "3636590",
    "end": "3644630"
  },
  {
    "text": "guess the more precise question\nwas if here is an explanation generated by state of the art\nmethod for the underlying model",
    "start": "3644630",
    "end": "3652160"
  },
  {
    "text": "would you trust this enough\nto deploy or at least consider deploying this kind of a\nmodel so unsurprisingly if you",
    "start": "3652160",
    "end": "3660140"
  },
  {
    "text": "show this model most of them\nwere like no this is bad it's using race it's using\ngender it's not it's a no go",
    "start": "3660140",
    "end": "3666380"
  },
  {
    "text": "right but if we show\nthat they're like Oh OK so it seems to be using\nreasonable features I use prior",
    "start": "3666380",
    "end": "3672860"
  },
  {
    "text": "to I use prior arrests I use the\ncriminal status of the person. So it's using all the\nfeatures that I would use.",
    "start": "3672860",
    "end": "3679620"
  },
  {
    "text": "So Yeah why not like\nat least consider deploying it right\nso this is again sort of the gap between\nthey're assuming",
    "start": "3679620",
    "end": "3688130"
  },
  {
    "text": "that an explanation will sort\nof be causal in some sense right at least with\nrespect to the model.",
    "start": "3688130",
    "end": "3695070"
  },
  {
    "text": "But that's not how all\nexplanations are constructed, right. So that is a problem.",
    "start": "3695070",
    "end": "3700950"
  },
  {
    "text": "So in fact, people are likely\nto trust that the underlying model with that\nexplanation times more",
    "start": "3700950",
    "end": "3708130"
  },
  {
    "text": "than an underlying model\nwith this explanation so and there are also a\nbunch of other studies",
    "start": "3708130",
    "end": "3717319"
  },
  {
    "text": "which basically demonstrate\nthat post hoc explanations can instill over trust.",
    "start": "3717320",
    "end": "3723360"
  },
  {
    "text": "So for example,\nthere is a nice study done by Microsoft Research\nand harmanpreet kaur",
    "start": "3723360",
    "end": "3728390"
  },
  {
    "text": "so they showed that domain\nexperts and end users first",
    "start": "3728390",
    "end": "3733519"
  },
  {
    "text": "of all, they are not\nentirely understanding what these explanations\nare doing and second of all, whenever there\nis an explanation they're",
    "start": "3733520",
    "end": "3740630"
  },
  {
    "text": "kind of over trusting the\nunderlying model no matter what the explanation is right\nso in some sense they actually",
    "start": "3740630",
    "end": "3747650"
  },
  {
    "text": "did these interviews\nwith data scientists and there are several\ninteresting codes but essentially what they sort\nof infer from their studies",
    "start": "3747650",
    "end": "3756109"
  },
  {
    "text": "that participations are\ntrusting their tools because of their fancy visualizations,",
    "start": "3756110",
    "end": "3761670"
  },
  {
    "text": "and they're well\ndocumented GitHub packages to put it very\nbluntly right so here",
    "start": "3761670",
    "end": "3768440"
  },
  {
    "text": "are some quotes from that\nstudy think let me see which is a good one so Yeah\nthis for example,",
    "start": "3768440",
    "end": "3775950"
  },
  {
    "text": "this last one the tool shows\nvisualization of ML models which is not something anything\nelse I've worked with has done",
    "start": "3775950",
    "end": "3782900"
  },
  {
    "text": "it's very transparent and that's\nwhy I trust it more right so I",
    "start": "3782900",
    "end": "3789529"
  },
  {
    "text": "think, again, there is a\nbig gap between how I think the research community is\nthinking about interpretability",
    "start": "3789530",
    "end": "3796580"
  },
  {
    "text": "and how practitioners, even data scientists\nyou don't have to go to doctors or\njudges or lawyers",
    "start": "3796580",
    "end": "3802609"
  },
  {
    "text": "like even data scientists\nare not fully on board with what these\napproaches are doing",
    "start": "3802610",
    "end": "3808250"
  },
  {
    "text": "and there is a huge\nrisk that they're over trusting some of\nthese things Kate",
    "start": "3808250",
    "end": "3814920"
  },
  {
    "text": "all right so I think maybe\nI'll cover this part a bit and then we can go for\na break so then there",
    "start": "3814920",
    "end": "3822960"
  },
  {
    "text": "is the question of utility right\nso a lot of times when we are motivating whether\nit's interpretations",
    "start": "3822960",
    "end": "3828539"
  },
  {
    "text": "or explanations we are\nbasically saying that Oh these are useful for\ndebugging for instance.",
    "start": "3828540",
    "end": "3834400"
  },
  {
    "text": "So one of the study done with\nthe housing price prediction task basically demonstrates\nthat Amazon Mechanical workers",
    "start": "3834400",
    "end": "3842340"
  },
  {
    "text": "are unable to really use linear\nmodel coefficients to capture or diagnose the mistakes that\nthe model is making right",
    "start": "3842340",
    "end": "3849990"
  },
  {
    "text": "but then you could\nargue that it's a very general population\nthey may not even interpret these\ncoefficients or model well,",
    "start": "3849990",
    "end": "3856870"
  },
  {
    "text": "which is reasonable but like\nthis is our finding right and similarly I think there\nis another paper that came out",
    "start": "3856870",
    "end": "3864690"
  },
  {
    "text": "in 2020 which showed that\nin a particular animal breed classification task users\nfamiliar with machine learning",
    "start": "3864690",
    "end": "3872549"
  },
  {
    "text": "are relying on labels\ninstead of saliency maps for diagnosing\nmodel errors right",
    "start": "3872550",
    "end": "3877980"
  },
  {
    "text": "but that said there\nis one thing which I would hint to take\nwith a pinch of salt",
    "start": "3877980",
    "end": "3883950"
  },
  {
    "text": "so the evidence at this point\nis really mixed so in some cases we are seeing that these whether\nit's model interpretations",
    "start": "3883950",
    "end": "3892590"
  },
  {
    "text": "or explanations are adding value\nand they're improving something about the decision making\nprocesses in some cases",
    "start": "3892590",
    "end": "3899849"
  },
  {
    "text": "we are seeing contrary\nevidence right. So I think that's why\nI was also highlighting to one of our prior\nquestions that I think",
    "start": "3899850",
    "end": "3906630"
  },
  {
    "text": "there is a need for a clearer\ncharacterization there is a need for more\nstandardized experimentation",
    "start": "3906630",
    "end": "3913380"
  },
  {
    "text": "and benchmarking so that\nat least we understand that if I use these settings\nif I do this exactly",
    "start": "3913380",
    "end": "3920819"
  },
  {
    "text": "this is the result\nthat I would get right or under these conditions these\nare the results that I would",
    "start": "3920820",
    "end": "3926520"
  },
  {
    "text": "get I think that understanding\ncurrently is not there it's of course an evolving field.",
    "start": "3926520",
    "end": "3931809"
  },
  {
    "text": "So we are still\ntrying to figure out what are the best practices\nhow to go about these things but currently the\nevidence in favor",
    "start": "3931810",
    "end": "3938579"
  },
  {
    "text": "or against these approaches\nare kind of mixed right so I think I'll pause\nhere and then we",
    "start": "3938580",
    "end": "3945809"
  },
  {
    "text": "can continue this\ndiscussion at 3 PM OK Thank you\nguys good all right",
    "start": "3945810",
    "end": "3958610"
  },
  {
    "text": "OK all right welcome\nback, everyone. So this is our last hour or\nlast part of the session.",
    "start": "3958610",
    "end": "3967829"
  },
  {
    "text": "Thanks for sticking\nsince morning. So I think we're almost\nat the tail end of it.",
    "start": "3967830",
    "end": "3973470"
  },
  {
    "text": "So let's jump back to what\nwe were discussing right when we took the break so we\nwere talking about utility",
    "start": "3973470",
    "end": "3981440"
  },
  {
    "text": "and thinking about how\npost hoc explanations are used in practice like what\nare people finding and so on.",
    "start": "3981440",
    "end": "3988730"
  },
  {
    "text": "So this is again some of the\nvery recent work out in 2022 by some of students\nin our group.",
    "start": "3988730",
    "end": "3996720"
  },
  {
    "text": "So the idea here is\nto basically think about I think at\nan abstract level,",
    "start": "3996720",
    "end": "4002870"
  },
  {
    "text": "we are all able to see that\ndifferent post hoc explanation methods are generating\npotentially different looking",
    "start": "4002870",
    "end": "4011080"
  },
  {
    "text": "explanations right so what\nare the implications of that like are people able to\nunderstand that there are",
    "start": "4011080",
    "end": "4017200"
  },
  {
    "text": "these differences and perceive\nthem appropriately right so what we did here was\nessentially a bunch of studies",
    "start": "4017200",
    "end": "4024579"
  },
  {
    "text": "with practitioners\ndata scientists this time working in some of\nthe tech startups which actually",
    "start": "4024580",
    "end": "4030760"
  },
  {
    "text": "focus on explainability. So there is like a\ndirect connection in terms of these people sort\nof using these tools on a very",
    "start": "4030760",
    "end": "4039940"
  },
  {
    "text": "regular basis we did a bunch\nof semi-structured interviews like 30 minute semi-structured\ninterviews with about 25 data",
    "start": "4039940",
    "end": "4047980"
  },
  {
    "text": "scientists and several of\nthem precisely 84% of them actually said that they\nencountered disagreement",
    "start": "4047980",
    "end": "4056050"
  },
  {
    "text": "between the explanations\noutput by these methods almost on a consistent basis\nwhat's also interesting",
    "start": "4056050",
    "end": "4062680"
  },
  {
    "text": "is they actually say that they\ntypically run not just one, but multiple methods when they\nhave to try to understand what",
    "start": "4062680",
    "end": "4071530"
  },
  {
    "text": "are the features or what is\nthe explanation of the model behavior for a\nparticular transaction.",
    "start": "4071530",
    "end": "4078190"
  },
  {
    "text": "So how are they thinking\nabout disagreement because we are saying,\nOK, these explanations are look different there is\nsomething different about them",
    "start": "4078190",
    "end": "4086050"
  },
  {
    "text": "but can be precisely\ncharacterize disagreement and according to\nthese people here is how they think about\ndisagreement right",
    "start": "4086050",
    "end": "4093400"
  },
  {
    "text": "and this also boils down to\nsome of the faithfulness metrics that we saw initially when we\ntalked about if there is ground",
    "start": "4093400",
    "end": "4100149"
  },
  {
    "text": "truth feature importance\nis of the model right so according to these people\nthey think to explanations",
    "start": "4100149",
    "end": "4106600"
  },
  {
    "text": "are disagreeing if the top\nfeatures in the explanation are different. So if I ask for a top k feature\nlist and two explanation",
    "start": "4106600",
    "end": "4114130"
  },
  {
    "text": "methods are giving me\ndifferent answers they consider that a disagreement and the\nordering among the top features",
    "start": "4114130",
    "end": "4120818"
  },
  {
    "text": "is different and\nthe direction, which is the positive or negative\nside of contribution",
    "start": "4120819",
    "end": "4126068"
  },
  {
    "text": "of these top features\nis different or even the relative ordering of\ncertain features of interest",
    "start": "4126069",
    "end": "4131318"
  },
  {
    "text": "is different right\nso OK so now they're saying Yeah we see a\nlot of differences.",
    "start": "4131319",
    "end": "4137620"
  },
  {
    "text": "This is how we think\nof a difference this is how we think to\nexplanations would qualify as being different right so\nwhat do they do to resolve",
    "start": "4137620",
    "end": "4145180"
  },
  {
    "text": "these disagreements so if\nyou run two methods you see different answers what do\nyou do right the answer",
    "start": "4145180",
    "end": "4151028"
  },
  {
    "text": "is actually not so optimistic\nwhen are a positive 1. So what we did was a\nstudy to actually see",
    "start": "4151029",
    "end": "4157720"
  },
  {
    "text": "how they are breaking\ntheir ties or what they do to proceed from this\nscenario of disagreement.",
    "start": "4157720",
    "end": "4163640"
  },
  {
    "text": "So we showed these\nusers explanations that disagree according to their\nown definitions of disagreement",
    "start": "4163640",
    "end": "4169929"
  },
  {
    "text": "and ask them to make a choice\nas to which explanation they would pick and why so\nthey're choosing methods",
    "start": "4169930",
    "end": "4177430"
  },
  {
    "text": "due to the following\nfactors associated theory or publication time.",
    "start": "4177430",
    "end": "4182989"
  },
  {
    "text": "So if a paper has more theory in\nthe method that it is proposing for some reason\nthat becomes more",
    "start": "4182990",
    "end": "4189549"
  },
  {
    "text": "authentic or correct or\nauthoritative than the other or if it's more\nrecently published then",
    "start": "4189550",
    "end": "4194890"
  },
  {
    "text": "that's more likely to be correct\nor if the explanations match your own intuition better of\nwhat the model should do then",
    "start": "4194890",
    "end": "4203980"
  },
  {
    "text": "they're potentially picking such\nexplanations or types of data. For example, if\nit's tabular data",
    "start": "4203980",
    "end": "4210310"
  },
  {
    "text": "I'll probably use Lyman shepherd\nnot Canadian based that's right so here are some codes and as\nyou can see the factors that we",
    "start": "4210310",
    "end": "4218199"
  },
  {
    "text": "just talked about are sort of\nsprinkled across these codes and you will see\nthem because it says",
    "start": "4218200",
    "end": "4223750"
  },
  {
    "text": "sharp is a more rigorous\napproach than line as it has theory or in\ntheory at least right so",
    "start": "4223750",
    "end": "4229780"
  },
  {
    "text": "things like that. But the aspect of\nOh maybe we should evaluate the faithfulness\nof these two explanations",
    "start": "4229780",
    "end": "4237160"
  },
  {
    "text": "and compare them somehow or what\nabout stability or fairness. So those are not\ncropping up as the things",
    "start": "4237160",
    "end": "4244600"
  },
  {
    "text": "that they would do to pick\none explanation over the other and that is concerned right so\nin some sense there are gaps",
    "start": "4244600",
    "end": "4251560"
  },
  {
    "text": "between how research in\nthese areas is progressing versus how practitioners they\nare taking all these tools",
    "start": "4251560",
    "end": "4259270"
  },
  {
    "text": "and trying to do something with\nthem in their day to day work and it's kind of\nwe are bordering",
    "start": "4259270",
    "end": "4266050"
  },
  {
    "text": "on all these kinds of\nissues which are clearly undesirable right so that's\nyet another cautionary tale",
    "start": "4266050",
    "end": "4274000"
  },
  {
    "text": "to bridge the gap between\nresearch that is happening and also the\npractitioner community.",
    "start": "4274000",
    "end": "4280310"
  },
  {
    "text": "So in summary, what\nwe have seen is that a lot of these\nempirical studies are analysis are showing that\nsome explanation methods do not",
    "start": "4280310",
    "end": "4289270"
  },
  {
    "text": "reflect or faithfully capture\nthe behavior of the underlying model some of these\nexplanation methods",
    "start": "4289270",
    "end": "4295150"
  },
  {
    "text": "can be easily manipulated\nslight changes to inputs can cause large changes in\nexplanation particularly",
    "start": "4295150",
    "end": "4301840"
  },
  {
    "text": "for methods that\nuse perturbations like line their\nutility in practice is something that we still\nneed to think more about right",
    "start": "4301840",
    "end": "4311500"
  },
  {
    "text": "so that's one part of it. And on the we are talking a\nlot about explanation methods",
    "start": "4311500",
    "end": "4317170"
  },
  {
    "text": "or post-hoc explanation methods. In fact, if you recall, we\nalso saw that attention weights",
    "start": "4317170",
    "end": "4322510"
  },
  {
    "text": "which falls under our inherently\ninterpretable models category. Those are also not corresponding\nto meaningful information",
    "start": "4322510",
    "end": "4329380"
  },
  {
    "text": "that was another result\nthat we saw right and there are no studies\non prototype layers",
    "start": "4329380",
    "end": "4336040"
  },
  {
    "text": "and adding them to this\nwhole deep learning pipeline and so which means at\nleast here exploration",
    "start": "4336040",
    "end": "4343510"
  },
  {
    "text": "is being done so we\nknow there are issues. So there are areas where there\nis not even exploration being",
    "start": "4343510",
    "end": "4349270"
  },
  {
    "text": "done and of course that. We'll talk about that a bit\nmore in future research problems",
    "start": "4349270",
    "end": "4354639"
  },
  {
    "text": "all right so let's\ntake a look at some of the theoretical results\nboth for the analysis of interpretable models\ninherently interpretable models",
    "start": "4354640",
    "end": "4362469"
  },
  {
    "text": "and post hoc explanations\nwhen it comes to inherently interpretable models\nyou'll potentially see like",
    "start": "4362470",
    "end": "4369100"
  },
  {
    "text": "or you'll predominantly\nsee two main classes of theoretical results. The first one is it's basically\nabout certifying the optimality",
    "start": "4369100",
    "end": "4379239"
  },
  {
    "text": "of the lists or sets that\nare learned using certain procedures right so if I use\nmy algorithm and generate such",
    "start": "4379240",
    "end": "4388060"
  },
  {
    "text": "a rule list or such a rule set\nI can certify that it will be within this much of the\noptimal so that is one kinds",
    "start": "4388060",
    "end": "4396039"
  },
  {
    "text": "of theoretical one set of\ntheoretical results you'll see and there are also some\nmore recent sort of results",
    "start": "4396040",
    "end": "4403540"
  },
  {
    "text": "on saying that there are no\naccuracy interpretability trade offs in certain settings that\nmeans you can come up with",
    "start": "4403540",
    "end": "4411730"
  },
  {
    "text": "an interpretable model which is\nsimple enough and it'll also be the most accurate model but\nwhile this is a good starting",
    "start": "4411730",
    "end": "4418630"
  },
  {
    "text": "point these settings are rather\nrestrictive for example it's like RL with small mazes\nright so in such a setting",
    "start": "4418630",
    "end": "4426550"
  },
  {
    "text": "people have shown that\nthere are no trade but that's clearly not\nrepresentative of a lot",
    "start": "4426550",
    "end": "4432310"
  },
  {
    "text": "of real world settings so while\nthere are preliminary results I think there is scope to do a lot\nmore than these now let's talk",
    "start": "4432310",
    "end": "4440230"
  },
  {
    "text": "about the theoretical results on\nthe post hoc explanation method site so this is\none of the papers",
    "start": "4440230",
    "end": "4446110"
  },
  {
    "text": "that I think kicked off the\ntheoretical analysis of post hoc explanation methods\nby Gary O'Toole they did",
    "start": "4446110",
    "end": "4452110"
  },
  {
    "text": "this analysis of\na variant of Lyme called tabular line which only\noperates on discrete features",
    "start": "4452110",
    "end": "4458530"
  },
  {
    "text": "and not on continuous features. So what they show is that they\nobtained closed form solutions",
    "start": "4458530",
    "end": "4465370"
  },
  {
    "text": "of the average\ncoefficients output by line when the underlying\nBlack box is a linear model.",
    "start": "4465370",
    "end": "4472100"
  },
  {
    "text": "So all these results are just\nwhen the underlying model itself is a linear model\nand they also show that",
    "start": "4472100",
    "end": "4479140"
  },
  {
    "text": "the coefficients obtained are\nproportional to the gradient of the function to be explained\nagain with respect to a linear",
    "start": "4479140",
    "end": "4486010"
  },
  {
    "text": "model and they show that the\nlocal error of the surrogate model is bounded away from 0\nwith a high probability right",
    "start": "4486010",
    "end": "4495580"
  },
  {
    "text": "so the explanations error is\nlike bounded away from 0 with a high probability all right\nso this is some of our work",
    "start": "4495580",
    "end": "4505540"
  },
  {
    "text": "on unifying some of these\nmethods which is specifically lime and smooth grid what\nwe consider is actually",
    "start": "4505540",
    "end": "4512500"
  },
  {
    "text": "a continuous version of Lyme\nwhere the perturbations are happening in a continuous\nspace and not in",
    "start": "4512500",
    "end": "4518710"
  },
  {
    "text": "a binary space or a\ndiscrete space and we show that these two\nmethods actually",
    "start": "4518710",
    "end": "4524380"
  },
  {
    "text": "converge our output the same\nexplanation in expectation. So if we basically\nhave the ability",
    "start": "4524380",
    "end": "4530950"
  },
  {
    "text": "to generate in finite\nnumber of perturbations and get an explanation, then\nthe resulting explanation",
    "start": "4530950",
    "end": "4537340"
  },
  {
    "text": "would sort of converge\nbetween these methods but it's not that\npessimistic we can actually",
    "start": "4537340",
    "end": "4543040"
  },
  {
    "text": "get it with finite\nsamples As well there is also that\nresult what we also show is that in expectation the\nresulting explanations are",
    "start": "4543040",
    "end": "4552280"
  },
  {
    "text": "provably robust according to the\nnotion of Lipschitz continuity so essentially you can give\nsome guarantees on the stability",
    "start": "4552280",
    "end": "4561039"
  },
  {
    "text": "or robustness of these\nexplanations in expectation and we also have finite\nsample complexity",
    "start": "4561040",
    "end": "4567730"
  },
  {
    "text": "bounds for the number\nof perturbed samples or perturbations required\nfor these two methods",
    "start": "4567730",
    "end": "4574510"
  },
  {
    "text": "to converge to their\nexpected output. So what is the minimum\nnumber of samples required for these two methods\nto converge to an output",
    "start": "4574510",
    "end": "4582760"
  },
  {
    "text": "result in the same output. So what would that look\nlike right so another one",
    "start": "4582760",
    "end": "4590200"
  },
  {
    "text": "of the recent works that\nwe have been involved with and I'm super excited\nabout this one specifically",
    "start": "4590200",
    "end": "4596170"
  },
  {
    "text": "is to sort of bring some\norder to a lot of these post hoc explanation methods that are\nout there right so in a very,",
    "start": "4596170",
    "end": "4604160"
  },
  {
    "text": "very recent paper\nwhat we show is that various feature\nattribution methods and when I use that\nword feature attribution",
    "start": "4604160",
    "end": "4611110"
  },
  {
    "text": "methods it is methods like\nlying continuous variant of lime sharp and all the gradient\nbased methods they all generate",
    "start": "4611110",
    "end": "4620110"
  },
  {
    "text": "some form of feature importance\nof feature attribution right so we show that these\nare all essentially",
    "start": "4620110",
    "end": "4626949"
  },
  {
    "text": "local linear function\napproximators so they're all trying to do a local linear\nfunction approximation",
    "start": "4626950",
    "end": "4634240"
  },
  {
    "text": "but the reason they are\ndisagree with each other as we have been seeing\nquite a bit in the past",
    "start": "4634240",
    "end": "4640390"
  },
  {
    "text": "hour or so is because\nthey adopt different loss functions and\ndifferent definitions",
    "start": "4640390",
    "end": "4647200"
  },
  {
    "text": "of local neighborhoods. And this is precisely\nthe different definitions of local neighborhoods and\nloss functions that they adopt.",
    "start": "4647200",
    "end": "4655910"
  },
  {
    "text": "So this is at least trying\nto put some order as to they're all sort of\nunified in some ways.",
    "start": "4655910",
    "end": "4662930"
  },
  {
    "text": "But at the same time\nthere are differences because they're all adopting\ntheir own definition of what",
    "start": "4662930",
    "end": "4668290"
  },
  {
    "text": "loss function to\nuse and then how to define a local neighborhood.",
    "start": "4668290",
    "end": "4673659"
  },
  {
    "text": "So we also have something\ncalled as a no free lunch theorem for explanation methods\ngiven that these methods all",
    "start": "4673660",
    "end": "4680500"
  },
  {
    "text": "adopt a particular definition of\nwhat a local neighborhood looks like no single method\ncan perform optimally",
    "start": "4680500",
    "end": "4688690"
  },
  {
    "text": "across all neighborhoods\nbecause they're all tailored towards certain\ndefinitions of what a local neighborhood should\nbe right so this theorem is",
    "start": "4688690",
    "end": "4697030"
  },
  {
    "text": "basically to me it's interesting\nand also I think it has more implications is that every\ntime a new paper comes out,",
    "start": "4697030",
    "end": "4704510"
  },
  {
    "text": "people show the performance that\nOh this beats it on everything is right so this\nresult basically",
    "start": "4704510",
    "end": "4709810"
  },
  {
    "text": "shows that no single\nmethod is going to work across all definitions\nof what a local neighborhood is",
    "start": "4709810",
    "end": "4716739"
  },
  {
    "text": "right so that claim that\nthis method beats everything at least with respect\nto certain aspects of it",
    "start": "4716740",
    "end": "4724510"
  },
  {
    "text": "will not hold because of\nthis theoretical result. So with that we are at\nthe end of this module",
    "start": "4724510",
    "end": "4731889"
  },
  {
    "text": "and then we are going to\ndiscuss more about open problems future research directions\nin the next 40 minutes or 45",
    "start": "4731890",
    "end": "4739100"
  },
  {
    "text": "minutes.",
    "start": "4739100",
    "end": "4740650"
  }
]