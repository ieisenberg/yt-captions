[
  {
    "start": "0",
    "end": "5510"
  },
  {
    "text": "Hi, everyone. We're going to go\nahead and get started. I'm Emma Brunskill. I'm delighted to welcome you to\nReinforcement Learning, CS234.",
    "start": "5510",
    "end": "13100"
  },
  {
    "text": "This is a brief\noverview of the class and what we're going\nto be covering today. And I just want to start that\nprobably everyone's heard",
    "start": "13100",
    "end": "21349"
  },
  {
    "text": "of reinforcement\nlearning these days. That wasn't true about\n10, 15 years ago. But you can describe what is\nhappening in reinforcement",
    "start": "21350",
    "end": "28340"
  },
  {
    "text": "learning by a pretty\nsimple statement, which is the idea of an\nautomated agent learning",
    "start": "28340",
    "end": "34460"
  },
  {
    "text": "through experience to\nmake good decisions. Now that's a pretty\nsimple statement to say.",
    "start": "34460",
    "end": "41074"
  },
  {
    "text": "It sort of encapsulates\na lot of what me and my lab and\nmany, many others have been trying to work on\nfor the last 10 to 15 years.",
    "start": "41075",
    "end": "47780"
  },
  {
    "text": "But it's sort of\ndeceptively simple because it involves a\nlot of different really challenging and\nimportant things.",
    "start": "47780",
    "end": "52960"
  },
  {
    "text": " So the first is that any\nsort of general agenda",
    "start": "52960",
    "end": "58510"
  },
  {
    "text": "to try to achieve general\nartificial intelligence has to include the\nability to make decisions.",
    "start": "58510",
    "end": "63850"
  },
  {
    "text": "There's been absolutely\nenormous progress in what we would call\nsort of perceptual machine",
    "start": "63850",
    "end": "69010"
  },
  {
    "text": "learning, things like being\nable to perceive faces or cats or identify cars.",
    "start": "69010",
    "end": "74930"
  },
  {
    "text": "And we call that often\nperceptual machine learning because it\nfocuses on trying to, say, identify something.",
    "start": "74930",
    "end": "81439"
  },
  {
    "text": "But, of course, in\nreality, what we're all trying to do all\nthe time is also to make decisions\nbased on our perception",
    "start": "81440",
    "end": "87070"
  },
  {
    "text": "and based on our\ninformation we're receiving. And so it's critical if we\nthink about what it means",
    "start": "87070",
    "end": "92469"
  },
  {
    "text": "to be intelligent to consider\nhow to make decisions, and not just any decisions, but what it\nmeans to have good decisions.",
    "start": "92470",
    "end": "100420"
  },
  {
    "text": "This sort of question over how\ncan we learn to make decisions, particularly faced by\nuncertainty and limited data,",
    "start": "100420",
    "end": "107860"
  },
  {
    "text": "has been a central question that\npeople have been thinking about at least since the 1950s,\nparticularly pioneered",
    "start": "107860",
    "end": "114340"
  },
  {
    "text": "by the ideas of Richard Bellman. And we'll hear a lot more\nabout Bellman's equation, which many of you might\nhave seen before,",
    "start": "114340",
    "end": "120350"
  },
  {
    "text": "later even in this\nlecture or next lecture. So there's one sort of\nargument for studying",
    "start": "120350",
    "end": "125690"
  },
  {
    "text": "reinforcement\nlearning, which is it's an essential part\nof intelligence. It has to be part\nof a general agenda",
    "start": "125690",
    "end": "131240"
  },
  {
    "text": "of artificial intelligence. And so we should study it\nto try to understand what it means to be intelligent.",
    "start": "131240",
    "end": "137610"
  },
  {
    "text": "And that certainly for me is one\nof the really big motivations. So I think there's just a lot of\nfundamental questions about what",
    "start": "137610",
    "end": "143239"
  },
  {
    "text": "is the data needed to learn\nto make good decisions. But there's another\nreally good motivation to study reinforcement learning,\nwhich is that it's practical",
    "start": "143240",
    "end": "150920"
  },
  {
    "text": "and it allows us to solve\nproblems we'd like to solve. So in particular, over\nthe last roughly decade,",
    "start": "150920",
    "end": "157350"
  },
  {
    "text": "there started to be a lot of\nreally impressive successes of using reinforcement\nlearning to tackle problems",
    "start": "157350",
    "end": "162920"
  },
  {
    "text": "or to get unprecedented\nperformance in a lot of really important domains.",
    "start": "162920",
    "end": "168020"
  },
  {
    "text": "So the first one is\nthe board game Go. So who here plays Go?",
    "start": "168020",
    "end": "173360"
  },
  {
    "text": "OK, a few people. Maybe not. You can talk to the people\nthat raise their hands. So it's an incredibly\npopular board game.",
    "start": "173360",
    "end": "178920"
  },
  {
    "text": "It's also an incredibly\nhard board game. It's far harder\nthan chess, and it was considered a really\nlong outstanding question",
    "start": "178920",
    "end": "185720"
  },
  {
    "text": "of artificial intelligence. But roughly like, I guess\nabout eight years ago now,",
    "start": "185720",
    "end": "190860"
  },
  {
    "text": "eight to nine years\nago, there was a team at DeepMind, which was still\na fairly small organization at that point, that thought\nthat they could make",
    "start": "190860",
    "end": "197720"
  },
  {
    "text": "significant headway at teaching\nAI agents to be able to play Go.",
    "start": "197720",
    "end": "202850"
  },
  {
    "text": "And the idea in this\ncase is that we're going to combine between the\nideas of reinforcement learning",
    "start": "202850",
    "end": "208730"
  },
  {
    "text": "and Monte Carlo Tree\nSearch, which is something we're going to hear about\nlater in this class, to create a system\nthat played Go better",
    "start": "208730",
    "end": "216260"
  },
  {
    "text": "than any humans in the world. And so there's even a\nmovie now about sort of one of the seminal games\nin that sort of endeavor",
    "start": "216260",
    "end": "224660"
  },
  {
    "text": "and how humans felt about that\nand how the creators of the AI systems felt about that.",
    "start": "224660",
    "end": "229790"
  },
  {
    "text": "But this feat was\nachieved far earlier than what people expected. And one of the key\nreasons for that",
    "start": "229790",
    "end": "235700"
  },
  {
    "text": "was using\nreinforcement learning.  Another really interesting\nplace that we've",
    "start": "235700",
    "end": "241990"
  },
  {
    "text": "seen progress of using\nreinforcement learning to tackle incredible\nchallenges is in the idea of fusion science.",
    "start": "241990",
    "end": "248320"
  },
  {
    "text": "Fusion is a potential\napproach for trying to tackle the huge\nenergy issues that we have in trying to\ntransition to more",
    "start": "248320",
    "end": "254319"
  },
  {
    "text": "sustainable options for that. And one of the challenges here--\nand I'm not a fusion expert--",
    "start": "254320",
    "end": "260028"
  },
  {
    "text": "is to manipulate and sort of\ncontrol things within a vessel.",
    "start": "260029",
    "end": "265990"
  },
  {
    "text": "And so what the reinforcement\nlearning question in this case is, how do you command\nthe controllers, the coil",
    "start": "265990",
    "end": "271810"
  },
  {
    "text": "controllers, in order\nto manipulate this into different types of shapes? And so this was a Nature\npaper from two years ago",
    "start": "271810",
    "end": "277841"
  },
  {
    "text": "where they showed you could\nuse deep reinforcement learning techniques to accomplish\nthis in a way that was far more flexible than\nhad previously been imagined.",
    "start": "277842",
    "end": "286870"
  },
  {
    "text": "One of my favorite examples\nof the applications of reinforcement learning\ncomes from a pretty recent important case, which\nis COVID testing.",
    "start": "286870",
    "end": "295240"
  },
  {
    "text": "So this was a system that\nwas deployed in Greece. They had limited\nresources, and they were trying to understand\nwho you should test in order",
    "start": "295240",
    "end": "302470"
  },
  {
    "text": "to help control the epidemic,\nbecause as many of you may know, there's a lot of sort of\nfree movement within Europe",
    "start": "302470",
    "end": "308150"
  },
  {
    "text": "and there was a\nlot of transitions. And they were trying\nto think about how to leverage their resources\nin a data-driven way",
    "start": "308150",
    "end": "313930"
  },
  {
    "text": "because, of course, the\nepidemic was changing. And so this is a beautiful paper\nby a Stanford graduate, Hamsa",
    "start": "313930",
    "end": "320230"
  },
  {
    "text": "Bastani, and her colleague. She's a professor\nover at Penn now that used reinforcement learning\nto really quickly do this.",
    "start": "320230",
    "end": "326810"
  },
  {
    "text": "And it was deployed. So Greece used this for\ntheir testing at the border.",
    "start": "326810",
    "end": "332920"
  },
  {
    "text": "But perhaps the most famous\nexample recently is ChatGPT. So I think that as\nmany of you might know,",
    "start": "332920",
    "end": "340345"
  },
  {
    "text": "natural language processing\nhas had incredible successes over the last decade. And there was a\nlot of work trying",
    "start": "340345",
    "end": "346970"
  },
  {
    "text": "to use transformers to\nmake really, really capable natural language systems.",
    "start": "346970",
    "end": "352159"
  },
  {
    "text": "But up till around, I guess,\nlike a year and a half ago, most of that work was not\nknown to the broader public.",
    "start": "352160",
    "end": "359050"
  },
  {
    "text": "So even though we were\ngetting these amazing advances in natural language processing,\nit wasn't at the state yet where everybody\nwas using it.",
    "start": "359050",
    "end": "366110"
  },
  {
    "text": "And so the key\nidea of ChatGPT was to use reinforcement\nlearning to create vastly more capable systems.",
    "start": "366110",
    "end": "373910"
  },
  {
    "text": "And I like to talk about\nChatGPT not because it's perhaps the most well-known success\nfor reinforcement learning,",
    "start": "373910",
    "end": "379680"
  },
  {
    "text": "but also because\nit exhibits a lot of the different technical\nchallenges and questions that we're going to be\ncovering in this class.",
    "start": "379680",
    "end": "385550"
  },
  {
    "text": "So let's just walk\nthrough sort of how at a very high level\nsort of this figure from ChatGPT of how the\nChatGPT system works",
    "start": "385550",
    "end": "393050"
  },
  {
    "text": "in terms of training. So the first thing\nit does is it does what we would probably\ncall behavior cloning",
    "start": "393050",
    "end": "401800"
  },
  {
    "text": "or imitation learning. We'll be covering\nthat in this class.",
    "start": "401800",
    "end": "406939"
  },
  {
    "text": "And we'll be talking more\nabout it even in this lecture. So what did it do? So, again, just to remind, I\nsuspect everybody in this class",
    "start": "406940",
    "end": "413600"
  },
  {
    "text": "probably uses ChatGPT\nprobably multiple times a day or Claude or Gemini or one of\nthe other large language model",
    "start": "413600",
    "end": "420080"
  },
  {
    "text": "systems. But just in case you have\nnot, the idea in this case is that you might have\nsome sort of prompt or task",
    "start": "420080",
    "end": "426170"
  },
  {
    "text": "you want your\nlanguage system to do, like explain reinforcement\nlearning to a six-year-old, and then someone\ngives a response,",
    "start": "426170",
    "end": "433580"
  },
  {
    "text": "like we give treats and\npunishments to teach, et cetera. And you can think you can\ntry this out with ChatGPT",
    "start": "433580",
    "end": "439460"
  },
  {
    "text": "and see how well you\nthink it explains it. And then what that\nwas treated as is sort of a direct\nsupervised learning problem.",
    "start": "439460",
    "end": "445770"
  },
  {
    "text": "So just trying to\ntake that input and then to produce that output. And we will call that also\nimitation learning or behavior",
    "start": "445770",
    "end": "453050"
  },
  {
    "text": "cloning in this class. And we'll talk about why. So that was the first step. And this is sort of what\npeople have been doing",
    "start": "453050",
    "end": "459200"
  },
  {
    "text": "in natural language processing. And the systems were good,\nbut they weren't that good. So the next idea was\nto try to explicitly",
    "start": "459200",
    "end": "466280"
  },
  {
    "text": "think about utility\nor rewards, like how good were these\nparticular labels",
    "start": "466280",
    "end": "471750"
  },
  {
    "text": "or these particular outputs. So here we're going to\nactually build a model. We're going to build a\nmodel of a reward which",
    "start": "471750",
    "end": "479940"
  },
  {
    "text": "relates to model-based\nreinforcement learning. And the way we're\ngoing to do this is--",
    "start": "479940",
    "end": "486000"
  },
  {
    "text": "or the way they did this is\nwe collect preference data. We ask people to compare or\nrank across different forms",
    "start": "486000",
    "end": "491550"
  },
  {
    "text": "of outputs. And then we use that to\nlearn a preference model. And we're going to cover\nthat in this class.",
    "start": "491550",
    "end": "497139"
  },
  {
    "text": "That's going to be one of\nthe differences to this class compared to a\ncouple of years ago that I think preference-based\nreward signals are really",
    "start": "497140",
    "end": "503699"
  },
  {
    "text": "important and very powerful. And so we're going to be\ncovering that in this class this term. So in this case, they\nwould learn a reward.",
    "start": "503700",
    "end": "510026"
  },
  {
    "text": "And again, don't worry if you\nhaven't-- if you're not familiar with what rewards are and stuff. We'll go through all of that. I just want to give\nyou a high level",
    "start": "510027",
    "end": "516070"
  },
  {
    "text": "sort of sense of how\nChatGPT is related to some of the things we're\ngoing to cover in the class. So they learned a reward signal.",
    "start": "516070",
    "end": "521919"
  },
  {
    "text": "And then they did\nreinforcement learning using that learned reward signal. So now they're going to\ndo reinforcement learning.",
    "start": "521919",
    "end": "528785"
  },
  {
    "text": " And this is called RLHF\nbecause it is reinforcement",
    "start": "528785",
    "end": "537050"
  },
  {
    "text": "learning from human feedback. And I'll just note here that\nthis was not the first time this idea was introduced.",
    "start": "537050",
    "end": "544350"
  },
  {
    "text": "It had been introduced maybe\nabout four to five years before this for sort of\nsimulated robotics tasks.",
    "start": "544350",
    "end": "551779"
  },
  {
    "text": "But ChatGPT demonstrated\nthat this really made a huge difference\nin performance.",
    "start": "551780",
    "end": "557630"
  },
  {
    "text": "And so I think it's\na really nice example of the types of ideas that\nwe're going to be covering, as well as sort of the\nincredible successes",
    "start": "557630",
    "end": "563630"
  },
  {
    "text": "that are possible.  Now, even before\nChatGPT came along,",
    "start": "563630",
    "end": "570135"
  },
  {
    "text": "there was starting to be a\nhuge interest in reinforcement learning. So some of you-- we have an\noptional textbook for the class.",
    "start": "570135",
    "end": "576870"
  },
  {
    "text": "It's by Sutton and Barto. Richard Sutton is\nfrom Canada and is one of the sort of\nfounders of the field.",
    "start": "576870",
    "end": "583490"
  },
  {
    "text": "And when I started in\nreinforcement learning and I would go give my\ntalks in conferences, it used to be like me and\nRich and 30 other people.",
    "start": "583490",
    "end": "590430"
  },
  {
    "text": "Nobody cared about\nreinforcement learning. I mean, it's not for, you know. A few of us did because we\nthought it was really amazing.",
    "start": "590430",
    "end": "596045"
  },
  {
    "text": "But as you can see, sort\nof through the 2000s, which is when I was getting\nmy training around here,",
    "start": "596045",
    "end": "601649"
  },
  {
    "text": "there just weren't\nthat many papers, and the community\nwasn't nearly as large. But this nice paper\nby Peter Henderson--",
    "start": "601650",
    "end": "608410"
  },
  {
    "text": "the y-axis is papers-- shows that there's been this\nsort of enormous increase",
    "start": "608410",
    "end": "614940"
  },
  {
    "text": "in interest. And I think a lot\nof this was really due to the fact that\nkind of around here",
    "start": "614940",
    "end": "620940"
  },
  {
    "text": "there was some amazing successes\non the Atari video games, where people showed that\nyou could learn directly",
    "start": "620940",
    "end": "627750"
  },
  {
    "text": "from pixel input\nto make decisions. And then there started to\nbe the successes in AlphaGo,",
    "start": "627750",
    "end": "632770"
  },
  {
    "text": "and then there became\nmore and more successes. So it is an incredible time\nfor reinforcement learning.",
    "start": "632770",
    "end": "638470"
  },
  {
    "text": "This curve is\ncontinued to go up. However, I think it's also\nimportant to notice that there is also a number of skeptics.",
    "start": "638470",
    "end": "646770"
  },
  {
    "text": "So there was a pretty famous\ntalk by Yann LeCun in 2016 at one of the major machine\nlearning conferences, NeurIPS.",
    "start": "646770",
    "end": "655050"
  },
  {
    "text": "So Yann LeCun, for those\nof you who don't know him, is one of the sort of seminal\nfigures in neural network",
    "start": "655050",
    "end": "660360"
  },
  {
    "text": "research. He has won the Turing Award. He is an amazing,\namazing researcher.",
    "start": "660360",
    "end": "665829"
  },
  {
    "text": "So he gave a keynote at NeurIPS. I believe it was a keynote. He certainly gave\na very famous talk",
    "start": "665830",
    "end": "671110"
  },
  {
    "text": "there, where he was\ntalking about the role of different types of machine\nlearning questions and subareas",
    "start": "671110",
    "end": "676950"
  },
  {
    "text": "in terms of making progress\non machine learning. And he very famously\ntalked about machine",
    "start": "676950",
    "end": "682050"
  },
  {
    "text": "learning as a cake. And so he said that\nthe main cake is really unsupervised learning. And that's really going to be\nthe body, the most important",
    "start": "682050",
    "end": "689190"
  },
  {
    "text": "aspect of machine learning. Things like representation\nlearning from unlabeled data, that's really going\nto be the core,",
    "start": "689190",
    "end": "695320"
  },
  {
    "text": "and that's where we're going\nto have huge amounts of data and we're going to\nmake a lot of progress. And then supervised\nlearning was the icing.",
    "start": "695320",
    "end": "701019"
  },
  {
    "text": "So that's still\npretty important. It's like very important\npart of the cake, at least in my opinion. But it doesn't have as much.",
    "start": "701020",
    "end": "706990"
  },
  {
    "text": "We don't have as much\nsupervised learning, and it's sort of [INAUDIBLE]. And then he argued that\nreinforcement learning was just",
    "start": "706990",
    "end": "713399"
  },
  {
    "text": "the cherry. Now cherries are important,\nbut not nearly as much,",
    "start": "713400",
    "end": "719520"
  },
  {
    "text": "perhaps, as the\nrest of the cake. And he went on and\ntalked about some places where he thought that RL\nstill might have a role.",
    "start": "719520",
    "end": "725310"
  },
  {
    "text": "But it was considered\na really important talk because what he was\nsort of demonstrating",
    "start": "725310",
    "end": "731010"
  },
  {
    "text": "is that reinforcement\nlearning was having a part to play in\nmachine learning, but maybe only a\nvery minor part.",
    "start": "731010",
    "end": "737302"
  },
  {
    "text": "Now I think it'd be interesting\nto talk to him today. I haven't talked\nto him recently, so I don't know what\nhis current opinion is,",
    "start": "737302",
    "end": "742590"
  },
  {
    "text": "but I think it's a\nreally important thing to think about\nlike, where are all of these different\ntechniques important,",
    "start": "742590",
    "end": "748230"
  },
  {
    "text": "and where will we be able to\nmake the most progress in terms of advancing AI? ",
    "start": "748230",
    "end": "755160"
  },
  {
    "text": "And so with that, we're\ngoing to try and do our first poll, which is\nabout why you guys want to take this class.",
    "start": "755160",
    "end": "760580"
  },
  {
    "text": "So we'll look through these. You'll have to bear with\nus a little bit with-- we had a few technical difficulties\nthat we're working with CTL on,",
    "start": "760580",
    "end": "766782"
  },
  {
    "text": "but it should work out. So if you go to either\nthe first link in Ed or you go to this\nHTTP, you can--",
    "start": "766782",
    "end": "772350"
  },
  {
    "text": "if you have any issues like,\ndo you want to be registered? If it's hanging, just skip\nthe registration, refresh,",
    "start": "772350",
    "end": "777730"
  },
  {
    "text": "that should all sort it out,\nand then just add in your SUN ID as your screen, and\njust take a second",
    "start": "777730",
    "end": "783600"
  },
  {
    "text": "and write down a bit about why\nyou want to take this class. And it could be anything.",
    "start": "783600",
    "end": "788840"
  },
  {
    "text": "It could be that you're really\ncurious about something. It could be because\nyou're doing an internship and they told you\nhad to take something",
    "start": "788840",
    "end": "794235"
  },
  {
    "text": "about reinforcement learning. Any of the things are fine. Just take a minute or two.",
    "start": "794235",
    "end": "799725"
  },
  {
    "start": "799725",
    "end": "975560"
  },
  {
    "text": "Thanks for all\nthe great reasons. I will talk about some of\nthose when I talk about also what we're going to cover\ntoday and try to address why.",
    "start": "975560",
    "end": "982327"
  },
  {
    "text": "I think a lot of the things\npeople are bringing up are things that we're\ngoing to be touching upon.",
    "start": "982327",
    "end": "987350"
  },
  {
    "text": "So I think if we\nwant to think about-- I think it's really\nimportant to start",
    "start": "987350",
    "end": "992510"
  },
  {
    "text": "thinking about what is\nreinforcement learning about, because if we understand\nwhat it's about, then we know what\ntypes of questions",
    "start": "992510",
    "end": "998840"
  },
  {
    "text": "we're interested in this space. And we also understand\nwhat sort of applications it might be helpful for.",
    "start": "998840",
    "end": "1003940"
  },
  {
    "text": "So, of course, your\ncreativity is unlimited, so you can see what you might\ncome up with other ideas that people may not have\nthought of for applied RL.",
    "start": "1003940",
    "end": "1010856"
  },
  {
    "text": "But the four things\nthat people typically think about when they think\nabout reinforcement learning as a discipline and as the\nsort of what reinforcement",
    "start": "1010856",
    "end": "1018520"
  },
  {
    "text": "learning involves\nis optimization, delayed consequences,\nexploration, and generalization.",
    "start": "1018520",
    "end": "1025720"
  },
  {
    "text": "So the first is optimization. And the optimization\naspect is really just saying that we're\nthinking about the best way",
    "start": "1025720",
    "end": "1033250"
  },
  {
    "text": "to make decisions, which\nmeans that we explicitly have to have some notion of utility.",
    "start": "1033250",
    "end": "1038436"
  },
  {
    "text": " An example of this\nwould be something",
    "start": "1038436",
    "end": "1043630"
  },
  {
    "text": "like finding the\nminimum distance route between two cities\ngiven a network of roads. This means you can directly\ncompare different solutions,",
    "start": "1043630",
    "end": "1050267"
  },
  {
    "text": "because if one solution has a\nsmaller distance than the other, it is strictly preferred. So there are many, many\nimportant optimization questions",
    "start": "1050267",
    "end": "1057177"
  },
  {
    "text": "and reinforcement learning\nbecause it is concerned with making good\ndecisions, cares about us being able\nto rank or decide",
    "start": "1057177",
    "end": "1063460"
  },
  {
    "text": "across those different ones. The second one is delayed\nconsequences, the idea",
    "start": "1063460",
    "end": "1069549"
  },
  {
    "text": "being that the decisions\nthat we make now can affect things far later. So maybe saving for retirement\nnow has some immediate cost,",
    "start": "1069550",
    "end": "1077570"
  },
  {
    "text": "but it leads to some\nsignificant benefit later. Or maybe there's\nsomething you can do early in a video game that later\nhas a lot of benefit.",
    "start": "1077570",
    "end": "1084460"
  },
  {
    "text": "There are two reasons\nwhy delayed consequences is challenging. One is for the\nreason of planning.",
    "start": "1084460",
    "end": "1090997"
  },
  {
    "text": "Many of you might have\nactually-- raise your hand if you've taken AI at Stanford.",
    "start": "1090998",
    "end": "1096140"
  },
  {
    "text": "OK, so about half of you. So you probably\nsaw planning in AI. And planning is the\nidea that even when",
    "start": "1096140",
    "end": "1101530"
  },
  {
    "text": "we understand how\nthe world works, it might be really complicated\nto try to decide what the optimal thing is to do.",
    "start": "1101530",
    "end": "1106960"
  },
  {
    "text": "So you can think\nof this like chess. All the rules are known. It's still really complicated\nto think about what's",
    "start": "1106960",
    "end": "1112150"
  },
  {
    "text": "the right thing to do. So when the decisions you make\ninvolve reasoning not just about the immediate outcomes but\nthe longer term ramifications,",
    "start": "1112150",
    "end": "1119390"
  },
  {
    "text": "the sort of planning\nproblems are even harder. But the other reason\nthis is really hard is when we're learning,\nmeaning that we don't know",
    "start": "1119390",
    "end": "1125980"
  },
  {
    "text": "how the world works and we're\ntrying to understand how through direct experience. So when we're learning, temporal\ncredit assignment is hard,",
    "start": "1125980",
    "end": "1134600"
  },
  {
    "text": "meaning that if you take some\naction now and later on you receive a good outcome\nor a bad outcome, how do you figure out\nwhich of your outcomes",
    "start": "1134600",
    "end": "1142030"
  },
  {
    "text": "caused that good or\nbad later result? So this happens all the\ntime to us as humans.",
    "start": "1142030",
    "end": "1148120"
  },
  {
    "text": "How do you know why\nyou got into Stanford? Well, I don't know. Was it because you colored you\nwon a coloring contest when",
    "start": "1148120",
    "end": "1153309"
  },
  {
    "text": "you were 6, because you\nscored well on the SAT, because you went to\na good high school or you wrote a\nreally good essay?",
    "start": "1153310",
    "end": "1158925"
  },
  {
    "text": "It's really hard\nto understand this. In some cases, it\nmay be impossible. But when we're getting to\nmake repeated decisions,",
    "start": "1158925",
    "end": "1165115"
  },
  {
    "text": "it's really\nimportant that we can start to use the prior\nexperience to figure out which decisions were important\nor led to good outcomes",
    "start": "1165115",
    "end": "1172100"
  },
  {
    "text": "so that we can repeat them. So that's one of the\nreasons why this is hard.",
    "start": "1172100",
    "end": "1177300"
  },
  {
    "text": "Exploration is one of my\nfavorite things in terms of reinforcement learning. And the idea of this is\nthat the agent can only",
    "start": "1177300",
    "end": "1184710"
  },
  {
    "text": "learn about the world\nthrough direct experience. So it's like trying to learn\nto ride a bike by trying",
    "start": "1184710",
    "end": "1190800"
  },
  {
    "text": "and failing and trying\nagain and through that direct experiencing,\nlearning the right way",
    "start": "1190800",
    "end": "1195870"
  },
  {
    "text": "to ride a bike. And the key idea about\nthis is that information",
    "start": "1195870",
    "end": "1202679"
  },
  {
    "text": "is censored in that you only\nget to learn about what you try. So, for example,\nright now, you don't",
    "start": "1202680",
    "end": "1208679"
  },
  {
    "text": "know how much worse your life\nwould be if you were MIT. I went to MIT for grad school. MIT is also a great place.",
    "start": "1208680",
    "end": "1214380"
  },
  {
    "text": "But you generally\ncan't ever understand what that counterfactual\nlife would have been like.",
    "start": "1214380",
    "end": "1220559"
  },
  {
    "text": "It's one of the\ncentral challenges. It's also a huge challenge\nin causal inference, which",
    "start": "1220560",
    "end": "1225660"
  },
  {
    "text": "is another big interest of mine\nand something my lab works on. So there's this\ngeneral challenge that you only get to learn\nabout the actual things",
    "start": "1225660",
    "end": "1232649"
  },
  {
    "text": "that you do as an agent, as a\nhuman, as an agent, et cetera. And so the question\nis, how do you",
    "start": "1232650",
    "end": "1238180"
  },
  {
    "text": "use that experience to figure\nout how to make good decisions? So as a concrete\nexample of this, you can imagine\nyou're a company,",
    "start": "1238180",
    "end": "1244450"
  },
  {
    "text": "and you give some promotion\nto all your customers. You can't know what it\nwould have been like if you didn't give the\npromotion to those customers.",
    "start": "1244450",
    "end": "1251320"
  },
  {
    "text": "And even if you can give it to\none customer and not another, they are not the same people. So I can't rewind\nand say, Dilip,",
    "start": "1251320",
    "end": "1257960"
  },
  {
    "text": "who is our head TA,\nthis time, I'm not going to give you the promotion. Let's see how that world\nwould have worked out.",
    "start": "1257960",
    "end": "1263289"
  },
  {
    "text": "That's one of the\ncentral challenges. So we'll talk a lot\nabout exploration later because it's one of the\nkey things that is different",
    "start": "1263290",
    "end": "1269260"
  },
  {
    "text": "compared to many\nprior approaches. And generalization has\nto do with this question",
    "start": "1269260",
    "end": "1276070"
  },
  {
    "text": "of really wanting\nto solve really big, challenging problems. So we'll talk a lot about\nwhat decision policies are.",
    "start": "1276070",
    "end": "1282039"
  },
  {
    "text": "But in general, you\ncan just think of them as a mapping from\nexperience to a decision. And you might think\nin those cases,",
    "start": "1282040",
    "end": "1288020"
  },
  {
    "text": "you could just preprogram it. So if your robot goes\ndown the hallway, if it hits the end of\nthe hallway, turn left.",
    "start": "1288020",
    "end": "1294159"
  },
  {
    "text": "But let's think\nabout a video game, which we can think\nof as just sort of generally having\nsome input image.",
    "start": "1294160",
    "end": "1299950"
  },
  {
    "text": "So let's imagine that it's\nsomething like 300 by 400. And let's say we have at\nleast 256 different colors.",
    "start": "1299950",
    "end": "1307250"
  },
  {
    "text": "So now we have set\nof images that we could see that is at least\n256 to the 300 cross 400.",
    "start": "1307250",
    "end": "1315520"
  },
  {
    "text": "So those are at least\nthe space of images. That's probably\nan underestimate. And now we get to\nthink about what",
    "start": "1315520",
    "end": "1321220"
  },
  {
    "text": "we would do in each of\nthose different scenarios. So the combinatorics are\ncompletely mind-blowing,",
    "start": "1321220",
    "end": "1327050"
  },
  {
    "text": "and we can't write\nthese down in a table. So this is why we\nwould need something like a deep neural network or\nsomething else in order for us",
    "start": "1327050",
    "end": "1333350"
  },
  {
    "text": "to try to make decisions in\nthese realistic settings which are extremely large in terms\nof the type of scenarios,",
    "start": "1333350",
    "end": "1340860"
  },
  {
    "text": "the number of scenarios we\nwant to make decisions on. So you've probably seen\nall of these ideas,",
    "start": "1340860",
    "end": "1347220"
  },
  {
    "text": "or at least most of\nthem in other classes for other types of AI\nor machine learning.",
    "start": "1347220",
    "end": "1352350"
  },
  {
    "text": "So I think it's useful\njust to contrast what is reinforcement\nlearning doing compared to these other ones.",
    "start": "1352350",
    "end": "1358250"
  },
  {
    "text": "So the first is AI planning. So in AI planning,\ngenerally, we're doing some form of optimization,\ntrying to minimize a distance",
    "start": "1358250",
    "end": "1365480"
  },
  {
    "text": "or something like that. We are often trying to\nhandle delayed consequences.",
    "start": "1365480",
    "end": "1371403"
  },
  {
    "text": "And those are the\ntwo main things. So we might also have\nto do generalization if the size of the\nspace is really large.",
    "start": "1371403",
    "end": "1378450"
  },
  {
    "text": "OK. So that is how-- so RL, in general, will\ninvolve all of these.",
    "start": "1378450",
    "end": "1383950"
  },
  {
    "text": "So this is how\nthose would compare. If we think about something\nlike supervised learning,",
    "start": "1383950",
    "end": "1389090"
  },
  {
    "text": "supervised learning\ndoes involve learning. So we learn from data, whether\nsomething is a cat or not.",
    "start": "1389090",
    "end": "1396160"
  },
  {
    "text": "And we have to do\ngeneralization.",
    "start": "1396160",
    "end": "1401240"
  },
  {
    "text": "So we have those two things. And again, this is\ngoing to be compared to reinforcement learning,\nwhich has all of those.",
    "start": "1401240",
    "end": "1406495"
  },
  {
    "text": " In contrast to\nsupervised learning where you get the correct labels,\nin unsupervised learning,",
    "start": "1406495",
    "end": "1413000"
  },
  {
    "text": "we don't get any labels. But we're still learning\nfrom experience, and we're still trying\nto do generalization. ",
    "start": "1413000",
    "end": "1422300"
  },
  {
    "text": "Now the next\nthing-- and this has become a really popular\nthing-- is to think about whether we can map reinforcement\nlearning to imitation learning.",
    "start": "1422300",
    "end": "1430460"
  },
  {
    "text": "We talked about this really\nbriefly about ChatGPT, and we'll talk about a\nlot more in the course.",
    "start": "1430460",
    "end": "1436850"
  },
  {
    "text": "So in imitation learning or\nbehavioral cloning or reducing reinforcement learning\nto supervised learning,",
    "start": "1436850",
    "end": "1443070"
  },
  {
    "text": "we generally assume that we get\naccess to expert trajectories. So this could be\nlike someone saying",
    "start": "1443070",
    "end": "1448750"
  },
  {
    "text": "what they would do in\nresponse to those prompts. It could be someone\ndriving a car, and then you want to\nmimic their behavior",
    "start": "1448750",
    "end": "1455590"
  },
  {
    "text": "or some other similar example. So these ideas is that we\nget input demonstrations of good policies.",
    "start": "1455590",
    "end": "1461679"
  },
  {
    "text": "And that allows us to reduce\nreinforcement learning back to supervised learning.",
    "start": "1461680",
    "end": "1466720"
  },
  {
    "text": "So we're sort of taking this,\nand we're reducing it back to here. ",
    "start": "1466720",
    "end": "1472400"
  },
  {
    "text": "I think, in general,\nthe idea of reductions is incredibly powerful. For those of you that have\ntaken CS theory classes,",
    "start": "1472400",
    "end": "1478528"
  },
  {
    "text": "that's what we do all the time. We reduce things to set\nor other things like that. And, in general, I think\nin computer science,",
    "start": "1478528",
    "end": "1484350"
  },
  {
    "text": "it's one of the\nstrengths of it that they think of how can we reduce\none problem to another and then inherit all\nthe progress that's",
    "start": "1484350",
    "end": "1489800"
  },
  {
    "text": "been made on that problem. So in this way,\nreinforcement learning is similar to other\naspects of computer science",
    "start": "1489800",
    "end": "1496100"
  },
  {
    "text": "in that we will try often to\nreduce reinforcement learning to other problems.",
    "start": "1496100",
    "end": "1501510"
  },
  {
    "text": "This is particularly done\nin the theoretical aspects of reinforcement learning. Yeah. Yeah.",
    "start": "1501510",
    "end": "1506850"
  },
  {
    "text": "So just before-- Whenever you ask--\njust because I'm going to try and learn names,\ncould you say your name, please? Yeah, my name's [AUDIO OUT].",
    "start": "1506850",
    "end": "1512679"
  },
  {
    "text": "So just to be clear,\nimitation learning then isn't like a separate technique. It's just an application\nof supervised",
    "start": "1512680",
    "end": "1518929"
  },
  {
    "text": "learning to the specific\nreinforcement learning context? It's a good question. So I think some of you--",
    "start": "1518930",
    "end": "1524300"
  },
  {
    "text": "I mean, there's a\nlot of techniques that think about\nwhen you're doing imitation learning specifically\nfor kind of decision data.",
    "start": "1524300",
    "end": "1530010"
  },
  {
    "text": "You can just think of it\njust reducing it back. If you want to do imitation\nlearning where you might recover",
    "start": "1530010",
    "end": "1537003"
  },
  {
    "text": "like the reward function--\nwe'll talk more about that soon and others-- then you may need to use other\ntypes of techniques as well.",
    "start": "1537003",
    "end": "1543290"
  },
  {
    "text": "But the most straightforward\naspect of it is just to say, I've got demonstrations. I'm going to ignore sort of like\nthis delayed consequences aspect",
    "start": "1543290",
    "end": "1552980"
  },
  {
    "text": "and exploration, and I'm\njust going to reduce it back. Yeah. And name first, please.",
    "start": "1552980",
    "end": "1558826"
  },
  {
    "text": "Wait, what do you mean by input\ndemonstrations of good policy? What does that mean? Great question.",
    "start": "1558826",
    "end": "1564110"
  },
  {
    "text": "So let me give you an example. So people have thought\na lot about this. Maybe one of the first\nexamples of this, or one of the first really\npublic examples of this,",
    "start": "1564110",
    "end": "1570545"
  },
  {
    "text": "was for driving. Like at least what you could\ndo is I could drive a car. It could record\neverything that I",
    "start": "1570545",
    "end": "1575830"
  },
  {
    "text": "do in terms of controlling\nthe steering wheel. And then we could-- if I'm a\ngood driver, they could say, that's a good demonstration.",
    "start": "1575830",
    "end": "1581990"
  },
  {
    "text": "So instead of the car trying\nto learn from itself how to steer the wheel in order\nto, say, successfully drive,",
    "start": "1581990",
    "end": "1588290"
  },
  {
    "text": "you could have humans\ndrive it, and it could try to figure\nout at each point how should I steer the wheel\nin order to have good behavior.",
    "start": "1588290",
    "end": "1597639"
  },
  {
    "text": "So the idea is that you\nactually have access already to good demonstrations\nof what is a good policy.",
    "start": "1597640",
    "end": "1604850"
  },
  {
    "text": "Yeah. Name first, please.  What do you exactly mean by\noptimization here [INAUDIBLE]?",
    "start": "1604850",
    "end": "1612200"
  },
  {
    "text": " Optimization and what? What do you mean by\noptimization and imitation?",
    "start": "1612200",
    "end": "1618820"
  },
  {
    "text": "Ah, OK. Good question. So what I mean is that when\nwe do imitation learning from good trajectories,\nwe are assuming",
    "start": "1618820",
    "end": "1625250"
  },
  {
    "text": "that we want to do well. So we want to actually\nget a good policy. So imitation learning,\nnormally, we're",
    "start": "1625250",
    "end": "1630980"
  },
  {
    "text": "not normally trying to\nimitate bad performance, but you could think of this as\nsort of reinforcement learning",
    "start": "1630980",
    "end": "1637200"
  },
  {
    "text": "but without the\nexploration part, because it's not trying\nto pick its own data. Why don't supervised\nlearning and",
    "start": "1637200",
    "end": "1643280"
  },
  {
    "text": "unsupervised have that thing? Have the optimization? Yeah. Yeah.",
    "start": "1643280",
    "end": "1648720"
  },
  {
    "text": "So I think because we\nnormally don't have the notion of utility in those. So you might say this is\na cat or it's not a cat.",
    "start": "1648720",
    "end": "1655809"
  },
  {
    "text": "It's not like a good\npicture of a cat or not. Whereas in decisions, we often\nhave a real valued scalar value",
    "start": "1655810",
    "end": "1661380"
  },
  {
    "text": "of like it was like\na 0.7 good decision. Yeah, name first, please.",
    "start": "1661380",
    "end": "1667050"
  },
  {
    "text": "[INAUDIBLE] also have\nthe loss function, which we intend to optimize.",
    "start": "1667050",
    "end": "1673750"
  },
  {
    "text": "Yes. So we do often-- we always\nhave loss functions. And that's a great--\nbut in those cases,",
    "start": "1673750",
    "end": "1679370"
  },
  {
    "text": "there's not normally\na utility that goes. So if you could get-- you could maybe have\nsome smooth notion there",
    "start": "1679370",
    "end": "1685360"
  },
  {
    "text": "of how well do you match\nlike a stochastic policy, a stochastic output there. But for many of those,\nit would be more--",
    "start": "1685360",
    "end": "1692679"
  },
  {
    "text": "if it's like, did you\nsay it was a cat or not, you would have a\nbinary 0-1 loss.",
    "start": "1692680",
    "end": "1698120"
  },
  {
    "text": "Yeah. Hi. [AUDIO OUT] So does that mean, if you have\nthe data for imitation learning,",
    "start": "1698120",
    "end": "1703410"
  },
  {
    "text": "it's like almost always better\nthan reinforcement learning? Because avoiding the\nfasteners, you can directly",
    "start": "1703410",
    "end": "1709590"
  },
  {
    "text": "learn what is good. Say that again. So if you have the data\nfor imitation learning,",
    "start": "1709590",
    "end": "1716350"
  },
  {
    "text": "if you have someone\nactually driving the car, does that mean that you\nwill probably learn a better",
    "start": "1716350",
    "end": "1722250"
  },
  {
    "text": "policy than\nreinforcement learning or that it's almost\nalways better? Great question. So we'll get into that. So the question\nwas-- if you can hear",
    "start": "1722250",
    "end": "1728070"
  },
  {
    "text": "that-- is if you have\ngood demonstration, say, of driving behavior that\nyou're using imitation learning, can that be better than\nreinforcement learning?",
    "start": "1728070",
    "end": "1734350"
  },
  {
    "text": "It will depend on your\nreinforcement learning algorithm. In general,\nreinforcement learning should always be able\nto equal or exceed",
    "start": "1734350",
    "end": "1741870"
  },
  {
    "text": "the performance of\nimitation learning. Yeah. So can you explain the\ndifference between IL and RLHF?",
    "start": "1741870",
    "end": "1750919"
  },
  {
    "text": "Yes, great question. So in imitation learning,\nwhat we would have-- and this is what-- this\nwas the first part.",
    "start": "1750920",
    "end": "1756980"
  },
  {
    "text": "You would say people give me-- given a prompt, I\nlook on the internet, and I assume that\nthose were good.",
    "start": "1756980",
    "end": "1763190"
  },
  {
    "text": "So in the internet,\nI see if someone said like how to\nexplain reinforcement learning to a six-year-old,\nthis is what they said back.",
    "start": "1763190",
    "end": "1769680"
  },
  {
    "text": "And so I just train on those. What RLHF said is that, well,\nthe internet is a big place.",
    "start": "1769680",
    "end": "1776070"
  },
  {
    "text": "Probably not all of\nit is good answers. So now let's actually ask people\nwhich of these two responses they prefer.",
    "start": "1776070",
    "end": "1781222"
  },
  {
    "text": "And now we're going to\ntry to do reinforcement learning on that to actually\nget to a better policy.",
    "start": "1781223",
    "end": "1786680"
  },
  {
    "text": "Yeah. Something I'd like to\nask-- so AlphaGo actually discovers some Go strategies\nthat are not invented by humans",
    "start": "1786680",
    "end": "1795810"
  },
  {
    "text": "that we have never\nexperienced before. So does it mean that if we apply\nimitation learning too much,",
    "start": "1795810",
    "end": "1801370"
  },
  {
    "text": "it might actually hinder\nthe model's capabilities to explore like what\nis actually good instead of what humans\nhave thought of,",
    "start": "1801370",
    "end": "1807790"
  },
  {
    "text": "which is probably wrong? Absolutely. And actually, I think\nthis is on the next slide. Let's go back. Good. OK, perfect.",
    "start": "1807790",
    "end": "1813070"
  },
  {
    "text": "So this turns as to where\nare some of the places that you might hope that\nreinforcement learning would be better than these\nother strategies.",
    "start": "1813070",
    "end": "1819390"
  },
  {
    "text": "So one of them is\nwhere you don't have examples of desired behavior. So this is exactly\nlike the example that was just brought up.",
    "start": "1819390",
    "end": "1825082"
  },
  {
    "text": "If you want to go beyond\nhuman performance, you cannot rely on human\nperformance just to do imitation learning because you're not\ngoing to be able to get better",
    "start": "1825082",
    "end": "1832890"
  },
  {
    "text": "than it. So there are a lot\nof application areas, I think, particularly in areas\nlike health care or education",
    "start": "1832890",
    "end": "1838890"
  },
  {
    "text": "and others where we think we\ncan go beyond human performance. And so in those\ncases, reinforcement",
    "start": "1838890",
    "end": "1844140"
  },
  {
    "text": "learning because it's trying\nto optimize performance could go beyond. It could be a particularly\nuseful technique.",
    "start": "1844140",
    "end": "1850540"
  },
  {
    "text": "Another is where you don't have\nany existing data for a task. So there might be\nsomething where you think of it as a\ndecision-making problem,",
    "start": "1850540",
    "end": "1857030"
  },
  {
    "text": "but you don't have prior data. And you need to\nlearn from scratch, and you want to\ndirectly optimize. So that's another place\nwhere reinforcement learning",
    "start": "1857030",
    "end": "1863890"
  },
  {
    "text": "can be very powerful. Another category is interesting\nbecause in some ways,",
    "start": "1863890",
    "end": "1869360"
  },
  {
    "text": "it's also kind of a\nreduction technique. And this is the place where\nwe have an enormous search or optimization problem\nwith delayed outcomes.",
    "start": "1869360",
    "end": "1877150"
  },
  {
    "text": "So there's been a\nnumber of examples of the work of doing this\nfrom DeepMind, which have been really extremely elegant.",
    "start": "1877150",
    "end": "1883730"
  },
  {
    "text": "So what I put up\nhere is AlphaTensor. If you haven't heard of\nit, it's a faster way to do matrix multiplication,\nwhich is kind of mind-blowing.",
    "start": "1883730",
    "end": "1891830"
  },
  {
    "text": "So what they did is\nthey said, all right, there's standard ways to\ndo matrix multiplication. This comes up all the time.",
    "start": "1891830",
    "end": "1897169"
  },
  {
    "text": "Could we learn an algorithm\nthat would be better at matrix multiplication? Not me as like a scientist try\nto write down an algorithm.",
    "start": "1897170",
    "end": "1904399"
  },
  {
    "text": "Have an agent learn one. And they showed yes. And the way they did that was\nwith reinforcement learning.",
    "start": "1904400",
    "end": "1911428"
  },
  {
    "text": "And they've done\nthis in other cases, too, like learning faster,\nsorting algorithms. So I think this is a\npretty incredible frontier.",
    "start": "1911428",
    "end": "1918145"
  },
  {
    "text": "The idea is saying,\ncould we have AI actually be inventing new algorithms? And one of the ways that\nthey framed it here--",
    "start": "1918145",
    "end": "1925340"
  },
  {
    "text": "and you can think of\nAlphaGo as similar-- is that it was a really, really,\nreally large search problem.",
    "start": "1925340",
    "end": "1931140"
  },
  {
    "text": "And the challenge with really,\nreally large search problems is that even there, we may\nnot have great techniques",
    "start": "1931140",
    "end": "1936389"
  },
  {
    "text": "for solving them. And so it's sort of a reduction. You can think of people\ntaking a planning problem",
    "start": "1936390",
    "end": "1941700"
  },
  {
    "text": "and trying to reduce it to a\nreinforcement learning problem to make it more tractable. So that's pretty wild.",
    "start": "1941700",
    "end": "1947394"
  },
  {
    "text": "Most of the time we\nthink of sort of RL been reduced in the other\ndirection or involving planning or above that. But here, in some\nways, you can think",
    "start": "1947395",
    "end": "1953880"
  },
  {
    "text": "of these as like either\nadversarial planning problems or Expectimax problems\nthat are being reduced back",
    "start": "1953880",
    "end": "1959490"
  },
  {
    "text": "to learning as a way to\njust more efficiently go through the search space. So those are two\nof the areas that I",
    "start": "1959490",
    "end": "1965670"
  },
  {
    "text": "think are particularly promising\nin terms of why reinforcement learning is still a really\npractical and really important",
    "start": "1965670",
    "end": "1972090"
  },
  {
    "text": "area to think about. I think I saw a question\nback, but maybe-- Yeah, so for--",
    "start": "1972090",
    "end": "1977140"
  },
  {
    "text": "Oh, what was your name? [AUDIO OUT] For AlphaTensor,\nis that like it's fast but within some error of\nthe correct matrix product?",
    "start": "1977140",
    "end": "1983930"
  },
  {
    "text": "It's faster but with some error. Some error? Or do you actually\nget the correct value? No, you get the correct\nvalue, which is wild.",
    "start": "1983930",
    "end": "1989520"
  },
  {
    "text": "Yeah. Yeah. So no, it's just better. Yeah. And one of the\nreally clever things they had to think\nof in this case was how do you know that\nthe answer is correct.",
    "start": "1989520",
    "end": "1996790"
  },
  {
    "text": "How could you\nprovably verify that? So it's incredibly elegant.",
    "start": "1996790",
    "end": "2002360"
  },
  {
    "text": "All right. Now we're going to go quickly\nthrough some course logistics before starting to\ndive into some content.",
    "start": "2002360",
    "end": "2007950"
  },
  {
    "text": "And feel free to interrupt me\nthroughout this or anything else if you have other questions. So in terms of\nthe content, we're",
    "start": "2007950",
    "end": "2015507"
  },
  {
    "text": "going to start off by talking\nabout Markov decision processes and planning. And then we're going to\ntalk about model-free policy",
    "start": "2015508",
    "end": "2022460"
  },
  {
    "text": "evaluation and\nmodel-free control. Don't worry if you don't\nknow what I mean by model. I'll specify it.",
    "start": "2022460",
    "end": "2027570"
  },
  {
    "text": "Then we're going to\njump into policy search. Policy search is things like\nproximal policy optimization",
    "start": "2027570",
    "end": "2035270"
  },
  {
    "text": "and reinforce and\nother approaches. Some of you guys\nmight have already seen related ideas, say, in\nrobotics if you've taken them.",
    "start": "2035270",
    "end": "2043010"
  },
  {
    "text": "And then I'm highlighting\nhere that this is one of the important\ndifferences compared to prior years.",
    "start": "2043010",
    "end": "2048090"
  },
  {
    "text": "So we're going to do a deep\ndive into offline reinforcement",
    "start": "2048090",
    "end": "2053210"
  },
  {
    "text": "learning, offline\nhere meaning that we have a fixed amount of data. And we want to learn from it\nto get a good decision policy.",
    "start": "2053210",
    "end": "2060230"
  },
  {
    "text": "And during this, we're going\nto talk about reinforcement learning from human feedback and\ndirect preference optimization.",
    "start": "2060230",
    "end": "2066109"
  },
  {
    "text": "So that's going to be a new\nthird part of the course that we haven't done\nassignments on before.",
    "start": "2066110",
    "end": "2073399"
  },
  {
    "text": "So I think that'll\nbe pretty exciting. And we'll also talk\nabout exploration and do advanced topics. ",
    "start": "2073400",
    "end": "2081719"
  },
  {
    "text": "So the high-level learning\ngoals of the class is that by the end\nof the class, you should be able to define the\nkey features of reinforcement",
    "start": "2081719",
    "end": "2086790"
  },
  {
    "text": "learning. You should be able to\ngiven an application, specify how you would write that\ndown as a reinforcement learning",
    "start": "2086790",
    "end": "2093480"
  },
  {
    "text": "problem, as well\nas whether or not you think it would be\ngood to use RL for it, that you can implement in\ncode common RL algorithms,",
    "start": "2093480",
    "end": "2100890"
  },
  {
    "text": "and that you understand the\ntheoretical and empirical approaches for evaluating the\nquality of an RL algorithm.",
    "start": "2100890",
    "end": "2106799"
  },
  {
    "text": "So as you could probably imagine\nfrom those papers going up, there's going to be continued\nprogress in this field,",
    "start": "2106800",
    "end": "2112280"
  },
  {
    "text": "and there's going to be a\nhuge number of different RL algorithms. And so one of the key things\nthat I hope to talk about",
    "start": "2112280",
    "end": "2117315"
  },
  {
    "text": "is sort of how do you\nevaluate and compare them, which might vary depending\non the application area you care about.",
    "start": "2117315",
    "end": "2124760"
  },
  {
    "text": "So the way that the\ncourse is structured is that we'll have\nlive lectures. We'll have three homeworks. We'll have a midterm.",
    "start": "2124760",
    "end": "2130740"
  },
  {
    "text": "We'll have a\nmultiple choice quiz. We'll do a final project. And then we'll have what\nI call check or refresh",
    "start": "2130740",
    "end": "2137690"
  },
  {
    "text": "your understanding exercises,\nwhich will be going through the Poll Everywhere.",
    "start": "2137690",
    "end": "2142849"
  },
  {
    "text": "And we'll have problem\nsessions which are optional. Problem sessions\nare a great chance",
    "start": "2142850",
    "end": "2148100"
  },
  {
    "text": "to think more about\nthe conceptual and the theoretical\naspects of the class. And they'll be held\nstarting next week.",
    "start": "2148100",
    "end": "2154119"
  },
  {
    "text": " So one of the main application\nareas I think about a lot",
    "start": "2154120",
    "end": "2160050"
  },
  {
    "text": "is education. I think education is\none of the greatest tools we have to try to\naddress poverty and inequality.",
    "start": "2160050",
    "end": "2166720"
  },
  {
    "text": "And so I'm really interested\nin evidence to think about how do we educate effectively.",
    "start": "2166720",
    "end": "2172000"
  },
  {
    "text": "So with respect\nto that, I wanted to share this paper\nthat came out, I guess, almost\na decade ago now,",
    "start": "2172000",
    "end": "2177400"
  },
  {
    "text": "where they did a\nstudy to look at how people who are taking\nmassive open online courses, how they spent their\ntime and how that related",
    "start": "2177400",
    "end": "2184710"
  },
  {
    "text": "to their learning outcomes. And what they found is that\nif you do more activities, there seem to be a six times\nlarger learning benefit compared",
    "start": "2184710",
    "end": "2192420"
  },
  {
    "text": "to watching videos or reading. And you might think this is just\nbased on time, but it wasn't.",
    "start": "2192420",
    "end": "2198460"
  },
  {
    "text": "In fact, it seemed like students\nspent less time per activity than reading a page. And I bring this up\nbecause sometimes I",
    "start": "2198460",
    "end": "2204960"
  },
  {
    "text": "have people who come talk to\nme right before the midterm. And they say, I rewatched your\nlectures like three times. What else can I do?",
    "start": "2204960",
    "end": "2210950"
  },
  {
    "text": "And while I'm\nflattered that they want to watch the\nlectures three times, I really highly recommend\nyou don't do that,",
    "start": "2210950",
    "end": "2217010"
  },
  {
    "text": "that, instead, you spend\ntime doing problems or going through problems\nfrom the sessions,",
    "start": "2217010",
    "end": "2222800"
  },
  {
    "text": "going through the homework,\ngoing through the check your understandings. It's far more effective\nand efficient, in general.",
    "start": "2222800",
    "end": "2228520"
  },
  {
    "text": "So in general, engage practice,\nparticularly forced recall, where you have to sort of think\nabout things without checking",
    "start": "2228520",
    "end": "2236110"
  },
  {
    "text": "the answers, is shown to be\nvery effective for learning. And so to achieve the\nclass learning goals,",
    "start": "2236110",
    "end": "2241484"
  },
  {
    "text": "I encourage you to spend\nas much time as you can or the time you have available\nfor the course on those type",
    "start": "2241485",
    "end": "2247810"
  },
  {
    "text": "of sort of directly\nengaging activities rather than more passive ones\nlike reading or watching.",
    "start": "2247810",
    "end": "2254300"
  },
  {
    "text": "Yeah. [INAUDIBLE] Name first. [AUDIO OUT] Do you have a time frame for\nwhen the problem sessions will",
    "start": "2254300",
    "end": "2263060"
  },
  {
    "text": "be held? Great question. We will announce those\nby the end of tomorrow. For those ones that we know it's\nlike impossible to coordinate",
    "start": "2263060",
    "end": "2270190"
  },
  {
    "text": "schedules-- so if\nyou can't make it, we encourage you\nto come in person. But if you can't\nmake it, we also release all the materials\nand the videos afterwards.",
    "start": "2270190",
    "end": "2277127"
  },
  {
    "text": " OK.",
    "start": "2277127",
    "end": "2282360"
  },
  {
    "text": "I will highlight-- I guess\njust also on this too-- and I saw several people\nasking about this. Well, we'll just go back to this\npart because [INAUDIBLE] cover.",
    "start": "2282360",
    "end": "2292260"
  },
  {
    "text": "So several people\nmentioned that they were excited about having\nsome more theoretical aspects.",
    "start": "2292260",
    "end": "2297280"
  },
  {
    "text": "This class does involve theory. It is perhaps-- there's\nprobably more theory,",
    "start": "2297280",
    "end": "2303160"
  },
  {
    "text": "I think, probably than the\nnormal machine learning and AI classes, probably\na little bit more, and not as much as like an\nadvanced seminar on theory.",
    "start": "2303160",
    "end": "2310810"
  },
  {
    "text": "So normally, most problem sets\nwill have one-theory question. And if you're not\nfamiliar with some",
    "start": "2310810",
    "end": "2317339"
  },
  {
    "text": "of the sort of theoretical\ntechniques, totally fine. You can come to\nproblem sessions.",
    "start": "2317340",
    "end": "2322450"
  },
  {
    "text": "You don't have to have any\nprior background in doing proofs to be able to succeed.",
    "start": "2322450",
    "end": "2327510"
  },
  {
    "text": "Another thing people asked about\nwere Monte Carlo Tree Search. Several people brought\nup reinforcement",
    "start": "2327510",
    "end": "2332820"
  },
  {
    "text": "learning from human feedback. We will be talking about that. Some people asked\nabout multi-agents. We're going to be thinking\nabout Monte Carlo Tree",
    "start": "2332820",
    "end": "2339760"
  },
  {
    "text": "Search and other ways to\nhave multiple agents that are making decisions. And a number of people\nsaid they wanted",
    "start": "2339760",
    "end": "2345160"
  },
  {
    "text": "to get up to speed\non sort of the latest ideas and reinforcement learning\nso they could read papers or do things in their applications.",
    "start": "2345160",
    "end": "2351290"
  },
  {
    "text": "And I think this is all\nvery relevant to that.  The final thing is just we\nhave five wonderful TAs who",
    "start": "2351290",
    "end": "2359430"
  },
  {
    "text": "will be supporting. The main ways to get\ninformation about the class is to go to the\nwebsite or go to Ed.",
    "start": "2359430",
    "end": "2365160"
  },
  {
    "text": "We'll be releasing our office\nhours by the end of tomorrow. And we'll start them for\nthe rest of the week. And all of you\nguys are completely",
    "start": "2365160",
    "end": "2372240"
  },
  {
    "text": "capable of succeeding in the\ncourse, and we're here to help. Yeah.",
    "start": "2372240",
    "end": "2377530"
  },
  {
    "text": "[AUDIO OUT] Yeah. Please. Go back to the\ncourse topic slide. Do some of those topics include\nmodel-based approaches as well?",
    "start": "2377530",
    "end": "2384920"
  },
  {
    "text": "Yeah. So the first part--\ngreat question. So when we first start\ntalking about-- here,",
    "start": "2384920",
    "end": "2390030"
  },
  {
    "text": "we'll talk about models at\nthe beginning and particularly when we're defining\nMarkov decision processes.",
    "start": "2390030",
    "end": "2395160"
  },
  {
    "text": "And then we will\nlikely be talking again about that more when we get\ninto the offline approach.",
    "start": "2395160",
    "end": "2400400"
  },
  {
    "text": "There's a lot of very\ninteresting questions about when we're picking\ndifferent-- there's a lot-- we'll get into\nthe fact that there's",
    "start": "2400400",
    "end": "2405740"
  },
  {
    "text": "a lot of different\nrepresentations you can use for\nreinforcement learning. And there's a lot of\nquestions over which to use",
    "start": "2405740",
    "end": "2410990"
  },
  {
    "text": "when or when you combine them. And in particular,\nwhere do errors propagate in the different\ntypes of representations",
    "start": "2410990",
    "end": "2416810"
  },
  {
    "text": "in terms of leading to error in\nthe final decisions you make? But model-based\nreinforcement learning",
    "start": "2416810",
    "end": "2422870"
  },
  {
    "text": "can certainly be a\nreally powerful tool. Any other questions\non the logistics?",
    "start": "2422870",
    "end": "2427980"
  },
  {
    "text": " All right. So let's start to dive\ninto the material.",
    "start": "2427980",
    "end": "2433965"
  },
  {
    "text": " All right. We're going to start with\na refresher exercise. So raise your hand if\nyou've seen reinforcement",
    "start": "2433965",
    "end": "2441430"
  },
  {
    "text": "learning at least a\nlittle bit in the past. So most people, not all. If you haven't, if\neverything I am about to say",
    "start": "2441430",
    "end": "2448177"
  },
  {
    "text": "doesn't make sense, don't worry. We're going to cover it. But I like to kind of get a\ngauge in case people are like,",
    "start": "2448177",
    "end": "2454040"
  },
  {
    "text": "I've seen all of this before\nfor the very beginning of the course. So this is going to be\na refresher exercise.",
    "start": "2454040",
    "end": "2459050"
  },
  {
    "text": "We're going to do it on Ed. I'll put the link up\nagain, or you can go to Ed. It'll be the second link. So here's the question.",
    "start": "2459050",
    "end": "2464770"
  },
  {
    "text": "We're going to think\nabout how would we formulate a particular problem\nas a reinforcement learning problem or as a Markov\ndecision process.",
    "start": "2464770",
    "end": "2471850"
  },
  {
    "text": "So one of the first\napplication areas to use reinforcement\nlearning for education used in roughly\nthe following way.",
    "start": "2471850",
    "end": "2479470"
  },
  {
    "text": "Not exactly. The idea was that you\nwould have a student that didn't know a set of topics. Let's here just consider\naddition, which we'll assume",
    "start": "2479470",
    "end": "2486730"
  },
  {
    "text": "is an easier topic\nfor people to learn, and subtraction, which we're\ngoing to assume is harder.",
    "start": "2486730",
    "end": "2491757"
  },
  {
    "text": "Imagine that maybe\nthe student doesn't know either of these things. And what the AI\ntutor agent can do",
    "start": "2491757",
    "end": "2497119"
  },
  {
    "text": "is they can provide\npractice problems. They can provide subtraction\npractice problems,",
    "start": "2497120",
    "end": "2502650"
  },
  {
    "text": "or they can provide\naddition practice problems. And what happens is the AI\nagent gets a reward of plus 1",
    "start": "2502650",
    "end": "2509012"
  },
  {
    "text": "if the agent-- if the student\ngets the problem right. And they get a minus 1, if the\nstudent gets the problem wrong.",
    "start": "2509012",
    "end": "2515057"
  },
  {
    "text": "And so what I'd like\nyou to think about here is to model it as\na decision process. What would like the state space\nbe, the action space, the reward",
    "start": "2515057",
    "end": "2522590"
  },
  {
    "text": "model? If you've taken classes with\nMarkov decision processes before and you don't remember,\nit's totally fine",
    "start": "2522590",
    "end": "2527878"
  },
  {
    "text": "to look up and\nrefresh your memory. This is not a test. I'd like you to write down\nsort of what would a dynamics",
    "start": "2527878",
    "end": "2533780"
  },
  {
    "text": "model represent in this case. And then in\nparticular, what would a policy to optimize\nthe expected discounted",
    "start": "2533780",
    "end": "2540560"
  },
  {
    "text": "sum of rewards do\nin this case for how I've set up this scenario?",
    "start": "2540560",
    "end": "2545780"
  },
  {
    "text": "So I'd like you to\nwrite down your answers, enter them into\nEd, and then we're going to do some small group\ndiscussion in about 5 minutes.",
    "start": "2545780",
    "end": "2552750"
  },
  {
    "start": "2552750",
    "end": "2629330"
  },
  {
    "text": "And if you're not familiar\nwith these particular words like state space,\net cetera, it's still fine just to\nthink about, given",
    "start": "2629330",
    "end": "2635030"
  },
  {
    "text": "what I've told you about the\nreward for an agent, what might happen in this case? ",
    "start": "2635030",
    "end": "2717680"
  },
  {
    "text": "So [INAUDIBLE] only\ngives the first question. Ah, OK. Sorry. You might have to\nswitch to the Ed.",
    "start": "2717680",
    "end": "2724187"
  },
  {
    "text": "OK. ",
    "start": "2724187",
    "end": "2832440"
  },
  {
    "text": "All right. Try to enter in something. It's OK if you're not sure. And then turn to someone near\nyou and compare what you did.",
    "start": "2832440",
    "end": "2838119"
  },
  {
    "text": " [INTERPOSING VOICES]",
    "start": "2838120",
    "end": "2845790"
  },
  {
    "start": "2845790",
    "end": "3014210"
  },
  {
    "text": "All right. We're going to come back. Hopefully, I heard a lot of\nreally fruitful discussions.",
    "start": "3014210",
    "end": "3019480"
  },
  {
    "text": "So let's see. I know at least\none group I talked to had a great idea for what\nthe state space could be.",
    "start": "3019480",
    "end": "3027393"
  },
  {
    "text": "Do you guys want to share\nwhat your state space was? And maybe tell\nyour name as well. Sure. ",
    "start": "3027393",
    "end": "3034720"
  },
  {
    "text": "You said the state space can be\njust like a set of word pairs of like two natural numbers\nor any kind of numbers",
    "start": "3034720",
    "end": "3040809"
  },
  {
    "text": "of like how good the student\nis at addition than how good the student is at subtraction.",
    "start": "3040810",
    "end": "3047180"
  },
  {
    "text": "Yeah. So you could imagine\nsomething which is at addition and subtraction.",
    "start": "3047180",
    "end": "3052888"
  },
  {
    "text": "So you could imagine something\nlike this where you just have a vector pair where\nit's like maybe they're",
    "start": "3052888",
    "end": "3058030"
  },
  {
    "text": "0.9 close to mastery for\naddition and like 0.4 close to mastery\nfor subtraction.",
    "start": "3058030",
    "end": "3064010"
  },
  {
    "text": "This is not the only way\nyou could write down. There's lots of choices\nfor the state space, but that would certainly\nbe one reasonable one.",
    "start": "3064010",
    "end": "3069609"
  },
  {
    "text": "Those are challenging\nin some ways because you can't\ndirectly observe them, but it's a pretty natural\nway to write it down.",
    "start": "3069610",
    "end": "3076220"
  },
  {
    "text": "And in fact, there\nare commercial systems that essentially do that\nwhere they have like-- for those of you familiar\nwith hidden Markov models,",
    "start": "3076220",
    "end": "3082450"
  },
  {
    "text": "it's basically a\nhidden Markov model over whether someone has\nmastered something or not. Don't we have a different\ntype of state space",
    "start": "3082450",
    "end": "3088240"
  },
  {
    "text": "that they wrote down? Yeah. [INAUDIBLE] talked about-- we\nbasically wanted to [INAUDIBLE].",
    "start": "3088240",
    "end": "3094539"
  },
  {
    "text": "Oh, could you say your\nname first, please? The knowledge that the student\nhas and also maybe the questions",
    "start": "3094540",
    "end": "3099550"
  },
  {
    "text": "that have already been asked\nto capture the environment, the current environment\nthat we're at. So I guess this is a better\nrepresentation of capturing",
    "start": "3099550",
    "end": "3107430"
  },
  {
    "text": "the knowledge the student has. We were thinking of also just-- like the history of questions\nand students' answers,",
    "start": "3107430",
    "end": "3112910"
  },
  {
    "text": "whether they got\nit right or not. I guess that's harder to\nrepresent on [INAUDIBLE]. No, that's beautiful.",
    "start": "3112910",
    "end": "3117920"
  },
  {
    "text": "So exactly what\n[AUDIO OUT] [INAUDIBLE]. So that was the other\none I was hoping people might come up with,\nwhich is the idea of this just",
    "start": "3117920",
    "end": "3123819"
  },
  {
    "text": "being a history, like a history\nof all the previous questions you've given or all the\nquestions the robot has",
    "start": "3123820",
    "end": "3129070"
  },
  {
    "text": "given the person and\nwhat they've responded. So you could imagine it's\nlike a observation question",
    "start": "3129070",
    "end": "3138690"
  },
  {
    "text": "reward dot, dot, dot. And in fact, those\ntwo representations",
    "start": "3138690",
    "end": "3144000"
  },
  {
    "text": "here, the history\nand how the student-- how good the student,\ncan be, depending on your representation,\nbe exactly isomorphic.",
    "start": "3144000",
    "end": "3150360"
  },
  {
    "text": "So sometimes this can be\na sufficient statistic to capture that history. And as [AUDIO OUT]\nwas pointing out,",
    "start": "3150360",
    "end": "3155890"
  },
  {
    "text": "one of the challenges\nwith histories is that they grow unboundedly. So if you want to have your\nneural network be predicting",
    "start": "3155890",
    "end": "3161790"
  },
  {
    "text": "something, you might be able\nto use something like an LSTM, or you might want to\nsummarize the state. So those are both great ideas\nfor what the states could be.",
    "start": "3161790",
    "end": "3169105"
  },
  {
    "text": "There's not a right answer. Both of them would be great. But there's also other ones. The actions I heard many people\nshare what the actions are.",
    "start": "3169105",
    "end": "3174910"
  },
  {
    "text": "Someone want to tell me\nwhat they are in there? I know you guys mentioned\nwhat the action space was. Sure. Just whether you pose in\naddition or subtraction.",
    "start": "3174910",
    "end": "3182320"
  },
  {
    "text": "Exactly. So these are just like\nwhat the agent can actually do, the teaching agent, addition\nquestion or subtraction.",
    "start": "3182320",
    "end": "3190320"
  },
  {
    "text": "And the reward model is plus 1\nif the student gets it right. ",
    "start": "3190320",
    "end": "3197310"
  },
  {
    "text": "I saw some questions about\nwhat a dynamics model is. And inside of the responses,\npeople are putting on the form.",
    "start": "3197310",
    "end": "3206090"
  },
  {
    "text": "What I mean by a\ndynamics model here-- and we'll talk a lot\nmore about this-- is what happens to the\nstate of the student",
    "start": "3206090",
    "end": "3212290"
  },
  {
    "text": "after a question is given. So in this case-- and\nI talked to some people",
    "start": "3212290",
    "end": "3217720"
  },
  {
    "text": "about this who had a great\nunderstanding of this already. The idea would be sort\nof, how does either that history change after you\ngive a question to the student,",
    "start": "3217720",
    "end": "3226720"
  },
  {
    "text": "or how does this the sort\nof internal knowledge of the student change? So the hope would be as long\nas this sort of curriculum",
    "start": "3226720",
    "end": "3233170"
  },
  {
    "text": "is vaguely reasonable, that\nafter you give the student an addition question, they\nnow know more about addition,",
    "start": "3233170",
    "end": "3239559"
  },
  {
    "text": "or they're more likely to\nhave mastered addition. So that would be sort of this\nidea of there being a dynamics",
    "start": "3239560",
    "end": "3245349"
  },
  {
    "text": "process that where you start in\none state, you get an action, and now you transition to\na new state afterwards.",
    "start": "3245350",
    "end": "3251890"
  },
  {
    "text": "And we'll talk a\nlot more about that. Now, what is the challenge with\nthis particular representation?",
    "start": "3251890",
    "end": "3257945"
  },
  {
    "text": "Yeah. And can you say your\nname first, please? Depending on your\nimplementation, there's a risk that the agent\njust gives really easy problems.",
    "start": "3257945",
    "end": "3267299"
  },
  {
    "text": "Yeah. In fact, [AUDIO OUT]\nexactly right. And in fact, that's exactly\nwhat we think will happen. So we think that an agent\nthat is maximizing its reward",
    "start": "3267300",
    "end": "3279580"
  },
  {
    "text": "should only give easy questions.",
    "start": "3279580",
    "end": "3285820"
  },
  {
    "text": "So in this paper, which\nI took the inspiration from for this example, it\nwas very close to this,",
    "start": "3285820",
    "end": "3291225"
  },
  {
    "text": "where they tried to\npick not correctness, but how long it took\npeople to do problems. And so if the student took\nless time to do problems--",
    "start": "3291225",
    "end": "3299080"
  },
  {
    "text": "which isn't necessarily\nbad in itself. It might indicate some\nnotion of fluency-- the agent got more reward.",
    "start": "3299080",
    "end": "3304550"
  },
  {
    "text": "But, of course, what that\nmeans is that you should just give really easy questions that\nwill take the student no time to do because then the\nagent can get lots and lots",
    "start": "3304550",
    "end": "3310583"
  },
  {
    "text": "and lots of reward. And this is probably not what\nthe intent-- like the designers",
    "start": "3310583",
    "end": "3316599"
  },
  {
    "text": "of this system to try to help\nstudents learn things intended. They probably actually\nwanted the students",
    "start": "3316600",
    "end": "3322060"
  },
  {
    "text": "to learn both addition\nand subtraction. But I bring this up because this\nis an example of what is often",
    "start": "3322060",
    "end": "3327730"
  },
  {
    "text": "called reward hacking, where\nthe reward that we specify does not necessarily provide\nthe behavior that we really",
    "start": "3327730",
    "end": "3335109"
  },
  {
    "text": "hope to achieve. And we will talk a\nlot more about this. In this case, it's a\nfairly simple example",
    "start": "3335110",
    "end": "3340940"
  },
  {
    "text": "where we can see\nit fairly quickly, but there are a\nlot of cases where it's a lot more subtle to\nunderstand whether or not",
    "start": "3340940",
    "end": "3346460"
  },
  {
    "text": "the system really will do\nwhat you hope it will do. And we'll talk about that\nmore throughout the course.",
    "start": "3346460",
    "end": "3353660"
  },
  {
    "text": "All right, great. So we're going to now\njust start to talk about sort of sequential\ndecision-making more broadly. And some of this will be\nreview for some of you,",
    "start": "3353660",
    "end": "3360180"
  },
  {
    "text": "but I think it's useful\nto go through and refresh our memories. So the idea in sequential\ndecision-making",
    "start": "3360180",
    "end": "3365930"
  },
  {
    "text": "under uncertainty\nis that we're going to have an agent that is\ntaking decisions or actions.",
    "start": "3365930",
    "end": "3371880"
  },
  {
    "text": "So I'm going to use actions\nand decisions interchangeably, which are going to\ninteract in the world.",
    "start": "3371880",
    "end": "3378802"
  },
  {
    "text": "And then they're going to get\nback some sort of observation and reward signal. So in the first example\nI just gave you,",
    "start": "3378802",
    "end": "3385170"
  },
  {
    "text": "it's like the agent provides\na problem to the student. And then they see\nwhether the student gets",
    "start": "3385170",
    "end": "3390800"
  },
  {
    "text": "that correctly or incorrect. And then they also use that\ninformation to get a reward. So it's giving\nreward and feedback.",
    "start": "3390800",
    "end": "3398300"
  },
  {
    "text": "And the goal in this\ncase is for the agent to select actions to maximize\nthe total expected future",
    "start": "3398300",
    "end": "3403430"
  },
  {
    "text": "reward, meaning both the\nimmediate reward they get now, as well as the rewards they're\ngoing to get over time.",
    "start": "3403430",
    "end": "3409190"
  },
  {
    "text": "And this generally\nis often going to involve balancing long-term\nand short-term rewards. So there are lots\nand lots of examples.",
    "start": "3409190",
    "end": "3415980"
  },
  {
    "text": "I'll just go through a couple of\nthem just to give you a sense. So one is something\nlike web advertising.",
    "start": "3415980",
    "end": "3421760"
  },
  {
    "text": "In this case,\nAmazon, for example, might choose like a web ad\nto show you or a product",
    "start": "3421760",
    "end": "3427280"
  },
  {
    "text": "to suggest to you. They might observe\nthings like view time and whether or not\nyou click on the ad,",
    "start": "3427280",
    "end": "3432300"
  },
  {
    "text": "whether or not you\nmake a purchase. And the goal in this\ncase could probably be for them to optimize\neither click time or view",
    "start": "3432300",
    "end": "3438349"
  },
  {
    "text": "time or revenue. In the context of\nsomething like robotics,",
    "start": "3438350",
    "end": "3443820"
  },
  {
    "text": "the control space or\nthe decision space might be something like\nhow to move a joint.",
    "start": "3443820",
    "end": "3449510"
  },
  {
    "text": "And then the feedback that\nthe robot might get back might be something like a\ncamera image of a kitchen. And perhaps they\njust get a plus 1",
    "start": "3449510",
    "end": "3455960"
  },
  {
    "text": "if there are no more\ndishes on the counter. Now, just a quick question,\ncould this potentially",
    "start": "3455960",
    "end": "3461000"
  },
  {
    "text": "be a reward-hacked\nspecification? I see some smiles. What could happen?",
    "start": "3461000",
    "end": "3467520"
  },
  {
    "text": "Yeah. [INAUDIBLE] Oh, sorry. Robot could just push\neverything off the counter.",
    "start": "3467520",
    "end": "3473300"
  },
  {
    "text": "Which I will say with-- it's tempting, right? Like, I'm just going\nto make it all go away.",
    "start": "3473300",
    "end": "3478410"
  },
  {
    "text": "But in fact, this does\nnot solve the problem. And now you just have broken\ndishes and food on the floor. So that would not be\na good thing to do.",
    "start": "3478410",
    "end": "3484020"
  },
  {
    "text": "So yeah, this would be probably\nnot a great reward to put. You probably want a reward\nmore like that the dishes are inside of the dishwasher\nand finally clean.",
    "start": "3484020",
    "end": "3490993"
  },
  {
    "text": "So not just that they\nwere put in there, but actually that you\nran the dishwasher. So this would be a second\nexample of a setting.",
    "start": "3490993",
    "end": "3498290"
  },
  {
    "text": "Another would be something like\nblood pressure control, where you could imagine that the\nagent gives recommendations",
    "start": "3498290",
    "end": "3504170"
  },
  {
    "text": "like exercise or medication. The feedback is things\nlike blood pressure. And then you would define\nsome reward like maybe",
    "start": "3504170",
    "end": "3510710"
  },
  {
    "text": "plus 1 if you're in a\nhealthy range, else some sort of sloping penalty for being\noutside of the healthy range.",
    "start": "3510710",
    "end": "3516145"
  },
  {
    "text": " All right. So all of these\nare nice examples",
    "start": "3516145",
    "end": "3521295"
  },
  {
    "text": "of the numerous\nways where we often try to make sequences of\ndecisions under uncertainty. In general, we're going\nto assume that we have",
    "start": "3521295",
    "end": "3528519"
  },
  {
    "text": "a finite series of time steps. So we're not going to be\nthinking about continuous time in this class.",
    "start": "3528520",
    "end": "3533575"
  },
  {
    "text": "Lots of interesting\nthings there. We're not going to cover it. What we're going to assume\nis that the agent is making a series of decisions.",
    "start": "3533575",
    "end": "3539140"
  },
  {
    "text": "So we're going to think of there\nbeing a series of time steps like 1 minute, 2 minutes,\n3 minute, 4 minute.",
    "start": "3539140",
    "end": "3544759"
  },
  {
    "text": "The agent will take an action. The world will update\ngiven that action and emit an observation and a reward.",
    "start": "3544760",
    "end": "3550430"
  },
  {
    "text": "And then the agent\nreceives that, updates, and then makes another decision. We just close this loop.",
    "start": "3550430",
    "end": "3555589"
  },
  {
    "text": "It's a feedback cycle. In this case, as we sort of just\ntalked about at a high level,",
    "start": "3555590",
    "end": "3561650"
  },
  {
    "text": "we can think of there\nbeing histories, which is sequences of\npast actions, rewards, and observations up to\nthe present time point.",
    "start": "3561650",
    "end": "3570010"
  },
  {
    "text": "So the history,\nht, would consist of all the previous\nactions of the agent, the observations it receives,\nand the reward it's got.",
    "start": "3570010",
    "end": "3578200"
  },
  {
    "text": "In general, this is something\nyou could use to make decisions. You could just keep\ntrack of everything you've experienced\nso far and then",
    "start": "3578200",
    "end": "3584740"
  },
  {
    "text": "condition on that to try\nto make your next decision. But we often are going\nto assume that there's",
    "start": "3584740",
    "end": "3590109"
  },
  {
    "text": "some sort of\nsufficient statistic that we can use to\nsummarize the history. It will be much more\npractical in many cases.",
    "start": "3590110",
    "end": "3597279"
  },
  {
    "text": "Yeah. Oh, sorry. Just [INAUDIBLE] observation\nis basically like the history, like a previous history\nof the [INAUDIBLE]",
    "start": "3597280",
    "end": "3603795"
  },
  {
    "text": "And what's your name? [AUDIO OUT] So the observation\nin this case would be something like the immediate\ninformation you get back",
    "start": "3603795",
    "end": "3610720"
  },
  {
    "text": "after the last action. So in the case of\nthe student, it would have been whether they get\nthe last problem correct or not.",
    "start": "3610720",
    "end": "3616118"
  },
  {
    "text": "So just like a single time step. And then the history\nwould be everything like up to this time point.",
    "start": "3616118",
    "end": "3621722"
  },
  {
    "text": "Good question.  So in particular, often\nto make things tractable",
    "start": "3621722",
    "end": "3627560"
  },
  {
    "text": "and because often, in reality,\nit's not a terrible assumption, we're going to normally\nmake the Markov assumption.",
    "start": "3627560",
    "end": "3633380"
  },
  {
    "text": "And the idea is that\nwe're going to try to come up with some sort\nof informative information state that is a sufficient\nstatistic of the history.",
    "start": "3633380",
    "end": "3640430"
  },
  {
    "text": "So we don't have\nto keep around all of the prior history\nof everything the agent's ever done or\nseen or gotten a reward for.",
    "start": "3640430",
    "end": "3647619"
  },
  {
    "text": "And what we say is a\nstate, St, is Markov if and only if the probability of\ngoing to the next state given",
    "start": "3647620",
    "end": "3655060"
  },
  {
    "text": "the current state in action is\nthe same as if you conditioned on the whole entire history.",
    "start": "3655060",
    "end": "3661420"
  },
  {
    "text": "So another way to say\nthis, which I think is kind of a nice\nevocative idea-- this is not from me,\nthis is from others--",
    "start": "3661420",
    "end": "3667540"
  },
  {
    "text": "is that the future is\nindependent of the past, given the present. That means if you have\na rich representation",
    "start": "3667540",
    "end": "3674458"
  },
  {
    "text": "of your current\nstate, you don't have to think about the\nprevious history. And, of course, in\ngeneral, this will be true",
    "start": "3674458",
    "end": "3681610"
  },
  {
    "text": "if you make S equal to ht. But in general, we're\ngoing to be thinking often",
    "start": "3681610",
    "end": "3687310"
  },
  {
    "text": "of sort of projecting down to\na much smaller state space. So for example, you\nmight say, well, I could think about someone's\nblood pressure from all of time,",
    "start": "3687310",
    "end": "3694490"
  },
  {
    "text": "but maybe it's sufficient just\nto think of their blood pressure over the last like two hours in\norder to make my next decision.",
    "start": "3694490",
    "end": "3700690"
  },
  {
    "text": "Yeah. Uh-huh. Is there a difference\nbetween state and observation",
    "start": "3700690",
    "end": "3707520"
  },
  {
    "text": "in this case? Great question. Yes, in general. So I'll give you a\nparticular example. Atari, which is these\nvideo games that DeepMind",
    "start": "3707520",
    "end": "3715230"
  },
  {
    "text": "learned an agent to play,\nwhat their state in that case was the last four frames.",
    "start": "3715230",
    "end": "3720359"
  },
  {
    "text": "So not just the last frame,\nthe last four frames. Does anybody have any\nidea why you might want four frames instead of one?",
    "start": "3720360",
    "end": "3725370"
  },
  {
    "text": "Yeah. Maybe like-- so you can\nsee if there's momentum to an object already moving.",
    "start": "3725370",
    "end": "3731290"
  },
  {
    "text": "Exactly. It gives you velocity\nand acceleration. Yeah. So there are a\nnumber of cases where you might think that there are\nparts of the state that really",
    "start": "3731290",
    "end": "3737880"
  },
  {
    "text": "depend on temporal differences. And then in those\ncases, you're going to want more than just\nthe immediate state.",
    "start": "3737880",
    "end": "3743170"
  },
  {
    "text": "Great. Great questions. All right. So why is this popular? It's used all the time.",
    "start": "3743170",
    "end": "3750059"
  },
  {
    "text": "It's simple. It can often be satisfied. As we were just discussing if\nyou use some history as part of the state.",
    "start": "3750060",
    "end": "3755759"
  },
  {
    "text": "Generally, there are many\ncases where you can just use the most recent state. Not always, but many cases.",
    "start": "3755760",
    "end": "3761680"
  },
  {
    "text": "And it has huge implications\nfor computational complexity data required in the\nresulting performance.",
    "start": "3761680",
    "end": "3767392"
  },
  {
    "text": "What I mean by the\nresulting performance is is that in many\nof these cases, just like in a lot of\nstatistics and machine learning,",
    "start": "3767393",
    "end": "3773450"
  },
  {
    "text": "there will be trade-offs\nbetween bias and variance. And so there'll be a trade-off\nbetween using states that",
    "start": "3773450",
    "end": "3778960"
  },
  {
    "text": "are really small and\neasy for us to work with but aren't really\nable to capture the complexity of the\nworld and the applications",
    "start": "3778960",
    "end": "3785230"
  },
  {
    "text": "we care about so\nthat it might be fast to learn with those\nsort of representations. But ultimately,\nperformance is poor.",
    "start": "3785230",
    "end": "3791770"
  },
  {
    "text": "So there will\noften be trade-offs with how we actually--\nthe expressive power of our representations\nversus how long it",
    "start": "3791770",
    "end": "3797410"
  },
  {
    "text": "takes us to learn.  Right. So one of the big\nquestions when we",
    "start": "3797410",
    "end": "3803408"
  },
  {
    "text": "talk about sequential\ndecision-making processes is, is the state Markov? And is the world\npartially observable?",
    "start": "3803408",
    "end": "3811010"
  },
  {
    "text": "So partial-- oh, yeah.  My question is that, doesn't\nthe Markov assumption",
    "start": "3811010",
    "end": "3818410"
  },
  {
    "text": "make this reward attribution\nproblem somehow harder? All right.",
    "start": "3818410",
    "end": "3824180"
  },
  {
    "text": "Good question. Well, I don't know. I guess you could imagine it\nmight make it easier or harder. There's still the\nquestion of you",
    "start": "3824180",
    "end": "3830695"
  },
  {
    "text": "might only get periodic rewards. And you still would\nhave to figure out which decisions caused\nyou to get to a state",
    "start": "3830695",
    "end": "3836590"
  },
  {
    "text": "where you got those rewards. [INAUDIBLE] Yeah. So let me think of\nit-- so you might",
    "start": "3836590",
    "end": "3842100"
  },
  {
    "text": "have a case where\nthe reward might be a function of\nyour current state.",
    "start": "3842100",
    "end": "3847190"
  },
  {
    "text": "Yeah. Let me think if I can't\nthink of a good example. OK, so let's say maybe you\nwant to run a marathon.",
    "start": "3847190",
    "end": "3857300"
  },
  {
    "text": "And you get a plus\n100 if you make it. Boston Marathon is a competitive\nmarathon to get into, so you get a plus 100 if\nyou can qualify for Boston.",
    "start": "3857300",
    "end": "3865028"
  },
  {
    "text": "And you do a lot of different\nthings in your training regime. You eat healthy, and you\nsleep, and you train.",
    "start": "3865028",
    "end": "3871610"
  },
  {
    "text": "And you get zero\nreward for any of that. And then on the\nday of your race, you see if you\nqualify for Boston.",
    "start": "3871610",
    "end": "3877760"
  },
  {
    "text": "So your state only-- like your reward for\ngetting into Boston only depends on\nthat current state.",
    "start": "3877760",
    "end": "3883440"
  },
  {
    "text": "But you don't know which\nof those decisions. Was it that you ate well? Was it that you slept? Was it that you\ntrained every week",
    "start": "3883440",
    "end": "3888637"
  },
  {
    "text": "for 17 weeks caused you to\nget to the state in which you qualified for Boston?",
    "start": "3888637",
    "end": "3894080"
  },
  {
    "text": "And so that's independent of the\nMarkov assumption in that case, because you still have\nthe question of what",
    "start": "3894080",
    "end": "3899190"
  },
  {
    "text": "series of decisions allowed\nyou to get to a state that achieved high reward. Great question.",
    "start": "3899190",
    "end": "3906360"
  },
  {
    "text": "So another thing is where the\nworld is partially observable. We will mostly not be talking\nabout this in this class. Mykel Kochenderfer\nhas a great class",
    "start": "3906360",
    "end": "3913230"
  },
  {
    "text": "where he talks about\nthis a lot, but this does relate to the case we\ntalked about with students.",
    "start": "3913230",
    "end": "3918360"
  },
  {
    "text": "So for students,\none way you could think about that is that\nthere's some latent state that you can't\ndirectly access, which",
    "start": "3918360",
    "end": "3923940"
  },
  {
    "text": "is whether or not\nthey know addition or they know subtraction. But you get noisy\nobservations when they do problems where they\nget it right or get it wrong.",
    "start": "3923940",
    "end": "3931680"
  },
  {
    "text": "And the reason it's noisy\nis because all of us make mistakes on\naddition sometimes, whereas I have complete faith\nthat everyone here actually",
    "start": "3931680",
    "end": "3938075"
  },
  {
    "text": "knows how to do addition. And sometimes you might guess\nright even if you don't know it. So the idea is that it's latent.",
    "start": "3938075",
    "end": "3943599"
  },
  {
    "text": "You don't directly\nget to observe it. This comes up in a lot of\nrobotics problems, too,",
    "start": "3943600",
    "end": "3949390"
  },
  {
    "text": "so I'll just give a\nquick example here. If you have a robot that uses a\nlaser rangefinder to figure out",
    "start": "3949390",
    "end": "3955980"
  },
  {
    "text": "these little arrows or lasers to\nfigure out its environment-- so",
    "start": "3955980",
    "end": "3962310"
  },
  {
    "text": "it could have 180 degrees\nof laser rangefinders. And what it's\ngetting back is just the distance in all of these\ndifferent angles to where",
    "start": "3962310",
    "end": "3968490"
  },
  {
    "text": "it hits a wall. So as you can imagine, many\nrooms would look identical. So any room that has like\nkind of the same dimensions",
    "start": "3968490",
    "end": "3975480"
  },
  {
    "text": "would look identical\nto that robot. And it wouldn't be\nable to tell is it on the third floor\nor the second floor.",
    "start": "3975480",
    "end": "3980640"
  },
  {
    "text": "So that would be a\npartially observable case where it can't uniquely\nidentify its state based on its observations.",
    "start": "3980640",
    "end": "3986640"
  },
  {
    "text": "So we won't talk\ntoo much about that, but it's important\nto know about. Another thing is\nwhether the dynamics are deterministic or stochastic.",
    "start": "3986640",
    "end": "3993060"
  },
  {
    "text": "So there are many\ncases where things are close to deterministic. Like, if I put down a piece\non a Go board, it goes there.",
    "start": "3993060",
    "end": "4001400"
  },
  {
    "text": "But there are other things that\nwe often treat as stochastic. Like, when I flip\na coin, I don't know whether it's going\nto be heads or tails.",
    "start": "4001400",
    "end": "4007609"
  },
  {
    "text": "So that will be an\nimportant decision. And then the final thing\nis whether the actions influence only immediate\nreward or reward in next state.",
    "start": "4007610",
    "end": "4017640"
  },
  {
    "text": "So as an example\nof this, you might imagine if you were\nmaking a policy for what ad to show to people.",
    "start": "4017640",
    "end": "4022770"
  },
  {
    "text": "And you just imagine for each\nperson coming onto the web, you just show them-- onto your website, you show\nan ad, and then they go away.",
    "start": "4022770",
    "end": "4029039"
  },
  {
    "text": "And they either buy\nsomething, or they don't. A bandit would be a case where\nyou just have-- bless you.",
    "start": "4029040",
    "end": "4034900"
  },
  {
    "text": "You have a series of\ncustomers coming in. And so whether or not\nI show a particular ad",
    "start": "4034900",
    "end": "4041609"
  },
  {
    "text": "and he clicks on it or does\nnot impact whether or not Ellen comes along and likes an ad.",
    "start": "4041610",
    "end": "4047549"
  },
  {
    "text": "So that's a case where it\nimpacts your immediate reward, but not the next state. We can talk more about that.",
    "start": "4047550",
    "end": "4054530"
  },
  {
    "text": "All right. Let's think about a particular\nsort of running example. We'll think of a Mars rover. So Mars rover is a\nMarkov decision process.",
    "start": "4054530",
    "end": "4061440"
  },
  {
    "text": "Imagine that Mars\nis really small. We only have seven\nplaces in Mars. So in this case, we\nwould have the state",
    "start": "4061440",
    "end": "4068040"
  },
  {
    "text": "is the location of the\nRover, which is one of seven discrete locations. We could have actions\ncalled try left",
    "start": "4068040",
    "end": "4074520"
  },
  {
    "text": "and try right, meaning that\nour Rover is not perfect. So sometimes it tries\nto go a direction, and it doesn't succeed.",
    "start": "4074520",
    "end": "4080650"
  },
  {
    "text": "And let's imagine\nthat we have rewards, which is that there's some\ninteresting field sites. And so if you spend time\nover here, you get a plus 1.",
    "start": "4080650",
    "end": "4088390"
  },
  {
    "text": "And you have spent over\nhere, you get a plus 10. And else, you get zero reward. So this would be\na particular case",
    "start": "4088390",
    "end": "4094349"
  },
  {
    "text": "where we could think of\nthere being these states and these actions and rewards. ",
    "start": "4094350",
    "end": "4101990"
  },
  {
    "text": "So when we think of a\nMarkov decision process, we think of there being a\ndynamics and a reward model.",
    "start": "4101990",
    "end": "4108890"
  },
  {
    "text": "So in particular,\nthe dynamics model is going to tell us\nhow the state evolves",
    "start": "4108890",
    "end": "4115068"
  },
  {
    "text": "as we make decisions. We will not always have\ndirect access to this, but the idea is\nthat in the world,",
    "start": "4115069",
    "end": "4120589"
  },
  {
    "text": "there is some dynamics\nprocess and things are changing as we make decisions. So in particular,\nwe generally want",
    "start": "4120590",
    "end": "4128028"
  },
  {
    "text": "to allow for stochastic\nsystems, meaning that given we're currently in a state\nand we take a particular action,",
    "start": "4128029",
    "end": "4133949"
  },
  {
    "text": "what is the distribution over\nnext states that we might reach? So for example, I'm\nthat Mars rover,",
    "start": "4133950",
    "end": "4139580"
  },
  {
    "text": "and I'm going to try\nto go to the right. It might be that I can go to\nthe right with 50% probability,",
    "start": "4139580",
    "end": "4146818"
  },
  {
    "text": "but I'm not a very\naccurate Rover. And so 50% of the\ntime I go to the left, or maybe I stay in\nthe same location.",
    "start": "4146819",
    "end": "4153500"
  },
  {
    "text": "So this dynamics model just\nspecifies what actually the distribution of outcomes\nthat can happen in the world",
    "start": "4153500",
    "end": "4158509"
  },
  {
    "text": "when I make a decision. The reward model predicts\nthe immediate reward,",
    "start": "4158510",
    "end": "4163720"
  },
  {
    "text": "which is if I'm in this\nstate and I take this action, what is my expected reward? I want to highlight here that\nthere are different conventions.",
    "start": "4163720",
    "end": "4171939"
  },
  {
    "text": "You could have the\nreward be a function only of the current state-- excuse me. It could be a function of the\nstate and the action you take,",
    "start": "4171939",
    "end": "4179049"
  },
  {
    "text": "or it could be a function of\nthe state, the action you take, and the next state you reach. You'll see all of these\nconventions in reinforcement",
    "start": "4179050",
    "end": "4186528"
  },
  {
    "text": "learning papers. Probably the most\ncommon one is this. But we'll try just to be\nspecific whenever we're",
    "start": "4186529",
    "end": "4192170"
  },
  {
    "text": "using it so that it's clear. And you can always ask\nme or ask any of the TAs if it's not clear.",
    "start": "4192170",
    "end": "4198430"
  },
  {
    "text": "Bless you. So let's think about sort of\nwhat a stochastic Mars rover model would be. So I've written down\na particular choice",
    "start": "4198430",
    "end": "4206760"
  },
  {
    "text": "for the reward. And let's imagine that\npart of the dynamics model is the following, which\nis if I start in state S1",
    "start": "4206760",
    "end": "4214740"
  },
  {
    "text": "and I try to go\nto the right, then I have some probability\nof going to S2,",
    "start": "4214740",
    "end": "4220320"
  },
  {
    "text": "else I have some\nprobability of staying here. What I want to be\nclear about here--",
    "start": "4220320",
    "end": "4226330"
  },
  {
    "text": "and this relates to the\nquestion before about models-- is that this is like the agent's\nidea of how the world works.",
    "start": "4226330",
    "end": "4233590"
  },
  {
    "text": "It doesn't have to be how\nthe world actually works. So what I told you in\nthe previous slides is that imagine in this\nworld, in reality, this",
    "start": "4233590",
    "end": "4242190"
  },
  {
    "text": "gives you plus 1,\nand this gives you plus 10 in terms of the reward. That's how the world\nactually works.",
    "start": "4242190",
    "end": "4247600"
  },
  {
    "text": "But the agent might\nhave the wrong model of how the world works because\nit only learns about the world through its experiences, or it\njust might have a bad model.",
    "start": "4247600",
    "end": "4255840"
  },
  {
    "text": "So this is an example of sort\nof like a model-based Markov system where the agent would\nhave a particular representation",
    "start": "4255840",
    "end": "4263500"
  },
  {
    "text": "of the dynamics model and a\nparticular assumption over how the rewards work. ",
    "start": "4263500",
    "end": "4271030"
  },
  {
    "text": "In these settings,\nwe have a policy. A decision policy is just going\nto be a mapping from states to actions.",
    "start": "4271030",
    "end": "4277449"
  },
  {
    "text": "It's like an if then table. If it's deterministic, we\njust have a single action",
    "start": "4277450",
    "end": "4282850"
  },
  {
    "text": "that we would take in\na particular state. Like, maybe we always show this\none ad to a particular customer.",
    "start": "4282850",
    "end": "4289659"
  },
  {
    "text": "Or we could have a stochastic\npolicy where we randomize this. So this would be something like,\noh, when this customer shows up,",
    "start": "4289660",
    "end": "4296150"
  },
  {
    "text": "I show a vacation ad,\nor I show a board game ad with 90%\nprobability versus 10%.",
    "start": "4296150",
    "end": "4303430"
  },
  {
    "text": "Both types of policies\nare really common, and it can depend in part\nwhat sort of domain you're in and whether you're trying to\nlearn from that experience.",
    "start": "4303430",
    "end": "4312730"
  },
  {
    "text": "OK, so let's see what that\nwould look like in this case. So for Mars rover, you could\nsay that no matter where it is,",
    "start": "4312730",
    "end": "4319070"
  },
  {
    "text": "it always just\ntries to go right. ",
    "start": "4319070",
    "end": "4324080"
  },
  {
    "text": "So that would just\nbe one example of a policy you could have. And it just requires you to\nspecify for every single state",
    "start": "4324080",
    "end": "4330380"
  },
  {
    "text": "what is the action\nyou would take or what is the distribution\nover actions you would take. ",
    "start": "4330380",
    "end": "4337350"
  },
  {
    "text": "So in this sort\nof setting, we're normally interested in\ntwo main-- oh, yeah. Yeah, question. So it's like making decisions\nbased on the state that it's in.",
    "start": "4337350",
    "end": "4346290"
  },
  {
    "text": "And it learn to switch from\ndifferent types of policies. So not just different actions\nfor some of the state,",
    "start": "4346290",
    "end": "4353680"
  },
  {
    "text": "but also switch to checking the\npast state, the future state. In the same way like\nin deep learning,",
    "start": "4353680",
    "end": "4359130"
  },
  {
    "text": "it tries a bunch of\ndifferent functions. Can it do that, or\ncan it not do that? Great. And remind me your name.",
    "start": "4359130",
    "end": "4365858"
  },
  {
    "text": "[AUDIO OUT] Yeah. So great question. It will in general. In general, when\nwe're learning, it",
    "start": "4365858",
    "end": "4371250"
  },
  {
    "text": "will change its policy\na lot over time. So it might start with\na particular policy. And then over time,\nit will explore",
    "start": "4371250",
    "end": "4376668"
  },
  {
    "text": "lots of different policies\nin trying to search for something that's good. That's a great\nquestion, and that relates to what I was\njust putting here,",
    "start": "4376668",
    "end": "4383380"
  },
  {
    "text": "which is two of the\ncentral questions we're going to talk a lot about,\nparticularly at the beginning, is evaluation and control.",
    "start": "4383380",
    "end": "4389340"
  },
  {
    "text": "Evaluation says someone\ngives you a fixed policy. And you want to\nknow how good it is. Like, maybe your\nboss says, hey, I",
    "start": "4389340",
    "end": "4395850"
  },
  {
    "text": "think this is the right way\nto advertise to customers, and we're going to\nmake a lot of money. And you go out, and you just\ndeploy that particular decision",
    "start": "4395850",
    "end": "4402490"
  },
  {
    "text": "policy. And you see how\nmuch money you make. So that would be evaluation. Control is you actually want\nto find the best policy.",
    "start": "4402490",
    "end": "4409560"
  },
  {
    "text": "And so in general, to\nactually find the best policy, we're going to have to do\na lot of trial and error.",
    "start": "4409560",
    "end": "4414700"
  },
  {
    "text": "And we want to do that in\na strategic, efficient way so we can quickly learn\nwhat that good policy is. ",
    "start": "4414700",
    "end": "4422140"
  },
  {
    "text": "So in general, we're going\nto be talking about things. I just want to highlight we're\ngoing to sort of build up in complexity in terms\nof the type of problems",
    "start": "4422140",
    "end": "4428170"
  },
  {
    "text": "we're talking about. So we're going to be thinking\nabout both like planning and control and sort\nof thinking about how",
    "start": "4428170",
    "end": "4437500"
  },
  {
    "text": "complicated these spaces are. So we're going to think\nabout evaluation and control",
    "start": "4437500",
    "end": "4445360"
  },
  {
    "text": "because evaluation is often\na subpart of doing control. If you know how\ngood a policy is, you may be able to improve it.",
    "start": "4445360",
    "end": "4451750"
  },
  {
    "text": "And then we're going to\ntalk about tabular function approximation\nmethods, because we're",
    "start": "4451750",
    "end": "4457119"
  },
  {
    "text": "going to want to be able to\nsolve really large problems. And then we're going to\ntalk about both planning",
    "start": "4457120",
    "end": "4465130"
  },
  {
    "text": "and learning. In planning, we're going\nto assume someone gives us that dynamics model and that\nreward model and the state",
    "start": "4465130",
    "end": "4473050"
  },
  {
    "text": "and action space. And we're just going to try\nto find a really good policy. And in learning, we're\ngoing to actually have to control the decisions\nwe make to give us",
    "start": "4473050",
    "end": "4479920"
  },
  {
    "text": "information that allows us to\nidentify an optimal policy. ",
    "start": "4479920",
    "end": "4485590"
  },
  {
    "text": "All right. So what we're\ngoing to start with is sort of the simplest of\nthe settings, which we're going to assume that we\nhave a finite set of states",
    "start": "4485590",
    "end": "4491795"
  },
  {
    "text": "and actions. We're given models of the\nworld, meaning someone writes down for us\nwhat those look like.",
    "start": "4491795",
    "end": "4497050"
  },
  {
    "text": "And we want to evaluate the\nperformance of the best decision policy and then compute\nthe optimal policy.",
    "start": "4497050",
    "end": "4503420"
  },
  {
    "text": "And we can think of this\nreally as AI planning. OK. So to think about\nhow this works,",
    "start": "4503420",
    "end": "4509920"
  },
  {
    "text": "we're going to start\nwith Markov processes and then build up to MDPs. And this is relevant\nbecause it turns out",
    "start": "4509920",
    "end": "4515290"
  },
  {
    "text": "you can think of evaluation as\nbasically being a Markov reward process.",
    "start": "4515290",
    "end": "4520989"
  },
  {
    "text": "OK, so how does a\nMarkov chain work? And just raise\nyour hand if you've seen Markov chains before. Awesome.",
    "start": "4520990",
    "end": "4526130"
  },
  {
    "text": "OK, so most people\nhave, which is great. So this is a memoryless\nrandom process. There's no rewards yet.",
    "start": "4526130",
    "end": "4531940"
  },
  {
    "text": "There's a finite set\nof states in this case. And we have a dynamics model. And if it's just a\nfinite set of states,",
    "start": "4531940",
    "end": "4537770"
  },
  {
    "text": "we can just write\nthis down as a matrix.  Just says, what's\nthe probability",
    "start": "4537770",
    "end": "4542810"
  },
  {
    "text": "of going to the next state\ngiven the previous state? And so you could just\nhave this say in our-- this would be a Markov\nchain transition",
    "start": "4542810",
    "end": "4549560"
  },
  {
    "text": "matrix for our Mars rover case. And if you wanted to get an\nepisode, you would just sample.",
    "start": "4549560",
    "end": "4556590"
  },
  {
    "text": "So let's say you always\ntouch down in state S4. You just sample episodes\nfrom that particular chain. Yeah.",
    "start": "4556590",
    "end": "4562250"
  },
  {
    "text": "[INAUDIBLE] rows and\ncolumns down to 1? All of the-- and\nwhat's your name?",
    "start": "4562250",
    "end": "4568489"
  },
  {
    "text": "Yeah, so all of the\nrows have to sum to 1.",
    "start": "4568490",
    "end": "4574032"
  },
  {
    "text": "OK.  Then is it coincidence\nthat columns sum to 1?",
    "start": "4574032",
    "end": "4579630"
  },
  {
    "text": "Yeah. OK. Yeah. I was thinking just now\nthat I should have changed that question because--\nand we'll see also",
    "start": "4579630",
    "end": "4586010"
  },
  {
    "text": "why that's important later. OK, In a Markov\nreward process, it's a Markov chain plus rewards.",
    "start": "4586010",
    "end": "4591890"
  },
  {
    "text": "So same as before. But now we have a reward\nfunction that tells us how good each of\nthose states are.",
    "start": "4591890",
    "end": "4597920"
  },
  {
    "text": "And we're also going to\nhave a discount factor. And I'll talk about\nthat in a second. We still have no actions.",
    "start": "4597920",
    "end": "4604520"
  },
  {
    "text": "And we can express\nR as a vector. So in this, we could imagine\nour Markov reward process",
    "start": "4604520",
    "end": "4611540"
  },
  {
    "text": "where we have a plus\n1 and S1, 10 in S7. So plus 1 [INAUDIBLE] and\n0 in all other states.",
    "start": "4611540",
    "end": "4618639"
  },
  {
    "text": " In this case, this\nis where we start to see the ideas that are going\nto be really useful for decision",
    "start": "4618640",
    "end": "4625180"
  },
  {
    "text": "processes, which is\nwe can start to think about how good particular\ntrajectories are. So we're going to\nhave a horizon,",
    "start": "4625180",
    "end": "4630815"
  },
  {
    "text": "and you're going to see\nthis in your homework 2, which is the number of\ntime steps in each episode. It could be infinite,\nor it could be finite.",
    "start": "4630815",
    "end": "4637805"
  },
  {
    "text": "It's like basically\nhow many time steps do you get to make decisions. And the return, which\nwe're going to call Gt,",
    "start": "4637805",
    "end": "4645040"
  },
  {
    "text": "is just going to be the\ndiscounted sum of rewards from the time step,\ncurrent time step, till the end of the horizon.",
    "start": "4645040",
    "end": "4651670"
  },
  {
    "text": "And a value function\nin this case is just going to be\nthe expected return.",
    "start": "4651670",
    "end": "4656770"
  },
  {
    "text": "In general, this is not going to\nbe the same as the actual return unless you just have a\ndeterministic process,",
    "start": "4656770",
    "end": "4662712"
  },
  {
    "text": "because the idea is that you're\ngoing to have stochasticity in the trajectories you reach. And because of that, you're\ngoing to get different rewards.",
    "start": "4662712",
    "end": "4669045"
  },
  {
    "text": " Right. So you might wonder-- if you haven't\nseen it before, why",
    "start": "4669045",
    "end": "4675340"
  },
  {
    "text": "do we have this\ndiscount factor thing? So we're sort of weighing\nearlier rewards more than later rewards.",
    "start": "4675340",
    "end": "4681280"
  },
  {
    "text": "Well, one is that it's\njust mathematically really convenient. It's going to help us not\nsum to infinity, particularly if we have infinite\nnumber of time steps",
    "start": "4681280",
    "end": "4688590"
  },
  {
    "text": "we can make decisions. And it turns out\nhumans often act as if there is a discount factor.",
    "start": "4688590",
    "end": "4694200"
  },
  {
    "text": "Like, often, we\nsort of implicitly weigh future rewards less\nthan immediate rewards.",
    "start": "4694200",
    "end": "4700130"
  },
  {
    "text": "And this is true for\norganizations too. And if the episode\nlengths are always finite,",
    "start": "4700130",
    "end": "4705510"
  },
  {
    "text": "you can always-- bless\nyou-- use gamma equal 1, meaning you don't have\nto make a large discount.",
    "start": "4705510",
    "end": "4711380"
  },
  {
    "text": "But when you have\ninfinite horizons, it's generally important\nto make this less than 1 so your rewards don't blow up.",
    "start": "4711380",
    "end": "4717153"
  },
  {
    "text": "Part of that is because\nit's really hard to compare infinities,\nso it's hard to say that this policy that\nhas infinite reward",
    "start": "4717153",
    "end": "4723678"
  },
  {
    "text": "is better than this\nother policy that has infinite reward, whereas\nyou can keep everything bounded if you have\na gamma less than 1.",
    "start": "4723678",
    "end": "4730489"
  },
  {
    "text": "All right. Next time, we will start to\ntalk about how we actually can compute the value of these\ntypes of Markov reward processes",
    "start": "4730490",
    "end": "4737870"
  },
  {
    "text": "and then start to connect\nit to decision processes. I'll see you on Wednesday. Thanks. ",
    "start": "4737870",
    "end": "4748000"
  }
]