[
  {
    "start": "0",
    "end": "5490"
  },
  {
    "text": "Hello, everyone. Thanks for coming. Today, we'll be\ntalking about the multi-task and goal-conditioned\nreinforcement learning.",
    "start": "5490",
    "end": "13360"
  },
  {
    "text": "But before that,\na few reminders. Homework 2 is due today.",
    "start": "13360",
    "end": "19320"
  },
  {
    "text": "Homework 3 is out. It will be a more\nlightweight homework so that you can spend\na little bit more time",
    "start": "19320",
    "end": "26070"
  },
  {
    "text": "on the-- start spending a little\nbit more time on the projects. But you should start\nwith it ASAP, especially",
    "start": "26070",
    "end": "32879"
  },
  {
    "text": "with setting up all the\ndependencies and so on to make sure that everything works. And if it doesn't, please\nput something on Ed,",
    "start": "32880",
    "end": "39810"
  },
  {
    "text": "and we'll try to help you. And secondly, there is a\nmid-quarter survey today.",
    "start": "39810",
    "end": "45700"
  },
  {
    "text": "And we really appreciate\nyour feedback, so please take the\ntime to fill it out.",
    "start": "45700",
    "end": "50765"
  },
  {
    "text": "We've actually\nincorporated a lot of the changes in the course\nbecause of that survey, things such as switching to PyTorch\nfrom TensorFlow before,",
    "start": "50765",
    "end": "58470"
  },
  {
    "text": "adjusting the lecture\ntopics, the homeworks, introducing homework\n0 and so on. So we really take it seriously.",
    "start": "58470",
    "end": "64180"
  },
  {
    "text": "So please, please fill it out. And also, thank you very\nmuch for the high resolution",
    "start": "64180",
    "end": "69300"
  },
  {
    "text": "feedback that some of you\nhave been submitting so far. It has been very useful to us.",
    "start": "69300",
    "end": "74670"
  },
  {
    "text": "And we take it really\nseriously so thank you. One other point is that\noffice hours can be actually",
    "start": "74670",
    "end": "84270"
  },
  {
    "text": "either in person or virtual. And there's been a few\nmisunderstandings where somebody thought that it was\nin person, and it wasn't.",
    "start": "84270",
    "end": "91229"
  },
  {
    "text": "So please check the calendar. The calendar should\nbe able to tell you whether it's in person or\nvirtual and follow that.",
    "start": "91230",
    "end": "98980"
  },
  {
    "text": "All right. So the plan for\ntoday's lecture is first we'll do a little bit\nof a recap from last week,",
    "start": "98980",
    "end": "106140"
  },
  {
    "text": "and we'll finish the\nQ learning aspect of reinforcement learning, which\nwe didn't get to talk about.",
    "start": "106140",
    "end": "112680"
  },
  {
    "text": "And then we'll dive into the\nmulti-task aspect of things. So we'll talk about\nmulti-task imitation learning and multi-task policy gradients,\nmulti-task Q learning and data",
    "start": "112680",
    "end": "121799"
  },
  {
    "text": "sharing and then finally about\ngoal-conditioned reinforcement learning, which is also\nthe topic of your homework.",
    "start": "121800",
    "end": "129429"
  },
  {
    "text": "All right. So let's start\nwith a short recap. So last week, we talked about\nthe anatomy of a reinforcement",
    "start": "129430",
    "end": "138030"
  },
  {
    "text": "learning algorithm. And we talked about\nthese three boxes. So there is the box where\nwe generate the samples",
    "start": "138030",
    "end": "143160"
  },
  {
    "text": "when we run the policy. There is a box where\nwe try to estimate the return of the policy, so\nwhat's the sum of the rewards",
    "start": "143160",
    "end": "149057"
  },
  {
    "text": "that the policy is going to get. And we have a few\ndifferent options here. We talked about the\nMonte Carlo approach,",
    "start": "149057",
    "end": "155250"
  },
  {
    "text": "where we just sum the\nrewards that we achieved. We talked a little bit\nabout fitting a Q function,",
    "start": "155250",
    "end": "161340"
  },
  {
    "text": "or the advantage\nfunction, that allows us to do this a little bit better. And we haven't talked\nabout the model-based part.",
    "start": "161340",
    "end": "168390"
  },
  {
    "text": "Maybe we'll talk about it next\nweek or on Wednesday actually.",
    "start": "168390",
    "end": "173890"
  },
  {
    "text": "And then, there is the last box\nwhere we improve the policy. And the only part that\nwe've talked so far about",
    "start": "173890",
    "end": "179370"
  },
  {
    "text": "is the policy gradient\napproach, where we take the gradient with\nrespect to our objective and we do gradient ascent\nprocedure to improve",
    "start": "179370",
    "end": "184980"
  },
  {
    "text": "our policy. And today, we'll briefly talk\nabout Q learning part as well. And then next week\ntoo, we'll talk",
    "start": "184980",
    "end": "191400"
  },
  {
    "text": "about the model-based part.  And then we introduced\nthis equation,",
    "start": "191400",
    "end": "198160"
  },
  {
    "text": "which expresses the gradient\nwith respect to our objective. And this is the gradient that\nwe use in the policy gradient",
    "start": "198160",
    "end": "203939"
  },
  {
    "text": "approach. And if we were to\nparse this equation, we have the part\nthat corresponds to the green box, which tells us\nwhat is the return of the given",
    "start": "203940",
    "end": "213209"
  },
  {
    "text": "policy. And this is just the sum\nof the rewards starting at the current action and the\ncurrent state up until the end.",
    "start": "213210",
    "end": "219360"
  },
  {
    "text": "These are the actions and\nstate that we have actually experienced. Then we have this\npart that corresponds",
    "start": "219360",
    "end": "224850"
  },
  {
    "text": "to the orange box, where\nwe do multiple roll-outs and we take the average. And then, this whole\npart all together",
    "start": "224850",
    "end": "231357"
  },
  {
    "text": "is how we estimate the gradient\nwith respect to our objective. And then we use that gradient\nin our policy gradient",
    "start": "231357",
    "end": "239070"
  },
  {
    "text": "procedure in gradient ascent to\nactually improve on our policy. All right.",
    "start": "239070",
    "end": "245470"
  },
  {
    "text": "Cool. Then, we talked a little bit\nabout multi-step prediction, about how in standard\npolicy gradient",
    "start": "245470",
    "end": "251610"
  },
  {
    "text": "you would just sum the\nrewards from now up until the end of the trajectory. We had this example\nfrom a movie,",
    "start": "251610",
    "end": "258180"
  },
  {
    "text": "where what you could do\ninstead is if you had an option to teleport to the state\nover and over again",
    "start": "258180",
    "end": "264990"
  },
  {
    "text": "and live multiple\nfutures, you would be able to estimate the sum\nof the rewards much better.",
    "start": "264990",
    "end": "271810"
  },
  {
    "text": "So this is a\ndifferent way that you can do this, which is the true\nexpectation of the rewards",
    "start": "271810",
    "end": "277380"
  },
  {
    "text": "given that you start in-- the sum of the rewards given\nthat you start in a given state and perform a certain action.",
    "start": "277380",
    "end": "283020"
  },
  {
    "text": "And then we talked about\nthe multi-step prediction where you, for\ninstance, play chess.",
    "start": "283020",
    "end": "288330"
  },
  {
    "text": "And when you play\nchess, you don't really have to wait until\nthe end of the chess match to update\nyour predictions.",
    "start": "288330",
    "end": "295420"
  },
  {
    "text": "So you don't have to wait\nuntil you finish the game. You can actually start updating\nit as soon as you make a move.",
    "start": "295420",
    "end": "300760"
  },
  {
    "text": "So if you made a\nmove and you feel like you did something\nwrong, you can straightaway update your prediction.",
    "start": "300760",
    "end": "307707"
  },
  {
    "text": "And then we talked about\nhow we can actually use this in the so-called\nbootstrap estimate.",
    "start": "307707",
    "end": "313620"
  },
  {
    "text": "And the way we do\nthis is the following. We try to fit a value\nfunction, and we did this",
    "start": "313620",
    "end": "319230"
  },
  {
    "text": "using supervised regression. So we have our value\nfunction, which is a neural network\nparameterized with some parameters phi\nthat takes state as an input",
    "start": "319230",
    "end": "326940"
  },
  {
    "text": "and outputs a scalar value. And then we need to fit it\nbased on some kind of targets.",
    "start": "326940",
    "end": "334837"
  },
  {
    "text": "So this is our value function\nthat takes states as the input, and output is just\na single scalar that tells us how\ngood is our policy.",
    "start": "334838",
    "end": "340560"
  },
  {
    "text": "What's the sum of the rewards\nfrom now till the end?  So one target we could have\nis the ideal target, which",
    "start": "340560",
    "end": "348840"
  },
  {
    "text": "is the expected sum of-- the\ntrue expectation, the expected sum of rewards under\nour current policy",
    "start": "348840",
    "end": "354030"
  },
  {
    "text": "given that we start\nin a certain state. And we can approximate\nit by taking the current reward\nplus the value",
    "start": "354030",
    "end": "360270"
  },
  {
    "text": "function of the next state. And we can approximate\nit even more",
    "start": "360270",
    "end": "365580"
  },
  {
    "text": "whereby, rather than taking\nthe true value function, which we don't have access\nto, we'll just take our current\nestimate of the value",
    "start": "365580",
    "end": "371213"
  },
  {
    "text": "function of the next state. And this is what we refer to\nas the bootstrap estimate.",
    "start": "371213",
    "end": "377169"
  },
  {
    "text": "So this is what allows us\nto make these predictions and update those predictions\nas soon as we make a step. We don't have to wait until\nthe end of the trajectory.",
    "start": "377170",
    "end": "385970"
  },
  {
    "text": "All right. So we talked a little\nbit about that. And then, we introduced\ntwo algorithms. We introduced the\nreinforce algorithm,",
    "start": "385970",
    "end": "391417"
  },
  {
    "text": "where we just sample a\nbunch of trajectories then apply our policy\ngradient formula to estimate",
    "start": "391417",
    "end": "396855"
  },
  {
    "text": "the gradient with respect\nto our objective and then the gradient ascent. ",
    "start": "396855",
    "end": "402550"
  },
  {
    "text": "And then, at the\nvery end, we talked about the online actor-critic\nalgorithm, where we also do a bunch of different roll-outs.",
    "start": "402550",
    "end": "408400"
  },
  {
    "text": "We save this kind of tuple\nstate action, next state,",
    "start": "408400",
    "end": "413620"
  },
  {
    "text": "and the reward. Then we update\nour value function according to this bootstrap\nestimate that we discussed.",
    "start": "413620",
    "end": "421030"
  },
  {
    "text": "In addition, we evaluate\nthe advantage function. And then we still do the-- we\nstill compute the gradient.",
    "start": "421030",
    "end": "427973"
  },
  {
    "text": "But now it has a\nslightly different form and has this advantage\nterm here as opposed to the sum of the rewards.",
    "start": "427973",
    "end": "433180"
  },
  {
    "text": "But once we do that, we apply\nthe gradient ascent procedure. And we do this over\nand over again.",
    "start": "433180",
    "end": "439730"
  },
  {
    "text": "All right. So this is just the\nprediction part, and that's the one we\nfinished last time. Are there any\nquestions to this part?",
    "start": "439730",
    "end": "444789"
  },
  {
    "text": " All right. There are no more questions.",
    "start": "444790",
    "end": "450147"
  },
  {
    "text": "Then let's talk about\nhow we can actually use these things, these Q\nfunctions, value functions,",
    "start": "450147",
    "end": "455340"
  },
  {
    "text": "and advantage functions\nto improve the policy, so not only to estimate better\nwhat's the sum of the returns,",
    "start": "455340",
    "end": "461080"
  },
  {
    "text": "but how we can make the\npolicy itself better. So far, the only way we are\nmaking the policy better is by doing gradient ascent.",
    "start": "461080",
    "end": "468670"
  },
  {
    "text": "All right. So far, we've been doing this\npolicy gradient approach, and this was our way of\ncomputing that gradient.",
    "start": "468670",
    "end": "475105"
  },
  {
    "text": " Or in the actor-critic\nway, we would just replace that with the advantage.",
    "start": "475105",
    "end": "482889"
  },
  {
    "text": "So we could make the green\nbox a little bit better by fitting our\nvalue function here. But now the question\nis, how can we",
    "start": "482890",
    "end": "489130"
  },
  {
    "text": "improve the policy using\nsome other procedure, not necessarily using\ngradient ascent?",
    "start": "489130",
    "end": "496479"
  },
  {
    "text": "And to think about\nthis, just remember that the advantage function is\nthe difference between the Q",
    "start": "496480",
    "end": "501760"
  },
  {
    "text": "function and the value function.  So the advantage\nfunction tells us",
    "start": "501760",
    "end": "507950"
  },
  {
    "text": "how good is an action compared\nto our standard policy pi that we would run otherwise.",
    "start": "507950",
    "end": "513750"
  },
  {
    "text": "So to give you a little bit\nmore of an intuition of what's going on, let's go back to our\nexercise that we had last time.",
    "start": "513750",
    "end": "519150"
  },
  {
    "text": "So we have this\nlittle stick figure that is dreaming of being\na drummer, all righted? And the way that\nthe stick figure",
    "start": "519150",
    "end": "526400"
  },
  {
    "text": "will be evaluated\nin their dream is that they will have to\nplay in a month this little drumming test.",
    "start": "526400",
    "end": "532283"
  },
  {
    "text": "And if they can play,\nthey will get the reward 1, and 0 otherwise, all right?",
    "start": "532283",
    "end": "537380"
  },
  {
    "text": "And they have these three\nactions to choose from. They can either\nchill, they can look-- they can watch other\ndrummers perform",
    "start": "537380",
    "end": "543329"
  },
  {
    "text": "and try to learn from\nthat, or they can actually practice drums. And they start at some state,\nst, let's say st equals 0,",
    "start": "543330",
    "end": "552380"
  },
  {
    "text": "so at t equals 0. So they start at\nthe very beginning. And their current\npolicy is this policy",
    "start": "552380",
    "end": "558440"
  },
  {
    "text": "where they want to\nchill all the time. So the probability\nof taking action 1",
    "start": "558440",
    "end": "563540"
  },
  {
    "text": "is 1, so they always\nwant to be doing this. And then we talked about the\nvalue function, Q function,",
    "start": "563540",
    "end": "569060"
  },
  {
    "text": "and the advantage\nfunction in this case. So to test your understanding,\nlet's change this policy a little bit.",
    "start": "569060",
    "end": "575149"
  },
  {
    "text": "Let's say that it's\nnot a stick figure that wants to chill all\nthe time, but it's a stick figure that just wants\nto watch people play drums",
    "start": "575150",
    "end": "581899"
  },
  {
    "text": "all the time. So now we'll say that the\npi of a2 given s is equal 1.",
    "start": "581900",
    "end": "587459"
  },
  {
    "text": "All right. So he just wants to watch\npeople playing drums. So what do you think is the\nvalue function in that case?",
    "start": "587460",
    "end": "594260"
  },
  {
    "start": "594260",
    "end": "601990"
  },
  {
    "text": "Yep. Probably like 0.25. Probably a bit bigger than 0.",
    "start": "601990",
    "end": "607630"
  },
  {
    "text": "I don't know how well\nyou can learn drums from just watching someone. All right. Let's say something like 0.25.",
    "start": "607630",
    "end": "615220"
  },
  {
    "text": "So you would have a fourth\nprobability, one fourth of a probability that\nyou will actually play this thing in a month\nby just watching people",
    "start": "615220",
    "end": "622115"
  },
  {
    "text": "playing drums all the time. OK, I think that's reasonable. What about the Q function\nof our current policy pi, so",
    "start": "622115",
    "end": "629200"
  },
  {
    "text": "Q pi of st, at. Any ideas here? ",
    "start": "629200",
    "end": "639360"
  },
  {
    "text": "So Q function tells us what's\nthe expected sum of rewards, given that we start\nin a given state--",
    "start": "639360",
    "end": "645790"
  },
  {
    "text": "So this is the same state we\nstart at the beginning-- given a specific action. And then, we perform\nthis one action,",
    "start": "645790",
    "end": "652170"
  },
  {
    "text": "and then after that we\ncontinue with our policy pi. So what do you\nthink this would be?",
    "start": "652170",
    "end": "657820"
  },
  {
    "text": "Yep. Probably [INAUDIBLE]. ",
    "start": "657820",
    "end": "664970"
  },
  {
    "text": "Right. But this won't be just a-- so this will be a scalar\nfor a particular action.",
    "start": "664970",
    "end": "672149"
  },
  {
    "text": "All right. So you're saying\nthat the Q of st where t equals 0 of comma a1.",
    "start": "672150",
    "end": "678500"
  },
  {
    "text": " Oh, sorry, a2. a2. [INAUDIBLE] OK. So if at is equal to a2,\nit would be equal to what?",
    "start": "678500",
    "end": "686913"
  },
  {
    "text": "Sorry. [INAUDIBLE] or something,\nhigher than a1 would be.",
    "start": "686913",
    "end": "694880"
  },
  {
    "text": "Right. Yeah. So it will be higher than what\na1 would be but lower than what",
    "start": "694880",
    "end": "699970"
  },
  {
    "text": "a3 would be. But it should be exactly the\nsame as the value function",
    "start": "699970",
    "end": "705820"
  },
  {
    "text": "because the value function was\nthinking that you're always-- your policy is always acting\nas of the a2 as the action",
    "start": "705820",
    "end": "713110"
  },
  {
    "text": "that you're performing. All right? So the Q function for st and\na2 should be equal exactly",
    "start": "713110",
    "end": "719590"
  },
  {
    "text": "to the value function. So we establish that there\nis one fourth of a chance that you will play this thing\nat the end of the month.",
    "start": "719590",
    "end": "725779"
  },
  {
    "text": "So let's say 0.25. Does that make sense? Starting to? OK. And what would it\nbe for a1 then?",
    "start": "725780",
    "end": "731740"
  },
  {
    "text": " Yes.",
    "start": "731740",
    "end": "737200"
  },
  {
    "text": "It might be less than 0.25. That's right. It will be slightly\nless than 0.25. That's right because you perform\nthis one action where you just",
    "start": "737200",
    "end": "743800"
  },
  {
    "text": "chill for this\nparticular moment. But then after this,\nyou will continue watching people play drums.",
    "start": "743800",
    "end": "749920"
  },
  {
    "text": "And for a3, it will be\na little bit higher. All right. Great. So now, what about the\nadvantage function?",
    "start": "749920",
    "end": "755574"
  },
  {
    "text": " And let's just consider the\nadvantage function only for a3.",
    "start": "755575",
    "end": "762040"
  },
  {
    "text": "All right. So the advantage function\nwould be of st, a3. Yes. A more positive number.",
    "start": "762040",
    "end": "768453"
  },
  {
    "text": "It's a more positive number. Yep, that's right. That's right. So the advantage function\nis the difference between the Q function\nand the value function.",
    "start": "768453",
    "end": "776500"
  },
  {
    "text": "So it kind of tells\nus how much better is this action\ncompared to the action that I would usually take. ",
    "start": "776500",
    "end": "784560"
  },
  {
    "text": "All right. So given that we have\nall of these things, we have the value function. Let's assume that we know it.",
    "start": "784560",
    "end": "789920"
  },
  {
    "text": "We know the value\nfunction, the Q function, and the advantage function. And we have our current policy. And let's say that our current\npolicy, we go back to chilling",
    "start": "789920",
    "end": "796639"
  },
  {
    "text": "all the time, so it's this. So given all of\nthese quantities, how can we improve the policy?",
    "start": "796640",
    "end": "802980"
  },
  {
    "text": "Do you have any ideas of a\nsimple algorithm that would allow us to improve the policy? ",
    "start": "802980",
    "end": "812440"
  },
  {
    "text": "Yes. We will have the positive\nadvantage function for the values of a2 and a3.",
    "start": "812440",
    "end": "819050"
  },
  {
    "text": "So we can leverage that so\nthen it looks [INAUDIBLE].. That's right. Yeah.",
    "start": "819050",
    "end": "824220"
  },
  {
    "text": "So just to repeat the answer. We'll have the positive\nadvantage for a2 and a3. So we could choose\namong these two.",
    "start": "824220",
    "end": "830350"
  },
  {
    "text": "And that would, if we\ndo that, then we'll have a policy that is better\nthan the policy we had before. Yeah.",
    "start": "830350",
    "end": "835540"
  },
  {
    "text": "That's great. Cool. So in particular, what\nwe're trying to do is we have this advantage\nfunction that tells us",
    "start": "835540",
    "end": "842139"
  },
  {
    "text": "how much better is at\nthan the average action according to our policy. And the one\nparticular way we can",
    "start": "842140",
    "end": "848150"
  },
  {
    "text": "do this, we can take the\narg max of that, right? So we don't necessarily have\nto choose between a2 and 3. We can just evaluate all of the\nactions and take the arg max.",
    "start": "848150",
    "end": "855550"
  },
  {
    "text": "And that action\nwill definitely be better, or the\npolicy that performs that action will be no\nworse than the policy",
    "start": "855550",
    "end": "861187"
  },
  {
    "text": "that we currently perform.  And so, it will be\nat least as good",
    "start": "861187",
    "end": "866830"
  },
  {
    "text": "as any action from the\npolicy regardless of what the policy actually is, right? So we don't need to have\naccess to the policy.",
    "start": "866830",
    "end": "872920"
  },
  {
    "text": "We just need to have access\nto the advantage function. ",
    "start": "872920",
    "end": "877930"
  },
  {
    "text": "Cool. So let's say our policy\nwould look exactly like this. So with probability 1,\nwe'll be choosing the arg",
    "start": "877930",
    "end": "884230"
  },
  {
    "text": "max of the advantage function. And this will be our action-- one second. And with probability 0,\nwe'll be doing anything else.",
    "start": "884230",
    "end": "891589"
  },
  {
    "text": "Yes. I just have a quick question\nabout the value function.",
    "start": "891590",
    "end": "896610"
  },
  {
    "text": "This one's not super\nclear part of the problem. It seems like the\nhardest part is actually",
    "start": "896610",
    "end": "903339"
  },
  {
    "text": "getting a good value\nfunction, because-- I don't know. Is it common in practice that it\nactually fits very accurately?",
    "start": "903340",
    "end": "911190"
  },
  {
    "text": "Yeah, that's a great question. So the question is it seems that\njust fitting the value function is the problem and whether\nit happens in practice",
    "start": "911190",
    "end": "918760"
  },
  {
    "text": "if we can fit it very well. Yeah. So I think in\npractice, we usually don't fit it extremely well.",
    "start": "918760",
    "end": "924520"
  },
  {
    "text": "We also use algorithms\nthat we will learn about in I think 2 minutes or\nso, where we are constantly",
    "start": "924520",
    "end": "930790"
  },
  {
    "text": "changing the underlying policy. And the value function is\nkind of constantly changing because it's evaluating\nvalue of different policies",
    "start": "930790",
    "end": "937870"
  },
  {
    "text": "all the time. And we don't fit it\nperfectly, but we fit it well enough to actually\nget better and better policies.",
    "start": "937870",
    "end": "943450"
  },
  {
    "text": "But you're right. Fitting the value function\nis actually difficult, yep. Cool. ",
    "start": "943450",
    "end": "949260"
  },
  {
    "text": "All right. So we have this new policy that\nwe know that should be better than our current one.",
    "start": "949260",
    "end": "955519"
  },
  {
    "text": "And this is what leads\nus to an algorithm called policy iteration. So it's a very simple algorithm,\nwhere in the first step",
    "start": "955520",
    "end": "963170"
  },
  {
    "text": "we'll be evaluating our\ncurrent advantage function, so the advantage function\nof the current policy.",
    "start": "963170",
    "end": "969240"
  },
  {
    "text": "And once we have that\nadvantage function, we'll set the policy\nto this new policy.",
    "start": "969240",
    "end": "974720"
  },
  {
    "text": "And we will do this\nover and over again. And the new policy will\nbe equal to just the arg max of the current advantage\nfunction, all right?",
    "start": "974720",
    "end": "982260"
  },
  {
    "text": "So very simple algorithm and we\nare iterating over the policy, so that's why it's called\nthe policy iteration.",
    "start": "982260",
    "end": "989990"
  },
  {
    "text": "OK, cool. And as before, the\nadvantage function is equal to the\nQ function, which",
    "start": "989990",
    "end": "996438"
  },
  {
    "text": "is the reward plus the value\nfunction of the next thing at the next step minus the value\nfunction at the current step. ",
    "start": "996438",
    "end": "1003910"
  },
  {
    "text": "All right. So now, there is two steps\nthat are sort of magical that get us to the final\nalgorithm that",
    "start": "1003910",
    "end": "1010462"
  },
  {
    "text": "is actually extremely useful. Yes. There is a question. Why even bother\ndefining something, like the advantage\nif it's just defining",
    "start": "1010462",
    "end": "1017296"
  },
  {
    "text": "in terms of Q and V\n[INAUDIBLE] stability or something like that. Right. So the question\nis why even bother",
    "start": "1017296",
    "end": "1023460"
  },
  {
    "text": "defining advantage\nfunction if it is defined in terms of\nQ and V. Why can't you just use Q instead?",
    "start": "1023460",
    "end": "1028740"
  },
  {
    "text": "Yeah. So the reason why people\nusually use advantage function is because it's\nbetter normalized.",
    "start": "1028740",
    "end": "1035430"
  },
  {
    "text": "So you subtract\nkind of the thing that is your baseline,\nwhich is the value function.",
    "start": "1035430",
    "end": "1041109"
  },
  {
    "text": "However-- we'll just learn\nabout it in a second-- people also use Q\nfunctions instead.",
    "start": "1041109",
    "end": "1046243"
  },
  {
    "text": "Yep, good stuff. Yep. [INAUDIBLE] it give\nus lower variance. That's right.",
    "start": "1046243",
    "end": "1051630"
  },
  {
    "text": "Yeah. So because it's normalized,\nthat gives us lower variance. And because of that, we\ncan do policy gradients a little better.",
    "start": "1051630",
    "end": "1057700"
  },
  {
    "text": "All right. So now, let's discuss\ntwo steps that are kind of interesting that\nwould allow us to get finally",
    "start": "1057700",
    "end": "1064110"
  },
  {
    "text": "to Q learning algorithm, which\nis applied all over the place. All right. So we had this policy,\nwhich was taking the arg",
    "start": "1064110",
    "end": "1072060"
  },
  {
    "text": "max of the advantage function. And when we doing this arg max,\nwhat we're actually doing--",
    "start": "1072060",
    "end": "1078960"
  },
  {
    "text": "if you remember the\ndrummer example-- we do this arg max\njust to give us the index of the action\nwe should be picking.",
    "start": "1078960",
    "end": "1085080"
  },
  {
    "text": "Right? So we take the arg max\nover the advantage, and that just tells us pick a3. It just gives us the index.",
    "start": "1085080",
    "end": "1090975"
  },
  {
    "text": " Then we know that the\nadvantage function",
    "start": "1090975",
    "end": "1095990"
  },
  {
    "text": "is equal to Q, which\nis expressed like this, minus the value function. And we know that the arg max\nof the advantage function",
    "start": "1095990",
    "end": "1103070"
  },
  {
    "text": "would be exactly the same as\nthe arg max of the Q function. Right? So we can remove\nthe value function because it's not\ndependent on the action,",
    "start": "1103070",
    "end": "1109400"
  },
  {
    "text": "and the arg max will\nbe exactly the same. So let's do that. It will be a little bit simpler.",
    "start": "1109400",
    "end": "1114740"
  },
  {
    "text": "You get rid of one term. So now we can just say\nthat the Q function is equal to the\nreward plus the value",
    "start": "1114740",
    "end": "1119780"
  },
  {
    "text": "function of the next step. Cool. ",
    "start": "1119780",
    "end": "1126350"
  },
  {
    "text": "So now we can change\nthe arg max here to be the arg max\nover the Q function And in the policy iteration\nalgorithm, rather than",
    "start": "1126350",
    "end": "1133040"
  },
  {
    "text": "having the advantage, we'll\nhave the Q function over here. So we kept everything the same. We just changed advantages to Q.",
    "start": "1133040",
    "end": "1141320"
  },
  {
    "text": "So now, here's one trick\nthat we'll start with. So because we are using this arg\nmax just to give us the index,",
    "start": "1141320",
    "end": "1148460"
  },
  {
    "text": "and then as soon as\nwe have that index, we'll have to evaluate it in\nthe next step again, all right?",
    "start": "1148460",
    "end": "1154550"
  },
  {
    "text": "So we kind of have\nthis new policy. But this policy is only useful\nas far as evaluating the Q",
    "start": "1154550",
    "end": "1160523"
  },
  {
    "text": "function of that new policy. We don't actually use\nthe policy itself. We just use the value of\nthat policy in the next step.",
    "start": "1160523",
    "end": "1167220"
  },
  {
    "text": "So the idea is let's\nskip the policy entirely. Let's kind of get\nrid of the middleman and just compute\nthe values directly.",
    "start": "1167220",
    "end": "1175740"
  },
  {
    "text": "So the arg max of\nour Q was the policy. And the value of\nthat new policy would",
    "start": "1175740",
    "end": "1181410"
  },
  {
    "text": "be actually the max of\nthe Q. So the arg max was kind of giving us\nthe index of the action. But the max of\nthe Q is giving us",
    "start": "1181410",
    "end": "1188370"
  },
  {
    "text": "the value of that\nnew policy, which was the arg max, all right? This is a little bit of a\ntrick, but it's actually",
    "start": "1188370",
    "end": "1197039"
  },
  {
    "text": "extremely useful. All right. So this approximates\nthe new value. ",
    "start": "1197040",
    "end": "1204402"
  },
  {
    "text": "All right. And that leads us to a value\niteration algorithm, which is kind of the similar\nversion to policy iteration,",
    "start": "1204402",
    "end": "1212200"
  },
  {
    "text": "but now we'll be iterating\non the value itself. So this is how it works. We set the Q to be equal to\nthe reward plus the value",
    "start": "1212200",
    "end": "1220420"
  },
  {
    "text": "of the next step. So this is what\nthe Q function is. But then we'll update\nthe value function",
    "start": "1220420",
    "end": "1226540"
  },
  {
    "text": "to be the max of the Q. All right. So this is kind of like\nin the previous step here,",
    "start": "1226540",
    "end": "1231713"
  },
  {
    "text": "we were updating the policy. Here we are updating the\nvalue of the new policy. So because the new policy\nwill be the arg max of the Q,",
    "start": "1231713",
    "end": "1238554"
  },
  {
    "text": "we can just say that the\nvalue of that new policy that is the arg max of the Q\nis just the max of the Q.",
    "start": "1238555",
    "end": "1243826"
  },
  {
    "text": "And we do this over\nand over again, and we are iterating\non the value. And this is a value iteration. ",
    "start": "1243826",
    "end": "1251299"
  },
  {
    "text": "Cool. So that means that, in the\ngreen box, we have this Q now.",
    "start": "1251300",
    "end": "1256380"
  },
  {
    "text": "And to improve the\npolicy, we are just setting the values equal\nto the max over actions of the Q. Cool.",
    "start": "1256380",
    "end": "1264010"
  },
  {
    "text": "So now, the question is\nwe have this algorithm,",
    "start": "1264010",
    "end": "1269650"
  },
  {
    "text": "and now we have to implement it. Right? We have to have some neural\nnetworks that actually learn",
    "start": "1269650",
    "end": "1276640"
  },
  {
    "text": "something and try to fit either\nthe value function or the Q function. So this is called\nvalue iteration.",
    "start": "1276640",
    "end": "1282560"
  },
  {
    "text": "So let's maybe start\nwith value function. All right. We'll try to have\na neural net that represents a value function.",
    "start": "1282560",
    "end": "1288080"
  },
  {
    "text": "So it takes a state as input\nand outputs a scalar, which is the expected sum of rewards. And let's see if we\ncan do that, all right?",
    "start": "1288080",
    "end": "1295539"
  },
  {
    "text": "So let's write that\non the board here. So let's see, our value\nfunction V of s parameterized",
    "start": "1295540",
    "end": "1306220"
  },
  {
    "text": "with some parameters\nphi would be equal to the max over\nactions of the Q,",
    "start": "1306220",
    "end": "1316856"
  },
  {
    "text": "but we don't have a neural\nnetwork that represents the Q. So let's express\nQ in terms of the V.",
    "start": "1316857",
    "end": "1322509"
  },
  {
    "text": "And we have that in the\nline in step number one. So this would be\nthe max of r of s,",
    "start": "1322510",
    "end": "1330940"
  },
  {
    "text": "a plus discounted expectation\nof the value at the next state.",
    "start": "1330940",
    "end": "1342379"
  },
  {
    "text": "All right. And this expectation is\nwith respect to p of s prime given s, a.",
    "start": "1342380",
    "end": "1349045"
  },
  {
    "text": " All right. So this would be our target\nfor the value function.",
    "start": "1349045",
    "end": "1360160"
  },
  {
    "text": "And we'll be trying to minimize\nthe difference between the two. And we'll optimize the\nneural network to do that.",
    "start": "1360160",
    "end": "1365230"
  },
  {
    "text": "There is a question. Yep. Instead of taking\nthe expectation, don't you still need to know\nthe transition dynamics,",
    "start": "1365230",
    "end": "1373600"
  },
  {
    "text": "like the transition\nproperties there? That's a great point. So the question was, don't you\nneed to know the transition",
    "start": "1373600",
    "end": "1381170"
  },
  {
    "text": "dynamics because you are\ntaking the expectation, and you need to know\nwhat the next state is. Yes, that's true. So there is this problem\nof this equation,",
    "start": "1381170",
    "end": "1388190"
  },
  {
    "text": "which is if we were actually\ntrying to implement this, we have the value function\nthat takes the state as input--",
    "start": "1388190",
    "end": "1393640"
  },
  {
    "text": "so we have state that\nis available to us-- and then we have to take\nthe max over all the actions",
    "start": "1393640",
    "end": "1398650"
  },
  {
    "text": "that are possible. So we can try different actions. So we would need to know\nthe reward function.",
    "start": "1398650",
    "end": "1404650"
  },
  {
    "text": "Let's say that we know\nthat for a specific state. We know, given a\nspecific action, what the reward function is.",
    "start": "1404650",
    "end": "1410290"
  },
  {
    "text": "But then we would\nneed to know what would be the next state\nthat that would end up in given that action so that\nwe can compute the value",
    "start": "1410290",
    "end": "1416890"
  },
  {
    "text": "function of the next state. All right? But to know this, we will be\ntesting different actions,",
    "start": "1416890",
    "end": "1422230"
  },
  {
    "text": "right, because we'll need to\ntake the max of our actions. So to notice, we\nwould need to know what would be the different\nstates that we would end up",
    "start": "1422230",
    "end": "1428932"
  },
  {
    "text": "in given those different\nactions, right? We would have to-- let's say we have 10 different\nactions to choose from.",
    "start": "1428932",
    "end": "1435340"
  },
  {
    "text": "For each one of\nthese actions, you would need to know\nwhat the next state is so that you can input this\nto your value function",
    "start": "1435340",
    "end": "1441160"
  },
  {
    "text": "to your neural net. So you need to know the\ntransition dynamics, which was the question. Yep.",
    "start": "1441160",
    "end": "1446410"
  },
  {
    "text": "Wouldn't we just\nrandomly sample like, assume that those are\ndifferent model examples",
    "start": "1446410",
    "end": "1452525"
  },
  {
    "text": "and say, OK, I've\nobserved one transition and I've taken that action.",
    "start": "1452525",
    "end": "1458210"
  },
  {
    "text": "So the question is, can\nyou just randomly sample? Because you have an\nexpected value of a sample",
    "start": "1458210",
    "end": "1463260"
  },
  {
    "text": "if you have [INAUDIBLE]\njust because your data that you're going to obtain\nis from that distribution.",
    "start": "1463260",
    "end": "1468433"
  },
  {
    "text": "Shouldn't samples\nfrom that distribution converge to the expected value? Right. But to sample from\nthis distribution,",
    "start": "1468433",
    "end": "1475030"
  },
  {
    "text": "you don't actually have access\nto all the different actions that you could perform. You only have access to the\nactions that you did perform.",
    "start": "1475030",
    "end": "1482230"
  },
  {
    "text": "Yeah. All right. OK. So in that case, you\nwould need to kind of know had I performed that other\naction that I've never",
    "start": "1482230",
    "end": "1488110"
  },
  {
    "text": "actually tried, what\nstate would I end up in. And you don't have\naccess to that. You don't know what\nthe future would be.",
    "start": "1488110",
    "end": "1495649"
  },
  {
    "text": "All right. So the value function\ndoesn't quite work. All right. And that's what leads us to the\nfinal algorithm, Q learning.",
    "start": "1495650",
    "end": "1503690"
  },
  {
    "text": "So again, we have the\nvalue iteration algorithm. But now, instead of\nfitting the value function,",
    "start": "1503690",
    "end": "1512809"
  },
  {
    "text": "let's just fit the Q\nfunction instead, all right? So we keep everything\nexactly the same.",
    "start": "1512810",
    "end": "1517940"
  },
  {
    "text": "But now, instead of\nlearning the value function, we will be learning\nthe Q function. So let's do that.",
    "start": "1517940",
    "end": "1526970"
  },
  {
    "text": "One second, OK, cool. So now, our targets\nwill be trying",
    "start": "1526970",
    "end": "1531980"
  },
  {
    "text": "to fit the Q function\nparameterized with some parameters s comma a. And this would be equal\nto the reward of s,",
    "start": "1531980",
    "end": "1544190"
  },
  {
    "text": "a plus the expected next value.",
    "start": "1544190",
    "end": "1549620"
  },
  {
    "text": "And the next value is the\nmax of the Q, all right? So the max over actions--",
    "start": "1549620",
    "end": "1555735"
  },
  {
    "text": "this is a different action. This is not this action\nthat is the input. This is what's called a prime\nof the Q of s prime, a prime.",
    "start": "1555735",
    "end": "1565100"
  },
  {
    "text": " All right. And this is the Q function\nthat was parameterized",
    "start": "1565100",
    "end": "1571669"
  },
  {
    "text": "with the same parameters. So it's exact same thing. Now, we are fitting the Q\nfunction instead of the value function.",
    "start": "1571670",
    "end": "1576710"
  },
  {
    "text": "Yes. Like, how are you getting\nthe s prime [INAUDIBLE]?? How are we getting s prime?",
    "start": "1576710",
    "end": "1582170"
  },
  {
    "text": "Yep. That's a great question. So let's think about\nthis equation first, and then we'll get how\nwe get the s prime.",
    "start": "1582170",
    "end": "1589830"
  },
  {
    "text": "So now, the input\nto our Q function is state and action, all right?",
    "start": "1589830",
    "end": "1595610"
  },
  {
    "text": "So now, our Q function looks\na little bit different. All right.",
    "start": "1595610",
    "end": "1600710"
  },
  {
    "text": "Our Q function now has two\ninputs states and the action and then outputs the\nvalue, all right?",
    "start": "1600710",
    "end": "1606259"
  },
  {
    "text": "So this is expressed here. And given state\nand in an action we have access to a\nreward function. So we can compute the reward.",
    "start": "1606260",
    "end": "1613890"
  },
  {
    "text": "And now we have\nthis max term again. But now it's a\nlittle bit different. So the max is taking the\nmax over all the actions",
    "start": "1613890",
    "end": "1622460"
  },
  {
    "text": "of the Q function of the\nnext state, all right? But now, let's ignore the\nnext state for a second.",
    "start": "1622460",
    "end": "1629160"
  },
  {
    "text": "But in terms of the actions,\nwe can actually do this, right? Our Q function is dependent\non both state and action.",
    "start": "1629160",
    "end": "1634309"
  },
  {
    "text": "So we can try-- we can plug-in\nall kinds of different actions that we haven't taken\nand compute the max.",
    "start": "1634310",
    "end": "1640024"
  },
  {
    "text": "All right? We don't have to simulate now\nwhat's going to happen next. We can just plug in the\nactions because this",
    "start": "1640025",
    "end": "1646040"
  },
  {
    "text": "is part of the input\nof our function and just try it out\nand see what happens. Now, how do we get\nthe next state?",
    "start": "1646040",
    "end": "1652580"
  },
  {
    "text": "So the way we would\ndo this is actually similar to how we did-- what we did in the\nactor-critic case.",
    "start": "1652580",
    "end": "1659370"
  },
  {
    "text": "We would just be saving. So this next state\nhere, this is just",
    "start": "1659370",
    "end": "1665390"
  },
  {
    "text": "the function of the current\nstate and the action, all right?",
    "start": "1665390",
    "end": "1671420"
  },
  {
    "text": "It's independent of the\npolicy or of the Q function or anything like this. So what we'll be saving as we\nrun the policy in the world,",
    "start": "1671420",
    "end": "1679280"
  },
  {
    "text": "we'll be saving\nso-called source tuples. So we'll be saving state,\naction, reward that we got,",
    "start": "1679280",
    "end": "1686690"
  },
  {
    "text": "and the next state\nthat we observe. And then from that,\nwe'll be approximating those expectations\nfrom the states",
    "start": "1686690",
    "end": "1692840"
  },
  {
    "text": "that we have actually seen. So we can just record-- as we run the policy, we'll\nbe recording these four values",
    "start": "1692840",
    "end": "1700650"
  },
  {
    "text": "so that we have access to\ns prime that we've actually experienced. And here, we'll be plugging\nthat s prime that we've seen.",
    "start": "1700650",
    "end": "1707090"
  },
  {
    "start": "1707090",
    "end": "1712500"
  },
  {
    "text": "Yep. [INAUDIBLE] different\ns primes, right?",
    "start": "1712500",
    "end": "1717740"
  },
  {
    "text": "Because it's not [INAUDIBLE]",
    "start": "1717740",
    "end": "1724170"
  },
  {
    "text": "Right. So you might experience\nmultiple s primes. Yeah, so we'll iterate\nthrough all of them.",
    "start": "1724170",
    "end": "1729650"
  },
  {
    "text": "So like for every source\ntuple, you will perform this. And then this is how you would\napproximate this expectation",
    "start": "1729650",
    "end": "1734880"
  },
  {
    "text": "there.  All right. Cool. So this is what gets\nus to Q learning,",
    "start": "1734880",
    "end": "1741060"
  },
  {
    "text": "doesn't require\nsimulation of actions. And we can do this. All right. So a quick recap.",
    "start": "1741060",
    "end": "1748120"
  },
  {
    "text": "A few definitions that\nwe introduced here. We have the value\nfunction that tells us the total rewards\nstarting from state",
    "start": "1748120",
    "end": "1753940"
  },
  {
    "text": "and then following our policy. So in other words, it\nanswers the question, how good is the state?",
    "start": "1753940",
    "end": "1760330"
  },
  {
    "text": "Then we have the\nQ function, which is the total reward starting\nfrom s taking a single action and then following\nthe policy, which",
    "start": "1760330",
    "end": "1767830"
  },
  {
    "text": "answers the question, how good\nis the given state action pair? And then for this\noptimal policy,",
    "start": "1767830",
    "end": "1775390"
  },
  {
    "text": "we just introduced this\nequation right here. We have this dependency\nthat says the Q function,",
    "start": "1775390",
    "end": "1783280"
  },
  {
    "text": "the optimal Q function which\nwill be depicted with star-- so this is now the optimal\npolicy, is pi star--",
    "start": "1783280",
    "end": "1789220"
  },
  {
    "text": "is equal to this expectation. And this is the\nreward that we got",
    "start": "1789220",
    "end": "1794650"
  },
  {
    "text": "for the given state and action. So this expectation is actually\nunnecessary for the reward plus the max over actions\nof the Q of the next action.",
    "start": "1794650",
    "end": "1802240"
  },
  {
    "text": "And this is what we refer\nto as the Bellman equation or the Bellman\noptimality equation. And this is one of the\nmost important equations",
    "start": "1802240",
    "end": "1807910"
  },
  {
    "text": "in reinforcement learning.  All right. So let's go back to\nour drumming example.",
    "start": "1807910",
    "end": "1815210"
  },
  {
    "text": "So we talked about the value\nfunction, the Q function. What is the Q function\nof the optimal policy?",
    "start": "1815210",
    "end": "1821179"
  },
  {
    "text": "So our Q star. So this is not Q pi, which\nis the current policy. And the current policy\nis the stick figure",
    "start": "1821180",
    "end": "1826840"
  },
  {
    "text": "wants to chill all the time. What's the Q function of\nthe optimal policy pi star?",
    "start": "1826840",
    "end": "1832690"
  },
  {
    "text": "So what's Q star? Any ideas? ",
    "start": "1832690",
    "end": "1841330"
  },
  {
    "text": "Yes. For a3, it'd be 1 because\nyou're starting in the state--",
    "start": "1841330",
    "end": "1847965"
  },
  {
    "text": "starting in state 0. And if you take a3,\nyou're going then perform a3 afterwards\nbecause a3 always brings out",
    "start": "1847965",
    "end": "1854973"
  },
  {
    "text": "the optimal policy. So in that case, you\nwould just converge to s1. And a2, I would stay\nit's a bit less than 1",
    "start": "1854973",
    "end": "1863460"
  },
  {
    "text": "because you're going to\nperform a2 once and then do a3 all the time. And the same is for a1,\nso you'll have [INAUDIBLE]",
    "start": "1863460",
    "end": "1872068"
  },
  {
    "text": "That's right. Yeah. So just to repeat\nthis for everyone. For a3, it would be 1.",
    "start": "1872068",
    "end": "1877673"
  },
  {
    "text": "That means that\nyou are practicing all the time, including\nthe current time step. For a2, it would be a\nlittle bit less than 1.",
    "start": "1877673",
    "end": "1883560"
  },
  {
    "text": "You just watch for a second, and\nthen you practice all the time. And for a1, it will\nbe a little bit less.",
    "start": "1883560",
    "end": "1889770"
  },
  {
    "text": "So you take an hour\noff, let's say. You chill, and then you practice\nfor the rest of the month.",
    "start": "1889770",
    "end": "1895320"
  },
  {
    "text": " And what would be the value\nfunction, the V-star of S0?",
    "start": "1895320",
    "end": "1902730"
  },
  {
    "text": " Please just don't be shy.",
    "start": "1902730",
    "end": "1908170"
  },
  {
    "text": "Raise your hand. Any ideas or just\nshout out the answer. ",
    "start": "1908170",
    "end": "1918105"
  },
  {
    "text": "Yes. [INAUDIBLE] That's right. That would be 1 because the\noptimal action will be a3.",
    "start": "1918105",
    "end": "1923750"
  },
  {
    "text": "Yeah. So this takes-- this\nassumes that you are acting optimally straightaway.",
    "start": "1923750",
    "end": "1929540"
  },
  {
    "text": "Cool. All right. So we introduced this fitted\nQ-iteration algorithm,",
    "start": "1929540",
    "end": "1935130"
  },
  {
    "text": "which works like this. We have a few hyperparameters. So first, we collect the\ndata set using some policy,",
    "start": "1935130",
    "end": "1942539"
  },
  {
    "text": "and we collect the source\ntuples-- so state, action, next state, and the reward.",
    "start": "1942540",
    "end": "1947670"
  },
  {
    "text": "And let's say we have a\ndata size N, data set size N some collection policy that\nwe collect the data with.",
    "start": "1947670",
    "end": "1957120"
  },
  {
    "text": "Then we set our targets\nfor the Q function according to the\nBellman equation that we just introduced.",
    "start": "1957120",
    "end": "1963730"
  },
  {
    "text": "And then we minimize the\ndistance between the Q function and the targets.",
    "start": "1963730",
    "end": "1969049"
  },
  {
    "text": "All right? And our Q function\nis this neural net that states an\naction as an input",
    "start": "1969050",
    "end": "1975660"
  },
  {
    "text": "and outputs a\nsingle scalar value. So we do this over\ns gradient steps.",
    "start": "1975660",
    "end": "1982559"
  },
  {
    "text": "We can do this a few times. And then we do\nthis-- we can iterate",
    "start": "1982560",
    "end": "1988080"
  },
  {
    "text": "in terms of updating\nthe targets K times. So we can apply a\nfew gradient steps.",
    "start": "1988080",
    "end": "1993960"
  },
  {
    "text": "And then we will\nupdate the targets, because now our Q\nfunction has changed, so this max term\nwill be changing.",
    "start": "1993960",
    "end": "1999600"
  },
  {
    "text": "And it's an iterative procedure. So we are getting kind of-- we\nare approximating Q star better and better over time.",
    "start": "1999600",
    "end": "2006350"
  },
  {
    "text": "And then we go back to\ncollecting the data set. And as a result, we\ncan get the policy",
    "start": "2006350",
    "end": "2011630"
  },
  {
    "text": "by just taking the arg max\nof the Q. There's a question. In step two, do we have to--",
    "start": "2011630",
    "end": "2017480"
  },
  {
    "text": "do we have to do a\nbunch of forward passes, one for every possible a prime?",
    "start": "2017480",
    "end": "2022980"
  },
  {
    "text": "Yeah. That's a great question. So the question is, do\nwe have to do in step two a bunch of forward passes,\none pass per a prime?",
    "start": "2022980",
    "end": "2032540"
  },
  {
    "text": "Technically, yes. But you can batch them together. Yep. ",
    "start": "2032540",
    "end": "2039080"
  },
  {
    "text": "In terms of step two, I\nthink you' should write phi prime instead of y pos.",
    "start": "2039080",
    "end": "2046100"
  },
  {
    "text": "In step three, you're going\nto take arg min with respect to phi and the yi [INAUDIBLE] If\nthey share the same parameters",
    "start": "2046100",
    "end": "2057040"
  },
  {
    "text": "then, I mean,\ntechnically we're going to pass the gradient\nfrom [INAUDIBLE] one.",
    "start": "2057040",
    "end": "2063619"
  },
  {
    "text": "So you're saying\nthat this Q over here should have different\nparameters than the Q over here?",
    "start": "2063620",
    "end": "2069138"
  },
  {
    "text": "Yeah, because [INAUDIBLE] Right. Yeah, yeah, yeah. So this is one of the\nimplementation things",
    "start": "2069139",
    "end": "2076310"
  },
  {
    "text": "that people introduce to\nmake this procedure much more stable. And they introduce a\ntarget network here,",
    "start": "2076310",
    "end": "2082339"
  },
  {
    "text": "because this network\nis constantly changing. We're constantly applying\ngradient steps to it. And it causes some\ninstability in learning.",
    "start": "2082340",
    "end": "2087830"
  },
  {
    "text": "So instead, what\npeople do is they copy the values of this network\nonce and then use that target",
    "start": "2087830",
    "end": "2094340"
  },
  {
    "text": "network with copied\nvalues here and then apply the gradient steps to\nthis network that is constantly",
    "start": "2094340",
    "end": "2100430"
  },
  {
    "text": "being updated. And every now and then, they\nwould update the target network to the network that\nthey've been using.",
    "start": "2100430",
    "end": "2105650"
  },
  {
    "text": "Yeah. That's a implementation\ntrick that people use. That's right. All right. So a few important nodes.",
    "start": "2105650",
    "end": "2111920"
  },
  {
    "text": "We can reuse data from\nprevious policies, right? So here we are just\nusing some policy.",
    "start": "2111920",
    "end": "2117170"
  },
  {
    "text": "It's not an on-policy algorithm. We don't have to be rolling\nout from the current policy. This is any policy.",
    "start": "2117170",
    "end": "2122240"
  },
  {
    "text": "There's a question. Just one quick question\ngoing with the a, if it's still fine doing a batch\nforward pass if it's discrete.",
    "start": "2122240",
    "end": "2131150"
  },
  {
    "text": "But what would you do different\nin a continuous space? Yeah. That's a great\nquestion, and we'll answer this in one minute.",
    "start": "2131150",
    "end": "2137480"
  },
  {
    "text": "Oh, OK. Yeah. Great question though. The question was, how would\nyou do it in a continuous-- how would you take that\nmax in a continuous case?",
    "start": "2137480",
    "end": "2144062"
  },
  {
    "text": "And can you batch\nthe actions then? All right. So we can reuse data\nfrom previous policies.",
    "start": "2144062",
    "end": "2151010"
  },
  {
    "text": "It's an off-policy algorithm. We don't have to be\nrunning the policy that we are currently learning. It could be-- we could\nbe running any policy.",
    "start": "2151010",
    "end": "2159559"
  },
  {
    "text": "In practice, what\npeople do is they push the source tuples\nto replay buffers. And then they have\none big replay buffer",
    "start": "2159560",
    "end": "2164930"
  },
  {
    "text": "with all the data, and they\njust sample from the buffer to do the updates. And importantly, this\nis not a gradient.",
    "start": "2164930",
    "end": "2171619"
  },
  {
    "text": "It's an algorithm, all right? So even though we are\ntrying to fit the Q function to the targets-- and this is where we\napply gradient descent--",
    "start": "2171620",
    "end": "2179610"
  },
  {
    "text": "the algorithm, how we learn\nbetter and better policies, is a recursive\niterative algorithm,",
    "start": "2179610",
    "end": "2186440"
  },
  {
    "text": "which uses dynamic programming. All right. So here we are constantly\nupdating our Q function",
    "start": "2186440",
    "end": "2192800"
  },
  {
    "text": "according to this formula. And this doesn't require\ngradient descent. We use gradient descent\nto just fit to the Q",
    "start": "2192800",
    "end": "2198770"
  },
  {
    "text": "functions of the current target. All right. So now, a quick example.",
    "start": "2198770",
    "end": "2205413"
  },
  {
    "text": "And this is the example\nof continuous actions. So we'll try to apply\nQ learning to robotics.",
    "start": "2205413",
    "end": "2211200"
  },
  {
    "text": "So this is our algorithm. So now, the question\nis, how do we take that max over\nactions, where",
    "start": "2211200",
    "end": "2216690"
  },
  {
    "text": "robots move in continuous\nspace, like their actions are continuous. They need to get to a\ncertain pose, for instance.",
    "start": "2216690",
    "end": "2223480"
  },
  {
    "text": "So we can just enumerate\nall the actions. This is a continuous action. So one way we can\ntake that max here",
    "start": "2223480",
    "end": "2231510"
  },
  {
    "text": "is by running a simple\noptimization algorithm and there is this great and free\noptimization algorithm called cross entropy method, or CEM.",
    "start": "2231510",
    "end": "2238079"
  },
  {
    "text": "And the way it works is\nactually really simple. You would start with\na normal distribution.",
    "start": "2238080",
    "end": "2244170"
  },
  {
    "text": "And then you would\nsample a bunch of actions from that normal\ndistribution and then score",
    "start": "2244170",
    "end": "2250170"
  },
  {
    "text": "each one of them. So I would start with,\nlet's say, 20 actions. I sample all of them. And then I query my\nQ function, what's",
    "start": "2250170",
    "end": "2256710"
  },
  {
    "text": "the value of each\none of those actions? And some of them will\nhave higher values. Some of them will\nhave lower values.",
    "start": "2256710",
    "end": "2262540"
  },
  {
    "text": "So the higher values\nhere are in gray. And then I can do\nanother iteration of CEM",
    "start": "2262540",
    "end": "2267630"
  },
  {
    "text": "where I would now fit\na Gaussian to my elites to the best actions that\nI-- the actions that achieve",
    "start": "2267630",
    "end": "2275490"
  },
  {
    "text": "the highest score, and then\nperform this procedure again. Yep. [INAUDIBLE]",
    "start": "2275490",
    "end": "2283990"
  },
  {
    "text": "So this is a type of\nevolutionary algorithm, or a gradient free\noptimization method.",
    "start": "2283990",
    "end": "2290040"
  },
  {
    "text": "You can use any other\noptimization method to just find the\nmax of a function.",
    "start": "2290040",
    "end": "2296807"
  },
  {
    "text": "This is just a very simple\nthing that was actually applied to the robots.",
    "start": "2296807",
    "end": "2302220"
  },
  {
    "text": "Cool. All right. So let's do all of this. So this is the work\ncalled QT-Opt done",
    "start": "2302220",
    "end": "2308210"
  },
  {
    "text": "by Dmitry Kalashnikov et al. at Google Brain. And the way it works\nis the following.",
    "start": "2308210",
    "end": "2313530"
  },
  {
    "text": "We store data from all\nthe past experiments. Then we have a bunch of buffers,\nthe off-policy buffer where",
    "start": "2313530",
    "end": "2320210"
  },
  {
    "text": "we put all the\ndata in, as well as the on-policy buffer\nwhere we currently collect the experience.",
    "start": "2320210",
    "end": "2326870"
  },
  {
    "text": "We have our Q network,\nwhich is parameterized by some parameters theta.",
    "start": "2326870",
    "end": "2332193"
  },
  {
    "text": "And then, we have\nthis separate process. This is running a large scale. This is our parallel. So we have the\nseparate process that's",
    "start": "2332193",
    "end": "2338510"
  },
  {
    "text": "called the Bellman updater which\nis just computing the target values for the current\nQ function, all right?",
    "start": "2338510",
    "end": "2343790"
  },
  {
    "text": "So all it does is just\ncompute the reward plus the max of the next Q. And\nthis uses the CEM optimization",
    "start": "2343790",
    "end": "2350960"
  },
  {
    "text": "to get that max. And then once it does that,\nit would push these targets",
    "start": "2350960",
    "end": "2357080"
  },
  {
    "text": "to a buffer as well. And then from this buffer, we'll\nhave a set of training jobs",
    "start": "2357080",
    "end": "2362083"
  },
  {
    "text": "that are just doing great\nin the sense that are just trying to fit the Q\nfunction to that Q target and update the parameters\nof the neural net.",
    "start": "2362083",
    "end": "2369530"
  },
  {
    "text": "This is heavily\nparameterized and running on thousands of machines. ",
    "start": "2369530",
    "end": "2374890"
  },
  {
    "text": "Cool. So this was applied to grasping. And the MDP definition for\ngrasping was the following.",
    "start": "2374890",
    "end": "2382430"
  },
  {
    "text": "So the state was\nthe image captured by the camera that was here,\nover the shoulder camera.",
    "start": "2382430",
    "end": "2387970"
  },
  {
    "text": "And this is what the\nimage looked like. The action was 4\ndegrees of freedom pose",
    "start": "2387970",
    "end": "2393880"
  },
  {
    "text": "change in the Cartesian space. So it was controlling--\nit was a top-down grasp. It was controlling the\nposition as well as",
    "start": "2393880",
    "end": "2399820"
  },
  {
    "text": "the yaw, the orientation here,\nplus the gripper control. So it could also tell\nwhether the gripper",
    "start": "2399820",
    "end": "2405100"
  },
  {
    "text": "should close or open. And the reward was binary. So it would just say at the\nvery end of the trajectory,",
    "start": "2405100",
    "end": "2412142"
  },
  {
    "text": "if you grab something\nsuccessfully, you get the reward\n1, 0, or otherwise. That's it.",
    "start": "2412142",
    "end": "2417770"
  },
  {
    "text": "And we actually implemented\nan automatic success detection mechanism for this,\nwhich was very simple.",
    "start": "2417770",
    "end": "2422990"
  },
  {
    "text": "So at the end of the\ntrajectory, the robot is either holding something\nin its hand or it's not. So what we would\ndo is we would take",
    "start": "2422990",
    "end": "2428660"
  },
  {
    "text": "an image when it's holding-- or at the end of the episode. And then we would open the\ngripper and take another image.",
    "start": "2428660",
    "end": "2434840"
  },
  {
    "text": "Now, if you are\nholding something, then the difference\nbetween these two images would be non-zero,\nright, because you just",
    "start": "2434840",
    "end": "2440030"
  },
  {
    "text": "dropped something into the bin. If you didn't hold\nanything, the difference would be zero, so very simple\nautomatic success detection",
    "start": "2440030",
    "end": "2447440"
  },
  {
    "text": "mechanism. All right. So in terms of the results,\nwe used seven robots to collect over 580,000 grasps.",
    "start": "2447440",
    "end": "2457280"
  },
  {
    "text": "And it was evaluated on\npreviously unseen objects. So we'd just drop a whole\nbunch of different objects into the bins, let the\nrobots run with them",
    "start": "2457280",
    "end": "2464360"
  },
  {
    "text": "and practice\ngrasping, and then we would evaluate it on the\nobjects that the robot's never seen before.",
    "start": "2464360",
    "end": "2470310"
  },
  {
    "text": "And the success rate\nwas really high. It was a 96% success rate.",
    "start": "2470310",
    "end": "2475880"
  },
  {
    "text": "And that was actually\nreally surprising to us that we could get to a number\nthis high, because grasping is not a new problem. People have been working on\ngrasping for 50 years or so.",
    "start": "2475880",
    "end": "2484650"
  },
  {
    "text": "And this is, I\nthink, the first time where we were able to see that\nthis problem can be actually solved, and we can grasp\nobjects that the robot's never",
    "start": "2484650",
    "end": "2491600"
  },
  {
    "text": "seen before. And here are some\nexamples of strategies that the robots learned.",
    "start": "2491600",
    "end": "2498000"
  },
  {
    "text": "So here, you can see\nthat this grasping is kind of full of contact. And it's interacting\nwith the environment, which is very difficult to\nprescribe or to engineer up",
    "start": "2498000",
    "end": "2505910"
  },
  {
    "text": "front. So here, it's\ntrying to single out the object that's moving the\nother objects apart, and so on, to grasp it.",
    "start": "2505910",
    "end": "2512210"
  },
  {
    "text": "Here, there is an example\nof a reactive behavior where it's trying to\ngrasp this tennis ball.",
    "start": "2512210",
    "end": "2518573"
  },
  {
    "text": "And then I think in a second you\nwill see an example where there is a mean experimenter, and it's\nmoving the tennis ball away.",
    "start": "2518573",
    "end": "2524390"
  },
  {
    "text": "And the robot understands that\nthe ball is not there anymore and follows it and gets to a\nstate that has high Q value.",
    "start": "2524390",
    "end": "2532434"
  },
  {
    "text": "Right here is one\nmore case where it's a very flat object that\nis really difficult to grasp. And the robot learn how\nto retry and kind of fully",
    "start": "2532435",
    "end": "2539300"
  },
  {
    "text": "understand what it means\nto grasp something. And it tries it a few times\nuntil it gets it right.",
    "start": "2539300",
    "end": "2544760"
  },
  {
    "text": " So it kind of understands\nthat some actions or some Q",
    "start": "2544760",
    "end": "2550444"
  },
  {
    "text": "values for some of\nthose states are not as high as they should be. So it runs this max to find the\nbetter action until the moment",
    "start": "2550445",
    "end": "2558490"
  },
  {
    "text": "where it actually grasps\nit and then succeeds. All right. So to summarize Q-learning,\nin terms of pros, we have--",
    "start": "2558490",
    "end": "2567310"
  },
  {
    "text": "this is more sample efficient\nthan on-policy methods. We can incorporate\noff-policy data, including",
    "start": "2567310",
    "end": "2573730"
  },
  {
    "text": "a fully offline setting that\nwe'll discuss in, I think, two weeks. And we can update the policy\neven without seeing the reward.",
    "start": "2573730",
    "end": "2580660"
  },
  {
    "text": "Kind of like in the chess\nmatch that we discussed before, as soon as you make\nthe next prediction, you can update the policy.",
    "start": "2580660",
    "end": "2586060"
  },
  {
    "text": "And it's relatively\neasy to parallelize how we just saw in QT-Opt case. And in terms of\nthe cons, there's",
    "start": "2586060",
    "end": "2592595"
  },
  {
    "text": "lots of tricks to make it\nwork, including target networks and other things. And potentially,\nit could be harder",
    "start": "2592595",
    "end": "2598599"
  },
  {
    "text": "to learn than just\nthe policy itself, because you need to model\nthe landscape of all of the actions.",
    "start": "2598600",
    "end": "2603670"
  },
  {
    "text": "All right. Any questions? Yes. Could you go back one slide? Uh-huh.",
    "start": "2603670",
    "end": "2608710"
  },
  {
    "text": "How did you guys encode\nthe fact that they we were supposed to pick up the\ntennis ball, for example?",
    "start": "2608710",
    "end": "2614576"
  },
  {
    "text": "Is that part of the state space? Yeah, great question. So the question was, how\ndo you encode the fact",
    "start": "2614576",
    "end": "2619660"
  },
  {
    "text": "that you want it to\ngrasp a tennis ball? This was just an accident.",
    "start": "2619660",
    "end": "2625060"
  },
  {
    "text": "So we didn't encode\nit in any way. So this was not an\ninstance grasping system. It just grasps\nanything that it sees.",
    "start": "2625060",
    "end": "2630280"
  },
  {
    "text": "[INAUDIBLE] it's\ntaking the whole part. It just grabs any of them. It wouldn't-- Right. So if you let it run\nfor a little longer,",
    "start": "2630280",
    "end": "2636310"
  },
  {
    "text": "it would eventually pick\nall of them one by one. Question. So in the picking up\nexample you just mentioned",
    "start": "2636310",
    "end": "2642970"
  },
  {
    "text": "here, you [INAUDIBLE] another\ncontinuous action space. So I'm curious to\nknow why you didn't",
    "start": "2642970",
    "end": "2650130"
  },
  {
    "text": "use DDPG to have\n[INAUDIBLE] action space because for me it's quite\nintuitive to use DDPG rather",
    "start": "2650130",
    "end": "2658483"
  },
  {
    "text": "than [INAUDIBLE]",
    "start": "2658483",
    "end": "2665150"
  },
  {
    "text": "Right. Yeah. So the question\nwas why didn't we use some other method like\nDDPG that is fairly popular.",
    "start": "2665150",
    "end": "2671079"
  },
  {
    "text": "That is another way of getting\nthe max of the Q function in a continuous space. The main reason is because\nit was much simpler",
    "start": "2671080",
    "end": "2677650"
  },
  {
    "text": "to do it this way. And we didn't want to\nhave a separate actor. And there is actually\na lot of instabilities",
    "start": "2677650",
    "end": "2682780"
  },
  {
    "text": "that come with using an\nactor-critic method like this. So we just wanted to make it\nsimple, easy to parallelize.",
    "start": "2682780",
    "end": "2688000"
  },
  {
    "text": "And that was good enough.  All right. Cool.",
    "start": "2688000",
    "end": "2694930"
  },
  {
    "text": "All right. So that's the end of the\nrecap and Q-learning. Let's dive into the\nmulti-task stuff, the exciting part about\nimitation and policy gradients.",
    "start": "2694930",
    "end": "2704160"
  },
  {
    "text": "So we talked a little bit about\nimitation learning before. We had this example where\nwe had human drivers collect",
    "start": "2704160",
    "end": "2710339"
  },
  {
    "text": "some images and the actions\nthat they were performing. And we would put them\nin the training data set",
    "start": "2710340",
    "end": "2717540"
  },
  {
    "text": "and then do supervised learning\non it doing imitation learning. So now, the question is, how do\nwe do this with multiple tasks?",
    "start": "2717540",
    "end": "2726340"
  },
  {
    "text": "All right. So first question\nis, well, how do we optimize multi-task\nimitation learning setting?",
    "start": "2726340",
    "end": "2732820"
  },
  {
    "text": "And the answer is really simple. And this is the reminder from\nlecture 2 that Chelsea gave.",
    "start": "2732820",
    "end": "2739980"
  },
  {
    "text": "We just use the vanilla\nmulti-task objective where we minimize the loss\nacross different tasks.",
    "start": "2739980",
    "end": "2745484"
  },
  {
    "text": "All right? So we can have some kind of way\nof specifying which task it is. And then we can compute the\nloss for each one of the tasks",
    "start": "2745485",
    "end": "2752100"
  },
  {
    "text": "and take the sum of\nthose and minimize that. So it's exactly the same\nas supervised learning.",
    "start": "2752100",
    "end": "2759100"
  },
  {
    "text": "So we can use same\narchitectures, stratified sampling,\neverything that you've learned about soft or\nhard weight sharing,",
    "start": "2759100",
    "end": "2765790"
  },
  {
    "text": "and so on, very simple, OK? So the other question is, how do\nyou specify a task, all right?",
    "start": "2765790",
    "end": "2772950"
  },
  {
    "text": "So let's say you're trying\nto teach a robot something. You need to specify\nmultiple tasks. Using imitation learning,\nyou collect demonstrations",
    "start": "2772950",
    "end": "2779347"
  },
  {
    "text": "for each one of them. How would you specify the task? Any ideas? ",
    "start": "2779347",
    "end": "2789549"
  },
  {
    "text": "How would you\ncommunicate to a robot what task you want it to do? Yes. [INAUDIBLE] with different\npoints [INAUDIBLE]",
    "start": "2789550",
    "end": "2804128"
  },
  {
    "text": "That's right. But you have to specify the\ntask to a robot somehow. So what would you use for that? ",
    "start": "2804128",
    "end": "2812180"
  },
  {
    "text": "How would you describe the task? Yes. By classifying the\nreward function.",
    "start": "2812180",
    "end": "2819150"
  },
  {
    "text": "By classifying a\nreward function? By classifying the option\nof a reward function. Right.",
    "start": "2819150",
    "end": "2824290"
  },
  {
    "text": "So different reward function--\ndifferent tasks would have different reward functions. But still, you would\nhave a neural network that would need to be\nconditioned on a task, right?",
    "start": "2824290",
    "end": "2831670"
  },
  {
    "text": "And this conditioning needs to\nbe specified somehow, right? You need to tell\nthe robot, or you need to explain what kind\nof task you want it to do.",
    "start": "2831670",
    "end": "2839650"
  },
  {
    "text": "It would have different\nrewards for all of these tasks. But how would the robot\nknow which task to perform?",
    "start": "2839650",
    "end": "2846170"
  },
  {
    "text": "Yes. You can use a one-hot vector. Yeah. You can use a one-hot\nvector, right. So that would be just one-hot\ntelling you which task to run.",
    "start": "2846170",
    "end": "2854400"
  },
  {
    "text": "Yes. If you use demos,\nwhatever it is, language. Yeah. Right. Yeah.",
    "start": "2854400",
    "end": "2859760"
  },
  {
    "text": "You can use demos. So you would perform\na demonstration. And that would be-- some kind of\nembedding of the demonstration would be the\ndescription of a task.",
    "start": "2859760",
    "end": "2865980"
  },
  {
    "text": "You can specify it\nby natural language. You can specify it by a\nvideo, by a goal image, all kinds of different things.",
    "start": "2865980",
    "end": "2872110"
  },
  {
    "text": "And these different things\nhave different tradeoffs. So there was this one\nwork that was published",
    "start": "2872110",
    "end": "2877230"
  },
  {
    "text": "at CoRL by Eric Jang et al. called BC-Z. And the way\nit works is the following.",
    "start": "2877230",
    "end": "2884710"
  },
  {
    "text": "So they collected a big\ndata set of demonstrations, which resulted in this\ndiverse multi-task data set.",
    "start": "2884710",
    "end": "2891720"
  },
  {
    "text": "So they are doing\nmany different tasks. And then, they were actually\nconditioning the policy on two different\nthings-- on a video,",
    "start": "2891720",
    "end": "2897792"
  },
  {
    "text": "let's say it's a human\nvideo of performing a task, and a natural\nlanguage description as well.",
    "start": "2897792",
    "end": "2904518"
  },
  {
    "text": "And then, they\nwould-- based on this, they would test how well\ndoes it generalize to a task that it's never seen\nbefore, all right?",
    "start": "2904518",
    "end": "2910420"
  },
  {
    "text": "So they would generalize some\ntasks that haven't been seen, which is kind of a\nnovel thing to do.",
    "start": "2910420",
    "end": "2918110"
  },
  {
    "text": "Their architecture\nlooks like this. This is the state. So they take the\nimage from the camera.",
    "start": "2918110",
    "end": "2924650"
  },
  {
    "text": "Then they random crop\nand downsample it so they have this\nsmaller image, which goes through a\npre-trained ResNet network",
    "start": "2924650",
    "end": "2930907"
  },
  {
    "text": "and then does a few\noperations and eventually ends up in outputs the action\nthat the robot should do.",
    "start": "2930907",
    "end": "2936380"
  },
  {
    "text": "And then they have this\nway of encoding the task. And they have two\ndifferent ways here. So they can either\ntake the human video.",
    "start": "2936380",
    "end": "2941930"
  },
  {
    "text": "So they record human videos\nthat are paired to the tasks that the robot was trying to do.",
    "start": "2941930",
    "end": "2947030"
  },
  {
    "text": "They pass it through\na video encoder. And this ends up being-- and this ends up being\nsome kind of embedding",
    "start": "2947030",
    "end": "2952100"
  },
  {
    "text": "vector in some latent space. And they also have a language,\nnatural language description",
    "start": "2952100",
    "end": "2959180"
  },
  {
    "text": "of the task, which goes\nthrough a language encoder-- in this case, I believe this was\na pre-trained language model--",
    "start": "2959180",
    "end": "2965599"
  },
  {
    "text": "which ends up in some space,\nin this embedding space. And then they use\nfilm conditioning to condition this ResNet on the\ntask that they want it to do.",
    "start": "2965600",
    "end": "2975359"
  },
  {
    "text": "In terms of the loss they\nuse, so they take the-- they do the minimum\nnegative log likelihood. So they're minimizing\nthis over all the tasks",
    "start": "2975360",
    "end": "2982065"
  },
  {
    "text": "that we just discussed. And they have the\nbehavioral cloning loss, where ZI here\ndepicts the embedding",
    "start": "2982065",
    "end": "2988080"
  },
  {
    "text": "of the task you're in. And then in addition to\nthis, for this to work well, they added one more loss, which\nis the language regression",
    "start": "2988080",
    "end": "2995680"
  },
  {
    "text": "loss. So they're trying to\nminimize the distance between the video\nencoder and the language encoder for a specific task.",
    "start": "2995680",
    "end": "3002342"
  },
  {
    "text": "And this makes the video\nencoder a little bit better. ",
    "start": "3002342",
    "end": "3007420"
  },
  {
    "text": "All right. So in terms of the results,\nhere are some of the tasks that held-out tasks today that\nthey've been evaluating it on.",
    "start": "3007420",
    "end": "3016079"
  },
  {
    "text": "And you can see that some\nof them kind of worked. Some of them work fairly\nwell, like 82% or so.",
    "start": "3016080",
    "end": "3021540"
  },
  {
    "text": "Some of them don't work at\nall, like some of the language descriptions that you see here. Like, place banana\nin a ceramic cup.",
    "start": "3021540",
    "end": "3028140"
  },
  {
    "text": "And overall, the held-out\ntask success is just 32%. But keep in mind that this\nis the first time where we're",
    "start": "3028140",
    "end": "3035340"
  },
  {
    "text": "actually trying to evaluate\nsomething on tasks that you've never seen before. So they trained on a completely\ndifferent set of tasks.",
    "start": "3035340",
    "end": "3040980"
  },
  {
    "text": "So just here, we are testing\ngeneralization to new tasks. So there is non-zero success\nfor 20 of the 28 tasks.",
    "start": "3040980",
    "end": "3049890"
  },
  {
    "text": "And the average was--\nthe average success of held-out tasks was 32%. And here are some examples\nof what this actually does.",
    "start": "3049890",
    "end": "3057430"
  },
  {
    "text": "So here is a task that\nit's never seen before. For instance, push purple\nball across the table. And you can see\nthat it does that.",
    "start": "3057430",
    "end": "3064333"
  },
  {
    "text": "Right here, another\ntest that it's never been seen before, place\nball in the tray, all right?",
    "start": "3064333",
    "end": "3069540"
  },
  {
    "text": "So in this case, they\nuse natural language as the task specification. And this allows them to do some\nzero shot generalization where",
    "start": "3069540",
    "end": "3078330"
  },
  {
    "text": "they can specify a\ndifferent X command, and the robot does that. All right.",
    "start": "3078330",
    "end": "3084290"
  },
  {
    "text": "Cool. So now, we also talk about\nreinforcement learning. And we talk a little\nbit about what is a task in\nreinforcement learning.",
    "start": "3084290",
    "end": "3090980"
  },
  {
    "text": "And we talked about this\ndefinition, the MDP, where we have the\nstate space, the action space, the initial state\ndistribution, the dynamics,",
    "start": "3090980",
    "end": "3097820"
  },
  {
    "text": "and the reward. So an alternative view that\nwas already said in the class",
    "start": "3097820",
    "end": "3103850"
  },
  {
    "text": "was to add the task identifier\nas part of the state.",
    "start": "3103850",
    "end": "3109110"
  },
  {
    "text": "So we can just say that our\noriginal state was this s bar. And then we'll add our\ntask description, ZI,",
    "start": "3109110",
    "end": "3116960"
  },
  {
    "text": "either natural language\nor something else. And this tuple would\nbe our new state.",
    "start": "3116960",
    "end": "3123119"
  },
  {
    "text": "So now, our new state\nalso encompasses what task you are trying to do. And if we take\nthat view, then we",
    "start": "3123120",
    "end": "3129440"
  },
  {
    "text": "can just say that this\nmulti-task reinforcement learning problem is\njust another MDP.",
    "start": "3129440",
    "end": "3134480"
  },
  {
    "text": "So in that case, the MDP\nwould be the state space is equal to the union of all the\nstate spaces of all the tasks.",
    "start": "3134480",
    "end": "3140990"
  },
  {
    "text": "The action space is the union\nof all the action spaces. The initial state\ndistribution is the mixture. And then it has some dynamics\nand the reward function.",
    "start": "3140990",
    "end": "3150300"
  },
  {
    "text": "So it can be cast\nas a standard MDP.  All right. We also talked about the\ngoal of reinforcement",
    "start": "3150300",
    "end": "3156839"
  },
  {
    "text": "learning in the\nprevious lecture. This was the single task case. The multi-task case\nis very similar.",
    "start": "3156840",
    "end": "3161980"
  },
  {
    "text": "So it's the same as\nbefore, except now we have this task identifier that\ntells us which task we're in.",
    "start": "3161980",
    "end": "3168510"
  },
  {
    "text": "An example of this\nwould be one-hot task ID or a language description\nor a desired goal image.",
    "start": "3168510",
    "end": "3173740"
  },
  {
    "text": "Now, if we use the--\nor the goal state. If we use the\ndesired goal state,",
    "start": "3173740",
    "end": "3179370"
  },
  {
    "text": "then our ZI, our\ndescription of a task, would be equal to some\nstate SG, all right?",
    "start": "3179370",
    "end": "3184590"
  },
  {
    "text": "So it would be in the\nsame dimensionality in the same space as\nthe states themselves. And this is actually a\nreally important case",
    "start": "3184590",
    "end": "3191340"
  },
  {
    "text": "that we'll spend a\nlittle bit more time on. And this is what\nwe'll be referring to as goal conditioned\nreinforcement learning.",
    "start": "3191340",
    "end": "3197770"
  },
  {
    "text": "All right? So we're conditioning\non the goal that we actually want to see in\nthe world, we want to achieve.",
    "start": "3197770",
    "end": "3204580"
  },
  {
    "text": "So what is the\nreward in this case? It could be the same as before. Or for goal-conditioned\nRL, the reward",
    "start": "3204580",
    "end": "3210540"
  },
  {
    "text": "could be just the distance\nor the negative distance between our current\nstate and the state that we want to get to.",
    "start": "3210540",
    "end": "3217619"
  },
  {
    "text": "So it will be just the negative\ndistance between the two. Some examples of that would\nbe the Euclidean distance",
    "start": "3217620",
    "end": "3225690"
  },
  {
    "text": "or a sparse reward. So if the two states are exactly\nthe same, you get reward. Otherwise, you don't,\nor something else.",
    "start": "3225690",
    "end": "3232850"
  },
  {
    "text": "So this is the goal-conditioned\nRL case, the multi-task CoRL case, fairly simple. ",
    "start": "3232850",
    "end": "3239675"
  },
  {
    "text": "All right.  Yeah. So in terms of the\nconditioning, we already",
    "start": "3239675",
    "end": "3248050"
  },
  {
    "text": "discussed two different\nways of specifying the task to an agent. And it turns out\nthat it actually",
    "start": "3248050",
    "end": "3254605"
  },
  {
    "text": "matters how you specify it. And it's not that-- it's not just that it\nmatters because it's maybe",
    "start": "3254605",
    "end": "3259750"
  },
  {
    "text": "more convenient\nto talk to a robot than specifying a one-hot\nvector to a robot. But it actually also\nhelps with learning.",
    "start": "3259750",
    "end": "3267819"
  },
  {
    "text": "So to discuss this, we'll\nlook at this benchmark called Meta-World, which consists\nof different 50 tasks, 50",
    "start": "3267820",
    "end": "3274953"
  },
  {
    "text": "different manipulation\ntasks, that have the same robot, the same\nstate space and action space, but they do slightly\ndifferent things.",
    "start": "3274953",
    "end": "3280660"
  },
  {
    "text": "All right, simple manipulation\ntasks that you see here. And it turns out\nthat, when we evaluate",
    "start": "3280660",
    "end": "3286510"
  },
  {
    "text": "a whole bunch of different\nmulti-task versions of popular multi-task\nreinforcement learning",
    "start": "3286510",
    "end": "3293470"
  },
  {
    "text": "algorithms, they\ndon't work that well. So even though you can solve\neach one of those tasks separately quite well, if\nyou put them all together",
    "start": "3293470",
    "end": "3300880"
  },
  {
    "text": "and you try to learn\nthem all together, it doesn't work that well. So the success rate\nis like 50% or so.",
    "start": "3300880",
    "end": "3309100"
  },
  {
    "text": "So there's this idea introduced\nby Sodhani et al. called CARE where instead of specifying\ntasks as one-hot vectors, which",
    "start": "3309100",
    "end": "3316930"
  },
  {
    "text": "is what was done\nhere before, they would introduce so-called\nmetadata, metadata right here.",
    "start": "3316930",
    "end": "3324260"
  },
  {
    "text": "So the metadata would be a\nshort language description telling you what the\ntask actually is. So for instance, it would say,\ninsert the peg inside the hole,",
    "start": "3324260",
    "end": "3334030"
  },
  {
    "text": "or something like this. Now, they would\nuse that metadata. They would pass it through\na pre-trained language model",
    "start": "3334030",
    "end": "3340660"
  },
  {
    "text": "and then have another\nfeedforward neural net to get some kind of\ncontext, all right?",
    "start": "3340660",
    "end": "3347980"
  },
  {
    "text": "And they would take\nthis context vector. And then separately,\nthey would have the state that has to pass\nthrough K different encoders",
    "start": "3347980",
    "end": "3355360"
  },
  {
    "text": "that are being trained. And then they would look at the\noutput of all of these encoders and then do attention over\nthose according to the context",
    "start": "3355360",
    "end": "3364818"
  },
  {
    "text": "that they got from the metadata\nfrom the pre-trained language model. So kind of the language\nmodel tells you what part of the\ncontext or which",
    "start": "3364818",
    "end": "3371920"
  },
  {
    "text": "encoders you should be\npaying attention to. Then they pass it\nthrough an MLP, so",
    "start": "3371920",
    "end": "3377740"
  },
  {
    "text": "a feedforward neural net. And then they combine these two. And then this is the\nrepresentation of the task.",
    "start": "3377740",
    "end": "3383380"
  },
  {
    "text": "And then they run it through\nthe policy algorithm. So it's a different way\nof conditioning, right?",
    "start": "3383380",
    "end": "3389530"
  },
  {
    "text": "So they're still running the\nsame multi-task reinforcement learning. But now, the task is\nspecified by the metadata,",
    "start": "3389530",
    "end": "3395470"
  },
  {
    "text": "and there is some kind\nof trickery as to how this is being processed.",
    "start": "3395470",
    "end": "3400660"
  },
  {
    "text": "But the important part is\nthat there is some language description that tells\nyou the connection between different tasks.",
    "start": "3400660",
    "end": "3407355"
  },
  {
    "text": "And it turns out that it\nworks a little bit better. And they actually showed\na few different ablations in their paper showing how\nthis works and why that is.",
    "start": "3407355",
    "end": "3415510"
  },
  {
    "text": "But one particular one\nthat I found interesting is right here. So they tested on 10 tasks.",
    "start": "3415510",
    "end": "3420800"
  },
  {
    "text": "And these are the\ndescriptions of the tasks. And here, they show this\nconfusion matrix showing you",
    "start": "3420800",
    "end": "3427420"
  },
  {
    "text": "the similarities between\nthe different tasks. So in here, this matrix\nright here, B, this",
    "start": "3427420",
    "end": "3435520"
  },
  {
    "text": "is the cosine similarity\nbetween the pre-trained language embeddings of those tasks. So we can see that tasks\nnumber 9 and 10 seem",
    "start": "3435520",
    "end": "3441700"
  },
  {
    "text": "to be very similar. And this has something to\ndo with pushing a window. So the pre-trained\nlanguage model",
    "start": "3441700",
    "end": "3447100"
  },
  {
    "text": "puts them kind of\nclose together. C is the case with\ntheir six encoders,",
    "start": "3447100",
    "end": "3453040"
  },
  {
    "text": "so they're a CARE model. And they can see that\nthere is actually a lot of similarities between\nthe different groups of tasks.",
    "start": "3453040",
    "end": "3458890"
  },
  {
    "text": "So not only 9 and\n10 are similar, but it looks like 4, 5,\nand 6 are fairly similar.",
    "start": "3458890",
    "end": "3464480"
  },
  {
    "text": "Right? So 4, 5, and 6 is open a\ndoor with a revolving joint, open a drawer, push\nand close a drawer.",
    "start": "3464480",
    "end": "3470530"
  },
  {
    "text": "So it seems like\nthe motions that you have to perform to perform\nthese tasks and also the way",
    "start": "3470530",
    "end": "3476170"
  },
  {
    "text": "that they're described\nshare a lot of similarities. So they can be mapped\nkind of together, and metadata helps with that.",
    "start": "3476170",
    "end": "3484010"
  },
  {
    "text": "And then, they also show what\nthis matrix looks like if they",
    "start": "3484010",
    "end": "3489610"
  },
  {
    "text": "don't use metadata at all. So there is no similarities\nbetween the tasks. So it seems that the right\ncontext not only can help",
    "start": "3489610",
    "end": "3496765"
  },
  {
    "text": "us interact with the robot. But it can actually\nprovide interesting context for the training\nalgorithm itself.",
    "start": "3496765",
    "end": "3502720"
  },
  {
    "text": "It shows certain similarities\nbetween the tasks. We describe tasks in certain way\nbecause there are similarities,",
    "start": "3502720",
    "end": "3508720"
  },
  {
    "text": "how we perceive the world, like\nif you push something or pick something or pull something. The reason why these words\nare sort of-- seem similar",
    "start": "3508720",
    "end": "3516670"
  },
  {
    "text": "is because these correspond\nto similar activities. And the model can actually\ntake advantage of that. ",
    "start": "3516670",
    "end": "3524310"
  },
  {
    "text": "All right. I'm going to skip some of\nthis because we're running out of time.",
    "start": "3524310",
    "end": "3529750"
  },
  {
    "text": "But I also wanted to\ntell you a little bit about the difficulties of\nmulti-task reinforcement learning.",
    "start": "3529750",
    "end": "3535740"
  },
  {
    "text": "So when we look at Meta-World\nfor the first time, we saw that if we train all\nthe tasks independently--",
    "start": "3535740",
    "end": "3542570"
  },
  {
    "text": "so each task by itself for\none of those 50 tasks-- it actually works\nmuch better than if we",
    "start": "3542570",
    "end": "3547910"
  },
  {
    "text": "train them all together\nusing one-hot conditioning or even using any\nother conditioning.",
    "start": "3547910",
    "end": "3553530"
  },
  {
    "text": "So we tried\ndifferent hypotheses. We tried making the\nnetwork much bigger, collecting much\nmore data, trying",
    "start": "3553530",
    "end": "3558640"
  },
  {
    "text": "all kinds of different things. But none of them\nreally seemed to help. So then we looked\nat one more thing.",
    "start": "3558640",
    "end": "3564650"
  },
  {
    "text": "We looked at the gradient\ncosine similarities between different tasks. So in particular, we took\nthe task 0 and task 1",
    "start": "3564650",
    "end": "3571069"
  },
  {
    "text": "and just plot over the\nnumber of gradient steps. What's the gradient\ncosine similarity? So the gradients\nfor different tasks,",
    "start": "3571070",
    "end": "3577520"
  },
  {
    "text": "how do they relate\nto each other? And it turned out that\nit's all over the place. So sometimes, it's\npositive, so that",
    "start": "3577520",
    "end": "3583355"
  },
  {
    "text": "means that they're aligned. Sometimes, they\nare perpendicular, which means the cosine is zero. And sometimes,\nthey are negative.",
    "start": "3583355",
    "end": "3589220"
  },
  {
    "text": "So one task is pulling\nin one direction. The other task is pulling\nin another direction, which kind of seems to be a problem.",
    "start": "3589220",
    "end": "3596025"
  },
  {
    "text": "So then, we came up with\nthis very simple algorithm which says, if two\ngradients conflict, then let's project each onto\nthe normal plane of the other.",
    "start": "3596025",
    "end": "3604070"
  },
  {
    "text": "And if they don't conflict,\nwe don't do anything. So it's a very\nsimple algorithm that looks at the gradient vectors\nfor two different tasks.",
    "start": "3604070",
    "end": "3611420"
  },
  {
    "text": "And if they are conflicting-- so\nif this angle here is more than 90 degrees-- it would project\none of the gradients",
    "start": "3611420",
    "end": "3617170"
  },
  {
    "text": "onto the normal\nplane of the other, and the same of the other one. And if there are\nnon-conflicting, we won't do anything.",
    "start": "3617170",
    "end": "3624269"
  },
  {
    "text": "All right. So we apply this. So first thing we started seeing\nis that we call this algorithm",
    "start": "3624270",
    "end": "3630029"
  },
  {
    "text": "PCGrad. And this is the work\nled by Kevin Yu.",
    "start": "3630030",
    "end": "3635262"
  },
  {
    "text": "And the first thing\nwe start to see is obviously, because of the\ndefinition of this algorithm,",
    "start": "3635262",
    "end": "3640588"
  },
  {
    "text": "we don't have conflicting\nratings anymore, right? Like every time the gradient is\nconflicting, we'll project it and it's not\nconflicting anymore.",
    "start": "3640588",
    "end": "3646780"
  },
  {
    "text": "So now, the plot looks\nlike the one in green. But then, in addition,\nwhat we started seeing",
    "start": "3646780",
    "end": "3652590"
  },
  {
    "text": "is that the performance now\nwith the PCGrad in purple is actually much, much better\nthan what we've seen before.",
    "start": "3652590",
    "end": "3658780"
  },
  {
    "text": "So not only it's\nbetter than the things that we've seen before when\nwe trained them jointly, but it's also better than if we\ntrain the tasks independently,",
    "start": "3658780",
    "end": "3665835"
  },
  {
    "text": "right? And that seems to indicate\nthat there is actually quite a lot of structure\nthat you can take advantage",
    "start": "3665835",
    "end": "3672660"
  },
  {
    "text": "of in the Meta-World task,\nwhich is not that surprising. These are similar\nmanipulation tasks.",
    "start": "3672660",
    "end": "3677730"
  },
  {
    "text": "And by fiddling a little\nbit with the optimization, you might be able to\nextract those advantages",
    "start": "3677730",
    "end": "3683640"
  },
  {
    "text": "and actually see that. And here, I think\nwe're just scratching the surface of the problems\nthat you see when optimizing",
    "start": "3683640",
    "end": "3691020"
  },
  {
    "text": "for multiple tasks and so on. And it shows that even the very\nsimple algorithms can actually",
    "start": "3691020",
    "end": "3696329"
  },
  {
    "text": "improve the performance\nquite significantly. ",
    "start": "3696330",
    "end": "3701630"
  },
  {
    "text": "All right. So to summarize the\nmulti-task RL algorithm, we change a policy\nto now include also",
    "start": "3701630",
    "end": "3709150"
  },
  {
    "text": "the task information, Zi. We can do the same\nwith the Q function. The Q function can\nnow also incorporate",
    "start": "3709150",
    "end": "3715480"
  },
  {
    "text": "the task information Zi. And it's analogous to\nmulti-task supervised learning. So we can use the same things\nagain, stratified sampling,",
    "start": "3715480",
    "end": "3722830"
  },
  {
    "text": "soft/hard weight\nsharing, et cetera. And if it's still a Markov\ndecision process, well,",
    "start": "3722830",
    "end": "3730030"
  },
  {
    "text": "why can't we just apply\nstandard RL algorithms? And so far, we've talked\nabout, more or less, standard RL algorithms, maybe\nwith the exception of PCGrad.",
    "start": "3730030",
    "end": "3738700"
  },
  {
    "text": "And the answer is,\nyeah, you can do that. And it actually\nworks fairly well. But we can often do\nmuch better than this.",
    "start": "3738700",
    "end": "3745910"
  },
  {
    "text": "And that's what\nwe'll discuss next. And to think about this, what\ndo you think we could do?",
    "start": "3745910",
    "end": "3752859"
  },
  {
    "text": "What is different\nabout reinforcement learning in contrast\nto supervised learning?",
    "start": "3752860",
    "end": "3757900"
  },
  {
    "text": "What are the other aspects\nthat we can control that could help us here?",
    "start": "3757900",
    "end": "3763150"
  },
  {
    "text": "Any ideas? Yes. We can choose where\nwant to explore.",
    "start": "3763150",
    "end": "3768515"
  },
  {
    "text": "We can choose where\nwe want to explore. That's right. Yeah, exactly. So the data distribution is\ncontrolled by the agent itself.",
    "start": "3768515",
    "end": "3774475"
  },
  {
    "text": " So the question that\nwe'll try to answer next is, can we share the data in\naddition to sharing the waste?",
    "start": "3774475",
    "end": "3781800"
  },
  {
    "text": "And how should we\nshare the data, or how should we\nexplore so that we",
    "start": "3781800",
    "end": "3787770"
  },
  {
    "text": "can take more advantage of\nthe multi-task RL algorithms?",
    "start": "3787770",
    "end": "3793180"
  },
  {
    "text": "All right. So this is where we talk\nabout multi-task Q-learning. Are there any questions\nat this point? ",
    "start": "3793180",
    "end": "3802300"
  },
  {
    "text": "All right. Cool. So to explain this\ndata sharing problem,",
    "start": "3802300",
    "end": "3808400"
  },
  {
    "text": "let's consider the\nfollowing example. Let's say we are playing\nhockey, and we want to learn two different tasks.",
    "start": "3808400",
    "end": "3815090"
  },
  {
    "text": "One task is passing. And task number two is shooting\ngoals, or scoring goals,",
    "start": "3815090",
    "end": "3820880"
  },
  {
    "text": "I guess. And let's say you\ncollected some experience where you were trying to\nmake a good pass, but--",
    "start": "3820880",
    "end": "3829410"
  },
  {
    "text": "oh, sorry. You were trying to\nshoot at the goal, but accidentally you\nperformed a good pass, right? So it's not what you intended,\nbut it just happened this way.",
    "start": "3829410",
    "end": "3837628"
  },
  {
    "text": "So what can you do in this case\nin the reinforcement learning setup? Well, you can store the\nexperience as normal.",
    "start": "3837628",
    "end": "3843770"
  },
  {
    "text": "So you can store that\nexperience of performing a pass, but you were trying\nto shoot the goal as just an example of trying\nto shoot a goal, right,",
    "start": "3843770",
    "end": "3852020"
  },
  {
    "text": "the example of task one-- or sorry, task two. But in addition to this, you\ncan also relabel that experience",
    "start": "3852020",
    "end": "3858950"
  },
  {
    "text": "with task two ID. So just you can pretend that\nyou were trying to shoot a goal from the very beginning.",
    "start": "3858950",
    "end": "3865460"
  },
  {
    "text": "This is-- sorry. I messed it up a little bit--\nso relabel the experience with task one ID and the reward\nand store that as well, right?",
    "start": "3865460",
    "end": "3874250"
  },
  {
    "text": "So you're trying to shoot a\ngoal, and you ended up passing. So you can store this as an\nexample of shooting a goal.",
    "start": "3874250",
    "end": "3879650"
  },
  {
    "text": "But you can also store it\nas an example of passing. So you can kind of double\nthe size of the data set now.",
    "start": "3879650",
    "end": "3884840"
  },
  {
    "text": " And this is what we're referring\nto as hindsight relabeling",
    "start": "3884840",
    "end": "3891059"
  },
  {
    "text": "or hindsight experience\nreplay, or in short, HER. ",
    "start": "3891060",
    "end": "3896390"
  },
  {
    "text": "Cool. So the way that the algorithm\nwould work with this-- and this is actually quite\nimportant, and this will be--",
    "start": "3896390",
    "end": "3902690"
  },
  {
    "text": "partly this is also the\ntopic of homework 3-- it would work as following.",
    "start": "3902690",
    "end": "3909390"
  },
  {
    "text": "We would collect the data\nusing some kind of policy, as we do in\nreinforcement learning. And this would result in some\nstate sequence, some action",
    "start": "3909390",
    "end": "3917150"
  },
  {
    "text": "sequence, and some\ndescription of a task. So we were trying\nto perform task I.",
    "start": "3917150",
    "end": "3923510"
  },
  {
    "text": "And we have the reward\nassociated with that task. Then, we'll store that\ndata in the replay buffer,",
    "start": "3923510",
    "end": "3929480"
  },
  {
    "text": "as we usually do. And then we would, in addition,\nperform this hindsight relabeling procedure.",
    "start": "3929480",
    "end": "3935720"
  },
  {
    "text": "So relabel the experience\nthat we just collected, but we'll relabel it for a\ndifferent task, for a task TJ,",
    "start": "3935720",
    "end": "3941235"
  },
  {
    "text": "all right? So it's the J task,\nnot the I task that we originally\ncollected the data for.",
    "start": "3941235",
    "end": "3947540"
  },
  {
    "text": "So now, that data\nwould look like this. It would still be the\nsame sequence of actions,",
    "start": "3947540",
    "end": "3952830"
  },
  {
    "text": "same sequence of states. But now, rather than\nsaying that this was Zi, we would just say this was\nZj, right, like I was actually",
    "start": "3952830",
    "end": "3959690"
  },
  {
    "text": "trying to pass. And then, we would also\nhave to relabel the reward as the reward that is\nthe reward for task J.",
    "start": "3959690",
    "end": "3969028"
  },
  {
    "text": "So we have to now score\nit according to did you pass or did you not pass. ",
    "start": "3969028",
    "end": "3975240"
  },
  {
    "text": "And then, we can store the\nrelabeled data in the replay buffer as well and\nthen update the policy",
    "start": "3975240",
    "end": "3981930"
  },
  {
    "text": "and do this over and over again. There's a question. Yeah. [INAUDIBLE] relabeling if our\naction was actually used for--",
    "start": "3981930",
    "end": "3990292"
  },
  {
    "text": "meets a certain threshold\nfor this other task, or do we always do it? Yeah. That's a great question. So the question is,\nwhen do we do this?",
    "start": "3990292",
    "end": "3997000"
  },
  {
    "text": "When do we do this relabeling? Do we do it only when\nwe perform something useful for this\nother task or not? Yeah.",
    "start": "3997000",
    "end": "4002420"
  },
  {
    "text": "And this is actually one of the\nbig questions in reinforcement learning as to how to do it.",
    "start": "4002420",
    "end": "4007590"
  },
  {
    "text": "So let's consider a\nfew options, what task. Which task should you choose? So one idea would be to\nchoose it randomly, just",
    "start": "4007590",
    "end": "4016220"
  },
  {
    "text": "relabel it to any other task. But the idea that\nyou had was, well, maybe we should relabel it\nto tasks in which trajectory",
    "start": "4016220",
    "end": "4024020"
  },
  {
    "text": "gets high reward. So only if you actually\npass to someone you should relabel\nit as passing.",
    "start": "4024020",
    "end": "4029120"
  },
  {
    "text": "If it resulted in something\ncompletely random, then you shouldn't relabel\nthis as passing.",
    "start": "4029120",
    "end": "4034898"
  },
  {
    "text": "And there are\ndifferent options here. And these different\noptions actually have really big influence\non the final results.",
    "start": "4034898",
    "end": "4040790"
  },
  {
    "text": "There's other\noptions here as well. And here are some papers that\ntalk about different ways, how you can relabel that.",
    "start": "4040790",
    "end": "4046670"
  },
  {
    "text": "And these either use\ncertain heuristics or look at offline cases or\noffline reinforcement learning",
    "start": "4046670",
    "end": "4051830"
  },
  {
    "text": "cases and so on. But usually, I think they\ncorrespond to something that you described, which\nis relabel to tasks that",
    "start": "4051830",
    "end": "4058023"
  },
  {
    "text": "have a high chance\nof achieving a reward or to tasks that seem like\nsomething that you could have done had you tried that task.",
    "start": "4058023",
    "end": "4065550"
  },
  {
    "text": "Yes. There's a question. In this case, do all\ntasks have same dynamics? Yes.",
    "start": "4065550",
    "end": "4070700"
  },
  {
    "text": "So we assume that they\nhave the same dynamics. That's a great point. So when can we apply relabeling?",
    "start": "4070700",
    "end": "4077450"
  },
  {
    "text": "So reward function\nneeds to be known, because we need to evaluate now\nthe state for a different task.",
    "start": "4077450",
    "end": "4086780"
  },
  {
    "text": "Dynamics need to be\nconsistent, as you said, across goals and tasks. And then, we also need to be\nusing an off-policy algorithm.",
    "start": "4086780",
    "end": "4093619"
  },
  {
    "text": "There is actually a way to use\non-policy algorithms with that as well. But off-policy algorithms\nwork much better. ",
    "start": "4093620",
    "end": "4101640"
  },
  {
    "text": "All right. So one more example as to why\noff- or on-policy, how do they compare here.",
    "start": "4101640",
    "end": "4107818"
  },
  {
    "text": "So let's look at\nanother example. So let's say one task\nis to close the drawer, and the other task is to\nopen a drawer, all right?",
    "start": "4107819",
    "end": "4114710"
  },
  {
    "text": "These are the two tasks. So you get high reward when the\nwhen the drawers are closed.",
    "start": "4114710",
    "end": "4120395"
  },
  {
    "text": "In the other case, you\nget the highest reward when the drawer is fully open. ",
    "start": "4120395",
    "end": "4126028"
  },
  {
    "text": "So now, the question\nis, can we use episodes from drawer opening tasks\nfor drawer closing tasks?",
    "start": "4126029",
    "end": "4131540"
  },
  {
    "text": "But more importantly, how\ndoes that answer change for Q-learning versus\npolicy gradient? And for the purpose\nof this exercise,",
    "start": "4131540",
    "end": "4138420"
  },
  {
    "text": "let's assume that if you're\ntrying to close a drawer, you will not\naccidentally open it. Or if you're trying\nto open a drawer,",
    "start": "4138420",
    "end": "4144810"
  },
  {
    "text": "you'll not\naccidentally close it. All right? So these sets are kind\nof completely orthogonal. You can't succeed at a task\nby trying the other task.",
    "start": "4144810",
    "end": "4152744"
  },
  {
    "text": "So what do you think? Can you use the episodes\nfrom one task for the other? And how does this answer change\nfor Q-learning and policy",
    "start": "4152745",
    "end": "4159750"
  },
  {
    "text": "gradient? Any ideas?  Yes.",
    "start": "4159750",
    "end": "4164774"
  },
  {
    "text": "It's [INAUDIBLE]\nfor the one task, you could relabel it for\nthe other task [INAUDIBLE]",
    "start": "4164774",
    "end": "4172318"
  },
  {
    "text": "If you fail to--\nsorry, say that again. The tasks are binary. So if you fail at\none task, you've",
    "start": "4172319",
    "end": "4177700"
  },
  {
    "text": "succeeded at the other one. [INAUDIBLE] relabel [INAUDIBLE]",
    "start": "4177700",
    "end": "4188278"
  },
  {
    "text": "Right. Right. So the answer was that, well,\nit's a binary sort of task. So you either-- the drawer's\neither open or closed.",
    "start": "4188279",
    "end": "4194440"
  },
  {
    "text": "So you can always relabel\nit as one or the other. So let's say that the\ndrawer can be also half open and, in which case, you don't\nget rewards for either of them,",
    "start": "4194440",
    "end": "4201292"
  },
  {
    "text": "all right? So like the drawer is like\nthis, you don't get the rewards.",
    "start": "4201292",
    "end": "4207690"
  },
  {
    "text": "You didn't open it. You didn't close it either. Yes. If I understand\ncorrectly, we'll get",
    "start": "4207690",
    "end": "4213090"
  },
  {
    "text": "a bunch of data of us\nopening be the drawers. For the drawer closing task, the\nreward for all those examples",
    "start": "4213090",
    "end": "4220470"
  },
  {
    "text": "are going to be zero. [INAUDIBLE] for Q-learning. But you won't be able\nto use it correctly",
    "start": "4220470",
    "end": "4225716"
  },
  {
    "text": "because Q-learning requires\nyou to actually succeed for a reward. But it's good value.",
    "start": "4225716",
    "end": "4232140"
  },
  {
    "text": "Right. Yeah. So it's partly the right answer. So just to repeat\nthe answer, you're",
    "start": "4232140",
    "end": "4238140"
  },
  {
    "text": "saying that when we are trying\nto-- when we look at the data that we collected by\ntrying to close the drawer, the rewards for opening\nthe drawer for that data",
    "start": "4238140",
    "end": "4245039"
  },
  {
    "text": "will be zero. And then you said\nthat because of that, Q-learning wouldn't work.",
    "start": "4245040",
    "end": "4250095"
  },
  {
    "text": "Now, the answer is actually-- so\nthis is the really good point, that when you try to\nclose the drawer-- that's what I was trying to get\nto-- you won't open it.",
    "start": "4250095",
    "end": "4256160"
  },
  {
    "text": "So the rewards for\nthat will be zero. But now, the answer would\nchange for policy gradient",
    "start": "4256160",
    "end": "4262260"
  },
  {
    "text": "versus Q-learning. And let's take a look again at\nthe policy gradient gradient estimation.",
    "start": "4262260",
    "end": "4267610"
  },
  {
    "text": "And this here, the gradient\nis dependent on the rewards that you actually achieve. So if the rewards\nare zero for when",
    "start": "4267610",
    "end": "4274437"
  },
  {
    "text": "you are trying to close\nthe drawer data set, and you relabel it to\nopening the drawers, this rewards are zero,\nthen this gradient",
    "start": "4274437",
    "end": "4279990"
  },
  {
    "text": "will be zero, gradient? Right. So with policy gradients,\nyou can't really use the negative\nexamples of other tasks",
    "start": "4279990",
    "end": "4285630"
  },
  {
    "text": "for your current task. For Q-learning however,\nyou can use that. So Q-learning-- remember\nthe chess example--",
    "start": "4285630",
    "end": "4292600"
  },
  {
    "text": "you can actually take into\naccount any interaction. And you can use that interaction\nto update your Q function.",
    "start": "4292600",
    "end": "4299370"
  },
  {
    "text": "Yes. If the rewards were\nplus 1, minus 1, wouldn't that change things?",
    "start": "4299370",
    "end": "4304480"
  },
  {
    "text": "If the rewards were\nplus 1 and minus 1, how would that change things? Yeah.",
    "start": "4304480",
    "end": "4309660"
  },
  {
    "text": "So I think what would happen\nin practice in that case is that you would try\nto normalize the rewards",
    "start": "4309660",
    "end": "4315780"
  },
  {
    "text": "and subtract the baseline. This is how people actually\nimplement this in practice",
    "start": "4315780",
    "end": "4321010"
  },
  {
    "text": "so that their rewards\nare between 0 and 1. So then, it would still\nresult in the same thing.",
    "start": "4321010",
    "end": "4327480"
  },
  {
    "text": " All right. Cool.",
    "start": "4327480",
    "end": "4332700"
  },
  {
    "text": "So one more example\nthat I wanted to share, which is the\nextension of QT-Opt but applied to multiple tasks.",
    "start": "4332700",
    "end": "4339060"
  },
  {
    "text": "So again, we use\na bunch of robots to collect data for a whole\nbunch of different manipulation",
    "start": "4339060",
    "end": "4344070"
  },
  {
    "text": "tasks. And then we collect an episode. We label it using a success\ndetector of success or failure. And then we call this\nprocess impersonation,",
    "start": "4344070",
    "end": "4351133"
  },
  {
    "text": "but it's the same as\nhindsight labeling. We can use that episode-- we'll\nsee it again in a second-- we can use this episode\nas successes or failures",
    "start": "4351133",
    "end": "4358890"
  },
  {
    "text": "for other tasks and\nreplicate it and use it for those other tasks. And then we sample from\nall of those tasks.",
    "start": "4358890",
    "end": "4364830"
  },
  {
    "text": "And we do the QT-Opt training\njust on multiple tasks to do this. Now, we can train a set\nof manipulation tasks.",
    "start": "4364830",
    "end": "4372720"
  },
  {
    "text": "So they're relatively\nsimple manipulation tasks, such as picking, stacking,\npicking specific objects,",
    "start": "4372720",
    "end": "4378900"
  },
  {
    "text": "pushing, all kinds\nof different things. ",
    "start": "4378900",
    "end": "4384710"
  },
  {
    "text": "And first thing\nwe see is that we get 80% average improvement over\nbaselines across all the tasks.",
    "start": "4384710",
    "end": "4391090"
  },
  {
    "text": "And in particular, we get 4x\nimprovement over single-task. So if we take the\nexact same data set and try to train a single\ntask, train multiple tasks",
    "start": "4391090",
    "end": "4398590"
  },
  {
    "text": "seems to be much, much better. So it looks like\nthe algorithm is able to kind of take\nadvantage of the structure",
    "start": "4398590",
    "end": "4404530"
  },
  {
    "text": "that it discovered among\nthese different tasks. In addition to this, we\nsee very large improvements",
    "start": "4404530",
    "end": "4410470"
  },
  {
    "text": "for tasks with very little data. So the less data there\nis, the more kind of the structure\nthat you discover from learning all of them at\nthe same time, seems to help.",
    "start": "4410470",
    "end": "4419390"
  },
  {
    "text": "And then lastly, we can also\nuse a pre-trained network that we pre-train on a\nbunch of different tasks and fine tune it to a new\ntasks relatively quickly.",
    "start": "4419390",
    "end": "4426760"
  },
  {
    "text": "So we can collect data for\nthis new task in just one day and achieve 92%\nsuccess for the tasks",
    "start": "4426760",
    "end": "4432520"
  },
  {
    "text": "that we've never seen before. But we use this neural\nnetwork that we train here as a pre-training. And this is the task\nthat we tried it on,",
    "start": "4432520",
    "end": "4439160"
  },
  {
    "text": "which is this covering\na block with a towel.  All right. So we talked a little\nbit about data sharing",
    "start": "4439160",
    "end": "4445480"
  },
  {
    "text": "and multi-task Q-learning. Are there any questions\nat this point? [INAUDIBLE] in\nthe previous case,",
    "start": "4445480",
    "end": "4451659"
  },
  {
    "text": "shouldn't every episode report\na benefit of one of the tasks? Either the door's opening,\nor the door's closing.",
    "start": "4451660",
    "end": "4458380"
  },
  {
    "text": "Right. So it would benefit it\nin the Q-learning case. So just to repeat the\nquestion, in the drawer case,",
    "start": "4458380",
    "end": "4464770"
  },
  {
    "text": "wouldn't any episode actually\nbenefit either the drawer opening or the drawer closing?",
    "start": "4464770",
    "end": "4469840"
  },
  {
    "text": "It would benefit\nit as long as you use something like Q-learning. If the rewards for the task\nthat you're relabeling it to",
    "start": "4469840",
    "end": "4475240"
  },
  {
    "text": "are zero, then they\ndon't contribute to the policy gradient case. Yes. Same example. So for Q-learning, you could\nuse drawer opening data",
    "start": "4475240",
    "end": "4483820"
  },
  {
    "text": "to train for closing. But if you only have the\ndrawer opening data--",
    "start": "4483820",
    "end": "4489869"
  },
  {
    "text": "I see. --that still wouldn't\nbe enough though. Right. [INAUDIBLE] You could\nonly use it to augment. Right.",
    "start": "4489870",
    "end": "4495090"
  },
  {
    "text": "Yeah, that's a great point. So, yeah, the question was,\nin the Q-learning case, if you only had data\nfrom one of the tasks,",
    "start": "4495090",
    "end": "4501910"
  },
  {
    "text": "like closing the drawer, and\nyou've never seen opening the drawer succeed-- you don't have that\nin your data set-- Q-learning still\nwouldn't help you,",
    "start": "4501910",
    "end": "4508389"
  },
  {
    "text": "because you wouldn't be\nable to discover that. So you can use it\nto augment the data but not necessarily to kind\nof generate the data fully.",
    "start": "4508390",
    "end": "4515110"
  },
  {
    "text": "[INAUDIBLE] It shouldn't hurt. [INAUDIBLE] if you ought\nto get close [INAUDIBLE]",
    "start": "4515110",
    "end": "4522650"
  },
  {
    "text": "Yeah. I think it shouldn't hurt. I think that's right. And probably, when the door\ndrawer is completely closed",
    "start": "4522650",
    "end": "4528160"
  },
  {
    "text": "and you only have\nthe opening data-- you're only trying\nto open it-- then the reward function for the very\ninitial state would be high.",
    "start": "4528160",
    "end": "4534580"
  },
  {
    "text": "And therefore, this is an\nexample of a drawer being open, and it would\nprobably still work.",
    "start": "4534580",
    "end": "4540280"
  },
  {
    "text": "All right. So there's one last point\nthat I wanted to get to, which is actually\nthe most important.",
    "start": "4540280",
    "end": "4545500"
  },
  {
    "text": "So this is the goal-conditioned\nreinforcement learning. So we talked about this\ngoal-conditioned RL",
    "start": "4545500",
    "end": "4550950"
  },
  {
    "text": "and how we use the states as\nthe description of a task. And this is actually\nthe topic of homework 3,",
    "start": "4550950",
    "end": "4556890"
  },
  {
    "text": "so how it will work with\nhindsight relabeling. So again, we have our\ndata that we stored.",
    "start": "4556890",
    "end": "4564450"
  },
  {
    "text": "We stored the states, actions,\nand now the task description is the actually goal state\nthat we want to get to, SG.",
    "start": "4564450",
    "end": "4571380"
  },
  {
    "text": "So we do it using some policy. We store the data in\nthe replay buffer. And now, we can perform\nhindsight pre-labeling.",
    "start": "4571380",
    "end": "4577530"
  },
  {
    "text": "So now, we can\nrelabel the experience that we just collected. But we can relabel it\nusing the last state",
    "start": "4577530",
    "end": "4583590"
  },
  {
    "text": "that we achieved as the goal\nthat we want it to get to. So now, any state\nin the trajectory",
    "start": "4583590",
    "end": "4588929"
  },
  {
    "text": "is a potential task description. So what we can say\nis that you perform some kind of trajectory,\nrandom trajectory.",
    "start": "4588930",
    "end": "4595350"
  },
  {
    "text": "But you can always say that the\nlast state in the trajectory is a goal that you\nwanted to achieve. And now, you have a\nsuccessful example you",
    "start": "4595350",
    "end": "4601380"
  },
  {
    "text": "of trying to achieve that goal. So we can do this.",
    "start": "4601380",
    "end": "4606430"
  },
  {
    "text": "So now we take the last\nstate that we achieved, and we use that\nas our SG, right?",
    "start": "4606430",
    "end": "4612000"
  },
  {
    "text": "This is our task\nthat we wanted to-- where the reward is the distance\nbetween the current state",
    "start": "4612000",
    "end": "4618330"
  },
  {
    "text": "and the final state that we got. So it's kind of like every\ntime we are doing something,",
    "start": "4618330",
    "end": "4624013"
  },
  {
    "text": "we are pretending that this\nis what we wanted to do. ",
    "start": "4624013",
    "end": "4629810"
  },
  {
    "text": "All right. Then we store that\nexperience in the replay buffer and updated the\npolicy using the buffer.",
    "start": "4629810",
    "end": "4636010"
  },
  {
    "text": "And we iterate over this. Now, can you think of any\nother relabeling strategies?",
    "start": "4636010",
    "end": "4641210"
  },
  {
    "text": "Actually, we're\nrunning out of time. So let me just tell you. In terms of other\nrelabeling strategies,",
    "start": "4641210",
    "end": "4646960"
  },
  {
    "text": "we can also use any other\nstate in the trajectory, right? We don't have to only\nlook at the final state. We can look at any of the\nstates that we've encountered",
    "start": "4646960",
    "end": "4654460"
  },
  {
    "text": "and just say that\nthis was the goal and now this sub trajectory\nis the trajectory",
    "start": "4654460",
    "end": "4659920"
  },
  {
    "text": "that I wanted to get to. The result of this is that\nit helps with exploration.",
    "start": "4659920",
    "end": "4666550"
  },
  {
    "text": "And this was actually shown\nin this original Hindsight Experience Replay paper\nat NeurlPS in 2017,",
    "start": "4666550",
    "end": "4673150"
  },
  {
    "text": "where we have goal-conditioned\nRL as simulated robot manipulation domain where\nwe were trying these three different tasks--",
    "start": "4673150",
    "end": "4678760"
  },
  {
    "text": "pushing, sliding,\nand picking place. And we can see that\nwhen we apply here,",
    "start": "4678760",
    "end": "4684159"
  },
  {
    "text": "so this hindsight\nrelabeling with goal states, it actually works\nmuch better, and it can get to higher rewards, or\nit can get there much faster.",
    "start": "4684160",
    "end": "4693484"
  },
  {
    "text": "All right. I'm going to skip the\nimage observations. But I wanted to tell\nyou one more thing.",
    "start": "4693484",
    "end": "4699550"
  },
  {
    "text": "So we can actually use that\ninsight for better learning and do kind of reinforcement\nlearning without doing",
    "start": "4699550",
    "end": "4705180"
  },
  {
    "text": "reinforcement learning at all. And to actually learn a\nlittle bit more about this, I encourage you to take a look\nat this blog post or this paper",
    "start": "4705180",
    "end": "4712293"
  },
  {
    "text": "right here. So if any data is optimal,\nbecause we can always",
    "start": "4712293",
    "end": "4717809"
  },
  {
    "text": "relabel it, then can we just\nuse supervised imitation learning instead? And the truth is that you can.",
    "start": "4717810",
    "end": "4725100"
  },
  {
    "text": "So you can collect\ndata using any policy. Then you can perform\nhindsight labeling--",
    "start": "4725100",
    "end": "4730380"
  },
  {
    "text": "so in that case, we will\ntake that experience and just pretend that the\ngoal that you ended up in was the goal that you wanted\nto achieve in the first place--",
    "start": "4730380",
    "end": "4738780"
  },
  {
    "text": "and then store the relabeled\ndata and update the policy now using the\nsupervised imitation. So you're just doing\nimitation learning, right?",
    "start": "4738780",
    "end": "4745860"
  },
  {
    "text": "So we have the\nbehavioral cloning case. And we'll be just conditioning\non the goal state. And now, since\nany trajectory can",
    "start": "4745860",
    "end": "4752969"
  },
  {
    "text": "be relabeled as an example of\na successful trajectory trying to get to that goal that\nyou actually got to,",
    "start": "4752970",
    "end": "4758070"
  },
  {
    "text": "we can now do\nsupervised imitation learning goal-condition\nimitation learning on any data",
    "start": "4758070",
    "end": "4763320"
  },
  {
    "text": "set. And there is one example\nright here where--",
    "start": "4763320",
    "end": "4770130"
  },
  {
    "text": "this is the work by\nCorey Lynch et al. called \"Learning Latent\nPlans for Play\"--",
    "start": "4770130",
    "end": "4775710"
  },
  {
    "text": "where they collect\nsome kind of data where they ask humans to just\nplay with the environment. So they just record\nit using some kind",
    "start": "4775710",
    "end": "4782670"
  },
  {
    "text": "of collaboration tool. They put some VR\nheadsets on people and ask them to just do anything\nthat they find interesting",
    "start": "4782670",
    "end": "4788310"
  },
  {
    "text": "in the environment. So they have the robot play with\nthe environment a little bit. And now, they can use\nthis unstructured data,",
    "start": "4788310",
    "end": "4795179"
  },
  {
    "text": "relabel it using our\ngoal-conditioned imitation learning. And now, they can set any goal--",
    "start": "4795180",
    "end": "4800227"
  },
  {
    "text": "so this is, for\ninstance, a goal image that they are trying to get to-- and then the robot is\ntrying to do everything",
    "start": "4800227",
    "end": "4806220"
  },
  {
    "text": "possible to get to that goal. So it tries to open the\ncabinet, press the right button, and so on to get to the goal.",
    "start": "4806220",
    "end": "4812219"
  },
  {
    "text": "So you can collect this kind\nof unstructured demonstration data set without any\ntasks, anything in mind,",
    "start": "4812220",
    "end": "4817949"
  },
  {
    "text": "and relabel everything\nin hindsight afterwards, and then have this\ngoal condition policy work quite well.",
    "start": "4817950",
    "end": "4825500"
  },
  {
    "text": "All right. So we talked a little\nbit about all four of these things--\nthe Q-learning, multi-task imitation,\npolicy gradient,",
    "start": "4825500",
    "end": "4830523"
  },
  {
    "text": "multi-task Q-learning,\ngoal-conditioned RL. There is a lot of\nremaining questions",
    "start": "4830523",
    "end": "4836140"
  },
  {
    "text": "that we'll answer in\nthe next few weeks. Can we use a model? What about meta-RL?",
    "start": "4836140",
    "end": "4841700"
  },
  {
    "text": "What about exploration\nstrategies? And what if we can't\ncollect any new data?",
    "start": "4841700",
    "end": "4847330"
  },
  {
    "text": "What about hierarchies\nof tasks as well? So we'll be talking about it\nin the next set of classes.",
    "start": "4847330",
    "end": "4852685"
  },
  {
    "text": " Few additional RL resources\nif you're interested,",
    "start": "4852685",
    "end": "4858730"
  },
  {
    "text": "these are really useful. Thanks a lot for coming. And if there are any questions,\nI'm happy to take them now. Thanks.",
    "start": "4858730",
    "end": "4864090"
  },
  {
    "start": "4864090",
    "end": "4869000"
  }
]