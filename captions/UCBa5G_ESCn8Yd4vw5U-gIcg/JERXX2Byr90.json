[
  {
    "start": "0",
    "end": "4850"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4850",
    "end": "6210"
  },
  {
    "text": "This is part three in our series\non contextual representations.",
    "start": "6210",
    "end": "9530"
  },
  {
    "text": "We have a bunch of famous\ntransformer-based architectures",
    "start": "9530",
    "end": "12680"
  },
  {
    "text": "that we're going to\ntalk about a bit later.",
    "start": "12680",
    "end": "14520"
  },
  {
    "text": "But before doing that, I thought\nit would be good to pause",
    "start": "14520",
    "end": "17570"
  },
  {
    "text": "and just reflect a little\nbit on this important notion",
    "start": "17570",
    "end": "20330"
  },
  {
    "text": "of positional encoding.",
    "start": "20330",
    "end": "21470"
  },
  {
    "text": "This is an idea that\nI feel the field took",
    "start": "21470",
    "end": "24050"
  },
  {
    "text": "for granted for too long.",
    "start": "24050",
    "end": "25279"
  },
  {
    "text": "I certainly took it for\ngranted for too long.",
    "start": "25280",
    "end": "27510"
  },
  {
    "text": "And I think we now\nsee that this is",
    "start": "27510",
    "end": "29660"
  },
  {
    "text": "a crucial factor in\nshaping the performance",
    "start": "29660",
    "end": "32270"
  },
  {
    "text": "of transformer-based models.",
    "start": "32270",
    "end": "34490"
  },
  {
    "text": "Let's start by reflecting on\nthe role of positional encoding",
    "start": "34490",
    "end": "37640"
  },
  {
    "text": "in the context of\nthe transformer.",
    "start": "37640",
    "end": "39500"
  },
  {
    "text": "I think the central\nobservation is",
    "start": "39500",
    "end": "41480"
  },
  {
    "text": "that the transformer itself has\nonly a very limited capacity",
    "start": "41480",
    "end": "45350"
  },
  {
    "text": "to keep track of word order.",
    "start": "45350",
    "end": "47430"
  },
  {
    "text": "The attention mechanisms are\nthemselves not directional.",
    "start": "47430",
    "end": "50630"
  },
  {
    "text": "It's just a bunch\nof dot products.",
    "start": "50630",
    "end": "52550"
  },
  {
    "text": "And there are no\nother interactions",
    "start": "52550",
    "end": "54860"
  },
  {
    "text": "between the columns.",
    "start": "54860",
    "end": "56360"
  },
  {
    "text": "And so we are in grave danger\nof losing track of the fact",
    "start": "56360",
    "end": "60410"
  },
  {
    "text": "that the input sequence A, B,\nC is different from the input",
    "start": "60410",
    "end": "64160"
  },
  {
    "text": "sequence C, B, A.\nPositional encodings",
    "start": "64160",
    "end": "67730"
  },
  {
    "text": "will ensure that we retain a\ndifference between those two",
    "start": "67730",
    "end": "70610"
  },
  {
    "text": "sequences no matter what we do\nwith the representations that",
    "start": "70610",
    "end": "74000"
  },
  {
    "text": "come from the model.",
    "start": "74000",
    "end": "75960"
  },
  {
    "text": "Kind of secondarily,\nthere's another purpose",
    "start": "75960",
    "end": "78180"
  },
  {
    "text": "that positional encodings play\nwhich is kind of hierarchical.",
    "start": "78180",
    "end": "81240"
  },
  {
    "text": "They've been used to keep track\nof things, premise, hypothesis",
    "start": "81240",
    "end": "84869"
  },
  {
    "text": "in natural language inference.",
    "start": "84870",
    "end": "86310"
  },
  {
    "text": "That was an important\nfeature of the BERT model",
    "start": "86310",
    "end": "88649"
  },
  {
    "text": "that we'll talk about a\nbit later in the series.",
    "start": "88650",
    "end": "92310"
  },
  {
    "text": "I think there are a lot of\nperspectives that you could",
    "start": "92310",
    "end": "94560"
  },
  {
    "text": "take on propositional encoding.",
    "start": "94560",
    "end": "96600"
  },
  {
    "text": "To keep things simple, I thought\nI would center our discussion",
    "start": "96600",
    "end": "99299"
  },
  {
    "text": "around two crucial questions.",
    "start": "99300",
    "end": "101950"
  },
  {
    "text": "The first is, does\nthe set of positions",
    "start": "101950",
    "end": "105090"
  },
  {
    "text": "need to be decided\nahead of time?",
    "start": "105090",
    "end": "107920"
  },
  {
    "text": "And the second is, does the\npropositional encoding scheme",
    "start": "107920",
    "end": "111659"
  },
  {
    "text": "hinder generalization\nto new positions?",
    "start": "111660",
    "end": "115800"
  },
  {
    "text": "I think those are good\nquestions to guide us.",
    "start": "115800",
    "end": "117930"
  },
  {
    "text": "One other rule that I wanted\nto introduce is the following.",
    "start": "117930",
    "end": "122100"
  },
  {
    "text": "Modern transformer architectures\nmight impose a max length",
    "start": "122100",
    "end": "125760"
  },
  {
    "text": "on sequences for many\nreasons related to how they",
    "start": "125760",
    "end": "128729"
  },
  {
    "text": "were designed and optimized.",
    "start": "128729",
    "end": "130530"
  },
  {
    "text": "I would like to kind\nof set all of that",
    "start": "130530",
    "end": "132690"
  },
  {
    "text": "aside and just ask whether\nthe positional encoding",
    "start": "132690",
    "end": "135690"
  },
  {
    "text": "scheme itself is imposing\nanything about length",
    "start": "135690",
    "end": "138990"
  },
  {
    "text": "generalization separately\nfrom all that other stuff that",
    "start": "138990",
    "end": "141690"
  },
  {
    "text": "might be happening.",
    "start": "141690",
    "end": "143500"
  },
  {
    "text": "So let's start with absolute\npropositional encoding.",
    "start": "143500",
    "end": "146250"
  },
  {
    "text": "This is the kind of scheme that\nwe have talked about so far.",
    "start": "146250",
    "end": "149490"
  },
  {
    "text": "On this scheme, we have\nword representations,",
    "start": "149490",
    "end": "152700"
  },
  {
    "text": "and we also have\npropositional representations",
    "start": "152700",
    "end": "155099"
  },
  {
    "text": "that we have learned\ncorresponding to some fixed",
    "start": "155100",
    "end": "157440"
  },
  {
    "text": "number of dimensions.",
    "start": "157440",
    "end": "158940"
  },
  {
    "text": "And to get our\nposition-sensitive word",
    "start": "158940",
    "end": "162390"
  },
  {
    "text": "representation, we simply\nadd together the word vector",
    "start": "162390",
    "end": "165810"
  },
  {
    "text": "with the position vector.",
    "start": "165810",
    "end": "167970"
  },
  {
    "text": "How is this scheme doing for\nour two crucial questions?",
    "start": "167970",
    "end": "171480"
  },
  {
    "text": "Well, not so well.",
    "start": "171480",
    "end": "173099"
  },
  {
    "text": "First, obviously, the set of\npositions that needs to be",
    "start": "173100",
    "end": "176620"
  },
  {
    "text": "decided ahead of time.",
    "start": "176620",
    "end": "178209"
  },
  {
    "text": "When we set up our model, we\nwill have some embedding space,",
    "start": "178210",
    "end": "181660"
  },
  {
    "text": "maybe up to 512.",
    "start": "181660",
    "end": "184120"
  },
  {
    "text": "And if we picked 512,\nwhen we hit position 513,",
    "start": "184120",
    "end": "188500"
  },
  {
    "text": "we will not have a\npositional representation",
    "start": "188500",
    "end": "191890"
  },
  {
    "text": "for that position.",
    "start": "191890",
    "end": "194290"
  },
  {
    "text": "And I also think it's clear\nthat this kind of scheme",
    "start": "194290",
    "end": "197280"
  },
  {
    "text": "can hinder generalization\nto new positions,",
    "start": "197280",
    "end": "200340"
  },
  {
    "text": "even for familiar phenomena.",
    "start": "200340",
    "end": "202800"
  },
  {
    "text": "Just consider the fact\nthat the Rock as a phrase,",
    "start": "202800",
    "end": "206340"
  },
  {
    "text": "if it occurs early\nin the sequence,",
    "start": "206340",
    "end": "208560"
  },
  {
    "text": "is simply a different\nrepresentation than the Rock",
    "start": "208560",
    "end": "211709"
  },
  {
    "text": "if it appears later\nin the sequence.",
    "start": "211710",
    "end": "214500"
  },
  {
    "text": "There will be some shared\nfeatures across these two",
    "start": "214500",
    "end": "217680"
  },
  {
    "text": "as a result of the fact that we\nhave two-word vectors involved",
    "start": "217680",
    "end": "221310"
  },
  {
    "text": "in both places.",
    "start": "221310",
    "end": "222550"
  },
  {
    "text": "But we add in those\npropositional representations",
    "start": "222550",
    "end": "225420"
  },
  {
    "text": "as equal partners in\nthis representation.",
    "start": "225420",
    "end": "227970"
  },
  {
    "text": "And I think the result\nis very heavy-handed",
    "start": "227970",
    "end": "230790"
  },
  {
    "text": "when it comes to learning\nrepresentations that",
    "start": "230790",
    "end": "233430"
  },
  {
    "text": "are heavily position dependent.",
    "start": "233430",
    "end": "235469"
  },
  {
    "text": "And that could make\nit hard for the model",
    "start": "235470",
    "end": "237300"
  },
  {
    "text": "to see that, in some\nsense, the Rock is",
    "start": "237300",
    "end": "239880"
  },
  {
    "text": "the same phrase whether it's\nat the start of the sequence",
    "start": "239880",
    "end": "243090"
  },
  {
    "text": "or the middle, or the end.",
    "start": "243090",
    "end": "246870"
  },
  {
    "text": "Another scheme we\ncould consider actually",
    "start": "246870",
    "end": "248659"
  },
  {
    "text": "goes all the way back to\nthe Transformers paper.",
    "start": "248660",
    "end": "251000"
  },
  {
    "text": "I've called this frequency-based\npositional encoding.",
    "start": "251000",
    "end": "254210"
  },
  {
    "text": "There are lots of ways\nwe could set this up,",
    "start": "254210",
    "end": "257000"
  },
  {
    "text": "but the essential idea\nhere is that we'll",
    "start": "257000",
    "end": "259278"
  },
  {
    "text": "define a mathematical function\nthat, given a position",
    "start": "259279",
    "end": "263180"
  },
  {
    "text": "will give us back a\nvector that encodes",
    "start": "263180",
    "end": "265940"
  },
  {
    "text": "information about that\nposition, kind of semantically",
    "start": "265940",
    "end": "269210"
  },
  {
    "text": "in its structure.",
    "start": "269210",
    "end": "270110"
  },
  {
    "text": "In the transformer paper,\nthey picked a scheme",
    "start": "270110",
    "end": "272930"
  },
  {
    "text": "that's based in\nfrequency oscillation,",
    "start": "272930",
    "end": "275300"
  },
  {
    "text": "so essentially based in\nsine and cosine frequencies",
    "start": "275300",
    "end": "278720"
  },
  {
    "text": "for these vectors where higher\npositions oscillate more",
    "start": "278720",
    "end": "282620"
  },
  {
    "text": "frequently, and that information\nis encoded in the position",
    "start": "282620",
    "end": "286370"
  },
  {
    "text": "vector that we create.",
    "start": "286370",
    "end": "288003"
  },
  {
    "text": "I think there are lots of other\nschemes that we could use.",
    "start": "288003",
    "end": "290419"
  },
  {
    "text": "The essential feature of\nthis argument pause here.",
    "start": "290420",
    "end": "294260"
  },
  {
    "text": "If you give this\nfunction position one,",
    "start": "294260",
    "end": "296930"
  },
  {
    "text": "it gives you a vector.",
    "start": "296930",
    "end": "298190"
  },
  {
    "text": "If you give it 513,\nit gives you a vector.",
    "start": "298190",
    "end": "300800"
  },
  {
    "text": "If you give it a million,\nit gives you a vector.",
    "start": "300800",
    "end": "303229"
  },
  {
    "text": "And all of those\nvectors manifestly",
    "start": "303230",
    "end": "306000"
  },
  {
    "text": "do encode information\nabout the relative position",
    "start": "306000",
    "end": "310260"
  },
  {
    "text": "of that input.",
    "start": "310260",
    "end": "312520"
  },
  {
    "text": "So we have definitely\novercome the first limitation.",
    "start": "312520",
    "end": "315599"
  },
  {
    "text": "The set of positions\ndoes not need",
    "start": "315600",
    "end": "317460"
  },
  {
    "text": "to be decided ahead\nof time in this scheme",
    "start": "317460",
    "end": "319350"
  },
  {
    "text": "because we can fire off a\nnew vector for any position",
    "start": "319350",
    "end": "322530"
  },
  {
    "text": "that you give us.",
    "start": "322530",
    "end": "323940"
  },
  {
    "text": "But I think our second\nquestion remains pressing.",
    "start": "323940",
    "end": "326850"
  },
  {
    "text": "Just as before, this\nscheme can hinder",
    "start": "326850",
    "end": "329670"
  },
  {
    "text": "generalization to new\npositions even for familiar",
    "start": "329670",
    "end": "332550"
  },
  {
    "text": "phenomena in virtue of the fact\nthat we are taking those word",
    "start": "332550",
    "end": "335909"
  },
  {
    "text": "representations and adding\nin these positional ones",
    "start": "335910",
    "end": "339750"
  },
  {
    "text": "for different positions as\nequal partners, as I said,",
    "start": "339750",
    "end": "343140"
  },
  {
    "text": "and I think that makes\nit hard for models",
    "start": "343140",
    "end": "345120"
  },
  {
    "text": "to see that the\nsame phrase could",
    "start": "345120",
    "end": "346860"
  },
  {
    "text": "appear in multiple places.",
    "start": "346860",
    "end": "350229"
  },
  {
    "text": "The third scheme is the\nmost promising of the three",
    "start": "350230",
    "end": "352843"
  },
  {
    "text": "that we're going to discuss.",
    "start": "352843",
    "end": "354009"
  },
  {
    "text": "This is relative\npropositional encoding.",
    "start": "354010",
    "end": "356410"
  },
  {
    "text": "We're going to take a few steps\nto build up an understanding",
    "start": "356410",
    "end": "359170"
  },
  {
    "text": "of how this scheme works.",
    "start": "359170",
    "end": "361070"
  },
  {
    "text": "Let's start with a reminder.",
    "start": "361070",
    "end": "362600"
  },
  {
    "text": "This is a picture\nof the attention",
    "start": "362600",
    "end": "364660"
  },
  {
    "text": "layer of the transformer.",
    "start": "364660",
    "end": "366500"
  },
  {
    "text": "We have our three\nposition-sensitive inputs here,",
    "start": "366500",
    "end": "369640"
  },
  {
    "text": "a input, b input, and c input.",
    "start": "369640",
    "end": "371680"
  },
  {
    "text": "And remember, it's\ncrucial that they",
    "start": "371680",
    "end": "373539"
  },
  {
    "text": "be position sensitive\nbecause of how much symmetry",
    "start": "373540",
    "end": "377080"
  },
  {
    "text": "there is in these dot\nproduct attention mechanisms.",
    "start": "377080",
    "end": "381189"
  },
  {
    "text": "Here's a reminder about how that\ncalculation works with respect",
    "start": "381190",
    "end": "384790"
  },
  {
    "text": "to position c over here.",
    "start": "384790",
    "end": "387130"
  },
  {
    "text": "For propositional\nencoding, we really",
    "start": "387130",
    "end": "389650"
  },
  {
    "text": "just add in some new parameters.",
    "start": "389650",
    "end": "391270"
  },
  {
    "text": "So what I've depicted at the\nbottom of the slide here is",
    "start": "391270",
    "end": "394150"
  },
  {
    "text": "the same calculation that's\nat the top, except now,",
    "start": "394150",
    "end": "397690"
  },
  {
    "text": "in two crucial places, I have\nadded in some new vectors",
    "start": "397690",
    "end": "401680"
  },
  {
    "text": "that we're going to learn\nrepresentations for.",
    "start": "401680",
    "end": "404080"
  },
  {
    "text": "Down in blue here, we\nhave key representations,",
    "start": "404080",
    "end": "407650"
  },
  {
    "text": "which get added into\nthis dot product.",
    "start": "407650",
    "end": "410150"
  },
  {
    "text": "And we up here in\nthe final step,",
    "start": "410150",
    "end": "412370"
  },
  {
    "text": "we have value\nrepresentations, which",
    "start": "412370",
    "end": "414380"
  },
  {
    "text": "get added into this\nmultiplied attention",
    "start": "414380",
    "end": "417290"
  },
  {
    "text": "mechanism plus the thing\nwe're attending to.",
    "start": "417290",
    "end": "420500"
  },
  {
    "text": "So those are the two\nnew crucial parameters",
    "start": "420500",
    "end": "424120"
  },
  {
    "text": "that we're adding in here.",
    "start": "424120",
    "end": "425350"
  },
  {
    "text": "And the essential idea\nis that having done this",
    "start": "425350",
    "end": "428560"
  },
  {
    "text": "with all the position\nsensitivity that's going",
    "start": "428560",
    "end": "430690"
  },
  {
    "text": "to be encoded in these vectors.",
    "start": "430690",
    "end": "432580"
  },
  {
    "text": "We don't need these green\nrepresentations here anymore",
    "start": "432580",
    "end": "435909"
  },
  {
    "text": "to have propositional\ninformation in them",
    "start": "435910",
    "end": "438130"
  },
  {
    "text": "because that propositional\ninformation is now",
    "start": "438130",
    "end": "440320"
  },
  {
    "text": "being introduced in\nthe attention layer",
    "start": "440320",
    "end": "443290"
  },
  {
    "text": "because we're going\nto have potentially",
    "start": "443290",
    "end": "445000"
  },
  {
    "text": "new vectors for every\ncombination of position",
    "start": "445000",
    "end": "447700"
  },
  {
    "text": "as indicated by\nthese subscripts.",
    "start": "447700",
    "end": "450820"
  },
  {
    "text": "But that's only\npart of the story.",
    "start": "450820",
    "end": "452960"
  },
  {
    "text": "I think the really powerful\nthing about this method",
    "start": "452960",
    "end": "456490"
  },
  {
    "text": "is the notion of having a\npositional encoding window.",
    "start": "456490",
    "end": "460270"
  },
  {
    "text": "So to illustrate that, I've\nrepeated the core calculation",
    "start": "460270",
    "end": "463750"
  },
  {
    "text": "at the top here as a reminder.",
    "start": "463750",
    "end": "465700"
  },
  {
    "text": "And now, for my\nillustration, I'm",
    "start": "465700",
    "end": "467500"
  },
  {
    "text": "going to set the\nwindow size to 2.",
    "start": "467500",
    "end": "470440"
  },
  {
    "text": "Here's the input sequence\nthat we'll use as an example.",
    "start": "470440",
    "end": "474100"
  },
  {
    "text": "And above that, I'm going\nto show you just integers",
    "start": "474100",
    "end": "477100"
  },
  {
    "text": "corresponding to the positions.",
    "start": "477100",
    "end": "478690"
  },
  {
    "text": "Those aren't directly\ningredients into the model,",
    "start": "478690",
    "end": "481640"
  },
  {
    "text": "but they will help us\nkeep track of where",
    "start": "481640",
    "end": "484070"
  },
  {
    "text": "we are in the calculations.",
    "start": "484070",
    "end": "486500"
  },
  {
    "text": "To start the illustration,\nlet's zoom in on position 4.",
    "start": "486500",
    "end": "491010"
  },
  {
    "text": "If we follow the letter of the\ndefinitions that I've offered",
    "start": "491010",
    "end": "494250"
  },
  {
    "text": "so far for the key values here,\nwe're going to have a vector",
    "start": "494250",
    "end": "498000"
  },
  {
    "text": "a4,4 corresponding to us\nattending from position 4",
    "start": "498000",
    "end": "502560"
  },
  {
    "text": "to position 4.",
    "start": "502560",
    "end": "504610"
  },
  {
    "text": "As part of creating this more\nlimited window-based version",
    "start": "504610",
    "end": "508270"
  },
  {
    "text": "of the model, we're\nactually going",
    "start": "508270",
    "end": "510039"
  },
  {
    "text": "to map that into a single\nvector w0 for the keys.",
    "start": "510040",
    "end": "515469"
  },
  {
    "text": "Now we travel to the\nposition 1 to the left.",
    "start": "515470",
    "end": "518449"
  },
  {
    "text": "And in this case, we would have\na vector a4,3 for the keys.",
    "start": "518450",
    "end": "523010"
  },
  {
    "text": "But what we're going\nto do is map that",
    "start": "523010",
    "end": "524980"
  },
  {
    "text": "into a single vector w\nminus 1, corresponding",
    "start": "524980",
    "end": "528160"
  },
  {
    "text": "to taking 3 minus 4.",
    "start": "528160",
    "end": "531569"
  },
  {
    "text": "When we travel one more to the\nleft, we get a position 4,2,",
    "start": "531570",
    "end": "535920"
  },
  {
    "text": "but now we're going to\nmap that to vector w",
    "start": "535920",
    "end": "538350"
  },
  {
    "text": "minus 2 again for the keys.",
    "start": "538350",
    "end": "540870"
  },
  {
    "text": "And then, because we\nset our window size to 2",
    "start": "540870",
    "end": "544050"
  },
  {
    "text": "when we get all the way\nto that leftmost position,",
    "start": "544050",
    "end": "546959"
  },
  {
    "text": "that's also just\nw minus 2 again.",
    "start": "546960",
    "end": "549900"
  },
  {
    "text": "4 minus 1, given\nthe window size,",
    "start": "549900",
    "end": "552480"
  },
  {
    "text": "takes us just to the\nmaximum of this window",
    "start": "552480",
    "end": "554880"
  },
  {
    "text": "in this case minus 2.",
    "start": "554880",
    "end": "557100"
  },
  {
    "text": "And then a parallel\nthing happens",
    "start": "557100",
    "end": "558930"
  },
  {
    "text": "when we travel to the right.",
    "start": "558930",
    "end": "560100"
  },
  {
    "text": "So we go from 4 to 5, that\ngives us vector w1 for the keys.",
    "start": "560100",
    "end": "564930"
  },
  {
    "text": "Then 4,6 gives us w2.",
    "start": "564930",
    "end": "567510"
  },
  {
    "text": "And then, when we get\nto the third position",
    "start": "567510",
    "end": "569520"
  },
  {
    "text": "from our starting\npoint, that again",
    "start": "569520",
    "end": "571980"
  },
  {
    "text": "just flattens out to w2\nbecause of our window size.",
    "start": "571980",
    "end": "576740"
  },
  {
    "text": "So actually represented\nin blue here,",
    "start": "576740",
    "end": "579160"
  },
  {
    "text": "we have just a few vectors,\nthe 01, the minus 1,",
    "start": "579160",
    "end": "583240"
  },
  {
    "text": "and the minus 2 one,\nand then the one,",
    "start": "583240",
    "end": "586390"
  },
  {
    "text": "two vectors as opposed to all\nthe distinctions that are made",
    "start": "586390",
    "end": "590680"
  },
  {
    "text": "with those alpha sub 4,3\nand 4,2 and so forth.",
    "start": "590680",
    "end": "595100"
  },
  {
    "text": "So we're collapsing\nthose down into a smaller",
    "start": "595100",
    "end": "597730"
  },
  {
    "text": "number of vectors corresponding\nto the window size.",
    "start": "597730",
    "end": "601300"
  },
  {
    "text": "And then, to continue\nthe illustration,",
    "start": "601300",
    "end": "603430"
  },
  {
    "text": "if we zoom in on position\n3, that would be vector a3,3",
    "start": "603430",
    "end": "608200"
  },
  {
    "text": "for the keys, but now\nthat gets mapped to w0k,",
    "start": "608200",
    "end": "611710"
  },
  {
    "text": "which is the same vector that\nwe have up here in that 4,4",
    "start": "611710",
    "end": "615422"
  },
  {
    "text": "position.",
    "start": "615422",
    "end": "616570"
  },
  {
    "text": "And a similar collapsing is\ngoing to happen down here.",
    "start": "616570",
    "end": "619190"
  },
  {
    "text": "So when we move one to the\nleft of that, we get minus 1,",
    "start": "619190",
    "end": "621880"
  },
  {
    "text": "which is the same vector as we\nhad up here, just to the right.",
    "start": "621880",
    "end": "626830"
  },
  {
    "text": "And then we have the\nsame thing over here,",
    "start": "626830",
    "end": "629230"
  },
  {
    "text": "minus 2 corresponding to the\nsame vector that we had above.",
    "start": "629230",
    "end": "633670"
  },
  {
    "text": "And that would continue.",
    "start": "633670",
    "end": "635149"
  },
  {
    "text": "And we have a parallel\ncalculation for the value",
    "start": "635150",
    "end": "638180"
  },
  {
    "text": "parameters that you\nsee in purple up",
    "start": "638180",
    "end": "640279"
  },
  {
    "text": "here, the same notions of\nrelative position and window",
    "start": "640280",
    "end": "643550"
  },
  {
    "text": "size.",
    "start": "643550",
    "end": "644430"
  },
  {
    "text": "So we actually learn a\nrelatively small number",
    "start": "644430",
    "end": "647330"
  },
  {
    "text": "of position vectors.",
    "start": "647330",
    "end": "649010"
  },
  {
    "text": "And what we're\ndoing is essentially",
    "start": "649010",
    "end": "651440"
  },
  {
    "text": "giving a small window\nrelative notion of position",
    "start": "651440",
    "end": "655700"
  },
  {
    "text": "that's going to kind of\nslide around and give us",
    "start": "655700",
    "end": "657890"
  },
  {
    "text": "a lot of ability to generalize\nto new positions based",
    "start": "657890",
    "end": "661640"
  },
  {
    "text": "on combinations that we've\nseen before, possibly",
    "start": "661640",
    "end": "664010"
  },
  {
    "text": "in other parts of these inputs.",
    "start": "664010",
    "end": "668140"
  },
  {
    "text": "A final thing I'll say\nis that this is actually",
    "start": "668140",
    "end": "670780"
  },
  {
    "text": "embedded in that full\ntheory of attention that",
    "start": "670780",
    "end": "672970"
  },
  {
    "text": "might have a lot of\nlearned parameters",
    "start": "672970",
    "end": "674620"
  },
  {
    "text": "and might even be multi-headed.",
    "start": "674620",
    "end": "676180"
  },
  {
    "text": "So what I've depicted here\nis just the full calculation",
    "start": "676180",
    "end": "679420"
  },
  {
    "text": "just to really give\nyou all the details.",
    "start": "679420",
    "end": "681829"
  },
  {
    "text": "But again, the\ncognitive shortcut",
    "start": "681830",
    "end": "684070"
  },
  {
    "text": "is that it's the\nprevious attention",
    "start": "684070",
    "end": "686710"
  },
  {
    "text": "calculation with these new\npositional elements added in.",
    "start": "686710",
    "end": "690370"
  },
  {
    "text": "And again, a reminder\nin this new mode,",
    "start": "690370",
    "end": "692890"
  },
  {
    "text": "we introduce position relativity\nin the attention layer,",
    "start": "692890",
    "end": "695950"
  },
  {
    "text": "not in the embedding layer.",
    "start": "695950",
    "end": "698690"
  },
  {
    "text": "So let's think about our\ntwo crucial questions.",
    "start": "698690",
    "end": "701020"
  },
  {
    "text": "First, we don't need to\ndecide the set of positions",
    "start": "701020",
    "end": "703617"
  },
  {
    "text": "ahead of time.",
    "start": "703617",
    "end": "704200"
  },
  {
    "text": "We just need to\ndecide on the window.",
    "start": "704200",
    "end": "706360"
  },
  {
    "text": "And then, for a potentially\nextremely long string,",
    "start": "706360",
    "end": "709959"
  },
  {
    "text": "we're just sliding\nit around in it",
    "start": "709960",
    "end": "711910"
  },
  {
    "text": "using a relatively few\nnumber of positional vectors",
    "start": "711910",
    "end": "715569"
  },
  {
    "text": "to keep track of\nrelative position.",
    "start": "715570",
    "end": "718300"
  },
  {
    "text": "And I think we have also\nlargely overcome the concern",
    "start": "718300",
    "end": "721930"
  },
  {
    "text": "that propositional embeddings\nmight hinder generalization",
    "start": "721930",
    "end": "725350"
  },
  {
    "text": "to new positions.",
    "start": "725350",
    "end": "726339"
  },
  {
    "text": "After all, if you\nconsider a phrase",
    "start": "726340",
    "end": "728990"
  },
  {
    "text": "like \"the Rock,\" the core\nposition vectors that",
    "start": "728990",
    "end": "733310"
  },
  {
    "text": "are involved there\nare 0, 1, and minus 1,",
    "start": "733310",
    "end": "736760"
  },
  {
    "text": "no matter where this\nappears in the string.",
    "start": "736760",
    "end": "739340"
  },
  {
    "text": "Now depending on\nwhere it appears,",
    "start": "739340",
    "end": "741290"
  },
  {
    "text": "there will be other\npropositional things that",
    "start": "741290",
    "end": "743389"
  },
  {
    "text": "are happening, and\nother information",
    "start": "743390",
    "end": "745070"
  },
  {
    "text": "will be brought in as\npart of the calculation,",
    "start": "745070",
    "end": "747510"
  },
  {
    "text": "but we do have this\nsense of constancy that",
    "start": "747510",
    "end": "749900"
  },
  {
    "text": "will allow the model\nto see that the rock is",
    "start": "749900",
    "end": "752990"
  },
  {
    "text": "the same essentially wherever\nit appears in the string.",
    "start": "752990",
    "end": "757490"
  },
  {
    "text": "So my hypothesis is that because\nwe have overcome these two",
    "start": "757490",
    "end": "761140"
  },
  {
    "text": "crucial limitations, relative\npropositional encoding",
    "start": "761140",
    "end": "764020"
  },
  {
    "text": "is a very good bet for how to do\npositional encoding in general",
    "start": "764020",
    "end": "767890"
  },
  {
    "text": "in the transformer,\nand I believe",
    "start": "767890",
    "end": "769510"
  },
  {
    "text": "that is now well\nsupported by results",
    "start": "769510",
    "end": "773260"
  },
  {
    "text": "across the field\nfor the transformer.",
    "start": "773260",
    "end": "776910"
  },
  {
    "start": "776910",
    "end": "781000"
  }
]