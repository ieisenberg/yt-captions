[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "start": "0",
    "end": "5880"
  },
  {
    "text": "OMAR KHATTAB: Hello, everyone.",
    "start": "5880",
    "end": "7130"
  },
  {
    "text": "Welcome to part 3 of the series.",
    "start": "7130",
    "end": "9700"
  },
  {
    "text": "This screencast will be\nthe first of two or three",
    "start": "9700",
    "end": "12700"
  },
  {
    "text": "on neural IR.",
    "start": "12700",
    "end": "14290"
  },
  {
    "text": "And in it, we'll be exploring\nthe inputs, outputs, training",
    "start": "14290",
    "end": "18340"
  },
  {
    "text": "and inference in the\ncontext of neural IR.",
    "start": "18340",
    "end": "20830"
  },
  {
    "start": "20830",
    "end": "24770"
  },
  {
    "start": "23000",
    "end": "81000"
  },
  {
    "text": "Let's quickly start with\na reminder of our setup",
    "start": "24770",
    "end": "27380"
  },
  {
    "text": "from the previous screencast.",
    "start": "27380",
    "end": "29830"
  },
  {
    "text": "Offline, we're given a large\ncorpus of text documents.",
    "start": "29830",
    "end": "34210"
  },
  {
    "text": "We will pre-process and index\nthis corpus for fast retrieval.",
    "start": "34210",
    "end": "39640"
  },
  {
    "text": "Online, we're giving a query\nthat we want to answer.",
    "start": "39640",
    "end": "43660"
  },
  {
    "text": "Our output will be a list of the\nTop-K most relevant documents",
    "start": "43660",
    "end": "47410"
  },
  {
    "text": "for this query.",
    "start": "47410",
    "end": "48130"
  },
  {
    "start": "48130",
    "end": "51616"
  },
  {
    "text": "In the classical\nIR screencast, we",
    "start": "51616",
    "end": "54170"
  },
  {
    "text": "discussed BM25 as a strong\nterm-matching retrieval model.",
    "start": "54170",
    "end": "59510"
  },
  {
    "text": "So should we just use BM25?",
    "start": "59510",
    "end": "62839"
  },
  {
    "text": "The short answer\nis that we could.",
    "start": "62840",
    "end": "65810"
  },
  {
    "text": "But if our interest is\ngetting the highest quality",
    "start": "65810",
    "end": "68630"
  },
  {
    "text": "that we can, then we should\nprobably be using neural IR.",
    "start": "68630",
    "end": "73039"
  },
  {
    "text": "As we will see, neural IR makes\na lot of use of our NLU work",
    "start": "73040",
    "end": "77330"
  },
  {
    "text": "in creative and\ninteresting ways.",
    "start": "77330",
    "end": "81700"
  },
  {
    "start": "81000",
    "end": "191000"
  },
  {
    "text": "The long answer to whether\nwe should be using BM25",
    "start": "81700",
    "end": "85270"
  },
  {
    "text": "is that it depends.",
    "start": "85270",
    "end": "86619"
  },
  {
    "text": "Among other factors, it\ndepends on our budget.",
    "start": "86620",
    "end": "91510"
  },
  {
    "text": "Each IR model poses a different\nefficiency-effectiveness",
    "start": "91510",
    "end": "95230"
  },
  {
    "text": "tradeoff.",
    "start": "95230",
    "end": "97220"
  },
  {
    "text": "In many cases, we're interested\nin maximizing effectiveness,",
    "start": "97220",
    "end": "100610"
  },
  {
    "text": "maximizing quality, as long\nas efficiency is acceptable.",
    "start": "100610",
    "end": "105870"
  },
  {
    "text": "Let's begin to explore this\non the MS MARCO collection",
    "start": "105870",
    "end": "110160"
  },
  {
    "text": "that we introduced in\nthe previous screencast.",
    "start": "110160",
    "end": "114500"
  },
  {
    "text": "Here we'll be\nmeasuring effectiveness",
    "start": "114500",
    "end": "117020"
  },
  {
    "text": "using the mean reciprocal\nrank at cut-off 10.",
    "start": "117020",
    "end": "120320"
  },
  {
    "text": "And we will measure\nefficiency, and in particular,",
    "start": "120320",
    "end": "122659"
  },
  {
    "text": "latency using milliseconds.",
    "start": "122660",
    "end": "125427"
  },
  {
    "start": "125427",
    "end": "128169"
  },
  {
    "text": "This figure here shows\nBM25 retrieval using",
    "start": "128169",
    "end": "131720"
  },
  {
    "text": "the popular toolkit called\nAnsereni as one data",
    "start": "131720",
    "end": "135310"
  },
  {
    "text": "point within a wide\nrange of MRR values",
    "start": "135310",
    "end": "138640"
  },
  {
    "text": "and latency possibilities.",
    "start": "138640",
    "end": "140560"
  },
  {
    "start": "140560",
    "end": "144550"
  },
  {
    "text": "Just as a reminder,\nlower latency is better.",
    "start": "144550",
    "end": "147850"
  },
  {
    "text": "And the latency here is\nshown on a logarithmic scale.",
    "start": "147850",
    "end": "153260"
  },
  {
    "text": "And higher MRR is also better.",
    "start": "153260",
    "end": "157430"
  },
  {
    "text": "The higher our MRR is, the\nbetter the model's quality.",
    "start": "157430",
    "end": "162370"
  },
  {
    "text": "So what else could exist in\nthis large empty space for now?",
    "start": "162370",
    "end": "167200"
  },
  {
    "text": "We're going to see\nthis space fill up",
    "start": "167200",
    "end": "170110"
  },
  {
    "text": "with many different neural IR\nmodels over the next couple",
    "start": "170110",
    "end": "173110"
  },
  {
    "text": "of screencasts.",
    "start": "173110",
    "end": "175340"
  },
  {
    "text": "And the central\nquestion now and then",
    "start": "175340",
    "end": "177709"
  },
  {
    "text": "will generally be, how\ncan we improve our MRR@10",
    "start": "177710",
    "end": "181220"
  },
  {
    "text": "or whatever effectiveness\nmetric we choose",
    "start": "181220",
    "end": "183500"
  },
  {
    "text": "to work with, possibly at the\nexpense of increasing latency",
    "start": "183500",
    "end": "187200"
  },
  {
    "text": "a bit?",
    "start": "187200",
    "end": "187700"
  },
  {
    "start": "187700",
    "end": "191319"
  },
  {
    "start": "191000",
    "end": "245000"
  },
  {
    "text": "OK, so let's\nactually take a look",
    "start": "191320",
    "end": "193990"
  },
  {
    "text": "at how neural IR models\nwill work, specifically",
    "start": "193990",
    "end": "197650"
  },
  {
    "text": "at their input and\noutput behavior.",
    "start": "197650",
    "end": "201099"
  },
  {
    "text": "For the purposes of\nthis short screencast,",
    "start": "201100",
    "end": "203440"
  },
  {
    "text": "we'll treat the neural\nranker as a black box.",
    "start": "203440",
    "end": "207520"
  },
  {
    "text": "We will consider\nvarious implementations",
    "start": "207520",
    "end": "209340"
  },
  {
    "text": "for this black box function\nin the next screencast.",
    "start": "209340",
    "end": "214700"
  },
  {
    "text": "We will feed this\nneural IR black box",
    "start": "214700",
    "end": "218510"
  },
  {
    "text": "a query and a document.",
    "start": "218510",
    "end": "220310"
  },
  {
    "text": "And the model will do its\nthing and return to us",
    "start": "220310",
    "end": "222900"
  },
  {
    "text": "a single score that estimates\nthe relevance of this query",
    "start": "222900",
    "end": "227629"
  },
  {
    "text": "to that document.",
    "start": "227630",
    "end": "230490"
  },
  {
    "text": "For the same query, we\nwill repeat this process",
    "start": "230490",
    "end": "233040"
  },
  {
    "text": "for every document\nthat we want to score.",
    "start": "233040",
    "end": "235900"
  },
  {
    "text": "And we will finally sort\nall of these documents",
    "start": "235900",
    "end": "238360"
  },
  {
    "text": "by decreasing relevance score.",
    "start": "238360",
    "end": "240990"
  },
  {
    "text": "And that will give us the\nTop-K list of results.",
    "start": "240990",
    "end": "243520"
  },
  {
    "start": "243520",
    "end": "246830"
  },
  {
    "start": "245000",
    "end": "331000"
  },
  {
    "text": "So far, this sounds\nsimple enough.",
    "start": "246830",
    "end": "249750"
  },
  {
    "text": "But how should we train this\nneural model for ranking?",
    "start": "249750",
    "end": "254750"
  },
  {
    "text": "This might not be super obvious,\nbut one pretty effective choice",
    "start": "254750",
    "end": "257898"
  },
  {
    "text": "is simply two-way\nclassification,",
    "start": "257899",
    "end": "260359"
  },
  {
    "text": "pair-wise classification.",
    "start": "260360",
    "end": "262879"
  },
  {
    "text": "Here, each training\nexample will be a triple.",
    "start": "262880",
    "end": "266480"
  },
  {
    "text": "Specifically, each\ntraining instance",
    "start": "266480",
    "end": "268730"
  },
  {
    "text": "will contain a query, a\nrelevant or positive document,",
    "start": "268730",
    "end": "272690"
  },
  {
    "text": "and an irrelevant\ndocument, or a negative.",
    "start": "272690",
    "end": "276880"
  },
  {
    "text": "In the forward pass\nduring training,",
    "start": "276880",
    "end": "279190"
  },
  {
    "text": "we'll feed the model the query\nand the positive document.",
    "start": "279190",
    "end": "282690"
  },
  {
    "text": "And separately, we'll feed the\nquery and the negative document",
    "start": "282690",
    "end": "285780"
  },
  {
    "text": "to the neural ranker.",
    "start": "285780",
    "end": "289190"
  },
  {
    "text": "And we optimize the\nentire neural network end",
    "start": "289190",
    "end": "292040"
  },
  {
    "text": "to end with gradient descent,\nusing simple classification",
    "start": "292040",
    "end": "295550"
  },
  {
    "text": "loss--",
    "start": "295550",
    "end": "297330"
  },
  {
    "text": "written in this case,\nCrossEntropy Loss with softmax.",
    "start": "297330",
    "end": "302210"
  },
  {
    "text": "The goal here is to maximize the\nscore of the positive document,",
    "start": "302210",
    "end": "305750"
  },
  {
    "text": "and to minimize\nthe score assigned",
    "start": "305750",
    "end": "308353"
  },
  {
    "text": "to the negative document.",
    "start": "308353",
    "end": "309395"
  },
  {
    "start": "309395",
    "end": "312069"
  },
  {
    "text": "Recall that we can get\npositives for each query",
    "start": "312070",
    "end": "315280"
  },
  {
    "text": "from our relevance assessments.",
    "start": "315280",
    "end": "316990"
  },
  {
    "text": "And that every document that\nwas not labeled as positive",
    "start": "316990",
    "end": "320620"
  },
  {
    "text": "can often be treated as\nan implicit negative.",
    "start": "320620",
    "end": "323350"
  },
  {
    "text": "So we could use this\nin generating triples",
    "start": "323350",
    "end": "326560"
  },
  {
    "text": "for 2-way classification\ntraining for our neural ranker.",
    "start": "326560",
    "end": "330460"
  },
  {
    "start": "330460",
    "end": "333750"
  },
  {
    "start": "331000",
    "end": "381000"
  },
  {
    "text": "Once our neural ranker\nis trained, inference",
    "start": "333750",
    "end": "336420"
  },
  {
    "text": "or actually conducting\nthe ranking is very easy.",
    "start": "336420",
    "end": "340170"
  },
  {
    "text": "Given a query, we'll\njust pick each document,",
    "start": "340170",
    "end": "342750"
  },
  {
    "text": "pass the query and the document\nthrough the neural network,",
    "start": "342750",
    "end": "345990"
  },
  {
    "text": "get a score, and then we'll\nsort all the documents by score.",
    "start": "345990",
    "end": "349780"
  },
  {
    "text": "And this will give us the\nTop-K list of documents.",
    "start": "349780",
    "end": "352540"
  },
  {
    "text": "However, there is just a\nsmall yet very major problem.",
    "start": "352540",
    "end": "358650"
  },
  {
    "text": "Collections often have\nmany millions, if not",
    "start": "358650",
    "end": "360610"
  },
  {
    "text": "billions of documents.",
    "start": "360610",
    "end": "362330"
  },
  {
    "text": "Even if our model is so\nfast that it processes",
    "start": "362330",
    "end": "365199"
  },
  {
    "text": "each document in\none microsecond, one",
    "start": "365200",
    "end": "367660"
  },
  {
    "text": "millionth of a\nsecond, it would still",
    "start": "367660",
    "end": "370090"
  },
  {
    "text": "require nine seconds per query\nfor a dataset like MS MARCO",
    "start": "370090",
    "end": "375070"
  },
  {
    "text": "with nine million\nmessages, which",
    "start": "375070",
    "end": "377110"
  },
  {
    "text": "is way too slow for most\npractical applications.",
    "start": "377110",
    "end": "382500"
  },
  {
    "start": "381000",
    "end": "461000"
  },
  {
    "text": "To deal with this in practice,\nneural IR models are often used",
    "start": "382500",
    "end": "385590"
  },
  {
    "text": "as re-rankers--",
    "start": "385590",
    "end": "387180"
  },
  {
    "text": "models that rescore\nonly the Top-K documents",
    "start": "387180",
    "end": "390090"
  },
  {
    "text": "obtained by another model to\nimprove the final ranking.",
    "start": "390090",
    "end": "395080"
  },
  {
    "text": "One of the most common\npipeline designs",
    "start": "395080",
    "end": "397300"
  },
  {
    "text": "is to re-rank the top 1,000\ndocuments obtained by BM25.",
    "start": "397300",
    "end": "402310"
  },
  {
    "text": "This can be great because\nit cuts down the work",
    "start": "402310",
    "end": "405820"
  },
  {
    "text": "for a collection with 10 million\npassages by a factor of 10,000,",
    "start": "405820",
    "end": "409690"
  },
  {
    "text": "because we only need\nto rank 1,000 documents",
    "start": "409690",
    "end": "411730"
  },
  {
    "text": "with the neural model.",
    "start": "411730",
    "end": "413580"
  },
  {
    "text": "But it also introduces an\nartificial ceiling on recall,",
    "start": "413580",
    "end": "416699"
  },
  {
    "text": "it limits recall in\nan artificial way.",
    "start": "416700",
    "end": "419290"
  },
  {
    "text": "Since now, all of the\nrelevant documents that BM25,",
    "start": "419290",
    "end": "422670"
  },
  {
    "text": "our first stage ranker, fails\nto retrieve cannot possibly be",
    "start": "422670",
    "end": "427410"
  },
  {
    "text": "ranked highly by our\nshiny new IR ranker.",
    "start": "427410",
    "end": "430500"
  },
  {
    "start": "430500",
    "end": "434290"
  },
  {
    "text": "So, can we do better?",
    "start": "434290",
    "end": "436760"
  },
  {
    "text": "It turns out that\nthe answer is yes.",
    "start": "436760",
    "end": "438520"
  },
  {
    "text": "We'll discuss the notion of\nend to end retrieval later,",
    "start": "438520",
    "end": "442180"
  },
  {
    "text": "where our neural model will\nbe able to quickly conduct",
    "start": "442180",
    "end": "444910"
  },
  {
    "text": "the search by itself over\nthe entire collection",
    "start": "444910",
    "end": "447760"
  },
  {
    "text": "without a re-ranking pipeline.",
    "start": "447760",
    "end": "449500"
  },
  {
    "text": "But first, we'll discuss a\nnumber of neural re-rankers",
    "start": "449500",
    "end": "452620"
  },
  {
    "text": "in detail in the\nnext screencast.",
    "start": "452620",
    "end": "456210"
  },
  {
    "start": "456210",
    "end": "461000"
  }
]