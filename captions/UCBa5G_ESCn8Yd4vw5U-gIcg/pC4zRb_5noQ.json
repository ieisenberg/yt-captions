[
  {
    "start": "0",
    "end": "5513"
  },
  {
    "text": "Thank you all for having me. It's exciting to be here. One of my favorite\nthings is talking about",
    "start": "5513",
    "end": "10539"
  },
  {
    "text": "what is going on\ninside neural networks, or at least what we're\ntrying to figure out is going on inside\nneural networks.",
    "start": "10540",
    "end": "16628"
  },
  {
    "text": "So it's always fun\nto chat about that.  Oh, gosh, I have to figure\nout how to kind of do things.",
    "start": "16628",
    "end": "23350"
  },
  {
    "text": "OK. What? OK. There we go. Now, we are advancing slides.",
    "start": "23350",
    "end": "28970"
  },
  {
    "text": "That seems promising. So I think interpretability\nmeans lots of different things to different people.",
    "start": "28970",
    "end": "35560"
  },
  {
    "text": "It's a very broad term. And people mean all sorts\nof different things by it. And so I wanted to talk\njust briefly about the kind",
    "start": "35560",
    "end": "42970"
  },
  {
    "text": "of interpretability that\nI spend my time thinking about, which is what I'd call\nmechanistic interpretability.",
    "start": "42970",
    "end": "48520"
  },
  {
    "text": "So most of my work actually\nhas not been on language models or on RNNs or transformers,\nbut on understanding vision",
    "start": "48520",
    "end": "56620"
  },
  {
    "text": "ConvNets and trying\nto understand how do the parameters\nin those models",
    "start": "56620",
    "end": "61870"
  },
  {
    "text": "actually map to algorithms. So you can think of the\nparameters of a neural network",
    "start": "61870",
    "end": "66970"
  },
  {
    "text": "as being like a compiled\ncomputer program and that the neurons are kind\nof like variables or registers.",
    "start": "66970",
    "end": "72820"
  },
  {
    "text": "And somehow there are these\ncomplex computer programs that are embedded in those weights.",
    "start": "72820",
    "end": "78640"
  },
  {
    "text": "And we'd like to turn them\nback in to computer programs that humans can understand. It's a kind of reverse\nengineering problem.",
    "start": "78640",
    "end": "86510"
  },
  {
    "text": "And so this is kind\nof a fun example that we found where\nthere was a car neuron. And you could actually see\nthat we have the car neuron.",
    "start": "86510",
    "end": "94360"
  },
  {
    "text": "And it's constructed\nfrom a wheel neuron. And it looks for-- in the case of the\nwheel neuron, it's",
    "start": "94360",
    "end": "99850"
  },
  {
    "text": "looking for the\nwheels on the bottom. Those are positive weights. And it doesn't want\nto see them on top. So it has negative\nweights there.",
    "start": "99850",
    "end": "106155"
  },
  {
    "text": "And there's also\na window neuron. It's looking for the windows on\nthe top and not on the bottom. And so what we're\nactually seeing there,",
    "start": "106155",
    "end": "112057"
  },
  {
    "text": "right, it's an algorithm. It's an algorithm\nthat goes and turns. It's saying, well, cars\nhas wheels on the bottom",
    "start": "112057",
    "end": "120790"
  },
  {
    "text": "and windows on the top\nand chrome in the middle. And that's actually just the\nstrongest neurons for that.",
    "start": "120790",
    "end": "126092"
  },
  {
    "text": "And so we're actually seeing\na meaningful algorithm. And that's not an exception. That's sort of the general\nstory that if you're",
    "start": "126092",
    "end": "132580"
  },
  {
    "text": "willing to go and look at\nneural network weights, and you're willing to\ninvest a lot of energy in trying to\nreverse-engineer them,",
    "start": "132580",
    "end": "137997"
  },
  {
    "text": "there's meaningful algorithms\nwritten in the weights waiting for you to find them. And there's a bunch\nof reasons I think",
    "start": "137998",
    "end": "144549"
  },
  {
    "text": "that's an interesting\nthing to think about. One is just no one knows\nhow to go and do the things that neural networks can do.",
    "start": "144550",
    "end": "150010"
  },
  {
    "text": "Like, no one knows how to\nwrite a computer program that can accurately\nclassify ImageNet, let alone the language modeling\ntasks that we're doing.",
    "start": "150010",
    "end": "156040"
  },
  {
    "text": "No one knows how\nto directly write a computer program that can\ndo the things that GPT-3 does. And yet somehow gradient descent\nis able to go and discover",
    "start": "156040",
    "end": "162790"
  },
  {
    "text": "a way to do this. And I want to know\nwhat's going on. I want to know what has\nit discovered that it",
    "start": "162790",
    "end": "168700"
  },
  {
    "text": "can do in these systems? There's another reason why\nI think this is important,",
    "start": "168700",
    "end": "173780"
  },
  {
    "text": "which is safety. So if we want to go and use\nthese systems in places where",
    "start": "173780",
    "end": "179290"
  },
  {
    "text": "they have a big\neffect on the world, I think a question\nwe need to ask ourselves is, what happens\nwhen these models have",
    "start": "179290",
    "end": "187233"
  },
  {
    "text": "unanticipated failure modes,\nfailure modes we didn't know how to go and test for or\nto look for or to check for? How can we discover\nthose things, especially",
    "start": "187233",
    "end": "194050"
  },
  {
    "text": "if they're really\npathological failure modes, that the model's, in some sense,\ndeliberately doing something that we don't want?",
    "start": "194050",
    "end": "199450"
  },
  {
    "text": "Well, the only way that I\nreally see that we can do that is if we can get to a\npoint where we really understand what's going\non inside these systems.",
    "start": "199450",
    "end": "207100"
  },
  {
    "text": "And so that's another reason\nthat I'm interested in this. Now, actually doing\ninterpretability",
    "start": "207100",
    "end": "212590"
  },
  {
    "text": "on language models and\ntransformers, it's new to me. Before this year, I\nspent, like, eight years",
    "start": "212590",
    "end": "217780"
  },
  {
    "text": "working on trying to reverse\nengineer ConvNets, and vision models. And so the idea is in\nthis talk are new things",
    "start": "217780",
    "end": "225250"
  },
  {
    "text": "that I've been thinking\nabout with my collaborators. And we're still probably\na month or two out, maybe longer from\npublishing them.",
    "start": "225250",
    "end": "232127"
  },
  {
    "text": "And this is also the\nfirst public talk that I've given on it. So the things I'm going to talk\nabout, I think, honestly, it's",
    "start": "232127",
    "end": "238579"
  },
  {
    "text": "still a little bit\nconfused for me and definitely are\ngoing to be confused in my articulation of them. So if I say things\nthat are confusing,",
    "start": "238580",
    "end": "244888"
  },
  {
    "text": "please feel free to\nask me questions. There might be\nsome points for me to go quickly because\nthere's a lot of content. But definitely at the end, I\nwill be available for a while",
    "start": "244888",
    "end": "251829"
  },
  {
    "text": "to chat about this stuff. And, yeah, also I apologize\nif I'm unfamiliar with Zoom",
    "start": "251830",
    "end": "259570"
  },
  {
    "text": "and make mistakes. But, yeah, so with that\nsaid, let's dive in.",
    "start": "259570",
    "end": "267060"
  },
  {
    "text": "And so I wanted to\nstart with a mystery. Before we go and\ntry to actually dig",
    "start": "267060",
    "end": "274330"
  },
  {
    "text": "into what's going on\ninside these models, I want to motivate it by\na really strange piece",
    "start": "274330",
    "end": "280210"
  },
  {
    "text": "of behavior that we discovered\nand wanted to understand. ",
    "start": "280210",
    "end": "286680"
  },
  {
    "text": "And by the way, I\nshould say all this work is done with my\ncolleagues in Anthropic, and especially my colleagues\nCatherine and Nelson.",
    "start": "286680",
    "end": "295180"
  },
  {
    "text": "OK, so on to the mystery. I think probably the most\ninteresting and most exciting thing about transformers\nis their ability",
    "start": "295180",
    "end": "304270"
  },
  {
    "text": "to do in-context learning,\nor sometimes people will call it meta learning. The GPT-3 paper goes\nand describes things",
    "start": "304270",
    "end": "312040"
  },
  {
    "text": "as language models\nare few-shot learners. There's lots of impressive\nthings about GPT-3. But they choose\nto focus on that.",
    "start": "312040",
    "end": "317945"
  },
  {
    "text": "And now everyone's talking\nabout prompt engineering. And Andrej Karpathy was joking\nabout how software 3.0 is",
    "start": "317945",
    "end": "325569"
  },
  {
    "text": "designing the prompt. And so the ability\nof language models of these large transformers\nto respond to their context",
    "start": "325570",
    "end": "331810"
  },
  {
    "text": "and learn from their context\nand change their behavior in response to their\ncontext, it really seems like probably\nthe most surprising",
    "start": "331810",
    "end": "338380"
  },
  {
    "text": "and striking and remarkable\nthing about them. And some of my\ncolleagues previously",
    "start": "338380",
    "end": "345940"
  },
  {
    "text": "published a paper that has a\ntrick in it that I really love, which is so we're all used to\nlooking at learning curves.",
    "start": "345940",
    "end": "351190"
  },
  {
    "text": "You train your model. And as your model trains,\nthe loss goes down.",
    "start": "351190",
    "end": "356960"
  },
  {
    "text": "Sometimes it's a little\nbit discontinuous. But it goes down. Another thing that you can\ndo is you can go and take",
    "start": "356960",
    "end": "362610"
  },
  {
    "text": "a fully trained model. And you can go and ask-- as we go through the context,\nas we go and we predict the first token and\nthen the second token",
    "start": "362610",
    "end": "369330"
  },
  {
    "text": "and the third\ntoken, we get better at predicting each token because\nwe have more information to go and predict it on.",
    "start": "369330",
    "end": "375060"
  },
  {
    "text": "So the first token,\nthe loss should be the entropy of the unigrams. And then the next token should\nbe the entry of the bigrams.",
    "start": "375060",
    "end": "381480"
  },
  {
    "text": "Then it falls. But it keeps falling. And it keeps getting better. And in some sense, that's\nthe model's ability",
    "start": "381480",
    "end": "389670"
  },
  {
    "text": "to go and predict, to go\nand do in-context learning. The ability to go\nand predict, to be",
    "start": "389670",
    "end": "396095"
  },
  {
    "text": "better at predicting\nlater tokens than you are at predicting early\ntokens, that is, in some sense, a mathematical\ndefinition of what",
    "start": "396095",
    "end": "401580"
  },
  {
    "text": "it means to be good at this\nmagical in-context learning, or meta learning, that\nthese models can do.",
    "start": "401580",
    "end": "407030"
  },
  {
    "text": "And so that's kind of cool\nbecause that gives us a way to go and look at what\nour models are good, at in-context learning.",
    "start": "407030",
    "end": "413901"
  },
  {
    "text": "Chris? Yeah. If I could just ask a\nquestion, like a clarification on what you mean by that. Yeah, please. When you say learning, there\nare no actual parameter updates",
    "start": "413901",
    "end": "420400"
  },
  {
    "text": "happening here, is that right? Oh, yeah, that is\nthe remarkable thing about in-context\nlearning, right? So, yeah, we traditionally\nthink about neural networks",
    "start": "420400",
    "end": "427260"
  },
  {
    "text": "as learning over the\ncourse of training by going and modifying\ntheir parameters. But somehow models\nappear to also be",
    "start": "427260",
    "end": "432960"
  },
  {
    "text": "able to learn, in some sense. If you give them a couple\nexamples in their context, they can then go and do\nthat later in their context,",
    "start": "432960",
    "end": "438906"
  },
  {
    "text": "even though no\nparameter's changed. And so it's some kind of quite\ndifferent notion of learning,",
    "start": "438907",
    "end": "443970"
  },
  {
    "text": "as you're gesturing. I think that's\nmaking more sense. So I mean, could you also just\ndescribe in-context learning",
    "start": "443970",
    "end": "452070"
  },
  {
    "text": "in this case as conditioning,\nas in, like, conditioning on the first 5 tokens\nof a 10-token sentence?",
    "start": "452070",
    "end": "458050"
  },
  {
    "text": "And predict the next 5 tokens? Yeah. I think the reason\nthat people sometimes think about this as in-context\nlearning, or meta learning,",
    "start": "458050",
    "end": "463337"
  },
  {
    "text": "is that you can do things where\nyou actually take a training set. And you embed the training\nset in your context,",
    "start": "463337",
    "end": "468968"
  },
  {
    "text": "like just two or three examples. And then suddenly your model\ncan go and do this task. And so you can do\na few-shot learning",
    "start": "468968",
    "end": "475500"
  },
  {
    "text": "by embedding things\nin the context. [INAUDIBLE] So yeah, the formal\nsetup is that you're just conditioning on this context.",
    "start": "475500",
    "end": "482920"
  },
  {
    "text": "And it's just that\nsomehow this ability-- like there's some sense for\na long time, people were--",
    "start": "482920",
    "end": "490080"
  },
  {
    "text": "I mean, I guess really\nthe history of this is that we started to get good\nat neural networks learning,",
    "start": "490080",
    "end": "495690"
  },
  {
    "text": "right? And we could go and train\nlanguage-- or train vision models and language\nmodels that could do all these remarkable things.",
    "start": "495690",
    "end": "500879"
  },
  {
    "text": "But then people\nstarted to be like, well, these systems, they\ntake so many more examples than humans do to go and learn.",
    "start": "500880",
    "end": "506970"
  },
  {
    "text": "How can we go and fix this? And we had all these ideas\nabout meta learning developed, where we want it to go and\ntrain models explicitly",
    "start": "506970",
    "end": "513943"
  },
  {
    "text": "to be able to learn\nfrom a few examples. And people developed all\nthese complicated schemes. And the truly, absurd thing\nabout transformer language",
    "start": "513943",
    "end": "520740"
  },
  {
    "text": "models is without\nany effort at all, we get this for free,\nthat you can go and just give them a couple of\nexamples in their context.",
    "start": "520740",
    "end": "527220"
  },
  {
    "text": "And they can learn\nin their context to go and do new things. I think that was-- that was, in some sense,\nthe most striking thing",
    "start": "527220",
    "end": "534060"
  },
  {
    "text": "about the GPT-3 paper. And so yeah, this\nability to go and have",
    "start": "534060",
    "end": "540780"
  },
  {
    "text": "the just conditioning\non a context go and give you new abilities\nfor free and the ability",
    "start": "540780",
    "end": "545970"
  },
  {
    "text": "to generalize to new\nthings is, in some sense, to me, the most striking\nand shocking thing about transformer\nlanguage models.",
    "start": "545970",
    "end": "554800"
  },
  {
    "text": "It makes sense. I guess from my\nperspective, I'm trying",
    "start": "554800",
    "end": "559930"
  },
  {
    "text": "to square the notion of learning\nin this case with if you or I were given a prompt of,\nlike, 1 plus 1 equals 2, 2",
    "start": "559930",
    "end": "568060"
  },
  {
    "text": "plus 3 equals 5 as the\nsort of few-shot setup. And then somebody else\nput, like, 5 plus 3 equals,",
    "start": "568060",
    "end": "576310"
  },
  {
    "text": "and we had to fill it out. In that case I wouldn't say\nthat we've learned arithmetic because we already\nsort of knew it.",
    "start": "576310",
    "end": "581830"
  },
  {
    "text": "But rather we're just sort\nof conditioning on the prompt to know what it is that we\nshould then generate, right?",
    "start": "581830",
    "end": "588149"
  },
  {
    "text": "But it seems to me like that's-- So that's-- Uh-huh? Yeah, I think that's\non the spectrum, though, because you\ncan also go and give",
    "start": "588150",
    "end": "594490"
  },
  {
    "text": "completely nonsensical problems,\nwhere the model would never have seen, like,\nmimic this function",
    "start": "594490",
    "end": "599893"
  },
  {
    "text": "and give a couple of\nexamples of the function. And the model's\nnever seen it before. And it can go and do that\nlater in the context.",
    "start": "599893",
    "end": "605680"
  },
  {
    "text": "And I think what you did learn\nin a lot of these cases-- so you might not have\nlearned arithmetic.",
    "start": "605680",
    "end": "612381"
  },
  {
    "text": "You might have had some\ninnate faculty for arithmetic that you're using. But you might have learned,\noh, OK, right now we're",
    "start": "612382",
    "end": "617470"
  },
  {
    "text": "doing arithmetic problems. Got it. And in the sense, I agree that\nthere's an element of semantics",
    "start": "617470",
    "end": "623143"
  },
  {
    "text": "here. Yeah. No, this is helpful\nthough just to clarify exactly sort of what you\nmean by in-context learning.",
    "start": "623143",
    "end": "629140"
  },
  {
    "text": "Thank you for\nwalking through this. Of course. So something that's I think\nreally striking about all of us",
    "start": "629140",
    "end": "637160"
  },
  {
    "text": "is-- well, OK, so\nwe've talked about how we can sort of look\nat the learning curve, and we can also look at this\nin-context learning curve.",
    "start": "637160",
    "end": "643760"
  },
  {
    "text": "But really, those\nare just two slices of a two-dimensional space. So in some sense, the\nmore fundamental thing",
    "start": "643760",
    "end": "650569"
  },
  {
    "text": "is how good are we at predicting\nthe n-th token at any given point in training? And something that you'll\nnotice if you look at this--",
    "start": "650570",
    "end": "658268"
  },
  {
    "text": "so when we talk\nabout the loss curve, where we're just talking if you\naverage over this dimension, if you average like\nthis and project",
    "start": "658268",
    "end": "665510"
  },
  {
    "text": "on to the training step,\nthat's your loss curve. And then the thing that we are\ncalling in-context learning",
    "start": "665510",
    "end": "672350"
  },
  {
    "text": "curve is just this line-- yeah, this line down\nat the end here.",
    "start": "672350",
    "end": "678740"
  },
  {
    "text": " And something that's\nkind of striking is there's this\ndiscontinuity in it.",
    "start": "678740",
    "end": "686720"
  },
  {
    "text": "There's this point\nwhere the model seems to get radically better in\na very, very short time span",
    "start": "686720",
    "end": "693199"
  },
  {
    "text": "and going and\npredicting late tokens. So it's not that different\nin early time steps. But in late time steps\nsuddenly, you get better.",
    "start": "693200",
    "end": "700010"
  },
  {
    "text": " And a way that you can\nmake this more striking",
    "start": "700010",
    "end": "705690"
  },
  {
    "text": "is you can take the\ndifference in your ability to predict the 50th\ntoken and your ability",
    "start": "705690",
    "end": "711135"
  },
  {
    "text": "to predict the 500th token. You can subtract from the 500th\ntoken the 50th token loss.",
    "start": "711135",
    "end": "716790"
  },
  {
    "text": "And what you see is that\nover the course of training,",
    "start": "716790",
    "end": "722212"
  },
  {
    "text": "you're not very good at this. And you get a little bit better. And then suddenly,\nyou have this cliff. And then you never get\nbetter than-- the difference",
    "start": "722212",
    "end": "728928"
  },
  {
    "text": "between these, at least,\nnever gets better. So the model gets better\nat predicting things. But its ability\nto go and predict",
    "start": "728928",
    "end": "734100"
  },
  {
    "text": "late tokens over early\ntokens never gets better. And so there's-- in the span\nof just a few hundred steps",
    "start": "734100",
    "end": "740610"
  },
  {
    "text": "in training, the model has\ngotten radically better at its ability to go and do this\nkind of in-context learning.",
    "start": "740610",
    "end": "748889"
  },
  {
    "text": "And so you might ask, what's\ngoing on at that point? And this is just one model. Well, so, first\nof all, it's worth",
    "start": "748890",
    "end": "755190"
  },
  {
    "text": "noting this isn't\na small change. ",
    "start": "755190",
    "end": "760680"
  },
  {
    "text": "We don't think about\nthis very often. But often, we just\nlook at loss curves. And we're like, did the model\ndo better than another model or worse than another model?",
    "start": "760680",
    "end": "766830"
  },
  {
    "text": "But you can think of\nthis in terms of nats. It's just the information\ntheoretic quantity.",
    "start": "766830",
    "end": "773100"
  },
  {
    "text": "And you can convert\nthat into to bits. And so one way you can interpret\nthis is it's something roughly",
    "start": "773100",
    "end": "778620"
  },
  {
    "text": "like the model at 0.4\nnats is about 0.5 bits is about every other token the\nmodel gets to go and sample",
    "start": "778620",
    "end": "785430"
  },
  {
    "text": "twice and pick the better one. It's actually-- it's\neven stronger than that. That's a sort of\nan underestimate",
    "start": "785430",
    "end": "790615"
  },
  {
    "text": "of how big a deal going and\ngetting better by 0.4 nats is. So this is a real big difference\nin the model's ability",
    "start": "790615",
    "end": "796980"
  },
  {
    "text": "to go and predict late tokens. And we can visualize\nthis in different ways.",
    "start": "796980",
    "end": "802530"
  },
  {
    "text": "We can also go and ask,\nhow much better are we getting at going and\npredicting later tokens? And look at the\nderivative, and then we",
    "start": "802530",
    "end": "808980"
  },
  {
    "text": "can see very\nclearly that there's some kind of discontinuity in\nthat derivative at this point. And we could take the\nsecond derivative then.",
    "start": "808980",
    "end": "815430"
  },
  {
    "text": "And we can-- well, derivative\nwith respect to training. And now we see that\nthere's very, very clearly",
    "start": "815430",
    "end": "822510"
  },
  {
    "text": "this line here. So something in just the\nspan of a few hundred steps is causing some big change.",
    "start": "822510",
    "end": "829209"
  },
  {
    "text": "And we have some kind of\nphase change going on. And this is true\nacross model sizes.",
    "start": "829210",
    "end": "835620"
  },
  {
    "text": "You can actually see it a\nlittle bit in the loss curve. And there's this\nlittle bump here. And that corresponds\nto the point",
    "start": "835620",
    "end": "841050"
  },
  {
    "text": "where you have this change. We actually could have seen\nin the loss curve earlier too. It's this bump here.",
    "start": "841050",
    "end": "848760"
  },
  {
    "text": "Excuse me. So we have this phase\nchange going on. And there's, I think, a really\ntempting theory to have,",
    "start": "848760",
    "end": "853990"
  },
  {
    "text": "which is that somehow,\nthis change in the model's output and its behaviors\nand in these sort",
    "start": "853990",
    "end": "861269"
  },
  {
    "text": "of outward-facing properties\ncorresponds presumably to some kind of change\nin the algorithms that",
    "start": "861270",
    "end": "866550"
  },
  {
    "text": "are running inside the model. So if we observe this big\nphase change, especially in a very small window\nin the model's behavior,",
    "start": "866550",
    "end": "873782"
  },
  {
    "text": "presumably there's some\nchange in the circuits inside the model\nthat is driving it. At least, that's a\nnatural hypothesis.",
    "start": "873783",
    "end": "880240"
  },
  {
    "text": "So if we want to\nask that, though, we need to go be able\nto understand, what are the algorithms that's\nrunning inside the model?",
    "start": "880240",
    "end": "886270"
  },
  {
    "text": "How can we turn the\nparameters in the model back into those algorithms? So that's going to be our goal.",
    "start": "886270",
    "end": "891519"
  },
  {
    "text": "Now, it's going to require\nus to cover a lot of ground in a relatively\nshort amount of time.",
    "start": "891520",
    "end": "896615"
  },
  {
    "text": "So I'm going to go\na little bit quickly through the next section. And I will highlight sort\nof the key takeaways.",
    "start": "896615",
    "end": "902079"
  },
  {
    "text": "And then I will be very happy\nto go and explore any of this",
    "start": "902080",
    "end": "907240"
  },
  {
    "text": "in as much depth-- I'm free for another\nhour after this call and just happy to\ntalk in as much depth",
    "start": "907240",
    "end": "912520"
  },
  {
    "text": "as people want about\nthe details of this. So it turns out\nthis phase change doesn't happen in a one-layer\nattention-only transformer.",
    "start": "912520",
    "end": "920410"
  },
  {
    "text": "And it does happen in a\ntwo-layer attention-only transformer. So if we could understand\na one-layer attention-only",
    "start": "920410",
    "end": "925576"
  },
  {
    "text": "transformer and a two-layer\nattention-only transformer, that might give us a pretty\nbig clue as to what's going on.",
    "start": "925577",
    "end": "933170"
  },
  {
    "text": "And so we're attention only. We're also going to leave\nout LayerNorm and biases to simplify things.",
    "start": "933170",
    "end": "939210"
  },
  {
    "text": "So one way you could describe\nan attention-only transformer is we're going to\nembed our tokens.",
    "start": "939210",
    "end": "945337"
  },
  {
    "text": "And then we're going to apply\na bunch of attention heads and add them into\nthe residual strain and then apply our unembedding.",
    "start": "945337",
    "end": "950820"
  },
  {
    "text": "And that will give\nus our logits. And we could go and write that\nout as equations, if we want, multiply it by an embedding\nmatrix, apply attention heads,",
    "start": "950820",
    "end": "959220"
  },
  {
    "text": "and then compute the\nlogits behind embedding. And the part here\nthat's a little tricky",
    "start": "959220",
    "end": "966730"
  },
  {
    "text": "is understanding\nthe attention heads. And this might be a\nsomewhat conventional way of describing attention heads. And it actually kind\nof obscures a lot",
    "start": "966730",
    "end": "974290"
  },
  {
    "text": "of the structure\nof attention heads. I think that oftentimes it's\nwe make attention heads more complex than they\nare, or we sort of",
    "start": "974290",
    "end": "980560"
  },
  {
    "text": "hide the interesting structure. So what is this saying? Well, it's saying, for every\ntoken, compute a value vector.",
    "start": "980560",
    "end": "987181"
  },
  {
    "text": "And then go and mix the\nvalue vectors according to the attention matrix. And then project them\nwith the output matrix",
    "start": "987182",
    "end": "992590"
  },
  {
    "text": "back into the residual string. So there's another\nnotation, which",
    "start": "992590",
    "end": "998379"
  },
  {
    "text": "you could think of this\nas using tensor products or using I guess left and\nright multiplying this.",
    "start": "998380",
    "end": "1005600"
  },
  {
    "text": "There's a few ways you\ncan interpret this. But I'll just sort\nof try to explain",
    "start": "1005600",
    "end": "1010649"
  },
  {
    "text": "what this notation means. So this means for every\nx, our residual stream,",
    "start": "1010650",
    "end": "1017370"
  },
  {
    "text": "we have a vector for\nevery single token. And this means, go and multiply\nindependently the vector",
    "start": "1017370",
    "end": "1024270"
  },
  {
    "text": "for each token by WV. So compute the value\nvector for every token.",
    "start": "1024270",
    "end": "1029400"
  },
  {
    "text": "This one, on the\nother hand, means-- notice that it's now on the-- A is on the left-hand side. It means, go and multiply\nthe attention matrix--",
    "start": "1029400",
    "end": "1038579"
  },
  {
    "text": "or go and do linear\ncombinations of value vectors. So don't change the\nvalue vectors pointwise,",
    "start": "1038579",
    "end": "1043608"
  },
  {
    "text": "but go and mix them together\naccording to the attention pattern. Create our weighted sum. And then, again, independently\nfor every position,",
    "start": "1043608",
    "end": "1050880"
  },
  {
    "text": "go and apply the output matrix. And you can apply the\ndistributive property to this. And it just reveals\nthat actually, it",
    "start": "1050880",
    "end": "1057330"
  },
  {
    "text": "didn't matter that\nyou did the attention sort of in the middle. You could have done the\nattention at the beginning. You could have\ndone it at the end.",
    "start": "1057330",
    "end": "1062440"
  },
  {
    "text": "That's independent. And the thing that\nactually matters is there's this WV WO\nmatrix that describes--",
    "start": "1062440",
    "end": "1068820"
  },
  {
    "text": "what it's really saying is-- WV WO describes what information\nthe attention head reads",
    "start": "1068820",
    "end": "1074102"
  },
  {
    "text": "from each position and how it\nwrites it to its destination, whereas A describes which tokens\nwe read from and write to.",
    "start": "1074102",
    "end": "1081390"
  },
  {
    "text": "And that's kind of getting\nmore the fundamental structure in an attention head. An attention head goes\nand moves information",
    "start": "1081390",
    "end": "1086669"
  },
  {
    "text": "from one position to another. And the process of which\nposition gets moved from and to is independent from\nwhat information gets moved.",
    "start": "1086670",
    "end": "1093915"
  },
  {
    "text": " And if you rewrite your\ntransformer that way--",
    "start": "1093915",
    "end": "1100480"
  },
  {
    "text": "well, first, we can\ngo and write the sum of attention heads, just as-- in this form.",
    "start": "1100480",
    "end": "1106870"
  },
  {
    "text": "And then we can go in and\nwrite that as the entire layer by going and adding\nin an identity.",
    "start": "1106870",
    "end": "1114410"
  },
  {
    "text": "And if we go and plug that\nall in to our transformer and go and expand, we have\nto go and multiply everything",
    "start": "1114410",
    "end": "1123980"
  },
  {
    "text": "through. We get this\ninteresting equation. And so we get this one term. This corresponds to\njust the path directly",
    "start": "1123980",
    "end": "1129980"
  },
  {
    "text": "through the residual string. And it's going to want to\nstore bigram statistics. It's just-- all it gets\nis the previous token",
    "start": "1129980",
    "end": "1136390"
  },
  {
    "text": "and tries to predict\nthe next token. And so it gets to\ntry and predict-- try to store bigram statistics.",
    "start": "1136390",
    "end": "1141730"
  },
  {
    "text": "And then for every\nattention head, we get this matrix\nthat says, OK, well, we have the attention pattern. So it looks-- that\ndescribes which",
    "start": "1141730",
    "end": "1147970"
  },
  {
    "text": "token looks at which token. And we have this\nmatrix here, which describes how, for\nevery possible token you could attend to, how\nit affects the logits.",
    "start": "1147970",
    "end": "1155695"
  },
  {
    "text": "And that's just a table\nthat you can look at. It just says, for this attention\nhead, it looks at this token. It's going to increase the\nprobability of these tokens.",
    "start": "1155695",
    "end": "1162550"
  },
  {
    "text": "In a one-layer\nattention-only transformer, that's all there is. ",
    "start": "1162550",
    "end": "1169070"
  },
  {
    "text": "Yeah, so this is just\nthe interpretation I was describing.",
    "start": "1169070",
    "end": "1174680"
  },
  {
    "text": "And another thing that's worth\nnoting is according to this, the attention-only\ntransformer is linear",
    "start": "1174680",
    "end": "1180559"
  },
  {
    "text": "if you fix the\nattention pattern. Now, of course, the attention\npattern isn't fixed. But whenever you\nhave the opportunity",
    "start": "1180560",
    "end": "1186140"
  },
  {
    "text": "to go and make something\nlinear, linear functions are really easy to understand. And so if you can fix a\nsmall number of things",
    "start": "1186140",
    "end": "1191240"
  },
  {
    "text": "and make something\nlinear, that's actually a lot of leverage. OK.",
    "start": "1191240",
    "end": "1197640"
  },
  {
    "text": "And yeah, we could talk\nabout how the attention pattern is computed as well. If you expand it out, you'll\nget an equation like this.",
    "start": "1197640",
    "end": "1205660"
  },
  {
    "text": "And notice-- well, I\nthink it'll be easier. OK. ",
    "start": "1205660",
    "end": "1212757"
  },
  {
    "text": "I think the core story,\nthough, to take away from all of these is we\nhave these two matrices that actually look kind of similar.",
    "start": "1212757",
    "end": "1218470"
  },
  {
    "text": "So this one here tells you,\nif you attend to a token, how are the logits affected?",
    "start": "1218470",
    "end": "1224580"
  },
  {
    "text": "And you can just think of\nit as a giant matrix of, for every possible\ninput token, how",
    "start": "1224580",
    "end": "1229740"
  },
  {
    "text": "is the logit-- how are the\nlogits affected by that token? Are they made more\nlikely or less likely? And we have this one,\nwhich sort of says,",
    "start": "1229740",
    "end": "1236400"
  },
  {
    "text": "how much does every token want\nto attend to every other token? ",
    "start": "1236400",
    "end": "1243020"
  },
  {
    "text": "One way that you\ncan picture this is, OK, there's really\nthree tokens involved",
    "start": "1243020",
    "end": "1249017"
  },
  {
    "text": "when we're thinking\nabout an attention head. We have the token that we're\ngoing to move information to",
    "start": "1249017",
    "end": "1256070"
  },
  {
    "text": "and that's attending backwards. We have the source token that's\ngoing to get attended to.",
    "start": "1256070",
    "end": "1261380"
  },
  {
    "text": "And we have the output\ntoken whose logits are going to be affected. And you can just\ntrace through this.",
    "start": "1261380",
    "end": "1266830"
  },
  {
    "text": "So you can ask, what happens-- how does attending to this\ntoken affect the output?",
    "start": "1266830",
    "end": "1272150"
  },
  {
    "text": "Well, first, we embed the token. Then we multiply by WV\nto get the value vector.",
    "start": "1272150",
    "end": "1278500"
  },
  {
    "text": "The information gets moved\nby the attention pattern. We multiply by WO to add it\nback into the residual string.",
    "start": "1278500",
    "end": "1283570"
  },
  {
    "text": "We get hit by the end embedding. And we affect the logits. And that's where that\none matrix comes from. And we can also\nask, what decides",
    "start": "1283570",
    "end": "1290620"
  },
  {
    "text": "whether a token gets a high\nscore when we're computing the attention pattern? And it just says,\nembed the token.",
    "start": "1290620",
    "end": "1299360"
  },
  {
    "text": "Turn it into a query. Embed the other token. Turn it into a key. And dot product them. And so that's where those\ntwo matrices come from.",
    "start": "1299360",
    "end": "1307980"
  },
  {
    "text": "So I know that I'm\ngoing quite quickly. Maybe I'll just\nbriefly pause here.",
    "start": "1307980",
    "end": "1314419"
  },
  {
    "text": "And if anyone wants to\nask for clarifications, this would be a good time. And then we'll actually go\nand reverse engineer and say,",
    "start": "1314420",
    "end": "1320697"
  },
  {
    "text": "everything that's going on\nin a one-layer attention-only transformer is now in\nthe palm of our hands.",
    "start": "1320697",
    "end": "1326270"
  },
  {
    "text": "It's a very toy model. No one actually uses one-layer\nattention-only transformers. But we'll be able to understand\nthe one-layer attention-only",
    "start": "1326270",
    "end": "1333140"
  },
  {
    "text": "transformer.  So just to be clear,\nso you're saying that the query-key circuit here\nis learning retention rates?",
    "start": "1333140",
    "end": "1342888"
  },
  {
    "text": "And essentially, it\njust ends up running some of the attention\nbetween different tokens? Yeah, yeah.",
    "start": "1342888",
    "end": "1349269"
  },
  {
    "text": "So this matrix, when it--\nyeah, all three of those parts are learned. But that's what expresses\nwhether a attention pattern--",
    "start": "1349270",
    "end": "1358110"
  },
  {
    "text": "yeah. That's what generates\nthe attention patterns, gets run for every\npair of tokens. And you can think of\nvalues in that matrix",
    "start": "1358110",
    "end": "1364280"
  },
  {
    "text": "as just being how\nmuch every token wants to attend to every other token\nif it was in the context. We're ignoring positional\nembeddings here.",
    "start": "1364280",
    "end": "1370820"
  },
  {
    "text": "So there's a little bit that\nwe're sort of aligning over there as well. But sort of in a\nglobal sense, how much does every token want to\nattend to every other token?",
    "start": "1370820",
    "end": "1377498"
  },
  {
    "text": "Right. And the other circuit, like,\nthe output-value circuit, is using the attention that's\ncalculated to, I guess,",
    "start": "1377498",
    "end": "1385970"
  },
  {
    "text": "affect the final outputs? It's sort of saying,\nif the attention head-- assume that the attention\nhead attends to some token.",
    "start": "1385970",
    "end": "1391880"
  },
  {
    "text": "So let's set aside the question\nof how that gets computed. Just assume that it\nattends to some token. How would it affect the outputs\nif it attended to that token?",
    "start": "1391880",
    "end": "1398472"
  },
  {
    "text": "Mm-hmm. And you can just calculate that. It's just a big table of values\nthat says, for this token,",
    "start": "1398472",
    "end": "1404440"
  },
  {
    "text": "it's going to make\nthis token more likely. This token will make\nthis token less likely. Right.",
    "start": "1404440",
    "end": "1409870"
  },
  {
    "text": "OK, and that's just-- And it's completely independent. Like, it's just two\nseparate matrices. They're not-- the formulas\nmight make them seem entangled.",
    "start": "1409870",
    "end": "1418360"
  },
  {
    "text": "But they're actually separate. Right. To me, it seems like the-- like just supervision is coming\nfrom the output value circuit.",
    "start": "1418360",
    "end": "1425670"
  },
  {
    "text": "And the query-key circuit\nseems to be more unsupervised, kind of thing,\nbecause there's no--",
    "start": "1425670",
    "end": "1430950"
  },
  {
    "text": "I mean, there are--\njust, I think, in the sense that every-- yeah, in a model, every\nneuron is, in some sense--",
    "start": "1430950",
    "end": "1439350"
  },
  {
    "text": "its signal is somehow downstream\nfrom the ultimate signal. And so, yeah, the\noutput-value signal--",
    "start": "1439350",
    "end": "1446202"
  },
  {
    "text": "the output-value\ncircuit is getting more direct-- it's perhaps\ngetting more direct signal. Correct.",
    "start": "1446202",
    "end": "1451610"
  },
  {
    "text": "But yeah. Yes. ",
    "start": "1451610",
    "end": "1456770"
  },
  {
    "text": "We will be able to dig into\nthis in lots of detail, in as much detail as you\nwant, in a little bit. So we can-- maybe\nI'll push forward.",
    "start": "1456770",
    "end": "1463976"
  },
  {
    "text": "And I think also,\nactually, an example of how to use this to reverse\nengineer a one-layer model will maybe make it a\nlittle bit more motivated.",
    "start": "1463977",
    "end": "1471450"
  },
  {
    "text": "OK. So just to emphasize this,\nthere's three different tokens",
    "start": "1471450",
    "end": "1476571"
  },
  {
    "text": "that we can talk about. There's the token\nthat gets attended to. There's the token that\ndoes the attention, which",
    "start": "1476572",
    "end": "1481672"
  },
  {
    "text": "I call the destination. And then there's the\ntoken that gets affected-- gets the next token, which its\nprobabilities are affected.",
    "start": "1481672",
    "end": "1486967"
  },
  {
    "text": " And so something we\ncan do is-- notice that the only token that\nconnects to both of these",
    "start": "1486967",
    "end": "1494370"
  },
  {
    "text": "is the token that\ngets attended to. So these two are sort of-- they're bridged by\ntheir interaction",
    "start": "1494370",
    "end": "1500963"
  },
  {
    "text": "with the source token. So something that's\nkind of natural is to ask, for a\ngiven source token, how does it interact\nwith both of these?",
    "start": "1500963",
    "end": "1507585"
  },
  {
    "text": " So let's take, for instance,\nthe token \"perfect.\"",
    "start": "1507585",
    "end": "1513890"
  },
  {
    "text": "Which tokens-- one\nthing we can ask is, which tokens want to\nattend to \"perfect\"?",
    "start": "1513890",
    "end": "1519410"
  },
  {
    "text": "Well, apparently, the\ntokens that most want to attend to perfect are\n\"are,\" and \"looks,\" and \"is,\"",
    "start": "1519410",
    "end": "1524682"
  },
  {
    "text": "and \"provides.\" So \"are\" is the most, \"looks\"\nis the next most, and so on. And then, when we\nattend to \"perfect\"--",
    "start": "1524682",
    "end": "1530840"
  },
  {
    "text": "and this is with one\nsingle attention head, so it'd be different if we did\na different attention head-- it wants to really increase\nthe probability of \"perfect,\"",
    "start": "1530840",
    "end": "1538015"
  },
  {
    "text": "and then, to a lesser extent,\n\"super,\" and \"absolute,\" and \"pure.\" And we can ask, what\nsequences of tokens",
    "start": "1538015",
    "end": "1545390"
  },
  {
    "text": "are made more likely by\nthis particular set of--",
    "start": "1545390",
    "end": "1550768"
  },
  {
    "text": "this particular set of things\nwanting to attend to each other and becoming more likely? Well, things of the form--",
    "start": "1550768",
    "end": "1557300"
  },
  {
    "text": "we have our token that\nwe attended back to. And we have some skip of\nsome number of tokens. They don't have to be adjacent.",
    "start": "1557300",
    "end": "1563059"
  },
  {
    "text": "But then, later on, we\nsee the token \"are.\" And it attends back to\n\"perfect\" and increases the probability of \"perfect.\"",
    "start": "1563060",
    "end": "1569450"
  },
  {
    "text": "So you can think\nof these as being-- like, we're sort of\ncreating-- changing the probability of what we\nmight call skip trigrams, where",
    "start": "1569450",
    "end": "1575450"
  },
  {
    "text": "we have-- we skip over a\nbunch of tokens in the middle. But we're affecting the\nprobability, really, of trigrams, so\n\"perfect,\" \"are perfect,\"",
    "start": "1575450",
    "end": "1583460"
  },
  {
    "text": "\"perfect,\" \"looks super.\" We can look at another one. So we get the token \"large.\"",
    "start": "1583460",
    "end": "1588770"
  },
  {
    "text": "These tokens,\n\"contains,\" \"using,\" \"specify,\" want to go\nand look back to it, and an increase\nin the probability of \"large\" and \"small.\"",
    "start": "1588770",
    "end": "1594590"
  },
  {
    "text": "And the skip trigrams\nthat are affected are things like\n\"large,\" \"using large,\" \"large,\" \"contains small,\"\nand things like this.",
    "start": "1594590",
    "end": "1605100"
  },
  {
    "text": "If we see the number two,\nwe increase the probability of other numbers. And we affect the\nprobability tokens,",
    "start": "1605100",
    "end": "1610790"
  },
  {
    "text": "or skip trigrams, like \"two,\"\n\"one two,\" \"two,\" \"has three.\"",
    "start": "1610790",
    "end": "1617590"
  },
  {
    "text": "Now, you're all in\na technical field. So you'll probably\nrecognize this one. We have \"lambda.\"",
    "start": "1617590",
    "end": "1623680"
  },
  {
    "text": "And then we see backslash. And then we want to\nincrease the probability of \"lambda,\" and \"sorted,\"\nand \"lambda,\" and \"operator.\"",
    "start": "1623680",
    "end": "1630280"
  },
  {
    "text": "So it's all LaTeX. It wants to-- if\nit sees \"lambda,\" it thinks that maybe next\ntime, I use a backslash.",
    "start": "1630280",
    "end": "1637000"
  },
  {
    "text": "I should go and put in\nsome LaTeX math symbol.",
    "start": "1637000",
    "end": "1642100"
  },
  {
    "text": "Also, same thing for HTML. We see \"nbsp\" for a\nnon-breaking space. Then we see an ampersand.",
    "start": "1642100",
    "end": "1647980"
  },
  {
    "text": "We want to go and\nmake that more likely. The takeaway from\nall this is that a one-layer attention-only\ntransformer is totally",
    "start": "1647980",
    "end": "1653382"
  },
  {
    "text": "acting on these skip trigrams. Everything that it does--",
    "start": "1653382",
    "end": "1658650"
  },
  {
    "text": "I mean, I guess it\nalso has this pathway by which it affects bigrams. But mostly, it's just\naffecting these skip trigrams. And there's lots of them.",
    "start": "1658650",
    "end": "1664490"
  },
  {
    "text": "It's just, like, these giant\ntables of skip trigrams that are made more\nor less likely. ",
    "start": "1664490",
    "end": "1671002"
  },
  {
    "text": "There's lots of other\nfun things it does. Sometimes the\ntokenization will split up a word in multiple ways. So, like, we have \"indy.\"",
    "start": "1671002",
    "end": "1677820"
  },
  {
    "text": "Well, that's not a good example. We have the word \"Pike.\" And then we see the token P.\nAnd then we predict \"ike.\"",
    "start": "1677820",
    "end": "1685730"
  },
  {
    "text": "And we predict \"spikes\"\nand stuff like that. Or these ones are kind of fun. Maybe they're actually worth\ntalking about for a second.",
    "start": "1685730",
    "end": "1692983"
  },
  {
    "text": "So we see the token \"Lloyd.\" And then we see an L, and\nmaybe we predict \"Lloyd,\" or R,",
    "start": "1692983",
    "end": "1700190"
  },
  {
    "text": "and we predict \"Ralph,\"\nC, \"Catherine.\" But we'll see in a second\nthat-- well, yeah, we'll",
    "start": "1700190",
    "end": "1706250"
  },
  {
    "text": "come back to that in a sec. So we increase the probability\nof things like \"Lloyd,\" \"Lloyd\" and \"Lloyd,\" \"Catherine.\" Or \"Pixmap\"-- if anyone's\nworked with QT, we see \"Pixmap,\"",
    "start": "1706250",
    "end": "1715640"
  },
  {
    "text": "and we increase the\nprobability of P, \"Pixmap\" again, but\nalso Q, \"Canvas.\"",
    "start": "1715640",
    "end": "1722750"
  },
  {
    "text": " But of course, there's\na problem with this,",
    "start": "1722750",
    "end": "1728800"
  },
  {
    "text": "which is, it doesn't get\nto pick which one of these goes with which one. So if you want to go and\nmake \"Pixmap,\" \"Pixmap\"",
    "start": "1728800",
    "end": "1736800"
  },
  {
    "text": "and \"Pixmap,\" \"QCanvas\"\nmore profitable, you also have to go\nand make \"Pixmap,\"",
    "start": "1736800",
    "end": "1742200"
  },
  {
    "text": "\"PCanvas\" more probable. And if you want to make\n\"Lloyd,\" \"Lloyd\" and \"Lloyd,\" \"Catherine\" more\nprobable, you also",
    "start": "1742200",
    "end": "1749130"
  },
  {
    "text": "have to make \"Lloyd,\" \"Cloyd\"\nand \"Lloyd,\" \"Latherine\" more probable.",
    "start": "1749130",
    "end": "1754680"
  },
  {
    "text": "And so there's actually\nbugs that transformers have, like weird-- at\nleast in these really tiny, one-layer attachment-only\ntransformers,",
    "start": "1754680",
    "end": "1761190"
  },
  {
    "text": "there's these bugs\nthat-- they seem weird until you\nrealize that it's this giant table of skip\ntrigrams that's operating.",
    "start": "1761190",
    "end": "1768860"
  },
  {
    "text": "And the nature of that is\nthat you're going to be-- ",
    "start": "1768860",
    "end": "1774570"
  },
  {
    "text": "it sort of forces\nyou, if you want to go and do this,\nto go and also make some weird predictions.",
    "start": "1774570",
    "end": "1780610"
  },
  {
    "text": "Chris, quickly,\nis there a reason why the source tokens\nhere you have a space before the first character?",
    "start": "1780610",
    "end": "1786880"
  },
  {
    "text": "Yes. That's just the-- I\nwas giving examples where the tokenization\nbreaks in a particular way.",
    "start": "1786880",
    "end": "1793130"
  },
  {
    "text": "And-- OK. --because spaces get included\nin the tokenization when",
    "start": "1793130",
    "end": "1798827"
  },
  {
    "text": "there's a space in\nfront of something, and then there's an example\nwhere the space isn't in front of it, they can get\ntokenized in different ways.",
    "start": "1798827",
    "end": "1804820"
  },
  {
    "text": "Got it. Cool. Thanks. Yeah. Great question. ",
    "start": "1804820",
    "end": "1812060"
  },
  {
    "text": "OK, so just to abstract\naway some common patterns that we're seeing, I think one\npretty common thing is what you",
    "start": "1812060",
    "end": "1817820"
  },
  {
    "text": "might describe as like b, a, b. So you go, and you\nsee some token, and then you see another token\nthat might precede that token.",
    "start": "1817820",
    "end": "1825240"
  },
  {
    "text": "And then you're\nlike, ah, probably, the token that I saw earlier\nis going to occur again. Or sometimes you predict a\nslightly different token.",
    "start": "1825240",
    "end": "1832400"
  },
  {
    "text": "So maybe an example of the\nfirst one is two, one, two. But you can also\ndo two, has, three.",
    "start": "1832400",
    "end": "1839757"
  },
  {
    "text": "And so three isn't the same as\ntwo, but it's kind of similar. So that's one thing. Another one is\nthis example, where",
    "start": "1839758",
    "end": "1844860"
  },
  {
    "text": "you have a token\nthat-- something that's tokenized\ntogether one time, and then it's split apart. So you see the token. And then you see\nsomething that might",
    "start": "1844860",
    "end": "1851210"
  },
  {
    "text": "be the first part of the token. And then you predict\nthe second part. ",
    "start": "1851210",
    "end": "1856965"
  },
  {
    "text": "I think the thing that's\nreally striking about this is these are all, in some\nways, a really crude kind",
    "start": "1856965",
    "end": "1862779"
  },
  {
    "text": "of in-context learning. And in particular, these\nmodels get about 0.1 nats",
    "start": "1862780",
    "end": "1869050"
  },
  {
    "text": "rather than about 0.4 nats\nof in-context learning. And they never go\nthrough the phase change. So they're doing some kind\nof really crude in-context",
    "start": "1869050",
    "end": "1875720"
  },
  {
    "text": "learning. And also, they're dedicating\nalmost all their attention heads to this kind of\ncrude in-context learning. So they're not very good at\nit, but they're dedicating",
    "start": "1875720",
    "end": "1883210"
  },
  {
    "text": "their capacity to it. I'm noticing that it's 10:37.",
    "start": "1883210",
    "end": "1889160"
  },
  {
    "text": "I want to just check\nhow long I can go, because maybe I should like\nsuper-accelerate if this is--",
    "start": "1889160",
    "end": "1894830"
  },
  {
    "text": "Chris, I think it's fine,\nbecause students are also asking questions in between. So you should be good.",
    "start": "1894830",
    "end": "1901030"
  },
  {
    "text": "OK, so maybe my plan\nwill be that I'll talk until like 10:55 or 11:00. Yeah, sure. And then if you guys\nwant, I can go and answer",
    "start": "1901030",
    "end": "1908049"
  },
  {
    "text": "questions for a\nwhile after that. Yeah, it works. Fantastic.",
    "start": "1908050",
    "end": "1913130"
  },
  {
    "text": "So you can see this\nas a very crude kind of in-context learning. Like, basically,\nwhat we're saying is, it's sort of all the flavor\nof, OK, well, I saw this token.",
    "start": "1913130",
    "end": "1920290"
  },
  {
    "text": "Probably, these other\ntokens, the same token or similar tokens are more\nlikely to go and occur later. And look, this is\nan opportunity that",
    "start": "1920290",
    "end": "1926380"
  },
  {
    "text": "sort of looks like\nI could inject the token that I saw earlier. I'm going to inject it here\nand say that it's more likely. That's basically\nwhat it's doing.",
    "start": "1926380",
    "end": "1933490"
  },
  {
    "text": "And it's dedicating almost\nall of its capacity to that. So it's sort of the\nopposite of what we thought with RNNs in the past.",
    "start": "1933490",
    "end": "1939010"
  },
  {
    "text": "Like, it used to be\nthat everyone was like, oh, RNNs, it's so hard\nto get them to care about long-distance contexts.",
    "start": "1939010",
    "end": "1944215"
  },
  {
    "text": "You know, maybe we need to\ngo and use dams or something. No. If you train a\ntransformer, it dedicates--",
    "start": "1944215",
    "end": "1949720"
  },
  {
    "text": "and you give it a\nlong enough context, it's dedicating almost\nall of its capacity to this type of stuff, which\nis kind of interesting.",
    "start": "1949720",
    "end": "1958720"
  },
  {
    "text": "There are some\nattention heads which are more primarily positional. Usually, we-- in\nthe model that I've",
    "start": "1958720",
    "end": "1964713"
  },
  {
    "text": "been training that\nhas two layer-- or it's only a one-layer\nmodel-- has 12 attention heads. And usually, around\ntwo or three of those",
    "start": "1964713",
    "end": "1970000"
  },
  {
    "text": "will become these\nmore positional, sort of shorter-term things\nthat do something more like local trigram statistics.",
    "start": "1970000",
    "end": "1975367"
  },
  {
    "text": "And then everything else\nbecomes these skip trigrams. ",
    "start": "1975367",
    "end": "1981970"
  },
  {
    "text": "Yeah, so some\ntakeaways from this. You can understand one-layer\nattention-only transformers",
    "start": "1981970",
    "end": "1987380"
  },
  {
    "text": "in terms of these\nOV and QK circuits. Transformers desperately want\nto do in-context learning.",
    "start": "1987380",
    "end": "1993110"
  },
  {
    "text": "They desperately,\ndesperately, desperately want to go and look at these\nlong-distance contexts",
    "start": "1993110",
    "end": "1998427"
  },
  {
    "text": "and go and predict things. There's just so much\nentropy that they can go and reduce out of that.",
    "start": "1998427",
    "end": "2003605"
  },
  {
    "text": "The constraints of a one-layer\nattention-only transformer force it to make certain bugs if\nit wants to do the right thing.",
    "start": "2003605",
    "end": "2009370"
  },
  {
    "text": "And if you freeze the\nattention patterns, these models are linear. OK.",
    "start": "2009370",
    "end": "2016090"
  },
  {
    "text": "A quick aside, because\nso far, this type of work has required us to do a lot\nof very manual inspection.",
    "start": "2016090",
    "end": "2021310"
  },
  {
    "text": "Like, we're looking through\nthese giant matrices. But there's a way that\nwe can escape that. We don't have to look\nat these giant matrices",
    "start": "2021310",
    "end": "2026590"
  },
  {
    "text": "if we don't want to. We can use eigenvalues\nand eigenvectors. So recall that an eigenvalue\nand an eigenvector",
    "start": "2026590",
    "end": "2033670"
  },
  {
    "text": "just means that if you multiply\nthat vector by the matrix, it's equivalent to just scaling.",
    "start": "2033670",
    "end": "2040380"
  },
  {
    "text": "And often, in my\nexperience, those haven't been very useful\nfor interpretability, because we're usually mapping\nbetween different spaces.",
    "start": "2040380",
    "end": "2046890"
  },
  {
    "text": "But if you're mapping\nonto the same space, eigenvalues and eigenvectors\nare a beautiful way to think about things.",
    "start": "2046890",
    "end": "2052149"
  },
  {
    "text": "So we're going to draw\nthem on a radial plot.",
    "start": "2052150",
    "end": "2057908"
  },
  {
    "text": "And we're going to have a log\nradial scale, because they're going to vary-- their\nmagnitude is going to vary by many orders of magnitude.",
    "start": "2057909",
    "end": "2065879"
  },
  {
    "text": "OK, so we can just go\nin and our OV circuit maps from tokens to tokens. That's the same vector space\non the input and the output.",
    "start": "2065880",
    "end": "2072489"
  },
  {
    "text": "And we can ask, you\nknow, what does it mean if we see eigenvalues\nof a particular kind? Well, positive\neigenvalues-- and this",
    "start": "2072489",
    "end": "2078060"
  },
  {
    "text": "is really the most important\npart-- mean copying. So if you have a\npositive eigenvalue, it means that there's\nsome set of tokens",
    "start": "2078060",
    "end": "2084989"
  },
  {
    "text": "where if you see them, you\nincrease their probability. And if you have a lot\nof positive eigenvalues,",
    "start": "2084989",
    "end": "2090129"
  },
  {
    "text": "you're doing a lot of copying. If you only have positive\neigenvalues, everything you do is copying.",
    "start": "2090130",
    "end": "2095250"
  },
  {
    "text": "Now, imaginary eigenvalues\nmean that you see a token, and then you want\nto go and increase the probability of\nunrelated tokens.",
    "start": "2095250",
    "end": "2100470"
  },
  {
    "text": "And finally, negative\neigenvalues are anti-copying. They're like, if\nyou see this token, you make it less\nprobable in the future.",
    "start": "2100470",
    "end": "2107262"
  },
  {
    "text": "Well, that's really\nnice, because now, we don't have to go and dig through\nthese giant matrices that are vocab size by vocab size. We can just look\nat the eigenvalues.",
    "start": "2107262",
    "end": "2115069"
  },
  {
    "text": "And so these are the\neigenvalues for our one-layer attention-only transformer. And we can see that\nfor many of these,",
    "start": "2115070",
    "end": "2123310"
  },
  {
    "text": "they're almost\nentirely positive. These one are sort\nof entirely positive. These ones are almost\nentirely positive.",
    "start": "2123310",
    "end": "2129485"
  },
  {
    "text": "And really, these ones are\neven almost entirely positive. And there's only two that\nhave a significant number",
    "start": "2129485",
    "end": "2135010"
  },
  {
    "text": "of imaginary and\nnegative eigenvalues. And so what this is telling us\nis-- it's just in one picture.",
    "start": "2135010",
    "end": "2140270"
  },
  {
    "text": "We can see, OK, they're really-- 10 out of 12 of these attention\nheads are just doing copying.",
    "start": "2140270",
    "end": "2147210"
  },
  {
    "text": "They just are doing this\nlong-distance, well, I saw a token, probably it's going\nto occur again, type stuff.",
    "start": "2147210",
    "end": "2152640"
  },
  {
    "text": "That's kind of cool. We can summarize\nit really quickly. OK.",
    "start": "2152640",
    "end": "2158410"
  },
  {
    "text": "Now, the other thing\nthat you can-- yeah, so this is for a\nsecond-- we're going to look at a two-layer\nmodel in a second.",
    "start": "2158410",
    "end": "2163538"
  },
  {
    "text": "And we'll see that\nalso, a lot of its heads are doing this kind\nof copying of stuff. They have large\npositive eigenvalues.",
    "start": "2163538",
    "end": "2170740"
  },
  {
    "text": "You can do a histogram. Like, one thing\nthat's cool is you can just add up the\neigenvalues and divide them",
    "start": "2170740",
    "end": "2175938"
  },
  {
    "text": "by their absolute values. And you get a number\nbetween 0 and 1, which is like how copying-- how copying is just the head--\nor between negative 1 and 1,",
    "start": "2175938",
    "end": "2181733"
  },
  {
    "text": "how copying is just the head. And you can just do a histogram. And you can see, oh yeah,\nalmost all of the heads are doing lots of copying.",
    "start": "2181733",
    "end": "2188570"
  },
  {
    "text": "It's nice to be able to go\nand summarize our model in a-- and I think this\nis sort of like, we've gone through a\nvery bottom-up way.",
    "start": "2188570",
    "end": "2194500"
  },
  {
    "text": "And we didn't start\nwith assumptions about what the model is doing. We tried to understand\nits structure. And then we were able to\nsummarize it in useful ways.",
    "start": "2194500",
    "end": "2200368"
  },
  {
    "text": "And now we're able to go\nand say something about it. Now, another thing\nyou might ask is, what do you think the\neigenvalues of the QK circuit",
    "start": "2200368",
    "end": "2207410"
  },
  {
    "text": "mean? And in our examples\nso far, they haven't been that-- they wouldn't\nhave been that interesting. But in a minute, they will be.",
    "start": "2207410",
    "end": "2213039"
  },
  {
    "text": "And so I'll briefly\ndescribe what they mean. A positive eigenvalue\nwould mean you want to attend to the same tokens.",
    "start": "2213040",
    "end": "2218200"
  },
  {
    "text": "An imaginary\neigenvalue-- and this is what you would\nmostly see in the models that we've seen\nso far-- means you want to go in and attend to an\nunrelated or a different token.",
    "start": "2218200",
    "end": "2226540"
  },
  {
    "text": "And a negative\neigenvalue would mean you want to avoid attending\nto the same token.",
    "start": "2226540",
    "end": "2232260"
  },
  {
    "text": "So that will be\nrelevant in a second. Yeah, so those are\ngoing to mostly be useful to think about in\nmultilayer attention-only",
    "start": "2232260",
    "end": "2238528"
  },
  {
    "text": "transformers, when we can have\nchains of attention heads. And so we can ask,\nyou know-- well, I'll get to that in a second.",
    "start": "2238528",
    "end": "2243950"
  },
  {
    "text": "Yeah, so that's a\ntable summarizing that. Unfortunately, this approach\ncompletely breaks down once you have MLP layers.",
    "start": "2243950",
    "end": "2249859"
  },
  {
    "text": "MLP layers, now you have\nthese nonlinearities. And so you don't\nget this property where your model\nis mostly linear,",
    "start": "2249860",
    "end": "2255147"
  },
  {
    "text": "and you can just\nlook at a matrix. But if you're working with only\nattention-only transformers, this is a very nice way\nto think about things.",
    "start": "2255147",
    "end": "2260990"
  },
  {
    "text": "OK. So recall that one-layer\nattention-only transformers don't undergo the space\nchange that we talked about in the beginning.",
    "start": "2260990",
    "end": "2266000"
  },
  {
    "text": "Like, right now,\nwe're on a hunt. We're trying to go and\nanswer this mystery of how the-- what the hell is going\non in that phase change,",
    "start": "2266000",
    "end": "2271625"
  },
  {
    "text": "where models suddenly get\ngood at in-context learning. We want to answer that. And one-layer\nattention-only transformers",
    "start": "2271625",
    "end": "2276817"
  },
  {
    "text": "don't undergo that phase change. But two-layer attention-only\ntransformers do. So we'd like to know what's\ndifferent about two-layer",
    "start": "2276817",
    "end": "2282488"
  },
  {
    "text": "attention-only transformers. ",
    "start": "2282488",
    "end": "2287560"
  },
  {
    "text": "OK, well-- so in\nour previous-- when we were dealing with one-layer\nattention-only transformers, we were able to go and\nrewrite them in this form.",
    "start": "2287560",
    "end": "2294520"
  },
  {
    "text": "And it gave us a lot of\nstructure-- of ability to go and understand the model,\nbecause we could go and say, well, this is bigrams.",
    "start": "2294520",
    "end": "2300099"
  },
  {
    "text": "And then each one of these\nis looking somewhere. And we have this\nmatrix that describes how it affects things.",
    "start": "2300100",
    "end": "2306230"
  },
  {
    "text": "And yeah, so that gave\nus a lot of ability to think about these things. And we can also just write it\nin this factored form, where",
    "start": "2306230",
    "end": "2313270"
  },
  {
    "text": "we have the embedding, and then\nwe have the attention heads, and then we have\nthe unembedding. OK.",
    "start": "2313270",
    "end": "2318600"
  },
  {
    "text": "Well-- oh, and for\nsimplicity, we often go and write WOV for WO\nWV, because they always",
    "start": "2318600",
    "end": "2325559"
  },
  {
    "text": "come together. It's always the case. Like, it's in some\nsense an illusion that WO and WV are\ndifferent matrices. They're just one\nlow-rank matrix.",
    "start": "2325560",
    "end": "2331770"
  },
  {
    "text": "They're never-- they're\nalways used together. And similarly, WQ and WK,\nit's sort of an illusion that they're different matrices.",
    "start": "2331770",
    "end": "2338293"
  },
  {
    "text": "There they're always\njust used together. And keys and queries are\njust sort of-- they're just an artifact of\nthese low-rank matrices.",
    "start": "2338293",
    "end": "2344750"
  },
  {
    "text": "So in any case, it's useful to\ngo and write those together. OK, great. So a two-layer attention-only\ntransformer, what we do",
    "start": "2344750",
    "end": "2351480"
  },
  {
    "text": "is we go through the\nembedding matrix. Then we go through the\nlayer 1 attention heads.",
    "start": "2351480",
    "end": "2357060"
  },
  {
    "text": "Then we go through the\nlayer 2 attention heads. And then we go through\nthe unembedding. And for the attention\nheads, we always",
    "start": "2357060",
    "end": "2362400"
  },
  {
    "text": "have this identity\nas well, which corresponds to just going\ndown the residual stream. So we can go down\nthe residual stream,",
    "start": "2362400",
    "end": "2368430"
  },
  {
    "text": "or we can go through\nan attention head. On the next step, we can also\ngo down the residual stream, or we can go through\nan attention head.",
    "start": "2368430",
    "end": "2374285"
  },
  {
    "text": " And there's this useful\nidentity, the mixed product",
    "start": "2374285",
    "end": "2380910"
  },
  {
    "text": "identity, that any tensor\nproduct or other ways of interpreting\nthis obey, which is",
    "start": "2380910",
    "end": "2386880"
  },
  {
    "text": "that if you have an attention\nhead, and we have, say-- we have the weights, and\nthe attention pattern,",
    "start": "2386880",
    "end": "2391950"
  },
  {
    "text": "and the WOV matrix, and\nthe attention pattern. The attention patterns\nmultiply together. And the OV circuits\nmultiply together.",
    "start": "2391950",
    "end": "2399390"
  },
  {
    "text": "And they behave nicely. OK, great. So we can just expand\nout that equation. We can just take\nthat big product",
    "start": "2399390",
    "end": "2405600"
  },
  {
    "text": "we had at the beginning. And we can just expand it out. And we get three\ndifferent kinds of terms. So one thing we do is\nwe get this path that",
    "start": "2405600",
    "end": "2411162"
  },
  {
    "text": "just goes directly through\nthe residual stream, where we embed and unembed. And that's going to want\nto represent some bigram",
    "start": "2411162",
    "end": "2416590"
  },
  {
    "text": "statistics. Then we get things that look\nlike the attention head terms",
    "start": "2416590",
    "end": "2422620"
  },
  {
    "text": "that we had previously.  And finally, we get\nthese terms that",
    "start": "2422620",
    "end": "2430660"
  },
  {
    "text": "correspond to going through\ntwo attention heads. ",
    "start": "2430660",
    "end": "2437480"
  },
  {
    "text": "Now, it's worth noting that\nthese terms are not actually the same as their-- because\nthe attention head--",
    "start": "2437480",
    "end": "2442580"
  },
  {
    "text": "the attention patterns\nin the second layer can be computed from the\noutputs of the first layer, those are also going\nto be more expressive.",
    "start": "2442580",
    "end": "2448590"
  },
  {
    "text": "But at a high level,\nyou can think of there as being these three\ndifferent kinds of terms. And we sometimes called these\nterms virtual attention heads,",
    "start": "2448590",
    "end": "2454700"
  },
  {
    "text": "because they don't\nexist in the sense-- like, they aren't sort\nof explicitly represented in the model. But they, in fact--",
    "start": "2454700",
    "end": "2460698"
  },
  {
    "text": "they have an attention pattern. They have no read circuit. They're sort of, in almost\nall functional ways, like a tiny little\nattention head.",
    "start": "2460698",
    "end": "2466580"
  },
  {
    "text": "And there's exponentially\nmany of them. It turns out they're\nnot going to be that important in this model. But in other models,\nthey can be important.",
    "start": "2466580",
    "end": "2474270"
  },
  {
    "text": "Right. So one thing that's\nnice about this is it allows us to think about\nattention heads in a really principled way. We don't have to go\nand think about--",
    "start": "2474270",
    "end": "2482090"
  },
  {
    "text": "I think there's-- like, people\nlook at attention patterns all the time. And I think a concern you\nwould have as well-- there's",
    "start": "2482090",
    "end": "2488813"
  },
  {
    "text": "multiple attention patterns. Like, the information that's\nbeing moved by one attention head, it might have been move\nthere by another attention head",
    "start": "2488813",
    "end": "2494210"
  },
  {
    "text": "and not have originated there. And it might still be\nmoved somewhere else. But in fact, this gives us a way\nto avoid all of those concerns",
    "start": "2494210",
    "end": "2500280"
  },
  {
    "text": "and just think about things\nin a single principled way. OK, in any case, an\nimportant question to ask",
    "start": "2500280",
    "end": "2506570"
  },
  {
    "text": "is, how important how are\nthese different terms? Like, we could\nstudy all of them. How important are they?",
    "start": "2506570",
    "end": "2512240"
  },
  {
    "text": "And it turns out, you can just-- there's an algorithm you can use\nwhere you knock out attention--",
    "start": "2512240",
    "end": "2517645"
  },
  {
    "text": "knock out these\nterms, and you go and you ask, how\nimportant are they? And it turns out that by\nfar, the most important thing",
    "start": "2517645",
    "end": "2524540"
  },
  {
    "text": "is these individual\nattention head terms-- in this model, by\nfar the most important thing. The virtual attention\nheads basically",
    "start": "2524540",
    "end": "2531050"
  },
  {
    "text": "don't matter that much. They only have an effect of 0.3\nnats, using to the above ones. And the bigrams are\nstill pretty useful.",
    "start": "2531050",
    "end": "2537359"
  },
  {
    "text": "So if we want to try to\nunderstand this model, we should probably go and\nfocus our attention on-- the virtual attention\nheads are not",
    "start": "2537360",
    "end": "2543080"
  },
  {
    "text": "going to be the best way to\ngo and focus our attention, especially since\nthere's a lot of them.",
    "start": "2543080",
    "end": "2548690"
  },
  {
    "text": "There's 124 of\nthem for 0.3 nats. It's very little that you\nwould understand for studying one of those terms.",
    "start": "2548690",
    "end": "2555030"
  },
  {
    "text": "So the thing that we\nprobably want to do-- we know that these\nare bigram statistics. So what we really\nwant to do is we",
    "start": "2555030",
    "end": "2560107"
  },
  {
    "text": "want to understand the\nindividual attention head terms. ",
    "start": "2560107",
    "end": "2567123"
  },
  {
    "text": "This is the algorithm. I'm going to skip\nover it for time. We can ignore that term\nbecause it's small.",
    "start": "2567123",
    "end": "2573000"
  },
  {
    "text": "And it turns out also\nthat the layer 2 attention heads are doing way more\nthan layer 1 attention heads. And that's not that surprising.",
    "start": "2573000",
    "end": "2579200"
  },
  {
    "text": "Like, the layer\n2 attention heads are more expressive\nbecause they can use the layer 1 attention heads\nto construct their attention patterns.",
    "start": "2579200",
    "end": "2585680"
  },
  {
    "text": "OK, so if we could\njust go and understand the layer 2\nattention heads, we'd probably understand a lot of\nwhat's going on in this model.",
    "start": "2585680",
    "end": "2594200"
  },
  {
    "text": "And the trick is that the\nattention heads are now constructed from\nthe previous layer, rather than just\nfrom the tokens.",
    "start": "2594200",
    "end": "2599730"
  },
  {
    "text": "So this is still the\nsame, the attention head. The attention pattern\nis more complex. And if you write it out, you\nget this complex equation that",
    "start": "2599730",
    "end": "2606890"
  },
  {
    "text": "says, you embed the tokens. Then you go and\nyou shuffle things around using the attention\nheads for the keys. Then you multiply it by WQK.",
    "start": "2606890",
    "end": "2613130"
  },
  {
    "text": "Then you multiply\nit, shuffle things around again for the queries. And then you go and\nmultiply by the embedding again, because\nthey were embedded.",
    "start": "2613130",
    "end": "2618590"
  },
  {
    "text": "And then you get\nback to the tokens. ",
    "start": "2618590",
    "end": "2624030"
  },
  {
    "text": "But let's actually look at them. So one thing that's-- remember\nthat when we see positive eigenvalues in the OV\ncircuit, we're doing copying.",
    "start": "2624030",
    "end": "2630730"
  },
  {
    "text": "So one thing we can say\nis, well, 7 out of 12, and in fact, the ones with\nthe largest eigenvalues,",
    "start": "2630730",
    "end": "2636120"
  },
  {
    "text": "are doing copying. So we still have a\nlot of attention heads that are doing copying. ",
    "start": "2636120",
    "end": "2644150"
  },
  {
    "text": "And yeah, the QK circuit-- so one thing you\ncould do is you could try to understand things in\nterms of this more complex QK",
    "start": "2644150",
    "end": "2649520"
  },
  {
    "text": "equation. You could also just try to\nunderstand what the attention patterns are doing empirically. So let's look at one\nof these copying ones.",
    "start": "2649520",
    "end": "2656240"
  },
  {
    "text": "I've given it the first\nparagraph of Harry Potter. And we can just look\nat where it attends. ",
    "start": "2656240",
    "end": "2663300"
  },
  {
    "text": "And something really\ninteresting happened. So almost all the time,\nwe just attend back to the first token we have, the\nspecial token at the beginning",
    "start": "2663300",
    "end": "2671100"
  },
  {
    "text": "of the sequence. And we usually think of that\nas just being a null attention operation. It's a way for it\nto not do anything.",
    "start": "2671100",
    "end": "2677049"
  },
  {
    "text": "In fact, if you look the value\nvector, it's basically zero. It's just not copying any\ninformation from there. ",
    "start": "2677050",
    "end": "2684190"
  },
  {
    "text": "But whenever we\nsee repeated text, something interesting happens. So when we get to\n\"Mr,\" it tries to look",
    "start": "2684190",
    "end": "2690100"
  },
  {
    "text": "at \"and.\" it's a\nlittle bit weak. Then we get to \"D,\" and\nit attends to \"urs.\"",
    "start": "2690100",
    "end": "2696510"
  },
  {
    "text": "That's interesting. And then we get to \"urs,\"\nand it tends to \"ley.\"",
    "start": "2696510",
    "end": "2704150"
  },
  {
    "text": "And so it's not attending\nto the same token. It's attending to the same\ntoken, shifted one forward.",
    "start": "2704150",
    "end": "2712640"
  },
  {
    "text": "Well, that's really interesting. And there's actually a\nlot of attention heads that are doing this. So here, we have one where\nnow, we hit the Potters,",
    "start": "2712640",
    "end": "2718730"
  },
  {
    "text": "\"Pot,\" then we attended \"ters.\" Maybe that's the\nsame attention head. I don't remember when I was\nconstructing this example.",
    "start": "2718730",
    "end": "2724670"
  },
  {
    "text": "It turns out this is\na super common thing. So you go and you look\nfor the previous example. You shift one forward.",
    "start": "2724670",
    "end": "2729920"
  },
  {
    "text": "And you're like, OK, well,\nlast time I saw this, this is what happened. Probably, the same thing\nis going to happen. ",
    "start": "2729920",
    "end": "2737290"
  },
  {
    "text": "And we can go and\nlook at the effect that the attention\nhead has on the logits. Most of the time, it's\nnot affecting things.",
    "start": "2737290",
    "end": "2743860"
  },
  {
    "text": "But in these cases, it's\nable to go and predict-- when it's doing this, this\nthing of going and looking one forward, it's able to go\nand predict the next token.",
    "start": "2743860",
    "end": "2751353"
  },
  {
    "text": "So we call this\nan induction head. An induction head looks\nfor the previous copy, looks forward, and\nsays, ah, probably",
    "start": "2751353",
    "end": "2756700"
  },
  {
    "text": "the same thing that happened\nlast time is going to happen. You can think of this as\nbeing a nearest neighbors. It's like an in-context\nnearest neighbors algorithm.",
    "start": "2756700",
    "end": "2763183"
  },
  {
    "text": "It's going and searching\nthrough your context, finding similar things,\nand then predicting that's what's going\nto happen next.",
    "start": "2763183",
    "end": "2768448"
  },
  {
    "text": " The way that these\nactually work is--",
    "start": "2768448",
    "end": "2774850"
  },
  {
    "text": "I mean, there's\nactually two ways. But in a model that uses\nrotary attention or something like this, you only have one.",
    "start": "2774850",
    "end": "2782500"
  },
  {
    "text": "You shift your key-- first, you-- an\nearlier attention head shifts your key forward one. So you would, like, take the\nvalue of the previous token,",
    "start": "2782500",
    "end": "2790059"
  },
  {
    "text": "and you embed it in\nyour present token. And then you have your query\nand your key go and look at--",
    "start": "2790060",
    "end": "2797110"
  },
  {
    "text": "yeah, try to go and match. So you look for the same thing. And then you go and you\npredict that whatever you saw",
    "start": "2797110",
    "end": "2803390"
  },
  {
    "text": "is going to be the next token. So that's the\nhigh-level algorithm. Sometimes you can\ndo clever things",
    "start": "2803390",
    "end": "2808580"
  },
  {
    "text": "where, actually, it'll care\nabout multiple earlier tokens. And it'll look for, like,\nshort phrases and so on. So induction heads\ncan really vary",
    "start": "2808580",
    "end": "2814440"
  },
  {
    "text": "in how much of the previous\ncontext they care about or what aspects of the previous\ncontext they care about. But this general trick of\nlooking for the same thing,",
    "start": "2814440",
    "end": "2821690"
  },
  {
    "text": "shift forward, predict that,\nis what induction heads do.",
    "start": "2821690",
    "end": "2826910"
  },
  {
    "text": "Lots of examples of this. And the cool thing\nis you can now-- you can use the QK eigenvalues\nto characterize this.",
    "start": "2826910",
    "end": "2833510"
  },
  {
    "text": "You can say, well, we're looking\nfor the same thing, shifted by one, but looking\nfor the same thing. If you expand\nthrough the attention",
    "start": "2833510",
    "end": "2839300"
  },
  {
    "text": "heads in the right\nway, that'll work out. And we're copying. And so an induction\nhead is one which has both positive OV\neigenvalues, and also",
    "start": "2839300",
    "end": "2847160"
  },
  {
    "text": "positive QK eigenvalues. ",
    "start": "2847160",
    "end": "2852405"
  },
  {
    "text": "And so you can just\nput that on a plot. And you have your induction\nheads in the corner. So you have your OV eigenvalues,\nyour QK eigenvalues.",
    "start": "2852405",
    "end": "2860480"
  },
  {
    "text": "And I think, actually,\nOV is this axis. QK is this one. Axis doesn't matter. And in the corner, you\nhave your eigenvalues,",
    "start": "2860480",
    "end": "2867550"
  },
  {
    "text": "or your induction heads.  Yeah, and so this seems to be--",
    "start": "2867550",
    "end": "2874040"
  },
  {
    "text": "well, OK. We now have an\nactual hypothesis. The hypothesis is, the way\nthat-- that phase change we're seeing, the phase\nchange is the discovery",
    "start": "2874040",
    "end": "2880640"
  },
  {
    "text": "of these induction heads. That would be the hypothesis. And these are way more\neffective than regular--",
    "start": "2880640",
    "end": "2886850"
  },
  {
    "text": "than this first\nalgorithm we had, which was just sort\nof blindly copy things wherever it could be plausible. Now we can go and actually\nrecognize patterns,",
    "start": "2886850",
    "end": "2892967"
  },
  {
    "text": "and look at what happened, and\npredict that similar things are going to happen again. That's a way better algorithm. ",
    "start": "2892967",
    "end": "2900608"
  },
  {
    "text": "Yeah, so there's other\nattention heads that are doing more local things. I'm going to go and skip over\nthat and return to our mystery,",
    "start": "2900608",
    "end": "2905940"
  },
  {
    "text": "because I am\nrunning out of time. I have five more minutes. OK, so what is going on with\nthis in-context learning?",
    "start": "2905940",
    "end": "2911070"
  },
  {
    "text": "Well, now we have a hypothesis. Let's check it. So we think it might\nbe induction heads. ",
    "start": "2911070",
    "end": "2917720"
  },
  {
    "text": "And there's a few\nreasons we believe this. So one thing is going to\nbe that induction heads--",
    "start": "2917720",
    "end": "2923340"
  },
  {
    "text": "well, OK. I'll just go over\nto the evidence. So one thing you can\ndo is you can just oblate the attention heads.",
    "start": "2923340",
    "end": "2929309"
  },
  {
    "text": "And it turns out you\ncan color-- here, we have attention heads\ncolored by how much they are an induction head.",
    "start": "2929310",
    "end": "2935619"
  },
  {
    "text": "And this is the\nstart of the bump. This is the end\nof the bump here. And we can see that they--\nfirst of all, induction heads",
    "start": "2935620",
    "end": "2942130"
  },
  {
    "text": "are forming. Like, previously, we didn't\nhave induction heads here. Now, they're just\nstarting to form here. And then we have really intense\ninduction heads here and here.",
    "start": "2942130",
    "end": "2951099"
  },
  {
    "text": "And the attention heads where\nyou oblate them, you get a--",
    "start": "2951100",
    "end": "2956425"
  },
  {
    "text": "you got a loss in-- so we're\nlooking not at the loss, but at this meta-learning\nscore, the difference between--",
    "start": "2956425",
    "end": "2961600"
  },
  {
    "text": "or an in-context learning\nscore, the difference between the 500th token\nand the 50th token.",
    "start": "2961600",
    "end": "2967240"
  },
  {
    "text": "That's all explained\nby induction heads. Now, we actually have\none induction head that doesn't contribute to it.",
    "start": "2967240",
    "end": "2973030"
  },
  {
    "text": "Actually, it does the opposite. So that's kind of interesting. Maybe it's doing something\nshorter distance.",
    "start": "2973030",
    "end": "2978237"
  },
  {
    "text": "And there's also this\ninteresting thing where they all rush to be induction heads. And then they discover only\na few win out in the end.",
    "start": "2978237",
    "end": "2984363"
  },
  {
    "text": "So there's some interesting\ndynamics going on there. But it really seems like\nin these small models,",
    "start": "2984363",
    "end": "2989740"
  },
  {
    "text": "all of in-context learning is\nexplained by these induction heads. OK, but what about large models?",
    "start": "2989740",
    "end": "2996203"
  },
  {
    "text": "Well, in large models,\nit's going to be harder to go and ask this. But one thing you can\ndo is you can ask, OK,",
    "start": "2996203",
    "end": "3001580"
  },
  {
    "text": "we can look at our induction--\nor our in-context learning score over time. We get this sharp phase change.",
    "start": "3001580",
    "end": "3007660"
  },
  {
    "text": "Oh, look. Induction heads form at\nexactly the same point in time.",
    "start": "3007660",
    "end": "3012947"
  },
  {
    "text": "So that's only\ncorrelational evidence. But it's pretty suggestive\ncorrelational evidence, especially given that we\nhave an obvious-- like,",
    "start": "3012947",
    "end": "3019270"
  },
  {
    "text": "the obvious the fact that\ninduction heads should have is this. I guess it could be that\nthere's other mechanisms being discovered at the same\ntime in large models.",
    "start": "3019270",
    "end": "3026083"
  },
  {
    "text": "But it has to be in\na very small window. So really suggests\nthat the thing that's driving that change\nis in-context learning.",
    "start": "3026083",
    "end": "3034609"
  },
  {
    "text": "OK, so obviously, induction\nheads can go and copy text.",
    "start": "3034610",
    "end": "3040240"
  },
  {
    "text": "But a question you might ask\nis, can they do translation? Like, there's all\nthese amazing things that models can do that it's\nnot obvious in-context learning,",
    "start": "3040240",
    "end": "3048877"
  },
  {
    "text": "or this sort of copying\nmechanism, could do. So I just want to very quickly\nlook at a few fun examples.",
    "start": "3048877",
    "end": "3056540"
  },
  {
    "text": "So here, we have an\nattention pattern. Oh, I guess I need\nto open Lexoscope.",
    "start": "3056540",
    "end": "3063943"
  },
  {
    "text": " Hmm.",
    "start": "3063944",
    "end": "3069140"
  },
  {
    "text": "Let me try doing that again. Sorry, I should have thought\nthis through a bit more before this talk.",
    "start": "3069140",
    "end": "3075490"
  },
  {
    "text": "Chris, could you zoom\nin a little, please? Yeah, yeah. Thank you. ",
    "start": "3075490",
    "end": "3099060"
  },
  {
    "text": "OK, I'm not-- my French\nisn't that great. But my name is Christopher. I'm from Canada.",
    "start": "3099060",
    "end": "3105653"
  },
  {
    "text": "What we can do\nhere is we can look at where this\nattention head attends as we go and we do this.",
    "start": "3105653",
    "end": "3110819"
  },
  {
    "text": "And it'll become especially\nclear on the second sentence. So here, we're on the period.",
    "start": "3110820",
    "end": "3116610"
  },
  {
    "text": "And we tend to \"je.\" Now we're on-- and\n\"je\" is I in French.",
    "start": "3116610",
    "end": "3121680"
  },
  {
    "text": "OK. Now we're on the \"I,\"\nand we attend to \"suis.\" And now we're on\nthe \"am,\" and we",
    "start": "3121680",
    "end": "3127800"
  },
  {
    "text": "attend to \"du,\" which is from,\nand then \"from\" to \"Canada.\" And so we're doing a\ncross-lingual induction",
    "start": "3127800",
    "end": "3135119"
  },
  {
    "text": "head, which we can\nuse for translation. And indeed, if you\nlook at examples,",
    "start": "3135120",
    "end": "3140380"
  },
  {
    "text": "this is where it seems to--\nthis to be a major driving force in the model's\nability to go",
    "start": "3140380",
    "end": "3145570"
  },
  {
    "text": "and correctly do translation. Another fun example is--",
    "start": "3145570",
    "end": "3151990"
  },
  {
    "text": "I think maybe the\nmost impressive thing about in-context learning to\nme has been the model's ability to go and learn\narbitrary functions.",
    "start": "3151990",
    "end": "3158075"
  },
  {
    "text": "Like, you just show\nthe model function. It can start mimicking\nthat function. Well, OK. Chris, I have a question.",
    "start": "3158075",
    "end": "3164120"
  },
  {
    "text": "Yes? So do these induction heads only\ndo kind of a look-ahead copy, or can they also do some sort of\ncomplex structure recognition?",
    "start": "3164120",
    "end": "3174370"
  },
  {
    "text": "Yeah, yeah. So they can both use a larger\ncontext, previous context, and they can copy\nmore abstract things.",
    "start": "3174370",
    "end": "3181030"
  },
  {
    "text": "So like, the translation one is\nshowing you that they can copy, rather than the literal\ntoken, a translated version. So it's what I call\nsoft induction head.",
    "start": "3181030",
    "end": "3188620"
  },
  {
    "text": "And yeah, you can have\nthem copy similar words. You can have them look\nat longer contexts. It can look for more\nstructural things.",
    "start": "3188620",
    "end": "3195850"
  },
  {
    "text": "The way that we usually\ncharacterize them is whether-- in large models-- is\nwhether they empirically behave like an induction head.",
    "start": "3195850",
    "end": "3201540"
  },
  {
    "text": "So the definition gets\na little bit blurry when you try to encompass\nthese more-- there's sort of a blurry boundary.",
    "start": "3201540",
    "end": "3207405"
  },
  {
    "text": "But yeah, there seem to be\na lot of attention heads that are doing sort of more\nand more abstract versions. OK.",
    "start": "3207405",
    "end": "3212820"
  },
  {
    "text": "Thank you. Yeah. My favorite version is this\none that I'm about to show you, which is used--",
    "start": "3212820",
    "end": "3218619"
  },
  {
    "text": "let's isolate a single\none of these-- which can do pattern recognition. So it can learn functions in the\ncontext and learn how to do it.",
    "start": "3218620",
    "end": "3225680"
  },
  {
    "text": "So I've just made up a\nnonsense function here. We're going to encode\none binary variable",
    "start": "3225680",
    "end": "3231430"
  },
  {
    "text": "with a choice of whether\nto do a color or a month as the first word. Then we're going to-- so\nwe have green or June here.",
    "start": "3231430",
    "end": "3240370"
  },
  {
    "text": "Let's zoom in more. So we have color or month\nand animal or fruit.",
    "start": "3240370",
    "end": "3247010"
  },
  {
    "text": "And then we have to map it\nto either true or false. So that's our goal. And it's going to be an XOR. So we have a binary variable\nrepresented in this way.",
    "start": "3247010",
    "end": "3253488"
  },
  {
    "text": "We do an XOR. I'm pretty confident this was\nnever in the training set, because I just made it up.",
    "start": "3253488",
    "end": "3258560"
  },
  {
    "text": "And it seems like\na nonsense problem. OK, so then we can go\nand ask, can the model",
    "start": "3258560",
    "end": "3263900"
  },
  {
    "text": "go and parse that? Well, it can. And it uses induction\nheads to do it. And what we can do is\nwe can look at the--",
    "start": "3263900",
    "end": "3269035"
  },
  {
    "text": "so we look at a\ncolon, where it's going to go and try and\npredict the next word. And for instance, here,\nwe have April dog.",
    "start": "3269035",
    "end": "3275960"
  },
  {
    "text": "So it's a month\nand then an animal. And it should be true. And what it does is it\nlooks for a previous--",
    "start": "3275960",
    "end": "3281850"
  },
  {
    "text": "previous cases where\nthere was an animal-- a month and then an\nanimal, especially one where the month was the\nsame, and goes, and looks,",
    "start": "3281850",
    "end": "3287930"
  },
  {
    "text": "and says that it's true. And so the model can go and\nlearn a function, a completely arbitrary function,\nby going and doing",
    "start": "3287930",
    "end": "3295130"
  },
  {
    "text": "this kind of pattern\nrecognition induction head. And so this, to me, made\nit a lot more plausible",
    "start": "3295130",
    "end": "3301290"
  },
  {
    "text": "that these models actually\ncan do in-context learning.",
    "start": "3301290",
    "end": "3307260"
  },
  {
    "text": "Like, the generality of\nall these amazing things we see these large\nlanguage models do",
    "start": "3307260",
    "end": "3312540"
  },
  {
    "text": "can be explained\nby induction heads. We don't know that. It could be that there's\nother things going on. It's very possible that there's\nlots of other things going on.",
    "start": "3312540",
    "end": "3319559"
  },
  {
    "text": "But it seems a lot\nmore plausible to me than it did when we started. I'm conscious that I\nam actually over time.",
    "start": "3319560",
    "end": "3326502"
  },
  {
    "text": "Let me just quickly go\nthrough these last few slides. Yeah, so I think,\nthinking about this as like an in-context\nnearest neighbors, I think,",
    "start": "3326502",
    "end": "3331872"
  },
  {
    "text": "is a really useful way\nto think about this. Other things could\nabsolutely be contributing.",
    "start": "3331872",
    "end": "3336880"
  },
  {
    "text": "This might explain\nwhy transformers do in-context learning over a\nlong context better than LSTMs.",
    "start": "3336880",
    "end": "3344160"
  },
  {
    "text": "An LSTM can't do\nthis, because it's not linear in the amount\nof compute it needs. It's like quadratic or n log\nn if it was really clever.",
    "start": "3344160",
    "end": "3351220"
  },
  {
    "text": "So transformers-- or LSTMs,\nimpossible to do this. Transformers do do this. And actually, they\ndiverge at the same point,",
    "start": "3351220",
    "end": "3357340"
  },
  {
    "text": "that if you looked-- well, I can go into this in\nmore detail after if you want.",
    "start": "3357340",
    "end": "3363425"
  },
  {
    "text": "There's a really nice paper\nby Marcus Hutter explaining-- trying to predict\nand explain why we observe scaling laws in models. It's worth noting that the\narguments in this paper",
    "start": "3363425",
    "end": "3370503"
  },
  {
    "text": "go exactly through\nto this example-- this theory. In fact, they sort\nof work better",
    "start": "3370503",
    "end": "3375820"
  },
  {
    "text": "for the case of thinking\nabout this in-context learning with, essentially, a nearest\nneighbors algorithm, than they",
    "start": "3375820",
    "end": "3382059"
  },
  {
    "text": "do in the regular case. So yeah. I'm happy to answer questions.",
    "start": "3382060",
    "end": "3387080"
  },
  {
    "text": "I can go into as much detail as\npeople want about any of this. And I can also, if\nyou send me an email, send you more information\nabout all of this.",
    "start": "3387080",
    "end": "3394000"
  },
  {
    "text": "And yeah. You know, again, this\nwork is not yet published. And you don't have\nto keep it secret.",
    "start": "3394000",
    "end": "3399690"
  },
  {
    "text": "But just, if you could be\nthoughtful about the fact that it's unpublished work\nand probably is a month or two away from coming out,\nI'd be really grateful for that.",
    "start": "3399690",
    "end": "3407420"
  },
  {
    "text": "Thank you so much for your time. Yeah. Thanks a lot, Chris. This was a great talk. Thank you, Chris.",
    "start": "3407420",
    "end": "3413413"
  },
  {
    "text": "So I'll just open with\nsome general questions. And then we can do a round of\nquestions from the students.",
    "start": "3413413",
    "end": "3418680"
  },
  {
    "text": "Sure. So I was very excited to know,\nso what is the line of work that you're\ncurrently working on? Is it like extending this?",
    "start": "3418680",
    "end": "3425290"
  },
  {
    "text": "So what do you think is\nthe next things you'll try to do to make it\nmore interpretable over the next year?",
    "start": "3425290",
    "end": "3430630"
  },
  {
    "text": "I mean, I want to just reverse\nengineer language models. I want to figure out the\nentirety of what's going on in these language models.",
    "start": "3430630",
    "end": "3437172"
  },
  {
    "text": "And one thing that we\ntotally don't understand",
    "start": "3437172",
    "end": "3443260"
  },
  {
    "text": "is NLP layers, more-- we understand some\nthings about them.",
    "start": "3443260",
    "end": "3448690"
  },
  {
    "text": "But we don't really understand\nNLP layers very well. There's a lot of stuff\ngoing on in large models",
    "start": "3448690",
    "end": "3453760"
  },
  {
    "text": "that we don't understand. I want to know how\nmodels do arithmetic. I want to know-- another thing that I'm\nvery interested in is,",
    "start": "3453760",
    "end": "3459400"
  },
  {
    "text": "what's going on when you\nhave multiple speakers? The model can\nclearly represent-- like, it has like a basic theory\nof mind, multiple speakers",
    "start": "3459400",
    "end": "3465367"
  },
  {
    "text": "in a dialogue. I want to understand\nwhat's going on with that. But honestly, there's just\nso much we don't understand.",
    "start": "3465367",
    "end": "3470368"
  },
  {
    "text": "It's really-- it's sort of\nhard to answer the question, because there's just\nso much to figure out.",
    "start": "3470368",
    "end": "3475839"
  },
  {
    "text": "And we have a lot of\ndifferent threads of research in doing this. But yeah, the interpretability\nteam at Anthropic",
    "start": "3475840",
    "end": "3482770"
  },
  {
    "text": "is just sort of-- has a bunch of threads\ntrying to go and figure out what's going inside\nthese models, and sort of a similar flavor\nto this of just trying",
    "start": "3482770",
    "end": "3489835"
  },
  {
    "text": "to figure out, how do\nthe parameters actually encode algorithms? And can we reverse\nengineer those into meaningful\ncomputer programs",
    "start": "3489835",
    "end": "3496570"
  },
  {
    "text": "that we can understand? Got it. Another question I\nhad is like-- so you were talking about how the\ntransformers are trying",
    "start": "3496570",
    "end": "3503570"
  },
  {
    "text": "to do meta-learning inherently. So it's like-- and you\nspent a lot of time talking about the induction heads.",
    "start": "3503570",
    "end": "3509020"
  },
  {
    "text": "And that was like\nvery interesting. But can you formalize the sort\nof meta-learning algorithm they might be learning?",
    "start": "3509020",
    "end": "3514123"
  },
  {
    "text": "Is it possible to say,\nlike, oh, maybe this is a sort of like internal\nalgorithm that's going, that's making them\ngood meta-learners,",
    "start": "3514123",
    "end": "3520390"
  },
  {
    "text": "or something like that? Like, I don't know if-- Yeah. I mean, I think-- so I think that there's\nroughly two algorithms. One is this algorithm we\nsaw in the one-layer model.",
    "start": "3520390",
    "end": "3527438"
  },
  {
    "text": "And we see it in other models\ntoo, especially early on, which is just, try to copy-- you saw a word.",
    "start": "3527438",
    "end": "3532720"
  },
  {
    "text": "Probably, a similar word\nis going to happen later. Look for places\nthat it might fit in and increase the probability.",
    "start": "3532720",
    "end": "3538359"
  },
  {
    "text": "So that's one thing that we see. And the other thing we\nsee is induction heads, which you can just summarize\nas in-context nearest",
    "start": "3538360",
    "end": "3545410"
  },
  {
    "text": "neighbors, basically. And it seems-- possibly,\nthere's other things. But it seems like\nthose two algorithms,",
    "start": "3545410",
    "end": "3551710"
  },
  {
    "text": "in the specific instantiations\nthat we are looking at, seem to be what's driving\nin-context learning. That would be my present theory.",
    "start": "3551710",
    "end": "3558470"
  },
  {
    "text": "Yeah. Sounds very interesting. Yeah. OK, so let's open a round\nof student questions.",
    "start": "3558470",
    "end": "3564869"
  },
  {
    "text": "So yeah, feel free to go\nahead and ask questions. ",
    "start": "3564870",
    "end": "3574000"
  }
]