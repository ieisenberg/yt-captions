[
  {
    "start": "0",
    "end": "5022"
  },
  {
    "text": "Welcome back, everyone.",
    "start": "5022",
    "end": "6089"
  },
  {
    "text": "This is part three in our\nseries on methods and metrics.",
    "start": "6090",
    "end": "8720"
  },
  {
    "text": "We're going to talk\nabout generation metrics.",
    "start": "8720",
    "end": "11240"
  },
  {
    "text": "In the previous\nscreencast, we talked",
    "start": "11240",
    "end": "13280"
  },
  {
    "text": "about classifier metrics.",
    "start": "13280",
    "end": "14630"
  },
  {
    "text": "Those seem conceptually\nstraightforward at first",
    "start": "14630",
    "end": "17760"
  },
  {
    "text": "but turn out to harbor\nlots of intricacies.",
    "start": "17760",
    "end": "21050"
  },
  {
    "text": "That goes double, at\nleast for generation.",
    "start": "21050",
    "end": "23779"
  },
  {
    "text": "Generation is incredibly\nconceptually challenging.",
    "start": "23780",
    "end": "27020"
  },
  {
    "text": "I would say the\nfundamental issue here",
    "start": "27020",
    "end": "29180"
  },
  {
    "text": "is that there is more\nthan one effective way",
    "start": "29180",
    "end": "31910"
  },
  {
    "text": "to say most things.",
    "start": "31910",
    "end": "33630"
  },
  {
    "text": "And so that immediately\nraises the question",
    "start": "33630",
    "end": "35960"
  },
  {
    "text": "of what we are even\ntrying to measure.",
    "start": "35960",
    "end": "38210"
  },
  {
    "text": "Is it fluency?",
    "start": "38210",
    "end": "39800"
  },
  {
    "text": "Is it truthfulness?",
    "start": "39800",
    "end": "41360"
  },
  {
    "text": "Communicative effectiveness?",
    "start": "41360",
    "end": "43100"
  },
  {
    "text": "Maybe something else.",
    "start": "43100",
    "end": "44149"
  },
  {
    "text": "These are all interestingly\ndifferent questions.",
    "start": "44150",
    "end": "47000"
  },
  {
    "text": "After all, you could have a\nsystem that was highly fluent",
    "start": "47000",
    "end": "50030"
  },
  {
    "text": "but spewing falsehoods\nor even a system that",
    "start": "50030",
    "end": "52940"
  },
  {
    "text": "was highly disfluent\nbut achieving its goals",
    "start": "52940",
    "end": "55309"
  },
  {
    "text": "in communication.",
    "start": "55310",
    "end": "56562"
  },
  {
    "text": "And those kind of examples\nshow that you really",
    "start": "56562",
    "end": "58520"
  },
  {
    "text": "need to have clarity\non the high-level goals",
    "start": "58520",
    "end": "61100"
  },
  {
    "text": "before you can even\nthink about which metrics",
    "start": "61100",
    "end": "63829"
  },
  {
    "text": "to choose for generation.",
    "start": "63830",
    "end": "67380"
  },
  {
    "text": "Let's begin the discussion\nwith perplexity.",
    "start": "67380",
    "end": "69869"
  },
  {
    "text": "That's a natural starting point.",
    "start": "69870",
    "end": "71590"
  },
  {
    "text": "It's a kind of\nanalog of accuracy",
    "start": "71590",
    "end": "74070"
  },
  {
    "text": "but in the generation space.",
    "start": "74070",
    "end": "76180"
  },
  {
    "text": "So for some sequence x and\nsome probability distribution",
    "start": "76180",
    "end": "79830"
  },
  {
    "text": "or model that can assign\nprobability distributions,",
    "start": "79830",
    "end": "82860"
  },
  {
    "text": "the perplexity for that\nsequence according to that model",
    "start": "82860",
    "end": "86040"
  },
  {
    "text": "is really just\nthe geometric mean",
    "start": "86040",
    "end": "88380"
  },
  {
    "text": "of the probabilities assigned\nto the individual time steps.",
    "start": "88380",
    "end": "92259"
  },
  {
    "text": "And then, when we average\nover an entire corpus",
    "start": "92260",
    "end": "95010"
  },
  {
    "text": "for the mean perplexity, we\njust do the geometric mean",
    "start": "95010",
    "end": "98550"
  },
  {
    "text": "of the individual perplexity\nscores per sequence.",
    "start": "98550",
    "end": "102940"
  },
  {
    "text": "Properties of perplexity.",
    "start": "102940",
    "end": "104620"
  },
  {
    "text": "Well, its bounds are 1 and\ninfinity, with 1 the best.",
    "start": "104620",
    "end": "107500"
  },
  {
    "text": "So we are seeking to\nminimize this quantity.",
    "start": "107500",
    "end": "110620"
  },
  {
    "text": "It is equivalent to\nthe exponentiation",
    "start": "110620",
    "end": "113170"
  },
  {
    "text": "of the cross-entropy loss.",
    "start": "113170",
    "end": "114700"
  },
  {
    "text": "This is really important.",
    "start": "114700",
    "end": "116450"
  },
  {
    "text": "Most modern-day language models\nuse a cross-entropy loss.",
    "start": "116450",
    "end": "120969"
  },
  {
    "text": "And what that means is that\nwhether you want it to or not,",
    "start": "120970",
    "end": "123760"
  },
  {
    "text": "you are effectively optimizing\nthat model for perplexity.",
    "start": "123760",
    "end": "129160"
  },
  {
    "text": "What's the value encoded?",
    "start": "129160",
    "end": "130750"
  },
  {
    "text": "It's something like, does the\nmodel assign high probability",
    "start": "130750",
    "end": "133840"
  },
  {
    "text": "to the input sequence?",
    "start": "133840",
    "end": "135739"
  },
  {
    "text": "So when we think about\nassessment, what that means",
    "start": "135740",
    "end": "138070"
  },
  {
    "text": "is that we have some\nassessment set of sequences.",
    "start": "138070",
    "end": "140860"
  },
  {
    "text": "We run our model\non those examples.",
    "start": "140860",
    "end": "143200"
  },
  {
    "text": "We get the average perplexity\nacross the examples,",
    "start": "143200",
    "end": "146110"
  },
  {
    "text": "and we report that number as\nan estimate of system quality.",
    "start": "146110",
    "end": "152130"
  },
  {
    "text": "Relatedly, there are a number\nof weaknesses that we really",
    "start": "152130",
    "end": "154830"
  },
  {
    "text": "need to think about.",
    "start": "154830",
    "end": "155790"
  },
  {
    "text": "First, perplexity is heavily\ndependent on the underlying",
    "start": "155790",
    "end": "159450"
  },
  {
    "text": "vocabulary.",
    "start": "159450",
    "end": "160200"
  },
  {
    "text": "One easy way to see\nthis is just to imagine",
    "start": "160200",
    "end": "162420"
  },
  {
    "text": "that we take every\ntoken in these sequences",
    "start": "162420",
    "end": "164640"
  },
  {
    "text": "and map them to a\nsingle UNK character.",
    "start": "164640",
    "end": "167490"
  },
  {
    "text": "In that case, we will\nhave perfect perplexity,",
    "start": "167490",
    "end": "170370"
  },
  {
    "text": "but we will have a\nterrible generation system.",
    "start": "170370",
    "end": "174900"
  },
  {
    "text": "Perplexity also\nreally does not allow",
    "start": "174900",
    "end": "177180"
  },
  {
    "text": "comparisons between datasets.",
    "start": "177180",
    "end": "179730"
  },
  {
    "text": "The issue here is that\nwe don't have any ground",
    "start": "179730",
    "end": "182370"
  },
  {
    "text": "truth on what's a\ngood or bad perplexity",
    "start": "182370",
    "end": "184620"
  },
  {
    "text": "separate from the data.",
    "start": "184620",
    "end": "186159"
  },
  {
    "text": "And what that means\nis that comparing",
    "start": "186160",
    "end": "187860"
  },
  {
    "text": "across two datasets\nwith perplexity numbers",
    "start": "187860",
    "end": "190320"
  },
  {
    "text": "is just comparing incomparables.",
    "start": "190320",
    "end": "193080"
  },
  {
    "text": "Relatedly, even comparisons\nbetween models is tricky.",
    "start": "193080",
    "end": "197010"
  },
  {
    "text": "You can see this emerging\nhere because we really need",
    "start": "197010",
    "end": "200069"
  },
  {
    "text": "a lot of things to be held\nconstant to do a model",
    "start": "200070",
    "end": "202650"
  },
  {
    "text": "comparison.",
    "start": "202650",
    "end": "203370"
  },
  {
    "text": "Certainly, the tokenization\nneeds to be the same.",
    "start": "203370",
    "end": "205920"
  },
  {
    "text": "Certainly, the datasets\nneed to be the same.",
    "start": "205920",
    "end": "208349"
  },
  {
    "text": "Ideally, many other\naspects of the systems",
    "start": "208350",
    "end": "211140"
  },
  {
    "text": "are held constant when we\ndo a perplexity comparison.",
    "start": "211140",
    "end": "214800"
  },
  {
    "text": "Otherwise, again\nwe are in danger",
    "start": "214800",
    "end": "216630"
  },
  {
    "text": "of comparing incomparables.",
    "start": "216630",
    "end": "218540"
  },
  {
    "start": "218540",
    "end": "221110"
  },
  {
    "text": "Word error rate might be\nbetter, and this is more tightly",
    "start": "221110",
    "end": "224950"
  },
  {
    "text": "aligned with actually\nhuman-created reference texts,",
    "start": "224950",
    "end": "227739"
  },
  {
    "text": "which could be a step up from\nperplexity in terms of how",
    "start": "227740",
    "end": "231430"
  },
  {
    "text": "we think about assessment.",
    "start": "231430",
    "end": "233930"
  },
  {
    "text": "This is really a\nfamily of measures.",
    "start": "233930",
    "end": "235790"
  },
  {
    "text": "You need to pick a distance\nmeasure between strings,",
    "start": "235790",
    "end": "238430"
  },
  {
    "text": "and then your word error rate is\nparameterized by that distance",
    "start": "238430",
    "end": "241980"
  },
  {
    "text": "metric.",
    "start": "241980",
    "end": "242480"
  },
  {
    "text": "Here's the full calculation.",
    "start": "242480",
    "end": "244700"
  },
  {
    "text": "For a goal text x\nand a predicted text",
    "start": "244700",
    "end": "248720"
  },
  {
    "text": "pred, we do the distance\nbetween x and pred according",
    "start": "248720",
    "end": "252560"
  },
  {
    "text": "to whatever distance\nmeasure we chose,",
    "start": "252560",
    "end": "254209"
  },
  {
    "text": "and we divide that by the length\nof the reference or gold text.",
    "start": "254210",
    "end": "258648"
  },
  {
    "text": "And then, when we average over\na whole corpus, what we do",
    "start": "258649",
    "end": "261859"
  },
  {
    "text": "is for the numerator sum up all\nof the distances between gold",
    "start": "261860",
    "end": "265879"
  },
  {
    "text": "and predicted text,\nand we divide that",
    "start": "265880",
    "end": "267830"
  },
  {
    "text": "by the total length of all\nof the reference texts.",
    "start": "267830",
    "end": "272819"
  },
  {
    "text": "Properties of word error rate.",
    "start": "272820",
    "end": "274410"
  },
  {
    "text": "Its bounds are 0 and\ninfinity, with 0 the best.",
    "start": "274410",
    "end": "276870"
  },
  {
    "text": "It's an error rate, so\nwe're trying to minimize it.",
    "start": "276870",
    "end": "279510"
  },
  {
    "text": "The value encoded is\nsomething like how",
    "start": "279510",
    "end": "281730"
  },
  {
    "text": "aligned is the\npredicted sequence",
    "start": "281730",
    "end": "283410"
  },
  {
    "text": "with the actual sequence.",
    "start": "283410",
    "end": "284830"
  },
  {
    "text": "So in that way, it's\nsimilar to F scores",
    "start": "284830",
    "end": "287520"
  },
  {
    "text": "once you have thought\nabout your distance metric.",
    "start": "287520",
    "end": "290990"
  },
  {
    "text": "Weaknesses.",
    "start": "290990",
    "end": "291979"
  },
  {
    "text": "Well, it can accommodate\njust one reference text.",
    "start": "291980",
    "end": "295130"
  },
  {
    "text": "Our fundamental\nchallenge here is",
    "start": "295130",
    "end": "296720"
  },
  {
    "text": "that there's more\nthan one good way",
    "start": "296720",
    "end": "298190"
  },
  {
    "text": "to say most things, whereas\nhere, we can accommodate",
    "start": "298190",
    "end": "301130"
  },
  {
    "text": "only a single way,\npresumably a good one,",
    "start": "301130",
    "end": "304010"
  },
  {
    "text": "of saying the thing\nthat we care about.",
    "start": "304010",
    "end": "306680"
  },
  {
    "text": "It's also a very syntactic\nnotion by default.",
    "start": "306680",
    "end": "310250"
  },
  {
    "text": "Most distance metrics\nare string edit metrics,",
    "start": "310250",
    "end": "313520"
  },
  {
    "text": "and they're very sensitive\nto the actual structure",
    "start": "313520",
    "end": "316160"
  },
  {
    "text": "of the string.",
    "start": "316160",
    "end": "316760"
  },
  {
    "text": "And so, as a result, by\nand large, these metrics",
    "start": "316760",
    "end": "320000"
  },
  {
    "text": "will treat it was\ngood, it was not good,",
    "start": "320000",
    "end": "322700"
  },
  {
    "text": "and it was great\nas all similarly",
    "start": "322700",
    "end": "324680"
  },
  {
    "text": "distant from each other when\nof course, semantically, it",
    "start": "324680",
    "end": "327650"
  },
  {
    "text": "was good, and it was\ngreat are kind of alike",
    "start": "327650",
    "end": "330710"
  },
  {
    "text": "and different from\nit was not good.",
    "start": "330710",
    "end": "335130"
  },
  {
    "text": "BLEU scores build on intuitions\naround word error rate,",
    "start": "335130",
    "end": "338195"
  },
  {
    "text": "and they're trying to be\nmore sensitive to the fact",
    "start": "338195",
    "end": "340320"
  },
  {
    "text": "that there's more than one\nway to say most things.",
    "start": "340320",
    "end": "343170"
  },
  {
    "text": "Here's how BLEU scores work.",
    "start": "343170",
    "end": "344520"
  },
  {
    "text": "It's again going to be a\nbalance of precision and recall",
    "start": "344520",
    "end": "347430"
  },
  {
    "text": "but now tailored to\nthe generation space.",
    "start": "347430",
    "end": "349979"
  },
  {
    "text": "The notion of precision is\nmodified n-gram precision.",
    "start": "349980",
    "end": "353730"
  },
  {
    "text": "Let's walk through this\nsimple example here.",
    "start": "353730",
    "end": "355950"
  },
  {
    "text": "We have a candidate,\nwhich is kind of unusual.",
    "start": "355950",
    "end": "358180"
  },
  {
    "text": "It's just seven occurrences\nof the word \"the.\"",
    "start": "358180",
    "end": "360780"
  },
  {
    "text": "So obviously not\na good candidate.",
    "start": "360780",
    "end": "362850"
  },
  {
    "text": "We have two reference texts.",
    "start": "362850",
    "end": "364890"
  },
  {
    "text": "These are presumed to\nbe human-created texts.",
    "start": "364890",
    "end": "367590"
  },
  {
    "text": "The modified n-gram precision\nfor \"the\" is 2 over 7.",
    "start": "367590",
    "end": "371970"
  },
  {
    "text": "There are seven occurrences\nof \"the\" in the candidate,",
    "start": "371970",
    "end": "375060"
  },
  {
    "text": "and the 2 comes from the\nreference text that contains",
    "start": "375060",
    "end": "378810"
  },
  {
    "text": "the maximum number of \"the.\"",
    "start": "378810",
    "end": "381490"
  },
  {
    "text": "And that's reference text 1.",
    "start": "381490",
    "end": "383220"
  },
  {
    "text": "It has two tokens of \"the,\"\nwhereas reference text two has",
    "start": "383220",
    "end": "386430"
  },
  {
    "text": "just one token of \"the.\"",
    "start": "386430",
    "end": "387990"
  },
  {
    "text": "So we get two points for that.",
    "start": "387990",
    "end": "389889"
  },
  {
    "text": "And then, the modified n-gram\nprecision for the is 2 over 7.",
    "start": "389890",
    "end": "394930"
  },
  {
    "text": "That's a notion of precision.",
    "start": "394930",
    "end": "396729"
  },
  {
    "text": "We need to balance that\nwith a notion of recall.",
    "start": "396730",
    "end": "399310"
  },
  {
    "text": "Otherwise, we might\nend up with systems",
    "start": "399310",
    "end": "401080"
  },
  {
    "text": "that do very short generations\nin order to be very precise.",
    "start": "401080",
    "end": "405759"
  },
  {
    "text": "And so, for recall,\nBLEU introduces what",
    "start": "405760",
    "end": "408280"
  },
  {
    "text": "they call a brevity penalty.",
    "start": "408280",
    "end": "409840"
  },
  {
    "text": "And in essence,\nwhat this is doing",
    "start": "409840",
    "end": "411669"
  },
  {
    "text": "is saying if the generated\ntext is shorter than the text",
    "start": "411670",
    "end": "416080"
  },
  {
    "text": "I expect from my corpus,\nI pay a little penalty.",
    "start": "416080",
    "end": "419229"
  },
  {
    "text": "Once I get to the\nexpected length,",
    "start": "419230",
    "end": "421060"
  },
  {
    "text": "you stop paying a recall\nor brevity penalty,",
    "start": "421060",
    "end": "424030"
  },
  {
    "text": "and you start to rely on the\nmodified n-gram precision.",
    "start": "424030",
    "end": "428200"
  },
  {
    "text": "And the BLEU score\nitself is just",
    "start": "428200",
    "end": "429820"
  },
  {
    "text": "a product of the\nbrevity penalty score",
    "start": "429820",
    "end": "431980"
  },
  {
    "text": "and the sum of the weighted\nmodified n-gram precision",
    "start": "431980",
    "end": "435100"
  },
  {
    "text": "values for each n.",
    "start": "435100",
    "end": "436760"
  },
  {
    "text": "What I mean by that is we\ncould do this for unigrams,",
    "start": "436760",
    "end": "439840"
  },
  {
    "text": "bigrams, trigrams.",
    "start": "439840",
    "end": "441522"
  },
  {
    "text": "We could assign different weight\nto those different notions",
    "start": "441522",
    "end": "443980"
  },
  {
    "text": "of n-gram, and all of\nthose are incorporated",
    "start": "443980",
    "end": "446650"
  },
  {
    "text": "if we want into the BLEU score.",
    "start": "446650",
    "end": "449979"
  },
  {
    "text": "Properties of the BLEU score.",
    "start": "449980",
    "end": "451600"
  },
  {
    "text": "Its bounds are 0 and\n1, with 1 the best",
    "start": "451600",
    "end": "453700"
  },
  {
    "text": "that we have no expectation\nfor naturalistic data",
    "start": "453700",
    "end": "456820"
  },
  {
    "text": "that any system will\nachieve one because there's",
    "start": "456820",
    "end": "458980"
  },
  {
    "text": "no way we can have all\nthe relevant reference",
    "start": "458980",
    "end": "461290"
  },
  {
    "text": "texts conceptually.",
    "start": "461290",
    "end": "463570"
  },
  {
    "text": "The value encoded is something\nlike an appropriate, we hope,",
    "start": "463570",
    "end": "466990"
  },
  {
    "text": "balance of precision and\nrecall as implemented",
    "start": "466990",
    "end": "470289"
  },
  {
    "text": "in that brevity penalty.",
    "start": "470290",
    "end": "472750"
  },
  {
    "text": "It's similar to the\nword error rate,",
    "start": "472750",
    "end": "474528"
  },
  {
    "text": "but it seeks to accommodate the\nfact that there are typically",
    "start": "474528",
    "end": "477070"
  },
  {
    "text": "multiple suitable outputs\nfor a given input.",
    "start": "477070",
    "end": "479500"
  },
  {
    "text": "Our kind of fundamental\nchallenge for generation.",
    "start": "479500",
    "end": "482470"
  },
  {
    "text": "Weaknesses.",
    "start": "482470",
    "end": "483490"
  },
  {
    "text": "Well, there's a long\nliterature on this,",
    "start": "483490",
    "end": "486430"
  },
  {
    "text": "some of it arguing\nthat BLEU fails",
    "start": "486430",
    "end": "488949"
  },
  {
    "text": "to correlate with human\nscoring for translation,",
    "start": "488950",
    "end": "491470"
  },
  {
    "text": "which is an important\napplication domain for BLEU.",
    "start": "491470",
    "end": "494020"
  },
  {
    "text": "So that's worrisome.",
    "start": "494020",
    "end": "495550"
  },
  {
    "text": "It's very sensitive to the\nn-gram order of things.",
    "start": "495550",
    "end": "498740"
  },
  {
    "text": "So in that way,\nit is kind of very",
    "start": "498740",
    "end": "500319"
  },
  {
    "text": "attuned to syntactic elements\nof these comparisons.",
    "start": "500320",
    "end": "503800"
  },
  {
    "text": "It's insensitive to n-gram type.",
    "start": "503800",
    "end": "506139"
  },
  {
    "text": "Again, that's a notion\nof string dependence.",
    "start": "506140",
    "end": "508810"
  },
  {
    "text": "So \"that dog,\" \"the\ndog,\" and \"that toaster\"",
    "start": "508810",
    "end": "512080"
  },
  {
    "text": "might be treated identically\nwith your BLEU scoring",
    "start": "512080",
    "end": "514630"
  },
  {
    "text": "even though \"that\"\ndog and \"the dog\" are",
    "start": "514630",
    "end": "516309"
  },
  {
    "text": "obviously closer to each\nother than \"that toaster.\"",
    "start": "516309",
    "end": "520900"
  },
  {
    "text": "And finally, for you\nhave to be really",
    "start": "520900",
    "end": "523240"
  },
  {
    "text": "thoughtful about the domain\nthat you're operating in",
    "start": "523240",
    "end": "525790"
  },
  {
    "text": "because BLEU might\nbe just mismatched",
    "start": "525790",
    "end": "527980"
  },
  {
    "text": "to the goals of\ngeneration in that space.",
    "start": "527980",
    "end": "529810"
  },
  {
    "text": "For example, Liu et al.",
    "start": "529810",
    "end": "531400"
  },
  {
    "text": "2016 in the process of\ndeveloping and evaluating",
    "start": "531400",
    "end": "534680"
  },
  {
    "text": "neural conversational agents,\njust argue against BLEU",
    "start": "534680",
    "end": "538220"
  },
  {
    "text": "as a metric for dialogue.",
    "start": "538220",
    "end": "540629"
  },
  {
    "text": "So think carefully about\nwhat your generations mean,",
    "start": "540630",
    "end": "544220"
  },
  {
    "text": "what kind of reference\ntexts you actually have",
    "start": "544220",
    "end": "546589"
  },
  {
    "text": "and whether or not\neverything is aligned",
    "start": "546590",
    "end": "548870"
  },
  {
    "text": "given your high-level goals.",
    "start": "548870",
    "end": "550339"
  },
  {
    "text": "Again, a common refrain for\nthis unit of the course.",
    "start": "550340",
    "end": "555320"
  },
  {
    "text": "I mentioned two\nreference-based metrics,",
    "start": "555320",
    "end": "557500"
  },
  {
    "text": "and I call them reference-based.",
    "start": "557500",
    "end": "558940"
  },
  {
    "text": "Word error rate and\nBLEU are both like this",
    "start": "558940",
    "end": "560860"
  },
  {
    "text": "because they depend on\nthese reference texts,",
    "start": "560860",
    "end": "563800"
  },
  {
    "text": "these human-created texts.",
    "start": "563800",
    "end": "566200"
  },
  {
    "text": "Others in that family\ninclude ROUGE and METEOR.",
    "start": "566200",
    "end": "569590"
  },
  {
    "text": "And what you can see happening\nhere, especially with METEOR,",
    "start": "569590",
    "end": "572590"
  },
  {
    "text": "is that we're trying to be\noriented toward a task, maybe",
    "start": "572590",
    "end": "575980"
  },
  {
    "text": "a semantic task like\nsummarization, and also",
    "start": "575980",
    "end": "578350"
  },
  {
    "text": "less sensitive to fine-grained\ndetails of the reference texts",
    "start": "578350",
    "end": "582759"
  },
  {
    "text": "and the generated text to key\ninto more semantic notions.",
    "start": "582760",
    "end": "586480"
  },
  {
    "text": "METEOR does that with things\nlike stemming and synonyms.",
    "start": "586480",
    "end": "590350"
  },
  {
    "text": "And with scoring procedures\nlike CIDEr and BERTScore,",
    "start": "590350",
    "end": "593410"
  },
  {
    "text": "we actually move into\nvector space models",
    "start": "593410",
    "end": "595779"
  },
  {
    "text": "that we might hope capture\nmany deep aspects of semantics.",
    "start": "595780",
    "end": "599500"
  },
  {
    "text": "CIDEr does this\nwith TF-IDF vectors.",
    "start": "599500",
    "end": "602380"
  },
  {
    "text": "It's a powerful\nidea though it does",
    "start": "602380",
    "end": "604030"
  },
  {
    "text": "make it heavily dependent on\nthe nature of the dataset.",
    "start": "604030",
    "end": "607600"
  },
  {
    "text": "And then BERTScore uses weighted\nMaxSim at the token level",
    "start": "607600",
    "end": "612009"
  },
  {
    "text": "to define scores\nbetween two texts.",
    "start": "612010",
    "end": "614870"
  },
  {
    "text": "That's a very semantic notion.",
    "start": "614870",
    "end": "616430"
  },
  {
    "text": "In fact, the scoring procedure\nlooks a lot like the one",
    "start": "616430",
    "end": "618800"
  },
  {
    "text": "that we use for the\nColBERT retrieval model.",
    "start": "618800",
    "end": "621890"
  },
  {
    "text": "What you can see happening,\nespecially with BERTScore,",
    "start": "621890",
    "end": "624500"
  },
  {
    "text": "is that we're trying\nto get away from all",
    "start": "624500",
    "end": "626660"
  },
  {
    "text": "the details of these strings\nand really key into deeper",
    "start": "626660",
    "end": "629779"
  },
  {
    "text": "aspects of meaning.",
    "start": "629780",
    "end": "630830"
  },
  {
    "start": "630830",
    "end": "633340"
  },
  {
    "text": "I thought I'd mention\nimage-based NLG metrics.",
    "start": "633340",
    "end": "635980"
  },
  {
    "text": "Some of you might be\ndeveloping systems",
    "start": "635980",
    "end": "637660"
  },
  {
    "text": "that take images as\ninput, produce text,",
    "start": "637660",
    "end": "639815"
  },
  {
    "text": "and then we want to\nask the question of",
    "start": "639815",
    "end": "641440"
  },
  {
    "text": "whether or not that's a\ngood text for the image.",
    "start": "641440",
    "end": "644710"
  },
  {
    "text": "For this task,\nreference-based metrics",
    "start": "644710",
    "end": "648160"
  },
  {
    "text": "like BLEU and word\nerror rate will be fine,",
    "start": "648160",
    "end": "650500"
  },
  {
    "text": "assuming that the human\nannotations exist and are kind",
    "start": "650500",
    "end": "653440"
  },
  {
    "text": "of aligned with the\nactual goal that you have",
    "start": "653440",
    "end": "656140"
  },
  {
    "text": "for the text that\nyou're generating",
    "start": "656140",
    "end": "657850"
  },
  {
    "text": "conditional on these images.",
    "start": "657850",
    "end": "660660"
  },
  {
    "text": "But that could be a large\nburden for many domains",
    "start": "660660",
    "end": "663420"
  },
  {
    "text": "and many tasks.",
    "start": "663420",
    "end": "664230"
  },
  {
    "text": "We won't have annotations\nin the right way",
    "start": "664230",
    "end": "666959"
  },
  {
    "text": "for these\nreference-based metrics.",
    "start": "666960",
    "end": "668800"
  },
  {
    "text": "So you might think about\nreference list metrics.",
    "start": "668800",
    "end": "671610"
  },
  {
    "text": "Metrics in this space seek\nto score text image pairs",
    "start": "671610",
    "end": "675120"
  },
  {
    "text": "with no need for\nhuman-created references.",
    "start": "675120",
    "end": "678060"
  },
  {
    "text": "At this moment, the\nmost popular of these",
    "start": "678060",
    "end": "680310"
  },
  {
    "text": "is certainly CLIPScore,\nbut there are",
    "start": "680310",
    "end": "681870"
  },
  {
    "text": "others like UMIC and SPURTS.",
    "start": "681870",
    "end": "684060"
  },
  {
    "text": "And the vision here\nis to drop the need",
    "start": "684060",
    "end": "685980"
  },
  {
    "text": "for human annotation, which is a\nmajor bottleneck for evaluation",
    "start": "685980",
    "end": "690630"
  },
  {
    "text": "and instead just score these\nimage text pairs in isolation.",
    "start": "690630",
    "end": "695540"
  },
  {
    "text": "I think this is\nreally promising.",
    "start": "695540",
    "end": "696949"
  },
  {
    "text": "We do have a paper Kreiss et al.",
    "start": "696950",
    "end": "698390"
  },
  {
    "text": "2022 where we criticize\nthese reference",
    "start": "698390",
    "end": "701300"
  },
  {
    "text": "list metrics on the\ngrounds that they",
    "start": "701300",
    "end": "703100"
  },
  {
    "text": "are insensitive to the\npurpose of the text",
    "start": "703100",
    "end": "705829"
  },
  {
    "text": "and the context in which\nthe image appeared.",
    "start": "705830",
    "end": "708410"
  },
  {
    "text": "Those are crucial aspects\nwhen you think about our goals",
    "start": "708410",
    "end": "711319"
  },
  {
    "text": "for generation in this context.",
    "start": "711320",
    "end": "713280"
  },
  {
    "text": "And it's a shame that these\nreference metrics are just",
    "start": "713280",
    "end": "716030"
  },
  {
    "text": "missing that information.",
    "start": "716030",
    "end": "717990"
  },
  {
    "text": "However, we are\noptimistic that we",
    "start": "717990",
    "end": "720140"
  },
  {
    "text": "can design variants of CLIPScore\nand related metrics that",
    "start": "720140",
    "end": "724130"
  },
  {
    "text": "can actually bring in\nthese notions of quality.",
    "start": "724130",
    "end": "726840"
  },
  {
    "text": "So I think reference\nmetrics may be a fruitful",
    "start": "726840",
    "end": "729680"
  },
  {
    "text": "path forward for evaluation\nfor image-based NLG.",
    "start": "729680",
    "end": "734318"
  },
  {
    "text": "And then, finally,\nto round this out,",
    "start": "734318",
    "end": "735860"
  },
  {
    "text": "I thought I'd offer a\nmore high-level comment",
    "start": "735860",
    "end": "738320"
  },
  {
    "text": "under the heading of\ntask-oriented metrics.",
    "start": "738320",
    "end": "741800"
  },
  {
    "text": "We've been very focused\nso far on comparisons",
    "start": "741800",
    "end": "744410"
  },
  {
    "text": "with reference texts and other\nnotions of intrinsic quality.",
    "start": "744410",
    "end": "748339"
  },
  {
    "text": "But we should\nreflect on the fact",
    "start": "748340",
    "end": "749990"
  },
  {
    "text": "that by and large\nwhen we do generation,",
    "start": "749990",
    "end": "751730"
  },
  {
    "text": "we're trying to achieve\nsome goal of communication",
    "start": "751730",
    "end": "754370"
  },
  {
    "text": "or to help an agent\ntake some action, right?",
    "start": "754370",
    "end": "757560"
  },
  {
    "text": "The classical off-the-shelf\nreference-based metrics",
    "start": "757560",
    "end": "760890"
  },
  {
    "text": "will capture aspects of\nthe task only to the extent",
    "start": "760890",
    "end": "764250"
  },
  {
    "text": "that the human annotations did.",
    "start": "764250",
    "end": "766110"
  },
  {
    "text": "So if your reference\ntexts aren't",
    "start": "766110",
    "end": "767670"
  },
  {
    "text": "sensitive to what the\ngoal of generation was,",
    "start": "767670",
    "end": "770279"
  },
  {
    "text": "then that won't be reflected\nin your evaluation.",
    "start": "770280",
    "end": "773430"
  },
  {
    "text": "You can imagine\nmodel-based metrics that",
    "start": "773430",
    "end": "775830"
  },
  {
    "text": "are tuned to specific tasks\nand therefore task-oriented",
    "start": "775830",
    "end": "779190"
  },
  {
    "text": "in their nature,\nbut that's actually",
    "start": "779190",
    "end": "781020"
  },
  {
    "text": "currently a very rare model.",
    "start": "781020",
    "end": "783360"
  },
  {
    "text": "I think it's fruitful,\nthough, to think",
    "start": "783360",
    "end": "785130"
  },
  {
    "text": "about what the goal of\nthe text was and consider",
    "start": "785130",
    "end": "787980"
  },
  {
    "text": "whether your evaluation\ncould be based in that goal.",
    "start": "787980",
    "end": "790470"
  },
  {
    "text": "This would be a new\nmode of thinking.",
    "start": "790470",
    "end": "792509"
  },
  {
    "text": "You would ask\nyourself, can an agent",
    "start": "792510",
    "end": "794370"
  },
  {
    "text": "received the generated text\nuse it to solve the task",
    "start": "794370",
    "end": "798010"
  },
  {
    "text": "and then your metric\nwould be task success?",
    "start": "798010",
    "end": "801720"
  },
  {
    "text": "Or was a specific\npiece of information",
    "start": "801720",
    "end": "804420"
  },
  {
    "text": "reliably communicated?",
    "start": "804420",
    "end": "805769"
  },
  {
    "text": "Again, we could\njust ask directly",
    "start": "805770",
    "end": "807810"
  },
  {
    "text": "whether the agent receiving\nthe message reliably",
    "start": "807810",
    "end": "811110"
  },
  {
    "text": "extracted the information\nwe care about.",
    "start": "811110",
    "end": "813480"
  },
  {
    "text": "Or did the message\nlead the person",
    "start": "813480",
    "end": "815310"
  },
  {
    "text": "to take a desirable\naction which would",
    "start": "815310",
    "end": "817080"
  },
  {
    "text": "be a more indirect measure\nof communicative success?",
    "start": "817080",
    "end": "820770"
  },
  {
    "text": "And that could be\nthe fundamental thing",
    "start": "820770",
    "end": "822780"
  },
  {
    "text": "that we use for our\nmetric for generation.",
    "start": "822780",
    "end": "825240"
  },
  {
    "text": "That will capture some aspects\nand leave out some others,",
    "start": "825240",
    "end": "828450"
  },
  {
    "text": "for example, fluency.",
    "start": "828450",
    "end": "829770"
  },
  {
    "text": "But I think overall,\nyou can imagine",
    "start": "829770",
    "end": "831720"
  },
  {
    "text": "that this is much more\ntightly aligned with the goals",
    "start": "831720",
    "end": "834810"
  },
  {
    "text": "that we actually have for\nour generation systems.",
    "start": "834810",
    "end": "838820"
  },
  {
    "start": "838820",
    "end": "843000"
  }
]