[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "start": "0",
    "end": "5950"
  },
  {
    "text": "Hi.",
    "start": "5950",
    "end": "6470"
  },
  {
    "start": "6000",
    "end": "10000"
  },
  {
    "text": "In this module, I'm\ngoing to cover the basics",
    "start": "6470",
    "end": "8344"
  },
  {
    "text": "of linear regression.",
    "start": "8345",
    "end": "10170"
  },
  {
    "start": "10000",
    "end": "55000"
  },
  {
    "text": "Our story of linear regression\nbegins on January 1st, 1801.",
    "start": "10170",
    "end": "14520"
  },
  {
    "text": "Italian astronomer Piazzi\nlooked up at the night sky",
    "start": "14520",
    "end": "18090"
  },
  {
    "text": "and discovered something\nwhich he named Ceres.",
    "start": "18090",
    "end": "21690"
  },
  {
    "text": "He didn't know what it was.",
    "start": "21690",
    "end": "22820"
  },
  {
    "text": "Was is a comet or a planet?",
    "start": "22820",
    "end": "24730"
  },
  {
    "text": "But he did make some\nobservations of the location",
    "start": "24730",
    "end": "27480"
  },
  {
    "text": "before it was\nobscured by the sun.",
    "start": "27480",
    "end": "29640"
  },
  {
    "text": "The data he collected\nlooked like this--",
    "start": "29640",
    "end": "32279"
  },
  {
    "text": "at a particular\ntime, two numbers",
    "start": "32280",
    "end": "34500"
  },
  {
    "text": "which represent the location\nof Ceres in the night sky.",
    "start": "34500",
    "end": "40430"
  },
  {
    "text": "But now the big\nquestion at the time",
    "start": "40430",
    "end": "42370"
  },
  {
    "text": "was when and where\nwas Ceres going",
    "start": "42370",
    "end": "44710"
  },
  {
    "text": "to be observed again to\nre-emerge from behind the Sun?",
    "start": "44710",
    "end": "48790"
  },
  {
    "text": "All the top\nastronomers at the time",
    "start": "48790",
    "end": "50710"
  },
  {
    "text": "went and tried to analyze this\ndata and figure out the answer.",
    "start": "50710",
    "end": "55329"
  },
  {
    "start": "55000",
    "end": "102000"
  },
  {
    "text": "So Carl Friedrich Gauss,\nfamous German mathematician,",
    "start": "55330",
    "end": "59680"
  },
  {
    "text": "took Piazzi's data and created\na model of Ceres' orbits",
    "start": "59680",
    "end": "63010"
  },
  {
    "text": "and makes a prediction.",
    "start": "63010",
    "end": "64900"
  },
  {
    "text": "This prediction was actually\nwildly different from all",
    "start": "64900",
    "end": "67570"
  },
  {
    "text": "the other predictions that\nother astronomers made,",
    "start": "67570",
    "end": "69850"
  },
  {
    "text": "but in December,\nCeres was located,",
    "start": "69850",
    "end": "73000"
  },
  {
    "text": "and Gauss's prediction was\nby far the most accurate.",
    "start": "73000",
    "end": "76470"
  },
  {
    "text": "So now, there's an\ninteresting story here.",
    "start": "76470",
    "end": "78220"
  },
  {
    "text": "Gauss was actually\nvery secretive",
    "start": "78220",
    "end": "79950"
  },
  {
    "text": "about what his method\nwas, and in 1805,",
    "start": "79950",
    "end": "83759"
  },
  {
    "text": "the French\nmathematician Legendre",
    "start": "83760",
    "end": "85860"
  },
  {
    "text": "was actually the first\nto publish the method,",
    "start": "85860",
    "end": "88890"
  },
  {
    "text": "before Gauss could publish\nin 1809, even though Gauss",
    "start": "88890",
    "end": "91770"
  },
  {
    "text": "had this method back in 1795.",
    "start": "91770",
    "end": "94859"
  },
  {
    "text": "The method here is\nnone other than least",
    "start": "94860",
    "end": "97290"
  },
  {
    "text": "squares linear regression, which\nis the topic of this module.",
    "start": "97290",
    "end": "103200"
  },
  {
    "start": "102000",
    "end": "214000"
  },
  {
    "text": "So here is the framework.",
    "start": "103200",
    "end": "104899"
  },
  {
    "text": "So we are given some\ntraining data which",
    "start": "104900",
    "end": "107750"
  },
  {
    "text": "consists of a set of examples.",
    "start": "107750",
    "end": "110300"
  },
  {
    "text": "Each example consists of\nan input x and output y.",
    "start": "110300",
    "end": "113910"
  },
  {
    "text": "So 1, 1, 2, 3, 4, 3, and we\ncan visualize these examples",
    "start": "113910",
    "end": "120360"
  },
  {
    "text": "on a 2D plot here,\nplotting y, the output,",
    "start": "120360",
    "end": "123450"
  },
  {
    "text": "against x, the input.",
    "start": "123450",
    "end": "125710"
  },
  {
    "text": "So here is 1, 1, here is\n2, 3, and here is 4, 3.",
    "start": "125710",
    "end": "132360"
  },
  {
    "text": "So what we want to\ndo is take this data,",
    "start": "132360",
    "end": "134640"
  },
  {
    "text": "have a learning algorithm.",
    "start": "134640",
    "end": "136470"
  },
  {
    "text": "Produce a predictor f, and\nthe predictor in this case",
    "start": "136470",
    "end": "142420"
  },
  {
    "text": "is let's say a line.",
    "start": "142420",
    "end": "145370"
  },
  {
    "text": "OK?",
    "start": "145370",
    "end": "145870"
  },
  {
    "text": "And what the predictor\nallows us to do",
    "start": "145870",
    "end": "147670"
  },
  {
    "text": "is to take new inputs,\nsuch as this 3 here,",
    "start": "147670",
    "end": "150790"
  },
  {
    "text": "and send it through and\nproduce an output, 2.71,",
    "start": "150790",
    "end": "155739"
  },
  {
    "text": "corresponding to this\npoint on this line here.",
    "start": "155740",
    "end": "160860"
  },
  {
    "text": "And there are three\ndesign decisions",
    "start": "160860",
    "end": "163950"
  },
  {
    "text": "that we need to make to\nflesh out this framework.",
    "start": "163950",
    "end": "166950"
  },
  {
    "text": "First, what are the possible\npredictors that the learning",
    "start": "166950",
    "end": "169890"
  },
  {
    "text": "algorithm is allowed to output?",
    "start": "169890",
    "end": "171750"
  },
  {
    "text": "Is it only lines, or\nis it curves as well?",
    "start": "171750",
    "end": "174160"
  },
  {
    "text": "This is a question of what\nis the hypothesis class.",
    "start": "174160",
    "end": "177660"
  },
  {
    "text": "Second question, how good is\na predictor, and the answer",
    "start": "177660",
    "end": "183490"
  },
  {
    "text": "is going to be framed in\nterms of determining a loss",
    "start": "183490",
    "end": "186190"
  },
  {
    "text": "function that judges\neach individual predictor",
    "start": "186190",
    "end": "189580"
  },
  {
    "text": "in the hypothesis class.",
    "start": "189580",
    "end": "191950"
  },
  {
    "text": "And finally, how do we actually\ncompute the best predictor?",
    "start": "191950",
    "end": "195010"
  },
  {
    "text": "There are a lot of\npredictors there,",
    "start": "195010",
    "end": "196970"
  },
  {
    "text": "and even if we have a\nloss function, how do we",
    "start": "196970",
    "end": "199420"
  },
  {
    "text": "go searching for them?",
    "start": "199420",
    "end": "201190"
  },
  {
    "text": "And this is going to be the\nquestion of the optimization",
    "start": "201190",
    "end": "203980"
  },
  {
    "text": "algorithm.",
    "start": "203980",
    "end": "205629"
  },
  {
    "text": "So this is a recipe\nthat we're going",
    "start": "205630",
    "end": "207720"
  },
  {
    "text": "to see over and\nover again, and it's",
    "start": "207720",
    "end": "209900"
  },
  {
    "text": "kind of like a build your\nown learning algorithm.",
    "start": "209900",
    "end": "213760"
  },
  {
    "text": "So we're going to start\nwith the first question.",
    "start": "213760",
    "end": "216580"
  },
  {
    "start": "214000",
    "end": "362000"
  },
  {
    "text": "What is the hypothesis class?",
    "start": "216580",
    "end": "219250"
  },
  {
    "text": "So here is that predictor that\nwe were looking at, f of x",
    "start": "219250",
    "end": "222150"
  },
  {
    "text": "equals 1 plus 0.57x, and that\ncorresponds to this red line.",
    "start": "222150",
    "end": "231290"
  },
  {
    "text": "And here's another one.",
    "start": "231290",
    "end": "232530"
  },
  {
    "text": "Here's a purple predictor\nwhich has an intercept of 2",
    "start": "232530",
    "end": "237650"
  },
  {
    "text": "and a slope of 0.2.",
    "start": "237650",
    "end": "240290"
  },
  {
    "text": "And in general, you can consider\npredictors of a following form,",
    "start": "240290",
    "end": "244189"
  },
  {
    "text": "f of x equals w1 plus\nw2 of x for arbitrary w1",
    "start": "244190",
    "end": "250100"
  },
  {
    "text": "which is the intercept\nand w2 which is the slope.",
    "start": "250100",
    "end": "255830"
  },
  {
    "text": "So now, we're going to\ngeneralize this using vector",
    "start": "255830",
    "end": "258370"
  },
  {
    "text": "notation.",
    "start": "258370",
    "end": "259970"
  },
  {
    "text": "So let's take w1 and w2 and pack\nthem up together into a vector,",
    "start": "259970",
    "end": "266060"
  },
  {
    "text": "which we will call w.",
    "start": "266060",
    "end": "268300"
  },
  {
    "text": "This is called a weight\nvector, and we're also",
    "start": "268300",
    "end": "271780"
  },
  {
    "text": "going to define a\nfeature extractor,",
    "start": "271780",
    "end": "273910"
  },
  {
    "text": "also known as a\nfeature map, phi.",
    "start": "273910",
    "end": "276370"
  },
  {
    "text": "So phi is going to take\nan arbitrary input x",
    "start": "276370",
    "end": "279550"
  },
  {
    "text": "and return the vector 1,\nx, at least in this case,",
    "start": "279550",
    "end": "284319"
  },
  {
    "text": "and 1, x is going to be\nknown as the feature vector.",
    "start": "284320",
    "end": "288620"
  },
  {
    "text": "So now, we can simply\nrewrite this equation up here",
    "start": "288620",
    "end": "291710"
  },
  {
    "text": "in vector notation.",
    "start": "291710",
    "end": "292889"
  },
  {
    "text": "So we're going to\nwrite f sub w to denote",
    "start": "292890",
    "end": "295580"
  },
  {
    "text": "that this predictor\ndepends on the weights",
    "start": "295580",
    "end": "297949"
  },
  {
    "text": "of a particular input x is\nequal to w dot phi of x.",
    "start": "297950",
    "end": "303890"
  },
  {
    "text": "And this w dot phi of x, which\nwe'll see over and over again,",
    "start": "303890",
    "end": "307310"
  },
  {
    "text": "is called the score.",
    "start": "307310",
    "end": "309810"
  },
  {
    "text": "OK?",
    "start": "309810",
    "end": "310310"
  },
  {
    "text": "So here's an example.",
    "start": "310310",
    "end": "311760"
  },
  {
    "text": "If you stick in 3\ninto this predictor,",
    "start": "311760",
    "end": "314930"
  },
  {
    "text": "then what we're doing is\ntaking the weight vector",
    "start": "314930",
    "end": "317660"
  },
  {
    "text": "and dotting it with the\nfeature vector applied to 3.",
    "start": "317660",
    "end": "322570"
  },
  {
    "text": "And remember the definition\nof feature vector is of 1, x,",
    "start": "322570",
    "end": "326620"
  },
  {
    "text": "so that's 1, 3 here.",
    "start": "326620",
    "end": "328250"
  },
  {
    "text": "And if you take the dot product,\n1 times 1 plus 0.57 times 3,",
    "start": "328250",
    "end": "332350"
  },
  {
    "text": "that gives you 2.71.",
    "start": "332350",
    "end": "336560"
  },
  {
    "text": "So now, finally,\nthe hypothesis class",
    "start": "336560",
    "end": "339440"
  },
  {
    "text": "is defined as a set script F\nis the set of all predictors f",
    "start": "339440",
    "end": "348220"
  },
  {
    "text": "sub w, where w can be an\narbitrary intercept and slope.",
    "start": "348220",
    "end": "352480"
  },
  {
    "text": "w is an arbitrary vector.",
    "start": "352480",
    "end": "356380"
  },
  {
    "text": "OK.",
    "start": "356380",
    "end": "356880"
  },
  {
    "text": "So that defines our\nhypothesis class",
    "start": "356880",
    "end": "359190"
  },
  {
    "text": "that we're going\nto be working with.",
    "start": "359190",
    "end": "363028"
  },
  {
    "start": "362000",
    "end": "516000"
  },
  {
    "text": "So now, let's turn to the\nsecond design decision.",
    "start": "363028",
    "end": "365070"
  },
  {
    "text": "How good is a predictor?",
    "start": "365070",
    "end": "367430"
  },
  {
    "text": "So let's take our predictor\nthat we're looking at,",
    "start": "367430",
    "end": "369949"
  },
  {
    "text": "the red one, and let's\nlook at some training data.",
    "start": "369950",
    "end": "373375"
  },
  {
    "text": "So this is the training\ndata that we've had before.",
    "start": "373375",
    "end": "375500"
  },
  {
    "text": "Let's plot the predictor and\nthe three data points, this one,",
    "start": "375500",
    "end": "380150"
  },
  {
    "text": "the 2, 3, and 4, 3.",
    "start": "380150",
    "end": "383389"
  },
  {
    "text": "So intuitively, how\ngood is a predictor",
    "start": "383390",
    "end": "385940"
  },
  {
    "text": "is how good it fits\nthe training data,",
    "start": "385940",
    "end": "390230"
  },
  {
    "text": "and we're going to\nquantify that by measuring",
    "start": "390230",
    "end": "392570"
  },
  {
    "text": "the distance between the\nprediction and a target.",
    "start": "392570",
    "end": "396650"
  },
  {
    "text": "This difference is\ncalled the residual.",
    "start": "396650",
    "end": "398277"
  },
  {
    "text": "So we're going to\nmeasure the residual",
    "start": "398277",
    "end": "399860"
  },
  {
    "text": "for each of our\npoints, and we're",
    "start": "399860",
    "end": "403189"
  },
  {
    "text": "going to take that into account.",
    "start": "403190",
    "end": "405090"
  },
  {
    "text": "So formally, we're going to\ndefine a loss function, which",
    "start": "405090",
    "end": "409340"
  },
  {
    "text": "is a function of an example x,\ny and a particular wave vector,",
    "start": "409340",
    "end": "414139"
  },
  {
    "text": "and that's going to be\nequal to the prediction",
    "start": "414140",
    "end": "417560"
  },
  {
    "text": "f of x minus the target y.",
    "start": "417560",
    "end": "421880"
  },
  {
    "text": "So that's the residual here,\nand I'm going to square it.",
    "start": "421880",
    "end": "426720"
  },
  {
    "text": "So that is called\nthe square loss.",
    "start": "426720",
    "end": "429940"
  },
  {
    "text": "So as an aside, you can also\ntake the absolute value here,",
    "start": "429940",
    "end": "433020"
  },
  {
    "text": "which gives you the\nabsolute deviation loss,",
    "start": "433020",
    "end": "434900"
  },
  {
    "text": "but we're going to stick\nwith the squared loss",
    "start": "434900",
    "end": "436817"
  },
  {
    "text": "for mathematical convenience.",
    "start": "436817",
    "end": "439879"
  },
  {
    "text": "So on these three examples\nwe can compute the loss.",
    "start": "439880",
    "end": "442890"
  },
  {
    "text": "So we take 1, 1 and\nthe weight vector.",
    "start": "442890",
    "end": "446870"
  },
  {
    "text": "We dot them together.",
    "start": "446870",
    "end": "447990"
  },
  {
    "text": "That's the prediction.",
    "start": "447990",
    "end": "449360"
  },
  {
    "text": "You subtract off the\ntarget and square it,",
    "start": "449360",
    "end": "454849"
  },
  {
    "text": "and that gives you 0.32.",
    "start": "454850",
    "end": "457530"
  },
  {
    "text": "A second example is 2, 3, and\nthe third example is 4, 3.",
    "start": "457530",
    "end": "463139"
  },
  {
    "text": "Each one gives you\na loss function",
    "start": "463140",
    "end": "465760"
  },
  {
    "text": "which corresponds to\nthe square of the length",
    "start": "465760",
    "end": "467970"
  },
  {
    "text": "of these dashed lines.",
    "start": "467970",
    "end": "472200"
  },
  {
    "text": "So now, we can define\nthe training loss",
    "start": "472200",
    "end": "475670"
  },
  {
    "text": "of a particular wave vector\nto be simply the average",
    "start": "475670",
    "end": "480260"
  },
  {
    "text": "over the losses.",
    "start": "480260",
    "end": "481530"
  },
  {
    "text": "So formally, this is going to\nbe a sum over all the examples",
    "start": "481530",
    "end": "487490"
  },
  {
    "text": "in our training set, so\nthese, and of the loss",
    "start": "487490",
    "end": "493009"
  },
  {
    "text": "function of a particular\nexample with respect",
    "start": "493010",
    "end": "495980"
  },
  {
    "text": "to that weight vector.",
    "start": "495980",
    "end": "497070"
  },
  {
    "text": "And then, finally, we're\ngoing to just divide",
    "start": "497070",
    "end": "499010"
  },
  {
    "text": "by the number of points in the\ntraining set from the line.",
    "start": "499010",
    "end": "503250"
  },
  {
    "text": "So in this example, we just\naverage these three numbers,",
    "start": "503250",
    "end": "506150"
  },
  {
    "text": "and we get 0.3.",
    "start": "506150",
    "end": "508669"
  },
  {
    "text": "OK?",
    "start": "508670",
    "end": "509170"
  },
  {
    "text": "So that is how we defined the\nsquared loss and the training",
    "start": "509170",
    "end": "513159"
  },
  {
    "text": "loss, in terms of\nthe squared loss.",
    "start": "513159",
    "end": "516750"
  },
  {
    "start": "516000",
    "end": "563000"
  },
  {
    "text": "So here is a training loss, as\nwe had from the previous slide,",
    "start": "516750",
    "end": "520409"
  },
  {
    "text": "and we can visualize this.",
    "start": "520409",
    "end": "521729"
  },
  {
    "text": "So for every single\nweight vector,",
    "start": "521730",
    "end": "523969"
  },
  {
    "text": "we can stick it in\nand get out a number.",
    "start": "523970",
    "end": "525800"
  },
  {
    "text": "So fortunately, w is only\ntwo dimensional here,",
    "start": "525800",
    "end": "528589"
  },
  {
    "text": "so we can plot this actually.",
    "start": "528590",
    "end": "530270"
  },
  {
    "text": "So here is a plot, w1, w2,\nand every point here gives you",
    "start": "530270",
    "end": "535760"
  },
  {
    "text": "a training loss on the z-axis.",
    "start": "535760",
    "end": "538440"
  },
  {
    "text": "So red here denotes high loss.",
    "start": "538440",
    "end": "541190"
  },
  {
    "text": "Blue here denotes low loss.",
    "start": "541190",
    "end": "542820"
  },
  {
    "text": "And so it's natural\nto think about how",
    "start": "542820",
    "end": "547250"
  },
  {
    "text": "you would find the point here\nwith the minimum training loss,",
    "start": "547250",
    "end": "551390"
  },
  {
    "text": "and that's captured\nmathematically as minimum",
    "start": "551390",
    "end": "554090"
  },
  {
    "text": "over w of TrainLoss of w.",
    "start": "554090",
    "end": "557600"
  },
  {
    "text": "So this is the optimization\nproblem that we want to solve.",
    "start": "557600",
    "end": "560584"
  },
  {
    "start": "560585",
    "end": "564040"
  },
  {
    "start": "563000",
    "end": "677000"
  },
  {
    "text": "So now, the third question is\nhow do you compute the best?",
    "start": "564040",
    "end": "567570"
  },
  {
    "text": "So fortunately, we already\nhave our well-defined goal.",
    "start": "567570",
    "end": "570780"
  },
  {
    "text": "We want to find the\nweight vector that",
    "start": "570780",
    "end": "572970"
  },
  {
    "text": "minimizes the training\nloss, and we're",
    "start": "572970",
    "end": "575519"
  },
  {
    "text": "going to adopt a very\nsimple strategy called",
    "start": "575520",
    "end": "577800"
  },
  {
    "text": "follow your nose.",
    "start": "577800",
    "end": "578880"
  },
  {
    "text": "OK?",
    "start": "578880",
    "end": "579390"
  },
  {
    "text": "So you start at a particular\nw, and then you sniff around,",
    "start": "579390",
    "end": "583430"
  },
  {
    "text": "and then you just move\nin a direction that",
    "start": "583430",
    "end": "585180"
  },
  {
    "text": "seems like it's going to\nreduce your loss the most.",
    "start": "585180",
    "end": "587990"
  },
  {
    "text": "More mathematically, we're\ngoing to define the gradient",
    "start": "587990",
    "end": "591779"
  },
  {
    "text": "as the direction that increases\nthe training loss the most,",
    "start": "591780",
    "end": "596460"
  },
  {
    "text": "and importantly, we want to\ngo in the opposite direction,",
    "start": "596460",
    "end": "599110"
  },
  {
    "text": "because we want to decrease the\ntraining loss, not increase it.",
    "start": "599110",
    "end": "602550"
  },
  {
    "text": "So pictorially, what the\nfollow the nose strategy,",
    "start": "602550",
    "end": "605519"
  },
  {
    "text": "or gradient descent,\nis going to look",
    "start": "605520",
    "end": "607110"
  },
  {
    "text": "like is you're going\nto start at some w,",
    "start": "607110",
    "end": "609975"
  },
  {
    "text": "and then you're going\nto follow the gradient.",
    "start": "609975",
    "end": "611850"
  },
  {
    "text": "And you're going to end\nup here, and then you're",
    "start": "611850",
    "end": "613620"
  },
  {
    "text": "going to compute the gradient.",
    "start": "613620",
    "end": "615045"
  },
  {
    "text": "You're going to end up here, and\nyou might bounce around a bit,",
    "start": "615045",
    "end": "617670"
  },
  {
    "text": "but hopefully, you'll decrease\nthe loss on average over time.",
    "start": "617670",
    "end": "623019"
  },
  {
    "text": "OK.",
    "start": "623020",
    "end": "623520"
  },
  {
    "text": "So here's the pseudocode\nfor gradient descent.",
    "start": "623520",
    "end": "626640"
  },
  {
    "text": "So we initialize\nw to be something,",
    "start": "626640",
    "end": "628850"
  },
  {
    "text": "let's say 0s for\nsimplicity, and then",
    "start": "628850",
    "end": "631699"
  },
  {
    "text": "we're going to\nrepeat big T times.",
    "start": "631700",
    "end": "633680"
  },
  {
    "text": "Big T is the number of epochs,\nand what we're going to do",
    "start": "633680",
    "end": "637670"
  },
  {
    "text": "is we're going to take our old\nvalue of the weight vector,",
    "start": "637670",
    "end": "640970"
  },
  {
    "text": "and we're going to\nsubtract out some eta which",
    "start": "640970",
    "end": "645680"
  },
  {
    "text": "is called the step size, which\nwe'll get into a little bit",
    "start": "645680",
    "end": "652050"
  },
  {
    "text": "later.",
    "start": "652050",
    "end": "652550"
  },
  {
    "text": "So here's the step size\ntimes the gradient,",
    "start": "652550",
    "end": "656540"
  },
  {
    "text": "so grad of returning loss of w.",
    "start": "656540",
    "end": "660230"
  },
  {
    "text": "So that's called\nthe gradient here.",
    "start": "660230",
    "end": "662350"
  },
  {
    "start": "662350",
    "end": "665529"
  },
  {
    "text": "OK?",
    "start": "665530",
    "end": "666030"
  },
  {
    "text": "So that's it.",
    "start": "666030",
    "end": "667410"
  },
  {
    "text": "There's three lines,\nand really only one line",
    "start": "667410",
    "end": "670319"
  },
  {
    "text": "that's of interest\nhere, and that's",
    "start": "670320",
    "end": "672840"
  },
  {
    "text": "all there is to\ngradient descent.",
    "start": "672840",
    "end": "675255"
  },
  {
    "start": "675255",
    "end": "677670"
  },
  {
    "start": "677000",
    "end": "804000"
  },
  {
    "text": "OK.",
    "start": "677670",
    "end": "678170"
  },
  {
    "text": "So at least at an\nabstract level,",
    "start": "678170",
    "end": "681829"
  },
  {
    "text": "so all that remains to do is to\nactually compute the gradient.",
    "start": "681830",
    "end": "685470"
  },
  {
    "text": "So remember, here is\nour objective function,",
    "start": "685470",
    "end": "688230"
  },
  {
    "text": "TrainLoss is the average\nover the individual losses,",
    "start": "688230",
    "end": "691220"
  },
  {
    "text": "which I've expanded the\nsquare loss right now.",
    "start": "691220",
    "end": "693620"
  },
  {
    "text": "But gradient descent is\nactually much more general",
    "start": "693620",
    "end": "696350"
  },
  {
    "text": "than just the square loss\nor even machine learning.",
    "start": "696350",
    "end": "700579"
  },
  {
    "text": "And now, we just need\nto compute the gradient,",
    "start": "700580",
    "end": "702900"
  },
  {
    "text": "and if you remember your\ncalculus, here's how you do it.",
    "start": "702900",
    "end": "707280"
  },
  {
    "text": "So the gradient with respect\nto w of the TrainLoss of w.",
    "start": "707280",
    "end": "711830"
  },
  {
    "text": "So remember, there's\na lot of symbols here,",
    "start": "711830",
    "end": "714020"
  },
  {
    "text": "but we are differentiating\nwith respect",
    "start": "714020",
    "end": "716150"
  },
  {
    "text": "to w, not x, not y, not phi.",
    "start": "716150",
    "end": "718850"
  },
  {
    "text": "And this is going to be\nequal to the gradient",
    "start": "718850",
    "end": "721519"
  },
  {
    "text": "of this expression.",
    "start": "721520",
    "end": "722720"
  },
  {
    "text": "This is just the constant.",
    "start": "722720",
    "end": "724509"
  },
  {
    "text": "So the gradient can be pushed\ninside, and this is a sum.",
    "start": "724510",
    "end": "728870"
  },
  {
    "text": "The gradient can be pushed\ninside of some linearity.",
    "start": "728870",
    "end": "732710"
  },
  {
    "text": "So this is the sum over the\ntraining set again, and then--",
    "start": "732710",
    "end": "736610"
  },
  {
    "text": "So now the interesting\nthing happens.",
    "start": "736610",
    "end": "738660"
  },
  {
    "text": "So here is something\nsquared, the gradient",
    "start": "738660",
    "end": "741470"
  },
  {
    "text": "of something squared.",
    "start": "741470",
    "end": "742550"
  },
  {
    "text": "You bring down the\n2, and then you",
    "start": "742550",
    "end": "745130"
  },
  {
    "text": "have the same something,\nwhich is, if you remember,",
    "start": "745130",
    "end": "749510"
  },
  {
    "text": "the residual times the\ngradient of what's inside here.",
    "start": "749510",
    "end": "754160"
  },
  {
    "text": "And what's inside here\nis w dot phi of x.",
    "start": "754160",
    "end": "757220"
  },
  {
    "text": "Phi of x is a constant.",
    "start": "757220",
    "end": "758279"
  },
  {
    "text": "y is a constant.",
    "start": "758280",
    "end": "759420"
  },
  {
    "text": "So the gradient of that\nresidual is just phi of x.",
    "start": "759420",
    "end": "765320"
  },
  {
    "text": "And notice that--\nthere's something",
    "start": "765320",
    "end": "767060"
  },
  {
    "text": "interesting I want\nto point out here,",
    "start": "767060",
    "end": "769110"
  },
  {
    "text": "which is that the\ngradient can be expressed",
    "start": "769110",
    "end": "772910"
  },
  {
    "text": "as the residual times a feature\nvector, where the residual is",
    "start": "772910",
    "end": "777110"
  },
  {
    "text": "the prediction minus the target.",
    "start": "777110",
    "end": "781339"
  },
  {
    "text": "So intuitively, you\ncan think about,",
    "start": "781340",
    "end": "783050"
  },
  {
    "text": "if the prediction is equal to\ntarget, then the gradient is 0.",
    "start": "783050",
    "end": "786410"
  },
  {
    "text": "So nothing will happen.",
    "start": "786410",
    "end": "788160"
  },
  {
    "text": "And if the prediction\nis not equal to target,",
    "start": "788160",
    "end": "790519"
  },
  {
    "text": "then the gradient will\nbe in the direction",
    "start": "790520",
    "end": "793910"
  },
  {
    "text": "that ascends the prediction\naway from the target.",
    "start": "793910",
    "end": "796819"
  },
  {
    "text": "And remember, we're\nalways minimizing,",
    "start": "796820",
    "end": "798650"
  },
  {
    "text": "so we're subtracting that off,\nin which we'll move the weights",
    "start": "798650",
    "end": "801740"
  },
  {
    "text": "in the right direction.",
    "start": "801740",
    "end": "804670"
  },
  {
    "start": "804000",
    "end": "924000"
  },
  {
    "text": "OK.",
    "start": "804670",
    "end": "805170"
  },
  {
    "text": "So let's walk through\nthe gradient descent",
    "start": "805170",
    "end": "807720"
  },
  {
    "text": "for our example here.",
    "start": "807720",
    "end": "809290"
  },
  {
    "text": "So here is a training\nexample, again.",
    "start": "809290",
    "end": "812310"
  },
  {
    "text": "Here is the expression\nfor the gradient",
    "start": "812310",
    "end": "814230"
  },
  {
    "text": "that we just computed\non the previous slide,",
    "start": "814230",
    "end": "816397"
  },
  {
    "text": "and here is the\ngradient update, where",
    "start": "816397",
    "end": "817980"
  },
  {
    "text": "I've taken the liberty of\njust substituting the step",
    "start": "817980",
    "end": "820147"
  },
  {
    "text": "size to be just 0.1,\njust for simplicity.",
    "start": "820147",
    "end": "823829"
  },
  {
    "text": "OK?",
    "start": "823830",
    "end": "824330"
  },
  {
    "text": "So we start with w equals 0, 0,\nand then what we're going to do",
    "start": "824330",
    "end": "829520"
  },
  {
    "text": "is plug in 0, 0 into\nthis training loss",
    "start": "829520",
    "end": "833540"
  },
  {
    "text": "expression, this somewhat\nnasty-looking thing,",
    "start": "833540",
    "end": "836660"
  },
  {
    "text": "and that is this.",
    "start": "836660",
    "end": "839350"
  },
  {
    "text": "This is just simply the average.",
    "start": "839350",
    "end": "841310"
  },
  {
    "text": "Three examples, 1 over 3,\nhere's the first example.",
    "start": "841310",
    "end": "843820"
  },
  {
    "text": "Here's a second example.",
    "start": "843820",
    "end": "844820"
  },
  {
    "text": "Here's a third example.",
    "start": "844820",
    "end": "846050"
  },
  {
    "text": "Each example consists\nof a dot product,",
    "start": "846050",
    "end": "849410"
  },
  {
    "text": "the prediction\nminus target times",
    "start": "849410",
    "end": "851810"
  },
  {
    "text": "the feature vector\nof that example.",
    "start": "851810",
    "end": "854307"
  },
  {
    "text": "So I'll let you go\nthrough the details here,",
    "start": "854307",
    "end": "856140"
  },
  {
    "text": "but if you do the math, you\nget minus 4.67, minus 12.67.",
    "start": "856140",
    "end": "861110"
  },
  {
    "text": "If you multiply by the step\nsize, and you get this weight.",
    "start": "861110",
    "end": "868990"
  },
  {
    "text": "OK.",
    "start": "868990",
    "end": "869490"
  },
  {
    "text": "So now, the second\nduration, you're",
    "start": "869490",
    "end": "870990"
  },
  {
    "text": "going to take this\nweight vector,",
    "start": "870990",
    "end": "873839"
  },
  {
    "text": "stick it into this\nexpression all over again.",
    "start": "873840",
    "end": "876780"
  },
  {
    "text": "You compute a new\ngradient, and then",
    "start": "876780",
    "end": "879510"
  },
  {
    "text": "you subtract that gradient times\n0.1 from this weight vector.",
    "start": "879510",
    "end": "884400"
  },
  {
    "text": "And you're going to\nget a new wave vector,",
    "start": "884400",
    "end": "886590"
  },
  {
    "text": "and then you keep on\nrepeating and repeating.",
    "start": "886590",
    "end": "889120"
  },
  {
    "text": "So after maybe 200\niterations, you're",
    "start": "889120",
    "end": "893490"
  },
  {
    "text": "going to end up with\nsomething like this.",
    "start": "893490",
    "end": "895899"
  },
  {
    "text": "And something\ninteresting happens.",
    "start": "895900",
    "end": "897990"
  },
  {
    "text": "If you're lucky, the gradient\nat the end will be 0.",
    "start": "897990",
    "end": "902310"
  },
  {
    "text": "So what does 0 mean?",
    "start": "902310",
    "end": "903279"
  },
  {
    "text": "So 0, if you subtract out\n0, you get the same thing.",
    "start": "903280",
    "end": "906970"
  },
  {
    "text": "So it means that gradient\ndescent has converged.",
    "start": "906970",
    "end": "909839"
  },
  {
    "text": "By subtracting off\nthe gradient, you're",
    "start": "909840",
    "end": "911580"
  },
  {
    "text": "not going to move anywhere.",
    "start": "911580",
    "end": "912750"
  },
  {
    "text": "So you might as well just\nstop, and the stopping point",
    "start": "912750",
    "end": "915300"
  },
  {
    "text": "is this weight\nvector 1, 0.57, which",
    "start": "915300",
    "end": "919320"
  },
  {
    "text": "is indeed the red classifier.",
    "start": "919320",
    "end": "921090"
  },
  {
    "start": "921090",
    "end": "924480"
  },
  {
    "start": "924000",
    "end": "1026000"
  },
  {
    "text": "So just to concretize\nthis even more,",
    "start": "924480",
    "end": "927459"
  },
  {
    "text": "let's do gradient\ndescent in Python.",
    "start": "927460",
    "end": "930720"
  },
  {
    "text": "OK, so I'm going to\npull up a terminal here.",
    "start": "930720",
    "end": "933490"
  },
  {
    "text": "So in practice, you\nprobably wouldn't",
    "start": "933490",
    "end": "935610"
  },
  {
    "text": "implement gradient\ndescent from scratch,",
    "start": "935610",
    "end": "937950"
  },
  {
    "text": "except for if you're\njust trying to learn",
    "start": "937950",
    "end": "940112"
  },
  {
    "text": "about gradient descent.",
    "start": "940112",
    "end": "941070"
  },
  {
    "text": "But for pedagogical\npurposes, let me do this.",
    "start": "941070",
    "end": "945090"
  },
  {
    "text": "OK?",
    "start": "945090",
    "end": "945870"
  },
  {
    "text": "So I'm going to do this in a\nkind of very barebones way.",
    "start": "945870",
    "end": "951520"
  },
  {
    "text": "So I'm going to use numpy\nrather than PyTorch or something",
    "start": "951520",
    "end": "957330"
  },
  {
    "text": "that can do gradients for you.",
    "start": "957330",
    "end": "960210"
  },
  {
    "text": "First, I'm going to define\nour training examples",
    "start": "960210",
    "end": "962550"
  },
  {
    "text": "as 1, 1, 2, 3, and 4, 3.",
    "start": "962550",
    "end": "967500"
  },
  {
    "text": "I believe those are\nthe training examples.",
    "start": "967500",
    "end": "969600"
  },
  {
    "text": "Let me just double check over\nhere, 1, 1, 2, 3, 4, 3, OK.",
    "start": "969600",
    "end": "974190"
  },
  {
    "text": "So now, I'll have to define\na feature vector of x",
    "start": "974190",
    "end": "978400"
  },
  {
    "text": "which is, remember, is 1, x.",
    "start": "978400",
    "end": "981190"
  },
  {
    "text": "So this is just a numpy array.",
    "start": "981190",
    "end": "985530"
  },
  {
    "text": "I'm going to initialize\nthe weight vector with,",
    "start": "985530",
    "end": "989520"
  },
  {
    "text": "let's call this\ninitialWeightVector,",
    "start": "989520",
    "end": "993030"
  },
  {
    "text": "and this is just going to be\nan all 0s vector of dimension 2",
    "start": "993030",
    "end": "997650"
  },
  {
    "text": "which is going to match\nthe dimensionality of phi.",
    "start": "997650",
    "end": "1002590"
  },
  {
    "text": "OK.",
    "start": "1002590",
    "end": "1003090"
  },
  {
    "text": "So now, I need to define\nthe training loss.",
    "start": "1003090",
    "end": "1006390"
  },
  {
    "text": "Training loss takes\na weight vector,",
    "start": "1006390",
    "end": "1008860"
  },
  {
    "text": "and I'm going to actually go\nto the previous slide here.",
    "start": "1008860",
    "end": "1012490"
  },
  {
    "text": "And it's just basically\ncopying down math",
    "start": "1012490",
    "end": "1015600"
  },
  {
    "text": "and turning it into code.",
    "start": "1015600",
    "end": "1017069"
  },
  {
    "text": "So this is 1 over the\nnumber of training examples",
    "start": "1017070",
    "end": "1023610"
  },
  {
    "text": "times the sum, and the sum is\nover for all training examples",
    "start": "1023610",
    "end": "1030540"
  },
  {
    "start": "1026000",
    "end": "1281000"
  },
  {
    "text": "xy in trainExamples.",
    "start": "1030540",
    "end": "1033060"
  },
  {
    "text": "And for each one I'm going\nto do w dot phi of x.",
    "start": "1033060",
    "end": "1039180"
  },
  {
    "text": "It's really, literally\nthe same thing, minus y.",
    "start": "1039180",
    "end": "1041790"
  },
  {
    "text": "And I'm going to take this\nexpression, the residual,",
    "start": "1041790",
    "end": "1045060"
  },
  {
    "text": "and I'm going to square it.",
    "start": "1045060",
    "end": "1047220"
  },
  {
    "text": "OK?",
    "start": "1047220",
    "end": "1047720"
  },
  {
    "text": "So let's make this\na little bit bigger.",
    "start": "1047720",
    "end": "1049740"
  },
  {
    "text": "OK.",
    "start": "1049740",
    "end": "1050240"
  },
  {
    "text": "So that's the training loss.",
    "start": "1050240",
    "end": "1053340"
  },
  {
    "text": "OK.",
    "start": "1053340",
    "end": "1053840"
  },
  {
    "text": "Now, I need to\ntake the gradient.",
    "start": "1053840",
    "end": "1055543"
  },
  {
    "text": "So I'm going to\ncheat a little bit",
    "start": "1055543",
    "end": "1056960"
  },
  {
    "text": "and just copy that\ndown here and edit it.",
    "start": "1056960",
    "end": "1059880"
  },
  {
    "text": "So the gradient of the\nTrainLoss is going to be 2 times",
    "start": "1059880",
    "end": "1066290"
  },
  {
    "text": "the residual times phi of x.",
    "start": "1066290",
    "end": "1069740"
  },
  {
    "text": "OK?",
    "start": "1069740",
    "end": "1070720"
  },
  {
    "text": "So that's it for\nthe training loss.",
    "start": "1070720",
    "end": "1073400"
  },
  {
    "text": "OK.",
    "start": "1073400",
    "end": "1073900"
  },
  {
    "text": "So now, I'm going to\nimplement gradient descent.",
    "start": "1073900",
    "end": "1078290"
  },
  {
    "text": "So I'm going to do it\nthis way, actually.",
    "start": "1078290",
    "end": "1083920"
  },
  {
    "text": "So gradient descent,\nlike I alluded to before,",
    "start": "1083920",
    "end": "1087195"
  },
  {
    "text": "is actually a general\npurpose optimization problem.",
    "start": "1087195",
    "end": "1089320"
  },
  {
    "text": "So all it needs is a\nfunction, gradient access",
    "start": "1089320",
    "end": "1093070"
  },
  {
    "text": "to that function, an\ninitial weight vector,",
    "start": "1093070",
    "end": "1095500"
  },
  {
    "text": "and it's ready to go.",
    "start": "1095500",
    "end": "1096980"
  },
  {
    "text": "OK?",
    "start": "1096980",
    "end": "1097480"
  },
  {
    "text": "So I'm going to initialize w\nto the initial weight vector,",
    "start": "1097480",
    "end": "1102730"
  },
  {
    "text": "and then I'm going\nto, let's set eta",
    "start": "1102730",
    "end": "1105220"
  },
  {
    "text": "to 0.1 for a number of\niterations t in range of,",
    "start": "1105220",
    "end": "1112690"
  },
  {
    "text": "let's just say, I don't\nknow, 500, just for fun.",
    "start": "1112690",
    "end": "1116870"
  },
  {
    "text": "I'm going to evaluate\nthe function at w.",
    "start": "1116870",
    "end": "1121682"
  },
  {
    "text": "I'm going to evaluate\nthe gradient,",
    "start": "1121682",
    "end": "1126530"
  },
  {
    "text": "and I'm just going to do the\none-line thing of subtracting",
    "start": "1126530",
    "end": "1131110"
  },
  {
    "text": "out eta times the gradient\nfrom the existing weight vector",
    "start": "1131110",
    "end": "1134500"
  },
  {
    "text": "and setting it to the\nnew weight vector.",
    "start": "1134500",
    "end": "1137420"
  },
  {
    "text": "And I'm going to\nprint out where I am,",
    "start": "1137420",
    "end": "1142230"
  },
  {
    "text": "so epoch t w equals w F\nof w equals of the value.",
    "start": "1142230",
    "end": "1150830"
  },
  {
    "text": "And let's do the\ngradient just for fun,",
    "start": "1150830",
    "end": "1152610"
  },
  {
    "text": "so grad gradient F\nequals the gradient.",
    "start": "1152610",
    "end": "1159670"
  },
  {
    "text": "OK?",
    "start": "1159670",
    "end": "1161110"
  },
  {
    "text": "OK.",
    "start": "1161110",
    "end": "1161610"
  },
  {
    "text": "So now, I just need to call\ngradient descent with--",
    "start": "1161610",
    "end": "1166170"
  },
  {
    "text": "what function am I optimizing?",
    "start": "1166170",
    "end": "1168000"
  },
  {
    "text": "The TrainLoss.",
    "start": "1168000",
    "end": "1170490"
  },
  {
    "text": "The gradient of the TrainLoss\nis the gradient of the TrainLoss",
    "start": "1170490",
    "end": "1175830"
  },
  {
    "text": "and the initial weight vector.",
    "start": "1175830",
    "end": "1178610"
  },
  {
    "text": "OK?",
    "start": "1178610",
    "end": "1180450"
  },
  {
    "text": "So that's all I have, and\nlet's actually just run",
    "start": "1180450",
    "end": "1185019"
  },
  {
    "text": "it, gradient descent.",
    "start": "1185020",
    "end": "1188010"
  },
  {
    "start": "1188010",
    "end": "1190720"
  },
  {
    "text": "So we see here that in epoch 0,\nthe weight vector is something,",
    "start": "1190720",
    "end": "1195900"
  },
  {
    "text": "and the function value is\nsomething gradient something.",
    "start": "1195900",
    "end": "1198820"
  },
  {
    "text": "And over time,\nthe function value",
    "start": "1198820",
    "end": "1201630"
  },
  {
    "text": "is going to decrease\nwhich is a good sign.",
    "start": "1201630",
    "end": "1204570"
  },
  {
    "text": "The gradient of f is going\nto start becoming more 0,",
    "start": "1204570",
    "end": "1209610"
  },
  {
    "text": "and the weight vectors are\nnow converging to 1, 0.57,",
    "start": "1209610",
    "end": "1217410"
  },
  {
    "text": "as advertised.",
    "start": "1217410",
    "end": "1219180"
  },
  {
    "text": "So I will declare\nthis program working.",
    "start": "1219180",
    "end": "1222060"
  },
  {
    "text": "Let's just, to kind of\nsummarize what we did here--",
    "start": "1222060",
    "end": "1225040"
  },
  {
    "text": "so I want to set\nthis up as follows.",
    "start": "1225040",
    "end": "1227500"
  },
  {
    "text": "So here is the\noptimization problem,",
    "start": "1227500",
    "end": "1230110"
  },
  {
    "text": "which is you have\nthe training example,",
    "start": "1230110",
    "end": "1233549"
  },
  {
    "text": "the feature vector, the\nloss, and gradient and so on,",
    "start": "1233550",
    "end": "1236400"
  },
  {
    "text": "and this is kind\nof a specification",
    "start": "1236400",
    "end": "1240120"
  },
  {
    "text": "of what the problem\nwe want to solve is.",
    "start": "1240120",
    "end": "1242710"
  },
  {
    "text": "And then, down here, we have\nthe optimization algorithm.",
    "start": "1242710",
    "end": "1248157"
  },
  {
    "text": "And we're going to be\ndoing this a few times",
    "start": "1248157",
    "end": "1249990"
  },
  {
    "text": "throughout the course,\ndrawing kind of module duals",
    "start": "1249990",
    "end": "1254160"
  },
  {
    "text": "where we can separate\nthe optimization",
    "start": "1254160",
    "end": "1256320"
  },
  {
    "text": "problem from the\noptimization algorithm.",
    "start": "1256320",
    "end": "1258299"
  },
  {
    "text": "Notice that the optimization\nalgorithm, again,",
    "start": "1258300",
    "end": "1261330"
  },
  {
    "text": "doesn't depend on anything\nrelating to machine learning",
    "start": "1261330",
    "end": "1265320"
  },
  {
    "text": "at all, and the optimization\nproblem doesn't say anything",
    "start": "1265320",
    "end": "1269039"
  },
  {
    "text": "about how you solve it.",
    "start": "1269040",
    "end": "1270520"
  },
  {
    "text": "So decoupling this what\nfrom the how I think",
    "start": "1270520",
    "end": "1273090"
  },
  {
    "text": "is a really important theme.",
    "start": "1273090",
    "end": "1276480"
  },
  {
    "text": "OK.",
    "start": "1276480",
    "end": "1276980"
  },
  {
    "text": "So that was gradient descent in\ncode, and let's summarize now.",
    "start": "1276980",
    "end": "1282530"
  },
  {
    "start": "1281000",
    "end": "1364000"
  },
  {
    "text": "So in summary, we\ntake training data.",
    "start": "1282530",
    "end": "1285290"
  },
  {
    "text": "We have a learning algorithm\nthat produces a predictor that",
    "start": "1285290",
    "end": "1288350"
  },
  {
    "text": "can produce new\npredictions on new inputs,",
    "start": "1288350",
    "end": "1291559"
  },
  {
    "text": "and there are three\ndesign decisions,",
    "start": "1291560",
    "end": "1294110"
  },
  {
    "text": "or build your own\nlearning algorithm.",
    "start": "1294110",
    "end": "1295860"
  },
  {
    "text": "Which predictions are possible?",
    "start": "1295860",
    "end": "1298309"
  },
  {
    "text": "That is a question of\nthe hypothesis class,",
    "start": "1298310",
    "end": "1300650"
  },
  {
    "text": "and we consider\nlinear functions here,",
    "start": "1300650",
    "end": "1302780"
  },
  {
    "text": "where the function is\nsimply w dot phi of x.",
    "start": "1302780",
    "end": "1307160"
  },
  {
    "text": "With a particular\nfeature map, 1, x,",
    "start": "1307160",
    "end": "1312350"
  },
  {
    "text": "you can imagine other things.",
    "start": "1312350",
    "end": "1313580"
  },
  {
    "text": "We'll see other things later,\nnonlinear features and even",
    "start": "1313580",
    "end": "1316789"
  },
  {
    "text": "neural networks, but it's\nstill the question of what",
    "start": "1316790",
    "end": "1319310"
  },
  {
    "text": "is the hypothesis class?",
    "start": "1319310",
    "end": "1321630"
  },
  {
    "text": "How good is a predictor?",
    "start": "1321630",
    "end": "1323460"
  },
  {
    "text": "That's a question of what\nis the loss function.",
    "start": "1323460",
    "end": "1325770"
  },
  {
    "text": "For regression, we looked\nat the squared loss.",
    "start": "1325770",
    "end": "1328507"
  },
  {
    "text": "Later, for classification,\nwe're going",
    "start": "1328507",
    "end": "1330090"
  },
  {
    "text": "to look at the hinge loss\nand the zero-one loss,",
    "start": "1330090",
    "end": "1333900"
  },
  {
    "text": "but this is orthogonal.",
    "start": "1333900",
    "end": "1335208"
  },
  {
    "text": "For neural networks,\nwe can also look",
    "start": "1335208",
    "end": "1336750"
  },
  {
    "text": "at the hinge loss\nor the square loss",
    "start": "1336750",
    "end": "1338460"
  },
  {
    "text": "or any of the other losses.",
    "start": "1338460",
    "end": "1341080"
  },
  {
    "text": "And finally, how do we\ncompute the best predictor?",
    "start": "1341080",
    "end": "1344100"
  },
  {
    "text": "This is a question of what is\nthe optimization algorithm?",
    "start": "1344100",
    "end": "1347280"
  },
  {
    "text": "And for this, we\nintroduce gradient descent",
    "start": "1347280",
    "end": "1349960"
  },
  {
    "text": "which is this lovely, simple,\nand very effective algorithm",
    "start": "1349960",
    "end": "1354720"
  },
  {
    "text": "for our optimization.",
    "start": "1354720",
    "end": "1356740"
  },
  {
    "text": "So that concludes this module.",
    "start": "1356740",
    "end": "1359130"
  },
  {
    "start": "1359130",
    "end": "1364000"
  }
]