[
  {
    "start": "0",
    "end": "4713"
  },
  {
    "text": "SPEAKER 1: Welcome\nback, everyone.",
    "start": "4713",
    "end": "6130"
  },
  {
    "text": "This is part three in our\nseries on in-context learning.",
    "start": "6130",
    "end": "8850"
  },
  {
    "text": "I've called this part\nthe current moment.",
    "start": "8850",
    "end": "10620"
  },
  {
    "text": "That is either foolish\nor foolhardy or both.",
    "start": "10620",
    "end": "13552"
  },
  {
    "text": "The current moment\nis surely going",
    "start": "13553",
    "end": "14970"
  },
  {
    "text": "to change very fast\nas the field changes.",
    "start": "14970",
    "end": "17820"
  },
  {
    "text": "I think I can say\nthat the lessons here",
    "start": "17820",
    "end": "20070"
  },
  {
    "text": "will be useful no matter what\ndirection the field takes next.",
    "start": "20070",
    "end": "24189"
  },
  {
    "text": "As always, I want\nto start with data,",
    "start": "24190",
    "end": "26280"
  },
  {
    "text": "data used for self-supervision.",
    "start": "26280",
    "end": "28010"
  },
  {
    "text": "This is an incredibly\nimportant ingredient",
    "start": "28010",
    "end": "29760"
  },
  {
    "text": "when it comes to\nunderstanding the behaviors",
    "start": "29760",
    "end": "32490"
  },
  {
    "text": "of our large language models.",
    "start": "32490",
    "end": "34500"
  },
  {
    "text": "This is a slide that I used\nin a previous Screencast,",
    "start": "34500",
    "end": "37110"
  },
  {
    "text": "but I augmented it with the\nColossal Clean Crawled Corpus",
    "start": "37110",
    "end": "40260"
  },
  {
    "text": "C4.",
    "start": "40260",
    "end": "41160"
  },
  {
    "text": "This is a data set\nthat was created",
    "start": "41160",
    "end": "43050"
  },
  {
    "text": "as part of the T5\nmodeling effort",
    "start": "43050",
    "end": "45360"
  },
  {
    "text": "and it is audited by\nDodge et al, 2021.",
    "start": "45360",
    "end": "49020"
  },
  {
    "text": "And as a sort of\ninteresting side note,",
    "start": "49020",
    "end": "51000"
  },
  {
    "text": "the Washington\nPost did an article",
    "start": "51000",
    "end": "52560"
  },
  {
    "text": "that is essentially about the\ndata set and the auditing work",
    "start": "52560",
    "end": "55410"
  },
  {
    "text": "that Dodge et al did.",
    "start": "55410",
    "end": "56640"
  },
  {
    "text": "They called that article\ninside the secret list",
    "start": "56640",
    "end": "59160"
  },
  {
    "text": "of websites that make AI\nlike ChatGPT sound smart.",
    "start": "59160",
    "end": "63239"
  },
  {
    "text": "I'm not sure secret\nis appropriate here",
    "start": "63240",
    "end": "65037"
  },
  {
    "text": "because it seems like everyone\nis being pretty open about what",
    "start": "65037",
    "end": "67620"
  },
  {
    "text": "is in C4.",
    "start": "67620",
    "end": "69030"
  },
  {
    "text": "But nonetheless, the article\nis very useful in terms",
    "start": "69030",
    "end": "71580"
  },
  {
    "text": "of helping you, people like\nus, audit what was in data",
    "start": "71580",
    "end": "75240"
  },
  {
    "text": "sets like that.",
    "start": "75240",
    "end": "76530"
  },
  {
    "text": "And undoubtedly,\nthe data that are",
    "start": "76530",
    "end": "79409"
  },
  {
    "text": "used for unsupervised\npre-training",
    "start": "79410",
    "end": "81330"
  },
  {
    "text": "are an incredibly\nimportant ingredient",
    "start": "81330",
    "end": "83040"
  },
  {
    "text": "when it comes to understanding\nwhat our models can do",
    "start": "83040",
    "end": "86140"
  },
  {
    "text": "and where they're limited.",
    "start": "86140",
    "end": "88340"
  },
  {
    "text": "But as I mentioned at the end\nof the previous Screencast,",
    "start": "88340",
    "end": "91359"
  },
  {
    "text": "this is no longer\nthe only ingredient.",
    "start": "91360",
    "end": "93370"
  },
  {
    "text": "We have left the era when all of\nthe language model pre-training",
    "start": "93370",
    "end": "97600"
  },
  {
    "text": "was simply unsupervised\nlanguage model pre-training.",
    "start": "97600",
    "end": "100689"
  },
  {
    "text": "We have now entered into the\nera of instruct fine tuning.",
    "start": "100690",
    "end": "105430"
  },
  {
    "text": "Unfortunately, we\nknow much less about",
    "start": "105430",
    "end": "107560"
  },
  {
    "text": "what is happening with\ninstruct fine tuning.",
    "start": "107560",
    "end": "109720"
  },
  {
    "text": "We don't really know what\nthe large industrial labs",
    "start": "109720",
    "end": "113200"
  },
  {
    "text": "are doing in terms of\ndata and protocols here.",
    "start": "113200",
    "end": "116439"
  },
  {
    "text": "We can infer that they\nare paying lots of people",
    "start": "116440",
    "end": "119440"
  },
  {
    "text": "to generate instruct\ndata and that",
    "start": "119440",
    "end": "121810"
  },
  {
    "text": "means that very often,\nthese people are doing",
    "start": "121810",
    "end": "123909"
  },
  {
    "text": "quite sophisticated things.",
    "start": "123910",
    "end": "125330"
  },
  {
    "text": "For example, I\nthink people might",
    "start": "125330",
    "end": "126850"
  },
  {
    "text": "be prompted with a\ntext that says, write",
    "start": "126850",
    "end": "129310"
  },
  {
    "text": "a certain kind of Python\nprogram, and then a human",
    "start": "129310",
    "end": "132250"
  },
  {
    "text": "actually writes\nthat Python program.",
    "start": "132250",
    "end": "134200"
  },
  {
    "text": "And that's just one instance of\nmany kinds of domains and areas",
    "start": "134200",
    "end": "138190"
  },
  {
    "text": "of expertise where they\nhave recruited people",
    "start": "138190",
    "end": "140530"
  },
  {
    "text": "to exemplify the\ndesired behavior.",
    "start": "140530",
    "end": "142910"
  },
  {
    "text": "Again, a reminder that the\nreally sophisticated things",
    "start": "142910",
    "end": "146260"
  },
  {
    "text": "that we're seeing from\nlanguage models these days",
    "start": "146260",
    "end": "149049"
  },
  {
    "text": "are not emerging\nin some magical way",
    "start": "149050",
    "end": "151270"
  },
  {
    "text": "from unsupervised\npre-training but rather",
    "start": "151270",
    "end": "154150"
  },
  {
    "text": "emerging very directly from\nstandard, good old fashioned",
    "start": "154150",
    "end": "158200"
  },
  {
    "text": "supervision.",
    "start": "158200",
    "end": "160300"
  },
  {
    "text": "I think we can also infer that\nthese large industrial labs are",
    "start": "160300",
    "end": "163450"
  },
  {
    "text": "using their own\nmodels to generate",
    "start": "163450",
    "end": "165440"
  },
  {
    "text": "examples and adjudicate\nbetween examples.",
    "start": "165440",
    "end": "167803"
  },
  {
    "text": "And in fact, we're\ngoing to review",
    "start": "167803",
    "end": "169220"
  },
  {
    "text": "a method along those\nlines, self-instruct,",
    "start": "169220",
    "end": "172160"
  },
  {
    "text": "in just a second.",
    "start": "172160",
    "end": "174480"
  },
  {
    "text": "If you would like to get a\nfeel for what instruct fine",
    "start": "174480",
    "end": "177050"
  },
  {
    "text": "tuning is like,\nI would encourage",
    "start": "177050",
    "end": "178880"
  },
  {
    "text": "you to check out the Stanford\nHuman Preferences Dataset.",
    "start": "178880",
    "end": "181730"
  },
  {
    "text": "This is a new release\non instruct fine tuning",
    "start": "181730",
    "end": "184400"
  },
  {
    "text": "data set that was derived\nfrom Reddit posts.",
    "start": "184400",
    "end": "188629"
  },
  {
    "text": "And so you could use that,\nmaybe using parts of it",
    "start": "188630",
    "end": "191450"
  },
  {
    "text": "or different protocols\nfor fine tuning",
    "start": "191450",
    "end": "193069"
  },
  {
    "text": "to get a feel for how instruct\ndata affects model behaviors",
    "start": "193070",
    "end": "198050"
  },
  {
    "text": "and that could be\nquite illuminating.",
    "start": "198050",
    "end": "201050"
  },
  {
    "text": "I mentioned\nself-instruct before, I",
    "start": "201050",
    "end": "202790"
  },
  {
    "text": "think this is a powerful method\nthat points to lots of new ways",
    "start": "202790",
    "end": "205790"
  },
  {
    "text": "in which we could use models\nto make models better.",
    "start": "205790",
    "end": "209209"
  },
  {
    "text": "For self-instruct, we\nbegin from 175 tasks",
    "start": "209210",
    "end": "213500"
  },
  {
    "text": "that were written by humans.",
    "start": "213500",
    "end": "215390"
  },
  {
    "text": "Those go into a\ntask pool and then",
    "start": "215390",
    "end": "217970"
  },
  {
    "text": "we have a language model\ngenerate some new instructions",
    "start": "217970",
    "end": "221030"
  },
  {
    "text": "via in-context learning.",
    "start": "221030",
    "end": "223400"
  },
  {
    "text": "The generated\ninstruction is then",
    "start": "223400",
    "end": "225230"
  },
  {
    "text": "fed back into that\nsame language model",
    "start": "225230",
    "end": "227360"
  },
  {
    "text": "with a new kind of prompt\nthat helps the model decide",
    "start": "227360",
    "end": "230570"
  },
  {
    "text": "whether the instruction\nis a classification task",
    "start": "230570",
    "end": "233300"
  },
  {
    "text": "or some other kind of task.",
    "start": "233300",
    "end": "235100"
  },
  {
    "text": "Depending on the generated\nresponse at step two,",
    "start": "235100",
    "end": "237800"
  },
  {
    "text": "we feed the generated\noutput into one",
    "start": "237800",
    "end": "240590"
  },
  {
    "text": "or another of these\ntwo prompts down here.",
    "start": "240590",
    "end": "242930"
  },
  {
    "text": "And that step gives us\nnew input/output pairs",
    "start": "242930",
    "end": "246980"
  },
  {
    "text": "that we can use for subsequent\nsupervised language model",
    "start": "246980",
    "end": "251540"
  },
  {
    "text": "pre-training.",
    "start": "251540",
    "end": "252769"
  },
  {
    "text": "There are some filtering\nthere for quality",
    "start": "252770",
    "end": "254840"
  },
  {
    "text": "and to make sure the\ndata set is diverse,",
    "start": "254840",
    "end": "256669"
  },
  {
    "text": "but then those generated\ninstructions go back",
    "start": "256670",
    "end": "259310"
  },
  {
    "text": "into the task pool\nand can participate",
    "start": "259310",
    "end": "261799"
  },
  {
    "text": "in parts of these prompts\nto generate more data.",
    "start": "261800",
    "end": "265909"
  },
  {
    "text": "And in that way, we can\nuse a language model",
    "start": "265910",
    "end": "268430"
  },
  {
    "text": "to bootstrap a new\ndata set and then",
    "start": "268430",
    "end": "271160"
  },
  {
    "text": "we can update that very\nsame language model",
    "start": "271160",
    "end": "273620"
  },
  {
    "text": "with the new data\nset in the hopes that",
    "start": "273620",
    "end": "276080"
  },
  {
    "text": "will lead it to have more\nand more diverse abilities.",
    "start": "276080",
    "end": "280220"
  },
  {
    "text": "That was kind of\nabstract, so let's walk",
    "start": "280220",
    "end": "282140"
  },
  {
    "text": "through how\nself-instruct happens",
    "start": "282140",
    "end": "283730"
  },
  {
    "text": "at the level of the\nprompts that they use.",
    "start": "283730",
    "end": "285800"
  },
  {
    "text": "So at step 1, we have\ninstruction generation.",
    "start": "285800",
    "end": "288229"
  },
  {
    "text": "This is the prompt.",
    "start": "288230",
    "end": "289220"
  },
  {
    "text": "You can see the model is\ngiven 8 demonstrations",
    "start": "289220",
    "end": "292070"
  },
  {
    "text": "and then asked to generate\na new instruction.",
    "start": "292070",
    "end": "294770"
  },
  {
    "text": "The majority of\nthese demonstrations",
    "start": "294770",
    "end": "296750"
  },
  {
    "text": "were human created, but\nin subsequent rounds",
    "start": "296750",
    "end": "299180"
  },
  {
    "text": "of self-instruct, some\nof them were actually",
    "start": "299180",
    "end": "302000"
  },
  {
    "text": "model generated instructions.",
    "start": "302000",
    "end": "304640"
  },
  {
    "text": "At step 2, we have\nclassification task",
    "start": "304640",
    "end": "306890"
  },
  {
    "text": "identification.",
    "start": "306890",
    "end": "307760"
  },
  {
    "text": "The generated response from\nstep 1 is fed into this prompt",
    "start": "307760",
    "end": "311060"
  },
  {
    "text": "and the model learns in context\nto predict whether or not",
    "start": "311060",
    "end": "314630"
  },
  {
    "text": "it was a classification task.",
    "start": "314630",
    "end": "316550"
  },
  {
    "text": "Depending on the\ngenerated response there,",
    "start": "316550",
    "end": "318830"
  },
  {
    "text": "we either feed it into\na classification task",
    "start": "318830",
    "end": "321680"
  },
  {
    "text": "prompt or a non\nclassification task prompt.",
    "start": "321680",
    "end": "324530"
  },
  {
    "text": "And the results here give\nus new input/output pairs",
    "start": "324530",
    "end": "328380"
  },
  {
    "text": "that we can use to augment\nour self-instruct data set.",
    "start": "328380",
    "end": "331660"
  },
  {
    "text": "And then as I said, we do\nsubsequent language model,",
    "start": "331660",
    "end": "334560"
  },
  {
    "text": "supervised pre-training\nin the standard way",
    "start": "334560",
    "end": "337650"
  },
  {
    "text": "or maybe with some\nother techniques",
    "start": "337650",
    "end": "339479"
  },
  {
    "text": "to update the model that\nwas used for this generation",
    "start": "339480",
    "end": "342210"
  },
  {
    "text": "process.",
    "start": "342210",
    "end": "343690"
  },
  {
    "text": "That's self-instruct.",
    "start": "343690",
    "end": "344710"
  },
  {
    "text": "Self-instruct was a major\nmechanism behind Alpaca.",
    "start": "344710",
    "end": "348250"
  },
  {
    "text": "Alpaca was an important\nrecent moment for the field",
    "start": "348250",
    "end": "350950"
  },
  {
    "text": "because it started\nto show people",
    "start": "350950",
    "end": "352480"
  },
  {
    "text": "that we could via\nself-instruct methods",
    "start": "352480",
    "end": "355330"
  },
  {
    "text": "take relatively small\nmodels like a 7 billion",
    "start": "355330",
    "end": "358689"
  },
  {
    "text": "parameter model, do some\ninstruct fine tuning,",
    "start": "358690",
    "end": "361420"
  },
  {
    "text": "and get a very capable\nresult as the output.",
    "start": "361420",
    "end": "364810"
  },
  {
    "text": "In more detail, the\nway Alpaca works",
    "start": "364810",
    "end": "366880"
  },
  {
    "text": "is we begin with a Llama model.",
    "start": "366880",
    "end": "369070"
  },
  {
    "text": "Llama is a class of models\nthat was released recently",
    "start": "369070",
    "end": "371800"
  },
  {
    "text": "by Meta AI.",
    "start": "371800",
    "end": "373900"
  },
  {
    "text": "The Alpaca team began\nfrom the 175 tasks that",
    "start": "373900",
    "end": "377889"
  },
  {
    "text": "were written by humans for\nthe self-instruct paper",
    "start": "377890",
    "end": "380710"
  },
  {
    "text": "and then they\nfollow self-instruct",
    "start": "380710",
    "end": "382569"
  },
  {
    "text": "with some miner simplifications\nusing Text DaVinci",
    "start": "382570",
    "end": "385810"
  },
  {
    "text": "3 as the engine to create\nthe new input/output pairs.",
    "start": "385810",
    "end": "389380"
  },
  {
    "text": "That gives them a data set\nultimately of 52,000 examples",
    "start": "389380",
    "end": "393610"
  },
  {
    "text": "and those examples were\nused to update the Llama",
    "start": "393610",
    "end": "396550"
  },
  {
    "text": "model to create Alpaca.",
    "start": "396550",
    "end": "398409"
  },
  {
    "text": "And the observation is that\nthe results of this relatively",
    "start": "398410",
    "end": "401470"
  },
  {
    "text": "small scale effort to\nupdate the Llama model",
    "start": "401470",
    "end": "404630"
  },
  {
    "text": "are actually quite\npowerful in terms",
    "start": "404630",
    "end": "406520"
  },
  {
    "text": "of imbuing Alpaca with new\ninstruct following behaviors.",
    "start": "406520",
    "end": "411030"
  },
  {
    "text": "And again, there's a major\nlesson there about technology.",
    "start": "411030",
    "end": "414442"
  },
  {
    "text": "And I think this is an exciting\nnew direction for the field",
    "start": "414442",
    "end": "416900"
  },
  {
    "text": "as we think about making these\nrelatively small models ever",
    "start": "416900",
    "end": "420259"
  },
  {
    "text": "more performant.",
    "start": "420260",
    "end": "421460"
  },
  {
    "text": "And there is also\na lesson for you",
    "start": "421460",
    "end": "423073"
  },
  {
    "text": "about what's going\nto be effective",
    "start": "423073",
    "end": "424490"
  },
  {
    "text": "for in-context learning because\nobviously, to the extent",
    "start": "424490",
    "end": "427370"
  },
  {
    "text": "that you can tune\nyour own prompts",
    "start": "427370",
    "end": "429979"
  },
  {
    "text": "to align with the\ninstruction fine tuning data",
    "start": "429980",
    "end": "432350"
  },
  {
    "text": "that models like\nAlpaca have seen,",
    "start": "432350",
    "end": "434390"
  },
  {
    "text": "you will be more successful.",
    "start": "434390",
    "end": "435830"
  },
  {
    "text": "And that lesson generalizes\nto all these large language",
    "start": "435830",
    "end": "438409"
  },
  {
    "text": "models.",
    "start": "438410",
    "end": "439310"
  },
  {
    "text": "For some, we have visibility\ninto the instruct fine tuning",
    "start": "439310",
    "end": "442070"
  },
  {
    "text": "data as with Alpaca, but for\nthe largest ones, we don't.",
    "start": "442070",
    "end": "445560"
  },
  {
    "text": "And so people have to\norganically discover which",
    "start": "445560",
    "end": "448790"
  },
  {
    "text": "prompting techniques\nwork, which is really",
    "start": "448790",
    "end": "450680"
  },
  {
    "text": "a process of\nuncovering, I believe,",
    "start": "450680",
    "end": "452630"
  },
  {
    "text": "what their instruct fine tuning\nphase was like at this point.",
    "start": "452630",
    "end": "457520"
  },
  {
    "text": "Alpaca, as I said, was\nexciting because it",
    "start": "457520",
    "end": "460220"
  },
  {
    "text": "bucked the trend of model\nsizes going up, up, up.",
    "start": "460220",
    "end": "463430"
  },
  {
    "text": "This is a slide that I\nused in the intro lecture",
    "start": "463430",
    "end": "466070"
  },
  {
    "text": "for the course.",
    "start": "466070",
    "end": "466890"
  },
  {
    "text": "We got all the way up to PaLM\nat 540 billion parameters.",
    "start": "466890",
    "end": "470330"
  },
  {
    "text": "It may be that GPT-4 is\nsubstantially larger even",
    "start": "470330",
    "end": "473960"
  },
  {
    "text": "than that.",
    "start": "473960",
    "end": "474979"
  },
  {
    "text": "As a result of this kind\nof instruct fine tuning,",
    "start": "474980",
    "end": "478520"
  },
  {
    "text": "we're starting to see\nthat model sizes might",
    "start": "478520",
    "end": "480740"
  },
  {
    "text": "come down and nonetheless\nbe very performant.",
    "start": "480740",
    "end": "484520"
  },
  {
    "text": "And that is incredibly exciting\nbecause it's going to happen.",
    "start": "484520",
    "end": "488009"
  },
  {
    "text": "There are lots of incentives,\nintellectual, technological,",
    "start": "488010",
    "end": "490760"
  },
  {
    "text": "financial for us to\nfind a way to have",
    "start": "490760",
    "end": "493130"
  },
  {
    "text": "smaller models be performant.",
    "start": "493130",
    "end": "494870"
  },
  {
    "text": "And I think that will be an\nimportant step toward actually",
    "start": "494870",
    "end": "497570"
  },
  {
    "text": "truly democratizing\naccess to large language",
    "start": "497570",
    "end": "500960"
  },
  {
    "text": "models and the capabilities\nthat they can enable.",
    "start": "500960",
    "end": "504460"
  },
  {
    "start": "504460",
    "end": "509000"
  }
]