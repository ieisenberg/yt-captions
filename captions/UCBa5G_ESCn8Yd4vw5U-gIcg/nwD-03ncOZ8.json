[
  {
    "start": "0",
    "end": "1230"
  },
  {
    "text": "OK.",
    "start": "1230",
    "end": "1800"
  },
  {
    "text": "Well, we can have\nanother R session.",
    "start": "1800",
    "end": "4109"
  },
  {
    "text": "This time, we're going to\nlook at validation methods,",
    "start": "4110",
    "end": "6810"
  },
  {
    "text": "cross validation, and we'll\nstart off with leave one",
    "start": "6810",
    "end": "11400"
  },
  {
    "text": "out cross validation.",
    "start": "11400",
    "end": "12790"
  },
  {
    "text": "So we'll go to our\nRStudio session",
    "start": "12790",
    "end": "15930"
  },
  {
    "text": "and see what we can\ndo with validation.",
    "start": "15930",
    "end": "19860"
  },
  {
    "text": "So here we are.",
    "start": "19860",
    "end": "22950"
  },
  {
    "text": "First of all, require ISLR.",
    "start": "22950",
    "end": "24750"
  },
  {
    "text": "This is our data\nfor our session.",
    "start": "24750",
    "end": "27380"
  },
  {
    "text": "And this time, we\nrequire the boot package.",
    "start": "27380",
    "end": "30662"
  },
  {
    "start": "30663",
    "end": "33630"
  },
  {
    "text": "And so require is just\nlike using library.",
    "start": "33630",
    "end": "38010"
  },
  {
    "text": "It will also return\na true and false",
    "start": "38010",
    "end": "39809"
  },
  {
    "text": "if the package doesn't exist.",
    "start": "39810",
    "end": "42600"
  },
  {
    "text": "So let's look at\nthe function CV.GLM.",
    "start": "42600",
    "end": "46940"
  },
  {
    "text": "So that's a general cross\nvalidation package for GLMs,",
    "start": "46940",
    "end": "50829"
  },
  {
    "text": "and it'll give you\nsome help on that.",
    "start": "50830",
    "end": "53180"
  },
  {
    "text": "And so again, it's always\nuseful before you use a package",
    "start": "53180",
    "end": "56650"
  },
  {
    "text": "or use a function, to\nlook at the help file",
    "start": "56650",
    "end": "58930"
  },
  {
    "text": "and make sure you're\nusing it correctly.",
    "start": "58930",
    "end": "62710"
  },
  {
    "text": "So we're going to\nuse the auto data",
    "start": "62710",
    "end": "64878"
  },
  {
    "text": "and in particular, we\nlook at two variables:",
    "start": "64879",
    "end": "67060"
  },
  {
    "text": "miles per gallon and horsepower.",
    "start": "67060",
    "end": "69670"
  },
  {
    "text": "So we'll make a\nplot of those data.",
    "start": "69670",
    "end": "71979"
  },
  {
    "text": "And plot takes a formula so\nyou can use a formula in plot",
    "start": "71980",
    "end": "76000"
  },
  {
    "text": "and tell it where to evaluate\nthe formula in the data set",
    "start": "76000",
    "end": "79090"
  },
  {
    "text": "auto.",
    "start": "79090",
    "end": "79899"
  },
  {
    "text": "So we make that plot and\nwe see, as we might expect,",
    "start": "79900",
    "end": "83290"
  },
  {
    "text": "miles per gallon drops\ndown quite substantially",
    "start": "83290",
    "end": "87940"
  },
  {
    "text": "as horsepower increases.",
    "start": "87940",
    "end": "91090"
  },
  {
    "text": "And now we're going\nto investigate--",
    "start": "91090",
    "end": "95270"
  },
  {
    "text": "we're going to use these\ndata set to investigate",
    "start": "95270",
    "end": "97539"
  },
  {
    "text": "cross-validation.",
    "start": "97540",
    "end": "99400"
  },
  {
    "text": "So the first thing we'll do is\nleave one out cross validation.",
    "start": "99400",
    "end": "103660"
  },
  {
    "text": "And so we'll fit a linear model\nand we'll use GLM to fit this,",
    "start": "103660",
    "end": "110530"
  },
  {
    "text": "even though we just\nfit in a linear model.",
    "start": "110530",
    "end": "112729"
  },
  {
    "text": "So GLM can fit nonlinear\nmodels as well.",
    "start": "112730",
    "end": "115870"
  },
  {
    "text": "In particular, logistic\nregression models,",
    "start": "115870",
    "end": "118600"
  },
  {
    "text": "but it will also\nfit linear models.",
    "start": "118600",
    "end": "120500"
  },
  {
    "text": "And so if you don't give the\nfamily to GLM by default,",
    "start": "120500",
    "end": "123320"
  },
  {
    "text": "it just fits a linear model.",
    "start": "123320",
    "end": "126159"
  },
  {
    "text": "And then we'll run CV.GLM\non that linear model.",
    "start": "126160",
    "end": "130490"
  },
  {
    "text": "And now just to remind you what\nleave one out cross validation",
    "start": "130490",
    "end": "138950"
  },
  {
    "text": "does, it fits the\nmodel repeatedly",
    "start": "138950",
    "end": "143180"
  },
  {
    "text": "n times if there's\nn observations.",
    "start": "143180",
    "end": "145459"
  },
  {
    "text": "Each time, it leaves\nout one observation,",
    "start": "145460",
    "end": "149480"
  },
  {
    "text": "produces the fit on\nall the other data,",
    "start": "149480",
    "end": "152730"
  },
  {
    "text": "and then makes a prediction at\nthe x value for that observation",
    "start": "152730",
    "end": "156390"
  },
  {
    "text": "that you left out.",
    "start": "156390",
    "end": "158860"
  },
  {
    "text": "And so CV.GLM actually\ndoes that by brute force.",
    "start": "158860",
    "end": "165810"
  },
  {
    "text": "Actually refits the\nmodel all those times.",
    "start": "165810",
    "end": "169709"
  },
  {
    "text": "It's a little slow.",
    "start": "169710",
    "end": "170730"
  },
  {
    "text": "You may have noticed\nit took a while",
    "start": "170730",
    "end": "172230"
  },
  {
    "text": "before the results came up.",
    "start": "172230",
    "end": "174030"
  },
  {
    "text": "And eventually, it came up\nand it produced two numbers.",
    "start": "174030",
    "end": "177940"
  },
  {
    "text": "We see them on the screen here.",
    "start": "177940",
    "end": "179393"
  },
  {
    "text": "Well, it produced\nquite a lot, actually,",
    "start": "179393",
    "end": "181060"
  },
  {
    "text": "but we just looked\nat the delta, which",
    "start": "181060",
    "end": "182940"
  },
  {
    "text": "is the cross-validated\nprediction error.",
    "start": "182940",
    "end": "185820"
  },
  {
    "text": "And even there, it\ngives two numbers",
    "start": "185820",
    "end": "187618"
  },
  {
    "text": "and if you look on the\nhelp file, you'll see why.",
    "start": "187618",
    "end": "189660"
  },
  {
    "text": "The first number is the\nraw leave one out or LOO",
    "start": "189660",
    "end": "194000"
  },
  {
    "text": "cross-validation result,\nand the second one",
    "start": "194000",
    "end": "196800"
  },
  {
    "text": "is a bias corrected\nversion of it.",
    "start": "196800",
    "end": "198910"
  },
  {
    "text": "And the bias correction\nis to do with the fact",
    "start": "198910",
    "end": "201090"
  },
  {
    "text": "that the data set\nthat we train it on",
    "start": "201090",
    "end": "204030"
  },
  {
    "text": "is slightly smaller than\nthe one that we actually",
    "start": "204030",
    "end": "208327"
  },
  {
    "text": "would like to get\nthe error for, which",
    "start": "208327",
    "end": "209909"
  },
  {
    "text": "is the full data set of size n.",
    "start": "209910",
    "end": "212040"
  },
  {
    "text": "It turns out that has more\nof an effect for k fold cross",
    "start": "212040",
    "end": "214950"
  },
  {
    "text": "validation.",
    "start": "214950",
    "end": "216569"
  },
  {
    "text": "Now, the thing is for leave\none out, cross validation",
    "start": "216570",
    "end": "220560"
  },
  {
    "text": "and for linear\nmodels, this function",
    "start": "220560",
    "end": "223920"
  },
  {
    "text": "doesn't exploit the nice simple\nformula we saw in the chapter.",
    "start": "223920",
    "end": "228790"
  },
  {
    "text": "So let me just remind you what\nthat nice simple formula is.",
    "start": "228790",
    "end": "234269"
  },
  {
    "text": "And it goes like follows.",
    "start": "234270",
    "end": "237240"
  },
  {
    "text": "We want the misclassification\nerror for each observation yi",
    "start": "237240",
    "end": "245520"
  },
  {
    "text": "minus--",
    "start": "245520",
    "end": "246660"
  },
  {
    "text": "I'll write y hat i minus i.",
    "start": "246660",
    "end": "252140"
  },
  {
    "text": "So this is what we'd\nlike to compute.",
    "start": "252140",
    "end": "254390"
  },
  {
    "text": "And we call that the leave\none out misclassification--",
    "start": "254390",
    "end": "258109"
  },
  {
    "text": "Sorry, the leave one out\nsum of squared errors.",
    "start": "258110",
    "end": "261528"
  },
  {
    "text": "And this notation here, y\nhat minus i what it means",
    "start": "261529",
    "end": "265460"
  },
  {
    "text": "is just what we said.",
    "start": "265460",
    "end": "266660"
  },
  {
    "text": "For each observation, the Ith\nobservation, you leave it out,",
    "start": "266660",
    "end": "271900"
  },
  {
    "text": "you compute the fit\nusing all the other data",
    "start": "271900",
    "end": "274150"
  },
  {
    "text": "and then you make a\nprediction at that point.",
    "start": "274150",
    "end": "276410"
  },
  {
    "text": "So that's what this\nnotation refers to.",
    "start": "276410",
    "end": "279100"
  },
  {
    "text": "And we have this\nreally nice formula",
    "start": "279100",
    "end": "281140"
  },
  {
    "text": "that says that this is\nequal to 1 over n summation.",
    "start": "281140",
    "end": "287530"
  },
  {
    "text": "I'll make this explicit here.",
    "start": "287530",
    "end": "288950"
  },
  {
    "text": "I going from 1 to n.",
    "start": "288950",
    "end": "290770"
  },
  {
    "text": "It's the ordinary\nresiduals, which I'll just",
    "start": "290770",
    "end": "294280"
  },
  {
    "text": "write as y hat squared, OK?",
    "start": "294280",
    "end": "301020"
  },
  {
    "text": "So these would be the\nordinary residuals",
    "start": "301020",
    "end": "304009"
  },
  {
    "text": "if you didn't leave\nthe observations out.",
    "start": "304010",
    "end": "305850"
  },
  {
    "text": "So that just comes from\nthe least squares fit.",
    "start": "305850",
    "end": "308450"
  },
  {
    "text": "But now, we have to divide\nthem by 1 minus HII squared.",
    "start": "308450",
    "end": "318700"
  },
  {
    "text": "And so this is like\na magic formula.",
    "start": "318700",
    "end": "321290"
  },
  {
    "text": "The HII that we have there\nis the diagonal element",
    "start": "321290",
    "end": "326320"
  },
  {
    "text": "of the hat matrix.",
    "start": "326320",
    "end": "328550"
  },
  {
    "text": "The hat matrix is a\noperator matrix that",
    "start": "328550",
    "end": "330860"
  },
  {
    "text": "produces the least squares fit.",
    "start": "330860",
    "end": "332610"
  },
  {
    "text": "This is also known as\nthe self influence.",
    "start": "332610",
    "end": "334939"
  },
  {
    "text": "It's a measure of how much\nobservation i contributes",
    "start": "334940",
    "end": "338690"
  },
  {
    "text": "to its own fit.",
    "start": "338690",
    "end": "340040"
  },
  {
    "text": "And you notice what happens\nif this value-- these values,",
    "start": "340040",
    "end": "343480"
  },
  {
    "text": "HII vary between 0 and 1.",
    "start": "343480",
    "end": "346740"
  },
  {
    "text": "And if HII is close\nto 1, in other words,",
    "start": "346740",
    "end": "349550"
  },
  {
    "text": "observation i really contributes\na lot to its own fit,",
    "start": "349550",
    "end": "352770"
  },
  {
    "text": "1 minus HII is small\nand that will inflate",
    "start": "352770",
    "end": "356069"
  },
  {
    "text": "that particular residual.",
    "start": "356070",
    "end": "358600"
  },
  {
    "text": "So this is like a magic formula.",
    "start": "358600",
    "end": "361450"
  },
  {
    "text": "It tells you that you can\nget your cross-validated fit",
    "start": "361450",
    "end": "364270"
  },
  {
    "text": "by a simple modification of the\nresiduals from the full fit,",
    "start": "364270",
    "end": "369280"
  },
  {
    "text": "and that's much more efficient\nand cheaper to compute.",
    "start": "369280",
    "end": "374900"
  },
  {
    "text": "OK.",
    "start": "374900",
    "end": "375400"
  },
  {
    "text": "So that's a slight detour.",
    "start": "375400",
    "end": "377759"
  },
  {
    "text": "Now we're going to write our\nown function to do that, OK?",
    "start": "377760",
    "end": "380270"
  },
  {
    "start": "380270",
    "end": "382916"
  },
  {
    "text": "And that's formula\n5.2 in the book.",
    "start": "382916",
    "end": "386030"
  },
  {
    "text": "So here we write our function.",
    "start": "386030",
    "end": "388170"
  },
  {
    "text": "We'll call it LOOCV, LOOCV.",
    "start": "388170",
    "end": "391910"
  },
  {
    "text": "Takes the fit as\nan argument, and it",
    "start": "391910",
    "end": "395090"
  },
  {
    "text": "uses a function\ncalled LLM influence,",
    "start": "395090",
    "end": "397340"
  },
  {
    "text": "and that's a\npostprocessor for LLM fit",
    "start": "397340",
    "end": "400100"
  },
  {
    "text": "and it will extract\nthe element h",
    "start": "400100",
    "end": "402860"
  },
  {
    "text": "from that, which gives you those\ndiagonal elements, HII, right?",
    "start": "402860",
    "end": "406830"
  },
  {
    "text": "So we'll put that in a vector h.",
    "start": "406830",
    "end": "409069"
  },
  {
    "text": "And then right on the fly,\nwe'll compute that quantity",
    "start": "409070",
    "end": "412190"
  },
  {
    "text": "on the right hand side\nof our panel over there.",
    "start": "412190",
    "end": "415490"
  },
  {
    "text": "First of all, the\nresiduals of the fit",
    "start": "415490",
    "end": "417740"
  },
  {
    "text": "give you the residuals\nfrom the full fit.",
    "start": "417740",
    "end": "420449"
  },
  {
    "text": "So those are the terms\nin the numerator.",
    "start": "420450",
    "end": "422750"
  },
  {
    "text": "And then we divide\nby 1 minus h squared.",
    "start": "422750",
    "end": "431000"
  },
  {
    "text": "And the residuals\nof foot is a vector",
    "start": "431000",
    "end": "434060"
  },
  {
    "text": "and 1 minus h is a vector.",
    "start": "434060",
    "end": "436500"
  },
  {
    "text": "And the divide now does\nelement by element division",
    "start": "436500",
    "end": "440090"
  },
  {
    "text": "in that vector and we\ntake the whole lot,",
    "start": "440090",
    "end": "441949"
  },
  {
    "text": "square them and take\nthe mean of that.",
    "start": "441950",
    "end": "444120"
  },
  {
    "text": "And so that's going\nto be computing",
    "start": "444120",
    "end": "446180"
  },
  {
    "text": "this formula over here.",
    "start": "446180",
    "end": "448590"
  },
  {
    "text": "So that we just built\ninto our function",
    "start": "448590",
    "end": "451970"
  },
  {
    "text": "and then end off our function.",
    "start": "451970",
    "end": "454590"
  },
  {
    "text": "And since that was the last\nquantity computed, that",
    "start": "454590",
    "end": "458030"
  },
  {
    "text": "will be what's returned, OK?",
    "start": "458030",
    "end": "460790"
  },
  {
    "text": "So let's see if that works.",
    "start": "460790",
    "end": "463230"
  },
  {
    "text": "We'll do LOOCV and lo\nand behold, very quickly,",
    "start": "463230",
    "end": "467940"
  },
  {
    "text": "it produced the 24.23 that we\nsaw above for the first element",
    "start": "467940",
    "end": "473036"
  },
  {
    "text": "of the results of CV.GLM.",
    "start": "473036",
    "end": "476180"
  },
  {
    "text": "So our function works.",
    "start": "476180",
    "end": "478380"
  },
  {
    "text": "OK, great.",
    "start": "478380",
    "end": "479880"
  },
  {
    "text": "So now we're going to use it.",
    "start": "479880",
    "end": "481780"
  },
  {
    "text": "And the way we're going\nto use it is we're",
    "start": "481780",
    "end": "484950"
  },
  {
    "text": "going to fit polynomials of\ndifferent degrees to our data.",
    "start": "484950",
    "end": "491580"
  },
  {
    "text": "Remember what the\ndata looked like.",
    "start": "491580",
    "end": "494349"
  },
  {
    "text": "Let's just go up here\nand plot it again.",
    "start": "494350",
    "end": "497605"
  },
  {
    "text": "The data looks very nonlinear.",
    "start": "497605",
    "end": "500110"
  },
  {
    "text": "So we've plotted it\nagain, and now we're",
    "start": "500110",
    "end": "502330"
  },
  {
    "text": "going to fit some polynomials\nof degrees 1 up to 5, right?",
    "start": "502330",
    "end": "506689"
  },
  {
    "text": "And so we set ourselves up.",
    "start": "506690",
    "end": "508280"
  },
  {
    "text": "We've got a vector for\ncollecting the errors.",
    "start": "508280",
    "end": "510975"
  },
  {
    "start": "510975",
    "end": "513544"
  },
  {
    "text": "And now, we create the\nvariable degree, which takes",
    "start": "513544",
    "end": "517840"
  },
  {
    "text": "values 1 to 5, and then\nwe go in a loop for D",
    "start": "517840",
    "end": "522099"
  },
  {
    "text": "in degree, fit the GLM using\na polynomial of that degree.",
    "start": "522100",
    "end": "529180"
  },
  {
    "text": "So we use the poly\nfunction, the function",
    "start": "529180",
    "end": "531630"
  },
  {
    "text": "of horsepower and degree.",
    "start": "531630",
    "end": "534520"
  },
  {
    "text": "And then we use our little\nfunction to compute the error,",
    "start": "534520",
    "end": "537940"
  },
  {
    "text": "the leave one out\ncross validation error",
    "start": "537940",
    "end": "540290"
  },
  {
    "text": "and put it in our error vector.",
    "start": "540290",
    "end": "542565"
  },
  {
    "start": "542565",
    "end": "545570"
  },
  {
    "text": "And look, it's finished already.",
    "start": "545570",
    "end": "547160"
  },
  {
    "text": "It's done all of them.",
    "start": "547160",
    "end": "548209"
  },
  {
    "text": "And if we plot this\nerror against degree,",
    "start": "548210",
    "end": "551230"
  },
  {
    "text": "we see that degree 1\ndoes pretty poorly,",
    "start": "551230",
    "end": "556269"
  },
  {
    "text": "degree 2 jumps down from\n24 down to just above 19,",
    "start": "556270",
    "end": "560330"
  },
  {
    "text": "and then higher degrees really\ndon't make much difference.",
    "start": "560330",
    "end": "563480"
  },
  {
    "text": "And we might have guessed that\nlooking at the plot of the data,",
    "start": "563480",
    "end": "566440"
  },
  {
    "text": "that a quadratic\nwould do a good job.",
    "start": "566440",
    "end": "571280"
  },
  {
    "text": "OK.",
    "start": "571280",
    "end": "571850"
  },
  {
    "text": "Well, that was leave one\nout cross validation.",
    "start": "571850",
    "end": "574209"
  },
  {
    "text": "Let's try 10-fold\ncross validation.",
    "start": "574210",
    "end": "576440"
  },
  {
    "text": "So recall with 10-fold\ncross validation,",
    "start": "576440",
    "end": "579650"
  },
  {
    "text": "you do actually much less work.",
    "start": "579650",
    "end": "581840"
  },
  {
    "text": "What you do here is you make--",
    "start": "581840",
    "end": "583940"
  },
  {
    "text": "you divide the data\nup into 10 pieces.",
    "start": "583940",
    "end": "587740"
  },
  {
    "text": "And each 10th is a test set and\nthe 9/10th acts as a training",
    "start": "587740",
    "end": "591940"
  },
  {
    "text": "set.",
    "start": "591940",
    "end": "592980"
  },
  {
    "text": "And so for 10-fold\ncross-validation,",
    "start": "592980",
    "end": "595680"
  },
  {
    "text": "you only have to fit\nthe model 10 times.",
    "start": "595680",
    "end": "598140"
  },
  {
    "text": "With leave one out, you\nhave to, in principle",
    "start": "598140",
    "end": "600360"
  },
  {
    "text": "fit the model n times where n is\nthe number of training points,",
    "start": "600360",
    "end": "604380"
  },
  {
    "text": "although we did have the\nshortcut for linear regression.",
    "start": "604380",
    "end": "608850"
  },
  {
    "text": "The reason CV.GLM\ndoesn't use that shortcut",
    "start": "608850",
    "end": "612329"
  },
  {
    "text": "is it's also set up to work\non logistic regressions",
    "start": "612330",
    "end": "616170"
  },
  {
    "text": "and other models and there\nthe shortcut doesn't work.",
    "start": "616170",
    "end": "619029"
  },
  {
    "text": "OK.",
    "start": "619030",
    "end": "619530"
  },
  {
    "text": "So here we'll do\n10-fold and we'll again,",
    "start": "619530",
    "end": "623280"
  },
  {
    "text": "set up a vector to\ncollect our errors.",
    "start": "623280",
    "end": "626200"
  },
  {
    "text": "And the same thing\ngoes through the list.",
    "start": "626200",
    "end": "629030"
  },
  {
    "text": "We have a loop, list of\ndegrees, fit our model,",
    "start": "629030",
    "end": "632410"
  },
  {
    "text": "and now we'll actually\nuse the CV.GLM",
    "start": "632410",
    "end": "635170"
  },
  {
    "text": "function to compute the errors.",
    "start": "635170",
    "end": "638360"
  },
  {
    "text": "And so we call CV.GLM\nand we tell it k is 10.",
    "start": "638360",
    "end": "642700"
  },
  {
    "text": "So that tells the\nnumber of folds.",
    "start": "642700",
    "end": "646486"
  },
  {
    "text": "And that's pretty quick because\nit's only fitting the model 10",
    "start": "646486",
    "end": "649460"
  },
  {
    "text": "times each time.",
    "start": "649460",
    "end": "650760"
  },
  {
    "text": "And now we'll include\nthe errors on our plot.",
    "start": "650760",
    "end": "654410"
  },
  {
    "text": "We'll color them in red.",
    "start": "654410",
    "end": "656389"
  },
  {
    "text": "And so we use the\nfunction lines.",
    "start": "656390",
    "end": "659250"
  },
  {
    "text": "And it's not much different.",
    "start": "659250",
    "end": "661390"
  },
  {
    "text": "So in this case, tenfold and\nleave one out cross validation",
    "start": "661390",
    "end": "663990"
  },
  {
    "text": "pretty much told\nus the same story.",
    "start": "663990",
    "end": "666390"
  },
  {
    "text": "In general, we favor 10-fold\ncross validation for computing",
    "start": "666390",
    "end": "670200"
  },
  {
    "text": "errors.",
    "start": "670200",
    "end": "671490"
  },
  {
    "text": "It tends to be a more stable\nmeasure than leave one",
    "start": "671490",
    "end": "674790"
  },
  {
    "text": "out cross validation\nand for the most time,",
    "start": "674790",
    "end": "678440"
  },
  {
    "text": "it's cheaper to compute.",
    "start": "678440",
    "end": "680500"
  },
  {
    "start": "680500",
    "end": "681000"
  }
]