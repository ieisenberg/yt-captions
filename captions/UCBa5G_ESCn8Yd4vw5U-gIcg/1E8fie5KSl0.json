[
  {
    "start": "0",
    "end": "5340"
  },
  {
    "text": "Welcome back to Stanford CS224W. Today, we're going to continue\nour discussion on machine",
    "start": "5340",
    "end": "13890"
  },
  {
    "text": "learning with graphs. And in particular,\nwe will be visiting a new family of solutions\nthat can do classification",
    "start": "13890",
    "end": "21240"
  },
  {
    "text": "on graphs. So earlier, we have been\nfocusing on the paradigm of graph neural networks.",
    "start": "21240",
    "end": "27250"
  },
  {
    "text": "And today, we will be visiting\nanother different idea, but share a lot of\nsimilar intuitions.",
    "start": "27250",
    "end": "32340"
  },
  {
    "text": "That is called\nlabel propagation. And label propagation\nhas a very long history.",
    "start": "32340",
    "end": "37360"
  },
  {
    "text": "It is like an invented way\nbefore graph neural networks.",
    "start": "37360",
    "end": "42420"
  },
  {
    "text": "And it has a lot of applications\nbeyond graph as well. Like in the-- yeah.",
    "start": "42420",
    "end": "52050"
  },
  {
    "text": "Besides like making\npredictions on graphs, people have used it in\nsemi-supervised learning and a lot of other\napplications as well.",
    "start": "52050",
    "end": "58989"
  },
  {
    "text": "So this will be the\ntopic for today. So here's the outline\nfor today's lecture.",
    "start": "58990",
    "end": "67900"
  },
  {
    "text": "The main question we want\nto ask is given a network with certain labels\non some of the nodes,",
    "start": "67900",
    "end": "73930"
  },
  {
    "text": "how do we assign the labels\nto the rest of the nodes? And for example, suppose\nthere is a social network,",
    "start": "73930",
    "end": "80680"
  },
  {
    "text": "we know some of the\nnodes are fraudsters and some others\nare fully trusted,",
    "start": "80680",
    "end": "85990"
  },
  {
    "text": "and how do you find\nthe remaining fraud",
    "start": "85990",
    "end": "91210"
  },
  {
    "text": "nodes and the remaining\ntrustworthy nodes? And as you know,\nthis setting we have been investigated\nthrough the idea",
    "start": "91210",
    "end": "98590"
  },
  {
    "text": "of embedding based methods. And these node\nembeddings can either be generated from random\nwalks, like Node2Vec,",
    "start": "98590",
    "end": "105939"
  },
  {
    "text": "DeepWalk that we have\nintroduced a few lectures back. Or we can generate\na node embedding",
    "start": "105940",
    "end": "111340"
  },
  {
    "text": "using graph neural\nnetworks based on the idea of\nneighborhood aggregation. And we have seen graph neural\nnetworks in the past five",
    "start": "111340",
    "end": "119860"
  },
  {
    "text": "different lectures. And the key question we want\nto ask today, that are there",
    "start": "119860",
    "end": "125900"
  },
  {
    "text": "alternative ways to still make\nthis prediction on networks based on what we know and\npredict the unknown node",
    "start": "125900",
    "end": "132840"
  },
  {
    "text": "labels.  So here is our setting. Suppose we are given a graph\nand we know some of the nodes,",
    "start": "132840",
    "end": "141799"
  },
  {
    "text": "and usually, very few\nset of the nodes labels. And we want to predict the\nlabels of the remaining",
    "start": "141800",
    "end": "148600"
  },
  {
    "text": "node in the network. Let's say, we have these\ntwo classes, green and red.",
    "start": "148600",
    "end": "154120"
  },
  {
    "text": "And then we want to\nmake predictions for all the remaining gray nodes.",
    "start": "154120",
    "end": "159880"
  },
  {
    "text": "And the terminology to\ndescribe this classification is transductive\nnode classification",
    "start": "159880",
    "end": "166330"
  },
  {
    "text": "that we have introduced\ntwo lectures back. The reason they are\ncalled transductive is because all the training\nand validation data,",
    "start": "166330",
    "end": "175570"
  },
  {
    "text": "they are all on\nthe same network. So we only focus on one network. We don't need to generalize\nto a separate network.",
    "start": "175570",
    "end": "181720"
  },
  {
    "text": "And then we know some\nof the nodes label, and we want to predict\nthe remaining ones. So that's why we\ncall it transductive",
    "start": "181720",
    "end": "187719"
  },
  {
    "text": "node classification. And another name\nfor this setting is so-called semi-supervised\nnode classification.",
    "start": "187720",
    "end": "195510"
  },
  {
    "text": "And this is kind\nof a legacy name to describe the setting\nof this problem. The motivation is that\nusually, the network is huge.",
    "start": "195510",
    "end": "203989"
  },
  {
    "text": "And we only know very\nfew labels of the nodes. So it's kind of not\na supervised learning",
    "start": "203990",
    "end": "211010"
  },
  {
    "text": "task, where we have\na lot of labels, but not like an\nunsupervised learning task we have no Labels at all.",
    "start": "211010",
    "end": "216240"
  },
  {
    "text": "So kind of an\nintermediate state. So people, sometimes, also\ncall it semi-supervised",
    "start": "216240",
    "end": "221690"
  },
  {
    "text": "node classification. So this will be the setting\nwe want to discuss today.",
    "start": "221690",
    "end": "226925"
  },
  {
    "text": " And today, we're going to\nsolve this problem using",
    "start": "226925",
    "end": "233049"
  },
  {
    "text": "a different methodology as\nwe have motivated, called label propagation.",
    "start": "233050",
    "end": "238750"
  },
  {
    "text": "And the main assumption\nor the main intuition here is that correlations\nwidely exist in networks.",
    "start": "238750",
    "end": "245920"
  },
  {
    "text": "And to be more\nspecific, what we mean is that we assume nodes\nthat are connected",
    "start": "245920",
    "end": "251800"
  },
  {
    "text": "tend to share the same labels. And this effect, as we\nwill dive deeper later ,",
    "start": "251800",
    "end": "258338"
  },
  {
    "text": "actually widely exists in a\nlot of real world networks. So this is a kind of a general\nassumption you can always",
    "start": "258339",
    "end": "267040"
  },
  {
    "text": "make for real world graphs. And to solve this problem,\nbased on this intuition,",
    "start": "267040",
    "end": "272560"
  },
  {
    "text": "we will specifically introduce\nthree different techniques. The first is the\nbasics of this idea",
    "start": "272560",
    "end": "279880"
  },
  {
    "text": "is called label propagation. And this topic is\nalso quite important. And you will probably\nencounter in this homework",
    "start": "279880",
    "end": "286819"
  },
  {
    "text": "too as well, how to\nuse label propagation to solve a specific\nprediction problem.",
    "start": "286820",
    "end": "292370"
  },
  {
    "text": "Another technique we\ncall is correct smooth. And this is actually the\ncurrent state of our methods",
    "start": "292370",
    "end": "300260"
  },
  {
    "text": "when we make\npredictions on networks. And it's dominating\na lot of benchmarks.",
    "start": "300260",
    "end": "305450"
  },
  {
    "text": "And the nice of this approach\nis that it can really combine label propagation\nand graph neural networks",
    "start": "305450",
    "end": "311630"
  },
  {
    "text": "that we learned earlier. So you can actually utilize\nthe idea from both sides.",
    "start": "311630",
    "end": "318600"
  },
  {
    "text": "And lastly, we're going to talk\nabout the approach called mask label prediction. And this is a new content we\nadd for this year's offering.",
    "start": "318600",
    "end": "327500"
  },
  {
    "text": "And the main motivation\nthat this solution is super simple to implement. And it may be useful for your\ncourse project if needed.",
    "start": "327500",
    "end": "333919"
  },
  {
    "text": " So let's get started\nwith the motivation.",
    "start": "333920",
    "end": "339819"
  },
  {
    "text": "Like why we want to\nassign similar labels",
    "start": "339820",
    "end": "344860"
  },
  {
    "text": "to nodes that are connected? And this is based on the\nobservation that correlation,",
    "start": "344860",
    "end": "349970"
  },
  {
    "text": "why they exist in network? And we found that\nbehaviors of nodes are connected across the\nlinks of the network.",
    "start": "349970",
    "end": "357760"
  },
  {
    "text": "For example, usually\nthe nodes that are sharing the same label,\nthey tend to cluster together.",
    "start": "357760",
    "end": "364660"
  },
  {
    "text": "Like there's a green\ncluster and the red cluster.",
    "start": "364660",
    "end": "370120"
  },
  {
    "text": "And there are some\nvery good reasons that why this assumption\nwill hold true, especially",
    "start": "370120",
    "end": "375520"
  },
  {
    "text": "in real world networks. And there are two\nterminologies here. One is called homophily.",
    "start": "375520",
    "end": "381670"
  },
  {
    "text": "The second is called influence. And these two observations\nfrom social science or network",
    "start": "381670",
    "end": "387610"
  },
  {
    "text": "science can give a\nlot of nice intuition that why this correlation\nexists in networks.",
    "start": "387610",
    "end": "394420"
  },
  {
    "text": "In particular, for\nhomophily, we want to argue that there are certain\nindividual characteristics",
    "start": "394420",
    "end": "400629"
  },
  {
    "text": "of different nodes that leads\nto the social connections between these nodes.",
    "start": "400630",
    "end": "405940"
  },
  {
    "text": "And vice versa, in\nterms of influence, because nodes are\nclosely tied together",
    "start": "405940",
    "end": "411970"
  },
  {
    "text": "through their\nsocial connections. In the end, they tend\nto share very similar individual characteristics.",
    "start": "411970",
    "end": "418190"
  },
  {
    "text": "So these are the two\nmotivations we have. And let's dive deeper\ninto the two effects.",
    "start": "418190",
    "end": "425360"
  },
  {
    "text": "First, regarding\nhomophily, homophily really means that the tendency of\nindividuals in a network",
    "start": "425360",
    "end": "431240"
  },
  {
    "text": "tend to bond or associate\ntogether with their neighbors. And it has been observed that\na vast array of network studies",
    "start": "431240",
    "end": "440840"
  },
  {
    "text": "can have this\nhomophily attribute. For example, the age, gender,\norganization roles of nodes",
    "start": "440840",
    "end": "448130"
  },
  {
    "text": "usually cluster together if\nthey have similar values.",
    "start": "448130",
    "end": "453230"
  },
  {
    "text": "Some concrete\nexamples, including on an academic\nnetwork, researchers",
    "start": "453230",
    "end": "459440"
  },
  {
    "text": "who focus on the\nsame research area tend to establish\ncloser connections",
    "start": "459440",
    "end": "465440"
  },
  {
    "text": "compared to researchers\nthat does not belong to the same area. This is because they can\ncite each other's papers.",
    "start": "465440",
    "end": "471590"
  },
  {
    "text": "They may meet at different\nconferences, et cetera. And this kind of motivated\nwhy those individual",
    "start": "471590",
    "end": "477980"
  },
  {
    "text": "characteristics\nkind of establish those social connections\nbetween different nodes.",
    "start": "477980",
    "end": "483785"
  },
  {
    "text": " And this homophily is a widely\nexisted in social networks.",
    "start": "483785",
    "end": "490980"
  },
  {
    "text": "And here is a concrete\nexample visualization of social networks.",
    "start": "490980",
    "end": "496210"
  },
  {
    "text": "And this network is built\naround different people and based on their\nfriendship relationship.",
    "start": "496210",
    "end": "501990"
  },
  {
    "text": "The colors means the\ninterest of different people. Like some people may interest\nin sports or some interest",
    "start": "501990",
    "end": "507960"
  },
  {
    "text": "in arts, et cetera. So given this real\nworld large networks, we can label each node\nbased on their interest.",
    "start": "507960",
    "end": "515729"
  },
  {
    "text": "And we can easily see\nfrom this visualization, then, the color of nodes\ntends to cluster together.",
    "start": "515730",
    "end": "522510"
  },
  {
    "text": "Meaning that nodes that\nshare similar labels, they are closely\nconnected as well.",
    "start": "522510",
    "end": "528370"
  },
  {
    "text": "So that's kind of a concrete\nexample of how homophily appears in real world networks.",
    "start": "528370",
    "end": "534269"
  },
  {
    "text": " Another motivation we have is\nthis notion called influence.",
    "start": "534270",
    "end": "542220"
  },
  {
    "text": "And influence says that\nthe social connections can influence individuals'\ncharacteristics.",
    "start": "542220",
    "end": "548770"
  },
  {
    "text": "For example, suppose a\ngroup of people often like",
    "start": "548770",
    "end": "553950"
  },
  {
    "text": "connect together and they\nmay share their interests and influence each other. Let's say, if I have interest\nin certain musical preferences,",
    "start": "553950",
    "end": "562740"
  },
  {
    "text": "then that may grow the\ninterest of my peers, who are connected\nwith me as well.",
    "start": "562740",
    "end": "567820"
  },
  {
    "text": "So this is kind\nof what influence is happening in real world. And consequently, because\nthis influence exists,",
    "start": "567820",
    "end": "574830"
  },
  {
    "text": "suppose nodes that are closely\nconnected together, in the end, they may tend to share\nsimilar features,",
    "start": "574830",
    "end": "581160"
  },
  {
    "text": "and eventually, similar labels. So that's another\nway to understand why this homophily and influence\ntend to appear in the network.",
    "start": "581160",
    "end": "593050"
  },
  {
    "text": "So, so far, we have seen\nsome nice motivations from neuroscience, why there\nare a lot of correlations",
    "start": "593050",
    "end": "599970"
  },
  {
    "text": "exist in networks. So the question is, how do\nwe leverage this observation and transform into machine\nlearning solutions,",
    "start": "599970",
    "end": "607350"
  },
  {
    "text": "and they can apply on the\nsemi-supervised classification that we care about\nin the networks.",
    "start": "607350",
    "end": "615550"
  },
  {
    "text": "So to show this, we were\ngoing to use concrete example to set up the problem setting.",
    "start": "615550",
    "end": "621010"
  },
  {
    "text": "Suppose we have this\nnetwork with a lot of nodes. And some of the\nnodes are labeled.",
    "start": "621010",
    "end": "626019"
  },
  {
    "text": "For example, this green nodes\nare like labeled as one. The red nodes are\nlabeled as zero.",
    "start": "626020",
    "end": "632230"
  },
  {
    "text": "And our goal is that\nhow we want to predict the labels for the\nremaining nodes in gray.",
    "start": "632230",
    "end": "639000"
  },
  {
    "text": "So formally, we're always\ngiven one particular graph. And only a very few of\nthe nodes are labeled.",
    "start": "639000",
    "end": "646410"
  },
  {
    "text": "And our goal is to\nfind the class, which is the color as in our example\nof the remaining nodes.",
    "start": "646410",
    "end": "654839"
  },
  {
    "text": "And our main assumption,\nthat there's homophily in the network. Meaning that suppose nodes\nthat are close together,",
    "start": "654840",
    "end": "661740"
  },
  {
    "text": "they tend to share\nsimilar labels. ",
    "start": "661740",
    "end": "667120"
  },
  {
    "text": "So to be more concrete,\nwe have this A as the adjacency matrix\nand Y to be a vector",
    "start": "667120",
    "end": "674950"
  },
  {
    "text": "of labels for each of the node. We consider this\nbinary classification. So note, only take 0 or 1.",
    "start": "674950",
    "end": "682100"
  },
  {
    "text": "So Yv equals 1 means\nthat node v has class 1.",
    "start": "682100",
    "end": "687310"
  },
  {
    "text": "And equals 0 means it\nbelongs to class 0. And there are unlabeled nodes,\nthen, we wish to classify.",
    "start": "687310",
    "end": "694480"
  },
  {
    "text": "Our goal is to predict\nwhich of the unlabeled nodes are likely to be\nclass 1 or class 0.",
    "start": "694480",
    "end": "701470"
  },
  {
    "text": "So we want to assign\na probabilistic vector for each of the unlabeled node.",
    "start": "701470",
    "end": "706510"
  },
  {
    "text": " So this probability vector can\nbe written as this P over Yv.",
    "start": "706510",
    "end": "715320"
  },
  {
    "text": "And this is like a\ngenerated, eventually, based on all the\nfeatures and the known",
    "start": "715320",
    "end": "722400"
  },
  {
    "text": "labels for the network. So we want to\nleverage everything we know from the\nnetwork and generate",
    "start": "722400",
    "end": "727770"
  },
  {
    "text": "this PYv for each of\nthe gray nodes that does not have any labels.",
    "start": "727770",
    "end": "733540"
  },
  {
    "text": "So this is our setting. And we will focus\non this problem",
    "start": "733540",
    "end": "739080"
  },
  {
    "text": "through three\ndifferent approaches that we just talk about. We will first visit the core\nidea of label propagation,",
    "start": "739080",
    "end": "746880"
  },
  {
    "text": "and then talk about how label\npropagation can be combined with graph neural network, which\nleads to the second solution,",
    "start": "746880",
    "end": "752730"
  },
  {
    "text": "correct and smooth. And lastly, we will talk\nabout a very practical and easy to implement approach\ncalled masked label prediction.",
    "start": "752730",
    "end": "760660"
  },
  {
    "text": "So this is our approach to\nsolve this problem setting. ",
    "start": "760660",
    "end": "766960"
  },
  {
    "text": "So we will first talk about\nlabel propagation as the first, like a foundational\nmethod in this area.",
    "start": "766960",
    "end": "775710"
  },
  {
    "text": "Our idea is that we can\npropagate the node label across the network. So from the node that we\nknow the ground truth label",
    "start": "775710",
    "end": "783149"
  },
  {
    "text": "to the unknown nodes. And our intuition that the class\nprobability of a given node v",
    "start": "783150",
    "end": "792149"
  },
  {
    "text": "should be a weighted\naverage of the probabilities of its neighbors. So concretely for a\ngiven node, like v,",
    "start": "792150",
    "end": "800550"
  },
  {
    "text": "will initialize the label of\nit based on the ground truth label we have.",
    "start": "800550",
    "end": "807180"
  },
  {
    "text": "And for those nodes we do\nnot have any information. So they are unlabeled. We will just initialize\nusing a value of 0.5.",
    "start": "807180",
    "end": "816029"
  },
  {
    "text": "And we will update all the label\nnodes until the convergence. And this update can\nhappen either in parallel.",
    "start": "816030",
    "end": "823269"
  },
  {
    "text": "So we compute the updated\nprobability vector for older nodes. Or we can also do\nlike a random update.",
    "start": "823270",
    "end": "830710"
  },
  {
    "text": "So pick some random\nand label nodes to update in a\ncertain iteration,",
    "start": "830710",
    "end": "836530"
  },
  {
    "text": "and until it's converged\nor until the maximum number of iterations reached.",
    "start": "836530",
    "end": "842780"
  },
  {
    "text": "So this is the intuition\nof label propagation. And we're going to introduce\nthe concrete mathematical form",
    "start": "842780",
    "end": "849700"
  },
  {
    "text": "for this algorithm.  So here is the math\nfor label propagation.",
    "start": "849700",
    "end": "858640"
  },
  {
    "text": "We have this update\nfunction that describe one specific\niteration of label propagation.",
    "start": "858640",
    "end": "867759"
  },
  {
    "text": "The input of this iteration\nwill be the probability vector for each of the node in\na certain time point t.",
    "start": "867760",
    "end": "876310"
  },
  {
    "text": "So this Pt over Yu equals c. So this is a\ntwo-dimensional vector,",
    "start": "876310",
    "end": "882290"
  },
  {
    "text": "tells us the\nprobability of either a node takes a value of 1\nor it takes a value of 0.",
    "start": "882290",
    "end": "888610"
  },
  {
    "text": "We will time this\nprobability vector with the adjacency matrix\nand take a summation.",
    "start": "888610",
    "end": "894860"
  },
  {
    "text": "So this basically says that\nI will sum the probability vector of all my neighbors.",
    "start": "894860",
    "end": "901149"
  },
  {
    "text": "And then we will do\na normalization term. So we will divide\nthe summed value",
    "start": "901150",
    "end": "906220"
  },
  {
    "text": "with the summation of\nall the neighbors, which is a degree of a given node.",
    "start": "906220",
    "end": "911290"
  },
  {
    "text": "So basically, what\nis saying here is that to compute the next\niteration value of a given",
    "start": "911290",
    "end": "917510"
  },
  {
    "text": "node, we will simply take\nthe average of its neighbors' probability vector. So it's kind of pretty\nsimple intuition here.",
    "start": "917510",
    "end": "926149"
  },
  {
    "text": "And you may recall\nthat this formulation is very similar to a mean\naggregation inside a graph",
    "start": "926150",
    "end": "931610"
  },
  {
    "text": "neural network, except\nthat now, we are not really learning any embedding, or\nthis is not even like a machine",
    "start": "931610",
    "end": "938480"
  },
  {
    "text": "learning algorithm. We do not learn anything. We just use this algorithm to\niteratively update the labels.",
    "start": "938480",
    "end": "945079"
  },
  {
    "text": "So there's no trainable\nparameters here. And our extension\nhere is, that suppose",
    "start": "945080",
    "end": "952490"
  },
  {
    "text": "we have some strength\nor weight information on the adjacency matrix. So in this case, the adjacency\nmatrix is no longer binary.",
    "start": "952490",
    "end": "960530"
  },
  {
    "text": "Some of the edges\nhave higher weights, some edges have\nlower weights, then, we can easily replace\nthis Avu from 0, 1",
    "start": "960530",
    "end": "970190"
  },
  {
    "text": "to the weight of the edges. And we will repeat this\nupdate until the final value",
    "start": "970190",
    "end": "980560"
  },
  {
    "text": "or probability vector converge\nto within certain threshold.",
    "start": "980560",
    "end": "985610"
  },
  {
    "text": "So this is like\nthe exact algorithm of how label propagation works.",
    "start": "985610",
    "end": "991435"
  },
  {
    "text": " So it's best to\nunderstand this algorithm",
    "start": "991435",
    "end": "996940"
  },
  {
    "text": "through a concrete example. So we'll have a running example\nthat describe how this labor",
    "start": "996940",
    "end": "1004110"
  },
  {
    "text": "progression works in reality. So given this networks, we\nhave some of the nodes labeled",
    "start": "1004110",
    "end": "1011880"
  },
  {
    "text": "as green or as red. So for nodes that are labeled\nas green, we set them to 1.",
    "start": "1011880",
    "end": "1019230"
  },
  {
    "text": "For nodes that are level 0,\nlabel as red, we label as 0.",
    "start": "1019230",
    "end": "1025199"
  },
  {
    "text": "And for all the\nremaining green nodes, we'll initialize them with 0.5.",
    "start": "1025200",
    "end": "1031230"
  },
  {
    "text": "So this notation like a PYi\nmeans that this given node",
    "start": "1031230",
    "end": "1037319"
  },
  {
    "text": "is probability of i equal to 1. And because it's a\nbinary classification.",
    "start": "1037319",
    "end": "1044028"
  },
  {
    "text": "Like probability of\ntaken of it being 0 is always 1 minus this value.",
    "start": "1044029",
    "end": "1049450"
  },
  {
    "text": "So instead of treating\nthis as a vector, we just treat it as a scalar.",
    "start": "1049450",
    "end": "1055610"
  },
  {
    "text": "But this idea can generalize\nto multi-class classification as well. So this is our initial state.",
    "start": "1055610",
    "end": "1061840"
  },
  {
    "text": "Like suppose, we\njust give a network, provided with a network, then\nwe'll initialize all the nodes",
    "start": "1061840",
    "end": "1067630"
  },
  {
    "text": "with this initial values.  So now, we'll\nbegin our iteration",
    "start": "1067630",
    "end": "1074820"
  },
  {
    "text": "with label propagation. So let's say, we want to\nupdate the probability",
    "start": "1074820",
    "end": "1081390"
  },
  {
    "text": "vector for a given node 3. Based on the algorithm\ndescribed, what we do",
    "start": "1081390",
    "end": "1087750"
  },
  {
    "text": "is that we simply\naverage the likelihood of each of the neighbor.",
    "start": "1087750",
    "end": "1093700"
  },
  {
    "text": "So in this case, this node\n3 has three neighbors, which has probability\nof 0.5, 0 and 0.",
    "start": "1093700",
    "end": "1103320"
  },
  {
    "text": "So based on this three\nneighbors probability value, we will take an average\nand assign the new value",
    "start": "1103320",
    "end": "1110279"
  },
  {
    "text": "to this node. And you can see that after this\nupdate, like the information",
    "start": "1110280",
    "end": "1116130"
  },
  {
    "text": "that we learned from a\ngiven unlabeled node, kind of has changed. Because we observed that\nit has two neighbors",
    "start": "1116130",
    "end": "1124260"
  },
  {
    "text": "that are labeled as 0. So its likelihood is now lowered\nbecause of its neighboring node",
    "start": "1124260",
    "end": "1131470"
  },
  {
    "text": "probability value.  So similarly, we can update the\nremaining nodes in the network.",
    "start": "1131470",
    "end": "1141090"
  },
  {
    "text": "For example, for this\nnode 4, we will also look at this four neighbors.",
    "start": "1141090",
    "end": "1146370"
  },
  {
    "text": "And some of them are like 0.5. Some of them are one.",
    "start": "1146370",
    "end": "1151440"
  },
  {
    "text": "And then we will take the\naverage of its neighbor's propagated value.",
    "start": "1151440",
    "end": "1156450"
  },
  {
    "text": "And this way, we can\nget the updated value for node 4, which is still 0.5.",
    "start": "1156450",
    "end": "1163260"
  },
  {
    "text": "So like our intuition\nor the algorithms guess, for the label of\nnode 4, does not",
    "start": "1163260",
    "end": "1169950"
  },
  {
    "text": "change after this\nlabel propagation. Question? ",
    "start": "1169950",
    "end": "1176370"
  },
  {
    "text": "Could it be faster if\ninstead of using the value from the previous\niteration, you used",
    "start": "1176370",
    "end": "1182220"
  },
  {
    "text": "whatever the current value was? Good point. So in our case here, we\njust derived the computation",
    "start": "1182220",
    "end": "1190320"
  },
  {
    "text": "for each of the node. The benefit of always\nusing the first layer is that you can parallelize\nthe computation.",
    "start": "1190320",
    "end": "1196750"
  },
  {
    "text": "So instead of\ncomputing-- in reality, instead of computing\none node at a time,",
    "start": "1196750",
    "end": "1202020"
  },
  {
    "text": "you can compute like\nthe updated value for all the nodes in parallel. And in the meantime, I think\nthe algorithm you suggested also",
    "start": "1202020",
    "end": "1210630"
  },
  {
    "text": "makes sense. So the suggestion\nalgorithm that like we just ignore like the so-called\nprevious state of a given node.",
    "start": "1210630",
    "end": "1218460"
  },
  {
    "text": "So we always update based\non the current value. I think that is also\nlike a valid algorithm",
    "start": "1218460",
    "end": "1223530"
  },
  {
    "text": "and also aligns with our\nintuition of homophily. But like this is like not really\nthe algorithm we introduce",
    "start": "1223530",
    "end": "1232110"
  },
  {
    "text": "here, I think both\nalgorithms are valid. ",
    "start": "1232110",
    "end": "1238290"
  },
  {
    "text": "OK. So this is like our running\nexample regarding computing",
    "start": "1238290",
    "end": "1244080"
  },
  {
    "text": "two nodes in the network. So using the same,\nidea we can continue to compute all of the\nunlabeled nodes in the network.",
    "start": "1244080",
    "end": "1252690"
  },
  {
    "text": "Say for node 5, we can compute\nits likelihood to be 0.75.",
    "start": "1252690",
    "end": "1259600"
  },
  {
    "text": "And after this iteration,\nnow we have the updated value for each of the unlabeled\nnodes in the network.",
    "start": "1259600",
    "end": "1266940"
  },
  {
    "text": "And you see that just after\nthis one iteration of update, the value already start\nto make some sense.",
    "start": "1266940",
    "end": "1275789"
  },
  {
    "text": "So in this case, like\nthe node 8 and 9, they have pretty\nhigh predicted value",
    "start": "1275790",
    "end": "1282660"
  },
  {
    "text": "because they are very\nclose to the label nodes. And for this node 3,\nas we talked about,",
    "start": "1282660",
    "end": "1288390"
  },
  {
    "text": "it has a pretty low\nprediction score. So just after one\niteration, we already",
    "start": "1288390",
    "end": "1294809"
  },
  {
    "text": "start to make some\nreasonable guess on the label on the network. ",
    "start": "1294810",
    "end": "1302260"
  },
  {
    "text": "So next, we're going to\nrepeat this computation for each of the unlabeled node. And we'll repeat until\nthe value stop to change.",
    "start": "1302260",
    "end": "1312760"
  },
  {
    "text": "So the convergence\ncriteria, we can",
    "start": "1312760",
    "end": "1317790"
  },
  {
    "text": "define a convergence\ncriteria based on when the change of\nthe probability value",
    "start": "1317790",
    "end": "1326970"
  },
  {
    "text": "is start to be smaller than\nthe given epsilon, which is a very small\nthreshold, then we",
    "start": "1326970",
    "end": "1332970"
  },
  {
    "text": "can call that the\nalgorithm has converged and we'll stop like the update. ",
    "start": "1332970",
    "end": "1341049"
  },
  {
    "text": "So we will like to\ncontinue this iteration until we find that in\nthis iteration 3 and 4,",
    "start": "1341050",
    "end": "1348159"
  },
  {
    "text": "you see the value\nbarely changed. And we can, in this case, we\nset the epsilon to be 0.01.",
    "start": "1348160",
    "end": "1355630"
  },
  {
    "text": "And because we have satisfied\nthe convergence criteria, then we will stop the algorithm.",
    "start": "1355630",
    "end": "1361150"
  },
  {
    "text": "And we will output, the\nprediction based on the final predicted values that we have.",
    "start": "1361150",
    "end": "1369310"
  },
  {
    "text": "So now, we can label all the\nunlabeled nodes as converged. And we can output our prediction\nbased on the final iteration",
    "start": "1369310",
    "end": "1377129"
  },
  {
    "text": "score. So we see that this algorithm\nis actually pretty simple. It does not involve any\ntrainable parameter.",
    "start": "1377130",
    "end": "1383580"
  },
  {
    "text": "Like it's not really\na machine learning. It's just like an algorithm that\ncan operate on an input graph.",
    "start": "1383580",
    "end": "1389040"
  },
  {
    "text": "But in the end it is kind of\nvery efficient to compute, and also, tend to\nbe useful suppose",
    "start": "1389040",
    "end": "1394799"
  },
  {
    "text": "this homophily assumption\nholds true in a given network. ",
    "start": "1394800",
    "end": "1404010"
  },
  {
    "text": "So there are a lot of\nreal world applications of this algorithm because it\nis a very simple and efficient",
    "start": "1404010",
    "end": "1409250"
  },
  {
    "text": "to compute. For example, people have used\nit for document classification, where the nodes are\ndocument, and then the edges",
    "start": "1409250",
    "end": "1416899"
  },
  {
    "text": "are the similarity\nbetween the document. For example, there are overlap\nof the words or sentences",
    "start": "1416900",
    "end": "1422780"
  },
  {
    "text": "in the document. And people have also\nused on social networks, for example, like a\nTwitter social networks,",
    "start": "1422780",
    "end": "1430010"
  },
  {
    "text": "the nodes could be different\nusers, tweets, words, et cetera. The edges could be\nlike a user address,",
    "start": "1430010",
    "end": "1438350"
  },
  {
    "text": "where users follow\neach other or user tweets edges that\ntells us some user",
    "start": "1438350",
    "end": "1445100"
  },
  {
    "text": "create some specific tweets. And we can construct an\nedge between the tweets",
    "start": "1445100",
    "end": "1452090"
  },
  {
    "text": "and the words or\nhashtags, et cetera. And on this, like\na network, suppose",
    "start": "1452090",
    "end": "1457580"
  },
  {
    "text": "we continue to update a network\nbased on the label propagation algorithm, in the\nend, we will be",
    "start": "1457580",
    "end": "1464490"
  },
  {
    "text": "able to classify which\nnodes are more popular or have certain interest towards\ntourism hashtag, et cetera.",
    "start": "1464490",
    "end": "1472300"
  },
  {
    "text": "So this is a kind\nof another good use case of label propagation. And people has also used it\nto do spam or fraud detection,",
    "start": "1472300",
    "end": "1481470"
  },
  {
    "text": "and for example, on\nfinancial networks or on email network to identify\nwhat are the accounts that",
    "start": "1481470",
    "end": "1488820"
  },
  {
    "text": "are kind of fraudulent. So this is the basic idea\nof label propagation.",
    "start": "1488820",
    "end": "1495630"
  },
  {
    "text": "And it has been used in a lot\nof real world applications. Question?",
    "start": "1495630",
    "end": "1501682"
  },
  {
    "text": "So remember, on the homework\nwhen we were doing the over sampling, it kind of\nshowed that it only",
    "start": "1501682",
    "end": "1507200"
  },
  {
    "text": "converged if there were certain\nproperties of the graph that weren't there. Like it was like or it had\nno bipartite components.",
    "start": "1507200",
    "end": "1515370"
  },
  {
    "text": "So is it the same\nthing with this or will the algorithm\nconverge for any graph? Yeah. Great point.",
    "start": "1515370",
    "end": "1521180"
  },
  {
    "text": "Using the algorithm\njust described, it also suffers from the\nnon-convergence issue.",
    "start": "1521180",
    "end": "1526710"
  },
  {
    "text": "So it's not always the case that\nthis algorithm can converge. So in that case, we can\nalso do some mitigation.",
    "start": "1526710",
    "end": "1534049"
  },
  {
    "text": "For example,\nrandomly, jump the-- like a shuffle or\nadd some random noise",
    "start": "1534050",
    "end": "1539690"
  },
  {
    "text": "to the a node value, similar\nto what we've seen in homework. But indeed, that is kind of one\nlimitation of the algorithm.",
    "start": "1539690",
    "end": "1548240"
  },
  {
    "text": "And actually, we will summarize,\nin this slide as well. So this idea is pretty\nnice and easy to implement,",
    "start": "1548240",
    "end": "1554809"
  },
  {
    "text": "but you suffer from some issues. Like one thing we\njust talked about is that indeed, this algorithm\ndoes not guarantee",
    "start": "1554810",
    "end": "1562350"
  },
  {
    "text": "to converge on any networks. And also, like even\nthough it may converge,",
    "start": "1562350",
    "end": "1567720"
  },
  {
    "text": "it could take like a lot\nof steps to converge. So in reality, like on\nreasonable sized network,",
    "start": "1567720",
    "end": "1573960"
  },
  {
    "text": "usually, people have to\nrepeat hundreds or even thousands of iterations\nuntil a satisfying a label",
    "start": "1573960",
    "end": "1581670"
  },
  {
    "text": "value is reached. And another major limitation\nof label propagation",
    "start": "1581670",
    "end": "1588480"
  },
  {
    "text": "is that it does not utilize\nany node attributes. So we only propagate\nthe label of the network",
    "start": "1588480",
    "end": "1595620"
  },
  {
    "text": "across from the known\nnodes to the unknown nodes. But we never really utilize\nany features or attributes",
    "start": "1595620",
    "end": "1603540"
  },
  {
    "text": "in the network. So this is another limitation. So based on these\nissues, our question",
    "start": "1603540",
    "end": "1609840"
  },
  {
    "text": "next is can we improve this\nidea of label propagation, so that we can leverage the very\nuseful feature information that",
    "start": "1609840",
    "end": "1618480"
  },
  {
    "text": "presented in the network. So this leads to\nthe second approach",
    "start": "1618480",
    "end": "1624010"
  },
  {
    "text": "we want to introduce today. That is correct and smooth. So it really combines the\nidea of label propagation",
    "start": "1624010",
    "end": "1631149"
  },
  {
    "text": "and the idea of machine learning\nwith predictions on networks. ",
    "start": "1631150",
    "end": "1638610"
  },
  {
    "text": "So the approach of\ncorrect and smooth, really achieved a lot of\nreal world success",
    "start": "1638610",
    "end": "1645419"
  },
  {
    "text": "over the past two years. So here is like a screenshot\nwe take a few days back",
    "start": "1645420",
    "end": "1652770"
  },
  {
    "text": "on this OGB leaderboard. So OGB is a so-called\nOpen Graph Benchmark.",
    "start": "1652770",
    "end": "1659799"
  },
  {
    "text": "It's the largest benchmark data\nset for graph machine learning.",
    "start": "1659800",
    "end": "1667390"
  },
  {
    "text": "And there are hundreds\nof different submissions from different teams that\nthey are competing with tens",
    "start": "1667390",
    "end": "1673800"
  },
  {
    "text": "of different data sets. So you see that correct\nand smooth, or C&S,",
    "start": "1673800",
    "end": "1680010"
  },
  {
    "text": "really tops the leaderboard. So a lot of solutions,\nthey have this component",
    "start": "1680010",
    "end": "1686010"
  },
  {
    "text": "of correct answers in\ntheir final submission. And this approach is kind of\nintroduced two years back.",
    "start": "1686010",
    "end": "1693030"
  },
  {
    "text": "So it has been like a\ndominant the leaderboard for a long time.",
    "start": "1693030",
    "end": "1698110"
  },
  {
    "text": "So that kind of gives\nsome real motivation that this approach is\nindeed very like a very--",
    "start": "1698110",
    "end": "1706510"
  },
  {
    "text": "perform pretty well in reality. So here is the motivation\nfor correct and smooth.",
    "start": "1706510",
    "end": "1715360"
  },
  {
    "text": "We have learned\nthis two approach. One is regarding label\npropagation on networks.",
    "start": "1715360",
    "end": "1720520"
  },
  {
    "text": "And another is for\ngraph neural networks that perform trainable like\nneighborhood aggregation",
    "start": "1720520",
    "end": "1727750"
  },
  {
    "text": "on networks. What we see is that for\nthis label propagation, we have assumed that suppose\nnodes that are connected,",
    "start": "1727750",
    "end": "1735880"
  },
  {
    "text": "they tend to share\nsimilar labels. And this will work because this\nhomophily feature, homophily",
    "start": "1735880",
    "end": "1744580"
  },
  {
    "text": "like a phenomenon, widely exists\nin real world social networks. And this approach is\npretty easy and fast",
    "start": "1744580",
    "end": "1751600"
  },
  {
    "text": "to compute because in\nthe end, what we really need is just some sparse\nmatrix vector multiplication.",
    "start": "1751600",
    "end": "1758120"
  },
  {
    "text": "So it's very fast to compute. But the issue is that the\napproach does not come with",
    "start": "1758120",
    "end": "1765400"
  },
  {
    "text": "or does not leverage the feature\ninformation in the network. So our motivation is to really\nleverage such information",
    "start": "1765400",
    "end": "1772570"
  },
  {
    "text": "in the network. On the right hand side,\nfor graph neural network,",
    "start": "1772570",
    "end": "1778659"
  },
  {
    "text": "we made an assumption that\nthe label of a given node really depend on this\nneighborhood structure.",
    "start": "1778660",
    "end": "1784750"
  },
  {
    "text": "So suppose two nodes share the\nsame neighborhood structure, we will get a same\nembedding for them,",
    "start": "1784750",
    "end": "1790750"
  },
  {
    "text": "and then definitely\nassume that they should share the same label. And it works because in\nmost real world cases,",
    "start": "1790750",
    "end": "1799840"
  },
  {
    "text": "the nodes come with very\ninformative features. So by iteratively\naggregating the information",
    "start": "1799840",
    "end": "1807160"
  },
  {
    "text": "across a network, we hope\nthat the neural network will be able to learn useful\ninformation from the network",
    "start": "1807160",
    "end": "1812980"
  },
  {
    "text": "structure in the features. The limitation of GNN\nis that it is definitely",
    "start": "1812980",
    "end": "1818200"
  },
  {
    "text": "slower than label\npropagation because it has tons of trainable parameters. And also, the computation\ncan be irregular.",
    "start": "1818200",
    "end": "1826150"
  },
  {
    "text": "Meaning that it will more\nchallenging to optimize through deep learning framework.",
    "start": "1826150",
    "end": "1831210"
  },
  {
    "text": "So like our motivation\nis then, can we assume that labels\nare correlated",
    "start": "1831210",
    "end": "1836480"
  },
  {
    "text": "and try to simplify the\ncomputation like for graph",
    "start": "1836480",
    "end": "1841490"
  },
  {
    "text": "neural network? So based on these two\nside of like motivation,",
    "start": "1841490",
    "end": "1847160"
  },
  {
    "text": "the researchers have\nproposed this approach called correct and smooth.",
    "start": "1847160",
    "end": "1853710"
  },
  {
    "text": "Question? We said here that label\npropagation is fast.",
    "start": "1853710",
    "end": "1859049"
  },
  {
    "text": "But earlier, we also said that\nit could be slow to converge. So what is the--",
    "start": "1859050",
    "end": "1864711"
  },
  {
    "text": "is it still much faster\nthan this, even though it's potentially slow to converge? Like how many iterations\ndoes it take in the graph.",
    "start": "1864711",
    "end": "1871020"
  },
  {
    "text": "Yeah. Good point. I think this claim\nthat the computation",
    "start": "1871020",
    "end": "1876480"
  },
  {
    "text": "is fast is regarding one\nparticular iteration. So one particular iteration is\nreally just one matrix vector",
    "start": "1876480",
    "end": "1883500"
  },
  {
    "text": "multiplication, where you\nhave this sparse adjacency matrix and some vector,\nthe probability value",
    "start": "1883500",
    "end": "1889217"
  },
  {
    "text": "for each node. Yeah. But because the convergence is\nnot guaranteed, in some cases,",
    "start": "1889217",
    "end": "1894792"
  },
  {
    "text": "it's pretty fast. But in other cases,\nit's pretty slow. Suppose it does not converge.",
    "start": "1894792",
    "end": "1900210"
  },
  {
    "text": "And then regardless\nof how slow it is, like I believe\nin most cases, it's definitely faster than\ntraining a deep neural network.",
    "start": "1900210",
    "end": "1907500"
  },
  {
    "text": "Because in that case,\nyou have to apply a lot of stochastic\ngradient descent steps. So that would be\ndefinitely even slower.",
    "start": "1907500",
    "end": "1915639"
  },
  {
    "text": "Necessarily faster than just\nrunning it for inference. Yeah. I think for inference, then,\nbecause you have a trained",
    "start": "1915640",
    "end": "1920981"
  },
  {
    "text": "network, then it's pretty fast. Yeah. Yeah. Cool. ",
    "start": "1920982",
    "end": "1929730"
  },
  {
    "text": "OK. So next, we're going to\ndiscuss a bit further that why",
    "start": "1929730",
    "end": "1936410"
  },
  {
    "text": "we have limitations with\ngraph neural network, and how we can use the\nidea of label propagation",
    "start": "1936410",
    "end": "1943160"
  },
  {
    "text": "to improve the\nprediction of genes. So we have seen this\nfigure multiple times",
    "start": "1943160",
    "end": "1949070"
  },
  {
    "text": "across our lectures. The idea of GNN is really\nto encode different nodes",
    "start": "1949070",
    "end": "1954740"
  },
  {
    "text": "based on its different\nneighborhood structure. And suppose we\nconsider this problem",
    "start": "1954740",
    "end": "1960320"
  },
  {
    "text": "by asking how does GNN\nreally utilize the label information in the network.",
    "start": "1960320",
    "end": "1966440"
  },
  {
    "text": "We will find that gene is kind\nof using the label information very indirectly in\nthe sense that we only",
    "start": "1966440",
    "end": "1971659"
  },
  {
    "text": "use the labels to train the\nweights of a neural network. And then after getting\nthe trained network,",
    "start": "1971660",
    "end": "1977780"
  },
  {
    "text": "we will make predictions\nfor different nodes. And we can find that\nthe prediction for nodes",
    "start": "1977780",
    "end": "1983690"
  },
  {
    "text": "are fully independent or\nuncorrelated because we will apply the network for each\nof the individual nodes",
    "start": "1983690",
    "end": "1992390"
  },
  {
    "text": "without explicitly considering\ntheir potential correlations in the network.",
    "start": "1992390",
    "end": "1998090"
  },
  {
    "text": "And in contrast, the idea\nof the label propagation that we just seen, it uses\nthe labels in prediction",
    "start": "1998090",
    "end": "2005679"
  },
  {
    "text": "pretty directly by directly\npropagating the nodes",
    "start": "2005680",
    "end": "2010900"
  },
  {
    "text": "from like the nodes with labels\nto the nodes without labels. So this is kind of a distinct,\nlike an important difference",
    "start": "2010900",
    "end": "2019480"
  },
  {
    "text": "between GNN and\nlabel propagation.",
    "start": "2019480",
    "end": "2025040"
  },
  {
    "text": "So here is a concrete\nexample that why this making uncorrelated\npredictions may be problematic.",
    "start": "2025040",
    "end": "2033190"
  },
  {
    "text": "In this setting, we are just\ngiving a very simple graph structure. It is essentially\nlike a sequence.",
    "start": "2033190",
    "end": "2040270"
  },
  {
    "text": "So we have, now, v1, v2, up to\nV6 that are connected together. It's a line graph or a sequence.",
    "start": "2040270",
    "end": "2047290"
  },
  {
    "text": "The input to a\ngraph neural network will be the local\nneighborhood structure. So to encode node v1,\nsuppose, the neural network",
    "start": "2047290",
    "end": "2056440"
  },
  {
    "text": "has one layer. To encode node V1, we will look\nat it as one hop neighbor v2.",
    "start": "2056440",
    "end": "2061870"
  },
  {
    "text": "And when we encode\nthe node v2, we will look at v1 v3, et cetera. So this will be the input to\na given graph neural network.",
    "start": "2061870",
    "end": "2070780"
  },
  {
    "text": "And let's examine\nthe predictions and see how the model may learn\nfrom the ground truth label",
    "start": "2070780",
    "end": "2078070"
  },
  {
    "text": "to make such predictions. And our claim is that\nsuppose the node features are",
    "start": "2078070",
    "end": "2085669"
  },
  {
    "text": "overwhelmingly\npredictive, then making such uncorrelated predictions\nfor each of the nodes",
    "start": "2085670",
    "end": "2091489"
  },
  {
    "text": "may be OK. So what I mean is that those\nlike a white and gray boxes",
    "start": "2091489",
    "end": "2101210"
  },
  {
    "text": "are the node features. And because in this case, the\nfeatures of different nodes are informative, in the\nend, the neural network",
    "start": "2101210",
    "end": "2108829"
  },
  {
    "text": "will be able to generate\ndifferent embeddings. And therefore, make\nlike a predictions",
    "start": "2108830",
    "end": "2114320"
  },
  {
    "text": "that are aligning with the\nlabels in the original network. So the claim is that\nsuppose the features are",
    "start": "2114320",
    "end": "2121730"
  },
  {
    "text": "informative for the prediction\ntask, then this paradigm of GNN",
    "start": "2121730",
    "end": "2127190"
  },
  {
    "text": "that first compute embedding,\nand then making the prediction is OK.",
    "start": "2127190",
    "end": "2133480"
  },
  {
    "text": "However, we can easily construct\nsome like a failure mode for this particular problem.",
    "start": "2133480",
    "end": "2140290"
  },
  {
    "text": "Let's say, we are still\ngiving the same network, but then we're labeling the\nnetwork with different colors.",
    "start": "2140290",
    "end": "2147460"
  },
  {
    "text": "So in this case, some of the\nnodes are labeled as red. Some of them are\nlabeled as blue.",
    "start": "2147460",
    "end": "2155460"
  },
  {
    "text": "The issue we found\nhere is that GNNs will fail to make\ndifferent predictions",
    "start": "2155460",
    "end": "2160950"
  },
  {
    "text": "in this particular case. And suppose, we examine the\ninput to the GNN for node v1",
    "start": "2160950",
    "end": "2167859"
  },
  {
    "text": "and the input for node\nv6, we find that the input",
    "start": "2167860",
    "end": "2173200"
  },
  {
    "text": "are exactly the same. Because the embedding is\nover like a small subgraph",
    "start": "2173200",
    "end": "2179650"
  },
  {
    "text": "with two nodes. And one node has a wide\nas the a node feature.",
    "start": "2179650",
    "end": "2185799"
  },
  {
    "text": "Another node has a gray box\nas the original feature. And this is the same\nfor node v6 right",
    "start": "2185800",
    "end": "2192490"
  },
  {
    "text": "it has exactly the same\nneighborhood structure. So suppose we want to encode\nthese two nodes using graph",
    "start": "2192490",
    "end": "2199570"
  },
  {
    "text": "neural network,\nthen what we found is that the embedding for\nnode v1 and v6 are identical.",
    "start": "2199570",
    "end": "2206890"
  },
  {
    "text": "And therefore, we will\nmake the same prediction for these two nodes.",
    "start": "2206890",
    "end": "2212000"
  },
  {
    "text": "However, remember that\nin this prediction task, we label the nodes differently. So on the one hand, we\nlabel node v1 as red.",
    "start": "2212000",
    "end": "2220090"
  },
  {
    "text": "On the other hand, we\nlabel the node v6 as blue. So this kind of construct like\na contradictory prediction",
    "start": "2220090",
    "end": "2229490"
  },
  {
    "text": "for the ground truth label. And kind of demonstrate\nwhy it could be",
    "start": "2229490",
    "end": "2234859"
  },
  {
    "text": "a problematic like a solution. Suppose we use GNN\nfor this type of task.",
    "start": "2234860",
    "end": "2240930"
  },
  {
    "text": "So this is really like a\nproblem we have encountered. And this is because\nthe features of a node",
    "start": "2240930",
    "end": "2247940"
  },
  {
    "text": "are no longer very\npredictive for the task that we care about. So suppose we simply aggregate\nbased on the node neighborhood",
    "start": "2247940",
    "end": "2256940"
  },
  {
    "text": "structure, we are still not\nable to make predictions for this particular task.",
    "start": "2256940",
    "end": "2263750"
  },
  {
    "text": "In comparison, suppose, we\njust run label propagation, which ignores node label--",
    "start": "2263750",
    "end": "2270080"
  },
  {
    "text": "sorry, ignores node\nfeature, it will still perform much better than GNN.",
    "start": "2270080",
    "end": "2276020"
  },
  {
    "text": "For example, given\nthis same network, we remove the label of\nnode v1 and node v6,",
    "start": "2276020",
    "end": "2283010"
  },
  {
    "text": "and we will propagate the label\nusing the remaining nodes.",
    "start": "2283010",
    "end": "2288140"
  },
  {
    "text": "Then in the end, we will\nfind that label propagation will assign very high likelihood\nfor node v1 of being red.",
    "start": "2288140",
    "end": "2295790"
  },
  {
    "text": "And then we will have very\nhigh likelihood for node v6, labeling as blue.",
    "start": "2295790",
    "end": "2302280"
  },
  {
    "text": "So label propagation,\nalthough being simple, will kind of fix this failure\nmode for graph neural network.",
    "start": "2302280",
    "end": "2308585"
  },
  {
    "text": " So let's formally introduce--\nbased on this motivation,",
    "start": "2308585",
    "end": "2316660"
  },
  {
    "text": "let's formally introduce\ncorrect and smooth as approach to bridge the gap of\ngraph neural network",
    "start": "2316660",
    "end": "2323020"
  },
  {
    "text": "and then label propagation. So our setting is the\nsame as we have seen.",
    "start": "2323020",
    "end": "2329660"
  },
  {
    "text": "So we have a network. Some of them are labeled. Some of nodes are not labeled. And then we have a\nlot of feature vectors",
    "start": "2329660",
    "end": "2338049"
  },
  {
    "text": "that are associated\nwith each of the node. And correct and smooth\nhas a three-step procedure",
    "start": "2338050",
    "end": "2345250"
  },
  {
    "text": "to make predictions on this\nsemi-supervised learning setting. The first step is to\ntrain a base predictor.",
    "start": "2345250",
    "end": "2352690"
  },
  {
    "text": "And we will later visit the\noptions of the base predictor. It can be as simple as a\nmultilayer perceptron, that",
    "start": "2352690",
    "end": "2360760"
  },
  {
    "text": "only make predictions\nfor each of the node. Or, it can be like a fully\nfunctional graph neural",
    "start": "2360760",
    "end": "2365860"
  },
  {
    "text": "network. And then given the\nbase predictor, we are going to predict a soft\nlabels for all of the node.",
    "start": "2365860",
    "end": "2373940"
  },
  {
    "text": "But of course, this\nprediction is not perfect based on the limitations\nthat we have just discussed.",
    "start": "2373940",
    "end": "2379470"
  },
  {
    "text": "So in the final step,\ncorrect and smooth, we will post-process\nthe predictions based",
    "start": "2379470",
    "end": "2384530"
  },
  {
    "text": "on the graph structure we have. And this way, we can\nobtain the final prediction",
    "start": "2384530",
    "end": "2390109"
  },
  {
    "text": "for all of the nodes by\ncorrecting and smoothing the prediction. So this is the overall\nalgorithm outline for CNNs.",
    "start": "2390110",
    "end": "2399635"
  },
  {
    "text": " OK. So the first step is to\ndefine a base predictor that",
    "start": "2399635",
    "end": "2406740"
  },
  {
    "text": "can make the first\nround predictions based on the attributes of the node.",
    "start": "2406740",
    "end": "2411810"
  },
  {
    "text": "And then we can get a soft\nprediction for a given node. Like for example,\nthe most simple case",
    "start": "2411810",
    "end": "2418770"
  },
  {
    "text": "can be a linear model or a\nmultilayer perceptron just build over the node features.",
    "start": "2418770",
    "end": "2425520"
  },
  {
    "text": "And of course, we can also\nuse a graph neural network to be such a base predictor.",
    "start": "2425520",
    "end": "2431560"
  },
  {
    "text": "So in our case, with\nthe base model will do is that we will consume\nthis network as the input.",
    "start": "2431560",
    "end": "2437740"
  },
  {
    "text": "And then we will generate a\nsoft label for each of the node. The reason it is soft because\nwe are making a prediction using",
    "start": "2437740",
    "end": "2446400"
  },
  {
    "text": "a neural network\nand the prediction is that will be the\nmodel like guess",
    "start": "2446400",
    "end": "2452610"
  },
  {
    "text": "on the label of a given node. So in most cases, it will\nbe a soft predictor vector.",
    "start": "2452610",
    "end": "2458920"
  },
  {
    "text": "So this is the first step that\nwe want to set up a base model, and then generate the first\nintuition of a given node.",
    "start": "2458920",
    "end": "2470220"
  },
  {
    "text": "And then given the\ntrend based predictor, we'll then obtain the soft\nlabel for all the existing",
    "start": "2470220",
    "end": "2478579"
  },
  {
    "text": "node in the network. So the idea that\nkind of distinguish",
    "start": "2478580",
    "end": "2485480"
  },
  {
    "text": "correct consumers\nand label propagation is that we want to\ngenerate prediction for all of the nodes in the network.",
    "start": "2485480",
    "end": "2492140"
  },
  {
    "text": "For label propagation, we\nonly generate the likelihood for the unknown nodes.",
    "start": "2492140",
    "end": "2498030"
  },
  {
    "text": "But for correct and\nsmooth, we really want to generate the prediction\nfor all of the nodes.",
    "start": "2498030",
    "end": "2503150"
  },
  {
    "text": "And the reason we will\ntalk about it later, that is, we want to\nleverage the difference",
    "start": "2503150",
    "end": "2508430"
  },
  {
    "text": "between our prediction\nand the ground truth to compute the error. And we want to propagate the\nerror across the network.",
    "start": "2508430",
    "end": "2516720"
  },
  {
    "text": "And after getting\nthis soft predictions for each of the\nnodes in the network, our motivation is that can we\nleverage the graph structure,",
    "start": "2516720",
    "end": "2525170"
  },
  {
    "text": "and based on the\nassumption of homophily to fine tune those predictions\nand making them more",
    "start": "2525170",
    "end": "2532350"
  },
  {
    "text": "accurate and more realistic.  So let's come to the central\npart of correct and smooth,",
    "start": "2532350",
    "end": "2541860"
  },
  {
    "text": "which consists of first,\ncorrecting the prediction, and then smoothing\nthe prediction.",
    "start": "2541860",
    "end": "2547510"
  },
  {
    "text": "So these are the post-processing\nstep we want to make. Suppose, we have got like\nthe first round prediction",
    "start": "2547510",
    "end": "2553560"
  },
  {
    "text": "from a base predictor. So the first step is\nthe correct stage,",
    "start": "2553560",
    "end": "2560950"
  },
  {
    "text": "where we want to correct the\npredictions made from this base",
    "start": "2560950",
    "end": "2566680"
  },
  {
    "text": "predictors, based on the\ngraph neighborhood structure. And what we'll do is to\ncompute the error for the nodes",
    "start": "2566680",
    "end": "2576220"
  },
  {
    "text": "that we have the label. And we want to propagate such\nerrors through the network.",
    "start": "2576220",
    "end": "2581380"
  },
  {
    "text": "And we'll talk about\nin more detail later. And the idea of the\nsmooth step is then,",
    "start": "2581380",
    "end": "2587890"
  },
  {
    "text": "we will smooth the predictions\nacross the network. And essentially,\nthis smooth step",
    "start": "2587890",
    "end": "2593830"
  },
  {
    "text": "is very similar to what we\nsee for label propagation. It's just like a slightly--",
    "start": "2593830",
    "end": "2601569"
  },
  {
    "text": "like you only have\nsome slight variant in the exact formulation. But the basic idea is the\nsame as label propagation.",
    "start": "2601570",
    "end": "2609279"
  },
  {
    "text": "So we will introduce\nthe concrete step of how to correct\nthe prediction,",
    "start": "2609280",
    "end": "2614559"
  },
  {
    "text": "and then how to\nsmooth the prediction. ",
    "start": "2614560",
    "end": "2620730"
  },
  {
    "text": "So first step is\nthe correct step. And our key idea is\nthat we want to expect",
    "start": "2620730",
    "end": "2626510"
  },
  {
    "text": "the arrow in the\nBayes prediction to be positively correlated\nacross the network.",
    "start": "2626510",
    "end": "2632460"
  },
  {
    "text": "So we're kind of slightly change\nthe assumption of homophily. So remember, in\nhomophily, we assume",
    "start": "2632460",
    "end": "2639230"
  },
  {
    "text": "that the label of a given\nnode is highly correlated based on the network structure.",
    "start": "2639230",
    "end": "2646349"
  },
  {
    "text": "And here, the assumption\nis that the errors that are made through\na neural network",
    "start": "2646350",
    "end": "2652280"
  },
  {
    "text": "also have this\nhomophily structure. So the errors are\nhighly correlated.",
    "start": "2652280",
    "end": "2657980"
  },
  {
    "text": "And based on this\nassumption, we can get the like a\nconclusion that suppose",
    "start": "2657980",
    "end": "2663080"
  },
  {
    "text": "the node, like the\nerror of a node, increases like at a certain--",
    "start": "2663080",
    "end": "2670110"
  },
  {
    "text": "sorry, given a certain node. Suppose this error\nis huge, we want to argue that such\nerror will similarly",
    "start": "2670110",
    "end": "2678560"
  },
  {
    "text": "appear in its neighbors. And based on this conclusion\nwe find, the exact algorithm",
    "start": "2678560",
    "end": "2686220"
  },
  {
    "text": "that we want to spread\nsuch uncertainty or spread those errors\nacross the network",
    "start": "2686220",
    "end": "2693210"
  },
  {
    "text": "to update our prediction. Here is a concrete example\nof the correct stage.",
    "start": "2693210",
    "end": "2701080"
  },
  {
    "text": "So as we said, we will\nget the prediction vector",
    "start": "2701080",
    "end": "2706090"
  },
  {
    "text": "for each of the\nnode in the network. And that includes\nthe labeled nodes. And because we know the ground\ntruth label for certain nodes,",
    "start": "2706090",
    "end": "2714580"
  },
  {
    "text": "we'll be able to compute\nthe error that we make for those label nodes.",
    "start": "2714580",
    "end": "2720850"
  },
  {
    "text": "For example, for\nthis green node 7, we know that the ground\ntruth label should be 1, 0.",
    "start": "2720850",
    "end": "2728470"
  },
  {
    "text": "Meaning that it's\nlabeled as green. And we have the predictions\nfrom the base predictor.",
    "start": "2728470",
    "end": "2735670"
  },
  {
    "text": "And the prediction\nwas 0.95 and 0.05.",
    "start": "2735670",
    "end": "2740770"
  },
  {
    "text": "And based on the ground\ntruth and the prediction, we'll be able to\ncompute the error.",
    "start": "2740770",
    "end": "2746329"
  },
  {
    "text": "So that describes the\nerror the base model has made on this particular node.",
    "start": "2746330",
    "end": "2753070"
  },
  {
    "text": "And similarly, we can compute\nthe error for all the remaining label node in the network.",
    "start": "2753070",
    "end": "2760380"
  },
  {
    "text": "And then for the\nremaining unlabeled nodes, because we never\nknow the ground truth, so we wouldn't be able\nto compute the error.",
    "start": "2760380",
    "end": "2768860"
  },
  {
    "text": "After this stage, we'll\nnow characterize the errors",
    "start": "2768860",
    "end": "2773930"
  },
  {
    "text": "that we have made\non this network. ",
    "start": "2773930",
    "end": "2779570"
  },
  {
    "text": "And then we will update\nthe arrow along the edges or along the graph\nstructure that we have seen.",
    "start": "2779570",
    "end": "2787020"
  },
  {
    "text": "And this is based\non our motivation, talk about earlier,\nthat is the error follow the homophily\nstructure on the network.",
    "start": "2787020",
    "end": "2797339"
  },
  {
    "text": "And in particular, we\nwill diffuse the error along the edges using\nthis diffusion matrix",
    "start": "2797340",
    "end": "2804290"
  },
  {
    "text": "A, tilde A, that is\ndefined in the next slide. So in particular,\nthe update algorithm",
    "start": "2804290",
    "end": "2813320"
  },
  {
    "text": "can be written as this. So we have the error\nvector for each",
    "start": "2813320",
    "end": "2818930"
  },
  {
    "text": "of the nodes in the network. We will sum the error\nwith the propagated error",
    "start": "2818930",
    "end": "2827870"
  },
  {
    "text": "with certain hyperparameter\nthat kind of do a weighted sum.",
    "start": "2827870",
    "end": "2833040"
  },
  {
    "text": "So after computing\nthe error messages and then computing the\npropagated error message",
    "start": "2833040",
    "end": "2839970"
  },
  {
    "text": "and sum them up, we can\nget the next step error for time t plus 1.",
    "start": "2839970",
    "end": "2846210"
  },
  {
    "text": "And then we will repeat\nthis process iteratively until they converge. So this is very similar\nto label propagation.",
    "start": "2846210",
    "end": "2854760"
  },
  {
    "text": "And also, similar to\nthe PageRank algorithm. ",
    "start": "2854760",
    "end": "2862780"
  },
  {
    "text": "So what is this normalized\nadjacency matrix? We will define the\nnormalized adjacency matrix",
    "start": "2862780",
    "end": "2871000"
  },
  {
    "text": "as the diagonal matrix\nof the node degree power",
    "start": "2871000",
    "end": "2876220"
  },
  {
    "text": "to the negative 0.5. And then times the\nadjacency matrix, and then",
    "start": "2876220",
    "end": "2881950"
  },
  {
    "text": "times this diagonal\nnode degree matrix. And why this formulation\nis desirable?",
    "start": "2881950",
    "end": "2887950"
  },
  {
    "text": "We will talk about\nin the next slide. And in practice, we also\nneed to add a self-loop",
    "start": "2887950",
    "end": "2893770"
  },
  {
    "text": "to this adjacency matrix, so\nthat we can keep or retain the information\nfor a given node.",
    "start": "2893770",
    "end": "2901430"
  },
  {
    "text": "And this t, this\nkind of the degree of matrix, which is\na diagonal matrix",
    "start": "2901430",
    "end": "2907839"
  },
  {
    "text": "tells about the\nspecific degree for each of the node in the network.",
    "start": "2907840",
    "end": "2913190"
  },
  {
    "text": "So this setting is first\nproposed in this paper in 2013.",
    "start": "2913190",
    "end": "2922680"
  },
  {
    "text": "So why do we want to have\nthis exact formulation for the diffusion matrix?",
    "start": "2922680",
    "end": "2928000"
  },
  {
    "text": "So this is based on the\nfact that the eigenvalue",
    "start": "2928000",
    "end": "2933510"
  },
  {
    "text": "of this matrix are always in\nthe range of negative 1 to 1.",
    "start": "2933510",
    "end": "2939600"
  },
  {
    "text": "And in particular, we can\nfind that the eigenvector of this normalized diffusion\nmatrix is D 1 over 2 times 1.",
    "start": "2939600",
    "end": "2952440"
  },
  {
    "text": "And then the corresponding\neigenvalue is 1. The reason for that finding\ncan be proved as follows.",
    "start": "2952440",
    "end": "2961390"
  },
  {
    "text": "So we manually multiply the\nnormalized diffusion matrix",
    "start": "2961390",
    "end": "2968010"
  },
  {
    "text": "with this eigenvector. And we can reason\nthat in the end,",
    "start": "2968010",
    "end": "2975990"
  },
  {
    "text": "the computer result is identical\nto 1 times this eigenvector.",
    "start": "2975990",
    "end": "2982450"
  },
  {
    "text": "So that kind of\nconclude the first. This D 1 over 2 times 1 is like\nan eigenvector of this matrix.",
    "start": "2982450",
    "end": "2991359"
  },
  {
    "text": "And second, the eigenvalue is 1. And this derivation\nutilize the finding",
    "start": "2991360",
    "end": "2998380"
  },
  {
    "text": "that suppose, we multiply\nthe adjacency matrix with 0,",
    "start": "2998380",
    "end": "3003555"
  },
  {
    "text": "1 vector, and then it is equal\nto multiply the diagonal degree",
    "start": "3003555",
    "end": "3010619"
  },
  {
    "text": "matrix with the 0, 1 vector. Because by computing\nwith both approaches,",
    "start": "3010620",
    "end": "3016230"
  },
  {
    "text": "we can get the\nnode degree vector. So using this property,\nwe'll be able to show",
    "start": "3016230",
    "end": "3022020"
  },
  {
    "text": "that the largest eigenvalue\nof this matrix is actually 1. And we can also find\nthe eigenvector.",
    "start": "3022020",
    "end": "3030270"
  },
  {
    "text": "So why this finding\nis important? This is because with\nthis derivation,",
    "start": "3030270",
    "end": "3035730"
  },
  {
    "text": "we can see that this normalized\nmatrix is well behaved. Like suppose, we power the\nmatrix for any k iteration,",
    "start": "3035730",
    "end": "3045600"
  },
  {
    "text": "because even we power\nit to the k iteration, the eigenvalues are still like\ncontrolled within the range",
    "start": "3045600",
    "end": "3053610"
  },
  {
    "text": "of negative 1 to 1. And then the largest\neigenvalue is always 1.",
    "start": "3053610",
    "end": "3060930"
  },
  {
    "text": "And with this nice property,\nsuppose we like iteratively",
    "start": "3060930",
    "end": "3066180"
  },
  {
    "text": "apply this like a normalized\nor diffusion matrix",
    "start": "3066180",
    "end": "3071579"
  },
  {
    "text": "to a vector in a given\nnetwork, the output is always",
    "start": "3071580",
    "end": "3076830"
  },
  {
    "text": "normalized. So we wouldn't get\nlike an exploded value of suppose we\nrepeatedly propagate",
    "start": "3076830",
    "end": "3084960"
  },
  {
    "text": "the error on the network. So in summary, this\nlike a slide tells",
    "start": "3084960",
    "end": "3090540"
  },
  {
    "text": "us like why we want to use\nthis normalized diffusion matrix to propagate\nerrors in the network.",
    "start": "3090540",
    "end": "3096645"
  },
  {
    "start": "3096645",
    "end": "3102520"
  },
  {
    "text": "And there is also an\nintuitive understanding that why this\nformulation makes sense.",
    "start": "3102520",
    "end": "3108620"
  },
  {
    "text": "So suppose, we look\nat the specific values of this normalized\nadjacency matrix.",
    "start": "3108620",
    "end": "3115300"
  },
  {
    "text": "It is really like a 1\nover the square root of di and square root dj.",
    "start": "3115300",
    "end": "3121985"
  },
  {
    "text": "And the intuitive way to\nunderstand this property is that suppose, there are\ntwo nodes that are connected.",
    "start": "3121985",
    "end": "3129580"
  },
  {
    "text": "And they have a very few\nneighbors of each other, then this value of\nAij is pretty large.",
    "start": "3129580",
    "end": "3138700"
  },
  {
    "text": "And in this particular\ncase, the value is one because each\nnode has a degree of 1.",
    "start": "3138700",
    "end": "3145180"
  },
  {
    "text": "And suppose, these two\nnodes are connected. But then, each of them has\na lot of other neighbors.",
    "start": "3145180",
    "end": "3152260"
  },
  {
    "text": "And in that case,\nthe value of Aij is small because the degree\nof each of the node, i and j,",
    "start": "3152260",
    "end": "3160960"
  },
  {
    "text": "is pretty large. So this is the kind of\nintuition to understand what is the effect of\nthis normalization.",
    "start": "3160960",
    "end": "3168200"
  },
  {
    "text": " So having defined this\npropagation matrix,",
    "start": "3168200",
    "end": "3176110"
  },
  {
    "text": "A will be able to\ndiffuse the training error across the network.",
    "start": "3176110",
    "end": "3181310"
  },
  {
    "text": "So using the concrete\nexample we have derived here, we will start with\ncomputed error",
    "start": "3181310",
    "end": "3188860"
  },
  {
    "text": "from the base predictor\nand the ground truth. And then we will propagate\nsuch error across the network.",
    "start": "3188860",
    "end": "3196640"
  },
  {
    "text": "In this case, we propagate\nit for three iterations. And then you can see all the\nerror vectors are now updated.",
    "start": "3196640",
    "end": "3205910"
  },
  {
    "text": "And we can find that some\nregions of the network have a less erroneous part.",
    "start": "3205910",
    "end": "3213079"
  },
  {
    "text": "So in this case, the error bars\nare smaller for these nodes. And for some remaining\npart of the graph,",
    "start": "3213080",
    "end": "3222940"
  },
  {
    "text": "the error are larger. So there are more erroneous. So this is kind of the\nconvergence result,",
    "start": "3222940",
    "end": "3230290"
  },
  {
    "text": "suppose you apply this error\ndiffusion for multiple steps. ",
    "start": "3230290",
    "end": "3238329"
  },
  {
    "text": "And in the end, like the way we\nutilize this diffuse training error is that we will\nstart like summing up",
    "start": "3238330",
    "end": "3247960"
  },
  {
    "text": "our soft predictions and\nthe diffuse training errors. So remember, the soft\nlabels are obtained",
    "start": "3247960",
    "end": "3255160"
  },
  {
    "text": "through the base\npredictor, either like a multi-layer\nperceptron or a GNN.",
    "start": "3255160",
    "end": "3262150"
  },
  {
    "text": "And then we will sum\nthe soft prediction with the diffuse training error.",
    "start": "3262150",
    "end": "3267790"
  },
  {
    "text": "And this will be like leading\nto the final predictions",
    "start": "3267790",
    "end": "3272830"
  },
  {
    "text": "after this correct stage. And there is like\na hyperparameter",
    "start": "3272830",
    "end": "3278260"
  },
  {
    "text": "s that scale the prediction-- scale the error of\nthe diffuse errors.",
    "start": "3278260",
    "end": "3286850"
  },
  {
    "text": "So this kind of finish the first\nstep of the correct and smooth, where we have the soft\nprediction and then",
    "start": "3286850",
    "end": "3292930"
  },
  {
    "text": "the diffuse training error. Question? So we have this\ntraining error we use to train the place directly?",
    "start": "3292930",
    "end": "3299800"
  },
  {
    "text": "No. It fully like a post-processing\nstage in the sense that the model is always trained\ndirectly with the labels.",
    "start": "3299800",
    "end": "3307210"
  },
  {
    "text": "But then this like an\nupdate on the predictions is fully post-processing stage.",
    "start": "3307210",
    "end": "3315130"
  },
  {
    "text": "Question? Is it possible to\napply this procedure when you are running\ninference on a different graph",
    "start": "3315130",
    "end": "3322990"
  },
  {
    "text": "than you trained on, so\nyou don't have access to any ground truth\ninformation for the new graph?",
    "start": "3322990",
    "end": "3329830"
  },
  {
    "text": "I think it is not possible based\non the original algorithm here.",
    "start": "3329830",
    "end": "3335920"
  },
  {
    "text": "Because it is really like\npropagating the error. So suppose, we do not\nhave ground truth,",
    "start": "3335920",
    "end": "3341020"
  },
  {
    "text": "then we couldn't\npropragate the error. So that's why in the beginning\nof this lecture, we talk about, we focus on this transductive\nlearning setting,",
    "start": "3341020",
    "end": "3347830"
  },
  {
    "text": "where we only have one network. But I think you raised like\na good point in the sense that we could instead of using\nthis error propagation, we",
    "start": "3347830",
    "end": "3356800"
  },
  {
    "text": "kind of revert to some smooth\nversion of your base predictor. For example, you can propagate\nyour prediction scores",
    "start": "3356800",
    "end": "3366160"
  },
  {
    "text": "for different nodes. And you can still\nsmooth your prediction. And that does not rely\non the ground truth",
    "start": "3366160",
    "end": "3373270"
  },
  {
    "text": "as an alternative algorithm. Another question\nabout that then.",
    "start": "3373270",
    "end": "3378569"
  },
  {
    "text": "Wouldn't it be a natural idea\nto take algorithm two, but also",
    "start": "3378570",
    "end": "3386820"
  },
  {
    "text": "include the labels or predicted\nlabels and the messages that you're passing? So then you solve the problem\nthat the labels are not",
    "start": "3386820",
    "end": "3393780"
  },
  {
    "text": "in the messages, but\nalso you can apply it in the inductive setting. Why not just do that\ninstead of all of this?",
    "start": "3393780",
    "end": "3400860"
  },
  {
    "text": "So you're suggesting combining\nthe labels in the GNN message passing. Yeah.",
    "start": "3400860",
    "end": "3405869"
  },
  {
    "text": "I think one motivation\nof this approach is that this kind of fully model\nagnostic in the sense that you",
    "start": "3405870",
    "end": "3413160"
  },
  {
    "text": "can choose any\nbase predictor like to make the first\nround prediction, and then like update\nthe prediction.",
    "start": "3413160",
    "end": "3420369"
  },
  {
    "text": "So this is also kind\nof another reason why this approach is very\npopular because you can always",
    "start": "3420370",
    "end": "3425910"
  },
  {
    "text": "make prediction with any\nof your favorite model. And in the end, you\nupdate the prediction with this simple approach.",
    "start": "3425910",
    "end": "3432090"
  },
  {
    "text": "But I think what you suggest is\ndefinitely another good idea. Like how to directly leverage\nthe label in message passing.",
    "start": "3432090",
    "end": "3439540"
  },
  {
    "text": "And we will visit like\na simplified version in the final of this lecture.",
    "start": "3439540",
    "end": "3444800"
  },
  {
    "text": "We'll talk about this\nmasked label prediction. I think kind of similar\nto what you proposed.",
    "start": "3444800",
    "end": "3451180"
  },
  {
    "text": "Another question? Can you describe the\nhyperparameter alpha is controlling? Yeah.",
    "start": "3451180",
    "end": "3456570"
  },
  {
    "text": "So this scale of s\ncontrols like how important",
    "start": "3456570",
    "end": "3462030"
  },
  {
    "text": "we think the homophily\nexists in a given network. Let's say, we said it has to\nbe very small, or even to zero,",
    "start": "3462030",
    "end": "3470339"
  },
  {
    "text": "then basically, we\nsaid that we do not believe that there's any\nhomophily in the network structure.",
    "start": "3470340",
    "end": "3475570"
  },
  {
    "text": "And on the contrary, suppose\nwe set x to be larger, then we want to emphasize that\nthe homophily assumption holds",
    "start": "3475570",
    "end": "3483240"
  },
  {
    "text": "in the network and we want to\ncorrect the prediction even further.",
    "start": "3483240",
    "end": "3488430"
  },
  {
    "text": "Yeah.  Question? If let's say my problem is a\ngraph classification problem,",
    "start": "3488430",
    "end": "3496529"
  },
  {
    "text": "can I still use like masks\nto basically make this happen and improve my performance?",
    "start": "3496530",
    "end": "3502000"
  },
  {
    "text": "So basically,\nimprove upon the node features that I had initially\nfor the graph classification",
    "start": "3502000",
    "end": "3507030"
  },
  {
    "text": "problem? I think for graph\nclassification, you wouldn't be able to directly\nuse it for the same reason.",
    "start": "3507030",
    "end": "3513690"
  },
  {
    "text": "Because for graph\nclassification, you always apply to a new graph. And for that new\nor unknown graph,",
    "start": "3513690",
    "end": "3520800"
  },
  {
    "text": "you do not have\nthe ground truth. So you couldn't directly use\nthis like an error propagation",
    "start": "3520800",
    "end": "3526560"
  },
  {
    "text": "for graph classification. And in order to improve my\npredictions during training,",
    "start": "3526560",
    "end": "3533000"
  },
  {
    "text": "I mean, let's say, if I\ndon't trust my features, I think that there are some\nerrors in the features that I put in initially into the\nnodes for the graphs using",
    "start": "3533000",
    "end": "3541790"
  },
  {
    "text": "for training. Yeah. I think you can kind\nof diffuse or like",
    "start": "3541790",
    "end": "3548480"
  },
  {
    "text": "update your features\nof your network. But still, like it's\nslightly different from this",
    "start": "3548480",
    "end": "3553793"
  },
  {
    "text": "correct and smooth paradigm. But it's more similar to\nthe label propagation idea. So instead of\npropagating labels,",
    "start": "3553793",
    "end": "3559427"
  },
  {
    "text": "you could propagate\nfeatures in your network to improve the feature. So similar ideas, but\nlike a slightly different",
    "start": "3559427",
    "end": "3566510"
  },
  {
    "text": "from what we discussed here. ",
    "start": "3566510",
    "end": "3572640"
  },
  {
    "text": "OK. So now we have seen the\nfirst part of the algorithm, how to correct the\nprediction based",
    "start": "3572640",
    "end": "3578210"
  },
  {
    "text": "on the errors made by the\nmodel and based on the network structure.",
    "start": "3578210",
    "end": "3583970"
  },
  {
    "text": "The next stage of this\nC&S is smooth operator.",
    "start": "3583970",
    "end": "3589830"
  },
  {
    "text": "So this stage is actually\nidentical to what we introduce about label propagation.",
    "start": "3589830",
    "end": "3596560"
  },
  {
    "text": "The assumption here is that\nthe neighboring nodes tend to share the similar labels.",
    "start": "3596560",
    "end": "3603780"
  },
  {
    "text": "So in this case, for\nthe training nodes, we will use the ground\ntruth hard labels",
    "start": "3603780",
    "end": "3610160"
  },
  {
    "text": "instead of the soft labels. So this is exactly what we\nsee in the label propagation. So instead of updating the\nnode, we have the labels,",
    "start": "3610160",
    "end": "3619279"
  },
  {
    "text": "we will just always enforce them\nto be the ground truth label, like a 1, 0, or 0, 1.",
    "start": "3619280",
    "end": "3624290"
  },
  {
    "text": "And then for all the remaining\nunseen or unknown labels, unknown nodes, we will\nuse what we have seen",
    "start": "3624290",
    "end": "3632865"
  },
  {
    "text": "in the first correct stage. So this way, we can initiate the\nsmooth step for the algorithm.",
    "start": "3632865",
    "end": "3642310"
  },
  {
    "text": "And then the exact\nway of the smoothing is identical to what we see\nin the label propagation,",
    "start": "3642310",
    "end": "3651660"
  },
  {
    "text": "except that we have this\nhyperparameter that smooths-- that control how strong\nthe smoothing factor is.",
    "start": "3651660",
    "end": "3660089"
  },
  {
    "text": " So what we do here is to\ndiffuse the label across edges",
    "start": "3660090",
    "end": "3668359"
  },
  {
    "text": "and update by setting the new\npredictive vector for a given",
    "start": "3668360",
    "end": "3675530"
  },
  {
    "text": "node as the average\nof its neighbors. So this is how to\ninterpret this A times D.",
    "start": "3675530",
    "end": "3683960"
  },
  {
    "text": "And then we'll do a weighted sum\nbetween the diffuse predictions",
    "start": "3683960",
    "end": "3689810"
  },
  {
    "text": "and the original predictions. And the way the weight\nof this weighted sum",
    "start": "3689810",
    "end": "3695870"
  },
  {
    "text": "is also controlled\nby a hyperparameter. And this hyperparameter\nbasically tells how strong we believe\nthat the labels should",
    "start": "3695870",
    "end": "3702920"
  },
  {
    "text": "be correlated. ",
    "start": "3702920",
    "end": "3709520"
  },
  {
    "text": "So after the\nsmoothing, you can take the output of the correct\nstage as the input,",
    "start": "3709520",
    "end": "3717460"
  },
  {
    "text": "and then perform smoothing\nfor a few iterations. And let's say, we'd do\nthree-step iterations",
    "start": "3717460",
    "end": "3723430"
  },
  {
    "text": "and set the smoothing\nvector to be 0.8. And one note we\nwant to make here",
    "start": "3723430",
    "end": "3729850"
  },
  {
    "text": "is that you can find the final\nprediction scores are not rigorous probability vector.",
    "start": "3729850",
    "end": "3736100"
  },
  {
    "text": "So you find that a\nlot of them, they do not sum to 1 like\nafter this update stage.",
    "start": "3736100",
    "end": "3743150"
  },
  {
    "text": "So in the end, because we just\ncare about making predictions, we will just pick the class\nwith the maximum score.",
    "start": "3743150",
    "end": "3751579"
  },
  {
    "text": "So what I mean is that, for\nexample, this 0.74 and 0.18,",
    "start": "3751580",
    "end": "3757150"
  },
  {
    "text": "they do not sum to 1. But because we know\nthat the first class has a higher predicted\nscore, then the model",
    "start": "3757150",
    "end": "3763750"
  },
  {
    "text": "will output class I for-- class 0 for this\ngiven prediction.",
    "start": "3763750",
    "end": "3770784"
  },
  {
    "text": " Because the algorithm\ndoes not enforce",
    "start": "3770784",
    "end": "3776980"
  },
  {
    "text": "a normalized\nprobability vector, we will just pick whichever class\nhas a higher predictor score.",
    "start": "3776980",
    "end": "3783280"
  },
  {
    "start": "3783280",
    "end": "3789660"
  },
  {
    "text": "So to summarize, we have\nstart the correct and smooth",
    "start": "3789660",
    "end": "3795180"
  },
  {
    "text": "with a base predictor. And then after\ncorrect and smooth, we will get a\nprediction like this.",
    "start": "3795180",
    "end": "3803550"
  },
  {
    "text": "And we find that\nthe algorithm is able to correct\na lot of mistakes",
    "start": "3803550",
    "end": "3809550"
  },
  {
    "text": "made by the base predictor. And it is kind of very\nsuccessfully correct",
    "start": "3809550",
    "end": "3815490"
  },
  {
    "text": "those issues. For example, some of nodes 8 and\nnode 3, they are misclassified.",
    "start": "3815490",
    "end": "3825210"
  },
  {
    "text": "Suppose we only\nuse the base model. But after the correct\nand smooth stage,",
    "start": "3825210",
    "end": "3832800"
  },
  {
    "text": "note 8 is now correctly\nlabeled as green and node 3 is correctly labeled as red.",
    "start": "3832800",
    "end": "3840130"
  },
  {
    "text": "So this demonstrates\nlike why this method is able to correct the mistakes\nmade with a base predictor.",
    "start": "3840130",
    "end": "3847200"
  },
  {
    "start": "3847200",
    "end": "3853540"
  },
  {
    "text": "So in practice, this approach\nworks extremely well. It can even predict\nlike an update",
    "start": "3853540",
    "end": "3860400"
  },
  {
    "text": "the prediction with a\nvery simple base predictor like a multi-layer perceptron.",
    "start": "3860400",
    "end": "3865980"
  },
  {
    "text": "So for example, consider this\nnode classification task.",
    "start": "3865980",
    "end": "3872020"
  },
  {
    "text": "So it's called OGB\nproducts data set. With the base\nmodel, you will only",
    "start": "3872020",
    "end": "3878670"
  },
  {
    "text": "achieve accuracy of 63,\nwhich is pretty low. So suppose, we do MLP\nplus the small stage only,",
    "start": "3878670",
    "end": "3887940"
  },
  {
    "text": "we can get a performance of 80%. And with the full correct\nand smooth approach,",
    "start": "3887940",
    "end": "3893460"
  },
  {
    "text": "we get like 84% accuracy. And this is almost\nalready on par",
    "start": "3893460",
    "end": "3899010"
  },
  {
    "text": "with the state of\nour performance. So this result demonstrates\nthat even with a simple or naive",
    "start": "3899010",
    "end": "3906960"
  },
  {
    "text": "prediction model, MLP after. The post-processing of\nthese predictions, the model",
    "start": "3906960",
    "end": "3913410"
  },
  {
    "text": "performance can\nbe greatly boost. ",
    "start": "3913410",
    "end": "3921160"
  },
  {
    "text": "To summarize, the idea\nof curriculum schools is to use the graph structure\nand the homophily assumption",
    "start": "3921160",
    "end": "3928839"
  },
  {
    "text": "to update the prediction or\npost-process the prediction. And in the correct step,\nwe make the assumption",
    "start": "3928840",
    "end": "3936190"
  },
  {
    "text": "that the errors follows\nthe homophily assumption. So we can diffuse and\ncorrect the errors",
    "start": "3936190",
    "end": "3944050"
  },
  {
    "text": "that are made from\nthe base predictor. And in the smooth\nstep, we will have",
    "start": "3944050",
    "end": "3950710"
  },
  {
    "text": "the assumption of\nthe labels follows the homophily assumption. And in this case, we will\njust smooth the prediction",
    "start": "3950710",
    "end": "3957160"
  },
  {
    "text": "of the base prediction. And this is a variant of the\nlabel propagation algorithm",
    "start": "3957160",
    "end": "3963040"
  },
  {
    "text": "we have seen earlier. And the strengths\nof this two-step are controlled by\ntwo hyperparameters",
    "start": "3963040",
    "end": "3970750"
  },
  {
    "text": "based on how important\nyou think homophily is for each of the stage.",
    "start": "3970750",
    "end": "3978610"
  },
  {
    "text": "And in practice, this\nis really model agnostic",
    "start": "3978610",
    "end": "3984700"
  },
  {
    "text": "because it can be combined\nwith any base predictor. So people have use it in\ncombination with graph neural",
    "start": "3984700",
    "end": "3992500"
  },
  {
    "text": "networks quite often as well. And we have shown a concrete\nexample that correct and smooth",
    "start": "3992500",
    "end": "3999730"
  },
  {
    "text": "can really achieve\nvery strong performance on this semi-supervised\nnode classification problem.",
    "start": "3999730",
    "end": "4005290"
  },
  {
    "text": "So it's worth considering. Suppose we want to\nimplement your own GNN",
    "start": "4005290",
    "end": "4010440"
  },
  {
    "text": "pipeline in practice. ",
    "start": "4010440",
    "end": "4015800"
  },
  {
    "text": "So, so far, we have talked about\nthe idea of labor progression and then a very\nsuccessful algorithm",
    "start": "4015800",
    "end": "4022460"
  },
  {
    "text": "called correct and smooth. In the final part\nof this lecture, we're going to visit a slightly\ndifferent, but similar idea",
    "start": "4022460",
    "end": "4030110"
  },
  {
    "text": "called masked labor prediction. And this approach is\nsignificantly easier to implement and\nalso very successful.",
    "start": "4030110",
    "end": "4039240"
  },
  {
    "text": "So here is the idea of\nmasked label prediction. So still, our main\nmotivation here",
    "start": "4039240",
    "end": "4047000"
  },
  {
    "text": "is to explicitly\nconsider a label information in a\nnetwork when we make",
    "start": "4047000",
    "end": "4052910"
  },
  {
    "text": "machine learning predictions. And this approach\nis actually inspired from the bird objective that are\nwidely used in natural language",
    "start": "4052910",
    "end": "4062000"
  },
  {
    "text": "processing. So the idea of\nthis bird objective",
    "start": "4062000",
    "end": "4067730"
  },
  {
    "text": "is a pretraining\nstrategy, where we want to make masked word prediction.",
    "start": "4067730",
    "end": "4074700"
  },
  {
    "text": "So more concretely,\ngiven an input, which is a sequence of different\nwords in this transformer",
    "start": "4074700",
    "end": "4083490"
  },
  {
    "text": "training stage,\nwhat we will do is that we will randomly mask\nsome of the input tokens",
    "start": "4083490",
    "end": "4092460"
  },
  {
    "text": "of this sequence of words. And then we will let the\nmodel to predict those values",
    "start": "4092460",
    "end": "4100859"
  },
  {
    "text": "to train the network. So in the end, we will have\nis to have partially observed",
    "start": "4100859",
    "end": "4106380"
  },
  {
    "text": "sequence, and then\npredict the remaining values in the sequence.",
    "start": "4106380",
    "end": "4111818"
  },
  {
    "text": "So this is the motivation of\nthis masked label prediction.",
    "start": "4111819",
    "end": "4117040"
  },
  {
    "text": "So in concretely, we will apply\nthe similar approach to graph--",
    "start": "4117040",
    "end": "4125939"
  },
  {
    "text": "sorry, for node level\nclassification as well. So the idea is that\ninstead of trading labels",
    "start": "4125939",
    "end": "4133229"
  },
  {
    "text": "as some additional attached\nvalue to a given node, we will treat the label\ndirectly as features.",
    "start": "4133229",
    "end": "4141210"
  },
  {
    "text": "But of course, we\nwant to make sure that the model is not cheating. So it's not like using the label\nto predicting the label itself.",
    "start": "4141210",
    "end": "4150100"
  },
  {
    "text": "So what we will do is to\nconstruct a machine learning setting that we use partially\nobserved labels to predict",
    "start": "4150100",
    "end": "4158219"
  },
  {
    "text": "the remaining unobserved\nfeature labels.",
    "start": "4158220",
    "end": "4163410"
  },
  {
    "text": "So what it means is that\nin the training stage, we will corrupt the\nnode label matrix",
    "start": "4163410",
    "end": "4171239"
  },
  {
    "text": "y by randomly masking a portion\nof the node labels to zeros.",
    "start": "4171240",
    "end": "4177509"
  },
  {
    "text": "And then we will\ntreat the remaining like a node labels as\nadditional features of the node.",
    "start": "4177510",
    "end": "4185759"
  },
  {
    "text": "So what we will do is we just\nconcatenate the node features with the masked node labels.",
    "start": "4185760",
    "end": "4192479"
  },
  {
    "text": "So we treat labels as features. And after defining\nthis new node features,",
    "start": "4192479",
    "end": "4199380"
  },
  {
    "text": "we'll use this new node feature\nto predict the masked node labels. So you can see that\nunder this formulation,",
    "start": "4199380",
    "end": "4207480"
  },
  {
    "text": "the model is not cheating. Although it is explicitly using\nthe labels of different nodes,",
    "start": "4207480",
    "end": "4214770"
  },
  {
    "text": "we are making sure\nthat we always use some of the node labels to\npredict the remaining node",
    "start": "4214770",
    "end": "4220440"
  },
  {
    "text": "labels. So this way, there is no direct\nleakage of the node labels.",
    "start": "4220440",
    "end": "4226420"
  },
  {
    "text": "And this is how we do\nat the training time. And suppose, the\nmodel is able to do that like successfully\nin the training time,",
    "start": "4226420",
    "end": "4232860"
  },
  {
    "text": "then at inference time, we will\nemploy all the observed node",
    "start": "4232860",
    "end": "4239159"
  },
  {
    "text": "labels to predict the remaining\nunobserved node label.",
    "start": "4239160",
    "end": "4244570"
  },
  {
    "text": "So this is what we do for\nboth validation and test set in the sense that we\nwill directly concatenate",
    "start": "4244570",
    "end": "4251850"
  },
  {
    "text": "like the feature X with the\nnode label Y as the updated node",
    "start": "4251850",
    "end": "4258330"
  },
  {
    "text": "feature. And then we will use that\ncombined node feature and labels to predict any\nremaining labels in the data",
    "start": "4258330",
    "end": "4264719"
  },
  {
    "text": "set. And you can find that this\napproach of setting up the training and inference\nsplit is very similar to we",
    "start": "4264720",
    "end": "4274000"
  },
  {
    "text": "have seen in the\nlink prediction. And it's for good reason\nbecause both approach",
    "start": "4274000",
    "end": "4280269"
  },
  {
    "text": "can be regarded\nas a special type of self-supervised\nlearning on a network. So in this case, we are\nconstructing the objective",
    "start": "4280270",
    "end": "4290590"
  },
  {
    "text": "based on what we\nhave in the network. So we will make sure that we\nwill use part of the input",
    "start": "4290590",
    "end": "4297550"
  },
  {
    "text": "as the input of the training and\nthe remaining part of the input",
    "start": "4297550",
    "end": "4303070"
  },
  {
    "text": "as the assumed prediction. So in link prediction,\nlike for certain edges,",
    "start": "4303070",
    "end": "4308380"
  },
  {
    "text": "we'll reserve some of the\nedges for message passing. And then using the remaining\nedges as the supervision.",
    "start": "4308380",
    "end": "4315040"
  },
  {
    "text": "And here, similarly, we\nreserve some of the labels as the input for the network.",
    "start": "4315040",
    "end": "4320770"
  },
  {
    "text": "And then use the remaining of\nthe labels as the supervision signals.",
    "start": "4320770",
    "end": "4326190"
  },
  {
    "text": "So that's why this\nalgorithm looks similar. So here, this concludes the\nidea of masked label prediction.",
    "start": "4326190",
    "end": "4334469"
  },
  {
    "text": "And you can see that is a pretty\nsimple and straightforward. You are not like a changing\nany of your machine",
    "start": "4334470",
    "end": "4340670"
  },
  {
    "text": "learning pipeline, except\nthat you set up the features and the label of the network\nis slightly different.",
    "start": "4340670",
    "end": "4347970"
  },
  {
    "text": "And it can be used\nfor any pipeline architecture we have defined.",
    "start": "4347970",
    "end": "4353480"
  },
  {
    "text": " Question?",
    "start": "4353480",
    "end": "4358940"
  },
  {
    "text": "You connect this third\nmethod to label propagation. Can you explain how this setup\nis propagating the labels?",
    "start": "4358940",
    "end": "4367210"
  },
  {
    "text": "Yeah. Well applied. So this setting is like a\nnot like a directly related",
    "start": "4367210",
    "end": "4374960"
  },
  {
    "text": "to some earlier. So in the previous approach\nof labor progression and correct and smooth, we\nexplicit propagate the label",
    "start": "4374960",
    "end": "4382040"
  },
  {
    "text": "information in the network\nthrough like a closed form algorithm. In this case, the model\nis like a implicitly",
    "start": "4382040",
    "end": "4390380"
  },
  {
    "text": "propagating the label\ninformation across the network. So this really depends on\nwhat model you choose here.",
    "start": "4390380",
    "end": "4398659"
  },
  {
    "text": "Let's say, we still use\ngraph neural network for this masked label\nprediction task.",
    "start": "4398660",
    "end": "4404960"
  },
  {
    "text": "Then in that sense, we are\nstill making propagation because GNN is making message\npassing or propagation",
    "start": "4404960",
    "end": "4413300"
  },
  {
    "text": "from certain node features to\nthe remaining node features. And because we treat labels\nas one special node feature.",
    "start": "4413300",
    "end": "4422490"
  },
  {
    "text": "So with GNN message\npassing, we are kind of also passing the label\ninformation across the network.",
    "start": "4422490",
    "end": "4431699"
  },
  {
    "text": "But this is kind of related\nto what model you choose here. So suppose we are not\nusing GNN and using an MLP,",
    "start": "4431700",
    "end": "4437790"
  },
  {
    "text": "then it wouldn't have this a\nnice property of propagation.",
    "start": "4437790",
    "end": "4443820"
  },
  {
    "text": "So this is kind of one way\nto understand this algorithm.",
    "start": "4443820",
    "end": "4448929"
  },
  {
    "text": "Another question? Can you explain the context\nhow is masked label prediction",
    "start": "4448930",
    "end": "4454570"
  },
  {
    "text": "transductive? So the question is why is\nthis approach is transductive?",
    "start": "4454570",
    "end": "4463040"
  },
  {
    "text": "So honestly, for this\napproach, it definitely works for transductive\nsetting, but it also",
    "start": "4463040",
    "end": "4468160"
  },
  {
    "text": "works for inductive setting. So the reason is\nthe setup here, we",
    "start": "4468160",
    "end": "4474700"
  },
  {
    "text": "do not use any information\nparticular to a network. So we are training a model using\nsome of the observed labels,",
    "start": "4474700",
    "end": "4486130"
  },
  {
    "text": "and then all the features to\npredict the unobserved labels. So this approach can be\ngeneralized to a new graph.",
    "start": "4486130",
    "end": "4496090"
  },
  {
    "text": "Because for a new graph, suppose\nwe know some of the labels, then we can still concatenate\nthe feature and the labels",
    "start": "4496090",
    "end": "4502119"
  },
  {
    "text": "to predict the remaining labels. So this can generalize\nto unseen graph as well.",
    "start": "4502120",
    "end": "4510090"
  },
  {
    "text": "And suppose you are asking\nabout the transduction setting, the approach also works because\nfor a given network suppose",
    "start": "4510090",
    "end": "4517880"
  },
  {
    "text": "you train on one special\nsplit of the feature and then",
    "start": "4517880",
    "end": "4522949"
  },
  {
    "text": "the label. Then you can still predict for\nthe remaining observed labels. ",
    "start": "4522950",
    "end": "4534480"
  },
  {
    "text": "So we can conclude, we have\nlearned in this lecture. So we talk about three\ndifferent approaches.",
    "start": "4534480",
    "end": "4541290"
  },
  {
    "text": "But really, the\nkey thing you can-- the key takeaway\nfrom the lecture is that we have introduced some\napproach that can explicitly",
    "start": "4541290",
    "end": "4549960"
  },
  {
    "text": "learn the label\ninformation in a network. So remember like\nearlier, we focused",
    "start": "4549960",
    "end": "4555210"
  },
  {
    "text": "on graph neural network. And it's really\nabout how to learn the structure and the feature.",
    "start": "4555210",
    "end": "4560820"
  },
  {
    "text": "So we didn't explicitly utilize\nlabel in those algorithms.",
    "start": "4560820",
    "end": "4565900"
  },
  {
    "text": "But what we learn today\nis three different options that can explicitly take label\ninformation into consideration.",
    "start": "4565900",
    "end": "4573450"
  },
  {
    "text": "And this approach\ninclude the basic version of label propagation\nthat we propagate",
    "start": "4573450",
    "end": "4578460"
  },
  {
    "text": "labels across the network. And it has a more\nadvanced formulation, where we combine a base\npredictor and label",
    "start": "4578460",
    "end": "4584550"
  },
  {
    "text": "propagation to smooth and\ncorrect the prediction. And we also see another way of\nmasked label prediction that",
    "start": "4584550",
    "end": "4592590"
  },
  {
    "text": "directly construct a\nmachine learning task to let the model incorporate\nthe label information.",
    "start": "4592590",
    "end": "4602020"
  },
  {
    "text": "And then in a higher level, we\nhave concluded our discussion",
    "start": "4602020",
    "end": "4607960"
  },
  {
    "text": "on machine learning with graphs. And we have talked about\nthree main paradigms regarding machine learning on\ngraphs, including",
    "start": "4607960",
    "end": "4614440"
  },
  {
    "text": "the node embedding based\nmethods, GNN label propagation. So for node embedding\nmethod, it does not",
    "start": "4614440",
    "end": "4620410"
  },
  {
    "text": "use any node attributes. Based on the assumption of-- we can train embeddings\nbased on the supposed nodes,",
    "start": "4620410",
    "end": "4628880"
  },
  {
    "text": "nodes are following this\nlike a short random walks. And we construct positive\nnode pairs, supposedly,",
    "start": "4628880",
    "end": "4635500"
  },
  {
    "text": "located in the same random\nwalk like if they co-occur in the same short random walk.",
    "start": "4635500",
    "end": "4642070"
  },
  {
    "text": "And then if not, it\nis a negative example. So the idea of node embedding is\nagnostic to the downstream task",
    "start": "4642070",
    "end": "4650530"
  },
  {
    "text": "is one way to construct a\nself-supervised learning setting. The GNN is more\nprincipled in the sense",
    "start": "4650530",
    "end": "4656949"
  },
  {
    "text": "that it learns an iterative\nneighborhood aggregation function that can process\nthe network structure,",
    "start": "4656950",
    "end": "4664270"
  },
  {
    "text": "as well as the\nnetwork attributes. And finally, today, we introduce\nthe label propagation idea,",
    "start": "4664270",
    "end": "4670780"
  },
  {
    "text": "which has an inductive bias of\nassuming the network follows a homophily property.",
    "start": "4670780",
    "end": "4676750"
  },
  {
    "text": "And then it can explicitly\nincorporate the label information when\nmaking predictions.",
    "start": "4676750",
    "end": "4683210"
  },
  {
    "text": "So these three paradigms all\nhave a very wide application",
    "start": "4683210",
    "end": "4688270"
  },
  {
    "text": "in reality. And to solve a specific\nmachine learning problem, you can either choose like\none of the paradigm here.",
    "start": "4688270",
    "end": "4695350"
  },
  {
    "text": "Or what we discussed\ntoday, you can combine",
    "start": "4695350",
    "end": "4700480"
  },
  {
    "text": "this different approach. For example,\nincorrect and smooth, it kind of combines the graph\nneural network and the label",
    "start": "4700480",
    "end": "4706630"
  },
  {
    "text": "propagation. So with that, I'm going\nto conclude this lecture.",
    "start": "4706630",
    "end": "4711890"
  },
  {
    "text": "Thank you very much. ",
    "start": "4711890",
    "end": "4719000"
  }
]