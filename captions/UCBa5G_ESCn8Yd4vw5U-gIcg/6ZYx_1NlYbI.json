[
  {
    "text": "So I guess let's get started. From today we're going to talk\nabout, just in two lectures,",
    "start": "5109",
    "end": "13959"
  },
  {
    "text": "the topic,\nreinforcement learning. So reinforcement learning is\na pretty important sub area",
    "start": "13959",
    "end": "19250"
  },
  {
    "text": "of machine learning. But it does have a\nslightly different flavor. We're not going to\nspend a lot of time.",
    "start": "19250",
    "end": "25019"
  },
  {
    "text": "We're going to have-- just because this course\nhas covered a lot of topics, we are only going to touch\non some very basic concepts",
    "start": "25019",
    "end": "31820"
  },
  {
    "text": "of reinforcement learning\nthis lecture and the lecture--",
    "start": "31820",
    "end": "36969"
  },
  {
    "text": "the two lectures after. The next lecture,\nwe're going to have a guest lecture on the broader\nimpact of machine learning,",
    "start": "36969",
    "end": "44010"
  },
  {
    "text": "like robustness, societal\nimpact, and so forth. And then we're going to\nhave the last lecture, which is also going to be on\nreinforcement learning.",
    "start": "44010",
    "end": "51329"
  },
  {
    "text": "So this lecture is\nhere just mostly because I think you need\nthis lecture for solving,",
    "start": "51329",
    "end": "56460"
  },
  {
    "text": "well, homework question\nin your Homework 4. This lecture will give\nyou the basic concept",
    "start": "56460",
    "end": "62350"
  },
  {
    "text": "so that you can solve\nthat homework question. OK. So reinforcement learning-- so\nI think reinforcement learning",
    "start": "62350",
    "end": "70120"
  },
  {
    "text": "is-- at least on the surface,\nit's very different. It sounds very\ndifferent from some of the other machine learning\nproblems because there are",
    "start": "70120",
    "end": "78040"
  },
  {
    "text": "a bunch of different things. So maybe, just to\ngive you a rough sense on what kind of questions we\nare trying to solve today.",
    "start": "78040",
    "end": "85329"
  },
  {
    "text": "So maybe the running\nexample you can have is that you have some-- you are trying to\nlet a robot learn",
    "start": "85330",
    "end": "91380"
  },
  {
    "text": "how to navigate a certain\npart of some space, right? So you want to control\nthe robot to do something.",
    "start": "91380",
    "end": "98259"
  },
  {
    "text": "So we are trying to solve\nthis controlling tasks. So maybe you want the robot\nto pick up some object",
    "start": "98259",
    "end": "105469"
  },
  {
    "text": "or maybe on the robot to go to\nsome place, so and so forth. So there are a bunch of\ndifferences between RL",
    "start": "105469",
    "end": "112659"
  },
  {
    "text": "and supervised learning\nor unsupervised learning. So the difference is--",
    "start": "112659",
    "end": "129720"
  },
  {
    "text": "so here are some differences. I'm not trying to be\nvery comprehensive. And also, there\nare intersections.",
    "start": "129720",
    "end": "135390"
  },
  {
    "text": "So there are certain\nsub-areas of RL which is more similar\nto supervised learning.",
    "start": "135390",
    "end": "142580"
  },
  {
    "text": "So here, I'm just\nonly going to talk about some high-level\ndifferences. So the first of the--",
    "start": "142580",
    "end": "148239"
  },
  {
    "text": "first of all, RL is about\nsequential decision-making,",
    "start": "148239",
    "end": "156409"
  },
  {
    "text": "so sequential.",
    "start": "156409",
    "end": "166440"
  },
  {
    "text": "So I guess there are\ntwo things that you need to pay attention here. So the first thing is that\nthis is about decision-making.",
    "start": "166440",
    "end": "172470"
  },
  {
    "text": "So before, when you talk\nabout supervised learning, you are talking about\nprediction, right? You are predicting\nthe house price.",
    "start": "172470",
    "end": "178480"
  },
  {
    "text": "You are predicting whether\nsomebody has cancer or not.",
    "start": "178480",
    "end": "183709"
  },
  {
    "text": "You are predicting some\ny's, some target, right? But here, you are\nnot really just only about prediction of what\nwill happen in this world,",
    "start": "183710",
    "end": "191150"
  },
  {
    "text": "you are also-- and this is also about\nwhether you should-- what kind of decisions you make.",
    "start": "191150",
    "end": "196269"
  },
  {
    "text": "Sometimes these\ndecisions are made after you make the predictions,\nbut maybe before or after you know that this\nperson might have cancer,",
    "start": "196270",
    "end": "204560"
  },
  {
    "text": "then maybe you need to give this\nperson some treatment, right? So that's the\ndifferent part where",
    "start": "204560",
    "end": "210500"
  },
  {
    "text": "you have to make decisions\nbased on a prediction. Sometimes you don't,\nyou don't predict, you just make\ndecisions directly.",
    "start": "210500",
    "end": "216209"
  },
  {
    "text": "That's also possible. But at the end of that,\nyou are making decisions. And the second, this is\nsequential decision-making.",
    "start": "216209",
    "end": "223329"
  },
  {
    "text": "So your decisions have a\nlong-term impact, right? So especially, maybe think\nabout controlling a robot.",
    "start": "223329",
    "end": "228659"
  },
  {
    "text": "So if you let a robot to\ngo forward at this step,",
    "start": "228659",
    "end": "233950"
  },
  {
    "text": "then at the next step, your-- the robot, the configuration,\nor the robot will move.",
    "start": "233950",
    "end": "242220"
  },
  {
    "text": "So the decision at this step\ndoes affect your decision at next step.",
    "start": "242220",
    "end": "247590"
  },
  {
    "text": "Now, think about, for\nexample, treating a patient. So if you give the patient\nsome kind of pill today,",
    "start": "247590",
    "end": "255170"
  },
  {
    "text": "then maybe the patient\nbecomes better. In the next day, then\nyou have to change your strategy, or, at least--",
    "start": "255170",
    "end": "260889"
  },
  {
    "text": "the decision you made\nyesterday probably would affect the\ndecision you make today and which would also affect\nthe decision you make tomorrow.",
    "start": "260889",
    "end": "268888"
  },
  {
    "text": "So I think that's the two\nimportant thing about the RL. And you will see that this\nis why this is challenging",
    "start": "268889",
    "end": "277740"
  },
  {
    "text": "because you have to consider\nthe long-term consequences, or long-term consequences\nof a decision.",
    "start": "277740",
    "end": "287210"
  },
  {
    "text": "It's not like you\ncan just say, I just greedily choose a decision\nbased on the current situation.",
    "start": "287210",
    "end": "293039"
  },
  {
    "text": "Choose the decision that can\nmake the next day the best. So maybe you can\nmake a decision that",
    "start": "293039",
    "end": "298740"
  },
  {
    "text": "makes the next day very good,\nbut then the day after tomorrow",
    "start": "298740",
    "end": "304620"
  },
  {
    "text": "you get into some\nweird situation. Think about your life decisions.",
    "start": "304620",
    "end": "310380"
  },
  {
    "text": "Sometimes you shoot\nfor short-term reward. You do something, but then\nyou miss out, for example, long-term investments in some\nother opportunities, right?",
    "start": "310380",
    "end": "318180"
  },
  {
    "text": "So greedy approach sometimes\ndoesn't work very well. And in many cases, greed,\njust purely greedy approach",
    "start": "318180",
    "end": "324159"
  },
  {
    "text": "doesn't work. And there are some other\nthings about-- another thing",
    "start": "324160",
    "end": "333060"
  },
  {
    "text": "about decision making\nis the following. So we all make decisions, right? So decisions can give you\nmultiple benefits, right?",
    "start": "333060",
    "end": "340710"
  },
  {
    "text": "So one thing is that\nyou are required to make a decision\nbecause that's the problem of\nformulation, right? When you control the robot,\nyou have to make a decision.",
    "start": "340710",
    "end": "346860"
  },
  {
    "text": "How do you control it,\nwhere you want it to turn, whether it turns\nleft or turn right. But another thing\nabout decision is",
    "start": "346860",
    "end": "352000"
  },
  {
    "text": "that when you decide\nwhat to do today, you also collect information\nfrom the environment about what",
    "start": "352000",
    "end": "359380"
  },
  {
    "text": "the environment will look like. So decisions also affect you\nin terms of the information.",
    "start": "359380",
    "end": "364770"
  },
  {
    "text": "So you can make decisions\nto kindo f query. In some sense, the\ndecision's part of the job is to query the environment.",
    "start": "364770",
    "end": "371050"
  },
  {
    "text": "So you collect more information. Maybe let's take the\ntreating patient example, as another example.",
    "start": "371050",
    "end": "376590"
  },
  {
    "text": "So maybe your decision\ncould be that I'm going to measure the patient in\nterms of certain measurement.",
    "start": "376590",
    "end": "381970"
  },
  {
    "text": "That's another decision. And that decision doesn't\nreally treat the patient, but it does collect\ninformation for you so that you can treat the\npatients later better.",
    "start": "381970",
    "end": "390310"
  },
  {
    "text": "So decisions also give you\nadditional information. So sometimes you have to think\nabout whether a decision--",
    "start": "390310",
    "end": "396710"
  },
  {
    "text": "you have to trade off the\ndifferent effects of decisions. Sometimes the decisions just\ndirectly give you some reward.",
    "start": "396710",
    "end": "404440"
  },
  {
    "text": "But sometimes the decisions\ndoesn't give you a reward, but give you information so that\nyou can use that information",
    "start": "404440",
    "end": "409960"
  },
  {
    "text": "to get reward in the future.",
    "start": "409960",
    "end": "415610"
  },
  {
    "text": "So I use the word reward,\nwhich I haven't really defined. So that's another actual\ndifference between this and supervised learning.",
    "start": "415610",
    "end": "421870"
  },
  {
    "text": "So here, there's no supervision,\nso no or little supervision.",
    "start": "421870",
    "end": "440270"
  },
  {
    "text": "So what does it mean? So for example, if you think\nabout controlling the robot, one supervision could\nbe that someone,",
    "start": "440270",
    "end": "446110"
  },
  {
    "text": "some human is\ntelling you that, how should you control the robot. Some expert knows that, how\nyou can play with this robot.",
    "start": "446110",
    "end": "453909"
  },
  {
    "text": "But the questions\nwe are solving here are those questions\nwhere you sometimes just",
    "start": "453910",
    "end": "459370"
  },
  {
    "text": "don't even know what's the\nright way to control the robot. For example, suppose you\nwant to fly a helicopter. So if you really fly\na real helicopter",
    "start": "459370",
    "end": "468810"
  },
  {
    "text": "there's some personnel,\nsome training lessons. You have to be\ntrained to be somebody who can fly the helicopter.",
    "start": "468810",
    "end": "475040"
  },
  {
    "text": "But suppose you are developing\na new helicopter that can fly automatically, then you\nare trying to figure out what's",
    "start": "475040",
    "end": "480860"
  },
  {
    "text": "the decisions you have to make. Or maybe, think about\ntreating the patient,",
    "start": "480860",
    "end": "486270"
  },
  {
    "text": "you have to find out the\nsequence of treatment you should give the patients. It's not like somebody\nalready knows it.",
    "start": "486270",
    "end": "491720"
  },
  {
    "text": "If somebody already knows\nit, then probably the problem is already easy. You should just use\nthat expert or policy.",
    "start": "491720",
    "end": "498430"
  },
  {
    "text": "But sometimes, you are\ntrying to figure out the best way to make decisions. So that's why there's no\nor very little supervision.",
    "start": "498430",
    "end": "507418"
  },
  {
    "text": "So in some sense,\nyou are not really trying to predict what's\nthe best prediction. You are trying figure\nout the best prediction",
    "start": "507419",
    "end": "512479"
  },
  {
    "text": "by interacting with the\nenvironment by some trials and errors just because\nhumans sometimes don't know",
    "start": "512479",
    "end": "519899"
  },
  {
    "text": "the optimal decisions, either. But if you don't\nhave any supervision, how do you figure out\nwhat's the best decision?",
    "start": "519900",
    "end": "527130"
  },
  {
    "text": "So the kind of the\nway to deal with is that you learn\nnot by imitating",
    "start": "527130",
    "end": "533180"
  },
  {
    "text": "some human supervisions,\nbut you are trying to learn from\nsome reward function.",
    "start": "533180",
    "end": "544959"
  },
  {
    "text": "So what does it mean? So humans specify\nthe reward function.",
    "start": "544959",
    "end": "550100"
  },
  {
    "text": "So the humans specify what\nyou want the robot to do. So if the robot picks up this\nobject, that means success,",
    "start": "550100",
    "end": "556089"
  },
  {
    "text": "then that's a reward function. That means reward is high. And if the robot fails to\ndo that, the reward is low.",
    "start": "556089",
    "end": "561250"
  },
  {
    "text": "So we specify the\nreward function, and then you let the\nmachine learning algorithm to figure out how to\nmaximize reward function.",
    "start": "561250",
    "end": "569930"
  },
  {
    "text": "And another kind of\naspect here, these",
    "start": "569930",
    "end": "575399"
  },
  {
    "text": "are not completely\nmutually exclusive. So I've mentioned this. So another interesting feature\nis that you collect more data",
    "start": "575399",
    "end": "584510"
  },
  {
    "text": "interactively.",
    "start": "584510",
    "end": "590530"
  },
  {
    "text": "I think this is pretty much\nlike what I've mentioned before. The decisions also\ngive you more data.",
    "start": "590530",
    "end": "596250"
  },
  {
    "text": "You can make a decision\nto query some information, or even if don't make\ndecisions deliberately",
    "start": "596250",
    "end": "602250"
  },
  {
    "text": "to query information. They'll still give you\nadditional information once you make the decision. So because it's sequential,\nyou make some decisions",
    "start": "602250",
    "end": "610450"
  },
  {
    "text": "and then you get some data. And this data could help\nyou in the next round to make better decisions.",
    "start": "610450",
    "end": "616279"
  },
  {
    "text": "Of course, there are\nmany different variations",
    "start": "616279",
    "end": "621860"
  },
  {
    "text": "of the reinforcement\nsetting where sometimes, you can assume you have more\nsupervision from some experts,",
    "start": "621860",
    "end": "627519"
  },
  {
    "text": "can give you a demonstration. And sometimes you don't\nhave reward function. Sometimes you cannot have\ninteractive collection",
    "start": "627519",
    "end": "634930"
  },
  {
    "text": "of the data. And there are a lot\nof different variants. So basically, you can add the\nadjective before reinforcement",
    "start": "634930",
    "end": "640580"
  },
  {
    "text": "learning to have a variant. So you can say offline\nreinforcement, learning. That means you don't have--",
    "start": "640580",
    "end": "646000"
  },
  {
    "text": "you have more supervision, but\ndon't have the interaction. You can have other\naccommodations. But this is the main set of\nfeatures of reinforcement.",
    "start": "646000",
    "end": "657180"
  },
  {
    "text": "Any question so far? I know this is very abstract. Yeah.",
    "start": "657180",
    "end": "662779"
  },
  {
    "text": "Could you go over a little\nlower-level supervision part again?",
    "start": "662779",
    "end": "667960"
  },
  {
    "text": "Yeah. So basically, you don't-- so what could be a\nsupervision here? Because you are trying to make\nprediction or make decisions-- one possible\nsupervision would be",
    "start": "667960",
    "end": "674269"
  },
  {
    "text": "that you like the\nexpert, an expert who knows how to solve the\ntask to demonstrate",
    "start": "674269",
    "end": "680389"
  },
  {
    "text": "how to solve the task. So for example, suppose you\nwant to control the robot, and you say, OK, maybe\nhow to control this robot.",
    "start": "680389",
    "end": "687050"
  },
  {
    "text": "And then you just\ndemonstrate to me how do you make the\nrobot to solve the task. And that would be\nyour supervision.",
    "start": "687050",
    "end": "693370"
  },
  {
    "text": "But in many cases, you don't\nneed to have the supervision. Sometimes you don't\nhave the supervision because the humans don't\nknow what are the best",
    "start": "693370",
    "end": "699410"
  },
  {
    "text": "way to control a robot. And sometimes you\nhave the supervision, but you don't need them. Or sometimes, you\nhave the supervision,",
    "start": "699410",
    "end": "704440"
  },
  {
    "text": "but it's hard to collect\nsupervision because you have to find the expert. So that's the basic\nidea actually.",
    "start": "704440",
    "end": "711600"
  },
  {
    "text": "But just to be fair, recently,\nI think in the last two or three years, people are moving towards\nmore and more human supervision",
    "start": "711600",
    "end": "719420"
  },
  {
    "text": "because it turns out\nthat there's trade-off. If you don't use\nsupervision, you",
    "start": "719420",
    "end": "724620"
  },
  {
    "text": "don't use human\nsupervision, that sounds great because\nyou don't need humans to demonstrate for you.",
    "start": "724620",
    "end": "729950"
  },
  {
    "text": "But you need to\ncollect more data and do have more\ntrials and errors. You have to try out different\nways to control your robots",
    "start": "729950",
    "end": "736990"
  },
  {
    "text": "and see which ones succeed and\nwhich one fails and then learn from that. But suppose the humans\ntell you something,",
    "start": "736990",
    "end": "743310"
  },
  {
    "text": "then you don't have to\ntry that many times. You can let the robot\nfall down less often.",
    "start": "743310",
    "end": "748649"
  },
  {
    "text": "Maybe without even\ntrying on the robot, you already know how to control\nthe robot just because you learn from the humans.",
    "start": "748650",
    "end": "754060"
  },
  {
    "text": "So I think in the\nlast few years, I think people are\nmoving towards using more and more human supervision.",
    "start": "754060",
    "end": "759890"
  },
  {
    "text": "And sometimes you are using\nimperfect human supervision or you use-- sometimes they don't have\nthe expert supervision.",
    "start": "759890",
    "end": "768360"
  },
  {
    "text": "Sometimes it could be just\nsome data from the past. So you have seen the\nrobot going around,",
    "start": "768360",
    "end": "773620"
  },
  {
    "text": "doing some tasks\nimperfectly in the past, and you use those\ndata as supervision",
    "start": "773620",
    "end": "778649"
  },
  {
    "text": "to learn something better\nthan the past data. That's even possible.",
    "start": "778649",
    "end": "784240"
  },
  {
    "text": "But we are not going to go\ninto all of these details. So for this lecture, there\nis no supervision, just--",
    "start": "784240",
    "end": "791320"
  },
  {
    "text": "you know nothing about\nwhat's the right decision. But you're going to\nfigure out the decision by trying different strategies.",
    "start": "791320",
    "end": "798950"
  },
  {
    "text": "So basically, all the algorithms\nwill look like this, somewhat like trials and errors. So you start with a robot.",
    "start": "798950",
    "end": "804320"
  },
  {
    "text": "You don't know\nhow to control it. You don't know how to use\nit to solve any tasks. And then you try different\ndecisions, actions.",
    "start": "804320",
    "end": "811579"
  },
  {
    "text": "And then some actions just\nhappens to pick up objects, for example.",
    "start": "811579",
    "end": "816610"
  },
  {
    "text": "And some other actions\njust happens to fail. And then you try to use\nthose actions that succeed.",
    "start": "816610",
    "end": "822660"
  },
  {
    "text": "You know whether\nit succeed or fail because that's your\nreward function. If you succeed, you\nhave better reward. And if you fail, you\nhave lower reward.",
    "start": "822660",
    "end": "829720"
  },
  {
    "text": "So you know whether it succeeds. And then you try to\nboost the chance to--",
    "start": "829720",
    "end": "835540"
  },
  {
    "text": "your algorithm tries\nto amplify or boost the chance to take those actions\nthat can succeed in the past.",
    "start": "835540",
    "end": "842060"
  },
  {
    "text": "And you kind of do\nthis bootstrap thing. And eventually, you\nfind one set of actions",
    "start": "842060",
    "end": "847379"
  },
  {
    "text": "or one set of policies\nthat just always succeed with very high probability.",
    "start": "847380",
    "end": "852589"
  },
  {
    "text": "And that's how the algorithm,\nroughly speaking, how it works.",
    "start": "852589",
    "end": "860470"
  },
  {
    "text": "So basically, everything is\nthrough this reward function. So the reward\nfunction is the signal",
    "start": "860470",
    "end": "865690"
  },
  {
    "text": "you rely on to learn what\nis good and what is bad.",
    "start": "865690",
    "end": "871120"
  },
  {
    "text": "Any other questions? [INAUDIBLE] what type of\n[INAUDIBLE] performance",
    "start": "871120",
    "end": "883410"
  },
  {
    "text": "regarding the reward functions? So the reward is\nsomething you collect. But you can also\nsee something more.",
    "start": "883410",
    "end": "888870"
  },
  {
    "text": "We can see, for example, when\nyou manipulate the robotic arm, you can see how the--",
    "start": "888870",
    "end": "894029"
  },
  {
    "text": "where the arm moves towards. So you can also\nobserve other things. I'll formalize that as well.",
    "start": "894029",
    "end": "900690"
  },
  {
    "text": "But roughly speaking,\nyou can observe. Yeah, basically, for example,\nif you treat a patient, you can observe something\nabout how the patient behaves",
    "start": "900690",
    "end": "908720"
  },
  {
    "text": "or performs. And if you train\na robot, then you can see how the robot\nkind of moves, right?",
    "start": "908720",
    "end": "915870"
  },
  {
    "text": "Those are additional\ninformations you can collect. And sometimes this information\nis via kind of a pixel.",
    "start": "915870",
    "end": "924139"
  },
  {
    "text": "You can collect the\ninformation via different ways. Sometimes it's from the\ncamera, sometimes it's from the internal\nrecording of the system,",
    "start": "924139",
    "end": "931779"
  },
  {
    "text": "sometimes it's from\nsomething else. Can you have multiple\nreward functions?",
    "start": "931779",
    "end": "937160"
  },
  {
    "text": "Yeah, you can have multiple\nreward functions in many cases, but then you have\nto say-- you decide which one is more important,\nhow do you balance them?",
    "start": "937160",
    "end": "944820"
  },
  {
    "text": "So in our lecture, we\nare going to just have one reward function.",
    "start": "944820",
    "end": "951190"
  },
  {
    "text": "And sometimes you can\nalso have constraints. For example, you\ncan say you have one reward and a\nbunch of constraints which you have to satisfy.",
    "start": "951190",
    "end": "957839"
  },
  {
    "text": "That's also a valid setup. I think I probably have said a\nlot about the high-level idea,",
    "start": "957839",
    "end": "966390"
  },
  {
    "text": "which is probably a little bit\nhard to map to the real thing. So let me try to define--",
    "start": "966390",
    "end": "972350"
  },
  {
    "text": "mathematically,\nhow does this work? And I'm going to use this\nrunning example, a very trivial",
    "start": "972350",
    "end": "978020"
  },
  {
    "text": "running example. So this running\nexample is that you are trying to control a\nrobot navigating a 1-D tape.",
    "start": "978020",
    "end": "993810"
  },
  {
    "text": "So suppose you have a\ntape which is like this.",
    "start": "993810",
    "end": "1000720"
  },
  {
    "text": "And you have a robot which-- it sits somewhere, and\nit can move to the left",
    "start": "1000720",
    "end": "1009220"
  },
  {
    "text": "or to the right. You can take some action\nto move it left or right. And maybe there's a\ngoal somewhere else.",
    "start": "1009220",
    "end": "1015130"
  },
  {
    "text": "Maybe this is the goal. So basically, you want\nto move this robot to the goal, which is trivial.",
    "start": "1015130",
    "end": "1020180"
  },
  {
    "text": "If you're human,\nknows this task, the person would\nbe able to move. So you just keep going,\nmoving to the goal.",
    "start": "1020180",
    "end": "1027220"
  },
  {
    "text": "That's easy. But we are going to let the\nalgorithms to figure out what's the right--",
    "start": "1027220",
    "end": "1032410"
  },
  {
    "text": "the strategy. And so this is my\nrunning example.",
    "start": "1032410",
    "end": "1038880"
  },
  {
    "text": "And I'm going to formulate\non the set of problems. The formulation is often\ncalled Markov decision process.",
    "start": "1038880",
    "end": "1052410"
  },
  {
    "text": "So let me define a bunch\nof quantity terminologies. So there is something\ncalled state, s.",
    "start": "1052410",
    "end": "1062139"
  },
  {
    "text": "And you can have\na set of states. And that's denoted by capital S.",
    "start": "1062140",
    "end": "1069950"
  },
  {
    "text": "So for this example,\na state is-- a state, basically, is the\nsituation that the robot is in.",
    "start": "1069950",
    "end": "1075650"
  },
  {
    "text": "So a state basically, is-- suppose you have say Actually, maybe, let me just\nchange the goal to be here",
    "start": "1075650",
    "end": "1087710"
  },
  {
    "text": "to be consistent with my notes. Maybe I say 10.",
    "start": "1087710",
    "end": "1092950"
  },
  {
    "text": "So a state basically\nis describing what's the current\nsituation of the robot.",
    "start": "1092950",
    "end": "1099440"
  },
  {
    "text": "So here the state is probably And here, you only\nneed 10 numbers.",
    "start": "1099440",
    "end": "1105169"
  },
  {
    "text": "There are only 10 states. So that's the only\nthing you care about. But if you really have\na real robot, maybe,",
    "start": "1105169",
    "end": "1111870"
  },
  {
    "text": "you have to describe this\nrobot in many other parameters or many other numerical numbers.",
    "start": "1111870",
    "end": "1120110"
  },
  {
    "text": "For example, you may describe\nthe robot speed, the velocity, the height, the center\nof mass, so and so forth.",
    "start": "1120110",
    "end": "1126270"
  },
  {
    "text": "Then you can have a\nhigh-dimensional state. So you have a lot of\nnumbers to describe the state of the robot.",
    "start": "1126270",
    "end": "1132210"
  },
  {
    "text": "And the family of\nthe states will-- basically state,\nin that case, will be a high-dimensional vector.",
    "start": "1132210",
    "end": "1138900"
  },
  {
    "text": "And the family of\nstates will be just all the high-dimensional vectors\nthat can describe the robot.",
    "start": "1138900",
    "end": "1146669"
  },
  {
    "text": "So maybe another\nexample would be that, if you think about playing Go.",
    "start": "1146669",
    "end": "1152160"
  },
  {
    "text": "And the state would be\nthe current board, so where all the how does\nthe board look like.",
    "start": "1152160",
    "end": "1160200"
  },
  {
    "text": "And then you have\na lot of states because there's 361\nentries on the board.",
    "start": "1160200",
    "end": "1165500"
  },
  {
    "text": "And every entry, you can have\na white and black and nothing. You have three choices. Then, you have 3 to the\npower of 361 possible states.",
    "start": "1165500",
    "end": "1176240"
  },
  {
    "text": "So this is the concept of state. And there is a concept\ncalled actions.",
    "start": "1176240",
    "end": "1181660"
  },
  {
    "text": "Sometimes, typically,\nwe use a for action. And there's a set of action.",
    "start": "1181660",
    "end": "1187750"
  },
  {
    "text": "This is the set of\nactions you can take, which is called A. So\nin this case, basically,",
    "start": "1187750",
    "end": "1194360"
  },
  {
    "text": "these are all possible things,\ndecisions you can make, right? And it's called action,\ntechnically, in this language.",
    "start": "1194360",
    "end": "1201520"
  },
  {
    "text": "So for example,\nhere, maybe I'll just allow the-- you take two\nactions left and right.",
    "start": "1201520",
    "end": "1207040"
  },
  {
    "text": "You can just move\nleft or move right. And when you control\na real robot,",
    "start": "1207040",
    "end": "1212389"
  },
  {
    "text": "probably you can\ntake other actions. You can change the-- you can\naccelerate or deaccelerate, or maybe you can change the\nforce at different joints",
    "start": "1212390",
    "end": "1222658"
  },
  {
    "text": "and many other things.",
    "start": "1222659",
    "end": "1228260"
  },
  {
    "text": "And if you play\nGo, then the action is to really just put\nsomething on the board.",
    "start": "1228260",
    "end": "1234059"
  },
  {
    "text": "OK, so this is a set of actions.",
    "start": "1234059",
    "end": "1239860"
  },
  {
    "text": "And then, I need something\ncalled dynamics or transitions to describe how does the\nactions influence the states.",
    "start": "1239860",
    "end": "1249140"
  },
  {
    "text": "So this is called dynamics. And I think also sometimes\nit's called transitions,",
    "start": "1249140",
    "end": "1258000"
  },
  {
    "text": "or transition probabilities,\nor sometimes it's",
    "start": "1258000",
    "end": "1267880"
  },
  {
    "text": "called state transition\nprobabilities. So basically, this is the--",
    "start": "1267880",
    "end": "1275330"
  },
  {
    "text": "so basically, you're\nasking, about the question. So I guess maybe let's\ndefine a notation. So this is Psa.",
    "start": "1275330",
    "end": "1282399"
  },
  {
    "text": "You're asking the question,\nwhen applying action a at state",
    "start": "1282400",
    "end": "1291180"
  },
  {
    "text": "s, the probability distribution.",
    "start": "1291180",
    "end": "1297080"
  },
  {
    "text": "You're asking where\nI should arrive at next time, so the probability\ndistribution of the next state.",
    "start": "1297080",
    "end": "1315620"
  },
  {
    "text": "And the next state, often just\nnotation-wise it's often called",
    "start": "1315620",
    "end": "1321710"
  },
  {
    "text": "S'. So basically, you're asking-- so in other words Psa(z)\nis the probability of S'",
    "start": "1321710",
    "end": "1335490"
  },
  {
    "text": "is equal to z, given s and a. So Psa is the\nprobability distribution.",
    "start": "1335490",
    "end": "1341250"
  },
  {
    "text": "And this probability\ndistribution is the conditional\nprobability distribution. Conditional, you are at--\ncurrently, you are at state s.",
    "start": "1341250",
    "end": "1347789"
  },
  {
    "text": "And you play action a. And you ask yourself\nwhat's the distribution of the possible next state.",
    "start": "1347789",
    "end": "1353669"
  },
  {
    "text": "And there's some\nrandomness here, right? So for robots, sometimes you\ncan think of it as deterministic",
    "start": "1353669",
    "end": "1359001"
  },
  {
    "text": "because you do something\nand deterministically it moves to some other place.",
    "start": "1359001",
    "end": "1364890"
  },
  {
    "text": "But for many other environments,\nyou take some action. But how does the environment,\nhow does the state changes?",
    "start": "1364890",
    "end": "1372020"
  },
  {
    "text": "It's probabilistic. There is some\nrandomness involved. So the randomness is\ntrying to capture on that.",
    "start": "1372020",
    "end": "1382610"
  },
  {
    "text": "So that's why in\nthis sense this-- so for every s and a, this\nPsa is a distribution.",
    "start": "1382610",
    "end": "1391320"
  },
  {
    "text": "So Psa, if you write Psa\nas this, the probability of every state--",
    "start": "1391320",
    "end": "1407169"
  },
  {
    "text": "suppose maybe, let's say--\nsuppose you have a state S, which is equal to 1,",
    "start": "1407169",
    "end": "1414690"
  },
  {
    "text": "So then if you-- you can also view\nthis as a vector.",
    "start": "1414690",
    "end": "1421409"
  },
  {
    "text": "You say this is Psa(1). This is the chance to\narrive at the state 1.",
    "start": "1421409",
    "end": "1428300"
  },
  {
    "text": "And this is the Psa\nat the last state. Maybe let's-- I\nthink I'll just--",
    "start": "1428300",
    "end": "1435419"
  },
  {
    "text": "I think, yeah. Nice. Maybe I'll just-- sorry. Maybe let's just--",
    "start": "1435420",
    "end": "1441120"
  },
  {
    "text": "I think I realized it. Maybe let's take this. Say M is the of states. So my state is just to have\nm states, 1 2, 3, 4, up to m.",
    "start": "1441120",
    "end": "1449750"
  },
  {
    "text": "Then you can list\nthe probability to arrive at each of the states. And write it as a vector. This is a vector that is in Rm.",
    "start": "1449750",
    "end": "1457470"
  },
  {
    "text": "And This the probability vector. So this vector itself-- all\nthe entries are summed up to 1.",
    "start": "1457470",
    "end": "1474158"
  },
  {
    "text": "Of course, you can also have\ndeterministic transition dynamics. So you can say that only\none of these numbers is 1,",
    "start": "1474159",
    "end": "1481000"
  },
  {
    "text": "and all the others are 0. And that's a deterministic\ntransition dynamics. It just means that\nyou just-- you",
    "start": "1481000",
    "end": "1486049"
  },
  {
    "text": "always translate to the same\nstate given the same action and state.",
    "start": "1486049",
    "end": "1492220"
  },
  {
    "text": "You always translate\nto the same next state.",
    "start": "1492220",
    "end": "1497990"
  },
  {
    "text": "OK, so I think I'm going to have\nan example based on this robot",
    "start": "1497990",
    "end": "1512130"
  },
  {
    "text": "thing, just to give\nyou a concrete idea. So suppose you say you have this\naction set, which is L and R.",
    "start": "1512130",
    "end": "1521190"
  },
  {
    "text": "And this means that you\npush the robot to the left or to the right. But let's say suppose\nthere's some randomness",
    "start": "1521190",
    "end": "1527139"
  },
  {
    "text": "in this environment. Even when you try to push it,\nit doesn't necessarily always move. So with just only a\ngood chance, it moves.",
    "start": "1527140",
    "end": "1532908"
  },
  {
    "text": "With some chance, you\njust fail to make it move. So maybe then-- maybe\nlet's say, suppose--",
    "start": "1532909",
    "end": "1542429"
  },
  {
    "text": "so L action, say, succeeds with\na probability of, let's say,",
    "start": "1542429",
    "end": "1552700"
  },
  {
    "text": "Then, that means\nthat if you look at, or if you use this\nnotation, it means",
    "start": "1552700",
    "end": "1558770"
  },
  {
    "text": "that suppose you have-- you are\nat P, say, you are the state 7",
    "start": "1558770",
    "end": "1567649"
  },
  {
    "text": "and you apply the action\nL. So Psa, S is the state and L is the action.",
    "start": "1567649",
    "end": "1574110"
  },
  {
    "text": "So then you ask,\nwhat's the chance to arrive at some other state? But you are at 7.",
    "start": "1574110",
    "end": "1580690"
  },
  {
    "text": "And you try to move\na left, but you know that it only succeeds\nwith probability of 0.9. That means that with 0.9,\nyou're going to arrive at 6.",
    "start": "1580690",
    "end": "1588150"
  },
  {
    "text": "So this will be 0.9. And with 0.1 chance, you're\ngoing to arrive at 7.",
    "start": "1588150",
    "end": "1595530"
  },
  {
    "text": "You stay at 7. So that's 0.1. And with 0 chance\nyou're going to arrive at any of other places.",
    "start": "1595530",
    "end": "1608770"
  },
  {
    "text": "For example, you are going\nto arrive at 5 with 0 chance. And then you're going arrive\nat other places with 0 chance.",
    "start": "1608770",
    "end": "1614309"
  },
  {
    "text": "So that's one example of\nthe transition dynamics. And then, you can write\ndown the transition dynamics",
    "start": "1614309",
    "end": "1620990"
  },
  {
    "text": "for other state or action. Maybe just, for example,\nsuppose you say P7,R,.",
    "start": "1620990",
    "end": "1627760"
  },
  {
    "text": "So now you're\nasking, at state 7, if I try to move to the right\nwhere it should arrive at.",
    "start": "1627760",
    "end": "1635470"
  },
  {
    "text": "We know that it should\narrive at 8 with some chance, with chance 0.-- say, 0.-- I guess,\nin my example,",
    "start": "1635470",
    "end": "1642799"
  },
  {
    "text": "I make it a little bit\ncomplicated just to-- so let's say R actions succeed.",
    "start": "1642799",
    "end": "1651289"
  },
  {
    "text": "I'm just making this up\njust to make it interesting. With probability",
    "start": "1651289",
    "end": "1657539"
  },
  {
    "text": "that with probability to arrive at 8, at the entry 8.",
    "start": "1657539",
    "end": "1667110"
  },
  {
    "text": "So you get 0.8. And with probability 0.2,\nyou are going to stay at 7. And with probability",
    "start": "1667110",
    "end": "1672710"
  },
  {
    "text": "to arrive at any other states. So that's mean P7,R,\nmaybe 5, will be 0.",
    "start": "1672710",
    "end": "1680799"
  },
  {
    "text": "And all the other P7,R(z) will\nbe 0 if z is not equal to 7",
    "start": "1680800",
    "end": "1687220"
  },
  {
    "text": "and 8.",
    "start": "1687220",
    "end": "1692610"
  },
  {
    "text": "And you can define whatever\ntranslations you want. So you can write--\nusing the same ways,",
    "start": "1692610",
    "end": "1700070"
  },
  {
    "text": "you write out all the\ntransition probabilities for every state-action path. OK, so that's my description\nof the environment.",
    "start": "1700070",
    "end": "1712929"
  },
  {
    "text": "By the way, by environment,\npeople generally refers to this entire system. So how does the--",
    "start": "1712929",
    "end": "1719990"
  },
  {
    "text": "how does things change\nbased on your action? The environment basically\njust means this entire system.",
    "start": "1719990",
    "end": "1725820"
  },
  {
    "text": "OK, so now I have defined\nthe transition probability.",
    "start": "1725820",
    "end": "1731289"
  },
  {
    "text": "And now, I have to talk\nabout sequential decisions. So far, I've only\ntalked about one action,",
    "start": "1731290",
    "end": "1736340"
  },
  {
    "text": "about how does one\naction affect the system. So when you have sequential\ndecisions, then what happens",
    "start": "1736340",
    "end": "1741920"
  },
  {
    "text": "is that you are interacting with\nthe environment or the system,",
    "start": "1741920",
    "end": "1749600"
  },
  {
    "text": "like the following. So first of all, you say, I'm\ngoing to have an initial state",
    "start": "1749600",
    "end": "1754630"
  },
  {
    "text": "s0, which is the initial state.",
    "start": "1754630",
    "end": "1760730"
  },
  {
    "text": "And let's say the\ninitial state is given, but sometimes you can also say\nthe initial state is randomly",
    "start": "1760730",
    "end": "1767000"
  },
  {
    "text": "drawn from some distribution. OK, then, so what you do is\nthat the algorithm chooses",
    "start": "1767000",
    "end": "1778020"
  },
  {
    "text": "some action a0 from\nthe set, action set.",
    "start": "1778020",
    "end": "1785399"
  },
  {
    "text": "And then after the algorithm\nchoose the action, the decision or action, the environment step\nis that you basically sample s1",
    "start": "1785399",
    "end": "1801169"
  },
  {
    "text": "from this distribution. So you basically translate\nyour state based on this rule",
    "start": "1801169",
    "end": "1806610"
  },
  {
    "text": "that we have described. So you say S1 is going\nto be sampled from Ps0a0. Psa0a0 is basically, if your\nprobability is 0 or s0, what's",
    "start": "1806610",
    "end": "1816278"
  },
  {
    "text": "the chance, or what's\nthe probability to arrive at new states? And you sample one\nstate, one concrete state",
    "start": "1816279",
    "end": "1823049"
  },
  {
    "text": "from this probability\ndistribution. And then, the algorithm chooses\na1, and in this action, a.",
    "start": "1823050",
    "end": "1835059"
  },
  {
    "text": "But here, a1 can depend on what? So a1 can depend on s1 and s0.",
    "start": "1835059",
    "end": "1846840"
  },
  {
    "text": "So s1 is considered as something\nthat is given to the algorithm. So you observe s1.",
    "start": "1846840",
    "end": "1852768"
  },
  {
    "text": "After you play the action,\nyou observe more information. And the more information is s1. And then, you can play your\nnew a1 based on s1 and s0.",
    "start": "1852769",
    "end": "1862059"
  },
  {
    "text": "And then you just\nkeep doing this. So the next round, you just\nsay, I'm going to continue. I'm going to say that s2\nis generated from Ps1a1.",
    "start": "1862059",
    "end": "1872070"
  },
  {
    "text": "And then algorithm picks a2.",
    "start": "1872070",
    "end": "1878340"
  },
  {
    "text": "And a2 can depend\non all the history, all the historical\nobservations you have seen.",
    "start": "1878340",
    "end": "1885888"
  },
  {
    "text": "So that's the idea. So for example, if you really\nthink about this thing.",
    "start": "1885889",
    "end": "1896080"
  },
  {
    "text": "Maybe you start from you start from the state 3.",
    "start": "1896080",
    "end": "1902020"
  },
  {
    "text": "And then, you can\napply R action. And then with some chances\nyou are going to arrive at 4.",
    "start": "1902020",
    "end": "1907490"
  },
  {
    "text": "And then, suppose you are\nright at 4, then you can say, I'm going to decide again\nwhat my action should be. And you say, OK, my\naction should still be R,",
    "start": "1907490",
    "end": "1914380"
  },
  {
    "text": "and then I can observe\nwhether it does move. So with some chance,\nit will move.",
    "start": "1914380",
    "end": "1920190"
  },
  {
    "text": "And you'll just keep doing this. OK, so that's describing\nthe decision process.",
    "start": "1920190",
    "end": "1933080"
  },
  {
    "text": "And there's one thing we haven't\ndescribed, which is the reward. So how do we decide when you\nsucceed or not eventually?",
    "start": "1933080",
    "end": "1939540"
  },
  {
    "text": "So the reward function-- there's something\ncalled reward function.",
    "start": "1939540",
    "end": "1947929"
  },
  {
    "text": "So the reward\nfunction is a function",
    "start": "1947929",
    "end": "1955690"
  },
  {
    "text": "that maps the state, the family\nof states, a set of states",
    "start": "1955690",
    "end": "1962669"
  },
  {
    "text": "to a real number. So basically, sometimes\nyou write it as R(s).",
    "start": "1962670",
    "end": "1967908"
  },
  {
    "text": "S is a state. And you apply the\nreward function. You get Rs. In some other\ncases, you can also",
    "start": "1967909",
    "end": "1973580"
  },
  {
    "text": "have reward function that\ndepends on the action. Sometimes it can depend\non the next state.",
    "start": "1973580",
    "end": "1979570"
  },
  {
    "text": "For the purpose of this course,\nlet's say the reward function only depends on state.",
    "start": "1979570",
    "end": "1986340"
  },
  {
    "text": "So basically, for example,\nfor this robot case, maybe you can say--",
    "start": "1986340",
    "end": "1991779"
  },
  {
    "text": "suppose you want to somehow\nhave a reward function that characterize whether you\nachieve the goal, state 6, maybe",
    "start": "1991779",
    "end": "1997149"
  },
  {
    "text": "you can just define your\nreward function to be-- R(6) to be 1.0.",
    "start": "1997150",
    "end": "2002279"
  },
  {
    "text": "So achieving a 6 gives\nyou a high reward. And then you say\nR of s is, say--",
    "start": "2002279",
    "end": "2007398"
  },
  {
    "text": "I am making this up. It doesn't really\nmatter exactly. But you say reward is very\nsmall or even inactive",
    "start": "2007399",
    "end": "2013240"
  },
  {
    "text": "if the state is not 6. So suppose you were to\ndefine a reward like this,",
    "start": "2013240",
    "end": "2018519"
  },
  {
    "text": "in some sense, you are\nencouraging the algorithm to reach 6 and not\nreach any other states,",
    "start": "2018519",
    "end": "2026240"
  },
  {
    "text": "because for other states you get\nnegative reward, and for the 6 you get positive reward.",
    "start": "2026240",
    "end": "2035019"
  },
  {
    "text": "But this is only a\nreward for one step. Can you have reward\nplus states and action? So if let's say\nyou want rewards--",
    "start": "2035019",
    "end": "2040789"
  },
  {
    "text": "you take a certain action\nthat is more expensive, but if you take a\ncertain another action",
    "start": "2040789",
    "end": "2047630"
  },
  {
    "text": "it could be less\nexpensive as well? Yeah, you can do that. So basically, yeah.",
    "start": "2047630",
    "end": "2053349"
  },
  {
    "text": "That's what I said. So your reward function\ncan be a function of s and a as well in\nmany other cases. But for simplicity, I just\nsaid the reward is the state.",
    "start": "2053349",
    "end": "2060679"
  },
  {
    "text": "The method didn't change match. It's almost the same. And sometimes, actually, it\ndepends on the state and action",
    "start": "2060679",
    "end": "2066780"
  },
  {
    "text": "at the next state. So you can depend on s prime. [INTERPOSING VOICES]",
    "start": "2066780",
    "end": "2074118"
  },
  {
    "text": "OK, cool. But this is only about one step.",
    "start": "2074119",
    "end": "2080310"
  },
  {
    "text": "Eventually, you\nhave to care about the sequential decision-making. It's just not about\none step, right?",
    "start": "2080310",
    "end": "2086240"
  },
  {
    "text": "So the total payoff\nis defined to be--",
    "start": "2086240",
    "end": "2094320"
  },
  {
    "text": "sometimes it's\ncalled total return. So this is defined to be R(s0)\nplus R(s1), so and so forth,",
    "start": "2094320",
    "end": "2104240"
  },
  {
    "text": "plus R(s)t. So this is the total reward.",
    "start": "2104240",
    "end": "2110250"
  },
  {
    "text": "Basically, you just the sum\nof all the reward at all the steps.",
    "start": "2110250",
    "end": "2115280"
  },
  {
    "text": "And now, here, I didn't specify\nhow many steps we can have. So if have an infinite\nnumber of steps,",
    "start": "2115280",
    "end": "2121620"
  },
  {
    "text": "and this doesn't seem\nto make a lot of sense because your reward will\nbe sometimes going up to going to\ninfinity-- if you have",
    "start": "2121620",
    "end": "2127520"
  },
  {
    "text": "an infinite number of\nsteps at each step you get a reward, something like 1,\nthen your reward eventually will be infinity.",
    "start": "2127520",
    "end": "2133560"
  },
  {
    "text": "And it becomes not\nvery informative, because you cannot compare one\ninfinity with another infinity.",
    "start": "2133560",
    "end": "2140220"
  },
  {
    "text": "And there are two ways\nto deal with this. So one way to deal\nwith it is that you say you have a discounted reward,\nbut you have infinite horizon.",
    "start": "2140220",
    "end": "2156130"
  },
  {
    "text": "By the way, horizon\nmeans how many steps you're going to play in\nthis sequential game.",
    "start": "2156130",
    "end": "2161410"
  },
  {
    "text": "So basically, the discounted\nreward is the following. So you have R(s0)\nplus some gamma",
    "start": "2161410",
    "end": "2169460"
  },
  {
    "text": "R(s1) plus some gamma square\nR(s2), so and so forth.",
    "start": "2169460",
    "end": "2177310"
  },
  {
    "text": "And where this gamma is less\nthan 1 and larger than 0 is the so-called\ndiscount factor.",
    "start": "2177310",
    "end": "2184920"
  },
  {
    "text": "And here, you have\ninfinite horizon.",
    "start": "2184920",
    "end": "2190838"
  },
  {
    "text": "Basically, it just means that\nyou take sum to infinity. What if that was the\nhorizon variable.",
    "start": "2190839",
    "end": "2197730"
  },
  {
    "text": "But is it ST? This is T. But\nhere, I still have-- I don't know. Here I'm being vague.",
    "start": "2197730",
    "end": "2204569"
  },
  {
    "text": "So I'm taking the-- [INAUDIBLE] In this case, [INAUDIBLE]?",
    "start": "2204569",
    "end": "2210349"
  },
  {
    "text": "Right. And this is just\na demonstration. This is not the-- the total payoff doesn't really\nmake a lot of sense if you",
    "start": "2210349",
    "end": "2215470"
  },
  {
    "text": "really-- if you just do this, but\nyou have infinite horizon, it doesn't really\nmake a lot of sense because it's going\nto be infinity.",
    "start": "2215470",
    "end": "2221410"
  },
  {
    "text": "So this is the real definition. We have infinite horizons. So you have discounted reward.",
    "start": "2221410",
    "end": "2226720"
  },
  {
    "text": "And the reason why you want\nto have this discounted reward is that--",
    "start": "2226720",
    "end": "2234160"
  },
  {
    "text": "so I guess there are several\nways to think about this. So one thing is that you can\nthink of this as interest.",
    "start": "2234160",
    "end": "2239950"
  },
  {
    "text": "It's like in the finance\nwhere the return you get,",
    "start": "2239950",
    "end": "2245480"
  },
  {
    "text": "the reward you\nget in the future, doesn't reward as much as\nthe reward you get right now",
    "start": "2245480",
    "end": "2251170"
  },
  {
    "text": "because there is an\ninterest rate or inflation rate, something like that. So basically, you\nsay that you discount",
    "start": "2251170",
    "end": "2258099"
  },
  {
    "text": "what you get in the future by\na little bit exponentially. If you get some\nreward after t times,",
    "start": "2258099",
    "end": "2264170"
  },
  {
    "text": "you're going to\nhave-- so basically here the t's term\nwould be like this.",
    "start": "2264170",
    "end": "2270020"
  },
  {
    "text": "So you discount your\nrewards in the future by some factor, gamma\nto the power of t.",
    "start": "2270020",
    "end": "2277599"
  },
  {
    "text": "Just like in finance\nor in economics, your return in the future\ndoesn't reward as much.",
    "start": "2277599",
    "end": "2284650"
  },
  {
    "text": "So that's one way\nto think about this. And another technical\nway to think about this is that if you do\nthis, then you--",
    "start": "2284650",
    "end": "2292269"
  },
  {
    "text": "if you have infinite horizon,\nthen your total reward is always bounded.",
    "start": "2292270",
    "end": "2302000"
  },
  {
    "text": "So suppose you have-- wait. Did I? Oh, this was from last week.",
    "start": "2302000",
    "end": "2313660"
  },
  {
    "text": "So basically, suppose\nyour gamma is less than 1 and bigger than 0.",
    "start": "2313660",
    "end": "2318910"
  },
  {
    "text": "And suppose each step of reward\nis bounded by minus M and M.",
    "start": "2318910",
    "end": "2327680"
  },
  {
    "text": "So suppose you\nhave this two, then that your total, your discounted\nreward, discounted payoff,",
    "start": "2327680",
    "end": "2335500"
  },
  {
    "text": "this, is at most, say, 1\nplus gamma or M. Sorry.",
    "start": "2335500",
    "end": "2347819"
  },
  {
    "text": "M. The first time,\nyou can only get M. And the second time,\nyou get M times gamma.",
    "start": "2347819",
    "end": "2353620"
  },
  {
    "text": "And the third time you\nget M times gamma squared, so and so forth.",
    "start": "2353620",
    "end": "2358651"
  },
  {
    "text": "And this series will converge. So the total sum\nwill be something",
    "start": "2358652",
    "end": "2364569"
  },
  {
    "text": "like M over 1 minus gamma. So basically, you're\nguaranteed that at least you",
    "start": "2364569",
    "end": "2370430"
  },
  {
    "text": "have a bounded return. And this is the maximum return\nyou can get in the best case.",
    "start": "2370430",
    "end": "2376250"
  },
  {
    "text": "So technically, this\nmakes it possible for to reasonable\ninfinite horizons because your return is\nalways, at least, bounded.",
    "start": "2376250",
    "end": "2383920"
  },
  {
    "text": "There's always a number,\na meaningful number for the return.",
    "start": "2383920",
    "end": "2389250"
  },
  {
    "text": "You have some questions? I thought so.",
    "start": "2389250",
    "end": "2398770"
  },
  {
    "text": "Oh, no questions, OK.",
    "start": "2398770",
    "end": "2406930"
  },
  {
    "text": "So if you look at the RL\npaper, in the literature, sometimes also, people talk\nabout a finite horizon, which",
    "start": "2406930",
    "end": "2412580"
  },
  {
    "text": "means that you just have\na hard cutoff for how long you're going to play this. So there is another\nway to formalize",
    "start": "2412580",
    "end": "2419660"
  },
  {
    "text": "the problem, which\nwe are not going to talk about in this lecture. But I'm just going to tell\nyou the existence of such",
    "start": "2419660",
    "end": "2427220"
  },
  {
    "text": "a definition. So if you have finite\nhorizon, you just say you have a horizon,\nwhich is called T. This is basically\nsaying that you just",
    "start": "2427220",
    "end": "2432560"
  },
  {
    "text": "have to stop at T step. You'll just have\nto stop at T step. And then your reward\nwill just be-- you don't",
    "start": "2432560",
    "end": "2438410"
  },
  {
    "text": "have to have discount factors. You just say I'm going to-- I adopt the first T steps.",
    "start": "2438410",
    "end": "2446170"
  },
  {
    "text": "So this finite horizon thing,\nand the infinite horizon, they don't have\nfundamental differences",
    "start": "2446170",
    "end": "2451849"
  },
  {
    "text": "from a technical point of view. If you know how to\nsolve one, you basically know how to solve the other.",
    "start": "2451849",
    "end": "2456859"
  },
  {
    "text": "Of course, there are\nsome dependencies if you really care\nabout a theory. So here, your-- all your\ndependencies depends on gamma.",
    "start": "2456859",
    "end": "2466670"
  },
  {
    "text": "And here, your\ndependence will be on T. But fundamentally, they don't\nreally matter that much. But the infinite horizon\ncase is a little bit easier",
    "start": "2466670",
    "end": "2474180"
  },
  {
    "text": "to understand in terms of--\nat least, for the beginning, if you just derive the\nmath, the math is cleaner.",
    "start": "2474180",
    "end": "2480990"
  },
  {
    "text": "So that's why we do the\ninfinite horizon case. And by the way, the gamma,\ntypically, in particular,",
    "start": "2480990",
    "end": "2486800"
  },
  {
    "text": "people do use gamma, do\nuse this discount factor. And gamma is something\nlike probably 0.99. And sometimes, in extreme\ncase, people even do 0.999.",
    "start": "2486800",
    "end": "2495630"
  },
  {
    "text": "So if you are in this regime,\nin some sense, the way you think about it\nis the following. So let's say gamma is 0.99.",
    "start": "2495630",
    "end": "2502880"
  },
  {
    "text": "What does this really mean? It means that you\nhave to really take a T, a power that\nis all over 100,",
    "start": "2502880",
    "end": "2509740"
  },
  {
    "text": "to make this gamma\nto the power T to be somewhat different from 1. So basically, 0.99 to\nthe power 10th, this",
    "start": "2509740",
    "end": "2517041"
  },
  {
    "text": "is still pretty close to 1. This is probably--\nI think this is-- how to do the math here if\nyou really care about this?",
    "start": "2517041",
    "end": "2523900"
  },
  {
    "text": "So it's 1 minus epsilon\ny to the power t. This is close to 1 minus epsilon\nt or something like this,",
    "start": "2523900",
    "end": "2531820"
  },
  {
    "text": "at least for the-- when t is small. So if you really do it-- so if you do the calculation,\nso this to the power--",
    "start": "2531820",
    "end": "2539170"
  },
  {
    "text": "to the 10, this is\nstill close to 1. This is probably like 0.9. But only if you raise\nthe power to a higher--",
    "start": "2539170",
    "end": "2545559"
  },
  {
    "text": "so you raise the power to 100,\nthen you're going to have this.",
    "start": "2545559",
    "end": "2551599"
  },
  {
    "text": "I think this is 1/e,\nsomething like that. It's like 0.2-- [INTERPOSING VOICES]",
    "start": "2551599",
    "end": "2556890"
  },
  {
    "text": "Yeah, exactly. So basically, this is\nsaying that the power has to be large enough so that\nthis discount factor starts",
    "start": "2556890",
    "end": "2564820"
  },
  {
    "text": "to matter. When the power is only 10,\nit doesn't really matter. And what exactly\nis power has to be,",
    "start": "2564820",
    "end": "2570779"
  },
  {
    "text": "I think if you do\nsome powerful math, I think what happens is that-- sorry, I'm just\nreusing these parts.",
    "start": "2570780",
    "end": "2577640"
  },
  {
    "text": "So 1 minus-- so gamma to the\npower of 1 over 1 minus gamma.",
    "start": "2577640",
    "end": "2582970"
  },
  {
    "text": "This is something like So only your power becomes",
    "start": "2582970",
    "end": "2588450"
  },
  {
    "text": "then you start to see the\neffect of the discount factor.",
    "start": "2588450",
    "end": "2593770"
  },
  {
    "text": "And after that it\ndecays pretty fast.",
    "start": "2593770",
    "end": "2599970"
  },
  {
    "text": "So in some sense,\nif you really want to of have a way to transit\nbetween finite horizons",
    "start": "2599970",
    "end": "2605470"
  },
  {
    "text": "and infinite horizons, then\npasscode this, 1 minus 1 over gamma, is your effective\nhorizon lines in some sense",
    "start": "2605470",
    "end": "2614910"
  },
  {
    "text": "because after-- when t is much, much bigger\nthan this 1 minus 1 over gamma,",
    "start": "2614910",
    "end": "2620660"
  },
  {
    "text": "then the discount factor just\nstarts to be super small. So then, you don't even have to\ncare about those kind of steps.",
    "start": "2620660",
    "end": "2630870"
  },
  {
    "text": "OK, so I guess, finally, after\nusing 6 boards, I already--",
    "start": "2630870",
    "end": "2640730"
  },
  {
    "text": "I'll defined this MDP. So this whole thing\nis called MDP. So this whole formulation is\ncalled the MDP, Markov decision",
    "start": "2640730",
    "end": "2648850"
  },
  {
    "text": "process. And you can see that this\nMarkov decision process are defined by a bunch of concepts.",
    "start": "2648850",
    "end": "2655450"
  },
  {
    "text": "So one thing is\nthe set of states. Another set is-- another\nthing is the set of actions,",
    "start": "2655450",
    "end": "2661480"
  },
  {
    "text": "and the set of transition\nprobabilities, Psa, s in S, a in A.",
    "start": "2661480",
    "end": "2669650"
  },
  {
    "text": "And you'll have a\ndiscount factor. And you'll have a\nreward function. So basically, after you specify\nthese six things, or five",
    "start": "2669650",
    "end": "2676890"
  },
  {
    "text": "things, then you specify MDP. And that's a\nwell-formatted problem. And the goal of\nthe MDP is that--",
    "start": "2676890",
    "end": "2686770"
  },
  {
    "text": "maybe I'll just write the goal\nhere even though it's pretty-- so the goal here is to\nmaximize the-- so basically, you want to find out a way to\nmaximize ",
    "start": "2686770",
    "end": "2702630"
  },
  {
    "text": "the discounted payoff, let's say.",
    "start": "2702630",
    "end": "2711849"
  },
  {
    "text": "So basically,\ngiven MDP, you want to figure out how to maximize\nthis discounted payoff.",
    "start": "2711850",
    "end": "2717030"
  },
  {
    "text": "Just to clarify [INAUDIBLE]\nrange, basically, [INAUDIBLE]??",
    "start": "2717030",
    "end": "2725019"
  },
  {
    "text": "The payoff I think means\nthe sum of the reward, discounted sum of the reward. And reward basically\nmeans one step.",
    "start": "2725019",
    "end": "2732750"
  },
  {
    "text": "I think people\nsometimes also call it the sum, the summation version. Sometimes they call it return.",
    "start": "2732750",
    "end": "2738360"
  },
  {
    "text": "And occasionally, also,\npeople just call it reward. But I think I like to have a\ndifferentiation of the terms",
    "start": "2738360",
    "end": "2745589"
  },
  {
    "text": "just to make it\nnot too confusing.",
    "start": "2745590",
    "end": "2751490"
  },
  {
    "text": "OK, any questions\nabout the formulation?",
    "start": "2751490",
    "end": "2763000"
  },
  {
    "text": "So far, we didn't really\ndo any derivations. So just to clarify-- so all\nof these are definitions,",
    "start": "2763000",
    "end": "2768150"
  },
  {
    "text": "how do you view this? It's like a world view. How do you view this\nworld, in some sense? What's the most\nimportant concepts",
    "start": "2768150",
    "end": "2774280"
  },
  {
    "text": "and what are the goals? I'm running a little\nbit slow, but it's fine.",
    "start": "2774280",
    "end": "2808200"
  },
  {
    "text": "So now the next question is\nhow do we solve this problem,",
    "start": "2808200",
    "end": "2819119"
  },
  {
    "text": "how do we find out\nthe best action. And for starters,\nwe're going to assume",
    "start": "2819119",
    "end": "2828160"
  },
  {
    "text": "that these Psa are given. So you know transition\nprobability. You are just trying\nto find out what's the best actions you should\ntake to maximize the reward.",
    "start": "2828160",
    "end": "2836640"
  },
  {
    "text": "But in reality, you don't\nknow necessarily know the Psa. You don't know the\ntransition probability. You have to somehow learn\nthem from observations.",
    "start": "2836640",
    "end": "2845180"
  },
  {
    "text": "So I think for this course,\nwe don't really talk too much about how to learn the Psas.",
    "start": "2845180",
    "end": "2850640"
  },
  {
    "text": "So in some sense, you can\nmostly assume that the Psa-- they are given. And the only question\nis to figure out",
    "start": "2850640",
    "end": "2857800"
  },
  {
    "text": "what's the best action to take. So the first thing to\nrealize is that there is",
    "start": "2857800",
    "end": "2863380"
  },
  {
    "text": "the so-called Markov property.",
    "start": "2863380",
    "end": "2869440"
  },
  {
    "text": "So I didn't emphasize that,\nbut let me do that right now. So the Markov property\nmeans that when",
    "start": "2869440",
    "end": "2875390"
  },
  {
    "text": "you take the environment,\nadvance the state by changing the state, it only look\nat the previous action",
    "start": "2875390",
    "end": "2881660"
  },
  {
    "text": "and the previous state. So the environment, how does the\nenvironment change the state? It only depends on previous\nstate and the previous actions.",
    "start": "2881660",
    "end": "2888578"
  },
  {
    "text": "That's described in this\nPsa, this framework where the Psa, the next\nstate only depends",
    "start": "2888579",
    "end": "2894450"
  },
  {
    "text": "on previous state\nand previous actions. So in some sense in your state\nthere is a Markov property.",
    "start": "2894450",
    "end": "2900420"
  },
  {
    "text": "So you only have to look\nat the previous state to decide what's the state. You don't have to look\nat the entire history.",
    "start": "2900420",
    "end": "2906280"
  },
  {
    "text": "So because of this\nMarkov property, it means that it means\nthat you only have to--",
    "start": "2906280",
    "end": "2916339"
  },
  {
    "text": "when you make the\ndecisions, you only have to look at the immediate\nstate, the current state.",
    "start": "2916340",
    "end": "2921520"
  },
  {
    "text": "So the optimal decision\nat time t, the optimal--",
    "start": "2921520",
    "end": "2932180"
  },
  {
    "text": "maybe let's call it\naction just to be consistent with\nthe terminologies.",
    "start": "2932180",
    "end": "2937260"
  },
  {
    "text": "The optimal action\nat time t only depends on the state St. Because\nanyway, whatever you do--",
    "start": "2937260",
    "end": "2948160"
  },
  {
    "text": "basically, after you see St,\nyou can forget about the history because the history\ndoesn't really matter. Conditional St,\nwhere the history",
    "start": "2948160",
    "end": "2954680"
  },
  {
    "text": "is independent with the\nfuture, conditional St. After you see St,\nyou know everything about the current configuration.",
    "start": "2954680",
    "end": "2961849"
  },
  {
    "text": "So you don't have to use the St\nto predict, to make decisions, and to maximize your reward.",
    "start": "2961849",
    "end": "2969510"
  },
  {
    "text": "So basically, because\nof this, there's this concept called policy.",
    "start": "2969510",
    "end": "2974530"
  },
  {
    "text": "So policy is a function\nthat takes in a state",
    "start": "2974530",
    "end": "2980380"
  },
  {
    "text": "and output an action.",
    "start": "2980380",
    "end": "2986029"
  },
  {
    "text": "So I guess,\ntechnically, I'm going to say this is my\nstate to action,",
    "start": "2986030",
    "end": "2991619"
  },
  {
    "text": "so it's action is equal to\nthe policy applied on a state.",
    "start": "2991619",
    "end": "2998029"
  },
  {
    "text": "So basically, you only have\nto look for policies instead of looking for the entire\ntrajectory of actions,",
    "start": "2998030",
    "end": "3005560"
  },
  {
    "text": "because anyway the\nway you make decisions is that you look at the\nimmediate current state",
    "start": "3005560",
    "end": "3011760"
  },
  {
    "text": "and then you apply some\nfunction on the state to get your action.",
    "start": "3011760",
    "end": "3017730"
  },
  {
    "text": "So basically, the unknown\nthing becomes a policy instead of a sequence of states. Does it make some sense?",
    "start": "3017730",
    "end": "3025680"
  },
  {
    "text": "Isn't this at the\nbeginning, you said you were trying to avoid\na greedy algorithm.",
    "start": "3025680",
    "end": "3035720"
  },
  {
    "text": "And this is really greedy\nthat you just looking at the current state and set of\nactions over review right now? That's a great question. So the question was that\nwhether you do this,",
    "start": "3035720",
    "end": "3042589"
  },
  {
    "text": "it sounds like it's greedy. So it's not in the\nfollowing sense. So when you decide\nwhat policy we use,",
    "start": "3042590",
    "end": "3049230"
  },
  {
    "text": "you do have to think about\nthe long-term ramifications of your action. But it's only that-- but your\naction doesn't depend on--",
    "start": "3049230",
    "end": "3056829"
  },
  {
    "text": "I think this is\nmore about you don't have to care about the history. So you don't have to care\nabout the previous history.",
    "start": "3056829",
    "end": "3063420"
  },
  {
    "text": "You just have about what\ncurrently-- condition of today. So what happens today\nis all that matters. But I can forget\nabout what happens.",
    "start": "3063420",
    "end": "3069650"
  },
  {
    "text": "I don't care about how did\nI arrive at this situation. So in some sense--",
    "start": "3069650",
    "end": "3076869"
  },
  {
    "text": "how do I say it? For example, if\nyou have a robot. So if the robot already\ndropped the bottle,",
    "start": "3076869",
    "end": "3083440"
  },
  {
    "text": "you don't care about how the\nrobot dropped the bottle. You just care about, the\nbottle now is on the ground. I have to go pick it up.",
    "start": "3083440",
    "end": "3091730"
  },
  {
    "text": "So this is more about\nforgetting the history. But when you decide\nthe policy, you do have to think\nabout the future.",
    "start": "3091730",
    "end": "3097599"
  },
  {
    "text": "You'll see that when we\noptimize the policy, when we find the best policy, we\ndo think for the future a lot. Yeah, I'll go back\nto that as well.",
    "start": "3097599",
    "end": "3106740"
  },
  {
    "text": "Cool. So this is the first thing. So you only have\nto find a policy. But this policy is\nnot something that's",
    "start": "3106740",
    "end": "3111809"
  },
  {
    "text": "trivial to find because\nthis policy is a function. So you have to figure\nout what's the best action for every state.",
    "start": "3111809",
    "end": "3117130"
  },
  {
    "text": "So basically, for\nevery state you have to figure out\nthe best action. And that will give you\nthe so-called policy.",
    "start": "3117130",
    "end": "3124480"
  },
  {
    "text": "So maybe one way\nto think about it",
    "start": "3124480",
    "end": "3132829"
  },
  {
    "text": "is that here, suppose you want\nto move your robot to the goal, then the policy problem\nwould be that if the state is",
    "start": "3132829",
    "end": "3138471"
  },
  {
    "text": "on the left of the\ngoal, your policy should be taking the right action. And if the state is here, you\nshould take the left action.",
    "start": "3138471",
    "end": "3152430"
  },
  {
    "text": "And, by the way, the policy\ncan also be randomized.",
    "start": "3152430",
    "end": "3159660"
  },
  {
    "text": "Here, I'm looking at a\ndeterministic action. So you say-- you have a state,\nyou output a single action.",
    "start": "3159660",
    "end": "3165109"
  },
  {
    "text": "For every state, you\nhave a single action. That's called the\noptimal policy. But in many cases, the policy\ncould be a randomized function,",
    "start": "3165109",
    "end": "3172690"
  },
  {
    "text": "or could be something\nlike conditional state, S. You can output the\ndistribution of actions,",
    "start": "3172690",
    "end": "3177930"
  },
  {
    "text": "and you choose the\nrandomly from them. For the purpose\nof this course, I think we are not going to have-- at least for this\nlecture, I'm not",
    "start": "3177930",
    "end": "3183818"
  },
  {
    "text": "going to have random policy. For the next\nlecture, I think I'm going to talk about\nrandomized policy.",
    "start": "3183819",
    "end": "3190140"
  },
  {
    "text": "So now I'm going to--",
    "start": "3190140",
    "end": "3196829"
  },
  {
    "text": "so how do I find the policy? So this sounds a little\neasier than finding",
    "start": "3196830",
    "end": "3202920"
  },
  {
    "text": "a sequence of actions. But how do I do that? So let me introduce\nanother notion which",
    "start": "3202920",
    "end": "3209480"
  },
  {
    "text": "is called the value function.",
    "start": "3209480",
    "end": "3214950"
  },
  {
    "text": "So let's say you have-- this is a value function V pi. This is a value\nfunction of a policy.",
    "start": "3214950",
    "end": "3221480"
  },
  {
    "text": "This is a function that maps\nthe state to a real number R. In some sense, this is trying to\ncapture the value of the state",
    "start": "3221480",
    "end": "3229801"
  },
  {
    "text": "under this policy, pi. So basically, this\nis equals to--",
    "start": "3229801",
    "end": "3237080"
  },
  {
    "text": "Vpi(s) is defined\nto be, in words,",
    "start": "3237080",
    "end": "3242410"
  },
  {
    "text": "the total payoff obtained--",
    "start": "3242410",
    "end": "3256980"
  },
  {
    "text": "of the executing policy\npi starting ",
    "start": "3256980",
    "end": "3268049"
  },
  {
    "text": "from state S. So basically, you think--\nyou start from state S. And you keep executing your\npolicy pi just every time",
    "start": "3268049",
    "end": "3276160"
  },
  {
    "text": "iteratively. Every time you see a new state,\nyou apply your policy pi. And then you collect\nsome total payoff,",
    "start": "3276160",
    "end": "3281790"
  },
  {
    "text": "total discounted payoff. I always have\ndiscounts just to-- for the rest of lecture.",
    "start": "3281790",
    "end": "3287069"
  },
  {
    "text": "So I compute the total payoff. And I call that the\nvalue of the state",
    "start": "3287069",
    "end": "3292440"
  },
  {
    "text": "S. This is demonstrating\nhow good this state S is under this policy pi because\nif the state S is good,",
    "start": "3292440",
    "end": "3298558"
  },
  {
    "text": "then you have-- it's a property of\nthe both pi and S. So if the policy is good,\nyou get better payoff.",
    "start": "3298559",
    "end": "3304280"
  },
  {
    "text": "If the S is good-- if the\nS is probably at a goal, then you probably\nget better payoff because you don't\nhave to move anything.",
    "start": "3304280",
    "end": "3309410"
  },
  {
    "text": "So this is just-- go ahead. A question about this\n[INAUDIBLE] policy that you're talking about\nhere in the value functions.",
    "start": "3309410",
    "end": "3315410"
  },
  {
    "text": "So when you say that you\nstart the policy from S,",
    "start": "3315410",
    "end": "3322339"
  },
  {
    "text": "is that the policy that is being\nrestricted to a certain state or what you're saying\nis that that policy",
    "start": "3322340",
    "end": "3329530"
  },
  {
    "text": "that you've taken\naction and then it goes to [INAUDIBLE] that's\nthree more candidates, and then you provide\nnew function for each",
    "start": "3329530",
    "end": "3336048"
  },
  {
    "text": "of them more [INAUDIBLE]? So what's-- OK. Do you expand?",
    "start": "3336049",
    "end": "3341730"
  },
  {
    "text": "Do you just [INAUDIBLE]\nalso this calculation? Yeah, so maybe let me-- yeah, I think that's\na good question.",
    "start": "3341730",
    "end": "3347770"
  },
  {
    "text": "Maybe let me just define it. Yeah, this is just\na intuition so far.",
    "start": "3347770",
    "end": "3360349"
  },
  {
    "text": "So what does this really mean? It's that you-- so you start\nwith s. s0 is equal to s.",
    "start": "3360349",
    "end": "3370589"
  },
  {
    "text": "And then you say sa-- a0 is equal to pi of s0.",
    "start": "3370590",
    "end": "3376799"
  },
  {
    "text": "And then you say\ns1 is equal to-- is sampled from Ps0,a0.",
    "start": "3376799",
    "end": "3383819"
  },
  {
    "text": "So you start with s. And then you take action, a,\naccording to the policy pi.",
    "start": "3383819",
    "end": "3391319"
  },
  {
    "text": "And then you say the\nenvironment-- take a step. And the environment's draw the\nnext state, given s0 and a0.",
    "start": "3391319",
    "end": "3397930"
  },
  {
    "text": "And then you play a1th\naccording to the observation s1, pi of s1, so and so forth.",
    "start": "3397930",
    "end": "3404600"
  },
  {
    "text": "You play this game. And then you say--\nyou look at the reward",
    "start": "3404600",
    "end": "3411319"
  },
  {
    "text": "that you've accumulated\nthroughout this process, and then to infinity.",
    "start": "3411319",
    "end": "3423789"
  },
  {
    "text": "And you say-- you\ntake this expectation.",
    "start": "3423790",
    "end": "3430270"
  },
  {
    "text": "Conditional s0 is equals to s. And this is definition\nof the value function.",
    "start": "3430270",
    "end": "3438270"
  },
  {
    "text": "The value function is the-- this is the expected total\npayoff, expected discounted",
    "start": "3438270",
    "end": "3444809"
  },
  {
    "text": "total payoff, of this game. The game is-- the processes\nthat you start with s.",
    "start": "3444810",
    "end": "3450789"
  },
  {
    "text": "And then you play this policy. And the environment\ndoes what it should do. And you always play this policy.",
    "start": "3450789",
    "end": "3456869"
  },
  {
    "text": "So that's the value\nof this status. I have a question.",
    "start": "3456869",
    "end": "3461880"
  },
  {
    "text": "How do you select the\ns1 and s P basically,",
    "start": "3461880",
    "end": "3467900"
  },
  {
    "text": "all the future states? [INAUDIBLE] the current states? All the future states are\nselected by the environment based on s0-- so the s1 is selected\nby s0 and a0. OK.",
    "start": "3467900",
    "end": "3473900"
  },
  {
    "text": "And s2 is selected by-- if you have-- maybe\nlet's just continue here. I was having an issue\nwith that if you include",
    "start": "3473900",
    "end": "3480390"
  },
  {
    "text": "all the future states\nthat are possible after a0 or you're just going\nthrough them all?",
    "start": "3480390",
    "end": "3487380"
  },
  {
    "text": "I'm sampling as well from-- Oh, you're sampling. I'm sampling from\nthe environment.",
    "start": "3487380",
    "end": "3494010"
  },
  {
    "text": "So if the environment\nis deterministic, I'm just going to talk-- it's just fixed. The environment decides it.",
    "start": "3494010",
    "end": "3499960"
  },
  {
    "text": "If there is a-- if the environment is random,\nthen you sample one from it. But eventually, you take\naverage over all the possible--",
    "start": "3499960",
    "end": "3506720"
  },
  {
    "text": "so basically, you're\nsimulating a world with all the possibilities. But, of course, each\npossibility has different--",
    "start": "3506720",
    "end": "3512828"
  },
  {
    "text": "each possible future has\ndifferent probabilities, some more likely to show up,\nsome less likely to show up.",
    "start": "3512829",
    "end": "3519480"
  },
  {
    "text": "And then, you look\nat the rewards for every possible\nreward of the future. And then, you take average\nover all the possible rewards.",
    "start": "3519480",
    "end": "3530030"
  },
  {
    "text": "So all? Do you take the average of\nall possible rewards for sS1?",
    "start": "3530030",
    "end": "3535609"
  },
  {
    "text": "No, for all of them,\ns0, Rs1, Rs2, and so on.",
    "start": "3535609",
    "end": "3542000"
  },
  {
    "text": "So basically, just\nyou apply this policy. And you try this policy\nin the real world. And you can-- the real\nworld is stochastic,",
    "start": "3542000",
    "end": "3549079"
  },
  {
    "text": "something random\nmay happen, and-- but you just try this out,\nand you collect all the reward",
    "start": "3549079",
    "end": "3554910"
  },
  {
    "text": "and you take expectation of\nthe reward, the total reward. [INAUDIBLE] This is a definition\nI'm trying to give you.",
    "start": "3554910",
    "end": "3562589"
  },
  {
    "text": "How you really do it,\nthat's a different question. OK. This is the definition\nof the concept.",
    "start": "3562589",
    "end": "3579619"
  },
  {
    "text": "So why I'm defining this,\nI'm defining this because-- there are two reasons\nto define this.",
    "start": "3579619",
    "end": "3585020"
  },
  {
    "text": "Well, one reason is\nthat this is capturing the value of the state S.\nIf the state S is good,",
    "start": "3585020",
    "end": "3592269"
  },
  {
    "text": "that means that this state\nis a good initial state. If you start with the state,\nyou can accumulate more rewards.",
    "start": "3592270",
    "end": "3598079"
  },
  {
    "text": "And also this\ndescribes the goodness or the quality of the policy.",
    "start": "3598079",
    "end": "3605950"
  },
  {
    "text": "If the policy is good, then\nthis value would be higher. If the policy is-- just\nkeep doing the right thing, you get more and more rewards.",
    "start": "3605950",
    "end": "3612310"
  },
  {
    "text": "So in some sense, you can say\nthat, actually, the problem",
    "start": "3612310",
    "end": "3617700"
  },
  {
    "text": "is really just\ntrying to figure out what's the right pi that can\noptimize your value function.",
    "start": "3617700",
    "end": "3629230"
  },
  {
    "text": "So you can\nreformulate a problem. Before we were trying to\nfind the sequence of action to maximize the reward.",
    "start": "3629230",
    "end": "3634730"
  },
  {
    "text": "So now, I think\nwe can just change our perspective of\nsaying that we are trying to find out the policy such\nthat I can optimize my value",
    "start": "3634730",
    "end": "3642609"
  },
  {
    "text": "function. So basically, your new\ngoal is that you are",
    "start": "3642609",
    "end": "3649259"
  },
  {
    "text": "maximizing over all policies. You maximize this Vpi(s0).",
    "start": "3649260",
    "end": "3655480"
  },
  {
    "text": "So here I'm assuming s0 is\ndeterministic, is given. OK, maybe I should just--",
    "start": "3655480",
    "end": "3661589"
  },
  {
    "text": "yeah, so suppose you\nare given some s0, so that the question\nis s0 is given, and you are just\nbasically maximizing--",
    "start": "3661589",
    "end": "3667840"
  },
  {
    "text": "you are trying to find\nall the best policies such that the value of s0 is\nmaximized under this policy.",
    "start": "3667840",
    "end": "3677230"
  },
  {
    "text": "So basically, in some\nsense, if you can see,",
    "start": "3677230",
    "end": "3683660"
  },
  {
    "text": "if you can find out what\nthis function is, Vpi(S0), for every pi, if you can know-- for every policy, suppose you\ncan figure out this number,",
    "start": "3683660",
    "end": "3691330"
  },
  {
    "text": "then you can just\nenumerate over all policies and see which one has the\nbest of return, total return.",
    "start": "3691330",
    "end": "3701558"
  },
  {
    "text": "So of course, that might not\nbe an efficient algorithm, but conceptually that's\nwhat we're trying to do. So we are trying to\nfigure out what's",
    "start": "3701559",
    "end": "3709318"
  },
  {
    "text": "the reward for every\npolicy, and then you try to pick the best policy. So basically\ncomputing this Vpi--",
    "start": "3709319",
    "end": "3720980"
  },
  {
    "text": "computing this is called--",
    "start": "3720980",
    "end": "3726320"
  },
  {
    "text": "sometimes it's called\npolicy evaluation. You are evaluating how\ngood this policy is.",
    "start": "3726320",
    "end": "3738140"
  },
  {
    "text": "And once we know how to\ndo the policy evaluation, then you can try to do\nthe policy maximization to maximize over the policy.",
    "start": "3738140",
    "end": "3745660"
  },
  {
    "text": "So first thing I'm\ngoing answer is how do you do the\npolicy evaluation.",
    "start": "3745660",
    "end": "3751300"
  },
  {
    "text": "So how do you do it? It turns out that,\nbasically, you just have to do a recursion.",
    "start": "3751300",
    "end": "3761068"
  },
  {
    "text": "So to do policy evaluation,\nyou just do some recursion.",
    "start": "3761069",
    "end": "3768440"
  },
  {
    "text": "So what does that mean? So I guess, suppose they think\nabout the policy at state s--",
    "start": "3768440",
    "end": "3778109"
  },
  {
    "text": "so let's just use\nthe definition. The definition is\nthat, basically, you start from state s and then--",
    "start": "3778109",
    "end": "3783880"
  },
  {
    "text": "the definition is\nthat you say this is the expectation of this\nthe total payoff, discounted",
    "start": "3783880",
    "end": "3791150"
  },
  {
    "text": "payoff. I'm assuming that you with\nstate s0 is equal to s.",
    "start": "3791150",
    "end": "3801960"
  },
  {
    "text": "And because s0 is equal to s--\nso you just say this is equal to R(s), this is the reward\nyou get in the first step,",
    "start": "3801960",
    "end": "3810930"
  },
  {
    "text": "and then you say-- we have some discount factor. You pull one gamma off\nbecause all the other terms",
    "start": "3810930",
    "end": "3817619"
  },
  {
    "text": "has one gamma there. And then you say this is\nR(s1) plus gamma R(s2)",
    "start": "3817619",
    "end": "3826289"
  },
  {
    "text": "plus gamma square\nR(s3), so and so forth.",
    "start": "3826290",
    "end": "3831470"
  },
  {
    "text": "Note that I pulled\none gamma out. So before it was\ngamma square R(s2), but now it becomes\nonly one gamma.",
    "start": "3831470",
    "end": "3840119"
  },
  {
    "text": "And if you look at\nthe rest of this term, this is something that is\nactually something you have,",
    "start": "3840119",
    "end": "3847490"
  },
  {
    "text": "we have-- this is also\nmeaningful in the sense that this term is really just\nthe total payoff ",
    "start": "3847490",
    "end": "3862230"
  },
  {
    "text": "obtained from starting at s1.",
    "start": "3862230",
    "end": "3871010"
  },
  {
    "text": "Basically, this is just\nthat you start with s1 and you apply this\npolicy iteratively,",
    "start": "3871010",
    "end": "3876329"
  },
  {
    "text": "and what's the payoff you\nshould get without the gamma. The gamma is a factor,\nso without the gamma.",
    "start": "3876330",
    "end": "3883500"
  },
  {
    "text": "This is basically the total\npayoff if you start with s1. So that means that this\nquantity is really, literally,",
    "start": "3883500",
    "end": "3890760"
  },
  {
    "text": "just the Vpi(S1).",
    "start": "3890760",
    "end": "3898220"
  },
  {
    "text": "So that means that you've got\na recursion, in some sense, between V and Vpil-- between Vs and Vs1.",
    "start": "3898220",
    "end": "3905150"
  },
  {
    "text": "So maybe just more formally--",
    "start": "3905150",
    "end": "3919119"
  },
  {
    "text": "so I can write this as this.",
    "start": "3919119",
    "end": "3934930"
  },
  {
    "text": "So I think maybe\ntechnically, I should say that, without expectations,\nthis pi is equal to--",
    "start": "3934930",
    "end": "3942000"
  },
  {
    "text": "I guess, let me not to\nbe too technical here. But you can probably\nsee what I mean.",
    "start": "3942000",
    "end": "3949710"
  },
  {
    "text": "So here, Vpi(s1) is\nbasically the reward you get from executing from s1.",
    "start": "3949710",
    "end": "3955480"
  },
  {
    "text": "But why are we still\nhaving an expectation here? This is because\ns1 is also random. It's not like s1 is\ndeterministically determined.",
    "start": "3955480",
    "end": "3964000"
  },
  {
    "text": "s1 is drawn from applying-- is drawing from\nthis environment.",
    "start": "3964000",
    "end": "3970510"
  },
  {
    "text": "So s1 has this distribution. So s1 is drawn from Ps0,a0.",
    "start": "3970510",
    "end": "3978470"
  },
  {
    "text": "A0 is pi of s0. So this is P of sS0, pi of s0.",
    "start": "3978470",
    "end": "3986140"
  },
  {
    "text": "Any question so far?",
    "start": "3986140",
    "end": "3997890"
  },
  {
    "text": "So maybe, just to\nbe more explicit. Basically, this is\nequals to R(s) plus--",
    "start": "3997890",
    "end": "4003519"
  },
  {
    "text": "if you write out\nthis expectation, you can write it as a sum. So basically, you draw\ns1 from this discussion.",
    "start": "4003520",
    "end": "4009799"
  },
  {
    "text": "That means that you-- it means that you just say, for\nevery possible s1, in a set s,",
    "start": "4009799",
    "end": "4015829"
  },
  {
    "text": "you look at the density of s1. This is the chance that you\nsee s1 in the next step.",
    "start": "4015830",
    "end": "4025220"
  },
  {
    "text": "And then times the Vpi(s1). That's just how I expanded it,\nthe definition of expectation.",
    "start": "4025220",
    "end": "4034869"
  },
  {
    "text": "I think maybe people,\ntypically, when sometimes--",
    "start": "4034869",
    "end": "4039960"
  },
  {
    "text": "it doesn't really matter\nwhat variables I use for s1. I can use any variables. So maybe I'll just use s'\njust to be consistent with",
    "start": "4039960",
    "end": "4052750"
  },
  {
    "text": "the technical notations. So basically, you loop\nover all possible s', all possible next states.",
    "start": "4052750",
    "end": "4058329"
  },
  {
    "text": "And you first say I'm looking\nat what's the chance to arrive at that state, s', and then I\nmultiply that with the value",
    "start": "4058329",
    "end": "4065038"
  },
  {
    "text": "function s'. So this is just the equivalent\nto this expectation above.",
    "start": "4065039",
    "end": "4077010"
  },
  {
    "text": "OK, so why this useful--",
    "start": "4077010",
    "end": "4082579"
  },
  {
    "text": "so first of all, let me say\nthis is called Bellman equation.",
    "start": "4082579",
    "end": "4089230"
  },
  {
    "text": "This is a equation about\nthe value of function V pi. And it's often called\nBellman equation.",
    "start": "4089230",
    "end": "4096410"
  },
  {
    "text": "And also, this is-- technically, I think, if\nyou really want to have a--",
    "start": "4096410",
    "end": "4101460"
  },
  {
    "text": "sometimes this is called\nBellman equation for V pi because there is going to be\nanother Bellman equation, which has exactly the same name.",
    "start": "4101460",
    "end": "4107630"
  },
  {
    "text": "People also call\nit Bellman equation for some other quantities. And we will define\nit in a moment.",
    "start": "4107630",
    "end": "4113210"
  },
  {
    "text": "So this Bellman equation--\nwhy it is useful. It's useful because \nthis is",
    "start": "4113210",
    "end": "4122609"
  },
  {
    "text": "a linear function in VpiS. So you can think of it as--",
    "start": "4122610",
    "end": "4129040"
  },
  {
    "text": "maybe I'll all use here.",
    "start": "4129040",
    "end": "4134100"
  },
  {
    "text": "So you can think of\nVpi(1) up to Vpi(1).",
    "start": "4134100",
    "end": "4141600"
  },
  {
    "text": "Recall that M is the number\nof states, I defined. So you can think of this, all\nof these, as the variables,",
    "start": "4141600",
    "end": "4150440"
  },
  {
    "text": "as M variables. And the Bellman equation\ngives M equations",
    "start": "4150440",
    "end": "4166969"
  },
  {
    "text": "about these variables.",
    "start": "4166969",
    "end": "4173960"
  },
  {
    "text": "Why there are M equations--\nbecause for every s-- this is true for\nevery s, for every s",
    "start": "4173960",
    "end": "4180130"
  },
  {
    "text": "it has the equation that\ninvolves these variables, pi of-- Vpi(s) and Vpi(s')\nin a linear way.",
    "start": "4180130",
    "end": "4189200"
  },
  {
    "text": "So the Bellman equation is\nthe states of linear system equations in this variable\nVpi(1) up to Vpi(M)",
    "start": "4189200",
    "end": "4198580"
  },
  {
    "text": "So to figure out what\nis Vpi(I), you just have to solve the\nsystem equations.",
    "start": "4198580",
    "end": "4205670"
  },
  {
    "text": "I'm not sure whether this make-- is this too abstract?",
    "start": "4205670",
    "end": "4212920"
  },
  {
    "text": "So in some sense, maybe, it just\ngive you a concrete example. So for this concrete\nexample here,",
    "start": "4212920",
    "end": "4220440"
  },
  {
    "text": "if you write out this Bellman\nquestion, what will happen is the following.",
    "start": "4220440",
    "end": "4230630"
  },
  {
    "text": "So you probably-- for\nthe concrete example, you have maybe\nsomething like a V pi.",
    "start": "4230630",
    "end": "4237270"
  },
  {
    "text": "I'm just plugging in some\nconcrete thing, say, maybe 6. So you're trying to figure out\nwhat's the equation for Vpi(6).",
    "start": "4237270",
    "end": "4244330"
  },
  {
    "text": "If you think about the\nequation, your first off thing is the R(6). This is the reward you\nget in the first step.",
    "start": "4244330",
    "end": "4250520"
  },
  {
    "text": "And then you-- times\ngamma times the reward you are getting in the future.",
    "start": "4250520",
    "end": "4255980"
  },
  {
    "text": "So where you have the\nsum, sum S' E S Psa,",
    "start": "4255980",
    "end": "4264100"
  },
  {
    "text": "something like S', Vpi(S'). And you plug in all\nof these numbers here.",
    "start": "4264100",
    "end": "4271080"
  },
  {
    "text": "And then you get an equation,\nwhich depends on Vpi(6) and all of the other Vpi(S').",
    "start": "4271080",
    "end": "4276620"
  },
  {
    "text": "But this is a linear equation. So think of this is a variable,\nall of these as variables.",
    "start": "4276620",
    "end": "4282469"
  },
  {
    "text": "This is an equation with\na bunch of variables, but they are linear\nin those variables.",
    "start": "4282469",
    "end": "4288300"
  },
  {
    "text": "And you can write\nthis for everything. You can say this is\nR(5) plus dot, dot.",
    "start": "4288300",
    "end": "4295020"
  },
  {
    "text": "And you have the\nsystem equations. Each equation is a linear\nequation in the variables.",
    "start": "4295020",
    "end": "4300560"
  },
  {
    "text": "So basically, this\nmeans that you're just computing Vpi(S) for every S\nby some linear equation solver,",
    "start": "4300560",
    "end": "4314820"
  },
  {
    "text": "by solving the linear equations.",
    "start": "4314820",
    "end": "4321790"
  },
  {
    "text": "And because they are linear,\nyou can use the efficient solver like just-- how do you solve\nlinear equations?",
    "start": "4321790",
    "end": "4327520"
  },
  {
    "text": "You can, I guess,\none way to do it is do some inverse of the matrix. Maybe the other way is\nto do a linear equation.",
    "start": "4327520",
    "end": "4335560"
  },
  {
    "text": "But that's a sub-module\nthat you can just invoke. You can invoke some\noff-the-shelf algorithm",
    "start": "4335560",
    "end": "4341449"
  },
  {
    "text": "to solve the linear equation.",
    "start": "4341449",
    "end": "4349170"
  },
  {
    "text": "Any questions? Is it possible there\nare infinite solutions? Can you say it again? Is it possible there are\ninfinite solutions [INAUDIBLE]??",
    "start": "4349170",
    "end": "4356810"
  },
  {
    "text": "Is it possible that they\nare infinite of solutions? So \nI think, in this case,",
    "start": "4356810",
    "end": "4367260"
  },
  {
    "text": "it's just not possible. Why it's not possible\nis probably not",
    "start": "4367260",
    "end": "4373090"
  },
  {
    "text": "super obvious to see. In some sense, you have--",
    "start": "4373090",
    "end": "4378270"
  },
  {
    "text": "at least it passes\nthe trivialness check. But if you count\nhow many equations, or how many variables,\nthey are exactly the same.",
    "start": "4378270",
    "end": "4386570"
  },
  {
    "text": "So typically, you probably\nshould have a reasonable-- you have a unique solution. And I think, in\nthis case, you can",
    "start": "4386570",
    "end": "4392610"
  },
  {
    "text": "prove that there is a\nunique solution just because this set of equations\nhas some special properties,",
    "start": "4392610",
    "end": "4399630"
  },
  {
    "text": "but maybe let's not\nget into that too much.",
    "start": "4399630",
    "end": "4406560"
  },
  {
    "text": "OK, great.",
    "start": "4406560",
    "end": "4413750"
  },
  {
    "text": "So we know how to-- So basically, we know\nhow to evaluate Vpi(S). In this equation, Psa\nwould be different",
    "start": "4413750",
    "end": "4421380"
  },
  {
    "text": "depending on the initial state? Uh-hum. Can you say it again?",
    "start": "4421380",
    "end": "4430659"
  },
  {
    "text": "The Psa-- This one? And the probability? This one? That would be different\ndepending on the initial state?",
    "start": "4430660",
    "end": "4438539"
  },
  {
    "text": "No, this Psa(s'), this\nP is the dynamics. It's the transition\nprobability, which is global,",
    "start": "4438540",
    "end": "4447000"
  },
  {
    "text": "which is what you-- it's a given\nproperty of the MDP. For example, if you subtract\nthis first state question--",
    "start": "4447000",
    "end": "4458530"
  },
  {
    "text": "so the second one would be\njust R0 6 minus [INAUDIBLE]??",
    "start": "4458530",
    "end": "4465369"
  },
  {
    "text": "If you subtract these two? Yeah. Oh, no, because--",
    "start": "4465370",
    "end": "4470750"
  },
  {
    "text": "Oh, right. That's a good point. Sorry. I think this s and\na, I think I'll",
    "start": "4470750",
    "end": "4476400"
  },
  {
    "text": "only partially replace this. This will be 6. Yeah. Oh, is that what you're asking?",
    "start": "4476400",
    "end": "4483340"
  },
  {
    "text": "Yeah. OK, cool. Sure. So I think this will be-- that's a great question.",
    "start": "4483340",
    "end": "4488940"
  },
  {
    "text": "So this should be And then, if you write this,\nthen you will have gamma 5 pi",
    "start": "4488940",
    "end": "4499630"
  },
  {
    "text": "of 5 S', Vpi(S'). So all the coefficients\nare different",
    "start": "4499630",
    "end": "4506000"
  },
  {
    "text": "for different equations--\ndifferent lines. Yeah, thanks.",
    "start": "4506000",
    "end": "4513040"
  },
  {
    "text": "OK, cool. So we have completed the Vpi.",
    "start": "4513040",
    "end": "4518830"
  },
  {
    "text": "But the Vpi-- But the next question\nis how do you-- so basically, we have solved\nthe policy evaluation.",
    "start": "4518830",
    "end": "4524360"
  },
  {
    "text": "Now we need to figure out how\ndo we maximize the policy part. How do we find out\nthe best policy?",
    "start": "4524360",
    "end": "4530960"
  },
  {
    "text": "So let me find out some places.",
    "start": "4530960",
    "end": "4550150"
  },
  {
    "text": "It turns out that you can\nuse a similar technique to find out the best policy.",
    "start": "4550150",
    "end": "4555739"
  },
  {
    "text": "So here is some definitions. So first of all, let's define\nV star S to be max pi Vpi(S).",
    "start": "4555739",
    "end": "4568250"
  },
  {
    "text": "So what does this mean? This means that you are looking\nat all the possible policies that you can use.",
    "start": "4568250",
    "end": "4574750"
  },
  {
    "text": "Starting from s. You look at-- basically you\ntry all different policies starting from s. And you ask which\npolicy give me the best",
    "start": "4574750",
    "end": "4582810"
  },
  {
    "text": "reward of total payoff. And the value of that total\npayoff will be the V stars.",
    "start": "4582810",
    "end": "4590500"
  },
  {
    "text": "So V stars is the\nintrinsic value of state s. So we're saying\nthat the state s-- just how valuable this\nstate s is, right?",
    "start": "4590500",
    "end": "4597940"
  },
  {
    "text": "And how valuable is measured by\nusing the best possible policy. Vpi(s) depends on both the pi\nand S. If you use a bad policy,",
    "start": "4597940",
    "end": "4608180"
  },
  {
    "text": "Vpi(S) may be low. But v star S is\njust saying that, what's the value of\nthis state if you use the best possible\npolicy in the future steps?",
    "start": "4608180",
    "end": "4617440"
  },
  {
    "text": "And then you can also define\nthe so-called pi star. This is the so-called\noptimal policy. This is it goes to the\narg max over pi Vpi(s).",
    "start": "4617440",
    "end": "4628960"
  },
  {
    "text": "This is asking what is-- basically, you are just\ndoing exactly the arg max of the previous statement.",
    "start": "4628960",
    "end": "4635139"
  },
  {
    "text": "So the best policy that\nachieves the maximizer",
    "start": "4635140",
    "end": "4642250"
  },
  {
    "text": "is defined to be the PI star. This is the optimal policy\nwe are trying to find out.",
    "start": "4642250",
    "end": "4647390"
  },
  {
    "text": "So with these two\nannotations I think I can--",
    "start": "4647390",
    "end": "4654760"
  },
  {
    "text": "so I'm going to first find\nout what the V star s. And then the pi star s will be\nrelatively easy to do, because,",
    "start": "4654760",
    "end": "4661690"
  },
  {
    "text": "as you'll see-- so the first question\nI want to answer is how do we find out\nwhat's the V star of s?",
    "start": "4661690",
    "end": "4667690"
  },
  {
    "text": "What's the intrinsic value\nof each of the states? So it turns out that you can\ndo a similar type of Bellman",
    "start": "4667690",
    "end": "4674270"
  },
  {
    "text": "equation as the V pi,\nbut just the whole thing involves a lot of max operators.",
    "start": "4674270",
    "end": "4682280"
  },
  {
    "text": "So here is what I mean. So if you think\nabout the V star s, again, you're trying\nto get a recursion",
    "start": "4682280",
    "end": "4689190"
  },
  {
    "text": "for V star s because V star s is\ncomplicated because it depends on all the future states. So you want it to\nhave a recursion.",
    "start": "4689190",
    "end": "4695678"
  },
  {
    "text": "So I'm going to be a little\nbit-- the exact math here. I think if you want\nto make it rigorous",
    "start": "4695679",
    "end": "4701300"
  },
  {
    "text": "you have to justify\nmore formally, but I'm not going\nto deal with that. So I'm going to be\nslightly sloppy here just",
    "start": "4701300",
    "end": "4708510"
  },
  {
    "text": "for simplicity. So you take an arg max\nover pi R(S) plus gamma.",
    "start": "4708510",
    "end": "4728849"
  },
  {
    "text": "So this, here, I'm using\nthe Bellman equation I just derived. So this is the Bellman\nequation I just",
    "start": "4728850",
    "end": "4735130"
  },
  {
    "text": "derived because Vpi(s)\nis equal to this, right?",
    "start": "4735130",
    "end": "4742310"
  },
  {
    "text": "And so, let's think about this. So first of all, you\nare maximizing over pi.",
    "start": "4742310",
    "end": "4748090"
  },
  {
    "text": "So this one doesn't\ndepend on pi even. So we can just take it out. So we'll just say\nthis is R(s) plus--",
    "start": "4748090",
    "end": "4779949"
  },
  {
    "text": "So now you look at this thing. So pi shows-- you\nwant to maximize this. And pi shows up in\nseveral different places.",
    "start": "4779950",
    "end": "4787170"
  },
  {
    "text": "pi shows up here and\npi shows up here. So the pi shows up here in\nthe sense that if you do use",
    "start": "4787170",
    "end": "4794130"
  },
  {
    "text": "different pi, you're\ngoing to arrive-- you're going to have a different\ntransition probability so that",
    "start": "4794130",
    "end": "4799159"
  },
  {
    "text": "you can arrive at different\nset of state s' with different probabilities.",
    "start": "4799160",
    "end": "4804510"
  },
  {
    "text": "And this pi here is\ntrying to capture what happens after this step.",
    "start": "4804510",
    "end": "4809890"
  },
  {
    "text": "So after we already see S',\nwhat's the future reward? This V(s') is\nbasically telling you,",
    "start": "4809890",
    "end": "4816050"
  },
  {
    "text": "if you have s', what's the\nfuture possible payoff after s'?",
    "start": "4816050",
    "end": "4821480"
  },
  {
    "text": "So I'm going to-- so I'm going to be a\nlittle sloppy here. But let's say,\nsuppose we optimize",
    "start": "4821480",
    "end": "4827690"
  },
  {
    "text": "this first Vpi(S) first,\nthis occurrence of pi(S). So if I choose the pi such\nthat this is the best,",
    "start": "4827690",
    "end": "4834610"
  },
  {
    "text": "I mean what you should do-- so what you should do is you\nshould try to figure out,",
    "start": "4834610",
    "end": "4841150"
  },
  {
    "text": "you should try to make the\npi(S) give you the best action.",
    "start": "4841150",
    "end": "4847170"
  },
  {
    "text": "So basically, what I'm saying\nis that this is equals to-- maybe let me write it\ndown, and then it's easier to explain\nwhat I want ",
    "start": "4847170",
    "end": "4868880"
  },
  {
    "text": "to prove. So basically, I'm\nsaying that I'm going to choose the pi(s) to\nbe the action a that maximizes",
    "start": "4868880",
    "end": "4874030"
  },
  {
    "text": "this. So pi(s) is some action. So I'll just write\na here and then say I tried to choose the\nbest a such that, such",
    "start": "4874030",
    "end": "4882020"
  },
  {
    "text": "that the pi(s) is equal to a. And I still want\nto maximize this. However, the pi\naffects two things.",
    "start": "4882020",
    "end": "4888150"
  },
  {
    "text": "pi also affect what\nhappens in the next-- so then, I need to also try\nto make sure the pi makes",
    "start": "4888150",
    "end": "4895780"
  },
  {
    "text": "the future steps the biggest.",
    "start": "4895780",
    "end": "4901400"
  },
  {
    "text": "So and then, the\nnice thing about this is that this term is something\nalready defined, which I can--",
    "start": "4901400",
    "end": "4909690"
  },
  {
    "text": "it's a recursion. This is just V star s'.",
    "start": "4909690",
    "end": "4935060"
  },
  {
    "text": "So basically, if you look\nat the final equation, it's like you are\ntrying to say-- you are trying to try\nall the possible action,",
    "start": "4935060",
    "end": "4941630"
  },
  {
    "text": "a's, you take at\nthis step, right? That's why you take max. So you try all\npossible a's, and use--",
    "start": "4941630",
    "end": "4947889"
  },
  {
    "text": "Oh, I guess-- sorry. My bad. s, a.",
    "start": "4947889",
    "end": "4954079"
  },
  {
    "text": "For any possible a's you have\na probability to arrive at s'. And then you say that after you\narrive at S', after that you",
    "start": "4954080",
    "end": "4964290"
  },
  {
    "text": "use the optimal policy, V\nstar S', starting from that. And this is the-- basically, this\npart is the reward,",
    "start": "4964290",
    "end": "4972580"
  },
  {
    "text": "the best reward\nyou can get if you apply action a at this step.",
    "start": "4972580",
    "end": "4977969"
  },
  {
    "text": "Basically, if you apply\naction a at this step, what's the reward\nthat you can get? You're going to have some\ntransition probabilities",
    "start": "4977970",
    "end": "4984800"
  },
  {
    "text": "to arrive at s'. And then after you arrive at\ns', you'll have some maximum possible reward V star s'.",
    "start": "4984800",
    "end": "4990760"
  },
  {
    "text": "S'. So that's why the sum is\nthe best possible reward you can get if you apply\naction a at this step.",
    "start": "4990760",
    "end": "4997448"
  },
  {
    "text": "And then, you max over a. And that's the best\nthing you can do. So I guess it's\nsimilar to earlier",
    "start": "4997449",
    "end": "5004450"
  },
  {
    "text": "when you finding out\nthe entire policy now to find the optimal\npolicy, again, we should eliminate variables. We solve for all of those\nto get the optimal policy?",
    "start": "5004450",
    "end": "5015760"
  },
  {
    "text": "Yeah. OK. So that's the next step. That's a good question. But maybe just any\nother questions before we move on\nto the next step?",
    "start": "5015760",
    "end": "5023920"
  },
  {
    "text": "OK, so great. So you get the equation. And let's see what equations. For every S, you\nhave an question.",
    "start": "5023920",
    "end": "5033739"
  },
  {
    "text": "And so, you have the\nsame number of variables and the same number of\nequations, the m variables and m equations.",
    "start": "5033739",
    "end": "5039480"
  },
  {
    "text": "But the problem is\nthat now the equations are not linear anymore. [INAUDIBLE]",
    "start": "5039480",
    "end": "5044580"
  },
  {
    "text": "So they are not\nlinear equations. So you still have\nnon-linear equations and it involves m variables. And you have m of\nthese equations.",
    "start": "5044580",
    "end": "5052830"
  },
  {
    "text": "So that's the challenge. So there is no\noff-the-shelf solver you can use to solve the set of\nequations that are nonlinear.",
    "start": "5052830",
    "end": "5061520"
  },
  {
    "text": "So that's why we need to\nintroduce this so-called value",
    "start": "5061520",
    "end": "5072420"
  },
  {
    "text": "iteration.",
    "start": "5072420",
    "end": "5083830"
  },
  {
    "text": "So how do we solve\nthese equations? So we solve the equations by\nthe so-called value iteration?",
    "start": "5083830",
    "end": "5092080"
  },
  {
    "text": "So first of all, let's\njust think of this V star.",
    "start": "5092080",
    "end": "5098580"
  },
  {
    "text": "Let's just define our\nnotation V star to be V star 1 up to V star m.",
    "start": "5098580",
    "end": "5105400"
  },
  {
    "text": "This is the vector in IRm. So I view this\nfunction as a vector.",
    "start": "5105400",
    "end": "5113670"
  },
  {
    "text": "Because I only have\nm possible inputs, I can view this function\nas a vector of dimension m.",
    "start": "5113670",
    "end": "5119290"
  },
  {
    "text": "And I'm going to say that-- and then my equations\ncan be written as this.",
    "start": "5119290",
    "end": "5126310"
  },
  {
    "text": "So it's V star equals to 1,\nis equals to something like--",
    "start": "5126310",
    "end": "5133550"
  },
  {
    "text": "maybe let's just say, each\nof these equations like this,",
    "start": "5133550",
    "end": "5139500"
  },
  {
    "text": "R(1) plus some max,\nsomething like this. And V star as 2 is equals\nto R(2) plus max, something",
    "start": "5139500",
    "end": "5147460"
  },
  {
    "text": "like this. That's my system\nequations that I-- I have so many equations.",
    "start": "5147460",
    "end": "5154050"
  },
  {
    "text": "Each equations look like this. And I can abstractly write\nthis as the following.",
    "start": "5154050",
    "end": "5160829"
  },
  {
    "text": "So I can abstractly write\nthis as this whole vector. Let's call it V star.",
    "start": "5160830",
    "end": "5168010"
  },
  {
    "text": "And the right-hand\nside is something that involves V star again. So you call this whole\nthing B of V star.",
    "start": "5168010",
    "end": "5179380"
  },
  {
    "text": "So basically, this is\njust a function of V star. So I'm just\nabstractly writing it",
    "start": "5179380",
    "end": "5184960"
  },
  {
    "text": "as a function of V star,\nwhich gives you a vector. This is also vector. So then, if I do this,\nthen, basically, my equation",
    "start": "5184960",
    "end": "5194270"
  },
  {
    "text": "can be written as V star\nis equal to B of V star. I'm not doing anything deep.",
    "start": "5194270",
    "end": "5200020"
  },
  {
    "text": "It's just rewriting the thing\nwith a very abstract notation. So this is my-- the\nform of my equation.",
    "start": "5200020",
    "end": "5206239"
  },
  {
    "text": "V star is going to B of V star. And B of V star is\nthe right-hand side of the Bellman equation.",
    "start": "5206239",
    "end": "5225380"
  },
  {
    "text": "So what I'm going\nto do is that-- the algorithm is very simple\nto find out the equations.",
    "start": "5225380",
    "end": "5231540"
  },
  {
    "text": "So this is taking inspiration\nfrom the so-called fixed point problem in math. If you haven't heard\nof it, that don't",
    "start": "5231540",
    "end": "5239650"
  },
  {
    "text": "matter-- it doesn't matter. Don't worry. It doesn't matter. But roughly speaking,\nthe thing is",
    "start": "5239650",
    "end": "5244900"
  },
  {
    "text": "that you think of this\nB as some operation. So you say that this V star is\na fixed point of this operation.",
    "start": "5244900",
    "end": "5252280"
  },
  {
    "text": "You apply this\noperation of V star, you arrive at the same thing. So that's the connection to the\nso-called fixed point problem.",
    "start": "5252280",
    "end": "5260050"
  },
  {
    "text": "But if you don't\nknow the connection, basically, somehow,\nthere is some theory math which says that if you want to\nsolve this fixed point problem,",
    "start": "5260050",
    "end": "5266820"
  },
  {
    "text": "you just have to iterate\nuntil it converges. So what does that mean? That really just means that\nyou have this so-called value,",
    "start": "5266820",
    "end": "5274150"
  },
  {
    "text": "iteration. So what you do is you\nsay you initialize some V",
    "start": "5274150",
    "end": "5284400"
  },
  {
    "text": "in this dimension, R to the m. Maybe you can just do V is 0.",
    "start": "5284400",
    "end": "5292510"
  },
  {
    "text": "I think that's fine. We just initialize somewhere\nrandomly or maybe just",
    "start": "5292510",
    "end": "5297790"
  },
  {
    "text": "initalize to zero. And then you just\nhave a for loop. So you have a loop such\nthat at every time you",
    "start": "5297790",
    "end": "5305850"
  },
  {
    "text": "say v is updated to B B of V.",
    "start": "5305850",
    "end": "5313030"
  },
  {
    "text": "And you just keep iterating. And there's a\nguarantee that you will",
    "start": "5313030",
    "end": "5318790"
  },
  {
    "text": "converge to the fixed point. The fixed point will satisfy,\nv, is equal to b of V. And that's the V star.",
    "start": "5318790",
    "end": "5326210"
  },
  {
    "text": "And this updated\nreally just means what? This estimate just\nreally means that you say V(s) is equals to\nR(s) plus, basically",
    "start": "5326210",
    "end": "5335310"
  },
  {
    "text": "the max right-hand side\nof the Bellman equation.",
    "start": "5335310",
    "end": "5348350"
  },
  {
    "text": "So this is what really\nmeans when you really implement algorithm,\nyou say, you compute the right-hand side\nof the equation",
    "start": "5348350",
    "end": "5354860"
  },
  {
    "text": "with the hypothetical V, right? And then, you give this\nvalue to the new value of v,",
    "start": "5354860",
    "end": "5361830"
  },
  {
    "text": "where you update\na new value of v by the right-hand side\nof the Bellman equation.",
    "start": "5361830",
    "end": "5369060"
  },
  {
    "text": "Here I'm using this means\nthat I compute this value, and then I give this\nvalue to the VS.",
    "start": "5369060",
    "end": "5375800"
  },
  {
    "text": "I'm going to guarantee\nthat Yes so I",
    "start": "5375800",
    "end": "5381520"
  },
  {
    "text": "think you can guarantee\nthat there's a unique one. And you can convert to it\nin a certain amount of time.",
    "start": "5381520",
    "end": "5387949"
  },
  {
    "text": "I think that's the\nhomework question. So to do that, you have to. I think the homework question\nhas some hints on it.",
    "start": "5387949",
    "end": "5393510"
  },
  {
    "text": "So you basically\ncompare the distance between this V and\nthe true V, and you can see that the\ndistance between this V",
    "start": "5393510",
    "end": "5399600"
  },
  {
    "text": "is working V with\nthe true V star is kind of shrinking\niteratively. I think I'm running quite--",
    "start": "5399600",
    "end": "5414150"
  },
  {
    "text": "Yeah, so there's\none other algorithm, which is called policy\niteration, which",
    "start": "5414150",
    "end": "5420191"
  },
  {
    "text": "is very similar. But I think I just\nleave that to you all",
    "start": "5420191",
    "end": "5425261"
  },
  {
    "text": "for reading the lecture notes. It's basically it's also\nnot required for homework.",
    "start": "5425261",
    "end": "5433030"
  },
  {
    "text": "So just optionally, you can\nread it if you're interested. Thanks.",
    "start": "5433030",
    "end": "5436670"
  }
]