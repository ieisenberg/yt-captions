[
  {
    "text": "Welcome back everyone. So this is Lecture 9, and today, we're gonna talk about Bayesian methods,",
    "start": "4100",
    "end": "11450"
  },
  {
    "text": "um, that's the plan for today. So today we're gonna, um, cover two different approaches in-, um,",
    "start": "11450",
    "end": "18690"
  },
  {
    "text": "in Bayesian methods, a parametric approach and a non-parametric approach. And as an example of parametric approach,",
    "start": "18690",
    "end": "24240"
  },
  {
    "text": "we're gonna look at Bayesian linear regression and as an example of a Bayesian non-parametric approach,",
    "start": "24240",
    "end": "30105"
  },
  {
    "text": "we're gonna look at Gaussian processes. Right, and before we jump into today's topics,",
    "start": "30105",
    "end": "36570"
  },
  {
    "text": "a quick recap of what we covered in Lecture eight. So lecture eight was all about kernels, right?",
    "start": "36570",
    "end": "44235"
  },
  {
    "text": "And, we- we saw that a kernel, a function k of two parameters,",
    "start": "44235",
    "end": "50415"
  },
  {
    "text": "x and z, and these are typically two examples in your training set or in your test set, et cetera.",
    "start": "50415",
    "end": "57560"
  },
  {
    "text": "These are examples. A function of two of our examples, K is called a kernel.",
    "start": "57560",
    "end": "62880"
  },
  {
    "text": "If K of x, z can be represented as Phi of x transpose,",
    "start": "62880",
    "end": "68880"
  },
  {
    "text": "Phi of z for some Phi, that maps your examples to a P-dimensional uh, real space.",
    "start": "68880",
    "end": "75090"
  },
  {
    "text": "Now, P here can be, um, infinite, so P could be um, um, infinity.",
    "start": "75090",
    "end": "83165"
  },
  {
    "text": "Um, and some of the properties of the kernels, we saw that first, it- it needs to be symmetric,",
    "start": "83165",
    "end": "89695"
  },
  {
    "text": "uh, in its arguments. So, um, x and z the arguments can just pop and it needs to evaluate to the same value.",
    "start": "89695",
    "end": "95960"
  },
  {
    "text": "And for all, uh, for any finite collection of examples, um,",
    "start": "95960",
    "end": "102285"
  },
  {
    "text": "and these examples could be any examples, not necessarily your training set for any finite collection of examples.",
    "start": "102285",
    "end": "107885"
  },
  {
    "text": "The kernel matrix, that you construct, such that Kij equals, you know- the value of the i_jth cell in the matrix.",
    "start": "107885",
    "end": "117295"
  },
  {
    "text": "Evaluated as the kernel function evaluated on the ith and jth example, is positive semi-definite.",
    "start": "117295",
    "end": "125070"
  },
  {
    "text": "Right. And basically, the Mercer's theorem, tells us that, this property 2,",
    "start": "125070",
    "end": "132550"
  },
  {
    "text": "is a necessary and sufficient condition for K to be a kernel. So if- if you have a function K that satisfies this,",
    "start": "132550",
    "end": "140765"
  },
  {
    "text": "then it necessarily must be a Kernel, which means it can be represented in this way.",
    "start": "140765",
    "end": "146390"
  },
  {
    "text": "So this was the definition of a kernel and these are properties, and Mercer's theorem says, you know,",
    "start": "146390",
    "end": "152660"
  },
  {
    "text": "we can go back and forth- this set of properties is equivalent to- to, the definition of a kernel.",
    "start": "152660",
    "end": "158629"
  },
  {
    "text": "An in- an informal way of thinking about Mercer's theorem is, a matrix,",
    "start": "158630",
    "end": "163640"
  },
  {
    "text": "which is possibly infinite by infinite uh, dimension is PSD if and only if every sub-matrix is PSD.",
    "start": "163640",
    "end": "171230"
  },
  {
    "text": "Right? And Y is- and the way to think of Y, a kernel K represented like this, you know, um,",
    "start": "171230",
    "end": "179045"
  },
  {
    "text": "is PSD is, this statement is basically telling that K,",
    "start": "179045",
    "end": "187025"
  },
  {
    "text": "which, you know, this is possibly infinite dimensional.",
    "start": "187025",
    "end": "192000"
  },
  {
    "text": "If K can be represented as phi transpose, phi of two examples,",
    "start": "192650",
    "end": "197855"
  },
  {
    "text": "then you have this kind of an Eigen decomposition, or some kind of a symmetric decomposition,",
    "start": "197855",
    "end": "203405"
  },
  {
    "text": "that you can take the matrix K, the infinite dimension matrix K, and split it into, some phi,",
    "start": "203405",
    "end": "210815"
  },
  {
    "text": "where phi is a collection of- of columns, where each column is,",
    "start": "210815",
    "end": "216694"
  },
  {
    "text": "the function um, phi i of X. For example, if you have phi of X equals X squared XQ Log x,",
    "start": "216695",
    "end": "229190"
  },
  {
    "text": "and some set of features. Then, we would call Phi- phi 1 of X to be X squared,",
    "start": "229190",
    "end": "237090"
  },
  {
    "text": "phi 2 of X to be X cubed, and so on. So, each of these phi i corresponds to one such function, right?",
    "start": "237090",
    "end": "245280"
  },
  {
    "text": "That's- that's the general idea you want to have, in terms of thinking about um, Kernel matrices that you know, these feature,",
    "start": "245280",
    "end": "252755"
  },
  {
    "text": "the corresponding features of the feature map that is implicit to the kernel,",
    "start": "252755",
    "end": "259445"
  },
  {
    "text": "are like the eigenbasis of that- of that PSD matrix.",
    "start": "259445",
    "end": "264815"
  },
  {
    "text": "Right? Now today we are gonna ,um, and- and the other thing we covered, is, er,",
    "start": "264815",
    "end": "271350"
  },
  {
    "text": "in the last class was SVMs, Is a kind of- is a kind of classification algorithm,",
    "start": "271350",
    "end": "277290"
  },
  {
    "text": "which tends to work well with kernel based methods, right? The reason I say it tends to work well with kernel based methods is because,",
    "start": "277290",
    "end": "285500"
  },
  {
    "text": "we saw that kernel- kernel based methods can be applied to linear regression, it can be applied to logistic regression.",
    "start": "285500",
    "end": "290750"
  },
  {
    "text": "Where the number of parameters, that- that we save with the kernel method,",
    "start": "290750",
    "end": "296985"
  },
  {
    "text": "is generally equal to, the number of examples that we have. Whereas, with a more classical,",
    "start": "296985",
    "end": "303259"
  },
  {
    "text": "linear regression or logistic regression, the number of parameters we save is equal to the number of features, right?",
    "start": "303260",
    "end": "308720"
  },
  {
    "text": "So we move from saving parameters, which used to be an order of the number of features that",
    "start": "308720",
    "end": "315590"
  },
  {
    "text": "we have with classical linear or logistic regression, to kernel methods where we save coefficients one for example.",
    "start": "315590",
    "end": "322430"
  },
  {
    "text": "Which means, in general, you would expect, kernel methods to not work very well, when your data set grows a lot.",
    "start": "322430",
    "end": "330840"
  },
  {
    "text": "SVM was a special case, where the coefficients of the- the coefficients of the examples,",
    "start": "330980",
    "end": "338769"
  },
  {
    "text": "tend to be sparse, meaning most of the, coefficients will be 0. And that's me- that makes SVM very attractive,",
    "start": "338769",
    "end": "346479"
  },
  {
    "text": "as a kernel method because not only do you get scalability to infinite number of features,",
    "start": "346480",
    "end": "351639"
  },
  {
    "text": "using the feature maps, but you also get scalability with number of examples because your coefficients will be sparse.",
    "start": "351640",
    "end": "359240"
  },
  {
    "text": "Okay. Any questions about that before we move on to today's topics? Okay, cool.",
    "start": "359240",
    "end": "367085"
  },
  {
    "text": "So let's jump in, um, Bayesian methods. So, so far, the methods that we've seen,",
    "start": "367085",
    "end": "377230"
  },
  {
    "text": "can be loosely be called Frequentist methods,",
    "start": "383330",
    "end": "388900"
  },
  {
    "text": "and that's because we assumed that an unknown para- unknown parameter theta,",
    "start": "395660",
    "end": "405920"
  },
  {
    "text": "which we assume to be some unknown constant, all right, we assume that theta is some unknown constant.",
    "start": "406020",
    "end": "413965"
  },
  {
    "text": "Vector-valued but unknown constant. Right. And the way we, uh go about estimating theta,",
    "start": "413965",
    "end": "421355"
  },
  {
    "text": "was to, define a likelihood on- on- on theta.",
    "start": "421355",
    "end": "426570"
  },
  {
    "text": "So we will define a likelihood, L of theta to be Log P of say,",
    "start": "426570",
    "end": "433770"
  },
  {
    "text": "theta, parameterized by theta. And, we would consider the negative of L of theta to be our last function.",
    "start": "433770",
    "end": "446300"
  },
  {
    "text": "And then we would, estimate theta equals arg min, oh not that, l",
    "start": "448820",
    "end": "463050"
  },
  {
    "text": "of Theta, and this is the same as arg max Theta L of Theta, right?",
    "start": "463050",
    "end": "470460"
  },
  {
    "text": "So this is the maximum likelihood estimate of, um, our- our, uh, parameters.",
    "start": "470460",
    "end": "476880"
  },
  {
    "text": "And in order to do this, we- we implicitly assume that Theta is some unknown constant.",
    "start": "476880",
    "end": "484905"
  },
  {
    "text": "Okay? Now, today we are gonna look at a- a- a different approach to attack this problem,",
    "start": "484905",
    "end": "495120"
  },
  {
    "text": "[NOISE] where Theta is a random variable, we think of it as a random variable [NOISE] that is unobserved.",
    "start": "495120",
    "end": "507030"
  },
  {
    "text": "[NOISE] Okay. So this- this, uh,",
    "start": "507030",
    "end": "513810"
  },
  {
    "text": "this small little change which may seem innocent, is basically the fundamental difference between Bayesian methods and frequentist methods.",
    "start": "513810",
    "end": "521280"
  },
  {
    "text": "In frequentist methods, there is some unknown constant, Theta, whereas in Bayesian methods there is a random variable Theta, which is unobserved.",
    "start": "521280",
    "end": "530775"
  },
  {
    "text": "Now, the moment we think of Theta as a random variable,",
    "start": "530775",
    "end": "536249"
  },
  {
    "text": "there needs to be some kind of a probability distribution associated with them, right? Just calling something random variable is",
    "start": "536249",
    "end": "544389"
  },
  {
    "text": "necessarily incomplete because we wanna say what is the associated distribution with it. Okay? And when- the first thing we do in, uh,",
    "start": "544389",
    "end": "553654"
  },
  {
    "text": "Bayesian methods is to assign some prior probability distribution on Theta.",
    "start": "553655",
    "end": "561180"
  },
  {
    "text": "So we are- we- we believe that Theta, uh, comes from some prior distribution, right?",
    "start": "561180",
    "end": "571890"
  },
  {
    "text": "And then we observe our x data.",
    "start": "571890",
    "end": "578760"
  },
  {
    "text": "We- we believe that, uh, the data comes from the distribution from p of x given Theta.",
    "start": "578760",
    "end": "588270"
  },
  {
    "text": "This is- generally has the same mathematical form as the likelihood function,",
    "start": "588270",
    "end": "594780"
  },
  {
    "text": "[NOISE] but it is different philosophically in the sense that we- we",
    "start": "594780",
    "end": "599820"
  },
  {
    "text": "are implicitly believing Theta as having some kind of a prior distribution.",
    "start": "599820",
    "end": "604995"
  },
  {
    "text": "But the functional form of p of x given Theta and the functional form of p of x,",
    "start": "604995",
    "end": "611295"
  },
  {
    "text": "you know, semicolon Theta parameterized by Theta are exactly the same. Okay. But the- the interpretation is- is, uh, different.",
    "start": "611295",
    "end": "617280"
  },
  {
    "text": "[NOISE] And now, the, uh, the main difference between the two is,",
    "start": "617280",
    "end": "623565"
  },
  {
    "text": "with the classical methods, we perform maximum likelihood on Theta,",
    "start": "623565",
    "end": "629085"
  },
  {
    "text": "on- on the- on the log-likelihood to get an estimate of Theta.",
    "start": "629085",
    "end": "634455"
  },
  {
    "text": "Okay? But in the Bayesian method, what we do is, we apply the Bayes rule.",
    "start": "634455",
    "end": "639660"
  },
  {
    "text": "So in Bayesian methods, we come up with p of x given Theta. And this is p of x- p of Theta given x is p of x given",
    "start": "639660",
    "end": "650820"
  },
  {
    "text": "Theta times p of Theta over p of x.",
    "start": "650820",
    "end": "658380"
  },
  {
    "text": "Okay. And p of x, we don't know how- what p of x is,",
    "start": "658380",
    "end": "663404"
  },
  {
    "text": "but p of x is,",
    "start": "663405",
    "end": "666310"
  },
  {
    "text": "integral p of x given Theta times p of Theta d Theta.",
    "start": "672050",
    "end": "681480"
  },
  {
    "text": "So basically it's the numerator. Take the numerator and integrate out Theta and that gives you p of x.",
    "start": "681480",
    "end": "688260"
  },
  {
    "text": "Is this obvious? Why this equals this? Right. If- if it's not obvious,",
    "start": "688260",
    "end": "694125"
  },
  {
    "text": "uh, you know, just the numerator, uh, is p of x,",
    "start": "694125",
    "end": "701790"
  },
  {
    "text": "Theta d Theta, right? P of x, Theta is p of x given Theta times p Theta,",
    "start": "701790",
    "end": "707654"
  },
  {
    "text": "that's p of x, Theta. And then you marginalize out, uh, marginalize out Theta from that, right?",
    "start": "707655",
    "end": "714404"
  },
  {
    "text": "So this is Bayesian method- this is the Bayesian approach, right? So in the Bayesian approach, there is no loss function, right?",
    "start": "714405",
    "end": "721815"
  },
  {
    "text": "We don't take maximum likelihood estimates, we don't calculate gradients, we don't do gradient ascent or descent.",
    "start": "721815",
    "end": "727440"
  },
  {
    "text": "There are no, you know, closed form MLE estimates. All we do is apply Bayes rule and get- update our beliefs about Theta given x,",
    "start": "727440",
    "end": "738570"
  },
  {
    "text": "given the data, right? All what- all that we do in Bayesian methods is apply Bayes' rule,",
    "start": "738570",
    "end": "745230"
  },
  {
    "text": "which is why it's called the Bayesian method. And with this, uh,",
    "start": "745230",
    "end": "750404"
  },
  {
    "text": "this is- is, uh, essentially, the counterpart of- of,",
    "start": "750405",
    "end": "756509"
  },
  {
    "text": "uh, performing maximum likelihood estimate. Here we calculate the posterior distribution. So this is called [NOISE] the posterior distribution.",
    "start": "756510",
    "end": "763259"
  },
  {
    "text": "[NOISE] Right.",
    "start": "763260",
    "end": "770180"
  },
  {
    "text": "And in case of machine learning, the way we, uh, go about, uh, using Bayesian methods, uh,",
    "start": "770180",
    "end": "777385"
  },
  {
    "text": "is- is- is as follows. So in machine learning, especially in- in supervised learning,",
    "start": "777385",
    "end": "782490"
  },
  {
    "text": "[NOISE] we are- we are trying to learn a mapping between x and y's, right?",
    "start": "782490",
    "end": "789705"
  },
  {
    "text": "So the data, uh, in- in a machine learning setting will also have y's over here. So x and y is our- our, uh, is our data.",
    "start": "789705",
    "end": "798555"
  },
  {
    "text": "And the- the, uh, likelihood- so maybe let me look me, uh,",
    "start": "798555",
    "end": "805440"
  },
  {
    "text": "[NOISE] so in supervised machine learning,",
    "start": "805440",
    "end": "814410"
  },
  {
    "text": "[NOISE] you start with",
    "start": "814410",
    "end": "821024"
  },
  {
    "text": "Theta having some prior distribution, right?",
    "start": "821025",
    "end": "826395"
  },
  {
    "text": "And then we observe, [NOISE] x and y. And over here, [NOISE] y comes from [NOISE] p [NOISE] of y given x,",
    "start": "826395",
    "end": "840824"
  },
  {
    "text": "common our Theta, right? Here, we make no assumptions about the distribution",
    "start": "840825",
    "end": "846899"
  },
  {
    "text": "of x. X's are just given to us somehow. We only assume y given x follows Theta,",
    "start": "846900",
    "end": "853695"
  },
  {
    "text": "whose prior is- is, you know, you make some assumption. So x is always given, right? We- we don't say anything about, uh, x's, right?",
    "start": "853695",
    "end": "862365"
  },
  {
    "text": "And from this we calculate the posterior distribution p of Theta given x, y.",
    "start": "862365",
    "end": "871110"
  },
  {
    "text": "And this will be [NOISE] p of y given [NOISE]",
    "start": "871110",
    "end": "876329"
  },
  {
    "text": "x, Theta [NOISE] times p of Theta divided by p of [NOISE]",
    "start": "876330",
    "end": "888580"
  },
  {
    "text": "x, Theta given x. And that is, uh,",
    "start": "888580",
    "end": "894870"
  },
  {
    "text": "we also make the assumption that Theta is independent of x. And this becomes just p of- oh, I'm sorry,",
    "start": "894870",
    "end": "902940"
  },
  {
    "text": "[NOISE] p of y given x. Um,",
    "start": "902940",
    "end": "911940"
  },
  {
    "text": "[NOISE] there is something p of- [NOISE] so p of Theta given x,",
    "start": "911940",
    "end": "919260"
  },
  {
    "text": "y is p of y given, uh, x times p of x or, [NOISE] they should, yeah, this- this correct.",
    "start": "919260",
    "end": "926160"
  },
  {
    "text": "So, uh, yeah, this right, um, p of, uh, y given x. Now, [NOISE] um- so we- we calculate, uh, the posterior.",
    "start": "926160",
    "end": "937950"
  },
  {
    "text": "[NOISE] The posterior is on Theta. And from the posterior distribution,",
    "start": "937950",
    "end": "945025"
  },
  {
    "text": "we construct what is called the posterior predictive distribution. [NOISE]",
    "start": "945025",
    "end": "957175"
  },
  {
    "text": "And the posterior predictive distribution is basically p of y star,",
    "start": "957175",
    "end": "963730"
  },
  {
    "text": "where x, y, x star.",
    "start": "963730",
    "end": "971019"
  },
  {
    "text": "So x star is a test example for which we only observe the x, the input. And x and y are part of your training set",
    "start": "971020",
    "end": "978280"
  },
  {
    "text": "for which we have observed both the inputs and the outputs. Right? And this is called the posterior predictive distribution.",
    "start": "978280",
    "end": "985165"
  },
  {
    "text": "And this is basically integral of p of y given x,",
    "start": "985165",
    "end": "993264"
  },
  {
    "text": "theta times p of theta",
    "start": "993265",
    "end": "998485"
  },
  {
    "text": "given x, y d theta.",
    "start": "998485",
    "end": "1004660"
  },
  {
    "text": "Right? So this is the general recipe of using Bayesian methods in machine learning.",
    "start": "1006620",
    "end": "1013230"
  },
  {
    "text": "Okay? So this is the Bayesian methods if you- if you- if you, um, if you are only interested in theta and you're not interested in making predictions,",
    "start": "1013230",
    "end": "1023175"
  },
  {
    "text": "this is more classical Bayesian statistics, where you come up with a posterior distribution on your theta and you're all done, right?",
    "start": "1023175",
    "end": "1031079"
  },
  {
    "text": "But as in machine learning, we want to use it to make predictions on unseen examples. We are given x and y.",
    "start": "1031080",
    "end": "1036765"
  },
  {
    "text": "And we want to make prediction on x star of what y star is. Yes question?",
    "start": "1036765",
    "end": "1044260"
  },
  {
    "text": "For the posterior, [NOISE] should it be [inaudible]. Yeah. So um, this- the -the question is,",
    "start": "1046070",
    "end": "1053445"
  },
  {
    "text": "should this be p- p of theta given x? Yes, uh, p of theta given- it should be ah p of theta given x.",
    "start": "1053445",
    "end": "1059639"
  },
  {
    "text": "But because we make theta and x, as being independent, this will just be that- just p of theta. Yeah, good question.",
    "start": "1059640",
    "end": "1065490"
  },
  {
    "text": "You make the assumption that x's, you know, on which we are conditioning to observe y's,",
    "start": "1065490",
    "end": "1071250"
  },
  {
    "text": "those x's are independent of theta. It's a pretty common assumption because, you know, yeah.",
    "start": "1071250",
    "end": "1077160"
  },
  {
    "text": "A good question, right.",
    "start": "1077160",
    "end": "1082800"
  },
  {
    "text": "And the way you think about, uh, you think about- about um,",
    "start": "1082800",
    "end": "1089115"
  },
  {
    "text": "this- this equation which- which- which looks pretty complex, it's an integral over the product of- of two,",
    "start": "1089115",
    "end": "1097625"
  },
  {
    "text": "um, of- of- of two probabilities and then, you're integrating out theta.",
    "start": "1097625",
    "end": "1103205"
  },
  {
    "text": "But um, this actually has a very- very, um, [NOISE] this is y star and x star and this is x and y, the training.",
    "start": "1103205",
    "end": "1113690"
  },
  {
    "text": "And so these are the test and these- these are the- the training. But this has a very nice interpretation. So, uh the interpretation is- is uh something like this.",
    "start": "1113690",
    "end": "1123510"
  },
  {
    "text": "With the posterior distribution, we are coming up with a probability estimate",
    "start": "1123890",
    "end": "1131865"
  },
  {
    "text": "of how likely a given theta is the correct theta, right?",
    "start": "1131865",
    "end": "1137595"
  },
  {
    "text": "So p of x theta is a probability distribution over theta. Right and theta is essentially our model.",
    "start": "1137595",
    "end": "1144240"
  },
  {
    "text": "Right? Now, what's happening here?",
    "start": "1144240",
    "end": "1149355"
  },
  {
    "text": "If you just look at this expression, think of theta to be some fixed, you know, um, some- some fixed vector, right?",
    "start": "1149355",
    "end": "1156930"
  },
  {
    "text": "P of y given x, um, theta is like making a prediction for y for some given value of theta, right?",
    "start": "1156930",
    "end": "1168150"
  },
  {
    "text": "And what we are essentially doing is [NOISE] considering the set of all models.",
    "start": "1168150",
    "end": "1173895"
  },
  {
    "text": "So let say, this is model 1, model 2, model 3.",
    "start": "1173895",
    "end": "1182294"
  },
  {
    "text": "And model 1 corresponds to some theta- theta 1, theta 2, theta 3.",
    "start": "1182295",
    "end": "1188655"
  },
  {
    "text": "But theta 1, theta 2 theta 3 is- is- is a full vector. So if um, so maybe,",
    "start": "1188655",
    "end": "1195210"
  },
  {
    "text": "let's call it theta 1, superscript 1 to indicate that's a full vector, theta 2, theta 3.",
    "start": "1195210",
    "end": "1203460"
  },
  {
    "text": "Right? So what we're essentially doing is we are taking the prediction of y given x for every possible value of theta.",
    "start": "1203460",
    "end": "1212010"
  },
  {
    "text": "So each of these will make a prediction of y. Right? And then, we are calculating a weighted average of",
    "start": "1212010",
    "end": "1219500"
  },
  {
    "text": "all these predictions where the weights are decided by the posterior distribution, right? So the posterior distribution is giving you some kind of a distribution or your model's.",
    "start": "1219500",
    "end": "1229360"
  },
  {
    "text": "Right? Theta is the model. Right? Once you know theta,",
    "start": "1229360",
    "end": "1234554"
  },
  {
    "text": "you're- you- you can make a pre- you can make a prediction, for example, in case of logistic reg- uh, logistic regression.",
    "start": "1234554",
    "end": "1241034"
  },
  {
    "text": "If you know theta, we do x transpose theta, get a scalar, run it through the sigmoid function and that's our, you know, uh, uh,",
    "start": "1241035",
    "end": "1248565"
  },
  {
    "text": "that's our p of y, that gives you the probability of whether y is zero or one. What's happening here, is we can- we are- the- the- the interpretation here,",
    "start": "1248565",
    "end": "1258470"
  },
  {
    "text": "is we are considering every possible value of theta, right? There are infinitely many, uh, possible values your theta vector can take, right?",
    "start": "1258470",
    "end": "1266640"
  },
  {
    "text": "And your posterior distribution is giving you a probability distribution over all these values of thetas.",
    "start": "1266640",
    "end": "1273645"
  },
  {
    "text": "What's the probability that each- that a given value of theta is- is the right value, right? That's- that's what the posterior distribution is striking.",
    "start": "1273645",
    "end": "1281440"
  },
  {
    "text": "And for prediction, we are- we are",
    "start": "1281450",
    "end": "1286740"
  },
  {
    "text": "essentially taking the prediction from every possible infinitely many possible values of theta and aggregating them by a weighted average where the weights are",
    "start": "1286740",
    "end": "1295650"
  },
  {
    "text": "decided by the posterior distribution. Yes question?",
    "start": "1295650",
    "end": "1301035"
  },
  {
    "text": "There is a true underlying distribution that this for this theta. And that is one of this three models is",
    "start": "1301035",
    "end": "1307559"
  },
  {
    "text": "the closest to- to that- that actual distribution? One of these models? Yeah. So why are we going to the contribution from",
    "start": "1307560",
    "end": "1313695"
  },
  {
    "text": "other model when one of them is actually going to represent the other one most closely? So the question is, if one of them is closest to the true, uh, um,",
    "start": "1313695",
    "end": "1323325"
  },
  {
    "text": "true model then, why are we taking, uh, like, you know, the- the prediction from all the models?",
    "start": "1323325",
    "end": "1328380"
  },
  {
    "text": "And the answer there is that we are not sure, right? The- the posterior distribution tells us that it is likely coming from this model.",
    "start": "1328380",
    "end": "1337559"
  },
  {
    "text": "It may also be coming from this model but you- but, you know, my belief that it's coming from this model is,",
    "start": "1337560",
    "end": "1343215"
  },
  {
    "text": "you know, 21%. When my belief that comes from this model is 20- 20.1%, you know.",
    "start": "1343215",
    "end": "1348600"
  },
  {
    "text": "It- it's giving you a posterior distribution of- of- of, uh, over the parameters, which basically is like the- our belief of what the true value of theta is.",
    "start": "1348600",
    "end": "1358110"
  },
  {
    "text": "So then it's highly susceptible to our our choice of prior when you start- Exactly. So the- so the choice of our prior is going",
    "start": "1358110",
    "end": "1366990"
  },
  {
    "text": "to determine what our posterior distribution is going to be, right? And in a way, Bayesian methods are subjective because, you know,",
    "start": "1366990",
    "end": "1374205"
  },
  {
    "text": "it- it- it takes this subjective element of the person who's modeling, right? If- if the- if the person who's modeling believes that",
    "start": "1374205",
    "end": "1382544"
  },
  {
    "text": "theta comes from a prior distribution where it has Gaussian then, you get some posterior distribution and therefore you make some- some,",
    "start": "1382545",
    "end": "1389760"
  },
  {
    "text": "uh, ah, kind of prediction. Whereas, you know, if all else being the same, you just believe that your theta comes from,",
    "start": "1389760",
    "end": "1395550"
  },
  {
    "text": "say, a Laplace distribution then, your prediction would be something different, right? And this is probably the biggest criticism against Bayesian methods that,",
    "start": "1395550",
    "end": "1404174"
  },
  {
    "text": "you know, there's a lot of subjectivism, uh, in- in play here, that the- and the subjectivism",
    "start": "1404175",
    "end": "1410325"
  },
  {
    "text": "comes into picture based on what prior you're choosing for theta. In the prior you cannot choose any value of theta to be 0, right?",
    "start": "1410325",
    "end": "1417420"
  },
  {
    "text": "Any probability cannot- can be? Yes. So the question is in your- can your prior assign 0 probability to any models?",
    "start": "1417420",
    "end": "1423120"
  },
  {
    "text": "So if your prior distribution over theta assigns 0 probability to- to some value of theta then,",
    "start": "1423120",
    "end": "1429615"
  },
  {
    "text": "necessarily in the posterior also it will be 0. So, um, in general, it's a bad idea to choose a prior which will assign 0 probability to some, right?",
    "start": "1429615",
    "end": "1441450"
  },
  {
    "text": "[NOISE] So, you- you want to think of this as- so this- the- the, um, the predictive distribution,",
    "start": "1441450",
    "end": "1448049"
  },
  {
    "text": "as the expected value of p of y star given x- x star,",
    "start": "1448050",
    "end": "1458654"
  },
  {
    "text": "theta where theta is sampled from p of theta given x, y.",
    "start": "1458655",
    "end": "1468240"
  },
  {
    "text": "So this is the training set. From the training set, you construct a posterior distribution, right?",
    "start": "1468240",
    "end": "1474090"
  },
  {
    "text": "And then, you take the- the uh, prediction of p of y given x, where theta here, is",
    "start": "1474090",
    "end": "1479490"
  },
  {
    "text": "a random variable that is distributed according to the posterior distribution. And- and this expectation is equal to this. Yes question.",
    "start": "1479490",
    "end": "1488940"
  },
  {
    "text": "Uh, why is this one any more subjective than let's say something like generated model [inaudible]?",
    "start": "1488940",
    "end": "1499650"
  },
  {
    "text": "Yeah, so in a- in a- I guess the question is, how is this different from generative models that we, uh, saw before.",
    "start": "1499650",
    "end": "1505545"
  },
  {
    "text": "But why is this any more subjective [inaudible]? Yeah, the, the- it is, uh, the question is why is this more subjective?",
    "start": "1505545",
    "end": "1511799"
  },
  {
    "text": "It is more subjective because we're making yet another assumption of the way Theta is, is, um, distributed.",
    "start": "1511800",
    "end": "1517680"
  },
  {
    "text": "For example, in, in GDA, when we- when we studied GDA, the mu and sigma, we did not have a prior on them, right?",
    "start": "1517680",
    "end": "1523860"
  },
  {
    "text": "There were just some unknown constant that we estimated using maximum likelihood. Whereas if you took a Bayesian approach there,",
    "start": "1523860",
    "end": "1529125"
  },
  {
    "text": "the Mus and Sigmas of your GDA would also have prior distributions on",
    "start": "1529125",
    "end": "1534510"
  },
  {
    "text": "them and you would calculate a posterior without doing maximum likelihood, right?",
    "start": "1534510",
    "end": "1540165"
  },
  {
    "text": "Yeah, so, um, yeah, so this is- this is one way to think of the, uh,",
    "start": "1540165",
    "end": "1545399"
  },
  {
    "text": "posterior predictive distribution that you're essentially taking the prediction from every possible model configuration that you can come up with,",
    "start": "1545400",
    "end": "1552855"
  },
  {
    "text": "and then taking an average of those- of those predictions as a weighted average",
    "start": "1552855",
    "end": "1558524"
  },
  {
    "text": "where the weights come from the posterior distribution.",
    "start": "1558525",
    "end": "1561550"
  },
  {
    "text": "And this approach is essentially what one might call parametric.",
    "start": "1564290",
    "end": "1571750"
  },
  {
    "text": "We call it parametric because p of y given x, Theta is- you know,",
    "start": "1574330",
    "end": "1581674"
  },
  {
    "text": "has- has some functional form for which Theta is a parameter, right? So y given x Theta is generally, you know, for example,",
    "start": "1581675",
    "end": "1591975"
  },
  {
    "text": "it could be logistic 1011 plus e _minus theta transpose x,",
    "start": "1591975",
    "end": "1596985"
  },
  {
    "text": "and because of this functional form, the degree of freedom that we have is to just change theta, right?",
    "start": "1596985",
    "end": "1605070"
  },
  {
    "text": "And that in a way, limits the set of all possible models that we can consider.",
    "start": "1605070",
    "end": "1612149"
  },
  {
    "text": "The set of all possible predictive functions that we have is limited by this functional form, right?",
    "start": "1612150",
    "end": "1620415"
  },
  {
    "text": "And different values of Theta gives us different specific predictive functions,",
    "start": "1620415",
    "end": "1625485"
  },
  {
    "text": "but the set of all these predictor functions is still somewhat limited. And one way to kind of, uh,",
    "start": "1625485",
    "end": "1632535"
  },
  {
    "text": "see that limitation is supposing we have, um- let's say we",
    "start": "1632535",
    "end": "1641730"
  },
  {
    "text": "have a regression problem, right?",
    "start": "1641730",
    "end": "1649304"
  },
  {
    "text": "Now, what's the correct hypothesis form for this?",
    "start": "1649305",
    "end": "1654465"
  },
  {
    "text": "Maybe it's linear, or maybe it is quadratic,",
    "start": "1654465",
    "end": "1659739"
  },
  {
    "text": "or maybe it is cubic. Or maybe as you observe more data- if you observe more data,",
    "start": "1660980",
    "end": "1675100"
  },
  {
    "text": "maybe it is a much higher order polynomial because there probably is some,",
    "start": "1675620",
    "end": "1682290"
  },
  {
    "text": "you know, well predictable, varying- varying- variants in it, right?",
    "start": "1682290",
    "end": "1687659"
  },
  {
    "text": "And by- by limiting ourselves to certain kinds of certain family of parametric functions,",
    "start": "1687660",
    "end": "1696360"
  },
  {
    "text": "we are essentially saying, no matter how much data is given to us, we just cannot be flexible to fit all this variance,",
    "start": "1696360",
    "end": "1704205"
  },
  {
    "text": "even though that probably is, uh, not noise and is true Sigma, right?",
    "start": "1704205",
    "end": "1709305"
  },
  {
    "text": "The- that's the fundamental limitation of parametric models is that the functional form of",
    "start": "1709305",
    "end": "1715785"
  },
  {
    "text": "the hypothesis function limits us from being flexible even in the presence of a lot of data,",
    "start": "1715785",
    "end": "1723390"
  },
  {
    "text": "which suggests that there are some other pattern. Which is why there is interest in non-parametric methods, right?",
    "start": "1723390",
    "end": "1732390"
  },
  {
    "text": "Non-parametric methods on the other hand are- the way to think of",
    "start": "1732390",
    "end": "1741540"
  },
  {
    "text": "non-parametric methods is that we consider the set of all possible functions, right?",
    "start": "1741540",
    "end": "1748485"
  },
  {
    "text": "And that seems pretty expensive right? We're considering the set of all possible functions, and it's,",
    "start": "1748485",
    "end": "1755355"
  },
  {
    "text": "it's hard to even imagine and write it down on on a piece of paper what that class even looks like, right?",
    "start": "1755355",
    "end": "1761805"
  },
  {
    "text": "The set of all possible functions. And, and then we let data kind of,",
    "start": "1761805",
    "end": "1769995"
  },
  {
    "text": "decide which function from that set we actually want, right?",
    "start": "1769995",
    "end": "1775500"
  },
  {
    "text": "So in non-parametric methods, we generally, you know, think of some function f and assume no functional form whatsoever, right?",
    "start": "1775500",
    "end": "1785385"
  },
  {
    "text": "We just work with a function f, which could be any kind of function.",
    "start": "1785385",
    "end": "1790660"
  },
  {
    "text": "So, uh, before we get into the details,",
    "start": "1790700",
    "end": "1797865"
  },
  {
    "text": "let's, let's have a look at, uh, Bayesian linear regression to get a better feel for how the Bayesian, uh,",
    "start": "1797865",
    "end": "1804825"
  },
  {
    "text": "the Bayesian method works in case of regression, and then we'll move on to non-parametric methods, that is, Gaussian processes.",
    "start": "1804825",
    "end": "1811929"
  },
  {
    "text": "So Bayesian linear regression.wwwwwwwwwwwwwwwwwwwww",
    "start": "1812390",
    "end": "1815770"
  },
  {
    "text": "Bayesian linear regression. So again,",
    "start": "1849350",
    "end": "1859755"
  },
  {
    "text": "in Bayesian linear regression, we are- we start with a training set, x_i, y_i, i equals 1 to n.",
    "start": "1859755",
    "end": "1868485"
  },
  {
    "text": "This is the full training set, right? It comes from some unknown distribution, and we make the, uh,",
    "start": "1868485",
    "end": "1874289"
  },
  {
    "text": "assumption that y_i equals Theta transpose x_i plus some unknown Epsilon,",
    "start": "1874290",
    "end": "1886005"
  },
  {
    "text": "where Epsilon i comes",
    "start": "1886005",
    "end": "1892320"
  },
  {
    "text": "from some normal distribution, Sigma square, and Theta comes from",
    "start": "1892320",
    "end": "1906990"
  },
  {
    "text": "some normal distribution,wwwww Tau squared, right?",
    "start": "1906990",
    "end": "1917655"
  },
  {
    "text": "So this looks very similar to the standard linear regression setting that we had, except we also apply a prior on Theta, right?",
    "start": "1917655",
    "end": "1926700"
  },
  {
    "text": "This is the only extra line from standard linear regression. And now [NOISE] Good question.",
    "start": "1926700",
    "end": "1939840"
  },
  {
    "text": "So what is, um, um, tau square i? So in this case, each epsilon i was a scalar,",
    "start": "1939840",
    "end": "1946350"
  },
  {
    "text": "so sigma-squared was a scalar. Over here, Theta is in R_d and x is also in R_d.",
    "start": "1946350",
    "end": "1956890"
  },
  {
    "text": "So theta- this, this was a standard, you know, this is just a normal distribution,",
    "start": "1958070",
    "end": "1963255"
  },
  {
    "text": "but this is the multivariate Gaussian distribution where 0 over here is a zero vector and Tau square i is a covariance matrix,",
    "start": "1963255",
    "end": "1972285"
  },
  {
    "text": "which is in this case a diagonal covariance matrix where the diagonal values are Tau square, right?",
    "start": "1972285",
    "end": "1977985"
  },
  {
    "text": "Thanks for asking that. So we're given this,",
    "start": "1977985",
    "end": "1983460"
  },
  {
    "text": "and in case of the standard or the,",
    "start": "1983460",
    "end": "1988740"
  },
  {
    "text": "the frequentist approach to linear regression, we constructed a likelihood function for- so in the frequentist approach,",
    "start": "1988740",
    "end": "2003480"
  },
  {
    "text": "l Theta equals, uh, log of p of y given x Theta, right?",
    "start": "2003790",
    "end": "2012155"
  },
  {
    "text": "And, um, and this turned out to be, um, if you remember,",
    "start": "2012155",
    "end": "2018125"
  },
  {
    "text": "log 1 over square root 2 by Sigma square exponent",
    "start": "2018125",
    "end": "2025190"
  },
  {
    "text": "of y minus Theta transpose x square minus half over Sigma squared, right?",
    "start": "2025190",
    "end": "2034970"
  },
  {
    "text": "And, and, uh, essentially in this, uh, it, it boils down to just the square term,",
    "start": "2034970",
    "end": "2043700"
  },
  {
    "text": "everything else just, just, um, cancels out or becomes a constant factor and we end up getting the,",
    "start": "2043700",
    "end": "2049504"
  },
  {
    "text": "the square loss function. And you perform- so performing maximum likelihood was like minimizing least squares.",
    "start": "2049505",
    "end": "2055115"
  },
  {
    "text": "So that was the frequent setting. But what about the Bayesian setting? In the Bayesian setting,",
    "start": "2055115",
    "end": "2060810"
  },
  {
    "text": "we get P of- we don't do maximum likelihood, so, you know,",
    "start": "2061030",
    "end": "2067350"
  },
  {
    "text": "in Bayesian linear regression, we just construct P of Theta given S. So let's call this S. S is the full training set.",
    "start": "2067750",
    "end": "2082409"
  },
  {
    "text": "We construct p of Theta given S using, using Bayes rule.",
    "start": "2083530",
    "end": "2089794"
  },
  {
    "text": "And in case of linear regression, you can work it out. This will be Theta given S will be, um,",
    "start": "2089795",
    "end": "2099360"
  },
  {
    "text": "distributed according to a normal distribution with mean 1",
    "start": "2099790",
    "end": "2105455"
  },
  {
    "text": "over Sigma square, A inverse,",
    "start": "2105455",
    "end": "2111490"
  },
  {
    "text": "x transpose y Sigma",
    "start": "2111490",
    "end": "2117050"
  },
  {
    "text": "A inverse where A is given by 1 over Sigma square,",
    "start": "2119880",
    "end": "2131480"
  },
  {
    "text": "x transpose, x plus 1 over",
    "start": "2131480",
    "end": "2136744"
  },
  {
    "text": "Tau square i, right?",
    "start": "2136745",
    "end": "2142805"
  },
  {
    "text": "So you can work it out. The, the, uh, Theta given x is distributed according to-",
    "start": "2142805",
    "end": "2149224"
  },
  {
    "text": "the correct way to write this would be Theta given S is distributed according to a normal distribution,",
    "start": "2149225",
    "end": "2157585"
  },
  {
    "text": "a multivariate Gaussian, with this being a mean vector and this being the covariance vector where A,",
    "start": "2157585",
    "end": "2166295"
  },
  {
    "text": "A has this 1. Yes question. [inaudible].",
    "start": "2166295",
    "end": "2179750"
  },
  {
    "text": "Yeah. Even in the posterior, uh, [inaudible]",
    "start": "2179750",
    "end": "2190390"
  },
  {
    "text": "So is the, is the question, uh, is p of theta given x comma y equal to p of t, w and y?",
    "start": "2190390",
    "end": "2202945"
  },
  {
    "text": "Uh, In, in general, no, uh, because independence may or may not change once,",
    "start": "2202945",
    "end": "2209950"
  },
  {
    "text": "once you condition on extra variables, right? Yeah, in general, this is not true,",
    "start": "2209950",
    "end": "2215365"
  },
  {
    "text": "yeah. Was there a question? Yes.",
    "start": "2215365",
    "end": "2222670"
  },
  {
    "text": "[inaudible]. So this is an arrow over y which says it's, it's a vector.",
    "start": "2222670",
    "end": "2230530"
  },
  {
    "text": "[inaudible]. Yes, this is from the, from the training set.",
    "start": "2230530",
    "end": "2236035"
  },
  {
    "text": "This is this y, and this looks a little bit cryptic. Maybe we can simplify this a little farther.",
    "start": "2236035",
    "end": "2243474"
  },
  {
    "text": "So, right? If I bring in- bring in A over here.",
    "start": "2243475",
    "end": "2250705"
  },
  {
    "text": "So this is going to be x transpose x plus,",
    "start": "2250705",
    "end": "2258445"
  },
  {
    "text": "because it has, um, an inverse x transpose x plus",
    "start": "2258445",
    "end": "2267590"
  },
  {
    "text": "a is 1 over sigma square, square over",
    "start": "2272400",
    "end": "2281470"
  },
  {
    "text": "[inaudible].",
    "start": "2281470",
    "end": "2308590"
  },
  {
    "text": "What do we have here? So here we have a normal distribution where the mean over here has this form and some variance,",
    "start": "2308590",
    "end": "2318325"
  },
  {
    "text": "and you might recognize that this looks, looks, you know, um, very similar to the normal equations.",
    "start": "2318325",
    "end": "2327744"
  },
  {
    "text": "In the normal equations we had x transpose x inverse x transpose y.",
    "start": "2327745",
    "end": "2335240"
  },
  {
    "text": "Right this looks very similar to the normal equation, except there is a small, an extra,",
    "start": "2335970",
    "end": "2341650"
  },
  {
    "text": "um, diagonal matrix that is being added to x transpose x, right?",
    "start": "2341650",
    "end": "2347815"
  },
  {
    "text": "And if you- I think somebody asked the question when we were deriving the normal equation,",
    "start": "2347815",
    "end": "2353425"
  },
  {
    "text": "how did we invert x transpose x? What if it's not invertible? Right? And one answer is over here.",
    "start": "2353425",
    "end": "2360400"
  },
  {
    "text": "In Bayesian methods, assuming, uh, prior on our data,",
    "start": "2360400",
    "end": "2367165"
  },
  {
    "text": "to have, um, some tau square and some noise epsilon squared.",
    "start": "2367165",
    "end": "2374125"
  },
  {
    "text": "We, we get this extra diagonal matrix that is added to x transpose x.",
    "start": "2374125",
    "end": "2381595"
  },
  {
    "text": "And you might remember, if you take a positive definite matrix or any matrix in general,",
    "start": "2381595",
    "end": "2386830"
  },
  {
    "text": "and if you add a diagonal matrix, then you're essentially increasing",
    "start": "2386830",
    "end": "2391944"
  },
  {
    "text": "the eigenvalues of that matrix by the corresponding diagonal terms. Right, which means now even if there were some eigenvalues, there was 0,",
    "start": "2391945",
    "end": "2401680"
  },
  {
    "text": "by adding this diagonal matrix, you're essentially making all of them non-zero, and that makes it invertible, right?",
    "start": "2401680",
    "end": "2409345"
  },
  {
    "text": "So this is also called regularization. We're, we're probably going to see this in,",
    "start": "2409345",
    "end": "2414505"
  },
  {
    "text": "in, uh, the context of regularization as well. Adding this, uh, uh, uh,",
    "start": "2414505",
    "end": "2419830"
  },
  {
    "text": "term is effectively equivalent to regularizing our uh, our model, right?",
    "start": "2419830",
    "end": "2425725"
  },
  {
    "text": "And- well, maybe there's a giveaway, but this, you know, this, this observation is probably going to solve a big chunk",
    "start": "2425725",
    "end": "2433360"
  },
  {
    "text": "of one of your homework problems in your next homework, but that's fine. Um, so the, the data given S in the Bayesian setting",
    "start": "2433360",
    "end": "2441670"
  },
  {
    "text": "is now a distribution that is centered around this value,",
    "start": "2441670",
    "end": "2446724"
  },
  {
    "text": "which is very close to the maximum likelihood estimate and has some, some, some variance over here.",
    "start": "2446725",
    "end": "2453650"
  },
  {
    "text": "And so this was the posterior.",
    "start": "2453920",
    "end": "2459369"
  },
  {
    "text": "And next we construct the posterior predictive distribution.",
    "start": "2462860",
    "end": "2467589"
  },
  {
    "text": "The, the kind of mental math you need to have is calculating the posterior is like fitting a model,",
    "start": "2476340",
    "end": "2483250"
  },
  {
    "text": "even though we are not doing gradient ascent, calculating the posterior is like somewhat similar to estimating your, uh, uh,",
    "start": "2483250",
    "end": "2491350"
  },
  {
    "text": "maximum likelihood estimates and calculating your posterior predictive distribution is, is, is,",
    "start": "2491350",
    "end": "2498685"
  },
  {
    "text": "is like calculating the function which will make predictions on your new data,",
    "start": "2498685",
    "end": "2504055"
  },
  {
    "text": "and in case of, uh, the frequentist method there wasn't a distinction between the two.",
    "start": "2504055",
    "end": "2509154"
  },
  {
    "text": "But in, in Bayesian methods there is, there is a distinction, yes. Uh, [inaudible].",
    "start": "2509155",
    "end": "2518890"
  },
  {
    "text": "A justification for this. So this has quite a lot of algebra that I don't wanna go into. You know, you can just, you know, uh,",
    "start": "2518890",
    "end": "2525565"
  },
  {
    "text": "just believe me that if you work out the algebra, you are going to get this form. Work out what algebra? Uh, apply Bayes' rule to calculate the posterior. Yes, question?",
    "start": "2525565",
    "end": "2534910"
  },
  {
    "text": "[inaudible].",
    "start": "2534910",
    "end": "2544030"
  },
  {
    "text": "So if, if, uh. [inaudible].",
    "start": "2544030",
    "end": "2552250"
  },
  {
    "text": "So the question is in, in the posterior predictive distribution [NOISE], yeah.",
    "start": "2552250",
    "end": "2561610"
  },
  {
    "text": "Do we have a y star? So this is y star. [inaudible].",
    "start": "2561610",
    "end": "2571900"
  },
  {
    "text": "Right. So in the, in the- for- in the posterior predictive distribution, the setting is that you are given the training set x and y.",
    "start": "2571900",
    "end": "2579205"
  },
  {
    "text": "Right? And on new examples, you only know x. You need to make a prediction on them. Like you don't know what it is, right?",
    "start": "2579205",
    "end": "2584905"
  },
  {
    "text": "And for that we, uh, we, we, uh, estimate it by this probability distribution with a.",
    "start": "2584905",
    "end": "2590320"
  },
  {
    "text": "[inaudible] or just the probability? Yes. So, um, this will give us a probability distribution over y star,",
    "start": "2590320",
    "end": "2598675"
  },
  {
    "text": "and if you want a point estimate, it is quite common to take the expectation of this distribution as,",
    "start": "2598675",
    "end": "2604525"
  },
  {
    "text": "as the, um, if you want to make a point prediction for y. A lot of times, um,",
    "start": "2604525",
    "end": "2611305"
  },
  {
    "text": "we want just point estimates to make a prediction, and in those cases it's, you know, it's, it's, um,",
    "start": "2611305",
    "end": "2617049"
  },
  {
    "text": "it's normal to take this distribution and then take yet",
    "start": "2617050",
    "end": "2622690"
  },
  {
    "text": "another expectation on it as our y hat as a point estimate of the predictive y distribution.",
    "start": "2622690",
    "end": "2632230"
  },
  {
    "text": "And in many other cases, you actually want to hold on to the full predictive distribution to know what is",
    "start": "2632230",
    "end": "2638200"
  },
  {
    "text": "the uncertainty you have in making that prediction, right? If, uh, if you have the full predictive distribution for y star,",
    "start": "2638200",
    "end": "2644950"
  },
  {
    "text": "and if that prediction has low variance, if it's kind of concentrated, then it generally means your model is pretty confident of what the value is.",
    "start": "2644950",
    "end": "2653019"
  },
  {
    "text": "Whereas if it's, if it has a high variance, then it means your model has less confident. [NOISE] Which, which, which,",
    "start": "2653020",
    "end": "2667029"
  },
  {
    "text": "um, brings to a good point. So one of the, the fundamental, um,",
    "start": "2667030",
    "end": "2672895"
  },
  {
    "text": "points about Bayesian approaches is that Bayesian approaches are used for estimating uncertainties, right?",
    "start": "2672895",
    "end": "2679525"
  },
  {
    "text": "In maximum likelihood estimation, we got a point estimate for theta hat, the parameters, but there was no uncertainty estimation that came along with it, right?",
    "start": "2679525",
    "end": "2690714"
  },
  {
    "text": "Uh, generally, if you have a lot of data, you should expect your estimate theta hat to be- you know,",
    "start": "2690715",
    "end": "2696280"
  },
  {
    "text": "to have more confidence in that estimate. Similarly, if your data was small, you should expect, you know,",
    "start": "2696280",
    "end": "2702550"
  },
  {
    "text": "less confidence in your estimate of theta and frequentist methods don't give you that,",
    "start": "2702550",
    "end": "2707725"
  },
  {
    "text": "you know, um, confidence estimation. Whereas with Bayesian approaches, we are estimating theta given S,",
    "start": "2707725",
    "end": "2714760"
  },
  {
    "text": "and what you'll see is that if you have a lot of data, the posterior distribution will be more peaked on certain values of theta, right?",
    "start": "2714760",
    "end": "2723744"
  },
  {
    "text": "And if you- if you have less data then your posterior distribution would be more spread out. And similarly, from- um,",
    "start": "2723745",
    "end": "2730180"
  },
  {
    "text": "while making predictions on y star or on x star.",
    "start": "2730180",
    "end": "2735319"
  },
  {
    "text": "If you have, um, um, if your posterior distribution is,",
    "start": "2735510",
    "end": "2740680"
  },
  {
    "text": "is pretty confident, is, is pretty peaked, and if x star is in the region where your training set",
    "start": "2740680",
    "end": "2747369"
  },
  {
    "text": "more or less was where you got them from then your predictions will also be pretty confident. Your p of y given x will also be pretty, uh, uh, concentrated.",
    "start": "2747370",
    "end": "2756369"
  },
  {
    "text": "But as if, if your point is far away from where your training set was, then naturally with, you know, the posterior prediction distribution will also have a high variance.",
    "start": "2756370",
    "end": "2763974"
  },
  {
    "text": "That's, that's one of the benefits of Bayesian approaches because you get uncertainty estimates for free, because you are estimating the full distribution of parameters and,",
    "start": "2763975",
    "end": "2772089"
  },
  {
    "text": "and predictions. Yes, question. [inaudible].",
    "start": "2772090",
    "end": "2779980"
  },
  {
    "text": "So the likelihood function appears in the Bayesian, uh, Bayesian method. Uh, so p of theta given S,",
    "start": "2779980",
    "end": "2786490"
  },
  {
    "text": "theta given S is P of S given theta times p of theta or p of S. So this is your likelihood function.",
    "start": "2786490",
    "end": "2799900"
  },
  {
    "text": "So I can- in your frequentist approach, you can still look at the confidence interval right? Like you can look at linear [inaudible] you can say you're sure of [inaudible] .",
    "start": "2799900",
    "end": "2809610"
  },
  {
    "text": "Yeah, so in, in Frequentist approaches, sure, so you can get confidence intervals on,",
    "start": "2809610",
    "end": "2815435"
  },
  {
    "text": "on, on Theta., but the, the theory is much more well-developed for Bayesian approaches.",
    "start": "2815435",
    "end": "2821825"
  },
  {
    "text": "Especially, you know, it's, it's, yeah, the, the theory is,",
    "start": "2821825",
    "end": "2827569"
  },
  {
    "text": "is more well-developed for Bayesian approaches for getting uncertainty estimates. And the conf- the,",
    "start": "2827570",
    "end": "2832625"
  },
  {
    "text": "the way you interpret a confidence interval is also pretty different from the,",
    "start": "2832625",
    "end": "2837635"
  },
  {
    "text": "the variance over here. So the confidence interval, wha- what you get from a frequentist approach, tells you,",
    "start": "2837635",
    "end": "2844490"
  },
  {
    "text": "if you were to repeat this experiment. Supposing you have a 95% confidence interval, right,",
    "start": "2844490",
    "end": "2851705"
  },
  {
    "text": "that confidence interval tells you, if you repeat this experiment, you know, an infinite number of times,",
    "start": "2851705",
    "end": "2857315"
  },
  {
    "text": "then 95% of those experiments will give you a Theta hat in this interval.",
    "start": "2857315",
    "end": "2863165"
  },
  {
    "text": "It is not telling you that your estimate of Theta hat lies in that interval with 95% probability, right?",
    "start": "2863165",
    "end": "2870590"
  },
  {
    "text": "But as with this, you are getting that, you know, it tells you, you know, what, what probability the, the true Theta,",
    "start": "2870590",
    "end": "2878374"
  },
  {
    "text": "what is the probability of the true Theta lying in some interval given by this, right? So that's the difference between confidence intervals and this. Yes, question.",
    "start": "2878374",
    "end": "2886265"
  },
  {
    "text": "You say you want to increase the variance of the Theta. Is that, is that [inaudible]",
    "start": "2886265",
    "end": "2895819"
  },
  {
    "text": "What's the question again? You have the distribution over Theta- Distribution over Theta.",
    "start": "2895820",
    "end": "2901520"
  },
  {
    "text": "[inaudible] versus the variance of [inaudible] Yeah. So, uh, if I, am I right saying that we need to differentiate,",
    "start": "2901520",
    "end": "2910430"
  },
  {
    "text": "like find the minimum of A inverse to get the minimum variance of Theta?",
    "start": "2910430",
    "end": "2915559"
  },
  {
    "text": "So, so the question is, should we differentiate and optimize this A to get low, low variance? So here there is no optimization going on.",
    "start": "2915560",
    "end": "2922730"
  },
  {
    "text": "There is no loss function, there is no gradient descent, there is no optimization going on anywhere. All we're doing is conditioning.",
    "start": "2922730",
    "end": "2928895"
  },
  {
    "text": "We condition on this and use the Bayes' Rule to construct a distribution,",
    "start": "2928895",
    "end": "2935150"
  },
  {
    "text": "and what we get is what we get, right? It's, it's very different from the Frequentist approach.",
    "start": "2935150",
    "end": "2940700"
  },
  {
    "text": "The Frequentist approach we are minimizing some kind of a loss or maximizing some kind of a likelihood to get estimates of your Theta.",
    "start": "2940700",
    "end": "2947345"
  },
  {
    "text": "Over here we just apply Bayes' Rule and we get what we get. But, uh, is it like, is it what we wish that variance should be low?",
    "start": "2947345",
    "end": "2954695"
  },
  {
    "text": "Isn't it what- We wish that variance is low, and the, the way the variance was-will be low is if you have a good amount of data, right?",
    "start": "2954695",
    "end": "2962810"
  },
  {
    "text": "If you, if you have a lot of data, then, you know, this will naturally be low. All right, so moving on.",
    "start": "2962810",
    "end": "2969800"
  },
  {
    "text": "So the posterior predictive distribution is essentially y star,",
    "start": "2969800",
    "end": "2975260"
  },
  {
    "text": "given x star, comma S, follows a normal distribution of one over sigma square,",
    "start": "2975260",
    "end": "2985319"
  },
  {
    "text": "x transpose A, A inverse,",
    "start": "2986470",
    "end": "2995105"
  },
  {
    "text": "x transpose Y, x star transpose A inverse,",
    "start": "2995105",
    "end": "3006385"
  },
  {
    "text": "x star plus sigma squared, right.",
    "start": "3006385",
    "end": "3011815"
  },
  {
    "text": "Again, this looks a little cryptic, but, you know it's, it's, we can still kind of break it down and make sense of it.",
    "start": "3011815",
    "end": "3017454"
  },
  {
    "text": "What's happening here is we had a distribution for Theta,",
    "start": "3017455",
    "end": "3023200"
  },
  {
    "text": "which was some normal distribution. Now if you have, if you have a distribution um, uh, Theta,",
    "start": "3023200",
    "end": "3031195"
  },
  {
    "text": "which is normal with some mean mu and covariance sigma,",
    "start": "3031195",
    "end": "3038470"
  },
  {
    "text": "where mu here is this and sigma is this, then x transpose Theta.",
    "start": "3038470",
    "end": "3046000"
  },
  {
    "text": "This is just a property of Gaussians, it's also normal, with mean mu transpose x and variance,",
    "start": "3046000",
    "end": "3054699"
  },
  {
    "text": "x transpose sigma x. And this is just a property of Gaussians, okay?",
    "start": "3054699",
    "end": "3062635"
  },
  {
    "text": "And now we have our Theta to have, you know, some normal distribution and to make a prediction.",
    "start": "3062635",
    "end": "3069385"
  },
  {
    "text": "Here, x star, you know, take x star, and we, and we dot-product, we take a dot product with,",
    "start": "3069385",
    "end": "3075160"
  },
  {
    "text": "with Theta because that's what linear regression does. And that gives us this expression,",
    "start": "3075160",
    "end": "3080200"
  },
  {
    "text": "where this expression is just like x transpose mu, where mu is this,",
    "start": "3080200",
    "end": "3086214"
  },
  {
    "text": "dot this with x star and you get this, and the covariance is, is this.",
    "start": "3086215",
    "end": "3094345"
  },
  {
    "text": "Basically, take A inverse and, take A inverse and, and,",
    "start": "3094345",
    "end": "3101200"
  },
  {
    "text": "and calculate the, and calculate the,",
    "start": "3101200",
    "end": "3106724"
  },
  {
    "text": "uh, uh, quadratic form with x star just like this. And then we add this extra sigma square,",
    "start": "3106725",
    "end": "3114510"
  },
  {
    "text": "because this is the posterior predictive distribution of y star. Because y star the obs- the, the,",
    "start": "3114510",
    "end": "3121785"
  },
  {
    "text": "the y-value that you might observe has the same- will have the same noise embedded in it in the test example as well.",
    "start": "3121785",
    "end": "3129430"
  },
  {
    "text": "So that- so to, to account for what the possible y star you might observe for the test example,",
    "start": "3129430",
    "end": "3136975"
  },
  {
    "text": "you want to account for the extra, extra variance and you're adding that variance because of, and you add the variance because of the IID assumption, right?",
    "start": "3136975",
    "end": "3144474"
  },
  {
    "text": "So, yeah, so this, this might look pretty complex, but it is- it follows pretty directly from Theta transpose x, Theta,",
    "start": "3144475",
    "end": "3153700"
  },
  {
    "text": "Theta given, Theta given S. And again, even this, which looks cryptic,",
    "start": "3153700",
    "end": "3158920"
  },
  {
    "text": "is actually very similar to the estimate that we get from the normal equation with some regularization term that comes in. Yes, question.",
    "start": "3158920",
    "end": "3165760"
  },
  {
    "text": "[inaudible]",
    "start": "3165760",
    "end": "3172900"
  },
  {
    "text": "Yeah, so Theta is what we have. So, right and, and S is x. Yeah,",
    "start": "3172900",
    "end": "3179724"
  },
  {
    "text": "so this is, you have the extra, extra S. So if Theta has this distribution,",
    "start": "3179725",
    "end": "3186325"
  },
  {
    "text": "then y star, y star is equal to this. Theta transpose x will have this distribution.",
    "start": "3186325",
    "end": "3192910"
  },
  {
    "text": "[inaudible] Right? So p of- so y star given x,",
    "start": "3192910",
    "end": "3199480"
  },
  {
    "text": "x star and S is basically the posterior transformed in this way. [inaudible]",
    "start": "3199480",
    "end": "3207220"
  },
  {
    "text": "Good question. It this clear, right?",
    "start": "3207220",
    "end": "3212575"
  },
  {
    "text": "Yeah, so this is, this is just an example of how you would apply a Bayesian method for linear regression.",
    "start": "3212575",
    "end": "3220180"
  },
  {
    "text": "And this is called Bayesian linear regression, right. It, it has a few more terms and identities compared",
    "start": "3220180",
    "end": "3227110"
  },
  {
    "text": "to the standard normal equations from the Frequentist method.",
    "start": "3227110",
    "end": "3232360"
  },
  {
    "text": "But you get, you know, uncertainty estimates by doing this extra, extra calculation.",
    "start": "3232360",
    "end": "3237595"
  },
  {
    "text": "Right? Any questions before we move on to Gaussian processes? Yes. Question.",
    "start": "3237595",
    "end": "3245320"
  },
  {
    "text": "[inaudible]",
    "start": "3245320",
    "end": "3253150"
  },
  {
    "text": "Sure. Yeah. So this would be the mean",
    "start": "3253150",
    "end": "3258640"
  },
  {
    "text": "and y star equals y hat plus epsilon star.",
    "start": "3258640",
    "end": "3265329"
  },
  {
    "text": "Right? Yeah, good point. Yeah, so this extra epsilon accounts for",
    "start": "3265330",
    "end": "3270520"
  },
  {
    "text": "this extra plus sigma squared. Thank you.",
    "start": "3270520",
    "end": "3275960"
  },
  {
    "text": "So in general, Bayesian methods tend to be heavy utilizers of probability theory because",
    "start": "3290280",
    "end": "3299365"
  },
  {
    "text": "all what you are doing is taking probability distributions and conditioning them to apply Bayes' rule and that's about it.",
    "start": "3299365",
    "end": "3307810"
  },
  {
    "text": "That's all what we- what you do in- in Bayesian methods. You assign everything, a prior probability and then",
    "start": "3307810",
    "end": "3314079"
  },
  {
    "text": "you observe some and depending on what you observe, you condition those probabilities on- based on what you observe,",
    "start": "3314080",
    "end": "3320859"
  },
  {
    "text": "and you update the beliefs of the unknown. And here the unknown could be your model parameters. It could be the unknown labels of new test examples but you follow the same recipe.",
    "start": "3320860",
    "end": "3332230"
  },
  {
    "text": "You assign a prior probability to everything, you observe something, and based on the observation,",
    "start": "3332230",
    "end": "3338455"
  },
  {
    "text": "you update your prior probabilities to a posterior probability by conditioning.",
    "start": "3338455",
    "end": "3343570"
  },
  {
    "text": "And you're using the Bayes rule to do that. Yes? [NOISE] Uh, Talked about linear predictors.",
    "start": "3343570",
    "end": "3349900"
  },
  {
    "text": "[OVERLAPPING] How is it",
    "start": "3349900",
    "end": "3359890"
  },
  {
    "text": "that about transpose x [inaudible] Yeah. But here, x itself is transposed,",
    "start": "3359890",
    "end": "3366190"
  },
  {
    "text": "so mu transpose x is x transpose y. Here x is transposed. X naught is transposed, rather. [inaudible]",
    "start": "3366190",
    "end": "3389020"
  },
  {
    "text": "[NOISE] So the question is, how is this x transpo- x star transpose of this? Right. So- So- um,",
    "start": "3389020",
    "end": "3396685"
  },
  {
    "text": "add an x theta star transpose and leave it as it is. And this is a scalar, just move it out and you get that.",
    "start": "3396685",
    "end": "3403090"
  },
  {
    "text": "Oh, you say x transpose is same as transpose x [inaudible] Yeah. That part is always- [NOISE] All right.",
    "start": "3403090",
    "end": "3417385"
  },
  {
    "text": "Gaussian processes.",
    "start": "3417385",
    "end": "3420170"
  },
  {
    "text": "Before we jump into Gaussian processes, it's probably a good time to again to remind ourselves",
    "start": "3430220",
    "end": "3437984"
  },
  {
    "text": "of some of the basics of functional analysis that we saw earlier on, right? In functional analysis, we saw this relation between vectors and functions, right?",
    "start": "3437984",
    "end": "3451910"
  },
  {
    "text": "Now, mentally it is the same relation between vectors and",
    "start": "3452670",
    "end": "3458680"
  },
  {
    "text": "functions that exists between multivariate Gaussian and Gaussian processes.",
    "start": "3458680",
    "end": "3469209"
  },
  {
    "text": "[NOISE] right?",
    "start": "3469209",
    "end": "3474880"
  },
  {
    "text": "So the multivariate Gaussian are defined on vectors,",
    "start": "3474880",
    "end": "3480115"
  },
  {
    "text": "and Gaussian processes are defined on functions. [NOISE] And it is the same- it's in fact exactly the same- same,",
    "start": "3480115",
    "end": "3490960"
  },
  {
    "text": "uh- uh, relations of- of- of how vectors and functions are related, that multivariate Gaussian are related to Gaussian processes.",
    "start": "3490960",
    "end": "3498445"
  },
  {
    "text": "Okay. And to start off Gaussian process, let's review some properties of the multivariate Gaussian distribution.",
    "start": "3498445",
    "end": "3509750"
  },
  {
    "text": "So [NOISE] properties",
    "start": "3510240",
    "end": "3517075"
  },
  {
    "text": "of multivariate Gaussian.",
    "start": "3517075",
    "end": "3524300"
  },
  {
    "text": "And also, um- um, for you to kind of expect what's to come.",
    "start": "3526200",
    "end": "3533470"
  },
  {
    "text": "The functions over which we are gonna define Gaussian processes is gonna be our hypothesis function, right?",
    "start": "3533470",
    "end": "3543490"
  },
  {
    "text": "So the- the idea to have in mind is that, the way we- the way we went from vectors to functions,",
    "start": "3543490",
    "end": "3551785"
  },
  {
    "text": "we can make the same leap from multivariate Gaussian to Gaussian processes as probability distribution of a functions,",
    "start": "3551785",
    "end": "3558265"
  },
  {
    "text": "and the functions over which we wanted to find Gaussian processes, are our hypothesis functions with- with which we want to make predictions, right?",
    "start": "3558265",
    "end": "3565675"
  },
  {
    "text": "So first property of Gaussians is normalization.",
    "start": "3565675",
    "end": "3573310"
  },
  {
    "text": "That is, integral with respect to X,",
    "start": "3575660",
    "end": "3581740"
  },
  {
    "text": "P of X mu sigma is equal to 1.",
    "start": "3581930",
    "end": "3591595"
  },
  {
    "text": "Where, ah, we are integrating over the full vector space. So if you integrate out everything,",
    "start": "3591595",
    "end": "3597880"
  },
  {
    "text": "you get, uh- uh, you get one. And this is just our- uh, factor of probability.",
    "start": "3597880",
    "end": "3604705"
  },
  {
    "text": "And supposing we have a vector X equals X_A,",
    "start": "3604705",
    "end": "3613135"
  },
  {
    "text": "X_B, right, which has a mean.",
    "start": "3613135",
    "end": "3621325"
  },
  {
    "text": "Let say this is according to a normal distribution. Mu A, mu B.",
    "start": "3621325",
    "end": "3631880"
  },
  {
    "text": "And covariance of sigma AA,",
    "start": "3632970",
    "end": "3639820"
  },
  {
    "text": "sigma AB, sigma BA, sigma BB.",
    "start": "3639820",
    "end": "3647120"
  },
  {
    "text": "Where we call this mu, and this sigma.",
    "start": "3649080",
    "end": "3654350"
  },
  {
    "text": "With this, we have the second property of Marginalization.",
    "start": "3657270",
    "end": "3663520"
  },
  {
    "text": "[NOISE] So for marginalization,",
    "start": "3663520",
    "end": "3673540"
  },
  {
    "text": "given, uh- uh, given a probability uh- uh, distribution that way,",
    "start": "3673540",
    "end": "3680425"
  },
  {
    "text": "so P of X_A equal to integrate out X_B",
    "start": "3680425",
    "end": "3689290"
  },
  {
    "text": "from P of X mu sigma dx_B.",
    "start": "3689290",
    "end": "3701540"
  },
  {
    "text": "So you take the full probability distribution and you integrate out only a few components.",
    "start": "3702510",
    "end": "3710080"
  },
  {
    "text": "Integrate out just XB and you get P of X_A and P of",
    "start": "3710080",
    "end": "3715555"
  },
  {
    "text": "X_A will be normal with mean mu_A.",
    "start": "3715555",
    "end": "3721690"
  },
  {
    "text": "Covariance sigma AA.",
    "start": "3721690",
    "end": "3725720"
  },
  {
    "text": "More generally, if you have, um, a vector x equal to x_1, x_2,",
    "start": "3731520",
    "end": "3742390"
  },
  {
    "text": "x_d, and if this distributed according to a normal distribution with mu_1, mu_2,",
    "start": "3742390",
    "end": "3753250"
  },
  {
    "text": "mu_d and covariance Sigma_1,",
    "start": "3753250",
    "end": "3759640"
  },
  {
    "text": "1 until Sigma_1d, Sigma_d1, Sigma_dd, right?",
    "start": "3759640",
    "end": "3773600"
  },
  {
    "text": "And if you want to estimate the marginal by integrating out,",
    "start": "3773730",
    "end": "3779140"
  },
  {
    "text": "say, x_2, let's say we want to marginalize out only x_2. The nice property about Gaussians is that by performing this over x_2,",
    "start": "3779140",
    "end": "3792260"
  },
  {
    "text": "the normal distribution that you end up getting- the- the distribution that you end up getting will also be normal with",
    "start": "3794310",
    "end": "3801345"
  },
  {
    "text": "just the corresponding entry is related to mu taken out, okay?",
    "start": "3801345",
    "end": "3809800"
  },
  {
    "text": "That's a very nice property of Gaussians. So if you're- if you're not- if you marginalize out x_2,",
    "start": "3809800",
    "end": "3814825"
  },
  {
    "text": "then you get a mean, uh, the marginalized distribution will have a mean vector with just that component removed,",
    "start": "3814825",
    "end": "3823330"
  },
  {
    "text": "and the covariance will have that corresponding row and column removed, right?",
    "start": "3823330",
    "end": "3828490"
  },
  {
    "text": "That's a very nice property about multivariate Gaussians. Marginalizing is super easy, right?",
    "start": "3828490",
    "end": "3833950"
  },
  {
    "text": "Just take off the things that you wanna marginalize and you get- you get the right answer.",
    "start": "3833950",
    "end": "3840500"
  },
  {
    "text": "Third thing is conditioning.",
    "start": "3841050",
    "end": "3844340"
  },
  {
    "text": "So in the same setting, if you have X_A and, um, x as normal, um,",
    "start": "3852570",
    "end": "3859135"
  },
  {
    "text": "having two parts, X_A and X_B. In this example, A and B are not necessarily scalars, they are just, you know,",
    "start": "3859135",
    "end": "3865450"
  },
  {
    "text": "sub vectors, vectors with, you know, uh, they can have multiple components.",
    "start": "3865450",
    "end": "3870680"
  },
  {
    "text": "So p of r X_A given X_B given X_B",
    "start": "3872010",
    "end": "3881360"
  },
  {
    "text": "is a normal distribution with mean mu_A plus Sigma_AB,",
    "start": "3881430",
    "end": "3892970"
  },
  {
    "text": "Sigma_BB inverse",
    "start": "3895800",
    "end": "3901700"
  },
  {
    "text": "times X_B minus mu_B.",
    "start": "3902520",
    "end": "3911330"
  },
  {
    "text": "This is a scary looking expression, but, you know, let's- let's- let's-, um, let's analyze it.",
    "start": "3912810",
    "end": "3920815"
  },
  {
    "text": "So minus Sigma_AB,",
    "start": "3920815",
    "end": "3924829"
  },
  {
    "text": "Sigma_BB inverse, Sigma_BA.",
    "start": "3927090",
    "end": "3933620"
  },
  {
    "text": "Wow, this looks- this looks pretty- pretty, um, scary, but it's actually, um,",
    "start": "3935550",
    "end": "3943015"
  },
  {
    "text": "not- not as complex as- as you might, uh, it- it might appear at first. Now, to- to kind of understand this a little more,",
    "start": "3943015",
    "end": "3950049"
  },
  {
    "text": "let's look at a case where X_A and X_B are scalars, right?",
    "start": "3950050",
    "end": "3955105"
  },
  {
    "text": "So if you have, um, suppose we have, let's say,",
    "start": "3955105",
    "end": "3960700"
  },
  {
    "text": "a- a- a normal vector a, b. That's normal with mu_a mu_b and covariance,",
    "start": "3960700",
    "end": "3976945"
  },
  {
    "text": "Sigma_a squared, Rho Sigma_a, Sigma_b Rho- Rho Sigma_a,",
    "start": "3976945",
    "end": "3987190"
  },
  {
    "text": "Sigma_b, and",
    "start": "3987190",
    "end": "3993759"
  },
  {
    "text": "Sigma_b squared, right? Any covariance matrix can be represented as this.",
    "start": "3993759",
    "end": "3999835"
  },
  {
    "text": "So take the diagonals, take the square roots, and you can write the, uh, covariance terms as the correlation coefficient times",
    "start": "3999835",
    "end": "4007770"
  },
  {
    "text": "the standard deviations of the two, right? So that's just the- this definition of covariance, right?",
    "start": "4007770",
    "end": "4013380"
  },
  {
    "text": "So let's see what- what conditioning looks like in case of this, right?",
    "start": "4013380",
    "end": "4019410"
  },
  {
    "text": "So a given b is now, according to this formula,",
    "start": "4019410",
    "end": "4026170"
  },
  {
    "text": "a normal distribution with mean mu_a plus Sigma_AB.",
    "start": "4026170",
    "end": "4035010"
  },
  {
    "text": "Sigma AB is just Rho Sigma_a, Sigma_b times Sigma_BB inverse,",
    "start": "4035010",
    "end": "4042930"
  },
  {
    "text": "so that is divided by Sigma_b squared times X_B,",
    "start": "4042930",
    "end": "4048740"
  },
  {
    "text": "X_B is b minus mu_B, it's just mu_B,",
    "start": "4048740",
    "end": "4058390"
  },
  {
    "text": "Sigma_AA, that's just, uh, Sigma_a square minus Sigma_AB, that's Rho a,",
    "start": "4060140",
    "end": "4068730"
  },
  {
    "text": "Rho Sigma_a, Rho Sigma_a, Sigma_b times, uh, Sigma_BB inverse,",
    "start": "4068730",
    "end": "4077085"
  },
  {
    "text": "that's divided by Sigma_b squared times Sigma_BA. That's again Rho- Rho Sigma_a, Sigma_b, right?",
    "start": "4077085",
    "end": "4089610"
  },
  {
    "text": "We can simplify this further. So this is the same as normal mu_a plus- we can cancel one Sigma_b here.",
    "start": "4089610",
    "end": "4099660"
  },
  {
    "text": "So you get Rho or Sigma_a",
    "start": "4099660",
    "end": "4106510"
  },
  {
    "text": "times Rho of B minus mu_b over Sigma,",
    "start": "4106580",
    "end": "4116080"
  },
  {
    "text": "Sigma_a square times- so one Sigma_b b, second Sigma B.",
    "start": "4117770",
    "end": "4125799"
  },
  {
    "text": "Right, and this is minus Rho square, Sigma_a square, okay?",
    "start": "4126230",
    "end": "4135900"
  },
  {
    "text": "And this, I'm gonna just write it as Sigma square times 1 minus Rho, right?",
    "start": "4135900",
    "end": "4147495"
  },
  {
    "text": "So now, it's- it's much simpler. So what this is basically saying is if we have a Gaussian distribution of",
    "start": "4147495",
    "end": "4153795"
  },
  {
    "text": "two variables and we observe one of them, b to have some specific value.",
    "start": "4153795",
    "end": "4161025"
  },
  {
    "text": "Then, what- what's happening here? If you take b divided by me- by its mean",
    "start": "4161025",
    "end": "4168540"
  },
  {
    "text": "and subtract its mean and divide by its standard deviation, this is like the z value of, of b, right?",
    "start": "4168540",
    "end": "4176654"
  },
  {
    "text": "So this- this- this is the z value of b. Take the z value of b and multiply it by the correlation coefficient,",
    "start": "4176655",
    "end": "4186673"
  },
  {
    "text": "and then re transform it back into a, okay? Multiplied by Sigma_a and add mu_a, okay?",
    "start": "4186674",
    "end": "4193170"
  },
  {
    "text": "Take B, get it down to this, uh, uh, the z value, transform it using the correlation coefficient and then rescale it",
    "start": "4193170",
    "end": "4201900"
  },
  {
    "text": "back into A's range, okay? And what's happening here?",
    "start": "4201900",
    "end": "4208179"
  },
  {
    "text": "Take a- by observing b the variance of a reduces by a factor of 1 minus Rho square,",
    "start": "4211910",
    "end": "4220470"
  },
  {
    "text": "where Rho is how much a and b are correlated. Yes, question.",
    "start": "4220470",
    "end": "4226620"
  },
  {
    "text": "[inaudible] So rho is the, is the correlation coefficient.",
    "start": "4226620",
    "end": "4233740"
  },
  {
    "text": "So the correlation coefficient is- um, so basically- [NOISE] basically uh,",
    "start": "4234440",
    "end": "4243975"
  },
  {
    "text": "the covariance equals rho times- ah,",
    "start": "4243975",
    "end": "4253080"
  },
  {
    "text": "rho times the- the standard deviation of a times standard deviation of- of b.",
    "start": "4253080",
    "end": "4262215"
  },
  {
    "text": "It's also called the Poisson correlation coefficient. It's the same thing, just the correlation coeff- how- how- how well are they correlated.",
    "start": "4262215",
    "end": "4268170"
  },
  {
    "text": "So if you have a Gaussian variable that looks like this,",
    "start": "4268170",
    "end": "4274140"
  },
  {
    "text": "then you have a positive correlation, because the higher the value of- of one variable,",
    "start": "4274140",
    "end": "4279255"
  },
  {
    "text": "the more likely the second one is also higher. If it is like this, then it has a negative correlation.",
    "start": "4279255",
    "end": "4284595"
  },
  {
    "text": "If it's a perfect sphere, then there is zero correlation, right? It's telling you how much information of a lies in b.",
    "start": "4284595",
    "end": "4291735"
  },
  {
    "text": "By observing b, how much can you learn about a, right? So, yeah, that's basically what conditioning is does- is doing.",
    "start": "4291735",
    "end": "4299655"
  },
  {
    "text": "And this is the- you can think of this as the multivariate version of- of this, right? So this is just um,",
    "start": "4299655",
    "end": "4307170"
  },
  {
    "text": "um calculating like the z value of- of the b vector and then pre-scaling it back into the a's scale.",
    "start": "4307170",
    "end": "4315704"
  },
  {
    "text": "And over here, from the variance of a, you're subtracting away some- some variance.",
    "start": "4315705",
    "end": "4321660"
  },
  {
    "text": "Basically you are getting information by observing b, right? And- and this is the same as- as this,",
    "start": "4321660",
    "end": "4328020"
  },
  {
    "text": "but there's just a- a multivariate version of this, okay? And if you're- if you're- um, um,",
    "start": "4328020",
    "end": "4335130"
  },
  {
    "text": "if you've done some advanced linear algebra, this term, what you see here, is also called the shared complement.",
    "start": "4335130",
    "end": "4341680"
  },
  {
    "text": "But if you don't know what that is, it's not necessary. But if you know the- this basically the shared complement of- of,",
    "start": "4344300",
    "end": "4350970"
  },
  {
    "text": "ah, of a, all right? And then we have one more property, which is the sum of two independent Gaussian variables.",
    "start": "4350970",
    "end": "4360855"
  },
  {
    "text": "So if you have x distributed as mu_1, sigma_1,",
    "start": "4360855",
    "end": "4367785"
  },
  {
    "text": "y as mu_2, sigma_2,",
    "start": "4367785",
    "end": "4373245"
  },
  {
    "text": "then x plus y is just sphered as a normal, mu_1 plus mu_2, and sigma_1 plus sigma_2.",
    "start": "4373245",
    "end": "4383380"
  },
  {
    "text": "And this is called properties summation.",
    "start": "4385370",
    "end": "4390760"
  },
  {
    "text": "With these properties, believe it or not, we are more than halfway into Gaussian processes.",
    "start": "4394520",
    "end": "4400065"
  },
  {
    "text": "You're just going to apply them and [NOISE] derive Gaussian processes.",
    "start": "4400065",
    "end": "4409150"
  },
  {
    "text": "Of these properties, the one that looks the scariest, is probably the conditioning property.",
    "start": "4415700",
    "end": "4422340"
  },
  {
    "text": "And you can make it- I guess you can approach it or make it less scary by seeing the analogy to the two vari- to the two, uh, variable case.",
    "start": "4422340",
    "end": "4431505"
  },
  {
    "text": "And just believe that, that monstrous expression is, it's just a multivariate generalization of that.",
    "start": "4431505",
    "end": "4438600"
  },
  {
    "text": "[NOISE].",
    "start": "4438600",
    "end": "4452250"
  },
  {
    "text": "Yeah, question. [inaudible] How do you go about proving something like that? Uh, the property three.",
    "start": "4452250",
    "end": "4459000"
  },
  {
    "text": "Property three, how do you go about proving that? Ah, I would say that's beyond the scope of this course. You know, just take it for granted.",
    "start": "4459000",
    "end": "4464699"
  },
  {
    "text": "And- and you can- there's a- there's a- there is uh, a book called, if you're interested,",
    "start": "4464700",
    "end": "4470534"
  },
  {
    "text": "it's called Gaussian Processes for Machine Learning.",
    "start": "4470535",
    "end": "4474040"
  },
  {
    "text": "And I would- I would point you to that. There's also some notes that I have linked to in- in- uh,",
    "start": "4476390",
    "end": "4484770"
  },
  {
    "text": "in the syllabus page and has an appendix where there's a proof for that. So, all right, so Gaussian processes.",
    "start": "4484770",
    "end": "4494580"
  },
  {
    "text": "[NOISE] So in Gaussian processes,",
    "start": "4494580",
    "end": "4502905"
  },
  {
    "text": "what we want to do is- so let me try to draw ah, an analogy with- with um,",
    "start": "4502905",
    "end": "4510090"
  },
  {
    "text": "multi- multivariate Gaussian and",
    "start": "4510090",
    "end": "4518290"
  },
  {
    "text": "Gaussian process.",
    "start": "4518960",
    "end": "4521980"
  },
  {
    "text": "In case of a multivariate Gaussian, we had [NOISE] say y_1,",
    "start": "4524030",
    "end": "4537600"
  },
  {
    "text": "y_2, y_d,",
    "start": "4537600",
    "end": "4541450"
  },
  {
    "text": "according to some normal with mean mu_1,",
    "start": "4542620",
    "end": "4549320"
  },
  {
    "text": "mu_2, mu_d,",
    "start": "4549320",
    "end": "4554110"
  },
  {
    "text": "and some- some covariance.",
    "start": "4556390",
    "end": "4561900"
  },
  {
    "text": "In Gaussian processes, what we instead have is a function f",
    "start": "4564290",
    "end": "4570280"
  },
  {
    "text": "distributed according to what you call as a GP with a mean function m",
    "start": "4574940",
    "end": "4583770"
  },
  {
    "text": "and a",
    "start": "4583770",
    "end": "4592620"
  },
  {
    "text": "kernel k. Okay? So this is, you know,",
    "start": "4592620",
    "end": "4599430"
  },
  {
    "text": "a direct analogy between multivariate Gaussian and Gaussian processes.",
    "start": "4599430",
    "end": "4604815"
  },
  {
    "text": "In multivariate Gaussian, we had a vector which had a corresponding mean vector and a covariance matrix.",
    "start": "4604815",
    "end": "4612735"
  },
  {
    "text": "In a Gaussian process, we have functions distributed according to a mean function and some kernel, right?",
    "start": "4612735",
    "end": "4621599"
  },
  {
    "text": "So there is a direct one-to-one mapping between positive definite matrices and kernel functions.",
    "start": "4621600",
    "end": "4629145"
  },
  {
    "text": "Okay? The- and here,",
    "start": "4629145",
    "end": "4634575"
  },
  {
    "text": "you are now going to use, the properties, you know, one, two, three, four, to derive Gaussian processes for machine learning or",
    "start": "4634575",
    "end": "4642630"
  },
  {
    "text": "Gaussian process regression in a pretty straightforward way. [NOISE] Okay?",
    "start": "4642630",
    "end": "4651030"
  },
  {
    "text": "Now, as you might remember, um, in vectors, we index them with some index.",
    "start": "4651030",
    "end": "4663435"
  },
  {
    "text": "And the corresponding thing to do with functions is to evaluate a function at some input, right?",
    "start": "4663435",
    "end": "4670170"
  },
  {
    "text": "So you can think of this as a vector, you know, which is basically set of all evaluated function.",
    "start": "4670170",
    "end": "4681355"
  },
  {
    "text": "f of x1, f of x2,",
    "start": "4681355",
    "end": "4686520"
  },
  {
    "text": "basically every possible input for that function will have a position,",
    "start": "4686520",
    "end": "4691739"
  },
  {
    "text": "you know, that's a mental picture to have. That's not the right definition, but it's a very good mental picture to have. And the value eva- evaluated by the function is",
    "start": "4691739",
    "end": "4699990"
  },
  {
    "text": "the value of this infinitely long vector at that position, right? So where you had indices, you have inputs.",
    "start": "4699990",
    "end": "4708150"
  },
  {
    "text": "Okay? And now we are going to use the conditioning property and the marginalization property of um,",
    "start": "4708150",
    "end": "4719490"
  },
  {
    "text": "of- of- of Gaussian, ah, Gaussian vectors. And limit ourselves to just those elements that exist in our training set.",
    "start": "4719490",
    "end": "4733395"
  },
  {
    "text": "And to the test example that you want to make a prediction on.",
    "start": "4733395",
    "end": "4738790"
  },
  {
    "text": "Right? So this means, if we [NOISE] marginalize",
    "start": "4739250",
    "end": "4749685"
  },
  {
    "text": "out all irrelevant examples.",
    "start": "4749685",
    "end": "4756960"
  },
  {
    "text": "[NOISE] So basically, examples",
    "start": "4756960",
    "end": "4766725"
  },
  {
    "text": "not in training and",
    "start": "4766725",
    "end": "4773460"
  },
  {
    "text": "[NOISE] not in test, right?",
    "start": "4773460",
    "end": "4778469"
  },
  {
    "text": "Then we basically get something that looks like this.",
    "start": "4778470",
    "end": "4784350"
  },
  {
    "text": "f of x1, f of xn,",
    "start": "4784350",
    "end": "4795280"
  },
  {
    "text": "and then let's call f of x_star 1",
    "start": "4796160",
    "end": "4802110"
  },
  {
    "text": "until f of x_star n star.",
    "start": "4802110",
    "end": "4810070"
  },
  {
    "text": "Right? The set of all examples in our training set and the set of all examples in our test set.",
    "start": "4810440",
    "end": "4816570"
  },
  {
    "text": "[NOISE]",
    "start": "4816570",
    "end": "4828750"
  },
  {
    "text": "And we are going to assume that the Gaussian process has a mean 0.",
    "start": "4828750",
    "end": "4834670"
  },
  {
    "text": "So most of the times we assume the mean function to be just [NOISE] a 0",
    "start": "4836530",
    "end": "4846949"
  },
  {
    "text": "function times",
    "start": "4846950",
    "end": "4854815"
  },
  {
    "text": "the kernel function evaluated, right?",
    "start": "4854815",
    "end": "4859844"
  },
  {
    "text": "The kernel function evaluated on the set of these examples. So this gives us the kernel matrix,",
    "start": "4859845",
    "end": "4865095"
  },
  {
    "text": "just like how we saw in the case of- of, um, the Mercer's theorem, that the, uh,",
    "start": "4865095",
    "end": "4870720"
  },
  {
    "text": "you take- you take a kernel function and evaluate it on a set of points to construct a kernel matrix,",
    "start": "4870720",
    "end": "4877230"
  },
  {
    "text": "then that kernel matrix is positive definite. Similarly, here if- evaluate k of,",
    "start": "4877230",
    "end": "4883409"
  },
  {
    "text": "in this case x^1, x^1 and so on,",
    "start": "4883410",
    "end": "4890954"
  },
  {
    "text": "k of x_star 1- or n^star.",
    "start": "4890955",
    "end": "4899470"
  },
  {
    "text": "Let me write this more clearly. K of x^1, right?",
    "start": "4902270",
    "end": "4913060"
  },
  {
    "text": "And Mercer's theorem tells us that this will be",
    "start": "4915470",
    "end": "4920610"
  },
  {
    "text": "positive semidefinite and so this is a valid,",
    "start": "4920610",
    "end": "4929400"
  },
  {
    "text": "um- um, multivariate Gaussian distribution. Yes question?",
    "start": "4929400",
    "end": "4935100"
  },
  {
    "text": "[inaudible]",
    "start": "4935100",
    "end": "4945570"
  },
  {
    "text": "No, it need not. [inaudible] - So- so is- is, uh, should each of these be non-zero?",
    "start": "4945570",
    "end": "4951795"
  },
  {
    "text": "No, they need not be non-zero. They can be positive. [inaudible]",
    "start": "4951795",
    "end": "4957060"
  },
  {
    "text": "No, they can be negative, some elements can be negative. If you have- [OVERLAPPING].",
    "start": "4957060",
    "end": "4963450"
  },
  {
    "text": "[inaudible] Yes, every submatrix has to be positive definite. So what is the meaning of submatrix? That's a good question. So if you have a matrix,",
    "start": "4963450",
    "end": "4970060"
  },
  {
    "text": "let's say it has n rows and n columns. Now, choose a few index values,",
    "start": "4970430",
    "end": "4977730"
  },
  {
    "text": "let's say 2 and 7, right? And extract how's- rows 2 and 7,",
    "start": "4977730",
    "end": "4987060"
  },
  {
    "text": "intersected with column 2 and 7. These four make a submatrix. So in order to reduce it to a- so a- a submatrix of size 1,",
    "start": "4987060",
    "end": "4996450"
  },
  {
    "text": "that can only be the diagonal values, right? So row two and column seven will not give you a submatrix.",
    "start": "4996450",
    "end": "5004400"
  },
  {
    "text": "So row 2 and column 2, that element is a submatrix. But then, everything along the diagonal has to be greater than [inaudible]",
    "start": "5004400",
    "end": "5012410"
  },
  {
    "text": "Yeah- yeah. So k of- so if you evaluate k on the same element in the two places,",
    "start": "5012410",
    "end": "5017420"
  },
  {
    "text": "that will always be non-negative. As a submatrix doesn't mean any arbitrary block.",
    "start": "5017420",
    "end": "5023780"
  },
  {
    "text": "[inaudible] Yeah. Yeah. Yeah. So submatrix means, um, means a matrix that you get by choosing",
    "start": "5023780",
    "end": "5031460"
  },
  {
    "text": "the same set of indices for rows and columns, and good question.",
    "start": "5031460",
    "end": "5037520"
  },
  {
    "text": "Thank you. All right?",
    "start": "5037520",
    "end": "5042920"
  },
  {
    "text": "Now, we will represent this in a more compact way where we'll write f to be the examples of the training set",
    "start": "5042920",
    "end": "5053465"
  },
  {
    "text": "and f_star as the function evaluation at the test set equal to normal of 0,",
    "start": "5053465",
    "end": "5066260"
  },
  {
    "text": "and we'll call this block to be k of x x,",
    "start": "5066260",
    "end": "5075769"
  },
  {
    "text": "this block to be, k of x, x_star.",
    "start": "5075770",
    "end": "5081905"
  },
  {
    "text": "Then I guess we've been writing star as a prefix- as a subscript. Then again, k of x_star and x and k of x_star x_star.",
    "start": "5081905",
    "end": "5096150"
  },
  {
    "text": "Again, so this is the same thing that's written about where we are,",
    "start": "5101670",
    "end": "5107660"
  },
  {
    "text": "um- um, looking at just those- those examples that are of our immediate concern.",
    "start": "5107660",
    "end": "5114050"
  },
  {
    "text": "So examples in the training set and examples in the test set and this is like, um- um,",
    "start": "5114050",
    "end": "5119165"
  },
  {
    "text": "you're marginalizing out all the other infinitely many examples that,",
    "start": "5119165",
    "end": "5124700"
  },
  {
    "text": "you know, are not of immediate concern. Yes question? [inaudible]",
    "start": "5124700",
    "end": "5135310"
  },
  {
    "text": "So kernel, uh, so the question is, do kernels act as some kind of a similarity metric in general,",
    "start": "5135310",
    "end": "5141820"
  },
  {
    "text": "or is it only in the case of Gaussian processes? The answer is, kernels always act as some kind of a similarity metric and, uh,",
    "start": "5141820",
    "end": "5149150"
  },
  {
    "text": "you can use any kernel and come up with a covariance matrix for a Gaussian process.",
    "start": "5149150",
    "end": "5155015"
  },
  {
    "text": "There's no limitation of which kernels you can or cannot use as long as it's a valid kernel that can serve as a covariance function for a co- for a covariance matrix- for a Gaussian process.",
    "start": "5155015",
    "end": "5163969"
  },
  {
    "text": "[inaudible].",
    "start": "5163970",
    "end": "5169610"
  },
  {
    "text": "Exactly, the reason is because a kernel can be interpreted as a- a dot product in some higher dimensional feature space,",
    "start": "5169610",
    "end": "5176240"
  },
  {
    "text": "and dot products are a good, you know, similarity metric, so to speak. Yeah. All right? Was there another question?",
    "start": "5176240",
    "end": "5185159"
  },
  {
    "text": "Right. So we started with an infinite dimensional functional space and then marginalized out",
    "start": "5185620",
    "end": "5194420"
  },
  {
    "text": "every example that is not of immediate concern to reduce ourselves to a Gaussian- Gaussian distribution- a multivariate Gaussian distribution,",
    "start": "5194420",
    "end": "5203510"
  },
  {
    "text": "or the set of training and test examples. All right? [NOISE] and",
    "start": "5203510",
    "end": "5215480"
  },
  {
    "text": "with that, we're almost there. [NOISE]",
    "start": "5215480",
    "end": "5233060"
  },
  {
    "text": "Once we are in vector land from process land all our properties that we just reviewed cannot be directly applied.",
    "start": "5233060",
    "end": "5243755"
  },
  {
    "text": "So these properties that we covered, conditioning, modular, conditioning especially, this is, you know,",
    "start": "5243755",
    "end": "5249590"
  },
  {
    "text": "you need to be able to invert a matrix, right? So this- this is- we are- we are dealing with finite dimensions here. [NOISE]",
    "start": "5249590",
    "end": "5262810"
  },
  {
    "text": "Right? So, um, we got the f vector to be this, and in general, our y vector,",
    "start": "5262810",
    "end": "5270130"
  },
  {
    "text": "our y we saw was f of x plus noise.",
    "start": "5270130",
    "end": "5281179"
  },
  {
    "text": "But it so happened conveniently that the noise was also Gaussian discriminant- Gaussian- followed a Gaussian distribution,",
    "start": "5281210",
    "end": "5289955"
  },
  {
    "text": "and we're gonna use the summation property to add these two, right? So we get y,",
    "start": "5289955",
    "end": "5296380"
  },
  {
    "text": "y star equals f,",
    "start": "5296380",
    "end": "5304270"
  },
  {
    "text": "f star plus Epsilon, Epsilon star,",
    "start": "5304270",
    "end": "5312085"
  },
  {
    "text": "and this is now distributed as normal distribution where the mean of f was 0,",
    "start": "5312085",
    "end": "5323470"
  },
  {
    "text": "the means of our errors were 0. So we are still at 0 here.",
    "start": "5323470",
    "end": "5329949"
  },
  {
    "text": "The covariance matrix is now k of x,",
    "start": "5329950",
    "end": "5335020"
  },
  {
    "text": "x plus Sigma square I, k of x, x star,",
    "start": "5335020",
    "end": "5344250"
  },
  {
    "text": "k of x, x star, and k of x star,",
    "start": "5344250",
    "end": "5351190"
  },
  {
    "text": "x star plus Sigma squared I.",
    "start": "5351190",
    "end": "5355250"
  },
  {
    "text": "Right? Any questions on this? So f followed this distribution",
    "start": "5356490",
    "end": "5363490"
  },
  {
    "text": "by marginalizing out all the examples that were not of immediate concern.",
    "start": "5363490",
    "end": "5369310"
  },
  {
    "text": "And by our noise hypotheses that we have Gaussian noise in our observation.",
    "start": "5369310",
    "end": "5378760"
  },
  {
    "text": "We get a distribution of what the y's to be x plus, um, the noise,",
    "start": "5378760",
    "end": "5385599"
  },
  {
    "text": "and over here we're just using the summation property and adding up the covariances, um,",
    "start": "5385600",
    "end": "5391360"
  },
  {
    "text": "Epsilon- the entire Epsilon, um, vector follows normal 0 and Sigma squared I.",
    "start": "5391360",
    "end": "5400930"
  },
  {
    "text": "Right? So you just add it here, it only adds along the diagonal, uh, and this is the- this is the matrix that we're gonna now work- work with, okay?",
    "start": "5400930",
    "end": "5411415"
  },
  {
    "text": "And given this matrix, we are just one step away from- from,",
    "start": "5411415",
    "end": "5417835"
  },
  {
    "text": "um, finishing up Gaussian processes. So the probability of y star given",
    "start": "5417835",
    "end": "5430045"
  },
  {
    "text": "y and x star and",
    "start": "5430045",
    "end": "5435175"
  },
  {
    "text": "x. X and x star is kind of implicit in- in the indices that we've chosen.",
    "start": "5435175",
    "end": "5441260"
  },
  {
    "text": "Is now, or another way to put it",
    "start": "5442560",
    "end": "5448780"
  },
  {
    "text": "is y star given y,",
    "start": "5448780",
    "end": "5456369"
  },
  {
    "text": "x, x star, distributed according to a normal distribution with some mean Mu,",
    "start": "5456370",
    "end": "5463840"
  },
  {
    "text": "call it Mu star, and some covariance Sigma star, [NOISE] where this Mu and Sigma star are calculated according to this conditioning rule.",
    "start": "5463840",
    "end": "5474489"
  },
  {
    "text": "We apply the conditioning rule to this mean and this covariance,",
    "start": "5474490",
    "end": "5480955"
  },
  {
    "text": "and we get Mu star equals k of x star,",
    "start": "5480955",
    "end": "5495460"
  },
  {
    "text": "x times k of x,",
    "start": "5495460",
    "end": "5501565"
  },
  {
    "text": "x plus Sigma square I inverse",
    "start": "5501565",
    "end": "5509275"
  },
  {
    "text": "y Sigma star equals k of x star,",
    "start": "5509275",
    "end": "5519370"
  },
  {
    "text": "x star plus Sigma square I minus k of x star,",
    "start": "5519370",
    "end": "5529790"
  },
  {
    "text": "x, this is times, x,",
    "start": "5532410",
    "end": "5539545"
  },
  {
    "text": "x plus Sigma square inverse k of x, x star.",
    "start": "5539545",
    "end": "5549429"
  },
  {
    "text": "This is, uh, a complex-looking expression, but this is exactly the same as this,",
    "start": "5549430",
    "end": "5555550"
  },
  {
    "text": "where we plug in k, k, x, x in place of, uh, Sigma AA, k, x star,",
    "start": "5555550",
    "end": "5562270"
  },
  {
    "text": "x star in place of Sigma BB and k of, uh, k- or rather k of x,",
    "start": "5562270",
    "end": "5569050"
  },
  {
    "text": "x plus Sigma I square in place of Sigma AA and so on. Right? We're just applying the conditioning rule",
    "start": "5569050",
    "end": "5574750"
  },
  {
    "text": "and we got our posterior predictive distribution.",
    "start": "5574750",
    "end": "5579020"
  },
  {
    "text": "This is the posterior predictive distribution of Gaussian processes. Yes, question.",
    "start": "5581930",
    "end": "5587320"
  },
  {
    "text": "[inaudible] know about the slogan of the covariance [inaudible]? Yeah, so over here, um,",
    "start": "5587320",
    "end": "5594595"
  },
  {
    "text": "this is just a short- shorthand notation for the kernel matrix you get for evaluating on that set of examples.",
    "start": "5594595",
    "end": "5602980"
  },
  {
    "text": "K- k is- so there's a one-to-one relation between kernels and covariances, right?",
    "start": "5602980",
    "end": "5608170"
  },
  {
    "text": "Covariance functions and kernel functions, there's a one-to-one relationship. Any kernel function can be used as a covariance function.",
    "start": "5608170",
    "end": "5616340"
  },
  {
    "text": "That's probably- that's probably the most important takeaway from this class, you know,",
    "start": "5616830",
    "end": "5622480"
  },
  {
    "text": "for, you know, in general, that kernel functions and covariance functions are- are essentially the same.",
    "start": "5622480",
    "end": "5627579"
  },
  {
    "text": "[NOISE] All right?",
    "start": "5627580",
    "end": "5634600"
  },
  {
    "text": "So this is the posterior predictive distribution for Gaussian processes and the expression looks pretty complex,",
    "start": "5634600",
    "end": "5641065"
  },
  {
    "text": "but everything here is straightforward linear algebra. Right? There is- we have a set of xs,",
    "start": "5641065",
    "end": "5646765"
  },
  {
    "text": "we have the set of x stars, um, and- and, you know, just compute this and you get",
    "start": "5646765",
    "end": "5652420"
  },
  {
    "text": "your posterior predictive distribution for the test examples. Nowhere here do we have y star,",
    "start": "5652420",
    "end": "5658675"
  },
  {
    "text": "that's important because we're, you know, making predictions about y star. We only have ys, xs, and x stars.",
    "start": "5658675",
    "end": "5665900"
  },
  {
    "text": "And given this, uh, in order to kind of,",
    "start": "5667320",
    "end": "5672355"
  },
  {
    "text": "uh, construct a good GP model, the most important decision step,",
    "start": "5672355",
    "end": "5680530"
  },
  {
    "text": "or rather the only decision step, is to come up with the right kernel function, or to come up with the right covariance matrix.",
    "start": "5680530",
    "end": "5687850"
  },
  {
    "text": "Right? Once you come up with the right covariance matrix, there's- everything else is pretty much set in stone, right?",
    "start": "5687850",
    "end": "5695125"
  },
  {
    "text": "Everything else just flows along once you decide- once you choose what your kernel matrix is. There is no loss function,",
    "start": "5695125",
    "end": "5700480"
  },
  {
    "text": "there is no gradient descent, there is no, you know, fitting of models. All that we do is given the set of xs and ys in our training set,",
    "start": "5700480",
    "end": "5708625"
  },
  {
    "text": "we just remember them, choose a kernel- kernel function, and then wait for test examples.",
    "start": "5708625",
    "end": "5714370"
  },
  {
    "text": "Once we get our first test example, we plug it into this expression and obtain our predictive distribution.",
    "start": "5714370",
    "end": "5721850"
  },
  {
    "text": "Right? And because we covered this in SVMs as well, and because this is a kernel-based method,",
    "start": "5723120",
    "end": "5730600"
  },
  {
    "text": "a consequence of that is that we need to remember our training set all the way into test time, right?",
    "start": "5730600",
    "end": "5737590"
  },
  {
    "text": "And that- that is basically showing up here because you need to have the xs and ys from your training set in order to make the-,",
    "start": "5737590",
    "end": "5743920"
  },
  {
    "text": "you know, the, uh, prediction. Yes, question?",
    "start": "5743920",
    "end": "5750489"
  },
  {
    "text": "[inaudible] what we get is very similar to the generative model.",
    "start": "5750490",
    "end": "5755860"
  },
  {
    "text": "[NOISE] So the, uh, the result, what we get here is similar to a generative model.",
    "start": "5755860",
    "end": "5762130"
  },
  {
    "text": "Can- can you expand on that a little bit? [inaudible]. Yeah. [inaudible]",
    "start": "5762130",
    "end": "5776650"
  },
  {
    "text": "Yeah. The- the fact that the posterior ended up being a Gaussian, is actually a property of Gaussians, right?",
    "start": "5776650",
    "end": "5784239"
  },
  {
    "text": "We saw that when you- when you start with a- a- a distribution that is Gaussian and you condition on a few variables,",
    "start": "5784240",
    "end": "5791005"
  },
  {
    "text": "what you end up with is also Gaussian. So that's a property that is kind of unique to Gaussian, where you start with a multivariate Gaussian,",
    "start": "5791005",
    "end": "5798119"
  },
  {
    "text": "you marginalize out a few of them, you still end up with a Gaussian, with the smaller dimension- in a smaller dimensional space.",
    "start": "5798120",
    "end": "5803205"
  },
  {
    "text": "You start with a Gaussian vector, you condition on a few of them, you still get a Gaussian with what's remaining.",
    "start": "5803205",
    "end": "5809165"
  },
  {
    "text": "Right? And- and that's why the posterior also is now- is now Gaussian. Because we condition on the- the ys that we- that we observed in the training set.",
    "start": "5809165",
    "end": "5819145"
  },
  {
    "text": "Why is it called non-parametric example? The reason why this is called non-parametric is because we did not come",
    "start": "5819145",
    "end": "5826810"
  },
  {
    "text": "up with any kind of a functional form for our prediction function.",
    "start": "5826810",
    "end": "5831850"
  },
  {
    "text": "Isn't that a function form, Mu star and Sigma star? So Mu star and Sigma star are-,",
    "start": "5831850",
    "end": "5838860"
  },
  {
    "text": "well, you can- you can- you can think of them as a functional form, but what you will- what you will realize is that there is- there is",
    "start": "5838860",
    "end": "5847105"
  },
  {
    "text": "a- there was no fundamental limit on what these values could have been.",
    "start": "5847105",
    "end": "5852340"
  },
  {
    "text": "So next I'm going to show some visualization that will probably give you a better- better, uh, uh, better hint of what- why this is called non-parametric.",
    "start": "5852340",
    "end": "5860534"
  },
  {
    "text": "Maybe- maybe that might answer the question. Let's- let's move onto some visualization. [NOISE] Any other questions on this? Yes, question?",
    "start": "5860535",
    "end": "5870250"
  },
  {
    "text": "Can we do the same thing for other exponentials families? Can we do the same thing for other exponential families? Great- great question.",
    "start": "5870250",
    "end": "5877225"
  },
  {
    "text": "So- so Gaussian processes is what's also a- is a type of a stochastic process.",
    "start": "5877225",
    "end": "5883720"
  },
  {
    "text": "And if you're, uh, if you're familiar with stochastic processes, stochastic process, you can think of them as, uh,",
    "start": "5883720",
    "end": "5889180"
  },
  {
    "text": "collection of random variables with- with some kind of an index set. And this is a- a Gaussian process is",
    "start": "5889180",
    "end": "5899530"
  },
  {
    "text": "essentially a collection of random variables where the index set is the domain of your function, right?",
    "start": "5899530",
    "end": "5906535"
  },
  {
    "text": "And the- in theory you could do the same, you know,",
    "start": "5906535",
    "end": "5913090"
  },
  {
    "text": "go through the same exercise for any other distribution, uh, other than- other than Gaussians,",
    "start": "5913090",
    "end": "5919660"
  },
  {
    "text": "but the other distributions may not have, uh, first of all, they may not have multivariate versions.",
    "start": "5919660",
    "end": "5926215"
  },
  {
    "text": "So for example, a Poisson distribution, uh, you know, there is no multivariate Poisson.",
    "start": "5926215",
    "end": "5932395"
  },
  {
    "text": "I mean, you can have a collection of, uh, uh, Poisson, but there's no multivariate version of it. You- you can still have, uh, for example,",
    "start": "5932395",
    "end": "5939625"
  },
  {
    "text": "uh, you still do have a Poisson process, but they don't have this- the nice property is that if you condition on something,",
    "start": "5939625",
    "end": "5949389"
  },
  {
    "text": "what remains also is a Poisson process, right? But that's true only for Gaussian processes.",
    "start": "5949390",
    "end": "5954475"
  },
  {
    "text": "That if you condition on something what remains is still Gaussian. If you marginalize out something, what remains is still Gaussian. [inaudible] actually made two same processes exponentials family.",
    "start": "5954475",
    "end": "5964810"
  },
  {
    "text": "Then [inaudible].",
    "start": "5964810",
    "end": "5970370"
  },
  {
    "text": "So, so I guess the question is, um, now what if your data is distributed according to some exponential family, right?",
    "start": "5970370",
    "end": "5977660"
  },
  {
    "text": "And- they're the most common uh, the common approach uh, that you'll see is the f over here.",
    "start": "5977660",
    "end": "5985625"
  },
  {
    "text": "Instead of f being y directly, so we assumed uh, y is equal to f plus Epsilon, right?",
    "start": "5985625",
    "end": "5995435"
  },
  {
    "text": "The most common thing to do is what- uh, there's gonna be some kind of- um,",
    "start": "5995435",
    "end": "6002515"
  },
  {
    "text": "you can call it an activation function or the g function that- that we use in GLMs.",
    "start": "6002515",
    "end": "6008695"
  },
  {
    "text": "And you can have a natural parameter be distributed according to Gaussian. You can- you can do something like this.",
    "start": "6008695",
    "end": "6015460"
  },
  {
    "text": "And your observations, y will now go through this nonlinear transformation um,",
    "start": "6015460",
    "end": "6022060"
  },
  {
    "text": "to- to make the actual observations. But once you do this, you lose the conditioning ability, right?",
    "start": "6022060",
    "end": "6030460"
  },
  {
    "text": "In Gaussian processes, because the noise was additive and the noise was also Gaussian,",
    "start": "6030460",
    "end": "6037495"
  },
  {
    "text": "we got the y's in a Gaussian form again, right? But if this is non-linear,",
    "start": "6037495",
    "end": "6043795"
  },
  {
    "text": "if it's not a simple summation of two Gaussian's, your y's will no longer be normally distributed and you- you cannot do the conditioning and so on.",
    "start": "6043795",
    "end": "6051475"
  },
  {
    "text": "So you- you can do it in principle, but you don't get these simple expressions. Next question?",
    "start": "6051475",
    "end": "6060650"
  },
  {
    "text": "So the question is, do these properties, do they hold for other exponential families?",
    "start": "6075450",
    "end": "6081775"
  },
  {
    "text": "Yes, so these properties are very unique to the Gaussian, which is what makes Gaussian processes kind of nice to work with, right?",
    "start": "6081775",
    "end": "6090640"
  },
  {
    "text": "And you may have actually also come across, you know, especially if you have some kind of a background in finance,",
    "start": "6090640",
    "end": "6095980"
  },
  {
    "text": "you may have come across uh, like Brownian motion and Wiener processes. And basically uh, uh,",
    "start": "6095980",
    "end": "6103480"
  },
  {
    "text": "the way to connect back with, with what we saw today is a Wiener process or Brownian motion,",
    "start": "6103480",
    "end": "6111650"
  },
  {
    "text": "is a GP, is a Gaussian process with k of say, s,",
    "start": "6112470",
    "end": "6118750"
  },
  {
    "text": "t equal to just min of s, t. For this particular choice of a kernel,",
    "start": "6118750",
    "end": "6125080"
  },
  {
    "text": "you get the Wiener process. If you don't know what a Wiener process you don't have to worry about. If you're, you know, if you have some background in finance and you",
    "start": "6125080",
    "end": "6131740"
  },
  {
    "text": "know- you know what's Brownian motion or Wiener process. It relates to what we saw today by this particular choice of a kernel function.",
    "start": "6131740",
    "end": "6139450"
  },
  {
    "text": "All right, let's do some visualization. Where's my browser? There you go, all right,",
    "start": "6139450",
    "end": "6146605"
  },
  {
    "text": "so this is, um,",
    "start": "6146605",
    "end": "6154870"
  },
  {
    "text": "I see another cursor here I don't know. [LAUGHTER] All right, so this is a webpage that  I've kind of preloaded.",
    "start": "6154870",
    "end": "6162655"
  },
  {
    "text": "It is from a website called distill.pub. I would highly encourage you to visit this website,",
    "start": "6162655",
    "end": "6168100"
  },
  {
    "text": "there are great articles explaining things about machine learning and I'm a big fan of it.",
    "start": "6168100",
    "end": "6173155"
  },
  {
    "text": "And you probably too should be one. Anyway, so this is they have some really cool visualization of Gaussian process.",
    "start": "6173155",
    "end": "6181000"
  },
  {
    "text": "And I'm going to scroll all the way down to the very last visualization.",
    "start": "6181000",
    "end": "6186385"
  },
  {
    "text": "Just pretty cool, oops. Yeah, over here. Okay, so this is some kind of a functional space and abstract functional space.",
    "start": "6186385",
    "end": "6196105"
  },
  {
    "text": "So the x-axis over here is our space of inputs. It need not necessarily be,",
    "start": "6196105",
    "end": "6201909"
  },
  {
    "text": "you know, just scalar real value. Think of it in some abstract space. And now, the moment we choose a kernel,",
    "start": "6201910",
    "end": "6209650"
  },
  {
    "text": "so there are three choices of kernels here, an RBF kernel, which is also called the Gaussian kernel,",
    "start": "6209650",
    "end": "6214855"
  },
  {
    "text": "a periodic kernel, and a linear kernel. So once we choose an RBF kernel, you know,",
    "start": "6214855",
    "end": "6220195"
  },
  {
    "text": "this defines a prior on our functional space, right? And these, these curves that we see here are samples of functions drawn",
    "start": "6220195",
    "end": "6228400"
  },
  {
    "text": "from this- from this Gaussian process prior, right? And once we have this, we can now",
    "start": "6228400",
    "end": "6235810"
  },
  {
    "text": "observe our training data and start conditioning on them. So- so if- if you click over here,",
    "start": "6235810",
    "end": "6241855"
  },
  {
    "text": "it's- this means we have no condition on this particular input in our training of having this particular y value.",
    "start": "6241855",
    "end": "6250255"
  },
  {
    "text": "And similarly, as you keep conditioning on things that we observe,",
    "start": "6250255",
    "end": "6255560"
  },
  {
    "text": "the posterior distribution of our GP, of our Gaussian process takes this form.",
    "start": "6259320",
    "end": "6266080"
  },
  {
    "text": "What this essentially means is for any test example that might have an x value over here.The GP,",
    "start": "6266080",
    "end": "6274870"
  },
  {
    "text": "if we marginalize out everything, everything, everything else except that single slice, right?",
    "start": "6274870",
    "end": "6282190"
  },
  {
    "text": "That's going to be our posterior predictive distribution for the new x star, Right?",
    "start": "6282190",
    "end": "6290380"
  },
  {
    "text": "And this basically tells for examples. For test examples in this region,",
    "start": "6290380",
    "end": "6296739"
  },
  {
    "text": "our posterior predictive distribution has a very small variance, it means we are very confident of what those values should be.",
    "start": "6296740",
    "end": "6303925"
  },
  {
    "text": "Whereas over here, in regions where we haven't seen much of training data,",
    "start": "6303925",
    "end": "6309130"
  },
  {
    "text": "the prediction value will be pretty, you know, pretty broad in the sense will have very high variance.",
    "start": "6309130",
    "end": "6315010"
  },
  {
    "text": "We are less confident about making predictions in this region where there weren't much training examples nearby.",
    "start": "6315010",
    "end": "6321175"
  },
  {
    "text": "As opposed to here where the prediction- the posterior distributions have very small variance, which means you're making very tight confident predictions. Next question?",
    "start": "6321175",
    "end": "6331910"
  },
  {
    "text": "So, uh, so back to the question. Does this look like over-fitting? Right? So, uh, so this is like",
    "start": "6341520",
    "end": "6348010"
  },
  {
    "text": "a fundamental trade-off between parametric and non-parametric models. Non-parametric models are extremely flexible in the sense,",
    "start": "6348010",
    "end": "6354400"
  },
  {
    "text": "if your data follows some pattern, your function, your posterior distribution in GPs will follow that pattern, right?",
    "start": "6354400",
    "end": "6361300"
  },
  {
    "text": "And there is a common criticism that non-parametric models can over-fit pretty- pretty easily.",
    "start": "6361300",
    "end": "6367480"
  },
  {
    "text": "But in- in cases where you have a reasonable amount of data, you know, non-parametric models tend to work pretty well.",
    "start": "6367480",
    "end": "6373870"
  },
  {
    "text": "This- this- this might look like over-fitting and I guess that's,",
    "start": "6373870",
    "end": "6379360"
  },
  {
    "text": "you know, as they say, it lies the- it lies in the eye of the beholder. You might call this over-fitting, but over-fitting is a,",
    "start": "6379360",
    "end": "6385600"
  },
  {
    "text": "is a loose concept in general, it's hard to kind of give a formal definition of what over-fitting means. And you- you could- you could call it over-fitting,",
    "start": "6385600",
    "end": "6393610"
  },
  {
    "text": "but, you know, um, [NOISE] for example, uh, if you use a different kernel, it would look like this.",
    "start": "6393610",
    "end": "6404695"
  },
  {
    "text": "Now, you unconditionally you get",
    "start": "6404695",
    "end": "6408530"
  },
  {
    "text": "this is the prior and these are samples from- the gray lines are samples from that GP prior.",
    "start": "6410490",
    "end": "6417400"
  },
  {
    "text": "And as we start observing training data, we're basically just conditioning. And these are the- the- the pink band is basically- represents,",
    "start": "6417400",
    "end": "6427060"
  },
  {
    "text": "uh, some, I guess 95% range of the posterior distribution. And without conditioning, it's like the 95% range of the prior distribution.",
    "start": "6427060",
    "end": "6436460"
  },
  {
    "text": "Or you could use a combination of the two- compose the two kernels and get something like this.",
    "start": "6437370",
    "end": "6444650"
  },
  {
    "text": "Yeah, so you can construct kernels by taking existing kernels and kind of composing them.",
    "start": "6447060",
    "end": "6454435"
  },
  {
    "text": "Um, you'll see some of that in your homework. Homework 2 we'll probably do some kernel composition.",
    "start": "6454435",
    "end": "6459715"
  },
  {
    "text": "As long- you know, any function that, that, that obeys the properties of, you know, a kernel is a kernel.",
    "start": "6459715",
    "end": "6466150"
  },
  {
    "text": "And you can take existing kernels and compose them with certain rules and still get palette kernels.",
    "start": "6466150",
    "end": "6471460"
  },
  {
    "text": "Right? So this- this is, I think of a pretty,",
    "start": "6471460",
    "end": "6476664"
  },
  {
    "text": "pretty intuitive and- and a nice visualization of a Gaussian process where you start with a prior,",
    "start": "6476665",
    "end": "6482440"
  },
  {
    "text": "the choice of the kernel gives you different priors, right? Just the choice of the kernel without observing any data.",
    "start": "6482440",
    "end": "6490324"
  },
  {
    "text": "Right? You choose different kernels, you get different priors. The choice of the kernel gives you a different GP prior.",
    "start": "6490325",
    "end": "6496270"
  },
  {
    "text": "And depending on the choice of the kernel that you've chosen, observing data gives you different kinds of posterior, GP posteriors.",
    "start": "6496270",
    "end": "6505284"
  },
  {
    "text": "Right? There's a GP posterior if you use a different card. And at test time,",
    "start": "6505285",
    "end": "6511525"
  },
  {
    "text": "to repeat, you know, your test example will lie on the horizontal axis at some point.",
    "start": "6511525",
    "end": "6517240"
  },
  {
    "text": "And you are predictive distribution is gonna be the- the vertical slice or- or,",
    "start": "6517240",
    "end": "6523935"
  },
  {
    "text": "you know, marginalize out everything else. And- and that's gonna be like the Gaussian distribution whose mean follows the red thick line.",
    "start": "6523935",
    "end": "6532525"
  },
  {
    "text": "And the- the- the-the- the borderlines kind of define the- the- the, uh, the 95% range of that Gaussian distribution? Yes, question.",
    "start": "6532525",
    "end": "6543500"
  },
  {
    "text": "Yes, so uh, uh, the question I guess is when do you use a Gaussian process, right?",
    "start": "6553230",
    "end": "6558840"
  },
  {
    "text": "And um, the, the answer is going to be pretty boring as for almost all answers in machine-learning,",
    "start": "6558840",
    "end": "6564404"
  },
  {
    "text": "see what works well on a cross-validation set. It- it sounds it- uh, and that is probably",
    "start": "6564404",
    "end": "6571030"
  },
  {
    "text": "the most pragmatic answer and that should be the answer you should always go with. Uh, always see any change that you do.",
    "start": "6571030",
    "end": "6577780"
  },
  {
    "text": "How well does the performance, uh, you know what- what kind of performance you get on a cross-validation set?",
    "start": "6577780",
    "end": "6584140"
  },
  {
    "text": "Yes, there's another question. Yeah, so the heat map on the right is showing, uh,",
    "start": "6584140",
    "end": "6591400"
  },
  {
    "text": "uh, this, like the kernel- kernel matrix. The kernel matrix that you get by, uh,",
    "start": "6591400",
    "end": "6598815"
  },
  {
    "text": "conditioning on, uh, uh, the covariance matrix that you get. So in the posterior predictive distribution, uh, we ended up with a mean and a covariance.",
    "start": "6598815",
    "end": "6606554"
  },
  {
    "text": "So this is like, uh, I think this is the posterior covariance. Yes, next question?",
    "start": "6606555",
    "end": "6612579"
  },
  {
    "text": "Yeah, so the question is, um, should the choice of kernel also be something that",
    "start": "6617280",
    "end": "6622900"
  },
  {
    "text": "works well on cross-validation? And the answer is yes. And there are also techniques with which you can learn a kernel.",
    "start": "6622900",
    "end": "6629845"
  },
  {
    "text": "We didn't cover it today, but the book that I refer to, the Gaussian process for machine learning, it gives you a good overview of how you can even learn kernels from data,",
    "start": "6629845",
    "end": "6638094"
  },
  {
    "text": "not just handcraft them. Yes, question. So can composition",
    "start": "6638095",
    "end": "6650770"
  },
  {
    "text": "of kernels be interpreted as a composition of two functions with two inputs? In general, no, I wouldn't think of it that way.",
    "start": "6650770",
    "end": "6659199"
  },
  {
    "text": "It's just that you get a different kernel. All right, that's about it for today.",
    "start": "6659200",
    "end": "6667490"
  }
]