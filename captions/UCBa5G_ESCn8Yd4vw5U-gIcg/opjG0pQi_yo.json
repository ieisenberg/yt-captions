[
  {
    "start": "0",
    "end": "223000"
  },
  {
    "text": "Hello and welcome to the section on constant predictors.",
    "start": "4310",
    "end": "9100"
  },
  {
    "text": "Now the idea of this section is to explore the simplest possible predictor.",
    "start": "11750",
    "end": "18340"
  },
  {
    "text": "That's the constant predictor. What constant means here is that, instead of g,",
    "start": "18340",
    "end": "24750"
  },
  {
    "text": "Theta of x, depending on x, g Theta of x totally ignores x.",
    "start": "24750",
    "end": "30900"
  },
  {
    "text": "It always returns the same y hat, the same prediction.",
    "start": "30900",
    "end": "36505"
  },
  {
    "text": "And so we'll just call that Theta. That will be the parameter that determines the predictor,",
    "start": "36505",
    "end": "44300"
  },
  {
    "text": "and the parameter therefore determines the prediction. And it will be a vector in Rm.",
    "start": "44300",
    "end": "50860"
  },
  {
    "text": "The purpose of looking at such a simple predictor is that it gives us an understanding of what the loss means.",
    "start": "51340",
    "end": "61250"
  },
  {
    "text": "And we will look at a number of different losses and be able to get some understanding of what it means to have a prediction that minimizes those particular losses.",
    "start": "61250",
    "end": "71730"
  },
  {
    "text": "Another way to think about what we're doing in this section is we're looking at a linear regression model where the features are the simplest possible features.",
    "start": "71840",
    "end": "79970"
  },
  {
    "text": "Every data element has just given feature Phi of u is equal to one.",
    "start": "79970",
    "end": "87025"
  },
  {
    "text": "Which is nice. It doesn't depend on u. And of course, we don't even need u.",
    "start": "87025",
    "end": "92990"
  },
  {
    "text": "How are we gonna do this?. Well, we're gonna use ERM empirical risk minimization to fit Theta to the data.",
    "start": "93380",
    "end": "100180"
  },
  {
    "text": "And because there's no sensitivity, we're going to have no need for regularization.",
    "start": "100180",
    "end": "107645"
  },
  {
    "text": "The predictor is completely insensitive. So as we'll see, different losses lead to different predictors.",
    "start": "107645",
    "end": "115780"
  },
  {
    "text": "So we're gonna have data, y_1 through y_n. Each one of those y's will be either a scalar or a vector in Rm,",
    "start": "119330",
    "end": "129820"
  },
  {
    "text": "and we'll have a loss function which will take two y's,",
    "start": "129820",
    "end": "136325"
  },
  {
    "text": "a y hat and y, and return us a real number. Um, and",
    "start": "136325",
    "end": "141180"
  },
  {
    "text": "I think in every one of the predictors we analyze today,",
    "start": "144370",
    "end": "149735"
  },
  {
    "text": "we're going to have m is equal to 1. So all of the y would in fact be a scalar. Let me write that on the slide, m is equal to 1.",
    "start": "149735",
    "end": "158630"
  },
  {
    "text": "So our loss function L of y hat, y quantifies how bad the y hat approximates y.",
    "start": "158800",
    "end": "167450"
  },
  {
    "text": "And we've seen a few different losses so far in this course.",
    "start": "167450",
    "end": "173224"
  },
  {
    "text": "Um, so for scalar, y, we've seen the quadratic loss, which is y hat minus y squared.",
    "start": "173225",
    "end": "180705"
  },
  {
    "text": "We've seen the absolute loss, which is the absolute value of y hat minus y.",
    "start": "180705",
    "end": "185920"
  },
  {
    "text": "We've seen the fractional loss, which is the max of y hat over y minus 1 and y of y hat minus 1.",
    "start": "185920",
    "end": "195835"
  },
  {
    "text": "And that's the percentage error once we scale it by 100.",
    "start": "195835",
    "end": "201095"
  },
  {
    "text": "Of course, the fractional loss only applies when both y and y hat are positive numbers.",
    "start": "201095",
    "end": "207720"
  },
  {
    "text": "And if we were looking at m greater than 1, where we had a vector y, we might look at the quadratic loss as the norm squared of y hat minus y.",
    "start": "209200",
    "end": "220470"
  },
  {
    "start": "223000",
    "end": "953000"
  },
  {
    "text": "So we're going to choose now a predictor with no data.",
    "start": "224150",
    "end": "229504"
  },
  {
    "text": "So our predictor just returns Theta, and I'm gonna choose the Theta to minimize the empirical risk,",
    "start": "229505",
    "end": "235865"
  },
  {
    "text": "and that's going to be the average of the loss of Theta and y_i.",
    "start": "235865",
    "end": "242170"
  },
  {
    "text": "Averaged over all the data elements y_i for i is 1, up to end.",
    "start": "242170",
    "end": "247870"
  },
  {
    "text": "And we'll-we'll solve it in these particular cases and in some other cases as well.",
    "start": "248030",
    "end": "253265"
  },
  {
    "text": "And we'll see that the result that you get is actually very interpretable.",
    "start": "253265",
    "end": "260820"
  },
  {
    "text": "Now one of the features of losses that make them tractable and make them-make them-make it possible for",
    "start": "264260",
    "end": "273350"
  },
  {
    "text": "us to determine either analytically or numerically exactly what",
    "start": "273350",
    "end": "280190"
  },
  {
    "text": "the optimal theta is, is convexity.",
    "start": "280190",
    "end": "286130"
  },
  {
    "text": "So here we have a function f mapping from Rk to the real numbers.",
    "start": "286130",
    "end": "292745"
  },
  {
    "text": "So this is a real valued function in a k dimensional Euclidean space.",
    "start": "292745",
    "end": "297794"
  },
  {
    "text": "And such functions we're going to call it convex if they satisfy the following inequality.",
    "start": "297795",
    "end": "304960"
  },
  {
    "text": "And that is that for all points w and z in Rk, and for all Alpha in the interval of 0-1,",
    "start": "304960",
    "end": "313664"
  },
  {
    "text": "if we evaluate f at Alpha w plus 1 minus Alpha Z,",
    "start": "313665",
    "end": "320235"
  },
  {
    "text": "that's less than or equal to Alpha times f of w plus 1 minus alpha in f of z.",
    "start": "320235",
    "end": "328009"
  },
  {
    "text": "Now this inequality-this expression has a nice meaning. Let's look at it.",
    "start": "328010",
    "end": "336350"
  },
  {
    "text": "Here, we have two points, w and z, and they live in a k-dimensional space,",
    "start": "336350",
    "end": "345840"
  },
  {
    "text": "and what this-this expression does, Alpha w plus 1 minus Alpha z",
    "start": "345840",
    "end": "353490"
  },
  {
    "text": "is it's a parametrization of the line segment connecting z to w. And when alpha is 0,",
    "start": "353490",
    "end": "363959"
  },
  {
    "text": "well then, Alpha w plus 1 minus Alpha z is just z.",
    "start": "363959",
    "end": "370180"
  },
  {
    "text": "When Alpha is 1, it's just w. When Alpha's a quarter,",
    "start": "370400",
    "end": "380740"
  },
  {
    "text": "we find that Alpha w plus 1 minus Alpha z is exactly one quarter of",
    "start": "385010",
    "end": "390550"
  },
  {
    "text": "the way along the line from z to w. So this expression,",
    "start": "390550",
    "end": "396240"
  },
  {
    "text": "Alpha w plus 1 minus Alpha z is called a convex combination of w and z,",
    "start": "396240",
    "end": "401560"
  },
  {
    "text": "and it's simply a way of parameterizing the line segment between two points.",
    "start": "401560",
    "end": "407410"
  },
  {
    "text": "On the right-hand side of this expression, you will notice we have a very similar convex combination,",
    "start": "407410",
    "end": "415195"
  },
  {
    "text": "but this is of two real numbers, f of w and f of z. And so we're parameterizing the line segment in one dimension from f of w, f of z.",
    "start": "415195",
    "end": "426655"
  },
  {
    "text": "Now what this means is the following.",
    "start": "426655",
    "end": "431845"
  },
  {
    "text": "If my point w is say,",
    "start": "431845",
    "end": "437530"
  },
  {
    "text": "at 0.1 and my point in z is say,",
    "start": "437530",
    "end": "443850"
  },
  {
    "text": "at 0.8, well then f of w is somewhere over here,",
    "start": "443850",
    "end": "449750"
  },
  {
    "text": "and then f of z is somewhere over here. And this inequality means that this line segment",
    "start": "449840",
    "end": "462080"
  },
  {
    "text": "lies above the function.",
    "start": "463050",
    "end": "475130"
  },
  {
    "text": "And if for any two points that I pick z and w, the line segment joining",
    "start": "475460",
    "end": "482845"
  },
  {
    "text": "the corresponding points on the curve lies above the curve itself,",
    "start": "482845",
    "end": "492744"
  },
  {
    "text": "then such a function is called convex. And over here on the right, we see an example where that fails.",
    "start": "492744",
    "end": "500840"
  },
  {
    "text": "Here are two points. Here is the line segment.",
    "start": "500840",
    "end": "506685"
  },
  {
    "text": "We can see that it falls underneath the graph of the function.",
    "start": "506685",
    "end": "513539"
  },
  {
    "text": "And so this is not a convex function. So this means that the function has to curve upwards,",
    "start": "513540",
    "end": "523055"
  },
  {
    "text": "another way to say that is it  has positive curvature. It has to curve upwards everywhere.",
    "start": "523055",
    "end": "528960"
  },
  {
    "text": "And if we have a differentiable function, well, convexity can be expressed in terms of the curvature directly.",
    "start": "530530",
    "end": "539630"
  },
  {
    "text": "The curvature of a differentiable function is the second derivative,",
    "start": "539630",
    "end": "545119"
  },
  {
    "text": "and so this expression is exactly equivalent to the requirement that the second derivative of",
    "start": "545119",
    "end": "553220"
  },
  {
    "text": "f at all points w has to be greater than or equal to 0.",
    "start": "553220",
    "end": "558600"
  },
  {
    "text": "And then another way of characterizing it as well, and that is to look at the first derivative of the function f,",
    "start": "565090",
    "end": "573649"
  },
  {
    "text": "and a function is convex if and only if,",
    "start": "573650",
    "end": "578700"
  },
  {
    "text": "its first derivative is non-decreasing. In other words, as you increase w,",
    "start": "578700",
    "end": "584060"
  },
  {
    "text": "then the gradient of f can't decrease. That's true. Those two conditions for derivatives",
    "start": "584060",
    "end": "593000"
  },
  {
    "text": "are true for functions from the reals to the reals.",
    "start": "593000",
    "end": "599700"
  },
  {
    "text": "And in particular, the second condition, that the second derivative has to be non-negative can be",
    "start": "608190",
    "end": "614920"
  },
  {
    "text": "generalized in a straightforward way to functions on R_k, but we won't need that here.",
    "start": "614920",
    "end": "621650"
  },
  {
    "text": "It's also worth pointing out that the notion of convexity defined by",
    "start": "622140",
    "end": "627640"
  },
  {
    "text": "this inequality doesn't have any requirements that the function be differentiable,",
    "start": "627640",
    "end": "634540"
  },
  {
    "text": "and it's quite reasonable to look at functions which have kinks in",
    "start": "634540",
    "end": "642220"
  },
  {
    "text": "them and which are- so this is a function which is l- which is linear on a region,",
    "start": "642220",
    "end": "651204"
  },
  {
    "text": "linear on another region and then has a curve for the third part and that's still a convex function.",
    "start": "651205",
    "end": "659065"
  },
  {
    "text": "It's simply not differentiable at this point or at this point.",
    "start": "659065",
    "end": "665120"
  },
  {
    "text": "Now, convexity as it- is a very important property when one",
    "start": "674160",
    "end": "680529"
  },
  {
    "text": "is trying to solve minimization problems or more general optimization problems.",
    "start": "680530",
    "end": "686335"
  },
  {
    "text": "And this is for this following property. And that, if I've got a differentiable function,",
    "start": "686335",
    "end": "692769"
  },
  {
    "text": "then a point w is the minimum of that function if and only if the gradient of f at w is equal to 0.",
    "start": "692770",
    "end": "703495"
  },
  {
    "text": "And when you read this expression, it's tempting to confuse this with",
    "start": "703495",
    "end": "709510"
  },
  {
    "text": "similar expressions that you've seen before in your calculus class, where one is trying to find the minimum or",
    "start": "709510",
    "end": "715870"
  },
  {
    "text": "the maximum of a function and one looks for stationary points. Now, the trouble with stationary points is that they might be a minimum,",
    "start": "715870",
    "end": "724510"
  },
  {
    "text": "they might be a local minimum, they might be a local maximum,",
    "start": "724510",
    "end": "729625"
  },
  {
    "text": "or it might be a saddle point or something else in higher dimensions. Here, this is not just a local minimum,",
    "start": "729625",
    "end": "741180"
  },
  {
    "text": "this is a global minimum. This is the true meaning of the word minimize, not the colloquial meaning of the word minimize.",
    "start": "741180",
    "end": "749430"
  },
  {
    "text": "This is w that actually finds the global minimum of the function f,",
    "start": "749430",
    "end": "754440"
  },
  {
    "text": "and so one can find the global minimum simply by looking for points where the derivative is 0.",
    "start": "754440",
    "end": "760660"
  },
  {
    "text": "Now, for convex functions on the reals, so that is one-dimensional convex functions,",
    "start": "764130",
    "end": "772614"
  },
  {
    "text": "we can characterize explicitly conditions under which we have a minimum point.",
    "start": "772614",
    "end": "778495"
  },
  {
    "text": "Now lets look at that, and we plot such a function.",
    "start": "778495",
    "end": "784430"
  },
  {
    "text": "And here I have a function which has a point at which it's not differentiable,",
    "start": "788520",
    "end": "795730"
  },
  {
    "text": "but yet that point is clearly the minimum. Now I can simply take the derivative because the function is not differentiable there.",
    "start": "795730",
    "end": "803305"
  },
  {
    "text": "However, for a convex function, there's always a left-hand derivative and a right-hand derivative.",
    "start": "803305",
    "end": "810834"
  },
  {
    "text": "What that means is that if I look at this point w, there's a slope to the right and a slope to the left,",
    "start": "810835",
    "end": "819280"
  },
  {
    "text": "and this slope to the left is labeled f dash minus of w,",
    "start": "819280",
    "end": "825895"
  },
  {
    "text": "and this slope to the right is labeled f dash plus of w. They're defined in a way very similar to the way we usually define derivatives.",
    "start": "825895",
    "end": "836500"
  },
  {
    "text": "This left-hand slope is the limit as t",
    "start": "836500",
    "end": "841525"
  },
  {
    "text": "tends to 0 of the slope f of w plus t minus f of w divided by t,",
    "start": "841525",
    "end": "847315"
  },
  {
    "text": "but we're allowed to take the limit only over negative t. And similarly",
    "start": "847315",
    "end": "852490"
  },
  {
    "text": "the right-hand slope is a limit taken only over positive t. Now,",
    "start": "852490",
    "end": "858190"
  },
  {
    "text": "in terms of these two derivatives, the left-hand and the right-hand derivative,",
    "start": "858190",
    "end": "864610"
  },
  {
    "text": "w is a minimum if and only if left-hand derivative is less than or equal to",
    "start": "864610",
    "end": "871000"
  },
  {
    "text": "0 and the right-hand derivative is greater than or equal to 0.",
    "start": "871000",
    "end": "875630"
  },
  {
    "text": "Even if f is not differentiable, we will have both the left-hand derivative and the right hand derivative,",
    "start": "876320",
    "end": "882209"
  },
  {
    "text": "and so this is a condition we can use for any convex function f. If the function is derivative- if the function is differentiable,",
    "start": "882210",
    "end": "891475"
  },
  {
    "text": "well then its left-hand derivative and its right-hand derivative will both be the same.",
    "start": "891475",
    "end": "896829"
  },
  {
    "text": "The slope will be the same whether we approach the point from the left-hand side or from the right-hand side,",
    "start": "896830",
    "end": "904330"
  },
  {
    "text": "and both of these will equal the usual derivative. For our simple example,",
    "start": "904330",
    "end": "910660"
  },
  {
    "text": "we might look at the absolute value function.",
    "start": "910660",
    "end": "914690"
  },
  {
    "text": "The absolute value function looks like that and we",
    "start": "919680",
    "end": "925540"
  },
  {
    "text": "can see that if I look at the point w is equal to 0,",
    "start": "925540",
    "end": "931345"
  },
  {
    "text": "well then f dash minus of 0 is minus 1 and f dash plus 0 is 1.",
    "start": "931345",
    "end": "939894"
  },
  {
    "text": "Clearly the left-hand derivative is nega- is negative and the left- and the right-hand derivative is positive and therefore w is a minimum.",
    "start": "939895",
    "end": "948199"
  },
  {
    "start": "953000",
    "end": "1037000"
  },
  {
    "text": "Now for many other loss functions of interest in machine learning, the loss function itself is a convex function of the prediction y-hat.",
    "start": "954720",
    "end": "967000"
  },
  {
    "text": "Certainly the ones we saw on the previous two slides, all of those loss functions are convex in the prediction.",
    "start": "967000",
    "end": "975380"
  },
  {
    "text": "It's also true that if you take a convex function and another convex function and add them up,",
    "start": "975870",
    "end": "984445"
  },
  {
    "text": "well then the sum of those two convex functions is itself convex, and if I scale a convex function by a positive number,",
    "start": "984445",
    "end": "993279"
  },
  {
    "text": "well then I get another convex function. And as a result, I can take the average of all of",
    "start": "993280",
    "end": "999010"
  },
  {
    "text": "the convex functions in the empirical risk, L of Theta, and get another convex function.",
    "start": "999010",
    "end": "1007455"
  },
  {
    "text": "So the empirical risk is a convex function of Theta.",
    "start": "1007455",
    "end": "1012495"
  },
  {
    "text": "And so by using the optimality conditions on the previous slide, we can characterize exactly when Theta minimizes the empirical risk.",
    "start": "1012495",
    "end": "1022690"
  },
  {
    "text": "All we have to do is look at the left-hand derivative and the right-hand derivative and check whether they have",
    "start": "1022850",
    "end": "1029670"
  },
  {
    "text": "the appropriate sign and that will tell us when theta is a minimum.",
    "start": "1029670",
    "end": "1033400"
  },
  {
    "start": "1037000",
    "end": "1196000"
  },
  {
    "text": "So first let's look at the simplest case, the case of square loss. Of course this function is convex,",
    "start": "1038780",
    "end": "1045720"
  },
  {
    "text": "but it is also differentiable. Then the loss l of y-hat and y is the norm of y-hat minus y squared.",
    "start": "1045720",
    "end": "1055405"
  },
  {
    "text": "In the case of a scalar y and y-hat, that's just the square of y-hat minus y.",
    "start": "1055405",
    "end": "1061085"
  },
  {
    "text": "And empirical risk is just the mean-square error. 1 over n, the sum over i,",
    "start": "1061085",
    "end": "1066274"
  },
  {
    "text": "Theta minus y_i squared. Because here we have constant predictor g Theta of x is just equal to Theta.",
    "start": "1066275",
    "end": "1076575"
  },
  {
    "text": "Now this is a simple least squares problem. We can simply differentiate the objective with respect to Theta,",
    "start": "1076575",
    "end": "1083625"
  },
  {
    "text": "and we'll find that the optimal Theta is 1 on n times the sum over i of y_i,",
    "start": "1083625",
    "end": "1090150"
  },
  {
    "text": "which is the average of the y_i's.",
    "start": "1090150",
    "end": "1093580"
  },
  {
    "text": "This is, ah, the best constant predictor when we're using the square loss.",
    "start": "1096710",
    "end": "1102914"
  },
  {
    "text": "It's the average or the mean of the data. The resulting mean-square error is the variance of the data.",
    "start": "1102915",
    "end": "1112149"
  },
  {
    "text": "As an example, here I have [NOISE] a bunch of data points.",
    "start": "1117910",
    "end": "1125375"
  },
  {
    "text": "One here, one here, one here. And, ah, the mean of those data points is given by this red line,",
    "start": "1125375",
    "end": "1136890"
  },
  {
    "text": "which is about 1.12, something like that.",
    "start": "1136890",
    "end": "1142635"
  },
  {
    "text": "If I plot the loss function as a function of Theta, this is this curve here,",
    "start": "1142635",
    "end": "1150610"
  },
  {
    "text": "and the minimum of that curve is exactly at the mean.",
    "start": "1155000",
    "end": "1161200"
  },
  {
    "text": "This loss function is a sum of- is an average of",
    "start": "1163130",
    "end": "1168525"
  },
  {
    "text": "the square loss applied to each of those different points.",
    "start": "1168525",
    "end": "1174615"
  },
  {
    "text": "So we've actually constructed this loss function by taking one on n times the sum of 1, 2, 3,",
    "start": "1174615",
    "end": "1182490"
  },
  {
    "text": "4, 5, 6, 7 square functions, each of the form Theta minus y_i squared.",
    "start": "1182490",
    "end": "1191710"
  },
  {
    "start": "1196000",
    "end": "1332000"
  },
  {
    "text": "Now let's look at the absolute loss case. Here we have the- the loss function l of y-hat,",
    "start": "1197300",
    "end": "1205590"
  },
  {
    "text": "y is the absolute value of y-hat minus y. And so the empirical risk is the mean-absolute error.",
    "start": "1205590",
    "end": "1213520"
  },
  {
    "text": "Now for this, we've got, ah, an empirical risk, which is the average of a bunch of absolute value functions.",
    "start": "1213770",
    "end": "1225375"
  },
  {
    "text": "It's certainly a convex function because the loss,",
    "start": "1225375",
    "end": "1230880"
  },
  {
    "text": "the absolute value of y-hat minus y is convex in y-hat. It's piecewise linear, but it's not smooth.",
    "start": "1230880",
    "end": "1241440"
  },
  {
    "text": "It's not differentiable at every point. It has kink points at the data values.",
    "start": "1241440",
    "end": "1246909"
  },
  {
    "text": "And we will actually see that the Theta that optimizes this empirical risk,",
    "start": "1247100",
    "end": "1254280"
  },
  {
    "text": "the minimum- the Theta that minimizes the empirical risk is the median of the data.",
    "start": "1254280",
    "end": "1259950"
  },
  {
    "text": "And this is a very reasonable way of making an approximation of the data.",
    "start": "1259950",
    "end": "1265960"
  },
  {
    "text": "So here, it's the same set of data points. And what we can see is that this function is actually piecewise linear.",
    "start": "1266000",
    "end": "1276750"
  },
  {
    "text": "If we look at these points right here, these are the kink points.",
    "start": "1276750",
    "end": "1282480"
  },
  {
    "text": "Let me make those a little bigger. These are kink points in the graph.",
    "start": "1282480",
    "end": "1290505"
  },
  {
    "text": "Between these points, the graph is a straight line and at those points,",
    "start": "1290505",
    "end": "1296010"
  },
  {
    "text": "there's an abrupt change in the derivative of the function.",
    "start": "1296010",
    "end": "1301540"
  },
  {
    "text": "The- here we have 1, 2, 3, 4, 5, 6, 7 data points, and so if we sort those data points,",
    "start": "1301730",
    "end": "1311000"
  },
  {
    "text": "the median is the fourth data point. It has three points to the left and three points to the right,",
    "start": "1311000",
    "end": "1317165"
  },
  {
    "text": "and that gives us this median value right there, which is precisely the value which minimizes the empirical risk.",
    "start": "1317165",
    "end": "1328990"
  },
  {
    "start": "1332000",
    "end": "1385000"
  },
  {
    "text": "Let's take a look at this a little bit more closely. We want to actually,",
    "start": "1333950",
    "end": "1338955"
  },
  {
    "text": "first of all define the median. And that's not quite as straightforward as it- at first appears.",
    "start": "1338955",
    "end": "1346110"
  },
  {
    "text": "Um, if we have an odd number of data points,",
    "start": "1346110",
    "end": "1351870"
  },
  {
    "text": "well then the median is the middle point. If we have an even number of data points,",
    "start": "1351870",
    "end": "1358049"
  },
  {
    "text": "well then we have to allow ourselves the possibility that any to any point,",
    "start": "1358050",
    "end": "1366975"
  },
  {
    "text": "which is- which has half of the data points to the left and half of the data points to",
    "start": "1366975",
    "end": "1373080"
  },
  {
    "text": "the right might be reasonably considered a median. Let's first of all write that down mathematically and then,",
    "start": "1373080",
    "end": "1380040"
  },
  {
    "text": "um, do the analysis. So the simplest- let's look at an example first.",
    "start": "1380040",
    "end": "1390105"
  },
  {
    "text": "Suppose we have a data y_1 through y_n. Uh, if n is odd,",
    "start": "1390105",
    "end": "1398684"
  },
  {
    "text": "then the median is simply the middle point, which is y_n plus 1 over 2.",
    "start": "1398685",
    "end": "1404025"
  },
  {
    "text": "And that's completely well-defined. If n is even,",
    "start": "1404025",
    "end": "1409815"
  },
  {
    "text": "well then we say Theta is a median. If Theta is anywhere between y_n on 2 and y_n over 2 plus 1.",
    "start": "1409815",
    "end": "1418785"
  },
  {
    "text": "The median is not unique. So if I have these four data points, minus 3.3,",
    "start": "1418785",
    "end": "1430260"
  },
  {
    "text": "minus 1.7, 0.4 and- well, there's the three data points and the median is just the middle one,",
    "start": "1430260",
    "end": "1437580"
  },
  {
    "text": "which is minus 1.7. If I have four data points,",
    "start": "1437580",
    "end": "1442980"
  },
  {
    "text": "so I add a new data point at 4.9, well then the median is any number between minus 1.7 and 0.4.",
    "start": "1442980",
    "end": "1451750"
  },
  {
    "text": "Now to characterize the median precisely, we will define two quantities.",
    "start": "1458840",
    "end": "1465300"
  },
  {
    "text": "Uh, the first of which is going to be called n_1 and it's a function of",
    "start": "1465300",
    "end": "1470520"
  },
  {
    "text": "Theta and it is the number of data points strictly less than Theta.",
    "start": "1470520",
    "end": "1478500"
  },
  {
    "text": "The second one is n_2, also a function of Theta, it's the number of data points strictly greater than Theta.",
    "start": "1478500",
    "end": "1485430"
  },
  {
    "text": "And Theta is a median of the data if n_1 divided by n,",
    "start": "1485430",
    "end": "1491745"
  },
  {
    "text": "the fraction of data points strictly less than Theta is less than or equal to half.",
    "start": "1491745",
    "end": "1497670"
  },
  {
    "text": "And n_2 divided by n, the fraction of data points strictly greater",
    "start": "1497670",
    "end": "1503265"
  },
  {
    "text": "than Theta is also less than or equal to one-half. Now, if Theta is not equal to a data point,",
    "start": "1503265",
    "end": "1512265"
  },
  {
    "text": "then there are a number of data points strictly less than Theta.",
    "start": "1512265",
    "end": "1517920"
  },
  {
    "text": "And the number of data points strictly greater than Theta are actually related of course. They add up n_1 plus n_2 is going to add up to",
    "start": "1517920",
    "end": "1526020"
  },
  {
    "text": "n and so both of these conditions collapsed now to one condition, which is just that n_1 divided by n is a half.",
    "start": "1526020",
    "end": "1533760"
  },
  {
    "text": "The number of data points less than Theta is a half, implies that the number of data points greater than Theta is a half.",
    "start": "1533760",
    "end": "1541350"
  },
  {
    "text": "If Theta is equal to a data point, well then we need two conditions,",
    "start": "1541350",
    "end": "1547649"
  },
  {
    "text": "not just one condition to characterize the median. Because there may be a certain number of data points equal to the value of Theta.",
    "start": "1547650",
    "end": "1556450"
  },
  {
    "text": "Now we can use this characterization in order to show that the median or a median,",
    "start": "1561770",
    "end": "1569010"
  },
  {
    "text": "because the median is in general not unique, minimizes the empirical risk when we're using",
    "start": "1569010",
    "end": "1576299"
  },
  {
    "text": "the absolute loss and",
    "start": "1576300",
    "end": "1583200"
  },
  {
    "text": "it is the case that there may be more than one minimizer of the risk.",
    "start": "1583200",
    "end": "1590370"
  },
  {
    "text": "And at every one of those risks, those minimizers is a median. And conversely that you pick any median of the data",
    "start": "1590370",
    "end": "1599145"
  },
  {
    "text": "and that will be a minimizer of the risk.",
    "start": "1599145",
    "end": "1605280"
  },
  {
    "text": "How do we see this? Well, let's first of all, just assume that data is sorted.",
    "start": "1605280",
    "end": "1610740"
  },
  {
    "text": "So we'll have y_1 less than or equal to y_2 all the way up to y_ n. Of course that doesn't make any difference",
    "start": "1610740",
    "end": "1616754"
  },
  {
    "text": "to the problem, it doesn't change the empirical risk because the empirical risk is just the average of the loss evaluated at those points.",
    "start": "1616754",
    "end": "1623370"
  },
  {
    "text": "So we can order them any way we want to. Let's evaluate the empirical risk.",
    "start": "1623370",
    "end": "1630495"
  },
  {
    "text": "The empirical risk is the sum over the data of the absolute value of Theta minus y_i,",
    "start": "1630495",
    "end": "1639690"
  },
  {
    "text": "all divided by 1 on n. For those data points for which Theta is less than y_ i.",
    "start": "1639690",
    "end": "1650115"
  },
  {
    "text": "That is- the absolute value of Theta minus y_ i is minus Theta minus y_ i.",
    "start": "1650115",
    "end": "1655830"
  },
  {
    "text": "For those data points, where Theta is greater than y_i the absolute value of Theta minus y_i is just Theta minus y_i.",
    "start": "1655830",
    "end": "1664559"
  },
  {
    "text": "So we split up that sum into those two categories.",
    "start": "1664560",
    "end": "1668830"
  },
  {
    "text": "So first of all, we sum over the points for which y_ i is less than Theta.",
    "start": "1672290",
    "end": "1681825"
  },
  {
    "text": "Those are the first n_1 data points. And then we sum over the points for which y_i is greater than Theta.",
    "start": "1681825",
    "end": "1692475"
  },
  {
    "text": "And those are the last n_2 data points. There may be some other data points at which Theta is equal to y_i.",
    "start": "1692475",
    "end": "1701309"
  },
  {
    "text": "Those contribute 0 because the loss of those points is 0.",
    "start": "1701310",
    "end": "1706560"
  },
  {
    "text": "And so they don't enter into this sum. If there aren't any such data points,",
    "start": "1706560",
    "end": "1712530"
  },
  {
    "text": "theta is not equal to a data value. Well, we can differentiate this sum.",
    "start": "1712530",
    "end": "1719085"
  },
  {
    "text": "Differentiating this with respect to Theta is easy. We get n_1 on n minus n_2 on n. Um,",
    "start": "1719085",
    "end": "1729550"
  },
  {
    "text": "however, we actually need to find the minimum point. And it may be that the minimum point of this function;",
    "start": "1729550",
    "end": "1737510"
  },
  {
    "text": "the other feature is at a kink point, at a point where the function is not differentiable.",
    "start": "1737510",
    "end": "1744390"
  },
  {
    "text": "And that may or may not be the case. We may be in the case where the function looks like this.",
    "start": "1744390",
    "end": "1751420"
  },
  {
    "text": "In which case the minimum is any point here.",
    "start": "1752510",
    "end": "1760965"
  },
  {
    "text": "Or we may be in the case where the function has a kink in it,",
    "start": "1760965",
    "end": "1770370"
  },
  {
    "text": "in which case the minimum is there. Now if we're at a point- if we're at the case",
    "start": "1770370",
    "end": "1777510"
  },
  {
    "text": "where there's a kink in that function and the minimum is at the kink. Well, now we can simply differentiate L of Theta.",
    "start": "1777510",
    "end": "1788390"
  },
  {
    "text": "And so what we need to do is look at the left-hand and right-hand derivatives.",
    "start": "1788390",
    "end": "1794310"
  },
  {
    "text": "To do this, we will assume, first of all, that Theta is just to the left of a data point.",
    "start": "1794310",
    "end": "1802629"
  },
  {
    "text": "Now, when Theta is just to the left of a data point, well suddenly there's no possibility that there is a data point at Theta.",
    "start": "1803510",
    "end": "1813870"
  },
  {
    "text": "And so n_1 and n_2 are related because n_1 plus n_2 equals n. And so",
    "start": "1813870",
    "end": "1823290"
  },
  {
    "text": "this- I can evaluate this expression knowing that n_2 is equal to n minus n_1.",
    "start": "1823290",
    "end": "1834075"
  },
  {
    "text": "And then I can differentiate it. And when I do that, I end up with this expression right here for L- minus Theta.",
    "start": "1834075",
    "end": "1846225"
  },
  {
    "text": "And I can do exactly the same thing when I'm looking at the right-hand derivative.",
    "start": "1846225",
    "end": "1851505"
  },
  {
    "text": "And when we look at the right-hand derivative, I can substitute and again n_1 plus n_2 is",
    "start": "1851505",
    "end": "1857730"
  },
  {
    "text": "equal to n because I know if Theta's not at a data point. And here we've chosen to eliminate n_1, rather",
    "start": "1857730",
    "end": "1865290"
  },
  {
    "text": "than eliminating n_2 and so we get a slightly different expression for L - plus.",
    "start": "1865290",
    "end": "1876120"
  },
  {
    "text": "Now,  theta's optimum means that L - minus a Theta is less than or equal to 0 and L - plus of Theta is greater than or equal to 0.",
    "start": "1876120",
    "end": "1884909"
  },
  {
    "text": "And those two conditions come immediately from here.",
    "start": "1884910",
    "end": "1890235"
  },
  {
    "text": "Uh 2n_1 over n minus 1  is less than or equal to 0, means n_1 over n is less than equal to a half.",
    "start": "1890235",
    "end": "1897075"
  },
  {
    "text": "Conversely, the right-hand condition becomes n_2 over n is less than or equal to half.",
    "start": "1897075",
    "end": "1904110"
  },
  {
    "text": "And those are the precise conditions under which Theta is the median.",
    "start": "1904110",
    "end": "1910184"
  },
  {
    "text": "And so we've shown that Theta is the median is equivalent to Theta minimizing the empirical risk.",
    "start": "1910185",
    "end": "1916650"
  },
  {
    "text": "[NOISE]",
    "start": "1916650",
    "end": "1924185"
  },
  {
    "start": "1923000",
    "end": "2014000"
  },
  {
    "text": "And I want to turn to a different loss function and we're going to construct it, using this function,",
    "start": "1924185",
    "end": "1930380"
  },
  {
    "text": "the tilted absolute value function and what it is, is it's a parameterized family of functions.",
    "start": "1930380",
    "end": "1937669"
  },
  {
    "text": "It has a parameter tag in it, which must be between 0  and 1 and then it gives us a penalty function,",
    "start": "1937670",
    "end": "1944150"
  },
  {
    "text": "we will use it to penalize the difference between the prediction Y hat and the actual Y,",
    "start": "1944150",
    "end": "1950525"
  },
  {
    "text": "so U here corresponds to Y hat minus Y and we're going to have P of U be either minus Tau times U,",
    "start": "1950525",
    "end": "1959825"
  },
  {
    "text": "when U is less than 0  or 1 minus Tau times U when U is greater than",
    "start": "1959825",
    "end": "1964940"
  },
  {
    "text": "or equal to 0 and it is exactly what the name suggests.",
    "start": "1964940",
    "end": "1970355"
  },
  {
    "text": "When Tau is a half. It's like an absolute value function, just scaled so, it's equal to one half the absolute value function.",
    "start": "1970355",
    "end": "1980870"
  },
  {
    "text": "If we increase Tau, then we see that it tilts and it is larger for negative view,",
    "start": "1980870",
    "end": "1990770"
  },
  {
    "text": "than for positive view and if we decrease Tau, then it tilt the other way and it becomes larger for",
    "start": "1990770",
    "end": "1998840"
  },
  {
    "text": "positive view than for negative view and there's a nice expression for it, which is explicit in a voiced the case is,",
    "start": "1998840",
    "end": "2005890"
  },
  {
    "text": "the half minus Tau times U plus one half multiplied by the absolute value of U.",
    "start": "2005890",
    "end": "2012290"
  },
  {
    "start": "2014000",
    "end": "2216000"
  },
  {
    "text": "Now, if we use it as a loss function, well then we've got the tilted absolute loss,",
    "start": "2014610",
    "end": "2020920"
  },
  {
    "text": "which is the loss of y hat Y, is P Tau of Y hat minus Y and so the risk will be the average tilted absolute loss.",
    "start": "2020920",
    "end": "2034615"
  },
  {
    "text": "Now. This function L of Theta, the risk is convex and it's piecewise linear because",
    "start": "2034615",
    "end": "2042595"
  },
  {
    "text": "the total absolute loss is a convex piecewise linear function of Y hat and when we take the average of such functions,",
    "start": "2042595",
    "end": "2049240"
  },
  {
    "text": "we get another one and it turns out that it has kink points at the data Y_1 to YN.",
    "start": "2049240",
    "end": "2056510"
  },
  {
    "text": "Now, if Tau is less than a half, well that means that when Y hat,",
    "start": "2057330",
    "end": "2063100"
  },
  {
    "text": "is less than Y, we are going to have a small value or",
    "start": "2063100",
    "end": "2068830"
  },
  {
    "text": "the penalty function compared to when Y hat is greater than Y, and so it's going to be worse to overestimate than to underestimate,",
    "start": "2068830",
    "end": "2077905"
  },
  {
    "text": "we're going to prefer to make underestimates. For Tau greater than a half we prefer,",
    "start": "2077905",
    "end": "2084440"
  },
  {
    "text": "to overestimate and it's worse to underestimate and so there are situations where,",
    "start": "2084440",
    "end": "2093649"
  },
  {
    "text": "one would prefer to make an overestimate, than an underestimate. If we're measuring the quantity of something dangerous,",
    "start": "2093650",
    "end": "2102234"
  },
  {
    "text": "we would prefer to overestimate that quantity rather than underestimate that quantity and we'll,",
    "start": "2102235",
    "end": "2108760"
  },
  {
    "text": "so we'll see that theta is optimal, if it's a Tau quantile of the data and that means that",
    "start": "2108760",
    "end": "2115869"
  },
  {
    "text": "roughly a fraction of the Y's less than Theta is around Tau,",
    "start": "2115870",
    "end": "2121130"
  },
  {
    "text": "so here's the empirical risk. We have here a collection of",
    "start": "2122910",
    "end": "2129250"
  },
  {
    "text": "data points and",
    "start": "2129250",
    "end": "2137679"
  },
  {
    "text": "the risk function has kinks in it, directly above the data points and in between those data points,",
    "start": "2137679",
    "end": "2149395"
  },
  {
    "text": "it is piecewise is linear and so these segments joining",
    "start": "2149395",
    "end": "2155950"
  },
  {
    "text": "the data points are straight lines and the optimal Theta,",
    "start": "2155950",
    "end": "2166630"
  },
  {
    "text": "it's right here at a data point, it's at 0.5 and this is where tau is 0.25 and we can see that this loss function,",
    "start": "2166630",
    "end": "2178990"
  },
  {
    "text": "is going to increase like this, it will get steeper and this loss function actually turns out,",
    "start": "2178990",
    "end": "2185410"
  },
  {
    "text": "it doesn't get any steeper at all and because these are the only data points that we have,",
    "start": "2185410",
    "end": "2194220"
  },
  {
    "text": "we've got we know that this loss function continues straight, and this loss function continue straight and we can see",
    "start": "2194220",
    "end": "2200700"
  },
  {
    "text": "that there's a penalty for underestimating, a penalty for overestimating,",
    "start": "2200700",
    "end": "2206565"
  },
  {
    "text": "and the penalty for overestimating is greater, than the penalty for underestimating.",
    "start": "2206565",
    "end": "2211960"
  },
  {
    "start": "2216000",
    "end": "2423000"
  },
  {
    "text": "So just like we did for the medians, we need to define carefully what a Tau quantile is and then we'll see that",
    "start": "2216870",
    "end": "2224500"
  },
  {
    "text": "the Tau quantiles are the things that minimize the empirical risk, so,",
    "start": "2224500",
    "end": "2230830"
  },
  {
    "text": "for Tau between 0 and 1 theta is a Tau quantile, if n_1 on n is less than or equal to Tau is less than equal to 1 minus n_2 on n, so,",
    "start": "2230830",
    "end": "2241900"
  },
  {
    "text": "remember n_1 is the number of data points strictly less than theta, so n_1 on n is the fraction of the data less than",
    "start": "2241900",
    "end": "2251400"
  },
  {
    "text": "Theta and n_2 on n is the fraction of the data greater than Theta and so 1 minus n_2,",
    "start": "2251400",
    "end": "2262030"
  },
  {
    "text": "on n, is the fraction of the data less than or equal to the predictor Y hat plus Theta.",
    "start": "2262030",
    "end": "2271585"
  },
  {
    "text": "Now, if theta doesn't equal any of the data points, well then n_1 and n_2 are related,",
    "start": "2271585",
    "end": "2277345"
  },
  {
    "text": "because n_1 plus n_2 is then going to equal n and so these two inequalities reduce to one inequality and",
    "start": "2277345",
    "end": "2286090"
  },
  {
    "text": "actually it was used in an equation, because we have n_1 on n is going to be less than or equal to Theta and",
    "start": "2286090",
    "end": "2292480"
  },
  {
    "text": "the other inequality is going to reduce to n_1 on n is greater, is less than or equal to, is greater than or equal to Tau so,",
    "start": "2292480",
    "end": "2299289"
  },
  {
    "text": "we're going to have two inequalities  that have the opposite sign and as a result the, those two inequalities boil down to Tau is n_1 on n. The fraction of",
    "start": "2299290",
    "end": "2308650"
  },
  {
    "text": "data points less than Theta is equal to Tau.",
    "start": "2308650",
    "end": "2314815"
  },
  {
    "text": "If you're at the data point, then you have to be careful to account for the number of data points equal to Theta.",
    "start": "2314815",
    "end": "2324440"
  },
  {
    "text": "Quantiles have names, uh, one of them we've seen already when Tau is a half is the median.",
    "start": "2324510",
    "end": "2332420"
  },
  {
    "text": "Another one are the quartiles, Tau is a quarter Tau is a half Tau is 0.75 ,",
    "start": "2335010",
    "end": "2342309"
  },
  {
    "text": "the deciles, Tau is 0.1 through 0.9 and the percentiles.",
    "start": "2342310",
    "end": "2348170"
  },
  {
    "text": "Let's look at some examples of quantiles. Here we have a plot.",
    "start": "2350310",
    "end": "2356635"
  },
  {
    "text": "On the left, we see the Tau quantiles and on the horizontal axis we see Tau.",
    "start": "2356635",
    "end": "2365305"
  },
  {
    "text": "We've got five data points 4, 7, 7, 8 and 9. Let's just mark those.",
    "start": "2365305",
    "end": "2371619"
  },
  {
    "text": "4, 7, 7, 8 and 9.",
    "start": "2371620",
    "end": "2378405"
  },
  {
    "text": "And if we pick a Tau of 0.1,",
    "start": "2378405",
    "end": "2388370"
  },
  {
    "text": "then that's a unique quantile, Theta is equal to 4.",
    "start": "2388370",
    "end": "2394800"
  },
  {
    "text": "If we pick a Tau of 0.2, then there's a range of quantiles between 4 and 7.",
    "start": "2395050",
    "end": "2406549"
  },
  {
    "text": "Any number between 4 and 7 is a 0.2 quantile. Here at 0.5, the corresponding 0.5 quantile is 7.",
    "start": "2406550",
    "end": "2422400"
  },
  {
    "start": "2423000",
    "end": "2781000"
  },
  {
    "text": "Now, the Tau quantile minimizes the empirical risk when you have tilted absolute loss.",
    "start": "2423250",
    "end": "2432860"
  },
  {
    "text": "And this is exactly like the argument we used in the case of the median. Let's say this precisely,",
    "start": "2432860",
    "end": "2438965"
  },
  {
    "text": "we'll say that Theta minimizes the empirical risk if and only if it's one of the Tau quantiles.",
    "start": "2438965",
    "end": "2446000"
  },
  {
    "text": "And here the empirical risk is defined with the tilted absolute loss, and the tilted absolute loss has parameter Tau in it.",
    "start": "2446000",
    "end": "2453785"
  },
  {
    "text": "And the argument goes exactly the same way. We assume the data is sorted and then the loss has n term in it.",
    "start": "2453785",
    "end": "2464795"
  },
  {
    "text": "Each one has the form p Tau Theta minus y_i, but those expressions for p Tau depend on",
    "start": "2464795",
    "end": "2473420"
  },
  {
    "text": "whether Theta is less than y_i or Theta is greater than y_i. And so we split that sum up into terms for which Theta is",
    "start": "2473420",
    "end": "2483290"
  },
  {
    "text": "less than y_i and the terms for which Theta is greater than y_i.",
    "start": "2483290",
    "end": "2490380"
  },
  {
    "text": "Now, if Theta is not equal to a data value, we can just immediately differentiate this with,",
    "start": "2490810",
    "end": "2496849"
  },
  {
    "text": "um, respect to Theta. And we find this nice expression. We can, uh, uh, uh,",
    "start": "2496850",
    "end": "2507320"
  },
  {
    "text": "evaluate this to find out whether or not we're at a point where Theta is, uh, L dash of Theta is 0.",
    "start": "2507320",
    "end": "2513170"
  },
  {
    "text": "But the- if we're looking to check whether or not we have a minimum,",
    "start": "2513170",
    "end": "2520369"
  },
  {
    "text": "we know that this function L is not differentiable, and so that's not a sufficient test for minimum- for Theta being optimal,",
    "start": "2520370",
    "end": "2528350"
  },
  {
    "text": "and instead we need to look at the left and the right derivatives. And we do the same trick as before.",
    "start": "2528350",
    "end": "2533665"
  },
  {
    "text": "We consider a point.",
    "start": "2533665",
    "end": "2539800"
  },
  {
    "text": "And in order to evaluate, uh, the derivative,",
    "start": "2539800",
    "end": "2545120"
  },
  {
    "text": "the left-hand derivative, we'll evaluate the left-hand derivative at a point Theta slightly to the left of the point.",
    "start": "2545120",
    "end": "2552230"
  },
  {
    "text": "If we move slightly left of the point, then n_1 doesn't change, but n_2 does because n_2 increases by the number of points actually at that minimum.",
    "start": "2552230",
    "end": "2563810"
  },
  {
    "text": "And so we eliminate n_2 from the equation since we know that n_1 plus n_2 is equal to n,",
    "start": "2563810",
    "end": "2569390"
  },
  {
    "text": "and that gives us this expression here; for L dash prime of Theta- L dash minus of Theta.",
    "start": "2569390",
    "end": "2577010"
  },
  {
    "text": "To evaluate the right-hand derivative, we move slightly to the right. When we move slightly to the right,",
    "start": "2577010",
    "end": "2583790"
  },
  {
    "text": "we know, uh, uh, we know,",
    "start": "2583790",
    "end": "2591020"
  },
  {
    "text": "again, we, ah, we know n_2, but we don't know n_1,",
    "start": "2591020",
    "end": "2596255"
  },
  {
    "text": "because n_1 depends on the number of points at the minimum.",
    "start": "2596255",
    "end": "2601670"
  },
  {
    "text": "And so we eliminate n_1, because we know n_1 plus n_2 is n, and we get a nice expression for L dash plus of Theta.",
    "start": "2601670",
    "end": "2609650"
  },
  {
    "text": "Then to check for optimality, we have to check the L dash minus of Theta is less than or equal to 0.",
    "start": "2609650",
    "end": "2614780"
  },
  {
    "text": "Now that's plus of Theta is greater than or equal to 0, which gives us the inequalities that define the median.",
    "start": "2614780",
    "end": "2624210"
  },
  {
    "text": "Let's look at one more case. This is the fractional loss case.",
    "start": "2627610",
    "end": "2632990"
  },
  {
    "text": "Now, the fractional loss is this loss right here, loss of y hat y is the maximum of y hat divided by y minus 1,",
    "start": "2632990",
    "end": "2642664"
  },
  {
    "text": "and y divided y hat minus 1, which has this nice expression as the exponential of",
    "start": "2642665",
    "end": "2648880"
  },
  {
    "text": "the absolute value of the difference between the logarithms minus 1. And it's this function. It's curved.",
    "start": "2648880",
    "end": "2659530"
  },
  {
    "text": "Here we are in the case where y is 2 and we're looking at y hat. If y is 2 and y hat is 2.5,",
    "start": "2659530",
    "end": "2668214"
  },
  {
    "text": "well, then y hat is 25% more than y.",
    "start": "2668215",
    "end": "2675680"
  },
  {
    "text": "If y is 2 and we're looking at,",
    "start": "2676360",
    "end": "2681635"
  },
  {
    "text": "uh, a y hat of 1, well, uh, then we have that y is 100% more than y hat.",
    "start": "2681635",
    "end": "2696650"
  },
  {
    "text": "And obviously as y hat tends to 0, the percentage more that y is then y hat is going to tend to infinity.",
    "start": "2696650",
    "end": "2706829"
  },
  {
    "text": "An empirical risk is therefore the average of the fractional loss.",
    "start": "2707380",
    "end": "2712400"
  },
  {
    "text": "Uh, this is a convex function because the fractional loss is convex,",
    "start": "2712400",
    "end": "2718609"
  },
  {
    "text": "as we can see from the plot. And we're going to call the Theta that minimizes L of Theta,",
    "start": "2718610",
    "end": "2726215"
  },
  {
    "text": "the fractional middle of y_1 through y_n, and that's actually not a standard term, but it's convenient.",
    "start": "2726215",
    "end": "2732320"
  },
  {
    "text": "Uh, this is a plot of the empirical risk as a function of Theta.",
    "start": "2732320",
    "end": "2741440"
  },
  {
    "text": "There are, um, in fact, kinks in this plot.",
    "start": "2741440",
    "end": "2746825"
  },
  {
    "text": "They can be quite hard to see, but they lie exactly above the data, right here,",
    "start": "2746825",
    "end": "2751880"
  },
  {
    "text": "and here, and here, but the function is not piecewise linear between any two kinks.",
    "start": "2751880",
    "end": "2757895"
  },
  {
    "text": "It is curved. The data is right here. And we can see that there is a minimum.",
    "start": "2757895",
    "end": "2764540"
  },
  {
    "text": "This is the fractional middle that's marked in red, that is, uh, doesn't occur at a data point.",
    "start": "2764540",
    "end": "2771200"
  },
  {
    "text": "And this segment right here is actually a curve segment that has a minimum right there.",
    "start": "2771200",
    "end": "2778859"
  },
  {
    "text": "And we go through exactly the same kind of analysis we did before, we split the data into data points less than Theta,",
    "start": "2781150",
    "end": "2789035"
  },
  {
    "text": "and data points greater than Theta, that gives us two sums.",
    "start": "2789035",
    "end": "2794099"
  },
  {
    "text": "For one of the sums, we end up with one of the expressions in the fractional loss.",
    "start": "2794710",
    "end": "2800720"
  },
  {
    "text": "And for the other sum, we end up with the other expression of fractional loss. We can collect all the terms involving one at the beginning,",
    "start": "2800720",
    "end": "2809645"
  },
  {
    "text": "and so then will end up with an expression like this. Now, if we're at a Theta between two particular data points,",
    "start": "2809645",
    "end": "2818480"
  },
  {
    "text": "y_k and y_k plus 1, then L dash of Theta is easy to evaluate.",
    "start": "2818480",
    "end": "2826385"
  },
  {
    "text": "We simply look at this fine expression we have, differentiate it with respect to Theta,",
    "start": "2826385",
    "end": "2832670"
  },
  {
    "text": "and we have this expression right here. Now, because the empirical risk is convex,",
    "start": "2832670",
    "end": "2841700"
  },
  {
    "text": "then the gradient is going to be an increasing function of Theta, and all we need to do to find the minimum is to find out where the gradient crosses 0.",
    "start": "2841700",
    "end": "2852530"
  },
  {
    "text": "So we go through all the data points one at a time, looking at k till we find, uh, uh,",
    "start": "2852530",
    "end": "2860765"
  },
  {
    "text": "till we find one of the k's such that when we evaluate this derivative here at the beginning of the interval,",
    "start": "2860765",
    "end": "2867575"
  },
  {
    "text": "at y_k, we have a derivative which is less than or equal to 0. And when we evaluate that derivative at the other end of the interval,",
    "start": "2867575",
    "end": "2876319"
  },
  {
    "text": "at y_k plus 1, we have a derivative which is greater than or equal to 0.",
    "start": "2876320",
    "end": "2881580"
  },
  {
    "text": "Then we found the interval in which the optimal Theta lies,",
    "start": "2883810",
    "end": "2890165"
  },
  {
    "text": "and then we can look at that derivative and, uh, set it to 0, and solve to find the corresponding Theta.",
    "start": "2890165",
    "end": "2896855"
  },
  {
    "text": "And that gives us this nice expression right here for the optimal Theta.",
    "start": "2896855",
    "end": "2904119"
  },
  {
    "text": "So the procedure to find that Theta is first, we have to find k. We have to find k such that this expression here is,",
    "start": "2904120",
    "end": "2916735"
  },
  {
    "text": "uh, less than or equal to 0 when we",
    "start": "2916735",
    "end": "2922280"
  },
  {
    "text": "evaluate it at Theta is y_k and it's greater than or equal to 0 when we evaluate it at Theta is y_k plus 1.",
    "start": "2922280",
    "end": "2928670"
  },
  {
    "text": "And then we just use this formula to give us the optimal Theta.",
    "start": "2928670",
    "end": "2934559"
  },
  {
    "text": "Let's summarize. The simplest predictor is a constant,",
    "start": "2940270",
    "end": "2945280"
  },
  {
    "text": "its y hat is Theta. Different losses give you different Thetas,",
    "start": "2945280",
    "end": "2952435"
  },
  {
    "text": "um, when you apply empirical risk minimization. And for some common losses,",
    "start": "2952435",
    "end": "2959315"
  },
  {
    "text": "you actually get well-known predictors; the square loss, the predictor is the mean.",
    "start": "2959315",
    "end": "2964400"
  },
  {
    "text": "The absolute loss gives you the median and the tilted absolute loss gives you the quantile.",
    "start": "2964400",
    "end": "2970320"
  },
  {
    "text": "It's worth also me pointing out at this point that even though this section had quite a lot of algebra and technicalities in it,",
    "start": "2970870",
    "end": "2978905"
  },
  {
    "text": "the technicalities in the algebra don't really matter. What matters here is the interpretation",
    "start": "2978905",
    "end": "2987545"
  },
  {
    "text": "of the losses and the interpretation of the results that they give you.",
    "start": "2987545",
    "end": "2993079"
  },
  {
    "text": "You should know that when you're going to do machine learning with a square loss,",
    "start": "2993080",
    "end": "2999560"
  },
  {
    "text": "you're going to get an answer which corresponds to the mean. And if you do use the absolute loss,",
    "start": "2999560",
    "end": "3006119"
  },
  {
    "text": "you're gonna get something corresponding to the median, and have some intuition about how those things behave.",
    "start": "3006120",
    "end": "3013045"
  },
  {
    "text": "In particular, we know that the median is kinda insensitive to the position of outliers,",
    "start": "3013045",
    "end": "3019464"
  },
  {
    "text": "whereas the mean is very sensitive to the position of outliers.",
    "start": "3019465",
    "end": "3025419"
  },
  {
    "text": "Um, the tilted absolute loss is very useful, because very often we really do want an estimate which is preferentially underestimating,",
    "start": "3025419",
    "end": "3039205"
  },
  {
    "text": "or preferentially overestimating the true Y.",
    "start": "3039205",
    "end": "3045380"
  }
]