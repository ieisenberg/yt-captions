[
  {
    "start": "0",
    "end": "83000"
  },
  {
    "start": "0",
    "end": "5860"
  },
  {
    "text": "OMAR KHATTAB: Hello, everyone.",
    "start": "5860",
    "end": "7110"
  },
  {
    "text": "Welcome to part 2 of our\nseries on NLU and IR.",
    "start": "7110",
    "end": "11330"
  },
  {
    "text": "The screen cast will be a\ncrash course in classical IR,",
    "start": "11330",
    "end": "14690"
  },
  {
    "text": "as well as evaluation methods\nand information retrieval.",
    "start": "14690",
    "end": "17980"
  },
  {
    "start": "17980",
    "end": "20619"
  },
  {
    "text": "Let us first define the\nsimplest form of our task,",
    "start": "20620",
    "end": "23560"
  },
  {
    "text": "namely ranked retrieval.",
    "start": "23560",
    "end": "26170"
  },
  {
    "text": "We will be given a large\ncollection of text documents.",
    "start": "26170",
    "end": "29320"
  },
  {
    "text": "This could be all of the\npassages in Wikipedia,",
    "start": "29320",
    "end": "32529"
  },
  {
    "text": "perhaps a crawl of\nparts of the web",
    "start": "32530",
    "end": "35050"
  },
  {
    "text": "or maybe all of the\ndocumentation of Hugging Face",
    "start": "35050",
    "end": "37480"
  },
  {
    "text": "or other software libraries.",
    "start": "37480",
    "end": "40870"
  },
  {
    "text": "This corpus will be provided to\nus offline, that is before we",
    "start": "40870",
    "end": "44620"
  },
  {
    "text": "interact with any users.",
    "start": "44620",
    "end": "46640"
  },
  {
    "text": "And we will be able to spend a\none time effort at organizing",
    "start": "46640",
    "end": "49899"
  },
  {
    "text": "or otherwise\nunderstanding the content",
    "start": "49900",
    "end": "51940"
  },
  {
    "text": "of these documents in the corpus\nbefore we start searching.",
    "start": "51940",
    "end": "57250"
  },
  {
    "text": "Online though, we\nwill receive a query",
    "start": "57250",
    "end": "59830"
  },
  {
    "text": "from the users, which\ncould be a natural language",
    "start": "59830",
    "end": "63160"
  },
  {
    "text": "question written in\nEnglish, for example.",
    "start": "63160",
    "end": "66370"
  },
  {
    "text": "The goal of our ranked\nretrieval system",
    "start": "66370",
    "end": "68770"
  },
  {
    "text": "will be to output a Top-K\nlist of documents, sorted",
    "start": "68770",
    "end": "71920"
  },
  {
    "text": "in decreasing order of\nrelevance to the information",
    "start": "71920",
    "end": "74409"
  },
  {
    "text": "need that the user\nexpressed in the query.",
    "start": "74410",
    "end": "76930"
  },
  {
    "start": "76930",
    "end": "79720"
  },
  {
    "text": "So this might be the top\n10 or the top 100 results.",
    "start": "79720",
    "end": "84440"
  },
  {
    "start": "83000",
    "end": "209000"
  },
  {
    "text": "So how do we conduct this\ntask of ranked retrieval?",
    "start": "84440",
    "end": "89510"
  },
  {
    "text": "As it turns out, we've\nalready looked at the way",
    "start": "89510",
    "end": "91740"
  },
  {
    "text": "for doing this before, when\ndiscussing matrix designs.",
    "start": "91740",
    "end": "95750"
  },
  {
    "text": "In particular, we know that\nwe can build Term-Document",
    "start": "95750",
    "end": "99200"
  },
  {
    "text": "Occurrence Matrices.",
    "start": "99200",
    "end": "100759"
  },
  {
    "text": "And in such a matrix\nlike the one shown,",
    "start": "100760",
    "end": "103670"
  },
  {
    "text": "each term-document pair\nhas a corresponding cell",
    "start": "103670",
    "end": "107390"
  },
  {
    "text": "in which the matrix will\nstore the number of times",
    "start": "107390",
    "end": "110030"
  },
  {
    "text": "that the term appears in\nthe document, in our corpus.",
    "start": "110030",
    "end": "112715"
  },
  {
    "start": "112715",
    "end": "115380"
  },
  {
    "text": "Of course, we will\nprobably want to apply",
    "start": "115380",
    "end": "117420"
  },
  {
    "text": "some sort of reweighting\nhere, because we don't want",
    "start": "117420",
    "end": "119759"
  },
  {
    "text": "to work with these row counts.",
    "start": "119760",
    "end": "121680"
  },
  {
    "text": "But once we've done\nthat, we can already",
    "start": "121680",
    "end": "123720"
  },
  {
    "text": "answer queries that contain\njust a single term pretty well.",
    "start": "123720",
    "end": "127890"
  },
  {
    "text": "And to do that, we would\nbasically just return",
    "start": "127890",
    "end": "130169"
  },
  {
    "text": "the K-documents with\nthe largest weight",
    "start": "130169",
    "end": "132180"
  },
  {
    "text": "after normalization or other\nprocesses for the single term.",
    "start": "132180",
    "end": "137476"
  },
  {
    "text": "And again, as it turns\nout, this is precisely what",
    "start": "137477",
    "end": "139560"
  },
  {
    "text": "is done in classical IR if\nwe have just a single query--",
    "start": "139560",
    "end": "143130"
  },
  {
    "text": "just a single term in our query.",
    "start": "143130",
    "end": "146340"
  },
  {
    "text": "When we have multiple\nterms in the same query,",
    "start": "146340",
    "end": "149160"
  },
  {
    "text": "classical IR tends to\ntreat them independently.",
    "start": "149160",
    "end": "152560"
  },
  {
    "text": "So we would basically\nadd the weights up",
    "start": "152560",
    "end": "154590"
  },
  {
    "text": "across all of the terms\nin the query per document.",
    "start": "154590",
    "end": "158470"
  },
  {
    "text": "And then, that's the\nscore for the document.",
    "start": "158470",
    "end": "160800"
  },
  {
    "text": "This is precisely\nthe computation",
    "start": "160800",
    "end": "162390"
  },
  {
    "text": "that is shown here, where we\ncompute the relevance score",
    "start": "162390",
    "end": "166380"
  },
  {
    "text": "between a query and a document.",
    "start": "166380",
    "end": "168480"
  },
  {
    "text": "We would go over all\nthe terms in the query",
    "start": "168480",
    "end": "170819"
  },
  {
    "text": "and simply add the corresponding\ndocument-term weights",
    "start": "170820",
    "end": "174750"
  },
  {
    "text": "for all of these terms\nfor that document.",
    "start": "174750",
    "end": "177960"
  },
  {
    "text": "This gives us a score\nfor the document",
    "start": "177960",
    "end": "180210"
  },
  {
    "text": "and we can then\nreturn the K-documents",
    "start": "180210",
    "end": "182220"
  },
  {
    "text": "with the largest total scores.",
    "start": "182220",
    "end": "185040"
  },
  {
    "text": "Interestingly, this reduces\nmuch of classical IR,",
    "start": "185040",
    "end": "188290"
  },
  {
    "text": "of course, not all\nof it, to thinking",
    "start": "188290",
    "end": "190110"
  },
  {
    "text": "about how do we best weigh\neach term-document pair, which",
    "start": "190110",
    "end": "195500"
  },
  {
    "text": "has an undeniable\nsimilarity to our first task",
    "start": "195500",
    "end": "198110"
  },
  {
    "text": "this quarter in Homework 1.",
    "start": "198110",
    "end": "200780"
  },
  {
    "text": "Except of course,\nthat here in IR,",
    "start": "200780",
    "end": "202370"
  },
  {
    "text": "we look at the term to document\nrelevance, and not word",
    "start": "202370",
    "end": "205430"
  },
  {
    "text": "to word relatedness.",
    "start": "205430",
    "end": "206870"
  },
  {
    "start": "206870",
    "end": "210629"
  },
  {
    "start": "209000",
    "end": "300000"
  },
  {
    "text": "So thinking about\nterm-document weighting,",
    "start": "210630",
    "end": "213240"
  },
  {
    "text": "here are some intuitions\nthat might be useful,",
    "start": "213240",
    "end": "216270"
  },
  {
    "text": "as we think about what makes\na strong term weighting",
    "start": "216270",
    "end": "218670"
  },
  {
    "text": "model in IR.",
    "start": "218670",
    "end": "220586"
  },
  {
    "text": "Of course, later, in the next\nscreencast in particular,",
    "start": "220586",
    "end": "224090"
  },
  {
    "text": "we'll be looking at neural\nmodels that go beyond this.",
    "start": "224090",
    "end": "229540"
  },
  {
    "text": "But for now, perhaps the two\nmost prominent intuitions",
    "start": "229540",
    "end": "233349"
  },
  {
    "text": "for term-document\nweighting are connected",
    "start": "233350",
    "end": "235420"
  },
  {
    "text": "to our first unit's discussion\nof frequency and normalization.",
    "start": "235420",
    "end": "239890"
  },
  {
    "text": "In particular, if a\nterm t occurs frequently",
    "start": "239890",
    "end": "243760"
  },
  {
    "text": "in document d.",
    "start": "243760",
    "end": "245189"
  },
  {
    "text": "The document is\nmore likely to be",
    "start": "245190",
    "end": "246820"
  },
  {
    "text": "relevant for queries\nthat include the term t",
    "start": "246820",
    "end": "249460"
  },
  {
    "text": "or so is one of our intuitions.",
    "start": "249460",
    "end": "253030"
  },
  {
    "text": "And in terms of normalization,\nif that term t is quite rare,",
    "start": "253030",
    "end": "258130"
  },
  {
    "text": "so if it occurs in only\na few documents overall,",
    "start": "258130",
    "end": "261040"
  },
  {
    "text": "we take that as a stronger\nsignal that document d",
    "start": "261040",
    "end": "265060"
  },
  {
    "text": "is even more likely to be\nrelevant for queries including",
    "start": "265060",
    "end": "268090"
  },
  {
    "text": "t.",
    "start": "268090",
    "end": "270570"
  },
  {
    "text": "Lastly, if document\nd is rather short,",
    "start": "270570",
    "end": "272940"
  },
  {
    "text": "we take that as also\nyet another signal",
    "start": "272940",
    "end": "275130"
  },
  {
    "text": "that might increase our\nconfidence that the term",
    "start": "275130",
    "end": "277890"
  },
  {
    "text": "t was included in that rather\nshort document for a reason.",
    "start": "277890",
    "end": "283130"
  },
  {
    "text": "Taking a step back and\nthinking more broadly,",
    "start": "283130",
    "end": "285470"
  },
  {
    "text": "we're still functioning\nunder the same statement",
    "start": "285470",
    "end": "287990"
  },
  {
    "text": "from the first unit.",
    "start": "287990",
    "end": "289310"
  },
  {
    "text": "Our goal is ultimately to\namplify the important signals,",
    "start": "289310",
    "end": "292940"
  },
  {
    "text": "trustworthy and the\nunusual and to deemphasize",
    "start": "292940",
    "end": "296480"
  },
  {
    "text": "the mundane and the quirky.",
    "start": "296480",
    "end": "297770"
  },
  {
    "start": "297770",
    "end": "301190"
  },
  {
    "start": "300000",
    "end": "404000"
  },
  {
    "text": "There are so many different\nterm-weighting functions in IR,",
    "start": "301190",
    "end": "304620"
  },
  {
    "text": "but most of them are\ndirectly inspired by TF-IDF",
    "start": "304620",
    "end": "307880"
  },
  {
    "text": "and take a very similar\ncomputational form.",
    "start": "307880",
    "end": "311390"
  },
  {
    "text": "For TF-IDF, this is a\nslightly different version",
    "start": "311390",
    "end": "314690"
  },
  {
    "text": "to the one used in Unit 1.",
    "start": "314690",
    "end": "316490"
  },
  {
    "text": "What I have here is\nslightly different.",
    "start": "316490",
    "end": "318341"
  },
  {
    "text": "This is the more popular\nversion in the context",
    "start": "318342",
    "end": "320300"
  },
  {
    "text": "of IR applications.",
    "start": "320300",
    "end": "321530"
  },
  {
    "text": "But TF-IDF is\noverloaded frequently,",
    "start": "321530",
    "end": "323540"
  },
  {
    "text": "and you will see\nmultiple implementations,",
    "start": "323540",
    "end": "327300"
  },
  {
    "text": "if you go look for them.",
    "start": "327300",
    "end": "329229"
  },
  {
    "text": "So we'll define N to be\nthe size of the collection.",
    "start": "329230",
    "end": "332040"
  },
  {
    "text": "And DF or document\nfrequency of a term,",
    "start": "332040",
    "end": "336270"
  },
  {
    "text": "DF of term to be the number\nof documents that contain",
    "start": "336270",
    "end": "341289"
  },
  {
    "text": "that term in the collection.",
    "start": "341290",
    "end": "343830"
  },
  {
    "text": "Then TF or term frequency\nof a term-document pair",
    "start": "343830",
    "end": "348180"
  },
  {
    "text": "will be defined as the\nlogarithm of the frequency",
    "start": "348180",
    "end": "350639"
  },
  {
    "text": "of this term in this\ndocument, with 1 just",
    "start": "350640",
    "end": "355290"
  },
  {
    "text": "for mathematical reasons.",
    "start": "355290",
    "end": "358550"
  },
  {
    "text": "IDF or inverse\ndocument frequency",
    "start": "358550",
    "end": "361009"
  },
  {
    "text": "is defined as the logarithm\nof N divided by the document",
    "start": "361010",
    "end": "364370"
  },
  {
    "text": "frequency of the term.",
    "start": "364370",
    "end": "365330"
  },
  {
    "start": "365330",
    "end": "367939"
  },
  {
    "text": "TF-IDF is then nothing\nbut the product",
    "start": "367940",
    "end": "370430"
  },
  {
    "text": "of these two values for each\nquery term summed up at the end",
    "start": "370430",
    "end": "374810"
  },
  {
    "text": "to assign a single overall\nscore to each document",
    "start": "374810",
    "end": "377810"
  },
  {
    "text": "by summing up across\nall query terms,",
    "start": "377810",
    "end": "379610"
  },
  {
    "text": "as we've discussed before.",
    "start": "379610",
    "end": "381259"
  },
  {
    "text": "Of course, higher\nscores are better",
    "start": "381260",
    "end": "383180"
  },
  {
    "text": "and the Top-K\nscoring documents are",
    "start": "383180",
    "end": "385039"
  },
  {
    "text": "those that we would\nreturn to the searcher",
    "start": "385040",
    "end": "386900"
  },
  {
    "text": "if we were to use TF-IDF.",
    "start": "386900",
    "end": "390430"
  },
  {
    "text": "Notice how both TF and IDF grow\nsub-linearly, in particular,",
    "start": "390430",
    "end": "395710"
  },
  {
    "text": "logarithmically with frequency\nand 1 over DF, respectively.",
    "start": "395710",
    "end": "401229"
  },
  {
    "start": "401230",
    "end": "405640"
  },
  {
    "start": "404000",
    "end": "445000"
  },
  {
    "text": "A much stronger term\nweighting model in practice",
    "start": "405640",
    "end": "408220"
  },
  {
    "text": "is BM25 or best match number 25.",
    "start": "408220",
    "end": "411590"
  },
  {
    "text": "And as you might imagine,\nit took many attempts",
    "start": "411590",
    "end": "413620"
  },
  {
    "text": "until BM25 was developed.",
    "start": "413620",
    "end": "415831"
  },
  {
    "start": "415831",
    "end": "418479"
  },
  {
    "text": "For our purposes, unlike\nTF-IDF, term frequency in BM25",
    "start": "418480",
    "end": "423730"
  },
  {
    "text": "saturates towards the\nconstant value for each term,",
    "start": "423730",
    "end": "427390"
  },
  {
    "text": "and also, it penalizes\nlonger documents",
    "start": "427390",
    "end": "429730"
  },
  {
    "text": "when counting frequencies,\nsince a longer",
    "start": "429730",
    "end": "432370"
  },
  {
    "text": "document will naturally contain\nmore occurrences of its terms.",
    "start": "432370",
    "end": "437360"
  },
  {
    "text": "These are the main\ndifferences, and it really",
    "start": "437360",
    "end": "439789"
  },
  {
    "text": "helps BM25 in practice be a much\nstronger term weighting model.",
    "start": "439790",
    "end": "446230"
  },
  {
    "start": "445000",
    "end": "530000"
  },
  {
    "text": "Now that we've decided the\nbehavior of these weighting",
    "start": "446230",
    "end": "450340"
  },
  {
    "text": "functions or at least\na couple of them,",
    "start": "450340",
    "end": "452800"
  },
  {
    "text": "how would we actually implement\nthis as an actual system",
    "start": "452800",
    "end": "456280"
  },
  {
    "text": "that we could use for search?",
    "start": "456280",
    "end": "459660"
  },
  {
    "text": "So let's think about this,\nwhereas the raw collection,",
    "start": "459660",
    "end": "462590"
  },
  {
    "text": "the actual text, supports fast\naccess from documents to terms,",
    "start": "462590",
    "end": "466850"
  },
  {
    "text": "so basically, [INAUDIBLE] gives\nus the terms of each document.",
    "start": "466850",
    "end": "471350"
  },
  {
    "text": "The term-document matrix\nthat we've studied so far,",
    "start": "471350",
    "end": "474680"
  },
  {
    "text": "allows fast access from\na term to the documents.",
    "start": "474680",
    "end": "477720"
  },
  {
    "text": "So it's a bit of\nthe reverse process.",
    "start": "477720",
    "end": "479570"
  },
  {
    "text": "Unfortunately, the term-document\nmatrix is way too sparse",
    "start": "479570",
    "end": "484280"
  },
  {
    "text": "and contains too many\nzeros to be useful,",
    "start": "484280",
    "end": "487040"
  },
  {
    "text": "since the average\nterm does not occur",
    "start": "487040",
    "end": "489560"
  },
  {
    "text": "in the vast majority\nof documents,",
    "start": "489560",
    "end": "491419"
  },
  {
    "text": "if you think about it.",
    "start": "491420",
    "end": "493570"
  },
  {
    "text": "For the inverted index,\nthat's where it comes in.",
    "start": "493570",
    "end": "495730"
  },
  {
    "text": "This is a data structure\nthat solves this problem.",
    "start": "495730",
    "end": "498550"
  },
  {
    "text": "It's essentially just a sparse\nrepresentation of our matrix",
    "start": "498550",
    "end": "501819"
  },
  {
    "text": "here, which maps each unique\nterm in the collection.",
    "start": "501820",
    "end": "505330"
  },
  {
    "text": "So each unique term\nin our vocabulary",
    "start": "505330",
    "end": "508120"
  },
  {
    "text": "is what we call a posting list.",
    "start": "508120",
    "end": "511300"
  },
  {
    "text": "The posting list\nof a term t simply",
    "start": "511300",
    "end": "514659"
  },
  {
    "text": "enumerates all of the actual\noccurrences of the term t",
    "start": "514659",
    "end": "517900"
  },
  {
    "text": "in the documents, recording\nboth the ID of each document",
    "start": "517900",
    "end": "521710"
  },
  {
    "text": "in which the term t appears.",
    "start": "521710",
    "end": "523450"
  },
  {
    "text": "And also, its frequency\nin each of these documents",
    "start": "523450",
    "end": "530980"
  },
  {
    "start": "530000",
    "end": "645000"
  },
  {
    "text": "So beyond term weighting\nmodels, IR, of course,",
    "start": "530980",
    "end": "533709"
  },
  {
    "text": "contains lots of models\nfor other things.",
    "start": "533710",
    "end": "536250"
  },
  {
    "text": "So they're models for expanding\nqueries and documents.",
    "start": "536250",
    "end": "539680"
  },
  {
    "text": "This basically entails\nadding new terms",
    "start": "539680",
    "end": "542050"
  },
  {
    "text": "to queries or to\ndocuments, or to both,",
    "start": "542050",
    "end": "544450"
  },
  {
    "text": "to help with the\nvocabulary mismatch problem",
    "start": "544450",
    "end": "546580"
  },
  {
    "text": "that we discussed in the first\nscreencast of the series,",
    "start": "546580",
    "end": "549700"
  },
  {
    "text": "basically, when queries and\ndocuments use different terms",
    "start": "549700",
    "end": "552580"
  },
  {
    "text": "to express the same thing.",
    "start": "552580",
    "end": "554870"
  },
  {
    "text": "There's also plenty of work\non term dependence and phrase",
    "start": "554870",
    "end": "557960"
  },
  {
    "text": "search.",
    "start": "557960",
    "end": "558460"
  },
  {
    "text": "Notice that so far, we've\nassumed the terms in each query",
    "start": "558460",
    "end": "561280"
  },
  {
    "text": "and in each document\nare independent,",
    "start": "561280",
    "end": "563200"
  },
  {
    "text": "and we function in a\nbag-of-words fashion.",
    "start": "563200",
    "end": "567100"
  },
  {
    "text": "But work on term dependence\nand phrase search",
    "start": "567100",
    "end": "569620"
  },
  {
    "text": "relaxes these assumptions\nthat each query",
    "start": "569620",
    "end": "572290"
  },
  {
    "text": "is a bag of independent terms.",
    "start": "572290",
    "end": "574389"
  },
  {
    "text": "Lastly, there is\nalso lots of work",
    "start": "574390",
    "end": "575950"
  },
  {
    "text": "on learning to rank\nwith various features,",
    "start": "575950",
    "end": "579160"
  },
  {
    "text": "like how to estimate\nrelevance when documents have",
    "start": "579160",
    "end": "581379"
  },
  {
    "text": "multiple fields, like maybe a\ntitle, a body, some headings,",
    "start": "581380",
    "end": "585040"
  },
  {
    "text": "a footer and also,\nanchor text, which",
    "start": "585040",
    "end": "587199"
  },
  {
    "text": "is a very strong signal when\nyou have it, like in web search.",
    "start": "587200",
    "end": "590270"
  },
  {
    "text": "So this is basically\nthe text from links",
    "start": "590270",
    "end": "592720"
  },
  {
    "text": "in other pages to your page.",
    "start": "592720",
    "end": "594699"
  },
  {
    "text": "The text in those links\nor around those links",
    "start": "594700",
    "end": "596890"
  },
  {
    "text": "tends to be very useful\nas a relevant signal.",
    "start": "596890",
    "end": "604040"
  },
  {
    "text": "And of course, also things like\nPageRank with link analysis",
    "start": "604040",
    "end": "606860"
  },
  {
    "text": "and lots of other\nfeatures for IR,",
    "start": "606860",
    "end": "609529"
  },
  {
    "text": "like recency and other stuff.",
    "start": "609530",
    "end": "612120"
  },
  {
    "text": "But I think it's worth\nmentioning that until recently,",
    "start": "612120",
    "end": "615300"
  },
  {
    "text": "if you just had a collection\nthat you want to search",
    "start": "615300",
    "end": "618269"
  },
  {
    "text": "and you didn't want\nto do a lot of tuning,",
    "start": "618270",
    "end": "621030"
  },
  {
    "text": "BM25 was a very strong\nbaseline on the best",
    "start": "621030",
    "end": "624900"
  },
  {
    "text": "that you could do ad hoc,\nso without lots of tuning",
    "start": "624900",
    "end": "627540"
  },
  {
    "text": "and without lots of\ntraining data, et cetera.",
    "start": "627540",
    "end": "631440"
  },
  {
    "text": "And this only\nchanged a year or two",
    "start": "631440",
    "end": "633360"
  },
  {
    "text": "ago with the advent of\nBERT-based ranking, which we'll",
    "start": "633360",
    "end": "636390"
  },
  {
    "text": "discuss in detail in\nthe next screencast",
    "start": "636390",
    "end": "638670"
  },
  {
    "text": "of this set of the series.",
    "start": "638670",
    "end": "639810"
  },
  {
    "start": "639810",
    "end": "645960"
  },
  {
    "start": "645000",
    "end": "927000"
  },
  {
    "text": "OK, so we just\nbuilt an IR system.",
    "start": "645960",
    "end": "650130"
  },
  {
    "text": "How do we evaluate our work?",
    "start": "650130",
    "end": "652230"
  },
  {
    "text": "What is success like?",
    "start": "652230",
    "end": "654149"
  },
  {
    "text": "Well, a search system,\nas you can imagine,",
    "start": "654150",
    "end": "656100"
  },
  {
    "text": "must be both efficient\nand effective.",
    "start": "656100",
    "end": "659069"
  },
  {
    "text": "If we had infinite time\nand infinite resources,",
    "start": "659070",
    "end": "661710"
  },
  {
    "text": "we would just hire experts to\nlook through all the documents",
    "start": "661710",
    "end": "664830"
  },
  {
    "text": "one by one to\nconduct the search,",
    "start": "664830",
    "end": "666880"
  },
  {
    "text": "but clearly, we don't\nhave that sort of ability.",
    "start": "666880",
    "end": "671950"
  },
  {
    "text": "So efficiency in\nIR is paramount,",
    "start": "671950",
    "end": "674290"
  },
  {
    "text": "after all, we want\nour retrieval models",
    "start": "674290",
    "end": "676509"
  },
  {
    "text": "to work with subsecond\nlatencies for collections that",
    "start": "676510",
    "end": "679300"
  },
  {
    "text": "may have hundreds of\nmillions of documents,",
    "start": "679300",
    "end": "681339"
  },
  {
    "text": "if not even larger than that.",
    "start": "681340",
    "end": "684130"
  },
  {
    "text": "The most common measure\nof efficiency in IR",
    "start": "684130",
    "end": "686740"
  },
  {
    "text": "is latency, which is simply\nthe time it takes to run",
    "start": "686740",
    "end": "689620"
  },
  {
    "text": "one query through the system.",
    "start": "689620",
    "end": "691240"
  },
  {
    "text": "Say, on average or\nperhaps, at the tail,",
    "start": "691240",
    "end": "694000"
  },
  {
    "text": "like the 95th\npercentile, for example.",
    "start": "694000",
    "end": "697460"
  },
  {
    "text": "But you can also\nmeasure throughput",
    "start": "697460",
    "end": "699520"
  },
  {
    "text": "in queries per second.",
    "start": "699520",
    "end": "701410"
  },
  {
    "text": "Space, how much, maybe\nthe inverted index",
    "start": "701410",
    "end": "704350"
  },
  {
    "text": "takes on disk versus say\na term-document matrix.",
    "start": "704350",
    "end": "708790"
  },
  {
    "text": "How well do you scale to\ndifferent collection sizes",
    "start": "708790",
    "end": "711172"
  },
  {
    "text": "in terms of the\nnumber of documents",
    "start": "711172",
    "end": "712630"
  },
  {
    "text": "or the size of the documents?",
    "start": "712630",
    "end": "714940"
  },
  {
    "text": "And how do you perform on\nthe different query loads?",
    "start": "714940",
    "end": "717250"
  },
  {
    "text": "Many queries, few queries,\nshort queries, long queries.",
    "start": "717250",
    "end": "720880"
  },
  {
    "text": "And lastly, of course, what\nsort of hardware do you require?",
    "start": "720880",
    "end": "723520"
  },
  {
    "text": "Is it just one CPU core?",
    "start": "723520",
    "end": "724930"
  },
  {
    "text": "Many cores?",
    "start": "724930",
    "end": "725800"
  },
  {
    "text": "A bunch of GPUs?",
    "start": "725800",
    "end": "727839"
  },
  {
    "text": "But latency tends to\nbe kind of, once you've",
    "start": "727840",
    "end": "729850"
  },
  {
    "text": "determined the other ones, it's\nthe go-to metric in most cases.",
    "start": "729850",
    "end": "733810"
  },
  {
    "start": "733810",
    "end": "736470"
  },
  {
    "text": "More central to our discussion\ntoday, and we'll focus on",
    "start": "736470",
    "end": "738889"
  },
  {
    "text": "this for the rest\nof the screencast",
    "start": "738890",
    "end": "741200"
  },
  {
    "text": "is IR effectiveness\nor the quality,",
    "start": "741200",
    "end": "743960"
  },
  {
    "text": "basically of an IR system.",
    "start": "743960",
    "end": "747620"
  },
  {
    "text": "And here we ask, do our\ntop-k rankings for a query",
    "start": "747620",
    "end": "751510"
  },
  {
    "text": "satisfy the users'\ninformation need?",
    "start": "751510",
    "end": "755100"
  },
  {
    "text": "Answering this question\ntends to be harder",
    "start": "755100",
    "end": "757949"
  },
  {
    "text": "than evaluation for typical\nmachine learning tasks,",
    "start": "757950",
    "end": "760530"
  },
  {
    "text": "like classification\nor regression",
    "start": "760530",
    "end": "763350"
  },
  {
    "text": "because we're not really just\ntaking an item and assigning it",
    "start": "763350",
    "end": "767130"
  },
  {
    "text": "a class.",
    "start": "767130",
    "end": "768180"
  },
  {
    "text": "We're trying to rank all\nof the items in our corpus",
    "start": "768180",
    "end": "771630"
  },
  {
    "text": "with respect to a query.",
    "start": "771630",
    "end": "774790"
  },
  {
    "text": "In practice, if you\nhave lots of users,",
    "start": "774790",
    "end": "776980"
  },
  {
    "text": "you could run online\nexperiments where you basically",
    "start": "776980",
    "end": "780579"
  },
  {
    "text": "give different versions of\nyour system to different users",
    "start": "780580",
    "end": "783760"
  },
  {
    "text": "and compare some metrics of\nsatisfaction or conversion,",
    "start": "783760",
    "end": "788770"
  },
  {
    "text": "basically in terms of\npurchases or otherwise.",
    "start": "788770",
    "end": "792340"
  },
  {
    "text": "But for research\npurposes, we're typically",
    "start": "792340",
    "end": "794530"
  },
  {
    "text": "interested in reusable\ntest collections.",
    "start": "794530",
    "end": "797530"
  },
  {
    "text": "That's collections that allow\nus to evaluate IR models offline",
    "start": "797530",
    "end": "800860"
  },
  {
    "text": "and then, compare them\nagainst each other.",
    "start": "800860",
    "end": "802810"
  },
  {
    "start": "802810",
    "end": "805700"
  },
  {
    "text": "Building a test collection\nentails three things.",
    "start": "805700",
    "end": "808787"
  },
  {
    "text": "First, we need to\ndecide on a document",
    "start": "808787",
    "end": "810370"
  },
  {
    "text": "collection for our corpus,\na set of test queries,",
    "start": "810370",
    "end": "814150"
  },
  {
    "text": "and we need to find or get or\nproduce relevant assessments",
    "start": "814150",
    "end": "818020"
  },
  {
    "text": "for each query.",
    "start": "818020",
    "end": "819820"
  },
  {
    "text": "If resources\npermit, a collection",
    "start": "819820",
    "end": "821590"
  },
  {
    "text": "could also include the\ntrain dev split of queries,",
    "start": "821590",
    "end": "825520"
  },
  {
    "text": "but given the high\nannotation cost,",
    "start": "825520",
    "end": "827470"
  },
  {
    "text": "it's actually not uncommon\nin IR to find or create",
    "start": "827470",
    "end": "830980"
  },
  {
    "text": "only a test set.",
    "start": "830980",
    "end": "833600"
  },
  {
    "text": "The key component\nof a test collection",
    "start": "833600",
    "end": "835310"
  },
  {
    "text": "is the relevance assessments.",
    "start": "835310",
    "end": "837200"
  },
  {
    "text": "These are basically\nhuman annotated labels",
    "start": "837200",
    "end": "839300"
  },
  {
    "text": "for each query that\nenumerate for us,",
    "start": "839300",
    "end": "842339"
  },
  {
    "text": "whether specific documents are\nrelevant or not to that query.",
    "start": "842340",
    "end": "847040"
  },
  {
    "text": "These query document\nassessments can either be binary",
    "start": "847040",
    "end": "850459"
  },
  {
    "text": "or they could take on a more\nfine grained graded nature.",
    "start": "850460",
    "end": "854970"
  },
  {
    "text": "An example of that is\ngrading a query document pair",
    "start": "854970",
    "end": "857240"
  },
  {
    "text": "as -1, 0, 1 or 2,\nwith meanings of hey,",
    "start": "857240",
    "end": "861589"
  },
  {
    "text": "this is a junk document, -1.",
    "start": "861590",
    "end": "863720"
  },
  {
    "text": "You should not retrieve it\nfor any query or this document",
    "start": "863720",
    "end": "866329"
  },
  {
    "text": "is irrelevant, but\nit might be useful",
    "start": "866330",
    "end": "869120"
  },
  {
    "text": "for other queries\nor this document",
    "start": "869120",
    "end": "870835"
  },
  {
    "text": "is quite relevant\nfor this query,",
    "start": "870835",
    "end": "872210"
  },
  {
    "text": "but it's not a perfect match.",
    "start": "872210",
    "end": "873830"
  },
  {
    "text": "Or here is a really,\nreally good match",
    "start": "873830",
    "end": "876920"
  },
  {
    "text": "for our query, which would\nbe a score of 2 or 3,",
    "start": "876920",
    "end": "879470"
  },
  {
    "text": "depending on the\ngrades that you're",
    "start": "879470",
    "end": "882740"
  },
  {
    "text": "using for the assessments.",
    "start": "882740",
    "end": "887260"
  },
  {
    "text": "As you might imagine,\nbecause we work",
    "start": "887260",
    "end": "889330"
  },
  {
    "text": "with potentially many\nmillions of documents,",
    "start": "889330",
    "end": "892130"
  },
  {
    "text": "it's usually infeasible to\njudge every single document",
    "start": "892130",
    "end": "895390"
  },
  {
    "text": "for every single query.",
    "start": "895390",
    "end": "897860"
  },
  {
    "text": "So instead, we're often\nforced to make the assumption",
    "start": "897860",
    "end": "900670"
  },
  {
    "text": "that unjudged documents\nare not relevant",
    "start": "900670",
    "end": "903880"
  },
  {
    "text": "or at least to ignore them\nin some metrics of IR.",
    "start": "903880",
    "end": "908080"
  },
  {
    "text": "Though for most purposes, they\nare treated as not relevant.",
    "start": "908080",
    "end": "911260"
  },
  {
    "text": "Some test collections take\nthis further and only label",
    "start": "911260",
    "end": "914260"
  },
  {
    "text": "one or two key documents\nper query as relevant",
    "start": "914260",
    "end": "917230"
  },
  {
    "text": "and assume everything\nelse is not relevant.",
    "start": "917230",
    "end": "920511"
  },
  {
    "text": "So this tends to\nbe useful when you",
    "start": "920512",
    "end": "921970"
  },
  {
    "text": "work with particular data sets.",
    "start": "921970",
    "end": "923839"
  },
  {
    "text": "And you want to keep it in\nmind as you do evaluation.",
    "start": "923840",
    "end": "926251"
  },
  {
    "start": "926251",
    "end": "930019"
  },
  {
    "start": "927000",
    "end": "991000"
  },
  {
    "text": "So many of the test\ncollections out there in IR",
    "start": "930020",
    "end": "932790"
  },
  {
    "text": "are annotated by TREC or the\nText Retrieval Conference,",
    "start": "932790",
    "end": "936529"
  },
  {
    "text": "which includes annual tracks\nfor competing and comparing",
    "start": "936530",
    "end": "940580"
  },
  {
    "text": "IR systems.",
    "start": "940580",
    "end": "942320"
  },
  {
    "text": "For instance, the\n2021 TREC Conference",
    "start": "942320",
    "end": "945230"
  },
  {
    "text": "has tracks for\nsearch in the context",
    "start": "945230",
    "end": "947000"
  },
  {
    "text": "of conversational assistance,\nhealth misinformation,",
    "start": "947000",
    "end": "950600"
  },
  {
    "text": "fair ranking, and has a\nvery popular deep learning",
    "start": "950600",
    "end": "954259"
  },
  {
    "text": "track, as well, which we'll\ndiscuss in more detail.",
    "start": "954260",
    "end": "957610"
  },
  {
    "text": "Each TREC campaign\nemphasizes careful evaluation",
    "start": "957610",
    "end": "960730"
  },
  {
    "text": "with a very small\nset of queries.",
    "start": "960730",
    "end": "962690"
  },
  {
    "text": "So just 50 queries is a\nvery typical size, actually.",
    "start": "962690",
    "end": "966970"
  },
  {
    "text": "But TREC extensively judges\nmany, many documents,",
    "start": "966970",
    "end": "969819"
  },
  {
    "text": "possibly hundreds of documents\nor even more for each query",
    "start": "969820",
    "end": "972770"
  },
  {
    "text": "here.",
    "start": "972770",
    "end": "973270"
  },
  {
    "start": "973270",
    "end": "975840"
  },
  {
    "text": "So you can imagine\nan alternative,",
    "start": "975840",
    "end": "977340"
  },
  {
    "text": "which we'll look at next where\nyou have lots of queries.",
    "start": "977340",
    "end": "980220"
  },
  {
    "text": "But you only judge\na very small number",
    "start": "980220",
    "end": "981889"
  },
  {
    "text": "of key documents for those\nqueries with the intention",
    "start": "981890",
    "end": "986750"
  },
  {
    "text": "that the performance\nthat you get",
    "start": "986750",
    "end": "988430"
  },
  {
    "text": "will average out over a\nlarge enough set of queries.",
    "start": "988430",
    "end": "991730"
  },
  {
    "start": "991000",
    "end": "1092000"
  },
  {
    "text": "And this is exactly what\nhappens in MS MARCO Ranking",
    "start": "991730",
    "end": "994579"
  },
  {
    "text": "Tasks, which is a collection\nof really popular IR",
    "start": "994580",
    "end": "1001480"
  },
  {
    "text": "benchmarks by Microsoft.",
    "start": "1001480",
    "end": "1004149"
  },
  {
    "text": "And this MARCO contains more\nthan half a million Bing search",
    "start": "1004150",
    "end": "1007000"
  },
  {
    "text": "queries.",
    "start": "1007000",
    "end": "1007660"
  },
  {
    "text": "And this is the largest\npublic IR benchmark.",
    "start": "1007660",
    "end": "1011500"
  },
  {
    "text": "Each query here is assessed with\none or two relevant documents,",
    "start": "1011500",
    "end": "1016030"
  },
  {
    "text": "and we assume everything\nelse is not relevant,",
    "start": "1016030",
    "end": "1019700"
  },
  {
    "text": "and having this\nsparse annotation",
    "start": "1019700",
    "end": "1022600"
  },
  {
    "text": "is often not a problem at all\nfor training because we have",
    "start": "1022600",
    "end": "1025780"
  },
  {
    "text": "so many training instances.",
    "start": "1025780",
    "end": "1028030"
  },
  {
    "text": "And so, MS MARCO provides a\ntremendous resource for us",
    "start": "1028030",
    "end": "1031930"
  },
  {
    "text": "when it comes to building\nand training IR models,",
    "start": "1031930",
    "end": "1034730"
  },
  {
    "text": "especially in the neural domain.",
    "start": "1034730",
    "end": "1037369"
  },
  {
    "text": "It also turns out that\nsparse labels are not",
    "start": "1037369",
    "end": "1039339"
  },
  {
    "text": "too bad for evaluation, either.",
    "start": "1039339",
    "end": "1041290"
  },
  {
    "text": "Especially because of the\nsize of the test queries,",
    "start": "1041290",
    "end": "1044410"
  },
  {
    "text": "we can use many thousands\nof test queries, an average",
    "start": "1044410",
    "end": "1047050"
  },
  {
    "text": "of results across all of them\nto get a pretty reliable signal",
    "start": "1047050",
    "end": "1050140"
  },
  {
    "text": "about how different\nsystems compare.",
    "start": "1050140",
    "end": "1053075"
  },
  {
    "text": "There are multiple test\ncollections out there",
    "start": "1053075",
    "end": "1054950"
  },
  {
    "text": "on top of MS MARCO.",
    "start": "1054950",
    "end": "1056840"
  },
  {
    "text": "And so, there's the\noriginal passage ranking",
    "start": "1056840",
    "end": "1058970"
  },
  {
    "text": "task and newer\ndocument ranking task",
    "start": "1058970",
    "end": "1061730"
  },
  {
    "text": "where the documents\nare much longer,",
    "start": "1061730",
    "end": "1064220"
  },
  {
    "text": "but there's fewer of them.",
    "start": "1064220",
    "end": "1066080"
  },
  {
    "text": "And then, there is also a\ntrack, the deep learning track,",
    "start": "1066080",
    "end": "1068570"
  },
  {
    "text": "which we've mentioned\nbefore, which",
    "start": "1068570",
    "end": "1070039"
  },
  {
    "text": "is happening every\nyear since 2019",
    "start": "1070040",
    "end": "1073110"
  },
  {
    "text": "and which uses the MS\nMARCO data, especially",
    "start": "1073110",
    "end": "1075920"
  },
  {
    "text": "for training, mostly.",
    "start": "1075920",
    "end": "1077780"
  },
  {
    "text": "But has far fewer queries\nfor testing with lots",
    "start": "1077780",
    "end": "1081470"
  },
  {
    "text": "more labels for evaluation,\na lot more extensive",
    "start": "1081470",
    "end": "1086210"
  },
  {
    "text": "assessments and\njudgments for evaluation,",
    "start": "1086210",
    "end": "1088710"
  },
  {
    "text": "so these are much denser labels.",
    "start": "1088710",
    "end": "1092789"
  },
  {
    "start": "1092000",
    "end": "1201000"
  },
  {
    "text": "There are also plenty of other\nrather domain specific IR",
    "start": "1092790",
    "end": "1095850"
  },
  {
    "text": "benchmarks, many of which\nare collected in this table",
    "start": "1095850",
    "end": "1101070"
  },
  {
    "text": "by Nandan et al.",
    "start": "1101070",
    "end": "1102659"
  },
  {
    "text": "in a very recent preprint.",
    "start": "1102660",
    "end": "1106160"
  },
  {
    "text": "As you can see, these\nbenchmarks vary greatly",
    "start": "1106160",
    "end": "1108410"
  },
  {
    "text": "in terms of the\ntraining size, if there",
    "start": "1108410",
    "end": "1110810"
  },
  {
    "text": "is any training at all.",
    "start": "1110810",
    "end": "1112720"
  },
  {
    "text": "The test set size, the\naverage query length,",
    "start": "1112720",
    "end": "1115010"
  },
  {
    "text": "the average document length,\nand many other factors.",
    "start": "1115010",
    "end": "1119470"
  },
  {
    "text": "BEIR, or benchmarking for IR\nis a recent effort by Nandan",
    "start": "1119470",
    "end": "1124210"
  },
  {
    "text": "et al. here to use all\nof these different data",
    "start": "1124210",
    "end": "1127360"
  },
  {
    "text": "sets for zero shot or out of\ndomain testing of IR models.",
    "start": "1127360",
    "end": "1131740"
  },
  {
    "text": "Specifically, in BEIR, we\ntake already trained IR models",
    "start": "1131740",
    "end": "1135429"
  },
  {
    "text": "that do not have access to any\nvalidation or training data",
    "start": "1135430",
    "end": "1138790"
  },
  {
    "text": "on these downstream IR tasks\nand test them out of the box",
    "start": "1138790",
    "end": "1142840"
  },
  {
    "text": "to observe their out of\ndomain retrieval quality,",
    "start": "1142840",
    "end": "1145779"
  },
  {
    "text": "so that is without training\non these new domains.",
    "start": "1145780",
    "end": "1149545"
  },
  {
    "start": "1149545",
    "end": "1156230"
  },
  {
    "text": "OK, so we now have\na test collection",
    "start": "1156230",
    "end": "1158870"
  },
  {
    "text": "with queries, documents,\nand assessments.",
    "start": "1158870",
    "end": "1162300"
  },
  {
    "text": "How do we compare IR\nsystems in this collection?",
    "start": "1162300",
    "end": "1165970"
  },
  {
    "text": "First, we will ask each IR\nsystem to produce its Top-K",
    "start": "1165970",
    "end": "1169240"
  },
  {
    "text": "ranking, say its top 10 results.",
    "start": "1169240",
    "end": "1171760"
  },
  {
    "text": "And we'll use an IR metric to\ncompare all of these systems",
    "start": "1171760",
    "end": "1175480"
  },
  {
    "text": "at that cutoff K.",
    "start": "1175480",
    "end": "1177580"
  },
  {
    "text": "The choice of IR\nmetric and the cutoff K",
    "start": "1177580",
    "end": "1179799"
  },
  {
    "text": "will depend entirely\non the task,",
    "start": "1179800",
    "end": "1181990"
  },
  {
    "text": "so I will briefly motivate each\nmetric as we go through them.",
    "start": "1181990",
    "end": "1186390"
  },
  {
    "text": "All of the metrics\nwe will go through",
    "start": "1186390",
    "end": "1188190"
  },
  {
    "text": "are simply averaged\nacross all queries.",
    "start": "1188190",
    "end": "1190830"
  },
  {
    "text": "And so, to keep\nthings simple, I will",
    "start": "1190830",
    "end": "1192982"
  },
  {
    "text": "show the computation\nof the metric",
    "start": "1192983",
    "end": "1194400"
  },
  {
    "text": "for just one query\nin each case, but you",
    "start": "1194400",
    "end": "1196500"
  },
  {
    "text": "want to keep in mind that this\nis averaged across queries.",
    "start": "1196500",
    "end": "1201530"
  },
  {
    "start": "1201000",
    "end": "1273000"
  },
  {
    "text": "Let us start with two of\nthe simplest IR metrics,",
    "start": "1201530",
    "end": "1205130"
  },
  {
    "text": "which are Success and MRR.",
    "start": "1205130",
    "end": "1207380"
  },
  {
    "text": "For a given query, let\nrank be the position",
    "start": "1207380",
    "end": "1210890"
  },
  {
    "text": "of the first relevant\ndocument that we can see",
    "start": "1210890",
    "end": "1213320"
  },
  {
    "text": "in the Top-K list of results.",
    "start": "1213320",
    "end": "1216059"
  },
  {
    "text": "Success@K will\njust be 1, if there",
    "start": "1216060",
    "end": "1218820"
  },
  {
    "text": "is a relevant result in the\nTop-K list, and 0 otherwise.",
    "start": "1218820",
    "end": "1223110"
  },
  {
    "text": "This is a very simple\nmetric, as you can see,",
    "start": "1223110",
    "end": "1225240"
  },
  {
    "text": "that can be useful\nin cases where",
    "start": "1225240",
    "end": "1227190"
  },
  {
    "text": "we assume that the\nuser just needs",
    "start": "1227190",
    "end": "1228840"
  },
  {
    "text": "one relevant result\nanywhere in the Top-K.",
    "start": "1228840",
    "end": "1231720"
  },
  {
    "text": "And in particular,\nit can be useful",
    "start": "1231720",
    "end": "1233490"
  },
  {
    "text": "if our retrieval is spread\nto a downstream model that",
    "start": "1233490",
    "end": "1236910"
  },
  {
    "text": "looks at the Top-K results and\nthen does something with them.",
    "start": "1236910",
    "end": "1239770"
  },
  {
    "text": "So you can read all of them,\nand it would read all of them",
    "start": "1239770",
    "end": "1242187"
  },
  {
    "text": "anyway, so we're just interested\nin buying the irrelevance here.",
    "start": "1242187",
    "end": "1246430"
  },
  {
    "text": "Mean reciprocal rank or MRR\nalso assumes that these only",
    "start": "1246430",
    "end": "1251470"
  },
  {
    "text": "needs one relevant\nquery in the Top-K,",
    "start": "1251470",
    "end": "1254640"
  },
  {
    "text": "but it assumes that\nthe user does care",
    "start": "1254640",
    "end": "1257277"
  },
  {
    "text": "about the position of\nthat relevant document",
    "start": "1257277",
    "end": "1259110"
  },
  {
    "text": "in the ranking.",
    "start": "1259110",
    "end": "1260760"
  },
  {
    "text": "So a relevant document at the\nsecond position for example,",
    "start": "1260760",
    "end": "1263950"
  },
  {
    "text": "is only given half of the\nweight of a relevant document",
    "start": "1263950",
    "end": "1266760"
  },
  {
    "text": "in the top position.",
    "start": "1266760",
    "end": "1267615"
  },
  {
    "start": "1267615",
    "end": "1276650"
  },
  {
    "start": "1273000",
    "end": "1310000"
  },
  {
    "text": "You're probably already familiar\nwith precision and recall,",
    "start": "1276650",
    "end": "1279760"
  },
  {
    "text": "but let's define them here\nin the context of Top-K",
    "start": "1279760",
    "end": "1282580"
  },
  {
    "text": "ranked retrieval.",
    "start": "1282580",
    "end": "1284309"
  },
  {
    "text": "For a given query, let Ret\nof K be the Top-K retrieved",
    "start": "1284310",
    "end": "1288540"
  },
  {
    "text": "documents, that set of\nTop-K retrieved documents.",
    "start": "1288540",
    "end": "1291570"
  },
  {
    "text": "And let Rel be the\nset of all documents",
    "start": "1291570",
    "end": "1293789"
  },
  {
    "text": "that we judged as relevance\nas part of our assessments.",
    "start": "1293790",
    "end": "1299140"
  },
  {
    "text": "In this case,\nPrecision@K is just",
    "start": "1299140",
    "end": "1301180"
  },
  {
    "text": "a fraction of the\nretrieved items",
    "start": "1301180",
    "end": "1302770"
  },
  {
    "text": "that are actually relevant.",
    "start": "1302770",
    "end": "1304750"
  },
  {
    "text": "And recall that\nK is the fraction",
    "start": "1304750",
    "end": "1306640"
  },
  {
    "text": "of all the relevant items\nthat are actually retrieved.",
    "start": "1306640",
    "end": "1309550"
  },
  {
    "start": "1309550",
    "end": "1313870"
  },
  {
    "start": "1310000",
    "end": "1427000"
  },
  {
    "text": "A pretty popular metric is also\na MAP or mean average precision",
    "start": "1313870",
    "end": "1317710"
  },
  {
    "text": "or just average precision\nfor one query, which",
    "start": "1317710",
    "end": "1320590"
  },
  {
    "text": "essentially brings together\nnotions from both precision",
    "start": "1320590",
    "end": "1323140"
  },
  {
    "text": "and recall.",
    "start": "1323140",
    "end": "1324430"
  },
  {
    "text": "To compute average\nprecision for one query,",
    "start": "1324430",
    "end": "1326950"
  },
  {
    "text": "we will add up the Precision@i\nfor every position i from 1",
    "start": "1326950",
    "end": "1331149"
  },
  {
    "text": "through K, where the i-th\ndocument is relevant.",
    "start": "1331150",
    "end": "1334900"
  },
  {
    "text": "We will divide\nthis whole quantity",
    "start": "1334900",
    "end": "1336730"
  },
  {
    "text": "by the total number of documents\nthat we're judged as relevant",
    "start": "1336730",
    "end": "1339460"
  },
  {
    "text": "for this query.",
    "start": "1339460",
    "end": "1340120"
  },
  {
    "start": "1340120",
    "end": "1343150"
  },
  {
    "text": "All of the metrics that we've\nconsidered so far only interact",
    "start": "1343150",
    "end": "1346090"
  },
  {
    "text": "with binary relevance,\nthat is they",
    "start": "1346090",
    "end": "1348070"
  },
  {
    "text": "just care whether each\ndocument that is retrieved",
    "start": "1348070",
    "end": "1350559"
  },
  {
    "text": "is considered relevant\nor not relevant.",
    "start": "1350560",
    "end": "1353260"
  },
  {
    "text": "DCG or discounted\ncumulative gain",
    "start": "1353260",
    "end": "1356200"
  },
  {
    "text": "works with graded relevance.",
    "start": "1356200",
    "end": "1357639"
  },
  {
    "text": "So for instance, 0, 1, 2, and 3.",
    "start": "1357640",
    "end": "1361520"
  },
  {
    "text": "For each position in the\nranking from 1 through K,",
    "start": "1361520",
    "end": "1364100"
  },
  {
    "text": "we will divide the\ngraded relevance",
    "start": "1364100",
    "end": "1366080"
  },
  {
    "text": "of the retrieved\ndocument at that position",
    "start": "1366080",
    "end": "1369140"
  },
  {
    "text": "by the logarithm of the\nposition, which essentially",
    "start": "1369140",
    "end": "1371960"
  },
  {
    "text": "discounts the value\nof a relevant document",
    "start": "1371960",
    "end": "1374330"
  },
  {
    "text": "if it appears late\nin the ranking.",
    "start": "1374330",
    "end": "1378419"
  },
  {
    "text": "Unlike the other\nmetrics, the maximum DCG",
    "start": "1378420",
    "end": "1380760"
  },
  {
    "text": "is often not equal to 1.",
    "start": "1380760",
    "end": "1383610"
  },
  {
    "text": "So we can also compute\nnormalized DCG or NDCG",
    "start": "1383610",
    "end": "1387809"
  },
  {
    "text": "by dividing for each\nquery by the ideal DCG.",
    "start": "1387810",
    "end": "1391590"
  },
  {
    "text": "This is obtained basically, if\nall of the relevant documents",
    "start": "1391590",
    "end": "1395130"
  },
  {
    "text": "are at the top of\nour Top-K ranking,",
    "start": "1395130",
    "end": "1397350"
  },
  {
    "text": "and they are sorted by\ndecreasing relevance,",
    "start": "1397350",
    "end": "1400890"
  },
  {
    "text": "so all of the 2's before all of\nthe 1's before all of the 0's",
    "start": "1400890",
    "end": "1404730"
  },
  {
    "text": "in this case that\nare not relevant.",
    "start": "1404730",
    "end": "1408580"
  },
  {
    "text": "All right, having discussed\nclassical IR and evaluation",
    "start": "1408580",
    "end": "1411480"
  },
  {
    "text": "in this screencast, we\nwill focus on Neural IR",
    "start": "1411480",
    "end": "1414240"
  },
  {
    "text": "and in particular,\nstate of the art",
    "start": "1414240",
    "end": "1415710"
  },
  {
    "text": "IR models that use what\nwe've learned so far in NLU",
    "start": "1415710",
    "end": "1419250"
  },
  {
    "text": "in the next screencast.",
    "start": "1419250",
    "end": "1421970"
  },
  {
    "start": "1421970",
    "end": "1427000"
  }
]