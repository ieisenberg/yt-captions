[
  {
    "start": "0",
    "end": "338000"
  },
  {
    "text": "What I'd like to do today is continue our discussion of supervised learning.",
    "start": "3380",
    "end": "10590"
  },
  {
    "text": "So last Wednesday, you saw the linear regression algorithm, uh, including both gradient descent, how to formulate the problem,",
    "start": "10590",
    "end": "19185"
  },
  {
    "text": "then gradient descent, and then the normal equations. What I'd like to do today is, um,",
    "start": "19185",
    "end": "25080"
  },
  {
    "text": "talk about locally weighted regression which is a, a way to modify linear regressions and make it",
    "start": "25080",
    "end": "30750"
  },
  {
    "text": "fit very non-linear functions so you aren't just fitting straight lines. And then I'll talk about a probabilistic interpretation of linear regression and that will",
    "start": "30750",
    "end": "39110"
  },
  {
    "text": "lead us into the first classification algorithm you see in this class called logistic regression,",
    "start": "39110",
    "end": "44300"
  },
  {
    "text": "and we'll talk about an algorithm called Newton's method for logistic regression. And so the dependency of ideas in this class is that,",
    "start": "44300",
    "end": "52160"
  },
  {
    "text": "um, locally weighted regression will depend on what you learned in linear regression.",
    "start": "52160",
    "end": "57200"
  },
  {
    "text": "And then, um, we're actually gonna just cover the key ideas of locally weighted regression,",
    "start": "57200",
    "end": "63070"
  },
  {
    "text": "and let you play with some of the ideas yourself in the, um, problem set 1 which we'll release later this week.",
    "start": "63070",
    "end": "68600"
  },
  {
    "text": "And then, um, I guess, give a probabilistic interpretation of linear regression, logistic regression depend on that, um,",
    "start": "68600",
    "end": "75875"
  },
  {
    "text": "and Newton's method is for logistic regression, okay? To recap the notation you saw on Wednesday,",
    "start": "75875",
    "end": "83675"
  },
  {
    "text": "we use this notation x_i, i- y_i to denote a single training example where x_i was n + 1 dimensional.",
    "start": "83675",
    "end": "93185"
  },
  {
    "text": "So if you have two features, the size of the house and the number of bedrooms, then x_i would be 2 + 1, it would 3-dimensional because we have introduced a new,",
    "start": "93185",
    "end": "103159"
  },
  {
    "text": "uh, sort of fake feature x_0 which was always set to the value of 1. Uh, and then yi,",
    "start": "103160",
    "end": "109535"
  },
  {
    "text": "in the case of regression is always a real number and what's the number of training examples and what's the number of features",
    "start": "109535",
    "end": "116000"
  },
  {
    "text": "and, uh, this was the hypothesis, right? It's a linear function of the features x, um,",
    "start": "116000",
    "end": "122430"
  },
  {
    "text": "including this feature x_0 which is always set to 1, and, uh, j was the cost function you would minimize,",
    "start": "122430",
    "end": "129804"
  },
  {
    "text": "you minimize this as function of j to find the parameters Theta for your straight line fit to the data, okay?",
    "start": "129804",
    "end": "137190"
  },
  {
    "text": "So that's what you saw last Wednesday. Um, now if you have a dataset, that looks like that,",
    "start": "137190",
    "end": "150910"
  },
  {
    "text": "where this is the size of a house and this is the price of a house. What you saw on Wednesday- last Wednesday,",
    "start": "150910",
    "end": "158195"
  },
  {
    "text": "was an algorithm to fit a straight line, right to this data so the hypothesis was of the form",
    "start": "158195",
    "end": "165110"
  },
  {
    "text": "Theta 0 + Theta 1 x_0- x_0- Theta 1 x_1 to be very specific.",
    "start": "165110",
    "end": "170405"
  },
  {
    "text": "But with this dataset maybe it actually looks, you know, maybe the data looks a little bit like that and so",
    "start": "170405",
    "end": "177380"
  },
  {
    "text": "one question that you have to address when, uh, fitting models to data is what are the features you want?",
    "start": "177380",
    "end": "183750"
  },
  {
    "text": "Do you want to fit a straight line to this problem or do you want to fit a hypothesis of the form, um,",
    "start": "183750",
    "end": "190425"
  },
  {
    "text": "Theta 1x + Theta 2x squared since this may be a quadratic function, right?",
    "start": "190425",
    "end": "199020"
  },
  {
    "text": "Now the problem with quadratic functions is that quadratic functions eventually start, you know, curving back down so that will be a quadratic function.",
    "start": "199020",
    "end": "205430"
  },
  {
    "text": "This arc starts curving back down. So maybe you don't want to fit a quadratic function. Uh, instead maybe you want, uh, to fit something like that.",
    "start": "205430",
    "end": "217260"
  },
  {
    "text": "If- if housing prices sort of curved down a little bit but you don't want it to eventually curve back down the way a quadratic function would, right?",
    "start": "217260",
    "end": "226055"
  },
  {
    "text": "Um, so- oh, and, and if you want to do this the way you would implement is",
    "start": "226055",
    "end": "231079"
  },
  {
    "text": "is you define the first feature x_1 = x and the second feature x_2 = x squared or you",
    "start": "231080",
    "end": "237620"
  },
  {
    "text": "define x_1 to be equal to x and x_2 = square root of x, right and by defining a new feature x_2 which would be the square root of x and square root of x.",
    "start": "237620",
    "end": "246710"
  },
  {
    "text": "Then the machinery that you saw from Wednesday of linear regression applies to fit these types of,",
    "start": "246710",
    "end": "252549"
  },
  {
    "text": "um, these types of functions of the data. So later this quarter you'll hear about feature selection algorithms,",
    "start": "252550",
    "end": "260000"
  },
  {
    "text": "which is a type of algorithm for automatically deciding, do you want x squared as a feature or",
    "start": "260000",
    "end": "265639"
  },
  {
    "text": "square root of x as a feature or maybe you want, um, log of x as a feature, right.",
    "start": "265640",
    "end": "271280"
  },
  {
    "text": "But what set of features, um, does the best job fitting the data that you have if it's not fit well by a perfectly straight line.",
    "start": "271280",
    "end": "279830"
  },
  {
    "text": "Um, what I would like to do today is- so, so you'll hear about feature selection later this quarter.",
    "start": "279830",
    "end": "285530"
  },
  {
    "text": "What I want to share with you today is a different way of addressing this out- this problem of whether the data isn't just fit well by",
    "start": "285530",
    "end": "292100"
  },
  {
    "text": "a straight line and in particular I wanna share with you an idea called, uh, locally weighted regression or locally weighted linear regression.",
    "start": "292100",
    "end": "298910"
  },
  {
    "text": "So let me use a slightly different, um, example to illustrate this.",
    "start": "298910",
    "end": "304090"
  },
  {
    "text": "Um, which is, uh, which is that, you know, if you have a dataset that looks like that.",
    "start": "304090",
    "end": "309320"
  },
  {
    "text": "[NOISE] So it's pretty clear what the shape of this data is.",
    "start": "309320",
    "end": "321110"
  },
  {
    "text": "Um, but how do you fit a curve that, you know, kind of looks like that, right? And it's, it's actually quite difficult to find features, is it square root of x,",
    "start": "321110",
    "end": "329370"
  },
  {
    "text": "log of x, x cube, like third root of x, x the power of 2/3. But what is the set of features that lets you do this? So",
    "start": "329370",
    "end": "335629"
  },
  {
    "text": "we'll sidestep all those problems with an algorithm called, uh, locally weighted regression.",
    "start": "335630",
    "end": "342180"
  },
  {
    "start": "353000",
    "end": "1292000"
  },
  {
    "text": "Um, okay,  and to introduce a bit more machine learning terminology.",
    "start": "353300",
    "end": "358895"
  },
  {
    "text": "Um, in machine learning we sometimes distinguish between parametric learning algorithms and non-parametric learning algorithms.",
    "start": "358895",
    "end": "368639"
  },
  {
    "text": "But in a parametric learning algorithm does, uh, uh, you fit some fixed set of parameters such as Theta",
    "start": "370030",
    "end": "383220"
  },
  {
    "text": "i to data and so linear regression as you saw last Wednesday is a parametric learning algorithm",
    "start": "383220",
    "end": "391400"
  },
  {
    "text": "because there's a fixed set of parameters, the Theta i's, so you fit to data and then you're done, right.",
    "start": "391400",
    "end": "396415"
  },
  {
    "text": "Locally weighted regression will be our first exposure to a non-parametric learning algorithm.",
    "start": "396415",
    "end": "409100"
  },
  {
    "text": "Um, and what that means is that the amount of data/parameters, uh,",
    "start": "409100",
    "end": "418290"
  },
  {
    "text": "you need to keep grows and in",
    "start": "418360",
    "end": "427669"
  },
  {
    "text": "this case it grows linearly with the size of the data,",
    "start": "427670",
    "end": "436070"
  },
  {
    "text": "with size of training set, okay? So with the parametric learning algorithm, no matter how big your training, uh,",
    "start": "436070",
    "end": "443280"
  },
  {
    "text": "your training set is, you fit the parameters Theta i. Then you could erase a training set from",
    "start": "443280",
    "end": "448460"
  },
  {
    "text": "your computer memory and make predictions just using the parameters Theta i and in a non-parametric learning algorithm which we'll see in a second,",
    "start": "448460",
    "end": "455449"
  },
  {
    "text": "the amount of stuff you need to keep around in computer memory or the amount of stuff you need to store around grows linearly as a function of the training set size.",
    "start": "455449",
    "end": "463880"
  },
  {
    "text": "Uh, and so this type of algorithm is your- may, may, may not be great if you have a really, really massive dataset because you need to keep all of the data around",
    "start": "463880",
    "end": "471595"
  },
  {
    "text": "your- in computer memory or on disk just to make predictions, okay? So- but we'll see an example of this",
    "start": "471595",
    "end": "477575"
  },
  {
    "text": "and one of the effects of this is that with that, it'll, it'll be able to fit that data that I drew up there, uh,",
    "start": "477575",
    "end": "484010"
  },
  {
    "text": "quite well without you needing to fiddle manually with features. Um, and again you get to practice implementing locally weighted regression in the homework.",
    "start": "484010",
    "end": "493760"
  },
  {
    "text": "So I'm gonna go over the height of ideas relatively quickly and then let you, uh, uh, gain practice, uh, in the problem set. All right.",
    "start": "493760",
    "end": "502790"
  },
  {
    "text": "So let me redraw that dataset, it'd be something like this. [NOISE] All right.",
    "start": "502790",
    "end": "510405"
  },
  {
    "text": "So- so say you have a dataset like this. Um, now for linear regression if you want to evaluate",
    "start": "510405",
    "end": "519890"
  },
  {
    "text": "h at a certain value of the input, right?",
    "start": "521080",
    "end": "532400"
  },
  {
    "text": "So to make a prediction at a certain value of x what you- for linear regression what you do is you fit theta,",
    "start": "532400",
    "end": "542070"
  },
  {
    "text": "you know, to minimize this cost function. [NOISE]",
    "start": "543490",
    "end": "556715"
  },
  {
    "text": "And then you return Theta transpose x, right? So you fit a straight line and then, you know,",
    "start": "556715",
    "end": "563380"
  },
  {
    "text": "if you want to make a prediction at this value x you then return say the transpose x.",
    "start": "563380",
    "end": "569065"
  },
  {
    "text": "For locally weighted regression,",
    "start": "569065",
    "end": "572870"
  },
  {
    "text": "um, you do something slightly different. Which is if this is the value of x",
    "start": "579750",
    "end": "586660"
  },
  {
    "text": "and you want to make a prediction around that value of x. What you do is you look in a lo- local neighborhood",
    "start": "586660",
    "end": "592449"
  },
  {
    "text": "at the training examples close to that point x where you want to make a prediction. And then, um, I'll describe this informally for now",
    "start": "592449",
    "end": "600910"
  },
  {
    "text": "but we'll- we'll formalize this in math for the second. Um, but focusing mainly on these examples and,",
    "start": "600910",
    "end": "607420"
  },
  {
    "text": "you know, looking a little bit at further all the examples. But really focusing mainly on these examples, you try to fit a straight line like that,",
    "start": "607420",
    "end": "615625"
  },
  {
    "text": "focusing on the training examples that are close to where you want to make a prediction. And by close I mean the values are similar, uh, on the x axis.",
    "start": "615625",
    "end": "624310"
  },
  {
    "text": "The x values are similar. And then to actually make a prediction, you will, uh,",
    "start": "624310",
    "end": "630774"
  },
  {
    "text": "use this green line that you just fit to make a prediction at that value of x, okay?",
    "start": "630775",
    "end": "637465"
  },
  {
    "text": "Now if you want to make a prediction at a different point. Um, let's say that, you know,",
    "start": "637465",
    "end": "643209"
  },
  {
    "text": "the user now says, \"Hey, make a prediction for this point.\" Then what you would do is you focus on this local area,",
    "start": "643210",
    "end": "650170"
  },
  {
    "text": "kinda look at those points. Um, and when I say focus say, you know, put most of the weights on these points but you",
    "start": "650170",
    "end": "655660"
  },
  {
    "text": "kinda take a glance at the points further away, but mostly the attention is on these for the straight line to that,",
    "start": "655660",
    "end": "661540"
  },
  {
    "text": "and then you use that straight line to make a prediction, okay. Um, and so to formalize this in locally weighted regression, um,",
    "start": "661540",
    "end": "675145"
  },
  {
    "text": "you will fit Theta to minimize a modified cost function",
    "start": "675145",
    "end": "681460"
  },
  {
    "text": "[NOISE]",
    "start": "681460",
    "end": "694000"
  },
  {
    "text": "Where wi is a weight function.",
    "start": "694000",
    "end": "697370"
  },
  {
    "text": "Um, and so a good- well the default choice, a common choice for wi will be this.",
    "start": "703440",
    "end": "709080"
  },
  {
    "text": "[NOISE] Right, um, I'm gonna add something to this equation a little bit later.",
    "start": "709080",
    "end": "719845"
  },
  {
    "text": "But, uh, wi is a weighting function where, notice that this, this formula has a defining property, right?",
    "start": "719845",
    "end": "729024"
  },
  {
    "text": "If xi - x is small, then the weight will be close to 1.",
    "start": "729025",
    "end": "739180"
  },
  {
    "text": "Because, uh, if xi x- so x is the location where you want to make a prediction and xi is the input x for your ith training example.",
    "start": "739180",
    "end": "750175"
  },
  {
    "text": "So wi is a weighting function, um, that's a value between 0 and 1 that tells you how much should you pay attention to",
    "start": "750175",
    "end": "759335"
  },
  {
    "text": "the values of xi, yi when fitting say this green line or that red line.",
    "start": "759335",
    "end": "765865"
  },
  {
    "text": "And so if xi - x is small",
    "start": "765865",
    "end": "771010"
  },
  {
    "text": "so that's a training example that is close to where you want to make the prediction for x. Then this is about e to the 0, right,",
    "start": "771010",
    "end": "779005"
  },
  {
    "text": "e to the -0 if the- if the numerator here is small and e to the 0 is close to 1.",
    "start": "779005",
    "end": "785425"
  },
  {
    "text": "Right, um, and conversely if xi - x is large,",
    "start": "785425",
    "end": "793000"
  },
  {
    "text": "then wi is close to 0. And so if xi is very far away so let's see if it's fitting this green line.",
    "start": "793000",
    "end": "802449"
  },
  {
    "text": "And this is your example, xi yi then it's saying, give this",
    "start": "802450",
    "end": "808210"
  },
  {
    "text": "example all the way out there if you're fitting the green line where you look at this first x saying that example should have weight fairly close to 0, okay?",
    "start": "808210",
    "end": "818410"
  },
  {
    "text": "Um, and so if you, um, look at the cost function,",
    "start": "818410",
    "end": "826329"
  },
  {
    "text": "the main modification to the cost function we've made is that we've added this weighting term, right?",
    "start": "826329",
    "end": "834175"
  },
  {
    "text": "And so what locally weighted regression does is the same. If an example xi is far from where you wanna make",
    "start": "834175",
    "end": "842320"
  },
  {
    "text": "a prediction multiply that error term by 0 or by a constant very close to 0.",
    "start": "842320",
    "end": "848895"
  },
  {
    "text": "Um, whereas if it's close to where you wanna make a prediction multiply that error term by 1.",
    "start": "848895",
    "end": "854675"
  },
  {
    "text": "And so the net effect of this is that this is summing if, if, you know, the terms multiplied by 0 disappear, right?",
    "start": "854675",
    "end": "861130"
  },
  {
    "text": "So the net effect of this is that the sums over essentially only the terms, uh,",
    "start": "861130",
    "end": "867805"
  },
  {
    "text": "for the squared error for the examples that are close to the value, close to the value of x where you want to make a prediction, okay?",
    "start": "867805",
    "end": "876925"
  },
  {
    "text": "Um, and that's why when you fit Theta to minimize this,",
    "start": "876925",
    "end": "885190"
  },
  {
    "text": "you end up paying attention only to the points, only to the examples close to where you wanna make a prediction and",
    "start": "885190",
    "end": "891730"
  },
  {
    "text": "fitting a line like a green line over there, okay? Um, so let me draw a couple more pictures to- to- to illustrate this.",
    "start": "891730",
    "end": "900985"
  },
  {
    "text": "Um, so if- let me draw a slightly smaller data set just to make this easier to illustrate.",
    "start": "900985",
    "end": "908935"
  },
  {
    "text": "Um, so that's your training set. So there's your examples x1, x2, x3, x4. And if you want to make a prediction here, right,",
    "start": "908935",
    "end": "916045"
  },
  {
    "text": "at that point x, then, um, this curve here looks the- the- the shape of this curve is actually like this, right?",
    "start": "916045",
    "end": "927399"
  },
  {
    "text": "Um, and this is the shape of a Gaussian bell curve. But this has nothing to do with a Gaussian density,",
    "start": "927400",
    "end": "933310"
  },
  {
    "text": "right, so this thing does not integrate to 1. So- so it's just sometimes you ask well, is this- is this using a Gaussian density? The answer is no.",
    "start": "933310",
    "end": "940570"
  },
  {
    "text": "Uh, this is just a function that, um, is shaped a lot like a Gaussian but, you know,",
    "start": "940570",
    "end": "945850"
  },
  {
    "text": "Gaussian densities, probability density functions have to integrate to 1 and this does not. So there's nothing to do with a Gaussian probability density. Question?",
    "start": "945850",
    "end": "952795"
  },
  {
    "text": "So how- how do you choose the width of the- Oh, so how do you choose the width, lemmi get back to that.",
    "start": "952795",
    "end": "958045"
  },
  {
    "text": "Yeah. Um, and so for this example this height",
    "start": "958045",
    "end": "964029"
  },
  {
    "text": "here says give this example a weight equal to the height of that thing.",
    "start": "964030",
    "end": "969985"
  },
  {
    "text": "Give this example a weight to the height of this, height of this, height of that, right?",
    "start": "969985",
    "end": "975175"
  },
  {
    "text": "Which is why if you actually- if you have an example this way out there, you know, is given a weight that's essentially 0.",
    "start": "975175",
    "end": "982480"
  },
  {
    "text": "Which is why it's weighting only the nearby examples when trying to fit a straight line, right, uh, for the- for making predictions close to this, okay?",
    "start": "982480",
    "end": "993384"
  },
  {
    "text": "Um, now so one last thing that I wanna mention which is,",
    "start": "993385",
    "end": "1000480"
  },
  {
    "text": "um, the- the- the question just now which is how do you choose the width of this Gaussian density, right?",
    "start": "1000480",
    "end": "1006209"
  },
  {
    "text": "How fat it is or how thin should it be? Um, and this decides how big a neighborhood should you look in",
    "start": "1006210",
    "end": "1012030"
  },
  {
    "text": "order to decide what's the neighborhood of points you should use to fit this, you know, local straight line.",
    "start": "1012030",
    "end": "1017205"
  },
  {
    "text": "And so, um, for Gaussian function like this, uh, this- I'm gonna call this the, um, bandwidth parameter tau, right?",
    "start": "1017205",
    "end": "1031964"
  },
  {
    "text": "And this is a parameter or a hyper-parameter of the algorithm.",
    "start": "1031965",
    "end": "1037005"
  },
  {
    "text": "And, uh, depending on the choice of tau, um, uh, you can choose a fatter or a thinner bell-shaped curve,",
    "start": "1037005",
    "end": "1044850"
  },
  {
    "text": "which causes you to look in a bigger or a narrower window in order to decide,",
    "start": "1044850",
    "end": "1050940"
  },
  {
    "text": "um, you know, how many nearby examples to use in order to fit the straight line, okay?",
    "start": "1050940",
    "end": "1056460"
  },
  {
    "text": "And it turns out that, um, and I wanna leave- I wanna leave you to discover this yourself in the problem set.",
    "start": "1056460",
    "end": "1061920"
  },
  {
    "text": "Um, if- if you've taken a little bit of machine learning elsewhere I've heard of the terms [inaudible] Test. It's on?",
    "start": "1061920",
    "end": "1072150"
  },
  {
    "text": "Okay, good. It was on. Good. It turns out that, um, the choice of the bandwidth tau has an effect on,",
    "start": "1072150",
    "end": "1078495"
  },
  {
    "text": "uh, overfitting and underfitting. If you don't know what those terms mean don't worry about it, we'll define them later in this course. But, uh, what you get to do in the problem set is, uh,",
    "start": "1078495",
    "end": "1087405"
  },
  {
    "text": "play with tau yourself and see why, um, if tau is too broad, you end up fitting, um,",
    "start": "1087405",
    "end": "1096235"
  },
  {
    "text": "you end up over-smoothing the data and if tau is too thin you end up fitting a very jagged fit to the data.",
    "start": "1096235",
    "end": "1101580"
  },
  {
    "text": "And if any of these things don't make sense yet don't worry about it they'll make sense after you play a bit in the- in the problem set, okay?",
    "start": "1101580",
    "end": "1108794"
  },
  {
    "text": "Um, so yeah, since- since you- you play with the varying tau in the problem set and see for yourself the net impact of that, okay? Question?",
    "start": "1108795",
    "end": "1118985"
  },
  {
    "text": "Is tau raised to power there or is that just a- just a- [NOISE] Thank you, uh, this is tau squared.",
    "start": "1118985",
    "end": "1124740"
  },
  {
    "text": "Yeah. Yeah.",
    "start": "1124740",
    "end": "1133760"
  },
  {
    "text": "So- so what happens if you need to invert, uh, the [inaudible] What happens if you need to infer the value of h outside the scope of the dataset?",
    "start": "1133760",
    "end": "1139005"
  },
  {
    "text": "It turns out that you can still use this algorithm. It's just that, um, its results may not be very good.",
    "start": "1139005",
    "end": "1146190"
  },
  {
    "text": "Yeah. It- it- it depends I guess. Um, locally linear regression is usually not greater than extrapolation,",
    "start": "1146190",
    "end": "1154200"
  },
  {
    "text": "but then most- many learning algorithms are not good at extrapolation. So all- all the formulas still work, you can still implement this.",
    "start": "1154200",
    "end": "1160125"
  },
  {
    "text": "But, um, yeah. You can also try- you can also try a linear problem set and see what happens. Yeah. One last question.",
    "start": "1160125",
    "end": "1166110"
  },
  {
    "text": "Is it possible to have like a vertical tau depending on whether some parts of your data have lots of- Yeah.",
    "start": "1166110",
    "end": "1174309"
  },
  {
    "text": "Yes, this is mostly for the variable tau depending- Uh, uh, yes, it is, uh, and there are quite complicated ways to choose",
    "start": "1174410",
    "end": "1181559"
  },
  {
    "text": "tau based on how many points there on the local region and so on. Yes. There's a huge literature on different formulas actually for example",
    "start": "1181560",
    "end": "1187980"
  },
  {
    "text": "instead of this Gaussian bump thing, uh, there's, uh, sometimes people use that triangle shape function.",
    "start": "1187980",
    "end": "1193260"
  },
  {
    "text": "So it actually goes to zero outside some small rings. So there are, there are many versions of this algorithm.",
    "start": "1193260",
    "end": "1198720"
  },
  {
    "text": "Um, so I tend to use, uh, a locally weighted linear regression when,",
    "start": "1198720",
    "end": "1203835"
  },
  {
    "text": "uh, you have a relatively low dimensional data set. So when the number of features is not too big, right?",
    "start": "1203835",
    "end": "1209280"
  },
  {
    "text": "So when n is quite small like 2 or 3 or something and we have a lot of data. And you don't wanna think about what features to use, right.",
    "start": "1209280",
    "end": "1216765"
  },
  {
    "text": "So- so that's the scenario. So if, if you actually have a data set that looks like these up in drawing, you know,",
    "start": "1216765",
    "end": "1222299"
  },
  {
    "text": "locally weighted linear regression is, is a, is a pretty good algorithm. Um, just one last question. Then we're moving on.",
    "start": "1222300",
    "end": "1229290"
  },
  {
    "text": "When you have a lot of data like this, does it usually complicate the question, since you're [BACKGROUND]",
    "start": "1229290",
    "end": "1235800"
  },
  {
    "text": "Oh, sure. Yes, if you have a lot of data, that wants to be computationally expensive, yes, it would be. Uh, I guess a lot of data is relative.",
    "start": "1235800",
    "end": "1242160"
  },
  {
    "text": "Uh, yes we have, you know, 2, 3, 4 dimensional data and hundreds of examples, I mean, thousands of examples.",
    "start": "1242160",
    "end": "1248340"
  },
  {
    "text": "Uh, it turns out the computation needed to fit the minimization is, uh, similar to the normal equations,",
    "start": "1248340",
    "end": "1254535"
  },
  {
    "text": "and so you- it involves solving a linear system of equations of dimension equal to the number of training examples you have.",
    "start": "1254535",
    "end": "1260700"
  },
  {
    "text": "So, if that's, you know, like a thousand or a few thousands, that's not too bad. If you have millions of examples then,",
    "start": "1260700",
    "end": "1266295"
  },
  {
    "text": "then there are also multiple scaling algorithms like KD trees and much more complicated algorithms to do this when you",
    "start": "1266295",
    "end": "1271440"
  },
  {
    "text": "have millions or hun- tens of millions of examples. Yeah. Okay. So you get a better sense of this algorithm when you play with it,",
    "start": "1271440",
    "end": "1281540"
  },
  {
    "text": "um, in the problem set. Now, the second topic-one of- so I'm gonna put aside locally weighted regression.",
    "start": "1281540",
    "end": "1289955"
  },
  {
    "text": "We won't talk about that set of ideas anymore, uh, today. But, but what I wanna do today is, uh,",
    "start": "1289955",
    "end": "1295304"
  },
  {
    "start": "1292000",
    "end": "2778000"
  },
  {
    "text": "on last Wednesday I had said that- I had promised last Wednesday that today I'll give a justification for why we use the squared error, right.",
    "start": "1295305",
    "end": "1304245"
  },
  {
    "text": "Why the squared error and why not to the fourth power or absolute value? Um, and so, um, what I want to show you today- now is",
    "start": "1304245",
    "end": "1313350"
  },
  {
    "text": "a probabilistic interpretation of linear regression and this probabilistic interpretation will put us in good standing as we go on to logistic regression today,",
    "start": "1313350",
    "end": "1321629"
  },
  {
    "text": "uh, and then generalize linear models later this week. We're going to keep up to-keep the notation there so we could continue to refer to it.",
    "start": "1321630",
    "end": "1331150"
  },
  {
    "text": "So why these squares? Why squared error? Um, I'm gonna present a set of assumptions",
    "start": "1344090",
    "end": "1350115"
  },
  {
    "text": "under which these squares using squared error falls out very naturally. Which is let's say for housing price prediction.",
    "start": "1350115",
    "end": "1358980"
  },
  {
    "text": "Let's assume that there's a true price of every house y i which is x transpose,",
    "start": "1358980",
    "end": "1367635"
  },
  {
    "text": "um, say there i, plus epsilon i.",
    "start": "1367635",
    "end": "1372900"
  },
  {
    "text": "Where epsilon i is an error term. That includes, um, unmodeled effects,",
    "start": "1372900",
    "end": "1381910"
  },
  {
    "text": "you know, and just random noise.",
    "start": "1384080",
    "end": "1387850"
  },
  {
    "text": "Okay. So let's assume that the way, you know, housing prices truly work is that every house's price",
    "start": "1389600",
    "end": "1396345"
  },
  {
    "text": "is a linear function of the size of the house and number of bedrooms, plus an error term that captures unmodeled effects such as maybe one day",
    "start": "1396345",
    "end": "1405179"
  },
  {
    "text": "that seller is in an unusually good mood or an unusually bad mood and so that makes the price go higher or lower.",
    "start": "1405180",
    "end": "1410294"
  },
  {
    "text": "We just don't model that, um, as well as random noise, right. Or, or maybe the model will skew this street, you know,",
    "start": "1410295",
    "end": "1417480"
  },
  {
    "text": "preset to persistent capture, that's one of the features, but other things have an impact on housing prices.",
    "start": "1417480",
    "end": "1422880"
  },
  {
    "text": "Um, and we're going to assume that, uh,",
    "start": "1422880",
    "end": "1428265"
  },
  {
    "text": "epsilon i is distributed Gaussian would mean 0 and co-variance sigma squared.",
    "start": "1428265",
    "end": "1436265"
  },
  {
    "text": "So I'm going to use this notation to mean- so the way you read this notation is epsilon i this twiddle you pronounce as, it's distributed.",
    "start": "1436265",
    "end": "1445600"
  },
  {
    "text": "And then stripped n parens 0, sigma squared. This is a normal distribution also called the Gaussian Distribution, same thing.",
    "start": "1445600",
    "end": "1452310"
  },
  {
    "text": "Normal distribution and Gaussian distribution mean the same thing. The normal distribution would mean 0 and,",
    "start": "1452310",
    "end": "1458309"
  },
  {
    "text": "um, a variance sigma squared. Okay. Um, and what this means is that the probability density of epsilon i is- this is",
    "start": "1458310",
    "end": "1467460"
  },
  {
    "text": "the Gaussian density, 1 over root 2 pi sigma e to the negative epsilon i squared over 2 sigma squared.",
    "start": "1467460",
    "end": "1476400"
  },
  {
    "text": "Okay. And unlike the Bell state-the bell-shaped curve I used earlier for locally weighted linear regression,",
    "start": "1476400",
    "end": "1482399"
  },
  {
    "text": "this thing does integrate to 1, right. This-this function integrates to 1. Uh, and so this is a Gaussian density,",
    "start": "1482400",
    "end": "1489419"
  },
  {
    "text": "this is a prob-prob-probability density function. Um, and this is the familiar, you know,",
    "start": "1489420",
    "end": "1497265"
  },
  {
    "text": "Gaussian bell-shaped curve with mean 0 and co-variance- and variance,",
    "start": "1497265",
    "end": "1503700"
  },
  {
    "text": "uh, uh, sigma squared where sigma kinda controls the width of this Gaussian.",
    "start": "1503700",
    "end": "1509010"
  },
  {
    "text": "Okay? Uh, and if you haven't seen Gaussian's for a while we'll go over some of the, er, probability, probability pre-reqs as well in the classes, Friday discussion sections.",
    "start": "1509010",
    "end": "1520510"
  },
  {
    "text": "So, in other words, um, we assume that the way housing prices are determined is that,",
    "start": "1523280",
    "end": "1528960"
  },
  {
    "text": "first is a true price theta transpose x. And then, you know, some random force of nature.",
    "start": "1528960",
    "end": "1534284"
  },
  {
    "text": "Right, the mood of the seller or, I-I-I don't know-I don't have other factors, right.",
    "start": "1534285",
    "end": "1540555"
  },
  {
    "text": "Perturbs it from this true value, theta transpose xi. Um, and the huge assumption we're gonna make is that the epsilon I's",
    "start": "1540555",
    "end": "1549929"
  },
  {
    "text": "these error terms are IID. And IID from statistics stands for Independently and Identically Distributed.",
    "start": "1549930",
    "end": "1556695"
  },
  {
    "text": "And what that means is that the error term for one house is independent, uh, as the error term for a different house.",
    "start": "1556695",
    "end": "1563414"
  },
  {
    "text": "Which is actually not a true assumption. Right. Because, you know, if, if one house is priced on one street is",
    "start": "1563415",
    "end": "1568559"
  },
  {
    "text": "unusually high, probably a price on a different house on the same street will also be unusually high. And so- but, uh, this assumption that these epsilon I's are",
    "start": "1568560",
    "end": "1577155"
  },
  {
    "text": "IID since they're independently and identically distributed. Um, is one of those assumptions that,",
    "start": "1577155",
    "end": "1582330"
  },
  {
    "text": "that, you know, is probably not absolutely true, but may be good enough that if you make this assumption, you get a pretty good model.",
    "start": "1582330",
    "end": "1588885"
  },
  {
    "text": "Okay. Um, and so let's see. Under these set of assumptions this implies that [NOISE] the density or",
    "start": "1588885",
    "end": "1604470"
  },
  {
    "text": "the probability of y i given x i and theta this is going to be this.",
    "start": "1604470",
    "end": "1613630"
  },
  {
    "text": "Um, and I'll, I'll take this and write it in another way.",
    "start": "1623510",
    "end": "1628420"
  },
  {
    "text": "In other words, given x and theta, what's the density- what's the probability of a particular house's price?",
    "start": "1641200",
    "end": "1651360"
  },
  {
    "text": "Well, it's going to be Gaussian with mean given by theta transpose xi or theta transpose x,",
    "start": "1651360",
    "end": "1657585"
  },
  {
    "text": "and the variance is, um, given by sigma squared. Okay. Um, and so, uh,",
    "start": "1657585",
    "end": "1665745"
  },
  {
    "text": "because the way that the price of a house is determined is by taking theta transpose x with the, you know,",
    "start": "1665745",
    "end": "1671700"
  },
  {
    "text": "quote true price of the house and then adding noise or adding error of variance sigma squared to it.",
    "start": "1671700",
    "end": "1677610"
  },
  {
    "text": "And so, um, the, the assumptions on the left imply that given x and theta,",
    "start": "1677610",
    "end": "1682980"
  },
  {
    "text": "the density of y, you know, has this distribution. Which is- really this is the random variable y,",
    "start": "1682980",
    "end": "1689085"
  },
  {
    "text": "and that's the mean, right, and that's the variance of the Gaussian density.",
    "start": "1689085",
    "end": "1696525"
  },
  {
    "text": "Okay. Now, um, two pieces of notation. Um, I want to, one that you should get familiar with.",
    "start": "1696525",
    "end": "1705855"
  },
  {
    "text": "Um, the reason I wrote the semicolon here is, uh, that- the way you read this equation is the semicolon should be read as parameterized as.",
    "start": "1705855",
    "end": "1716890"
  },
  {
    "text": "Right, um, and so because, uh, uh, the,",
    "start": "1717320",
    "end": "1723434"
  },
  {
    "text": "the alternative way to write this would be to say P of xi given yi, excuse me, P of y given xi comma theta.",
    "start": "1723435",
    "end": "1731100"
  },
  {
    "text": "But if you were to write this notation this way, this would be conditioning on theta,",
    "start": "1731100",
    "end": "1736783"
  },
  {
    "text": "but theta is not a random variable. So you shouldn't condition on theta, which is why I'm gonna write a semicolon.",
    "start": "1736784",
    "end": "1743070"
  },
  {
    "text": "And so the way you read this is, the probability of yi given xi and parameterize, oh, excuse me, parameterized by theta is equal to that formula, okay?",
    "start": "1743070",
    "end": "1753720"
  },
  {
    "text": "Um, if, if, if you don't understand this distinction, again, don't worry too much about it. In, in statistics there are multiple schools of statistics called Bayesian statistics,",
    "start": "1753720",
    "end": "1762149"
  },
  {
    "text": "frequentist statistics, this is a frequentist interpretation. Uh, for the purposes of machine learning, don't worry about it,",
    "start": "1762150",
    "end": "1767820"
  },
  {
    "text": "but I find that being more consistent with terminology prevents some of our statistician friends from getting really upset, but, but,",
    "start": "1767820",
    "end": "1773565"
  },
  {
    "text": "but, you know, I'll try to follow statistics convention. Uh, so- because just only unnecessary flack I guess,",
    "start": "1773565",
    "end": "1781170"
  },
  {
    "text": "um, but for the per- for practical purposes this is not that important. If you forget this notation on your homework. don't worry about it we won't penalize you,",
    "start": "1781170",
    "end": "1787559"
  },
  {
    "text": "but I'll try to be consistent. Um, but this just means that theta in this view is not a random variable,",
    "start": "1787560",
    "end": "1793019"
  },
  {
    "text": "it's just theta is a set of parameters that parameterizes this probability distribution. Okay? Um, and the way to read the second equation is, um,",
    "start": "1793020",
    "end": "1803955"
  },
  {
    "text": "when you write these equations usually don't write them with parentheses, but the way to parse this equation is to say that this thing is a random variable.",
    "start": "1803955",
    "end": "1811769"
  },
  {
    "text": "The random variable y given x and parameterized by theta. This thing that I just drew in",
    "start": "1811770",
    "end": "1817290"
  },
  {
    "text": "green parentheses is just a distributed Gaussian with that distribution, okay? All right. Um, any questions about this?",
    "start": "1817290",
    "end": "1829870"
  },
  {
    "text": "Okay. So it turns out that",
    "start": "1831140",
    "end": "1837990"
  },
  {
    "text": "[NOISE] if you are willing to make those assumptions,",
    "start": "1837990",
    "end": "1845475"
  },
  {
    "text": "then linear regression, um, falls out almost naturally of the assumptions we just made.",
    "start": "1845475",
    "end": "1857130"
  },
  {
    "text": "And in particular, under the assumptions we just made, um,",
    "start": "1857130",
    "end": "1862725"
  },
  {
    "text": "the likelihood of the parameters theta,",
    "start": "1862725",
    "end": "1869429"
  },
  {
    "text": "so this is pronounced the likelihood of the parameters theta,",
    "start": "1869430",
    "end": "1877470"
  },
  {
    "text": "uh, L of theta which is defined as the probability of the data.",
    "start": "1877470",
    "end": "1884020"
  },
  {
    "text": "Right? So this is probability of all the values of y of y1 up to ym given all the xs and given,",
    "start": "1884720",
    "end": "1891870"
  },
  {
    "text": "uh, the parameters theta parameterized by theta. Um, this is equal to the product from I equals",
    "start": "1891870",
    "end": "1902760"
  },
  {
    "text": "1 through m of p of yi given xi parameterized by theta.",
    "start": "1902760",
    "end": "1912130"
  },
  {
    "text": "Um, because we assumed the examples were- because we assume the errors are IID, right,",
    "start": "1913490",
    "end": "1919410"
  },
  {
    "text": "that the error terms are independently and identically distributed to each other, so the probability of all of the observations,",
    "start": "1919410",
    "end": "1926850"
  },
  {
    "text": "of all the values of y in your training set is equal to the product of the probabilities, because of the independence assumption we made.",
    "start": "1926850",
    "end": "1932520"
  },
  {
    "text": "And so plugging in the definition of p of y given x parameterized by theta that we had up there,",
    "start": "1932520",
    "end": "1938265"
  },
  {
    "text": "this is equal to product of that.",
    "start": "1938265",
    "end": "1945070"
  },
  {
    "text": "Okay? Now, um, again, one more piece of terminology.",
    "start": "1956000",
    "end": "1964440"
  },
  {
    "text": "Uh, you know, another question I've always been asked if you say, hey, Andrew, what's the difference between likelihood and probability, right?",
    "start": "1964440",
    "end": "1971145"
  },
  {
    "text": "And so the likelihood of the parameters is exactly the same thing as the probability of the data,",
    "start": "1971145",
    "end": "1976785"
  },
  {
    "text": "uh, but the reason we sometimes talk about likelihood, and sometimes talk of probability is, um, we think of likelihood.",
    "start": "1976785",
    "end": "1983115"
  },
  {
    "text": "So this, this is some function, right? This thing is a function of the data as well as a function of the parameters theta.",
    "start": "1983115",
    "end": "1989670"
  },
  {
    "text": "And if you view this number, whatever this number is, if you view this thing as a function of the parameters holding the data fixed,",
    "start": "1989670",
    "end": "1996450"
  },
  {
    "text": "then we call that the likelihood. So if you think of the training set the data as a fixed thing, and then varying parameters theta,",
    "start": "1996450",
    "end": "2003200"
  },
  {
    "text": "then I'm going to use the term likelihood. Whereas if you view the parameters theta as fixed and maybe varying the data,",
    "start": "2003200",
    "end": "2010070"
  },
  {
    "text": "I'm gonna say probability, right? So, so you hear me use- well, I'll, I'll try to be consistent.",
    "start": "2010070",
    "end": "2015725"
  },
  {
    "text": "I find I'm, I'm pretty good at being consistent but not perfect, but I'm going to try to say likelihood of the parameters,",
    "start": "2015725",
    "end": "2022280"
  },
  {
    "text": "and probability of the data even though those evaluate to the same thing as just, you know,",
    "start": "2022280",
    "end": "2027440"
  },
  {
    "text": "for this function, this function is a function of theta and the parameters which one are you viewing as fixed and which one are you viewing as, as variables.",
    "start": "2027440",
    "end": "2033695"
  },
  {
    "text": "So when you view this as a function of theta, I'm gonna use this term likelihood. Uh, but- so, so hopefully you hear me say likelihood of the parameters.",
    "start": "2033695",
    "end": "2042125"
  },
  {
    "text": "Hopefully you won't hear me say likelihood of the data, right? And, and similarly, hopefully you hear me say probability of",
    "start": "2042125",
    "end": "2048649"
  },
  {
    "text": "the data and not the probability of the parameters, okay? Yeah. [inaudible].",
    "start": "2048650",
    "end": "2059089"
  },
  {
    "text": "Like other parameters. [inaudible]. Uh, okay.",
    "start": "2059090",
    "end": "2065119"
  },
  {
    "text": "So probability of the data. No. Uh, uh, theta, I got it sorry, yes.",
    "start": "2065120",
    "end": "2072649"
  },
  {
    "text": "Likelihood of theta. Got it. Yes. Sorry. Yes. Likelihood of theta. That's right.",
    "start": "2072650",
    "end": "2084620"
  },
  {
    "text": "[inaudible]. Oh, uh, no. So- no. Uh, uh, so theta is a set of parameters, it's not a random variable.",
    "start": "2084620",
    "end": "2090575"
  },
  {
    "text": "So we- likelihood of theta doesn't mean theta is a random variable. Right. Cool. Yeah. Thank you.",
    "start": "2090575",
    "end": "2096050"
  },
  {
    "text": "Um, by the way, the, the, the stuff about what's a random variable and what's not, the semicolon versus comma thing.",
    "start": "2096050",
    "end": "2101660"
  },
  {
    "text": "We explained this in more detail in the lecture notes. To me this is part of, um, uh, you know,",
    "start": "2101660",
    "end": "2107915"
  },
  {
    "text": "a little bit paying homage to the- to the religion of Bayesian frequencies versus Bayesian,",
    "start": "2107915",
    "end": "2114185"
  },
  {
    "text": "uh, frequentist versus Bayesians in statistics. From a- from a machine- from an applied machine learning",
    "start": "2114185",
    "end": "2119675"
  },
  {
    "text": "operational what you write code point of view, it doesn't matter that much. Uh, yeah. But theta is not a random variable,",
    "start": "2119675",
    "end": "2126785"
  },
  {
    "text": "we have likelihood of parameters which are not a variable. Yeah. Go ahead. [inaudible]. Oh, what's the rationale for choosing,",
    "start": "2126785",
    "end": "2137585"
  },
  {
    "text": "uh, oh, sure, why is epsilon i Gaussian? So, uh, uh, turns out because of central limit theorem,",
    "start": "2137585",
    "end": "2145670"
  },
  {
    "text": "uh, from statistics, uh, most error distributions are Gaussian, right? If something is- if there's an era that's made up of",
    "start": "2145670",
    "end": "2152090"
  },
  {
    "text": "lots of little noise sources which are not too correlated, then by central limit theorem it will be Gaussian.",
    "start": "2152090",
    "end": "2157520"
  },
  {
    "text": "So if you think that, most perturbations are, the mood of the seller, what's the school district, you know,",
    "start": "2157520",
    "end": "2162710"
  },
  {
    "text": "what's the weather like, or access to transportation, and all of these sources are not too correlated, and you add them up then the distribution will be Gaussian.",
    "start": "2162710",
    "end": "2169895"
  },
  {
    "text": "Um, and, and I think- well, yeah. So you can use the central limit theorem,",
    "start": "2169895",
    "end": "2175849"
  },
  {
    "text": "I think the Gaussian has become a default noise distribution. But for things where the true noise distribution is very far from Gaussian,",
    "start": "2175850",
    "end": "2183770"
  },
  {
    "text": "uh, this model does do that as well. And in fact, for when you see generalized linear models on Wednesday,",
    "start": "2183770",
    "end": "2189619"
  },
  {
    "text": "you see when- how to generalize all of these algorithms to very different distributions like Poisson, and so on.",
    "start": "2189620",
    "end": "2196440"
  },
  {
    "text": "All right. So, um, so we've seen the likelihood of the parameters theta.",
    "start": "2196570",
    "end": "2205355"
  },
  {
    "text": "Um, so I'm gonna use lower case l to denote the log-likelihood.",
    "start": "2205355",
    "end": "2213119"
  },
  {
    "text": "And the log-likelihood is just the log of the likelihood. Um, and so- well, just- right.",
    "start": "2215170",
    "end": "2230225"
  },
  {
    "text": "And so, um, log of a product is equal to the sum of the logs.",
    "start": "2230225",
    "end": "2235610"
  },
  {
    "text": "Uh, and so this is equal to-",
    "start": "2235610",
    "end": "2238370"
  },
  {
    "text": "and so this is m log 1 over root.",
    "start": "2251800",
    "end": "2256470"
  },
  {
    "text": "Okay? Um. And so, um, one of the, uh, you know,",
    "start": "2269500",
    "end": "2278510"
  },
  {
    "text": "well-tested letters in statistics estimating parameters is to use maximum likelihood estimation or MLE",
    "start": "2278510",
    "end": "2289230"
  },
  {
    "text": "which means you choose theta",
    "start": "2300550",
    "end": "2304230"
  },
  {
    "text": "to maximize the likelihood, right?",
    "start": "2306880",
    "end": "2313099"
  },
  {
    "text": "So given the data set, how would you like to estimate theta?",
    "start": "2313100",
    "end": "2318665"
  },
  {
    "text": "Well, one natural way to choose theta is to choose whatever value of theta has a highest likelihood.",
    "start": "2318665",
    "end": "2324410"
  },
  {
    "text": "Or in other words, choose a value of theta so that that value of theta maximizes the probability of the data, right?",
    "start": "2324410",
    "end": "2331880"
  },
  {
    "text": "And so, um, for- to simplify the algebra rather than",
    "start": "2331880",
    "end": "2337190"
  },
  {
    "text": "maximizing the likelihood capital L is actually easier to maximize the log likelihood.",
    "start": "2337190",
    "end": "2342665"
  },
  {
    "text": "But the log is a strictly monotonically increasing function. So the value of theta that maximizes",
    "start": "2342665",
    "end": "2347900"
  },
  {
    "text": "the log likelihood should be the same as the value of theta that maximizes the likelihood. And if you divide the log likelihood, um,",
    "start": "2347900",
    "end": "2355310"
  },
  {
    "text": "we conclude that if you're using maximum likelihood estimation, what you'd like to do is choose a value of theta that maximizes this thing, right?",
    "start": "2355310",
    "end": "2363380"
  },
  {
    "text": "But, uh, this first term is just a constant, theta doesn't even appear in this first term.",
    "start": "2363380",
    "end": "2369950"
  },
  {
    "text": "And so what you'd like to do is choose the value of theta that maximizes this second term.",
    "start": "2369950",
    "end": "2375800"
  },
  {
    "text": "Ah, notice there's a minus sign there. And so what you'd like to do is,",
    "start": "2375800",
    "end": "2381035"
  },
  {
    "text": "uh, uh, i.e, you know, choose theta to minimize this term.",
    "start": "2381035",
    "end": "2393119"
  },
  {
    "text": "Right. Also, sigma squared is just a constant. Right. No matter what sigma squared is,",
    "start": "2401680",
    "end": "2407615"
  },
  {
    "text": "you know, so, so, uh, so if you want to minimize this term, excuse me, if you want to maximize this term,",
    "start": "2407615",
    "end": "2413630"
  },
  {
    "text": "negative of this thing, that's the same as minimizing this term. Uh, but this is just J of theta.",
    "start": "2413630",
    "end": "2422454"
  },
  {
    "text": "The cost function you saw earlier for linear regression. Okay? So this little proof shows that,",
    "start": "2422455",
    "end": "2430880"
  },
  {
    "text": "um, choosing the value of theta to minimize the least squares errors,",
    "start": "2430880",
    "end": "2436174"
  },
  {
    "text": "like you saw last Wednesday, that's just finding the maximum likelihood estimate",
    "start": "2436175",
    "end": "2441575"
  },
  {
    "text": "for the parameters theta under this set of assumptions we made, that the error terms are Gaussian and IID.",
    "start": "2441575",
    "end": "2449494"
  },
  {
    "text": "Okay, go ahead. Oh, thank you.",
    "start": "2449495",
    "end": "2454625"
  },
  {
    "text": "Yes. Great. Thanks. Go ahead. [inaudible].",
    "start": "2454625",
    "end": "2467810"
  },
  {
    "text": "Oh, is there a situation where using this formula instead of least squares cost function will be a good idea? No. So this- I think this derivation shows that",
    "start": "2467810",
    "end": "2474140"
  },
  {
    "text": "this- this is completely equivalent to least squares. Right. That if- if you want- if you're willing",
    "start": "2474140",
    "end": "2479930"
  },
  {
    "text": "to assume that the error terms are Gaussian and IID and if you want to use Maximum Likelihood Estimation which is a very natural procedure in statistics,",
    "start": "2479930",
    "end": "2488240"
  },
  {
    "text": "then, you know, then you should use least squares. Right. So yeah.",
    "start": "2488240",
    "end": "2493880"
  },
  {
    "text": "If you know for some reason that the errors are not IID, like, is there a better way to figure out a better cost function?",
    "start": "2493880",
    "end": "2503299"
  },
  {
    "text": "If you know for some reason errors are not IID, could you figure out a better cost function? Yes and no. I think that, um, you know, when building learners algorithms,",
    "start": "2503300",
    "end": "2510589"
  },
  {
    "text": "ah, often we make model- we make assumptions about the world that we just know are not 100% true because it leads to algorithms that are computationally efficient.",
    "start": "2510590",
    "end": "2518930"
  },
  {
    "text": "Um, and so if you knew that your- if you knew that your training set was very very non IID,",
    "start": "2518930",
    "end": "2524930"
  },
  {
    "text": "there are- there're more sophisticated models you could build. But, um, ah, ah, yeah.",
    "start": "2524930",
    "end": "2530615"
  },
  {
    "text": "But- but very often we wouldn't bother I think. Yeah. More often than not we might not bother.",
    "start": "2530615",
    "end": "2536090"
  },
  {
    "text": "Ah, I can think of a few special cases where you would bother there but only if you think the assumption is really really bad.",
    "start": "2536090",
    "end": "2542015"
  },
  {
    "text": "Ah, if you don't have enough data or something- something. Quite- quite rare. All right. Um, lemme think why, all right.",
    "start": "2542015",
    "end": "2550475"
  },
  {
    "text": "I want to move on to make sure we get through the rest of things. Any burning questions? Yeah, okay, cool.",
    "start": "2550475",
    "end": "2556520"
  },
  {
    "text": "All right. Um, so out of this machinery.",
    "start": "2556520",
    "end": "2563150"
  },
  {
    "text": "Right. So- so- so what did we do here? Was we set up a set of probabilistic assumptions,",
    "start": "2563150",
    "end": "2568280"
  },
  {
    "text": "we made certain assumptions about P of Y given X, where the key assumption was Gaussian errors in IID.",
    "start": "2568280",
    "end": "2574390"
  },
  {
    "text": "And then through maximum likelihood estimation, we derived an algorithm which turns out to be exactly the least squares algorithm.",
    "start": "2574390",
    "end": "2581230"
  },
  {
    "text": "Right? Um, what I'd like to do is take this framework, ah, and apply it to our first classification problem.",
    "start": "2581230",
    "end": "2589145"
  },
  {
    "text": "Right. And so the- the key steps are, you know, one, make an assumption about P of Y given X,",
    "start": "2589145",
    "end": "2595085"
  },
  {
    "text": "P of Y given X parameters theta, and then second is figure out maximum likelihood estimation. So I'd like to take this framework and apply it to a different type of problem,",
    "start": "2595085",
    "end": "2602735"
  },
  {
    "text": "where the value of Y is now either 0 or 1. So is a classification problem.",
    "start": "2602735",
    "end": "2608405"
  },
  {
    "text": "Okay? So, um, let's see.",
    "start": "2608405",
    "end": "2614910"
  },
  {
    "text": "So the classification problem. In our first classification problem,",
    "start": "2619180",
    "end": "2624260"
  },
  {
    "text": "we're going to start with binary classification. So the value of Y is either 0 or 1.",
    "start": "2624260",
    "end": "2630440"
  },
  {
    "text": "And sometimes we call this binary classification because there are two clauses.",
    "start": "2630440",
    "end": "2635525"
  },
  {
    "text": "Classification. Right. Um, and so right- so that's",
    "start": "2635525",
    "end": "2648650"
  },
  {
    "text": "a data set where I guess this is X and this is Y. Um, so something that's not a good idea is to apply linear regression to this data set.",
    "start": "2648650",
    "end": "2658355"
  },
  {
    "text": "Some- sometimes you will do it and maybe you'll get away with it but I wouldn't do it and here's. Which is, um, is- is tempting to just fit a straight line to",
    "start": "2658355",
    "end": "2666020"
  },
  {
    "text": "this data and then take the straight line and threshold it at 0.5, and then say, oh, if it's above 0.5 round off to 1,",
    "start": "2666020",
    "end": "2673070"
  },
  {
    "text": "if it's below 0.5 round it off to 0. But it turns out that this, um, is not a good idea,",
    "start": "2673070",
    "end": "2680615"
  },
  {
    "text": "uh, for classification problems. And- and here's why? Which is- for this data set it's really obvious what the- what the pattern is.",
    "start": "2680615",
    "end": "2687500"
  },
  {
    "text": "Right? Everything to the left of this point predict 0. Everything to the right of that point predict 1. But let's say we now change the data set to just add one more example there.",
    "start": "2687500",
    "end": "2697715"
  },
  {
    "text": "Right. And the pattern is still really obvious. It says everything to the left of this predict 0, everything to the right of that predict 1.",
    "start": "2697715",
    "end": "2703984"
  },
  {
    "text": "But if you fit a straight line to this data set with this extra one point there, and just not even the outlier it's really",
    "start": "2703985",
    "end": "2709520"
  },
  {
    "text": "obvious at this point way out there should be labeled one. But with this extra example, um,",
    "start": "2709520",
    "end": "2714785"
  },
  {
    "text": "if we fit a straight line to the data, you end up with maybe something like that.",
    "start": "2714785",
    "end": "2719810"
  },
  {
    "text": "Um, and somehow adding this one example, it really didn't change anything, right?",
    "start": "2719810",
    "end": "2725180"
  },
  {
    "text": "But somehow the straight line fit moved from the green line to the, uh, moved from the blue line to the green line.",
    "start": "2725180",
    "end": "2730730"
  },
  {
    "text": "And if you now threshold it at 0.5, you end up with a very different decision boundary.",
    "start": "2730730",
    "end": "2736070"
  },
  {
    "text": "And so linear regression is just not a good algorithm for classification. Some people use it and sometimes they get lucky and it's not too bad but",
    "start": "2736070",
    "end": "2743840"
  },
  {
    "text": "I- I- I personally never use linear regression for classification algorithms. Right. Because you just don't know if you end up with",
    "start": "2743840",
    "end": "2750380"
  },
  {
    "text": "a really bad fit to the data like this, okay? Um, so oh and- and- and the other unnatural thing",
    "start": "2750380",
    "end": "2760640"
  },
  {
    "text": "about using linear regression for a classification problem is that, um, you know for a classification problem that the values are,",
    "start": "2760640",
    "end": "2768005"
  },
  {
    "text": "you know, 0 or 1. Right. And so it outputs negative values or values even",
    "start": "2768005",
    "end": "2773240"
  },
  {
    "text": "greater than 1 seems- seems strange, um. So what I'd like to share with you now is really,",
    "start": "2773240",
    "end": "2783530"
  },
  {
    "start": "2778000",
    "end": "3957000"
  },
  {
    "text": "probably by far the most commonly used classification algorithm ah, called logistic regression.",
    "start": "2783530",
    "end": "2789050"
  },
  {
    "text": "Now let's say the two learning algorithms",
    "start": "2789050",
    "end": "2795500"
  },
  {
    "text": "I probably use the most often are linear regression and logistic regression. Yeah, probably these two, actually. Um, and, uh, this is the algorithm.",
    "start": "2795500",
    "end": "2806375"
  },
  {
    "text": "So, um, as- as we designed a logistic regression algorithm, one of the things we might naturally want is for",
    "start": "2806375",
    "end": "2814145"
  },
  {
    "text": "the hypothesis to output values between 0 and 1. Right. And this is mathematical notation for the values for H of X or H prime,",
    "start": "2814145",
    "end": "2824704"
  },
  {
    "text": "H subscript theta of X, uh, lies in the set from 0 to 1. Right? This 0 to 1 square bracket is the set of all real numbers from 0 to 1.",
    "start": "2824705",
    "end": "2833194"
  },
  {
    "text": "So this says, we want the hypothesis output values in you know between 0 and 1, so that in the set of all numbers between z- from 0 to  1.",
    "start": "2833195",
    "end": "2840980"
  },
  {
    "text": "Um, and so we're going to choose the following form of the hypothesis.",
    "start": "2840980",
    "end": "2846635"
  },
  {
    "text": "Um, so. Okay. So we're gonna define a function,",
    "start": "2846635",
    "end": "2863205"
  },
  {
    "text": "g of z, that looks like this. And this is called the sigmoid, uh,",
    "start": "2863205",
    "end": "2872540"
  },
  {
    "text": "or the logistic function.",
    "start": "2872540",
    "end": "2878550"
  },
  {
    "text": "Uh, these are synonyms, they mean exactly the same thing. So, uh, it can be called the sigmoid function,",
    "start": "2878550",
    "end": "2883680"
  },
  {
    "text": "or the logistic function, it means exactly the same thing. But we're gonna choose a function, g of z.",
    "start": "2883680",
    "end": "2889440"
  },
  {
    "text": "Uh, and this function is shaped as follows. If you plot this function, you find that it looks like this.",
    "start": "2889440",
    "end": "2896894"
  },
  {
    "text": "Um, where if the horizontal axis is z, then this is g of z.",
    "start": "2896895",
    "end": "2902640"
  },
  {
    "text": "And so it crosses x intercept at 0, um, and it, you know,",
    "start": "2902640",
    "end": "2908940"
  },
  {
    "text": "starts off, well, really, really close to 0, rises, and then asymptotes towards 1.",
    "start": "2908940",
    "end": "2916230"
  },
  {
    "text": "Okay? And so g of z output values are between 0 and 1. And, um, what logistic regression does is instead of- let's see.",
    "start": "2916230",
    "end": "2927650"
  },
  {
    "text": "So previously, for linear regression, we had chosen this form for the hypothesis, right?",
    "start": "2927650",
    "end": "2932930"
  },
  {
    "text": "We just made a choice that we'll say the housing prices are a linear function of the features x. And what logistic regression does is theta transpose x could be bigger than 1,",
    "start": "2932930",
    "end": "2942300"
  },
  {
    "text": "it can be less than 0, which is not very natural. But instead, it's going to take theta transpose x and pass it",
    "start": "2942300",
    "end": "2947580"
  },
  {
    "text": "through this sigmoid function g. So this force, the output values only between 0 and 1.",
    "start": "2947580",
    "end": "2954359"
  },
  {
    "text": "Okay? Um, so you know,",
    "start": "2954360",
    "end": "2959910"
  },
  {
    "text": "when designing a learning algorithm, uh, sometimes you just have to choose the form of the hypothesis.",
    "start": "2959910",
    "end": "2965204"
  },
  {
    "text": "How are you gonna represent the function h, or h of- h subscript theta. And so we're making that choice here today.",
    "start": "2965205",
    "end": "2971910"
  },
  {
    "text": "And if you're wondering, you know, there are lots of functions that we could have chosen, right?",
    "start": "2971910",
    "end": "2977250"
  },
  {
    "text": "There are lots of why, why not, why not this function? Or why not, you know, there are lots of functions with vaguely this shape,",
    "start": "2977250",
    "end": "2983850"
  },
  {
    "text": "they go between 0 and 1. So why are we choosing this specifically? It turns out that there's a broader class of algorithms called generalized linear models.",
    "start": "2983850",
    "end": "2992250"
  },
  {
    "text": "You'll hear about on Wednesday, uh, of which this is a special case. So we've seen linear regression, you'll see logistic regression in a second, and on Wednesday,",
    "start": "2992250",
    "end": "3000080"
  },
  {
    "text": "you'll see that both of these examples of a much bigger set of algorithms derived using a broader set of principles.",
    "start": "3000080",
    "end": "3005570"
  },
  {
    "text": "So, so for now, just, you know, take my word for it tha- that we want to use the logistic function.",
    "start": "3005570",
    "end": "3010640"
  },
  {
    "text": "Uh, uh, it'll turn out- you'll see on Wednesday that there's a way to derive even this function from, uh, from more basic principles,",
    "start": "3010640",
    "end": "3017690"
  },
  {
    "text": "rather than just putting all this, this all out. But for now, let me just pull this out of a hat and say, that's the one we want to use.",
    "start": "3017690",
    "end": "3023600"
  },
  {
    "text": "Okay. [NOISE]",
    "start": "3023600",
    "end": "3030230"
  },
  {
    "text": "So, um, let's make some assumptions",
    "start": "3030230",
    "end": "3047600"
  },
  {
    "text": "about the distribution of y given x parameterized by theta.",
    "start": "3047600",
    "end": "3052685"
  },
  {
    "text": "So I'm going to assume that the data has the following distribution.",
    "start": "3052685",
    "end": "3059060"
  },
  {
    "text": "The probability of y being 1, uh, again, from the breast cancer prediction that we had,",
    "start": "3059060",
    "end": "3064160"
  },
  {
    "text": "from, uh, the first lecture. Right? It will be the chance of a tumor being cancerous,",
    "start": "3064160",
    "end": "3069559"
  },
  {
    "text": "or being, um, um, malignant. Chance of y being 1, given the size of the tumor,",
    "start": "3069559",
    "end": "3074660"
  },
  {
    "text": "that's the feature x parameterized by theta. That this is equal to the output of your hypothesis.",
    "start": "3074660",
    "end": "3084170"
  },
  {
    "text": "So in other words, we're gonna assume that, um, what you want your learning algorithm to do is input",
    "start": "3084170",
    "end": "3089750"
  },
  {
    "text": "the features and tell me what's the chance that this tumor is malignant. Right? What's the chance that y is equal to 1?",
    "start": "3089750",
    "end": "3096620"
  },
  {
    "text": "Um, and by logic, I guess, because y can be only 1 or 0,",
    "start": "3096620",
    "end": "3103565"
  },
  {
    "text": "the chance of y being equal to 0, this has got to be 1 minus that.",
    "start": "3103565",
    "end": "3109890"
  },
  {
    "text": "Right? Because if a tumor has a 10% chance of being malignant, that means it has a 1 minus that.",
    "start": "3110710",
    "end": "3117560"
  },
  {
    "text": "It means it must have a 90% chance of being benign. Right? Since these two probabilities must add up to 1. Okay? Yeah.",
    "start": "3117560",
    "end": "3123380"
  },
  {
    "text": "[inaudible]",
    "start": "3123380",
    "end": "3134299"
  },
  {
    "text": "Say that again. [inaudible]. Oh, can we change the parameters here? Yes, you can,",
    "start": "3134300",
    "end": "3140450"
  },
  {
    "text": "but I'm not- yeah. But I think just to stick with convention in logistic regression. You, you- yeah.",
    "start": "3140450",
    "end": "3145685"
  },
  {
    "text": "Sure. We can assume that p of y equals 1 was this, and p of y equals 1 was that, but I think either way. It's just one you call positive example,",
    "start": "3145685",
    "end": "3151760"
  },
  {
    "text": "one you call a negative example. Right. So, so, uh, use this convention. Okay. Um, and now,",
    "start": "3151760",
    "end": "3159065"
  },
  {
    "text": "bearing in mind that y, right? By definition, because it is a binary classification problem.",
    "start": "3159065",
    "end": "3166190"
  },
  {
    "text": "But bear in mind that y can only take on two values, 0 or 1. Um, there's a nifty,",
    "start": "3166190",
    "end": "3172909"
  },
  {
    "text": "sort of little algebra way to take these two equations and write them in one equation,",
    "start": "3172909",
    "end": "3178599"
  },
  {
    "text": "and this will make some of the math a little bit easier. When I take these two equations, take these two assumptions and take these two facts,",
    "start": "3178600",
    "end": "3184255"
  },
  {
    "text": "and compress it into one equation, which is this. [NOISE] [BACKGROUND] Okay?",
    "start": "3184255",
    "end": "3195155"
  },
  {
    "text": "Oh, and I dropped the theta subscript just to simplify the notation of it. But I'm, I'm gonna be a little bit sloppy sometimes.",
    "start": "3195155",
    "end": "3201200"
  },
  {
    "text": "Well, a little less formal, whether I write the theta there or not. Okay? Um, but these two definitions of p of y given x parameterized by theta,",
    "start": "3201200",
    "end": "3210079"
  },
  {
    "text": "bearing in mind that y is either 0 or 1, can be compressed into one equation like this. Uh, and, and let me just say why.",
    "start": "3210080",
    "end": "3217190"
  },
  {
    "text": "Right? It's because if y- use a different color.",
    "start": "3217190",
    "end": "3225240"
  },
  {
    "text": "Right. If y is equal to 1,",
    "start": "3225760",
    "end": "3230780"
  },
  {
    "text": "then this becomes h of x to the power of 1 times this thing to the power of 0.",
    "start": "3230780",
    "end": "3237455"
  },
  {
    "text": "Right? If y is equal to 1, then, um, 1 - y is 0. And, you know, anything to the power of 0 is just equal to 1.",
    "start": "3237455",
    "end": "3248599"
  },
  {
    "text": "[NOISE] And so if y is equal to 1, you end up with p of y given x parameterized by theta equals h of x.",
    "start": "3248600",
    "end": "3258120"
  },
  {
    "text": "Right? Which is just what we had there.",
    "start": "3258130",
    "end": "3263670"
  },
  {
    "text": "And conversely, if y is equal to 0,",
    "start": "3263980",
    "end": "3269029"
  },
  {
    "text": "then, um, this thing will be 0, and this thing will be 1.",
    "start": "3269029",
    "end": "3274460"
  },
  {
    "text": "And so you end up with p of y given x parameterized theta is equal to 1 minus h of x,",
    "start": "3274460",
    "end": "3280940"
  },
  {
    "text": "which is just equal to that second equation. Okay? Right. Um, and so this is a nifty way",
    "start": "3280940",
    "end": "3290660"
  },
  {
    "text": "to take these two equations and compress them into one line, because depending on whether y is 0 or 1,",
    "start": "3290660",
    "end": "3296675"
  },
  {
    "text": "one of these two terms switches off, because it's exponentiated to the power of 0.",
    "start": "3296675",
    "end": "3301760"
  },
  {
    "text": "Um, and anything to the power of 0 is just equal to 1. Right? So one of these terms is just, you know, 1.",
    "start": "3301760",
    "end": "3308345"
  },
  {
    "text": "Just leaving the other term, and just selecting the, the appropriate equation, depending on whether y is 0 or 1.",
    "start": "3308345",
    "end": "3313924"
  },
  {
    "text": "Okay? So with that, um, uh, so with this little, uh,",
    "start": "3313925",
    "end": "3319115"
  },
  {
    "text": "on a notational trick, it will make the data derivations simpler.",
    "start": "3319115",
    "end": "3325280"
  },
  {
    "text": "Okay? Um, yeah. So let me use a new board.",
    "start": "3325280",
    "end": "3342920"
  },
  {
    "text": "[NOISE] I want that.",
    "start": "3342920",
    "end": "3345690"
  },
  {
    "text": "All right. Actually we can reuse along with this.",
    "start": "3353260",
    "end": "3357180"
  },
  {
    "text": "All right. So, uh, we're gonna use maximum likelihood estimation again. So let's write down the likelihood of the parameters.",
    "start": "3358990",
    "end": "3368690"
  },
  {
    "text": "Um, so well, it's actually p of all the y's given all the x's parameterized by theta was equal to this, uh,",
    "start": "3368690",
    "end": "3377210"
  },
  {
    "text": "which is now equal to product from i equals 1 through m, h of x_i to the power of y_i,",
    "start": "3377210",
    "end": "3385595"
  },
  {
    "text": "times 1 minus h of x_i to the power of 1 minus y_i.",
    "start": "3385595",
    "end": "3392420"
  },
  {
    "text": "Okay. Where all I did was take this definition of p of y given x parameterized by theta, uh, you know, from that,",
    "start": "3392420",
    "end": "3399654"
  },
  {
    "text": "after we did that little exponentiation trick and wrote it in here. Okay. Um. [NOISE]",
    "start": "3399655",
    "end": "3410605"
  },
  {
    "text": "And then, uh, with maximum likelihood estimation we'll want to find the value of theta that maximizes the likelihood,",
    "start": "3410605",
    "end": "3418960"
  },
  {
    "text": "maximizes the likelihood of the parameters. And so, um, same as what we did for linear regression to make the algebra,",
    "start": "3418960",
    "end": "3427255"
  },
  {
    "text": "you have to, to, to make the algebra a bit more simple, we're going to take the log of the likelihood and so compute the log likelihood.",
    "start": "3427255",
    "end": "3434050"
  },
  {
    "text": "And so that's equal to, um, [NOISE] let's see, right.",
    "start": "3434050",
    "end": "3441175"
  },
  {
    "text": "And so if you take the log of that, um, you end up with- you end up with that.",
    "start": "3441175",
    "end": "3463070"
  },
  {
    "text": "Okay? And, um, it- so, so, in other words, uh,",
    "start": "3465420",
    "end": "3473515"
  },
  {
    "text": "the last thing you want to do is, try to choose the value of theta to try to",
    "start": "3473515",
    "end": "3483250"
  },
  {
    "text": "maximize L of theta. Okay. Now, so, so just,",
    "start": "3483250",
    "end": "3491860"
  },
  {
    "text": "just to summarize where we are, right. Uh, if you're trying to predict, your malignancy and benign, uh,",
    "start": "3491860",
    "end": "3497875"
  },
  {
    "text": "tumors, you'd have a training set with XI YI. You define the likelihood, define the log-likelihood.",
    "start": "3497875",
    "end": "3505555"
  },
  {
    "text": "And then what you need to do is have an algorithm such as gradient descent, or gradient descent, talk about that in a sec to try to find the value of theta that maximizes the log-likelihood.",
    "start": "3505555",
    "end": "3515065"
  },
  {
    "text": "And then having chosen the value of theta when a new patient walks into the doctor's office you would take the features of the new tumor",
    "start": "3515065",
    "end": "3523599"
  },
  {
    "text": "and then use H of theta to estimate the chance of this new tumor in the new patient",
    "start": "3523600",
    "end": "3529000"
  },
  {
    "text": "that walks in tomorrow to estimate the chance that this new thing is ah is- is malignant or benign.",
    "start": "3529000",
    "end": "3534789"
  },
  {
    "text": "Okay? So the algorithm we're going to use to",
    "start": "3534790",
    "end": "3541840"
  },
  {
    "text": "choose theta to try to maximize the log-likelihood is a gradient ascent or batch gradient ascent.",
    "start": "3541840",
    "end": "3549295"
  },
  {
    "text": "And what that means is we will update the parameters theta J according to theta J",
    "start": "3549295",
    "end": "3556930"
  },
  {
    "text": "plus the partial derivative with respect to the log-likelihood.",
    "start": "3556930",
    "end": "3565089"
  },
  {
    "text": "Okay? Um, and the differences from what you saw that linear regression from last time is the following.",
    "start": "3565090",
    "end": "3573325"
  },
  {
    "text": "Just two differences I guess. For linear regression. Last week, I have written this down,",
    "start": "3573325",
    "end": "3579670"
  },
  {
    "text": "theta J gets updated as theta J minus partial with respect to theta J of J of theta, right?",
    "start": "3579670",
    "end": "3586450"
  },
  {
    "text": "So you saw this on Wednesday. So the two differences between that is well, first instead of J of theta you're now",
    "start": "3586450",
    "end": "3595000"
  },
  {
    "text": "trying to optimize the log-likelihood instead of this squared cost function. And the second change is, previously you were trying to minimize the squared error.",
    "start": "3595000",
    "end": "3603385"
  },
  {
    "text": "That's why we had the minus. And today you're trying to maximize the log-likelihood which is why there's a plus sign.",
    "start": "3603385",
    "end": "3610765"
  },
  {
    "text": "Okay? And so, um, so gradient descent you know,",
    "start": "3610765",
    "end": "3616390"
  },
  {
    "text": "is trying to climb down this hill whereas gradient ascent has a,",
    "start": "3616390",
    "end": "3622434"
  },
  {
    "text": "um, uh, has a- has a concave function like this. And it's trying to, like,",
    "start": "3622435",
    "end": "3628225"
  },
  {
    "text": "climb up the hill rather than climb down the hill. So that's why there's a plus symbol here instead of",
    "start": "3628225",
    "end": "3634510"
  },
  {
    "text": "a minus symbol because we are trying to maximize the function rather than minimize the function.",
    "start": "3634510",
    "end": "3640369"
  },
  {
    "text": "So the last thing to really flesh out this algorithm which is done in the lecture notes,",
    "start": "3640800",
    "end": "3647740"
  },
  {
    "text": "but I don't want to do it here today is to plug in the definition of H of theta into this equation and then take this thing.",
    "start": "3647740",
    "end": "3657115"
  },
  {
    "text": "So that's the log-likelihood of theta and then through calculus and algebra you can take derivatives of this whole thing with respect to theta.",
    "start": "3657115",
    "end": "3667180"
  },
  {
    "text": "This is done in detail in the lecture notes. I don't want to use this in class, but go ahead and take derivatives of this big formula with respect to",
    "start": "3667180",
    "end": "3674425"
  },
  {
    "text": "the parameters theta in order to figure out what is that thing, right? What is this thing that I just circled?",
    "start": "3674425",
    "end": "3681055"
  },
  {
    "text": "And it turns out that if you do so you will find that batch gradient ascent is the following.",
    "start": "3681055",
    "end": "3691750"
  },
  {
    "text": "You update theta J according to- oh,",
    "start": "3691750",
    "end": "3701350"
  },
  {
    "text": "actually I'm sorry, I forgot the learning rate. Yeah, it's your learning rate Alpha. Okay. Learning rate Alpha times this.",
    "start": "3701350",
    "end": "3709750"
  },
  {
    "text": "Okay? Because this term here is the partial derivative respect to Theta J after log-likelihood.",
    "start": "3713240",
    "end": "3723350"
  },
  {
    "text": "Okay? And the full calculus and so on derivations given the lecture notes.",
    "start": "3726900",
    "end": "3733045"
  },
  {
    "text": "Okay? Um, yeah. [inaudible].",
    "start": "3733045",
    "end": "3738940"
  },
  {
    "text": "Is there a chance of local maximum in this case? No. There isn't. It turns out that this function that the log-likelihood",
    "start": "3738940",
    "end": "3745000"
  },
  {
    "text": "function L of Theta full logistic regression it always looks like that. Uh, so this is a concave function.",
    "start": "3745000",
    "end": "3751555"
  },
  {
    "text": "So there are no local op. The only maximum is a global maxima. There's actually another reason why we chose the logistic function because if you",
    "start": "3751555",
    "end": "3758410"
  },
  {
    "text": "choose a logistic function rather than some other function that will give you 0 to 1, you're guaranteed that the likelihood function has only one global maximum.",
    "start": "3758410",
    "end": "3766675"
  },
  {
    "text": "And this, there's actually a big class about, actually what you'll see on Wednesday, this is a big class of algorithms of which linear regression is one example,",
    "start": "3766675",
    "end": "3775450"
  },
  {
    "text": "logistic regression is another example and for all of the algorithms in this class there are no local optima problems when you- when you derive them this way.",
    "start": "3775450",
    "end": "3782440"
  },
  {
    "text": "So you see that on Wednesday when we talk about generalized linear models. Okay? Um, so actually, but now that I think about,",
    "start": "3782440",
    "end": "3789819"
  },
  {
    "text": "there's just one question for you to think about. This looks exactly the same as what we've figured out for linear regression, right?",
    "start": "3789820",
    "end": "3797035"
  },
  {
    "text": "That when actually the difference for linear regression was I had a minus sign here and I reversed these two terms.",
    "start": "3797035",
    "end": "3802930"
  },
  {
    "text": "I think I had H theta of XI minus YI. If you put the minus sign there and reverse these two terms,",
    "start": "3802930",
    "end": "3809200"
  },
  {
    "text": "so take the minus minus, this is actually exactly the same as what we had come up with for linear regression.",
    "start": "3809200",
    "end": "3814569"
  },
  {
    "text": "So why, why, why is this different, right? I started off saying, don't use linear regression for classification problems",
    "start": "3814570",
    "end": "3820000"
  },
  {
    "text": "because of ah because of that problem that a single example could really you know- I started off with an example assuming that linear regression is",
    "start": "3820000",
    "end": "3827829"
  },
  {
    "text": "really bad for classification and we did all this work and I came back to the same algorithm. So what happened? Just, yeah go ahead.",
    "start": "3827830",
    "end": "3835960"
  },
  {
    "text": "[BACKGROUND]. Yeah. All right, cool. Awesome. Right? So what happened is",
    "start": "3835960",
    "end": "3841990"
  },
  {
    "text": "the definition of H of theta is now different than before but the surface level of the equation turns out to be the same.",
    "start": "3841990",
    "end": "3849685"
  },
  {
    "text": "Okay? And again it turns out that for every algorithm in this class of algorithms you'll see you on Wednesday you end up with the same thing.",
    "start": "3849685",
    "end": "3856210"
  },
  {
    "text": "Actually this is a general property of a much bigger class of algorithms called generalized linear models.",
    "start": "3856210",
    "end": "3862210"
  },
  {
    "text": "Although, yeah, i- i- interesting historical diverge, because of the confusion",
    "start": "3862210",
    "end": "3868990"
  },
  {
    "text": "between these two algorithms in the early history of machine learning there was some debate about you know between academics saying,",
    "start": "3868990",
    "end": "3875020"
  },
  {
    "text": "no, I invented that, no, I invented that. And then he goes, no, it's actually different algorithms.",
    "start": "3875020",
    "end": "3880240"
  },
  {
    "text": "[LAUGHTER] Alright, any questions? Oh go ahead.",
    "start": "3880240",
    "end": "3887380"
  },
  {
    "text": "[BACKGROUND].",
    "start": "3887380",
    "end": "3893349"
  },
  {
    "text": "Oh, great question. Is there a equivalent of normal equations to logistic regression? Um, short answer is no.",
    "start": "3893350",
    "end": "3900295"
  },
  {
    "text": "So for linear regression the normal equations gives you like a one shot way to just find the best value of theta.",
    "start": "3900295",
    "end": "3906355"
  },
  {
    "text": "There is no known way to just have a close form equation unless you find the best value of theta which is why you always have to use an algorithm,",
    "start": "3906355",
    "end": "3913930"
  },
  {
    "text": "an iterative optimization algorithm such as gradient ascent or ah and we'll see in a second Newton's method.",
    "start": "3913930",
    "end": "3920720"
  },
  {
    "text": "All right, cool. So, um, there's a great lead in to, um,",
    "start": "3921120",
    "end": "3930220"
  },
  {
    "text": "the last topic for today which is Newton's method.",
    "start": "3930220",
    "end": "3936520"
  },
  {
    "text": "[NOISE]",
    "start": "3936520",
    "end": "3957120"
  },
  {
    "start": "3957000",
    "end": "4774000"
  },
  {
    "text": "Um, you know, gradient ascent right is a good algorithm. I use gradient ascent all the time but it takes a baby step, takes a baby step,",
    "start": "3957120",
    "end": "3963865"
  },
  {
    "text": "take a baby step, it takes a lot of iterations for gradient assent to converge. Um, there's another algorithm called Newton's method",
    "start": "3963865",
    "end": "3971455"
  },
  {
    "text": "which allows you to take much bigger jumps so that's theta, you know, so- so, uh, there are problems where you might need you know,say",
    "start": "3971455",
    "end": "3978760"
  },
  {
    "text": "100 iterations or 1000 iterations of gradient ascent. That if you run this algorithm called Newton's method you might need",
    "start": "3978760",
    "end": "3985540"
  },
  {
    "text": "only 10 iterations to get a very good value of theta. But each iteration will be more expensive.",
    "start": "3985540",
    "end": "3991900"
  },
  {
    "text": "We'll talk about pros and cons in a second. But, um, let's see how- let's- let's describe this algorithm which is sometimes much",
    "start": "3991900",
    "end": "3998830"
  },
  {
    "text": "faster for gradient than gradient ascent for optimizing the value of theta.",
    "start": "3998830",
    "end": "4004560"
  },
  {
    "text": "Okay? So what we'd like to do is, uh, all right,",
    "start": "4004560",
    "end": "4010530"
  },
  {
    "text": "so let me- let me use this simplified one-dimensional problem to describe Newton's method.",
    "start": "4010530",
    "end": "4017020"
  },
  {
    "text": "So I'm going to solve a slightly different problem with Newton's method which is say you have some function f, right,",
    "start": "4022700",
    "end": "4031290"
  },
  {
    "text": "and you want to find a theta such that f of theta is equal to 0.",
    "start": "4031290",
    "end": "4042315"
  },
  {
    "text": "Okay? So this is a problem that Newton's method solves. And the way we're going to use this later is what you",
    "start": "4042315",
    "end": "4049170"
  },
  {
    "text": "really want is to maximize L of theta,",
    "start": "4049170",
    "end": "4053680"
  },
  {
    "text": "right, and well at the maximum the first derivative must be 0.",
    "start": "4057710",
    "end": "4063525"
  },
  {
    "text": "So i.e. you want to value where the derivative L prime of theta is equal to 0, right?",
    "start": "4063525",
    "end": "4072105"
  },
  {
    "text": "And L prime is the derivative of theta because this is where L prime is another notation for the first derivative of theta.",
    "start": "4072105",
    "end": "4079620"
  },
  {
    "text": "So you want to maximize a function or minimize a function. What that really means is you want to find a point where the derivative is equal to 0.",
    "start": "4079620",
    "end": "4086805"
  },
  {
    "text": "So the way we're going to use Newton's method is we're going to set F of theta equal to the derivative and then try to find the point where the derivative is equal to 0.",
    "start": "4086805",
    "end": "4095700"
  },
  {
    "text": "Okay? But to explain Newton's method I'm gonna, you know, work on this other problem where you have a function F and you just",
    "start": "4095700",
    "end": "4102870"
  },
  {
    "text": "want to find the value of theta where F of theta is equal to 0 and then- and we'll set F equal to L prime theta and that's how we'll we'll apply this to um, logistic regression.",
    "start": "4102870",
    "end": "4113055"
  },
  {
    "text": "So, let me draw in pictures how this algorithm works.",
    "start": "4113055",
    "end": "4119009"
  },
  {
    "text": "Uh. [NOISE] [BACKGROUND] All right.",
    "start": "4119010",
    "end": "4133619"
  },
  {
    "text": "So let's say that's the function f, and, you know, to make this drawable on a whiteboard,",
    "start": "4133620",
    "end": "4139545"
  },
  {
    "text": "I'm gonna assume theta is just a real number for now. So theta is just a single, you know, like a scalar, a real number.",
    "start": "4139545",
    "end": "4145995"
  },
  {
    "text": "Um, so this is how Newton's method works.",
    "start": "4145995",
    "end": "4151259"
  },
  {
    "text": "Um, oh, and the goal is to find this point. Right? The goal is to find the value of theta where f of theta is equal to 0.",
    "start": "4151260",
    "end": "4159944"
  },
  {
    "text": "Okay? So let's say you start off, um, right.",
    "start": "4159945",
    "end": "4165275"
  },
  {
    "text": "Let's say you start off at this point. Right? At the first iteration, you have randomly initialized data, and actually theta is zero or something.",
    "start": "4165275",
    "end": "4171555"
  },
  {
    "text": "But let's say you start off at that point. This is how one iteration of Newton's method will work,",
    "start": "4171555",
    "end": "4178170"
  },
  {
    "text": "which is- let me use a different color.",
    "start": "4178170",
    "end": "4183464"
  },
  {
    "text": "Right. Start off with theta 0, that's just a first value consideration. What we're going to do is look at the function f,",
    "start": "4183465",
    "end": "4190140"
  },
  {
    "text": "and then find a line that is just tangent to f. So take the derivative of f and",
    "start": "4190140",
    "end": "4195570"
  },
  {
    "text": "find a line that is just tangent to f. So take that red line.",
    "start": "4195570",
    "end": "4201869"
  },
  {
    "text": "It just touches the function f. And we're gonna use, if you will, use a straight line approximation to f,",
    "start": "4201870",
    "end": "4207165"
  },
  {
    "text": "and solve for where f touches the horizontal axis. So we're gonna solve for the point where this straight line touches the horizontal axis.",
    "start": "4207165",
    "end": "4216195"
  },
  {
    "text": "Okay? And then we're going to set this, and that's one iteration of Newton's method.",
    "start": "4216195",
    "end": "4221909"
  },
  {
    "text": "So we're gonna move from this value to this value, right? And then in the second iteration of Newton's method,",
    "start": "4221910",
    "end": "4229245"
  },
  {
    "text": "we're gonna look at this point. And again, you know, take a line that is just tangent to it,",
    "start": "4229245",
    "end": "4235349"
  },
  {
    "text": "and then solve for where this touches the horizontal axis, and then that's after two iterations of Newton's method.",
    "start": "4235350",
    "end": "4244680"
  },
  {
    "text": "Right. And then you repeat. Take this, sometimes you can overshoot a little bit, but that's okay.",
    "start": "4244680",
    "end": "4250200"
  },
  {
    "text": "Right? And then that's, um, there's a cycle back to red. Let's take the three,",
    "start": "4250200",
    "end": "4256260"
  },
  {
    "text": "then you take this, let's take the four. [NOISE] Excuse me.",
    "start": "4256260",
    "end": "4268900"
  },
  {
    "text": "So you can tell that Newton's method is actually a pretty fast algorithm.",
    "start": "4271130",
    "end": "4277110"
  },
  {
    "text": "Right? When in just one, two, three, four iterations, we've gotten really really close to the point where f of theta is equal to 0.",
    "start": "4277110",
    "end": "4288244"
  },
  {
    "text": "So let's write out the math for how you do this.",
    "start": "4288245",
    "end": "4293315"
  },
  {
    "text": "So um, let's see. I'm going to- so let me just write out the,",
    "start": "4293315",
    "end": "4299520"
  },
  {
    "text": "the derive, um, you know, how you go from theta 0 to theta 1. So I'm going to use this horizontal distance.",
    "start": "4299520",
    "end": "4306764"
  },
  {
    "text": "I'm gonna denote this as, uh, delta. This triangle is uppercase Greek alphabet delta.",
    "start": "4306765",
    "end": "4313875"
  },
  {
    "text": "Right? This is lowercase delta, that's uppercase delta. Right? Uh, and then the height here,",
    "start": "4313875",
    "end": "4319755"
  },
  {
    "text": "well that's just f of theta 0. Right? This is the height of- it's just f of theta 0.",
    "start": "4319755",
    "end": "4325815"
  },
  {
    "text": "And so, um, let's see.",
    "start": "4325815",
    "end": "4332940"
  },
  {
    "text": "Right. So, uh, what we'd like to do is solve for the value of delta,",
    "start": "4332940",
    "end": "4340080"
  },
  {
    "text": "because one iteration of Newton's method is a set, you know, of theta 1 is set to theta 0 minus delta.",
    "start": "4340080",
    "end": "4349800"
  },
  {
    "text": "Right? So how do you solve for delta? Well, from, uh, calculus we know that",
    "start": "4349800",
    "end": "4355559"
  },
  {
    "text": "the slope of the function f is the height over the run. Well, height over the width.",
    "start": "4355560",
    "end": "4360570"
  },
  {
    "text": "And so we know that the derivative of del- f prime, that's the derivative of f at the point theta 0,",
    "start": "4360570",
    "end": "4368310"
  },
  {
    "text": "that's equal to the height, that's f of theta, divided by the horizontal. Right? So the derivative,",
    "start": "4368310",
    "end": "4376755"
  },
  {
    "text": "meaning the slope of the red line is by definition the derivative is this ratio between this height over this width.",
    "start": "4376755",
    "end": "4383670"
  },
  {
    "text": "Um, and so delta is equal to f of theta 0 over f prime of theta 0.",
    "start": "4383670",
    "end": "4393360"
  },
  {
    "text": "And if you plug that in, then you find that a single iteration of Newton's method is",
    "start": "4393360",
    "end": "4399389"
  },
  {
    "text": "the following rule of theta t plus 1 gets updated as",
    "start": "4399390",
    "end": "4405015"
  },
  {
    "text": "theta t minus f of theta t over f prime of theta t. Okay.",
    "start": "4405015",
    "end": "4416640"
  },
  {
    "text": "Where instead of 0 and 1 I replaced them with t and t plus 1.",
    "start": "4416640",
    "end": "4421740"
  },
  {
    "text": "Right? Um, and finally to, to- you know,",
    "start": "4421740",
    "end": "4426900"
  },
  {
    "text": "the very first thing we did was let's let f of theta be equal to say L prime of theta.",
    "start": "4426900",
    "end": "4435255"
  },
  {
    "text": "Right? Because we wanna find the place where the first derivative of L is 0.",
    "start": "4435255",
    "end": "4440460"
  },
  {
    "text": "Then this becomes theta t plus 1, gets updated as theta t minus L prime of",
    "start": "4440460",
    "end": "4448710"
  },
  {
    "text": "theta t over L double prime of theta t. So it's really,",
    "start": "4448710",
    "end": "4456170"
  },
  {
    "text": "uh, the first derivative divided by the second derivative. Okay?",
    "start": "4456170",
    "end": "4461370"
  },
  {
    "text": "So Newton's method",
    "start": "4462640",
    "end": "4480120"
  },
  {
    "text": "is a very fast algorithm, and, uh, it has, um,",
    "start": "4480120",
    "end": "4485220"
  },
  {
    "text": "Newton's method enjoys a property called quadratic convergence.",
    "start": "4485220",
    "end": "4490990"
  },
  {
    "text": "Not a great name. Don't worry- don't worry too much about what it means. But informally, what it means is that, um,",
    "start": "4491360",
    "end": "4497970"
  },
  {
    "text": "if on one iteration Newton's method has 0.01 error,",
    "start": "4497970",
    "end": "4503580"
  },
  {
    "text": "so on the X axis, you're 0.01 away from the, from the value, from the true minimum,",
    "start": "4503580",
    "end": "4508980"
  },
  {
    "text": "or the true value of f is equal to 0. Um, after one iteration, the error could go to 0.0001 error,",
    "start": "4508980",
    "end": "4516825"
  },
  {
    "text": "and after two iterations it goes 0.00000001.",
    "start": "4516825",
    "end": "4520420"
  },
  {
    "text": "But roughly Newton's method, um, under certain assumptions, uh, uh,",
    "start": "4524090",
    "end": "4529545"
  },
  {
    "text": "that functions move not too far from quadratic, the number of significant digits that you have",
    "start": "4529545",
    "end": "4535140"
  },
  {
    "text": "converged, the minimum doubles on a single iteration. So this is called quadratic convergence. Um, and so when you get near the minimum,",
    "start": "4535140",
    "end": "4542010"
  },
  {
    "text": "Newton's method converges extremely rapidly. Right? So, so after a single iteration, it becomes much more accurate,",
    "start": "4542010",
    "end": "4547500"
  },
  {
    "text": "after another iteration it becomes way, way, way more accurate, which is why Newton's method requires relatively few iterations.",
    "start": "4547500",
    "end": "4554940"
  },
  {
    "text": "Um, and, uh, let's see. I have written out Newton's method for when theta is a real number.",
    "start": "4554940",
    "end": "4563610"
  },
  {
    "text": "Um, when theta is a vector, right?",
    "start": "4563610",
    "end": "4569830"
  },
  {
    "text": "Then the generalization of the rule I wrote above is the following, theta t plus 1 gets updated as theta t plus H that,",
    "start": "4572720",
    "end": "4585105"
  },
  {
    "text": "where H is the Hessian matrix.",
    "start": "4585105",
    "end": "4590080"
  },
  {
    "text": "So these details are written in the lecture notes. Um, but to give you a sense,",
    "start": "4595040",
    "end": "4600315"
  },
  {
    "text": "it- when theta is a vector, this is the vector of derivatives.",
    "start": "4600315",
    "end": "4605980"
  },
  {
    "text": "All right, so  I guess this R_n plus 1 dimensional. If theta is in R_n plus 1,",
    "start": "4606560",
    "end": "4614595"
  },
  {
    "text": "then this derivative respect to theta of the log-likelihood becomes a vector of derivatives,",
    "start": "4614595",
    "end": "4620910"
  },
  {
    "text": "and the Hessian matrix, this becomes a matrix as R_n plus 1 by n plus 1.",
    "start": "4620910",
    "end": "4628320"
  },
  {
    "text": "So it becomes a squared matrix with the dimension equal to the parameter vector theta. And the Hessian matrix is defined as the matrix of partial derivatives.",
    "start": "4628320",
    "end": "4638830"
  },
  {
    "text": "Right? So um, [NOISE] and so the disadvantage of Newton's method is that in high-dimensional problems,",
    "start": "4642530",
    "end": "4651674"
  },
  {
    "text": "if theta is a vector, then each step of Newton's method is much more expensive,",
    "start": "4651674",
    "end": "4656729"
  },
  {
    "text": "because, um, you're, you're either solving a linear system equations, or having to invert a pretty big matrix.",
    "start": "4656729",
    "end": "4662340"
  },
  {
    "text": "So if theta is ten-dimensional, you know, this involves inverting a 10 by 10 matrix, which is fine.",
    "start": "4662340",
    "end": "4668790"
  },
  {
    "text": "But if theta was 10,000 or 100,000, then each iteration requires computing like a",
    "start": "4668790",
    "end": "4674850"
  },
  {
    "text": "100,000 by a 100,000 matrix and inverting that, which is very hard. Right? It's actually very difficult to do that in very high-dimensional problems.",
    "start": "4674850",
    "end": "4682005"
  },
  {
    "text": "Um, so, you know, some rules of thumb, um, if the number of parameters you have",
    "start": "4682005",
    "end": "4689190"
  },
  {
    "text": "for- if the number of parameters in your iteration is not too big, if you have 10 parameters, or 50 parameters,",
    "start": "4689190",
    "end": "4694950"
  },
  {
    "text": "I would almost certainly- I would very likely use Newton's method, uh, because then you probably get convergence in maybe 10 iterations,",
    "start": "4694950",
    "end": "4704460"
  },
  {
    "text": "or, you know, 15 iterations, or even less than 10 iterations. But if you have a very large number of parameters,",
    "start": "4704460",
    "end": "4710280"
  },
  {
    "text": "if you have, you know, 10,000 parameters, then rather than dealing with a 10,000 by 10,000 matrix, or even bigger,",
    "start": "4710280",
    "end": "4716940"
  },
  {
    "text": "the 50 by 1000 by 50,000 matrix, and you have 50,000 parameters, I will use, uh, gradient descent then.",
    "start": "4716940",
    "end": "4723000"
  },
  {
    "text": "Okay? But if the number of parameters is not too big, so that the computational cost per iteration is manageable,",
    "start": "4723000",
    "end": "4729659"
  },
  {
    "text": "then Newton's method converges in a very small number of iterations, and, and could be much faster algorithm than gradient descent.",
    "start": "4729660",
    "end": "4736575"
  },
  {
    "text": "All right. So, um, that's it for, uh, Newton's method.",
    "start": "4736575",
    "end": "4742770"
  },
  {
    "text": "Um, on Wednesday, I guess we are running out of time. On Wednesday, you'll hear about generalized linear models.",
    "start": "4742770",
    "end": "4747824"
  },
  {
    "text": "Um, I think unfortunately I- I promised to be in Washington DC, uh, uh, tonight, I guess through Wednesday.",
    "start": "4747825",
    "end": "4754065"
  },
  {
    "text": "So, uh, you'll hear from some- I think Anand will give the lecture on Wednesday, uh, but I will be back next week.",
    "start": "4754065",
    "end": "4761415"
  },
  {
    "text": "So un- unfortunately was trying to do this, but because of his health things, he can't lecture. So Anand will do this Wednesday.",
    "start": "4761415",
    "end": "4767715"
  },
  {
    "text": "Thanks everyone. See you on Wednesday.",
    "start": "4767715",
    "end": "4769530"
  }
]