[
  {
    "text": "Great. So, uh, welcome back to the class.",
    "start": "4010",
    "end": "7620"
  },
  {
    "text": "Uh, we're starting a very exciting, uh,",
    "start": "7620",
    "end": "10184"
  },
  {
    "text": "new topic, uh, for which we have been building up,",
    "start": "10185",
    "end": "13320"
  },
  {
    "text": "uh, from the beginning of the course.",
    "start": "13320",
    "end": "15045"
  },
  {
    "text": "So today, we are going to talk about, uh,",
    "start": "15045",
    "end": "17415"
  },
  {
    "text": "deep learning for graphs and in particular,",
    "start": "17415",
    "end": "19649"
  },
  {
    "text": "uh, techniques around graph neural networks.",
    "start": "19649",
    "end": "22635"
  },
  {
    "text": "And this should be one of the most central topics of the- of the class.",
    "start": "22635",
    "end": "26100"
  },
  {
    "text": "And we are going to spend the next, uh, two weeks, uh,",
    "start": "26100",
    "end": "29595"
  },
  {
    "text": "discussing this and going, uh,",
    "start": "29595",
    "end": "31650"
  },
  {
    "text": "deeper, uh, into this exciting topic.",
    "start": "31650",
    "end": "34335"
  },
  {
    "text": "So the way we think of this is the following: so far,",
    "start": "34335",
    "end": "37640"
  },
  {
    "text": "we have been talking about node embeddings,",
    "start": "37640",
    "end": "39725"
  },
  {
    "text": "where our intuition was to map nodes of",
    "start": "39725",
    "end": "42800"
  },
  {
    "text": "the graph into d-dimensional embeddings such that,",
    "start": "42800",
    "end": "46010"
  },
  {
    "text": "uh, nodes similar in the graph are embedded close together.",
    "start": "46010",
    "end": "50030"
  },
  {
    "text": "And our goal was to learn this, uh,",
    "start": "50030",
    "end": "52085"
  },
  {
    "text": "function f that takes in a graph and gives us the positions,",
    "start": "52085",
    "end": "57095"
  },
  {
    "text": "the embeddings of individual nodes.",
    "start": "57095",
    "end": "60715"
  },
  {
    "text": "And the way we thought about this is,",
    "start": "60715",
    "end": "63025"
  },
  {
    "text": "we thought about this in this encoder-decoder framework where we said",
    "start": "63025",
    "end": "67295"
  },
  {
    "text": "that we want the similarity of nodes in the network,",
    "start": "67295",
    "end": "72069"
  },
  {
    "text": "uh, denoted as here to be similar or to",
    "start": "72069",
    "end": "75815"
  },
  {
    "text": "match the similarity of the nodes in the embedding space.",
    "start": "75815",
    "end": "79550"
  },
  {
    "text": "And here, by similarity,",
    "start": "79550",
    "end": "81050"
  },
  {
    "text": "we could measure distance.",
    "start": "81050",
    "end": "82750"
  },
  {
    "text": "Uh, there are many types of distances you can- you can quantify.",
    "start": "82750",
    "end": "86210"
  },
  {
    "text": "One type of a distance that we were interested in was cosine distance, a dot products.",
    "start": "86210",
    "end": "90860"
  },
  {
    "text": "So we say that, you know,",
    "start": "90860",
    "end": "92300"
  },
  {
    "text": "the dot product of the embeddings of the nodes",
    "start": "92300",
    "end": "94985"
  },
  {
    "text": "has to match the notion of similarity of them in the network.",
    "start": "94985",
    "end": "98360"
  },
  {
    "text": "So the goal was, that given an input network,",
    "start": "98360",
    "end": "100835"
  },
  {
    "text": "I want to encode the nodes by computing",
    "start": "100835",
    "end": "103940"
  },
  {
    "text": "their embeddings such that if nodes are similar in the network,",
    "start": "103940",
    "end": "107510"
  },
  {
    "text": "they are similar in the embedding space as well.",
    "start": "107510",
    "end": "110030"
  },
  {
    "text": "So their similarity, um, is higher.",
    "start": "110030",
    "end": "112735"
  },
  {
    "text": "And of course, when we talked about this,",
    "start": "112735",
    "end": "115385"
  },
  {
    "text": "we were defining about what does it mean to be",
    "start": "115385",
    "end": "117350"
  },
  {
    "text": "similar and what does it mean, uh, to encode?",
    "start": "117350",
    "end": "120354"
  },
  {
    "text": "And, ah, as I mentioned,",
    "start": "120355",
    "end": "122345"
  },
  {
    "text": "there are two key components in",
    "start": "122345",
    "end": "124700"
  },
  {
    "text": "this node embedding framework that we talked to- in- in the class so far.",
    "start": "124700",
    "end": "129259"
  },
  {
    "text": "So our goal was to make each node to a low-dimensional vector.",
    "start": "129260",
    "end": "133340"
  },
  {
    "text": "And, um, we wanted to encode the embedding of a node in this vector, uh, z_v.",
    "start": "133340",
    "end": "139870"
  },
  {
    "text": "And this is a d-dimensional embedding.",
    "start": "139870",
    "end": "143140"
  },
  {
    "text": "So, you know, a vector of d numbers.",
    "start": "143140",
    "end": "145700"
  },
  {
    "text": "And, uh, we also needed to specify the similarity function that specifies how",
    "start": "145700",
    "end": "150650"
  },
  {
    "text": "the relationships in the vector space",
    "start": "150650",
    "end": "153275"
  },
  {
    "text": "mapped to the relationships in the original network, right?",
    "start": "153275",
    "end": "156110"
  },
  {
    "text": "So we said, um, you know,",
    "start": "156110",
    "end": "157610"
  },
  {
    "text": "this is the similarity defined in terms of the network.",
    "start": "157610",
    "end": "160280"
  },
  {
    "text": "This is the similarity defined, uh,",
    "start": "160280",
    "end": "162425"
  },
  {
    "text": "in terms of the, um, embedding space,",
    "start": "162425",
    "end": "166080"
  },
  {
    "text": "and this similarity computation in the embedding space,",
    "start": "166080",
    "end": "169085"
  },
  {
    "text": "we can call this a dot,",
    "start": "169085",
    "end": "171005"
  },
  {
    "text": "uh, uh, we can call this a decoder.",
    "start": "171005",
    "end": "172685"
  },
  {
    "text": "So our goal was to encode the coordinate so that when we decode them,",
    "start": "172685",
    "end": "176974"
  },
  {
    "text": "they decode the similarity in the network.",
    "start": "176975",
    "end": "180175"
  },
  {
    "text": "Um, so far, we talked about what is called shallow encoding,",
    "start": "180175",
    "end": "184580"
  },
  {
    "text": "which is the simplest approach to, uh,",
    "start": "184580",
    "end": "186560"
  },
  {
    "text": "learning that encoder which is that basically encoder is just an embedding-look- look-up,",
    "start": "186560",
    "end": "191965"
  },
  {
    "text": "which means we said we are going to learn this matrix z,",
    "start": "191965",
    "end": "195004"
  },
  {
    "text": "where every node will have our column,",
    "start": "195005",
    "end": "197300"
  },
  {
    "text": "uh, reserved in this matrix.",
    "start": "197300",
    "end": "199265"
  },
  {
    "text": "And this- the way we are going to decode the- the embedding of a given node will simply",
    "start": "199265",
    "end": "204860"
  },
  {
    "text": "be to basically look at the appropriate column of",
    "start": "204860",
    "end": "207620"
  },
  {
    "text": "this matrix and say here is when it's embedding the store.",
    "start": "207620",
    "end": "210485"
  },
  {
    "text": "So this is what we mean by shallow because basically,",
    "start": "210485",
    "end": "213500"
  },
  {
    "text": "you are just memorizing the embedding of every node.",
    "start": "213500",
    "end": "216590"
  },
  {
    "text": "We are directly learning,",
    "start": "216590",
    "end": "218224"
  },
  {
    "text": "directly determining the embedding of every node.",
    "start": "218225",
    "end": "221890"
  },
  {
    "text": "Um, so what are some of the limitations of the approaches?",
    "start": "221890",
    "end": "225830"
  },
  {
    "text": "Um, like deep walk or nodes to end that we have talked about, uh,",
    "start": "225830",
    "end": "229850"
  },
  {
    "text": "so far that apply this, uh,",
    "start": "229850",
    "end": "231845"
  },
  {
    "text": "shallow approach to- to learning node embeddings.",
    "start": "231845",
    "end": "235010"
  },
  {
    "text": "First is that this is extremely expensive in",
    "start": "235010",
    "end": "238040"
  },
  {
    "text": "terms of the number of parameters that are needed, right?",
    "start": "238040",
    "end": "241219"
  },
  {
    "text": "The number of parameters that the model has,",
    "start": "241220",
    "end": "243470"
  },
  {
    "text": "the number of variables it has to",
    "start": "243470",
    "end": "245510"
  },
  {
    "text": "learn is proportional to the number of nodes in the network.",
    "start": "245510",
    "end": "248614"
  },
  {
    "text": "It is basically d times number of nodes.",
    "start": "248615",
    "end": "251550"
  },
  {
    "text": "This- the reason for this being is that for every node we have",
    "start": "251550",
    "end": "254300"
  },
  {
    "text": "to determine or learn d-parameters,",
    "start": "254300",
    "end": "257000"
  },
  {
    "text": "d-values that determine its embedding.",
    "start": "257000",
    "end": "259144"
  },
  {
    "text": "So, um, this means that, uh,",
    "start": "259145",
    "end": "261694"
  },
  {
    "text": "for huge graphs the parameter space will be- will be giant.",
    "start": "261695",
    "end": "265490"
  },
  {
    "text": "Um, there is no parameter sharing, uh, between the nodes.",
    "start": "265490",
    "end": "269599"
  },
  {
    "text": "Um, in some sense, every node has to determine its own unique embedding.",
    "start": "269600",
    "end": "273260"
  },
  {
    "text": "So there is a lot of computation that we need to do.",
    "start": "273260",
    "end": "276095"
  },
  {
    "text": "Then, um, this is what is called, uh, transductive.",
    "start": "276095",
    "end": "280685"
  },
  {
    "text": "Um, this means that in transductive learning,",
    "start": "280685",
    "end": "283235"
  },
  {
    "text": "we can only make predictions over the examples that we have actually seen,",
    "start": "283235",
    "end": "287645"
  },
  {
    "text": "uh, during the training phase.",
    "start": "287645",
    "end": "289310"
  },
  {
    "text": "So this means, in this case,",
    "start": "289310",
    "end": "290595"
  },
  {
    "text": "if you cannot generate an embedding for",
    "start": "290595",
    "end": "292610"
  },
  {
    "text": "a node that- for a node that was not seen during training.",
    "start": "292610",
    "end": "296009"
  },
  {
    "text": "We cannot transfer embeddings for one graph to another because for every graph,",
    "start": "296010",
    "end": "300470"
  },
  {
    "text": "for every node, we have to directly learn- learn that embedding in the training space.",
    "start": "300470",
    "end": "305989"
  },
  {
    "text": "And then, um, another important,",
    "start": "305989",
    "end": "308280"
  },
  {
    "text": "uh, uh, drawback of this shallow encoding- encoders,",
    "start": "308280",
    "end": "311345"
  },
  {
    "text": "as I said, like a deep walk or, uh,",
    "start": "311345",
    "end": "313430"
  },
  {
    "text": "node correct, is that they do not incorporate node features, right?",
    "start": "313430",
    "end": "316970"
  },
  {
    "text": "Many graphs have features,",
    "start": "316970",
    "end": "318830"
  },
  {
    "text": "properties attached to the nodes of the network.",
    "start": "318830",
    "end": "321370"
  },
  {
    "text": "Um, and these approaches do not naturally, uh, leverage them.",
    "start": "321370",
    "end": "325375"
  },
  {
    "text": "So today, we are going to talk about deep graph encoders.",
    "start": "325375",
    "end": "330200"
  },
  {
    "text": "So we are going to discuss graph neural networks that are examples of deep, um,",
    "start": "330200",
    "end": "336320"
  },
  {
    "text": "graph encoders where the idea is that this encoding of, uh, er, er,",
    "start": "336320",
    "end": "341570"
  },
  {
    "text": "of an embedding of our node v is",
    "start": "341570",
    "end": "344060"
  },
  {
    "text": "a multi-layers of nonlinear transformations based on the graph structure.",
    "start": "344060",
    "end": "348770"
  },
  {
    "text": "So basically now, we'll really think about",
    "start": "348770",
    "end": "350794"
  },
  {
    "text": "deep neural networks and how they are transforming",
    "start": "350795",
    "end": "353870"
  },
  {
    "text": "information through multiple layers of",
    "start": "353870",
    "end": "356570"
  },
  {
    "text": "nonlinear transforms to come up with the final embedding.",
    "start": "356570",
    "end": "360110"
  },
  {
    "text": "And important to note that all these deep encoders",
    "start": "360110",
    "end": "364039"
  },
  {
    "text": "can be combined also with node similarity functions in Lecture 3.",
    "start": "364040",
    "end": "367960"
  },
  {
    "text": "So we could say I want to learn a deep encoder that encodes the similarity function,",
    "start": "367960",
    "end": "373069"
  },
  {
    "text": "uh, for example, the random walk similarity function that we used,",
    "start": "373070",
    "end": "376445"
  },
  {
    "text": "uh, in the previous lectures.",
    "start": "376445",
    "end": "377845"
  },
  {
    "text": "Or in this case,",
    "start": "377845",
    "end": "379250"
  },
  {
    "text": "we can actually directly learn the- the-",
    "start": "379250",
    "end": "381380"
  },
  {
    "text": "the encoder so that it is able to decode the node labels,",
    "start": "381380",
    "end": "386660"
  },
  {
    "text": "which means we can actually directly train these models for, uh,",
    "start": "386660",
    "end": "390200"
  },
  {
    "text": "node classification or any kind of,",
    "start": "390200",
    "end": "392750"
  },
  {
    "text": "uh, graph prediction task.",
    "start": "392750",
    "end": "395575"
  },
  {
    "text": "So intuitively, what we would like to do,",
    "start": "395575",
    "end": "398480"
  },
  {
    "text": "or what we are going to do is we are going to develop",
    "start": "398480",
    "end": "400955"
  },
  {
    "text": "deep neural networks that on the left will- on the input will take graph,",
    "start": "400955",
    "end": "405694"
  },
  {
    "text": "uh, as a structure together with, uh, properties,",
    "start": "405695",
    "end": "408890"
  },
  {
    "text": "features of nodes, uh,",
    "start": "408890",
    "end": "410730"
  },
  {
    "text": "and potentially of edges.",
    "start": "410730",
    "end": "412095"
  },
  {
    "text": "We will send this to the multiple layers of",
    "start": "412095",
    "end": "414800"
  },
  {
    "text": "non-linear transformations in the network so that at the end,",
    "start": "414800",
    "end": "418669"
  },
  {
    "text": "in the output, we get, for example,",
    "start": "418670",
    "end": "420860"
  },
  {
    "text": "node embeddings, we also embed entire sub-graphs.",
    "start": "420860",
    "end": "424664"
  },
  {
    "text": "We can embed the pairs of nodes and make various kinds of predictions, uh, in the end.",
    "start": "424665",
    "end": "430070"
  },
  {
    "text": "And the good thing would be that we are able to train this in an end-to-end fashion.",
    "start": "430070",
    "end": "434960"
  },
  {
    "text": "So from the labels, uh,",
    "start": "434960",
    "end": "436694"
  },
  {
    "text": "on the right all the way down to the,",
    "start": "436695",
    "end": "439095"
  },
  {
    "text": "uh, graph structure, uh, on the left.",
    "start": "439095",
    "end": "441720"
  },
  {
    "text": "Um, and this would be in some sense,",
    "start": "441720",
    "end": "444300"
  },
  {
    "text": "task, uh, agnostic or it will be applicable to many different tasks.",
    "start": "444300",
    "end": "448490"
  },
  {
    "text": "We'll be able to do, uh, node classification,",
    "start": "448490",
    "end": "451055"
  },
  {
    "text": "we'll be able to do, uh, link prediction,",
    "start": "451055",
    "end": "453789"
  },
  {
    "text": "we'll be able to do any kind of clustering community detection,",
    "start": "453790",
    "end": "457160"
  },
  {
    "text": "as well as to measure similarity, um, or,",
    "start": "457160",
    "end": "460620"
  },
  {
    "text": "um, compatibility between different graphs or different sub-networks.",
    "start": "460620",
    "end": "464419"
  },
  {
    "text": "So, uh, this, uh, will really, uh,",
    "start": "464420",
    "end": "466700"
  },
  {
    "text": "allow us to be applied to any of these,",
    "start": "466700",
    "end": "469505"
  },
  {
    "text": "uh, different, uh, tasks.",
    "start": "469505",
    "end": "471970"
  },
  {
    "text": "So why is this interesting or why is it hard or why is it different from,",
    "start": "471970",
    "end": "477150"
  },
  {
    "text": "let's say, classical, uh,",
    "start": "477150",
    "end": "478544"
  },
  {
    "text": "machine learning or classical deep learning?",
    "start": "478545",
    "end": "480950"
  },
  {
    "text": "If you think about the classical deep learning toolbox,",
    "start": "480950",
    "end": "483935"
  },
  {
    "text": "it is designed for simple data types.",
    "start": "483935",
    "end": "486870"
  },
  {
    "text": "Essentially, what tra- traditional or current toolbox is really good at is,",
    "start": "486870",
    "end": "491449"
  },
  {
    "text": "we know how to process fixed-size matrices, right?",
    "start": "491450",
    "end": "495110"
  },
  {
    "text": "So basically, I can resize every image and I represent it as",
    "start": "495110",
    "end": "497629"
  },
  {
    "text": "a fixed-size matrix or as a fixed-size grid graph.",
    "start": "497630",
    "end": "500935"
  },
  {
    "text": "And I can also take,",
    "start": "500935",
    "end": "502230"
  },
  {
    "text": "for example, texts, speech,",
    "start": "502230",
    "end": "503805"
  },
  {
    "text": "and represent- think of it basically as a linear sequence,",
    "start": "503805",
    "end": "506810"
  },
  {
    "text": "as a chain graph.",
    "start": "506810",
    "end": "508070"
  },
  {
    "text": "And we know how to process that, uh, really, really well.",
    "start": "508070",
    "end": "510980"
  },
  {
    "text": "So kind of the claim, uh,",
    "start": "510980",
    "end": "512990"
  },
  {
    "text": "and motivation for this is that",
    "start": "512990",
    "end": "514459"
  },
  {
    "text": "modern deep learning toolbox is designed for simple data types,",
    "start": "514460",
    "end": "518240"
  },
  {
    "text": "meaning sequences, uh, linear sequences,",
    "start": "518240",
    "end": "520810"
  },
  {
    "text": "and, uh, fixed-size grids.",
    "start": "520810",
    "end": "522890"
  },
  {
    "text": "Um, and of course the question is,",
    "start": "522890",
    "end": "524600"
  },
  {
    "text": "how can we generalize that?",
    "start": "524600",
    "end": "525904"
  },
  {
    "text": "How can we apply deep learning representation learning to more complex data types?",
    "start": "525905",
    "end": "531215"
  },
  {
    "text": "And this is where graph neural networks come into play because they allow",
    "start": "531215",
    "end": "537020"
  },
  {
    "text": "us to apply representation learning to",
    "start": "537020",
    "end": "539510"
  },
  {
    "text": "much more complex data types than just the span of two very simple,",
    "start": "539510",
    "end": "543275"
  },
  {
    "text": "uh, uh, data types,",
    "start": "543275",
    "end": "545090"
  },
  {
    "text": "meaning the fixed size grids,",
    "start": "545090",
    "end": "546600"
  },
  {
    "text": "uh, and linear sequences.",
    "start": "546600",
    "end": "548589"
  },
  {
    "text": "And why is this hard?",
    "start": "548590",
    "end": "550640"
  },
  {
    "text": "Why this non-trivial to do?",
    "start": "550640",
    "end": "552215"
  },
  {
    "text": "It's because networks have a lot of complexity, right?",
    "start": "552215",
    "end": "555350"
  },
  {
    "text": "They have arbitrary size and they have complex topological structure, right?",
    "start": "555350",
    "end": "560300"
  },
  {
    "text": "There is no spatial locality like in grids.",
    "start": "560300",
    "end": "563625"
  },
  {
    "text": "Uh, this means there is also no reference point or at no fixed orderings on the nodes,",
    "start": "563625",
    "end": "569320"
  },
  {
    "text": "which means that in a graph there is no top left or bottom right as there is in a grid,",
    "start": "569320",
    "end": "575400"
  },
  {
    "text": "or you know there is no left and right,",
    "start": "575400",
    "end": "577490"
  },
  {
    "text": "as you can define in a text because it's a sequence.",
    "start": "577490",
    "end": "580024"
  },
  {
    "text": "In a graph, there is no notion of a reference point.",
    "start": "580025",
    "end": "582890"
  },
  {
    "text": "There is no notion of direction.",
    "start": "582890",
    "end": "584770"
  },
  {
    "text": "Um, and more interestingly, right,",
    "start": "584770",
    "end": "586490"
  },
  {
    "text": "these graphs are often dynamic and have multiple,",
    "start": "586490",
    "end": "589834"
  },
  {
    "text": "um multi-modal features assigned to the nodes and edges of them.",
    "start": "589835",
    "end": "594005"
  },
  {
    "text": "So this becomes, uh,",
    "start": "594005",
    "end": "595400"
  },
  {
    "text": "very- very interesting because it really, um, expands, uh,",
    "start": "595400",
    "end": "600090"
  },
  {
    "text": "ways in which we can describe the data and in which we",
    "start": "600090",
    "end": "602930"
  },
  {
    "text": "can represent the underlying domain in underlying data.",
    "start": "602930",
    "end": "606410"
  },
  {
    "text": "Because not everything can be represented as",
    "start": "606410",
    "end": "609690"
  },
  {
    "text": "a ma- fixed size matrix or as a linear sequence.",
    "start": "609690",
    "end": "613145"
  },
  {
    "text": "And there are- as I showed in the beginning,",
    "start": "613145",
    "end": "615300"
  },
  {
    "text": "right, in the first lecture,",
    "start": "615300",
    "end": "617225"
  },
  {
    "text": "there is a lot of domains,",
    "start": "617225",
    "end": "618319"
  },
  {
    "text": "a lot of use cases where, um,",
    "start": "618320",
    "end": "621115"
  },
  {
    "text": "proper graphical representation is,",
    "start": "621115",
    "end": "624149"
  },
  {
    "text": "uh, very, very important.",
    "start": "624150",
    "end": "626739"
  }
]