[
  {
    "start": "0",
    "end": "31000"
  },
  {
    "text": "Welcome, everyone, to the class.",
    "start": "4010",
    "end": "6720"
  },
  {
    "text": "Um, we are going to continue with the discussion of, uh,",
    "start": "6720",
    "end": "11070"
  },
  {
    "text": "what kind of design choices that we have when, uh,",
    "start": "11070",
    "end": "14099"
  },
  {
    "text": "training or, uh, designing graph neural networks.",
    "start": "14100",
    "end": "17984"
  },
  {
    "text": "Um, and then in the last part of the lecture today,",
    "start": "17984",
    "end": "21119"
  },
  {
    "text": "we're going to talk about graph neural network training, um,",
    "start": "21120",
    "end": "24255"
  },
  {
    "text": "and various other aspects of how do we make these,",
    "start": "24255",
    "end": "27390"
  },
  {
    "text": "um, models, uh, work.",
    "start": "27390",
    "end": "29685"
  },
  {
    "text": "So to start, um, to remind everyone,",
    "start": "29685",
    "end": "32744"
  },
  {
    "start": "31000",
    "end": "57000"
  },
  {
    "text": "we are discussing deep graph, uh,",
    "start": "32745",
    "end": "34469"
  },
  {
    "text": "encoders, in particular, graph neural networks,",
    "start": "34470",
    "end": "37589"
  },
  {
    "text": "where the idea is that given the input graph on the left,",
    "start": "37589",
    "end": "41550"
  },
  {
    "text": "we wanna transform it through several layers of non-linear, uh, transformations.",
    "start": "41550",
    "end": "46980"
  },
  {
    "text": "Uh, several layers of a neural network to, um,",
    "start": "46980",
    "end": "50515"
  },
  {
    "text": "come up with good predictions at the level of node's, uh,",
    "start": "50515",
    "end": "54110"
  },
  {
    "text": "address, as well as entire graphs.",
    "start": "54110",
    "end": "57275"
  },
  {
    "start": "57000",
    "end": "238000"
  },
  {
    "text": "Um, and the, the formalism we have",
    "start": "57275",
    "end": "60440"
  },
  {
    "text": "defined is called graph neural network, where basically,",
    "start": "60440",
    "end": "63655"
  },
  {
    "text": "for every node in the network,",
    "start": "63655",
    "end": "65420"
  },
  {
    "text": "we define the computation graph that is",
    "start": "65420",
    "end": "68420"
  },
  {
    "text": "based on the network neighborhood around that given target node.",
    "start": "68420",
    "end": "72619"
  },
  {
    "text": "So essentially this means that",
    "start": "72620",
    "end": "74210"
  },
  {
    "text": "the input graph network structure",
    "start": "74210",
    "end": "77195"
  },
  {
    "text": "around the target node defines the neural network structure.",
    "start": "77195",
    "end": "80645"
  },
  {
    "text": "And then we discussed that now,",
    "start": "80645",
    "end": "83240"
  },
  {
    "text": "in order to make this neural network, uh,",
    "start": "83240",
    "end": "85549"
  },
  {
    "text": "architecture work because every node gets to define its own neural network architecture,",
    "start": "85550",
    "end": "90800"
  },
  {
    "text": "its own computational graph that will depend on the position of the node in the network.",
    "start": "90800",
    "end": "96005"
  },
  {
    "text": "Then what we can do in the, um,",
    "start": "96005",
    "end": "98674"
  },
  {
    "text": "to make this work is we have to define several different operators,",
    "start": "98675",
    "end": "102575"
  },
  {
    "text": "um, in the architecture.",
    "start": "102575",
    "end": "104689"
  },
  {
    "text": "So first we said we need to define what we call a message passing function,",
    "start": "104690",
    "end": "108185"
  },
  {
    "text": "a message transformation function that will take the message from the child,",
    "start": "108185",
    "end": "112250"
  },
  {
    "text": "transform it, and pass it towards the parent.",
    "start": "112250",
    "end": "114840"
  },
  {
    "text": "We have to define the notion of message aggregation that",
    "start": "114840",
    "end": "118195"
  },
  {
    "text": "will take the transformed messages from the children",
    "start": "118195",
    "end": "121970"
  },
  {
    "text": "and aggregate them in",
    "start": "121970",
    "end": "123890"
  },
  {
    "text": "an order invariant way to produce the- the one combined message from the children.",
    "start": "123890",
    "end": "130250"
  },
  {
    "text": "Then we said that when this message arrives to the parent,",
    "start": "130250",
    "end": "135080"
  },
  {
    "text": "we need to decide how to combine it with the parent's own message",
    "start": "135080",
    "end": "138860"
  },
  {
    "text": "from the previous level to then create the embedding of the node,",
    "start": "138860",
    "end": "143090"
  },
  {
    "text": "which can then be passed on.",
    "start": "143090",
    "end": "145330"
  },
  {
    "text": "So this is how we defined a single layer of a graph neural network.",
    "start": "145330",
    "end": "150065"
  },
  {
    "text": "Then we discussed how to combine or how do you link, or stack multiple layers together.",
    "start": "150065",
    "end": "156925"
  },
  {
    "text": "Um, and what we are going to discuss today,",
    "start": "156925",
    "end": "159815"
  },
  {
    "text": "is the point Number 4 around what kind of graph and",
    "start": "159815",
    "end": "163520"
  },
  {
    "text": "feature augmentation can we create to shape the structure of this neural network?",
    "start": "163520",
    "end": "169130"
  },
  {
    "text": "As well as we are going to talk about",
    "start": "169130",
    "end": "171635"
  },
  {
    "text": "learning objectives and how to make the training work.",
    "start": "171635",
    "end": "175069"
  },
  {
    "text": "So that's, uh, the plan for today.",
    "start": "175070",
    "end": "177450"
  },
  {
    "text": "So first, let's talk about graph augmentation for,",
    "start": "177450",
    "end": "181625"
  },
  {
    "text": "uh, graph neural networks.",
    "start": "181625",
    "end": "183190"
  },
  {
    "text": "So the idea is that raw input graph does not",
    "start": "183190",
    "end": "187790"
  },
  {
    "text": "necessarily need to present the underlying computation graph, all right?",
    "start": "187790",
    "end": "192519"
  },
  {
    "text": "So what I discussed so far was that if I wanna create",
    "start": "192520",
    "end": "195410"
  },
  {
    "text": "a graph neural network for a given target node in the network,",
    "start": "195410",
    "end": "198635"
  },
  {
    "text": "then I take the information from the neighbors of it.",
    "start": "198635",
    "end": "201980"
  },
  {
    "text": "Here are the neighbors,",
    "start": "201980",
    "end": "203075"
  },
  {
    "text": "and then each of the neighbors takes information from its own neighbors.",
    "start": "203075",
    "end": "206360"
  },
  {
    "text": "And this defines the graph neural network.",
    "start": "206360",
    "end": "208745"
  },
  {
    "text": "However, um, this translation from",
    "start": "208745",
    "end": "211790"
  },
  {
    "text": "the input graph structure to",
    "start": "211790",
    "end": "213799"
  },
  {
    "text": "the graph neural network structure does not need to be kind of one-to-one.",
    "start": "213800",
    "end": "217565"
  },
  {
    "text": "I don't need to take the raw input graph and,",
    "start": "217565",
    "end": "221045"
  },
  {
    "text": "uh, interpret it as the computational graph.",
    "start": "221045",
    "end": "223565"
  },
  {
    "text": "I can use, um,",
    "start": "223565",
    "end": "224975"
  },
  {
    "text": "various kinds of techniques to create the computation graph of the graph neural network.",
    "start": "224975",
    "end": "229565"
  },
  {
    "text": "And the two techniques we are going to talk about is",
    "start": "229565",
    "end": "232235"
  },
  {
    "text": "graph feature augmentation and graph structure augmentation.",
    "start": "232235",
    "end": "237440"
  },
  {
    "text": "So what we assume so far as I said,",
    "start": "237440",
    "end": "240140"
  },
  {
    "start": "238000",
    "end": "349000"
  },
  {
    "text": "is that the raw input graph directly defines",
    "start": "240140",
    "end": "242270"
  },
  {
    "text": "the computational graph of the graph neural network.",
    "start": "242270",
    "end": "245060"
  },
  {
    "text": "And there are many good reasons why we would- why we would want to break this assumption.",
    "start": "245060",
    "end": "249170"
  },
  {
    "text": "So, um, we would wanna break it at the level of node features.",
    "start": "249170",
    "end": "252995"
  },
  {
    "text": "Many times, for example,",
    "start": "252995",
    "end": "254525"
  },
  {
    "text": "input graphs may lack, um,",
    "start": "254525",
    "end": "256745"
  },
  {
    "text": "features, attributes, perhaps you wanna- sometimes,",
    "start": "256745",
    "end": "261005"
  },
  {
    "text": "um, the features are also, uh,",
    "start": "261005",
    "end": "262790"
  },
  {
    "text": "hard to encode so we may wanna help the neural network to learn a given concept easier.",
    "start": "262790",
    "end": "269330"
  },
  {
    "text": "And then in terms of graph structure, sometimes,",
    "start": "269330",
    "end": "272300"
  },
  {
    "text": "graphs- input graphs tend to be too sparse and it's",
    "start": "272300",
    "end": "275854"
  },
  {
    "text": "inefficient to do message passing over",
    "start": "275855",
    "end": "278030"
  },
  {
    "text": "a very sparse graph, it would take a lot of iterations,",
    "start": "278030",
    "end": "280595"
  },
  {
    "text": "a lot of GNN depth.",
    "start": "280595",
    "end": "282100"
  },
  {
    "text": "Sometimes they are too dense and the message passing becomes too costly.",
    "start": "282100",
    "end": "286205"
  },
  {
    "text": "If you think, for example,",
    "start": "286205",
    "end": "287509"
  },
  {
    "text": "doing message passing on an - on top of",
    "start": "287510",
    "end": "289670"
  },
  {
    "text": "an Instagram or Twitter network and you hit the Kim Kardashian node,",
    "start": "289670",
    "end": "294890"
  },
  {
    "text": "then you need to aggregate from all her gazillions of followers, right?",
    "start": "294890",
    "end": "299960"
  },
  {
    "text": "So that is very expensive.",
    "start": "299960",
    "end": "301910"
  },
  {
    "text": "So the question is when you hit a,",
    "start": "301910",
    "end": "303230"
  },
  {
    "text": "a high degree node,",
    "start": "303230",
    "end": "304600"
  },
  {
    "text": "what do you- what do you do?",
    "start": "304600",
    "end": "306490"
  },
  {
    "text": "Do you really need to aggregate from all the neighbors of that high degree node?",
    "start": "306490",
    "end": "311224"
  },
  {
    "text": "Or perhaps can you just select a subset of the neighbors?",
    "start": "311225",
    "end": "314660"
  },
  {
    "text": "And then another important consideration is that sometimes this graph is just too large,",
    "start": "314660",
    "end": "319535"
  },
  {
    "text": "so we cannot fit the computation graph into the GPU memory.",
    "start": "319535",
    "end": "324000"
  },
  {
    "text": "And again, certain augmentation techniques are needed.",
    "start": "324000",
    "end": "327785"
  },
  {
    "text": "So basically, the point is that sometimes",
    "start": "327785",
    "end": "330890"
  },
  {
    "text": "it is unlikely that the input graph happens to be",
    "start": "330890",
    "end": "333620"
  },
  {
    "text": "the optimal computation graph for computing",
    "start": "333620",
    "end": "336835"
  },
  {
    "text": "GNN-based embeddings and the techniques we are",
    "start": "336835",
    "end": "339319"
  },
  {
    "text": "going to discuss next will give you some ideas.",
    "start": "339320",
    "end": "341930"
  },
  {
    "text": "What can we do to improve the structure of the graph so that it",
    "start": "341930",
    "end": "345350"
  },
  {
    "text": "lends better to the graph neural network embeddings?",
    "start": "345350",
    "end": "349795"
  },
  {
    "start": "349000",
    "end": "401000"
  },
  {
    "text": "So we are going to talk about, uh,",
    "start": "349795",
    "end": "352380"
  },
  {
    "text": "augmentation approaches and we are going to talk",
    "start": "352380",
    "end": "355550"
  },
  {
    "text": "about in particular the graph feature augmentation, where,",
    "start": "355550",
    "end": "359039"
  },
  {
    "text": "um, it- it can be the case that the input graph lacks attributes,",
    "start": "359040",
    "end": "364715"
  },
  {
    "text": "lacks features and we are going to create features so that,",
    "start": "364715",
    "end": "368780"
  },
  {
    "text": "uh, GNN has easier time to learn.",
    "start": "368780",
    "end": "370790"
  },
  {
    "text": "And then we'll also talk about the graph structure augmentation.",
    "start": "370790",
    "end": "374975"
  },
  {
    "text": "As I said, if graph is too sparse,",
    "start": "374975",
    "end": "376685"
  },
  {
    "text": "we can- we will be able to add virtual nodes and edges.",
    "start": "376685",
    "end": "379685"
  },
  {
    "text": "If it's too dense,",
    "start": "379685",
    "end": "380900"
  },
  {
    "text": "we can decide to do some kind of sampling of neighbors when doing message-passing.",
    "start": "380900",
    "end": "385384"
  },
  {
    "text": "And if graph is too large,",
    "start": "385385",
    "end": "387170"
  },
  {
    "text": "then we can subsample subgraphs to compute the embeddings.",
    "start": "387170",
    "end": "391325"
  },
  {
    "text": "And this last point we are going to,",
    "start": "391325",
    "end": "393725"
  },
  {
    "text": "to talk in more detail when we discuss scaling up GNNs.",
    "start": "393725",
    "end": "398060"
  },
  {
    "text": "But these are some of the techniques we are going to learn about today.",
    "start": "398060",
    "end": "401725"
  },
  {
    "start": "401000",
    "end": "1073000"
  },
  {
    "text": "So why do we need feature augmentation, right?",
    "start": "401725",
    "end": "405450"
  },
  {
    "text": "So first, I wanna talk about graph feature augmentation.",
    "start": "405450",
    "end": "408660"
  },
  {
    "text": "So let's discuss why do we need this.",
    "start": "408660",
    "end": "410775"
  },
  {
    "text": "Sometimes, input graphs do not have any node features.",
    "start": "410775",
    "end": "414440"
  },
  {
    "text": "This is common, right?",
    "start": "414440",
    "end": "415760"
  },
  {
    "text": "If the input is just the graph adjacency matrix.",
    "start": "415760",
    "end": "418985"
  },
  {
    "text": "And what I'm going to discuss next is several standard approaches.",
    "start": "418985",
    "end": "422990"
  },
  {
    "text": "How do you deal with this situation and what can you do?",
    "start": "422990",
    "end": "425889"
  },
  {
    "text": "So first idea is that you simply assign a constant value,",
    "start": "425890",
    "end": "430580"
  },
  {
    "text": "a constant feature to every node.",
    "start": "430580",
    "end": "433444"
  },
  {
    "text": "So basically all the nodes have the same future value, value of 1.",
    "start": "433445",
    "end": "437225"
  },
  {
    "text": "And then if you think of what aggregation does,",
    "start": "437225",
    "end": "439670"
  },
  {
    "text": "it basically counts how many neighbors,",
    "start": "439670",
    "end": "441860"
  },
  {
    "text": "how do- does a node have at level- at Level 1?",
    "start": "441860",
    "end": "445039"
  },
  {
    "text": "How many do they have at Level 2?",
    "start": "445040",
    "end": "446750"
  },
  {
    "text": "How many do they have at Level 3?",
    "start": "446750",
    "end": "448495"
  },
  {
    "text": "So this would, in some sense,",
    "start": "448495",
    "end": "450000"
  },
  {
    "text": "allow you still to capture some notion of how does",
    "start": "450000",
    "end": "452690"
  },
  {
    "text": "the network neighborhood structure about a given node look like,",
    "start": "452690",
    "end": "456260"
  },
  {
    "text": "even though all the nodes have the same feature,",
    "start": "456260",
    "end": "459770"
  },
  {
    "text": "which is uh, which has a value 1.",
    "start": "459770",
    "end": "462610"
  },
  {
    "text": "Another idea that you can do is to assign unique IDs to nodes.",
    "start": "462610",
    "end": "467435"
  },
  {
    "text": "So basically these IDs are then converted to one-hot vectors, right?",
    "start": "467435",
    "end": "472205"
  },
  {
    "text": "So basically it means if you have a network on, um, six nodes,",
    "start": "472205",
    "end": "476930"
  },
  {
    "text": "then the idea is that you can assign a one-hot encoding to every node in the network.",
    "start": "476930",
    "end": "482419"
  },
  {
    "text": "So what I mean by this is now a feature vector for every node in",
    "start": "482420",
    "end": "486080"
  },
  {
    "text": "the network is simply a six-dimensional binary vector where you know,",
    "start": "486080",
    "end": "489919"
  },
  {
    "text": "node ID Number 5 has a- has a value of 1 up here.",
    "start": "489920",
    "end": "494345"
  },
  {
    "text": "If this is, you know, the node ID Number 5.",
    "start": "494345",
    "end": "497495"
  },
  {
    "text": "Um, also notice that this ordering of the nodes is totally arbitrary.",
    "start": "497495",
    "end": "503270"
  },
  {
    "text": "So there are some issues with one-hot encodings because it might",
    "start": "503270",
    "end": "507080"
  },
  {
    "text": "be hard or impossible to generalize them across different graphs.",
    "start": "507080",
    "end": "510830"
  },
  {
    "text": "But if you work with a single graph,",
    "start": "510830",
    "end": "512510"
  },
  {
    "text": "then this type of approach might be fine because",
    "start": "512510",
    "end": "516050"
  },
  {
    "text": "every node basically has now a unique one-hot encoding.",
    "start": "516050",
    "end": "520490"
  },
  {
    "text": "There is a flag value 1 at the ID of that single node.",
    "start": "520490",
    "end": "525605"
  },
  {
    "text": "And this now allows you to,to learn very expressive models because the models know",
    "start": "525605",
    "end": "531260"
  },
  {
    "text": "actually what are the IDs of the neighbors of the node in the network.",
    "start": "531260",
    "end": "537340"
  },
  {
    "text": "Of course, it might be costly because now your feature representation, your,",
    "start": "537340",
    "end": "542450"
  },
  {
    "text": "your number of attributes that the node has number of features,",
    "start": "542450",
    "end": "546680"
  },
  {
    "text": "the node has equals the number of nodes in the network.",
    "start": "546680",
    "end": "549050"
  },
  {
    "text": "So- so, um, that,",
    "start": "549050",
    "end": "550540"
  },
  {
    "text": "that is quite an expensive feature representation",
    "start": "550540",
    "end": "553550"
  },
  {
    "text": "for a node if the network is large. [NOISE]",
    "start": "553550",
    "end": "557035"
  },
  {
    "text": "So how do these,",
    "start": "557035",
    "end": "558769"
  },
  {
    "text": "uh, two approaches compare, right?",
    "start": "558770",
    "end": "560600"
  },
  {
    "text": "How does this adding a constant feature versus one-hot encoding,",
    "start": "560600",
    "end": "564154"
  },
  {
    "text": "uh, how do they compare, right?",
    "start": "564155",
    "end": "565730"
  },
  {
    "text": "In terms of their expressive power constant feature,",
    "start": "565730",
    "end": "568940"
  },
  {
    "text": "so every node having a value of 1,",
    "start": "568940",
    "end": "571265"
  },
  {
    "text": "um, has kind of medium expressive power, right?",
    "start": "571265",
    "end": "574265"
  },
  {
    "text": "Not- all nodes are identical,",
    "start": "574265",
    "end": "575974"
  },
  {
    "text": "but as- as we will talk about this later, uh,",
    "start": "575974",
    "end": "579005"
  },
  {
    "text": "GNN can still learn about",
    "start": "579005",
    "end": "580940"
  },
  {
    "text": "the graph structure and the neighborhood structure around the node, right?",
    "start": "580940",
    "end": "583790"
  },
  {
    "text": "In some sense, let's say an aggregation function like summation allows you to say,",
    "start": "583790",
    "end": "587720"
  },
  {
    "text": "\"Hi, I have three- if this is the node of interest,",
    "start": "587720",
    "end": "589970"
  },
  {
    "text": "I have three neighbors at level 1.",
    "start": "589970",
    "end": "591889"
  },
  {
    "text": "I have, let's say, uh,",
    "start": "591890",
    "end": "593060"
  },
  {
    "text": "two neigh- two neighbors at level 2,\" and so on and so forth, right?",
    "start": "593060",
    "end": "596090"
  },
  {
    "text": "So this is what this we'll be able to learn.",
    "start": "596090",
    "end": "597770"
  },
  {
    "text": "So it's still able to capture some simple part of the graph, uh, structure.",
    "start": "597770",
    "end": "601700"
  },
  {
    "text": "Uh, one-hot encoding, uh,",
    "start": "601700",
    "end": "603935"
  },
  {
    "text": "has high expressive power, right?",
    "start": "603935",
    "end": "605555"
  },
  {
    "text": "Each node has a unique ID,",
    "start": "605555",
    "end": "607190"
  },
  {
    "text": "so a node's specific information can be stored or can be retrieved or learned.",
    "start": "607190",
    "end": "612035"
  },
  {
    "text": "So you can learn, \"Oh,",
    "start": "612035",
    "end": "613100"
  },
  {
    "text": "I have a neighbor with ID number 2,\" and",
    "start": "613100",
    "end": "615560"
  },
  {
    "text": "perhaps that is important to determine your own label.",
    "start": "615560",
    "end": "618410"
  },
  {
    "text": "So, um, the expressive power is very high.",
    "start": "618410",
    "end": "621004"
  },
  {
    "text": "Um, you know, do we allow for- does this approach allow for,",
    "start": "621005",
    "end": "624035"
  },
  {
    "text": "uh, in, uh, an inductive learning capability?",
    "start": "624035",
    "end": "626365"
  },
  {
    "text": "What- what does this mean is, uh,",
    "start": "626365",
    "end": "628145"
  },
  {
    "text": "generalization to unseen nodes or generalization to, um,",
    "start": "628145",
    "end": "631940"
  },
  {
    "text": "nodes that are not yet part of the network or",
    "start": "631940",
    "end": "634280"
  },
  {
    "text": "generalization to a new graph that I have never seen during training.",
    "start": "634280",
    "end": "638060"
  },
  {
    "text": "Of course, the constant feature has high inductive, uh,",
    "start": "638060",
    "end": "642154"
  },
  {
    "text": "learning, uh, capability, because it's simple to generalize the new nodes, to new graphs.",
    "start": "642155",
    "end": "646610"
  },
  {
    "text": "We assign constant feature to them and just apply a GNN.",
    "start": "646610",
    "end": "649745"
  },
  {
    "text": "While for example, in, um, uh,",
    "start": "649745",
    "end": "652100"
  },
  {
    "text": "in the one-hot encoding case,",
    "start": "652100",
    "end": "654185"
  },
  {
    "text": "uh, we cannot really do that, right?",
    "start": "654185",
    "end": "655790"
  },
  {
    "text": "Because, uh, we cannot generalize to new nodes, right?",
    "start": "655790",
    "end": "658625"
  },
  {
    "text": "New nodes introduce new IDs.",
    "start": "658625",
    "end": "660740"
  },
  {
    "text": "Those IDs were not part of the training.",
    "start": "660740",
    "end": "662915"
  },
  {
    "text": "The GNN does not know how to,",
    "start": "662915",
    "end": "665389"
  },
  {
    "text": "uh, embed unseen nodes.",
    "start": "665390",
    "end": "667220"
  },
  {
    "text": "Um, so this is the- this is the issue,",
    "start": "667220",
    "end": "670564"
  },
  {
    "text": "um, in terms of one-hot encodings.",
    "start": "670565",
    "end": "672740"
  },
  {
    "text": "You need to know the entire node set at the time of training,",
    "start": "672740",
    "end": "676654"
  },
  {
    "text": "and you need to know the edges of that node set at the time of training.",
    "start": "676655",
    "end": "680180"
  },
  {
    "text": "In terms of computational cost, um,",
    "start": "680180",
    "end": "682305"
  },
  {
    "text": "as I said, uh,",
    "start": "682305",
    "end": "683825"
  },
  {
    "text": "constant node feature is very cheap.",
    "start": "683825",
    "end": "686060"
  },
  {
    "text": "It's just one- one value per node,",
    "start": "686060",
    "end": "688370"
  },
  {
    "text": "while the one-hot encoding has high because each node has a,",
    "start": "688370",
    "end": "693080"
  },
  {
    "text": "um, uh, feature vector that is length of the size of the network, right?",
    "start": "693080",
    "end": "698060"
  },
  {
    "text": "Its, uh, number of vertices is the dimensionality of the feature vector,",
    "start": "698060",
    "end": "701540"
  },
  {
    "text": "so we cannot apply these to larger graphs.",
    "start": "701540",
    "end": "703430"
  },
  {
    "text": "And then, you know, when would you apply one or the other?",
    "start": "703430",
    "end": "705980"
  },
  {
    "text": "You would, um, you can apply the constant feature essentially to any graph,",
    "start": "705980",
    "end": "709610"
  },
  {
    "text": "uh, whenever you care about inductive setting and generalizing to new nodes.",
    "start": "709610",
    "end": "715160"
  },
  {
    "text": "Uh, but, uh, the expressive power of the model will be limited.",
    "start": "715160",
    "end": "718985"
  },
  {
    "text": "On the other hand, one-hot encoding, um,",
    "start": "718985",
    "end": "721279"
  },
  {
    "text": "is very powerful, allows you to learn much more intricate structure around the network.",
    "start": "721280",
    "end": "726500"
  },
  {
    "text": "But it can only be applied to small graphs and to",
    "start": "726500",
    "end": "729020"
  },
  {
    "text": "transductive settings basically where all the nodes",
    "start": "729020",
    "end": "731600"
  },
  {
    "text": "and all the edges are known at the time of the training of the model.",
    "start": "731600",
    "end": "737060"
  },
  {
    "text": "So this was in terms of, uh,",
    "start": "737060",
    "end": "739620"
  },
  {
    "text": "feature- one idea of feature augmentation when we have no features, uh, on the nodes.",
    "start": "739620",
    "end": "744545"
  },
  {
    "text": "Another motivation for why we would want to do",
    "start": "744545",
    "end": "747649"
  },
  {
    "text": "some feature augmentation is that sometimes,",
    "start": "747650",
    "end": "751490"
  },
  {
    "text": "uh, certain structures are hard to learn for a GNN.",
    "start": "751490",
    "end": "754615"
  },
  {
    "text": "So sometimes, we actually want to encode a bit of",
    "start": "754615",
    "end": "757310"
  },
  {
    "text": "a graph structure into the node attribute vector as well.",
    "start": "757310",
    "end": "760460"
  },
  {
    "text": "So, uh, the idea, for example,",
    "start": "760460",
    "end": "762380"
  },
  {
    "text": "is that, um, to give you an example,",
    "start": "762380",
    "end": "764810"
  },
  {
    "text": "like kind of an edge example is that it's very hard for a GNN to count,",
    "start": "764810",
    "end": "769400"
  },
  {
    "text": "uh, you know, what's the length of a cycle a node is on?",
    "start": "769400",
    "end": "772475"
  },
  {
    "text": "So, um, and the question is, you know,",
    "start": "772475",
    "end": "774620"
  },
  {
    "text": "could a- could a GNN learn the length of a cycle a given node, uh, resides in?",
    "start": "774620",
    "end": "779375"
  },
  {
    "text": "And unless you have discriminative node features,",
    "start": "779375",
    "end": "782410"
  },
  {
    "text": "uh, this is not possible, right?",
    "start": "782410",
    "end": "784279"
  },
  {
    "text": "So what I mean by it is, for example,",
    "start": "784280",
    "end": "785745"
  },
  {
    "text": "here in this example,",
    "start": "785745",
    "end": "787020"
  },
  {
    "text": "node v_1 resides on a cycle of length 3,",
    "start": "787020",
    "end": "790565"
  },
  {
    "text": "while here this, the node v_1 resides on a cycle of length 4.",
    "start": "790565",
    "end": "794630"
  },
  {
    "text": "And in both cases, right,",
    "start": "794630",
    "end": "796100"
  },
  {
    "text": "v_1 has degree 2,",
    "start": "796100",
    "end": "797540"
  },
  {
    "text": "and its neighbors have degree 2.",
    "start": "797540",
    "end": "799370"
  },
  {
    "text": "So it's kind of the question is, you know,",
    "start": "799370",
    "end": "801275"
  },
  {
    "text": "how do I- how does this node know that now it's in a cycle of,",
    "start": "801275",
    "end": "804260"
  },
  {
    "text": "uh, length, uh, 4 versus 3.",
    "start": "804260",
    "end": "806690"
  },
  {
    "text": "Something like this is very important, for example,",
    "start": "806690",
    "end": "809150"
  },
  {
    "text": "in chemistry because these- these could be different kinds of, uh,",
    "start": "809150",
    "end": "812390"
  },
  {
    "text": "chemical structures or different kinds of, uh, ring structures.",
    "start": "812390",
    "end": "816380"
  },
  {
    "text": "And, uh, the reason why a, uh, uh,",
    "start": "816380",
    "end": "820010"
  },
  {
    "text": "plain GNN cannot differentiate between,",
    "start": "820010",
    "end": "823135"
  },
  {
    "text": "you know, node 1 in, uh,",
    "start": "823135",
    "end": "825560"
  },
  {
    "text": "a cycle of length 3 versus a cycle of leng- length 4",
    "start": "825560",
    "end": "828130"
  },
  {
    "text": "is that if you look at the GNN computation graph,",
    "start": "828130",
    "end": "831065"
  },
  {
    "text": "re- both- for both of these nodes V_1 and V_2,",
    "start": "831065",
    "end": "834315"
  },
  {
    "text": "the computation graph is exactly the same.",
    "start": "834315",
    "end": "836705"
  },
  {
    "text": "Meaning, V_1- V_1 and V_2 have two neighbors,",
    "start": "836705",
    "end": "839660"
  },
  {
    "text": "um, each, and then, you know,",
    "start": "839660",
    "end": "841550"
  },
  {
    "text": "these neighbors have, uh,",
    "start": "841550",
    "end": "842870"
  },
  {
    "text": "one neighbor each and the computation graph will always look like this.",
    "start": "842870",
    "end": "846725"
  },
  {
    "text": "Uh, unless, right, you have some way to discriminate nodes based on the features.",
    "start": "846725",
    "end": "851704"
  },
  {
    "text": "But if the nodes have the same,",
    "start": "851705",
    "end": "854035"
  },
  {
    "text": "um, set of, uh,",
    "start": "854035",
    "end": "855290"
  },
  {
    "text": "features that not- you cannot discriminate them based on their attributes,",
    "start": "855290",
    "end": "858910"
  },
  {
    "text": "then you cannot learn,",
    "start": "858910",
    "end": "860649"
  },
  {
    "text": "uh, to discriminate node V_1 from V_2.",
    "start": "860650",
    "end": "863315"
  },
  {
    "text": "They will- from the GNN point of view,",
    "start": "863315",
    "end": "865150"
  },
  {
    "text": "they will all, uh, look the same.",
    "start": "865150",
    "end": "867430"
  },
  {
    "text": "I'm going to go into more, uh,",
    "start": "867430",
    "end": "869630"
  },
  {
    "text": "depth, uh, uh, around this,",
    "start": "869630",
    "end": "872170"
  },
  {
    "text": "uh, this example and, er,",
    "start": "872170",
    "end": "873970"
  },
  {
    "text": "what are some very important implications of it and",
    "start": "873970",
    "end": "876639"
  },
  {
    "text": "consequences of it when we are going to discuss the theory of,",
    "start": "876640",
    "end": "880625"
  },
  {
    "text": "uh, graph neural networks.",
    "start": "880625",
    "end": "881950"
  },
  {
    "text": "But for now, uh,",
    "start": "881950",
    "end": "883120"
  },
  {
    "text": "the important thing is to see to understand that a GNN has a hard time capturing, um,",
    "start": "883120",
    "end": "888365"
  },
  {
    "text": "or is not able to capture whether a node is on",
    "start": "888365",
    "end": "890890"
  },
  {
    "text": "a length 3 cycle or a length 4 cycle unless these nodes,",
    "start": "890890",
    "end": "895390"
  },
  {
    "text": "um, on the cycle would have some discriminative features.",
    "start": "895390",
    "end": "898475"
  },
  {
    "text": "And the reason why we are not able to distinguish between",
    "start": "898475",
    "end": "901370"
  },
  {
    "text": "these two nodes or why GNN is not able to dis- distinguish,",
    "start": "901370",
    "end": "904385"
  },
  {
    "text": "uh, between these nodes is because computation graph looks,",
    "start": "904385",
    "end": "907280"
  },
  {
    "text": "uh, the same in both cases, right?",
    "start": "907280",
    "end": "908825"
  },
  {
    "text": "A node has two neighbors,",
    "start": "908825",
    "end": "910100"
  },
  {
    "text": "and then each of these neighbors has two other neighbors.",
    "start": "910100",
    "end": "912410"
  },
  {
    "text": "And if- if all the neighbors look the same,",
    "start": "912410",
    "end": "914629"
  },
  {
    "text": "they don't have any discriminative colors to them,",
    "start": "914630",
    "end": "917210"
  },
  {
    "text": "then computation graphs, in all cases,",
    "start": "917210",
    "end": "919370"
  },
  {
    "text": "will look the same so,",
    "start": "919370",
    "end": "920960"
  },
  {
    "text": "uh, the embeddings will be the same.",
    "start": "920960",
    "end": "923090"
  },
  {
    "text": "So it doesn't matter whether you are part of",
    "start": "923090",
    "end": "924860"
  },
  {
    "text": "a cycle or you are a part of a infinite length,",
    "start": "924860",
    "end": "928010"
  },
  {
    "text": "uh, chain, the computation graph will always be the same.",
    "start": "928010",
    "end": "931460"
  },
  {
    "text": "So the GNN won't be able to distinguish the nodes again,",
    "start": "931460",
    "end": "934865"
  },
  {
    "text": "unless there is some discrimination between the nodes.",
    "start": "934865",
    "end": "938660"
  },
  {
    "text": "So if nodes have different colors,",
    "start": "938660",
    "end": "940490"
  },
  {
    "text": "then a GNN could,",
    "start": "940490",
    "end": "941810"
  },
  {
    "text": "uh, capture the pattern.",
    "start": "941810",
    "end": "943529"
  },
  {
    "text": "So, um, what is the solution?",
    "start": "943530",
    "end": "946370"
  },
  {
    "text": "The solution is to create a feature vector for every node that would,",
    "start": "946370",
    "end": "949850"
  },
  {
    "text": "uh, that would, for example,",
    "start": "949850",
    "end": "951285"
  },
  {
    "text": "give me the cycle count, right?",
    "start": "951285",
    "end": "952970"
  },
  {
    "text": "So basically, I would augment the node features with the cycle count information.",
    "start": "952970",
    "end": "956660"
  },
  {
    "text": "So one idea, for example would be is to create this,",
    "start": "956660",
    "end": "959555"
  },
  {
    "text": "uh, um, vector where, you know,",
    "start": "959555",
    "end": "961610"
  },
  {
    "text": "this is the number of cycles of length 0 the node participates in,",
    "start": "961610",
    "end": "965195"
  },
  {
    "text": "number of cycles of length 1,",
    "start": "965195",
    "end": "967085"
  },
  {
    "text": "length 2, length 3, right?",
    "start": "967085",
    "end": "968930"
  },
  {
    "text": "So length 1 is a self-loop,",
    "start": "968930",
    "end": "970639"
  },
  {
    "text": "length 2 would be, um,",
    "start": "970639",
    "end": "972649"
  },
  {
    "text": "if in a directed graph would be a, uh,",
    "start": "972650",
    "end": "974870"
  },
  {
    "text": "reciprocated connection, length 3 is a,",
    "start": "974870",
    "end": "978500"
  },
  {
    "text": "um, a triangle, you know,",
    "start": "978500",
    "end": "980000"
  },
  {
    "text": "length 4 is a square, right?",
    "start": "980000",
    "end": "981950"
  },
  {
    "text": "And, uh, you could now, uh,",
    "start": "981950",
    "end": "983315"
  },
  {
    "text": "append this type of feature vector to",
    "start": "983315",
    "end": "985880"
  },
  {
    "text": "whatever feature vector you already have, uh, for the nodes,",
    "start": "985880",
    "end": "988925"
  },
  {
    "text": "and this way increase the expressive power of the graph neural network,",
    "start": "988925",
    "end": "993845"
  },
  {
    "text": "especially if your intuition is that, let's say,",
    "start": "993845",
    "end": "996045"
  },
  {
    "text": "cycle information, uh, is important.",
    "start": "996045",
    "end": "998714"
  },
  {
    "text": "And of course, there are many other, uh,",
    "start": "998715",
    "end": "1001145"
  },
  {
    "text": "commonly used, uh, uh,",
    "start": "1001145",
    "end": "1002665"
  },
  {
    "text": "techniques to augment features.",
    "start": "1002665",
    "end": "1004824"
  },
  {
    "text": "People very much like to include node degree.",
    "start": "1004825",
    "end": "1007840"
  },
  {
    "text": "It's a very simple to compute feature, but again,",
    "start": "1007840",
    "end": "1010030"
  },
  {
    "text": "allows you to disti- distinguish between different nodes.",
    "start": "1010030",
    "end": "1013240"
  },
  {
    "text": "You can include clustering coefficient that essentially",
    "start": "1013240",
    "end": "1017080"
  },
  {
    "text": "counts how many cycles of length 3 a node participates in.",
    "start": "1017080",
    "end": "1021670"
  },
  {
    "text": "So this is, um, triangle counting.",
    "start": "1021670",
    "end": "1024100"
  },
  {
    "text": "Uh, you could also have other types of, uh,",
    "start": "1024100",
    "end": "1026750"
  },
  {
    "text": "features like PageRank or,",
    "start": "1026750",
    "end": "1028480"
  },
  {
    "text": "uh, node centrality metrics, right?",
    "start": "1028480",
    "end": "1030339"
  },
  {
    "text": "So essentially, any features we have introduced in Lecture 2, um,",
    "start": "1030340",
    "end": "1034914"
  },
  {
    "text": "you could include to augment, uh,",
    "start": "1034915",
    "end": "1037464"
  },
  {
    "text": "the GNN to help it learn the network structure,",
    "start": "1037465",
    "end": "1040839"
  },
  {
    "text": "uh, better and, uh, faster, right?",
    "start": "1040840",
    "end": "1043390"
  },
  {
    "text": "So in some sense, in many cases,",
    "start": "1043390",
    "end": "1045350"
  },
  {
    "text": "the goal of the- of the machine learning, uh,",
    "start": "1045350",
    "end": "1048069"
  },
  {
    "text": "scientist is to essentially understand the intuition betw- be- behind",
    "start": "1048070",
    "end": "1052179"
  },
  {
    "text": "the learning algorithm and tries to help the- the algorithm to learn the patterns,",
    "start": "1052180",
    "end": "1057349"
  },
  {
    "text": "perhaps, we as domain scientists would know are important.",
    "start": "1057349",
    "end": "1060400"
  },
  {
    "text": "And by encoding some of the graph features,",
    "start": "1060400",
    "end": "1062660"
  },
  {
    "text": "many times, you can very much, uh,",
    "start": "1062660",
    "end": "1064390"
  },
  {
    "text": "speed up and improve the performance of the model because you kind of help,",
    "start": "1064390",
    "end": "1068090"
  },
  {
    "text": "uh, you know, you point the model to the- to the places where there might be good signal.",
    "start": "1068090",
    "end": "1072789"
  },
  {
    "text": "So this was about augmenting the features, uh,",
    "start": "1072790",
    "end": "1077170"
  },
  {
    "start": "1073000",
    "end": "1355000"
  },
  {
    "text": "of the node, and we talked about adding a constant feature,",
    "start": "1077170",
    "end": "1080530"
  },
  {
    "text": "we talked about adding, um,",
    "start": "1080530",
    "end": "1082465"
  },
  {
    "text": "one-hot encoding, and we also talked about,",
    "start": "1082465",
    "end": "1086029"
  },
  {
    "text": "um, adding various kinds of graph, uh,",
    "start": "1086030",
    "end": "1089100"
  },
  {
    "text": "structure information like the cycle count or node degree,",
    "start": "1089100",
    "end": "1092799"
  },
  {
    "text": "uh, to- to augment the node feature information.",
    "start": "1092800",
    "end": "1096340"
  },
  {
    "text": "Now, I'm going to switch, uh,",
    "start": "1096340",
    "end": "1098845"
  },
  {
    "text": "gears and I'm going to talk about adding and changing the graph structure information.",
    "start": "1098845",
    "end": "1105695"
  },
  {
    "text": "So we are going to augment",
    "start": "1105695",
    "end": "1107500"
  },
  {
    "text": "the underlying graph structure again to help, uh, with to the learning.",
    "start": "1107500",
    "end": "1111780"
  },
  {
    "text": "The way we are going to do this is to add virtual nodes and virtual edges.",
    "start": "1111780",
    "end": "1117225"
  },
  {
    "text": "So the first motivation we wanna discuss is we wanna augment sparse graphs,",
    "start": "1117225",
    "end": "1122554"
  },
  {
    "text": "so we wanna add a virtual address.",
    "start": "1122555",
    "end": "1125805"
  },
  {
    "text": "A common approach, for example,",
    "start": "1125805",
    "end": "1127580"
  },
  {
    "text": "would be to connect two hop neighbors via virtual edges.",
    "start": "1127580",
    "end": "1131429"
  },
  {
    "text": "So the intuition is,",
    "start": "1131430",
    "end": "1132950"
  },
  {
    "text": "or one way to say this is that instead of using",
    "start": "1132950",
    "end": "1134965"
  },
  {
    "text": "the adjacency matrix A of a- for a GNN computation,",
    "start": "1134965",
    "end": "1138980"
  },
  {
    "text": "we are going to use A plus A squared, right?",
    "start": "1138980",
    "end": "1141850"
  },
  {
    "text": "If you, again, uh,",
    "start": "1141850",
    "end": "1143169"
  },
  {
    "text": "remember early on in the course,",
    "start": "1143170",
    "end": "1145295"
  },
  {
    "text": "maybe Lecture 2, 3,",
    "start": "1145295",
    "end": "1147085"
  },
  {
    "text": "we discussed that powering the adjacency matrix, um, counts,",
    "start": "1147085",
    "end": "1152725"
  },
  {
    "text": "um, the number of, uh, nodes that,",
    "start": "1152725",
    "end": "1155720"
  },
  {
    "text": "uh, are neighbors at level 2,",
    "start": "1155720",
    "end": "1157549"
  },
  {
    "text": "level 3, and so on.",
    "start": "1157550",
    "end": "1158865"
  },
  {
    "text": "So by basically adding- by powering the matrix and adding, uh,",
    "start": "1158865",
    "end": "1163365"
  },
  {
    "text": "A adding it to the adjacency matrix,",
    "start": "1163365",
    "end": "1165615"
  },
  {
    "text": "now basically we connect all the nodes that are two-hop neighbors.",
    "start": "1165615",
    "end": "1169665"
  },
  {
    "text": "Um, and this is very interesting, for example,",
    "start": "1169665",
    "end": "1171940"
  },
  {
    "text": "especially in bipartite graphs.",
    "start": "1171940",
    "end": "1174014"
  },
  {
    "text": "Because if in a bipartite graph where for example,",
    "start": "1174015",
    "end": "1176865"
  },
  {
    "text": "authors and the papers they author,",
    "start": "1176865",
    "end": "1179020"
  },
  {
    "text": "you create a square,",
    "start": "1179020",
    "end": "1181035"
  },
  {
    "text": "then basically you create a projection and you",
    "start": "1181035",
    "end": "1183870"
  },
  {
    "text": "created a paper co-authorship network,",
    "start": "1183870",
    "end": "1186840"
  },
  {
    "text": "or an author colle- collaboration network, right?",
    "start": "1186840",
    "end": "1189890"
  },
  {
    "text": "So this would mean that you either connect",
    "start": "1189890",
    "end": "1191485"
  },
  {
    "text": "two authors that have written at least one paper together,",
    "start": "1191485",
    "end": "1194320"
  },
  {
    "text": "or you connect two papers if they were written, uh, by the same author.",
    "start": "1194320",
    "end": "1198039"
  },
  {
    "text": "It just depends do you do, um,",
    "start": "1198040",
    "end": "1200409"
  },
  {
    "text": "you know,  A times A transpose or do you do, uh,",
    "start": "1200409",
    "end": "1204790"
  },
  {
    "text": "A transpose times A,",
    "start": "1204790",
    "end": "1206900"
  },
  {
    "text": "in a case of a bipartite graph because the adjacency matrix, uh, won't be square.",
    "start": "1206900",
    "end": "1212240"
  },
  {
    "text": "So, uh, that's- that's one idea how you can kind of include",
    "start": "1212240",
    "end": "1216115"
  },
  {
    "text": "additional information and what this will help in",
    "start": "1216115",
    "end": "1218765"
  },
  {
    "text": "the graph neural network is that rather than, you know,",
    "start": "1218765",
    "end": "1221500"
  },
  {
    "text": "if you think about message passing,",
    "start": "1221500",
    "end": "1223400"
  },
  {
    "text": "an author sending a message to the paper and then paper",
    "start": "1223400",
    "end": "1226240"
  },
  {
    "text": "sending it back to the author by connecting two authors,",
    "start": "1226240",
    "end": "1229540"
  },
  {
    "text": "they will be able to directly exchange messages,",
    "start": "1229540",
    "end": "1232000"
  },
  {
    "text": "which means that the depth- the number of layers of the graph neural network will be,",
    "start": "1232000",
    "end": "1237020"
  },
  {
    "text": "uh, able to be smaller, um,",
    "start": "1237020",
    "end": "1238975"
  },
  {
    "text": "and you'll be able to train it, uh, faster.",
    "start": "1238975",
    "end": "1241615"
  },
  {
    "text": "Of course, the um,",
    "start": "1241615",
    "end": "1244630"
  },
  {
    "text": "- the issue will then become that you have too few- that you will have",
    "start": "1244630",
    "end": "1249160"
  },
  {
    "text": "too many neighbors to aggregate from and that may add er, more complexity.",
    "start": "1249160",
    "end": "1254020"
  },
  {
    "text": "But we'll talk about how to fix that um, er later.",
    "start": "1254020",
    "end": "1258205"
  },
  {
    "text": "So if graphs are too sparse,",
    "start": "1258205",
    "end": "1261490"
  },
  {
    "text": "er, as I said,",
    "start": "1261490",
    "end": "1262840"
  },
  {
    "text": "one idea is to- to connect nodes that are-that are",
    "start": "1262840",
    "end": "1266559"
  },
  {
    "text": "two edges-virtual edges between length 2 or length 3 connected nodes.",
    "start": "1266560",
    "end": "1271675"
  },
  {
    "text": "Another idea is to add a virtual node um,",
    "start": "1271675",
    "end": "1275395"
  },
  {
    "text": "and the virtual node will then connect to, let's say,",
    "start": "1275395",
    "end": "1277870"
  },
  {
    "text": "all or some carefully chosen subset of the nodes in the graph, right?",
    "start": "1277870",
    "end": "1283059"
  },
  {
    "text": "So for example, imagine you have",
    "start": "1283060",
    "end": "1284800"
  },
  {
    "text": "a super sparse graph where have two nodes are very far apart in the graph, right?",
    "start": "1284800",
    "end": "1289240"
  },
  {
    "text": "They are, let's say, ten hops apart, right?",
    "start": "1289240",
    "end": "1291280"
  },
  {
    "text": "And now if one node needs to send a message to the other node,",
    "start": "1291280",
    "end": "1295660"
  },
  {
    "text": "you need a length, you need",
    "start": "1295660",
    "end": "1297580"
  },
  {
    "text": "a ten layer graph neural network to be able to allow for that communication.",
    "start": "1297580",
    "end": "1302620"
  },
  {
    "text": "But in some cases,",
    "start": "1302620",
    "end": "1304465"
  },
  {
    "text": "you may know that these two nodes actually need to communicate.",
    "start": "1304465",
    "end": "1307090"
  },
  {
    "text": "Even though they are farther apart in the original graph,",
    "start": "1307090",
    "end": "1309309"
  },
  {
    "text": "they need to send messages to each other, right?",
    "start": "1309310",
    "end": "1311230"
  },
  {
    "text": "One depends on the other.",
    "start": "1311230",
    "end": "1312565"
  },
  {
    "text": "So what you can do in that case is you can create a virtual node and then connect,",
    "start": "1312565",
    "end": "1317575"
  },
  {
    "text": "for example, several nodes to it.",
    "start": "1317575",
    "end": "1319510"
  },
  {
    "text": "And this way, you can connect nodes that are very far in the original graph structure er,",
    "start": "1319510",
    "end": "1325165"
  },
  {
    "text": "to be able to communicate with each other much more efficiently, right?",
    "start": "1325165",
    "end": "1329020"
  },
  {
    "text": "So basically, after adding a virtual node,",
    "start": "1329020",
    "end": "1331240"
  },
  {
    "text": "er, all the nodes will have a smaller distance to each other.",
    "start": "1331240",
    "end": "1334585"
  },
  {
    "text": "So you'll- the message passing will be more er,",
    "start": "1334585",
    "end": "1338799"
  },
  {
    "text": "- more efficient, it will happen faster.",
    "start": "1338800",
    "end": "1341005"
  },
  {
    "text": "The depth of the graph neural network won't have to be er, that large.",
    "start": "1341005",
    "end": "1345820"
  },
  {
    "text": "Er, so that's another technique that sometimes um, is er,",
    "start": "1345820",
    "end": "1350019"
  },
  {
    "text": "-is-is a good idea to have in your toolbox um,",
    "start": "1350020",
    "end": "1352330"
  },
  {
    "text": "if the basic approaches don't work.",
    "start": "1352330",
    "end": "1355659"
  },
  {
    "start": "1355000",
    "end": "1429000"
  },
  {
    "text": "And then er, the last thing I want to talk about- about graph structure information is",
    "start": "1355660",
    "end": "1361885"
  },
  {
    "text": "not when you have too few edges in the graph and you want",
    "start": "1361885",
    "end": "1364690"
  },
  {
    "text": "to kind of make the message passing er, more efficient.",
    "start": "1364690",
    "end": "1367690"
  },
  {
    "text": "Er, the question becomes,",
    "start": "1367690",
    "end": "1369460"
  },
  {
    "text": "what if you have too many edges?",
    "start": "1369460",
    "end": "1370794"
  },
  {
    "text": "What if the graph is too large, right?",
    "start": "1370795",
    "end": "1373270"
  },
  {
    "text": "Again, think of my er,",
    "start": "1373270",
    "end": "1374515"
  },
  {
    "text": "Kim Kardashian example, right?",
    "start": "1374515",
    "end": "1376135"
  },
  {
    "text": "Or Lady Gaga used to be er,",
    "start": "1376135",
    "end": "1378190"
  },
  {
    "text": "the highest degree node in the Twitter network a few years ago,",
    "start": "1378190",
    "end": "1381669"
  },
  {
    "text": "but I think she is not number one anymore.",
    "start": "1381670",
    "end": "1384095"
  },
  {
    "text": "But the point is, right, you have these high degree nodes in networks and",
    "start": "1384095",
    "end": "1388409"
  },
  {
    "text": "aggregating messages from millions- over tens of millions or hundreds of millions of er,",
    "start": "1388410",
    "end": "1393105"
  },
  {
    "text": "people that are connected to it can become er, quite er, expensive.",
    "start": "1393105",
    "end": "1397710"
  },
  {
    "text": "So um- so far, right?",
    "start": "1397710",
    "end": "1399570"
  },
  {
    "text": "We said, let's use all the nodes, all the neighbors er,",
    "start": "1399570",
    "end": "1402375"
  },
  {
    "text": "for message passing when defining",
    "start": "1402375",
    "end": "1404610"
  },
  {
    "text": "the graph neural network and the idea that we are going to explore here and",
    "start": "1404610",
    "end": "1408415"
  },
  {
    "text": "introduce is, what if I sample node's neighborhood er, for message passing?",
    "start": "1408415",
    "end": "1413770"
  },
  {
    "text": "And of course I could do random sampling,",
    "start": "1413770",
    "end": "1415870"
  },
  {
    "text": "but it turns out there are far better heuristics er,",
    "start": "1415870",
    "end": "1419530"
  },
  {
    "text": "that allow you to really carefully select",
    "start": "1419530",
    "end": "1422305"
  },
  {
    "text": "what neighbors to collect information from and what neighbors to ignore.",
    "start": "1422305",
    "end": "1426730"
  },
  {
    "text": "So um, here is the idea.",
    "start": "1426730",
    "end": "1429085"
  },
  {
    "start": "1429000",
    "end": "1670000"
  },
  {
    "text": "So the idea is that, for example,",
    "start": "1429085",
    "end": "1431590"
  },
  {
    "text": "what if we randomly choose two neighbors to pass messages from in a given layer?",
    "start": "1431590",
    "end": "1436975"
  },
  {
    "text": "So for example, for our neighbors of node A,",
    "start": "1436975",
    "end": "1439570"
  },
  {
    "text": "we-we would decide that out of the three neighbors,",
    "start": "1439570",
    "end": "1442255"
  },
  {
    "text": "we are only going to select two of them and ignore the third one.",
    "start": "1442255",
    "end": "1445810"
  },
  {
    "text": "So this would, for example,",
    "start": "1445810",
    "end": "1447220"
  },
  {
    "text": "in this case mean that A only collects information from B and D but ignores",
    "start": "1447220",
    "end": "1451390"
  },
  {
    "text": "the C. So now our message-passing computation graph would look like this.",
    "start": "1451390",
    "end": "1456160"
  },
  {
    "text": "Why is this good? This is good because now computation graph is much smaller.",
    "start": "1456160",
    "end": "1460675"
  },
  {
    "text": "Um, of course, why is it bad?",
    "start": "1460675",
    "end": "1462580"
  },
  {
    "text": "It's because perhaps node C has",
    "start": "1462580",
    "end": "1464860"
  },
  {
    "text": "very important information that would allow",
    "start": "1464860",
    "end": "1467020"
  },
  {
    "text": "us to make a better prediction at node A,",
    "start": "1467020",
    "end": "1469675"
  },
  {
    "text": "but because we ignored it,",
    "start": "1469675",
    "end": "1471565"
  },
  {
    "text": "this will be- it will be harder for the neural network er,",
    "start": "1471565",
    "end": "1475615"
  },
  {
    "text": "to learn that better, right?",
    "start": "1475615",
    "end": "1477550"
  },
  {
    "text": "So this is kind of the trade-off here is,",
    "start": "1477550",
    "end": "1479350"
  },
  {
    "text": "yes, you gain computational efficiency,",
    "start": "1479350",
    "end": "1481809"
  },
  {
    "text": "but in the worst case,",
    "start": "1481810",
    "end": "1483310"
  },
  {
    "text": "you kind of lose some of the expressive power er,",
    "start": "1483310",
    "end": "1486790"
  },
  {
    "text": "because you-you-you dropped out er,",
    "start": "1486790",
    "end": "1489550"
  },
  {
    "text": "some edges, you dropped out some information that might be important.",
    "start": "1489550",
    "end": "1492985"
  },
  {
    "text": "And of course, in practice,",
    "start": "1492985",
    "end": "1494500"
  },
  {
    "text": "um, if you have a super high degree node,",
    "start": "1494500",
    "end": "1497890"
  },
  {
    "text": "you can really sub-sample how many neighbors you- you er,",
    "start": "1497890",
    "end": "1502765"
  },
  {
    "text": "aggregate information from because you really want to kind of aggregate from",
    "start": "1502765",
    "end": "1506770"
  },
  {
    "text": "important neighbors and from all the kind of noisy unimportant ones,",
    "start": "1506770",
    "end": "1510865"
  },
  {
    "text": "er, you can er, - you can ignore them.",
    "start": "1510865",
    "end": "1512920"
  },
  {
    "text": "That's kind of the intuition.",
    "start": "1512920",
    "end": "1514960"
  },
  {
    "text": "So- and of course we can, um, er,",
    "start": "1514960",
    "end": "1517794"
  },
  {
    "text": "do this sampling differently er, every time.",
    "start": "1517795",
    "end": "1521650"
  },
  {
    "text": "We could even make it, er,",
    "start": "1521650",
    "end": "1522775"
  },
  {
    "text": "such that sampling changes between er,",
    "start": "1522775",
    "end": "1525205"
  },
  {
    "text": "layers and this way,",
    "start": "1525205",
    "end": "1526450"
  },
  {
    "text": "the network- the aggregation functions actually become er,",
    "start": "1526450",
    "end": "1530110"
  },
  {
    "text": "robust to how many neighbors do you sam- do you collect the information from, right?",
    "start": "1530110",
    "end": "1535450"
  },
  {
    "text": "So the idea would be, for example,",
    "start": "1535450",
    "end": "1536769"
  },
  {
    "text": "in the-in the next layer,",
    "start": "1536770",
    "end": "1538390"
  },
  {
    "text": "or in the next er, minute- er,",
    "start": "1538390",
    "end": "1540430"
  },
  {
    "text": "in the next er, epoch of training,",
    "start": "1540430",
    "end": "1542995"
  },
  {
    "text": "we can sample different nodes er,",
    "start": "1542995",
    "end": "1544855"
  },
  {
    "text": "for the same node A , right?",
    "start": "1544855",
    "end": "1546520"
  },
  {
    "text": "So what this would mean is that, for example,",
    "start": "1546520",
    "end": "1548020"
  },
  {
    "text": "now we re-sample and we decide to collect from C and D but ignore er,",
    "start": "1548020",
    "end": "1552370"
  },
  {
    "text": "B and this is now how the computation graph would look",
    "start": "1552370",
    "end": "1554980"
  },
  {
    "text": "like and this is also good because it adds a",
    "start": "1554980",
    "end": "1557590"
  },
  {
    "text": "robustness to our neural network training approach, right?",
    "start": "1557590",
    "end": "1561490"
  },
  {
    "text": "It er,-it now means that er,",
    "start": "1561490",
    "end": "1563170"
  },
  {
    "text": "A will be able to collect information-will learn how to robustly collect",
    "start": "1563170",
    "end": "1567010"
  },
  {
    "text": "information from some subsets of its- of its neighbors and won't er,",
    "start": "1567010",
    "end": "1571600"
  },
  {
    "text": "suffer to much if, for example,",
    "start": "1571600",
    "end": "1573370"
  },
  {
    "text": "an edge is er, missing in the network.",
    "start": "1573370",
    "end": "1575590"
  },
  {
    "text": "So that's also, um,",
    "start": "1575590",
    "end": "1577225"
  },
  {
    "text": "one reason why this er,",
    "start": "1577225",
    "end": "1578590"
  },
  {
    "text": "neighborhood sampling, er, is a good approach.",
    "start": "1578590",
    "end": "1581544"
  },
  {
    "text": "Um, and why is this interesting?",
    "start": "1581545",
    "end": "1584200"
  },
  {
    "text": "It's because in expectation,",
    "start": "1584200",
    "end": "1585580"
  },
  {
    "text": "right after a lot of different er,",
    "start": "1585580",
    "end": "1587260"
  },
  {
    "text": "random samplings, we will get embeddings",
    "start": "1587260",
    "end": "1589885"
  },
  {
    "text": "similar to the case when all nodes are being used.",
    "start": "1589885",
    "end": "1593245"
  },
  {
    "text": "Er, but the benefit is that this greatly reduces computational cost and,",
    "start": "1593245",
    "end": "1598135"
  },
  {
    "text": "you know, in the small graphs,",
    "start": "1598135",
    "end": "1599260"
  },
  {
    "text": "this might not be er,",
    "start": "1599260",
    "end": "1600490"
  },
  {
    "text": "so obvious but if you-if, you know,",
    "start": "1600490",
    "end": "1603070"
  },
  {
    "text": "node A has 30 million neighbors,",
    "start": "1603070",
    "end": "1606205"
  },
  {
    "text": "then we cannot aggregate from 30 million.",
    "start": "1606205",
    "end": "1608919"
  },
  {
    "text": "The question is, can we decide what are the top 100,",
    "start": "1608920",
    "end": "1611590"
  },
  {
    "text": "maybe top 1,000 most important nodes?",
    "start": "1611590",
    "end": "1614575"
  },
  {
    "text": "Most important, let's say friends,",
    "start": "1614575",
    "end": "1616419"
  },
  {
    "text": "if this is a social network or the true friends er, of this person,",
    "start": "1616420",
    "end": "1620605"
  },
  {
    "text": "and we want to aggregate information er,",
    "start": "1620605",
    "end": "1622674"
  },
  {
    "text": "from them rather than from all the kind of, er,",
    "start": "1622675",
    "end": "1625210"
  },
  {
    "text": "random followers, er, all over the world, right?",
    "start": "1625210",
    "end": "1627940"
  },
  {
    "text": "Um, and by doing this sub-sampling,",
    "start": "1627940",
    "end": "1630129"
  },
  {
    "text": "we can make the computation graphs much, much,",
    "start": "1630130",
    "end": "1632590"
  },
  {
    "text": "much smaller, which allows us to scale GNNs to massive graphs.",
    "start": "1632590",
    "end": "1637299"
  },
  {
    "text": "And this is very important in industrial applications",
    "start": "1637300",
    "end": "1639790"
  },
  {
    "text": "like recommender systems and social networks,",
    "start": "1639790",
    "end": "1642550"
  },
  {
    "text": "where you have, you know,",
    "start": "1642550",
    "end": "1643720"
  },
  {
    "text": "networks of billions and tens of billions of er,",
    "start": "1643720",
    "end": "1646284"
  },
  {
    "text": "nodes and edges and you need these  kind of,",
    "start": "1646285",
    "end": "1648925"
  },
  {
    "text": "um, techniques to be able to scale.",
    "start": "1648925",
    "end": "1651955"
  },
  {
    "text": "Er, and in practice, this is a very good approach to scale",
    "start": "1651955",
    "end": "1654610"
  },
  {
    "text": "up- to scale up graph neural networks.",
    "start": "1654610",
    "end": "1657370"
  },
  {
    "text": "So er, this is what I wanted to say about neighborhood sampling er,",
    "start": "1657370",
    "end": "1662020"
  },
  {
    "text": "and give you this er, example.",
    "start": "1662020",
    "end": "1664580"
  }
]