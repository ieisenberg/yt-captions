[
  {
    "start": "0",
    "end": "5450"
  },
  {
    "text": "Welcome to CS 330. Today we'll be talking about\noffline reinforcement learning and offline multitask\nreinforcement learning.",
    "start": "5450",
    "end": "12780"
  },
  {
    "text": "And these are some of the\nhottest parts of reinforcement learning these days. There's a lot of progress\nhappening in these fields,",
    "start": "12780",
    "end": "18147"
  },
  {
    "text": "so it's an exciting topic. Before we get started, a\nreminder, the optional homework",
    "start": "18147",
    "end": "25850"
  },
  {
    "text": "4 is due next Monday. So hopefully, you are\nalready working on that.",
    "start": "25850",
    "end": "33400"
  },
  {
    "text": "Cool. So let's zoom out a little bit. We talked a little bit about\nmultitask reinforcement",
    "start": "33400",
    "end": "40600"
  },
  {
    "text": "learning, just reinforcement\nlearning, meta reinforcement learning. But let's just zoom\nout and see what",
    "start": "40600",
    "end": "46480"
  },
  {
    "text": "is the recipe that has worked\nso far for machine learning in general for modern\nmachine learning methods.",
    "start": "46480",
    "end": "53870"
  },
  {
    "text": "So if I were to zoom out far\nenough, we need two parts. We need a lot of data.",
    "start": "53870",
    "end": "60170"
  },
  {
    "text": "So for instance, the\ndataset of a size of an ImageNet, where we have\nmillion images or large corpora",
    "start": "60170",
    "end": "66009"
  },
  {
    "text": "of text like we had in the\nnatural language processing example or natural\nlanguage processing case,",
    "start": "66010",
    "end": "73270"
  },
  {
    "text": "all of Wikipedia, for instance. And then we need\nexpressive capable models",
    "start": "73270",
    "end": "78670"
  },
  {
    "text": "that are able to digest all\nof that data, all right? So for computer\nvision traditionally,",
    "start": "78670",
    "end": "83800"
  },
  {
    "text": "these have been\nconvolutional neural nets. For natural language\nprocessing, transformers, now",
    "start": "83800",
    "end": "88840"
  },
  {
    "text": "being used also in vision\nand many other applications.",
    "start": "88840",
    "end": "94240"
  },
  {
    "text": "Cool. So let's see if the\nreinforcement learning recipe fits into this.",
    "start": "94240",
    "end": "100620"
  },
  {
    "text": "So the reinforcement learning\nrecipe looks like this. We need a lot of data and we\nneed expressive capable models.",
    "start": "100620",
    "end": "107730"
  },
  {
    "text": "So in terms of expressive\ncapable models, it's relatively easy. We can just we use\nsimilar libraries.",
    "start": "107730",
    "end": "114620"
  },
  {
    "text": "So we can use the\nbiggest advancements that we've seen in\nvision or in language and just apply them to\nreinforcement learning.",
    "start": "114620",
    "end": "120510"
  },
  {
    "text": "So we can use\nconvolutional neural nets, we can use transformers. That's fine. But then when it\ncomes to data, so far",
    "start": "120510",
    "end": "127439"
  },
  {
    "text": "the way that we've been\ndiscussing reinforcement learning, we have\nthis loop where we are constantly generating\ndata for any given experiment.",
    "start": "127440",
    "end": "135530"
  },
  {
    "text": "This is an active learning loop\nwhere we go into the world, then we collect some\ndata with our agent,",
    "start": "135530",
    "end": "142280"
  },
  {
    "text": "then we fit something\nto that data. We try to learn\nsomething from it and then we have this\nnew updated network,",
    "start": "142280",
    "end": "148610"
  },
  {
    "text": "we go into the world\nand then collect more data with the agent, and\nwe do it over and over again.",
    "start": "148610",
    "end": "155625"
  },
  {
    "text": "So this is a little\nbit different to the supervised\nlearning recipe where you can just access\na lot of data that's",
    "start": "155625",
    "end": "161560"
  },
  {
    "text": "already available out there. Here we do this actively\nover and over again.",
    "start": "161560",
    "end": "167820"
  },
  {
    "text": "And this is actually\na limiting factor when it comes to\nreinforced learning and the amounts of\ndata we can digest.",
    "start": "167820",
    "end": "173820"
  },
  {
    "text": "So it's hard to achieve the\nsame level of generalization that we've seen in\nsupervised learning with this per experiment\nactive learning loop.",
    "start": "173820",
    "end": "182970"
  },
  {
    "text": "So what offline reinforcement\nlearning tries to do is try to get rid of this\nactive data collection loop",
    "start": "182970",
    "end": "189860"
  },
  {
    "text": "and instead just access big\ndatasets that were collected offline, assuming that we\nare not able to collect",
    "start": "189860",
    "end": "195590"
  },
  {
    "text": "any additional data. And this is what we're\ngoing to talk about today. And hopefully, if\nwe can do that,",
    "start": "195590",
    "end": "202290"
  },
  {
    "text": "then we can able to\naccess big datasets, digest a lot of data that\nwas collected offline and then see similar\nexciting generalization",
    "start": "202290",
    "end": "208850"
  },
  {
    "text": "results that we've seen\nin supervised learning. All right, so the plan for\ntoday is to first discuss",
    "start": "208850",
    "end": "215530"
  },
  {
    "text": "the offline RL\nproblem formulation then talk about some\nsolutions to this problem.",
    "start": "215530",
    "end": "224350"
  },
  {
    "text": "Then we'll talk about\noffline multitask reinforcement learning and how\nit influences data sharing.",
    "start": "224350",
    "end": "230050"
  },
  {
    "text": "And then we'll end on offline\ngoal condition reinforcement learning, which is a really\nexciting area of research",
    "start": "230050",
    "end": "235180"
  },
  {
    "text": "right now. All right, cool. Let's start with the offline\nRL problem formulation.",
    "start": "235180",
    "end": "242740"
  },
  {
    "text": "So if you remember, we had\nthis anatomy of a reinforcement learning algorithm, and we\nwent through this a few times",
    "start": "242740",
    "end": "249159"
  },
  {
    "text": "now, where we first\ngenerate samples then we try to fit a model\nto estimate return. And we have a few\ndifferent options here,",
    "start": "249160",
    "end": "255550"
  },
  {
    "text": "either policy\ngradient, Q learning, or we can use model\nbased methods as well. And then we have\none more box that",
    "start": "255550",
    "end": "261010"
  },
  {
    "text": "tries to improve the policy\ngiven that model that estimates the return.",
    "start": "261010",
    "end": "266180"
  },
  {
    "text": "So what we'll try to\ndo in the offline case is we'll try to move\naway from this pattern",
    "start": "266180",
    "end": "274270"
  },
  {
    "text": "where we keep generating\ndata where we keep generating samples by running\nthe policy and instead",
    "start": "274270",
    "end": "279910"
  },
  {
    "text": "just access a dataset that\nis already available to us. So the one thing\nthat we are changing",
    "start": "279910",
    "end": "285160"
  },
  {
    "text": "is that we are not allowed to\nuse this orange box anymore. ",
    "start": "285160",
    "end": "291770"
  },
  {
    "text": "All right, so in\norder to do this, we can use some\nof the algorithms",
    "start": "291770",
    "end": "296955"
  },
  {
    "text": "that we already know. And we've learned about\ntwo types of reinforcement learning algorithms, on-policy\nand off-policy algorithms.",
    "start": "296955",
    "end": "303060"
  },
  {
    "text": "And this is just a\nquick reminder slide of what the difference\nbetween these two is. So for on-policy\nalgorithms, data",
    "start": "303060",
    "end": "309950"
  },
  {
    "text": "has to come from\nthe current policy. So we are constantly collecting\ndata with the freshest policy that we have.",
    "start": "309950",
    "end": "316190"
  },
  {
    "text": "It's compatible with\nall our algorithms, and we can't re-use data\nfrom previous policies.",
    "start": "316190",
    "end": "322160"
  },
  {
    "text": "We have to be\ncollecting the data and using the data from the\npolicy that is the most fresh.",
    "start": "322160",
    "end": "328759"
  },
  {
    "text": "And for the off-policy case,\ndata can come from any policy. We don't really care what\npolicy it was collected with.",
    "start": "328760",
    "end": "335180"
  },
  {
    "text": "It works with\nspecific RL algorithms such as value based methods. And it's much more\nsample efficient",
    "start": "335180",
    "end": "341510"
  },
  {
    "text": "because we can reuse old data. We can do multiple gradient\nsteps on the same data. ",
    "start": "341510",
    "end": "349199"
  },
  {
    "text": "All right, so\ngiven these two, do you have any idea\nwhich one you would",
    "start": "349200",
    "end": "354840"
  },
  {
    "text": "use for the offline case given\nthese two characteristics?",
    "start": "354840",
    "end": "361440"
  },
  {
    "start": "361440",
    "end": "367250"
  },
  {
    "text": "Yes. [INAUDIBLE] ",
    "start": "367250",
    "end": "373520"
  },
  {
    "text": "Yeah, so it's probably\nthe off-policy case. Instead of data\ncoming from a policy, we'll just use any data.",
    "start": "373520",
    "end": "378950"
  },
  {
    "text": "That's right. All right, so we'll be\nusing off-policy RL,",
    "start": "378950",
    "end": "385040"
  },
  {
    "text": "but there is a small difference\nbetween off-policy RL and offline RL. So here's a little graphic\nexplaining the difference.",
    "start": "385040",
    "end": "393270"
  },
  {
    "text": "So in the online\nreinforcement learning, just to reiterate this,\nwe have a world",
    "start": "393270",
    "end": "398630"
  },
  {
    "text": "that we can collect data on. We do rollouts on this world. So we get the next state and\nthe reward from the world,",
    "start": "398630",
    "end": "404660"
  },
  {
    "text": "then we can execute our policy\nthat gives us the actions and we do multiple\nrollouts in this loop.",
    "start": "404660",
    "end": "410630"
  },
  {
    "text": "Then we take that rollout\ndata that we just generated and we update the\npolicy using the data,",
    "start": "410630",
    "end": "415970"
  },
  {
    "text": "and then we push the new\npolicy into the world to collect more action,\nto collect more rollouts.",
    "start": "415970",
    "end": "423139"
  },
  {
    "text": "In the off-policy case, there is\none more addition, the addition of this replay buffer, where we\ncan collect a bunch of rollouts",
    "start": "423140",
    "end": "430160"
  },
  {
    "text": "and then the rollout\ndata goes into the buffer and that buffer now\nconsists of all the rollouts that we've ever had, right?",
    "start": "430160",
    "end": "436370"
  },
  {
    "text": "So it doesn't just need to be\nthe most fresh rollout data. This is any rollout\nthat we've collected",
    "start": "436370",
    "end": "442640"
  },
  {
    "text": "in the history of\nthis experiment. Then we take samples from that\nbuffer to update our policy",
    "start": "442640",
    "end": "449540"
  },
  {
    "text": "and then this\nupdated policy gets to execute and do multiple\nrollouts in the real world.",
    "start": "449540",
    "end": "457017"
  },
  {
    "text": "And now this is the offline\nreinforcement learning case. So in the offline\nreinforcement learning case,",
    "start": "457017",
    "end": "462060"
  },
  {
    "text": "we assume that data collected\nis only once with any policy, and we will call this policy\npi beta, policy of the buffer.",
    "start": "462060",
    "end": "469400"
  },
  {
    "text": "We don't really know what\nkind of policy this is. This was collected. We didn't have access to it.",
    "start": "469400",
    "end": "474840"
  },
  {
    "text": "It was collected\nbefore we even started working on this experiment. It's just we have some\ndata in the buffer already.",
    "start": "474840",
    "end": "481669"
  },
  {
    "text": "So we have access\nto the buffer, then we can sample from the\nbuffer and learn our policy. And we can use as many gradient\nsteps as we would like.",
    "start": "481670",
    "end": "490110"
  },
  {
    "text": "But we can't really collect\nany additional data. We don't have\naccess to the world to collect additional data\nthat can be used for learning.",
    "start": "490110",
    "end": "498710"
  },
  {
    "text": "So instead, we'll be just\nsampling from the buffer, learning our policy. And then once we are done, we'll\njust deploy it in the world",
    "start": "498710",
    "end": "505112"
  },
  {
    "text": "and see how well it works.  All right, so a few notations.",
    "start": "505112",
    "end": "512229"
  },
  {
    "text": "Here will be the\ncapital D symbol will be referring to\nthe source tuples that we have in\nour replay buffer.",
    "start": "512230",
    "end": "519020"
  },
  {
    "text": "So this is the state action,\nnext state and the reward. ",
    "start": "519020",
    "end": "524279"
  },
  {
    "text": "Then our states will come from\nthe marginal state distribution of our policy pi data.",
    "start": "524280",
    "end": "530610"
  },
  {
    "text": "And pi data is the policy\nthat we have in the buffer. So we don't really\nknow what it is.",
    "start": "530610",
    "end": "536130"
  },
  {
    "text": "It's an unknown policy. We don't have any form of it. We just have access to\nsamples from this policy,",
    "start": "536130",
    "end": "541620"
  },
  {
    "text": "and this is what we\nhave in the buffer. So the actions that\nwe see in the buffer are the actions\nthat were executed",
    "start": "541620",
    "end": "547500"
  },
  {
    "text": "by this unknown policy. And the states that\nwe have in the buffer are the marginal that\nthe policy presented.",
    "start": "547500",
    "end": "555153"
  },
  {
    "text": "And then there is\na dynamics model that we don't have\naccess to as well. ",
    "start": "555153",
    "end": "561210"
  },
  {
    "text": "In terms of the\nobjective, it's similar to the standard reinforcement\nlearning objective. We were trying to maximize\nthe future sum of returns",
    "start": "561210",
    "end": "568590"
  },
  {
    "text": "and to find the arc\nmax of the policy. So in this case, we are\nsumming over all the time steps",
    "start": "568590",
    "end": "575250"
  },
  {
    "text": "and then we are taking the\nexpectation under the state distribution that we see in\nthe buffer actions that we pick",
    "start": "575250",
    "end": "582090"
  },
  {
    "text": "according to our\npolicy that we are learning right now by theta. And we are trying to\nmaximize the sum of return,",
    "start": "582090",
    "end": "588180"
  },
  {
    "text": "the discounted sum of returns. ",
    "start": "588180",
    "end": "593740"
  },
  {
    "text": "All right, so given\nall this, can you think of any\npotential applications of offline reinforcement\nlearning given the differences",
    "start": "593740",
    "end": "600040"
  },
  {
    "text": "to online off-policy and so on? Yes. Maybe that it's expensive or\ndangerous to let the agent find it, so for example in\na medical situation it's not, like [INAUDIBLE] it's\njust too expensive or too hard, costly, or it's [INAUDIBLE]\nin an dangerous environment, or something like that.",
    "start": "600040",
    "end": "605387"
  },
  {
    "start": "605387",
    "end": "623290"
  },
  {
    "text": "Yeah, right. Just to repeat the\nanswer, any time where it's costly or dangerous\nto collect additional data",
    "start": "623290",
    "end": "630917"
  },
  {
    "text": "in the environment, for\nexample, in medical applications or in dangerous environments. Any other ideas of applications\nof offline reinforcement",
    "start": "630917",
    "end": "638780"
  },
  {
    "text": "learning? ",
    "start": "638780",
    "end": "650480"
  },
  {
    "text": "All right, yeah so\nbasically that covers I think most of the cases. So any time when it's very\ndifficult to deploy the agent,",
    "start": "650480",
    "end": "658430"
  },
  {
    "text": "this was something\nwhere we can just access the data that\nwas already collected or if it's very\ndifficult to collect that data in the real world,\nlike for instance, as you said,",
    "start": "658430",
    "end": "665930"
  },
  {
    "text": "in the medical applications. This could be extremely\nuseful, and we can find ways to learn from\nthat already collected data",
    "start": "665930",
    "end": "673040"
  },
  {
    "text": "to act better in the\nfuture, even better than the policy that\ncollected the data.",
    "start": "673040",
    "end": "678930"
  },
  {
    "text": "All right, so speaking about\noffline and reinforcement learning and what it can be used\nfor, what can it actually do?",
    "start": "678930",
    "end": "685990"
  },
  {
    "text": "How can it work\nthat you can just access some data that\nwas collected previously by some policies and you can\nsomehow extract something",
    "start": "685990",
    "end": "692310"
  },
  {
    "text": "from it? So it can do three things. So first, it can just find\nbest behaviors in a dataset.",
    "start": "692310",
    "end": "698940"
  },
  {
    "text": "So if you have a bunch\nof different behaviors that was collected by\ndifferent policies, it can just pick which ones were\nthe best and just use those.",
    "start": "698940",
    "end": "706282"
  },
  {
    "text": "And that's relatively simple.  Secondly, it can also\ngeneralize these best behaviors",
    "start": "706282",
    "end": "712959"
  },
  {
    "text": "to similar situations. So here we are training\nthis whole thing with deep neural networks,\nwhich can generalize quite well.",
    "start": "712960",
    "end": "719878"
  },
  {
    "text": "So if we've seen\nthat something works in a particular situation, now\nbecause of the generalization properties of neural\nnetworks, we'll",
    "start": "719878",
    "end": "726040"
  },
  {
    "text": "be able to apply similar\nbehaviors in similar situations through generalization. ",
    "start": "726040",
    "end": "732860"
  },
  {
    "text": "And then thirdly, it\ncan also stitch together parts of good behaviors\ninto even a better behavior.",
    "start": "732860",
    "end": "738170"
  },
  {
    "text": "And this is maybe a\nlittle bit less intuitive. So here's a little example. If we had two trajectories\nin the buffer, one",
    "start": "738170",
    "end": "745519"
  },
  {
    "text": "that goes from A to B\nand another one that goes through B to\nC, and we wanted to find the best way that goes\nfrom the shortest path that",
    "start": "745520",
    "end": "752690"
  },
  {
    "text": "goes from A to C, it\nshould be able to stitch these trajectories\nsuch that it uses the first part of\ntrajectory from A to B",
    "start": "752690",
    "end": "759199"
  },
  {
    "text": "and the second part of the\ntrajectory from B to C, stitch them together and\nget a much better trajectory than either one of the ones\nthat we've collected before.",
    "start": "759200",
    "end": "767253"
  },
  {
    "text": "And we can do the stitching\nbecause of dynamic programming properties of offline\nreinforcement learning.",
    "start": "767253",
    "end": "773870"
  },
  {
    "text": "Or another case where\nwe have two points we need to get from\nthis point to that point and we have lots of\nnoisy trajectories",
    "start": "773870",
    "end": "780353"
  },
  {
    "text": "that we collected that\ngo between these points or sometimes don't even touch\nany of these points, right? Just lots of noisy trajectories\nthat were somewhat collected.",
    "start": "780353",
    "end": "789128"
  },
  {
    "text": "Offline reinforcement\nlearning should be able to stitch\nthese behaviors to find the policy that finds the\nshortest path between these two",
    "start": "789128",
    "end": "795980"
  },
  {
    "text": "points. All right, so we\ncan learn a policy that is better than the policy\nthat the data was collected",
    "start": "795980",
    "end": "802560"
  },
  {
    "text": "with.  All right, so we said that we\ncould use off-policy algorithms",
    "start": "802560",
    "end": "811020"
  },
  {
    "text": "to solve this\nproblem that are more applicable than\non-policy algorithms because they can use any data.",
    "start": "811020",
    "end": "817690"
  },
  {
    "text": "So let's try it. We learned about this fitted\nQ-iteration algorithm.",
    "start": "817690",
    "end": "823199"
  },
  {
    "text": "It more or less went like this. We have a few algorithm\nhyperparameters.",
    "start": "823200",
    "end": "828480"
  },
  {
    "text": "We have a dataset size,\na collection policy, and then we are trying to\nlearn a Q-function that",
    "start": "828480",
    "end": "834390"
  },
  {
    "text": "takes as input state\nand action and outputs a Q value, a scalar\nthat tells us what's the predicted sum of rewards.",
    "start": "834390",
    "end": "841589"
  },
  {
    "text": "And then we are iterating,\nwe are doing gradient steps on this objective right here.",
    "start": "841590",
    "end": "847930"
  },
  {
    "text": "We are trying to fit the\nfunction to the target, and the target is the reward\nplus the max over action",
    "start": "847930",
    "end": "855089"
  },
  {
    "text": "of the next Q. So this\nis the main equation that we use for this, for\nthe target of our Q function.",
    "start": "855090",
    "end": "863500"
  },
  {
    "text": "So we do this over k\niterations, and then we'll continue collecting the\ndataset using some policy",
    "start": "863500",
    "end": "869070"
  },
  {
    "text": "and do this iteratively. After we do this, we\ncan get the policy",
    "start": "869070",
    "end": "874870"
  },
  {
    "text": "by just using the arg max\nover actions from the Q.",
    "start": "874870",
    "end": "880120"
  },
  {
    "text": "All right, so a few\nnotes that we noted while we were introducing it. We can reuse data from\nprevious policies, right?",
    "start": "880120",
    "end": "885790"
  },
  {
    "text": "So we can use any policy. Here we still do\nthis iteratively, but you can imagine that we\njust want to do this loop.",
    "start": "885790",
    "end": "892180"
  },
  {
    "text": "We'll just access that\ndataset, and we will not collect additional data.",
    "start": "892180",
    "end": "897950"
  },
  {
    "text": "And then if it was an\noff-policy algorithm, so we can use replay buffers\nto sample the data from.",
    "start": "897950",
    "end": "905076"
  },
  {
    "text": "And it was not a gradient\ndescent algorithm, it was a dynamic\nprogramming algorithm. So one question I\nhave to you is do",
    "start": "905077",
    "end": "912940"
  },
  {
    "text": "you remember what this\nequation was called? This equation that we\ncompute Q targets with. ",
    "start": "912940",
    "end": "920870"
  },
  {
    "text": "Bellman equation, that's right. Bellman optimality\nequation, and this is a really important equation. So we'll get back\nto it in a second.",
    "start": "920870",
    "end": "928610"
  },
  {
    "text": "All right, so this is the\nfitted iteration algorithm. And then we talked about how we\ncan apply this with the Bellman",
    "start": "928610",
    "end": "935209"
  },
  {
    "text": "equation to a\nrobotic application to grasping and achieve\npretty good results.",
    "start": "935210",
    "end": "941699"
  },
  {
    "text": "So the way it worked,\nit was described in this paper called QT Opt\nwhere we would store data from all the past experiments.",
    "start": "941700",
    "end": "948590"
  },
  {
    "text": "Then we would have\na buffer that has all the off-policy experiences.",
    "start": "948590",
    "end": "953815"
  },
  {
    "text": "In addition to\nthis, we'd collect a little bit of\non-policy data and have a separate buffer for that.",
    "start": "953815",
    "end": "959040"
  },
  {
    "text": "And then we'll try to\ntrain this Q function. We'll have a set of jobs\nthat are running parallel",
    "start": "959040",
    "end": "964345"
  },
  {
    "text": "that compute the\nBellman updates, so they run the\nBellman equation. Inside of there they're\nalso doing cross entropy",
    "start": "964345",
    "end": "970880"
  },
  {
    "text": "minimization to find this max\nover continuous action space. And then as the result\nof the Bellman update,",
    "start": "970880",
    "end": "977960"
  },
  {
    "text": "we get the label\npairs that are then being put in the training\njob to actually apply the gradients to the network.",
    "start": "977960",
    "end": "985009"
  },
  {
    "text": "And this actually\nworked quite well. So this we use seven robots that\ncollected over 580,000 grasps.",
    "start": "985010",
    "end": "994370"
  },
  {
    "text": "And on unseen test\nobjects we were able to see some really cool\ngeneralization behaviors where",
    "start": "994370",
    "end": "999440"
  },
  {
    "text": "the robot was able to\nsimulate the object, do some re-grasping behavior and\nactually grasp a lot of them.",
    "start": "999440",
    "end": "1007930"
  },
  {
    "text": "The result that they reported\nback then a few weeks ago was 96%.",
    "start": "1007930",
    "end": "1012980"
  },
  {
    "text": "So we were able to grasp\npreviously unseen objects with 96% success rate.",
    "start": "1012980",
    "end": "1018647"
  },
  {
    "text": "But there is a little\ncaveat that they didn't tell you about. So this 96%, this result was\nachieved with this dataset.",
    "start": "1018647",
    "end": "1027069"
  },
  {
    "text": "It was 580,000\noffline trajectories plus a very small portion of\nonline trajectories as well.",
    "start": "1027069",
    "end": "1034429"
  },
  {
    "text": "So 28,000 trajectories\ncollected online and that's what led to 96%.",
    "start": "1034430",
    "end": "1041530"
  },
  {
    "text": "Now we also evaluated the\nperformance of the system only on the 580,000\noffline trajectories",
    "start": "1041530",
    "end": "1047740"
  },
  {
    "text": "without any online\ndata collection. All right, so this\nis the offline--",
    "start": "1047740",
    "end": "1052840"
  },
  {
    "text": "sorry, offline\nreinforcement learning case. There is no on-policy data\ncollection whatsoever.",
    "start": "1052840",
    "end": "1058430"
  },
  {
    "text": "And you can tell that the\nfraction of online data collection is so small\nthat it's very unlikely",
    "start": "1058430",
    "end": "1064630"
  },
  {
    "text": "that this small\nfraction of data, the small additional dataset has\nsuddenly boosted generalization",
    "start": "1064630",
    "end": "1072460"
  },
  {
    "text": "in some crazy way. If it helped, it's probably\nbecause of the fact",
    "start": "1072460",
    "end": "1077500"
  },
  {
    "text": "that it's online not because\nof the fact that it's 28,000. So if you run just\nthis, just on this data",
    "start": "1077500",
    "end": "1083800"
  },
  {
    "text": "and we apply our\nQT Opt algorithm, we get the result of 87%. And it might seem not\nthat big of a difference,",
    "start": "1083800",
    "end": "1090470"
  },
  {
    "text": "but if you look\nat the error rate, here the error rate is\n4%, here the error is 13%.",
    "start": "1090470",
    "end": "1096049"
  },
  {
    "text": "So basically, we can cut\nit in 3 times, right? We just have an extra\n25,000 online trajectories.",
    "start": "1096050",
    "end": "1105500"
  },
  {
    "text": "So there is something\ngoing on here where the online data collection\nis really, really important and it can boost our\nresults significantly.",
    "start": "1105500",
    "end": "1113030"
  },
  {
    "text": "So let's take a look at what's\ngoing on in the offline case, why it isn't working\nas well as it should?",
    "start": "1113030",
    "end": "1120190"
  },
  {
    "text": " All right, so let's talk about\nsome offline RL, the problem",
    "start": "1120190",
    "end": "1126059"
  },
  {
    "text": "and the solutions.  All right, so in\nterms of the problem,",
    "start": "1126060",
    "end": "1132220"
  },
  {
    "text": "we talked about\nthe main equation in Q learning, the\nBellman equation.",
    "start": "1132220",
    "end": "1137299"
  },
  {
    "text": "So just to repeat, we said\nthat the optimal Q-function is equal to this expectation\nover the dynamics,",
    "start": "1137300",
    "end": "1143020"
  },
  {
    "text": "of the reward, of\nthe current step plus the max of the\nQ-function at the next step.",
    "start": "1143020",
    "end": "1148710"
  },
  {
    "text": "So let's analyze this\nsituation that we had in the previous slide. And we'll analyze it on this\nhalf cheetah environment.",
    "start": "1148710",
    "end": "1155450"
  },
  {
    "text": "That's a popular environment\nusing reinforcement learning. So the way we would analyze\nit is the following.",
    "start": "1155450",
    "end": "1161010"
  },
  {
    "text": "We would collect a\nbunch of trajectories while we are training\nthis agent to run forward",
    "start": "1161010",
    "end": "1166670"
  },
  {
    "text": "as fast as possible. And we will train it on-policy\nusing an off-policy algorithm",
    "start": "1166670",
    "end": "1172970"
  },
  {
    "text": "that will collect\ndata in the meantime, and we can get it to a\nmediocre performance that achieves the performance\nof around 5,000.",
    "start": "1172970",
    "end": "1180230"
  },
  {
    "text": "That's the reward that it gets. And then we'll take that\ndata set, that RL trace,",
    "start": "1180230",
    "end": "1185899"
  },
  {
    "text": "so all the data that\nagent generated. And then we'll try to learn\nthe same policy but by using",
    "start": "1185900",
    "end": "1192140"
  },
  {
    "text": "an offline RL algorithm. All right, so here are\nthe two important graphs.",
    "start": "1192140",
    "end": "1199280"
  },
  {
    "text": "So this shows the average\nreturn over the number of gradient steps if\nwe take that RL trace",
    "start": "1199280",
    "end": "1206080"
  },
  {
    "text": "and if we change the amount of\ndata that we have access to. So we can either generate\njust 1,000 trajectories",
    "start": "1206080",
    "end": "1213370"
  },
  {
    "text": "or I think in this\ncase, tuples that are 10,000 or 100,000\nor a million of them.",
    "start": "1213370",
    "end": "1218720"
  },
  {
    "text": "And we can look at\nthe average return of the agent that is training\nfrom the dataset offline.",
    "start": "1218720",
    "end": "1225262"
  },
  {
    "text": "And remember, the policy\nthat was collecting the data was achieving the\nreward of over 6,000.",
    "start": "1225262",
    "end": "1230470"
  },
  {
    "text": "Here we are at around\n0 or actually below 0. So we are not learning\nanything really, right?",
    "start": "1230470",
    "end": "1237519"
  },
  {
    "text": "Even though we have\ndata that definitely has some trajectories that\ncan achieve higher return,",
    "start": "1237520",
    "end": "1242950"
  },
  {
    "text": "our offline reinforcement\nlearning algorithm is not able to extract that. And then here on\nthe right there is",
    "start": "1242950",
    "end": "1250210"
  },
  {
    "text": "another plot which shows\nthe log of the Q-function. So basically, it tries\nto answer the question,",
    "start": "1250210",
    "end": "1255740"
  },
  {
    "text": "how well does our offline\nreinforcement learning algorithm thinks it does?",
    "start": "1255740",
    "end": "1261870"
  },
  {
    "text": "So this plot on\nthe left tells us how well it actually does\nand it doesn't do very well,",
    "start": "1261870",
    "end": "1267300"
  },
  {
    "text": "and the plot on the right tells\nus how well it thinks it does, right? And we can see that\nit actually thinks",
    "start": "1267300",
    "end": "1272987"
  },
  {
    "text": "that it's getting better and\nbetter for all of these cases, all right? So there is something\ngoing on here.",
    "start": "1272987",
    "end": "1279910"
  },
  {
    "text": "So does anybody have any\nidea what might be wrong? Yes. I have a question. In the [INAUDIBLE] why\nis the performance based on the smallest\nnumber of samples?",
    "start": "1279910",
    "end": "1286297"
  },
  {
    "start": "1286297",
    "end": "1292200"
  },
  {
    "text": "Yeah, that's a great question. So why is it that for a\nsmaller number of samples",
    "start": "1292200",
    "end": "1297300"
  },
  {
    "text": "we get higher Q-function? Yeah, I'm sure-- yeah, at\nthis point we don't know.",
    "start": "1297300",
    "end": "1303835"
  },
  {
    "text": "It's a good question.  Yes. Would you have the rewards\nbe sparse, could it be that the rewards are sparse?",
    "start": "1303835",
    "end": "1309938"
  },
  {
    "text": " Yes, in this case, even if we\nmake the rewards nodes sparse,",
    "start": "1309938",
    "end": "1315210"
  },
  {
    "text": "I think actually in half\ncheetah they are not sparse. It's still we can't really\nget good behavior for using",
    "start": "1315210",
    "end": "1321269"
  },
  {
    "text": "offline reinforcement learning. Good idea, though. Yes. It's overestimating its\nperformance because probably it went nuts over the best action\nassuming the [INAUDIBLE] on how much it [INAUDIBLE]",
    "start": "1321270",
    "end": "1327929"
  },
  {
    "start": "1327930",
    "end": "1336100"
  },
  {
    "text": "Right. So just to repeat the\nanswer, it's probably overestimating how well it's\ndoing for the Q-function",
    "start": "1336100",
    "end": "1343529"
  },
  {
    "text": "because that max term\nover here is probably sampling some actions that\nwe've never seen before,",
    "start": "1343530",
    "end": "1348670"
  },
  {
    "text": "and it thinks that those\nactions are doing really well. Yeah, so that's a\nreally good answer.",
    "start": "1348670",
    "end": "1354900"
  },
  {
    "text": "So there is something going\non with this max term that is causing our algorithm\nto not do very well",
    "start": "1354900",
    "end": "1361260"
  },
  {
    "text": "or our Q-function to\nbe overly optimistic. So let's analyze it\na little bit further.",
    "start": "1361260",
    "end": "1368559"
  },
  {
    "text": "All right, so this is\nour Bellman equation. And now let's imagine\nthat we are trying to fit our neural network\nto a true Q-function, which",
    "start": "1368560",
    "end": "1377730"
  },
  {
    "text": "is what we're trying to do. And let's try to\nplot it like this. So the true Q-function here\nis the plotting in green.",
    "start": "1377730",
    "end": "1385440"
  },
  {
    "text": "This is the true Q-function\nthat we are trying to fit to. And then we are doing a decent\njob with trying to fit to it,",
    "start": "1385440",
    "end": "1392910"
  },
  {
    "text": "and our fitted curve\nis the one in blue. All right, so it's a little\nbit off here and there, but overall, we are doing\na fairly decent job.",
    "start": "1392910",
    "end": "1401850"
  },
  {
    "text": "All right, so now\nthis term right here, this max over actions for\nour learned Q-function",
    "start": "1401850",
    "end": "1411029"
  },
  {
    "text": "will try to find the max for\nthe blue function, right? So in this case, the\nmax will be this peak",
    "start": "1411030",
    "end": "1416850"
  },
  {
    "text": "that we have right there. All right, and the\nproblem with this peak is that basically that\nour Q-learning algorithm",
    "start": "1416850",
    "end": "1424950"
  },
  {
    "text": "will try to find the max\nno matter what, right? So it will look for the\npeaks even though these peaks",
    "start": "1424950",
    "end": "1431130"
  },
  {
    "text": "might not be real. So it's kind of looking for\nthe part of the function",
    "start": "1431130",
    "end": "1438360"
  },
  {
    "text": "where we have the biggest\npositive error because it's just trying to find the max.",
    "start": "1438360",
    "end": "1444090"
  },
  {
    "text": "So in a sense, Q-learning\nis an adversarial algorithm. It's trying to find\nthe biggest error",
    "start": "1444090",
    "end": "1449520"
  },
  {
    "text": "that we have in the currently\nfitted Q-function, just the biggest positive error.",
    "start": "1449520",
    "end": "1454870"
  },
  {
    "text": "And the reason why\nthis is happening is because we don't know\nthe value of the actions that we haven't taken.",
    "start": "1454870",
    "end": "1460200"
  },
  {
    "text": "We don't know the\ncounterfactuals. So if you see all the data that\nwas collected by some policy,",
    "start": "1460200",
    "end": "1465399"
  },
  {
    "text": "it's really hard to tell\nwhat would have happened had I taken this other action? What kind of return\nwould I have gotten?",
    "start": "1465400",
    "end": "1472779"
  },
  {
    "text": "So it's exactly as\nthe previous answer. This max of our actions\ncan query some actions",
    "start": "1472780",
    "end": "1478440"
  },
  {
    "text": "that we've never seen before. And that leads to a\ndistribution shift. And then Q-learning\nwill pick those actions",
    "start": "1478440",
    "end": "1485310"
  },
  {
    "text": "because of the max operator. All right, so one\nquestion is, what",
    "start": "1485310",
    "end": "1490649"
  },
  {
    "text": "happens in the online case? How come this is not the\nproblem in the online case? Yes. Because the online\ncase will then update the policy with resampling\nthe states and actions for the next distribution.",
    "start": "1490650",
    "end": "1495663"
  },
  {
    "text": "So if you have a case like\nthis, then in the next case the action and\nreward will be seen, [INAUDIBLE] and so\nit will correct it. ",
    "start": "1495663",
    "end": "1511935"
  },
  {
    "text": "Yeah, that's right. Yeah, so in the online case,\njust to repeat the answer, we would still think that this\nis the best action to take.",
    "start": "1511935",
    "end": "1519669"
  },
  {
    "text": "But in the next iteration,\nwe'll actually take that action and then we'll see that\nthe reward for the action is not that high.",
    "start": "1519670",
    "end": "1525490"
  },
  {
    "text": "And then we'll fit the\nscript a little bit better. We'll push it down because\nwe'll just experience that the reward isn't as high.",
    "start": "1525490",
    "end": "1531840"
  },
  {
    "text": "And this online kind of\nactive correction process will make sure that our\nQ-function is correct.",
    "start": "1531840",
    "end": "1538529"
  },
  {
    "text": "In the offline case, we\ndon't have access to that. We can't really correct\nfor these errors.",
    "start": "1538530",
    "end": "1544789"
  },
  {
    "text": "All right, so let's think about\nsome solutions to our final RL problem. And first of all, we'll talk\nabout explicit set of solutions",
    "start": "1544790",
    "end": "1550670"
  },
  {
    "text": "and then about some\nimplicit solutions that seem to work a little better. So just to reiterate the problem\non a slightly different plot,",
    "start": "1550670",
    "end": "1560190"
  },
  {
    "text": "here we have on the\ny-axis, the value of the Q and on the x-axis, we have\nthe action that we pick.",
    "start": "1560190",
    "end": "1569250"
  },
  {
    "text": "And let's say that\nif we were to plot the distribution of\nthe actions that we see in the buffer\nof pi beta, it will",
    "start": "1569250",
    "end": "1576510"
  },
  {
    "text": "look like this dashed red line. All right, so we\nhave these actions that we've actually\nexperienced, and if we",
    "start": "1576510",
    "end": "1582630"
  },
  {
    "text": "were to fit a policy to it, it\nwould probably look like this, right? This dashed red line.",
    "start": "1582630",
    "end": "1588330"
  },
  {
    "text": "So then we can try to\nfit our Q-function, the blue line to this. And it would probably\nwork fairly well based",
    "start": "1588330",
    "end": "1593880"
  },
  {
    "text": "on the actions that we've seen. But then it would be completely\ncrazy for anything else, right?",
    "start": "1593880",
    "end": "1599472"
  },
  {
    "text": "We don't have any\nconstraint there. We don't know what these\nactions should result in. So it will experience\nthis distribution shift.",
    "start": "1599472",
    "end": "1607680"
  },
  {
    "text": "Now, the biggest peak will be\nused for the backups, right? So what we need is some\nkind of constraint, right?",
    "start": "1607680",
    "end": "1616620"
  },
  {
    "text": "We need to somehow\ntell our Q-function to not use these\npeaks right here",
    "start": "1616620",
    "end": "1623070"
  },
  {
    "text": "because we don't\nreally have support for those peaks in our data. So you can only pick the\npeak of your Q-function",
    "start": "1623070",
    "end": "1629790"
  },
  {
    "text": "that is supported by what\nwe have seen in the buffer. ",
    "start": "1629790",
    "end": "1635230"
  },
  {
    "text": "So we can optimize\nour Q-function or optimize the\nsum of the returns. But with this\nadditional constraint",
    "start": "1635230",
    "end": "1642040"
  },
  {
    "text": "saying that our policy\nthat we're trying to learn should be relatively\nclose to the policy that we have in the\nbuffer, all right?",
    "start": "1642040",
    "end": "1649660"
  },
  {
    "text": "So we are not able to\ndeviate from that policy. We have to stay close to it. We can only pick among those.",
    "start": "1649660",
    "end": "1657080"
  },
  {
    "text": "So now we have a few\ndifferent options on how to implement\nthis constraint. One option is to use a\nKL-divergence constraint.",
    "start": "1657080",
    "end": "1664240"
  },
  {
    "text": "So this is a way of measuring\nthe distance between two distributions. So we can measure explicitly\nthe distance between our policy",
    "start": "1664240",
    "end": "1671950"
  },
  {
    "text": "and the policy of the buffer. And then we can also use\nanother constraint, a constraint",
    "start": "1671950",
    "end": "1678840"
  },
  {
    "text": "that says that these\ntwo policies should have the same support. They don't have to be\nthe same distributions.",
    "start": "1678840",
    "end": "1684150"
  },
  {
    "text": "They don't have to be closed\nin terms of the distributions, but they should have\nthe same support. So whenever the policy can be--\nthe probability of some actions",
    "start": "1684150",
    "end": "1691620"
  },
  {
    "text": "can be larger than 0\nonly if the probability of the same actions\nunder our buffer policy",
    "start": "1691620",
    "end": "1698400"
  },
  {
    "text": "is larger than epsilon, right. And these two differences,\nthe difference",
    "start": "1698400",
    "end": "1703843"
  },
  {
    "text": "between the KL-divergence\nconstraint and the support constraint are actually\nquite significant. So if we were to do\ndistribution matching,",
    "start": "1703843",
    "end": "1710750"
  },
  {
    "text": "KL-divergence constraint,\nthat means that we could only pick policies that\nare very similarly",
    "start": "1710750",
    "end": "1719240"
  },
  {
    "text": "looking to the policies that\nwe have seen in the buffer, all right? So these purple policies\nthat you see right here.",
    "start": "1719240",
    "end": "1725300"
  },
  {
    "text": "These will be the\npolicies that have relatively small KL-divergence\nbetween the buffer",
    "start": "1725300",
    "end": "1731990"
  },
  {
    "text": "policy and the policy\nthat we're learning. However, when we use\nthe support constraint,",
    "start": "1731990",
    "end": "1738500"
  },
  {
    "text": "we can use any policy as long\nas it has the same support as our policy in the buffer.",
    "start": "1738500",
    "end": "1744810"
  },
  {
    "text": "So now it can be\nmuch more peaky. It can be much sharper. But as long as it has the same\nsupport, we are OK, all right?",
    "start": "1744810",
    "end": "1756830"
  },
  {
    "text": "Do you have any suggestions as\nto which one would be better? Which one would\nbe better to use?",
    "start": "1756830",
    "end": "1761870"
  },
  {
    "text": "The support constraint or\nthe KL-divergence constraint? ",
    "start": "1761870",
    "end": "1772090"
  },
  {
    "text": "Yes. With the support model\nyou get more freedom.  It's more fun because of\nmore degrees of freedom?",
    "start": "1772090",
    "end": "1777630"
  },
  {
    "text": "Yeah, that's correct. So if the support or the\ndistribution matching 1, you're quite\nconservative, right?",
    "start": "1777630",
    "end": "1784870"
  },
  {
    "text": "You can be only as\ngood as the policy that you've seen in the buffer. But here we can see that\nthis action is definitely",
    "start": "1784870",
    "end": "1790179"
  },
  {
    "text": "much better than these two. So maybe we might want to be\na little bit sharper, right? We want to make our\ngaps a little bit more",
    "start": "1790180",
    "end": "1797320"
  },
  {
    "text": "peaky like this one. But we wouldn't\nbe able to do that because of the\nKL-divergence constraint.",
    "start": "1797320",
    "end": "1802360"
  },
  {
    "text": "So the support constraint\nis not as conservative and allows us to do\na little bit more.",
    "start": "1802360",
    "end": "1808120"
  },
  {
    "text": "And that's however a little\nbit harder to implement. So people usually in\nthese explicit methods refer to the\nKL-divergence constraint.",
    "start": "1808120",
    "end": "1815603"
  },
  {
    "text": " All right, cool so there\nare some methods that",
    "start": "1815603",
    "end": "1822660"
  },
  {
    "text": "actually do this, and\nhere are some references. But they actually don't\nwork that well because of explicit\nrepresentation of this,",
    "start": "1822660",
    "end": "1828360"
  },
  {
    "text": "pi data is actually\nquite tricky. So the most modern methods\ndo this implicitly,",
    "start": "1828360",
    "end": "1833640"
  },
  {
    "text": "and I'll discuss two of them. So again, this is\nour function, and we",
    "start": "1833640",
    "end": "1839370"
  },
  {
    "text": "are trying to find the\npolicy that maximizes the Q subject to this\nconstraint so that we have",
    "start": "1839370",
    "end": "1846120"
  },
  {
    "text": "to be close to our pi data. So this is a constraint\noptimization problem",
    "start": "1846120",
    "end": "1852230"
  },
  {
    "text": "that we can actually solve via\nbranch multipliers via duality. And I won't go through the\nwhole derivation of this,",
    "start": "1852230",
    "end": "1859640"
  },
  {
    "text": "but if you're interested, it\nwas actually derived a few times in a few different papers. But I think one of the\nmost accessible ones",
    "start": "1859640",
    "end": "1866090"
  },
  {
    "text": "is the one described by Jan\nPeters et al in the paper called REPS, Relative\nEntropy Policy something,",
    "start": "1866090",
    "end": "1874580"
  },
  {
    "text": "policy optimization. All right, so the result of\nthis equation looks like this.",
    "start": "1874580",
    "end": "1880880"
  },
  {
    "text": "So our optimal policy is equal\nto the policy of the buffer",
    "start": "1880880",
    "end": "1886570"
  },
  {
    "text": "that we don't have access to. So the policy of the buffer\nthat is exponentially weighted by the advantage\nof our policy, right.",
    "start": "1886570",
    "end": "1897070"
  },
  {
    "text": "So we can actually\nimplement this, and I'll tell you a little bit\nabout the intuition of what this actually means in a second.",
    "start": "1897070",
    "end": "1903005"
  },
  {
    "text": "So if we were to implement\nthis, it would look like this. Our new policy would be equal\nto the max likelihood objective,",
    "start": "1903005",
    "end": "1909770"
  },
  {
    "text": "right? We are trying to\nfit our policy using the samples from the buffer. So at this point, we are\njust doing behavioral cloning",
    "start": "1909770",
    "end": "1917590"
  },
  {
    "text": "if we didn't have\nthese terms right here. This is, we are just trying\nto fit our policy to whatever",
    "start": "1917590",
    "end": "1923590"
  },
  {
    "text": "we've seen in the buffer. But then additionally, we\nweighted by some weight, right?",
    "start": "1923590",
    "end": "1930190"
  },
  {
    "text": "So there are some weights that\ndepends on state and action. And then this weight is\nexponentiated advantage.",
    "start": "1930190",
    "end": "1937490"
  },
  {
    "text": "So what that means is that we\nneed to estimate our advantage function, which tells us how\ngood is an action compared to all the other actions.",
    "start": "1937490",
    "end": "1943338"
  },
  {
    "text": "And then we'll put a\nlittle bit more weight on the actions that are better\nin terms of our advantage. So we'll do this weighted\nmaximum likelihood",
    "start": "1943338",
    "end": "1950650"
  },
  {
    "text": "where we won't just\ncopy all the actions, but we'll say that the actions\nthat have better advantage functions should have a\nlittle bit higher weight.",
    "start": "1950650",
    "end": "1957040"
  },
  {
    "text": "We want to do\nthese actions more.  And this is actually implemented\nin this algorithm called",
    "start": "1957040",
    "end": "1963640"
  },
  {
    "text": "advantage weighted regression\nas well as this algorithm by Ashvin Nair et al.",
    "start": "1963640",
    "end": "1968680"
  },
  {
    "text": "called AWAC. All right, so there's\none more algorithm.",
    "start": "1968680",
    "end": "1974429"
  },
  {
    "text": "Yes, there's a question. [INAUDIBLE] Yeah, so this is a normalization\nconstant, which usually we",
    "start": "1974430",
    "end": "1980640"
  },
  {
    "text": "treat as a hyper-parameter.  [INAUDIBLE] I guess more\nof these [INAUDIBLE]",
    "start": "1980640",
    "end": "1986249"
  },
  {
    "text": "it seems to be very dependent\non how good the pi beta is, so that how do you make\nthat into a sample using direct representative\n[INAUDIBLE] distribution to start it? ",
    "start": "1986249",
    "end": "2001680"
  },
  {
    "text": "Yeah, that's a great question. So to repeat the question,\nis that so far we rely that our pi beta has\nto be good enough so that we",
    "start": "2001680",
    "end": "2009870"
  },
  {
    "text": "can find a really good solution,\ngiven our support in pi beta. So how do we make sure that\npi beta is really good?",
    "start": "2009870",
    "end": "2015580"
  },
  {
    "text": "We don't. So we are trying to find\nthe best policy we can, the best possible policy\ngiven pi beta, right.",
    "start": "2015580",
    "end": "2021840"
  },
  {
    "text": "So we just have\naccess to some dataset and we will try to do\nthe best job we can",
    "start": "2021840",
    "end": "2027870"
  },
  {
    "text": "given that dataset, right? And if the data set\nisn't very good, then our policy\nwon't be very good. But hopefully, it will\nbe still quite good.",
    "start": "2027870",
    "end": "2036590"
  },
  {
    "text": "All right, so here's another\nsolution, an implicit solution that's actually\nfairly straightforward",
    "start": "2036590",
    "end": "2041720"
  },
  {
    "text": "that seems to be working\nreally, really well. Yes. A lot of people\nasked this question, is there an upper bound on\nhow long you can do the event, like a pi beta, like [INAUDIBLE]\nit would be so much better, use offline RL? ",
    "start": "2041720",
    "end": "2056712"
  },
  {
    "text": "Yeah, so the\nquestion is, is there an upper bound how well\nyou can do given pi beta? ",
    "start": "2056713",
    "end": "2064429"
  },
  {
    "text": "I believe that-- I think it's really tricky to\nfind that upper bound because",
    "start": "2064429",
    "end": "2069800"
  },
  {
    "text": "of the way that it\ncan be stitched. It's actually really\ntricky to find out what's the best possible\npolicy you can do.",
    "start": "2069800",
    "end": "2075800"
  },
  {
    "text": "There is a problem in offline\nreinforcement learning that talks about how you can\nrank different policies given",
    "start": "2075800",
    "end": "2081440"
  },
  {
    "text": "the dataset. But I'm not sure if\nyou can upper bound it. Yeah, it's a great question.",
    "start": "2081440",
    "end": "2088690"
  },
  {
    "text": "All right, cool. So there's one more solution\nto this problem that is an implicit solution that\nactually works really well",
    "start": "2088690",
    "end": "2094580"
  },
  {
    "text": "and it's relatively\nstraightforward, and it's called\nconservative Q-learning. And this was introduced\nby Avril Kumar in 2020.",
    "start": "2094580",
    "end": "2102250"
  },
  {
    "text": "So it works as following. So we have our\nfunctions as usual,",
    "start": "2102250",
    "end": "2107569"
  },
  {
    "text": "and now we have this peak\nthat was a problem, right? So this was the\npeak that was just the max of our actions\nfor our Q-function.",
    "start": "2107570",
    "end": "2114579"
  },
  {
    "text": " So what we can do is we\ncan take that peak, right?",
    "start": "2114580",
    "end": "2119920"
  },
  {
    "text": "We know where that peak is. This is what we do in\nour Q-target computation for Bellman equation, and\nwe can just push it down.",
    "start": "2119920",
    "end": "2127390"
  },
  {
    "text": "We can just say\nevery peak that we find, let's just push the\nvalue of it down, all right.",
    "start": "2127390",
    "end": "2133630"
  },
  {
    "text": "So let's try to do this. So this is the equation for CQL,\nthis conservative Q-learning.",
    "start": "2133630",
    "end": "2139315"
  },
  {
    "text": "It might be a\nlittle complicated, but it's actually\nfairly straightforward. So this second part here, this\nis just standard Q-learning.",
    "start": "2139315",
    "end": "2145880"
  },
  {
    "text": "So here we are just\nsaying that our current Q, the difference between our\ncurrent Q and the reward",
    "start": "2145880",
    "end": "2150970"
  },
  {
    "text": "plus the Q at the next\nstate should be minimized. All right, so this is just\nminimizing Bellman error.",
    "start": "2150970",
    "end": "2157030"
  },
  {
    "text": "But we have this\nadditional term right here. And this additional\nterm right here",
    "start": "2157030",
    "end": "2162910"
  },
  {
    "text": "is saying that we should be\npushing down big Q values. So here we are trying\nto maximize the policy.",
    "start": "2162910",
    "end": "2169900"
  },
  {
    "text": "So we are trying to\nfind the policy that maximizes the Q, right. So this is the policy\nthat takes that max.",
    "start": "2169900",
    "end": "2175720"
  },
  {
    "text": "And then we say take those\nQ values and push them down. So find those peaks and\npush each one of them down.",
    "start": "2175720",
    "end": "2184730"
  },
  {
    "text": "And you can do this,\nand it actually already works quite OK. And you can then show that\nthis Q-function computed",
    "start": "2184730",
    "end": "2192960"
  },
  {
    "text": "with this equation, it's upper\nbounded by the true Q-function,",
    "start": "2192960",
    "end": "2199990"
  },
  {
    "text": "right, because we are\npushing all the values down. So it already kind\nof works, but there",
    "start": "2199990",
    "end": "2208210"
  },
  {
    "text": "is one more problem with it in\nthat you're pushing everything down, right? So including some points\nthat we're actually",
    "start": "2208210",
    "end": "2213610"
  },
  {
    "text": "seeing in the data. So you can actually do a\nsmall modification to it and add one more term.",
    "start": "2213610",
    "end": "2219680"
  },
  {
    "text": "So here we are pushing\ndown big Q values, but in addition to this,\nwe'll add this other term that",
    "start": "2219680",
    "end": "2225190"
  },
  {
    "text": "will be pushing up everything\nthat we've seen in the data, right? So here is this additive\nof the negative term",
    "start": "2225190",
    "end": "2231819"
  },
  {
    "text": "as a negative term. So we are pushing up, we are\nminimizing this whole thing. So we are pushing this up,\nand we are sampling actions",
    "start": "2231820",
    "end": "2238180"
  },
  {
    "text": "from our dataset. And we were saying, push up\nthe Q values of everything that you've actually\nseen in the data.",
    "start": "2238180",
    "end": "2245810"
  },
  {
    "text": "So what this amounts\nto is that we want to push\neverything down that we haven't seen in the data. And if we have seen\nit in the data,",
    "start": "2245810",
    "end": "2251528"
  },
  {
    "text": "we'll push it back up, right? So for things that we've\nactually seen in the data, these two terms will\ncancel out and we'll just",
    "start": "2251528",
    "end": "2258172"
  },
  {
    "text": "do our normal Bellman equation. ",
    "start": "2258173",
    "end": "2264090"
  },
  {
    "text": "All right, so this\nis the final equation that we would use in CQL.",
    "start": "2264090",
    "end": "2270300"
  },
  {
    "text": "And the intuition\nis the following. So before what we had\nis this is another plot",
    "start": "2270300",
    "end": "2275790"
  },
  {
    "text": "of showing in the\nx-axis the Q value and the x-axis our actions. And then this is our\ntrue actual Q-function.",
    "start": "2275790",
    "end": "2284130"
  },
  {
    "text": "And let's say we have\na very small action support in our dataset. This is the actions that\nwe've seen in pi beta.",
    "start": "2284130",
    "end": "2291010"
  },
  {
    "text": "So if we were to fit\na naive Q-function, it would do a decent job\nin the action support case,",
    "start": "2291010",
    "end": "2296579"
  },
  {
    "text": "but everywhere else it\nwill be a little bit wild. And we'll pick the wrong\nvalues for our max operator.",
    "start": "2296580",
    "end": "2303880"
  },
  {
    "text": "So in the CQL case, we will fit\nthis conservative Q-function that will be conservative\nanywhere where we",
    "start": "2303880",
    "end": "2309870"
  },
  {
    "text": "haven't seen any data, right? So where we don't\nhave action support, our conservative\nQ-function here in blue",
    "start": "2309870",
    "end": "2315630"
  },
  {
    "text": "will be a little bit lower\nthan the actual Q-function, and this is OK. We want to be\npessimistic in this case.",
    "start": "2315630",
    "end": "2322380"
  },
  {
    "text": "However, in the action support\nwill be relatively well fitted and sometimes we will not\nunderestimate those values.",
    "start": "2322380",
    "end": "2329267"
  },
  {
    "text": "Sometimes we might\noverestimate them a little bit, but that might be\ngood enough for us to have a good Q-function.",
    "start": "2329267",
    "end": "2334680"
  },
  {
    "text": " All right, so let's\nsee how it works. So we can actually\nimplement it fairly easily.",
    "start": "2334680",
    "end": "2341430"
  },
  {
    "text": "We can just update\nour Q-function according to this\nloss that we just described where we sample\ndata from our dataset",
    "start": "2341430",
    "end": "2347579"
  },
  {
    "text": "and then we update\nthe policy in turn. And this algorithm\nwas actually tested",
    "start": "2347580",
    "end": "2352830"
  },
  {
    "text": "on three different datasets. One consists of this\nlittle ant robot",
    "start": "2352830",
    "end": "2359580"
  },
  {
    "text": "that is navigating in a maze. There is another\nset of environments that we're collecting\nsome demonstrations",
    "start": "2359580",
    "end": "2365520"
  },
  {
    "text": "and they were doing some\nhand manipulation skills and another one where we had a\nFranka robot that was deployed",
    "start": "2365520",
    "end": "2372030"
  },
  {
    "text": "in a simulated\nkitchen and had to do a bunch of different\nthings as well.",
    "start": "2372030",
    "end": "2377230"
  },
  {
    "text": "And here are the\ndifferent results showing all kinds of different\nbaseline comparisons. So you can do\nbehavioral cloning,",
    "start": "2377230",
    "end": "2382650"
  },
  {
    "text": "you can do some popular self\nactor-critic algorithms, other explicit\nmethods for offering",
    "start": "2382650",
    "end": "2388859"
  },
  {
    "text": "reinforcement learning. And you can see that CQL\nhere, this part, this CQL is",
    "start": "2388860",
    "end": "2394290"
  },
  {
    "text": "the one that pushes down the\nactions that achieve high or pushes down the\nQ values of actions",
    "start": "2394290",
    "end": "2401280"
  },
  {
    "text": "that we haven't\nseen and also has this additional\nterm that pushes up the actions that we have seen. And you can see\nthat this one works",
    "start": "2401280",
    "end": "2407760"
  },
  {
    "text": "much better across the board\nthan all the other algorithms, and so it's relatively simple. Yes, there is a question.",
    "start": "2407760",
    "end": "2412950"
  },
  {
    "text": "How would you expect, like,\nwhy would CQL perform with SEC? Is SEC using like,\ndoes it have to access to online interactions? ",
    "start": "2412950",
    "end": "2423089"
  },
  {
    "text": "So the question was\ndoes SEC have access to online interactions? No. So all of them have access\njust to the offline dataset.",
    "start": "2423090",
    "end": "2429942"
  },
  {
    "start": "2429942",
    "end": "2437295"
  },
  {
    "text": "Hi, sorry. Does that term that pushes\ndown for Q values for actions",
    "start": "2437295",
    "end": "2443069"
  },
  {
    "text": "outside of the action support? Does that sort of guarantee\nthat those Q values are going to be more than\nthe Q values or actions",
    "start": "2443070",
    "end": "2449940"
  },
  {
    "text": "inside the support, or\nis it still possible for those actions to have a high\nQ-value then after that term?",
    "start": "2449940",
    "end": "2456390"
  },
  {
    "text": "Yeah, that's a great question. Yeah, so it does guarantee that. It guarantees that because we\nwill be pushing all of them",
    "start": "2456390",
    "end": "2463110"
  },
  {
    "text": "down, that they will eventually\nbe lower than the ones that we see in the\naction support. And there is actually proof\nin a paper showing that.",
    "start": "2463110",
    "end": "2468960"
  },
  {
    "text": "Great question. ",
    "start": "2468960",
    "end": "2474550"
  },
  {
    "text": "All right, cool. Are there any other\nquestions at this point? Cool, all right, so we\nlearned about some solutions",
    "start": "2474550",
    "end": "2481990"
  },
  {
    "text": "to this offline reinforcement\nlearning problem. We know what the\nproblem actually is. We know how we can regularize\nit a little bit better",
    "start": "2481990",
    "end": "2488900"
  },
  {
    "text": "so that we can\nfind policies that are better than the policies\nthat we've collected before.",
    "start": "2488900",
    "end": "2496150"
  },
  {
    "text": "All right, so let's think a\nlittle bit about multitasking reinforcement learning\nand how it works in the offline multitask case.",
    "start": "2496150",
    "end": "2503950"
  },
  {
    "text": "So we talked already about\nmultitasking reinforcement learning algorithms,\nand we talked about how it's relatively\neasy to take an existing",
    "start": "2503950",
    "end": "2511987"
  },
  {
    "text": "reinforcement learning algorithm\nand make it a multitask reinforcement\nlearning algorithm. We can just add an\nadditional task identifier",
    "start": "2511987",
    "end": "2519090"
  },
  {
    "text": "z that tells us\nwhich task we're in, and we can add it as\npart of the state. So we can then change the policy\nto be conditioned on the task.",
    "start": "2519090",
    "end": "2527035"
  },
  {
    "text": "Similarly, we can\nchange the Q-function to be conditioned on the task. And then we said that\nthere is one difference",
    "start": "2527035",
    "end": "2532620"
  },
  {
    "text": "with reinforcement\nlearning, where in addition to sharing weights\nbetween different tasks,",
    "start": "2532620",
    "end": "2538200"
  },
  {
    "text": "we can also control\nthe data distribution. So in addition to\nsharing weights,",
    "start": "2538200",
    "end": "2543300"
  },
  {
    "text": "we can also share\nthe data, al right? So we can take data\ngenerated by one task",
    "start": "2543300",
    "end": "2548849"
  },
  {
    "text": "and relabel it as\ndata that would have come from another task. And then we went\nfor this one example",
    "start": "2548850",
    "end": "2556350"
  },
  {
    "text": "of multitask Q-learning applied\nto robotics called MT-opt. This is the multitask\nextension of QTM that we discussed before.",
    "start": "2556350",
    "end": "2564280"
  },
  {
    "text": "And it worked as following. There is this little GIF\nwhere we collect an episode. That episode is labeled\nas success or failure,",
    "start": "2564280",
    "end": "2570759"
  },
  {
    "text": "and then we can impersonate\nthat or relabel it for other tasks that are\nsuccesses and failure. And then use those episodes\nfor all the other tasks",
    "start": "2570760",
    "end": "2578140"
  },
  {
    "text": "and then take a stratified\nbatch and use it for multitask Q-learning.",
    "start": "2578140",
    "end": "2583910"
  },
  {
    "text": "And then we can learn many\ndifferent manipulation tasks. And we talked a little bit\nabout the average improvement",
    "start": "2583910",
    "end": "2590300"
  },
  {
    "text": "that we get over\ntraining single tasks and how it improves\nmore the tasks that have very little data\nand how we can quickly",
    "start": "2590300",
    "end": "2596420"
  },
  {
    "text": "fine tune to new\ntasks using data, collecting new data that we\nrequire just one day of data",
    "start": "2596420",
    "end": "2603050"
  },
  {
    "text": "collection and get really\nhigh performance on a task we haven't seen before. But there was one\nthing that I mentioned",
    "start": "2603050",
    "end": "2609940"
  },
  {
    "text": "when we were talking about this,\nwhich is the way we impersonate these tasks. So when we get the\nepisode and when",
    "start": "2609940",
    "end": "2615378"
  },
  {
    "text": "we have to decide\nwhether this episode will be reliable for this task\nor that one or all of them or none of them, this\ndecision actually matters,",
    "start": "2615378",
    "end": "2622880"
  },
  {
    "text": "and it matters a lot. And at that time when we were\nworking on this algorithm, we didn't understand\nthat, right.",
    "start": "2622880",
    "end": "2628240"
  },
  {
    "text": "So we just realized\nthat it's subtle. If you share data\nacross all of the tasks, it doesn't work very well.",
    "start": "2628240",
    "end": "2634743"
  },
  {
    "text": "If you don't share\ndata at all, it doesn't work very well either. But if you share data\nusing some heuristics,",
    "start": "2634743",
    "end": "2640060"
  },
  {
    "text": "sometimes it works really well. All right, so let's see if\nwe can use our offline RL",
    "start": "2640060",
    "end": "2646510"
  },
  {
    "text": "expertise now to maybe\ndecipher this a little bit to see what's going on.",
    "start": "2646510",
    "end": "2652117"
  },
  {
    "text": "All right, so we want\nto answer the question whether we can share data\nacross distinct tasks.",
    "start": "2652117",
    "end": "2657950"
  },
  {
    "text": "So we'll do this in a\nslightly smaller environment so we can understand\na little bit better.",
    "start": "2657950",
    "end": "2663920"
  },
  {
    "text": "So we'll be mixing data from\nthree different policies. We have this 2D walker,\nand we collected data",
    "start": "2663920",
    "end": "2669280"
  },
  {
    "text": "where it tries to run\nforward when it runs backward and when it jumps. And then we put all of\nthese data together.",
    "start": "2669280",
    "end": "2676490"
  },
  {
    "text": "All right, so these are\nthe different data sizes for these different runs. And here we vary a little bit\nhow we collected the data.",
    "start": "2676490",
    "end": "2683630"
  },
  {
    "text": "So sometimes we would get a\nperformance that is mediocre, and we'll take the\nentire replay buffer that got us to that performance.",
    "start": "2683630",
    "end": "2690742"
  },
  {
    "text": "And then we'll mix\nit all together, and then we'll see if we don't\nshare the data versus where we share all the data,\nwhat's the difference",
    "start": "2690742",
    "end": "2697150"
  },
  {
    "text": "in the performance? So in this case,\nfor these datasets, it seems like if we\ndon't share the data,",
    "start": "2697150",
    "end": "2703140"
  },
  {
    "text": "it's a little bit better than\nif we do share data across all the tasks. Now, if we use slightly\ndifferent replay buffers,",
    "start": "2703140",
    "end": "2710120"
  },
  {
    "text": "then it seems like\nthe opposite is true. Sharing the data is\nactually much better than not sharing\nthe data at all.",
    "start": "2710120",
    "end": "2717770"
  },
  {
    "text": "And then if we pick\nanother combination, it seems like average\nperformance is yet again better for no sharing than for\nsharing across all the data.",
    "start": "2717770",
    "end": "2726410"
  },
  {
    "text": "And it's especially true for\nthe tasks where we try to jump,",
    "start": "2726410",
    "end": "2731411"
  },
  {
    "text": "where we use actually\nthe expert dataset. ",
    "start": "2731412",
    "end": "2736430"
  },
  {
    "text": "All right, so\nremember that we still have everything that applies to\noffline reinforcement learning,",
    "start": "2736430",
    "end": "2741870"
  },
  {
    "text": "right. So we have an unknown\nbuffer policy pi beta.",
    "start": "2741870",
    "end": "2746970"
  },
  {
    "text": "So this is a little\npeculiar, right? There is no-- it's really hard\nto tell what's going on here.",
    "start": "2746970",
    "end": "2752369"
  },
  {
    "text": "Sometimes it helps to share\ndata, like in this case, sometimes it really hurts. Sometimes it's\nbetter not to share.",
    "start": "2752370",
    "end": "2758690"
  },
  {
    "text": "Sometimes it depends on what\ntask you're evaluating it on. So let's look a little\nbit more into it.",
    "start": "2758690",
    "end": "2765710"
  },
  {
    "text": "So it seems that sharing data\ngenerally helps, but it can hurt performance in some cases.",
    "start": "2765710",
    "end": "2770790"
  },
  {
    "text": "So can you characterize\nwhy it hurts performance? So then we added one more\nplot, one more number",
    "start": "2770790",
    "end": "2779600"
  },
  {
    "text": "that we started measuring. So we actually explicitly fit\nthe behavioral cloning policy to what we had in the buffer.",
    "start": "2779600",
    "end": "2786380"
  },
  {
    "text": "We did this explicit method. And then we tried to-- then we computed\nthe KL-divergence",
    "start": "2786380",
    "end": "2792500"
  },
  {
    "text": "between our policy\nand the policy that we have in the buffer. And then you can see that,\nespecially for this case",
    "start": "2792500",
    "end": "2799970"
  },
  {
    "text": "right here, when we\nshare the data, when we share the data\nacross all the tasks, the KL-divergence between\nthe policy that we",
    "start": "2799970",
    "end": "2806599"
  },
  {
    "text": "have in the buffer and\nour policy is huge, right? We are far much further\naway from the policy",
    "start": "2806600",
    "end": "2813530"
  },
  {
    "text": "that if we were not\nsharing data at all, right?",
    "start": "2813530",
    "end": "2819170"
  },
  {
    "text": "So do you have any idea\nwhy that could be the case? ",
    "start": "2819170",
    "end": "2833720"
  },
  {
    "text": "Yeah, this is a little tricky. This is a difficult question. All right, well, we'll\nthink about it some more.",
    "start": "2833720",
    "end": "2840680"
  },
  {
    "text": "But the main hint\nis that sharing data exacerbates the\ndistribution shift",
    "start": "2840680",
    "end": "2845950"
  },
  {
    "text": "that we talked about before. All right, so let's talk\na little bit about how",
    "start": "2845950",
    "end": "2851710"
  },
  {
    "text": "it happens, all right. ",
    "start": "2851710",
    "end": "2858430"
  },
  {
    "text": "So sharing data does exacerbate\nthe distribution shift, so we'll try to minimize this. But we'll first explain\nwhy that happens.",
    "start": "2858430",
    "end": "2866150"
  },
  {
    "text": "So to do this, let's introduce\none small piece of notation. We will assume that relabeling\ndata from some task j, right,",
    "start": "2866150",
    "end": "2875530"
  },
  {
    "text": "here depicted as Dj to task\ni generates a dataset Dj",
    "start": "2875530",
    "end": "2880540"
  },
  {
    "text": "to i which is then additionally\nused to train on task i, all right. So we can take data from task\nj and relabel it to task i.",
    "start": "2880540",
    "end": "2888730"
  },
  {
    "text": "And this is all depicted\nwith this symbol here. And then the effective\ndataset for task I",
    "start": "2888730",
    "end": "2895240"
  },
  {
    "text": "after relabeling\nwill be then given by the dataset that we had\nfor that task plus everything",
    "start": "2895240",
    "end": "2900609"
  },
  {
    "text": "that we re-labeled from\ntask j to task i, all right. So we expanded the\ndataset for that task",
    "start": "2900610",
    "end": "2907570"
  },
  {
    "text": "as you did in homework\n3, for instance. OK, cool.",
    "start": "2907570",
    "end": "2912990"
  },
  {
    "text": "So what it means is that now\nbecause we decide what kind",
    "start": "2912990",
    "end": "2918240"
  },
  {
    "text": "of dataset this task\nwill have, right, we decide what kind of\ntask or which episodes",
    "start": "2918240",
    "end": "2924210"
  },
  {
    "text": "get relabeled to\ntask i, we now get to control the dataset\nourselves or the behavior",
    "start": "2924210",
    "end": "2929610"
  },
  {
    "text": "policy, the policy of\nthe buffer ourselves. Does that make sense?",
    "start": "2929610",
    "end": "2934660"
  },
  {
    "text": "This is actually quite\ninsightful, quite a good insight here, right?",
    "start": "2934660",
    "end": "2940650"
  },
  {
    "text": "So we still have a\nfixed dataset that was collected for\nmany different tasks,",
    "start": "2940650",
    "end": "2946050"
  },
  {
    "text": "but now we can morph this\ndataset into something else because we can take some\ndata collected by one task",
    "start": "2946050",
    "end": "2951060"
  },
  {
    "text": "and relabel it to another task. So we'll change the distribution\nof the data collected by this other task, right?",
    "start": "2951060",
    "end": "2959020"
  },
  {
    "text": "Cool. Are there any questions to that? Does that make sense? All right, cool.",
    "start": "2959020",
    "end": "2965205"
  },
  {
    "text": "So this is not the first\ntime where we can actually control what's in the dataset\nin a sense by relabeling things.",
    "start": "2965205",
    "end": "2971950"
  },
  {
    "text": "Cool, so let's use\nthat fact to try to think about what's happening\nin this multitask offline",
    "start": "2971950",
    "end": "2977890"
  },
  {
    "text": "reinforcement learning. All right, so in the standard\noffline reinforcement learning,",
    "start": "2977890",
    "end": "2983320"
  },
  {
    "text": "if we were to simplify\nour objective, it would look like this. We are trying to maximize\nthe reward, right,",
    "start": "2983320",
    "end": "2989620"
  },
  {
    "text": "according to our dataset. So we are trying to find a\npolicy that maximizes this. But then in addition to this,\nwe have this regularizer",
    "start": "2989620",
    "end": "2997000"
  },
  {
    "text": "towards the data\nbehavior policy. So the policy that we see in\nthe buffer pi beta, right?",
    "start": "2997000",
    "end": "3003450"
  },
  {
    "text": "So this is the regularizer\nthat we've been talking about. OK, so then now so far\nwe've been just trying",
    "start": "3003450",
    "end": "3011120"
  },
  {
    "text": "to optimize for the\nbest policy that we can find that will optimize\nthis entire equation, right?",
    "start": "3011120",
    "end": "3017480"
  },
  {
    "text": "We assume that we have\nno control over pi beta. But we do have\ncontrol over pi beta.",
    "start": "3017480",
    "end": "3025800"
  },
  {
    "text": "So here we'll change the\nobjective a little bit and we'll say the following. In addition to\nfinding the arg max,",
    "start": "3025800",
    "end": "3032600"
  },
  {
    "text": "in addition to\nfinding our policy, now here we are operating not\non just dataset that we've",
    "start": "3032600",
    "end": "3037850"
  },
  {
    "text": "seen, but actually this\neffective dataset that includes the task that we relabel to.",
    "start": "3037850",
    "end": "3042980"
  },
  {
    "text": "We'll also be able to\nfind the best data, so",
    "start": "3042980",
    "end": "3048050"
  },
  {
    "text": "optimized for the\neffective behavior policy to maximize the reward and\nminimize the distribution shift.",
    "start": "3048050",
    "end": "3053880"
  },
  {
    "text": "So we are just now adding\nthis additional knob that we not only have to\nor can optimize our policy,",
    "start": "3053880",
    "end": "3060620"
  },
  {
    "text": "but we can also change what the\ndata distribution looks like, right? And this is depicted by this\npi beta and this pi beta",
    "start": "3060620",
    "end": "3069170"
  },
  {
    "text": "here and the fact that our\ndataset is now being changed by how we relabel the tasks.",
    "start": "3069170",
    "end": "3075970"
  },
  {
    "text": "OK, so now we can\nactually introduce a very simple way of sharing\ndata that should probably help.",
    "start": "3075970",
    "end": "3083680"
  },
  {
    "text": "So it's called\nconservative data sharing and we will share data only\nwhen conservative Q value will",
    "start": "3083680",
    "end": "3090430"
  },
  {
    "text": "increase for that task. So what this means\nis the following.",
    "start": "3090430",
    "end": "3095630"
  },
  {
    "text": "This is the data that we\nwould relabel to task I, and we can compute\nthe Q function",
    "start": "3095630",
    "end": "3101410"
  },
  {
    "text": "for that state and action pair. And we can compare\nit to the Q-function for the state and action pair\nfrom the dataset for that task.",
    "start": "3101410",
    "end": "3110650"
  },
  {
    "text": "And if it's better,\nthen we can use that. We can relabel to\nthat task, right? So we can then pick\nand choose which",
    "start": "3110650",
    "end": "3117640"
  },
  {
    "text": "episodes should be relatable\nto other tasks, which shouldn't, and we do this based\non our conservative Q value.",
    "start": "3117640",
    "end": "3125300"
  },
  {
    "text": "So we can take an episode that\nwas generated by some task. And then based on\nthis equation, we",
    "start": "3125300",
    "end": "3131850"
  },
  {
    "text": "can decide should\nthis episode be pushed into the buffer\nfor this task or desk task or some other task?",
    "start": "3131850",
    "end": "3138640"
  },
  {
    "text": "All right, there's a question. The Q function that\nyou were using, that's an estimate\nfrom the [INAUDIBLE]",
    "start": "3138640",
    "end": "3143740"
  },
  {
    "text": " Yeah, the Q-function we're\nusing is the current estimate",
    "start": "3143740",
    "end": "3150980"
  },
  {
    "text": "of task I. It's a\nconservative Q-function. So we were using CQL. [INAUDIBLE] once we add\nany state and actions for the dataset, can you\nnow remove them, literal or-",
    "start": "3150980",
    "end": "3157763"
  },
  {
    "start": "3157763",
    "end": "3162940"
  },
  {
    "text": "Yeah, that's a great question. So the question is we might have\nadded some previous transitions",
    "start": "3162940",
    "end": "3168760"
  },
  {
    "text": "based on our back then\nestimate of the Q-function. Can we now remove them? Yeah, so it's true.",
    "start": "3168760",
    "end": "3174910"
  },
  {
    "text": "We can. So as we are iterating\nover the dataset and we kind of decide every\ntime how to share the data,",
    "start": "3174910",
    "end": "3180520"
  },
  {
    "text": "this decision changes\nover the time of training. So we might find\nthat initially it's better to share data across\nall of the different tasks,",
    "start": "3180520",
    "end": "3186922"
  },
  {
    "text": "but later on maybe we should\nonly share among these tasks or something like that. [INAUDIBLE] these\ndifferent tasks, and this I think [INAUDIBLE] which one to\npick for the task [INAUDIBLE]",
    "start": "3186922",
    "end": "3193705"
  },
  {
    "start": "3193705",
    "end": "3199935"
  },
  {
    "text": "That's correct. Yeah, so you're always deciding\nfor any given state action pair how to relabel it. So every time you\nsee it, you have",
    "start": "3199935",
    "end": "3205660"
  },
  {
    "text": "to decide what task you\nshould relabel it to.",
    "start": "3205660",
    "end": "3211329"
  },
  {
    "text": "Cool. All right, so we\ncan implement this. We can implement this\nconservative data sharing",
    "start": "3211330",
    "end": "3218170"
  },
  {
    "text": "algorithm and then\nsee whether it prevents this additional\ndistribution shift.",
    "start": "3218170",
    "end": "3224540"
  },
  {
    "text": "So first we start seeing\nthat if we do implement this, we see that the KL-divergence\nbetween our policy",
    "start": "3224540",
    "end": "3230470"
  },
  {
    "text": "and the policy that\nwe see in the buffer actually goes down\nsignificantly, especially for this jump problem\nthat we had before.",
    "start": "3230470",
    "end": "3237817"
  },
  {
    "text": "So it seems that it reduces the\nKL-divergence between the data distribution and\nthe learning policy. So that's already a plus.",
    "start": "3237817",
    "end": "3244569"
  },
  {
    "text": "But this also translates\nto improved performance. So for this walker2d\ncase, the CDS algorithm",
    "start": "3244570",
    "end": "3252819"
  },
  {
    "text": "is able to achieve the best\nresult. All right, in addition",
    "start": "3252820",
    "end": "3258430"
  },
  {
    "text": "to this, we also try to send\nthis vision-based manipulation tasks that were actually\njust simulated tasks that we",
    "start": "3258430",
    "end": "3263770"
  },
  {
    "text": "used in MT-opt. So the tasks included things\nlike picking things up, putting",
    "start": "3263770",
    "end": "3270099"
  },
  {
    "text": "them on the ball and\nthings like this, things that we also had in the real\nworld for MT-opt experiments.",
    "start": "3270100",
    "end": "3275990"
  },
  {
    "text": "And we ran a few comparisons. One comparison is\nthis work called HIPI, where we decide\nhow to share task",
    "start": "3275990",
    "end": "3282040"
  },
  {
    "text": "based on the highest return. So we'll only share the\nepisode with another task if this other task would\nresult in high reward.",
    "start": "3282040",
    "end": "3290440"
  },
  {
    "text": "And then we also compare\nto this heuristic that we designed\nfor MT-opt which we call the skill-based heuristic.",
    "start": "3290440",
    "end": "3296960"
  },
  {
    "text": "So we'll just pick\nthe tasks that we thought should be sharing data\nand we just set it manually.",
    "start": "3296960",
    "end": "3302214"
  },
  {
    "text": " And then we can see\nthat across the board, CDS is much better than\nany of these equivalents.",
    "start": "3302215",
    "end": "3309579"
  },
  {
    "text": "It's also better than sharing\ndata across all the tasks or not sharing data at all.",
    "start": "3309580",
    "end": "3314990"
  },
  {
    "text": "So it seems that we can\ntake the tools that we just learn about from offline\nreinforcement learning.",
    "start": "3314990",
    "end": "3320180"
  },
  {
    "text": "And then use these tools\nto understand better what's going on in multitask\nreinforcement learning,",
    "start": "3320180",
    "end": "3325610"
  },
  {
    "text": "whether it's online or\noffline and understand better the underlying problem and\nthen use these tools to design",
    "start": "3325610",
    "end": "3331270"
  },
  {
    "text": "better task sharing algorithms\nor task relabeling algorithms that then help the\noverall performance.",
    "start": "3331270",
    "end": "3336400"
  },
  {
    "text": " All right, cool. So we talked about\noffline multitask RL",
    "start": "3336400",
    "end": "3341480"
  },
  {
    "text": "and how it influences\ndata sharing. Are there any questions\nat this point? ",
    "start": "3341480",
    "end": "3350290"
  },
  {
    "text": "All right, cool. So now the last part, which\nis offline goal-conditioned",
    "start": "3350290",
    "end": "3356619"
  },
  {
    "text": "reinforced learning,\nwhich I believe is one of the most exciting\nparts in reinforcement learning these days. So I'm particularly\nexcited about this topic.",
    "start": "3356620",
    "end": "3364250"
  },
  {
    "text": "If you have any questions,\nplease shout out and we'll try to\ngo through this. And hopefully, I can get across\nmy excitement about this.",
    "start": "3364250",
    "end": "3373383"
  },
  {
    "text": "All right, so we\ntalk a little bit about goal-conditioned\nreinforcement learning with hindsight relabeling. That was the topic\nof homework three.",
    "start": "3373383",
    "end": "3379893"
  },
  {
    "text": "And the way it worked\nwas the following. We were collecting some\ndata using some policy where",
    "start": "3379893",
    "end": "3385030"
  },
  {
    "text": "the reward was\nassociated with the goal that we wanted to achieve. And then we would store that\ndata in the replay buffer.",
    "start": "3385030",
    "end": "3393650"
  },
  {
    "text": "And then we can perform\nadditional hindsight relabeling, where we can relabel\nthe experience in the data",
    "start": "3393650",
    "end": "3399160"
  },
  {
    "text": "that we just collected where we\ncan use last state as the goal. So we can pretend\nthat this was the plan",
    "start": "3399160",
    "end": "3404320"
  },
  {
    "text": "that we wanted to\nget to and then we can relabel it with\nthat reward and started relabel data in\nthe replay buffer",
    "start": "3404320",
    "end": "3411110"
  },
  {
    "text": "and then update the\npolicy using this. All right, but you also\ntalk about other labeling strategies, how we can use\nany state from the trajectory,",
    "start": "3411110",
    "end": "3418180"
  },
  {
    "text": "not just the last state\nrelabeled to that. So now the question\nis, what happens if we",
    "start": "3418180",
    "end": "3424870"
  },
  {
    "text": "do it fully offline, all right? And now a little\ncaveat to this is",
    "start": "3424870",
    "end": "3434020"
  },
  {
    "text": "that if we can do this fully\noffline, what does that mean? So that means that now we\ncan take any dataset that",
    "start": "3434020",
    "end": "3443860"
  },
  {
    "text": "has some trajectories in it. It just needs to have\nstates and actions. So it doesn't need to\nhave rewards, right?",
    "start": "3443860",
    "end": "3449800"
  },
  {
    "text": "We don't need to collect\nadditional data for it. It doesn't need to have\ntasks, nothing like this.",
    "start": "3449800",
    "end": "3455140"
  },
  {
    "text": "We can just take any dataset\nthat has trajectories and apply this hindsight\ngoal-conditioned offline",
    "start": "3455140",
    "end": "3461950"
  },
  {
    "text": "reinforcement learning\nalgorithm that we would design in a second\nto it and learn something from it, right?",
    "start": "3461950",
    "end": "3467380"
  },
  {
    "text": "We can combine\nmultiple datasets. We remove tons of requirements\nthat we had before, where we needed to\nhave rewards, we",
    "start": "3467380",
    "end": "3473230"
  },
  {
    "text": "needed to have tasks and\nall these other things. Now we don't need\nany of this, right? We can just take any dataset,\nrelabel all the states",
    "start": "3473230",
    "end": "3480760"
  },
  {
    "text": "that we've accomplished\nto the goal states and then use offline\nreinforcement learning. We don't need any\nonline interactions,",
    "start": "3480760",
    "end": "3485953"
  },
  {
    "text": "and we learn from all of that. Yes, there is a question. [INAUDIBLE]",
    "start": "3485953",
    "end": "3491184"
  },
  {
    "text": "Yeah, so the rewards we\ncan just automatically set based on the stage that\nwe've actually achieved.",
    "start": "3491185",
    "end": "3497280"
  },
  {
    "text": "So we can just set that the\nreward function is very simple. It's a sparse reward\nfunction that says, if you got to that stage,\nyou get the reward one,",
    "start": "3497280",
    "end": "3504062"
  },
  {
    "text": "and otherwise, you\nget reward zero. And we can do this\nfor any dataset. So in a sense, we're\ndoing the rewards",
    "start": "3504062",
    "end": "3510000"
  },
  {
    "text": "but we can design\nthem on the spot. Yeah, we don't need\nthe reward function",
    "start": "3510000",
    "end": "3516000"
  },
  {
    "text": "that was used for the dataset. So this would finally\nput us in the situation",
    "start": "3516000",
    "end": "3521280"
  },
  {
    "text": "where we can access lots of\ndata, lots of big datasets that are out there that we\nused in supervised learning,",
    "start": "3521280",
    "end": "3527430"
  },
  {
    "text": "and we can also do this\nfor reinforcement learning. All right, so let's talk\nabout how we can do this.",
    "start": "3527430",
    "end": "3533010"
  },
  {
    "text": "And this is the work that was\ndone by Yevgen Chebotar called Actionable Models.",
    "start": "3533010",
    "end": "3539040"
  },
  {
    "text": "And the motivation for\nthis work was actually a little bit different. So the motivation for this\nwork was the following.",
    "start": "3539040",
    "end": "3544930"
  },
  {
    "text": "We had this dataset that\nwe collected with MT-opt where we had the robot doing\nall kinds of different things.",
    "start": "3544930",
    "end": "3550740"
  },
  {
    "text": "And we wanted to scale\nup the number of tasks that the robot was doing. So here we use like\n12 ablation tasks.",
    "start": "3550740",
    "end": "3556079"
  },
  {
    "text": "We got maybe 16 or\n20 tasks or so on. But at some point we\nrealized that actually,",
    "start": "3556080",
    "end": "3561300"
  },
  {
    "text": "when we tried to scale\nthis up, the task definitions themselves\nbecome a bottleneck. It's actually\nsurprisingly tricky",
    "start": "3561300",
    "end": "3567810"
  },
  {
    "text": "to just sit down and think about\nwhat kind of different things the robot can do given\nits constrained workspace,",
    "start": "3567810",
    "end": "3573570"
  },
  {
    "text": "constrained action\nspace and so on. And if you want to do thousands\nand thousands of tasks.",
    "start": "3573570",
    "end": "3578859"
  },
  {
    "text": "It's actually difficult to come\nup with all of them yourself. So then we thought, well,\ngoal state is a task, right?",
    "start": "3578860",
    "end": "3585810"
  },
  {
    "text": "It's an insight that we\nalready know as well. So we can get rewards\nthrough hindsight relabeling.",
    "start": "3585810",
    "end": "3592320"
  },
  {
    "text": "We can relabel the\nepisode and then set that the reward for\naccomplishing the last state",
    "start": "3592320",
    "end": "3598560"
  },
  {
    "text": "is just 1, everything else is 0. And this way we can get lots\nof different tasks, right?",
    "start": "3598560",
    "end": "3604350"
  },
  {
    "text": "Like every episode\nwould give us a task. A task would be just\ndefined by the goal image. ",
    "start": "3604350",
    "end": "3611253"
  },
  {
    "text": "But then there's\none more problem. If we do this,\nthen we'll generate only positive examples. We'll only tell the robot how to\nachieve something, what to do.",
    "start": "3611253",
    "end": "3620110"
  },
  {
    "text": "But we wont really\ntell it what not to do. So in addition to\nthis, we thought, well, let's employ\nconservative Q-learning",
    "start": "3620110",
    "end": "3626290"
  },
  {
    "text": "to create artificial negative\nexamples so that we can tell it what not to do as well.",
    "start": "3626290",
    "end": "3632299"
  },
  {
    "text": "All right, so here's a little-- wait one second. Here's a little GIF\nshowing how it works,",
    "start": "3632300",
    "end": "3638050"
  },
  {
    "text": "but we'll go over\nthis in detail. But one second, let it\nstart from the beginning.",
    "start": "3638050",
    "end": "3645190"
  },
  {
    "text": "All right, so we take\nan episode from MT-opt. This is the video\nof that episode, and we can take all\nthe states from that.",
    "start": "3645190",
    "end": "3651700"
  },
  {
    "text": "With then artificial we can\nregularize our Q-function based on artificial\nnegatives, and we",
    "start": "3651700",
    "end": "3656920"
  },
  {
    "text": "can take all the subsequences,\nrelabel the last state as the goal and then\nuse that little episode",
    "start": "3656920",
    "end": "3662440"
  },
  {
    "text": "for the actual model\ntraining where we do offline reinforcement learning. And actually, the infrastructure\nand the algorithms that we use",
    "start": "3662440",
    "end": "3669369"
  },
  {
    "text": "are very similar to\nMT-opt except now this is goal-conditioned, and we\ndo this artificial negatives and relabeling with the goals.",
    "start": "3669370",
    "end": "3675535"
  },
  {
    "text": " All right, so there's a few more\ninteresting points about this.",
    "start": "3675535",
    "end": "3683760"
  },
  {
    "text": "So if we learn\nsomething like this, it gives us something\nlike a world model, like what you learn about in\nmodel-based RL lectures, where",
    "start": "3683760",
    "end": "3692540"
  },
  {
    "text": "it's independent of the reward. It's just a model that tells\nyou how the world works. So this is actually\nquite similar.",
    "start": "3692540",
    "end": "3700000"
  },
  {
    "text": "This gives us a functional\nunderstanding of the world. It tells you that given my\ncurrent image and some image",
    "start": "3700000",
    "end": "3706520"
  },
  {
    "text": "that I want to get to,\nwhat's the probability that I will actually get there? How likely it is that I'm\nable to achieve that future.",
    "start": "3706520",
    "end": "3713960"
  },
  {
    "text": "And it's task-independent. It's all based on the\ngoal images, right? There is no tasks involved. It's just images.",
    "start": "3713960",
    "end": "3722092"
  },
  {
    "text": "But in addition\nto a world model, it also gives you an\nactionable policy. So not only it tells you\nwell, how probable is it",
    "start": "3722092",
    "end": "3728920"
  },
  {
    "text": "that I can get from this\nstate to that state, but it can also tell\nyou well, what's the action that will\ntake me there, right?",
    "start": "3728920",
    "end": "3734140"
  },
  {
    "text": "You can find the\nbest action according to your goal-conditioned\nQ-function and take that action and that's\nthe best action that the model",
    "start": "3734140",
    "end": "3740050"
  },
  {
    "text": "thinks will get you there.  And then in addition\nto this, we can use it",
    "start": "3740050",
    "end": "3745170"
  },
  {
    "text": "as an unsupervised\nobjective for robotics or for reinforcement\nlearning, right? We don't need any supervision.",
    "start": "3745170",
    "end": "3750490"
  },
  {
    "text": "We don't need additional\nrewards and so on. We can take any dataset\nand just apply it to it and just see what happens.",
    "start": "3750490",
    "end": "3757960"
  },
  {
    "text": "And then we can use it for\nzero shot visual tasks. So we can present\nit a goal image that it's never seen before\nand see how well it does.",
    "start": "3757960",
    "end": "3764550"
  },
  {
    "text": "But we can also use it as an\nunsupervised free training that we can then use further\nfor downstream fine tuning.",
    "start": "3764550",
    "end": "3771960"
  },
  {
    "text": "Cool. So let's see how it works. So we start with this offline\ndataset of robotic experience.",
    "start": "3771960",
    "end": "3779250"
  },
  {
    "text": "So these are just\nsome trajectories that we've collected. In this case, these were\nprecisely the trajectories",
    "start": "3779250",
    "end": "3786109"
  },
  {
    "text": "that we collected with\nthe MT-opt setting. And then we can relabel\nall of these trajectories",
    "start": "3786110",
    "end": "3792500"
  },
  {
    "text": "and all of their\nsubsequences with goals and mark as successes,\nsimilarly to what you're doing in homework three.",
    "start": "3792500",
    "end": "3799490"
  },
  {
    "text": "So at this point, we have\nall these trajectories and the subsequences of those,\nbut they're all successful,",
    "start": "3799490",
    "end": "3804619"
  },
  {
    "text": "all right? So we have this problem\nthat the Q-function just knows what to do. ",
    "start": "3804620",
    "end": "3810849"
  },
  {
    "text": "All right, so to resolve this,\nwe'll use some of the insights that we just got from offline\nreinforcement learning.",
    "start": "3810850",
    "end": "3815900"
  },
  {
    "text": "We'll create\nartificial negatives, and it will work as follows. So right now we have\nonly positive examples.",
    "start": "3815900",
    "end": "3823630"
  },
  {
    "text": "We need some negatives. So we'll employ this\nconservative strategy where we say we'll minimize\nthe Q values of actions",
    "start": "3823630",
    "end": "3830230"
  },
  {
    "text": "that we have not seen,\nso very similar to what CQL was doing, right? Well I'll just say that if these\nare the actions that I have",
    "start": "3830230",
    "end": "3837520"
  },
  {
    "text": "seen, right, these are the\nactions that I just relabeled, these are positives, I will also\nsample some actions that I've",
    "start": "3837520",
    "end": "3844180"
  },
  {
    "text": "never seen, and I'll just\nminimize the Q values on those.",
    "start": "3844180",
    "end": "3849890"
  },
  {
    "text": "But in particular, I won't\nbe just sampling any action. I will be sampling the actions\nthat the Q-function currently",
    "start": "3849890",
    "end": "3855160"
  },
  {
    "text": "thinks are particularly\ngood, right? I will be just pushing\ndown on those peaks that we are talking about.",
    "start": "3855160",
    "end": "3861890"
  },
  {
    "text": "So we'll sample contrastive\nartificial negative actions, and we'll sample them according\nto the exponent of the Q.",
    "start": "3861890",
    "end": "3867500"
  },
  {
    "text": "So we'll ask our\nQ-function, what do you think about this action? If it thinks of it\nvery highly, then we'll push down the\nvalue of it, right.",
    "start": "3867500",
    "end": "3874750"
  },
  {
    "text": "It's very similar to CQL. You can think of\nit as CQL just in the goal-conditioned learning.",
    "start": "3874750",
    "end": "3882150"
  },
  {
    "text": "Cool. All right, so we can take\nthese relabeled sequences",
    "start": "3882150",
    "end": "3887550"
  },
  {
    "text": "and add conservative\naction negatives and mark those as failures. And at this point,\nwe can consider",
    "start": "3887550",
    "end": "3894188"
  },
  {
    "text": "training a goal-conditioned\nQ-function that should be able to accomplish\nany goal that it's seen inside the\ntrajectory, all right.",
    "start": "3894188",
    "end": "3903400"
  },
  {
    "text": "But there's one\nmore little caveat. So at this point, we'll learn\nhow to get from any point",
    "start": "3903400",
    "end": "3910510"
  },
  {
    "text": "or from a starting\npoint in a trajectory to any goal in that\ntrajectory, all right? So I can get from here to here.",
    "start": "3910510",
    "end": "3916240"
  },
  {
    "text": "I can get from here to here\nand from here to there. But I don't really know how\nto get from here to here,",
    "start": "3916240",
    "end": "3924100"
  },
  {
    "text": "for instance, all right? I don't know how to get from\nbeginning of one trajectory",
    "start": "3924100",
    "end": "3929710"
  },
  {
    "text": "to the end or the middle\nof another trajectory. We don't really know how\nto chain these goals.",
    "start": "3929710",
    "end": "3935109"
  },
  {
    "text": "We don't know how to chain\nthem across the episodes. So far we've been only doing\nit within an episode, OK?",
    "start": "3935110",
    "end": "3940420"
  },
  {
    "text": " So let's think about\nhow we can do this.",
    "start": "3940420",
    "end": "3945640"
  },
  {
    "text": "Are there any ideas how\nwe could accomplish this?",
    "start": "3945640",
    "end": "3951089"
  },
  {
    "text": "Yes. By just hoping it generalizes. By just hoping it generalizes. Yes, I think that's one option\nthat maybe the goal that we've",
    "start": "3951090",
    "end": "3960565"
  },
  {
    "text": "accomplished in this trajectory\nlooks similar to the goal that we accomplish\nin this trajectory and for our generalizer to work.",
    "start": "3960565",
    "end": "3966450"
  },
  {
    "text": "Yes. [INAUDIBLE] we can\nsort of branch out, so [INAUDIBLE] the trajectories\nare very close to each other, we can [INAUDIBLE] ",
    "start": "3966450",
    "end": "3985440"
  },
  {
    "text": "I see. So if two states are similar\nfor moving the same trajectory-- Based on that, we can splice\nin trajectories from that.",
    "start": "3985440",
    "end": "3991662"
  },
  {
    "text": " I see. We can merge two different\ntrajectories from that point,",
    "start": "3991662",
    "end": "3997059"
  },
  {
    "text": "right. So we would need to know\nwhich states are similar or find some kind of\nsimilarity metric.",
    "start": "3997060",
    "end": "4002225"
  },
  {
    "text": "Yeah, I think something\nlike that could work. We would need to\nprobably find how",
    "start": "4002225",
    "end": "4007313"
  },
  {
    "text": "to tell whether two\nstates are similar or not. But yeah, that's\nprobably possible, mm-hm.",
    "start": "4007313",
    "end": "4012920"
  },
  {
    "text": "Any other ideas?  All right, cool. So this is actually a\npractical problem that we had.",
    "start": "4012920",
    "end": "4020190"
  },
  {
    "text": "So with MT-opt, we\ncollect the trajectories for two different tasks. This is just an example\nfor two different tasks.",
    "start": "4020190",
    "end": "4026760"
  },
  {
    "text": "So we had a picking task,\nand the picking task would start with the arm\nbeing withdrawn from the bin.",
    "start": "4026760",
    "end": "4032990"
  },
  {
    "text": "And it would end with\nthe arm having something in the gripper, right? And that would be the\nend of the picking task.",
    "start": "4032990",
    "end": "4039800"
  },
  {
    "text": "And then it will have a separate\ntask called placing task. So it would be a\nseparate trajectory. So let's say this\nis the picking task.",
    "start": "4039800",
    "end": "4046100"
  },
  {
    "text": "It would start at the\narm being withdrawn, and it would end with the image\nof the arm holding something.",
    "start": "4046100",
    "end": "4052370"
  },
  {
    "text": "And then it will have a second\ntrajectory, the second task that was a placing\ntask which would start",
    "start": "4052370",
    "end": "4057680"
  },
  {
    "text": "from the image of\nthe arm holding something in the\ngripper, and it would end with the arm withdrawn\nand the object in the ball",
    "start": "4057680",
    "end": "4064730"
  },
  {
    "text": "or something like\nthat, all right? So it would be a\ndifferent trajectory that now starts when this\nother task ended, and it",
    "start": "4064730",
    "end": "4071540"
  },
  {
    "text": "ends in a very different\ngoal image, right. So there is almost no\nintersection between these two",
    "start": "4071540",
    "end": "4077900"
  },
  {
    "text": "trajectories except maybe\nthe last image of this one is similar to the first\nimage of this one,",
    "start": "4077900",
    "end": "4082940"
  },
  {
    "text": "sometimes not even that. All right, so in that\ncase, we can't really",
    "start": "4082940",
    "end": "4088620"
  },
  {
    "text": "ask a robot to start from\nan arm withdrawal state to put something in a\nball because it's never",
    "start": "4088620",
    "end": "4096899"
  },
  {
    "text": "seen that goal from\nthis initial state. ",
    "start": "4096899",
    "end": "4102359"
  },
  {
    "text": "All right, so we'll do\nthis through a procedure called goal chaining. And it will be very similar\nto what you described.",
    "start": "4102359",
    "end": "4110929"
  },
  {
    "text": "All right, so we'll just\nsample random goals, random images from our\ndataset, all right,",
    "start": "4110930",
    "end": "4116240"
  },
  {
    "text": "any random image goes. We'll just sample random\nimages from the dataset.",
    "start": "4116240",
    "end": "4121609"
  },
  {
    "text": "And then we'll recondition our\ntrajectory on that random goal to enable chaining goals\nacross episodes, right?",
    "start": "4121609",
    "end": "4128778"
  },
  {
    "text": "So we can just take\nour trajectory, sample a random state\nand just say, well, what would happen if that\ntrajectory is conditioned",
    "start": "4128779",
    "end": "4135770"
  },
  {
    "text": "on that goal state? Yes, there is a question. What action do you use,\nbecause the trajectory states and actions, this is\npick the random state before it picked the action?",
    "start": "4135770",
    "end": "4141766"
  },
  {
    "start": "4141766",
    "end": "4147970"
  },
  {
    "text": "Right, so the question\nis what action do you use because you\njust took a random state and there was no action\nassociated with it?",
    "start": "4147970",
    "end": "4153640"
  },
  {
    "text": "Yeah, so we just use it\nas a potential goal state. So for the goal state,\nyou don't need an action.",
    "start": "4153640",
    "end": "4158710"
  },
  {
    "text": "Right, so our Q-function is\nstate, action, and the goal. So we just set the goal\nstate for the random state,",
    "start": "4158710",
    "end": "4165429"
  },
  {
    "text": "but we still have\nstates and actions from our current\ntrajectory, right.",
    "start": "4165430",
    "end": "4170770"
  },
  {
    "text": "Would it be negative or no?  Yeah, most of them\nwill be negative. That's right.",
    "start": "4170770",
    "end": "4177729"
  },
  {
    "text": "So now if there exists\na pathway to that goal, dynamic programming\nwill find that path",
    "start": "4177729",
    "end": "4185199"
  },
  {
    "text": "and will propagate\nthat reward, right? If there was some similarity\nbetween the states, like you are saying,\ndynamic programming itself",
    "start": "4185200",
    "end": "4192339"
  },
  {
    "text": "for generalization will\nbe able to find a state and then say, well,\nthe Q-function for this is actually quite high. You can get to that goal, right?",
    "start": "4192340",
    "end": "4199329"
  },
  {
    "text": "I found a pathway\nthrough the episodes, for instance in the\npicking and placing case. I found that if\nyou just continue,",
    "start": "4199330",
    "end": "4206020"
  },
  {
    "text": "you will actually get\nto the goal you want to. And we'll find that\npathway and we'll set the Q-function correctly.",
    "start": "4206020",
    "end": "4212530"
  },
  {
    "text": "But if there is no pathway\nto the goal, as you said, the conservative strategy\nwill minimize those Q values,",
    "start": "4212530",
    "end": "4218469"
  },
  {
    "text": "and then because of that, we\nwon't erroneously think that we can get there even\nthough we can't. ",
    "start": "4218470",
    "end": "4225719"
  },
  {
    "text": "Cool. So now we can recondition on any\nrandom goal and it should work. ",
    "start": "4225720",
    "end": "4232031"
  },
  {
    "text": "All right, so we added these\nconservative action negatives. We added random goals\nfor goal training. So we are now ready to train\na goal-conditioned Q-function.",
    "start": "4232032",
    "end": "4239590"
  },
  {
    "text": "And we do this using\ncontinuous action Q-learning like in MT-opt.",
    "start": "4239590",
    "end": "4245650"
  },
  {
    "text": "We already have these\nconservative actions, so we can do this with just\nan additional goal image.",
    "start": "4245650",
    "end": "4252572"
  },
  {
    "text": "All right, so we talked\nabout two applications, and first of them\nwas a goal region. So we can see if we can\nreach certain goal images.",
    "start": "4252572",
    "end": "4259900"
  },
  {
    "text": "So let's try this. And again, this is\nthe exact same dataset that we used for MT-opt. We didn't collect\nany additional data.",
    "start": "4259900",
    "end": "4266313"
  },
  {
    "text": " All right, so actually, let\nme pause this for a second.",
    "start": "4266313",
    "end": "4272920"
  },
  {
    "text": "All right, so here\nin the top right, you can see what the\nrobot sees currently. This is the livestream from\nthe camera image of the robot.",
    "start": "4272920",
    "end": "4280990"
  },
  {
    "text": "And here in the\nbottom right, you can see the goal image\nthat we requested. All right, so here you\ncan see that the goal",
    "start": "4280990",
    "end": "4286780"
  },
  {
    "text": "image we requested is\nthat the robot is holding a carrot inside its gripper,\nand the gripper is on the right.",
    "start": "4286780",
    "end": "4291949"
  },
  {
    "text": "So now the robot will\ntry to do anything it can to get to\nan image that looks very similar to this image.",
    "start": "4291950",
    "end": "4298670"
  },
  {
    "text": "So that doesn't only mean that\nyou have to pick the carrot. It also means that you have\nto position your arm correctly",
    "start": "4298670",
    "end": "4303850"
  },
  {
    "text": "so it looks the same and\nall the other objects look the same and so on. ",
    "start": "4303850",
    "end": "4310372"
  },
  {
    "text": "All right, so this is an\nexample of picking a carrot. It seems to be\ndoing a decent job. Now we are asking you\nto pick a grasp corncob",
    "start": "4310372",
    "end": "4317260"
  },
  {
    "text": "by showing an image of that. It seems to be\ndoing this as well. Now it tries to do\nthis with a broccoli.",
    "start": "4317260",
    "end": "4322735"
  },
  {
    "text": " And then we can also do\nthis container placing.",
    "start": "4322735",
    "end": "4329330"
  },
  {
    "text": "So here we show an\nimage of a carrot being inside the container. And you can see that it\nputs the counter there",
    "start": "4329330",
    "end": "4334460"
  },
  {
    "text": "and then it positions\nthe arm so that it's similar to the goal image. Here we can also show the image\nof a banana being in a bowl",
    "start": "4334460",
    "end": "4340699"
  },
  {
    "text": "and the robot tries to\naccomplish that goal image. Here it also moves\nbroccoli into the bowl",
    "start": "4340700",
    "end": "4346675"
  },
  {
    "text": "and then it moves\nthe bowl a little bit so that it matches the image. It can also do a\nrearrangement task,",
    "start": "4346675",
    "end": "4352500"
  },
  {
    "text": "and it tries to do a\ndecent job but it's OK, where it moves one object to be\non the other side of the other.",
    "start": "4352500",
    "end": "4360120"
  },
  {
    "text": "Here it moves the carrot\naway from the tomato. And you can see that very often\nit doesn't push the object.",
    "start": "4360120",
    "end": "4365250"
  },
  {
    "text": "It picks them up and\ndrops them, which is very similar to\nexperiences that it experienced in MT-opt, where we\nare picking and placing things.",
    "start": "4365250",
    "end": "4371880"
  },
  {
    "text": "So it's trying to stay within\nthis pi beta distribution. ",
    "start": "4371880",
    "end": "4377250"
  },
  {
    "text": "Here you can also move\nsome cupcakes away with broccoli and banana. ",
    "start": "4377250",
    "end": "4383820"
  },
  {
    "text": "And then we also have-- also I think maybe\nwe'll see this next. ",
    "start": "4383820",
    "end": "4389862"
  },
  {
    "text": "We can see that\nsometimes it tries to push it over\nand over so that it gets closer to the goal image.",
    "start": "4389862",
    "end": "4396450"
  },
  {
    "text": "And eventually, it\nalmost gets there. And then we also tried\nto test it on objects that it's never seen before.",
    "start": "4396450",
    "end": "4402160"
  },
  {
    "text": "So we've never seen the\nsilver spoon before, but it seems that it\ncan still figure out what actions will take you\nto this goal image that also",
    "start": "4402160",
    "end": "4409020"
  },
  {
    "text": "involves the silver spoon\neven though you've never seen that silver spoon before. So it pushes the spoon\ntowards the ball.",
    "start": "4409020",
    "end": "4416085"
  },
  {
    "text": " This is the same with an another\nexample of an unseen object.",
    "start": "4416085",
    "end": "4422770"
  },
  {
    "text": "In this case, it's\na fork that is being moved towards the cupcakes.",
    "start": "4422770",
    "end": "4429160"
  },
  {
    "text": "And then we try to categorize\nsome of these results, and we kind of do\nthis post-factum. So we just look at\nthem and try to see",
    "start": "4429160",
    "end": "4436360"
  },
  {
    "text": "what these would correspond to. And we then try to\nmeasure the success rate. So for instance, grasping\ngoal images we get to 92%,",
    "start": "4436360",
    "end": "4442990"
  },
  {
    "text": "for rearrangement 74%, and\ncontainment placing 66%. Yeah, there's a question. What is the reward to\nsay to stop moving?",
    "start": "4442990",
    "end": "4448793"
  },
  {
    "text": " Right. So the question is, how does\nthe robot decide to stop moving?",
    "start": "4448793",
    "end": "4455020"
  },
  {
    "text": "So it has an\nadditional action which is called a terminate action. So if it sends that action,\nthat means it terminates",
    "start": "4455020",
    "end": "4460969"
  },
  {
    "text": "and then we decide the reward. ",
    "start": "4460970",
    "end": "4466540"
  },
  {
    "text": "All right, cool. And here it also plays with\nsome other unseen object. And this is a\ndeformable object that is actually quite\ntricky to manipulate,",
    "start": "4466540",
    "end": "4473080"
  },
  {
    "text": "and it's never seen this before. But it pokes around and tries\nto do something with it. Yes. So [INAUDIBLE] someone\nhad the action space, the object in that\norientation, and then it",
    "start": "4473080",
    "end": "4478195"
  },
  {
    "text": "would be [INAUDIBLE] ",
    "start": "4478195",
    "end": "4489179"
  },
  {
    "text": "Right. So the question is, how do\nyou get the goal images? Yeah, so in this\ncase, we are manually",
    "start": "4489180",
    "end": "4494739"
  },
  {
    "text": "doing this, which\nkind of defeats the purpose of the\nentire exercise, because you have to\naccomplish the task first",
    "start": "4494740",
    "end": "4499810"
  },
  {
    "text": "to show it to a robot,\nwhat you want it to do and then it does it again. But it's mostly to show that\nthis unsupervised objective",
    "start": "4499810",
    "end": "4508630"
  },
  {
    "text": "seems to be working, right? It seems that it's going to\naccomplish certain goal images. However, as an interface, it's\nnot a great interface, right?",
    "start": "4508630",
    "end": "4515110"
  },
  {
    "text": "It might work in some cases. Maybe you want to take a\npicture of a clean room and you want to ask the\nrobot to always bring it",
    "start": "4515110",
    "end": "4520470"
  },
  {
    "text": "back to that state. But that doesn't\nmake sense maybe for tasks like\nthat where you need to move certain things\nto a certain other place",
    "start": "4520470",
    "end": "4527110"
  },
  {
    "text": "and then you have to\ndo it yourself first. ",
    "start": "4527110",
    "end": "4532690"
  },
  {
    "text": "All right, cool. So we talk a bit about\nhow it can reach goals, and it's not the best interface.",
    "start": "4532690",
    "end": "4538100"
  },
  {
    "text": "But we also before\ntalked about how it can be used as an\nunsupervised pre-training for downstream tasks, right?",
    "start": "4538100",
    "end": "4543590"
  },
  {
    "text": "We can kind of use\nit in any dataset, we'll just unsupervised\npre-train on everything and see how it does\nwhen we fine tune",
    "start": "4543590",
    "end": "4549770"
  },
  {
    "text": "into something we care about. All right, so we did\nthis in simulation first.",
    "start": "4549770",
    "end": "4555450"
  },
  {
    "text": "So here we did it in\ntwo different ways. We either use it\nas pre-training. So we pre-train for this\ngoal-conditioned Q-function",
    "start": "4555450",
    "end": "4561050"
  },
  {
    "text": "with actionable models or as an\nauxiliary objective, all right. So you'll be training both\nat the same time, the task",
    "start": "4561050",
    "end": "4567260"
  },
  {
    "text": "and action model. Or we can just\nnot use it at all. So for simulation tasks,\nthese are the results.",
    "start": "4567260",
    "end": "4574710"
  },
  {
    "text": "So we can see that by using\nthe pre-training mechanism or auxiliary objective,\nwe get to a higher--",
    "start": "4574710",
    "end": "4580340"
  },
  {
    "text": "not only to a higher reward, but\nwe also get there much faster. So it seems that this\nfunctional understanding of the world as we got\nfrom actionable models",
    "start": "4580340",
    "end": "4586820"
  },
  {
    "text": "actually helps for\ndownstream tasks, for downstream fine tuning. And then we also tried\nit on real world data,",
    "start": "4586820",
    "end": "4594295"
  },
  {
    "text": "where we have a\nvery small amount of data for a particular task. And for these tasks\nif we don't pre-train, we get very bad performance.",
    "start": "4594295",
    "end": "4601130"
  },
  {
    "text": "With pre-training, we can get\nto reasonable performance, and we can continue fine tuning\nto get better and better.",
    "start": "4601130",
    "end": "4606350"
  },
  {
    "text": " All right, so we talked about\noffline RL problem formulation,",
    "start": "4606350",
    "end": "4613450"
  },
  {
    "text": "why it's important. We talked about some\nsolutions to it. And then we used some\nof those insights to think a little bit\nabout how to share",
    "start": "4613450",
    "end": "4620710"
  },
  {
    "text": "data between different tasks in\nmultitask RL and then finally, how to do offline\ngoal-conditioned reinforcement",
    "start": "4620710",
    "end": "4626890"
  },
  {
    "text": "learning, which removes\na lot of the obstacles from reinforcement\nlearning pipeline and allows us to\nuse reinforcement",
    "start": "4626890",
    "end": "4632949"
  },
  {
    "text": "learning and big datasets\nwithout any rewards tasks and so on.",
    "start": "4632950",
    "end": "4638619"
  },
  {
    "text": "And for next time,\non Wednesday, we'll talk about long horizon\ntasks, how we can solve them with hierarchical reinforcement\nlearning, with skill discovery.",
    "start": "4638620",
    "end": "4646240"
  },
  {
    "text": "And as a reminder, homework\nfour is due next Monday. I'm happy to take any questions,\nbut thanks to all of you",
    "start": "4646240",
    "end": "4652270"
  },
  {
    "text": "for your attention. ",
    "start": "4652270",
    "end": "4659000"
  }
]