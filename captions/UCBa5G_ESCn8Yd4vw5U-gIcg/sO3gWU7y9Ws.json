[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "start": "0",
    "end": "3882"
  },
  {
    "text": "CHRISTOPHER POTTS:\nHello, everyone.",
    "start": "3882",
    "end": "5340"
  },
  {
    "text": "Welcome to part 6 in our\nseries on supervised sentiment",
    "start": "5340",
    "end": "7730"
  },
  {
    "text": "analysis.",
    "start": "7730",
    "end": "8330"
  },
  {
    "text": "This screencast is going to\ncover two important methods",
    "start": "8330",
    "end": "11000"
  },
  {
    "text": "in this space, hyperparameter\nsearch and classifier",
    "start": "11000",
    "end": "14030"
  },
  {
    "text": "comparisons.",
    "start": "14030",
    "end": "15900"
  },
  {
    "start": "15000",
    "end": "310000"
  },
  {
    "text": "So let's begin with\nhyperparameter search.",
    "start": "15900",
    "end": "17680"
  },
  {
    "text": "And first, I'll just\noffer the rationale.",
    "start": "17680",
    "end": "20390"
  },
  {
    "text": "Let's say that the\nparameters of a model",
    "start": "20390",
    "end": "22849"
  },
  {
    "text": "are those whose values\nare learned as part",
    "start": "22850",
    "end": "25160"
  },
  {
    "text": "of optimizing the model itself.",
    "start": "25160",
    "end": "26850"
  },
  {
    "text": "So for the classifiers\nwe've been studying,",
    "start": "26850",
    "end": "29000"
  },
  {
    "text": "the parameters are\nreally just the weights",
    "start": "29000",
    "end": "31640"
  },
  {
    "text": "that you learn on each of\nthe individual features.",
    "start": "31640",
    "end": "34675"
  },
  {
    "text": "And those are the things\nthat are directly targeted",
    "start": "34675",
    "end": "36800"
  },
  {
    "text": "by the optimization process.",
    "start": "36800",
    "end": "39052"
  },
  {
    "text": "The parameters of a model\nare typically pretty",
    "start": "39053",
    "end": "40970"
  },
  {
    "text": "crisply defined because\nthey kind of follow",
    "start": "40970",
    "end": "42980"
  },
  {
    "text": "from the structure\nmathematically of the model",
    "start": "42980",
    "end": "45680"
  },
  {
    "text": "under investigation.",
    "start": "45680",
    "end": "47450"
  },
  {
    "text": "Much more diffuse are\nthe hyperparameters.",
    "start": "47450",
    "end": "49730"
  },
  {
    "text": "We can say the\nhyperparameters of a model",
    "start": "49730",
    "end": "51649"
  },
  {
    "text": "are any settings that are\noutside of the optimization",
    "start": "51650",
    "end": "54890"
  },
  {
    "text": "process mentioned in 1.",
    "start": "54890",
    "end": "56690"
  },
  {
    "text": "So examples from models\nwe've seen, our GloVe and LSA",
    "start": "56690",
    "end": "59660"
  },
  {
    "text": "have that\ndimensionality setting.",
    "start": "59660",
    "end": "61850"
  },
  {
    "text": "The model itself\ngives you no guidance",
    "start": "61850",
    "end": "63530"
  },
  {
    "text": "about what to choose\nfor the dimensionality.",
    "start": "63530",
    "end": "66049"
  },
  {
    "text": "And the dimensionality\nis not selected",
    "start": "66050",
    "end": "68030"
  },
  {
    "text": "as part of the optimization\nof the model itself.",
    "start": "68030",
    "end": "70460"
  },
  {
    "text": "You have to choose it via\nsome external mechanism,",
    "start": "70460",
    "end": "73580"
  },
  {
    "text": "making it a hyperparameter.",
    "start": "73580",
    "end": "76382"
  },
  {
    "text": "And GloVe actually has two\nother additional prominent",
    "start": "76382",
    "end": "78590"
  },
  {
    "text": "hyperparameters, xmax and alpha.",
    "start": "78590",
    "end": "80880"
  },
  {
    "text": "Again, those are not\noptimized by the model.",
    "start": "80880",
    "end": "83360"
  },
  {
    "text": "You have to select them via\nsome external mechanism.",
    "start": "83360",
    "end": "86510"
  },
  {
    "text": "And for the classifiers\nthat we've been studying,",
    "start": "86510",
    "end": "88730"
  },
  {
    "text": "we have regularization terms.",
    "start": "88730",
    "end": "90380"
  },
  {
    "text": "Those are classic\nhyperparameters.",
    "start": "90380",
    "end": "92210"
  },
  {
    "text": "If you have a deep\nclassifier, then",
    "start": "92210",
    "end": "93830"
  },
  {
    "text": "the hidden dimensionalities\nin the model",
    "start": "93830",
    "end": "95990"
  },
  {
    "text": "could also be considered\nhyperparameters.",
    "start": "95990",
    "end": "98119"
  },
  {
    "text": "Learning rates, any core feature\nof the optimization method",
    "start": "98120",
    "end": "101750"
  },
  {
    "text": "itself could be considered\nhyperparameters.",
    "start": "101750",
    "end": "104420"
  },
  {
    "text": "And even things that\nmight be considered",
    "start": "104420",
    "end": "106280"
  },
  {
    "text": "kind of architectural,\nlike the activation",
    "start": "106280",
    "end": "109040"
  },
  {
    "text": "function and the\ndeep classifier,",
    "start": "109040",
    "end": "111020"
  },
  {
    "text": "you might think of it as kind of\nan intrinsic part of the model",
    "start": "111020",
    "end": "114201"
  },
  {
    "text": "that you're evaluating.",
    "start": "114202",
    "end": "115160"
  },
  {
    "text": "But since it's an easy choice\npoint for us at this point,",
    "start": "115160",
    "end": "118350"
  },
  {
    "text": "you'll be tempted to explore\na few different options",
    "start": "118350",
    "end": "120890"
  },
  {
    "text": "for that particular\narchitectural choice.",
    "start": "120890",
    "end": "123200"
  },
  {
    "text": "And in that way, it could\nbecome a hyperparameter.",
    "start": "123200",
    "end": "125960"
  },
  {
    "text": "And at this point, even\nthe optimization methods",
    "start": "125960",
    "end": "128959"
  },
  {
    "text": "could also emerge\nas a hyperparameter",
    "start": "128960",
    "end": "131270"
  },
  {
    "text": "that you would like\nto do search over.",
    "start": "131270",
    "end": "134060"
  },
  {
    "text": "And so forth and so\non, you should probably",
    "start": "134060",
    "end": "136459"
  },
  {
    "text": "take a fairly expansive view\nof what the hyperparameters",
    "start": "136460",
    "end": "139370"
  },
  {
    "text": "of your model are if you can.",
    "start": "139370",
    "end": "142159"
  },
  {
    "text": "Now here's the crux\nof the argument.",
    "start": "142160",
    "end": "144350"
  },
  {
    "text": "Hyperparameter optimization\nis crucial to building",
    "start": "144350",
    "end": "147170"
  },
  {
    "text": "a persuasive argument.",
    "start": "147170",
    "end": "148370"
  },
  {
    "text": "Fundamentally, for any\nkind of comparison we make,",
    "start": "148370",
    "end": "151489"
  },
  {
    "text": "we want to put every model\nin its very best light.",
    "start": "151490",
    "end": "155180"
  },
  {
    "text": "We could take it for granted\nthat for any sufficiently",
    "start": "155180",
    "end": "157549"
  },
  {
    "text": "complicated model there's some\nsetting of its hyperparameters",
    "start": "157550",
    "end": "160952"
  },
  {
    "text": "that's kind of\ndegenerate and would",
    "start": "160952",
    "end": "162409"
  },
  {
    "text": "make the model look very bad.",
    "start": "162410",
    "end": "164653"
  },
  {
    "text": "And so you certainly\nwouldn't want",
    "start": "164653",
    "end": "166070"
  },
  {
    "text": "to do any comparisons against\nthat really problematic set",
    "start": "166070",
    "end": "168920"
  },
  {
    "text": "of choices.",
    "start": "168920",
    "end": "170000"
  },
  {
    "text": "Rather, what we\nwant to do is say,",
    "start": "170000",
    "end": "171840"
  },
  {
    "text": "let's put all the models\nin their best light",
    "start": "171840",
    "end": "174140"
  },
  {
    "text": "by choosing optimal\nhyperparameters for them",
    "start": "174140",
    "end": "176390"
  },
  {
    "text": "to the best of our ability.",
    "start": "176390",
    "end": "177862"
  },
  {
    "text": "And then we can say that one\nmodel is better than the other",
    "start": "177862",
    "end": "180319"
  },
  {
    "text": "if it emerges victorious in\nthat very rigorous setting.",
    "start": "180320",
    "end": "185487"
  },
  {
    "text": "And the final thing I'll say\nabout this methodologically",
    "start": "185487",
    "end": "187819"
  },
  {
    "text": "is that, of course, all\nhyperparameter tuning",
    "start": "187820",
    "end": "190640"
  },
  {
    "text": "must be done only on train\nand development data.",
    "start": "190640",
    "end": "193760"
  },
  {
    "text": "You can consider that\nall fair game in terms",
    "start": "193760",
    "end": "197209"
  },
  {
    "text": "of using it however\nyou want to choose",
    "start": "197210",
    "end": "199250"
  },
  {
    "text": "the optimal hyperparameters.",
    "start": "199250",
    "end": "201080"
  },
  {
    "text": "But once that choice\nis set, it is fixed.",
    "start": "201080",
    "end": "203300"
  },
  {
    "text": "And those are the parameters\nthat you use at test time.",
    "start": "203300",
    "end": "205910"
  },
  {
    "text": "And that is the\nfundamental evaluation",
    "start": "205910",
    "end": "207920"
  },
  {
    "text": "that you would use for any\nkind of model comparison.",
    "start": "207920",
    "end": "210890"
  },
  {
    "text": "And at no point should you be\ntuning these hyperparameters",
    "start": "210890",
    "end": "214010"
  },
  {
    "text": "on the test data itself.",
    "start": "214010",
    "end": "215670"
  },
  {
    "text": "That would be\ncompletely illegitimate.",
    "start": "215670",
    "end": "219720"
  },
  {
    "text": "I hope we've made\nit really easy to do",
    "start": "219720",
    "end": "221513"
  },
  {
    "text": "this kind of\nhyperparameter search",
    "start": "221513",
    "end": "222930"
  },
  {
    "text": "in the context of\nthe work you're",
    "start": "222930",
    "end": "224305"
  },
  {
    "text": "doing for supervised\nsentiment analysis.",
    "start": "224305",
    "end": "226450"
  },
  {
    "text": "Here are some code snippets\nthat show how that can happen.",
    "start": "226450",
    "end": "229470"
  },
  {
    "text": "In loading my libraries,\nI have a pointer",
    "start": "229470",
    "end": "231810"
  },
  {
    "text": "to our sentiment data.",
    "start": "231810",
    "end": "233130"
  },
  {
    "text": "And here I have a fixed\nfeature function, which is just",
    "start": "233130",
    "end": "235500"
  },
  {
    "text": "a unigram feature function.",
    "start": "235500",
    "end": "237990"
  },
  {
    "text": "The change happens\ninside the model wrapper.",
    "start": "237990",
    "end": "241260"
  },
  {
    "text": "Whereas before,\nessentially, all we did",
    "start": "241260",
    "end": "243180"
  },
  {
    "text": "was set up a logistic\nregression model",
    "start": "243180",
    "end": "245099"
  },
  {
    "text": "and then call its fit method,\nhere we set up that model",
    "start": "245100",
    "end": "248730"
  },
  {
    "text": "but also established a\ngrid of hyperparameters.",
    "start": "248730",
    "end": "251830"
  },
  {
    "text": "These are different\nchoice points",
    "start": "251830",
    "end": "253230"
  },
  {
    "text": "for this logistic\nregression model",
    "start": "253230",
    "end": "254830"
  },
  {
    "text": "like whether or\nnot I have a bias",
    "start": "254830",
    "end": "256829"
  },
  {
    "text": "term, the value of the\nregularization parameter,",
    "start": "256829",
    "end": "260310"
  },
  {
    "text": "and even the algorithm\nused for regularization",
    "start": "260310",
    "end": "263040"
  },
  {
    "text": "itself, L1 or L2.",
    "start": "263040",
    "end": "265320"
  },
  {
    "text": "The model will explore the\nfull grid of these options.",
    "start": "265320",
    "end": "268170"
  },
  {
    "text": "It's going to do five-fold\ncross validation, so test one",
    "start": "268170",
    "end": "271080"
  },
  {
    "text": "each five times on different\nsplits of the data.",
    "start": "271080",
    "end": "274319"
  },
  {
    "text": "And in that very\nlong search process,",
    "start": "274320",
    "end": "276850"
  },
  {
    "text": "it will find what it takes\nto be the best setting of all",
    "start": "276850",
    "end": "279870"
  },
  {
    "text": "of these hyperparameters,\nof all the combinations",
    "start": "279870",
    "end": "282419"
  },
  {
    "text": "that can logically be set.",
    "start": "282420",
    "end": "285510"
  },
  {
    "text": "And that is the model that we\nfinally return here, right?",
    "start": "285510",
    "end": "288340"
  },
  {
    "text": "So now you can see the\nvalue of having a wrapper",
    "start": "288340",
    "end": "290340"
  },
  {
    "text": "around these fit methods.",
    "start": "290340",
    "end": "291540"
  },
  {
    "text": "Because then I could do\nall of this extra work",
    "start": "291540",
    "end": "294510"
  },
  {
    "text": "without changing the interface\nto sst.experiment at all.",
    "start": "294510",
    "end": "297720"
  },
  {
    "text": "The experiments look just as\nthey did in the previous mode.",
    "start": "297720",
    "end": "301230"
  },
  {
    "text": "It's just that they will\ntake a lot longer because you",
    "start": "301230",
    "end": "303540"
  },
  {
    "text": "are running dozens and\ndozens of experiments",
    "start": "303540",
    "end": "306000"
  },
  {
    "text": "as part of this\nexhaustive search of all",
    "start": "306000",
    "end": "308860"
  },
  {
    "text": "the possible settings.",
    "start": "308860",
    "end": "310930"
  },
  {
    "start": "310000",
    "end": "611000"
  },
  {
    "text": "OK, part 2 is\nclassifier comparison.",
    "start": "310930",
    "end": "313380"
  },
  {
    "text": "Once again, begin\nwith the rationale.",
    "start": "313380",
    "end": "315180"
  },
  {
    "text": "Suppose you've assessed\na baseline Model B",
    "start": "315180",
    "end": "317850"
  },
  {
    "text": "and your favorite model M. And\nyour chosen assessment metric",
    "start": "317850",
    "end": "321300"
  },
  {
    "text": "favors M. And this seems like\na little victory for you.",
    "start": "321300",
    "end": "324849"
  },
  {
    "text": "But you should still ask\nyourself, is M really better,",
    "start": "324850",
    "end": "327270"
  },
  {
    "text": "right?",
    "start": "327270",
    "end": "328560"
  },
  {
    "text": "Now if the difference\nbetween B and M",
    "start": "328560",
    "end": "330780"
  },
  {
    "text": "is clearly of\npractical significance,",
    "start": "330780",
    "end": "332887"
  },
  {
    "text": "then you might not need to\ndo anything beyond presenting",
    "start": "332887",
    "end": "335220"
  },
  {
    "text": "the numbers, right?",
    "start": "335220",
    "end": "336450"
  },
  {
    "text": "If each one of your\nclassification decisions",
    "start": "336450",
    "end": "338820"
  },
  {
    "text": "corresponds to something\nreally important in the world",
    "start": "338820",
    "end": "341520"
  },
  {
    "text": "and your classifier\nmakes thousands",
    "start": "341520",
    "end": "343410"
  },
  {
    "text": "more good predictions\nthan the other model,",
    "start": "343410",
    "end": "345570"
  },
  {
    "text": "that might be enough\nfor the argument.",
    "start": "345570",
    "end": "348030"
  },
  {
    "text": "But even in that\nsituation, you might",
    "start": "348030",
    "end": "349950"
  },
  {
    "text": "ask whether there's variation\nin how these two models",
    "start": "349950",
    "end": "352540"
  },
  {
    "text": "B and M perform.",
    "start": "352540",
    "end": "353520"
  },
  {
    "text": "Did you just get\nlucky when you saw",
    "start": "353520",
    "end": "355020"
  },
  {
    "text": "what looked like a\npractical difference",
    "start": "355020",
    "end": "356645"
  },
  {
    "text": "and with minor changes to the\ninitialization or something,",
    "start": "356645",
    "end": "359669"
  },
  {
    "text": "you would see very\ndifferent outcomes?",
    "start": "359670",
    "end": "361650"
  },
  {
    "text": "If the answer is\npossibly yes, then you",
    "start": "361650",
    "end": "363780"
  },
  {
    "text": "might still want to do some\nkind of classifier comparison.",
    "start": "363780",
    "end": "368280"
  },
  {
    "text": "Now there's this nice\npaper by Demsar 2006",
    "start": "368280",
    "end": "371250"
  },
  {
    "text": "that advises using the Wilcoxon\nsigned-rank test for situations",
    "start": "371250",
    "end": "374910"
  },
  {
    "text": "in which you can afford\nto repeatedly assess",
    "start": "374910",
    "end": "377850"
  },
  {
    "text": "your two models B and M on\ndifferent train test splits,",
    "start": "377850",
    "end": "380835"
  },
  {
    "text": "right?",
    "start": "380835",
    "end": "382050"
  },
  {
    "text": "And we'll talk later in the\nterm about the precise rationale",
    "start": "382050",
    "end": "384879"
  },
  {
    "text": "for this.",
    "start": "384880",
    "end": "385380"
  },
  {
    "text": "But the idea is just that you\nwould do a lot of experiments",
    "start": "385380",
    "end": "388470"
  },
  {
    "text": "on slightly different\nviews of your data",
    "start": "388470",
    "end": "390750"
  },
  {
    "text": "and then kind of\naverage across them",
    "start": "390750",
    "end": "392700"
  },
  {
    "text": "to get a sense for how the two\nmodels compare with each other.",
    "start": "392700",
    "end": "397920"
  },
  {
    "text": "In situations where you can't\nrepeatedly assess B and M,",
    "start": "397920",
    "end": "402225"
  },
  {
    "text": "McNemar's test is a\nreasonable alternative.",
    "start": "402225",
    "end": "405000"
  },
  {
    "text": "It operates on the confusion\nmatrices produced by the two",
    "start": "405000",
    "end": "407820"
  },
  {
    "text": "models, testing the null\nhypothesis that the two models",
    "start": "407820",
    "end": "410820"
  },
  {
    "text": "have the same error rate.",
    "start": "410820",
    "end": "413220"
  },
  {
    "text": "The reason you might\nopt for McNemar's test",
    "start": "413220",
    "end": "415250"
  },
  {
    "text": "is, for example, if you're\ndoing a deep learning",
    "start": "415250",
    "end": "417360"
  },
  {
    "text": "experiment where all the models\ntake a few weeks to optimize.",
    "start": "417360",
    "end": "420990"
  },
  {
    "text": "Then of course,\nyou can't probably",
    "start": "420990",
    "end": "422430"
  },
  {
    "text": "afford to do dozens and dozens\nof experiments with each one.",
    "start": "422430",
    "end": "426280"
  },
  {
    "text": "So you might be compelled to\nuse McNemar's based on one",
    "start": "426280",
    "end": "429030"
  },
  {
    "text": "single run of the two models.",
    "start": "429030",
    "end": "430830"
  },
  {
    "text": "It's a much weaker argument\nbecause, of course,",
    "start": "430830",
    "end": "432960"
  },
  {
    "text": "precisely the point is\nthat we might see variation",
    "start": "432960",
    "end": "435630"
  },
  {
    "text": "across different runs.",
    "start": "435630",
    "end": "436650"
  },
  {
    "text": "And McNemar's is\nnot really going",
    "start": "436650",
    "end": "438180"
  },
  {
    "text": "to grapple with that in\nthe way that the Wilcoxon",
    "start": "438180",
    "end": "440759"
  },
  {
    "text": "signed-rank test will.",
    "start": "440760",
    "end": "442650"
  },
  {
    "text": "But this is arguably better\nthan nothing in most situations.",
    "start": "442650",
    "end": "445235"
  },
  {
    "text": "So you might default to\nMcNemar's if the Wilcoxon",
    "start": "445235",
    "end": "448710"
  },
  {
    "text": "is too expensive.",
    "start": "448710",
    "end": "450815"
  },
  {
    "text": "And let me just\nshow you how easy",
    "start": "450815",
    "end": "452190"
  },
  {
    "text": "this can be in the\ncontext of our code base.",
    "start": "452190",
    "end": "454230"
  },
  {
    "text": "So by way of illustration, what\nwe're essentially going to do",
    "start": "454230",
    "end": "457800"
  },
  {
    "text": "is compare logistic\nregression and naive Bayes.",
    "start": "457800",
    "end": "461729"
  },
  {
    "text": "I encourage you, when you're\ndoing these comparisons,",
    "start": "461730",
    "end": "464620"
  },
  {
    "text": "to have only one\npoint of variation.",
    "start": "464620",
    "end": "467199"
  },
  {
    "text": "So we're going to fix\nthe data and we're",
    "start": "467200",
    "end": "468960"
  },
  {
    "text": "going to fix the\nfeature function",
    "start": "468960",
    "end": "470729"
  },
  {
    "text": "and compare only the\nmodel architectures.",
    "start": "470730",
    "end": "473790"
  },
  {
    "text": "You could separately\nsay I'm going",
    "start": "473790",
    "end": "475380"
  },
  {
    "text": "to have a single fixed model\nlike logistic regression",
    "start": "475380",
    "end": "478170"
  },
  {
    "text": "and explore a few different\nfeature functions.",
    "start": "478170",
    "end": "481050"
  },
  {
    "text": "But I would advise against\nexploring two different feature",
    "start": "481050",
    "end": "483870"
  },
  {
    "text": "functions as combined\nwith two different models",
    "start": "483870",
    "end": "486030"
  },
  {
    "text": "because when you observe\ndifferences in the end,",
    "start": "486030",
    "end": "488530"
  },
  {
    "text": "you won't be sure whether that\nwas caused by the model choice",
    "start": "488530",
    "end": "491880"
  },
  {
    "text": "or by the feature functions.",
    "start": "491880",
    "end": "493360"
  },
  {
    "text": "We want to isolate these things\nand do systematic comparisons.",
    "start": "493360",
    "end": "498219"
  },
  {
    "text": "So here, I'm going to do\na systematic comparison",
    "start": "498220",
    "end": "500220"
  },
  {
    "text": "of logistic regression\nand naive Bayes",
    "start": "500220",
    "end": "502710"
  },
  {
    "text": "on the SST using\nthe Wilcoxon test.",
    "start": "502710",
    "end": "505650"
  },
  {
    "text": "And here's the setup.",
    "start": "505650",
    "end": "506880"
  },
  {
    "text": "The function is\nsst_compare_models.",
    "start": "506880",
    "end": "509620"
  },
  {
    "text": "I point it to my training data.",
    "start": "509620",
    "end": "511992"
  },
  {
    "text": "You can have two\nfeature functions.",
    "start": "511992",
    "end": "513450"
  },
  {
    "text": "But in that case, you should\nhave just one model wrapper.",
    "start": "513450",
    "end": "516330"
  },
  {
    "text": "Here, I've got one feature\nfunction used for both models.",
    "start": "516330",
    "end": "518705"
  },
  {
    "text": "And I'll have these\ntwo different wrappers",
    "start": "518705",
    "end": "520455"
  },
  {
    "text": "corresponding to the\nevaluation that I",
    "start": "520455",
    "end": "522120"
  },
  {
    "text": "want to do of those\ntwo model classes.",
    "start": "522120",
    "end": "525300"
  },
  {
    "text": "I'm going to use the\nWilcoxon as advised.",
    "start": "525300",
    "end": "527760"
  },
  {
    "text": "I'll do 10 trials of each on the\ntrain size of 70% of the data.",
    "start": "527760",
    "end": "532770"
  },
  {
    "text": "And as always, in\nthis setting, I'll",
    "start": "532770",
    "end": "534660"
  },
  {
    "text": "use the macro F1 as my score.",
    "start": "534660",
    "end": "537480"
  },
  {
    "text": "So what this will\ndo internally is",
    "start": "537480",
    "end": "539010"
  },
  {
    "text": "run 10 experiments on\ndifferent train test",
    "start": "539010",
    "end": "542500"
  },
  {
    "text": "splits for each one\nof these models.",
    "start": "542500",
    "end": "544440"
  },
  {
    "text": "That gives us a score vector,\n10 numbers for each model.",
    "start": "544440",
    "end": "549300"
  },
  {
    "text": "And then what the\nWilcoxon is doing",
    "start": "549300",
    "end": "550890"
  },
  {
    "text": "is comparing whether\nthe-- or assessing",
    "start": "550890",
    "end": "552930"
  },
  {
    "text": "whether the means of\nthose two score vectors",
    "start": "552930",
    "end": "555420"
  },
  {
    "text": "are statistically\nsignificantly different.",
    "start": "555420",
    "end": "558326"
  },
  {
    "text": "And here it looks like\nwe have some evidence",
    "start": "558327",
    "end": "560160"
  },
  {
    "text": "that we can reject the null\nhypothesis that these models",
    "start": "560160",
    "end": "563100"
  },
  {
    "text": "are identical, which is\npresumably the argument that we",
    "start": "563100",
    "end": "566220"
  },
  {
    "text": "were trying to build.",
    "start": "566220",
    "end": "568259"
  },
  {
    "text": "Now, of course,\nthat's very expensive",
    "start": "568260",
    "end": "570030"
  },
  {
    "text": "because we had to run 20\nexperiments in this situation.",
    "start": "570030",
    "end": "572797"
  },
  {
    "text": "And of course, you\ncould run many more",
    "start": "572797",
    "end": "574380"
  },
  {
    "text": "if you were also doing\nhyperparameter tuning as part",
    "start": "574380",
    "end": "577590"
  },
  {
    "text": "of your experimental workflow.",
    "start": "577590",
    "end": "579810"
  },
  {
    "text": "So in situations where you can't\nafford to do something that",
    "start": "579810",
    "end": "582690"
  },
  {
    "text": "involves so many\nexperiments, as I said,",
    "start": "582690",
    "end": "584700"
  },
  {
    "text": "you couldn't default\nto McNemar's.",
    "start": "584700",
    "end": "587310"
  },
  {
    "text": "That is included\nin utils.mcnemar.",
    "start": "587310",
    "end": "590550"
  },
  {
    "text": "And the return values\nof SST experiment",
    "start": "590550",
    "end": "593399"
  },
  {
    "text": "will give you all the\ninformation you need.",
    "start": "593400",
    "end": "595200"
  },
  {
    "text": "Essentially from McNemar's you\nneed the actual goal vector",
    "start": "595200",
    "end": "598520"
  },
  {
    "text": "of labels and then\nthe two vectors",
    "start": "598520",
    "end": "600570"
  },
  {
    "text": "of predictions for each\none of your experiments.",
    "start": "600570",
    "end": "602700"
  },
  {
    "text": "So that's a simple\nalternative in the situation",
    "start": "602700",
    "end": "604860"
  },
  {
    "text": "in which Wilcoxon's\nwas just too expensive.",
    "start": "604860",
    "end": "608329"
  },
  {
    "start": "608330",
    "end": "612000"
  }
]