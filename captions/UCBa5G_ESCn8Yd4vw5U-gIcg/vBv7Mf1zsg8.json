[
  {
    "start": "0",
    "end": "6410"
  },
  {
    "text": "All right. OK. So I thought we could finish\nthe lecture from last time",
    "start": "6410",
    "end": "14599"
  },
  {
    "text": "and keep talking about\ndiffusion models and some another lecture ready on\ntraining latent variable",
    "start": "14600",
    "end": "22208"
  },
  {
    "text": "models with discrete\nvariables, but I thought we didn't finish this. And there was quite\na bit of interest",
    "start": "22208",
    "end": "27250"
  },
  {
    "text": "that we can go through\nthe remaining slides and really see the\nconnections with all",
    "start": "27250",
    "end": "32330"
  },
  {
    "text": "these efficient sampling\nstrategies and all that good stuff. So as a reminder, we've seen\nthat we can think of-- there is",
    "start": "32330",
    "end": "42740"
  },
  {
    "text": "this close connection between\nscore-based models and denoising diffusion--",
    "start": "42740",
    "end": "47989"
  },
  {
    "text": "DDPMs, denoising diffusion\nprobabilistic models. The basic idea is that you can\nthink of score-based models",
    "start": "47990",
    "end": "58579"
  },
  {
    "text": "as basically trying to go from\nnoise to data by essentially",
    "start": "58580",
    "end": "65720"
  },
  {
    "text": "running these Langevin\ndynamics chains. And alternatively, we can\nthink about a process that",
    "start": "65720",
    "end": "71490"
  },
  {
    "text": "does something very similar\nfrom the perspective of a variational autoencoder. So there is a process\nthat basically adds noise",
    "start": "71490",
    "end": "81290"
  },
  {
    "text": "to the data, which you can\nthink of it as an encoder. And all these transitions\nhere, q of xt given xt minus 1,",
    "start": "81290",
    "end": "90320"
  },
  {
    "text": "this is just a Gaussian, which\nis centered at xt minus 1 and you just add a little\nbit of noise to get xt.",
    "start": "90320",
    "end": "97400"
  },
  {
    "text": "And so every step you add\na little bit of noise, and then eventually\nafter many steps,",
    "start": "97400",
    "end": "104689"
  },
  {
    "text": "you added so much\nnoise to the data that all the structure is lost\nand you're left with pure noise",
    "start": "104690",
    "end": "109970"
  },
  {
    "text": "at the end of this chain. And as in a regular\nVAE, there is a decoder,",
    "start": "109970",
    "end": "118920"
  },
  {
    "text": "which is a joint distribution\nover the same random variables. And we parameterize it\nin the reverse direction,",
    "start": "118920",
    "end": "126479"
  },
  {
    "text": "so we go from noise\nto clean data. And we have this\nsequence of decoders,",
    "start": "126480",
    "end": "133310"
  },
  {
    "text": "and the decoders are\nbasically this p theta of xt minus 1 given xt.",
    "start": "133310",
    "end": "138950"
  },
  {
    "text": "And so given xt, you\ntry to guess what is the value of xt minus 1.",
    "start": "138950",
    "end": "144600"
  },
  {
    "text": "And these decoders are also-- and the DDPM formulation\nare also simple in the sense",
    "start": "144600",
    "end": "150360"
  },
  {
    "text": "that they are Gaussian\ndistributions, and the parameters of these\nGaussian distributions are computed using\nneural networks",
    "start": "150360",
    "end": "157409"
  },
  {
    "text": "just like in a regular VAE. And what we're\nseeing is that we can",
    "start": "157410",
    "end": "163590"
  },
  {
    "text": "train these models\nthe usual way, which is by optimizing an\nevidence lower bound, which essentially tries\nto minimize the KL",
    "start": "163590",
    "end": "171990"
  },
  {
    "text": "divergence between the\ndistribution defined by the decoder and the\ndistribution defined",
    "start": "171990",
    "end": "178260"
  },
  {
    "text": "by the encoder. It's trying to match those\ntwo joint distributions. And if you look at the ELBO\nobjective, it looks like this.",
    "start": "178260",
    "end": "188620"
  },
  {
    "text": "And it turns out that if\nyou do a little bit of math, this objective ends\nup being exactly",
    "start": "188620",
    "end": "195900"
  },
  {
    "text": "the denoising\nscore-matching objective. So in order to-- ",
    "start": "195900",
    "end": "202730"
  },
  {
    "text": "essentially, if you want\nto learn the best way, the best decoder, the best way\nof guessing xt minus 1 given x",
    "start": "202730",
    "end": "209510"
  },
  {
    "text": "t, essentially what you have\nto do is you have to learn the score of the noise-perturbed\ndata density which we know",
    "start": "209510",
    "end": "219500"
  },
  {
    "text": "corresponds to-- can be done by solving\na denoising problem. And so essentially\noptimizing the ELBO",
    "start": "219500",
    "end": "227600"
  },
  {
    "text": "corresponds to learning\na sequence of denoisers. The same as the\nnoise conditional",
    "start": "227600",
    "end": "234920"
  },
  {
    "text": "score models, essentially. And that's the\nmain thing here is",
    "start": "234920",
    "end": "243530"
  },
  {
    "text": "that we can interpret\nthe whole thing as a variational autoencoder,\nminimizing the ELBO corresponds to essentially\na sum of denoising",
    "start": "243530",
    "end": "252590"
  },
  {
    "text": "score-matching\nobjectives, each one corresponding to a\ndifferent noise level that we have in this chain.",
    "start": "252590",
    "end": "258440"
  },
  {
    "text": "And so there is this very\nkind of the resulting training",
    "start": "258440",
    "end": "264840"
  },
  {
    "text": "and inference procedure\nin a denoising diffusion probabilistic model\nis very, very similar",
    "start": "264840",
    "end": "270300"
  },
  {
    "text": "to the one in a\nscore-based model. During training time,\nyou are essentially",
    "start": "270300",
    "end": "276630"
  },
  {
    "text": "learning a sequence\nof denoisers, one for every time step. And once you have the denoisers\nto generate samples, what you do",
    "start": "276630",
    "end": "286770"
  },
  {
    "text": "is you just use the\ndecoders just like you would do in a normal VAE, and\nbecause basically the means",
    "start": "286770",
    "end": "294599"
  },
  {
    "text": "of these Gaussians\nthat are defined in the decoders at optimality\nessentially correspond",
    "start": "294600",
    "end": "301290"
  },
  {
    "text": "to the score functions. The updates that you do end\nup looking very, very similar",
    "start": "301290",
    "end": "307020"
  },
  {
    "text": "to the ones you would do in\na annealed Langevin dynamics procedure where at every\nstep you would essentially",
    "start": "307020",
    "end": "314400"
  },
  {
    "text": "follow the score. And you add a little bit\nof noise at every step because the decoders\nare Gaussians.",
    "start": "314400",
    "end": "321430"
  },
  {
    "text": "And so in order to\nsample from a Gaussian, you would compute their\nmean, and then you would add a little bit\nof noise to that vector.",
    "start": "321430",
    "end": "328710"
  },
  {
    "text": "And so very similar\nto the procedure that we will do in\na Langevin dynamics",
    "start": "328710",
    "end": "335000"
  },
  {
    "text": "where again, you would\nfollow the gradient and you would add a\nlittle bit of noise.",
    "start": "335000",
    "end": "340080"
  },
  {
    "text": "And, yeah, we've seen the\narchitectures are also very similar. But where we stopped\nthe last time",
    "start": "340080",
    "end": "346020"
  },
  {
    "text": "was to think about the diffusion\nversion of this, which is really",
    "start": "346020",
    "end": "351330"
  },
  {
    "text": "the case when we have an\ninfinite number of noise levels. So instead of having, let's\nsay, 1,000 different versions",
    "start": "351330",
    "end": "357840"
  },
  {
    "text": "of the data density that has\nbeen perturbed with increasingly large amounts of noise, we\ncan consider this continuum,",
    "start": "357840",
    "end": "366330"
  },
  {
    "text": "this spectrum of\ndistributions that are now indexed by this\nvariable t, which",
    "start": "366330",
    "end": "372330"
  },
  {
    "text": "you can think of it as time\nthat goes from 0 to 30. And so just like\nbefore, on the one end,",
    "start": "372330",
    "end": "379210"
  },
  {
    "text": "we have the clean\ndata distribution. At the other end, we have a\npure noise kind of distribution,",
    "start": "379210",
    "end": "385770"
  },
  {
    "text": "but now we have a continuum. And this continuum\nis actually going to be useful because it\nexposes additional structure",
    "start": "385770",
    "end": "394140"
  },
  {
    "text": "in the model that we can\ntake advantage for coming up with more efficient samplers,\nfor evaluating likelihoods",
    "start": "394140",
    "end": "401030"
  },
  {
    "text": "exactly, and so forth. So you can think of the\nvariational VAE perspective",
    "start": "401030",
    "end": "412290"
  },
  {
    "text": "that we talked about so far\nas some kind of discretization of this continuum version of the\nprocess where we only look at,",
    "start": "412290",
    "end": "421860"
  },
  {
    "text": "let's say, 1,000 different\nslices in this sequence.",
    "start": "421860",
    "end": "427020"
  },
  {
    "text": "But it makes sense to think\nabout the continuous version",
    "start": "427020",
    "end": "435509"
  },
  {
    "text": "because, as we'll see, it allows\nus to do more, essentially.",
    "start": "435510",
    "end": "441810"
  },
  {
    "text": "And so once we go in\nthe continuous version,",
    "start": "441810",
    "end": "447960"
  },
  {
    "text": "again, basically, there\nis a stochastic process that describes this process of\ngoing from data to noise, where",
    "start": "447960",
    "end": "455610"
  },
  {
    "text": "at every step you add\na little bit of noise, just like in the previous\ncase, except that we",
    "start": "455610",
    "end": "462479"
  },
  {
    "text": "get this continuous time process\nby thinking about what happens",
    "start": "462480",
    "end": "467520"
  },
  {
    "text": "if you were to take increasingly\nsmall discretization steps in the\nprevious perspective.",
    "start": "467520",
    "end": "474000"
  },
  {
    "text": "And so before we were jumping. We were taking 1,000\ndifferent steps, adding more and more and more\nnoise until we get to the end.",
    "start": "474000",
    "end": "483270"
  },
  {
    "text": "You can imagine a\ncontinuous process that goes from left to\nright, where at every step,",
    "start": "483270",
    "end": "489220"
  },
  {
    "text": "we add an infinitesimally\nsmall amount of noise. But, of course, over time, if\nyou integrate all of this noise,",
    "start": "489220",
    "end": "495990"
  },
  {
    "text": "you get the same effect,\nbasically destroying the entire structure\nin the data. ",
    "start": "495990",
    "end": "503910"
  },
  {
    "text": "So, formally, what\nwe're dealing with here is a stochastic process\nwhere we have a collection",
    "start": "503910",
    "end": "509400"
  },
  {
    "text": "of random variables. And now this collection\nof random variables we have an infinite number\nof random variables.",
    "start": "509400",
    "end": "516059"
  },
  {
    "text": "Before, we had, let's say, 1000. You had a VAE with maybe\n1,000 different layers.",
    "start": "516059",
    "end": "522639"
  },
  {
    "text": "And so you had 1,000 different\nrandom variables, one for every discrete time step.",
    "start": "522640",
    "end": "527715"
  },
  {
    "text": "Now we have an infinite\nnumber of random variables. There is one for every\nt and t is continuous.",
    "start": "527715",
    "end": "532960"
  },
  {
    "text": "So you can take an infinite\nnumber of values between 0 and capital T. And these random\nvariables have densities.",
    "start": "532960",
    "end": "542370"
  },
  {
    "text": "Just like in the\nVAE perspective, there is a density\nprobability density function associated with each\none of these random variables.",
    "start": "542370",
    "end": "550240"
  },
  {
    "text": "And it turns out\nthat we can describe how these random\nvariables are related to each other through a\nstochastic differential",
    "start": "550240",
    "end": "556150"
  },
  {
    "text": "equation, which you can think of\nit as a way that would allow you to sample values for\nthese random variables.",
    "start": "556150",
    "end": "562163"
  },
  {
    "text": "It would take a whole\nquarter to explain exactly what that\nnotation mean and what the stochastic\ndifferential equation is,",
    "start": "562163",
    "end": "567230"
  },
  {
    "text": "but essentially, you can\nimagine that this is really what happens if you take\nthe previous VAE perspective",
    "start": "567230",
    "end": "575590"
  },
  {
    "text": "and you make the intervals, the\ntime steps between one slice",
    "start": "575590",
    "end": "581050"
  },
  {
    "text": "and the next one\nvery, very small. So dxt is basically the\ndifference between xt and xt",
    "start": "581050",
    "end": "590050"
  },
  {
    "text": "plus delta and a\nneighboring slice. And the difference between\nthese two random variables",
    "start": "590050",
    "end": "596620"
  },
  {
    "text": "is given by some deterministic\nvalue, which is just like the drift it's called,\nplus a little bit of noise,",
    "start": "596620",
    "end": "604120"
  },
  {
    "text": "an infinitesimal\namount of noise. And for simplicity here\nwe can think about--",
    "start": "604120",
    "end": "610740"
  },
  {
    "text": "if you think about the\nprocess of just adding noise, you can describe it with a very\nsimple stochastic differential",
    "start": "610740",
    "end": "616620"
  },
  {
    "text": "equation where the\ndifference between the value of the random variable\nat time t and the value",
    "start": "616620",
    "end": "622770"
  },
  {
    "text": "of the random variable at time\nt plus epsilon or t plus delta t is just an infinitesimally small\namount of noise, which is what",
    "start": "622770",
    "end": "630990"
  },
  {
    "text": "this equation really means. Yeah. So the drift is\nbasically telling you",
    "start": "630990",
    "end": "638880"
  },
  {
    "text": "how you should\nchange, basically. You can imagine that there\nis some kind of velocity",
    "start": "638880",
    "end": "644319"
  },
  {
    "text": "like you think\nabout the dynamics, like if you think\nabout how xt evolves.",
    "start": "644320",
    "end": "651519"
  },
  {
    "text": "So xt is, let's say, an image. And as you increase time, the\nvalue of the pixels change.",
    "start": "651520",
    "end": "658620"
  },
  {
    "text": "And if you don't have the\ndrift, then the change is entirely driven by noise,\nwhich is what we're doing here.",
    "start": "658620",
    "end": "667529"
  },
  {
    "text": "What we will see is that when we\nreverse the direction of time, then it becomes very\nimportant to actually take into account the\ndrift because we want",
    "start": "667530",
    "end": "675029"
  },
  {
    "text": "to have some velocity\nfield that is pushing the images towards\nthe directions",
    "start": "675030",
    "end": "681000"
  },
  {
    "text": "where we know there is\na high probability mass. And so it's going to be\nimportant to have a drift",
    "start": "681000",
    "end": "687600"
  },
  {
    "text": "because if you think about-- if you flip the\ndirection of time and you want to go\nfrom noise to data,",
    "start": "687600",
    "end": "695490"
  },
  {
    "text": "it's not a purely\nrandom process. You have to change the values of\nthe pixels in a very structured",
    "start": "695490",
    "end": "701339"
  },
  {
    "text": "way to generate at the other\nend, something that is indeed",
    "start": "701340",
    "end": "706380"
  },
  {
    "text": "looking like an image. And so if you see at this\nend, all the probability mass is spread out according\nto a Gaussian.",
    "start": "706380",
    "end": "713980"
  },
  {
    "text": "But if you want to\nget to something that looks like this where all\nthe probability mass is here and here, then you have\nto have some velocity that",
    "start": "713980",
    "end": "721350"
  },
  {
    "text": "is pushing the particles, that\nis pushing this trajectory",
    "start": "721350",
    "end": "726449"
  },
  {
    "text": "to go either here or here,\nessentially because you want to have the\nright probability mass at the other end.",
    "start": "726450",
    "end": "731990"
  },
  {
    "text": "This is a special case of\nthis where the drift is 0. And it turns out this kind of\nstochastic differential equation",
    "start": "731990",
    "end": "739210"
  },
  {
    "text": "is the one that captures this\nrelatively simple behavior of just adding\nnoise to the data.",
    "start": "739210",
    "end": "745030"
  },
  {
    "text": "More generally, you\ncould have a drift, and we'll see that we\nneed the drift to talk about the reverse process.",
    "start": "745030",
    "end": "751300"
  },
  {
    "text": "But what I'm saying here\nis that this process of adding noise\nto the data, which is just a very fine\ndiscretization of what happens",
    "start": "751300",
    "end": "759730"
  },
  {
    "text": "in the previous VAE\ncan be described by this simple stochastic\ndifferential equation where",
    "start": "759730",
    "end": "766330"
  },
  {
    "text": "the dxt is the difference in the\nvalue of the random variables take at a very\nsmall time increment",
    "start": "766330",
    "end": "772930"
  },
  {
    "text": "is just an infinitesimally small\namount of noise that you get. So I guess if you\nadd Gaussian noise",
    "start": "772930",
    "end": "778900"
  },
  {
    "text": "at the level of the densities,\nyou are essentially convolving. So there is an\nimplicit convolution",
    "start": "778900",
    "end": "785362"
  },
  {
    "text": "that is happening here. So if you think about the\nshape of the densities, like you have a density\nhere and then you get that--",
    "start": "785362",
    "end": "792040"
  },
  {
    "text": "if you take one of\nthe slices here, you get a different\ndensity, which is actually the previous density convolved\nwith a Gaussian kernel",
    "start": "792040",
    "end": "799720"
  },
  {
    "text": "because that's what\nhappened if you sum up two independent random variables.",
    "start": "799720",
    "end": "805180"
  },
  {
    "text": "So there is an implicit\nconvolution happening here at the level of the densities. ",
    "start": "805180",
    "end": "811410"
  },
  {
    "text": "Cool now, the reason\nthis is interesting is that we can think about just\nchanging the direction of time.",
    "start": "811410",
    "end": "819690"
  },
  {
    "text": "So we can think\nabout the process as going from right to\nleft in the previous slide.",
    "start": "819690",
    "end": "826690"
  },
  {
    "text": "So going from noise to data. And so again, we have\nthese trajectories",
    "start": "826690",
    "end": "832700"
  },
  {
    "text": "which are samples from\nthis stochastic process. So these are realizations of\nthese random variables that",
    "start": "832700",
    "end": "839390"
  },
  {
    "text": "are consistent with the\nunderlying stochastic differential equation. And if you could somehow sample\nfrom this stochastic process,",
    "start": "839390",
    "end": "847700"
  },
  {
    "text": "then you would be\nable to generate data by basically just discarding\neverything and just looking at the final endpoint\nof this trajectory.",
    "start": "847700",
    "end": "856850"
  },
  {
    "text": "And the beauty is that\nif you essentially",
    "start": "856850",
    "end": "862940"
  },
  {
    "text": "do a change of variables-- and it's a little\nbit more complicated because this is\nstochastic, but essentially",
    "start": "862940",
    "end": "868879"
  },
  {
    "text": "if you apply a change of\nvariables and you swap, you replace t with\ncapital T minus t prime,",
    "start": "868880",
    "end": "874980"
  },
  {
    "text": "so you just literally flip\nthe direction of the time axis and you can obtain a new\nstochastic differential",
    "start": "874980",
    "end": "884420"
  },
  {
    "text": "equation that describes\nexactly the reverse process.",
    "start": "884420",
    "end": "889800"
  },
  {
    "text": "And the interesting bit is that\nthis stochastic differential equation now has a drift term,\nwhich is this red term here,",
    "start": "889800",
    "end": "898080"
  },
  {
    "text": "which the score of\nthe corresponding",
    "start": "898080",
    "end": "903900"
  },
  {
    "text": "perturbed data\ndensity at time t. So this is the exact\nreverse of this SDE.",
    "start": "903900",
    "end": "912230"
  },
  {
    "text": "The forward SDE doesn't\nhave the drift term. If it had the drift\nterm, then you would have to account for\nit in the reverse SDE.",
    "start": "912230",
    "end": "918720"
  },
  {
    "text": "You would have to\nbasically flip it. But we don't have to\nbecause this is for now--",
    "start": "918720",
    "end": "924635"
  },
  {
    "text": "but you could include\nit if you wanted. This is the simplest case\nwhere I don't have the drift. If you had it, you\ncould include it.",
    "start": "924635",
    "end": "931079"
  },
  {
    "text": "So in the forward process,\nthere is no drift. It's purely driven by noise.",
    "start": "931080",
    "end": "937220"
  },
  {
    "text": "So you can think\nof it literally-- it's like a random walk. So at every step,\nlet's say if it",
    "start": "937220",
    "end": "943850"
  },
  {
    "text": "was a one-dimensional\nrandom walk, you can go either left or\nright with some probability.",
    "start": "943850",
    "end": "948930"
  },
  {
    "text": "And then after a sufficiently\nlarge amount of time, you forget about the\ninitial condition and you get an\nunknown distribution.",
    "start": "948930",
    "end": "955940"
  },
  {
    "text": "This is like the\ncontinuous time version of that where at every step\nyou move by a little bit.",
    "start": "955940",
    "end": "961940"
  },
  {
    "text": "And the amount you\nmove is basically this dw, which is the amount\nthat you move towards the left",
    "start": "961940",
    "end": "969800"
  },
  {
    "text": "or towards the\nright, essentially. But it's essentially\na random walk. And it turns out that\nyou can reverse it.",
    "start": "969800",
    "end": "978290"
  },
  {
    "text": "And that there is\na way to describe this reverse random walk where\nyou go from noise to data",
    "start": "978290",
    "end": "985710"
  },
  {
    "text": "and he can be captured exactly\nif you knew the score function. So they're describing\nexactly the same thing.",
    "start": "985710",
    "end": "992140"
  },
  {
    "text": "So both of these SDEs describe\nthese kinds of trajectories.",
    "start": "992140",
    "end": "997680"
  },
  {
    "text": "The only thing that\nshould happen-- the only thing that\nhas happened here is that we're changing\nthe direction of time.",
    "start": "997680",
    "end": "1003920"
  },
  {
    "text": "So if you flip the direction\nof time and you go--",
    "start": "1003920",
    "end": "1011410"
  },
  {
    "text": "if you start from noise\nand you solve this SDE, you get exactly the\nsame traces that you",
    "start": "1011410",
    "end": "1017259"
  },
  {
    "text": "would have gotten if you\nwere to start from data and add noise to it in\nthe other direction.",
    "start": "1017260",
    "end": "1023090"
  },
  {
    "text": "These two are exactly equivalent\nto the extent that, you know, the score function.",
    "start": "1023090",
    "end": "1028119"
  },
  {
    "text": "So in this case, you\nhave to go towards-- because this is trying to sample\nfrom the data distribution,",
    "start": "1028119",
    "end": "1033589"
  },
  {
    "text": "so if the data distribution\nhad a mixture of two Gaussians, so there's two possible\nimages, let's say,",
    "start": "1033589",
    "end": "1041470"
  },
  {
    "text": "and it's either\none or the other, then you would want the\nprocess to go there. What we'll see, and that's\ntowards the end of this lecture,",
    "start": "1041470",
    "end": "1048790"
  },
  {
    "text": "is that how to do\ncontrollable generation, which is the idea if you want it\nto only sample one of them,",
    "start": "1048790",
    "end": "1053980"
  },
  {
    "text": "maybe one is cat and\nthe other one is dog. And let's say that you had\na classifier that tells you",
    "start": "1053980",
    "end": "1059650"
  },
  {
    "text": "if you're dealing with a cat or\na dog, this kind of perspective allows you to do it in\na very principled way.",
    "start": "1059650",
    "end": "1067610"
  },
  {
    "text": "So there is a relatively\nsimple algebra that allows you to\nbasically change the drift.",
    "start": "1067610",
    "end": "1072730"
  },
  {
    "text": "And essentially,\nall you have to do is you just basically\napply Bayes' rule and you change the drift to\nalso push you towards x's that",
    "start": "1072730",
    "end": "1081130"
  },
  {
    "text": "are likely to be classified\nas, let's say, you want a dog that are likely\nto be classified as a dog.",
    "start": "1081130",
    "end": "1086510"
  },
  {
    "text": "So just by changing the-- it is basically a principled way\nto change the drift, to push you",
    "start": "1086510",
    "end": "1092650"
  },
  {
    "text": "in a certain direction. And that is probably the right\nway of basically sampling from a conditional distribution\nthat might be defined in terms",
    "start": "1092650",
    "end": "1100360"
  },
  {
    "text": "of, say, a classifier or the\nmore relevant example would be text to image, where you have\na distribution over images that",
    "start": "1100360",
    "end": "1108940"
  },
  {
    "text": "have corresponding\ncaptions, now you want to be able to sample images\nwith a particular caption.",
    "start": "1108940",
    "end": "1114880"
  },
  {
    "text": "Then you don't want\nto necessarily sample from the marginal distribution\nover all the images that you had in the\ntraining set but you",
    "start": "1114880",
    "end": "1120670"
  },
  {
    "text": "want to be able to sample\nfrom the condition. So we'll see that there is a\nway to change this reverse SDE",
    "start": "1120670",
    "end": "1126250"
  },
  {
    "text": "to actually sample from\nnot the data distribution, but some version of the data\ndistribution that is, let's say,",
    "start": "1126250",
    "end": "1133460"
  },
  {
    "text": "more skewed towards a particular\ncaption that you want. So this is the gradient only\nwith respect to x at a given t.",
    "start": "1133460",
    "end": "1141350"
  },
  {
    "text": " There is actually\nways to also try to estimate the\nscore with respect",
    "start": "1141350",
    "end": "1148210"
  },
  {
    "text": "to t, which is the partial\nderivative with respect to t. It turns out you\ncan also estimate the score-matching losses.",
    "start": "1148210",
    "end": "1154660"
  },
  {
    "text": "We actually had a paper on\ndoing these things where you-- the nice thing\nabout that is that--",
    "start": "1154660",
    "end": "1160799"
  },
  {
    "text": "I guess a lot of the\nstructure deals with the-- I guess very specific to\nthe diffusion kind of math. And the moment that one\ndoesn't hold anymore, let's",
    "start": "1160800",
    "end": "1169419"
  },
  {
    "text": "say, if you're\ntrying to interpolate between two different data sets,\nthen it's no longer a diffusion.",
    "start": "1169420",
    "end": "1174740"
  },
  {
    "text": "And so the math is\ndifferent and in that case, you do need the score with the\ngradient with respect to T0's.",
    "start": "1174740",
    "end": "1182203"
  },
  {
    "text": "So you can do more interesting\nthings if you had it, but here you don't need it. It turns out-- because of\nthe Fokker-Planck equation,",
    "start": "1182203",
    "end": "1189750"
  },
  {
    "text": "the gradient with respect\nto t is completely determined by these objects. In the forward SDE,\nthere is no drift.",
    "start": "1189750",
    "end": "1196450"
  },
  {
    "text": "This is just a random walk\nwhere you are essentially just adding noise to the data.",
    "start": "1196450",
    "end": "1202080"
  },
  {
    "text": "So there is no\nparticular direction. If you're going\nfrom data to noise,",
    "start": "1202080",
    "end": "1207360"
  },
  {
    "text": "there is no particular\ndirection that you want to bias your\ntrajectories towards.",
    "start": "1207360",
    "end": "1215684"
  },
  {
    "text": "The score is\ndeterministic drift. Yeah, yeah, yeah. And then there is still noise. As you said, there\nis also a little bit",
    "start": "1215685",
    "end": "1220860"
  },
  {
    "text": "of noise at every step. In the second one, we have\nboth deterministic drift and random noise.",
    "start": "1220860",
    "end": "1226020"
  },
  {
    "text": "So it still has both. And you can think of it as-- if you were to\ndiscretize this SDE,",
    "start": "1226020",
    "end": "1232049"
  },
  {
    "text": "you would get essentially\nLangevin dynamics or essentially the same sampling\nprocedure of DDPM where you would\nfollow the gradient",
    "start": "1232050",
    "end": "1239610"
  },
  {
    "text": "and add a little bit\nof noise at every step. You can think of that\nas basically just",
    "start": "1239610",
    "end": "1245809"
  },
  {
    "text": "the discretization\nof the system. And then basically,\nwhat you can do",
    "start": "1245810",
    "end": "1250900"
  },
  {
    "text": "is you can build a generative\nmodel here by learning this--",
    "start": "1250900",
    "end": "1257560"
  },
  {
    "text": "as usual, if you knew\nthis score functions,",
    "start": "1257560",
    "end": "1262570"
  },
  {
    "text": "then you could just\nsolve this SDE, generate trajectories\nlike-- wait,",
    "start": "1262570",
    "end": "1267810"
  },
  {
    "text": "I can't get the\nanimation going again. Yeah. So if you could somehow\nsimulate this process,",
    "start": "1267810",
    "end": "1276050"
  },
  {
    "text": "this SDE, you solve\nthis SDE, then we would be able to\ngenerate samples at the end. But to do that, you need\nto know this red term.",
    "start": "1276050",
    "end": "1283980"
  },
  {
    "text": "You need to know the score,\nwhich we know it exists, but we don't know\nthe value of it.",
    "start": "1283980",
    "end": "1289630"
  },
  {
    "text": "The only thing we have\naccess to as usual, is data. And so that you can\nget a generative model",
    "start": "1289630",
    "end": "1294950"
  },
  {
    "text": "by basically trying to learn\nthese score functions using",
    "start": "1294950",
    "end": "1300080"
  },
  {
    "text": "a neural network. Just like before there\nis a neural network that",
    "start": "1300080",
    "end": "1307070"
  },
  {
    "text": "parameterized by theta,\nthat every x tries to estimate the corresponding\nscore at that x for the density",
    "start": "1307070",
    "end": "1314720"
  },
  {
    "text": "corresponding to time t. So this is the same\nas in the DDPM case. You had exactly this thing, but\nyou only cared about, let's say,",
    "start": "1314720",
    "end": "1323190"
  },
  {
    "text": "1,000 different\ntime indexes, which were those 1,000 different views\nof the original data density",
    "start": "1323190",
    "end": "1329270"
  },
  {
    "text": "that you were considering in\nyour variational autoencoder. Now, again, we have an infinite\ncollection of score functions",
    "start": "1329270",
    "end": "1335900"
  },
  {
    "text": "because these are\nreal value here.",
    "start": "1335900",
    "end": "1341510"
  },
  {
    "text": "And you can-- as usual,\nyou can basically estimate these things\nusing score-matching.",
    "start": "1341510",
    "end": "1346820"
  },
  {
    "text": "So you have the usual\nL2 regression loss, where you try to\nfind and make sure",
    "start": "1346820",
    "end": "1352340"
  },
  {
    "text": "that your estimated\nscore at every x is close to the true\nscore as measured",
    "start": "1352340",
    "end": "1357860"
  },
  {
    "text": "by L2 distance on average\nwith respect to the data distribution.",
    "start": "1357860",
    "end": "1364280"
  },
  {
    "text": "And whenever you want to\nestimate the score of data plus noise, this is\nsomething that we",
    "start": "1364280",
    "end": "1369770"
  },
  {
    "text": "can do with denoising\nscore-matching, essentially. So again, solving this\ntraining objective",
    "start": "1369770",
    "end": "1376880"
  },
  {
    "text": "corresponds to learning\na sequence of denoisers.",
    "start": "1376880",
    "end": "1382310"
  },
  {
    "text": "And it's not a collection of\n1,000 different denoisers. It's an infinite collection\nof denoisers once for every t,",
    "start": "1382310",
    "end": "1388490"
  },
  {
    "text": "but again, it's the usual thing. And now what you can do\nis now you can plug that in into that reverse time SDE.",
    "start": "1388490",
    "end": "1396560"
  },
  {
    "text": "And if you discretize\nthis SDE, which basically",
    "start": "1396560",
    "end": "1401660"
  },
  {
    "text": "means that you just\ndiscretize the time axis,",
    "start": "1401660",
    "end": "1409550"
  },
  {
    "text": "and so you just look\nat instead of dx, you have xt plus 1\nminus xt, essentially.",
    "start": "1409550",
    "end": "1418610"
  },
  {
    "text": "And you integrate that\nstochastic differential equation, you get once\nagain some update rule",
    "start": "1418610",
    "end": "1428900"
  },
  {
    "text": "that is essentially the same-- that is very similar\nto Langevin dynamics and it's exactly\nthe same update rule",
    "start": "1428900",
    "end": "1435919"
  },
  {
    "text": "that you would use in DDPM,\nwhich is an average step, follow the gradient, and then\nadd a little bit of noise, which",
    "start": "1435920",
    "end": "1445410"
  },
  {
    "text": "is the same thing as Langevin\ndynamics, follow the score, add a little bit of noise.",
    "start": "1445410",
    "end": "1451390"
  },
  {
    "text": "But you can think\nof this process as basically trying\nto start from noise",
    "start": "1451390",
    "end": "1456720"
  },
  {
    "text": "and then you're\ntrying to compute this red curve to get to a good\napproximation of a data point.",
    "start": "1456720",
    "end": "1463909"
  },
  {
    "text": "We are dealing with a computer,\nso you cannot deal with infinite truly continuous time processes.",
    "start": "1463910",
    "end": "1470880"
  },
  {
    "text": "So you have to discretize time. You have to discretize\nthe time axis and you can try to approximate\nthis red trajectory",
    "start": "1470880",
    "end": "1479510"
  },
  {
    "text": "with essentially some\nTaylor expansion. So this is really\nwhat this thing",
    "start": "1479510",
    "end": "1485120"
  },
  {
    "text": "is, is just a Taylor expansion\nto what you should be doing.",
    "start": "1485120",
    "end": "1490370"
  },
  {
    "text": "And that's what DDPM does. So DDPM basically has 1,000\ndifferent time slices.",
    "start": "1490370",
    "end": "1499320"
  },
  {
    "text": "And then you will try to\napproximate this red curve by taking steps\naccording to the--",
    "start": "1499320",
    "end": "1508310"
  },
  {
    "text": "essentially, following\nthis white arrow corresponds to sampling from\none of the decoders that",
    "start": "1508310",
    "end": "1516930"
  },
  {
    "text": "defines the DDPM model. And because you're\nlike discretizing time,",
    "start": "1516930",
    "end": "1524539"
  },
  {
    "text": "there is going to be some\nerror that is happening. There's going to be some\nnumerical errors that",
    "start": "1524540",
    "end": "1531080"
  },
  {
    "text": "can accumulate over time. And you can think of what a\nscore-based model, MCMC, does",
    "start": "1531080",
    "end": "1539600"
  },
  {
    "text": "as essentially trying to\nuse Langevin dynamics to get a good sample from the density\ncorresponding to that time.",
    "start": "1539600",
    "end": "1550260"
  },
  {
    "text": "And so you can actually combine\nthe sampling procedure of DDPM with the sampling procedure\nof a score-based model.",
    "start": "1550260",
    "end": "1557320"
  },
  {
    "text": "And you're going to\nincrease compute cost, but you can get a\ncloser approximation",
    "start": "1557320",
    "end": "1564150"
  },
  {
    "text": "to the solution of this\nstochastic differential equation that is defined\nover a continuous time.",
    "start": "1564150",
    "end": "1571750"
  },
  {
    "text": "One of the nice things about\nthis whole SDE perspective is you might wonder where are\nwe going through all of this,",
    "start": "1571750",
    "end": "1576870"
  },
  {
    "text": "is that there is a way\nto obtain an equivalent-- ",
    "start": "1576870",
    "end": "1584280"
  },
  {
    "text": "there is basically a\nway to convert the SDE to an ordinary\ndifferential equation where there is no longer\nnoise added at every step.",
    "start": "1584280",
    "end": "1592260"
  },
  {
    "text": "And so that basically\ncorresponds to converting a VAE into a flow model, because\nthis is essentially",
    "start": "1592260",
    "end": "1601800"
  },
  {
    "text": "an infinitely deep VAE\nwhere there is a lot of-- as you go through\nthis trajectory,",
    "start": "1601800",
    "end": "1608340"
  },
  {
    "text": "you're sampling from a\nlot of different decoders. And a VAE is the\ndecoders are stochastic.",
    "start": "1608340",
    "end": "1615080"
  },
  {
    "text": "So you would always add a little\nbit of noise at every step. If you think about a flow model\nthat would be deterministic.",
    "start": "1615080",
    "end": "1621490"
  },
  {
    "text": "So there is randomness in\nthe prior, which is basically the initial condition\nof this process,",
    "start": "1621490",
    "end": "1628040"
  },
  {
    "text": "but then the dynamics are\ncompletely deterministic, all the transformations\nare deterministic. And that it's also\npossible to do it",
    "start": "1628040",
    "end": "1635320"
  },
  {
    "text": "because we have the\ncontinuous-time formulation. If you don't use it, then\nyou have a score-based model.",
    "start": "1635320",
    "end": "1641660"
  },
  {
    "text": "So don't use the predictor,\nyou just use corrector, then you have a\nscore-based model. If you just use predictor,\nthen you have a DDPM.",
    "start": "1641660",
    "end": "1649960"
  },
  {
    "text": "If you use both, you get\nsomething a little bit more expensive that actually gives\nyou better samples because it's",
    "start": "1649960",
    "end": "1655150"
  },
  {
    "text": "a closer approximation\nbasically, underlying red curve, which\nis what you would really want,",
    "start": "1655150",
    "end": "1662980"
  },
  {
    "text": "essentially. Recall that basically\nthe ELBO objective is trying to essentially invert--",
    "start": "1662980",
    "end": "1670411"
  },
  {
    "text": "the decoder is trying\nto invert the encoder. And the decoder is forced to\nbe Gaussian just by definition.",
    "start": "1670411",
    "end": "1679830"
  },
  {
    "text": "And basically, the\ntrue denoising process",
    "start": "1679830",
    "end": "1684840"
  },
  {
    "text": "is not necessarily\nGaussian, the one that you would get if you were\nto really invert the denoising",
    "start": "1684840",
    "end": "1690679"
  },
  {
    "text": "process. And so no matter\nhow clever you are in selecting the mean and\nthe variance of your Gaussian",
    "start": "1690680",
    "end": "1696650"
  },
  {
    "text": "decoder, there might\nalways be a gap, if you think about the\nELBO between the encoder",
    "start": "1696650",
    "end": "1704360"
  },
  {
    "text": "and the inverse of the decoder. And so you might not be-- which\nmeans that the ELBO is not tight",
    "start": "1704360",
    "end": "1711049"
  },
  {
    "text": "and means that you are not\nmodeling the data distribution perfectly. And they only--\nbasically, another way",
    "start": "1711050",
    "end": "1718400"
  },
  {
    "text": "to think about this\nmath is that only in the limit of continuous-time\nor basically an infinitely",
    "start": "1718400",
    "end": "1724610"
  },
  {
    "text": "large number of steps it's\npossible to essentially get a tight ELBO, where if the\nforward process is Gaussian,",
    "start": "1724610",
    "end": "1732980"
  },
  {
    "text": "the reverse process\nis also Gaussian. So you're not losing anything,\nbut basically assuming",
    "start": "1732980",
    "end": "1739809"
  },
  {
    "text": "that the decoders are Gaussian. But that's only true\nif you really have an infinite number of steps.",
    "start": "1739810",
    "end": "1746530"
  },
  {
    "text": "So it's only true\nin continuous time. So the predictor would\njust take one step. The corrector is\njust using Langevin",
    "start": "1746530",
    "end": "1753280"
  },
  {
    "text": "to try to generate a sample\nfrom the density corresponding to that. So that you would\nstill do, let's say,",
    "start": "1753280",
    "end": "1759910"
  },
  {
    "text": "1,000 different steps, but\nnot an infinite number. In this case, I guess\nI'm showing three steps.",
    "start": "1759910",
    "end": "1765770"
  },
  {
    "text": "In reality, you would have 1,000\nof these different white arrows. Cool.",
    "start": "1765770",
    "end": "1770860"
  },
  {
    "text": "So the interesting thing\nis that so far, we've been talking about stochastic\ndifferential equations",
    "start": "1770860",
    "end": "1778000"
  },
  {
    "text": "where we have these paths that\nyou can think either in forward or reverse going from data\nto noise or noise to data.",
    "start": "1778000",
    "end": "1785870"
  },
  {
    "text": "It turns out that it's possible\nto define a process where",
    "start": "1785870",
    "end": "1792380"
  },
  {
    "text": "the dynamics are\nentirely deterministic. And it's equivalent in the\nsense that the densities",
    "start": "1792380",
    "end": "1801100"
  },
  {
    "text": "that you get at\nevery time step are the same as the\none you would get by solving the stochastic\ndifferential equation",
    "start": "1801100",
    "end": "1808539"
  },
  {
    "text": "either forward or reverse time. So we basically have two\ndifferent stochastic processes,",
    "start": "1808540",
    "end": "1816190"
  },
  {
    "text": "one is basically you have a\nstochastic initial condition, and then deterministic dynamics.",
    "start": "1816190",
    "end": "1823010"
  },
  {
    "text": "Those are those white\ntrajectories that you see. And another one where there\nis toxicity at the beginning",
    "start": "1823010",
    "end": "1831309"
  },
  {
    "text": "and then also at every step. And the processes are\nthe same in the sense",
    "start": "1831310",
    "end": "1837520"
  },
  {
    "text": "that for every slice\nthat you want to take, so for every time index, the\nmarginal distributions are",
    "start": "1837520",
    "end": "1845470"
  },
  {
    "text": "the same. So if you look at how frequently\nyou see a white line versus one",
    "start": "1845470",
    "end": "1853000"
  },
  {
    "text": "of the colored lines\npassing through a point, you will get exactly the\nsame kind of density,",
    "start": "1853000",
    "end": "1861220"
  },
  {
    "text": "including at time 0, which is\nthe one that we care about,",
    "start": "1861220",
    "end": "1866440"
  },
  {
    "text": "which is the one corresponding\nto data, basically. So what this means is\nthat we can basically",
    "start": "1866440",
    "end": "1878080"
  },
  {
    "text": "define a process that is\nentirely deterministic. And as we were saying\nbefore, this essentially",
    "start": "1878080",
    "end": "1884500"
  },
  {
    "text": "corresponds to converting\na VAE into a flow model.",
    "start": "1884500",
    "end": "1889520"
  },
  {
    "text": "So in a VAE, you would have this\nprocess where at every step,",
    "start": "1889520",
    "end": "1894680"
  },
  {
    "text": "you have a sample from a\ndecoder which has stochasticity.",
    "start": "1894680",
    "end": "1899750"
  },
  {
    "text": "In a flow model, you would\nhave all these layers that are just\ntransforming the data",
    "start": "1899750",
    "end": "1905659"
  },
  {
    "text": "through some invertible\ntransformation. And this is essentially\nwhat's going on here.",
    "start": "1905660",
    "end": "1912170"
  },
  {
    "text": "What we're doing is we're\nconverting the model into an infinitely deep flow\nmodel, a continuous time",
    "start": "1912170",
    "end": "1920059"
  },
  {
    "text": "normalizing flow\nmodel where there is an infinite sequence of\ninvertible transformations",
    "start": "1920060",
    "end": "1927169"
  },
  {
    "text": "which basically correspond\nto the dynamics defined",
    "start": "1927170",
    "end": "1932240"
  },
  {
    "text": "by this ordinary\ndifferential equation. So the difference here is that\nif you look at this equation,",
    "start": "1932240",
    "end": "1940400"
  },
  {
    "text": "this is no longer a stochastic\ndifferential equation, there is no noise\nadded at every step.",
    "start": "1940400",
    "end": "1946530"
  },
  {
    "text": "Now, we only have\na drift term here and there is absolutely no\nnoise added during the sampling",
    "start": "1946530",
    "end": "1955160"
  },
  {
    "text": "process. And again, you can see that\nthe only thing that you",
    "start": "1955160",
    "end": "1961740"
  },
  {
    "text": "need in order to\nbe able to define this ordinary differential\nequation is the score function.",
    "start": "1961740",
    "end": "1968280"
  },
  {
    "text": "So if you have the score\nfunction or the sequence of score functions one\nfor every time step,",
    "start": "1968280",
    "end": "1973620"
  },
  {
    "text": "then you can equivalently\ngenerate data",
    "start": "1973620",
    "end": "1979934"
  },
  {
    "text": "from your data distribution by\nsolving an ordinary differential equation.",
    "start": "1979935",
    "end": "1986700"
  },
  {
    "text": "So you just initialize\nthis trajectory, basically. Once again, flip the\ndirection of time.",
    "start": "1986700",
    "end": "1993220"
  },
  {
    "text": "You sample an initial\ncondition by sampling from the prior, which is this\nusual pure noise distribution.",
    "start": "1993220",
    "end": "1999580"
  },
  {
    "text": "And then you follow one of\nthese white trajectories. And at the end, you get a\ndata point, which is exactly",
    "start": "1999580",
    "end": "2008333"
  },
  {
    "text": "the kind of thing you\nwould do in a flow model where you sample from\nthe prior and then you transform it using a\ndeterministic invertible",
    "start": "2008333",
    "end": "2015950"
  },
  {
    "text": "transformation to\nget a data point. And so that's basically\nwhat you would do.",
    "start": "2015950",
    "end": "2023600"
  },
  {
    "text": "We have this process\nand we can think",
    "start": "2023600",
    "end": "2030080"
  },
  {
    "text": "of this basically\nas a continuous time normalizing flow. And the reason\nthis is, you know,",
    "start": "2030080",
    "end": "2037250"
  },
  {
    "text": "this is indeed or intuitively\nthe reason you can think of this as a normalizing flow is because\nthese ordinary differential",
    "start": "2037250",
    "end": "2044930"
  },
  {
    "text": "equations have a\nunique solution. So basically, these white\ntrajectories they cannot cross",
    "start": "2044930",
    "end": "2051830"
  },
  {
    "text": "each other. They cannot overlap, which\nmeans that there is some kind",
    "start": "2051830",
    "end": "2057469"
  },
  {
    "text": "of mapping which is invertible\nthat goes from here to here",
    "start": "2057469",
    "end": "2063679"
  },
  {
    "text": "which is the mapping defined by\nthis solution of the ordinary differential equation, which\nup to some technical condition",
    "start": "2063679",
    "end": "2071239"
  },
  {
    "text": "exists and is unique. And so we can think of this\nas a very flexible flow model",
    "start": "2071239",
    "end": "2081750"
  },
  {
    "text": "where the invertible\nmapping is defined by the dynamics of our\nordinary differential equation,",
    "start": "2081750",
    "end": "2089129"
  },
  {
    "text": "where the dynamics are\ndefined by a neural network. So it's a neural\nODE if you've seen",
    "start": "2089130",
    "end": "2095699"
  },
  {
    "text": "these kinds of models, a neural\nordinary differential equation. So it's a deep learning model\nwhere the computation is defined",
    "start": "2095699",
    "end": "2103980"
  },
  {
    "text": "by what you get by solving\nan ordinary differential equation, where the\ndynamics are defined",
    "start": "2103980",
    "end": "2109859"
  },
  {
    "text": "by a neural network, which in\nthis case is the score function. The ODE is equivalent\nto the SDE,",
    "start": "2109860",
    "end": "2116400"
  },
  {
    "text": "so they define exactly the\nsame kind of distribution at every step. So the distribution that\nyou get at this end.",
    "start": "2116400",
    "end": "2125850"
  },
  {
    "text": "So a capital T is\nexactly the same that you would have gotten\nif you were to just add-- and we're doing a\nrandom walk where you",
    "start": "2125850",
    "end": "2132150"
  },
  {
    "text": "add the noise at every step. This is true if you\nhave the exact score",
    "start": "2132150",
    "end": "2137290"
  },
  {
    "text": "function or the average\nstep, which is never the case in practice,\nbut to the extent",
    "start": "2137290",
    "end": "2143349"
  },
  {
    "text": "that you have the\nexact score function, the mapping between the\nSDE and the ODE is exact.",
    "start": "2143350",
    "end": "2149090"
  },
  {
    "text": "So they're exactly\ndefining the same. It's a different\nstochastic process with exactly the same\nmarginal distributions.",
    "start": "2149090",
    "end": "2155877"
  },
  {
    "text": "Another way to think about\nit is that you're essentially reparameterizing the randomness.",
    "start": "2155877",
    "end": "2160910"
  },
  {
    "text": "So remember that\nwhen we're thinking about variational\ninference and how to backprop through\nbasically the encoder, which",
    "start": "2160910",
    "end": "2170650"
  },
  {
    "text": "is like a stochastic\nkind of computation, we were showing that\nit's possible to sample",
    "start": "2170650",
    "end": "2177730"
  },
  {
    "text": "from a Gaussian by\nbasically transforming some simple noise through\na deterministic kind of transformation.",
    "start": "2177730",
    "end": "2183369"
  },
  {
    "text": "And in some sense\nwhat's happening here is that we are reparameterizing. This is a computation graph\nwhere we add randomness",
    "start": "2183370",
    "end": "2189700"
  },
  {
    "text": "at every step. And we're defining a somewhat\nequivalent computation graph where we're putting\nall the randomness",
    "start": "2189700",
    "end": "2196270"
  },
  {
    "text": "in the initial condition. And then we're transforming\nit deterministically.",
    "start": "2196270",
    "end": "2202395"
  },
  {
    "text": " But, yeah. The key thing here is that\nthe mapping is invertible",
    "start": "2202395",
    "end": "2210480"
  },
  {
    "text": "because if you think about the\nordinary differential equation, there is a\ndeterministic dynamic.",
    "start": "2210480",
    "end": "2216410"
  },
  {
    "text": "So whenever you are somewhere,\nthe ordinary differential equation will push you\nsomewhere in the next location",
    "start": "2216410",
    "end": "2224540"
  },
  {
    "text": "based on the dynamics. And they cannot bifurcate. There is only one next stage\nthat you get by solving the ODE.",
    "start": "2224540",
    "end": "2232430"
  },
  {
    "text": "And so there is no way for\ntwo things to possibly cross.",
    "start": "2232430",
    "end": "2237829"
  },
  {
    "text": "And we can invert\nit by basically going backwards\nfrom capital T to 0,",
    "start": "2237830",
    "end": "2243200"
  },
  {
    "text": "which is from noise to data. And this is important\nfor several reasons.",
    "start": "2243200",
    "end": "2252240"
  },
  {
    "text": "The main one is that\nwe can now think--",
    "start": "2252240",
    "end": "2257280"
  },
  {
    "text": "if you think of this\nprocess of going from some prior, simple prior,\na Gaussian distribution to data",
    "start": "2257280",
    "end": "2264660"
  },
  {
    "text": "through an invertible\nmapping, this is once again a normalizing flow.",
    "start": "2264660",
    "end": "2271140"
  },
  {
    "text": "So what you can do\nis you can actually compute the likelihood\nof any x or any image",
    "start": "2271140",
    "end": "2282050"
  },
  {
    "text": "by using essentially a\nchange of variable formula. As in a regular\nflow model, if you",
    "start": "2282050",
    "end": "2287359"
  },
  {
    "text": "want to evaluate the likelihood\nof some x under the flow model, what you would do is\nyou would invert the floor",
    "start": "2287360",
    "end": "2295869"
  },
  {
    "text": "to go in the prior\nspace, which in this case corresponds to solving\nthe ODE backwards",
    "start": "2295870",
    "end": "2301900"
  },
  {
    "text": "and find the corresponding\npoint in the latent space.",
    "start": "2301900",
    "end": "2307579"
  },
  {
    "text": "Evaluating the\nlikelihood of that point under the prior and the\nprior is known as fixed",
    "start": "2307580",
    "end": "2314000"
  },
  {
    "text": "so we can do it\nefficiently, then as usual, you have to keep track of that\nchange of variable formula,",
    "start": "2314000",
    "end": "2319700"
  },
  {
    "text": "essentially. So you have to keep track of\nhow much the volume is squeezed",
    "start": "2319700",
    "end": "2324790"
  },
  {
    "text": "or expanded as you\ntransform a data point through this\nordinary differential",
    "start": "2324790",
    "end": "2331730"
  },
  {
    "text": "equation integrating it. So it looks like this. So you have to integrate the--",
    "start": "2331730",
    "end": "2338330"
  },
  {
    "text": "so it's an ordinary\ndifferential equation. So you can imagine if you\nwere to discretize it,",
    "start": "2338330",
    "end": "2345380"
  },
  {
    "text": "the score would give you\nthe direction that you should move by a little bit and\nthen you need to recompute it.",
    "start": "2345380",
    "end": "2351059"
  },
  {
    "text": "So you still need to somehow\nsolve an ordinary differential equation on a computer, which\ninvolves discretizations.",
    "start": "2351060",
    "end": "2358460"
  },
  {
    "text": "But what you get\nis that people have spent 50 years or more\ndeveloping really good methods",
    "start": "2358460",
    "end": "2364880"
  },
  {
    "text": "for solving ordinary\ndifferential equations very efficiently. There's very clever schemes for\nchoosing the step size, very",
    "start": "2364880",
    "end": "2374630"
  },
  {
    "text": "clever schemes for reducing the\nnumerical errors that you get",
    "start": "2374630",
    "end": "2379910"
  },
  {
    "text": "as you go from left to\nright, and all that machinery can be used and has been used to\nbasically accelerate sampling,",
    "start": "2379910",
    "end": "2388040"
  },
  {
    "text": "generate higher quality samples. And that's one of the main\nreasons this perspective is so powerful because once you\nreduce sampling to solve an ODE,",
    "start": "2388040",
    "end": "2397319"
  },
  {
    "text": "you suddenly have access to a\nlot of really smart techniques that people have\ndeveloped to come up",
    "start": "2397320",
    "end": "2406250"
  },
  {
    "text": "with good numerical\napproximations to the ordinary\ndifferential equations. If you recall, you can\nthink of DDPM as a VAE",
    "start": "2406250",
    "end": "2413720"
  },
  {
    "text": "with a fixed encoder,\nwhich happens to also have the same dimension. And that's very important\nfor getting the method",
    "start": "2413720",
    "end": "2421385"
  },
  {
    "text": "to work in practice. We'll talk about\nlatent diffusion models in a few slides.",
    "start": "2421385",
    "end": "2426930"
  },
  {
    "text": "And that basically embraces\nmore the VAE perspective of say,",
    "start": "2426930",
    "end": "2432290"
  },
  {
    "text": "well, let's have a first\nencoder and decoder that will map the data to\na lower dimensional space",
    "start": "2432290",
    "end": "2438560"
  },
  {
    "text": "and then learn a diffusion\nmodel over that latent space. And so you get the\nbest of both worlds",
    "start": "2438560",
    "end": "2444560"
  },
  {
    "text": "where you've both reduced\nthe dimensionality, and you can still use this\nmachinery tool that we",
    "start": "2444560",
    "end": "2450740"
  },
  {
    "text": "don't practice works very well. The Fokker-Planck equation\nis basically telling you",
    "start": "2450740",
    "end": "2455930"
  },
  {
    "text": "how the densities change. It's a partial\ndifferential equation that relates the\npartial derivative",
    "start": "2455930",
    "end": "2464160"
  },
  {
    "text": "of pt, basically, of the ptx. So the probability of\nx across as you change",
    "start": "2464160",
    "end": "2470460"
  },
  {
    "text": "t to spatial derivatives,\nwhich is essentially",
    "start": "2470460",
    "end": "2475970"
  },
  {
    "text": "the trace of the Jacobian. And so that's why\nthe things work out.",
    "start": "2475970",
    "end": "2481349"
  },
  {
    "text": "And that's actually how you\ndo the conversion from the SDE to the ODE. You just basically work through\nthe Fokker-Planck equation",
    "start": "2481350",
    "end": "2488550"
  },
  {
    "text": "like everything is relying\non this underlying diffusion",
    "start": "2488550",
    "end": "2494190"
  },
  {
    "text": "structure. ",
    "start": "2494190",
    "end": "2499529"
  },
  {
    "text": "Yeah. But basically, what\nI'm saying here is that you can\nuse something that",
    "start": "2499530",
    "end": "2506820"
  },
  {
    "text": "is very similar to the vanilla\nchange of variable formula that we were using\nin flow models",
    "start": "2506820",
    "end": "2512520"
  },
  {
    "text": "to actually compute exact\nlikelihoods using these models. And again, basically,\nif you want to evaluate the probability of a\ndata point x0, what you would do",
    "start": "2512520",
    "end": "2521430"
  },
  {
    "text": "is you would solve\nthe backwards, so you would go\nfrom data to noise.",
    "start": "2521430",
    "end": "2526800"
  },
  {
    "text": "To get xt, you would evaluate\nthe probability of that latent",
    "start": "2526800",
    "end": "2533160"
  },
  {
    "text": "variable under the prior. And that's this piece.",
    "start": "2533160",
    "end": "2539730"
  },
  {
    "text": "And then you have to look at\nbasically how the volume is",
    "start": "2539730",
    "end": "2544740"
  },
  {
    "text": "changed along the trajectory. And it's no longer the\ndeterminant of the Jacobian",
    "start": "2544740",
    "end": "2551430"
  },
  {
    "text": "that you have to look at. It turns out that\nwhat you have to do is you have to integrate the\ntrace of the Jacobian, which",
    "start": "2551430",
    "end": "2559830"
  },
  {
    "text": "is something that\nyou can actually evaluate pretty efficiently. So that's basically what\nour consistency model does.",
    "start": "2559830",
    "end": "2567540"
  },
  {
    "text": "There is this recent model\nwas developed by Yang actually at OpenAI. And essentially\nwhat they do is they",
    "start": "2567540",
    "end": "2575220"
  },
  {
    "text": "try to learn a\nneural network that directly outputs the solution\nof the ODE in one step.",
    "start": "2575220",
    "end": "2585009"
  },
  {
    "text": "And because there is\nan underlying ODE, there is some clever\nobjectives that you can use for training\nthe neural network.",
    "start": "2585010",
    "end": "2591609"
  },
  {
    "text": "But yeah, as we'll see, once\nyou take this perspective you can distill down.",
    "start": "2591610",
    "end": "2598570"
  },
  {
    "text": "Then you can get very\nfast sampling procedures by taking advantage of this\nunderlying ODE perspective.",
    "start": "2598570",
    "end": "2605868"
  },
  {
    "text": "You're just trying to solve ODEs\nand there is a lot of tricks that you can use to\nget very fast solvers. ",
    "start": "2605868",
    "end": "2614950"
  },
  {
    "text": "But one nice thing you get is\nyou can compute likelihoods. So you can convert the\nVAE into a flow model,",
    "start": "2614950",
    "end": "2621619"
  },
  {
    "text": "and then you can\ncompute likelihoods. So the good thing is that\nonce you learn the score once",
    "start": "2621620",
    "end": "2629080"
  },
  {
    "text": "and then it's opening up. There's many different\nways of using the score at inference\ntime to generate samples.",
    "start": "2629080",
    "end": "2637069"
  },
  {
    "text": "And ODEs are good to generate\nsamples very efficiently.",
    "start": "2637070",
    "end": "2642380"
  },
  {
    "text": "The SDE is still valuable. In some cases, it can actually\ngenerate higher-quality samples.",
    "start": "2642380",
    "end": "2648849"
  },
  {
    "text": "And the reason is\nthat if you think about what happens\nwhen you solve the ODE,",
    "start": "2648850",
    "end": "2658170"
  },
  {
    "text": "you start with pure\nnoise and then you follow this denoiser\nessentially to try",
    "start": "2658170",
    "end": "2663930"
  },
  {
    "text": "to approximate one of\nthese trajectories. But then let's say that\nthe denoiser is not perfect",
    "start": "2663930",
    "end": "2671220"
  },
  {
    "text": "and you're making some\nsmall mistakes, then the kind of images that\nyou see around the middle",
    "start": "2671220",
    "end": "2679500"
  },
  {
    "text": "of this trajectory,\nthey're supposed to look like data plus noise. But they're not quite\ngoing to be data plus noise",
    "start": "2679500",
    "end": "2685710"
  },
  {
    "text": "because your score\nfunction is not perfect. Then you're starting\nto feed the data that",
    "start": "2685710",
    "end": "2692095"
  },
  {
    "text": "is a little bit\ndifferent from the ones you've seen during training\nin your denoiser, which is your score model.",
    "start": "2692095",
    "end": "2697660"
  },
  {
    "text": "And so you're going to\nhave compounding errors because the images\nthat you're feeding",
    "start": "2697660",
    "end": "2703200"
  },
  {
    "text": "into your denoiser,\nwhich is basically what you get by following\nthese trajectories, are not quite going to be\nexactly the ones that you've",
    "start": "2703200",
    "end": "2710370"
  },
  {
    "text": "used for training\nthe model, which is what you get by actually\ngoing from data to data",
    "start": "2710370",
    "end": "2717240"
  },
  {
    "text": "plus noise by really just\nadding noise to the data. And so if you think about\nthe SDE, on the other hand,",
    "start": "2717240",
    "end": "2723540"
  },
  {
    "text": "you are actually adding\nnoise at every step. And that's good\nbecause you're making",
    "start": "2723540",
    "end": "2728940"
  },
  {
    "text": "the inputs to the denoiser\nlook more like the ones you've seen during training. The problem is that solving SDEs\nefficiently is a lot harder.",
    "start": "2728940",
    "end": "2739470"
  },
  {
    "text": "And so if you want\nfast sampling, the ODE perspective is much\nmore convenient to work with.",
    "start": "2739470",
    "end": "2746769"
  },
  {
    "text": " Yep. So we can get likelihoods and--",
    "start": "2746770",
    "end": "2755330"
  },
  {
    "text": "what you have to do\nis to basically solve an ODE where you solve this\nintegral over time by--",
    "start": "2755330",
    "end": "2764150"
  },
  {
    "text": "you can literally call\na black box ODE solver and compute this quantity.",
    "start": "2764150",
    "end": "2771110"
  },
  {
    "text": "And it turns out that\nit's very competitive. So even though--\nthese models are not",
    "start": "2771110",
    "end": "2777020"
  },
  {
    "text": "trained by maximum\nlikelihood, so they are not trained as a flow model. And the reason they are not\ntrained as a flow model--",
    "start": "2777020",
    "end": "2783620"
  },
  {
    "text": "because you could, in principle. You could try to optimize.",
    "start": "2783620",
    "end": "2789800"
  },
  {
    "text": "You could do max exact\nmaximum likelihood because you could\ntry to evaluate",
    "start": "2789800",
    "end": "2795109"
  },
  {
    "text": "this expression\nover your data set and optimize it as\na function of theta.",
    "start": "2795110",
    "end": "2800180"
  },
  {
    "text": "But it's numerically very\ntricky and very, very expensive because you have to\ndifferentiate through an ODE",
    "start": "2800180",
    "end": "2808880"
  },
  {
    "text": "solver because you'd have to\noptimize the parameters theta such that the result\nof the ODE solver",
    "start": "2808880",
    "end": "2815570"
  },
  {
    "text": "gives you high likelihood, which\nis extremely difficult to do in practice. So you don't train the\nmodel on maximum likelihood.",
    "start": "2815570",
    "end": "2823080"
  },
  {
    "text": "You still train it\nby score-matching, but still, you get\nvery good likelihoods.",
    "start": "2823080",
    "end": "2829610"
  },
  {
    "text": "You can actually-- this is\nachieving state-of-the-art results on image\ndata sets because--",
    "start": "2829610",
    "end": "2837890"
  },
  {
    "text": "yeah, it's unclear why, but-- I mean, well, we know sort\nof why because as we've seen,",
    "start": "2837890",
    "end": "2845839"
  },
  {
    "text": "score matching has an\nELBO interpretation. So it's not too surprising\nby matching gradients,",
    "start": "2845840",
    "end": "2855590"
  },
  {
    "text": "by doing score-matching, you're\noptimizing an ELBO and evidence lower bound. So it's not too surprising that\nthe likelihoods are good, too.",
    "start": "2855590",
    "end": "2864590"
  },
  {
    "text": "But yeah, the results\nare very, very good in terms of likelihoods.",
    "start": "2864590",
    "end": "2869990"
  },
  {
    "text": "The other thing\nthat you can do is you can get accelerated samples. Specifically, DDIM is very often\nused as a sampling strategy",
    "start": "2869990",
    "end": "2879270"
  },
  {
    "text": "where instead of having\nto go through, let's say, 1,000 different\ndenoising steps, which",
    "start": "2879270",
    "end": "2885059"
  },
  {
    "text": "is what you would do if you\nhad a DDPM model, essentially",
    "start": "2885060",
    "end": "2890220"
  },
  {
    "text": "what you can do is\nyou can coarsely discretize the time axis. Let's say you only look at\n30 different steps instead",
    "start": "2890220",
    "end": "2899940"
  },
  {
    "text": "of 1,000, and then you take\nbig steps, essentially. You take big jumps.",
    "start": "2899940",
    "end": "2905130"
  },
  {
    "text": "And you're going to pay\na price because there's going to be more numerical\nerrors, but it's much faster.",
    "start": "2905130",
    "end": "2914980"
  },
  {
    "text": "And in practice, this\nis what people use. And there is a little bit more--\nit can be a little bit more",
    "start": "2914980",
    "end": "2920010"
  },
  {
    "text": "clever than this because there\nis some special structure like a piece of\nthe OD is linear,",
    "start": "2920010",
    "end": "2925110"
  },
  {
    "text": "so you can actually\nsolve it in closed form, but essentially, this is\nhow you get fast sampling.",
    "start": "2925110",
    "end": "2930820"
  },
  {
    "text": "You just coarsely\ndiscretize the time axis and you take big steps.",
    "start": "2930820",
    "end": "2936280"
  },
  {
    "text": "So you can generate an image. Instead of doing\n1,000 steps, you maybe only need to do 30 steps.",
    "start": "2936280",
    "end": "2943630"
  },
  {
    "text": "And that becomes a\nparameter that you can use to decide how much\ncompute you want to use,",
    "start": "2943630",
    "end": "2949930"
  },
  {
    "text": "how much effort you want to\nput in at inference time. The more steps you\ntake, the more accurate",
    "start": "2949930",
    "end": "2955900"
  },
  {
    "text": "the solution to the ODE\nbecomes, but of course, the more expensive it actually is.",
    "start": "2955900",
    "end": "2962880"
  },
  {
    "text": "Just to clarify, there is not\na score function for the ODE and one for the SDE.",
    "start": "2962880",
    "end": "2969100"
  },
  {
    "text": "There is just a\nsingle score function which is the score function of\nthe data density plus noise.",
    "start": "2969100",
    "end": "2974950"
  },
  {
    "text": "And so it's the same whether\nyou take the ODE perspective or the SDE perspective.",
    "start": "2974950",
    "end": "2980890"
  },
  {
    "text": "The marginals that you get\nwith the two perspectives are the same. And so the scores are the\nsame and they are always",
    "start": "2980890",
    "end": "2988420"
  },
  {
    "text": "learned by score-matching. Then at inference time, you\ncan do different things. At the inference time,\nyou can solve the SDE,",
    "start": "2988420",
    "end": "2995049"
  },
  {
    "text": "you can solve the ODE, but\nthe scores are the same.",
    "start": "2995050",
    "end": "3000920"
  },
  {
    "text": "This is one way to get\nvery fast sampling, and there is a lot\nof better now--",
    "start": "3000920",
    "end": "3006760"
  },
  {
    "text": "there is a lot of\nother clever ways of solving ordinary differential\nequations, Heun kind of solvers",
    "start": "3006760",
    "end": "3015400"
  },
  {
    "text": "where you take half steps. There is a lot of\nclever ideas that people",
    "start": "3015400",
    "end": "3020530"
  },
  {
    "text": "have developed for numerically\nsolving ordinary differential equation pretty accurately\nwith relatively small amounts",
    "start": "3020530",
    "end": "3028090"
  },
  {
    "text": "of compute. And yeah, this can give you\nvery, very big speed-ups with comparable sample quality.",
    "start": "3028090",
    "end": "3034680"
  },
  {
    "text": " Another fun thing you can do is\nyou can actually use parallel.",
    "start": "3034680",
    "end": "3041045"
  },
  {
    "text": "This is actually a\nrecent paper that we have on using these fancy ODE\nsolvers which are basically",
    "start": "3041045",
    "end": "3050230"
  },
  {
    "text": "parallel in time where\ninstead of trying to compute the trajectory of the\nsolution of the ODE one step",
    "start": "3050230",
    "end": "3057460"
  },
  {
    "text": "at a time, which is\nwhat the DDPM would do, you try to guess the whole\ntrajectory by leveraging",
    "start": "3057460",
    "end": "3065050"
  },
  {
    "text": "many, many GPUs. And so instead of trying\nto go one step at a time,",
    "start": "3065050",
    "end": "3071440"
  },
  {
    "text": "trying to find a\ngood approximation to the true underlying\ntrajectories, you use multiple GPUs\nto denoise the whole--",
    "start": "3071440",
    "end": "3080960"
  },
  {
    "text": "a bunch of images, basically,\na batch of images in parallel.",
    "start": "3080960",
    "end": "3086980"
  },
  {
    "text": "And so if you're willing to\ntrade more compute for speed,",
    "start": "3086980",
    "end": "3092560"
  },
  {
    "text": "you can actually get\nexactly the same solution that you would\nhave gotten if you were to go through\nindividual steps",
    "start": "3092560",
    "end": "3098510"
  },
  {
    "text": "using a lot more parallel\ncompute but in a vastly smaller",
    "start": "3098510",
    "end": "3103580"
  },
  {
    "text": "amount of wall clock time.  Now, I don't want to\ngo into too much detail",
    "start": "3103580",
    "end": "3109280"
  },
  {
    "text": "but basically, there are\ntricks for using, again, advanced ODE solvers to further\nspeed up the sampling process.",
    "start": "3109280",
    "end": "3119250"
  },
  {
    "text": "And let's see whether\nI can get that. Basically, what you're\ndoing is instead of going DDPM you would go one\nstep at a time trying to compute",
    "start": "3119250",
    "end": "3127700"
  },
  {
    "text": "the trajectory, which\nis the brown kind of dot that you see moves slowly.",
    "start": "3127700",
    "end": "3133160"
  },
  {
    "text": "What we're doing is\nwe're using multiple GPUs to compute a whole piece of\nthe trajectory in parallel.",
    "start": "3133160",
    "end": "3141700"
  },
  {
    "text": "So it's a way to basically trade\noff compute for reduced wall clock time.",
    "start": "3141700",
    "end": "3148600"
  },
  {
    "text": "Another thing you can\ndo is distillation. The basic idea is that you\ncan think of DDIM as a teacher",
    "start": "3148600",
    "end": "3158079"
  },
  {
    "text": "model. So you have a model\nthat would compute, let's say, the\nsolution of the ODE",
    "start": "3158080",
    "end": "3164890"
  },
  {
    "text": "based on some kind of\ndiscretization of the time axis. And then what you do is\nyou train a student model",
    "start": "3164890",
    "end": "3172870"
  },
  {
    "text": "that basically does in one step\nwhat DDIM would do in two steps.",
    "start": "3172870",
    "end": "3179860"
  },
  {
    "text": "So DDIM maybe would\ntake two steps to go from here to here\nand then from here to here.",
    "start": "3179860",
    "end": "3185180"
  },
  {
    "text": "And you can train a\nseparate student model, which is another score-based\nmodel that is trying to skip",
    "start": "3185180",
    "end": "3192579"
  },
  {
    "text": "and doing basically-- it's trying to define a\nnew score function such",
    "start": "3192580",
    "end": "3197890"
  },
  {
    "text": "that if you were to take one\nstep according to that score function, you would get the same\nresult as what you would have",
    "start": "3197890",
    "end": "3205000"
  },
  {
    "text": "gotten if you were to take two\nsteps of the original score function under DDIM.",
    "start": "3205000",
    "end": "3210760"
  },
  {
    "text": "So again, it's trying to\ndistill the solution of the ODE",
    "start": "3210760",
    "end": "3216130"
  },
  {
    "text": "according to a different--\nor find a different ODE that",
    "start": "3216130",
    "end": "3221980"
  },
  {
    "text": "would give you the same\nsolution but using less steps.",
    "start": "3221980",
    "end": "3227800"
  },
  {
    "text": "And then what you do is\nyou recursively apply this. So then you use this new\nstudent model as the teacher,",
    "start": "3227800",
    "end": "3235990"
  },
  {
    "text": "and you get another\nstudent that tries to do in one step what\nthe other thing does",
    "start": "3235990",
    "end": "3241810"
  },
  {
    "text": "in two steps, which becomes four\nsteps of the original model. And you keep doing this\nuntil you can distill down",
    "start": "3241810",
    "end": "3248740"
  },
  {
    "text": "to a very small number of steps. So these are some\nof the results. Not quite one map,\nbut these are some--",
    "start": "3248740",
    "end": "3255700"
  },
  {
    "text": "the recent paper we have on\nthis progressive distillation. And this is on\ntext-to-image models.",
    "start": "3255700",
    "end": "3261470"
  },
  {
    "text": "This is with the Stability\nAI's Stable Diffusion, where you can actually, let's\nsay, you can see here",
    "start": "3261470",
    "end": "3268990"
  },
  {
    "text": "images generated in just two\nsteps or four steps or eight steps by distilling a model\nthat originally had 1,000 steps",
    "start": "3268990",
    "end": "3277690"
  },
  {
    "text": "and essentially using this trick\nof reducing in half and half",
    "start": "3277690",
    "end": "3282849"
  },
  {
    "text": "and half until you get\ndown to 2, 4, or 8 steps. And you can see the quality\nis pretty good in terms",
    "start": "3282850",
    "end": "3288800"
  },
  {
    "text": "of the sample quality. And this is, of course,\nmuch more efficient. It's also tempting to get at the\nidea of generating in one step.",
    "start": "3288800",
    "end": "3300260"
  },
  {
    "text": "Consistency Models directly\ntry to do it in just one step.",
    "start": "3300260",
    "end": "3308210"
  },
  {
    "text": "And so they directly\ntry to learn the mapping from what you would\nget at the end",
    "start": "3308210",
    "end": "3313450"
  },
  {
    "text": "of this progressive\ndistillation. And they do it with\na different loss, so there is no\nprogressive distillation.",
    "start": "3313450",
    "end": "3319570"
  },
  {
    "text": "They just do it in\none shot, essentially. But they're trying to get at\na very similar kind of result.",
    "start": "3319570",
    "end": "3330670"
  },
  {
    "text": "Cool. And so, yeah,\ndistillation is a good way",
    "start": "3330670",
    "end": "3336640"
  },
  {
    "text": "to achieve fast sampling. There's also this thing\ncalled Consistency Models that is essentially anything you\nmight have seen in Stability AI.",
    "start": "3336640",
    "end": "3344270"
  },
  {
    "text": "They recently released\na model yesterday I think on real-time that allows\nyou to do real-time generation.",
    "start": "3344270",
    "end": "3350320"
  },
  {
    "text": "It's something like this, some\nversion of score distillation",
    "start": "3350320",
    "end": "3355360"
  },
  {
    "text": "plus some GAN that\nthey throw in. But it's like a combination\nof this thing plus a GAN.",
    "start": "3355360",
    "end": "3360850"
  },
  {
    "text": "And they were apparently\nable to generate to get a model that is so fast\nthat it's basically real-time.",
    "start": "3360850",
    "end": "3367572"
  },
  {
    "text": "It's a text-to-image model\nwhere you can type what you want and it generates\nimages in real time.",
    "start": "3367572",
    "end": "3373100"
  },
  {
    "text": "Yeah, combination. Basing, yes.  Speaking of Stable\nDiffusion and Stability AI,",
    "start": "3373100",
    "end": "3382700"
  },
  {
    "text": "the key difference\nbetween what they do and what we've been talking so\nfar is that their use of latent",
    "start": "3382700",
    "end": "3388490"
  },
  {
    "text": "diffusion model. And essentially, what they\ndo is they add an extra--",
    "start": "3388490",
    "end": "3397060"
  },
  {
    "text": "to think about diffusion\nmodel as a VAE, what they do is they add another encoder and\ndecoder layer at the beginning.",
    "start": "3397060",
    "end": "3409730"
  },
  {
    "text": "And the purpose of this\nencoder and decoder is to reduce the\ndimensionality of the data.",
    "start": "3409730",
    "end": "3416290"
  },
  {
    "text": "So instead of having to do\ndiffusion model over pixels,",
    "start": "3416290",
    "end": "3423080"
  },
  {
    "text": "you train a diffusion\nmodel over the latent space of an autoencoder or\nvariational autoencoder.",
    "start": "3423080",
    "end": "3429520"
  },
  {
    "text": "But literally, you\ncan think of what's happening as just an extra--",
    "start": "3429520",
    "end": "3434728"
  },
  {
    "text": "if you think of the\nhierarchical VAE, you just add an extra\nencoder, an extra decoder",
    "start": "3434728",
    "end": "3441430"
  },
  {
    "text": "at the very end of the stack. So those distilled models\nwere actually distilled latent",
    "start": "3441430",
    "end": "3447640"
  },
  {
    "text": "diffusion models. So the reason you\nmight want to do this is that it's a lot\nfaster to train models,",
    "start": "3447640",
    "end": "3456200"
  },
  {
    "text": "let's say, on low-resolution\nimages or low-dimensional data in terms of the\nmemory that you need",
    "start": "3456200",
    "end": "3462310"
  },
  {
    "text": "for training a diffusion model. It's actually much faster\nto train a diffusion model",
    "start": "3462310",
    "end": "3469130"
  },
  {
    "text": "if you could somehow train it\nnot on the original pixel space but you could do\nit over some sort",
    "start": "3469130",
    "end": "3474550"
  },
  {
    "text": "of low-dimensional\nrepresentation space. And the other advantage of this,\nif you take this perspective,",
    "start": "3474550",
    "end": "3482839"
  },
  {
    "text": "is that now you can suddenly\nuse diffusion models for essentially any data\nmodality, including text.",
    "start": "3482840",
    "end": "3489750"
  },
  {
    "text": "So some of the diffusion models\nthat people have tried on text essentially take\nthis perspective.",
    "start": "3489750",
    "end": "3496220"
  },
  {
    "text": "So you start with\ndiscrete inputs x and then you encode them into a\ncontinuous latent space.",
    "start": "3496220",
    "end": "3503420"
  },
  {
    "text": "And then you decode them\nback with the decoder, and then you train\nthe diffusion model over the latent space,\nwhich is now continuous,",
    "start": "3503420",
    "end": "3509758"
  },
  {
    "text": "and so the math works out. And, of course, this\nonly works to the extent that the original encoder and\ndecoder does a pretty good job,",
    "start": "3509758",
    "end": "3519020"
  },
  {
    "text": "but basically\nreconstructing the data. And yeah, what\nStable Diffusion does",
    "start": "3519020",
    "end": "3525950"
  },
  {
    "text": "is they actually\npretrain the autoencoder. So it's not trained end to\nend even though you could.",
    "start": "3525950",
    "end": "3533060"
  },
  {
    "text": "Because it's just a VAE,\nso you could actually train the whole\nthing end to end. What they do is they\npretrain the autoencoder,",
    "start": "3533060",
    "end": "3540950"
  },
  {
    "text": "and they really just care about\ngetting good reconstruction. So they don't care too\nmuch about the distribution",
    "start": "3540950",
    "end": "3548690"
  },
  {
    "text": "of the latent space to\nbe similar to a Gaussian. They just care about getting a\ngood autoencoder essentially.",
    "start": "3548690",
    "end": "3556650"
  },
  {
    "text": "And then in a\nseparate stage, they keep the initial\nautoencoder fixed",
    "start": "3556650",
    "end": "3561960"
  },
  {
    "text": "and they just train the\ndiffusion model over the latent space. And that works really well.",
    "start": "3561960",
    "end": "3570619"
  },
  {
    "text": "And these were some of the-- they got a lot of success\nwith this approach.",
    "start": "3570620",
    "end": "3576712"
  },
  {
    "text": "They were one of\nthe first to train a large-scale model on a lot of\nonline large-scale image data",
    "start": "3576712",
    "end": "3583839"
  },
  {
    "text": "sets. And it's been widely adopted\nby a lot of the community. People have actually\nbeen successful even",
    "start": "3583840",
    "end": "3589150"
  },
  {
    "text": "in training diffusion\nmodels in pixel space. But the most successful\nversions are usually either",
    "start": "3589150",
    "end": "3599349"
  },
  {
    "text": "on the latent space\nor downscaled versions of the images. So they have-- this\nencoder and decoder",
    "start": "3599350",
    "end": "3607060"
  },
  {
    "text": "is more like a downscaling\nand an upscaling. But essentially\nthe trick is being",
    "start": "3607060",
    "end": "3613210"
  },
  {
    "text": "able to train a model over\na low-resolution data. Literally, what you do is\nyou encode all your data",
    "start": "3613210",
    "end": "3621789"
  },
  {
    "text": "set that pretend that the\ndata is whatever comes out from the original encoder\nand train your diffusion",
    "start": "3621790",
    "end": "3627740"
  },
  {
    "text": "model over whatever you get. So you regularize it to\nbe close to a Gaussian,",
    "start": "3627740",
    "end": "3633470"
  },
  {
    "text": "if I remember correctly, but\nit's a very weak regularization. Really, all they care\nabout is reconstruction.",
    "start": "3633470",
    "end": "3639290"
  },
  {
    "text": "So if you think about the\nELBO as reconstruction plus matching the\nprior, they don't",
    "start": "3639290",
    "end": "3645057"
  },
  {
    "text": "care too much about\nmatching the prior because they're not really\ngoing to sample from--",
    "start": "3645057",
    "end": "3650630"
  },
  {
    "text": "essentially, they use\nthe diffusion model as the prior and the diffusion\nmodel can generate anything.",
    "start": "3650630",
    "end": "3656960"
  },
  {
    "text": "It's a very powerful\nprior distribution. So you don't have to\nregularize the VAE to have",
    "start": "3656960",
    "end": "3663830"
  },
  {
    "text": "a distribution over latent\nthat is close to Gaussian because anyways,\nthen you're going to fit a VAE to whatever comes\nout from the original-- you're",
    "start": "3663830",
    "end": "3671413"
  },
  {
    "text": "going to fit a diffusion\nmodel to whatever comes out from the encoder of\nthis nature of VAE.",
    "start": "3671413",
    "end": "3676790"
  },
  {
    "text": "So it's not really\nnecessary to regularize. So maybe there are two priors.",
    "start": "3676790",
    "end": "3683339"
  },
  {
    "text": "So if you just think\nabout the basic VAE that goes from high-dimensional\nto low-dimensional data,",
    "start": "3683340",
    "end": "3690640"
  },
  {
    "text": "you could have a prior there\nwhen you pretrain this model.",
    "start": "3690640",
    "end": "3696039"
  },
  {
    "text": "But since you're not\nreally going to use that-- wanted to sample\nfrom, you don't really care about matching the prior.",
    "start": "3696040",
    "end": "3704480"
  },
  {
    "text": "In the diffusion model,\nthe prior at this end is the usual Gaussian.",
    "start": "3704480",
    "end": "3710289"
  },
  {
    "text": "So the diffusion\nmodel that you learn over the latent space of\nthe initial autoencoder",
    "start": "3710290",
    "end": "3715770"
  },
  {
    "text": "has a Gaussian prior. How do you get text into\none of these models?",
    "start": "3715770",
    "end": "3722510"
  },
  {
    "text": "And there are several\nways to do it. So let's say that you have\na data set that is not just",
    "start": "3722510",
    "end": "3728670"
  },
  {
    "text": "images x but it's\nimages and captions, where I'm denoting the\ncaption with y here",
    "start": "3728670",
    "end": "3735500"
  },
  {
    "text": "because it could also be\na class label, let's say. So really what\nyou're trying to do",
    "start": "3735500",
    "end": "3741660"
  },
  {
    "text": "is you're trying not to learn\nthe joint distribution over x, y",
    "start": "3741660",
    "end": "3746785"
  },
  {
    "text": "because you don't\ncare about generating images and the\ncorresponding captions, you just care about learning\nthe conditional distribution",
    "start": "3746785",
    "end": "3753420"
  },
  {
    "text": "of an image x given\nthe corresponding label or given the\ncorresponding caption y.",
    "start": "3753420",
    "end": "3758940"
  },
  {
    "text": " And essentially, if you want to\nuse a diffusion model for this",
    "start": "3758940",
    "end": "3767850"
  },
  {
    "text": "or a score-based\nmodel, this boils down to learning a score function for\nthis conditional distribution",
    "start": "3767850",
    "end": "3776280"
  },
  {
    "text": "of x given y. So now the score function\nor the denoiser as usual",
    "start": "3776280",
    "end": "3784420"
  },
  {
    "text": "needs to take as input xt,\nwhich is a noisy image. It needs to take\nas input t, which is the time variable in\nthe diffusion process",
    "start": "3784420",
    "end": "3792849"
  },
  {
    "text": "or the sigma level,\nthe amount of noise that you're adding to the image.",
    "start": "3792850",
    "end": "3799280"
  },
  {
    "text": "And now basically the\ndenoiser or the score function gets this side information\ny as an extra input.",
    "start": "3799280",
    "end": "3808210"
  },
  {
    "text": "So the denoiser knows what\nis the caption of the image. And it's allowed to take\nadvantage of that information",
    "start": "3808210",
    "end": "3814690"
  },
  {
    "text": "while it's trying to\nguess the denoise level or equivalently\ndenoise the image.",
    "start": "3814690",
    "end": "3823950"
  },
  {
    "text": "And so in some sense,\nyou can think of it as a slightly easier problem\nbecause the denoiser has access",
    "start": "3823950",
    "end": "3832710"
  },
  {
    "text": "to the class label\ny or the caption y while it's trying to\ndenoise images, essentially.",
    "start": "3832710",
    "end": "3840690"
  },
  {
    "text": "And so then it becomes\na matter of cooking up a suitable architecture where\nyou're fitting in into the unit,",
    "start": "3840690",
    "end": "3847980"
  },
  {
    "text": "you're fitting in t, you're\nfitting in the image xt, and then you need to\nfit in y, which is,",
    "start": "3847980",
    "end": "3853200"
  },
  {
    "text": "let's say a caption\ninto the architecture. And the way to do\nit, it would be,",
    "start": "3853200",
    "end": "3859790"
  },
  {
    "text": "for example, you have some\npretrained language model that somehow can take text and map\nit to a vector representation",
    "start": "3859790",
    "end": "3868880"
  },
  {
    "text": "of the meaning of\nthe caption and then you incorporate it in\nthe neural network.",
    "start": "3868880",
    "end": "3874280"
  },
  {
    "text": "And there's different ways\nof doing it, but maybe doing some kind of cross-attention--\nthere's different architectures",
    "start": "3874280",
    "end": "3880010"
  },
  {
    "text": "but essentially, you want to\nadd caption-wise and additional",
    "start": "3880010",
    "end": "3885320"
  },
  {
    "text": "input to your neural\nnetwork architecture. This is the most\nlike vanilla way",
    "start": "3885320",
    "end": "3892510"
  },
  {
    "text": "of doing things, which is just\ntrain a conditional model. ",
    "start": "3892510",
    "end": "3898570"
  },
  {
    "text": "Now, the more interesting\nthing, I think, is when you want to control\nthe generation process",
    "start": "3898570",
    "end": "3905920"
  },
  {
    "text": "but you don't want to\ntrain a different model. So the idea is that\nyou might have trained",
    "start": "3905920",
    "end": "3911260"
  },
  {
    "text": "a generative model over images. And let's say there's\ntwo types of images--",
    "start": "3911260",
    "end": "3916810"
  },
  {
    "text": "dogs, and cats. And then let's say that now we\nonly want to generate-- back",
    "start": "3916810",
    "end": "3924940"
  },
  {
    "text": "to the question that was asked\ninitially during the class is, what if you want to\ngenerate an image only of dogs?",
    "start": "3924940",
    "end": "3933640"
  },
  {
    "text": "So if you have some kind\nof classifier, p of y given x, that can tell you\nwhether an image x corresponds",
    "start": "3933640",
    "end": "3941650"
  },
  {
    "text": "to the label dog\nor not, how do we generate an image x that\nwould be labeled as a dog,",
    "start": "3941650",
    "end": "3952320"
  },
  {
    "text": "with a label y equals dog? Mathematically,\nwhat we want to do is we want to combine\nthis prior distribution",
    "start": "3952320",
    "end": "3960240"
  },
  {
    "text": "p of x with this likelihood p\nof y given x, which, let's say, is given by a classifier,\nand what we want to do",
    "start": "3960240",
    "end": "3967650"
  },
  {
    "text": "is we want to sample from the\nposterior distribution, x given y.",
    "start": "3967650",
    "end": "3973870"
  },
  {
    "text": "So we know that we want a\ndog and we want a sample from the conditional\ndistribution of x,",
    "start": "3973870",
    "end": "3981340"
  },
  {
    "text": "given that the label is dog. And if you recall, this\nconditional distribution",
    "start": "3981340",
    "end": "3989050"
  },
  {
    "text": "of x given y is\ncompletely determined by p of x and p of y given\nx through Bayes' rule.",
    "start": "3989050",
    "end": "3997500"
  },
  {
    "text": "This is inverse distribution\nis obtained by that equation",
    "start": "3997500",
    "end": "4005850"
  },
  {
    "text": "that you see there, which\nis just Bayes' rule. So if you want to\nget p of x given y,",
    "start": "4005850",
    "end": "4011510"
  },
  {
    "text": "you multiply the prior\nwith the likelihood and then you normalize to\nget a valid distribution.",
    "start": "4011510",
    "end": "4017160"
  },
  {
    "start": "4017160",
    "end": "4024289"
  },
  {
    "text": "The denominator here is\nin principle something you can obtain by integrating\nthe numerator over x.",
    "start": "4024290",
    "end": "4030994"
  },
  {
    "text": " So what you have in the\nnumerator is p of x, y,",
    "start": "4030995",
    "end": "4038300"
  },
  {
    "text": "and in the\ndenominator, you have p of y, which is\nwhat you would get if you integrate over all\npossible x's, p of x, y.",
    "start": "4038300",
    "end": "4046109"
  },
  {
    "text": "So everything is\ncompletely determined in terms of the prior\nand this classifier.",
    "start": "4046110",
    "end": "4052590"
  },
  {
    "text": "So in theory, if you\nhave a pretrained, let's say, generative model\nof images and somebody",
    "start": "4052590",
    "end": "4058529"
  },
  {
    "text": "gives you a classifier, you\nhave all the information that you need to define this\nconditional distribution of x",
    "start": "4058530",
    "end": "4065920"
  },
  {
    "text": "given y. It's just a matter of\ncomputing that expression",
    "start": "4065920",
    "end": "4071269"
  },
  {
    "text": "using Bayes' rule. And unfortunately, even though\nin theory you have access",
    "start": "4071270",
    "end": "4077990"
  },
  {
    "text": "to the prior, you have\naccess to the likelihood, computing the denominator\nis the usual hard part.",
    "start": "4077990",
    "end": "4084170"
  },
  {
    "text": "It's the same problem as\ncomputing normalization constants in energy-based\nmodels, basically.",
    "start": "4084170",
    "end": "4090260"
  },
  {
    "text": "It requires an integral over x\nand you cannot really compute it.",
    "start": "4090260",
    "end": "4095829"
  },
  {
    "text": "And so in practice, even though\neverything is well defined and you have all the\ninformation that you need, it's not tractable.",
    "start": "4095830",
    "end": "4102520"
  },
  {
    "text": "But if you work at\nthe level of scores, so if you take the gradients\nof the log of that expression,",
    "start": "4102520",
    "end": "4110199"
  },
  {
    "text": "you get that the score of\nthe inverse distribution",
    "start": "4110200",
    "end": "4115660"
  },
  {
    "text": "is given by the\nscore of the prior, the score of the likelihood. And then we have this\nterm which is the score",
    "start": "4115660",
    "end": "4121270"
  },
  {
    "text": "of the normalization constant. And just like in\nenergy-based models,",
    "start": "4121270",
    "end": "4126599"
  },
  {
    "text": "remember that that term goes\nto 0 because it does not depend on X.",
    "start": "4126600",
    "end": "4132540"
  },
  {
    "text": "And so basically, if you're\nworking at the level of scores, there is very simple\nalgebra that you",
    "start": "4132540",
    "end": "4138210"
  },
  {
    "text": "need to do to get the\nscore of the posterior is you just sum up\nthe score of the prior",
    "start": "4138210",
    "end": "4144540"
  },
  {
    "text": "and the score of the likelihood. And what this means\nis that basically,",
    "start": "4144540",
    "end": "4152470"
  },
  {
    "text": "when you think about that SDE\nor the ODE, all you have to do is you have to just\nreplace the score",
    "start": "4152470",
    "end": "4160509"
  },
  {
    "text": "of the prior with the\nscore of the posterior. And really, all\nyou have to do is",
    "start": "4160510",
    "end": "4166564"
  },
  {
    "text": "if you know the\nscore of the prior, you have a pretrained model. And let's say you\nhave a classifier that",
    "start": "4166564",
    "end": "4171790"
  },
  {
    "text": "is able to tell you what is\nthe label y for a given x, as long as it can take gradients\nof that object with respect",
    "start": "4171790",
    "end": "4179799"
  },
  {
    "text": "to x, which is basically if\nyou have a, let's say, an image classifier, you just\nneed to be able to take",
    "start": "4179800",
    "end": "4185830"
  },
  {
    "text": "gradients of the classifier\nwith respect to the inputs. Then you can just sum them up\nand you have the exact score",
    "start": "4185830",
    "end": "4192549"
  },
  {
    "text": "of the posterior. So if basically you\ndo Langevin dynamics or you solve the SDE\nor the ODE, and instead",
    "start": "4192550",
    "end": "4200800"
  },
  {
    "text": "of following the\ngradient of the prior, you follow the gradient of\nthe prior plus the likelihood,",
    "start": "4200800",
    "end": "4207820"
  },
  {
    "text": "you do the right thing. So intuitively, if you think\nabout Langevin dynamics, what you're doing is you're trying\nto follow the direction that",
    "start": "4207820",
    "end": "4216520"
  },
  {
    "text": "increases the likelihood of the\nimage with respect to the prior. But at the same\ntime, you're trying",
    "start": "4216520",
    "end": "4222310"
  },
  {
    "text": "to make sure that the classifier\nwill predict that image as belonging to the class dog.",
    "start": "4222310",
    "end": "4228400"
  },
  {
    "text": "And so you're just changing\nthe drift in the ODE to push the samples\ntowards the ones that",
    "start": "4228400",
    "end": "4234070"
  },
  {
    "text": "will be classified as a dog. In reality, you would need\nto have this classifier",
    "start": "4234070",
    "end": "4240880"
  },
  {
    "text": "with respect to xt, which\nis like a noisy version of the image, but roughly.",
    "start": "4240880",
    "end": "4247930"
  },
  {
    "text": "And if you had a latent\nvariable model, then yeah, it's a little bit\nmore complicated because you also have to go\nthrough the original encoder",
    "start": "4247930",
    "end": "4253960"
  },
  {
    "text": "and decoder. But if you're working on\npixel space, this can be used.",
    "start": "4253960",
    "end": "4259740"
  },
  {
    "text": "And we've used it for\na number of things. You can use it to do\nediting if you want",
    "start": "4259740",
    "end": "4265440"
  },
  {
    "text": "to go from strokes to images. Basically, it's possible to\ndefine a forward model in closed",
    "start": "4265440",
    "end": "4272430"
  },
  {
    "text": "form and you can follow it,\nand you can do image synthesis, or if y is a\ncaption and then you",
    "start": "4272430",
    "end": "4279540"
  },
  {
    "text": "have some kind of image\ncaptioning network, you can steer a generative\nmodel towards one that",
    "start": "4279540",
    "end": "4285720"
  },
  {
    "text": "is producing images\nthat are consistent, that would be captioned\nin a particular way. And you can use it to do\nconditional generation",
    "start": "4285720",
    "end": "4293530"
  },
  {
    "text": "and you can do text\ngeneration and so forth. You can actually also do\nmedical imaging problems",
    "start": "4293530",
    "end": "4301480"
  },
  {
    "text": "where the likelihood\nis specified by how the machine works\nlike the MRI machine works",
    "start": "4301480",
    "end": "4308020"
  },
  {
    "text": "and why it's a measurement\nthat you get from the machine. And then you can try to\ncreate a medical image that",
    "start": "4308020",
    "end": "4314590"
  },
  {
    "text": "is likely under the\nprior and is consistent with a particular measurement\nthat you get from the machine.",
    "start": "4314590",
    "end": "4320180"
  },
  {
    "text": "So a lot of interesting\nproblems can be solved this way. And even\nclassifier-free guidance",
    "start": "4320180",
    "end": "4326440"
  },
  {
    "text": "is basically based\non this kind of idea. And I guess we don't have\ntime to go through it, but it's a trick\nto essentially get",
    "start": "4326440",
    "end": "4335140"
  },
  {
    "text": "the classifier as the difference\nof two diffusion models, but roughly the same thing. In practice, you can\neither approximate it",
    "start": "4335140",
    "end": "4341739"
  },
  {
    "text": "just with a classifier\nthat works on clean data by using the denoiser to go\nfrom noisy to clean and then use",
    "start": "4341740",
    "end": "4347560"
  },
  {
    "text": "the classifier. In some cases, it can\nbe done in closed form, or you can do this trick\nwhere you basically",
    "start": "4347560",
    "end": "4354670"
  },
  {
    "text": "train two diffusion models, one\nthat is conditional on some side information, one that is not.",
    "start": "4354670",
    "end": "4362330"
  },
  {
    "text": "And then you can get the\nclassifier implicitly by taking the difference of the two, which\nis what classifier-free guidance",
    "start": "4362330",
    "end": "4368690"
  },
  {
    "text": "does which is widely used\nin state-of-the-art models. But essentially, they avoid\ntraining the classifier",
    "start": "4368690",
    "end": "4375080"
  },
  {
    "text": "by taking the difference\nof two diffusion models. So they train one. Let's say that is the\np of x given y, which",
    "start": "4375080",
    "end": "4381620"
  },
  {
    "text": "would be just a\ndiffusion model that takes a caption y as an input. Then they have another model\nthat is essentially not",
    "start": "4381620",
    "end": "4388010"
  },
  {
    "text": "looking at the captions. And then during\nthe sampling, you push the model to go in the\ndirection of the images that",
    "start": "4388010",
    "end": "4398220"
  },
  {
    "text": "are consistent with the given\ncaption, and away from the ones that are-- from the typical\nimage under the prior.",
    "start": "4398220",
    "end": "4405150"
  },
  {
    "text": "And that's the trick that\nthey used to generate good-quality images. ",
    "start": "4405150",
    "end": "4415000"
  }
]