[
  {
    "start": "0",
    "end": "5522"
  },
  {
    "text": "So today, we're going to\ntalk about part 2 of failure probability estimation. OK. So now let's get into\nit with a little bit",
    "start": "5522",
    "end": "12830"
  },
  {
    "text": "of a recap of\nimportance sampling. So I realized the\nlast time, there was a lot of distributions\ngoing on, a lot",
    "start": "12830",
    "end": "17902"
  },
  {
    "text": "different distributions that\nmean different things, samples coming from different places. So what I want to\ndo is try to boil it",
    "start": "17902",
    "end": "24650"
  },
  {
    "text": "down to the minimum things\nthat I want you to take away before you at least maybe go\nback through and read the book.",
    "start": "24650",
    "end": "30930"
  },
  {
    "text": "I understand you're probably\nstudying for the quiz now, so maybe you haven't\nread this chapter yet. But I want to break it down\nto the minimum set of things",
    "start": "30930",
    "end": "37489"
  },
  {
    "text": "that I want you to\ntake away at first, just to understand the\nrest of this lecture so we're all on the same page.",
    "start": "37490",
    "end": "44000"
  },
  {
    "text": "OK. So we were talking\nabout this thing called importance sampling. And the whole reason that\nwe wanted to talk about it",
    "start": "44000",
    "end": "50000"
  },
  {
    "text": "was because we had this\ndirect estimation algorithm",
    "start": "50000",
    "end": "55100"
  },
  {
    "text": "where we would just sample\nfrom the nominal trajectory distribution and estimate\nthe probability of failure",
    "start": "55100",
    "end": "60120"
  },
  {
    "text": "from that. But then we realized that, if\nfailure events are rare, then we might not even get\nany failure samples.",
    "start": "60120",
    "end": "66087"
  },
  {
    "text": "And so that's not going to work. So well, we're just going\nto have an estimate of 0. And we also saw, when we\nlooked at the variance,",
    "start": "66087",
    "end": "71852"
  },
  {
    "text": "I showed you the\nformula for the variance of the direct\nestimation estimator. And we saw that,\nas the probability",
    "start": "71852",
    "end": "78390"
  },
  {
    "text": "of failure decreases,\nthat variance increases. So basically, the estimator\nwill get less and less accurate as our probability\nof failure decreases.",
    "start": "78390",
    "end": "85800"
  },
  {
    "text": "And the only way to fix\nthat is to get more samples. But we might require billions\nand billions of samples.",
    "start": "85800",
    "end": "91030"
  },
  {
    "text": "So this could be\nvery inefficient. So we said, OK, let's\ntry to do something else. And the something\nelse that we do",
    "start": "91030",
    "end": "97020"
  },
  {
    "text": "is that we just want to sample\nfrom a different distribution that we call q of tau. We call this a\nproposal distribution.",
    "start": "97020",
    "end": "103690"
  },
  {
    "text": "And the idea is that we want\nthis distribution to hopefully produce more failures than\nthe nominal distribution.",
    "start": "103690",
    "end": "110460"
  },
  {
    "text": "So in this case, for example,\nwe picked this q of tau here. And we actually were able to\nsample some failures when we",
    "start": "110460",
    "end": "116729"
  },
  {
    "text": "sample from this distribution. But then we're\nlike, well, we still want to estimate the\nprobability of failure",
    "start": "116730",
    "end": "121762"
  },
  {
    "text": "under this nominal distribution. So we can't just take the\nnumber of failures here and divide it by the\ntotal number of samples",
    "start": "121762",
    "end": "127350"
  },
  {
    "text": "because that will give us\nthe probability of failure under this distribution. And so what we\nactually need to do",
    "start": "127350",
    "end": "132360"
  },
  {
    "text": "to get the probability of\nfailure under this distribution is we do a similar thing. But we just have to weight\nall of these samples",
    "start": "132360",
    "end": "138000"
  },
  {
    "text": "based on their likelihood\nunder the nominal distribution. So we saw that those weights\nwere basically the likelihood",
    "start": "138000",
    "end": "144217"
  },
  {
    "text": "under the nominal distribution\ndivided by the likelihood under this proposal. ",
    "start": "144217",
    "end": "150930"
  },
  {
    "text": "And then what I was trying to\nshow you in that Pluto notebook was that some proposals\nwork better than others.",
    "start": "150930",
    "end": "156575"
  },
  {
    "text": "So it really\nmatters what we pick for this proposal distribution. So what this is showing here,\nthis is a graphic from the book.",
    "start": "156575",
    "end": "164340"
  },
  {
    "text": "Our nominal distribution is\njust a simple Gaussian again, where the failure\nthreshold is minus 2.",
    "start": "164340",
    "end": "170610"
  },
  {
    "text": "And we can see how that does. Again, we're plotting\nthe estimation error. So we want to be lower\non this plot, ideally,",
    "start": "170610",
    "end": "178040"
  },
  {
    "text": "and then we want to have lower\nspread among our estimates because that means,\non average, we'll be closer to the true estimate.",
    "start": "178040",
    "end": "185930"
  },
  {
    "text": "OK. And so then this\nis the performance of this direct estimator. And now we could see if\nwe shift our proposal over",
    "start": "185930",
    "end": "194620"
  },
  {
    "text": "to be this blue\nproposal here, we're now centered around the\nvery high likelihood region",
    "start": "194620",
    "end": "199659"
  },
  {
    "text": "of the failure distribution. And we were able to get a much\nlower variance estimator that's also much more accurate.",
    "start": "199660",
    "end": "206410"
  },
  {
    "text": "But what I also\nwant to show you is that it is possible to\npick a bad proposal. So for example, if we pick\nthis orange proposal here,",
    "start": "206410",
    "end": "214040"
  },
  {
    "text": "it's now assigning\nhigh likelihood to low likelihood failures. So we are going to\nsample failures with it because anything to the left\nof minus 2 is a failure.",
    "start": "214040",
    "end": "221569"
  },
  {
    "text": "But we're not\nreally assigning it to failures that have very\nmuch probability mass. And so we can actually\nend up doing worse",
    "start": "221570",
    "end": "227650"
  },
  {
    "text": "than our direct estimator. So it really matters\nwhat proposal we pick. And that's the point I'm\ntrying to drive home here,",
    "start": "227650",
    "end": "234836"
  },
  {
    "text": "which is that picking a\ngood proposal distribution is very much critical to success\nwith importance sampling.",
    "start": "234837",
    "end": "240285"
  },
  {
    "text": " And so then we went\non to look at, say,",
    "start": "240285",
    "end": "245690"
  },
  {
    "text": "can we find what the\nactual optimal proposal distribution is with respect to\nthe variance of the estimator?",
    "start": "245690",
    "end": "251332"
  },
  {
    "text": "So I didn't derive\nthis, but I just showed you this is actually what\nthe variance of the importance",
    "start": "251332",
    "end": "257018"
  },
  {
    "text": "sampling estimator looks like. And we went on, and\nwe showed that if we plug in this particular\ndensity for q of tau,",
    "start": "257019",
    "end": "265070"
  },
  {
    "text": "then this variance becomes 0. And we get the estimator\nthat has zero variance.",
    "start": "265070",
    "end": "270860"
  },
  {
    "text": "And this would be optimal. But then we said--\nactually, this is the failure distribution.",
    "start": "270860",
    "end": "276130"
  },
  {
    "text": "And so on the top here, we have\nthe unnormalized failure density we've been used to seeing. And then we said\nthat the normalizing",
    "start": "276130",
    "end": "282340"
  },
  {
    "text": "constant for the\nfailure distribution is just the\nprobability of failure. And so we had this problem where\nwe're like, OK, well, if we",
    "start": "282340",
    "end": "288690"
  },
  {
    "text": "want to be optimal, we would\nuse this distribution to sample from. And we need to know\nthis full density here.",
    "start": "288690",
    "end": "294620"
  },
  {
    "text": "But the whole point\nis that we want to estimate p fail so there's\nno way that we could possibly know this density.",
    "start": "294620",
    "end": "300980"
  },
  {
    "text": "So that led to this\nline of thinking, which was that the optimal\nproposal distribution is the failure distribution.",
    "start": "300980",
    "end": "306120"
  },
  {
    "text": "But we don't know the\nnormalized density for the failure distribution. And that's what we would\nneed for importance sampling.",
    "start": "306120",
    "end": "311840"
  },
  {
    "text": "So the next best\nthing we could do is try to select a\nproposal distribution that we do know the\ndensity of, that's",
    "start": "311840",
    "end": "318410"
  },
  {
    "text": "as close as possible to\nthe failure distribution.",
    "start": "318410",
    "end": "323570"
  },
  {
    "text": "And that's all the\ninnovation that goes on in all the different\nimportance sampling algorithms that we're going to talk about.",
    "start": "323570",
    "end": "328650"
  },
  {
    "text": "So really what these\nalgorithms are trying to do is just pick the best\npossible proposal, knowing that we can never\nsample from the optimal 1, which",
    "start": "328650",
    "end": "335583"
  },
  {
    "text": "is the failure distribution. So how can we pick a proposal\nthat's as close as possible that we do know the density of?",
    "start": "335583",
    "end": "343340"
  },
  {
    "text": "OK. So one more note on picking\nthis proposal that-- I think there was a question\nabout, last Tuesday,",
    "start": "343340",
    "end": "349800"
  },
  {
    "text": "but I skipped over,\nwhich is that we must pick a proposal\nsuch that the probability",
    "start": "349800",
    "end": "356130"
  },
  {
    "text": "density of the proposal\nassigns non-zero density to every possible failure.",
    "start": "356130",
    "end": "361265"
  },
  {
    "text": "So everywhere that the\nfailure distribution density is non-zero, meaning\nthat a failure is possible for\nthat trajectory, it",
    "start": "361265",
    "end": "367705"
  },
  {
    "text": "needs to be possible that we\ncould sample that trajectory from our proposal. So if we just look at this one\non this example on the left",
    "start": "367705",
    "end": "374910"
  },
  {
    "text": "here where our proposal is\na Gaussian distribution, can someone tell me if\nthis property is satisfied?",
    "start": "374910",
    "end": "382155"
  },
  {
    "text": "Yeah, people\nnodding their heads. Yep, that's satisfied. So a Gaussian distribution\nassigns non-zero probability everywhere.",
    "start": "382155",
    "end": "387520"
  },
  {
    "text": "This is not going\nto be an issue. OK. How about over here where we\nhave this uniform distribution as our proposal?",
    "start": "387520",
    "end": "393460"
  },
  {
    "text": "Yeah, no, because\nthere's some region here where the probability density\nof the failure distribution",
    "start": "393460",
    "end": "399479"
  },
  {
    "text": "is non-zero. But we would never sample\nfailures from that region. And so our importance sampling\nestimator would be inaccurate.",
    "start": "399480",
    "end": "405009"
  },
  {
    "text": "Yeah. But in some sense, that's not a\nproblem because, in this figure, our most likely failure that\nwe're trying to optimize for",
    "start": "405010",
    "end": "411930"
  },
  {
    "text": "is still within\nthe distribution. We're trying to estimate\nthe probability of failure. Oh, OK. Yeah, so we need to be able to\nsample all possible failures.",
    "start": "411930",
    "end": "419555"
  },
  {
    "text": " OK. Sorry. This is following\nup on way before.",
    "start": "419555",
    "end": "426610"
  },
  {
    "text": "But in that case, for the one\nthat we're trying to simply-- so for trying to\nestimate the failure--",
    "start": "426610",
    "end": "434820"
  },
  {
    "text": "sorry, trying to\nfalsify the failure-- there's the same constraint. Technically, that's not\nnecessarily a hard constraint",
    "start": "434820",
    "end": "442260"
  },
  {
    "text": "anymore in that case, right? Yeah, when you're just trying\nto falsify, you don't have to--",
    "start": "442260",
    "end": "450060"
  },
  {
    "text": "if you're trying to find\nthe most likely failure and you assign zero probability\ndensity to unlikely failures,",
    "start": "450060",
    "end": "457240"
  },
  {
    "text": "that would be fine. Yeah, just depends on the task. ",
    "start": "457240",
    "end": "464009"
  },
  {
    "text": "So back to where we were. So we're going to now talk about\na bunch of different importance sampling algorithms\nand different ways that we could go about\nselecting this proposal.",
    "start": "464010",
    "end": "471670"
  },
  {
    "text": "So one option for this\nproposal distribution is we could just hand-design it. So we could just be like, I know\nwhat the failure distribution is",
    "start": "471670",
    "end": "477130"
  },
  {
    "text": "supposed to look like. We did that in project 1. And so I'm just going\nto pick something that I think would be similar\nto the failure distribution.",
    "start": "477130",
    "end": "484449"
  },
  {
    "text": "And hint, hint-- maybe\nyou're fuzzing distribution that you used for project\n1 would be a good thing",
    "start": "484450",
    "end": "489760"
  },
  {
    "text": "to start with for\nproject 2 if you want to hand-design your proposal.",
    "start": "489760",
    "end": "495315"
  },
  {
    "text": "But hand-designing\ndoesn't always work. Sometimes we have very\ncomplicated systems. We don't really know what the\nfailure distribution looks like,",
    "start": "495315",
    "end": "500510"
  },
  {
    "text": "and it's not super obvious\nhow we could pick something. So it might be a\nlittle bit better if we could actually\ntry to do something",
    "start": "500510",
    "end": "507130"
  },
  {
    "text": "more adaptive or automatic. And so this other option that\nwe talked about on Tuesday",
    "start": "507130",
    "end": "512469"
  },
  {
    "text": "is that we do know\nhow to draw samples from the failure distribution. So that's the last topic\nthat's on your quiz.",
    "start": "512470",
    "end": "519370"
  },
  {
    "text": "We talked about how we\ncould use Markov chain Monte Carlo or rejection\nsampling to draw samples",
    "start": "519370",
    "end": "525130"
  },
  {
    "text": "with just knowledge of what\nthe unnormalized density for the failure\ndistribution looks like. So we do know samples from it.",
    "start": "525130",
    "end": "531053"
  },
  {
    "text": "We just don't know how\nto compute the density. So then we say, OK, well, why\ndon't we just fit a distribution",
    "start": "531053",
    "end": "536087"
  },
  {
    "text": "from a model class that we do\nknow how to compute the density of-- so for example, maybe\na Gaussian distribution or something--",
    "start": "536088",
    "end": "541810"
  },
  {
    "text": "to the samples from the\nfailure distribution in hopes that we'll\nget a distribution that's close to the\nfailure distribution.",
    "start": "541810",
    "end": "547640"
  },
  {
    "text": "And then we can use that as our\nimportance sampling proposal. So we looked at how that\nlooked towards the end of class",
    "start": "547640",
    "end": "555760"
  },
  {
    "text": "on Tuesday. Here's just another\nvisualization of it. So here we have our nominal\ndistribution over here.",
    "start": "555760",
    "end": "562040"
  },
  {
    "text": "We used MCMC to draw samples\nfrom our failure distribution. So normally, we don't know what\nthe failure distribution looks",
    "start": "562040",
    "end": "568060"
  },
  {
    "text": "like. For the simple Gaussian, we\ncan't actually compute it. But in general, we don't\nknow what it looks like. We just have these\nsamples down here.",
    "start": "568060",
    "end": "574950"
  },
  {
    "text": "And then we just took\nall of these samples and we said, OK, let's\njust fit maximum likelihood",
    "start": "574950",
    "end": "580180"
  },
  {
    "text": "Gaussian distribution\nto these samples. So that's how we get the\npurple distribution here.",
    "start": "580180",
    "end": "585230"
  },
  {
    "text": "And you can see, it does-- normally, we don't\nget to know this-- but it does actually\nlook a decent amount like the failure distribution,\nwhich is the red one there.",
    "start": "585230",
    "end": "592850"
  },
  {
    "text": "And so we got pretty close\nto the optimal proposal I would say. And you can see\nthat in the results where here's the results\nof doing direct estimation.",
    "start": "592850",
    "end": "600720"
  },
  {
    "text": "And you can see that we do much\nbetter if we use the proposal that we fit to the samples.",
    "start": "600720",
    "end": "607610"
  },
  {
    "text": "So there's a few challenges\nwith doing that though. The first one is that we saw--",
    "start": "607610",
    "end": "613460"
  },
  {
    "text": "we had an entire lecture on how\nit's a very non-trivial task to actually produce these\nsamples from the failure distribution.",
    "start": "613460",
    "end": "618600"
  },
  {
    "text": "So we had to do rejection\nsampling or we had to do MCMC. And this isn't a super\nobvious thing to do,",
    "start": "618600",
    "end": "625250"
  },
  {
    "text": "or it's certainly\na non-trivial task. And the second challenge is,\nlet's say we do get samples",
    "start": "625250",
    "end": "630470"
  },
  {
    "text": "from the failure distribution. If this failure distribution\nis high-dimensional, so we were just looking\nat this 1D Gaussian.",
    "start": "630470",
    "end": "635790"
  },
  {
    "text": "But remember, it could\nbe hundreds of dimensions because we're talking about\nfull trajectories here. And it could have\nmultiple failure modes.",
    "start": "635790",
    "end": "641790"
  },
  {
    "text": "It could be very\ncomplex looking. And it might not be so\neasy to pick a nice model class and parameters\nthat actually",
    "start": "641790",
    "end": "647490"
  },
  {
    "text": "fit all of those samples well. So that's two key\nchallenges with this method.",
    "start": "647490",
    "end": "653670"
  },
  {
    "text": "For now, I just want to-- there's ways to\naddress both of them. But for now, I just want to\ntalk about this first challenge,",
    "start": "653670",
    "end": "660330"
  },
  {
    "text": "and that it might be difficult\nfor us to actually draw samples from our failure distribution.",
    "start": "660330",
    "end": "665560"
  },
  {
    "text": "Maybe our MCMC isn't\nworking super well, or we just don't want to\ngo through that process.",
    "start": "665560",
    "end": "670980"
  },
  {
    "text": "And so let's think\nof this question like, what if we could\nstill fit a good proposal but we use samples from a\ndifferent distribution that's",
    "start": "670980",
    "end": "677610"
  },
  {
    "text": "easier to sample from? So we don't draw samples from\nour failure distribution. We draw samples from\nsome other distribution.",
    "start": "677610",
    "end": "684180"
  },
  {
    "text": "And so that's going to lead\nus to our first topic today, which is the\ncross-entropy method--",
    "start": "684180",
    "end": "689650"
  },
  {
    "text": "Michael's favorite method\nin the entire world. It's a form of adaptive\nimportance sampling.",
    "start": "689650",
    "end": "695247"
  },
  {
    "text": "And then we're going\nto talk about something called multiple importance\nsampling-- extend that to population Monte\nCarlo, which is actually",
    "start": "695247",
    "end": "701639"
  },
  {
    "text": "an adaptive version of\nmultiple importance sampling. Talk about something called\nsequential Monte Carlo.",
    "start": "701640",
    "end": "707020"
  },
  {
    "text": "And then finally,\nsuper, super briefly, I'm going to mention\nsomething called ratio of normalizing constants.",
    "start": "707020",
    "end": "713790"
  },
  {
    "text": "But it's an advanced topic. It won't be on any quizzes. So yeah, very briefly just\nbecause I have some good jokes,",
    "start": "713790",
    "end": "719760"
  },
  {
    "text": "and that's it. So let's start with the\ncross-entropy method.",
    "start": "719760",
    "end": "724950"
  },
  {
    "text": "So what we were\njust saying is we don't want to draw\na bunch of samples from the failure distribution. That's hard. That takes up time.",
    "start": "724950",
    "end": "730872"
  },
  {
    "text": "Can we just use samples from\na different distribution that's easier to sample\nfrom and still try to fit a good proposal?",
    "start": "730872",
    "end": "738495"
  },
  {
    "text": "So here's how this\nis going to work. The first thing that we'll\ndo is we'll draw samples from some initial proposal\ndistribution that's",
    "start": "738495",
    "end": "744899"
  },
  {
    "text": "easy to sample from. So something we ideally\ndon't have to use MCMC for. We can just call\nRAND and get samples.",
    "start": "744900",
    "end": "751290"
  },
  {
    "text": "And then-- sorry,\nfor a lot of text here, but we'll break it down. We're going to use these\nsamples to fit a new proposal",
    "start": "751290",
    "end": "758520"
  },
  {
    "text": "distribution from a model class\nthat we also know how to sample from-- so this is like\nanother easy distribution--",
    "start": "758520",
    "end": "765760"
  },
  {
    "text": "by selecting parameters that\nminimize the cross-entropy between the proposal\ndistribution and the failure",
    "start": "765760",
    "end": "771310"
  },
  {
    "text": "distribution. So let me just repeat\nthat one more time because it's a lot of text. So what we want to do is\nwe have a bunch of samples",
    "start": "771310",
    "end": "777700"
  },
  {
    "text": "from some initial distribution. We want to fit a better\nproposal distribution using these samples.",
    "start": "777700",
    "end": "783339"
  },
  {
    "text": "And we'll do that by picking\nthe proposal distribution that minimizes the cross-entropy\nbetween the parameters",
    "start": "783340",
    "end": "791668"
  },
  {
    "text": "for the proposal\ndistribution we pick and the true failure\ndistribution, which is what we, ideally, would want\nto sample from if we could.",
    "start": "791668",
    "end": "799270"
  },
  {
    "text": "And this minimize the\ncross-entropy idea. So cross-entropy\nis a way to-- you",
    "start": "799270",
    "end": "804340"
  },
  {
    "text": "can think of it as almost\na distance between two distributions. So if we minimize\nthe cross-entropy, we're minimizing the\ndistance between the proposal",
    "start": "804340",
    "end": "811300"
  },
  {
    "text": "distribution we're going to use\nand the failure distribution. And so what we want\nto do is basically",
    "start": "811300",
    "end": "816940"
  },
  {
    "text": "select a new distribution given\nall these samples that we have that gets us as close as\npossible to the failure",
    "start": "816940",
    "end": "823569"
  },
  {
    "text": "distribution, which, again,\nwas our whole goal of picking a good proposal.",
    "start": "823570",
    "end": "829720"
  },
  {
    "text": "So let's break down these\nsteps a little bit more though. So we draw samples from some\ninitial proposal distribution",
    "start": "829720",
    "end": "835060"
  },
  {
    "text": "that we'll call q. And then, again, sorry\nfor all the text. This is probably\nthe most text I'll",
    "start": "835060",
    "end": "840100"
  },
  {
    "text": "have on a slide this\nwhole class maybe, but I'm breaking down\nwhat it means to do this.",
    "start": "840100",
    "end": "845840"
  },
  {
    "text": "So it turns out, for many\ncommon model classes-- so for example, the Gaussian\ndistribution that we often use--",
    "start": "845840",
    "end": "851830"
  },
  {
    "text": "minimizing the\ncross-entropy is actually equivalent to just computing\nthis weighted maximum likelihood",
    "start": "851830",
    "end": "857500"
  },
  {
    "text": "estimate for the parameters\nof the distribution. So for a Gaussian\ndistribution, for example,",
    "start": "857500",
    "end": "863090"
  },
  {
    "text": "we just weight\nall of the samples and compute the weighted mean\nand the weighted standard deviation to estimate\nthe Gaussian distribution",
    "start": "863090",
    "end": "869500"
  },
  {
    "text": "from those samples. And those weights are basically\nequivalent to these importance",
    "start": "869500",
    "end": "874660"
  },
  {
    "text": "weights that we've\nbeen talking about. So the weight for any sample\nthat's not a failure will be 0. We don't want to fit to\nthings that aren't failures",
    "start": "874660",
    "end": "881380"
  },
  {
    "text": "because, ideally, we want to\nmatch the failure distribution.",
    "start": "881380",
    "end": "886620"
  },
  {
    "text": "And then samples that are\nmore likely under the failure distribution will have\nhigher weights also",
    "start": "886620",
    "end": "892829"
  },
  {
    "text": "depending on this q of tau.  So I'm going to show you\nwhat all of this looks like.",
    "start": "892830",
    "end": "898630"
  },
  {
    "text": "So hopefully, it'll start maybe\nmaking a little bit more sense here. But first before\nwe do that, we need to pick a proposal distribution\nto try to start out with.",
    "start": "898630",
    "end": "906237"
  },
  {
    "text": "So again, this should\nbe an initial proposal that's easy to sample from. So we need to be able\nto sample from it.",
    "start": "906237",
    "end": "911380"
  },
  {
    "text": "And we're going to need to be\nable to compute this density, q of tau. So we need something\nthat's easy to sample from",
    "start": "911380",
    "end": "916638"
  },
  {
    "text": "and that we know the density of. Can anyone think of a\ngood distribution to try?",
    "start": "916638",
    "end": "922842"
  },
  {
    "text": "Gaussian. Gaussian? Yeah. Yeah, OK. And specifically,\na Gaussian happens",
    "start": "922842",
    "end": "928980"
  },
  {
    "text": "to be the nominal\ntrajectory distribution for the simple system that\nwe've been thinking about.",
    "start": "928980",
    "end": "935460"
  },
  {
    "text": "So yeah, the normal\ntrajectory distribution-- we know how to sample from it. We just do rollouts. And then we also know how to\ncompute the density of it.",
    "start": "935460",
    "end": "942420"
  },
  {
    "text": "We just multiply the probability\nof all the initial states and disturbances that we saw.",
    "start": "942420",
    "end": "947775"
  },
  {
    "text": "So let's see what\nthat looks like.  Yeah.",
    "start": "947775",
    "end": "953100"
  },
  {
    "text": "I just have a\nhigher-level question. All this time we've\nbeen talking about tau as if it's this distribution\nof easily manipulate.",
    "start": "953100",
    "end": "960069"
  },
  {
    "text": "But in reality, there's\nmany, many components of tau. And they're conjugate\nin this giant tuple with a bunch of things in it.",
    "start": "960070",
    "end": "966930"
  },
  {
    "text": "Computing, is there\nany considerations in practical when\nwe're actually doing this, in terms of computing\nprobabilities of stuff",
    "start": "966930",
    "end": "974320"
  },
  {
    "text": "and so forth? Yeah, so we're assuming that we\nalways have a model of p of tau.",
    "start": "974320",
    "end": "979740"
  },
  {
    "text": "So we always know the nominal\ndisturbance distribution and the initial\nstate distribution and the techniques we\ntalked about in chapter 2",
    "start": "979740",
    "end": "985688"
  },
  {
    "text": "where we talked about modeling. That's how we would\nget that model. So assuming we have that model,\nwe're always able to sample",
    "start": "985688",
    "end": "992040"
  },
  {
    "text": "from the nominal\ntrajectory distribution because we just do a\nrollout using that model. And we're also always\nable to compute",
    "start": "992040",
    "end": "998820"
  },
  {
    "text": "the probability of a trajectory\nunder the nominal distribution. Does that ever get\nintractable because it feels like, for falsification,\nwe do some rollouts and be done?",
    "start": "998820",
    "end": "1006240"
  },
  {
    "text": "But here we're\nreally relying on p of tau being very easily\nevaluated, it seems.",
    "start": "1006240",
    "end": "1011274"
  },
  {
    "text": " I suppose if your\ntrajectories are really long,",
    "start": "1011275",
    "end": "1016379"
  },
  {
    "text": "maybe it'd be an\nissue, but I think there's a lot of other\nscaling issues you would run into before that.",
    "start": "1016380",
    "end": "1024079"
  },
  {
    "text": "But yeah, I think as we\ngo through the methods, we'll get more and more--",
    "start": "1024079",
    "end": "1030140"
  },
  {
    "text": "I don't want to say \"complex,\"\nbut maybe more and more involved. And that's because\nthey found problems as you try to scale\nthese things up.",
    "start": "1030140",
    "end": "1036060"
  },
  {
    "text": "And so we're going to about\nbetter and better ways to do these things such\nthat we can extend them beyond a simple Gaussian.",
    "start": "1036060",
    "end": "1041814"
  },
  {
    "text": "Thank you. Yeah. OK. So here's what we're doing.",
    "start": "1041815",
    "end": "1046920"
  },
  {
    "text": "We drew a bunch of samples\nfrom our nominal trajectory distribution. So that's shown here.",
    "start": "1046920",
    "end": "1052380"
  },
  {
    "text": "And then we've\nreweighted them so that we can try to fit a\nnew distribution to them. So all the samples\nthat are shown in black",
    "start": "1052380",
    "end": "1059909"
  },
  {
    "text": "have zero weight because\nthey're not failures. And then these are the\nweights-- it turns out they all have the same\nweight when we use",
    "start": "1059910",
    "end": "1065488"
  },
  {
    "text": "this as our initial proposal. But these are all\nthe weighted samples. So basically, what\nwe're doing is",
    "start": "1065488",
    "end": "1071100"
  },
  {
    "text": "we're fitting this\npurple distribution to these yellow samples\nbecause all of the black ones got zero weight. And so this ends up being\nsomewhat close to the failure",
    "start": "1071100",
    "end": "1078390"
  },
  {
    "text": "distribution, which is nice. And creating this purple\ndistribution that's equivalent to minimizing\nthe cross-entropy",
    "start": "1078390",
    "end": "1084840"
  },
  {
    "text": "between this purple distribution\nand the failure distribution, which we implicitly represent\nthrough our samples.",
    "start": "1084840",
    "end": "1093600"
  },
  {
    "text": "And so we can see now that\nthis has worked pretty well. So for direct estimation,\nit has this big variance.",
    "start": "1093600",
    "end": "1102370"
  },
  {
    "text": "And then similarly to\nwhat we've seen before, we get a much lower\nvariance and more accurate estimate when we use this\nproposal that we fit.",
    "start": "1102370",
    "end": "1111240"
  },
  {
    "text": "So that's all working well\nfor this threshold of minus 2. You can imagine, if we\ndecrease the threshold,",
    "start": "1111240",
    "end": "1117580"
  },
  {
    "text": "failures will become more rare. So we're going to get fewer\nand fewer samples that we can use to do this fitting.",
    "start": "1117580",
    "end": "1123760"
  },
  {
    "text": "And let's keep\ndecreasing it here. ",
    "start": "1123760",
    "end": "1129340"
  },
  {
    "text": "So I'm just going to tell you\nright now, for the next time step or for the\nnext decrease here,",
    "start": "1129340",
    "end": "1135170"
  },
  {
    "text": "we're going to end up\nwith no samples that lie within the failure region. So what do you think is going\nto happen to the algorithm?",
    "start": "1135170",
    "end": "1145510"
  },
  {
    "text": "There's no solution [INAUDIBLE] Yeah, there's-- you\ncan't do anything. ",
    "start": "1145510",
    "end": "1153280"
  },
  {
    "text": "[LAUGHTER] OK. So yeah, we're in trouble now. From here on out, we don't\nhave any-- all of our weights",
    "start": "1153280",
    "end": "1159730"
  },
  {
    "text": "are zeros. There's just nothing\nthat we can do. So this didn't work so well. So again, we're\nhaving this issue",
    "start": "1159730",
    "end": "1166000"
  },
  {
    "text": "that comes up over\nand over, which is like, if we start with\na nominal distribution, we're having all of\nthese issues now when",
    "start": "1166000",
    "end": "1171258"
  },
  {
    "text": "we have more rare failures. In reality, for\nexample, you just",
    "start": "1171258",
    "end": "1179200"
  },
  {
    "text": "keep sampling until you\nhave a sufficient support for the cross-entropy.",
    "start": "1179200",
    "end": "1185800"
  },
  {
    "text": "So you could just keep\nsampling until you have a sufficient support. But I'm going to show\nyou a technique right now",
    "start": "1185800",
    "end": "1191020"
  },
  {
    "text": "that will be much better.  So basically, what we\njust saw, if we sample",
    "start": "1191020",
    "end": "1198250"
  },
  {
    "text": "from the normal distribution\nand failures are rare, then we might not\nget any failures. And then all of our weights\nare zero, and we're like,",
    "start": "1198250",
    "end": "1204128"
  },
  {
    "text": "oh, we don't know what to do. So one thing that\nwe could do is we could start with the\nnormal distribution",
    "start": "1204128",
    "end": "1209980"
  },
  {
    "text": "and try to iteratively\nadapt it to move closer to the failure distribution. So rather than trying to make\nthis big jump all in one,",
    "start": "1209980",
    "end": "1216182"
  },
  {
    "text": "we start with a\nnormal distribution, and we take smaller jumps to\nmake our way to the failure distribution.",
    "start": "1216182",
    "end": "1222160"
  },
  {
    "text": "So let me show you\nhow to do that. And by the way, this is why\nit's called adaptive importance sampling because we're\ngoing to adapt this proposal",
    "start": "1222160",
    "end": "1228155"
  },
  {
    "text": "distribution. So here's how it works. What we do, before we start,\nwe pick an initial proposal",
    "start": "1228155",
    "end": "1234590"
  },
  {
    "text": "distribution to adapt. So maybe that's, for example,\nour nominal trajectory distribution.",
    "start": "1234590",
    "end": "1240110"
  },
  {
    "text": "And then the other\nthing we're going to need to do for this\nmethod, similar to what we've done in the past, is we need to\npick some notion of closeness",
    "start": "1240110",
    "end": "1246559"
  },
  {
    "text": "to failure. So again, in this\ncase, we could just use, for example,\nthe robustness. So maybe for some sample,\nsome sample trajectory,",
    "start": "1246560",
    "end": "1254610"
  },
  {
    "text": "the distance to failure\nor closeness to failure is just going to be this\ndistance to the failure region.",
    "start": "1254610",
    "end": "1261110"
  },
  {
    "text": "And we're going to add one more\nrequirement to this function f of tau that gives us our\ncloseness to failure, which",
    "start": "1261110",
    "end": "1266809"
  },
  {
    "text": "is that f of tau\nneeds to be less than or equal to 0 for all\nfailure trajectories.",
    "start": "1266810",
    "end": "1271950"
  },
  {
    "text": "So that way, we can\nrewrite this p tau given tau as a failure as p\ntau given f of tau is less than",
    "start": "1271950",
    "end": "1278720"
  },
  {
    "text": "or equal to 0. So we're just saying these\nare now equivalent conditions in terms of f of tau.",
    "start": "1278720",
    "end": "1284570"
  },
  {
    "text": "It'll be clear in\na minute or two why we want to actually\nwrite it like this instead.",
    "start": "1284570",
    "end": "1291260"
  },
  {
    "text": "OK, so this is our\nfailure region now. And we know that every\ntrajectory in this region will have an f of tau that's\nless than or equal to 0.",
    "start": "1291260",
    "end": "1301100"
  },
  {
    "text": "So that's our setup. And ideally, what we want\nto do, what we were just trying to do before,\nis we want to minimize",
    "start": "1301100",
    "end": "1307490"
  },
  {
    "text": "the cross-entropy between this\ndistribution and our failure distribution. So between this distribution\nand this p tau given f of tau",
    "start": "1307490",
    "end": "1316460"
  },
  {
    "text": "less than or equal to 0, which\nis our failure distribution. But we just saw that if\nwe draw a bunch of samples",
    "start": "1316460",
    "end": "1321477"
  },
  {
    "text": "from this distribution, it's\npossible that all of the weights are going to be 0 because\nwe didn't get any samples from the failure region.",
    "start": "1321478",
    "end": "1326970"
  },
  {
    "text": "So we're not actually able to\nminimize this cross-entropy. So this is what we do instead.",
    "start": "1326970",
    "end": "1332669"
  },
  {
    "text": "So we're going to take\nan iterative process now, where at each iteration,\nwe're going to draw samples from the current proposal.",
    "start": "1332670",
    "end": "1338730"
  },
  {
    "text": "So we have samples now from our\nnominal trajectory distribution. And then we're going\nto compute f of tau",
    "start": "1338730",
    "end": "1345260"
  },
  {
    "text": "for each of these samples. So each of their\ndistance to failure. And then we're going to pick\nthe top m elite samples.",
    "start": "1345260",
    "end": "1351669"
  },
  {
    "text": "So m elite is just some number. It's like a hyperparameter of\nthe algorithm you need to pick. So I think here we're taking the\ntop 50 or something like that.",
    "start": "1351670",
    "end": "1358692"
  },
  {
    "text": "And so we're going\nto just look at, what are the top 50\nobjective values? And then we're going\nto set a threshold",
    "start": "1358692",
    "end": "1365100"
  },
  {
    "text": "based on the highest objective\nvalue of all of those. So we're going to look at,\nwhat is the worst elite sample?",
    "start": "1365100",
    "end": "1370990"
  },
  {
    "text": "And we're going to pick a\nthreshold gamma to be there.",
    "start": "1370990",
    "end": "1376230"
  },
  {
    "text": "And then what we're going to\ndo with that threshold is now, instead of computing\nthe cross-entropy or minimizing the cross-entropy\nbetween this distribution",
    "start": "1376230",
    "end": "1383310"
  },
  {
    "text": "and the failure\ndistribution, is we're going to relax that\nassumption a little bit. And we're going to\nsay, instead, let's",
    "start": "1383310",
    "end": "1389190"
  },
  {
    "text": "minimize the cross-entropy\nbetween this distribution and p of tau given that\nf of tau is less than",
    "start": "1389190",
    "end": "1395280"
  },
  {
    "text": "or equal to this threshold. So it doesn't need to be\nless than or equal to 0. It doesn't need to\nbe a full failure. It just needs to be a little\nbit closer to failure.",
    "start": "1395280",
    "end": "1402760"
  },
  {
    "text": "And so we've defined\nthis new threshold that's maybe relaxed version\nof this problem here.",
    "start": "1402760",
    "end": "1409840"
  },
  {
    "text": "So now we know that,\nfor sure, we're going to have samples that have\nnonzero weight because we've picked the threshold to have at\nleast m elite of these samples.",
    "start": "1409840",
    "end": "1418880"
  },
  {
    "text": "And so we can fit\nour distribution to this set of samples now. So that's what we do.",
    "start": "1418880",
    "end": "1424670"
  },
  {
    "text": "We just fit the next\nproposal distribution using the weighted\nmaximum likelihood to that set of samples.",
    "start": "1424670",
    "end": "1430370"
  },
  {
    "text": "And then now we're\ngetting a little closer to the failure distribution. And if we just keep\nrepeating this process,",
    "start": "1430370",
    "end": "1435730"
  },
  {
    "text": "ideally, we'll get all the way\nto the failure distribution. So we'll repeat. We'll draw samples from\nthis distribution now.",
    "start": "1435730",
    "end": "1442630"
  },
  {
    "text": "We'll compute the threshold\nbased on these elite samples. We'll weight them and then\nfit a new distribution.",
    "start": "1442630",
    "end": "1449390"
  },
  {
    "text": "And we can just keep doing this\nfor some number of iterations. And then eventually,\nwe'll end up with--",
    "start": "1449390",
    "end": "1455620"
  },
  {
    "text": "we could look and see if our\nthreshold becomes less than 0. That means we've got\nenough failure samples to fit to the failure\ndistribution--",
    "start": "1455620",
    "end": "1462440"
  },
  {
    "text": "and then we end up with some\nkind of final distribution. Yeah. So I guess the idea\nis that you just",
    "start": "1462440",
    "end": "1469150"
  },
  {
    "text": "keep on moving the gamma\nall the way to the left until it gets to\nthe failure rate, and then you just entirely\ncover the failure distribution.",
    "start": "1469150",
    "end": "1476060"
  },
  {
    "text": "Yep. Yeah, exactly. So the question was, you just\nkeep moving gamma to the left until you get to\nthe failure region. So that's exactly right.",
    "start": "1476060",
    "end": "1481920"
  },
  {
    "text": "And I guess, specifically, it's\nto the left for this example. The more general idea\nis you're requiring",
    "start": "1481920",
    "end": "1487990"
  },
  {
    "text": "that you be closer and\ncloser to a failure, whatever your closeness metric is. So for the pendulum,\nthat might be",
    "start": "1487990",
    "end": "1493570"
  },
  {
    "text": "you're closer and closer to\nfalling over or something like that. Just guarantee that you're\ngoing to move closer to failure",
    "start": "1493570",
    "end": "1498850"
  },
  {
    "text": "every time or not necessarily. I don't know.",
    "start": "1498850",
    "end": "1504909"
  },
  {
    "text": "It's a stochastic algorithm. You can get unlucky, and\nthere's no guarantee. True.",
    "start": "1504910",
    "end": "1510310"
  },
  {
    "text": "We could get samples\nall the way over here and then fit a distribution. You're very-- Well. I guess you'd get\nall zeros there, but.",
    "start": "1510310",
    "end": "1516020"
  },
  {
    "text": "Yeah. Yeah. By the time that\nwe hit the failure or [AUDIO OUT] did we ever\nconvert our weight back",
    "start": "1516020",
    "end": "1523550"
  },
  {
    "text": "to the binary objective\nof value or not? Or do we always keep\nlosing, like [AUDIO OUT]",
    "start": "1523550",
    "end": "1529490"
  },
  {
    "text": "Oh, I think you want to--\nyou never want your threshold to become less than 0.",
    "start": "1529490",
    "end": "1535632"
  },
  {
    "text": "Is that the question\nyou're asking? You never want your\nthreshold to be less than 0? Yeah, because if you make\nyour threshold, be like--",
    "start": "1535632",
    "end": "1542070"
  },
  {
    "text": "let's say like we put\nour threshold over here, then we're going to fit a\ndistribution over there. But that's not going to be\na good proposal distribution",
    "start": "1542070",
    "end": "1548552"
  },
  {
    "text": "for importance sampling. You sort of then cap it at 0. You cap it at 0. Yeah. That's kind of a detail\nI didn't put in here.",
    "start": "1548552",
    "end": "1554070"
  },
  {
    "text": "But if you look at the\nimplementation in the book, you'll see that we\nactually max it with zero. Yeah.",
    "start": "1554070",
    "end": "1559820"
  },
  {
    "text": "Does your [INAUDIBLE] ",
    "start": "1559820",
    "end": "1565005"
  },
  {
    "text": "Yeah.  I have to think about that.",
    "start": "1565005",
    "end": "1571040"
  },
  {
    "text": "If it does anything like that,\nif it's not monotonically increasing, then\nif you're selecting",
    "start": "1571040",
    "end": "1577880"
  },
  {
    "text": "[INAUDIBLE] or something--\nor sorry, [INAUDIBLE]",
    "start": "1577880",
    "end": "1587417"
  },
  {
    "text": "I have to think about it because\nyou're drawing a lot-- you're drawing a lot of samples. So you might get over\none of those hills.",
    "start": "1587417",
    "end": "1593760"
  },
  {
    "text": "You're selecting like the\n[INAUDIBLE], and your top 10 and if it's not\nmonotonically increasing",
    "start": "1593760",
    "end": "1601950"
  },
  {
    "text": "in the direction of travel,\nyou will not be guaranteed. ",
    "start": "1601950",
    "end": "1607700"
  },
  {
    "text": "But if it's a cosine curve,\nI can select it anywhere. ",
    "start": "1607700",
    "end": "1614659"
  },
  {
    "text": "Yeah. Nothing's really guaranteed. But you can imagine\na failure where-- a failure definition\nwhere tau has to be--",
    "start": "1614660",
    "end": "1625190"
  },
  {
    "text": "the absolute value of f\nhas to be greater than 4",
    "start": "1625190",
    "end": "1630379"
  },
  {
    "text": "so you can have two different\nfailure regions, right?",
    "start": "1630380",
    "end": "1635960"
  },
  {
    "text": "So you might actually\ngo to one that you can use a different proposal\ndistribution like a Gaussian mixture.",
    "start": "1635960",
    "end": "1641900"
  },
  {
    "text": "Yeah. Does that answer your question? But I still think\nthe issue is not solved because there has to be\nsome guarantees that [INAUDIBLE]",
    "start": "1641900",
    "end": "1648525"
  },
  {
    "text": "If you make your\n[INAUDIBLE] to be cosine. And you're sending\nsomething to an [INAUDIBLE]",
    "start": "1648525",
    "end": "1654539"
  },
  {
    "text": "Well, it can't be cosine because\nyou need this requirement such that it's less\nthan or equal to 0. And it's a failure\nfor all of those.",
    "start": "1654540",
    "end": "1661890"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1661890",
    "end": "1667110"
  },
  {
    "text": "I'll have to think about\nit and get back to you. Typically, when you're\nusing robustness, you don't have an issue.",
    "start": "1667110",
    "end": "1673108"
  },
  {
    "text": "And that's typically\nwhat people use. But in general, yeah, there\nmight be other constraints on that function.",
    "start": "1673108",
    "end": "1678538"
  },
  {
    "text": "Was there another\nquestion over here? This is a similar question. Could you fit a\nmultimodal distribution",
    "start": "1678538",
    "end": "1683909"
  },
  {
    "text": "or do you-- whenever you\nfit the distribution, do you change the\ntype of distribution",
    "start": "1683910",
    "end": "1689160"
  },
  {
    "text": "that you fed from\niteration to iteration? Yeah. So the question is, could you\nfit a multimodal distribution?",
    "start": "1689160",
    "end": "1694870"
  },
  {
    "text": "And that's exactly right. So if you have\nmultiple failure modes, then you probably would want to\nfit a multimodal distribution.",
    "start": "1694870",
    "end": "1701120"
  },
  {
    "text": "So imagine there was\nanother threshold over here. Then our elite samples\nare going to be like over here and over here.",
    "start": "1701120",
    "end": "1706777"
  },
  {
    "text": "So we're to have these\ntwo groups of samples that we want to fit a\nsingle distribution to. And so we're probably\ngoing to want to fit like a mixture in that case.",
    "start": "1706777",
    "end": "1713070"
  },
  {
    "start": "1713070",
    "end": "1718659"
  },
  {
    "text": "OK. So yeah. Once we're done, we just\ntake our final proposal that now looks pretty good.",
    "start": "1718660",
    "end": "1724550"
  },
  {
    "text": "It looks kind of the\nfailure distribution. And we'll perform\nimportance sampling with this final proposal.",
    "start": "1724550",
    "end": "1730480"
  },
  {
    "text": "And just by the\nway, because we know what the failure distribution\nfor this system looks like, we can actually see\nhow well we did.",
    "start": "1730480",
    "end": "1735980"
  },
  {
    "text": "And you see we get pretty\nclose to the failure distribution in Gaussian form. So as close as we can get\nif we're fitting a Gaussian",
    "start": "1735980",
    "end": "1742900"
  },
  {
    "text": "distribution to things.  OK.",
    "start": "1742900",
    "end": "1748813"
  },
  {
    "text": "And then just kind of one\nother way to visualize it. So we've now graduated from 1D\nGaussians and we're moving up to 2D Gaussians.",
    "start": "1748813",
    "end": "1755150"
  },
  {
    "text": "So this is a similar problem\nthough, just in two dimensions. So our nominal\ndistribution is a Gaussian",
    "start": "1755150",
    "end": "1761350"
  },
  {
    "text": "that's centered at the origin\nhere with one covariance of the identity matrix.",
    "start": "1761350",
    "end": "1766850"
  },
  {
    "text": "And then our failure\nregion is up here. As you can see at\nthis first iteration",
    "start": "1766850",
    "end": "1771910"
  },
  {
    "text": "where we just draw samples\nfrom our nominal trajectory distribution, none\nof the samples overlap with the failure region.",
    "start": "1771910",
    "end": "1778100"
  },
  {
    "text": "So we pick the\nelite samples that are as close as we can\nget to the failure region,",
    "start": "1778100",
    "end": "1783950"
  },
  {
    "text": "and then we iteratively\ncontinue to adapt this threshold until we make our way to\nthe failure distribution.",
    "start": "1783950",
    "end": "1792850"
  },
  {
    "text": "Any questions on that? But this is not good, right? Because this is not even\ncovering or anything",
    "start": "1792850",
    "end": "1799720"
  },
  {
    "text": "close to it. Oh yeah. Good question. So the question was\nthis doesn't look good because it's like barely in\nthe failure distribution.",
    "start": "1799720",
    "end": "1806170"
  },
  {
    "text": "So I'll show in a minute a\nlittle more about this problem because we're going\nto use it again. But the nominal trajectory\ndistribution for this problem",
    "start": "1806170",
    "end": "1812413"
  },
  {
    "text": "is centered here\nwith unit variance. So the contours are\ngoing like this. So actually, the failure\ndistribution at this point,",
    "start": "1812413",
    "end": "1819510"
  },
  {
    "text": "it's very high likelihood here. Well, I won't say very high. Like highest likelihood here,\nand like much lower likelihood",
    "start": "1819510",
    "end": "1826520"
  },
  {
    "text": "out here because you're even\nfurther away from the origin. So we're trying to\nactually estimate the probability is negligible\nwhether or not [INAUDIBLE]",
    "start": "1826520",
    "end": "1835132"
  },
  {
    "text": "I don't know if you can say that\nfor sure, but in general, yeah. You want to-- the\nfailure distribution is very concentrated\nat this corner.",
    "start": "1835133",
    "end": "1841260"
  },
  {
    "text": "So we are doing a good job. Cool. So that's the\ncross-entropy method.",
    "start": "1841260",
    "end": "1847809"
  },
  {
    "text": "All right. So the next topic we're\ngoing to go back to-- we're going to stop adapting\nthings for a little bit and just go back to another\nversion of importance sampling.",
    "start": "1847810",
    "end": "1855740"
  },
  {
    "text": "And that's multiple\nimportance sampling. So we're back to this\nwhole indecisive thing that I've talked\nabout before where--",
    "start": "1855740",
    "end": "1862330"
  },
  {
    "text": "for a regular\nimportance sampling, we have to pick a\nproposal distribution. We only get to pick one. So we better choose wisely.",
    "start": "1862330",
    "end": "1868380"
  },
  {
    "text": "Or we could just do\nmultiple importance sampling and we could just pick\nmany proposal distributions",
    "start": "1868380",
    "end": "1873480"
  },
  {
    "text": "and we don't have this issue,\nwhich is perfect for me because I'm again,\nsuper indecisive.",
    "start": "1873480",
    "end": "1879532"
  },
  {
    "text": "So that's really all that's\ngoing on in multiple importance sampling is that we\njust do the same thing but with multiple\nproposal distributions.",
    "start": "1879532",
    "end": "1886580"
  },
  {
    "text": "So the steps are going\nto look very similar. Now we just draw samples from\nall of our current proposals.",
    "start": "1886580",
    "end": "1892200"
  },
  {
    "text": "So each I represents\na different proposal. And specifically, QI tau\nwould be the proposal",
    "start": "1892200",
    "end": "1898370"
  },
  {
    "text": "used to generate trajectory I. And so for example, let's say\nwe have two proposals like this.",
    "start": "1898370",
    "end": "1903720"
  },
  {
    "text": "So you could have more than two,\nwhich we'll see in a minute. But let's just say we\nhave Q1 tau and Q2 tau.",
    "start": "1903720",
    "end": "1909779"
  },
  {
    "text": "And we're just going to\ndraw samples from them. So maybe we'll just draw a\nbunch of samples from Q1. We'll draw a bunch\nof samples from Q2.",
    "start": "1909780",
    "end": "1917100"
  },
  {
    "text": "And then now we\nhave samples that we can use to estimate our\nprobability of failure. And then we're going to estimate\nour probability of failure",
    "start": "1917100",
    "end": "1924140"
  },
  {
    "text": "from these samples in a very\nsimilar way in that we're just going to take this weighted sum.",
    "start": "1924140",
    "end": "1930000"
  },
  {
    "text": "And the weight calculation\nis super similar. All we do is kind of almost\nexactly what you would think.",
    "start": "1930000",
    "end": "1935170"
  },
  {
    "text": "We computed density under\nthe nominal distribution, and then we compute its\ndensity under the distribution",
    "start": "1935170",
    "end": "1942090"
  },
  {
    "text": "or the proposal that\nit was sampled from. So the only difference\nhere is just this I here. So we just make sure\nthat we're computing",
    "start": "1942090",
    "end": "1948000"
  },
  {
    "text": "the density under\nthe distribution it was sampled from. It's not updating. Oh, that's weird.",
    "start": "1948000",
    "end": "1956040"
  },
  {
    "text": "Now it is. Oh. Very strange. OK. So here it is in a square.",
    "start": "1956040",
    "end": "1962760"
  },
  {
    "text": "So again, we're taking\nthis weighted sum, and the weights are the density\nunder the nominal distribution",
    "start": "1962760",
    "end": "1970590"
  },
  {
    "text": "divided by the density under\nthe proposal that it came from. And we call this standard\nmultiple importance sampling",
    "start": "1970590",
    "end": "1977100"
  },
  {
    "text": "or SMIS. It turns out in\nthis crazy fashion, there's actually\nanother way that you",
    "start": "1977100",
    "end": "1983250"
  },
  {
    "text": "could do this weighting\nthat's equally valid. So this kind of took me a minute\nto understand in my brain.",
    "start": "1983250",
    "end": "1989680"
  },
  {
    "text": "So we'll see how it goes. But I think it's kind of\ninteresting to think about. So what's going on\nhere is when we think",
    "start": "1989680",
    "end": "1996010"
  },
  {
    "text": "about standard\nimportance sampling, we're assuming that this\ndata, these sample points came from a specific process.",
    "start": "1996010",
    "end": "2002350"
  },
  {
    "text": "And that process\nis that we started with one of our\nproposals, so Q1 tau, and we drew a bunch of samples.",
    "start": "2002350",
    "end": "2008260"
  },
  {
    "text": "So this is what we saw before. And then we went over to Q2\nand we drew a bunch of samples from Q2.",
    "start": "2008260",
    "end": "2013930"
  },
  {
    "text": "And then we get weights kind\nof the way we would expect. But now there's also another\noption or another way",
    "start": "2013930",
    "end": "2020460"
  },
  {
    "text": "that this data could have been\ngenerated, which is like this. So what we could instead\ndo is say, at every time",
    "start": "2020460",
    "end": "2028410"
  },
  {
    "text": "that we're going to\ngenerate a new sample, we're going to pick\nrandomly between Q1 and Q2.",
    "start": "2028410",
    "end": "2034930"
  },
  {
    "text": "Let's say that we pick Q2. Then we're going to draw a\nsample from that distribution. Then when we draw\nthe next sample,",
    "start": "2034930",
    "end": "2040630"
  },
  {
    "text": "we're going to pick\nrandomly between the two. Let's say this time we pick Q1. And we draw a sample from\nQ1, pick randomly again,",
    "start": "2040630",
    "end": "2047340"
  },
  {
    "text": "draw a sample from Q2, and so on\nuntil we get all of our samples.",
    "start": "2047340",
    "end": "2052500"
  },
  {
    "text": "And the idea here is that\nthis process that I just showed you is actually\nequivalent to sampling from a mixture\nmodel of Q1 and Q2.",
    "start": "2052500",
    "end": "2060520"
  },
  {
    "text": "So we'll call this Q mix. And it has this density. And so we could also say\nthat all of these samples",
    "start": "2060520",
    "end": "2065580"
  },
  {
    "text": "were actually sampled\nfrom this mixture model. And it turns out\nthat if we do this,",
    "start": "2065580",
    "end": "2070600"
  },
  {
    "text": "we actually get a\ndifferent weight that we would want to use\nfor our important samples. It looks like this.",
    "start": "2070600",
    "end": "2075908"
  },
  {
    "text": "And it's actually\nconsidering the full mixture of those distributions\nwhen we compute the density of the sample.",
    "start": "2075909",
    "end": "2083677"
  },
  {
    "text": "And this is kind of crazy. Both of these estimates work. But it turns out that\nactually if you do it",
    "start": "2083677",
    "end": "2089908"
  },
  {
    "text": "this way, where you assume it\ncomes from this mixture model, it's been shown to\nhave lower variance.",
    "start": "2089909",
    "end": "2094989"
  },
  {
    "text": "So sometimes it\ncan perform better. Yeah. I don't know. This was totally not\nintuitive to me at first.",
    "start": "2094989",
    "end": "2100119"
  },
  {
    "text": "Yeah. Wait. So is it basically just the same\nbut you have different weights? It's the same, but you\nhave different weights.",
    "start": "2100120",
    "end": "2105820"
  },
  {
    "text": "OK. Yeah. I think you're reusing I. So\nI is sampled and [INAUDIBLE]",
    "start": "2105820",
    "end": "2115270"
  },
  {
    "text": "I'll fix that. Yeah. We should have a J. So everything in the sum is J? Everything in the sum is J.",
    "start": "2115270",
    "end": "2120580"
  },
  {
    "text": "Very cool. Well, the tau is I. The Q is J. That's true at the top one too.",
    "start": "2120580",
    "end": "2127510"
  },
  {
    "text": "The top one should be QI. You don't have number\nof distributions equal to number of samples.",
    "start": "2127510",
    "end": "2132820"
  },
  {
    "text": "You do. Well, the QI could be like Q1.",
    "start": "2132820",
    "end": "2138440"
  },
  {
    "text": "Let's say I took the\nsecond sample from Q1, then Q2 is equal to QI.",
    "start": "2138440",
    "end": "2144690"
  },
  {
    "text": "Sorry. That was a really\nconfusing way to say. You can have tau1,\ntau2, tau1, tau2. ",
    "start": "2144690",
    "end": "2153520"
  },
  {
    "text": "You have n-- you have 3,000\nsamples from [INAUDIBLE] Yeah. That's fine.",
    "start": "2153520",
    "end": "2158970"
  },
  {
    "text": "And then the first half of\nthem are from one and-- yeah. This is what we need. [INAUDIBLE]",
    "start": "2158970",
    "end": "2165220"
  },
  {
    "text": "Yeah. We were trying to\nmess with notation earlier in our\nmeeting for the book. It's not an easy\nthing, that's for sure.",
    "start": "2165220",
    "end": "2173055"
  },
  {
    "text": " Sorry. Can you clarify once\nagain what [INAUDIBLE]",
    "start": "2173055",
    "end": "2182540"
  },
  {
    "text": "Yeah. So here, let me write it out. This should be a J.\nAnd that should be a J.",
    "start": "2182540",
    "end": "2191905"
  },
  {
    "text": "Oh, OK. I'm sorry. [INAUDIBLE] Do that. Oh I'll make sure I fix these\nbefore I post the slides.",
    "start": "2191905",
    "end": "2197930"
  },
  {
    "text": "[INAUDIBLE] the I's\nfrom the distribution. From the distribution\nit came from? Tau I came from QI.",
    "start": "2197930",
    "end": "2204200"
  },
  {
    "text": "I mean, QI could be Q1. It could be Q2. It could be [INAUDIBLE]\nI don't know. Whatever it was.",
    "start": "2204200",
    "end": "2209570"
  },
  {
    "text": "Yeah. Yeah. Sorry. Just to clarify,\ncompared to SIMS,",
    "start": "2209570",
    "end": "2216040"
  },
  {
    "text": "like, the first sampling\npart using the Q1 and Q2, is that kind of-- do we just follow the\nsame procedure and then",
    "start": "2216040",
    "end": "2222609"
  },
  {
    "text": "when it comes to applying\nthe weights, we just perform the summation over\nthe two distributions? Yeah.",
    "start": "2222610",
    "end": "2227900"
  },
  {
    "text": "So this is a great question. So the idea is the\nprocedure for sampling",
    "start": "2227900",
    "end": "2233600"
  },
  {
    "text": "doesn't necessarily change. But we can-- once\nwe have the samples, we can assume that they came\nfrom any procedure we want.",
    "start": "2233600",
    "end": "2239040"
  },
  {
    "text": "So basically, the\nsecond one is assuming that they came from the second\nprocedure that I showed you, and the first one\nis assuming they",
    "start": "2239040",
    "end": "2244777"
  },
  {
    "text": "came from the first\nprocedure and they're equally valid assumptions. OK. So so the summation\nessentially just",
    "start": "2244777",
    "end": "2252410"
  },
  {
    "text": "creates the mixture\ndistribution. We don't have to [INAUDIBLE] Exactly. You don't have to actually\nsample it that way.",
    "start": "2252410",
    "end": "2258030"
  },
  {
    "text": "Yeah.  OK. So yeah. So this is kind of super\nunintuitive to me at first.",
    "start": "2258030",
    "end": "2264599"
  },
  {
    "text": "So no worries. You have to go back\nthrough it a few times. So we'll go back\nto this 2D problem.",
    "start": "2264600",
    "end": "2270539"
  },
  {
    "text": "And now we have\ntwo failure modes. So we have the upper left corner\nand the upper right corner. And we see the nominal\ntrajectory distribution is just",
    "start": "2270540",
    "end": "2278869"
  },
  {
    "text": "this Gaussian distribution\nthat's centered at the origin. And then the\nfailure distribution",
    "start": "2278870",
    "end": "2284250"
  },
  {
    "text": "is just going to be\nthat kind of cutoff. So again, like you\nsee, we-- now I'm actually showing the\nfailure distribution. We have much higher likelihood\nat these front corners here.",
    "start": "2284250",
    "end": "2291615"
  },
  {
    "text": "So that's where we'd\nwant to be sampling from to get a good estimate of\nthe probability of failure.",
    "start": "2291615",
    "end": "2297540"
  },
  {
    "text": "And so now we can look\nat what this looks like. So if we wanted to only pick\none importance sampling proposal",
    "start": "2297540",
    "end": "2303210"
  },
  {
    "text": "and let's say we wanted it to\nbe a Gaussian distribution, then we're going\nto have to pick one that's kind of big enough to\ncover both of these failure",
    "start": "2303210",
    "end": "2310290"
  },
  {
    "text": "modes. But if we now have\nthe opportunity to pick more than one importance\nsampling distribution,",
    "start": "2310290",
    "end": "2315849"
  },
  {
    "text": "we can spread them out\nto cover the regions that we think are important. So here we've now picked\na few distributions",
    "start": "2315850",
    "end": "2322710"
  },
  {
    "text": "that sample from those regions\nthat we think are important. And it turns out\nthat if we do this,",
    "start": "2322710",
    "end": "2329485"
  },
  {
    "text": "we can generally do\na little bit better than regular\nimportance sampling. So you can see-- it's kind of hidden\nbehind in gray there,",
    "start": "2329485",
    "end": "2335980"
  },
  {
    "text": "but the regular importance\nsampling estimator has this kind of big\nvariance over here.",
    "start": "2335980",
    "end": "2341320"
  },
  {
    "text": "And then you can also\nsee what's really interesting is like\nwe said, sometimes this deterministic\nmixture assumption",
    "start": "2341320",
    "end": "2347280"
  },
  {
    "text": "can actually do better than the\nstandard multiple importance sampling. So if we weight it according\nto the deterministic mixture,",
    "start": "2347280",
    "end": "2354960"
  },
  {
    "text": "we get this line here. ",
    "start": "2354960",
    "end": "2363660"
  },
  {
    "text": "OK. So that's multiple\nimportance sampling.",
    "start": "2363660",
    "end": "2368740"
  },
  {
    "text": "So when we looked at the\ncross entropy method, we saw that we had trouble kind\nof picking our own importance",
    "start": "2368740",
    "end": "2375600"
  },
  {
    "text": "sampling proposal where\nwe just had to pick one. So we were like, let's\npick one and adapt it. We're going to do\nthe same exact idea,",
    "start": "2375600",
    "end": "2382260"
  },
  {
    "text": "but now with multiple\nimportance sampling. So we might be like, OK, we\nnow get to pick more than one, but that's more things\nwe have to choose.",
    "start": "2382260",
    "end": "2388930"
  },
  {
    "text": "How are we going to choose\nall of those proposals? And so what population\nMonte Carlo allows us to do is take multiple importance\nsampling proposals",
    "start": "2388930",
    "end": "2396120"
  },
  {
    "text": "and adapt those to get a\nbetter set of proposals. So the key idea here is let's\nadapt a whole population",
    "start": "2396120",
    "end": "2404349"
  },
  {
    "text": "of proposal distributions. So first I'm just going\nto visually show you what this looks\nlike, and then I'll",
    "start": "2404350",
    "end": "2410410"
  },
  {
    "text": "show you the actual\nsteps of the algorithm. But basically, what\nwe do is we start out with a whole bunch of proposals.",
    "start": "2410410",
    "end": "2416510"
  },
  {
    "text": "So every single one of these--\nthis is one distribution. This is another proposal. This is another\nproposal and so on. So we have this whole\nset of many proposals.",
    "start": "2416510",
    "end": "2424030"
  },
  {
    "text": "Oops. And then what we're going to\ndo-- the first step that we're going to do is we're\njust going to draw",
    "start": "2424030",
    "end": "2430870"
  },
  {
    "text": "a sample from every single\none of these proposals. So each sample or each proposal\nwe just get one sample from.",
    "start": "2430870",
    "end": "2436790"
  },
  {
    "text": "So here's our sample from this\nproposal, from this proposal, and so on. And now we're left with a bunch\nof samples from our proposals.",
    "start": "2436790",
    "end": "2443772"
  },
  {
    "text": "And the next thing\nthat we're going to do is weight these samples\naccording to their importance",
    "start": "2443772",
    "end": "2449950"
  },
  {
    "text": "weights. So any sample that's not\nin the failure region is just going to get 0 weight. And then the samples that\nare in the failure region",
    "start": "2449950",
    "end": "2457030"
  },
  {
    "text": "are going to be weighted\naccording to that P over Q. So p being what\nit's weight or what",
    "start": "2457030",
    "end": "2462700"
  },
  {
    "text": "its density is under the\nnominal trajectory distribution. So that's why you\nsee brighter values",
    "start": "2462700",
    "end": "2467860"
  },
  {
    "text": "where the nominal distribution\nwill have a higher density, divided by Q, which\nwould be the density that--",
    "start": "2467860",
    "end": "2474040"
  },
  {
    "text": "from the proposal\nthat it came from. And then what we're\ngoing to do is",
    "start": "2474040",
    "end": "2479410"
  },
  {
    "text": "we're actually going to\nresample these samples according to these weights. So we're never going to get any\nnew samples outside the failure",
    "start": "2479410",
    "end": "2485885"
  },
  {
    "text": "region because they\nall have a weight of 0. And then we're going to be\nmore likely to resample points in areas where they\nhave a high weight.",
    "start": "2485885",
    "end": "2494260"
  },
  {
    "text": "So that's what this looks like. And it looks like we\njust kind of decreased the number of samples a ton. We didn't actually decrease\nthe number of samples,",
    "start": "2494260",
    "end": "2501230"
  },
  {
    "text": "but they're all kind\nof on top of each other here because we've resampled\nthe same amount of samples, but from a distribution\nwhere it was",
    "start": "2501230",
    "end": "2507849"
  },
  {
    "text": "more likely to sample something\nthat had a higher importance weight. So this might be like five\nsamples all on top of each other",
    "start": "2507850",
    "end": "2513580"
  },
  {
    "text": "here. So we still have the\nsame amount of samples. We've just resampled them.",
    "start": "2513580",
    "end": "2518900"
  },
  {
    "text": "Yeah. Sorry. Where did the-- so these are\nall samples, not distributions?",
    "start": "2518900",
    "end": "2524233"
  },
  {
    "text": "These are all samples. If we only had a bunch\nof distribution prior on, how do we know what\nsample corresponding to what distribution or how many\nsamples from each distribution?",
    "start": "2524233",
    "end": "2531500"
  },
  {
    "text": "I think you can decide how\nmany you want to sample from each distribution. So at this step, we\njust sampled one point",
    "start": "2531500",
    "end": "2536570"
  },
  {
    "text": "from every distribution. Right. But then like-- so\ntwo steps later, now you have a bunch\nof points and you",
    "start": "2536570",
    "end": "2542390"
  },
  {
    "text": "can fit the same number\nof distributions out? Like-- Well, I haven't finished that. OK.",
    "start": "2542390",
    "end": "2547700"
  },
  {
    "text": "Got it. Sorry. I think you'll see in\nprobably exactly right now. So now we have a\nbunch of new samples,",
    "start": "2547700",
    "end": "2554330"
  },
  {
    "text": "and all we do is\nput new proposals. Kind of slap new\nproposals on them. So we just say maybe they're--",
    "start": "2554330",
    "end": "2559490"
  },
  {
    "text": "our new set of\nproposals is going to be centered--\neach proposal is going to be centered\nat our sample",
    "start": "2559490",
    "end": "2564740"
  },
  {
    "text": "with some constant variance. So all we did was take all\nthe samples here and turn them into proposals by saying,\nwe'll just make a new proposal",
    "start": "2564740",
    "end": "2572060"
  },
  {
    "text": "where the mean of that proposal\nis the sample that we had, and we put maybe some\nconstant variance with them.",
    "start": "2572060",
    "end": "2578710"
  },
  {
    "text": " OK. And so now we're back\nto where we started, and so you could imagine we\ncould repeat this process",
    "start": "2578710",
    "end": "2585508"
  },
  {
    "text": "and continue to try to\nimprove these proposals. But already this is\nlooking pretty good",
    "start": "2585508",
    "end": "2590970"
  },
  {
    "text": "because if you remember\nthese kind of proposals that we hand designed for\nthis same exact problem looked",
    "start": "2590970",
    "end": "2596310"
  },
  {
    "text": "like this. And we kind of have\nalready automatically found a very similar set of proposals.",
    "start": "2596310",
    "end": "2603069"
  },
  {
    "text": "So this might work well. So let's put some actual\nsteps to what we just did.",
    "start": "2603070",
    "end": "2609460"
  },
  {
    "text": "So again, we start with\nsome initial population of proposals that's spread\nout over the entire space of possible trajectories.",
    "start": "2609460",
    "end": "2616470"
  },
  {
    "text": "And one note that\nI want to give here is that it's very important\nthat this initial population of proposals cover the\nspace of possibilities",
    "start": "2616470",
    "end": "2623880"
  },
  {
    "text": "as best as possible,\nbecause if we never have any samples in the\nfailure region, then all our weights\nare going to be 0.",
    "start": "2623880",
    "end": "2629980"
  },
  {
    "text": "Same problem as\nbefore, and we're not going to actually find anything. So we need to make sure-- and this is a\nnon-trivial thing to do.",
    "start": "2629980",
    "end": "2635470"
  },
  {
    "text": "This can be very\ndifficult. So we need to make sure\nthough that we pick a bunch of proposals\nthat will cover",
    "start": "2635470",
    "end": "2640980"
  },
  {
    "text": "the space of possibilities.  And then after this, we drew\na sample from each proposal.",
    "start": "2640980",
    "end": "2650190"
  },
  {
    "text": "And then we computed\nthe importance weight for each sample. So that was that P\nover Q, multiplied by the indicator function.",
    "start": "2650190",
    "end": "2658140"
  },
  {
    "text": "And then we resampled based\non those importance weights. So we were more likely to get\nnew samples where the importance",
    "start": "2658140",
    "end": "2665580"
  },
  {
    "text": "weights were high. And then we just kind of\nslapped on a new proposal to all of our new resample.",
    "start": "2665580",
    "end": "2671590"
  },
  {
    "text": "So again, it's not\nlike we decrease the number of proposals. They're just kind of\non top of each other. Yeah.",
    "start": "2671590",
    "end": "2678330"
  },
  {
    "text": "And then we can, like\nI said, repeat this until we're satisfied for\nsome set number of iterations.",
    "start": "2678330",
    "end": "2683710"
  },
  {
    "text": "And then once we're done,\nwe will take these proposals that we ended up with and\nperform multiple importance",
    "start": "2683710",
    "end": "2688859"
  },
  {
    "text": "sampling with them\nto try to estimate our probability of failure. ",
    "start": "2688860",
    "end": "2696369"
  },
  {
    "text": "Any questions on\npopulation Monte Carlo? Yeah. For each acceptance sample,\nwe add some variance",
    "start": "2696370",
    "end": "2704740"
  },
  {
    "text": "that we allow for\nthe sampling process and that variance that we\nallow for the population is",
    "start": "2704740",
    "end": "2711700"
  },
  {
    "text": "like a heuristic or something? Yeah, exactly. So the question was like so\nfor each sample we have left,",
    "start": "2711700",
    "end": "2716960"
  },
  {
    "text": "we now make these\nnew distributions. Like, how do we\npick the variance we're going to put on them? Yeah. It's kind of a hyperparameter.",
    "start": "2716960",
    "end": "2722660"
  },
  {
    "text": "So you don't even have\nto make it Gaussian. It's kind of up to you how\nyou want to take a sample and turn that into\na distribution.",
    "start": "2722660",
    "end": "2728900"
  },
  {
    "text": "But one very\nstraightforward way is to just say that it's a\nGaussian distribution centered at that sample with\nsome constant variance.",
    "start": "2728900",
    "end": "2735590"
  },
  {
    "text": "And you can imagine that the\nbigger variance that you pick, maybe the more exploration\nof the space you'll get.",
    "start": "2735590",
    "end": "2741260"
  },
  {
    "text": "But then there's this\nless exploitation because you knew that\nthat sample was good because it had a high weight.",
    "start": "2741260",
    "end": "2746330"
  },
  {
    "text": "So yeah, it's something\nyou have to balance.  [INAUDIBLE]",
    "start": "2746330",
    "end": "2753015"
  },
  {
    "text": "To like, decide the parameter? To decide the parameters? Is that what you're saying? Yeah.",
    "start": "2753016",
    "end": "2758140"
  },
  {
    "text": "You could do some any\nhyperparameter optimization. Yeah. ",
    "start": "2758140",
    "end": "2764920"
  },
  {
    "text": "OK. So now we're going to\nextend population Monte Carlo to something called\nsequential Monte Carlo.",
    "start": "2764920",
    "end": "2772359"
  },
  {
    "text": "And so the idea with\nsequential Monte Carlo is with population Monte Carlo,\nwe had all these samples, but we were still maintaining\nthese explicit distributions",
    "start": "2772360",
    "end": "2779590"
  },
  {
    "text": "around them. So we still had these\nGaussian distributions that we had a density\nfor and everything.",
    "start": "2779590",
    "end": "2784780"
  },
  {
    "text": "And it's not always\nsuper easy to do this. So in general, when you\nhave very complex failure",
    "start": "2784780",
    "end": "2790180"
  },
  {
    "text": "distributions,\nyou might not want to maintain these very specific\nparametric distributions like Gaussian distributions\nor something like that.",
    "start": "2790180",
    "end": "2796897"
  },
  {
    "text": "And it kind of gets to\nthat second challenge we talked about at the\nvery beginning of lecture, where it's not super\neasy to just fit",
    "start": "2796897",
    "end": "2803529"
  },
  {
    "text": "a distribution to\na set of samples from a very complex,\nmulti-modal, high dimensional",
    "start": "2803530",
    "end": "2808660"
  },
  {
    "text": "failure distribution. And so maybe what we\nwant to do instead is just take a bunch of samples.",
    "start": "2808660",
    "end": "2814756"
  },
  {
    "text": "And now we're only going\nto keep track of samples. We're never going\nto actually pick some type of\nGaussian distribution",
    "start": "2814757",
    "end": "2819950"
  },
  {
    "text": "or anything like that. And what we're\ngoing to do is we're going to try to move samples\nfrom the nominal distribution.",
    "start": "2819950",
    "end": "2825840"
  },
  {
    "text": "So we have a set\nof samples that we took from the nominal\ntrajectory distribution, and we're going to\ntry to move those",
    "start": "2825840",
    "end": "2831290"
  },
  {
    "text": "to the failure distribution. And it turns out\nthat we can estimate the probability of\nfailure based on the path",
    "start": "2831290",
    "end": "2837890"
  },
  {
    "text": "that the samples\ntake as we move them from the nominal distribution\nto the failure distribution.",
    "start": "2837890",
    "end": "2843337"
  },
  {
    "text": "So this might all\nseem like magic now. I'm just going to very slowly\ngo through how we do this.",
    "start": "2843337",
    "end": "2849740"
  },
  {
    "text": "OK. So typically the way--\nlet's start with just like how do we even-- we\nhave a bunch of samples from the nominal distribution.",
    "start": "2849740",
    "end": "2855040"
  },
  {
    "text": "How do we move them to\nthe failure distribution? What's going on there? And typically what people\ndo is they move samples",
    "start": "2855040",
    "end": "2861440"
  },
  {
    "text": "through a series of\nintermediate distributions. OK. And so then how\ndo we get a series",
    "start": "2861440",
    "end": "2867187"
  },
  {
    "text": "of intermediate\ndistributions that go from the nominal distribution\nto the failure distribution? It turns out we can use this\nidea of smoothing that we",
    "start": "2867187",
    "end": "2874630"
  },
  {
    "text": "used when we were doing MCMC. So let me show you\nwhat that looks like. ",
    "start": "2874630",
    "end": "2882070"
  },
  {
    "text": "What we want to do\nis we want to move from the normal distribution\nto the failure distribution. When we first\nintroduced smoothing,",
    "start": "2882070",
    "end": "2887924"
  },
  {
    "text": "we actually kind of\nwent the other way. So we said we have--\nwhen smoothing is 0, our distribution is\nexactly equivalent",
    "start": "2887925",
    "end": "2894130"
  },
  {
    "text": "to the unnormalized density\nof the failure distribution. And then we said as we increase\nthe amount of smoothing,",
    "start": "2894130",
    "end": "2900430"
  },
  {
    "text": "we start to assign\nnon-zero probability. So we're increasing\nthat epsilon value, that variance in the\nnormal distribution",
    "start": "2900430",
    "end": "2907090"
  },
  {
    "text": "that we use to replace\nthe indicator function. And we said, as we keep\nincreasing it, we actually",
    "start": "2907090",
    "end": "2912609"
  },
  {
    "text": "move from the\nfailure distribution to the nominal distribution.",
    "start": "2912610",
    "end": "2917890"
  },
  {
    "text": "And so our goal here is\nnow kind of the opposite. We want to go from the\nnominal distribution to the failure distribution.",
    "start": "2917890",
    "end": "2923347"
  },
  {
    "text": "We want to find a series of\nintermediate distributions in between the two. So we can just do\nthis in reverse.",
    "start": "2923347",
    "end": "2928670"
  },
  {
    "text": "So we just start at infinity\nand slowly decrease this epsilon parameter until we get to\nthe nominal distribution.",
    "start": "2928670",
    "end": "2939280"
  },
  {
    "text": "One more time. This is kind of a series of\nintermediate distributions that gets you back to\nthe full distribution.",
    "start": "2939280",
    "end": "2946910"
  },
  {
    "text": "Let me say that backwards. We can also do this in\n2D for our 2D problem. So here in two\ndimensions, we're trying",
    "start": "2946910",
    "end": "2953529"
  },
  {
    "text": "to get from the nominal\ndistribution, which is this Gaussian that's\ncentered at the origin,",
    "start": "2953530",
    "end": "2958930"
  },
  {
    "text": "to the failure distribution,\nwhich looks like this up here. And we end up getting\nthese teardrop shaped",
    "start": "2958930",
    "end": "2965770"
  },
  {
    "text": "intermediate distributions if we\njust apply this smoothing where we move from one to the other.",
    "start": "2965770",
    "end": "2973030"
  },
  {
    "start": "2973030",
    "end": "2983140"
  },
  {
    "text": "Cool. Any questions on that? ",
    "start": "2983140",
    "end": "2988870"
  },
  {
    "text": "Yes. No? It's like magical. Yeah. It's pretty cool, right? Yeah. [INAUDIBLE] I'm not\nsure what's going on.",
    "start": "2988870",
    "end": "2996610"
  },
  {
    "text": "Oh, you're not sure\nwhat's going on. Another way to have magic.",
    "start": "2996610",
    "end": "3002430"
  },
  {
    "text": "So does this one make sense or? No, no, no. Like, number goes down\nand distribution moves.",
    "start": "3002430",
    "end": "3008950"
  },
  {
    "text": "That seems very-- So this number, though,\nis that the variance-- remember how we replaced\nthe indicator function with",
    "start": "3008950",
    "end": "3015270"
  },
  {
    "text": "a Gaussian-- a normal distribution\nthat was like-- had this kind of small variance? And we said, as we\nmove the variance--",
    "start": "3015270",
    "end": "3021970"
  },
  {
    "text": "so we're doing this\nnormal distribution with small variance multiplied\nby the nominal distribution.",
    "start": "3021970",
    "end": "3027910"
  },
  {
    "text": "And as we increase\nthat variance, we actually go from the\nfailure distribution to the nominal distribution.",
    "start": "3027910",
    "end": "3037045"
  },
  {
    "text": "OK. Let me just check out. Yeah. Do you have the equation\nof it somewhere?",
    "start": "3037045",
    "end": "3044339"
  },
  {
    "text": "OK. There we go. I don't know how this\nis doing this, but. Well, we ignore\nthe normalization.",
    "start": "3044340",
    "end": "3050150"
  },
  {
    "text": "That's just to get the-- like,\nthe values to look good, but. There you go.",
    "start": "3050150",
    "end": "3056620"
  },
  {
    "text": "So this is the\nnominal distribution. Yeah. And then this is that smoothed\nversion of the indicator",
    "start": "3056620",
    "end": "3064809"
  },
  {
    "text": "function where epsilon\nis the variance of that. Oh, I see. OK, great. Thanks.",
    "start": "3064810",
    "end": "3071020"
  },
  {
    "text": "All right. See, this is a great\nad for Pluto notebooks. The code is right there,\nas messy as it might be.",
    "start": "3071020",
    "end": "3077960"
  },
  {
    "text": "Yeah.  So doing the smoothing,\nI guess, relies",
    "start": "3077960",
    "end": "3083590"
  },
  {
    "text": "on us being able to\nsoften up this constraint with this indicator\nfunction thing.",
    "start": "3083590",
    "end": "3089170"
  },
  {
    "text": "But that kind of\nimplies that always we can define having a failure not\njust in a binary sense, yes, no,",
    "start": "3089170",
    "end": "3096620"
  },
  {
    "text": "but give it an actual number\nthat we can loosen up, right?",
    "start": "3096620",
    "end": "3102190"
  },
  {
    "text": "Do we always assume that the\nfailures are numerically-- can we assign a\nvalue to the failure",
    "start": "3102190",
    "end": "3108610"
  },
  {
    "text": "and not just say, yes, this is a\nfailure, this is not a failure? Yeah. That's a great question. So typically, what we use\nfor that is robustness.",
    "start": "3108610",
    "end": "3116578"
  },
  {
    "text": "So anything you can write\ndown in signal temporal logic you can define that value for. But yeah, in general,\nif you can't write down",
    "start": "3116578",
    "end": "3123010"
  },
  {
    "text": "your specification\nthat way, then maybe it won't be a\nstraightforward how to pick something that\ngives you that property.",
    "start": "3123010",
    "end": "3130130"
  },
  {
    "text": "OK. ",
    "start": "3130130",
    "end": "3135470"
  },
  {
    "text": "So here's what we\njust showed, but now I'm actually showing all of\nthe intermediate distributions in between.",
    "start": "3135470",
    "end": "3140780"
  },
  {
    "text": "Or we've picked\nsome set of epsilons that will give us this\nnice smooth transition",
    "start": "3140780",
    "end": "3145840"
  },
  {
    "text": "from the nominal distribution\nto the failure distribution. There is one other way to do\nthis that's way less cool.",
    "start": "3145840",
    "end": "3152329"
  },
  {
    "text": "Seems way less like\nmagic, but it also works well, which\nis similar to what we were doing in the\ncross-entropy method, where",
    "start": "3152330",
    "end": "3158110"
  },
  {
    "text": "we specify this threshold\non the distance to failure, and then we just decrease\nthat threshold as we go.",
    "start": "3158110",
    "end": "3164010"
  },
  {
    "text": "So that's another way\nthat you can also do this.",
    "start": "3164010",
    "end": "3169190"
  },
  {
    "text": "So now we've talked about\nhow we get this series of intermediate distributions.",
    "start": "3169190",
    "end": "3174619"
  },
  {
    "text": "How do we actually go\nabout moving samples from one distribution\nto the next? Because we're going\nto start with samples",
    "start": "3174620",
    "end": "3179780"
  },
  {
    "text": "from the nominal distribution,\nand we need to figure out how we can move them from\nthe nominal distribution through all of those\nintermediate distributions.",
    "start": "3179780",
    "end": "3187190"
  },
  {
    "text": "And it turns out that for these\nintermediate distributions, if you remember when\nwe applied smoothing, we just applied it to\nthe unnormalized density",
    "start": "3187190",
    "end": "3196023"
  },
  {
    "text": "of the failure distribution. So all of these densities\nfor the smooth distributions are also going to\nbe unnormalized.",
    "start": "3196023",
    "end": "3201240"
  },
  {
    "text": "And how do we sample from\nunnormalized distributions? Well MCMC, of course. So we're going to do this\nusing Markov chain Monte Carlo.",
    "start": "3201240",
    "end": "3209599"
  },
  {
    "text": "So let's say that\nwe've picked out our intermediate distribution. So we picked a few\ndifferent values of epsilon.",
    "start": "3209600",
    "end": "3214800"
  },
  {
    "text": "And we've gotten these\nsmooth distributions now. And so we want to go from\nsamples from this distribution,",
    "start": "3214800",
    "end": "3220068"
  },
  {
    "text": "which is the nominal one. We know how to draw samples\nfrom this distribution through these series of\nintermediate distributions",
    "start": "3220068",
    "end": "3226110"
  },
  {
    "text": "to the failure distribution. OK. So now let's say that we just\nwant to look at this first step.",
    "start": "3226110",
    "end": "3232167"
  },
  {
    "text": "So we want to go from\nthis blue distribution to this kind of purplish one. And so what we do is\nwe get our samples",
    "start": "3232167",
    "end": "3238380"
  },
  {
    "text": "from the blue distribution. So again, we know\nhow to get these from the nominal\ntrajectory distribution. And now what I'm about to show\nyou is for-- imagine for each",
    "start": "3238380",
    "end": "3246630"
  },
  {
    "text": "sample-- so let's say we just\nstart at this sample here. We're going to kick\noff an MCMC chain",
    "start": "3246630",
    "end": "3252960"
  },
  {
    "text": "from this sample with this\ndistribution as the target distribution. So you can imagine it's going\nto wiggle around for a while",
    "start": "3252960",
    "end": "3259619"
  },
  {
    "text": "and eventually move itself\nto these more high likelihood areas of this distribution.",
    "start": "3259620",
    "end": "3264780"
  },
  {
    "text": "And we're going to kick\noff a different MCMC chain for every single\none of these samples. And we're going to do some\nnumber of iterations for that,",
    "start": "3264780",
    "end": "3271240"
  },
  {
    "text": "and they should slowly wiggle\ntheir way to being from the-- being samples from\nthis distribution here.",
    "start": "3271240",
    "end": "3279630"
  },
  {
    "text": "So let's see what\nthat looks like. Wow.",
    "start": "3279630",
    "end": "3285060"
  },
  {
    "text": "Pretty cool. Maybe we'll see\nit one more time. So what's happening is that\neach of those kind of movements",
    "start": "3285060",
    "end": "3291480"
  },
  {
    "text": "is one MCMC step\nfor those samples. And then I took that\nwas just 50 MCMC steps,",
    "start": "3291480",
    "end": "3297190"
  },
  {
    "text": "and now these samples\nappear like they come from this\nintermediate distribution.",
    "start": "3297190",
    "end": "3303330"
  },
  {
    "text": "Now we want to get to\nthe next distribution. Just do the same thing. So kick off an MCMC\nchain from each one",
    "start": "3303330",
    "end": "3308730"
  },
  {
    "text": "of these samples with this-- the density of this distribution\nas our target density",
    "start": "3308730",
    "end": "3314410"
  },
  {
    "text": "that's deciding whether we\naccept or reject these samples. And we just do it all again.",
    "start": "3314410",
    "end": "3320430"
  },
  {
    "text": "Yeah. So when you [INAUDIBLE] do we\nfit a distribution to these and then resample and\nthen kick off from those,",
    "start": "3320430",
    "end": "3327580"
  },
  {
    "text": "or do we just take what we\nhad that's already moved and just keep moving them?",
    "start": "3327580",
    "end": "3332682"
  },
  {
    "text": "In what I'm showing you now,\nwe just take what we had and we just move them.",
    "start": "3332682",
    "end": "3338799"
  },
  {
    "text": "There is actually often\npeople do this sampling step, which I'm going to talk about\nin a second, where it generally",
    "start": "3338800",
    "end": "3344800"
  },
  {
    "text": "makes the MCMC more efficient\nif you do some of sampling. But for now, we're just\nsaying we took some samples,",
    "start": "3344800",
    "end": "3350900"
  },
  {
    "text": "we kicked off an MCMC chain with\nthis as the target distribution. Then we took those exact samples\nwe ended with and kicked off",
    "start": "3350900",
    "end": "3357430"
  },
  {
    "text": "another chain with this now\nas the target distribution. And in theory, we can also just\nnot smooth out this, kind of",
    "start": "3357430",
    "end": "3364825"
  },
  {
    "text": "try to have a giant-- not a jump, but in\ncase the density",
    "start": "3364825",
    "end": "3371410"
  },
  {
    "text": "is non-zero throughout\nthe entire space, then we can also\njust hope that MCMC will get there by itself if we\ndidn't do any of this, right?",
    "start": "3371410",
    "end": "3378890"
  },
  {
    "text": "Yeah. So we don't technically\nhave to smooth it. We could just go straight\nto the failure distribution, but MCMC is going to be\na lot less efficient.",
    "start": "3378890",
    "end": "3384995"
  },
  {
    "text": "It makes sense. Yeah. Here you need to pick like\na sequence of epsilons",
    "start": "3384995",
    "end": "3390490"
  },
  {
    "text": "that define each of\nthe different smoothing distributions. It seems to me in general\nquite difficult to pick",
    "start": "3390490",
    "end": "3397450"
  },
  {
    "text": "the right sequence or a\nuseful sequence of epsilons if you don't know a lot\nabout your kind of situation.",
    "start": "3397450",
    "end": "3404570"
  },
  {
    "text": "Does it sound about right? Yeah. The question is like, how\ndifficult is it to pick them?",
    "start": "3404570",
    "end": "3410255"
  },
  {
    "text": "Would be a good person\nto talk to about this. He has more experience\nwith trying to do this. But I think in general, you can\nat least look at how you did.",
    "start": "3410255",
    "end": "3419165"
  },
  {
    "text": "Like, you can look at\nthe samples by the end. I'll show you an example\nwith the pendulum and you can see if your\nsamples towards the end",
    "start": "3419165",
    "end": "3424930"
  },
  {
    "text": "are starting to move\ntowards failures. You can at least look at the\nsamples that you're getting. And if you find that\nthere's this huge jump",
    "start": "3424930",
    "end": "3431332"
  },
  {
    "text": "between the last step and your\nactual failure distribution, then you probably need\nto re-pick epsilon. So there's ways to at least\nkind of guess and check.",
    "start": "3431332",
    "end": "3437805"
  },
  {
    "text": " So then we have one\nmore step to go through.",
    "start": "3437805",
    "end": "3442940"
  },
  {
    "text": "So we'll just again, start\nwith all of these samples and do a bunch of MCMC steps. And now we've kind of made our\nway to the failure distribution.",
    "start": "3442940",
    "end": "3449270"
  },
  {
    "text": "I think there's one more\nstep that I didn't show, which is like now you actually\ndo it with the failure distribution as the target. But I don't have the\nanimation for that.",
    "start": "3449270",
    "end": "3455950"
  },
  {
    "text": "OK. So then we have all\nof these samples. But we're trying to estimate\nthe probability of failure.",
    "start": "3455950",
    "end": "3461595"
  },
  {
    "text": "So how do we actually estimate\nthe probability of failure from all this? And so like I said, it turns out\nthat we could look at the path",
    "start": "3461595",
    "end": "3467900"
  },
  {
    "text": "that each sample took\nand get an estimate for the probability of failure. So what I'm going to\ndo now is I'm just",
    "start": "3467900",
    "end": "3473000"
  },
  {
    "text": "going to label these\nintermediate distributions. So G1 is always going to be our\nnominal trajectory distribution.",
    "start": "3473000",
    "end": "3480030"
  },
  {
    "text": "And then Gn, in\nthis case, G5 will be the failure distribution. And then it turns out what\nwe do is we keep track",
    "start": "3480030",
    "end": "3486860"
  },
  {
    "text": "of a weight for each sample. So we initialize all\nthe weights to one. And then every time\nwe make a transition,",
    "start": "3486860",
    "end": "3492260"
  },
  {
    "text": "we keep track of a\nweight that's basically the previous weight\ntimes the likelihood under the new\ndistribution divided",
    "start": "3492260",
    "end": "3498650"
  },
  {
    "text": "by the likelihood under\nthe old distribution that we were coming from.",
    "start": "3498650",
    "end": "3503780"
  },
  {
    "text": "It's not obvious that\nthis works at all. And it turns out\nwe thought about including the proof in the\nbook and it was very involved.",
    "start": "3503780",
    "end": "3511760"
  },
  {
    "text": "So if you're interested in\nwhere this all comes from, there's a reference\nin the book to a paper",
    "start": "3511760",
    "end": "3518180"
  },
  {
    "text": "that you could read\nmore thoroughly. But this is definitely\nnot obvious. So this is something\nyou're just going to have to believe me on\nor go look up that paper.",
    "start": "3518180",
    "end": "3524720"
  },
  {
    "text": "But it turns out that\nif we then average all of the weights at\nthe final iteration-- so we've gotten here,\nwe've kept track",
    "start": "3524720",
    "end": "3530210"
  },
  {
    "text": "of a weight for every sample,\nand we take this average of all the weights, we\nactually get an estimate of the probability of failure.",
    "start": "3530210",
    "end": "3536200"
  },
  {
    "start": "3536200",
    "end": "3541609"
  },
  {
    "text": "So that's our estimate. So a few notes on this.",
    "start": "3541610",
    "end": "3546770"
  },
  {
    "text": "So one, as you were kind\nof asking about earlier, it's actually very common\nto do a sampling step, similar to the sampling that's\ndone in population Monte",
    "start": "3546770",
    "end": "3554480"
  },
  {
    "text": "Carlo for each transition. And if you want to\nsee how that's done, there's a nice example in the\nbook that visualizes this.",
    "start": "3554480",
    "end": "3561440"
  },
  {
    "text": "Basically, it allows\nus to move our samples to the next distribution\nin this kind of sampling",
    "start": "3561440",
    "end": "3569089"
  },
  {
    "text": "based way before we do MCMC. And so it might make\nthe MCMC more efficient, so we don't have to\ndo as many MCMC steps",
    "start": "3569090",
    "end": "3576660"
  },
  {
    "text": "to transition from one\ndistribution to the next. So this is a very common\nthing that people do.",
    "start": "3576660",
    "end": "3584880"
  },
  {
    "text": "Another thing that I think\nis a really big thing to highlight here that I\nwas talking about earlier is like SMC is non-parametric.",
    "start": "3584880",
    "end": "3591070"
  },
  {
    "text": "So we didn't like pick any\nparametrized distributions for any of this. We did everything with samples.",
    "start": "3591070",
    "end": "3596109"
  },
  {
    "text": "And what's really\ncool about that is if we have these very\ncomplex multimodal failure distributions, we\ndon't have to try",
    "start": "3596110",
    "end": "3602670"
  },
  {
    "text": "to fit some complex distribution\nto it with some parameters. We can just maintain\na bunch of samples",
    "start": "3602670",
    "end": "3607830"
  },
  {
    "text": "to represent that very\ncomplex distribution. And so to highlight that,\nlet's move from our 2D Gaussian",
    "start": "3607830",
    "end": "3615810"
  },
  {
    "text": "to this pendulum example, which\nis like around 15 dimensions. So each sample now\nis a trajectory.",
    "start": "3615810",
    "end": "3622230"
  },
  {
    "text": "And what we do is we've picked\nthis kind of series of epsilons and we've slowly moved samples.",
    "start": "3622230",
    "end": "3628730"
  },
  {
    "text": "So these were the\noriginal initial samples, and we've kind of slowly\nmoved them towards the failure distribution using this MCMC.",
    "start": "3628730",
    "end": "3635745"
  },
  {
    "text": " Cool. We might finish early today.",
    "start": "3635745",
    "end": "3642210"
  },
  {
    "text": "All right. So we've gone through\nall of these things. And now I'm going to talk super,\nsuper briefly about this ratio",
    "start": "3642210",
    "end": "3648790"
  },
  {
    "text": "of normalizing constants idea. So this is basically the\nonly kind of legitimate thing I'm going to tell\nyou about this.",
    "start": "3648790",
    "end": "3656410"
  },
  {
    "text": "And this is an advanced\ntopic unfortunately. But it turns out what's\nreally interesting is",
    "start": "3656410",
    "end": "3662349"
  },
  {
    "text": "that importance sampling,\neverything I just showed you, is actually a special case\nof a more general problem",
    "start": "3662350",
    "end": "3669340"
  },
  {
    "text": "of determining the ratio\nof normalizing constants between two distributions. So this should make\nsense to you because I",
    "start": "3669340",
    "end": "3675760"
  },
  {
    "text": "said that the\nprobability of failure is the normalizing constant\nof the failure distribution.",
    "start": "3675760",
    "end": "3680960"
  },
  {
    "text": "So there's one\nnormalizing constant. And it turns out if you\ncan determine this ratio and do it in a certain way,\nyou can actually kind of",
    "start": "3680960",
    "end": "3687860"
  },
  {
    "text": "land on the importance\nsampling estimator. But if you view it as kind\nof a more general problem,",
    "start": "3687860",
    "end": "3695100"
  },
  {
    "text": "you can derive other\ntypes of estimators, which is really interesting. So if we step back and\nsay, let's move away from importance sampling, let's\nthink about the general problem",
    "start": "3695100",
    "end": "3702650"
  },
  {
    "text": "of determining these ratios\nof normalizing constants, you can derive a bunch of\ndifferent types of estimators.",
    "start": "3702650",
    "end": "3708540"
  },
  {
    "text": "So the ones we talk\nabout in the book are like self normalizing\nimportance sampling and bridge",
    "start": "3708540",
    "end": "3715010"
  },
  {
    "text": "sampling and umbrella sampling. And they're all\nsuper interesting. And yeah. So this is me right now\nbecause I absolutely",
    "start": "3715010",
    "end": "3721930"
  },
  {
    "text": "love everything we wrote\nin the book about this. I think these\nconcepts are like-- they require a lot of distilling\nfrom very academic mathy papers,",
    "start": "3721930",
    "end": "3730619"
  },
  {
    "text": "and I think we did a\nreally nice job of it. So if you are interested in\nthese topics, do check them out.",
    "start": "3730620",
    "end": "3735720"
  },
  {
    "text": "But we're back here and we have\ntoo much to do and not enough time. So we don't get to do this.",
    "start": "3735720",
    "end": "3741615"
  },
  {
    "text": "And so I really wanted\nto show it all to you, but we don't get to. But there is one\nfigure specifically",
    "start": "3741615",
    "end": "3749690"
  },
  {
    "text": "in the book that is\nparticularly special to me, and that's figure 7.12.",
    "start": "3749690",
    "end": "3755480"
  },
  {
    "text": "So you already know\nwhere this is going. So this is the optimal\nproposal distribution",
    "start": "3755480",
    "end": "3762109"
  },
  {
    "text": "for self normalized\nimportance sampling when we're doing something\nsimilar to umbrella sampling. And that's not all supposed\nto make sense to you.",
    "start": "3762110",
    "end": "3769470"
  },
  {
    "text": "But what's interesting\nabout this particular figure is that about early last\nyear, I was designing",
    "start": "3769470",
    "end": "3777350"
  },
  {
    "text": "invitations for my wedding. And I was visiting\nmy mom in San Diego,",
    "start": "3777350",
    "end": "3783390"
  },
  {
    "text": "working on these\ninvitations, and I-- so it should come as\nno surprise to you",
    "start": "3783390",
    "end": "3790460"
  },
  {
    "text": "that I wanted a dark theme\nwedding invitation, especially at this point. I think this is pretty clear.",
    "start": "3790460",
    "end": "3795990"
  },
  {
    "text": "So I started designing a dark\ntheme wedding invitation, and I was really\nexcited about it. And then my then\nfiance, now husband,",
    "start": "3795990",
    "end": "3802670"
  },
  {
    "text": "walked by while I was doing this\nand he was like, absolutely not. This looks like an\ninvitation to a funeral. We can't have a dark\ntheme wedding invitation.",
    "start": "3802670",
    "end": "3810020"
  },
  {
    "text": "So I was like oh, bummer. And then I was working on\nspecifically this figure at the time, so I just went\nback to my work on the book.",
    "start": "3810020",
    "end": "3816470"
  },
  {
    "text": "And then my mom\nwalked behind me, and she was like, oh,\nthat looks pretty cool. Why don't you just do that?",
    "start": "3816470",
    "end": "3821990"
  },
  {
    "text": "And so the unbeknownst to\nprobably 99% of the guests who attended my wedding, the\noptimal self normalized",
    "start": "3821990",
    "end": "3829490"
  },
  {
    "text": "importance sampling\ndistribution was actually on my wedding invitation. So yeah, it was probably\none of the few people",
    "start": "3829490",
    "end": "3836119"
  },
  {
    "text": "to have PGFplots on\ntheir wedding invitation. And yeah, I think my other point\nhere is, if for some reason",
    "start": "3836120",
    "end": "3844460"
  },
  {
    "text": "the book isn't that enticing\nto you, one bummer, but two. No worries. The figures also\nmake good decorations",
    "start": "3844460",
    "end": "3850400"
  },
  {
    "text": "so you can try it out yourself. OK. That's it.",
    "start": "3850400",
    "end": "3855500"
  },
  {
    "text": "That's all I have today. We're going to wrap\nup early, I guess. ",
    "start": "3855500",
    "end": "3863000"
  }
]