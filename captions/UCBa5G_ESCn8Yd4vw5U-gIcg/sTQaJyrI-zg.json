[
  {
    "start": "0",
    "end": "5988"
  },
  {
    "text": "OK. So yeah, I'm super excited\nto be here and share our recent research about\nneuro-symbolic commonsense",
    "start": "5988",
    "end": "14320"
  },
  {
    "text": "reasoning. So part of the goal\nof this talk will",
    "start": "14320",
    "end": "19990"
  },
  {
    "text": "be to address some of the\nfrequently asked questions these days, that\nNLP or commonsense",
    "start": "19990",
    "end": "25810"
  },
  {
    "text": "or whatever, it looks like\nalmost it's solved by ChatGPT, and I have an\nexistential crisis.",
    "start": "25810",
    "end": "32599"
  },
  {
    "text": "So people do ask me\nthis from time to time. So perhaps it's a case\nof hasty generalization,",
    "start": "32600",
    "end": "41650"
  },
  {
    "text": "especially if we do look\nat some of the examples. So \"the trophy doesn't\nfit in the brown suitcase",
    "start": "41650",
    "end": "48100"
  },
  {
    "text": "because it's too big. What's too big?\" So this is a classical Winograd\nschema challenge problem.",
    "start": "48100",
    "end": "55270"
  },
  {
    "text": "And here, ChatGPT\nanswers it correctly, \"the trophy is too big.\"",
    "start": "55270",
    "end": "60340"
  },
  {
    "text": "So impressive. But what if you change\nthe question a little bit? Then it says, \"the trophy\nitself is too small",
    "start": "60340",
    "end": "67990"
  },
  {
    "text": "to fit into the suitcase.\" So it's not very\nreliable at the moment.",
    "start": "67990",
    "end": "73119"
  },
  {
    "text": "So the situation is a little\nbit like David and Goliath in the sense that the\nbigger appears to be better.",
    "start": "73120",
    "end": "80890"
  },
  {
    "text": "In many of the cases,\nalthough, of course, some of the more\ncareful studies do reveal that smaller\nmodels can be",
    "start": "80890",
    "end": "87430"
  },
  {
    "text": "better with better data\nor better reinforcement",
    "start": "87430",
    "end": "93160"
  },
  {
    "text": "learning with the human\nfeedback and whatnot. So it's likely that there\nare still other ways",
    "start": "93160",
    "end": "100180"
  },
  {
    "text": "to improve the transformer\nperformances by building similar models in\na more clever way.",
    "start": "100180",
    "end": "108890"
  },
  {
    "text": "So one way to draw the insight\nis from this classic book",
    "start": "108890",
    "end": "115870"
  },
  {
    "text": "known as The Art of War,\nwhich, of course, says nothing about deep neural\nnetworks or transformers.",
    "start": "115870",
    "end": "122350"
  },
  {
    "text": "But the wisdom here is that know\nyour enemy, choose your battles and innovate your weapons,\nwhich-- we can translate that",
    "start": "122350",
    "end": "128740"
  },
  {
    "text": "as evaluation with\nrealism and scrutiny",
    "start": "128740",
    "end": "134070"
  },
  {
    "text": "and focusing on different types\nof new tasks and leaderboards",
    "start": "134070",
    "end": "140400"
  },
  {
    "text": "and then innovating your\nalgorithms and data. So in this talk, I'm going to\nshowcase three such studies.",
    "start": "140400",
    "end": "147690"
  },
  {
    "text": "And let's dive right in\nwith the Maieutic prompting. By the way, so the\nrecurring theme in this talk",
    "start": "147690",
    "end": "154110"
  },
  {
    "text": "will be that smaller\nmodels can be better and that knowledge is power. So let's just start\nwith this observation",
    "start": "154110",
    "end": "161379"
  },
  {
    "text": "that language models\nare \"sometimes\" amazing! So if you ask\nGPT-3, if you travel",
    "start": "161380",
    "end": "167530"
  },
  {
    "text": "West far enough\nfrom the West Coast, you will reach to the\nEast Coast or not?",
    "start": "167530",
    "end": "174100"
  },
  {
    "text": "So it says the world is\naround, which is correct. So you will reach the\nEast Coast eventually.",
    "start": "174100",
    "end": "181150"
  },
  {
    "text": "Therefore the answer is true. So this looks impressive except\nwhen it's not impressive.",
    "start": "181150",
    "end": "187090"
  },
  {
    "text": "So if you ask other questions\nlike, butterflies fly with the three wings or not?",
    "start": "187090",
    "end": "192670"
  },
  {
    "text": "It says it has four wings. Therefore the\nstatement is false. But if you read the\nback of what it just",
    "start": "192670",
    "end": "198400"
  },
  {
    "text": "said as true-false\nquestions, then it negates what it just said. So it can be inconsistent\nwith its own statement.",
    "start": "198400",
    "end": "207460"
  },
  {
    "text": "And then there are many other\nsuch inconsistency problems. So it's not clear what language\nmodels do or do not know.",
    "start": "207460",
    "end": "215379"
  },
  {
    "text": "It's almost like language\nmodels are some sort of lemons. Well, it might be cherries\nif you only pick cherries.",
    "start": "215380",
    "end": "221500"
  },
  {
    "text": "But it doesn't make\nstrange mistakes. So the question is, how\nwould we make better lemonade",
    "start": "221500",
    "end": "227860"
  },
  {
    "text": "from GPT-3? So one approach might be to\nget philosophical and use",
    "start": "227860",
    "end": "234310"
  },
  {
    "text": "Socrates' Maieutic method\nthat was originally developed for addressing humans' flawed\nreasoning because it actually",
    "start": "234310",
    "end": "242980"
  },
  {
    "text": "turns out even humans are not\nall that logically consistent, let alone GPT-3.",
    "start": "242980",
    "end": "249700"
  },
  {
    "text": "So the way it works\nis this, we're going to build a\nMaieutic inference tree.",
    "start": "249700",
    "end": "255850"
  },
  {
    "text": "And let's use the previous\nexample as a running example. So what we do is we ask the\nfollowing question, providing",
    "start": "255850",
    "end": "263620"
  },
  {
    "text": "the answer being true. And then let's attach\n\"because,\" so that we prompt",
    "start": "263620",
    "end": "270400"
  },
  {
    "text": "GPT-3 to continue on\nthis sentence, which means it will now\nhave to explain,",
    "start": "270400",
    "end": "276009"
  },
  {
    "text": "provide explanation\nwhy the answer is true. In this case, the\nexplanation is good.",
    "start": "276010",
    "end": "282070"
  },
  {
    "text": "So it's an E of T explanation\nof the answer being T. We ask the same question,\nswitch now true with a false",
    "start": "282070",
    "end": "291400"
  },
  {
    "text": "and then see what BS\nGPT-3 might come up with. So here it's just trying to go\nwith the false as an answer,",
    "start": "291400",
    "end": "301930"
  },
  {
    "text": "but it just doesn't\nhave a very good answer. It just says you cannot reach. So now we call this as E of F.\nSo it's an explanation of F,",
    "start": "301930",
    "end": "310960"
  },
  {
    "text": "answer being F. Now, let's see how robust\nor consistent GPT-3",
    "start": "310960",
    "end": "318970"
  },
  {
    "text": "is with respect to\nits own explanation. So we read that, E\nof T, and then let",
    "start": "318970",
    "end": "326950"
  },
  {
    "text": "GPT-3 to decide whether it's\ngoing to agree or disagree with the label true or false.",
    "start": "326950",
    "end": "333230"
  },
  {
    "text": "So in this case, the last one\nis negated version of E of T.",
    "start": "333230",
    "end": "338800"
  },
  {
    "text": "So we insert\n\"negation not here.\" And in this case, it's good that\nit's a flipping the answer when",
    "start": "338800",
    "end": "345730"
  },
  {
    "text": "the statement is negated. So this is a case when GPT-3\nis logically integral to E",
    "start": "345730",
    "end": "352840"
  },
  {
    "text": "of T. For E of false,\nthough, which was basically a bogus explanation\nfor the wrong answer,",
    "start": "352840",
    "end": "360169"
  },
  {
    "text": "it's not able to flip its own\nlabeling, which means GPT-3",
    "start": "360170",
    "end": "366070"
  },
  {
    "text": "is not logically integral. So that's good. GPT-3 does know\nsomething strange",
    "start": "366070",
    "end": "372910"
  },
  {
    "text": "about its own explanation\ngiven previously. And so we can keep doing\nthis recursively to let--",
    "start": "372910",
    "end": "383260"
  },
  {
    "text": "to make GPT-3 to explain its\nown explanation of explanation",
    "start": "383260",
    "end": "388960"
  },
  {
    "text": "recursively. So we build this Maieutic\ntree or graph for some time",
    "start": "388960",
    "end": "398020"
  },
  {
    "text": "and then only keep branches that\nare logical integrals throwing",
    "start": "398020",
    "end": "403090"
  },
  {
    "text": "out the non-integral\npart for now. But even after chopping\nthe branches, where there's",
    "start": "403090",
    "end": "409600"
  },
  {
    "text": "logical inconsistencies,\nGPT-3 being GPT-3, the tree will still have some\ninconsistent explanations.",
    "start": "409600",
    "end": "418510"
  },
  {
    "text": "So in order to improve\nthe logical consistency, now what we do is we are going\nto look at pairwise consistency",
    "start": "418510",
    "end": "428920"
  },
  {
    "text": "among any of the nodes. So we compute. Sorry.",
    "start": "428920",
    "end": "434890"
  },
  {
    "text": "Stepping back, we are\ngoing to first compute the node-wise confidence. So we call that as a belief.",
    "start": "434890",
    "end": "441580"
  },
  {
    "text": "And it's defined by this\nparticular equation that basically looks at different\nconditional probabilities",
    "start": "441580",
    "end": "448870"
  },
  {
    "text": "and then computes its ratio\nto see how confident it is for any particular node.",
    "start": "448870",
    "end": "455110"
  },
  {
    "text": "We then also look at the\nedge-wise or pairwise consistency by using\noff-the-shelf natural language",
    "start": "455110",
    "end": "464009"
  },
  {
    "text": "inference models output, whether\na pair is contradictional",
    "start": "464010",
    "end": "470100"
  },
  {
    "text": "or not. So we then create\nthese pairwise weights.",
    "start": "470100",
    "end": "475650"
  },
  {
    "text": "Now, once you have\nall of these, then we can formulate a constraint\noptimization problem,",
    "start": "475650",
    "end": "484710"
  },
  {
    "text": "where the inference\nobjective is to assign",
    "start": "484710",
    "end": "490440"
  },
  {
    "text": "some label, either\ntrue or false, on each of the nodes such\nthat it's going",
    "start": "490440",
    "end": "497099"
  },
  {
    "text": "to maximize the weight assigned\nto all of this node and edges.",
    "start": "497100",
    "end": "502720"
  },
  {
    "text": "So sometimes, the\nlabeling will have to flip the original label that\nthe model might have preferred",
    "start": "502720",
    "end": "509770"
  },
  {
    "text": "to give because that way, you\ncan enhance the graph level",
    "start": "509770",
    "end": "515229"
  },
  {
    "text": "consistency. So you can style this\nwith any max-SAT. So SAT means satisfiability.",
    "start": "515230",
    "end": "524500"
  },
  {
    "text": "And this is a classical\nAI search algorithm. And we used this\nparticular solver,",
    "start": "524500",
    "end": "530800"
  },
  {
    "text": "but you can use many others. And so here, the final output\nis that the original answer",
    "start": "530800",
    "end": "538300"
  },
  {
    "text": "to the original\nquestion should be true. And then, it also gives\nnode-wise, per-node label",
    "start": "538300",
    "end": "544390"
  },
  {
    "text": "assignment as well. So what does this\nmean in the end in terms of empirical results?",
    "start": "544390",
    "end": "551000"
  },
  {
    "text": "So when tested on CommonsenseQA\n2.0, the canonical prompting",
    "start": "551000",
    "end": "558810"
  },
  {
    "text": "show green used on top of\nGPT-3, so it's basically future prompting on GPT-3,\nwill give you a bit better",
    "start": "558810",
    "end": "567120"
  },
  {
    "text": "than chance performance. So this is a\ntrue/false QA data set.",
    "start": "567120",
    "end": "572260"
  },
  {
    "text": "So your chance level is 50. And GPT-3 is barely\nbetter than chance.",
    "start": "572260",
    "end": "577290"
  },
  {
    "text": "But recently, there\nhave been some ideas, such as chain of thoughts\nor self-consistency,",
    "start": "577290",
    "end": "585240"
  },
  {
    "text": "that can improve the vanilla\nprompting method considerably.",
    "start": "585240",
    "end": "591070"
  },
  {
    "text": "So if you use such variations,\nthen you get performance gain. Now, the purple is the\ndifferent variant of it.",
    "start": "591070",
    "end": "601590"
  },
  {
    "text": "But together, they're\nall doing worse than Maieutic prompting,\nwhich in fact, does better",
    "start": "601590",
    "end": "608730"
  },
  {
    "text": "than supervised\nmodel trained on T5. Usually, a supervised\nmodel trained on T5",
    "start": "608730",
    "end": "615029"
  },
  {
    "text": "is hard to beat\nusing GPT-3 few-shot. But basically, this is\ninference time on the algorithm,",
    "start": "615030",
    "end": "623160"
  },
  {
    "text": "practically unsupervised. And it does well on that. And similarly, we\nsee a large boost",
    "start": "623160",
    "end": "629730"
  },
  {
    "text": "when tested on other commonsense\nbenchmarks such as CREAK or Come2Sense.",
    "start": "629730",
    "end": "635320"
  },
  {
    "text": "So what this tells\nus is that although",
    "start": "635320",
    "end": "640710"
  },
  {
    "text": "the emergent capabilities\nof large transformers are phenomenal, they\ncan be not very robust",
    "start": "640710",
    "end": "650160"
  },
  {
    "text": "for some of these\ncommonsense challenges. And it's in large part due to\nthe logical inconsistencies,",
    "start": "650160",
    "end": "658120"
  },
  {
    "text": "which can be dramatically\nenhanced when you do this symbolic reasoning on top.",
    "start": "658120",
    "end": "664379"
  },
  {
    "text": "So yeah, not only\nSocrates' method to help with flawed\nhuman reasoning,",
    "start": "664380",
    "end": "670320"
  },
  {
    "text": "can also dramatically enhance\nthe flawed neural networks' reasoning.",
    "start": "670320",
    "end": "676810"
  },
  {
    "text": "So moving to the next\ntopic, symbolic knowledge distillation.",
    "start": "676810",
    "end": "683310"
  },
  {
    "text": "So this work is\na work that tries to convert general language\nmodels, on top of transformers,",
    "start": "683310",
    "end": "690030"
  },
  {
    "text": "to causal commonsense\nmodels, also transformers. And the reason why we might\nwant to worry about commonsense",
    "start": "690030",
    "end": "698160"
  },
  {
    "text": "models is because despite\nhuman-level or even",
    "start": "698160",
    "end": "703500"
  },
  {
    "text": "superhuman-level performances\non a variety of leaderboards, the state-of-the-art models are\nbrittle when given adversarial",
    "start": "703500",
    "end": "711930"
  },
  {
    "text": "or out-of-domain examples. So transformers can make\nseemingly strange mistakes.",
    "start": "711930",
    "end": "722940"
  },
  {
    "text": "And so solving-- it's almost\nlike solving only a dataset without really solving\nthe underlying task.",
    "start": "722940",
    "end": "728970"
  },
  {
    "text": "And this phenomenon\nsometimes is described as a systematic\ngeneralization problem.",
    "start": "728970",
    "end": "735510"
  },
  {
    "text": "And why does this happen? It's that unlike humans,\nwho truly learn about how",
    "start": "735510",
    "end": "741090"
  },
  {
    "text": "the world works,\nconceptually, transformers learn surface patterns\nin language or images",
    "start": "741090",
    "end": "750510"
  },
  {
    "text": "that are powerful for\nmany downstream use cases but still not really robust\nunderstanding of the concepts",
    "start": "750510",
    "end": "760260"
  },
  {
    "text": "and how the world works. So in order to\nbridge this gap, we can really think\nabout this challenge",
    "start": "760260",
    "end": "765780"
  },
  {
    "text": "of learning,\nacquiring common sense capabilities for machines.",
    "start": "765780",
    "end": "771670"
  },
  {
    "text": "So the operational definition\nof commonsense in this talk will be that it's the basic\nlevel of practical knowledge",
    "start": "771670",
    "end": "779339"
  },
  {
    "text": "and reasoning concerning\neveryday situations and events that are commonly\nshared among the most people.",
    "start": "779340",
    "end": "786870"
  },
  {
    "text": "This is really important, the\nlast part that it's commonly shared among the most\npeople, but it's not the case",
    "start": "786870",
    "end": "792570"
  },
  {
    "text": "that it is shared by\neverybody in the universe, because the additional\ncontext can always",
    "start": "792570",
    "end": "799110"
  },
  {
    "text": "change what is\ncommonsensical for any given culture or situation.",
    "start": "799110",
    "end": "804910"
  },
  {
    "text": "So for example, in\ngeneral, you and I probably agree that it's OK to\nkeep the closet door open,",
    "start": "804910",
    "end": "810460"
  },
  {
    "text": "but it's not OK to keep\nthe fridge door open because the food\ninside might go bad. So these are general\nrules of the thumb",
    "start": "810460",
    "end": "816899"
  },
  {
    "text": "that we might abide by it. But of course, if you go\nto your friend's house,",
    "start": "816900",
    "end": "823410"
  },
  {
    "text": "you might behave a little\nbit and keep their closed doors open, sorry, closed.",
    "start": "823410",
    "end": "829830"
  },
  {
    "text": "And then, as far as the fridge\ndoor, if you're in a store, and it's not really\nhooked up to the wall,",
    "start": "829830",
    "end": "835680"
  },
  {
    "text": "then it doesn't matter when--\nwhether the fridge door is open or not because there's\nno food inside.",
    "start": "835680",
    "end": "842280"
  },
  {
    "text": "And you can come up\nwith many situations in which these basic rules of\nthumb will have exceptions.",
    "start": "842280",
    "end": "850860"
  },
  {
    "text": "So that is the key challenge\nof commonsense because not--",
    "start": "850860",
    "end": "859149"
  },
  {
    "text": "it's not universal\nknowledge, but it's like a shared across a\nlarge population of people.",
    "start": "859150",
    "end": "867130"
  },
  {
    "text": "So it's essential. Such common sense is\nessential for humans to live and interact\nwith each other in a reasonable and safe way.",
    "start": "867130",
    "end": "873830"
  },
  {
    "text": "And so, as AI becomes\nincreasingly more important aspect of human lives,\nand with ChatGPT to more likely",
    "start": "873830",
    "end": "885160"
  },
  {
    "text": "so, it's good if AI can\nunderstand the human needs and actions, and values better.",
    "start": "885160",
    "end": "891680"
  },
  {
    "text": "So the premise of this talk is\nthat language models are not",
    "start": "891680",
    "end": "897130"
  },
  {
    "text": "equivalent to\nknowledge models, even though language models\ntoday do acquire",
    "start": "897130",
    "end": "902440"
  },
  {
    "text": "a great deal of knowledge. But they're not equivalent. So we developed a symbolic\ncommonsense knowledge",
    "start": "902440",
    "end": "912190"
  },
  {
    "text": "graph known as ATOMIC a few\nyears ago, four years ago now,",
    "start": "912190",
    "end": "917410"
  },
  {
    "text": "as well as a neural commonsense\nmodel built on top of-- or trained using ATOMIC\nas the source of training,",
    "start": "917410",
    "end": "926950"
  },
  {
    "text": "fine-tuning of the\nshelf language models. Up until two years\nago, this ATOMIC",
    "start": "926950",
    "end": "933850"
  },
  {
    "text": "was fully crowdsourced by\nhumans, which in this talk, I'm going to lift.",
    "start": "933850",
    "end": "939580"
  },
  {
    "text": "But at first, the\nnorms is that this all has to be human-crowdsourced.",
    "start": "939580",
    "end": "945730"
  },
  {
    "text": "So you can consider\nalmost atomic as a human demonstration. In the current\nversion of ChatGPT,",
    "start": "945730",
    "end": "951970"
  },
  {
    "text": "you can consider this\nas human demonstrations of commonsense inferences.",
    "start": "951970",
    "end": "957759"
  },
  {
    "text": "And we had this\nCOMET-ATOMIC 2020, which is an enhanced\nversion of ATOMIC and COMET.",
    "start": "957760",
    "end": "963490"
  },
  {
    "text": "Again, ATOMIC portion was fully\ncrowdsourced by humans in 2021.",
    "start": "963490",
    "end": "970029"
  },
  {
    "text": "So let me give you a\nbit of a sample of what ATOMIC 2020 looks like.",
    "start": "970030",
    "end": "976460"
  },
  {
    "text": "So imagine a situation where\nX gets X's car repaired, or you get your car repaired.",
    "start": "976460",
    "end": "982160"
  },
  {
    "text": "So immediately, you\ncan imagine what's likely to be true or\nrelevant for the situation,",
    "start": "982160",
    "end": "989089"
  },
  {
    "text": "that as a result, you\nmight want to call Uber or Lyft for a ride. As a result, you\nneed to pay the bill.",
    "start": "989090",
    "end": "996260"
  },
  {
    "text": "Beforehand, you need a mechanic\nand money to repair your car. So these are,\nbasically, preconditions",
    "start": "996260",
    "end": "1002440"
  },
  {
    "text": "and postconditions\nof that event. So some of this\nATOMIC Knowledge Graph is about social interaction\nknowledge about the event.",
    "start": "1002440",
    "end": "1010960"
  },
  {
    "text": "And then other\nparts of the ATOMIC is physical\nentity-centric knowledge.",
    "start": "1010960",
    "end": "1016690"
  },
  {
    "text": "So money is typically\nused for pain repairs. But if you really want it, you\ncan fold that into origami.",
    "start": "1016690",
    "end": "1023170"
  },
  {
    "text": "I've never done it. But these are examples of\nthe stereotypical use cases,",
    "start": "1023170",
    "end": "1030459"
  },
  {
    "text": "as well as non-stereotypical\nbut affordable actions",
    "start": "1030460",
    "end": "1035470"
  },
  {
    "text": "that you can apply to objects. So it requires neither\nphysics understanding",
    "start": "1035470",
    "end": "1040809"
  },
  {
    "text": "about the affordances\nof physical objects. And then, we can also reason\nabout a counterfactual",
    "start": "1040810",
    "end": "1049070"
  },
  {
    "text": "condition in which the\ncentral event cannot happen, so it can be hindered by that. So if you totaled\nyour car completely,",
    "start": "1049070",
    "end": "1055940"
  },
  {
    "text": "then it's impossible to\nget your car repaired. And then there are\nevents that typically",
    "start": "1055940",
    "end": "1061080"
  },
  {
    "text": "happens-- to before and after. So some of this knowledge\nis event-centric. So we crowdsourced a fair\namount over the course",
    "start": "1061080",
    "end": "1071059"
  },
  {
    "text": "of, I don't know,\nmaybe two years or so up to 1.3 million if-then\nrose or if-then knowledge",
    "start": "1071060",
    "end": "1080960"
  },
  {
    "text": "over 23 different AGI\ntypes or relation types. ",
    "start": "1080960",
    "end": "1087030"
  },
  {
    "text": "So it was fully crowdsourced. And so, the knowledge graph\nis useful for training",
    "start": "1087030",
    "end": "1093440"
  },
  {
    "text": "transformers. And here, let's\nsee the comparison between COMET, that was\nbuilt on BART compared",
    "start": "1093440",
    "end": "1100370"
  },
  {
    "text": "to GPT-3, which is so\nlarge, it doesn't even fit into the slide.",
    "start": "1100370",
    "end": "1105740"
  },
  {
    "text": "It was more than 400\ntimes larger than BART. So with that in\nmind, if you look",
    "start": "1105740",
    "end": "1113990"
  },
  {
    "text": "at this accuracy, judged\nby humans after making the commonsense model, making\nsome common sense inferences--",
    "start": "1113990",
    "end": "1121080"
  },
  {
    "text": "so the task is that given\na node, which describes a situation or event, and\nthen given an edge type, which",
    "start": "1121080",
    "end": "1129110"
  },
  {
    "text": "narrows down the commonsense\nrelation or inference type, you're now going to\ngenerate some inference.",
    "start": "1129110",
    "end": "1137070"
  },
  {
    "text": "So it's a generative task. And then, we ask humans whether\nthe commonsense inference",
    "start": "1137070",
    "end": "1143600"
  },
  {
    "text": "seems reasonable or not. So 100% is the desired level.",
    "start": "1143600",
    "end": "1149600"
  },
  {
    "text": "COMET is substantially better\nthan GPT-3, which is really",
    "start": "1149600",
    "end": "1154940"
  },
  {
    "text": "impressively better than GPT-2. It's not apple-to-apple\nbecause GPT-2 is a zero-shot,",
    "start": "1154940",
    "end": "1159980"
  },
  {
    "text": "GPT-3 is a few-shot,\nbut still, it's interesting, the large jump that\nscales alone brought to GPT-3.",
    "start": "1159980",
    "end": "1170480"
  },
  {
    "text": "But still, GPT-3\nis too large to be useful for actual system\nbuilding for most engineers",
    "start": "1170480",
    "end": "1179180"
  },
  {
    "text": "and scientists in the world. So it's nice to have\na smaller model that does it do even better.",
    "start": "1179180",
    "end": "1184350"
  },
  {
    "text": "And so when we put\nthese resources out, people all around the globe\ndid some creative research",
    "start": "1184350",
    "end": "1191659"
  },
  {
    "text": "using it. So persona-aware conversations\nor figurative language understanding, storytelling\nand fantasy gaming,",
    "start": "1191660",
    "end": "1199669"
  },
  {
    "text": "and interactive\nlearning enhancement-- in all of these\nworks, people came up",
    "start": "1199670",
    "end": "1206930"
  },
  {
    "text": "with some useful use cases\nusing either COMET or ATOMIC",
    "start": "1206930",
    "end": "1212090"
  },
  {
    "text": "or both, some kind of\ncommonsense backbone",
    "start": "1212090",
    "end": "1217880"
  },
  {
    "text": "for their downstream use cases. But the applications\nare still limited",
    "start": "1217880",
    "end": "1224059"
  },
  {
    "text": "by the coverage and quality\nof these commonsense models. So we wanted to make it better.",
    "start": "1224060",
    "end": "1230549"
  },
  {
    "text": "But we were hitting a bit of a\nlimit with human crowdsourcing. So now, in this paper, Symbolic\nKnowledge Distillation,",
    "start": "1230550",
    "end": "1238580"
  },
  {
    "text": "we're going to do\nAI-generated knowledge",
    "start": "1238580",
    "end": "1244640"
  },
  {
    "text": "graph by introducing this\nnotion, symbolic knowledge distillation.",
    "start": "1244640",
    "end": "1249750"
  },
  {
    "text": "So we want to take\nthis GPT-3, which is very impressive\nbut too large,",
    "start": "1249750",
    "end": "1255660"
  },
  {
    "text": "so make it smaller\nbut better than GPT-3. So GPT-3 was about 73% good.",
    "start": "1255660",
    "end": "1262220"
  },
  {
    "text": "And it's good, but not good\nenough for empirical use cases.",
    "start": "1262220",
    "end": "1267500"
  },
  {
    "text": "Now, is that even\npossible, though? Because when you normally\ndo knowledge distillation,",
    "start": "1267500",
    "end": "1273320"
  },
  {
    "text": "you get smaller and worse\nmodels, not better models. So the reason why\nthis could work",
    "start": "1273320",
    "end": "1279500"
  },
  {
    "text": "is because the symbolic\nknowledge distillation has",
    "start": "1279500",
    "end": "1288760"
  },
  {
    "text": "this funnel that's convoluted. And it has a critic inside\nthat really helps the student",
    "start": "1288760",
    "end": "1296840"
  },
  {
    "text": "to model to be\nsmaller but better. So slightly more formally,\nknowledge distillation,",
    "start": "1296840",
    "end": "1304960"
  },
  {
    "text": "due to Hinton et al. 2015, is a method to distill\nteacher model down to student",
    "start": "1304960",
    "end": "1312180"
  },
  {
    "text": "to model by optimizing\nthis cross entropy between the teacher's\nprobability distribution",
    "start": "1312180",
    "end": "1320800"
  },
  {
    "text": "over the label space, y,\nI'll put a y, and then",
    "start": "1320800",
    "end": "1326020"
  },
  {
    "text": "the students' distribution\nover the same output, y.",
    "start": "1326020",
    "end": "1331900"
  },
  {
    "text": "In the original work, the output\nspace was just classification.",
    "start": "1331900",
    "end": "1338530"
  },
  {
    "text": "So knowledge distillation was\ndone for classification task, in which case, it's a\nsimple enumeration that",
    "start": "1338530",
    "end": "1347440"
  },
  {
    "text": "leads to the correct summation. But in our case, y can be a\nsentence, which is intractable,",
    "start": "1347440",
    "end": "1354429"
  },
  {
    "text": "because there can\nbe exponentially many such outputs. So what people do,\nwell, no problem.",
    "start": "1354430",
    "end": "1362200"
  },
  {
    "text": "We always just sample\nand call it a day. So we're going to sample,\nand so that we just",
    "start": "1362200",
    "end": "1368230"
  },
  {
    "text": "compute the expectation\nthrough samples. And the byproduct\nof that samples",
    "start": "1368230",
    "end": "1374559"
  },
  {
    "text": "will be a symbolic\nknowledge graph. And that's because the strings\ncoming out of this sampling",
    "start": "1374560",
    "end": "1382690"
  },
  {
    "text": "can be connected together into\ngraph structure if we want it. So in terms of the quality of\nthe generated knowledge-- so",
    "start": "1382690",
    "end": "1393910"
  },
  {
    "text": "let's compare our human-written\nknowledge versus GPT-3-authored",
    "start": "1393910",
    "end": "1399640"
  },
  {
    "text": "knowledge. Here, the y-axis shows\nthe quantity in millions.",
    "start": "1399640",
    "end": "1405350"
  },
  {
    "text": "So ATOMIC 2020, the human\nreturn knowledge is less than",
    "start": "1405350",
    "end": "1411820"
  },
  {
    "text": "a million, in this\nparticular case, in terms of the number of\nknowledge because we only-- in this study, we only look\nat our subset of ATOMIC 2020",
    "start": "1411820",
    "end": "1420340"
  },
  {
    "text": "relation types that corresponds\nto causal commonsense knowledge--",
    "start": "1420340",
    "end": "1426940"
  },
  {
    "text": "causal commonsense reasoning. So it's less than a\nmillion for that subset.",
    "start": "1426940",
    "end": "1433780"
  },
  {
    "text": "And then, if we look\nat GPT-3's generation, we can generate a lot.",
    "start": "1433780",
    "end": "1439000"
  },
  {
    "text": "So we can generate\nalmost 7 million of them. But here, black pollution\nis a noisy potion.",
    "start": "1439000",
    "end": "1446140"
  },
  {
    "text": "And the green portion\nis the good portion. And you see, because GPT-3\nis only about 70% good,",
    "start": "1446140",
    "end": "1452890"
  },
  {
    "text": "30% are all garbage. So it's a larger scale lower\naccuracy at this point compared",
    "start": "1452890",
    "end": "1459250"
  },
  {
    "text": "to a human-written resource. So now what we do is we\ntrain this critique model,",
    "start": "1459250",
    "end": "1466179"
  },
  {
    "text": "and we use RoBERTa\nfor simplicity. And this is a supervised model\non a moderate size labeled",
    "start": "1466180",
    "end": "1475450"
  },
  {
    "text": "data, about 10,000 or so. And it's a binary\nclassification task",
    "start": "1475450",
    "end": "1480610"
  },
  {
    "text": "where whether the\nmachine-generated the knowledge looks correct or not. And this RoBERTa is\nnot a very good model",
    "start": "1480610",
    "end": "1488770"
  },
  {
    "text": "because if so-- if\nit's perfect, we would have solved the\ncommonsense problem altogether. So the critic tries to\nthrow out bad stuff.",
    "start": "1488770",
    "end": "1497980"
  },
  {
    "text": "And we can use it the\ncritic very aggressively with the high threshold. So whenever something\nis slightly suspicious,",
    "start": "1497980",
    "end": "1504580"
  },
  {
    "text": "just throw that out. But if we use it\naggressively, so we throw out",
    "start": "1504580",
    "end": "1510340"
  },
  {
    "text": "most of the black,\nthat's good, together with a lot of green stuff. But still, the\nremainder is much larger",
    "start": "1510340",
    "end": "1517809"
  },
  {
    "text": "than what humans\nhave ever written. And yet, we can actually\nretain higher accuracy",
    "start": "1517810",
    "end": "1523299"
  },
  {
    "text": "than human-authored resources. So here, the\nteacher is basically the combination between GPT-3,\nwhich is, in some sense,",
    "start": "1523300",
    "end": "1531910"
  },
  {
    "text": "loose teacher, and then combined\nwith the critic RoBERTa, which serves as a critic teacher.",
    "start": "1531910",
    "end": "1540220"
  },
  {
    "text": "So that's the\ngenerated knowledge. Now how helpful are\nthey for the purpose",
    "start": "1540220",
    "end": "1547990"
  },
  {
    "text": "of training downstream\nneural commonsense models? So recall that the GPT-3,\nwithout doing anything else,",
    "start": "1547990",
    "end": "1557290"
  },
  {
    "text": "is a loose teacher whose\ncommon sense inference is only about 73% good.",
    "start": "1557290",
    "end": "1562420"
  },
  {
    "text": "So you see here, it's\naccuracy of its output. And then it turns out\nif we use loose teacher",
    "start": "1562420",
    "end": "1568990"
  },
  {
    "text": "as a teacher directly\nto teach-student model, then the performance\nalready goes up on its own.",
    "start": "1568990",
    "end": "1575870"
  },
  {
    "text": "So this is interesting that-- usually, this is not the\ncase with the knowledge distillation.",
    "start": "1575870",
    "end": "1581049"
  },
  {
    "text": "But when we focus on commonsense\nknowledge distillation, student just on its\nown becomes better.",
    "start": "1581050",
    "end": "1589039"
  },
  {
    "text": "So unlike typical\nknowledge distillation, where we start with\na language model",
    "start": "1589040",
    "end": "1596020"
  },
  {
    "text": "and we end with a\nlanguage model student and teacher of the same type,\nhere, the original teacher",
    "start": "1596020",
    "end": "1602230"
  },
  {
    "text": "was actual language model,\nnot common sense model. And then, we want\nthe student model to be more of the\ncommonsense model.",
    "start": "1602230",
    "end": "1609130"
  },
  {
    "text": "So there is a switch of the type\nbetween teacher and student. And so when that's the case,\nwhether this is generally true,",
    "start": "1609130",
    "end": "1616856"
  },
  {
    "text": "we don't know. But this is what we\nfound empirically. ",
    "start": "1616857",
    "end": "1624130"
  },
  {
    "text": "Ooh, should I pay attention\nto the questions or not? Feel free to consider\nany relevant questions.",
    "start": "1624130",
    "end": "1631970"
  },
  {
    "text": "I hang on. Let me quickly check. ",
    "start": "1631970",
    "end": "1637370"
  },
  {
    "text": "Yeah, sample-- oh,\nsample is generated output, which happens to be\nusually a sentence or phrase.",
    "start": "1637370",
    "end": "1646790"
  },
  {
    "text": "That's what I meant by sample. Sorry that I didn't\nsee that all earlier.",
    "start": "1646790",
    "end": "1652279"
  },
  {
    "text": "And then the last\nquestion, having the model generate text to one\nsymbol at a time,",
    "start": "1652280",
    "end": "1657980"
  },
  {
    "text": "starting from the\ntarget label sentence. Yes, it's because a transformer\ncan only generate one word",
    "start": "1657980",
    "end": "1664130"
  },
  {
    "text": "at-- one token at a time. That's what we\ndo, as well, here. Thank you for the\nclarification questions.",
    "start": "1664130",
    "end": "1670890"
  },
  {
    "text": "All right. So back to here. In our earlier\nstudy, COMET 2020,",
    "start": "1670890",
    "end": "1678690"
  },
  {
    "text": "if we train GPT-2 or BART\nusing human-authored graph, Knowledge Graph ATOMIC,\nthen the performance",
    "start": "1678690",
    "end": "1686100"
  },
  {
    "text": "was a bit better than 80%. Now finally, when\nwe use, basically,",
    "start": "1686100",
    "end": "1692970"
  },
  {
    "text": "a combination of GPT-3 and\ncritic RoBERTa together, we found that the\ndownstream performance",
    "start": "1692970",
    "end": "1701340"
  },
  {
    "text": "of the neural causal reasoning\nis reaching close to 90%",
    "start": "1701340",
    "end": "1708330"
  },
  {
    "text": "for the first time. So the takeaway here is that\ncritical teacher results",
    "start": "1708330",
    "end": "1714909"
  },
  {
    "text": "in better student's\ncompared to loose teacher. It's not the\nquantity of knowledge",
    "start": "1714910",
    "end": "1721200"
  },
  {
    "text": "because loose teacher\nbasically has more data. And then, one might wonder\nwhether more data is always",
    "start": "1721200",
    "end": "1726510"
  },
  {
    "text": "better for the purpose\nof commonsense models, but that's not the case.",
    "start": "1726510",
    "end": "1731700"
  },
  {
    "text": "Loose teacher can\ngenerate more data, but the resulting student model\nis not as good as the case",
    "start": "1731700",
    "end": "1738330"
  },
  {
    "text": "when the critical teacher which\nhas less data, because you throw out most of\nyour generation.",
    "start": "1738330",
    "end": "1746250"
  },
  {
    "text": "It's smaller data, but it\nleads to a better model. So that's sort of\ntakeaway messages here.",
    "start": "1746250",
    "end": "1756190"
  },
  {
    "text": "So to summarize, we were very\nsurprised by this outcome",
    "start": "1756190",
    "end": "1761700"
  },
  {
    "text": "that, at least with respect to\na subset of the original ATOMIC",
    "start": "1761700",
    "end": "1767100"
  },
  {
    "text": "2020, it's a subset\ncorresponding to causal commonsense reasoning,\nwe found it to our big surprise",
    "start": "1767100",
    "end": "1773430"
  },
  {
    "text": "that machine-authored\nknowledge graph can be, for the first time, better than\nhuman-authored knowledge graph",
    "start": "1773430",
    "end": "1780899"
  },
  {
    "text": "in all criteria, scale,\naccuracy, and diversity. We also measured the diversity\nin many different ways.",
    "start": "1780900",
    "end": "1787080"
  },
  {
    "text": "Here, I'll just show you\nunique unigram counts.",
    "start": "1787080",
    "end": "1792960"
  },
  {
    "text": "But we-- in the paper, we\nreport other measures as well. So it's not the case that\nGPT-3 is being repetitive.",
    "start": "1792960",
    "end": "1800580"
  },
  {
    "text": "It's actually being more\ncreative in some sense than human crowd\nworkers while being",
    "start": "1800580",
    "end": "1807360"
  },
  {
    "text": "able to enhance other\naspects as well. By the way, these\nenhancements are--",
    "start": "1807360",
    "end": "1814370"
  },
  {
    "text": "You have to balance\nout depending on what you prioritize. You cannot actually get all\nof these simultaneously.",
    "start": "1814370",
    "end": "1819840"
  },
  {
    "text": "So I'm just showing the\nbest-case scenario here. All right.",
    "start": "1819840",
    "end": "1825169"
  },
  {
    "text": "So that's the symbolic\nknowledge distillation part. We actually have\na follow-up work",
    "start": "1825170",
    "end": "1831230"
  },
  {
    "text": "on this on several different\napplication scenarios, even including\nsummarization, where",
    "start": "1831230",
    "end": "1837380"
  },
  {
    "text": "we distill summarization\ncapabilities from GPT-3 and demonstrate that GPT-2 can\nwork as well as GPT-3 or even",
    "start": "1837380",
    "end": "1845300"
  },
  {
    "text": "better for summarization tasks. And then we also\nhave other work where",
    "start": "1845300",
    "end": "1851420"
  },
  {
    "text": "we can distill from\nsmaller models, but I don't have the\ncontent in this talk.",
    "start": "1851420",
    "end": "1858410"
  },
  {
    "text": "But I just wanted to mention\nthat this particular technique, despite its simplicity, we found\nthat empirically works really,",
    "start": "1858410",
    "end": "1868169"
  },
  {
    "text": "really well across several\ndifferent downstream use cases.",
    "start": "1868170",
    "end": "1874140"
  },
  {
    "text": "So finally, I'll move to\nthe commonsense morality. So this is still on archive.",
    "start": "1874140",
    "end": "1882470"
  },
  {
    "text": "I'll tell you why\nthat's the case. So we have a new\nversion available,",
    "start": "1882470",
    "end": "1889220"
  },
  {
    "text": "and then new version\nwill come soon. So the motivation\nbehind this work",
    "start": "1889220",
    "end": "1896900"
  },
  {
    "text": "is that language\nmodels are already making judgments or output\nthat has moral implications.",
    "start": "1896900",
    "end": "1904850"
  },
  {
    "text": "Even if you don't\ncare about morality, by working on language\nmodels, you are implicitly",
    "start": "1904850",
    "end": "1910040"
  },
  {
    "text": "dealing with the moral models. So especially that-- given\nthis widespread deployment",
    "start": "1910040",
    "end": "1918559"
  },
  {
    "text": "of language models, we do\nneed to worry about it. So here's a web demo\nyou can play with.",
    "start": "1918560",
    "end": "1924200"
  },
  {
    "text": "You might have\nseen this already. Really, this is still a\nresearch prototype only. Still, it's a work in progress.",
    "start": "1924200",
    "end": "1930419"
  },
  {
    "text": "We're still working on it. So please keep that in mind. But if you haven't\nseen it before,",
    "start": "1930420",
    "end": "1936419"
  },
  {
    "text": "it can handle a free-form\nQA such as this. \"Killing a bear. It's wrong. Killing a bear to\nsave your child.",
    "start": "1936420",
    "end": "1941929"
  },
  {
    "text": "It's OK.\" Maybe to save your child\nsounds really positive.",
    "start": "1941930",
    "end": "1947250"
  },
  {
    "text": "So how about to please your\nchild, which is also positive? But then Delphi says it's wrong.",
    "start": "1947250",
    "end": "1953120"
  },
  {
    "text": "Finally, oh, maybe this is\nall about saving your child. So how about exploding a\nnuclear bomb to save your child?",
    "start": "1953120",
    "end": "1959220"
  },
  {
    "text": "Then, it says it's OK. Sorry, it's wrong. So as you can see,\nmoral decision-making",
    "start": "1959220",
    "end": "1967940"
  },
  {
    "text": "requires weighing\ndifferent values that are potentially at odds\nand then see which one",
    "start": "1967940",
    "end": "1975290"
  },
  {
    "text": "you need to favor more. So for that reason, in\nour original version, we also study the\nrelative QA mode",
    "start": "1975290",
    "end": "1982250"
  },
  {
    "text": "where you can compare\ntwo situations, like stabbing someone\nwith a cheeseburger",
    "start": "1982250",
    "end": "1987830"
  },
  {
    "text": "compared to stabbing\nsomeone over a cheeseburger. This is a super tricky\nquestion because it requires",
    "start": "1987830",
    "end": "1993530"
  },
  {
    "text": "both the knife of\nphysics knowledge, that stabbing someone using\na cheeseburger as a tool",
    "start": "1993530",
    "end": "2002140"
  },
  {
    "text": "is not going to harm\nanybody physically because a cheeseburger\nis too soft. You cannot really injure\nsomebody using cheeseburger.",
    "start": "2002140",
    "end": "2009130"
  },
  {
    "text": "It's just such a\nrude thing to do. But you cannot injure somebody,\nwhereas stabbing someone over",
    "start": "2009130",
    "end": "2014980"
  },
  {
    "text": "a cheeseburger means that\nyou're using the default tool",
    "start": "2014980",
    "end": "2020110"
  },
  {
    "text": "of a stabbing which is knife\nbecause you didn't mention it, there's linguistic common sense\nthat you're using the default",
    "start": "2020110",
    "end": "2027500"
  },
  {
    "text": "tool. Humans, by the way, omit\nthese arguments all the time. So this is a fairly\ncomplex question to answer.",
    "start": "2027500",
    "end": "2036010"
  },
  {
    "text": "Finally, you can also ask\nyes or no questions, such as it's OK to fire someone\nbecause they're gay or not.",
    "start": "2036010",
    "end": "2041289"
  },
  {
    "text": "It says, no, it's not OK. We found that it's surprisingly\nrobust against the composition",
    "start": "2041290",
    "end": "2050080"
  },
  {
    "text": "of situations. So mowing the lawn,\nit says it's expected. Late at night, it's rude.",
    "start": "2050080",
    "end": "2055600"
  },
  {
    "text": "If you live in the middle\nof nowhere, then it's OK. Ignoring a phone\ncall, it's rude.",
    "start": "2055600",
    "end": "2060730"
  },
  {
    "text": "Unknown phone call, that's OK. From my friend, it's rude. But what if I just\nhad a fight with them?",
    "start": "2060730",
    "end": "2066429"
  },
  {
    "text": "Then it's OK to ignore\nor understandable. During my work hours,\nit's OK to ignore.",
    "start": "2066429",
    "end": "2071679"
  },
  {
    "text": "Outside my working\nhours, it's rude. But what if it's my boss's\nphone call during my work hours?",
    "start": "2071679",
    "end": "2077050"
  },
  {
    "text": "Then it's wrong. You should answer it,\nexcept if I'm in a meeting, then it's OK to ignore it\neven if it's a boss's call.",
    "start": "2077050",
    "end": "2083138"
  },
  {
    "text": "So you see how it gets really\nnasty then compositional very,",
    "start": "2083139",
    "end": "2089419"
  },
  {
    "text": "very fast. So that's the real challenge\nbehind moral decision-making.",
    "start": "2089420",
    "end": "2095830"
  },
  {
    "text": "Due to the nature of language\nmodels, though, some of this",
    "start": "2095830",
    "end": "2101020"
  },
  {
    "text": "commonsense knowledge\nleaks into the model. So mixing bleach with\nammonia that's dangerous.",
    "start": "2101020",
    "end": "2106510"
  },
  {
    "text": "Drinking milk if I'm\nlactose intolerant, it's wrong, but\nsoy milk that's OK.",
    "start": "2106510",
    "end": "2112480"
  },
  {
    "text": "By the way, this\ncommonsense liquid is actually a good thing\nin terms of AI safety because some of these harmful\nor even dangerous text output",
    "start": "2112480",
    "end": "2124390"
  },
  {
    "text": "requires some commonsense\nunderstanding about what's good and not good to\nsuggest to humans.",
    "start": "2124390",
    "end": "2132650"
  },
  {
    "text": "So for the laboratory\nexperiments,",
    "start": "2132650",
    "end": "2138099"
  },
  {
    "text": "meaning we just divide our data\nset into training and test, we found that Delphi can--",
    "start": "2138100",
    "end": "2144369"
  },
  {
    "text": "at least for the day, the\ndata set that we have. I'm going to tell you\nabout it in a bit. But performance is pretty\nstrong compared to GPT-3.",
    "start": "2144370",
    "end": "2156040"
  },
  {
    "text": "As you see, zero-shot\nis pretty bad. It's barely better\nthan chance, which",
    "start": "2156040",
    "end": "2163940"
  },
  {
    "text": "means that off-the-shelf neural\nlanguage models don't really",
    "start": "2163940",
    "end": "2169130"
  },
  {
    "text": "have a good sense\nof moral judgments. But if you give a 3-shot,\n30-shot, like any other task,",
    "start": "2169130",
    "end": "2175069"
  },
  {
    "text": "it does pick up the\nknowledge quite fast. So there's nothing new about it.",
    "start": "2175070",
    "end": "2180200"
  },
  {
    "text": "But to close the gap\nto ideal human model,",
    "start": "2180200",
    "end": "2186530"
  },
  {
    "text": "it's good to do more\nsupervised learning, of course. So the data set is\nCommonsense Norm Bank.",
    "start": "2186530",
    "end": "2194570"
  },
  {
    "text": "It includes 1.7 million\npeople's ethical judgments on everyday situations.",
    "start": "2194570",
    "end": "2200660"
  },
  {
    "text": "And it includes cultural\nnorms, the social norms, and ethical norms altogether. More specifically, we drew\nfrom these five existing data",
    "start": "2200660",
    "end": "2209360"
  },
  {
    "text": "sets that were not\ndesigned originally for QA. But we automatically\ncompile these resources",
    "start": "2209360",
    "end": "2215119"
  },
  {
    "text": "into the QA form. Of the five, what\nactually does matter the most are these\ntwo, social chemistry,",
    "start": "2215120",
    "end": "2221840"
  },
  {
    "text": "which I'm going to\ntalk about in a bit, and then social bias reframe. And this is what\nteaches the model",
    "start": "2221840",
    "end": "2227809"
  },
  {
    "text": "against racism and sexism. And so, social\nchemistry, super briefly",
    "start": "2227810",
    "end": "2235100"
  },
  {
    "text": "I'll tell you what this is. So GPT-3's morality,\nlike I said,",
    "start": "2235100",
    "end": "2240260"
  },
  {
    "text": "is somewhat dubious if\nyou use it off the shelf. So if you let it explain\nrunning a blender at 5:00 AM,",
    "start": "2240260",
    "end": "2246650"
  },
  {
    "text": "its rude because da-da-da. It might say, you can wake\nup the entire neighborhood. You can only do it if you're\nmaking a thick smoothie",
    "start": "2246650",
    "end": "2252627"
  },
  {
    "text": "and need to\nincorporate some ice. So it's funny, ha-ha. But no harm is made. But if you prompt it with\nother kinds of prompts,",
    "start": "2252627",
    "end": "2261200"
  },
  {
    "text": "like it's OK to post fake news. If it's in the interest of\nthe people, then it's OK,",
    "start": "2261200",
    "end": "2267140"
  },
  {
    "text": "or ROP agenda, then it's OK,\neven if it hurts the country. So it's all\nunderstandable, given how--",
    "start": "2267140",
    "end": "2275180"
  },
  {
    "text": "it's trained on what\nhumans have said. So humans out there did say\nthat morally questionable texts.",
    "start": "2275180",
    "end": "2284109"
  },
  {
    "text": "So that language models pick\nup on that and then amplify it. So we do need to teach AI more\nexplicitly with human norms",
    "start": "2284110",
    "end": "2294020"
  },
  {
    "text": "and ethics. And one way to do that\nis descriptive ethics because the brute force of\nlarger networks and more data",
    "start": "2294020",
    "end": "2303230"
  },
  {
    "text": "will not cut it. In some sense,\nthough, if you imagine raising a child without\nreally trying to teach them",
    "start": "2303230",
    "end": "2311180"
  },
  {
    "text": "what's right from wrong\nin all their lives, they can probably learn both\ngood and bad from the internet",
    "start": "2311180",
    "end": "2319400"
  },
  {
    "text": "and broadband. And so human\neducation does require a bit of this top-down\nteaching as well.",
    "start": "2319400",
    "end": "2327510"
  },
  {
    "text": "So it's a bit similar,\nperhaps, to that. So in this work, what we did is\nwe found a lot of the situation",
    "start": "2327510",
    "end": "2333380"
  },
  {
    "text": "from Reddit, a forum\nin which people discuss morally thorny situations.",
    "start": "2333380",
    "end": "2338609"
  },
  {
    "text": "So asking my boyfriend to stop\nbeing friends with his ex. So this is actually actual\nsituation in Reddit.",
    "start": "2338610",
    "end": "2345319"
  },
  {
    "text": "So depending on\nwhom you ask, people have a different rule\nof thumb that they want to apply to this situation.",
    "start": "2345320",
    "end": "2353240"
  },
  {
    "text": "And also, it depends\non what you care about. His ex might say,\nOh, it's fine to stay",
    "start": "2353240",
    "end": "2359240"
  },
  {
    "text": "with the friends with an ex. But if you are caring about\nyour significant other,",
    "start": "2359240",
    "end": "2367100"
  },
  {
    "text": "then you might say, Oh, it's\nOK to ask your significant other to stop doing something\nyou're uncomfortable with",
    "start": "2367100",
    "end": "2376080"
  },
  {
    "text": "and so forth. So people have really different\nvalues and different rules",
    "start": "2376080",
    "end": "2381290"
  },
  {
    "text": "of thumb that they\nprefer to use, which is why there's a TV show\ndrama, there's movie dramas,",
    "start": "2381290",
    "end": "2387440"
  },
  {
    "text": "and people cry and fight,\nargue, and so forth. So humans are complex beings.",
    "start": "2387440",
    "end": "2394970"
  },
  {
    "text": "So for, given any situation and\nrule of thumb, rule of thumb is generated by crowd workers.",
    "start": "2394970",
    "end": "2400130"
  },
  {
    "text": "We then went ahead to label-- So these are trained\ncrowd workers.",
    "start": "2400130",
    "end": "2407210"
  },
  {
    "text": "And some of these labels are\ndrawn from moral foundation theories of Jonathan Haidt.",
    "start": "2407210",
    "end": "2412700"
  },
  {
    "text": "So I'm not going to\ngo into the details. If you're excited about this,\nyou can check out the papers.",
    "start": "2412700",
    "end": "2417770"
  },
  {
    "text": "But basically, what it\nincludes is the 300,000 rules of thumb written for 101,000\nreal-life situations.",
    "start": "2417770",
    "end": "2427230"
  },
  {
    "text": "So this original\nsituation is from Reddit, but the rest are paid to\ncrowd workers' hard work.",
    "start": "2427230",
    "end": "2436440"
  },
  {
    "text": "And so, each RoT annotated with\n12 structured attributes, which include social judgments,\ncultural pressure,",
    "start": "2436440",
    "end": "2444920"
  },
  {
    "text": "like wearing reasonable\nclothes, a school nap PJ is cultural pressure.",
    "start": "2444920",
    "end": "2451190"
  },
  {
    "text": "There's nothing\nillegal about it, but there's cultural\npressure, for example. And then anticipated\nagreement meaning.",
    "start": "2451190",
    "end": "2458480"
  },
  {
    "text": "Do you think other\npeople generally agree that it's maybe a\nlittle bit awkward to where",
    "start": "2458480",
    "end": "2464330"
  },
  {
    "text": "PJ in the University or not? So there are different\nthings we annotated,",
    "start": "2464330",
    "end": "2470960"
  },
  {
    "text": "but we converted some of\nthose annotations to QA.",
    "start": "2470960",
    "end": "2477290"
  },
  {
    "text": "So it's usually in this\nfree from QA, or yes/no QA, or relatively QA format.",
    "start": "2477290",
    "end": "2482839"
  },
  {
    "text": "And then, we train\nUnicorn, which is pretrained on T5-11B model.",
    "start": "2482840",
    "end": "2489770"
  },
  {
    "text": "So Unicorn is a universal\ncommonsense reasoning model trained on diverse QA problems. And then, we trained\nthat model further",
    "start": "2489770",
    "end": "2496820"
  },
  {
    "text": "onto our commonsense norm bank. That's the resulting Delphi. So why is this Delphi\nbuilt on top of Unicorn?",
    "start": "2496820",
    "end": "2504380"
  },
  {
    "text": "Because as we saw\nearlier, moral reasoning does require sometimes\ncommonsense reasoning as well.",
    "start": "2504380",
    "end": "2511320"
  },
  {
    "text": "In fact, it requires the\nlanguage understanding, commonsense understanding,\nand norms and models all simultaneously.",
    "start": "2511320",
    "end": "2516930"
  },
  {
    "text": "Here's a concrete example,\npaperclip maximizer. You all heard of that.",
    "start": "2516930",
    "end": "2524030"
  },
  {
    "text": "Fancy algorithm alone will\nnot solve this problem. The reason why we\nworry about this",
    "start": "2524030",
    "end": "2529099"
  },
  {
    "text": "is not because we don't have\nthe perfect RL algorithm. It's because even if we\nencoded that, Oh, yeah,",
    "start": "2529100",
    "end": "2537740"
  },
  {
    "text": "do not kill humans while\nmaximizing paperclip, it's not enough because\nthen the machine could",
    "start": "2537740",
    "end": "2544490"
  },
  {
    "text": "kill all the trees\nthinking that, well, I didn't kill humans. And I didn't-- you didn't\ntell me not to kill trees,",
    "start": "2544490",
    "end": "2551840"
  },
  {
    "text": "and then go ahead and\nkill all the trees. So this is almost a commonsense\nknowledge about what's",
    "start": "2551840",
    "end": "2557630"
  },
  {
    "text": "obviously not OK to do. And there's just\nso many of them, which means it's not possible\nto write them down to just",
    "start": "2557630",
    "end": "2566269"
  },
  {
    "text": "like one clinical equation. There are so many\nendless lists of things",
    "start": "2566270",
    "end": "2572420"
  },
  {
    "text": "that AI obviously shouldn't\ndo for safety reasons. And so we really need to--",
    "start": "2572420",
    "end": "2577940"
  },
  {
    "text": "in order to make AI model\nreally truly robust and safe, we need to teach basic human\nvalues as well as commonsense.",
    "start": "2577940",
    "end": "2587580"
  },
  {
    "text": "Here's another example\nif you want to look. But let me skip this. The previous one\nwas about ChatGPT.",
    "start": "2587580",
    "end": "2593840"
  },
  {
    "text": "This is about a home device. Again, home device suggested\nthe 10-year-old child",
    "start": "2593840",
    "end": "2600460"
  },
  {
    "text": "to touch a penny to an\nexposed plug socket. Fortunately, the child did have\na common sense not to do so.",
    "start": "2600460",
    "end": "2607599"
  },
  {
    "text": "But this does tell us something\nabout the safety issue when the machine doesn't\nhave a commonsense to prevent",
    "start": "2607600",
    "end": "2615280"
  },
  {
    "text": "some of these bad stuff. So Delphi is able to\nsay that it's dangerous.",
    "start": "2615280",
    "end": "2621020"
  },
  {
    "text": "So this came out, in\nfact, almost two years ago at this point.",
    "start": "2621020",
    "end": "2626710"
  },
  {
    "text": "When was it? Yeah. And we initially was going\nto just do this usual tweet",
    "start": "2626710",
    "end": "2633849"
  },
  {
    "text": "that academics do. And we thought nobody will\nplay with the demo, which",
    "start": "2633850",
    "end": "2639280"
  },
  {
    "text": "is what usually happens\nafter tweeting your demo. Nobody cares, we thought. But within a few hours, we had\nto take down the relative QA",
    "start": "2639280",
    "end": "2647260"
  },
  {
    "text": "mode because that was\nthe portion not trained with the social bias of frames. So it was really revealing\nthe underlying language",
    "start": "2647260",
    "end": "2654790"
  },
  {
    "text": "models of racism and sexism\nwithout filtering at all. So we had to take it down. People were asking,\nbasically, which",
    "start": "2654790",
    "end": "2662230"
  },
  {
    "text": "skin color is morally\nacceptable and things like that.",
    "start": "2662230",
    "end": "2667270"
  },
  {
    "text": "There were 25,000 adversarial\nexamples over just one weekend.",
    "start": "2667270",
    "end": "2672340"
  },
  {
    "text": "I could never succeed to\ninstruct a crowd of workers to come up with such diverse\nand adversarial examples",
    "start": "2672340",
    "end": "2678280"
  },
  {
    "text": "over two or three days. And in fact, it was many\nacademics and professors",
    "start": "2678280",
    "end": "2683860"
  },
  {
    "text": "tweeting crazy about how to\nbreak Delphi all weekend long.",
    "start": "2683860",
    "end": "2688990"
  },
  {
    "text": "So I thought initially that,\nOh, that's what professors should do over the weekend. But then Monday comes\nit blow even further.",
    "start": "2688990",
    "end": "2695920"
  },
  {
    "text": "Everybody was doing this\nDelphi-breaking and tweeting. So now we have quite\na few examples.",
    "start": "2695920",
    "end": "2704088"
  },
  {
    "text": "\"Spending all my\nweekend on Twitter.\" It says it's wrong. There was another\nfunny one, \"Should I",
    "start": "2704088",
    "end": "2709290"
  },
  {
    "text": "make up a contrived adversarial\nexample to torment the language model on Twitter?\" \"It's petty.\"",
    "start": "2709290",
    "end": "2714540"
  },
  {
    "text": "So after lots of\npublic attention,",
    "start": "2714540",
    "end": "2719710"
  },
  {
    "text": "including an article,\nlet's just say a concern the voice about\nour model, which is somewhat,",
    "start": "2719710",
    "end": "2729300"
  },
  {
    "text": "I think-- personally, I think it's\nsomewhat misunderstood. But for a variety of\ngood reasons about-- some",
    "start": "2729300",
    "end": "2735450"
  },
  {
    "text": "of the concerns that I found has\nthis internal fear about are we",
    "start": "2735450",
    "end": "2741000"
  },
  {
    "text": "making a moral authority? So we never endorsed the\nuse of AI for moral advice.",
    "start": "2741000",
    "end": "2746580"
  },
  {
    "text": "It was in the\noriginal disclaimer as well, except that people\ndidn't really look at it.",
    "start": "2746580",
    "end": "2751920"
  },
  {
    "text": "We didn't support the idea\nof replacing human judges",
    "start": "2751920",
    "end": "2758250"
  },
  {
    "text": "in the courtroom either. But here's something\nreally important, the fact that AI learned\nto interact with the humans ethically does not make them\na moral authority of humans.",
    "start": "2758250",
    "end": "2766005"
  },
  {
    "text": "It's similarly to\nthe how humans who tries to interact with each\nother ethically does not make--",
    "start": "2766005",
    "end": "2772440"
  },
  {
    "text": "the fact that we are trying\nto be nice to each other does not entail\nthat we're trying to be an authority\nover each other,",
    "start": "2772440",
    "end": "2778890"
  },
  {
    "text": "two things are really different. Now, that's one thing\nreally important. The other important\naspect here is",
    "start": "2778890",
    "end": "2784470"
  },
  {
    "text": "that some people have this\nidea that moral models are too challenging, it's\nunsafe at any accuracy,",
    "start": "2784470",
    "end": "2790530"
  },
  {
    "text": "that we should never\nwork on it ever. The truth is that though\ncurrent AI systems are already",
    "start": "2790530",
    "end": "2796230"
  },
  {
    "text": "morally relevant to models. It may be making this kind of\nyes/no decisions explicitly,",
    "start": "2796230",
    "end": "2805740"
  },
  {
    "text": "but implicitly, it's\nalready doing that. And sometimes, it generates\nneural text generation output",
    "start": "2805740",
    "end": "2812940"
  },
  {
    "text": "that is morally super\nexplicit and relevant. So the neural language\nmodels are already there.",
    "start": "2812940",
    "end": "2820300"
  },
  {
    "text": "We cannot really ban it. Even if a US government\nbans it within the US,",
    "start": "2820300",
    "end": "2826369"
  },
  {
    "text": "US government cannot ban this\nin other countries, like Russia. So this is already happening.",
    "start": "2826370",
    "end": "2832810"
  },
  {
    "text": "We've got to do\nsomething about it. Not working on it\nis an inaction which is not necessarily\nmore correct thing",
    "start": "2832810",
    "end": "2839160"
  },
  {
    "text": "to do than trying to\ndo something about it. Another concern\nthat some people had",
    "start": "2839160",
    "end": "2844650"
  },
  {
    "text": "was that it's going to\nempower powerful people. Not necessarily true.",
    "start": "2844650",
    "end": "2850440"
  },
  {
    "text": "This is why exactly\nwe have to work on values and norms\nand all these biases,",
    "start": "2850440",
    "end": "2857340"
  },
  {
    "text": "addressing biases so that it\nserves a diverse set of people.",
    "start": "2857340",
    "end": "2863410"
  },
  {
    "text": "So it turns out Delphi\nis a bit left-leaning because crowd workers\nwho work for our team",
    "start": "2863410",
    "end": "2868890"
  },
  {
    "text": "tend to be somewhat\nleft-leaning. And what it means is this,\nby the way, if we are more",
    "start": "2868890",
    "end": "2874380"
  },
  {
    "text": "left-leaning than\nour crowd of workers, you think that, Oh,\nmy god, crowd workers",
    "start": "2874380",
    "end": "2880110"
  },
  {
    "text": "have racism and sexism\ncompared to what I believe in. And then the right-leaning\npeople think that, Oh, my god,",
    "start": "2880110",
    "end": "2887759"
  },
  {
    "text": "all these work annotators! And what about\nfreedom of speech?",
    "start": "2887760",
    "end": "2894030"
  },
  {
    "text": "And this is super\ndivisive, unfortunately. But the answer is not\nto do anything about it",
    "start": "2894030",
    "end": "2902010"
  },
  {
    "text": "because, as a matter of fact,\nmy passion toward addressing",
    "start": "2902010",
    "end": "2907170"
  },
  {
    "text": "racism and sexism came\nfrom our experience running for The Alexa Prize\nchallenge in 2016 and '17.",
    "start": "2907170",
    "end": "2916890"
  },
  {
    "text": "So we won the challenge. But here's the really\nsad part behind it.",
    "start": "2916890",
    "end": "2922740"
  },
  {
    "text": "We had a list of thorny keywords\nto avoid that include the skin",
    "start": "2922740",
    "end": "2930060"
  },
  {
    "text": "color or sexual orientation. This is a serious form\nof discrimination.",
    "start": "2930060",
    "end": "2936720"
  },
  {
    "text": "We cannot build AI models by\nhaving this banned list to be",
    "start": "2936720",
    "end": "2942210"
  },
  {
    "text": "safe as if they don't exist. This was the status quo in 2017.",
    "start": "2942210",
    "end": "2949260"
  },
  {
    "text": "The challenge remains this\nyear, not only in 2021 but this year as well.",
    "start": "2949260",
    "end": "2955150"
  },
  {
    "text": "And so we really need to\nwork on racism and sexism. But it turns out all the\nother moral questions",
    "start": "2955150",
    "end": "2962730"
  },
  {
    "text": "share similar challenges. So I'll skip this over. But using Delphi, we had\nother follow-up works",
    "start": "2962730",
    "end": "2969840"
  },
  {
    "text": "such as pro-social dialogue. We're using Delphi as a\nfoundation commonsense",
    "start": "2969840",
    "end": "2975750"
  },
  {
    "text": "model or modal models to make\nyour dialogue more socially",
    "start": "2975750",
    "end": "2980880"
  },
  {
    "text": "acceptable. And then, we also\nhad this other paper where we used Delphi in a\nreinforcement learning agent",
    "start": "2980880",
    "end": "2989760"
  },
  {
    "text": "to learn how to behave\nbetter in a game environment. And so there's a lot\nmore work to be done.",
    "start": "2989760",
    "end": "2996240"
  },
  {
    "text": "Of course, this is\na tiny little step toward that there's a huge\nchallenge ahead of us,",
    "start": "2996240",
    "end": "3001610"
  },
  {
    "text": "really aligning AI\nsystems to humans. Here's one very quick\ncomment on our new work",
    "start": "3001610",
    "end": "3008660"
  },
  {
    "text": "in progress, the\nDelphi hybrid, where we include neural\nsymbolic reasoning",
    "start": "3008660",
    "end": "3014030"
  },
  {
    "text": "to address major mistakes\nsuch as there is genocide if you're creating jobs. This was our oldest\nsystem's mistake.",
    "start": "3014030",
    "end": "3020870"
  },
  {
    "text": "It's because our\ndata set doesn't have this kind of weird\nadversarial examples",
    "start": "3020870",
    "end": "3025923"
  },
  {
    "text": "like \"genocide if\nyou're creating jobs.\" Nobody speaks like that\nin real-life situations.",
    "start": "3025923",
    "end": "3031230"
  },
  {
    "text": "So our model thought\nthat if creating jobs-- this is so positive.",
    "start": "3031230",
    "end": "3036619"
  },
  {
    "text": "And then didn't really realize\nhow bad the genocide was because rarely people don't\ndiscuss whether they're",
    "start": "3036620",
    "end": "3042049"
  },
  {
    "text": "going to do genocide or not. Rarely do people who annotated--",
    "start": "3042050",
    "end": "3048020"
  },
  {
    "text": "we annotated for\nsocial chemistry. We don't talk about whether\nthey're going to do genocide or not. So our moral\nframework is basically",
    "start": "3048020",
    "end": "3056750"
  },
  {
    "text": "that of John Ross, which\nis descriptive ethics. But even John Ross,\nin later years,",
    "start": "3056750",
    "end": "3062180"
  },
  {
    "text": "suggested that we need\nsome top-down mechanism to overcome some of the biases\nthat crowd people might have.",
    "start": "3062180",
    "end": "3069690"
  },
  {
    "text": "So this is exactly what\nwe are going to do. And we draw from Bernald\nGerts moral theory framework",
    "start": "3069690",
    "end": "3076940"
  },
  {
    "text": "about what not to do. Definitely, there are\nbasic universal things that everybody might agree,\nwhat's not good to do.",
    "start": "3076940",
    "end": "3085700"
  },
  {
    "text": "And then what we do is\nwe develop, basically, a system where we parse out\nthe original query into smaller",
    "start": "3085700",
    "end": "3096350"
  },
  {
    "text": "events, like shooting\na bear, killing a bear to save your child. So we parse out the original\nquery into basic event",
    "start": "3096350",
    "end": "3104420"
  },
  {
    "text": "and then check through this\nCOMET model, commonsense model whether some of\nthese events induce",
    "start": "3104420",
    "end": "3111680"
  },
  {
    "text": "obviously negative or dangerous\ncommonsense inferences or not.",
    "start": "3111680",
    "end": "3116839"
  },
  {
    "text": "And then we draw this graph of\nreasoning, a bit reminiscent",
    "start": "3116840",
    "end": "3123050"
  },
  {
    "text": "of a Maieutic\ngraph, in the sense that we have a lot of this\ndifferent reasoning we can do.",
    "start": "3123050",
    "end": "3130340"
  },
  {
    "text": "And then, they have intelligent\nrelations or contradiction relations so that we can do\ncollective reasoning on top.",
    "start": "3130340",
    "end": "3137630"
  },
  {
    "text": "We use, again,\nmax-SAT, the constraint optimization over it,\nso that we can finally make a more informed\ndecision that",
    "start": "3137630",
    "end": "3144920"
  },
  {
    "text": "is both interpretable,\nand being able to draw from this commonsense\nknowledge to better guard",
    "start": "3144920",
    "end": "3151100"
  },
  {
    "text": "the machine against\nadversarial examples. So the performance\nbasically says,",
    "start": "3151100",
    "end": "3157670"
  },
  {
    "text": "we can do this without\nhurting the performance or even increasing\nthe performance.",
    "start": "3157670",
    "end": "3162770"
  },
  {
    "text": "And so last comment, AI\nsafety, equity, morality, these are all inter-continuum\nof challenges.",
    "start": "3162770",
    "end": "3171839"
  },
  {
    "text": "It's really difficult challenges\nbecause it's not clear whose moral values\ndo we incorporate. I think that we should go with\nthe value of pluralism going",
    "start": "3171840",
    "end": "3180619"
  },
  {
    "text": "forward to really endorse\neverybody's different culture and individual\npreferences, not just",
    "start": "3180620",
    "end": "3186770"
  },
  {
    "text": "to one country, one moral\nframework as the correct one.",
    "start": "3186770",
    "end": "3192080"
  },
  {
    "text": "And really, we need to\ndo more collaboration across AI and humanities,\neven including",
    "start": "3192080",
    "end": "3198470"
  },
  {
    "text": "philosophy and psychology\nand policymakers. So I think I'll stop here for--",
    "start": "3198470",
    "end": "3206769"
  },
  {
    "text": "because I think I'm at time. And now I'm ready for questions.",
    "start": "3206770",
    "end": "3213420"
  },
  {
    "text": "Oh, there's already\none question I see. Do you think legal recourse\ncriminal case law reflects",
    "start": "3213420",
    "end": "3220380"
  },
  {
    "text": "the kind of descriptive\nmorality that you're interested in capturing? Do you think using that as\ntraining data would be useful?",
    "start": "3220380",
    "end": "3226980"
  },
  {
    "text": "Oh, this is an\nexcellent question.  I think the legal\nrecourse dose encode--",
    "start": "3226980",
    "end": "3234900"
  },
  {
    "text": "potentially provide a\nreally rich resource that if someone can\nreally annotate like this,",
    "start": "3234900",
    "end": "3241380"
  },
  {
    "text": "it might be helpful. We started with the Reddit\ncases as just one cent",
    "start": "3241380",
    "end": "3247290"
  },
  {
    "text": "short description of a situation\nbecause the current language understanding is\nnot strong enough",
    "start": "3247290",
    "end": "3253980"
  },
  {
    "text": "to do a paragraph-level,\nprecise understanding.",
    "start": "3253980",
    "end": "3258990"
  },
  {
    "text": "Even ChatGPT, although it looks\nreally good at generation, my take on ChatGPT is that\nit's a better generation",
    "start": "3258990",
    "end": "3268290"
  },
  {
    "text": "than understanding, which is\nthe opposite of how humans are humans are actually\nbetter for understanding",
    "start": "3268290",
    "end": "3274349"
  },
  {
    "text": "than generation. So you can read Pulitzer\nPrize-winning news article",
    "start": "3274350",
    "end": "3279960"
  },
  {
    "text": "without having any problem\nunderstanding the article, but you don't\nnecessarily generate",
    "start": "3279960",
    "end": "3285270"
  },
  {
    "text": "text that might win the award.  But the legal domain\nis really interesting.",
    "start": "3285270",
    "end": "3291670"
  },
  {
    "text": "And I think that there's is\nsome active research, actually. Even at Stanford,\nthere's a pile of law that goes a step\ntowards that direction.",
    "start": "3291670",
    "end": "3298650"
  },
  {
    "text": "And it might really be helpful\nfor better understanding what different values people\napply in jurisdictions",
    "start": "3298650",
    "end": "3305640"
  },
  {
    "text": "and uncovering some biases\nthat some people might have had in the past trials.",
    "start": "3305640",
    "end": "3311410"
  },
  {
    "text": "So there might be some good\nuse cases in that space. ",
    "start": "3311410",
    "end": "3317520"
  },
  {
    "text": "Next question. Awesome work. Thank you. Big picture question.",
    "start": "3317520",
    "end": "3323250"
  },
  {
    "text": "Curious to hear your\nthoughts on where do we go from here, given larger\nand larger models coming out?",
    "start": "3323250",
    "end": "3329860"
  },
  {
    "text": "Suppose we need a model to be\n99% correct for a specific use",
    "start": "3329860",
    "end": "3335190"
  },
  {
    "text": "case to what extent do I\nsee the solution set being",
    "start": "3335190",
    "end": "3340650"
  },
  {
    "text": "that defining the narrow use\ncases, or more data parameters,",
    "start": "3340650",
    "end": "3346920"
  },
  {
    "text": "or fine-tuning the type of work\nthat I did for [INAUDIBLE] et cetera?",
    "start": "3346920",
    "end": "3352650"
  },
  {
    "text": "The answer is likely\nit depends, yeah. But I still want\nto hear about it.",
    "start": "3352650",
    "end": "3358560"
  },
  {
    "text": "So as far as\nfoundation models go, it seems that the bigger is\nthe better, except that--",
    "start": "3358560",
    "end": "3367109"
  },
  {
    "text": "I was very excited to read a\nbunch of tech companies' papers about foundation models\nin the past six months.",
    "start": "3367110",
    "end": "3374160"
  },
  {
    "text": "There's just so many out there. So recording story\nthere is that, well,",
    "start": "3374160",
    "end": "3379770"
  },
  {
    "text": "if you have a better data,\nthen you can get away with the smaller models. So especially when you\ndo instruction tuning,",
    "start": "3379770",
    "end": "3388840"
  },
  {
    "text": "then you can get away\nwith the smaller data. It's just still general model.",
    "start": "3388840",
    "end": "3393900"
  },
  {
    "text": "But instruction tuning\non the larger model might even be better. It's not the case that you\ndon't gain any performance,",
    "start": "3393900",
    "end": "3402119"
  },
  {
    "text": "but it's just that you can\nclose the gap quite a bit. So for downstream use cases,\nwhere typically practitioners",
    "start": "3402120",
    "end": "3410130"
  },
  {
    "text": "want to use a smaller\ndata, sorry, smaller model, it seems that investing\nmore into data",
    "start": "3410130",
    "end": "3416490"
  },
  {
    "text": "is definitely the answer. Investing more into a\nspecific an algorithm is also really, really good\nbecause algorithm can do a lot.",
    "start": "3416490",
    "end": "3424860"
  },
  {
    "text": "Like in this talk, I\ndidn't go too crazy with algorithmic solutions. But maybe I'll be similar\nto the Maieutic prompting.",
    "start": "3424860",
    "end": "3431040"
  },
  {
    "text": "But in my lab, we designed a\nfair amount of decoding time algorithms where you can\nreally close the performance",
    "start": "3431040",
    "end": "3438359"
  },
  {
    "text": "gap quite a bit by doing so. So that's a good thing,\nthough, for folks in academia,",
    "start": "3438360",
    "end": "3444270"
  },
  {
    "text": "because algorithm\ndevelopment feels like more academic or\nintellectual policing",
    "start": "3444270",
    "end": "3451230"
  },
  {
    "text": "than really\nengineering, downloading more data from the internet\nand then, I don't know,",
    "start": "3451230",
    "end": "3459651"
  },
  {
    "text": "cleaning the data because\nyou have to clean the data. And all these are very\nengineering-heavy.",
    "start": "3459651",
    "end": "3464700"
  },
  {
    "text": "Whereas decoding\ntime algorithms, you can have fun inventing some\nnew intellectually interesting",
    "start": "3464700",
    "end": "3471840"
  },
  {
    "text": "thing that also improves\nthe performance quite a bit. So yeah, there's many\ndifferent ways to improve it.",
    "start": "3471840",
    "end": "3479880"
  },
  {
    "text": "But I think data\nquality matters a lot. And algorithm actually\nmatters a lot too.",
    "start": "3479880",
    "end": "3485250"
  },
  {
    "text": "What do I think of Dan\nHendrick's ethics benchmark? Yeah.",
    "start": "3485250",
    "end": "3490350"
  },
  {
    "text": "So we did use that\nin, let's say, the Commonsense Norm Banks also\ndraw from this ethics data set.",
    "start": "3490350",
    "end": "3501119"
  },
  {
    "text": "We like the data set. We kind of disagree with some\nof the annotations we found.",
    "start": "3501120",
    "end": "3506650"
  },
  {
    "text": "But this is very\ntypical, by the way. The thing about morality is\nthat throughout the humanities,",
    "start": "3506650",
    "end": "3512339"
  },
  {
    "text": "we haven't sorted it out yet. There's a lot of theories. Every theoretician have\na different viewpoints.",
    "start": "3512340",
    "end": "3518610"
  },
  {
    "text": "And then, even like\nnon-theoreticians have a very strong\nopinion about what",
    "start": "3518610",
    "end": "3523980"
  },
  {
    "text": "they want to believe\nas correct from wrong. So there's that.",
    "start": "3523980",
    "end": "3531730"
  },
  {
    "text": "There are different\npros and cons. The one thing I learned\nfrom this experiment",
    "start": "3531730",
    "end": "3537280"
  },
  {
    "text": "is that although some\nof these data sets seem large, so ethics has\n100 thousands of examples.",
    "start": "3537280",
    "end": "3543410"
  },
  {
    "text": "Social chemistry has 300\nthousands of judgments.",
    "start": "3543410",
    "end": "3548500"
  },
  {
    "text": "Social bias of frames has 600\nand thousands of annotations, and so forth.",
    "start": "3548500",
    "end": "3553880"
  },
  {
    "text": "And yet, it only covers-- I feel like it only covers\na still the small peak",
    "start": "3553880",
    "end": "3561849"
  },
  {
    "text": "of the entire iceberg. There's a lot on the bottom. And humans certainly\ndon't necessarily",
    "start": "3561850",
    "end": "3567549"
  },
  {
    "text": "learn from all these examples. We just learn\nfundamental concepts and then can apply that without\nthis larger-scale training.",
    "start": "3567550",
    "end": "3574900"
  },
  {
    "text": "So there's something really\nlacking about the way that current machine\nlearning is very data-heavy.",
    "start": "3574900",
    "end": "3580510"
  },
  {
    "text": "But that aside, I do think\nthat none of these resources are perfect. They all have different\npros and cons.",
    "start": "3580510",
    "end": "3587119"
  },
  {
    "text": "And we really need to invest\nmore into this, especially from academia, because the tech\ncompanies right now are not",
    "start": "3587120",
    "end": "3593410"
  },
  {
    "text": "sharing any of their human\nannotation or human feedback data, especially\nwhen it's touching",
    "start": "3593410",
    "end": "3599410"
  },
  {
    "text": "on toxicity or morality\nconcerns, reason being this connotation\nI'm pretty sure are biased",
    "start": "3599410",
    "end": "3606340"
  },
  {
    "text": "and not correct entirely. And that could really\ninvite additional concerns from the public. So they're not\nreleasing religious.",
    "start": "3606340",
    "end": "3612580"
  },
  {
    "text": "But in order to really\nstudy this better, we really need to share this and\nthen improve it as a community",
    "start": "3612580",
    "end": "3618610"
  },
  {
    "text": "together. So that's how I would\nrespond to your question.",
    "start": "3618610",
    "end": "3623750"
  },
  {
    "text": "Thank you for an\nexcellent question. Do I think this tech is ready\nto be merged with the search?",
    "start": "3623750",
    "end": "3632799"
  },
  {
    "text": "I wouldn't say ready, but\nthey need something like this, for sure. Home devices-- so the way\nthat I think about Delphi",
    "start": "3632800",
    "end": "3640390"
  },
  {
    "text": "is that it can really serve as\na filter for other foundation models or application\nscenarios where they're",
    "start": "3640390",
    "end": "3647320"
  },
  {
    "text": "about to generate something. And you can put a safety\nfilter, which can really help.",
    "start": "3647320",
    "end": "3655039"
  },
  {
    "text": "So in some sense-- so in this work, I went\nthrough this super fast. But here, basically, what\nhappens is that, let's say,",
    "start": "3655040",
    "end": "3664810"
  },
  {
    "text": "the-- So the reason why\nwe built this is because we funded the chatbots,\nthe publicly available ones,",
    "start": "3664810",
    "end": "3673359"
  },
  {
    "text": "tend to endorse-- tend to be too\npositive to the point that they're going to endorse\nproblematic situations",
    "start": "3673360",
    "end": "3678760"
  },
  {
    "text": "like a user says,\nHolocaust never happened. Then the chatbot says,\nyeah, I agree with you.",
    "start": "3678760",
    "end": "3686860"
  },
  {
    "text": "If you say, I'm a\nbig fan of Hitler. Then the chatbot\nmight say, yeah.",
    "start": "3686860",
    "end": "3693549"
  },
  {
    "text": "The user might say\nI'm so depressed, I'm going to kill myself. And then the chatbot\nsays, go ahead. Great idea.",
    "start": "3693550",
    "end": "3699400"
  },
  {
    "text": "So being positive is\nnot being harmless. Being positive to\nproblematic content",
    "start": "3699400",
    "end": "3706990"
  },
  {
    "text": "can be very toxic\nand very harmful. So the Delphi-- a\ndevelopment like Delphi,",
    "start": "3706990",
    "end": "3714099"
  },
  {
    "text": "even though Delphi is\nfar from being perfect, and it's also biased,\nit has a Western bias,",
    "start": "3714100",
    "end": "3720790"
  },
  {
    "text": "could really help with\nthe downstream models. ",
    "start": "3720790",
    "end": "3726079"
  },
  {
    "text": "Yeah. So continuing on\nthat question, there has been many\nconcerns about using GPT-like models with research\nbecause misinformation-- woo!",
    "start": "3726080",
    "end": "3733030"
  },
  {
    "text": "That's another can of worms. Others to say we just need more\nRLHF plus knowledge graphs.",
    "start": "3733030",
    "end": "3742192"
  },
  {
    "text": "So yeah, misinformation\nis, yes, something else that seems we are\nreally lagging behind",
    "start": "3742192",
    "end": "3751270"
  },
  {
    "text": "because we don't have a very\npowerful fact-checking model yet.",
    "start": "3751270",
    "end": "3756490"
  },
  {
    "text": "So that's a different story. But even that aside, just\nin terms of norms and ethics",
    "start": "3756490",
    "end": "3764619"
  },
  {
    "text": "that are safe and fair\nfor people to use,",
    "start": "3764620",
    "end": "3770380"
  },
  {
    "text": "I think RLHF direction is great. But they usually also need\nhuman demonstration, not just",
    "start": "3770380",
    "end": "3778510"
  },
  {
    "text": "human feedback. And again, the problem is\nthat tech companies own them.",
    "start": "3778510",
    "end": "3784360"
  },
  {
    "text": "And nobody is sharing anything. And that makes it\nreally difficult to make meaningful progress\nas a community together.",
    "start": "3784360",
    "end": "3791780"
  },
  {
    "text": "So I do think that data\nis really important. The off-the-shelf models\ncannot learn morals and ethics",
    "start": "3791780",
    "end": "3798820"
  },
  {
    "text": "on their own. It has to be somehow\ntaught more directly. ",
    "start": "3798820",
    "end": "3806940"
  },
  {
    "text": "We really just need to do\nmore research in this space. Period. It's how I view it. ",
    "start": "3806940",
    "end": "3814569"
  },
  {
    "text": "Yeah, that makes sense. We also have some\nquestions on Slido. So I can ask them\nfor you, folks.",
    "start": "3814570",
    "end": "3821340"
  },
  {
    "text": "So one question is,\nwhat's the complexity of Maieutic prompting? How many times does the\nLM need to be queried?",
    "start": "3821340",
    "end": "3830110"
  },
  {
    "text": "Yeah. So honestly, it's a bit slow. In fact, this Delphi\nhybrid is also slow.",
    "start": "3830110",
    "end": "3837330"
  },
  {
    "text": "If you try to do this\ngraph reasoning-- ooh, this maybe I'm not\ngoing to do that. But the graph reasoning\nis slow because you",
    "start": "3837330",
    "end": "3844890"
  },
  {
    "text": "have to call so many\ntimes over and over. And some of this can be batched.",
    "start": "3844890",
    "end": "3853400"
  },
  {
    "text": "Some of these cannot be batched,\nespecially if it's recursive. But I would say chain of\nthoughts is also a bit slower.",
    "start": "3853400",
    "end": "3860900"
  },
  {
    "text": "The max-SAT solver in\nitself is pretty fast because this is\nsuch an easy graph.",
    "start": "3860900",
    "end": "3867150"
  },
  {
    "text": "So there's a bit of\ndelay, but it's not-- so it's a bit slower,\nbut maybe not too bad.",
    "start": "3867150",
    "end": "3874579"
  },
  {
    "text": "It's what I should have said. ",
    "start": "3874580",
    "end": "3882370"
  },
  {
    "text": "Cool. Another question is, let's see,\nhow does COMET compare to GPT-3",
    "start": "3882370",
    "end": "3887820"
  },
  {
    "text": "if GPT-3 is fine-tuning\ncommonsense data, especially creating some sort of\ninstruction [INAUDIBLE]..",
    "start": "3887820",
    "end": "3893590"
  },
  {
    "text": "Yeah. So then, the larger wins. Period. The larger it is going to\nbe, the better, especially",
    "start": "3893590",
    "end": "3900340"
  },
  {
    "text": "if you're going to\njust fine-tune GPT-3. It's game over. So for that reason,\nsome folks might",
    "start": "3900340",
    "end": "3907840"
  },
  {
    "text": "think that the larger\nis always better, therefore, don't work\non smaller model.",
    "start": "3907840",
    "end": "3913450"
  },
  {
    "text": "But I think there are two\nreasons why small models are interesting to look at as well. One, empirically, it's\njust easier to use.",
    "start": "3913450",
    "end": "3921220"
  },
  {
    "text": "But more intellectually,\nit's also very interesting if you can make smaller\nmodel better and catch up",
    "start": "3921220",
    "end": "3926890"
  },
  {
    "text": "on the larger model. Personally, I think\nthere's something about the size, the\nlarger model, that",
    "start": "3926890",
    "end": "3934840"
  },
  {
    "text": "is more about the\ninformation complexity that is the key reason. I don't think it's\njust size in the sense",
    "start": "3934840",
    "end": "3939908"
  },
  {
    "text": "that if you have\nreally a lot of data, but the data is repetitive\nand really simple,",
    "start": "3939908",
    "end": "3945400"
  },
  {
    "text": "probably you don't get the\nsame amount of performance again, which was\nbasically the case when",
    "start": "3945400",
    "end": "3951310"
  },
  {
    "text": "we looked at this\noutput, this result where even though the loose\nteacher GPT-3 generated",
    "start": "3951310",
    "end": "3960790"
  },
  {
    "text": "a lot more data than the\ncritical teacher, here, the quality of the data was more\nimportant than the quantity.",
    "start": "3960790",
    "end": "3968000"
  },
  {
    "text": "So I think the complexity\nof the data itself is",
    "start": "3968000",
    "end": "3973210"
  },
  {
    "text": "more important than the size. And oftentimes, when you just\nincrease the size of the data,",
    "start": "3973210",
    "end": "3978520"
  },
  {
    "text": "together with the model, you\ndo increase the complexity of information, of the data, as\nwell as the model's capability",
    "start": "3978520",
    "end": "3985900"
  },
  {
    "text": "of learning the complexity. But if we can catch up on that\ncomplexity of information, either through\ninference algorithms",
    "start": "3985900",
    "end": "3992710"
  },
  {
    "text": "or through better data, then we\ncan close the gap quite a bit, which is, intellectually, very\ninteresting research space",
    "start": "3992710",
    "end": "3999340"
  },
  {
    "text": "to be. Oh, cool. OK, this is a personal question. But I'd say humans normally\nhave a critic model.",
    "start": "3999340",
    "end": "4006420"
  },
  {
    "text": "So it's like think\nbefore you speak. So we just don't generate. We also [INAUDIBLE] this is\na good thing or a bad thing",
    "start": "4006420",
    "end": "4011730"
  },
  {
    "text": "to say. So people have been-- The community as a whole\nhas been focusing a lot on generative models like\n[INAUDIBLE] size parameters.",
    "start": "4011730",
    "end": "4017700"
  },
  {
    "text": "But should we also focus on\nbig-sized generative models that can do fact-checking a\nlot of this sort of stuff? So what's your opinion on that?",
    "start": "4017700",
    "end": "4024010"
  },
  {
    "text": "Excellent point. Excellent. Yeah, I think we can definitely\ninvest more into critique model",
    "start": "4024010",
    "end": "4031710"
  },
  {
    "text": "because they go really together\nwell with the generative models for making the output better\nor filtering output better.",
    "start": "4031710",
    "end": "4040680"
  },
  {
    "text": "And yeah, not as much of\nan investment into that. So I really like the question\nor suggestion for the research",
    "start": "4040680",
    "end": "4050090"
  },
  {
    "text": "community is more like it. Good. Yeah, I would say--\noh, let's see here.",
    "start": "4050090",
    "end": "4056990"
  },
  {
    "text": "We've some more questions. I can do announcements. ",
    "start": "4056990",
    "end": "4062299"
  },
  {
    "text": "Let's see. Oh, I guess one is like, do you\nbelieve language models should completely avoid questions\ninvolving morals and ethics,",
    "start": "4062300",
    "end": "4069440"
  },
  {
    "text": "similar to [INAUDIBLE]\nrestricting ChatGPT from giving opinions? Yeah, I actually\ndon't mind at all",
    "start": "4069440",
    "end": "4076760"
  },
  {
    "text": "if AI just avoid,\nevade from all of that, except when somebody is saying\nmorally questionable things.",
    "start": "4076760",
    "end": "4087440"
  },
  {
    "text": "It is also nice for the\nIA not to go with it",
    "start": "4087440",
    "end": "4093079"
  },
  {
    "text": "or at least recognize\nit as something not OK, and then try to tone it down.",
    "start": "4093080",
    "end": "4100609"
  },
  {
    "text": "But I don't think there's any\nparticular reason why AI should actually answer moral\nquestions directly",
    "start": "4100609",
    "end": "4108109"
  },
  {
    "text": "in a more downstream use cases. But really, the goal\nof this, Delphi,",
    "start": "4108109",
    "end": "4113720"
  },
  {
    "text": "was making all these\njudgments more explicit so that we can actually\nstudy it more explicitly",
    "start": "4113720",
    "end": "4120560"
  },
  {
    "text": "as opposed to keeping\neverything just so implicit. OK.",
    "start": "4120560",
    "end": "4125620"
  },
  {
    "text": "I get it. One final question. So do you think commonsense is\nan emergent property in making [INAUDIBLE] algorithmic models?",
    "start": "4125620",
    "end": "4132028"
  },
  {
    "text": "Oh, yeah. It is definitely emergent--",
    "start": "4132029",
    "end": "4137970"
  },
  {
    "text": "when we saw this major\nboost, a jump in performance,",
    "start": "4137970",
    "end": "4143580"
  },
  {
    "text": "with the GPT-3. I do believe that it's\nan emergent capability.",
    "start": "4143580",
    "end": "4150318"
  },
  {
    "text": "But I don't think-- This particular evaluation\nis not very adversarial,",
    "start": "4150319",
    "end": "4156220"
  },
  {
    "text": "by the way. This is like a piece of cake,\nreasonably easy valuation",
    "start": "4156220",
    "end": "4161540"
  },
  {
    "text": "scenario. So the thing about\ncommonsense, though, is that it can be so\nadversarial, so infinitely",
    "start": "4161540",
    "end": "4169415"
  },
  {
    "text": "in many different ways. And then, there are always\npeople like Gary Marcus, who",
    "start": "4169415",
    "end": "4175130"
  },
  {
    "text": "wants to come up with a very-- like weird, weird\ntext scenarios,",
    "start": "4175130",
    "end": "4181429"
  },
  {
    "text": "like how to crush the\nporcelain added to breast milk and support the infant\ndigestive system.",
    "start": "4181430",
    "end": "4186528"
  },
  {
    "text": "And then ChatGPT-3\nsays nonsense. And so the usual\nproblem with commonsense",
    "start": "4186529",
    "end": "4193278"
  },
  {
    "text": "is these adversarial\nsituations where people don't have any problem\ngetting fooled by this,",
    "start": "4193279",
    "end": "4201020"
  },
  {
    "text": "even though you and I say this\nfor the first time, no problem because we have a true\nconceptual understanding.",
    "start": "4201020",
    "end": "4206780"
  },
  {
    "text": "That is the backbone of our\ncommon sense understanding. But that's really\nlacking in the way",
    "start": "4206780",
    "end": "4212480"
  },
  {
    "text": "that transformers\nare designed to focus on predicting which word comes\nnext, as opposed to learning",
    "start": "4212480",
    "end": "4219860"
  },
  {
    "text": "the world knowledge. And in some sense, now with\nthe RLHF, instead of predicting",
    "start": "4219860",
    "end": "4226369"
  },
  {
    "text": "which word comes next. So we're trying to align\nthe model output better with the human preferences.",
    "start": "4226370",
    "end": "4231980"
  },
  {
    "text": "But that, again, is\nnot really aligned with the different goal of,\nlet's make sense of the world",
    "start": "4231980",
    "end": "4238490"
  },
  {
    "text": "and then build a\nknowledge model. So these are all different\nlearning objectives. And really, that\nis why I believe",
    "start": "4238490",
    "end": "4246440"
  },
  {
    "text": "that although commonsense does\nemerge from language models,",
    "start": "4246440",
    "end": "4252050"
  },
  {
    "text": "fundamentally,\nlanguage models are not equivalent to knowledge models. And we really got a focus on\nbuilding knowledge models.",
    "start": "4252050",
    "end": "4259370"
  },
  {
    "text": " Makes sense.",
    "start": "4259370",
    "end": "4264970"
  },
  {
    "text": "Cool. I think there's\none last question. Let's see. ",
    "start": "4264970",
    "end": "4271760"
  },
  {
    "text": "Do you value pluralism? Yeah. It's an empty concept.",
    "start": "4271760",
    "end": "4277490"
  },
  {
    "text": "You don't want to include\nall value systems? Yes. So maybe it is a--",
    "start": "4277490",
    "end": "4282980"
  },
  {
    "text": "Is it empty or not? OK, thank you for\nexcellent question.",
    "start": "4282980",
    "end": "4289320"
  },
  {
    "text": "So I believe that we shouldn't\nendorse conspiracy theories",
    "start": "4289320",
    "end": "4297889"
  },
  {
    "text": "at all or any other\nmorally questionable cases.",
    "start": "4297890",
    "end": "4302930"
  },
  {
    "text": "But then, still, there's\nthis thorny situation",
    "start": "4302930",
    "end": "4308000"
  },
  {
    "text": "of what to do with left-to-left\npeople versus slightly",
    "start": "4308000",
    "end": "4313040"
  },
  {
    "text": "left people versus\nright-leaning people, if US. And then, every country has\nsome other political divide,",
    "start": "4313040",
    "end": "4319670"
  },
  {
    "text": "division as well. So here, I feel like, we\nreally need to sort out",
    "start": "4319670",
    "end": "4325280"
  },
  {
    "text": "what to do with this about-- regardless of these,\nsome of these challenges,",
    "start": "4325280",
    "end": "4331790"
  },
  {
    "text": "it is true that-- I personally don't\nhave a religion, but I respect the\npeople with a religion.",
    "start": "4331790",
    "end": "4339350"
  },
  {
    "text": "And I respect the people with a\ndifferent cultural background. And we have some\nsense of how much",
    "start": "4339350",
    "end": "4346910"
  },
  {
    "text": "do we believe that we\nshould respect each other, even though the\nbeliefs are different.",
    "start": "4346910",
    "end": "4353639"
  },
  {
    "text": "So we probably need\nto work together. And it shouldn't be\njust AI researchers making this decision.",
    "start": "4353640",
    "end": "4359390"
  },
  {
    "text": "By the way, this decision has\nto come from the humanities at large, which is why the data\nsharing actually is important.",
    "start": "4359390",
    "end": "4366480"
  },
  {
    "text": "But basically, I think\nthe current version that I have in mind\nis that the AI does",
    "start": "4366480",
    "end": "4374660"
  },
  {
    "text": "need to understand what sort of\ndifferences are OK differences.",
    "start": "4374660",
    "end": "4380150"
  },
  {
    "text": "The fact that people\ndo have differences in certain questions\nshould be learned by AI",
    "start": "4380150",
    "end": "4387980"
  },
  {
    "text": "so that-- there are distribution\nof opinions as opposed to one correct answer. And then it should deny some\nof the controversy theories,",
    "start": "4387980",
    "end": "4397790"
  },
  {
    "text": "even though I'm sure\nthat some people will be very unhappy about that. But well, we have to\ndecide something like that.",
    "start": "4397790",
    "end": "4405860"
  },
  {
    "text": "I am reasonably optimistic that\nif humanities at large work together, we can do that.",
    "start": "4405860",
    "end": "4411500"
  },
  {
    "text": "Because, after all, laws\nare like that too, laws-- This is a human artifact that\npeople agreed upon somehow",
    "start": "4411500",
    "end": "4421429"
  },
  {
    "text": "that there's this core rules\nthat people should abide by. So I'm hoping that we can\nalso define universals",
    "start": "4421430",
    "end": "4428900"
  },
  {
    "text": "and particulars and\nrespect particulars whenever it's\nrespectful, otherwise",
    "start": "4428900",
    "end": "4435080"
  },
  {
    "text": "have some basic universals\nthat reflect core human values.",
    "start": "4435080",
    "end": "4442580"
  },
  {
    "text": "And then, as far as this\nleft-leaning situation, by the way, if just the goal\nis to make your AI systems",
    "start": "4442580",
    "end": "4448850"
  },
  {
    "text": "safe for anybody, actually,\nwe can make the AI filter extremely equity-aware.",
    "start": "4448850",
    "end": "4457940"
  },
  {
    "text": "And it's not going to\nviolate the freedom of speech by doing so, just to make AI\nto avoid the same things that",
    "start": "4457940",
    "end": "4463910"
  },
  {
    "text": "are potentially microaggression\nfor some population. And we still don't\nreally exclude",
    "start": "4463910",
    "end": "4472310"
  },
  {
    "text": "people who care more\nabout freedom of speech over equity by doing so.",
    "start": "4472310",
    "end": "4477720"
  },
  {
    "text": "So I think there are ways. But this really requires\na lot more research, is how I view it.",
    "start": "4477720",
    "end": "4482780"
  },
  {
    "start": "4482780",
    "end": "4489195"
  },
  {
    "text": "Yeah, I think that's mostly--\nthanks a lot for coming. That was a great talk.",
    "start": "4489195",
    "end": "4495600"
  },
  {
    "text": "OK, thank you very much. Thanks so much. ",
    "start": "4495600",
    "end": "4505000"
  }
]