[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "start": "0",
    "end": "4698"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4698",
    "end": "6240"
  },
  {
    "text": "This is part 5 in our series\non analysis methods in NLP.",
    "start": "6240",
    "end": "8940"
  },
  {
    "text": "We're going to be talking about\nfeature attribution methods.",
    "start": "8940",
    "end": "11440"
  },
  {
    "text": "This is fundamentally\na powerful tool kit",
    "start": "11440",
    "end": "13870"
  },
  {
    "text": "for helping you understand\nhow the features in your model",
    "start": "13870",
    "end": "16600"
  },
  {
    "text": "contribute to its\noutput predictions.",
    "start": "16600",
    "end": "19940"
  },
  {
    "text": "Our fundamental question\nhere is, kind of,",
    "start": "19940",
    "end": "22010"
  },
  {
    "start": "20000",
    "end": "117000"
  },
  {
    "text": "why does your model make the\npredictions that it makes?",
    "start": "22010",
    "end": "24710"
  },
  {
    "text": "There are many motivations\nfor asking this question,",
    "start": "24710",
    "end": "27140"
  },
  {
    "text": "here are just a few.",
    "start": "27140",
    "end": "28378"
  },
  {
    "text": "To start, you might\njust want to understand",
    "start": "28378",
    "end": "30170"
  },
  {
    "text": "whether your model is\nsystematic with regard",
    "start": "30170",
    "end": "32180"
  },
  {
    "text": "to some specific\nlinguistic phenomenon.",
    "start": "32180",
    "end": "34100"
  },
  {
    "text": "Has it actually captured\nthat phenomenon?",
    "start": "34100",
    "end": "37172"
  },
  {
    "text": "You might also want\nto know whether it's",
    "start": "37173",
    "end": "38840"
  },
  {
    "text": "robust to minor\nperturbations in its input.",
    "start": "38840",
    "end": "42260"
  },
  {
    "text": "You might use these techniques\nto diagnose unwanted biases",
    "start": "42260",
    "end": "45289"
  },
  {
    "text": "in your model.",
    "start": "45290",
    "end": "45890"
  },
  {
    "text": "And relatedly, you might\nuse them to find weaknesses",
    "start": "45890",
    "end": "48920"
  },
  {
    "text": "in your model that\nan adversary could",
    "start": "48920",
    "end": "50570"
  },
  {
    "text": "exploit to lead your model to\ndo really problematic things.",
    "start": "50570",
    "end": "55010"
  },
  {
    "text": "Fundamentally, I think that\nthis is a tool kit that",
    "start": "55010",
    "end": "57140"
  },
  {
    "text": "will help you write\nreally excellent analysis",
    "start": "57140",
    "end": "59870"
  },
  {
    "text": "sections for your paper.",
    "start": "59870",
    "end": "61312"
  },
  {
    "text": "To that end, I'm going\nto try to show you",
    "start": "61312",
    "end": "63020"
  },
  {
    "text": "a bunch of code that\nwill help you get hands",
    "start": "63020",
    "end": "65059"
  },
  {
    "text": "on with these techniques.",
    "start": "65060",
    "end": "66860"
  },
  {
    "text": "I'll do it at a kind of high\nlevel in the screen cast.",
    "start": "66860",
    "end": "69200"
  },
  {
    "text": "And I've just contributed\nthis new notebook,",
    "start": "69200",
    "end": "71360"
  },
  {
    "text": "feature attribution, to\nthe course code repository.",
    "start": "71360",
    "end": "74870"
  },
  {
    "text": "And that should be\nflexible and adaptable,",
    "start": "74870",
    "end": "77090"
  },
  {
    "text": "and help you take\nthese techniques",
    "start": "77090",
    "end": "78560"
  },
  {
    "text": "and apply them to\nwhatever models and ideas",
    "start": "78560",
    "end": "81140"
  },
  {
    "text": "you're exploring\nfor your projects.",
    "start": "81140",
    "end": "84240"
  },
  {
    "text": "The star of our show, really the\nonly reason that I can do this,",
    "start": "84240",
    "end": "87345"
  },
  {
    "text": "is this amazing\nCaptum.ai library.",
    "start": "87345",
    "end": "90270"
  },
  {
    "text": "It implements a wide range of\nfeature attribution techniques.",
    "start": "90270",
    "end": "94049"
  },
  {
    "text": "We're going to talk extensively\nabout the integrated gradients",
    "start": "94050",
    "end": "97140"
  },
  {
    "text": "method and use the\ngradient-based method",
    "start": "97140",
    "end": "99659"
  },
  {
    "text": "as a kind of simple\nbaseline for that method.",
    "start": "99660",
    "end": "102360"
  },
  {
    "text": "But as you can see here,\nCaptum implements a wide range",
    "start": "102360",
    "end": "105210"
  },
  {
    "text": "of different\nalgorithms, some very",
    "start": "105210",
    "end": "107159"
  },
  {
    "text": "particular to specific model\ndesigns and others completely",
    "start": "107160",
    "end": "110820"
  },
  {
    "text": "agnostic about what kind\nof model you're exploring.",
    "start": "110820",
    "end": "113070"
  },
  {
    "text": "So it's a very\nexciting tool kit.",
    "start": "113070",
    "end": "114810"
  },
  {
    "start": "114810",
    "end": "117320"
  },
  {
    "start": "117000",
    "end": "207000"
  },
  {
    "text": "The Sundararajan\net al 2017 paper",
    "start": "117320",
    "end": "120080"
  },
  {
    "text": "introduced the integrated\ngradients method.",
    "start": "120080",
    "end": "122210"
  },
  {
    "text": "It's also a lovely contribution\nbecause it gives us",
    "start": "122210",
    "end": "124909"
  },
  {
    "text": "a kind of framework for thinking\nabout feature attribution",
    "start": "124910",
    "end": "129039"
  },
  {
    "text": "methods in general.",
    "start": "129039",
    "end": "130399"
  },
  {
    "text": "And as part of that,\nthey offer two axioms",
    "start": "130400",
    "end": "132760"
  },
  {
    "text": "that I'm going to use to\nguide this discussion.",
    "start": "132760",
    "end": "135110"
  },
  {
    "text": "The first, and the more\nimportant one, is sensitivity.",
    "start": "135110",
    "end": "138280"
  },
  {
    "text": "If two inputs x and x prime\ndiffer only at dimension i",
    "start": "138280",
    "end": "142330"
  },
  {
    "text": "and lead to different\npredictions,",
    "start": "142330",
    "end": "144610"
  },
  {
    "text": "then the feature associated\nwith that dimension",
    "start": "144610",
    "end": "146980"
  },
  {
    "text": "must have non-zero attribution.",
    "start": "146980",
    "end": "149769"
  },
  {
    "text": "And with my simple\nexample here, you",
    "start": "149770",
    "end": "151270"
  },
  {
    "text": "can get a sense\nfor why sensitivity",
    "start": "151270",
    "end": "153250"
  },
  {
    "text": "is such a fundamental axiom.",
    "start": "153250",
    "end": "155170"
  },
  {
    "text": "If for some model m and three\ndimensional input 1, 0, 1,",
    "start": "155170",
    "end": "159670"
  },
  {
    "text": "we get a prediction of positive.",
    "start": "159670",
    "end": "161810"
  },
  {
    "text": "And if for that same\nmodel the input 1, 1, 1,",
    "start": "161810",
    "end": "164530"
  },
  {
    "text": "leads to the\nprediction negative,",
    "start": "164530",
    "end": "166480"
  },
  {
    "text": "then we really ought to expect\nthat the feature associated",
    "start": "166480",
    "end": "169510"
  },
  {
    "text": "with the second position must\nhave non-zero attribution.",
    "start": "169510",
    "end": "173019"
  },
  {
    "text": "Because it must be decisive\nin leading the model",
    "start": "173020",
    "end": "176200"
  },
  {
    "text": "to make these two\ndifferent predictions.",
    "start": "176200",
    "end": "179650"
  },
  {
    "text": "The second axiom\nis going to be less",
    "start": "179650",
    "end": "181150"
  },
  {
    "text": "important to our discussion,\nbut it's nonetheless worth",
    "start": "181150",
    "end": "183442"
  },
  {
    "text": "having in mind.",
    "start": "183442",
    "end": "184370"
  },
  {
    "text": "It is implementation invariance.",
    "start": "184370",
    "end": "186340"
  },
  {
    "text": "If two models m and m prime\nhave identical input/output",
    "start": "186340",
    "end": "189730"
  },
  {
    "text": "behavior, then the\nattributions for m and m prime",
    "start": "189730",
    "end": "192730"
  },
  {
    "text": "are identical.",
    "start": "192730",
    "end": "193879"
  },
  {
    "text": "This is really just saying\nthat the attributions we give",
    "start": "193880",
    "end": "196660"
  },
  {
    "text": "should be separate from any\nincidental differences in model",
    "start": "196660",
    "end": "200530"
  },
  {
    "text": "implementation that don't\naffect the input/output behavior",
    "start": "200530",
    "end": "203980"
  },
  {
    "text": "of that model.",
    "start": "203980",
    "end": "206750"
  },
  {
    "text": "To start our\ndiscussion let's begin",
    "start": "206750",
    "end": "208640"
  },
  {
    "start": "207000",
    "end": "294000"
  },
  {
    "text": "with this simple baseline,\nwhich is simply multiplying",
    "start": "208640",
    "end": "211460"
  },
  {
    "text": "the gradients by the inputs.",
    "start": "211460",
    "end": "213480"
  },
  {
    "text": "This is implemented\nin Captum as input",
    "start": "213480",
    "end": "215330"
  },
  {
    "text": "by gradient that I'm\nshowing with respect",
    "start": "215330",
    "end": "218090"
  },
  {
    "text": "to some particular feature\ni, given model M and input x.",
    "start": "218090",
    "end": "222379"
  },
  {
    "text": "And we simply get the\ngradients for that feature",
    "start": "222380",
    "end": "225560"
  },
  {
    "text": "and then multiply it by the\nactual value of that feature.",
    "start": "225560",
    "end": "229050"
  },
  {
    "text": "It's as simple as that.",
    "start": "229050",
    "end": "230150"
  },
  {
    "text": "Here are two implementations.",
    "start": "230150",
    "end": "231620"
  },
  {
    "text": "The first one in\ncell 2 does this kind",
    "start": "231620",
    "end": "234409"
  },
  {
    "text": "of using raw PyTorch\njust to show you",
    "start": "234410",
    "end": "236600"
  },
  {
    "text": "how we can use PyTorch's\nautograd functionality",
    "start": "236600",
    "end": "240110"
  },
  {
    "text": "to implement this method.",
    "start": "240110",
    "end": "241880"
  },
  {
    "text": "And the second implementation\nis from Captum.",
    "start": "241880",
    "end": "244190"
  },
  {
    "text": "And it's probably more flexible.",
    "start": "244190",
    "end": "245690"
  },
  {
    "text": "And it uses this input\nby gradient class.",
    "start": "245690",
    "end": "250530"
  },
  {
    "text": "To give you a full\nillustration here,",
    "start": "250530",
    "end": "252180"
  },
  {
    "text": "I've just set up a simple\nsynthetic classification",
    "start": "252180",
    "end": "254790"
  },
  {
    "text": "problem using scikit tools.",
    "start": "254790",
    "end": "256949"
  },
  {
    "text": "My model will be a\nTorchShallowNeuralClassifier",
    "start": "256950",
    "end": "259799"
  },
  {
    "text": "which I fit on that data.",
    "start": "259800",
    "end": "261838"
  },
  {
    "text": "And then in cells 9 and 10, I\nuse those two implementations",
    "start": "261839",
    "end": "265169"
  },
  {
    "text": "of this method.",
    "start": "265170",
    "end": "266470"
  },
  {
    "text": "And you can see in\n11 and 12, that they",
    "start": "266470",
    "end": "268170"
  },
  {
    "text": "give identical outputs.",
    "start": "268170",
    "end": "270420"
  },
  {
    "text": "Another thing worth\nnoting here is",
    "start": "270420",
    "end": "271860"
  },
  {
    "text": "that I have used the\nmethod by taking gradients",
    "start": "271860",
    "end": "273990"
  },
  {
    "text": "with respect to the actual\nlabels in our data set.",
    "start": "273990",
    "end": "277199"
  },
  {
    "text": "You can often get\na different picture",
    "start": "277200",
    "end": "279150"
  },
  {
    "text": "if you take gradients\nwith respect",
    "start": "279150",
    "end": "280620"
  },
  {
    "text": "to the predictions\nof that model.",
    "start": "280620",
    "end": "282637"
  },
  {
    "text": "And that might give\nyou a better sense",
    "start": "282637",
    "end": "284220"
  },
  {
    "text": "for why the model is making\nthe predictions that it makes.",
    "start": "284220",
    "end": "288390"
  },
  {
    "text": "In this case, since\nthe model is very good,",
    "start": "288390",
    "end": "290190"
  },
  {
    "text": "the attributions are\nonly slightly different.",
    "start": "290190",
    "end": "293990"
  },
  {
    "text": "That's our kind of baseline.",
    "start": "293990",
    "end": "295729"
  },
  {
    "start": "294000",
    "end": "350000"
  },
  {
    "text": "I want to show you now that\nthe input by gradients method",
    "start": "295730",
    "end": "298490"
  },
  {
    "text": "fails the sensitivity test.",
    "start": "298490",
    "end": "300300"
  },
  {
    "text": "And this is an example from\nthe Sundararajan et al paper.",
    "start": "300300",
    "end": "304159"
  },
  {
    "text": "They give this\nsimple model M here,",
    "start": "304160",
    "end": "306090"
  },
  {
    "text": "which is effectively just ReLU\napplied to 1 minus the input.",
    "start": "306090",
    "end": "311330"
  },
  {
    "text": "And then you take 1 minus\nthat ReLU calculation there.",
    "start": "311330",
    "end": "314745"
  },
  {
    "text": "And that's the model.",
    "start": "314745",
    "end": "315620"
  },
  {
    "text": "It's got one dimensional\ninputs and outputs.",
    "start": "315620",
    "end": "318680"
  },
  {
    "text": "If you calculate for input\n0, you get an outcome of 0.",
    "start": "318680",
    "end": "322850"
  },
  {
    "text": "And if you give the model input\n2, you get an output of 1.",
    "start": "322850",
    "end": "326990"
  },
  {
    "text": "Since we have differing\noutput predictions,",
    "start": "326990",
    "end": "329599"
  },
  {
    "text": "sensitivity tells\nus that we have",
    "start": "329600",
    "end": "331550"
  },
  {
    "text": "to have differing\nattributions for these two",
    "start": "331550",
    "end": "335090"
  },
  {
    "text": "cases here, these two\none dimensional inputs.",
    "start": "335090",
    "end": "338449"
  },
  {
    "text": "But unfortunately, when\nyou calculate through",
    "start": "338450",
    "end": "340580"
  },
  {
    "text": "with this method you get 0\nattribution in both cases.",
    "start": "340580",
    "end": "345139"
  },
  {
    "text": "That's a failure of\nsensitivity and points",
    "start": "345140",
    "end": "347270"
  },
  {
    "text": "to a weakness of this method.",
    "start": "347270",
    "end": "351259"
  },
  {
    "start": "350000",
    "end": "390000"
  },
  {
    "text": "Let's move now to\nintegrated gradients.",
    "start": "351260",
    "end": "353030"
  },
  {
    "text": "And let me start by giving\nyou the intuition for how",
    "start": "353030",
    "end": "355430"
  },
  {
    "text": "this method is going to work.",
    "start": "355430",
    "end": "356900"
  },
  {
    "text": "Imagine we have a simple two\ndimensional feature space,",
    "start": "356900",
    "end": "359540"
  },
  {
    "text": "feature x1 and x2.",
    "start": "359540",
    "end": "361580"
  },
  {
    "text": "So here's the actual\npoint represented here.",
    "start": "361580",
    "end": "365159"
  },
  {
    "text": "The idea behind\nintegrated gradients",
    "start": "365160",
    "end": "367130"
  },
  {
    "text": "is that we're going to\ncompare that with respect",
    "start": "367130",
    "end": "369260"
  },
  {
    "text": "to some baseline.",
    "start": "369260",
    "end": "370610"
  },
  {
    "text": "That typical baseline for us\nwill be the all 0s vector.",
    "start": "370610",
    "end": "374232"
  },
  {
    "text": "And then to do the comparison\nwhat we'll actually do",
    "start": "374233",
    "end": "376400"
  },
  {
    "text": "is interpolate a bunch of\npoints between that baseline",
    "start": "376400",
    "end": "379850"
  },
  {
    "text": "and our actual input, take\ngradients with respect",
    "start": "379850",
    "end": "383060"
  },
  {
    "text": "to each one of them, and average\nall of those gradient results.",
    "start": "383060",
    "end": "386630"
  },
  {
    "text": "And that will give us some\nmeasure of future importance.",
    "start": "386630",
    "end": "390720"
  },
  {
    "start": "390000",
    "end": "434000"
  },
  {
    "text": "Here's the calculation of\nthe method in full detail.",
    "start": "390720",
    "end": "393290"
  },
  {
    "text": "I've taken this presentation\nfrom this really excellent",
    "start": "393290",
    "end": "396170"
  },
  {
    "text": "tutorial from TensorFlow\nintegrated gradients.",
    "start": "396170",
    "end": "399980"
  },
  {
    "text": "It does all these annotations\nthat I find quite helpful.",
    "start": "399980",
    "end": "402950"
  },
  {
    "text": "Here's fundamentally\nhow this works.",
    "start": "402950",
    "end": "404990"
  },
  {
    "text": "The core thing is in purple.",
    "start": "404990",
    "end": "406759"
  },
  {
    "text": "We're going to\ninterpolate a bunch",
    "start": "406760",
    "end": "408410"
  },
  {
    "text": "of different inputs\nbetween that baseline",
    "start": "408410",
    "end": "410540"
  },
  {
    "text": "of all 0s and our actual input.",
    "start": "410540",
    "end": "413120"
  },
  {
    "text": "That's what's happening here.",
    "start": "413120",
    "end": "414650"
  },
  {
    "text": "And we'll take the\ngradients with respect",
    "start": "414650",
    "end": "416630"
  },
  {
    "text": "to each one of\nthose with respect",
    "start": "416630",
    "end": "418190"
  },
  {
    "text": "to each one of the features.",
    "start": "418190",
    "end": "420210"
  },
  {
    "text": "And we're going to sum\nthose up and average them.",
    "start": "420210",
    "end": "422600"
  },
  {
    "text": "And that gives us the\ncore calculation here.",
    "start": "422600",
    "end": "425450"
  },
  {
    "text": "And then in 5, we just kind of\nscale that resulting average",
    "start": "425450",
    "end": "429230"
  },
  {
    "text": "with respect to the\noriginal input to put it",
    "start": "429230",
    "end": "431630"
  },
  {
    "text": "back on the same scale.",
    "start": "431630",
    "end": "433500"
  },
  {
    "text": "And as I showed here,\nintegrated gradients",
    "start": "433500",
    "end": "435800"
  },
  {
    "start": "434000",
    "end": "485000"
  },
  {
    "text": "obeys the sensitivity axiom.",
    "start": "435800",
    "end": "437750"
  },
  {
    "text": "Let's go back to\nthat original example",
    "start": "437750",
    "end": "439610"
  },
  {
    "text": "of that simple ReLU-based\nmodel presented here.",
    "start": "439610",
    "end": "443150"
  },
  {
    "text": "I showed you that the\ninput by gradients method",
    "start": "443150",
    "end": "445430"
  },
  {
    "text": "failed sensitivity\nfor this model.",
    "start": "445430",
    "end": "448009"
  },
  {
    "text": "Integrated gradients\nof course is",
    "start": "448010",
    "end": "449750"
  },
  {
    "text": "sensitive in the relevant sense.",
    "start": "449750",
    "end": "451550"
  },
  {
    "text": "And you can kind\nof see why that's",
    "start": "451550",
    "end": "452990"
  },
  {
    "text": "happening, because our\ncore calculation now",
    "start": "452990",
    "end": "455539"
  },
  {
    "text": "is not with respect\nto a single input,",
    "start": "455540",
    "end": "457610"
  },
  {
    "text": "in the case of the input\n2, but rather with respect",
    "start": "457610",
    "end": "460219"
  },
  {
    "text": "to all of those interpolated\nfeature representations.",
    "start": "460220",
    "end": "463460"
  },
  {
    "text": "Although some of those\ninterpolated feature",
    "start": "463460",
    "end": "465440"
  },
  {
    "text": "representations give a gradient\nof 0 not all of them do.",
    "start": "465440",
    "end": "469430"
  },
  {
    "text": "And the result in\neffect is that you'll",
    "start": "469430",
    "end": "471199"
  },
  {
    "text": "get a feature attribution\nof approximately 1",
    "start": "471200",
    "end": "474230"
  },
  {
    "text": "for this case of an input 2.",
    "start": "474230",
    "end": "476720"
  },
  {
    "text": "The desired result\nshowing sensitivity,",
    "start": "476720",
    "end": "479210"
  },
  {
    "text": "because of course the\ninput of 0 in this case",
    "start": "479210",
    "end": "481759"
  },
  {
    "text": "would give an attribution of 0.",
    "start": "481760",
    "end": "485457"
  },
  {
    "start": "485000",
    "end": "649000"
  },
  {
    "text": "Now let me walk you through\na few examples that show you",
    "start": "485457",
    "end": "487790"
  },
  {
    "text": "how you can use Captum to get\nhands on with the integrated",
    "start": "487790",
    "end": "491690"
  },
  {
    "text": "gradients method.",
    "start": "491690",
    "end": "492590"
  },
  {
    "text": "And I'm going to do that\nfor two classes of model.",
    "start": "492590",
    "end": "495100"
  },
  {
    "text": "The first one is just a\nsimple feed-forward network.",
    "start": "495100",
    "end": "497990"
  },
  {
    "text": "And what I'm doing is\nreconnecting with the Stanford",
    "start": "497990",
    "end": "500630"
  },
  {
    "text": "Sentiment Treebank that we\nused during our sentiment unit.",
    "start": "500630",
    "end": "503660"
  },
  {
    "text": "So on this slide, I've just\nset up an SST experiment",
    "start": "503660",
    "end": "507380"
  },
  {
    "text": "using sst.experiment\nfrom that SST module.",
    "start": "507380",
    "end": "512789"
  },
  {
    "text": "My feature\nrepresentations are going",
    "start": "512789",
    "end": "514375"
  },
  {
    "text": "to be essentially\na bag of words.",
    "start": "514375",
    "end": "515750"
  },
  {
    "text": "And I've filtered off stop\nwords to make this a little more",
    "start": "515750",
    "end": "518208"
  },
  {
    "text": "interpretable.",
    "start": "518208",
    "end": "519380"
  },
  {
    "text": "And our classifier is a\nTorchShallowNeuralClassifier.",
    "start": "519380",
    "end": "523130"
  },
  {
    "text": "I run the experiment\nand a lot of information",
    "start": "523130",
    "end": "525470"
  },
  {
    "text": "about that experiment,\nyou'll recall,",
    "start": "525470",
    "end": "527180"
  },
  {
    "text": "is stored in this\nvariable, experiment.",
    "start": "527180",
    "end": "531450"
  },
  {
    "text": "Here I extract the model\nfrom that experiment report.",
    "start": "531450",
    "end": "535800"
  },
  {
    "text": "And here, we get a\nbunch of other metadata",
    "start": "535800",
    "end": "537660"
  },
  {
    "text": "that we're going to use to run\nthe IntegratedGradients method.",
    "start": "537660",
    "end": "540629"
  },
  {
    "text": "The feature representations\nof our test examples,",
    "start": "540630",
    "end": "543660"
  },
  {
    "text": "the actual labels, and\nthe predictive labels,",
    "start": "543660",
    "end": "546870"
  },
  {
    "text": "along with the feature names.",
    "start": "546870",
    "end": "548339"
  },
  {
    "text": "And the one thing to note here\nis that for the sake of Captum,",
    "start": "548340",
    "end": "551160"
  },
  {
    "text": "we need to turn the\nstring class names",
    "start": "551160",
    "end": "553889"
  },
  {
    "text": "into their\ncorresponding indices.",
    "start": "553890",
    "end": "555770"
  },
  {
    "text": "And that's what's\nhappening in cell 9 here.",
    "start": "555770",
    "end": "558210"
  },
  {
    "text": "Then we set up the\nintegrated gradients",
    "start": "558210",
    "end": "560130"
  },
  {
    "text": "using the forward\nmethod for our model.",
    "start": "560130",
    "end": "563040"
  },
  {
    "text": "And we set up the baseline,\nwhich is that all 0s vector.",
    "start": "563040",
    "end": "566399"
  },
  {
    "text": "And then finally use\nthe attribute method.",
    "start": "566400",
    "end": "568810"
  },
  {
    "text": "And here I'm taking\nattributions with respect",
    "start": "568810",
    "end": "571410"
  },
  {
    "text": "to the predictions of the model.",
    "start": "571410",
    "end": "574569"
  },
  {
    "text": "I think this can be a\npowerful device for doing",
    "start": "574570",
    "end": "576580"
  },
  {
    "text": "some simple error analysis.",
    "start": "576580",
    "end": "578020"
  },
  {
    "text": "And that's what I've set\nup on this slide here.",
    "start": "578020",
    "end": "580300"
  },
  {
    "text": "I've offered two\nfunctions, error analysis",
    "start": "580300",
    "end": "582610"
  },
  {
    "text": "and create attribution\nlookup that",
    "start": "582610",
    "end": "584589"
  },
  {
    "text": "will help you understand\nhow features in this model",
    "start": "584590",
    "end": "587230"
  },
  {
    "text": "are relating to its\noutput predictions.",
    "start": "587230",
    "end": "589579"
  },
  {
    "text": "You can see in cell\n14 here, I'm looking",
    "start": "589580",
    "end": "591520"
  },
  {
    "text": "for cases where the\nactual label is neutral",
    "start": "591520",
    "end": "593680"
  },
  {
    "text": "and the model\npredicted positive.",
    "start": "593680",
    "end": "595540"
  },
  {
    "text": "We can find those attributions.",
    "start": "595540",
    "end": "596980"
  },
  {
    "text": "And this is actually an\ninformative picture here.",
    "start": "596980",
    "end": "599410"
  },
  {
    "text": "It looks like the model\nhas overfit to features",
    "start": "599410",
    "end": "603009"
  },
  {
    "text": "like the period and the comma.",
    "start": "603010",
    "end": "604750"
  },
  {
    "text": "This ought to be indicative\nof the neutral category.",
    "start": "604750",
    "end": "607420"
  },
  {
    "text": "But here it's using\nthem in ways that",
    "start": "607420",
    "end": "609310"
  },
  {
    "text": "lead to a positive prediction.",
    "start": "609310",
    "end": "611029"
  },
  {
    "text": "So that's something that\nwe might want to address.",
    "start": "611030",
    "end": "613600"
  },
  {
    "text": "And we can go one level\nfurther if we choose and look",
    "start": "613600",
    "end": "615880"
  },
  {
    "text": "at individual examples.",
    "start": "615880",
    "end": "617660"
  },
  {
    "text": "So here I have pulled out\nan individual example.",
    "start": "617660",
    "end": "620050"
  },
  {
    "text": "\"No one goes\nunindicted here, which",
    "start": "620050",
    "end": "621910"
  },
  {
    "text": "is probably for the best.\"",
    "start": "621910",
    "end": "623470"
  },
  {
    "text": "This is a case where the\ncorrect label is neutral.",
    "start": "623470",
    "end": "626379"
  },
  {
    "text": "And our model\npredicted positive.",
    "start": "626380",
    "end": "628640"
  },
  {
    "text": "And I think the\nattributions again",
    "start": "628640",
    "end": "630070"
  },
  {
    "text": "help us understand\nwhy because by far",
    "start": "630070",
    "end": "632590"
  },
  {
    "text": "the feature with the highest\nattribution is this \"best\" one.",
    "start": "632590",
    "end": "636350"
  },
  {
    "text": "And this is revealing that\nthe model just does not",
    "start": "636350",
    "end": "638649"
  },
  {
    "text": "understand the context in\nwhich the word \"best\" is",
    "start": "638650",
    "end": "641770"
  },
  {
    "text": "used in this example.",
    "start": "641770",
    "end": "643510"
  },
  {
    "text": "That might point to a\nfundamental weakness",
    "start": "643510",
    "end": "645790"
  },
  {
    "text": "of the bag of words approach.",
    "start": "645790",
    "end": "647125"
  },
  {
    "start": "647125",
    "end": "649690"
  },
  {
    "start": "649000",
    "end": "837000"
  },
  {
    "text": "For my second\nexample let's connect",
    "start": "649690",
    "end": "651340"
  },
  {
    "text": "with transformer models, since\nI assume that a lot of you",
    "start": "651340",
    "end": "653763"
  },
  {
    "text": "will be working\nwith these models.",
    "start": "653763",
    "end": "655180"
  },
  {
    "text": "And these present\nexciting new opportunities",
    "start": "655180",
    "end": "657460"
  },
  {
    "text": "for feature attribution.",
    "start": "657460",
    "end": "658870"
  },
  {
    "text": "Because in these models, we\nhave so many representations",
    "start": "658870",
    "end": "662440"
  },
  {
    "text": "that we could think about\ndoing attributions for.",
    "start": "662440",
    "end": "665320"
  },
  {
    "text": "Here's a kind of general picture\nof a BERT-like model, where",
    "start": "665320",
    "end": "668560"
  },
  {
    "text": "I have the outputs up here.",
    "start": "668560",
    "end": "670300"
  },
  {
    "text": "You have many layers of\ntransformer block outputs.",
    "start": "670300",
    "end": "673360"
  },
  {
    "text": "Those are given in purple and\nprobably an embedding layer",
    "start": "673360",
    "end": "676390"
  },
  {
    "text": "in green.",
    "start": "676390",
    "end": "677290"
  },
  {
    "text": "And that embedding layer\nmight be itself composed",
    "start": "677290",
    "end": "680410"
  },
  {
    "text": "of like a word-embedding layer\nand a positional embedding",
    "start": "680410",
    "end": "683680"
  },
  {
    "text": "layer, and maybe others.",
    "start": "683680",
    "end": "685490"
  },
  {
    "text": "All of these layers\nare potential targets",
    "start": "685490",
    "end": "688209"
  },
  {
    "text": "for integrated gradients.",
    "start": "688210",
    "end": "689920"
  },
  {
    "text": "And Captum again makes\nthat relatively easy.",
    "start": "689920",
    "end": "693310"
  },
  {
    "text": "So to start this off, I just\ndownloaded from HuggingFace",
    "start": "693310",
    "end": "695800"
  },
  {
    "text": "a RoBERTa-based\nTwitter sentiment model",
    "start": "695800",
    "end": "699250"
  },
  {
    "text": "that seemed really interesting.",
    "start": "699250",
    "end": "700840"
  },
  {
    "text": "And I wrote a\npredict_one_proba method",
    "start": "700840",
    "end": "702910"
  },
  {
    "text": "that will help us with the error\nanalysis that we want to do.",
    "start": "702910",
    "end": "707730"
  },
  {
    "text": "This next step here\ndoes the encodings",
    "start": "707730",
    "end": "709709"
  },
  {
    "text": "of both the actual example,\nusing the model's tokenizer",
    "start": "709710",
    "end": "714150"
  },
  {
    "text": "as well as the\nbaseline of all 0s",
    "start": "714150",
    "end": "716100"
  },
  {
    "text": "that we'll use for comparisons.",
    "start": "716100",
    "end": "718290"
  },
  {
    "text": "In cell 7, I've just designed\na small custom forward method",
    "start": "718290",
    "end": "722269"
  },
  {
    "text": "to help Captum out,\nbecause this model",
    "start": "722270",
    "end": "724380"
  },
  {
    "text": "has slightly different output\nstructure than is expected.",
    "start": "724380",
    "end": "729130"
  },
  {
    "text": "Here in cell 8, we set up the\nlayer that we want to target.",
    "start": "729130",
    "end": "732180"
  },
  {
    "text": "And as you can see I'm\ntargeting the embedding layer.",
    "start": "732180",
    "end": "734560"
  },
  {
    "text": "But many other layers\ncould be targeted.",
    "start": "734560",
    "end": "737110"
  },
  {
    "text": "Captum makes that easy.",
    "start": "737110",
    "end": "738709"
  },
  {
    "text": "For our example, we use,\n\"This is illuminating!\"",
    "start": "738710",
    "end": "741430"
  },
  {
    "text": "which I'll take to have\ntrue class positive.",
    "start": "741430",
    "end": "744740"
  },
  {
    "text": "We do our encodings in cell\n11 of both the actual example",
    "start": "744740",
    "end": "747820"
  },
  {
    "text": "and the baseline.",
    "start": "747820",
    "end": "748720"
  },
  {
    "text": "And then that's the\nbasis for our attribution",
    "start": "748720",
    "end": "750699"
  },
  {
    "text": "of this single example.",
    "start": "750700",
    "end": "752990"
  },
  {
    "text": "Now for BERT, because\nwe have high dimensional",
    "start": "752990",
    "end": "756680"
  },
  {
    "text": "representations for\neach one of the tokens",
    "start": "756680",
    "end": "758839"
  },
  {
    "text": "that we're looking\nat, we need to perform",
    "start": "758840",
    "end": "760970"
  },
  {
    "text": "another layer of\ncompression that we",
    "start": "760970",
    "end": "762589"
  },
  {
    "text": "didn't have to for the\nfeed-forward example.",
    "start": "762590",
    "end": "765120"
  },
  {
    "text": "As you can see here,\nthe attributions",
    "start": "765120",
    "end": "767150"
  },
  {
    "text": "have for one example\ndimensionality 6 by 768.",
    "start": "767150",
    "end": "771420"
  },
  {
    "text": "This is one vector\nper word token.",
    "start": "771420",
    "end": "774110"
  },
  {
    "text": "To summarize those at the level\nof individual word tokens,",
    "start": "774110",
    "end": "777320"
  },
  {
    "text": "we'll just sum them\nup and then z-score",
    "start": "777320",
    "end": "779570"
  },
  {
    "text": "normalize them to kind of put\nthem on a consistent scale.",
    "start": "779570",
    "end": "782570"
  },
  {
    "text": "So that will reduce\nthe attributions down",
    "start": "782570",
    "end": "785060"
  },
  {
    "text": "to one per sub-word token.",
    "start": "785060",
    "end": "788660"
  },
  {
    "text": "And that feeds into our final\nkind of cumulative analysis.",
    "start": "788660",
    "end": "792060"
  },
  {
    "text": "So we'll do the\nprobabilistic predictions,",
    "start": "792060",
    "end": "794450"
  },
  {
    "text": "look at the actual class,\nconvert the input to something",
    "start": "794450",
    "end": "798200"
  },
  {
    "text": "that Captum can digest, and\nthen use this visualization data",
    "start": "798200",
    "end": "801530"
  },
  {
    "text": "recorder method to\nbring this all together",
    "start": "801530",
    "end": "804230"
  },
  {
    "text": "into a nice tabular\nvisualization.",
    "start": "804230",
    "end": "806584"
  },
  {
    "text": "And that's what's\nhappening here.",
    "start": "806585",
    "end": "807960"
  },
  {
    "text": "You can see, for example,\nwe have the True Label,",
    "start": "807960",
    "end": "810620"
  },
  {
    "text": "the Predicted Label with\nthe associated probability.",
    "start": "810620",
    "end": "813650"
  },
  {
    "text": "And then the really interesting\npart, per word token,",
    "start": "813650",
    "end": "817140"
  },
  {
    "text": "we have a summary\nof its attributions.",
    "start": "817140",
    "end": "819080"
  },
  {
    "text": "And you can see that\ngreen is associated",
    "start": "819080",
    "end": "820910"
  },
  {
    "text": "with positive, white with\nneutral, and red with negative.",
    "start": "820910",
    "end": "824509"
  },
  {
    "text": "And this is giving us\na reassuring picture",
    "start": "824510",
    "end": "826610"
  },
  {
    "text": "about the systematicity\nof these predictions.",
    "start": "826610",
    "end": "829100"
  },
  {
    "text": "It's a positive prediction.",
    "start": "829100",
    "end": "830779"
  },
  {
    "text": "And most of that is the result\nof the word \"illuminating\"",
    "start": "830780",
    "end": "833450"
  },
  {
    "text": "and the exclamation mark.",
    "start": "833450",
    "end": "835940"
  },
  {
    "text": "And that kind of\nfeeds into a nice kind",
    "start": "835940",
    "end": "837680"
  },
  {
    "start": "837000",
    "end": "932000"
  },
  {
    "text": "of error\nanalysis/challenge analysis",
    "start": "837680",
    "end": "841250"
  },
  {
    "text": "that you can do with models\nlike this using Captum.",
    "start": "841250",
    "end": "844190"
  },
  {
    "text": "For this slide here, I've\nposed a little challenge",
    "start": "844190",
    "end": "846470"
  },
  {
    "text": "or adversarial test to see how\ndeeply my model understands",
    "start": "846470",
    "end": "850759"
  },
  {
    "text": "sentences like, \"They\nsaid it would be great",
    "start": "850760",
    "end": "852920"
  },
  {
    "text": "and they were right.\"",
    "start": "852920",
    "end": "854385"
  },
  {
    "text": "You can see it makes the\ncorrect prediction in that case.",
    "start": "854385",
    "end": "856760"
  },
  {
    "text": "And when I change it to,\n\"They said it would be great",
    "start": "856760",
    "end": "859070"
  },
  {
    "text": "and they were wrong.\",\nit predicts negative.",
    "start": "859070",
    "end": "861350"
  },
  {
    "text": "That's reassuring, and so\nare the future attributions.",
    "start": "861350",
    "end": "863990"
  },
  {
    "text": "It seems to be keying\ninto exactly the pieces",
    "start": "863990",
    "end": "866930"
  },
  {
    "text": "of information that I would\nhope and even doing it",
    "start": "866930",
    "end": "869510"
  },
  {
    "text": "in a context sensitive way.",
    "start": "869510",
    "end": "871950"
  },
  {
    "text": "For the next two\nexamples, I just",
    "start": "871950",
    "end": "873390"
  },
  {
    "text": "change up the syntax\nto see whether it's",
    "start": "873390",
    "end": "875340"
  },
  {
    "text": "kind of overfit to the position\nof these words in the string.",
    "start": "875340",
    "end": "878640"
  },
  {
    "text": "And it again looks robust.",
    "start": "878640",
    "end": "880110"
  },
  {
    "text": "\"They were right to say\nthat it would be great.\"",
    "start": "880110",
    "end": "882269"
  },
  {
    "text": "Prediction of positive.",
    "start": "882270",
    "end": "883470"
  },
  {
    "text": "\"They were wrong to say\nthat it would be great.\"",
    "start": "883470",
    "end": "885470"
  },
  {
    "text": "Prediction of negative.",
    "start": "885470",
    "end": "886620"
  },
  {
    "text": "Very reassuring.",
    "start": "886620",
    "end": "888060"
  },
  {
    "text": "As is the second\nto last example,",
    "start": "888060",
    "end": "890070"
  },
  {
    "text": "\"They said it would be stellar\nand they were correct.\"",
    "start": "890070",
    "end": "892650"
  },
  {
    "text": "The only disappointing thing\nin this challenge problem",
    "start": "892650",
    "end": "895530"
  },
  {
    "text": "is for this final example\nit predicts neutral",
    "start": "895530",
    "end": "898230"
  },
  {
    "text": "for, \"They said it\nwould be stellar",
    "start": "898230",
    "end": "900120"
  },
  {
    "text": "and they were incorrect.\"",
    "start": "900120",
    "end": "901410"
  },
  {
    "text": "And the attributions are\nalso a little bit worrisome",
    "start": "901410",
    "end": "903959"
  },
  {
    "text": "about the extent to\nwhich the model has truly",
    "start": "903960",
    "end": "906360"
  },
  {
    "text": "understood this example.",
    "start": "906360",
    "end": "909042"
  },
  {
    "text": "Maybe we can think about\nhow to address that problem.",
    "start": "909042",
    "end": "911250"
  },
  {
    "text": "But the fundamental\ntakeaway for now",
    "start": "911250",
    "end": "913530"
  },
  {
    "text": "is simply that you can see how\nyou can use feature attribution",
    "start": "913530",
    "end": "917070"
  },
  {
    "text": "together with challenge\nexamples to kind of home",
    "start": "917070",
    "end": "920130"
  },
  {
    "text": "in on exactly how systematic\na model's predictions are",
    "start": "920130",
    "end": "924480"
  },
  {
    "text": "for an interesting\nclass of cases.",
    "start": "924480",
    "end": "927380"
  },
  {
    "start": "927380",
    "end": "932000"
  }
]