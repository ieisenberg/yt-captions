[
  {
    "start": "0",
    "end": "10800"
  },
  {
    "text": "Yeah, so I've attended\nmany of these seminars. And I think it's a\ngreat seminar series, and I'm happy to be able\nto present here today.",
    "start": "10800",
    "end": "18540"
  },
  {
    "text": "So Mark gave already a really\ngreat summary of the research that we do in my lab. My lab is the\nMulti-robot Systems Lab.",
    "start": "18540",
    "end": "26070"
  },
  {
    "text": "And lately, actually\nmost of what I'm talking about in my talk\ntoday is not multi-robot. In fact, I'm trying to think\nif there is any multi-robot.",
    "start": "26070",
    "end": "33210"
  },
  {
    "text": "I think there isn't. Lately, I've been\nreally fascinated with NeRFs, Neural Radiance\nFields, which you'll",
    "start": "33210",
    "end": "41070"
  },
  {
    "text": "hear all about in this talk. And not only NeRFs, but\nassociated rich deep neural network-based data-driven\n3D environment representations",
    "start": "41070",
    "end": "49938"
  },
  {
    "text": "and what they have\nto offer robotics, and how do we integrate them\ninto the robot autonomy stack.",
    "start": "49938",
    "end": "55079"
  },
  {
    "text": "So if you're not\nfamiliar with the NeRF-- I'm sure many of\nyou are by now--",
    "start": "55080",
    "end": "60510"
  },
  {
    "text": "NeRFs are, I use the term NeRF\nas kind of a catchall term. There's a specific\nthing called a NeRF that",
    "start": "60510",
    "end": "65850"
  },
  {
    "text": "was introduced in\na paper in 2020, but by NeRF I really mean all\nkind of vision-based light field",
    "start": "65850",
    "end": "73570"
  },
  {
    "text": "representing models that try\nto capture 3D structure and 3D light fields for a scene.",
    "start": "73570",
    "end": "80080"
  },
  {
    "text": "So this is an example of a NeRF. This is Old Union, as\nyou can probably tell, which is a building\ncomplex here on campus.",
    "start": "80080",
    "end": "86649"
  },
  {
    "text": "And this is a deep\nneural network",
    "start": "86650",
    "end": "92230"
  },
  {
    "text": "that you can image in a way\nthat I'll discuss in a minute. So all of the information you're\nseeing, even though it looks",
    "start": "92230",
    "end": "98260"
  },
  {
    "text": "like a video feed, is\nactually stored in the weights of a deep neural network. And that deep neural\nnetwork is trained by images",
    "start": "98260",
    "end": "105220"
  },
  {
    "text": "taken from a regular iPhone. So one of my PhD students\ntook out his iPhone, walked around the\ncourtyard of Old Union,",
    "start": "105220",
    "end": "112450"
  },
  {
    "text": "and then took the\nimages, fed them into what is now a fairly\nstandard training pipeline.",
    "start": "112450",
    "end": "117520"
  },
  {
    "text": "And this is the\nmodel that emerges. So you can see that\nthe model allows you to interact with the world\nin a video game sort of way,",
    "start": "117520",
    "end": "127940"
  },
  {
    "text": "right? You can free fly a virtual\ncamera around the scene.",
    "start": "127940",
    "end": "133480"
  },
  {
    "text": "And so this gives\nyou the capability to predict what a camera or\nwhat a camera will see if it's",
    "start": "133480",
    "end": "139900"
  },
  {
    "text": "in some part of the scene. And it also gives\nyou the ability to reason about the 3D\nstructure of the scene,",
    "start": "139900",
    "end": "145330"
  },
  {
    "text": "and to close feedback\nloops, and to do planning, and to also reason\nabout semantics and object-level\nreasoning, and do",
    "start": "145330",
    "end": "152080"
  },
  {
    "text": "all these really exciting things\nthat were not possible before. So that's the main idea, is\nstoring 3D environment geometry",
    "start": "152080",
    "end": "159760"
  },
  {
    "text": "and lighting and richer\nmaybe semantic information in a neural network.",
    "start": "159760",
    "end": "165640"
  },
  {
    "text": "And so in this talk, I'll talk\nabout scene-wide representation",
    "start": "165640",
    "end": "171190"
  },
  {
    "text": "as well as object-centric\nrepresentation. They're very closely related. In both cases, the\nconcept is represent",
    "start": "171190",
    "end": "178660"
  },
  {
    "text": "the geometry and the\nlighting as a neural network. So what's new here? Why can't we do this with\nexisting geometry or environment",
    "start": "178660",
    "end": "187090"
  },
  {
    "text": "representations in robotics? I would argue you can, but\nit's just really clunky",
    "start": "187090",
    "end": "193420"
  },
  {
    "text": "and multifaceted, lots of\ndifferent interlocking parts that become very elegantly\nstreamlined if you",
    "start": "193420",
    "end": "200097"
  },
  {
    "text": "treat this as a neural\nenvironment representation. So what are the existing\nrepresentation architectures",
    "start": "200097",
    "end": "206680"
  },
  {
    "text": "in robotics and also\nin computer vision? So the standard in computer\nvision is a triangular mesh. This is the workhorse, the\nbackbone of all computer",
    "start": "206680",
    "end": "214336"
  },
  {
    "text": "graphics in computer\nvision, you could argue, until maybe two\nor three years ago. So if you watch a CGI movie\nor a movie with CGI effects,",
    "start": "214337",
    "end": "222190"
  },
  {
    "text": "those effects are produced\nby a triangular mesh. So it's an extremely\nrich representation, but it's actually\npretty rare in robotics",
    "start": "222190",
    "end": "228730"
  },
  {
    "text": "to deal directly with\ntriangular meshes. And the reason is because it's\ndifficult to build triangular",
    "start": "228730",
    "end": "234340"
  },
  {
    "text": "meshes online from\nrobot sensing data. It's great if you have\ncomplete control of the model,",
    "start": "234340",
    "end": "240370"
  },
  {
    "text": "and you're constructing the\nmodel in a graphics engine. That's great. But if you're actually\nbuilding a model",
    "start": "240370",
    "end": "246190"
  },
  {
    "text": "to represent the observed\nnatural world, it's difficult. It's difficult to correct these\nmodels if mistakes are made,",
    "start": "246190",
    "end": "253810"
  },
  {
    "text": "and it's difficult to\nreason about uncertainty with these models, and\nso they're typically not used in robotics.",
    "start": "253810",
    "end": "259539"
  },
  {
    "text": "Instead, in robotics we tend\nto use point clouds, which are the natural\nrepresentation if you have",
    "start": "259540",
    "end": "265750"
  },
  {
    "text": "LiDAR sensors or RGB-D\nsensors and voxel grids, which",
    "start": "265750",
    "end": "270993"
  },
  {
    "text": "is a really natural\nrepresentation if you're doing planning, and\nyou're worried about safety. And oftentimes, both of\nthese representations",
    "start": "270993",
    "end": "276910"
  },
  {
    "text": "live together in an\nautonomous stack, right? So maybe your sensors induce\na point cloud-based map, and then you process the point\ncloud map to get a voxel grid",
    "start": "276910",
    "end": "284350"
  },
  {
    "text": "representation. And then you do planning and\ncontrol on the voxel grid representation. So what's missing in these\ntwo standard representations?",
    "start": "284350",
    "end": "293169"
  },
  {
    "text": "There's no light information. There's no way of taking\nthe very rich, you know,",
    "start": "293170",
    "end": "300010"
  },
  {
    "text": "color and texture and context\nthat comes from an image and somehow reasoning\nabout that in the 3D scene.",
    "start": "300010",
    "end": "305992"
  },
  {
    "text": "It just doesn't exist. It's all geometry,\n100% geometry. Also, point clouds,\nyou can't carefully",
    "start": "305993",
    "end": "314430"
  },
  {
    "text": "reason about collisions\nin point clouds because everything's free space\nexcept for these little singular points.",
    "start": "314430",
    "end": "319590"
  },
  {
    "text": "And voxel grids are, in some\nsense, storage inefficient.",
    "start": "319590",
    "end": "326100"
  },
  {
    "text": "You can make them more\nefficient with clever grid-style representations, but they tend\nto be storage inefficient.",
    "start": "326100",
    "end": "333930"
  },
  {
    "text": "And they're kind of\noverapproximate geometry representations, right? Everything is either a big\nblock that's there or not there,",
    "start": "333930",
    "end": "340350"
  },
  {
    "text": "and so they tend to lead\nto over safe maneuvers. And neither of these\nrepresentations",
    "start": "340350",
    "end": "345539"
  },
  {
    "text": "do a very good job at\nhandling uncertainty. And so my pitch here is that\nneural representations-- maybe.",
    "start": "345540",
    "end": "352380"
  },
  {
    "text": "It's early days-- but\nI'd like to believe that we can go a lot farther\nwith neural representations",
    "start": "352380",
    "end": "357960"
  },
  {
    "text": "by covering all of\nthese deficiencies that exist currently. And furthermore, you can get\npoint clouds and voxel grids out",
    "start": "357960",
    "end": "364830"
  },
  {
    "text": "of neural representation,\nso they're, in some sense, kind of a unifying\nrepresentation.",
    "start": "364830",
    "end": "369870"
  },
  {
    "text": "OK, so let's get into a\nlittle bit of nuts and bolts about NeRFs. Exactly how do you store the\nlight field in a neural network,",
    "start": "369870",
    "end": "378420"
  },
  {
    "text": "and does storing the\nlight field actually help you with 3D geometry\nand avoiding collisions?",
    "start": "378420",
    "end": "383760"
  },
  {
    "text": "So the paper that introduced\nthe NeRF concept is listed here. That came out of Berkeley\na couple of years ago,",
    "start": "383760",
    "end": "391380"
  },
  {
    "text": "super influential paper. It's already got\nthousands of citations. This was really a seminal paper,\nI would say, in computer vision.",
    "start": "391380",
    "end": "399150"
  },
  {
    "text": "So the idea is, you have\na fairly small, typically a fairly small neural\nnetwork, usually a few layers,",
    "start": "399150",
    "end": "405270"
  },
  {
    "text": "something like five to\nseven layers of 512 neurons. They're all standard,\nusually ReLU neurons,",
    "start": "405270",
    "end": "410612"
  },
  {
    "text": "except for there's some special\nneurons on the first layer. These are all details. The main idea is that\nthis neural network",
    "start": "410612",
    "end": "416520"
  },
  {
    "text": "stores a map, which\ngoes from a 3D query point in the environment. So you ask, you\nknow, right here,",
    "start": "416520",
    "end": "422520"
  },
  {
    "text": "that's the query point, and\nthen a viewing direction. So I'm looking at that\npoint from this perspective or from this perspective\nor from this perspective.",
    "start": "422520",
    "end": "429343"
  },
  {
    "text": "You have a point and\nyour viewing direction. And then the neural network\nspits out a density, which is basically--",
    "start": "429343",
    "end": "435420"
  },
  {
    "text": "it can be interpreted as\na differential probability of occupancy, so think of\nit as a continuous version",
    "start": "435420",
    "end": "441930"
  },
  {
    "text": "of an occupancy grid. And radiance, which is\njust the RGB color vector. So at every point\nin the 3D space,",
    "start": "441930",
    "end": "448740"
  },
  {
    "text": "this function tells\nyou the probability that that point is occupied\nby something and the color",
    "start": "448740",
    "end": "454320"
  },
  {
    "text": "that that point appears as from\na certain viewing direction. This alone doesn't\ngive you the ability",
    "start": "454320",
    "end": "461760"
  },
  {
    "text": "to produce images, right? This is just this\nvolumetric model of the light field and\nthe occupancy field.",
    "start": "461760",
    "end": "468330"
  },
  {
    "text": "So the NeRF is paired with a\nvolumetric rendering scheme,",
    "start": "468330",
    "end": "475650"
  },
  {
    "text": "which is more or less a\nstandard component in graphics and computer vision.",
    "start": "475650",
    "end": "482370"
  },
  {
    "text": "It's called ray tracing,\nand what you do is, you computationally\npose your camera. You say, I want my camera to be\nimaging from this perspective,",
    "start": "482370",
    "end": "489750"
  },
  {
    "text": "and you draw a ray from the\nfocal point of the camera through one pixel. And then you go along that ray\nand sample your neural network.",
    "start": "489750",
    "end": "497910"
  },
  {
    "text": "You say, what's the\ndensity in color here? What's it here? What is it here? What is it here? And at all of these\npoints, you get a density,",
    "start": "497910",
    "end": "503250"
  },
  {
    "text": "which tells you a weighting,\nand then the color, which is the color that you\nwant to appear on that pixel.",
    "start": "503250",
    "end": "508800"
  },
  {
    "text": "And then you combine these\nweights and these colors in-- I'm going to say a\nweighted average. It's not exactly that.",
    "start": "508800",
    "end": "514740"
  },
  {
    "text": "There's something a\nlittle more sophisticated, but conceptually it's\na weighted average. So you take that\nweighted average,",
    "start": "514740",
    "end": "519899"
  },
  {
    "text": "and you get an expected color. And so if you have\na lot of gray sample points with high weighting\nand a couple green sample",
    "start": "519900",
    "end": "527065"
  },
  {
    "text": "points with low weighting,\nyou take the weighted average. It's roughly gray, and then you\nimage the pixel as being gray,",
    "start": "527065",
    "end": "532260"
  },
  {
    "text": "right? And you do this across all\nthe pixels in your image. You define the\nresolution of your image. It's a continuous\nrepresentation,",
    "start": "532260",
    "end": "538230"
  },
  {
    "text": "so you can have a super coarse\nimage, an extremely fine image. It doesn't matter. It's totally adaptable. And you get just\nincredibly lifelike images.",
    "start": "538230",
    "end": "546630"
  },
  {
    "text": "So the video feed\nthat you see here is a sequence of images that\nwas never taken by any camera.",
    "start": "546630",
    "end": "553710"
  },
  {
    "text": "This is the result of\nimaging this neural network. Now, the neural network\nwas trained from images,",
    "start": "553710",
    "end": "558948"
  },
  {
    "text": "from different images than\nthe ones you're seeing here. So there are images that\nsomeone took a camera",
    "start": "558948",
    "end": "564209"
  },
  {
    "text": "and took some images and then\ntrained the neural network. And then now it's\ngenerating fake images from perspectives that have\nnever been seen by a camera.",
    "start": "564210",
    "end": "571889"
  },
  {
    "text": "But you'd never know this. This totally tricks the eye. I don't know about you guys, but\nto me this is as good as real.",
    "start": "571890",
    "end": "578310"
  },
  {
    "text": "Now, the ability to produce\nlifelike images is cool. It's a nice trick. But as a roboticist, I want\nto know about 3D geometry.",
    "start": "578310",
    "end": "585300"
  },
  {
    "text": "I want to know if I can\navoid hitting things, or I want to know if I\ncan go pick things up.",
    "start": "585300",
    "end": "590610"
  },
  {
    "text": "And so what's maybe more\nsurprising and more interesting as a roboticist is that\nthis NeRF also captures very",
    "start": "590610",
    "end": "600900"
  },
  {
    "text": "high-fidelity 3D structure. As you can see in the bottom\nimages, those are depth images. So I told you how to\nrender color images.",
    "start": "600900",
    "end": "607320"
  },
  {
    "text": "There's another pipeline\nfor rendering depth images. There's another pipeline\nfor rendering point clouds. This kind of primitive of the\nvolumetric neural representation",
    "start": "607320",
    "end": "617829"
  },
  {
    "text": "can be coupled with any kind\nof imaging sort of sensor model you like to produce\nsynthetic sensor data.",
    "start": "617830",
    "end": "626510"
  },
  {
    "text": "OK, so there's a lot of\ngood 3D structure in there. How do we pull it out, and\nhow do we do planning safely, and how do we think\nabout manipulation",
    "start": "626510",
    "end": "634160"
  },
  {
    "text": "or think about maybe interacting\nwith the world at a higher semantic level of abstraction,\nthese kinds of things?",
    "start": "634160",
    "end": "641570"
  },
  {
    "text": "So those are the questions\nthat I'll address in this talk, and I'm talking about\nresearch that we've already",
    "start": "641570",
    "end": "648019"
  },
  {
    "text": "done in my lab as well as\nwork that's kind of underway. I'm showing you kind of hot\noff the presses results here.",
    "start": "648020",
    "end": "655670"
  },
  {
    "text": "My lab, along with several\nother labs here at Stanford, have been sort of early\nmovers in this space, kind",
    "start": "655670",
    "end": "662510"
  },
  {
    "text": "of taking NeRF-type models\nand integrating them into the robot autonomy stack. But there have been a number\nof notable contributions",
    "start": "662510",
    "end": "670457"
  },
  {
    "text": "that didn't come\nfrom my lab as well. So one of the\nearliest contributions came from Alberto\nRodriguez's lab at MIT",
    "start": "670458",
    "end": "679580"
  },
  {
    "text": "and Phillip Isola's lab at MIT. And they came up with a\nreally beautiful idea. This came like months after\nthe original NeRF paper.",
    "start": "679580",
    "end": "686192"
  },
  {
    "text": "I don't know how these\nguys work so fast. But they propose a\nmethod called iNeRF,",
    "start": "686192",
    "end": "691740"
  },
  {
    "text": "which says, OK, you've trained\nyour NeRF, you have this NeRF. Now, a robot or a camera or\nwhatever is in the world,",
    "start": "691740",
    "end": "697770"
  },
  {
    "text": "it takes a picture and has no\nidea where it took the picture. It just takes a picture. Can you figure out\nwhere the camera",
    "start": "697770",
    "end": "703589"
  },
  {
    "text": "was to have taken that picture? And they solve that problem. That's called iNeRF, and I'll be\nbuilding on some techniques that",
    "start": "703590",
    "end": "710040"
  },
  {
    "text": "are based on iNeRF. Another paper that came out\nactually before the NeRF paper was called iMAP.",
    "start": "710040",
    "end": "715830"
  },
  {
    "text": "It came from Andy Davidson's\nlab at Imperial College, London, I think.",
    "start": "715830",
    "end": "722130"
  },
  {
    "text": "And they basically\nbuild a SLAM pipeline on top of not quite NeRF, but\nsomething very much like NeRF.",
    "start": "722130",
    "end": "730440"
  },
  {
    "text": "It's kind of a NeRF lite, so\nto speak, and this, as I said, came out before the NeRF paper.",
    "start": "730440",
    "end": "735899"
  },
  {
    "text": "And then in the space\nof manipulation, there's been quite\na number of works. This was just an early one\nthat actually came also",
    "start": "735900",
    "end": "742110"
  },
  {
    "text": "from Philip Isola and\nAlberto Rodriguez. And it was basically\ncombining the concept of dense descriptors.",
    "start": "742110",
    "end": "748020"
  },
  {
    "text": "If you're not familiar with\nthe concept, not a big deal. It's basically the idea of\nlearning a color map that",
    "start": "748020",
    "end": "753810"
  },
  {
    "text": "matches to an object. So all forks, in whatever\npose they appear, in whatever background\nclutter they appear,",
    "start": "753810",
    "end": "761640"
  },
  {
    "text": "are, through this neural\nnetwork, mapped to a color map so that you can distinguish\nthe prongs of the fork",
    "start": "761640",
    "end": "768333"
  },
  {
    "text": "from the handle of the fork. And then that's useful\nfor manipulation. So basically, they\ntook NeRFs together",
    "start": "768333",
    "end": "774240"
  },
  {
    "text": "with this dense\ndescriptor concept and used the NeRFs to\nproduce training data to make",
    "start": "774240",
    "end": "780030"
  },
  {
    "text": "better dense descriptors. These are just a few examples. There's a lot more\nout there these days. There's been a real explosion\nin this field lately.",
    "start": "780030",
    "end": "787770"
  },
  {
    "text": "So as far as work\nfrom my lab, I'm going to talk about three works\nthat I'm really excited about. Two of them have papers already.",
    "start": "787770",
    "end": "793693"
  },
  {
    "text": "One of them is a\npaper in progress, and the lead author of that\nwork is in the back here.",
    "start": "793693",
    "end": "799740"
  },
  {
    "text": "So I'll try to\nshout out the names of all the authors from my\nlab who produced these works.",
    "start": "799740",
    "end": "805960"
  },
  {
    "text": "So first question\nthat came to mind when we started learning\nabout NeRFs is, OK, great,",
    "start": "805960",
    "end": "811890"
  },
  {
    "text": "can we use the NeRF as a\nbackbone for a safe planning? Can we navigate a robot\nthrough the world based only",
    "start": "811890",
    "end": "818340"
  },
  {
    "text": "on a NeRF map with some kind\nof assurances about safety or avoiding collisions?",
    "start": "818340",
    "end": "824820"
  },
  {
    "text": "So let's launch into\nthat to begin with. Can we plan safely\nthrough a NeRF? What exactly is\nthe problem here?",
    "start": "824820",
    "end": "831960"
  },
  {
    "text": "Why is it hard? Why might it be hard to\nplan safely through a NeRF? Well, typically when you talk\nabout guaranteed safe planning,",
    "start": "831960",
    "end": "840390"
  },
  {
    "text": "you're talking about\na map representation that has definite\noccupied and free regions.",
    "start": "840390",
    "end": "847500"
  },
  {
    "text": "So the occupancy grid is perfect\nfor guaranteed safe planning. So you know what's occupied,\nyou know what's free.",
    "start": "847500",
    "end": "853350"
  },
  {
    "text": "All you have to do is\nmake sure your robot stays in the free space. That's great. Maybe if your robot\npath is noisy,",
    "start": "853350",
    "end": "859080"
  },
  {
    "text": "then you worry about the\nprobability of your noisy robot intersecting with the\nfree space-- or excuse me, with the occupied space.",
    "start": "859080",
    "end": "865080"
  },
  {
    "text": "Great. The problem with the NeRF\nis that it doesn't represent free and occupied space. It represents a\ncontinuous density field,",
    "start": "865080",
    "end": "871890"
  },
  {
    "text": "where roughly the\ndensity is high in places that are occupied, and it's low\nin places that aren't occupied.",
    "start": "871890",
    "end": "878339"
  },
  {
    "text": "But exactly how do\nyou scale the density to pull out a\nprobability of occupancy?",
    "start": "878340",
    "end": "884430"
  },
  {
    "text": "That was really unknown until we\nstarted digging into this paper. And this is not an\nartificial problem.",
    "start": "884430",
    "end": "890670"
  },
  {
    "text": "So you might think,\nwell, maybe in practice, the density is\nessentially 0 here",
    "start": "890670",
    "end": "896440"
  },
  {
    "text": "and then jumps up to\nessentially 100 here, and so we're just talking\nabout minor details. That's not the case at all.",
    "start": "896440",
    "end": "902560"
  },
  {
    "text": "In NeRFs, the density is quite\nsquishy and well distributed throughout the scene. And the squishiness of the\ndensity, we noticed empirically,",
    "start": "902560",
    "end": "910570"
  },
  {
    "text": "corresponds very much to the\nquality of your training data. So if you have really a lot\nof training data right here,",
    "start": "910570",
    "end": "915970"
  },
  {
    "text": "you'll get that crisp edge. But if you don't have\nas much over here, you'll get a very foggy\nand noisy field of density.",
    "start": "915970",
    "end": "922840"
  },
  {
    "text": "So how do we deal with this\nnoisy field of density? So here we have an example of\na NeRF trained of a coffee cup,",
    "start": "922840",
    "end": "930009"
  },
  {
    "text": "and we took test points and\nmoved them through the density field and then plotted,\nfor each test point,",
    "start": "930010",
    "end": "936280"
  },
  {
    "text": "the density magnitude for each\nof these three test points. And you can see that the\ndistribution of density",
    "start": "936280",
    "end": "942100"
  },
  {
    "text": "in some of these test points is\nquite peaky, like the green one. In others, it's pretty spread\nout, like the blue one.",
    "start": "942100",
    "end": "947440"
  },
  {
    "text": "And in all cases, it's\nvery noisy, right? So if you're relying\non a particular point",
    "start": "947440",
    "end": "953680"
  },
  {
    "text": "estimate of the density, it's\nnot a very reliable thing. So how do we reason about\nprobabilities of collision",
    "start": "953680",
    "end": "960910"
  },
  {
    "text": "in this kind of\ndensity-based model? So this is primarily the work\nof Tim Chen and a former student",
    "start": "960910",
    "end": "967360"
  },
  {
    "text": "of mine, Preston Culbertson,\nwho's now a postdoc at Caltech. We thought hard\nabout this, and we",
    "start": "967360",
    "end": "973269"
  },
  {
    "text": "realized that the\nequations that describe the rendering for NeRFs,\nthe model that I showed you",
    "start": "973270",
    "end": "981970"
  },
  {
    "text": "that takes the NeRF\nand produces a picture, those equations really suggest\nthat the underlying density",
    "start": "981970",
    "end": "988930"
  },
  {
    "text": "field is actually the density\nfield of a Poisson point process, which is\na well understood random process, which I'm\ngoing to explain very briefly.",
    "start": "988930",
    "end": "997750"
  },
  {
    "text": "This connection wasn't\npreviously made. We made this connection in\na paper that's under review.",
    "start": "997750",
    "end": "1003149"
  },
  {
    "text": "So what the heck is a\nPoisson point process? Poisson point\nprocess is something that is described by a density\nfield over some subset of R2,",
    "start": "1003150",
    "end": "1015420"
  },
  {
    "text": "R3, R4, R5, whatever. And the density you\ncan use to induce",
    "start": "1015420",
    "end": "1023520"
  },
  {
    "text": "the statistics or\nthe probability distribution of\nfinding balls that",
    "start": "1023520",
    "end": "1030419"
  },
  {
    "text": "randomly appear\nin the scene, it's a little bit hard to explain. Let me explain what I mean. So suppose you\nhave a query region",
    "start": "1030420",
    "end": "1038159"
  },
  {
    "text": "A, so you have some subset\nof your larger space A, this potato-looking thing.",
    "start": "1038160",
    "end": "1045060"
  },
  {
    "text": "And you want to know,\nwhat's the probability distribution over finding any\nnumber of points in that set A?",
    "start": "1045060",
    "end": "1050670"
  },
  {
    "text": "What's the probability there\nare no points, 1.2 point, 100 points? Each one of those\nnumber of points has a certain probability\nattached to it,",
    "start": "1050670",
    "end": "1057660"
  },
  {
    "text": "and that probability,\nit turns out, is described by a Poisson\nprocess, not a Poisson point process, but the old\nfashioned Poisson process.",
    "start": "1057660",
    "end": "1064890"
  },
  {
    "text": "Which means that there's\nsome Poisson rate which describes the probability that\neach of these numbers of balls",
    "start": "1064890",
    "end": "1070530"
  },
  {
    "text": "appears, and that Poisson\nrate you obtain by integrating the density over the set\nA. So capital lambda is",
    "start": "1070530",
    "end": "1077580"
  },
  {
    "text": "the Poisson rate\nassociated with the set, and you obtain\nthat by integrating lowercase lambda, the\ndensity, over the set A. Then",
    "start": "1077580",
    "end": "1085500"
  },
  {
    "text": "you get the rate of the\nPoisson process, which has this kind of exponential--",
    "start": "1085500",
    "end": "1090540"
  },
  {
    "text": "sorry, the probability\ndistribution of the Poisson process, which\nis given by this kind of",
    "start": "1090540",
    "end": "1096059"
  },
  {
    "text": "scaled exponential\ndistribution here. So the higher lowercase lambda\nis, the higher capital lambda is",
    "start": "1096060",
    "end": "1104367"
  },
  {
    "text": "and the more likely you're\ngoing to have a larger number of balls in that set. And so why do we care?",
    "start": "1104368",
    "end": "1109830"
  },
  {
    "text": "We care because we can\ncalculate rigorous probabilities about collision or the\nseverity of collision",
    "start": "1109830",
    "end": "1116760"
  },
  {
    "text": "with a Poisson point process. So if you take the\nset of interest as being the set that's\noccupied by the robot body,",
    "start": "1116760",
    "end": "1125100"
  },
  {
    "text": "you can then query the\nPoisson point process. So you have your robot\nbody, you pose it in a particular pose\nin your environment,",
    "start": "1125100",
    "end": "1131580"
  },
  {
    "text": "and then you ask the\nPoisson point process, what's the probability that\nthere are no points intersecting",
    "start": "1131580",
    "end": "1136810"
  },
  {
    "text": "with this volume? And that's a number. It's 0.99 or 0.2, right?",
    "start": "1136810",
    "end": "1142240"
  },
  {
    "text": "And it's totally calibrated. That's really the probability\nof there being a collision,",
    "start": "1142240",
    "end": "1148060"
  },
  {
    "text": "given the certainty or lack\nof certainty with which we've modeled the environment.",
    "start": "1148060",
    "end": "1154420"
  },
  {
    "text": "So you can make draws. Just like any other\nrandom process,",
    "start": "1154420",
    "end": "1161020"
  },
  {
    "text": "you can take a draw from\na Poisson point process. And if you take a\ndraw from the NeRF as a Poisson point process,\nwhat does a draw look like?",
    "start": "1161020",
    "end": "1168670"
  },
  {
    "text": "It looks like a bunch of\nrandomly distributed balls. It's a point cloud. So if you take a draw\nfrom the Poisson point",
    "start": "1168670",
    "end": "1174429"
  },
  {
    "text": "process, which is underlying\nthe NeRF, it looks like this. So there's sensible 3D\ngeometry right in there.",
    "start": "1174430",
    "end": "1181870"
  },
  {
    "text": "And the NeRF that we\ntook the draw from is what you see one\nof the training images up there, so it's\nfrom the flight",
    "start": "1181870",
    "end": "1188080"
  },
  {
    "text": "lab in the basement\nof the Duran building. We set up some\npillars as features. We've moved a camera around\nthe scene, trained a NeRF,",
    "start": "1188080",
    "end": "1195400"
  },
  {
    "text": "and then we drew the Poisson\npoint process from that NeRF. So this is one random draw.",
    "start": "1195400",
    "end": "1200930"
  },
  {
    "text": "If I were to take\nanother random draw, any specific ball\nwould be located in a totally different position,\nbut the bulk 3D geometry",
    "start": "1200930",
    "end": "1207890"
  },
  {
    "text": "would be the same. So this is the nature of\na Poisson point process. Any particular ball\nis randomly placed,",
    "start": "1207890",
    "end": "1213710"
  },
  {
    "text": "but the overall\nstructure of the scene emerges from all\nthese different balls.",
    "start": "1213710",
    "end": "1219049"
  },
  {
    "text": "And you see that there\nare what we call floaters. There are balls out\nthere in free space which indicate that there is a lack\nof information from the training",
    "start": "1219050",
    "end": "1227330"
  },
  {
    "text": "data in those regions. We don't really know if\nthat's free or occupied. Whereas where the dense\nballs are clustered,",
    "start": "1227330",
    "end": "1233000"
  },
  {
    "text": "we know quite confidently\nthat that space is occupied. So that seems like\na useful tool,",
    "start": "1233000",
    "end": "1238135"
  },
  {
    "text": "but why do we have\nany confidence that we can interpret\nthe density of the NeRF as the density of an underlying\nPoisson point process?",
    "start": "1238135",
    "end": "1245120"
  },
  {
    "text": "Well, this has to do with\nthe rendering equation, the underlying equation\nthat goes into rendering",
    "start": "1245120",
    "end": "1251240"
  },
  {
    "text": "the images from the NeRF. ",
    "start": "1251240",
    "end": "1256340"
  },
  {
    "text": "So essentially, if you suppose\nthat the environment is",
    "start": "1256340",
    "end": "1264179"
  },
  {
    "text": "represented as a\nPoisson point process, and you compute the\nexpected color of a pixel,",
    "start": "1264180",
    "end": "1270960"
  },
  {
    "text": "you get exactly\nthe same equations as those rendering equations. So it's entirely by sort of\ntemplate matching, right?",
    "start": "1270960",
    "end": "1277987"
  },
  {
    "text": "If I start with a\nhypothesis, Poisson point process, what is the\ncolor of a pixel, I get the rendering equation.",
    "start": "1277988",
    "end": "1283950"
  },
  {
    "text": "And so that's the intuition. And then, now, how does\nthis tie with actually training of the NeRF?",
    "start": "1283950",
    "end": "1288960"
  },
  {
    "text": "Well, so essentially,\nwe interpret training of the NeRF\nas moment matching,",
    "start": "1288960",
    "end": "1295380"
  },
  {
    "text": "so we're basically\nmatching the expected value of the pixel color between\nthe data and the model.",
    "start": "1295380",
    "end": "1303360"
  },
  {
    "text": "And this is kind of a\nstandard statistical-- a standard way of doing\ndensity estimation,",
    "start": "1303360",
    "end": "1310799"
  },
  {
    "text": "just doing density\nestimation, so estimating the parameters of an\nunknown probability distribution through a moment matching.",
    "start": "1310800",
    "end": "1317070"
  },
  {
    "text": "OK, so that's the\ntheory backbone. So now, we have a\nway of quantifying",
    "start": "1317070",
    "end": "1324310"
  },
  {
    "text": "the probability of collision for\na robot that's posed in a NeRF. We want to wrap that inside\nof a trajectory optimizer,",
    "start": "1324310",
    "end": "1333010"
  },
  {
    "text": "so we take our standard optimal\ncontrol objective function. Maybe we have some desired\ntrajectory, which is x bar.",
    "start": "1333010",
    "end": "1340570"
  },
  {
    "text": "We have the state\nof the robot, which we're trying to make follow x\nbar, but we want it to be safe.",
    "start": "1340570",
    "end": "1347680"
  },
  {
    "text": "We have some penalty\nfor control effort. We have the dynamical\nconstraints of the robot. So maybe it's a drone.",
    "start": "1347680",
    "end": "1353260"
  },
  {
    "text": "Maybe it's a ground robot. Maybe it's a mobile manipulator. Maybe it's an autonomous car. It doesn't really matter.",
    "start": "1353260",
    "end": "1358750"
  },
  {
    "text": "It's just some\ndynamical equations that furnished constraints\nfor our optimizer. And then we have\nthis magic piece,",
    "start": "1358750",
    "end": "1364990"
  },
  {
    "text": "which is the\nprobability of collision as computed from the\nPoisson point process, which is given to us by the NeRF.",
    "start": "1364990",
    "end": "1371020"
  },
  {
    "text": "And so this is our program,\nwhich we try and solve. There are lots of ways to\npotentially solve this. We tried sort of classical\ntrajectory optimization methods,",
    "start": "1371020",
    "end": "1381130"
  },
  {
    "text": "where we're doing either\ngradient descent or Newton's method to solve these equations. We have a paper on\nthat, and the problem",
    "start": "1381130",
    "end": "1390130"
  },
  {
    "text": "is, numerically\nit's a bit tricky, and it's quite\ndifficult to enforce exactly this probability\nof collision constraint",
    "start": "1390130",
    "end": "1397600"
  },
  {
    "text": "using these tools. So instead, we've\ntaken a bit of a pivot and instead looked at basically\ninducing an occupancy grid",
    "start": "1397600",
    "end": "1407590"
  },
  {
    "text": "map from the underlying\nNeRF density,",
    "start": "1407590",
    "end": "1413409"
  },
  {
    "text": "where the occupancy grid\nmap is carefully constructed to embed the collision\nprobability constraints",
    "start": "1413410",
    "end": "1420477"
  },
  {
    "text": "that we're worried about. So let me tell you\nhow that works. So essentially, what we do is,\nwe take the NeRF density, which",
    "start": "1420478",
    "end": "1431650"
  },
  {
    "text": "is related to the Poisson\npoint process density, and we integrate that NeRF\ndensity over a voxel unit.",
    "start": "1431650",
    "end": "1438550"
  },
  {
    "text": "And we get the probability\nthat that voxel unit is free. There are some details in here. I see who's the mastermind\nbehind this work going",
    "start": "1438550",
    "end": "1446140"
  },
  {
    "text": "like this because I'm totally\nbutchering the explanation. Professor-level\nexplanation, you integrate",
    "start": "1446140",
    "end": "1451240"
  },
  {
    "text": "the density over the\nlittle voxel block, and you get the\nprobability that it's free. There's a lot more in there. You can talk to him\nabout the details.",
    "start": "1451240",
    "end": "1457870"
  },
  {
    "text": "And then you do that over\nall the little voxel blocks in your grid, and\nthe ones that--",
    "start": "1457870",
    "end": "1464453"
  },
  {
    "text": "OK, so then you get a continuous\nprobability of occupancy, and then you threshold\nthat probability to give a binary occupancy grid.",
    "start": "1464453",
    "end": "1471250"
  },
  {
    "text": "And then you get a map\nthat looks like this. So the block is\ncolored in, it means that you've exceeded the\nprobability of occupancy",
    "start": "1471250",
    "end": "1478480"
  },
  {
    "text": "for that grid cell. It's not colored in,\nthat means you're below that constraint\nfor probability.",
    "start": "1478480",
    "end": "1483519"
  },
  {
    "text": "Great. And then you take\nyour robot body, you put a bounding\nsphere around it. There are a lot of ways you\ncould potentially do this.",
    "start": "1483520",
    "end": "1489760"
  },
  {
    "text": "This was the simplest and\nsafest thing we could do. Put a bounding sphere\naround it, and then convolve the geometry of that\nbounding sphere with one",
    "start": "1489760",
    "end": "1496150"
  },
  {
    "text": "of these voxel units, and you\nget what we're calling a kernel. And then you convolve the\nkernel with the intensity grid,",
    "start": "1496150",
    "end": "1503080"
  },
  {
    "text": "and this is kind of\nthe classical robotics trick of making your\nmap fat so that you",
    "start": "1503080",
    "end": "1509633"
  },
  {
    "text": "don't have to worry about\nthe geometry of the robot. If you're not familiar\nwith that concept, you probably will become\nfamiliar with it soon.",
    "start": "1509633",
    "end": "1515580"
  },
  {
    "text": "You're given a map,\nand you basically buffer all of the\nwalls of the map to make the map fatter\nso that you only",
    "start": "1515580",
    "end": "1522270"
  },
  {
    "text": "have to worry about the\ncenter point of the robot. This is basically\nwhat we're doing here. So we're making this\nintensity grid fatter",
    "start": "1522270",
    "end": "1529320"
  },
  {
    "text": "by the dimensions\nsuggested by the robot so that now we have a new grid,\nwhich we call the PURR, which",
    "start": "1529320",
    "end": "1535890"
  },
  {
    "text": "is the Probabilistically\nUnsafe Robot Region, which is robot dependent.",
    "start": "1535890",
    "end": "1541410"
  },
  {
    "text": "It kind of convolves\nthe geometry with the robot geometry, the\nscene geometry with the robot geometry. And it says, as long as\nthe center of your robot",
    "start": "1541410",
    "end": "1549330"
  },
  {
    "text": "is in free space\nin the PURR, then the whole body of your robot is\ncolliding with probability less",
    "start": "1549330",
    "end": "1555120"
  },
  {
    "text": "than the desired\nprobability constraint. OK? So now, we take this PURR, and\nwe've just planned trajectories",
    "start": "1555120",
    "end": "1562560"
  },
  {
    "text": "through the free\nspace of this PURR. You could do this in\nmany different ways. You could do poly splines. You could do A star.",
    "start": "1562560",
    "end": "1567990"
  },
  {
    "text": "It turns out we do A star, and\nthe way that we do A star is-- no, excuse me.",
    "start": "1567990",
    "end": "1573580"
  },
  {
    "text": "We do both. We do A star combined\nwith poly spline. So we start with an A star\nsort of base path, which",
    "start": "1573580",
    "end": "1579390"
  },
  {
    "text": "we know to be safe. And then starting from\nthat A star base path, we take all the\nstraight line segments and we push out the sides\nof a rectangular prism",
    "start": "1579390",
    "end": "1589470"
  },
  {
    "text": "to clear out the biggest free\nspace that we can in this PURR. So now, we have this kind of\ncollection of rectangular prisms",
    "start": "1589470",
    "end": "1597168"
  },
  {
    "text": "that are all stuck together. ",
    "start": "1597168",
    "end": "1603120"
  },
  {
    "text": "All the space within\nthose rectangular prisms is free space. So as long as the\ncenter of our robot stays within the union of\nthese rectangular prisms,",
    "start": "1603120",
    "end": "1610529"
  },
  {
    "text": "we're guaranteed to satisfy\nthe collision constraint. And then we plan a poly spline\noptimized to be efficient",
    "start": "1610530",
    "end": "1616230"
  },
  {
    "text": "for trajectory cost, so low\nactuator cost and low path length and so on.",
    "start": "1616230",
    "end": "1622720"
  },
  {
    "text": "And this is the\nplanning pipeline. And so you can see now these\nare sort of trajectory tubes",
    "start": "1622720",
    "end": "1628540"
  },
  {
    "text": "that are superimposed\non that PURR, and these are all\nguaranteed to be safe.",
    "start": "1628540",
    "end": "1633640"
  },
  {
    "text": "OK, so then we've got our plan. It turns out this is really fast\nto replan on because you're just",
    "start": "1633640",
    "end": "1639850"
  },
  {
    "text": "doing A star in this\nkind of geometry and then a poly spline fit. So the idea is that we'll apply\nthis in an MPC receding horizon",
    "start": "1639850",
    "end": "1649600"
  },
  {
    "text": "loop, right? So you plan your original\npaths, you move your robot one or two steps\nalong there, and then",
    "start": "1649600",
    "end": "1654732"
  },
  {
    "text": "your robot has moved a little\nbit because of model mismatch and disturbances. And so then you replan, move,\nreplan, move, and so on.",
    "start": "1654732",
    "end": "1661580"
  },
  {
    "text": "So in order to do\nthat, we need a way of localizing our robot after\nit takes a couple of steps.",
    "start": "1661580",
    "end": "1668860"
  },
  {
    "text": "So we have a guess of\nwhere it should be, which is suggested by\nthe trajectory plan. But then it's going to\nstray from that plan,",
    "start": "1668860",
    "end": "1675265"
  },
  {
    "text": "and we need to figure\nout how much it strayed from the plan and then replan.",
    "start": "1675265",
    "end": "1680305"
  },
  {
    "text": "I'm going to fly through\nthis because I think I'm probably a bit low on time. Yeah, OK.",
    "start": "1680305",
    "end": "1685420"
  },
  {
    "text": "So now, we have a state\nestimation problem. We have our robot with\nan onboard camera.",
    "start": "1685420",
    "end": "1692860"
  },
  {
    "text": "We know where the\nrobot should be, and we can synthesize\nan image that it should see because of the NeRF.",
    "start": "1692860",
    "end": "1698735"
  },
  {
    "text": "This is a special\nproperty of the NeRF. You can fake images, right? So our robot should be here. We synthesize a fake image.",
    "start": "1698735",
    "end": "1704710"
  },
  {
    "text": "The robot is actually here. We don't know where that is,\nbut we take a real image here. So we have a fake\nimage from where",
    "start": "1704710",
    "end": "1711040"
  },
  {
    "text": "it should be and a real image\nfrom where it is, and we don't know where it is. And there's discrepancy,\nphotographic discrepancy",
    "start": "1711040",
    "end": "1718000"
  },
  {
    "text": "between those images. And that's what you see here in\nthe kind of two ghosting images. One is the real image, and\none is the synthetic image.",
    "start": "1718000",
    "end": "1725020"
  },
  {
    "text": "And then what we\ndo is, we optimize the guess of the pose based\non the photographic similarity",
    "start": "1725020",
    "end": "1731919"
  },
  {
    "text": "until the photographs line\nup as best as possible. So this is called the\nphotometric error. We look at the\npixelwise difference",
    "start": "1731920",
    "end": "1737968"
  },
  {
    "text": "between the photographs,\nand we basically backprop through that into the\npose to align the two images.",
    "start": "1737968",
    "end": "1743680"
  },
  {
    "text": "So actually, what\nI just described is the baby version\nof what we do. What we do is, we take that\nkind of mechanism aligning",
    "start": "1743680",
    "end": "1751960"
  },
  {
    "text": "the images, and we put it inside\nof a Kalman filter sort of. It's actually an\noptimization-based filter that's",
    "start": "1751960",
    "end": "1758980"
  },
  {
    "text": "based on the Kalman filter. So we're able to estimate the\nfull dynamical state, not just the pose.",
    "start": "1758980",
    "end": "1764050"
  },
  {
    "text": "So we have three position\nvariables, three orientation variables, three\nposition velocities, and three orientation\nrates, rates of change.",
    "start": "1764050",
    "end": "1771820"
  },
  {
    "text": "So we have the full\n12-dimensional state of the vehicle, and\nwe assume that--",
    "start": "1771820",
    "end": "1778780"
  },
  {
    "text": "so we're taking a\nGaussian filter approach, so we assume that we have\nsome prior mean and covariance",
    "start": "1778780",
    "end": "1784300"
  },
  {
    "text": "over this 12-dimensional state. And then we minimize\na loss function,",
    "start": "1784300",
    "end": "1789367"
  },
  {
    "text": "which combines the\nphotometric loss-- that's the thing I just described,\nhow different the images are-- with the process loss,\nor the prediction loss,",
    "start": "1789367",
    "end": "1795880"
  },
  {
    "text": "which is how confident we are\nin our model of the robot based on its dynamics.",
    "start": "1795880",
    "end": "1801370"
  },
  {
    "text": "So we have these two fighting\nterms, reliance on the model, reliance on the sensor.",
    "start": "1801370",
    "end": "1806590"
  },
  {
    "text": "And that's inherent to\nKalman filtering, right? And then we optimize\nthis thing, which is a little tricky because\nit involves optimizing",
    "start": "1806590",
    "end": "1813820"
  },
  {
    "text": "through the neural\nnetwork weights because you have to optimize\nthrough an image, which is",
    "start": "1813820",
    "end": "1819250"
  },
  {
    "text": "synthesized through the NeRF. It's OK. We can do that. We just backprop in\nPyTorch and so on.",
    "start": "1819250",
    "end": "1824770"
  },
  {
    "text": "And then once we've\noptimized this thing, we obtain one 12D vector, which\nis the mean of the posterior",
    "start": "1824770",
    "end": "1831580"
  },
  {
    "text": "distribution. And then we take the Hessian\nat this optimized mean value,",
    "start": "1831580",
    "end": "1837850"
  },
  {
    "text": "and that becomes the covariance\nof the posterior distribution. Actually, the\ninverse covariance.",
    "start": "1837850",
    "end": "1843190"
  },
  {
    "text": "And then now we\nhave the posterior, which becomes the prior\nfor the next round, and we go in a loop like that.",
    "start": "1843190",
    "end": "1849220"
  },
  {
    "text": "So this state estimator, there's\nnot many state estimators that can do this, that can\nget full 12D dynamical state",
    "start": "1849220",
    "end": "1858090"
  },
  {
    "text": "from images alone. So what we did is, we took an\nexisting method called iNeRF--",
    "start": "1858090",
    "end": "1865382"
  },
  {
    "text": "I gave a little reference to\niNeRF at the top of the talk-- and we kind of augmented that\nwith some Kalman filtering-like",
    "start": "1865382",
    "end": "1870539"
  },
  {
    "text": "features and then compared\nour method to iNeRF. And so you see here our method,\nthe mean is the solid line,",
    "start": "1870540",
    "end": "1877289"
  },
  {
    "text": "and the covariance is\nindicated by the spread. Ours is the brown and red,\nand the iNeRF plus plus, if you want to say it\nthat way, is the green.",
    "start": "1877290",
    "end": "1884310"
  },
  {
    "text": "And so you see that the\nrotational errors are lower. The translational\nerrors are lower. They also have a tighter spread.",
    "start": "1884310",
    "end": "1889860"
  },
  {
    "text": "For whatever reason, the angular\nrates are about the same, and the velocity\nerrors are lower. So overall, this filter seems\nto be doing a lot better job.",
    "start": "1889860",
    "end": "1897510"
  },
  {
    "text": "And then we stick the\nplanner and the replanning together with the pose estimator\nor the state estimator,",
    "start": "1897510",
    "end": "1903270"
  },
  {
    "text": "and we do the MPC. And you see here, this is a\nsimulation with model mismatch. So this is when you're flying\nwith the incorrect model",
    "start": "1903270",
    "end": "1911250"
  },
  {
    "text": "on the open-loop\nplan, you hit things. When you're flying with\nthe incorrect model on the closed-loop plan, with\nstate estimation and replanning,",
    "start": "1911250",
    "end": "1918640"
  },
  {
    "text": "you can accommodate\nthose modeling errors. Same thing with wind gusts, you\ncan recover from wind gusts.",
    "start": "1918640",
    "end": "1925590"
  },
  {
    "text": "OK, and now we're in the process\nof putting all this together into a functioning\nautonomy stack for a drone.",
    "start": "1925590",
    "end": "1932640"
  },
  {
    "text": "What I'm showing you\nis a piece of that. So what we've done\nis, we've offline captured the NeRF,\npreplanned the trajectory,",
    "start": "1932640",
    "end": "1939217"
  },
  {
    "text": "and now we're executing\nthe trajectory on the drone with\nOptiTrack feedback. So stay tuned.",
    "start": "1939217",
    "end": "1945420"
  },
  {
    "text": "We're working hard right now on\ndropping the OptiTrack feedback and instead incorporating\nthat online state estimator",
    "start": "1945420",
    "end": "1952890"
  },
  {
    "text": "from the onboard vision. In any case, with the\nOptiTrack feedback, with the motion capture system\nfeedback, it works very well,",
    "start": "1952890",
    "end": "1960360"
  },
  {
    "text": "but that's not surprising. OK. So one element of this\nthat you might wonder",
    "start": "1960360",
    "end": "1966580"
  },
  {
    "text": "about is, well, your\nwhole planning pipeline depended on a NeRF.",
    "start": "1966580",
    "end": "1971740"
  },
  {
    "text": "Where do you get the NeRF from? Why is it reasonable\nto expect that you have this beautiful\nhigh-fidelity 3D",
    "start": "1971740",
    "end": "1976899"
  },
  {
    "text": "model before you even\ndeploy your robot? So clearly, an important\npiece in this autonomy puzzle",
    "start": "1976900",
    "end": "1982810"
  },
  {
    "text": "is being able to train\na NeRF online as you're moving the robot around, right?",
    "start": "1982810",
    "end": "1989530"
  },
  {
    "text": "In the past six months\nor so, nine months maybe, there have emerged a few\nonline NeRF training pipelines.",
    "start": "1989530",
    "end": "1998440"
  },
  {
    "text": "One of those is ours. So we basically have\na set of ROS tools",
    "start": "1998440",
    "end": "2008789"
  },
  {
    "text": "that implement pieces\nof this pipeline, that then you can\nbuild on top of, right?",
    "start": "2008790",
    "end": "2014010"
  },
  {
    "text": "And so we call this NeRFBridge. To be honest, there's\nnot a whole lot of foundational research here. This is basically\ntool building or sort",
    "start": "2014010",
    "end": "2020640"
  },
  {
    "text": "of a merging of existing tools. So what we do for\nNeRFBridge is, we",
    "start": "2020640",
    "end": "2026830"
  },
  {
    "text": "take a SLAM pipeline\nto get camera poses.",
    "start": "2026830",
    "end": "2031870"
  },
  {
    "text": "A little bit of a\ndetail here, so the NeRF requires camera images\nand camera poses.",
    "start": "2031870",
    "end": "2038720"
  },
  {
    "text": "Traditionally,\nthose camera poses were obtained from a structure\nfrom motion preprocessing step,",
    "start": "2038720",
    "end": "2045020"
  },
  {
    "text": "something called COLMAP. It's like a standard\ncomputer vision structure from motion package.",
    "start": "2045020",
    "end": "2050030"
  },
  {
    "text": "And if you use any of the\nopen-source NeRF training pipelines out there,\nthat's what they do. They take your\nimages without poses.",
    "start": "2050030",
    "end": "2055568"
  },
  {
    "text": "They put them through COLMAP\nto get poses and then take the images and poses, put them\nin the neural network training.",
    "start": "2055568",
    "end": "2061310"
  },
  {
    "text": "The problem is that the neural\nnetwork training takes seconds0 by now.",
    "start": "2061310",
    "end": "2066419"
  },
  {
    "text": "Not originally, but now\nin the state of the art, it takes, you know,\nwithin 5 or 10 seconds, you get a reasonable model, and\nwithin 30 seconds to a minute,",
    "start": "2066420",
    "end": "2074210"
  },
  {
    "text": "you get a really good model. But that preprocessing, getting\nthe poses from the images still takes 30 to 40 minutes\non modern open-source packages.",
    "start": "2074210",
    "end": "2083480"
  },
  {
    "text": "So we don't know how to\nsolve this in robotics. This is standard. It's called SLAM, right? We can get the poses online\nas the robot flies around.",
    "start": "2083480",
    "end": "2091310"
  },
  {
    "text": "So now, what we\ndid in NeRFBridge is, we took the SLAM package,\nan open-source SLAM package,",
    "start": "2091310",
    "end": "2097190"
  },
  {
    "text": "ORB-SLAM3, I believe, to\ntake the camera images and obtain the poses from\nthe sequence of images.",
    "start": "2097190",
    "end": "2103500"
  },
  {
    "text": "And then we take the\nposes and the images and put them into a data\nloader for NeRF training.",
    "start": "2103500",
    "end": "2109112"
  },
  {
    "text": "But we had to re-engineer\nthe data loader because the data loader\nwas designed to be static. It's all in the NeRF universe.",
    "start": "2109112",
    "end": "2115170"
  },
  {
    "text": "It's all just batch\nprocessing, right? You take your\ncollection of images, you put them in your\nneural network trainer,",
    "start": "2115170",
    "end": "2120839"
  },
  {
    "text": "and boom, there's\nyour NeRF model. So if we want to\ndo this online, we need to have kind of a pipeline\ndata loader, where we're",
    "start": "2120840",
    "end": "2126270"
  },
  {
    "text": "taking images and\nposes, putting them in the queue, training,\npopping them out of the queue, new images and\nposes, training, and so on.",
    "start": "2126270",
    "end": "2133283"
  },
  {
    "text": "We do this in a loop. And so long story\nshort is that now we can fly a drone with a camera\non board and train NeRFs",
    "start": "2133283",
    "end": "2144660"
  },
  {
    "text": "at the same time. So here is the image feed that's\nbeing used to train the NeRF.",
    "start": "2144660",
    "end": "2150690"
  },
  {
    "text": "Here's the drone that you\ncan see moving around the lab and from third-person view. And here's the NeRF that's\nbeing trained in real time.",
    "start": "2150690",
    "end": "2158190"
  },
  {
    "text": "There's no preprocessing. This is really the NeRF in real\ntime as it's being trained.",
    "start": "2158190",
    "end": "2164190"
  },
  {
    "text": "You see it's very\nfoggy at first, but as the image\ndiversity increases, you get sharper and\nsharper NeRF renderings.",
    "start": "2164190",
    "end": "2172740"
  },
  {
    "text": "And what's cool about this is,\nthen you can land your drone",
    "start": "2172740",
    "end": "2177990"
  },
  {
    "text": "and then virtually\nfly around your NeRF. That's the advantage\nof the NeRF, right? Suppose you're in a\ndisaster response setting,",
    "start": "2177990",
    "end": "2185339"
  },
  {
    "text": "and you send your drones\nout, and they build the NeRF. And then the rescue\nworkers can asynchronously",
    "start": "2185340",
    "end": "2190860"
  },
  {
    "text": "go interrogate the model and\nsay, what's there, what's there? Is there damage here? Are there survivors here? You don't need to send the\ndrone there to get images.",
    "start": "2190860",
    "end": "2198240"
  },
  {
    "text": "It's all captured in\nthe NeRF, which then you can fly around virtually. And this all works\noutdoors as well, so here's",
    "start": "2198240",
    "end": "2205690"
  },
  {
    "text": "an outdoor demonstration\nat the Elliott Center, which is a conference\ncenter on Lake Lag.",
    "start": "2205690",
    "end": "2211930"
  },
  {
    "text": "This is Lake Lag, and here's\nthe drone flying outside. Here's some of the\nSLAM pipeline that you",
    "start": "2211930",
    "end": "2217390"
  },
  {
    "text": "can see the SLAM features, the\nORB features being visualized. And then here's\nthe NeRF, which is",
    "start": "2217390",
    "end": "2223030"
  },
  {
    "text": "the building facade\nof the Elliott Center that's coming into focus. OK, so now we can\nplan in a NeRF.",
    "start": "2223030",
    "end": "2229900"
  },
  {
    "text": "We can train NeRFs online. We're working on\ncomposing those things. Now that we can train NeRFs\nonline as we move around,",
    "start": "2229900",
    "end": "2238030"
  },
  {
    "text": "and we can plan\nsafely in a NeRF, what else might we want to do? Well, we probably\nwant to maybe interact",
    "start": "2238030",
    "end": "2244780"
  },
  {
    "text": "with the world at a\nhigher semantic level and, you know, think\nabout avoiding collisions",
    "start": "2244780",
    "end": "2252220"
  },
  {
    "text": "with things based on\nsemantic constructions, or think about looking\nfor things based",
    "start": "2252220",
    "end": "2257320"
  },
  {
    "text": "on semantic constructions, based\non open vocabulary queries, right? So there's another tool that\nrecently emerged, unfortunately",
    "start": "2257320",
    "end": "2265562"
  },
  {
    "text": "not from my lab, although\nwe had a lot of discussions about this kind of thing. Another lab beat us to it.",
    "start": "2265562",
    "end": "2271640"
  },
  {
    "text": "Ken Goldberg and Andrew\nKanazawa at Berkeley,",
    "start": "2271640",
    "end": "2276769"
  },
  {
    "text": "they produced this\ntool called LERF, which is Language\nEmbedded Radiance",
    "start": "2276770",
    "end": "2282530"
  },
  {
    "text": "Fields, very simple idea. You take the original NeRF idea,\nand you supervise the training",
    "start": "2282530",
    "end": "2292640"
  },
  {
    "text": "of this architecture with\nan extra output head, which gives you CLIP embeddings.",
    "start": "2292640",
    "end": "2298790"
  },
  {
    "text": "So CLIP, I hope a lot of you\nalready have heard about CLIP. It's an OpenAI foundation model\nthat basically takes images",
    "start": "2298790",
    "end": "2305150"
  },
  {
    "text": "and then encodes\nthem into a latent code, which is just a vector,\njust a 512 dimension vector.",
    "start": "2305150",
    "end": "2314180"
  },
  {
    "text": "And it also takes natural\nlanguage prompts and encodes them into the same vector space\nof 512 dimensional vector.",
    "start": "2314180",
    "end": "2319880"
  },
  {
    "text": "And then you can compare the\ntwo with cosine similarity with basically a dot product. And so it allows you\nto say, how similar",
    "start": "2319880",
    "end": "2327410"
  },
  {
    "text": "is this image to\nthis text prompt? It's a really nice\nfundamental tool. And so basically\nin this framework,",
    "start": "2327410",
    "end": "2335360"
  },
  {
    "text": "they preprocessed all the\ntraining images for the NeRF with their CLIP embeddings,\nwith their CLIP codes,",
    "start": "2335360",
    "end": "2341930"
  },
  {
    "text": "and then supervised the\nNeRF to embed these 2D CLIP codes into 3D.",
    "start": "2341930",
    "end": "2347870"
  },
  {
    "text": "This is called distillation. So basically, now you have a 3D\nfield where at every 3D point,",
    "start": "2347870",
    "end": "2352910"
  },
  {
    "text": "you can ask, what's the\nsemantic CLIP embedding? And then you can\ncompare it with a query.",
    "start": "2352910",
    "end": "2360020"
  },
  {
    "text": "So somebody can say, find-- ",
    "start": "2360020",
    "end": "2365050"
  },
  {
    "text": "I don't know that CLIP\nknows about that-- but say, find [AUDIO OUT]. And then you can computationally\ncheck all of the pixels,",
    "start": "2365050",
    "end": "2373720"
  },
  {
    "text": "or you can computationally check\nany 3D point for its similarity to [AUDIO OUT].",
    "start": "2373720",
    "end": "2379385"
  },
  {
    "text": "And the collection of 3D points\nthat are like [AUDIO OUT] will pop up in a heat map. And so that's the idea here, is\nthat you have a cluttered scene,",
    "start": "2379385",
    "end": "2387190"
  },
  {
    "text": "and at runtime there\nis nothing that said \"toy elephant\" in the training.",
    "start": "2387190",
    "end": "2393150"
  },
  {
    "text": "You know, entirely\nat runtime, you're given a novel prompt,\nwhich is toy elephant. And you do some\nprocessing, and you",
    "start": "2393150",
    "end": "2399370"
  },
  {
    "text": "can light up the 3D\nregion in the scene which corresponds to toy elephant. So it's a really\ncool powerful tool.",
    "start": "2399370",
    "end": "2405549"
  },
  {
    "text": "So we thought,\nwell, why can't we-- this is sort of research\nthat's in action right now.",
    "start": "2405550",
    "end": "2412750"
  },
  {
    "start": "2412750",
    "end": "2418000"
  },
  {
    "text": "Let's see. How do I explain this? Why can't we online\nlook for something",
    "start": "2418000",
    "end": "2423940"
  },
  {
    "text": "based on an open-world\nquery, and train a LERF, and use the relevance between\nthe current LERF and the query",
    "start": "2423940",
    "end": "2434230"
  },
  {
    "text": "to go collect more\nimages to get a better idea of where that thing\nis that we're looking for? So the idea that\nwould drive this",
    "start": "2434230",
    "end": "2442720"
  },
  {
    "text": "is an information-seeking\nplanner, right? We want to be able to plan\na set of poses or emotion",
    "start": "2442720",
    "end": "2449770"
  },
  {
    "text": "for the drone to\ncollect images, where we believe those images have\nhigh information content that's",
    "start": "2449770",
    "end": "2456039"
  },
  {
    "text": "relevant to the\nopen-world query. So maybe the open-world query\nis, \"Find the dump truck.\"",
    "start": "2456040",
    "end": "2461619"
  },
  {
    "text": "We need a way of quantifying\nwhether this image or this image in the future is likely to\nhave lots of information",
    "start": "2461620",
    "end": "2468220"
  },
  {
    "text": "about the dump truck. We haven't taken\nthose images yet, so we need to predict what\nthe information content will",
    "start": "2468220",
    "end": "2474760"
  },
  {
    "text": "be from those different\nimage possibilities. So what we've come\nup with is the idea",
    "start": "2474760",
    "end": "2481640"
  },
  {
    "text": "of quantifying the\nprobability that any point is both occupied and\nrelevant to the query.",
    "start": "2481640",
    "end": "2488510"
  },
  {
    "text": "So the way that we get\nthe occupancy is just by querying the density\nof the NeRF at that point, and the way that\nwe get relevance",
    "start": "2488510",
    "end": "2494600"
  },
  {
    "text": "is by querying the cosine\nsimilarity between the user prompt, like dump truck,\nand the code that's",
    "start": "2494600",
    "end": "2502640"
  },
  {
    "text": "embedded in the LERF. So we take that\ncosine similarity and you get the\nprobability of relevance, given that it's occupied.",
    "start": "2502640",
    "end": "2508670"
  },
  {
    "text": "So you have the probability of\noccupied times the probability that it's relevant,\ngiven that it's occupied, and you get the probability\nthat it's occupied and relevant.",
    "start": "2508670",
    "end": "2516800"
  },
  {
    "text": "That's a pointwise quantity. And then we consider that\nquantity over the whole camera frustrum.",
    "start": "2516800",
    "end": "2522230"
  },
  {
    "text": "We have to do it\nvolumetrically because we don't know where the things\nare in that volume, right?",
    "start": "2522230",
    "end": "2528650"
  },
  {
    "text": "So we integrate basically\nan entropy-like quantity",
    "start": "2528650",
    "end": "2534140"
  },
  {
    "text": "over that camera frustrum. So let me explain what I mean\nby entropy-like quantity. So you know Shannon entropy is\nlike this way of quantifying",
    "start": "2534140",
    "end": "2543290"
  },
  {
    "text": "uncertainty in a\nrandom variable, so we're basically\nusing that concept. So here, we have the density\ntimes the relevance, which",
    "start": "2543290",
    "end": "2552063"
  },
  {
    "text": "is basically the\nprobability that you're occupied and\nrelevant, and we want to quantify the entropy in that\nquantity over the frustrum,",
    "start": "2552063",
    "end": "2559040"
  },
  {
    "text": "over this kind of pyramid\nthat's tipped on its side. So this is just the\nformula for entropy. This is the probabilistic\nquantity and then",
    "start": "2559040",
    "end": "2566090"
  },
  {
    "text": "the natural log of that\nprobabilistic quantity integrated over the frustrum. So this quantifies\nhow unsure are we",
    "start": "2566090",
    "end": "2575600"
  },
  {
    "text": "in occupancy and relevancy\nover the frustrum. ",
    "start": "2575600",
    "end": "2582410"
  },
  {
    "text": "Based on the current\nstate of the LERF, if we're quite unsure,\nwell, we better go take that image so we can\nbecome more sure about it.",
    "start": "2582410",
    "end": "2590450"
  },
  {
    "text": "You know, conversely, if\nwe're very sure already, based on the current\nstate of the LERF, then why do we need\nto go take that image?",
    "start": "2590450",
    "end": "2597050"
  },
  {
    "text": "We don't because it's\nalready in the LERF, right? So we're basically quantifying\nthe lack of information on a camera view-by-view basis\nand going after the images that",
    "start": "2597050",
    "end": "2606500"
  },
  {
    "text": "have low information in\nthe current model in order to boost that\ninformation in the model.",
    "start": "2606500",
    "end": "2611540"
  },
  {
    "text": "And then we wrap this around\na planner, the typical sort of trajectory planning thing. So this is the entropy\nand the frustrum.",
    "start": "2611540",
    "end": "2617990"
  },
  {
    "text": "And in fact, I should\nhave a minus sign there because I'm minimizing. So I want to maximize the\nentropy in the frustrum,",
    "start": "2617990",
    "end": "2623030"
  },
  {
    "text": "so I want to\nminimize the negative of the entropy in the frustrum. I don't want to use too\nmuch control effort. I have some dynamics to my\nrobots that I need to respect,",
    "start": "2623030",
    "end": "2630320"
  },
  {
    "text": "and I want to stay\nin free space. So I have this kind of\ncartoony X, stay in X",
    "start": "2630320",
    "end": "2636620"
  },
  {
    "text": "free, but really this could\nbe a collision probability, like I talked about at\nthe beginning of the talk. And then you optimize this thing\nto find a sequence of viewpoints",
    "start": "2636620",
    "end": "2645080"
  },
  {
    "text": "for your drone or\nfor your robot. And we haven't actually\nimplemented this yet. This is very much\nhot off the presses,",
    "start": "2645080",
    "end": "2651500"
  },
  {
    "text": "but the idea is that we\ncould optimize this thing in a very quick and dirty\nway, because it turns out these trajectory\noptimization problems based",
    "start": "2651500",
    "end": "2658430"
  },
  {
    "text": "on information\ntheoretic objectives are very hard to optimize. They're computationally\nreally intensive, and the cost landscape\nis just really nasty.",
    "start": "2658430",
    "end": "2666620"
  },
  {
    "text": "So what we would do is\nsomething quick and dirty, just roll out a bunch of\nrandom trajectory options,",
    "start": "2666620",
    "end": "2672320"
  },
  {
    "text": "score each one based\non this cost function, and then pick the best\none that's still safe.",
    "start": "2672320",
    "end": "2679160"
  },
  {
    "text": "So by rolling out\nrandom control inputs, you already know that you're\ndynamically feasible, right?",
    "start": "2679160",
    "end": "2684440"
  },
  {
    "text": "You just you randomly sample a\nwhole bunch of control inputs, and then you simulate\nyour robot's motion",
    "start": "2684440",
    "end": "2691205"
  },
  {
    "text": "along those control inputs. So you get dynamic\nfeasibility for free, and then you just have\nto worry about safety",
    "start": "2691205",
    "end": "2696590"
  },
  {
    "text": "and then optimizing\nthe cost that you're trying to optimize for. OK, so that's the idea.",
    "start": "2696590",
    "end": "2701780"
  },
  {
    "text": "One minute, OK. So then we need to be able to\ntrain the NeRF online, train",
    "start": "2701780",
    "end": "2708830"
  },
  {
    "text": "the LERF online to be able to do\nthis in a receding horizon loop. So then this is the next\npiece of the puzzle,",
    "start": "2708830",
    "end": "2715040"
  },
  {
    "text": "is to take what I call\nNeRFBridge, the ROS tool",
    "start": "2715040",
    "end": "2720318"
  },
  {
    "text": "that allows us to\ntrain NeRFs online, and augment it to be LERFBridge\nto train these language embedded",
    "start": "2720318",
    "end": "2725640"
  },
  {
    "text": "NeRFs in real time. This is future work. There are some challenges, but\nwe're going to try and do that.",
    "start": "2725640",
    "end": "2733410"
  },
  {
    "text": "OK, and then I'm\ngoing to talk briefly about one final idea,\nwhich is to produce kind",
    "start": "2733410",
    "end": "2740430"
  },
  {
    "text": "of a reactive\nreasoning feedback loop",
    "start": "2740430",
    "end": "2745650"
  },
  {
    "text": "by embedding a large language\nmodel within our autonomy stack.",
    "start": "2745650",
    "end": "2751140"
  },
  {
    "text": "OK, this is a separate\nidea from information, you know, informative\nplanning in the NeRF.",
    "start": "2751140",
    "end": "2759330"
  },
  {
    "text": "That's idea one. And now idea two,\nwhich is primarily the work of John Tucker,\nis, you have an instruction",
    "start": "2759330",
    "end": "2766830"
  },
  {
    "text": "by a human user,\nopen-world instruction, like you're flying a\ndrone, and you say, drone,",
    "start": "2766830",
    "end": "2772230"
  },
  {
    "text": "go land on that table. Now, a lot of ambiguity\nabout how a drone should",
    "start": "2772230",
    "end": "2777520"
  },
  {
    "text": "land on the table, and\nthe robot has no idea how to resolve that ambiguity. So the idea is to use the common\nsense, to use a large language",
    "start": "2777520",
    "end": "2785050"
  },
  {
    "text": "model as what we're calling\na common sense core, as a way of iterating\non the prompt to get more details about how\nit should do the thing that it's",
    "start": "2785050",
    "end": "2792940"
  },
  {
    "text": "asked to do. Right? And so at a high level, the\nidea is this: land on the table.",
    "start": "2792940",
    "end": "2800109"
  },
  {
    "text": "And then we engineer a\nprompt for the large language model that knows that\nit's a quadrotor that",
    "start": "2800110",
    "end": "2806395"
  },
  {
    "text": "needs to land on the table. So how should a quadrotor\nland on a table? And the LLM comes back\nand says, on a clear spot,",
    "start": "2806395",
    "end": "2814750"
  },
  {
    "text": "which is obvious. Any person knows\nyou land the drone on the clear spot on\nthe table, but the drone",
    "start": "2814750",
    "end": "2820089"
  },
  {
    "text": "doesn't know that, right? You have a table that's\nsuper cluttered over here, and there's a nice\nclear spot here. The drone has no way of\nknowing that it needs",
    "start": "2820090",
    "end": "2826450"
  },
  {
    "text": "to go for the clear spot, so the\nLLM gives us that common sense. And then we query the\nLERF, not for the table,",
    "start": "2826450",
    "end": "2835060"
  },
  {
    "text": "but for the clear spot\non the table, which the person didn't\ntell us to look for the clear spot on the table. The person just said,\nland on the table,",
    "start": "2835060",
    "end": "2841497"
  },
  {
    "text": "but we do this\niteration with the LLM, and we know we're looking for\na clear spot on the table. We call this an\naffordance, right?",
    "start": "2841497",
    "end": "2847390"
  },
  {
    "text": "This is how a robot should do a\nthing, what part of the geometry it should interact with\nto effectively do a thing.",
    "start": "2847390",
    "end": "2853390"
  },
  {
    "text": "So we use the large\nlanguage model to tell us what the\nright affordance is to accomplish the task.",
    "start": "2853390",
    "end": "2858730"
  },
  {
    "text": "And this is real results. Excuse me. We have a LERF\ntrained of a cluttered",
    "start": "2858730",
    "end": "2864810"
  },
  {
    "text": "table, which is cluttered on\none side and free on the other. And we query the LERF,\n\"clear spot on the table,\"",
    "start": "2864810",
    "end": "2870340"
  },
  {
    "text": "and the reddest part, the most\nrelevant part to the query",
    "start": "2870340",
    "end": "2876280"
  },
  {
    "text": "is indeed the clear\nspot on the table. The cluttered spot\nis a bit orangy, so it's a bit less relevant.",
    "start": "2876280",
    "end": "2882400"
  },
  {
    "text": "So it's kind of working. And then we, you know, let\nthis loose on a planner",
    "start": "2882400",
    "end": "2888340"
  },
  {
    "text": "on a robot in the lab. So here's a drone in the\nlab, and it will indeed go and fly and land on a\nclear spot on the table.",
    "start": "2888340",
    "end": "2896507"
  },
  {
    "text": "You've all seen\ndrones flying before, so this probably isn't\nsuper illuminating, but there it is landing on--\nit will land on the table.",
    "start": "2896508",
    "end": "2902850"
  },
  {
    "text": " There it goes.",
    "start": "2902850",
    "end": "2908310"
  },
  {
    "text": "And it lands on the\nclear spot on the table. So one thing that we're\nhoping to do with this is augment this with on-board\nvision on the drone, right?",
    "start": "2908310",
    "end": "2916290"
  },
  {
    "text": "So yeah, from the\nprior LERF model, we want to land on the\nclear spot on the table. But maybe we go and bring\nour camera to that place,",
    "start": "2916290",
    "end": "2923789"
  },
  {
    "text": "and we find that what was once\nclear now has a coffee cup there. We want to be able to be\na little bit more reactive",
    "start": "2923790",
    "end": "2929005"
  },
  {
    "text": "to those real-world changes. OK, so I'm going to I'm going\nto wrap up my talk there.",
    "start": "2929005",
    "end": "2935880"
  },
  {
    "text": "I had one more\nsegment, but clearly I completely overshot on time. And I'm just going to speed\nto my thank you slide.",
    "start": "2935880",
    "end": "2946474"
  },
  {
    "text": " So I'm going to acknowledge all\nmy collaborators, coauthors,",
    "start": "2946475",
    "end": "2954720"
  },
  {
    "text": "and students, who are the\nreal brains behind this work. And then I'm happy to\ntake any questions.",
    "start": "2954720",
    "end": "2960300"
  },
  {
    "text": "Thanks. All right. [APPLAUSE] ",
    "start": "2960300",
    "end": "2969000"
  }
]