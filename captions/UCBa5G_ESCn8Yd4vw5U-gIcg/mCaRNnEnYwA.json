[
  {
    "text": "All right, so we're\nready to get started. Today, we're going to continue\ntalking about diffusion models.",
    "start": "5240",
    "end": "11070"
  },
  {
    "text": "But we're going\nto see how we can use diffusion models\nto model discrete data",
    "start": "11070",
    "end": "18140"
  },
  {
    "text": "and, in particular, text. And we have a guest\nlecturer by Aaron, who is a PhD student in my lab.",
    "start": "18140",
    "end": "23609"
  },
  {
    "text": "And he did some\ngroundbreaking work in this space of using diffusion\nmodels for discrete data",
    "start": "23610",
    "end": "29600"
  },
  {
    "text": "and language. And so, yeah, take\nit away, Aaron. Thanks, Stefano, for the\nintroduction and glad to get",
    "start": "29600",
    "end": "35750"
  },
  {
    "text": "started. Let's get started. To start, I'd like to talk a\nbit about the general framing",
    "start": "35750",
    "end": "42740"
  },
  {
    "text": "of our generative model problem\nand how things work, generally.",
    "start": "42740",
    "end": "47900"
  },
  {
    "text": "So typically, we're\ngiven a dataset x1 to xn, which we assume is\nsampled IID from some data",
    "start": "47900",
    "end": "54890"
  },
  {
    "text": "distribution p data. Our goal is to fit a\nparameterized model p",
    "start": "54890",
    "end": "60620"
  },
  {
    "text": "theta, often parameterized\nby a neural network that approximates our ground truth\ndata distribution p data.",
    "start": "60620",
    "end": "67350"
  },
  {
    "text": "And assuming we can learn\nour p theta well enough, we can generate new samples,\nmaybe new interesting samples",
    "start": "67350",
    "end": "73790"
  },
  {
    "text": "would be the interesting part\nusing our parameterized p theta.",
    "start": "73790",
    "end": "78850"
  },
  {
    "text": "And now, if we can do\neverything together, everything works out, we profit.",
    "start": "78850",
    "end": "84310"
  },
  {
    "text": "But there's kind\nof a bit of math that goes in between,\nas you all know.",
    "start": "84310",
    "end": "89850"
  },
  {
    "text": "So in this class, you\nguys have learned a lot about different generative\nmodeling paradigms such as GANs,",
    "start": "89850",
    "end": "97200"
  },
  {
    "text": "VAEs, and diffusion models. And the thing that\nyou'll notice for all",
    "start": "97200",
    "end": "102660"
  },
  {
    "text": "of these different models\nor most of these models that you've learned\nis that whenever they draw a schematic diagram\nabout what you should be doing,",
    "start": "102660",
    "end": "109950"
  },
  {
    "text": "they normally have a picture-- they normally use an image as\nyour most common data modality.",
    "start": "109950",
    "end": "116160"
  },
  {
    "text": "So here, we have a picture of\na dog, a picture of a number, and a picture of a\nsmaller, cuter dog.",
    "start": "116160",
    "end": "121740"
  },
  {
    "text": "And this type of\ncoincidence actually-- it's not just a coincidence.",
    "start": "121740",
    "end": "126850"
  },
  {
    "text": "It's actually a very fundamental\nreason why we do this. And the reason why is because\nall of our different data--",
    "start": "126850",
    "end": "133800"
  },
  {
    "text": "all of these different\ngenerative models, they're building on the\nfact that we're working over a continuous data space.",
    "start": "133800",
    "end": "140530"
  },
  {
    "text": "So here, our data space\nX is equal to some R d, where you can think of\nR as each pixel value and d",
    "start": "140530",
    "end": "146640"
  },
  {
    "text": "as a total number of pixels,\nso the values of the pixels. And if we visualize this using\nsome spectrum like as follows,",
    "start": "146640",
    "end": "156270"
  },
  {
    "text": "then what's nice is that\nwe can sample points like here, here, or there. And these three different\nsamples are all valid images,",
    "start": "156270",
    "end": "164760"
  },
  {
    "text": "so to speak. And this is a\nfundamental property of continuous spaces and the\nfact that we can interpolate.",
    "start": "164760",
    "end": "171000"
  },
  {
    "text": "Now, what I and a lot of other\npeople are interested in, which is a bit converse\nto this type of setup,",
    "start": "171000",
    "end": "176880"
  },
  {
    "text": "is discrete data\nspace as follows. So instead of having X equals\nR d, we have X is equal to 1",
    "start": "176880",
    "end": "183120"
  },
  {
    "text": "to n to the power of d, where\nn is the total number of-- or is the vocabulary\nsize, so to speak and d is the number\nof dimensions--",
    "start": "183120",
    "end": "189750"
  },
  {
    "text": "is a number of dimensions. We replace our data\npoints with data",
    "start": "189750",
    "end": "194760"
  },
  {
    "text": "points x where x is just a\nsequence of tokens x1 to xd. And then if we\nhave the setup, we",
    "start": "194760",
    "end": "201720"
  },
  {
    "text": "can visualize it\nwith another diagram. This is a lattice,\nwhich is the simplest version of a discrete space.",
    "start": "201720",
    "end": "209070"
  },
  {
    "text": "And while it's true we can\ngenerate samples like here and here, which are valid\ndiscrete data point samples,",
    "start": "209070",
    "end": "216280"
  },
  {
    "text": "we can't really generate\nsamples here or there. We can't generate samples in\nbetween or outside of the values",
    "start": "216280",
    "end": "221520"
  },
  {
    "text": "because that just doesn't make\nany sense for discrete data distribution. And as such, this makes discrete\ndata fundamentally a harder",
    "start": "221520",
    "end": "228510"
  },
  {
    "text": "problem as we'll see. So now, you might be asking\nyourself the question, OK, Aaron, we've learned about\nGANs, diffusion models, VAEs.",
    "start": "228510",
    "end": "237215"
  },
  {
    "text": "These all work pretty nice. Why do we have to go to a\ncompletely different regime? Why do we have to go to discrete\ndata, and why does this matter?",
    "start": "237215",
    "end": "244980"
  },
  {
    "text": "And I would be remiss if I\ndidn't mention our good friends at OpenAI who have released\nthese big large language",
    "start": "244980",
    "end": "253280"
  },
  {
    "text": "models like ChatGPT,\nwhich have really like transformed the world\nin the last couple of years.",
    "start": "253280",
    "end": "258690"
  },
  {
    "text": "Also, I would be\nremiss if I didn't mention other competitors. But yeah, fundamentally, we\nhave this new novel paradigm",
    "start": "258690",
    "end": "264889"
  },
  {
    "text": "of large language\nmodeling, which is perhaps arguably the largest\nadvancement in computer science",
    "start": "264890",
    "end": "272520"
  },
  {
    "text": "machine learning in the\nlast couple of years. And what's interesting\nabout this data domain is that sentences are\nfundamentally discrete.",
    "start": "272520",
    "end": "280490"
  },
  {
    "text": "So for sentences,\nit's a sequence of discrete tokens or discrete\nwords that we build up.",
    "start": "280490",
    "end": "286520"
  },
  {
    "text": "So as such, it would\nmake the most sense to have a probabilistic model\nthat can generate discrete data",
    "start": "286520",
    "end": "292500"
  },
  {
    "text": "like sentences as such. And in particular, if you\nare familiar with the LLM",
    "start": "292500",
    "end": "299400"
  },
  {
    "text": "natural language\nprocessing in general, you may have heard of\nsomething called language model pre-training. This is kind of the core\nstep for many of these models",
    "start": "299400",
    "end": "306210"
  },
  {
    "text": "where you learn distribution\nover all of your input sentences. And really what they mean by\nlanguage model pre-training",
    "start": "306210",
    "end": "314700"
  },
  {
    "text": "is you're just fitting a\ndiscrete probabilistic model to your internet scale data.",
    "start": "314700",
    "end": "319750"
  },
  {
    "text": "So we can see that this idea\nis pretty fundamental here. And other applications\ninclude stuff",
    "start": "319750",
    "end": "326400"
  },
  {
    "text": "in like natural biology and\nnatural sciences more broadly.",
    "start": "326400",
    "end": "331810"
  },
  {
    "text": "We have data modalities such\nas DNA, molecules and proteins.",
    "start": "331810",
    "end": "337139"
  },
  {
    "text": "And all of these\ndifferent data modalities are fundamentally discrete, and\nit would make the most sense to try to generate new novel\nDNA sequences, novel molecules,",
    "start": "337140",
    "end": "347544"
  },
  {
    "text": "and novel proteins which can\nhave a big impact in our day to day lives. And this all requires a\ndiscrete generative model.",
    "start": "347545",
    "end": "354010"
  },
  {
    "text": "And finally, and this is\na bit counterintuitive. We also see a return\nto discreteness",
    "start": "354010",
    "end": "360600"
  },
  {
    "text": "for stuff like images. So this is the schematic\nfor a VQVAE backbone",
    "start": "360600",
    "end": "366900"
  },
  {
    "text": "or VQGAN is one of the many\nbuilding blocks in systems like Stable Diffusion.",
    "start": "366900",
    "end": "372450"
  },
  {
    "text": "And in the middle, we have this\ndiscretized representation, this discretized\nlatent space vectors.",
    "start": "372450",
    "end": "378660"
  },
  {
    "text": "And more recent work-- and\nthis is extremely recent like in the last couple of months\nout of Google and CMU--",
    "start": "378660",
    "end": "384180"
  },
  {
    "text": "has shown that if you\njust like throw away any continuous notion of\nyour discrete latent space,",
    "start": "384180",
    "end": "390850"
  },
  {
    "text": "you only have the\ndiscrete component. This actually leads to a\nbroader improvement in results.",
    "start": "390850",
    "end": "395970"
  },
  {
    "text": "And results like these tend to\nshow that maybe in the future it's possible to reconcile\nimages into this broadly",
    "start": "395970",
    "end": "402600"
  },
  {
    "text": "discrete paradigm as well. But, yeah. So now, we know why\ndiscrete data is important.",
    "start": "402600",
    "end": "408990"
  },
  {
    "text": "So let's ask the question,\nwhy is it so hard? And you might say\nsomething like, hey, Aaron,",
    "start": "408990",
    "end": "414620"
  },
  {
    "text": "this is all very interesting. Why can we just adapt\nexisting continuous space model like a flow or a GAN?",
    "start": "414620",
    "end": "420560"
  },
  {
    "text": "Why can't we just\ntake that and just adapt it to the discrete case? And, well, we have\nsomething like this.",
    "start": "420560",
    "end": "426910"
  },
  {
    "text": "So we have this diagram. We take some random noise. We push it through some\nf theta neural network, and it generates an image.",
    "start": "426910",
    "end": "432620"
  },
  {
    "text": "And this is a good way to\ndo our sampling and whatnot. And the intuitive\nidea here would",
    "start": "432620",
    "end": "438710"
  },
  {
    "text": "be like why can we just\nparametrize f theta to output only discrete values? And since it only\noutputs discrete values,",
    "start": "438710",
    "end": "445040"
  },
  {
    "text": "then we can generate\nsomething like text. And we can go through\nsome examples here. But let's say for flows,\nwe have this kind of core.",
    "start": "445040",
    "end": "453890"
  },
  {
    "text": "We have this coupling\nwhere we take noise to data, data to noise through\nour f theta or f theta inverse",
    "start": "453890",
    "end": "460220"
  },
  {
    "text": "that you guys have seen. And the way that this works is\nthat you can stretch and pull",
    "start": "460220",
    "end": "465960"
  },
  {
    "text": "your space. And this allows you to\ntake the model complex data",
    "start": "465960",
    "end": "471060"
  },
  {
    "text": "distribution with a\nsimple base distribution and a change of variables\nformula as such.",
    "start": "471060",
    "end": "477280"
  },
  {
    "text": "Now, if we replace all\nof this type of stuff with discrete data,\nso let's say we go from a discrete\nrandom sequence to",
    "start": "477280",
    "end": "483720"
  },
  {
    "text": "we map it like bijectively\nto another real sequence",
    "start": "483720",
    "end": "488880"
  },
  {
    "text": "that we want to model. Well, we have a\nchange of variables. Well, we don't really have\na change of variables. In fact, the best\nthat we can get",
    "start": "488880",
    "end": "495210"
  },
  {
    "text": "is this type of setup where\nour x has the same probability as this other x.",
    "start": "495210",
    "end": "500880"
  },
  {
    "text": "And because of this,\nyour base distribution has to be as expressive as\nyour data distribution, which",
    "start": "500880",
    "end": "507330"
  },
  {
    "text": "is why this type of setup\nstruggles really hard. And for this question,\nyeah, we have this flow. It doesn't really generalize.",
    "start": "507330",
    "end": "514650"
  },
  {
    "text": "Also, we have let's say GANs. OK, we take a noise. We map it to an image. We have a discriminator.",
    "start": "514650",
    "end": "521039"
  },
  {
    "text": "And then the idea here\nis we can backpropagate gradients to update our f\ntheta from our discriminator.",
    "start": "521039",
    "end": "527010"
  },
  {
    "text": "And if we replace the\ncomponents with discrete values. So we parametrize it to only\nallow for discrete outputs,",
    "start": "527010",
    "end": "537060"
  },
  {
    "text": "then these gradients\ndon't really backpropagate through a\ndiscrete valued input. And the reason why\nthis doesn't work",
    "start": "537060",
    "end": "543030"
  },
  {
    "text": "is because we don't\nhave calculus. So from these two examples,\nwe can broadly see that our--",
    "start": "543030",
    "end": "548735"
  },
  {
    "text": "we go through this\nslide quickly. But our conclusion is that\nour models are currently too reliant on calculus,\nand it's hard to extend it.",
    "start": "548735",
    "end": "555620"
  },
  {
    "text": "Before transformers, this is\nkind of a modeling question. It is kind of an architectural\nproblem, not a modeling problem.",
    "start": "555620",
    "end": "561649"
  },
  {
    "text": "If that makes sense. So for transformers, when you\nmap it like the input sequence",
    "start": "561650",
    "end": "566760"
  },
  {
    "text": "to a discrete-- or to a\ncontinuous value, really the reason why this works\nand the reason why people",
    "start": "566760",
    "end": "572390"
  },
  {
    "text": "do it is because it like\ntakes your 50,000 or whatever token space down to\n700, which is much more",
    "start": "572390",
    "end": "578870"
  },
  {
    "text": "amenable for computation. But you don't really\nhave to do it. So this is kind of an\narchitectural decision and has nothing to do with the\nfundamental modeling component",
    "start": "578870",
    "end": "587690"
  },
  {
    "text": "or probabilistic modeling part. OK, why can't we just embed the\ntokens into continuous space and do this type of we embed\nthe tokens in continuous space,",
    "start": "587690",
    "end": "595380"
  },
  {
    "text": "and then when we generate,\nwe just generate the values, and we discretize kind of. And actually, this\nis kind of a--",
    "start": "595380",
    "end": "602060"
  },
  {
    "text": "people actually do\nthis for some things. Like, in particular, we can take\na look at something like images.",
    "start": "602060",
    "end": "607129"
  },
  {
    "text": "For images, an image\nis-- we don't actually store like the whole\ncontinuous values of an image",
    "start": "607130",
    "end": "613462"
  },
  {
    "text": "because that would be\nimpossible in a computer. We only have finite [INAUDIBLE]. Generally speaking, we\ndiscretize it up to 0 to 255.",
    "start": "613462",
    "end": "621673"
  },
  {
    "text": "We have this discretized\nrepresentation. This is what we store as our\nquote-unquote \"ground truth",
    "start": "621673",
    "end": "627180"
  },
  {
    "text": "image.\" And the idea here is that\nfor our generative modeling-- what people do use for\na system like Stable",
    "start": "627180",
    "end": "633605"
  },
  {
    "text": "Diffusion or any\ngenerative model broadly is a 2-step procedure. First, you have your\ncontinuous generative model.",
    "start": "633605",
    "end": "640290"
  },
  {
    "text": "You generate a continuous image. And then you discretize\nit to the nearest value, and this becomes\nyour discrete image,",
    "start": "640290",
    "end": "646709"
  },
  {
    "text": "which is kind of what people\ndo generally speaking. So this is how they get around\nthe discrete nature of images",
    "start": "646710",
    "end": "653490"
  },
  {
    "text": "and use continuous models here. And the reason why\nthis works for images,",
    "start": "653490",
    "end": "658770"
  },
  {
    "text": "in particular, is because if we\nhave the images values 0, 255. This is a typical range\nfor an image pixel value.",
    "start": "658770",
    "end": "666330"
  },
  {
    "text": "And then we can just embed\nthis into some continuous space directly. So we just embed it on\nthe real number line.",
    "start": "666330",
    "end": "672430"
  },
  {
    "text": "And if we generate\nany one of these three different generations,\nwell, what we can do",
    "start": "672430",
    "end": "678000"
  },
  {
    "text": "is that we can just easily\ndiscretize because we just round to the nearest number. This is very simple. Now, if we have\nsomething like tokens,",
    "start": "678000",
    "end": "684930"
  },
  {
    "text": "let's say, for\nnatural language, OK, there's no way we embed this\ninto a continuous number line",
    "start": "684930",
    "end": "691320"
  },
  {
    "text": "like that. And generally, the\nway that people do this is like something\nvery high dimensional. Like this is two\ndimensions, people generally",
    "start": "691320",
    "end": "697830"
  },
  {
    "text": "do much higher\ndimensions than this when we try to do our embeddings. And if we try to generate\nstuff here and try",
    "start": "697830",
    "end": "704850"
  },
  {
    "text": "to generate and\ndiscretize something here, what you're going to\nend up with is like, OK, yeah, sometimes, your\ngenerations will be good.",
    "start": "704850",
    "end": "711600"
  },
  {
    "text": "OK, so if we have generally\ntokens like in the green X marks there, It's all good.",
    "start": "711600",
    "end": "716980"
  },
  {
    "text": "But if we are-- most of the space is empty. So we end up having a lot\nof empty room between tokens",
    "start": "716980",
    "end": "723060"
  },
  {
    "text": "that maybe it's possible\nthough to discretize it into a nearest neighbor token,\nbut it's kind of much more--",
    "start": "723060",
    "end": "729779"
  },
  {
    "text": "like it's much more like not-- it's much less obvious\nwhy this would work.",
    "start": "729780",
    "end": "735830"
  },
  {
    "text": "Sometimes, when we\ngo between tokens, it doesn't really make sense. And this is kind of a\nfundamentally hard problem in graph theory, is\nactually the reality.",
    "start": "735830",
    "end": "743390"
  },
  {
    "text": "Yeah, so this would work if\nyour model is like perfect. And it would work if\nyour model is perfect.",
    "start": "743390",
    "end": "749360"
  },
  {
    "text": "But in practice, this is kind\nof not the inductive bias we want to build into our model.",
    "start": "749360",
    "end": "754850"
  },
  {
    "text": "It typically makes it pretty\nhard to learn, for instance. So for something like, let's\nsay, a diffusion model.",
    "start": "754850",
    "end": "761120"
  },
  {
    "text": "And there's like language\ndiffusion models that do this exact same procedure. You continuously\nembed your tokens.",
    "start": "761120",
    "end": "767240"
  },
  {
    "text": "You take a diffusion\nprocess there. The issue that we'll\nsee for those models that we've seen for those models\nis that it's kind of they're",
    "start": "767240",
    "end": "773899"
  },
  {
    "text": "not really competitive with your\nstandard autoregressive model. And also, they take way\ntoo long because you can't",
    "start": "773900",
    "end": "780530"
  },
  {
    "text": "have any error whatsoever. If you have any\nerror whatsoever, you're just kind of lost. It doesn't work. For autoregressive modeling,\nwe model the probabilities,",
    "start": "780530",
    "end": "788490"
  },
  {
    "text": "which is a different quantity\nthan modeling the actual value. So we can model\nthe probabilities. So like, let's say, the\nprobability of the word \"the\"",
    "start": "788490",
    "end": "795620"
  },
  {
    "text": "or versus the probability\nof the word \"times.\" and this is like a very\ncontinuous quantity. But if we were to say like, hey,\nlet's just take a transformer.",
    "start": "795620",
    "end": "804180"
  },
  {
    "text": "We push it through a linear\nlayer, and you select one value. This would be the setup. We would select one value\nfrom your continuous space.",
    "start": "804180",
    "end": "812279"
  },
  {
    "text": "You don't have probabilities\nfor all the other tokens. You just select one value. This will become\nmore difficult. We",
    "start": "812280",
    "end": "817589"
  },
  {
    "text": "try to generate something\nthat's in distribution. So something near\none of these tokens. But yeah, this is the case.",
    "start": "817590",
    "end": "822900"
  },
  {
    "text": "Yeah, basically, we're\nmodeling a probabilistic model. We try to discretize. And this is just a\nlot of empty space. This is not a good inductive\nbias to learn over.",
    "start": "822900",
    "end": "830220"
  },
  {
    "text": "And so because of\nthese various issues, we only have one really good\ndiscrete probabilistic model.",
    "start": "830220",
    "end": "837930"
  },
  {
    "text": "You notice that's the\nautoregressive model, transformers, just very typical.",
    "start": "837930",
    "end": "843720"
  },
  {
    "text": "The idea here is\nthat instead of-- the idea here is you model\nthe probability of each of your sequences x by\ndecomposing it by token.",
    "start": "843720",
    "end": "851190"
  },
  {
    "text": "So you model the\nfirst token here. Then you take the second\ntoken given the first token. That's the next probability,\nand you just multiply it out",
    "start": "851190",
    "end": "858325"
  },
  {
    "text": "to get the probability\nof the last token given every token beforehand,\nwhich is your typical setup.",
    "start": "858325",
    "end": "865040"
  },
  {
    "text": "And, yeah. And for language\nin particular, this decomposes as the idea\nof context in green.",
    "start": "865040",
    "end": "871170"
  },
  {
    "text": "So you have a context tokens and\nyou have a next word prediction in purple basically.",
    "start": "871170",
    "end": "877370"
  },
  {
    "text": "And this is the reason\nwhy this works so well. And, yeah, there's\nseveral good upsides",
    "start": "877370",
    "end": "883430"
  },
  {
    "text": "to this autoregressive\nmodeling paradigm. So in particular,\nit's very scalable.",
    "start": "883430",
    "end": "888770"
  },
  {
    "text": "The idea here is\nthat when you compute each next token probability. You only need to compute a\nprobability over your D total",
    "start": "888770",
    "end": "897649"
  },
  {
    "text": "tokens or your n total values. And this is very scalable.",
    "start": "897650",
    "end": "903020"
  },
  {
    "text": "It's very easy to\ndo this as long as you can build your\narchitecture sufficiently good. This should work\nout pretty well.",
    "start": "903020",
    "end": "910589"
  },
  {
    "text": "Another thing is\nthat if you have a very strong neural network, if\nyou have a neural network that is sufficiently powerful,\nthen you can theoretically",
    "start": "910590",
    "end": "917250"
  },
  {
    "text": "represent any probability\nover your sequences by this decomposition,\nwhich is counterintuitive,",
    "start": "917250",
    "end": "922980"
  },
  {
    "text": "but it actually works itself\nout due to this decomposition nature. And finally, it's actually a\npretty reasonable inductive bias",
    "start": "922980",
    "end": "931200"
  },
  {
    "text": "for stuff like natural language. So for natural\nlanguage, we speak, we write from left to right. So it's pretty natural\nthat we would do",
    "start": "931200",
    "end": "938790"
  },
  {
    "text": "so here for modeling language. For modeling languages, this\nwill make sense as well.",
    "start": "938790",
    "end": "945810"
  },
  {
    "text": "There are several\ndownsides though, which have been\nlargely unaddressed by most people because\nof this overreliance",
    "start": "945810",
    "end": "952089"
  },
  {
    "text": "on autoregressive modeling. One famous argument that\npeople like Yann LeCun",
    "start": "952090",
    "end": "958060"
  },
  {
    "text": "have really been\nproponents of is the idea that sampling and autoregressive\nmodeling tends to drift.",
    "start": "958060",
    "end": "965205"
  },
  {
    "text": "So when you sample from an\nautoregressive sequence, you just generate new tokens,\nbut you can accumulate an error.",
    "start": "965205",
    "end": "970570"
  },
  {
    "text": "As you continuously\naccumulate the error, this will cause your\ngeneration to veer off course.",
    "start": "970570",
    "end": "976130"
  },
  {
    "text": "This is a very famous\nargument for why we're not going to get AGI\nthrough autoregressive modeling. And another issue is that\nfor non-language tasks",
    "start": "976130",
    "end": "984490"
  },
  {
    "text": "like, let's say, DNA sequence. Well, there's DNA sequencing. There's like no reason why DNA\nsequences have to be generated",
    "start": "984490",
    "end": "990400"
  },
  {
    "text": "from left to right. I mean, this is not very-- this doesn't make sense\nas an inductive bias.",
    "start": "990400",
    "end": "996760"
  },
  {
    "text": "Furthermore, and\nthis is something that people haven't\nreally been thinking of. But actually, when we have an\nautoregressive transformer,",
    "start": "996760",
    "end": "1003840"
  },
  {
    "text": "there's actually a\nlot of constraints that we need to place on our\nautoregressive transformer, in particular, making sure that\nthe attention mask is causal,",
    "start": "1003840",
    "end": "1011920"
  },
  {
    "text": "that people haven't really\nbeen like a cognizant of. But they are\ndefinitely still-- they are definitely still problems\nfor like this probabilistic",
    "start": "1011920",
    "end": "1019360"
  },
  {
    "text": "modeling paradigm. And finally, because\nwe sample iteratively,",
    "start": "1019360",
    "end": "1024790"
  },
  {
    "text": "in autoregressive models,\nwe generate next tokens. This is actually a\npretty slow technique.",
    "start": "1024790",
    "end": "1030692"
  },
  {
    "text": "Because of the fact that\nit's like rather iterative, you have to generate tokens one\nat a time, which is not great.",
    "start": "1030692",
    "end": "1038510"
  },
  {
    "text": "So yeah, we have all these\nproblems and benefits of autoregressive modeling. So the question that we\nneed to ask ourselves,",
    "start": "1038510",
    "end": "1045459"
  },
  {
    "text": "is there something more to this? And we can think about it in\nterms of score matching, which",
    "start": "1045460",
    "end": "1050890"
  },
  {
    "text": "I'm sure you're all aware of. And yeah, the key idea\nbehind why we can't just model p theta of x\ndirectly instead of why",
    "start": "1050890",
    "end": "1058360"
  },
  {
    "text": "we have to do this\nautoregressive decomposition is because p theta of\nx, we have to make sure that as we sum over all\nthe different sequences,",
    "start": "1058360",
    "end": "1065530"
  },
  {
    "text": "we have to sum up to be 1. And this is like\nimpossible because there's an exponential\nnumber of sequences that we need to sum over.",
    "start": "1065530",
    "end": "1072440"
  },
  {
    "text": "And so this is very similar\nto this idea of score matching that we've\njust been talking",
    "start": "1072440",
    "end": "1077980"
  },
  {
    "text": "about for the last\ncouple of lectures, where we model like this\ngradient log probability function. And when we do that,\nwe don't have to make--",
    "start": "1077980",
    "end": "1084590"
  },
  {
    "text": "when we do that, we\ndon't have to sum up over all the possible numbers. We don't have to integrate out--",
    "start": "1084590",
    "end": "1089740"
  },
  {
    "text": "we don't have to integrate\nout the distribution to be one, which tends\nto work pretty well with when you combine it with\nstuff like diffusion models.",
    "start": "1089740",
    "end": "1096640"
  },
  {
    "text": "So the real question\nhere and the thing that we'll talk about for\nthe rest of the lecture is how we can generalize these\ntechniques from score matching",
    "start": "1096640",
    "end": "1106430"
  },
  {
    "text": "to our more discrete case. And that is the real\nquestion of this lecture. Can we do it, and how\nwell does it work?",
    "start": "1106430",
    "end": "1112919"
  },
  {
    "text": "So let's take a look\nat the outline of how we can go about doing this. There's three steps.",
    "start": "1112920",
    "end": "1118577"
  },
  {
    "text": "The first is how do we\nextend score matching to discrete spaces? This is not a very well known--",
    "start": "1118577",
    "end": "1123620"
  },
  {
    "text": "this is a pretty\nwell-known problem, and there haven't been really\nmany good solutions have been proposed previously.",
    "start": "1123620",
    "end": "1130760"
  },
  {
    "text": "The next question\nis, that once we learn to score, which,\nin the discrete case, is called the concrete score.",
    "start": "1130760",
    "end": "1136010"
  },
  {
    "text": "How do we generate new\nsamples using concrete scores? And finally, when we build\nthis generative model",
    "start": "1136010",
    "end": "1142490"
  },
  {
    "text": "to generate new sequences,\ncan we evaluate likelihoods? And the reason why we want\nto evaluate likelihoods",
    "start": "1142490",
    "end": "1148070"
  },
  {
    "text": "is to compare fairly with\nautoregressive modeling on a lot of\nperplexity like tasks.",
    "start": "1148070",
    "end": "1154640"
  },
  {
    "text": "And so yeah, first\npoint, can we look at generalizing score\nmatching to discrete spaces?",
    "start": "1154640",
    "end": "1161549"
  },
  {
    "text": "So when we when we think\nabout the core building block of the score matching.",
    "start": "1161550",
    "end": "1168230"
  },
  {
    "text": "We really think\nabout the gradient. And the gradient actually has a\nnice way that we can build it.",
    "start": "1168230",
    "end": "1174720"
  },
  {
    "text": "There's a nice generalization\nof the gradient to a discrete space. So the idea here is that our\ngradient of a function of f when",
    "start": "1174720",
    "end": "1181340"
  },
  {
    "text": "we evaluate at a position x. This is actually like\na finite difference because a finite difference\nis the generalization",
    "start": "1181340",
    "end": "1187789"
  },
  {
    "text": "of a derivative if we assume\nthat a space is not continuous. So yeah, so this gradient\nbecomes of fy minus fx.",
    "start": "1187790",
    "end": "1196760"
  },
  {
    "text": "And we just index\nover all other y's. And this is like the\ngeneralization of a gradient.",
    "start": "1196760",
    "end": "1202610"
  },
  {
    "text": "And using this, we can\nbuild a generalization of a score function. So the score function\nis a gradient",
    "start": "1202610",
    "end": "1208270"
  },
  {
    "text": "at position x of\nthe log probability. And really what\nthis is a gradient",
    "start": "1208270",
    "end": "1213440"
  },
  {
    "text": "of the probability over the\nprobability when we do chain rule to get out the logarithm. And then when we\nsubstitute our definition",
    "start": "1213440",
    "end": "1220820"
  },
  {
    "text": "of our finite difference\ninstead of the gradient in this second line there,\nwhat we actually get",
    "start": "1220820",
    "end": "1227090"
  },
  {
    "text": "is that this is the collection\nof all py over px minus 1. So this py over px\nindexed over all y,",
    "start": "1227090",
    "end": "1235639"
  },
  {
    "text": "we'll learn this is\ncalled the concrete score. And this directly generalizes\nthe score function from continuous space to\ndiscrete space, which is nice.",
    "start": "1235640",
    "end": "1244620"
  },
  {
    "text": "But how do we learn this? All right, and, well,\nthere's one thing,",
    "start": "1244620",
    "end": "1249660"
  },
  {
    "text": "which is that for py over px,\nwhen we try to model all py's over px's, this will make a\nlot of sense computationally.",
    "start": "1249660",
    "end": "1258259"
  },
  {
    "text": "So in this case,\nfor y, if we just let y be any other neighbor of\nx, any other value that's not x,",
    "start": "1258260",
    "end": "1264429"
  },
  {
    "text": "then we end up with\nthis issue where we have the model too many quantities. O of n to the power of 2d\nexponential doesn't work.",
    "start": "1264430",
    "end": "1271279"
  },
  {
    "text": "But instead if we\nmodel these ratios,",
    "start": "1271280",
    "end": "1276340"
  },
  {
    "text": "we model the ratios\nbetween two sequences that differ by only one position. So instead, let's model the\nratios between any two sequences",
    "start": "1276340",
    "end": "1284000"
  },
  {
    "text": "that only differ at one point\nor at one position, which is a much more local construction.",
    "start": "1284000",
    "end": "1289700"
  },
  {
    "text": "And if we do this, this is only\ncomplexity of O of n times D. So this is much more like\ncomputationally feasible.",
    "start": "1289700",
    "end": "1298010"
  },
  {
    "text": "It's only the size\nof our sequence. And when we model these ratios\nbetween two sequences that",
    "start": "1298010",
    "end": "1304490"
  },
  {
    "text": "differ only at one\nposition, we can actually parameterize-- but like,\nwe'll normally write it out",
    "start": "1304490",
    "end": "1310340"
  },
  {
    "text": "with this first\nvalue, this py over px for all of our derivations. But all the derivations can\ngeneralize pretty easily",
    "start": "1310340",
    "end": "1317030"
  },
  {
    "text": "to this multi-dimensional\ncase here, just like as a precursor\nfor the rest of the talk.",
    "start": "1317030",
    "end": "1322100"
  },
  {
    "text": "That's because it's simpler\nto write it out this way. But, yeah. We can also model these\nratios pretty easily",
    "start": "1322100",
    "end": "1330740"
  },
  {
    "text": "with a neural network. So if we feed into the neural\nnetwork a sequence x1 to xd, we push it through a sequence\nto sequence neural network.",
    "start": "1330740",
    "end": "1338300"
  },
  {
    "text": "And then we can get\nanother sequence with another dimension\nattached to it like as such.",
    "start": "1338300",
    "end": "1344330"
  },
  {
    "text": "And we can have probability\nof 1x2 all the way to xd over probability of x1 over xd.",
    "start": "1344330",
    "end": "1350930"
  },
  {
    "text": "And we can directly\nmodel these ratios of all successive\nneighbors which differ only",
    "start": "1350930",
    "end": "1357740"
  },
  {
    "text": "at one point this way. Sequence to sequence, so you\njust go from this sequence. It's one dimensional sequence\nto a D dimensional-- or D",
    "start": "1357740",
    "end": "1364065"
  },
  {
    "text": "dimensional sequence. So we just push it through a\nneural network in parallel. So you can think about it as a\nnon-autoregressive Bert style",
    "start": "1364065",
    "end": "1370640"
  },
  {
    "text": "transformer. OK, so, yeah, this is the idea. We have a sequence\nto sequence model. We can generate the\nratios of places that",
    "start": "1370640",
    "end": "1378559"
  },
  {
    "text": "differ only at one position. So how do we learn it? This is a very obvious question.",
    "start": "1378560",
    "end": "1384289"
  },
  {
    "text": "How do we learn this concrete\nscore, and how do we do this?",
    "start": "1384290",
    "end": "1390060"
  },
  {
    "text": "So yeah, our goal\nhere is to learn a neural network s theta x. Such that when we parameterize\ns theta of x at a position y,",
    "start": "1390060",
    "end": "1397140"
  },
  {
    "text": "then we can get the\nrelative ratio py over px. And we need to find\na way to do this",
    "start": "1397140",
    "end": "1404240"
  },
  {
    "text": "in a very principled manner. As in we can't allow like\nnegative values for our s theta.",
    "start": "1404240",
    "end": "1409280"
  },
  {
    "text": "And also, when we\nhave enough data, we should also be able to\nrecover a ground truth. Enough data, enough\nmodel capacity.",
    "start": "1409280",
    "end": "1415760"
  },
  {
    "text": "And so the way\nthat we do this is very similar to score matching,\nwhich is the following loss function.",
    "start": "1415760",
    "end": "1422820"
  },
  {
    "text": "We're calling it score\nentropy because it's based off of-- it's very related\nto stuff like cross entropy.",
    "start": "1422820",
    "end": "1428659"
  },
  {
    "text": "But I guess the idea\nhere is that it's very-- the idea here is that it's a\ndiscrete generalization of score",
    "start": "1428660",
    "end": "1434390"
  },
  {
    "text": "matching in the\nsense that we first sample-- we take an integral\nover all x and our probability",
    "start": "1434390",
    "end": "1439760"
  },
  {
    "text": "function p. And then we sum over all of\nour y that are neighbors. And then we minimize\nthis type of new type",
    "start": "1439760",
    "end": "1447650"
  },
  {
    "text": "of divergence\nfunction in the middle here in order to optimize it. And, yeah, the reason why\nthis is in such a way,",
    "start": "1447650",
    "end": "1455850"
  },
  {
    "text": "that we'll see it,\nwe'll see why we need to do this type\nof construction soon.",
    "start": "1455850",
    "end": "1461192"
  },
  {
    "text": "But the idea here is that\nwe have this score entropy. It's a generalization\nof score matching. But for our discrete\nscores instead.",
    "start": "1461192",
    "end": "1468410"
  },
  {
    "text": "And you might not believe me,\nbut actually, this score entropy function actually does\nrecover our ground truth.",
    "start": "1468410",
    "end": "1474480"
  },
  {
    "text": "So if we just get rid\nof all the x and y, if we simplify our\nnotation a bit, and we want to minimize\nthe following quantity.",
    "start": "1474480",
    "end": "1482159"
  },
  {
    "text": "Well, this quantity is minimized\nwhen our derivative-- we just set the derivative to be 0.",
    "start": "1482160",
    "end": "1488630"
  },
  {
    "text": "And we get this one\nminus py over px times 1 over s equal to 0.",
    "start": "1488630",
    "end": "1494030"
  },
  {
    "text": "And then we move it back over. We multiply it out,\nand we get that. When we minimize\nit correctly, it's",
    "start": "1494030",
    "end": "1499640"
  },
  {
    "text": "this s value should be\nequal to py over px. And we can also visualize the\nloss function for something",
    "start": "1499640",
    "end": "1506180"
  },
  {
    "text": "like a ground\ntruth ratio of 0.2. And we can clearly\nsee that it satisfies all of our requirements.",
    "start": "1506180",
    "end": "1511820"
  },
  {
    "text": "Basically, it's convex. And it will recover the true\nvalue if we just minimize this.",
    "start": "1511820",
    "end": "1517850"
  },
  {
    "text": "And finally, we can do this\nbetween for all pairs x and y. So we can just do\nthis independently",
    "start": "1517850",
    "end": "1523870"
  },
  {
    "text": "for all of our x and y. Which means that if we\nlearn everything correctly, we should recover the\nground truth for every pair",
    "start": "1523870",
    "end": "1530920"
  },
  {
    "text": "x and y basically. So, yeah. We have this score\nentropy loss function,",
    "start": "1530920",
    "end": "1537410"
  },
  {
    "text": "and how do we\nactually optimize it? Very similarly to\nscore matching. Here's the issue, right,\nwe have this loss function,",
    "start": "1537410",
    "end": "1544670"
  },
  {
    "text": "but this py over px\nis ground truth value, is not known to us at all. I mean, if we knew it,\nwe could just use it.",
    "start": "1544670",
    "end": "1550820"
  },
  {
    "text": "So it would make\nsense that we have to find a different way around\nthis in order to learn it.",
    "start": "1550820",
    "end": "1556279"
  },
  {
    "text": "And we have two\ndifferent ways of doing this, one of these\nalternative loss functions,",
    "start": "1556280",
    "end": "1563480"
  },
  {
    "text": "which we're calling\nimplicit score entropy. This is a natural generalization\nof implicit score matching. But we won't be covering\nit in this lecture.",
    "start": "1563480",
    "end": "1569870"
  },
  {
    "text": "But just nice to\nknow that exists. And we also have\nanother loss function called denoising score entropy\nor denoising score entropy.",
    "start": "1569870",
    "end": "1576950"
  },
  {
    "text": "And this is analogous to\ndenoising score matching for our score entropy case.",
    "start": "1576950",
    "end": "1583280"
  },
  {
    "text": "The look at denoising\nscore entropy. Here's the idea. If we assume that our px\nis equal to a convolution",
    "start": "1583280",
    "end": "1591740"
  },
  {
    "text": "between a base distribution\np0 and some kernel p,",
    "start": "1591740",
    "end": "1597929"
  },
  {
    "text": "well, then, we can write\nout our probability in this summation over all x0.",
    "start": "1597930",
    "end": "1603570"
  },
  {
    "text": "And when we do that, we can\ntake a look at our initial score entropy loss.",
    "start": "1603570",
    "end": "1609150"
  },
  {
    "text": "And then yeah, the idea here\nis that we can just remove the expectation as first.",
    "start": "1609150",
    "end": "1614460"
  },
  {
    "text": "So we instead, we\njust move in px, and this gets rid of the\npx and the denominator,",
    "start": "1614460",
    "end": "1619470"
  },
  {
    "text": "but the summation over all x. Then in order to look at\nthings more concretely,",
    "start": "1619470",
    "end": "1626460"
  },
  {
    "text": "we take a look at this\ndecomposition above. And we just apply\nit to this py term to get out this following\ndecomposition, which",
    "start": "1626460",
    "end": "1633510"
  },
  {
    "text": "is basically, we add in\nan expectation over x0.",
    "start": "1633510",
    "end": "1639570"
  },
  {
    "text": "We can basically move\naround our values a bit. So we can move the\nsummation term to the front by Fubini theorem.",
    "start": "1639570",
    "end": "1645630"
  },
  {
    "text": "And we can also add\nin a p of x given x0-- all over p of x given x0\nso we can rework it here.",
    "start": "1645630",
    "end": "1655750"
  },
  {
    "text": "And then once we have\neverything in this setup, then we can basically just\ntake the last two terms",
    "start": "1655750",
    "end": "1661830"
  },
  {
    "text": "into our expectation. We just move those terms, take\nthem away from the summation, and move it into\nour expectation.",
    "start": "1661830",
    "end": "1667800"
  },
  {
    "text": "And this gives us\nan equivalent form. And the nice thing that we'll\nnotice for this equivalent form here is that we only\nhave this relative ratio of p",
    "start": "1667800",
    "end": "1677160"
  },
  {
    "text": "of y given x0 over p of x given\nx0, not p of y over p of x.",
    "start": "1677160",
    "end": "1682500"
  },
  {
    "text": "And as such, this is\npossible to compute. And this is something that's\npossible to compute because we",
    "start": "1682500",
    "end": "1689220"
  },
  {
    "text": "can assume that our transition\nkernel p is tractable, but we can't assume that our\ndata distribution p is tractable",
    "start": "1689220",
    "end": "1697200"
  },
  {
    "text": "basically. This x0 is kind of\nyour base data point. And this transition kernel,\nit can be anything basically.",
    "start": "1697200",
    "end": "1704520"
  },
  {
    "text": "It can be anything. So much more than just\nthis type of noise. But in the continuous\ncase, you would also",
    "start": "1704520",
    "end": "1710340"
  },
  {
    "text": "write out like this but\nfor practical reasons. That's why people choose\nto use a small Gaussian",
    "start": "1710340",
    "end": "1718320"
  },
  {
    "text": "addition in order to do\nthis same exact thing. But this is basically\nthe same exact thing. So we have this way to\nget rid of the y over px.",
    "start": "1718320",
    "end": "1728130"
  },
  {
    "text": "And as such, we have the\nfollowing denoising score entropy loss function. And it's particularly scalable\nbecause we can sample an x0.",
    "start": "1728130",
    "end": "1736380"
  },
  {
    "text": "We can sample through this-- we can sample through\nthe perturbation kernel. And then we only need to\ncompute this s theta of x",
    "start": "1736380",
    "end": "1743039"
  },
  {
    "text": "once for this summation\nvalue because we're only using theta of x.",
    "start": "1743040",
    "end": "1748260"
  },
  {
    "text": "And then, finally,\nwe can compute this ratio of transition\nkernels by just the way",
    "start": "1748260",
    "end": "1754290"
  },
  {
    "text": "that we define it. So everything becomes\ncomputationally tractable, and we can optimize this loss.",
    "start": "1754290",
    "end": "1760940"
  },
  {
    "text": "So now, we have a way of\nlearning the concrete score, the next question is, how can we\nsample using a concrete score? We have this way of\nestimating the concrete score,",
    "start": "1760940",
    "end": "1769669"
  },
  {
    "text": "learning the ratio of\nthe data distribution, how do we generate new samples? And this is really\ndiffusion oriented.",
    "start": "1769670",
    "end": "1776640"
  },
  {
    "text": "So in order to do\nthis, we have to define a diffusion process for our\ndiscrete tokens kind of.",
    "start": "1776640",
    "end": "1785120"
  },
  {
    "text": "And as we all know, diffusion is\njust a probabilistic evolution, a way to go from p0 to some pt.",
    "start": "1785120",
    "end": "1791330"
  },
  {
    "text": "So we can work off of\nthis direction directly. Our pt now is just a big vector.",
    "start": "1791330",
    "end": "1796580"
  },
  {
    "text": "We can think about\nit as a big vector. So because each\nof our probability",
    "start": "1796580",
    "end": "1802280"
  },
  {
    "text": "at a certain sequence\nis basically just like some number that is\ngreater than or equal to 0 and everything sums up to be 1.",
    "start": "1802280",
    "end": "1807980"
  },
  {
    "text": "So we can think about it\nas a big vector kind of. And in a way that we\nevolve our distribution",
    "start": "1807980",
    "end": "1814730"
  },
  {
    "text": "is with an ordinary\ndifferential equation. This is the most natural\nway of doing things. So our pt is a vector.",
    "start": "1814730",
    "end": "1821270"
  },
  {
    "text": "We take a time derivative\nwith respect to that. And then we can compute\nthe transition based off",
    "start": "1821270",
    "end": "1826640"
  },
  {
    "text": "of this like matrix Qt\ntimes our initial vector pt.",
    "start": "1826640",
    "end": "1832230"
  },
  {
    "text": "So we do a matrix\nvector multiplication. And some things about\nthis diffusion matrix",
    "start": "1832230",
    "end": "1838620"
  },
  {
    "text": "that are not obvious but these\nare just hard requirements. We need to make sure that\nthis diffusion matrix has",
    "start": "1838620",
    "end": "1845400"
  },
  {
    "text": "columns which sum up to be 0. And also, we need to make sure\nthat this diffusion matrix is non-negative at all, like\nnon-diagonal points basically.",
    "start": "1845400",
    "end": "1855010"
  },
  {
    "text": "And the idea here\nis that Qt controls how often we go if we jump\nfrom one state to another.",
    "start": "1855010",
    "end": "1860050"
  },
  {
    "text": "And we can do this\npretty directly here. So basically, if we want\nto jump from a state i",
    "start": "1860050",
    "end": "1865289"
  },
  {
    "text": "to state j over a period\nof delta t, then basically, we just take a look\nat whether or not",
    "start": "1865290",
    "end": "1870837"
  },
  {
    "text": "we stay at the current\nfunction, and then we just add the following\nmatrix term times delta t. And then we have some\nsecond order term",
    "start": "1870837",
    "end": "1877410"
  },
  {
    "text": "that we get rid of for\npractical purposes. This is kind of like the\nanalog of Euler-Maruyama",
    "start": "1877410",
    "end": "1883139"
  },
  {
    "text": "sampling for diffusion models,\nthis time discretization of our sampling process.",
    "start": "1883140",
    "end": "1888820"
  },
  {
    "text": "And so we clearly see\nhere that our Qt is-- these matrix entries are the\njump transition rates between i",
    "start": "1888820",
    "end": "1896590"
  },
  {
    "text": "to j kind of. And so, once we\nhave this set up,",
    "start": "1896590",
    "end": "1902540"
  },
  {
    "text": "let's take a look at\na couple of examples. This is not a very\nintuitive thing. But let's take a look\nat the following Qt.",
    "start": "1902540",
    "end": "1908780"
  },
  {
    "text": "Our Qt is given by this matrix,\nthis negative 2, negative 2, negative 2 on the\ndiagonal 1, 1, 1, every 1",
    "start": "1908780",
    "end": "1914210"
  },
  {
    "text": "everywhere else matrix. And let's say we take an initial\ndistribution of 0.5, 0.2, 0.3.",
    "start": "1914210",
    "end": "1921620"
  },
  {
    "text": "When we multiply this stuff out. We get a transition rate\nof negative 0.5, 0.1, 0.4.",
    "start": "1921620",
    "end": "1929510"
  },
  {
    "text": "And what's\ninteresting about this is that the values sum up to be\n0, which is important in order",
    "start": "1929510",
    "end": "1935422"
  },
  {
    "text": "to maintain the fact that our\nprobability is always sums up to be 1. And also, it's always\na valid transition rate",
    "start": "1935422",
    "end": "1942440"
  },
  {
    "text": "between different states. For this type of\nsetup, we can actually",
    "start": "1942440",
    "end": "1947930"
  },
  {
    "text": "compute the intermediate\ndensities pt by just exponentiating out this\nmatrix times this 0.5, 0.3,",
    "start": "1947930",
    "end": "1953825"
  },
  {
    "text": "0.2 initial vector,\nwhich allows us to compute intermediate\ndensities by solving",
    "start": "1953825",
    "end": "1958909"
  },
  {
    "text": "the ODE basically. And if we do this,\nwe can actually",
    "start": "1958910",
    "end": "1964919"
  },
  {
    "text": "also check to make sure that the\ntransition actually satisfies this above statement basically.",
    "start": "1964920",
    "end": "1970770"
  },
  {
    "text": "Or for the first value,\nyou're losing mass at a rate of negative 0.5.",
    "start": "1970770",
    "end": "1975990"
  },
  {
    "text": "And then the other two are\ngaining mass at 0.1, 0.4. So the total mass\nremains the same, but like the relative\nratios change.",
    "start": "1975990",
    "end": "1983370"
  },
  {
    "text": "Building off of that,\nbasically generally speaking, we'll take a Qt is equal to a\nnoise level times a Q matrix.",
    "start": "1983370",
    "end": "1992549"
  },
  {
    "text": "And then once we have that,\nthis becomes a linear ODE. Everything linearizes. We have a linear ODE.",
    "start": "1992550",
    "end": "1998730"
  },
  {
    "text": "And in order to solve\nbasically just very general, we can solve the\nintermediate densities by doing this matrix\nexponentiation in order",
    "start": "1998730",
    "end": "2007010"
  },
  {
    "text": "to solve the linear ODE. Basically here, in\nmany ways, compute this like exponentiation.",
    "start": "2007010",
    "end": "2012110"
  },
  {
    "text": "But simpler is better. And the idea here is that we can\ncalculate the transition rates with a long horizon transition\nrates by taking column and--",
    "start": "2012110",
    "end": "2021810"
  },
  {
    "text": "by taking entries of our\nexponentiated matrix basically.",
    "start": "2021810",
    "end": "2027180"
  },
  {
    "text": "So, yeah, this is great. And another thing\nthat's also important for diffusion is that as\nt goes to infinity or pt",
    "start": "2027180",
    "end": "2035160"
  },
  {
    "text": "will go to p base basically. So this is just making sure\nthat we approach a nice base",
    "start": "2035160",
    "end": "2040500"
  },
  {
    "text": "distribution basically. I guess other thing to-- I mean, in this case, we can\ntake a look at the following",
    "start": "2040500",
    "end": "2046980"
  },
  {
    "text": "matrix as negative 2 matrix. We exponentiate out\nwith respect to some t, and we get this thing basically.",
    "start": "2046980",
    "end": "2053460"
  },
  {
    "text": "It's not as bad as it looks. And then as we go\nto infinite time, we just go to a random\nvalue basically.",
    "start": "2053460",
    "end": "2060000"
  },
  {
    "text": "So this is a uniform\ntransition matrix. We just go from an initial\npoint to any other point randomly eventually.",
    "start": "2060000",
    "end": "2066989"
  },
  {
    "text": "Similarly, we have\nthis masking thing where we add a new dimension. We add a new dimension to\nour three-dimensional case.",
    "start": "2066989",
    "end": "2073480"
  },
  {
    "text": "And basically, we only have\ntransitions to this new state. And our exponentiated\nmatrix looks like this.",
    "start": "2073480",
    "end": "2080609"
  },
  {
    "text": "And as we take infinite\ntime, the diagonal disappears and everything goes\nto mask basically.",
    "start": "2080610",
    "end": "2086460"
  },
  {
    "text": "This is a mask transition. Well, the first case\nbasically the idea here is you just randomly go\nfrom your initial value",
    "start": "2086460",
    "end": "2093929"
  },
  {
    "text": "to any other random value. And in the second\ncase is you randomly go from your initial value\nto a mask value basically.",
    "start": "2093929",
    "end": "2101369"
  },
  {
    "text": "It just determines\nwhere you're moving. We have this continuous\ntime Markov chain setup.",
    "start": "2101370",
    "end": "2107660"
  },
  {
    "text": "And generally, we're\nlooking at sequences. The idea here is a set\nof-- coming from sequence to sequence, which would be\nvery expensive because we have",
    "start": "2107660",
    "end": "2115490"
  },
  {
    "text": "to consider the transitions\nbetween in our sequence to any other sequence.",
    "start": "2115490",
    "end": "2121050"
  },
  {
    "text": "And this is a\ncomputationally intractable. We instead like go\nfrom token to token. So instead, we just\nflip one token at a time",
    "start": "2121050",
    "end": "2127760"
  },
  {
    "text": "would be the idea. And as such, this is O of\nd squared because we only",
    "start": "2127760",
    "end": "2133339"
  },
  {
    "text": "have to consider one token. And because of this, when\nwe do our overall transition",
    "start": "2133340",
    "end": "2138770"
  },
  {
    "text": "between sequences, this\nbecomes the overall transitions between tokens. So it factorizes basically. This is just like\nanother point there.",
    "start": "2138770",
    "end": "2148650"
  },
  {
    "text": "And what's nice\nabout this is that we can change this with\nour score entropy to estimate the\nintermediate density ratios.",
    "start": "2148650",
    "end": "2156420"
  },
  {
    "text": "So if we assume that\nour samples are from-- assume we have some\nsamples x0 given from p0,",
    "start": "2156420",
    "end": "2162100"
  },
  {
    "text": "then we can learn our s theta. We now add a t value\nin order to estimate",
    "start": "2162100",
    "end": "2167690"
  },
  {
    "text": "the pt values, PT ratios. We have another extra t input,\nbut it's the same setup.",
    "start": "2167690",
    "end": "2174680"
  },
  {
    "text": "And now, we have our denoising\nscore entropy loss function. And the idea here is now, we can\ntake these transition values,",
    "start": "2174680",
    "end": "2183750"
  },
  {
    "text": "this transition between\ntwo different states. This is all given by our\ninitial rate matrix Q basically.",
    "start": "2183750",
    "end": "2190760"
  },
  {
    "text": "So more or less,\nwhat this is saying is that we can\noptimize our denoising score entropy\nusing this Q setup,",
    "start": "2190760",
    "end": "2198170"
  },
  {
    "text": "using this diffusion setup. It's all very natural. And so the question here\nis that, OK, now, we",
    "start": "2198170",
    "end": "2204400"
  },
  {
    "text": "have a way of going\nfrom data to noise. We also have a way of estimating\nthe intermediate ratios.",
    "start": "2204400",
    "end": "2209950"
  },
  {
    "text": "What can we do with this? Well, the idea here is we can\nreverse the diffusion process.",
    "start": "2209950",
    "end": "2215030"
  },
  {
    "text": "So if we go from p0 to pt, which\nis p data to p base roughly speaking, the idea here is,\ncan we go back from p base",
    "start": "2215030",
    "end": "2223869"
  },
  {
    "text": "back to p data? And actually, there is\na way of doing this. So there is this other type of\ndiffusion, reverse diffusion",
    "start": "2223870",
    "end": "2231370"
  },
  {
    "text": "process where, basically,\nwe take the time derivative. But in here, we're\ngoing backwards in time.",
    "start": "2231370",
    "end": "2237590"
  },
  {
    "text": "And we have a new matrix Q bar,\nwhich is a bit more different. And the idea behind Q\nbar is that Q bar is--",
    "start": "2237590",
    "end": "2245800"
  },
  {
    "text": "an input j and i this is\nequal to the density ratio pt of j over pt of i and times\nthis initial Qtij for any i",
    "start": "2245800",
    "end": "2254380"
  },
  {
    "text": "and j not equal. Basically, we have this\nfollowing relationship between the forward and\nreverse diffusion matrices,",
    "start": "2254380",
    "end": "2261810"
  },
  {
    "text": "which is pretty neat. And also, I guess the\nother thing to note here-- I won't write it out-- is that\nfor Qti of i or bar QTi of i.",
    "start": "2261810",
    "end": "2269820"
  },
  {
    "text": "I'm not going to write\nthat out because you just need to make sure\nthe columns sum to 0. So we just assume\nit's just some--",
    "start": "2269820",
    "end": "2276710"
  },
  {
    "text": "we can extract it from the\nother values basically. It's i and j represents\nan index basically.",
    "start": "2276710",
    "end": "2282230"
  },
  {
    "text": "So, I mean, for our purposes\nwill be like a sequence, but this is kind of\nhard to write out. But you can think about in\nmatrix, you just take the--",
    "start": "2282230",
    "end": "2289760"
  },
  {
    "text": "matrix and vector,\nso the matrix, you just take the jth\nrow ith column entry.",
    "start": "2289760",
    "end": "2296060"
  },
  {
    "text": "And then you take the\nratio between the two corresponding entries\nin the vector, which",
    "start": "2296060",
    "end": "2302300"
  },
  {
    "text": "is the probability vector. But, yeah. So, yeah, we have\nthis reverse setup. And again, what's\nnice is that we",
    "start": "2302300",
    "end": "2309650"
  },
  {
    "text": "have this appearance\nof our ratio basically of our concrete score.",
    "start": "2309650",
    "end": "2316755"
  },
  {
    "text": "So in particular, we\ncan approximate it with our learned concrete\nscore network as theta.",
    "start": "2316755",
    "end": "2322009"
  },
  {
    "text": "And this goes back\nto the reason why we like parametrize\neverything this way",
    "start": "2322010",
    "end": "2327849"
  },
  {
    "text": "is that the way that we do it\nis that we have initial state i, and then we basically compute\nthe concrete score of s theta",
    "start": "2327850",
    "end": "2335000"
  },
  {
    "text": "it. And this goes over all\nthe various j indices. And if we do this, it\nallows us to, in parallel,",
    "start": "2335000",
    "end": "2341410"
  },
  {
    "text": "jump to any of the\nother states that we want to jump in because of the\nway that we parameterize things.",
    "start": "2341410",
    "end": "2346720"
  },
  {
    "text": "So everything kind of works\nits way together in this setup.",
    "start": "2346720",
    "end": "2349810"
  },
  {
    "text": "As an example, we can have\nthis initial matrix here. We multiply it out. This is the rate,\nnegative 0.5, 0.1, 0.4.",
    "start": "2352770",
    "end": "2359880"
  },
  {
    "text": "And then we can construct like\nthe corresponding reverse matrix here. This reverse matrix,\nif you work itself out,",
    "start": "2359880",
    "end": "2366930"
  },
  {
    "text": "it looks something\nlike this basically, where we add in the ratios of\nthe data vector at the time.",
    "start": "2366930",
    "end": "2374460"
  },
  {
    "text": "And then we multiply\nthis reverse matrix by this probability vector. And actually, what you'll\nget out is the exact reverse.",
    "start": "2374460",
    "end": "2382170"
  },
  {
    "text": "It's the exact reverse, the\n0.5, negative 0.1, negative 0.4. So here, we can see that\njust it works basically.",
    "start": "2382170",
    "end": "2391200"
  },
  {
    "text": "And as an example,\nwe can visualize as follows between the\nuniform where we basically just go with two\nother random values,",
    "start": "2391200",
    "end": "2397650"
  },
  {
    "text": "and eventually, denoises\nto some initial sequence. And also, we have\nit for the mask",
    "start": "2397650",
    "end": "2403319"
  },
  {
    "text": "basically, where we can go from\nmask to our initial tokens.",
    "start": "2403320",
    "end": "2408450"
  },
  {
    "text": "So this is all pretty nice,\nand we have this nice setup. And, well, there's\nonly one other problem",
    "start": "2408450",
    "end": "2415089"
  },
  {
    "text": "that we kind of have,\nwhich is basically, when we try to actually\ndo this reverse sampling,",
    "start": "2415090",
    "end": "2420160"
  },
  {
    "text": "when we try to go\nthrough the various-- when we try to simulate the\nreverse, it's pretty slow.",
    "start": "2420160",
    "end": "2425740"
  },
  {
    "text": "The reason why it's\nso slow is because we are jumping from-- this is\nall fundamentally comes down",
    "start": "2425740",
    "end": "2431320"
  },
  {
    "text": "to our computational\nconsideration. Basically, our x1\nto xd, we're only",
    "start": "2431320",
    "end": "2437500"
  },
  {
    "text": "jumping between that and\nanother sequence which only differs at one\npoint or at one position.",
    "start": "2437500",
    "end": "2443869"
  },
  {
    "text": "And so when we\nconstruct a reverse, we can only also jump between\nsequences that differ only",
    "start": "2443870",
    "end": "2450130"
  },
  {
    "text": "by one position,\nwhich you can imagine would be very expensive,\nespecially if you need a jump, if you to continuously refine\nthe individual position",
    "start": "2450130",
    "end": "2458750"
  },
  {
    "text": "like as such. And so we cheat basically,\nand that's how we sample. We basically allow\nmultiple steps within--",
    "start": "2458750",
    "end": "2466090"
  },
  {
    "text": "we allow one to sample multiple\njumps in one sample step basically.",
    "start": "2466090",
    "end": "2471880"
  },
  {
    "text": "So instead of going individually\nunmasking the tokens, let's say, it was the mask of mask.",
    "start": "2471880",
    "end": "2477860"
  },
  {
    "text": "We just unmask both of\nthese tokens simultaneously. We can do this pretty\neasily given our setup.",
    "start": "2477860",
    "end": "2483740"
  },
  {
    "text": "But it's more or less kind\nof a way we can do this. And instead of sample,\nit was the best",
    "start": "2483740",
    "end": "2490010"
  },
  {
    "text": "of times in one step\nallowing us to go through two different jumps at once.",
    "start": "2490010",
    "end": "2496832"
  },
  {
    "text": "So, yeah, we can put\neverything together. We have an entire setup\nbuilt. So the first idea",
    "start": "2496832",
    "end": "2502197"
  },
  {
    "text": "is that we get some samples from\nour desired data distribution that we want the model. We define a forward\ndiffusion process,",
    "start": "2502197",
    "end": "2508660"
  },
  {
    "text": "whether it be like the\nuniform or the mask or whatever or\nmaybe something more exotic of these for diffusion\nprocess given the transitions.",
    "start": "2508660",
    "end": "2518100"
  },
  {
    "text": "And then we can learn the ratios\nusing our score entropy loss function that we've defined.",
    "start": "2518100",
    "end": "2525000"
  },
  {
    "text": "And then we can use these\nratios to reverse the diffusion process, including adding\nand some discretization",
    "start": "2525000",
    "end": "2530970"
  },
  {
    "text": "to make sampling faster. And let's see how this works. So this is an example\nof a text sequence",
    "start": "2530970",
    "end": "2537690"
  },
  {
    "text": "that we were able to generate\njust randomly from our corpus. This is a GPT-2 level sampling\nprocedure or GPT-2 level dataset",
    "start": "2537690",
    "end": "2548670"
  },
  {
    "text": "and model size. And yeah, it's reasonably\ncoherent, and everything is--",
    "start": "2548670",
    "end": "2555970"
  },
  {
    "text": "it works. That's kind of an idea. It works. But the idea here\nis like how does it compare with\nautoregressive modeling",
    "start": "2555970",
    "end": "2562480"
  },
  {
    "text": "on the scale of dataset? So we can compare\nlike samples as such.",
    "start": "2562480",
    "end": "2568000"
  },
  {
    "text": "And we have a GPT-2. We're calling our model score\nentropy discrete diffusion, so S-E-D-D, SEDD.",
    "start": "2568000",
    "end": "2573070"
  },
  {
    "text": "And so we have the\nGPT-2 model at the top. We have a SEDD model with an\nabsorbing transition, which is",
    "start": "2573070",
    "end": "2579549"
  },
  {
    "text": "like you go to the mask token. And we have a set with a\nuniform transition SEDD-U, which",
    "start": "2579550",
    "end": "2587080"
  },
  {
    "text": "means that you go from your\ntoken to another random token whenever you transition. And generally, we're able\nto see that our S-E-D-D,",
    "start": "2587080",
    "end": "2595480"
  },
  {
    "text": "SEDD models tend to outperform\nGPT-2 in terms of coherence when we do this baseline\nsampling method,",
    "start": "2595480",
    "end": "2601750"
  },
  {
    "text": "when we try to sample\nfrom the distribution. And we can also\nvisualize this more like as a function of number of\nsampling steps versus quality.",
    "start": "2601750",
    "end": "2610960"
  },
  {
    "text": "So like in this\ngraph on the right here, we have our\ntwo GPT-2 models.",
    "start": "2610960",
    "end": "2617580"
  },
  {
    "text": "And if we try to generate\nout long sequences, it tends to look something like\nthis where we generate out--",
    "start": "2617580",
    "end": "2622660"
  },
  {
    "text": "it takes 1,024 network\nevaluation functional evaluations in order to\ngenerate one of these outputs.",
    "start": "2622660",
    "end": "2629200"
  },
  {
    "text": "And yeah, it tends\nto be pretty high. When we feed in these generated\nsequences into another larger",
    "start": "2629200",
    "end": "2637420"
  },
  {
    "text": "model, they tend to say,\nhey, these sequences are very high perplexity. These sequences are\nvery low likelihood.",
    "start": "2637420",
    "end": "2644010"
  },
  {
    "text": "These sequences don't\nmake sense basically. So we can see here that like\nas GPT-2 tends to pretty--",
    "start": "2644010",
    "end": "2651339"
  },
  {
    "text": "it tends to be pretty bad\nin terms of our evaluation as such, even from both\nthe small and the medium.",
    "start": "2651340",
    "end": "2656859"
  },
  {
    "text": "But these lines, which are\nour SEDD models basically, we can kind of trade off the\ncompute versus the number",
    "start": "2656860",
    "end": "2663220"
  },
  {
    "text": "of-- we can try to trade off\nquality and compute basically. So if we only take\n64 steps, which",
    "start": "2663220",
    "end": "2668770"
  },
  {
    "text": "means that we're doing\na lot of discretization, we take a lot of\nsimultaneous jumps, we end up with this model\nthat matches GPT-2 in terms",
    "start": "2668770",
    "end": "2677650"
  },
  {
    "text": "of its generated quality. But it's much faster basically. And also, if we really crank up\nthe number of iteration steps.",
    "start": "2677650",
    "end": "2686260"
  },
  {
    "text": "So we take, let's\nsay 1,024 or even 2,048 sampling\ndiscretization steps, what we see here is\nthat our quality gets",
    "start": "2686260",
    "end": "2693280"
  },
  {
    "text": "progressively better\nand better in a log log linear type of fashion. So basically, we're able to\ngenerate sequences that are just",
    "start": "2693280",
    "end": "2701079"
  },
  {
    "text": "significantly lower in terms\nof generative perplexity, which means that they're much better\nsequences if we just crank up",
    "start": "2701080",
    "end": "2707680"
  },
  {
    "text": "the number of steps. We can't do it with GPT-3\nor GPT-4 mostly because of model size. Like in this case,\nour model sizes",
    "start": "2707680",
    "end": "2714460"
  },
  {
    "text": "are pretty small, like\n100 million parameters, 400 million parameters. So we're matching\nthe models by color.",
    "start": "2714460",
    "end": "2721809"
  },
  {
    "text": "So blue models are small. Orange models are medium. For GPT-3 and GPT-4,\nthe other issue is that the dataset\nis private basically.",
    "start": "2721810",
    "end": "2728740"
  },
  {
    "text": "But for our GPT-2, the dataset\nis like web text or open web text, which is why we can do\nan apples to apples comparison.",
    "start": "2728740",
    "end": "2738060"
  },
  {
    "text": "But, yeah. So the conclusion here is that,\nyeah, so quite surprisingly and pretty nice, and this is a\npretty strong motivating factor",
    "start": "2738060",
    "end": "2748320"
  },
  {
    "text": "is that this discrete diffusion\nmodel with score entropy tends to outperform\nautoregressive transformers,",
    "start": "2748320",
    "end": "2754110"
  },
  {
    "text": "at least for generation\nquality and speed. And I guess another\ninteresting thing, and this is another\nimportant thing",
    "start": "2754110",
    "end": "2759210"
  },
  {
    "text": "is that for this type\nof generative modeling technique, what we need\nto do is we need to have",
    "start": "2759210",
    "end": "2764280"
  },
  {
    "text": "controllable generation. We need to be able to\ncontrol how we generate. And at least, in this case,\nwe can do something similar.",
    "start": "2764280",
    "end": "2771630"
  },
  {
    "text": "We can do prompting. But the new and\ninteresting thing is that we can prompt from\nan arbitrary location.",
    "start": "2771630",
    "end": "2777490"
  },
  {
    "text": "So if we have this\ntop one here, we can take our blue prompt text. And the idea here is that when\nwe generate our new sequence,",
    "start": "2777490",
    "end": "2784335"
  },
  {
    "text": "we just generate around it. We don't change the\nprompt text, but we just generate everything\nelse around it.",
    "start": "2784335",
    "end": "2789480"
  },
  {
    "text": "This actually is principled\nif you go through the math. And it allows you to fill in the\nrest of the information there.",
    "start": "2789480",
    "end": "2796640"
  },
  {
    "text": "So we also have something\nlike in the middle, where we have these two prompt tokens,\nsequences of prompt tokens",
    "start": "2796640",
    "end": "2802276"
  },
  {
    "text": "to the middle. We just generate around it. And this allows us to infill. And yeah, it typically\ntends to produce",
    "start": "2802277",
    "end": "2808500"
  },
  {
    "text": "pretty coherent\nstatements basically, which means that we're able\nto control the generation",
    "start": "2808500",
    "end": "2813600"
  },
  {
    "text": "process in a new more\ninteresting way kind of.",
    "start": "2813600",
    "end": "2819320"
  },
  {
    "text": "Yeah, you can't do this with a\ntypical autoregressive model. So now that we have\nour generation quality,",
    "start": "2819320",
    "end": "2824337"
  },
  {
    "text": "the last thing that\nwe need to look at is how do we actually\nevaluate likelihoods of this generative process.",
    "start": "2824337",
    "end": "2830550"
  },
  {
    "text": "So we've shown how we can learn. We've shown how we can\ngenerate, how can we evaluate for likelihoods.",
    "start": "2830550",
    "end": "2836660"
  },
  {
    "text": "So the typical\nmetric that people use for evaluating\nlikelihoods is perplexity. The perplexity of\nan input sequence x",
    "start": "2836660",
    "end": "2843490"
  },
  {
    "text": "is basically just this e\nto the power of negative 1 over d times the log\nprobability of x1 to xd.",
    "start": "2843490",
    "end": "2849190"
  },
  {
    "text": "This is a very\ntypical metric for use for autoregressive modeling. The reason why is because it's\na relatively principled way",
    "start": "2849190",
    "end": "2857170"
  },
  {
    "text": "of measuring the model ability. So if we can have a\nvery low perplexity on some other dataset,\nit means that we're",
    "start": "2857170",
    "end": "2863050"
  },
  {
    "text": "generalizing pretty well. We're compressing things,\nwhich is typically a good sign.",
    "start": "2863050",
    "end": "2868750"
  },
  {
    "text": "Also, we can directly-- this is directly computable\nfor autoregressive modeling because we can compute\nthis p theta directly.",
    "start": "2868750",
    "end": "2877010"
  },
  {
    "text": "And finally, we also tend\nto optimize with respect to something like this. Because at least for\nautoregressive modeling,",
    "start": "2877010",
    "end": "2882845"
  },
  {
    "text": "we optimize with respect to\nthis negative log probability and just taking the\nexponential or effectively",
    "start": "2882845",
    "end": "2888050"
  },
  {
    "text": "optimizing this\nsomething similar. So which is why we can report\nsomething like this basically.",
    "start": "2888050",
    "end": "2894350"
  },
  {
    "text": "And so for diffusion\nmodels, long story short, we can also do something similar.",
    "start": "2894350",
    "end": "2900150"
  },
  {
    "text": "The math actually tends to\nbe kind of a bit involved. But the key insight here\nis that we can take this",
    "start": "2900150",
    "end": "2906170"
  },
  {
    "text": "under some mild conditions,\nlike some very mild conditions on our base distribution,\nhow long we diffuse for.",
    "start": "2906170",
    "end": "2913010"
  },
  {
    "text": "Generative process has\nthe following likelihood bound basically. So our negative\nlog likelihood is",
    "start": "2913010",
    "end": "2918590"
  },
  {
    "text": "bounded above by this integral\nand with this expected value and all this stuff. And when we also add\nsome known constants",
    "start": "2918590",
    "end": "2924920"
  },
  {
    "text": "C The C constant\nis known, a priori. And what's interesting here is\nthat this integral or whatnot,",
    "start": "2924920",
    "end": "2932270"
  },
  {
    "text": "this is exactly our denoising\nscore entropy loss if we recall",
    "start": "2932270",
    "end": "2937770"
  },
  {
    "text": "back to a couple of slides ago. And the only new\nthing is that we have to wait about\nthis Qt of xty,",
    "start": "2937770",
    "end": "2944580"
  },
  {
    "text": "which is this other weighting. It doesn't really\naffect anything for any of the\ncomputations basically.",
    "start": "2944580",
    "end": "2950460"
  },
  {
    "text": "It's just this other weighting. But yeah, which means\nwe can basically",
    "start": "2950460",
    "end": "2955529"
  },
  {
    "text": "train with respect to this loss,\nthis log likelihood bound loss.",
    "start": "2955530",
    "end": "2959820"
  },
  {
    "text": "And so we end up getting\na perplexity bound because we can just take\nthe perplexity of the input sequence. We just feed it through\nthis denoising score entropy",
    "start": "2962770",
    "end": "2969457"
  },
  {
    "text": "loss with this weighting. And we basically get an\nupper bound just by the fact that things are-- like the\nE is monotonic basically,",
    "start": "2969457",
    "end": "2977970"
  },
  {
    "text": "which allows us to report\nperplexity values as well. How does it work in practice?",
    "start": "2977970",
    "end": "2983200"
  },
  {
    "text": "Well, across these\ndifferent models, we do this whole GPT-2\ntrain on open web text,",
    "start": "2983200",
    "end": "2989400"
  },
  {
    "text": "evaluate on other types\nof datasets type of setup. And what we see here\npretty consistently",
    "start": "2989400",
    "end": "2995760"
  },
  {
    "text": "is that our GPT-2 model, it\ndoes tend to produce the best likelihood values.",
    "start": "2995760",
    "end": "3001490"
  },
  {
    "text": "But SEDD with absorbing, the\nabsorbing masking transition,",
    "start": "3001490",
    "end": "3006590"
  },
  {
    "text": "it tends to be very\nclose basically. So for most of these datasets ,\nif we have a pretty close value",
    "start": "3006590",
    "end": "3012980"
  },
  {
    "text": "within plus 10% or\nso, we underline it. And the reason why we\nhave this plus 10% cut off",
    "start": "3012980",
    "end": "3018927"
  },
  {
    "text": "is because of the\nfact that we're only reporting a bound and\nnot the true ground truth like non-bound\nlikelihood perplexity.",
    "start": "3018927",
    "end": "3026460"
  },
  {
    "text": "But we have this underline\nhere, and we also had the best results bolded.",
    "start": "3026460",
    "end": "3032130"
  },
  {
    "text": "And what we consistently\nsee is that our SEDD model, it can basically match on\nlike WikiText2, WikiText103.",
    "start": "3032130",
    "end": "3039480"
  },
  {
    "text": "It has to fall within\nthis perplexity bound. And for something\nlike PTB, it actually outperforms the existing\nmodel, like an existing GPT-2",
    "start": "3039480",
    "end": "3047190"
  },
  {
    "text": "pre-trained model. And sometimes, by a pretty\nconsiderable margin as shown in this middle bar here.",
    "start": "3047190",
    "end": "3053250"
  },
  {
    "text": "Basically this middle line here. And so this is great because,\nnow, we can show that.",
    "start": "3053250",
    "end": "3058440"
  },
  {
    "text": "Basically, we can challenge\nautoregressive modeling not only on like\ngeneration quality, which",
    "start": "3058440",
    "end": "3063750"
  },
  {
    "text": "is a bit more like of-- there's\nmore moving parts there, but also on perplexity which\nis a more streamlined, more",
    "start": "3063750",
    "end": "3072150"
  },
  {
    "text": "compact way of comparing between\ntwo different autoregressive models.",
    "start": "3072150",
    "end": "3077490"
  },
  {
    "text": "Well, what you would do\nhere is you would just like generate up to\na end of text token, and you just\npost-process it kind of.",
    "start": "3077490",
    "end": "3084490"
  },
  {
    "text": "And typically, for this\nopen web text data, sequences are pretty long. Like 700 or so tokens\nout of the 1,024.",
    "start": "3084490",
    "end": "3091650"
  },
  {
    "text": "So it's pretty\ncomparable basically. But, yeah, OK,\njust to summarize.",
    "start": "3091650",
    "end": "3098160"
  },
  {
    "text": "So the first thing is\nthat it's pretty hard to build probabilistic\nmodels for discrete spaces. We have GANs, VAEs,\ndiffusion models.",
    "start": "3100730",
    "end": "3108500"
  },
  {
    "text": "And a lot of these things are\npretty hard to naively extend from continuous space\nto discrete space, which",
    "start": "3108500",
    "end": "3114319"
  },
  {
    "text": "is why we only really have\nautoregressive modeling as a way of doing things.",
    "start": "3114320",
    "end": "3120387"
  },
  {
    "text": "Basically, so,\nautoregressive modeling is the only really viable\nparadigm in this space.",
    "start": "3120387",
    "end": "3126710"
  },
  {
    "text": "The idea here is that we can\nextend score based models to discrete spaces. And we can do this by\ninstead of modeling",
    "start": "3126710",
    "end": "3134055"
  },
  {
    "text": "the gradients of the\ndata distribution, we model the ratios of the\ndata distribution, also known as a concrete score.",
    "start": "3134055",
    "end": "3140960"
  },
  {
    "text": "We optimize this new score\nmatching loss called score entropy, which we can also have\nthese denoising and implicit",
    "start": "3140960",
    "end": "3147020"
  },
  {
    "text": "variants of which\nmake it tractable. And then we can sample\nfrom our process.",
    "start": "3147020",
    "end": "3153902"
  },
  {
    "text": "We can sample from our score\nbased model using a diffusion process, using a forward and\na reverse diffusion process.",
    "start": "3153903",
    "end": "3159810"
  },
  {
    "text": "So in particular, the\nforward diffusion process synergizes with our\ndenoising score entropy loss, which makes everything\npretty seamless integrate",
    "start": "3159810",
    "end": "3166290"
  },
  {
    "text": "together. And we can make it\nfast and controllable for our generation,\nwhich is nice.",
    "start": "3166290",
    "end": "3174540"
  },
  {
    "text": "And finally, our\ngeneration quality can surpass autoregressive\nmodeling because of the fact",
    "start": "3174540",
    "end": "3180660"
  },
  {
    "text": "that we don't have to\nworry about contacts. We can just generate a\nwhole sequence in parallel. And this allows us\nmore information",
    "start": "3180660",
    "end": "3187349"
  },
  {
    "text": "during the generation process. Finally, we also have\nour likelihood bound",
    "start": "3187350",
    "end": "3192420"
  },
  {
    "text": "based off of score entropy. This basically\nlines up perfectly with-- our score entropy\nloss basically lines up",
    "start": "3192420",
    "end": "3200472"
  },
  {
    "text": "perfectly with the\nlikelihood bound that one would hope to\noptimize or compare with.",
    "start": "3200472",
    "end": "3205530"
  },
  {
    "text": "And for this task, we\nbasically challenge autoregressive dominance\nfor the first time on any large enough sequence\nlike GPT-2 level result.",
    "start": "3205530",
    "end": "3217350"
  },
  {
    "text": "For this case, we're computing\na bound on the negative log likelihood. And the negative log likelihood\ngoes into the perplexity here.",
    "start": "3217350",
    "end": "3227070"
  },
  {
    "text": "So perplexity will kind of be\nlike a forward KL divergence. And this is like a reverse KL\ndivergence, will be the way",
    "start": "3227070",
    "end": "3233610"
  },
  {
    "text": "this model. So basically, so GPT-\n2, if we remove the fact",
    "start": "3233610",
    "end": "3240210"
  },
  {
    "text": "that we're only reporting a\nbound, it tends to outperform. Really, it's much\ncloser than that,",
    "start": "3240210",
    "end": "3246420"
  },
  {
    "text": "which means that it's\ncovering the modes of the data distribution sufficiently well. But it also has\nleakage basically.",
    "start": "3246420",
    "end": "3254285"
  },
  {
    "text": "We can generate sequences\nthat are low probability, but this doesn't show up\nin our KL divergence loss.",
    "start": "3254285",
    "end": "3260079"
  },
  {
    "text": "So previously, you had this\nembed into continuous space. Generally, the issue\nthat people have found",
    "start": "3260080",
    "end": "3265680"
  },
  {
    "text": "is that it doesn't\nreally work as well. Like the log likelihoods are-- I mean, so we had--",
    "start": "3265680",
    "end": "3271080"
  },
  {
    "text": "typically, let's take a look\nat this graph, thing here. So for previous continuous\ndiffusion models,",
    "start": "3271080",
    "end": "3279119"
  },
  {
    "text": "it would be way worse\nbasically, like much, much worse like 2.5 times worse,\nsomething like this.",
    "start": "3279120",
    "end": "3285260"
  },
  {
    "text": "This is typically like the\nrange for discontinuous discrete diffusion model,\nwhere we discretize the tokens.",
    "start": "3285260",
    "end": "3291260"
  },
  {
    "text": "And also, the issue is that\nfor generation quality, it's much slower. Basically, when we try to\ngenerate the sequences,",
    "start": "3291260",
    "end": "3297290"
  },
  {
    "text": "it becomes like-- because it's like\nso sparse, we have to make sure we don't\nhave much error.",
    "start": "3297290",
    "end": "3303269"
  },
  {
    "text": "So we have to take a lot\nof discretization steps. So for example, for\nsome models, we'd have to take like\n4,000 steps in order",
    "start": "3303270",
    "end": "3310520"
  },
  {
    "text": "to generate a 1,000\nlength sequence, which is just too much basically.",
    "start": "3310520",
    "end": "3315980"
  },
  {
    "text": "The idea is that hopefully\nthe error isn't that much, and you can jump\nbetween the two. There are some principled\nway of doing it.",
    "start": "3315980",
    "end": "3322170"
  },
  {
    "text": "And it's called tau leaping,\nif you're familiar with the-- it's called tau leaping in the\nchemical engineering literature",
    "start": "3322170",
    "end": "3330860"
  },
  {
    "text": "or whatever. And it kind of works. So if you take very\nsmall steps, it's",
    "start": "3330860",
    "end": "3336200"
  },
  {
    "text": "going to be reasonably\nconditionally independent assuming your\nratios don't change too much.",
    "start": "3336200",
    "end": "3341270"
  },
  {
    "text": "Your model doesn't\nchange too much. So it's a discretization\nscheme basically. So like diffusion\nmodels, we also",
    "start": "3341270",
    "end": "3346590"
  },
  {
    "text": "have a similar discretization\nscheme in [INAUDIBLE].. Presumably, you can learn\nany probability distribution",
    "start": "3346590",
    "end": "3353010"
  },
  {
    "text": "over your discrete\nspace with both methods. But the question\nhere is which one",
    "start": "3353010",
    "end": "3358860"
  },
  {
    "text": "builds in a better\ninductive bias and is more amenable\nfor optimization? Qt is kind of like\nthe transition rate",
    "start": "3358860",
    "end": "3365519"
  },
  {
    "text": "but we exponentiate it. So like in particular,\nwe have this--",
    "start": "3365520",
    "end": "3370859"
  },
  {
    "text": "so basically, Qt is\na transition rate. And this exponentiated matrix\nis a transition kernel. We can do it for\nmany time steps.",
    "start": "3370860",
    "end": "3378540"
  },
  {
    "text": "But the issue here is that it's\nbetter to put in a time step so it becomes easier.",
    "start": "3378540",
    "end": "3383580"
  },
  {
    "text": "Basically, the fundamental\nQ tends to stay the same, but just we just multiply\nit by some noise level.",
    "start": "3383580",
    "end": "3389140"
  },
  {
    "text": "So it's kind of\nlike all built in. Q is just a transition rate so\nsomething like this basically.",
    "start": "3389140",
    "end": "3396690"
  },
  {
    "text": "It would go from uniformly. This is how we go other\nthings uniformly kind of.",
    "start": "3396690",
    "end": "3402740"
  },
  {
    "text": "Or in this case, where\nwe go to a mask token. At each time step, the Q\nbasically is scaled effectively.",
    "start": "3402740",
    "end": "3410310"
  },
  {
    "text": "So we have a scaling\nto how much noise we add in at each time step. Yeah, a true scale is\ncontrolled by sigma.",
    "start": "3410310",
    "end": "3416160"
  },
  {
    "text": "So this bounds basically\nthe elbow bound from a VAE. So you would assume that\nyour diffusion model,",
    "start": "3416160",
    "end": "3422330"
  },
  {
    "text": "you have your forward\ndiffusion process, which is your encoding\nlayer and your VAE. And you're learning the reverse,\nlike, real reverse diffusion",
    "start": "3422330",
    "end": "3432860"
  },
  {
    "text": "process, which is your decoder. And then if you just work\nthat out, you plug it in, this is the ELBO that\nyou get out basically.",
    "start": "3432860",
    "end": "3441540"
  },
  {
    "text": "This architecture right\nhere is the key idea here. And the sequence to\nsequence neural network is just like a transformer.",
    "start": "3441540",
    "end": "3448200"
  },
  {
    "text": "We basically make a transformer. But we have a\nnon-causal mask, which allows us to go like-- which\nallows the attention layer to go",
    "start": "3448200",
    "end": "3455460"
  },
  {
    "text": "from [INAUDIBLE] completely\nfrom everything to everything, basically.",
    "start": "3455460",
    "end": "3461010"
  },
  {
    "text": "So it's like BERT basically. It would be like this-- for question answering, it\nwould be like this basically.",
    "start": "3461010",
    "end": "3467250"
  },
  {
    "text": "You just fill it in. You fill it in. We're separating out between the\nGPT-2 small, the small models",
    "start": "3467250",
    "end": "3474300"
  },
  {
    "text": "and the medium models. And between the\nmedium models, we have the absorbing and\nuniform state basically.",
    "start": "3474300",
    "end": "3480130"
  },
  {
    "text": "So we have this uniform\ntransition matrix and what is masking\ntransition matrix basically. And typically, we\nsee that the uniform",
    "start": "3480130",
    "end": "3487769"
  },
  {
    "text": "tends to produce worse results\nthan the masking basically,",
    "start": "3487770",
    "end": "3493200"
  },
  {
    "text": "so like just randomly\nflipping words. And this makes sense because\nif you randomly flip words,",
    "start": "3493200",
    "end": "3498330"
  },
  {
    "text": "then you're going to\nend up with sequences that kind of don't make sense. Whereas if you\njust mask the word,",
    "start": "3498330",
    "end": "3504400"
  },
  {
    "text": "then the sequences still\nmake sense broadly. I mean, if you assume that\nwe can fill in the masks.",
    "start": "3504400",
    "end": "3509440"
  },
  {
    "text": "And in this case, this is\nour generative perplexity, which is basically we\ngenerate a new sequence,",
    "start": "3509440",
    "end": "3514990"
  },
  {
    "text": "and then we take a\nGPT-2 large model. And we evaluate the perplexity\nof this generated sequence",
    "start": "3514990",
    "end": "3520060"
  },
  {
    "text": "on our GPT-2 large. And it's a pretty\ncommon evaluation to use like GPT-2 large\nevaluate the things.",
    "start": "3520060",
    "end": "3528400"
  },
  {
    "text": "There's a bunch of\ndifferent metrics that are built off of this. We also took a look at\nFréchet distance metric.",
    "start": "3528400",
    "end": "3534730"
  },
  {
    "text": "And it also tends to work,\nlike see an improvement there basically. So basically, you can\ntake like a larger model,",
    "start": "3534730",
    "end": "3540610"
  },
  {
    "text": "and then try to extract\nsome feature representations or some values in order to mask,\nview your smaller model outputs.",
    "start": "3540610",
    "end": "3549162"
  },
  {
    "text": "The issue here is that\nwe need to compute this exponential quickly. And if our Q is, let's say,\na GPT-2 tokenizer size,",
    "start": "3549162",
    "end": "3556690"
  },
  {
    "text": "a total number of\ntokens is like 50,000. Well, if we want to compute\nthis matrix exponential,",
    "start": "3556690",
    "end": "3561820"
  },
  {
    "text": "it takes way too long. Like it will take like 10\nseconds just to compute this or whatever even on CUDA, even\non GPU because of how massive",
    "start": "3561820",
    "end": "3570670"
  },
  {
    "text": "it is. We tried experimenting with\nother more complex Q that",
    "start": "3570670",
    "end": "3575890"
  },
  {
    "text": "would allow us to do\nthis computation easier. But it doesn't tend to\nwork because of the fact",
    "start": "3575890",
    "end": "3582070"
  },
  {
    "text": "that it is too much\nof-- it is kind of a fundamentally different\narchitectural design",
    "start": "3582070",
    "end": "3588190"
  },
  {
    "text": "choice basically. It's not built for CUDA to\ndo this matrix exponential. Thanks, everyone, for attending.",
    "start": "3588190",
    "end": "3594727"
  },
  {
    "text": "Thanks, everyone, for listening. I hope you learned something.",
    "start": "3594727",
    "end": "3598500"
  }
]