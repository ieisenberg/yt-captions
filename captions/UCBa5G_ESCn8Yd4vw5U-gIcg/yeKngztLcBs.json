[
  {
    "start": "0",
    "end": "5630"
  },
  {
    "text": "OK. Hello, everyone. ",
    "start": "5630",
    "end": "10670"
  },
  {
    "text": "I'd like to start by\nexpressing empathy",
    "start": "10670",
    "end": "16100"
  },
  {
    "text": "and condolences to the Israelis,\nthe Palestinians among you",
    "start": "16100",
    "end": "21710"
  },
  {
    "text": "those who have friends\nand family from that area.",
    "start": "21710",
    "end": "28669"
  },
  {
    "text": "I personally am one such person\nand it's not easy these days.",
    "start": "28670",
    "end": "37130"
  },
  {
    "text": "So if you need any support,\naccommodation, let us know.",
    "start": "37130",
    "end": "43870"
  },
  {
    "text": "I know that I do. I know that I was not able\neven to prepare this one",
    "start": "43870",
    "end": "50260"
  },
  {
    "text": "meager lecture that I was\nsupposed to finally give,",
    "start": "50260",
    "end": "55269"
  },
  {
    "text": "so I'll do what I can. I'll tell you a little bit about\nthe AEP and mismatched decoding",
    "start": "55270",
    "end": "66040"
  },
  {
    "text": "or mismatched\ncompression, and then I'll hand it back to the more capable\nparts of our teaching staff.",
    "start": "66040",
    "end": "75940"
  },
  {
    "text": " So I want to tell you a little\nbit about the AEP, that's",
    "start": "75940",
    "end": "87500"
  },
  {
    "text": "the Asymptotic Equipartition\nProperty, and how it relates",
    "start": "87500",
    "end": "104000"
  },
  {
    "text": "to compression--\nlossless compression and the fundamental\nlimits on compression.",
    "start": "104000",
    "end": "113010"
  },
  {
    "text": "So and we're going to-- those of you and\nhopefully most of you",
    "start": "113010",
    "end": "119930"
  },
  {
    "text": "will take 276 in\nthe next quarter. We're going to do this stuff\nvery kind of rigorously,",
    "start": "119930",
    "end": "129000"
  },
  {
    "text": "mathematically, concretely. So here I'm allowing myself\nto just try and give you",
    "start": "129000",
    "end": "136200"
  },
  {
    "text": "the gist, the essence\nqualitatively, what this is about.",
    "start": "136200",
    "end": "141780"
  },
  {
    "text": "I'm going to spare you the\nepsilons and the deltas. And in fact I'm going to do this\nthrough just one example which",
    "start": "141780",
    "end": "153569"
  },
  {
    "text": "if you understand the\nessence of this example, then you really\nunderstand the AEP.",
    "start": "153570",
    "end": "161640"
  },
  {
    "text": "So let's consider-- yes. [INAUDIBLE] a couple\npoints font size is bigger.",
    "start": "161640",
    "end": "172690"
  },
  {
    "text": "OK. Bigger? And I can use this\nthe whole thing.",
    "start": "172690",
    "end": "177810"
  },
  {
    "text": "[INAUDIBLE] OK. So up to here. OK. Great but bigger than this.",
    "start": "177810",
    "end": "184445"
  },
  {
    "text": "Yeah, [INAUDIBLE]. OK. So let's consider data like xn.",
    "start": "184445",
    "end": "192810"
  },
  {
    "text": "We use this notation\nfor the n-tuple. x1, x2, up to xn.",
    "start": "192810",
    "end": "202050"
  },
  {
    "text": "So this is your an n-tuple of\ndata that you want to compress. You want to represent\nit with a bit.",
    "start": "202050",
    "end": "209750"
  },
  {
    "text": "And let's suppose\nthat this again, the simplest non-trivial example\nis when these guys are generated",
    "start": "209750",
    "end": "218680"
  },
  {
    "text": "IID according to some generic\nrandom variable x, which--",
    "start": "218680",
    "end": "228099"
  },
  {
    "text": "let's assume for this example\nis also binary, and with some--",
    "start": "228100",
    "end": "236890"
  },
  {
    "text": "oops, sorry, I\nforgot the bigger-- with some Bernoulli parameter p.",
    "start": "236890",
    "end": "243340"
  },
  {
    "text": "So each one of these\ncomponents is just generated IID like\nfair coin flip,",
    "start": "243340",
    "end": "250030"
  },
  {
    "text": "the probability of a 1\nfor each of these is p.",
    "start": "250030",
    "end": "255160"
  },
  {
    "text": "So that's your source of data\nthat you want to compress.",
    "start": "255160",
    "end": "260528"
  },
  {
    "text": "So what do we know\nthen about this n-tuple",
    "start": "260529",
    "end": "265960"
  },
  {
    "text": "that you need to\nrepresent, well, we know that with high\nprobability, and this is",
    "start": "265960",
    "end": "272740"
  },
  {
    "text": "from the law of large numbers. So this means with\nhigh probability,",
    "start": "272740",
    "end": "278650"
  },
  {
    "text": "with all likelihood,\nthis n-tuple xn",
    "start": "278650",
    "end": "283960"
  },
  {
    "text": "is going to have\na few properties. So let's see. It's going to have, well, number\none, how many approximately?",
    "start": "283960",
    "end": "296160"
  },
  {
    "text": "So again, these guys are\nIID where [INAUDIBLE] p. So approximately, how many\nones am I going to see here?",
    "start": "296160",
    "end": "307740"
  },
  {
    "text": "I'm going to see 0s and 1s,\napproximately what fraction",
    "start": "307740",
    "end": "313349"
  },
  {
    "text": "of these components\nis going to be 1s? P.",
    "start": "313350",
    "end": "319550"
  },
  {
    "text": "P. That's the law of large\nnumbers applied to this case. So approximately, we're\ngoing to see n times p 1s.",
    "start": "319550",
    "end": "334950"
  },
  {
    "text": " Everybody OK with this?",
    "start": "334950",
    "end": "340840"
  },
  {
    "text": "And similarly, how\nmany approximately, like then how many 0s\nare we going to see here?",
    "start": "340840",
    "end": "349630"
  },
  {
    "text": " The remaining places.",
    "start": "349630",
    "end": "355650"
  },
  {
    "text": "Right, the remaining places\nwhich is fraction 1 minus p of the places, so\napproximately n times 1",
    "start": "355650",
    "end": "364190"
  },
  {
    "text": "minus p of the component\nare going to be 0s. ",
    "start": "364190",
    "end": "375580"
  },
  {
    "text": "And so number three,\nfor this sequence",
    "start": "375580",
    "end": "384060"
  },
  {
    "text": "that was realized by\nflipping these coins, I'm going to look\nat the sequence",
    "start": "384060",
    "end": "389820"
  },
  {
    "text": "and ask, oh, in retrospect,\nwhat was its probability?",
    "start": "389820",
    "end": "395010"
  },
  {
    "text": "So I'm going to ask, and I'm\nsorry for a slight abuse. I mean, this is p that\nrepresents the Bernoulli",
    "start": "395010",
    "end": "400710"
  },
  {
    "text": "parameter, but this\nis like bigger P that represents the probability\nof this realized sequence.",
    "start": "400710",
    "end": "410207"
  },
  {
    "text": "So I'm going to look\nat this realized sequence, some 0s,\nsome 1s, and I'm going to ask hey, what's\nthe probability of seeing",
    "start": "410208",
    "end": "420720"
  },
  {
    "text": "such a sequence? So that's going to be\nright approximately--",
    "start": "420720",
    "end": "431100"
  },
  {
    "text": "in fact, before approximately,\nlet's ask ourselves, exactly what is\nthis going to be?",
    "start": "431100",
    "end": "437490"
  },
  {
    "text": "So what's the probability of\nseeing some particular sequence that has 0 here 0 1 1 0 1 1 0?",
    "start": "437490",
    "end": "446970"
  },
  {
    "text": "What's the probability of\nseeing exactly that sequence? ",
    "start": "446970",
    "end": "453770"
  },
  {
    "text": "1 over 2 to the n. ",
    "start": "453770",
    "end": "461030"
  },
  {
    "text": "What was the attempt? 1 over 2 to then. Oh, 1 over 2 to the n. 1 Note that would have\nbeen true if p were a half,",
    "start": "461030",
    "end": "474440"
  },
  {
    "text": "then all sequences\nare equally likely. Each has a probability\nof 1 over 2 to the n.",
    "start": "474440",
    "end": "480950"
  },
  {
    "text": "But now these sequences come\nwith a bias, a probability",
    "start": "480950",
    "end": "487250"
  },
  {
    "text": "of a 1 being P. So what's\nthe probability of seeing a particular sequence 0\n0 1 1 0 0 1 1 if the way",
    "start": "487250",
    "end": "496940"
  },
  {
    "text": "that I generated these\nsequences was IID Bernoulli p?",
    "start": "496940",
    "end": "502070"
  },
  {
    "text": "It depends on the number. Right. How? To the power of k multiplied\nby 1 minus p [INAUDIBLE]..",
    "start": "502070",
    "end": "509480"
  },
  {
    "text": "Exactly. So it's going to be p. So this is small p.",
    "start": "509480",
    "end": "515690"
  },
  {
    "text": "So it's going to be in\nfact exactly small p, the small p that governs\nthe bias in these coin",
    "start": "515690",
    "end": "523640"
  },
  {
    "text": "flips, to the power let's just\nwrite in words, number of ones",
    "start": "523640",
    "end": "532400"
  },
  {
    "text": "in the sequence xn times 1 minus\np to the power number of zeros",
    "start": "532400",
    "end": "547170"
  },
  {
    "text": "in xn.  Every one here gives me a\nfactor of p, every 0 gives me",
    "start": "547170",
    "end": "555509"
  },
  {
    "text": "a factor of 1 minus p. And so approximately,\nby these two properties,",
    "start": "555510",
    "end": "565199"
  },
  {
    "text": "this is going to be then what? p this small p to the\npower, a number of 1s",
    "start": "565200",
    "end": "575699"
  },
  {
    "text": "with high probability is\ngoing to be about np times",
    "start": "575700",
    "end": "585990"
  },
  {
    "text": "1 minus p to the power of zeros,\nwhich with high probability,",
    "start": "585990",
    "end": "592560"
  },
  {
    "text": "is going to be n\ntimes 1 minus P.",
    "start": "592560",
    "end": "603050"
  },
  {
    "text": "So that's equal to what? It's equal to, well, I can\nalways write this as two",
    "start": "603050",
    "end": "610970"
  },
  {
    "text": "to the power log base\ntwo of the same thing.",
    "start": "610970",
    "end": "617029"
  },
  {
    "text": "Right so it's 2 to\nthe power log base 2 when I don't write the base.",
    "start": "617030",
    "end": "622550"
  },
  {
    "text": "It's just base 2 to the power\nlog base 2 of the same thing.",
    "start": "622550",
    "end": "628130"
  },
  {
    "text": "That's pn times p times\n1 minus p and times 1",
    "start": "628130",
    "end": "635570"
  },
  {
    "text": "minus p, which is what?",
    "start": "635570",
    "end": "644140"
  },
  {
    "text": "2 to the power n, log\nof or I have here the p",
    "start": "644140",
    "end": "656190"
  },
  {
    "text": "comes here, p times log of--",
    "start": "656190",
    "end": "661920"
  },
  {
    "text": "in fact, let me put a-- ",
    "start": "661920",
    "end": "667220"
  },
  {
    "text": "let me do this. 2 to the minus. Let me pull out a minus here. So I have p times the minus\nlog of p, or in other words",
    "start": "667220",
    "end": "680560"
  },
  {
    "text": "log of 1 over p, plus 1 minus\np log of over 1 minus p.",
    "start": "680560",
    "end": "693740"
  },
  {
    "start": "693740",
    "end": "706029"
  },
  {
    "text": "Does this look familiar? ",
    "start": "706030",
    "end": "711390"
  },
  {
    "text": "What is this in terms of a\nquantity that you recently",
    "start": "711390",
    "end": "717990"
  },
  {
    "text": "encountered and defined?  This is exactly the entropy\nof these components,",
    "start": "717990",
    "end": "729199"
  },
  {
    "text": "of these random variable\nx that describes them. It's p the entropy you\nshould remind ourselves,",
    "start": "729200",
    "end": "739500"
  },
  {
    "text": "in general, the entropy\nof a random variable x was defined as the sum over all\nthe possible values, small x,",
    "start": "739500",
    "end": "750410"
  },
  {
    "text": "this random variable can\ntake, its probability, the probability of this small\nx log 1 over the probability.",
    "start": "750410",
    "end": "759500"
  },
  {
    "text": "In the case of\nBernoulli p, something that has two possible values one\nwith probability P, so that's",
    "start": "759500",
    "end": "769220"
  },
  {
    "text": "the p log 1 over p, and\nthen the other value the other possible value is 0\nthat has probability 1 minus p.",
    "start": "769220",
    "end": "777930"
  },
  {
    "text": "So that's the other value, 1\nminus p log 1 over 1 minus p.",
    "start": "777930",
    "end": "782940"
  },
  {
    "text": "So this is nothing but the\nentropy of the source which",
    "start": "782940",
    "end": "790850"
  },
  {
    "text": "is generating your data. ",
    "start": "790850",
    "end": "800410"
  },
  {
    "text": "So with high probability,\nwith all likelihood, the sequence that you're going\nto get is going to have--",
    "start": "800410",
    "end": "810490"
  },
  {
    "text": "when you look at\nit, you're going to ask, oh, what's the\nprobability, in retrospect,",
    "start": "810490",
    "end": "816610"
  },
  {
    "text": "that's the sequence that I got? Well, it's\nprobability is about 2",
    "start": "816610",
    "end": "821680"
  },
  {
    "text": "to the minus n\ntimes the entropy. ",
    "start": "821680",
    "end": "827330"
  },
  {
    "text": "Everybody OK with this so far? And so now let's ask\nourselves, how many sequences,",
    "start": "827330",
    "end": "837200"
  },
  {
    "text": "how many sequences with this,\nlet's say with this property,",
    "start": "837200",
    "end": "864050"
  },
  {
    "text": "with this-- ",
    "start": "864050",
    "end": "870120"
  },
  {
    "text": "So how many sequences\nhave the property-- so this is a--",
    "start": "870120",
    "end": "875970"
  },
  {
    "text": "these are binary sequences. Overall, we have 2 to\nthe n possible sequences,",
    "start": "875970",
    "end": "882000"
  },
  {
    "text": "but how many of them\nhave this property that their probability is\nabout 2 to the minus nh?",
    "start": "882000",
    "end": "894000"
  },
  {
    "text": " Is there a question? ",
    "start": "894000",
    "end": "903339"
  },
  {
    "text": "So let's think about it. Collectively, all the\nsequences with this property,",
    "start": "903340",
    "end": "914860"
  },
  {
    "text": "so collectively, their\nprobability is essentially 1",
    "start": "914860",
    "end": "926260"
  },
  {
    "text": "because I said we talked about\nhow with high probability, the law of large numbers tells\nus that with high probability,",
    "start": "926260",
    "end": "931930"
  },
  {
    "text": "with all likelihood, in\nother words with probability essentially 1, the sequence\nthat we're going to get",
    "start": "931930",
    "end": "941230"
  },
  {
    "text": "is going to have this property.  So collectively, all the\nsequences with this property",
    "start": "941230",
    "end": "952259"
  },
  {
    "text": "have probability about\n1, and each of them,",
    "start": "952260",
    "end": "958410"
  },
  {
    "text": "by definition of\nthis property, each of these has this probability.",
    "start": "958410",
    "end": "968130"
  },
  {
    "text": " Has probability approximately\n2 to the minus nh.",
    "start": "968130",
    "end": "974670"
  },
  {
    "start": "974670",
    "end": "983260"
  },
  {
    "text": "So how many such sequences\nare there approximately?",
    "start": "983260",
    "end": "988810"
  },
  {
    "text": " We've got a collection of\nsequences, a collection of--",
    "start": "988810",
    "end": "997579"
  },
  {
    "text": "I'm asking about a collection\nof possible sequences that could be realized.",
    "start": "997580",
    "end": "1003190"
  },
  {
    "text": " Collectively, all the\nsequences in this collection",
    "start": "1003190",
    "end": "1010060"
  },
  {
    "text": "collectively have probability\nabout 1, and each of them",
    "start": "1010060",
    "end": "1015550"
  },
  {
    "text": "individually has essentially\nthe same probability about 2",
    "start": "1015550",
    "end": "1020740"
  },
  {
    "text": "to the minus nh.  So how many sequences are\nthere, must there be in this set",
    "start": "1020740",
    "end": "1031649"
  },
  {
    "text": "that I'm asking about? They each have essentially\nthe same probability, the same small probability,\nand collectively,",
    "start": "1031650",
    "end": "1040799"
  },
  {
    "text": "the sum of all their\nprobabilities is about 1. ",
    "start": "1040800",
    "end": "1046449"
  },
  {
    "text": "Get to the power of nh. Right. Essentially, 1 over their\nindividual probabilities,",
    "start": "1046450",
    "end": "1056740"
  },
  {
    "text": "they each have essentially\nthe same small probability, let's say epsilon. ",
    "start": "1056740",
    "end": "1063480"
  },
  {
    "text": "I'm asking about the size of\na set, each component of which",
    "start": "1063480",
    "end": "1069169"
  },
  {
    "text": "has the same small\nprobability, epsilon, and the sum of all the\nprobabilities of the elements",
    "start": "1069170",
    "end": "1076010"
  },
  {
    "text": "in this set is about one. So the size of the set-- so the\nnumber of sequences is about 1",
    "start": "1076010",
    "end": "1082159"
  },
  {
    "text": "over that epsilon.  It's essentially like\na uniform distribution",
    "start": "1082160",
    "end": "1089530"
  },
  {
    "text": "on this set that has most\nof the probability about 1.",
    "start": "1089530",
    "end": "1095290"
  },
  {
    "text": " So the conclusion is that\nthere are approximately 2",
    "start": "1095290",
    "end": "1106930"
  },
  {
    "text": "to the n, basically 1 over\nthis epsilon, so about 2 to the nh, such sequences.",
    "start": "1106930",
    "end": "1115210"
  },
  {
    "start": "1115210",
    "end": "1123340"
  },
  {
    "text": "And they are called-- we call them-- they are referred\nto as typical sequences.",
    "start": "1123340",
    "end": "1135610"
  },
  {
    "start": "1135610",
    "end": "1143620"
  },
  {
    "text": "So it slightly depends-- we won't get into\nit here, again 276,",
    "start": "1143620",
    "end": "1151090"
  },
  {
    "text": "we'll say more, sometimes\ntypical sequences,",
    "start": "1151090",
    "end": "1156820"
  },
  {
    "text": "require all these\nthree properties, or we require just\nthis, but in any case,",
    "start": "1156820",
    "end": "1162730"
  },
  {
    "text": "it's essentially the same\nthing with high probability. Because of the law of large\nnumbers, we're going to see--",
    "start": "1162730",
    "end": "1170740"
  },
  {
    "text": "the sequence that we are\ngoing to see will be typical. What does it mean to be typical?",
    "start": "1170740",
    "end": "1177190"
  },
  {
    "text": "Well, about in\nthis case fraction of p 1s, fraction of 1 minus p\n0s, and therefore a probability",
    "start": "1177190",
    "end": "1189010"
  },
  {
    "text": "which is about 2\nto the minus nh. ",
    "start": "1189010",
    "end": "1195140"
  },
  {
    "text": "And the set of all such\nsequences, how many are there?",
    "start": "1195140",
    "end": "1201090"
  },
  {
    "text": "About 2 to the nh. And it's a really small\nset because if you",
    "start": "1201090",
    "end": "1211260"
  },
  {
    "text": "think about the set of\nall possible sequences, so here are--",
    "start": "1211260",
    "end": "1216899"
  },
  {
    "text": " so this is just a cartoon.",
    "start": "1216900",
    "end": "1223490"
  },
  {
    "text": "So this is the set of all-- let's say, again in this\nexample, binary sequences. So this is all binary sequences.",
    "start": "1223490",
    "end": "1236600"
  },
  {
    "text": " How many binary sequences\nare there of length n?",
    "start": "1236600",
    "end": "1243485"
  },
  {
    "text": " How many binary, so among\nall the possible values",
    "start": "1243485",
    "end": "1253200"
  },
  {
    "text": "that this thing can take,\nhow many possible values does a binary can\na binary antipodal",
    "start": "1253200",
    "end": "1260940"
  },
  {
    "text": "take through to the end? All binary sequences,\nso the size",
    "start": "1260940",
    "end": "1268880"
  },
  {
    "text": "of the set of all binary\nsequences is two to the n.",
    "start": "1268880",
    "end": "1275840"
  },
  {
    "text": "And in it is a really small--",
    "start": "1275840",
    "end": "1281419"
  },
  {
    "text": "I mean, it's not\na baller I mean, it's a really small subset\nof the typical sequences,",
    "start": "1281420",
    "end": "1293810"
  },
  {
    "text": "the set of typical sequences of\nsize essentially 2 to the nh.",
    "start": "1293810",
    "end": "1312200"
  },
  {
    "start": "1312200",
    "end": "1319740"
  },
  {
    "text": "So with all likelihood,\nthe sequence I'm going to be dealing with\nis going to be in this set,",
    "start": "1319740",
    "end": "1328440"
  },
  {
    "text": "in this typical set, which is\na minuscule fraction of all",
    "start": "1328440",
    "end": "1334950"
  },
  {
    "text": "the possible sequences. Let's see about\nwhat's the fraction?",
    "start": "1334950",
    "end": "1340049"
  },
  {
    "text": "How big is this set? This set, its size\nis about 2 to the nh,",
    "start": "1340050",
    "end": "1349110"
  },
  {
    "text": "relative to all the\npossible binary sequences.",
    "start": "1349110",
    "end": "1356260"
  },
  {
    "text": "That's what it's 2 to the\nminus n times 1 minus h.",
    "start": "1356260",
    "end": "1364240"
  },
  {
    "text": "In this, again, we're now\nfocusing on our binary example",
    "start": "1364240",
    "end": "1370550"
  },
  {
    "text": "here.  So now the entropy,\nthis is the entropy",
    "start": "1370550",
    "end": "1380210"
  },
  {
    "text": "of the source which is\ngenerating our data. So this is the entropy of a\nBernoulli p random variable.",
    "start": "1380210",
    "end": "1388535"
  },
  {
    "text": " And if p is a bi source,\nif this is not a half,",
    "start": "1388535",
    "end": "1396010"
  },
  {
    "text": "these are not fair coin flips\nbut this is some p which is not",
    "start": "1396010",
    "end": "1401170"
  },
  {
    "text": "a half, then what\ncan you tell me about the entropy\nrelative to one?",
    "start": "1401170",
    "end": "1410820"
  },
  {
    "text": "Is it bigger or smaller?  Smaller.",
    "start": "1410820",
    "end": "1416590"
  },
  {
    "text": "Among all Bernoulli variables,\nthe one that has largest entropy is Bernoulli half, and that's\ngoing to have entropy one.",
    "start": "1416590",
    "end": "1425590"
  },
  {
    "text": "So anything else which is biased\nwill have entropy less than 1,",
    "start": "1425590",
    "end": "1431620"
  },
  {
    "text": "and so this is not\nfair coin flips. Entropy is less than 1.",
    "start": "1431620",
    "end": "1437650"
  },
  {
    "text": "The fraction of the\ntypical sequences that you might actually see with\nany non-negligible probability",
    "start": "1437650",
    "end": "1445450"
  },
  {
    "text": "is an exponentially\nsmall fraction",
    "start": "1445450",
    "end": "1451460"
  },
  {
    "text": "of all the possible sequences. So this is a very big deal. So this is not drawn to scale.",
    "start": "1451460",
    "end": "1456919"
  },
  {
    "text": "This thing is actually\nminuscule, exponentially small",
    "start": "1456920",
    "end": "1461930"
  },
  {
    "text": "for large n or as n\nbecomes bigger and bigger,",
    "start": "1461930",
    "end": "1467210"
  },
  {
    "text": "and this has a really\nfundamental ramification",
    "start": "1467210",
    "end": "1472460"
  },
  {
    "text": "for compression\nbecause what it means is that I can and should\nfocus all my coding",
    "start": "1472460",
    "end": "1481519"
  },
  {
    "text": "efforts on this typical set.",
    "start": "1481520",
    "end": "1486530"
  },
  {
    "text": "I should think about how do I\nactually design a compressor under this assumption,\nthis value assumption",
    "start": "1486530",
    "end": "1493280"
  },
  {
    "text": "that with all\nlikelihood, the sequence I'm going to have to represent\nto describe to the decoder",
    "start": "1493280",
    "end": "1499399"
  },
  {
    "text": "is going to be here\nin this small set. So, in fact, who can suggest\nbased on this observation, who",
    "start": "1499400",
    "end": "1514639"
  },
  {
    "text": "can suggest a\ncoding scheme that's",
    "start": "1514640",
    "end": "1520940"
  },
  {
    "text": "going to get a\ncompressor, that's going to get us\nclose to the entropy.",
    "start": "1520940",
    "end": "1527794"
  },
  {
    "start": "1527795",
    "end": "1549170"
  },
  {
    "text": "Anyone?  If I know that my source--",
    "start": "1549170",
    "end": "1558740"
  },
  {
    "text": "the sequence that I\nwill have to describe,",
    "start": "1558740",
    "end": "1564140"
  },
  {
    "text": "let's say I knew the probability\n1 that the sequence that I'm",
    "start": "1564140",
    "end": "1572300"
  },
  {
    "text": "going to have to describe\nis in this set, two sizes 2",
    "start": "1572300",
    "end": "1579630"
  },
  {
    "text": "to the nh, how many bits? ",
    "start": "1579630",
    "end": "1585379"
  },
  {
    "text": "How many bits of description\nam I going to need",
    "start": "1585380",
    "end": "1593540"
  },
  {
    "text": "If I'm going to\nhave to-- if I need to describe one possible\nsequence or one possible element",
    "start": "1593540",
    "end": "1602679"
  },
  {
    "text": "among a set of a certain size,\nthen as we've seen essentially",
    "start": "1602680",
    "end": "1609580"
  },
  {
    "text": "up to rounding off\nthe number of bits that I need to basically\ndescribe or index",
    "start": "1609580",
    "end": "1617080"
  },
  {
    "text": "the possible elements in a set\nof certain size is log the size.",
    "start": "1617080",
    "end": "1623245"
  },
  {
    "start": "1623245",
    "end": "1630420"
  },
  {
    "text": "So if I knew that my\nsequence for sure is in here,",
    "start": "1630420",
    "end": "1639570"
  },
  {
    "text": "I would need\nessentially log the size of the set, which is essentially\nn times h bits describe it.",
    "start": "1639570",
    "end": "1647970"
  },
  {
    "text": "But now I know that\nwith all likelihood, it's going to be here, and with\nsome negligible probability,",
    "start": "1647970",
    "end": "1656580"
  },
  {
    "text": "it's going to be something else. Then how can I use\nthat to construct",
    "start": "1656580",
    "end": "1664080"
  },
  {
    "text": "a maybe variable length code,\nvariable length compressor that",
    "start": "1664080",
    "end": "1672299"
  },
  {
    "text": "will come close to compressing\ndown to essentially",
    "start": "1672300",
    "end": "1679500"
  },
  {
    "text": "the entropy of the source? Anyone want to try? ",
    "start": "1679500",
    "end": "1692660"
  },
  {
    "text": "It's inside the set, then\nthe compressed string starts with one and then\nthere's some sequence",
    "start": "1692660",
    "end": "1699470"
  },
  {
    "text": "of bits that just enumerates\nthe [INAUDIBLE] set, and we know that's\nn times h bits.",
    "start": "1699470",
    "end": "1705620"
  },
  {
    "text": "And then, otherwise,\nit starts with 0, and then it's just [INAUDIBLE]. ",
    "start": "1705620",
    "end": "1714070"
  },
  {
    "text": "That sounds like a\npretty good idea. So let me try and recap.",
    "start": "1714070",
    "end": "1720429"
  },
  {
    "text": "So what the encoder is\ngoing to output is first",
    "start": "1720430",
    "end": "1726790"
  },
  {
    "text": "basically one bit to indicate\nwhether this the source",
    "start": "1726790",
    "end": "1739150"
  },
  {
    "text": "sequence is typical-- to indicate whether the\nsequence is typical or not.",
    "start": "1739150",
    "end": "1753310"
  },
  {
    "text": "So I'm going to see my\nrealized source sequence. I'm going to ask, wait,\nis it in this set or not?",
    "start": "1753310",
    "end": "1759430"
  },
  {
    "text": "I'm going to first give you\noutput one bit to tell you whether or not it's typical. If it's typical, then\nafter this first bit,",
    "start": "1759430",
    "end": "1770590"
  },
  {
    "text": "I'm going to use\nessentially an h",
    "start": "1770590",
    "end": "1777380"
  },
  {
    "text": "bit because I'm going\nto use essentially log the size of the\ntypical set bits.",
    "start": "1777380",
    "end": "1782870"
  },
  {
    "text": "That's all I need in order to\ndescribe one element in this set",
    "start": "1782870",
    "end": "1798110"
  },
  {
    "text": "to describe it. That's if it's typical. So if it was typical,\nthen the decoder",
    "start": "1798110",
    "end": "1806600"
  },
  {
    "text": "knows now that we're focusing\non-- you're describing me now just one\npossibility from here,",
    "start": "1806600",
    "end": "1814340"
  },
  {
    "text": "and I need just log the size\nof this set to describe it.",
    "start": "1814340",
    "end": "1820590"
  },
  {
    "text": "But if that bit indicated\nthat it's not here it's maybe somewhere here, then\nlet's just use the end bits.",
    "start": "1820590",
    "end": "1832159"
  },
  {
    "text": "Let's be wasteful. Maybe we can get away\nwith less, but let's say. Now so it's just some\nbinary sequence of length n.",
    "start": "1832160",
    "end": "1841460"
  },
  {
    "text": "Definitely, I don't\nneed more than, so I'm just going to use\nn bits to describe it",
    "start": "1841460",
    "end": "1848779"
  },
  {
    "text": "if it's not typical",
    "start": "1848780",
    "end": "1859450"
  },
  {
    "text": "So for this scheme, what\nis the expected length,",
    "start": "1859450",
    "end": "1872860"
  },
  {
    "text": "the expected number of\nbits that this scheme",
    "start": "1872860",
    "end": "1879130"
  },
  {
    "text": "will require for encoding, for\ndescribing the source n-tuple",
    "start": "1879130",
    "end": "1886900"
  },
  {
    "text": "xn? So it's essentially, well,\nit's going to be one bit.",
    "start": "1886900",
    "end": "1899140"
  },
  {
    "text": "It's going to be definitely\nno more than 1 bit.",
    "start": "1899140",
    "end": "1907200"
  },
  {
    "text": "Does the one bit indicate\nwhether a sequence is typical or not? And then plus with\nall likelihood,",
    "start": "1907200",
    "end": "1917159"
  },
  {
    "text": "with probability\nessentially 1, then I'm going to need n times h bits.",
    "start": "1917160",
    "end": "1926960"
  },
  {
    "text": "And then I also need to\naccount into this expectation",
    "start": "1926960",
    "end": "1932360"
  },
  {
    "text": "the really small probability,\nlet's call it epsilon n. ",
    "start": "1932360",
    "end": "1939549"
  },
  {
    "text": "This is the probability that\nthe sequence is not typical.",
    "start": "1939550",
    "end": "1949210"
  },
  {
    "text": " On that event, I'm\ngoing to need n bits.",
    "start": "1949210",
    "end": "1957745"
  },
  {
    "start": "1957745",
    "end": "1964990"
  },
  {
    "text": "And in terms of then on\naverage expected number of bits",
    "start": "1964990",
    "end": "1971020"
  },
  {
    "text": "per source symbol,\nso I'm normalizing by the number of source\nsymbols that I'm describing.",
    "start": "1971020",
    "end": "1979180"
  },
  {
    "start": "1979180",
    "end": "1985520"
  },
  {
    "text": "So this is going to be about h.",
    "start": "1985520",
    "end": "1993200"
  },
  {
    "text": "So for n, as n grows,\nthis is going to be,",
    "start": "1993200",
    "end": "1999350"
  },
  {
    "text": "again, this is going to be\nextremely small, because this is the probability of being\natypical, and this is vanishing.",
    "start": "1999350",
    "end": "2009460"
  },
  {
    "text": "So with this kind\nof theme, we are seeing that we can\nget arbitrarily close to achieving the entropy.",
    "start": "2009460",
    "end": "2018475"
  },
  {
    "text": " As we work with larger\nand larger blocks of data,",
    "start": "2018475",
    "end": "2041420"
  },
  {
    "text": "everybody OK with this? ",
    "start": "2041420",
    "end": "2055638"
  },
  {
    "text": "So this AEP-- by the way, why\nis it called the Asymptotic",
    "start": "2055639",
    "end": "2065030"
  },
  {
    "text": "Equipartition Property? Because of this property\nthat asymptotic because it's",
    "start": "2065030",
    "end": "2072138"
  },
  {
    "text": "sort of when n is\nlarge enough, so kind of the law of large\nnumbers starts kicking in,",
    "start": "2072139",
    "end": "2078770"
  },
  {
    "text": "then essentially what we have\nis that all the action happens",
    "start": "2078770",
    "end": "2084138"
  },
  {
    "text": "in this small subset\nwhere effectively we",
    "start": "2084139",
    "end": "2091579"
  },
  {
    "text": "have a uniform distribution\non the typical sequences. They each have probability\nabout 2 to the minus nh.",
    "start": "2091580",
    "end": "2098750"
  },
  {
    "text": " And also, in fact,\nimplicit in this",
    "start": "2098750",
    "end": "2105640"
  },
  {
    "text": "AEP is the fact that we\ncan come close, arbitrarily",
    "start": "2105640",
    "end": "2111789"
  },
  {
    "text": "close to compressing with\nno more than essentially h",
    "start": "2111790",
    "end": "2117760"
  },
  {
    "text": "the source entropy bits per\nsource sequence, per component,",
    "start": "2117760",
    "end": "2125380"
  },
  {
    "text": "but it also actually implies-- ",
    "start": "2125380",
    "end": "2132460"
  },
  {
    "text": "or let me ask you,\ncould we do better?",
    "start": "2132460",
    "end": "2137690"
  },
  {
    "text": "Like potentially can we do\nbetter than h the entropy? Can we get away with\nlossless compression",
    "start": "2137690",
    "end": "2145000"
  },
  {
    "text": "with less than h bits\nper source symbol? And actually implicit in\neverything that we just did is",
    "start": "2145000",
    "end": "2159470"
  },
  {
    "text": "the fact that you cannot. So let's think about this. So can we do better?",
    "start": "2159470",
    "end": "2167090"
  },
  {
    "start": "2167090",
    "end": "2172150"
  },
  {
    "text": "So consider any compressor.",
    "start": "2172150",
    "end": "2177609"
  },
  {
    "text": "So you give me a compressor,\nand let's say that it",
    "start": "2177610",
    "end": "2185930"
  },
  {
    "text": "uses n times r bits on average.",
    "start": "2185930",
    "end": "2195740"
  },
  {
    "text": "Let's say it uses nr bits to\ndescribe the source sequence. ",
    "start": "2195740",
    "end": "2202380"
  },
  {
    "text": "So in other words r\nbits per source symbol. ",
    "start": "2202380",
    "end": "2219119"
  },
  {
    "text": "Let me just say this, that\nbasically any compressor that you give me\nthat uses nr bits,",
    "start": "2219120",
    "end": "2227640"
  },
  {
    "text": "it's going to be\nable to represent 2",
    "start": "2227640",
    "end": "2242799"
  },
  {
    "text": "to the nr possible sequences. ",
    "start": "2242800",
    "end": "2257710"
  },
  {
    "text": "So now what if r is less than h?",
    "start": "2257710",
    "end": "2264550"
  },
  {
    "text": " So what if your compressor is\nusing r bits per source symbol",
    "start": "2264550",
    "end": "2276100"
  },
  {
    "text": "when r is less than h? ",
    "start": "2276100",
    "end": "2284130"
  },
  {
    "text": "Then let's say that this is-- ",
    "start": "2284130",
    "end": "2291900"
  },
  {
    "text": "so if you're only using\nnr bits, there's only-- I don't know. ",
    "start": "2291900",
    "end": "2299200"
  },
  {
    "text": "Denote this as basically this\nis a set of size 2 to the nr",
    "start": "2299200",
    "end": "2311710"
  },
  {
    "text": "corresponding to the different\npossible sequences that this--",
    "start": "2311710",
    "end": "2320290"
  },
  {
    "text": "I don't whatever\nthis scheme was. Whatever this scheme was,\nit's using only nr bits,",
    "start": "2320290",
    "end": "2326080"
  },
  {
    "text": "and the decoder is\ngoing to see nr bits and so what the decoder is going\nto decode can only be one of 2",
    "start": "2326080",
    "end": "2336069"
  },
  {
    "text": "to the nr possibilities--\npossible sequences. So there's like this set\nof possible sequences",
    "start": "2336070",
    "end": "2348579"
  },
  {
    "text": "that the decoder can output,\nand if r is less than h",
    "start": "2348580",
    "end": "2356600"
  },
  {
    "text": "then, in fact, this\nset is necessarily a very negligible probability.",
    "start": "2356600",
    "end": "2363920"
  },
  {
    "text": "Why? Because this part of\nthe set is atypical.",
    "start": "2363920",
    "end": "2372410"
  },
  {
    "text": "I mean, this part has\nabout probability one, I mean, this whole\npart is negligible.",
    "start": "2372410",
    "end": "2377930"
  },
  {
    "text": "Of course, this\npart is negligible. And then this part its size\nis even less than 2 to the nr,",
    "start": "2377930",
    "end": "2385790"
  },
  {
    "text": "to the nr if r is less than\nh is of negligible size relative to this typical set\nwhere we have essentially",
    "start": "2385790",
    "end": "2395330"
  },
  {
    "text": "a uniform distribution. So I've got essentially a\nuniform distribution here, and this is a very small\nin size part of it.",
    "start": "2395330",
    "end": "2407850"
  },
  {
    "text": "So this thing also has a\nreally small probability. So for now, we're not going to\nget more rigorous than that,",
    "start": "2407850",
    "end": "2415480"
  },
  {
    "text": "but I hope that at\nleast qualitatively, intuitively you're\nwith me, that this set",
    "start": "2415480",
    "end": "2425820"
  },
  {
    "text": "of size 2 to the nr,\nwhatever it might be whatever your\ncompressor might be trying to do necessarily, the\nprobability that it will succeed",
    "start": "2425820",
    "end": "2438640"
  },
  {
    "text": "is the probability that\nthe source sequence falls in some set whose\nsize is 2 to the nr.",
    "start": "2438640",
    "end": "2447520"
  },
  {
    "text": "If r is less than\nh, it's necessarily a really, really negligible set\nin terms of its probability.",
    "start": "2447520",
    "end": "2453970"
  },
  {
    "start": "2453970",
    "end": "2468320"
  },
  {
    "text": "So let me just leave\nit at this compressor. ",
    "start": "2468320",
    "end": "2475160"
  },
  {
    "text": "Cannot be lossless. No way in hell that this\nis a lossless compressor.",
    "start": "2475160",
    "end": "2484370"
  },
  {
    "text": "Not only is it not lossless\nwith high probability,",
    "start": "2484370",
    "end": "2490580"
  },
  {
    "text": "not only is it with\nhigh probability,",
    "start": "2490580",
    "end": "2496910"
  },
  {
    "text": "it's going to screw up\nbecause with high probability, the realized\nsequence is not going",
    "start": "2496910",
    "end": "2503369"
  },
  {
    "text": "to be in the set of sequences\nthat this compressor knows how to reconstruct.",
    "start": "2503370",
    "end": "2508920"
  },
  {
    "start": "2508920",
    "end": "2531990"
  },
  {
    "text": "Is everybody OK with\nthis at least on a-- at least on a qualitative level?",
    "start": "2531990",
    "end": "2538640"
  },
  {
    "text": "Why this AEP tells us that we\ncan achieve compression lossless",
    "start": "2538640",
    "end": "2547109"
  },
  {
    "text": "compression on average\narbitrarily close to-- with rate in terms of\nbits per source component.",
    "start": "2547110",
    "end": "2556530"
  },
  {
    "text": "Arbitrarily close to\nh, the source entropy. And that no scheme in the world\nthat tries to operate at a rate",
    "start": "2556530",
    "end": "2567930"
  },
  {
    "text": "less than the entropy\ncan be lossless. ",
    "start": "2567930",
    "end": "2575790"
  },
  {
    "text": "Yes. Just a quick question,\nso you mentioned how this AEP can get\narbitrarily close to entropy.",
    "start": "2575790",
    "end": "2586290"
  },
  {
    "text": "Is this better performance\nthan when we were just-- because when we do the\nblock codes of size n,",
    "start": "2586290",
    "end": "2591810"
  },
  {
    "text": "we could still get the\nentropy within 1 over n, and here we'll also add\nthe entry within 1 over n.",
    "start": "2591810",
    "end": "2597840"
  },
  {
    "text": "Is this better than that\nor are they the same? That's a great question\nand in fact Shubham",
    "start": "2597840",
    "end": "2604799"
  },
  {
    "text": "is going to very shortly\naddress it with some plots where we've actually realized\nboth this kind of compressor,",
    "start": "2604800",
    "end": "2616020"
  },
  {
    "text": "which we're calling\na typical encoder and decoder or a\ntypical compressor,",
    "start": "2616020",
    "end": "2621270"
  },
  {
    "text": "what you're suggesting\nit corresponds to,",
    "start": "2621270",
    "end": "2627150"
  },
  {
    "text": "for example, using Shannon code. ",
    "start": "2627150",
    "end": "2638220"
  },
  {
    "text": "And that's another way of seeing\nhow we can come arbitrarily close to achieving the entropy.",
    "start": "2638220",
    "end": "2645990"
  },
  {
    "text": "Like with the Shannon code\nyou're going to take then",
    "start": "2645990",
    "end": "2651500"
  },
  {
    "text": "the length function associated\nwith your n-tuple is going to be",
    "start": "2651500",
    "end": "2659660"
  },
  {
    "text": "log 1 over or let's\ncall it minus-- ",
    "start": "2659660",
    "end": "2667650"
  },
  {
    "text": "log of 1 over the\nprobability of xn be precise.",
    "start": "2667650",
    "end": "2679230"
  },
  {
    "text": "This is going to\nbe your code length which is no more than\nwithin 1 bit of this,",
    "start": "2679230",
    "end": "2692670"
  },
  {
    "text": "and so right what's going to be\nyour expected code length if you",
    "start": "2692670",
    "end": "2697829"
  },
  {
    "text": "use Shannon code-- ",
    "start": "2697830",
    "end": "2711570"
  },
  {
    "text": "what is this expectation? ",
    "start": "2711570",
    "end": "2719880"
  },
  {
    "text": "What's the\nexpectation of the log",
    "start": "2719880",
    "end": "2725700"
  },
  {
    "text": "of 1 over the\nprobability of something? ",
    "start": "2725700",
    "end": "2732349"
  },
  {
    "text": "Exactly. So in this case, this is the\nentropy of this something.",
    "start": "2732350",
    "end": "2739759"
  },
  {
    "text": " And if the components are\nIID, then and I think--",
    "start": "2739760",
    "end": "2752850"
  },
  {
    "text": "did we say this\nexplicitly or will this be an exercise\nthat the sum--",
    "start": "2752850",
    "end": "2757980"
  },
  {
    "text": "that the entropy of an\nn-tuple with IID components-- [INAUDIBLE]",
    "start": "2757980",
    "end": "2764140"
  },
  {
    "text": "OK. Great. Great. So this you know is\nnothing but n times",
    "start": "2764140",
    "end": "2771150"
  },
  {
    "text": "the entropy of each of\nthe individual components",
    "start": "2771150",
    "end": "2776309"
  },
  {
    "text": "because they have\nthe same distribution and they're independent,\nand so absolutely.",
    "start": "2776310",
    "end": "2783660"
  },
  {
    "text": "So here too you\nsee a scheme whose",
    "start": "2783660",
    "end": "2789270"
  },
  {
    "text": "number of bits per source symbol\ncomes arbitrarily close to--",
    "start": "2789270",
    "end": "2806040"
  },
  {
    "text": "oops, sorry. 1 over n times n times hx. ",
    "start": "2806040",
    "end": "2812760"
  },
  {
    "text": "So with the Shannon\ncode, you can also come arbitrarily close to\nachieving the source entropy",
    "start": "2812760",
    "end": "2820770"
  },
  {
    "text": "if you apply it,\nconstruct it and apply it on a sufficiently\nlarge block of data.",
    "start": "2820770",
    "end": "2828780"
  },
  {
    "text": "And which of them is better? You're going to see\nsome plots very shortly,",
    "start": "2828780",
    "end": "2835140"
  },
  {
    "text": "and you're also going to see\nthat in practice, this is not what we want to be doing\nto get close to the entropy",
    "start": "2835140",
    "end": "2843180"
  },
  {
    "text": "because the complexity of\nthese schemes both in time and in memory is\ngrowing exponentially.",
    "start": "2843180",
    "end": "2850920"
  },
  {
    "text": "We need lookup tables that\nare growing exponentially,",
    "start": "2850920",
    "end": "2857910"
  },
  {
    "text": "even though it's only let's\nsay the size of a typical set with a typical encoder,\nstill and it's not",
    "start": "2857910",
    "end": "2864240"
  },
  {
    "text": "the set of all\nsequences but still it's an exponentially\ngrowing set with n. So it's not very\nrealistic when n is large,",
    "start": "2864240",
    "end": "2876720"
  },
  {
    "text": "and we're going to talk about\nhow to actually get arbitrarily close to this thing with\nvery reasonable complexity.",
    "start": "2876720",
    "end": "2884550"
  },
  {
    "text": "Very shortly, the\nlast thing that I will do speaking\nof the Shannon code",
    "start": "2884550",
    "end": "2892920"
  },
  {
    "text": "is briefly talk about\nmismatched compression.",
    "start": "2892920",
    "end": "2904049"
  },
  {
    "text": "So let's say you knew\nthat your compressor--",
    "start": "2904050",
    "end": "2912320"
  },
  {
    "text": " so let's consider a compressor\nwith a length function.",
    "start": "2912320",
    "end": "2925875"
  },
  {
    "text": " Let's say generically now,\nit's just like one element.",
    "start": "2925875",
    "end": "2932760"
  },
  {
    "text": "Forget n-tuples. Now you've got an\nelement in a set that you want to\ndevise a compressor,",
    "start": "2932760",
    "end": "2940790"
  },
  {
    "text": "a lossless compressor\nfor, and so we know that you can always\ndevise a Shannon code assuming",
    "start": "2940790",
    "end": "2951290"
  },
  {
    "text": "that it has some distribution. Let's say you assume it has a\ndistribution of probability mass function q, then we can always\nconstruct a Shannon code that",
    "start": "2951290",
    "end": "2964960"
  },
  {
    "text": "will have this length function.",
    "start": "2964960",
    "end": "2972720"
  },
  {
    "text": "And very precisely with\nan upper integer value.",
    "start": "2972720",
    "end": "2978619"
  },
  {
    "text": "But essentially, what's\none bit among friends, let's just say roughly\nup to that one bit.",
    "start": "2978620",
    "end": "2990000"
  },
  {
    "text": "If I thought that\nwhat I'm compressing is governed by a distribution\nq probability mass function q,",
    "start": "2990000",
    "end": "2998280"
  },
  {
    "text": "then I would construct-- I would try to construct a\ncompressor whose length function",
    "start": "2998280",
    "end": "3004580"
  },
  {
    "text": "essentially log 1 over q\nbecause we saw that if it were",
    "start": "3004580",
    "end": "3010090"
  },
  {
    "text": "distributed q,\nthen this is going to get me close to the\nentropy of that thing.",
    "start": "3010090",
    "end": "3018549"
  },
  {
    "text": "So but what if-- so I constructed\na compressor that",
    "start": "3018550",
    "end": "3026180"
  },
  {
    "text": "assumes that it's coming from\na probability mass function q when, in fact, x,\nin fact, God decided",
    "start": "3026180",
    "end": "3042049"
  },
  {
    "text": "that she wants to\ngenerate it according to some other distribution p.",
    "start": "3042050",
    "end": "3048575"
  },
  {
    "start": "3048575",
    "end": "3055160"
  },
  {
    "text": "Then what's the price\nI'm going to pay?",
    "start": "3055160",
    "end": "3064170"
  },
  {
    "text": "What's a price that I'm paying\nfor constructing a compressor assuming it came\nfrom a distribution q",
    "start": "3064170",
    "end": "3070410"
  },
  {
    "text": "while it was actually\ndistributed p? Well, let's see what my--",
    "start": "3070410",
    "end": "3075450"
  },
  {
    "text": " what the expected number\nof bits that I'm going",
    "start": "3075450",
    "end": "3080910"
  },
  {
    "text": "to need under this scheme.  Well, it's the expectation,\nso all the possible values",
    "start": "3080910",
    "end": "3091740"
  },
  {
    "text": "for x, it's distributed p. So the expectation is the\nprobability of x times l of x.",
    "start": "3091740",
    "end": "3106240"
  },
  {
    "text": "That's probability of x log 2.",
    "start": "3106240",
    "end": "3115500"
  },
  {
    "text": "So this is log 1 over q.",
    "start": "3115500",
    "end": "3122665"
  },
  {
    "text": " You will allow me to say inside\nthe log multiply it by 1.",
    "start": "3122665",
    "end": "3132744"
  },
  {
    "start": "3132745",
    "end": "3144550"
  },
  {
    "text": "So what am I going to get,\nsome over the possible values",
    "start": "3144550",
    "end": "3149740"
  },
  {
    "text": "of x, px log 1 over px\nplus the sum over xpx log--",
    "start": "3149740",
    "end": "3169130"
  },
  {
    "text": "what do I still owe you? So I've got log 1 over p is\nhere, so now what remains",
    "start": "3169130",
    "end": "3174809"
  },
  {
    "text": "is p over q log of p over q.",
    "start": "3174810",
    "end": "3181415"
  },
  {
    "start": "3181415",
    "end": "3188350"
  },
  {
    "text": "So what's this guy?",
    "start": "3188350",
    "end": "3193900"
  },
  {
    "text": "Look familiar? ",
    "start": "3193900",
    "end": "3200460"
  },
  {
    "text": "Exactly this is\nthe entropy of x. And this guy sum over the\npossible values of xpx log px",
    "start": "3200460",
    "end": "3216430"
  },
  {
    "text": "over qx is what's known--",
    "start": "3216430",
    "end": "3221530"
  },
  {
    "text": "in fact, we have\na notation for it. We denote it. ",
    "start": "3221530",
    "end": "3229990"
  },
  {
    "text": "It's denoted. We also defined it as part of\nthe proof or proof [INAUDIBLE]",
    "start": "3229990",
    "end": "3238870"
  },
  {
    "text": "the lower bound\nproof [INAUDIBLE].. OK. Great. So what is this, guys?",
    "start": "3238870",
    "end": "3244180"
  },
  {
    "text": "What is this? It's entropy. This is the relative entropy\nor Kullback-Leibler divergence",
    "start": "3244180",
    "end": "3251110"
  },
  {
    "text": "between p and q. So we're seeing that\nobviously we already know",
    "start": "3251110",
    "end": "3261190"
  },
  {
    "text": "what you can't do\nbetter than the entropy, but what we're seeing here\nis that this relative entropy",
    "start": "3261190",
    "end": "3269320"
  },
  {
    "text": "between p and q is\nthe price of mismatch. It's the price in terms\nof excess number of bits",
    "start": "3269320",
    "end": "3277840"
  },
  {
    "text": "that you're going to have to\npay for compressing something",
    "start": "3277840",
    "end": "3284950"
  },
  {
    "text": "that came from distribution\np when you designed a compressor that assumes it\ncame from a distribution q.",
    "start": "3284950",
    "end": "3292010"
  },
  {
    "text": "And so obviously, you can never\ndo better than the entropy, and if you assumed--",
    "start": "3292010",
    "end": "3297530"
  },
  {
    "text": "and if you did something-- assumed a different\ndistribution, you're obviously going\nto pay, so this thing",
    "start": "3297530",
    "end": "3304340"
  },
  {
    "text": "is always non-negative, and, in\nfact, it's going to be positive",
    "start": "3304340",
    "end": "3311750"
  },
  {
    "text": "and growing the more\ndifferent q is from p.",
    "start": "3311750",
    "end": "3318290"
  },
  {
    "text": " I think I'm going\nto leave it at that",
    "start": "3318290",
    "end": "3323870"
  },
  {
    "text": "and let Shubham now show you\nsome really interesting plots.",
    "start": "3323870",
    "end": "3329015"
  },
  {
    "start": "3329015",
    "end": "3335809"
  },
  {
    "text": "Hi, everyone. Thank you, Saki,\nfor the lecture.",
    "start": "3335810",
    "end": "3342230"
  },
  {
    "text": "So as Saki said,\nwe will now look at some real\nexperimental results",
    "start": "3342230",
    "end": "3350630"
  },
  {
    "text": "for some of the coders\nwe have seen so far, and depending on\ntime a few things",
    "start": "3350630",
    "end": "3356420"
  },
  {
    "text": "to wrap up on Huffman coding. So, again, the SLV, we basically\nimplemented the typical set",
    "start": "3356420",
    "end": "3363800"
  },
  {
    "text": "coder. You can find it in-- we'll provide a link.",
    "start": "3363800",
    "end": "3369980"
  },
  {
    "text": "So basically the same logic. If you look at some\nof the functions,",
    "start": "3369980",
    "end": "3375470"
  },
  {
    "text": "you will like see there\nis a function called is typical with just checks if a\nparticular sequence is typical.",
    "start": "3375470",
    "end": "3383270"
  },
  {
    "text": "There is a function\ncalled generate typical set code or\nlookup tables, which",
    "start": "3383270",
    "end": "3389360"
  },
  {
    "text": "goes through all the\nn-tuples for each of them, checks if it is typical, and\nenumerates them as the coding",
    "start": "3389360",
    "end": "3398000"
  },
  {
    "text": "scheme suggested basically. And then the actual coding, you\ncan see here, so here line 114,",
    "start": "3398000",
    "end": "3407869"
  },
  {
    "text": "if chunk to encode\nis typical, then you put a 0 followed by the\nencoding of the-- encoding",
    "start": "3407870",
    "end": "3414410"
  },
  {
    "text": "within the typical set. Otherwise you put a 1\nfollowed by the encoding",
    "start": "3414410",
    "end": "3419480"
  },
  {
    "text": "in the entire set. So take a look at home. It's a bit slow as you\ncan expect whenever you",
    "start": "3419480",
    "end": "3428440"
  },
  {
    "text": "have exponential complexity. Like blocks of 10\nwork fine, 20, it takes like several minutes,\n30, it will never finish.",
    "start": "3428440",
    "end": "3439099"
  },
  {
    "text": "So we will run it until 10. So what we did was we set up a--",
    "start": "3439100",
    "end": "3449940"
  },
  {
    "text": "we will share this notebook. Notebooks are very hard\nto scroll for some reason.",
    "start": "3449940",
    "end": "3455410"
  },
  {
    "text": "On this, we'll see. ",
    "start": "3455410",
    "end": "3467608"
  },
  {
    "text": "Let's just look at the results. I think this will. So what we did was we\nhave here three coders.",
    "start": "3467608",
    "end": "3474210"
  },
  {
    "text": "We have the Huffman code,\nwe have the Shannon code, and we have this typical set\ncode that we just learned about.",
    "start": "3474210",
    "end": "3480630"
  },
  {
    "text": "There is a parameter here when\nyou go into the details too. What do we mean\nby approximately?",
    "start": "3480630",
    "end": "3485910"
  },
  {
    "text": "Equal basically, so we\nchose a reasonable parameter for this block size.",
    "start": "3485910",
    "end": "3491700"
  },
  {
    "text": "So for the Bernoulli\n0.01 source, we show the red dotted\nline at the bottom that shows the entropy,\nso that's basically",
    "start": "3491700",
    "end": "3498990"
  },
  {
    "text": "the best you can ever do. And the purple dotted line,\nthat is the x plus 1 by n bound",
    "start": "3498990",
    "end": "3506339"
  },
  {
    "text": "that you just saw here. So that bound is for\nShannon code also applies to Huffman codes.",
    "start": "3506340",
    "end": "3512940"
  },
  {
    "text": "So you see here that all\nthree codes do pretty well. They are very close\nto each other.",
    "start": "3512940",
    "end": "3519150"
  },
  {
    "text": "Huffman codes are the best\nbecause we know that they are the optimal codes.",
    "start": "3519150",
    "end": "3524970"
  },
  {
    "text": "So they are at the bottom. And then typical and\nShannon, there is sort of a-- depending on the block\nsize, one or the other",
    "start": "3524970",
    "end": "3531369"
  },
  {
    "text": "does better, so this is good. The things we saw\nthere it's gradually getting close to entropy.",
    "start": "3531370",
    "end": "3536740"
  },
  {
    "text": "It's not quite there\nat a block size of 10, but it will get there because\nwe have that boundary that's",
    "start": "3536740",
    "end": "3543300"
  },
  {
    "text": "x plus 1 by n, so we know\nthat at block size 20, you will get within\n0.05 of entropy,",
    "start": "3543300",
    "end": "3549010"
  },
  {
    "text": "and eventually, you will\nget very, very close.  So all three codes have\nexponential complexity just",
    "start": "3549010",
    "end": "3561080"
  },
  {
    "text": "for constructing the codebook\nbecause the typical set code you enumerate over all the\nsequences find the typical ones",
    "start": "3561080",
    "end": "3567560"
  },
  {
    "text": "that's exponential complexity. Shannon code, again, you have\nto go through every sequence.",
    "start": "3567560",
    "end": "3573289"
  },
  {
    "text": "Just building that tree is\nexponential complexity for all three of them. And then storing that tree\nor storing the codebook",
    "start": "3573290",
    "end": "3580250"
  },
  {
    "text": "in your memory. That's also exponential,\nso we also plot below.",
    "start": "3580250",
    "end": "3586880"
  },
  {
    "text": "This shows the time\nfor code generation, so generating the\ncode book or the tree versus block size\nfor the same source.",
    "start": "3586880",
    "end": "3595980"
  },
  {
    "text": "Hoffman is really like\ndominating all of this. I wouldn't worry too much\nabout their relative scaling",
    "start": "3595980",
    "end": "3602490"
  },
  {
    "text": "because that also depends\non our implementation. Huffman code is just\ndoing a lot of work with that particular algorithm\nto merge things and so on.",
    "start": "3602490",
    "end": "3610420"
  },
  {
    "text": "So to maybe make\nit easier to see, here I put the time\non a log scale.",
    "start": "3610420",
    "end": "3616830"
  },
  {
    "text": "So a linear growth\non a log scale is exponential growth\nin the normal thing.",
    "start": "3616830",
    "end": "3622890"
  },
  {
    "text": "So you see all three\nare going basically linearly on this scale. So all three are exponential.",
    "start": "3622890",
    "end": "3628980"
  },
  {
    "text": "None of them is going\nto scale very well. And if you want to use\nthese codes, basically, what",
    "start": "3628980",
    "end": "3634738"
  },
  {
    "text": "will happen is you will\nneed to restrict yourself to some smallish block length,\nand you will deprive yourself",
    "start": "3634738",
    "end": "3640560"
  },
  {
    "text": "of actually achieving entropy. So that's just not\nideal situation.",
    "start": "3640560",
    "end": "3647190"
  },
  {
    "text": "So what you will learn\nin the next two lectures is a way to achieve entropy.",
    "start": "3647190",
    "end": "3653250"
  },
  {
    "text": "Effectively, have\nvery big block sizes without having this\nexponential complexity. So that will be the\nfocus of the next week.",
    "start": "3653250",
    "end": "3661119"
  },
  {
    "text": "This was, I think,\nfor a bit now, this will be the last\ntheoretical lecture. After this, it's\nmostly algorithms",
    "start": "3661120",
    "end": "3667030"
  },
  {
    "text": "for the next five\nlectures or so. OK. Any questions on\nany of these plots?",
    "start": "3667030",
    "end": "3673385"
  },
  {
    "start": "3673385",
    "end": "3681170"
  },
  {
    "text": "And we will provide a\nlink to this notebook,",
    "start": "3681170",
    "end": "3686609"
  },
  {
    "text": "and the typical set code\nor code on the website. ",
    "start": "3686610",
    "end": "3694450"
  },
  {
    "text": "We have some time let me\nwrap up a few things I wanted to do in the last lecture.",
    "start": "3694450",
    "end": "3700480"
  },
  {
    "text": "I want to recap,\nI think it should be pretty fresh in your minds. Ask me if you have any doubts.",
    "start": "3700480",
    "end": "3707020"
  },
  {
    "text": "So as you remember, we\ntalked about Huffman codes and talked about the\ntree construction, we talked about how it\nwas the optimal code.",
    "start": "3707020",
    "end": "3713230"
  },
  {
    "text": "We looked at some examples\non some real-life data. We also talked about this\ntable-based decoding,",
    "start": "3713230",
    "end": "3719740"
  },
  {
    "text": "which is a faster way of\ndecoding Huffman codes, which you will also see\nin your homework.",
    "start": "3719740",
    "end": "3725910"
  },
  {
    "text": "One thing we didn't get to was\nwhere is Huffman code used. So it's used in a lot of\nvery common algorithms.",
    "start": "3725910",
    "end": "3732510"
  },
  {
    "text": "Some of them which you\nuse almost every day like jpeg in\nparticular uses Huffman coding as its last stage.",
    "start": "3732510",
    "end": "3738960"
  },
  {
    "text": "All of these do a lot of\nthings, but at the end of the day when you have some\nstream you want to entropy code,",
    "start": "3738960",
    "end": "3744990"
  },
  {
    "text": "you use Huffman\ncoding in these ones. So let's just look\nat one of them.",
    "start": "3744990",
    "end": "3752059"
  },
  {
    "text": "Let's look at deflate\nwhich already opened up. So this is RFC which describes\nthe entire format for deployed.",
    "start": "3752060",
    "end": "3760880"
  },
  {
    "start": "3760880",
    "end": "3767269"
  },
  {
    "text": "So if you have already\nlooked at question 5 in your homework, that talks\nabout if you have a Huffman",
    "start": "3767270",
    "end": "3774260"
  },
  {
    "text": "code, you need to when you\nsend it to the decoder, how will the decoder\nknow what Huffman code",
    "start": "3774260",
    "end": "3779870"
  },
  {
    "text": "is, the encoder using? So there should be a way to\neither you send the probability distribution over or you\nsend the Huffman code over,",
    "start": "3779870",
    "end": "3787370"
  },
  {
    "text": "but there is a\nthird possibility. The third possibility is\nshown in section 3.2.6",
    "start": "3787370",
    "end": "3797240"
  },
  {
    "text": "which is compression\nwith fixed Huffman codes. So in many cases,\nthe gzip files,",
    "start": "3797240",
    "end": "3802910"
  },
  {
    "text": "they just have a hard-coded\nHuffman code in them. They just looked\nat a lot of data, found a reasonable\ndistribution, and it just",
    "start": "3802910",
    "end": "3810260"
  },
  {
    "text": "uses that standard Huffman code. Both the encoder and the\ndecoder have that in there-- basically, it's part\nof the format itself.",
    "start": "3810260",
    "end": "3817980"
  },
  {
    "text": "So in a lot of\npractical applications, you will see that you don't\ntry to learn the Huffman",
    "start": "3817980",
    "end": "3823190"
  },
  {
    "text": "tree on the fly, you just\nuse a standard Huffman tree, which you learn basically\ndesigned beforehand.",
    "start": "3823190",
    "end": "3831740"
  },
  {
    "text": "It's like a hyperparameter\nsort of thing, where you just\nfix it beforehand. Then you could also have\na dynamic Huffman code.",
    "start": "3831740",
    "end": "3839900"
  },
  {
    "text": "So both are supported\nin the format. So now the question\ncomes like, how do you represent the\ndynamic Huffman code?",
    "start": "3839900",
    "end": "3847160"
  },
  {
    "text": "How do you transmit it\nto the decoder side? So in the homework problem,\nwhat you are doing is",
    "start": "3847160",
    "end": "3852260"
  },
  {
    "text": "you are encoding the probability\ndistribution sending it over, and then the decoder decodes\nthe probability distribution",
    "start": "3852260",
    "end": "3858170"
  },
  {
    "text": "and builds the Huffman\ntree on the decoder side.",
    "start": "3858170",
    "end": "3863450"
  },
  {
    "text": "Gzip does not do that,\nand the main reason is like if you\nlook at the gzip, I encourage you, maybe not\nnow, maybe after lecture 10,",
    "start": "3863450",
    "end": "3871010"
  },
  {
    "text": "read this entire RFC which\ndescribes the gzip format, and you will see how\nfor each and every bit",
    "start": "3871010",
    "end": "3877130"
  },
  {
    "text": "they are like think like twice. How can I avoid spending this\none bit while being super fast?",
    "start": "3877130",
    "end": "3884630"
  },
  {
    "text": "So it's very interesting. So let's-- this is from the\njpeg format where, again,",
    "start": "3884630",
    "end": "3892560"
  },
  {
    "text": "you have some fixed\nHuffman codes in there. But what I want to talk about is\nan alternate way of transmitting",
    "start": "3892560",
    "end": "3900089"
  },
  {
    "text": "your Huffman code to the\ndecoder, not the thing you will do in your homework.",
    "start": "3900090",
    "end": "3905250"
  },
  {
    "text": "This is different, and\nnot what you will do. So this is like--",
    "start": "3905250",
    "end": "3911940"
  },
  {
    "text": "I just drew a Huffman code. This is some Huffman code-- don't worry about\nthe distribution.",
    "start": "3911940",
    "end": "3917790"
  },
  {
    "text": "Some Huffman code. There are a couple\nproperties here.",
    "start": "3917790",
    "end": "3924290"
  },
  {
    "text": "I guess, let me draw\nanother Huffman code here. So you have this Huffman code.",
    "start": "3924290",
    "end": "3929485"
  },
  {
    "text": "Maybe you can have\nanother Huffman code where you just like reorder the\nsymbols a bit, you do like--",
    "start": "3929485",
    "end": "3935110"
  },
  {
    "text": "you still put a here.  And then maybe you can\nlike reorder C and D,",
    "start": "3935110",
    "end": "3942220"
  },
  {
    "text": "and then you can put B here. This is basically\nequivalent Huffman code",
    "start": "3942220",
    "end": "3947820"
  },
  {
    "text": "to the one on the\nleft, and it will get you the same expected\ncode length and everything.",
    "start": "3947820",
    "end": "3953410"
  },
  {
    "text": "So Huffman codes are not unique. When you're breaking ties,\nwhen you're building the tree, you have some\nflexibility, and then",
    "start": "3953410",
    "end": "3959698"
  },
  {
    "text": "when you're putting,\nwhich one to put on left, which one\nto put on right, there is some flexibility. So just generally, you will\nsee that the algorithm is not",
    "start": "3959698",
    "end": "3966730"
  },
  {
    "text": "like fully deterministic. There are multiple Huffman\ncodes corresponding to the same distribution.",
    "start": "3966730",
    "end": "3972640"
  },
  {
    "text": "So that's what makes\ntransmitting it so hard. I don't know if you\nstarted on question 5. Last year lots of\npeople have had",
    "start": "3972640",
    "end": "3978730"
  },
  {
    "text": "issues because they\nwere just like, you do just need to be careful\nthat you do the exact same thing",
    "start": "3978730",
    "end": "3984882"
  },
  {
    "text": "on the encoder and\nthe decoder otherwise, the whole thing breaks down. So there are a\ncouple of properties",
    "start": "3984882",
    "end": "3991869"
  },
  {
    "text": "that you have in\nthe left code, which is one is that there are\nlonger codewords to the left",
    "start": "3991870",
    "end": "3999130"
  },
  {
    "text": "and a shorter\ncodeword to the right. Sure. And then there is another\nproperty that you have C and D,",
    "start": "3999130",
    "end": "4006670"
  },
  {
    "text": "and C comes like\nlexicographically before D. It's alphabetically smaller,\nbut it's to the right of D.",
    "start": "4006670",
    "end": "4014050"
  },
  {
    "text": "So this code according to the\ndefinition of canonical codes is not canonical.",
    "start": "4014050",
    "end": "4019840"
  },
  {
    "text": "Canonical codes have\na few properties they are sorted by\ncodeword length. So anything on the left should\nbe shorter than anything",
    "start": "4019840",
    "end": "4026350"
  },
  {
    "text": "on the right. So as you go from left to\nright, your thing should grow. And if you have two\ncodewords of the same length,",
    "start": "4026350",
    "end": "4034690"
  },
  {
    "text": "the one on the left should be\nthe lexicographically smaller. So C should come before\nD. A should come before B",
    "start": "4034690",
    "end": "4040690"
  },
  {
    "text": "if and B have the same length.  So what you can show,\nyou can reason about it.",
    "start": "4040690",
    "end": "4048660"
  },
  {
    "text": "Basically you can just\nlike shift around the tree, and you can show that\nfor any Huffman code, you can convert it into\na canonical code, which",
    "start": "4048660",
    "end": "4055500"
  },
  {
    "text": "has these two\nproperties basically that it's sorted by codeword\nlength from left to right, and for the same length, it's\nsorted lexicographically.",
    "start": "4055500",
    "end": "4063900"
  },
  {
    "text": "Any questions on this? ",
    "start": "4063900",
    "end": "4071880"
  },
  {
    "text": "Let's do one\nexample where what I",
    "start": "4071880",
    "end": "4076950"
  },
  {
    "text": "will try to show is for\na canonical Huffman code, you don't need to send\nthe probabilities, you don't need to send the\nstructure of the tree, nothing.",
    "start": "4076950",
    "end": "4083867"
  },
  {
    "text": "You just need to send the\nlength of the codewords, and that is enough\nto reconstruct",
    "start": "4083867",
    "end": "4089190"
  },
  {
    "text": "the codebook basically. So let's try to do this. So you start with\nthe smallest one.",
    "start": "4089190",
    "end": "4096913"
  },
  {
    "text": "So this is the one\nyou start with. B equal to 1. And you know that this\nhas to be to the left",
    "start": "4096913",
    "end": "4102290"
  },
  {
    "text": "of every other codeword\nso first thing you do is that this B has to be\non left of everything.",
    "start": "4102290",
    "end": "4111670"
  },
  {
    "text": "Which is the next\none I should pick? A. A. Can anybody tell what\nis the code word for A?",
    "start": "4111670",
    "end": "4121299"
  },
  {
    "text": "Yeah, it has to be 1 0 because\nwe know that we are always going left to right, so this is\nthe only possibility basically.",
    "start": "4121300",
    "end": "4128649"
  },
  {
    "text": "So A. Now I have both C and\nD which are length 3.",
    "start": "4128649",
    "end": "4137043"
  },
  {
    "text": "So which one should\ngo on the left and which one should\ngo on the right? ",
    "start": "4137043",
    "end": "4143261"
  },
  {
    "text": "Yeah, C has to be on the left\nagain because of the property of canonical codes. So if you have a\ncanonical code, you",
    "start": "4143262",
    "end": "4150339"
  },
  {
    "text": "just need the lengths\nof the code words to be able to reconstruct\nthe entire code.",
    "start": "4150340",
    "end": "4156339"
  },
  {
    "text": "So it's a very, very\nefficient representation as opposed to storing\nprobabilities. Also cheap to reconstruct\non the decoder side.",
    "start": "4156340",
    "end": "4163989"
  },
  {
    "text": "It's a very simple algorithm\nto get back to the codebook. Any questions on this?",
    "start": "4163990",
    "end": "4170115"
  },
  {
    "start": "4170115",
    "end": "4176850"
  },
  {
    "text": "So gzip uses canonical\nHuffman codes when they're using this\ndynamic coding strategy.",
    "start": "4176850",
    "end": "4184640"
  },
  {
    "text": "Can anybody suggest how you\nwould store these lengths? Any ideas on how you\nwould actually because you",
    "start": "4184640",
    "end": "4190910"
  },
  {
    "text": "need to transmit the lengths. You need to put the lengths\nin the compressed file. How would you store the lengths? Any ideas?",
    "start": "4190910",
    "end": "4196099"
  },
  {
    "start": "4196100",
    "end": "4203280"
  },
  {
    "text": "For example, let's say I know\nthat the lengths are at most 16 or let's the lengths are at\nmost 8, so how would you--",
    "start": "4203280",
    "end": "4209670"
  },
  {
    "text": "how could you\nrepresent the lengths? ",
    "start": "4209670",
    "end": "4219830"
  },
  {
    "text": "It's not a trick. It's a simple question\nfrom what you've learned. ",
    "start": "4219830",
    "end": "4227680"
  },
  {
    "text": "So the lengths have to\nbe a prefix-free code so that the other side\ncan reconstruct it right.",
    "start": "4227680",
    "end": "4233920"
  },
  {
    "text": "So one simple thing you could\ndo is you just store with-- let's say if the\nmaximum length is 8,",
    "start": "4233920",
    "end": "4239550"
  },
  {
    "text": "you just do like\n3 bits per length, and you have however\nmany lengths, you just transmit that way.",
    "start": "4239550",
    "end": "4245565"
  },
  {
    "text": " Obviously, as I said, gzip\nis very, very concerned",
    "start": "4245565",
    "end": "4254170"
  },
  {
    "text": "about being very efficient. ",
    "start": "4254170",
    "end": "4266940"
  },
  {
    "text": "So what gzip does\nis the Huffman codes for the two alphabets\nappear in the block immediately after\nthe header, whatever.",
    "start": "4266940",
    "end": "4273940"
  },
  {
    "text": "Each code is defined by a\nsequence of code lengths using the canonical description. For even greater\ncompactness, the code length",
    "start": "4273940",
    "end": "4281620"
  },
  {
    "text": "sequences themselves are\ncompressed using a Huffman code. So you compress your\ndata with Huffman code,",
    "start": "4281620",
    "end": "4287349"
  },
  {
    "text": "and then to describe\nthe Huffman code, you store that with\nanother Huffman code.",
    "start": "4287350",
    "end": "4292770"
  },
  {
    "text": "So you will see how much they're\nlike concerned about the space. And the way gzip does it\nis it uses a static Huffman",
    "start": "4292770",
    "end": "4302110"
  },
  {
    "text": "code for storing the lengths. So in gzip what happens is-- ",
    "start": "4302110",
    "end": "4309200"
  },
  {
    "text": "so you have the whatever stream. So that's stored with\na dynamic Huffman code,",
    "start": "4309200",
    "end": "4320239"
  },
  {
    "text": "and then you need to store the\nlengths of this thing, which are stored with a\nstatic Huffman code.",
    "start": "4320240",
    "end": "4327610"
  },
  {
    "text": "So it's a Huffman code whose\ntable is fixed basically.",
    "start": "4327610",
    "end": "4332900"
  },
  {
    "text": "So just like--\nthere are different ways people use these\nthings, but in gzip, this",
    "start": "4332900",
    "end": "4338600"
  },
  {
    "text": "is how it's done. ",
    "start": "4338600",
    "end": "4343889"
  },
  {
    "text": "Any questions? ",
    "start": "4343890",
    "end": "4356230"
  },
  {
    "text": "In the remaining time, let's\ngo over the quiz quickly. ",
    "start": "4356230",
    "end": "4362980"
  },
  {
    "text": "I hope most of these, you will\nbe able to help me out with. So we had a question 1.",
    "start": "4362980",
    "end": "4369989"
  },
  {
    "text": "We asked you to construct the\nHuffman code for Bernoulli 0.5. Anybody? ",
    "start": "4369990",
    "end": "4380210"
  },
  {
    "text": "0 1. Yeah, just like\nit's a simple code. Not much you can do\nfor just two symbols.",
    "start": "4380210",
    "end": "4389210"
  },
  {
    "text": "Bernoulli 0.9?  Yeah, it's the same thing.",
    "start": "4389210",
    "end": "4394389"
  },
  {
    "text": "OK. Good. ",
    "start": "4394390",
    "end": "4400349"
  },
  {
    "text": "OK. 1.3. How many think it's true? ",
    "start": "4400350",
    "end": "4408450"
  },
  {
    "text": "A lot of people think it's true. And it is true. Why is it true?",
    "start": "4408450",
    "end": "4414810"
  },
  {
    "text": "Because we're [INAUDIBLE]\nbecause it's like 1.5. Yeah, so true because\nentropy is 1 in this case,",
    "start": "4414810",
    "end": "4422309"
  },
  {
    "text": "and so you can't do better. How about 1.4? ",
    "start": "4422310",
    "end": "4429660"
  },
  {
    "text": "I see some heads shaking. So the answer is false because\nentropy is less than 1,",
    "start": "4429660",
    "end": "4438179"
  },
  {
    "text": "how do you square\nthis with the fact that Huffman codes are\nsupposed to be optimal? ",
    "start": "4438180",
    "end": "4445250"
  },
  {
    "text": "We can do a block codes. Yeah, so I'm hearing that we can\ndo block codes which is true.",
    "start": "4445250",
    "end": "4453769"
  },
  {
    "text": "If you are forced to do\njust one symbol at a time, then this is indeed\noptimal, but we didn't say that you are\nforced to do that here.",
    "start": "4453770",
    "end": "4459980"
  },
  {
    "text": "So if you assume that and\nthen you wrote the answer, I guess talk to us, we'll see.",
    "start": "4459980",
    "end": "4466310"
  },
  {
    "text": "But yeah, so basically you\ncan do block codes here. And we saw in that\nexample how it was",
    "start": "4466310",
    "end": "4473840"
  },
  {
    "text": "dropping with the block size. ",
    "start": "4473840",
    "end": "4479929"
  },
  {
    "text": "Any questions on this? Let's move on.",
    "start": "4479930",
    "end": "4485740"
  },
  {
    "text": "Then we have the\nsecond question where you had an alphabet with four\nsymbols and uniform distribution",
    "start": "4485740",
    "end": "4491800"
  },
  {
    "text": "over the four. So the Huffman code as you know\nfor a uniform distribution,",
    "start": "4491800",
    "end": "4498160"
  },
  {
    "text": "it will be a fixed-length code,\nfor a uniform distribution over like four symbols.",
    "start": "4498160",
    "end": "4504579"
  },
  {
    "text": "So we have the\nHuffman code here. And then there was\nthis other code that it's a prefix-free code,\nno particular properties as yet.",
    "start": "4504580",
    "end": "4517290"
  },
  {
    "text": "And then we said you have a\nparticular sequence AAABBBCD. ",
    "start": "4517290",
    "end": "4523260"
  },
  {
    "text": "So first part was like the\nencoded length with Gandhi code, and you can actually do the\nencoding 000101010110111.",
    "start": "4523260",
    "end": "4540870"
  },
  {
    "text": "So I'm getting 15, I\nhope that's correct. And then for Huffman code,\nyou have 8 symbols, 2, so 16.",
    "start": "4540870",
    "end": "4550690"
  },
  {
    "text": "So we see that in\nthis particular case, Huffman code was\nbeaten by Gandhi code.",
    "start": "4550690",
    "end": "4556645"
  },
  {
    "text": " So question 2.1, Gandhi codes\nare optimal for this source.",
    "start": "4556645",
    "end": "4566460"
  },
  {
    "text": "How many think it's true? ",
    "start": "4566460",
    "end": "4572230"
  },
  {
    "text": "How many think it's false?  But we saw it, we\nsaw that you beat it.",
    "start": "4572230",
    "end": "4581430"
  },
  {
    "start": "4581430",
    "end": "4591120"
  },
  {
    "text": "Yeah, so I guess the last part\ngives you a hint in a way, but-- oh, sorry, this\nshould be 2.3, 2.4.",
    "start": "4591120",
    "end": "4607300"
  },
  {
    "text": "So this is important\nto understand. You have a source. A source is a\nprobabilistic source,",
    "start": "4607300",
    "end": "4613450"
  },
  {
    "text": "and when we say it's\noptimal, what we mean is that the expected length cannot\nbe better than the entropy",
    "start": "4613450",
    "end": "4619810"
  },
  {
    "text": "or Huffman codes give you gives\nyou the minimum expected length. We don't say that for each\nand every particular sequence",
    "start": "4619810",
    "end": "4627070"
  },
  {
    "text": "realization. Huffman codes are\ngoing to be better. That's just not possible. You can't have a\ncode that's better",
    "start": "4627070",
    "end": "4633640"
  },
  {
    "text": "than every other code on every\npossible sequence because in particular, what\nyou could do is",
    "start": "4633640",
    "end": "4639190"
  },
  {
    "text": "like if you want\nto only transmit one particular sequence,\nwhat you would do is-- ",
    "start": "4639190",
    "end": "4645940"
  },
  {
    "text": "and if you can-- if you're\nallowed to use block codes, what you could do is\nfor that sequence, you define the code as zero,\nand for the other ones,",
    "start": "4645940",
    "end": "4652469"
  },
  {
    "text": "you then define whatever. So if you know like\nyou have to transmit this particular sequence,\nyou can always cheat,",
    "start": "4652470",
    "end": "4660030"
  },
  {
    "text": "but the whole game is about\ntransmitting something useful, and it's useful only if\nthere is uncertainty.",
    "start": "4660030",
    "end": "4666137"
  },
  {
    "text": "So that's why we work in\nthe probabilistic setting where the decoder\ndoesn't already know what you are going to transmit.",
    "start": "4666137",
    "end": "4671190"
  },
  {
    "text": " This happens a\nlot in compression where there are these\ncompression challenges where",
    "start": "4671190",
    "end": "4679093"
  },
  {
    "text": "like you're given a\nfile, and you really want to compress it to\nbe as small as possible. So what I could do is I could\ncreate a compressor decompressor",
    "start": "4679093",
    "end": "4687270"
  },
  {
    "text": "pair where the compressor\njust sends nothing or is just sends like one byte,\nand the decompressor",
    "start": "4687270",
    "end": "4694260"
  },
  {
    "text": "has the entire file as\npart of the decompressor. So the decompressor just gets\nthat one byte, and expands it",
    "start": "4694260",
    "end": "4701340"
  },
  {
    "text": "out to the entire file. So that's like a way\nof cheating in a way, so that's why like\nthese compression",
    "start": "4701340",
    "end": "4706570"
  },
  {
    "text": "challenges also\nhave you can't have a decompressor that's too big.",
    "start": "4706570",
    "end": "4712570"
  },
  {
    "text": "So the whole thing is like\nthe decoder shouldn't already know what is going\nto be transmitted, otherwise the decoder,\nwhat is the point?",
    "start": "4712570",
    "end": "4719230"
  },
  {
    "text": "Why are we even\nsending anything? Anyway, so for this particular\ninstance of the source,",
    "start": "4719230",
    "end": "4726670"
  },
  {
    "text": "you can actually show that if\nyou look at the distribution here, A occurs three\nout of eight times,",
    "start": "4726670",
    "end": "4735940"
  },
  {
    "text": "B occurs three out of\neight times, C is 1 over 8, D is 1 over 8.",
    "start": "4735940",
    "end": "4741410"
  },
  {
    "text": "And if you use this and you\nconstruct the Huffman code, you will see that the\nHuffman code is this one.",
    "start": "4741410",
    "end": "4748100"
  },
  {
    "text": "So this is the Huffman\ncode for this distribution, and hence it is actually\noptimal for this thing.",
    "start": "4748100",
    "end": "4755240"
  },
  {
    "text": "So it's important. For particular\ninstances, you can do things which are different\nfrom the overall optimal source,",
    "start": "4755240",
    "end": "4762800"
  },
  {
    "text": "and when we say optimal,\nwe mean in expectation.",
    "start": "4762800",
    "end": "4768739"
  },
  {
    "text": "So this is already done today. Theoretical interaction\nbetween entropy block coding.",
    "start": "4768740",
    "end": "4775620"
  },
  {
    "text": "And as I said,\nnext lecture, we'll talk about practical codes which\nwill be useful not only to get",
    "start": "4775620",
    "end": "4780659"
  },
  {
    "text": "closer to entropy, but also very\ncrucial for adaptive schemes,",
    "start": "4780660",
    "end": "4785920"
  },
  {
    "text": "non-ID source\ncompression, places where you don't\nwant to assume you know the distribution already,\njust a lot of exciting stuff",
    "start": "4785920",
    "end": "4794670"
  },
  {
    "text": "coming up. Thank you. ",
    "start": "4794670",
    "end": "4803000"
  }
]