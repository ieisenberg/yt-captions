[
  {
    "start": "0",
    "end": "5530"
  },
  {
    "text": "The plan for today is\nto continue talking about normalizing flow models.",
    "start": "5530",
    "end": "11440"
  },
  {
    "text": "So recall that in\nthe last lecture, we've introduced this idea\nof building a latent variable",
    "start": "11440",
    "end": "19660"
  },
  {
    "text": "model that will allow us to\nevaluate likelihoods exactly, so without having to rely\non variational inference.",
    "start": "19660",
    "end": "27790"
  },
  {
    "text": "And so it's going to be similar\nto a variational autoencoder,",
    "start": "27790",
    "end": "33070"
  },
  {
    "text": "in the sense that there's going\nto be two sets of variables. There's going to be\nobserved variables, X, and latent variables, Z.\nAnd the key difference",
    "start": "33070",
    "end": "41710"
  },
  {
    "text": "is that the relationship\nbetween these two sets of random variables\nis deterministic.",
    "start": "41710",
    "end": "47750"
  },
  {
    "text": "So in a way, you\nwould say, sample X given Z by using some\nsimple distribution,",
    "start": "47750",
    "end": "56320"
  },
  {
    "text": "like a Gaussian, where the\nparameters of X given Z might depend on the particular\nvalue of the latent variables.",
    "start": "56320",
    "end": "62800"
  },
  {
    "text": "In a flow model,\nthe relationship is deterministic and invertible. So you obtain X by applying\nthis transformation, which",
    "start": "62800",
    "end": "73100"
  },
  {
    "text": "we denote F theta here. And because the\nmapping is invertible, you can also go back.",
    "start": "73100",
    "end": "79950"
  },
  {
    "text": "So inferring the\nlatent variables, given the observed\none, only requires",
    "start": "79950",
    "end": "85940"
  },
  {
    "text": "you to somehow be\nable to compute the inverse of this mapping.",
    "start": "85940",
    "end": "90979"
  },
  {
    "text": "And here we are denoting\nthese mappings F theta because they are going\nto be parameterized",
    "start": "90980",
    "end": "96080"
  },
  {
    "text": "using neural networks. And what we'll see\ntoday is that we're going to think about\nways to parameterize",
    "start": "96080",
    "end": "103670"
  },
  {
    "text": "this kind of invertible\ntransformations using neural networks\nand then learn them from data, essentially.",
    "start": "103670",
    "end": "110510"
  },
  {
    "text": "And the nice thing,\nrecall, of using an invertible transformation\nis that the likelihood",
    "start": "110510",
    "end": "118700"
  },
  {
    "text": "is tractable. So you can evaluate\nthe probability that this particular\nmodel generates",
    "start": "118700",
    "end": "124759"
  },
  {
    "text": "a data point x by\nessentially using the change of variable formula,\nwhich is fairly intuitive,",
    "start": "124760",
    "end": "131319"
  },
  {
    "text": "especially the first\npiece is very intuitive. You're saying if\nyou want to evaluate the probability of generating an\nimage, let's say X, what you do",
    "start": "131320",
    "end": "139570"
  },
  {
    "text": "is you invert to compute\nthe corresponding Z, and then you evaluate\nhow likely that Z",
    "start": "139570",
    "end": "145540"
  },
  {
    "text": "was under the prior, which\nis this distribution, PZ. And then recall that\nthis is not enough.",
    "start": "145540",
    "end": "154580"
  },
  {
    "text": "If you just do that, you're not\ngoing to get a valid probability density function. To get something\nthat is normalized,",
    "start": "154580",
    "end": "161600"
  },
  {
    "text": "so it integrates 2 to 1 if you\ngo through all possible axes, you have to rescale things\nby this absolute value",
    "start": "161600",
    "end": "169780"
  },
  {
    "text": "of the determinant of the\nJacobian of the inverse mapping. And that's basically\ntelling you,",
    "start": "169780",
    "end": "176980"
  },
  {
    "text": "intuitively, what you do is you\nlinearize the function locally by looking at the Jacobian,\nand then the determinant",
    "start": "176980",
    "end": "185349"
  },
  {
    "text": "of the Jacobian\ntells you how much or how little that\ntransformation expands",
    "start": "185350",
    "end": "193400"
  },
  {
    "text": "or shrinks, like a unit volume,\naround the data point X.",
    "start": "193400",
    "end": "198530"
  },
  {
    "text": "And so it's very similar. Remember, we worked out the\nexample of the linear mapping in the last lecture where\nwe defined a random variable",
    "start": "198530",
    "end": "206690"
  },
  {
    "text": "by transforming a simple\nrandom vector by multiplying it",
    "start": "206690",
    "end": "213290"
  },
  {
    "text": "by a matrix. Essentially, this is\nwhat's going on here, and you have the same\nkind of expression.",
    "start": "213290",
    "end": "219980"
  },
  {
    "text": "And so the key thing\nto note is that this can be computed exactly and\nbasically without introducing",
    "start": "219980",
    "end": "229490"
  },
  {
    "text": "any approximation, to\nthe extent that you can compute these things. You can invert the mapping.",
    "start": "229490",
    "end": "235400"
  },
  {
    "text": "You can compute the\ndeterminant of the Jacobian. You can do those things,\nthen you can evaluate the likelihoods, exact.",
    "start": "235400",
    "end": "241492"
  },
  {
    "text": "So you don't have to rely\non variational inference where you have to\nuse this encoder",
    "start": "241492",
    "end": "246769"
  },
  {
    "text": "to try to guess\nthe Z given an X, and you have to do that\nintegral because there is many",
    "start": "246770",
    "end": "253010"
  },
  {
    "text": "possible Z's that could\ngive you any given X. So you don't have\nto do any of this. And so this is as nice\nas having something",
    "start": "253010",
    "end": "260720"
  },
  {
    "text": "like an autoregressive\nmodel where you can evaluate\nlikelihoods exactly just using this equation.",
    "start": "260720",
    "end": "268070"
  },
  {
    "text": "And one of the\nvarious limitations",
    "start": "268070",
    "end": "273440"
  },
  {
    "text": "of this kind of model\nfamily is that X and Z need to have the\nsame dimensionality.",
    "start": "273440",
    "end": "280400"
  },
  {
    "text": "And so that's different from a\nvariational autoencoder, where we've seen that Z could\nbe very low-dimensional,",
    "start": "280400",
    "end": "287330"
  },
  {
    "text": "and you can use it to discover\nsome compact representation of the data.",
    "start": "287330",
    "end": "292460"
  },
  {
    "text": "That's no longer\npossible in a flow model because, for the mapping\nto be invertible,",
    "start": "292460",
    "end": "299000"
  },
  {
    "text": "Z and X need to have\nthe same dimensionality. Cool, so now, how do\nwe actually do this?",
    "start": "299000",
    "end": "308199"
  },
  {
    "text": "How do we turn this math, this\ngeneral idea, into a model that you can actually\nuse in practice?",
    "start": "308200",
    "end": "316300"
  },
  {
    "text": "Well, the idea is the usual\nstory, like in deep learning, is to combine relatively simple\nbuilding blocks to define",
    "start": "316300",
    "end": "326410"
  },
  {
    "text": "flexible architectures. And so a normalizing flow is\nessentially a generative model",
    "start": "326410",
    "end": "334270"
  },
  {
    "text": "based on what we\nare going to use, essentially, neural\nnetworks to represent",
    "start": "334270",
    "end": "339940"
  },
  {
    "text": "this mapping, F theta, which\nis really the only thing-- you have the prior\nover Z and the F theta",
    "start": "339940",
    "end": "345069"
  },
  {
    "text": "mapping, that's the only\nthing you can really change. And it's called a\nnormalizing flow",
    "start": "345070",
    "end": "350680"
  },
  {
    "text": "because the change\nof variable formula gives us a normalized\ndensity if you account for the determinant\nof the Jacobian.",
    "start": "350680",
    "end": "358390"
  },
  {
    "text": "And it's called a\nflow, exactly what I was saying, because it's\nlike this deep learning idea of defining the\nmapping that we need",
    "start": "358390",
    "end": "366949"
  },
  {
    "text": "by composing individual\nblocks, which could be relatively simple.",
    "start": "366950",
    "end": "372439"
  },
  {
    "text": "So we're going to essentially\ndefine an architecture where",
    "start": "372440",
    "end": "379340"
  },
  {
    "text": "there is going to be multiple\nlayers of invertible mappings where we essentially start\nwith a random vector,",
    "start": "379340",
    "end": "389870"
  },
  {
    "text": "Z0, which could be, let's say,\ndescribed by the prior Gaussian",
    "start": "389870",
    "end": "394970"
  },
  {
    "text": "or something like that. And then what we do is-- or it could even be that, yeah,\ndepending which way you want",
    "start": "394970",
    "end": "402080"
  },
  {
    "text": "to see it, we start on one\nend, with a random vector, Z0, and then we apply\nthese transformations--",
    "start": "402080",
    "end": "410990"
  },
  {
    "text": "F1, F2, F3, Fn, all the\nway through n in this case. And essentially, what\nthis notation means",
    "start": "410990",
    "end": "420920"
  },
  {
    "text": "is that what we do\nis we take that Z0, we pass it through the\nfirst neural network,",
    "start": "420920",
    "end": "427669"
  },
  {
    "text": "and then we take\nthe output of that, and we pass it to the second\nneural network, and so forth.",
    "start": "427670",
    "end": "432980"
  },
  {
    "text": "And we denote this\narchitecture that we get by stacking together\nmultiple invertible layers F",
    "start": "432980",
    "end": "441740"
  },
  {
    "text": "theta. And it's pretty\neasy to see that, as long as each individual\nlayer is invertible,",
    "start": "441740",
    "end": "447320"
  },
  {
    "text": "the combination of\nmultiple layers that you get by doing this kind of\noperation is also invertible.",
    "start": "447320",
    "end": "456740"
  },
  {
    "text": "And so to the extent that\nwe are able to come up with reasonable neural\nnetwork architectures that",
    "start": "456740",
    "end": "462770"
  },
  {
    "text": "define an individual\nlayer, we're going to be able to stack them\ntogether and get something more flexible.",
    "start": "462770",
    "end": "469130"
  },
  {
    "text": "The thetas are not the same\nin each of the F functions, are there? Yeah, so that's a good question.",
    "start": "469130",
    "end": "475230"
  },
  {
    "text": "I think this notation is a\nlittle bit overloading here, the meaning of theta.",
    "start": "475230",
    "end": "481550"
  },
  {
    "text": "The parameters of the\nindividual mappings are going to be different,\nso they are not necessarily tied together.",
    "start": "481550",
    "end": "487639"
  },
  {
    "text": "There's going to be-- we're going to use\ntheta to denote the union of all the parameters\nthat you need to specify",
    "start": "487640",
    "end": "494750"
  },
  {
    "text": "each individual layer.  And so that's the story of\nthis flow of transformations.",
    "start": "494750",
    "end": "503900"
  },
  {
    "text": "You start with a simple\ndistribution for Z0, the first, let's say, at the top,\ntopmost level of your flow,",
    "start": "503900",
    "end": "511120"
  },
  {
    "text": "for example, by sampling\nfrom a Gaussian distribution. This is the usual\nprior, the same thing you had in a\nvariational autoencoder.",
    "start": "511120",
    "end": "517419"
  },
  {
    "text": "And then you apply this sequence\nof invertible transformation",
    "start": "517419",
    "end": "522429"
  },
  {
    "text": "to obtain your final sample. And so you feed it through\nall these different layers.",
    "start": "522429",
    "end": "529430"
  },
  {
    "text": "And then, let's say, after M of\nthem, you get your final sample, X.",
    "start": "529430",
    "end": "535480"
  },
  {
    "text": "And the good thing is that,\nyeah, if each individual mapping is invertible, then\nthe combination",
    "start": "535480",
    "end": "541390"
  },
  {
    "text": "is also going to be invertible. And you can actually\nwork out what's the corresponding change\nof variable formula.",
    "start": "541390",
    "end": "549260"
  },
  {
    "text": "And to the extent that you\nunderstand the determinant of the Jacobian, of\neach individual layer,",
    "start": "549260",
    "end": "556389"
  },
  {
    "text": "then you can work out the\ncorresponding determinant",
    "start": "556390",
    "end": "562330"
  },
  {
    "text": "of the Jacobian, of the\ncombination of these mappings. So all you have\nto do is you have",
    "start": "562330",
    "end": "568060"
  },
  {
    "text": "to be able to invert this\nfunction, F theta, that you get by combining all\nthese neural networks.",
    "start": "568060",
    "end": "574580"
  },
  {
    "text": "And if you can invert\neach individual layer, you can, of course,\ninvert the full function.",
    "start": "574580",
    "end": "580399"
  },
  {
    "text": "And to the extent that you\ncan linearize, basically, and you understand how each\nof the individual layers",
    "start": "580400",
    "end": "586420"
  },
  {
    "text": "behave locally,\nso you understand how that determinant of the\nJacobian looks like, then",
    "start": "586420",
    "end": "593470"
  },
  {
    "text": "you can get that\ndeterminant of the Jacobian of the full mapping. And this is because,\nyeah, basically,",
    "start": "593470",
    "end": "600230"
  },
  {
    "text": "the determinant of\nthe product equals the product of determinants. Or equivalently, you\ncan also get this rule,",
    "start": "600230",
    "end": "606390"
  },
  {
    "text": "like if you recursively apply\nchange of variable formula, you get this sort of expression.",
    "start": "606390",
    "end": "611940"
  },
  {
    "text": "Or to figure out, basically,\nby how much the full mapping",
    "start": "611940",
    "end": "618280"
  },
  {
    "text": "distorts the volume locally. You just need to figure out by\nhow much the individual layers",
    "start": "618280",
    "end": "625750"
  },
  {
    "text": "distort the space,\nand then you just combine the cumulative effect\nof all these various layers.",
    "start": "625750",
    "end": "634120"
  },
  {
    "text": "And so what this is saying is\nthat we are in a good place if we can somehow define\nclasses of neural networks",
    "start": "634120",
    "end": "642820"
  },
  {
    "text": "that are invertible, ideally,\nthat we can invert efficiently, and that we can compute the\ndeterminant of the Jacobian",
    "start": "642820",
    "end": "651490"
  },
  {
    "text": "also efficiently.  And here is a visualization\nof this, how that normalizing",
    "start": "651490",
    "end": "661370"
  },
  {
    "text": "flow works. This is a particular type of\nnormalizing flow called a planar flow.",
    "start": "661370",
    "end": "666810"
  },
  {
    "text": "It's not super important, but\nto give you the intuition, you start on one end with\nthis random variable,",
    "start": "666810",
    "end": "673220"
  },
  {
    "text": "Z0, which let's say\nis Gaussian, and then you get a new random variable\nZ1 by transforming it",
    "start": "673220",
    "end": "679130"
  },
  {
    "text": "through the first layer\nand Z2 by transforming the Z1 by another simple\nlayer, and so forth.",
    "start": "679130",
    "end": "686100"
  },
  {
    "text": "And you can see the effect\nof these transformations. So you start, let's say, with a\ntwo-dimensional random variable,",
    "start": "686100",
    "end": "692090"
  },
  {
    "text": "Z0, which is just\na unit, Gaussian, so this is just a Gaussian with\nspherical covariance, which",
    "start": "692090",
    "end": "698840"
  },
  {
    "text": "basically has a\ndensity that like this. There is the mean in the middle\nand the probability mass,",
    "start": "698840",
    "end": "704510"
  },
  {
    "text": "has a relatively simple shape. It's not something you can use\nto model complicated data sets.",
    "start": "704510",
    "end": "710390"
  },
  {
    "text": "But then you apply, let's\nsay, that first invertible transformation, and you get\na new random variable, Z1,",
    "start": "710390",
    "end": "717465"
  },
  {
    "text": "which now has a more\ncomplicated density. Then you apply another one,\nand you get something even more",
    "start": "717465",
    "end": "723149"
  },
  {
    "text": "complicated. And after 10 layers,\nafter 10 individual--",
    "start": "723150",
    "end": "729839"
  },
  {
    "text": "after 10 invertible\ntransformations, you can get something that\nis much more interesting.",
    "start": "729840",
    "end": "734850"
  },
  {
    "text": "And it has the flavor of\na mixture distribution where you can spread out the\nprobability mass in a much more",
    "start": "734850",
    "end": "742380"
  },
  {
    "text": "flexible way. Is there that we can't go to\nthe 10th transformation using",
    "start": "742380",
    "end": "749590"
  },
  {
    "text": "some neural network that can\nrepresent all of them at once? Because if I'm thinking\nabout it in terms of simple functions\nlike matrices,",
    "start": "749590",
    "end": "757750"
  },
  {
    "text": "I can show that [INAUDIBLE]. We just don't see it. Yeah, yeah, so there is\ncertainly a mapping that could--",
    "start": "757750",
    "end": "766300"
  },
  {
    "text": "an invertible mapping that\nwould get you from the beginning to the end, which is\njust the composition of those neural networks.",
    "start": "766300",
    "end": "773680"
  },
  {
    "text": "The beauty is the deep\nlearning strategies that the individual layers are\ngoing to be relatively simple.",
    "start": "773680",
    "end": "780400"
  },
  {
    "text": "So the individual F\ntheta I that we will see are actually relatively\nsimple transformations.",
    "start": "780400",
    "end": "787430"
  },
  {
    "text": "Think about, it's\nnot quite linear but something almost linear. And even though that's simple,\nby stacking them together,",
    "start": "787430",
    "end": "795579"
  },
  {
    "text": "you can get some very\nflexible transformations. So it's similar to a\ndeep neural network, and maybe the individual\nlayers are not",
    "start": "795580",
    "end": "801555"
  },
  {
    "text": "particularly complicated. Maybe it's just a linear\ncombination or a nonlinearity, but if you stack them together,\nyou can get a very flexible map.",
    "start": "801555",
    "end": "809860"
  },
  {
    "text": "And yeah, that's\nwhat's going on here. Yeah? You know how there's a\nconstraint that the weights are",
    "start": "809860",
    "end": "816780"
  },
  {
    "text": "actually invertible themselves? Because you could\nhave a sigmoid, which is like a one-to-one mapping,\nbut what about enforcing",
    "start": "816780",
    "end": "823570"
  },
  {
    "text": "a quadratic assumption? Because I feel that's where\nyou'd actually have issues. Yeah, yeah, so the\nquestion, I think,",
    "start": "823570",
    "end": "829320"
  },
  {
    "text": "is how do you ensure that,\nif you learn these thetas, you get a mapping\nthat is invertible?",
    "start": "829320",
    "end": "835450"
  },
  {
    "text": "And so what we\nwill have to do is we will have to\ndesign architectures in a very special way, such\nthat you're guaranteed that",
    "start": "835450",
    "end": "842130"
  },
  {
    "text": "regardless of how you\nchoose the parameters, the mapping is invertible. And not only that,\nwe'll also need",
    "start": "842130",
    "end": "847899"
  },
  {
    "text": "to be able to invert\nit efficiently, ideally because if you want to-- you\nneed to be able to go both ways.",
    "start": "847900",
    "end": "853630"
  },
  {
    "text": "And that's also not enough. You also need to\nbe able to compute that determinant of the\nJacobian relatively efficiently,",
    "start": "853630",
    "end": "860880"
  },
  {
    "text": "because naively,\nit could take you n cube, where n is the\nnumber of variables, the number of dimensions,\nthe number of pixels.",
    "start": "860880",
    "end": "867790"
  },
  {
    "text": "That's horrible, basically. So that's what's\ngoing to come up next, ways of defining\nthese mappings",
    "start": "867790",
    "end": "873928"
  },
  {
    "text": "and then how to\nlearn them from data, which is going to be trivial\nbecause we have access to the likelihood. ",
    "start": "873928",
    "end": "881450"
  },
  {
    "text": "Right, and so here's\nanother example. This is different, what\nyou see at the bottom.",
    "start": "881450",
    "end": "887520"
  },
  {
    "text": "Same idea, but the\nprior is uniform. So here the prior is Gaussian,\nand we transform it to something",
    "start": "887520",
    "end": "894560"
  },
  {
    "text": "like this. Here the prior is a\nuniform, random variable. Again, two-dimensional. So all the probability mass\nis, let's say, between 0101,",
    "start": "894560",
    "end": "904640"
  },
  {
    "text": "so it's like a square. And then by applying these\ninvertible transformations, you are able to map it to,\nagain, something much more",
    "start": "904640",
    "end": "913279"
  },
  {
    "text": "interesting. And you can see,\nso it's normalizing because each individual\nrandom variable that you",
    "start": "913280",
    "end": "921660"
  },
  {
    "text": "get by applying an\ninvertible transformation is automatically normalized by\nthe change of variable formula.",
    "start": "921660",
    "end": "927190"
  },
  {
    "text": "And it's a flow because you're\napplying many transformations, one after the other.",
    "start": "927190",
    "end": "932350"
  },
  {
    "text": "So it's like your probability\nmass is flowing around through these transformations.",
    "start": "932350",
    "end": "938810"
  },
  {
    "text": "In the examples we showed\non each transformation, are they the same\ntransformation,",
    "start": "938810",
    "end": "943843"
  },
  {
    "text": "or are they different? Do you apply each\ndifferent transformation? Yeah, good question. So this is a planar\nflow, which is",
    "start": "943843",
    "end": "949280"
  },
  {
    "text": "one way of defining\nan invertible layer through a neural network.",
    "start": "949280",
    "end": "954330"
  },
  {
    "text": "And so the functional form\nis the same at every layer, but the parameters\nare different,",
    "start": "954330",
    "end": "959810"
  },
  {
    "text": "like what was asked before. So it's like the same transform. It's the same layer but\nwith different parameters.",
    "start": "959810",
    "end": "967170"
  },
  {
    "text": " And yeah, so you\ncan see the takeaway",
    "start": "967170",
    "end": "974095"
  },
  {
    "text": "is, this is the intuition. This is the only thing\nthat is easy to visualize, but you can imagine\nwe're going to try",
    "start": "974095",
    "end": "979930"
  },
  {
    "text": "to do something similar over a\nmuch higher dimensional space where we're going to\ntry to model, let's say,",
    "start": "979930",
    "end": "986020"
  },
  {
    "text": "images on the right-hand side.",
    "start": "986020",
    "end": "991610"
  },
  {
    "text": "Cool, so how do we do-- the first thing is, well, we\nneed to parameterize, somehow,",
    "start": "991610",
    "end": "996829"
  },
  {
    "text": "this mapping, and\nthat's going to be the main topic of this lecture. The other thing\nthat we need to do",
    "start": "996830",
    "end": "1003370"
  },
  {
    "text": "is we need to be\nable to do learning. So once you've defined a\nspace of invertible mappings,",
    "start": "1003370",
    "end": "1008590"
  },
  {
    "text": "how do you choose these\nparameters, theta, ideally to fit some data distribution?",
    "start": "1008590",
    "end": "1014860"
  },
  {
    "text": "And it turns out that\nthat's very easy. Because we have access\nto the likelihood, we can basically\ndo the same thing",
    "start": "1014860",
    "end": "1021100"
  },
  {
    "text": "that we did for\nautoregressive models. So the most natural way\nof training a flow model",
    "start": "1021100",
    "end": "1027040"
  },
  {
    "text": "is to just pick\nparameters, theta, that maximize the probability\nof a given data set or the log",
    "start": "1027040",
    "end": "1034480"
  },
  {
    "text": "probability of a\nparticular data set. Or equivalently, you go\nthrough your data set D,",
    "start": "1034480",
    "end": "1040599"
  },
  {
    "text": "and you try to find\nparameters that maximize the probability assigned to--",
    "start": "1040599",
    "end": "1045760"
  },
  {
    "text": "or the log, the average\nlog probability, across all the data\npoints in your data set.",
    "start": "1045760",
    "end": "1051170"
  },
  {
    "text": "So intuitively, you're\ntrying to find-- you have a bunch of\ndata points, which",
    "start": "1051170",
    "end": "1056240"
  },
  {
    "text": "you can think of\npoints in this space, and then you're trying to find\nthe parameters of the flows",
    "start": "1056240",
    "end": "1061940"
  },
  {
    "text": "to put as much probability\nmass around the data points that you have\nin the training set.",
    "start": "1061940",
    "end": "1070169"
  },
  {
    "text": "And the good thing,\nagain, is that unlike a variational\nautoencoder,",
    "start": "1070170",
    "end": "1075299"
  },
  {
    "text": "we can actually evaluate\nthis likelihood. We can figure out what was\nthe probability of generating",
    "start": "1075300",
    "end": "1081660"
  },
  {
    "text": "any particular data point,\nX. All you have to do is you use the usual formula of\nthe change of variable formula.",
    "start": "1081660",
    "end": "1088440"
  },
  {
    "text": "So you take the data\npoint, you invert it, you find the probability with\nrespect to the prior, which",
    "start": "1088440",
    "end": "1095280"
  },
  {
    "text": "is whatever something simple. PZ is, again, like what\nyou have here on the left. It could be a Gaussian.",
    "start": "1095280",
    "end": "1101050"
  },
  {
    "text": "It could be uniform,\nsomething simple. And then you account for that\ndeterminant of the Jacobian.",
    "start": "1101050",
    "end": "1108850"
  },
  {
    "text": "And because it's\na log of product, it becomes a sum of logs. And so again, all you have\nto do is to basically--",
    "start": "1108850",
    "end": "1117539"
  },
  {
    "text": "well, maybe it's on the next--\nno, it's not on this slide. But basically, you have to\nfigure out how much the--",
    "start": "1117540",
    "end": "1124551"
  },
  {
    "text": "what is the log\ndeterminant of the Jacobian for the full transformation? Which can also be broken\ndown into the log determinant",
    "start": "1124551",
    "end": "1131590"
  },
  {
    "text": "of the Jacobians of\nthe individual pieces define your flow model. ",
    "start": "1131590",
    "end": "1138610"
  },
  {
    "text": "And then what you do is if\nyou can evaluate this loss,",
    "start": "1138610",
    "end": "1143995"
  },
  {
    "text": "or I guess, this is not\na loss, because we're trying to maximize this, but if\nyou can evaluate this function",
    "start": "1143995",
    "end": "1151750"
  },
  {
    "text": "as a function of theta,\nthen you can take gradients, and you can try to\noptimize it, essentially.",
    "start": "1151750",
    "end": "1158580"
  },
  {
    "text": "And so to the extent that\nyou can invert this function and to the extent that you\ncan evaluate those Jacobian",
    "start": "1158580",
    "end": "1167610"
  },
  {
    "text": "determinant, Jacobian\nterm efficiently, we have a loss that\nwe can try to optimize as a function of theta.",
    "start": "1167610",
    "end": "1175680"
  },
  {
    "text": "And so you can do exact\nloss likelihood evaluation",
    "start": "1175680",
    "end": "1181380"
  },
  {
    "text": "by using this inverse mapping,\nso go from theta to prior, and then after you've\ntrained the model,",
    "start": "1181380",
    "end": "1187650"
  },
  {
    "text": "you can generate new samples. So you want to\ngenerate new images or you want to generate\nnew speech, or sound,",
    "start": "1187650",
    "end": "1194910"
  },
  {
    "text": "or whatever you're\ntrying to model, then we know how to do it. It's just basically just\nthe forward direction,",
    "start": "1194910",
    "end": "1200230"
  },
  {
    "text": "just like in a VA\nthat has not changed. You sample Z from\nthe prior, and then",
    "start": "1200230",
    "end": "1205440"
  },
  {
    "text": "you transform it\nthrough your mapping. And that produces an output.",
    "start": "1205440",
    "end": "1210934"
  },
  {
    "text": " And if you care about getting\nlatent representations,",
    "start": "1210935",
    "end": "1220130"
  },
  {
    "text": "like in a VA, in a VA,\nyou would use the encoder to try to infer z given\nX. In a flow model,",
    "start": "1220130",
    "end": "1226550"
  },
  {
    "text": "it's relatively, again,\neasy to figure out what is the corresponding\nZ for any particular x.",
    "start": "1226550",
    "end": "1232880"
  },
  {
    "text": "All you have to do is you\nhave to invert the mapping. But again, it's\nquestionable, whether this",
    "start": "1232880",
    "end": "1238340"
  },
  {
    "text": "can be thought as\na latent variable, because it has the same\ndimension as the data. And so it's not going to\nbe compressed in any way.",
    "start": "1238340",
    "end": "1245965"
  },
  {
    "text": " Yeah?",
    "start": "1245965",
    "end": "1251070"
  },
  {
    "text": "Because there surely is\nsome structure, right? Because after all,\neven if it's one-to-one",
    "start": "1251070",
    "end": "1257580"
  },
  {
    "text": "it's no longer random? So there might be something. Yeah, so I'll show you\ntraining models on images,",
    "start": "1257580",
    "end": "1266170"
  },
  {
    "text": "then you can do interpolation\nin the latent space, and you get reasonable results. So it's certainly doing\nsomething meaningful,",
    "start": "1266170",
    "end": "1273120"
  },
  {
    "text": "but it might not be compressive\nas you would expect a VA, for example.",
    "start": "1273120",
    "end": "1278500"
  },
  {
    "text": "So it's a different\nkind of latent variable, but it's still a latent\nvariable for sure. ",
    "start": "1278500",
    "end": "1286900"
  },
  {
    "text": "OK, Yeah? So do we parameterize both the\nF theta and the inverse process?",
    "start": "1286900",
    "end": "1296650"
  },
  {
    "text": "So good question. Do we parameterize\nF theta, or do we parameterize F theta inverse?",
    "start": "1296650",
    "end": "1302410"
  },
  {
    "text": "You only parameterize\none because the other one is obtained directly\nby-- hopefully,",
    "start": "1302410",
    "end": "1309220"
  },
  {
    "text": "it's really invertible,\nand so hopefully, you can actually do it. But it's a good question,\nwhether you should parameterize",
    "start": "1309220",
    "end": "1316179"
  },
  {
    "text": "F theta, the direction\nthat you need for sampling, or should you directly\nparameterize the inverse,",
    "start": "1316180",
    "end": "1323860"
  },
  {
    "text": "because that's what you\nneed during training? And so those are\ntwo valid choices.",
    "start": "1323860",
    "end": "1329270"
  },
  {
    "text": "And there might be-- if you have to, let's say,\nnumerically invert a-- maybe you know it's invertible,\nbut maybe it's not cheap,",
    "start": "1329270",
    "end": "1337690"
  },
  {
    "text": "and that maybe\ncomputing an inverse requires you to solve a\nlinear system of equations",
    "start": "1337690",
    "end": "1343140"
  },
  {
    "text": "or something. It's invertible. It's possible to compute\nthis F theta minus inverse,",
    "start": "1343140",
    "end": "1349710"
  },
  {
    "text": "but it's maybe too expensive\nif you have to do this over and over during training.",
    "start": "1349710",
    "end": "1354929"
  },
  {
    "text": "Maybe depending on\nwhat you want to do, you might want to\nparameterize one or the other, or you might choose\nan F theta that",
    "start": "1354930",
    "end": "1361050"
  },
  {
    "text": "can be inverted very quickly. And so we'll see the\nkind of trade-offs",
    "start": "1361050",
    "end": "1366510"
  },
  {
    "text": "that you get by doing\none or the other. F theta, the inverse, have the\nsame form as [INAUDIBLE] loss",
    "start": "1366510",
    "end": "1377730"
  },
  {
    "text": "between them to ensure that\nthey were roughly the same? Is that something we can do? Yeah, so the question is, well,\nwhat if it's not quite fully",
    "start": "1377730",
    "end": "1385400"
  },
  {
    "text": "invertible, or could\nyou parameterize both and try to make one the\ninverse of the other? People have explored these kind\nof things, where then, they",
    "start": "1385400",
    "end": "1392850"
  },
  {
    "text": "try to make sure that you\ncan do both directions, and what's the other way\nof distilling models that",
    "start": "1392850",
    "end": "1399570"
  },
  {
    "text": "can be efficiently\nevaluated in one direction into ones that can be\nefficiently evaluated",
    "start": "1399570",
    "end": "1405630"
  },
  {
    "text": "in the other direction? So yeah, we'll talk a\nlittle bit about this. ",
    "start": "1405630",
    "end": "1412990"
  },
  {
    "text": "Cool, all right, so what do\nwe want from a flow model? We have a simple prior that you\ncan sample from efficiently,",
    "start": "1412990",
    "end": "1419920"
  },
  {
    "text": "and you can evaluate\nprobabilities, because we need that PZ here.",
    "start": "1419920",
    "end": "1424930"
  },
  {
    "text": "When you do this\nformula, you need to be able to evaluate\nprobabilities under the prior. So typically, something\nlike a Gaussian is used.",
    "start": "1424930",
    "end": "1433630"
  },
  {
    "text": "We need invertible mappings\nthat can be tractably evaluated. So if you want to\nevaluate likelihoods,",
    "start": "1433630",
    "end": "1440590"
  },
  {
    "text": "you need to be able\nto go from, let's say, X to Z efficiently or as\nefficiently as possible.",
    "start": "1440590",
    "end": "1447280"
  },
  {
    "text": "But if you want to sample, then\nyou need to do the opposite. So again, going back to what\nwe were just talking about,",
    "start": "1447280",
    "end": "1453487"
  },
  {
    "text": "two things, depending\non what you want to do or depending which one you want\nit to be as fast as possible, you might want to\ndo one or the other.",
    "start": "1453487",
    "end": "1460895"
  },
  {
    "text": "And then the other\nbig thing is that we need to be able to compute\nthis determinant of Jacobians.",
    "start": "1460895",
    "end": "1466420"
  },
  {
    "text": "And these Jacobian\nmatrices are pretty big. They are like n by n, where\nn is the data dimensionality.",
    "start": "1466420",
    "end": "1474580"
  },
  {
    "text": "And if you recall, computing\nthe determinant of a generic n",
    "start": "1474580",
    "end": "1481809"
  },
  {
    "text": "by n matrix takes order\nof n cube operations. So even if n is relatively\nsmall, like 1000,",
    "start": "1481810",
    "end": "1492160"
  },
  {
    "text": "this is super expensive. So computing these kind\nof determinants naively is very, very tricky.",
    "start": "1492160",
    "end": "1499210"
  },
  {
    "text": "And so what we'll\nhave to do is we'll have to choose\ntransformations, such that not only they\nare invertible,",
    "start": "1499210",
    "end": "1505299"
  },
  {
    "text": "but the Jacobian has\na special structure. So then we can compute the\ndeterminant more efficiently.",
    "start": "1505300",
    "end": "1512070"
  },
  {
    "text": "And the simplest\nway of doing it is to choose matrices that\nare basically triangular,",
    "start": "1512070",
    "end": "1519570"
  },
  {
    "text": "because in that case, then you\ncan compute the determinant in basically linear time.",
    "start": "1519570",
    "end": "1526090"
  },
  {
    "text": "You just multiply\ntogether the entries on the diagonal of the matrix.",
    "start": "1526090",
    "end": "1531510"
  },
  {
    "text": "And so one way to do it is to\nbasically define the function F,",
    "start": "1531510",
    "end": "1539685"
  },
  {
    "text": "such that basically,\nthe Jacobian-- we want to make sure-- we want to\ndefine a function, F, basically,",
    "start": "1539685",
    "end": "1545790"
  },
  {
    "text": "which again, is as n inputs\nand then outputs, such that the corresponding\nJacobian, which",
    "start": "1545790",
    "end": "1552240"
  },
  {
    "text": "is this matrix of partial\nderivatives, is triangular. So there needs to be a lot of\n0s, basically, in the matrix.",
    "start": "1552240",
    "end": "1561659"
  },
  {
    "text": "And one way to do\nit-- and recall, the Jacobian looks like this. So this is a function.",
    "start": "1561660",
    "end": "1568800"
  },
  {
    "text": "F is a vector-valued function. It has n different outputs. So there is like n functions, F\nscalar functions, F1 through Fn.",
    "start": "1568800",
    "end": "1576670"
  },
  {
    "text": "And the Jacobian requires\nyou to compute, basically, the gradients with\nrespect to the inputs",
    "start": "1576670",
    "end": "1581980"
  },
  {
    "text": "of each individual function. So you can think of\neach of these columns",
    "start": "1581980",
    "end": "1587380"
  },
  {
    "text": "as being the gradient of\na scalar valued function with respect to the inputs, now\nwith respect to the parameters.",
    "start": "1587380",
    "end": "1594580"
  },
  {
    "text": "And a triangular\nmatrix is basically a matrix where all the elements\nabove the diagonal, let's say,",
    "start": "1594580",
    "end": "1602800"
  },
  {
    "text": "are 0. And so any guess on, how\ndo we make, let's say,",
    "start": "1602800",
    "end": "1609670"
  },
  {
    "text": "the derivative of F1\nwith respect to Zn 0?",
    "start": "1609670",
    "end": "1615100"
  },
  {
    "text": "Yeah, that doesn't depend on it. So if you choose the\ncomputation graph such that the Ith output only\ndepends on the previous inputs,",
    "start": "1615100",
    "end": "1627610"
  },
  {
    "text": "like in an autoregressive\nmodel, then by definition,",
    "start": "1627610",
    "end": "1632710"
  },
  {
    "text": "a lot of the derivatives\nare going to be 0, and you get a matrix that has\nthe right kind of structure.",
    "start": "1632710",
    "end": "1639220"
  },
  {
    "text": "So it's lower triangular. And if it's lower\ntriangular, you can get the determinant\njust by multiplying together",
    "start": "1639220",
    "end": "1647970"
  },
  {
    "text": "all the entries on the diagonal. And there is n of them, and\nso it becomes linear time.",
    "start": "1647970",
    "end": "1652995"
  },
  {
    "text": " And so that's one way of\ngetting efficient efficiency",
    "start": "1652995",
    "end": "1663179"
  },
  {
    "text": "on this type of operation, is\nto choose the computation graph, such that it reminds us a little\nbit of autoregressive models,",
    "start": "1663180",
    "end": "1672965"
  },
  {
    "text": "in the sense that\nthere is an ordering, and then the Ith output only\ndepends on the inputs that come",
    "start": "1672965",
    "end": "1680970"
  },
  {
    "text": "before it in this ordering. ",
    "start": "1680970",
    "end": "1688600"
  },
  {
    "text": "And of course, you can also\nmake it upper triangular. So if Xi, the I\noutput, only depends",
    "start": "1688600",
    "end": "1696240"
  },
  {
    "text": "on entries of the inputs\nthat come after it, then you're going to\nget a matrix that is",
    "start": "1696240",
    "end": "1701430"
  },
  {
    "text": "going to be upper triangular. And that's also something\nthat you can evaluate the determinant in linear time.",
    "start": "1701430",
    "end": "1707470"
  },
  {
    "text": " So just to recap,\nnormalizing flows,",
    "start": "1707470",
    "end": "1716789"
  },
  {
    "text": "transform simple distribution\nto complex with a sequence of invertible transformations. You can think of it\nas a latent variable",
    "start": "1716790",
    "end": "1723510"
  },
  {
    "text": "model with exact\nlikelihood evaluation. We need invertible\nmappings and somehow",
    "start": "1723510",
    "end": "1731520"
  },
  {
    "text": "Jacobians that have special\nstructure so that we can compute the determinant of the\nJacobian and the change",
    "start": "1731520",
    "end": "1737159"
  },
  {
    "text": "of variable formula efficiently. And what we're\ngoing to see today is various ways of achieving it.",
    "start": "1737160",
    "end": "1743010"
  },
  {
    "text": "There is a lot of different\nneural network architectures or layers that you can\nbasically use that achieve",
    "start": "1743010",
    "end": "1751409"
  },
  {
    "text": "these sort of properties. And the simplest one is\nsomething called NICE,",
    "start": "1751410",
    "end": "1759299"
  },
  {
    "text": "and then here you can see more. Let's see, NICE. The simplest way of doing\nthis is something like this.",
    "start": "1759300",
    "end": "1765340"
  },
  {
    "text": "It's an additive coupling layer. So what you do is you\npartition this Z variables",
    "start": "1765340",
    "end": "1773350"
  },
  {
    "text": "into two groups. Again, there is an ordering\nof the Z variables, and you take the first D,\nZ1 through the D and then",
    "start": "1773350",
    "end": "1781790"
  },
  {
    "text": "the remaining n\nminus D. So we have two groups of the Z\nvariables, and you",
    "start": "1781790",
    "end": "1787400"
  },
  {
    "text": "pick a D. Can be anything. And then to define the\nforward mapping that gets you",
    "start": "1787400",
    "end": "1793669"
  },
  {
    "text": "from Z to X, what you do\nis you keep the first D",
    "start": "1793670",
    "end": "1798680"
  },
  {
    "text": "components unchanged, so\nyou just pass them through. ",
    "start": "1798680",
    "end": "1804630"
  },
  {
    "text": "And then you modify the\nremaining components, the remaining n\nminus D components,",
    "start": "1804630",
    "end": "1810840"
  },
  {
    "text": "in the simplest possible way,\nwhich is just shift them. So there is a\nneural network which",
    "start": "1810840",
    "end": "1816960"
  },
  {
    "text": "can be basically arbitrary,\nwhich I'm calling m theta, which takes the first D\ninputs to this layer",
    "start": "1816960",
    "end": "1826200"
  },
  {
    "text": "and computes n minus\nD shifts that then you apply to the remaining\nn minus DZ variables.",
    "start": "1826200",
    "end": "1835700"
  },
  {
    "text": "So you can see that the first\nD components remain the same. You just pass them through.",
    "start": "1835700",
    "end": "1840800"
  },
  {
    "text": "The remaining n\nminus D components, you obtain them just by shifting\nthe inputs by a little bit.",
    "start": "1840800",
    "end": "1846715"
  },
  {
    "text": " And by how much you\nshift the inputs, it can be a learnable parameter.",
    "start": "1846715",
    "end": "1854420"
  },
  {
    "text": "Now, is this mapping invertible? It's pretty easy to see\nthat it's invertible.",
    "start": "1854420",
    "end": "1859700"
  },
  {
    "text": "And how do you get the Z\nif you had access to the x?",
    "start": "1859700",
    "end": "1865519"
  },
  {
    "text": "So how do you get Z1 through\nD if you have access to the X?",
    "start": "1865520",
    "end": "1872720"
  },
  {
    "text": "Well, the first D\ncomponents are not changed, so you just keep them. It's just, again, the\nidentity transformation.",
    "start": "1872720",
    "end": "1880190"
  },
  {
    "text": "How do we get the\nremaining, if you want to compute Z, D,\nthe n minus D inputs?",
    "start": "1880190",
    "end": "1888919"
  },
  {
    "text": "Given all the outputs,\nhow do you get them? You just basically subtract.",
    "start": "1888920",
    "end": "1896520"
  },
  {
    "text": "You just reverse this thing. You just write a Z\nequals X minus m theta.",
    "start": "1896520",
    "end": "1902760"
  },
  {
    "text": "And crucially, we\ncan't compute m theta, because we know that X-- because the first D component\nin the input and the output",
    "start": "1902760",
    "end": "1910980"
  },
  {
    "text": "are the same, so when\nwe do the inversion, we know by how much\nwe should shift, because we're just passing\nthrough the first D components.",
    "start": "1910980",
    "end": "1919470"
  },
  {
    "text": "And so we can apply this\nshift by doing this. You can just\nsubtract off m theta,",
    "start": "1919470",
    "end": "1925710"
  },
  {
    "text": "and we can compute m\ntheta as a function of X1 through D, because X1\nthrough D is the same as Z1",
    "start": "1925710",
    "end": "1931590"
  },
  {
    "text": "through D, which is what we\nused to compute the shift. ",
    "start": "1931590",
    "end": "1937289"
  },
  {
    "text": "And m theta here can be\nan arbitrary function, can be an arbitrary\nneural network, basically.",
    "start": "1937290",
    "end": "1942363"
  },
  {
    "text": " Now, what do we need to--",
    "start": "1942363",
    "end": "1949718"
  },
  {
    "text": "the other thing we\nneed to figure out is, can we compute the Jacobian\nof the forward mapping?",
    "start": "1949718",
    "end": "1955610"
  },
  {
    "text": "So what are the matrices\nof partial derivatives?",
    "start": "1955610",
    "end": "1961170"
  },
  {
    "text": "It has the right\ntriangular structure. And you can see that the\nonly thing that we're doing",
    "start": "1961170",
    "end": "1968100"
  },
  {
    "text": "is shifting. And so when you look at the\npartial derivatives of what happens on the\ndiagonal, it's just all",
    "start": "1968100",
    "end": "1974820"
  },
  {
    "text": "going to be identity matrices.  So if you look at, how does the\nfunction on the second line here",
    "start": "1974820",
    "end": "1987210"
  },
  {
    "text": "depend on the various\nZ's, on the later Z's? You see that it's just\na shift, so that matrix",
    "start": "1987210",
    "end": "1993960"
  },
  {
    "text": "of partial derivatives that you\nget here at the bottom right is just another identity matrix.",
    "start": "1993960",
    "end": "2001830"
  },
  {
    "text": "So what this means is that,\nwhat is the determinant of the Jacobian of this matrix?",
    "start": "2001830",
    "end": "2007355"
  },
  {
    "text": " 1. Just 1. It's the product of all the\nelements on the diagonal.",
    "start": "2007355",
    "end": "2014280"
  },
  {
    "text": "They are all 1s, so the\ndeterminant of the Jacobian is 1, which means that\nit's trivial to compute",
    "start": "2014280",
    "end": "2020380"
  },
  {
    "text": "this term in the change\nof variable formula. It seems like Z and\nX are so connected.",
    "start": "2020380",
    "end": "2027370"
  },
  {
    "text": "How can we find an\neasy distribution from where we can\nsample, will Z ever take",
    "start": "2027370",
    "end": "2033280"
  },
  {
    "text": "a Gaussian form [INAUDIBLE]? Yeah, so great question. Is this flexible enough?",
    "start": "2033280",
    "end": "2038870"
  },
  {
    "text": "I'll show you some empirical\nresults that this model is not the best one. It's probably the\nsimplest you can think of,",
    "start": "2038870",
    "end": "2045190"
  },
  {
    "text": "but it's already quite powerful. You can actually already use\nthis to model images, which is pretty surprising,\nbecause it means",
    "start": "2045190",
    "end": "2050710"
  },
  {
    "text": "that by stacking a sufficiently\nlarge number of these very simple layers, you\ncan actually transform",
    "start": "2050710",
    "end": "2056289"
  },
  {
    "text": "a complicated distribution\nover images into, let's say, a Gaussian. Yeah?",
    "start": "2056290",
    "end": "2061629"
  },
  {
    "text": "What if you have [INAUDIBLE]\nyou put some entries over 0, diagonal is always 0?",
    "start": "2061630",
    "end": "2067989"
  },
  {
    "text": "Say again? What if we have a diagonal\nentry that's just 0, then the term is always 0?",
    "start": "2067989",
    "end": "2073164"
  },
  {
    "text": "So is that-- It basically means that\nit's not invertible, if you have zeros on\nthe diagonal, basically.",
    "start": "2073164",
    "end": "2079412"
  },
  {
    "text": " And now this is called a volume\npreserving transformation,",
    "start": "2079413",
    "end": "2087719"
  },
  {
    "text": "because recall that basically,\nif the determinant is 1, it means that you're not\nexpanding that unit hypercube",
    "start": "2087719",
    "end": "2094530"
  },
  {
    "text": "or you're not shrinking it. It just stays the same. But you can move around\nprobability mass.",
    "start": "2094530",
    "end": "2102150"
  },
  {
    "text": "And now the final component\nthat you use in this model called NICE is rescaling.",
    "start": "2102150",
    "end": "2109650"
  },
  {
    "text": "So you can basically imagine\nmany of these coupling layers where you can change the\nordering of the variables",
    "start": "2109650",
    "end": "2116700"
  },
  {
    "text": "in-between. So you don't have to keep the\nsame ordering in every layer. You can pick them--",
    "start": "2116700",
    "end": "2122640"
  },
  {
    "text": "any order you want\nis fine, as long as you satisfy that kind of\nproperty that we had before.",
    "start": "2122640",
    "end": "2128520"
  },
  {
    "text": "And then the final\nlayer is rescaling. So again, something\nsuper simple.",
    "start": "2128520",
    "end": "2134530"
  },
  {
    "text": "You just element Y as\na of all the entries, with some parameters, Si,\nwhich are going to be learned,",
    "start": "2134530",
    "end": "2142470"
  },
  {
    "text": "and it's just a\nscaling that you apply. So again, the simplest\ntransformation you",
    "start": "2142470",
    "end": "2149050"
  },
  {
    "text": "can think of, what\nis the inverse? Again, you just divide\nby 1 over S, so trivial.",
    "start": "2149050",
    "end": "2156340"
  },
  {
    "text": "To compute the inverse\nmapping and the determinant of the Jacobian, well,\nif you think about it,",
    "start": "2156340",
    "end": "2163570"
  },
  {
    "text": "the matrix of\npartial derivatives is a diagonal matrix. On the elements on the\ndiagonal are these Si terms.",
    "start": "2163570",
    "end": "2172520"
  },
  {
    "text": "And so what is the determinant\nof this diagonal matrix?",
    "start": "2172520",
    "end": "2178020"
  },
  {
    "text": "It's just going to be, again,\nthe product of all these Si's, basically, so yeah.",
    "start": "2178020",
    "end": "2183690"
  },
  {
    "text": "And you might think\nthis is super simple. How can you possibly\nlearn something",
    "start": "2183690",
    "end": "2190590"
  },
  {
    "text": "useful with a model like this? But if you stack\nenough of these layers, you can actually learn\nsome decent models.",
    "start": "2190590",
    "end": "2198280"
  },
  {
    "text": "And so if you train\na NICE model on MNIST and then you generate\nsamples, they look like this.",
    "start": "2198280",
    "end": "2203880"
  },
  {
    "text": "You train it on faces, you can\nget samples that look like that. So not the best generative\nmodel, of course,",
    "start": "2203880",
    "end": "2211200"
  },
  {
    "text": "but it's already\nsomewhat promising that something so simple\nalready figured out",
    "start": "2211200",
    "end": "2216570"
  },
  {
    "text": "how to map a complex\ndistribution over pixels into a Gaussian,\nbasically, just by stacking",
    "start": "2216570",
    "end": "2223110"
  },
  {
    "text": "a sufficiently large number\nof simple coupling layers like the one we saw before. ",
    "start": "2223110",
    "end": "2231180"
  },
  {
    "text": "Yeah? What would be the\nZ that we would use for generation in this case? It depends what you\nchoose for the prior.",
    "start": "2231180",
    "end": "2237420"
  },
  {
    "text": "In this model, you\nwould typically use a Gaussian univariate,\na unit Gaussian.",
    "start": "2237420",
    "end": "2244630"
  },
  {
    "text": "So that's what you would use. An image of the same dimensions? Same dimension, every\nentry is Gaussian.",
    "start": "2244630",
    "end": "2251890"
  },
  {
    "text": "And if you have unit\ncovariance, then you can just sample each\ncomponent independently. And then so you start\nwith pure noise,",
    "start": "2251890",
    "end": "2259319"
  },
  {
    "text": "then you fit it through the,\nI guess, the inverse mapping, which we know how to\ncompute, because we",
    "start": "2259320",
    "end": "2264750"
  },
  {
    "text": "know how to-- if you\nwant to just invert layer by layer, the final\nlayer, you invert it",
    "start": "2264750",
    "end": "2269970"
  },
  {
    "text": "like this, the previous\nlayers, you invert them by applying this\ntransformation, and then",
    "start": "2269970",
    "end": "2277680"
  },
  {
    "text": "you gradually turn that noise\ninto an image, essentially. ",
    "start": "2277680",
    "end": "2285310"
  },
  {
    "text": "Yeah? The first half of its\nidentical transformation means regenerates the image, which\nbasically provide the first half",
    "start": "2285310",
    "end": "2293859"
  },
  {
    "text": "as given information? Yeah, the question is because\nthe first D components are not",
    "start": "2293860",
    "end": "2301869"
  },
  {
    "text": "changed, then yeah, we're\nbasically passing them through. And it doesn't have to be half. It can be any D. It can be an\narbitrary fraction that you're",
    "start": "2301870",
    "end": "2311410"
  },
  {
    "text": "basically keeping\nunchanged, and then you modify the rest by\njust shifting, essentially.",
    "start": "2311410",
    "end": "2316560"
  },
  {
    "text": " So that's, perhaps, the\nsimpler, and here you",
    "start": "2316560",
    "end": "2322300"
  },
  {
    "text": "can see other examples. If you train it on\na data set of SVHN, these are house numbers\ntrained on CIFAR-10.",
    "start": "2322300",
    "end": "2331839"
  },
  {
    "text": "Again, not the best samples,\nbut it's doing something.",
    "start": "2331840",
    "end": "2337150"
  },
  {
    "text": "You can kind of see numbers here\nin different colors on the left, kind of see samples\non the right.",
    "start": "2337150",
    "end": "2342990"
  },
  {
    "text": " Now, what's the natural\nextension of this?",
    "start": "2342990",
    "end": "2350320"
  },
  {
    "text": "Instead of just shifting,\nwe can shift and scale. And that's a much more powerful\nmodel called RealNVP, which",
    "start": "2350320",
    "end": "2358510"
  },
  {
    "text": "is basically essentially\nidentical to NICE, except that at every\nlayer, we don't just shift,",
    "start": "2358510",
    "end": "2365710"
  },
  {
    "text": "but we shift and scale. So the forward mapping\nis like before.",
    "start": "2365710",
    "end": "2372020"
  },
  {
    "text": "We pass through D components,\nso we apply an identity",
    "start": "2372020",
    "end": "2377570"
  },
  {
    "text": "transformation. And then for the remaining\nones, instead of just shifting,",
    "start": "2377570",
    "end": "2384710"
  },
  {
    "text": "we shift, which is now\nthis neural network, new theta, which is the\nsame m that I had before.",
    "start": "2384710",
    "end": "2392150"
  },
  {
    "text": "But now we also, element-wise,\nscale each entry. And again, the\nscaling coefficients",
    "start": "2392150",
    "end": "2398720"
  },
  {
    "text": "are allowed to depend in a\ncomplicated way on the first D components, as long\nas-- and I'm and I'm",
    "start": "2398720",
    "end": "2406330"
  },
  {
    "text": "taking an exponential\nhere so that I'm guaranteed that these\nscaling factors are non-0",
    "start": "2406330",
    "end": "2412600"
  },
  {
    "text": "and then I can invert them, but\nessentially, these matrices, mu theta, alpha, theta can be any.",
    "start": "2412600",
    "end": "2417980"
  },
  {
    "text": " How do we invert the mapping?",
    "start": "2417980",
    "end": "2424050"
  },
  {
    "text": " Again, it's the same thing. So yeah, these are neural\nnetworks, arbitrary parameters,",
    "start": "2424050",
    "end": "2433060"
  },
  {
    "text": "basically arbitrary\nneural networks, and they're\nparameterized by theta. And that's what\nyou actually learn.",
    "start": "2433060",
    "end": "2439690"
  },
  {
    "text": "How do we get the\ninverse mapping? How do we get Z from X?",
    "start": "2439690",
    "end": "2445539"
  },
  {
    "text": "Again, the first D components,\nyou don't do anything. You just look them up, and it's,\nagain, an identity mapping.",
    "start": "2445540",
    "end": "2451760"
  },
  {
    "text": "And then for the second\none, you have to figure out, how do you recover Z given X?",
    "start": "2451760",
    "end": "2457480"
  },
  {
    "text": "And so what you do, you\ntake X, you shift it by mu, and then you divide\nby X of alpha,",
    "start": "2457480",
    "end": "2464110"
  },
  {
    "text": "and that gives you the\nZ, element-wise, which",
    "start": "2464110",
    "end": "2469420"
  },
  {
    "text": "equivalently is like this. You take the X,\nyou shift it by mu,",
    "start": "2469420",
    "end": "2474549"
  },
  {
    "text": "and then you multiply\nit by e to the minus alpha, which is the same as\ndividing by e to the alpha,",
    "start": "2474550",
    "end": "2482140"
  },
  {
    "text": "which is dividing\nby the coefficients that you're\nmultiplying for here. ",
    "start": "2482140",
    "end": "2488690"
  },
  {
    "text": "So again, trivial, very easy\nto do the foreword, very easy to do the inverse. What about the determinant\nof the Jacobian?",
    "start": "2488690",
    "end": "2498349"
  },
  {
    "text": "What does the\nJacobian look like? Again, it has the nice property\nthat it is lower triangular.",
    "start": "2498350",
    "end": "2506480"
  },
  {
    "text": "And now it's a little\nbit more complicated. The way you operate on this Z is\nbecause now, there is a shift.",
    "start": "2506480",
    "end": "2516520"
  },
  {
    "text": "There is a scale, which is\nlike the last layer of NICE, except that it's learned. And so it's like what\nwe were doing before.",
    "start": "2516520",
    "end": "2523290"
  },
  {
    "text": "Before, we were just shifting. Now we're shifting and\nscaling, and so you have all these extra\nscaling factors",
    "start": "2523290",
    "end": "2530190"
  },
  {
    "text": "that you're applying\nto the last n minus D dimensions of the data. ",
    "start": "2530190",
    "end": "2538817"
  },
  {
    "text": "And again, this is\njust what you would get if you applied,\nif you compute partial derivatives of\nthese outputs with respect",
    "start": "2538817",
    "end": "2544630"
  },
  {
    "text": "to the inputs, you're going to\nget this kind of expression. ",
    "start": "2544630",
    "end": "2550990"
  },
  {
    "text": "And how do you get\nthe determinant? Well, you multiply\ntogether a bunch of 1s,",
    "start": "2550990",
    "end": "2558190"
  },
  {
    "text": "and then you multiply\ntogether all the elements of the diagonal, of\nthis diagonal matrix.",
    "start": "2558190",
    "end": "2564203"
  },
  {
    "text": "And so it's just going\nto be the product of the individual\nscaling factors that you apply on\nevery dimension.",
    "start": "2564203",
    "end": "2570670"
  },
  {
    "text": "Or equivalently, it's like\nthe exponential of the sum of these log parameterization.",
    "start": "2570670",
    "end": "2576020"
  },
  {
    "text": " So basically, you can choose\narbitrary neural networks,",
    "start": "2576020",
    "end": "2583460"
  },
  {
    "text": "mu theta, alpha theta. And if you apply that\nkind of transformation,",
    "start": "2583460",
    "end": "2589420"
  },
  {
    "text": "you get something\nthat is invertible. It's easy to compute\nthe forward mapping, it's easy to compute\nthe reverse mapping,",
    "start": "2589420",
    "end": "2595570"
  },
  {
    "text": "and it's easy to figure out by\nhow much it shrinks or expand,",
    "start": "2595570",
    "end": "2601120"
  },
  {
    "text": "like the volume locally,\nwhich is just these scalings. So if the scalings\nare all 1, then it's",
    "start": "2601120",
    "end": "2609310"
  },
  {
    "text": "the same as in a coupling\nlayer that we had before. But because alpha\nthetas are learned,",
    "start": "2609310",
    "end": "2614500"
  },
  {
    "text": "this is strictly more flexible\nthan what we had before, because you can learn to shrink\nor expand certain dimensions.",
    "start": "2614500",
    "end": "2624549"
  },
  {
    "text": "And of course, this is, in\ngeneral, nonvolume preserving, because in general, this\ndeterminant of the Jacobian",
    "start": "2624550",
    "end": "2630610"
  },
  {
    "text": "is not always one,\nso it can do more interesting transformations. ",
    "start": "2630610",
    "end": "2641570"
  },
  {
    "text": "Yeah? [INAUDIBLE] free sample\nthe latent images? So basically saying\nthat, for every Z--",
    "start": "2641570",
    "end": "2648290"
  },
  {
    "text": "like, let's say we\nsample an associated image for each of them? Because to me, it\nseems if you have",
    "start": "2648290",
    "end": "2653390"
  },
  {
    "text": "an invertible\ntransformation, and you need to actually also sample\nyour Z's and associate each",
    "start": "2653390",
    "end": "2659480"
  },
  {
    "text": "of them to an image,\nwould that be an issue over here or [INAUDIBLE]? So to sample, what you would do\nis you would randomly draw a Z,",
    "start": "2659480",
    "end": "2667070"
  },
  {
    "text": "and then you would pass it\nthrough this inverse map, I guess. I'm parameterizing the foreword,\nso you would just pass it",
    "start": "2667070",
    "end": "2674330"
  },
  {
    "text": "through the forward map. During training, you\nincrease sampling or just",
    "start": "2674330",
    "end": "2680420"
  },
  {
    "text": "do it dynamically? So during training,\nyou don't have to--",
    "start": "2680420",
    "end": "2685740"
  },
  {
    "text": "the Z's are computed by\ninverting the mapping. So the Z's are\nobtained from the data.",
    "start": "2685740",
    "end": "2692150"
  },
  {
    "text": "So you just feed\nyour image at the one end of this big,\ninvertible neural network,",
    "start": "2692150",
    "end": "2698850"
  },
  {
    "text": "and then you hopefully\nare able to invert it. And if each individual layer\nhas this kind of shape,",
    "start": "2698850",
    "end": "2703890"
  },
  {
    "text": "then you know how to invert it. And then you get a Z at the end. You evaluate how likely\nthe Z is under the prior,",
    "start": "2703890",
    "end": "2710369"
  },
  {
    "text": "and then you account for\nthe local shift in change that you get through\nthe changeover.",
    "start": "2710370",
    "end": "2716662"
  },
  {
    "text": "So it's not like\ngoing to be an e, where you have to guess\nthe Z. You get it exactly through the invertible mapping.",
    "start": "2716663",
    "end": "2721870"
  },
  {
    "text": " Yeah? Can I say, during inference,\nwhen we sample a Z,",
    "start": "2721870",
    "end": "2731070"
  },
  {
    "text": "and then the generation process\nis actually deterministic? Yes, so the generation\nprocess is deterministic",
    "start": "2731070",
    "end": "2737610"
  },
  {
    "text": "given Z, because the mapping\nitself is deterministic. That's a big\nlimitation, but it's",
    "start": "2737610",
    "end": "2742620"
  },
  {
    "text": "also what gives you\ntractability, basically. ",
    "start": "2742620",
    "end": "2748570"
  },
  {
    "text": "OK, now, this is a model\nthat works much better. Here you can see some examples.",
    "start": "2748570",
    "end": "2755030"
  },
  {
    "text": "Again, this might seem\nlike a very simple kind of transformation that\nyou're applying to the data,",
    "start": "2755030",
    "end": "2760090"
  },
  {
    "text": "but if you train these\nmodels on image data sets, this is starting\nto generate samples",
    "start": "2760090",
    "end": "2766810"
  },
  {
    "text": "that are much better\nin terms of quality. You can see bedrooms or people.",
    "start": "2766810",
    "end": "2774070"
  },
  {
    "text": "These models are pretty decent. They're somewhat\nlow-resolution and everything,",
    "start": "2774070",
    "end": "2779440"
  },
  {
    "text": "but it's generating samples\nthat have the right structure. They're already pretty decent.",
    "start": "2779440",
    "end": "2785500"
  },
  {
    "text": "Which part is [INAUDIBLE]? I think these are\nthe training samples,",
    "start": "2785500",
    "end": "2791680"
  },
  {
    "text": "and these are the generations\nthat you see on the right. So maybe the first row, not\nso good, but for the bedrooms,",
    "start": "2791680",
    "end": "2800630"
  },
  {
    "text": "I think this is [INAUDIBLE]. This is [INAUDIBLE],,\nI think, you can see. It's pretty decent.",
    "start": "2800630",
    "end": "2806680"
  },
  {
    "text": "Yeah? How many layers are these\nmodels using right now? Good question. I don't remember\nhow many of those",
    "start": "2806680",
    "end": "2812337"
  },
  {
    "text": "they use but a\npretty decent number. Yeah. ",
    "start": "2812337",
    "end": "2817950"
  },
  {
    "text": "Yeah? So you mentioned,\nthe different layers",
    "start": "2817950",
    "end": "2824520"
  },
  {
    "text": "will have different parameters,\nso they're not sharing. They're not sharing. Yeah, yeah.",
    "start": "2824520",
    "end": "2831540"
  },
  {
    "text": "And back to the question of,\nwhat do the Z's actually mean?",
    "start": "2831540",
    "end": "2836580"
  },
  {
    "text": "What you can try to do is, you\ncan try to interpolate between different Z's. And what they show\nin this paper is",
    "start": "2836580",
    "end": "2842369"
  },
  {
    "text": "that basically, if you start\nwith the four actual samples, which are shown at the\ncorner of this image here,",
    "start": "2842370",
    "end": "2850170"
  },
  {
    "text": "and then you get the\ncorresponding Z vectors, which are Z1, Z2, Z3, Z4, just\nby inverting the network,",
    "start": "2850170",
    "end": "2857670"
  },
  {
    "text": "and then you\ninterpolate them using this kind of strange\nformula, which is because the latent\nspace is Gaussian,",
    "start": "2857670",
    "end": "2863430"
  },
  {
    "text": "it doesn't matter too much. And then you get new\nZ's, and then you",
    "start": "2863430",
    "end": "2869400"
  },
  {
    "text": "pass them through the forward\nmapping to generate images. You get the reasonable\ninterpolations.",
    "start": "2869400",
    "end": "2875200"
  },
  {
    "text": "You see that you go from\none image from one person to another person, and it slowly\ndrifts from one to the other.",
    "start": "2875200",
    "end": "2885640"
  },
  {
    "text": "And you can see examples here on\nthis buildings, and you can see,",
    "start": "2885640",
    "end": "2891880"
  },
  {
    "text": "so basically, in\neach of these images, the four corners\nare real images. And what you see\nin-between is what",
    "start": "2891880",
    "end": "2897250"
  },
  {
    "text": "you get if you were to\ninterpolate the Z vectors of two real images and then decode\nthem back into an image.",
    "start": "2897250",
    "end": "2905280"
  },
  {
    "text": "So even though, yeah, the latent\nvariables are not compressive, they have the same\nnumber of variables.",
    "start": "2905280",
    "end": "2911440"
  },
  {
    "text": "They have meaningful structure,\nlike as we were discussing before, in the sense that\nif you do interpolations,",
    "start": "2911440",
    "end": "2917190"
  },
  {
    "text": "you get reasonable results. ",
    "start": "2917190",
    "end": "2925290"
  },
  {
    "text": "OK, now, what we can\nsee is that actually,",
    "start": "2925290",
    "end": "2933240"
  },
  {
    "text": "autoregressive models or certain\nkinds of autoregressive models, you can also think of\nthem as normalizing flows.",
    "start": "2933240",
    "end": "2941100"
  },
  {
    "text": "And so just to see\nthis, you can think about an autoregressive\nmodel, a continuous one,",
    "start": "2941100",
    "end": "2950520"
  },
  {
    "text": "where we are defining the\ndensity, the full joint, as a product of conditionals.",
    "start": "2950520",
    "end": "2956520"
  },
  {
    "text": "And let's say that\neach conditional is a Gaussian with\nparameters computed",
    "start": "2956520",
    "end": "2962730"
  },
  {
    "text": "as usual by some\nneural network that takes as input the\nprevious variables and then computes a mean\nand a standard deviation,",
    "start": "2962730",
    "end": "2970330"
  },
  {
    "text": "and that's how\nyou define the Ith conditional in your\nautoregressive model. So this is not the\nlanguage model version",
    "start": "2970330",
    "end": "2977880"
  },
  {
    "text": "where each of these\nconditionals is like a categorical distribution. This is the continuous\nversion where",
    "start": "2977880",
    "end": "2984750"
  },
  {
    "text": "the conditionals\nthemselves are, let's say, Gaussians in this case.",
    "start": "2984750",
    "end": "2991500"
  },
  {
    "text": "And what I'm going\nto show you next is that you can think\nof this, actually, as an autoregressive\nmodel, as a flow model.",
    "start": "2991500",
    "end": "3000110"
  },
  {
    "text": "As defined like this is just\nan autoregressive model. You can think about how\nyou would generate samples",
    "start": "3000110",
    "end": "3006050"
  },
  {
    "text": "from this model. And the way you would generate\nsamples is something like this.",
    "start": "3006050",
    "end": "3011480"
  },
  {
    "text": "You could imagine\ngenerating one sample",
    "start": "3011480",
    "end": "3018950"
  },
  {
    "text": "from a standard normal\ndistribution for every I, for every component, for every\nrandom variable, individual,",
    "start": "3018950",
    "end": "3027410"
  },
  {
    "text": "random variable that\nyou're modeling. And then what you do is, well,\nto sample from the conditionals,",
    "start": "3027410",
    "end": "3034490"
  },
  {
    "text": "you have to-- the conditionals are\nGaussian with certain means and standard deviations.",
    "start": "3034490",
    "end": "3039570"
  },
  {
    "text": "So like using the\nreparameterization trick, you can obtain a\nsample for the--",
    "start": "3039570",
    "end": "3046320"
  },
  {
    "text": "as usual, you sample the\nfirst random variable, then the second sample,\nthe second given the first. And to do that,\nyou need to sample",
    "start": "3046320",
    "end": "3052650"
  },
  {
    "text": "from these Gaussians,\nwhich have certain means and certain standard deviations. So you would generate a\nsample from, let's say,",
    "start": "3052650",
    "end": "3058800"
  },
  {
    "text": "the first pixel or the\nfirst random variable by just shifting and\nscaling this unit standard,",
    "start": "3058800",
    "end": "3068460"
  },
  {
    "text": "random Gaussian,\nrandom variable, which is just Gaussian-distributed.",
    "start": "3068460",
    "end": "3075359"
  },
  {
    "text": "So it's starting to\nlook a little bit like a RealNVP of model, where\nyou have the Z's, and then you",
    "start": "3075360",
    "end": "3081000"
  },
  {
    "text": "shift and scale them. How do you sample X2? Well, you sample X2 given X1.",
    "start": "3081000",
    "end": "3087480"
  },
  {
    "text": "So you take the value of X1. You feed it into these\ntwo neural networks. You compute the mean and\nthe standard deviation",
    "start": "3087480",
    "end": "3094140"
  },
  {
    "text": "of the next conditional,\nand then you sample. And so you do that,\nand then you sample",
    "start": "3094140",
    "end": "3101960"
  },
  {
    "text": "X2 by shifting and scaling\nthis unit, random variable, Z2.",
    "start": "3101960",
    "end": "3108720"
  },
  {
    "text": "Remember that if ZI is\na Gaussian with mean 0",
    "start": "3108720",
    "end": "3114210"
  },
  {
    "text": "and standard deviation 1,\nif you shift it by mu 2 and you rescale it\nby this constant,",
    "start": "3114210",
    "end": "3120690"
  },
  {
    "text": "you get a Gaussian\nwith the right mean and the right\nstandard deviation.",
    "start": "3120690",
    "end": "3126400"
  },
  {
    "text": "And again, this feels a lot\nlike a normalizing flow model",
    "start": "3126400",
    "end": "3131890"
  },
  {
    "text": "that we saw before. Given X1 and X2, two, we\ncan compute the parameters",
    "start": "3131890",
    "end": "3138790"
  },
  {
    "text": "of the next conditional, so a\nmean and a standard deviation, and we can compute, we\ncan sample, the third,",
    "start": "3138790",
    "end": "3145570"
  },
  {
    "text": "let's say, pixel by\nshifting and scaling these basic random variables\nthat we had access to.",
    "start": "3145570",
    "end": "3154320"
  },
  {
    "text": "And so all-in-all, we\ncan think of sampling",
    "start": "3154320",
    "end": "3159750"
  },
  {
    "text": "what you get by sampling from\nthis autoregressive model as a flow in the sense that you\nstart with this random vector",
    "start": "3159750",
    "end": "3167340"
  },
  {
    "text": "of simple normals. And then you shift\nthem and scale them",
    "start": "3167340",
    "end": "3174450"
  },
  {
    "text": "in some interesting way\nusing these conditionals, using these neural\nnetworks, mu and alpha, that",
    "start": "3174450",
    "end": "3181830"
  },
  {
    "text": "define the conditionals in\nthe autoregressive model. ",
    "start": "3181830",
    "end": "3188330"
  },
  {
    "text": "These two, sampling the\nautoregressive model, just by going through\nthe conditionals one",
    "start": "3188330",
    "end": "3194570"
  },
  {
    "text": "at a time is equivalent to\ndoing this, which you can think of as taking a bunch of simple\nrandom variables, ZI's, or just",
    "start": "3194570",
    "end": "3204170"
  },
  {
    "text": "Gaussian independent\nof each other, and then you just feed them\nthrough this interesting transformation to get your\nfinal output, X1 through XN.",
    "start": "3204170",
    "end": "3214010"
  },
  {
    "start": "3214010",
    "end": "3224230"
  },
  {
    "text": "Questions on this? ",
    "start": "3224230",
    "end": "3237539"
  },
  {
    "text": "I see some confused faces. Yes? How can we get all\nof them backwards?",
    "start": "3237540",
    "end": "3244200"
  },
  {
    "text": "Won't we need one? How? Let's see. How do we invert it? Yeah, great question.",
    "start": "3244200",
    "end": "3250180"
  },
  {
    "text": "I think that's\ngoing to come next. The forward mapping,\nyou can think of it",
    "start": "3250180",
    "end": "3256515"
  },
  {
    "text": "as a flow that\nbasically does this. You use the Z's to compute\nthe first X. And then",
    "start": "3256515",
    "end": "3265890"
  },
  {
    "text": "what you do is you compute\nthe new parameters, and then you get the\nnew X, blah, blah.",
    "start": "3265890",
    "end": "3271650"
  },
  {
    "text": "And you can see that\nsampling in this model",
    "start": "3271650",
    "end": "3277710"
  },
  {
    "text": "is slow, like in\nautoregressive models, because in order to compute\nthe parameters that you need",
    "start": "3277710",
    "end": "3283920"
  },
  {
    "text": "to transform the Ith simple\nprior or random variable, ZI,",
    "start": "3283920",
    "end": "3291270"
  },
  {
    "text": "you need to have\nall the previous X's to figure out, what's\nthe right shift in scale",
    "start": "3291270",
    "end": "3296559"
  },
  {
    "text": "that you need to apply? What is the inverse mapping?",
    "start": "3296560",
    "end": "3301849"
  },
  {
    "text": "How do you go from X to Z? Well, the good news is that\nyou can compute all these mu's",
    "start": "3301850",
    "end": "3309220"
  },
  {
    "text": "and alphas in parallel, because\nonce you have the image, you have all the X's, so you can\ncompute all the mu's and alphas",
    "start": "3309220",
    "end": "3318550"
  },
  {
    "text": "or the shifts in\nscales in parallel. And then you compute\nthe corresponding Z's",
    "start": "3318550",
    "end": "3326170"
  },
  {
    "text": "by just inverting that shift\nin scale transformation. So if you recall, if\nyou want to, you know,",
    "start": "3326170",
    "end": "3333730"
  },
  {
    "text": "compute Z1 from X1, what\nyou do is you take X1, you subtract mu1, and you\ndivide by this exponential,",
    "start": "3333730",
    "end": "3342273"
  },
  {
    "text": "by this scaling. Just like in real MVP, that's\nhow you do the transformation.",
    "start": "3342273",
    "end": "3348579"
  },
  {
    "text": "And so sampling, you can\nsee, you go from Z to X, and you need to\ndo one at a time.",
    "start": "3348580",
    "end": "3354289"
  },
  {
    "text": "But because these alphas\nand mu's depend on the X's at inference time\nor during learning,",
    "start": "3354290",
    "end": "3361000"
  },
  {
    "text": "you can compute all the\nmu's and alpha in parallel.",
    "start": "3361000",
    "end": "3367040"
  },
  {
    "text": "And then you can compute the\nZ's, again, all in parallel, just by shifting and scaling.",
    "start": "3367040",
    "end": "3373549"
  },
  {
    "text": "And then the Jacobian\nis still lower diagonal,",
    "start": "3373550",
    "end": "3380010"
  },
  {
    "text": "and so you have an efficient\ndeterminant computation, and so you can\nevaluate likelihoods",
    "start": "3380010",
    "end": "3386810"
  },
  {
    "text": "efficiently in parallel, just\nlike in an autoregressive model. If you remember, the nice thing\nabout an autoregressive model",
    "start": "3386810",
    "end": "3392990"
  },
  {
    "text": "is that in principle, you can\nevaluate all the conditionals in parallel, because\nyou have all you need.",
    "start": "3392990",
    "end": "3399440"
  },
  {
    "text": "You know how to compute it. Once you have all\nthe whole X vector, you can compute all\nthe conditionals, and you can evaluate the loss\non each individual component",
    "start": "3399440",
    "end": "3409700"
  },
  {
    "text": "of your random variable. And the same is true here. So you can basically define a\nmodel inspired by autoregressive",
    "start": "3409700",
    "end": "3423020"
  },
  {
    "text": "models, which is called a\nmasked autoregressive flow, MF,",
    "start": "3423020",
    "end": "3429870"
  },
  {
    "text": "that basically transforms simple\nrandom variables, Z into X, or equivalently, X to Z's.",
    "start": "3429870",
    "end": "3437270"
  },
  {
    "text": "And if you parameterize\nit this way, then you can get efficient\nlearning, basically,",
    "start": "3437270",
    "end": "3445109"
  },
  {
    "text": "because you can go from X to Z\nvery efficiently in parallel. But as expected,\nsampling is slow,",
    "start": "3445110",
    "end": "3451952"
  },
  {
    "text": "because it's just an\nautoregressive model at the end of the day. So if you want to sample, you\nto go through this process of,",
    "start": "3451952",
    "end": "3460965"
  },
  {
    "text": "basically, transforming\neach individual ZI variable one at a time.",
    "start": "3460965",
    "end": "3467790"
  },
  {
    "text": "So this is basically\njust interpreting an autoregressive\nmodel as a flow model,",
    "start": "3467790",
    "end": "3474359"
  },
  {
    "text": "and it inherits the properties\nof autoregressive model, which is the same model. So sampling is sequential\nand slow, but as",
    "start": "3474360",
    "end": "3482160"
  },
  {
    "text": "expected, you can\nevaluate likelihoods, because it's just basically\na change of variable formula, and so it's possible to\nactually compute all these.",
    "start": "3482160",
    "end": "3492330"
  },
  {
    "text": "The likelihood is exactly like\nin an autoregressive model. And so this is another way of\nbuilding a flow model, which",
    "start": "3492330",
    "end": "3500350"
  },
  {
    "text": "is basically, you start with\nan autoregressive model, a continuous one, and then you\ncan essentially think of it--",
    "start": "3500350",
    "end": "3508270"
  },
  {
    "text": "at least if it's a Gaussian\nautoregressive model, you can interpret\nit as a continuous--",
    "start": "3508270",
    "end": "3514869"
  },
  {
    "text": "as a normalizing flow model.  The other thing you can\ndo is, if you need a model",
    "start": "3514870",
    "end": "3522359"
  },
  {
    "text": "that you can sample\nfrom efficiently, we know that one of the issues\nwith autoregressive models",
    "start": "3522360",
    "end": "3529290"
  },
  {
    "text": "is that sampling is slow,\nbecause you have to generate one variable at a time.",
    "start": "3529290",
    "end": "3534859"
  },
  {
    "text": "If you just-- once\nyou start thinking of an autoregressive\nmodel as a flow model, you can just turn this picture\naround and call the X a Z",
    "start": "3534860",
    "end": "3546900"
  },
  {
    "text": "and the Z an X.\nAnd at that point,",
    "start": "3546900",
    "end": "3551947"
  },
  {
    "text": "it's just another\ninvertible transformation. So which one is the input? Which one is the output? Doesn't actually matter.",
    "start": "3551947",
    "end": "3557670"
  },
  {
    "text": "It's just an invertible\nneural network, and you can use it one way or\nyou can use it the other way, and it's still an\ninvertible neural network.",
    "start": "3557670",
    "end": "3564270"
  },
  {
    "text": "And if you do that,\nyou get something called an inverse autoregressive\nflow, which is basically just",
    "start": "3564270",
    "end": "3569940"
  },
  {
    "text": "the same, neural network used\nin the other direction, where",
    "start": "3569940",
    "end": "3575700"
  },
  {
    "text": "if you do it in the\nother direction, now you are allowed to do the\nforward mapping from Z to X",
    "start": "3575700",
    "end": "3581010"
  },
  {
    "text": "in parallel. So you can actually generate\nin a single shot, essentially.",
    "start": "3581010",
    "end": "3587260"
  },
  {
    "text": "You can generate each component\nof the output in parallel without waiting for the previous\nor the previous entries.",
    "start": "3587260",
    "end": "3595420"
  },
  {
    "text": "Because we know that the\ncomputation in that direction is parallel, basically, you can\nsample all the Z's independently",
    "start": "3595420",
    "end": "3603309"
  },
  {
    "text": "from the prior. And if the mu's and alphas\ndepend on the Z's, then you already have them, and you\ncan compute all of them,",
    "start": "3603310",
    "end": "3611800"
  },
  {
    "text": "again, in parallel.  And then you just shift\nand scale all the outputs",
    "start": "3611800",
    "end": "3618430"
  },
  {
    "text": "by the right amount, and\nthen you produce a sample. And so if you basically\nflip things around,",
    "start": "3618430",
    "end": "3626130"
  },
  {
    "text": "you get a model where you can\ndo very efficient sampling. It's no longer sequential,\nlike an autoregressive model,",
    "start": "3626130",
    "end": "3632490"
  },
  {
    "text": "but everything can\nbe done in parallel. Of course, the downside of this\nis that now, inverting the model",
    "start": "3632490",
    "end": "3640710"
  },
  {
    "text": "is sequential. So it's still an\ninvertible mapping, but now if we want\nto go from X to Z,",
    "start": "3640710",
    "end": "3646950"
  },
  {
    "text": "let's say, because we\nwant to train this model, so we want to do maximum\nlikelihood training,",
    "start": "3646950",
    "end": "3651990"
  },
  {
    "text": "then we need to be\nable to go from images, let's say from X,\nto latent variables.",
    "start": "3651990",
    "end": "3659160"
  },
  {
    "text": "You have to be able to do it\nfor every single data point. And if you try to figure out,\nwhat does the computation graph",
    "start": "3659160",
    "end": "3664619"
  },
  {
    "text": "look like? You can see that it\nbecomes sequential, because what you have to\ndo is you have to shift--",
    "start": "3664620",
    "end": "3672240"
  },
  {
    "text": "you have to compute Z1 by\ninverting this relationship. So you take the first pixel,\nyou shift it and scale it,",
    "start": "3672240",
    "end": "3678780"
  },
  {
    "text": "and you get the new\nlatent variable. Now you can use\nthat latent variable to compute the new shift and\nscale for the second dimension.",
    "start": "3678780",
    "end": "3687210"
  },
  {
    "text": "These mu's, they still\ndepend on the alphas, and the mules depend on\nthe previous variables.",
    "start": "3687210",
    "end": "3692319"
  },
  {
    "text": "So now that you have Z1, you\ncan compute mu2 and alpha 2.",
    "start": "3692320",
    "end": "3698030"
  },
  {
    "text": "And now you can shift\nand scale X2 to get Z2. And now you can use Z1 and\nZ2 to compute the new shift",
    "start": "3698030",
    "end": "3705950"
  },
  {
    "text": "in scale and so forth. So that's basically\nthe same thing",
    "start": "3705950",
    "end": "3711700"
  },
  {
    "text": "you would have to\ndo when you were-- that you normally\ndo when you sample from an autoregressive model.",
    "start": "3711700",
    "end": "3718070"
  },
  {
    "text": "So you have to generate\none variable at a time. Here you have to invert\none variable at a time",
    "start": "3718070",
    "end": "3724480"
  },
  {
    "text": "before you can invert the next. And so this is a\ngreat model that",
    "start": "3724480",
    "end": "3730210"
  },
  {
    "text": "allows you to sample\nvery efficiently, but it's very expensive\nto actually compute likelihoods of data points.",
    "start": "3730210",
    "end": "3735950"
  },
  {
    "text": "So this would be a tricky\nmodel to use during training, because you would have to go\nthrough each individual variable",
    "start": "3735950",
    "end": "3744670"
  },
  {
    "text": "to be able to invert and\nto be able to compute, to be able to\ncompute likelihoods.",
    "start": "3744670",
    "end": "3749840"
  },
  {
    "text": "The good thing is\nthat it's actually fast to evaluate likelihoods\nof a generated point.",
    "start": "3749840",
    "end": "3755660"
  },
  {
    "text": "So if you generate\nthe data yourself, then it's easy to evaluate\nlikelihoods because you already",
    "start": "3755660",
    "end": "3762230"
  },
  {
    "text": "have all the Z's. Then you map them to X,\nwhich you can do efficiently if you store the\nlatent vector that you",
    "start": "3762230",
    "end": "3770860"
  },
  {
    "text": "use to generate a\nparticular data point, then you don't have\nto recompute it. You already have it,\nso you can actually",
    "start": "3770860",
    "end": "3777250"
  },
  {
    "text": "evaluate likelihoods\nof data points you generate yourself\nvery efficiently. ",
    "start": "3777250",
    "end": "3784799"
  },
  {
    "text": "Because all you need\nis you need to be able to evaluate the\nlikelihood of these,",
    "start": "3784800",
    "end": "3790170"
  },
  {
    "text": "Z1 through Zn and the prior. You need to be able to evaluate\nthe determinant of the Jacobian, which depends on\nthese alphas and which",
    "start": "3790170",
    "end": "3797790"
  },
  {
    "text": "you can compute, because you\nhave all the Z's to begin with, if you generate the\ndata point yourself.",
    "start": "3797790",
    "end": "3804980"
  },
  {
    "text": "And we'll see that this is\ngoing to be somewhat useful when we talk about how\nto distill models,",
    "start": "3804980",
    "end": "3813320"
  },
  {
    "text": "so that if you have a model that\nis maybe autoregressive and is slow to sample from, we're\ngoing to see that it's",
    "start": "3813320",
    "end": "3821900"
  },
  {
    "text": "possible to distill it into\na model of this type, so different kind of flow\nthat, after you train",
    "start": "3821900",
    "end": "3828890"
  },
  {
    "text": "a model, like a\nstudent model that is much faster than\nthe teacher model",
    "start": "3828890",
    "end": "3835010"
  },
  {
    "text": "that you train autoregressively\nand can generate in one shot,",
    "start": "3835010",
    "end": "3840200"
  },
  {
    "text": "in parallel, like this. And yeah, this property\nat the end here, the fact that you can\nevaluate likelihoods of points",
    "start": "3840200",
    "end": "3847260"
  },
  {
    "text": "you generate yourself\nis going to be useful when we talk about that. And again, like these\ntwo normalizing flows,",
    "start": "3847260",
    "end": "3857920"
  },
  {
    "text": "MAF, IAF, are actually the\nsame model, essentially. It's just, if you swap\nthe role of X and Z,",
    "start": "3857920",
    "end": "3865120"
  },
  {
    "text": "they are essentially the\nsame, the same kind of thing. If you think of it from\nthe perspective of MAF,",
    "start": "3865120",
    "end": "3871809"
  },
  {
    "text": "then you compute\nthe X, the alphas, and the mu's, as a\nfunction of the X's.",
    "start": "3871810",
    "end": "3876950"
  },
  {
    "text": "And that's the way you would do\nit in an autoregressive model. If you just flip\nthings around, you",
    "start": "3876950",
    "end": "3882640"
  },
  {
    "text": "can get an inverse\nautoregressive flow by just having the\nmus and the alphas",
    "start": "3882640",
    "end": "3890350"
  },
  {
    "text": "depend on the Z's, which is\nbasically what you get if you relabel Z and X in that figure.",
    "start": "3890350",
    "end": "3898680"
  },
  {
    "text": "And so that's another\nway to get a flow model, is to basically start with a\nGaussian autoregressive model,",
    "start": "3898680",
    "end": "3905200"
  },
  {
    "text": "and then you can get\na flow model that way. ",
    "start": "3905200",
    "end": "3911060"
  },
  {
    "text": "And yeah, so they're\nessentially do-- they're essentially\nthe same thing. ",
    "start": "3911060",
    "end": "3918670"
  },
  {
    "text": "And so the trade-offs,\nlike our MAF, it's basically an\nautoregressive model.",
    "start": "3918670",
    "end": "3924859"
  },
  {
    "text": "So you have fast likelihood\nevaluation, slow sampling, one variable at a time. IAF is the opposite\nbecause it's the reverse,",
    "start": "3924860",
    "end": "3932420"
  },
  {
    "text": "so you can get fast\nsampling, but then you have slow likelihood evaluation.",
    "start": "3932420",
    "end": "3937869"
  },
  {
    "text": "MAF is good for training\nbecause what we need is we need to be able to\nevaluate likelihoods efficiently",
    "start": "3937870",
    "end": "3944650"
  },
  {
    "text": "for every data point, if you\nwant to do maximum likelihood training. And so MF is much\nbetter for that.",
    "start": "3944650",
    "end": "3951220"
  },
  {
    "text": "On the other hand,\nif you need something where you need to be\nable to generate very, very quickly, IAF would be\na better kind of solution.",
    "start": "3951220",
    "end": "3958290"
  },
  {
    "text": " And natural question, can we\nget the best of both worlds?",
    "start": "3958290",
    "end": "3966070"
  },
  {
    "text": "And that's what they did with\nthis parallel wavenet, which used to be a state of the art\nmodel for speech generation.",
    "start": "3966070",
    "end": "3976090"
  },
  {
    "text": "And the basic idea was to\nstart with a really good autoregressive model and then\ndistill it, which is just MAF,",
    "start": "3976090",
    "end": "3985770"
  },
  {
    "text": "basically, and then distill it\ninto an IAF student model that",
    "start": "3985770",
    "end": "3993030"
  },
  {
    "text": "is going to be hopefully\nclose to the teacher and is going to be much faster\nto generate samples from.",
    "start": "3993030",
    "end": "4000070"
  },
  {
    "text": "And so that's basically\nthe strategy they used. They used an MAF, which is\njust an autoregressive model,",
    "start": "4000070",
    "end": "4007119"
  },
  {
    "text": "to train a student,\na teacher model. You can compute\nlikelihoods efficiently.",
    "start": "4007120",
    "end": "4013290"
  },
  {
    "text": "It's just an\nautoregressive model, so it's easy to\ntrain the usual way.",
    "start": "4013290",
    "end": "4018539"
  },
  {
    "text": "And once you've\ntrained this teacher, you can train a student model\nto be close to the teacher.",
    "start": "4018540",
    "end": "4026460"
  },
  {
    "text": "But because it's an\nIAF model by design, it would allow you to sample\nmuch more efficiently.",
    "start": "4026460",
    "end": "4035800"
  },
  {
    "text": "And the key observation\nthat we mentioned before is that you can actually\nevaluate likelihoods",
    "start": "4035800",
    "end": "4041800"
  },
  {
    "text": "on your own samples. So if you generate\nthe samples yourself, you can actually evaluate\nlikelihoods efficiently.",
    "start": "4041800",
    "end": "4050020"
  },
  {
    "text": "And then basically,\none way to do it is this objective function,\nwhich is basically based",
    "start": "4050020",
    "end": "4059599"
  },
  {
    "text": "on KL divergence, where\nwhat you would do is you would first train the teacher\nmodel by maximum likelihood.",
    "start": "4059600",
    "end": "4066760"
  },
  {
    "text": "This is your\nautoregressive model that is expensive to sample from. And then you define some\nkind of KL divergence",
    "start": "4066760",
    "end": "4073369"
  },
  {
    "text": "between the student\ndistribution, which is an IAF model, efficient to\nsample from, and the teacher",
    "start": "4073370",
    "end": "4078380"
  },
  {
    "text": "model. And this is just\nthe KL divergence between student and teacher.",
    "start": "4078380",
    "end": "4084359"
  },
  {
    "text": "And this is important that we're\ndoing it in this direction. You could also do\ndivergence teacher-student,",
    "start": "4084360",
    "end": "4089660"
  },
  {
    "text": "but here we're doing\ndivergence student-teacher. And the KL divergence,\nif you expand it,",
    "start": "4089660",
    "end": "4097200"
  },
  {
    "text": "it basically has this form. And you can see that this\nobjective is good for training,",
    "start": "4097200",
    "end": "4103719"
  },
  {
    "text": "because what we need to do in\norder to evaluate that objective and optimize it is we need to\nbe able to generate samples",
    "start": "4103720",
    "end": "4110680"
  },
  {
    "text": "from the student\nmodel efficiently. The student model\nis an IAF model, so it's very easy\nto sample from.",
    "start": "4110680",
    "end": "4117370"
  },
  {
    "text": "We need to be able to evaluate\nthe log probability of a sample according to the teacher model.",
    "start": "4117370",
    "end": "4125049"
  },
  {
    "text": "That's, again, easy to\ndo, because it's just an autoregressive model, so\nevaluating likelihoods is easy.",
    "start": "4125050",
    "end": "4130420"
  },
  {
    "text": " To evaluate the likelihood\nof the data point",
    "start": "4130420",
    "end": "4136568"
  },
  {
    "text": "that you generate yourself\nusing the student model, which is what you\nneed for this term,",
    "start": "4136569",
    "end": "4141970"
  },
  {
    "text": "again, that's efficient\nto do if you have an IAF model because you've generated\nthe samples yourself, so you know the Z, so you know\nhow to evaluate likelihoods.",
    "start": "4141970",
    "end": "4151299"
  },
  {
    "text": "And so this kind of objective\nis very, very suitable for this kind of setting,\nwhere the student model",
    "start": "4151300",
    "end": "4157479"
  },
  {
    "text": "is something you can sample\nfrom efficiently from, you can evaluate likelihoods on\nyour own samples efficiently,",
    "start": "4157479",
    "end": "4162640"
  },
  {
    "text": "and then you have a\nteacher model for which you can evaluate likelihoods. Maybe it's expensive\nto sample from,",
    "start": "4162640",
    "end": "4168910"
  },
  {
    "text": "but we don't care,\nbecause we never sample from the teacher model. You just need to be able to do\ngood [INAUDIBLE] training, which",
    "start": "4168910",
    "end": "4175870"
  },
  {
    "text": "we know we can do with\nautoregressive models. And to the extent that this\nKL divergence is small,",
    "start": "4175870",
    "end": "4180880"
  },
  {
    "text": "then the student\ndistribution is going to be close to the\nteacher distribution. So if you sample from\nthe student model,",
    "start": "4180880",
    "end": "4186140"
  },
  {
    "text": "you're going to get something\nsimilar to what you would have gotten if you were to sample\nfrom the teacher model,",
    "start": "4186140",
    "end": "4191489"
  },
  {
    "text": "but it's much, much faster.  And all the operations\nthat you see there,",
    "start": "4191490",
    "end": "4198620"
  },
  {
    "text": "they can be implemented\nefficiently. And that's what they did\nfor this parallel WaveNet.",
    "start": "4198620",
    "end": "4205170"
  },
  {
    "text": "You train a teacher model\nby maximum likelihood, and then you train\na student IAF model",
    "start": "4205170",
    "end": "4210680"
  },
  {
    "text": "to minimize this KL divergence. And at test time, then you\nthrow away your teacher. And at test time, what\nyou put on mobile phones",
    "start": "4210680",
    "end": "4220070"
  },
  {
    "text": "to generate samples\nvery efficiently is to use the student model. I am wondering, in\nthe competition,",
    "start": "4220070",
    "end": "4227390"
  },
  {
    "text": "so we use the IAF first to\nsample then to assign density.",
    "start": "4227390",
    "end": "4233240"
  },
  {
    "text": "Then doing backpropagation,\ndo we also update the parameters for the density\ncalculation part of the student,",
    "start": "4233240",
    "end": "4243230"
  },
  {
    "text": "not only the sampling part? Yes, so you do\noptimize this function. So you do need to optimize\nboth this, the sampling,",
    "start": "4243230",
    "end": "4252020"
  },
  {
    "text": "but you don't need to\noptimize T. T is fixed. And because everything\ncan be reparameterized,",
    "start": "4252020",
    "end": "4257360"
  },
  {
    "text": "so you can still\nbackpropagate through that, because essentially, it's just\nlike a big reparameterization",
    "start": "4257360",
    "end": "4263540"
  },
  {
    "text": "trick that you're doing\non the student model, is just taking with\nstarting with simple noise",
    "start": "4263540",
    "end": "4268670"
  },
  {
    "text": "and then transforming it. And so it's easy\nto figure out how, if you were to update the\nparameter of the student model,",
    "start": "4268670",
    "end": "4274310"
  },
  {
    "text": "how would the sample change? You can do it in this\ncase, because it's",
    "start": "4274310",
    "end": "4279440"
  },
  {
    "text": "the same as reparameterization. ",
    "start": "4279440",
    "end": "4284920"
  },
  {
    "text": "And yeah, that's what\nthey did, and they were able to get very,\nvery impressive speed-ups.",
    "start": "4284920",
    "end": "4291790"
  },
  {
    "text": "This was, yeah, a paper\nfrom Google a few years ago, and that's how they were\nable to actually deploy the models in production.",
    "start": "4291790",
    "end": "4298250"
  },
  {
    "text": "They train a really\ngood teacher model by training it autoregressively. That was too slow\nto generate samples,",
    "start": "4298250",
    "end": "4304330"
  },
  {
    "text": "but then by thinking it from\nthis flow model perspective,",
    "start": "4304330",
    "end": "4309890"
  },
  {
    "text": "and there was a pretty\nnatural way of distilling down into something similar but\nthat has the opposite property",
    "start": "4309890",
    "end": "4317500"
  },
  {
    "text": "of being able to\nsample efficiently, even though you cannot\nget likelihoods. If you just care\nabout inference,",
    "start": "4317500",
    "end": "4322870"
  },
  {
    "text": "you just care about\ngeneration, that's a more convenient\nway of parameterizing",
    "start": "4322870",
    "end": "4328540"
  },
  {
    "text": "the family of distributions. Is it possible to do something\nsimilar for language models?",
    "start": "4328540",
    "end": "4335470"
  },
  {
    "text": "That would be great. The problem with\nthe-- the question is, can we do this\nfor language models? The problem is that if you\nhave a language model that's",
    "start": "4335470",
    "end": "4342282"
  },
  {
    "text": "discrete, and so you\ncan't necessarily think of it as a\nflow model, and so",
    "start": "4342282",
    "end": "4350240"
  },
  {
    "text": "you can't really think of\nsampling from a language model as transforming a simple\ndistribution, at least not",
    "start": "4350240",
    "end": "4357560"
  },
  {
    "text": "in a differentiable, invertible\nway, because the X is discrete,",
    "start": "4357560",
    "end": "4363390"
  },
  {
    "text": "and so there is not\nreally a way to transform a continuous distribution\ninto a discrete one.",
    "start": "4363390",
    "end": "4368450"
  },
  {
    "text": "And so you can't do it this\nway, basically, unfortunately. ",
    "start": "4368450",
    "end": "4374929"
  },
  {
    "text": "[INAUDIBLE] why not being\ncontinuous with doing this flow",
    "start": "4374930",
    "end": "4380150"
  },
  {
    "text": "model? Yeah, Flow models are only\napplicable to probability",
    "start": "4380150",
    "end": "4385820"
  },
  {
    "text": "density functions, so you cannot\napply them to probability mass functions where you would have\ndiscrete random variables.",
    "start": "4385820",
    "end": "4393840"
  },
  {
    "text": "So it's only applicable to\ncontinuous random variables, because otherwise, the change a\nvariable format does not apply, so you cannot use it anymore.",
    "start": "4393840",
    "end": "4400469"
  },
  {
    "text": "So you cannot get\nthe IAF distinction? Essentially, yes.",
    "start": "4400470",
    "end": "4405644"
  },
  {
    "text": "OK. Cool, so that's another family.",
    "start": "4405644",
    "end": "4411430"
  },
  {
    "text": "And now for the\nremaining few minutes, we can just go through\na few other options that you have if you want to\nbuild on invertible mappings.",
    "start": "4411430",
    "end": "4419910"
  },
  {
    "text": "One natural thing you might\nwant to do if you start thinking of autoregressive models\nare basically flow models,",
    "start": "4419910",
    "end": "4428580"
  },
  {
    "text": "we know that you can build-- you can use\nconvolutional networks in autoregressive models\nas long as you mask them",
    "start": "4428580",
    "end": "4435930"
  },
  {
    "text": "in the right way. And so the natural\nthing you can ask is if it's possible to\ndefine invertible layers that",
    "start": "4435930",
    "end": "4443070"
  },
  {
    "text": "are convolutional in\nsome way, because we know convolutions are great. And by itself, a convolution\nwould not be invertible,",
    "start": "4443070",
    "end": "4451875"
  },
  {
    "text": "but if you mask it\nin the right way, you can get the structure\nor the computation structure",
    "start": "4451875",
    "end": "4457560"
  },
  {
    "text": "of an autoregressive\nmodel, and you can build up a layer that\nis actually invertible.",
    "start": "4457560",
    "end": "4464895"
  },
  {
    "text": "And if you do things\nin the right way, you can actually make it such\nthat it's not only invertible,",
    "start": "4464895",
    "end": "4470520"
  },
  {
    "text": "but you can actually evaluate\nthe determinant of the Jacobian efficiently. And like in autoregressive\nmodels, like in PixelCNN,",
    "start": "4470520",
    "end": "4480420"
  },
  {
    "text": "really all you have to do is you\nhave to mask the convolutions so that there is some\nkind of ordering.",
    "start": "4480420",
    "end": "4485540"
  },
  {
    "text": "So then that would\ngive you the-- it would not only allow you to\ninvert things more efficiently,",
    "start": "4485540",
    "end": "4490600"
  },
  {
    "text": "but it would also\nallow you to compute the determinant of the\nJacobian efficiently, because it basically makes\nthe Jacobian lower triangular,",
    "start": "4490600",
    "end": "4498020"
  },
  {
    "text": "and so then, we can compute\nJacobian's determinant efficiently. And yeah, which is what I just\nsaid; and basically, what you",
    "start": "4498020",
    "end": "4509290"
  },
  {
    "text": "can do is you can try to\nenforce certain conditions on the parameters of\nthe neural network so that the transformation is\nguaranteed to be invertible.",
    "start": "4509290",
    "end": "4516677"
  },
  {
    "text": "And you can read the\npaper for more details, but essentially,\nwhat it boils down is something like this, where if\nyou have a three-channel input",
    "start": "4516677",
    "end": "4524230"
  },
  {
    "text": "image, like the one\nyou have on the left, and you have, let's say, a\n3-by-3 kernel convolutional",
    "start": "4524230",
    "end": "4530260"
  },
  {
    "text": "kernel, which looks at,\nlet's say, R, G, and B, what you can do is you can mask the\nparameters of that kernel, which",
    "start": "4530260",
    "end": "4539440"
  },
  {
    "text": "in this case is\njust like this cube. That is a cube for\nthe three channels,",
    "start": "4539440",
    "end": "4544540"
  },
  {
    "text": "and you can mask them so\nthat you only look at the--",
    "start": "4544540",
    "end": "4549770"
  },
  {
    "text": "that you only look at the pixels\nthat come before you, basically, in the ordering. So you can see the receptive\nfields of these kernels",
    "start": "4549770",
    "end": "4556730"
  },
  {
    "text": "here on the right. And when you produce the three\nvalues in the three channels,",
    "start": "4556730",
    "end": "4563780"
  },
  {
    "text": "they are produced by a\ncomputation that is basically consistent with this ordering. And you can see that just\nlike in the PixelCNN,",
    "start": "4563780",
    "end": "4571490"
  },
  {
    "text": "you have to decide on which\ncolors you start from. And then you have to be\ncausal, also, with respect",
    "start": "4571490",
    "end": "4579380"
  },
  {
    "text": "to the channels that\nyou have in the image. ",
    "start": "4579380",
    "end": "4586770"
  },
  {
    "text": "And yeah, so\nbasically, there are ways to define convolutional\nkernels that would give you",
    "start": "4586770",
    "end": "4595250"
  },
  {
    "text": "invertible mappings, and\nyou're losing out something",
    "start": "4595250",
    "end": "4600740"
  },
  {
    "text": "because the receptive\nfields, you're no longer be able to look\nat everything in the image.",
    "start": "4600740",
    "end": "4606660"
  },
  {
    "text": "You're restricted in\nwhat you can look at. But what you gain is that\nyou get attractable Jacobian,",
    "start": "4606660",
    "end": "4612440"
  },
  {
    "text": "basically. And you can build a\nflow model by stacking these kind of layers, and\nthis works reasonably well.",
    "start": "4612440",
    "end": "4620670"
  },
  {
    "text": "Here's some examples, MNIST\nsamples, CIFAR-10, ImageNet,",
    "start": "4620670",
    "end": "4625895"
  },
  {
    "text": "samples that you get\nby training, basically, a flow model where you have all\nthese convolutional layers that",
    "start": "4625895",
    "end": "4631940"
  },
  {
    "text": "are crafted in a certain way so\nthat the filters are basically invertible.",
    "start": "4631940",
    "end": "4639599"
  },
  {
    "text": "The other quick thing\nI wanted to mention is a different\nperspective on what",
    "start": "4639600",
    "end": "4648030"
  },
  {
    "text": "happens when you train a flow\nmodel, this idea that you can either think about training a\nmodel such that the distribution",
    "start": "4648030",
    "end": "4656850"
  },
  {
    "text": "of the samples that you get is\nclose to the data distribution. Or you can think of training\nthe model as basically saying,",
    "start": "4656850",
    "end": "4664409"
  },
  {
    "text": "if I were to transform\nmy data according to the inverse\nmapping, I should be getting something that is close\nto the prior of the flow model,",
    "start": "4664410",
    "end": "4671760"
  },
  {
    "text": "as close, for example,\na Gaussian distribution. And so you can use\nthis dual perspective",
    "start": "4671760",
    "end": "4676770"
  },
  {
    "text": "to construct other kinds\nof layers that can get you, that basically try--",
    "start": "4676770",
    "end": "4682830"
  },
  {
    "text": "where every layer is\ntrying to make the data look more Gaussian, essentially.",
    "start": "4682830",
    "end": "4689390"
  },
  {
    "text": "And the basic intuition\nis something like this. If you have a flow model\nwhere you transform",
    "start": "4689390",
    "end": "4695450"
  },
  {
    "text": "a Gaussian random\nvariable into data X, and then you have some\ntrue data distribution,",
    "start": "4695450",
    "end": "4702330"
  },
  {
    "text": "so a true random\nvariable X tilde, which is the one that is\ndistributed really according to the data, if you do maximum\nlikelihood training, what you do",
    "start": "4702330",
    "end": "4710630"
  },
  {
    "text": "is you minimize the KL\ndivergence between the data distribution and\nthe distribution that you get by sampling\nfrom this model,",
    "start": "4710630",
    "end": "4717860"
  },
  {
    "text": "by transforming Gaussian noise\nthrough this invertible mapping F theta. Or equivalently,\nyou're minimizing",
    "start": "4717860",
    "end": "4724429"
  },
  {
    "text": "the KL divergence between the\ndistribution of the true X tilde, which is distributed\naccording to the data,",
    "start": "4724430",
    "end": "4730790"
  },
  {
    "text": "and this new random\nvariable, X, that you get by applying--\nby transforming Gaussian random noise,\nwhich is basically,",
    "start": "4730790",
    "end": "4740690"
  },
  {
    "text": "this is saying that if\nyou take Gaussian samples and you transform them\nthrough this mapping, you should get something\nclose to the data.",
    "start": "4740690",
    "end": "4747720"
  },
  {
    "text": "Equivalently, because of\nproperties of the KL divergence,",
    "start": "4747720",
    "end": "4752790"
  },
  {
    "text": "which is invariant to\ninvertible transformations, you can also think\nof this as trying to minimize the KL divergence\nof what you get by transforming",
    "start": "4752790",
    "end": "4760620"
  },
  {
    "text": "the true data according\nto the inverse mapping and transforming the samples\nthrough the inverse mapping.",
    "start": "4760620",
    "end": "4767610"
  },
  {
    "text": "And we know what we get. If we transform samples\nthrough the inverse mapping, we get the prior.",
    "start": "4767610",
    "end": "4774450"
  },
  {
    "text": "And so equivalently,\nyou can think of training a model\nas transforming",
    "start": "4774450",
    "end": "4783720"
  },
  {
    "text": "through this random vector,\nX tilde, which is distributed according to the\ndata, into one that",
    "start": "4783720",
    "end": "4790200"
  },
  {
    "text": "is distributed as a Gaussian. And so you can think\nof the flow model as basically\nGaussian-izing the data.",
    "start": "4790200",
    "end": "4796580"
  },
  {
    "text": "You start out with a\ncomplicated distribution. And if you go through the flow\nin the backward direction,",
    "start": "4796580",
    "end": "4802660"
  },
  {
    "text": "you're mapping it\ninto something that has to look like a Gaussian.",
    "start": "4802660",
    "end": "4808200"
  },
  {
    "text": "And how to achieve this? One natural way of doing it is\nto basically, at least if you",
    "start": "4808200",
    "end": "4813570"
  },
  {
    "text": "have one-dimensional data\nis through the inverse CDF. And so going through quickly,\nbecause I don't have time,",
    "start": "4813570",
    "end": "4821400"
  },
  {
    "text": "but if you have a\nrandom variable that has some kind of\ndata distribution,",
    "start": "4821400",
    "end": "4828585"
  },
  {
    "text": "if you apply the inverse\nor the CDF of the data--",
    "start": "4828585",
    "end": "4835140"
  },
  {
    "text": "there is going to be a CDF\nfor the data distribution. And if you apply the CDF\nof the data distribution",
    "start": "4835140",
    "end": "4843060"
  },
  {
    "text": "to this random vector\nor random variable, you're going to get a\nuniform random variable. That's basically\nthe way you sample",
    "start": "4843060",
    "end": "4850107"
  },
  {
    "text": "from-- that's one of the ways\nto sample from a random variable with known CDF. You sample uniformly,\nyou inverse the CDF",
    "start": "4850107",
    "end": "4857730"
  },
  {
    "text": "and you get a\nsample from X tilde. And so basically, this\nkind of transformation",
    "start": "4857730",
    "end": "4866219"
  },
  {
    "text": "where you are\ntransforming a data sample through the CDF, which\nis a way to widen the data,",
    "start": "4866220",
    "end": "4873600"
  },
  {
    "text": "It's like the thing you would do\nby subtracting the mean divided by the standard deviation,\nsomething similar,",
    "start": "4873600",
    "end": "4879931"
  },
  {
    "text": "if you apply this kind\nof transformation, you get something\nthat is uniform. So it's guaranteed to\nbe between 0 and 1.",
    "start": "4879932",
    "end": "4886980"
  },
  {
    "text": "And once you have a uniform\nrandom variable, what you can do is you can apply the\ninverse CDF of a Gaussian,",
    "start": "4886980",
    "end": "4892350"
  },
  {
    "text": "and you can transform\nit into something that is exactly Gaussian. ",
    "start": "4892350",
    "end": "4897950"
  },
  {
    "text": "And this, basically,\nthe composition of the true CDF of the data and\nthe inverse CDF of a Gaussian",
    "start": "4897950",
    "end": "4906050"
  },
  {
    "text": "will transform any random\nvector into a Gaussian one.",
    "start": "4906050",
    "end": "4911800"
  },
  {
    "text": "And that's basically the\nidea of Gaussian-izing flows, is that you stack a\nbunch of transformations",
    "start": "4911800",
    "end": "4917920"
  },
  {
    "text": "trying to make the data\nmore and more Gaussian. And I guess I'm\ngoing to skip this,",
    "start": "4917920",
    "end": "4923560"
  },
  {
    "text": "but if you know\nabout copula models, these are a famous kind\nof statistical model.",
    "start": "4923560",
    "end": "4929290"
  },
  {
    "text": "It's often used in Wall Street. You can think of it as a\nshallow kind of normalizing flow",
    "start": "4929290",
    "end": "4935199"
  },
  {
    "text": "where you only apply one\nlayer of Gaussianization each individual dimension.",
    "start": "4935200",
    "end": "4940370"
  },
  {
    "text": "So you start with data that\nis not Gaussian-distributed, then you apply this\nGaussian CDF trick",
    "start": "4940370",
    "end": "4945760"
  },
  {
    "text": "to basically make each\nindividual dimension Gaussian. And even though jointly,\nit's not Gaussian,",
    "start": "4945760",
    "end": "4951850"
  },
  {
    "text": "that's your model of the data. And then you can\nstack them together, and then you keep\ndoing this thing",
    "start": "4951850",
    "end": "4958840"
  },
  {
    "text": "and you apply some\nrotations, then you can transform\nanything into a Gaussian. And this is another way\nof basically building",
    "start": "4958840",
    "end": "4965469"
  },
  {
    "text": "invertible transformations. But I'm out of time, so I think\nthis is a good point to stop.",
    "start": "4965470",
    "end": "4972570"
  },
  {
    "start": "4972570",
    "end": "4978000"
  }
]